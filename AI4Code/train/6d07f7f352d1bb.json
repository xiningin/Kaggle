{"cell_type":{"058bff2f":"code","48f9dd60":"code","bab780ec":"code","1c939629":"code","1822f7d0":"code","2aba60d0":"code","19a3b15c":"code","caadca5a":"code","c7e6fc93":"code","9fa8ea72":"code","655be768":"code","11975180":"code","7a86adf0":"code","52751cfb":"code","ddc39e6e":"code","bcc3b795":"code","2a0e5723":"code","0b7a590b":"code","8cb27ba3":"code","3e6c97a8":"code","d22e222b":"code","8cfaf5dc":"code","9df79ddb":"code","54c4b451":"code","47919ef6":"code","57bc50e7":"code","f5cb080a":"code","f316f824":"code","bdbd03b9":"code","60e82f5d":"code","5d9ad1b7":"code","d82c7975":"code","ff897a49":"code","2c8adb34":"code","e6c8299d":"code","933a4002":"code","0cc7111d":"code","bf54e61c":"code","a9caa66b":"code","70c31e67":"code","be754cd1":"code","7a003c89":"code","b1d4b663":"code","3ac7a852":"code","c7bc21f6":"code","3a30b6b0":"code","f8514eca":"code","85e4ff22":"markdown","65fd52dc":"markdown","af1077aa":"markdown","932cc403":"markdown","d0f01e08":"markdown","5b809aef":"markdown","2cc99ea9":"markdown","e01dc63b":"markdown","8800fc0c":"markdown","2d746d18":"markdown","2fd761f8":"markdown","af21dcf3":"markdown","e2446aa0":"markdown","bacb1f03":"markdown","f13559bb":"markdown","dfc500f7":"markdown","217255b2":"markdown","2c262eaf":"markdown","48106f93":"markdown","56059ac5":"markdown","f30baaa6":"markdown","b353afc5":"markdown","80bc71a3":"markdown","5addb54e":"markdown","8c81bc2d":"markdown","f742f7de":"markdown","73e92210":"markdown","8f427b5c":"markdown","8e6a8b62":"markdown","10c9e27d":"markdown","25f8e388":"markdown","862ac70e":"markdown","18605e7a":"markdown","4cd9f4d5":"markdown","30a1c5d9":"markdown","dbeb7902":"markdown","ddd2db35":"markdown","12befe54":"markdown","04e4be07":"markdown","6868456e":"markdown","8c40dfb1":"markdown","9a52a9bd":"markdown","196abae2":"markdown","64635615":"markdown","b700d51f":"markdown","a0d07767":"markdown","0c1dff9b":"markdown","aa2de56a":"markdown","1afd848a":"markdown","7ea00a19":"markdown","e42f0040":"markdown","c8a5fb07":"markdown","baefd19c":"markdown"},"source":{"058bff2f":"\"\"\"\n\u201cI confirm that this is my own work, except where clearly indicated.\u201d\n\"\"\"\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nimport os\nimport numpy as np \nfrom scipy.stats import norm \nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nimport plotly\nplotly.offline.init_notebook_mode() # For not show up chart error\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n%matplotlib inline\nfrom tqdm import tqdm\n\npd.set_option('display.max_columns', 100)","48f9dd60":"# Loading the data\ntrain = pd.read_csv('..\/input\/train-3\/train_3.csv')\ntest = pd.read_csv('..\/input\/test-3\/test_3.csv')\n","bab780ec":"# Projecting the first 5 rows\ntrain.head()","1c939629":"train.tail()","1822f7d0":"# Checking the dimensions\ntrain.shape","2aba60d0":"# Dropping duplicates\ntrain.drop_duplicates()\n\n# Checking the dimensions again\ntrain.shape","19a3b15c":"# Checking \ntest.shape","caadca5a":"train.info()","c7e6fc93":"# Setting categorical variables for training set\ntrain['continent'] = train['continent'].astype('category')\ntrain['country_code'] = train['country_code'].astype('category')\ntrain['Country_Region'] = train['Country_Region'].astype('category')\n\n# Setting categorical variables for test set\ntest['continent'] = test['continent'].astype('category')\ntest['country_code'] = test['country_code'].astype('category')\ntest['Country_Region'] = test['Country_Region'].astype('category')\n","9fa8ea72":"# Creating the meta data\n\n\n## Something to store information\ndata = []\n\n## Creating a loop\nfor f in train.columns:\n    \n    # Defining the role for each variable\n    if f == 'ConfirmedCases':\n        role = 'ConfirmedCases'\n    elif f == 'Fatalities':\n        role = 'Fatalities'\n    elif f == 'Id':\n        role = 'Id'\n    elif f == 'Date':\n        role = 'Date'\n    else:\n        role = 'input'\n         \n    # Defining the level\n    if 'int' in f or f == 'ConfirmedCases':\n        level = 'inter'\n    elif 'int' in f or f == 'Fatalities':\n        level = 'inter'\n    elif 'int' in f or f == 'population':\n        level = 'interval'\n    elif 'cat' in f or f == 'continent':\n        level = 'nominal'\n    elif 'cat' in f or f == 'country_code':\n        level = 'nominal'\n    elif 'cat' in f or f == 'Country_Region':\n        level = 'nominal'\n    elif 'cat' in f or f == 'Id':\n        level = 'nominal'\n    elif train[f].dtype == float:\n        level = 'interval'\n    elif train[f].dtype == int:\n        level = 'interval'\n    elif train[f].dtype == 'object':\n        level = 'ordinal'\n\n        \n    # Initialize keep to True for all variables except for id\n    keep = True\n    if f == 'Id':\n        keep = False\n    \n    # Defining the data type \n    dtype = train[f].dtype\n    \n    # Creating a Dict that contains all the metadata for the variable\n    f_dict = {\n        'varname': f,\n        'role': role,\n        'level': level,\n        'keep': keep,\n        'dtype': dtype\n    }\n    data.append(f_dict)\n    \n#Saving the meta-train data\nmeta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\nmeta.set_index('varname', inplace=True)\n\n# Saving the meta-test data\nmeta_test = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\nmeta_test.set_index('varname', inplace=True)","655be768":" meta","11975180":"pd.DataFrame({'count' : meta.groupby(['role', 'level'])['role'].size()}).reset_index()","7a86adf0":"pd.DataFrame({'count' : meta_test.groupby(['role', 'level'])['role'].size()}).reset_index()","52751cfb":"# Calling the interval data\nv = meta[(meta.level == 'interval') & (meta.keep)].index\ntrain[v].describe()","ddc39e6e":"# Initiating an empty vector to store information\nvars_with_missing = []\n\n# Going through every column in the interval data and calculating how many missing values we have\nfor f in train.columns:\n    missings = train[train[f] == -1][f].count()\n    if missings > 0:\n        vars_with_missing.append(f)\n        missings_perc = missings\/train.shape[0]\n        \n        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n        \nprint('In total, there are {} variables with missing values'.format(len(vars_with_missing)))","bcc3b795":"# Calling the nominal data\nv = meta[(meta.level == 'nominal') & (meta.keep)].index\ntrain[v].describe()","2a0e5723":"df_now = train.groupby(['Date','Country_Region']).sum().sort_values(['Country_Region','Date']).reset_index()\ndf_now['New Cases'] = df_now['ConfirmedCases'].diff()\ndf_now['New Fatalities'] = df_now['Fatalities'].diff()\ndf_now = df_now.groupby('Country_Region').apply(lambda group: group.iloc[-1:]).reset_index(drop = True)\n\n\ndf_now = df_now.sort_values('ConfirmedCases', ascending = False)\nfig = make_subplots(rows = 2, cols = 2)\nfig.add_bar(x=df_now['Country_Region'].head(10), y = df_now['ConfirmedCases'].head(10), row=1, col=1, name = 'Total cases')\n\ndf_now = df_now.sort_values('Fatalities', ascending=False)\nfig.add_bar(x=df_now['Country_Region'].head(10), y = df_now['Fatalities'].head(10), row=1, col=2, name = 'Total Fatalities')","0b7a590b":"# Calling the nominal data\nv = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(20,10))\n    \n     # Calculate the Fatalities per category value\n    cat_perc = train[[f, 'Fatalities']].groupby([f],as_index=False).mean()\n    cat_perc.sort_values(by='Fatalities', ascending=False, inplace=True)\n    \n    # Bar plot\n    # Order the bars descending on target mean\n    sns.barplot(ax=ax, x=f, y='Fatalities', data=cat_perc, order=cat_perc[f])\n    plt.ylabel('Fatalities', fontsize=18)\n    plt.xlabel(f, fontsize=18)\n    plt.tick_params(axis='both', which='major', labelsize=18)\n    plt.show();","8cb27ba3":"v = meta[(meta.level == 'interval') & (meta.keep)].index\n\nfor f in v:\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(20,10))\n    \n    # Calculate the percentage of target=1 per category value\n    cat_perc = train[[f, 'ConfirmedCases']].groupby([f],as_index=False).mean()\n    cat_perc.sort_values(by='ConfirmedCases', ascending=False, inplace=True)\n    \n    # Bar plot\n    # Order the bars descending on target mean\n    sns.barplot(ax=ax, x=f, y='ConfirmedCases', data=cat_perc, order=cat_perc[f])\n    plt.ylabel('ConfirmedCases', fontsize=18)\n    plt.xlabel(f, fontsize=18)\n    plt.tick_params(axis='both', which='major', labelsize=18)\n    plt.show();","3e6c97a8":"# Correlation matrix\ncorrmat = train.corr() \n  \n# Creating the plot\ncg = sns.clustermap(corrmat, cmap =\"YlGnBu\", linewidths = 0.1); \nplt.setp(cg.ax_heatmap.yaxis.get_majorticklabels(), rotation = 0) \n  \ncg ","d22e222b":"# ConfirmedCases correlation matrix \n# k : number of variables for heatmap \nk = 30\n  \ncols = corrmat.nlargest(k, 'ConfirmedCases')['ConfirmedCases'].index \n  \ncm = np.corrcoef(train[cols].values.T) \nf, ax = plt.subplots(figsize =(12, 10)) \n  \nsns.heatmap(cm, ax = ax, cmap =\"binary\", \n            linewidths = 0.1, yticklabels = cols.values,  \n                              xticklabels = cols.values) ","8cfaf5dc":"# Calling the nominal data\nv = meta[(meta.level == 'nominal') & (meta.keep)].index\nprint('Before dummification we have {} variables in train'.format(train.shape[1]))\ntrain = pd.get_dummies(train, columns=v, drop_first=True)\nprint('After dummification we have {} variables in train'.format(train.shape[1]))","9df79ddb":"# Calling the nominal data\nv = meta_test[(meta_test.level == 'nominal') & (meta_test.keep)].index\nprint('Before dummification we have {} variables in train'.format(test.shape[1]))\ntest = pd.get_dummies(test, columns=v, drop_first=True)\nprint('After dummification we have {} variables in train'.format(test.shape[1]))","54c4b451":"# Calling the interval data\nv = meta[(meta.level == 'interval') & (meta.keep)].index\npoly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n\n# Creating the df with the interactions\ninteractions = pd.DataFrame(data=poly.fit_transform(train[v]), columns=poly.get_feature_names(v))\ninteractions.drop(v, axis=1, inplace=True)  # Remove the original columns\n\n# Concat the interaction variables to the train data\nprint('Before creating interactions we have {} variables in train'.format(train.shape[1]))\ntrain = pd.concat([train, interactions], axis=1)\nprint('After creating interactions we have {} variables in train'.format(train.shape[1]))","47919ef6":"# Calling the interval data\nv = meta_test[(meta_test.level == 'interval') & (meta_test.keep)].index\npoly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n\n# Creating the df with the interactions\ninteractions = pd.DataFrame(data=poly.fit_transform(test[v]), columns=poly.get_feature_names(v))\ninteractions.drop(v, axis=1, inplace=True)  # Remove the original columns\n\n# Concat the interaction variables to the train data\nprint('Before creating interactions we have {} variables in train'.format(test.shape[1]))\ntest = pd.concat([test, interactions], axis=1)\nprint('After creating interactions we have {} variables in train'.format(test.shape[1]))","57bc50e7":"# Dropping NA values\ntrain = train.dropna()\n\n# Verifying that no N\/A values exist\ntrain.isnull().sum().sum()","f5cb080a":"# Setting the variance threshold\nselector = VarianceThreshold(threshold=.01)\nselector.fit(train.drop(['Id', 'ConfirmedCases','Fatalities','Date'], axis=1)) # Fit to train without the variables we need for submitting\n\nf = np.vectorize(lambda x : not x) # Function to toggle boolean array elements\n\n# finding variables with lower variance than threshold\nv = train.drop(['Id', 'ConfirmedCases','Fatalities','Date'], axis=1).columns[f(selector.get_support())]\nprint('{} variables have too low variance.'.format(len(v)))","f316f824":"# Getting the train and labels\nX_train = train.drop(['Id', 'ConfirmedCases','Fatalities','Date'], axis=1)\ny_train = train['Fatalities']\n\n# Getting the columns\nfeat_labels = X_train.columns\n\n# Fitting a Random Forest Classifier\nrf = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\n\nrf.fit(X_train, y_train)\n\n# Getting the importances calculated from the RFC\nimportances = rf.feature_importances_\n\n# Sorting the variables by importance\nindices = np.argsort(rf.feature_importances_)[::-1]\n\n# Creating a loop that is going to show the importances per variable ranked from most important to less important\nfor f in range(20):\n    print(\"%2d) %-*s %f\" % (f + 1, 30,feat_labels[indices[f]], importances[indices[f]]))","bdbd03b9":"# Setting the threshold for which variables to keep based on their variance contribution\nsfm = SelectFromModel(rf, threshold='median', prefit=True)\nprint('Number of features before selection: {}'.format(X_train.shape[1]))\n\n# Throwing away all the variables which fall below the threshold level specified above\nn_features = sfm.transform(X_train).shape[1]\nprint('Number of features after selection: {}'.format(n_features))\n\n# Creating a list with the selected variables\nselected_vars = list(feat_labels[sfm.get_support()])","60e82f5d":"# Forming the final training set based on the feature selection \ntrain = train[selected_vars + ['ConfirmedCases','Fatalities','Date']]\n\n# Applying the selected variables to the test set as well\ntest = test[selected_vars + ['Date']]\n\ntrain_copy = train\ntest_copy = test","5d9ad1b7":"# Creating a copy of the training and test sets\ntrain_unscaled = train_copy\ntest_unscaled = test_copy","d82c7975":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit( n_splits = 1, test_size = 0.2)\nfor train_index, test_index in split.split(train_unscaled, train_unscaled[\"population\"]):\n    train_strat = train_unscaled.loc[train_index]\n    valid_strat = train_unscaled.loc[test_index]\n\ny_train = train_strat[['ConfirmedCases','Fatalities']]\nx_train = train_strat.drop(['ConfirmedCases','Fatalities'], axis=1)\n\ny_valid = valid_strat[['ConfirmedCases','Fatalities']]\nx_valid = valid_strat.drop(['ConfirmedCases','Fatalities'], axis=1)","ff897a49":"# Creating copies of the datasets\ntrain_1 = x_train\ntest_1 = test_unscaled\nvalid1 = x_valid","2c8adb34":"## Start encoding\n\n## Assigning distinct numbers to every set\ntrain_1['train_1']=2\nvalid1['train_1']=1\ntest_1['train_1']=0\n\n## Combining the 3 sets\ncombined = pd.concat([train_1, valid1, test_1])\n\n# Getting dummies from the combined dataset\ndf = pd.get_dummies(combined['Date'])\n\n# Concatinating the dummy set with the combined set\ncombined = pd.concat([combined,df], axis = 1)\n\n## Forming the 3 sets using the distinct numbers that we initially set.\ntrain_df = combined[combined[\"train_1\"]== 2]\nvalid_df = combined[combined[\"train_1\"]== 1]\ntest_df = combined[combined[\"train_1\"]==0]\n\n# Forming the end sets\ntrain_df.drop([\"train_1\"], axis = 1, inplace = True)\nvalid_df.drop([\"train_1\"], axis = 1, inplace = True)\ntest_df.drop([\"train_1\"], axis = 1, inplace = True)","e6c8299d":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Splitting between numerical and other variables\ntrain_num = train_df.select_dtypes(include=[\"number\"])\ntrain_cat = train_df.select_dtypes(exclude=[\"number\"])\n\n# Creating a pipeline\nnum_pipeline = Pipeline([\n    ('std_scaler', StandardScaler()),\n])\n\n## Getting the numerical and categorical variables\nnum_attribs = list(train_num)\ncat_attribs = list(train_cat)\n\nfull_pipeline = ColumnTransformer([\n    (\"num\", num_pipeline, num_attribs),\n])\n\n# Applying the transformation\nx_train = full_pipeline.fit_transform(train_df)\nx_valid = full_pipeline.fit_transform(valid_df)\ntest_pip = full_pipeline.fit_transform(test_df)","933a4002":"test_df","0cc7111d":"test_pip.shape","bf54e61c":"from numpy import absolute\nfrom numpy import mean\nfrom sklearn.metrics import mean_squared_log_error\nfrom numpy import std\nfrom sklearn.datasets import make_regression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.metrics import  make_scorer\n\nnp.random.seed(9)\n# Creating the mean squared log error metric to let Scikit library use it in cross-validation\nscorer = make_scorer(mean_squared_log_error, greater_is_better=False)\n\n# define model\nmodel = DecisionTreeRegressor()\n\n# evaluate model\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, x_train, y_train, scoring= scorer, cv=cv, n_jobs=1)\n\n# summarize performance\nn_scores = absolute(n_scores)\nn_scores = np.sqrt(n_scores)\nprint('Result: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))","a9caa66b":"np.random.seed(15)\n\n# Fitting the model on the training set\nmodel.fit(x_train,y_train)\n\n# Getting the predictions\ny_pred = model.predict(x_valid)\n\n# Calculating the loss\nloss = np.sqrt(mean_squared_log_error( y_valid, y_pred ))\nprint(loss)","70c31e67":"from tensorflow import keras\nfrom functools import partial\nfrom sklearn.model_selection import KFold\nfrom tensorflow import keras\nimport tensorflow as tf\nimport pandas as pd\n\n\n# Adding early stopping rules, checkpoint rules and Learning scheduling to improve the learning rate.\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\"keras_model_assign_2.h5\",save_best_only = True) # making sure the model is saved at every epoch and we are saving the best weights\nearly_stopping_cb = keras.callbacks.EarlyStopping( patience = 10, restore_best_weights=True) # Early stopping rule while preserving best weights\nlr_scheduler = keras.callbacks.ReduceLROnPlateau(factor = 0.5, patience=5) # reduces the learning rate by 0.5 when the validation score doesn't improve for 5 rounds\n\noptimizer = keras.optimizers.SGD(lr= 0.001, momentum = 0.9, nesterov=True) #adding an optimization parameter to improve learning rate\n\n\n####################\n###################\nseed = 7\ncvscores = []\n\n# Converting the train set to array for indexing\nX = np.array(x_train)\nY = np.array(y_train)\n\nX_valid = np.array(x_valid)\nY_valid = np.array(y_valid)\n\nnp.random.seed(seed)\n# define 5-fold cross validation test harness\nkfold = KFold(5, True, 1)\n\nfor train, test in kfold.split(X, Y):\n  # create model\n    model2 = tf.keras.models.Sequential([\n    keras.layers.Flatten(input_shape = x_train.shape[1:]),\n    keras.layers.Dense(100, activation = \"selu\", kernel_initializer = \"lecun_normal\"),\n    keras.layers.Dense(100, activation = \"selu\", kernel_initializer = \"lecun_normal\"),\n    keras.layers.Dense(100,activation = \"selu\", kernel_initializer = \"lecun_normal\"),\n    keras.layers.Dense(50, activation = \"selu\", kernel_initializer = \"lecun_normal\"),\n    keras.layers.Dropout(rate = 0.1),\n    keras.layers.Dense(2, activation = \"relu\", kernel_initializer = \"he_normal\")\n    ])\n    \n    # Compiling the model\n    model2.compile(loss=\"mean_squared_logarithmic_error\",\n             optimizer = optimizer)\n    \n    # Fitting the model\n    history_2 = model2.fit(X[train], Y[train],epochs=200, verbose=0,\n                        validation_data = (X_valid, Y_valid),\n                        callbacks = [checkpoint_cb, early_stopping_cb, lr_scheduler])\n    \n    # Evaluating the model\n    y_pred = model2.predict(X[test])\n    loss = np.sqrt(mean_squared_log_error( Y[test], y_pred ))\n    cvscores.append(loss)\n    \nprint(\"Scores:\",cvscores)\nprint(\"Mean:\",np.mean(cvscores))\nprint(\"Standard Deviation:\",np.std(cvscores))","be754cd1":"checkpoint_cb_2 = keras.callbacks.ModelCheckpoint(\"keras_model2_assign_2.h5\",\n                                               save_best_only = True) # making sure the model is saved at every epoch\n\n\n# defining Monte Carlo Dropout layers\nclass MCDropout(keras.layers.Dropout):\n    def call (self,inputs):\n        return super().call(inputs, training = True)\n\n################\n################\nseed = 7\ncvscores = []\nnp.random.seed(seed)\n\n# define 5-fold cross validation test harness\nkfold = KFold(5, True, 1)\n\n# Initiating K-Fold Cross-Validation while fitting the model\nfor train, test in kfold.split(X, Y):\n  # create model\n    model3 = tf.keras.models.Sequential([\n    keras.layers.Flatten(input_shape = x_train.shape[1:]),\n    keras.layers.Dense(100, activation = \"selu\", kernel_initializer = \"lecun_normal\"),\n    keras.layers.Dense(100, activation = \"selu\", kernel_initializer = \"lecun_normal\"),\n    keras.layers.Dense(100,activation = \"selu\", kernel_initializer = \"lecun_normal\"),\n    keras.layers.Dense(50, activation = \"selu\", kernel_initializer = \"lecun_normal\"),\n    MCDropout(rate =0.1),\n    keras.layers.Dense(2, activation = \"relu\", kernel_initializer = \"he_normal\")\n    ])\n    \n    # Compiling the model\n    model3.compile(loss=\"mean_squared_logarithmic_error\",\n             optimizer = optimizer)\n    \n    # Fitting the model\n    history_3 = model3.fit(X[train], Y[train],epochs=200,verbose=0, \n                        validation_data = (X_valid, Y_valid),\n                        callbacks = [early_stopping_cb,checkpoint_cb_2, lr_scheduler])\n    \n    # Evaluating the model\n    y_pred = model3.predict(X[test])\n    loss = np.sqrt(mean_squared_log_error( Y[test], y_pred ))\n    cvscores.append(loss)\n    \nprint(\"Scores:\",cvscores)\nprint(\"Mean:\",np.mean(cvscores))\nprint(\"Standard Deviation:\",np.std(cvscores))","7a003c89":"# Creating a new checkpoint for a new model\ncheckpoint_3_cb = keras.callbacks.ModelCheckpoint(\"keras_model3_assign_2.h5\",\n                                               save_best_only = True) # making sure the model is saved at every epoch\n\n# Setting the model\n################\n################\nseed = 7\ncvscores = []\nnp.random.seed(seed)\n\n# define 5-fold cross validation test harness\nkfold = KFold(5, True, 1)\n\n# Initiating K-Fold Cross-Validation while fitting the model\nfor train, test in kfold.split(X, Y):\n  # create model\n    model_4 = keras.models.Sequential([\n    keras.layers.Flatten(input_shape = x_train.shape[1:]),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dense(100, activation = \"selu\", kernel_initializer = \"lecun_normal\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Activation(\"selu\"),\n    keras.layers.Dense(100, activation = \"selu\", kernel_initializer = \"lecun_normal\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Activation(\"selu\"),\n    keras.layers.Dense(100, activation = \"selu\", kernel_initializer = \"lecun_normal\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Activation(\"selu\"),\n    keras.layers.Dense(50, activation = \"selu\", kernel_initializer = \"lecun_normal\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Activation(\"selu\"),\n    MCDropout(rate =0.15),\n    keras.layers.Dense(2, activation = \"relu\", kernel_initializer = \"he_normal\")\n])\n\n    # Compiling the model\n    model_4.compile(loss=\"mean_squared_logarithmic_error\",\n             optimizer = optimizer)\n    \n    # Fit the model\n    history_4 = model_4.fit(X[train], Y[train],epochs=200, verbose=0,\n                        validation_data = (X_valid, Y_valid),\n                        callbacks = [early_stopping_cb, checkpoint_3_cb, lr_scheduler])\n    \n    # evaluate the model\n    y_pred = model_4.predict(X[test])\n    loss = np.sqrt(mean_squared_log_error( Y[test], y_pred ))\n    cvscores.append(loss)\n    \nprint(\"Scores:\",cvscores)\nprint(\"Mean:\",np.mean(cvscores))\nprint(\"Standard Deviation:\",np.std(cvscores))","b1d4b663":"from scipy.stats import reciprocal \nfrom sklearn.model_selection import RandomizedSearchCV\n\n\nsamples_split = range(25,50)\nmax_depth = range(90,155)\n\nparameters={'min_samples_split': samples_split,\n            'max_depth': max_depth}\nseed = 7\nrnd_search_cv = RandomizedSearchCV(model, \n                                   parameters, \n                                   n_iter = 100, \n                                   cv=3, \n                                   scoring = scorer, \n                                   random_state=0)\n\nrnd_search_cv.fit(x_train, y_train)","3ac7a852":"# Collecting the results\ncvres = rnd_search_cv.cv_results_\n\n# Creating a loop that goes through the values tested and their associated scores\nfor mean_score, params in zip(-cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(mean_score,params)","c7bc21f6":"## Best parameters from optimization\nrnd_search_cv.best_params_","3a30b6b0":"# Forming the final model\noptimized_dtc = rnd_search_cv.best_estimator_\n\n# Getting predictions\nopti_dtc_final_predictions = optimized_dtc.predict(x_valid)\nmodel2_final_predictions = model2.predict(x_valid)\n\ndtc_loss = np.sqrt(mean_squared_log_error( y_valid, opti_dtc_final_predictions ))\nmodel2_loss = np.sqrt(mean_squared_log_error( y_valid, model2_final_predictions ))\n    \nprint(\"DTC Score:\",dtc_loss)\nprint(\"DNN Score:\",model2_loss)","f8514eca":"# Getting predictions\nfinal_predictions = optimized_dtc.predict(test_pip)\n\n# Creating the final submission file\nsub = pd.DataFrame(final_predictions)\nsub[\"ConfirmedCases\"] = sub[0].astype(int)\nsub[\"Fatalities\"] = sub[1].astype(int)\ncols = [0,1]\nsub.drop(sub.columns[cols],axis=1,inplace=True)\nsub.round() \nsub['ForecastId'] = range(1, len(sub) + 1)\nsub = sub[['ForecastId', 'ConfirmedCases','Fatalities']]\nsub.to_csv(\"submission.csv\", index=False)","85e4ff22":"No duplicates. Let's make sure that test set has the dimensions that is suppose to have","65fd52dc":"Here is an excerpt of the the data description for the competition:\n\n* Values of -1 indicate that the feature was missing from the observation.\n* We are after 2 outputs from the model we are going to build: ConfirmedCases and Fatalities. These columns are labeld as inter later on\n\nOk, that's important information to get us started. Let's have a quick look at the first and last rows to confirm all of this.\n","af1077aa":"No NA values. Let's move on with Feature selection.","932cc403":"### Interval Data","d0f01e08":"## Exploratory Data Visualization","5b809aef":"Here is also a more consise version of the data types in the data set","2cc99ea9":"We have approximately 1% of observations missing from each column. Since I was the one who compiled information from various sources, I knew this already. One of the countries\/regions in the given sets is the cruise ship \"Diamond Princess\" which obviously doesn't have any demographic information as a region as the rest of the countries\/regions. There are also a couple other countries mainly from Africa for which the WHO did not have any available demographic information however, these observations do not constitute a large number of the observations. Hence, I decide to leave these observations untouched for now.","e01dc63b":"Next, we raise the interval variables to **polynomial degree=2** and create interactions between variables. Thanks to the get_feature_names method we can assign column names to these new variables.","8800fc0c":"It appears that Confirmed cases in Asia top the confirmed cases anywhere else but the Fatalities are more severe in Europe. As we saw above Italy and Spain play a major role to that.","2d746d18":"Before proceeding with encoding, we split the training set to training and validation sets using **Stratified Sampling** based on the population column which as we saw earlier is skewed. ","2fd761f8":"Training the RandomForest.","af21dcf3":"We confirm our observation from the descriptive statistics part above that interval variables' distribution vary substantially across the board. We can also see here that the data seem to be right skewed, meaning we have some high potive values. ","e2446aa0":"### VarianceThreshold\n\nBy default it removes features with zero variance. This will be really helpful here as we will see below that there are quite a few zero-variance variables. If we choose to remove features with less than 1% variance, we remove 346 variables as seen below.","bacb1f03":"# Model 3 - DNN using MC Dropout\n\nInstead of using Dropout, let's try and use Monte Carlo Dropout. MC Dropout attempts to mitigate the problem of representing model uncertainty without sacrificing either computational complexity or test accuracy so let's give it a try.","f13559bb":"# Model Fitting\n\n## Model 1 - Decision Tree Regressor\n\nStarting the model fitting part wih a **DecisionTreeRegressor**. DecisionTreeRegressor is one the most powerful algorithms there are, mainly because of its ability to fit both parametric and non-parametric data. \n\nWhile fitting the model, will use 10-fold cross-validation with 3 repeats. As a performance metric, I use **Root Mean Squared Log Error** as required from the competition.","dfc500f7":"## Output\n\nAll done, creating the output file.","217255b2":"## Data at first sight","2c262eaf":"Making sure that the dataset contains no NA values.","48106f93":"We are missing 2 variables but these are the columns that we are after, so we are good. Next, let's take a first look at the data types in the training set.","56059ac5":"# Fine Tuning\n\nThe better two models in terms of performance on the validation set are the **DecisionTreeClassifier** and the **DNN with Dropout layer** (model2). To decide which one to use, let's optimize the Decision Tree Classifier using **RandomizedSearchCV** and test them both again on the validation set. \n\nThrough trial and error, I found that **sample_split** above 50 leads to overfit so I limit this variable to 50. I also set **max_depth** to 90:155 again because through trial and error values between 90:155 lead to smaller generalization error.","f30baaa6":"The model performed well. Let's evaluate it on the validation set.","b353afc5":"Let's see if there are any duplicate observations in the training set.","80bc71a3":"Applying the same technique to the test set.","5addb54e":"## Feature normalization\n\nIn the last step prior to fitting a model, there are 2 things that remain to be done:\n1. Encode the **Date** variable\n2. Scale all numerical variables\n\nThe problem with the former is that the training set and the test set have a different number of observations and different dates in each set. Thus, if we try to apply the **OneHotEncoder** this leads to different size of columns for the two sets which wouldn't work for modelling since we want both datasets to have the same exact columns to be able to predict. A get around technique is applied to make the 2 column sets equal.","8c81bc2d":"The Optimized Decision Tree Classifier performs better, hence this will be the final model.","f742f7de":"## Model 2 - Deep Neural Network using Dropout\n\nA Neural network model can be a good option for the purposes of this competition. The algorithm is extremely useful in finding patterns that are too complex for being manually extracted and taught to recognize to the machine. So let\u2019s fit a simple DNN with a small dropout rate.\n\nFor activation function in the hidden layers, **selu** is being used in order to avoid the **vanishing\/exploding gradients** problem. For further explanation about the vanishing\/exploding gradients problem feel free to see to this article https:\/\/www.semanticscholar.org\/paper\/Understanding-the-exploding-gradient-problem-Pascanu-Mikolov\/c5145b1d15fea9340840cc8bb6f0e46e8934827f. \n\nUsing selu as activation functions also leads to self-regularization which is good. Since we are using selu activation, one of the conditions for selu to work is to use **LeCun initialization**. For further information about selu activation function feel free to read this article https:\/\/arxiv.org\/pdf\/1804.02763.pdf\n\nAs for the output layer, **Relu** is being used in order ensure we only get positive values. Lastly, I am adding a **Learning Scheduler**, namely **ReduceOnPlateu**, to help improve the learning rate of the algorithm when the validation loss does not improve after 5 rounds.","73e92210":"Before proceeding with the analysis, let's first make sure that there are no duplicate observations.","8f427b5c":"Let's start via looking at the distribution of the interval data first.","8e6a8b62":"Clearly, there are high correlations amongst some variables. Will let the algorithm further below handle this when we are dealing with feature selection. ","10c9e27d":"Looking at the scores from every iteration, it appears that as the trainng set is shuffled in each iteration, the performance of the model deteriorates.","25f8e388":"## Descriptive statistics","862ac70e":"Okay, now we are ready to encode the categorical variables (**Date**) and standardise the data. Doing both at the same time for all three sets (x_train, x_valid, test) using the handy tool called **pipeline** .","18605e7a":"Total cases in the US are most than any other country in the world. In fatalities however, Italy has the most followed by Spain.","4cd9f4d5":"Let's compare.","30a1c5d9":"Below the number of variables per role and level are displayed.","dbeb7902":" ## Feature Engineering","ddd2db35":"### Nominal\n\nLet's visualise the Confirmed cases and Fatalities per Country\/Region. \n\nThe code for this visualization comes from the Kee's kernel found in this link https:\/\/www.kaggle.com\/keedong\/covid19-exponential-model2-kee\n","12befe54":"# Model 4 - DNN using Batch Normalisation\n\nBatch Normalization makes the networks much less sensitive to the weight initialization. The drawback is that we are adding extra computation at each layer which makes the model slower to converge and predict. Also, I choose to retain the Monte Carlo Dropout layer since it appears to improve the performance.","04e4be07":"Adding a MCDropout layer improved the model's performance however the same problem with the first model persists. Let's fit a model that requires less tuning of hypermarameters and see how it performs.","6868456e":"There are quite a few interval data as denoted by the data type **int64** and **float64**. There are also some categorical as denoted by the dtype **object**. This implies that later on we shall create dummy variables as we will see below. But first, let's turn object variables to category to let python know that these are categorical variables. ","8c40dfb1":"Let's see which values were trialled during Randomized Search CV.","9a52a9bd":"## Feature selection\u00b6\n\nPersonally, I prefer to let the classifier algorithm chose which features to keep as i find it more robust. Here we use RandomForest to do the job. But there is one thing that we can do ourselves. That is removing features with no or a very low variance. Sklearn has a handy method to do that; VarianceThreshold","196abae2":"As for the the meta data, the structure is will be as follows:\n\n**role**: input, ID, ConfirmedCases, Fatalities\n\n**level**: nominal, interval, ordinal keep: True or False dtype: int, float, str\n\n**keep**: True or False\n\n**dtype**: int, float, str","64635615":"Great. Everything seem to be working fine. Let's proceed with the analysis","b700d51f":"Quick observations:\n\n* 188 distinct _**Country_Code**_ values\n* 173 distinct _**Country_Region**_ values\n* 5 distinct _**Continent**_ values\n* America seems to be the continent with the most observations in the dataset. More precisely, America seems to capture appx 33% of the total observations.\n\nThe fact that we have fewer **Country_Region** values steams from that we have some Provinces included in country_code and have their own distinct country code.","a0d07767":"The values of the categorical variables do not represent any order or magnitude. For instance, category 2 is not twice the value of category 1. Therefore we can create dummy variables to deal with that. We drop the first dummy variable as this information can be derived from the other dummy variables generated for the categories of the original variable.","0c1dff9b":"### Nominal Data\n\nLet's look at the nominal data next. Let's start with the cardinality.\u00b6\n","aa2de56a":"Let's also look at the test set. This also helps verify that everything is okay with the test set as well.","1afd848a":"### Interval","7ea00a19":"Doing the same thing for test set.","e42f0040":"First, it seems that the distributions of the variables in the dataset differ quite significantly. More precisely, the mean and the  standard deviation differs by large across variables. Also min and max are quite volatile as well. This suggests the need to scale the variables later on. \n\nIt is also clear that we have missing values (denoted by -1) in quite a few of the columns in the dataset. \n\nLet's see the quality of the data, how many values are missing from each variable.","c8a5fb07":"## Data Management\n\nTo facilitate the data management, we'll store meta-information about the variables in a DataFrame. The method for the preparation of meta-data is mainly inspired from Bert Careman's kernel from another competition https:\/\/www.kaggle.com\/bertcarremans\/data-preparation-exploration. It's great how we can learn new things via participating in competitions such as Kaggle. \n\nSo all kudos for the technique of data management seen here go to Bert.","baefd19c":"### Creating dummy variables\u00b6"}}