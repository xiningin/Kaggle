{"cell_type":{"644938dc":"code","112578b9":"code","c8d17aad":"code","49b2260b":"code","088df740":"code","9b281c8d":"code","82bd3f5c":"code","8bad3219":"code","2b52559c":"code","da31a5eb":"code","83848db0":"code","e558f4e4":"markdown","00a13a43":"markdown","97325174":"markdown","edffd666":"markdown","55c77bb9":"markdown","76c2384e":"markdown","3519925a":"markdown","7255ce3b":"markdown","e15dd349":"markdown"},"source":{"644938dc":"#1.kutuphaneler\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\nfrom sklearn import linear_model, tree, ensemble\nfrom pandas.plotting import scatter_matrix\nfrom sklearn import model_selection\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","112578b9":"\n#2.veri onisleme\n#2.1.veri yukleme\nveriler = pd.read_csv('..\/input\/wine-customer-segmentation\/Wine.csv')\n\n\nprint(veriler)\n#box graph\n#plt k\u0131saltmas\u0131 matplotlib k\u00fct\u00fcphanesinin k\u0131saltmas\u0131 olarak kullan\u0131l\u0131r.\n#subplot: grafiklerin d\u00fczlemini ve ka\u00e7\u0131nc\u0131 grafik oldu\u011funu g\u00f6sterir.\nveriler.plot(kind=\"box\",subplots=True, sharex=False, sharey=False)\nplt.show()\n\n#histogram\nveriler.hist()\nplt.show()\n\nscatter_matrix(veriler)\nplt.show()","c8d17aad":"#Veriyi e\u011fitim ve test olarak ay\u0131raca\u011f\u0131m.\n# train\/test\n#e\u011fitim ve test olarak ay\u0131rmak i\u00e7in genel yakla\u015f\u0131m: %70-80 e\u011fitim, %30-20 test \n\nX=veriler.values[:,0:4]\nY=veriler.values[:,4]\n\nX_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=0.25, random_state=7)\nprint(\"veri seti:\")\nprint(veriler)","49b2260b":"#Lojistik regresyon, ikili sonu\u00e7 veren binary de\u011fi\u015fkenlerin modellenmesinde kullan\u0131lmaktad\u0131r\n\n\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\n#Yukar\u0131da g\u00f6rd\u00fc\u011f\u00fcn\u00fcz kod FutureWarning hatas\u0131 i\u00e7in kullan\u0131ld\u0131.\n\n\nfrom sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nlr.fit(X_train,Y_train)\n#Accuracy(Do\u011fruluk): Sistemde do\u011fru olarak yap\u0131lan tahminlerin t\u00fcm tahminlere oran\u0131d\u0131r. \nprint('lr accuracy :', lr.score(X_test,Y_test))\n\n# confusion matrix\n#Modelin ba\u015far\u0131 metrikleri: Confusion matrix\n#Hata Matrisi ( Confusion Matrix) makine \u00f6\u011frenmesinde kulland\u0131\u011f\u0131m\u0131z s\u0131n\u0131fland\u0131rma modelinin performans\u0131n\u0131 hesaplarken kullan\u0131l\u0131yor.\n#Burada True Positive-TP , True Negative-TN, False Positive \u2013 FP ve False Negative-FN isminde kavramlar var.\n#E\u011fer sonucu do\u011fru olan bir \u015fey do\u011fru tahmin edilirse True Positive olarak adland\u0131r\u0131l\u0131ken sonucu yanl\u0131\u015f olan bir \u015fey do\u011fru olarak tahmin edilirse buna True Negative deniliyor. \n#Ayn\u0131 \u015fekilde sonucu yanl\u0131\u015f olan bir \u015fey do\u011fru olarak tahmin edilirse False Positive, sonucu yanl\u0131\u015f olan bir \u015fey yanl\u0131\u015f olarak tahmin edilirse False negative deniliyor.\ny_pred = lr.predict(X_test)\ny_true = Y_test\nfrom sklearn.metrics import confusion_matrix\n\ncm_lr = confusion_matrix(y_true,y_pred)","088df740":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=3)\n#n_neighbors=3 (En yak\u0131n 3 kom\u015fu i\u00e7erisindeki yo\u011funlu\u011fa g\u00f6re karar verecek) \nknn.fit(X_train,Y_train)\nprint('knn accuracy :',knn.score(X_test,Y_test))\n\n# confisioun matrix\n#Modelin ba\u015far\u0131 metrikleri: Confusion matrix\ny_pred = knn.predict(X_test)\ny_true = Y_test\n\nfrom sklearn.metrics import confusion_matrix\ncm_knn = confusion_matrix(y_true,y_pred)","9b281c8d":"score_list=[]\nfor each in range(1,15):\n    knn2=KNeighborsClassifier(n_neighbors=each)\n    knn2.fit(X_train,Y_train)\n    score_list.append(knn2.score(X_test,Y_test))\n#Accuracy(Do\u011fruluk): Sistemde do\u011fru olarak yap\u0131lan tahminlerin t\u00fcm tahminlere oran\u0131d\u0131r. \nprint('bk accuracy :', knn2.score(X_test,Y_test))\nplt.plot(range(1,15),score_list)\nplt.xlabel('k values')\nplt.ylabel('accuracy')\nplt.show()\n\n# confisioun matrix\n#Modelin ba\u015far\u0131 metrikleri: Confusion matrix\ny_pred = knn2.predict(X_test)\ny_true = Y_test\n\nfrom sklearn.metrics import confusion_matrix\ncm_knn2 = confusion_matrix(y_true,y_pred)\n","82bd3f5c":"from sklearn.svm import SVC\nsvm=SVC(random_state=1)\nsvm.fit(X_train,Y_train)\n#Accuracy(Do\u011fruluk): Sistemde do\u011fru olarak yap\u0131lan tahminlerin t\u00fcm tahminlere oran\u0131d\u0131r. \nprint('svm accuracy :', svm.score(X_test,Y_test))\n\n# confisuon matrix\ny_pred = svm.predict(X_test)\ny_true = Y_test\nfrom sklearn.metrics import confusion_matrix\n\ncm_svm = confusion_matrix(y_true,y_pred)","8bad3219":"from sklearn.naive_bayes import GaussianNB\nnb=GaussianNB()\nnb.fit(X_train,Y_train)\n#Accuracy(Do\u011fruluk): Sistemde do\u011fru olarak yap\u0131lan tahminlerin t\u00fcm tahminlere oran\u0131d\u0131r. \nprint('nb accuracy : ', nb.score(X_test,Y_test))\n\n# confisuon matrix\ny_pred = nb.predict(X_test)\ny_true = Y_test\nfrom sklearn.metrics import confusion_matrix\n\ncm_nb = confusion_matrix(y_true,y_pred)","2b52559c":"from sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier()\ndt.fit(X_train,Y_train)\n#Accuracy(Do\u011fruluk): Sistemde do\u011fru olarak yap\u0131lan tahminlerin t\u00fcm tahminlere oran\u0131d\u0131r. \nprint('dt.accuracy : ', nb.score(X_test,Y_test))\n\n# confisuon matrix\ny_pred = dt.predict(X_test)\ny_true = Y_test\nfrom sklearn.metrics import confusion_matrix\n\ncm_dt = confusion_matrix(y_true,y_pred)","da31a5eb":"from sklearn import model_selection\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n\n\nwine_dataset = pd.read_csv(\"..\/input\/wine-customer-segmentation\/Wine.csv\")\n\nx=wine_dataset.values[:,0:4]\ny=wine_dataset.values[:,4]\n\nX_train, X_test, Y_train, Y_test = model_selection.train_test_split(x, y, test_size=0.25, random_state=7)\n\n#uygulanacak modelleri models i\u00e7ine koydum.\nmodels=[\n    (\"lR\", LogisticRegression()),\n    (\"KNN\",KNeighborsClassifier()),\n    (\"KN\",KNeighborsClassifier()),\n    (\"SVM\",SVC()),\n    (\"Naive Bayes\",GaussianNB()),\n    (\"DT\",DecisionTreeClassifier()),\n]\nresults=[]\nnames=[]","83848db0":"def warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\n#Yukar\u0131da g\u00f6rd\u00fc\u011f\u00fcn\u00fcz kod FutureWarning hatas\u0131 i\u00e7in kullan\u0131ld\u0131.\n\n\n\n\n#K herhangi bir say\u0131 olabilir, ancak genellikle K = 10 \u00f6nerilir\n#Dengesiz bir veri k\u00fcmesiyle u\u011fra\u015f\u0131yorsan\u0131z, tahmin edilen \u00f6zelliklerin s\u0131n\u0131flar\u0131n\u0131 dengeleyebilir.\n#Hiperparametre Ayar\u0131: \u00c7apraz Do\u011frulama, algoritman\u0131n verimlili\u011fini art\u0131rmak i\u00e7in hiperparametrelerin optimum de\u011ferini bulmaya yard\u0131mc\u0131 olur\n\nfor name,model in models:\n    kfold=model_selection.KFold(n_splits=10,random_state=7)\n    cv_results=model_selection.cross_val_score(model,X_train,Y_train,cv=kfold,scoring=\"accuracy\")\n    results.append(cv_results)\n    names.append(name)\n    print(\"%s: %f (%f)\" %(name, cv_results.mean(), cv_results.std()))\n   \n\n","e558f4e4":"Find best k value","00a13a43":"Naive Bayes","97325174":"NEDEN CROSS-VALIDATION KULLANMALIYIZ?\n\n\u00c7apraz do\u011frulama (cross-validation) ise parametre ayarlama s\u0131ras\u0131nda e\u011fitim setinin her seferinde farkl\u0131 bir k\u0131sm\u0131 ile do\u011frulama yapma i\u015flemine denir. \u00d6rnek ile a\u00e7\u0131klamak gerekirse, veri setinin se\u00e7ilen orandaki k\u0131sm\u0131 test seti olarak ayr\u0131ld\u0131ktan sonra kalan k\u0131sm\u0131n ilk %25\u2019lik k\u0131sm\u0131n\u0131 ilk model e\u011fitiminde, ikinci %25\u2019lik k\u0131sm\u0131n\u0131 ikinci model e\u011fitiminde\u2026 \u015feklinde s\u0131ras\u0131 ile uygulanarak t\u00fcm k\u0131s\u0131mlar hem do\u011frulama hem e\u011fitim s\u0131ras\u0131nda kullan\u0131l\u0131r. Bu sayede do\u011frulama setinin do\u011fru \u015fekilde se\u00e7ilip se\u00e7ilmedi\u011fi kayg\u0131s\u0131 ortadan kalkar, parametre se\u00e7imi ise her bir \u00e7apraz do\u011frulama (cross-validation) seti ve e\u011fitim (training) seti ile \u00f6l\u00e7\u00fclen metrik skorlar\u0131n\u0131n (hata, do\u011fruluk, f1 vs.) ortalamas\u0131 al\u0131narak yap\u0131l\u0131r.","edffd666":"SVM","55c77bb9":"**Makine \u00d6\u011frenmesinde \u00c7apraz Do\u011frulama ve Test**\n\nMakine \u00f6\u011frenmesinde modelleri e\u011fitirken beklenti kullan\u0131lan veriyle en iyi genellemeyi yapmas\u0131d\u0131r. E\u011fer e\u011fitilen model yine e\u011fitim s\u0131ras\u0131nda kullan\u0131lan veri ile test edilirse model kullan\u0131lan veriye overfitting (a\u015f\u0131r\u0131 uyum) mi yoksa iyi bir genelleme mi anlayamay\u0131z.\n\u00c7apraz do\u011frulama (cross-validation) parametre ayarlama s\u0131ras\u0131nda e\u011fitim setinin her seferinde farkl\u0131 bir k\u0131sm\u0131 ile do\u011frulama yapma i\u015flemidir. \u00d6rnek ile a\u00e7\u0131klamak gerekirse, veri setinin se\u00e7ilen orandaki k\u0131sm\u0131 test seti olarak ayr\u0131ld\u0131ktan sonra kalan k\u0131sm\u0131n ilk %25\u2019lik k\u0131sm\u0131n\u0131 ilk model e\u011fitiminde, ikinci %25\u2019lik k\u0131sm\u0131n\u0131 ikinci model e\u011fitiminde\u2026 \u015feklinde s\u0131ras\u0131 ile uygulanarak t\u00fcm k\u0131s\u0131mlar hem do\u011frulama hem e\u011fitim s\u0131ras\u0131nda kullan\u0131l\u0131r. Bununla birlikte do\u011frulama setinin do\u011fru \u015fekilde se\u00e7ilip se\u00e7ilmedi\u011fi kayg\u0131s\u0131 ortadan kalkar, parametre se\u00e7imi ise her bir \u00e7apraz do\u011frulama (cross-validation) seti ve e\u011fitim (training) seti ile \u00f6l\u00e7\u00fclen metrik skorlar\u0131n\u0131n (hata, do\u011fruluk, f1 vs.) ortalamas\u0131 al\u0131narak yap\u0131l\u0131r.","76c2384e":"Simdi az once uyguladigim modelleri cross-validation ile uygulayip gorelim.","3519925a":"K-Nearest Neighbors","7255ce3b":"LOGISTIC REGRESSION","e15dd349":"Decision Tree"}}