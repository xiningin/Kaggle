{"cell_type":{"fc55c723":"code","2c7efa3e":"code","75dcd739":"code","84fb837c":"code","0c6b5853":"code","99c7866e":"code","bbad87c4":"code","fd3b7cf8":"code","59f17f33":"code","b3e42b2f":"code","9d5f2f64":"code","ac5de2eb":"code","df2decb7":"code","635ccfd4":"markdown","aa74e2fb":"markdown"},"source":{"fc55c723":"import numpy as np\nfrom PIL import Image\nfrom collections import Counter\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport wordcloud","2c7efa3e":"import nltk # for removing stopwords from word set\nfrom nltk.corpus import stopwords\n\n# if you have problems with using stopwords.words(\"english\")\n# you should manually download stopword dictionart by running the code below\n# nltk.download(\"stopwords\")","75dcd739":"ESW = stopwords.words(\"english\")\nESW = ESW + [\"\u2014\", \"would\", \"didn\u2019t\", \"don\u2019t\", \"it.\"]","84fb837c":"def read_docu(document):\n    \n    words = []\n    \n    with open(document, \"r\", encoding = \"utf-8\") as input_file:\n        for line in input_file:\n            line = line.lower()\n            line = line.strip().split()\n            words += line\n            \n        return words","0c6b5853":"def stopword_filter(words):\n    \n    filtered_words = []\n    \n    for word in words:\n        if not word in ESW:\n            filtered_words.append(word)\n        \n    return filtered_words","99c7866e":"def word_counter(words):\n    \n    word_counts = Counter(words)\n        \n    return word_counts","bbad87c4":"def bgimg_get(imgfile):\n    \n    bgimg = np.array(Image.open(imgfile))\n    \n    return bgimg","fd3b7cf8":"def set_wordcloud(word_counts, bgimg):\n    \n    cloud = wordcloud.WordCloud(background_color=\"white\", mask = bgimg, collocations = False)\n    word_cloud = cloud.generate_from_frequencies(word_counts)\n    \n    return word_cloud","59f17f33":"def draw_wordcloud(document, imgfile):\n    \n    words = read_docu(document)\n    fd_words = stopword_filter(words)\n    word_counts = word_counter(fd_words)\n    bgimg = bgimg_get(imgfile)\n    word_cloud = set_wordcloud(word_counts, bgimg)\n    \n    plt.figure(figsize=(10, 10))\n    plt.imshow(word_cloud)\n    plt.axis(\"off\")\n    plt.show()","b3e42b2f":"jobs_speech = \"..\/input\/jobscommencement\/jobscommencement.txt\"\njobs_img = \"..\/input\/stevejobs-pic\/stevejobs.jpg\"\ndraw_wordcloud(jobs_speech, jobs_img)","9d5f2f64":"import squarify","ac5de2eb":"def draw_treemap(document):\n    \n    # read document and count words\n    words = read_docu(document)\n    fd_words = stopword_filter(words)\n    word_counts = word_counter(fd_words)\n    \n    # settings for treemap\n    common_n = [n for (w, n) in word_counts.most_common(30)] # top30 words' counts\n    common_w = [w for (w, n) in word_counts.most_common(30)] # top30 words\n    norm = matplotlib.colors.Normalize(vmin = min(common_n), vmax = max(common_n))\n    # you can change color(set with Blues now) here\n    colors = [matplotlib.cm.Blues(norm(value)) for value in common_n] \n    \n    # draw a treemap\n    plt.figure(figsize=(10, 10))\n    squarify.plot(label = common_w, sizes = common_n, color = colors, alpha = 0.5)\n    plt.title(\"Word Treemap\")\n    plt.axis(\"off\")\n    plt.show()","df2decb7":"draw_treemap(\"..\/input\/jobscommencement\/jobscommencement.txt\")","635ccfd4":"We need to slightly update the stopwords to eliminate extra meaningless words that are not filtered by stopwords dictionary we downloaded.","aa74e2fb":"**Data**: I used the full script of Jobs' commencement address at Stanford University in 2005. For image of the wordcloud, I used his image that you can easily find by googling."}}