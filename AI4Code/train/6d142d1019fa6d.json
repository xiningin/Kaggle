{"cell_type":{"f88af411":"code","631e0730":"code","dae3b3f2":"code","28a9d48a":"code","ec615622":"code","ef7d6a30":"code","b2ce9853":"code","ad0f9236":"code","7f37e649":"code","794a38cc":"code","be08bf2d":"code","098bfa62":"code","2d66e18b":"code","66ad4440":"code","210c35eb":"code","f5cd431a":"code","ca241f7e":"code","bf003058":"code","1d95725c":"code","959e5f02":"code","d9e0f602":"code","14257800":"code","ac15c93d":"code","9ac92aa2":"code","b35d6c5b":"code","e3f6833d":"code","3570bf50":"code","8701c497":"code","4fdbde6b":"code","702856e7":"code","c7392375":"code","d0af9600":"code","7f83b044":"code","395ec095":"code","09055601":"code","9a1c89b1":"code","09464002":"code","5e8e65d4":"code","545e27dd":"code","51c73f47":"code","862873be":"code","a7eae57b":"code","ed1f4683":"code","5731173d":"code","70307ac9":"code","0078dabd":"code","09a21493":"code","adaf156c":"markdown","463460e8":"markdown","8adf34a4":"markdown","8d21c92c":"markdown","41371373":"markdown","80d63fc2":"markdown"},"source":{"f88af411":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","631e0730":"#importing the data sets\n\ntrain_data = pd.read_csv(r'\/kaggle\/input\/into-the-future\/train.csv')\ntest_data = pd.read_csv(r'\/kaggle\/input\/into-the-future\/test.csv')","dae3b3f2":"id1 = test_data['id']\ntime1= test_data['time']\ndata = {'id': id1, 'time':time1 , 'feature_1':test_data['feature_1']}\nfinal_df = pd.DataFrame(data)","28a9d48a":"#exploring the data sets\ntrain_data.head()","ec615622":"train_data.info()","ef7d6a30":"train_data.describe()","b2ce9853":"#data preprocessing\n#dropping the id column\ntrain_data.drop(['id'],axis=1,inplace=True)","ad0f9236":"train_data.head()","7f37e649":"#now we can just remove '2019-03-19 ' from the time attribute because it appears in every entry\nfor i in range (0,564):\n    train_data['time'][i]=train_data['time'][i].replace('2019-03-19 ','')","794a38cc":"train_data.head()","be08bf2d":"#now visualizing the general trends in data \nimport matplotlib.pyplot as plt\nx = train_data['time']   #time attribute\ny = train_data['feature_1']   #feature_1\nz = train_data['feature_2']   #feature_2","098bfa62":"#plotting time vs feature_1\nplt.plot(x,y)\nplt.show()","2d66e18b":"#plotting time vs feature_2\nplt.plot(x,z)\nplt.show()","66ad4440":"#plot to visualize the dependencies of feature_1 and feature_2\nplt.plot(y,z)\nplt.show()","210c35eb":"#we can see that the first value in feature_2 is erroneous\n#hence, we can drop the first row\ntrain_data = train_data[train_data['time']!='00:00:00']","f5cd431a":"train_data.head()","ca241f7e":"#let's visualize the plot for feature_2 again\nx = train_data['time']\nz = train_data['feature_2']\nplt.plot(x,z)\nplt.show()","bf003058":"#evaluating the correlation between the attributes \ncorr = train_data.corr()\ncorr","1d95725c":"# hence, we can see that both the features show high negative correlation\n# using automatic time series decompostion","959e5f02":"from statsmodels.tsa.seasonal import seasonal_decompose\nresult = seasonal_decompose(train_data['feature_2'], model='multiplicative',period = 1)\nresult.plot()\nplt.show()","d9e0f602":"result1 = seasonal_decompose(train_data['feature_2'], model='addtive',period = 1)\nresult1.plot()\nplt.show()","14257800":"# from above we can see, that the data shows no seasonal residual features and the trend is the same as \n# the observed data\n# the curve goes up\n# hence we can convert time to seconds","ac15c93d":"df = train_data.copy()","9ac92aa2":"for i in range (1,564):\n    df['time'][i]=df['time'][i].split(':')","b35d6c5b":"df['time'][0:10]","e3f6833d":"for i in range (1,564):\n    df['time'][i]=int(df['time'][i][2]) + int(df['time'][i][1])*60 + int(df['time'][i][0])*3600","3570bf50":"df['time'][0:10]","8701c497":"from sklearn.model_selection import train_test_split\nX = df[['time','feature_1']]\nY = df['feature_2']\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=42)","4fdbde6b":"# training a linear model\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X_train, y_train)\nyhat = reg.predict(X_test)\nyhat","702856e7":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nimport math\nscore = mean_squared_error(y_test, yhat)\nprint(math.sqrt(score))\nprint(r2_score(y_test, yhat))","c7392375":"from sklearn.preprocessing import PolynomialFeatures\npoly_features = PolynomialFeatures(degree=2)\nX_train_poly = poly_features.fit_transform(X_train)\npoly_model = LinearRegression()\npoly_model.fit(X_train_poly, y_train)\n# predicting on training data-set\ny_train_predicted = poly_model.predict(X_train_poly)\n# predicting on test data-set\ny_test_predict = poly_model.predict(poly_features.fit_transform(X_test))\ny_test_predict","d0af9600":"# evaluating the model on training dataset\nrmse_train = np.sqrt(mean_squared_error(y_test, y_test_predict))","7f83b044":"rmse_train","395ec095":"from sklearn.tree import DecisionTreeRegressor  \n  \n# create a regressor object \nregressor = DecisionTreeRegressor(random_state = 0)  \n  \n# fit the regressor with X and Y data \nregressor.fit(X_train, y_train)\ny_pred = regressor.predict(X_test)\ny_pred","09055601":"score = mean_squared_error(y_test, y_pred)\nprint(math.sqrt(score))\nprint(r2_score(y_test, y_pred))","9a1c89b1":"test_data.info()","09464002":"for i in range(0,374):\n    test_data['time'][i]=test_data['time'][i].replace('2019-03-19 ','')\n","5e8e65d4":"test_data['time'][374]=test_data['time'][374].replace('2019-03-19 ','')","545e27dd":"for i in range (0,375):\n    test_data['time'][i]=test_data['time'][i].split(':')","51c73f47":"for i in range (0,375):\n    test_data['time'][i]=int(test_data['time'][i][2]) + int(test_data['time'][i][1])*60 + int(test_data['time'][i][0])*3600","862873be":"test_data.drop(['id'],axis=1,inplace=True)","a7eae57b":"test_data.head()","ed1f4683":"final_y = poly_model.predict(poly_features.fit_transform(test_data))\nfinal_y","5731173d":"final_df['feature_2']=final_y\nfinal_df.drop(['time','feature_1'],axis=1,inplace=True)","70307ac9":"final_df.head()","0078dabd":"mycsvfile = final_df","09a21493":"mycsvfile.to_csv(\"\/kaggle\/working\/mycsvfile.csv\",index=False)","adaf156c":"**WORKING WITH TEST DATA**","463460e8":"**TRAINING A DECISION TREE REGRESSOR**","8adf34a4":"**TRANSFORMING THE TEST DATA IN THE SAME WAY AS THE TRAINING DATA**","8d21c92c":"**SPLITTING THE DATA**","41371373":"**TRAINING A LINEAR MODEL**","80d63fc2":"**CONVERTING TIME TO SECONDS**"}}