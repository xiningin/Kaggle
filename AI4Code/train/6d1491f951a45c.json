{"cell_type":{"5ded35dc":"code","4e25317f":"code","0a966ccd":"code","4b35cb06":"code","db5fce23":"code","45ec2381":"code","aa6e9bd8":"code","604d09af":"code","02ccbe44":"code","19ef6264":"code","42b95887":"code","31362801":"code","41a5f6ff":"code","dfcd1e59":"code","0c0ad40e":"code","8e0372cd":"code","3759a76d":"code","4815a04f":"code","af277537":"code","47d2f3cc":"code","c3935805":"code","7bf798fd":"code","2427421d":"code","d7160129":"code","cea36bbc":"code","d7bc8ea0":"code","300814c2":"code","5639d3a0":"code","fe540b43":"code","005be294":"code","7011a395":"code","9f193297":"code","2d717566":"code","4e5bcce2":"code","94f8a277":"code","a5642a9b":"code","89e5c3c7":"code","a9564ecb":"code","a76e19cd":"code","f590a4d2":"code","9286c37d":"markdown","ed5945b1":"markdown","9891e8d5":"markdown","fe2e5039":"markdown","bb84b4af":"markdown","e6cc963e":"markdown","f8826f12":"markdown","ca41d7ae":"markdown","b02969e7":"markdown","2377a978":"markdown","ca465938":"markdown","869ee653":"markdown"},"source":{"5ded35dc":"import torch\nimport torchvision\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.transforms import ToTensor\nfrom torchvision.utils import make_grid\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data import random_split\n%matplotlib inline","4e25317f":"project_name='PredictingImagesOfCifar10DtDtWithFeedfrwrdNN'","0a966ccd":"# Downloading dataset\ndataset = CIFAR10(root='data\/', download=True, transform=ToTensor())\ntest_dataset = CIFAR10(root='data\/', train=False, transform=ToTensor())","4b35cb06":"dataset","db5fce23":"test_dataset","45ec2381":"dataset.classes","aa6e9bd8":"val_size = 10000\ntrain_size = len(dataset) - val_size\ntrain_ds, val_ds = random_split(dataset, [train_size, val_size])\nlen(train_ds), len(val_ds)","604d09af":"batch_size=128","02ccbe44":"train_loader = DataLoader(\n                         train_ds, \n                         batch_size, \n                         shuffle=True, \n                         num_workers=4, \n                         pin_memory=True\n                         )\nval_loader = DataLoader(\n                         val_ds, \n                         batch_size*2, \n                         num_workers=4, \n                         pin_memory=True\n                       )\ntest_loader = DataLoader(\n                         test_dataset, \n                         batch_size*2, \n                         num_workers=4, \n                         pin_memory=True\n                        )","19ef6264":"for images, _ in train_loader:\n    print('images.shape:', images.shape)\n    plt.figure(figsize=(16,8))\n    plt.axis('off')\n    plt.imshow(make_grid(images, nrow=16).permute((1, 2, 0)))\n    break","42b95887":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))","31362801":"class CIFAR10Model(nn.Module):\n    \"\"\"Feedfoward neural network with 1 hidden layer\"\"\"\n    def __init__(self, in_size, out_size):\n        super().__init__()\n        # hidden layer\n        self.linear1 = nn.Linear(in_size, 64)\n        # hidden layer 2\n        self.linear2 = nn.Linear(64, 128)\n        # hidden layer 3\n        self.linear3 = nn.Linear(128,256)\n        # hidden Layer 4\n        self.linear4 = nn.Linear(256, 512)\n        # output layer\n        self.linear5 = nn.Linear(512, out_size)\n        \n    def forward(self, xb):\n        # Flatten the image tensors\n        out = xb.view(xb.size(0), -1)\n\n        # Get intermediate outputs using hidden layer 1\n        out = self.linear1(out)\n        # Apply activation function\n        out = F.relu(out)\n\n        # Get intermediate outputs using hidden layer 2\n        out = self.linear2(out)\n        # Apply activation function\n        out = F.relu(out)\n\n        # Get intermediate outputs using hidde layer 3\n        out = self.linear3(out)\n        # Apply activation function\n        out = F.relu(out)\n\n        # Get intermediate outputs using hidde layer 4\n        out = self.linear4(out)\n        # Apply activation function\n        out = F.relu(out)\n\n        # Get predictions using output layer\n        out = self.linear5(out)\n        return out\n    \n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss, 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))","41a5f6ff":"torch.cuda.is_available()","dfcd1e59":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')","0c0ad40e":"device = get_default_device()\ndevice","8e0372cd":"def to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)","3759a76d":"class DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n\n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","4815a04f":"train_loader = DeviceDataLoader(train_loader, device)\nval_loader = DeviceDataLoader(val_loader, device)\ntest_loader = DeviceDataLoader(test_loader, device)","af277537":"def evaluate(model, val_loader):\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        for batch in train_loader:\n            loss = model.training_step(batch)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","47d2f3cc":"input_size = 3*32*32\nnum_classes = 10","c3935805":"model = CIFAR10Model(input_size, out_size=num_classes)\nto_device(model, device)","7bf798fd":"history = [evaluate(model, val_loader)]\nhistory","2427421d":"history += fit(50, 0.01, model, train_loader, val_loader)","d7160129":"history += fit(10, 0.001, model, train_loader, val_loader)","cea36bbc":"#history += fit(20, 0.0001, model, train_loader, val_loader)","d7bc8ea0":"# Lets define a function for plotting graphs\ndef plot_accuracies(history):\n    accuracies = [r['val_acc'] for r in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs')\n\ndef plot_losses(history):\n    losses = [x['val_loss'] for x in history]\n    plt.plot(losses, '-x', color='red')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.title('Loss vs No. of Epoch')","300814c2":"plot_accuracies(history)","5639d3a0":"plot_losses(history)","fe540b43":"# Evaluate on test dataset\nresult = evaluate(model, test_loader)\nresult","005be294":"def predict_image(img, model):\n    xb = to_device(img.unsqueeze(0),device)\n    yb = model(xb)\n    _, preds  = torch.max(yb, dim=1)\n    return preds[0].item()","7011a395":"img, label = test_dataset[10000-4143]\nplt.imshow(img[0])\nprint('Label:', test_dataset.classes[label], ', Predicted:', dataset.classes[predict_image(img, model)])","9f193297":"for i in range(10):\n    img, label = test_dataset[i]\n    print('Label:', test_dataset.classes[label], ', Predicted:', dataset.classes[predict_image(img, model)])","2d717566":"evaluate(model, test_loader)","4e5bcce2":"Stop here","94f8a277":"# E.g. \"3 layers (16,32,10)\" (16, 32 and 10 represent output sizes of each layer)\narch = \"5 layers (64, 128, 256, 512, 10)\"\n# the list of learning rates used while training.\nlrs = [0.01, 0.001]\n# the list of no. of epochs used while training.\nepochs = [50, 10]\n# the final test accuracy & test loss?\ntest_acc = 0.4920898377895355\ntest_loss = 1.4203943014144897","a5642a9b":"# let's save the trained model weights to disk, so we can use this model later.\ntorch.save(model.state_dict(), 'Mcifar10-feedforwardNN.pth')","89e5c3c7":"!pip install jovian --upgrade --quiet\nimport jovian\n# Clear previously recorded hyperparams & metrics\n#jovian.reset()","a9564ecb":"jovian.log_hyperparams(arch=arch, \n                       lrs=lrs, \n                       epochs=epochs)","a76e19cd":"jovian.log_metrics(test_loss=test_loss, test_acc=test_acc)","f590a4d2":"jovian.commit(project=project_name, outputs=['Mcifar10-feedforwardNN.pth'], environment=None, message='4th commit achieved 49% accu', is_cli=True)","9286c37d":"## Lets record our parameters and results to jovian.ml","ed5945b1":"## **Predictions**","9891e8d5":"## **Using A GPU**","fe2e5039":"# **THE END**","bb84b4af":"## Preparing the Data","e6cc963e":"## Lets View a set of Data","f8826f12":"## **Training the Model**","ca41d7ae":"Lets define a function to move all our data and the model from athe cpu to GPU","b02969e7":"Finally, we can commit the notebook to Jovian, attaching the hypeparameters, metrics and the trained model weights.","2377a978":"## **Model**","ca465938":"# **Image Classification Using Feed Forward Neural Network in PyTorch with CIFAR-10 Data Set**","869ee653":"## Data Loaders"}}