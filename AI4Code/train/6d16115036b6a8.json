{"cell_type":{"91c137fa":"code","82939f4b":"code","c9061700":"code","6ae713c4":"code","63e31ad9":"code","1e580a10":"code","5eaffb99":"code","b48cbb3e":"code","65cdd650":"code","39874fee":"code","e19ec81c":"code","7e9009c5":"code","67a9ac87":"code","92eef010":"code","05b3bfa1":"code","8cc60e6a":"code","7bc70323":"code","c32b24f5":"code","ce0a2ee7":"code","8b09e5ef":"code","d0050c88":"code","701163b0":"code","58bbb136":"code","ad6510b7":"markdown","86dd64d3":"markdown","253c835b":"markdown","a1e0111e":"markdown","873de112":"markdown","cfcd7fa2":"markdown","99b18737":"markdown","235faf4a":"markdown","1fad9518":"markdown"},"source":{"91c137fa":"import os\nfrom typing import List\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom gensim.models import Word2Vec\n\nfrom tqdm import tqdm","82939f4b":"train = pd.read_csv('..\/input\/data-science-winter-osaka2\/train.csv')\ntest = pd.read_csv('..\/input\/data-science-winter-osaka2\/test.csv')","c9061700":"def eval_data(row):\n    if row == row:\n        return eval(row)\n    else:\n        return np.nan\n\nfor column in ['popular_tags', 'categories', 'minimum_requirements', 'recommended_requirements']:\n    train[column] = train[column].apply(eval_data)\n    test[column] = test[column].apply(eval_data)","6ae713c4":"train['is_train'] = True\ntest['user_reviews'] = np.nan\ntest['is_train'] = False\n\ndf = pd.concat([train, test], axis=0)\ndf = df.reset_index(drop=True)","63e31ad9":"def category_numeric_preprocess(df: pd.DataFrame) -> pd.DataFrame:\n    # \u30bf\u30b0\u306e\u6570\u306b\u3088\u3063\u3066\u8a55\u4fa1\u304c\u5909\u308f\u308b\uff1f\n    df['popular_tags_len'] = df['popular_tags'].fillna(\"\").apply(len)\n    df['categories_len'] = df['categories'].fillna(\"\").apply(len)\n    \n    # \u5024\u6bb5\u306b\u3088\u3063\u3066\u8a55\u4fa1\u304c\u5909\u308f\u308b\uff1f\n    df = price_preprocess(df)\n    \n    # developer\u3068publisher\u3092label encode\u3059\u308b\u3002\n    for column in ['developer', 'publisher']:\n        df = apply_label_encode(df, column)\n    \n    # \u5e74\u306b\u3088\u3063\u3066\u8a55\u4fa1\u304c\u5909\u308f\u308b\uff1f\n    # \u6b63\u898f\u8868\u73fe\u3067\u5e74\u3092\u62bd\u51fa\u3059\u308b\u3002\n    df['year'] = df['release_date'].str.extract(r'(\\d{4})')\n    df['year'] = df['year'].astype(float)\n    return df\n\ndef price_preprocess(df: pd.DataFrame) -> pd.DataFrame:\n    df['price'] = df['price'].replace({'free':0})\n    price_null_index = df[df['price']==''].index\n    df.loc[price_null_index, 'price'] = -1\n    df['price'] = df['price'].astype(float)\n    return df\n\ndef apply_label_encode(df:pd.DataFrame, column:str)-> pd.DataFrame:\n    label_df = df[~df[column].isnull()]\n    le = LabelEncoder()\n    le_data = le.fit_transform(label_df[column])\n    label_df_index = label_df.index\n    df.loc[label_df_index, f'{column}_label_encoding'] = le_data\n    return df","1e580a10":"df = category_numeric_preprocess(df)","5eaffb99":"def tfidf_preprocess(df: pd.DataFrame, column: str, embedding_dim: int=20)-> pd.DataFrame:\n    # tfidf\u3067\u5909\u63db\u3057\u305f\u5f8c\u306b\u3001SVD\u3067\u6642\u9650\u524a\u6e1b\u3059\u308b\u3002\n    tfidf_vec = TfidfVectorizer(ngram_range=(1,2))\n    text_tfidf = tfidf_vec.fit_transform(df[column].fillna('').values.tolist())\n    svd = TruncatedSVD(n_components=embedding_dim, algorithm='arpack',random_state=9999)\n    return svd.fit_transform(text_tfidf)\n\nclass W2VSWEM(object):\n    def __init__(self, word_list: List[str], embedding_dim:int=20):\n        # word2vec\u3067\u7279\u5fb4\u91cf\u3092\u4f5c\u3063\u3066\u3001wrod2vec\u306e\u5e73\u5747\u5024\u3092\u8a08\u7b97\u3059\u308b\u3002\n        self.word_list = word_list\n        self.embedding_dim = embedding_dim\n        self.__validation_type()\n        self.__train_w2v()\n        \n    def __validation_type(self):\n        if isinstance(self.word_list, pd.Series):\n            self.word_list = self.word_list.tolist()\n        if not isinstance(self.word_list, list):\n            raise TypeError(f'you should use list object, however you are using {type(word_list)}.')\n        \n    def __train_w2v(self):\n        self.w2v_model = Word2Vec(self.word_list, vector_size=self.embedding_dim, workers=1, seed=71)#seed\u3092\u56fa\u5b9a\n        self.vocab = self.w2v_model.wv.key_to_index\n        \n    def _get_single_column_vec(self, words) -> np.array:\n        result = []\n        \n        for word in words:\n            if word in self.vocab:\n                \n                vector = self.w2v_model.wv[word]\n                result.append(vector)\n        result = np.array(result)\n        return result\n    \n    def get_result(self):\n        self.swem_result = np.zeros([len(self.word_list), self.embedding_dim])\n        for num, i in enumerate(self.word_list):\n            swem = self._get_single_column_vec(i)\n            if len(swem):\n                swem = np.mean(swem, axis=0)\n            else:\n                swem = np.zeros(self.embedding_dim)\n            self.swem_result[num, :] = swem    ","b48cbb3e":"# word2vec\u3067tags\u3092encode\u3059\u308b\ntags_df = []\nfor column in ['categories', 'popular_tags']:\n    df[column] = df[column].fillna('')\n    w2v_swem = W2VSWEM(df[column].tolist())\n    w2v_swem.get_result()\n    result = w2v_swem.swem_result\n    _df = pd.DataFrame(result, columns=[f'w2v_{column}_{x}' for x in range(result.shape[1])])\n    tags_df.append(_df)\ntags_df = pd.concat(tags_df, axis=1)","65cdd650":"# \u30b2\u30fc\u30e0\u306e\u8aac\u660e\u3092tfidf\u3067\u7279\u5fb4\u91cf\u306b\u3059\u308b\ntext_svd = tfidf_preprocess(df, 'description')\ntext_svd_df = pd.DataFrame(text_svd, columns=[f'text_svd_{x}' for x in range(text_svd.shape[1])])","39874fee":"nlp_df = pd.concat([tags_df, text_svd_df], axis=1)","e19ec81c":"use_df = df.copy()\nuse_columns = ['price', 'popular_tags_len', 'categories_len', 'year', 'developer_label_encoding', 'publisher_label_encoding', 'user_reviews', 'is_train']\nuse_df = use_df[use_columns]","7e9009c5":"use_df = pd.concat([use_df, nlp_df], axis=1)","67a9ac87":"train = use_df[use_df['is_train']==True].reset_index(drop=True)\ntest =  use_df[use_df['is_train']!=True].reset_index(drop=True)","92eef010":"train = train.drop(columns=['is_train'])\ntest = test.drop(columns=['is_train', 'user_reviews'])","05b3bfa1":"X_train = train.copy().drop(columns=['user_reviews'])\ny_train = train['user_reviews']\nX_test = test.copy()","8cc60e6a":"y_train = y_train.map({'c0':0, 'c1':1, 'c2':2})","7bc70323":"#\u5fc5\u8981\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\nfrom sklearn.model_selection import KFold\nfrom lightgbm import LGBMRegressor\n\n# \u5909\u63db\u3059\u308b\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3092\u30ea\u30b9\u30c8\u306b\u683c\u7d0d\ncat_cols = ['developer_label_encoding']\nscores = []\n\n# \u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306efold\u3054\u3068\u306btarget encoding\u3092\u3084\u308a\u76f4\u3059\n#\u4eca\u56de\u306f5\u5206\u5272\u4ea4\u5dee\u691c\u5b9a\u3092\u884c\u3044\u307e\u3059\ncv = KFold(n_splits=5, shuffle=True, random_state=71)\nfor i, (train_ix, val_ix) in tqdm(enumerate(cv.split(X_train, y_train))):\n\n    # \u5b66\u7fd2\u30c7\u30fc\u30bf\u304b\u3089\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3092\u5206\u3051\u308b\n    X_train_, y_train_ = X_train.iloc[train_ix], y_train.iloc[train_ix]\n    X_val, y_val = X_train.iloc[val_ix], y_train.iloc[val_ix]\n\n    # \u5909\u6570\u3092\u30eb\u30fc\u30d7\u3057\u3066target encoding\n    for c in cat_cols:\n        # \u5b66\u7fd2\u30c7\u30fc\u30bf\u5168\u4f53\u3067\u5404\u30ab\u30c6\u30b4\u30ea\u306b\u304a\u3051\u308btarget\u306e\u5e73\u5747\u3092\u8a08\u7b97\n        data_tmp = pd.DataFrame({c: X_train_[c], 'target': y_train_})\n        target_mean = data_tmp.groupby(c)['target'].mean()\n        # \u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u306e\u30ab\u30c6\u30b4\u30ea\u3092\u7f6e\u63db\n        X_val.loc[:, c] = X_val[c].map(target_mean)\n\n        # \u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u5909\u63db\u5f8c\u306e\u5024\u3092\u683c\u7d0d\u3059\u308b\u914d\u5217\u3092\u6e96\u5099\n        tmp = np.repeat(np.nan, X_train_.shape[0])\n        cv_encoding = KFold(n_splits=5, shuffle=True, random_state=71)\n        for idx_1, idx_2 in cv_encoding.split(X_train_):\n            # out-of-fold\u3067\u5404\u30ab\u30c6\u30b4\u30ea\u306b\u304a\u3051\u308b\u76ee\u7684\u5909\u6570\u306e\u5e73\u5747\u3092\u8a08\u7b97\n            target_mean = data_tmp.iloc[idx_1].groupby(c)['target'].mean()\n            # \u5909\u63db\u5f8c\u306e\u5024\u3092\u4e00\u6642\u914d\u5217\u306b\u683c\u7d0d\n            tmp[idx_2] = X_train_[c].iloc[idx_2].map(target_mean)\n\n        X_train_.loc[:, c] = tmp\n    \n\n\n#\u3053\u3053\u304b\u3089LightGBM\u306e\u8a2d\u5b9a\u3092\u3057\u3066\u3044\u304d\u307e\u3059\n    model = LGBMRegressor(n_estimators=9999, \n                          learning_rate=0.1,\n                          random_state=71, \n                          objective='multiclass', \n                          num_class=3)\n    \n    model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)], eval_metric='multi_logloss', early_stopping_rounds=50)\n      \n    y_pred = model.predict(X_val)\n    y_pred_max = np.argmax(y_pred, axis=1) \n\n    score = sum(y_val == y_pred_max) \/ len(y_val)\n    scores.append(score)\n    \n    print('CV Scores of Fold_%d is %f' % (i, score))   ","c32b24f5":"# \u5e73\u5747\u30b9\u30b3\u30a2\u3092\u7b97\u51fa\nnp.array(scores).mean()","ce0a2ee7":"# \u5909\u6570\u3092\u30eb\u30fc\u30d7\u3057\u3066target encoding\nfor c in cat_cols:\n    # \u5b66\u7fd2\u30c7\u30fc\u30bf\u5168\u4f53\u3067\u5404\u30ab\u30c6\u30b4\u30ea\u306b\u304a\u3051\u308btarget\u306e\u5e73\u5747\u3092\u8a08\u7b97\n    data_tmp = pd.DataFrame({c: X_train[c], 'target': y_train})\n    target_mean = data_tmp.groupby(c)['target'].mean()\n    # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u30ab\u30c6\u30b4\u30ea\u3092\u7f6e\u63db\n    X_test[c] = X_test[c].map(target_mean)\n\n    # \u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u5909\u63db\u5f8c\u306e\u5024\u3092\u683c\u7d0d\u3059\u308b\u914d\u5217\u3092\u6e96\u5099\n    tmp = np.repeat(np.nan, X_train.shape[0])\n\n    # \u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u5206\u5272\n    cv = KFold(n_splits=5, shuffle=True, random_state=71)\n    for idx_1, idx_2 in cv.split(X_train):\n        # out-of-fold\u3067\u5404\u30ab\u30c6\u30b4\u30ea\u306b\u304a\u3051\u308b\u76ee\u7684\u5909\u6570\u306e\u5e73\u5747\u3092\u8a08\u7b97\n        target_mean = data_tmp.iloc[idx_1].groupby(c)['target'].mean()\n        # \u5909\u63db\u5f8c\u306e\u5024\u3092\u4e00\u6642\u914d\u5217\u306b\u683c\u7d0d\n        tmp[idx_2] = X_train[c].iloc[idx_2].map(target_mean)\n\n    # \u5909\u63db\u5f8c\u306e\u30c7\u30fc\u30bf\u3067\u5143\u306e\u5909\u6570\u3092\u7f6e\u63db\n    X_train[c] = tmp\n\n\nbest_iter = model.best_iteration_\nmodel = LGBMRegressor(n_estimators=best_iter, \n                      learning_rate=0.1, \n                      random_state=71, \n                      objective='multiclass', \n                      num_class=3)\n\nmodel.fit(X_train, y_train)","8b09e5ef":"y_pred_test = model.predict(X_test)\n\n # \u8fd4\u308a\u5024\u306f\u78ba\u7387\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u6700\u5c24\u306b\u5bc4\u305b\u308b\ny_pred_test_max=np.argmax(y_pred_test, axis=1)","d0050c88":"# \u63d0\u51fa\u7528\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3092\u8aad\u307f\u8fbc\u3093\u3067\u4e88\u6e2c\u5024\u3092\u4ee3\u5165\u3057\u307e\u3059\u3002\nsubmission = pd.read_csv('..\/input\/data-science-winter-osaka2\/sample_submission.csv', index_col=0)\nsubmission.user_reviews = y_pred_test_max\n\nsubmission.user_reviews = submission.user_reviews.replace(0,'c0').replace(1,'c1').replace(2,'c2')","701163b0":"submission","58bbb136":"submission.to_csv('submission.csv', index=True)","ad6510b7":"\uff08\u4e00\u53e3\u30e1\u30e2\uff09<br>\nTFIDF\u3082W2V\u3082\u3001embedding_dim: int=20\u3000\u3092int=50\u304f\u3089\u3044\u306b\u5909\u3048\u305f\u307b\u3046\u304c\u30b9\u30b3\u30a2\u306f\u4e0a\u304c\u308b\u3068\u601d\u3044\u307e\u3059\u3002<br>\nTFIDF\u306e\"ngram_range=(1,2)\"\u3092\"ngram_range=(1,1)\"\u306b\u3059\u308b\u3068\u3001\u5358\u8a9e\u3057\u304b\u62fe\u3048\u306a\u304f\u306a\u308a\u307e\u3059\u304c\u51e6\u7406\u306f\u65e9\u304f\u6e08\u307f\u307e\u3059\u3002<br>\n","86dd64d3":"lightGBM\u3067\u30bf\u30fc\u30b2\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3092\u3057\u3066\u307f\u307e\u3057\u305f\u3002<br>\n\u5358\u7d14\u306b\u30bf\u30fc\u30b2\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3092\u3059\u308b\u3068\u30ea\u30fc\u30af\u3057\u3066\u3057\u307e\u3046\u305f\u3081\u3001\u5de5\u592b\u304c\u5fc5\u8981\u306a\u306e\u3067\u3059\u304c\u3001<br>\n\u4eca\u56de\u306f\u305d\u306e\u5de5\u592b\u306e1\u4f8b\u3092\u5171\u6709\u3057\u3088\u3046\u3068\u601d\u3044\u307e\u3059\u3002<br>\n<br>\n\u203b\u9593\u9055\u3063\u305f\u3053\u3068\u3092\u66f8\u3044\u3066\u3044\u305f\u3089\u6307\u6458\u3057\u3066\u9802\u3051\u308b\u3068\u52a9\u304b\u308a\u307e\u3059\uff08\u81ea\u5206\u306e\u30b3\u30fc\u30c9\u306e\u7b54\u3048\u5408\u308f\u305b\u3082\u517c\u306d\u3066\u307e\u3059(;^\u03c9^)\uff09","253c835b":"\u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306b\u304a\u3044\u3066target encording\u3092\u9069\u7528\u3057\u307e\u3059\u3002<br>\n\uff08\u66f8\u7c4d\u3000\"Kaggle\u3067\u52dd\u3064 \u30c7\u30fc\u30bf\u5206\u6790\u306e\u6280\u8853\"\u3000P146\u53c2\u7167\uff09<br>\n<br>\n\u5b66\u7fd2\u306fLightGBM\u30b7\u30f3\u30b0\u30eb\u3067\u3044\u304d\u307e\u3059\uff08\u305d\u306e\u307b\u3046\u304c\u7c21\u5358\u3067\u5206\u304b\u308a\u3084\u3059\u3044\u304b\u3068\u601d\u3046\u306e\u3067\uff09<br>\n\u4eca\u56de\u306f\u4f8b\u3068\u3057\u3066'developer_label_encoding'\u3092target encording\u3057\u3066\u307f\u307e\u3059\u3002<br>\n\u3044\u308d\u3044\u308d\u8a66\u3057\u3066\u52b9\u679c\u306e\u5927\u304d\u304f\u306a\u308b\u7279\u5fb4\u91cf\u3092\u63a2\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044","a1e0111e":"# **\u30bf\u30fc\u30b2\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3092\u3057\u3066\u5b66\u7fd2\u3055\u305b\u308b**","873de112":"# **\u5168\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u518d\u5b66\u7fd2**","cfcd7fa2":"# **\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080**\nEDA\u306f\u7701\u7565\u3057\u307e\u3059","99b18737":"# **\u524d\u51e6\u7406\u3092\u884c\u3046**","235faf4a":"# **\u8a00\u8a9e\u7cfb\u306e\u7279\u5fb4\u91cf\u3092\u4f5c\u308b**","1fad9518":"# **Submisson file\u3092\u4f5c\u308b**"}}