{"cell_type":{"5817d6a3":"code","68477099":"code","a963c48d":"code","86cbc716":"code","7bef179e":"code","19e76f16":"code","1eeb085b":"code","65c13eec":"code","f636f1fe":"code","51419541":"code","15ca1fde":"code","cd4759af":"code","ea303e4b":"code","1fd27a12":"code","6d381b06":"code","6438996f":"code","c449d2e4":"code","8c2bdd50":"code","a7d31059":"code","fdc8299b":"code","89083ae9":"code","189791fc":"code","9442a723":"code","0fbb0009":"code","30680599":"code","ccdc28c6":"code","d61d449a":"code","dcabf679":"code","078e6eaf":"code","7fe44318":"code","b2505102":"code","cf9b6f93":"code","6cdce4c1":"markdown","c20d13dd":"markdown"},"source":{"5817d6a3":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os, sys, gc, warnings, random, datetime, math, psutil\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom multiprocessing import Pool\n\nwarnings.filterwarnings('ignore')","68477099":"########################### Helpers\n#################################################################################\n## Multiprocessing Run.\n# :df - DataFrame to split                      # type: pandas DataFrame\n# :func - Function to apply on each split       # type: python function\n# This function is NOT 'bulletproof', be carefull and pass only correct types of variables.\ndef df_parallelize_run(df, func):\n    num_partitions, num_cores = psutil.cpu_count(), psutil.cpu_count()  # number of partitions and cores\n    df_split = np.array_split(df, num_partitions)\n    pool = Pool(num_cores)\n    df = pd.concat(pool.map(func, df_split))\n    pool.close()\n    pool.join()\n    return df\n\ndef check_state():\n    if LOCAL_TEST:\n        bad_uids = full_df.groupby(['uid'])['isFraud'].agg(['nunique', 'count'])\n        bad_uids = bad_uids[(bad_uids['nunique']==2)]\n        print('Inconsistent groups',len(bad_uids))\n\n    print('Cleaning done...')\n    print('Total groups:', len(full_df['uid'].unique()), \n          '| Total items:', len(full_df),\n          '| Total fraud', full_df['isFraud'].sum())\n    \n    \n########################### Sainity check \ndef sanity_check_run(temp_df, verbose=False):\n    temp_df = temp_df.copy()\n    temp_df = temp_df.sort_values(by='TransactionID').reset_index(drop=True)\n    bad_uids_groups = pd.DataFrame()\n\n    \"\"\"\n    for col in ['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']:\n        temp_df['sanity_check'] = temp_df.groupby(['uid'])[col].shift()\n        temp_df['sanity_check'] = (temp_df[col]-temp_df['sanity_check']).fillna(0).clip(None,0)\n\n        bad_uids = temp_df.groupby(['uid'])['sanity_check'].agg(['sum']).reset_index()\n        bad_uids = bad_uids[bad_uids['sum']<0]\n        bad_uids_groups = pd.concat([bad_uids_groups,bad_uids])\n        if verbose: print(col, len(bad_uids), bad_uids['uid'].values[:2])\n    \"\"\"\n    \n    bad_uids = temp_df.groupby(['uid'])['V313'].agg(['nunique']).reset_index()\n    bad_uids = bad_uids[(bad_uids['nunique']>2)]\n    bad_uids_groups = pd.concat([bad_uids_groups,bad_uids])\n    if verbose: print('V313:', len(bad_uids), bad_uids['uid'].values[:2])\n\n    bad_uids_groups = bad_uids_groups[['uid']].drop_duplicates()\n    if verbose: print('Total bad groups:', len(bad_uids_groups))\n    return bad_uids_groups\n\n\n\ndef parallel_check(bad_uids_groups):\n    bad_uids_items = []\n    if True:\n        for cur_uid in list(bad_uids_groups['uid'].unique()):\n            temp_df = full_df[full_df['uid']==cur_uid].reset_index(drop=True)\n            v313_values = temp_df['V313'].value_counts()\n            if len(v313_values)>1: \n                v313_values = [[col for col in list(v313_values.index)[:2] if col!=0][0]] + [0]\n            else: \n                v313_values = [list(v313_values.index)[0]]+[0]\n                \n            for i in range(1,len(temp_df)):\n                item_1 = temp_df.iloc[i]\n                item_2 = temp_df.iloc[i-1]\n\n                check_if_match = temp_df.drop([i])\n                check_if_match = sanity_check_run(check_if_match)\n                if len(check_if_match) == 0:\n                    bad_uids_items.append(item_1['TransactionID'])\n                    break\n\n                if i!=1:\n                    check_if_match = temp_df.drop([i-1])\n                    check_if_match = sanity_check_run(check_if_match)\n                    if len(check_if_match) == 0:\n                        bad_uids_items.append(item_2['TransactionID'])\n                        break\n                \n                \"\"\"\n                for col in ['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']:\n                    check_sanity = item_1[col]<item_2[col]\n                    if check_sanity:\n                        bad_uids_items.append(item_1['TransactionID'])\n                        break \n                        \n                if check_sanity:\n                    break\n                \"\"\"    \n                check_sanity = item_1['V313'] not in v313_values\n                if check_sanity:\n                    bad_uids_items.append(item_1['TransactionID'])\n                    break \n                    \n                if temp_df['TransactionID'].isin(problem_items['TransactionID']).sum()==0:\n                    if cur_uid>=0:\n                        check_sanity = ((item_2['DT_day']-item_1['uid_td_D3'])**2)**0.5>1\n\n                        if check_sanity:\n                            bad_uids_items.append(item_1['TransactionID'])\n                            break\n\n    bad_uids_items = pd.DataFrame(bad_uids_items, columns=['uid'])\n    return bad_uids_items\n ","a963c48d":"LOCAL_TEST = True\nCHECK_ORDER = True\nTRUST_D1 = True\nMULTI_UID_CHECK = False\nFULL_GROUP_CHECK = False","86cbc716":"########################### DATA LOAD\n#################################################################################\nprint('Load Data')\ntrain_df = pd.read_pickle('..\/input\/ieee-data-minification-private\/train_transaction.pkl')\ntest_df = pd.read_pickle('..\/input\/ieee-data-minification-private\/test_transaction.pkl')\n\n# Full Data set (careful with target encoding)\nfull_df = pd.concat([train_df, test_df]).reset_index(drop=True)\n\nif LOCAL_TEST:\n    full_df = full_df.iloc[:10000] #full_df[(full_df['DT_M']==12)]","7bef179e":"########################### Base prepartion\n\nfull_df['full_addr'] = full_df['addr1'].astype(str)+'_'+full_df['addr2'].astype(str)\n\nfor col in ['D'+str(i) for i in [1,2,3,5,10,11,15]]: \n    new_col = 'uid_td_'+str(col)\n    \n    full_df[new_col] = full_df['TransactionDT'] \/ (24*60*60)\n    full_df[new_col] = np.floor(full_df[new_col] - full_df[col]) + 1000\n\nfull_df['DT_day'] = np.floor(full_df['TransactionDT']\/(24*60*60)) + 1000\n\nfull_df['TransactionAmt_fix'] = np.round(full_df['TransactionAmt'],2)\nfull_df['V313_fix'] = np.round(full_df['V313'],2)\nfull_df['uid'] = np.nan\n\nv_cols = []\nv_fix_cols = []\nfor col in ['V'+str(i) for i in range(1,340)]:\n    if (full_df[col].fillna(0) - full_df[col].fillna(0).astype(int)).sum()!=0:\n        if col not in ['V313']:\n            v_cols.append(col)\n            v_fix_cols.append(col+'_fix')\n            full_df[col+'_fix_ground'] = np.round(full_df[col],2)\n            full_df[col+'_fix'] = full_df[col+'_fix_ground'] + full_df['TransactionAmt_fix']\n\nglobal_bad_items = full_df[full_df['D1'].isna()]\nfull_df = full_df[~full_df['TransactionDT'].isin(global_bad_items['TransactionDT'])]\nall_items = full_df.copy()\nbkp_items = full_df.copy()\n\nprint('Total number of transactions:', len(full_df))","19e76f16":"########################### Single Transaction\n# Let's filter single card apearence card1\/D1 -> single transaction per card\nfull_df['count'] = full_df.groupby(['card1','uid_td_D1'])['TransactionID'].transform('count')\nsingle_items = full_df[full_df['count']==1]\nsingle_items['uid'] = single_items['TransactionID']\ndel full_df, single_items['count']\n\nall_items = all_items[~all_items['TransactionID'].isin(single_items['TransactionID'])]\nprint('Single transaction',len(single_items))","1eeb085b":"### Clean full_df\nfull_df = pd.DataFrame()\n###","65c13eec":"# First appearance of card1\n\nfirst_df = all_items.copy()\nfirst_df['counts'] = first_df.groupby(['card1','uid_td_D1']).cumcount()\nfirst_df = first_df[first_df['counts']==0]\ndel first_df['counts']\n\nfirst_df['uid'] = first_df['TransactionID']\nprint('First time in dataset', len(first_df))\n\nfull_df = pd.concat([full_df,first_df])\nfull_df = full_df.sort_values(by='TransactionID').reset_index(drop=True)\ndel first_df","f636f1fe":"check_state()","51419541":"# Lets Check unassigned items again\n# Let's find itmes with roots out of our dataset\nnan_df_check = all_items[~all_items['TransactionID'].isin(full_df['TransactionID'])]\n\n# if 'uid_td_D3'>1000 it means that root item is in our dataset\n# >1000 will also filter NaNs values\nnan_df_check['uid'] = np.where(nan_df_check['uid_td_D3']>=1001, \n                               np.nan, nan_df_check['TransactionID'])\nnan_df_check = nan_df_check[~nan_df_check['uid'].isna()]\n\nfull_df = pd.concat([full_df,nan_df_check])\nfull_df = full_df.sort_values(by='TransactionID').reset_index(drop=True)\n\nprint('Roots out of dataset', len(nan_df_check))\n#del nan_df_check\nout_of_bonds = nan_df_check[['TransactionID']]","15ca1fde":"check_state()","cd4759af":"########################### VERY IMPORTANT\n# Do not do sanity D3 check for gap items\nproblem_items = full_df[(full_df['uid_td_D3']>1182)&(full_df['uid_td_D3']<1213)]\n\nout_of_bonds = pd.concat([out_of_bonds, problem_items[['TransactionID']]])","ea303e4b":"########################### Sort\nall_items = all_items.sort_values(by='TransactionID').reset_index(drop=True)\nsingle_items = single_items.sort_values(by='TransactionID').reset_index(drop=True)\nfull_df = full_df.sort_values(by='TransactionID').reset_index(drop=True)\nout_of_bonds = out_of_bonds.sort_values(by='TransactionID').reset_index(drop=True)","1fd27a12":"def find_and_append_root(df):\n    new_uids_items = {'TransactionID': [],\n                      'uid': [],\n                      }\n\n    for i in range(len(df)):\n        item = df.iloc[i]\n        if item['TransactionID'] not in list(problem_items['TransactionID']):\n            mask_1 = bkp_items['card1'] == item['card1']\n            mask_2 = bkp_items['uid_td_D1'] == item['uid_td_D1']\n            mask_3 = bkp_items['TransactionID'] < item['TransactionID']\n            mask_4 = ((bkp_items['DT_day'] == item['uid_td_D3'] + 1)|\n                      (bkp_items['DT_day'] == item['uid_td_D3'] - 1)|\n                      (bkp_items['DT_day'] == item['uid_td_D3']))\n\n            df_masked = bkp_items[mask_1 & mask_2 & mask_3 & mask_4]\n            no_match = len(df_masked) == 0\n\n            if no_match:\n                new_uids_items['TransactionID'].append(item['TransactionID'])\n                new_uids_items['uid'].append(item['TransactionID'])\n \n    return_df = pd.DataFrame.from_dict(new_uids_items)\n    return return_df","6d381b06":"########################### PART X - > 100% Root\n\nnan_df = all_items[~all_items['TransactionID'].isin(full_df['TransactionID'])]\nnan_df = nan_df[nan_df['DT_M']<18]\nprint('Items to check:', len(nan_df))\n\ndf_cleaned = df_parallelize_run(nan_df, find_and_append_root)\ndf_cleaned = df_cleaned[~df_cleaned['uid'].isna()]\n\ndf_cleaned.index = df_cleaned['TransactionID']\ntemp_dict = df_cleaned['uid'].to_dict()\nnan_df['uid'] = nan_df['TransactionID'].map(temp_dict)\nnan_df = nan_df[~nan_df['uid'].isna()]\nprint('Assigned root items:', len(nan_df))\n\n# append found items\nfull_df = pd.concat([full_df,nan_df]).sort_values(by='TransactionID').reset_index(drop=True)\ncheck_state()","6438996f":"def append_item_to_uid(df):\n    new_uids_items = {'TransactionID': [],\n                      'uid': [],\n                      }\n\n    for i in range(len(df)):\n        item = df.iloc[i]\n\n        mask_1 = full_df['card1'] == item['card1']\n        mask_2 = full_df['uid_td_D1'] == item['uid_td_D1']\n        mask_3 = full_df['TransactionID'] < item['TransactionID']\n        mask_4 = full_df['DT_day'] <= item['uid_td_D3'] + 1  # +1 just to ensure that there is no\n        df_masked = full_df[mask_1 & mask_2 & mask_3 & mask_4]\n\n        has_match = len(df_masked) > 0\n        can_be_root = True\n\n        # New check\n        # addr2 should be nan or same as group addr2\n        for col in ['addr2','addr1']:\n            if has_match:\n                if not np.isnan(item[col]):\n                    mask = ((df_masked[col] == item[col]) | (df_masked[col].isna()))\n                    df_masked = df_masked[mask]\n                else:\n                    df_masked = df_masked\n\n                if len(df_masked) == 0:\n                    has_match = False\n        \"\"\"\n        # Order                   \n        for col in ['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']:\n            if has_match:\n                mask = df_masked[col] <= item[col]\n                df_masked = df_masked[mask]\n\n                if len(df_masked) == 0:\n                    has_match = False\n        \"\"\"\n        \n        if has_match:\n            mask = (df_masked['TransactionID'] > item['TransactionID']).astype(int)\n            for col in v_cols:\n                mask += (df_masked[col+'_fix'] == item[col+'_fix_ground']).astype(int)\n            mask = mask>0\n            df_masked = df_masked[mask]\n            \n            if len(df_masked) == 0:\n                has_match = False\n                    \n        # Assign  \n        if has_match and len(df_masked['uid'].unique()) == 1:\n            check_if_match = df_masked.append(item)\n            check_if_match = sanity_check_run(check_if_match)\n            if len(check_if_match) == 0:\n                new_uids_items['TransactionID'].append(item['TransactionID'])\n                new_uids_items['uid'].append(df_masked['uid'].unique()[0])\n\n\n    return_df = pd.DataFrame.from_dict(new_uids_items)\n    return return_df","c449d2e4":"########################### PART X - > 100% single match\nfor i in range(5):\n    print('Check round:', i)\n        \n    nan_df = all_items[~all_items['TransactionID'].isin(full_df['TransactionID'])]\n    print('Items to check:', len(nan_df))\n\n    df_cleaned = df_parallelize_run(nan_df, append_item_to_uid)\n    df_cleaned = df_cleaned[~df_cleaned['uid'].isna()]\n\n    df_cleaned.index = df_cleaned['TransactionID']\n    temp_dict = df_cleaned['uid'].to_dict()\n    nan_df['uid'] = nan_df['TransactionID'].map(temp_dict)\n    nan_df = nan_df[~nan_df['uid'].isna()]\n    print('Assigned items:', len(nan_df))\n\n    # append found items\n    full_df = pd.concat([full_df,nan_df]).sort_values(by='TransactionID').reset_index(drop=True)\n    \n    for i in range(100):\n        bad_uids_groups = sanity_check_run(full_df, False)\n        if len(bad_uids_groups)==0:\n            break\n        elif len(bad_uids_groups)>64:\n            bad_uids_items = df_parallelize_run(bad_uids_groups, parallel_check)\n        else:\n            bad_uids_items = parallel_check(bad_uids_groups)\n\n        print('Found bad items', len(bad_uids_items))\n        full_df['uid'] = np.where(full_df['TransactionID'].isin(bad_uids_items['uid']), np.nan, full_df['uid'])\n        full_df = full_df[~full_df['uid'].isna()].sort_values(by='TransactionID').reset_index(drop=True)\n        if len(bad_uids_items)<2:\n            break\n    check_state()","8c2bdd50":"def find_and_append_root_test(df):\n    new_uids_items = {'TransactionID': [],\n                      'uid': [],\n                      }\n\n    for i in range(len(df)):\n        item = df.iloc[i]\n        if item['TransactionID'] not in list(problem_items['TransactionID']):\n            mask_1 = bkp_items['card1'] == item['card1']\n            mask_2 = bkp_items['uid_td_D1'] == item['uid_td_D1']\n            mask_3 = bkp_items['TransactionID'] < item['TransactionID']\n            mask_4 = ((bkp_items['DT_day'] == item['uid_td_D3'] + 1)|\n                      (bkp_items['DT_day'] == item['uid_td_D3'] - 1)|\n                      (bkp_items['DT_day'] == item['uid_td_D3']))\n\n            df_masked = bkp_items[mask_1 & mask_2 & mask_3 & mask_4]\n            no_match = len(df_masked) == 0\n\n            if no_match:\n                new_uids_items['TransactionID'].append(item['TransactionID'])\n                new_uids_items['uid'].append(item['TransactionID'])\n \n    return_df = pd.DataFrame.from_dict(new_uids_items)\n    return return_df","a7d31059":"########################### PART X - > 100% Root\n\nnan_df = all_items[~all_items['TransactionID'].isin(full_df['TransactionID'])]\nnan_df = nan_df[nan_df['DT_M']>18]\nprint('Items to check:', len(nan_df))\n\ndf_cleaned = df_parallelize_run(nan_df, find_and_append_root_test)\ndf_cleaned = df_cleaned[~df_cleaned['uid'].isna()]\ndf_cleaned = df_cleaned[~df_cleaned['TransactionID'].isin(full_df['TransactionID'])]\n\ndf_cleaned.index = df_cleaned['TransactionID']\ntemp_dict = df_cleaned['uid'].to_dict()\nnan_df['uid'] = nan_df['TransactionID'].map(temp_dict)\nnan_df = nan_df[~nan_df['uid'].isna()]\nprint('Assigned root items:', len(nan_df))\n\n# append found items\nfull_df = pd.concat([full_df,nan_df]).sort_values(by='TransactionID').reset_index(drop=True)\ncheck_state()","fdc8299b":"########################### PART X - > 100% single match\nfor i in range(3):\n    print('Check round:', i)\n        \n    nan_df = all_items[~all_items['TransactionID'].isin(full_df['TransactionID'])]\n    print('Items to check:', len(nan_df))\n\n    df_cleaned = df_parallelize_run(nan_df, append_item_to_uid)\n    df_cleaned = df_cleaned[~df_cleaned['uid'].isna()]\n\n    df_cleaned.index = df_cleaned['TransactionID']\n    temp_dict = df_cleaned['uid'].to_dict()\n    nan_df['uid'] = nan_df['TransactionID'].map(temp_dict)\n    nan_df = nan_df[~nan_df['uid'].isna()]\n    print('Assigned items:', len(nan_df))\n\n    # append found items\n    full_df = pd.concat([full_df,nan_df]).sort_values(by='TransactionID').reset_index(drop=True)\n    \n    for i in range(100):\n        bad_uids_groups = sanity_check_run(full_df, False)\n        if len(bad_uids_groups)==0:\n            break\n        elif len(bad_uids_groups)>64:\n            bad_uids_items = df_parallelize_run(bad_uids_groups, parallel_check)\n        else:\n            bad_uids_items = parallel_check(bad_uids_groups)\n\n        print('Found bad items', len(bad_uids_items))\n        full_df['uid'] = np.where(full_df['TransactionID'].isin(bad_uids_items['uid']), np.nan, full_df['uid'])\n        full_df = full_df[~full_df['uid'].isna()].sort_values(by='TransactionID').reset_index(drop=True)\n        if len(bad_uids_items)<2:\n            break\n    check_state()","89083ae9":"full_df[['TransactionID','uid']].to_csv('uids_part_1_v6.csv')","189791fc":"def find_multigroup(df):\n    new_uids_items = {'TransactionID': [],\n                      'multi_uid': [],\n                      }\n\n    for i in range(len(df)):\n        item = df.iloc[i]\n        if item['TransactionID'] not in problem_items['TransactionID']:\n            mask_1 = full_df['card1'] == item['card1']\n            mask_2 = full_df['uid_td_D1'] == item['uid_td_D1']\n            mask_3 = full_df['TransactionID'] < item['TransactionID']\n            mask_4 = ((full_df['DT_day'] == item['uid_td_D3'] + 1)|\n                      (full_df['DT_day'] == item['uid_td_D3'] - 1)|\n                      (full_df['DT_day'] == item['uid_td_D3']))\n\n            df_masked = full_df[mask_1 & mask_2 & mask_3 & mask_4]\n            has_match = len(df_masked) > 0\n\n            if has_match:\n                new_uids_items['TransactionID'].append(item['TransactionID'])\n                new_uids_items['multi_uid'].append(list(df_masked['uid'].unique()))\n \n    return_df = pd.DataFrame.from_dict(new_uids_items)\n    return return_df\n\ndef find_and_filter_groups(df):\n    filtered_groups = []\n\n    for i in range(len(df)):\n        test_id = df.iloc[i]['TransactionID']\n        test_item = all_items[all_items['TransactionID']==test_id].iloc[0]\n        possible_groups = df.iloc[i]['multi_uid']\n        clean_group = find_right_uid(possible_groups, test_item)\n        filtered_groups.append([test_id, clean_group])\n    filtered_groups = pd.DataFrame(filtered_groups, columns=['TransactionID','uid'])    \n    return filtered_groups\n\nimport operator\n\ndef find_right_uid(possible_groups, test_item):\n    separated_uids = {}\n\n    test_features_set1 = {\n        'TransactionAmt':2,\n        'card2':1,\n        'card3':1,\n        'card4':1,\n        'card5':1,\n        'card6':1,\n        'uid_td_D2':2,\n        'uid_td_D10':2,\n        'uid_td_D11':2,\n        'uid_td_D15':2,\n        'C14':1,\n        'addr1':1,\n        'addr2':1,\n        'P_emaildomain':1,\n        'V313_fix':1,\n        }\n\n    groups_score = {}\n\n    for possible_group in possible_groups:\n        masked_df = full_df[full_df['uid']==possible_group]\n        cur_score = 0\n        for col in test_features_set1:\n            if test_item[col] in list(masked_df[col]):\n                cur_score += test_features_set1[col]\n        \n        for col in v_cols:\n            if test_item[col]!=0:\n                if test_item[col+'_fix_ground'] in list(masked_df[col+'_fix']):\n                    cur_score += 1\n                    \n        check_if_match = masked_df.append(test_item)\n        check_if_match = sanity_check_run(check_if_match)\n        if len(check_if_match)==0:\n            groups_score[possible_group] = cur_score\n        \n    new_uid = np.nan\n    try:\n        new_uid = max(groups_score.items(), key=operator.itemgetter(1))[0]\n    except:\n        pass\n    return new_uid","9442a723":"########################### PART X - > With multigroup check\nnan_df = all_items[~all_items['TransactionID'].isin(full_df['TransactionID'])]\nprint('Items to check:', len(nan_df))\n\ndf_cleaned = df_parallelize_run(nan_df, find_multigroup)\ndf_cleaned = df_cleaned[~df_cleaned['multi_uid'].isna()]\n\nfiltered_groups = df_parallelize_run(df_cleaned, find_and_filter_groups)\nfiltered_groups.index = filtered_groups['TransactionID']\ntemp_dict = filtered_groups['uid'].to_dict()\nnan_df['uid'] = nan_df['TransactionID'].map(temp_dict)\nnan_df = nan_df[~nan_df['uid'].isna()]\nprint('Assigned items:', len(nan_df))\n\n# append found items\nfull_df = pd.concat([full_df,nan_df]).sort_values(by='TransactionID').reset_index(drop=True)\ncheck_state()\n","0fbb0009":"for i in range(100):\n    bad_uids_groups = sanity_check_run(full_df, False)\n    if len(bad_uids_groups)==0:\n        break\n    elif len(bad_uids_groups)>64:\n        bad_uids_items = df_parallelize_run(bad_uids_groups, parallel_check)\n    else:\n        bad_uids_items = parallel_check(bad_uids_groups)\n\n    print('Found bad items', len(bad_uids_items))\n    full_df['uid'] = np.where(full_df['TransactionID'].isin(bad_uids_items['uid']), np.nan, full_df['uid'])\n    full_df = full_df[~full_df['uid'].isna()].sort_values(by='TransactionID').reset_index(drop=True)\n    if len(bad_uids_items)<2:\n        break\ncheck_state()","30680599":"print('Start items:', len(bkp_items))\nprint('Start items Frauds:', bkp_items['isFraud'].sum())","ccdc28c6":"print('Uids Items:', len(single_items)+len(full_df))\nprint('Uids Frauds:', full_df['isFraud'].sum() + single_items['isFraud'].sum())","d61d449a":"full_df_final = pd.concat([full_df, \n                     single_items, \n                     global_bad_items,\n                     all_items[~all_items['TransactionID'].isin(full_df['TransactionID'])]\n                    ])","dcabf679":"print('Combined Uids:', len(full_df_final))\nprint('CombinedUids Frauds:', full_df_final['isFraud'].sum())","078e6eaf":"full_df['count'] = full_df.groupby(['uid'])['TransactionID'].transform('count')\nfull_df['count'].mean()","7fe44318":"len(full_df_final['uid'].unique())","b2505102":"check_state()","cf9b6f93":"########################### Export\nfull_df_final[['TransactionID','uid']].to_csv('uids_full_v6.csv')","6cdce4c1":"We found all root items. There is just very rare cases that a new root can appear \n_____","c20d13dd":"----\n### Export"}}