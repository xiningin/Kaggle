{"cell_type":{"57cfa654":"code","81dcdc72":"code","11fbb766":"code","c2ff19e2":"code","c8d42435":"code","962c05bc":"code","7fc0bddf":"code","4a14dfcc":"code","d8f1dc5f":"code","5bf50ca6":"code","a469e72d":"code","4fe5ca2c":"code","284f43c2":"code","ee7190b7":"code","08c85cd0":"code","06fc4084":"code","e6050135":"code","130f3b8f":"code","6468f4ee":"code","3aa7a2a1":"code","744123cd":"code","2615beb1":"code","f830cfc8":"code","752fae88":"code","6d1fd2cb":"code","24bba16f":"code","cbbe556c":"code","2a349fcd":"code","de81e66d":"code","3f8ff4c0":"code","fce9c37e":"code","a61d0a12":"code","323e3fff":"code","5efa369f":"code","8350a52d":"code","e9a0677d":"code","966e3255":"code","6a1bc875":"code","a3bd479a":"code","f7cbb416":"code","054c91c3":"code","420357f1":"code","79fd31d9":"markdown","ed21635a":"markdown","056e4241":"markdown","ac2a091f":"markdown","636f4350":"markdown","56deb6f1":"markdown","20749ce8":"markdown","28817b8f":"markdown","fb2c3ff9":"markdown","3049a059":"markdown","f8598523":"markdown","31436859":"markdown","ae67e03a":"markdown","74e0e1fa":"markdown","a79045a0":"markdown","b553c5bd":"markdown","8de34d8b":"markdown","b84ec5cc":"markdown","a2de4fd7":"markdown","50ac30af":"markdown","a8d3fe33":"markdown","221fedf9":"markdown","e4b1b352":"markdown","a461a6a3":"markdown","832d371e":"markdown","d7744080":"markdown","c422fcbe":"markdown","2562d1f8":"markdown","22c06b8e":"markdown","e75fd4de":"markdown","b9a91a93":"markdown","f3617c91":"markdown","c268b9b2":"markdown","cc159b7a":"markdown","8755c02b":"markdown","56deb9cd":"markdown","c53a663c":"markdown","dc33dae7":"markdown"},"source":{"57cfa654":"#!pip install plotly==4.7.1 cufflinks\nimport plotly\nprint(plotly.__version__)\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn.impute import SimpleImputer\n# Basic packages\nimport pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns, gc\nfrom scipy import stats; from scipy.stats import zscore, norm, randint\nimport matplotlib.style as style; style.use('fivethirtyeight')\n# Models\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score, learning_curve\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Display settings\npd.options.display.max_rows = 400\npd.options.display.max_columns = 100\npd.options.display.float_format = \"{:.2f}\".format\n\nrandom_state = 42\nnp.random.seed(random_state)\n\n# Suppress warnings\nimport warnings; warnings.filterwarnings('ignore')","81dcdc72":"data = pd.read_csv(\"..\/input\/industrial-safety-and-health-analytics-database\/IHMStefanini_industrial_safety_and_health_database_with_accidents_description.csv\")\ndata.head()","11fbb766":"data.columns","c2ff19e2":"data.drop(\"Unnamed: 0\", axis=1, inplace=True)\ndata.rename(columns={'Data':'Date', 'Countries':'Country', 'Genre':'Gender', 'Employee or Third Party':'Employee type'}, inplace=True)\ndata.head(3)","c8d42435":"data.info()\n","962c05bc":"columns = data[data.columns[~data.columns.isin(['Description', 'Unnamed: 0', 'Data'])]].columns.tolist()\nfor cols in columns:\n    print(f'Unique values for {cols} is \\n{data[cols].unique()}\\nUnique count per label is:\\n{data[cols].value_counts()}\\n')","7fc0bddf":"# replacing categorical values\nreplace_struc = {'Local_01': 1, 'Local_02': 2, 'Local_03': 3, 'Local_04': 4, 'Local_05': 5, 'Local_06': 6, 'Local_07': 7, 'Local_08': 8, 'Local_09': 9, 'Local_10': 10, 'Local_11': 11, 'Local_12': 12}\ndata['Local'] = data['Local'].map(replace_struc)\nreplace_struc = {'I': 0, 'II': 1, 'III': 3, 'IV': 4, 'V': 5}\ndata['Accident Level'] = data['Accident Level'].map(replace_struc)\nreplace_struc = {'I': 0, 'II': 1, 'III': 3, 'IV': 4, 'V': 5, 'VI': 6}\ndata['Potential Accident Level'] = data['Potential Accident Level'].map(replace_struc)\ndel replace_struc","4a14dfcc":"print(f'Dataset after mapping categorical values\\n')\ndata.head()","d8f1dc5f":"data.isnull().sum()","5bf50ca6":"data.shape","a469e72d":"#For time series analysis\ndata['Date'] = pd.to_datetime(data['Date'])\ndata['Year'] = data['Date'].apply(lambda x : x.year)\ndata['Month'] = data['Date'].apply(lambda x : x.month)\ndata['Day'] = data['Date'].apply(lambda x : x.day)\ndata['Weekday'] = data['Date'].apply(lambda x : x.day_name())\ndata.head(3)","4fe5ca2c":"def month_quarter_Conversion(x):\n    if x in [10, 11, 12]:\n        season = 'Fourth'\n    elif x in [1, 2, 3]:\n        season = 'First'\n    elif x in [4, 5, 6]:\n        season = 'Second'\n    elif x in [7, 8, 9]:\n        season = 'Third'\n    return season\ndata['Quarter'] = data['Month'].apply(month_quarter_Conversion)\ndata.tail(3)","284f43c2":"fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(20,5))\nacc_level = \"Accident Level\"\ndata[acc_level].reset_index().groupby(acc_level).count().sort_values(by= \n       \"index\").plot(kind=\"barh\", legend=False, \n        ax=ax[0]).grid(axis='x')\n\npot_acc_level = \"Potential Accident Level\"\ndata[pot_acc_level].reset_index().groupby(pot_acc_level).count().sort_values(by= \n       \"index\").plot(kind=\"barh\", legend=False, \n        ax=ax[1]).grid(axis='x')\n\n\nplt.show()","ee7190b7":"fig = px.pie(data, names='Country', template='seaborn')\nfig.update_traces(rotation=90, pull=[0.2,0.03,0.1,0.03,0.1], textinfo=\"percent+label\", showlegend=False)\nfig.show()","08c85cd0":"fig = px.pie(data, names='Industry Sector', template='seaborn')\nfig.update_traces(rotation=90, pull=[0.2,0.03,0.1,0.03,0.1], textinfo=\"percent+label\", showlegend=False)\nfig.show()","06fc4084":"fig = px.pie(data, names='Employee type', template='seaborn')\nfig.update_traces(rotation=90, pull=[0.2,0.03,0.1,0.03,0.1], textinfo=\"percent+label\", showlegend=False)\nfig.show()","e6050135":"plt.figure(figsize=(20,5))\ndescending_order = data['Critical Risk'].value_counts().sort_values(ascending=False).index\nsns.countplot(x=data['Critical Risk'],order=descending_order)\nplt.xticks(rotation = 'vertical')","130f3b8f":"fig = px.pie(data, names='Quarter', template='seaborn')\nfig.update_traces(rotation=90, pull=[0.2,0.03,0.1,0.03,0.1], textinfo=\"percent+label\", showlegend=False)\nfig.show()","6468f4ee":"fig = px.pie(data, names='Local', template='seaborn')\nfig.update_traces(rotation=90, pull=[0.2,0.03,0.1,0.03,0.1], textinfo=\"percent+label\", showlegend=False)\nfig.show()","3aa7a2a1":"# Helper function for relation between Accident Level\/Potential Accident levels\n# and other labels\ndef target_count(df, col1):\n    fig = plt.figure(figsize = (15, 7.2))\n    ax = fig.add_subplot(121)\n    sns.countplot(x = col1, data = df, ax = ax, orient = 'v',\n                  hue = 'Accident Level').set_title(col1.capitalize() +' count plot by Accident Level', \n                                                                      fontsize = 13)\n    plt.legend(labels = df['Accident Level'].unique())\n    plt.xticks(rotation = 90)\n    \n    ax = fig.add_subplot(122)\n    sns.countplot(x = col1, data = df, ax = ax, orient = 'v', \n                  hue = 'Potential Accident Level').set_title(col1.capitalize() +' count plot by Potential Accident Level', \n                                                                      fontsize = 13)\n    plt.legend(labels = df['Potential Accident Level'].unique())\n    plt.xticks(rotation = 90)\n    return plt.show()","744123cd":"target_count(data, 'Gender')","2615beb1":"target_count(data, 'Employee type')","f830cfc8":"target_count(data, 'Industry Sector')","752fae88":"target_count(data, 'Country')","6d1fd2cb":"target_count(data, 'Month')","24bba16f":"target_count(data, 'Quarter')","cbbe556c":"target_count(data, 'Year')","2a349fcd":"target_count(data, 'Weekday')","de81e66d":"sns.countplot(x=\"Country\", data=data, hue=\"Employee type\")","3f8ff4c0":"sns.countplot(x=\"Country\", data=data, hue=\"Industry Sector\")","fce9c37e":"sns.countplot(x=\"Country\", data=data,hue=\"Gender\")","a61d0a12":"sns.countplot(x=\"Employee type\", data=data,hue=\"Gender\")","323e3fff":"fig,axs = plt.subplots(nrows=2,ncols=2,figsize=(15,10))\nsns.countplot(x=data['Year'],hue='Industry Sector',data=data,ax=axs[0][0])\nsns.countplot(x=data['Year'],hue='Accident Level',data=data,ax=axs[0][1])\nsns.countplot(x=data['Industry Sector'],hue='Accident Level',data=data,ax=axs[1][0])\nsns.countplot(x=data['Accident Level'],hue='Industry Sector',data=data,ax=axs[1][1])\n","5efa369f":"data['Description'].str.len().hist()","8350a52d":"data['Description'].str.split().map(lambda x: len(x)).hist()","e9a0677d":"data['Description'].str.split().apply(lambda x : [len(i) for i in x]). \\\n   map(lambda x: np.mean(x)).hist()","966e3255":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom nltk.corpus import stopwords\n\ndef plot_top_stopwords_barchart(text):\n    stop=set(stopwords.words('english'))\n    \n    new= text.str.split()\n    new=new.values.tolist()\n    corpus=[word for i in new for word in i]\n    from collections import defaultdict\n    dic=defaultdict(int)\n    for word in corpus:\n        if word in stop:\n            dic[word]+=1\n            \n    top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    x,y=zip(*top)\n    plt.bar(x,y)\n    \nplot_top_stopwords_barchart(data['Description'])    ","6a1bc875":"from collections import  Counter\n\ndef plot_top_non_stopwords_barchart(text):\n    stop=set(stopwords.words('english'))\n    \n    new= text.str.split()\n    new=new.values.tolist()\n    corpus=[word for i in new for word in i]\n\n    counter=Counter(corpus)\n    most=counter.most_common()\n    x, y=[], []\n    for word,count in most[:40]:\n        if (word not in stop):\n            x.append(word)\n            y.append(count)\n            \n    sns.barplot(x=y,y=x)\n\nplot_top_non_stopwords_barchart(data['Description'])    ","a3bd479a":"from sklearn.feature_extraction.text import CountVectorizer\nfrom collections import  Counter\n\ndef plot_top_ngrams_barchart(text, n=2):\n    stop=set(stopwords.words('english'))\n\n    new= text.str.split()\n    new=new.values.tolist()\n    corpus=[word for i in new for word in i]\n\n    def _get_top_ngram(corpus, n=None):\n        vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n        bag_of_words = vec.transform(corpus)\n        sum_words = bag_of_words.sum(axis=0) \n        words_freq = [(word, sum_words[0, idx]) \n                      for word, idx in vec.vocabulary_.items()]\n        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n        return words_freq[:10]\n\n    top_n_bigrams=_get_top_ngram(text,n)[:10]\n    x,y=map(list,zip(*top_n_bigrams))\n    sns.barplot(x=y,y=x)\n    \nplot_top_ngrams_barchart(data['Description'],2)","f7cbb416":"plot_top_ngrams_barchart(data['Description'],3)","054c91c3":"plot_top_ngrams_barchart(data['Description'],4)","420357f1":"from nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud, STOPWORDS\nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')\n\ndef plot_wordcloud(text):\n    nltk.download('stopwords')\n    stop=set(stopwords.words('english'))\n\n    def _preprocess_text(text):\n        corpus=[]\n        stem=PorterStemmer()\n        lem=WordNetLemmatizer()\n        for news in text:\n            words=[w for w in word_tokenize(news) if (w not in stop)]\n\n            words=[lem.lemmatize(w) for w in words if len(w)>2]\n\n            corpus.append(words)\n        return corpus\n    \n    corpus=_preprocess_text(text)\n    \n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=set(STOPWORDS),\n        max_words=100,\n        max_font_size=30, \n        scale=3,\n        random_state=1)\n    \n    wordcloud=wordcloud.generate(str(corpus))\n\n    fig = plt.figure(1, figsize=(12, 12))\n    plt.axis('off')\n \n    plt.imshow(wordcloud)\n    plt.show()\n   \nplot_wordcloud(data['Description'])","79fd31d9":"### Analysis of Year wrt Accident\/ Potential Accident Level","ed21635a":"### Analysis of Gender wrt Accident\/ Potential Accident Level","056e4241":"### Top non-stop words plot","ac2a091f":"### Rename columns","636f4350":"### Risk factor analysis","56deb6f1":"### Analysis of Weekday wrt Accident\/ Potential Accident Level","20749ce8":"### Analyze Country and Industry Sector","28817b8f":"### Analysis of Month wrt Accident\/ Potential Accident Level","fb2c3ff9":"# Exploratory Data Analysis","3049a059":"### Analyze Country and Employee Type","f8598523":"## Analyzing N-Grams","31436859":"### Mostly affected gender","ae67e03a":"### Average word length","74e0e1fa":"### Analyze Country and Gender","a79045a0":"### Analysis of Quarter wrt Accident\/ Potential Accident Level","b553c5bd":"### Mostly affected sector","8de34d8b":"### Check for null values","b84ec5cc":"## Multivariate Analysis","a2de4fd7":"## Univariate Analysis","50ac30af":"The dataset is imbalanced.","a8d3fe33":"## Analyzing text statistics","221fedf9":"### Number of characters present in each sentence.","e4b1b352":"### Analyze Employee Type and Gender","a461a6a3":"### Stop word analysis","832d371e":"### Display datatypes","d7744080":"### Mostly affected quarter","c422fcbe":"### Analysis of Country wrt Accident\/ Potential Accident Level","2562d1f8":"### Display columns","22c06b8e":"### Replace categorical values","e75fd4de":"## Import Dataset and basic info","b9a91a93":"## Data Preparation","f3617c91":"### Display count of unique values per column","c268b9b2":"### Analysis of Industry Sector wrt Accident\/ Potential Accident Level","cc159b7a":"### Which country is most affected??","8755c02b":"### Number of words appearing in each description","56deb9cd":"### Analysis of Employee type wrt Accident\/ Potential Accident Level","c53a663c":"## Import packages","dc33dae7":"### Mostly affected local"}}