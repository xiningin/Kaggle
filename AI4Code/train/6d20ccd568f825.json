{"cell_type":{"edd247a0":"code","446e0399":"code","25baa8f8":"code","ba42dc9d":"code","baddfcdd":"code","bb643b6f":"code","2007fe72":"code","d8b35d43":"markdown","9f7c391a":"markdown","0dee7029":"markdown","68fb2659":"markdown","d72255da":"markdown","fbe8118a":"markdown","2e119597":"markdown"},"source":{"edd247a0":"%%HTML\n<style type=\"text\/css\">\n\ndiv.h1 {\n    font-size: 32px; \n    margin-bottom:2px;\n    background-color: steelblue; \n    color: white; \n    text-align: center;\n}\ndiv.h2 {\n    background-color: steelblue; \n    color: white; \n    padding: 8px; \n    padding-right: 300px; \n    font-size: 24px; \n    max-width: 1500px; \n    margin-top: 50px;\n    margin-bottom:4px;\n    \n}\ndiv.h3 {\n    color: steelblue; \n    font-size: 20px; \n    margin-top: 4px; \n    margin-bottom:8px;\n}\ndiv.h4 {\n    font-size: 15px; \n    margin-top: 20px; \n    margin-bottom: 8px;\n}\n\n<\/style>","446e0399":"# Familiar imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n # For ordinal encoding categorical variables, splitting data\nfrom sklearn.preprocessing import OrdinalEncoder\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nprint(\"loaded libraries\")","25baa8f8":"# Load the training data\ntrain = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/sample_submission.csv\")\n# Preview the data\ntrain.head()","ba42dc9d":"#creating a list of categorical variables\n#cat_col=train.select_dtypes(include='object').columns.to_list()\ncat_col = [col for col in train.columns if 'cat' in col]\n\ncat_col","baddfcdd":"#get only usefulfeatures\nuseful_features=[col for col in train.columns if col not in ['id','target','kfold']]\n\n# List of categorical columns\n\ntest=test[useful_features]\n\n\n# for col in cat_col:\n#     train[f\"cont_{col}\"] = train.groupby(col)[col].transform(\"count\")\n#     test[f\"cont_{col}\"] = test.groupby(col)[col].transform(\"count\")\n\n    \n# useful_features = [c for c in train.columns if c not in (\"id\", \"target\", \"kfold\")]\n# cat_col = [col for col in useful_features if col.startswith(\"cat\")]","bb643b6f":"\nfinal_pred=[] #save your predictions in this list\nfor fold in range(5):\n    \n    xtrain=train[train.kfold!=fold].reset_index(drop=True) # get kfold which is not equal to fold\n    xvalid=train[train.kfold==fold].reset_index(drop=True) # K is equal to fold\n    \n    ytrain=xtrain.target\n    yvalid=xvalid.target\n    xtest= test.copy()\n \n    xtrain=xtrain[useful_features]\n    xvalid=xvalid[useful_features]\n\n    ordinal_encoder = OrdinalEncoder()\n    xtrain[cat_col] = ordinal_encoder.fit_transform(xtrain[cat_col])\n    xvalid[cat_col] = ordinal_encoder.transform(xvalid[cat_col])\n    xtest[cat_col] = ordinal_encoder.transform(xtest[cat_col])\n    \n    model = XGBRegressor(random_state=42, \n                         n_jobs=4,\n                         n_estimators= 6000,\n                         tree_method='gpu_hist',\n                         learning_rate= 0.08,\n                         subsample= 0.9,\n                         max_depth= 3,\n                         colsample_bytree= 0.5,\n                         reg_alpha = 15,eval_metric='rmse')\n    model.fit(xtrain, ytrain)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_pred.append(test_preds)\n    print(fold, mean_squared_error(yvalid, preds_valid, squared=False))\n    \n","2007fe72":"\n#get the mean of all the predictions.\npreds = np.mean(np.column_stack(final_pred), axis=1)\nsample_submission.target = preds\nsample_submission.to_csv(\"submission.csv\", index=False)","d8b35d43":"<div class=\"h1\">Table of Contents<\/div>\n\n  - <a href='#il'>Import Libraries<\/a>\n  - <a href='#rd'>Read and Understand Data<\/a>\n  - <a href='#mb'>Model Building <\/a>\n ","9f7c391a":"<a id='xg'><\/a>\n<div class=\"h3\">Understanding Hyperparameters for XGboost<\/div>\n\n- **njobs**\n    - To make use of all cpu\n- **tree_method='gpu_hist'**\n    - To use Gpu and runs faster\n- **n_estimators**\n    - The number of sequential trees to be modeled.\n    - Though XGB is fairly robust at higher number of trees but it can still overfit at a         point. Hence, this should be tuned using CV for a particular learning rate.\n- **learning_rate**\n    - This determines the impact of each tree on the final outcome  XGB works by starting with an initial estimate which is updated using the output of each tree. The learning parameter controls the magnitude of this change in the estimates.\n    - Lower values are generally preferred as they make the model robust to the specific characteristics of tree and thus allowing it to generalize well.\n    - Lower values would require higher number of trees to model all the relations and will be computationally expensive.\n\n- **colsample_bytree**\n    -  Denotes the fraction of columns to be randomly  samples for each tree.\n    -  Typical values: 0.5-1\n- **reg_alpha**\n    - L1 regularization term on weight (analogous to Lasso regression)\n    - Can be used in case of very high dimensionality so that the algorithm runs faster           when implemented\n\n- **subsample**\n    -  Denotes the fraction of observations to be randomly samples for each tree.\n    - Lower values make the algorithm more conservative and prevents overfitting but too        small values might lead to under-fitting.\n    - Typical values: 0.5-1\n\n- **max_depth**\n     - Used to control over-fitting as higher depth will allow model to learn relations          very  specific to a particular sample.\n    - Should be tuned using CV.\n    - Typical values: 3-10\n- **gamma**\n    - A node is split only when the resulting split gives a positive reduction in the loss       function. \n    - Gamma specifies the minimum loss reduction required to make a split.\n    - Makes the algorithm conservative. The values can vary depending on the loss function      and should be tuned.\n    \n- **eval_metric**\n    - The metric to be used for validation data.\n    - The default values are rmse for regression and error for classification.\n    - Typical values are:\n        - rmse \u2013 root mean square error\n        - mae \u2013 mean absolute error\n        - logloss \u2013 negative log-likelihood\n        - error \u2013 Binary classification error rate (0.5 threshold)\n        - merror \u2013 Multiclass classification error rate\n        - mlogloss \u2013 Multiclass logloss\n        - auc: Area under the curve","0dee7029":"<a id='il'><\/a>\n<div class=\"h3\">Import Libraries<\/div>","68fb2659":"This notebook is created as part  of 30 day ML challenge. This book is divided in two part EDA & kfold(https:\/\/www.kaggle.com\/yogidsba\/eda-kfold-30-day-ml) and Modelbuilding\n\n**About the data**\nThe dataset is used for this competition is synthetic, but based on a real dataset. The original dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features.\n\n\n**Evaluation**\n\nSubmissions are scored on the root mean squared error.\n\n\n**Special mention**\nI learned about kfold and finding mean of prediction from Abhishek Thakur YoutubeChannel. For that code you can refer his notebook or watch youtube channel.Thank you Abhisek","d72255da":"\n<div class=\"h1\">30 Days of ML- Competition<\/div>\n\n","fbe8118a":"<a id='rd'><\/a>\n<div class=\"h3\">Read and Understand Data<\/div>\n\nNext, we'll load the train_kfold csv that was created before and has additional columns containing kfold and test data.  \n","2e119597":"There are 10 categorical variables which we will have to encode "}}