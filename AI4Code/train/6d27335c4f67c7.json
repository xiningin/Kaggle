{"cell_type":{"34748803":"code","16113db6":"code","a9e66fb4":"code","9c30db4c":"code","bdd0eadf":"code","77496d14":"code","e4869bf0":"code","6bbbdae4":"code","a54819de":"code","c746d645":"code","921ce5a5":"code","a4e43980":"markdown","f0cb1da0":"markdown"},"source":{"34748803":"import time\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_diabetes  \ndata_diabetes = load_diabetes()    \ndata_diabetes","16113db6":"data = data_diabetes['data']\ntarget = data_diabetes['target']\nfeature_names = data_diabetes['feature_names']\n#\u4e09\u4e2a\u6570\u636e\u90fd\u662fnumpy\u7684\u4e00\u7ef4\u6570\u636e\u5f62\u5f0f\uff0c\u5c06\u5176\u7ec4\u5408\u6210dataframe\uff0c\u53ef\u4ee5\u66f4\u76f4\u89c2\u5730\u89c2\u5bdf\u6570\u636e\ndf = pd.DataFrame(data,columns = feature_names)\ndf = df.iloc[:,2:9]\ndf.head()","a9e66fb4":"df.info()","9c30db4c":"## \u62bd\u53d6\u8bad\u7ec3\u96c6\u5408\u6d4b\u8bd5\u96c6\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df,target,test_size=0.3, random_state=1) ","bdd0eadf":"## \u6a21\u578b\u9009\u62e9\uff1a\u786e\u5b9a\u03bb\u7684\u503c\nfrom sklearn.linear_model import LassoCV, LassoLarsCV, LassoLarsIC\n\n# 1\u3001Information-criteria \nmodel_bic = LassoLarsIC(criterion=\"bic\", normalize=False)\nt1 = time.time()\nmodel_bic.fit(X_train, y_train)\nt_bic = time.time() - t1\nalpha_bic_ = model_bic.alpha_\nprint(alpha_bic_)  # \u8f93\u51faBIC\u6700\u5c0f\u65f6\u7684alpha\u503c\n\nmodel_aic = LassoLarsIC(criterion=\"aic\", normalize=False)\nmodel_aic.fit(X_train, y_train)\nalpha_aic_ = model_aic.alpha_\nprint(alpha_aic_)  # \u8f93\u51faAIC\u6700\u5c0f\u65f6\u7684alpha\u503c\n\n# Display results\nEPSILON = 1e-4 # This is to avoid division by zero while doing np.log10\ndef plot_ic_criterion(model, name, color):\n    criterion_ = model.criterion_\n    plt.semilogx(\n        model.alphas_ + EPSILON,\n        criterion_,\n        \"--\",\n        color=color,\n        linewidth=3,\n        label=\"%s criterion\" % name,\n    )\n    plt.axvline(\n        model.alpha_ + EPSILON,\n        color=color,\n        linewidth=3,\n        label=\"alpha: %s estimate\" % name,\n    )\n    plt.xlabel(r\"$\\alpha$\")\n    plt.ylabel(\"criterion\")\n\n\nplt.figure()\nplot_ic_criterion(model_aic, \"AIC\", \"b\")\nplot_ic_criterion(model_bic, \"BIC\", \"r\")\nplt.legend()\nplt.title(\"Information-criterion for model selection (training time %.3fs)\" % t_bic)","77496d14":"## LassoCV: coordinate descent\n\n# Lasso\u56de\u5f52\u6a21\u578b\u7684\u4ea4\u53c9\u9a8c\u8bc1\nt1 = time.time()\nmodel = LassoCV(cv=20).fit(X_train, y_train)\nt_lasso_cv = time.time() - t1\n\n# \u8f93\u51fa\u6700\u4f73\u7684alpha\u503c\nlasso_best_alpha1 = model.alpha_ \nprint(lasso_best_alpha1)\n\n# Compute paths\nprint(\"Computing regularization path using the coordinate descent lasso...\")\n\n# Display results\nplt.figure()\nymin, ymax = 2300, 3800\nplt.semilogx(model.alphas_ + EPSILON, model.mse_path_, \":\")\nplt.plot(\n    model.alphas_ + EPSILON,\n    model.mse_path_.mean(axis=-1),\n    \"k\",\n    label=\"Average across the folds\",\n    linewidth=2,\n)\nplt.axvline(\n    model.alpha_ + EPSILON, linestyle=\"--\", color=\"k\", label=\"alpha: CV estimate\"\n)\n\nplt.legend()\n\nplt.xlabel(r\"$\\alpha$\")\nplt.ylabel(\"Mean square error\")\nplt.title(\n    \"Mean square error on each fold: coordinate descent (train time: %.2fs)\"\n    % t_lasso_cv\n)\nplt.axis(\"tight\")\nplt.ylim(ymin, ymax)","e4869bf0":"## LassoLarsCV: least angle regression\n\n# Lasso\u56de\u5f52\u6a21\u578b\u7684\u4ea4\u53c9\u9a8c\u8bc1\nt1 = time.time()\nmodel = LassoLarsCV(cv=20, normalize=False).fit(X_train, y_train)\nt_lasso_lars_cv = time.time() - t1\n\n# \u8f93\u51fa\u6700\u4f73\u7684alpha\u503c\nlasso_best_alpha2 = model.alpha_ \nprint(lasso_best_alpha2)\n\n# Compute paths\nprint(\"Computing regularization path using the Lars lasso...\")\n\n# Display results\nplt.figure()\nplt.semilogx(model.cv_alphas_ + EPSILON, model.mse_path_, \":\")\nplt.semilogx(\n    model.cv_alphas_ + EPSILON,\n    model.mse_path_.mean(axis=-1),\n    \"k\",\n    label=\"Average across the folds\",\n    linewidth=2,\n)\nplt.axvline(model.alpha_, linestyle=\"--\", color=\"k\", label=\"alpha CV\")\nplt.legend()\n\nplt.xlabel(r\"$\\alpha$\")\nplt.ylabel(\"Mean square error\")\nplt.title(\"Mean square error on each fold: Lars (train time: %.2fs)\" % t_lasso_lars_cv)\nplt.axis(\"tight\")\nplt.ylim(ymin, ymax)\n\nplt.show()","6bbbdae4":"# Lasso\u56de\u5f52\u6a21\u578b1\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_error,r2_score,explained_variance_score\n\n# \u57fa\u4e8e\u4fe1\u606f\u51c6\u5219\u5f97\u5230\u7684\u6700\u4f73alpha\u503c\u5efa\u6a21\nlasso = Lasso(alpha=alpha_aic_ ,normalize=True, max_iter=10000)\nlasso.fit(X_train, y_train)\n\n# \u8fd4\u56deLasso\u56de\u5f52\u7684\u7cfb\u6570\nres = pd.Series(index=['Intercept'] + X_train.columns.tolist(), data=[lasso.intercept_] + lasso.coef_.tolist())\nprint(res)\n\n# \u6a21\u578b\u9884\u6d4b\ny_lasso_predict = lasso.predict(X_test)\n\n# \u6a21\u578b\u6548\u679c\u8bc4\u4ef7\nRMSE = np.sqrt(mean_squared_error(y_test,y_lasso_predict))  \nprint(\"RMSE on test data : %f\" % RMSE)\nr2_score_lasso = r2_score(y_test, y_lasso_predict)\nprint(\"r^2 on test data : %f\" % r2_score_lasso)\nexplained_variance_score_lasso = explained_variance_score(y_test,y_lasso_predict)\nprint(\"explained variance score on test data : %f\" %explained_variance_score_lasso) ","a54819de":"# Lasso\u56de\u5f52\u6a21\u578b2\n# \u57fa\u4e8e\u4ea4\u53c9\u9a8c\u8bc1\u5f97\u5230\u7684\u6700\u4f73alpha\u503c\u5efa\u6a21\nlasso = Lasso(alpha=lasso_best_alpha2,normalize=True, max_iter=10000)\nlasso.fit(X_train, y_train)\n\n# \u8fd4\u56deLasso\u56de\u5f52\u7684\u7cfb\u6570\nres = pd.Series(index=['Intercept'] + X_train.columns.tolist(), data=[lasso.intercept_] + lasso.coef_.tolist())\nprint(res)\n\n# \u6a21\u578b\u9884\u6d4b\ny_lasso_predict = lasso.predict(X_test)\n\n# \u6a21\u578b\u6548\u679c\u8bc4\u4ef7\nRMSE = np.sqrt(mean_squared_error(y_test,y_lasso_predict))  \nprint(\"RMSE on test data : %f\" % RMSE)\nr2_score_lasso = r2_score(y_test, y_lasso_predict)\nprint(\"r^2 on test data : %f\" % r2_score_lasso)\nexplained_variance_score_lasso = explained_variance_score(y_test,y_lasso_predict)\nprint(\"explained variance score on test data : %f\" %explained_variance_score_lasso) ","c746d645":"from sklearn.linear_model import Ridge,RidgeCV\nalphas=np.logspace(-10, 10, 100)\nridge = RidgeCV(alphas)# \u901a\u8fc7RidgeCV\u53ef\u4ee5\u8bbe\u7f6e\u591a\u4e2a\u53c2\u6570\u503c\uff0c\u7b97\u6cd5\u4f7f\u7528\u4ea4\u53c9\u9a8c\u8bc1\u83b7\u53d6\u6700\u4f73\u53c2\u6570\u503c\nridge.fit(X_train, y_train)   # \u7ebf\u6027\u56de\u5f52\u5efa\u6a21\nprint('\u56de\u5f52\u7cfb\u6570:\\n',ridge.coef_)\n#print('\u7ebf\u6027\u56de\u5f52\u6a21\u578b:\\n',model)\nprint('\u4ea4\u53c9\u9a8c\u8bc1\u6700\u4f73alpha\u503c\uff1a%.4f' % ridge.alpha_)  # \u53ea\u6709\u5728\u4f7f\u7528RidgeCV\u7b97\u6cd5\u65f6\u624d\u6709\u6548\n# \u4f7f\u7528\u6a21\u578b\u9884\u6d4b\ny_ridge_predict = ridge.predict(X_test)","921ce5a5":"# \u6a21\u578b\u6548\u679c\u8bc4\u4ef7\nRMSE = np.sqrt(mean_squared_error(y_test,y_ridge_predict))  \nprint(\"RMSE on test data : %f\" % RMSE)\nr2_score_ridge = r2_score(y_test, y_ridge_predict)\nprint(\"r^2 on test data : %f\" % r2_score_ridge)\nexplained_variance_score_ridge = explained_variance_score(y_test,y_ridge_predict)\nprint(\"explained variance score on test data : %f\" %explained_variance_score_ridge) ","a4e43980":"**\u8fd8\u8981\u5b8c\u5584\u5982\u4f55display**","f0cb1da0":"# Ridge Regression"}}