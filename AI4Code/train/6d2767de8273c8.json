{"cell_type":{"84b6d7bd":"code","b47d352d":"code","f9fb1b18":"code","9d16a3d5":"code","5d2c7cc1":"code","5f172c46":"code","55504c89":"code","ecd4dcf1":"code","dab9a8b7":"code","a96f2512":"code","041282f6":"code","72878290":"code","847cc985":"code","639e4179":"code","733648af":"code","12bf82ed":"code","c95d943c":"code","7da0d9da":"markdown","22739508":"markdown"},"source":{"84b6d7bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b47d352d":"df = pd.read_csv(\"..\/input\/movie_lines.tsv\", encoding='utf-8-sig',header = None)\ndf.head()","f9fb1b18":"lines = df[0].str.split('\\t')","9d16a3d5":"lines[0:5]","5d2c7cc1":"dialogues = list()\nfor line in lines:\n    dialogues.append(line[4])\n    \ndialogues[0:10]","5f172c46":"dialogue_path = \"..\/input\/movie_lines.tsv\"\nvocab_size = 5000\nembedding_dim = 500","55504c89":"from keras.preprocessing.text import Tokenizer\nfrom statistics import median\nkeras_tokenizer = Tokenizer(num_words=vocab_size, filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}\\t\\n')\nkeras_tokenizer.fit_on_texts(dialogues)","ecd4dcf1":"len(keras_tokenizer.word_index)","dab9a8b7":"text_sequences = keras_tokenizer.texts_to_sequences(dialogues)[:2000]","a96f2512":"max_seq_len = int(median(len(sequence) for sequence in text_sequences))\nprint(max_seq_len)","041282f6":"from keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras.layers import Input, Dense, RepeatVector, LSTM, Conv1D, Masking, Embedding\nfrom keras.layers.wrappers import TimeDistributed, Bidirectional\nfrom keras.models import Model\nfrom keras.preprocessing.sequence import pad_sequences","72878290":"x_train = pad_sequences(text_sequences, maxlen=max_seq_len, padding='post', truncating='post', value=0)","847cc985":"x_train_rev = list()\nfor x_vector in x_train:\n    x_rev_vector = list()\n    for index in x_vector:\n        char_vector = np.zeros(vocab_size)\n        char_vector[index] = 1\n        x_rev_vector.append(char_vector)\n    x_train_rev.append(np.asarray(x_rev_vector))\nx_train_rev = np.asarray(x_train_rev)","639e4179":"x_train_rev.shape","733648af":"def seq_2_seq_model():\n    inputs = Input(shape=x_train[0].shape, dtype='int32', name='input_layer')\n    embedding = Embedding(\n        input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True, input_length=max_seq_len)(inputs)\n    bi_lstm_1 = Bidirectional(LSTM(units=2048, name='bi_lstm_1'))(embedding)\n    repeat = RepeatVector(max_seq_len, name='repeat_vector')(bi_lstm_1)\n    bi_lstm_3 = Bidirectional(LSTM(units=2048, return_sequences=True, name='bi_lstm_3'))(repeat)\n    output = TimeDistributed(Dense(vocab_size, activation='softmax'))(bi_lstm_3)\n    \n    model = Model(inputs, output)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","12bf82ed":"seq_model = seq_2_seq_model()","c95d943c":"seq_model.fit(x_train, x_train_rev, batch_size=128, epochs=20, verbose=1)","7da0d9da":"**BUILD NN MODEL**","22739508":"**Padding**\n\nPad sequences which are less than max sequence length.\nTruncate sequences which are greater than max sequence length\n"}}