{"cell_type":{"7dfeaf21":"code","87d1313d":"code","bdf5ffb5":"code","a699e9e1":"code","1b805172":"code","fb2a556f":"code","34fcdfff":"code","5fab490b":"code","37ae0502":"code","96ef0288":"code","5dd6c076":"code","ff37292f":"code","226c9b64":"code","88a0b310":"code","82f26d92":"code","772f9f11":"code","8b67b3e0":"code","6e8e7622":"code","6de00863":"code","cce1aa11":"code","5af8ba4f":"code","8856db34":"code","5f80a8ff":"code","dfcca121":"code","f40227b2":"code","9d36d5ec":"code","65bc434d":"code","87985994":"code","a6acf36d":"code","385ae22d":"code","8b60de01":"code","2e1fd701":"code","943978f8":"code","19fa0747":"code","a6449843":"markdown","dc548173":"markdown","dfdbfa89":"markdown","a20bf50b":"markdown","c5988086":"markdown","9168e23f":"markdown","77f0b12a":"markdown","d78213f6":"markdown","904d2f17":"markdown","d14f9af0":"markdown","fea1344d":"markdown","3c879176":"markdown","30c265c6":"markdown"},"source":{"7dfeaf21":"# Importig standard Libraries \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport seaborn as sns\n\n# Train \/ Test split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\n#!pip install wordcloud\nfrom wordcloud import WordCloud,STOPWORDS\n\n# Import the natural language toolkit library \n#!pip install nltk\nimport nltk\n#nltk.download(\"punkt\")\n#nltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\n\n# Text tokenization\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Importing Metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\n# Loading the model and Ploting its architecture.\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.utils import plot_model\n\n#Ploting the confusion matrix\nfrom sklearn.metrics import confusion_matrix","87d1313d":"%matplotlib inline\n\n# read the csv files\nd_true = pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/True.csv\")\nd_fake = pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv\")","bdf5ffb5":"d_true.head()","a699e9e1":"d_true.describe()","1b805172":"d_fake.head()","fb2a556f":"d_fake.describe()","34fcdfff":"# add a column with the name label that has value 1 for reliable news and 0 for fake news\n\nd_true[\"label\"] = 1\nd_fake[\"label\"] = 0","5fab490b":"# plot the true dataset's wordcloud using top 500 words\n\nplt.figure(figsize = (15,15))\nwc = WordCloud(max_words = 500 , width = 1000 , height = 500, background_color=\"rgba(255, 255, 255, 0)\", stopwords = STOPWORDS).generate(\" \".join(d_true.text))\nplt.imshow(wc , interpolation = 'bilinear')\nplt.axis('off')\n#plt.savefig(\"..\/True dataset's world cloud.png\", bbox_inches='tight')","37ae0502":"# plot the true dataset's wordcloud using top 500 words\n\nplt.figure(figsize = (15,15))\nwc = WordCloud(max_words = 500 , width = 1000 , height = 500 , background_color=\"rgba(255, 255, 255, 0)\", stopwords = STOPWORDS).generate(\" \".join(d_fake.text))\nplt.imshow(wc)\nplt.axis('off')\n#plt.savefig(\"..\/Fake dataset's world cloud.png\", bbox_inches='tight')","96ef0288":"# Concatenate the title with the article text\n\nd_true['text'] = d_true['title'] +\" \"+ d_true['text']\nd_fake['text'] = d_fake['title'] +\" \"+ d_fake['text']","5dd6c076":"# drop the unnecessary columns\n\nd_true.drop([\"title\", \"subject\", \"date\"], axis=1, inplace= True)\nd_fake.drop([\"title\", \"subject\", \"date\"], axis=1, inplace= True)","ff37292f":"# concatenate the two dataframes and shuffle the result\n\ndata = pd.concat([d_true, d_fake], axis=0, ignore_index = True)\ndata = shuffle(data)\n\ndata = data.reset_index(drop= True)\ndata.head()","226c9b64":"# check if there is any null values\n\ndata.isnull().sum()","88a0b310":"data.shape","82f26d92":"#check the data distribution\n\ndata.label.value_counts().plot(kind='bar', color=['b', 'g'])","772f9f11":"#check the number of articles in each type (fake or true)\n# 0 for fake and 1 for true\n\ndata.label.value_counts()","8b67b3e0":"# plotting the number of words in texts\n\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,10))\ntext_len=data[data['label']==0]['text'].str.split().map(lambda x: len(x))\nax1.hist(text_len,color='SkyBlue')\nax1.set_title('Fake news texts')\n\ntext_len=data[data['label']==1]['text'].str.split().map(lambda x: len(x))\nax2.hist(text_len,color='PeachPuff')\nax2.set_title('Real news texts')\nfig.suptitle('Number of Words in texts')\nplt.show()","6e8e7622":"# defining a function that takes care of cleaning stopwords and punctuations using nltk library.\nstop_words = set(stopwords.words('english'))\ndef process(text):\n    \"\"\"Converting the texts into lowercase characters and removing punctuations and stopwords using the nltk library.\"\"\"\n    text = text.lower()\n    words = nltk.word_tokenize(text)\n    new_words= [word for word in words if word.isalnum() and word not in stop_words]\n    text = \" \".join(new_words)\n    return text","6de00863":"# cleaning the text and seperating the target(label) variable and the feature(text) variable.\ndata = shuffle(data)\n\ndata['text'] = data['text'].apply(process)\nX = data['text'].to_frame()\nY = data['label'].to_frame()","cce1aa11":"# Ploting the number of words after removing stopwords\ntext_len=X['text'].str.split().map(lambda x: len(x))\nplt.hist(text_len,color='SkyBlue')\nplt.title('number of words')","5af8ba4f":"# Calculating the average number of words in the texts in order to use it as the length of sequences.\n# Calculting the number of unique words in order to pass it as argument to the tensorflow tokenizer.\n\nAvg_len = text_len.mean()\nAvg_len = round(Avg_len)\nlst = []\nfor i in X['text']:\n    tmp = i.split()\n    lst.extend(tmp)\nlst = set(lst)\nVocab_size = len(lst)\nprint(\"the average number of words in the texts is : \", Avg_len)\nprint(\"the texts contains\", Vocab_size, \"unique words\")","8856db34":"tokenizer = Tokenizer(num_words=Vocab_size)\ntokenizer.fit_on_texts(X['text'])\nsequences = tokenizer.texts_to_sequences(X['text'])\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","5f80a8ff":"# padding the sequences created by the tokenizer using the the average number of words in the texts + 2 = 235 as the maxlen.\n# also setting up the truncation and the padding to be at the end of the sequence.\n\ndata = pad_sequences(sequences, maxlen=Avg_len+2, padding='post', truncating='post')","dfcca121":"X_train, X_test, Y_train, Y_test = train_test_split(data, Y, test_size=0.25, random_state=25)\nprint(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)","f40227b2":"# Import tensor representations for words\n# GloVe is an unsupervised learning algorithm for obtaining vector representations for words\n\nembeddings_index = {};\nwith open(\"\/kaggle\/input\/glove6b100dtxt\/glove.6B.100d.txt\", encoding=\"utf8\") as f:\n    for line in f:\n        values = line.split();\n        word = values[0];\n        coefs = np.asarray(values[1:], dtype='float32');\n        embeddings_index[word] = coefs;\nprint(len(coefs))\n\nembeddings_matrix = np.zeros((Vocab_size+1, 100));\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word);\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector;\n","9d36d5ec":"print(embeddings_matrix.shape)","65bc434d":"# Building the architecture of the model\n     \nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(len(word_index)+1, 100, weights=[embeddings_matrix], trainable = False),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.summary()","87985994":"# using an early stop callback to stop the trainning if the loss function cannot be improved anymore.\n\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])\n\nhistory = model.fit(X_train, Y_train, epochs=10, validation_split=0.1, batch_size=32, shuffle=True, callbacks=[early_stop])","a6acf36d":"model.save(\"\/kaggle\/output\/model1.h5\")","385ae22d":"# Plotting the architecture of the model.\n\nplot_model(model, to_file='\/kaggle\/output\/model_schema.png', show_shapes=True)","8b60de01":"# evaluating the model with the evaluate method.\nmodel.evaluate(X_test, Y_test)","2e1fd701":"# predict the labels of test set.\nY_pred = (model.predict(X_test) >= 0.5).astype(\"int\")","943978f8":"# Evaluating the model using sklearn metrics.\n\naccuracy = accuracy_score(list(Y_test['label']), Y_pred)\nprecision = precision_score(Y_test, Y_pred)\nrecall = recall_score(Y_test, Y_pred)\n\nprint(\"Model Accuracy : \", accuracy)\nprint('Precision on testing set:', precision)\nprint('Recall on testing set:', recall)","19fa0747":"# Ploting the confusion matrix using the seaborn library\n\ngraph = confusion_matrix(Y_test, Y_pred)\nplt.figure(figsize=(12, 10))\nax= plt.subplot()\nsns.heatmap(graph, annot=True, ax = ax)\nax.xaxis.set_ticklabels(['Fake','True'], size=15)\nax.yaxis.set_ticklabels(['Fake','True'], size=15)\nplt.savefig(\"..\/Confusion_matrix.png\", bbox_inches='tight')","a6449843":"**True Dataset's WordCloud**","dc548173":"### 2. TEXT TOKENIZATION","dfdbfa89":"### 6. Model training","a20bf50b":"### 3. Spliting the data into train \/ test sets","c5988086":"### 4. Using GloVe for Word Embeddings","9168e23f":"# Fake news detection","77f0b12a":"**Importing the ISOT dataset files**","d78213f6":"##### Save the model","904d2f17":"#### TEXT PROCESSING WITH THE NLTK LIBRARY","d14f9af0":"**fake Dataset's WordCloud**","fea1344d":"### 1. EDA & DATA PREPROCESSING","3c879176":"### 5. Building the architecture of the model","30c265c6":"### 7. Model Evaluation"}}