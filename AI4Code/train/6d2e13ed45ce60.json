{"cell_type":{"d663edc7":"code","c443e124":"code","549218f6":"code","1633c344":"code","3b973d10":"code","a0be9a00":"code","0e774801":"code","1d77a8bd":"code","21dd7ca7":"code","a5cbfc36":"code","4f973094":"code","beb44374":"code","7eda2f93":"code","4f4db406":"code","b8f14286":"code","3c466df1":"code","a75305a7":"code","379a462b":"code","d202ae49":"code","e8e1384c":"code","d8ebe9e7":"code","6bd3e1c8":"code","cf2adc6a":"code","78266d80":"code","838c6ac7":"code","d9c05a66":"code","feac864f":"code","a6f3b1f0":"code","fab2b9e4":"code","91ecbdf7":"code","3b122c82":"code","f87b34f7":"code","46b4b90f":"code","057f198c":"code","31e75dab":"code","eaa52ce2":"code","c755d541":"code","b0d6c32d":"code","43f71719":"code","a8bbdcda":"code","47181b23":"code","a9552f8a":"code","0dbdb4c5":"code","6f3bd239":"code","dfb386fa":"code","1094ae01":"markdown","c43ec103":"markdown"},"source":{"d663edc7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c443e124":"import pandas as pd","549218f6":"df = pd.read_csv('\/kaggle\/input\/stocknews\/Combined_News_DJIA.csv')","1633c344":"df.shape","3b973d10":"df.head()","a0be9a00":"df.isna().sum()","0e774801":"df[df.isna()['Top25']]","1d77a8bd":"df.fillna('market remains same', inplace = True)","21dd7ca7":"df.isna().sum()","a5cbfc36":"train_df = df[df['Date']<'2014-12-31']\ntest_df = df[df['Date']>'2014-12-31']","4f973094":"train_df.shape, test_df.shape","beb44374":"pip install contractions","7eda2f93":"import nltk\nimport spacy\nimport re\nfrom bs4 import BeautifulSoup\nimport requests\nimport unicodedata\nimport contractions\n\nnlp = spacy.load('en_core_web_sm')","4f4db406":"def remove_special_characters(text, remove_digits= True):\n    #text = text.replace('$', 'currency')\n    pattern = r'[^\\w]+' if not remove_digits else r'[^a-zA-Z]'\n    text = re.sub(pattern,\" \",text)\n    text = re.sub(r'\\s+',' ', text)\n    return text\n\ndef remove_accented_characters(text):\n    text =  unicodedata.normalize('NFKD',text).encode('ascii','ignore').decode('utf-8','ignore')\n    return text\n\ndef spacy_lemma(text):\n    text = nlp(text)\n    new_text = []\n    words = [word.lemma_ for word in text]\n    for small in words:\n        if small == '-PRON-':\n            pass\n        else:\n            new_text.append(small)\n\n    return ' '.join(new_text)\n\ndef contractions_text(text):\n    return contractions.fix(text)\n\ndef stop_words_removal(text, is_lower_case = False, stopwords = None):\n    if stopwords == None:\n        stopwords = nlp.Defaults.stop_words\n    \n    if not is_lower_case:\n        text = text.lower()\n    tokens = nltk.word_tokenize(text)\n    new_token = []\n    for i in tokens:\n        if len(i)<=1:\n            pass\n        else:\n            new_token.append(i)\n    \n    removed_text = [word for word in new_token if word not in stopwords]\n    \n    return ' '.join(removed_text)\n\ndef join_news(text):\n    full_text = []\n    for ind in range(len(text)):\n        combine_text = []\n        for col in range(2,len(text.columns[2:])+2):\n            combine_text.append(text.iloc[ind,col])\n        full_text.append(' '.join(combine_text))\n    return full_text","b8f14286":"train_data = join_news(train_df)\ntest_data = join_news(test_df)","3c466df1":"train_data[0]","a75305a7":"test_data[0]","379a462b":"import tqdm","d202ae49":"def preprocessor_engine(text):\n    corpus =[]\n    for sent in tqdm.tqdm(text):\n        sent = remove_accented_characters(sent)\n        sent = contractions_text(sent)\n        sent = remove_special_characters(sent)\n        sent = spacy_lemma(sent)\n        sent = stop_words_removal(sent)\n        corpus.append(sent)\n    return corpus","e8e1384c":"train_data_pro = preprocessor_engine(train_data)","d8ebe9e7":"test_data_pro = preprocessor_engine(test_data)","6bd3e1c8":"train_data_pro[0]","cf2adc6a":"test_data_pro[0]","78266d80":"import tensorflow as tf","838c6ac7":"tokenzer = tf.keras.preprocessing.text.Tokenizer(oov_token = '<UNK>')\ntokenzer.fit_on_texts(train_data_pro)","d9c05a66":"#Converting into number token\ntrain_sequences = tokenzer.texts_to_sequences(train_data_pro)\ntest_sequences = tokenzer.texts_to_sequences(test_data_pro)","feac864f":"print(\"Vocabulary size ={}\".format(len(tokenzer.word_index)))\nprint(\"Number of Documents={}\".format(tokenzer.document_count))","a6f3b1f0":"pd.Series(train_data_pro).apply(lambda x : len(x.split())).max()","fab2b9e4":"MAX_SEQUENCE_LENGTH = 424\n\ntrain_pad_sequences = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen = MAX_SEQUENCE_LENGTH, padding='post')\ntest_pad_sequneces = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen = MAX_SEQUENCE_LENGTH, padding='post')","91ecbdf7":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, RNN, LSTM, GRU, Bidirectional, Embedding, Dropout\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom tensorflow.keras.callbacks import EarlyStopping","3b122c82":"y_train = train_df['Label']\ny_test = test_df['Label']","f87b34f7":"def metrics(y_true, y_pred):\n    print('Accuracy Score:', round(accuracy_score(y_true, y_pred),2))\n    print('\\nClassification Score:\\n', classification_report(y_true, y_pred))\n    print('\\nConfusion Matrix:\\n', confusion_matrix(y_true, y_pred))","46b4b90f":"def deep_model(layer_name, epochs=50):\n    SEED = 42\n    np.random.seed(SEED)\n    tf.random.set_seed(SEED)\n\n    EMBEDDING_DIM = 300 #Dimension for dense embedding for each token\n    VOCAB_SIZE = len(tokenzer.word_index)\n    model = Sequential()\n    model.add((Embedding(input_dim =VOCAB_SIZE+1,output_dim = EMBEDDING_DIM,input_length = MAX_SEQUENCE_LENGTH)))\n    model.add((layer_name(256)))\n    model.add((Dense(256,activation = 'relu')))\n    model.add(Dense(1,activation = 'sigmoid'))\n\n    model.compile(loss = 'binary_crossentropy',optimizer=\"adam\",metrics =['accuracy'])\n    model.summary()\n    fit_the_model(model, epochs=50)","057f198c":"def predictions(model):\n    train_pred = model.predict_classes(train_pad_sequences)\n    test_pred = model.predict_classes(test_pad_sequneces)\n    metrics(y_train, train_pred)\n    metrics(y_test, test_pred)\n\ndef fit_the_model(model, epochs=50):\n    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n    model.fit(train_pad_sequences, y_train, epochs=epochs, validation_data=(test_pad_sequneces, y_test), callbacks=[early_stop],verbose=0)\n    predictions(model)","31e75dab":"deep_model(LSTM)","eaa52ce2":"deep_model(GRU)","c755d541":"def stack_model(layer_name, epochs=50):\n    SEED = 42\n    np.random.seed(SEED)\n    tf.random.set_seed(SEED)\n\n    EMBEDDING_DIM = 300 #Dimension for dense embedding for each token\n    VOCAB_SIZE = len(tokenzer.word_index)\n    model = Sequential()\n    model.add((Embedding(input_dim =VOCAB_SIZE+1,output_dim = EMBEDDING_DIM,input_length = MAX_SEQUENCE_LENGTH)))\n    model.add((layer_name(256, return_sequences=True)))\n    model.add((layer_name(128, return_sequences=False)))\n    model.add((Dense(256,activation = 'relu')))\n    model.add(Dense(2,activation = 'softmax'))\n\n    model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),optimizer=\"adam\",metrics =['accuracy'])\n    model.summary()\n    fit_the_model(model, epochs=50)","b0d6c32d":"y_train.value_counts()","43f71719":"stack_model(LSTM)","a8bbdcda":"stack_model(GRU)","47181b23":"def bidirect_model(layer_name, epochs=50, dropout=False):\n    SEED = 42\n    np.random.seed(SEED)\n    tf.random.set_seed(SEED)\n\n    EMBEDDING_DIM = 300 #Dimension for dense embedding for each token\n    VOCAB_SIZE = len(tokenzer.word_index)\n    model = Sequential()\n    model.add((Embedding(input_dim =VOCAB_SIZE+1,output_dim = EMBEDDING_DIM,input_length = MAX_SEQUENCE_LENGTH)))\n    \n    model.add(Bidirectional(layer_name(256, return_sequences=True)))\n    if dropout:\n        model.add(Dropout(0.2))\n    model.add(Bidirectional(layer_name(128, return_sequences=False)))\n    if dropout:\n        model.add(Dropout(0.2))\n    model.add((Dense(256,activation = 'relu')))\n    model.add(Dense(2,activation = 'softmax'))\n\n    model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),optimizer=\"adam\",metrics =['accuracy'])\n    model.summary()\n    fit_the_model(model, epochs=50)","a9552f8a":"bidirect_model(LSTM)","0dbdb4c5":"#DropOut\nbidirect_model(LSTM, dropout=True)","6f3bd239":"bidirect_model(GRU)","dfb386fa":"#GRU\nbidirect_model(GRU, dropout=True)","1094ae01":"#### Tokenizer","c43ec103":"#### Modelling"}}