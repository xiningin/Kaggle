{"cell_type":{"7eacc984":"code","174b9eca":"code","6ff919af":"code","2babc794":"code","57e0760b":"code","6942204c":"code","ec192ccb":"code","392528bb":"code","abb8a711":"code","0a9c3c2f":"code","c60ed981":"code","8ee43fb3":"code","54210a92":"code","7ef23f6c":"code","fa0d2ab5":"code","75c0cef4":"code","96921007":"code","156f255f":"code","6883ca2a":"code","465448d6":"code","df951c6f":"code","c97ecedc":"code","8a8d626e":"code","3623f295":"code","fee5c580":"code","ac432e39":"code","6def8ef7":"code","9ff79be2":"code","31175078":"code","f71e0790":"code","ce97e75e":"code","28e1ef8b":"code","061c26de":"code","b545afb5":"code","3e423056":"code","58fc340a":"code","cc1e5fb2":"code","0e2fad87":"code","bcad3c80":"code","3a09ed1c":"code","8e606648":"code","9669e5b0":"code","68adebed":"code","81ad47bc":"code","a4741822":"code","d58cdae8":"code","d5e6011f":"code","6c584c7f":"code","d4235f61":"code","94fcf17b":"code","d72d4f7b":"code","1f917516":"code","655f8ab0":"code","39ee6da3":"code","ec251241":"code","8f01503d":"code","71d5a1a9":"code","04f7bbd2":"code","412e254f":"code","1efc3819":"code","10cb3433":"code","4f7c685a":"code","de925742":"code","1b9f0ec9":"code","7adc75b3":"code","d80c9d07":"code","53705051":"code","f8effe11":"code","d3b95c38":"code","f8c6ba66":"code","3123a791":"code","c61b31c7":"code","6364f063":"code","6f3abc3b":"code","7d5afc06":"code","508cc56f":"code","f9875f95":"code","2c87feea":"code","519cdc76":"code","3b66b1bd":"code","c2b7e50c":"code","3e5a0316":"code","2764c612":"code","ba694316":"code","27264026":"code","8cbbcb6a":"code","571f2458":"code","b9709d9e":"code","a4f00138":"code","fa4a750a":"code","6d8dd518":"markdown","c3eb12d6":"markdown","a29f84c9":"markdown","31c3af0c":"markdown","f5af580e":"markdown","f5afabd2":"markdown","ada9460e":"markdown","d2809133":"markdown","fabda9d5":"markdown","aa0e3371":"markdown","a38fbf85":"markdown","6c248af7":"markdown","8eb31dfd":"markdown","e5d9ac3b":"markdown","b805f6ba":"markdown","38bae401":"markdown","9638ec5c":"markdown","442933ca":"markdown","561c028a":"markdown","062fb531":"markdown","27c453b7":"markdown","18aac66e":"markdown","fbb05a35":"markdown","ebc26459":"markdown","078a0004":"markdown","4868eede":"markdown","3a994053":"markdown","3f7c69b3":"markdown","e110fef4":"markdown","9783dc2d":"markdown","7bcff59e":"markdown","687ad9b7":"markdown"},"source":{"7eacc984":"#Importing Packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nsns.set(style=\"darkgrid\")","174b9eca":"# Dowloading and reading data\ndf = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")","6ff919af":"#Check NaN\ndf.isnull().values.any()","2babc794":"df.head()","57e0760b":"df.shape","6942204c":"#Column Names\ndf.columns","ec192ccb":"df.info()","392528bb":"#Summary Stastiscs\ndf.describe()","abb8a711":"pd.pivot_table(df, index=[\"Outcome\"], aggfunc=[np.mean])","0a9c3c2f":"pd.pivot_table(df, index=[\"Outcome\"], aggfunc=[np.std])","c60ed981":"# Distribution of Each Feature: Boxplot\ncolnames = df.columns\nfig, ax=plt.subplots(nrows=2, ncols=4, figsize=(15,12))\nfor i in range(4):\n    x = colnames[i]\n    ax[0,i].boxplot(df[str(x)], labels=[str(x)])\n    ax[0,i].set_ylabel(str(x))\n    ax[0,i].set_title(str(x) + \"\\nBoxplot\")\n\nfor i in range(4,8):\n    x = colnames[i]\n    ax[1,i-4].boxplot(df[str(x)], labels=[str(x)])\n    ax[1,i-4].set_ylabel(str(x))\n    ax[1,i-4].set_title(str(x) + \"\\nBoxplot\")\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.5, \n                    hspace=0.4)","8ee43fb3":"# Distribution of Each Feature: Histogram\n\nfig, ax=plt.subplots(nrows=2, ncols=4, figsize=(15,12), sharey=True)\nfor i in range(4):\n    x = colnames[i]\n    ax[0,i].hist(df[str(x)], color=\"red\", bins=20)\n    ax[0,i].set_ylabel(str(x))\n    ax[0,i].set_title(str(x) + \"\\nHistogram\")\n\nfor i in range(4,8):\n    x = colnames[i]\n    ax[1,i-4].hist(df[str(x)], color=\"red\", bins=20)\n    ax[1,i-4].set_ylabel(str(x))\n    ax[1,i-4].set_title(str(x) + \"\\nHistogram\")\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.3, \n                    hspace=0.4)","54210a92":"# Distribution of Each Feature: Violinplot\n\nfig, ax=plt.subplots(nrows=2, ncols=4, figsize=(15,12))\nfor i in range(4):\n    x = colnames[i]\n    ax[0,i].violinplot(df[str(x)])\n    ax[0,i].set_ylabel(str(x))\n    ax[0,i].set_title(str(x) + \"\\nViolinplot\")\n\nfor i in range(4,8):\n    x = colnames[i]\n    ax[1,i-4].violinplot(df[str(x)])\n    ax[1,i-4].set_ylabel(str(x))\n    ax[1,i-4].set_title(str(x) + \"\\nViolinplot\")\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.5, \n                    hspace=0.4)","7ef23f6c":"# Distribution of Each Feature By Outcome: Barplot\n\nfig, ax =plt.subplots(2,4, figsize=(15,12))\n\nfor i in range(4):\n    x = colnames[i]\n    sns.barplot(data=df, y=str(x), x=\"Outcome\", ax=ax[0, i])\n    ax[0,i].set_title(str(x) + \"\\nDistribution by Outcome\")\n\nfor i in range(4,8):\n    x = colnames[i]\n    sns.barplot(data=df, y=str(x), x=\"Outcome\", ax=ax[1, i-4])\n    ax[1,i-4].set_title(str(x) + \"\\nDistribution by Outcome\")\n    \nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.5, \n                    hspace=0.4)","fa0d2ab5":"# Distribution of Each Feature By Outcome: Boxplot\n\nfig, ax =plt.subplots(2,4, figsize=(15,12))\n\nfor i in range(4):\n    x = colnames[i]\n    sns.boxplot(data=df, y=str(x), x=\"Outcome\", ax=ax[0, i])\n    ax[0,i].set_title(str(x) + \"\\nDistribution by Outcome\")\n\nfor i in range(4,8):\n    x = colnames[i]\n    sns.boxplot(data=df, y=str(x), x=\"Outcome\", ax=ax[1, i-4])\n    ax[1,i-4].set_title(str(x) + \"\\nDistribution by Outcome\")\n    \nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.5, \n                    hspace=0.4)","75c0cef4":"# Distribution of Each Feature By Outcome: Violinplot\n\nfig, ax =plt.subplots(2,4, figsize=(15,12))\n\nfor i in range(4):\n    x = colnames[i]\n    sns.violinplot(data=df, y=str(x), x=\"Outcome\", ax=ax[0, i])\n    ax[0,i].set_title(str(x) + \"\\nDistribution by Outcome\")\n\nfor i in range(4,8):\n    x = colnames[i]\n    sns.violinplot(data=df, y=str(x), x=\"Outcome\", ax=ax[1, i-4])\n    ax[1,i-4].set_title(str(x) + \"\\nDistribution by Outcome\")\n    \nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.5, \n                    hspace=0.4)","96921007":"# Distribution of Each Feature By Outcome: Stripplot\n\nfig, ax =plt.subplots(2,4, figsize=(15,12))\n\nfor i in range(4):\n    x = colnames[i]\n    sns.stripplot(data=df, y=str(x), x=\"Outcome\", ax=ax[0, i])\n    ax[0,i].set_title(str(x) + \"\\nDistribution by Outcome\")\n\nfor i in range(4,8):\n    x = colnames[i]\n    sns.stripplot(data=df, y=str(x), x=\"Outcome\", ax=ax[1, i-4])\n    ax[1,i-4].set_title(str(x) + \"\\nDistribution by Outcome\")\n    \nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.5, \n                    hspace=0.4)","156f255f":"# Distribution of Each Feature By Outcome: Density plot\n\nfig, ax =plt.subplots(2,4, figsize=(15,12))\n\nfor i in range(4):\n    x = colnames[i]\n    sns.kdeplot(data=df, x=str(x), hue=\"Outcome\", shade=True, ax=ax[0, i])\n    ax[0,i].set_title(str(x) + \"\\nDistribution by Outcome\")\n\nfor i in range(4,8):\n    x = colnames[i]\n    sns.kdeplot(data=df, x=str(x), hue=\"Outcome\", shade=True, ax=ax[1, i-4])\n    ax[1,i-4].set_title(str(x) + \"\\nDistribution by Outcome\")\n    \nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.5, \n                    hspace=0.4)","6883ca2a":"#Using Pearson Correlation to create Correlation heatmap\nplt.figure(figsize=(12,10))\ncor = df.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","465448d6":"# Pairwise Plotting\n# Principal Diagonal Plots- Univaraite Analysis (Density plots) coloured by Outcome\n# Other Plots: Bivariate Analysis (Scatter plots) coloured by Outcome\nsns.pairplot(data=df, hue=\"Outcome\")","df951c6f":"#Importing Necessary Packages:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nsns.set(style=\"darkgrid\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c97ecedc":"#Split x and y\nx=df.drop(['Outcome'], axis=1)\ny=df['Outcome']\n\n#Scale x\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\nx_scaled=scaler.fit_transform(x)\nx=pd.DataFrame(x_scaled, columns=x.columns)\n\n#Split test and train, and validation\nfrom sklearn.model_selection import train_test_split\nitrain_x, test_x, itrain_y, test_y=train_test_split(x, y, random_state=56, stratify=y, test_size=0.1)\ntrain_x, valid_x, train_y, valid_y=train_test_split(itrain_x, itrain_y, random_state=56, stratify=itrain_y, test_size=1\/9)\nprint(train_x.shape[0]\/x.shape[0], valid_x.shape[0]\/x.shape[0], test_x.shape[0]\/x.shape[0])","8a8d626e":"#Implement knn classifier\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\n\n#KNN Classifier for some random k=3 without validation:\nclf=KNN(n_neighbors=10)\nclf.fit(itrain_x, itrain_y)\ntest_predict=clf.predict(test_x)\nprint(\"Recall:\", recall_score(test_y, test_predict), \n      \"\\nF1 Score:\", f1_score(test_y, test_predict), \n      \"\\nAccuracy:\", accuracy_score(test_y, test_predict))","3623f295":"#Elbow Curve (Thresholding) for optimising k\ndef Elbow(K):\n    clf=KNN(n_neighbors=K)\n    clf.fit(train_x, train_y)\n    valid_predict=clf.predict(valid_x)\n    rec=recall_score(valid_y, valid_predict)\n    return 1-rec\n       \n\nk=list(range(1, 40, 2))\ner=list(map(Elbow, k))\nplt.plot(k, er)\nplt.xlabel(\"K\")\nplt.ylabel(\"Error (FNR) on Validation Set\")\nplt.title(\"Elbow Curve for KNN Classifier\")","fee5c580":"#KNN Classifier on Optimum k:\nclf=KNN(n_neighbors=k[er.index(min(er))])\nclf.fit(itrain_x, itrain_y)\ntest_predict=clf.predict(test_x)\nprint(\"Recall:\", recall_score(test_y, test_predict), \n    \"\\nF1 Score:\", f1_score(test_y, test_predict), \n    \"\\nAccuracy:\", accuracy_score(test_y, test_predict))\n","ac432e39":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n#Confusion Matrix:\ncm = confusion_matrix(test_y, test_predict, labels=[1,0])\nax= plt.subplot()\nsns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(['1', '0']); ax.yaxis.set_ticklabels(['1', '0']);\n\n# classification report for precision, recall f1-score and accuracy\nmatrix = classification_report(test_y, test_predict, labels=[1,0])\nprint('Classification report : \\n',matrix)","6def8ef7":"#KNN ROC Curve:\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\nclf=KNN(n_neighbors=5)\nclf.fit(itrain_x, itrain_y)\ntest_scores = clf.predict_proba(test_x)\nfpr, tpr, threshold = roc_curve(test_y, test_scores[:, 1])\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC Curve of KNN')\nplt.show()","9ff79be2":"#Packages:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nsns.set(style=\"darkgrid\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Split x and y\nx=df.drop(['Outcome'], axis=1)\ny=df['Outcome']\n\n#Scale x\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\nx_scaled=scaler.fit_transform(x)\nx=pd.DataFrame(x_scaled, columns=x.columns)\n\n#Split test and train\nfrom sklearn.model_selection import train_test_split\ntrain_x, test_x, train_y, test_y=train_test_split(x, y, random_state=56, stratify=y, test_size=0.1)\n\n#Implement knn classifier\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\n#7-Fold Cross validation Mean Score Elbow Curve (Thresholding)  for optimising k\ndef Elbow_cross_val_mean(K):\n    score=cross_val_score(KNN(n_neighbors=K), X=train_x, y=train_y, cv=7, scoring='recall')\n    return score.mean()*100\n\n#7-Fold Cross validation STD score Elbow Curve (Thresholding) for optimising k\ndef Elbow_cross_val_std(K):\n    score=cross_val_score(KNN(n_neighbors=K), X=train_x, y=train_y, cv=7, scoring='recall')\n    return score.std()*1000\n\n#The Elbow Curve Plot\nk=list(range(1, 40, 1))\nfig, ax = plt.subplots(figsize=(5,5))\nax.plot(k, list(map(Elbow_cross_val_mean, k)))\nax.set_xlabel(\"K\")\nax.set_ylabel(\"Mean_Score_Metric\")\nax.set_title(\"Score Elbow Curve for KNN Classifier\")\nax2=ax.twinx()\nax2.plot(k, list(map(Elbow_cross_val_std, k)), color=\"red\")\nax2.set_ylabel(\"STD_Score_Metric\")","31175078":"#Zoomed:\n#The Elbow Curve Plot\nk=list(range(1, 40, 1))\nfig, ax = plt.subplots(figsize=(5,5))\nax.plot(k[1:10], list(map(Elbow_cross_val_mean, k))[1:10])\nax.set_xlabel(\"K\")\nax.set_ylabel(\"Mean_Score_Metric\")\nax.set_title(\"Score Elbow Curve for KNN Classifier\")\nax2=ax.twinx()\nax2.plot(k[1:10], list(map(Elbow_cross_val_std, k))[1:10], color=\"red\")\nax2.set_ylabel(\"STD_Score_Metric\")\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.5, \n                    hspace=0.4)","f71e0790":"#Run on optimum k\n#KNN Classifier on Optimum k:\nclf=KNN(n_neighbors=3)\nclf.fit(train_x, train_y)\ntest_predict_cv=clf.predict(test_x)\nprint( \"Accuracy on Test:\", accuracy_score(test_y, test_predict_cv),\n      \"\\nRecall on Test:\", recall_score(test_y, test_predict_cv),\n     \"\\nF1 Score on Test:\", f1_score(test_y, test_predict_cv))","ce97e75e":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n#Confusion Matrix:\ncm = confusion_matrix(test_y, test_predict_cv, labels=[1,0])\nax= plt.subplot()\nsns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(['1', '0']); ax.yaxis.set_ticklabels(['1', '0']);\n\n# classification report for precision, recall f1-score and accuracy\nmatrix = classification_report(test_y, test_predict_cv, labels=[1,0])\nprint('Classification report : \\n',matrix)","28e1ef8b":"#KNN ROC Curve:\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\nclf=KNN(n_neighbors=3)\nclf.fit(train_x, train_y)\ntest_scores = clf.predict_proba(test_x)\nfpr, tpr, threshold = roc_curve(test_y, test_scores[:, 1])\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC Curve of KNN')\nplt.show()","061c26de":"for i in range(8):\n    z=df.iloc[7,i]\n    print(type(z))","b545afb5":"#Packages:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nsns.set(style=\"darkgrid\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Split x and y\nx=df.drop(['Outcome'], axis=1)\ny=df['Outcome']\n\n#Split test and train\nitrain_x, test_x, itrain_y, test_y=train_test_split(x, y, random_state=56, stratify=y, test_size=0.1)\ntrain_x, valid_x, train_y, valid_y=train_test_split(itrain_x, itrain_y, random_state=56, stratify=itrain_y, test_size=1\/9)\n\n#Implement Linear Regressor\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\n# Creating instance of Logistic Regresssion\nlr = LR(normalize=True)\n\n# Fitting the model\nlr.fit(train_x, train_y)\n\n# Predicting over the Validation Set\nvalid_predict = lr.predict(valid_x)\n\n#Decision Rule (By maximising f1 score on the validation set):\ner=[]\nfor i in range(0, 100, 1):\n    v=i\/100\n    def func_dec(x):\n        if x >= v:\n            return 1\n        else:\n            return 0\n    valid_predict_clf=list(map(func_dec, valid_predict))\n    er_f1 = 1-f1_score(valid_y, valid_predict_clf)\n    er.append(er_f1)\n    \ni=list(range(0, 100, 1))\nv = list(map(lambda x: x\/100, i))\nplt.plot(v, er)\nplt.xlabel(\"Decision Rule Parameter\")\nplt.ylabel(\"Error on Validation Set\")\nplt.title(\"Elbow Curve for Linear Regressor\")","3e423056":"#Using algorithm on entire train (itrain set) set and looking at coefficients\nlr = LR()\nlr.fit(itrain_x, itrain_y)\ntest_predict = lr.predict(test_x)\n\nprint(\"Regression Coefficients: \", lr.coef_)\n\nplt.figure(dpi=120, facecolor='w', edgecolor='b')\nx = range(len(train_x.columns))\ny = lr.coef_\nplt.bar( x, y )\nplt.xlabel( \"Variables\")\nplt.ylabel('Coefficients')\nplt.title('Coefficient plot')","58fc340a":"#Making Final Predictions and evaluating performance\n\n#Using Decision Rule with Optimum Parameter:\ndef func_dec(x):\n    if x >= v[er.index(min(er))]:\n        return 1\n    else:\n        return 0\ntest_predict_clf=list(map(func_dec, test_predict))\n\nprint( \"Accuracy on Test:\", accuracy_score(test_y, test_predict_clf),\n      \"\\nRecall on Test:\", recall_score(test_y, test_predict_clf),\n     \"\\nF1 Score on Test:\", f1_score(test_y, test_predict_clf))","cc1e5fb2":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n#Confusion Matrix:\ncm = confusion_matrix(test_y, test_predict_clf, labels=[1,0])\nax= plt.subplot()\nsns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(['1', '0']); ax.yaxis.set_ticklabels(['1', '0']);\n\n# classification report for precision, recall f1-score and accuracy\nmatrix = classification_report(test_y, test_predict_clf, labels=[1,0])\nprint('Classification report : \\n',matrix)","0e2fad87":"#Linear Regression ROC Curve:\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\nlr = LR()\nlr.fit(itrain_x, itrain_y)\ntest_scores = lr.predict(test_x)\nfpr, tpr, threshold = roc_curve(test_y, test_scores)\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC Curve of  Multivariate Linear Regression')\nplt.show()","bcad3c80":"#Packages:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nsns.set(style=\"darkgrid\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Split x and y\nx=df.drop(['Outcome'], axis=1)\ny=df['Outcome']\n\n#Scale x\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\nx_scaled=scaler.fit_transform(x)\nx=pd.DataFrame(x_scaled, columns=x.columns)\n\n#Split test and train\ntrain_x, test_x, train_y, test_y=train_test_split(x, y, random_state=56, stratify=y, test_size=0.1)\n\n#Implement Logistic Regressor\nfrom sklearn.linear_model import LogisticRegression as LogReg\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\n# Creating instance of Logistic Regresssion\nlogreg = LogReg()\n\n# Fitting the model\nlogreg.fit(train_x, train_y)\n\n# Predicting over the Test Set\ntest_predict = logreg.predict(test_x)\n\n#Printing the coefficients\nprint(\"Regression Coefficients: \", logreg.coef_)\n\nplt.figure(figsize=(8, 6), dpi=120, facecolor='w', edgecolor='b')\nx = range(len(train_x.columns))\nc = logreg.coef_.reshape(-1)\nplt.bar( x, c )\nplt.xlabel( \"Variables\")\nplt.ylabel('Coefficients')\nplt.title('Coefficient plot')","3a09ed1c":"#Making Final Predictions and evaluating performance\n\n\nprint( \"Accuracy on Test:\", accuracy_score(test_y, test_predict),\n      \"\\nRecall on Test:\", recall_score(test_y, test_predict),\n     \"\\nF1 Score on Test:\", f1_score(test_y, test_predict))","8e606648":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n#Confusion Matrix:\ncm = confusion_matrix(test_y, test_predict, labels=[1,0])\nax= plt.subplot()\nsns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(['1', '0']); ax.yaxis.set_ticklabels(['1', '0']);\n\n# classification report for precision, recall f1-score and accuracy\nmatrix = classification_report(test_y, test_predict, labels=[1,0])\nprint('Classification report : \\n',matrix)","9669e5b0":"#Logistic Regression ROC Curve:\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\nlr = LR()\nlr.fit(itrain_x, itrain_y)\ntest_scores = logreg.predict_proba(test_x)\nfpr, tpr, threshold = roc_curve(test_y, test_scores[:,1])\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC Curve of  Logistic Regression')\nplt.show()","68adebed":"#Packages:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nsns.set(style=\"darkgrid\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Split x and y\nx=df.drop(['Outcome'], axis=1)\ny=df['Outcome']\n\n#Split test and train\nitrain_x, test_x, itrain_y, test_y=train_test_split(x, y, random_state=56, stratify=y, test_size=0.1)\ntrain_x, valid_x, train_y, valid_y=train_test_split(itrain_x, itrain_y, random_state=56, stratify=itrain_y, test_size=1\/9)\n\n#Implement Ridge Linear Regressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\n#Creating a list of regularisation strength values\nalpha_ridge = [0, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20, 25, 1000]\n\n#Creating empyty lists to store f1 score and recall for ridge regression (with optimal dceision parameter) \n#for each value of alpha:\nrecall_alpha=[]\nf1_alpha=[]\n\n#Running the simulations\ndef ridge_alpha_max_f1(alpha):\n    # Creating instance of Ridge Regresssion\n    ridgelr = Ridge(normalize=True, alpha=alpha)\n    # Fitting the model\n    ridgelr.fit(train_x, train_y)\n    # Predicting over the Validation Set\n    valid_predict = ridgelr.predict(valid_x)\n    #Decision Rule (By maximising f1 score on the validation set):\n    er=[]\n    for i in range(0, 100, 1):\n        v=i\/100\n        def func_dec(x):\n            if x >= v:\n                return 1\n            else:\n                return 0\n        valid_predict_clf=list(map(func_dec, valid_predict))\n        er_f1 = 1-f1_score(valid_y, valid_predict_clf)\n        er.append(er_f1)\n    #Using optimum decision parameter\n    j=list(range(0, 100, 1))\n    v = list(map(lambda x: x\/100, j))\n    ridgelr = Ridge(alpha=alpha)\n    ridgelr.fit(train_x, train_y)\n    valid_predict = ridgelr.predict(valid_x)\n    def func_dec_opt(x):\n        if x >= v[er.index(min(er))]:\n            return 1\n        else:\n            return 0\n    valid_predict_clf=list(map(func_dec_opt, valid_predict))\n    recall_alpha.append(recall_score(valid_y, valid_predict_clf))\n    f1_alpha.append(f1_score(valid_y, valid_predict_clf))\n    i=list(range(0, 100, 1))\n    v = list(map(lambda x: x\/100, i))\n    f, ax = plt.subplots(figsize=(10, 5))\n    ax.plot(v, er)\n    plt.xlabel(\"Decision Rule Parameter\")\n    plt.ylabel(\"Error on Validation Set\")\n    plt.title(\"Elbow Curve for Linear Regressor\")\n    \nfor i in alpha_ridge:\n    plt.figure(i)\n    ridge_alpha_max_f1(i)\n","81ad47bc":"pd.DataFrame({\"Alpha\":alpha_ridge, \"F1\":f1_alpha, \"Recall\":recall_alpha})","a4741822":"fig, ax = plt.subplots(figsize=(5,5))\nax.plot(alpha_ridge, f1_alpha)\nax.set_xlabel(\"Alpha (Regularization Strength)\")\nax.set_ylabel(\"F1 Score\")\nax.set_title(\"Performance vs Alpha\")\nax2=ax.twinx()\nax2.plot(alpha_ridge, recall_alpha, color=\"red\")\nax2.set_ylabel(\"Recall Score\")","d58cdae8":"fig, ax = plt.subplots(figsize=(5,5))\nax.plot(alpha_ridge[0:7], f1_alpha[0:7])\nax.set_xlabel(\"Alpha (Regularization Strength)\")\nax.set_ylabel(\"F1 Score\")\nax.set_title(\"Performance vs Alpha\")\nax2=ax.twinx()\nax2.plot(alpha_ridge[0:7], recall_alpha[0:7], color=\"red\")\nax2.set_ylabel(\"Recall Score\")","d5e6011f":"ridgelr = Ridge(normalize=True, alpha=1)\nridgelr.fit(train_x, train_y)\nvalid_predict = ridgelr.predict(valid_x)\ner=[]\nfor i in range(0, 100, 1):\n    v=i\/100\n    def func_dec(x):\n        if x >= v:\n            return 1\n        else:\n            return 0\n    valid_predict_clf=list(map(func_dec, valid_predict))\n    er_f1 = 1-f1_score(valid_y, valid_predict_clf)\n    er.append(er_f1)\n\n\n#Using algorithm on entire train (itrain set) set and looking at coefficients\nridgelr = Ridge(normalize=True, alpha=1)\nridgelr.fit(itrain_x, itrain_y)\ntest_predict = ridgelr.predict(test_x)\n\nprint(\"Ridge Regression Coefficients: \", ridgelr.coef_)\n\nplt.figure(dpi=120, facecolor='w', edgecolor='b')\nx = range(len(train_x.columns))\ny = lr.coef_\nplt.bar( x, y )\nplt.xlabel( \"Variables\")\nplt.ylabel('Coefficients')\nplt.title('Coefficient plot')","6c584c7f":"#Using Decision Rule with Optimum Parmeter\ni=list(range(0, 100, 1))\nv = list(map(lambda x: x\/100, i))\ndef func_dec_opt(x):\n    if x >= v[er.index(min(er))]:\n        return 1\n    else:\n        return 0\ntest_predict_clf=list(map(func_dec_opt, test_predict))\nprint( \"Accuracy on Test:\", accuracy_score(test_y, test_predict_clf),\n      \"\\nRecall on Test:\", recall_score(test_y, test_predict_clf),\n     \"\\nF1 Score on Test:\", f1_score(test_y, test_predict_clf))","d4235f61":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n#Confusion Matrix:\ncm = confusion_matrix(test_y, test_predict_clf, labels=[1,0])\nax= plt.subplot()\nsns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(['1', '0']); ax.yaxis.set_ticklabels(['1', '0']);\n\n# classification report for precision, recall f1-score and accuracy\nmatrix = classification_report(test_y, test_predict_clf, labels=[1,0])\nprint('Classification report : \\n',matrix)","94fcf17b":"#Ridge Regression ROC Curve:\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\nridgelr = Ridge(normalize=True, alpha=1)\nridgelr.fit(itrain_x, itrain_y)\ntest_scores = ridgelr.predict(test_x)\nfpr, tpr, threshold = roc_curve(test_y, test_scores)\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC Curve of  Ridge Linear Regression')\nplt.show()","d72d4f7b":"sns.relplot(kind=\"scatter\", data=df, x=\"Glucose\", y=\"DiabetesPedigreeFunction\", hue=\"Outcome\")","1f917516":"#Packages:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nsns.set(style=\"darkgrid\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Split x and y\nx=df.drop(['Outcome'], axis=1)\ny=df['Outcome']\n\n# standardizing the data (Since K Means Clustering is a distance based Algorithm)\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx_scaled = scaler.fit_transform(x)\nx=pd.DataFrame(x_scaled, columns=x.columns)\n\n#Implement K Means\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\n#In some cases, if the initialization of clusters is not appropriate, K-Means can result in arbitrarily bad clusters. \n#This is where K-Means++ helps. It specifies a procedure to initialize the cluster centers before moving forward with the standard k-means clustering algorithm.\nkmeans = KMeans(n_clusters=2, init='k-means++', random_state=52)\nkmeans.fit(x)\n\n#Looking at the coordinates of the clustre centres\nkmeans.cluster_centers_","655f8ab0":"# inertia on the fitted data\nkmeans.inertia_","39ee6da3":"y_predict = kmeans.predict(x)\nprint( \"Accuracy on Test:\", accuracy_score(y, y_predict),\n      \"\\nRecall on Test:\", recall_score(y, y_predict),\n     \"\\nF1 Score on Test:\", f1_score(y, y_predict))","ec251241":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n#Confusion Matrix:\ncm = confusion_matrix(y, y_predict, labels=[1,0])\nax= plt.subplot()\nsns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(['1', '0']); ax.yaxis.set_ticklabels(['1', '0']);\n\n# classification report for precision, recall f1-score and accuracy\nmatrix = classification_report(y, y_predict, labels=[1,0])\nprint('Classification report : \\n',matrix)","8f01503d":"# fitting multiple k-means algorithms and storing the values in an empty list\nSSE = []\nfor cluster in range(1,20):\n    kmeans = KMeans(n_jobs = -1, n_clusters = cluster, init='k-means++', random_state=52)\n    kmeans.fit(x)\n    SSE.append(kmeans.inertia_)\n\n# converting the results into a dataframe and plotting them\nframe = pd.DataFrame({'Cluster':range(1,20), 'SSE':SSE})\nplt.figure(figsize=(12,6))\nplt.plot(frame['Cluster'], frame['SSE'], marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')","71d5a1a9":"from sklearn import metrics\nfrom sklearn.metrics import pairwise_distances\n\nSSo = []\nfor cluster in range(2,20):\n    kmeans = KMeans(n_jobs = -1, n_clusters = cluster, init='k-means++', random_state=52)\n    kmeans.fit(x)\n    labels = kmeans.labels_\n    score = metrics.silhouette_score(x, labels, metric='euclidean')\n    SSo.append(score)\n\n    \n# converting the results into a dataframe and plotting them\nframe = pd.DataFrame({'Cluster':range(2,20), 'SSE':SSo})\nplt.figure(figsize=(12,6))\nplt.plot(frame['Cluster'], frame['SSE'], marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette Score')","04f7bbd2":"#Packages:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nsns.set(style=\"darkgrid\")\nimport tensorflow as tf\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Split x and y\nx=df.drop(['Outcome'], axis=1)\ny=df['Outcome']\n\n#Performing Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx_scaled = scaler.fit_transform(x)\nx=pd.DataFrame(x_scaled, columns=x.columns)\n\n#Split test and train, and validation\nfrom sklearn.model_selection import train_test_split\nitrain_x, test_x, itrain_y, test_y=train_test_split(x, y, random_state=56, stratify=y, test_size=0.1)\ntrain_x, valid_x, train_y, valid_y=train_test_split(itrain_x, itrain_y, random_state=56, stratify=itrain_y, test_size=1\/9)\n","412e254f":"#defining f1 score and recall score metric\nimport keras.backend as K\ndef f1_metric(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)\/(precision+recall+K.epsilon())\n    return f1_val\ndef recall_metric(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\nnp.random.seed(52)\n#Initialising ANN\nann = tf.keras.models.Sequential()\n\n#Adding First Hidden Layer with 6 neurons\nann.add(tf.keras.layers.Dense(units=6,activation=\"relu\"))\n\n#Adding Second Hidden Layer with 6 neurons\nann.add(tf.keras.layers.Dense(units=6,activation=\"relu\"))\n\n#Adding Output Layer with 1 neuron as it's a classification problem.\nann.add(tf.keras.layers.Dense(units=1,activation=\"sigmoid\"))\n\n#Compiling ANN\nann.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[f1_metric])\n#Binary cross entropy is the loss function we use for classification purposes.\n#The popular adam optimizer has been used as a trial.\n\nnp.random.seed(52)\n#Fitting ANN\nann.fit(train_x,train_y,batch_size=128,epochs = 100)\n#Used a mini batch gradient descent for trial, 32 is a popular batch size and each training sample will be use 100 times.\n","1efc3819":"#ANN Classifier\n\nvalid_predict = ann.predict(valid_x)\nvalid_predict","10cb3433":"#Decision Rule (By maximising f1 score on the validation set):\ner=[]\nfor i in range(0, 100, 1):\n    v=i\/100\n    def func_dec(x):\n        if x >= v:\n            return 1\n        else:\n            return 0\n    valid_predict_clf=list(map(func_dec, valid_predict))\n    er_f1 = 1-f1_score(valid_y, valid_predict_clf)\n    er.append(er_f1)\n    \ni=list(range(0, 100, 1))\nv = list(map(lambda x: x\/100, i))\nplt.plot(v, er)\nplt.xlabel(\"Decision Rule Parameter\")\nplt.ylabel(\"Error on Validation Set\")\nplt.title(\"Elbow Curve for ANN\")","4f7c685a":"#Making Final Predictions and evaluating performance\nnp.random.seed(52)\nann.fit(itrain_x,itrain_y,batch_size=128,epochs = 100)\ntest_predict = ann.predict(test_x)\n#Using Decision Rule with Optimum Parameter:\ndef func_dec_opt(x):\n    if x >= v[er.index(min(er))]:\n        return 1\n    else:\n        return 0\ntest_predict_clf=list(map(func_dec_opt, test_predict))\n\nprint( \"Accuracy on Test:\", accuracy_score(test_y, test_predict_clf),\n      \"\\nRecall on Test:\", recall_score(test_y, test_predict_clf),\n     \"\\nF1 Score on Test:\", f1_score(test_y, test_predict_clf))","de925742":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n#Confusion Matrix:\ncm = confusion_matrix(test_y, test_predict_clf, labels=[1,0])\nax= plt.subplot()\nsns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(['1', '0']); ax.yaxis.set_ticklabels(['1', '0']);\n\n# classification report for precision, recall f1-score and accuracy\nmatrix = classification_report(test_y, test_predict_clf, labels=[1,0])\nprint('Classification report : \\n',matrix)","1b9f0ec9":"np.random.seed(52)\ntest_predict = ann.predict(test_x)\n#Untuned ANN ROC Curve:\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\nfpr, tpr, threshold = roc_curve(test_y, test_predict)\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC Curve of  Artificial Neural Network \\n(Untuned)')\nplt.show()","7adc75b3":"#Packages:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization, Dropout\nfrom tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom math import floor\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom keras.layers import LeakyReLU\nLeakyReLU = LeakyReLU(alpha=0.1)\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option(\"display.max_columns\", None)\n\n# Create function\ndef ann_cl_cv(neurons, activation, optimizer, learning_rate):\n    np.random.seed(52)\n    optimizerL = ['Adam', 'SGD']\n    optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate)}\n    activationL = ['relu', 'sigmoid', 'tanh']\n    neurons = round(neurons)\n    activation = activationL[round(activation)]\n    opt = SGD(lr = learning_rate,  momentum=0.9, clipnorm=0.5)\n    ann = tf.keras.models.Sequential()\n    ann.add(tf.keras.layers.Dense(units=neurons, activation=activation))\n    ann.add(tf.keras.layers.Dense(units=neurons, activation=activation))\n    ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n    ann.compile(loss='binary_crossentropy', optimizer=opt, metrics=[f1_metric])\n    return ann\n\nann = KerasClassifier(build_fn=ann_cl_cv, verbose=0)\n\n# Set paramaters\nparams_ann ={\n    'neurons': (6, 12),\n    'activation':(0, 1, 2),\n    'optimizer':(0,1),\n    'learning_rate':(0.02, 1),\n    'batch_size':(32, 128),\n    'epochs':(100, 500)\n}\n\ngrid = GridSearchCV(estimator=ann, param_grid = params_ann, scoring='f1', cv=5)","d80c9d07":"np.random.seed(52)\ngrid_result = grid.fit(itrain_x, itrain_y)","53705051":"means = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","f8effe11":"def plot_search_results(grid):\n    \"\"\"\n    Params: \n        grid: A trained GridSearchCV object.\n    \"\"\"\n    ## Results from grid search\n    results = grid.cv_results_\n    means_test = results['mean_test_score']\n    stds_test = results['std_test_score']\n\n    ## Getting indexes of values per hyper-parameter\n    masks=[]\n    masks_names= list(grid.best_params_.keys())\n    for p_k, p_v in grid.best_params_.items():\n        masks.append(list(results['param_'+p_k].data==p_v))\n\n    params=grid.param_grid\n    \n    ## Ploting results\n    fig, ax = plt.subplots(1,len(params),sharex='none', sharey='all',figsize=(20,5))\n    fig.suptitle('Score per parameter')\n    fig.text(0.04, 0.5, 'MEAN SCORE', va='center', rotation='vertical')\n    pram_preformace_in_best = {}\n    for i, p in enumerate(masks_names):\n        m = np.stack(masks[:i] + masks[i+1:])\n        pram_preformace_in_best\n        best_parms_mask = m.all(axis=0)\n        best_index = np.where(best_parms_mask)[0]\n        x = np.array(params[p])\n        y_1 = np.array(means_test[best_index])\n        e_1 = np.array(stds_test[best_index])\n        ax[i].errorbar(x, y_1, e_1, linestyle='--', marker='o', label='test')\n        ax[i].set_xlabel(p.upper())\n\n    plt.legend()\n    plt.show()\n\nplot_search_results(grid_result)","d3b95c38":"# printing the best parameters\nprint('\\n#### Best hyperparamters ####')\ngrid_result.best_params_","f8c6ba66":"#Running Model with the best hyperparameters:\nnp.random.seed(52)\n\n#Initialising ANN\nann = tf.keras.models.Sequential()\n\n#Learning rate for gradient descent\nopt = SGD(lr = 0.02)\n\n#Adding First Hidden Layer with 12 neurons\nann.add(tf.keras.layers.Dense(units=12,activation=\"relu\"))\n\n#Adding Second Hidden Layer with 12 neurons\nann.add(tf.keras.layers.Dense(units=12,activation=\"relu\"))\n\n#Adding Output Layer with 1 neuron as it's a classification problem.\nann.add(tf.keras.layers.Dense(units=1,activation=\"relu\"))\n\n#Compiling ANN\nann.compile(optimizer=opt ,loss=\"binary_crossentropy\",metrics=[f1_metric])\n\nnp.random.seed(52)\n#Fitting ANN\nann.fit(train_x,train_y,batch_size=128,epochs = 100)\n","3123a791":"#ANN Classifier\n\nvalid_predict = ann.predict(valid_x)\n\n#Decision Rule (By maximising f1 score on the validation set):\ner=[]\nfor i in range(0, 100, 1):\n    v=i\/100\n    def func_dec(x):\n        if x >= v:\n            return 1\n        else:\n            return 0\n    valid_predict_clf=list(map(func_dec, valid_predict))\n    er_f1 = 1-f1_score(valid_y, valid_predict_clf)\n    er.append(er_f1)\n    \ni=list(range(0, 100, 1))\nv = list(map(lambda x: x\/100, i))\nplt.plot(v, er)\nplt.xlabel(\"Decision Rule Parameter\")\nplt.ylabel(\"Error on Validation Set\")\nplt.title(\"Elbow Curve for ANN\")","c61b31c7":"#Making Final Predictions and evaluating performance\n\nnp.random.seed(10)\nann.fit(itrain_x,itrain_y,batch_size=128,epochs = 100)\n\ntest_predict = ann.predict(test_x)\n\n#Using Decision Rule with Optimum Parameter:\ndef func_dec_opt(x):\n    if x >= v[er.index(min(er))]:\n        return 1\n    else:\n        return 0\ntest_predict_clf=list(map(func_dec_opt, test_predict))\n\nprint( \"Accuracy on Test:\", accuracy_score(test_y, test_predict_clf),\n      \"\\nRecall on Test:\", recall_score(test_y, test_predict_clf),\n     \"\\nF1 Score on Test:\", f1_score(test_y, test_predict_clf))","6364f063":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n#Confusion Matrix:\ncm = confusion_matrix(test_y, test_predict_clf, labels=[1,0])\nax= plt.subplot()\nsns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(['1', '0']); ax.yaxis.set_ticklabels(['1', '0']);\n\n# classification report for precision, recall f1-score and accuracy\nmatrix = classification_report(test_y, test_predict_clf, labels=[1,0])\nprint('Classification report : \\n',matrix)","6f3abc3b":"test_predict = ann.predict(test_x)\n#Untuned ANN ROC Curve:\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\nfpr, tpr, threshold = roc_curve(test_y, test_predict)\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC Curve of  Artificial Neural Network \\n(Tuned)')\nplt.show()","7d5afc06":"#Packages:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nsns.set(style=\"darkgrid\")\nimport tensorflow as tf\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.decomposition import PCA\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Split x and y\nx=df.drop(['Outcome'], axis=1)\ny=df['Outcome']\n\n#Performing Feature Scaling (Necessary as PCA requires distance computations which are affected by scale)\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx_scaled = scaler.fit_transform(x)\nx=pd.DataFrame(x_scaled, columns=x.columns)","508cc56f":"#Looking at the relevant features in our data set.\npca = PCA().fit(x)\n\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nfig, ax = plt.subplots()\nxi = np.arange(1, 9, step=1)\nscree_y = np.cumsum(pca.explained_variance_ratio_)\nscree_y=[round(num, 3) for num in scree_y]\ndfab=pd.DataFrame({\"xi\":xi, \"scree_y\":scree_y})\n\nplt.ylim(0.0,1.1)\nplt.plot(dfab.xi, dfab.scree_y, marker='o',  linewidth=2, color='blue')\n\nplt.xlabel('Number of Components')\nplt.xticks(np.arange(0, 9, step=1)) #change from 0-based array index to 1-based human-readable label\nplt.ylabel('Cumulative variance (%)')\nplt.title('The number of components needed to explain variance')\n\nplt.axhline(y=0.90, color='r', linestyle='-')\nplt.text(0.5, 0.85, '90% cut-off threshold', fontsize=16)\nfor row in dfab.itertuples():\n    plt.text(row.Index+0.8, row.scree_y+0.04, s=row.scree_y, horizontalalignment= 'center', verticalalignment='bottom')\n\nax.grid(axis='x')\nplt.show()","f9875f95":"#define PCA model to use\npca = PCA(n_components=8)\n\n#fit PCA model to data\npca_fit = pca.fit_transform(x)\n\n#Scree Plot:\nPC_values = np.arange(pca.n_components_) + 1\n\nscree_y=[round(num, 3) for num in pca.explained_variance_ratio_]\ndfab=pd.DataFrame({\"xi\":PC_values, \"scree_y\":scree_y})\n\nplt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\nplt.title('Scree Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Explained Variance Ratio')\nfor row in dfab.itertuples():\n    plt.text(row.Index+1.3, row.scree_y, s=row.scree_y, horizontalalignment= 'center', verticalalignment='bottom')\nplt.show()","2c87feea":"plt.rcParams[\"figure.figsize\"] = (12,6)\n\n#define PCA model to use\npca = PCA(n_components=6)\n\n#fit PCA model to data\npca_fit = pca.fit_transform(x)\n\n#Scree Plot:\nPC_values = np.arange(pca.n_components_) + 1\n\nscree_y=[round(num, 3) for num in pca.explained_variance_ratio_]\ndfab=pd.DataFrame({\"xi\":PC_values, \"scree_y\":scree_y})\n\nplt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\nplt.title('Scree Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Explained Variance Ratio')\nfor row in dfab.itertuples():\n    plt.text(row.Index+1.3, row.scree_y, s=row.scree_y, horizontalalignment= 'center', verticalalignment='bottom')\nplt.show()","519cdc76":"# Next, let's create a DataFrame that will have the principal component values for all observations.\npca = PCA(n_components=6)\npca_fit = pca.fit_transform(x)\npcai_df = pd.DataFrame(data = pca_fit\n             , columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', 'principal component 5', 'principal component 6'])\npca_df=pd.concat([pcai_df, y], axis=1)\npca_df.head()","3b66b1bd":"#Importing Necessary Packages:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nsns.set(style=\"darkgrid\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#Split pca_x and pca_y\npca_x=pca_df.drop(['Outcome'], axis=1)\npca_y=df['Outcome']\n\n#Scale pca_x\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\npca_x_scaled=scaler.fit_transform(pca_x)\npca_x=pd.DataFrame(pca_x_scaled, columns=pca_x.columns)\n\n#Split test and train, and validation\nfrom sklearn.model_selection import train_test_split\nitrain_x, test_x, itrain_y, test_y=train_test_split(pca_x, pca_y, random_state=56, stratify=pca_y, test_size=0.1)\ntrain_x, valid_x, train_y, valid_y=train_test_split(itrain_x, itrain_y, random_state=56, stratify=itrain_y, test_size=1\/9)\nprint(train_x.shape[0]\/x.shape[0], valid_x.shape[0]\/x.shape[0], test_x.shape[0]\/x.shape[0])\n\n#Implement knn classifier\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\n\n#Elbow Curve (Thresholding) for optimising k\ndef Elbow(K):\n    clf=KNN(n_neighbors=K)\n    clf.fit(train_x, train_y)\n    valid_predict=clf.predict(valid_x)\n    rec=recall_score(valid_y, valid_predict)\n    return 1-rec\n\nk=list(range(1, 40, 2))\ner=list(map(Elbow, k))\nplt.plot(k, er)\nplt.xlabel(\"K\")\nplt.ylabel(\"Error (FNR) on Validation Set\")\nplt.title(\"Elbow Curve for KNN Classifier in 6-PCA data\")","c2b7e50c":"#KNN Classifier on Optimum k:\nclf=KNN(n_neighbors=k[er.index(min(er))])\nclf.fit(itrain_x, itrain_y)\ntest_predict=clf.predict(test_x)\nprint(\"Recall:\", recall_score(test_y, test_predict), \n    \"\\nF1 Score:\", f1_score(test_y, test_predict), \n    \"\\nAccuracy:\", accuracy_score(test_y, test_predict))","3e5a0316":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n#Confusion Matrix:\ncm = confusion_matrix(test_y, test_predict, labels=[1,0])\nax= plt.subplot()\nsns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(['1', '0']); ax.yaxis.set_ticklabels(['1', '0']);\n\n# classification report for precision, recall f1-score and accuracy\nmatrix = classification_report(test_y, test_predict, labels=[1,0])\nprint('Classification report : \\n',matrix)","2764c612":"#KNN ROC Curve:\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\nclf=KNN(n_neighbors=5)\nclf.fit(itrain_x, itrain_y)\ntest_scores = clf.predict_proba(test_x)\nfpr, tpr, threshold = roc_curve(test_y, test_scores[:, 1])\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC Curve of KNN')\nplt.show()","ba694316":"plt.rcParams[\"figure.figsize\"] = (12,6)\n\n#define PCA model to use\npca = PCA(n_components=7)\n\n#fit PCA model to data\npca_fit = pca.fit_transform(x)\n\n#Scree Plot:\nPC_values = np.arange(pca.n_components_) + 1\n\nscree_y=[round(num, 3) for num in pca.explained_variance_ratio_]\ndfab=pd.DataFrame({\"xi\":PC_values, \"scree_y\":scree_y})\n\nplt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\nplt.title('Scree Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Explained Variance Ratio')\nfor row in dfab.itertuples():\n    plt.text(row.Index+1.3, row.scree_y, s=row.scree_y, horizontalalignment= 'center', verticalalignment='bottom')\nplt.show()","27264026":"pca = PCA(n_components=7)\npca_fit = pca.fit_transform(x)\npcai_df = pd.DataFrame(data = pca_fit\n             , columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', 'principal component 5', 'principal component 6', 'principal component 7'])\npca_df=pd.concat([pcai_df, y], axis=1)\npca_df.head()","8cbbcb6a":"#Importing Necessary Packages:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nsns.set(style=\"darkgrid\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#Split pca_x and pca_y\npca_x=pca_df.drop(['Outcome'], axis=1)\npca_y=df['Outcome']\n\n#Scale pca_x\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\npca_x_scaled=scaler.fit_transform(pca_x)\npca_x=pd.DataFrame(pca_x_scaled, columns=pca_x.columns)\n\n#Split test and train, and validation\nfrom sklearn.model_selection import train_test_split\nitrain_x, test_x, itrain_y, test_y=train_test_split(pca_x, pca_y, random_state=56, stratify=pca_y, test_size=0.1)\ntrain_x, valid_x, train_y, valid_y=train_test_split(itrain_x, itrain_y, random_state=56, stratify=itrain_y, test_size=1\/9)\nprint(train_x.shape[0]\/x.shape[0], valid_x.shape[0]\/x.shape[0], test_x.shape[0]\/x.shape[0])\n\n#Implement knn classifier\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\n\n#Elbow Curve (Thresholding) for optimising k\ndef Elbow(K):\n    clf=KNN(n_neighbors=K)\n    clf.fit(train_x, train_y)\n    valid_predict=clf.predict(valid_x)\n    rec=recall_score(valid_y, valid_predict)\n    return 1-rec\n\nk=list(range(1, 40, 2))\ner=list(map(Elbow, k))\nplt.plot(k, er)\nplt.xlabel(\"K\")\nplt.ylabel(\"Error (FNR) on Validation Set\")\nplt.title(\"Elbow Curve for KNN Classifier in 6-PCA data\")","571f2458":"#KNN Classifier on Optimum k:\nclf=KNN(n_neighbors=k[er.index(min(er))])\nclf.fit(itrain_x, itrain_y)\ntest_predict=clf.predict(test_x)\nprint(\"Recall:\", recall_score(test_y, test_predict), \n    \"\\nF1 Score:\", f1_score(test_y, test_predict), \n    \"\\nAccuracy:\", accuracy_score(test_y, test_predict))","b9709d9e":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n#Confusion Matrix:\ncm = confusion_matrix(test_y, test_predict, labels=[1,0])\nax= plt.subplot()\nsns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(['1', '0']); ax.yaxis.set_ticklabels(['1', '0']);\n\n# classification report for precision, recall f1-score and accuracy\nmatrix = classification_report(test_y, test_predict, labels=[1,0])\nprint('Classification report : \\n',matrix)","a4f00138":"#KNN ROC Curve:\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\nclf=KNN(n_neighbors=5)\nclf.fit(itrain_x, itrain_y)\ntest_scores = clf.predict_proba(test_x)\nfpr, tpr, threshold = roc_curve(test_y, test_scores[:, 1])\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC Curve of KNN')\nplt.show()","fa4a750a":"results_x = [\"KNN \\nClassifier \\n Without PCA\", \n\"KNN \\nClassifier \\nWith PCA (6)\", \n\"KNN \\nClassifier \\nWith PCA (7)\"]\n\nresults_y_Recall = [0.63, 0.52, 0.67]\n\nresults_y_F1 = [0.63, 0.58, 0.69]\n\nresults_y_Accuracy = [0.74, 0.74, 0.79] \n\nresults_y_AUCROC = [0.76, 0.74, 0.74]\n\n# Prepare Data\ndfa=pd.DataFrame({\"Model\":results_x, \"Recall\":results_y_Recall})\ndfa.sort_values('Recall', inplace=True)\ndfa.reset_index(inplace=True)\n\ndfb=pd.DataFrame({\"Model\":results_x, \"F1\":results_y_F1})\ndfb.sort_values('F1', inplace=True)\ndfb.reset_index(inplace=True)\n\ndfc=pd.DataFrame({\"Model\":results_x, \"Accuracy\":results_y_Accuracy})\ndfc.sort_values('Accuracy', inplace=True)\ndfc.reset_index(inplace=True)\n\ndfd=pd.DataFrame({\"Model\":results_x, \"AUCROC\":results_y_AUCROC})\ndfd.sort_values('AUCROC', inplace=True)\ndfd.reset_index(inplace=True)\n\n# Draw plot\nfig, ax = plt.subplots(2,2, figsize=(20,15))\n\nax[0,0].vlines(x=dfa.index, ymin=0, ymax=dfa.Recall, color='firebrick', alpha=0.7, linewidth=2)\nax[0,0].scatter(x=dfa.index, y=dfa.Recall, s=75, color='firebrick', alpha=0.7)\nax[0,0].set_title('Comparative Model Performance on Test Set (Recall)', fontdict={'size':14})\nax[0,0].set_ylabel('Recall')\nax[0,0].set_xticks(dfa.index)\nax[0,0].set_xticklabels(dfa.Model.str.upper())\nax[0,0].set_ylim(0, 1)\nfor row in dfa.itertuples():\n    ax[0,0].text(row.Index, row.Recall+.05, s=row.Recall, horizontalalignment= 'center', verticalalignment='bottom')\n    \nax[0,1].vlines(x=dfb.index, ymin=0, ymax=dfb.F1, color='firebrick', alpha=0.7, linewidth=2)\nax[0,1].scatter(x=dfb.index, y=dfb.F1, s=75, color='firebrick', alpha=0.7)\nax[0,1].set_title('Comparative Model Performance on Test Set (F1)', fontdict={'size':14})\nax[0,1].set_ylabel('F1 Score')\nax[0,1].set_xticks(dfb.index)\nax[0,1].set_xticklabels(dfb.Model.str.upper())\nax[0,1].set_ylim(0, 1)\nfor row in dfb.itertuples():\n    ax[0,1].text(row.Index, row.F1+.05, s=row.F1, horizontalalignment= 'center', verticalalignment='bottom')\n    \nax[1,0].vlines(x=dfc.index, ymin=0, ymax=dfc.Accuracy, color='firebrick', alpha=0.7, linewidth=2)\nax[1,0].scatter(x=dfc.index, y=dfc.Accuracy, s=75, color='firebrick', alpha=0.7)\nax[1,0].set_title('Comparative Model Performance on Test Set (Accuracy)', fontdict={'size':14})\nax[1,0].set_ylabel('Accuracy')\nax[1,0].set_xticks(dfa.index)\nax[1,0].set_xticklabels(dfc.Model.str.upper())\nax[1,0].set_ylim(0, 1)\nfor row in dfc.itertuples():\n    ax[1,0].text(row.Index, row.Accuracy+.05, s=row.Accuracy, horizontalalignment= 'center', verticalalignment='bottom')\n    \nax[1,1].vlines(x=dfd.index, ymin=0, ymax=dfd.AUCROC, color='firebrick', alpha=0.7, linewidth=2)\nax[1,1].scatter(x=dfd.index, y=dfd.AUCROC, s=75, color='firebrick', alpha=0.7)\nax[1,1].set_title('Comparative Model Performance on Test Set (AUCROC)', fontdict={'size':14})\nax[1,1].set_ylabel('AUCROC')\nax[1,1].set_xticks(dfd.index)\nax[1,1].set_xticklabels(dfd.Model.str.upper())\nax[1,1].set_ylim(0, 1)\nfor row in dfd.itertuples():\n    ax[1,1].text(row.Index, row.AUCROC+.05, s=row.AUCROC, horizontalalignment= 'center', verticalalignment='bottom')\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.3, \n                    hspace=0.3)","6d8dd518":"### 1) KNN Classifier (Stratified Hold Out Validation)","c3eb12d6":"True Positive Rate\/Recall (0.63) and False Negative Rate is (0.37) are specially relevant in models built for disease prediction as the risk of a diabetic person being misclassified as healthy is more important than the risk of a healthy person being misclassified as diabetic.","a29f84c9":"### 2) KNN Classifier (Using k-Fold Cross Validation)","31c3af0c":"### ANN Tuning the Hyperparameters:\nHyperparameters that will be tuned:\n1. Number of neurons in a layer to choose\n2. Choice of the optimization function (optimizer to be used in order to perform stochastic gradient descent)\n3. Choice of the learning rate for optimization function (ontrols how much to change the model in response to the estimated error each time the model weights are updated)\n4. Choice of activation function (to get the output of the node)\n5. Batch Size (number of samples processed before the model is updated)\n6. Epoch (number of complete passes through the training dataset.)","f5af580e":"### 5) Ridge Regularised Multivariate Linear Regression (Stratified Hold Out Validation for Tuning Decision Parameter and Regularisation Strength)","f5afabd2":"#### Silhouette Scores","ada9460e":"## V) Supervised Deep Learning Model: Artificial Neural Network (ANN)","d2809133":"### 3) Multivariate Linear Regression (Stratified Hold Out Validation for Decison Rule)","fabda9d5":"## VI) Principle Component Analysis (Dimension Reduction)\n\nThe idea behind PCA is to rotate the coordinate axes, in a such way that axis captures almost all the information content or the variance of highly correlated features. it does so by taking a lower dimensional projection of the irrelevant dimensions from a high dimensional data set with a motive to capture as much information as possible.","aa0e3371":"True Positive Rate\/Recall (0.67) and False Negative Rate is (0.37) are specially relevant in models built for disease prediction as the risk of a diabetic person being misclassified as healthy is more important than the risk of a healthy person being misclassified as diabetic.\nFor this model, the recall and the precision and consequently the F1 score is more.","a38fbf85":"$$L=\\cfrac{1}{n} \\sum_{i=1}^n (\\hat{Y}_i-Y_i)^2 + \\cfrac{\\lambda}{n}\\sum_{j=1}^m \\beta_j^2$$","6c248af7":"It doesn't make much sense to split dataset for unsupervised learning since one doesn't have labels to automatically calculate the accuracy\/effectiveness of the unsupervised model.","8eb31dfd":"# Diabetes Prediction Project","e5d9ac3b":"The first principal component explains 26.2% of the total variation in the dataset.  \nThe second principal component explains 21.6% of the total variation.  \nThe third principal component explains 12.9% of the total variation.  \nThe fourth principal component explains 10.9% of the total variation.  \nThe fifth principal component explains 9.5% of the total variation.  \nThe sixth principal component explains 8.5% of the total variation.  ","b805f6ba":"To explain 90% of the variance, we need 6 of the data set features instead of 8. It's not much of a reduction, but a reductiion still! We shall now set n_components to 6 to reduce our data features from 8 to 6.","38bae401":"The silhouette method computes silhouette coefficients of each point that measure how much a point is similar to its own cluster compared to other clusters. The value of the silhouette ranges between -1 and 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. A high silhouette score is therefore desirable. ","9638ec5c":"Best Result: Linear Regression with Decision Rule Optimization  \nAccuracy: 81%  \nF1 Score: 73%","442933ca":"#### Calculating predictions for new pca (7 PCA) implemented set via KNN (Stratified Hold Out Validation) and comparing results with the original data set.","561c028a":"## I) Data Set Description and Preliminary Analysis:\n\n**Description:** This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n**Source of Dataset:** Kaggle (https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database)  \n<br>\n**Objective:** Use Machine Learning Algrorithms (Supervised & Unsupervised) to predict whether a given patient has Diabetes or not.  \n**Procedure:** The datasets consist of several medical predictors (independent) variables and one target (dependent) variable, Outcome. Independent variables include the number of pregnancies the patient has had, their BMI, insulin level, age, blood pressure, skin thickness, glucose level, pedigree diabetes function (family history) etc.","062fb531":"## II) Visualization","27c453b7":"#### Calculating predictions for new pca implemented (6-PCA) set via KNN (Stratified Hold Out Validation) and comparing results with the original data set.","18aac66e":"#### Repeating the above process, but using 7 components intead of 6 to capture 95% variance","fbb05a35":"True Positive Rate\/Recall (0.81) and False Negative Rate is (0.19) are specially relevant in models built for disease prediction as the risk of a diabetic person being misclassified as healthy is more important than the risk of a healthy person being misclassified as diabetic.\nFor this model, the recall is considerably higher than the knn model.","ebc26459":"**Note:** Prevalence (proportion of Diabetic Patients) is seen to be 34%, indicating that the dataset is somewhat unbalanced (not too much though). Hence, accuracy alone would not be a good enough instrument for evaluating the performance of the algorithms that would be used in the subsequent sections.","078a0004":"Problems with this Model:\n1. Theshold is shifting with every new data point added.\n2. R=There are also negetive values after performing regression, interpreting which as a a probability is abstract. In, other words, it's difficult to interpret the model in the extrememes. \n\nTherefore, we use logistic regression.","4868eede":"## III) Prediction Models: Supervised","3a994053":"### Untuned ANN","3f7c69b3":"**Note:** True Positive Rate\/recall and False Negative Rate specially relevant in models built for disease prediction as the risk of a diabetic person being misclassified as healthy is more important than the risk of a healthy person being misclassified as diabetic. Thus, our main metric would be Recall.","e110fef4":"## IV) Prediction Models: Unsupervised: K Means Clustering","9783dc2d":"#### Capturing 90% of the variance using 6 components:","7bcff59e":"### 4) Logistic Regression","687ad9b7":"The first principal component explains 26.2% of the total variation in the dataset.  \nThe second principal component explains 21.6% of the total variation.  \nThe third principal component explains 12.9% of the total variation.  \nThe fourth principal component explains 10.9% of the total variation.  \nThe fifth principal component explains 9.5% of the total variation.  \nThe sixth principal component explains 8.5% of the total variation.  \nThe seventh principal component explains 5.2% of the total variation.  "}}