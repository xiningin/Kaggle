{"cell_type":{"a968e9aa":"code","9d35ff12":"code","10371f90":"code","b373e707":"code","16c4a61a":"code","928e46a7":"code","2a8711ad":"code","e7de1e22":"code","e719a6c7":"code","b1e364ae":"code","68dc43f5":"code","a23d78bd":"code","53ed51f0":"code","bcf227e7":"code","9d20a850":"code","4253e73a":"code","6eeffe6a":"code","3492062f":"code","307528c6":"code","2aa0b2ad":"code","9e90a4c8":"code","acaeb177":"markdown","62ff5347":"markdown","1bcccfc6":"markdown","993fac56":"markdown","79102750":"markdown","1812739b":"markdown","95972c52":"markdown","ac4e95b6":"markdown","f4ae1f07":"markdown","969e6c16":"markdown","12e1029c":"markdown","4b431ca7":"markdown","e08969df":"markdown","ebd5002c":"markdown","3593a0ed":"markdown","18ed7462":"markdown","f320b814":"markdown","cef5c084":"markdown"},"source":{"a968e9aa":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","9d35ff12":"from sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_moons\nfrom sklearn.datasets import make_circles\n\nfrom sklearn.model_selection import train_test_split","10371f90":"datasets = {}\n\ndatasets['blobs'] = {}\ndatasets['moons'] = {}\ndatasets['circles'] = {}","b373e707":"# blobs\nX, y = make_blobs(n_samples=100, centers=2, n_features=2)\n\ndatasets['blobs']['X'] = X\ndatasets['blobs']['y'] = y","16c4a61a":"# moons\nX, y = make_moons(n_samples=5000, random_state=42, noise=0.1)\n\ndatasets['moons']['X'] = X\ndatasets['moons']['y'] = y","928e46a7":"# circle\nX, y = make_circles(n_samples=100, noise=0.05)\n\ndatasets['circles']['X'] = X\ndatasets['circles']['y'] = y","2a8711ad":"for dkey in list(datasets.keys()):\n    X = datasets[dkey]['X']\n    y = datasets[dkey]['y']\n    df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n    colors = {0:'red', 1:'blue', 2:'green'}\n    fig, ax = plt.subplots()\n    fig.suptitle(dkey, fontsize=20)\n    grouped = df.groupby('label')\n    for key, group in grouped:\n        group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n    plt.show()","e7de1e22":"def shuffle(X, y):\n    Z = np.column_stack((X, y))\n    np.random.shuffle(Z)\n    return Z[:, :-1], Z[:, -1]","e719a6c7":"n_feature = 2\nn_class = 2\n\n\ndef make_network(n_hidden=100):\n    model = dict(\n        W1=np.random.randn(n_feature, n_hidden),\n        W2=np.random.randn(n_hidden, n_class)\n    )\n\n    return model","b1e364ae":"def softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x \/ e_x.sum()","68dc43f5":"def forward(x, model):\n    h = x @ model['W1']\n    h[h < 0] = 0\n\n    prob = softmax(h @ model['W2'])\n\n    return h, prob\n\n\ndef backward(model, xs, hs, errs):\n    dW2 = hs.T @ errs\n\n    dh = errs @ model['W2'].T\n    dh[hs < 0] = 0\n    dW1 = xs.T @ dh\n\n    return dict(W1=dW1, W2=dW2)","a23d78bd":"def get_minibatch_grad(model, X_train, y_train):\n    xs, hs, errs = [], [], []\n\n    for x, cls_idx in zip(X_train, y_train):\n        h, y_pred = forward(x, model)\n\n        y_true = np.zeros(n_class)\n        y_true[int(cls_idx)] = 1.\n        err = y_true - y_pred\n\n        xs.append(x)\n        hs.append(h)\n        errs.append(err)\n\n    return backward(model, np.array(xs), np.array(hs), np.array(errs))\n\n\ndef get_minibatch(X, y, minibatch_size):\n    minibatches = []\n\n    X, y = shuffle(X, y)\n\n    for i in range(0, X.shape[0], minibatch_size):\n        X_mini = X[i:i + minibatch_size]\n        y_mini = y[i:i + minibatch_size]\n\n        minibatches.append((X_mini, y_mini))\n\n    return minibatches","53ed51f0":"def sgd(model, X_train, y_train, minibatch_size):\n    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n\n    for iter in range(1, n_iter + 1):\n        idx = np.random.randint(0, len(minibatches))\n        X_mini, y_mini = minibatches[idx]\n\n        grad = get_minibatch_grad(model, X_mini, y_mini)\n\n        for layer in grad:\n            model[layer] += alpha * grad[layer]\n\n    return model","bcf227e7":"def momentum(model, X_train, y_train, minibatch_size):\n    velocity = {k: np.zeros_like(v) for k, v in model.items()}\n    gamma = .9\n\n    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n\n    for iter in range(1, n_iter + 1):\n        idx = np.random.randint(0, len(minibatches))\n        X_mini, y_mini = minibatches[idx]\n\n        grad = get_minibatch_grad(model, X_mini, y_mini)\n\n        for layer in grad:\n            velocity[layer] = gamma * velocity[layer] + alpha * grad[layer]\n            model[layer] += velocity[layer]\n\n    return model","9d20a850":"def nesterov(model, X_train, y_train, minibatch_size):\n    velocity = {k: np.zeros_like(v) for k, v in model.items()}\n    gamma = .9\n\n    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n\n    for iter in range(1, n_iter + 1):\n        idx = np.random.randint(0, len(minibatches))\n        X_mini, y_mini = minibatches[idx]\n\n        model_ahead = {k: v + gamma * velocity[k] for k, v in model.items()}\n        grad = get_minibatch_grad(model, X_mini, y_mini)\n\n        for layer in grad:\n            velocity[layer] = gamma * velocity[layer] + alpha * grad[layer]\n            model[layer] += velocity[layer]\n\n    return model","4253e73a":"def adagrad(model, X_train, y_train, minibatch_size):\n    cache = {k: np.zeros_like(v) for k, v in model.items()}\n\n    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n\n    for iter in range(1, n_iter + 1):\n        idx = np.random.randint(0, len(minibatches))\n        X_mini, y_mini = minibatches[idx]\n\n        grad = get_minibatch_grad(model, X_mini, y_mini)\n\n        for k in grad:\n            cache[k] += grad[k]**2\n            model[k] += alpha * grad[k] \/ (np.sqrt(cache[k]) + eps)\n\n    return model","6eeffe6a":"def rmsprop(model, X_train, y_train, minibatch_size):\n    cache = {k: np.zeros_like(v) for k, v in model.items()}\n    gamma = .9\n\n    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n\n    for iter in range(1, n_iter + 1):\n        idx = np.random.randint(0, len(minibatches))\n        X_mini, y_mini = minibatches[idx]\n\n        grad = get_minibatch_grad(model, X_mini, y_mini)\n\n        for k in grad:\n            cache[k] = gamma * cache[k] + (1 - gamma) * (grad[k]**2)\n            model[k] += alpha * grad[k] \/ (np.sqrt(cache[k]) + eps)\n\n    return model","3492062f":"def adam(model, X_train, y_train, minibatch_size):\n    M = {k: np.zeros_like(v) for k, v in model.items()}\n    R = {k: np.zeros_like(v) for k, v in model.items()}\n    beta1 = .9\n    beta2 = .999\n\n    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n\n    for iter in range(1, n_iter + 1):\n        t = iter\n        idx = np.random.randint(0, len(minibatches))\n        X_mini, y_mini = minibatches[idx]\n\n        grad = get_minibatch_grad(model, X_mini, y_mini)\n\n        for k in grad:\n            M[k] = beta1 * M[k] + (1. - beta1) * grad[k]\n            R[k] = beta2 * R[k] + (1. - beta2) * grad[k]**2\n\n            m_k_hat = M[k] \/ (1. - beta1**(t))\n            r_k_hat = R[k] \/ (1. - beta2**(t))\n\n            model[k] += alpha * m_k_hat \/ (np.sqrt(r_k_hat) + eps)\n\n    return model","307528c6":"n_iter = 100\neps = 1e-8\nalpha = 1e-2\nminibatch_size = 100\nn_experiment = 3","2aa0b2ad":"optimizers = dict(\n    sgd=sgd,\n    momentum=momentum,\n    nesterov=nesterov,\n    adagrad=adagrad,\n    rmsprop=rmsprop,\n    adam=adam\n)","9e90a4c8":"for dkey in list(datasets.keys()):\n    X = datasets[dkey]['X']\n    y = datasets[dkey]['y']\n    \n    print('Dataset:', dkey)\n    print('-' * 50)\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n    \n    algo_accs = {k: np.zeros(n_experiment) for k in optimizers}\n\n    for algo_name, algo in optimizers.items():\n\n        for k in range(n_experiment):\n            model = make_network()\n            model = algo(model, X_train, y_train, minibatch_size)\n\n            y_pred = np.zeros_like(y_test)\n\n            for i, x in enumerate(X_test):\n                _, prob = forward(x, model)\n                y = np.argmax(prob)\n                y_pred[i] = y\n\n            algo_accs[algo_name][k] = np.mean(y_pred == y_test)\n\n    print()\n\n    for k, v in algo_accs.items():\n        print('{0:10s} => mean accuracy: {1:.5f}, std: {2:.5f}'.format(k, v.mean(), v.std()))\n        \n    print()\n    print('-' * 50)","acaeb177":"## Additional functionality","62ff5347":"# AdaGrad\n\nAdaGrad (for adaptive gradient algorithm) is a modified stochastic gradient descent algorithm with per-parameter learning rate, first published in 2011. Informally, this increases the learning rate for sparser parameters and decreases the learning rate for ones that are less sparse. This strategy often improves convergence performance over standard stochastic gradient descent in settings where data is sparse and sparse parameters are more informative.","1bcccfc6":"# Plot Datasets","993fac56":"# Momentum\n\nStochastic gradient descent with momentum remembers the update \u0394w at each iteration, and determines the next update as a linear combination of the gradient and the previous update.","79102750":"## Model implementation","1812739b":"## Forward-backward propagation","95972c52":"# RMSProp\n\nRMSProp (for Root Mean Square Propagation) is also a method in which the learning rate is adapted for each of the parameters. The idea is to divide the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight. So, first the running average is calculated in terms of means square, ","ac4e95b6":"# Prepare the datasets","f4ae1f07":"# Stochastic Gradient Descent used in Machine Learning","969e6c16":"## Activation function","12e1029c":"# Training","4b431ca7":"# Adam\n\nAdam (short for Adaptive Moment Estimation) is an update to the RMSProp optimizer. In this optimization algorithm, running averages of both the gradients and the second moments of the gradients are used.","e08969df":"# Stochastic gradient\n\nStochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the computational burden, achieving faster iterations in trade for a lower convergence rate. descent (SGD)","ebd5002c":"## Parameters","3593a0ed":"## Mini-batches gradients","18ed7462":"# Goal\n\nThe goal of this notebook is to show different optimizers implemented with Numpy.<\/br>\nI will also compare the results for each optimizer.","f320b814":"## Optimizers","cef5c084":"# Nesterov Momentum\n\nNesterov momentum, or Nesterov Accelerated Gradient (NAG), is a slightly modified version of Stochastic Gradient Descent Momentum with stronger theoretical convergence guarantees for convex functions. In practice, it has produced slightly better results than classical Momentum."}}