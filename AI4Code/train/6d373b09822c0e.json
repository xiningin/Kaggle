{"cell_type":{"982b63db":"code","11aed098":"code","2f7195bb":"code","3d9c8157":"code","28eae4f0":"code","087cf051":"code","2d132d82":"code","eebd88a2":"code","c748a744":"code","4332afb0":"code","c72a517c":"code","7ebac9c0":"code","ab331c02":"code","54aaf3e2":"code","b3da8cd3":"code","a5a1a235":"code","43fe186d":"code","cafa22e6":"code","395c176d":"code","0dcfaf7d":"code","22c4c201":"code","a2bae447":"code","7cb1def6":"code","c76a802c":"code","42a08f19":"code","64f757c6":"code","d923481e":"code","1d1b4201":"code","f589f5cb":"code","155ee736":"code","9161cc1c":"code","c526a107":"code","f6b7a4cb":"code","fbc552a5":"code","d05e78ab":"code","d5d776ed":"code","51c3f0d9":"code","04e1f07c":"code","d3625f80":"code","56c6d68a":"code","a8594966":"code","57e4738d":"code","a58e37fc":"code","dd59e641":"code","00f9a180":"code","b6991754":"code","dbd6178b":"code","129aeca7":"code","9e14dd53":"code","2eae37a9":"code","cc3f0b4f":"code","4b550703":"code","2260ce43":"code","59df2412":"code","475e4bba":"code","a5f5fe9a":"code","4a5459b4":"code","bba3f388":"code","1a792c7d":"code","16f076e5":"code","990557fe":"code","7e3097c4":"code","c3896c28":"code","27751fd7":"code","74028ee4":"code","4538be58":"code","53997a10":"code","f0b62793":"code","e25b5091":"code","df6bff85":"code","a7bd5aa5":"code","0836174c":"code","2fe1a89f":"code","70ad1934":"code","c9b93b17":"code","88ba36cc":"code","c448d3e3":"code","5fea3214":"code","efbdd034":"markdown","db59367e":"markdown","8b6ab450":"markdown","e9908a33":"markdown","94cceb88":"markdown","fd641ba7":"markdown","ec0fed72":"markdown","5b9a0d3d":"markdown","b121f1b7":"markdown","1ba64aba":"markdown","73210dd2":"markdown","61612a34":"markdown","ddd45a95":"markdown","ddba112e":"markdown"},"source":{"982b63db":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","11aed098":"## \ubaa9\ud45c: \ub300\ucd9c\uc744 \ubc1b\uc740 \uace0\uac1d\uc774 \uc0c1\ud658\ub2a5\ub825\uc774 \uc788\ub294\uc9c0 \uc5c6\ub294\uc9c0\ub97c \ubd84\ub958\ud558\ub294 \uc5d0\uce21 \ubaa8\ub378\uc744 \ub9cc\ub4dc\ub294 \uac83","2f7195bb":"# numpy and pandas for data manipulation\nimport numpy as np\nimport pandas as pd\n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# File system management\nimport os\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","3d9c8157":"# List files available\nprint(os.listdir(\"..\/input\/\"))","28eae4f0":"# Training data\napp_train = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","087cf051":"# Testing data\napp_test = pd.read_csv('..\/input\/home-credit-default-risk\/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","2d132d82":"## The test set is considerably smaller and lacks a Target column","eebd88a2":"app_train['TARGET'].value_counts()","c748a744":"app_train['TARGET'].astype(int).plot.hist();","4332afb0":"### \uc774 \uc815\ubcf4\uc5d0\uc11c \uc6b0\ub9ac\ub294 \uc774\uac83\uc774 \ubd88\uade0\ud615 \ud074\ub798\uc2a4 \ubb38\uc81c\uc784\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n### \uac1a\uc9c0 \uc54a\uc740 \ub300\ucd9c\ubcf4\ub2e4 \uc81c\ub54c \uac1a\uc740 \ub300\ucd9c\uc774 \ud6e8\uc52c \ub354 \ub9ce\ub2e4. \n### \uc880 \ub354 \uc815\uad50\ud55c \uae30\uacc4 \ud559\uc2b5 \ubaa8\ub378\uc5d0 \ub4e4\uc5b4\uac00\uba74 \uc774\ub7ec\ud55c \ubd88\uade0\ud615\uc744 \ubc18\uc601\ud558\uae30 \uc704\ud574 \ub370\uc774\ud130\uc758 \ud45c\ud604\uc5d0 \ub530\ub77c \ud074\ub798\uc2a4\uc5d0 \uac00\uc911\uce58\ub97c \ubd80\uc5ec\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.","c72a517c":"# Function to calculate missing values by column Funct\n\ndef missing_values_table(df):\n    # Total missing values\n    mis_val = df.isnull().sum()\n    \n    # Percentage of missing values\n    mis_val_percent = 100 * df.isnull().sum()\/len(df)\n    \n    # Make a table with the results\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n    \n    # Rename the columns\n    mis_val_table_ren_columns = mis_val_table.rename(columns = {0: 'Missing Values', 1: '% of Total Values'})\n    \n    # Sort the table by percentage of missing descending\n    mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1]!=0].sort_values(\n    '% of Total Values', ascending=False).round(1)\n    \n    #Print some summary information\n    print(\"Your selected dataframe has\" + str(df.shape[1])+\"columns.\\n\"\n         \"There are\" + str(mis_val_table_ren_columns.shape[0])+\n         \"columns that have missing values.\")\n    \n    # Return the dataframe with missing information\n    return mis_val_table_ren_columns\n    ","7ebac9c0":"# Missing values statistics\nmissing_values = missing_values_table(app_train)\nmissing_values.head(20)","ab331c02":"### \uae30\uacc4 \ud559\uc2b5 \ubaa8\ub378\uc744 \uad6c\ucd95\ud560 \ub54c\uac00 \ub418\uba74 \uc774\ub7ec\ud55c \uacb0\uce21\uac12\uc744 \ucc44\uc6cc\uc57c \ud569\ub2c8\ub2e4\n### \uc774\ud6c4 \uc791\uc5c5\uc5d0\uc11c \uc6b0\ub9ac\ub294 \ub300\uce58\ud560 \ud544\uc694 \uc5c6\uc774 \uacb0\uce21\uac12\uc744 \ucc98\ub9ac\ud560 \uc218 \uc788\ub294 XGBoost\uc640 \uac19\uc740 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4. \n### \ub610 \ub2e4\ub978 \uc635\uc158\uc740 \ub204\ub77d\ub41c \uac12\uc758 \ube44\uc728\uc774 \ub192\uc740 \uc5f4\uc744 \uc0ad\uc81c\ud558\ub294 \uac83\uc774\uc9c0\ub9cc \uc774\ub7ec\ud55c \uc5f4\uc774 \uc6b0\ub9ac \ubaa8\ub378\uc5d0 \ub3c4\uc6c0\uc774 \ub420\uc9c0 \ubbf8\ub9ac \uc54c \uc218\ub294 \uc5c6\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c \uc9c0\uae08\uc740 \ubaa8\ub4e0 \uc5f4\uc744 \uc720\uc9c0\ud569\ub2c8\ub2e4.","54aaf3e2":"## Number of each type of column\napp_train.dtypes.value_counts()","b3da8cd3":"# Number of unique classes in each object column\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis=0)","a5a1a235":"### Encoding Categorical Variables\n\n# 1.Label encoding: \ubc94\uc8fc\ud615 \ubcc0\uc218\uc758 \uac01 \uace0\uc720 \ubc94\uc8fc\ub97c \uc815\uc218\ub85c \ud560\ub2f9\ud569\ub2c8\ub2e4. \uc0c8 \uc5f4\uc774 \uc0dd\uc131\ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\n\n# 2. One-hot encoding: \ubc94\uc8fc\ud615 \ubcc0\uc218\uc758 \uac01 \uace0\uc720 \ubc94\uc8fc\uc5d0 \ub300\ud574 \uc0c8 \uc5f4\uc744 \ub9cc\ub4ed\ub2c8\ub2e4. \uac01 \uad00\ucc30\uc740 \ud574\ub2f9 \ubc94\uc8fc\uc5d0 \ub300\ud574 \uc5f4\uc5d0 1\uc744 \ubc1b\uace0 \ub2e4\ub978 \ubaa8\ub4e0 \uc0c8 \uc5f4\uc5d0 0\uc744 \ubc1b\uc2b5\ub2c8\ub2e4.\n\n## \ub808\uc774\ube14 \uc778\ucf54\ub529\uc758 \ubb38\uc81c\ub294 \ubc94\uc8fc\uc5d0 \uc784\uc758\uc758 \uc21c\uc11c\ub97c \ubd80\uc5ec\ud55c\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4. \n## \uac01 \ubc94\uc8fc\uc5d0 \ud560\ub2f9\ub41c \uac12\uc740 \ubb34\uc791\uc704\uc774\uba70 \ubc94\uc8fc\uc758 \uace0\uc720\ud55c \uce21\uba74\uc744 \ubc18\uc601\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\n## \ub530\ub77c\uc11c \ub808\uc774\ube14 \uc778\ucf54\ub529\uc744 \uc218\ud589\ud560 \ub54c \ubaa8\ub378\uc740 \uae30\ub2a5\uc758 \uc0c1\ub300 \uac12(\uc608: \ud504\ub85c\uadf8\ub798\uba38 = 4 \ubc0f \ub370\uc774\ud130 \uacfc\ud559\uc790 = 1)\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc6b0\ub9ac\uac00 \uc6d0\ud558\ub294 \uac83\uc774 \uc544\ub2cc \uac00\uc911\uce58\ub97c \ud560\ub2f9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \n## \ubc94\uc8fc\ud615 \ubcc0\uc218(\uc608: \ub0a8\uc131\/\uc5ec\uc131)\uc5d0 \ub300\ud574 \uace0\uc720\ud55c \uac12\uc774 \ub450 \uac1c\ubfd0\uc778 \uacbd\uc6b0 \ub808\uc774\ube14 \uc778\ucf54\ub529\uc740 \uad1c\ucc2e\uc9c0\ub9cc \uace0\uc720 \ubc94\uc8fc\uac00 2\uac1c \uc774\uc0c1\uc778 \uacbd\uc6b0 \uc6d0 \ud56b \uc778\ucf54\ub529\uc774 \uc548\uc804\ud55c \uc635\uc158\uc785\ub2c8\ub2e4.\n## \ud074\ub798\uc2a4\uac00 \ub9ce\uc740 \ubc94\uc8fc\ud615 \ubcc0\uc218\uc758 \uacbd\uc6b0 \uc6d0-\ud56b \uc778\ucf54\ub529\uc774 \ubc94\uc8fc\uc5d0 \uc784\uc758\uc758 \uac12\uc744 \ubd80\uacfc\ud558\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0 \uac00\uc7a5 \uc548\uc804\ud55c \uc811\uadfc \ubc29\uc2dd\uc774\ub77c\uace0 \uc0dd\uac01\ud569\ub2c8\ub2e4. \n## \uc6d0-\ud56b \uc778\ucf54\ub529\uc758 \uc720\uc77c\ud55c \ub2e8\uc810\uc740 \uae30\ub2a5\uc758 \uc218(\ub370\uc774\ud130 \ucc28\uc6d0)\uac00 \ub9ce\uc740 \ubc94\uc8fc\uc758 \ubc94\uc8fc\ud615 \ubcc0\uc218\ub85c \ud3ed\ubc1c\ud560 \uc218 \uc788\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4. \n## \uc774\ub97c \ucc98\ub9ac\ud558\uae30 \uc704\ud574 \uc6d0-\ud56b \uc778\ucf54\ub529\uc744 \uc218\ud589\ud55c \ud6c4 PCA \ub610\ub294 \uae30\ud0c0 \ucc28\uc6d0 \ucd95\uc18c \ubc29\ubc95\uc744 \uc218\ud589\ud558\uc5ec \ucc28\uc6d0 \uc218\ub97c \uc904\uc77c \uc218 \uc788\uc2b5\ub2c8\ub2e4.","43fe186d":"## Label Encoding and One-Hot Encoding\n\n# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        # IF 2 or fewer unique categories\n        if len(list(app_train[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(app_train[col])\n            # Transform both training and testing data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            #Keep track of how many columns were label encoded\n            le_count += 1\n            \n        print('%d columns were label encoded.'%le_count)","cafa22e6":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Feature shape: ', app_test.shape)","395c176d":"# Aligning Training and Testing Data\n\ntrain_labels = app_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join='inner', axis=1)\n## \ub450 \uac1d\uccb4 \ub458 \ub2e4 \uc788\ub294 \uc778\ub371\uc2a4\ub97c \uad50\ucc28\n\n# Add the target back in\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Feature shape: ', app_test.shape)","0dcfaf7d":"# DAYS_BIRTH \uc5f4\uc758 \uc22b\uc790\ub294 \ud604\uc7ac \ub300\ucd9c \uc2e0\uccad\uc744 \uae30\uc900\uc73c\ub85c \uae30\ub85d\ub418\uae30 \ub54c\ubb38\uc5d0 \uc74c\uc218\uc785\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ud1b5\uacc4\ub97c \uc5f0\ub3c4 \ub2e8\uc704\ub85c \ubcf4\ub824\uba74 -1\uc744 \uacf1\ud558\uace0 1\ub144\uc758 \uc77c\uc218\ub85c \ub098\ub20c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n# \uc774\uc0c1\uce58\ub294 \uc5c6\ub294 \uac83 \uac19\n\n(app_train['DAYS_BIRTH']\/-365).describe()","22c4c201":"app_train['DAYS_EMPLOYED'].describe()","a2bae447":"app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","7cb1def6":"anom = app_train[app_train['DAYS_EMPLOYED']==365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED']!=365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100*non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100*anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment'% len(anom))","c76a802c":"### \uc608\uc678 \ucc98\ub9ac\ub294 \uc815\ud574\uc9c4 \uaddc\uce59 \uc5c6\uc774 \uc815\ud655\ud55c \uc0c1\ud669\uc5d0 \ub530\ub77c \ub2e4\ub985\ub2c8\ub2e4. \n### \uac00\uc7a5 \uc548\uc804\ud55c \uc811\uadfc \ubc29\uc2dd \uc911 \ud558\ub098\ub294 \uc608\uc678\ub97c \uacb0\uce21\uac12\uc73c\ub85c \uc124\uc815\ud55c \ub2e4\uc74c \uae30\uacc4 \ud559\uc2b5 \uc804\uc5d0 (\ub300\uce58\ub97c \uc0ac\uc6a9\ud558\uc5ec) \ucc44\uc6b0\ub294 \uac83\uc785\ub2c8\ub2e4. \n### \uc774 \uacbd\uc6b0 \ubaa8\ub4e0 \ubcc0\uce59\uc758 \uac12\uc774 \uc815\ud655\ud788 \uac19\uc73c\ubbc0\ub85c \uc774\ub7ec\ud55c \ubaa8\ub4e0 \ub300\ucd9c\uc774 \uacf5\ud1b5\uc810\uc744 \uacf5\uc720\ud560 \uacbd\uc6b0\ub97c \ub300\ube44\ud558\uc5ec \ub3d9\uc77c\ud55c \uac12\uc73c\ub85c \ucc44\uc6b0\uace0\uc790 \ud569\ub2c8\ub2e4. \n### \ube44\uc815\uc0c1\uc801\uc778 \uac12\uc740 \uc5b4\ub290 \uc815\ub3c4 \uc911\uc694\ud55c \uac83 \uac19\uc73c\ubbc0\ub85c \uc2e4\uc81c\ub85c \uc774 \uac12\uc744 \ucc44\uc6e0\ub294\uc9c0 \uba38\uc2e0 \ub7ec\ub2dd \ubaa8\ub378\uc5d0 \uc54c\ub9ac\uace0 \uc2f6\uc2b5\ub2c8\ub2e4. \n### \ud574\uacb0\ucc45\uc73c\ub85c \ube44\uc815\uc0c1\uc801\uc778 \uac12\uc744 \uc22b\uc790(np.nan)\uac00 \uc544\ub2cc \uac12\uc73c\ub85c \ucc44\uc6b4 \ub2e4\uc74c \uac12\uc774 \ube44\uc815\uc0c1\uc801\uc778\uc9c0 \uc5ec\ubd80\ub97c \ub098\ud0c0\ub0b4\ub294 \uc0c8 \ubd80\uc6b8 \uc5f4\uc744 \ub9cc\ub4ed\ub2c8\ub2e4.","42a08f19":"# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train['DAYS_EMPLOYED'] == 365243\n\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243:np.nan}, inplace=True)\n\napp_train['DAYS_EMPLOYED'].plot.hist(title='Days Employment Histogram');\nplt.xlabel('Days Employment');","64f757c6":"## \ud6c8\ub828 \ub370\uc774\ud130\uc5d0 \ub300\ud574 \uc218\ud589\ud558\ub294 \ubaa8\ub4e0 \uc791\uc5c5\uc740 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc5d0\ub3c4 \uc218\ud589\ud574\uc57c \ud569\ub2c8\ub2e4. \n## \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc5d0\uc11c \uc0c8 \uc5f4\uc744 \ub9cc\ub4e4\uace0 \uae30\uc874 \uc5f4\uc744 np.nan\uc73c\ub85c \ucc44\uc6b0\ub3c4\ub85d \ud569\uc2dc\ub2e4.","d923481e":"app_test['DAYS_EMPLOYED_ANOM'] = app_test['DAYS_EMPLOYED']==365243\napp_test['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace=True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (app_test['DAYS_EMPLOYED_ANOM'].sum(), len(app_test)))","1d1b4201":"# Find correlations with the target and sort\ncorrelations = app_train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","f589f5cb":"### \uc124\uba85\uc11c\ub97c \ubcf4\uba74 DAYS_BIRTH\ub294 \ub300\ucd9c \ub2f9\uc2dc \ud074\ub77c\uc774\uc5b8\ud2b8\uc758 \ub098\uc774\ub85c \uc74c\uc218\uc77c(\uc774\uc720\uac00 \ubb34\uc5c7\uc774\ub4e0!)\uc785\ub2c8\ub2e4. \n### \uc0c1\uad00 \uad00\uacc4\ub294 \uc591\uc218\uc774\uc9c0\ub9cc \uc774 \uae30\ub2a5\uc758 \uac12\uc740 \uc2e4\uc81c\ub85c \uc74c\uc218\uc785\ub2c8\ub2e4. \n### \uc989, \uace0\uac1d\uc774 \ub098\uc774\uac00 \ub4e4\uc218\ub85d \ub300\ucd9c \ubd88\uc774\ud589 \uac00\ub2a5\uc131\uc774 \uc904\uc5b4\ub4ed\ub2c8\ub2e4(\uc989, \ubaa9\ud45c == 0). \n### \uc774\uac83\uc740 \uc57d\uac04 \ud63c\ub780\uc2a4\ub7fd\uae30 \ub54c\ubb38\uc5d0 \ud2b9\uc131\uc758 \uc808\ub300\uac12\uc744 \ucde8\ud558\uba74 \uc0c1\uad00 \uad00\uacc4\uac00 \uc74c\uc218\uac00 \ub429\ub2c8\ub2e4.","155ee736":"# Find the correlation of the positive days since birth and target\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])","9161cc1c":"# Set the style of plots\nplt.style.use('fivethirtyeight')\n\n# Plot the distribution of ages in years\nplt.hist(app_train['DAYS_BIRTH']\/365, edgecolor = 'k', bins=25)\nplt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');","c526a107":"plt.figure(figsize = (10,8))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET']==0, 'DAYS_BIRTH']\/365, label='target==0')\n\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET']==1, 'DAYS_BIRTH']\/365, label='target==1')\n\n# Labeling of plot\nplt.xlabel('Age(years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');","f6b7a4cb":"# Age information into a separate dataframe\nage_data = app_train[['TARGET','DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH']\/365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins=np.linspace(20,70,num=11))\nage_data.head(10)","fbc552a5":"# Group by the bin and calculate averages\nage_groups = age_data.groupby('YEARS_BINNED').mean()\nage_groups","d05e78ab":"plt.figure(figsize=(8,8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100*age_groups['TARGET'])\n\n# plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay(%)')\nplt.title('Failure to Repay by Age Group');","d5d776ed":"### \ubd84\uba85\ud55c \ucd94\uc138\uac00 \uc788\uc2b5\ub2c8\ub2e4.\n### \uc80a\uc740 \uc9c0\uc6d0\uc790\ub294 \ub300\ucd9c\uc744 \uc0c1\ud658\ud558\uc9c0 \uc54a\uc744 \uac00\ub2a5\uc131\uc774 \ub354 \ub192\uc2b5\ub2c8\ub2e4\n### \uc5f0\uccb4\uc728\uc740 \ucd5c\uc5f0\uc18c 3\uc138 10% \uc774\uc0c1, \uace0\ub839 5% \ubbf8\ub9cc\uc774\ub2e4.","51c3f0d9":"# Extract the EXT_SOURCE variables and show correlations\next_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2',\n                     'EXT_SOURCE_3','DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","04e1f07c":"plt.figure(figsize=(8,6))\n\n# Heatmap of correlations\nsns.heatmap(ext_data_corrs, cmap=plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax=0.6)\nplt.title('Correlation Heatmap');","d3625f80":"plt.figure(figsize = (10,12))\n\n# iterate through the sources\nfor i,source in enumerate(['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3']):\n    \n    #create a new subplot for each source\n    plt.subplot(3,1,i+1)\n    \n    #plot repaid loans\n    sns.kdeplot(app_train.loc[app_train['TARGET']==0, source], label='target==0')\n    \n    #plot loans that were not repaid\n    sns.kdeplot(app_train.loc[app_train['TARGET']==1, source],label='target==1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n\n    plt.tight_layout(h_pad = 2.5)","56c6d68a":"## Pairs Plot\uc740 \ub2e8\uc77c \ubcc0\uc218\uc758 \ubd84\ud3ec\ubfd0\ub9cc \uc544\ub2c8\ub77c \uc5ec\ub7ec \uc30d\uc758 \ubcc0\uc218 \uac04\uc758 \uad00\uacc4\ub97c \ubcfc \uc218 \uc788\uac8c \ud574\uc90c","a8594966":"# Copy the data for plotting\nplot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n\n# Add in the age of the client in years\nplot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n\n# Drop na values and limit to first 100000 rows\nplot_data = plot_data.dropna().loc[:100000,:]\n\n# Function to calculate correlation coefficient between two columns\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x,y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r={:.2f}\".format(r),\n               xy=(.2, .8), xycoords=ax.transAxes,\n               size = 20)\n    \n# Create the pairgrid object\ngrid = sns.PairGrid(data=plot_data, size=3, diag_sharey=False,\n                   hue = 'TARGET',\n                   vars = [x for x in list(plot_data.columns)\n                          if x != 'TARGET'])\n\n# Upper is a scatter plot\ngrid.map_upper(plt.scatter, alpha = 0.2)\n\n# Diagonal is a histogram\ngrid.map_diag(sns.kdeplot)\n\n# Bottom is density plot\ngrid.map_lower(sns.kdeplot, cmap=plt.cm.OrRd_r);\n\nplt.suptitle('Ext Source and Age Features Pairs Plot', size=32, y=1.05);","57e4738d":"### \ud53c\uccd0 \uc5d4\uc9c0\ub2c8\uc5b4\ub9c1\uc740 \uc77c\ubc18\uc801\uc778 \ud504\ub85c\uc138\uc2a4\ub97c \ub9d0\ud558\uba70 \ud53c\uccd0 \uad6c\uc131(\uae30\uc874 \ub370\uc774\ud130\uc5d0\uc11c \uc0c8 \ud53c\uccd0 \ucd94\uac00)\uacfc \ud53c\uccd0 \uc120\ud0dd(\uac00\uc7a5 \uc911\uc694\ud55c \ud53c\uccd0\ub9cc \uc120\ud0dd\ud558\uac70\ub098 \ub2e4\ub978 \ucc28\uc6d0 \ucd95\uc18c \ubc29\ubc95)\uc744 \ubaa8\ub450 \ud3ec\ud568\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \n### \ud53c\ucc98\ub97c \uc0dd\uc131\ud558\uace0 \ud53c\ucc98\ub97c \uc120\ud0dd\ud558\ub294 \ub370 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \ub9ce\uc740 \uae30\uc220\uc774 \uc788\uc2b5\ub2c8\ub2e4.","a58e37fc":"## \ub2e4\ud56d \ud68c\uadc0\ub780, \ub370\uc774\ud130\ub4e4 \uac04\uc758 \ud615\ud0dc\uac00 \ube44\uc120\ud615\uc77c \ub54c \ub370\uc774\ud130\uc5d0 \uac01 \ud2b9\uc131\uc758 \uc81c\uacf1\uc744 \ucd94\uac00\ud574 \uc8fc\uc5b4\uc11c \ud2b9\uc131\uc774 \ucd94\uac00\ub41c \ube44\uc120\ud615 \ub370\uc774\ud130\ub97c \uc120\ud615 \ud68c\uadc0 \ubaa8\ub378\ub85c \ud6c8\ub828\uc2dc\ud0a4\ub294 \ubc29\ubc95\n\n## Scikit-Learn\uc5d0\ub294 \uc9c0\uc815\ub41c \uc815\ub3c4\uae4c\uc9c0 \ub2e4\ud56d\uc2dd\uacfc \uc0c1\ud638 \uc791\uc6a9 \ud56d\uc744 \uc0dd\uc131\ud558\ub294 PolynomialFeatures\ub77c\ub294 \uc720\uc6a9\ud55c \ud074\ub798\uc2a4\uac00 \uc788\uc2b5\ub2c8\ub2e4. \n## \uacb0\uacfc\ub97c \ubcf4\uae30 \uc704\ud574 \ucc28\uc218 3\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4(\ub2e4\ud56d\uc2dd \ud2b9\uc9d5\uc744 \uc0dd\uc131\ud560 \ub54c \ud2b9\uc9d5\uc758 \uc218\uac00 \ucc28\uc218\uc5d0 \ub530\ub77c \uae30\ud558\uae09\uc218\uc801\uc73c\ub85c \ud655\uc7a5\ub418\uace0 \ubb38\uc81c\uac00 \ubc1c\uc0dd\ud560 \uc218 \uc788\uae30 \ub54c\ubb38\uc5d0 \ub108\ubb34 \ub192\uc740 \ucc28\uc218\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc744 \ud53c\ud558\uace0 \uc2f6\uc2b5\ub2c8\ub2e4. \uacfc\uc801\ud569)","dd59e641":"# Make a new dataframe for polynomial features\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH','TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']]\n\n# imputer for handling missing values\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy = 'median')\n\npoly_target = poly_features['TARGET']\n\npoly_features = poly_features.drop(columns=['TARGET'])\n\n# Need to impute missing values\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.transform(poly_features_test)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Create the polynomail object with specified degree\npoly_transformer = PolynomialFeatures(degree=3)","00f9a180":"# Train the polynomial features\npoly_transformer.fit(poly_features)\n\n# Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\n\nprint('Polynomial Features shape: ', poly_features.shape)","b6991754":"## \ub2e4\ud56d\uc2dd \uae30\ub2a5\n\npoly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH'])[:15]","dbd6178b":"# Create a dataframe of the features\npoly_features = pd.DataFrame(poly_features,\n                            columns = poly_transformer.get_feature_names(['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']))\n\n# Add in the target\npoly_features['TARGET'] = poly_target\n\n# Find the correlations with the target\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\n# Display most negative and most positive\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail(5))","129aeca7":"# Put test features into dataframe\npoly_features_test = pd.DataFrame(poly_features_test, columns=poly_transformer.get_feature_names(['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']))\n\n# Merge polynomial features into training dataframe\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how='left')\n\n# Merge polnomial features into testing dataframe\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how='left')\n\n# Align the dataframes\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join='inner', axis=1)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape: ', app_train_poly.shape)\nprint('Testing data with polynomial features shape: ', app_test_poly.shape)","9e14dd53":"app_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT']\/app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY']\/app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY']\/app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED']\/app_train_domain['DAYS_BIRTH']","2eae37a9":"app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] \/ app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] \/ app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] \/ app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] \/ app_test_domain['DAYS_BIRTH']","cc3f0b4f":"plt.figure(figsize = (12,20))\n\n# iterate through the new features\nfor i, feature in enumerate(['CREDIT_INCOME_PERCENT','ANNUITY_INCOME_PERCENT','CREDIT_TERM','DAYS_EMPLOYED_PERCENT']):\n    \n    # create a new subplot for each source\n    plt.subplot(4, 1, i+1)\n    \n    # plot repaid loans\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET']==0, feature], label='target==0')\n    \n    # plot loans that were not repaid\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET']==1, feature], label='target==1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % feature)\n    plt.xlabel('%s' % feature); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","4b550703":"### \uae30\uc900\uc120\uc744 \uc5bb\uae30 \uc704\ud574 \ubc94\uc8fc\ud615 \ubcc0\uc218\ub97c \uc778\ucf54\ub529\ud55c \ud6c4 \ubaa8\ub4e0 \uae30\ub2a5\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \n### \ub204\ub77d\ub41c \uac12\uc744 \ucc44\uc6b0\uace0(\ub300\uce58) \uae30\ub2a5\uc758 \ubc94\uc704\ub97c \uc815\uaddc\ud654(\uae30\ub2a5 \uc2a4\ucf00\uc77c\ub9c1)\ud558\uc5ec \ub370\uc774\ud130\ub97c \uc804\ucc98\ub9ac\ud569\ub2c8\ub2e4. ","2260ce43":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\n# Drop the target from the training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns=['TARGET'])\nelse:\n    train = app_train.copy()\n    \n# Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = app_test.copy()\n\n# Median imputation of missing values\nimputer = SimpleImputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0,1))\n\n# Fit on the training data\nimputer.fit(train)\n\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)\n\n# Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","59df2412":"# \uccab \ubc88\uc9f8 \ubaa8\ub378\uc5d0 \ub300\ud574 Scikit-Learn\uc758 LogisticRegression\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \n# \uae30\ubcf8 \ubaa8\ub378 \uc124\uc815\uc5d0\uc11c \ubcc0\uacbd\ud560 \uc218 \uc788\ub294 \uc720\uc77c\ud55c \ubcc0\uacbd \uc0ac\ud56d\uc740 \uacfc\uc801\ud569\uc758 \uc591\uc744 \uc81c\uc5b4\ud558\ub294 \uc815\uaddc\ud654 \ub9e4\uac1c\ubcc0\uc218 C\ub97c \ub0ae\ucd94\ub294 \uac83\uc785\ub2c8\ub2e4(\ub0ae\uc740 \uac12\uc740 \uacfc\uc801\ud569\uc744 \uc904\uc5ec\uc57c \ud568). \n# \uc774\ub807\uac8c \ud558\uba74 \uae30\ubcf8 LogisticRegression\ubcf4\ub2e4 \uc57d\uac04 \ub354 \ub098\uc740 \uacb0\uacfc\ub97c \uc5bb\uc744 \uc218 \uc788\uc9c0\ub9cc \ud5a5\ud6c4 \ubaa8\ub378\uc5d0 \ub300\ud574\uc11c\ub294 \uc5ec\uc804\ud788 \ub0ae\uc740 \uae30\uc900\uc744 \uc124\uc815\ud569\ub2c8\ub2e4.","475e4bba":"from sklearn.linear_model import LogisticRegression\n\n# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n\n# Train on the training data\nlog_reg.fit(train, train_labels)","a5f5fe9a":"## \uccab\ubc88\uc9f8 \uc5f4\uc740 \ub300\uc0c1\uc774 0\uc77c \ud655\ub960\uc774\uace0 \ub450\ubc88\uc9f8 \uc5f4\uc740 \ub300\uc0c1\uc774 1\uc77c \ud655\ub960\n## \ub2e8\uc77c \ud589\uc758 \uacbd\uc6b0, \ub450 \uc5f4\uc758 \ud569\uc774 1\uc774 \ub418\uc5b4\uc57c \ud568\n## \uc6b0\ub9ac\ub294 \ub300\ucd9c\uc774 \uc0c1\ud658\ub418\uc9c0 \uc54a\uc744 \ud655\ub960\uc744 \uc6d0\ud558\ubbc0\ub85c \ub450 \ubc88\uc9f8 \uc5f4\uc744 \uc120\ud0dd","4a5459b4":"# Make predictions\n# Make sure to select the second column only\nlog_reg_pred = log_reg.predict_proba(test)[:,1]","bba3f388":"# Submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","1a792c7d":"# Save the submission to a csv file\nsubmit.to_csv('log_reg_baseline.csv', index=False)","16f076e5":"from sklearn.ensemble import RandomForestClassifier\n\n# Make the random forest classifier\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose=1, n_jobs=-1)","990557fe":"# Train on the training data\nrandom_forest.fit(train, train_labels)\n\n# Extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature':features, 'importance': feature_importance_values})\n\n# Make predictions on the test data\npredictions = random_forest.predict_proba(test)[:, 1]","7e3097c4":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline.csv', index = False)","c3896c28":"poly_features_names = list(app_train_poly.columns)\n\n# Impute the polynomial features\nimputer = SimpleImputer(strategy = 'median')\n\npoly_features = imputer.fit_transform(app_train_poly)\npoly_features_test = imputer.transform(app_test_poly)\n\n# Scale the polynomial features\nscaler = MinMaxScaler(feature_range = (0,1))\n\npoly_features = scaler.fit_transform(poly_features)\npoly_features_test = scaler.transform(poly_features_test)\n\nrandom_forest_poly = RandomForestClassifier(n_estimators=100,\n                                           random_state = 50, verbose = 1, n_jobs = -1)","27751fd7":"# Train on the training data\nrandom_forest_poly.fit(poly_features, train_labels)\n\n# Make predictions on the test data\npredictions = random_forest_poly.predict_proba(poly_features_test)[:,1]","74028ee4":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline_engineered.csv', index = False)","4538be58":"### Testing Domain Features\n\napp_train_domain = app_train_domain.drop(columns = 'TARGET')\n\ndomain_features_names = list(app_train_domain.columns)\n\n# Impute the domainnomial features\nimputer = SimpleImputer(strategy = 'median')\n\ndomain_features = imputer.fit_transform(app_train_domain)\ndomain_features_test = imputer.transform(app_test_domain)\n\n# Scale the domainnomial features\nscaler = MinMaxScaler(feature_range = (0,1))\n\ndomain_features = scaler.fit_transform(domain_features)\ndomain_features_test = scaler.transform(domain_features_test)\n\nrandom_forest_domain = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose=1, n_jobs = -1)\n\n# Train on the training data\nrandom_forest_domain.fit(domain_features, train_labels)\n\n# Extract feature importances\nfeature_importance_values_domain = random_forest_domain.feature_importances_\nfeature_importance_domain = pd.DataFrame({'feature':domain_features_names,'importance':feature_importance_values_domain})\n\n# Make predictions on the test data\npredictions = random_forest_domain.predict_proba(domain_features_test)[:, 1]","53997a10":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline_domain.csv', index=False)","f0b62793":"def plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","e25b5091":"# Show the feature importances for the default features\nfeature_importances_sorted = plot_feature_importances(feature_importances)","df6bff85":"### LightGBM\uc740 \ud604\uc7ac \uad6c\uc870\ud654\ub41c \ub370\uc774\ud130 \uc138\ud2b8(\ud2b9\ud788 Kaggle\uc5d0\uc11c) \ud559\uc2b5\uc744 \uc704\ud55c \ucd5c\uace0\uc758 \ubaa8\ub378","a7bd5aa5":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport gc\n\ndef model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            # \uc624\ube0c\uc81d\ud2b8\uc774\uba74\n            if features[col].dtype == 'object':\n                \n                #\ub77c\ubca8 \uc778\ucf54\ub354\ub97c \uc9c4\ud589\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object \n    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n    \n    # Empty array for feature importances\n    # \ud2b9\uc131 \uc911\uc694\ub3c4 \ubc30\uc5f4\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    # \ud14c\uc2a4\ud2b8 \uc608\uce21\ud588\uc744 \ub54c \ub123\uc744 \ubc30\uc5f4\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    # validation\ud560 \ub54c \ub098\uc624\ub294 \uc608\uce21\uac12\ub4e4 \ub123\ub294 \ubc30\uc5f4\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # \ud55c \ubc88 kfold\ub97c \ub3cc\uace0 \ubaa8\ub378\uc744 \ud6c8\ub828\uc2dc\ud0a8 \ub4a4 \ucd5c\uace0 \uc131\uc801\uc744 \uac00\uc838\uc624\uace0\n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        # \ud55c \ubc88 kfold \ub3cc\uace0 \ubaa8\ub378\uc744 \ud6c8\ub828\uc2dc\ud0a8 \ub4a4 \uadf8 \ud6c8\ub828\ud560 \ub54c \uc911\uc694\ud558\uac8c \ubcf8 \ud2b9\uc131\uc744 \uac00\uc838\uc640\uc11c feature_importance_values\uc5d0 \ub123\uc5b4\uc900\ub2e4.\n        # kfold\ub9cc\ud07c \ub3c4\ub2c8\uae4c \ub098\ub220\uc900\ub2e4\n        # \uc5ec\uae30\uc11c\ub294 \ud2b9\uc131 \uc911\uc694\ub3c4\ub97c kfold\ud560 \ub54c\ub9c8\ub2e4 \uc801\uc6a9\uc2dc\ud0a8 \uac83\uc774\ub2c8\uae4c \ub098\ub220\uc918\uc57c \ud55c\ub2e4\n        feature_importance_values += model.feature_importances_ \/ k_fold.n_splits\n        \n        # Make predictions\n        # \uc608\uce21\uc744 \ub123\uc5b4\uc900\ub2e4. \uc5ec\uae30\uc11c\ub3c4 kfold\ub9cc\ud07c \ub3c4\ub2c8\uae50 \ub098\ub220\uc900\ub2e4.\n        # \uc5ec\uae30\uc11c\ub294 test\ub2c8\uae4c test \ub370\uc774\ud130 \uc804\uccb4\ub97c \ub123\uc5b4\uc90c. \n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] \/ k_fold.n_splits\n        \n        # Record the out of fold predictions\n        # validation\uc5d0 \ub300\ud55c \uac12\uc744 \ub123\uc5b4\uc8fc\uace0\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        # \ucd5c\uace0 \uc88b\uc740 \uc131\uc801\uc744 \ubf51\uc544\ub0c4\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    # test\uc5d0 \ub300\ud55c \uc608\uce21\uac12\uc744 \ub123\uc5b4\uc8fc\uace0\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","0836174c":"## Out of Fold(OOF) \ubc29\ubc95\uc73c\ub85c \ubaa8\ub378 \ud3c9\uac00\n### \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\ub294 \ubc29\ubc95\uc73c\ub85c\uc11c, \uc2e4\ubb34\ubcf4\ub2e4\ub294 Kaggle, Dacon\uacfc \uac19\uc740 \uc608\uce21 \uc54c\uace0\ub9ac\uc998 \ub300\ud68c\uc5d0\uc11c \uc790\uc8fc \uc0ac\uc6a9\ub418\ub294 \ubc29\uc2dd\n### K-fold\ub97c \uc774\uc6a9\ud55c \uac83\uc774 OOF\ub77c\uace0 \ud560 \uc218 \uc788\uc73c\uba70, OOF\uc548\uc5d0\ub294 K-fold\uac00 \uc18d\ud55c\ub2e4\uace0 \ubcfc \uc218 \uc788\ub2e4\n\n### \ucc38\uace0 \ub9c1\ud06c: https:\/\/techblog-history-younghunjo1.tistory.com\/142","2fe1a89f":"submission, fi, metrics = model(app_train, app_test)\nprint('Baseline metrics')\nprint(metrics)","70ad1934":"fi_sorted = plot_feature_importances(fi)","c9b93b17":"submission.to_csv('baseline_lgb.csv', index=False)","88ba36cc":"app_train_domain['TARGET'] = train_labels\n\n# Test the domain knowledge features\nsubmission_domain, fi_domain, metrics_domain = model(app_train_domain, app_test_domain)\n\nprint('Baseline with domain knowledge features metrics')\nprint(metrics_domain)","c448d3e3":"fi_sorted = plot_feature_importances(fi_domain)","5fea3214":"submission_domain.to_csv('baseline_lgb_domain_features.csv', index=False)","efbdd034":"### Improved Model: Random Forest","db59367e":"### Conclusions\n\n#### We followed the general outline of a machine learning project:\n\n#### 1. Understand the problem and the data\n#### 2. Data cleaning and formatting (this was mostly done for us)\n#### 3. Exploratory Data Analysis\n#### 4. Baseline model\n#### 5. Improved model\n#### 6. Model interpretation (just a little)","8b6ab450":"### Column Types","e9908a33":"### Feature Engineering","94cceb88":"### Just for Fun: Light Gradient Boosting Machine","fd641ba7":"### Logistic Regression Implementation","ec0fed72":"### Make Predictions using Engineered Features","5b9a0d3d":"### Read in Data","b121f1b7":"#### Examine the Distribution of the Target Column","1ba64aba":"## Effect of Age on Repayment","73210dd2":"### Imports\n\n#### We are using a typical data science stack: numpy, pandas, sklearn, matplotlib","61612a34":"### Back to Exploratory Data Analysis","ddd45a95":"### Polynomial Features","ddba112e":"### Exploratory Data Analysis"}}