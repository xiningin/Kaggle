{"cell_type":{"66d443c8":"code","b9042abf":"code","9f95e1e5":"code","91248663":"code","bb600e45":"code","b51494a9":"code","a254214c":"code","d97ff96e":"code","410f6511":"code","63545475":"code","bfc1f5ed":"markdown","1286331a":"markdown","cd25b6ad":"markdown","9596dc68":"markdown","06e4f8f6":"markdown","46df3d10":"markdown","71c26d8f":"markdown","d2f28ee3":"markdown","772d2ef1":"markdown"},"source":{"66d443c8":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport datetime as dt\nimport pickle\nimport os\nimport inspect\n\nimport lib_accii_53\nimport lib_prepare as prepare\nimport lib_ga","b9042abf":"fn1 = '..\/input\/tr-learning-rate-regul-portf\/model_ga.pkl'\nfn2 = '.\/model_ga.pkl'\n\nif os.path.exists(fn1):\n    os.system(f'cp {fn1} {fn2}')","9f95e1e5":"# %%time\nd_data = lib_accii_53.get_dOHLCV()\nd_data.shape","91248663":"index_data = lib_accii_53.get_index()\nindex_data.shape","bb600e45":"def getQualityMethod(units,\n                     kernel,\n                     patienceEarlyStopping,\n                     patienceReduceLROnPlateau,\n                     window,\n                     batch_size,\n                     gorizont,\n                     train_size,\n                     val_size,\n                     learning_rate_pow,\n                     regimCalc = False,\n                     calc_d_data = None,\n                     calc_index_data = None):\n    \n    import lib_accii_53\n    import lib_prepare as prepare\n    import numpy as np\n    import tensorflow as tf\n    from tensorflow import keras\n    \n    learning_rate = np.power(10.0,learning_rate_pow)\n    \n    output = lib_accii_53.get_pct_change(gorizont).values\n    \n    if regimCalc:\n        inpData1 = prepare.data_to_window(calc_d_data,window)\n        inpData2 = prepare.date_to_input(calc_index_data)\n    else:\n        inpData1 = prepare.data_to_window(d_data,window)\n        inpData2 = prepare.date_to_input(index_data)\n    \n    inpData1Learn,inpData2Learn,inpData1Calc,inpData2Calc = prepare.split_learn_calc(inpData1,inpData2,gorizont=gorizont)\n    \n    if regimCalc:\n        inpData1Learn,inpData2Learn,output,learnDates = prepare.chistim_pustoty(inpData1Learn,\n                                                                                inpData2Learn,\n                                                                                output,\n                                                                                calc_index_data.values[:,np.newaxis])\n    else:\n        inpData1Learn,inpData2Learn,output,learnDates = prepare.chistim_pustoty(inpData1Learn,\n                                                                                inpData2Learn,\n                                                                                output,\n                                                                                index_data.values[:,np.newaxis])\n        \n    X1_train, X1_test, \\\n    X2_train, X2_test,\\\n    y_train, y_test, \\\n    d_train, d_test = prepare.learn_val_random_vib(inpData1Learn,\n                                                   inpData2Learn,\n                                                   output,\n                                                   learnDates,\n                                                   learnVibLen = train_size,\n                                                   valVibLen = val_size)\n    \n    \n    \n    normLayer = keras.layers.experimental.preprocessing.Normalization(axis=-1)\n    if regimCalc:\n        normLayer.adapt(inpData1Learn[-train_size:,...])\n    else:\n        normLayer.adapt(X1_train)\n    \n    input1 = keras.layers.Input(shape=X1_train.shape[1:])\n    x1 = normLayer(input1)\n\n    x1 = keras.layers.Conv1D(units,\n                             kernel,\n                             padding='same',\n                             activation='relu')(x1)\n\n    x1 = keras.layers.Flatten()(x1)\n\n    input2 = keras.layers.Input(shape=(X2_train.shape[1],))\n\n    x = keras.layers.Concatenate()([x1,input2])\n\n    outputs = keras.layers.Dense(y_train.shape[1],activation='softmax',name='output')(x)\n    \n    model = keras.Model((input1,input2),outputs)\n    \n    def my_loss(y_true, y_pred):\n        return -tf.reduce_sum(y_true * y_pred,axis=-1)\n    \n    model.compile(optimizer = tf.optimizers.Adam(learning_rate=learning_rate),\n                  loss = my_loss)\n    \n    cb = [tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                           patience=patienceEarlyStopping,\n                                           restore_best_weights=True,\n                                           verbose=False),\n          tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n                                               patience=patienceReduceLROnPlateau,\n                                               verbose=False)]\n    \n    \n    if regimCalc:\n        \n        model.fit((inpData1Learn[-train_size:,...],inpData2Learn[-train_size:,...]),\n                  list(output[-train_size:,:].T),\n                  epochs=1000,\n                  validation_split=0.3,\n                  verbose=False,\n                  callbacks = cb,\n                  batch_size=batch_size)\n\n        return model.predict((inpData1Calc,inpData2Calc))\n        \n    else:\n    \n        model.fit((X1_train,X2_train),\n                  list(y_train.T),\n                  epochs=1000,\n                  validation_split=0.3,\n                  verbose=False,\n                  callbacks = cb,\n                  batch_size=batch_size)\n\n        y_pred = model.predict((X1_test,X2_test))\n\n        return (y_pred * y_test).sum(axis=1).mean()\/gorizont\n\nwith open('optim_func.py','w') as fp:\n    fp.write(inspect.getsource(getQualityMethod))\n\n# getQualityMethod(units=5,\n#                  kernel=3,\n#                  patienceEarlyStopping=30,\n#                  patienceReduceLROnPlateau=3,\n#                  window=5,\n#                  batch_size=32,\n#                  gorizont=5,\n#                  train_size=700,\n#                  val_size=10,\n#                  learning_rate_pow=-3,\n#                  regimCalc = True,\n#                  calc_d_data = d_data)","b51494a9":"modelGA = lib_ga.ListGenetic( pop_size = 200,\n                              units=range(3,100),\n                              kernel=range(2,10),\n                              patienceEarlyStopping=range(10,100),\n                              patienceReduceLROnPlateau=range(5,100),\n                              window = range(3,100),\n                              batch_size = range(4,128+1),\n                              gorizont = range(1,14+1),\n                              train_size=range(400,1000+1),\n                              val_size=range(7,100),\n                              learning_rate_pow = np.arange(-5,-0.1,0.1),\n                              quality_method=getQualityMethod)","a254214c":"fn = fn2\n\nif os.path.exists(fn):\n    with open(fn,'rb') as fp:\n        modelGA = pickle.load(fp)","d97ff96e":"modelGA.fit(10000,echo_time=10*60,stop_time_sec=8*60*60)\nwith open(fn,'wb') as fp:\n    pickle.dump(modelGA,fp)","410f6511":"modelGA.plot_hist_new(('quality','units','kernel','patienceEarlyStopping','patienceReduceLROnPlateau','window', 'batch_size','gorizont','train_size','val_size','learning_rate_pow'))","63545475":"best_par = modelGA.getBestParams()\n\nwith open('best_par.pkl','wb') as fp:\n    pickle.dump(best_par,fp)\n    \nbest_par","bfc1f5ed":"# \u041f\u0435\u0440\u0435\u0434\u0430\u0447\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u0437 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0435\u0439 \u0432\u0435\u0440\u0441\u0438\u0438","1286331a":"# \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438","cd25b6ad":"# \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438","9596dc68":"# \u041f\u043e\u0434\u043a\u043b\u044e\u0447\u0430\u0435\u043c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438","06e4f8f6":"# \u0426\u0438\u043a\u043b \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438","46df3d10":"# \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u043a\u043e\u0442\u0438\u0440\u043e\u0432\u043e\u043a","71c26d8f":"# \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430, \u0435\u0441\u043b\u0438 \u0443\u0436\u0435 \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442","d2f28ee3":"# \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043b\u0443\u0447\u0448\u0435\u0439 \u043c\u043e\u0434\u0435\u043b\u0438","772d2ef1":"# \u041e\u0442\u0447\u0435\u0442 \u043f\u043e \u0438\u0441\u0442\u043e\u0440\u0438\u0438 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438"}}