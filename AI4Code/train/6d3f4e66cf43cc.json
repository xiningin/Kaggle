{"cell_type":{"d259937c":"code","3d4d5799":"code","5ac580d9":"code","0ce30f3a":"code","6f1f1d7e":"code","0cb1eae4":"code","689131fa":"code","99179aad":"code","11f5425a":"code","7be112d0":"code","1988a7d1":"code","84e1336f":"code","c68a9104":"code","c64981a8":"code","c69b9ff9":"code","89413efa":"code","c656966f":"code","a1c74bb5":"code","f232ef7c":"code","61f99d0e":"code","796d2018":"code","a9119869":"code","df5a88e1":"code","e9f6ed3a":"code","65a38982":"code","6bda9c1a":"code","a309d78f":"code","87eb6425":"code","035cc77e":"code","a39d122f":"code","e0908933":"code","4ac32706":"code","f327db9c":"code","7a19bc20":"code","90a04ea3":"code","c025502c":"code","3ddf4a8f":"code","f96e2ecd":"code","eee968fa":"code","d3b62f32":"code","a385798d":"code","82645a42":"code","22c2ecd9":"code","e127edd8":"code","c6c61eaa":"code","7e39379e":"code","eeb7a1bc":"code","445700dd":"code","c58f2880":"code","c66f94c4":"code","04145ba6":"code","5c44e4b3":"code","48f46032":"code","78fd7f92":"code","3b98bb9c":"code","52e7de79":"code","943f7c7b":"code","edaed31a":"code","dd9cfd37":"code","2d4a796d":"code","666468a3":"code","c39d3a4a":"code","f690534b":"code","1a5d0e70":"code","510f2ce2":"code","48d6cb72":"code","4d72d94a":"code","3c2282b2":"code","1be329f0":"code","61818c23":"code","e76f4df4":"code","fcf230bb":"code","a7f5f401":"code","b33ce12a":"code","2141e29b":"code","e2ae8b4e":"code","5f19fe16":"code","f7dc6c5f":"code","e16fb95e":"code","a722971b":"code","df1ecb59":"code","bbf9dec3":"code","34df51eb":"code","6f16a943":"code","78e28a00":"code","2dfdcb13":"code","fca09a6d":"code","aa3c290b":"code","75f5b5bf":"code","b826c048":"code","2b4d8bf4":"code","b68da5ee":"code","47c244ca":"code","f63baf10":"code","35c866c9":"code","d53cb520":"code","7f9d8b7f":"code","fdc88ae9":"code","a861f5a7":"code","612fa95d":"code","6e3f0e1e":"code","c7a379ab":"code","a1580228":"code","c75a4d9b":"code","ab658ae6":"code","db5c8bfa":"code","fd779335":"code","29297dae":"code","ea2c1508":"code","c73353ec":"code","2e768515":"code","45840f28":"code","7bbc10ce":"markdown","87c07656":"markdown","1dd904ee":"markdown","6399d07d":"markdown","2b6a8dca":"markdown","91b2879e":"markdown","1e39fe22":"markdown","0bcb8b70":"markdown","0e84ad0c":"markdown","252228ac":"markdown","ced095a0":"markdown","ced72654":"markdown","07ad5904":"markdown","571df3cc":"markdown","32bc993e":"markdown","fe3af661":"markdown","484da864":"markdown","b31089ab":"markdown","7a9325bd":"markdown","3b004439":"markdown","f9a9eaf9":"markdown","7c3f1b8a":"markdown","cb94ab48":"markdown","f0f80621":"markdown","1dde6653":"markdown","bd07262f":"markdown","caec61da":"markdown","fca791bc":"markdown","7d4c0ed3":"markdown","840dbe02":"markdown","c915fe43":"markdown"},"source":{"d259937c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","3d4d5799":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5ac580d9":"df = pd.read_excel(r\"\/kaggle\/input\/sales-pipeline-conversion-at-a-saas-startup\/Sales Dataset.xlsx\")\ndf.head()","0ce30f3a":"df.info()","6f1f1d7e":"df.shape","0cb1eae4":"df.describe()","689131fa":"df_dub = df.copy()\n\n# Checking for duplicates and dropping the entire duplicate row if any\ndf_dub.drop_duplicates(subset=None, inplace=True)\ndf_dub.shape","99179aad":"df.shape","11f5425a":"# List of variables to map\n\nvarlist =  ['Opportunity Status']\n\n# Defining the map function\ndef binary_map(x):\n    return x.map({'Won': 1, \"Loss\": 0})\n\n# Applying the function to the housing list\ndf[varlist] = df[varlist].apply(binary_map)\ndf.head()","7be112d0":"df= df.drop(['Opportunity ID'],1)\ndf.head()","1988a7d1":"df['Technology\\nPrimary'].describe()","84e1336f":"df['Technology\\nPrimary'].value_counts()","c68a9104":"plt.figure(figsize = (10,5))\nax= sns.countplot(df['Technology\\nPrimary'])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nax.set_yscale('log')\nplt.xticks(rotation = 90)\nplt.show()","c64981a8":"plt.figure(figsize = (20,6))\nax= sns.countplot(x = \"Technology\\nPrimary\", hue = \"Opportunity Status\", data = df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","c69b9ff9":"df['City'].describe()","89413efa":"df['City'].value_counts()","c656966f":"plt.figure(figsize = (10,5))\nax= sns.countplot(df['City'])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nax.set_yscale('log')\nplt.xticks(rotation = 90)\nplt.show()","a1c74bb5":"plt.figure(figsize = (20,6))\nax= sns.countplot(x = \"City\", hue = \"Opportunity Status\", data = df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","f232ef7c":"df['B2B Sales Medium'].describe()","61f99d0e":"df['B2B Sales Medium'].value_counts()","796d2018":"plt.figure(figsize = (10,5))\nax= sns.countplot(df['B2B Sales Medium'])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nax.set_yscale('log')\nplt.xticks(rotation = 90)\nplt.show()","a9119869":"plt.figure(figsize = (20,6))\nax= sns.countplot(x = \"B2B Sales Medium\", hue = \"Opportunity Status\", data = df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","df5a88e1":"df['Sales Velocity'].describe()","e9f6ed3a":"plt.figure(figsize = (10,5))\nax= sns.violinplot(df['Sales Velocity'])\nplt.show()","65a38982":"# As we can see there are a number of outliers in the data.\n# We will cap the outliers to 95% value for analysis.","6bda9c1a":"percentiles = df['Sales Velocity'].quantile([0.05,0.95]).values\ndf['Sales Velocity'][df['Sales Velocity'] <= percentiles[0]] = percentiles[0]\ndf['Sales Velocity'][df['Sales Velocity'] >= percentiles[1]] = percentiles[1]","a309d78f":"plt.figure(figsize = (10,5))\nax= sns.violinplot(df['Sales Velocity'])\nplt.show()","87eb6425":"plt.figure(figsize = (10,5))\nsns.violinplot(y = 'Sales Velocity', x = 'Opportunity Status', data = df)\nplt.show()","035cc77e":"df['Opportunity Status'].describe()","a39d122f":"df['Opportunity Status'].value_counts()","e0908933":"plt.figure(figsize = (10,5))\nax= sns.countplot(df['Opportunity Status'])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nax.set_yscale('log')\nplt.show()","4ac32706":"df['Sales Stage Iterations'].describe()","f327db9c":"plt.figure(figsize = (10,5))\nax= sns.violinplot(df['Sales Stage Iterations'])\nplt.show()","7a19bc20":"percentiles = df['Sales Stage Iterations'].quantile([0.05,0.95]).values\ndf['Sales Stage Iterations'][df['Sales Stage Iterations'] <= percentiles[0]] = percentiles[0]\ndf['Sales Stage Iterations'][df['Sales Stage Iterations'] >= percentiles[1]] = percentiles[1]","90a04ea3":"plt.figure(figsize = (10,5))\nax= sns.violinplot(df['Sales Stage Iterations'])\nplt.show()","c025502c":"plt.figure(figsize = (10,5))\nsns.violinplot(y = 'Sales Stage Iterations', x = 'Opportunity Status', data = df)\nplt.show()","3ddf4a8f":"df['Opportunity Size (USD)'].describe()","f96e2ecd":"plt.figure(figsize = (10,5))\nax= sns.violinplot(df['Opportunity Size (USD)'])\nplt.show()","eee968fa":"percentiles = df['Opportunity Size (USD)'].quantile([0.05,0.95]).values\ndf['Opportunity Size (USD)'][df['Opportunity Size (USD)'] <= percentiles[0]] = percentiles[0]\ndf['Opportunity Size (USD)'][df['Opportunity Size (USD)'] >= percentiles[1]] = percentiles[1]","d3b62f32":"plt.figure(figsize = (10,5))\nax= sns.violinplot(df['Opportunity Size (USD)'])\nplt.show()","a385798d":"plt.figure(figsize = (10,5))\nsns.violinplot(y = 'Opportunity Size (USD)', x = 'Opportunity Status', data = df)\nplt.show()","82645a42":"df['Client Revenue Sizing'].describe()","22c2ecd9":"df['Client Revenue Sizing'].value_counts()","e127edd8":"plt.figure(figsize = (10,5))\nax= sns.countplot(df['Client Revenue Sizing'])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nax.set_yscale('log')\nplt.xticks(rotation = 90)\nplt.show()","c6c61eaa":"plt.figure(figsize = (20,6))\nax= sns.countplot(x = \"Client Revenue Sizing\", hue = \"Opportunity Status\", data = df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","7e39379e":"df['Client Employee Sizing'].describe()","eeb7a1bc":"df['Client Employee Sizing'].value_counts()","445700dd":"plt.figure(figsize = (10,5))\nax= sns.countplot(df['Client Employee Sizing'])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nax.set_yscale('log')\nplt.xticks(rotation = 90)\nplt.show()","c58f2880":"plt.figure(figsize = (20,6))\nax= sns.countplot(x = \"Client Employee Sizing\", hue = \"Opportunity Status\", data = df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","c66f94c4":"df['Business from Client Last Year'].describe()","04145ba6":"df['Business from Client Last Year'].value_counts()","5c44e4b3":"plt.figure(figsize = (10,5))\nax= sns.countplot(df['Business from Client Last Year'])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nax.set_yscale('log')\nplt.xticks(rotation = 90)\nplt.show()","48f46032":"plt.figure(figsize = (20,6))\nax= sns.countplot(x = \"Business from Client Last Year\", hue = \"Opportunity Status\", data = df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","78fd7f92":"df['Compete Intel'].describe()","3b98bb9c":"df['Compete Intel'].value_counts()","52e7de79":"plt.figure(figsize = (10,5))\nax= sns.countplot(df['Compete Intel'])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nax.set_yscale('log')\nplt.xticks(rotation = 90)\nplt.show()","943f7c7b":"plt.figure(figsize = (20,6))\nax= sns.countplot(x = \"Compete Intel\", hue = \"Opportunity Status\", data = df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","edaed31a":"df['Opportunity Sizing'].describe()","dd9cfd37":"df['Opportunity Sizing'].value_counts()","2d4a796d":"plt.figure(figsize = (10,5))\nax= sns.countplot(df['Opportunity Sizing'])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nax.set_yscale('log')\nplt.xticks(rotation = 90)\nplt.show()","666468a3":"plt.figure(figsize = (20,6))\nax= sns.countplot(x = \"Opportunity Sizing\", hue = \"Opportunity Status\", data = df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","c39d3a4a":"# Creating a dummy variable for some of the categorical variables and dropping the first one.\ndummy1 = pd.get_dummies(df[['Technology\\nPrimary', 'City', 'B2B Sales Medium', 'Client Revenue Sizing',\n                            'Client Employee Sizing', 'Business from Client Last Year',\n                            'Compete Intel', 'Opportunity Sizing']], drop_first=True)\ndummy1.head()\n","f690534b":"# Adding the results to the master dataframe\ndf = pd.concat([df, dummy1], axis=1)\ndf.head()","1a5d0e70":"df = df.drop(['Technology\\nPrimary', 'City', 'B2B Sales Medium', 'Client Revenue Sizing',\n              'Client Employee Sizing', 'Business from Client Last Year',\n              'Compete Intel', 'Opportunity Sizing'], axis = 1)\ndf.head()","510f2ce2":"from sklearn.model_selection import train_test_split\n\n# Putting feature variable to X\nX = df.drop(['Opportunity Status'], axis=1)","48d6cb72":"X.head()","4d72d94a":"X.shape","3c2282b2":"# Putting response variable to y\ny = df['Opportunity Status']","1be329f0":"y.head()","61818c23":"y.shape","e76f4df4":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=125)","fcf230bb":"X_train.head()","a7f5f401":"X_train.shape","b33ce12a":"X_test.head()","2141e29b":"X_test.shape","e2ae8b4e":"y_train.head()","5f19fe16":"y_train.shape","f7dc6c5f":"y_test.head()","e16fb95e":"y_test.shape","a722971b":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train[['Sales Velocity','Sales Stage Iterations',\n         'Opportunity Size (USD)']] = scaler.fit_transform(X_train[['Sales Velocity','Sales Stage Iterations',\n                                                                    'Opportunity Size (USD)']])\n\nX_train.head()","df1ecb59":"X_test[['Sales Velocity','Sales Stage Iterations',\n         'Opportunity Size (USD)']] = scaler.transform(X_test[['Sales Velocity','Sales Stage Iterations',\n                                                               'Opportunity Size (USD)']])\n\nX_test.head()","bbf9dec3":"# Checking the Opportunity Status Rate\nOpportunity = round((sum(df['Opportunity Status'])\/len(df['Opportunity Status'].index))*100,2)\nprint(\"We have almost {} %  Opportunity rate after successful data manipulation\".format(Opportunity))","34df51eb":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nmodel = GradientBoostingClassifier(n_estimators=150,max_depth=6)","6f16a943":"# fit the model with the training data\nmodel.fit(X_train,y_train)","78e28a00":"# predict the target on the train dataset\npredict_train = model.predict(X_train)\npredict_train","2dfdcb13":"trainaccuracy = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', trainaccuracy)","fca09a6d":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","aa3c290b":"features_to_remove = vif.loc[vif['VIF'] >= 4.99,'Features'].values\nfeatures_to_remove = list(features_to_remove)\nprint(features_to_remove)","75f5b5bf":"X_train = X_train.drop(columns=features_to_remove, axis = 1)\nX_train.head()","b826c048":"X_test = X_test.drop(columns=features_to_remove, axis = 1)\nX_test.head()","2b4d8bf4":"# fit the model with the training data\nmodel.fit(X_train,y_train)","b68da5ee":"# predict the target on the train dataset\npredict_train = model.predict(X_train)\npredict_train","47c244ca":"accuracytrain = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', accuracytrain)","f63baf10":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","35c866c9":"from sklearn import metrics\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train, predict_train )\nprint(confusion)","d53cb520":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","7f9d8b7f":"# Let's see the sensitivity of our model\ntrainsensitivity= TP \/ float(TP+FN)\ntrainsensitivity","fdc88ae9":"# Let us calculate specificity\ntrainspecificity= TN \/ float(TN+FP)\ntrainspecificity","a861f5a7":"# Calculate false postive rate - predicting Opportunity when company does not have Opportunity\nprint(FP\/ float(TN+FP))","612fa95d":"# Positive predictive value \nprint (TP \/ float(TP+FP))","6e3f0e1e":"# Negative predictive value\nprint(TN \/ float(TN+ FN))","c7a379ab":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","a1580228":"draw_roc(y_train,predict_train)","c75a4d9b":"#Using sklearn utilities for the same","ab658ae6":"from sklearn.metrics import precision_score, recall_score\nprecision_score(y_train,predict_train)","db5c8bfa":"recall_score(y_train,predict_train)","fd779335":"# predict the target on the test dataset\npredict_test = model.predict(X_test)\nprint('Target on test data\\n\\n',predict_test)","29297dae":"confusion2 = metrics.confusion_matrix(y_test, predict_test )\nprint(confusion2)","ea2c1508":"# Let's check the overall accuracy.\ntestaccuracy= accuracy_score(y_test,predict_test)\ntestaccuracy","c73353ec":"# Let's see the sensitivity of our model\ntestsensitivity=TP \/ float(TP+FN)\ntestsensitivity","2e768515":"# Let us calculate specificity\ntestspecificity= TN \/ float(TN+FP)\ntestspecificity","45840f28":"# Let us compare the values obtained for Train & Test:\nprint(\"Train Data Accuracy    :{} %\".format(round((trainaccuracy*100),2)))\nprint(\"Train Data Sensitivity :{} %\".format(round((trainsensitivity*100),2)))\nprint(\"Train Data Specificity :{} %\".format(round((trainspecificity*100),2)))\nprint(\"Test Data Accuracy     :{} %\".format(round((testaccuracy*100),2)))\nprint(\"Test Data Sensitivity  :{} %\".format(round((testsensitivity*100),2)))\nprint(\"Test Data Specificity  :{} %\".format(round((testspecificity*100),2)))","7bbc10ce":"# Duplicate Check","87c07656":"## GBM","1dd904ee":"Sales Velocity","6399d07d":"Opportunity Status","2b6a8dca":"# VIF ","91b2879e":"This assignment is around a case study about TechnoServe, a fictional tech SaaS (Software as a service) startup that specialises in different types of cloud-based software services to the small and medium enterprise customers. The products provided by the company are inclined towards increasing the productivity for the customers.\n\nThe revenue that the company generates is highly dependent on the consumption of the cloud services that they provide. Therefore, the revenue in-flow in the company is highly dependent on the number of clients that the company has. The company is facing a very pertinent problem faced in the IT industry today, declining conversions across its sales funnel.\n\n \n\nThe problem that the company is facing is that its pipeline conversion percentage has dropped from 35% at the end of the last fiscal (FY 2019-20) to 25% at present. The company needs a solution to solve the issue, and they have asked you to come up with one.\n\n \n    Here are a few details about TechnoServe that you should be aware of:\n\nThe company is based out of Pune and started its operations in 2010.\nIt has clients spread over Pune and other cities as well.\nThere are more than 600 employees, distributed over multiple teams.\nIt has a wide variety of IT solutions spread across different industries.\nTo get a brief idea about IT solutions,","1e39fe22":"Making predictions on the test set","0bcb8b70":"Business from Client Last Year","0e84ad0c":"Technology\\nPrimary","252228ac":"## Problem Statement","ced095a0":"Client Employee Sizing","ced72654":"Opportunity Sizing","07ad5904":"The shape after running the drop duplicate command is same as the original dataframe.\n\nHence we can conclude that there were zero duplicate values in the dataset.","571df3cc":"Client Revenue Sizing","32bc993e":"GBM is a boosting algorithm used when we deal with plenty of data to make a prediction with high prediction power. Boosting is actually an ensemble of learning algorithms which combines the prediction of several base estimators in order to improve robustness over a single estimator. It combines multiple weak or average predictors to a build strong predictor.","fe3af661":"# Data Preparation","484da864":"# Final Observation:","b31089ab":"Converting some binary variables (Won\/Lost) to 1\/0","7a9325bd":"# Precision and Recall","3b004439":"#Dproping 'Opportunity ID' field as it will not help to take decision ","f9a9eaf9":"# VIF","7c3f1b8a":"Opportunity Size (USD)","cb94ab48":"# Plotting the ROC Curve","f0f80621":"City","1dde6653":"Compete Intel","bd07262f":"Sales Stage Iterations\t","caec61da":"B2B Sales Medium","fca791bc":"# Model Building","7d4c0ed3":"Understand the problem, come up with possible hypotheses for low conversions faced by TechnoServe. Once that is done, you need to analyse the dataset given below to validate the hypotheses and form the solution strategy that they should employ to solve the problem. \nThe dataset and the data dictionary are given below","840dbe02":"# Feature Scaling","c915fe43":"# Business Problem: Sales Pipeline Conversion at a SaaS Startup\n \n"}}