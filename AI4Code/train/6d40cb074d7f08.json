{"cell_type":{"b24a253c":"code","ec3e4343":"code","806deb56":"code","9eac6f02":"code","26192ff2":"code","c478b6e2":"code","7947e574":"code","805dfb9b":"code","01aeb398":"code","c5778de4":"code","9804ab63":"code","b8a98a5c":"code","b6c537a9":"code","3e23f04c":"code","309a28fc":"code","e55371c2":"code","2ef702be":"code","e4fe20f3":"code","5e02baab":"code","4be5c94c":"code","1e5803ec":"code","3af85f6d":"code","87a249ec":"code","4bf03f82":"code","507372e5":"code","4fdd45db":"code","5e47e900":"code","458ecac2":"code","640707e2":"code","f145259d":"code","1d742d15":"code","790f067e":"code","1f9d3cc7":"code","016e44ee":"code","2677d94a":"code","b8bf72ee":"code","d0a23f61":"code","dd991860":"code","b472fbbc":"code","489b6480":"code","fff79692":"code","bfb41171":"code","d65cd57f":"code","368eeeb1":"code","784366d3":"code","2648bb08":"code","a5b61aaa":"code","d2b46527":"code","ca446fee":"code","75c96e3d":"code","09c09878":"code","de87e550":"code","0e39be58":"code","64108c05":"code","f332d139":"code","ab38ca9c":"code","b08b77d6":"code","0365d866":"code","20bfd13c":"code","3b9aeaf2":"code","b088e1a2":"code","aca3398f":"code","30d16ec6":"code","7b1d0c43":"code","cc04116c":"code","26e7d085":"code","3373dec6":"code","f80e4d39":"code","1812479d":"code","193cf4cc":"code","c504cde8":"code","e4e8a6ee":"markdown","484b051d":"markdown","36356243":"markdown","4fefeaf5":"markdown","46a8e615":"markdown","2ec8cded":"markdown"},"source":{"b24a253c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","ec3e4343":"train_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-mar-2021\/test.csv\")\nsample = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-mar-2021\/sample_submission.csv\")","806deb56":"train_df.head()","9eac6f02":"cat_feats = [col for col in train_df.columns if col.startswith(\"cat\")]\nnum_feats = [col for col in train_df.columns if col.startswith(\"cont\")]","26192ff2":"test_df['target'] = -1","c478b6e2":"all_df = pd.concat([train_df, test_df])","7947e574":"all_df.head()","805dfb9b":"dummies_df = pd.get_dummies(all_df[cat_feats], drop_first=True)\nnew_df = pd.concat([all_df['id'],dummies_df, all_df[num_feats], all_df['target']], axis=1)","01aeb398":"new_df.head()","c5778de4":"train = new_df[new_df[\"target\"] != -1]\ntest = new_df[new_df[\"target\"] == -1]\ntest = test.drop(\"target\", axis=1)","9804ab63":"train.head()","b8a98a5c":"test.head()","b6c537a9":"from statsmodels.stats.outliers_influence import variance_inflation_factor","3e23f04c":"vif = pd.DataFrame()\nvif[\"variables\"] = num_feats\nvif[\"VIF\"] = [variance_inflation_factor(train[num_feats].values, i) for i in range(train[num_feats].shape[1])]","309a28fc":"vif = vif.sort_values(by=[\"VIF\"], ascending=False)\nvif.style.background_gradient(cmap=\"magma\")","e55371c2":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score","2ef702be":"train = train.sample(frac=1).reset_index(drop=True)","e4fe20f3":"y = train['target'].values","5e02baab":"skf = StratifiedKFold(n_splits=5)\n\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train, y)):\n    train.loc[valid_idx, \"kfold\"] = fold","4be5c94c":"def run_training(df, algo, fold, model_name, test):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    xtrain = df_train.drop([\"id\", \"kfold\", \"target\"], axis=1).values\n    xvalid = df_valid.drop([\"id\", \"kfold\", \"target\"], axis=1).values\n    \n    ytrain = df_train.target.values\n    yvalid = df_valid.target.values\n    \n    model = algo\n    model.fit(xtrain, ytrain)\n    \n    preds = model.predict_proba(xvalid)[:, 1]\n    auc = roc_auc_score(yvalid, preds)\n    print(f\"fold={fold}, auc={auc}\")\n    \n    df_valid.loc[:, model_name] = preds\n    \n    sub_preds = model.predict_proba(test)[:, 1]\n    \n    return df_valid[[\"id\", \"kfold\", \"target\", model_name]], sub_preds","1e5803ec":"from sklearn.ensemble import RandomForestClassifier","3af85f6d":"clf = RandomForestClassifier(n_estimators=200, verbose=1)","87a249ec":"test_df_sub = pd.DataFrame({\"id\": test[\"id\"].values})","4bf03f82":"dfs = []\ntest_temp = np.zeros(len(test))\n\nfor fold in range(5):\n    temp_df, test_preds = run_training(train, clf, fold, \"random_forest\",test.drop(\"id\", axis=1))\n    dfs.append(temp_df)\n    test_temp += test_preds\n    \ntest_df_sub[f\"forest_mean_fold\"] = test_temp \/ 5   \nfin_valid_df_rfc = pd.concat(dfs)","507372e5":"fin_valid_df_rfc.head()","4fdd45db":"roc_auc_score(fin_valid_df_rfc[\"target\"], fin_valid_df_rfc[\"random_forest\"])","5e47e900":"from sklearn.linear_model import LogisticRegression","458ecac2":"lr = LogisticRegression(max_iter=100000,verbose=1)","640707e2":"dfs = []\ntest_temp = np.zeros(len(test))\n\nfor fold in range(5):\n    temp_df, test_preds = run_training(train, lr, fold, \"logisticRegression\",test.drop(\"id\", axis=1))\n    dfs.append(temp_df)\n    test_temp += test_preds\n    \ntest_df_sub[f\"lr_mean_fold\"] = test_temp \/ 5   \nfin_valid_df_lr = pd.concat(dfs)","f145259d":"fin_valid_df_lr.head()","1d742d15":"roc_auc_score(fin_valid_df_lr[\"target\"], fin_valid_df_lr[\"logisticRegression\"])","790f067e":"from xgboost import XGBClassifier","1f9d3cc7":"xgb = XGBClassifier(use_label_encoder=False)","016e44ee":"dfs = []\ntest_temp = np.zeros(len(test))\n\nfor fold in range(5):\n    temp_df, test_preds = run_training(train, xgb, fold, \"xgboost\",test.drop(\"id\", axis=1))\n    dfs.append(temp_df)\n    test_temp += test_preds\n    \ntest_df_sub[f\"xgb_mean_fold\"] = test_temp \/ 5   \nfin_valid_df_xgb = pd.concat(dfs)","2677d94a":"fin_valid_df_xgb.head()","b8bf72ee":"roc_auc_score(fin_valid_df_xgb[\"target\"], fin_valid_df_xgb[\"xgboost\"])","d0a23f61":"test_df_sub.head()","dd991860":"from functools import partial\nfrom scipy.optimize import fmin","b472fbbc":"class OptimizerAUC:\n    def __init__(self):\n        self.coef_ = 0\n        \n    def _auc(self, coef, X, y):\n        x_coef = X * coef\n        predictions = np.sum(x_coef, axis=1)\n        auc_score = roc_auc_score(y, predictions)\n        return -1.0 * auc_score\n    \n    def fit(self, X, y):\n        partial_loss = partial(self._auc, X=X, y=y)\n        init_coef = np.random.dirichlet(np.ones(X.shape[1]))\n        self.coef_ = fmin(partial_loss, init_coef, disp=True)\n    \n    def predict(self, X):\n        x_coef = X * self.coef_\n        predictions = np.sum(x_coef, axis=1)\n        return predictions      \n\n\ndef run_training2(pred_df, fold, col_names):\n\n    train_df = pred_df[pred_df.kfold !=fold].reset_index(drop=True)\n    valid_df = pred_df[pred_df.kfold == fold].reset_index(drop=True)\n    \n    xtrain = train_df[col_names].values\n    xvalid = valid_df[col_names].values\n    \n    ytrain = train_df.target.values\n    yvalid = valid_df.target.values\n    \n    opt = OptimizerAUC()\n    opt.fit(xtrain, ytrain)\n    preds = opt.predict(xvalid)\n    \n    auc = roc_auc_score(yvalid, preds)\n    print(f\"Fold={fold}, AUC={auc}\")\n    \n    return opt.coef_","489b6480":"df = None\n\ndfs_list = [fin_valid_df_lr, fin_valid_df_rfc, fin_valid_df_xgb]\nfor i in range(len(dfs_list)):\n    if df is None:\n        df = dfs_list[i]\n    else:\n        df = df.merge(dfs_list[i], on=\"id\", how=\"left\")","fff79692":"df.head()","bfb41171":"targets = df.target.values\ncol_names = [\"logisticRegression\", \"random_forest\", \"xgboost\"]","d65cd57f":"coefs = []\nfor j in range(5):\n    coefs.append(run_training2(df, j, col_names))","368eeeb1":"coefs = np.array(coefs)\ncoefs_mean = np.mean(coefs, axis=0)\nprint(coefs_mean)","784366d3":"col_names","2648bb08":"wt_avg = (\n    coefs_mean[0] * df.logisticRegression.values\n    + coefs_mean[1] * df.random_forest.values\n    + coefs_mean[2] * df.xgboost.values\n)\nprint(\"Optimal auc after finding coefs\")\nwt_auc = roc_auc_score(targets, wt_avg)\nprint(f\"Optimized weighted avg of auc: {wt_auc}\")","a5b61aaa":"test_df_sub","d2b46527":"wt_avg_blend = (\n    coefs_mean[0] * test_df_sub[\"lr_mean_fold\"].values\n    + coefs_mean[1] * test_df_sub[\"forest_mean_fold\"].values\n    + coefs_mean[2] + test_df_sub[\"xgb_mean_fold\"].values\n)","ca446fee":"sample['target'] = wt_avg_blend","75c96e3d":"sample.to_csv(\"blend_avg_weights_sub2.csv\", index=False)","09c09878":"col_names","de87e550":"col_names.append(\"id\")","0e39be58":"new_train = train.merge(df[col_names], on=\"id\", how=\"left\")","64108c05":"new_test = test.merge(test_df_sub, on=\"id\", how=\"left\")","f332d139":"new_train.head()","ab38ca9c":"xgb = XGBClassifier(use_label_encoder=False)","b08b77d6":"test_df_sub2 = pd.DataFrame()","0365d866":"dfs = []\ntest_temp = np.zeros(len(new_test))\n\nfor fold in range(5):\n    temp_df, test_preds = run_training(new_train, xgb, fold, \"xgboost\",new_test.drop(\"id\", axis=1))\n    dfs.append(temp_df)\n    test_temp += test_preds\n    \ntest_df_sub2[f\"xgb_mean_fold\"] = test_temp \/ 5   \nfin_valid_df_xgb = pd.concat(dfs)","20bfd13c":"roc_auc_score(fin_valid_df_xgb[\"target\"], fin_valid_df_xgb[\"xgboost\"])","3b9aeaf2":"test_df_sub2.head()","b088e1a2":"rfc = RandomForestClassifier(n_estimators=200, verbose=1)","aca3398f":"dfs = []\ntest_temp = np.zeros(len(new_test))\n\nfor fold in range(5):\n    temp_df, test_preds = run_training(new_train, rfc, fold, \"random_forest\",new_test.drop(\"id\", axis=1))\n    dfs.append(temp_df)\n    test_temp += test_preds\n    \ntest_df_sub2[\"rfc_mean_fold\"] = test_temp \/ 5   \nfin_valid_df_rfc = pd.concat(dfs)","30d16ec6":"roc_auc_score(fin_valid_df_rfc[\"target\"], fin_valid_df_rfc[\"random_forest\"])","7b1d0c43":"test_df_sub2.head()","cc04116c":"lr = LogisticRegression(max_iter=100000,verbose=1)","26e7d085":"dfs = []\ntest_temp = np.zeros(len(test))\n\nfor fold in range(5):\n    temp_df, test_preds = run_training(new_train, lr, fold, \"logisticRegression\",new_test.drop(\"id\", axis=1))\n    dfs.append(temp_df)\n    test_temp += test_preds\n    \ntest_df_sub2[f\"lr_mean_fold\"] = test_temp \/ 5   \nfin_valid_df_lr = pd.concat(dfs)\n\nroc_auc_score(fin_valid_df_lr[\"target\"], fin_valid_df_lr[\"logisticRegression\"])","3373dec6":"df = None\n\ndfs_list = [fin_valid_df_lr, fin_valid_df_rfc, fin_valid_df_xgb]\nfor i in range(len(dfs_list)):\n    if df is None:\n        df = dfs_list[i]\n    else:\n        df = df.merge(dfs_list[i], on=\"id\", how=\"left\")\n        \ntargets = df.target.values\ncol_names = [\"logisticRegression\", \"random_forest\", \"xgboost\"]\n\ncoefs = []\nfor j in range(5):\n    coefs.append(run_training2(df, j, col_names))","f80e4d39":"coefs = np.array(coefs)\ncoefs_mean = np.mean(coefs, axis=0)\nprint(coefs_mean)","1812479d":"wt_avg = (\n    coefs_mean[0] * df.logisticRegression.values\n    + coefs_mean[1] * df.random_forest.values\n    + coefs_mean[2] * df.xgboost.values\n)\nprint(\"Optimal auc after finding coefs\")\nwt_auc = roc_auc_score(targets, wt_avg)\nprint(f\"Optimized weighted avg of auc: {wt_auc}\")","193cf4cc":"test_df_sub2.head()","c504cde8":"wt_avg_blend = (\n    coefs_mean[0] * test_df_sub[\"lr_mean_fold\"].values\n    + coefs_mean[1] * test_df_sub[\"forest_mean_fold\"].values\n    + coefs_mean[2] + test_df_sub[\"xgb_mean_fold\"].values\n)\n\nsample['target'] = wt_avg_blend\nsample.to_csv(\"blend_avg_weights_sub3.csv\", index=False)","e4e8a6ee":"Indeed, adding pedictions as new features can improve the model as we can see it in this notebook, however all those actions haven't improve my score in Kaggle competition LB. Can anyone explain that to me. Please leave feedback if you found it interesting.","484b051d":"## Submit blending predictions","36356243":"Some suggest that adding predictions as new features can improve the model, let's test that.","4fefeaf5":"## Add more features","46a8e615":"# Blending - optimal weights","2ec8cded":"Hi Kagglers.\nIn this notebook I would like to improve my model performance with blending. It seems to me that in most competitions kagglers using average predictions or blending prediction with weights to get better results. I will blend results of 3 models with their defaults, use their predictions to find optimal weights and blend it."}}