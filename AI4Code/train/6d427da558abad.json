{"cell_type":{"4c968ef9":"code","ec6eee49":"code","cbe85baf":"code","30f9eac9":"code","db8d7561":"code","ff9f608f":"code","1a7b1ee9":"code","8b01d983":"code","2e120f35":"code","583c08af":"code","904b4693":"code","34a3dfea":"code","02bfd825":"code","0e0007b2":"code","9843d964":"code","aa5a457a":"code","54e186f7":"code","58ec8e74":"code","da1ee561":"code","f852ebd2":"code","d72bfb98":"code","b59fc2ff":"code","90ddbac9":"code","0e4c2395":"code","0ef14f67":"markdown","f4787993":"markdown","34ebc37c":"markdown","c84171f5":"markdown","c57938c7":"markdown","1fa5c4dd":"markdown"},"source":{"4c968ef9":"import numpy as np\nimport pandas as pd\nfrom time import time\n#For displaying complete rows info\npd.options.display.max_colwidth=500\nimport tensorflow as tf\nimport spacy\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport json\nimport os\nimport seaborn as sns\nimport missingno as msno\n%matplotlib inline\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ec6eee49":"#sns.set_theme(style=\"whitegrid\")\n\ndef read_json(input_file):\n    '''\n    Read Json Lines File\n    '''\n    with open(input_file) as f:\n        lines = f.read().splitlines()    \n    \n    df = pd.DataFrame(lines)\n    df.columns = ['json_element']\n    df = pd.json_normalize(df['json_element'].apply(json.loads))\n    \n    return df\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cbe85baf":"df_item = read_json('\/kaggle\/input\/meli-data-challenge-2020\/item_data.jl') \ndf_item.head()","30f9eac9":"df = read_json('\/kaggle\/input\/meli-data-challenge-2020\/train_dataset.jl').sample(n=300000)\ndf.shape","db8d7561":"df_search = list()\nfor i in range(df.shape[0]):\n    if i % 10000 == 0:\n        print(i)\n    row = [j['event_info'] for j in df.iloc[i]['user_history'] if isinstance(j['event_info'], str)]\n    df_search.extend(row)\n    \ndf_search = np.unique(df_search)\ndf_search[:10]","ff9f608f":"import multiprocessing\n\nimport gensim\nimport string\nimport re\n\ncores = multiprocessing.cpu_count() # Count the number of cores in a computer\n","1a7b1ee9":"import re\nfrom unidecode import unidecode\n\ndef func_tokenizer(text):\n    # print(text)\n\n    text = str(text)\n\n    # # Remove acentua\u00e7\u00e3o\n    text = unidecode(text)\n\n    # # lowercase\n    text = text.lower()\n\n    # #remove tags\n    text = re.sub(\"<!--?.*?-->\", \"\", text)\n\n    # # remove special characters and digits\n    text = re.sub(\"(\\\\d|\\\\W)+\", \" \", text)\n    text = re.sub('[^A-Za-z0-9]+', ' ', text)\n\n    # # punk\n    text = re.sub(r'[?|!|\\'|#]', r'', text)\n    text = re.sub(r'[.|,|:|)|(|\\|\/]', r' ', text)\n\n    # Clean onde\n    tokens = [t.strip() for t in text.split() if len(t) > 1]\n\n    # remove stopwords\n    #stopwords = self.load_stopwords()\n    #tokens    = [t for t in tokens if t not in stopwords]\n\n    if len(tokens) == 0:\n        tokens.append(\"<pad>\")\n    # print(tokens)\n    # print(\"\")\n    # if len(tokens) < 2:\n    #    print(tokens)\n    return tokens","8b01d983":"df_item.iloc[4].title, func_tokenizer(df_item.iloc[4].title)","2e120f35":"from tqdm import tqdm\n\nsentences=[]\n\n# make corpus\nfor i in tqdm(range(len(df_item[\"title\"]))):\n    sentences.append(func_tokenizer(df_item.iloc[i]['title']))\n    \nfor i in tqdm(range(len(df_search))):    \n    sentences.append(func_tokenizer(df_search))    ","583c08af":"sentences[:5]","904b4693":"model = gensim.models.Word2Vec(min_count=50,\n                                 window=2,\n                                 size=100,\n                                 sample=6e-5, \n                                 alpha=0.03, \n                                 min_alpha=0.0007, \n                                 negative=20,\n                                 workers=cores)\n\nmodel.build_vocab(sentences, progress_per=1000)","34a3dfea":"model.corpus_count","02bfd825":"list(model.wv.vocab.keys())[:10]","0e0007b2":"from gensim.models.callbacks import CallbackAny2Vec\nfrom gensim.models import Word2Vec\n\n# init callback class\nclass callback(CallbackAny2Vec):\n    \"\"\"\n    Callback to print loss after each epoch\n    \"\"\"\n    def __init__(self):\n        self.epoch = 0\n\n    def on_epoch_end(self, model):\n        loss = model.get_latest_training_loss()\n        if self.epoch == 0:\n            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n        else:\n            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n        self.epoch += 1\n        self.loss_previous_step = loss","9843d964":"t = time()\n\nepochs = 100\n\nmodel.train(sentences, \n            total_examples=model.corpus_count, \n            epochs=epochs, \n            report_delay=1,\n            compute_loss = True,\n            callbacks=[callback()])\n\nprint('Time to train the model: {} mins'.format(round((time() - t) \/ 60, 2)))","aa5a457a":"model.wv.similar_by_word('dell')","54e186f7":"# model = gensim.models.Word2Vec(sentences=frase_tokens, min_count=2,size=100,workers=4)\n# model","58ec8e74":"import seaborn as sns\nsns.set_style(\"darkgrid\")\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\ndef tsnescatterplot(model, word, list_names):\n    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n    its list of most similar words, and a list of words.\n    https:\/\/www.kaggle.com\/pierremegret\/gensim-word2vec-tutorial#Training-the-model\n    \"\"\"\n    arrays = np.empty((0, 100), dtype='f')\n    word_labels = [word]\n    color_list  = ['red']\n\n    # adds the vector of the query word\n    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n    \n    # gets list of most similar words\n    close_words = model.wv.most_similar([word])\n    \n    # adds the vector for each of the closest words to the array\n    for wrd_score in close_words:\n        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n        word_labels.append(wrd_score[0])\n        color_list.append('blue')\n        arrays = np.append(arrays, wrd_vector, axis=0)\n    \n    # adds the vector for each of the words from list_names to the array\n    for wrd in list_names:\n        wrd_vector = model.wv.__getitem__([wrd])\n        word_labels.append(wrd)\n        color_list.append('green')\n        arrays = np.append(arrays, wrd_vector, axis=0)\n        \n    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n    #reduc = PCA(n_components=50).fit_transform(arrays)\n    \n    # Finds t-SNE coordinates for 2 dimensions\n    np.set_printoptions(suppress=True)\n    \n    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(arrays)\n    \n    # Sets everything up to plot\n    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n                       'y': [y for y in Y[:, 1]],\n                       'words': word_labels,\n                       'color': color_list})\n    \n    fig, _ = plt.subplots()\n    fig.set_size_inches(9, 9)\n    \n    # Basic plot\n    p1 = sns.regplot(data=df,\n                     x=\"x\",\n                     y=\"y\",\n                     fit_reg=False,\n                     marker=\"o\",\n                     scatter_kws={'s': 40,\n                                  'facecolors': df['color']\n                                 }\n                    )\n    \n    # Adds annotations one by one with a loop\n    for line in range(0, df.shape[0]):\n         p1.text(df[\"x\"][line],\n                 df['y'][line],\n                 '  ' + df[\"words\"][line].title(),\n                 horizontalalignment='left',\n                 verticalalignment='bottom', size='medium',\n                 color=df['color'][line],\n                 weight='normal'\n                ).set_size(15)\n\n    \n    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n            \n    plt.title('t-SNE visualization for {}'.format(word.title()))","da1ee561":"word = 'dell'\ntsnescatterplot(model, word, [i[0] for i in model.wv.most_similar(negative=[word])])","f852ebd2":"word = 'xaomi'\ntsnescatterplot(model, word, [i[0] for i in model.wv.most_similar(negative=[word])])","d72bfb98":"word = 'carro'\ntsnescatterplot(model, word, [i[0] for i in model.wv.most_similar(negative=[word])])","b59fc2ff":"# save the word2vec model\nmodel.save('\/kaggle\/working\/word2vec.model')","90ddbac9":"model.wv.save_word2vec_format('\/kaggle\/working\/mercadolivre-100d.bin', binary=True)","0e4c2395":"model = gensim.models.KeyedVectors.load_word2vec_format('\/kaggle\/working\/mercadolivre-100d.bin', binary=True)\nmodel['celular']","0ef14f67":"## Read Texto in Dataset\n\nRead all text dataset from Item and Search interaction","f4787993":"# Training word2vec for Item Title\n\nWord2Vec was introduced in two papers between September and October 2013, by a team of researchers at Google. Along with the papers, the researchers published their implementation in C. The Python implementation was done soon after the 1st paper, by Gensim.\n\n\n![](https:\/\/miro.medium.com\/fit\/c\/1838\/551\/0*_j8UK1NpsCY_yUk2)\n\nref: https:\/\/www.kaggle.com\/pierremegret\/gensim-word2vec-tutorial#Training-the-model","34ebc37c":"## Gensim Word2Vec Implementation\n\nWe use Gensim implementation of word2vec: https:\/\/radimrehurek.com\/gensim\/models\/word2vec.html","c84171f5":"### t-SNE visualizations\n\nVisualization Similarity Words","c57938c7":"## Save Model\n\nSave dictionary and embs","1fa5c4dd":"### Train a model"}}