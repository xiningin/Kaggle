{"cell_type":{"d51acbd0":"code","f1a44827":"code","fdf30d9f":"code","06ab2bfa":"code","561754e1":"code","370209d4":"code","f4578fea":"code","6cb723c2":"code","41207e4c":"code","75cce025":"code","3d4120ef":"code","845aa251":"code","e8a4849b":"code","aea66c4b":"markdown","d35dbc35":"markdown","a5d680b6":"markdown","a13c1042":"markdown","115b9a59":"markdown","028ff8f2":"markdown","a68b1ae1":"markdown","1f39c616":"markdown","0c053ade":"markdown","54c48ac6":"markdown","553f516d":"markdown","f572d315":"markdown","2be66f17":"markdown"},"source":{"d51acbd0":"# This R environment comes with many helpful analytics packages installed\n# It is defined by the kaggle\/rstats Docker image: https:\/\/github.com\/kaggle\/docker-rstats\n# For example, here's a helpful package to load\n\nlibrary(tidyverse) # metapackage of all tidyverse packages\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nlist.files(path = \"..\/input\")\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f1a44827":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn import datasets","fdf30d9f":"X, y = datasets.load_wine(return_X_y=True)","06ab2bfa":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=1)\nX_train, X_unl, y_train, y_unl = train_test_split(\n    X_train, y_train, test_size=0.7, random_state=1)","561754e1":"print(X_train.shape)\nprint(X_test.shape)\nprint(X_unl.shape)","370209d4":"type(X_train)","f4578fea":"clf = svm.SVC(kernel='linear', probability=True,C=1).fit(X_train, y_train)\nclf.score(X_test, y_test)","6cb723c2":"clp= clf.predict_proba(X_unl)\nlab=clf.predict(X_unl)","41207e4c":"df = pd.DataFrame(clp, columns = ['C1Prob', 'C2Prob','C3Prob']) \ndf['lab']=lab\ndf['actual']=y_unl\ndf['max']=df[[\"C1Prob\", \"C2Prob\",\"C3Prob\"]].max(axis=1)","75cce025":"df","3d4120ef":"nc=np.arange(.35,1,.03)\nacc=np.empty(22)\ni=0\nfor k in np.nditer(nc):\n    conf_ind=df[\"max\"]>k\n    X_train1 = np.append(X_train,X_unl[conf_ind,:],axis=0)\n    y_train1 = np.append(y_train,df.loc[conf_ind,['lab']])\n    clf = svm.SVC(kernel='linear', probability=True,C=1).fit(X_train1, y_train1)\n    acc[i]=  clf.score(X_test, y_test)\n    i = i + 1","845aa251":"plt.hist(df[\"max\"])","e8a4849b":"import matplotlib.pyplot as plt\nx=pd.Series(acc,index=nc)\nx.plot()\n# Add title and axis names\nplt.title('Confidence vs Accuracy')\nplt.xlabel('Confidence')\nplt.ylabel('Accuracy')\nplt.show() ","aea66c4b":"## Choosing the right confidence level","d35dbc35":"## Importing the library and the datasets","a5d680b6":"## Confidence vs Accuracy","a13c1042":"* Step 1: Build a classifier on the labeled data (routine stuff)\n* Step 2: Use this to predict the labels of the unlabeled data. However, apart from the prediction, you also check your confidence level.\n* Step 3: Add those observations to the training data on which you are super confident. These are called as pseudo-labeled as contrasted to labeled data.\n* Step 4: Use this augmented dataset, now for training, and use this model.\n\n\n**As we are using the unsupervised data to augment the training data for supervised learning, this comes somewhere in between and hence the name semi-supervised.**\n","115b9a59":"## Training on the labeled set","028ff8f2":"![image.png](attachment:image.png)","a68b1ae1":"## Dividing the dataset train (labeled), unl(Unlabeled)","1f39c616":"* We are working on a classification problem\n* The use-case is simple, we have some data that maybe 100 observations, and out of that 30 are labeled (Supervised), the rest are unlabelled (Unsupervised)\n* I think all of you will agree that the performance that can be achieved by training on 30 cases or observations will be lesser if we could have used all 100 observations, unfortunately, 70 of them are unlabeled.","0c053ade":"![image.png](attachment:image.png)","54c48ac6":"# Steps","553f516d":"## Semi supervised Learning Prelude","f572d315":"*We all have come across semi-supervised learning as a type of machine learning problem. But it is a concept not understood really well. It\u2019s best to understand this by getting our hands dirty and precisely that\u2019s what we are bringing on.*","2be66f17":"## Plotting the confidence "}}