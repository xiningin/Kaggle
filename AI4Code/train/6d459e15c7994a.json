{"cell_type":{"d70fce08":"code","447bb34a":"code","a68e39d2":"code","7eefa865":"code","45ad2770":"code","087acfa3":"code","a243d5a8":"code","c6614e2f":"code","ef4d48aa":"code","2fbcbb49":"code","07cf5ed1":"code","b7666b7b":"code","6cba490a":"code","c79ac6fa":"code","4cd6128b":"code","16cd7787":"code","6c0148bf":"code","d342a27c":"code","82d664f4":"code","b734e513":"code","2c261d18":"code","9d54f291":"markdown","62928d07":"markdown","b1531655":"markdown","5f608739":"markdown","faba742c":"markdown","9c800e44":"markdown","17eaf244":"markdown","fbb214b2":"markdown","79dbfba3":"markdown","cf814a9d":"markdown","2ef0e210":"markdown","47b10d15":"markdown","19dd6ec1":"markdown","aeb5bba5":"markdown","1c1e9839":"markdown"},"source":{"d70fce08":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pathlib import Path\nimport plotly.express as px\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","447bb34a":"# Install packages\n!pip install plotly statsmodels pandas numpy tokenizers nltk mosestokenizer transformers\n!wget https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/bert-base-uncased-vocab.txt -P data\n\nimport nltk\nnltk.download('perluniprops')","a68e39d2":"DATADIR = Path(\"\/kaggle\/working\/data\")\nFILEPATH = Path(\"\/kaggle\/input\/commonlitreadabilityprize\/train.csv\")","7eefa865":"df = pd.read_csv(FILEPATH)\ndf.tail(10)","45ad2770":"X = df.excerpt.apply(lambda x: len(x))\nY = df.target\nfig = px.scatter(x=X, y=Y, labels={'x': \"Length of sentence\", 'y': \"Target Value\"})\nfig.show()","087acfa3":"fig = px.scatter(x=X, y=Y, opacity=0.65, trendline='ols', trendline_color_override='red')\nfig.show()","a243d5a8":"X.corr(Y)","c6614e2f":"from Levenshtein import distance","ef4d48aa":"sample = df.excerpt[2].replace('\\n', '').lower()\nprint(sample)","2fbcbb49":"from tokenizers import BertWordPieceTokenizer\ntokenizer = BertWordPieceTokenizer('\/kaggle\/working\/data\/bert-base-uncased-vocab.txt', lowercase=True)\nprint(tokenizer)\nprint()\noutput = tokenizer.encode(sample)\ndecoded = tokenizer.decode(output.ids).replace('\" ', '\"').replace('? \"', '?\"').replace(' \"', '\"')\nprint(decoded)","07cf5ed1":"distance(decoded, sample)","b7666b7b":"from nltk.tokenize import word_tokenize\nfrom mosestokenizer import MosesDetokenizer\ntokens = word_tokenize(sample)\nprint(tokens)","6cba490a":"decoded = ' '.join(tokens)\nprint(decoded)","c79ac6fa":"distance(decoded, sample)","4cd6128b":"detokenizer = MosesDetokenizer()\ndecoded2 = detokenizer(tokens)\ndistance(decoded2, sample)","16cd7787":"from nltk.tokenize import sent_tokenize\nfrom fastprogress import progress_bar\nimport random\n\nN = 10 # Number of new samples to generated from `target +- standard_error`","6c0148bf":"newdf = {\"id\": [], \"excerpt\": [], \"target\":[], \"standard_error\": []}\nfor idx in progress_bar(range(len(df))):\n    row = df.iloc[idx]\n    sample_id, excerpt, target, standard_error = row.id, row.excerpt, row.target, row.standard_error\n    for i, sentence in enumerate(sent_tokenize(excerpt)): # Break paragraph into sentences\n        frac_error = standard_error * .10 # Taking only 10% of the original error to keep the target range narrow\n        _from, _to = target + frac_error, target - frac_error\n        for _ in range(N): # target +- standard_error random N values\n            new_target = random.uniform(_from, _to)\n            newdf[\"id\"].append(sample_id)\n            newdf[\"excerpt\"].append(sentence)\n            newdf[\"target\"].append(new_target)\n            newdf[\"standard_error\"].append(standard_error)","d342a27c":"newdf = pd.DataFrame.from_dict(newdf)\nnewdf.to_csv(f\"{DATADIR}\/generated_data.csv\")\nprint(\"Number of training examples after augmentation =\", newdf.shape[0])","82d664f4":"newdf.sample(10)","b734e513":"X = newdf.excerpt.apply(lambda x: len(x))\nY = newdf.target\nX.corr(Y)","2c261d18":"fig = px.scatter(x=X, y=Y, opacity=0.65, trendline='ols', trendline_color_override='red')\nfig.show()","9d54f291":"Worse results than BertWordPieceTokenizer","62928d07":"As you can see below, there is slight downslope. i.e. as the length of sentence increases the target is more negative. This is weird because as the length of sentence increses, it should be more difficult to read.","b1531655":"let's look at the correlation of new data","5f608739":"The tokenizer seem to not recreate the exact same sentence from encoding and decoding. Is there a bug in tokenizer?\n\nThe distance will be zero if the two sentences exactly matches to each other.\n\nBut, here it is not zero =(","faba742c":"Note that the each sample has one sentence (not paragraph).","9c800e44":"**Please let me know in the comments if this information was helpful and the augmentation is useful!**\n\n*If you find this information useful, please <span style=\"color:green\">upvote<\/span> this notebook.*\n\n**That's all =)**","17eaf244":"# Trying different tokenizers","fbb214b2":"Here, I'm splitting the paragraph to sentences but you can also use the whole paragraph.","79dbfba3":"Let's also see the correlation between these two. Ah! as expected. A -ve correlation.","cf814a9d":"## Tokenize words with NLTK word_tokenize","2ef0e210":"### Levenshtein distance\n\nIn information theory, linguistics, and computer science, the Levenshtein distance is a string metric for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits required to change one word into the other. - *Wikipedia*","47b10d15":"Trying a decoder. But still not better than BertWordPieceTokenizer","19dd6ec1":"# Data Augmentation and Excerpt-Target Correlation Analysis \ud83d\udd25\ud83d\udd25","aeb5bba5":"## Tokenize words with BertWordPieceTokenizer","1c1e9839":"# Data Augmentation\n\n## Split the data into sentences and create new samples with target +- standard_error\n\n1. Convert sample paragraphs to single sentences.\n\n2. Suppose target = -0.340259 and standard_error = 0.464009. create *N* samples in the range  [(-0.340259 - 0.464009), (-0.340259 + 0.464009)] while leaving the excerpt same.\n"}}