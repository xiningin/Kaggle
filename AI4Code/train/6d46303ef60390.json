{"cell_type":{"96539454":"code","e707db03":"code","b80b662f":"code","893b6c87":"code","d88514de":"code","b8b4de65":"code","d0455560":"code","d09d1dfa":"code","e6b3a827":"code","fb21315d":"code","689eb5b7":"code","71df36d8":"code","fb7b3722":"code","5c183665":"code","94c66a27":"code","d7d3e5c3":"code","517da142":"code","b796590c":"code","7dd6b5f5":"code","f3590f35":"code","5444c0b2":"code","aa76293e":"code","b7eebdab":"code","6e95ac08":"code","81bcd5c4":"code","011a8bda":"code","a363c346":"code","15c04d98":"code","dd4aa346":"code","e3c93a4c":"code","8ba0a53c":"code","2f334d39":"code","d50b2537":"code","e5508b3f":"code","ecf4c2b3":"code","f68ff73f":"code","2f6bfe0f":"code","6f558760":"code","0e6c99dd":"code","84d23a33":"code","f50942ba":"code","b043f075":"code","834abf5e":"code","3e108d4c":"code","0312ceb6":"code","e1f1c3f0":"code","0a38036b":"code","87a84cae":"code","2b412de4":"code","e92fee82":"code","ceeabfae":"code","f17424d3":"code","83f78790":"code","23519716":"code","f4ee9395":"code","fd327da3":"code","8bcde7e6":"code","ba44f512":"code","5d1f16b0":"code","506437bc":"code","5ae87ff8":"code","b5289293":"code","17c9dbed":"code","c92fea75":"code","16bec5ec":"code","423b9d47":"code","a8d2c5c0":"code","95801494":"code","f62bbe81":"code","56689912":"code","b835309b":"code","c76a26eb":"code","d3724d7e":"code","feeff879":"code","14382986":"code","fabd7275":"code","a278dc6b":"code","e50df049":"code","ac6414aa":"code","844715f6":"code","2d8fe9fe":"code","2acb2826":"code","1a49c1d2":"code","5a5d9cc6":"code","1bafab32":"code","3383bab2":"markdown","4301ff69":"markdown","73a23eaa":"markdown","168e1586":"markdown","a05eb6cb":"markdown","d63ecfe4":"markdown","67934bee":"markdown","08c645cd":"markdown","743c38bc":"markdown","f61f04d6":"markdown","5c5f51c5":"markdown","29bd2931":"markdown","dffe739a":"markdown","57045cc6":"markdown","5667c5fa":"markdown","bf1fe2cd":"markdown","73c66244":"markdown","cf8383ec":"markdown","81acabb7":"markdown","286d7dda":"markdown","b9208ce1":"markdown","487af322":"markdown","4fbf2227":"markdown","4bd2c499":"markdown","5c6862e1":"markdown","ea015e82":"markdown","f5c13219":"markdown","32fd081d":"markdown","c1c8eee6":"markdown","3597451c":"markdown","503e3d6c":"markdown","7ea57bda":"markdown","b39e01b4":"markdown","daa86275":"markdown","954254b6":"markdown","98a8b8ac":"markdown","7b64685c":"markdown","0b52b28d":"markdown","03405dfe":"markdown","35c971b7":"markdown","a68bdd0a":"markdown","9cf8ddaf":"markdown","9b5badf1":"markdown"},"source":{"96539454":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e707db03":"# Data file tames\n# test:..\/input\/30-days-of-ml\/test.csv\n# train: ..\/input\/30-days-of-ml\/train.csv","b80b662f":"import pandas as pd \n\n# Read the data \ntest_data = pd.read_csv('..\/input\/30-days-of-ml\/test.csv', index_col=0)\ntrain_data= pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col=0)\n","893b6c87":"#shape of the data set\nprint(f\" test_data : {test_data.shape}, \\n train_data: {train_data.shape}\")","d88514de":"test_data.head()","b8b4de65":"train_data.head()","d0455560":"# comment \/ uncomment to run the cell \n\n#test_data.isnull().sum()\n\ntrain_data.isnull().sum()\n","d09d1dfa":"#columns in test data\nprint(test_data.columns)\n\n#columns in test data\nprint('\\n',train_data.columns)","e6b3a827":"# all libraries we may need..\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error \nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor \n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OrdinalEncoder \n\n# save the trained model as trained_model_name.joblib \nimport joblib\n\n\n","fb21315d":"def print_cat_columns(dataset):\n    #Get list of categorical variables \n    cl= (dataset.dtypes == 'object') \n    cat_vars = list(cl[cl].index) \n    print(f\"\\nCategorical variables Data-Set:\") \n    print(cat_vars)\n\nprint_cat_columns(test_data)\n\nprint_cat_columns(train_data)","689eb5b7":"# Create a copy of the train_data without the 'target' column\ntrain_drop_target = train_data.drop(['target'], axis = 1)\n\n# Create y (the target)\ny = train_data['target'] \n","71df36d8":"# check train_drop_target [the target column should not be here]\n# Run the cell to see the data\ntrain_drop_target\n","fb7b3722":"# check y [ this should be the target column]\n# Run the cell to see the data\ny.head()","5c183665":"#Create X, and X_test\nX = train_drop_target.copy()\nX_test = test_data.copy()\n","94c66a27":"#Get list of categorical variables from train_data \ncat_o = (train_data.dtypes == 'object') \nobject_cols = list(cat_o[cat_o].index) \n\n# check the object_cols\nobject_cols","d7d3e5c3":"# Apply ordinal encoder to each column with categorical data \nordinal_encoder = OrdinalEncoder() \n\nX[object_cols] = ordinal_encoder.fit_transform(train_drop_target[object_cols]) \nX_test[object_cols] = ordinal_encoder.transform(test_data[object_cols]) \n\n","517da142":"#check the encoding\n# Run the Cell to see the data.\nX.head()","b796590c":"# Check the X_test\n# Run the Cell to see the data.\nX_test","7dd6b5f5":"# split the data to train and validate (, train_size=0.08, test_size=0.02)\nX_train, X_val, y_train, y_val = train_test_split(X, y,train_size = 0.08, \n                                                  test_size = 0.02,random_state = 0)\n","f3590f35":"# train the model RandomForestRegressor\ndef train_the_model(m_d,n,ran,X_train_d,y_train_d):\n    \n    model = RandomForestRegressor(max_depth = m_d, n_estimators = n, random_state = ran, n_jobs=-1)     \n    model.fit(X_train_d,y_train_d) \n    \n    return model\n","5444c0b2":"# set-up of the variables.\nran = 0\nn = 500  # n_estimators\nm_d = 600  # max_depth\n\n\"\"\"\n# Call the Function to train the model\nmodel_01 = train_the_model(m_d,n,ran,X_train, y_train)\n\"\"\"\n# Predicting on X_test\n\"\"\"\npredict_0 = model_01.predict(X_val) \nprint(' MSE: ',mean_squared_error(y_val,predict_0,squared=False))\n\"\"\"","aa76293e":"\n\"\"\"\n# predict\npredict_0 = model_01.predict(X_val) \nprint(' MSE: ',mean_squared_error(y_val,predict_0,squared=False))\n\"\"\"\n \n# 0.7367030 (max=500,n_es=900) \n# 0.7367578 (max=500,n_es=950)\n# 0.7366918 (max=500,n_es=890)\n# 0.7367176 (max=500,n_es=870) \n# 0.7366638 (max=500,n_es=880) [on n_es = 885 MSE go up. so n_es=880 best]\n# 0.7366638 (max=700,n_es=880)\n# 0.7375422 (max=100,n_es=880) \n","b7eebdab":"# After training the Model, and use deffiernt set-up \n# we can do a prediction on X_test data\n\n\"\"\"\npredict_0 = model_01.predict(X_test) \n\"\"\"","6e95ac08":"def save_output_file(sfname,predic):\n    # save the prediction, and submit it.\n    output = pd.DataFrame({'id':test_data.index,'target':predic})\n\n    output.to_csv(sfname, index=False) \n    print(\"Your Submission Was Successfully Saved!\") ","81bcd5c4":"# Save the output file \n\n\"\"\"\nsave_output_file('30day_comp_alihr20_pred_0.csv',predict_0)\n\"\"\"","011a8bda":"# Call the Function for our best combanation of m_d,n,ran as:\n\"\"\"\nran = 0\nn = 1100  # n_estimators\nm_d = 200  # max_depth\n\nmodel_0 = train_the_model(m_d,n,ran,X_train, y_train)\n\"\"\"","a363c346":"\n#predict_0 = model_0.predict(X_val) \n\n#print(' MSE: ',mean_squared_error(y_val,predict_0,squared=False))\n","15c04d98":"# Start Prediction \n\n\"\"\"\n# Predicting on X_test\npredict_1 = model.predict(X_test) \n\n# Read the Prediction\npredict_1\n\"\"\"","dd4aa346":"# Run the code to save the output file for \n\"\"\"\nsave_output_file('30day_comp_alihr20.csv',predict_1)\n\"\"\"","e3c93a4c":"# imports .. will be moved to uper cell\nfrom sklearn.pipeline import make_pipeline \nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.model_selection import cross_val_score \n","8ba0a53c":"\nnumerical_cols = [cname for cname in X_train.columns if \n                X_train[cname].dtype in ['int64', 'float64']]\n\ncategorical_cols = [cname for cname in X_train.columns if\n                    X_train[cname].nunique() < 10 and \n                    X_train[cname].dtype == \"object\"]\n\n# Comment \/ uncomment the coming commands to see the differents  \n# Preprocessing for numerical data using strategy='constant'\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Preprocessing for numerical data, remove the strategy='constant'  \n#numerical_transformer = SimpleImputer()\n\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n","2f334d39":"# Best is : m_d = 200, n = 700, ran = 0\n\n\"\"\"m_d = 200\nn = 700\nran = 0\nmodel_pip_1 = train_the_model(m_d,n,ran)\n\"\"\"","d50b2537":"m_d = 200 # Max_depth\nn = 100  #n_estimators\nran = 0\n\n\n\"\"\"\n# Set the Model.\nmodel_pip_1 = RandomForestRegressor(n_estimators=n, random_state=ran)\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model_pip_1)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npredict_pip_1 = my_pipeline.predict(X_val)\n\n\n# Evaluate the model\n\n#print(f' MSE: on Max_Depth: {m_d} n_estimators: {n} Random_state: {ran} ',mean_squared_error(y_val,predict_pip_1,squared=False))\nprint(f'At Max_Depth:{m_d}, n_estimators:{n}, Random_state:{ran},\\n   MSE = ',mean_squared_error(y_val,predict_pip_1,squared=False))\n\"\"\"\n\n# Some result  \n# 0.7367030678892429 Max_Depth:200, n_estimators:900, Random_state:0, \n# 0.7367030678892429 Max_Depth:450, n_estimators:900, Random_state:0,\n# 0.7368636491954773 Max_Depth:200, n_estimators:700, Random_state:0,\n\n# 0.73912  @ m_d = 200, n = 110, ran = 0 \n","e5508b3f":"#mean_squared_error(y_val,predict_pip_1,squared=False)\n\n\"\"\"\nprint(f'At Max_Depth:{m_d}, n_estimators:{n}, Random_state:{ran},  MSE = ',mean_squared_error(y_val,predict_pip_1,squared=False))\n\"\"\"","ecf4c2b3":"# pipline predict on X_test\n# Evaluate the model\n\n\n\"\"\"\n# uncomment to predect\npredict_pip_1 = my_pipeline.predict(X_test)\n\"\"\"\n","f68ff73f":"# We call the save_output_file on pip_predict_1\n# Uncomment to save the output file for submition..\n\"\"\"\nsave_output_file('30day_comp_alihr20_pip_1.csv',predict_pip_1)\n\"\"\"","2f6bfe0f":"\"\"\"\ndef get_score_1(n_estimators,cv):\n    #Return the average MAE over 3 CV folds of random forest model.\n    \n    #Keyword argument:\n    #n_estimators -- the number of trees in the forest\n     \n    my_pipeline_sc = Pipeline(steps=[('preprocessor', SimpleImputer()), \n        ('model', RandomForestRegressor(n_estimators = n_estimators, random_state=0)) ])\n        \n    scores = -1 * cross_val_score(my_pipeline_sc, X_train, y_train, cv = cv, scoring='neg_mean_absolute_error') \n    \n    \n    #MSE = ',mean_squared_error(y_val,predict_pip_1,squared=False)\n    #print(f'At Max_Depth:{m_d}, n_estimators:{n}, Random_state:{ran},  MSE = ',mean_squared_error(y_val,predict_pip_1,squared=False))\n\n    \n    print(n_estimators,scores.mean)\n    return scores.mean()\n    \"\"\"\n","6f558760":"# We run three batches as:\n# Batch-1: n_estimators range (50,450,50)\n# Batch-2: n_estimators range (10,60,10)\n# Batch-3: n_estimators range (1,50,4)\n\n\"\"\"\nresults = {n_estimators:get_score_1(n_estimators,3) for n_estimators in range(700,900,50)}\n\"\"\"\n\n","0e6c99dd":"#results ","84d23a33":"# results_1, Running n_estimators range (50,450,50)\nresults_1= {50: 0.59385830, 100: 0.59047283, 150: 0.588993895,\n            200: 0.58853823, 250: 0.58808874, 300 :0.58774519,\n            350: 0.58765965, 400: 0.58764073 \n            }\n\n# results_2, Running n_estimators range (10,60,10)\nresults_2 = {10: 0.6190699299146266, 20: 0.6038904402253776, \n             30: 0.5981113259743939, 40: 0.5955497338577485, \n             50: 0.5938583067423432}\n\n# results_3, Running n_estimators range (1,40,4)\nresults_3 = {1: 0.8506650729232857, 5: 0.6486136228235378, \n              9: 0.6222465905372415, 13: 0.6120300541243124, \n             17: 0.6052385452158985, 21: 0.6027528079745619, \n             25: 0.6003264787883423, 29: 0.5987941727829528, \n             33: 0.5969943346734276, 37: 0.5961028287882347, \n             41: 0.5953545177205646, 45: 0.5949774704264793}","f50942ba":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.plot(list(results_1.keys()), list(results_1.values()),color ='Black')\n#plt.title (\"Running n_estimators range 50-450-50\")\nplt.xlabel(\"n_estimators range (50,450,50)\")\nplt.ylabel(\"Prediction\")\nplt.title('Running n_estimators range (50,450,50)')\nplt.show()\n\n\nplt.plot(list(results_2.keys()), list(results_2.values()))\nplt.xlabel(\"n_estimators range (10,60,10)\")\nplt.ylabel(\"Prediction\")\nplt.title(\"Running n_estimators range (10,60,10)\")\nplt.show()\n\nplt.plot(list(results_3.keys()), list(results_3.values()), color ='Green')\nplt.xlabel(\"n_estimators range (1,50,4)\")\nplt.ylabel(\"Prediction\")\nplt.title(\"Running n_estimators range (1,50,4)\")\nplt.show()\n","b043f075":"\n\"\"\"\nn_estimators = 900\nmax_d = 200\nmy_pipeline_2 = Pipeline(steps=[('preprocessor', SimpleImputer()), \n        ('model', RandomForestRegressor(max_depth = max_d, n_estimators = n_estimators, random_state = 0)) ])\n\nmy_pipeline_2.fit(X_train, y_train)\n\"\"\"    \n#scores = -1 * cross_val_score(my_pipeline_2, X_train, y_train, cv = 3, scoring='neg_mean_absolute_error') \n","834abf5e":"# Evaluate the model my_pipeline_2 on X_val \n\n\"\"\"\npredict_pip_01 = my_pipeline_2.predict(X_val)\nprint(' MSE: ',mean_squared_error(y_val,predict_pip_01,squared=False))\n\"\"\"","3e108d4c":"# uncomment to predict\n\n\"\"\"\npredict_pip_2 = my_pipeline_2.predict(X_test)\n\"\"\"\n#print(' MSE: ',mean_squared_error(y_val,predict_pip_2,squared=False))\n","0312ceb6":"# read prediction\n#predict_pip_2","e1f1c3f0":"# We call the save_output_file on pip_predict_1\n\n\"\"\"\nsave_output_file('30day_comp_alihr20_pip_2.csv',predict_pip_2)\n\"\"\"","0a38036b":"# set the model name and fit the data\n\n\n\"\"\"\nfrom xgboost import XGBRegressor \n\n\nmy_model_XGB_3 = XGBRegressor(n_estimators= 880, learning_rate=0.01, n_jobs = -1) \nmy_model_XGB_3.fit(X_train, y_train, early_stopping_rounds = 15, \n             eval_set=[(X_val, y_val)], verbose=False)\n\"\"\"\n","87a84cae":"0.7324938357517158\n\n\"\"\"\npredictions_XGB_3 = my_model_XGB_3.predict(X_val) \n\nprint(' MSE: ',mean_squared_error(y_val,predictions_XGB_3,squared=False))\n\"\"\"\n# 0.733086   n=1000\n# 0.73577   n =100\n# 0.7330863314443636   n = 800\n# 0.7327246791465205   n = 800, l_r =0.01\n# 0.732557323136032    n = 800, l_r =0.01, stop_r = 9\n# 0.7324938357517158   n = 800, l_r =0.01, stop_r = 14\n\n# This setup was the best n = 800, l_r =0.01, stop_r = 14\n","2b412de4":"\"\"\"\nfrom sklearn.metrics import mean_absolute_error \n\n# uncomment to predict\n#predictions_XGB_3 = my_model_XGB_3.predict(X_test) \n\n#print(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions_XGB_3, y_val)))\n\"\"\"\n#predictions_XGB_3 = my_model_XGB_3.predict(X_test) \n","e92fee82":"# check the prediction\n#predictions_XGB_3","ceeabfae":"#predictions_XGB_3 = my_model_XGB_3.predict(X_test) \n\n# We call the save_output_file on predictions_XGB_3\n#save_output_file('30day_comp_alihr20_XGB_3.csv',predictions_XGB_3)","f17424d3":"# we can see this .. for tuning the thing \n# get the most important column in the prediction process \n\"\"\"\nimport eli5 \nfrom eli5.sklearn import PermutationImportance \n\nperm = PermutationImportance(model_pip_1, random_state=1).fit(X_train, y_train) \n\neli5.show_weights(perm, feature_names = X.columns.tolist())\n\"\"\"","83f78790":"#X.columns","23519716":"# Create a copy of the train_data and test_data as:\n\ntrain_data2 = train_data \ntest_data2 = test_data\n","f4ee9395":"# Drop some columns from train_data2, test_data2\n# List of Not important columns.\n\"\"\"\ntodrop=['cat0', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat9', 'cont12']\n\n# Drop some columns from train_data2, test_data2\ntrain_data2 = train_data2.drop(todrop, axis = 1)\ntest_data2 = test_data.drop(todrop, axis = 1)\n\"\"\"\n","fd327da3":"#Check train_data2\n#train_data2.head()","8bcde7e6":"# Drop the target column from the train_data2\n\n\"\"\"\ntrain2_drop_target = train_data2.drop(['target'], axis = 1)\n\n# Create y2 (the target)\ny2 = train_data2['target'] \n\"\"\"\n","ba44f512":"# Check  train2_drop_target\n\"\"\"\ntrain2_drop_target \n\"\"\"","5d1f16b0":"#Get list of categorical variables from train_data \n\n\"\"\"\ncat_o_2 = (train2_drop_target.dtypes == 'object') \nobject_cols_2 = list(cat_o_2[cat_o_2].index) \n\n# check the object_cols\nobject_cols_2\n\"\"\"","506437bc":"\n#Create X\n\n\"\"\"\nX_drop_c2 = train2_drop_target.copy()\nX_test_drop_c2 = test_data2.copy()\n\n# Apply ordinal encoder to each column with categorical data \nordinal_encoder = OrdinalEncoder() \n\nX_drop_c2[object_cols_2] = ordinal_encoder.fit_transform(train2_drop_target[object_cols_2]) \nX_test_drop_c2[object_cols_2] = ordinal_encoder.transform(test_data2[object_cols_2]) \n\"\"\"\n","5ae87ff8":"#y2","b5289293":"\n\"\"\"\nX_train_f, X_val_f, y_train_f, y_val_f = train_test_split(X_drop_c2, y2,train_size = 0.08, \n                                                         test_size = 0.02,random_state = 0)\n\"\"\"\n","17c9dbed":"# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n\"\"\"\nlow_cardinality_cols = [cname for cname in X_train_f.columns if X_train_f[cname].nunique() < 10 and \n                        X_train_f[cname].dtype == \"object\"]\n\"\"\"","c92fea75":"# Without dropping the category columns \nlow_cardinality_cols = [cname for cname in X_train.columns if X_train[cname].nunique() < 20 and \n                    X_train[cname].dtype == \"object\"]\n","16bec5ec":"# Select numeric columns\n\"\"\"\nnumeric_cols = [cname for cname in X_train_f.columns if X_train_f[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX_train_0 = X_train_f[my_cols].copy()\nX_val_0 = X_val_f[my_cols].copy()\nX_test_0 = X_test_drop_c2[my_cols].copy()\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX_train_0 = pd.get_dummies(X_train_f)\nX_valid_0 = pd.get_dummies(X_val_f)\nX_test_0 = pd.get_dummies(X_test_drop_c2)\nX_train_0, X_val_0 = X_train_0.align(X_val_0, join='left', axis=1)\nX_train_0, X_test_0 = X_train_0.align(X_test_drop_c2, join='left', axis=1)\n\"\"\"","423b9d47":"# Without dropping the category columns \nnumeric_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX_train_0 = X_train[my_cols].copy()\nX_val_0 = X_val[my_cols].copy()\nX_test_0 = X_test[my_cols].copy()\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX_train_0 = pd.get_dummies(X_train)\nX_valid_0 = pd.get_dummies(X_val)\nX_test_0 = pd.get_dummies(X_test)\nX_train_0, X_val_0 = X_train_0.align(X_val_0, join='left', axis=1)\nX_train_0, X_test_0 = X_train_0.align(X_test, join='left', axis=1)\n","a8d2c5c0":"\"\"\"\n# Define the model\nmy_model_low_card = XGBRegressor() \n\n# Fit the model\nmy_model_low_card.fit(X_train_0, y_train_f) \n\"\"\"","95801494":"# Without dropping the category columns \n\n# Define the model\nmy_model_low_card = XGBRegressor(max_depth = 5 ,n_estimators = 104, learning_rate = 0.1) \n\n# Fit the model\nmy_model_low_card.fit(X_train_0, y_train) \n","f62bbe81":"# Get predictions\npredictions_low_card = my_model_low_card.predict(X_valid_0)\n","56689912":"\"\"\"\n#mae_1 = mean_absolute_error(predictions_low_card, y_val_f)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions_low_card, y_val_f)))\n\n# Uncomment to print MAE\n#print(\"Mean Absolute Error:\" , mae_1)\n\"\"\"\n","b835309b":"# without dropping the category columns \n\nprint(' MSE: ',mean_squared_error(y_val,predictions_low_card,squared=False))\n\n# 0.7559075252974563\n\n# 0.7336776154750382  n_estimators = 100, learning_rate = 0.1\n# 0.7333521230069813  n_estimators = 90, learning_rate = 0.1\n# 0.7333487672877517  n_estimators = 89, learning_rate = 0.1  (Best) \n\n# 0.7682598838787539  max_depth = 100, n_estimators = 89, learning_rate = 0.1\n\n# 0.7318804134633515  max_depth = 5, n_estimators = 89, learning_rate = 0.1\n# 0.7326389825188204  max_depth = 4, n_estimators = 89, learning_rate = 0.1\n\n# 0.7318620249307588  max_depth = 5, n_estimators = 92, learning_rate = 0.1\n# 0.7318531621650024  max_depth = 5, n_estimators = 99, learning_rate = 0.1\n\n# 0.7316710642773564  max_depth = 5, n_estimators = 105, learning_rate = 0.1\n\n# (Best) 0.7316563837739007   max_depth = 5, n_estimators = 104, learning_rate = 0.1\n\n","c76a26eb":"predictions_low_card = my_model_low_card.predict(X_test_0)\n","d3724d7e":"# We call the save_output_file on predictions_XGB_3\nsave_output_file('30day_comp_alihr20_low_card.csv',predictions_low_card)","feeff879":"\n\n\"\"\"\nfrom xgboost import XGBRegressor \n\n\nmy_model_XGB_AF_drop = XGBRegressor(n_estimators= 880, learning_rate= 0.04, n_jobs = -1) \nmy_model_XGB_AF_drop.fit(X_train_f, y_train_f, early_stopping_rounds = 14, \n             eval_set=[(X_val_f, y_val_f)], verbose=False)\n\n\npredictions_XGB_AF_drop = my_model_XGB_AF_drop.predict(X_val_f) \n\nprint(' MSE: ',mean_squared_error(y_val_f,predictions_XGB_AF_drop,squared=False))\n\n\"\"\"\n\n#  0.7363499856219745   n = 880, l_r = 0.01, stop_r =14\n#  0.7368995058068227   n = 880, l_r = 0.01, stop_r = 5\n#  0.7359973835823964   n = 880, l_r = 0.05, stop_r = 5\n","14382986":"#predictions_XGB_3 = my_model_XGB_3.predict(X_test) \n\n# We call the save_output_file on predictions_XGB_3\n#save_output_file('30day_comp_alihr20_XGB_3.csv',predictions_XGB_3)\n","fabd7275":"# Trying my_pipline model with X_train_f data","a278dc6b":"\"\"\"\nm_d = 200\nn = 700\nran = 0\nmodel_pip_af_drop = train_the_model(m_d,n,ran)\n\"\"\"","e50df049":"\"\"\"\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline_af_drop = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model_pip_af_drop)\n                             ])\n\"\"\"","ac6414aa":"#X_train_f.describe()","844715f6":"# Preprocessing of training data, fit model \n#my_pipeline_af_drop.fit(X_train_f, y_train_f)\n\n","2d8fe9fe":"\"\"\"\nfrom xgboost import XGBRegressor \nmax_d = 310\nn_estim = 900\nl_rate = 0.05\nmy_model_XGB_4 = XGBRegressor(max_depth = max_d,n_estimators = n_estim, \n                              learning_rate=l_rate, n_jobs = -1,    \n                       num_parallel_tree = 5) \nmy_model_XGB_4.fit(X_train_f, y_train_f, early_stopping_rounds = 5, \n             eval_set=[(X_val_f, y_val_f)], verbose=False)\n            \n\"\"\"","2acb2826":"\"\"\"\npredictions_XGB_4 = my_model_XGB_4.predict(X_val_f) \n\nprint(f'max_d:{max_d}, n_estime{n_estim}, l_rate:{l_rate}\\n')\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions_XGB_4, y_val_f)))\nprint(' MSE: ',mean_squared_error(y_val_f,predictions_XGB_4,squared=False))\n\"\"\"","1a49c1d2":"#print(' MSE: ',mean_squared_error(y_val_f,predictions_XGB_4,squared=False))","5a5d9cc6":"#(0.05 > 0.04)","1bafab32":"\"\"\"\nfrom sklearn.metrics import mean_absolute_error \n\npredictions_XGB_4 = my_model_XGB_4.predict(X_test_drop_c2) \n\"\"\"","3383bab2":"# Plot\nploting the results to see the best variavle for  n_estimators","4301ff69":"Running for Parameters:\nn_estimators: (700,900,50), cv = 3","73a23eaa":"# Pipeline After Droping some column","168e1586":"# Numerical and Categorical Columns \n\nHere we creatting two variables **numerical_cols** and **categorical_cols** to be used with SimpleImputer,  OneHotEncoder and Pipline function.","a05eb6cb":"From the above cell, we know there is no Null data.\n\n# Columns \n\nDisplay all columns in the data-Set by .columns command.","d63ecfe4":"**NOTE:** From above, train_data has the column name **'target'** \nthis column will be the one we shoul predict.","67934bee":"# Split the Data\n\nSpliting the data into four parts to train and test the model. ","08c645cd":"# MSE for n_estimators, max_depth and random_state:\n\nWe will change the n_estimators and random_state to see the MSE for each change:\n# Batch-1\nRUN-1: n_estimators = 100, random_state = 1   ----> MSE = 0.7391 (0.74)\n\nRUN-2: max_depth = 300, random_state = 5   ----> MSE = 0.74145 \n\nRUN-3: max_depth = 300, random_state = 0   ----> MSE = 0.7395 \n\nRUN-4: n_estimators = 200, random_state = 25   ----> MSE = 0.7380 \n\nRUN-3: max_depth = 300, random_state = 15   ----> MSE = 0.7406 \n\nFROM RUN -2 :\n\nRUN-5: max_depth = 250, random_state = 10   ----> MSE = 0.741450\n\n**[NOTE]:** Then, the better prediction was on: max_depth = 250, random_state = 5 \n\n# MSE on Leaderboard\n\nAfter several submiting on **Leaderboard** I fond that higher prediction get's lower rank on **Leaderboard**  (Run-4 MSE:0.7380 in Leaderboard is better than RUN-5 MSE: 0.741450), So i try to get lower MSE in Batch-2. \n\n# Batch-2\n We run the Function for several weights of Max_depth, n_estimators, random_state  \nRUN-1: MSE: 0.741696  @ n_estimators = 300, m_d = 1 ran = 0\n\nRUN-2: MSE: 0.736863  @ n_estimators = 700, m_d = 100, ran = 0\n\nRUN-3: MSE: 0.736863  @ n_estimators = 700, m_d = 200, ran = 0\n\nRUN-4: MSE: 0.736703  @ n_estimators = 900, m_d = 200, ran = 0\n\nRUN-5: MSE: 0.736725  @ n_estimators = 1100, m_d = 200,ran = 0\n\nRUN-6: MSE: 0.736703  @ n_estimators = 900, m_d = 300, ran = 0\n\nRUN-7: MSE: 0.736725  @ n_estimators = 1100, m_d = 500,ran = 0\n\nSo in the Batch-2 the best MSE is RUN-4 AND RUN-6  ","743c38bc":"# Another method: \n","f61f04d6":"# Conclusions: \n\nWe will get best Prediction on n_estimators = 1","5c5f51c5":"We are using MSE: mean_squared_error so at n_estimators = 450 , random_state = 0 we got 0.7371519706104738.\n   ","29bd2931":"# Import Libraries\n\n**Libraries:** In this cell we will load all the libraries we need, if we need another one we will put it here and run the cell again.","dffe739a":"# Null Data\n**Null Data:** Check if there is any Null data in both test and train data","57045cc6":"Now, we can save the work in a file.  ","5667c5fa":"Look at the data using .head() command","bf1fe2cd":"# Data Shape\n**shape:** check the number of rows and columns in both test_data and train_data.\nAlso check the data using .head() command.","73c66244":"# Prediction on splited data\n\nUsing the RandomForestRegressor to train the model then predict usint X_val data (X_val is splited data from traing data) ","cf8383ec":"# Predict on n_estimators = 1 \n\nNow we will call the pipline method on n_estimators = 1, also we run the model for coming list:\n\n 0.7395623110651337  @ n_estimators = 100 , random_state = 0\n \n 0.7407657567952979  @ n_estimators = 100 , random_state = 50\n \n 0.7374427167979802  @ n_estimators = 400 , random_state = 0\n \n 0.737156155486999   @ n_estimators = 400 , random_state = 50\n \n 0.7371519706104738  @ n_estimators = 450 , random_state = 0\n","81acabb7":"# Drop Not Important Columns \n\nRemoving not important columns from the train2,test2 data","286d7dda":"# Read This: \n\nIn this Note book we will try to use different methods to Train a Machine Learning Model and predict the targes column.\n\nAfter each method, the Training, fiting and prediction code will be commented so **they will Not Run** while saving, only the last code\/ method will be run to predict and submit into competition.\n\nThe main purpose of this Note-Book is to deliver a **step-by-step** notes to train and run a Machine-Learning Model.","b9208ce1":"# Save output \n\nWe create a Function called **save_output_file(sfname,predic)** to Save our Prediction on a file for Submiting, argument the fileName, Prediction.\n\n","487af322":"cat0', '', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', '', 'cat9', '', '', '', '', '', '', '', '', '', '', '', '', 'cont12', ''], dtype='object')","4fbf2227":"# Pipline prediction \n\nusing Pipline to predict on X_test data","4bd2c499":"**[NOTE]** Runing the coming cell takes long time, it's may take more than 30-To-40min!! The out-put will be stored as dictionary in results, then we can plot it. \n","5c6862e1":"# Categorical Variables \nIn coming codes we will create a list of all catigory columns.   ","ea015e82":"# Training the Model \n\nNow, we call the **RandomForestRegressor** model and fit the data \n\nWe will create a Function **def train_the_model(m_d,n,ran,X_train_d,y_train_d):** to create the ML Model and train it.\n\n**Functioin Arguments:** this fumnction will take 5 arguments as follow:\n\nm_d: Max_depth\n\nn: n_estimators\n\nran: random_state\n\nX_train_d,y_train_d: The data that we will use to train the Model. \n\nand the Function will return a trained Model.\n\n","f5c13219":"# Prediction \n\nNow, we call the Function **train_the_model** for our best combanation of m_d,n,ran as:\n\nran = 0\nn = 900 (n_estimators)\nm_d = 200 (max_depth)\n\nThen, we run the model on X_test data to get the prediction that we can submit. ","32fd081d":"# Ordinal Encoding\n\nApplying **Ordinal Encoding** to categories column in the X and X_test.","c1c8eee6":"# Important Column\n\nUsing **PermutationImportance** we will Check for the most important column in the prediction process, then will remove thouse are not important","3597451c":"# Prediction\n\nPredict using X_test","503e3d6c":"From our observation above, we will run the model on max_depth = 200,n_estimators = 600 then changing the numbers. ","7ea57bda":"# XGBoost and XGBRegressor\n\nNow, we will build and optimize the models with Gradient Boosting.","b39e01b4":"max_d: 300, n_estime: 900, l_rate: 0.15\n\nAbsolute Error: 0.6098427781207426 \n           MSE: 0.7700933389575458\n                      \nmax_d: 300, n_estime: 900 , l_rate: 0.05\n\nMean Absolute Error: 0.5969632081669546 \n                MSE: 0.75610054526858\n\n\n======================  == == === == == === = == =  =\n\nmax_d: 300, n_estime: 900, l_rate: 0.04 \n\nMean Absolute Error: 0.601149919787575 \n                MSE: 0.7586533797292233\n                \nmax_d: 300, n_estime: 900, l_rate: 0.06 \n\nMean Absolute Error: 0.6015493797055632 \n                MSE: 0.759787583295585\n                                 ","daa86275":"\n\nMSE: 0.7700933","954254b6":"To save the time, I save the results after each batch run as in coming cell, so just run the Next cell, then the cell after to see it on plot.  ","98a8b8ac":"# Results\n\n **Bach-1: Running n_estimators range (50,450,50)**\n \n Results = {\n \n 50: 0.59385830, 100: 0.59047283, 150: 0.58899389, 200: 0.88543823\n \n 250: 0.58808874, 300 :0.58774519, 350: 0.58765965, 400: 0.58764073 \n }\n \n We notes that the prediction goes-up when n_estimators goes-down, So we will run another Bach with range (0,60,10)  \n \n **Bach-2: Running n_estimators range (10,60,10)**\n \nresults_2 = {\n10: 0.6190699299146266, 20: 0.6038904402253776, 30: 0.5981113259743939, \n\n40: 0.5955497338577485, 50: 0.5938583067423432\n\n}\n\n**NOTE** Again, the prediction does-up with less n_estimators, so we will go for Batch-3 .   \n\n\n **Bach-3: Running n_estimators range (1,50,4)** \n \n results_3 = {1: 0.8506650729232857, 5: 0.6486136228235378, \n              9: 0.6222465905372415, 13: 0.6120300541243124, \n             17: 0.6052385452158985, 21: 0.6027528079745619, \n             25: 0.6003264787883423, 29: 0.5987941727829528, \n             33: 0.5969943346734276, 37: 0.5961028287882347, \n             41: 0.5953545177205646, 45: 0.5949774704264793}\n","7b64685c":"# Catigory Columns:\n\n**NOTE:** From the overview we know there is a catigory columns, here we will pass the data set to a function that will print-out the categorical variables.","0b52b28d":"# Test Different Parameter values \nWe will use Different Parameter of n_estimators (50 T0 450) and save the output in a variable result then plot it .","03405dfe":"# Preparing the Data \n\n[Not sorted]\n1. Create X.\n2. Create y.\n3. get all object columns (categories)\n4. Applay ordinal encoder.\n","35c971b7":"..","a68bdd0a":"# Preproccessing and Pipeline\nUsing **RandomForestRegressor** as Model and Preproccessingin Pipeline function.\n\nUncomment and Run.","9cf8ddaf":"# Starting Here\n\n\n**First** We upload the data file, test_data and train_data ","9b5badf1":"# XGBRegressor After Dropping columns"}}