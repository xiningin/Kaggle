{"cell_type":{"e8c58a6c":"code","031f135c":"code","b0df49cf":"code","ac72fdee":"code","2bdb8897":"code","40894cfc":"code","1ef3af58":"code","8d16e5be":"code","ea3ae72c":"code","8b3c3e48":"markdown","7db2cde3":"markdown","18005f45":"markdown","47b76fec":"markdown","7941053d":"markdown","bc2a024f":"markdown"},"source":{"e8c58a6c":"import matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.linear_model import LogisticRegression, Ridge\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import mean_squared_error, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","031f135c":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b0df49cf":"nRowsRead = None # specify 'None' if want to read whole file\ndf1 = pd.read_csv('\/kaggle\/input\/diabete.csv', delimiter=',', nrows = nRowsRead)\ndf1.dataframeName = 'diabete.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","ac72fdee":"df1.head(5)","2bdb8897":"df1.describe()","40894cfc":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n    \nplotCorrelationMatrix(df1, 8)","1ef3af58":"y = df1.pop('diabete')\nx = df1\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, random_state=5)\n","8d16e5be":"# CONFIG\nalphas = [0, 0.004, 0.2, 1, 5, 20, 100]\ncriteria = [\"entropy\",\"gini\"]\nMAX_DEPTH = 10\nMIN_DEG = 1\nMAX_DEG = 5\nres = list()\n\nfor depth in range(1, MAX_DEPTH+1):\n    for c in criteria:\n        model_tree = DecisionTreeClassifier(criterion=c, max_depth=depth)\n        model_tree.fit(X_train, Y_train)\n        predictions = model_tree.predict(X_test)\n        res.append((f\"Decision tree with criteria={c} and max_depth={depth}\",accuracy_score(Y_test, predictions)))\nfor deg in range(MIN_DEG, MAX_DEG+1):\n    model_tree = make_pipeline(PolynomialFeatures(degree=deg), LogisticRegression())\n    model_tree.fit(X_train, Y_train)\n    predictions = model_tree.predict(X_test)\n    res.append((f\"Logistic regression with degree={deg}\",accuracy_score(Y_test, predictions)))\nfor deg in range(MIN_DEG, MAX_DEG+1):\n    for a in alphas:\n        model_reg = make_pipeline(PolynomialFeatures(degree=deg), Ridge(alpha=a))\n        model_reg.fit(X_train, Y_train)\n        predictions = list(map(lambda p: 0 if p>0.5 else 1, model_reg.predict(X_test)))\n        res.append((f\"Ridge regression with degree={deg} and alpha={a}\",accuracy_score(Y_test, predictions)))\n\nres.sort(key=lambda a: a[1])\nplt.figure(figsize=(10,10))\nplt.title(\"Accuracy of different models\")\nplt.plot(range(0,len(res)), list(map(lambda a: a[1], res)), 'bo-', linewidth=2, markersize=4)\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Model\")\nplt.show()\n","ea3ae72c":"# Legend\nfor i in range(len(res)):\n    print(f\"Model {i}: {res[i]}\")","8b3c3e48":"Here there is a function to plot the correlation matrix. This can be useful to see visually which features are more correlated: for example (and obviously) diabete and glucose.","7db2cde3":"# Applying the algorithms\nIn the following lines of code I prepare the train and test dataset.","18005f45":"Here there is the core. We can see in the plot the different models and the respective accuracy (correct predictions \/ number of predictions).","47b76fec":"# Ridge Regression, Logistic Regression and Decision Tree : Diabete\nIn this notebook I analyze a dataset related to Diabete. In particulare I'm interested to tell if a person has diabete depending from other features, like glucose.\nI tried to use Ridge Regression width different alphas and degrees, Logistic Regression with different degrees, and Decision Tree with different criteria and limiting depths.\n\nObviously, because we want a Binary Classifier, using Ridge Regression will not give good results. I added it too because I think it's interesting too see how inefficace is to use this algorithm for a classification problem.\nIn particular, with Ridge Regression we consider that a person doesn't have diabete if and only if the prediction is lower than 0.5.\n\n## Intro to logistic regression\nLogistic regression is a statistical method for predicting binary classes. It is a special case of regression where the target variable is categorical in nature (discrete). In particular, this algorithm predicts the probability of occurrence of a binary event and it needs a function that has a codomain from 0 to 1 (obviously), for example the inverse of [logit function](https:\/\/en.wikipedia.org\/wiki\/Logit).\nIn order  to estimate the parameters of the features, this algorithm uses maximum likelihood.","7941053d":"# Conclusion \nObviously we can notice that Ridge Regression has very bad results.\n\nDecision Tree is good in particolar using gini criteria for information gain. If we put high max_depth the tree reduces the accuracy probably due the overfitting.\n\nLogistic Regression is very good, but with high degrees there is overfitting (for example with degree>3).","bc2a024f":"First of all, I analyze the dataset. We can see there are 768 samples with 9 features (including the target: diabete)."}}