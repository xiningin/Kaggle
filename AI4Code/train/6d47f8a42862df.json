{"cell_type":{"eb5d5812":"code","dc20fa18":"code","ad3f547b":"code","173dad0d":"code","74b88c87":"code","ff9f3652":"code","8b83da60":"code","410cfce9":"code","e306ff9f":"code","8917db7e":"code","63af4b44":"code","d744e9b4":"code","60f621a8":"code","be265558":"code","cbfc2c2d":"code","df5d8f65":"code","a36a6cfa":"code","edefee2c":"code","f4df26df":"code","c250213a":"code","c70775bb":"code","564f4709":"code","13b846cf":"code","4d28763f":"code","29d9cbcf":"code","0a7d9504":"code","b2232cab":"code","6c647256":"code","695aff0c":"code","36afb73a":"code","8b35952e":"code","affde957":"code","4d6517a9":"code","dc425d94":"code","f6bf1c6a":"code","d6db764a":"code","586c6f15":"code","ca39d3f9":"code","4fac48fb":"code","138c8f7e":"code","665a8f8e":"code","96a9db90":"code","8a4c104d":"code","fe0b94c0":"code","8e77910b":"code","46ff0d69":"code","c8b94676":"code","762a5ab2":"code","f41b85c7":"code","b7cba6a5":"code","7fa6cad1":"code","8a734777":"code","16a455b9":"code","70c37ae4":"markdown","bda8c152":"markdown","6cdc8338":"markdown","933aaebf":"markdown","d5ca0569":"markdown","95488b1f":"markdown","bcfed8d3":"markdown","5dcb99c9":"markdown","fb7f64f1":"markdown","e9484da1":"markdown","30510fb6":"markdown","07082a7a":"markdown","b3711637":"markdown","af3de361":"markdown","0e29e5cc":"markdown","9a7eac9d":"markdown","f297a14d":"markdown","fadb222b":"markdown","70fa2401":"markdown","1ce49f9d":"markdown","78febbb9":"markdown","828a3964":"markdown","5867db6a":"markdown","16466d2e":"markdown","c6383242":"markdown","14d835fb":"markdown","b81dec67":"markdown","6ba7a29c":"markdown","2799f544":"markdown","c8886e3d":"markdown","db73c15d":"markdown","0dbab042":"markdown","d3aec935":"markdown","c15bfd17":"markdown","2f3e6ddc":"markdown","eae7eef3":"markdown","498aac1d":"markdown","5bd5b6d0":"markdown","c6bccd3c":"markdown","e89e71b8":"markdown","635da25b":"markdown","28ce7646":"markdown","0875de66":"markdown","2dfed338":"markdown","a52fd711":"markdown","a7ec88e6":"markdown","b49c77b5":"markdown","df14c6c9":"markdown","d1b0d862":"markdown","ee932af6":"markdown","428cdb40":"markdown","110c00d1":"markdown","346154cf":"markdown","0181131f":"markdown","24b8512e":"markdown","266f5a33":"markdown","460360cc":"markdown","1a44d6ce":"markdown","b838d159":"markdown","eaa966aa":"markdown","cc2f52e8":"markdown"},"source":{"eb5d5812":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\n\n# import plot libraries\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# import stats to analyze and fit histograms\nfrom scipy import stats \n\n# import ML libraries\nfrom sklearn.compose import ColumnTransformer\n\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\n\n","dc20fa18":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nprint('importing:\\n')\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ad3f547b":"# open raw data\ntrain_data_full = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv')\ntest_data_full = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv')\n\ntrain_data_full.tail()","173dad0d":"# target to model is the 'SalePrice' column\n\n# Remove rows with missing target, separate target from predictors\ntrain_data_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n# show statistics\ntrain_data_full['SalePrice'].describe()","74b88c87":"Q01 = train_data_full['SalePrice'].quantile(0.01)\nQ99 = train_data_full['SalePrice'].quantile(0.99)\nprint(\"We keep house prices between %d $ and %d $\"%(Q01, Q99))\n\ntrain_data = train_data_full[(train_data_full['SalePrice'] > Q01) & (train_data_full['SalePrice'] < Q99)].copy(deep=True)\nprint(\"Size of new data set: \",train_data.shape)","ff9f3652":"fig = sns.distplot(train_data['SalePrice'])","8b83da60":"train_data['log SalePrice'] = np.log(train_data['SalePrice'])\n\nfig = sns.distplot(train_data['log SalePrice'])\n\nprint(\"Skewness of target was %f, reduced to %f after log transform\"%(train_data['SalePrice'].skew(), train_data['log SalePrice'].skew()));","410cfce9":"columns_to_drop = set() #will save the set of features to drop all along our analysis\n\ncolumns_to_drop = {'SalePrice'} # drop as we took the log for the target and it should not be considered as a feature\n\n# drop columns without interest\ncolumns_to_drop = columns_to_drop | {'GarageYrBlt','Condition2', 'BsmtFinSF1','BsmtFinSF2', 'Exterior2nd', 'LotFrontage', 'MasVnrArea', \\\n                   'BsmtFullBath','BsmtHalfBath','HalfBath', 'GarageArea', 'PoolQC', 'MiscFeature', 'MiscVal', 'OverallCond'}\n\n# make a deep copy to avoid modifying train_data  \nnew_train_data = train_data.copy(deep=True) \n\n# create new features by combining existing ones\nnew_train_data['newHouseStyle'] = train_data['BldgType'] + \"_\" + train_data['HouseStyle']\nnew_train_data['newExterQual'] = train_data['ExterQual'] + \"_\" + train_data['ExterCond']\nnew_train_data['newGarageQual'] = train_data['GarageQual'] + \"_\" + train_data['GarageCond'] + \"_\" + train_data['GarageFinish']\nnew_train_data['newLand'] = train_data['LandContour'] + \"_\" + train_data['LandSlope']\nnew_train_data['has1stfloor'] = (train_data['1stFlrSF'] > 0).astype(object)\nnew_train_data['has2ndfloor'] = (train_data['2ndFlrSF'] > 0).astype(object)\nnew_train_data['newPorchSF'] = train_data['OpenPorchSF'] +  train_data['EnclosedPorch'] + train_data['3SsnPorch'] + train_data['ScreenPorch']\n\n# drop features used in building the new ones\ncolumns_to_drop = columns_to_drop | {'BldgType', 'HouseStyle', 'ExterQual', 'ExterCond', 'GarageQual', 'GarageCond', 'GarageFinish',\\\n                                     'LandContour', 'LandSlope', '1stFlrSF', '2ndFlrSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch'}\n\nnew_train_data.drop(columns=columns_to_drop, inplace=True)\nnew_train_data.head()","e306ff9f":"date_df = train_data[['YrSold','MoSold','SalePrice']].copy(deep=True)\ndate_df['Day'] = 1 # add a column with a day value\n\ndate_df.rename(columns={'YrSold' : 'Year', 'MoSold' : 'Month'}, inplace=True)\n\ndate_df['Date sold'] = pd.to_datetime(date_df[['Year','Month','Day']])\nnew_train_data['Date sold'] = date_df['Date sold'] # add to data\nnew_train_data.sort_values(by='Date sold', inplace=True) # sort the data by date\nnew_train_data['Date_sold_int'] = new_train_data['Date sold'].astype(np.int) # could be useful if we need a numerical feature related to the date\n\nplt.figure()\nsns.scatterplot(x=date_df['Date sold'],y=date_df['SalePrice'])\nplt.xlim('2005-01-01', '2011-01-01')\nplt.ylim(0,400000);","8917db7e":"date_df.groupby(['Year', 'Month']).mean()[['SalePrice']].plot();\nplt.title('Monthly average of sale price');","63af4b44":"# check columns with missing data\nmissing_data_cols = set(new_train_data.columns[new_train_data.isna().any()].tolist())\n\n# display the fraction of missing data\nnpts = len(new_train_data)\ndf_pct_missing = pd.DataFrame((npts - new_train_data[missing_data_cols].count())\/npts)*100\ndf_pct_missing.columns = ['Missing data [%]']\ndf_pct_missing.sort_values('Missing data [%]')","d744e9b4":"# drop columns that have more than 75% values that are NAN\n\ncolumns_to_drop = columns_to_drop | set(new_train_data.count()[new_train_data.count() < 0.25*max(new_train_data.count())].index.tolist())\nprint(\"We drop the following columns because more than 75% of the entries are missing: \\n\",new_train_data.count()[new_train_data.count() < 0.25*max(new_train_data.count())].index.tolist())","60f621a8":"# Visualize histograms of features with missing data\n\ncategorical_cols = {cname for cname in new_train_data.columns if new_train_data[cname].dtype == \"object\"}\nnumerical_cols = {cname for cname in new_train_data.columns if new_train_data[cname].dtype in ['int64', 'float64']}\n\nn=len(new_train_data[missing_data_cols - columns_to_drop].columns) # number of plots\nf, axes = plt.subplots(nrows=(n-1)\/\/4 +1,ncols=4,squeeze=False,figsize=(18,4*((n-1)\/\/4 +1))) # represent them on 4 columnms\nf.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.4, hspace=0.4) # increase space between plots\n\nfor col, ax in zip(missing_data_cols - columns_to_drop, axes.flatten()[:n]):\n    if col in categorical_cols:\n        sns.countplot(data=new_train_data, x=col, ax=ax)\n    else:\n        sns.distplot(data=new_train_data, x=col, ax=ax)\n\nplt.show()","be265558":"new_train_data.fillna(\"missing\", inplace=True)","cbfc2c2d":"# Select numerical columns\nnumerical_cols = {cname for cname in new_train_data.drop(columns=['log SalePrice']).columns if new_train_data[cname].dtype in ['int64', 'float64']}\n\n# Visualize all numerical features\nn=len(new_train_data[numerical_cols - columns_to_drop].columns) # number of plots\nf, axes = plt.subplots(nrows=(n-1)\/\/4 +1,ncols=4,squeeze=False,figsize=(18,4*((n-1)\/\/4 +1))) # represent them on 4 columnms\nf.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.4, hspace=0.4) # increase space between plots\n\nfor col, ax in zip(numerical_cols - columns_to_drop, axes.flatten()[:n]):\n    sns.regplot(data=new_train_data,x=col,y='log SalePrice', ax=ax)\n\nplt.show()","df5d8f65":"new_train_data.drop(new_train_data['TotalBsmtSF'][new_train_data['TotalBsmtSF'] > 4000].index, inplace=True)\nnew_train_data.drop(new_train_data['LotArea'][new_train_data['LotArea'] > 100000].index, inplace=True)\nnew_train_data.drop(new_train_data['GrLivArea'][new_train_data['GrLivArea'] > 4000].index, inplace=True)\n\ncolumns_to_drop = columns_to_drop | {\"KitchenAbvGr\", \"Date_sold_int\", \"LowQualFinSF\", \"Id\", \"YrSold\", \"BsmtUnfSF\", \"MsSubClass\", \"MoSold\",  'PoolArea', \"newPorchSF\"}","a36a6cfa":"# Select numerical columns\nnumerical_cols = {cname for cname in new_train_data.drop(columns=['log SalePrice']).columns if new_train_data[cname].dtype in ['int64', 'float64']} \n\n# Visualize all numerical features\nn=len(new_train_data[numerical_cols - columns_to_drop].columns) # number of plots\nf, axes = plt.subplots(nrows=(n-1)\/\/4 +1,ncols=4,squeeze=False,figsize=(18,4*((n-1)\/\/4 +1))) # represent them on 4 columnms\nf.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.4, hspace=0.4) # increase space between plots\n\nfor col, ax in zip(numerical_cols - columns_to_drop, axes.flatten()[:n]):\n    sns.distplot(new_train_data[col], ax=ax)\n\nplt.show()","edefee2c":"new_train_data['log GrLivArea'] = np.log(new_train_data['GrLivArea'])\nnew_train_data['log LotArea'] = np.log(new_train_data['LotArea'])\n\ncols_to_log_tsf = ['GrLivArea', 'LotArea']\ncolumns_to_drop = columns_to_drop | set(cols_to_log_tsf)\nnew_train_data.drop(cols_to_log_tsf, axis=1, inplace=True)\n\n\nfig = plt.figure(figsize=(9,4))\nax0 = fig.add_subplot(121) # add subplot 1 (211 = 2 rows, 1 column, first plot)\nax1 = fig.add_subplot(122) # add subplot 2 \nsns.distplot(new_train_data['log GrLivArea'], ax=ax0)\nsns.distplot(new_train_data['log LotArea'], ax=ax1);","f4df26df":"# sort the features as a function of their correlation to the target\nnumerical_cols = {cname for cname in new_train_data.drop(columns=['log SalePrice']).columns if new_train_data[cname].dtype in ['int64', 'float64']}\n\ncorr_df = new_train_data[list(numerical_cols - columns_to_drop | {'log SalePrice'})].corr()\ncorr_df['log SalePrice'] = np.abs(corr_df['log SalePrice']) # take absolute value\ncorr_df.sort_values('log SalePrice', ascending=False, inplace=True)\ncorr_df.drop('log SalePrice', axis=0, inplace=True)\n\n# make a list that ranks the best features\nranked_num_cols = corr_df[['log SalePrice']].index.to_list()\nprint(\"Numerical features ranked by correlation to target: \\n\",ranked_num_cols)\n\ncorr_df[['log SalePrice']]","c250213a":"# plot a heatmap of the correlations between features\nf, ax = plt.subplots(figsize=(14,12))\nplt.title('Correlation between features', size=16)\nsns.heatmap(new_train_data[list(numerical_cols - columns_to_drop)].corr()) \nplt.show()\n\nnew_train_data[list(numerical_cols - columns_to_drop)].corr()","c70775bb":"columns_to_drop = columns_to_drop | set(corr_df[['log SalePrice']][corr_df['log SalePrice'].between(-0.4, 0.4)].index.to_list())","564f4709":"# Visualize all categorical features\ncategorical_cols = {cname for cname in new_train_data.columns if new_train_data[cname].dtype == \"object\" or new_train_data[cname].dtype == \"bool\"}\n\nn=len(new_train_data[categorical_cols - columns_to_drop].columns) # number of plots\nf, axes = plt.subplots(nrows=(n-1)\/\/4 +1,ncols=4,squeeze=False,figsize=(18,4*((n-1)\/\/4 +1))) # represent them on 4 columnms\nf.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.4, hspace=0.4) # increase space between plots\n\nfor col, ax in zip(categorical_cols - columns_to_drop, axes.flatten()[:n]):\n    sns.countplot(x=col, data=new_train_data, ax=ax)\n\nplt.show()","13b846cf":"# remove features that have essentially one value\ncolumns_to_drop = columns_to_drop | {\"newLand\", \"Condition1\", \"PavedDrive\", \"Functional\", \"Electrical\", \"Utilities\", \"SaleCondition\", \"SaleType\", \"BsmtFinType2\", \"Street\", \"RoofMat1\", \"Heating\", \"has1stfloor\"}\n\n# remove features that do not have sufficient statistics (too many values)\ncolumns_to_drop = columns_to_drop | {\"newHouseStyle\", \"Exterior1st\"}","4d28763f":"# Visualize all categorical features\ncategorical_cols = {cname for cname in new_train_data.columns if new_train_data[cname].dtype == \"object\" or new_train_data[cname].dtype == \"bool\"}\n\nn=len(new_train_data[categorical_cols - columns_to_drop].columns) # number of plots\nf, axes = plt.subplots(nrows=(n-1)\/\/4 +1,ncols=4,squeeze=False,figsize=(18,4*((n-1)\/\/4 +1))) # represent them on 4 columnms\nf.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.4, hspace=0.4) # increase space between plots\n\nfor col, ax in zip(categorical_cols - columns_to_drop, axes.flatten()[:n]):\n    sns.boxplot(data=new_train_data,x=col,y='log SalePrice', ax=ax)\n\nplt.show()","29d9cbcf":"# remove feature that do not seem very correlated with the price\n\ncolumns_to_drop = columns_to_drop | {\"LotConfig\", \"LotShape\", \"RoofStyle\", \"has2ndFloor\", \"NewGarageQual\"}","0a7d9504":"nb_rows = new_train_data['log SalePrice'].count()\n\nmean_target = new_train_data['log SalePrice'].mean()\nstd_target = new_train_data['log SalePrice'].std()\n\ntarget_encoding_cols = set()\none_hot_cols = set()\n\n\n\nfor col in (categorical_cols - columns_to_drop) :\n    \n    value_list = []\n    count_list = []\n    mean_list = []\n    \n    # list of unique values\n    value_list = new_train_data[col].unique()\n    \n    \n    # loop through all possible values for the feature\n    if len(value_list) == 1:\n        # only 1 value --> no information\n        columns_to_drop = columns_to_drop | {col}\n    elif len(value_list) <=3:\n        # low cardinality --> one-hot encoding is better\n        \n        for value in value_list:\n            # count elements per value\n            count_list = count_list + [new_train_data[col][new_train_data[col] == value].count()]    \n           \n        if max(count_list) < nb_rows * 0.95 : # check that all instances do not have the same value   \n            one_hot_cols = one_hot_cols | {col}\n        \n    else:\n        # target encoding\n        \n        for value in value_list:\n            # count elements per value\n            count_list = count_list + [new_train_data[col][new_train_data[col] == value].count()]\n            mean_list = mean_list + [new_train_data['log SalePrice'][new_train_data[col] == value].mean()]\n            \n        delta_to_mean_list = [abs(mean - mean_target) for mean in mean_list]\n        \n        # select features that have a significantly different target values \n        if np.max(delta_to_mean_list) > std_target or np.mean(delta_to_mean_list) > std_target\/3 : \n            target_encoding_cols = target_encoding_cols | {col}\n\n                \nprint(\"Features that could be one-hot encoded: \\n\", one_hot_cols) \nprint(\"\\n\")\nprint(\"Features that could be target encoded: \\n\", target_encoding_cols)","b2232cab":"def data_engineering(raw_data, is_trainset):\n    # preprocess DataFrame 'raw_data'    \n    \n    \n    # replace nan \n    num_cols = {cname for cname in raw_data.columns if raw_data[cname].dtype in ['int64', 'float64']}\n    cat_cols = {cname for cname in raw_data.columns if raw_data[cname].dtype == \"object\"}\n    raw_data[num_cols].fillna(raw_data[num_cols].median(), inplace=True) # replace nan by median in numerical columns\n    raw_data[cat_cols].fillna(\"missing\", inplace=True) # replace nan by \"missing\" in categorical columns\n    \n    \n    \n    # leave input data unchanged\n    data = pd.DataFrame(columns = raw_data.columns, index=raw_data.index)\n    data = raw_data.copy(deep=True)\n    \n    if is_trainset:\n        # remove outliers from target\n        Q01 = data['SalePrice'].quantile(0.01)\n        Q99 = data['SalePrice'].quantile(0.99)\n        data = data[(data['SalePrice'] > Q01) & (data['SalePrice'] < Q99)]\n        \n        # remove outliers from training set\n        data.drop(data['TotalBsmtSF'][data['TotalBsmtSF'] > 4000].index, inplace=True)\n        data.drop(data['LotArea'][data['LotArea'] > 100000].index, inplace=True)\n        data.drop(data['GrLivArea'][data['GrLivArea'] > 4000].index, inplace=True)\n        \n    \n    # create new columns\n    data['newHouseStyle'] = data['BldgType'] + \"_\" + data['HouseStyle']\n    data['newExterQual'] = data['ExterQual'] + \"_\" + data['ExterCond']\n    data['newGarageQual'] = data['GarageQual'] + \"_\" + data['GarageCond'] + \"_\" + data['GarageFinish']\n    data['newLand'] = data['LandContour'] + \"_\" + data['LandSlope']\n    data['has1stfloor'] = (data['1stFlrSF'] > 0).astype(bool)\n    data['has2ndfloor'] = (data['2ndFlrSF'] > 0).astype(bool)\n    data['newPorchSF'] = data['OpenPorchSF'] +  data['EnclosedPorch'] + data['3SsnPorch'] + data['ScreenPorch']\n    data['hasPool'] = (data['PoolArea'] > 0).astype(int)\n    \n    date_df = data[['YrSold','MoSold']].copy(deep=True)\n    date_df['Day'] = 1 # add a column with a day value\n    date_df.rename(columns={'YrSold' : 'Year', 'MoSold' : 'Month'}, inplace=True)\n    date_df['Date sold'] = pd.to_datetime(date_df[['Year','Month','Day']])\n    data['Date sold'] = date_df['Date sold'] # add to data\n    data.sort_values(by='Date sold', inplace=True) # sort the data by date\n    data['Date_sold_int'] = data['Date sold'].astype(np.int) # could be useful if we need a numerical feature related to the date\n    \n    # log transform features\n    data['log GrLivArea'] = np.log(data['GrLivArea'])\n    data['log LotArea'] = np.log(data['LotArea'])\n    data.drop(['GrLivArea', 'LotArea'], axis=1, inplace=True)\n    \n    if is_trainset:\n        # take log of target and split it from the features\n        data['log SalePrice'] = np.log(data['SalePrice'])\n        \n        \n    \n    \n    \n    # drop columns based on analysis above\n    data.drop(set(data) & columns_to_drop, axis=1, inplace=True)\n    \n    \n    return data\n","6c647256":"train_data = data_engineering(train_data_full, is_trainset=True)\n\ny = train_data['log SalePrice'].copy()\nX = train_data.drop(['log SalePrice'], axis=1).copy();","695aff0c":"import category_encoders as ce\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\ndef preprocess_data(my_cols):\n    # Drop irrelevant columns from DataFrame before passing it to this function\n    # and pass columns of dataframe as input \"my_cols\"\n    \n    \n    # Preprocessing for numerical data\n    numerical_transformer = SimpleImputer(strategy='median')\n\n    # Preprocessing for categorical data\n    one_hot_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n        ('onehot', ce.one_hot.OneHotEncoder(handle_unknown='ignore'))])\n    \n    target_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n        ('target', ce.TargetEncoder(handle_unknown='ignore')),\n        ('imputer2', SimpleImputer(strategy='median'))]) # put second imputer as sometimes TargetEncoder seems to generate nan values. Maybe not optimal\n    \n    # Bundle preprocessing for numerical and categorical data\n    preprocessor = ColumnTransformer(\n    transformers = [('num', numerical_transformer, list(numerical_cols & set(my_cols) )), \\\n                 ('OOE', one_hot_transformer, list(one_hot_cols & set(my_cols) )), \\\n          ('target', target_transformer, list(target_encoding_cols & set(my_cols) ))], ) \n    \n    return preprocessor","36afb73a":"def MAE_score_model(X,y,model):\n    # Compute the MAE by train-test split on the features X and target y (80-20 split)\n    # model can be chosen for comparison\n    \n    preprocessor = preprocess_data(X.columns)\n\n    # Bundle preprocessing and modeling code in a pipeline\n    my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ],verbose=False)\n    \n    # Break off validation set from training data\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,random_state=0)\n    \n    \n    # Preprocessing of training data, fit model \n    my_pipeline.fit(X_train, y_train)\n    \n    \n    # Preprocessing of validation data, get predictions\n    preds = my_pipeline.predict(X_valid)\n    \n    return mean_absolute_error(y_valid, preds)","8b35952e":"def MAE_CV_score_model(X,y,model):\n    # Compute the MAE by cross-validation on the features X and target y\n    # model can be chosen for comparison\n    \n    \n    preprocessor = preprocess_data(X.columns)\n\n    # Bundle preprocessing and modeling code in a pipeline\n    my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ],verbose=False)\n    \n    # Multiply by -1 since sklearn calculates *negative* MAE\n    scores = -1 * cross_val_score(my_pipeline, X, y,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n    \n    \n    return scores.mean()","affde957":"def plot_predict_error(X,logy, model):\n    # Make a plot of the prediction error on the validation data from a train-test split\n    # the target logy is assumed log-transformed, but we plot the graphs for the original target y\n    \n    preprocessor = preprocess_data(X.columns)\n\n    # Bundle preprocessing and modeling code in a pipeline\n    my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ],verbose=False)\n    \n    # Break off validation set from training data\n    X_train, X_valid, y_train, y_valid = train_test_split(X, logy, train_size=0.8, test_size=0.2,random_state=0)\n    \n    \n    # Preprocessing of training data, fit model \n    my_pipeline.fit(X_train, y_train)\n\n    # Preprocessing of validation data, get predictions\n    preds = my_pipeline.predict(X_valid)\n    \n    # plot error\n    fig = plt.figure(figsize=(18,4))\n    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.3, hspace=0.3) # increase space between plots\n    ax0 = fig.add_subplot(131) # add subplot 1 (121 = 1 row, 2 columns, first plot)\n    ax1 = fig.add_subplot(132) # add subplot 2 \n    ax2 = fig.add_subplot(133) # add subplot 3 \n    \n    ax0.scatter(np.exp(y_valid), np.exp(preds)-np.exp(y_valid))\n    ax0.set_title(\"error plot (absolute)\")\n    ax0.set_xlabel(\"price [$]\")\n    ax0.set_ylabel(\"error on price [$]\")\n    \n    MSE = mean_absolute_error(np.exp(y_valid), np.exp(preds))\n    print(\"MSE = \", MSE)\n    ax1.scatter(np.exp(y_valid),(np.exp(preds)-np.exp(y_valid))\/np.exp(y_valid))\n    ax1.set_title(\"error plot (relative)\")\n    ax1.set_xlabel(\"price [$]\")\n    ax1.set_ylabel(\"error on price [%]\")\n    # put axis in percent\n    vals = ax1.get_yticks()\n    ax1.set_yticklabels(['{:,.2%}'.format(x) for x in vals])\n    \n    \n    ax2.hist(np.exp(preds)-np.exp(y_valid), bins = 20, range=(-3*MSE,3*MSE))\n    ax2.set_title(\"error histogram\")\n    ax2.set_xlabel(\"error on price [$]\")\n    ax2.set_ylabel(\"counts\")","4d6517a9":"from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nMAE = MAE_CV_score_model(X[['OverallQual']], y, model)\nprint('MAE = %.2e '%(MAE))","dc425d94":"plot_predict_error(X[['OverallQual']], y, model)","f6bf1c6a":"ranked_num_cols = list(set(ranked_num_cols) & set(X.columns))\n\nfor it in range(1, len(ranked_num_cols)):\n    cols = ranked_num_cols[0:it]\n    MAE = MAE_CV_score_model(X[cols],y,model)\n    print('%d features: MAE = %.2e '%(it, MAE))","d6db764a":"model = LinearRegression()\nMAE = MAE_CV_score_model(X,y,model)\nprint('MAE = %.2e '%(MAE))","586c6f15":"plot_predict_error(X, y, model)","ca39d3f9":"# test impact of parameter n_estimators\n\nfor n_estimators in [50,100,500,1000,5000]:\n    model = RandomForestRegressor(n_estimators, random_state=0)\n    MAE = MAE_score_model(X,y,model)\n    print('MAE = %.2e for n_estimators = %d'%(MAE, n_estimators))\n    ","4fac48fb":"n_estimators=100\nmodel = RandomForestRegressor(n_estimators, random_state=0)\nMAE = MAE_score_model(X[list(set(X.columns) & numerical_cols)],y,model)\nprint('Numerical features only: MAE = %.2e for n_estimators = %d'%(MAE, n_estimators))","138c8f7e":"from sklearn.linear_model import Lasso\n\nfor alpha in [0.0001, 0.0005, 0.001,0.1,0.2,0.5]:\n    model = Lasso(random_state=0, alpha=alpha) \n    MAE = MAE_score_model(X,y,model)\n    print('MAE = %.2e for alpha = %f '%(MAE, alpha))","665a8f8e":"from sklearn.linear_model import Ridge\n\nfor alpha in [0.0001, 0.001, 0.1, 0.2, 0.3, 0.5, 0.7, 1]:\n    model = Ridge(random_state=0, alpha=alpha) \n    MAE = MAE_score_model(X,y,model)\n    print('MAE = %.2e for alpha = %f '%(MAE, alpha))","96a9db90":"# Plot error\n\nmodel =  Ridge(random_state=0, alpha=0.3) \n\nplot_predict_error(X,y,model)","8a4c104d":"from sklearn.ensemble import GradientBoostingRegressor\n\n# let' try gradient boost\n\nfor n_estimators in [10,50,100,500,1000]:\n    learning_rate = 0.1\n    model = GradientBoostingRegressor(random_state=0, n_estimators=n_estimators, learning_rate=learning_rate) \n    MAE = MAE_score_model(X,y,model)\n    print('MAE = %.2e for n_estimators = %d and learning_rate = %f'%(MAE, n_estimators, learning_rate))","fe0b94c0":"# cover over both parameters\n# there are built-in methods like grid search that make this certainly better, \n# but for now we will keep it simple\n\nn_estimators_list = [50,75,100,125,150]\nlearning_rate_list = [0.1, 0.15, 0.2, 0.25, 0.3]\n\nscore_mat = []\n\nfor n_estimators in n_estimators_list:\n    for learning_rate in learning_rate_list:\n        model = GradientBoostingRegressor(random_state=0, n_estimators=n_estimators, learning_rate=learning_rate) \n        score = MAE_score_model(X,y,model)\n        score_mat.append({'n_estimators' : n_estimators, 'learning_rate': learning_rate, 'score': score})\n        \nscore_df = pd.DataFrame(score_mat)\n\n# plot heatmap\nscore_df = score_df.pivot(\"n_estimators\", \"learning_rate\", \"score\")\nsns.heatmap(score_df, annot=True);","8e77910b":"model = GradientBoostingRegressor(random_state=0, n_estimators=75, learning_rate=0.2) \n\nplot_predict_error(X,y,model)","46ff0d69":"from sklearn.neighbors import KNeighborsRegressor\n\nfor n_neighbors in [3,5,10,15,20]:\n    model = KNeighborsRegressor(n_neighbors=n_neighbors, weights='distance') \n    MAE = MAE_score_model(X ,y,model)\n    print('MAE = %.2e for n_neighbors = %d'%(MAE, n_neighbors))","c8b94676":"# check how many features give the optimal result\n\nfor nb_features in range(1, len(ranked_num_cols)):\n    model = KNeighborsRegressor(n_neighbors=10, weights='distance') \n    # take categorical columns only\n    MAE = MAE_score_model(X[ranked_num_cols[0:nb_features]] ,y,model)\n    print('MAE = %.2e for %d best features '%(MAE, nb_features))","762a5ab2":"for n_neighbors in [3,5,10,15,20,25]:\n    model = KNeighborsRegressor(n_neighbors=n_neighbors, weights='distance') \n    # take categorical columns only\n    MAE = MAE_score_model(X[ranked_num_cols[0:3]] ,y,model)\n    print('MAE = %.2e for n_neighbors = %d'%(MAE, n_neighbors))","f41b85c7":"# check if there are outliers in the test set\n        \ntest_outlier_df = test_data_full[(test_data_full['TotalBsmtSF'] > 4000) | (test_data_full['LotArea'] > 100000) | (test_data_full['GrLivArea'] > 4000)]\nrows_outliers = test_outlier_df.index.values\ntest_outlier_df[['TotalBsmtSF', 'LotArea', 'GrLivArea']]","b7cba6a5":"test_data_full.loc[1089, 'TotalBsmtSF'] = 4000\ntest_data_full.loc[1089, 'GrLivArea'] = 4000","7fa6cad1":"test_data = data_engineering(test_data_full, is_trainset=False)\n\nmodel = GradientBoostingRegressor(random_state=0, n_estimators=75, learning_rate=0.2) \n\npreprocessor = preprocess_data(X.columns)\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                  ('model', model)\n                 ],verbose=False)\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X, y)\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(test_data)\n","8a734777":"# compare with train data\n\nfig = plt.figure(figsize=(9,5))\nsns.distplot(train_data_full['SalePrice'])\nsns.distplot(np.exp(preds));\n\nplt.title('comparison between house prices in train and test sets')\nplt.legend(['train', 'test']);\nplt.ylabel('relative count');","16a455b9":"# Put \"Id\" back in place\noutput = pd.DataFrame(columns = ['SalePrice'], index=test_data.index)\noutput['SalePrice'] = np.exp(preds)\noutput = output.merge(test_data_full[['Id']], left_index=True, right_index=True)\n\noutput.sort_values(by=['Id'], inplace=True)\n\noutput[['Id', 'SalePrice']].to_csv('submission.csv', index=False)","70c37ae4":"## Nearest neighbors","bda8c152":"## 2.2 Data fields analysis","6cdc8338":" ## 2.1 Target \n \nThe traget will be the SalePrice.\n\nThe statistics are the following.","933aaebf":"We want to predict sales in the future. Therefore, we should sort our data by the date of the sale. \n\nWe have features with the year and months of the sale, let us combine them to create the sale date.","d5ca0569":"Let's have a look to the categorical features now. First of all, let's visualize the distribution of each categorical feature. \n\nIt doesn't make much sense to keep features that have one value dominating all the others, unless there is a good reason for (for instance if it were a luxury indicator).","95488b1f":"Again, the result is similar to the linear regression.","bcfed8d3":"## 3.3 Random Forest\nLet's try another class of models, namely the Random forest algorithm. There are many more parameters to that model, let's swipe over 'n_estimators'","5dcb99c9":"## 2.6 Categorical columns and encoding","fb7f64f1":"## 2.7 Summary","e9484da1":"The mean error for the price is about 2500$ lower than that for the linear regression. Also, the error histogram is pretty symmetric.","30510fb6":"Visualizing the histograms:","07082a7a":"Let's define some fuctions to evaluate our models.","b3711637":"## 3.1 Defining functions\n\nHere are a few functins used to preprocess the data and display the results.","af3de361":"These graphs give an idea of the relevance of each feature to estimate the SalePrice. Using a regression plot might be misleading as we could be biased towards linear dependencies, but it is still informative (for instance for features like 'YearBuilt').\n\nAn other interesting observation is that some features like 'PoolArea' have a lot of values equal to 0. This most likely means that there is no value at all, and the actual entry could be NAN. The doesn't mean that there is no information in such a feature; for instance we could transform 'PoolArea' into a boolean that codes if the value is 0 or not. However, there are so few points with information here that we can discard the feature.\n\nBesides that, these graphs give some hindsight about outliers:\n* TotalBsmtSF > 4000\n* LotArea > 100000\n* GrLivArea > 4000\n\nFinally, some features do not seem to bring anything (cf. Id, LowQualFinSF, KitchenAbvGr)","0e29e5cc":"Box plots are a great way to get an idea of the number of instances (cardinality), the outliers, and whether there is a correlation with the price.","9a7eac9d":"The Gradient Boost achieves better performance that the linear regression. There are 2 main parameters: the learning rate and the number of estimators. Let's scan the result for both values.","f297a14d":"The error above is the mean error of the log of the target. It is more intuitive to show this back in the original units.","fadb222b":"We already get a good performance with 3 features. Optimizing for the number of neighbours in this case, we get:","70fa2401":"The fact that the error decreases when alpha goes to zero is a good sign: it means that the Lasso is scoring better when keeping all features and that we are not over-fitting with the linear regression. This is also why we obtain similar results in both cases.\n\nIn such a case, it would be interesting to compare with Ridge regression algorithms. The comparison between Lasso and Ridge is given in [this article](https:\/\/towardsdatascience.com\/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b).\n","1ce49f9d":"Let's open the data and analyze its contents.","78febbb9":"As expected, we reach similar values for the error as the minimum of the Lasso algorithm. We can visualize the error plots to see how the errors are distributed (the graphs are shown for the Price target and not for the log of the Price and this is our final objective).","828a3964":"For features that have a low cardinality, one-hot encoding is the best strategy as it keeps all the information in the data. Note that the feature \"has1stfloor\" has only one value and therefore provides no information.\n\nWith higher cardinality, one-hot-encoding will dramatically increase the dimensionality of our data and we could expect some machine learning algorithms to perfom worse (espeically on regression techniques). Therefore we will try to target-encode them (encode them by the average target value for each category value). Refer to https:\/\/maxhalford.github.io\/blog\/target-encoding-done-the-right-way\/ and https:\/\/github.com\/brendanhasz\/target-encoding\/blob\/master\/Target_Encoding.ipynb for more information about target encoding.","5867db6a":"# 4. Predicting values for the test set <a class=\"anchor\" id=\"test_set\"><\/a>","16466d2e":"# 1. Import data <a class=\"anchor\" id=\"analyse_data\"><\/a>\n\nLet's import the libraries and data needed for this competition.","c6383242":"There is only one outlier. It is a house with a surface 20% higher than the maximum dealt with in the model (4000). Therefore, it should not be too much of an approximation to reduce this surface to 4000.","14d835fb":"## 3.2 Benchmarking and linear regressions\nWe want to benchmark the power of what we do: using complicated models is fun but should also serve some purpose. To this end, let's start evaluate the most simple model possible (the kind we would make in Excel in 5 minutes): a linear model on the feature that turned out to be best correlated with the price (namely 'OverallQual').","b81dec67":"We don't have a lot of data (only 1460 rows). We choose to keep prices within the percentiles 0.01 and 0.99 so that our models does not get too influenced by outliers.","6ba7a29c":"As we can see from the histogram above, there is a slight asymmetry in the error on the price.","2799f544":"All remaining features with missing data are categorical features. We will add a \"missing\" value for them instead of nan.\n\nIn case there would be missing data in the numerical features of the test set, we will replace them by the median.","c8886e3d":"## 2.5 Numerical features","db73c15d":"The optimum is obtained with learning rate = 0.2 and 75 estimators.","0dbab042":"The house price distribution obtained for the test set is similar to that in the train set. Some discrepancies can be expected in the very high prices that were removed from the train set because the statistics was too low.","d3aec935":"The model accuracy improved significantly with the first 6 features and then saturates.\n\nIf we add the encoded catergorical features features, we can slightly improve the result. This might be misleading though as target encoding induces to target leakage. ","c15bfd17":"Lets's see how we can infer the other missing data.","2f3e6ddc":"Many features are somewhat correlated between themselves. This is not very surprising: the quality is clearly correlated to the last renovation date for instance. The \"OverallQual\" feature is very correlated with other features because it is actually a good proxy for the price.\n\nThe average correlation of a feature to the other ones is between 0.2 and 0.4. Therefore, we decide to get rid of features whose correlation to the target is below 0.4.","eae7eef3":"Based on the previous section, the Gradient boost algorithm provides the best results. \n\nHowever, this model was trained on a data set that excludes outliers. We should verify if we have similar outliers in our test set. I can see 2 ways to deal with those: \n1. replace the values by the maximum in the acceptable zone\n2. build a second model that does not include the features that have outliers to determine the value for these entries","498aac1d":"## 2.4 Missing data\n\nSome featues have missing data:","5bd5b6d0":"The results above are not very good. The nearest neighbors method does not deal very well with a small number of features. Showing the result as a function of the number of features we keep.","c6bccd3c":"In addition, we can visualize the histograms of the features.","e89e71b8":"## 3.4 Lasso","635da25b":"# 3. Results from Machine Learning models <a class=\"anchor\" id=\"ML_results\"><\/a>","28ce7646":"Let's see the correlations with the target and between features.","0875de66":"Numerical features best analyzed in a scatter plot versus the target. This gives a visual indication of the correlation, the outliers, and how missing data could be interpolated.","2dfed338":"From observing the count plots, we can already remove all features that have either only one value either too many different categories with low statistics for each.","a52fd711":"# 4. Conclusion and next steps <a class=\"anchor\" id=\"conclusion\"><\/a>\n\nThis notebook addresses the improvements suggested in the previous one: https:\/\/www.kaggle.com\/raphalvanroermund\/house-price-prediction\n\nUsing some feature engineering lead to much better results from the linear regressions. This is not surprising given the fact that most relationships are pretty linear in the scatter plots.\n\nSome extensive cleanup of the features is meant to be as predictive as possible on the test set and avoid overfitting.\n\nThere is clearly room for more improvement: other ML models could be tested, some assumptions about the features to keep or not could be refined, etc. Please leave your comments if you have any ideas about this!","a7ec88e6":"From the graph above, the date of the sale doesn't seem to have a strong correlation with price, which seems a bit surprising on a 5-years timescale. The month of the sale impacts the price more that the year on this data set. It is even more surprising as the date range covers the subprime crisis.\n\nTaking the monthly average leads to the same conclusion:","b49c77b5":"The sequence of operations is the following:\n* remove outliers\n* create\/merge new columns based on their description\n* log transform features with asymmetric distributions\n* optional here: sort by selling date for the later train-test split\n* transform the target by taking its log\n* replace missing data (replace \"nan\" by \"missing\" in categorical features, replace by median in numerical features)\n* encode categorical columns (one-hot or target encoding depending on the number of single instances)\n* drop unnecessary columns","df14c6c9":"This distribution is clearly skewed. Some algorithms will perform better with a symmetric distribution (typically linear models or neural networks). We can take the logarithm of the price as the target, which is clearly more symmetric as shown below:","d1b0d862":" Let us see the histogram of the SalePrice.","ee932af6":"The performance is equivalent to the linear regression. it seems that the results do not improve much after n_estimators = 500, so let's keep that value.\n\nLet's try to see if in this case the model makes good use of the categorical features.","428cdb40":"# Introduction\nThis notebook serves is an update of [this one](https:\/\/www.kaggle.com\/raphalvanroermund\/house-price-prediction) in which I made several visualizations of the house price data set. In the current notebook, I refined the data enegineering part in order to better deal with outliers, categorical data, and skewed features.\n\nAs we will see, this leads to a significant improvement of the linear regressions.","110c00d1":"## TOC:\n* [1. Import data](#analyse_data)\n* [2. Feature Engineering](#FE)\n* [3. Results from Machine Learning models](#ML_results)\n* [4. Test set](#test_set)\n* [Conclusion and next steps](#conclusion)","346154cf":"Two features would gain to be log-transformed as we did for the target: GrLivArea and LotArea. ","0181131f":"## 3.5 Gradient boost","24b8512e":"# 2. Feature engineering <a class=\"anchor\" id=\"FE\"><\/a>","266f5a33":"We should make informed choices about the data before junping blindlessly into complicated models. Let us group them by them in order to identify which ones could be related between each other.\n\nLookin at the descriptions in the data set, we can already decide to drop a few columns that have either irrelevant information either information included in other columns:\n* GarageYrBlt: is very strongly correlated with YearBuilt\n* Condition2, which should not bring much more value than Condition1\n* BsmtFinSF1 and 'BsmtFinSF2, whose sum is basically equal to TotalBsmtSF\n* Exterior2nd: mostly included in Exterior1st\n* LotFrontage: scales with LotArea\n* MasVnrArea: scales with GrLivArea (and seems to have a lot of missing entries)\n* BsmtFullBath, BsmtHalfBath and HalfBath: info of FullBath seems sufficient\n* GarageArea: correlated to GarageCars\n* PoolQC: determined by PoolArea\n* MiscFeature and MiscVal: too vague to be reliable\n\nIn addtion, other features could be combined as they relate to very similar features:\n* BldgType and HouseStyle\n* OverallQual and OverallCond\n* ExterQual and ExterCond\n* GarageFinish, GarageQual and GarageCond\n* LandContour and LandSlope\n* 1stFlrSF and 2ndFlrSF can be transformed into a boolean as the area is already present in GrLivArea\n* Sum up 'OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch'","460360cc":"Ok, so a simple linear fit of that feature gives us an error of about 28k\\$. Not so bad for a mean price of 180k\\$. \n\nOur next models will have to do much better than that. How much better do we get if we include more numerical features in the game?","1a44d6ce":"\nTechnically, we should not care too much about data leakage related to time on this very example, and we could do some cross-validation on the training set. \n\nHowever, this doesn't seem good practice if we want to make a model that reamains robust as new data come in and I will rather split the data chornologically. ","b838d159":"We can see that some columns have only very little data missing while others are basically empty. Let's get rid of the features that have more that 75% missing data.","eaa966aa":"## 2.3 Time dependence","cc2f52e8":"Finally, we can output the data to csv for the submission."}}