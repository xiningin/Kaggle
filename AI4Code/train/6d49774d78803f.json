{"cell_type":{"2e441b24":"code","b585aad0":"code","2e51454f":"code","b062dfcd":"code","3fbd1d78":"code","c92e82e7":"code","60b27c50":"code","7a153f3e":"code","96bd0e63":"code","4ff4aceb":"code","cd86856e":"markdown"},"source":{"2e441b24":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b585aad0":"import pandas as pd\nimport numpy as np\nimport os \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sn\nfrom sklearn import preprocessing","2e51454f":"def init():\n    test_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n    train_df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n    test_df[\"Survived\"]=-8888\n    Main_df=pd.concat((train_df, test_df), axis=0, sort=False)\n    print(\"Columns with null values {}\".format(Main_df.columns[Main_df.isnull().any()]))\n\n\n    global title_map\n    title_map={\n    \"mr\": \"Mr\",\"mrs\": \"Mrs\",\"miss\": \"Miss\",\"master\": \"Master\",\"don\": \"Royal\", \"rev\": \"Officer\", \"dr\": \"Officer\",\n    \"mme\": \"Mrs\", \"dona\": \"Royal\", \"jonkheer\" : \"Royal\", \"the countess\": \"Royal\", \"capt\": \"Officer\", \"col\": \"Officer\",\n    \"ms\": \"Mrs\", \"mlle\": \"Miss\", \"major\": \"Officer\", \"lady\" : \"Royal\", \"sir\" : \"Royal\"\n    }\n    return(Main_df)","b062dfcd":"def get_title(x):\n    x=x.split(\",\")[1].split(\".\")[0].strip().lower()\n    return(title_map[x])\n\ndef is_in_group(df):\n    gdf=pd.DataFrame({\"GroupSize\": df.groupby(\"Ticket\")[\"Ticket\"].count()}).reset_index()\n    df=pd.merge(df, gdf, how=\"left\", on=\"Ticket\").set_index(df.index)\n   \n    df[\"Fare\"].fillna(8.0, inplace=True) # Median fare for this route.\n    df[\"ActualFare\"]=df[\"Fare\"].astype(int)\/df[\"GroupSize\"]\n    df[\"IsMother\"]= np.where((df[\"Sex\"]==\"female\") & (df[\"Parch\"] > 0) & (df[\"Title\"] != \"Miss\"), 1, 0) # Mothers\n    \n    df['ST_third_class'] = np.where((df['Ticket'].str.match(pat='(SOTON)|(W.\/C.)|(A\/5)')) & (df[\"Pclass\"]==3) & (df[\"Cabin\"].isnull()), 1, 0)\n    df[\"PC\"]= np.where((df[\"Pclass\"]==1) & (df['Ticket'].str.match(pat='(PC)')) & (df[\"Cabin\"].isnull()), 1, 0)\n    df['ST_WC'] = np.where((df['Ticket'].str.match(pat='(W.\/C.)')) & (df[\"Cabin\"].isnull()), 1, 0)\n    return(df)\n\ndef adjust_columns(df):\n    columns=[\"Title\", \"Sex\",\"Pclass\", \"Embarked\"]\n    df=pd.get_dummies(df, columns=columns)\n    df.drop([\"Name\", \"Ticket\", \"Cabin\", \"Fare\", \"Cabin\"], axis=1, inplace=True)\n    df.drop([\"Title_Royal\"], axis=1, inplace=True) # Not significant contribution\n\n    return(df)\n\ndef age(df):\n    \n    #null mothers age will be given median age of existing mothers\n    cond = df[\"IsMother\"]==1 \n    median = df.loc[cond, \"Age\"].median()\n    df.loc[cond, \"Age\"]=df.loc[cond, \"Age\"].fillna(median)\n\n    # Median age of adults with no childrens and no parents\n    cond=df[\"Parch\"]==0 \n    median = df.loc[cond, \"Age\"].median()\n    df.loc[cond, \"Age\"]=df.loc[cond, \"Age\"].fillna(median)\n\n \n\n    #remaining null values substituted by median age of Sex+Title\n    df[\"Age\"].fillna(df.groupby(['Sex','Title'])[\"Age\"].transform(\"median\"), inplace=True) # Fill age based on median \n    \n    #Now let's see the distribution of age\n    fig, (axis1) = plt.subplots(1,1, figsize=(10,5))\n    sn.distplot(df[\"Age\"], ax=axis1)\n    \n    \n    return(df)","3fbd1d78":"if __name__==\"__main__\":\n    df=init() # Init\n    \n    df[\"Title\"]=df[\"Name\"].map(get_title) #Getting title for each person\n    df=is_in_group(df) # Find group based on tickets and family members#.\n    df=age(df) # get Age values for null Age\n\n    ##Get the survival rate of key features\n    sex_survival_rate=df.loc[df[\"Survived\"]!=-8888, [\"Sex\", \"Survived\"]].groupby(\"Sex\").mean().reset_index()\n    pclass_survival_rate=df.loc[df[\"Survived\"]!=-8888, [\"Pclass\", \"Survived\"]].groupby(\"Pclass\").mean().reset_index()\n    Group_survival_rate=df.loc[df[\"Survived\"]!=-8888, [\"GroupSize\", \"Survived\"]].groupby(\"GroupSize\").mean().reset_index()\n    Title_survival_rate=df.loc[df[\"Survived\"]!=-8888, [\"Title\", \"Survived\"]].groupby(\"Title\").mean().reset_index()\n    ST_survival_rate=df.loc[df[\"Survived\"]!=-8888, [\"ST_third_class\", \"Survived\"]].groupby(\"ST_third_class\").mean().reset_index()\n\n\n    fig, (axis1, axis2, axis3, axis4, axis5) = plt.subplots(1,5, figsize=(15,5))\n    sn.barplot(x='Sex', y='Survived', data=sex_survival_rate, ax=axis1)\n    sn.barplot(x='Pclass', y='Survived', data=pclass_survival_rate, ax=axis2)\n    sn.barplot(x='GroupSize', y='Survived', data=Group_survival_rate, ax=axis3)\n    sn.barplot(x='Title', y='Survived', data=Title_survival_rate, ax=axis4)\n    sn.barplot(x='ST_third_class', y='Survived', data=ST_survival_rate, ax=axis5)\n\n\n\n    \n    scaler = preprocessing.StandardScaler()\n    df[\"ActualFare\"]=scaler.fit_transform(np.array(df[\"ActualFare\"].values).reshape(-1,1))\n    df[\"Age\"]=scaler.fit_transform(np.array(df[\"Age\"].values).reshape(-1,1))\n    df[\"GroupSize\"]=scaler.fit_transform(np.array(df[\"GroupSize\"].values).reshape(-1,1))\n    df[\"Parch\"]=scaler.fit_transform(np.array(df[\"Parch\"].values).reshape(-1,1))\n    df[\"SibSp\"]=scaler.fit_transform(np.array(df[\"SibSp\"].values).reshape(-1,1))\n\n    print(\"Columns with null values {}\".format(df.columns[df.isnull().any()]))\n\n    df=adjust_columns(df) # Feature encoding and selections.","c92e82e7":"#Now model\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV,cross_val_score, KFold, learning_curve, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\nwarnings.filterwarnings(\"ignore\")","60b27c50":"train=df.loc[df[\"Survived\"]!=-8888]\ntest=df.loc[df[\"Survived\"]==-8888]\n\ncols=list(train.columns)\ncols.remove(\"Survived\")\ncols.remove(\"PassengerId\")\nprint(cols)\n\nX_train=train.drop([\"Survived\",\"PassengerId\"], axis=1).values\nY_train=train[\"Survived\"].values\n\nX_test=test.drop([\"Survived\",\"PassengerId\"], axis=1).values\nprint(df.shape, X_train.shape, Y_train.shape, X_test.shape)","7a153f3e":"#Let's use cross validation to find best model\nkfold = StratifiedKFold(n_splits=5)\n\nclf=[\"RandomForest\", \"LogisticRegression\", \"KNN\"]\n\nrandom_state = 2\nclassifiers = []\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(KNeighborsClassifier(n_neighbors=5))\n\nresults, means, std = [], [], []\nfor classifier in classifiers :\n    results.append(cross_val_score(classifier, X_train, y = Y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\nfor result in results:\n    means.append(result.mean())\n    std.append(result.std())\n\nresult_DF=pd.DataFrame({\"Classifiers\":clf, \"ResultMeans\":means, \"ResultSTD\":std })\nprint(result_DF)","96bd0e63":"def grid_search_param(algo):\n    if algo == \"Logistic\":\n        clf=LogisticRegression()\n        c_space = np.logspace(-5, 8, 15) \n        penalty=[\"l1\", \"l2\"]\n        param_grid = {'C': c_space, 'penalty' : penalty}\n    elif algo==\"RandomForest\":\n        clf=RandomForestClassifier()\n        param_grid = { 'n_estimators': [700],\n                      'max_features': ['auto', 'sqrt', 'log2'],\n                      'max_depth' : [6],\n                      'criterion' :['gini', 'entropy'],\n                     }  \n    elif algo==\"KNN\":\n        clf=KNeighborsClassifier()\n\n        param_grid ={'n_neighbors' : [6,7,8,9,10,11,12,14,16,18,20,22],\n                    'algorithm' : ['auto'],\n                    'weights' : ['uniform', 'distance'],\n                    'leaf_size' : list(range(1,50,5))\n                    }\n    return(param_grid, clf)\n\ndef get_best_param(clf, param):\n    grid = GridSearchCV(clf, param, cv = 10) \n    grid.fit(X_train, Y_train) \n    print(\"Tuned   Parameters: {}\".format(grid.best_params_))  \n    print(\"Best score is {}\".format(grid.best_score_)) \n    return(grid)\n\n\ndef run_clf(grid, clf, algo):\n    if algo == \"Logistic\":\n        c_value=grid.best_params_[\"C\"]\n        penalty=grid.best_params_[\"penalty\"]\n        print(\"Best parameter: C is {} and Penalty is {}\".format(c_value, penalty))\n        clf=LogisticRegression(C=c_value, penalty=penalty)\n    \n    elif algo==\"RandomForest\":\n        n_estimators=grid.best_params_[\"n_estimators\"]\n        max_features=grid.best_params_[\"max_features\"]\n        max_depth=grid.best_params_[\"max_depth\"]\n        criterion=grid.best_params_[\"criterion\"]\n        clf=RandomForestClassifier(n_estimators=n_estimators, max_features=max_features, max_depth=max_depth, criterion=criterion)\n    \n    elif algo==\"KNN\":\n        n_neighbors=grid.best_params_[\"n_neighbors\"]\n        algorithm=grid.best_params_[\"algorithm\"]\n        weights=grid.best_params_[\"weights\"]\n        leaf_size=grid.best_params_[\"leaf_size\"]\n        clf=KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=algorithm,weights=weights,leaf_size=leaf_size)\n        \n    clf.fit(X_train, Y_train)\n    print(confusion_matrix(Y_train, clf.predict(X_train)))\n    print(\"{} score is {}\".format(algo, clf.score(X_train,Y_train)))\n    return(clf)\n\n        ","4ff4aceb":"\nprint(\"Running Logistic regression\")\nparam, clf=grid_search_param(\"Logistic\")\n(grid)= get_best_param(clf, param)\nclfLR=run_clf(grid, clf, \"Logistic\")\npred_values=clfLR.predict(X_test)\ntest[\"Survived\"]=pd.Series(pred_values, index=test.index)\ntest[[\"PassengerId\", \"Survived\"]].to_csv(\"LRPrediction.csv\", index=False)\n\nprint(\"Running Random Forest\")\nparam, clf=grid_search_param(\"RandomForest\")\n(grid)= get_best_param(clf, param)\nclfRF=run_clf(grid, clf, \"RandomForest\")\nimportances = clfRF.feature_importances_\nDF=pd.DataFrame({\"Features\": cols, \"Importance\":  importances})\nprint(DF.sort_values(by ='Importance', ascending = False))\n\npred_values=clfRF.predict(X_test)\ntest[\"Survived\"]=pd.Series(pred_values, index=test.index)\ntest[[\"PassengerId\", \"Survived\"]].to_csv(\"RFPrediction.csv\", index=False)\n\n\n#print(\"Running KNN\")\n#param, clf=grid_search_param(\"KNN\")\n#(grid)= get_best_param(clf, param)\n#clfKNN=run_clf(grid, clf, \"KNN\")\n#pred_values=clfKNN.predict(X_test)\n#test[\"Survived\"]=pd.Series(pred_values, index=test.index)\n#test[[\"PassengerId\", \"Survived\"]].to_csv(\"KNNPrediction.csv\", index=False)\n","cd86856e":"**Helper Functions:**\n\n**Get Title:** \n> The function will get the titles of each passanger. titles are broadly divided in to following categories.\n1. Mr\n2. Mrs\n3. Master\n4. Royal families\n5. Officers\n6. Miss\n\n**Is In Group:**\n> In my opinion , ticket number is most un explored feature. I have used it extensively in indentifying the patterns\nBased on ticket number, The function will calculate\/find the \n* group size : Many kernels uses Family size. i think that do not consider the fact that many people are travelling in a group.\n* actual fare : the tickets booked in groups must have common fare. it make sense to divide the fare by group size.  \n* special tickets in third class\/Private cabins etc have been also considered.\n\n\n**Age:**\nWe will impute the missing age values.\n\n\n**Visualization**\n* Created several visualization to show the relationship of critical features w.r.t Survival.\n\nThe code has been modularized for better reading . I have also used Grid search to fine tune the hyper parameters as well as used Cross fold validation to get the mean and standard deviation of each algorithm.\n\npls upvote if you like the kernel :-) \n"}}