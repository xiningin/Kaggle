{"cell_type":{"38992898":"code","ff40ea06":"code","bb053d7e":"code","b8cca49a":"code","af3991ab":"code","c27dccad":"code","1b2c7736":"code","928e8ae2":"code","cdcffe50":"code","1fe09416":"code","8ed7473b":"code","ee8a6276":"code","b4d1dd75":"code","601b9a25":"code","6449d783":"code","ed40e6c3":"code","dc6a8f86":"code","6d2743b6":"code","f171d0d0":"code","d0a654e3":"code","4d66b7cf":"code","9a01a805":"code","61358041":"code","40ea047c":"code","df176712":"code","ea147721":"code","88a9b911":"code","9607b158":"code","5944056a":"code","cf71775a":"code","47e09a7d":"code","3740d09e":"code","ecb54d75":"code","ba231a2e":"code","9d8e479d":"code","811c9d1b":"code","207baaaf":"code","0fc0a873":"markdown","56617d8d":"markdown","b7278154":"markdown","01857705":"markdown","8e72f588":"markdown","5f72b987":"markdown","f93f1c98":"markdown","eb8c9005":"markdown","e4dd8126":"markdown","8b94ff6e":"markdown","44986f2d":"markdown","ff02e7c7":"markdown","8f46fa4b":"markdown","bf28e6d9":"markdown","996d5301":"markdown","533f3752":"markdown","52804aa7":"markdown","c36c26b4":"markdown","0e683e6c":"markdown","d21e9c24":"markdown","96f5830e":"markdown","22481ac4":"markdown","e6ecc0e0":"markdown","1057cc36":"markdown","33b03427":"markdown","39e5fe57":"markdown","7fc9084f":"markdown","69da0de6":"markdown","9325046c":"markdown"},"source":{"38992898":"import tensorflow as tf\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))","ff40ea06":"## Had some problems here with unknown errors corresponding to DistilBERT as mentioned here: https:\/\/github.com\/huggingface\/transformers\/issues\/1829\n# !pip install transformers\n## ... so installed from source\n!pip install git+https:\/\/github.com\/huggingface\/transformers\n\nimport transformers\n\n# We will use the official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py\n    \nimport tokenization","bb053d7e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport string\nimport urllib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\n\nfrom tensorflow.keras.layers import Dense, Input, Flatten, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub","b8cca49a":"train_data = pd.read_csv(\"\/home\/train.csv\")\ntest_data = pd.read_csv(\"\/home\/test.csv\")\nsample_sub = pd.read_csv(\"\/home\/sample_submission.csv\")\n\n#train_data = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\n#test_data = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\n#sample_sub = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nprint('Train data shape: {}'.format(train_data.shape))\nprint('Test data shape: {}'.format(test_data.shape))\nprint('Sample submission shape: {}'.format(sample_sub.shape))","af3991ab":"train_data.head()","c27dccad":"label = 'Disaster', 'Non-Disaster'\ndata = train_data.target.value_counts()\n\nfig1, ax1 = plt.subplots()\nax1.pie(data, labels=label,  startangle=90, autopct='%1.1f%%')","1b2c7736":"train_data.target.value_counts() #Label counts","928e8ae2":"train_data.isnull().sum()","cdcffe50":"## Adopted from https:\/\/www.kaggle.com\/sagar7390\/nlp-on-disaster-tweets-eda-glove-bert-using-tfhub\/comments#data\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train_data[train_data['target']==1]['text'].str.len()\nax1.hist(tweet_len, color='red')\nax1.set_title('disaster tweets')\ntweet_len=train_data[train_data['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","1fe09416":"# Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\nabbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}\n\ndef convert_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n\n# Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\ndef convert_abbrev_in_text(text):\n    tokens = word_tokenize(text)\n    tokens = [convert_abbrev(word) for word in tokens]\n    text = ' '.join(tokens)\n    return text","8ed7473b":"def clean_tweets(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    html=re.compile(r'<.*?>')\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    table=str.maketrans('','',string.punctuation)\n    \n    text = url.sub(r'',text)\n    text = html.sub(r'',text)\n    text = emoji_pattern.sub(r'', text)\n    text = text.translate(table)\n\n    return text","ee8a6276":"test_data.head","b4d1dd75":"sample_sub.head","601b9a25":"%%time\ntrain_data['text']=train_data['text'].apply(lambda x : clean_tweets(x))\ntest_data['text']=test_data['text'].apply(lambda x : clean_tweets(x))\n\ntest_data['text']=test_data['text'].apply(lambda x : convert_abbrev_in_text(x))\ntrain_data['text']=train_data['text'].apply(lambda x : convert_abbrev_in_text(x))","6449d783":"dropout_num=0\nmax_length=160\nmax_sequence_length=512\nlearning_rate_BERT=1e-6\nval_split_BERT=0.2\nepochs_BERT=1\nbatch_size_BERT=8","ed40e6c3":"# Adopted with max_len placeholder for easy parameter manipulation from https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub\ndef bert_encode(texts, tokenizer, max_len=max_sequence_length):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","dc6a8f86":"# Adopted with dropout_num, learning_rate and max_len placeholders for easy parameter manipulation from https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub\ndef build_model(bert_layer, Dropout_num=dropout_num, max_len=max_sequence_length):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    \n    if Dropout_num == 0:\n        # Without Dropout\n        out = Dense(1, activation='sigmoid')(clf_output)\n    else:\n        # With Dropout(Dropout_num), Dropout_num > 0\n        x = Dropout(Dropout_num)(clf_output)\n        out = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=learning_rate_BERT), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","6d2743b6":"%%time\n##module_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/2\"\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/2\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","f171d0d0":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","d0a654e3":"# Adopted the max_len with placeholders from https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert?scriptVersionId=42583022\ntrain_input = bert_encode(train_data.text.values, tokenizer, max_length)\ntest_input = bert_encode(test_data.text.values, tokenizer, max_length)\ntrain_labels = train_data.target.values","4d66b7cf":"model_BERT_1024 = build_model(bert_layer, max_len=max_length)\nmodel_BERT_1024.summary()","9a01a805":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history_1024 = model_BERT_1024.fit(\n    train_input, train_labels,\n    validation_split=val_split_BERT,\n    epochs=epochs_BERT,\n    callbacks=[checkpoint],\n    batch_size=batch_size_BERT\n)","61358041":"%%time\nmodel_BERT_1024.load_weights('model.h5')\ntest_pred = model_BERT_1024.predict(test_input)","40ea047c":"# Thanks to https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert?scriptVersionId=42583022\n# Prediction by BERT model with my tuning for the training data - for the Confusion Matrix\n%%time\ntrain_pred_BERT = model_BERT_1024.predict(train_input)\ntrain_pred_BERT_int = train_pred_BERT.round().astype('int')","df176712":"sample_sub['target'] = test_pred.round().astype(int)\nsample_sub.to_csv(\".\/submission.csv\", index=False, header=True)","ea147721":"check_sample = pd.read_csv(\".\/submission.csv\")\ncheck_sample.head(20)","88a9b911":"# For DistilBERT:\n## https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-distilbert-in-tf\n\n%%time\ntransformer_layer = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')\ntokenizer_DistilBERT = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')","9607b158":"def build_model_DistilBERT(transformer, max_len=max_sequence_length):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=learning_rate_BERT), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","5944056a":"model_DistilBERT = build_model_DistilBERT(transformer_layer, max_len=max_length)\nmodel_DistilBERT.summary()","cf71775a":"train_input_DistilBERT = bert_encode(train_data.text.values, tokenizer_DistilBERT, max_len=160)\ntest_input_DistilBERT = bert_encode(test_data.text.values, tokenizer_DistilBERT, max_len=160)\ntrain_labels_DistilBERT = train_data.target.values","47e09a7d":"train_history_DistilBERT = model_DistilBERT.fit(\n    train_input, train_labels,\n    validation_split=val_split_BERT,\n    epochs=epochs_BERT,\n    batch_size=batch_size_BERT\n)","3740d09e":"%%time\ntest_pred_DistilBERT = model_DistilBERT.predict(test_input_DistilBERT, verbose=1)","ecb54d75":"# Thanks to https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert?scriptVersionId=42583022\n# Prediction by BERT model with my tuning for the training data - for the Confusion Matrix\n%%time\ntrain_pred_DistilBERT = model_DistilBERT.predict(train_input_DistilBERT)\ntrain_pred_DistilBERT_int = train_pred_DistilBERT.round().astype('int')","ba231a2e":"sample_sub['target'] = test_pred_DistilBERT.round().astype(int)\nsample_sub.to_csv('submission_DistilBERT.csv', index=False, header=True)\n\ncheck_sample_DistilBERT = pd.read_csv(\".\/submission_DistilBERT.csv\")\ncheck_sample_DistilBERT.head(20)","9d8e479d":"# Showing Confusion Matrix, thanks to https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert?scriptVersionId=42583022\ndef plot_cm(y_true, y_pred, title, figsize=(5,5)):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","811c9d1b":"# Showing Confusion Matrix for BERT model\nplot_cm(train_pred_BERT_int, train_data['target'].values, 'Confusion matrix for BERT 1024 model', figsize=(7,7))","207baaaf":"# Showing Confusion Matrix for BERT model\nplot_cm(train_pred_DistilBERT_int, train_data['target'].values, 'Confusion matrix for DistilBERT model', figsize=(7,7))","0fc0a873":"\n---\n**There was an error while cleaning the data so only the train data was cleaned, not the test data. This has with high probability falsified the values of the following runs:**\n\n---\n\n**Using the bert_en_uncased_L-24_H-1024_A-16\/2 model (without cleaning the test data):**  \n  - batch_size=16, validation_split=0.2, max_sequence_length=512, lr=6e-6, decay = 0.01, test_size=0.2 -> val_acc 83.45% -> score: 82.439%\n  - batch_size=16, validation_split=0.3, max_sequence_length=512, lr=1e-6, test_size=0.2, epoche=3 -> val_acc 82.67% \n  - batch_size=16, validation_split=0.3, max_sequence_length=512, lr=1e-6, test_size=0.2, epoche=5 -> val_acc 82.31% \n  - batch_size=8, validation_split=0.2, max_sequence_length=512, lr=6e-6, test_size=0.2, epoche=5 -> val_acc 83.26% -> score: 81.703% \n  - batch_size=16, validation_split=0.2, max_sequence_length=512, lr=2e-5 -> val_acc 83.32% -> score: 82.960%\n  - batch_size=8, validation_split=0.2, max_sequence_length=160, lr=2e-5 -> val_acc 83.06% -> score: 82.776%\n\n\n--- \n\nNot running on kaggle using 1024 model: Everythin over batch_size>16 when max_sequence_length is over 100\n\n---\n","56617d8d":"# Data Cleaning (Feature engineering)\n\nBefore being able to start with the classification, the data must be cleaned up and transferred into a proper structure (tokenization) so that the classifier is able to handle the tweets.\n\n---","b7278154":"## Data description\nFiles\n\n    train.csv - the training set\n    test.csv - the test set\n    sample_submission.csv - a sample submission file in the correct format\n\nColumns\n\n    id - a unique identifier for each tweet\n    text - the text of the tweet\n    location - the location the tweet was sent from (may be blank)\n    keyword - a particular keyword from the tweet (may be blank)\n    target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n","01857705":"# Showing Confusion Matrices for BERT models (to compare)\n\nThe scores are not bad to test different models against each other but since it is only possible to upload 5 submissions a day it is not sufficient when running several projects\/models. I also compared the val_acc and loss numbers with each other to get a better understanding but what really helps in my opinion are these model confusion matrices when displayed next to each other.","8e72f588":"As you can see in the confusion matrices, it is almost identical for one epoche. Slight differences can be seen in the false positive\/negative with the BERT 1024 model being supperior.\n\n---","5f72b987":"Cleaning and exchanging abbreviations, links, smileys.","f93f1c98":"# Introduction\n\n---\n[This notebook](https:\/\/www.kaggle.com\/alexs2020\/bert-1024-and-distilbert-comparison) shows my approach for the classification of desaster tweets regarding the [NLP Twitter kaggle challange]( https:\/\/www.kaggle.com\/c\/nlp-getting-started\/overview).\n\nMy initial idea was to compare several BERT implementations with each other ([BERT Uncased 1024](https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/2), [BERT Uncased 768](https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/2), [DistilBERT](https:\/\/huggingface.co\/distilbert-base-uncased)) to get a feeling how well they performe given the task of the kaggle challange. For this, reasearch was done by getting used to the [BERT paper](https:\/\/arxiv.org\/pdf\/1810.04805v2.pdf) to understand the basic foundations of the algorithm as well as the tokenizer. This [colab notebook ](https:\/\/colab.research.google.com\/github\/jalammar\/jalammar.github.io\/blob\/master\/notebooks\/bert\/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb#scrollTo=q1InADgf5xm2) which is trying to visualize some things was a great inspiration and helped a lot to setup my initial notebook.\n\n---\nThis notebook is seperated into different sections, relating to the common data science workflow (except the hypothesis and data collection where already done). \n\n1.   Setup and Check Infrastructure\n2.   Having a first look at the Data (EDA)\n3.   Helper Functions for Data Cleaning\n4.   Data Cleaning (Feature Engineering)\n5.   Pre-trained Bert Uncased model 1024 (& 768) (Model building 1\/2)\n6.   Pre-trained DistilBert model ((Model building 2\/2)\n7.   Showing Confusion Matrices for BERT models (to compare \/ evaluate)\n\n---\n\n\n## Approach:\n\nI started with common values mentioned in different notebooks on kaggle (see inspiration in first section) to get a feeling what influences what and how does it generelly work. After gaining some experience I combined several approaches from the notebooks and general information from the internet. The most promising was one relatively at the beginning with a kaggle score of 84.247% (userID: AlexS2020, rank: 116 at 14.09.2020). This involved a relatively simple notebook running on kaggle, using the parameters mentioned in the list in section 5. It was not possible for me to beat this afterwards, although I tried a lot of different ways and optimizations (added apprevations, cleaned only partly, left links in the tweets, emojis, ...).\n\n## Evaluation:\nAs will be mentioned in the last section, I used to compare the different val_acc and loss values of different models as well as uploading the submission files to get a direct comperision via the score. Last step to evaluate are the confusion_matrices to see how many false positive\/negative and true postivie\/negative.\n\n## Conclusion:\nOverall it was an interesting project \/ competition which enabled me to learn new stuff and try out several tools. It is interesting to see, that the lightweight DistilBERT model is relatively competitive to the way larger BERT 1024 model and the scores do not differ that much. This gets even more interesting when comparing the runtime the 1024 model needs in an equal setup 1458 seconds per epoche, the DistilBERT just 248 seconds (with almost same results after one epoche, see the confusion matrices)!\n\nLast learning for today: DistilBERT tends to overfit when using higher learning rates and more epochs way faster then the BERT 1024 model.\n\n## Outlook:\nThere are several ways how to improve these scores from my point of view. Some of them can be seen in other notebooks already (especially the larger ones).\n\n1.   Run the notebook on larger hardware to play around with larger batch_sizes>16 and larger len_max>160.\n2.   Adding dropouts to fight the overfitting\n3.   Using different models behind each other as mentioned in the BERT paper to optimize output\n4.   Modify the uncased model of BERT with more\/different layers and play around with different activation functions, here for example: https:\/\/www.kaggle.com\/sokolheavy\/multi-dropout-aug-oof\n5.   Write a script on local machine running different learning rates and other parameters so that different optimas can be found. \n6.   An adaptive learning rate could also help with optimizing the results\n\n## Limitations:\n- Some of the points mentioned in Outlook were not possible since running code in browser + only 36h GPU-time per week is hardly enough\n- Automation of parameter search not feasible\n- Getting OOR erros quite offen due to limited GPU storage (its quite big for free use case, still...)\n\n---","eb8c9005":"Saving the prediction to the submission file - probably most important step of this notebook...","e4dd8126":"Predicting (classifying) the test tweets with the pretrained and finetuned model.","8b94ff6e":"This is where the real training\/finetuning of the pretrained model happens, everything with placeholder so they can all be manipulated at the start of this section without jumping around in the code.","44986f2d":"Building the model with given Shape as stated in the summary below. When shape is to large, OOM is thrown.","ff02e7c7":"# Pre-trained Bert Uncased model 1024 (& 768) (Model building 1\/2)\n\nLoading different BERT models (english) and testing them. General takeaways:\n\nIn direct comparison between the 1024 and 768 model, the larger model was mostly more accurate, although it needed more time due to more learnable parameters (almost double the time). So for a slightly less accurate model you can safe almost half the training time, this can be especially relevant and interesting for less performaned hardware I would say.\n\nThis gets even more interesting when comparing against the DistilBERT model, the 1024 needs in an equal setup 1458 seconds per epoche, the DistilBERT just 248 seconds (with almost same results after one epoche, see the confusion matrices)!\n\nDue to a coding error at the beginning of the project, the first trainings where always with a cleaned training dataset and an uncleaned test dataset. This error remained unnoticed for a while and eventually got solved. This point is stated clearly in the following list of scores and valuation_accuracys.\n\n---\n\n### **Default epoche=3 and max_len=160 of layers if not mentioned otherwise.**\n\nUsing the bert_en_uncased_L-24_H-1024_A-16\/2 model:\n  - batch_size=16, validation_split=0.2, max_sequence_length=512, lr=1e-5\n  - batch_size=16, validation_split=0.2, max_sequence_length=512, lr=6e-6 -> seems to overfit when using over 5 epochs (val_acc decreases slightly or stagnates, val_acc 11 % gap compared to acc), best result with 4 epochs\n  - batch_size=8, validation_split=0.2, max_sequence_length=512, lr=6e-6, epochs=5 -> score: 84.247 --- **best score yet** ---\n  - batch_size=16, validation_split=0.2, max_sequence_length=100, lr=2e-5, decay = 0.01 -> val_acc looks good, only 2% discrepancy to acc in second epoche, 4% in third -> score: 83.30%\n  - batch_size=16, validation_split=0.2, max_sequence_length=320, lr=2e-5, decay = 0.01 -> val_acc looks good, only 2% discrepancy to acc in second epoche, 4% in third -> score: 83.38%\n  - batch_size=16, validation_split=0.2, max_sequence_length=512, lr=2e-5, decay = 0.01 -> val_acc 84.31, only 4% discrepancy to acc in third epoche -> score: 83.879%\n  - batch_size=16, validation_split=0.2, max_sequence_length=512, lr=6e-6, decay = 0.01 -> val_acc 83.39% -> score: 81.795\n  - batch_size=16, validation_split=0.2, max_sequence_length=512, lr=6e-6, decay = 0.01, test_size=0.2 -> val_acc 83.91% -> score: 82.19%\n  - batch_size=16, validation_split=0.2, max_sequence_length=512, lr=1e-6, decay = 0.01, test_size=0.2, epochs=5 -> val_acc 83.13% -> score: 81.520%\n\nWithout clean_tweets and abbrevations:\n  - batch_size=16, validation_split=0.2, max_sequence_length=512, lr=2e-5 -> val_acc 81.62% -> score: 83.512%\n\nWith abbrevations without clean_tweets:\n  - batch_size=8, validation_split=0.2, max_sequence_length=512, lr=6e-6 -> val_acc 83.45% -> score: 83.634%\n\n---\nUsing the bert_en_uncased_L-12_H-768_A-12\/2 model:\n  - batch_size=16, validation_split=0.2, max_lemax_sequence_lengthngth=512, lr=2e-5, decay = 0.01 -> val_acc=82.99% -> score=82.42% (overfitting)\n  - batch_size=32, validation_split=0.2, max_sequence_length=512, lr=2e-5, decay = 0.01 -> val_acc=82.80% -> score=82.35% (overfitting)\n","8f46fa4b":"Checking how many true (1) and false (0) desaster tweets are in the set. Looks fairly distributed so no hustle due to an imbalanced dataset needed here.","bf28e6d9":"Parameters for BERT model:","996d5301":"# Setup & Check Infrastructure\n\nThe first step is to get the infrastructure up and running. Important to mention here is that it is strongly recommended to use a GPU for calculation purpose. Next step in this section is to install the needed tools (for DistilBERT and tokenizer) and to import all needed librarys I used at some point in this project.","533f3752":"So we beginn here with a short description of the data files, followed by the some general inspiration links which helped at different stages throughout the dev process. At some points I also used functions from these repos to optimize my own code.\n\n## References:\nInspirations for implementation and design: \n- https:\/\/www.kaggle.com\/c\/nlp-getting-started\/notebooks\n- https:\/\/www.kaggle.com\/bryanafreeman\/disaster-tweets-with-nlp\n- https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\n- https:\/\/www.kaggle.com\/choubane\/lstm-for-disaster-tweets-80-validation-accuracy\n- https:\/\/colab.research.google.com\/github\/jalammar\/jalammar.github.io\/blob\/master\/notebooks\/bert\/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb#scrollTo=q1InADgf5xm2\n\nI hope you enjoy going through my first kaggle notebook, if there are any questions or proposals how to do stuff better - please let me know!\n\n---","52804aa7":"Import Librarys for data visualization, data cleaning and ML models","c36c26b4":"# Having a first look at the Data (EDA)\n\nBefore we can train something which classifies the phrases we first have to take a look what data we have available. An important part is to check whether there is an unacceptable difference between the different labeled groups. Afterwards we do the regular checking of NaNs and then visualizing the word counts in the tweets. ","0e683e6c":"Dropouts got mostly OOM errors. On bigger hardware it would be good to play around with it.\n\nPlayed around with different learning rates, going from 1e-5 to 6e-6, some of them did not converge in the given epochs or got OOMs.\n\n---","d21e9c24":"# Pre-trained DistilBERT model (Model building 2\/2) and comparison\n\nThis is a good evaluation model to test against the BERT 1024 one, since it is more lightweight then the other two BERT models, optimized for low performance hardware. Most of the comments\/explanations from the section beforehand apply and are not listed again.\n\n---\n\n**Example 1 comparison with same parameters:**\n* dropout_num=0\n* max_length=160\n* max_sequence_length=512\n* learning_rate_BERT=2e-6\n* val_split_BERT=0.2\n* epochs_BERT=3\n* batch_size_BERT=8\n\n\n**BERT 1024 score: 82.837 % running appr. 18 min for 3 epoches**\n\n**DistilBERT score: 82.224 % running appr. 4 min for 3 epoches**\n\n---\n\n**Example 2 comparison with same parameters:**\n* dropout_num=0\n* max_length=160\n* max_sequence_length=512\n* learning_rate_BERT=1e-5\n* val_split_BERT=0.2\n* epochs_BERT=5\n* batch_size_BERT=8\n\n**BERT 1024 score: 83.910 % running appr. 35 min for 5 epoches**\n\n**DistilBERT score: 81.336 % running appr. 7 min for 5 epoches**\n\n---\n\n\n**Example 3 comparison with same parameters:**\n- dropout_num=0\n- max_length=160\n- max_sequence_length=512\n- learning_rate_BERT=6e-6\n- val_split_BERT=0.2\n- epochs_BERT=5\n- batch_size_BERT=8\n\n**BERT 1024: 421s 553ms\/step - loss: 0.1136 - accuracy: 0.9585 - val_loss: 0.6983 - val_accuracy: 0.7905**\n\n**DistilBERT: 89s 117ms\/step - loss: 0.1950 - accuracy: 0.9286 - val_loss: 0.5285 - val_accuracy: 0.8142**\n","96f5830e":"Checking the number of characters showing up per tweet in the two groups. They look pretty much alike, no huge discrepancy visible. A lot of Tweets seem to be in the range of 100 - 140 characters.","22481ac4":"Installing transformer needed for encoding (BERT)","e6ecc0e0":"Having a look at the data and how often NaN shows up. Its obvious that location misses in around a third of the tweets, keywords are only rarely missing.","1057cc36":"---\nRunning into allocation errors here all the time so had to try different options\n- OOM (out of Memory) when allocating tensor with shape[16,16,64,160] and type float on \/job:localhost\/replica:0\/task:0\/device:GPU:0 by allocator GPU_0_bfc\n\nMajor problem seems to be batch_size=16 with max_sequence_length=512 which is to high for colab with a validation split of 0.2. GPU storage is full then. This can partly be resolved when developing on kaggle since the following setups are running smothly when not on colab:\n\n\nRelating threads: \n- https:\/\/www.kaggle.com\/c\/tensorflow2-question-answering\/discussion\/118868\n- https:\/\/stackoverflow.com\/questions\/59617755\/training-a-bert-based-model-causes-an-outofmemory-error-how-do-i-fix-this\n- https:\/\/stackoverflow.com\/questions\/50760543\/error-oom-when-allocating-tensor-with-shape\/50764934\n\n---","33b03427":"# Helper functions\n\nAs pictured in the above list there is a lot of information in these tweets which is potentially not relevant for our classification task. In order to optimize our runtime and to focuse on the important information we have to filter the tweets. This will be done in the next section EDA 2\/2.","39e5fe57":"When running trainings with larger epochs (3-5) it is very clear that the DistilBERT has less error rates on the training data and overfits slightly more than the larger models. This effects intensivies when largening the learning rate ...\n\n---","7fc9084f":"Here the data is loaded for easy access in the project. These paths have to be manipulated if running in colab.\n\nPrinting the shapes to get a feeling for the structure before jumping into the next section.","69da0de6":"Checking if GPU is enabled, CPU will take way longer. Should say at least \"1\"","9325046c":"In the tweets there are many different special characters and links, which should be removed at the beginning since the take calculation power and do not add value to the classification. I combined several cleanings in the helper functions to keep the lambda function calls as clean as possible.\n\nAlso tried different versions as mentioned in the next section with cleaning only specific things up (in example links, tags, emojis) or just cleaning the train data, not the test data. Overall, it worked better with the functions in place."}}