{"cell_type":{"a78088ad":"code","7d7bab79":"code","39a29043":"code","f624206e":"code","42b9561d":"code","42644333":"code","7dc1bf9e":"code","a15a1009":"code","54ca8c2e":"code","d52993fd":"code","5e34d761":"code","588d54b5":"code","c587ec01":"code","56c44dc0":"code","9d9cb6c9":"code","504a7eb2":"code","77bbef0a":"code","5e94f978":"code","632a8952":"code","17187867":"code","8faefd85":"code","44012a8d":"code","b5e10090":"code","2be1801c":"code","2c457cdd":"code","4cfd0317":"code","a4ebe203":"code","9b44afe9":"code","5f02d364":"code","ed54bd56":"markdown","295da826":"markdown","2125718d":"markdown","8ab22637":"markdown","cda7d217":"markdown","d5f29fff":"markdown","74fd8084":"markdown","2013dfbc":"markdown","7c102a6b":"markdown","78475d76":"markdown","e037a751":"markdown"},"source":{"a78088ad":"# import dataset \nimport pandas as pd \nimport warnings \nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7d7bab79":"base_dataset=pd.read_csv(\"\/kaggle\/input\/google-play-store-apps\/googleplaystore_user_reviews.csv\")\nbase_dataset.shape","39a29043":"base_dataset.head()","f624206e":"base_dataset['Sentiment'].unique()","42b9561d":"base_dataset.dropna(axis=0,inplace=True)","42644333":"base_dataset.shape","7dc1bf9e":"base_dataset['Sentiment'].unique()","a15a1009":"x=[]\nfor i in base_dataset['Sentiment']:\n    if i=='Positive':\n        x.append(0)\n    elif i=='Neutral':\n        x.append(1)\n    else :\n        x.append(2)","54ca8c2e":"base_dataset['y']=x","d52993fd":"base_dataset.head()","5e34d761":"base_dataset['y'].value_counts()","588d54b5":"base_dataset=base_dataset.sample(8000)","c587ec01":"unique_words=[]\ntotal_words=[]\nfor i in base_dataset['Translated_Review'].str.split():\n    for j in i:\n        total_words.append(j)\n        if i not in unique_words:\n            unique_words.append(i)\nlen(unique_words),len(total_words)","56c44dc0":"x=[]  \nfor i in unique_words:  \n    count=0  \n    for j in base_dataset['Translated_Review']:  \n        for k in j.split(): \n            if i==k:\n                count=count+1  \n    x.append([i,count])","9d9cb6c9":"import matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nplt.subplots(figsize=(12,12))\nwordcloud=WordCloud(background_color=\"white\",width=1024,height=768).generate(\" \".join(total_words))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","504a7eb2":"from nltk.corpus import stopwords  \nfrom nltk.tokenize import word_tokenize  \n \nexample_sent = \"This is a sample sentence, showing off the stop words filtration.\"  \n \nstop_words = set(stopwords.words('english'))  \n \nword_tokens = word_tokenize(example_sent)  \n \nfiltered_sentence = [w for w in word_tokens if not w in stop_words]  \n \nfiltered_sentence = []  \n \nfor w in word_tokens:  \n    if w not in stop_words:  \n        filtered_sentence.append(w)  \n \nprint(word_tokens)  \nprint(filtered_sentence) ","77bbef0a":"# import these modules \nfrom nltk.stem import PorterStemmer \n\n   \nps = PorterStemmer() \n  \n# choose some words to be stemmed \nwords = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"] \n  \nfor w in words: \n    print(w, \" : \", ps.stem(w))","5e94f978":"x=base_dataset['Translated_Review']\ny=base_dataset['Sentiment']","632a8952":"y.unique()","17187867":"from sklearn.model_selection import train_test_split\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.2)","8faefd85":"print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","44012a8d":"from sklearn.feature_extraction.text import CountVectorizer  \n# Fit the CountVectorizer to the training data  \nvect = CountVectorizer(stop_words='english',strip_accents='ascii',max_features=1000).fit(X_train)  \nlen(vect.get_feature_names())","b5e10090":"X_train.shape","2be1801c":"# transform the documents in the training data to a document-term matrix  \nX_train_vectorized = vect.transform(X_train)  \nX_train_vectorized","2c457cdd":"X_train_vectorized.toarray()","4cfd0317":"x_test_transformed = vect.transform(X_test)","a4ebe203":"from sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier()\ndt.fit(X_train_vectorized,y_train)\n\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test,dt.predict(x_test_transformed)))\n\nfrom sklearn.metrics import accuracy_score\nprint(\"accuracy_score : \",accuracy_score(y_test,dt.predict(x_test_transformed)))","9b44afe9":"from sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(random_state=121)\nrf.fit(X_train_vectorized,y_train)\nrf.predict(x_test_transformed)\n\npd.DataFrame([X_test.values,rf.predict(x_test_transformed)]).T\n\nrf.predict(x_test_transformed)[1:20] # first 20 sentiment prediction","5f02d364":"test=vect.transform([\"the app is nice fantastic in using and fun to use\"])\n\nrf.predict(test)","ed54bd56":"### stop words\n\n#### --> process for stopwords removal ","295da826":"### Train_test_split datadet","2125718d":"## Google app review on sentiment and translated_review classification of sentiment","8ab22637":"### stemming - concept","cda7d217":"# Actual model","d5f29fff":"### final prediction","74fd8084":"## wordCloud ","2013dfbc":"### count vctorization","7c102a6b":"# Manual Pre Processing","78475d76":"### DecisionTree","e037a751":"### RandomForest"}}