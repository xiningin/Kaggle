{"cell_type":{"5755dbd3":"code","df2a1f49":"code","ae41d5ed":"code","a75cbc13":"code","bde95519":"code","a9524097":"code","1019475d":"code","68e27aee":"code","be0755e8":"code","db31c4e4":"code","97edbba5":"code","3bd09683":"code","9fe3c996":"code","2a003e5e":"code","9ba8c9dc":"code","9fc0b8b4":"code","6c191d4e":"code","279ddc58":"code","1ec2aab9":"code","64948a82":"code","9ab7190b":"code","5007aba8":"code","26db37b1":"code","31ec11da":"code","de1117db":"code","a1ce761c":"code","e6676393":"code","3131ca80":"code","f6432b49":"code","9d29ee0c":"code","e6328e5a":"code","3f051500":"code","eaca31f6":"code","cb3053eb":"code","e6f7c2a3":"code","5ad04f0a":"code","423ca9ff":"code","6da9a4b4":"code","ed3d0c40":"code","8075356e":"code","d50dd54e":"code","0dcc61e5":"code","3931e194":"code","2c8cdee2":"code","4e30c0e3":"code","4e811d9e":"code","820cc927":"code","46301c53":"code","f8902668":"code","576cf37b":"code","ea2deecb":"code","4b093595":"code","d8178eed":"code","0d7dea1b":"code","839666a5":"code","18bec010":"code","57c9ce03":"code","bf57e5d3":"code","0430784e":"code","02cd6f74":"code","85aaa486":"code","87de8c5d":"code","8830b9da":"code","518faa6a":"code","356aa82f":"code","076bb134":"code","7babaf24":"code","86e33137":"code","0dde912b":"code","c9c7cf72":"code","a9aa4374":"code","83edb09a":"code","da106eae":"code","5ddd6d8d":"code","f4987b31":"code","50a8ef55":"markdown","5027daa8":"markdown","108f1f32":"markdown","b4b03d27":"markdown","dfc8a354":"markdown","a94dffdf":"markdown","84a4a3a1":"markdown","14517e64":"markdown","3987c0e4":"markdown","cf9394be":"markdown","e89254f7":"markdown","37103a47":"markdown","caffa9c7":"markdown","937fe22f":"markdown","89454bc9":"markdown","ee007aed":"markdown","7dd78431":"markdown","0b332b5e":"markdown","1d7f1777":"markdown","1115addf":"markdown","0e4ff391":"markdown","3367a948":"markdown","36f0928e":"markdown","42d49400":"markdown","fbfd7017":"markdown","3d270b2c":"markdown","5f1d16f7":"markdown","f39f1b3d":"markdown","10205574":"markdown","0199453a":"markdown","486cad36":"markdown","722de789":"markdown","1fde054d":"markdown","796e6f21":"markdown","56ce6407":"markdown","37148698":"markdown","016003fb":"markdown","ce1d7830":"markdown","f5767288":"markdown","a08a431b":"markdown","624b26a6":"markdown","76d035f3":"markdown","21d3f58d":"markdown","3c9a65c1":"markdown","05282735":"markdown","125dffd9":"markdown","f8fdd3e5":"markdown","3bd95df0":"markdown","2ebf414d":"markdown","4e774ea2":"markdown","7e88fadb":"markdown","04f54a62":"markdown","233f21cd":"markdown","86f0ab10":"markdown","49782aa3":"markdown","e466120e":"markdown","8bf22ab9":"markdown","a8732064":"markdown","b3fe378e":"markdown","8a1827ed":"markdown","d71650c4":"markdown","20c4b75d":"markdown","572bbaab":"markdown","665eb2d6":"markdown","b9eae333":"markdown","231af1ac":"markdown","a0347745":"markdown","c2daaefd":"markdown","af1bbb92":"markdown","54c6343d":"markdown","c3144679":"markdown","61316333":"markdown","b3ee29f4":"markdown","3d5d3b4a":"markdown","23b692aa":"markdown","be3308e8":"markdown","7ea334ea":"markdown","e747c69e":"markdown","efd8235a":"markdown","101a1bbe":"markdown","bc8b7c9b":"markdown","94d24765":"markdown","b2c3857a":"markdown","05ecace2":"markdown","15366b98":"markdown","a77d66b2":"markdown","90c62ef7":"markdown","a8b7b20c":"markdown","48d2c9ca":"markdown","61a598d5":"markdown","6191301f":"markdown","e8b74e9c":"markdown","8a7b80f2":"markdown","5ca0050d":"markdown","acbd07e2":"markdown","01a5c8a1":"markdown","bf8d3bec":"markdown","50831b2d":"markdown","be2f1451":"markdown","3cd9c2ec":"markdown","590b4359":"markdown","f3eaf152":"markdown","426bf221":"markdown","b12f2338":"markdown","04b4228e":"markdown","a93ed8b4":"markdown","53df1048":"markdown","dcd3db20":"markdown","290dc04d":"markdown","6f9c3d24":"markdown","03e4f7f0":"markdown","593f9352":"markdown","aabfe67a":"markdown","2e34c64d":"markdown","29c21cd9":"markdown","04c5c3c3":"markdown","4be6ff34":"markdown","521169f8":"markdown","1c7e4bb5":"markdown"},"source":{"5755dbd3":"import os\nimport tarfile\nimport urllib\nimport pandas as pd\n\nDOWNLOAD_ROOT = \"https:\/\/raw.githubusercontent.com\/ageron\/handson-ml2\/master\/\"\nHOUSING_PATH = os.path.join(\"datasets\", \"housing\")\nHOUSING_URL = DOWNLOAD_ROOT + \"datasets\/housing\/housing.tgz\"\n\n# Create dataset directory, download housing.tgz, extract housing.csv.\ndef fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n    os.makedirs(housing_path, exist_ok=True)\n    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n    urllib.request.urlretrieve(housing_url, tgz_path)\n    housing_tgz = tarfile.open(tgz_path)\n    housing_tgz.extractall(path=housing_path)\n    housing_tgz.close()\n\n# DataFrame.\ndef housing_data(housing_path=HOUSING_PATH):\n    csv_path = os.path.join(housing_path, \"housing.csv\")\n    return pd.read_csv(csv_path)","df2a1f49":"fetch_housing_data()\nhousing = housing_data()","ae41d5ed":"housing.head()","a75cbc13":"housing.info()","bde95519":"housing['ocean_proximity'].value_counts()","a9524097":"housing.describe()","1019475d":"%matplotlib inline\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(20, 15))","68e27aee":"import numpy as np\n\ndef split_train_test(data, test_ratio):\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]\n\ntrain_set, test_set = split_train_test(housing, 0.2)","be0755e8":"train_set.head()","db31c4e4":"test_set.head()","97edbba5":"from sklearn.model_selection import train_test_split # btw: much better name for a function that returns something\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)","3bd09683":"train_set.head()","9fe3c996":"test_set.head()","2a003e5e":"housing['median_income'].hist(bins=50)","9ba8c9dc":"housing['income_cat'] = pd.cut(housing['median_income'],\n                               bins=[0, 1.5, 3.0, 4.5, 6, np.inf],\n                               labels=[1, 2, 3, 4, 5])\n\nhousing['income_cat'].hist()","9fc0b8b4":"from sklearn.model_selection import StratifiedShuffleSplit\n\n# Split the data once (into 2 parts), 80\/20%, and set the random seed at 42.\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train_index, test_index in split.split(housing, housing['income_cat']):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]","6c191d4e":"strat_test_set['income_cat'].value_counts() \/ len(strat_test_set)","279ddc58":"for set_ in (strat_train_set, strat_test_set):\n    set_.drop('income_cat', axis=1, inplace=True)","1ec2aab9":"housing = strat_train_set.copy()","64948a82":"housing.plot(kind='scatter', x='longitude', y='latitude')","9ab7190b":"housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1)","5007aba8":"housing.plot(\n    kind='scatter',\n    x='longitude',\n    y='latitude',\n    alpha=0.4,\n    s=housing['population']\/100, # 100 is arbitrary. Adjust to get the desired scale.\n    label='population',\n    figsize=(11,10), # Adjust till we get the shape of California.\n    c='median_house_value', # Set color based on median_house_value.\n    cmap=plt.get_cmap('jet'), # Use the \"jet\" color map.\n    colorbar=True\n    )","26db37b1":"housing.corr()['median_house_value'].sort_values(ascending=False)","31ec11da":"from pandas.plotting import scatter_matrix\n\nattributes = ['median_house_value', 'median_income', 'total_rooms', 'housing_median_age']\nscatter_matrix(housing[attributes], figsize=(12, 8))","de1117db":"housing.plot(\n    kind='scatter',\n    x='median_income',\n    y='median_house_value',\n    alpha=0.1\n    )","a1ce761c":"housing['rooms_per_household'] = housing['total_rooms']\/housing['households']\nhousing['bedrooms_per_room'] = housing['total_bedrooms']\/housing['total_rooms']\nhousing['population_per_household'] = housing['population']\/housing['households'] ","e6676393":"housing.corr()['median_house_value'].sort_values(ascending=False)","3131ca80":"housing.plot(\n    kind='scatter',\n    x='bedrooms_per_room',\n    y='median_house_value',\n    alpha=0.1\n    )","f6432b49":"housing = strat_train_set.drop('median_house_value', axis=1)\nhousing_labels = strat_train_set['median_house_value'].copy()","9d29ee0c":"# 1. Get rid of the corresponding individuals.\n# housing.dropna(subset['total_bedrooms'])\n\n# 2. Get rid of the whole attribute\/column.\n# housing.drop('total_bedrooms', axis=1)\n\n# 3. Fill in the missing features with a sensible value (such as the median).\n# median = housing['total_bedrooms'].median()\n# housing['total_bedrooms'].fillna(median, inplace=True)","e6328e5a":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='median') # Same as using Pandas fillna() with the median","3f051500":"housing_num = housing.drop('ocean_proximity', axis=1)","eaca31f6":"imputer.fit(housing_num)","cb3053eb":"imputer.statistics_","e6f7c2a3":"housing_num.median().values","5ad04f0a":"X = imputer.transform(housing_num)","423ca9ff":"housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)\nhousing_tr","6da9a4b4":"housing_cat = housing[['ocean_proximity']]\nhousing_cat.head(10)","ed3d0c40":"from sklearn.preprocessing import OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\nhousing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)","8075356e":"housing_cat_encoded[:10]","d50dd54e":"ordinal_encoder.categories_","0dcc61e5":"from sklearn.preprocessing import OneHotEncoder\ncat_encoder = OneHotEncoder()\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot","3931e194":"housing_cat_1hot.toarray()","2c8cdee2":"cat_encoder.categories_","4e30c0e3":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# Indices.\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room=True): # no *args or *kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self # nothing else to do\n    def transform(self, X):\n        rooms_per_household = X[:, rooms_ix] \/ X[:, households_ix]\n        population_per_household = X[:, population_ix] \/ X[:, households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] \/ X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n        ","4e811d9e":"attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)","820cc927":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\n\n# Create the pipeline for numeric transformations on housing_num. \nnum_pipeline = Pipeline([\n    # This is a list of tuples.\n    # Each tuple is a name\/estimator pair.\n    # The name is our own unique ID that will come in handy for hyperparameter tuning.\n    ('imputer', SimpleImputer(strategy='median')),\n    ('attribs_adder', CombinedAttributesAdder()),\n    ('std_scaler', StandardScaler())\n])\n\n# Create the full pipeline for both numerical and categorical data.\nnum_attribs = list(housing_num) # returns housing_num column names\ncat_attribs = ['ocean_proximity']\nfull_pipeline = ColumnTransformer([\n    # More name\/estimator tuples.\n    ('num', num_pipeline, num_attribs), # num_pipeline is just another transformation\n    ('cat', OneHotEncoder(), cat_attribs)\n])\n\n# Save the data we'll use for training our model.\nhousing_prepared = full_pipeline.fit_transform(housing)","46301c53":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)","f8902668":"some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)","576cf37b":"lin_reg.predict(some_data_prepared)","ea2deecb":"list(some_labels)","4b093595":"from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","d8178eed":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)","0d7dea1b":"housing_predictions = tree_reg.predict(housing_prepared)\nhousing_predictions","839666a5":"tree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","18bec010":"from sklearn.model_selection import cross_val_score\n\ndef display_scores(scores):\n    print('Scores:', scores)\n    print('Mean:', scores.mean())\n    print('Standard deviation:', scores.std())","57c9ce03":"lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","bf57e5d3":"tree_scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-tree_scores)    \ndisplay_scores(tree_rmse_scores)","0430784e":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor()\nforest_reg.fit(housing_prepared, housing_labels)","02cd6f74":"housing_predictions = forest_reg.predict(housing_prepared)\nhousing_predictions","85aaa486":"forest_mse = mean_squared_error(housing_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","87de8c5d":"forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","8830b9da":"from sklearn.model_selection import GridSearchCV\n\n# Array of hyperparameter combinations.\n# They keys are hyperparameters; the arrays are the values we want to test.\n# 18 total configurations.\nparam_grid = [\n    # 3 \u00d7 4 = 12 value combinations\n    {\n        'n_estimators': [3, 10, 30],\n        'max_features': [2, 4, 6, 8]\n    },\n    # 1 \u00d7 2 \u00d7 3 = 6 value combinations\n    {\n        'bootstrap': [False],\n        'n_estimators': [3, 10],\n        'max_features': [2, 3, 4]\n    }\n]\n\nforest_reg = RandomForestRegressor()\n\n# 5-fold cross validator.\n# Will check all 18 configs 5 times.\n# This might take a while.\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n\ngrid_search.fit(housing_prepared, housing_labels)\n\n# Get the highest performing hyperparameter values.\ngrid_search.best_params_","518faa6a":"grid_search.best_estimator_","356aa82f":"cv_results = grid_search.cv_results_\nfor mean_score, params in zip(cv_results['mean_test_score'], cv_results['params']):\n    print(np.sqrt(-mean_score), params)","076bb134":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","7babaf24":"extra_attribs = ['rooms_per_hhold', 'pop_per_hhold', 'bedrooms_per_room']\ncat_encoder = full_pipeline.named_transformers_['cat']\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)","86e33137":"# Grab our best model.\nfinal_model = grid_search.best_estimator_\n\n# Get the test set and remove the value we want to predict (median_house_value).\nX_test = strat_test_set.drop('median_house_value', axis=1)\n\n# Keep a copy of the labels so we can measure performance.\ny_test = strat_test_set['median_house_value'].copy()\n\n# Clean up the data with our pipeline.\nX_test_prepared = full_pipeline.transform(X_test)\n\n# Make the predictions.\nfinal_predictions = final_model.predict(X_test_prepared)\n\n# Measure performance.\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse","0dde912b":"from scipy import stats\n\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, \n                         loc=squared_errors.mean(),\n                         scale=stats.sem(squared_errors)))","c9c7cf72":"from sklearn.externals import joblib\n\njoblib.dump(final_model, 'median-house-value.joblib')","a9aa4374":"print(full_pipeline.named_transformers_['cat'].get_feature_names())","83edb09a":"housing.describe()","da106eae":"training_stats = housing.describe().copy().iloc[1:3, :]\ntraining_stats","5ddd6d8d":"rooms_per_household = housing.iloc[:, rooms_ix] \/ housing.iloc[:, households_ix]\npopulation_per_household = housing.iloc[:, population_ix] \/ housing.iloc[:, households_ix]\nbedrooms_per_room = housing.iloc[:, bedrooms_ix] \/ housing.iloc[:, rooms_ix]\n\ntraining_stats['rooms_per_household'] = [rooms_per_household.mean(), rooms_per_household.std()]\ntraining_stats['population_per_household'] = [population_per_household.mean(), population_per_household.std()]\ntraining_stats['bedrooms_per_room'] = [bedrooms_per_room.mean(), bedrooms_per_room.std()]\ntraining_stats","f4987b31":"qa = X_test.head(10).copy()\nqa['PREDICTION'] = final_predictions[:10]\nqa","50a8ef55":"What's happened so far:\n* Noticed that some columns\/attributes have tail-heavy distributions.  \n  These might need transformations before being fed into the machine learning algorithm.\n* Found a linear correlation between median_house_value and median_income.\n* Spotted some quirks (horizontal patterns) in the median_house_value-to-median_income plot.  \n  Might need to remove some data to prevent the algorithm from learning the quirks.","5027daa8":"### Analyze the Best Models and Their Errors\n\nYou'll often gain insights on the problem by inspecting the best models. For example, the `RandomForestRegressor` can indicate the relative importance of each attribute for making accurate predictions:","108f1f32":"#### Approach 2: Mean Absolute Error (MAE)\n\nThis is the preferred approach if there are many outliers.","b4b03d27":"**With more adjustments to the plot, we can get more insights.**","dfc8a354":"### Get some values we can use to test in the app.\n\nUse these values to verify the Vue.js app works. The user should be able to input the features and get the PREDICTION back.","a94dffdf":"#### Predictions:","84a4a3a1":"#### Start by looking at the kinds of values within the categorical attributes.\nOur dataset has only one categorical attribute: ocean_proximity. To use this attribute in a machine learning algorithm, we need translate these text values to numbers.","14517e64":"#### Scikit-learn can take care of all missing features with its `SimpleImputer` class.","3987c0e4":"### Conclusion\n\nJudging by lowest mean score, Random Forest is outperforming the rest. But notice that the Random Forest RMSE is still very low compared to the cross-validation scores. This means the model is still overfitting the data. Possible solutions are to simplify the model, constrain or \"regularize\" it, or get a lot more training data. \n\nWe'll move on for now. The goal here is just to shortlist a few models for fine-tuning.","cf9394be":"ML algorithms can't work with missing features. We'll need to fix total_bedrooms to have features for all individuals. ","e89254f7":"### Visualizing Geographical Data\nSince there are geographical features (lat and long), we should take a look this on a scatter plot.","37103a47":"#### Using the `statistics_` instance variable, we can see the imputer's medians for each attribute.","caffa9c7":"**Let's see a summary of the numerical fields using the `describe()` method.**","937fe22f":"#### See all the text categories using the `categories_` instance variable.\n\n\u26a0\ufe0focean_proximity is not a value scale: \"<1 OCEAN\" isn't conceptually less than \"INLAND\" or the other values. Machine learning algorithms will assume that two nearby values are more similar than two distant values.","89454bc9":"**\u26a0\ufe0f If we run the program again or change the dataset, the indices will be different.**\n\n#### Approach 2: use SciKit Learn to split the dataset.\n\nThe train and test sets should remain the same whenever we restart the program.","ee007aed":"**Looks like California!**\n**If we make the points transparent, we can see where the houses are concentrated:**","7dd78431":"### Approach 1: Linear Regression","0b332b5e":"# Select and Train a Model","1d7f1777":"#### What is the current solution?\n\nDistrict housing prices are estimated manually by experts. It's costly and time consuming, with estimates often incorrect by 20%.","1115addf":"### Decode StandardScaler for app UI inputs.\n\nRemember that the transformation pipeline includes a StandardScaler for standardizing all the features. The feature values are transformed into standard scores using this formula:\n\n$score = \\frac{feature - \\bar{x}}{s}$\n\nThe model expects all feature inputs to be transformed, so we'll need to use this formula to transform the user's non-standardized input values. The front-end will need the means ($\\bar{x}$) and standard deviations ($s$) of every attribute in the training set.","0e4ff391":"The performance measure will gauage the accuracy of our model's predictions. Both of the following approaches express the **average difference between predicted values and actual values**, but the one we choose depends on the quality of the data.\n\n$m$ = sample size, or number of rows in $\\mathbf{X}$\n\n$\\mathbf{X}$ = the sample matrix, or training\u00a0data set\n\n$\\mathbf{x}^\\left(i\\right)$ = a row in the sample matrix, or an instance of features\n\n$h \\left( \\mathbf{x}^\\left(i\\right) \\right)$ = the predicted value or hypothesis for $\\mathbf{x}^\\left(i\\right)$\n\n$y^\\left(i\\right)$ = the actual value for $\\mathbf{x}^\\left(i\\right)$\n","3367a948":"Some manual math will show us a lot of differnece between the predictions and the actuals. We'll use our performance measure, RMSE, to take a closer look.","36f0928e":"#### Predictions:","42d49400":"**Notice also that ocean_proximity is a categorical attribute. We can see all the categories and their totals using `value_counts()`.**","fbfd7017":"### Evaluate the System on the Test Set","3d270b2c":"# CHAPTER 2: End-to-End Machine Learning Project\n\nA model for median house value in 1990 California.","5f1d16f7":"### Approach 2: Decision Tree Regressor\n\nA decision tree is a more powerful model, capable of finding nonlinear relationships.","f39f1b3d":"# Get the Data","10205574":"Start with a copy of the training data, so that we don't accidently change it.\nNote: this overrides the `housing` variable we've been using this whole time.","0199453a":"#### RMSE is a \\\\$47,572 generalization error.\n\nIn other words, predictions could be off by an average of \\\\$47,572. But this point estimate might be too exact a way of expressing potential error. We can instead share a 95% **confidence interval** that expresses the error as a range:","486cad36":"### Randomized Search\n\nGrid Search works well for a few hyperparameters, but when there are a bunch, Randomized Search is more practical. It's structured like Grid Search, but you don't have to specify the values. Randomized Search uses random values for each hyperparameter at each iteration. You can set the number of iterations to as low or high as you want, giving you control over your computing budget.","722de789":"### Data Cleaning","1fde054d":"### Select a Performance Measure","796e6f21":"### Better Evaluation Using Cross-Validation \n\n1. Split the training set into 2 parts: a smaller training set, and a validation set.\n2. Train each model with the smaller training set.\n3. Test each model against the validation set. \n4. Compare the performance of each model and find the highest performer.\n5. Train the highest performing model against the entire training set (including the validation set).\n\nWe can automate steps 1\u20134 using Scikit-Learn's *K-fold cross-validation* feature via the `cross_val_score` function. The function splits the training set into *K* distinct subsets called *folds*. Each fold is a smaller training set like the one in step 1 above. The function trains the model against each fold and returns the evaluation score for each.\n\nThis improves the accuracy of our process, but slows down training.\n\nIn the following exercise, we'll do a 10-fold cross-validation for a few different models. A custom `display_scores` function will print out the individual scores, the mean score, and the standard deviation.","56ce6407":"#### Run the transformer.","37148698":"#### We can get the best estimator directly:","016003fb":"A pipeline streamlines all our transformations:\n\n* Imputing missing features\n* Creating aggregate columns to show new correlations\n* Converting text\/categorical data to numbers\n* Scaling features\n\nWe'll use Scikit-Learn's `Pipeline` class to build this.\n\nRemember: `housing_num` was defined a ways back, as a copy of the housing data with only numerical attributes. `housing_num` dropped the categorical ocean_proximity attribute. The following approach ","ce1d7830":"Feature scaling is a transformation that normalizes value ranges between attributes\/columns. Two types:\n\n* **min-max scaling** or **normalization**: converts values to a number between 0 and 1. Seems best for neural networks and (whose cells are activated along a scale of 0\u20131) and when attributes don't have outliers.\n* **standardization** or **z-scoring**: normalizes without using a specific range of numbers. Better for attributes with outliers. (An outlier would skew the 0\u20131 scale of min-max scaling.)","f5767288":"### Feature Scaling","a08a431b":"#### We can verify that the DataFrame `median()` method returns the same values.","624b26a6":"#### Root mean squared error:","76d035f3":"#### Start a custom data frame to capture our means and standard deviations.","21d3f58d":"### Download the Data","3c9a65c1":"#### Importance scores next to attribute names:","05282735":"# Launch, Monitor, and Maintain the System\n\nThis last section of the chapter describes the concepts that come after finishing your ML model prototype. There's no code in this section, so I decided to make up my own project: connecting a Vue.js web front-end to our ML model.\n\n1. Export the model to a joblib file.\n2. Use Flask to write a Python web server that turns the model into a REST API.\n3. Use Vue.js to begin the front-end.\n4. Apply the standard scaler in Vue.js to convert user input to model input.","125dffd9":"**Notice several horizontal trends:**\n* \\\\$500,000 -the arbitrary house value cap of the dataset\n* \\\\$450,000\n* \\\\$350,000\n* \\\\$325,000\n* \\\\$280,000\n\nWe may want to remove the corresponding districts to prevent the algorithm from learning to reproduce these quirks.","f8fdd3e5":"#### Calculate the medians using the `fit()` method.","3bd95df0":"Start by reverting to a clean training set. `housing` will contain a copy of the training set, but without the labels. We'll keep the labels in `housing_labels`.","2ebf414d":"### Create a Test Set using Random Sampling\n\nAvoid *data snooping* bias. Don't over-analyze the data and start forming an algorithm hypothesis (regression, classification, etc.) before setting aside some portion for testing. This will help avoid overfitting.","4e774ea2":"### Experimenting with Attribute Combinations","7e88fadb":"#### Use `OneHotEncoder` instead of `OrdinalEncoder`\n\n`OrdinalEncoder` is good only for text values along a scale (ex: \"dislike,\" \"indifferent,\" \"like\"). For ocean_proximity, we'll want to treat each possible text value as a mutually exclusive binary attribute using a process called *one-hot encoding*. Scikit-learn's `OneHotEncoder` class converts a categorial values into one-hot vectors.","04f54a62":"### Transformation Pipelines","233f21cd":"#### The result is a NumPy array of the transformed features. Let's convert to a DataFrame.","86f0ab10":"### Take a Quick Look at the Data Structure","49782aa3":"### Ensemble Methods\n\nAnother way to fine-tune: combine the models that work the best. More on that in Chapter 7.","e466120e":"**Do the stratfied sampling using Scikit-Learn's `StratifiedShuffleSplit` class.**","8bf22ab9":"#### We're 95% confident the error will be between \\\\$45,597 and \\\\$49,468.\n\nOr we're 95% confident the error is \\\\$47,533 \u00b1\\\\$1,936","a8732064":"#### And the cross-validation scores for every one of the 18 combinations:","b3fe378e":"#### Labels (actuals):","8a1827ed":"California housing prices: red is expensive, blue is cheap. Larger circles mean higher populations.","d71650c4":"# Fine-Tune the Model\n\nNow it's about getting the hyperparameters right.","20c4b75d":"### Create a Test Set using Stratified Sampling\n\nSuppose our SME says median income is a very important attribute for predicting median housing prices. In this case, we'll want to ensure the test is representative of the different kinds of median incomes. We'll do this by creating an income category attribute based on the range of median incomes.\n\nIf we look back at the median income histogram, we'll see that most of the incomes are concentrated between 1.5\u20136 (15,000\u201360,000 USD), but some go beyond 6:","572bbaab":"#### Before using the imputer, we need to get rid of any categorical (non-numerical) attributes such as ocean_proximity.","665eb2d6":"$\n\\mathrm{RMSE} \\left( \\mathbf{X}, h \\right) = \\sqrt{ \\frac{1}{m} \\sum_{i=1}^m \\left( h \\left( \\mathbf{x}^\\left(i\\right) \\right) - y^\\left(i\\right) \\right)^2 }\n$","b9eae333":"#### Pandas' DataFrame has 3 options for dealing with missing features:","231af1ac":"#### Test on the first 5 instances from the training set.","a0347745":"#### Attributes like median_income, whether or not the district is inland, and the population per household seem to matter most.\n\nConsider dropping the less relevant attributes that might be dragging down the accuracy of the model. ","c2daaefd":"# Prepare the Data for Machine Learning Algorithms","af1bbb92":"#### Cross-validation:","54c6343d":"\u26a0\ufe0fFit the scalers to the training data only, not the test data.","c3144679":"#### Use Scikit-learn's `OrdinalEncoder` class to translate text to numbers.","61316333":"#### What's the objective?\n\nOur model's output (district median housing price) will be one of several **signals** that feed into another machine learning system. The downstream system will determine whether it's worth investing in certain areas.","b3ee29f4":"#### Root mean squared error:","3d5d3b4a":"### Frame the Problem","23b692aa":"#### Root mean squared error:","be3308e8":"### Handling Text and Categorical Attributes","7ea334ea":"#### Create a transformer class that adds the combined attributes from before: rooms_per_household, population_per_household, bedrooms_per_room.\n\nThe constructor takes a single Boolean parameter \u2013\u00a0a **hyperparameter** \u2013 that indicates whether or not to add the bedrooms_per_room attribute. This hyperparameter makes it easier for us to test whether or not bedrooms_per_room actually improves the ML algorithm. This parametric approach is a general best practice for any data prep step whose effectiveness we're not sure about.","e747c69e":"**Now consider creating some aggregate columns\/attributes that might make each data point more useful:**\n* Total number of rooms per district isn't useful if we don't know how many households there are: what we want are number of rooms per household.\n* Total number of bedrooms isn't useful by itself: we want to compare it to the number of rooms.\n* Population per household could also be an interesting aggregate attribute.","efd8235a":"**Approach 2: use Pandas' `scatter_matrix()` to see correlations visually.**\n\nThis will generate a grid of scatter plots that map each column against the other. Notice the diagonal set of charts from top left to bottom right: these plots should map a single column against itself, which would be useless. Mapping a column against itself will always create a direct linear correlation, or perfectly straight line. So the `scatter_matrix()` function shows some other useful plot instead: by default, a histogram of the column values. We can set this to another kind of plot if we want.","101a1bbe":"#### Now let's use the `transform()` method to replace the missing features with the corresponding medians.","bc8b7c9b":"\"Most of the times, your dataset will contain features highly varying in magnitudes, units and range. But since, most of the machine learning algorithms use Eucledian distance between two data points in their computations, this is a problem... **these algorithms only take in the magnitude of features neglecting the units**. The results would vary greatly between different units, 5kg and 5000gms. The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes.\"\n\n\u2014[Why, How and When to Scale your Features](https:\/\/medium.com\/greyatom\/why-how-and-when-to-scale-your-features-4b30ab09db5e)","94d24765":"### Export the model using joblib. \n\nThe joblib file is a portable version of the model. The Flask server will load the joblib file and make it accessible through an API.","b2c3857a":"The data cleaning process is about **transforming** the data in preparation for learning. Scikit-learn has a bunch of transformers built in, like the `OrdinalEncoder` and `OneHotEncoder` we used earlier. But for more customized cleaning operations, we'll need to make our own transformers. To make a transformer, we just need to create a class that implements 3 methods: `fit`, `transform`, and `fit_transform`. That last methods comes for free when we add the `TransformerMixin` to our class.","05ecace2":"**Notice that total_bedrooms has only 20433 non-null values. We'll need to take care of this later.**","15366b98":"**\u26a0\ufe0f Both these approaches use *random sampling*. Remember: to avoid *sampling bias* in this method, every individual needs an equal chance of being selected at random. If the sample is relatively small, we should consider *stratified sampling* to get a more accurate representation of the population.**","a77d66b2":"# Discover and Visualize the Data to Gain Insights","90c62ef7":"#### Approach 1: Root Mean Square Error (RMSE)\n\nThis is the typical approach for a regression problem, and the one we'll use in this project. Best for bell-shaped curves with no outliers.","a8b7b20c":"#### We've fine-tuned the model! \ud83c\udf89\n\nThe lowest error is \\\\$49,792 using max_features = 6 and n_estimators = 30, which is lower than what we got with the default hyperparameters in the last section. ","48d2c9ca":"It's important to have enough individuals in the dataset for each stratum, otherwise we might bias toward the larger strata. For stratified sampling to work, don't have too many strata, and make sure there are enough individuals in each stratum.\n\n**Add an 'income_cat' column to the dataset, then use the Pandas `cut()` function to create the strata.**","61a598d5":"#### Get the means and standard deviations of the combined attributes.\n\nAdd these as columns to our new training_stats data frame.","6191301f":"$\n\\mathrm{MAE} \\left( \\mathbf{X}, h \\right) = \\frac{1}{m} \\sum_{i=1}^m \\left| h \\left( \\mathbf{x}^\\left(i\\right) \\right) - y^\\left(i\\right) \\right|\n$","e8b74e9c":"\u26a0\ufe0f Since 30 is the max estimator value tested, we should probably try searching again with higher values. The score may continue to improve.","8a7b80f2":"### Grid Search\n\nWe could manually tune hyperparameters, but it's faster to use Scikit-Learn's `GridSearchCV`. The following code searches for the best combination of hyperparameters for the `RandomForestGenerator`.","5ca0050d":"Here are a couple things to ask in a project kickoff.","acbd07e2":"### Get the names of the one-hot encoder attributes.\n\nThe front-end Vue.js app will submit a POST request to the Flask server. The request body will contain key-value pairs for a set of features. The keys must correspond to the attribute names. We can get most of the attribute names by looking at the training data frame, but remember that the transformation pipeline added new attributes: the combined attributes that we manually named, and the hidden attributes generated by the one-hot encoder. The following code reveals the one-hot encoder attributes.","01a5c8a1":"#### What ML approach might we take?\n- We're starting from labeled data, so we'll want a **supervised** learning algorithm.\n- We'll use multiple features to predict a value, making this a **multiple regression** problem.\n- It's a **univariate regression** because we're predicting only one value.","bf8d3bec":"### Test 3: Random Forest\n\nWe need to set up the model for this one first.","50831b2d":"#### Approach 1: select 20% of the dataset indices at random for testing.","be2f1451":"According to the RMSE, predictions could be off by an average of \\\\$68,628. That's not great if most districts' median_housing_values range between \\\\$120,000 and \\\\$265,000.\n\nThis is an example of **underfitting** the data. Possible causes:\n1. The features don't provide enough info to make good predictions.\n2. The model (linear regression in this case) isn't powerful enough.\n3. Reduce constraints on the model. \n\nCan't do #3 because the model isn't **regularized**. #1 is a possibility, but we'll try #2 for the sake of time.\n\nLet's try some more complex models...","3cd9c2ec":"**Approach 1: use `corr()` method to get correlation coefficient and spot linear correlations with median_house_value**\n\nThe coefficient ranges from -1 to 1:\n* -1 = indirect relationship; negative linear correlation.\n*  0 = no relationship\n*  1 = direct relationship; positive linear correlation.\n\nNotice that:\n* median_house_value is directly correlated with itself (1.000000)\n* there's a strong correlation with median_income (0.687160)\n* there's a slight indirect correlation with latitude (median_house_value is slightly lower further north)","590b4359":"#### See all the text categories using the `categories_` instance variable.\n\nJust like we did with `OrdinalEncoder`, we can use the `categories_` instance variable to get the categories.","f3eaf152":"#### Get the means and standard deviations of the original attributes.\n\nGet these from the *training* set (`housing`) not the test set.","426bf221":"#### See all the one-hot vectors.\n\nThe encoder's `fit_transform()` method returns a sparse matrix instead of a NumPy array. This is a more memory-efficient format that stores only the locations of all the 1s, and leaves the 0s out. To see the all the 1s and 0s, convert to an array with `toarray()`, and notice that each one-hot vector has a single 1 that corresponds to the categorical attribute.","b12f2338":"#### Predictions:","04b4228e":"**Create the new attributes.**","a93ed8b4":"#### See how the text gets translated to numbers.\n\n\"<1H OCEAN\" becomes 0, \"NEAR OCEAN\" becomes 4, \"INLAND\" becomes 1, etc.","53df1048":"### Custom Transformers","dcd3db20":"**Another quick way to get a feel for the data is to plot a histogram for each numerical attribute.** ","290dc04d":"RMSE = 0 means there's no error, but that's dubious. The model probably **overfit** the data.\n\nThe best thing to do in evaluating different models is to use **cross-validation**.","6f9c3d24":"**It's working! Notice that categories 2 and 3 have a higher percentage of the individuals.**\n\n**Now that we have our split, remove the income_cat column from the dataset using Pandas' `drop()` function.**","03e4f7f0":"#### \u26a0\ufe0fDon't use OneHotEncoder if there are too many categories.\n\nThis results in a ton of features and slower training time. Consider replacing the categorical attribute with more specific numeric attributes, for example, changing ocean_proximity to a an actual distance to the ocean. We can also replace each category with a low-dimensional vector called an *embedding*.","593f9352":"### Test 2: Decision Tree","aabfe67a":"# Look at the Big Picture","2e34c64d":"### Test 1: Linear Regression","29c21cd9":"### Looking for Correlations\n\nAfter looking at geospatial data, we'll want to see if there are any correlations between the columns in the dataset. We'll take two approaches here: using a correlation coefficient, and using scatterplots. ","04c5c3c3":"We're tasked with using California census data to build a model of housing prices in the state. Each census record represents a district of 600 to 3,000 people. Our model should learn from this data to predict the median housing price in any district.","4be6ff34":"**Revisit the correlation coefficients.**\n\nNotice that the new bedrooms_per_room attribute has a slight indirect correlation with median_house_value. As the bedroom-to-room ratio goes down, the median house value goes up slightly. Maybe this is because a low bedroom-to-room ratio signals a larger property, like a single family home, versus something that is \"mostly bedroom\" like a condo or studio.","521169f8":"**Based on the correlation coefficient and the scatter matrix, we can see that median_income is a promising attribute for predicting median_house_value. Let's take a closer look at median_income.**","1c7e4bb5":"**Verify the distribution using `value_counts()`. Divide by the number of individuals in `strat_test_set` to see the ratio.**"}}