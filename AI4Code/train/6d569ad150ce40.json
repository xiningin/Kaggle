{"cell_type":{"12ee212b":"code","1ddbe253":"code","bb0ec540":"code","f88d6d90":"code","b310535b":"code","e06ebeb4":"code","114df654":"code","2f009ffb":"code","daf74989":"code","ecae318d":"code","437e9bbd":"code","1139c6db":"code","21fe4804":"code","b38b2e7b":"code","865345e3":"code","4e258391":"code","ed955b05":"code","c3a3d31f":"markdown","13050067":"markdown","7d23435d":"markdown","c13c5483":"markdown","a60b93f2":"markdown","7f5192ff":"markdown","e4eb986c":"markdown","eda6f4b0":"markdown","b7901755":"markdown","6ae6b7a5":"markdown","49666b09":"markdown","0c81e6d2":"markdown","603e3494":"markdown","4d9c484c":"markdown","fe1777db":"markdown","c76ae8d4":"markdown"},"source":{"12ee212b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1ddbe253":"#standard imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_digits","bb0ec540":"data_sets = load_digits()\ndata_sets","f88d6d90":"X,y = data_sets.data,data_sets.target\nfor class_name,class_count in zip(data_sets.target_names, np.bincount(data_sets.target)):\n    print(class_name,class_count)","b310535b":"y_binary_imbalanced = y.copy()\ny_binary_imbalanced[y_binary_imbalanced != 1] = 0\n\nprint('Original labels:\\t', y[1:30])\nprint('New binary labels:\\t', y_binary_imbalanced[1:30])","e06ebeb4":"np.bincount(y_binary_imbalanced)","114df654":"from sklearn.svm import SVC\nX_train,X_test,y_train,y_test = train_test_split(X,\n                                                 y_binary_imbalanced,\n                                                 random_state=0)\nsvm = SVC(kernel='rbf',C=1)\nsvm.fit(X_train,y_train)\nsvm.score(X_test,y_test)","2f009ffb":"from sklearn.dummy import DummyClassifier\n# since negative class 0 is most frequent.\ndummy = DummyClassifier(strategy='most_frequent')\ndummy.fit(X_train,y_train)\ndummy.score(X_test,y_test)","daf74989":"dummy_prediction = dummy.predict(X_test)\ndummy_prediction","ecae318d":"svm = SVC(kernel='linear',C=1)\nsvm.fit(X_train,y_train)\nsvm.score(X_test,y_test)","437e9bbd":"from sklearn.metrics import confusion_matrix,plot_confusion_matrix\n#negative class 0 is most frequent\ndummy = DummyClassifier(strategy='most_frequent')\ndummy.fit(X_train,y_train)\ndummy_predicted = dummy.predict(X_test)\nconfusion = confusion_matrix(y_test,dummy_predicted)\nplot_confusion_matrix(dummy,X_test,y_test,cmap=plt.cm.Blues)\n\nprint('Most frequent class (dummy classifier):\\n',confusion)\n","1139c6db":"# produces random predictions w\/ same class proportion as training set\ndummy = DummyClassifier(strategy='stratified')\ndummy.fit(X_train,y_train)\ndummy_predicted = dummy.predict(X_test)\nconfusion = confusion_matrix(y_test,dummy_predicted)\nplot_confusion_matrix(dummy,X_test,y_test,cmap=plt.cm.Blues)\n\nprint('Random class proportional Prediction (dummy classifier):\\n',confusion)","21fe4804":"svm = SVC(kernel='linear',C=1)\nsvm.fit(X_train,y_train)\nsvm_predict = svm.predict(X_test)\nconfusion = confusion_matrix(y_test,svm_predict)\nplot_confusion_matrix(svm,X_test,y_test)\n\nprint('Support vector machine classifier:\\n',confusion)","b38b2e7b":"from sklearn.linear_model import LogisticRegression\nlg = LogisticRegression()\nlg.fit(X_train,y_train)\nlg_pred = lg.predict(X_test)\nconfusion = confusion_matrix(y_test,lg_pred)\nplot_confusion_matrix(lg,X_test,y_test,cmap=plt.cm.Blues)\nprint('Logistic Regression Classifier:\\n',confusion)","865345e3":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(max_depth=2)\ndt.fit(X_train,y_train)\ndt_pred = dt.predict(X_test)\nconfusion = confusion_matrix(y_test,dt_pred)\nplot_confusion_matrix(dt,X_test,y_test)\n\nprint('Decision Tree Classifier:\\n',confusion)","4e258391":"from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n# Accuracy = TP + TN \/ (TP + TN + FP + FN)\n# Precision = TP \/ (TP + FP)\n# Recall = TP \/ (TP + FN)  Also known as sensitivity, or True Positive Rate\n# F1 = 2 * Precision * Recall \/ (Precision + Recall) \n\nprint('Accuracy: {:.2f}'.format(accuracy_score(y_test,dt_pred)))\nprint('Precision: {:.2f}'.format(precision_score(y_test,dt_pred)))\nprint('recall: {:.2f}'.format(recall_score(y_test,dt_pred)))\nprint('f1: {:.2f}'.format(f1_score(y_test,dt_pred)))","ed955b05":"from sklearn.metrics import classification_report\nprint('Random class proportional dummy\\n:',\n     classification_report(y_test,dummy_predicted,target_names=['Not 1','1']))\nprint('Svm\\n:',\n     classification_report(y_test,svm_predict,target_names=['Not 1','1']))\nprint('Logistic Regression\\n:',\n     classification_report(y_test,lg_pred,target_names=['Not 1','1']))\nprint('Decision Tree\\n:',\n     classification_report(y_test,dt_pred, target_names=['Not 1','1']))","c3a3d31f":"## Combined report of all the above metrices","13050067":"### Decision Tree","7d23435d":"**Using the digits data set from scikit learn library for evaluation.**","c13c5483":"# Confusion Matrix\n\n<img src='https:\/\/miro.medium.com\/max\/924\/1*7EYylA6XlXSGBCF77j_rOA.png'>\n\n**True Positive:**\n\nInterpretation: You predicted positive and it\u2019s true.\nYou predicted that a woman is pregnant and she actually is.\n\n\n**True Negative:**\n\nInterpretation: You predicted negative and it\u2019s true.\nYou predicted that a man is not pregnant and he actually is not.\n\n\n**False Positive: (Type 1 Error)**\n\nInterpretation: You predicted positive and it\u2019s false.\nYou predicted that a man is pregnant but he actually is not.\n\n\n**False Negative: (Type 2 Error)**\n\nInterpretation: You predicted negative and it\u2019s false.\nYou predicted that a woman is not pregnant but she actually is.\n\nsource=https:\/\/towardsdatascience.com\/understanding-confusion-matrix-a9ad42dcfd62\n","a60b93f2":"# Dummy Classifier","7f5192ff":"### Logistic Regression","e4eb986c":"The accracy of support vector classifier comes out to be nearly 100%.","eda6f4b0":"A dummy classifier is a type of classifier which does not generate any insight about the data and classifies the given data using only simple rules. The classifier\u2019s behavior is completely independent of the training data as the trends in the training data are completely ignored and instead uses one of the strategies to predict the class label.\nIt is used only as a simple baseline for the other classifiers i.e. any other classifier is expected to perform better on the given dataset. It is especially useful for datasets where are sure of a class imbalance. It is based on the philosophy that any analytic approach for a classification problem should be better than a random guessing approach.\n\nBelow are a few strategies used by the dummy classifier to predict a class label \u2013\n\nMost Frequent: The classifier always predicts the most frequent class label in the training data.\nStratified: It generates predictions by respecting the class distribution of the training data. It is different from the \u201cmost frequent\u201d strategy as it instead associates a probability with each data point of being the most frequent class label.\nUniform: It generates predictions uniformly at random.\nConstant: The classifier always predicts a constant label and is primarily used when classifying non-majority class labels. \n\nsource:https:\/\/www.geeksforgeeks.org\/ml-dummy-classifiers-using-sklearn\/","b7901755":"Previously we have used the kernel for support vector classifier as rbf. Now, lets change the kernel to linear to predict the output.","6ae6b7a5":"Displaying the classification report for the different algorithms.","49666b09":"### From the above output we can see that the negative class 0 is the most frequent class. lets find its size using the numpy bincount method","0c81e6d2":"### Using the support vector machine","603e3494":"## Binary Confusion Matrix","4d9c484c":"## Creating the datasets with imbalance binary classes.\n* Positive class 1 is (digit 1)\n* Negative class 1 is (non digit 1)","fe1777db":"# Evaluation for classification\n\nClassification is a supervised learning approach in which a target variable is discrete (or categorical). Evaluating a machine learning model is as important as building it. We are creating models to perform on new, previously unseen data. Hence, a thorough and versatile evaluation is required to create a robust model. When it comes to classification models, evaluation process gets somewhat tricky.\n\n## Classification Accuracy\nClassification accuracy shows how many of the predictions are correct.\n\n<img src='https:\/\/miro.medium.com\/max\/1314\/1*Pyg6Rbz96GYbKQr3OwCKww.png'>\n\nIn some cases, it represents how good a model is but there are some cases in which accuracy is simply not enough. For example, 93% means that we correctly predicted 93 out of 100 samples. It seems acceptable without knowing the details of the task.\nAssume we are creating a model to perform binary classification on a dataset with an unbalanced class distribution. 93% of data points are in class A and 7% in class B.\n\n<img src='https:\/\/miro.medium.com\/max\/918\/1*wf_9QJy62fGzglrLfLlRlw.png'>\n\nWe have a model that only predicts class A. It is hard to even call it a \u201cmodel\u201d because it predicts class A without any calculation. However, since 93% of the samples are in class A, the accuracy of our model is 93%.\n","c76ae8d4":"# Evaluate Matrics for Binary Classification\n\n<img src = 'https:\/\/secureservercdn.net\/198.71.233.197\/l87.de8.myftpupload.com\/wp-content\/uploads\/2016\/09\/diagram-768x494.png'>\nFig. Evaluation results for classification model\n\nLet\u2019s dig deep into all the parameters shown in the figure above.\n\nThe first thing you will see here is ROC curve and we can determine whether our ROC curve is good or not by looking at AUC (Area Under the Curve) and other parameters which are also called as Confusion Metrics. A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. All the measures except AUC can be calculated by using left most four parameters. So, let\u2019s talk about those four parameters first.\n\n<img src='https:\/\/secureservercdn.net\/198.71.233.197\/l87.de8.myftpupload.com\/wp-content\/uploads\/2016\/09\/table-blog.png'>\n\nTrue positive and true negatives are the observations that are correctly predicted and therefore shown in green. We want to minimize false positives and false negatives so they are shown in red color. These terms are a bit confusing. So let\u2019s take each term one by one and understand it fully.\n\nTrue Positives (TP) - These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes. E.g. if actual class value indicates that this passenger survived and predicted class tells you the same thing.\n\nTrue Negatives (TN) - These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no. E.g. if actual class says this passenger did not survive and predicted class tells you the same thing.\n\nFalse positives and false negatives, these values occur when your actual class contradicts with the predicted class.\n\nFalse Positives (FP) \u2013 When actual class is no and predicted class is yes. E.g. if actual class says this passenger did not survive but predicted class tells you that this passenger will survive.\n\nFalse Negatives (FN) \u2013 When actual class is yes but predicted class in no. E.g. if actual class value indicates that this passenger survived and predicted class tells you that passenger will die.\n\nOnce you understand these four parameters then we can calculate Accuracy, Precision, Recall and F1 score.\n\nAccuracy - Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations. One may think that, if we have high accuracy then our model is best. Yes, accuracy is a great measure but only when you have symmetric datasets where values of false positive and false negatives are almost same. Therefore, you have to look at other parameters to evaluate the performance of your model. For our model, we have got 0.803 which means our model is approx. 80% accurate.\n\nAccuracy = TP+TN\/TP+FP+FN+TN\n\nPrecision - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The question that this metric answer is of all passengers that labeled as survived, how many actually survived? High precision relates to the low false positive rate. We have got 0.788 precision which is pretty good.\n\nPrecision = TP\/TP+FP\n\nRecall (Sensitivity) - Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. The question recall answers is: Of all the passengers that truly survived, how many did we label? We have got recall of 0.631 which is good for this model as it\u2019s above 0.5.\n\nRecall = TP\/TP+FN\n\nF1 score - F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it\u2019s better to look at both Precision and Recall. In our case, F1 score is 0.701.\n\nF1 Score = 2*(Recall * Precision) \/ (Recall + Precision)\n\nsource = https:\/\/blog.exsilio.com\/all\/accuracy-precision-recall-f1-score-interpretation-of-performance-measures\/#:~:text=Precision%20%2D%20Precision%20is%20the%20ratio,the%20total%20predicted%20positive%20observations.&text=F1%20score%20%2D%20F1%20Score%20is,and%20false%20negatives%20into%20account."}}