{"cell_type":{"43cb0bd6":"code","961beef3":"code","cc597e1d":"code","ab581a60":"code","ddd21d12":"code","31723aeb":"code","b5ac7f75":"code","06798d7f":"code","67fce12c":"code","b2b4be2c":"code","cedb666a":"code","45769b63":"code","a79e5e8a":"code","c6c3eaa0":"code","f8a23f90":"code","aa32f15f":"code","b7ae799c":"code","cd1cab3b":"code","13ab1b2e":"code","3d9d4399":"code","62bf3eca":"code","97fbb1c9":"code","a0082bed":"code","80a492d6":"code","6b8ee26c":"code","af2d8aa6":"code","d14e75b1":"code","1c64d6df":"code","95d33b45":"code","3ec1a95c":"code","58669e23":"markdown","e0f77434":"markdown","b4103e27":"markdown","30d70b91":"markdown","2f637f3e":"markdown","977a1974":"markdown","812412a8":"markdown","0d85b551":"markdown","e94d5f00":"markdown","0204156e":"markdown","3c34cbb2":"markdown","8745b152":"markdown","f482cb77":"markdown","1e82d59e":"markdown","85de374d":"markdown","9734a012":"markdown","4ce215ec":"markdown"},"source":{"43cb0bd6":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgbm\nfrom sklearn.metrics import mean_squared_error","961beef3":"train = pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\ntest = pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')","cc597e1d":"train.head(20)","ab581a60":"train_corr = train.corr()\nplt.figure(figsize = (13,13))\nsns.heatmap(train_corr, vmax=1, vmin=-1, center=0,annot=True)","ddd21d12":"train[\"Pawpularity\"].plot.hist(bins=50)","31723aeb":"def training_exe (train):\n    kf = KFold(n_splits = 3)\n    models = []\n    rmses =[]\n    categories = [\"Id\"]\n    \n    train[\"Id\"] = train[\"Id\"].astype('category')\n    X_train = train.drop(['Pawpularity'], axis=1)\n    Y_train = train['Pawpularity']\n    \n    lgbm_params = {\n        \"objective\":\"regression\",\n        \"random_seed\":1234\n    }\n    \n    for train_index, val_index in kf.split(X_train):\n        XX_train = X_train.iloc[train_index]\n        XX_valid = X_train.iloc[val_index]\n        \n        YY_train = Y_train.iloc[train_index]\n        YY_valid = Y_train.iloc[val_index]\n        \n        lgbm_train = lgbm.Dataset(XX_train, YY_train, categorical_feature = categories)\n        lgbm_eval = lgbm.Dataset(XX_valid, YY_valid, categorical_feature = categories, reference=lgbm_train)\n        \n        model_lgbm = lgbm.train(lgbm_params,\n                           lgbm_train,\n                           valid_sets = lgbm_eval,\n                           num_boost_round = 200,\n                           early_stopping_rounds =20,\n                           verbose_eval = 10,\n                           )\n        y_pred = model_lgbm.predict(XX_valid, num_iteration = model_lgbm.best_iteration)\n        \n        tmp_rmse = np.sqrt(mean_squared_error(YY_valid, y_pred))\n        print (tmp_rmse)\n        models.append(model_lgbm)\n        rmses.append(tmp_rmse)\n        \n    ave_rmse = sum(rmses)\/len(rmses)    \n    \n    return models, ave_rmse","b5ac7f75":"def pred_exe (test):\n    preds=[]\n    test[\"Id\"] = test[\"Id\"].astype('category')\n\n    for model in models:\n        pred =model.predict(test)\n        preds.append(pred)\n    \n    preds_array = np.array(preds)\n    preds_mean = np.mean(preds_array, axis=0)\n    \n    return preds_mean","06798d7f":"models, ave_rmse = training_exe (train)\nave_rmse","67fce12c":"test.head()","b2b4be2c":"pred = pred_exe(test)\n\nsub = pd.DataFrame()\nsub['Id']=test['Id']\n#sub['Pawpularity'] = pred\n\n#sub.to_csv('submission.csv',index=False)\nsub.head()","cedb666a":"def add_feature(df) :\n    df[\"Attractive\"] = df[\"Eyes\"] + df[\"Face\"] + df[\"Near\"] + df[\"Subject Focus\"]\n    df[\"Humantic\"]   = df[\"Human\"] + df[\"Collage\"]\n    df[\"Addition\"]   = df[\"Accessory\"] + df[\"Info\"]\n    return df  ","45769b63":"train_df1 = train.copy()\nadd_feature(train_df1)","a79e5e8a":"models, ave_rmse = training_exe (train_df1)\nave_rmse","c6c3eaa0":"test_df1 = test.copy()\n\nadd_feature(test)","f8a23f90":"pred = pred_exe(test)\n\n#sub = pd.DataFrame()\n#sub['Id']=test['Id']\nsub['Pawpularity'] = pred\n#sub.to_csv('submission.csv',index=False)\nsub.head()","aa32f15f":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgbm\nfrom sklearn.metrics import mean_squared_error\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport tensorflow as tf\nimport cv2","b7ae799c":"import missingno as msno\n\ntrain=pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\ntest=pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')","cd1cab3b":"train[\"file_path\"] = train[\"Id\"].apply(lambda x: \"..\/input\/petfinder-pawpularity-score\/train\/\" + x + \".jpg\")\ntest[\"file_path\"] = test[\"Id\"].apply(lambda x: \"..\/input\/petfinder-pawpularity-score\/test\/\" + x + \".jpg\")","13ab1b2e":"%%time\n\ndef preprocess(image_url):\n    image_string = tf.io.read_file(image_url)\n    image = tf.image.decode_jpeg(image_string, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    image = tf.image.central_crop(image, 1.0)\n    image = tf.image.resize(image, (128, 128))\n    return image\nx_train=[]\nfor i in train['file_path']:\n    x1=preprocess(i)\n    x_train.append(x1)","3d9d4399":"plt.figure(figsize=(20, 20))\nrow, col = 5, 4\nfor i in range(row * col):\n    plt.subplot(row, col, i+1)\n    image = cv2.imread(train.loc[i, 'file_path'])\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    target = train.loc[i, 'Pawpularity']\n    plt.imshow(image)\n    plt.title(f\"No: {i}\" f\"   Pawpularity: {target}\")\nplt.show()","62bf3eca":"test1=[]\nfor i in test['file_path']:\n    x1=preprocess(i)\n    test1.append(x1)\ntest1=np.array(test1)","97fbb1c9":"x_train=np.array(x_train)\ny_train=train['Pawpularity']\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x_train,y_train,test_size=0.2)","a0082bed":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\nfrom tensorflow.keras.layers import GlobalMaxPooling2D\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import regularizers","80a492d6":"model = Sequential()\ninput_shape = (128, 128, 3)","6b8ee26c":"\n# Layer1 : CNN1\nmodel.add(\n    Conv2D(\n        filters=32,\n        kernel_size = (3,3),\n        strides =2,\n        padding = 'same',\n        activation = 'relu',\n        input_shape = input_shape,\n    ))\n\n# Layer2 : CNN2\nmodel.add(\n    Conv2D(\n        filters=64,\n        kernel_size = (3,3),\n        strides =2,\n        padding = 'same',\n        activation = 'relu',        \n    ))\n\n# Layer3 : CNN3\nmodel.add(\n    Conv2D(\n        filters=32,\n        kernel_size = (3,3),\n        strides =2,\n        padding = 'same',\n        activation = 'relu',        \n    ))\n\n# Flatten\nmodel.add(Flatten())\n\n# Layer4\nmodel.add(\n    Dense(128, activation = 'relu'))\nmodel.add(Dropout(0.25))\n\n# Output\nmodel.add(Dense(1))\n\n# Compile\nmodel.compile(\n    loss = tf.keras.losses.MeanSquaredError(),    \n    metrics=[tf.keras.metrics.RootMeanSquaredError(name=\"rmse\"), \"mae\", \"mape\"],\n    optimizer = tf.keras.optimizers.Adam(1e-3)\n)\n","af2d8aa6":"total_train = x_train.shape[0]\ntotal_validate = y_train.shape[0]\nbatch_size = 64","d14e75b1":"\n%%time\n\nimport math\nfrom tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, Callback\n\ndef step_decay(epoch):\n    initial_lrate = 0.001\n    drop = 0.5\n    epochs_drop = 10.0\n    lrate = initial_lrate * math.pow(\n        drop,\n        math.floor((epoch)\/epochs_drop)\n    )\n    return lrate\n\nlrate = LearningRateScheduler(step_decay)\n\nearstop = EarlyStopping(\n    monitor = 'val_loss',\n    min_delta = 0,\n    patience = 5)\n\nhistory = model.fit(\n    x_train,\n    y_train,\n    epochs = 20,\n    batch_size=64,\n    validation_data = (x_test,y_test),\n    verbose = 1,\n    callbacks = [lrate, earstop]\n)\n","1c64d6df":"cnn_pred=model.predict(test1)","95d33b45":"#sub=pd.DataFrame()\n#sub['Id']=test['Id']\nsub['Pawpularity']=cnn_pred\n#sub.to_csv('submission.csv',index=False)\nsub","3ec1a95c":"sub['Pawpularity'] = pred*0.5 + cnn_pred*0.5\nsub.to_csv('submission.csv',index=False)\nsub","58669e23":"# (3) Normal TensorFlow CNN using Photo-image data\n\n","e0f77434":"After submitting, Public score shows **20.73773**... Much worse than simple LGBM using Meta-data only... Actual Leaderboard will evaluate the prediction using Photo-image data... Let's try to improve.\n\nSubmit\u3057\u3066\u307f\u308b\u3068\u3001Public Score\u306f**20.73773**\u3002Meta-data\u3092\u4f7f\u3063\u305f\u3060\u3051\u306eSimple\u306aLGBM\u3088\u308a\u60aa\u3044\u3002\u3002\u3002\u5b9f\u969b\u306f\u3053\u3061\u3089\u306e\u5199\u771f\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u305f\u4e88\u6e2c\u304c\u8a55\u4fa1\u3055\u308c\u308b\u306e\u3067\u3001\u3053\u308c\u304b\u3089\u6539\u5584\u3092\u56f3\u3063\u3066\u3044\u3053\u3046\u3002\u3002","b4103e27":"Next, let's try to predict using CNN for actual Photo-image data.\n\n\u3067\u306f\u3001\u5b9f\u969b\u306e\u5199\u771f\u30c7\u30fc\u30bf\u3092CNN\u3067\u753b\u50cf\u8a8d\u8b58\u3055\u305b\u3066\u3001\u4e88\u6e2c\u3057\u3066\u307f\u3088\u3046\u3002","30d70b91":"After predicting using test data, tried to submit. The result of Public Score is **20.49737** \n\n\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u30e2\u30c7\u30eb\u306b\u304b\u3051\u3066\u30b5\u30d6\u30df\u30c3\u30b7\u30e7\u30f3\u3057\u3066\u307f\u308b\u3002Public Score\u306e\u7d50\u679c\u306f\u3001**20.49737**","2f637f3e":"# (1) Normal LightGBM\n\nOnce, let's throw the training data as it is without any preprocessing.\n(Dividing in 3portion, and take the average)\n\n\u3044\u3063\u305f\u3093\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u306f\u4f55\u3082\u51e6\u7406\u305b\u305a\u306b\u3001\u305d\u306e\u307e\u307eLightGBM\u306b\u304b\u3051\u3066\u307f\u308b\u3002\n\uff083\u5206\u5272\u3057\u3066\u305d\u306e\u5e73\u5747\u3092\u3068\u308b\uff09","977a1974":"Hmm, do not find any columns specifically corresponding to Pawpularity... \n\n\u3046\uff5e\u3093\u3001Pawpularity\u3068\u76f8\u95a2\u95a2\u4fc2\u306b\u3042\u308bField\u7279\u306b\u898b\u5f53\u305f\u3089\u306a\u3044\u3051\u3069\u306a\u30fc","812412a8":"Any specific columns which is deeply related to \"Pawpularity\"??\nLet's see Heatmap.\n\n\u4f55\u304b\u7279\u306bPawpularity\u3068\u76f8\u95a2\u306e\u6df1\u305d\u3046\u306aColumn\u306f\u3042\u308b\u306e\u304b\u306a\uff1f\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3092\u898b\u3066\u307f\u3088\u3046\u3002","0d85b551":"I can see a feature in Pawpularity histogram. Basically, near to normal distribution, but Perfect score(100) seems extreamly many.\n\n\u30bf\u30fc\u30b2\u30c3\u30c8\u3068\u306a\u308bPawpularity\u306f\u3001\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u306b\u3061\u3087\u3063\u3068\u7279\u5fb4\u3042\u308b\u306a\u3002\u6b63\u898f\u5206\u5e03\u306b\u8fd1\u3044\u5f62\u3092\u3057\u3066\u3044\u308b\u3082\u306e\u306e\u3001100\uff08\u6e80\u70b9\uff09\u304c\u7570\u5e38\u306b\u591a\u3044\u3002","e94d5f00":"One more try to change CNN (Filter 8 > 16 > Pooling > 32 > 64)  \nHowever, the result of Public score is **20.83332**, which is further worse... Umm.. it may not be good to just change the CNN parameters...\n\n\u3055\u3089\u306b\u3001\u5909\u3048\u3066\u307f\u308b\uff08Filter 8 > 16 > Pooling > 32 > 64\uff09\u3002  \n\u304c\u3001\u7d50\u679c\u306f**20.83332**\u3068\u3055\u3089\u306b\u60aa\u5316\u3002\u3046\uff5e\u3093\u3001CNN\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u5909\u3048\u308b\u3060\u3051\u3060\u3068\u30c0\u30e1\u306a\u306e\u304b\u306a\u3002\u3002\u3002","0204156e":"At this point, the RMSE shows 20.713.... Let's try test data using this model.\n\n\u3068\u308a\u3042\u3048\u305a\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u7d50\u679c\u306f20.5825\u30fb\u30fb\u30fb\u3002\u3053\u308c\u3067\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u304b\u3051\u3066\u307f\u307e\u3059\u304b\u3002","3c34cbb2":"It takes a bit time to read Photo-image data..\nLet's see some actual photos \n\n\u5199\u771f\u30a4\u30e1\u30fc\u30b8\u306e\u8aad\u307f\u8fbc\u307f\u306f\u5c11\u3057\u6642\u9593\u304b\u304b\u308b\u306a\u3041\u30fc\u3000\u5b9f\u969b\u306e\u5199\u771f\u3092\u5c11\u3057\u898b\u3066\u307f\u308b\u3002","8745b152":"# (2) Adding new feature\n\nBased on the guess that LGBM cannot predict very well since the original table is too simple, try to add new features using the existing fealds which seem corresponding to each other.\nThe result is, slightly improved to **20.49137**.\n\n\u4eca\u306eTraining\u30c6\u30fc\u30d6\u30eb\u306f\u30b7\u30f3\u30d7\u30eb\u3059\u304e\u3066\u3046\u307e\u304f\u4e88\u6e2c\u3067\u304d\u306a\u3044\u3068\u601d\u3046\u306e\u3067\u3001\u95a2\u4fc2\u306e\u3042\u308a\u305d\u3046\u306a\u30d5\u30a3\u30fc\u30eb\u30c9\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u3042\u3089\u305f\u306a\u7279\u5fb4\u91cf\u3092\u4f5c\u3063\u3066\u307f\u308b\u3002\n\u7d50\u679c\u306f\u3001**20.49137**\u3078\u3068\u308f\u305a\u304b\u306b\u6539\u5584\u3002","f482cb77":"# (4) TF CNN (change Filters and add Pooling layer)\n\nAs an attempt, changed filters and added Pooling layers.  \n(Filter 32 > 32 > Pooling > 64 > 64 > Pooling)  \nThe score shows **20.74033**. Slightly got worse or almost unchanged..\n\n\u8a66\u3057\u306b\u3001Filter\u6570\u3092\u5909\u3048\u3066\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\u3092\u8ffd\u52a0\u3057\u3066\u307f\u308b\u3002  \n(Filter 32 > 32 > Pooling > 64 > 64 > Pooling)  \n\u304c\u3001\u30b9\u30b3\u30a2\u306f**20.74033**\u3068\u3001\u82e5\u5e72\u60aa\u5316\u3068\u3044\u3046\u304b\u307b\u307c\u5909\u308f\u3089\u305a\u3002\u3002\n","1e82d59e":"Hmmm...Interesting. I guess Pawpularity means \"Popularity\", but why No.11 is pawpularity=2, which is almost the worst, while No.19 is pawpularity =100, which is perfect score??  I understand No.15 is pawpularity=98 (Cute!), but cannot understand why No.19 is perfect score... OK, let's ignore and go ahead.\n\n\u3075\u3080\u3075\u3080\u3002Pawpularity\u3063\u3066\u3001\u300c\u4eba\u6c17\u5ea6\u300d\uff08\uff1dpopularity\uff09\u306e\u306f\u305a\u306a\u3093\u3060\u3051\u3069\u3001No.11\u304c\u4eba\u6c17\u5ea62\u3068\u307b\u307c\u6700\u4f4e\u70b9\u3067\u3001No.19\u304c\u4eba\u6c17\u5ea6100\u3068\u6e80\u70b9\u306a\u3093\u3060\u3002No.15\u306e\u4eba\u6c17\u5ea698\u306f\u306a\u3093\u3068\u306a\u304f\u5206\u304b\u308b\u3051\u3069\u3001No.19\u304c\u306a\u3093\u3067\u6e80\u70b9\u306a\u306e\uff1f\uff1f\u3000\u307e\u3041\u3001\u3044\u3044\u3084\u3002\u6c17\u306b\u305b\u305a\u5148\u306b\u9032\u307f\u307e\u3057\u3087\u3046\u3002","85de374d":"Meta Data is just a supplemental explanation and hence very simple table, like Eyes - 1or0 (Both eyes are facing front or near-front) , Face - 1or0 (Decently clear face, facing front), or something like..  From a look at row11 and row15, I can see even if the data itself is exactly same (Eyes, Face, Near is 1 and the rest is 0), the Pawpularity is totally different (2 and 98)...\n\nMeta Data\u306f\u3001Photo\u306e\u4ed8\u5c5e\u8aac\u660e\u306a\u306e\u3067\u3001Eyes\uff08\u76ee\u304c\u3061\u3083\u3093\u3068\u524d\u3092\u5411\u3044\u3066\u3044\u308b\u304b 1or0\uff09\u3001Face\uff08\u9854\u304c\u304f\u3063\u304d\u308a\u3068\u524d\u3092\u5411\u3044\u3066\u3044\u308b\u304b 1or0\uff09\u3068\u3044\u3063\u305f\u975e\u5e38\u306b\u30b7\u30f3\u30d7\u30eb\u306a\u30c6\u30fc\u30d6\u30eb\u300211\u884c\u76ee\u306815\u884c\u76ee\u3092\u898b\u3066\u3082\u308f\u304b\u308b\u304c\u3001\u307e\u3063\u305f\u304f\u540c\u3058\u30c7\u30fc\u30bf\uff08Eyes, Face, Near\u304c1\u3002\u305d\u308c\u4ee5\u5916\u306f0\uff09\u3067\u3082\u3001Pawpularity\u306f2\u306898\u3068\u3044\u3046\u3088\u3046\u306b\u5168\u7136\u9055\u3046\u7d50\u679c\u3068\u306a\u3063\u3066\u3044\u308b\u3002","9734a012":"(Thanks to K589K589 for image-data reading method.)\n\n\nThis is the 3rd time for me to join Competition. I'm still a beginner.\nLet's see how much Score I can have with Meta data only, first.\nThen, I'll try to predict with CNN using Photo-image data.\n\n\u30b3\u30f3\u30da\u53c2\u52a0\u306f\u4eca\u56de\u30673\u56de\u76ee\u3002\u307e\u3060\u307e\u3060\u521d\u5fc3\u8005\u3002\n\u307e\u305a\u306fMeta Data\u3067\u3069\u306e\u7a0b\u5ea6\u306eScore\u304c\u51fa\u308b\u304b\u898b\u3066\u307f\u305f\u3044\u3068\u601d\u3046\u3002\n\u305d\u306e\u5f8c\u3001\u5199\u771f\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u30b7\u30f3\u30d7\u30eb\u306aCNN\u3067\u4e88\u6e2c\u3092\u3057\u3066\u307f\u3088\u3046\u3002\n\n**Public Score**\n\n**[ LGBM using Meta-data only ]**\n\n   (1st) **20.49737** : Normal LightGBM  \n   (2nd) **20.49137** : Add new features\n   \n**[ TensorFlow CNN using Photo-image data ]**\n\n   (3rd) **20.73773** : TensorFlow CNN  (Filter 8 > 16 > 32)  \n   (4th) **20.74033** : TensorFlow CNN  (Filter 32 > 32 > pooling >64 > 64 >pooling)\n","4ce215ec":"Just as an attempt, try to simply ensemble the result of LGBM and CNN (may not make sense sinse the data source is different...)  \nThe result is, Public Score shows **20.60492**, which is just slightly better than the simple average of LGBM-only 20.49137 and CNN-only 20.73773... As assumed, does not work very well..hmm..\n\nLGBM\u306e\u7d50\u679c\u3068CNN\u306e\u7d50\u679c\u3092\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3057\u3066\u307f\u308b\uff08\u30c7\u30fc\u30bf\u30bd\u30fc\u30b9\u304c\u9055\u3046\u306e\u3067\u4f59\u308a\u610f\u5473\u306f\u306a\u3044\u304b\u3082\u3057\u308c\u306a\u3044\u304c\u3002\u3002\u3002\uff09  \n\u7d50\u679c\u306f\u3001Public Score\u3000**20.60492**\u3068\u306a\u3063\u305f\u3002LGBM\u5358\u72ec\u30b9\u30b3\u30a220.49137\u3068CNN\u5358\u72ec\u30b9\u30b3\u30a220.73773\u306e\u5358\u7d14\u5e73\u5747\u3088\u308a\u308f\u305a\u304b\u306b\u826f\u3044\u7a0b\u5ea6\u3002\u3084\u3063\u3071\u308a\u3042\u307e\u308a\u610f\u5473\u306a\u304b\u3063\u305f\u304b\u306a\u3041\u3002"}}