{"cell_type":{"ab3a8f2f":"code","307bae5d":"code","9225ba7b":"code","b2215489":"code","e5a65c56":"code","20fdbcbb":"code","9f0e19b4":"code","3581ad5c":"code","287552a5":"code","b367bc91":"code","64064b37":"code","8fad63dc":"code","91a63e35":"code","6b325413":"code","339ba876":"code","8f23b3b2":"code","24b6600d":"code","2269882c":"code","ca2e0fdf":"code","e3940782":"code","a774bcfe":"code","2d877148":"code","2e2c6968":"code","4232d9e2":"code","89740c6a":"code","68c9ccb3":"code","8b1fed97":"code","8ee6cda5":"code","af3e2845":"code","bff2095a":"code","56ec3da4":"code","f5ebaa08":"code","cc72838c":"code","f2b2106b":"code","5f98b298":"code","8d0855b8":"code","8bad6b74":"code","13e86ea9":"code","56c1bcdb":"code","f1d794fb":"code","39595afe":"code","7450110a":"code","29f6356e":"code","7aa5445d":"code","8f8665d9":"code","65c12e2b":"code","76148ac6":"code","537abfd3":"code","8c3c8bd4":"code","7fa9f3ce":"code","b89945c8":"code","0f85ca50":"code","10aed37b":"code","75789916":"code","5a4d8708":"code","ce49df2a":"code","d42adf71":"code","58a33c15":"code","b3daa2a5":"code","43e91fde":"code","87a9fa53":"code","f5c1fc1f":"code","04175b82":"code","3964dce5":"markdown","c3464864":"markdown","679b89f0":"markdown","1c798f94":"markdown","1574a8c9":"markdown","77d68675":"markdown","e460e236":"markdown","c4720057":"markdown","21ca24bb":"markdown","b9968a67":"markdown","97381b38":"markdown","74e87e24":"markdown","583bb000":"markdown","c79d5108":"markdown","ed0b7c45":"markdown","a850e4ef":"markdown","85cb02e3":"markdown","6ea538cf":"markdown","6f8e3cd0":"markdown","659562c1":"markdown","65b1e743":"markdown"},"source":{"ab3a8f2f":"#Check the dataset sizes(in MB) \ub370\uc774\ud130\uc14b \ud06c\uae30 \uccb4\ud06c\n!du -l ..\/input\/*","307bae5d":"#import required packages\n#basics\nimport pandas as pd \nimport numpy as np\n\n#misc\nimport gc\nimport time\nimport warnings\n\n#stats\nfrom scipy.misc import imread\nfrom scipy import sparse\nimport scipy.stats as ss\n\n#viz\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec \nimport seaborn as sns\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom PIL import Image\nimport matplotlib_venn as venn\n\n#nlp\nimport string\nimport re    #for regex\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer   \n\n\n#FeatureEngineering\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\n\n\n\n\n\n#settings\nstart_time=time.time()\ncolor = sns.color_palette()\nsns.set_style(\"dark\")\neng_stopwords = set(stopwords.words(\"english\"))\nwarnings.filterwarnings(\"ignore\")\n\nlem = WordNetLemmatizer()\ntokenizer=TweetTokenizer()\n\n%matplotlib inline","9225ba7b":"#importing the dataset\ntrain=pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip\")\ntest=pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip\")\n#!ls ..\/input\/jigsaw-toxic-comment-classification-challenge\/\n'''\n\ub370\uc774\ud130\uc14b\uc774 csv\uc5d0\uc11c zip\uc73c\ub85c \ubcc0\uacbd\ub418\uc5c8\ub124\uc694.\n'''","b2215489":"#take a peak\ntrain.tail(10)","e5a65c56":"nrow_train=train.shape[0]\nnrow_test=test.shape[0]\nsum=nrow_train+nrow_test\nprint(\"       : train : test\")\nprint(\"rows   :\",nrow_train,\":\",nrow_test)\nprint(\"perc   :\",round(nrow_train*100\/sum),\"   :\",round(nrow_test*100\/sum))","20fdbcbb":"x=train.iloc[:,2:].sum()\n#marking comments without any tags as \"clean\"\n#\uc5b4\ub5a4 \ud0dc\uadf8\ub3c4 \uc5c6\ub294 \ub313\uae00\ub4e4\uc740 \"clean\"\uc73c\ub85c \ub9c8\ud0b9\nrowsums=train.iloc[:,2:].sum(axis=1)\ntrain['clean']=(rowsums==0)\n#count number of clean entries\n#clean\uc758 \uc218\ub97c \uacc4\uc0b0\ntrain['clean'].sum()\nprint(\"Total comments = \",len(train))\nprint(\"Total clean comments = \",train['clean'].sum())\nprint(\"Total tags =\",x.sum())","9f0e19b4":"train.head(10) #(\ud3b8\uc9d1\uc790\uc758 \ud655\uc778)","3581ad5c":"print(\"Check for missing values in Train dataset\")\nnull_check=train.isnull().sum()\nprint(null_check)\nprint(\"Check for missing values in Test dataset\")\nnull_check=test.isnull().sum()\nprint(null_check)\nprint(\"filling NA with \\\"unknown\\\"\")\ntrain[\"comment_text\"].fillna(\"unknown\", inplace=True)\ntest[\"comment_text\"].fillna(\"unknown\", inplace=True)","287552a5":"x=train.iloc[:,2:].sum()\n#plot\nplt.figure(figsize=(8,4))\nax= sns.barplot(x.index, x.values, alpha=0.8)\nplt.title(\"# per class\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('Type ', fontsize=12)\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()","b367bc91":"x=rowsums.value_counts()\n\n#plot\nplt.figure(figsize=(8,4))\nax = sns.barplot(x.index, x.values, alpha=0.8,color=color[2])\nplt.title(\"Multiple tags per comment\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('# of tags ', fontsize=12)\n\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()","64064b37":"temp_df=train.iloc[:,2:-1]\n# filter temp by removing clean comments\n# temp_df=temp_df[~train.clean]\n\ncorr=temp_df.corr()\nplt.figure(figsize=(10,8))\nsns.heatmap(corr,\n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values, annot=True)","8fad63dc":"# https:\/\/pandas.pydata.org\/pandas-docs\/stable\/style.html\ndef highlight_min(data, color='yellow'):\n    '''\n    highlight the maximum in a Series or DataFrame\n    '''\n    attr = 'background-color: {}'.format(color)\n    if data.ndim == 1:  # Series from .apply(axis=0) or axis=1\n        is_min = data == data.min()\n        return [attr if v else '' for v in is_min]\n    else:  # from .apply(axis=None)\n        is_max = data == data.min().min()\n        return pd.DataFrame(np.where(is_min, attr, ''),\n                            index=data.index, columns=data.columns)","91a63e35":"#Crosstab\n# Since technically a crosstab between all 6 classes is impossible to vizualize, lets take a \n# look at toxic with other tags\nmain_col=\"toxic\"\ncorr_mats=[]\nfor other_col in temp_df.columns[1:]:\n    confusion_matrix = pd.crosstab(temp_df[main_col], temp_df[other_col])\n    corr_mats.append(confusion_matrix)\nout = pd.concat(corr_mats,axis=1,keys=temp_df.columns[1:])\n\n#cell highlighting\nout = out.style.apply(highlight_min,axis=0)\nout","6b325413":"#https:\/\/stackoverflow.com\/questions\/20892799\/using-pandas-calculate-cram%C3%A9rs-coefficient-matrix\/39266194\ndef cramers_corrected_stat(confusion_matrix):\n    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n        uses correction from Bergsma and Wicher, \n        Journal of the Korean Statistical Society 42 (2013): 323-328\n    \"\"\"\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2 - ((k-1)*(r-1))\/(n-1))    \n    rcorr = r - ((r-1)**2)\/(n-1)\n    kcorr = k - ((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr \/ min( (kcorr-1), (rcorr-1)))","339ba876":"#Checking for Toxic and Severe toxic for now\nimport pandas as pd\ncol1=\"toxic\"\ncol2=\"severe_toxic\"\nconfusion_matrix = pd.crosstab(temp_df[col1], temp_df[col2])\nprint(\"Confusion matrix between toxic and severe toxic:\")\nprint(confusion_matrix)\nnew_corr=cramers_corrected_stat(confusion_matrix)\nprint(\"The correlation between Toxic and Severe toxic using Cramer's stat=\",new_corr)","8f23b3b2":"print(\"toxic:\")\nprint(train[train.severe_toxic==1].iloc[3,1])\n#print(train[train.severe_toxic==1].iloc[5,1])","24b6600d":"print(\"severe_toxic:\")\nprint(train[train.severe_toxic==1].iloc[4,1])\n#print(train[train.severe_toxic==1].iloc[4,1])","2269882c":"print(\"Threat:\")\nprint(train[train.threat==1].iloc[1,1])\n#print(train[train.threat==1].iloc[2,1])","ca2e0fdf":"print(\"Obscene:\")\nprint(train[train.obscene==1].iloc[1,1])\n#print(train[train.obscene==1].iloc[2,1])","e3940782":"print(\"identity_hate:\")\nprint(train[train.identity_hate==1].iloc[4,1])\n#print(train[train.identity_hate==1].iloc[4,1])","a774bcfe":"!ls ..\/input\/imagesforkernal\/\nstopword=set(STOPWORDS)","2d877148":"#clean comments\nclean_mask=np.array(Image.open(\"..\/input\/imagesforkernal\/safe-zone.png\"))\nclean_mask=clean_mask[:,:,1]\n#wordcloud for clean comments\nsubset=train[train.clean==True]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=2000,mask=clean_mask,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(20,10))\nplt.axis(\"off\")\nplt.title(\"Words frequented in Clean Comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'viridis' , random_state=17), alpha=0.98)\nplt.show()","2e2c6968":"toxic_mask=np.array(Image.open(\"..\/input\/imagesforkernal\/toxic-sign.png\"))\ntoxic_mask=toxic_mask[:,:,1]\n#wordcloud for clean comments\nsubset=train[train.toxic==1]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=4000,mask=toxic_mask,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(20,20))\nplt.subplot(221)\nplt.axis(\"off\")\nplt.title(\"Words frequented in Toxic Comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'gist_earth' , random_state=244), alpha=0.98)\n\n#Severely toxic comments\nplt.subplot(222)\nsevere_toxic_mask=np.array(Image.open(\"..\/input\/imagesforkernal\/bomb.png\"))\nsevere_toxic_mask=severe_toxic_mask[:,:,1]\nsubset=train[train.severe_toxic==1]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=2000,mask=severe_toxic_mask,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.axis(\"off\")\nplt.title(\"Words frequented in Severe Toxic Comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'Reds' , random_state=244), alpha=0.98)\n\n#Threat comments\nplt.subplot(223)\nthreat_mask=np.array(Image.open(\"..\/input\/imagesforkernal\/anger.png\"))\nthreat_mask=threat_mask[:,:,1]\nsubset=train[train.threat==1]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=2000,mask=threat_mask,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.axis(\"off\")\nplt.title(\"Words frequented in Threatening Comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'summer' , random_state=2534), alpha=0.98)\n\n#insult\nplt.subplot(224)\ninsult_mask=np.array(Image.open(\"..\/input\/imagesforkernal\/swords.png\"))\ninsult_mask=insult_mask[:,:,1]\nsubset=train[train.insult==1]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=2000,mask=insult_mask,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.axis(\"off\")\nplt.title(\"Words frequented in insult Comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'Paired_r' , random_state=244), alpha=0.98)\n\nplt.show()\n","4232d9e2":"merge=pd.concat([train.iloc[:,0:2],test.iloc[:,0:2]])\ndf=merge.reset_index(drop=True)","89740c6a":"## Indirect features\n\n#Sentense count in each comment:\n    #  '\\n' can be used to count the number of sentences in each comment\ndf['count_sent']=df[\"comment_text\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n#Word count in each comment:\ndf['count_word']=df[\"comment_text\"].apply(lambda x: len(str(x).split()))\n#Unique word count\ndf['count_unique_word']=df[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n#Letter count\ndf['count_letters']=df[\"comment_text\"].apply(lambda x: len(str(x)))\n#punctuation count\ndf[\"count_punctuations\"] =df[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n#upper case words count\ndf[\"count_words_upper\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n#title case words count\ndf[\"count_words_title\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n#Number of stopwords\ndf[\"count_stopwords\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n#Average length of the words\ndf[\"mean_word_len\"] = df[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","68c9ccb3":"#derived features\n#Word count percent in each comment:\ndf['word_unique_percent']=df['count_unique_word']*100\/df['count_word']\n#derived features\n#Punct percent in each comment:\ndf['punct_percent']=df['count_punctuations']*100\/df['count_word']","8b1fed97":"#serperate train and test features\ntrain_feats=df.iloc[0:len(train),]\ntest_feats=df.iloc[len(train):,]\n#join the tags\ntrain_tags=train.iloc[:,2:]\ntrain_feats=pd.concat([train_feats,train_tags],axis=1)","8ee6cda5":"train_feats['count_sent'].loc[train_feats['count_sent']>10] = 10 \nplt.figure(figsize=(12,6))\n## sentenses\nplt.subplot(121)\nplt.suptitle(\"Are longer comments more toxic?\",fontsize=20)\nsns.violinplot(y='count_sent',x='clean', data=train_feats,split=True)\nplt.xlabel('Clean?', fontsize=12)\nplt.ylabel('# of sentences', fontsize=12)\nplt.title(\"Number of sentences in each comment\", fontsize=15)\n# words\ntrain_feats['count_word'].loc[train_feats['count_word']>200] = 200\nplt.subplot(122)\nsns.violinplot(y='count_word',x='clean', data=train_feats,split=True,inner=\"quart\")\nplt.xlabel('Clean?', fontsize=12)\nplt.ylabel('# of words', fontsize=12)\nplt.title(\"Number of words in each comment\", fontsize=15)\n\nplt.show()","af3e2845":"train_feats['count_unique_word'].loc[train_feats['count_unique_word']>200] = 200\n#prep for split violin plots\n#For the desired plots , the data must be in long format\ntemp_df = pd.melt(train_feats, value_vars=['count_word', 'count_unique_word'], id_vars='clean')\n#spammers - comments with less than 30% unique words\nspammers=train_feats[train_feats['word_unique_percent']<30]","bff2095a":"plt.figure(figsize=(16,12))\nplt.suptitle(\"What's so unique ?\",fontsize=20)\ngridspec.GridSpec(2,2)\nplt.subplot2grid((2,2),(0,0))\nsns.violinplot(x='variable', y='value', hue='clean', data=temp_df,split=True,inner='quartile')\nplt.title(\"Absolute wordcount and unique words count\")\nplt.xlabel('Feature', fontsize=12)\nplt.ylabel('Count', fontsize=12)\n\nplt.subplot2grid((2,2),(0,1))\nplt.title(\"Percentage of unique words of total words in comment\")\n#sns.boxplot(x='clean', y='word_unique_percent', data=train_feats)\nax=sns.kdeplot(train_feats[train_feats.clean == 0].word_unique_percent, label=\"Bad\",shade=True,color='r')\nax=sns.kdeplot(train_feats[train_feats.clean == 1].word_unique_percent, label=\"Clean\")\nplt.legend()\nplt.ylabel('Number of occurances', fontsize=12)\nplt.xlabel('Percent unique words', fontsize=12)\n\nx=spammers.iloc[:,-7:].sum()\nplt.subplot2grid((2,2),(1,0),colspan=2)\nplt.title(\"Count of comments with low(<30%) unique words\",fontsize=15)\nax=sns.barplot(x=x.index, y=x.values,color=color[3])\n\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\n\nplt.xlabel('Threat class', fontsize=12)\nplt.ylabel('# of comments', fontsize=12)\nplt.show()","56ec3da4":"print(\"Clean Spam example:\")\nprint(spammers[spammers.clean==1].comment_text.iloc[1])\nprint(\"Toxic Spam example:\")\nprint(spammers[spammers.toxic==1].comment_text.iloc[2])","f5ebaa08":"#Leaky features\ndf['ip']=df[\"comment_text\"].apply(lambda x: re.findall(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",str(x)))\n#count of ip addresses\ndf['count_ip']=df[\"ip\"].apply(lambda x: len(x))\n\n#links\ndf['link']=df[\"comment_text\"].apply(lambda x: re.findall(\"http:\/\/.*com\",str(x)))\n#count of links\ndf['count_links']=df[\"link\"].apply(lambda x: len(x))\n\n#article ids\ndf['article_id']=df[\"comment_text\"].apply(lambda x: re.findall(\"\\d:\\d\\d\\s{0,5}$\",str(x)))\ndf['article_id_flag']=df.article_id.apply(lambda x: len(x))\n\n#username\n##              regex for     Match anything with [[User: ---------- ]]\n# regexp = re.compile(\"\\[\\[User:(.*)\\|\")\ndf['username']=df[\"comment_text\"].apply(lambda x: re.findall(\"\\[\\[User(.*)\\|\",str(x)))\n#count of username mentions\ndf['count_usernames']=df[\"username\"].apply(lambda x: len(x))\n#check if features are created\n#df.username[df.count_usernames>0]\n\n# Leaky Ip\ncv = CountVectorizer()\ncount_feats_ip = cv.fit_transform(df[\"ip\"].apply(lambda x : str(x)))\n\n\n# Leaky usernames\n\ncv = CountVectorizer()\ncount_feats_user = cv.fit_transform(df[\"username\"].apply(lambda x : str(x)))\n\n","cc72838c":"df[df.count_usernames!=0].comment_text.iloc[0]","f2b2106b":"# check few names\ncv.get_feature_names()[120:130]","5f98b298":"leaky_feats=df[[\"ip\",\"link\",\"article_id\",\"username\",\"count_ip\",\"count_links\",\"count_usernames\",\"article_id_flag\"]]\nleaky_feats_train=leaky_feats.iloc[:train.shape[0]]\nleaky_feats_test=leaky_feats.iloc[train.shape[0]:]","8d0855b8":"#filterout the entries without ips\ntrain_ips=leaky_feats_train.ip[leaky_feats_train.count_ip!=0]\ntest_ips=leaky_feats_test.ip[leaky_feats_test.count_ip!=0]\n#get the unique list of ips in test and train datasets\ntrain_ip_list=list(set([a for b in train_ips.tolist() for a in b]))\ntest_ip_list=list(set([a for b in test_ips.tolist() for a in b]))\n\n# get common elements\ncommon_ip_list=list(set(train_ip_list).intersection(test_ip_list))\nplt.title(\"Common IP addresses\")\nvenn.venn2(subsets=(len(train_ip_list),len(test_ip_list),len(common_ip_list)),set_labels=(\"# of unique IP in train\",\"# of unique IP in test\"))\nplt.show()","8bad6b74":"#filterout the entries without links\ntrain_links=leaky_feats_train.link[leaky_feats_train.count_links!=0]\ntest_links=leaky_feats_test.link[leaky_feats_test.count_links!=0]\n#get the unique list of ips in test and train datasets\ntrain_links_list=list(set([a for b in train_links.tolist() for a in b]))\ntest_links_list=list(set([a for b in test_links.tolist() for a in b]))\n\n# get common elements\ncommon_links_list=list(set(train_links_list).intersection(test_links_list))\nplt.title(\"Common links\")\nvenn.venn2(subsets=(len(train_links_list),len(test_links_list),len(common_links_list)),\n           set_labels=(\"# of unique links in train\",\"# of unique links in test\"))\nplt.show()","13e86ea9":"#filterout the entries without users\ntrain_users=leaky_feats_train.username[leaky_feats_train.count_usernames!=0]\ntest_users=leaky_feats_test.username[leaky_feats_test.count_usernames!=0]\n#get the unique list of ips in test and train datasets\ntrain_users_list=list(set([a for b in train_users.tolist() for a in b]))\ntest_users_list=list(set([a for b in test_users.tolist() for a in b]))\n\n# get common elements\ncommon_users_list=list(set(train_users_list).intersection(test_users_list))\nplt.title(\"Common usernames\")\nvenn.venn2(subsets=(len(train_users_list),len(test_users_list),len(common_users_list)),\n           set_labels=(\"# of unique Usernames in train\",\"# of unique usernames in test\"))\nplt.show()","56c1bcdb":"#https:\/\/en.wikipedia.org\/wiki\/Wikipedia:Database_reports\/Indefinitely_blocked_IPs)\n\nblocked_ips=[\"216.102.6.176\",\n\"216.120.176.2\",\n\"203.25.150.5\",\n\"203.217.8.30\",\n\"66.90.101.58\",\n\"125.178.86.75\",\n\"210.15.217.194\",\n\"69.36.166.207\",\n\"213.25.24.253\",\n\"24.60.181.235\",\n\"71.204.14.32\",\n\"216.91.92.18\",\n\"212.219.2.4\",\n\"194.74.190.162\",\n\"64.15.152.246\",\n\"59.100.76.166\",\n\"146.145.221.129\",\n\"146.145.221.130\",\n\"74.52.44.34\",\n\"68.5.96.201\",\n\"65.184.176.45\",\n\"209.244.43.209\",\n\"82.46.9.168\",\n\"209.200.236.32\",\n\"209.200.229.181\",\n\"202.181.99.22\",\n\"220.233.226.170\",\n\"212.138.64.178\",\n\"220.233.227.249\",\n\"72.14.194.31\",\n\"72.249.45.0\/24\",\n\"72.249.44.0\/24\",\n\"80.175.39.213\",\n\"81.109.164.45\",\n\"64.157.15.0\/24\",\n\"208.101.10.54\",\n\"216.157.200.254\",\n\"72.14.192.14\",\n\"204.122.16.13\",\n\"217.156.39.245\",\n\"210.11.188.16\",\n\"210.11.188.17\",\n\"210.11.188.18\",\n\"210.11.188.19\",\n\"210.11.188.20\",\n\"64.34.27.153\",\n\"209.68.139.150\",\n\"152.163.100.0\/24\",\n\"65.175.48.2\",\n\"131.137.245.197\",\n\"131.137.245.199\",\n\"131.137.245.200\",\n\"64.233.172.37\",\n\"66.99.182.25\",\n\"67.43.21.12\",\n\"66.249.85.85\",\n\"65.175.134.11\",\n\"201.218.3.198\",\n\"193.213.85.12\",\n\"131.137.245.198\",\n\"83.138.189.74\",\n\"72.14.193.163\",\n\"66.249.84.69\",\n\"209.204.71.2\",\n\"80.217.153.189\",\n\"83.138.136.92\",\n\"83.138.136.91\",\n\"83.138.189.75\",\n\"83.138.189.76\",\n\"212.100.250.226\",\n\"212.100.250.225\",\n\"212.159.98.189\",\n\"87.242.116.201\",\n\"74.53.243.18\",\n\"213.219.59.96\/27\",\n\"212.219.82.37\",\n\"203.38.149.226\",\n\"66.90.104.22\",\n\"125.16.137.130\",\n\"66.98.128.0\/17\",\n\"217.33.236.2\",\n\"24.24.200.113\",\n\"152.22.0.254\",\n\"59.145.89.17\",\n\"71.127.224.0\/20\",\n\"65.31.98.71\",\n\"67.53.130.69\",\n\"204.130.130.0\/24\",\n\"72.14.193.164\",\n\"65.197.143.214\",\n\"202.60.95.235\",\n\"69.39.89.95\",\n\"88.80.215.14\",\n\"216.218.214.2\",\n\"81.105.175.201\",\n\"203.108.239.12\",\n\"74.220.207.168\",\n\"206.253.55.206\",\n\"206.253.55.207\",\n\"206.253.55.208\",\n\"206.253.55.209\",\n\"206.253.55.210\",\n\"66.64.56.194\",\n\"70.91.90.226\",\n\"209.60.205.96\",\n\"202.173.191.210\",\n\"169.241.10.83\",\n\"91.121.195.205\",\n\"216.70.136.88\",\n\"72.228.151.208\",\n\"66.197.167.120\",\n\"212.219.232.81\",\n\"208.86.225.40\",\n\"63.232.20.2\",\n\"206.219.189.8\",\n\"212.219.14.0\/24\",\n\"165.228.71.6\",\n\"99.230.151.129\",\n\"72.91.11.99\",\n\"173.162.177.53\",\n\"60.242.166.182\",\n\"212.219.177.34\",\n\"12.104.27.5\",\n\"85.17.92.13\",\n\"91.198.174.192\/27\",\n\"155.246.98.61\",\n\"71.244.123.63\",\n\"81.144.152.130\",\n\"198.135.70.1\",\n\"71.255.126.146\",\n\"74.180.82.59\",\n\"206.158.2.80\",\n\"64.251.53.34\",\n\"24.29.92.238\",\n\"76.254.235.105\",\n\"68.96.242.239\",\n\"203.202.234.226\",\n\"173.72.89.88\",\n\"87.82.229.195\",\n\"68.153.245.37\",\n\"216.240.128.0\/19\",\n\"72.46.129.44\",\n\"66.91.35.165\",\n\"82.71.49.124\",\n\"69.132.171.231\",\n\"75.145.183.129\",\n\"194.80.20.237\",\n\"98.207.253.170\",\n\"76.16.222.162\",\n\"66.30.100.130\",\n\"96.22.29.23\",\n\"76.168.140.158\",\n\"202.131.166.252\",\n\"89.207.212.99\",\n\"81.169.155.246\",\n\"216.56.8.66\",\n\"206.15.235.10\",\n\"115.113.95.20\",\n\"204.209.59.11\",\n\"27.33.141.67\",\n\"41.4.65.162\",\n\"99.6.65.6\",\n\"60.234.239.169\",\n\"2620:0:862:101:0:0:2:0\/124\",\n\"183.192.165.31\",\n\"50.68.6.12\",\n\"37.214.82.134\",\n\"96.50.0.230\",\n\"60.231.28.109\",\n\"64.90.240.50\",\n\"49.176.97.12\",\n\"209.80.150.137\",\n\"24.22.67.116\",\n\"206.180.81.2\",\n\"195.194.39.100\",\n\"87.41.52.6\",\n\"169.204.164.227\",\n\"50.137.55.117\",\n\"50.77.84.161\",\n\"90.202.230.247\",\n\"186.88.129.224\",\n\"2A02:EC80:101:0:0:0:2:0\/124\",\n\"142.4.117.177\",\n\"86.40.105.198\",\n\"120.43.20.149\",\n\"198.199.64.0\/18\",\n\"192.34.56.0\/21\",\n\"192.81.208.0\/20\",\n\"2604:A880:0:0:0:0:0:0\/32\",\n\"108.72.107.229\",\n\"2602:306:CC2B:7000:41D3:B92D:731C:959D\",\n\"185.15.59.201\",\n\"180.149.1.229\",\n\"207.191.188.66\",\n\"210.22.63.92\",\n\"117.253.196.217\",\n\"119.160.119.172\",\n\"90.217.133.223\",\n\"194.83.8.3\",\n\"194.83.164.22\",\n\"217.23.228.149\",\n\"65.18.58.1\",\n\"168.11.15.2\",\n\"65.182.127.31\",\n\"207.106.153.252\",\n\"64.193.88.2\",\n\"152.26.71.2\",\n\"199.185.67.179\",\n\"117.90.240.73\",\n\"108.176.58.170\",\n\"195.54.40.28\",\n\"185.35.164.109\",\n\"192.185.0.0\/16\",\n\"2605:E000:1605:C0C0:3D3D:A148:3039:71F1\",\n\"107.158.0.0\/16\",\n\"85.159.232.0\/21\",\n\"69.235.4.10\",\n\"86.176.166.206\",\n\"108.65.152.51\",\n\"10.4.1.0\/24\",\n\"103.27.227.139\",\n\"188.55.31.191\",\n\"188.53.13.34\",\n\"176.45.58.252\",\n\"176.45.22.37\",\n\"24.251.44.140\",\n\"108.200.140.191\",\n\"117.177.169.4\",\n\"72.22.162.38\",\n\"24.106.242.82\",\n\"79.125.190.93\",\n\"107.178.200.1\",\n\"123.16.244.246\",\n\"83.228.167.87\",\n\"128.178.197.53\",\n\"14.139.172.18\",\n\"207.108.136.254\",\n\"184.152.17.217\",\n\"186.94.29.73\",\n\"217.200.199.2\",\n\"66.58.141.104\",\n\"166.182.81.30\",\n\"89.168.206.116\",\n\"92.98.163.145\",\n\"77.115.31.71\",\n\"178.36.118.74\",\n\"157.159.10.14\",\n\"103.5.212.139\",\n\"203.174.180.226\",\n\"69.123.252.95\",\n\"199.200.123.233\",\n\"121.45.89.82\",\n\"71.228.87.155\",\n\"68.189.67.92\",\n\"216.161.176.152\",\n\"98.17.30.139\",\n\"2600:1006:B124:84BD:0:0:0:103\",\n\"117.161.0.0\/16\",\n\"12.166.68.34\",\n\"96.243.149.64\",\n\"74.143.90.218\",\n\"76.10.176.221\",\n\"104.250.128.0\/19\",\n\"185.22.183.128\/25\",\n\"89.105.194.64\/26\",\n\"202.45.119.0\/24\",\n\"73.9.140.64\",\n\"164.127.71.72\",\n\"50.160.129.2\",\n\"49.15.213.207\",\n\"83.7.192.0\/18\",\n\"201.174.63.79\",\n\"2A02:C7D:4643:8F00:D09D:BE1:D2DE:BB1F\",\n\"125.60.195.230\",\n\"49.145.113.145\",\n\"168.18.160.134\",\n\"72.193.218.222\",\n\"199.216.164.10\",\n\"120.144.130.89\",\n\"104.130.67.208\",\n\"50.160.221.147\",\n\"163.47.141.50\",\n\"91.200.12.136\",\n\"83.222.0.0\/19\",\n\"67.231.16.0\/20\",\n\"72.231.0.196\",\n\"180.216.68.197\",\n\"183.160.178.135\",\n\"183.160.176.16\",\n\"24.25.221.150\",\n\"92.222.109.43\",\n\"142.134.243.215\",\n\"216.181.221.72\",\n\"113.205.170.110\",\n\"74.142.2.98\",\n\"192.235.8.3\",\n\"2402:4000:BBFC:36FC:E469:F2F0:9351:71A0\",\n\"80.244.81.191\",\n\"2607:FB90:1377:F765:D45D:46BF:81EA:9773\",\n\"2600:1009:B012:7D88:418B:54BA:FCBC:4584\",\n\"104.237.224.0\/19\",\n\"2600:1008:B01B:E495:C05A:7DD3:926:E83C\",\n\"168.8.249.234\",\n\"162.211.179.36\",\n\"138.68.0.0\/16\",\n\"145.236.37.195\",\n\"67.205.128.0\/18\",\n\"2A02:C7D:2832:CE00:B914:19D6:948D:B37D\",\n\"107.77.203.212\",\n\"2607:FB90:65C:A136:D46F:23BA:87C2:3D10\",\n\"2A02:C7F:DE2F:7900:5D64:E991:FFF0:FA93\",\n\"82.23.32.186\",\n\"106.76.243.74\",\n\"82.33.48.223\",\n\"180.216.160.0\/19\",\n\"94.102.184.35\",\n\"94.102.184.26\",\n\"109.92.162.54\",\n\"2600:8800:7180:BF00:4C27:4591:347C:736C\",\n\"178.41.186.50\",\n\"184.97.134.128\",\n\"176.221.32.0\/22\",\n\"207.99.40.142\",\n\"109.97.241.134\",\n\"82.136.64.19\",\n\"91.236.74.119\",\n\"197.210.0.0\/16\",\n\"173.230.128.0\/19\",\n\"162.216.16.0\/22\",\n\"80.111.222.211\",\n\"191.37.28.21\",\n\"124.124.103.194\",\n\"50.207.7.198\",\n\"220.233.131.98\",\n\"107.77.241.11\",\n\"68.112.39.0\/27\",\n\"173.236.128.0\/17\",\n\"49.49.240.24\",\n\"96.31.10.178\",\n\"50.251.229.75\"]\n","f1d794fb":"train_ip_list=list(set([a for b in train_ips.tolist() for a in b]))\ntest_ip_list=list(set([a for b in test_ips.tolist() for a in b]))\n\n# get common elements\nblocked_ip_list_train=list(set(train_ip_list).intersection(blocked_ips))\nblocked_ip_list_test=list(set(test_ip_list).intersection(blocked_ips))\n\nprint(\"There are\",len(blocked_ip_list_train),\"blocked IPs in train dataset\")\nprint(\"There are\",len(blocked_ip_list_test),\"blocked IPs in test dataset\")","39595afe":"end_time=time.time()\nprint(\"total time till Leaky feats\",end_time-start_time)","7450110a":"corpus=merge.comment_text","29f6356e":"#https:\/\/drive.google.com\/file\/d\/0B1yuv8YaUVlZZ1RzMFJmc1ZsQmM\/view\n# Aphost lookup dict\nAPPO = {\n\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"\n}","7aa5445d":"def clean(comment):\n    \"\"\"\n    This function receives comments and returns clean word-list\n    \"\"\"\n    #Convert to lower case , so that Hi and hi are the same\n    comment=comment.lower()\n    #remove \\n\n    comment=re.sub(\"\\\\n\",\"\",comment)\n    # remove leaky elements like ip,user\n    comment=re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",comment)\n    #removing usernames\n    comment=re.sub(\"\\[\\[.*\\]\",\"\",comment)\n    \n    #Split the sentences into words\n    words=tokenizer.tokenize(comment)\n    \n    # (')aphostophe  replacement (ie)   you're --> you are  \n    # ( basic dictionary lookup : master dictionary present in a hidden block of code)\n    words=[APPO[word] if word in APPO else word for word in words]\n    words=[lem.lemmatize(word, \"v\") for word in words]\n    words = [w for w in words if not w in eng_stopwords]\n    \n    clean_sent=\" \".join(words)\n    # remove any non alphanum,digit character\n    #clean_sent=re.sub(\"\\W+\",\" \",clean_sent)\n    #clean_sent=re.sub(\"  \",\" \",clean_sent)\n    return(clean_sent)","8f8665d9":"corpus.iloc[12235]","65c12e2b":"clean(corpus.iloc[12235])","76148ac6":"clean_corpus=corpus.apply(lambda x :clean(x))\n\nend_time=time.time()\nprint(\"total time till Cleaning\",end_time-start_time)","537abfd3":"# To do next:\n# Slang lookup dictionary for sentiments\n#http:\/\/slangsd.com\/data\/SlangSD.zip\n#http:\/\/arxiv.org\/abs\/1608.05129\n# dict lookup \n#https:\/\/bytes.com\/topic\/python\/answers\/694819-regular-expression-dictionary-key-search\n","8c3c8bd4":"### Unigrams -- TF-IDF \n# using settings recommended here for TF-IDF -- https:\/\/www.kaggle.com\/abhishek\/approaching-almost-any-nlp-problem-on-kaggle\n\n#some detailed description of the parameters\n# min_df=10 --- ignore terms that appear lesser than 10 times \n# max_features=None  --- Create as many words as present in the text corpus\n    # changing max_features to 10k for memmory issues\n# analyzer='word'  --- Create features from words (alternatively char can also be used)\n# ngram_range=(1,1)  --- Use only one word at a time (unigrams)\n# strip_accents='unicode' -- removes accents\n# use_idf=1,smooth_idf=1 --- enable IDF\n# sublinear_tf=1   --- Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf)\n\n\n#temp settings to min=200 to facilitate top features section to run in kernals\n#change back to min=10 to get better results\nstart_unigrams=time.time()\ntfv = TfidfVectorizer(min_df=200,  max_features=10000, \n            strip_accents='unicode', analyzer='word',ngram_range=(1,1),\n            use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\ntfv.fit(clean_corpus)\nfeatures = np.array(tfv.get_feature_names())\n\ntrain_unigrams =  tfv.transform(clean_corpus.iloc[:train.shape[0]])\ntest_unigrams = tfv.transform(clean_corpus.iloc[train.shape[0]:])","7fa9f3ce":"#https:\/\/buhrmann.github.io\/tfidf-analysis.html\ndef top_tfidf_feats(row, features, top_n=25):\n    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n    topn_ids = np.argsort(row)[::-1][:top_n]\n    top_feats = [(features[i], row[i]) for i in topn_ids]\n    df = pd.DataFrame(top_feats)\n    df.columns = ['feature', 'tfidf']\n    return df\n\ndef top_feats_in_doc(Xtr, features, row_id, top_n=25):\n    ''' Top tfidf features in specific document (matrix row) '''\n    row = np.squeeze(Xtr[row_id].toarray())\n    return top_tfidf_feats(row, features, top_n)\n\ndef top_mean_feats(Xtr, features, grp_ids, min_tfidf=0.1, top_n=25):\n    ''' Return the top n features that on average are most important amongst documents in rows\n        indentified by indices in grp_ids. '''\n    \n    D = Xtr[grp_ids].toarray()\n\n    D[D < min_tfidf] = 0\n    tfidf_means = np.mean(D, axis=0)\n    return top_tfidf_feats(tfidf_means, features, top_n)\n\n# modified for multilabel milticlass\ndef top_feats_by_class(Xtr, features, min_tfidf=0.1, top_n=20):\n    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n        calculated across documents with the same class label. '''\n    dfs = []\n    cols=train_tags.columns\n    for col in cols:\n        ids = train_tags.index[train_tags[col]==1]\n        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n        feats_df.label = label\n        dfs.append(feats_df)\n    return dfs","b89945c8":"#get top n for unigrams\ntfidf_top_n_per_lass=top_feats_by_class(train_unigrams,features)\n\nend_unigrams=time.time()\n\nprint(\"total time in unigrams\",end_unigrams-start_unigrams)\nprint(\"total time till unigrams\",end_unigrams-start_time)\n","0f85ca50":"plt.figure(figsize=(16,22))\nplt.suptitle(\"TF_IDF Top words per class(unigrams)\",fontsize=20)\ngridspec.GridSpec(4,2)\nplt.subplot2grid((4,2),(0,0))\nsns.barplot(tfidf_top_n_per_lass[0].feature.iloc[0:9],tfidf_top_n_per_lass[0].tfidf.iloc[0:9],color=color[0])\nplt.title(\"class : Toxic\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\nplt.subplot2grid((4,2),(0,1))\nsns.barplot(tfidf_top_n_per_lass[1].feature.iloc[0:9],tfidf_top_n_per_lass[1].tfidf.iloc[0:9],color=color[1])\nplt.title(\"class : Severe toxic\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.subplot2grid((4,2),(1,0))\nsns.barplot(tfidf_top_n_per_lass[2].feature.iloc[0:9],tfidf_top_n_per_lass[2].tfidf.iloc[0:9],color=color[2])\nplt.title(\"class : Obscene\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.subplot2grid((4,2),(1,1))\nsns.barplot(tfidf_top_n_per_lass[3].feature.iloc[0:9],tfidf_top_n_per_lass[3].tfidf.iloc[0:9],color=color[3])\nplt.title(\"class : Threat\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.subplot2grid((4,2),(2,0))\nsns.barplot(tfidf_top_n_per_lass[4].feature.iloc[0:9],tfidf_top_n_per_lass[4].tfidf.iloc[0:9],color=color[4])\nplt.title(\"class : Insult\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.subplot2grid((4,2),(2,1))\nsns.barplot(tfidf_top_n_per_lass[5].feature.iloc[0:9],tfidf_top_n_per_lass[5].tfidf.iloc[0:9],color=color[5])\nplt.title(\"class : Identity hate\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.subplot2grid((4,2),(3,0),colspan=2)\nsns.barplot(tfidf_top_n_per_lass[6].feature.iloc[0:19],tfidf_top_n_per_lass[6].tfidf.iloc[0:19])\nplt.title(\"class : Clean\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\nplt.show()","10aed37b":"\n#temp settings to min=150 to facilitate top features section to run in kernals\n#change back to min=10 to get better results\ntfv = TfidfVectorizer(min_df=150,  max_features=30000, \n            strip_accents='unicode', analyzer='word',ngram_range=(2,2),\n            use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\ntfv.fit(clean_corpus)\nfeatures = np.array(tfv.get_feature_names())\ntrain_bigrams =  tfv.transform(clean_corpus.iloc[:train.shape[0]])\ntest_bigrams = tfv.transform(clean_corpus.iloc[train.shape[0]:])\n#get top n for bigrams\ntfidf_top_n_per_lass=top_feats_by_class(train_bigrams,features)","75789916":"plt.figure(figsize=(16,22))\nplt.suptitle(\"TF_IDF Top words per class(Bigrams)\",fontsize=20)\ngridspec.GridSpec(4,2)\nplt.subplot2grid((4,2),(0,0))\nsns.barplot(tfidf_top_n_per_lass[0].feature.iloc[0:5],tfidf_top_n_per_lass[0].tfidf.iloc[0:5],color=color[0])\nplt.title(\"class : Toxic\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\nplt.subplot2grid((4,2),(0,1))\nsns.barplot(tfidf_top_n_per_lass[1].feature.iloc[0:5],tfidf_top_n_per_lass[1].tfidf.iloc[0:5],color=color[1])\nplt.title(\"class : Severe toxic\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.subplot2grid((4,2),(1,0))\nsns.barplot(tfidf_top_n_per_lass[2].feature.iloc[0:5],tfidf_top_n_per_lass[2].tfidf.iloc[0:5],color=color[2])\nplt.title(\"class : Obscene\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.subplot2grid((4,2),(1,1))\nsns.barplot(tfidf_top_n_per_lass[3].feature.iloc[0:5],tfidf_top_n_per_lass[3].tfidf.iloc[0:5],color=color[3])\nplt.title(\"class : Threat\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.subplot2grid((4,2),(2,0))\nsns.barplot(tfidf_top_n_per_lass[4].feature.iloc[0:5],tfidf_top_n_per_lass[4].tfidf.iloc[0:5],color=color[4])\nplt.title(\"class : Insult\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.subplot2grid((4,2),(2,1))\nsns.barplot(tfidf_top_n_per_lass[5].feature.iloc[0:5],tfidf_top_n_per_lass[5].tfidf.iloc[0:5],color=color[5])\nplt.title(\"class : Identity hate\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\n\nplt.subplot2grid((4,2),(3,0),colspan=2)\nsns.barplot(tfidf_top_n_per_lass[6].feature.iloc[0:9],tfidf_top_n_per_lass[6].tfidf.iloc[0:9])\nplt.title(\"class : Clean\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\nplt.show()","5a4d8708":"end_time=time.time()\nprint(\"total time till bigrams\",end_time-start_time)","ce49df2a":"tfv = TfidfVectorizer(min_df=100,  max_features=30000, \n            strip_accents='unicode', analyzer='char',ngram_range=(1,4),\n            use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\ntfv.fit(clean_corpus)\nfeatures = np.array(tfv.get_feature_names())\ntrain_charngrams =  tfv.transform(clean_corpus.iloc[:train.shape[0]])\ntest_charngrams = tfv.transform(clean_corpus.iloc[train.shape[0]:])\nend_time=time.time()\nprint(\"total time till charngrams\",end_time-start_time)","d42adf71":"#Credis to AlexSanchez https:\/\/www.kaggle.com\/jhoward\/nb-svm-strong-linear-baseline-eda-0-052-lb#261316\n#custom NB model\n\nclass NbSvmClassifier(BaseEstimator, ClassifierMixin):\n    def __init__(self, C=1.0, dual=False, n_jobs=1):\n        self.C = C\n        self.dual = dual\n        self.n_jobs = n_jobs\n\n    def predict(self, x):\n        # Verify that model has been fit\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict(x.multiply(self._r))\n\n    def predict_proba(self, x):\n        # Verify that model has been fit\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict_proba(x.multiply(self._r))\n\n    def fit(self, x, y):\n        # Check that X and y have correct shape\n        y = y.values\n        x, y = check_X_y(x, y, accept_sparse=True)\n\n        def pr(x, y_i, y):\n            p = x[y==y_i].sum(0)\n            return (p+1) \/ ((y==y_i).sum()+1)\n\n        self._r = sparse.csr_matrix(np.log(pr(x,1,y) \/ pr(x,0,y)))\n        x_nb = x.multiply(self._r)\n        self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n        return self\n    \n# model = NbSvmClassifier(C=4, dual=True, n_jobs=-1)\n","58a33c15":"SELECTED_COLS=['count_sent', 'count_word', 'count_unique_word',\n       'count_letters', 'count_punctuations', 'count_words_upper',\n       'count_words_title', 'count_stopwords', 'mean_word_len',\n       'word_unique_percent', 'punct_percent']\ntarget_x=train_feats[SELECTED_COLS]\n# target_x\n\nTARGET_COLS=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ntarget_y=train_tags[TARGET_COLS]\n\n# Strat k fold due to imbalanced classes\n# split = StratifiedKFold(n_splits=2, random_state=1)\n\n#https:\/\/www.kaggle.com\/yekenot\/toxic-regression","b3daa2a5":"#Just the indirect features -- meta features\nprint(\"Using only Indirect features\")\nmodel = LogisticRegression(C=3)\nX_train, X_valid, y_train, y_valid = train_test_split(target_x, target_y, test_size=0.33, random_state=2018)\ntrain_loss = []\nvalid_loss = []\nimportance=[]\npreds_train = np.zeros((X_train.shape[0], len(y_train)))\npreds_valid = np.zeros((X_valid.shape[0], len(y_valid)))\nfor i, j in enumerate(TARGET_COLS):\n    print('Class:= '+j)\n    model.fit(X_train,y_train[j])\n    preds_valid[:,i] = model.predict_proba(X_valid)[:,1]\n    preds_train[:,i] = model.predict_proba(X_train)[:,1]\n    train_loss_class=log_loss(y_train[j],preds_train[:,i])\n    valid_loss_class=log_loss(y_valid[j],preds_valid[:,i])\n    print('Trainloss=log loss:', train_loss_class)\n    print('Validloss=log loss:', valid_loss_class)\n    importance.append(model.coef_)\n    train_loss.append(train_loss_class)\n    valid_loss.append(valid_loss_class)\nprint('mean column-wise log loss:Train dataset', np.mean(train_loss))\nprint('mean column-wise log loss:Validation dataset', np.mean(valid_loss))\n\nend_time=time.time()\nprint(\"total time till Indirect feat model\",end_time-start_time)","43e91fde":"importance[0][0]","87a9fa53":"plt.figure(figsize=(16,22))\nplt.suptitle(\"Feature importance for indirect features\",fontsize=20)\ngridspec.GridSpec(3,2)\nplt.subplots_adjust(hspace=0.4)\nplt.subplot2grid((3,2),(0,0))\nsns.barplot(SELECTED_COLS,importance[0][0],color=color[0])\nplt.title(\"class : Toxic\",fontsize=15)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=45)\nplt.xlabel('Feature', fontsize=12)\nplt.ylabel('Importance', fontsize=12)\n\nplt.subplot2grid((3,2),(0,1))\nsns.barplot(SELECTED_COLS,importance[1][0],color=color[1])\nplt.title(\"class : Severe toxic\",fontsize=15)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=45)\nplt.xlabel('Feature', fontsize=12)\nplt.ylabel('Importance', fontsize=12)\n\nplt.subplot2grid((3,2),(1,0))\nsns.barplot(SELECTED_COLS,importance[2][0],color=color[2])\nplt.title(\"class : Obscene\",fontsize=15)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=45)\nplt.xlabel('Feature', fontsize=12)\nplt.ylabel('Importance', fontsize=12)\n\n\n\nplt.subplot2grid((3,2),(1,1))\nsns.barplot(SELECTED_COLS,importance[3][0],color=color[3])\nplt.title(\"class : Threat\",fontsize=15)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=45)\nplt.xlabel('Feature', fontsize=12)\nplt.ylabel('Importance', fontsize=12)\n\n\nplt.subplot2grid((3,2),(2,0))\nsns.barplot(SELECTED_COLS,importance[4][0],color=color[4])\nplt.title(\"class : Insult\",fontsize=15)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=45)\nplt.xlabel('Feature', fontsize=12)\nplt.ylabel('Importance', fontsize=12)\n\n\nplt.subplot2grid((3,2),(2,1))\nsns.barplot(SELECTED_COLS,importance[5][0],color=color[5])\nplt.title(\"class : Identity hate\",fontsize=15)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=45)\nplt.xlabel('Feature', fontsize=12)\nplt.ylabel('Importance', fontsize=12)\n\n\n# plt.subplot2grid((4,2),(3,0),colspan=2)\n# sns.barplot(SELECTED_COLS,importance[6][0],color=color[0])\n# plt.title(\"class : Clean\",fontsize=15)\n# locs, labels = plt.xticks()\n# plt.setp(labels, rotation=90)\n# plt.xlabel('Feature', fontsize=12)\n# plt.ylabel('Importance', fontsize=12)\n\nplt.show()","f5c1fc1f":"from scipy.sparse import csr_matrix, hstack\n\n#Using all direct features\nprint(\"Using all features except leaky ones\")\ntarget_x = hstack((train_bigrams,train_charngrams,train_unigrams,train_feats[SELECTED_COLS])).tocsr()\n\n\nend_time=time.time()\nprint(\"total time till Sparse mat creation\",end_time-start_time)","04175b82":"model = NbSvmClassifier(C=4, dual=True, n_jobs=-1)\nX_train, X_valid, y_train, y_valid = train_test_split(target_x, target_y, test_size=0.33, random_state=2018)\ntrain_loss = []\nvalid_loss = []\npreds_train = np.zeros((X_train.shape[0], len(y_train)))\npreds_valid = np.zeros((X_valid.shape[0], len(y_valid)))\nfor i, j in enumerate(TARGET_COLS):\n    print('Class:= '+j)\n    model.fit(X_train,y_train[j])\n    preds_valid[:,i] = model.predict_proba(X_valid)[:,1]\n    preds_train[:,i] = model.predict_proba(X_train)[:,1]\n    train_loss_class=log_loss(y_train[j],preds_train[:,i])\n    valid_loss_class=log_loss(y_valid[j],preds_valid[:,i])\n    print('Trainloss=log loss:', train_loss_class)\n    print('Validloss=log loss:', valid_loss_class)\n    train_loss.append(train_loss_class)\n    valid_loss.append(valid_loss_class)\nprint('mean column-wise log loss:Train dataset', np.mean(train_loss))\nprint('mean column-wise log loss:Validation dataset', np.mean(valid_loss))\n\n\nend_time=time.time()\nprint(\"total time till NB base model creation\",end_time-start_time)","3964dce5":"## Topic modeling:\nDue to kernal limitations(kernal timeout at 3600s), I had to continue the exploration in a seperate kernal( [Understanding the \"Topic\" of toxicity](https:\/\/www.kaggle.com\/jagangupta\/understanding-the-topic-of-toxicity)) to aviod a timeout. \n# Next steps:\n* Add Glove vector features\n* Explore sentiement scores\n* Add LSTM, LGBM\n\n## To be continued","c3464864":"# Spam is toxic to the model too!\n\n<!--\nThese spam entries are bad if we design our model to contain normal word counts features.\nImagine the scenario in which our model picked up the words \"mitt romney\" from any comment and classified it as toxic :(\n-->\n\n\ub9cc\uc57d \uc6b0\ub9ac\uac00 \uc77c\ubc18\uc801\uc778 \ub2e8\uc5b4 \uc218\uc5d0 \ub300\ud55c feature\ub97c \ubaa8\ub378\uc5d0 \ud3ec\ud568\ud55c\ub2e4\uba74, \uc774\ub7f0 \uc2a4\ud338\ub4e4\uc740 \uc88b\uc9c0 \uc54a\ub2e4.  \n\uc6b0\ub9ac\uc758 \ubaa8\ub378\uc774 \uc5b4\ub5a4 \ub313\uae00\uc5d0\uc11c \"mitt romney\"\ub77c\ub294 \ub2e8\uc5b4\ub97c \uc120\ud0dd\ud558\uace0 \uadf8\uac83\uc744 toxic\uc73c\ub85c \ubd84\ub958\ud55c\ub2e4\ub294 \uc2dc\ub098\ub9ac\uc624\ub97c \uc0dd\uac01\ud574\ubcf4\uc790. :(","679b89f0":"<!--\nThe feature stability (aka) the reoccurance of train dataset usernames in the test dataset seems to be minimal. \nWe can just use the intersection (eg) the common IPs\/links for test and train in our feature engineering.\n\nAnother usecase for the list of IPs would be to find out if they are a part of the [blocked IP list](https:\/\/en.wikipedia.org\/wiki\/Wikipedia:Database_reports\/Indefinitely_blocked_IPs)\n-->\n\n(feature \uc548\uc815\uc131) test \ub370\uc774\ud130\uc14b\uc5d0\uc11c\uc758 \uc720\uc800\uba85\uc774 train \ub370\uc774\ud130\uc14b\uc5d0\uc11c\uc758 \uc7ac\ucd9c\uc5f0\uc774 \ucd5c\uc18c\uc778 \uac83 \uac19\ub2e4.\n\uc6b0\ub9ac\ub294 \ub2e8\uc21c\ud788 \uad50\uc9d1\ud569\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4. (\uc608) \uc6b0\ub9ac\uc758 feature engineering\uc5d0\uc11c\uc758 \uacf5\ud1b5\uc758 test\uc640 train\uc5d0\uc11c\uc758 IP\ub098 \ub9c1\ud06c\ub4e4\n\n\ub610 \ub2e4\ub978 IP \ubaa9\ub85d\uc744 \uc704\ud55c usecase\ub294 [blocked IP list](https:\/\/en.wikipedia.org\/wiki\/Wikipedia:Database_reports\/Indefinitely_blocked_IPs)\uc5d0 \uc788\ub294 \uc9c0 \uc5c6\ub294 \uc9c0\ub97c \uc54c\uc544\ub0b4\ub294 \uac83\uc774\ub2e4.","1c798f94":"### Word count VS unique word count:\n\n<!--\nThere are noticeable shifts in the mean of both word count and unique word count across clean and toxic comments.\n   * Chart desc: The first chart is a split violin chart. It is a variation of the traditional box chart\/violin chart which allows us to split the violin in the middle based on a categorical variable.\n-->\n\n\ub2e8\uc5b4\uc758 \uac1c\uc218\uc640 \uc720\uc77c\ud55c \ub2e8\uc5b4\uc758 \uac1c\uc218 \uc591\ucabd\uc758 \ud3c9\uade0\uc5d0\uc11c clean\uacfc toxic \ub313\uae00\uc5d0 \ub530\ub77c \ubd84\uba85\ud55c \uc774\ub3d9\uc774 \uc788\ub2e4.\n    * Chart desc: \uccab\ubc88\uc9f8 \ucc28\ud2b8\ub294 \ubd84\ub9ac\ub41c Violin chart\uc774\ub2e4. \uc774\uac83\uc740 \uc804\ud1b5\uc801\uc778 box chart\/violin chart\uc758 \ubcc0\ud615\uc73c\ub85c, categorical \ubcc0\uc218\uc5d0 \uae30\ubc18\uc73c\ub85c \uc911\uac04\uc5d0 violin\uc744 \ub098\ub204\uac8c \ud574\uc900\ub2e4.\n\n### Unique word count percent:\n\n<!--\nThere is a bulge near the 0-10% mark which indicates a large number of toxic comments which contain very little variety of words.\n   * Chart desc: The second chart is an overlay of two kernel density estimation plots of percentage of unique words out of all the words in the comment, done for both clean and toxic comments\n-->\n\n0-10% \uadfc\ucc98\uc758 \ubcfc\ub85d\ud55c \ubd80\ubd84\uc740 \uc5b4\ub5a4 \ub9e4\uc6b0 \uc801\uc740 \ub2e8\uc5b4\uc758 \ubcc0\ud654\ub97c \ud3ec\ud568\ud558\uace0 \uc788\ub294 \ub9ce\uc740 \uc218\uc758 \uc720\ub3c5\ud55c \ub2e8\uc5b4\ub4e4\uc744 \uac00\ub9ac\ud0a8\ub2e4.\n    * Chart desc: \ub450\ubc88\uc9f8 \ucc28\ud2b8\ub294 clean\uacfc toxic \ub313\uae00\uc758 \ubaa8\ub4e0 \ub2e8\uc5b4 \uc911\uc5d0\uc11c \uc720\uc77c\ud55c \ub2e8\uc5b4\uc758 \ube44\uc728\uc744 \ub450 \uac1c\uc758 \ucee4\ub110 \ubc00\ub3c4 \ucd94\uccad(kernel density estimation) plot\uc744 \uacb9\uce5c \uac83\uc774\ub2e4. \n\n<!--\nEven though the number of clean comments dominates the dataset(~90%), there are only 75 clean comments that are spam, which makes it a powerful indicator of a toxic comment.\n-->\n\n\ube44\ub85d clean \ub313\uae00\uc758 \uc218\uac00 \ub370\uc774\ud130\uc14b\uc758(\uc57d90%)\ub85c \uc810\ub839\ud588\uc9c0\ub9cc, \uc624\uc9c1 75\uac1c\uc758 clean \ub313\uae00\uc774 \uc2a4\ud338\uc774\uc5c8\uace0, toxic \ub313\uae00\uc744 \uac15\ub825\ud55c \uc9c0\uc2dc\uc790\uac00 \ub418\uc5c8\ub2e4.\n# Spammers are more toxic!\n\n<!--\nNo surprises here. Let's take a look at some clean and toxic spam messages\n-->\n\n\uc5ec\uae30\uc11c \ub180\ub77c\uc9c0\ub9d0\uace0, clean\uacfc toxic \uc2a4\ud338 \uba54\uc2dc\uc9c0\ub97c \ubcf4\uc790.","1574a8c9":"<!--\nThere is a 30:70 train: test split and the test set might change in the future too.\nLet's take a look at the class imbalance in the train set.\n-->\n\n\ud604\uc7ac train : test\uc758 \ube44\uc728\uc740 51:47\uc785\ub2c8\ub2e4. (\uc6d0\ubcf8 \ub178\ud2b8\ubd81 \uc791\uc131 \uc2dc\uae30\uc5d0\ub294 30:70 \uc774\uc5c8\ub2e4\uace0 \ud569\ub2c8\ub2e4.)  \ntrain set\uc5d0\uc11c class \ubd88\uade0\ud615\uc744 \ud655\uc778\ud574\ubd05\uc2dc\ub2e4.\n\n### Class Imbalance:","77d68675":"# Leaky features\n\n<!--\n**Caution:** Even-though including these features might help us perform better in this particular scenario, it will not make sence to add them in the final model\/general purpose model.\n\nHere we are creating our own custom count vectorizer to create count variables that match our regex condition.\n-->\n\n**\uc704\ud5d8** \ube44\ub85d \uc774\ub7f0 feature\ub4e4\uc774 \uc6b0\ub9ac\uac00 \uc774\ub7f0 \ud2b9\uc815 \uc2dc\ub098\ub9ac\uc624\uc5d0 \ub354 \uc88b\uc740 \uc131\ub2a5\uc744 \ub0b4\uac8c \ub3c4\uc640\uc904 \uc9c0 \ubab0\ub77c\ub3c4, \ucd5c\uc885 \ubaa8\ub378\uc774\ub098 \uc77c\ubc18\uc801\uc778 \ubaa9\uc801\uc758 \ubaa8\ub378\uc5d0 \uc774\ub7f0 \uac83\ub4e4\uc744 \ucd94\uac00\ud558\ub294 \uac83\uc740 \uc774\ud574\uac00 \ub418\uc9c0 \uc54a\uc744 \uac83\uc774\ub2e4.  \n\n\uc5ec\uae30\uc5d0 \uc6b0\ub9ac\ub294 \uc815\uaddc\uc2dd \uc870\uac74\uc744 \ub9e4\uce6d\ud558\uc5ec \ubcc0\uc218\uc758 \uc218\ub97c \uc0dd\uc131\ud558\ub294 \uc6b0\ub9ac \uc790\uc2e0\uc758 \uac1c\uc218 \ubca1\ud130\ud654\ub97c \ub9cc\ub4e4\uac83\uc774\ub2e4.","e460e236":"<!--\nLong sentences or more words do not seem to be a significant indicator of toxicity.\n\nChart desc: Violin plot is an alternative to the traditional box plot. The inner markings show the percentiles while the width of the \"violin\" shows the volume of comments at that level\/instance.\n-->\n\n\uae34 \ubb38\uc7a5\uc774\ub098 \ub354 \ub9ce\uc740 \ub2e8\uc5b4\uac00 \uc788\ub2e4\uace0 \ud574\uc11c \uc720\ub3c5\uc131\uc5d0 \uc0c1\ub2f9\ud55c \uc601\ud5a5\uc744 \ub07c\uce58\ub294 \uac83\uc740 \uc544\ub2cc \uac83 \uac19\ub2e4.\n\nChart desc: Violin plot\uc740 \uc804\ud1b5\uc801\uc778 box plot\uc758 \ub300\uc548\uc774\ub2e4. \ub0b4\ubd80\uc758 \ub9c8\ud0b9\uc740 \ud37c\uc13c\ud2f0\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\uace0, \"violin\"\uc758 \ub108\ube44\ub294 \uadf8 level\/instance\uc5d0\uc11c\uc758 \ub313\uae00\uc758 \uac1c\uc218\ub97c \ubcf4\uc5ec\uc900\ub2e4.\n\n    \uac00\uc6b4\ub370 \uc810\uc774 \uc911\uc559\uac12, \ub9c9\ub300\ub294 \uc0ac\ubd84\uc704\uad6c\uac04, \uc587\uc740 \uc2e4\uc120\uc740 \uc2e0\ub8b0\uad6c\uac04 95%","c4720057":"\n## Example Comments:","21ca24bb":"<!--\nOnly ~10% of the total comments have some sort of toxicity in them. There are certain comments(20) that are marked as all of the above!\n-->\n\n\uc804\uccb4 \ub313\uae00 \uc911 \uc57d 10%\uc758 \ub313\uae00\uc774 \uc5ec\ub7ec \uc885\ub958\uc758 \uc720\ub3c5\uc131\uc744 \uac00\uc9c0\uace0 \uc788\ub2e4. 31\uac1c\uc758 \ub313\uae00\uc740 \ubaa8\ub4e0 \uc885\ub958\uc5d0 \ud0dc\uadf8\ub418\uc5b4 \uc788\ub2e4.\n\n## Which tags go together?\n\n<!--\nNow let's have a look at how often the tags occur together. A good indicator of that would be a correlation plot.\n-->\n\n\uc774\uc81c \uc5bc\ub9c8\ub098 \uc790\uc8fc \uac19\uc774 \ud0dc\uadf8\uac00 \ub418\ub294 \uc9c0 \ud655\uc778\ud574\ubcf4\uc790. correlation plot\uc774 \uc88b\uc740 \ubc29\ud5a5\uc774 \ub420 \uac83 \uac19\ub2e4.","b9968a67":"<!--\nThe above plot indicates a pattern of co-occurance but Pandas's default Corr function which uses Pearson correlation does not apply here, since the variables invovled are Categorical (binary) variables.\n-->\n\n\uc704 plot\uc740 co-occuraance \ud328\ud134\uc744 \uac00\ub9ac\ud0a8\ub2e4. \uadf8\ub7ec\ub098 Pearson correlation\uc744 \uc0ac\uc6a9\ud558\ub294 Pandas\uc758 \uae30\ubcf8 Corr \ud568\uc218\ub294 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ub418\uc9c0 \uc54a\ub294\ub2e4. \uc65c\ub0d0\ud558\uba74 \ubcc0\uc218\ub4e4\uc740 Categorical (binary) \ubcc0\uc218\ub4e4\uc744 \ud3ec\ud568\ud558\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc774\ub2e4.\n\n<!--\nSo, to find a pattern between two categorical variables we can use other tools like \n* Confusion matrix\/Crosstab\n* Cramer's V Statistic\n    * Cramer's V stat is an extension of the chi-square test where the extent\/strength of association is also measured\n-->\n\n\uadf8\ub798\uc11c, \ub450 \uac1c\uc758 categorical \ubcc0\uc218 \uc0ac\uc774\uc758 \ud328\ud134\uc744 \ucc3e\uae30 \uc704\ud574 \ub2e4\uc74c\uacfc \uac19\uc740 \ub2e4\ub978 \ubc29\uc2dd\ub4e4\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4.\n* Confusion matrix\/Crosstab\n* Cramer's V Statistic\n    * Cramer's V stat\uc740 extent\/strength \uc758 \uad00\ub828\uc131\uc744 \uce21\uc815\ud558\ub294 chi-quare test\uc758 \ud655\uc7a5\uc774\ub2e4.","97381b38":"<!--\nThat was a whole lot of toxicity. Some weird observations:\n\n* Some of the comments are extremely and mere copy paste of the same thing\n* Comments can still contain IP addresses(eg:62.158.73.165), usernames(eg:ARKJEDI10) and some mystery numbers(i assume is article-IDs)\n\nPoint 2 can cause huge overfitting.\n-->\n\n\uc804\uccb4\uc758 \ub9ce\uc740 \uc720\ub3c5\uc131\uc774 \uc788\ub2e4. \uba87\uba87\uc758 \uc774\uc0c1\ud55c \uad00\uce21\uacb0\uacfc\uac00 \uc788\ub2e4:\n\n* \uba87\uba87 \ub313\uae00\uc740 \uc2ec\ud558\uac70\ub098 \ub2e8\uc21c\ud788 \uac19\uc740 \uac83\uc758 \ubcf5\uc0ac\ubcf8\uc774\ub2e4.\n* \ub313\uae00\ub4e4\uc740 \uc544\uc9c1\ub3c4 IP \uc8fc\uc18c\ub97c \uac00\uc9c0\uace0 \uc788\uac70\ub098, \uc720\uc800\uc774\ub984\uacfc \uc774\uc0c1\ud55c \uc22b\uc790\ub97c \ud3ec\ud568\ud558\uace0 \uc788\uc744 \uc218 \uc788\ub2e4.\n\n\uc774 2\uac00\uc9c0\uc810\uc740 \uc5c4\uccad\ub09c overfitting\uc744 \uc77c\uc73c\ud0ac \uc218 \uc788\ub2e4.\n\n# Wordclouds - Frequent words:\n\n<!--\nNow, let's take a look at words that are associated with these classes.\n\nChart Desc: The visuals here are word clouds (ie) more frequent words appear bigger. A cool way to create word clouds with funky pics is given [here](https:\/\/www.kaggle.com\/arthurtok\/spooky-nlp-and-topic-modelling-tutorial). It involves the following steps.  \n\n    * Search for an image and its base 64 encoding\n    * Paste encoding in a cell and convert it using codecs package to image\n    * Create word cloud with the new image as a mask\nA simpler way would be to create a new kaggle dataset and import images from there.\n-->\n\n\uc774\uc81c, \uc774 \ud074\ub798\uc2a4\ub4e4\uc5d0 \uc5f0\uad00\ub41c \ub2e8\uc5b4\ub4e4\uc744 \uc0b4\ud3b4\ubcf4\uc790.\n\n    \ucc28\ud2b8 \uc0c1\uc138: \uc5ec\uae30 \uc2dc\uac01\ud654\ub294 word clouds\uc774\ub2e4. (\uc608) \ub354 \uc790\uc8fc \ub4f1\uc7a5\ud55c \ub2e8\uc5b4\ub4e4\uc774 \ub354 \ud06c\uac8c \ub098\ud0c0\ub09c\ub2e4. \uba4b\uc9c4 \uadf8\ub9bc\uc73c\ub85c word clouds\ub97c \ub9cc\ub4e4\uae30 \uc704\ud574 \uc88b\uc740 \ubc29\uc2dd\uc740 [here](https:\/\/www.kaggle.com\/arthurtok\/spooky-nlp-and-topic-modelling-tutorial) \uc5d0 \uc8fc\uc5b4\uc9c4\ub2e4. \uc774\uac83\uc740 \ub2e4\uc74c \ub2e8\uacc4\ub4e4\uc744 \ud3ec\ud568\ud558\uace0 \uc788\ub2e4.\n    * \uc774\ubbf8\uc9c0\ub97c \ucc3e\uace0 base64 \ubc29\uc2dd\uc73c\ub85c \uc778\ucf54\ub529\ub41c \uac83\uc744 \ucc3e\ub294\ub2e4.\n    * \ud55c \uc140\uc5d0 \uc778\ucf54\ub529\uc744 \ubd99\uc774\uace0, \uadf8\uac83\uc744 \ucf54\ub371 \ud328\ud0a4\uc9c0\ub97c \ud65c\uc6a9\ud574\uc11c \uc774\ubbf8\uc9c0\ub85c \ubcc0\ud658\ud55c\ub2e4.\n    * \uc0c8 \uc774\ubbf8\uc9c0\ub97c \uac00\uc9c4 word cloud\ub97c mask\ub85c \ub9cc\ub4e0\ub2e4.\n\ub354 \ub2e8\uc21c\ud55c \ubc29\uc2dd\uc740 \uc0c8\ub85c\uc6b4 \uce90\uae00 \ub370\uc774\ud130\uc14b\uc744 \ub9cc\ub4e4\uace0 \uac70\uae30\uc5d0\uc11c \uc774\ubbf8\uc9c0\ub4e4\uc744 \ubd88\ub7ec\uc624\ub294 \uac83\uc774\ub2e4.","74e87e24":"<!--\nAn interesting but somewhat insignificant finding. There are 6 blocked IP mentions in the comments overall. \n\nAnyways, moving on to cleaning the dataset.\n-->\n\n\ud765\ubbf8\ub85c\uc6b0\ub098 \uc57d\uac04 \ubd88\ucda9\ubd84\ud55c \ubc1c\uacac\uc774\ub2e4. \uc804\uccb4 \ub313\uae00\uc5d0\uc11c 6\uac1c\uc758 \ucc28\ub2e8\ub41c IP\uac00 \uc788\uc5c8\ub2e4.\n\n\uc5b4\uca0b\ub4e0, \ub370\uc774\ud130\uc14b\uc744 \uc815\ub9ac\ud574\ubcf4\uc790.","583bb000":"<!--\nThe above table represents the Crosstab\/ consufion matix of Toxic comments with the other classes. \n\nSome interesting observations:\n\n* A Severe toxic comment is always toxic\n* Other classes seem to be a subset of toxic barring a few exceptions\n-->\n\n\uc704 \ud14c\uc774\ube14\uc740 Crosstab\/ \ud63c\ud569 matrix\ub85c \ub2e4\ub978 \ud074\ub798\uc2a4\ub4e4\uc758 \uc720\ub3c5\uc131 \ub313\uae00\uc744 \ub098\ud0c0\ub0b8 \uac83\uc774\ub2e4.\n\n\uba87\uba87\uc758 \ud765\ubbf8\ub85c\uc6b4 \uad00\ucc30\uc744 \ubcfc \uc218 \uc788\ub2e4:\n\n* Severe toxic \ub313\uae00\uc740 \ud56d\uc0c1 toxic\uc774\ub2e4.\n* \ub2e4\ub978 \ud074\ub798\uc2a4\ub4e4\uc740 \uba87\uba87\uc758 \uc608\uc678\ub97c \uc81c\uc678\ud558\uace4 toxic\uc758 subset\uc73c\ub85c \ubcf4\uc778\ub2e4.","c79d5108":"# Leaky Feature Stability:\n\n<!--\nChecking the re-occurance of leaky features to check their utility in predicting the test set. \n-->\n\n\ud14c\uc2a4\ud2b8\uc14b\uc744 \uc608\uce21\ud558\ub294\ub370 \uc720\uc6a9\uc131\uc744 \ud655\uc778\ud558\uae30 \uc704\ud574 leaky features\uc758 \uc7ac\ucd9c\ud604\uc744 \ud655\uc778\ud558\uae30\n\n[Discussion on leaky feature stability](https:\/\/www.kaggle.com\/jagangupta\/stop-the-s-toxic-comments-eda#263577)\n\n","ed0b7c45":"# Baseline Model:","a850e4ef":"# Direct features:\n\n## 1)Count based features(for unigrams):\n\n<!--\nLets create some features based on frequency distribution of the words. Initially lets consider taking words one at a time (ie) Unigrams\n\nPython's SKlearn provides 3 ways of creating count features.All three of them first create a vocabulary(dictionary) of words and then create a [sparse matrix](#https:\/\/en.wikipedia.org\/wiki\/Sparse_matrix) of word counts for the words in the sentence that are present in the dictionary. A brief description of them:\n* CountVectorizer\n    * Creates a matrix with frequency counts of each word in the text corpus\n* TF-IDF Vectorizer\n    * TF - Term Frequency -- Count of the words(Terms) in the text corpus (same of Count Vect)\n    * IDF - Inverse Document Frequency -- Penalizes words that are too frequent. We can think of this as regularization\n* HashingVectorizer\n    * Creates a hashmap(word to number mapping based on hashing technique) instead of a dictionary for vocabulary\n    * This enables it to be more scalable and faster for larger text coprus\n    * Can be parallelized across multiple threads\n        \nUsing TF-IDF here.\nNote: Using the concatenated dataframe \"merge\" which contains both text from train and test dataset to ensure that the vocabulary that we create does not missout on the words that are unique to testset.\n-->\n\n\ub2e8\uc5b4\uc758 \ube48\ub3c4\uc218 \ubd84\ud3ec\ub97c \uae30\ubc18\uc73c\ub85c \uba87\uba87\uc758 feature\ub97c \ub9cc\ub4e4\uc5b4\ubcf4\uc790. \ucd5c\ucd08\ub85c, \ud55c \ubc88\uc5d0 \ud558\ub098\uc758 \ub2e8\uc5b4\ub97c \uac00\uc838\uc624\ub294 \uac83\uc744 \uace0\ub824\ud574\ubcf4\uc790. (\uc608) Unigrams\n\nPython\uc758 SKlearn\uc740 3\uac00\uc9c0 \ubc29\uc2dd\uc758 count feature\ub97c \uc0dd\uc131\ubc95\uc744 \uc81c\uacf5\ud55c\ub2e4. \ubaa8\ub4e0 3\uac1c\uc758 \ubc29\uc2dd\uc740 \uba3c\uc800 \ub2e8\uc5b4\uc758 \uc5b4\ud718(\uc0ac\uc804)\ub97c \uc0dd\uc131\ud558\uace0, \ud604\uc7ac \uc0ac\uc804 \ub0b4\uc758 \ubb38\uc7a5\uc5d0\uc11c\uc758 \ub2e8\uc5b4\ub4e4\uc5d0 \ub300\ud55c \ub2e8\uc5b4 \uac1c\uc218\uc758 [sparse matrix](#https:\/\/en.wikipedia.org\/wiki\/Sparse_matrix) \ub97c \ub9cc\ub4e0\ub2e4. \uadf8\uac83\ub4e4\uc758 \uac04\ub7b5\ud55c \uc694\uc57d\uc740 \ub2e4\uc74c\uacfc \uac19\ub2e4:\n* CounVectorizer\n    * text \ub9d0\ubb49\uce58\uc5d0\uc11c \uac01\uac01\uc758 \ub2e8\uc5b4\uc758 \uc218\uc758 \ube48\ub3c4\uc5d0 \ub300\ud55c \ud589\ub82c\uc744 \ub9cc\ub4e0\ub2e4.\n* TF-IDF Vectorizer\n    * TF - Term Frequency -- \ud14d\uc2a4\ud2b8 \ub9d0\ubb49\uce58\uc5d0\uc11c\uc758 \ub2e8\uc5b4(\uc6a9\uc5b4)\uc758 \uac1c\uc218 (same of Count Vect)\n    * IDF - Inverse Document Frequency -- \ub108\ubb34 \uc790\uc8fc \ub4f1\uc7a5\ud558\ub294 \ub2e8\uc5b4\ub97c \ucc98\ubc8c\ud55c\ub2e4. \uc6b0\ub9ac\ub294 \uc774\uac83\uc744 \uaddc\uc81c\ub85c \uc0dd\uac01\ud560 \uc218 \uc788\ub2e4.\n* HashingVectorizer\n    * \uc5b4\ud718\uc5d0 \ub300\ud55c \uc0ac\uc804 \ub300\uc2e0\uc5d0 \ud574\uc26c\ub9f5(\ud574\uc2f1 \uae30\ubc95\uc744 \uae30\ubc18\uc73c\ub85c\ud55c \ub2e8\uc5b4\uc640 \uc22b\uc790 \ub9e4\ud551)\uc744 \ub9cc\ub4e0\ub2e4.\n    * \uc774\uac83\uc740 \ud070 \ud14d\uc2a4\ud2b8 \ub9d0\ubb49\uce58\uc5d0 \ub300\ud574 \ub354 \ud655\uc7a5\uac00\ub2a5\ud558\uace0 \ube60\ub974\uac8c \ud574\uc900\ub2e4.\n    * \ub2e4\uc591\ud55c \uc4f0\ub808\ub4dc\ub97c \ub530\ub77c \ubcd1\ub82c\ud654\uac00 \uac00\ub2a5\ud558\ub2e4.\n\n\uc5ec\uae30\uc120 TF-IDF\ub97c \uc0ac\uc6a9\ud55c\ub2e4.  \nNote: \uc6b0\ub9ac\uac00 \uc0dd\uc131\ud55c \uc5b4\ud718\uac00 testset\uc5d0 \uc788\ub294 \uc720\uc77c\ud55c \ub2e8\uc5b4\ub4e4\uc744 \ub204\ub77d\uc2dc\ud0a4\uc9c0 \uc54a\uae30 \uc704\ud574, train\uacfc test \ub370\uc774\ud130\uc14b \ubaa8\ub450\ub97c \ud3ec\ud568\ud558\ub294 \ud569\uccd0\uc9c4 dataframe\uc778 \"merge\"\uc744 \uc0ac\uc6a9\ud55c\ub2e4.","85cb02e3":"# Corpus cleaning:\n\n<!--\nIts important to use a clean dataset before creating count features. \n-->\n\ncount features\ub97c \ub9cc\ub4e4\uae30 \uc804\uc5d0 \uae68\ub057\ud55c \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\ub294 \uac8c \uc911\uc694\ud558\ub2e4.","6ea538cf":"<!--\n* The toxicity is not evenly spread out across classes. Hence we might face class imbalance problems\n* There are ~95k comments in the training dataset and there are ~21 k tags and ~86k clean comments!?\n    * This is only possible when multiple tags are associated with each comment (eg) a comment can be classified as both toxic and obscene. \n-->\n\n* \uc720\ub3c5\uc131\uc740 \uacb0\uad6d classes\ub97c \ub530\ub77c \ud37c\uc838\uc788\uc9c0 \uc54a\ub2e4. class \ubd88\uade0\ud615 \ubb38\uc81c\uc5d0 \uc9c1\uba74\ud574\uc788\ub2e4.\n* \uc57d 15\ub9cc \uac1c\uc758 \ub313\uae00\uc774 training dataset\uc5d0 \uc788\uace0, \uc57d 3\ub9cc\uac1c\uc758 \ub313\uae00\uc774 tag\ub418\uc5b4 \uc788\uace0, 14\ub9cc\uac1c\uc758 \ub313\uae00\uc774 clean\ud55c \ub313\uae00\uc774\ub2e4!?\n    * \uc774\uac83\uc740 \uac01 \ub313\uae00\uc5d0 \uc5ec\ub7ec\uac1c\uac00 \ud0dc\uae45\ub418\uc5c8\uc744 \ub54c\ub9cc \uac00\ub2a5\ud558\ub2e4. (\uc608) \ud55c \ub313\uae00 toxic\uacfc obscene \ub458 \ub2e4 \ubd84\ub958\ub420 \uc218 \uc788\ub2e4.\n\n### Multi-tagging:\n\n<!--\nLet's check how many comments have multiple tags.\n-->\n\n\uc5bc\ub9c8\ub098 \ub9ce\uc740 \ub313\uae00\ub4e4\uc774 \uc5ec\ub7ec\uac1c\uc758 \ud0dc\uadf8\ub97c \uac00\uc9c0\uace0 \uc788\ub294 \uc9c0 \ud655\uc778\ud574\ubcf4\uc790.","6f8e3cd0":"# Feature engineering:\n\n<!--\nI've broadly classified my feature engineering ideas into the following three groups\n-->\n\n\uc800\ub294 feature engineering \uc544\uc774\ub514\uc5b4\ub97c \ub2e4\uc74c\uc758 3\uac1c\uc758 \uadf8\ub8f9\uc73c\ub85c \ub113\uac8c \ubd84\ub958\ud588\uc2b5\ub2c8\ub2e4.\n## Direct features:\n\n<!--\nFeatures which are a directly due to words\/content.We would be exploring the following techniques\n* Word frequency features\n    * Count features\n    * Bigrams\n    * Trigrams\n* Vector distance mapping of words (Eg: Word2Vec)\n* Sentiment scores\n-->\n\n\ub2e8\uc5b4\/\ub0b4\uc6a9\uc5d0 \ub300\ud574 \uc9c1\uc811\uc801\uc778 feature, \uc6b0\ub9ac\ub294 \ub2e4\uc74c\uc758 \uae30\ubc95\ub4e4\uc744 \ud0d0\uc0c9\ud560 \uc218 \uc788\ub2e4.\n* \uc790\uc8fc \ub098\uc624\ub294 \ub2e8\uc5b4 feature\n    * Count feature\n    * Bigrams\n    * Trigrams\n* Vector distance mapping of words (\uc608: Word2Vec)\n* Sentiment scores\n\n## Indirect features:\n\n<!--\nSome more experimental features.\n* count of sentences \n* count of words\n* count of unique words\n* count of letters \n* count of punctuations\n* count of uppercase words\/letters\n* count of stop words\n* Avg length of each word\n-->\n\n\uba87\uba87\uc758 \ub354 \uc2e4\ud5d8\uc801\uc778 features.\n* \ubb38\uc7a5\uc758 \uc218\n* \ub2e8\uc5b4\uc758 \uc218\n* \uc720\uc77c\ud55c \ub2e8\uc5b4\uc758 \uc218\n* \ubb38\uc790\uc758 \uc218\n* \uad6c\ub450\uc810\uc758 \uc218\n* \ub300\ubb38\uc790\uc758 \ub2e8\uc5b4\/\ubb38\uc790\uc758 \uc218\n* stop word\uc758 \uc218\n* \uac01 \ub2e8\uc5b4\uc758 \ud3c9\uade0 \uae38\uc774\n\n## Leaky features:\n\n<!--\nFrom the example, we know that the comments contain identifier information (eg: IP, username,etc.).\nWe can create features out of them but, it will certainly lead to **overfitting** to this specific Wikipedia use-case.\n* toxic IP scores\n* toxic users\n-->\n\n\uc608\ub97c \ubcf4\uba74, \uc6b0\ub9ac\ub294 \ub313\uae00\uc740 \uac1c\uc778 \uc815\ubcf4\ub97c \uc54c \uc218 \uc788\ub2e4. (\uc608, IP, \uc774\ub984 \ub4f1)  \n\uc6b0\ub9ac\ub294 \uadf8\uac83\ub4e4 \uc911\uc5d0\uc11c feature\ub97c \ub9cc\ub4e4 \uc218 \uc788\uc5c8\ub2e4. \uadf8\ub7ec\ub098 \uadf8\uac83\uc740 \ubd84\uba85\ud788 \uc774 \uad6c\uccb4\uc801\uc778 \uc704\ud0a4\ud53c\ub514\uc544 use-case\uc5d0 **overfitting**\uc5d0 \uc774\ub04c \uac83\uc774\ub2e4.\n* \uc720\ub3c5\ud55c IP \uc810\uc218\n* \uc720\ub3c5\ud55c \uc720\uc800\ub4e4\n\n**Note:** \n\n<!--\nCreating the indirect and leaky features first. There are two reasons for this,\n* Count features(Direct features) are useful only if they are created from a clean corpus\n* Also the indirect features help compensate for the loss of information when cleaning the dataset\n-->\n\n\uac04\uc811\uc801\uc774\uace0 \uc704\ud5d8\ud55c feature\ub97c \uba3c\uc800 \ub9cc\ub4e0\ub2e4. \uc774\uac83\uc744 \ud558\ub294 \uac83\uc740 2\uac00\uc9c0\uc758 \uc774\uc720\uac00 \uc788\ub2e4.\n* fature\uc758 \uc218(\uc9c1\uc811\uc801\uc778 features)\ub294 \uc624\uc9c1 \uadf8\uac83\ub4e4\uc774 clean \ub9d0\ubb49\uce58\uc5d0\uc11c \uc0dd\uc131\ub418\uc5c8\ub2e4\uba74 \uc720\uc6a9\ud55c\ub2e4.\n* \ub610\ud55c \uac04\uc811\uc801\uc778 feature\ub294 \ub370\uc774\ud130\uc14b\uc77c \uc5c6\uc5b4\uc9c8 \ub54c \uc815\ubcf4\uc758 \uc190\uc2e4\uc744 \ubcf4\uc0c1\ud558\ub294 \ub370 \ub3c4\uc640\uc900\ub2e4.","659562c1":"# Update: \n<!--The kernal has been updated for the new test and train datasets.-->\n\ucee4\ub110\uc740 \uc0c8\ub85c\uc6b4 test \ubc0f train datasets\uc73c\ub85c \uc5c5\ub370\uc774\ud2b8\ub418\uc5c8\ub2e4.","65b1e743":"# Introduction:\n\n<!--\nBeing anonymous over the internet can sometimes make people say nasty things that they normally would not in real life.\nLet's filter out the hate from our platforms one comment at a time. \n-->\n\n\uc778\ud130\ub137\uc744 \ud1b5\ud574 \uc775\uba85\uc758 \uc0ac\ub78c\ub4e4\uc740 \ub54c\ub54c\ub85c \uc0ac\ub78c\ub4e4\uc774 \ubcf4\ud1b5 \uc2e4\uc81c \uc0b6\uc5d0\uc11c\ub294 \uc5c6\uc744 \ub054\ucc0d\ud55c \uac83\uc744 \ub9d0\ud558\uac8c \ud55c\ub2e4.  \n\uc774\ubc88\uc5d0 \uc6b0\ub9ac\uc758 \ud50c\ub7ab\ud3fc\uc758 \ud55c \ub313\uae00\uc5d0\uc11c \uc99d\uc624\ub97c \uc5c6\uc560\ubcf4\uc790.  \n\n## Objective:\n\n<!--\nTo create an EDA\/ feature-engineering starter notebook for toxic comment classification.\n-->\n\ntoxic comment classification\uc744 \uc704\ud55c EDA\/ feature-enginerring \uae30\ubcf8 \ub178\ud2b8\ubd81\uc744 \ub9cc\ub4e4\uc5b4\ubcf4\uae30\n\n## Data Overview:\n\n<!--\nThe dataset here is from wiki corpus dataset which was rated by human raters for toxicity.\nThe corpus contains 63M comments from discussions relating to user pages and articles dating from 2004-2015. \n-->\n\n\uc774 \ub370\uc774\ud130\uc14b\uc740 \uc720\ub3c5\uc131\uc5d0 \ub300\ud55c \ud3c9\uac00\uc790\ub4e4\uc5d0 \uc758\ud574 \ud3c9\uac00\ub41c \uc704\ud0a4 \ub9d0\ubb49\uce58 \ub370\uc774\ud130\uc14b\uc774\ub2e4.  \n\ub9d0\ubb49\uce58\ub294 2004-2015 \ub3d9\uc548 user page\uc640 \uae30\uc0ac\uc5d0 \uad00\ub828\ub41c \ud1a0\ub860\uc5d0\uc11c\uc758 63\ubc31\ub9cc \ub313\uae00\uc744 \ud3ec\ud568\ud55c\ub2e4.  \n\n<!--\nDifferent platforms\/sites can have different standards for their toxic screening process. Hence the comments are tagged in the following five categories\n* toxic\n* severe_toxic\n* obscene\n* threat\n* insult\n* identity_hate\n-->\n\n\ub2e4\ub978 \ud50c\ub7ab\ud3fc\uc774\ub098 \uc0ac\uc774\ud2b8\ub294 \uadf8\ub4e4\uc758 \uc720\ub3c5\uc131 \ucc28\ub2e8 \ucc98\ub9ac\ub97c \ub2e4\ub978 \ud45c\uc900\uc744 \uac00\uc9c0\uace0 \uc788\ub2e4. \ub313\uae00\ub4e4\uc740 \ub2e4\uc74c 5\uac1c\uc758 \uce74\ud14c\uace0\ub9ac\uc5d0 \ud0dc\uadf8\ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n* toxic\n* server_toxic\n* obscene\n* threat\n* insult\n* identity_hate\n\n<!--\nThe tagging was done via **crowdsourcing** which means that the dataset was rated by different people and the tagging might not be 100% accurate too. The same  is being discussed [here](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/discussion\/46131).\n-->\n\n\ud0dc\uae45\uc740 \ub2e4\ub978 \uc0ac\ub78c\ub4e4\uc5d0 \uc758\ud574 \ud3c9\uac00\ub418\uace0 100% \uc815\ud655\ud558\uc9c0 \uc54a\uc744 \uc218 \uc788\ub294 \uac83\uc744 \uc758\ubbf8\ud558\ub294 **\ud06c\ub77c\uc6b0\ub4dc\uc18c\uc2f1** \uc73c\ub85c \ud589\ud574\uc9c4\ub2e4. \uac19\uc740 \uac71\uc815\uc774 \ud1a0\ub860\ub418\uace0 \uc788\ub2e4. [here](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/discussion\/46131)\n\n<!--\nThe [source paper](https:\/\/arxiv.org\/pdf\/1610.08914.pdf) also contains more interesting details about the dataset creation.\n-->\n\n\uc774 [source paper](https:\/\/arxiv.org\/pdf\/1610.08914.pdf) \ub610\ud55c \ub370\uc774\ud130\uc14b \uc0dd\uc131\uc5d0 \ub300\ud55c \ub354 \ud765\ubbf8\ub85c\uc6b4 \ub514\ud14c\uc77c\uc744 \ud3ec\ud568\ud558\uace0 \uc788\ub2e4.\n\n## Note:\n\n<!--\nA [New test dataset](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/discussion\/46177) is being created by the organizers as the test set labels are present [here](https:\/\/figshare.com\/articles\/Wikipedia_Talk_Labels_Toxicity\/4563973).\n\nThe kernal has been updated for the new data.\n-->\n\n\uc774 [New test dataset](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/discussion\/46177) \uc740 \uc8fc\uad00\uc790\ub4e4\uc5d0 \uc758\ud574 \uc0dd\uc131\ub41c\ub2e4. \ud14c\uc2a4\ud2b8\uc14b \ub77c\ubca8\uc740 \ud604\uc7ac [here](https:\/\/figshare.com\/articles\/Wikipedia_Talk_Labels_Toxicity\/4563973) \ub2e4.\n\n<!--\nThe kernal has been updated for the new data.\n-->\n\n\uc774 \ucee4\ub110\uc740 \uc0c8\ub85c\uc6b4 \ub370\uc774\ud130\ub97c \uc704\ud574 \uc5c5\ub370\uc774\ud2b8\ub418\uc5c8\ub2e4."}}