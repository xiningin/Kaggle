{"cell_type":{"3205758a":"code","78b2097c":"code","00af8a26":"code","0e6508a1":"code","219ad7bc":"code","cad47699":"code","de36d668":"code","ca6f4a18":"code","74776369":"code","df6b8b92":"code","e17216a5":"code","cc44f236":"code","8e531c08":"code","85b9297a":"code","f154c0cd":"code","2811d743":"code","9c0a2a60":"code","36e12375":"code","6069d0ef":"code","f8b877af":"code","22dbdcb4":"code","f92f5d7d":"code","75579baa":"code","2486b9dc":"code","49c72745":"code","50b1fb87":"markdown","4b4bbb16":"markdown","dfe68123":"markdown","ddf59684":"markdown","ca7549b9":"markdown","f102888f":"markdown","fb1ff73d":"markdown","76bfa108":"markdown","176c0053":"markdown","e82203ec":"markdown","97b492c2":"markdown","7cfe4b7b":"markdown","c9dbec9c":"markdown","ffb82150":"markdown","e6514e2b":"markdown","63aa5bef":"markdown","70dfc6e8":"markdown","359356e5":"markdown","175df759":"markdown","61d13337":"markdown","5bae0d6e":"markdown","9a04f84b":"markdown","f07d0714":"markdown","5a083c2e":"markdown","3737eb60":"markdown","0a4eda62":"markdown","dd3a8ef9":"markdown","e431a710":"markdown","78818c63":"markdown","6f1cad4d":"markdown","9c7c8f85":"markdown","5aa96e1f":"markdown","4f7ea498":"markdown","eec06b16":"markdown","f0fd0ecc":"markdown","bb05f720":"markdown","f871f18f":"markdown","9178551a":"markdown","14754e31":"markdown","a802b680":"markdown","3663aedd":"markdown"},"source":{"3205758a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler , LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import mode\n\n\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras.utils import to_categorical\n\nfrom matplotlib import ticker\nimport time\nimport warnings\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\nwarnings.filterwarnings('ignore')","78b2097c":"train = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\")\n\n\ntrain.drop([\"Id\"] , axis = 1 , inplace = True)\ntest.drop([\"Id\"] , axis = 1 , inplace = True)\nTARGET = 'Cover_Type'\nFEATURES = [col for col in train.columns if col not in ['id', TARGET]]\nRANDOM_STATE = 12 ","00af8a26":"train.head()","0e6508a1":"print(f'Number of rows in train data: {train.shape[0]}')\nprint(f'Number of columns in train data: {train.shape[1]}')\nprint(f'No of missing values in train data: {sum(train.isna().sum())}')","219ad7bc":"train.describe()","cad47699":"test.head()","de36d668":"print(f'Number of rows in test data: {test.shape[0]}')\nprint(f'Number of columns in test data: {test.shape[1]}')\nprint(f'No of missing values in test data: {sum(test.isna().sum())}')","ca6f4a18":"test.describe()","74776369":"submission.head()","df6b8b92":"train.iloc[:, :-1].describe().T.sort_values(by='std' , ascending = False)\\\n                     .style.background_gradient(cmap='GnBu')\\\n                     .bar(subset=[\"max\"], color='#BB0000')\\\n                     .bar(subset=[\"mean\",], color='green')","e17216a5":"df = pd.concat([train[FEATURES], test[FEATURES]], axis=0)\n\ncat_features = [col for col in FEATURES if df[col].nunique() < 25]\ncont_features = [col for col in FEATURES if df[col].nunique() >= 25]\n\ndel df\nprint(f'Total number of features: {len(FEATURES)}')\nprint(f'Number of categorical features: {len(cat_features)}')\nprint(f'Number of continuos features: {len(cont_features)}')\n\nplt.pie([len(cat_features), len(cont_features)], \n        labels=['Categorical', 'Continuos'],\n        colors=['#76D7C4', '#F5B7B1'],\n        textprops={'fontsize': 13},\n        autopct='%1.1f%%')\nplt.show()","cc44f236":"ncols = 5\nnrows = int(len(cont_features) \/ ncols + (len(FEATURES) % ncols > 0))-1\n\nfig, axes = plt.subplots(nrows, ncols, figsize=(18, 8), facecolor='#EAEAF2')\n\nfor r in range(nrows):\n    for c in range(ncols):\n        col = cont_features[r*ncols+c]\n        sns.kdeplot(x=train[col], ax=axes[r, c], color='#58D68D', label='Train data')\n        sns.kdeplot(x=test[col], ax=axes[r, c], color='#DE3163', label='Test data')\n        axes[r, c].set_ylabel('')\n        axes[r, c].set_xlabel(col, fontsize=8, fontweight='bold')\n        axes[r, c].tick_params(labelsize=5, width=0.5)\n        axes[r, c].xaxis.offsetText.set_fontsize(4)\n        axes[r, c].yaxis.offsetText.set_fontsize(4)\nplt.show()","8e531c08":"if len(cat_features) == 0 :\n    print(\"No Categorical features\")\nelse:\n    ncols = 5\n    nrows = int(len(cat_features) \/ ncols + (len(FEATURES) % ncols > 0)) \n\n    fig, axes = plt.subplots(nrows, ncols, figsize=(18, 45), facecolor='#EAEAF2')\n\n    for r in range(nrows):\n        for c in range(ncols):\n            if r*ncols+c >= len(cat_features):\n                break\n            col = cat_features[r*ncols+c]\n            sns.countplot(x=train[col], ax=axes[r, c], color='#58D68D', label='Train data')\n            sns.countplot(x=test[col], ax=axes[r, c], color='#DE3163', label='Test data')\n            axes[r, c].set_ylabel('')\n            axes[r, c].set_xlabel(col, fontsize=8, fontweight='bold')\n            axes[r, c].tick_params(labelsize=5, width=0.5)\n            axes[r, c].xaxis.offsetText.set_fontsize(4)\n            axes[r, c].yaxis.offsetText.set_fontsize(4)\n    plt.show()","85b9297a":"target_df = pd.DataFrame(train[TARGET].value_counts()).reset_index()\ntarget_df.columns = [TARGET, 'count']\nfig = px.bar(data_frame =target_df, \n             x = 'Cover_Type',\n             y = 'count' , \n             color = \"count\",\n             color_continuous_scale=\"Emrld\") \nfig.show()\ntarget_df.sort_values(by =TARGET , ignore_index = True)","f154c0cd":"train = train.drop(index = int(np.where(train[\"Cover_Type\"] == 5 )[0]))\ntrain = train.drop(labels = [\"Soil_Type7\" , \"Soil_Type15\"] ,axis = 1)\nFEATURES.remove('Soil_Type7')\nFEATURES.remove('Soil_Type15')","2811d743":"train[\"mean\"] = train[FEATURES].mean(axis=1)\ntrain[\"std\"] = train[FEATURES].std(axis=1)\ntrain[\"min\"] = train[FEATURES].min(axis=1)\ntrain[\"max\"] = train[FEATURES].max(axis=1)\n\ntest[\"mean\"] = test[FEATURES].mean(axis=1)\ntest[\"std\"] = test[FEATURES].std(axis=1)\ntest[\"min\"] = test[FEATURES].min(axis=1)\ntest[\"max\"] = test[FEATURES].max(axis=1)\n\nFEATURES.extend(['mean', 'std', 'min', 'max'])","9c0a2a60":"scaler = StandardScaler()\nfor col in FEATURES:\n    train[col] = scaler.fit_transform(train[col].to_numpy().reshape(-1,1))\n    test[col] = scaler.transform(test[col].to_numpy().reshape(-1,1))\n    \nX = train[FEATURES].to_numpy().astype(np.float32)\ny = train[TARGET].to_numpy().astype(np.float32)\nX_test = test[FEATURES].to_numpy().astype(np.float32)\n\ndel train, test","36e12375":"lgb_params = {\n    'objective' : 'multiclass',\n    'metric' : 'multi_logloss',\n    'device' : 'gpu',\n}\n\n\nlgb_predictions = []\nlgb_scores = []\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X = X, y = y)):\n\n    print(10*\"=\", f\"Fold={fold+1}\", 10*\"=\")\n    start_time = time.time()\n    x_train = X[train_idx, :]\n    x_valid = X[valid_idx, :]\n    y_train = y[train_idx]\n    y_valid = y[valid_idx]\n    \n    model = LGBMClassifier(**lgb_params)\n    model.fit(x_train, y_train,\n          early_stopping_rounds=200,\n          eval_set=[(x_valid, y_valid)],\n          verbose=0)\n    \n    preds_valid = model.predict(x_valid)\n    acc = accuracy_score(y_valid,  preds_valid)\n    lgb_scores.append(acc)\n    run_time = time.time() - start_time\n    print(f\"Fold={fold+1}, acc: {acc:.8f}, Run Time: {run_time:.2f}\")\n    test_preds = model.predict(X_test)\n    lgb_predictions.append(test_preds)\n    \nprint(\"Mean Accuracy :\", np.mean(lgb_scores))","6069d0ef":"catb_params = {\n    \"objective\": \"MultiClass\",\n    \"task_type\": \"GPU\",\n}\n\ncatb_predictions = []\ncatb_scores = []\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X = X, y = y)):\n\n    print(10*\"=\", f\"Fold={fold+1}\", 10*\"=\")\n    start_time = time.time()\n    x_train = X[train_idx, :]\n    x_valid = X[valid_idx, :]\n    y_train = y[train_idx]\n    y_valid = y[valid_idx]\n    \n    model = CatBoostClassifier(**catb_params)\n    model.fit(x_train, y_train,\n          early_stopping_rounds=200,\n          eval_set=[(x_valid, y_valid)],\n          verbose=0)\n    \n    preds_valid = model.predict(x_valid)\n    acc = accuracy_score(y_valid,  preds_valid)\n    catb_scores.append(acc)\n    run_time = time.time() - start_time\n    print(f\"Fold={fold+1}, acc: {acc:.8f}, Run Time: {run_time:.2f}\")\n    test_preds = model.predict(X_test)\n    catb_predictions.append(test_preds)\n    \nprint(\"Mean Accuracy:\", np.mean(catb_scores))","f8b877af":"xgb_params = {\n    'objective': 'multi:softmax',\n    'eval_metric': 'mlogloss',\n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor',\n    }\n\nxgb_predictions = []\nxgb_scores = []\n\nxgb_predictions = []\nxgb_scores = []\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X = X, y = y)):\n\n    print(10*\"=\", f\"Fold={fold+1}\", 10*\"=\")\n    start_time = time.time()\n    x_train = X[train_idx, :]\n    x_valid = X[valid_idx, :]\n    y_train = y[train_idx]\n    y_valid = y[valid_idx]\n    \n    model = XGBClassifier(**xgb_params)\n    model.fit(x_train, y_train,\n          early_stopping_rounds=200,\n          eval_set=[(x_valid, y_valid)],\n          verbose=0)\n    preds_valid = model.predict(x_valid)\n    acc = accuracy_score(y_valid,  preds_valid)\n    xgb_scores.append(acc)\n    run_time = time.time() - start_time\n    print(f\"Fold={fold+1}, acc: {acc:.8f}, Run Time: {run_time:.2f}\")\n    test_preds = model.predict(X_test)\n    xgb_predictions.append(test_preds)\n    \nprint(\"Mean Accuracy:\", np.mean(xgb_scores))","22dbdcb4":"LEARNING_RATE = 0.0001\nBATCH_SIZE = 2048\nEPOCHS = 100\nVALIDATION_RATIO = 0.05\n\nLE = LabelEncoder()\ny = to_categorical(LE.fit_transform(y))\nX_train , X_valid ,y_train ,y_valid  = train_test_split(X,y , test_size = VALIDATION_RATIO , random_state=RANDOM_STATE)\n\n\ndef load_model(): \n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(2048, activation = 'swish', input_shape = [X.shape[1]]),\n        tf.keras.layers.Dense(1024, activation ='swish'),\n        tf.keras.layers.Dense(512, activation ='swish'),\n        tf.keras.layers.Dense(6, activation='softmax'),\n    ])\n    model.compile(\n        optimizer= tf.keras.optimizers.Adam(learning_rate = LEARNING_RATE),\n        loss='categorical_crossentropy',\n        metrics=['acc'],\n    )\n    return model\n    \n    \nearly_stopping = callbacks.EarlyStopping(\n        patience=10,\n        min_delta=0,\n        monitor='val_loss',\n        restore_best_weights=True,\n        verbose=0,\n        mode='min', \n        baseline=None,\n    )\nplateau = callbacks.ReduceLROnPlateau(\n            monitor='val_loss', \n            factor=0.2, \n            patience=4, \n            verbose=0,\n            mode='min')\n\nnn_model = load_model()\nhistory = nn_model.fit(  X_train , y_train,\n                validation_data = (X_valid , y_valid),\n                batch_size = BATCH_SIZE, \n                epochs = EPOCHS,\n                callbacks = [early_stopping , plateau],\n              )\nnn_preds = nn_model.predict(X_test , batch_size=BATCH_SIZE)","f92f5d7d":"lgb_submission = submission.copy()\nlgb_submission['Cover_Type'] = np.squeeze(mode(np.column_stack(lgb_predictions),axis = 1)[0]).astype('int')\nlgb_submission.to_csv(\"lgb-subs.csv\",index=None)\nlgb_submission.head()","75579baa":"catb_submission = submission.copy()\ncatb_submission['Cover_Type'] = np.squeeze(mode(np.column_stack(catb_predictions),axis = 1)[0]).astype('int')\ncatb_submission.to_csv(\"catb-subs.csv\",index=None)\ncatb_submission.head()","2486b9dc":"xgb_submission = submission.copy()\nxgb_submission['Cover_Type'] = np.squeeze(mode(np.column_stack(xgb_predictions),axis = 1)[0]).astype('int')\nxgb_submission.to_csv(\"xgb-subs.csv\",index=None)\nxgb_submission.head()","49c72745":"nn_submission = submission.copy()\nnn_submission[\"Cover_Type\"] = LE.inverse_transform(np.argmax((nn_preds), axis=1)).astype(int)\nnn_submission.to_csv(\"nn-sub.csv\" , index= False)\nnn_submission.head()","50b1fb87":"<a id=\"5\"><\/a>\n#  Feature Engineering","4b4bbb16":"<a id=\"4\"><\/a>\n# EDA","dfe68123":"### Neural Network Submission","ddf59684":"### XGBoost Classifier Submission","ca7549b9":"### Quick view of Test Data","f102888f":"<a id=\"4.4\"><\/a>\n## Feature Distribution of Categorical Features","fb1ff73d":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","76bfa108":"<a id=\"6.2\"><\/a>\n## Catboost Classifier","176c0053":"<a id=\"6\"><\/a>\n#  Modelling","e82203ec":"**The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. This dataset is based off of the original Forest Cover Type Prediction competition.**\n\n**Submissions are evaluated on multi-class classification accuracy.**","97b492c2":"<a id=\"3.2\"><\/a>\n## Exploring Test Data","7cfe4b7b":"<a id=\"4.2\"><\/a>\n## Continuos and Categorical Data Distribution","c9dbec9c":"**```Soil_type7``` and ```Soil_Type15``` are all zero values**","ffb82150":"<a id=\"3.3\"><\/a>\n## Submission File","e6514e2b":"<a id=\"4.1\"><\/a>\n## Overview of Data","63aa5bef":"# Table of Contents\n<a id=\"toc\"><\/a>\n- [1. Introduction](#1)\n- [2. Imports](#2)\n- [3. Data Loading and Preperation](#3)\n    - [3.1 Exploring Train Data](#3.1)\n    - [3.2 Exploring Test Data](#3.2)\n    - [3.3 Submission File](#3.3)\n- [4. EDA](#4)\n    - [4.1 Overview of Data](#4.1)\n    - [4.1 Continuos and Categorical Data Distribution](#4.1)\n    - [4.2 Feature Distribution of Continous Features](#4.2)\n    - [4.3 Feature Distribution of Categorical Features](#4.3)\n    - [4.4 Target Distribution ](#4.4)\n- [5. Feature Engineering](#5)\n- [6. Modelling](#6)\n    - [6.1 LGBM Classifier](#6.1)\n    - [6.2 Catboost Classifier](#6.2)\n    - [6.3 XGBoost Classifier](#6.3)\n    - [6.4 Neural Network](#6.4)\n- [7. Submission](#7)","70dfc6e8":"### Basic statistics of test data","359356e5":"**There are total 7 different output classes**","175df759":"### Basic statistics of training data","61d13337":"<a id=\"4.5\"><\/a>\n## Target Distribution","5bae0d6e":"<a id=\"1\"><\/a>\n# Introduction","9a04f84b":"# <center> [TPS-DEC] \ud83d\udccaEDA + MODELLING\ud83d\udd25<\/center>\n## <center>If you find this notebook useful, support with an upvote\ud83d\udc4d<\/center>","f07d0714":"<a id=\"6.3\"><\/a>\n## XGBoost Classifier","5a083c2e":"### Quick view of Train Data","3737eb60":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","0a4eda62":"<a id=\"3\"><\/a>\n# Data Loading and Preperation","dd3a8ef9":"### LGBM Classifier Submission","e431a710":"<a id=\"6.1\"><\/a>\n## LGBM Classifier","78818c63":"### Catboost Classifier Submission","6f1cad4d":"<a id=\"2\"><\/a>\n# Imports","9c7c8f85":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","5aa96e1f":"**Created by Sanskar Hasija**\n\n**[TPS-DEC] \ud83d\udccaEDA + Modelling\ud83d\udd25**\n\n**1 DECEMBER 2021**\n","4f7ea498":"<a id=\"3.1\"><\/a>\n## Exploring Train Data","eec06b16":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","f0fd0ecc":"<a id=\"6.4\"><\/a>\n## Neural Network","bb05f720":"<a id=\"7\"><\/a>\n#  Submission","f871f18f":"### Removing Unwanted Rows and columns","9178551a":"<a id=\"4.3\"><\/a>\n## Feature Distribution of Continous Features","14754e31":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","a802b680":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","3663aedd":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>"}}