{"cell_type":{"0a7d7d23":"code","85747cf9":"code","52d62d2e":"code","c387f845":"code","57ddd61d":"code","94302320":"code","0086c4c9":"code","e19b0e32":"code","f3e4b8d4":"markdown","56034f2f":"markdown","f8fbdec9":"markdown","96d563ab":"markdown","f7c5a204":"markdown","8592f63c":"markdown","573d9f9c":"markdown","e2464700":"markdown"},"source":{"0a7d7d23":"import numpy as np, pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis","85747cf9":"print('Loading Train')\ntrain = pd.read_csv('..\/input\/train.csv')\nprint('Loading Test')\ntest = pd.read_csv('..\/input\/test.csv')\nprint('Finish')","52d62d2e":"oof = np.zeros(len(train))\npreds = np.zeros(len(test))\n\noof_QDA = np.zeros(len(train))\npreds_QDA = np.zeros(len(test))\n\ncols = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\n\nfor i in range(512):\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n    \n    # Adding quadratic polynomial features can help linear model such as Logistic Regression learn better\n    poly = PolynomialFeatures(degree=2)\n    sc = StandardScaler()\n    data = pd.concat([pd.DataFrame(train2[cols]), pd.DataFrame(test2[cols])])\n    data2 = sc.fit_transform(poly.fit_transform(VarianceThreshold(threshold=1.5).fit_transform(data[cols])))\n    train3 = data2[:train2.shape[0]]; test3 = data2[train2.shape[0]:]\n    \n    data = pd.concat([pd.DataFrame(train2[cols]), pd.DataFrame(test2[cols])])\n    data2 = VarianceThreshold(threshold=1.5).fit_transform(data[cols])\n    train4 = data2[:train2.shape[0]]; test4 = data2[train2.shape[0]:]\n    \n    # STRATIFIED K FOLD\n    skf = StratifiedKFold(n_splits=11, random_state=42)\n    for train_index, test_index in skf.split(train2, train2['target']):\n\n        clf = LogisticRegression(solver='saga',penalty='l2',C=0.01,tol=0.001)\n        clf.fit(train3[train_index,:],train2.loc[train_index]['target'])\n        oof[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n        preds[idx2] += clf.predict_proba(test3)[:,1] \/ skf.n_splits\n        \n        clf_QDA = QuadraticDiscriminantAnalysis(reg_param=0.5)\n        clf_QDA.fit(train4[train_index,:],train2.loc[train_index]['target'])\n        oof_QDA[idx1[test_index]] = clf_QDA.predict_proba(train4[test_index,:])[:,1]\n        preds_QDA[idx2] += clf_QDA.predict_proba(test4)[:,1] \/ skf.n_splits\n        \n    if i%64==0:\n        print(i, 'LR oof auc : ', round(roc_auc_score(train['target'][idx1], oof[idx1]), 5))\n        print(i, 'QDA oof auc : ', round(roc_auc_score(train['target'][idx1], oof_QDA[idx1]), 5))","c387f845":"# INITIALIZE VARIABLES\ntest['target'] = preds\noof = np.zeros(len(train))\npreds = np.zeros(len(test))\n\n# BUILD 512 SEPARATE MODELS\nfor k in range(512):\n    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==k] \n    train2p = train2.copy(); idx1 = train2.index \n    test2 = test[test['wheezy-copper-turtle-magic']==k]\n    \n    # ADD PSEUDO LABELED DATA\n    test2p = test2[ (test2['target']<=0.01) | (test2['target']>=0.99) ].copy()\n    test2p.loc[ test2p['target']>=0.5, 'target' ] = 1\n    test2p.loc[ test2p['target']<0.5, 'target' ] = 0 \n    train2p = pd.concat([train2p,test2p],axis=0)\n    train2p.reset_index(drop=True,inplace=True)\n    \n     # FEATURE SELECTION AND ADDING POLYNOMIAL FEATURES\n    sel = VarianceThreshold(threshold=1.5).fit(train2p[cols])     \n    train3p = sel.transform(train2p[cols])\n    train3 = sel.transform(train2[cols])\n    test3 = sel.transform(test2[cols])   \n    poly = PolynomialFeatures(degree=2).fit(train3p)\n    train3p = poly.transform(train3p)\n    train3 = poly.transform(train3)\n    test3 = poly.transform(test3)\n    sc2 = StandardScaler()\n    train3p = sc2.fit_transform(train3p)\n    train3 = sc2.transform(train3)\n    test3 = sc2.transform(test3)\n        \n    # STRATIFIED K FOLD\n    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n    for train_index, test_index in skf.split(train3p, train2p['target']):\n        test_index3 = test_index[ test_index<len(train3) ] # ignore pseudo in oof\n        \n        # MODEL AND PREDICT WITH LR\n        clf = LogisticRegression(solver='saga',penalty='l2',C=0.01,tol=0.001)\n        clf.fit(train3p[train_index,:],train2p.loc[train_index]['target'])\n        oof[idx1[test_index3]] = clf.predict_proba(train3[test_index3,:])[:,1]\n        preds[test2.index] += clf.predict_proba(test3)[:,1] \/ skf.n_splits\n        \n    if k%64==0:  \n        print(k, 'LR2 oof auc : ', round(roc_auc_score(train['target'][idx1], oof[idx1]), 5))","57ddd61d":"# INITIALIZE VARIABLES\ntest['target'] = preds_QDA\noof_QDA2 = np.zeros(len(train))\npreds_QDA2 = np.zeros(len(test))\n\n# BUILD 512 SEPARATE MODELS\nfor k in range(512):\n    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==k] \n    train2p = train2.copy(); idx1 = train2.index \n    test2 = test[test['wheezy-copper-turtle-magic']==k]\n    \n    # ADD PSEUDO LABELED DATA\n    test2p = test2[ (test2['target']<=0.01) | (test2['target']>=0.99) ].copy()\n    test2p.loc[ test2p['target']>=0.5, 'target' ] = 1\n    test2p.loc[ test2p['target']<0.5, 'target' ] = 0 \n    train2p = pd.concat([train2p,test2p],axis=0)\n    train2p.reset_index(drop=True,inplace=True)\n    \n    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n    sel = VarianceThreshold(threshold=1.5).fit(train2p[cols])     \n    train3p = sel.transform(train2p[cols])\n    train3 = sel.transform(train2[cols])\n    test3 = sel.transform(test2[cols])\n        \n    # STRATIFIED K FOLD\n    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n    for train_index, test_index in skf.split(train3p, train2p['target']):\n        test_index3 = test_index[ test_index<len(train3) ] # ignore pseudo in oof\n        \n        # MODEL AND PREDICT WITH QDA\n        clf_QDA2 = QuadraticDiscriminantAnalysis(reg_param=0.5)\n        clf_QDA2.fit(train3p[train_index,:],train2p.loc[train_index]['target'])\n        oof_QDA2[idx1[test_index3]] = clf_QDA2.predict_proba(train3[test_index3,:])[:,1]\n        preds_QDA2[test2.index] += clf_QDA2.predict_proba(test3)[:,1] \/ skf.n_splits\n       \n    if k%64==0:\n        print(k, 'QDA2 oof auc : ', round(roc_auc_score(train['target'][idx1], oof_QDA2[idx1]), 5))","94302320":"print('LR auc: ', round(roc_auc_score(train['target'], oof),5))\nprint('QDA auc: ', round(roc_auc_score(train['target'], oof_QDA2),5))","0086c4c9":"w_best = 0\noof_best = oof_QDA2\nfor w in np.arange(0,0.55,0.001):\n    oof_blend = w*oof+(1-w)*oof_QDA2\n    if (roc_auc_score(train['target'], oof_blend)) > (roc_auc_score(train['target'], oof_best)):\n        w_best = w\n        oof_best = oof_blend\n        print(w_best)\nprint('best weight: ', w_best)\nprint('auc_best: ', round(roc_auc_score(train['target'], oof_best), 5))","e19b0e32":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['target'] = w_best*preds + (1-w_best)*preds_QDA2\nsub.to_csv('submission.csv', index=False)\nsub.head()","f3e4b8d4":"# Conclusion\n\nIn this kernel, I have tested PolyLR with pseudo labelling and blending with QDA. The results show that PolyLR does not increase the prediction performance of QDA since it has a very similar decision boundary to QDA, as illustrated by Chris in [Examples of Top 6 Classifiers][1].\n\n[1]: https:\/\/www.kaggle.com\/c\/instant-gratification\/discussion\/94179","56034f2f":"**PolyLR with Pseudo Labelling**","f8fbdec9":"**Load Data**","96d563ab":"**QDA with Pseudo Labelling (Chris's)**","f7c5a204":"**Find the Best Weights**\n\nLet's see if Pseudo-Labelled PolyLR can increase the performance (if w_best > 0)","8592f63c":"# Pseudo-Labelled PolyLR and QDA \n\nThis kernel shows the potential of adding quadratic polynomial features, a simple logistic regression can learn just like QDA. I also tested pseudo labelling and blending with QDA.\n\nThanks to Chris's great kernels [LR][1], [SVC][2], [probing][3], [pseudo labelling][5] and mhviraf's kernel [make_classification][4] which shows how the dataset was generated. Please also upvote those kernels.\n\n[1]: https:\/\/www.kaggle.com\/cdeotte\/logistic-regression-0-800\n[2]: https:\/\/www.kaggle.com\/cdeotte\/support-vector-machine-0-925\n[3]: https:\/\/www.kaggle.com\/cdeotte\/private-lb-probing-0-950\n[4]: https:\/\/www.kaggle.com\/mhviraf\/synthetic-data-for-next-instant-gratification\n[5]: https:\/\/www.kaggle.com\/cdeotte\/pseudo-labeling-qda-0-969","573d9f9c":"**Blending with the Best Weights**","e2464700":"**PolyLR and QDA**"}}