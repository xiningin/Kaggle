{"cell_type":{"50133c6d":"code","c229f8ce":"code","07e5008c":"code","89d83a01":"code","1c3fe4f8":"code","49756803":"code","2aeafd28":"code","90ea1293":"code","c7602155":"code","b0808f4b":"code","c4acb6c1":"code","a201f3f4":"code","c34d355a":"code","6feab0d9":"code","ccbbd0be":"code","09f9e8d3":"code","431e4007":"code","fd7ac45c":"code","f8302aa6":"code","6716abb0":"code","ab629d9c":"code","5834ea4d":"code","036e359c":"code","a5bd6a90":"code","54bfb1b8":"code","567b1772":"code","0c04ff17":"code","751a74a3":"code","e606a378":"code","dd33146b":"code","1f5271c4":"code","8156e873":"code","41ea5e99":"code","752e8ab0":"code","23f7e278":"code","ba9b2074":"markdown","149fa2e8":"markdown","fadbc3b6":"markdown","4db2c86f":"markdown","c500c12c":"markdown","0954b588":"markdown","f28be7bc":"markdown","c46470ee":"markdown","7396afba":"markdown"},"source":{"50133c6d":"!pip install seaborn","c229f8ce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","07e5008c":"#imports\nimport re\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport wordcloud\nfrom wordcloud import WordCloud,STOPWORDS\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\n\nimport sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nfrom collections import Counter\nfrom collections import defaultdict\nfrom plotly import tools\nfrom plotly.subplots import make_subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go","89d83a01":"#Reading Data\ndf_train = pd.read_csv('..\/input\/quora-insincere-questions-classification\/train.csv')\ndf_test = pd.read_csv('..\/input\/quora-insincere-questions-classification\/test.csv')","1c3fe4f8":"#shape\nprint('train_shape',df_train.shape)\nprint('test_shape',df_test.shape)\n\n#null check\nprint(set(df_train.isnull().sum()))\nprint(set(df_test.isnull().sum()))\n\n#duplicates check\nprint(df_train.shape[0]-df_train.drop_duplicates().shape[0])\nprint(df_test.shape[0]-df_test.drop_duplicates().shape[0])","49756803":"df_train.head()","2aeafd28":"df_test.head()","90ea1293":"# class balance check\npos = df_train[df_train.target==1].shape[0]\nneg = df_train[df_train.target==0].shape[0]\nprint('Positives',pos)\nprint('Negatives',neg)\nprint('P:N',np.round(pos*100\/neg,2))","c7602155":"# ## cerating new columns, \n\n# #length of question_test\n# df_train['text_len'] = df_train['question_text'].apply(lambda x: len(x))\n\n\n# # punctuations count\n# def punc_count(x):\n#     punc_lst = list(string.punctuation)\n#     rtn = [p for p in punc_lst if p in x]\n#     return len(rtn)\n# df_train['punctuation_count'] = df_train['question_text'].apply(lambda x: punc_count(x))\n\n\n# # stop words count\n# def stopwords_count(x):\n#     stopwords_lst = list(stopwords.words('english'))\n#     rtn = [s for s in stopwords_lst if s in x]\n#     return len(rtn)\n# df_train['stopwords_count'] = df_train['question_text'].apply(lambda x: stopwords_count(x))\n\n# # urls\n# def url_count(x):\n#     rtn = [u for u in x.split(' ') if 'http' in u]\n#     return len(rtn)\n# df_train['url_count'] = df_train['question_text'].apply(lambda x: url_count(x))\n\n\n# #positive and negative reviews\n# df_train_pos = df_train[df_train.target==1]\n# df_train_neg = df_train[df_train.target==0]\n\n\n# #plots\n# fig,axes = plt.subplots(1,3,figsize=(12,5))\n\n# sns.distplot(df_train_pos.text_len,ax=axes[0]).set_title('text length')\n# sns.distplot(df_train_neg.text_len,ax=axes[0]).set_title('text length')\n\n# sns.distplot(df_train_pos.punctuation_count,ax=axes[1]).set_title('Punctuation Count')\n# sns.distplot(df_train_neg.punctuation_count,ax=axes[1]).set_title('Punctuation Count')\n\n# sns.distplot(df_train_pos.stopwords_count,ax=axes[2]).set_title('Stopwords Count')\n# sns.distplot(df_train_neg.stopwords_count,ax=axes[2]).set_title('Stopwords Count')\n\n# plt.show()","b0808f4b":"#positive and negative reviews\ndf_train_pos = df_train[df_train.target==1]\ndf_train_neg = df_train[df_train.target==0]","c4acb6c1":"# random 10 negative questions\n_lst = list(df_train_pos['question_text'].sample(10))\nfor x in _lst:\n    print(x)","a201f3f4":"# random 10 positive questions\n_lst = list(df_train_neg['question_text'].sample(10))\nfor x in _lst:\n    print(x)","c34d355a":"class metaFeatures:\n    \n    def __init__(self,df):\n        self.df = df\n    \n    #Number of words in the text\n    @staticmethod\n    def num_of_words(ech_row):\n        rtn = len(ech_row.split())\n        return rtn\n    \n    #Number of unique words in the text\n    @staticmethod\n    def num_of_unqwords(ech_row):\n        rtn = len(set(ech_row.split()))\n        return rtn\n    \n    #Number of characters in the text\n    @staticmethod\n    def num_of_chars(ech_row):\n        rtn = len(set(ech_row))\n        return rtn\n    \n    #Number of stopwords\n    @staticmethod\n    def num_of_stopwords(ech_row):\n        stopwords_lst = list(stopwords.words('english'))\n        rtn = len([s for s in str(ech_row).lower().split() if s in stopwords_lst])\n        return rtn\n    \n    #Number of punctuations\n    @staticmethod\n    def num_of_punctuations(ech_row):\n        punc_lst = list(string.punctuation)\n        rtn = len([p for p in str(ech_row).lower().split() if p in punc_lst])\n        return rtn\n    \n    #Number of upper case words\n    @staticmethod\n    def num_of_uppercase(ech_row):\n        rtn = len([p for p in str(ech_row).split() if p.isupper()])\n        return rtn\n    \n    #Number of title case words\n    @staticmethod\n    def num_of_titlecase(ech_row):\n        rtn = len([p for p in str(ech_row).split() if p.istitle()])\n        return rtn\n    \n    #Number of numericals in the text\n    @staticmethod\n    def num_of_numericals(ech_row):\n        numer_lst = ['0','1','2','3','4','5','6','7','8','9']\n        rtn = len([p for p in numer_lst if p in ech_row])\n        return rtn\n    \n    #Average length of the words\n    @staticmethod\n    def words_avglen(ech_row):\n        rtn = np.round( np.mean([len(p) for p in str(ech_row).split()]) ,2)\n        return rtn\n    \n    # URLs Check\n    @staticmethod\n    def urls_count(ech_row):\n        rtn = len([h for h in str(ech_row).lower().split() if 'http' in h or 'https' in h])\n        return rtn\n    \n    #final calculations\n    def calc(self):\n        \n        self.df['text_len'] = self.df['question_text'].apply(lambda x: len(x))\n        self.df['num_of_words'] = self.df['question_text'].apply(lambda x: self.num_of_words(x))\n        self.df['num_of_unqwords'] = self.df['question_text'].apply(lambda x: self.num_of_unqwords(x))\n        self.df['num_of_chars'] = self.df['question_text'].apply(lambda x: self.num_of_chars(x))\n        self.df['num_of_stopwords'] = self.df['question_text'].apply(lambda x: self.num_of_stopwords(x))\n        self.df['num_of_punctuations'] = self.df['question_text'].apply(lambda x: self.num_of_punctuations(x))\n        self.df['num_of_uppercase'] = self.df['question_text'].apply(lambda x: self.num_of_uppercase(x))\n        self.df['num_of_titlecase'] = self.df['question_text'].apply(lambda x: self.num_of_titlecase(x))\n        self.df['num_of_numericals'] = self.df['question_text'].apply(lambda x: self.num_of_numericals(x))\n        self.df['words_avglen'] = self.df['question_text'].apply(lambda x: self.words_avglen(x))\n        self.df['urls_count'] = self.df['question_text'].apply(lambda x: self.urls_count(x))\n        \n        return self.df\n\nmetafeatures = metaFeatures(df_train)\ndf_train_feat = metafeatures.calc()\n\n#positive and negative reviews\ndf_train_pos = df_train_feat[df_train_feat.target==1]\ndf_train_neg = df_train_feat[df_train_feat.target==0]\n\ndf_train_feat.head()","6feab0d9":"plot_cols = ['text_len', 'num_of_words', 'num_of_unqwords', 'num_of_chars',\n             'num_of_stopwords', 'num_of_punctuations', 'num_of_uppercase',\n             'num_of_titlecase', 'num_of_numericals', 'words_avglen','urls_count']\n\n#plots\nfig,axes = plt.subplots(6,2,figsize=(14,20),constrained_layout=True)\nfor k in range(0,len(plot_cols)):\n    j = 0 if k%2==0 else 1\n    i = k\/\/2\n    col = plot_cols[k]\n    df = df_train_feat\n    sns.boxplot(x='target', y=col, data=df, ax=axes[i,j])\n\nfig.delaxes(axes[5,1])\nplt.show()","ccbbd0be":"plot_cols = ['text_len', 'num_of_words', 'num_of_unqwords', 'num_of_chars',\n             'num_of_stopwords', 'num_of_punctuations', 'num_of_uppercase',\n             'num_of_titlecase', 'num_of_numericals', 'words_avglen','urls_count']\n\n#plots\nfig,axes = plt.subplots(6,2,figsize=(14,20),constrained_layout=True)\n\nfor k in range(0,len(plot_cols)):\n    j = 0 if k%2==0 else 1\n    i = k\/\/2\n    col = plot_cols[k]\n    sns.violinplot(x='target', y=col, data=df_train_feat, ax=axes[i,j])\n\nfig.delaxes(axes[5,1])\nplt.show()","09f9e8d3":"# plot_cols = ['text_len', 'num_of_words', 'num_of_unqwords', 'num_of_chars',\n#              'num_of_stopwords', 'num_of_punctuations', 'num_of_uppercase',\n#              'num_of_titlecase', 'num_of_numericals', 'words_avglen','urls_count']\n\n# #plots\n# fig,axes = plt.subplots(6,2,figsize=(14,20),constrained_layout=True)\n\n# for k in range(0,len(plot_cols)):\n#     j = 0 if k%2==0 else 1\n#     i = k\/\/2\n#     col = plot_cols[k]\n#     val_0 = df_train_feat[df_train_feat.target==0][col]\n#     val_1 = df_train_feat[df_train_feat.target==1][col]\n#     val_all = df_train_feat[col]\n    \n#     #sns.distplot(val_0,kde=False,color='red', ax=axes[i,j])\n#     #sns.distplot(val_1,kde=False,color='blue', ax=axes[i,j])\n#     sns.distplot(val_all,kde=False,color='green', ax=axes[i,j])\n\n# fig.delaxes(axes[5,1])\n# plt.show()","431e4007":"#WordCloud Visualizations\n#Method for creating wordclouds\nfrom PIL import Image\ndef display_cloud(data,color):\n    plt.subplots(figsize=(15,15))\n    mask = None\n    wc = WordCloud(stopwords=STOPWORDS, \n                   mask=mask, background_color=\"white\", contour_width=2, contour_color=color,\n                   max_words=2000, max_font_size=256,\n                   random_state=42)\n    wc.generate(' '.join(data))\n    plt.imshow(wc, interpolation=\"bilinear\")\n    plt.axis('off')\n    plt.show()\n    \n# good and bad questions--using this terminology from now on\ndf_train_bad = df_train_feat[df_train_feat.target==1]\ndf_train_good = df_train_feat[df_train_feat.target==0]","fd7ac45c":"display_cloud(df_train_bad['question_text'],'red')","f8302aa6":"display_cloud(df_train_good['question_text'],'red')","6716abb0":"stopword=set(stopwords.words('english'))\ndef generate_grams(txt,n_gram):\n    tokens = [t for t in txt.lower().split() if t != \"\" if t not in stopword]\n    ngrams = zip(*[tokens[i:] for i in range(n_gram)])\n    final_tokens=[\" \".join(z) for z in ngrams]\n    return final_tokens\n\ndef calculate_frequency(df_series,n_gram):\n    dict_rtn = defaultdict(int)\n    for txt in df_series:\n        for _key in generate_grams(txt,n_gram):\n            dict_rtn[_key] += 1\n    \n    sort_dict = sorted(dict_rtn.items(),key=lambda itm:itm[1],reverse=True)\n    df_rtn = pd.DataFrame(sort_dict, columns=['n_gram_words', 'n_gram_frequency'])\n    return df_rtn\n\ndef plotly_bar_chart(df,color):\n    trace = go.Bar(\n        y=df[\"n_gram_words\"].values[::-1],\n        x=df[\"n_gram_frequency\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\ndef final_plots(df,n_gram=1,top=20):\n    freq_df_0 = calculate_frequency(df[df.target==0]['question_text'],n_gram)\n    trace_0=plotly_bar_chart(freq_df_0[:top],'orange')\n    \n    freq_df_1 = calculate_frequency(df[df.target==1]['question_text'],n_gram)\n    trace_1=plotly_bar_chart(freq_df_1[:top],'orange')\n    \n    fig = make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of good questions\", \n                                          \"Frequent words of bad questions\"])\n    fig.append_trace(trace_0, 1, 1)\n    fig.append_trace(trace_1, 1, 2)\n    fig['layout'].update(height=1200, width=1000, paper_bgcolor='rgb(233,233,233)', title=f\"Word Count(ngrams={n_gram}) Plots\")\n    py.iplot(fig, filename='word-plots')","ab629d9c":"print('Uni-Gram')\nfinal_plots(df_train_feat,n_gram=1,top=10)","5834ea4d":"print('Bi-Gram')\nfinal_plots(df_train_feat,n_gram=2,top=10)","036e359c":"print('Tri-Gram')\nfinal_plots(df_train_feat,n_gram=3,top=10)","a5bd6a90":"print('Tetra-Gram')\nfinal_plots(df_train_feat,n_gram=4,top=10)","54bfb1b8":"class CleanIt:\n    \n    def __init__(self,df):\n        self.df = df\n    \n    #Removes Punctuations\n    @staticmethod\n    def remove_punctuations(ech_row):\n        punct_tag = re.compile(r'[^\\w\\s]')\n        rtn = punct_tag.sub(r'',ech_row)\n        return rtn\n    \n    #Removes HTML syntaxes\n    @staticmethod\n    def remove_html(ech_row):\n        html_tag=re.compile(r'<.*?>')\n        rtn=html_tag.sub(r'',ech_row)\n        return rtn\n    \n    #Removes URL data\n    @staticmethod\n    def remove_url(ech_row):\n        url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n        rtn=url_clean.sub(r'',ech_row)\n        return rtn\n    \n    #Removes Emojis\n    @staticmethod\n    def remove_emoji(ech_row):\n        emoji_clean= re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n        rtn=emoji_clean.sub(r'',ech_row)\n        url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n        rtn=url_clean.sub(r'',rtn)\n        return rtn\n    \n    #Lower Case\n    @staticmethod\n    def make_it_lower(ech_row):\n        rtn=ech_row.lower()\n        return rtn\n    \n    #Remove extra spaces\n    @staticmethod\n    def remove_extraSpace(ech_row):\n        rtn=' '.join(ech_row.split())\n        return rtn\n    \n    #Replace abbreviated pronouns with full forms\n    @staticmethod\n    def remove_abb(data):\n        data = re.sub(r\"he's\", \"he is\", data)\n        data = re.sub(r\"there's\", \"there is\", data)\n        data = re.sub(r\"We're\", \"We are\", data)\n        data = re.sub(r\"That's\", \"That is\", data)\n        data = re.sub(r\"won't\", \"will not\", data)\n        data = re.sub(r\"they're\", \"they are\", data)\n        data = re.sub(r\"Can't\", \"Cannot\", data)\n        data = re.sub(r\"wasn't\", \"was not\", data)\n        data = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", data)\n        data= re.sub(r\"aren't\", \"are not\", data)\n        data = re.sub(r\"isn't\", \"is not\", data)\n        data = re.sub(r\"What's\", \"What is\", data)\n        data = re.sub(r\"haven't\", \"have not\", data)\n        data = re.sub(r\"hasn't\", \"has not\", data)\n        data = re.sub(r\"There's\", \"There is\", data)\n        data = re.sub(r\"He's\", \"He is\", data)\n        data = re.sub(r\"It's\", \"It is\", data)\n        data = re.sub(r\"You're\", \"You are\", data)\n        data = re.sub(r\"I'M\", \"I am\", data)\n        data = re.sub(r\"shouldn't\", \"should not\", data)\n        data = re.sub(r\"wouldn't\", \"would not\", data)\n        data = re.sub(r\"i'm\", \"I am\", data)\n        data = re.sub(r\"I\\x89\u00db\u00aam\", \"I am\", data)\n        data = re.sub(r\"I'm\", \"I am\", data)\n        data = re.sub(r\"Isn't\", \"is not\", data)\n        data = re.sub(r\"Here's\", \"Here is\", data)\n        data = re.sub(r\"you've\", \"you have\", data)\n        data = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", data)\n        data = re.sub(r\"we're\", \"we are\", data)\n        data = re.sub(r\"what's\", \"what is\", data)\n        data = re.sub(r\"couldn't\", \"could not\", data)\n        data = re.sub(r\"we've\", \"we have\", data)\n        data = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", data)\n        data = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", data)\n        data = re.sub(r\"It\\x89\u00db\u00aas\", \"It is\", data)\n        data = re.sub(r\"Here\\x89\u00db\u00aas\", \"Here is\", data)\n        data = re.sub(r\"who's\", \"who is\", data)\n        data = re.sub(r\"I\\x89\u00db\u00aave\", \"I have\", data)\n        data = re.sub(r\"y'all\", \"you all\", data)\n        data = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", data)\n        data = re.sub(r\"would've\", \"would have\", data)\n        data = re.sub(r\"it'll\", \"it will\", data)\n        data = re.sub(r\"we'll\", \"we will\", data)\n        data = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", data)\n        data = re.sub(r\"We've\", \"We have\", data)\n        data = re.sub(r\"he'll\", \"he will\", data)\n        data = re.sub(r\"Y'all\", \"You all\", data)\n        data = re.sub(r\"Weren't\", \"Were not\", data)\n        data = re.sub(r\"Didn't\", \"Did not\", data)\n        data = re.sub(r\"they'll\", \"they will\", data)\n        data = re.sub(r\"they'd\", \"they would\", data)\n        data = re.sub(r\"DON'T\", \"DO NOT\", data)\n        data = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", data)\n        data = re.sub(r\"they've\", \"they have\", data)\n        data = re.sub(r\"i'd\", \"I would\", data)\n        data = re.sub(r\"should've\", \"should have\", data)\n        data = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", data)\n        data = re.sub(r\"where's\", \"where is\", data)\n        data = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", data)\n        data = re.sub(r\"we'd\", \"we would\", data)\n        data = re.sub(r\"i'll\", \"I will\", data)\n        data = re.sub(r\"weren't\", \"were not\", data)\n        data = re.sub(r\"They're\", \"They are\", data)\n        data = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", data)\n        data = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", data)\n        data = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", data)\n        data = re.sub(r\"let's\", \"let us\", data)\n        data = re.sub(r\"it's\", \"it is\", data)\n        data = re.sub(r\"can't\", \"cannot\", data)\n        data = re.sub(r\"dont\", \"do not\", data)\n        data = re.sub(r\"don't\", \"do not\", data)\n        data = re.sub(r\"you're\", \"you are\", data)\n        data = re.sub(r\"i've\", \"I have\", data)\n        data = re.sub(r\"that's\", \"that is\", data)\n        data = re.sub(r\"i'll\", \"I will\", data)\n        data = re.sub(r\"doesn't\", \"does not\",data)\n        data = re.sub(r\"i'd\", \"I would\", data)\n        data = re.sub(r\"didn't\", \"did not\", data)\n        data = re.sub(r\"ain't\", \"am not\", data)\n        data = re.sub(r\"you'll\", \"you will\", data)\n        data = re.sub(r\"I've\", \"I have\", data)\n        data = re.sub(r\"Don't\", \"do not\", data)\n        data = re.sub(r\"I'll\", \"I will\", data)\n        data = re.sub(r\"I'd\", \"I would\", data)\n        data = re.sub(r\"Let's\", \"Let us\", data)\n        data = re.sub(r\"you'd\", \"You would\", data)\n        data = re.sub(r\"It's\", \"It is\", data)\n        data = re.sub(r\"Ain't\", \"am not\", data)\n        data = re.sub(r\"Haven't\", \"Have not\", data)\n        data = re.sub(r\"Could've\", \"Could have\", data)\n        data = re.sub(r\"youve\", \"you have\", data)  \n        data = re.sub(r\"don\u00e5\u00abt\", \"do not\", data)\n        return data\n\n    #final calculations\n    def calc(self):\n        self.df['question_text'] = self.df['question_text'].apply(lambda x: self.remove_extraSpace(x))\n        \n        self.df['question_text'] = self.df['question_text'].apply(lambda x: self.remove_punctuations(x))\n        self.df['question_text'] = self.df['question_text'].apply(lambda x: self.remove_html(x))\n        self.df['question_text'] = self.df['question_text'].apply(lambda x: self.remove_url(x))\n        self.df['question_text'] = self.df['question_text'].apply(lambda x: self.remove_emoji(x))\n        self.df['question_text'] = self.df['question_text'].apply(lambda x: self.remove_abb(x))\n        \n        self.df['question_text'] = self.df['question_text'].apply(lambda x: self.make_it_lower(x))\n        return self.df\n\nimport re\ndata_clean = CleanIt(df_train)\ndf_train_clean_feat = data_clean.calc()\n\n#positive and negative reviews\ndf_train_clean_pos = df_train_clean_feat[df_train_clean_feat.target==1]\ndf_train_clean_neg = df_train_clean_feat[df_train_clean_feat.target==0]\n\ndf_train_clean_feat.head()","567b1772":"#WordCloud Visualizations\n#Method for creating wordclouds\nfrom PIL import Image\ndef display_cloud(data,color):\n    plt.subplots(figsize=(15,15))\n    mask = None\n    wc = WordCloud(stopwords=STOPWORDS, \n                   mask=mask, background_color=\"white\", contour_width=2, contour_color=color,\n                   max_words=2000, max_font_size=256,\n                   random_state=42)\n    wc.generate(' '.join(data))\n    plt.imshow(wc, interpolation=\"bilinear\")\n    plt.axis('off')\n    plt.show()\n    \n# good and bad questions--using this terminology from now on\ndf_train_clean_bad = df_train_clean_feat[df_train_clean_feat.target==1]\ndf_train_clean_good = df_train_clean_feat[df_train_clean_feat.target==0]","0c04ff17":"display_cloud(df_train_clean_bad['question_text'],'red')","751a74a3":"display_cloud(df_train_clean_good['question_text'],'red')","e606a378":"print('Uni-Gram')\nfinal_plots(df_train_clean_feat,n_gram=1,top=10)","dd33146b":"print('Tri-Gram')\nfinal_plots(df_train_clean_feat,n_gram=3,top=10)","1f5271c4":"print('Tetra-Gram')\nfinal_plots(df_train_clean_feat,n_gram=4,top=10)","8156e873":"import nltk\nfrom nltk.stem import WordNetLemmatizer","41ea5e99":"# #Lemmatize the dataset\n# def lemma_traincorpus(data):\n#     lemmatizer=WordNetLemmatizer()\n#     out_data=\"\"\n#     for words in data:\n#         out_data+= lemmatizer.lemmatize(words)\n#     return out_data\n\n# df_train_clean_feat['question_text']=df_train_clean_feat['question_text'].apply(lambda ech_row: lemma_traincorpus(ech_row))\n\n# df_train_clean_feat['question_text'].head()","752e8ab0":"# import sklearn\n# from sklearn.feature_extraction.text import CountVectorizer\n# from sklearn.feature_extraction.text import TfidfTransformer","23f7e278":"# %%time\n# tfidf_vect=TfidfVectorizer(stop_words='english',ngram_range=(1,3))\n# train_tfidf=tfidf_vect.fit_transform(df_train_clean_feat['question_text'].values.tolist())\n# train_tfidf.shape","ba9b2074":"### **Inference:-**\n1. length of question(`text_len`) gives some minor distinction b\/w positivies and negatives\n2. `num_of_words` and `num_of_unqwords` have similar distribution as that of `text_len`, this is logical as text length increases `num_of_words` and `num_of_unqwords` also increase\n3. `num_of_stopwords` has interesting distribution","149fa2e8":"### N-Gram Analysis","fadbc3b6":"### **Vectorization - TFIDF and Count**","4db2c86f":"### **Meta features Creation**\nBased on this SRK's EDA [kernal](https:\/\/www.kaggle.com\/sudalairajkumar\/simple-feature-engg-notebook-spooky-author), creating following meta-features:-\n1. Length of test\n2. Number of words in the text\n3. Number of unique words in the text\n4. Number of characters in the text\n5. Number of stopwords\n6. Number of punctuations\n7. Number of upper case words\n8. Number of title case words\n9. Number of numericals in the text\n10. Average length of the words\n11. Check for urls\n\nThe idea behind creating above features is to check if it helps in identifing distinction b\/w negatives and positives","c500c12c":"As length of text has some role lets look at frequency of words... `Word Cloud`\n\nsome of below functions are refered from this [kernal](https:\/\/www.kaggle.com\/colearninglounge\/nlp-end-to-end-cll-nlp-workshop)","0954b588":"**Stemming :** This is the final and most important part of the preprocessing. stemming converts words to its stem.\nFor example playing and played are the same type of words which basically indicate an action play. Stemmer does exactly this, it reduces the word to its stem. we are going to use a library called porter-stemmer which is a rule based stemmer. Porter-Stemmer identifies and removes the suffix or affix of a word. The words given by the stemmer need note be meaningful few times<br>\n\n**Lemmatisation :** is a way to reduce the word to root synonym of a word. Unlike Stemming, Lemmatisation makes sure that the reduced word is again a dictionary word (word present in the same language). WordNetLemmatizer can be used to lemmatize any word.\n<br><br>\n**Stemming vs Lemmatization**\n* Stemming \u2014 need not be a dictionary word, removes prefix and affix based on few rules\n* Lemmatization \u2014 will be a dictionary word. reduces to a root synonym.\n\n![image.png](attachment:image.png)\n\nRefer: https:\/\/towardsdatascience.com\/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089","f28be7bc":"## Data Cleaning","c46470ee":"Looks interesting, here good questions have words describing <u>actions<\/u>, where as bad questions have words used for some kind of <u>description<\/u>","7396afba":"### **Basic Analysis**"}}