{"cell_type":{"8b985fa8":"code","d3b02f3c":"code","3f984a76":"code","757a158c":"code","b88c8d49":"code","5ec186ad":"code","f8e83d38":"code","956d9366":"code","c4d05008":"code","7fdaf7da":"code","48f1d8c3":"code","5e366d5e":"code","d7993814":"code","f9d50fa4":"code","5b419068":"code","3470c8de":"code","9c6030b1":"code","90ebb742":"code","e270c9d0":"code","560eab0e":"markdown","f5574749":"markdown","a268e11c":"markdown","eff1137f":"markdown","ddda32e0":"markdown","57e3f3b7":"markdown","cd59e201":"markdown","b12bef52":"markdown","4b394af7":"markdown","49c9c780":"markdown","02d6b1d4":"markdown","f59e43cd":"markdown","9e6016a6":"markdown","39c47cf1":"markdown","1b2c9794":"markdown","fccd90c2":"markdown","379d1de5":"markdown","eb349ad2":"markdown"},"source":{"8b985fa8":"!pip install cortado\n!pip install xgboost","d3b02f3c":"import pandas as pd\n\ncsvpath = \"..\/input\/airlinetrain1m\/airlinetrain1m.csv\"\ndf_pd = pd.read_csv(csvpath)\ndf_pd.head()","3f984a76":"df_pd.info()","757a158c":"from scipy.sparse import coo_matrix\nimport numpy as np\n\ncovariates_xg = [\"DepTime\", \"Distance\"]\nfactors_xg = [\"Month\", \"DayofMonth\", \"DayOfWeek\", \"UniqueCarrier\", \"Origin\", \"Dest\"]\n\nsparse_covariates = list(map(lambda col: df_pd[col].astype(pd.SparseDtype(\"float32\", 0.0)), covariates_xg))\nsparse_factors = list(map(lambda col: pd.get_dummies(df_pd[col], prefix=col, sparse=True, dtype=np.float32), factors_xg))\n\ndata = pd.concat(sparse_factors + sparse_covariates, axis=1)\nsparse_data = coo_matrix(data.sparse.to_coo()).tocsr()\n","b88c8d49":"label_xg = df_pd[\"dep_delayed_15min\"].map({\"N\": 0, \"Y\": 1})","5ec186ad":"eta = 0.1\nnrounds = 100\nmax_depth = 6","f8e83d38":"import xgboost as xgb\nfrom datetime import datetime\n\nstart = datetime.now()\nmodel = xgb.XGBClassifier(max_depth=max_depth, nthread=1, learning_rate=eta, tree_method=\"exact\", n_estimators=nrounds)\nmodel.fit(sparse_data, label_xg)\npred_xg = model.predict_proba(sparse_data)\nend = datetime.now()\nprint(\"xgboost elapsed: {e}\".format(e=(end - start)))\n","956d9366":"import cortado as cr\n\ndf_cr = cr.DataFrame.from_pandas(df_pd)","c4d05008":"df_cr.covariates","7fdaf7da":"df_cr.factors","48f1d8c3":"deptime = cr.Factor.from_covariate(df_cr[\"DepTime\"])\ndistance = cr.Factor.from_covariate(df_cr[\"Distance\"])","5e366d5e":"deptime.levels[:5]","d7993814":"deptime = deptime.cached()\ndistance = distance.cached()","f9d50fa4":"dep_delayed_15min = df_cr[\"dep_delayed_15min\"]\nlabel = cr.Covariate.from_factor(dep_delayed_15min, lambda level: level == \"Y\")\nprint(label)","5b419068":"factors = df_cr.factors + [deptime, distance]\nfactors.remove(dep_delayed_15min)","3470c8de":"deptime.isordinal","9c6030b1":"df_cr[\"Month\"].isordinal","90ebb742":"start = datetime.now()\ntrees, pred_cr = cr.xgblogit(label, factors,  eta = eta, lambda_ = 1.0, gamma = 0.0, minh = 1.0, nrounds = nrounds, maxdepth = max_depth, slicelen=1000000)\nend = datetime.now()\nprint(\"cortado elapsed: {e}\".format(e=(end - start)))","e270c9d0":"from sklearn.metrics import roc_auc_score\ny = label.to_array() # convert to numpy array\nauc_cr = roc_auc_score(y, pred_cr) # cortado auc\nauc_xg = roc_auc_score(y, pred_xg[:, 1]) # xgboost auc\nprint(\"cortado auc: {auc_cr}\".format(auc_cr=auc_cr))\nprint(\"xgboost auc: {auc_xg}\".format(auc_xg=auc_xg))\ndiff = np.max(np.abs(pred_xg[:, 1] - pred_cr))\nprint(\"max pred diff: {diff}\".format(diff=diff))","560eab0e":"We will use these model parameters:","f5574749":"*Factor.from_covariate()* will bucketize the covariate using all of its unique values. Internally factors keep a list of unique levels and a *uint8* or *uint16* array of level indices:","a268e11c":"We have 2 numeric columns and 7 categorical ones. We would like to use *dep_delayed_15min* as our label and all other columns as features.\n\nLet's start with XGBoost. It expects all features to be numeric so we will one hot encode categorical features with pandas *get_dummies* and convert all data into sparse format:","eff1137f":"*cortado* logistic xgboost implementation expects the label to be numeric (covariate). We can easily convert from a factor into a covariate:","ddda32e0":"*cortado* supports categorical data out of the box so we do not have to create dummy vars, hot encode etc. It knows a difference between ordinal and not ordinal factors. For ordinal factors the tree boosting algorithm will only consider range splits: < x and >= x. For non ordinal factors the possible splits are only \"level x \" vs \"not level x\". Each factor has a property *isordinal*: \n","57e3f3b7":"The result is a list of *trees* and predicted probabilities:","cd59e201":"We will be using a dataset with 1M observations:","b12bef52":"We can now try to run extreme boosted tree model in **cortado**.\n\nFirst we import the data directly from pandas dataframe:","4b394af7":"We also need to make the label a numeric feature:","49c9c780":"*deptime* is ordinal because it is a result of bucketization so its levels are naturally ordered.","02d6b1d4":"**cortado** extreme boosted tree is single threaded at the moment and effectively uses *tree_method = 'exact'* so we will use the same options for easy comparison:","f59e43cd":"**cortado** contains a high performance implementation of extreme gradient boosted tree. Let's try it!\n\nFirst, we install cortado and xgboost:","9e6016a6":"*Factor.from_covariate* is a lazy operation: it does not compute the actual factor data (level indices). This will save memory but if you want speed then you can cache any factor or covariate in memory:**","39c47cf1":"During the import all numeric features will be converted into cortado *covariates* and non numeric into *factors*:","1b2c9794":"Cortado and XGBoost give the same result but cortado is 3x faster!","fccd90c2":"We can now create a list of factors which will be used in the model as features:","379d1de5":"**cortado** expects all features in a boosted tree model to be categorical factors. We have 2 numeric covariates which we can easily convert to factors:","eb349ad2":"We can now run extreme boosted tree with *xgblogit*:"}}