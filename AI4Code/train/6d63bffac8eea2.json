{"cell_type":{"903c1a55":"code","22c20691":"code","9f2d7b03":"code","2b229fac":"code","a6ef3a43":"code","30f35ff2":"code","e7866c71":"code","fb876268":"code","93d457e9":"code","ef300749":"code","92904bd5":"markdown","431d1f81":"markdown","64d7fc86":"markdown","0c1c1fe4":"markdown","1dbecaed":"markdown","e4305cdf":"markdown","868d6e82":"markdown"},"source":{"903c1a55":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport xgboost as xgb\nfrom imblearn.under_sampling import RandomUnderSampler","22c20691":"df = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","9f2d7b03":"df.head()","2b229fac":"df['Class'].value_counts()","a6ef3a43":"X = df.drop('Class', axis = 1)\ny = df['Class']\n\nXtrn, Xval, ytrn, yval = train_test_split(X, y, test_size=0.33, random_state=1)\n\nmdl = RandomForestClassifier(n_jobs = -1, n_estimators = 20)\nmdl.fit(Xtrn, ytrn)\npred = mdl.predict(Xval)\nprint(confusion_matrix(yval,pred))\nprint('False Negative MEAN: {}'.format(confusion_matrix(yval,pred)[1,0]\/(confusion_matrix(yval,pred)[1,1] + confusion_matrix(yval,pred)[1,0])))","30f35ff2":"X = df.drop('Class', axis = 1)\ny = df['Class']\nmdl = RandomForestClassifier(n_jobs = -1)\n\nXtrn, Xval, ytrn, yval = train_test_split(X, y, test_size=0.33, random_state=0)\n\nru = RandomUnderSampler()\nXres, yres = ru.fit_sample(Xtrn, ytrn)\n\nmdl.fit(Xres, yres)\npred = mdl.predict(Xval)\nprint(confusion_matrix(yval,pred))\nprint('False Negative MEAN: {}'.format(confusion_matrix(yval,pred)[1,0]\/(confusion_matrix(yval,pred)[1,1] + confusion_matrix(yval,pred)[1,0])))","e7866c71":"\nXtrn, Xval, ytrn, yval = train_test_split(X, y, test_size=0.33, random_state=0)\nru = RandomUnderSampler()\nXres, yres = ru.fit_sample(Xtrn, ytrn)\n\nmdl = xgb.XGBClassifier()\nmdl.fit(Xres, yres)\npred = mdl.predict(Xval)\nprint(confusion_matrix(yval,pred))\nprint('False Negative MEAN: {}'.format(confusion_matrix(yval,pred)[1,0]\/(confusion_matrix(yval,pred)[1,1] + confusion_matrix(yval,pred)[1,0])))","fb876268":"def cross_val(X, y, mdl):\n    \n    rkfold = RepeatedKFold(n_splits = 3, n_repeats = 5, random_state = 0)\n    result1 = []\n    result0 = []\n    for treino, teste in rkfold.split(X):\n        Xtrn, Xval = X.iloc[treino], X.iloc[teste]\n        ytrn, yval = y.iloc[treino], y.iloc[teste]\n        \n        ru = RandomUnderSampler()\n        Xres, yres = ru.fit_sample(Xtrn, ytrn)\n        \n        mdl.fit(Xres, yres)\n        pred = mdl.predict(Xval)\n        erro_1 = confusion_matrix(yval,pred)[1,0]\/(confusion_matrix(yval,pred)[1,1] + confusion_matrix(yval,pred)[1,0])\n        \n        erro_0 = confusion_matrix(yval,pred)[0,1]\/(confusion_matrix(yval,pred)[0,0] + confusion_matrix(yval,pred)[0,1])\n        \n        result1.append(erro_1)\n        result0.append(erro_0)\n        \n    print ('False Negatives MEAN:',np.mean(result1))\n    print ('False Positives MEAN:',np.mean(result0))","93d457e9":"X = df.drop('Class', axis = 1)\ny = df['Class']\nmdl = RandomForestClassifier(n_jobs = -1)\n\ncross_val(X, y, mdl)","ef300749":"X = df.drop('Class', axis = 1)\ny = df['Class']\nmdl = xgb.XGBClassifier()\n\ncross_val(X, y, mdl)","92904bd5":"# XGB, RF and RandomUnderSampler","431d1f81":"# RF + RandomUnderSampler()","64d7fc86":"### Demonstration of the benefits of using RandomUnderSampler when dealing with unbalanced data.","0c1c1fe4":"# Comparing RF and XGB","1dbecaed":"We can see that XGB is a better option if we want to reduce the false negatives, but it's a tradeoff with the false positive rate. This is a simple approach to the problem, if you want to go deeper in this dataset it's recommended making feature engineering and tuning hyperparameters.","e4305cdf":"# Cross-Validation + Confusion Matrix function","868d6e82":"# Baseline:"}}