{"cell_type":{"3511943c":"code","d12505b8":"code","91a59a8c":"code","be4e57ca":"code","530edc52":"code","53ee72b4":"code","eb34b403":"code","c6bb4b3f":"code","3aa81a4a":"code","3ecd2c3d":"code","995830b6":"code","d3f2539e":"code","fdc81ba9":"code","393f3fbc":"code","a309d3b5":"code","cd2bb54f":"code","06a67a6c":"code","23318f94":"code","87790a27":"code","58cd8ace":"code","49265428":"code","71f8a61c":"markdown","f722dcac":"markdown","d96669aa":"markdown","4f8cd257":"markdown","f943caaa":"markdown","08c87b49":"markdown","f18c1329":"markdown","75f6d7e3":"markdown","e257e3f0":"markdown","5eb59dd2":"markdown","c4aef6f9":"markdown","21ca12d6":"markdown","25c215fc":"markdown","4c76dac9":"markdown","aedc12b5":"markdown","ade1d8ef":"markdown","e1a99f19":"markdown","db1e7fe8":"markdown"},"source":{"3511943c":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d12505b8":"assets = pd.read_csv('..\/input\/g-research-crypto-forecasting\/asset_details.csv')\nsup_train = pd.read_csv('..\/input\/g-research-crypto-forecasting\/supplemental_train.csv')\nsample = pd.read_csv('..\/input\/g-research-crypto-forecasting\/example_sample_submission.csv')\nex_test = pd.read_csv('..\/input\/g-research-crypto-forecasting\/example_test.csv')","91a59a8c":"train = pd.read_csv('..\/input\/g-research-crypto-forecasting\/train.csv')","be4e57ca":"train.head()","530edc52":"train.shape","53ee72b4":"train['timestamp'].nunique()","eb34b403":"assets","c6bb4b3f":"plt.subplots(figsize=(20,10))\nsns.barplot(x='Asset_Name', y='Weight', data=assets.sort_values(by=['Weight'],ascending=False))","3aa81a4a":"train[train['timestamp'] == 1514764860]","3ecd2c3d":"train.isnull().sum()","995830b6":"df_train = pd.merge(train, assets, how=\"left\", on=[\"Asset_ID\"])\ndf_train.head()","d3f2539e":"df = df_train.dropna()","fdc81ba9":"plt.subplots(figsize=(20,10))\nsns.countplot(x='Asset_Name', data = df)","393f3fbc":"df.sample(20)","a309d3b5":"plt.subplots(figsize=(20,10))\nsns.heatmap(df.corr())","cd2bb54f":"def reduce_mem_usage(df,do_categoricals=False):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            if do_categoricals==True:\n                df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))   \n    return df\n\ndtrain = reduce_mem_usage(df)","06a67a6c":"dtrain['fulldate'] = pd.to_datetime(dtrain['timestamp'], unit='s')\ndtrain['date'] = dtrain['fulldate'].apply(lambda d: d.date())\ndtrain['time'] = dtrain['fulldate'].apply(lambda d: d.time())\ndtrain.head()","23318f94":"dtrain_sample = dtrain.sample(10000)","87790a27":"dtrain_sample.head()","58cd8ace":"fig, axes = plt.subplots(7, 2, figsize=(20, 50))\nfor i,asset in enumerate(assets['Asset_Name']):\n    df_crypt = dtrain_sample[dtrain_sample['Asset_Name'] == asset]\n    sns.lineplot(x=\"date\", y=\"VWAP\", data=df_crypt, ax=axes[int(i\/2),i%2])\n    axes[int(i\/2),i%2].set_title(asset)","49265428":"print(\"Available data period\")\nfor i,asset in enumerate(assets['Asset_Name']):\n    df_crypt = dtrain[dtrain['Asset_Name'] == asset]\n    print( \"{} to {} ------> {}\".format(df_crypt.sort_values(by=['timestamp'])['date'].iloc[0], df_crypt.sort_values(by=['timestamp'])['date'].iloc[-1], asset ))","71f8a61c":"### Lets check missing values","f722dcac":"### Lets see total distribution per bitcoin","d96669aa":"### Comparatively there are less records in Maker, Dogecoin, IOTA & Monero","4f8cd257":"### Support the notebook if you like it. Upvote is FREE :)","f943caaa":"### Join Asset tables for Analysis.","08c87b49":"### Lets check how weights are distributed","f18c1329":"### Lets see the data distribution in each timestamp","75f6d7e3":"# Read data","e257e3f0":"### We have 24 Million records with 10 columns","5eb59dd2":"### We have total of 1.9 Million timestamps","c4aef6f9":"# Imports","21ca12d6":"### Correlation here is Obvious as Open, Close, High & Low, VWAP (avg volume) values should be very close within a timestamp. Please note that no signaificant correlation found for Target column","25c215fc":"# Vizualize","4c76dac9":"# EDA","aedc12b5":"### Lets explore assets csv file","ade1d8ef":"# Cleaning","e1a99f19":"Please note that there are total of 14 asset data (crypto currencies) given.\nFor a particular timestamp, we need not necessarily have a record of each asset. \nFrom above table, we have only 7 asset details. Target value for one of them is NaN. ","db1e7fe8":"# References\n\n1. [Let's Talk Validation: GroupTimeSeriesSplit](https:\/\/www.kaggle.com\/yamqwe\/let-s-talk-validation-grouptimeseriessplit\/notebook#References)"}}