{"cell_type":{"8084531a":"code","5149e457":"code","bb3d7949":"code","ef162187":"code","e06273ce":"code","5765553b":"code","6d52a7f9":"code","b687ddcf":"code","5467cd69":"code","d3bb99fb":"code","656678fa":"code","5c1df956":"code","4d931020":"code","99200a98":"code","84f5f8ed":"code","f197f881":"code","e79352bc":"code","2a8587da":"code","aa8e3e6d":"markdown","c82bff2b":"markdown","ee122b70":"markdown","f8072b97":"markdown","c2bf4250":"markdown"},"source":{"8084531a":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, GRU, LSTM, Dropout\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","5149e457":"num_words = 10000\nmax_review_len = 200","bb3d7949":"train = pd.read_csv('..\/input\/yelp-review-dataset\/yelp_review_polarity_csv\/train.csv',\n                   header=None,\n                   names=['Label', 'Review'])\ntest = pd.read_csv('..\/input\/yelp-review-dataset\/yelp_review_polarity_csv\/test.csv',\n                   header=None,\n                   names=['Label', 'Review'])","ef162187":"train","e06273ce":"#The number of label should be interpretated as: \n    #'2' - a good review \n    #'1' - a bad one. \n    \n#Let's change the encoding to the \n    #'1' - a good review \n    #'0' - a bad one\n\ny_train, y_test = train['Label'] - 1, test['Label'] - 1","5765553b":"reviews = train['Review']\nreviews[:5]","6d52a7f9":"tokenizer = Tokenizer(num_words=num_words)\ntokenizer.fit_on_texts(reviews)\ntokenizer.word_index","b687ddcf":"# Now we can transform the reviews to it's numerical value\nsequences = tokenizer.texts_to_sequences(reviews)","5467cd69":"# Let's try to match the dictionary values and the reviews\nindex = 42\nprint(reviews[index])\nprint(sequences[index])","d3bb99fb":"print(tokenizer.word_index['some'])\nprint(tokenizer.word_index['of'])\nprint(tokenizer.word_index['the'])\nprint(tokenizer.word_index['worst'])","656678fa":"# we should limit the sequnces length, the shorter reviews will be added by zeroes, the longer will be cut\nx_train = pad_sequences(sequences, maxlen=max_review_len)\nx_train[0]","5c1df956":"model = Sequential()\nmodel.add(Embedding(num_words, 64, input_length=max_review_len))\nmodel.add(GRU(128))\nmodel.add(Dense(1, activation='sigmoid'))","4d931020":"model.compile(optimizer='adam', \n              loss='binary_crossentropy', \n              metrics=['accuracy'])","99200a98":"model_save_path = '.\/best_model.h5'\ncheckpoint_callback = ModelCheckpoint(model_save_path, \n                                      monitor='val_accuracy',\n                                      save_best_only=True,\n                                      verbose=1)","84f5f8ed":"history = model.fit(x_train, \n                    y_train, \n                    epochs=5,\n                    batch_size=128,\n                    validation_split=0.1,\n                    callbacks=[checkpoint_callback])","f197f881":"model.load_weights(model_save_path)","e79352bc":"test_sequences = tokenizer.texts_to_sequences(test['Review'])\nx_test = pad_sequences(test_sequences, maxlen=max_review_len)\nx_test[0]","2a8587da":"scores = model.evaluate(x_test, y_test, verbose=1)\nprint(\"The percent of correct answers:\", round(scores[1] * 100, 4))","aa8e3e6d":"Now it's time to process our test data. Make sure that you fit test sequnces on the same tokenizer that you used for the train data","c82bff2b":"The column names are omitted, so we will assign them manualy","ee122b70":"This client was very upset about his pizza but we should be glad - our dictionary values matches the sequences","f8072b97":"It was my first NLP RNN so I guess I can be glad with the result","c2bf4250":"Let's create our **tokenizer**. \n\nTensorflow has a very convinient class for this task (details are [here](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/text\/Tokenizer)). We are to fit this tokenizer on our training reviews. After the fitting we recieve a so-called frequency dictionary(key - word, value - frequency). Higher the frequency - lower the value:"}}