{"cell_type":{"865e16e8":"code","9ea6e721":"code","50511e25":"code","8dba0df2":"code","b9aa7f9a":"code","5c55683a":"code","ca3a6e45":"code","73f36b29":"code","9aeea6c6":"code","4750c59e":"code","d5815205":"code","29a3d08f":"code","2f03831b":"code","d490e6db":"code","f57e475b":"code","367c79af":"code","620c3362":"code","725877fe":"code","7009d63e":"code","4652a4d4":"code","f5ef39b7":"code","fad3ecf1":"code","f22867a2":"code","4ccafe4c":"code","a394481f":"code","1e772af0":"code","41c82b69":"code","f88c6330":"code","8763b3ec":"code","178a1246":"code","2aaceae3":"code","b98081a2":"code","66319cc4":"code","8c619ddc":"code","93a14d21":"code","c0054f81":"code","73f371c0":"code","3b7d271d":"markdown","a5d03be1":"markdown","7d99ebe5":"markdown","a5974a7f":"markdown","c8a9708d":"markdown","f87177a8":"markdown","916b18f9":"markdown","55e834e5":"markdown"},"source":{"865e16e8":"# packages\n\n# standard\nimport numpy as np\nimport pandas as pd\nimport time\n\n# plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# machine learning tools\nimport h2o\nfrom h2o.estimators import H2OGeneralizedLinearEstimator\nfrom h2o.estimators import H2ORandomForestEstimator\nfrom h2o.estimators import H2OGradientBoostingEstimator","9ea6e721":"pd.set_option('display.max_columns', None) # show all columns in data frames","50511e25":"# load data + first glance\nt1 = time.time()\ndf_train = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\ndf_sub = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\nt2 = time.time()\nprint('Elapsed Time[s]:', np.round(t2-t1,4))","8dba0df2":"# first glance (training data)\ndf_train.head()","b9aa7f9a":"# dimensions\nprint('Train Set:', df_train.shape)\nprint('Test Set :', df_test.shape)","5c55683a":"df_train.info(verbose=True, show_counts=True)","ca3a6e45":"df_test.info(verbose=True, show_counts=True)","73f36b29":"df_train.claim.value_counts().plot(kind='bar')\nplt.title('Target: claim')\nplt.grid()\nplt.show()","9aeea6c6":"df_train['nan_count'] = df_train.isnull().sum(axis=1)\ndf_test['nan_count'] = df_test.isnull().sum(axis=1)","4750c59e":"print(df_train.nan_count.value_counts())\ndf_train.nan_count.value_counts().plot(kind='bar')\nplt.title('NaN count - Training')\nplt.grid()\nplt.show()","d5815205":"print(df_test.nan_count.value_counts())\ndf_test.nan_count.value_counts().plot(kind='bar')\nplt.title('NaN count - Test')\nplt.grid()\nplt.show()","29a3d08f":"# cross table - absolute counts...\nctab = pd.crosstab(df_train.claim, df_train.nan_count)\nctab","2f03831b":"# ...and normalized by column\nctab_norm = ctab \/ ctab.sum()\nctab_norm","d490e6db":"# visualize\nplt.figure(figsize=(14,5))\np1 = plt.bar(ctab_norm.columns, ctab_norm.iloc[0])\nbot = ctab_norm.iloc[0]\np2 = plt.bar(ctab_norm.columns, ctab_norm.iloc[1], bottom=bot)\nplt.xlabel('nan_count')\nplt.ylabel('Relative Frequency of Target Classes')\nplt.title('Target vs nan_count')\nplt.legend((p1[0],p2[0]), ('0', '1'))\nplt.grid()\nplt.show()","f57e475b":"# numerical features\nfeatures_num = ['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10',\n                'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20',\n                'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30',\n                'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40',\n                'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50',\n                'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60',\n                'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70',\n                'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80',\n                'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90',\n                'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', \n                'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109',\n                'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118',\n                'nan_count']","367c79af":"# basic stats\ndf_train[features_num].describe()","620c3362":"# cross table - target vs example feature\nexample_feature = 'f48'\n# discretize feature first (quantile based to achieve balanced bucket sizes)\ndf_train['temp'] = pd.qcut(df_train[example_feature],10)","725877fe":"# calc crosstab\nctab = pd.crosstab(df_train.claim, df_train.temp)\n# ...and normalized by column\nctab_norm = ctab \/ ctab.sum()\nctab_norm","7009d63e":"# visualize\nxx = list(map(str,ctab_norm.columns)) # convert intervals to strings\nyy1 = ctab_norm.iloc[0].values\nyy2 = ctab_norm.iloc[1].values\n\nplt.figure(figsize=(14,5))\np1 = plt.bar(xx, yy1)\nbot = ctab_norm.iloc[0]\np2 = plt.bar(xx, yy2, bottom=bot)\nplt.xlabel('')\nplt.ylabel('Relative Frequency of Target Classes')\nplt.title('Target vs ' + example_feature)\nplt.legend((p1[0],p2[0]), ('0', '1'))\nplt.xticks(rotation=90)\nplt.grid()\nplt.show()","4652a4d4":"# start H2O\nh2o.init(max_mem_size='12G', nthreads=4) # Use maximum of 12 GB RAM and 4 cores","f5ef39b7":"# upload data frames in H2O environment\nt1 = time.time()\ntrain_hex = h2o.H2OFrame(df_train)\ntest_hex = h2o.H2OFrame(df_test)\nt2 = time.time()\nprint('Elapsed time[s]: ', np.round(t2-t1,4))","fad3ecf1":"# force categorical target\ntrain_hex['claim'] = train_hex['claim'].asfactor()","f22867a2":"# define predictors and target\npredictors = features_num\ntarget = 'claim'","4ccafe4c":"# define GBM model\nn_cv = 5\nfit_1 = H2OGradientBoostingEstimator(ntrees=300,\n                                     learn_rate=0.05,\n                                     max_depth=6,\n                                     min_rows=5,\n                                     sample_rate=0.5, # sample rows\n                                     col_sample_rate=0.5, # sample columns\n                                     nfolds=n_cv,\n                                     score_each_iteration=True,\n                                     #stopping_metric='auc',\n                                     #stopping_rounds=5,\n                                     #stopping_tolerance=0.001, # default 0.001\n                                     seed=999)\n\n# train model\nt1 = time.time()\nfit_1.train(x=predictors,\n            y=target,\n            training_frame=train_hex)\nt2 = time.time()\n\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","a394481f":"# scoring history (training)\nfit_1.plot()","1e772af0":"# show scoring history - training vs cross validations\nfor i in range(n_cv):\n    cv_model_temp = fit_1.cross_validation_models()[i]\n    df_cv_score_history = cv_model_temp.score_history()\n    my_title = 'CV ' + str(1+i) + ' - Scoring History'\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.training_auc, \n                c='blue', label='training')\n    plt.ylim(0.75,0.85)\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.validation_auc, \n                c='darkorange', label='validation')\n    plt.title(my_title)\n    plt.xlabel('Number of Trees')\n    plt.legend()\n    plt.grid()\n    plt.show()","41c82b69":"# cross validation metrics\nfit_1.cross_validation_metrics_summary()","f88c6330":"# training performance\nperf_train = fit_1.model_performance(train=True)\nperf_train.plot()","8763b3ec":"# CV performance\nperf_cv = fit_1.model_performance(xval=True)\nperf_cv.plot()","178a1246":"# variable importance\nfit_1.varimp_plot(25)","2aaceae3":"# predict on training data\npred_train = fit_1.predict(train_hex)\n# add actual target\npred_train['target'] = train_hex[target]\npred_train = pred_train.as_data_frame()\npred_train.head()","b98081a2":"# check calibration\nn_actual = sum(df_train.claim)\nn_pred = sum(pred_train.p1)\nn_train = df_train.shape[0]\n\nprint('Actual Frequency    :', n_actual)\nprint('Predicted Frequency :', n_pred)\nprint('Calibration Ratio   :', n_pred \/ n_actual)\nprint('Train Set Size      :', n_train)\nprint('Predicted Ratio     :', n_pred\/n_train)","66319cc4":"# plot probabilities\nplt.figure(figsize=(8,4))\nplt.hist(pred_train.p1, bins=50)\nplt.title('Predictions on Train Set')\nplt.grid()\nplt.show()","8c619ddc":"# predict on test set\npred_test = fit_1.predict(test_hex)\npred_test = pred_test.as_data_frame()\npred_test.head()","93a14d21":"plt.figure(figsize=(8,4))\nplt.hist(pred_test.p1, bins=50)\nplt.title('Predictions on Test Set')\nplt.grid()\nplt.show()","c0054f81":"# frequency check\nn_test_pred = np.round(pred_test.p1.sum(),2)\nn_test = df_test.shape[0]\nprint('Predicted count:', n_test_pred)\nprint('Test Set Size:',n_test)\nprint('Predicted Ratio:', n_test_pred\/n_test)# frequency check","73f371c0":"# submission\ndf_sub_1 = df_sub.copy()\ndf_sub_1.claim = pred_test.p1\ndisplay(df_sub_1.head())\n# save to file\ndf_sub_1.to_csv('submission_1.csv', index=False)","3b7d271d":"# Table of Contents\n* [Target Exploration](#1)\n* [Feature Engineering](#2)\n* [Features EDA](#3)\n* [Build GBM Model](#4)\n* [Evaluate Model](#5)","a5d03be1":"<a id='4'><\/a>\n# Build GBM Model","7d99ebe5":"### Target vs Feature","a5974a7f":"### Show impact of nan_count on target:","c8a9708d":"<a id='2'><\/a>\n# Feature Engineering\n### Credits to https:\/\/www.kaggle.com\/c\/tabular-playground-series-sep-2021\/discussion\/270206","f87177a8":"<a id='3'><\/a>\n# Features EDA","916b18f9":"<a id='1'><\/a>\n# Target Exploration","55e834e5":"<a id='5'><\/a>\n# Evaluate Model"}}