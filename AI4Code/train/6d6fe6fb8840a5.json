{"cell_type":{"b537a5e4":"code","ea751aec":"code","56a8076b":"code","15e5d2a5":"code","4fcff169":"code","8bcc6e79":"code","fd904c18":"code","d821cd6e":"code","71fff28e":"code","ba358417":"code","a20b15ef":"code","3057876e":"code","89965ddc":"code","68a1650a":"code","6553ced9":"code","e29ee455":"code","721b107e":"code","03a0d9dc":"code","cf47b140":"code","184d7b66":"code","99bf86bd":"markdown"},"source":{"b537a5e4":"import os\nimport sys\nimport numpy as np\nimport pandas as pd\nfrom time import time\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\nfrom torch.utils.data import TensorDataset\nfrom torchvision import datasets, transforms","ea751aec":"print(\"CUDA Available:\",torch.cuda.is_available())\nprint(\"CUDA Device Name:\",torch.cuda.get_device_name(0))","56a8076b":"# let's load our train dataframe\ntrain_df = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\n\n# let's load our test dataframe\ntest_df = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")","15e5d2a5":"train_df.head() # shows first 5 rows of train_df, label column holds label for the images whose data is in pixel0 to pixel 783 of size(28*28)","4fcff169":"test_df.head() # shows first 5 rows of test_df, notice how test doesn't have label column as this dataframe will be used for testing our model","8bcc6e79":"labels = train_df.label # labels for images in train_df, we'll drop this column to create a dataset that's ready to train\nlabels","fd904c18":"train_df = train_df.drop(\"label\", axis = 1) # dropping column \"label\" from the dataframe","d821cd6e":"# shows the first image, first the data of row is converted to array and then resahped to 28*28\nplt.imshow(train_df[0:1].values.reshape((28,28)))","71fff28e":"# Convert the data into model feedable dataset where 1st half is the image data and \n# 2nd is its label(pandas series needs to be converted to numpy array first before converted to torch tensor)\ntrain_ds = TensorDataset( torch.tensor( train_df.values), torch.tensor( labels.to_numpy()))\n# This one will be dataframe we'll test our model on\ntest_ds = torch.tensor(test_df.values)","ba358417":"len(train_ds)","a20b15ef":"# Now we wish to divide our 42000 elements strong dataset into 32000(training)+10000(validation)\n# A validation dataset is a sample of data held back from training your model that is used \n# to give an estimate of model skill while tuning model\u2019s hyperparameters.\n\n# The validation dataset is different from the test dataset that is also held back from \n# the training of the model, but is instead used to give an unbiased estimate of the skill\n# of the final tuned model when comparing or selecting between final models.\n\n# Simple, right?\ntrain_ds , val_ds = random_split( train_ds,(32000,10000)) ","3057876e":"# Define hyperparameters\nbatch_size = 150 # Batch size refers to the number of training examples utilized in one iteration. \n\n# learning rate is a configurable hyperparameter used in the training of neural networks\n# controls how quickly the model is adapted to the problem.\nlearning_rate = 0.01 \n\n# for each image\ninput_size = 784 # no of datapoints in a image\nnum_class = 10 # no of classes that image can be classified into","89965ddc":"# loads the train_ds, valid_ds and test_ds, we divide datas according to batch_size but notice how we only \n# shuffle train_ds and not valid_ds or test_ds cuz they are for checking not training\ntrain_loader = DataLoader(train_ds , batch_size, shuffle = True)\nval_loader = DataLoader(val_ds , batch_size, shuffle = False)\ntest_loader = DataLoader(test_ds , batch_size, shuffle = False)","68a1650a":"dataiter = iter(train_loader) # We can iterate over train_loader to get batches of data\nimages, labels = dataiter.next()\n\nprint(images.shape) # batch of 150 with 784 (28x28) data points\nprint(labels.shape) # corresponding 150 labels ","6553ced9":"# means CNN inherits from nn.Module\nclass CNN(nn.Module):\n    def __init__(self, in_channels=1, num_classes=10):\n        super(CNN, self).__init__() #  super is so that child classes that may be using cooperative multiple inheritance will call the correct next parent class function in the Method Resolution Order (MRO).\n        # Idk what this super does honestly, after this is a bunch of shady stuff that I can't explain, sorry dear\n        self.conv1 = nn.Conv2d(in_channels=in_channels,out_channels=8,kernel_size=(3,3),stride=(1,1),padding=(1,1))\n        self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3,3), stride=(1,1),padding=(1,1),)\n        self.fc1 = nn.Linear(16 * 7 * 7, num_classes)\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        x = x.reshape(x.shape[0], -1)\n        x = self.fc1(x)\n        return x\nmodel = CNN().to('cuda')\n\n#optimizer\nopt = torch.optim.Adam # our optimiser is adam, combination of rms prop and momentum\nopt = opt(model.parameters(),lr=learning_rate) # adam needs learning rate parameter","e29ee455":"# Function that's used to fit on the data\ndef fit(epochs,model,data):\n    loss_fun = F.cross_entropy # our loss function is cross_entropy\n    hist = [] # we want to store loss here\n    # for each epoch\n    for Epoch in range(epochs):\n        # for each batch in that data\n        for img,label in data:\n            img = img.reshape(img.shape[0],1,28,28) # \n            # print(img.shape) # torch.Size([150, 1, 28, 28]) this is full batch right here\n            img = img.to('cuda') \n            label = label.to('cuda')\n            out = model(img\/255) # forward propagation\n            loss = loss_fun(out,label) # loss calculation by comparision with prediction and accurate one \n            loss.backward() # backward propagation\n            opt.step() # function can be called once the gradients are computed using e.g. backward(). and it updates parameters\n            opt.zero_grad() # zero out of calculated gradients before rinse repeat\n            hist.append(loss)\n        \n        # print loss after every 10 epochs till epoch no 200\n        if (Epoch+1)%10 ==0:\n            print(f\"Epoch:[{Epoch+1}\/{epochs}] ; Loss : {loss}\")\n    return hist","721b107e":"hist=fit(200,model,train_loader)","03a0d9dc":"# This is where we check our accuracy for the validation data\ndef acc(data):\n    accy=[] # we will run this on multiple batches from validation, those will be stored in this\n    for img ,label in data:\n        img = img.reshape(img.shape[0],1,28,28) # batch reshape\n        img, label = img.to('cuda'), label.to('cuda')\n        out = model(img\/255) #forward propagation\n        # print(\"Forward:\",out[0])\n        out = F.softmax(out) # To get the prediction and confidence in different classes\n        # print(\"Softmax:\",out[0])\n        _,pred_index = torch.max(out, dim=1) # find max in those confidences\n        # print(\"Pred_ Index:\",pred_index)\n        x=torch.sum(pred_index == label)\/len(pred_index) # calculate no of true positives diveded by total cases\n        # print(x)\n        x = x*100 # conversion to 1-100\n        x = x.to(\"cpu\").numpy()\n        accy.append(x) # append the accuracies\n        break\n    return np.mean(accy)  \n\nprint(\"Final Mean Accuracy:\",acc(val_loader))","cf47b140":"plt.imshow(test_ds[2].reshape(28,28).numpy()) # this no 2 image is 9 right, let's predict it in next cell","184d7b66":"print(\"Test Dataset shape:\",test_ds.shape)\nprint(\"Single Input fit:\",test_ds[0].reshape(1,784).shape)\n\ndef prediction(data):\n    LABEL=[] # output label\n    data = data.reshape(data.shape[0],1,28,28) # input reshape\n    data = data.to('cuda')\n    out = model(data\/255) # forward propagation\n    out = F.softmax(out) # softmax on forward prop\n    _,pred_index =torch.max(out,dim=1) # find max confidence one label\n    LABEL.append(pred_index) # save that label\n    return LABEL\n\nx = prediction(test_ds[2].reshape(1,784)) # input to get predictions \nx # our prediction as we can see is correct. # x=x[0].to('cpu').numpy() can be useful when using full arrays","99bf86bd":"# Project Description\n- Name: Simple MNIST in Pytorch\n- Description: \n  - MNIST (\"Modified National Institute of Standards and Technology\") is the de facto \u201chello world\u201d dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike. Goal is to correctly identify digits from a dataset of tens of thousands of handwritten images.\n- Accelerator: Tesla P100-PCIE-16GB\n"}}