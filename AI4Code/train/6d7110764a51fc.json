{"cell_type":{"e4694522":"code","3cd2c67f":"code","2e72afcf":"code","42b3a8cc":"code","06771954":"code","227838d7":"code","9bdd410f":"code","53a31ce0":"code","fa138ff8":"code","6912983c":"code","59fbf1b3":"code","a7289a9f":"code","809280a7":"code","d1040c41":"code","2a59e864":"code","0c875793":"markdown","60cf40b0":"markdown","a6de0074":"markdown","1c40171b":"markdown","f44cabfc":"markdown","5bf6410b":"markdown","d35bfd66":"markdown"},"source":{"e4694522":"dat = pd.DataFrame([\"10,000\", \"20,000\", \"$20,500\",\"\u00a310,000\", '\u00a3500', '$90,000','hi'], columns=['money'])\ndat","3cd2c67f":"dat['money'] = dat['money'].str.replace(r'[,$\u00a3]','')\ndat","2e72afcf":"val = pd.to_numeric(dat['money'], errors='coerce')\nval","42b3a8cc":"dat[val.isnull()]","06771954":"import pandas as pd\nimport numpy as np\nimport warnings\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings('ignore')","227838d7":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9bdd410f":"data = pd.read_csv('\/kaggle\/input\/youtube-new\/GBvideos.csv')\ndata.head(3)","53a31ce0":"data.drop_duplicates(inplace=True)\n\nprint('There are {} rows in GBvideos.csv removing duplications'.format(data.shape[0]))","fa138ff8":"temp = data[['video_id','likes','dislikes']].groupby(by='video_id').agg('max')\nx = temp.query(\"dislikes > likes\").shape[0]\n\nprint(\"{} have dislikes more than likes\".format(x))","6912983c":"temp_1 = data.query(\"trending_date == '18.22.01' and comment_count > 10000\").shape[0]\n\nprint(\"{} VDO that are trending on 22 Jan 2018 with comments more than 10,000 comments\".format(x))","59fbf1b3":"data.groupby(by='trending_date')[['comment_count']].agg('mean').sort_values(by='comment_count', ascending=True).head()","a7289a9f":"print(\"17.5.11 has the minimum average number of comments per VDO\")","809280a7":"import json\n\nwith open('\/kaggle\/input\/youtube-new\/GB_category_id.json') as file:\n  category = json.load(file)\n\ncat = dict()\n\nfor i in range(len(category['items'])):\n  key = int(category['items'][i]['id'])\n  value = category['items'][i]['snippet']['title']\n\n  cat[key] = value\n\ndata['category_id'] =  data['category_id'].map(cat)","d1040c41":"temp = data[data['category_id'].isin(['Sports', 'Comedy'])]\n\ntemp_2 = pd.pivot_table(\n                    temp,\n                    index = ['trending_date'],\n                    columns = ['category_id'],\n                    values = ['views'],\n                    aggfunc = 'sum'\n                )\n\ntemp_2.head()","2a59e864":"mask = temp_2[('views','Comedy')] > temp_2[('views','Sports')]\n\nout = temp_2.loc[~mask,:]\n\nprint('{} days that there are more total daily views of VDO in \"Sports\" category than in \"Comedy\" category'.format(out.shape[0]))","0c875793":"## How many rows are there in the GBvideos.csv after removing duplications?","60cf40b0":"## Compare \"Sports\" and \"Comedy\", how many days that there are more total daily views of VDO in \"Sports\" category than in \"Comedy\" category?","a6de0074":"## Which date that has the minimum average number of comments per VDO?","1c40171b":"## How many VDO that have \"dislikes\" more than \"likes\"?  Make sure that you count only unique title!","f44cabfc":"## How many VDO that are trending on 22 Jan 2018 with comments more than 10,000 comments?","5bf6410b":"To simplify data retrieval process on Colab, we heck if we are in the Colab environment and download data files from a shared drive and save them in folder \"data\".\n\nFor those using jupyter notebook on the local computer, you can read data directly assuming you save data in the folder \"data\".","d35bfd66":"# Assignment - Basic Pandas\n<sup>Created by Natawut Nupairoj, Department of Computer Engineering, Chulalongkorn University<\/sup>\n\nUsing pandas to explore youtube trending data from GB (GBvideos.csv and GB_category_id.json) and answer the questions."}}