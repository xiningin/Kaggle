{"cell_type":{"b899d757":"code","f06ebf4d":"code","17d8a01c":"code","b8f137e4":"code","04538423":"code","d214e10e":"code","d6175d6d":"code","61437280":"code","cd171e33":"code","3286234d":"code","7e0c0311":"code","7a4f6310":"code","8465f91b":"code","7a742c6f":"code","a27e491b":"code","b75dfff7":"code","d68ad289":"code","066ea9db":"code","1b541bf2":"code","e06cdfa4":"code","4f25a8a0":"code","48d8f8ae":"code","dbdce715":"code","40f085aa":"code","9739041d":"code","6ad6fa62":"code","1cc69f48":"code","541d47e8":"code","c85cf9e6":"code","e80f1b01":"code","e337c53b":"code","f301591b":"code","4341afc5":"code","0522dd55":"code","b1ddf0e9":"code","4be36826":"code","042af241":"code","b382b1cf":"code","f94dbe4c":"code","015bb0ee":"code","f8f64032":"code","b657fa1c":"code","c74c9e66":"code","044f0455":"markdown","11e30e67":"markdown","82f950df":"markdown","35c0f242":"markdown","b8b0d7cd":"markdown","7f780b1d":"markdown","779db27c":"markdown","f7896e62":"markdown","525505f3":"markdown","7aa51477":"markdown","d78a5211":"markdown","b6310edc":"markdown","3defe3b2":"markdown","085fe758":"markdown","7c6577f1":"markdown","8a0a6265":"markdown","307073f3":"markdown","185abe94":"markdown","c984902a":"markdown","7516a98a":"markdown","87ac27a7":"markdown","9d149bf1":"markdown","ed74b3e4":"markdown","ddcb9b6d":"markdown","22b38fdc":"markdown","5efb7cdb":"markdown","3bda77b4":"markdown","687a3754":"markdown","3b593a19":"markdown","7c9f8dc5":"markdown","87cb1b52":"markdown","a79ccaf5":"markdown","dd4c8c08":"markdown","6d7bb5dc":"markdown","8e7a751f":"markdown","8d7fca87":"markdown","e769c3bf":"markdown","fee6826d":"markdown","6d7d4216":"markdown","a4fef5a3":"markdown","84422e37":"markdown","5d1b4a9f":"markdown","752471c1":"markdown","94b0076b":"markdown","0769928c":"markdown","8cfd0787":"markdown"},"source":{"b899d757":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f06ebf4d":"#import some of the necessary libraries\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy.stats import norm\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(15,12)})\nimport matplotlib.pyplot as plt\n%matplotlib inline","17d8a01c":"train_df = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","b8f137e4":"train_df.head(10)","04538423":"train_df.columns","d214e10e":"len(train_df.columns)","d6175d6d":"train_df['SalePrice'].describe()","61437280":"train_df.shape","cd171e33":"train_ID = train_df['Id']\ntest_ID = test_df['Id']\ntrain_df.drop(\"Id\", axis = 1, inplace = True)\ntest_df.drop(\"Id\", axis = 1, inplace = True)\n#Deleting outliers\ntrain_df = train_df.drop(train_df[(train_df['GrLivArea']>4000) & (train_df['SalePrice']<300000)].index)","3286234d":"sns.set(rc={'figure.figsize':(18,8)})\nsns.distplot(train_df['SalePrice'],fit=norm)\n\n(mu, sig) = norm.fit(train_df['SalePrice'])\n#Now plot the distribution\nplt.legend(['Normal Distribution Curve ($\\mu=$ {:.2f} & $\\sigma=$ {:.2f} )'.format(mu, sig)])\nplt.ylabel('Frequency')\nplt.show()","7e0c0311":"print(\"Skewness of Sale Price is: \",train_df['SalePrice'].skew())","7a4f6310":"train_df['SalePrice'] = np.log(train_df['SalePrice']+1)\nsns.distplot(train_df['SalePrice'],fit=norm)\n\n(mu, sig) = norm.fit(train_df['SalePrice'])\n#Now plot the distribution\nplt.legend(['Normal Distribution Curve ($\\mu=$ {:.2f} & $\\sigma=$ {:.2f} )'.format(mu, sig)])\nplt.ylabel('Frequency')\nplt.show()","8465f91b":"print(\"Skewness of Sale Price is: \",train_df['SalePrice'].skew())","7a742c6f":"corremap = train_df.corr()\nplt.subplots(figsize=(15,12))\nsns.heatmap(corremap, vmax=0.9, square=True)","a27e491b":"sns.set()\ncolumns = ['OverallQual', 'GrLivArea', 'TotalBsmtSF','GarageCars', 'GarageArea','1stFlrSF', 'FullBath', 'YearBuilt','SalePrice']\nsns.pairplot(train_df[columns], size = 2)\nplt.show();","b75dfff7":"var = 'GrLivArea'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\ndata.plot.scatter(var,'SalePrice');","d68ad289":"#scatter plot grlivarea\/saleprice\nvar = 'TotalBsmtSF'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\ndata.plot.scatter(var,'SalePrice');","066ea9db":"#scatter plot grlivarea\/saleprice\nvar = 'GarageArea'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\ndata.plot.scatter(var,'SalePrice');","1b541bf2":"#scatter plot grlivarea\/saleprice\nvar = '1stFlrSF'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\ndata.plot.scatter(var, 'SalePrice');","e06cdfa4":"#box plot overallqual\/saleprice\nvar = 'OverallQual'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\nsns.boxplot(x=var,y='SalePrice',hue=var,data=data)","4f25a8a0":"#box plot overallqual\/saleprice\nf, ax = plt.subplots(figsize=(20, 16))\nplt.xticks(rotation='90')\nvar = 'YearBuilt'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\nsns.boxplot(x=var,y='SalePrice',data=data)","48d8f8ae":"#box plot overallqual\/saleprice\nvar = 'GarageCars'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\nsns.violinplot(x=var,y='SalePrice',data=data,palette='rainbow', hue = 'GarageCars')","dbdce715":"var = 'FullBath'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\nsns.violinplot(x=var,y='SalePrice',data=data,palette='rainbow', hue = 'FullBath')","40f085aa":"train_df.shape","9739041d":"ntrain = train_df.shape[0]\nntest = test_df.shape[0]\ny_train = train_df.SalePrice.values\ncomp_data = pd.concat((train_df, test_df)).reset_index(drop=True)\ncomp_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"Comp_data size is : {}\".format(comp_data.shape))","6ad6fa62":"missing_val = comp_data.isnull().sum().sort_values(ascending=False)\nmissing_val_df = pd.DataFrame({'Feature':missing_val.index, 'Count':missing_val.values})\nmissing_val_df = missing_val_df.drop(missing_val_df[missing_val_df.Count == 0].index)\nmissing_val_df","1cc69f48":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='60')\nplt.title('Count of Missing Data Per Feature', fontsize=15)\nsns.barplot(x = 'Feature', y = 'Count', data = missing_val_df,\n            palette = 'cool', edgecolor = 'b')","541d47e8":"comp_data[\"LotFrontage\"] = comp_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    comp_data[col] = comp_data[col].fillna(0)\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    comp_data[col] = comp_data[col].fillna(0)\ncomp_data['MSZoning'] = comp_data['MSZoning'].fillna(comp_data['MSZoning'].mode()[0])\ncomp_data[\"MasVnrArea\"] = comp_data[\"MasVnrArea\"].fillna(0)\ncomp_data['Electrical'] = comp_data['Electrical'].fillna(comp_data['Electrical'].mode()[0])\ncomp_data['SaleType'] = comp_data['SaleType'].fillna(comp_data['SaleType'].mode()[0])\ncomp_data['KitchenQual'] = comp_data['KitchenQual'].fillna(comp_data['KitchenQual'].mode()[0])\ncomp_data['Exterior1st'] = comp_data['Exterior1st'].fillna(comp_data['Exterior1st'].mode()[0])\ncomp_data['Exterior2nd'] = comp_data['Exterior2nd'].fillna(comp_data['Exterior2nd'].mode()[0])","c85cf9e6":"for col in ('PoolQC', 'MiscFeature', 'Alley'):\n    comp_data[col] = comp_data[col].fillna('None')\ncomp_data[\"MasVnrType\"] = comp_data[\"MasVnrType\"].fillna(\"None\")\ncomp_data[\"Fence\"] = comp_data[\"Fence\"].fillna(\"None\")\ncomp_data[\"FireplaceQu\"] = comp_data[\"FireplaceQu\"].fillna(\"None\")\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    comp_data[col] = comp_data[col].fillna('None')\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    comp_data[col] = comp_data[col].fillna('None')\ncomp_data['MSSubClass'] = comp_data['MSSubClass'].fillna(\"None\")\ncomp_data['SaleType'] = comp_data['SaleType'].fillna(comp_data['SaleType'].mode()[0])\ncomp_data = comp_data.drop(['Utilities'], axis=1)\ncomp_data['OverallCond'] = comp_data['OverallCond'].astype(str)\ncomp_data[\"Functional\"] = comp_data[\"Functional\"].fillna(\"Typ\")  \n","e80f1b01":"comp_data['MSSubClass'] = comp_data['MSSubClass'].apply(str)\ncomp_data['YrSold'] = comp_data['YrSold'].astype(str)\ncomp_data['MoSold'] = comp_data['MoSold'].astype(str)\nfrom sklearn.preprocessing import LabelEncoder\ncolumns = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\nfor col in columns:\n    labl = LabelEncoder() \n    labl.fit(list(comp_data[col].values)) \n    comp_data[col] = labl.transform(list(comp_data[col].values))     \nprint('Shape all_data: {}'.format(comp_data.shape))\ncomp_data['TotalSF'] = comp_data['TotalBsmtSF'] + comp_data['1stFlrSF'] + comp_data['2ndFlrSF']","e337c53b":"comp_data.shape","f301591b":"from scipy.stats import norm, skew\nnumeric_feats = comp_data.dtypes[comp_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = comp_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","4341afc5":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    comp_data[feat] = boxcox1p(comp_data[feat], lam)","0522dd55":"comp_data = pd.get_dummies(comp_data)\nprint(comp_data.shape)","b1ddf0e9":"train_df = comp_data[:ntrain]\ntest_df = comp_data[ntrain:]","4be36826":"train_df.shape","042af241":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\nn_folds = 5\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train_df.values)\n    rmse= np.sqrt(-cross_val_score(model, train_df.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","b382b1cf":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import Ridge,ElasticNet,Lasso\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.kernel_ridge import KernelRidge\nimport xgboost as xgb\nimport lightgbm as lgb\n\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\nscore = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","f94dbe4c":"from sklearn.metrics import mean_squared_error\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","015bb0ee":"model_xgb.fit(train_df, y_train)\nxgb_train_pred = model_xgb.predict(train_df)\nxgb_pred = np.expm1(model_xgb.predict(test_df))\nprint(rmsle(y_train, xgb_train_pred))","f8f64032":"model_lgb.fit(train_df, y_train)\nlgb_train_pred = model_lgb.predict(train_df)\nlgb_pred = np.expm1(model_lgb.predict(test_df.values))\nprint(rmsle(y_train, lgb_train_pred))","b657fa1c":"ensemble = xgb_pred*0.5 + lgb_pred*0.5","c74c9e66":"submission = pd.DataFrame()\nsubmission['Id'] = test_ID\nsubmission['SalePrice'] = ensemble\n\nprint(\"Creating Submission File\")\nsubmission.to_csv(\"submission.csv\", index=False)","044f0455":"Let's Concat the Training and the Test Data to a Complete Dataframe. This has to be done because both our training and testing data might contain missing values, outliers and may require categorical features to be handelled. \n\nTherefore rather than doing them seperatly we can do it together to save our time, so that we can waste the time somewhere else!","11e30e67":"**2. Handling Missing Values in Numerical Data**","82f950df":"Now Let's See the Relationship Between the Predictors and the \"SalePrice\"\n\n**Let's See who makes the Best Couple?**","35c0f242":"**Conclusion**\n\nThat's it! We reached the end of our exercise. We Saw how a Simple Model gave us Great Results!\n\nIf you liked the Kernal then don't forget to hit the Upvote! :) Also, Suggestion are always Welcomed! Post your Doubts and Suggestions on the Comment Section.","b8b0d7cd":"Here, **SalePrice** is what we have to Predict. So we will first start with Checking the Distribution of the Variable and Let's see how much Skewness it has got.","7f780b1d":"**Box Cox Transformation on Skewed Features**","779db27c":"**By Analyzing the Graph we can see the following:**\n\n* Deviation from the Normal Distribution.\n* Have Positive Skewness.\n* Show Peakedness\n\nAlso let's see the measurement of Skewness: ","f7896e62":"**References**\n\n* [COMPREHENSIVE DATA EXPLORATION WITH PYTHON](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python)","525505f3":"**From the Correlation Map, We Got: **\n\n* OverallQual, GrLivArea, TotalBsmtSF, GarrageCars, GarrageArea, 1stFlrSF, YearBuilt, FullBath are the most important Predictors.\n* We can see from the above graph that how significantly they are related to our output variable \"SalePrice\"","7aa51477":"**Analyze the Above Results Well!**","d78a5211":"Now let's have the Training and Testing ID's  aved in a dataframe for future references. As you know for any machine learning based problem the Id doesn't make a feature, so we are going to drop the ID column from our train and test dataframe.","b6310edc":"**Let's Analyze others as well!**","3defe3b2":"**Let's Create the Folds for Our Cross Validation Model**","085fe758":"In this notebook, let us try and explore the data given for **House Prices: Advanced Regression Techniques**. Before we dive deep into the data, let us know a little more about the competition.\n\n**What decides a House Price?**\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But the ral world experiments proves that price negotiations are much more dependent on other **Factors** rather than the number of bedrooms or a white-picket fence.\n\n**Objective:**\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.","7c6577f1":"Let's do a little exploration on the training file.","8a0a6265":"**Final Training and Prediction**","307073f3":"**Now, Let's do Some More Feature Analysis**\n\nThe First and Foremost Important thing is to See that What are the Relevant Features. Not all features might be useful for our prediction and Having all the unnecessary features is going to make our model complex and we don't want the dimensionality to be huge!","185abe94":"* XGBoost","c984902a":"**Our Feature Engineering begins with Handling Missing Data!**","7516a98a":"'TotalBsmtSF' also have a great bond with 'SalePrice' but this seems a much more emotional relationship! Everything is ok and suddenly, in a strong linear (exponential?) reaction, everything changes. We can call it a **Mood Swing!** Moreover, it's clear that sometimes 'TotalBsmtSF' closes in itself and gives no credit to 'SalePrice'.","87ac27a7":"**According to our tarot card, these are the variables most correlated with 'SalePrice'**\n\n* 'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'. \n* 'GarageCars' and 'GarageArea' are also some of the most strongly correlated variables. However, as we discussed in the last sub-point,    the number of cars that fit into the garage is a consequence of the garage area. 'GarageCars' and 'GarageArea' are like twin brothers. \n* 'TotalBsmtSF' and '1stFloor' also seem to be twin brothers. \n* 'FullBath'?? Really?\n* 'TotRmsAbvGrd' and 'GrLivArea', twin brothers again.\n* It seems that 'YearBuilt' is slightly correlated with 'SalePrice'. Honestly, it scares me to think about 'YearBuilt' because I start     feeling that we should do a little bit of time-series analysis to get this right.","9d149bf1":"* LightGBM","ed74b3e4":"Creating Dummy Columns for Label Encoding the Categorical Features.","ddcb9b6d":"So, there are 2920 observations in are Complete dataframe, i,.e 1460 in both train and test dataframe. Now if we see the top three predictors from our missing value dataframe we see that most of them are close to 2920 that means,most of the observations from those predictors are not present.","22b38fdc":"**Now that we are done with most of the Feature Analysis, Let's Beging with the Feature Engineering!**","5efb7cdb":"Let us first import the training and the test data.","3bda77b4":"**Looks like our Data is Skewed Towards Right.**\n\n* We are normalising the data by simply takithe Natural Log and then adding 1.\n\n**Why do we need to make the Data Normal?**\n\nSince Machine Learning or Data Science is nothing but Glorified Statistics at the end of the day and most of the algorithms assumes that the data is that the data is normal and it calculates various stats assuming this. So the more the data is close to normal, the better it fits the assumption.\n\n**Log Transformation: -**","687a3754":"**This is the Normalised Data**","3b593a19":"**Numbers our not my Stuff! Let's See some Graphs!**","7c9f8dc5":"**Let's Handle the Missing Values!**","87cb1b52":"**And Finally! Let's Make the Submission Dataframe!**\n**You have Done it!**","a79ccaf5":"**Analyzing the Categorical Predictors**","dd4c8c08":"It seems that 'SalePrice' and 'GrLivArea' are really in love with each other, with a **linear relationship**","6d7bb5dc":"**Now it looks Beautiful!**","8e7a751f":"We are going to generate our final submission by Ensembling the XGBoost and LightGBM Results","8d7fca87":"**Let's Define the Models and Do a Scoring!**","e769c3bf":"**Let's have an Eagle's Eye View! **","fee6826d":"**Looking at Skewed Features**","6d7d4216":"**Thus,**\n\n* Total Observations in the Training Data : - 1460\n* Total Features in the Training Data : - 79, Excluding Id Column and Dependent Variable i.e. SalePrice\n    \n**Description of the Training Data: -**\n\n1. Mean Value      -->  180921.195890\n2. Std. Deviation  -->  79442.502883\n3. Min Value       -->  34900.00\n4. Max Value       -->  755000.00","a4fef5a3":"**3. Handling Missing Values in Categorical Data**","84422e37":"# Contents\n1. [Importing Packages](#p1)\n2. [Loading Data](#p2)\n3. [Imputing Null Values](#p3)\n4. [Feature Engineering](#p4)\n5. [Creating, Training, Evaluating, Validating, and Testing ML Models](#p5)\n6. [Submission](#p6)","5d1b4a9f":"Let's Generate the Correlation Matrix [](http:\/\/)","752471c1":"**Let's Do Some Real \"Work\"**\n\nWe are creating this Notebook to illustrate that how you can \"Approach almost any Regression Problem\" \n\n**A Comprehensive Checklist for Solving Any Regression Problem:**\n\n* Data Fetching\n* Understanding the Data\n* Checking the Skwewness of the Output Variable\n* Performing Log Transformation (if required)\n* Exploratory Data Analysis\n* Analysing Correlation\n* Finding out Important Predictors\n* Feature Engineering: -\n     1. Missing Values\n     2. Outliers\n     3. Categorical Feature Encoding     \n* Creating Folds and Defining Fold Map\n* Defining Models\n* Fitting the Model and Running Cross Validation\n* Stacking and Ensembling\n* Hyperparameter Optimization\n\nThe above mentioned checklist is very importnant for solving any regression based problem. The similar kind of checklist can also be prepared for other Machine Learning based problems. I will cover them in my upcoming kernals.\n\nLet's begin!\n","94b0076b":"Now that we are done with our Feature Engineering,We can now seperate thhe Training and Test Data from the complete data.","0769928c":"The above dataframe illustrates the count of Missing values corresponding to each observation.","8cfd0787":"**Let's Handle the Categorical Features!**"}}