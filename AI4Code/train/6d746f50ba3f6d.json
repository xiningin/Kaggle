{"cell_type":{"c4d404a5":"code","73a3d241":"code","08957fea":"code","01b7dc80":"code","abbe9f5b":"code","2d0d2e27":"code","eff00337":"code","7b83954f":"code","23c436c6":"code","cf6c41cc":"code","1bf09665":"code","145be5d9":"code","680e38be":"code","832bab37":"code","cb878f86":"code","dcbd30d6":"code","25d8ab68":"code","3518d521":"code","93adc86f":"code","1ec5e260":"code","db246316":"code","b3d4df7f":"code","5b2eedcd":"code","16014f18":"code","96d47350":"code","e304e983":"code","f3deca4d":"code","635c9f23":"code","d5c4ad39":"code","ffd2d241":"code","8ea52a4e":"code","ac2b9869":"code","fd256b8b":"code","ac539b66":"code","87d3f03b":"code","5967dda8":"code","4b05442b":"code","531ec8ae":"code","285c1f55":"code","0c801750":"code","6e2fe238":"code","297b4fcc":"code","9c300265":"markdown","6f4838b1":"markdown","6ed4e6cc":"markdown","f8c75f4c":"markdown","0e1612af":"markdown","e2e19710":"markdown","b971886e":"markdown","6e9252c0":"markdown","53fa5bc0":"markdown","7a99b4f1":"markdown","3064a67b":"markdown","29593345":"markdown","02a86f53":"markdown","50a314c6":"markdown","f1b0b042":"markdown","24aa3716":"markdown","eb95abe2":"markdown","a77550a1":"markdown","a7af2ae8":"markdown","6d6de5ae":"markdown","3c87b5bd":"markdown","724d9d1a":"markdown","459a5d58":"markdown","f3361c8f":"markdown","e750c5c9":"markdown","c4f5d8c5":"markdown","dcd0012b":"markdown","bfa6ebad":"markdown","092cc1a1":"markdown","8f271cfd":"markdown","a6751ac4":"markdown","ebf0e946":"markdown"},"source":{"c4d404a5":"import pandas as pd\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport matplotlib.ticker as ticker\nplt.rc(\"font\", size=14)\nimport warnings\nwarnings.simplefilter(action='ignore')","73a3d241":"df = pd.read_csv(\"..\/input\/stroke\/stroke.csv\", header=0)","08957fea":"df.head()","01b7dc80":"df.shape","abbe9f5b":"df.describe()","2d0d2e27":"df.info()","eff00337":"g = sns.pairplot(data=df, hue = 'stroke')\ng.map(plt.scatter)","7b83954f":"#divide dataset into two parts(categorical, contineous)\ncategorical, numerical = [],[]\nfor z in df.columns:\n    t = df.dtypes[z]\n    if t=='object':\n        categorical.append(z)\n    else:\n        numerical.append(z)\nprint(\"CategoricaL:\\n{}\".format(categorical))\nprint(\"\\nNumericaL:\\n{}\".format(numerical))","23c436c6":"df['bmi']=df['bmi'].fillna(df['bmi'].mean())","cf6c41cc":"#finding the unique values in each column (type object)\nfor col in df.select_dtypes('O').columns:\n    print('We have {} unique values in {} column : {}'.format(len(df[col].unique()),col,df[col].unique()))\n    print('-'*100)","1bf09665":"plt.figure(figsize=(16, 6))\nheatmap =sns.heatmap(df.corr(), annot = True, cmap= 'Purples')\nheatmap.set_title('Correlation Heatmap', fontdict = {'fontsize':20}, pad =14);","145be5d9":"features = [ 'gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'smoking_status', 'stroke']\n\nplt.figure(figsize=(30, 20))\nsns.set(font_scale= 1.2)\nsns.set_style('darkgrid')\n\nfor i, feature in enumerate(features):\n    plt.subplot(3, 4, i+1)\n    sns.countplot(data=df, x=feature, palette=\"rocket\")  \n    \n    \nsns.despine()","680e38be":"features = [ 'gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n\nplt.figure(figsize=(30, 20))\nsns.set(font_scale= 1.2)\nsns.set_style('darkgrid')\n\nfor i, feature in enumerate(features):\n    plt.subplot(3, 4, i+1)\n    sns.countplot(data=df, x=feature, hue = 'stroke', palette=\"viridis\")  \n    \n    \nsns.despine()","832bab37":"features = [ 'gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type','stroke','smoking_status']\n\nplt.figure(figsize=(30, 20))\nsns.set(font_scale= 1.2)\nsns.set_style('darkgrid')\n\nfor i, feature in enumerate(features):\n    plt.subplot(3, 4, i+1)\n    sns.boxenplot(data=df, x=feature, y = 'age', palette=\"dark\")  \n    \n    \nsns.despine()","cb878f86":"features = [ 'gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type','stroke','smoking_status']\n\nplt.figure(figsize=(30, 20))\nsns.set(font_scale= 1.2)\nsns.set_style('darkgrid')\n\nfor i, feature in enumerate(features):\n    plt.subplot(3, 4, i+1)\n    sns.boxenplot(data=df, x=feature, y = 'age',hue='stroke', palette=\"colorblind\")  \n    \n    \nsns.despine()","dcbd30d6":"plt.figure(figsize=(8, 5))\nsns.kdeplot(data=df,x=\"bmi\",fill=True, color=\"red\", shade=True, alpha=.5, linewidth=1)","25d8ab68":"dff = pd.get_dummies(df, columns=['gender', 'ever_married', 'Residence_type','work_type', 'smoking_status'])\ndel dff['gender_Female']\ndel dff['work_type_Never_worked']\ndel dff['Residence_type_Rural']\ndel dff['smoking_status_Unknown']\ndel dff['ever_married_No']","3518d521":"dff.head()","93adc86f":"from sklearn.model_selection import train_test_split","1ec5e260":"x = dff.drop(['stroke'], axis=1)\ny = dff[['stroke']]","db246316":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)","b3d4df7f":"X_train.shape","5b2eedcd":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import classification_report\nimport statsmodels.api as sm\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score, roc_auc_score, precision_score, auc\nfrom sklearn.metrics import roc_curve\nimport warnings\nwarnings.filterwarnings('ignore')","16014f18":"classifier = GaussianNB()\nclassifier.fit(X_train,y_train)\n\n#Predict the response for test dataset\ny_pred1 = classifier.predict(X_test)\nprint(\"Classification report - \\n\", classification_report(y_test,y_pred1))\nNaive=accuracy_score(y_test, y_pred1)","96d47350":"cm = confusion_matrix(y_test, y_pred1)\nplt.figure(figsize = (6,4))\n\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nsns.heatmap(conf_matrix, annot=True,  cmap = 'tab10')\n\n# print the scores on training and test set\nall_sample_title = 'Accuracy Score: {0}'.format(accuracy_score(y_test, y_pred1))\n\nplt.title(all_sample_title, size = 19)\nplt.savefig(\"pne.png\")","e304e983":"#ROC Curve for Model\ny_score1 = classifier.predict_proba(X_test)[:,1]\nfalse_positive_rate1, true_positive_rate1, threshold1 = roc_curve(y_test, y_score1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\n\n# plot roc curves\nplt.plot(false_positive_rate1, true_positive_rate1, linestyle='--',color='maroon', label = 'Naive')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\nall_sample_title = 'ROC_AUC_SCORE: {0}'.format(roc_auc_score(y_test, y_score1))\nplt.title(all_sample_title, size= 18)\nplt.legend(loc='best')\nplt.show();","f3deca4d":"v = sns.countplot(data=df, x='stroke', palette=\"rocket\") \nv.set_xticklabels(['Not Stroke', 'Stroke'])\nplt.show()","635c9f23":"!pip install imblearn","d5c4ad39":"not_stroke = dff[dff['stroke']==0]\nstroke = dff[dff['stroke']==1]","ffd2d241":"not_stroke.shape,stroke.shape","8ea52a4e":"from imblearn.combine import SMOTETomek","ac2b9869":"smk = SMOTETomek(random_state =42)\nx_resample,y_resample=smk.fit_resample(x,y)","fd256b8b":"x_resample.shape,y_resample.shape","ac539b66":"X_train, X_test, y_train, y_test = train_test_split(x_resample, y_resample, test_size=0.2, random_state=42)","87d3f03b":"from sklearn.preprocessing import StandardScaler\nx_train = StandardScaler().fit_transform(X_train)\nx_test = StandardScaler().fit_transform(X_test)","5967dda8":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV","4b05442b":"parm = {'n_neighbors': np.arange(1,15)}","531ec8ae":"knn = KNeighborsClassifier()\ncv = GridSearchCV(knn, parm, cv = 5)\ncv.fit(x_train,y_train)","285c1f55":"cv.best_params_","0c801750":"#Predict the response for test dataset\ny_pred = cv.predict(x_test)\nprint(\"Classification report - \\n\", classification_report(y_test,y_pred))\nNaive=accuracy_score(y_test, y_pred)","6e2fe238":"cm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize = (6,4))\n\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nsns.heatmap(conf_matrix, annot=True,  cmap = 'tab10')\n\n# print the scores on training and test set\nall_sample_title = 'Accuracy Score: {0}'.format(accuracy_score(y_test, y_pred))\n\nplt.title(all_sample_title, size = 19)\nplt.savefig(\"pne.png\")","297b4fcc":"#ROC Curve for Model\ny_score2 =cv.predict_proba(x_test)[:,1]\nfalse_positive_rate1, true_positive_rate1, threshold1 = roc_curve(y_test, y_score2)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\n\n# plot roc curves\nplt.plot(false_positive_rate1, true_positive_rate1, linestyle='--',color='green', label = 'KNN')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\nall_sample_title = 'ROC_AUC_SCORE: {0}'.format(roc_auc_score(y_test, y_score2))\nplt.title(all_sample_title, size= 18)\nplt.legend(loc='best')\nplt.show();","9c300265":"1. Naive Bayes Classifier are a collection of classification algorithm based on bayes theorm.\n\n\n2. it is as classification technique based on 'Bayes Theorm' with an assumption of independant among \n   predictore\n     \n     \n3. for eg, suppose a fruit may be considered to be an apple if it is a red, round and about three \n   inches in diameter, even if these features depend on each other. all these properties\n   independantly contribute to the probability that this fruit is an apple. that is why it is known      as Naive.\n   \n   \n                                            \n<center><img src = 'https:\/\/uc-r.github.io\/public\/images\/analytics\/naive_bayes\/naive_bayes_icon.png' width = 550, height = 1000>","6f4838b1":"## **Resampling Technique**\n\n* A widly adopted technique for dealing with highly unbalanced datasets is called resampling.\n\n\n### **Types of resampling.**\n\n   1)Under sampling(removing samples from the majority class)\n   \n   2)Over sampling(adding more samples in minority class)\n   \n   3)Under sampling : Tomek Links\n   \n   4)Synthetic Minority Oversampling Technique(SMOTE)\n   \n   5)Under sampling : NearMiss\n \n\n* **under-sampling**:- Under-sampling is the simplest technique which removing random records from the majority class, which can cause loss of information.\n\n\n* **over-sampling**:- Is the simplest technique involves duplicate random records from the minority class, which can cause overfitting. over-sampling can be good choise when you don't have a ton of data to work with.\n\n\n* **Tomek links**:- These are pairs of very close records of opposite classes. In this technique we removing the record of majority class of each pair. and increases the space between the two classes. Tomek links exists if the two samples are the nearest neighbors of each other.\n\n* **Synthetic Minority Oversampling Technique(SMOTE)**:- In this technique we randomly pick a point from the minority class and computing the k-nearest neighbores for this point. The synthetic points are added between the choosen point and its neighbors.\n\n* **NearMiss**:- Is an Under-sampling technique. Instead of resampling the minority class, using a distance, this will make the majority class equal to the minority class.","6ed4e6cc":"# **\u2714Train-Test split**","f8c75f4c":"#### **Application**","0e1612af":"  # **<font color= 'purple'>Imbalanced Data<\/font>**","e2e19710":" # **<font color= 'purple'>K-Nearest Neighbors(KNN)<\/font>**","b971886e":"#### **Limitation**","6e9252c0":"Let's understand what exactly does k influence in the algorithm.\n\n* The number of neighbours(K) in KNN is a parameter that we need to select at a time of model           fitting. acurrcy of the model depend upon our choice of k value.\n* A small value of k means that noise will have higher influence on the result. So, probability of overfitting is very high.\n* A large value of k makes it computationally expensive in terms of time to build the kNN model. Also, a large value of k will have a smoother decision boundary which means lower variance but higher bias.\n","53fa5bc0":"## **Introduction**","7a99b4f1":"#### **How does Naive Bayes work?**","3064a67b":"# **\u2714Import Library**","29593345":"# **\u2714 Create dummy variables**","02a86f53":"# **\u2714Preproccesing\ud83d\udc40**","50a314c6":"Let's understand it!\n* Below i have a traning dataset of stroke prediction and corresponding target variable is stroke or   not (Suggesting probabilities of having stroke) based on other independant features like BMI,         smoking, Glucose Level.\n\n\n* He\/She will have a stroke if He\/She is smoker, is this statement correct? we can solve it by       using posterior probability.\n\n\n$$\\textrm{P(Yes|Smoker)} = \\frac{\\textrm{P(Smoker|Yes)}\\textrm{P(Yes)}} {\\textrm{P(Smoker)}}$$\n\n\n* If probability is greater than 0.5 then we say higher the probability of getting stroke when He\/She   is smoker.\n\n\n* Naive Bayes uses a similar method to predict the probability of different class based on various     attributes.","f1b0b042":"7. Naive bayes is the assumption of independant predictors. In real life, it is almost impossible        that we get a set of predictors which are completely independant.","24aa3716":"#### **This dataset is used to predict whether a patient is likely to get stroke based on the input features like gender, hypertension, age and smoking status.**","eb95abe2":"<center><img src = 'https:\/\/cdn-images-1.medium.com\/fit\/t\/1600\/480\/0*49LgGsY4l09sNwcR.png' width = 550, height = 1000>","a77550a1":"  # **<font color= 'purple'>\u2714 Visualization\ud83d\udcca<\/font>**","a7af2ae8":"#### **What should be the value of nearest neighbors(k)!**","6d6de5ae":"   * Real Time Prediction\n   * Multi class Prediction\n   * Text Classification\/ Spam Filtering \/ Sentiment Analysis","3c87b5bd":"Because the KNN classifier predict the class of a given test observation by identifying the observations that are nearest to it, the scale of the variable matters. Any variables that are on a large scale will have a much larger effect on the distance between the observations.","724d9d1a":"* Select the k number of the neighbors.\n\n\n* Calculate the distance between a point from k number of neighbores using methods like Euclidean,     Manhattan or Hamming distance.(most common method for calculating distance is Euclidean)\n\n\n* Select the smallest Euclidian distance from point to k nearest neighbores.\n\n\n* Count the number of data points in each category, among these k nearest neighbores.\n\n\n* Assgin new data point that category fro which the number of the neighbore is maximum","459a5d58":"#### **How does KNN work?**","f3361c8f":"  # **<font color= 'purple'>Naive Bayes Classifier<\/font>**","e750c5c9":"   # **<font color= 'purple'>Standard Scalar<\/font>**","c4f5d8c5":"#### **Types of Naive Bayes Algorithm**","dcd0012b":"\n* When observation in one class is higher than the onservation in other classes then there exists a class imbalance.\n\n\n* Here we have a dataset of stroke where we have to predict whether the person having a stroke or not.\n\n\n* In above Naive Bayes Model we can see 93% accuracy, we got very high accuracy because it is \n  predicting mostly the majority class i.e 0(Not having stroke). Thus accuracy is not the best metric to use when evaluating imbalanced datasets as it can be missleading hence we have to check ROC-AUC score for the model and in above model it is very low.\n  \n  \n* From above countplot we can see our dataset is imbalanced, then in such cases, we get a pretty high accuracy just by predicting the majority class i.e not not having a stroke or 0's, but we fail to capture the minority class i.e having a stroke.","bfa6ebad":"# <center>\ud83d\udccc<font color= 'red'>Stroke Prediction Dataset\ud83d\udccc<\/font>\n\n\n<center><img src = 'https:\/\/wp02-media.cdn.ihealthspot.com\/wp-content\/uploads\/sites\/520\/2020\/04\/08160759\/iStock-1168179082.jpg' width = 750, height = 700>","092cc1a1":"    \n   * Posterior Probability:- is the probablity of event A occuring given that event B has occured. it\n     takes new information into account.\n     \n     \n  * Prior Probability:- is the probability of an event on the current knowledge before as experiment      is performed.","8f271cfd":"1. K-Nearest Neighbor(KNN) is one of the simplest algorithm in machine learning algorithms. it is \n   Supervised Learning technique. KNN is non-parametric algotrihtm, which means it does not take any    assumption on underlying data.\n   \n    \n2. KNN algorithm stores all the available data and classifies a new data points based on train\n   data. simply when new data appears then it can be easily classified into well suite category\n   by using KNN.\n   \n   \n3. KNN algorithm does not train the data. it only learn from the traning data instead the it            stores the dataset and at the time of classification it performs an action on dataset, that is why    it is called as lazy learner algorithm.\n\n4. for eg. Suppose there are two categories, i.e., category 1 and category 2, and we have a new data    point A, so this data point will lie in which of these two category. To solve such type of problem    we use KNN algorithm, using KNN algorithm we can easily identify the category of a particular        dataset.\n\n\n\n<center><img src = 'https:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1531424125\/KNN_final_a1mrv9.png' width = 550, height = 500>\n","a6751ac4":"# **\u2714Objective**\nVisualize the relationships between various features to Heart Strokes and predict the stroke probability with best model.\n","ebf0e946":"6. There are three types of Naive Bayes algorithm.\n\n\n * Bernoulli Naive Bayes:- Used for discrete data, where features are only in binary form. it is         also called as conditional probability.\n    \n    \n * Multinomial Naive Bayes:- Widely used classifier for document classification which keeps the \n   count of frequent words present in the document.Suppose you have a text document and you              extract all the unique words and create multiple features where each feature represents the count    of the word in the document. In such a case, we have a frequency as a feature. In such a scenario,    we use multinomial Naive Bayes\n    \n    \n * Gaussian Naive Bayes :- This type of Naive Bayes is used when variables are continuous in            nature. It assumes that all the variables have a normal distribution. So if you have some            variables which do not have this property, you might want to transform them to the features          having distribution normal."}}