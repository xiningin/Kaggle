{"cell_type":{"639b2920":"code","79042def":"code","850dfebf":"code","b3c75d2b":"code","10020a1e":"code","ed4ced28":"code","dce68de4":"code","7152ad76":"code","5a508052":"code","e536bc5b":"code","61de5595":"code","9a2b5c49":"code","1cd9232c":"code","d5dda588":"code","46a53fe4":"code","a3e5d0d9":"code","cbf2a71f":"code","5c217acb":"code","233c99db":"code","0a655ed1":"code","fcdf9a93":"code","23daf2ef":"code","1974ddee":"code","b07aa1a2":"code","06daa6cc":"code","7bc94627":"code","d749f4a1":"code","42703946":"code","f4a30faf":"code","0dcbfbdd":"code","aa6ee3ce":"code","fb53a22c":"code","e9838d86":"code","68e73186":"code","02e143d7":"code","064a5b57":"code","edd4d55c":"code","b889ba97":"code","8e7d58f5":"code","582f1373":"code","397fb65c":"code","65726703":"markdown","f05d9278":"markdown","489bb2b3":"markdown","4c654fdb":"markdown","317e1dcf":"markdown","69e6fda1":"markdown","4918770a":"markdown","e3b89cd3":"markdown","1478deb9":"markdown","368d619e":"markdown","bb104bf5":"markdown","7e2bf457":"markdown","0c0d43f3":"markdown","ae65c6b4":"markdown","cefdac8c":"markdown","f8f7e8f1":"markdown","1191ed20":"markdown","d8fb4ebf":"markdown","593f9c9f":"markdown","cbaecdd8":"markdown","f4716d84":"markdown","4ef7c92b":"markdown","228e7676":"markdown","f2e3e37a":"markdown","c6c59b75":"markdown","f88205bc":"markdown","3033ac41":"markdown","2bee8b82":"markdown","b1636599":"markdown","7fffb374":"markdown","f29669b5":"markdown","c925108e":"markdown","210a587e":"markdown","968f9a9f":"markdown","df46edfb":"markdown","eb3e8b1e":"markdown","2d0c3a33":"markdown","946e4fbf":"markdown"},"source":{"639b2920":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport warnings\nwarnings.simplefilter(\"ignore\")","79042def":"data=pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndata.dropna(axis=0).any()\nX=set(data.columns)\nX.remove('Outcome')\nX=data[X]\ny=data['Outcome']\ndata.head()","850dfebf":"print(data.info())\nprint(data.describe())","b3c75d2b":"age_data=pd.DataFrame(data.groupby(['Age'],as_index=False)['Outcome'].count())\ninterval={}\ntemp_sum=0\nfor i in range(len(age_data)):\n    temp_sum+=int(age_data.iloc[i,1])\n    if age_data.iloc[i,0]==35:\n        interval.update({\"20-35\":temp_sum})\n        temp_sum=0\n    elif age_data.iloc[i,0]==50:\n        interval.update({\"35-50\":temp_sum})\n        temp_sum=0\n    elif age_data.iloc[i,0]==81:\n        interval.update({\"+50\":temp_sum})\nplt.figure(figsize=(12,8))\nplt.bar(interval.keys(),interval.values(),color=['#cc6699','#339933','#006666'])\n\nplt.xlabel(\"Age\")\nplt.ylabel(\"Count\")\nplt.title(\"Counts of Age Interval\")\nplt.show()","10020a1e":"fig, ax = plt.subplots(figsize=(12,7))    \nsns.heatmap(data.corr(), annot=True,ax=ax)","ed4ced28":"sns.pairplot(data, hue=\"Outcome\") ","dce68de4":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nx_scaled=scaler.fit_transform(X)\ndata_scaled=pd.concat([pd.DataFrame(x_scaled,columns=data.iloc[:,:-1].columns),y],axis=1)","7152ad76":"from sklearn.decomposition import PCA\npca=PCA().fit(x_scaled)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.title(\"How many variable represents our model with acceptable error PCA\")\nplt.xlabel(\"Number of variable\")\nplt.ylabel(\"Variance\")\nplt.grid()","5a508052":"pca=PCA(n_components=5)\nx_reduced=pca.fit_transform(data_scaled.iloc[:,:-1])\ndata_reduced=pd.concat([pd.DataFrame(x_reduced),y],axis=1)\ndata_reduced.head()","e536bc5b":"before_x=[]\nafter_x=[]\ndata4iqr=data.copy()\nfor i in range(len(data4iqr.columns)-1):\n    col=data4iqr.iloc[:,i:i+1]\n\n    Q1=col.quantile(0.25)\n    Q3=col.quantile(0.75)\n    IQR=Q3-Q1\n    lower=Q1-1.5*IQR\n    upper=Q3+1.5*IQR\n    new_col=col[~((col<lower)|(col>upper)).any(axis=1)]\n    ex_col=col[((col<lower)|(col>upper)).any(axis=1)]\n    before_x.append(col)\n    data4iqr.drop(index=ex_col.index,axis=0,inplace=True)\n    after_x.append(data4iqr.iloc[:,i:i+1])\ndata4iqr.reset_index(inplace=True)\nprint(\"IQR METHOD\",len(data)-len(data4iqr),\" Row Effected\")\n####IQR Visualization####\nf, axes = plt.subplots(2,5, figsize=(22, 7))\nj=0\nfor i in range(5):\n    if data4iqr.columns[i+1]==\"Outcome\":\n        continue\n    sns.boxplot(before_x[i],ax=axes[0,j]).set_title(data4iqr.columns[i+1]+\" Before IQR\")\n    sns.boxplot(after_x[i],ax=axes[1,j]).set_title(data4iqr.columns[i+1]+\" After IQR\")\n    j+=1\nplt.show()\n","61de5595":"from sklearn.neighbors import LocalOutlierFactor\nclf=LocalOutlierFactor(n_neighbors=20,contamination=0.1)\noutlier_pred=clf.fit_predict(data_reduced)\nx_score=clf.negative_outlier_factor_\nx_score=np.abs(x_score)\nxscr_mean=x_score.mean()\nxscr_std=np.std(x_score)\nlower=xscr_mean-(1*xscr_std)\nupper=xscr_mean+(1*xscr_std)","9a2b5c49":"inliers=data[~((x_score>upper)| (x_score<lower))]\nprint(len(inliers))\nlof_data=inliers.copy()","1cd9232c":"from sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV,cross_val_score","d5dda588":"columns=set(lof_data.columns)\ncolumns.remove('Outcome')\nx_reduced=lof_data[columns]\ny=lof_data['Outcome']\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x_reduced,y,test_size=0.33,random_state=58)","46a53fe4":"from sklearn.linear_model import LogisticRegression as log_rec\nlogistic_model=log_rec(C=0.0001,solver='newton-cg')\nlogistic_model.fit(x_train,y_train)\nlogistic_pred=logistic_model.predict(x_test)\nprint(\"Logistic Regression Accuracy Score Before Tuning %.5f\"% metrics.accuracy_score(logistic_pred,y_test))\nprint(\"Logistic Regression F1 Score Before Tuning %.5f\"%metrics.f1_score(logistic_pred,y_test))","a3e5d0d9":"logistic_params={'C':[0.001,0.01,0.1,1,10,100,1000],'solver':[ \"liblinear\", \"sag\", \"saga\",\"lbfgs\"]}\ngrid=GridSearchCV(log_rec(),logistic_params,scoring='accuracy',cv=3)\ngrid.fit(x_train,y_train)\ngrid.best_params_","cbf2a71f":"logistic_model=log_rec(C=grid.best_params_['C'],solver=grid.best_params_['solver'])\nlogistic_model.fit(x_train,y_train)\nlogistic_pred=logistic_model.predict(x_test)\nprint(\"Logistic Regression Accuracy Score After Tuning %.5f\"%metrics.accuracy_score(logistic_pred,y_test))\nprint(\"Logistic Regression F1 Score After Tuning %.5f\"%metrics.f1_score(logistic_pred,y_test))","5c217acb":"from sklearn.neighbors import KNeighborsClassifier\nknn_model=KNeighborsClassifier(n_neighbors=3)\nknn_model.fit(x_train,y_train)\nknn_pred=knn_model.predict(x_test)\nprint(\"KNN Accuracy Score Before Tuning %.5f\"% metrics.accuracy_score(knn_pred,y_test))\nprint(\"KNN F1 Score Before Tuning %.5f\"% metrics.f1_score(knn_pred,y_test))","233c99db":"knn_params={'n_neighbors':np.arange(3,90,2)}\ngrid=GridSearchCV(KNeighborsClassifier(),knn_params,scoring='accuracy',cv=3)\ngrid.fit(x_train,y_train)\ngrid.best_params_","0a655ed1":"knn_model=KNeighborsClassifier(n_neighbors=grid.best_params_['n_neighbors'])\nknn_model.fit(x_train,y_train)\nknn_pred=knn_model.predict(x_test)\nprint(\"KNN Accuracy Score After Tuning %.5f\"% metrics.accuracy_score(knn_pred,y_test))\nprint(\"KNN F1 Score After Tuning %.5f\" % metrics.f1_score(knn_pred,y_test))\n#print(knn_model.predict_proba(x_test[3:5]))","fcdf9a93":"from sklearn.tree import DecisionTreeClassifier\ndt_model=DecisionTreeClassifier()\ndt_model.fit(x_train,y_train)\ndt_pred=dt_model.predict(x_test)\nprint(\"Decision Tree Accuracy Score Before Tuning %.5f\"% metrics.accuracy_score(dt_pred,y_test))\nprint(\"Decision Tree F1 Score Before Tuning %.5f\"% metrics.f1_score(dt_pred,y_test))","23daf2ef":"dt_params={'criterion':['gini','entropy'],'max_depth':(2,4,6,8,10,12,16,18,20)}\ngrid=GridSearchCV(DecisionTreeClassifier(),dt_params,scoring='accuracy')\ngrid.fit(x_train,y_train)\ngrid.best_params_","1974ddee":"dt_model=DecisionTreeClassifier(criterion=grid.best_params_['criterion'],max_depth=4)\ndt_model.fit(x_train,y_train)\ndt_pred=dt_model.predict(x_test)\nprint(\"Decision Tree Accuracy Score After Tuning %.5f\"% metrics.accuracy_score(dt_pred,y_test))\nprint(\"Decision Tree F1 Score After Tuning %.5f\"% metrics.f1_score(dt_pred,y_test))","b07aa1a2":"\nfrom sklearn import tree\nplt.figure(figsize=(60,40),dpi=400)\ntree.plot_tree(dt_model,filled=True,rounded=True,feature_names=X.columns,\n            class_names=['Diabetes','No Diabetes'])\nplt.show()\n#plt.savefig(\"tree_visual.png\")","06daa6cc":"from sklearn.svm import SVC\nsvc_model=SVC()\nsvc_model.fit(x_train,y_train)\nsvc_pred=svc_model.predict(x_test)\nprint(\"SVM Accuracy Score Before Tuning %.5f\"% metrics.accuracy_score(svc_pred,y_test))\nprint(\"SVM F1 Score Before Tuning %.5f\"%metrics.f1_score(svc_pred,y_test))","7bc94627":"svc_params=({'kernel':['rbf'],'C':[0.001,0.1,1,10,100],'gamma':['auto','scale']})\ngrid=GridSearchCV(SVC(),param_grid=svc_params,scoring=\"accuracy\",cv=3)\ngrid.fit(x_train,y_train)\ngrid.best_params_","d749f4a1":"svc_model=SVC(C=grid.best_params_['C'],kernel=grid.best_params_['kernel'],gamma=grid.best_params_['gamma'])\nsvc_model.fit(x_train,y_train)\nsvc_pred=svc_model.predict(x_test)\nprint(\"SVM Accuracy Score After Tuning %.5f\"% metrics.accuracy_score(svc_pred,y_test))\nprint(\"SVM  F1 Score After Tuning %.5f\"%metrics.f1_score(svc_pred,y_test))","42703946":"from sklearn.ensemble import RandomForestClassifier\nrf_model=RandomForestClassifier()\nrf_model.fit(x_train,y_train)\nrf_pred=rf_model.predict(x_test)\nprint(\"RandomForest Accuracy Score Before Tuning %.5f\"% metrics.accuracy_score(rf_pred,y_test))\nprint(\"RandomForest F1 Score Before Tuning %.5f\"%metrics.f1_score(rf_pred,y_test))","f4a30faf":"rf_params={'n_estimators':range(10,110,10),'criterion':['gini','entropy']}\ngrid=GridSearchCV(RandomForestClassifier(),rf_params,cv=3,scoring='accuracy')\ngrid.fit(x_train,y_train)\ngrid.best_params_","0dcbfbdd":"rf_model=RandomForestClassifier(n_estimators=grid.best_params_['n_estimators'],criterion=grid.best_params_['criterion'],max_depth=4)\nrf_model.fit(x_train,y_train)\nrf_pred=rf_model.predict(x_test)\nprint(\"RandomForest Accuracy Score After Tuning %.5f\"% metrics.accuracy_score(rf_pred,y_test))\nprint(\"RandomForest F1 Score After Tuning %.5f\"%metrics.f1_score(rf_pred,y_test))","aa6ee3ce":"from xgboost import XGBClassifier\nxg_model=XGBClassifier()\nxg_model.fit(x_train,y_train)\nxg_pred=xg_model.predict(x_test)\nprint(\"XGBoost Accuracy Score Before Tuning %.5f\"% metrics.accuracy_score(xg_pred,y_test))\nprint(\"XGBoost F1 Score Before Tuning %.5f\"%metrics.f1_score(xg_pred,y_test))","fb53a22c":"from sklearn.naive_bayes import GaussianNB\nnb_model=GaussianNB()\nnb_model.fit(x_train,y_train)\nnb_pred=nb_model.predict(x_test)\nprint(\"NaiveBayes Accuracy Score  %.5f\"% metrics.accuracy_score(nb_pred,y_test))\nprint(\"NaiveBayes F1 Score  %.5f\"%metrics.f1_score(nb_pred,y_test))","e9838d86":"from sklearn.cluster import KMeans\nkm_model=KMeans(n_clusters=2)\nkm_model.fit(x_train)\nkm_pred=km_model.predict(x_test)\nif metrics.accuracy_score(km_pred,y_test)<0.5:\n    zeros=np.where(km_pred==0)\n    ones=np.where(km_pred==1)\n    km_pred[zeros]=1\n    km_pred[ones]=0\nmetrics.accuracy_score(km_pred,y_test)","68e73186":"km_params={'algorithm':[\"auto\", \"full\", \"elkan\"],'max_iter':[100,200,300,400,500,600],'init':['k-means++','random']}\ngrid=GridSearchCV(KMeans(n_clusters=2,random_state=12),km_params,scoring='accuracy',cv=3)\ngrid.fit(x_train,y_train)\ngrid.best_params_","02e143d7":"km_model=KMeans(n_clusters=2,init=grid.best_params_['init'],algorithm=grid.best_params_['algorithm'],max_iter=grid.best_params_['max_iter'])\nkm_model.fit(x_train)\nkm_pred=km_model.predict(x_test)\nif metrics.accuracy_score(km_pred,y_test)<0.5:\n    zeros=np.where(km_pred==0)\n    ones=np.where(km_pred==1)\n    km_pred[zeros]=1\n    km_pred[ones]=0\nmetrics.accuracy_score(km_pred,y_test)","064a5b57":"from keras.models import Sequential\nfrom keras.layers import Dense,Flatten\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold","edd4d55c":"model = Sequential()\nmodel.add(Dense(60, input_dim=x_train.shape[1], activation='relu'))\nmodel.add(Dense(30,activation='relu'))\nmodel.add(Dense(30,activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\nmodel.fit(x_train,y_train,epochs=50)","b889ba97":"eval_score=model.evaluate(x_train, y_train)\nprint(\"Loss:\",eval_score[0],\"Accuracy:\",eval_score[1])\n","8e7d58f5":"from sklearn.ensemble import VotingClassifier ","582f1373":"est=[]\nest.append(('svm',svc_model))\nest.append(('rf',rf_model))\nest.append(('lr',logistic_model))\nest.append(('nb',nb_model))\nest.append(('xg',xg_model))\nvcls=VotingClassifier(estimators=est,voting='hard')\nvcls.fit(x_train,y_train)","397fb65c":"voting_pred=vcls.predict(x_test)\nprint(\"Voting Accuracy Score  %.5f\"% metrics.accuracy_score(voting_pred,y_test))\nprint(\"Voting F1 Score  %.5f\"%metrics.f1_score(voting_pred,y_test))","65726703":"# K-means ","f05d9278":"We can learn some statistical informations using info and describe methods","489bb2b3":"Logistic Regression is one of the simplest binary classification algorithm. We are going to tune its parameters.","4c654fdb":"## Dimension Reduction","317e1dcf":"## Logistic Regression Hyperparameter Tuning","69e6fda1":"Now we have 694 rows. Lof and Iqr are althernative methods, I select lof because generally iqr using if data has one independent variable","4918770a":"# Support Vector Machine(s)","e3b89cd3":"# Data Preprocessing","1478deb9":"# Artificial Neural Network with Backpropagation","368d619e":"## Decision Tree Hyperparameter Tuning","bb104bf5":"## Local Outlier Factor","7e2bf457":"# Logistic Regression","0c0d43f3":"# Random Forest","ae65c6b4":"## Feature Scaling","cefdac8c":"Correlation helps us to understand relation between 2 column. If correlation coefficient close to 1, it means when first feature increase, second one is going to increase too. If coefficient close to -1 means, when first one increase second feature is going to decrease (negative relation) But we can not say only check correlation coefficient then they 2 feature affect together directly. Data scientist should check they are related or not.","f8f7e8f1":"IQR method helps us removing outliers. It checks each column,finds standard deviation and remove which are out of (mean + 2 std) and (mean - 2std) ","1191ed20":"# K- Nearest Neighbors Algorithm (KNN)","d8fb4ebf":"Iqr select %95.4 (if feature distr normal) You should not keep outliers whether if they are real. Most machine learning algorithms are not suitable for kind of outliers.","593f9c9f":"* With pairplot function we see the relation on each 2 feature pairs. It gives us an idea that can we seperate data with a linear seperator or should we use soft margin or should we use one of tree based algorithm. Except diagonal,figure above demonstrate, linear seperator will not be able to seperate well.Even so we will apply these algorithms.****","cbaecdd8":"# Majority Voting","f4716d84":"## SVM HyperParameter Tuning","4ef7c92b":"## Visualization Of Decision Tree","228e7676":"![Gaussian Distrubution](https:\/\/i.hizliresim.com\/G8HuaU.jpg)","f2e3e37a":"## Standardization\n![Standardization](https:\/\/i.hizliresim.com\/1zOWgC.png)\n## Normalization\n![Normalization](http:\/\/i.hizliresim.com\/DG8T4r.jpg)\n","c6c59b75":"## Random Forest Tuning","f88205bc":" Scaling one of the most important part about preprocessing.Not only this database but also others,different features might have different scales. Assume that we have a dataset about vehicles. Production year is important. Speed is important also. Let us say production year is 2010 , speed is 280 km \/ h price is 100.000$.\n Second car one has year is 2020, speed is 280 kmh price is 220000 \\$.As the third option Year is 2010 speed , is 290 km \/ h and price is 120.000 \\$. Are the difference of year and speed same? Changing of 10 unit affects our cost different.Because year scale has just 4 digit and scale of price has on average 6 or more.In This and similar situations we should apply feature scaling. (Normalization & Standardization mostly)","3033ac41":"## How PCA Works\n![How PCA Works](https:\/\/i.hizliresim.com\/CqpxsW.png)","2bee8b82":"Lof is one of the most effective outlier detection algorithm. It is an unsupervised method which finds an optimum cluster. We chose number of neighbors 20.","b1636599":"# Explority Data Analysis","7fffb374":"In dataset, we might get rid of unnecessary features. Actually besides being unnecessary they also might decrease our accurracy.So we need to remove them. One of the most famous dimension reduction algorithm is Principal Component Analysis (PCA). We are going to use this algorithm today.","f29669b5":"Generally we use standardization (sklearn standardscaler) instead of normalization (min-max scaling) Because standardization function recreates number around dataset. Normalization gives around -1,1. So out of exceptional datasets I suggest standardization. ","c925108e":"# Decision Tree\n","210a587e":"# XGBoost","968f9a9f":"## K Nearest Neighbors Hyperparameter Tuning","df46edfb":"With this approach, we choose independent and succesfull models and each one has a vote. Hard voting means select which model choose more. Soft voting based on averaging (Mostly using for regression).","eb3e8b1e":"Basically PCA tries to represent 2 dimensional feature as one feature.If data has more than 2 features,pca applies this algorithm same dimensional pairs .Some points could not represents but we need to  both save time and hardware resources.","2d0c3a33":"Kmeans is a unsupervised algorithm. We will not give train labels and wait model to outputs.","946e4fbf":"# Naive Bayes"}}