{"cell_type":{"53223d44":"code","631df245":"code","73abdc1c":"code","802f3174":"code","797bb756":"code","f018fdf7":"code","2d8cd270":"code","4a809cde":"code","1d4cb4ba":"code","26a87167":"code","cd7bbdcb":"code","8489727a":"code","0c1fff2b":"code","c708bcca":"code","3b11b6bb":"code","9859ebb3":"code","8f0ae392":"code","38702393":"code","6ebf1733":"code","c1efc98b":"markdown","d53d2db3":"markdown","2ed64a3e":"markdown","0d0ab984":"markdown","e7d2626c":"markdown","1812ad46":"markdown","171404ca":"markdown","b55b5fe8":"markdown","0ccbe6ce":"markdown","17429b62":"markdown","9500127d":"markdown","74719bec":"markdown","89f347e2":"markdown","4d8b9f5f":"markdown","2f8b2f62":"markdown","ec7a423e":"markdown","6b8f416e":"markdown","e37a0dd6":"markdown","d49ee0d7":"markdown","44f79232":"markdown"},"source":{"53223d44":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import rgb2hex\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport os\nprint(os.listdir(\"..\/input\"))\nplt.style.use('bmh')","631df245":"# read data\ndf = pd.read_csv('..\/input\/measures_v2.csv')\ntarget_features = ['pm', 'stator_tooth', 'stator_yoke', 'stator_winding']\ndf.shape","73abdc1c":"df.describe()","802f3174":"p_counts = df.groupby('profile_id')['pm'].agg('count')\nax = p_counts.plot.barh(figsize=(10, 20), title='Sample size')\n_ = ax.set_xticks(2*3600*np.arange(1, 8)) # 2Hz sample rate\n_ = ax.set_xticklabels(list(range(1, 8)))\n_ = ax.set_xlabel('Time in hours')","797bb756":"ax = p_counts.plot.hist(title='Sample size distribution', bins=50, figsize=(10, 5), grid=True)\n_ = ax.set_xticks(2*3600*np.arange(1, 8)) # 2Hz sample rate\n_ = ax.set_xticklabels(list(range(1, 8)))\n_ = ax.set_xlabel('Time in hours')","f018fdf7":"corr = df.corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nplt.figure(figsize=(14,14))\n_ = sns.heatmap(corr, mask=mask, cmap=cmap, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","2d8cd270":"dfui = df.drop(['profile_id'], axis=1)  # dataframe under investigation\n\n# prepare colors\ncolor_list = plt.cm.tab10(np.linspace(0, 1, 10)[list(range(10))+[0, 1]])\ncoi = target_features + [c for c in dfui if c not in target_features]  # columns of interest\nfeat_clrs = {k: rgb2hex(color_list[i][:3]) for i, k in enumerate(coi)} if color_list is not None else {}\n\nn_cols = 4\nn_rows = np.ceil(dfui.shape[1] \/ n_cols).astype(int)\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(2.8*n_cols, n_rows*3))\nfor i, (ax, col) in enumerate(zip(axes.flatten(), list(dfui.columns))):\n    sns.distplot(dfui[col], color=feat_clrs[col], ax=ax)\n    if i % n_cols == 0:\n        ax.set_ylabel('Frequency')\nplt.tight_layout()","4a809cde":"grpd = {pid: df_ for pid, df_ in df.groupby('profile_id')}\ncoi = target_features\nn_cols = 4\nn_rows = np.ceil(len(grpd) \/ n_cols).astype(int)\nfig, axes = plt.subplots(n_rows, n_cols, sharey=True, figsize=(2.8*n_cols, n_rows*3))\nfor i, (ax, (p_id, df_)) in enumerate(zip(axes.flatten(), grpd.items())):\n    for c in coi:\n        lines = ax.plot(df_[c].reset_index(drop=True), label=c, color=feat_clrs[c])\n    ax.set_title(f'profile {p_id}')\n    if i % n_cols == 0:\n        ax.set_ylabel('Temp in \u00b0C')\n    if i >= (len(grpd) - n_cols):\n        ax.set_xlabel('Sample')\nfig.tight_layout()\n_ = ax.legend(ncol=15, loc='lower center', bbox_to_anchor=(.5, 1), bbox_transform=fig.transFigure)\n","1d4cb4ba":"coi = [c for c in dfui if c not in target_features + ['profile_id']]\nmax_values_per_col = dfui.abs().max(axis=0)\nfig, axes = plt.subplots(n_rows, n_cols, sharey=True, figsize=(2.8*n_cols, n_rows*3))\nfor i, (ax, (p_id, df_)) in enumerate(zip(axes.flatten(), grpd.items())):\n    for c in coi:\n        lines = ax.plot(df_[c].reset_index(drop=True)\/max_values_per_col[c], label=c, color=feat_clrs[c])\n    ax.set_title(f'profile {p_id}')\n    if i % n_cols == 0:\n        ax.set_ylabel('Normalized feature')\n    if i >= (len(grpd) - n_cols):\n        ax.set_xlabel('Sample')\nfig.tight_layout()\n_ = ax.legend(ncol=15, loc='lower center', bbox_to_anchor=(.5, 1), bbox_transform=fig.transFigure)","26a87167":"fig, axes = plt.subplots(n_rows, n_cols, sharey=True, sharex=True, figsize=(2.8*n_cols, n_rows*3))\nfor i, (ax, (p_id, df_)) in enumerate(zip(axes.flatten(), grpd.items())):\n    ax.plot(df_.motor_speed, df_.torque)\n    ax.set_title(f'profile {p_id}')\n    if i % n_cols == 0:\n        ax.set_ylabel('Torque in Nm')\n    if i >= len(grpd)-n_cols:\n        ax.set_xlabel('Motor speed in rpm')","cd7bbdcb":"# heat map\nN_BINS = 100\ndfui = df.assign(binned_torque=pd.cut(df.torque.values.ravel(), bins=N_BINS, include_lowest=True),\n                 binned_speed=pd.cut(df.motor_speed.values.ravel(), bins=N_BINS, include_lowest=True))\n\nts_map = dfui.loc[:, ['binned_torque', 'binned_speed', 'torque']]\\\n            .groupby(['binned_torque', 'binned_speed']).count().fillna(0).reset_index()\\\n            .rename(columns={'torque': 'count'})\\\n            .pivot(index='binned_torque', columns='binned_speed', values='count')\\\n            .sort_values('binned_torque', ascending=False)\nplt.figure(figsize=(6, 4))\nax = sns.heatmap(ts_map, robust=True, square=True, yticklabels=49, xticklabels=49, cbar=False)\n_ = ax.set_ylabel('Torque p.u.')\n_ = ax.set_xlabel('Motor speed p.u.')\n_ = ax.set_xticklabels([0, 0.5, 1], rotation=0)\n_ = ax.set_yticklabels([1, 0, -1])\nplt.tight_layout()","8489727a":"_df = df.loc[~df.profile_id.isin([46, 47])].reset_index(drop=True)\n# normalize\n_df = _df \/ _df.abs().max(axis=0)\ntransformed = PCA().fit_transform(_df.drop(['profile_id']+target_features, axis=1))\nN = len(_df.profile_id.unique())\ncols = min(10, N)\nrows = np.ceil(N\/10)\nplt.figure(figsize=(2*cols, 2*rows))\nfor i, (sess_id, sess_df) in enumerate(_df.groupby(['profile_id'])):\n    plt.subplot(rows, cols, i+1)\n    _trans = transformed[sess_df.index, :]\n    plt.scatter(_trans[:, 0], _trans[:, 1], c=_df.loc[sess_df.index, 'pm'].values, cmap=plt.get_cmap('coolwarm'), marker='.', vmin=_df['pm'].min(), vmax=_df['pm'].max())\n    plt.xlim(-1, 1)\n    plt.ylim(-1, 1)\n    plt.tick_params(axis='both', which='both', bottom=False, top=False,\n                            labelbottom=False, right=False, left=False,\n                            labelleft=False)\n    plt.annotate(str(sess_id), (4.8, -4.8))\nplt.tight_layout()","0c1fff2b":"fig = plt.figure(figsize=(17, 3))\ncols = 4\nfor i in range(cols):\n    plt.subplot(1, cols, i+1)\n    plt.scatter(transformed[:, i], transformed[:, i+1], c=_df.pm.values, cmap=plt.get_cmap('coolwarm'), marker='.', vmin=_df['pm'].min(), vmax=_df['pm'].max())\n    plt.tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False, right=False, left=False, labelleft=False)\nplt.show()","c708bcca":"def evaluate_baseline(data):\n    scaler = StandardScaler()\n    # please note that we standardize the full data here, yet this is not statistically sound procedure.\n    # In order to get an unflawed generalization measure of any model evaluated on the data only the training set should be used for fitting the scaler.\n    # Depending on the CV used this might mean to scale repeatedly with different subsets\n    scaled_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n    trainset = scaled_data.loc[:, [c for c in data.columns if c not in ['profile_id']+target_features]]\n    target = scaled_data.loc[:, 'pm']\n    \n    # Use k-fold CV to measure generalizability\n    ols = LinearRegression(fit_intercept=False)\n    print('Start fitting OLS...')\n    scores = cross_val_score(ols, trainset, target, cv=5, scoring='neg_mean_squared_error')\n    print(f'OLS MSE: {-scores.mean():.4f} (+\/- {scores.std()*2:.3f})\\n')  # mean and 95% confidence interval\n\n    rf = RandomForestRegressor(n_estimators=20, n_jobs=-1)\n    print('Start fitting RF...')\n    scores = cross_val_score(rf, trainset, target, cv=5, scoring='neg_mean_squared_error')\n    print(f'RF MSE: {-scores.mean():.4f} (+\/- {scores.std()*2:.3f})\\n')  # mean and 95% confidence interval","3b11b6bb":"evaluate_baseline(df)","9859ebb3":"extra_feats = {\n     'i_s': lambda x: np.sqrt(x['i_d']**2 + x['i_q']**2),  # Current vector norm\n     'u_s': lambda x: np.sqrt(x['u_d']**2 + x['u_q']**2),  # Voltage vector norm\n     'S_el': lambda x: x['i_s']*x['u_s'],                  # Apparent power\n     'P_el': lambda x: x['i_d'] * x['u_d'] + x['i_q'] *x['u_q'],  # Effective power\n     'i_s_x_w': lambda x: x['i_s']*x['motor_speed'],\n     'S_x_w': lambda x: x['S_el']*x['motor_speed'],\n}\ndf = df.assign(**extra_feats)","8f0ae392":"spans = [6360, 3360, 1320, 9480]  # these values correspond to cutoff-frequencies in terms of low pass filters, or half-life in terms of EWMAs, respectively\nmax_span = max(spans)\nenriched_profiles = []\nfor p_id, p_df in df.groupby(['profile_id']):\n    target_df = p_df.loc[:, target_features].reset_index(drop=True)\n    # pop out features we do not want to calculate the EWMA from\n    p_df = p_df.drop(target_features + ['profile_id'], axis=1).reset_index(drop=True)\n    \n    # prepad with first values repeated until max span in order to get unbiased EWMA during first observations\n    prepadding = pd.DataFrame(np.zeros((max_span, len(p_df.columns))),\n                              columns=p_df.columns)\n    temperature_cols = [c for c in ['ambient', 'coolant'] if c in df]\n    prepadding.loc[:, temperature_cols] = p_df.loc[0, temperature_cols].values\n\n    # prepad\n    prepadded_df = pd.concat([prepadding, p_df], axis=0, ignore_index=True)\n    ewma = pd.concat([prepadded_df.ewm(span=s).mean().rename(columns=lambda c: f'{c}_ewma_{s}') for s in spans], axis=1).astype(np.float32)\n    ewma = ewma.iloc[max_span:, :].reset_index(drop=True)  # remove prepadding\n    assert len(p_df) == len(ewma) == len(target_df), f'{len(p_df)}, {len(ewma)}, and {len(target_df)} do not match'\n    new_p_df = pd.concat([p_df, ewma, target_df], axis=1)\n    new_p_df['profile_id'] = p_id\n    enriched_profiles.append(new_p_df.dropna())\nenriched_df = pd.concat(enriched_profiles, axis=0, ignore_index=True)  \n\n\np_ids = enriched_df.pop('profile_id')\n\n","38702393":"enriched_df.head()","6ebf1733":"evaluate_baseline(enriched_df)","c1efc98b":"We find:\n* While motor excitations (motor_speed, torque, coolant) are sometimes of high dynamic, sometimes of stepwise nature, target temperatures always exhibit low-pass behavior with exponential rise and falls,\n* Coolant temperature suffers from measurement artefacts expressed by sharp drops in temperature, which recover as fast,\n* PM (Permanent Magnet -> Rotor) temperature expresses the slowest time constant and follows stator temperatures\n* Motor_speed and torque (the excitation features) follow sometimes step-wise cycles, sometimes random walks\n* Some profiles denote resting phases, where all input features stay constant most of the time","d53d2db3":"## Dimensionality reduced visualization\nWe further depict all recording sessions in terms of their principal component axes, shifting the color from blue to red as the permanent magnet temperature rises.\n\n### All profiles separated\nShowing the two most significant principal components.","2ed64a3e":"## Operation points\n\nThe operation point of a motor is often explained by its location in the motor_speed-torque-plane.","0d0ab984":"## Fitting on raw data","e7d2626c":"# Linear correlations","1812ad46":"We observe a very high positve linear correlation between *i_q* and *torque*.\nMoreover, *u_d* is highly negative linearly correlated with *torque* and *i_q*.\nIndeed, for the former insight we can refer to electric drive theory, where either higher torque is exclusively dependent on *i_q* in case of similar sized inductances in *d*- and *q*-axis, or increasing with higher *i_q* and slightly decreasing *i_d* elsewise (more common in practice).","171404ca":"# Measurement session lengths\nThe plots below show that all measurement sessions range from 20 minutes to around 6 hours.\nThe two short session ids \"46\" and \"47\" might be not very representative as temperatures inside electric motors need time to vary.","b55b5fe8":"It becomes obvious that lower profile IDs are of simpler driving cycles, not moving much in feature space.\nHigher profile IDs are driving cycles of high dynamics - excitation happened through random walks in the motor_speed-torque-plane.\n\n### All profiles condensed \nShowing increasing principal component significance from right to left.","0ccbe6ce":"# Time series gestalt\n\nFirst we'll get an overview of the target temperature trends.","17429b62":"## Fitting on engineered data","9500127d":"# Electric Motor Temperature\n\nThe data set comprises several sensor data collected from a permanent magnet synchronous motor (PMSM) deployed on a test bench. The PMSM represents a german OEM's prototype model. Test bench measurements were collected by the [LEA department](https:\/\/ei.uni-paderborn.de\/en\/lea\/) at Paderborn University. \nThis data set is mildly anonymized.\n\n## Content\n\nAll recordings are sampled at 2 Hz. \nThe data set consists of multiple measurement sessions, which can be distinguished from each other by column \"profile_id\".\nA measurement session can be between one and six hours long.\n\nThe motor is excited by hand-designed driving cycles denoting a reference motor speed and a reference torque.\nCurrents in d\/q-coordinates (columns \"i_d\" and i_q\") and voltages in d\/q-coordinates (columns \"u_d\" and \"u_q\") are a result of a standard control strategy trying to follow the reference speed and torque.\nColumns \"motor_speed\" and \"torque\" are the resulting quantities achieved by that strategy, derived from set currents and voltages.\n\n\n## Inspiration\n\nThe most interesting target features are rotor temperature (\"pm\"), stator temperatures (\"stator_*\") and torque.\nEspecially rotor temperature and torque are not reliably and economically measurable in a commercial vehicle.\n\n![reduced_motor_schnitt.png](attachment:992b19fd-6d46-4e08-8eba-d2d6a336c850.png)\n\nBeing able to have strong estimators for the rotor temperature helps the automotive industry to manufacture motors with less material and enables control strategies to utilize the motor to its maximum capability.\nA precise torque estimate leads to more accurate and adequate control of the motor, reducing power losses and eventually heat build-up.\n\nThis is a [typical stator winding damage](http:\/\/www.aawva.com\/resource\/2016\/3\/25\/typical-failures-in-three-phase-stator-windings):\n![](https:\/\/images.squarespace-cdn.com\/content\/v1\/56c20776f699bbc849dc0a69\/1458920344064-XRVKU3O788XYJNS0TQ3A\/ke17ZwdGBToddI8pDm48kH6fpDepVnHDbJBHVrcFH3VZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PICKwaTIMLhQJ9fACjqaf26fF01mQeTIdmmTPu11VCbBkKMshLAGzx4R3EDFOm1kBS\/image-asset.jpeg?format=500w)\n\nOn the rotor part, irreversible demagnetization of the permanent magnets can be caused by overheating, which represents an even more severe instance of motor damage.\n\nIn the following, we analyze the data set and exhibit its peculiarities.","74719bec":"Now we have a look at the input features","89f347e2":"# Distributions","4d8b9f5f":"Let's have a look at a heat map that shows which operation points where visited how often (brighter areas are visited more often).","2f8b2f62":"\n\nWe find\n* distributions are not very gaussian,\n* we often have multi-modal distributions,\n* some features show a significant spike in their distribution. This is reasonable as these are the values that are held during no excitation (resting\/cooldown phase)\n* target temperatures are less spikey, indicating a slower progress or response to input features (read 'greater time constants').\n","ec7a423e":"## Feature Engineering\nWe add some static features, that we can infer from the given raw signals and might help expose relevant patterns.","6b8f416e":"We see that some driving cycles only capture little of the valid operation region, while other profiles do comprehensive random walks over the full operative range.\n\nNote that motors are power rated, and since power is defined as $P = motorspeed \\cdot torque$, there are elliptical borders that can't be exceeded.\n\nBy the way, profile no. 11 denotes the repeated [FTP-75](https:\/\/dieselnet.com\/standards\/cycles\/ftp75.php).","e37a0dd6":"# Import libs and load data","d49ee0d7":"We find:\n* No gaussian distributions are recognizable,\n* nor is the target temperature distinguishable in the spatial dimension,\n* good features need to be found, different from the raw sensor data.\n\n# Baseline Predictions with Ordinary Least Squares and Random Forest Regression\nIn the following we try to predict the permanent magnet temperature *pm*.","44f79232":"Moreover, the trend in the raw signals is of very high information as has been mentioned in literature (see [ResearchGate Paper](https:\/\/www.researchgate.net\/publication\/331976678_Empirical_Evaluation_of_Exponentially_Weighted_Moving_Averages_for_Simple_Linear_Thermal_Modeling_of_Permanent_Magnet_Synchronous_Machines)).\nWe can compute the trend by calculating _exponentially weighted moving averages_ (EWMA). Note, that this is nothing more than low-pass filtering the signals."}}