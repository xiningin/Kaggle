{"cell_type":{"fade5b2a":"code","36b16851":"code","ddce5552":"code","a98f45b7":"code","04e24fbc":"code","ff3984f5":"code","244afda7":"code","ad2c54fb":"code","af68c2e1":"code","f3fbf69f":"code","7abab302":"code","a02e07d6":"code","bca53d2d":"code","0976ec9f":"code","39b50c81":"code","0667d2f7":"code","e4a01e27":"code","8d3ec00d":"code","3187770d":"code","20e52efd":"code","2978c048":"code","dee8ccf1":"code","8fadfd1c":"code","16a9d182":"code","957d3aad":"code","3b976bc4":"code","01bfb5bf":"code","0d809858":"code","ae2370b4":"code","1fa41c35":"code","52780257":"code","e7c5bd16":"code","7c37eded":"code","3023630f":"code","9caae8e2":"code","e5a6db3f":"code","51c23bd6":"code","b17a3269":"code","33745266":"code","200db20d":"code","46aa10bf":"code","66254ef0":"code","084e8d94":"code","636acb28":"code","477f3ff4":"code","8855d858":"code","9d4a6569":"code","8b4da37b":"code","ff20fa2c":"code","efd93de9":"code","f6dc1347":"code","0c08e591":"code","8aebd0e4":"code","6fa4ba66":"code","4e89cf5e":"code","b3a9e3f9":"code","c0b017ac":"code","73cf3d86":"code","c4512dcb":"code","23da0d92":"code","588b95d1":"code","9227c051":"code","c2302d17":"code","07a62a38":"code","076d6996":"markdown","cf630224":"markdown","27ba2016":"markdown","37289522":"markdown","5806aaff":"markdown","9011ea5d":"markdown","a783f0dd":"markdown","6fcff3b0":"markdown","904f0523":"markdown","cba046d4":"markdown","248f2865":"markdown","b68b7aa5":"markdown","eae23ebd":"markdown","8893f5c6":"markdown","1a383d2c":"markdown","5c9fd1ae":"markdown","5512324e":"markdown","0d180c8c":"markdown","700fcbdd":"markdown","c9a73b3d":"markdown","58947143":"markdown","7828e4d5":"markdown","4372a838":"markdown","9ea1b2c1":"markdown","f6a79d10":"markdown","edee2cf3":"markdown","ffce6ec4":"markdown","9496e9f4":"markdown","ad1b39ef":"markdown","b40cb020":"markdown","f50d22d6":"markdown","dec68f78":"markdown","8ea730ee":"markdown","f5b04caa":"markdown","83469783":"markdown","d18c9cf6":"markdown","66d0c83c":"markdown","356c605a":"markdown","3b03fa3a":"markdown","d4b41ffe":"markdown","374a748c":"markdown","cb9295ec":"markdown","4e9c6d9b":"markdown","f2f9cc25":"markdown","533c7bd7":"markdown","8b6506ef":"markdown","325ce998":"markdown","a92e20e1":"markdown","b533dc2f":"markdown","8911877a":"markdown","2ae38e16":"markdown"},"source":{"fade5b2a":"%matplotlib inline\n\nfrom typing import (Any, Callable, ContextManager, Generic, Iterable, List,\n                    Mapping, Optional, Sequence, Tuple, TypeVar, Union)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","36b16851":"def _get_format_timedelta64(\n    values: Union[np.ndarray, pd.TimedeltaIndex, pd.arrays.TimedeltaArray],\n    nat_rep: str = \"NaT\",\n    box: bool = False,\n) -> Callable:\n    \"\"\"\n    Return a formatter function for a range of timedeltas.\n    These will all have the same format argument\n\n    If box, then show the return in quotes\n    \"\"\"\n\n    from pandas._libs.tslibs import iNaT\n    from pandas.core.dtypes.common import is_scalar\n    from pandas.core.dtypes.missing import isna\n\n    values_int = values.view(np.int64)\n\n    consider_values = values_int != iNaT\n\n    one_day_nanos = 86400 * 10 ** 9\n    even_days = (\n        np.logical_and(consider_values, values_int % one_day_nanos != 0).sum() == 0\n    )\n    all_sub_day = (\n        np.logical_and(consider_values, np.abs(values_int) >= one_day_nanos).sum() == 0\n    )\n\n    if even_days:\n        format = None\n    elif all_sub_day:\n        format = \"sub_day\"\n    else:\n        format = \"long\"\n\n    def _formatter(x):\n        if x is None or (is_scalar(x) and isna(x)):\n            return nat_rep\n\n        if not isinstance(x, pd.Timedelta):\n            x = pd.Timedelta(x)\n        result = x._repr_base(format=format)\n        if box:\n            result = f\"'{result}'\"\n        return result\n\n    return _formatter\n\npd.io.formats.format._get_format_timedelta64 = _get_format_timedelta64","ddce5552":"data_root = '\/kaggle\/input\/fast-stock-data-preprocessing-for-day-trading\/'\nDF_5M = pd.read_parquet(data_root + 'df_5m.parquet')\nDF_1D = pd.read_parquet(data_root + 'df_1d.parquet')","a98f45b7":"DF_5M","04e24fbc":"DF_1D","ff3984f5":"DF_5M.columns, DF_1D.columns","244afda7":"NUM_5M_CANDLES = 78\nNUM_5M_CANDLES_EARLY = 42\nDAY_DTYPE = np.datetime64('', 'D')\n\nHISTORY_1D = 20\nHISTORY_5M = NUM_5M_CANDLES \/\/ 2","ad2c54fb":"na_day_lvls = [f'Day Level {i}' for i in range(2, 6)]\nDF_1D[na_day_lvls] = DF_1D[\n    na_day_lvls + ['Day Level 1']].fillna(method='bfill', axis=1).iloc[:, :-1]","af68c2e1":"DATES_5M = DF_5M.index.get_level_values('Datetime').floor('D')\nDF_5M['True Worth'].groupby(DATES_5M).agg(sum)","f3fbf69f":"skip_1d_history = (\n    DF_1D.iloc[:, 0].groupby('Ticker', sort=True).cumcount() >= HISTORY_1D - 1).to_numpy()\nscreener_1d = skip_1d_history & (DF_1D['ADV'] >= 500_000).to_numpy()\ndates_1d = DF_1D.index.get_level_values('Date')\n\nTRAIN_MASK_1D = screener_1d & (dates_1d < '2017-12-04')\nVALID_MASK_1D = screener_1d & (dates_1d >= '2017-12-04')\n\nskip_5m_history = (\n    DF_5M.iloc[:, 0].groupby('Ticker', sort=True).cumcount() >= HISTORY_5M - 1).to_numpy()\nTICKER_DATE_5M = pd.MultiIndex.from_arrays([DF_5M.index.get_level_values('Ticker'), DATES_5M])\n\nTRAIN_MASK_5M = skip_5m_history & TICKER_DATE_5M.isin(DF_1D.index[TRAIN_MASK_1D])\nVALID_MASK_5M = skip_5m_history & TICKER_DATE_5M.isin(DF_1D.index[VALID_MASK_1D])","7abab302":"DF_5M[VALID_MASK_5M]['Indicator Worth'].mean()","a02e07d6":"DF_5M[VALID_MASK_5M]['True Worth'].mean()","bca53d2d":"def rolling_window(x: np.ndarray, window: int) -> np.ndarray:\n    from numpy.lib.stride_tricks import as_strided\n\n    return as_strided(x, x.shape[:-1] + (x.shape[-1] - window + 1, window),\n                      x.strides + (x.strides[-1],), writeable=False)\n\ndef ravel_history(df: pd.DataFrame, history: int,\n                  target_cols: Union[str, Tuple[str, ...]] = tuple()) -> Optional[pd.DataFrame]:\n    if isinstance(target_cols, str):\n        target_cols = target_cols,\n    feature_cols = df.columns.difference(target_cols, sort=False)\n    features = df.loc[:, feature_cols].to_numpy(np.float)\n    if features.shape[0] < history:\n        return None\n\n    roll = rolling_window(features.T, history).transpose(0, 2, 1)\n    data = roll.ravel('F').reshape(roll.shape[-1], -1)\n    cols = [f'({i}) {c}' if i != 0 else c\n            for i in range(-history + 1, 1)\n            for c in feature_cols]\n\n    if target_cols:\n        targ = df.loc[:, target_cols].to_numpy(np.float)\n        data = np.concatenate([data, targ[history - 1:]], axis=1)\n        cols += list(target_cols)\n\n    return pd.DataFrame(data, df.index[history - 1:], cols)","0976ec9f":"RAVEL_DF_1D = (DF_1D.groupby('Ticker', sort=True, group_keys=False)\n                    .apply(ravel_history, history=HISTORY_1D))\nRAVEL_DF_1D","39b50c81":"import math\n\nTRUE_COLS_5M = 'True Worth', 'True Target'\nINDICATOR_COLS_5M = 'Indicator Worth', 'Indicator Target'\n\ndef xgb_batch(df_5m: pd.DataFrame, pct_per_iter: float,\n              true_targets=False) -> Iterable[np.ndarray]:\n    if (pct_per_iter <= 0) or (pct_per_iter > 1):\n        raise ValueError(f'pct_per_iter must be between (0, 1], got {pct_per_iter}.')\n\n    targ_cols = (TRUE_COLS_5M if true_targets else INDICATOR_COLS_5M) + ('True ETA',)\n    # Remove Indicator when True is used, and vice versa.\n    cols = df_5m.columns.difference(INDICATOR_COLS_5M if true_targets else TRUE_COLS_5M,\n                                    sort=False)\n    df_5m = df_5m[cols]\n\n    chunk = math.ceil(len(df_5m) * pct_per_iter)\n    rows_cumsum = df_5m.iloc[:, 0].groupby('Datetime', sort=True).agg(len).cumsum()\n\n    dates = None\n    for i in range(1, 1 + math.ceil(1 \/ pct_per_iter)):\n        idx = (rows_cumsum > (i - 1) * chunk) & (rows_cumsum <= i * chunk)\n        new_dates = rows_cumsum.index[idx]\n        dates = (dates[-HISTORY_5M + 1:].append(new_dates)\n                 if dates is not None else new_dates)\n        data = df_5m.loc[pd.IndexSlice[:, dates], :]\n\n        # max() - at least one ticker has the data.\n        if data.iloc[:, 0].groupby('Ticker', sort=True).agg(len).max() < HISTORY_5M:\n            raise ValueError(f'pct_per_iter ({pct_per_iter}) too small '\n                             f'to provide a history of at least {HISTORY_5M}.')\n\n        data = (data.groupby('Ticker', sort=True, group_keys=False)\n                    .apply(ravel_history, history=HISTORY_5M, target_cols=targ_cols))\n\n        data = RAVEL_DF_1D.merge(\n            data, left_index=True,\n            right_on=['Ticker', data.index.get_level_values('Datetime').floor('D')],\n            suffixes=(' Daily', None)\n        ).drop(columns='key_1').sort_index(level='Datetime').to_numpy()\n\n        yield data","0667d2f7":"import abc\n\nT, U = TypeVar('T'), TypeVar('U')\n\nclass Learner(Generic[T, U], abc.ABC):\n    @abc.abstractmethod\n    def prepare_for_train(self, *args, **kwargs):\n        pass\n\n    @abc.abstractmethod\n    def do_epoch(self, epoch: int, train: bool) -> Iterable[T]:\n        pass\n\n    def restorable_state(self) -> ContextManager[None]:\n        class StateSaviour:\n            def __init__(self, learner: Learner):\n                super().__init__()\n                self.learner = learner\n\n            def __enter__(self):\n                self._state = self.learner.state()\n\n            def __exit__(self, exc_type, exc_value, traceback):\n                self.learner.restore_state(self._state)\n\n        return StateSaviour(self)\n\n    @abc.abstractmethod\n    def state(self) -> U:\n        pass\n\n    @abc.abstractmethod\n    def restore_state(self, state: U):\n        pass","e4a01e27":"import dataclasses\nimport pickle\n\nfrom tqdm.auto import trange, tqdm\n\nimport xgboost as xgb\n\n# TRAIN_MASK_5M and VALID_MASK_5M exclude five minute history.\n# For XGBoost history must be preserved, since it is necessary for ravel_history.\nDF_5M_TRAIN = DF_5M[TICKER_DATE_5M.isin(DF_1D.index[TRAIN_MASK_1D])]\nDF_5M_VALID = DF_5M[TICKER_DATE_5M.isin(DF_1D.index[VALID_MASK_1D])]\n\n@dataclasses.dataclass\nclass XgbStats:\n    data: np.ndarray\n    prediction: np.ndarray\n    loss: float\n\n@dataclasses.dataclass\nclass XgbLearner(Learner[XgbStats, bytes]):\n    predict_targets: bool\n    true_targets: bool\n\n    train_pct_per_iter: float = 1 \/ 5\n    valid_pct_per_iter: float = 1 \/ 3\n\n    model: Optional[xgb.Booster] = None\n    params: Optional[Mapping[str, Union[str, float]]] = None\n\n    def prepare_for_train(self, xgb_params: Mapping[str, Union[str, float]] = None):\n        if xgb_params is not None:\n            self.params = {k: v for k, v in xgb_params.items()}\n        if self.params is None:\n            self.params = {}\n\n        if self.predict_targets:\n            self.params['objective'] = 'reg:squarederror'\n        else:\n            self.params['objective'] = 'binary:logistic'\n            self.params['eval_metric'] = 'logloss'\n            self.params['scale_pos_weight'] = (\n                1 \/ DF_5M_TRAIN['True Worth' if self.true_targets else 'Indicator Worth'].mean())\n\n    def do_epoch(self, epoch: int, train: bool) -> Iterable[XgbStats]:\n        if train:\n            df, pct_per_iter = DF_5M_TRAIN, self.train_pct_per_iter\n        else:\n            df, pct_per_iter = DF_5M_VALID, self.valid_pct_per_iter\n\n        for data in tqdm(xgb_batch(df, pct_per_iter, self.true_targets),\n                         total=math.floor(1 \/ pct_per_iter), leave=False):\n            dmat = xgb.DMatrix(data[:, :-3], label=data[:, -2 if self.predict_targets else -3])\n            if train:\n                error = {}\n                self.model = xgb.train(self.params, dmat, 1, [(dmat, 'train')],\n                                       evals_result=error, verbose_eval=False,\n                                       xgb_model=self.model)\n                assert len(error['train']) == 1\n                error = next(iter(error['train'].values()))[-1]\n            else:\n                error = self.model.eval(dmat)\n                error = float(error[error.index(':') + 1 :])\n\n            yield XgbStats(data, self.model.predict(dmat), error)\n\n    def state(self) -> bytes:\n        return pickle.dumps(self.model)\n\n    def restore_state(self, state: bytes):\n        self.model = pickle.loads(state)","8d3ec00d":"class Metric(Generic[T], abc.ABC):\n    @abc.abstractmethod\n    def update(self, stats: T):\n        pass\n\n    @abc.abstractmethod\n    def get(self) -> float:\n        pass\n\nclass LossBase(Metric[T], abc.ABC):\n    def __init__(self):\n        super().__init__()\n        self._num_samples, self._loss = 0, 0.\n\n    def _update_loss(self, num_samples: int, loss: float):\n        self._num_samples += num_samples\n        self._loss += loss\n\n    def get(self) -> float:\n        return self._loss \/ self._num_samples if self._num_samples != 0 else 0.\n\nclass XgbLoss(LossBase[XgbStats]):\n    def update(self, stats: XgbStats):\n        self._update_loss(1, stats.loss)\n\nclass PositivePctBase(Metric[T], abc.ABC):\n    def __init__(self):\n        super().__init__()\n        self._num_samples, self._num_pos = 0, 0\n\n    def _update_pct(self, positive: np.array):\n        self._num_samples += positive.size\n        self._num_pos += positive.sum()\n\n    def get(self) -> float:\n        return self._num_pos \/ self._num_samples if self._num_samples != 0 else 0\n\nclass XgbPositivePctFromWorth(PositivePctBase[XgbStats]):\n    def __init__(self, threshold=.5):\n        super().__init__()\n        self.threshold = threshold\n\n    def update(self, stats: XgbStats):\n        self._update_pct(stats.prediction > self.threshold)\n\ndef worthy_trades(close_sup_res: np.ndarray, target: np.ndarray) -> np.ndarray:\n    stop_loss = np.where(target > close_sup_res[:, 0], close_sup_res[:, 1], close_sup_res[:, 2])\n    loss = close_sup_res[:, 0] - stop_loss\n    loss = np.where(loss == 0., np.inf, loss)\n    ratio = (target - close_sup_res[:, 0]) \/ loss\n    return ratio >= 2\n\nclass XgbPositivePctFromTarget(PositivePctBase[XgbStats]):\n    def update(self, stats: XgbStats):\n        # Close, Support, Resistance\n        close_sup_res = stats.data[:, [-18, -5, -4]]\n        self._update_pct(worthy_trades(close_sup_res, stats.prediction))\n\nclass PrecisionBase(Metric[T], abc.ABC):\n    def __init__(self):\n        super().__init__()\n        self._num_positive, self._num_true_positive = 0, 0\n\n    def _update_precision(self, positive: np.ndarray):\n        self._num_positive += len(positive)\n        self._num_true_positive += positive.sum()\n\n    def get(self) -> float:\n        return (self._num_true_positive \/ self._num_positive\n                if self._num_positive != 0 else 0.)\n\nclass XgbPrecisionFromWorth(PrecisionBase[XgbStats]):\n    def __init__(self, threshold=.5):\n        super().__init__()\n        self.threshold = threshold\n\n    def update(self, stats: XgbStats):\n        reference_worth = stats.data[:, -3]\n        self._update_precision(reference_worth[stats.prediction > self.threshold])\n\nclass XgbPrecisionFromTarget(PrecisionBase[XgbStats]):\n    def update(self, stats: XgbStats):\n        reference_worth = stats.data[:, -3]\n        # Close, Support, Resistance\n        close_sup_res = stats.data[:, [-18, -5, -4]]\n        self._update_precision(reference_worth[worthy_trades(close_sup_res, stats.prediction)])","3187770d":"class MetricsTable:\n    def __init__(self, columns: Tuple[str, ...]):\n        from IPython.display import display\n\n        super().__init__()\n        idx = pd.MultiIndex.from_tuples([], names=['Epoch', 'Time'])\n        cols = pd.MultiIndex.from_product([('Train', 'Valid'), columns])\n        self.stats_df = pd.DataFrame(index=idx, columns=cols)\n        self.display_handle = display(self.stats_df, display_id=True)\n\n    def update(self, elapsed_time: pd.Timedelta,\n               train_metrics: Mapping[str, float], val_metrics: Mapping[str, float]):\n        assert train_metrics.keys() == val_metrics.keys()\n        assert set(train_metrics.keys()) == set(self.stats_df.columns.levels[1])\n\n        self.stats_df = self.stats_df.append(\n            pd.Series(tuple(train_metrics.values()) + tuple(val_metrics.values()),\n                      self.stats_df.columns, name=(len(self.stats_df) + 1, elapsed_time))\n        )\n        self.display_handle.update(self.stats_df)","20e52efd":"@dataclasses.dataclass\nclass BestMetric:\n    metric: str\n    comparator: Union[np.greater, np.less]\n\ndef highest(metric: str) -> BestMetric:\n    return BestMetric(metric, np.greater)\n\ndef lowest(metric: str) -> BestMetric:\n    return BestMetric(metric, np.less)\n\ndef train(learner: Learner[T, U], num_epochs: int,\n          metrics: Mapping[str, Callable[[], Metric]],\n          best_model_metric: Optional[BestMetric] = None, **params):\n    from IPython.display import clear_output, display\n\n    metrics_keys = tuple(metrics.keys())\n    if best_model_metric is not None:\n        if best_model_metric.metric not in metrics_keys:\n            raise ValueError(f'{best_model_metric} must be one of {metrics_keys}.')\n        best_metric = np.inf if best_model_metric.comparator is np.less else -np.inf\n\n    def collect_metrics(epoch: int, train=True) -> Mapping[str, float]:\n        ms = {k: v() for k, v in metrics.items()}\n        for stats in learner.do_epoch(epoch, train):\n            for m in ms.values():\n                m.update(stats)\n        return {k: v.get() for k, v in ms.items()}\n\n    learner.prepare_for_train(**params)\n    table = MetricsTable(metrics_keys)\n    for e in trange(num_epochs, leave=False):\n        start_time = pd.Timestamp.now()\n\n        train_metrics = collect_metrics(e)\n        val_metrics = collect_metrics(e, train=False)\n\n        if best_model_metric is not None:\n            m = val_metrics[best_model_metric.metric]\n            if best_model_metric.comparator(m, best_metric):\n                best_metric = m\n                best_model = learner.state()\n\n        elapsed = (pd.Timestamp.now() - start_time).round('S')\n        table.update(elapsed, train_metrics, val_metrics)\n\n    clear_output()  # Remove progress bars.\n\n    display(table.stats_df)\n    if best_model_metric is not None:\n        print('Lowest ' if best_model_metric.comparator is np.less else 'Highest '\n              + f'{best_model_metric.metric}: {best_metric:.6}')\n        learner.restore_state(best_model)","2978c048":"from functools import partial\n\nXGB_PARAMS_TARGET = {\n    'tree_method': 'hist',\n    'booster': 'dart',\n    'rate_drop': .1,\n    'subsample': .5,\n    'colsample_bynode': .5\n}\nXGB_METRICS_TARGET = {\n    'Loss': XgbLoss,\n    'Positive %': XgbPositivePctFromTarget,\n    'Precision': XgbPrecisionFromTarget\n}\n\nXGB_PARAMS_WORTH = dict(max_delta_step=1, **XGB_PARAMS_TARGET)\nXGB_METRICS_WORTH = {\n    'Loss': XgbLoss,\n    'Positive %': XgbPositivePctFromWorth,\n    'Precision': XgbPrecisionFromWorth,\n    'High Positive %': partial(XgbPositivePctFromWorth, threshold=.75),\n    'High Precision': partial(XgbPrecisionFromWorth, threshold=.75)\n}\n\nXGB_EPOCHS = 5","dee8ccf1":"xgb_learner = XgbLearner(predict_targets=False, true_targets=False)\ntrain(xgb_learner, XGB_EPOCHS, XGB_METRICS_WORTH, highest('High Precision'),\n      xgb_params=XGB_PARAMS_WORTH)","8fadfd1c":"xgb_learner = XgbLearner(predict_targets=False, true_targets=True)\ntrain(xgb_learner, XGB_EPOCHS, XGB_METRICS_WORTH, highest('High Precision'),\n      xgb_params=XGB_PARAMS_WORTH)","16a9d182":"xgb_learner = XgbLearner(predict_targets=True, true_targets=False)\ntrain(xgb_learner, XGB_EPOCHS, XGB_METRICS_TARGET, highest('Precision'),\n      xgb_params=XGB_PARAMS_TARGET)","957d3aad":"xgb_learner = XgbLearner(predict_targets=True, true_targets=True)\ntrain(xgb_learner, XGB_EPOCHS, XGB_METRICS_TARGET, highest('Precision'),\n      xgb_params=XGB_PARAMS_TARGET)","3b976bc4":"from bottleneck import nanmean, nanstd\n\ndef standardize(*args: np.ndarray) -> Tuple[np.ndarray, Tuple[np.ndarray, ...]]:\n    all_cat = np.array([])\n    splits: List[int] = []\n    for a in args:\n        all_cat = np.concatenate([all_cat, a.reshape(-1)])\n        splits.append(len(all_cat))\n\n    mean, std = nanmean(all_cat), nanstd(all_cat)\n    all_cat -= mean\n    all_cat \/= std\n\n    return np.array([mean, std]), tuple(\n        scaled.reshape(orig.shape) for orig, scaled in zip(args, np.split(all_cat, splits)))","01bfb5bf":"from numpy.random import default_rng\n\nRNG = default_rng()\ntest_data = RNG.random(10_000)","0d809858":"%timeit test_data.mean(), test_data.std()","ae2370b4":"%timeit nanmean(test_data), nanstd(test_data)","1fa41c35":"from collections import namedtuple\n\nimport torch\nfrom torch.utils.data import DataLoader, IterableDataset\n\nticker_len = max(len(tk) for df in [DF_5M, DF_1D] for tk in df.index.levels[0])\nticker_dtype = np.dtype(f'U{ticker_len}')\n\n# Convert dataframes to NumPy for performance reasons.\nTICKERS_5M = DF_5M.index.get_level_values('Ticker').to_numpy(ticker_dtype)\nDATETIME_5M = DF_5M.index.get_level_values('Datetime').to_numpy()\nPRICE_5M = DF_5M[['Open', 'High', 'Low', 'Close', 'ATR', 'Day High', 'Day Low',\n                  'VWAP', 'EMA(9)', 'EMA(20)', 'MA(50)', 'MA(200)',\n                  'Full $ Above', 'Half $ Above', 'Half $ Below', 'Full $ Below',\n                  'Support', 'Resistance']].to_numpy()\nVOL_5M = DF_5M[['Volume', 'Day Volume']].to_numpy()\nENC_TIME_5M = DF_5M[['sin(t)', 'cos(t)']].to_numpy()\nRSI_5M = DF_5M['RSI'].to_numpy()[:, None]\nETA_5M = DF_5M['True ETA'].to_numpy()\n\nIDX_1D = DF_1D.index.to_numpy(np.dtype([('ticker', f'U{ticker_len}'), ('date', DAY_DTYPE)]))\nPRICE_1D = DF_1D[(['ATR', 'Premarket Gap', 'PCL', 'Y High', 'Y Low', 'YY High', 'YY Low']\n                  + [f'Day Level {i}' for i in range(1, 6)])].to_numpy()\nADV_1D = DF_1D['ADV'].to_numpy()[:, None]\nEARLY_CLOSE_1D = DF_1D['Early Close'].to_numpy()[:, None]\n\n# PyTorch cannot collate dataclasses.\nStockX = namedtuple('StockX', ['x_5m', 'x_1d'])\nStockScale = namedtuple('StockScale', ['price_mean_std', 'eta_scale'])\nStockData = namedtuple('StockData', ['x', 'y', 'scale'])\n\ndef as_f32_tensor(x: Union[np.ndarray, List[float]]) -> torch.Tensor:\n    return torch.as_tensor(x, dtype=torch.float)\n\n@dataclasses.dataclass(frozen=True)\nclass StockDataset(IterableDataset):\n    mask_5m: np.ndarray\n    mask_1d: np.ndarray\n    true_targets: bool = False\n    randomize: bool = True\n    rand_min_pct: float = .5\n    balance_classes: bool = True\n    scale: bool = True\n    out_5m_candles: int = HISTORY_5M\n    out_1d_candles: int = HISTORY_1D\n\n    def __post_init__(self):\n        # Normalize RSI to [0, 1] instead of [0, 100] if scaling is requested.\n        object.__setattr__(self, '_rsi_5m', RSI_5M \/ 100 if self.scale else RSI_5M)\n\n        worth_col, targ_col = TRUE_COLS_5M if self.true_targets else INDICATOR_COLS_5M\n        object.__setattr__(self, '_worth', DF_5M[worth_col].to_numpy(np.bool))\n        object.__setattr__(self, '_target', DF_5M[targ_col].to_numpy(np.bool))\n        object.__setattr__(\n            self, '_worthy_idx', (self.mask_5m & self._worth).nonzero()[0])\n        object.__setattr__(\n            self, '_worthless_idx', (self.mask_5m & ~self._worth).nonzero()[0])\n\n    @property\n    def out_5m_channels(self):\n        return sum(x.shape[1] for x in [PRICE_5M, VOL_5M, ENC_TIME_5M, self._rsi_5m])\n\n    @property\n    def out_1d_channels(self):\n        return PRICE_1D.shape[1] + ADV_1D.shape[1]\n\n    def _iloc_1d(self, ticker: str, date: np.datetime64) -> int:\n        tk_date = np.array([(ticker, date)], dtype=IDX_1D.dtype)[0]\n        iloc_1d = IDX_1D.searchsorted(tk_date)\n        if iloc_1d >= len(IDX_1D) or IDX_1D[iloc_1d] != tk_date or not self.mask_1d[iloc_1d]:\n            raise ValueError(f'({ticker}, {date}) either not present or masked out.')\n        return iloc_1d\n\n    def __iter__(self) -> Iterable[Tuple[float, StockData]]:\n        if self.randomize:\n            if self.balance_classes:\n                n = min(len(self._worthy_idx), len(self._worthless_idx))\n                npicks = RNG.integers(n * self.rand_min_pct, n, endpoint=True)\n                picks = np.concatenate([RNG.choice(self._worthy_idx, npicks, replace=False),\n                                        RNG.choice(self._worthless_idx, npicks, replace=False)])\n            else:\n                n = self.mask_5m.sum()\n                npicks = RNG.integers(n * self.rand_min_pct, n, endpoint=True)\n                picks = RNG.choice(self.mask_5m.nonzero()[0], npicks, replace=False)\n        else:\n            picks = self.mask_5m.nonzero()[0]\n\n        # Maintain order by datetime.\n        picks = picks[DATETIME_5M[picks].argsort()]\n        assert self.mask_5m[picks].all()\n\n        for i, iloc_5m in enumerate(picks, start=1):\n            ticker, day = TICKERS_5M[iloc_5m], DATETIME_5M[iloc_5m].astype(DAY_DTYPE)\n            iloc_1d = self._iloc_1d(ticker, day)\n            slice_5m = slice(iloc_5m + 1 - self.out_5m_candles, iloc_5m + 1)\n            slice_1d = slice(iloc_1d + 1 - self.out_1d_candles, iloc_1d + 1)\n            # Ensure slices are valid and do not span more than one ticker.\n            assert (slice_5m.start >= 0 and slice_1d.start >= 0\n                    and TICKERS_5M[slice_5m.start] == ticker\n                    and IDX_1D['ticker'][slice_1d.start] == ticker)\n\n            price_5m, price_1d, targ = (\n                PRICE_5M[slice_5m], PRICE_1D[slice_1d], self._target[iloc_5m])\n            vol_5m, adv_1d = VOL_5M[slice_5m], ADV_1D[slice_1d]\n            eta = ETA_5M[iloc_5m]\n\n            if self.scale:\n                # Normalize ETA to [0, 1].\n                eta_scale = NUM_5M_CANDLES_EARLY if EARLY_CLOSE_1D[iloc_1d] else NUM_5M_CANDLES\n                eta \/= eta_scale\n\n                price_mean_std, (price_5m, price_1d) = standardize(price_5m, price_1d)\n                targ -= price_mean_std[0]\n                targ \/= price_mean_std[1]\n\n                _, (vol_5m, adv_1d) = standardize(vol_5m, adv_1d)\n            else:\n                price_mean_std, eta_scale = np.array([0., 1.]), 1.\n\n            x_5m = np.concatenate([\n                price_5m, vol_5m, ENC_TIME_5M[slice_5m], self._rsi_5m[slice_5m]\n            ], axis=1)\n            x_1d = np.concatenate([price_1d, adv_1d], axis=1)\n            y = torch.tensor([self._worth[iloc_5m], targ, eta], dtype=torch.float)\n\n            assert x_5m.shape[0] == self.out_5m_candles and x_5m.shape[1] == self.out_5m_channels\n            assert x_1d.shape[0] == self.out_1d_candles and x_1d.shape[1] == self.out_1d_channels\n\n            yield as_f32_tensor([i \/ len(picks)]), StockData(\n                StockX(as_f32_tensor(x_5m), as_f32_tensor(x_1d)), y,\n                StockScale(as_f32_tensor(price_mean_std), eta_scale))","52780257":"DS_TRAIN_INDICATOR = StockDataset(TRAIN_MASK_5M, TRAIN_MASK_1D)\nDS_VALID_INDICATOR = StockDataset(VALID_MASK_5M, VALID_MASK_1D, randomize=False)\n\nDS_TRAIN_TRUE = StockDataset(TRAIN_MASK_5M, TRAIN_MASK_1D, true_targets=True)\nDS_VALID_TRUE = StockDataset(VALID_MASK_5M, VALID_MASK_1D, true_targets=True, randomize=False)","e7c5bd16":"def tqdm_stocks(source: Union[DataLoader, StockDataset],\n                leave=False) -> Iterable[Tuple[torch.Tensor, StockData]]:\n    ds = source.dataset if isinstance(source, DataLoader) else source\n    if not isinstance(ds, StockDataset):\n        raise TypeError(f'{type(ds).__name__} is not a StockDataset')\n\n    source = iter(enumerate(source, start=1))\n    seen, (pct, data) = next(source, (None, (None, None)))\n    if seen is None:\n        return\n\n    def total(seen: int, pct: torch.Tensor) -> int:\n        return int(seen \/ pct[-1].item())\n\n    with tqdm(total=total(seen, pct), leave=leave) as pbar:\n        yield pct, data\n        pbar.update(1)\n\n        for seen, (pct, data) in source:\n            pbar.total = total(seen, pct)\n            yield pct, data\n            pbar.update(1)\n\n        pbar.update(pbar.total - pbar.n)","7c37eded":"import torch.optim as optim\n\ndef centralize(x: torch.Tensor, min_dim=1) -> torch.Tensor:\n    if x.dim() < min_dim:\n        return x\n    return x.add_(-x.mean(dim=tuple(range(1, x.dim())), keepdim=True))\n\nclass MaCache:\n    __slots__ = 'step', 'rho_t', 'step_size'\n\n    def __init__(self):\n        self.step = 0  # Real steps start with 1.\n\nclass Ranger(optim.Optimizer):\n    def __init__(self, params: Iterable[Union[torch.Tensor, Mapping[str, Any]]],\n                 lr=1e-3, betas=(.95, 0.999), eps=1e-5, weight_decay=0,  # RAdam options\n                 centralize_raw_grad=False,                              # GC options\n                 la_alpha=0.5, la_steps=6, la_moms=False):               # Lookahead options\n        if lr <= 0:\n            raise ValueError(f'lr must be positive, got {lr}.')\n        if not 0 <= betas[0] < 1:\n            raise ValueError(f'beta_1 must be in [0, 1), got {betas[0]}.')\n        if not 0 <= betas[1] < 1:\n            raise ValueError(f'beta_2 must be in [0, 1), got {betas[1]}.')\n        if eps < 0:\n            raise ValueError(f'eps must be non-negative, got {eps}.')\n        if weight_decay < 0:\n            raise ValueError(f'weight_decay must be non-negative, got {weight_decay}.')\n        if not 0 <= la_alpha <= 1:\n            raise ValueError(f'la_alpha must be in [0, 1], got {la_alpha}.')\n        if la_steps < 1:\n            raise ValueError(f'la_steps must be greater than 1, got {la_steps}.')\n\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n                        ma_cache=MaCache())\n\n        params = list(params)\n        if isinstance(params[0], dict):\n            for p in params:\n                if 'betas' in p and p['betas'] != betas:\n                    p['ma_cache'] = MaCache()\n\n        super().__init__(params, defaults)\n        self.centralize_raw_grad = centralize_raw_grad\n        self.la_alpha, self.la_steps, self.la_moms = la_alpha, la_steps, la_moms\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            ma_cache = group['ma_cache']\n            for p in group['params']:\n                grad = p.grad\n\n                if grad is None:\n                    continue\n                if grad.is_sparse:\n                    raise RuntimeError(f'{type(self).__name__} does not support sparse gradients.')\n\n                if self.centralize_raw_grad:\n                    grad = centralize(grad)\n                p.mul_(1 - group['lr'] * group['weight_decay'])\n\n                state = self.state[p]\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p)\n                    state['exp_avg_sq'] = torch.zeros_like(p)\n                    state['slow_weights'] = p.clone().detach()\n                    if self.la_moms:\n                        state['slow_exp_avg'] = torch.zeros_like(p)\n                        state['slow_exp_avg_sq'] = torch.zeros_like(p)\n\n                state['step'] += 1\n                step, (beta_1, beta_2) = state['step'], group['betas']\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n\n                exp_avg.mul_(beta_1).add_(grad, alpha=1 - beta_1)\n                exp_avg_sq.mul_(beta_2).addcmul_(grad, grad, value=1 - beta_2)\n\n                # Do not recompute rectification constants\n                # if they were already computed for a previous parameter.\n                assert ma_cache.step == step or ma_cache.step == step - 1\n                if ma_cache.step != step:\n                    ma_cache.step = step\n                    beta_2_t = beta_2 ** step\n                    rho_inf = 2 \/ (1 - beta_2) - 1\n                    ma_cache.rho_t = rho_inf - 2 * step * beta_2_t \/ (1 - beta_2_t)\n                    # Threshold in RAdam article is 4, but official implementation uses 5\n                    # since it's more conservative.\n                    if ma_cache.rho_t >= 5:\n                        ma_cache.step_size = math.sqrt(\n                            (1 - beta_2_t) * (ma_cache.rho_t - 4) * (ma_cache.rho_t - 2) * rho_inf\n                            \/ ((rho_inf - 4) * (rho_inf - 2) * ma_cache.rho_t))\n                    else:\n                        ma_cache.step_size = 1.\n                    ma_cache.step_size \/= (1 - beta_1 ** step)\n\n                grad = ((exp_avg \/ exp_avg_sq.sqrt().add_(group['eps']))\n                        if ma_cache.rho_t >= 5 else exp_avg)\n                if not self.centralize_raw_grad:\n                    grad = centralize(grad)\n\n                p.add_(grad, alpha=-ma_cache.step_size * group['lr'])\n\n                if step % self.la_steps == 0:\n                    slow_p = state['slow_weights']\n                    slow_p.add_(p - slow_p, alpha=self.la_alpha)\n                    p.copy_(slow_p)\n                    if self.la_moms:\n                        slow_exp_avg = state['slow_exp_avg']\n                        slow_exp_avg.add_(exp_avg - slow_exp_avg, alpha=self.la_alpha)\n                        exp_avg.copy_(slow_exp_avg)\n\n                        slow_exp_avg_sq = state['slow_exp_avg_sq']\n                        slow_exp_avg_sq.add_(exp_avg_sq - slow_exp_avg_sq, alpha=self.la_alpha)\n                        exp_avg.copy_(slow_exp_avg_sq)\n\n        return loss","3023630f":"@dataclasses.dataclass\nclass Scheduler(abc.ABC):\n    opt: optim.Optimizer\n\n    @abc.abstractmethod\n    def step(self, completed_epochs: float):\n        pass\n\n    def state(self) -> Mapping[str, Any]:\n        from copy import deepcopy\n\n        return {k: deepcopy(v) for k, v in self.__dict__.items() if k != 'opt'}\n\n    def restore_state(self, state: Mapping[str, Any]):\n        self.__dict__.update(state)\n\nGlobalOrLayeredParam = Union[float, Sequence[float]]\n\ndef as_layered(param: GlobalOrLayeredParam, opt: optim.Optimizer) -> Tuple[float, ...]:\n    if isinstance(param, (list, tuple)):\n        if len(param) != len(opt.param_groups):\n            raise ValueError(f'Number of groups does not match: {len(param)} in param,'\n                             f' but {len(opt.param_groups)} in optimizer.')\n        return param\n    return (param,) * len(opt.param_groups)\n\n@dataclasses.dataclass\nclass ExpScheduler(Scheduler):\n    start_lr: GlobalOrLayeredParam\n    end_lr: GlobalOrLayeredParam\n    num_iter: int\n\n    def __post_init__(self):\n        if self.num_iter <= 0:\n            raise ValueError(f'num_iter must be positive, got {self.num_iter}.')\n\n        self.start_lr = as_layered(self.start_lr, self.opt)\n        self.end_lr = as_layered(self.end_lr, self.opt)\n        self._step_num = 0\n\n        for lr, g in zip(self.start_lr, self.opt.param_groups):\n            g['lr'] = lr\n\n    def step(self, completed_epochs: float):\n        self._step_num += 1\n        for start_lr, end_lr, g in zip(self.start_lr, self.end_lr, self.opt.param_groups):\n            g['lr'] = start_lr * (end_lr \/ start_lr) ** (self._step_num \/ self.num_iter)\n\ndef cos_anneal(start: float, end: float, pct: float) -> float:\n    return end + (start - end) * (1 + math.cos(pct * math.pi)) \/ 2.\n\nclass FlatCosineScheduler(Scheduler):\n    def __init__(self, opt: optim.Optimizer, num_epochs: int, flat_pct=0.72):\n        super().__init__(opt)\n        self._anneal_start = num_epochs * flat_pct\n        self._anneal_steps = num_epochs - self._anneal_start\n        self._base_lr = tuple(g['lr'] for g in opt.param_groups)\n\n    def step(self, completed_epochs: float):\n        train_pct = completed_epochs - self._anneal_start\n        if train_pct < 0:\n            return\n        mult = cos_anneal(1., 0., train_pct \/ self._anneal_steps)\n        for g, base_lr in zip(self.opt.param_groups, self._base_lr):\n            g['lr'] = base_lr * mult","9caae8e2":"import io\n\nimport torch.nn as nn\n\nDEFAULT_DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\nNORM_TYPES = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.SyncBatchNorm,\n              nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d,\n              nn.LayerNorm, nn.GroupNorm)\nNORM_NAMES = set(t.__name__ for t in NORM_TYPES)\n\ndef is_norm_module(m: nn.Module) -> bool:\n    return (isinstance(m, torch.jit.ScriptModule) and m.original_name in NORM_NAMES\n            or isinstance(m, NORM_TYPES))\n\ndef split_decay_no_decay(module: nn.Module) -> Tuple[Mapping[str, Any], Mapping[str, Any]]:\n    norm_prefixes = [n for n, m in module.named_modules() if is_norm_module(m)]\n    decay, no_decay = [], []\n    for n, param in module.named_parameters():\n        if 'bias' in n or any(n.startswith(p) for p in norm_prefixes):\n            no_decay.append(param)\n        else:\n            decay.append(param)\n    assert len(decay) + len(no_decay) == len(tuple(module.parameters()))\n    return {'params': decay}, {'params': no_decay, 'weight_decay': 0.}\n\ndef torch_serialize(obj: Any) -> bytes:\n    # Always save to RAM, do not consume GPU memory.\n    buf = io.BytesIO()\n    torch.save(obj, buf)\n    return buf.getvalue()\n\n@dataclasses.dataclass\nclass NnStats:\n    data: StockData\n    prediction: torch.Tensor\n    loss: float\n\n@dataclasses.dataclass\nclass NnState:\n    model: bytes\n    optimizer: Optional[bytes]\n    scheduler: Optional[Any]\n\n@dataclasses.dataclass(frozen=True)\nclass NnLearner(Learner[NnStats, NnState]):\n    model: nn.Module\n    loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor]\n    opt_ctor: Callable[..., optim.Optimizer]\n    sched_ctor: Callable[..., Scheduler]\n\n    ds_train: StockDataset = dataclasses.field(repr=False)\n    ds_valid: StockDataset = dataclasses.field(repr=False)\n\n    train_dl: DataLoader = dataclasses.field(init=False)\n    valid_dl: DataLoader = dataclasses.field(init=False)\n\n    device: torch.device = DEFAULT_DEVICE\n    batch_size: int = 4096\n    drop_last_batch: bool = False\n\n    def __post_init__(self):\n        for dl, ds in zip(['train_dl', 'valid_dl'], [self.ds_train, self.ds_valid]):\n            object.__setattr__(\n                self, dl, DataLoader(ds, self.batch_size, drop_last=self.drop_last_batch))\n\n    def _create_opt(self, opt_params: Mapping[str, GlobalOrLayeredParam]):\n        object.__setattr__(self, 'opt', self.opt_ctor(split_decay_no_decay(self.model)))\n        for k, v in opt_params.items():\n            if k not in self.opt.defaults:\n                raise ValueError(f'{k} is not supported by optimizer.')\n            for v, g in zip(as_layered(v, self.opt), self.opt.param_groups):\n                g[k] = v\n\n    def prepare_for_train(self, sched_params: Optional[Mapping[str, Any]] = None,\n                          **opt_params: GlobalOrLayeredParam):\n        if sched_params is None:\n            sched_params = {}\n\n        self.model.to(self.device)\n        create_opt = not hasattr(self, 'opt') or opt_params\n        if create_opt:\n            self._create_opt(opt_params)\n        if create_opt or not hasattr(self, 'sched') or sched_params:\n            object.__setattr__(self, 'sched', self.sched_ctor(self.opt, **sched_params))\n\n    def do_epoch(self, epoch: int, train=True, progress_bar=True) -> Iterable[NnStats]:\n        dl = self.train_dl if train else self.valid_dl\n        if progress_bar:\n            dl = tqdm_stocks(dl)\n\n        for epoch_pct, batch in dl:\n            # If training and validation are run in parallel (as in find_lr),\n            # train flags must be reset on each loop iteration.\n            self.model.train(train)\n            with torch.set_grad_enabled(train):\n                assert torch.is_grad_enabled() == train\n\n                x_5m, x_1d, y = (t.to(self.device) for t in (batch.x.x_5m, batch.x.x_1d, batch.y))\n                if train:\n                    self.opt.zero_grad()\n\n                pred = self.model(x_5m, x_1d)\n                loss = self.loss(pred, y)\n\n                if train:\n                    loss.backward()\n                    self.opt.step()\n                    self.sched.step(epoch + epoch_pct[-1].item())\n\n            yield NnStats(StockData(StockX(x_5m.detach().cpu(), x_1d.detach().cpu()),\n                                    y.detach().cpu(), batch.scale),\n                          pred.detach().cpu(), loss.item())\n\n    def state(self) -> NnState:\n        from copy import deepcopy\n\n        return NnState(torch_serialize(self.model.state_dict()),\n                       torch_serialize(self.opt.state_dict()) if hasattr(self, 'opt') else None,\n                       deepcopy(self.sched.state()) if hasattr(self, 'sched') else None)\n\n    def restore_state(self, state: NnState):\n        self.model.load_state_dict(torch.load(io.BytesIO(state.model)))\n\n        if state.optimizer is None:\n            if hasattr(self, 'opt'):\n                object.__delattr__(self, 'opt')\n        else:\n            if not hasattr(self, 'opt'):\n                self.prepare_for_train()\n            self.opt.load_state_dict(torch.load(io.BytesIO(state.optimizer)))\n\n        if state.scheduler is None:\n            if hasattr(self, 'sched'):\n                object.__delattr__(self, 'sched')\n        else:\n            if not hasattr(self, 'sched'):\n                self.prepare_for_train()\n            self.sched.restore_state(state.scheduler)","e5a6db3f":"import torch.nn.functional as F\n\ndef worth_loss(pred: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n    return F.binary_cross_entropy_with_logits(pred, y[:, 0:1])\n\ndef target_loss(pred: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n    return F.mse_loss(pred, y[:, 1:2])\n\ndef target_eta_loss(pred: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n    return F.mse_loss(pred, y[:, 1:])\n\ndef worth_target_eta_loss(\n        pred: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n    return worth_loss(pred[:, 0:1], y) + target_eta_loss(pred[:, 1:], y)","51c23bd6":"class PredictorBase(nn.Module, abc.ABC):\n    def __init__(self, in_features: int, out_features: int, sigmoid_cols: Sequence[int] = tuple()):\n        super().__init__()\n        self.lin = nn.Linear(in_features, out_features)\n        self.sigmoid_cols = sigmoid_cols\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.lin(x)\n        x[:, self.sigmoid_cols] = x[:, self.sigmoid_cols].sigmoid()\n        return x\n\nclass WorthOrTarget(PredictorBase):\n    def __init__(self, num_features: int):\n        super().__init__(num_features, 1)\n\nclass TargetEta(PredictorBase):\n    def __init__(self, num_features: int):\n        super().__init__(num_features, 2, [-1])\n\nclass WorthTargetEta(PredictorBase):\n    def __init__(self, num_features: int):\n        super().__init__(num_features, 3, [-1])\n\nPredictorCtor = Callable[[int], PredictorBase]","b17a3269":"class NnLoss(LossBase[NnStats]):\n    def update(self, stats: NnStats):\n        self._update_loss(stats.data.y.size(0), stats.loss)\n\nclass NnPositivePctFromWorth(PositivePctBase[NnStats]):\n    def __init__(self, threshold=.5):\n        super().__init__()\n        self.threshold = threshold\n\n    def update(self, stats: NnStats):\n        if stats.prediction.size(1) == 1:  # Only target.\n            target = stats.prediction.squeeze()\n        self._update_pct(np.array(stats.prediction[:, 0].sigmoid() > self.threshold))\n\ndef extract_close_sup_res_and_target(\n        stats: NnStats, target: torch.Tensor) -> Tuple[np.ndarray, np.ndarray]:\n    mean_std = stats.data.scale.price_mean_std\n    mean, std = mean_std[:, 0], mean_std[:, 1]\n    target = np.array(target * std + mean)\n    # Close, Support, Resistance\n    close_sup_res = np.array(\n        stats.data.x.x_5m[:, -1, [3, 16, 17]] * std[:, None] + mean[:, None])\n    return close_sup_res, target\n\nclass NnPositivePctFromTarget(PositivePctBase[NnStats]):\n    def update(self, stats: NnStats):\n        if stats.prediction.size(1) == 1:  # Only target.\n            target = stats.prediction.squeeze()\n        elif stats.prediction.size(1) == 2:  # Target and ETA\n            target = stats.prediction[:, 0]\n        elif stats.prediction.size(1) == 3:  # Worth, Target and ETA\n            target = stats.prediction[:, 1]\n        else:\n            raise ValueError(f'Unsupported prediction shape: {stats.prediction.size()}')\n\n        close_sup_res, target = extract_close_sup_res_and_target(stats, target)\n        self._update_pct(worthy_trades(close_sup_res, target))\n\nclass NnPositivePctFromWorthAndTarget(PositivePctBase[NnStats]):\n    def __init__(self, threshold=.5):\n        super().__init__()\n        self.threshold = threshold\n\n    def update(self, stats: NnStats):\n        if stats.prediction.size(1) != 3:\n            raise ValueError(f'Unsupported prediction shape: {stats.prediction.size()}')\n\n        close_sup_res, target = extract_close_sup_res_and_target(stats, stats.prediction[:, 1])\n        self._update_pct(np.array(stats.prediction[:, 0].sigmoid() > self.threshold)\n                         & worthy_trades(close_sup_res, target))\n\nclass NnPrecisionFromWorth(PrecisionBase[NnStats]):\n    def __init__(self, threshold=.5):\n        super().__init__()\n        self.threshold = threshold\n\n    def update(self, stats: NnStats):\n        reference_worth = np.array(stats.data.y[:, 0])\n        pred = np.array(stats.prediction[:, 0].sigmoid())\n        self._update_precision(reference_worth[pred > self.threshold])\n\nclass NnPrecisionFromTarget(PrecisionBase[NnStats]):\n    def update(self, stats: NnStats):\n        if stats.prediction.size(1) == 1:  # Only target.\n            target = stats.prediction.squeeze()\n        elif stats.prediction.size(1) == 2:  # Target and ETA\n            target = stats.prediction[:, 0]\n        elif stats.prediction.size(1) == 3:  # Worth, Target and ETA\n            target = stats.prediction[:, 1]\n        else:\n            raise ValueError(f'Unsupported prediction shape: {stats.prediction.size()}')\n\n        reference_worth = np.array(stats.data.y[:, 0])\n        close_sup_res, target = extract_close_sup_res_and_target(stats, target)\n        self._update_precision(reference_worth[worthy_trades(close_sup_res, target)])\n\nclass NnPrecisionFromWorthAndTarget(PrecisionBase[NnStats]):\n    def __init__(self, threshold=.5):\n        super().__init__()\n        self.threshold = threshold\n\n    def update(self, stats: NnStats):\n        if stats.prediction.size(1) != 3:\n            raise ValueError(f'Unsupported prediction shape: {stats.prediction.size()}')\n\n        reference_worth = np.array(stats.data.y[:, 0])\n        pred = np.array(stats.prediction[:, 0].sigmoid())\n        close_sup_res, target = extract_close_sup_res_and_target(stats, stats.prediction[:, 1])\n\n        self._update_precision(\n            reference_worth[(pred > self.threshold) & worthy_trades(close_sup_res, target)])","33745266":"@dataclasses.dataclass\nclass LrFindResult:\n    learning_rate: np.ndarray\n    loss: np.ndarray\n\n    def __post_init__(self):\n        assert len(self.learning_rate) == len(self.loss)\n\ndef find_lr(learner: NnLearner, min_lr: float, max_lr: float,\n            train_seconds=10, loss_smoothing=.98, **params: Mapping[str, Any]) -> LrFindResult:\n    from time import time\n\n    if min_lr >= max_lr:\n        raise ValueError('min_lr is greater than max_lr.')\n    if train_seconds < 1:\n        raise ValueError(f'train_seconds must be positive, got {train_seconds}.')\n    if loss_smoothing < 0 or loss_smoothing >= 1:\n        raise ValueError(f'Invalid loss_smoothing value: {loss_smoothing}.')\n\n    start = time()\n    with learner.restorable_state():\n        learner.prepare_for_train(**params)\n        next(iter(learner.do_epoch(0, progress_bar=False)))\n    num_iter = math.ceil(train_seconds \/ (time() - start))\n\n    with learner.restorable_state():\n        sched_ctor = partial(ExpScheduler, start_lr=min_lr, end_lr=max_lr, num_iter=num_iter)\n        lr_learner = dataclasses.replace(learner, sched_ctor=sched_ctor)\n\n        if 'sched_params' in params:\n            del params['sched_params']\n        lr_learner.prepare_for_train(**params)\n\n        def infinite_epochs(train: bool) -> Iterable[NnStats]:\n            epoch = -1\n            while True:\n                epoch += 1\n                yield from lr_learner.do_epoch(epoch, train, progress_bar=False)\n\n        learning_rate, losses = [], []\n        lr, best_loss = lr_learner.opt.param_groups[0]['lr'], np.inf\n\n        # tqdm is controlled manually because it does not interact well with break.\n        with tqdm(total=num_iter, leave=False) as pbar:\n            for i, _, stats in zip(range(1, num_iter + 1),\n                                   infinite_epochs(train=True),\n                                   infinite_epochs(train=False)):\n                pbar.update(1)\n\n                loss = (loss_smoothing * losses[-1] + (1 - loss_smoothing) * stats.loss\n                        if losses else stats.loss)\n                if loss < best_loss:\n                    best_loss = loss\n                if (i >= num_iter) or (loss > 4 * best_loss):\n                    pbar.update(pbar.total - pbar.n)\n                    break\n\n                learning_rate.append(lr)\n                losses.append(loss)\n                assert len(learning_rate) == len(losses) == i\n\n                lr = lr_learner.opt.param_groups[0]['lr']\n\n    return LrFindResult(np.array(learning_rate), np.array(losses))\n\ndef find_lr_wd_bs(learner: NnLearner, min_lr: float, max_lr: float,\n                  weight_decay=(1e-4, 1e-3, 1e-2, 1e-1),\n                  batch_sizes: Iterable[Union[int, Iterable[int]]] = (1024, 2048, 4096, 8192),\n                  train_seconds=10, loss_smoothing=.98,\n                  fig_kw: Optional[Mapping[str, Any]] = None,\n                  **params: Mapping[str, Any]):\n    from IPython.display import clear_output\n\n    if fig_kw is None:\n        fig_kw = {'figsize': (16, 4)}\n    batch_sizes = np.atleast_2d(batch_sizes)\n\n    results = []\n    for bs in tqdm(batch_sizes.ravel(), leave=False):\n        batch_result = []\n        lrn = dataclasses.replace(learner, batch_size=int(bs))\n        for wd in tqdm(weight_decay, leave=False):\n            r = find_lr(lrn, min_lr, max_lr, train_seconds, loss_smoothing,\n                        weight_decay=wd, **params)\n            batch_result.append(r)\n        results.append(batch_result)\n\n    clear_output()  # Remove progress bars.\n\n    fig, axes = plt.subplots(*batch_sizes.shape, sharey='row', **fig_kw)\n    axes = np.atleast_2d(axes)\n\n    for ax_row in axes:\n        ax_row[0].set_ylabel('Loss')\n\n    for bs, batch_result, ax in zip(batch_sizes.ravel(), results, axes.ravel()):\n        ax.set_title(f'Batch Size: {bs}')\n        ax.set_xlabel('Learning Rate')\n\n        for wd, r in zip(weight_decay, batch_result):\n            ax.semilogx(r.learning_rate, r.loss, label=f'Weight decay: {wd:.2e}')\n        ax.legend()","200db20d":"@torch.no_grad()\ndef init_nn(m: nn.Module):\n    if isinstance(m, (nn.Linear, nn.Conv1d)):\n        nn.init.kaiming_normal_(m.weight.data, nonlinearity='relu')\n        nn.init.zeros_(m.bias.data)\n    elif isinstance(m, nn.LSTM):\n        for n, p in m.named_parameters():\n            if n.startswith('weight'):\n                nn.init.orthogonal_(p)\n            elif n.startswith('bias'):\n                nn.init.zeros_(p)","46aa10bf":"class MultilayerTrader(nn.Module):\n    def __init__(self, num_candles_5m: int, num_channels_5m: int,\n                 num_candles_1d: int, num_channels_1d: int, predictor_ctor: PredictorCtor,\n                 hidden_layers=4, hidden_dim=128, dropout=.1):\n        super().__init__()\n\n        def fc_layer(dim_in: int, dim_out: int):\n            return nn.Sequential(nn.Linear(dim_in, dim_out), nn.ReLU(),\n                                 nn.Dropout(dropout, inplace=True), nn.BatchNorm1d(dim_out))\n\n        self.in_layer = fc_layer(\n            num_channels_5m * num_candles_5m + num_candles_1d * num_channels_1d, hidden_dim)\n        self.hidden_layers = nn.Sequential(\n            *[fc_layer(hidden_dim, hidden_dim) for _ in range(hidden_layers)])\n        self.predictor = predictor_ctor(hidden_dim)\n\n    def forward(self, x_5m: torch.Tensor, x_1d: torch.Tensor) -> torch.Tensor:\n        x = torch.cat([x_5m.flatten(1), x_1d.flatten(1)], dim=1)\n        x = self.in_layer(x)\n        x = self.hidden_layers(x)\n        return self.predictor(x)","66254ef0":"NN_EPOCHS = 10\n\nOPT_CTOR = partial(Ranger, weight_decay=1e-3)\nSCHED_CTOR = partial(FlatCosineScheduler, num_epochs=NN_EPOCHS)\n\nNN_METRICS_WORTH = dict(metrics={\n    'Loss': NnLoss,\n    'Positive %': NnPositivePctFromWorth,\n    'Precision': NnPrecisionFromWorth,\n    'High Positive %': partial(NnPositivePctFromWorth, threshold=.75),\n    'High Precision': partial(NnPrecisionFromWorth, threshold=.75)\n}, best_model_metric=highest('High Precision'))\n\nNN_METRICS_TARGET = dict(metrics={'Loss': NnLoss,\n                                  'Positive %': NnPositivePctFromTarget,\n                                  'Precision': NnPrecisionFromTarget},\n                         best_model_metric=highest('Precision'))\n\nNN_METRICS_BOTH = dict(metrics={\n    'Loss': NnLoss,\n    'Positive %': NnPositivePctFromWorthAndTarget,\n    'Precision': NnPrecisionFromWorthAndTarget,\n    'High Positive %': partial(NnPositivePctFromWorthAndTarget, threshold=.75),\n    'High Precision': partial(NnPrecisionFromWorthAndTarget, threshold=.75)\n}, best_model_metric=highest('High Precision'))","084e8d94":"model = MultilayerTrader(\n    DS_TRAIN_INDICATOR.out_5m_candles, DS_TRAIN_INDICATOR.out_5m_channels,\n    DS_TRAIN_INDICATOR.out_1d_candles, DS_TRAIN_INDICATOR.out_1d_channels,\n    WorthOrTarget\n).apply(init_nn)\n\nnn_learner = NnLearner(model, worth_loss, OPT_CTOR, SCHED_CTOR,\n                       DS_TRAIN_INDICATOR, DS_VALID_INDICATOR)\n\nfind_lr_wd_bs(nn_learner, 1e-6, 1)","636acb28":"nn_learner = dataclasses.replace(nn_learner, batch_size=1024)\n\ntrain(nn_learner, NN_EPOCHS, lr=1e-1, weight_decay=1e-3,\n      **NN_METRICS_WORTH)","477f3ff4":"model = MultilayerTrader(\n    DS_TRAIN_TRUE.out_5m_candles, DS_TRAIN_TRUE.out_5m_channels,\n    DS_TRAIN_TRUE.out_1d_candles, DS_TRAIN_TRUE.out_1d_channels,\n    WorthOrTarget\n).apply(init_nn)\n\nnn_learner = NnLearner(model, worth_loss, OPT_CTOR, SCHED_CTOR,\n                       DS_TRAIN_TRUE, DS_VALID_TRUE)\n\nfind_lr_wd_bs(nn_learner, 1e-6, 1)","8855d858":"nn_learner = dataclasses.replace(nn_learner, batch_size=1024)\n\ntrain(nn_learner, NN_EPOCHS, lr=1e-1, weight_decay=1e-3,\n      **NN_METRICS_WORTH)","9d4a6569":"model = MultilayerTrader(\n    DS_TRAIN_INDICATOR.out_5m_candles, DS_TRAIN_INDICATOR.out_5m_channels,\n    DS_TRAIN_INDICATOR.out_1d_candles, DS_TRAIN_INDICATOR.out_1d_channels,\n    WorthOrTarget\n).apply(init_nn)\n\nnn_learner = NnLearner(model, target_loss, OPT_CTOR, SCHED_CTOR,\n                       DS_TRAIN_INDICATOR, DS_VALID_INDICATOR)\n\nfind_lr_wd_bs(nn_learner, 1e-6, 1)","8b4da37b":"nn_learner = dataclasses.replace(nn_learner, batch_size=8192)\n\ntrain(nn_learner, NN_EPOCHS, lr=1e-2, weight_decay=1e-2,\n      **NN_METRICS_TARGET)","ff20fa2c":"model = MultilayerTrader(\n    DS_TRAIN_TRUE.out_5m_candles, DS_TRAIN_TRUE.out_5m_channels,\n    DS_TRAIN_TRUE.out_1d_candles, DS_TRAIN_TRUE.out_1d_channels,\n    WorthOrTarget\n).apply(init_nn)\n\nnn_learner = NnLearner(model, target_loss, OPT_CTOR, SCHED_CTOR,\n                       DS_TRAIN_TRUE, DS_VALID_TRUE)\n\nfind_lr_wd_bs(nn_learner, 1e-6, 1)","efd93de9":"nn_learner = dataclasses.replace(nn_learner, batch_size=4096)\n\ntrain(nn_learner, NN_EPOCHS, lr=1e-1, weight_decay=1e-3,\n      **NN_METRICS_TARGET)","f6dc1347":"model = MultilayerTrader(\n    DS_TRAIN_TRUE.out_5m_candles, DS_TRAIN_TRUE.out_5m_channels,\n    DS_TRAIN_TRUE.out_1d_candles, DS_TRAIN_TRUE.out_1d_channels,\n    TargetEta\n).apply(init_nn)\n\nnn_learner = NnLearner(model, target_eta_loss, OPT_CTOR, SCHED_CTOR,\n                       DS_TRAIN_TRUE, DS_VALID_TRUE)\n\nfind_lr_wd_bs(nn_learner, 1e-6, 1)","0c08e591":"nn_learner = dataclasses.replace(nn_learner, batch_size=1024)\n\ntrain(nn_learner, NN_EPOCHS, lr=1e-1, weight_decay=1e-4,\n      **NN_METRICS_TARGET)","8aebd0e4":"model = MultilayerTrader(\n    DS_TRAIN_TRUE.out_5m_candles, DS_TRAIN_TRUE.out_5m_channels,\n    DS_TRAIN_TRUE.out_1d_candles, DS_TRAIN_TRUE.out_1d_channels,\n    WorthTargetEta\n).apply(init_nn)\n\nnn_learner = NnLearner(model, worth_target_eta_loss, OPT_CTOR, SCHED_CTOR,\n                       DS_TRAIN_TRUE, DS_VALID_TRUE)\n\nfind_lr_wd_bs(nn_learner, 1e-6, 1)","6fa4ba66":"nn_learner = dataclasses.replace(nn_learner, batch_size=8192)\n\ntrain(nn_learner, NN_EPOCHS, lr=1e-2, weight_decay=1e-2,\n      **NN_METRICS_BOTH)","4e89cf5e":"def broadcast_dim(t: torch.Tensor, dim: int, size: int) -> torch.Tensor:\n    s = list(t.size())\n    assert s[dim] == 1\n    s[dim] = size\n    return torch.broadcast_tensors(torch.empty(s), t)[1]\n\nclass RecurrentTrader(nn.Module):\n    def __init__(self, num_candles_5m: int, num_channels_5m: int,\n                 num_candles_1d: int, num_channels_1d: int,\n                 predictor_ctor: PredictorCtor, hidden_dim=64, num_layers=2, dropout=.1, bidi=True):\n        super().__init__()\n\n        self.lstm_1d = nn.LSTM(num_channels_1d, hidden_dim,\n                               num_layers, dropout=dropout, bidirectional=bidi)\n        self.lstm_5m = nn.LSTM(num_channels_5m + (2 if bidi else 1) * hidden_dim, hidden_dim,\n                               num_layers, dropout=dropout, bidirectional=bidi)\n\n        self.qk_combine = nn.Linear((1 + (2 if bidi else 1)) * num_layers * hidden_dim,\n                                    hidden_dim)\n        self.pre_softmax = nn.Linear(hidden_dim, 1)\n        self.softmax = nn.Softmax(dim=0)\n        self.dropout = nn.Dropout(dropout)\n\n        num_features = (2 if bidi else 1) * hidden_dim * num_candles_5m\n        self.predictor = predictor_ctor(num_features)\n\n    def forward(self, x_5m: torch.Tensor, x_1d: torch.Tensor) -> torch.Tensor:\n        # (N, L, C) -> (L, N, C).\n        x_5m, x_1d = x_5m.transpose(0, 1), x_1d.transpose(0, 1)\n        length_5m, (length_1d, batch_size, _) = x_5m.size(0), x_1d.size()\n\n        keyvals, hidden = self.lstm_1d(x_1d)\n\n        # (L, N, C) -> (N, L, C).\n        keyvals = keyvals.transpose(0, 1)\n        queries = hidden[0].transpose(0, 1)\n        queries = broadcast_dim(queries.reshape(batch_size, 1, -1), dim=1, size=length_1d)\n\n        attn_weights = self.dropout(self.softmax(self.pre_softmax(\n            torch.tanh(self.qk_combine(torch.cat((queries, keyvals), dim=-1)))\n        ))).transpose(1, 2)  # (N, L, 1) -> (N, 1, L).\n\n        attn_applied = torch.bmm(attn_weights, keyvals).transpose(0, 1)  # (N, 1, C) -> (1, N, C).\n        attn_applied = broadcast_dim(attn_applied, 0, length_5m)  # (1, N, C) -> (L, N, C).\n\n        out, _ = self.lstm_5m(torch.cat((attn_applied, x_5m), dim=-1), hidden)\n        return self.predictor(out.transpose(0, 1).flatten(1))  # (L, N, C) -> (N, C).","b3a9e3f9":"model = RecurrentTrader(\n    DS_TRAIN_TRUE.out_5m_candles, DS_TRAIN_TRUE.out_5m_channels,\n    DS_TRAIN_TRUE.out_1d_candles, DS_TRAIN_TRUE.out_1d_channels,\n    WorthTargetEta\n).apply(init_nn)\n\nnn_learner = NnLearner(model, worth_target_eta_loss, OPT_CTOR, SCHED_CTOR,\n                       DS_TRAIN_TRUE, DS_VALID_TRUE, drop_last_batch=True)\n\nfind_lr_wd_bs(nn_learner, 1e-6, 1, batch_sizes=[16, 32, 64, 128])","c0b017ac":"nn_learner = dataclasses.replace(nn_learner, batch_size=16)\n\ntrain(nn_learner, NN_EPOCHS, lr=1e-1, weight_decay=1e-3,\n      **NN_METRICS_BOTH)","73cf3d86":"DS_TRAIN_INDICATOR.out_5m_channels, DS_TRAIN_INDICATOR.out_1d_channels","c4512dcb":"class InceptionBlock(nn.Module):\n    def __init__(self, in_channels: int, kernels: Tuple[int, ...],\n                 kernel_out_channels: Union[int, Tuple[int, ...]], dropout=.1):\n        from itertools import repeat\n\n        super().__init__()\n\n        if isinstance(kernel_out_channels, int):\n            kernel_out_channels = tuple(repeat(kernel_out_channels, len(kernels)))\n        assert len(kernels) == len(kernel_out_channels)\n\n        self.convs = nn.ModuleList([nn.Conv1d(in_channels, c, k, padding=k \/\/ 2)\n                                    for k, c in zip(kernels, kernel_out_channels)])\n        self.act = nn.ReLU()\n        self.drop = nn.Dropout(dropout, inplace=True)\n        self.bn = nn.BatchNorm1d(in_channels + sum(kernel_out_channels))\n\n    @property\n    def out_channels(self) -> int:\n        return self.bn.num_features\n\n    # (N, C, L) -> (N, C, L)\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        features = [x]\n        for c in self.convs:\n            features.append(c(x))\n        return self.bn(self.drop(\n            self.act(torch.cat(features, dim=1))))","23da0d92":"class DenseBlock(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int,\n                 inception_kernels=(3, 5, 7, 9), growth_rate=3, dropout=.1):\n        super().__init__()\n\n        self.inception_blocks = nn.ModuleList()\n        while in_channels < out_channels:\n            ib = InceptionBlock(in_channels, inception_kernels, growth_rate, dropout)\n            in_channels += ib.out_channels\n            self.inception_blocks.append(ib)\n        self.out_channels = out_channels\n\n    # (N, C, L) -> (N, C, L)\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        features = [x]\n        for ib in self.inception_blocks:\n            features.append(ib(torch.cat(features, dim=1)))\n        return torch.cat(features, dim=1)[:, :self.out_channels, :]","588b95d1":"class PositionalEncoder(nn.Module):\n    def __init__(self, max_len: int, channels: int, dropout: float):\n        super().__init__()\n\n        pe = torch.zeros(max_len, channels)\n        pos = torch.arange(max_len)[:, None] * torch.exp(\n            -torch.arange(0., channels, 2) * math.log(10000) \/ channels)\n        pe[:, 0::2] = torch.sin(pos)\n        pe[:, 1::2] = torch.cos(pos)[:, : channels \/\/ 2]\n        pe.unsqueeze_(1)\n        self.register_buffer('pe', pe)\n\n        self.drop = nn.Dropout(dropout, inplace=True)\n\n    # (L, N, C) -> (L, N, C).\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.drop(x + self.pe[:x.size(0)])","9227c051":"class TransformerTrader(nn.Module):\n    def __init__(self, num_candles_5m: int, num_channels_5m: int,\n                 num_candles_1d: int, num_channels_1d: int,\n                 predictor_ctor: PredictorCtor, dense_channels=128,\n                 tf_heads=4, tf_layers=3, tf_dim_ff=256, dropout=.1):\n        super().__init__()\n\n        self.dense_5m = DenseBlock(num_channels_5m, dense_channels)\n        self.dense_1d = DenseBlock(num_channels_1d, dense_channels)\n\n        self.pe_5m = PositionalEncoder(num_candles_5m, dense_channels, dropout)\n        self.pe_1d = PositionalEncoder(num_candles_1d, dense_channels, dropout)\n\n        self.tf = nn.Transformer(dense_channels, tf_heads, tf_layers,\n                                 tf_layers, tf_dim_ff, dropout)\n        self.predictor = predictor_ctor(num_candles_5m * dense_channels)\n\n    def forward(self, x_5m: torch.Tensor, x_1d: torch.Tensor) -> torch.Tensor:\n        # (N, L, C) -> (N, C, L) -> (L, N, C)\n        x_5m = self.dense_5m(x_5m.transpose(1, 2)).permute(2, 0, 1)\n        x_1d = self.dense_1d(x_1d.transpose(1, 2)).permute(2, 0, 1)\n\n        out = self.tf(self.pe_1d(x_1d), self.pe_5m(x_5m))\n        return self.predictor(out.transpose(0, 1).flatten(1))  # (L, N, C) -> (N, C).","c2302d17":"model = TransformerTrader(\n    DS_TRAIN_TRUE.out_5m_candles, DS_TRAIN_TRUE.out_5m_channels,\n    DS_TRAIN_TRUE.out_1d_candles, DS_TRAIN_TRUE.out_1d_channels,\n    WorthTargetEta\n).apply(init_nn)\n\nnn_learner = NnLearner(model, worth_target_eta_loss, OPT_CTOR, SCHED_CTOR,\n                       DS_TRAIN_TRUE, DS_VALID_TRUE)\n\nfind_lr_wd_bs(nn_learner, 1e-6, 5e-3, batch_sizes=[16, 32, 64, 128])","07a62a38":"nn_learner = dataclasses.replace(nn_learner, batch_size=32)\n\ntrain(nn_learner, NN_EPOCHS, lr=1e-3, weight_decay=1e-1,\n      **NN_METRICS_BOTH)","076d6996":"Here are the datasets to be used.","cf630224":"## The Data\n\nPreviously, I talked about [how to turn stock data for ~7k tickers](\nhttps:\/\/www.kaggle.com\/niladmirari\/fast-stock-data-preprocessing-for-day-trading) into a form\namenable for machine learning. Let's take a look at the data and briefly recap our goals.","27ba2016":"The following modules, intended to be used as ultimate layers in a neural net, must match\nloss functions above. `WorthOrTarget` is used for both worth and target predictions,\nsince sigmoid is already incorporated into `worth_loss`.","37289522":"Model predicts all ones: positive percentage > 90%.\n\n`True Target` next.","5806aaff":"Actual stock data no longer contains NaNs, `bottleneck`s `nan*` functions are used because\nthey are simply faster.","9011ea5d":"Due to autograd, neural nets are much more flexible when it comes to loss functions.\nXGBoost was used to predict either worth or profit target. Neural nets can predict both\nat the same time. Plus, it may be useful to predict ETA to gauge a neural net to the time\nwhen the target is going to be reached.\n\n`worth_loss` expects logits in `pred` instead of probabilities to utilize functions with better\nnumerical stability properties.","a783f0dd":"To save time, only `Worth`\/`Target`\/`ETA` will be considered. Note that the maximum learning rate\nfor `TransformerTrader` is ~0.001; higher values result in NaN loss.","6fcff3b0":"Five minute dataframe should be transformed in the same way and then joined with the daily.\nUnfortunately, the result doesn't fit into memory. The solution is to use mini-batches:\nduring gradient boosting different parts of a dataframe will be used to train different trees.","904f0523":"Because of randomization, `StockDataset` is an `IterableDataset`: each epoch yields a different\nnumber of batches. However, `StockDataset` does yield a current percentage. It can be used in\nlearning rate schedulers, or to implement a nice progress bar.","cba046d4":"Flat cosine is a learning rate scheduler for Ranger. We will also need exponential scheduler\nfor learning rate finder.","248f2865":"Adding `True ETA` didn't improve regression results.\n\nFinally, let's try to predict the triplet `True Worth`, `True Target`, and `True ETA`.","b68b7aa5":"Here's a neural net learner.\n\nFirst, there are some auxiliary functions used to split model parameters into two groups\nto disable weight decay for the norm layers and the biases.\n\nWhether to predict `Worth` or `Target` is controlled by the ultimate layer of the `model`\nand the `loss` function. `ds_train` and `ds_valid` datasets control whether worths or targets\nare `Indicator` or `True`. `drop_last_batch` is necessary for RNNs. `opt_ctor`, `sched_ctor`,\nand `batch_size` control optimizer, scheduler, and a batch size.","eae23ebd":"## XGBoost\n\nGradient boosting is a popular approach to tabular data. To use it, we first need to transform\ndataframes into (feature columns, target columns) format. For a single point in time,\nfeature columns are indicators discussed above, but we must not forget to include\nprevious time points as well.","8893f5c6":"The following XGBoost parameters will be used for training. To prevent overfitting,\n10% dropout is used and only half of rows and half of columns are active.\n`max_delta_step` is set to one when predicting `Worth` to compensate for unbalanced classes.\n\nWith default `train_pct_per_iter` of 1 \/ 5 there are five boosting rounds per epoch.\nNumber of epochs should not be large: five epochs already result in 25 trees.\n\nWhen predicting `Worth`, two versions of positive percentage and precision are tracked: one with\na standard threshold of 1 \/ 2, and the other with a high threshold of 3 \/ 4.","1a383d2c":"These parameters will be used when training all neural networks.","5c9fd1ae":"Let's combine the pieces together.","5512324e":"After convolutions comes Transformer. PyTorch has `Transformer` already,\nbut it doesn't have `PositionalEncoder`.","0d180c8c":"We start with `Indicator Worth`.","700fcbdd":"This is how it looks like for a daily dataframe. Note that transformed data for ticker `a`\nstarts with 2017-11-21 (starting point of a five minute dataframe), not with 2017-10-25\nas in the original daily dataframe; 2017-10-25 was turned into `(-19) ...` columns.","c9a73b3d":"Transformer with convolutional features predicts all zeros just like the RNN.\n\n## Conclusion\n\nNone of the models tried (XGBoost and neural nets) achieved acceptable precision.\n\nGradient boosting achieved the precision of 18%.\n\nNeural network were much worse. Specialized architectures used in time series analysis\n(RNN and Transformer) were no better than a simple fully connected neural network.","58947143":"Let's make neural net learn new features using CNNs. Why CNNs? Human traders process candles\nvisually, so it may be useful to adopt some techniques from computer vision. [Self-Attention in CV\npaper](https:\/\/arxiv.org\/abs\/1906.05909) shows that neural networks with convolutional first layers\nlearn better than purely attentional neural networks. That is essentially what is going to be\nimplemented here: computing new features with convolutional layers before switching to Transformer.\n\nIn CV, where input is two-dimensional, convolution complexity is quadratic in kernel size,\nand that's why everyone is using 3&times;3 kernels. For one-dimensional inputs, complexity\nis linear; thus more options are available. In fact, we can use ideas from InceptionNet to run\nconvolutions with multiple kernel sizes and then concatenate outputs in the channel dimension.","7828e4d5":"There are lots of columns, let's discuss their meaning.","4372a838":"Data is really scarce before 2017-11-21. There are eight days from 2017-11-21 till 2017-12-04,\nand three days after that. If everything after 2017-12-04 is put into the validation set,\nit will be a roughly 75%:25% split. Splitting is done with Boolean masks: `DF_5M[TRAIN_MASK_5M]` is\na training part of the five minute dataframe etc. While splitting, it makes sense to filter out\nilliquid stocks, since you're not going to trade them anyway.","9ea1b2c1":"Again regression is worse than classification.\n\nBest result was achieved when predicting `True Worth` with high confidence threshold.\nNone of the boosting models achieved acceptable performance (breakeven precision is 34%).\n\nLet's switch to another machine learning algorithm: neural networks.\n\n## Neural Networks\n\nTrees don't require parameters to be standardized, neural networks do. Standardizing stock data\nis somewhat problematic.\n\nDifferent columns have different units: you cannot mix prices with volumes, but all prices must be\nstandardized in the same way, and ditto for all volumes. You cannot standardize across tickers\nsince they often have incomparable prices. Standardization across the entire dataset for\na single ticker leaks data: neural network will learn to predict prices to make sure they have\nmean 0 and standard deviation 1.\n\nThe following function turns its arguments into 0-mean, 1 standard deviation, and returns them\nalong with the mean and standard deviation. It allows to standardize both daily and\nfive minute data simultaneously without combining them into a single array.","f6a79d10":"For boosting, regression result is worse than classification. Percentage of positives starts at\nall ones and then decays to smaller values.\n\nLet's predict `True Target`, and finish with the boosting.","edee2cf3":"Here are the metrics for neural nets. Positive percentage and precision comes in three variants:\nwhen predicting worth, when predicting targets, and when predicting both.","ffce6ec4":"Let's turn to training per se. Learner is an abstract base class, since besides XGBoost\nthere'll also be an implementation for neural nets. `prepare_for_train` will be called\nbefore training begins to configure the learner. `do_epoch` does a single pass over\nthe entire dataset and yields statistics about that pass. `state` and `restore_state` allow\nto save and restore learner's state.","9496e9f4":"Again, standard precision is like predicting all ones. High threshold precision is mostly zero.\nEven though it did climb above 10%, this result most likely will not reproduce.\n\nHaving finished with classification, let's switch to regression, starting with `Indicator Target`.","ad1b39ef":"Let's start with `Indicator Worth`.","b40cb020":"Standard precision is not all that different from predicting all ones (0.0216). Note that\nthe percentage of positives is an order of magnitude higher than the true frequency.\n\nHigh threshold precision almost twice bigger than predicting all ones, but still very low.\nHigh threshold percentage of worthy trades stays low.\n\nLet's switch to `True Worth`.","f50d22d6":"I usually use `fast.ai` as a neural net training library. Unfortunately, `fast.ai` does not work\nwith `IterableDataset`s. Relevant parts of `fast.ai` missing in bare-bones PyTorch need\nto be implemented manually.\n\nLet's start with an optimizer. According to `fast.ai` team, [the current best optimizer is Ranger](\nhttps:\/\/github.com\/lessw2020\/Best-Deep-Learning-Optimizers). Here's a slightly reformatted copy\nof its [official implementation](https:\/\/github.com\/lessw2020\/Ranger-Deep-Learning-Optimizer);\nvarious parameters were renamed to match [the paper](https:\/\/arxiv.org\/abs\/1908.03265).","dec68f78":"It is important to control various metrics during training; precision is of primary interest.\nPercent of positives allows one to check whether the model finds too many worthy trades,\nor whether the ones it found are misclassified.\n\nMetrics are computed from statistics returned by learner's `do_epoch`. Abstract base classes\nfor loss, positives, and precision are, again, because of neural nets. Positive percentage\nand precision come in two variants: one is used when predicting the worth, and the other&mdash;when\npredicting profit target.","8ea730ee":"Among other things, XGBoost provided for free, was a support for dataset randomization\nand class balancing. All that must be implemented from scratch for neural nets.\n\nWe start by extracting prices, volumes, and other columns from dataframes for the purpose\nof later rescaling. Standardization factors for price and volume are computer using `standardize`\nfrom above. Notice that the value to be predicted (target price) does not influence\nstandardization. `RSI`, which is originally from 0 to 100, is scaled to \\[0, 1\\].\n`True ETA` is scaled to \\[0, 1\\] as well; scaling factor is the number of five minute candles\nin that day. Since predicted target price and ETA must be rescaled back, dataset yields scaling\nfactors along with feature and target columns. For class balancing, worthy trades are separated\nfrom worthless, and the same number of random picks is made from both classes.\n\nMost stocks follow the market: if the majority of stocks starts to sell off in the future,\nit might serve as a signal to sell in the present. To avoid such data leaks, all picks\nare datetime-ordered: stock from a future point in time is not seen until all stocks\nfrom previous points in time have been seen.","f5b04caa":"First model to try is the simplest one: multilayer fully-connected neural network.","83469783":"Predicted precision is not much different from predicting all ones. Positive percentage is\nan order of magnitude higher than necessary.\n\nIn contrast to XGBoost, raising threshold didn't help. With high threshold, model mostly\npredicts all zeros.\n\n`True Worth` goes next.","d18c9cf6":"Implementation of a `Learner` for neural nets is complete. Let's switch to the next thing\nfrom `fast.ai`: learning rate finder. It trains a neural net for a small number of iterations,\nincreasing learning rate exponentially from `min_lr` to `max_lr`.\n\nOther `params` are passed directly to `prepare_for_train`. Running `find_lr` multiple times\nmakes it possible to search for optimal values of `weight_decay` and `batch_size`.\n\nNumber of training iteration is controlled by `train_seconds`. According to Leslie Smith, who is\nthe inventor of learning rate finder, a proper way to find an optimal batch size is by [controlling\nthe total execution time](https:\/\/arxiv.org\/abs\/1803.09820). For a fixed number of iterations,\nlarge batches push more data through the neural network. It favours large batches even though small\nbatch sizes act as a regularizer. Fixing execution time makes large and small batches comparison\nfairer.","66d0c83c":"There can possibly be at most five `Day Level #`, but not all are present, some are NaN.\nThese NaNs must be filled with values before feeding them into ML model. Levels are sorted\nby how frequently they were tested; the first level was tested the most. Let's fill NaNs\nwith the most frequently tested level.","356c605a":"Before we begin. Pandas 1.1.0 broke timedelta display; now it always has 0 days in it.\nTimedelta indices are used extensively in this notebook, so a proper display is desirable.\nThe following restores `sub_day` formatting from pre-1.1.0 era.","3b03fa3a":"Recurrent neural network learns to predict zeros all the time even with\na standard threshold precision.\n\nThe final architecture to try, replaces LSTMs with Transformer. Compared to LSTMs,\nTransformer requires encoder and decoder inputs to have the same number of features.\nCurrently they aren't.","d4b41ffe":"Precision is higher this time, but so is the frequency of worthy trades (0.0503).\n\nHigh threshold precision consistently stays above 10% during all epochs, but it's not enough\nfor breakeven.\n\nLet's switch from classification to regression, i.e. predicting `Indicator Target`.\nWorthy trades can be found given the profit target, and comparing with the ground truth,\nprecision can still be derived.","374a748c":"Let's go even deeper and combine multiple `InceptionBlock`s into a `DenseBlock`\n(named after DenseNet). From the generated features, only `out_channels` are retained.\nIt makes it possible to equalize the number of daily and five minute channels.","cb9295ec":"Note that worthy trades occur less frequently when profit target is based on indicators.\nAlso note that these values indicate precision of an algorithm that predicts all ones.","4e9c6d9b":"OHLCV columns don't need an explanation: it's the raw stock market data. Other columns\nare indicators used by day traders and will be briefly explained later.\n\nThe goal is to find trades where reward (distance to profit target) is at least twice the risk\n(distance to stop loss). This asymmetry between profits and losses makes it possible to make\nmistakes and still make money: to achieve breakeven the model must only be 34% precise.\n\nTo squeeze the most out of a trade, all longs must be entered into when the price hits the bottom\nand exited when the price reaches the top, and vice versa for shorts. All that means that\nthe nearest local extremum is the most natural choice for a profit target, and it is represented by\na `True Target` column. `True ETA` is the number of candles till that extremum.\n\nWhere to put your stop loss? It is conventional to use price levels provided by various indicators.\nIndicators that are going to be used are taken from Andrew Aziz books about day trading. The same\nindicators are usually used by day traders as an estimate of a profit target, since the real value\nof the next extremum cannot be known up front. `Indicator Target` is such an estimate.\nIt is obtained from `True Target` by rounding it to an indicator price level: closest from below\nfor longs and closest from above for shorts. Trade direction is encoded implicitly\nin `True Target`, which is the next extremum: if it's greater than `Close`&mdash;it's long,\nif it's smaller&mdash;it's short. `Indicator Target` is there to check whether machine would be\nbetter off by trying to replicate the work of day traders, who only operate with estimates,\ninstead of using true values for the extrema. There is no `Indicator ETA`: due to rounding,\nestimate is reached before the real extremum.\n\nPrice levels that can serve as a stop loss\/profit target are:\n\n- `VWAP`\n- Moving averages: `EMA(9)`, `EMA(20)`, `MA(50)`, `MA(200)`\n- `Day Low` and `Day High`\n- Previous Close (`PCL`)\n- Yesterday's and two days ago low and high (`Y Low`, `Y High`, `YY Low`, `YY High`)\n- Discretionary support and resistance levels on daily charts (`Day Level #`)\n- Closest half dollars and full dollars (`Full $ Above`, `Half $ Above`, `Half $ Below`,\n  `Full $ Below`).\n\nAmong these, the closest level from below is chosen to be `Support`, and the closest from above\nis chosen to be `Resistance`. Support and resistance are used as stop losses. When stop loss\nis too close to the current price there is a danger of being stopped out immediately. Actual\nsupport and resistance were chosen to ensure there is a gap of at least twice the five minute\nAverage True Range between the current price and the level: price is expected to move by `ATR`\nin any direction, but if it moves towards the stop loss by more than that, probably it's time\nto exit the trade for a small loss. (Current price &plusmn; 2 ATR is used as a support\/resistance\nif there are no levels beyond it.)\n\n`Worth` is a Boolean indicating trades with a reward-to-loss ratio of at least two:\n\\\\[\n\\frac{\\textrm{Target} - \\textrm{Close}}{\\textrm{Close} - \\textrm{Stop Loss}} \\ge 2.\n\\\\]\n`Worth` comes in two flavours: `True` or `Indicator`, depending on what kind of Target\nhas been used. Stop Loss is `Support` for longs and `Resistance` for shorts.\n\nTwo indicators encode time for machine learning models:\n\n- `sin(t)`, `cos(t)`: encoded time of day. Trading is most active at the open and at the close.\n- `Early Close`: true if market closed early on that day, which means that out of\n  the usual 78 five minute candles only 42 are available.\n\nRemaining indicators are used by Andrew Aziz in market scanners to find stocks with\ngood day trading opportunities:\n\n- Average Daily Volume (`ADV`). Used to filter out illiquid stocks. In conjunction\n  with `Day Volume` it can be used to find stocks that are unusually active.\n- Daily Average True Range (`ATR`). Used to filter out stocks whose prices don't change much\n  during the day.\n- `Premarket Gap`. Shows unusual interest in the stock. (Actual premarket data is not available.\n  `Open` &minus; `PCL` is used as a proxy.)\n- Relative Strength Index (`RSI`) on five minute charts: extreme values (> 90, < 10) show stocks\n  whose prices are either soaring or falling very rapidly.\n\nThe last thing to notice is that five minute data for e.g. ticker `a` starts at 2017-11-21,\nwhile daily data starts at 2017-10-25. Predictions are made based on historic data, and 20 trading\ndays have been retained for exactly that purpose. Similarly, a number of five minute candles\nmust be set aside as historic data before the first prediction can be made; let's settle on a half\nof a trading day.","f2f9cc25":"Last preparatory step: initializer for neural nets.","533c7bd7":"Predicting the triplet of `Worth`\/`Tarhet`\/`ETA` didn't improve the results either.\n\nLet's switch to more complex architectures. Encoder-decoder with attention is a popular\narchitecture in time-series analysis, in particular, text processing. When you're doing\ne.g. a translation, an encoder compresses the input into an internal representation\nthat is consumed by the decoder along with the previous decoder output.\n\nFor stocks, it can be adapted in the following way. Encoder takes daily candles as input.\nDecoder takes five minute candles along with daily data processed by the encoder.\n\nThe following encoder-decoder is built using LSTMs.","8b6506ef":"To save time, let's check only `Worth`\/`Target`\/`ETA`, which is the second best result of\na previous neural network. The best result was achieved when predicting only `Worth`, but that\nwas likely an accident since most of the time the model predicted all zeros. Notice that complex\nnetworks require smaller batch sizes.","325ce998":"Here's an XGBoost implementation.\n\nThere are two ways to find a worthy trade. First one can try to predict `Worth` per se, which is\na classification task. The second option is to predict the profit target, which is\na regression task. `Worth` is then derived from profit target via:\n\\\\[\n\\frac{\\textrm{Target} - \\textrm{Close}}{\\textrm{Close} - \\textrm{Stop Loss}} \\ge 2.\n\\\\]\n`predict_targets` controls whether to do classification or regression. `true_targets` controls\nwhether to use `True` or `Indicator` `Worth` and `Target`.\n\n`prepare_for_train` accepts a map of XGBoost parameters, then sets an appropriate objective\nfunction for either worth or target prediction. Dataset is not balanced for worth prediction,\n`scale_pos_weight` accounts for unbalance.\n\nFinally, `train_pct_per_iter` and `valid_pct_per_iter` control how much of the entire dataset\nto load into memory at once.","a92e20e1":"We need to split the dataset into training and validation. Let's see how many worthy trades\nwe have on each day.","b533dc2f":"The same all ones as for the `Indicator Target`.\n\nThanks to autograd, it is very easy to construct an arbitrary loss function. Let's try to predict\nboth `True Target` and `True ETA`, which we haven't tried with XGBoost. Remember that\n`Indicator ETA` is not available.","8911877a":"`train` function drives the entire training process. It trains a `learner` for\na given `num_epochs`, collecting and displaying various `metrics`. Free-form `params` are passed\nto learner's `prepare_for_train`. In addition it can track the best achieved value\nof a given metric, and restore the model to the state with the best value when training finishes.","2ae38e16":"Metrics are displayed using Pandas DataFrame."}}