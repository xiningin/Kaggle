{"cell_type":{"e771c176":"code","ce7dcb2f":"code","6c92f4d3":"code","bd8bd44a":"code","f4b48c76":"code","971a38e2":"code","70af6cc8":"code","53ccdc26":"code","a8621fba":"markdown","ce76ad31":"markdown","cf41f5ae":"markdown","a4da4bcf":"markdown","1fe557c4":"markdown","9cd0a091":"markdown"},"source":{"e771c176":"import numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport random\nimport keras\nimport tensorflow as tf\nimport json\nsys.path.insert(0, '..\/input\/pretrained-bert-including-scripts\/master\/bert-master')\n!cp -r '..\/input\/kerasbert\/keras_bert' '\/kaggle\/working'\nBERT_PRETRAINED_DIR = '..\/input\/pretrained-bert-including-scripts\/uncased_l-12_h-768_a-12\/uncased_L-12_H-768_A-12'\nprint('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\nimport tokenization  #Actually keras_bert contains tokenization part, here just for convenience","ce7dcb2f":"lr = 2e-5\nweight_decay = 0.01\nnb_epochs=1\nbsz = 32\nmaxlen=220\n## Training data\ntrain_df = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\ntrain_df = train_df.sample(frac=1.0,random_state = 42)\n#train_df['comment_text'] = train_df['comment_text'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n',  ' ', regex=True)\ntrain_lines, train_labels = train_df['comment_text'].values, train_df.target.values \n## step parameter \ndecay_steps = int(nb_epochs*train_lines.shape[0]\/bsz)\nwarmup_steps = int(0.1*decay_steps)","6c92f4d3":"from keras_bert.keras_bert.bert import get_model\nfrom keras_bert.keras_bert.loader import load_trained_model_from_checkpoint\nprint('begin_build')\n\nconfig_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\ncheckpoint_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\nmodel = load_trained_model_from_checkpoint(config_file, checkpoint_file, training=True,seq_len=maxlen)\n#model.summary(line_length=120)","bd8bd44a":"from keras import backend as K\nclass AdamWarmup(keras.optimizers.Optimizer):\n    def __init__(self, decay_steps, warmup_steps, min_lr=0.0,\n                 lr=0.001, beta_1=0.9, beta_2=0.999,\n                 epsilon=None, kernel_weight_decay=0., bias_weight_decay=0.,\n                 amsgrad=False, **kwargs):\n        super(AdamWarmup, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.decay_steps = K.variable(decay_steps, name='decay_steps')\n            self.warmup_steps = K.variable(warmup_steps, name='warmup_steps')\n            self.min_lr = K.variable(min_lr, name='min_lr')\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n            self.lr = K.variable(lr, name='lr')\n            self.beta_1 = K.variable(beta_1, name='beta_1')\n            self.beta_2 = K.variable(beta_2, name='beta_2')\n            self.kernel_weight_decay = K.variable(kernel_weight_decay, name='kernel_weight_decay')\n            self.bias_weight_decay = K.variable(bias_weight_decay, name='bias_weight_decay')\n        if epsilon is None:\n            epsilon = K.epsilon()\n        self.epsilon = epsilon\n        self.initial_kernel_weight_decay = kernel_weight_decay\n        self.initial_bias_weight_decay = bias_weight_decay\n        self.amsgrad = amsgrad\n\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        t = K.cast(self.iterations, K.floatx()) + 1\n\n        lr = K.switch(\n            t <= self.warmup_steps,\n            self.lr * (t \/ self.warmup_steps),\n            self.lr * (1.0 - K.minimum(t, self.decay_steps) \/ self.decay_steps),\n        )\n\n        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) \/\n                     (1. - K.pow(self.beta_1, t)))\n\n        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        if self.amsgrad:\n            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        else:\n            vhats = [K.zeros(1) for _ in params]\n        self.weights = [self.iterations] + ms + vs + vhats\n\n        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n            if self.amsgrad:\n                vhat_t = K.maximum(vhat, v_t)\n                p_t = m_t \/ (K.sqrt(vhat_t) + self.epsilon)\n                self.updates.append(K.update(vhat, vhat_t))\n            else:\n                p_t = m_t \/ (K.sqrt(v_t) + self.epsilon)\n\n            if 'bias' in p.name or 'Norm' in p.name:\n                if self.initial_bias_weight_decay > 0.0:\n                    p_t += self.bias_weight_decay * p\n            else:\n                if self.initial_kernel_weight_decay > 0.0:\n                    p_t += self.kernel_weight_decay * p\n            p_t = p - lr_t * p_t\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n            new_p = p_t\n\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {\n            'decay_steps': float(K.get_value(self.decay_steps)),\n            'warmup_steps': float(K.get_value(self.warmup_steps)),\n            'min_lr': float(K.get_value(self.min_lr)),\n            'lr': float(K.get_value(self.lr)),\n            'beta_1': float(K.get_value(self.beta_1)),\n            'beta_2': float(K.get_value(self.beta_2)),\n            'epsilon': self.epsilon,\n            'kernel_weight_decay': float(K.get_value(self.kernel_weight_decay)),\n            'bias_weight_decay': float(K.get_value(self.bias_weight_decay)),\n            'amsgrad': self.amsgrad,\n        }\n        base_config = super(AdamWarmup, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","f4b48c76":"from keras.layers import Dense,Input,Flatten,concatenate,Dropout,Lambda\nfrom keras.models import Model\nimport keras.backend as K\nimport re\nimport codecs\nadamwarm = AdamWarmup(lr=lr,decay_steps = decay_steps, warmup_steps = warmup_steps,kernel_weight_decay = weight_decay)\nsequence_output  = model.layers[-6].output\npool_output = Dense(1, activation='sigmoid',kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),name = 'real_output')(sequence_output)\nmodel3  = Model(inputs=model.input, outputs=pool_output)\nmodel3.compile(loss='binary_crossentropy', optimizer=adamwarm)\nmodel3.summary()","971a38e2":"names = [weight.name for layer in model3.layers for weight in layer.weights]\nweights = model3.get_weights()\n\nfor name, weight in zip(names, weights):\n    print(name, weight.shape)","70af6cc8":"def convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for i in range(example.shape[0]):\n      tokens_a = tokenizer.tokenize(example[i])\n      if len(tokens_a)>max_seq_length:\n        tokens_a = tokens_a[:max_seq_length]\n        longer += 1\n      one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n      all_tokens.append(one_token)\n    print(longer)\n    return np.array(all_tokens)\n    \n\ndict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\ntokenizer = tokenization.FullTokenizer(vocab_file=dict_path, do_lower_case=True)\nprint('build tokenizer done')\nprint('sample used',train_lines.shape)\ntoken_input = convert_lines(train_lines,maxlen,tokenizer)\nseg_input = np.zeros((token_input.shape[0],maxlen))\nmask_input = np.ones((token_input.shape[0],maxlen))\nprint(token_input.shape)\nprint(seg_input.shape)\nprint(mask_input.shape)\nprint('begin training')\nmodel3.fit([token_input, seg_input, mask_input],train_labels,batch_size=bsz,epochs=nb_epochs)","53ccdc26":"#load test data\ntest_df = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\n#test_df['comment_text'] = test_df['comment_text'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n',  ' ', regex=True)\neval_lines = test_df['comment_text'].values\nprint(eval_lines.shape)\nprint('load data done')\ntoken_input2 = convert_lines(eval_lines,maxlen,tokenizer)\nseg_input2 = np.zeros((token_input2.shape[0],maxlen))\nmask_input2 = np.ones((token_input2.shape[0],maxlen))\nprint('test data done')\nprint(token_input2.shape)\nprint(seg_input2.shape)\nprint(mask_input2.shape)\nhehe = model3.predict([token_input2, seg_input2, mask_input2],verbose=1,batch_size=bsz)\nsubmission = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv', index_col='id')\nsubmission['prediction'] = hehe\nsubmission.reset_index(drop=False, inplace=True)\nsubmission.to_csv('submission.csv', index=False)","a8621fba":"As the Extract layer extracts only the first token where \"['CLS']\" used to be, we just take the layer and connect to the single neuron output.","ce76ad31":"## Prepare Data, Training, Predicting\n\nFirst the model need train data like [token_input,seg_input,masked input], here we set all segment input to 0 and all masked input to 1.\n\nStill I am finding a more efficient way to do token-convert-to-ids","cf41f5ae":"## Parameters and load training data","a4da4bcf":"## Build classification model with adamwarmup\n\nFirst folk the optimizer with excluding \"bias\" and \"Norm\" parameters from weight decay:","1fe557c4":"## Introduction\n\nWe have already seen many BERT model in raw tensorflow and pytorch. \nAs keras is with user-friendly UI and easy to use, I want to find out whether there is BERT model in keras.\nFinally, https:\/\/github.com\/CyberZHG\/keras-bert is what I need. I have packed it into my database (https:\/\/www.kaggle.com\/httpwwwfszyc\/kerasbert).\n\nHere I just redo the thing similar to what taindow did in keras bert(https:\/\/www.kaggle.com\/taindow\/bert-a-fine-tuning-example)\n* No data prepocessing and ~~no warm up~~\n* We only use 1\/100 of the training data as a demo\n* We use a maximum sequence length of 72\n\nSimilarly, thanks for Jon Mischo (https:\/\/www.kaggle.com\/supertaz) for uploading BERT Models + Scripts\n\n## Update on May 18\n\nWarmup optimizer is folked from the raw author https:\/\/github.com\/CyberZHG\/keras-bert. But I am not sure whether *layernormlayers* are also excluded in weight-decay.","9cd0a091":"## Load raw model"}}