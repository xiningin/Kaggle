{"cell_type":{"717b5556":"code","8eb4bfce":"code","3f5e22c9":"code","af4c6f37":"code","9bd7e3b5":"code","5c148ee4":"code","0cdf781d":"code","30e99dea":"code","bc77dab1":"code","831dd639":"code","ca6ce120":"code","af2c2a5c":"code","771035f4":"code","2081bc04":"code","907b6ac3":"code","dc93b718":"code","1b056570":"code","9dc6c0b5":"code","0d78df7d":"code","657f664e":"code","942b08ab":"code","7f50c48c":"code","6ad29725":"code","15fc9e28":"code","540d4141":"code","86a37c87":"code","8e7358cb":"code","92d1094b":"code","076ac48a":"code","0b892449":"code","075c8497":"code","dedb0d6f":"code","88e1367f":"code","4b42205e":"code","91ea9979":"code","b29eb979":"code","0f8173cd":"code","d1534421":"code","9e402434":"code","e351c655":"code","814023d6":"code","843c5153":"code","d3a5d1bd":"code","a7f38f28":"markdown","217b4841":"markdown"},"source":{"717b5556":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport string\nimport gc\nimport nltk\nfrom tensorflow.keras import Model\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Embedding, Dense, GRU, Dropout, Activation, Bidirectional, LSTM, Attention, Input, Flatten, RepeatVector, Permute, Multiply, Lambda, dot, concatenate\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.compat.v1.keras.initializers import Constant\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8eb4bfce":"train_df=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_df=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","3f5e22c9":"train_df","af4c6f37":"test_df","9bd7e3b5":"count=train_df['target'].value_counts()\nsns.barplot(count.index, count.values )","5c148ee4":"def prepprocessing(text):\n    text=text.replace(u'[#:}{[]\/\\']',' ')\n    text=text.replace(u'?', '')\n    text=text.replace(u'_', '')\n    text=text.translate(str.maketrans('', '', string.punctuation))\n    text=text.strip()\n    text=text.lower()\n    return text","0cdf781d":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n","30e99dea":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)","bc77dab1":"def remove_stopwords(text):\n    stopword=set(stopwords.words('english'))\n    tokens=word_tokenize(text)\n    filtered_text = [w for w in tokens if not w in stopword] \n    return filtered_text","831dd639":"target=train_df['target']","ca6ce120":"del train_df['target']","af2c2a5c":"train_size=train_df.shape[0]","771035f4":"df=pd.concat([train_df,test_df])","2081bc04":"df.reset_index(inplace=True)","907b6ac3":"df.drop(['index', 'id', 'keyword', 'location'], axis=1, inplace=True)","dc93b718":"df['text']=df['text'].apply(lambda x : remove_html(x))\ndf['text']=df['text'].apply(lambda x : remove_URL(x))\ndf['text']=df['text'].apply(lambda x : prepprocessing(x))\n# df['text']=df['text'].apply(lambda x :remove_stopwords(x))","1b056570":"t=Tokenizer()\nt.fit_on_texts(df['text'])","9dc6c0b5":"# df['text'] = df['text'].apply(lambda x : ' '.join(x))","0d78df7d":"word_count = lambda sentence: len(word_tokenize(sentence))\nlongest_sentence = max(df['text'], key=word_count)\nMAX_LEN = len(word_tokenize(longest_sentence))","657f664e":"VOCAB_SIZE=len(t.word_index) + 1","942b08ab":"seq=t.texts_to_sequences(df['text'])","7f50c48c":"padded_tweet=pad_sequences(seq, MAX_LEN, padding='post', truncating='post')","6ad29725":"padded_tweet.shape","15fc9e28":"tweet_train=padded_tweet[:train_size]","540d4141":"tweet_test=padded_tweet[train_size:]","86a37c87":"embedding_dict={}\n\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word = values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","8e7358cb":"embedding_matrix=np.zeros((VOCAB_SIZE,100))\n\nfor word,i in t.word_index.items():\n    if i < VOCAB_SIZE:\n        emb_vec=embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i]=emb_vec ","92d1094b":"from tensorflow.python.keras import backend as K\n\ndef recall_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\ndef precision_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    \ndef f1(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","076ac48a":"def attention_3d_block(hidden_states):\n    # @author: felixhao28.\n    # hidden_states.shape = (batch_size, time_steps, hidden_size)\n    hidden_size = int(hidden_states.shape[2])\n    # Inside dense layer\n    #              hidden_states            dot               W            =>           score_first_part\n    # (batch_size, time_steps, hidden_size) dot (hidden_size, hidden_size) => (batch_size, time_steps, hidden_size)\n    # W is the trainable weight matrix of attention Luong's multiplicative style score\n    score_first_part = Dense(hidden_size, use_bias=False, name='attention_score_vec')(hidden_states)\n    #            score_first_part           dot        last_hidden_state     => attention_weights\n    # (batch_size, time_steps, hidden_size) dot   (batch_size, hidden_size)  => (batch_size, time_steps)\n    h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name='last_hidden_state')(hidden_states)\n    score = dot([score_first_part, h_t], [2, 1], name='attention_score')\n    attention_weights = Activation('softmax', name='attention_weight')(score)\n    # (batch_size, time_steps, hidden_size) dot (batch_size, time_steps) => (batch_size, hidden_size)\n    context_vector = dot([hidden_states, attention_weights], [1, 1], name='context_vector')\n    pre_activation = concatenate([context_vector, h_t], name='attention_output')\n    attention_vector = Dense(128, use_bias=False, activation='tanh', name='attention_vector')(pre_activation)\n    return attention_vector","0b892449":"# def attention_block(inputs):\n#     # inputs.shape = (batch_size, time_steps, input_dim)\n#     input_dim = int(inputs.shape[2])\n#     attention = Dense(1, activation='tanh')(inputs)                             # input shape = batch * time_steps * 1\n#     attention = Flatten()(attention)                                            # input shape = batch * time_steps\n#     attention = Activation('softmax')(attention)                                # input shape = batch * time_steps\n#     attention = RepeatVector(input_dim)(attention)                              # input shape = batch * input_dim * time_steps\n#     attention = Permute([2, 1])(attent`a\n#                                 ion)                                      # input shape = batch * time_step * input_dim\n#     sent_representation = Multiply()([inputs, attention] )              # input shape = batch * time_step * input_dim\n#     sent_representation = Lambda(lambda xin: K.sum(xin, axis=-2), output_shape=(input_dim,))(sent_representation)              # input shape = batch * input_dim \n#     return sent_representation","075c8497":"def create_model():\n    inp = Input(shape=(MAX_LEN,))\n    embedding=Embedding(VOCAB_SIZE, 100, weights=[embedding_matrix], input_length=MAX_LEN)(inp)\n    X=Bidirectional(GRU(100, activation='tanh', dropout=0.5, return_sequences=True))(embedding)\n    X=attention_3d_block(X)\n#     X=Dropout(0.2)(X)\n#     X=Bidirectional(GRU(64, activation='tanh', dropout=0.2))(X)\n    X=Dense(1, activation='sigmoid')(X)\n    model = Model(inputs=inp, outputs=X)\n    model.compile(loss='binary_crossentropy',optimizer='sgd',metrics=['accuracy', f1])\n    return model","dedb0d6f":"# def create_model():\n#     model=Sequential()\n#     embedding=Embedding(VOCAB_SIZE, 100, weights=[embedding_matrix], input_length=MAX_LEN)\n#     model.add(embedding)\n#     model.add(Bidirectional(LSTM(128, activation='tanh', return_sequences=True)))\n#     model.add(Bidirectional(LSTM(64, activation='tanh')))\n#     model.add(Dropout(0.2))\n#     model.add(Dense(1, activation='sigmoid'))\n#     model.compile(loss='binary_crossentropy',optimizer='sgd',metrics=['accuracy', f1])\n#     return model","88e1367f":"X_train, X_test, y_train, y_test=train_test_split(tweet_train, target, test_size=0.10)","4b42205e":"model=create_model()\nmodel.summary()","91ea9979":"BATCH_SIZE=32\nEPOCHS=50","b29eb979":"history=model.fit(x=X_train, y=y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, validation_data=(X_test, y_test))","0f8173cd":"print(history.history.keys())","d1534421":"def plot_history(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training acc')\n    plt.plot(x, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\nplot_history(history)","9e402434":"results=model.predict(tweet_test, batch_size=BATCH_SIZE, verbose=1)","e351c655":"list1=[]\nfor i in results: \n    if i>0.5:\n        list1.append(1)\n    else:\n        list1.append(0)\n        ","814023d6":"sub=pd.DataFrame(test_df['id'])","843c5153":"sub['target']=list1","d3a5d1bd":"sub.to_csv('prediction4.csv', index=False, sep=',')","a7f38f28":"## Preprocessing","217b4841":"## Using Glove"}}