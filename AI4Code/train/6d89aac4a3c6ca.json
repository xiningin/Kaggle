{"cell_type":{"3e6dd174":"code","44c92f52":"code","1cf9b330":"code","426a8b04":"code","b6a62600":"code","c0d686d6":"code","acd6427c":"code","d98e42dc":"code","90f158e7":"code","ea6dc9a1":"code","2bf9c020":"code","c305608c":"code","a9c50be9":"code","17726ead":"code","e0f34d5e":"code","6abae585":"code","57cb2483":"code","0091c6b5":"code","0d36328a":"code","514dafdf":"code","df32b4cd":"markdown","7c8fc2c1":"markdown","a3ce67f8":"markdown","888aa10c":"markdown","c066a26d":"markdown","ed301fb7":"markdown","7a850433":"markdown","64276016":"markdown","6d93b4a1":"markdown","59eb81f9":"markdown"},"source":{"3e6dd174":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","44c92f52":"data = pd.read_csv('\/kaggle\/input\/stockmarket-sentiment-dataset\/stock_data.csv')\nprint(data.head(10))","1cf9b330":"data.Sentiment.value_counts()","426a8b04":"\nimport plotly.graph_objects as go\nfrom plotly.offline import iplot\nimport plotly.express as px\n\nfig = px.bar(x=data.Sentiment.unique(),y=[data.Sentiment.value_counts()],color=[\"1\",\"-1\"],text=data.Sentiment.value_counts())\nfig.update_traces(hovertemplate=\"Sentiment:'%{x}' Counted: %{y}\")\nfig.update_layout(title={\"text\":\"Sentiment Counts\"},xaxis={\"title\":\"Sentiment\"},yaxis={\"title\":\"Count\"})\nfig.show()\n","b6a62600":"wordList = list()\nfor i in range(len(data)):\n    temp = data.Text[i].split()\n    for k in temp:\n        wordList.append(k)","c0d686d6":"from collections import Counter\nwordCounter = Counter(wordList)\ncountedWordDict = dict(wordCounter)\nsortedWordDict = sorted(countedWordDict.items(),key = lambda x : x[1],reverse=True)\nsortedWordDict[0:20]","acd6427c":"num = 100\nlist1 = list()\nlist2 = list()\nfor i in range(num):\n    list1.append(wordCounter.most_common(num)[i][0])\n    list2.append(wordCounter.most_common(num)[i][1])","d98e42dc":"fig2 = px.bar(x=list1,y=list2,color=list2,hover_name=list1,hover_data={'Word':list1,\"Count\":list2})\nfig2.update_traces(hovertemplate=\"Word:'%{x}' Counted: %{y}\")\nfig2.update_layout(title={\"text\":\"Word Counts\"},xaxis={\"title\":\"Words\"},yaxis={\"title\":\"Count\"})\nfig2.show()","90f158e7":"from wordcloud import WordCloud\nfrom nltk.corpus import stopwords\n\nwordList2 = \" \".join(wordList)\nstopwordCloud = set(stopwords.words(\"english\"))\nwordcloud = WordCloud(stopwords=stopwordCloud,max_words=2000,background_color=\"white\",min_font_size=3).generate_from_frequencies(countedWordDict)\nplt.figure(figsize=[13,10])\nplt.axis(\"off\")\nplt.title(\"Most used words\",fontsize=20)\nplt.imshow(wordcloud)\nplt.show()","ea6dc9a1":"# First of all, We need to change negative ones to zeros for our NN\nprint(\"***********Before************\")\nprint(data.Sentiment.head(10))\ndata.Sentiment = data.Sentiment.replace(-1,0)\nprint(\"***********After*************\")\nprint(data.Sentiment.head(10))\nfig = px.bar(x=data.Sentiment.unique(),y=[data.Sentiment.value_counts()],color=[\"1\",\"0\"],text=data.Sentiment.value_counts())\nfig.update_traces(hovertemplate=\"Sentiment:'%{x}' Counted: %{y}\")\nfig.update_layout(title={\"text\":\"Sentiment Counts\"},xaxis={\"title\":\"Sentiment\"},yaxis={\"title\":\"Count\"})\nfig.show()","2bf9c020":"# Secondly, It's not very important but I wanna use same sizes of values due to overfitting\ndata2 = data.sort_values(by=\"Sentiment\")\ndata2 = data2.reset_index().iloc[0:,1:3]\nprint(\"2105:\",data2[\"Sentiment\"][2105])\nprint(\"2106:\",data2[\"Sentiment\"][2106])\ndata3 = data2.iloc[0:2106*2]\nprint(\"New value counts\")\nprint(data3.Sentiment.value_counts())\ndata = data3","c305608c":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk import word_tokenize, WordNetLemmatizer\n\nps = PorterStemmer()\nlemma = WordNetLemmatizer()\nstopwordSet = set(stopwords.words('english'))","a9c50be9":"# So let's print one by one to see what is going on\nprint(\"1)\",data['Text'][0])\ntext = re.sub('[^a-zA-Z]',\" \",data['Text'][0]) # clearing special characters and numbers\nprint(\"2)\",text)\ntext = text.lower()                            # lower\nprint(\"3)\",text)\ntext = word_tokenize(text,language='english')  # split\nprint(\"4)\",text)\ntext1 = [word for word in text if not word in stopwordSet] #clearing stopwords like \"to\", \"it\", \"over\"\ntext2 = [lemma.lemmatize(word) for word in text]           #same thing\ntext = [lemma.lemmatize(word) for word in text if(word) not in stopwordSet] # I prefer using both but as you can see they are same\nprint(\"5.1)\",text1)\nprint(\"5.2)\",text2)\nprint(\"5)\",text)\ntext = \" \".join(text)                          # list -> string\nprint(\"6)\",text)","17726ead":"textList = list()\nfor i in range(len(data)):\n    text = re.sub('[^a-zA-Z]',\" \",data['Text'][i])\n    text = text.lower()\n    text = word_tokenize(text,language='english')\n    text = [lemma.lemmatize(word) for word in text if(word) not in stopwordSet]\n    text = \" \".join(text)\n    textList.append(text)","e0f34d5e":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\ncv = CountVectorizer(max_features=5001)  # you can change max_features to see different results\nx = cv.fit_transform(textList).toarray() # strings to 1 and 0\n#cvs = x.sum(axis=0)\n#print(cvs)          # to see word sum column by column\n\ny = data[\"Sentiment\"]\n\npca = PCA(n_components=256) # you can change n_components to see different results\nx = pca.fit_transform(x)    # fits 5001 columns to 256 with minimal loss\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=21) # splitting x and y for train\/test","6abae585":"from sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\n\nmodelList = []\nmodelList.append((\"LogisticReg\",LogisticRegression()))\nmodelList.append((\"GaussianNB\",GaussianNB()))\nmodelList.append((\"BernoulliNB\",BernoulliNB()))\nmodelList.append((\"DecisionTree\",DecisionTreeClassifier()))\nmodelList.append((\"RandomForest\",RandomForestClassifier()))\nmodelList.append((\"KNeighbors\",KNeighborsClassifier(n_neighbors=5)))\nmodelList.append((\"SVC\",SVC()))\nmodelList.append((\"XGB\",XGBClassifier()))\n\ndef train_predict(x_train,x_test,y_train,y_test):\n    for name, classifier in modelList:\n        classifier.fit(x_train,y_train)\n        y_pred = classifier.predict(x_test)\n        print(\"{} Accuracy: {}\".format(name,accuracy_score(y_test,y_pred)))","57cb2483":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.utils import plot_model\n\ndef build_model():\n    model = Sequential()\n    \n    model.add(Dense(units=16,activation=\"relu\",init=\"uniform\",input_dim=x.shape[1]))\n    model.add(Dense(units=16,activation=\"relu\",init=\"uniform\"))\n    model.add(Dense(units=1,activation=\"sigmoid\",init=\"uniform\"))\n    \n    optimizer = Adam(lr=0.0001,beta_1=0.9,beta_2=0.999)\n    #optimizer = RMSprop(lr=0.0001,rho=0.9)\n    \n    model.compile(optimizer=optimizer,metrics=[\"accuracy\"],loss=\"binary_crossentropy\")\n    return model","0091c6b5":"model = build_model()\nplot_model(model,show_shapes=True)","0d36328a":"model.fit(x_train,y_train,epochs=15,verbose=1)\ny_pred3 = model.predict_classes(x_test)","514dafdf":"train_predict(x_train,x_test,y_train,y_test)\nprint(\"ANN Accuracy: \",accuracy_score(y_test,y_pred3.ravel()))\nprint(\"ANN Confusion Matrix\")\nprint(confusion_matrix(y_test,y_pred3.ravel()))","df32b4cd":"<a id=\"7\"><\/a>\n# Model Training and Prediction","7c8fc2c1":"# Thanks for reading, I'm open to your advices.","a3ce67f8":"<a id=\"1\"><\/a>\n# **Import Data**","888aa10c":"<a id=\"3\"><\/a>","c066a26d":"<a id=\"5\"><\/a>\n# NLP Preparation","ed301fb7":"<a id=\"6\"><\/a>\n# Building Models","7a850433":"# Basic NLP Classify\n**In this kernel, I will try to classify \"Sentiments\" with different classifier models.**\n* [Import Data](#1)\n* [Sentiment Value Count](#2)\n* [Word Value Count](#3)\n* [Data Preparation](#4)\n* [NLP Preparation](#5)\n* [Building Models](#6)\n* [Model Training and Prediction](#7)","64276016":"**As everyone can see we have 2 columns in this dataset:**\n1.  \"Text\"      : Text with special characters and numbers\n2. \" Sentiment\" : Positive and Negative ones\n\nLet's see value counts and most used words","6d93b4a1":"<a id=\"2\"><\/a>","59eb81f9":"<a id=\"4\"><\/a>\n# Data Preparation "}}