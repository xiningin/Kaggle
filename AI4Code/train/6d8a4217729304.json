{"cell_type":{"f5ff1e4c":"code","54203da3":"code","a7f632bf":"code","65ca5451":"code","c82a0cc8":"code","167410be":"code","d9cecf2e":"code","a69335ff":"code","a26cd7cd":"code","3cbd9c51":"code","44b13bf5":"code","4cc7785c":"code","c106dc38":"markdown","26e10c99":"markdown","dcf8670f":"markdown","64999960":"markdown","765904a3":"markdown","4267b72a":"markdown","0de64b6b":"markdown","1f7954ec":"markdown","f14dfb12":"markdown","5c1fe449":"markdown","a18ddcee":"markdown"},"source":{"f5ff1e4c":"import os","54203da3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport string\nimport re\n!pip install pyspellchecker\nfrom spellchecker import SpellChecker\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\n\n\ntrain_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","a7f632bf":"# Printing the head of the DataFrame to get an overview of what it looks like\nprint(train_df.head())","65ca5451":"print(train_df.info())\nprint(test_df.info())","c82a0cc8":"sns.set_style('whitegrid')\nsns.countplot(x='target', data=train_df)\n\ntrain_df['target'].value_counts(normalize='True')","167410be":"train_df['word_count']=train_df['text'].str.split().map(lambda x: len(x))\ntrain_df['char_count']=train_df['text'].str.len()\n\ngrid = sns.FacetGrid(train_df, col='target')\n\ngrid.map(plt.hist, 'word_count')\n\nprint('The average word count for Non-Disaster tweets is {}'.format(train_df[train_df['target']==0]['word_count'].mean()))\nprint('The average word count for Disaster tweets is {}'.format(train_df[train_df['target']==1]['word_count'].mean()))","d9cecf2e":"grid = sns.FacetGrid(train_df, col='target')\n\ngrid.map(plt.hist, 'char_count')\n\nprint('The average character count for Non-Disaster tweets is {}'.format(train_df[train_df['target']==0]['char_count'].mean()))\nprint('The average character count for Disaster tweets is {}'.format(train_df[train_df['target']==1]['char_count'].mean()))","a69335ff":"# Removing Punctuation\ndef remove_punctuation(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ntrain_df['text']=train_df['text'].apply(lambda x : remove_punctuation(x))\ntest_df['text']=test_df['text'].apply(lambda x : remove_punctuation(x))\n\n\n# Removing HTML tags\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ntrain_df['text']=train_df['text'].apply(lambda x : remove_html(x))\ntest_df['text']=test_df['text'].apply(lambda x : remove_html(x))\n\n\n# Removing URLs\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ntrain_df['text']=train_df['text'].apply(lambda x : remove_URL(x))\ntest_df['text']=test_df['text'].apply(lambda x : remove_URL(x))\n\n\n# Correct Spelling\nspell = SpellChecker()\n\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n\n\n#train_df['text']=train_df['text'].apply(lambda x : correct_spellings(x))\n#test_df['text']=test_df['text'].apply(lambda x : correct_spellings(x))","a26cd7cd":"X_train, X_test, y_train, y_test = train_test_split(train_df['text'], train_df['target'], random_state=1)","3cbd9c51":"pl = Pipeline([\n        ('vec', CountVectorizer()),\n        ('clf', LogisticRegression())\n    ])","44b13bf5":"pl.fit(X_train, y_train)\n\naccuracy = pl.score(X_test, y_test)\n\nprint(accuracy)","4cc7785c":"# Finally we'll input our predictions into the sample submission and submit to Kaggle for final scoring\n\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\n\nsubmission[\"target\"] = pl.predict(test_df['text'])\n\nsubmission.to_csv(\"submission.csv\", index=False)","c106dc38":"Overall there isn't much to take away other than the following:\n\nOn average, the word count and character count of tweets relating to a disaster are longer than those not relating to a disaster. However, both types of tweets seem to be limited by Twitter's 144 character count maximum, shown by the bunching of data points at the high end. Perhaps a more recent data set that allows up to 280 characters (current Twitter limit) would widen the gap between these statistics.\n","26e10c99":"## Importing required libraries and reading in our csv files","dcf8670f":"Thankfully it seems like most of the data is populated. Although, the location column is missing over 30% of its values. This may become a problem if our model makes use of this field.","64999960":"After printing the head of our training DataFrame we can see that this is a very simple data set. Containing the an ID, keyword, location, the tweet, and the classification. It is concerning that none of the visible rows contain values for keyword or location. Let's dig a little deeper.","765904a3":"## Contents\n\n* Exploratory Data Analysis\n* Data Cleansing\n* Classifier and Predictions\n* Evaluation and Submission","4267b72a":"# Evaluation and submission","0de64b6b":"We have more non-disaster tweets than disaster tweets at a balance of 57% vs. 43%.\n\nNext let's see if we can identify any obvious differences between the tweets in terms of length, characters, punctuation, etc.","1f7954ec":"# Cleaning the data\n\nAs shown in the previous analysis it is clear that we need to do some cleaning before we go anywhere near modelling. We will clean the data in the following ways:\n* Removing Uneccesary Punctuation\n* Removing HTML\/URLs\n* Spelling Correction","f14dfb12":"# Creating our classifier and making predictions\n\nIn this first attempt, we'll be using the simple Bag-of-words method to represent our text for machine learning. This method discards information about grammar and word order and just works with frequency of occurance. This is far from optimal but is a quick and easy way to get our first submission in the books.\n\nThe CountVectorizer() that we'll be using goes through a 3 step process:\n1. First, it will tokenize all of the strings\n2. Second, it builds a \"vocabulary\" of words that occur\n3. Third, it counts the occurances of each token in the vocabulary","5c1fe449":"# Introduction\n\nThis is my first attempt at working with NLP. Aiming to just get a reasonable submission in the easiest and fastest way possible.","a18ddcee":"# Some basic EDA with comparisons between the Disaster\/Non-Disaster tweets"}}