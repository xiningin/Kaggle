{"cell_type":{"99741849":"code","8728b722":"code","4d486011":"code","98f56dd7":"code","e5ae4838":"code","316ee6e4":"code","b116c9e4":"code","1511c6e6":"code","fdba83dd":"code","5da414a8":"code","b4b5c1f4":"code","2a32fc5f":"code","51d18311":"code","9eba93b8":"code","5ca2fc27":"code","d3c29cff":"code","d02c8e40":"code","c8961188":"code","98178fcd":"code","90ccc3ab":"code","db6b73e7":"code","db84ece6":"code","a309ffdf":"code","bb5cd0e2":"code","8779b619":"code","1fa25fe5":"code","c9bd7c46":"code","3e4ea73c":"code","817412ce":"code","708622bf":"code","4c28e44a":"code","b38914e8":"code","6d3ec5cc":"code","832362ca":"code","905a98a8":"code","ac676311":"code","dde6623c":"markdown","64889d5d":"markdown","ac316b15":"markdown","95aa9111":"markdown","17b665fb":"markdown","a48ea1d2":"markdown","8ce59b03":"markdown","df7ca92f":"markdown","b46b590e":"markdown","c104eca8":"markdown","45e63579":"markdown"},"source":{"99741849":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8728b722":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport random\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import layers, optimizers, Sequential","4d486011":"# set random seed for tf and numpy\ntf.random.set_seed(42)\nnp.random.seed(42)","98f56dd7":"# load\/read data\ntrain_path = '\/kaggle\/input\/fashionmnist\/fashion-mnist_train.csv'\ntest_path = '\/kaggle\/input\/fashionmnist\/fashion-mnist_test.csv'\n# data = tf.keras.utils.get_file('train.csv',URL) to download file\ntrain = pd.read_csv(train_path)\ntest  = pd.read_csv(test_path)\ntrain.shape , test.shape","e5ae4838":"train.head(10)","316ee6e4":"train.info()","b116c9e4":"# Normalize the data\ntrain_arr = np.array(train, dtype='float32')\nx = train_arr[:,1:]\/255\ny = train_arr[:,0]\n\ntest_arr = np.array(test, dtype='float32')\nx_test = test_arr[:,1:]\/255\ny_test = test_arr[:,0]\n\ntrain_arr.shape","1511c6e6":"# reshape train data for DNN model\ndnn_data = x.reshape(-1, 28, 28)\ndnn_data.shape","fdba83dd":"# reshape test data for DNN model\nx_test = x_test.reshape(x_test.shape[0], 28, 28)\nx_test.shape","5da414a8":"x_train, x_val, y_train, y_val = train_test_split(dnn_data,y,test_size = 0.1)","b4b5c1f4":"# build an ANN model\ndnn_model = Sequential([\n    layers.Flatten(input_shape = [28,28]),\n    layers.Dense(300, activation = 'relu'),\n    layers.Dropout(0.5),\n    layers.Dense(150, activation = 'relu'),\n    layers.Dropout(0.5),\n    layers.Dense(75, activation = 'relu'),\n    layers.Dense(10, activation = 'softmax')    \n])\ndnn_model.summary()","2a32fc5f":"optimizer = optimizers.SGD(0.03)\ndnn_model.compile(optimizer= optimizer,loss = 'sparse_categorical_crossentropy',metrics = ['accuracy'])","51d18311":"history = dnn_model.fit(x_train, y_train, epochs = 20, validation_data =(x_val, y_val))","9eba93b8":"# plot a learning curve 300,150,75,10\npd.DataFrame(history.history).plot(figsize=(8,5))\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()\n# results 40 epochs\n# loss: 0.2515 - accuracy: 0.9050 - val_loss: 0.2789 - val_accuracy: 0.9000","5ca2fc27":"# tweak some hyper parameters\n# this is a very legnthy process, better to prefer when training data is less or choose less hyperparameters.\n# create function of model for randomized cv \n\ndef build_model(neurons_arr = [300,200,100],learning_rate = 1e-3):\n    model = Sequential()\n    model.add(layers.Flatten(input_shape = [28,28]))\n    for num_neuron in neurons_arr:\n        model.add(layers.Dense(num_neuron ,activation = 'relu'))\n    model.add(layers.Dense(10, activation = 'softmax'))\n    optimizer = optimizers.SGD(learning_rate)\n    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n    return model\n","d3c29cff":"# wrap model to make sklearn like model then apply randomizedsearchcv\nfrom sklearn.model_selection import RandomizedSearchCV\n#model_wrap = tf.keras.wrappers.scikit_learn.KerasClassifier(build_model)\n\nparams = {\n        'neurons_arr': [[500,300,200,100], [784, 392, 196, 98], [300,200,100]],\n        'learning_rate': [1e-3, 1e-4, 3e-3, 3e-5]\n}\n\n#rnd_search  = RandomizedSearchCV(model_wrap, params, n_iter = 10, cv = 3)\n#rnd_search.fit(x_train, y_train, epochs = 20, validation_data = (x_val, y_val))","d02c8e40":"#print(rnd_search.best_params_)\n#print(rnd_search.best_score_)","c8961188":"# final DNN model from randomizedcv's para score was 0.86 + some more manual trail\/error tweaks\nrnd_dnn_model = Sequential([\n    layers.Flatten(input_shape = [28,28]),\n    layers.Dense(392, activation = 'relu'),\n    \n    layers.Dense(196, activation = 'relu'),\n    layers.Dropout(0.5),\n    \n    layers.Dense(98, activation = 'relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.5),\n    layers.Dense(10, activation = 'softmax')    \n])\nrnd_dnn_model.summary()","98178fcd":"optimizer = optimizers.Adam(0.001)\nrnd_dnn_model.compile(optimizer = optimizer,loss = 'sparse_categorical_crossentropy',metrics = ['accuracy'])\nhistory = rnd_dnn_model.fit(x_train, y_train, epochs = 15, validation_data =(x_val, y_val))","90ccc3ab":"# plot a learning curve 500,300,150\npd.DataFrame(history.history).plot(figsize=(8,5))\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()","db6b73e7":"# squeeze the last bits of accuracy with SGD at lower lr\noptimizer = optimizers.SGD(0.0001)\nrnd_dnn_model.compile(optimizer = optimizer,loss = 'sparse_categorical_crossentropy',metrics = ['accuracy'])\nhistory = rnd_dnn_model.fit(x_train, y_train, epochs = 15, validation_data =(x_val, y_val))","db84ece6":"# with 20 more epochs (5 with adam and 15 with sgd) accuracy landed around ~93.5\n# predict classes using new model\np = rnd_dnn_model.predict_classes(x_test)\ny = y_test\ncorrect = np.nonzero(p==y)[0]\nincorrect = np.nonzero(p!=y)[0]","a309ffdf":"print(\"Correct predicted classes:\",correct.shape[0])\nprint(\"Incorrect predicted classes:\",incorrect.shape[0])","bb5cd0e2":"# reshape the data for cnn\nx = train_arr[:,1:]\/255\ny = train_arr[:,0]\ncnn_data = x.reshape(-1, 28,28,1)\ncnn_data.shape, y.shape","8779b619":"# reshape cnn test data\nx_test = test_arr[:,1:]\/255\nx_test = x_test.reshape(x_test.shape[0], 28, 28,1)\nx_test.shape","1fa25fe5":"x_train, x_val, y_train, y_val = train_test_split(cnn_data, y, test_size = 0.1)","c9bd7c46":"cnn_model1 = Sequential([\n    layers.Conv2D(64, kernel_size = (3,3), padding ='same', activation = 'relu', input_shape = [28,28,1]),\n    layers.MaxPool2D(pool_size = 2, strides = 2, padding = 'valid'),\n    layers.Conv2D(128, kernel_size = (3,3)),\n    layers.MaxPool2D(pool_size = 2, strides = 2, padding = 'valid'),\n    layers.Flatten(),\n    layers.Dense(64, activation = 'relu'),\n    layers.Dropout(0.3),\n    layers.Dense(32, activation = 'relu'),\n    layers.Dense(10, activation = 'softmax')\n])\ncnn_model1.summary()","3e4ea73c":"cnn_model1.compile(optimizer = 'Adam',loss = 'sparse_categorical_crossentropy',metrics = ['accuracy'])\nhistory = cnn_model1.fit(x_train, y_train, epochs = 10, validation_data = (x_val, y_val))","817412ce":"# more epochs are leading to overfit the model\npd.DataFrame(history.history).plot(figsize=(8,5))\nplt.grid(True)\nplt.gca().set_xlim(0,10)\nplt.gca().set_ylim(0,1)\nplt.show()","708622bf":"from functools import partial\nLeakyReLU = layers.LeakyReLU(alpha=0.1)\n# create a default layer to save repetations\nDefConv2D = partial(layers.Conv2D, kernel_size = (3,3),activation = LeakyReLU, padding = 'valid')\n\ncnn_model = Sequential([\n    layers.Conv2D(64, kernel_size = (3,3), padding ='same', input_shape = [28,28,1], activation = LeakyReLU),\n    layers.MaxPool2D(pool_size = 2, strides = 2, padding = 'same'),\n    layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n\n    DefConv2D(filters = 128),\n    layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n    \n    DefConv2D(filters = 128),\n    layers.MaxPool2D(pool_size = 2, strides = 2, padding = 'same'),\n    layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n    \n    DefConv2D(filters = 256),\n    layers.MaxPool2D(pool_size = 2, strides = 2, padding = 'same'),\n    layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"),\n    layers.Dropout(0.5),\n    \n    layers.Flatten(),\n    layers.Dense(64, activation = LeakyReLU),\n    layers.Dropout(0.5),\n    layers.Dense(10, activation = 'softmax')\n])\ncnn_model.summary()","4c28e44a":"optimizer = optimizers.Adam(0.003)\ncnn_model.compile(optimizer = optimizer,loss = 'sparse_categorical_crossentropy',metrics = ['accuracy'])\nhistory = cnn_model.fit(x_train, y_train, epochs = 20, validation_data = (x_val, y_val), verbose = 0)","b38914e8":"# more epochs are leading to overfit the model\npd.DataFrame(history.history).plot(figsize=(8,5))\nplt.grid(True)\nplt.gca().set_xlim(0,10)\nplt.gca().set_ylim(0,1)\nplt.show()\n# ignore this comments\n# changes !!55th epoch\n# conv 32,64,128(5,5)   dense layer 128(dropout 0.5),64(dropout 0.5),10   \n# Results loss 071  acc 97   val_loss 33  val_acc 92(9215 to 9290)    found train loss is way low, almost hit 93!!","6d3ec5cc":"optimizer = optimizers.Adam(0.0003)\ncnn_model.compile(optimizer = optimizer,loss = 'sparse_categorical_crossentropy',metrics = ['accuracy'])\nhistory = cnn_model.fit(x_train, y_train, epochs = 10, validation_data = (x_val, y_val))","832362ca":"# squeeze the very last bit acc with ~very little val_acc\noptimizer = optimizers.SGD(0.0003)\ncnn_model.compile(optimizer = optimizer,loss = 'sparse_categorical_crossentropy',metrics = ['accuracy'])\nhistory = cnn_model.fit(x_train, y_train, epochs = 10, validation_data = (x_val, y_val))","905a98a8":"# this results around acc ~98 and val ~93.5\n# last successful run: loss: 0.0501 - accuracy: 0.9815 - val_loss: 0.2669 - val_accuracy: 0.9363\np = cnn_model.predict_classes(x_test)\ny = y_test\ncorrect = np.nonzero(p==y)[0]\nincorrect = np.nonzero(p!=y)[0]","ac676311":"print(\"Correct predicted classes:\",correct.shape[0])\nprint(\"Incorrect predicted classes:\",incorrect.shape[0])","dde6623c":"# DNN Model 1","64889d5d":"# Summary","ac316b15":"# Model 1","95aa9111":"# Model 2","17b665fb":"More Optimizations\n* Data augumentation will boost upto 1+ maybe 2 accuracy.\n* More layers + Normalization + dropouts, More dense layers.\n* Let me Know What you Know!!","a48ea1d2":"More Optimizations\n* Add more layers + dropouts\n* Neurons with less size gaps\n* More training(epochs) with SGD with lower lr at beginning and very low near end\n* LeakyReLU for gradient exploring\/vanishing \n* Maybe more BatchNormalization(in my case the results were better off without them)\n* Let me Know What you Know!!","8ce59b03":"**Models parameter and results.**<br>\nDNN model 2.<br>\nParameters: 405,436<br>\nResults: loss: 0.2148 - accuracy: 0.9218 - val_loss: 0.2932 - val_accuracy: 0.8983 (almost ~93 acc)<br>\n\nCNN model 2.<br>\nParameters: 585,802<br>\nResults: loss: 0.0501 - accuracy: 0.9815 - val_loss: 0.2669 - val_accuracy: 0.9363 (almost ~98 acc)<br>\n<br>\nThe clear difference with a normal CNN is the parameters, they are less even with bigger model\/complex model and we achieved higher accuracy(~5+) with less epochs(though at last I couldn't resist so did more) than a tuned DNN would need. <br>\n\nWith Adam at the beginning we can converge fast, later with SGD lower lr we can squeeze the remaining accuracy. Cheers !!!","df7ca92f":"# DNN vs CNN","b46b590e":"# Create a CNN","c104eca8":"# Model 2","45e63579":"A simple demostration between DNN and CNN models with basic to better architecture reach a satisfying accuracy.<br>\nBoth DNN and CNN will have two types of models:\n1. Basic build-up model.\n2. Advance model\/tuned with better hyperparameters."}}