{"cell_type":{"f841358b":"code","e8d2a216":"code","7010c0eb":"code","34aa7efb":"code","1cd189ee":"code","797cf72c":"code","f15957c0":"code","6ebaef3d":"code","28622f02":"code","74bb4389":"code","8c913b02":"code","5cdd1d90":"code","220d224c":"code","1ae4a9a1":"code","22762657":"code","beb888e8":"code","cab54ac6":"code","166b3cd0":"code","63658a07":"markdown","76f92203":"markdown","aa784124":"markdown","68d87f98":"markdown","7ce5b2db":"markdown","c1e1d06d":"markdown","239da812":"markdown"},"source":{"f841358b":"from time import time\nimport psutil\n\nimport sys\n\nimport numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'font.size':13})\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression as LOGIT\nfrom sklearn.ensemble import RandomForestClassifier as RFC\nfrom sklearn.metrics import accuracy_score as acu\n\nimport gc\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e8d2a216":"class NBConfig:\n    general = {\n        \"seed\": 2021,\n        \"folds\": 4\n    }\n    method = {\n        \"jobs\": 4, \n        \"criterion\": \"gini\",\n        \"samples\": 0.65,\n        \"feat_frac\": 0.55,\n        \"depth\": 20,\n        \"n_trees\": 130,\n        \"leafSize\": 1,\n        \"costs\": 0.0\n    }","7010c0eb":"trainpath = \"..\/input\/ventilator-pressure-prediction\/train.csv\"\ntestpath = \"..\/input\/ventilator-pressure-prediction\/test.csv\"\nsamsubpath = \"..\/input\/ventilator-pressure-prediction\/sample_submission.csv\"\ntrain, test, samSub = pd.read_csv(trainpath, index_col=\"id\"), pd.read_csv(testpath, index_col=\"id\"), pd.read_csv(samsubpath)","34aa7efb":"train[\"Dummy\"] = 0\ntest[\"Dummy\"] = 1\n\nall_df = pd.concat([train.drop(columns=[\"pressure\"]), test])\ndel(train, test)","1cd189ee":"print(\"Data shape is: \" + str(all_df.shape))","797cf72c":"all_df.tail()","f15957c0":"all_df.describe()","6ebaef3d":"%%time\nall_df.reset_index(drop=True, inplace=True)\nall_df[\"timeDiff\"] = all_df[\"time_step\"].groupby(all_df[\"breath_id\"]).diff(1).fillna(-1)","28622f02":"%%time\nall_df[\"maxu_in\"] = all_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").transform(\"max\")[\"u_in\"]\nall_df[\"minu_in\"] = all_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").transform(\"min\")[\"u_in\"]\nall_df[\"meanu_in\"] = all_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").transform(\"mean\")[\"u_in\"]","74bb4389":"def ByBreath(method: str, DF, lags=None, center=False, fillNas=0):\n    \n    start = time()\n\n    output = pd.DataFrame()\n    if center == True:\n        c = \"c\"\n    else:\n        c = \"\"\n    \n    if method == \"mean\":\n        if lags is None:\n            sys.exit(\"specify lags\")\n        for l in lags:\n            agg = \\\n            DF[[\"breath_id\", \"u_in\", \"u_out\"]].groupby(\"breath_id\").rolling(window=l, center=center).mean().fillna(fillNas)\n            output[[\"{0}mu_in_l{1}\".format(c, l), \"{0}mu_out_l{1}\".format(c, l)]] = agg[[\"u_in\", \"u_out\"]]\n            gc.collect()\n            \n    elif method == \"max\":\n        if lags is None:\n            sys.exit(\"specify lags\")\n        for l in lags:\n            agg = \\\n            DF[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(window=l, center=center).max().fillna(fillNas)  \n            output[[\"{0}mxu_in_l{1}\".format(c, l)]] = agg[[\"u_in\"]]\n            gc.collect()\n            \n    elif method == \"min\":\n        if lags is None:\n            sys.exit(\"specify lags\")\n        for l in lags:\n            agg = \\\n            DF[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(window=l, center=center).min().fillna(fillNas)  \n            output[[\"{0}miu_in_l{1}\".format(c, l)]] = agg[[\"u_in\"]]\n            gc.collect()\n            \n    elif method == \"std\":\n        if lags is None:\n            sys.exit(\"specify lags\")\n        for l in lags:\n            agg = \\\n            DF[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(window=l, center=center).std().fillna(fillNas)  \n            output[\"{0}su_in_l{1}\".format(c, l)] = agg[\"u_in\"]\n            gc.collect()\n            \n    elif method == \"shift\":\n        if lags is None:\n            sys.exit(\"specify lags\")\n        for l in lags:\n            agg = \\\n            DF[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").shift(l).fillna(fillNas)  \n            output[\"sftu_in_l{0}\".format(l)] = agg[\"u_in\"]\n            gc.collect()     \n        \n    elif method == \"diff\":\n        if lags is None:\n            sys.exit(\"specify lags\")\n        for l in lags:\n            agg = \\\n            DF[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").diff(l).fillna(fillNas)  \n            output[\"du_in_l{0}\".format(l)] = agg[\"u_in\"]\n            gc.collect()  \n            \n    elif method == \"log\":\n        output[\"lgu_in\"] = np.log1p(DF[\"u_in\"].values)\n        gc.collect()  \n        \n    elif method == \"cumsum\":\n            agg = \\\n            DF[[\"breath_id\", \"u_in\", \"u_out\"]].groupby(\"breath_id\").cumsum() \n            output[[\"csu_in\", \"csu_out\"]] = agg[[\"u_in\", \"u_out\"]]\n            gc.collect()   \n            \n    elif method == \"area\":\n            agg = \\\n            DF[[\"time_step\", \"u_in\", \"breath_id\"]]\n            agg[\"area\"] = agg[\"time_step\"] * agg[\"u_in\"]\n            output[\"area\"] = agg.groupby(\"breath_id\")[\"area\"].cumsum()\n            gc.collect()   \n            \n    elif method == \"centering\":\n            agg = \\\n            DF[[\"u_in\", \"breath_id\"]].groupby(\"breath_id\").transform('mean')#does not aggregate like just mean()\n            output[\"cenu_in\"] = DF[\"u_in\"] - agg[\"u_in\"]\n            gc.collect()  \n    end = time()\n    print(c + method + \" created in \" + str(round(end - start)) + \" seconds.\" + \"RAM usage: \" + str(psutil.virtual_memory()[2]) + \"%\")\n    return output","8c913b02":"def assignment(DF, mDF):\n    DF = DF.copy()\n    colNames = mDF.columns\n    for n in colNames:\n        DF[\"{0}\".format(n)] = mDF[\"{0}\".format(n)].values\n    gc.collect()\n    return DF","5cdd1d90":"all_df = assignment(all_df, ByBreath(\"area\", all_df))\nall_df = assignment(all_df, ByBreath(\"mean\", all_df, lags=[6,9]))\nall_df = assignment(all_df, ByBreath(\"mean\", all_df, center=True, lags=[6]))\nall_df = assignment(all_df, ByBreath(\"max\", all_df, lags=[9]))\nall_df = assignment(all_df, ByBreath(\"min\", all_df, lags=[9]))\nall_df = assignment(all_df, ByBreath(\"diff\", all_df, lags=[1,2]))\nall_df = assignment(all_df, ByBreath(\"log\", all_df))\nall_df = assignment(all_df, ByBreath(\"std\", all_df, lags=[6]))\nall_df = assignment(all_df, ByBreath(\"shift\", all_df, lags=[-2,-1,1,2]))\nall_df = assignment(all_df, ByBreath(\"cumsum\", all_df))\nall_df = assignment(all_df, ByBreath(\"centering\", all_df))","220d224c":"all_df.nunique().to_frame()","1ae4a9a1":"y = all_df.Dummy\nnames = [c for c in all_df.columns if c not in [\"Dummy\", \"u_out\", \"mu_out\", \"cmu_out\", \"sftu_out_l-1\", \"sftu_out_l-2\"]]\nall_df = all_df[names]\ngc.collect()","22762657":"kf = KFold(\n    n_splits=NBConfig.general[\"folds\"], \n    random_state=NBConfig.general[\"seed\"], \n    shuffle=True\n)","beb888e8":"baseline = 1-y.sum()\/y.shape[0]\nprint(\"Baseline: \" + str(round(baseline, 5)))","cab54ac6":"importances = []\n\nfor k, (train_index, test_index) in enumerate(kf.split(all_df)):\n    \n    print(\"Fold: \" + str(k+1))\n    x_tr, x_te = all_df.loc[train_index], all_df.loc[test_index]\n    y_tr, y_te = y[train_index], y[test_index]  \n    baseline = 1-y_te.sum()\/y_te.shape[0]\n    print(\"Train data has \" + str(x_tr.shape[0]) + \" observations\" + \\\n          \"\\n\" + \"Test data has \" + str(x_te.shape[0]) + \" observations\")\n    print('RAM memory used after setting train and test:', psutil.virtual_memory()[2], \"%\")\n\n\n    reg = RFC(\n        random_state=NBConfig.general[\"seed\"], \n        n_jobs=NBConfig.method[\"jobs\"], \n        criterion=NBConfig.method[\"criterion\"],\n        max_samples=NBConfig.method[\"samples\"],\n        max_features=NBConfig.method[\"feat_frac\"],\n        max_depth=NBConfig.method[\"depth\"],\n        n_estimators=NBConfig.method[\"n_trees\"],\n        min_samples_leaf=NBConfig.method[\"leafSize\"],\n        ccp_alpha=NBConfig.method[\"costs\"],\n        verbose=0\n    )\n\n    reg.fit(x_tr.drop(columns=[\"breath_id\"]), y_tr); gc.collect()\n    gc.collect()\n    \n    score = acu(reg.predict(x_te.drop(columns=[\"breath_id\"])), y_te)\n    print(\"RF accuracy: \" + str(score) + \" is \" + str(round((score - baseline), 3)) + \" higher than the baseline!\" + \"\\n\")\n\n    gc.collect()\n    \n    imps = reg.feature_importances_\n    importances.append(imps)","166b3cd0":"impts = pd.DataFrame(importances, columns=names[1:])\nmeans = impts.mean(axis=0).to_frame(name=\"means\")\nstds = impts.std(axis=0)\nmeans[\"stds\"] = stds\nmeans.sort_values(by=\"means\", ascending=True, inplace=True)\n\nfig, ax = plt.subplots(figsize=(15,15))\nmeans.means.plot.barh(yerr=means.stds.values, ax=ax)\nax.set_title(\"Mean Feature Importances with Standard Deviation\")\nax.set_ylabel(\"Mean decrease in impurity\")\nfig.tight_layout()","63658a07":"# Importances ","76f92203":"# Random Forest Approach","aa784124":"# Feature Engineering","68d87f98":"**Define a function for flexible feature engineering**","7ce5b2db":"Number of unique values after feature engineering","c1e1d06d":"**In adversarial validation, the test and training dataset is first marked by a binary dummy. Then one tries to classify whether an observation is to be assigned to the training or test dataset. If the split between the training and the test dataset is not systematic - i.e. the split is only random - then an assignment should not be possible. In this case, an AUC of 0.5 or an accuracy equal to the proportion of the test dataset to all data would occur. If the metrics are higher, this is an indication of dissimilarity. The degree of dissimilarity can be concluded depending on the characteristics of the metrics. With an AUC of 1, the datasets would be systematically different. If the metrics are less pronounced, it can be assumed that at least partial systematics exist that make the datasets different from another. E.g. in one of the two datasets patterns exist, which do not occur in the other.**","239da812":"**To some degree, it is possible to predict whether an observation belongs to the test or training data set. This means that training data set and test data set differ structurally (but only partially) in certain areas.\nThis circumstance can strengthen the shakeup.**\n\n"}}