{"cell_type":{"6fe82bc9":"code","2ef115c3":"code","bf1add09":"code","c7d2bdce":"code","afb6ecee":"code","8f64afd7":"code","f9cdeec9":"code","4af1c24f":"code","ca9a72b7":"markdown","8b9a24ec":"markdown","bf4c7608":"markdown","3fc01eba":"markdown","186455d8":"markdown","66c89833":"markdown","19f86e8c":"markdown","2fd718e9":"markdown"},"source":{"6fe82bc9":"import path\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom google.cloud import videointelligence\n# or you can import versioned module:\n# from google.cloud import videointelligence_v1\n\n# Client instance\nvideo_client = videointelligence.VideoIntelligenceServiceClient()","2ef115c3":"# Process video\n# Create video context\nmode = videointelligence.LabelDetectionMode.SHOT_AND_FRAME_MODE\nconfig = videointelligence.LabelDetectionConfig(label_detection_mode=mode)\ncontext = videointelligence.VideoContext(label_detection_config=config)\n\noperation = video_client.annotate_video(\n    request={\n        \"features\": [videointelligence.Feature.LABEL_DETECTION],\n        \"input_uri\": \"gs:\/\/cloud-samples-data\/video\/chicago.mp4\",\n        \"video_context\": context\n        })\n\nprint(\"Processing video for label annotations:\")\n\n# Long running operation withing a 1 minute time frame.\n%time result = operation.result(timeout=60)\nprint(\"\\nFinished processing.\")","bf1add09":"# Display video\/segment level label annotations\nsegment_labels = result.annotation_results[0].segment_label_annotations\nfor i, segment_label in enumerate(segment_labels):\n    print(\"Video label description: {}\".format(segment_label.entity.description))\n    for category_entity in segment_label.category_entities:\n        print(\n            \"\\tLabel category description: {}\".format(category_entity.description)\n        )\n\n    for i, segment in enumerate(segment_label.segments):\n        start_time = (\n            segment.segment.start_time_offset.seconds\n            + segment.segment.start_time_offset.microseconds \/ 1e6\n        )\n        end_time = (\n            segment.segment.end_time_offset.seconds\n            + segment.segment.end_time_offset.microseconds \/ 1e6\n        )\n        positions = \"{}s to {}s\".format(start_time, end_time)\n        confidence = segment.confidence\n        print(\"\\tSegment {}: {}\".format(i, positions))\n        print(\"\\tConfidence: {}\".format(confidence))\n    print(\"\\n\")\n\n# Process shot level label annotations\nshot_labels = result.annotation_results[0].shot_label_annotations\nfor i, shot_label in enumerate(shot_labels):\n    print(\"Shot label description: {}\".format(shot_label.entity.description))\n    for category_entity in shot_label.category_entities:\n        print(\n            \"\\tLabel category description: {}\".format(category_entity.description)\n        )\n\n    for i, shot in enumerate(shot_label.segments):\n        start_time = (\n            shot.segment.start_time_offset.seconds\n            + shot.segment.start_time_offset.microseconds \/ 1e6\n        )\n        end_time = (\n            shot.segment.end_time_offset.seconds\n            + shot.segment.end_time_offset.microseconds \/ 1e6\n        )\n        positions = \"{}s to {}s\".format(start_time, end_time)\n        confidence = shot.confidence\n        print(\"\\tSegment {}: {}\".format(i, positions))\n        print(\"\\tConfidence: {}\".format(confidence))\n    print(\"\\n\")\n\n# Process frame level label annotations\nframe_labels = result.annotation_results[0].frame_label_annotations\nfor i, frame_label in enumerate(frame_labels):\n    print(\"Frame label description: {}\".format(frame_label.entity.description))\n    for category_entity in frame_label.category_entities:\n        print(\n            \"\\tLabel category description: {}\".format(category_entity.description)\n        )\n\n    # Each frame_label_annotation has many frames,\n    # here we print information only about the first frame.\n    frame = frame_label.frames[0]\n    time_offset = frame.time_offset.seconds + frame.time_offset.microseconds \/ 1e6\n    print(\"\\tFirst frame time offset: {}s\".format(time_offset))\n    print(\"\\tFirst frame confidence: {}\".format(frame.confidence))\n    print(\"\\n\")","c7d2bdce":"# Process video\n\n# Configure the request. We want to request all available annotations now. Please note that functions are in beta.\n# https:\/\/cloud.google.com\/video-intelligence\/docs\/reference\/rest\/v1p3beta1\/videos\/annotate#PersonDetectionConfig\nconfig = videointelligence.PersonDetectionConfig(\n    include_bounding_boxes=True,\n    include_attributes=True,\n    include_pose_landmarks=True,\n)\n\n# Start the asynchronous request\noperation = video_client.annotate_video(\n    request={\n        \"features\": [videointelligence.Feature.PERSON_DETECTION],\n        \"input_uri\": \"gs:\/\/cloud-samples-data\/video\/chicago.mp4\",\n        \"video_context\": videointelligence.VideoContext(person_detection_config=config)\n    }\n)\n\nprint(\"Processing video for person detection annotations.\")\n%time result = operation.result(timeout=300)\n\nprint(\"\\nFinished processing.\")","afb6ecee":"# Display results\n# Retrieve the first result, because a single video was processed.\nannotation_result = result.annotation_results[0]\n\nfor annotation in annotation_result.person_detection_annotations:\n    print(\"Person detected:\")\n    for track in annotation.tracks:\n        print(\n            \"Segment: {}s to {}s\".format(\n                track.segment.start_time_offset.seconds\n                + track.segment.start_time_offset.microseconds \/ 1e6,\n                track.segment.end_time_offset.seconds\n                + track.segment.end_time_offset.microseconds \/ 1e6,\n            )\n        )\n\n        # Each segment includes timestamped objects that include\n        # characteristics - -e.g.clothes, posture of the person detected.\n        # Grab the first timestamped object\n        timestamped_object = track.timestamped_objects[0]\n        box = timestamped_object.normalized_bounding_box\n        print(\"Bounding box:\")\n        print(\"\\t  left: {}\".format(box.left))\n        print(\"\\t   top: {}\".format(box.top))\n        print(\"\\t right: {}\".format(box.right))\n        print(\"\\tbottom: {}\".format(box.bottom))\n\n        # Attributes include unique pieces of clothing,\n        # poses, or hair color.\n        print(\"Attributes:\")\n        for attribute in timestamped_object.attributes:\n            print(\n                \"\\t{}:{} {}\".format(\n                    attribute.name, attribute.value, attribute.confidence\n                )\n            )\n\n        # 'Landmarks' in person detection include body parts such as\n        # left_shoulder, right_ear, and right_ankle\n        print(\"Body parts:\")\n        for landmark in timestamped_object.landmarks:\n            print(\n                \"\\t{}: {} (x={}, y={})\".format(\n                    landmark.name,\n                    landmark.confidence,\n                    landmark.point.x,  # Normalized vertex\n                    landmark.point.y,  # Normalized vertex\n                )\n            )\n","8f64afd7":"# Process video\nfeatures = [videointelligence.Feature.LOGO_RECOGNITION]\noperation = video_client.annotate_video(\n    request={\"features\": features, \"input_uri\": \"gs:\/\/cloud-samples-data\/video\/gbikes_dinosaur.mp4\"})\n\nprint(u\"Waiting for operation to complete... it's about 80 seconds\")\n%time response = operation.result(timeout=180)\nprint(\"\\nFinished processing.\")","f9cdeec9":"# Get the first response, since we sent only one video.\nannotation_result = response.annotation_results[0]\n\nlogos = pd.DataFrame([], columns=['description', 'start', 'end'])\nfor logo_recognition_annotation in annotation_result.logo_recognition_annotations:\n    # All video segments where the recognized logo appears. There might be\n    # multiple instances of the same logo class appearing in one VideoSegment.\n    serie = map(lambda segment: dict(\n                description=logo_recognition_annotation.entity.description,\n                start=segment.start_time_offset.total_seconds(),\n                end=segment.end_time_offset.total_seconds()\n                ),\n            logo_recognition_annotation.segments)\n    logos=logos.append(pd.DataFrame.from_dict(serie))\n\n# Sort logos by first appearance\nlogos=logos.sort_values(by=['start'], ascending=False)\n\nbegin = logos['start']\nend = logos['end']\n\nplt.figure(figsize=(18,4))\nplt.barh(logos['description'], (end-begin), .4, left=begin)\nplt.tick_params(axis='both', which='major', labelsize=15)\nplt.tick_params(axis='both', which='minor', labelsize=20)\nplt.title('Video timeline', fontsize = '18')\nplt.xlabel('Seconds', fontsize = '14')\nplt.xlim(0, end.max())\nplt.show()\ndisplay(logos)","4af1c24f":"# Annotations for list of logos detected, tracked and recognized in video.\nfor logo_recognition_annotation in annotation_result.logo_recognition_annotations:\n    entity = logo_recognition_annotation.entity\n\n    # Opaque entity ID. Some IDs may be available in [Google Knowledge Graph\n    # Search API](https:\/\/developers.google.com\/knowledge-graph\/).\n    print(u\"Entity Id : {}\".format(entity.entity_id))\n\n    print(u\"Description : {}\".format(entity.description))\n\n    # All logo tracks where the recognized logo appears. Each track corresponds\n    # to one logo instance appearing in consecutive frames.\n    for track in logo_recognition_annotation.tracks:\n\n        # Video segment of a track.\n        print(u\"\\n\\tStart Time Offset : {}\".format(track.segment.start_time_offset))\n        print(u\"\\t  End Time Offset : {}\".format(track.segment.end_time_offset))\n        print(u\"\\t       Confidence : {}\".format(track.confidence))\n\n        # The object with timestamp and attributes per frame in the track.\n        for timestamped_object in track.timestamped_objects:\n            # Normalized Bounding box in a frame, where the object is located.\n            normalized_bounding_box = timestamped_object.normalized_bounding_box\n            print(u\"\\n\\t\\t  Left: {}\".format(normalized_bounding_box.left))\n            print(u\"\\t\\t   Top: {}\".format(normalized_bounding_box.top))\n            print(u\"\\t\\t Right: {}\".format(normalized_bounding_box.right))\n            print(u\"\\t\\tBottom: {}\".format(normalized_bounding_box.bottom))\n\n            # Optional. The attributes of the object in the bounding box.\n            for attribute in timestamped_object.attributes:\n                print(u\"\\n\\t\\t\\t    Name: {}\".format(attribute.name))\n                print(u\"\\t\\t\\tConfidence: {}\".format(attribute.confidence))\n                print(u\"\\t\\t\\t     Value: {}\".format(attribute.value))\n\n        # Optional. Attributes in the track level.\n        for track_attribute in track.attributes:\n            print(u\"\\n\\t\\t    Name: {}\".format(track_attribute.name))\n            print(u\"\\t\\tConfidence: {}\".format(track_attribute.confidence))\n            print(u\"\\t\\t     Value: {}\".format(track_attribute.value))","ca9a72b7":"# Google Gloud Platform pretrained ML services Notebook series\n\nWelcome to this introduction level series about how to use Google Cloud pretrained ML services Python clients from Kaggle notebooks, without raw HTTP API calls. Four notebooks are prepared as reference guides:\n\n* [Cloud Translation quick start notebook](https:\/\/www.kaggle.com\/kornelregius\/google-cloud-translation-tutorial)\n* [Cloud Natural Language quick start notebook](https:\/\/www.kaggle.com\/kornelregius\/google-cloud-natural-language-tutorial)\n* [Cloud Video Intelligence quick start notebook](https:\/\/www.kaggle.com\/kornelregius\/google-cloud-video-intelligence-tutorial)\n* [Cloud Vision quick start notebook](https:\/\/www.kaggle.com\/kornelregius\/google-cloud-vision-tutorial)\n\n\n\n## Before start, please read these important notes\n\nFirstly, please note that <mark>Cloud Video Intelligence is a paid service<\/mark> and requires a GCP project with billing enabled to use. Please refer to [pricing page](https:\/\/cloud.google.com\/video-intelligence#pricing). Also keep in mind, Video Intelligence API apply a [quota limit](https:\/\/cloud.google.com\/video-intelligence\/quotas) for video\/request, video size, etc.\n\nWhen this notebook created, the first 1000 minutes were free of streamed video processing each month!\nLuckily, you're eligible for a $300 credit when creating the first GCP project.\n\nBefore you can start using any of these services, you must have a Google Cloud project that has the service API enabled.\n\n1. Select or create a GCP Project\n2. Enable Billing. Remember to understand pricing prior to this step.\n3. [Enable Video Intelligence API](https:\/\/console.cloud.google.com\/apis\/library\/videointelligence.googleapis.com)\n\nIn order to use Google Cloud Services from Kagge notebooks, you need to attach your Google Cloud Platform account to this notebbok. Select 'Add-ons' menu above then 'Google Cloud Services'.\n- If you have already attached Google Cloud account to your Kaggle account, please just attach it to this notebook as well.\n- Otherwise you need to attach your account to your profile (and this notebook) by selecting 'Google Cloud AI Platform' integration and adding an authorized account.\n\nMake sure you've **linked both 'Google Cloud AI Platform' and 'Cloud Storage'** for this tutorial.\n*Unsuccefull attachment, will cause \"Could not automatically determine credentials.\" error response when you want to instantiate a Python client.* \n\n## Google Cloud Platfrom [Video Intelligence](https:\/\/cloud.google.com\/video-intelligence) (or Video AI)\nQuick start guide: https:\/\/cloud.google.com\/video-intelligence\/docs\/quickstart. Probably you find interesting a little more complex [code examples on GitHub](https:\/\/github.com\/googleapis\/python-videointelligence\/tree\/master\/samples).\n\nIn the following examples we are processing public demo videos from [this GCS bucket](https:\/\/console.cloud.google.com\/storage\/browser\/cloud-samples-data\/video). Original code samples are processing and displaying the result in a single step, but we follwing the practice to split in to cells the long running and expensive video processing and results evaluation.","8b9a24ec":"## Detect a person in a video (beta)\n\nDetect a person and body parts in a video of the traffic in a street of Chicago.","bf4c7608":"### List detailed results of logo recognition\nVideo Intelligence detects logos in video and tracks them providing bounding boxes coordinates around the logos. ","3fc01eba":"## Finishing part \n\nVideo Intelligence has more exiting features. Please check [Video Intelligence API Codelabs](https:\/\/codelabs.developers.google.com\/codelabs\/cloud-video-intelligence-python3), where you'll find examples for:\n* Detecting shots of the video, series of frames with visual continuity.\n* Performing explicit adult content detection\n* More comprehensive object tracking\n* and how to transcribe speech in a video\n","186455d8":"### Show logos in video timeline\nDisplay a video timeline to show which brands are appear in which time range.","66c89833":"## Recognize logos, finding brands.\nDetect, track, and recognize the presence of logos in video content recorded around the Google campus.\n>Running this cell takes around 80 seconds.","19f86e8c":"### Create API client\nClient library reference: https:\/\/googleapis.dev\/python\/videointelligence\/latest\/index.html","2fd718e9":"## Annotate a video file in Cloud Storage\nDetects labels of a [GCS stored video file](https:\/\/storage.cloud.google.com\/cloud-samples-data\/video\/chicago.mp4) of the traffic in a street of Chicago.\nDuring the label detection and annotation process, the Video Intelligence service try to detect well know object such as bus, dog or flower, etc.\n\nActual annotation feature list of v1 API: https:\/\/cloud.google.com\/video-intelligence\/docs\/reference\/rest\/v1\/videos\/annotate#Feature\n\n> Running this cell takes aroud 20 seconds."}}