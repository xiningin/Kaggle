{"cell_type":{"e211d53d":"code","f1096798":"code","b801e72c":"code","74434e51":"code","58f98e8b":"code","9a46aff3":"code","1f129efe":"code","7b0a5a76":"code","f0228bf4":"code","415507b2":"code","6848ef50":"code","365e457d":"code","a53ed894":"code","e66da63e":"code","115a5388":"code","545976fb":"code","7f7a40c4":"code","6b93f821":"code","5f419bec":"code","8dc76e65":"code","9fac2e4c":"code","da215025":"code","250e8c7f":"code","aa8cfcf7":"code","0b3ea91f":"code","cdf776e6":"code","c7788623":"code","405e93b8":"code","cca92090":"code","5d39b045":"code","9fbeff02":"code","3fd736dc":"code","672f1f57":"code","f20e8b2e":"code","1375bc61":"code","a726ba82":"code","e07cdbc3":"code","9723845c":"code","46f1147e":"code","5a496188":"code","49fe6ab7":"code","5508d3b9":"code","a2ba3175":"code","a3fa5cb4":"code","2761c78b":"code","5d57a125":"code","bf5fb748":"code","901b5184":"code","8f1d8ab2":"code","6c5695b1":"code","32ca2469":"code","f2f5a17b":"code","a6c913c2":"code","6fcf40bf":"code","8c25ebe2":"code","4b8eb4a7":"code","bebbb8b0":"code","937954b8":"code","8b9b16f2":"code","9c41dd33":"code","8e75dd5c":"code","f43fc5ba":"code","5a3cc2f7":"markdown","fdf769ac":"markdown"},"source":{"e211d53d":"# from google.colab import drive\n# drive.mount('\/content\/gdrive')\n# data_dir = 'gdrive\/MyDrive\/kaggle\/ventilator-pressure-prediction\/data'\n# SAVE_MODEL_PATH = 'gdrive\/MyDrive\/kaggle\/ventilator-pressure-prediction\/code\/models\/model_class28'\n\n\ndata_dir = '..\/input\/ventilator-pressure-prediction'\nSAVE_MODEL_PATH = '\/kaggle\/working\/'","f1096798":"# !pip install pandas --upgrade\n!pip install tez\n#!pip install Time2Vec-PyTorch","b801e72c":"# !pip install transformers > \/dev\/null\n# !pip install git+https:\/\/github.com\/rashmibanthia\/tez.git@b81953fa2e5ae2f4ea8d560de35c3a857c5e59e6 > \/dev\/null","74434e51":"import os\nimport torch\nimport tez\n\nimport random\nimport gc \n\nimport pandas as pd\nimport torch.nn as nn\nimport numpy as np\n\nfrom sklearn import metrics\nfrom tez.callbacks import EarlyStopping\nfrom tqdm import tqdm\n\nimport torch.nn.functional as F\nfrom transformers import get_linear_schedule_with_warmup\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.notebook import tqdm as tqdm\nfrom torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingLR,ReduceLROnPlateau,CosineAnnealingWarmRestarts\nfrom sklearn.preprocessing import RobustScaler","58f98e8b":"def seed_everything(seed: int) -> None:\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","9a46aff3":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, X, y, w):\n        if y is None:\n            y = np.zeros(len(X), dtype=np.float32)\n\n        self.X = X.astype(np.float32)\n        self.y = y.astype(np.float32)\n        self.w = w.astype(np.float32)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, i):\n        return self.X[i], self.y[i], self.w[i]","1f129efe":"# change fold here\nclass args:\n    fold = 4\n    learning_rate = 2e-3  \n    batch_size = 1024\n    epochs = 300\n    accumulation_steps = 1\n    output_folder = SAVE_MODEL_PATH\n    fp16 = False\n\n    device = 'cuda'\n    input_dim = 13 #8 #5\n    dense_dim = 512\n    lstm_dim = 512\n    logit_dim = 512\n\n    EMBED_SIZE = 64\n    HIDDEN_SIZE = 256\n    \n#     ALL_FEATURES = ['u_in', 'u_out', 'time_step', 'u_in_cumsum', 'u_in_cummean', 'area', 'cross', 'cross2', 'R_cate', 'C_cate', 'RC_sum', 'RC_dot',\n#  'u_in_lag_1','u_in_lag_2','u_in_lag_3','u_in_lag_4', 'u_in_time1', 'u_in_time2' , 'u_in_time3', 'u_in_time4',\n# 'u_out_lag_1','u_out_lag_2','u_out_lag_3','u_out_lag_4']\n\n    ALL_FEATURES = ['time_step', 'u_in', 'u_out', \n       'area', 'u_in_cumsum', 'u_in_lag1', 'u_out_lag1', 'u_in_lag_back1',\n       'u_out_lag_back1', 'u_in_lag2', 'u_out_lag2', 'u_in_lag_back2',\n       'u_out_lag_back2', 'u_in_lag3', 'u_out_lag3', 'u_in_lag_back3',\n       'u_out_lag_back3', 'u_in_lag4', 'u_out_lag4', 'u_in_lag_back4',\n       'u_out_lag_back4', 'breath_id__u_in__max', 'breath_id__u_out__max',\n       'u_in_diff1', 'u_out_diff1', 'u_in_diff2', 'u_out_diff2',\n       'breath_id__u_in__diffmax', 'breath_id__u_in__diffmean', 'u_in_diff3',\n       'u_out_diff3', 'u_in_diff4', 'u_out_diff4', 'cross', 'cross2', 'R_20',\n       'R_5', 'R_50', 'C_10', 'C_20', 'C_50', 'R__C_20__10', 'R__C_20__20',\n       'R__C_20__50', 'R__C_50__10', 'R__C_50__20', 'R__C_50__50',\n       'R__C_5__10', 'R__C_5__20', 'R__C_5__50', \n       'u_in_1st_derivative', 'expand_mean_1sr_der', 'u_in_1st_der_mean10',\n       'time_diff', 'power', 'power_cumsum', 'ewm_u_in_mean', 'ewm_u_in_std',\n       'ewm_u_in_corr', 'delta_u_in', 'delta_u_in_exp',\n       'delta_rolling_10_mean', 'delta_rolling_10_max', 'work', 'work_roll_10',\n       'work_roll_15', 'u_in_rol_q0.1', 'u_in_rol_q0.25', 'u_in_rol_q0.5',\n       'u_in_rol_q0.75', 'u_in_rol_q0.9']\n\n    \nprint(len(args.ALL_FEATURES))","7b0a5a76":"# !cp ..\/input\/ventilator-train-classification-ce-pytorch-optim\/*.csv \/kaggle\/working\/\n# !cp ..\/input\/ventilator-train-classification-ce-pytorch-optim\/model_f0_r1.bin \/kaggle\/working\/model_f0_split6.bin","f0228bf4":"import pandas as pd\nfrom sklearn import model_selection\n#df=pd.read_csv('..\/input\/ventilator-train-classification-ce-pytorch-optim\/train_folds.csv')\n''' \ndf=pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv')\ndf[\"kfold\"] = -1\ny = df.pressure.values\n\nkf = model_selection.GroupKFold(n_splits=7)\n\nfor f, (t_, v_) in enumerate(kf.split(X=df, y=y, groups=df.breath_id.values)):\n    df.loc[v_, \"kfold\"] = f\n\ndf.to_csv(\"train_folds.csv\", index=False)\n''' \n\ndf=pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv')\ndf[\"kfold\"] = -1\ny = df.pressure.values\nkf = model_selection.GroupKFold(n_splits=7)\nfor f, (t_, v_) in enumerate(kf.split(X=df, y=y, groups=df.breath_id.values)):\n    df.loc[v_, \"kfold\"] = f\ndf.to_csv(\"train_folds7.csv\", index=False)","415507b2":" #df_uout_0=df[df.u_out==0].reset_index(drop=True)","6848ef50":"#df['cycle_last_ts']=df.groupby(['breath_id','u_out'])['time_step'].transform('max') \n#df['cycle_last_ts_diff']=df['cycle_last_ts']-df['time_step']","365e457d":"#df[(df.breath_id==1) & (df.u_out==0)]\ndf.iloc[:,2:5] ","a53ed894":"seed_everything(42)\n\n# ====================================================\n# Data Loading\n# ====================================================\ntrain_df = df \n#test_df = pd.read_csv(f'{data_dir}\/test.csv')\nsub = pd.read_csv(f'{data_dir}\/sample_submission.csv')\n\nfor c in ['u_in']:\n    train_df[c] = np.log1p(train_df[c])\n    #test_df[c] = np.log1p(test_df[c])\n\n","e66da63e":"oof = np.zeros(len(train_df)).astype('float32')\ntest_preds_lst = []\n\ntarget_dic = {v:i for i, v in enumerate(sorted(train_df['pressure'].astype('float32').unique().tolist()))}\ntarget_dic_inv = {v: k for k, v in target_dic.items()}\ngc.collect()","115a5388":"#train_df=df\n#target_dict_inv_array=np.zeros(len(target_dic_inv))\ntarget_dict_inv_array=np.array(list(target_dic_inv.values())).astype('float32')\ntarget_dict_inv_array.shape\ntarget_dict_inv_array=torch.tensor(target_dict_inv_array) \n\ndel target_dic_inv","545976fb":"#target_dict_inv_array[27] ,target_dic ","7f7a40c4":"train_df['tc']=train_df['R']*(train_df['C']\/1000)\n#test_df['tc']=test_df['R']*(test_df['C']\/1000)\n\ntrain_df['id']=train_df.id.astype('int32')\ntrain_df['R']=train_df.R.astype('int32')\ntrain_df['C']=train_df.C.astype('int32')\ntrain_df['u_out']=train_df.u_out.astype('int32')\ntrain_df['time_step']=train_df.time_step.astype('float32')\ntrain_df['u_in']=train_df.u_in.astype('float32')\ntrain_df['pressure']=train_df.pressure.astype('float32')\ntrain_df['kfold']=train_df.kfold.astype('int8')","6b93f821":"#import cudf","5f419bec":"#train_df=cudf.DataFrame().from_pandas(train_df)","8dc76e65":"gc.collect()\n\n#train_df.groupby(train_df['breath_id'])['u_in'].rolling(window=10, min_periods=1)#.fillna(0)#.quantile(0.9)#.reset_index(level=0,drop=True)     \n#train_df.groupby('breath_id')['u_in'].ewm(halflife=10).mean().reset_index(level=0,drop=True)    \n\n#train_df.groupby('breath_id')['u_in'].expanding(2).mean().reset_index(level=0,drop=True)\n#train_df.groupby('breath_id')['u_in'].rolling(window=10, min_periods=1).mean().reset_index( drop=True) \nembedding_df=df.iloc[:,2:5]   ","9fac2e4c":"def add_feature(df):\n    df['time_delta'] = df.groupby('breath_id')['time_step'].diff().fillna(0)\n    df['delta'] = df['time_delta'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['delta'].cumsum()\n    \n    #df = df.drop(['time_delta','delta'], axis=1)\n    df = df.drop(['delta'], axis=1)\n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    \n    df[\"u_out_sum\"] = df.groupby(\"breath_id\")[\"u_out\"].transform(\"sum\").astype('int32')\n    df['cycle_last_ts']=df.groupby(['breath_id','u_out'])['time_step'].transform('max') \n    df['cycle_last_ts_diff']=df['cycle_last_ts']-df['time_step']\n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n    df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1).fillna(0).astype('float32')\n    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n    df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1).fillna(0).astype('float32')\n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2).fillna(0).astype('float32')\n    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n    df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2).fillna(0).astype('float32')\n    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n    #df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3).fillna(0).astype('float32')\n    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n    #df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3).fillna(0).astype('float32')\n    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n    #df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4).fillna(0).astype('float32')\n    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n    #df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4).fillna(0).astype('float32')\n    \n    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n    #df['breath_id__u_out__max'] = df.groupby(['breath_id'])['u_out'].transform('max')\n    \n    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n    df['u_out_diff1'] = (df['u_out'] - df['u_out_lag1']).fillna(0).astype('float32')\n    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n    df['u_out_diff2'] = (df['u_out'] - df['u_out_lag2']).fillna(0).astype('float32')\n    \n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    #df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    #df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n    #df['u_out_diff3'] = (df['u_out'] - df['u_out_lag3']).fillna(0).astype('float32')\n    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n    #df['u_out_diff4'] = (df['u_out'] - df['u_out_lag4']).fillna(0).astype('float32')\n    #df['cross']= df['u_in']*df['u_out']\n    #df['cross2']= df['time_step']*df['u_out']\n\n\n    #https:\/\/www.kaggle.com\/alexxanderlarko\/lgbm-sel-feat-1\n    df['u_in_1st_derivative'] = (df['u_in'].diff().fillna(0) \/ df['time_step'].diff().fillna(0)).fillna(0)\n    df['expand_mean_1sr_der'] = df.groupby('breath_id')['u_in_1st_derivative'].expanding(2).mean().reset_index(level=0,drop=True)\n    #df['u_in_1st_der_mean10'] = df.groupby('breath_id')['u_in_1st_derivative'].rolling(window=10, min_periods=1).mean().reset_index(level=0,drop=True)\n    #df['time_diff'] = df['time_step']-df['time_step'].shift(1)\n    #df['power'] = df['time_diff']*df['u_in']\n    df['power']=df['time_delta']*df['u_in']\n    df['power_cumsum'] = df.groupby(['breath_id'])['power'].cumsum()\n    df['ewm_u_in_mean'] = df.groupby('breath_id')['u_in'].ewm(halflife=10).mean().reset_index(level=0,drop=True)\n    df['ewm_u_in_std'] = df.groupby('breath_id')['u_in'].ewm(halflife=10).std().reset_index(level=0,drop=True)\n    df['ewm_u_in_corr'] = df.groupby('breath_id')['u_in'].ewm(halflife=10).corr().reset_index(level=0,drop=True)\n    df['delta_u_in'] = abs(df.groupby(df['breath_id'])['u_in'].diff().fillna(0)).reset_index(level=0,drop=True)\n    #cudf\n     \n    df['u_in_1st_der_mean10'] = df.groupby('breath_id')['u_in_1st_derivative'].rolling(window=10, min_periods=1).mean().reset_index(level=0,drop=True)\n    \n    #df['delta_u_in_exp'] = df.groupby(df['breath_id'])['delta_u_in'].rolling(window=10, min_periods=1).mean().reset_index(level=0,drop=True)\n    df['delta_rolling_10_mean'] = df.groupby('breath_id')['delta_u_in'].rolling(window=10, min_periods=1).mean().reset_index(level=0,drop=True)\n    df['delta_rolling_10_max'] = df.groupby('breath_id')['delta_u_in'].rolling(window=10, min_periods=1).max().reset_index(level=0,drop=True)\n    #cudf end\n    df['work']=((df['u_in'] + df['u_in'].shift(1).fillna(0))\/2 * df['time_step'].diff().fillna(0)).clip(0,)\n    df['work_roll_10']=df.groupby(df['breath_id'])['work'].rolling(window=10, min_periods=1).sum().reset_index(level=0,drop=True)\n    df['work_roll_15']=df.groupby(df['breath_id'])['work'].rolling(window=15, min_periods=1).sum().reset_index(level=0,drop=True)\n    \n    \n    df['u_in_rol_q0.1'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.1).reset_index(level=0,drop=True)\n    df['u_in_rol_q0.25'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.25).reset_index(level=0,drop=True)\n    df['u_in_rol_q0.5'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.5).reset_index(level=0,drop=True)\n    df['u_in_rol_q0.75'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.75).reset_index(level=0,drop=True)\n    df['u_in_rol_q0.9'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.9).reset_index(level=0,drop=True)     \n    \n    #df['u_in_diff1_back'] = df['u_in'] - df['u_in_lag_back1']\n    #df['u_in_diff2_back'] = df['u_in'] - df['u_in_lag_back2']\n    #df['u_out_lagback_diff1'] = df['u_out'] - df['u_out_lag_back1']\n    #df['u_out_lagback_diff2'] = df['u_out'] - df['u_out_lag_back2']\n\n    df = df.fillna(0)\n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    df = pd.get_dummies(df)\n\n    return df\n\n\ntrain_df = add_feature(train_df)\n#test_df = add_feature(test_df)\n\nids = train_df[['id']].values.reshape(-1, 80)\n#print(train_df.shape)\n\n#print(len(X_all))\n#ids.shape\n","da215025":"embedding_df.head()","250e8c7f":"# def add_feature(df):\n#     df['time_delta'] = df.groupby('breath_id')['time_step'].diff().fillna(0)\n#     df['delta'] = df['time_delta'] * df['u_in']\n#     df['area'] = df.groupby('breath_id')['delta'].cumsum()\n\n#     df = df.drop(['time_delta','delta'], axis=1)\n\n#     df['cross']= df['u_in']*df['u_out']\n#     df['cross2']= df['time_step']*df['u_out']\n    \n#     df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n#     df['one'] = 1\n#     df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n#     df['u_in_cummean'] =df['u_in_cumsum'] \/ df['count']\n    \n#     df = df.drop(['count','one'], axis=1)\n\n#     df['breath_id_lag1']=df['breath_id'].shift(1).fillna(0)\n#     df['breath_id_lag1same']=np.select([df['breath_id_lag1']==df['breath_id']], [1], 0)\n#     df['u_in_lag_1'] = df['u_in'].shift(1).fillna(0) * df[f'breath_id_lag1same']\n#     df['u_in_time1'] = df['u_in'] - df['u_in_lag_1']\n#     df['u_out_lag_1'] = df['u_out'].shift(1).fillna(0) * df['breath_id_lag1same']\n\n#     df['breath_id_lag2']=df['breath_id'].shift(1).fillna(0)\n#     df['breath_id_lag2same']=np.select([df['breath_id_lag2']==df['breath_id']], [1], 0)\n#     df['u_in_lag_2'] = df['u_in'].shift(1).fillna(0) * df[f'breath_id_lag1same']\n#     df['u_in_time2'] = df['u_in'] - df['u_in_lag_2']\n#     df['u_out_lag_2'] = df['u_out'].shift(1).fillna(0) * df['breath_id_lag2same']\n\n#     df['breath_id_lag3']=df['breath_id'].shift(1).fillna(0)\n#     df['breath_id_lag3same']=np.select([df['breath_id_lag3']==df['breath_id']], [1], 0)\n#     df['u_in_lag_3'] = df['u_in'].shift(1).fillna(0) * df[f'breath_id_lag3same']\n#     df['u_in_time3'] = df['u_in'] - df['u_in_lag_3']\n#     df['u_out_lag_3'] = df['u_out'].shift(1).fillna(0) * df['breath_id_lag3same']\n\n#     df['breath_id_lag4']=df['breath_id'].shift(1).fillna(0)\n#     df['breath_id_lag4same']=np.select([df['breath_id_lag4']==df['breath_id']], [1], 0)\n#     df['u_in_lag_4'] = df['u_in'].shift(1).fillna(0) * df[f'breath_id_lag4same']\n#     df['u_in_time4'] = df['u_in'] - df['u_in_lag_4']\n#     df['u_out_lag_4'] = df['u_out'].shift(1).fillna(0) * df['breath_id_lag4same']\n\n#     drop_columns = ['breath_id_lag1', 'breath_id_lag2', 'breath_id_lag3', 'breath_id_lag4', \n#                     'breath_id_lag1same', 'breath_id_lag2same', 'breath_id_lag3same', 'breath_id_lag4same']\n\n#     df = df.drop(drop_columns, axis=1)\n\n#     c_dic = {10: 0, 20: 1, 50:2}\n#     r_dic = {5: 0, 20: 1, 50:2}\n#     rc_sum_dic = {v: i for i, v in enumerate([15, 25, 30, 40, 55, 60, 70, 100])}\n#     rc_dot_dic = {v: i for i, v in enumerate([50, 100, 200, 250, 400, 500, 2500, 1000])}    \n\n#     df['C_cate'] = df['C'].map(c_dic)\n#     df['R_cate'] = df['R'].map(r_dic)\n#     df['RC_sum'] = (df['R'] + df['C']).map(rc_sum_dic)\n#     df['RC_dot'] = (df['R'] * df['C']).map(rc_dot_dic)\n#     df = df.drop(['R','C'], axis=1)\n\n#     # fill na by zero\n#     df = df.fillna(0)\n\n#     return df\n\n\n# train_df = add_feature(train_df)\n# test_df = add_feature(test_df)\n\n\n","aa8cfcf7":"#targets = train_df[['pressure']].to_numpy().reshape(-1, 80)\n#u_outs = train_df[['u_out']].to_numpy().reshape(-1, 80)\n#u_outs = train_df['u_out'] \nkfolds = train_df[['kfold']] #.to_numpy().reshape(-1, 80)\ntrain_breathid = train_df.breath_id.values \n#test_breathid = test_df.breath_id.values\ntrain_ids = train_df.id.values\n#test_ids = test_df.id.values \n\ntrain_pressure = train_df.pressure.values\n# test_pressure = test.pressure.values \n\ntrain_df.drop(['pressure', 'id', 'breath_id', 'kfold'], axis=1, inplace=True)\n#test_df = test_df.drop(['id', 'breath_id'], axis=1)\n#train_df.shape, test_df.shape","0b3ea91f":"!git clone https:\/\/github.com\/ojus1\/Time2Vec-PyTorch\/ ","cdf776e6":"import sys\n\nsys.path.append('\/kaggle\/working\/Time2Vec-PyTorch\/')\n\n","c7788623":"from Model import *\nModel(\"sin\", 8)","405e93b8":"traincols = train_df.columns\n\n#train_df['u_out_orig']=df['u_out'].values\n\nRS = RobustScaler()\ntrain_df = RS.fit_transform(train_df[traincols])\ntrain_df=train_df.astype('float32')\n\n#train_df.shape, test_df.shape,len(traincols),len(testcols)\n","cca92090":"#train_df=train_df.astype('float32')\nimport gc\ngc.collect()\ngc.collect()","5d39b045":"train_df.dtype\n\n","9fbeff02":"from periodic_activations import SineActivation , CosineActivation,t2v\nfrom Data import ToyDataset\nfrom torch import nn\nimport torch","3fd736dc":"\n\nclass SineActivation(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(SineActivation, self).__init__()\n        self.out_features = out_features\n        self.w0 = nn.parameter.Parameter(torch.randn(in_features, 1))\n        self.b0 = nn.parameter.Parameter(torch.randn(in_features, 1))\n        self.w = nn.parameter.Parameter(torch.randn(in_features, out_features-1))\n        self.b = nn.parameter.Parameter(torch.randn(in_features, out_features-1))\n        self.f = torch.sin\n\n    def forward(self, tau):\n        return t2v(tau, self.f, self.out_features, self.w, self.b, self.w0, self.b0)\n\n\ndef t2v1(tau, f, out_features, w, b, w0, b0, arg=None):\n    if arg:\n        v1 = f(torch.matmul(tau, w) + b, arg)\n    else:\n        #print(w.shape, tau.shape, b.shape)\n        v1 = f(torch.matmul(tau, w) + b)\n    v2 = torch.matmul(tau, w0) + b0\n    #print(v1.shape,v2.shape)\n    return torch.cat([v1, v2], -1)\nt2v=t2v1\n\nclass Model(nn.Module):\n    def __init__(self, activation, hiddem_dim):\n        super(Model, self).__init__()\n        if activation == \"sin\":\n            self.l1 = SineActivation(1, hiddem_dim)\n        elif activation == \"cos\":\n            self.l1 = CosineActivation(1, hiddem_dim)\n        \n        self.fc1 = nn.Linear(hiddem_dim, 2)\n    \n    def forward(self, x):\n        #x = x.unsqueeze(1)\n        x = self.l1(x)\n        x = self.fc1(x)\n        return x\nclass VPPModel(tez.Model):\n    \n    \n    def __init__(self, learning_rate):\n        hidden = [768, 512, 256, 128]\n        super().__init__()\n        self.learning_rate = learning_rate\n        # self.loss_fct = custom_ce_loss()\n        self.step_scheduler_after = \"batch\" #\"epoch\"\n        self.time_step=Model(\"sin\", 8)\n        self.u_in=Model(\"sin\", 8)\n        self.r=nn.Embedding(3,2)\n        self.c=nn.Embedding(3,2)\n\n        #self.lstm = nn.LSTM(len(args.ALL_FEATURES), args.HIDDEN_SIZE, batch_first=True, bidirectional=True, dropout=0.0, num_layers=4)\n        # self.head = nn.Linear(args.HIDDEN_SIZE * 2,950)\n        '''\n        self.head = nn.Sequential(\n            nn.Linear(args.HIDDEN_SIZE * 2, args.HIDDEN_SIZE * 2),\n            nn.LayerNorm(args.HIDDEN_SIZE * 2),\n            nn.ReLU(),\n            nn.Linear(args.HIDDEN_SIZE * 2, 950),\n        )\n        '''\n        \n        self.head = nn.Sequential(\n            nn.Linear( hidden[3] * 2, hidden[3] * 2),\n            nn.LayerNorm( hidden[3] * 2),\n            nn.ReLU(),\n            nn.Linear( hidden[3] * 2, 950),\n        )\n        \n        self.lstm1 = nn.LSTM( len(traincols)+1+2+2, hidden[0],\n                             batch_first=True, bidirectional=True)\n        self.gru = nn.LSTM(2 * hidden[0], hidden[0],\n                             batch_first=True, bidirectional=True)\n        self.lstm3 = nn.LSTM(2 * hidden[0], hidden[2],\n                             batch_first=True, bidirectional=True)\n        self.lstm4 = nn.LSTM(2 * hidden[2], hidden[3],\n                             batch_first=True, bidirectional=True)\n        \n        \n        self._reinitialize()\n        \n        '''\n        # LSTM\n        for n, m in self.named_modules():\n            if isinstance(m, nn.LSTM):\n                print(f'init {m}')\n                for param in m.parameters():\n                    if len(param.shape) >= 2:\n                        nn.init.orthogonal_(param.data)\n                    else:\n                        nn.init.normal_(param.data)\n        '''\n                        \n    def _reinitialize(self):\n        \"\"\"\n        Tensorflow\/Keras-like initialization\n        \"\"\"\n        for name, p in self.named_parameters():\n            if 'lstm' in name:\n                if 'weight_ih' in name:\n                    nn.init.xavier_uniform_(p.data)\n                    #nn.init.orthogonal_(p.data)\n                elif 'weight_hh' in name:\n                    nn.init.orthogonal_(p.data)\n                elif 'bias_ih' in name:\n                    p.data.fill_(0)\n                    # Set forget-gate bias to 1\n                    n = p.size(0)\n                    p.data[(n \/\/ 4):(n \/\/ 2)].fill_(1)\n                elif 'bias_hh' in name:\n                    p.data.fill_(0)\n            elif 'fc' in name or 'conv1' in name:\n                if 'weight' in name:\n                    #nn.init.xavier_uniform_(p.data)\n                    nn.init.normal_(p.data)\n                elif 'bias' in name:\n                    p.data.fill_(0)\n\n    \n    \n    \n    def custom_ce_loss(self, logits=None, y=None, u_out=None):\n        criterion = nn.CrossEntropyLoss()\n\n        loss = criterion(logits.reshape(-1, 950), y.reshape(-1))\n\n        for lag, w in [(1, 0.4), (2, 0.2), (3, 0.1), (4, 0.1)]:\n            # negative lag loss\n            # if target < 0, target = 0\n            neg_lag_target = F.relu(y.reshape(-1) - lag)\n            neg_lag_target = neg_lag_target.long()\n            neg_lag_loss = criterion(logits.reshape(-1, 950), neg_lag_target)\n\n            # positive lag loss\n            # if target > 949, target = 949\n            pos_lag_target = 949 - F.relu((949 - (y.reshape(-1) + lag)))\n            pos_lag_target = pos_lag_target.long()\n            pos_lag_loss = criterion(logits.reshape(-1, 950), pos_lag_target)\n\n            loss += (neg_lag_loss + pos_lag_loss) * w\n\n        return loss\n\n    \n    def monitor_metrics(self, outputs, targets, real_pressure, loss, u_out):\n        w = 1 - u_out     \n        #out = torch.tensor([[target_dic_inv[j.item()] for j in i] for i in torch.argmax(outputs,axis=2)]).to(args.device)\n        out= target_dict_inv_array[torch.argmax(outputs,axis=2)].to(args.device) \n        mae = w * (real_pressure - out).abs()\n        mae = mae.sum(-1) \/ w.sum(-1) \n        \n        return {\"ce\": loss.item(), \"mae\":mae.mean().item()}\n\n    def fetch_scheduler(self):\n        # sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n        #     self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1\n        # )\n        \n        num_train_steps = int((len(train_dataset)\/args.batch_size) * args.epochs)  #int(len(train_loader) * args.epochs)\n        num_warmup_steps = int(num_train_steps \/ 10)\n        #sch = get_cosine_schedule_with_warmup(self.optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n        if resume :\n             print('resume')\n            \n             sch=  torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n                  self.optimizer, T_0=40, T_mult=1, eta_min=4e-6, last_epoch=-1\n              )\n        else:\n             sch=  torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n                  self.optimizer, T_0=40, T_mult=2, eta_min=8e-6, last_epoch=-1\n              )\n            \n        return sch\n\n    def fetch_optimizer(self):\n        opt = torch.optim.Adam(self.parameters(), lr=self.learning_rate,weight_decay=3e-5)\n        return opt\n\n    def forward(self, X, y=None,real_pressure=None, u_out=None,encod_R=None,encod_C=None):\n        bs = X.shape[0]\n        #print(X[:,:,0].unsqueeze(-1).shape)\n        x_ts=self.time_step(X[:,:,0].unsqueeze(-1) ) # we need (bs,seq len,feature )\n        r_emb=self.r(encod_R)\n        c_emb=self.c(encod_C)\n        #print('ts',X.shape,x_ts.shape)\n        X=torch.cat([x_ts,X[:,:,1:]],dim=-1)\n        #print('cat',X.shape)\n        X=torch.cat([X,r_emb,c_emb],dim=-1)\n        #print('lstm',X.shape)\n        x,_ =self.lstm1(X)\n        x,_=self.gru(x)\n        x, _ = self.lstm3(x)\n        \n        #print('dsl3',l3.size())\n        x, _ = self.lstm4(x)\n        \n        #out, _ = self.lstm(X, None) \n        logits = self.head(x)\n\n        if y is not None:\n            loss = self.custom_ce_loss(logits, y, u_out)\n            \n            # loss = self.loss_fct(\n            #     logits.reshape(-1, 950),\n            #     y.reshape(-1)\n            # ) \n\n            metrics = self.monitor_metrics(logits, y, real_pressure, loss, u_out)\n            # print(\"***\", metrics )\n            return logits, loss, metrics\n\n        return logits, 0, {}\n","672f1f57":"class VPPDataset:    \n    def __init__(self, df, label_dic=None,feature_cols=traincols):\n        self.dfs = [_df for _, _df in df.groupby(\"breath_id\")]\n        self.label_dic = label_dic\n        self.features=feature_cols\n        #self.breath_id=df.groupby(\"breath_id\").index.values\n    def __len__(self):\n        return len(self.dfs)\n    \n    def __getitem__(self, item):\n        df = self.dfs[item]\n        #X = df[args.ALL_FEATURES].values     \n        X = df[self.features].values  \n        u_out = df['u_out'].values\n        encod_r=df['encod_R'].values\n        encod_c=df['encod_C'].values\n        if self.label_dic is None:\n            # label = None #[-1]\n            # real_pressure = [-1]\n            pass\n        else:\n            y = self.dfs[item]['pressure'].values\n            #label = [self.label_dic[i] for i in y]\n            label=list(map(lambda i: self.label_dic[i],y))\n            real_pressure = y\n\n        if self.label_dic is None:\n            d = {\n                \"X\": torch.tensor(X).float(),\n                \"u_out\" : torch.tensor(u_out).long(),\n            }            \n        else:\n            d = {\n                \"X\": torch.tensor(X).float(),\n                \"y\" : torch.tensor(label).long(),\n                \"real_pressure\": torch.tensor(real_pressure).float(),\n                \"u_out\" : torch.tensor(u_out).long(),\n                \"encod_R\":torch.tensor(encod_r).long(),\n                \"encod_C\":torch.tensor(encod_c).long()\n            }\n            #print(torch.tensor(u_out).long().sum())\n        \n        return d","f20e8b2e":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","1375bc61":"class VentilatorLoss(nn.Module):\n    \"\"\"\n    Directly optimizes the competition metric\n    \"\"\"\n    def __call__(self, logits=None, y=None, u_out=None):\n        #w = 1 - u_out\n        #w=torch.ones(2,80) if u_out is None else u_out\n        #print(w.size(),y.size(),preds.size())\n        #print(w.sum(-1),preds,'preds')\n        #print(y)\n        #mae = w * (y - preds.squeeze(-1)).abs()\n        #mae = mae.sum(-1) \/ w.sum(-1)\n        criterion = nn.CrossEntropyLoss()\n\n        loss = criterion(logits.reshape(-1, 950), y.reshape(-1))\n\n        for lag, w in [(1, 0.4), (2, 0.2), (3, 0.1), (4, 0.1)]:\n            # negative lag loss\n            # if target < 0, target = 0\n            neg_lag_target = F.relu(y.reshape(-1) - lag)\n            neg_lag_target = neg_lag_target.long()\n            neg_lag_loss = criterion(logits.reshape(-1, 950), neg_lag_target)\n\n            # positive lag loss\n            # if target > 949, target = 949\n            pos_lag_target = 949 - F.relu((949 - (y.reshape(-1) + lag)))\n            pos_lag_target = pos_lag_target.long()\n            pos_lag_loss = criterion(logits.reshape(-1, 950), pos_lag_target)\n\n            loss += (neg_lag_loss + pos_lag_loss) * w\n\n        return loss\n\n        return mae","a726ba82":"class VentilatorLoss_val(nn.Module):\n    \"\"\"\n    Directly optimizes the competition metric\n    \"\"\"\n    def __call__(self, preds, y, u_out=None):\n        #w = 1 - u_out\n        out = torch.tensor([[target_dic_inv[j.item()] for j in i] for i in torch.argmax(outputs,axis=2)]).to(args.device)\n         \n        w=torch.ones(2,80) if u_out is None else u_out\n        #print(w.size(),y.size(),preds.size())\n        #print(w.sum(-1),preds,'preds')\n        #print(y)\n        mae = w * (y - preds.squeeze(-1)).abs()\n        mae = mae.sum(-1) \/ w.sum(-1)\n\n        return mae","e07cdbc3":" def custom_ce_loss(self, logits=None, y=None, u_out=None):\n        criterion = nn.CrossEntropyLoss()\n\n        loss = criterion(logits.reshape(-1, 950), y.reshape(-1))\n\n        for lag, w in [(1, 0.4), (2, 0.2), (3, 0.1), (4, 0.1)]:\n            # negative lag loss\n            # if target < 0, target = 0\n            neg_lag_target = F.relu(y.reshape(-1) - lag)\n            neg_lag_target = neg_lag_target.long()\n            neg_lag_loss = criterion(logits.reshape(-1, 950), neg_lag_target)\n\n            # positive lag loss\n            # if target > 949, target = 949\n            pos_lag_target = 949 - F.relu((949 - (y.reshape(-1) + lag)))\n            pos_lag_target = pos_lag_target.long()\n            pos_lag_loss = criterion(logits.reshape(-1, 950), pos_lag_target)\n\n            loss += (neg_lag_loss + pos_lag_loss) * w\n\n        return loss","9723845c":"#criterion = VentilatorLoss()\n#criterion_val= VentilatorLoss_val()\n#VentilatorLoss()\ndef evaluate(model, loader_val):\n    tb = time.time()\n    was_training = model.training\n    model.eval()\n\n    loss_sum = 0\n    score_sum = 0\n    n_sum = 0\n    y_pred_all = []\n    losses_val=AverageMeter()\n    for ibatch, (x, y, w) in enumerate(loader_val):\n        n = y.size(0)\n        x = x.to(device)\n        y = y.to(device)\n        w = w.to(device)\n\n        with torch.no_grad():\n            y_pred = model(x).squeeze()\n\n        #loss = criterion(y_pred, y)\n        loss = criterion_val(y_pred, y,w).mean()\n        losses_val.update(loss.item(),n)\n        n_sum += n\n        loss_sum += n*loss.item()\n        if ibatch % 10 == 0 or ibatch == (len(loader_val)-1):\n                print('Epoch: [{0}][{1}\/{2}] '\n                      #'Elapsed {remain:s} '\n                      'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                      #'Grad: {grad_norm:.4f}  '\n                      'LR: {lr:.6f}  '\n                      .format(\n                       iepoch+1, ibatch, len(loader_val),\n                       #remain=timeSince(start, float(step+1)\/len(train_loader)),\n                       loss=losses_val,\n                       \n                       lr=1e-3,\n                       ))\n        \n        y_pred_all.append(y_pred.cpu().detach().numpy())\n\n    loss_val = loss_sum \/ n_sum\n\n    model.train(was_training)\n\n    d = {'loss': loss_val,\n         'time': time.time() - tb,\n         'y_pred': np.concatenate(y_pred_all, axis=0)}\n\n    return d\n","46f1147e":"from tqdm.notebook import tqdm as tqdm\nfrom torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingLR,ReduceLROnPlateau,CosineAnnealingWarmRestarts","5a496188":"#ids = train[['id']].values.reshape(-1, 80)","49fe6ab7":"'''\n\nnfold = 5\nkfold = KFold(n_splits=nfold, shuffle=True, random_state=228)\nepochs = 2 if debug else 292\nlr = 1e-3\nbatch_size = 768\nmax_grad_norm = 1000\nimport time\nlog = {}\noof_pred = []\noof_target = []\noof_ids=[]\nfor ifold, (idx_train, idx_val) in enumerate(kfold.split(X_all)):\n    \n    tb = time.time()\n    model = Model(input_size)\n    model.to(device)\n    model.train()\n    if ifold >=2: # due to time limit\n        continue\n    print('Fold %d' % ifold)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=3e-5)\n    #scheduler = ReduceLROnPlateau(optimizer, factor=0.55, patience=10,verbose=True)\n    scheduler = CosineAnnealingWarmRestarts(optimizer, 28, 2,eta_min=3e-5)\n \n    X_train = X_all[idx_train]\n    y_train = y_all[idx_train]\n    w_train = w_all[idx_train]\n    ids_val=ids[idx_val]\n    X_val = X_all[idx_val]\n    y_val = y_all[idx_val]\n    w_val = w_all[idx_val]\n\n    dataset_train = Dataset(X_train, y_train, w_train)\n    dataset_val = Dataset(X_val, y_val, w_val)\n    loader_train = torch.utils.data.DataLoader(dataset_train, shuffle=False,\n                         batch_size=batch_size, drop_last=True)\n    loader_val = torch.utils.data.DataLoader(dataset_val, shuffle=False,\n                         batch_size=batch_size, drop_last=False)\n    \n     \n    scheduler1=OneCycleLR(optimizer, 1.5e-3, total_steps=len(loader_train)*batch_size\n                                 ,cycle_momentum=False, \n                                 pct_start=0.6, anneal_strategy='cos',\n                                 div_factor=30.0, final_div_factor=1,\n                                 last_epoch=-1, verbose=False)\n     \n    #scheduler = ReduceLROnPlateau(optimizer, factor=0.6, patience=10)\n    \n    losses_train = []\n    losses_val = []\n    lrs = []\n    time_val = 0\n    best_score = np.inf\n   \n    print('epoch loss_train loss_val lr time')\n    best_score=np.inf\n     \n    for iepoch in range(epochs):\n        loss_train = 0\n        n_sum = 0\n        losses = AverageMeter()\n        for ibatch, (x, y, w) in enumerate(tqdm(loader_train)):\n            n = y.size(0)\n            x = x.to(device)\n            y = y.to(device)\n            w = w.to(device)\n            optimizer.zero_grad()\n\n            y_pred = model(x).squeeze()\n            loss = criterion_val(y_pred,y )\n            #loss = criterion_val(y_pred, y,w).mean()\n            losses.update(loss.item(),n)\n            loss_train += n*loss.item()\n            n_sum += n\n\n            loss.backward()\n            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n            if ibatch % 30 == 0 or ibatch == (len(loader_train)-1):\n                print('Epoch: [{0}][{1}\/{2}] '\n                      #'Elapsed {remain:s} '\n                      'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                      #'Grad: {grad_norm:.4f}  '\n                      'LR: {lr:.6f}  '\n                      .format(\n                       iepoch+1, ibatch, len(loader_train),\n                       #remain=timeSince(start, float(step+1)\/len(train_loader)),\n                       loss=losses,\n                       \n                       #lr=1e-3,\n                       lr=scheduler.get_last_lr()[-1] \n                       ))\n\n            optimizer.step()\n            scheduler.step()\n        #scheduler.step()\n        val = evaluate(model, loader_val)\n        \n        loss_val = val['loss']\n        time_val += val['time']\n        if val['loss']<best_score:\n            best_score=val['loss']\n            \n            print('saving best wt',best_score)\n            time.sleep(1)\n            torch.save(model.state_dict(),f'fold_best{ifold}.pt')\n        losses_train.append(loss_train \/ n_sum)\n        losses_val.append(val['loss'])\n        lrs.append(optimizer.param_groups[0]['lr'])\n\n        print('%3d %9.6f %9.6f %7.3e %7.1f %6.1f' %\n              (iepoch + 1,\n               losses_train[-1], losses_val[-1], \n               lrs[-1], time.time() - tb, time_val))\n\n        #scheduler.step(losses_val[-1])\n        #capure oof\n        #oof_pred.append( val['y_pred'])\n        #oof_target.append(y_val)\n        #oof_ids.append(ids_val)\n     \n    print('generate fold',ifold ,' oofs')\n    model.load_state_dict(torch.load(f'\/kaggle\/working\/fold_best{ifold}.pt'))\n    val = evaluate(model, loader_val)\n    oof_pred.append(val['y_pred'])\n    oof_target.append(y_val)\n    oof_ids.append(ids_val)\n    #oof_target=np.concatenate(oof_target)\n    #oof_ids=np.concatenate(oof_ids)\n    #ofilename = 'model%d.pth' % ifold\n    #torch.save(model.state_dict(), ofilename)\n    #print(ofilename, 'written')\n\n    log['fold%d' % ifold] = {\n        'loss_train': np.array(losses_train),\n        'loss_val': np.array(losses_val),\n        'learning_rate': np.array(lrs),\n        'y_pred': val['y_pred'],\n        'idx': idx_val\n    }\n'''","5508d3b9":"#train_df=train_df.astype('float32')\n#train_df.dtype","a2ba3175":"train_df = pd.DataFrame(train_df,columns=traincols)\n#test_df = pd.DataFrame(test_df,columns=testcols)\ntest=False\nif test:\n    \n    testcols = test_df.columns\n    test_df = RS.transform(test_df[testcols])\n    test_df=test_df.astype('float32')\n    test_df['breath_id'] = test_breathid\n    test_df['id'] = test_ids\n    print(test_df.shape)\n\ntrain_df['breath_id'] = train_breathid\n#test_df['breath_id'] = test_breathid\ntrain_df['pressure'] = train_pressure\ntrain_df['id'] = train_ids\n\ntrain_df['kfold'] = kfolds.kfold\n\ntrain_df.shape","a3fa5cb4":"train_df.head()\n\nr_mapping={5:0,20:1,50:2}\n\nc_mapping={10:0,20:1,50:2}\n\nembedding_df['encod_R']=embedding_df['R'].map(r_mapping)\nembedding_df['encod_C']=embedding_df['C'].map(c_mapping)","2761c78b":"#r_mapping[20]","5d57a125":"#enc_cat_mapping={'time_step':0,'encod_R':1,'encod_C':2}\n#enc_cat_columns=['encod_R','encod_C','time_step']","bf5fb748":"train_df=pd.concat([train_df,embedding_df.iloc[:,-2:]],axis=1)","901b5184":"train_df.columns","8f1d8ab2":"df_train = train_df[train_df.kfold != args.fold].reset_index(drop=True)\ndf_valid = train_df[train_df.kfold == args.fold].reset_index(drop=True)\n\ntrain_dataset = VPPDataset(\n        df = df_train[list(traincols)+['breath_id','encod_R','encod_C' ,'pressure']],\n        label_dic = target_dic,\n        feature_cols=traincols\n    )\n\nvalid_dataset = VPPDataset(\n    df = df_valid[list(traincols)+['breath_id','encod_R','encod_C' ,'pressure']],\n    label_dic = target_dic,\n    feature_cols=traincols\n)\n\ndel train_df\ngc.collect()","6c5695b1":"#!cp ..\/input\/ventilator-train-classification-ce-pytorch-optim\/model_f0_r1.bin \/kaggle\/working\/ \n\n#sample_data=train_dataset[0] ","32ca2469":"'''\n\nSineAct=SineActivation\nsineact = SineAct(1, 64)\ncosact = CosineActivation(1, 64)\n#print(torch.randn(2,80,1).shape)\nprint(sineact(torch.randn(2,80,1)).shape)\n#print(cosact(torch.Tensor([[7]])).shape)\n'''","f2f5a17b":"#del model\nresume=False\nif resume :\n    model=VPPModel(learning_rate = 7e-4)\n    #model.load_state_dict(torch.load('\/kaggle\/working\/model_f0_r1.bin'))\n    #model.to(args.device)\n    #model.eval()\nelse:\n    model = VPPModel(learning_rate = args.learning_rate)\n\nes = EarlyStopping(\n    monitor= \"valid_mae\",\n    model_path=f\"{args.output_folder}\/model_f{args.fold}.bin\",\n    patience=120,\n    mode=\"min\",\n    save_weights_only=True,\n    delta=0.0001\n)\ngc.collect()","a6c913c2":"'''\nX=sample_data['X'].unsqueeze(0)\ny=sample_data['y'].unsqueeze(0)\nu_out=sample_data['u_out'].unsqueeze(0)\nrp=sample_data['real_pressure'].unsqueeze(0)\neR=sample_data['encod_R'].unsqueeze(0)\neC=sample_data['encod_C'].unsqueeze(0)\nmodel(X,y,rp,u_out,eR,eC)\n'''","6fcf40bf":"gc.collect()","8c25ebe2":"print(\"Fold:\",args.fold)\n\ntrain=True\nif train:\n\n    model.fit(\n        train_dataset,\n        valid_dataset=valid_dataset,\n        train_bs=args.batch_size,\n        valid_bs=3 * args.batch_size,\n        device=args.device,\n        epochs=180,\n        callbacks=[es],\n        fp16=args.fp16,\n        accumulation_steps=args.accumulation_steps,\n        n_jobs = 4\n    ) \n","4b8eb4a7":"'''\n# LSR scheduler\nimport torch\nfrom torch import optim\nfrom matplotlib import pyplot as plt\nmodel = torch.nn.Linear(10, 2)\noptimizer = optim.SGD(model.parameters(), lr=3e-3)\nsteps = 10\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)\nscheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 40, 2)\nlrs=[]\nfor epoch in range(300):\n    for idx in range(62):\n        None\n        #scheduler.step()\n        #print(scheduler.get_lr())\n        scheduler.step()\n        lrs.append(scheduler.get_lr())\n'''","bebbb8b0":"#plt.plot(lrs)","937954b8":"try:\n    \n    del train_dataset\n    gc.collect()\n    gc.collect()\n\n    from tqdm.notebook import tqdm as tqdm\n    #model.load(f\"{args.output_folder}\/model_f{args.fold}.bin\", device=\"cuda\", weights_only=True)\n    model.load(f\"{args.output_folder}\/model_f{args.fold}.bin\", device=\"cuda\", weights_only=True)\n    valid_predictions = model.predict(valid_dataset, batch_size=args.batch_size, n_jobs=-1)\n\n    final_valid_predictions = []\n    for preds in tqdm(valid_predictions):\n       #out = [[target_dic_inv[j.item()] for j in i] for i in np.argmax(preds,axis=2)]\n        out= target_dict_inv_array[np.argmax(preds,axis=2)] \n        out=out.cpu().numpy()\n        final_valid_predictions.extend(np.array(out).flatten().tolist())\n\n    df_valid = df_valid[[\"id\", \"pressure\", \"u_out\"]]\n    df_valid[\"target\"] = final_valid_predictions \n\n    df_valid.to_csv(f\"{args.output_folder}\/valid_predictions_f{args.fold}.csv\", index=False)\nexcept:\n    print('x')\n ","8b9b16f2":"import pandas as pd\nimport numpy as np\n","9c41dd33":"try:\n    df_valid=pd.read_csv(f\"{args.output_folder}\/valid_predictions_f{args.fold}.csv\")\n    \n    df_valid['error']=np.abs(df_valid.pressure-df_valid.target)\n    print(df_valid[df_valid.u_out==-1].error.mean())\nexcept:\n    print('x')","8e75dd5c":"'''\ndel train_dataset, valid_dataset\ngc.collect()\n\nsub = pd.read_csv(f'{data_dir}\/sample_submission.csv')\n\ntest_dataset = VPPDataset(\n    df = test_df\n)\nmodel = VPPModel(learning_rate = args.learning_rate)\n\nmodel.load(f\"{args.output_folder}\/model_f{args.fold}.bin\", device=\"cuda\", weights_only=True)\n\ndel train_df\ngc.collect()\n\ntest_predictions = model.predict(test_dataset, batch_size=args.batch_size, n_jobs=0)\nfinal_test_predictions = []\n\nfor preds in tqdm(test_predictions):\n    out = [[target_dic_inv[j.item()] for j in i] for i in np.argmax(preds,axis=2)]\n    final_test_predictions.extend(np.array(out).flatten().tolist())\n\nsub[\"pressure\"] = final_test_predictions\nsub.to_csv(f\"{args.output_folder}\/test_predictions_f{args.fold}.csv\", index=False)\n'''","f43fc5ba":"! rm -rf \/kaggle\/working\/Time2Vec-PyTorch","5a3cc2f7":"## Validation","fdf769ac":"## Test"}}