{"cell_type":{"91df8008":"code","2c502781":"code","a736c8fe":"code","a412ad7a":"code","e9d3d302":"code","b1f0459d":"code","70d5fdd8":"code","64871e30":"code","854600b8":"code","45008729":"code","a003d5ce":"code","5c0ed4e4":"code","aeb4cf1c":"code","808e336f":"code","39b29285":"code","6455bef3":"code","9b9d8af9":"code","10a63ed3":"code","8506d193":"code","886f223a":"code","f6a40b15":"code","f89b5757":"code","77452e44":"code","9d66e75d":"code","3befcab7":"code","3be69b9b":"code","7cb8f032":"code","1b0fe87e":"code","620c4fd8":"code","2c699e8c":"code","af3a9466":"code","690157cb":"code","811093b2":"code","7e306e7e":"code","104e0317":"code","9ae3a4db":"code","bcbdc236":"code","31441f3f":"code","1e13bdca":"code","ec0fe758":"code","33f00e18":"code","f860bc4a":"code","b66f36e7":"code","90ad41c8":"code","408b23cb":"code","8bdf2e57":"code","29b3c4e4":"code","bd5ac4fb":"code","91678bf8":"code","20cb4f14":"code","d8bafc8f":"code","19dfe6a9":"code","221b9aef":"code","edf325af":"code","e4128998":"code","1d86cd3f":"code","3e382dc6":"code","9dfaa376":"code","439d56a4":"code","5784138a":"code","cb837667":"code","145f5467":"code","3ad7b7af":"code","592d377c":"code","5bfddd84":"code","2b436f43":"code","403273f9":"code","b04e0e41":"code","97d54367":"markdown","e2eed3b3":"markdown","fdb3e10b":"markdown","61d195cb":"markdown","7ddcf96b":"markdown","89d91a78":"markdown","e8e2198c":"markdown","c27aa153":"markdown","f8b4355a":"markdown","f6fc8b76":"markdown","62335d59":"markdown","90ce1f84":"markdown","93f6065e":"markdown","ce3f2c36":"markdown","d259274f":"markdown","5f67d619":"markdown","d9590517":"markdown","7f25fe14":"markdown","121b00c4":"markdown","12a73d24":"markdown","4d748ed8":"markdown","aca65c43":"markdown","5a4281fb":"markdown","12d783a5":"markdown","20859780":"markdown","8e916455":"markdown","31ea2288":"markdown","bd3e554a":"markdown","6c75fb52":"markdown","0df348c2":"markdown","147e16d8":"markdown"},"source":{"91df8008":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","2c502781":"# Reading the dataset and inputting appropriate column names\ndf = pd.read_csv('..\/input\/aiml-ca1-datasets\/agaricus-lepiota.data', names=['edibility', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat'])\npd.options.display.max_columns = None\ndf","a736c8fe":"df.info()","a412ad7a":"f = plt.figure(figsize=(20,10))\nfor i in range(0, 23):\n    f.add_subplot(3,8,i+1)\n    sns.countplot(x=df.iloc[:, i])\nplt.tight_layout()","e9d3d302":"#  stalk-root has missing data encoded as '?', will leave it as 'unknown' as imputing it to 'most_frequent' will cause the data to be heavily skewed as stalk-root already has a lot of 'b' data.\ndf.replace('?', 'unknown', inplace=True)\ndf","b1f0459d":"# Drop veil-type as all data have veil-type as 'p' so it would not affect the model\ndf = df.drop('veil-type', axis='columns')\ndf","70d5fdd8":"# Encoding categorical variables into numerical variables\ndf = pd.get_dummies(df, drop_first=True)\ndf","64871e30":"# Separate Target and Feature variables\nfeatures = df.drop('edibility_p', axis='columns')\nfeatures","854600b8":"target = df['edibility_p']\ntarget","45008729":"from sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\n# Classfication Models\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC","a003d5ce":"# Split data into Training and Testing data\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3)","5c0ed4e4":"# fit a k-nearest neighbor model to the data\nmodel = KNeighborsClassifier()\nmodel.fit(X_train, y_train)\nprint(model)\n\n# make predictions\nexpected = y_test\npredicted = model.predict(X_test)\n\n# summarize the fit of the model\nprint(classification_report(expected, predicted))\nprint(confusion_matrix(expected, predicted))\nknn_score = model.score(X_test, y_test)","aeb4cf1c":"# fit model to data\nknn = KNeighborsClassifier()\nparameters = {'n_neighbors': np.arange(1, 11), 'weights': ['uniform', 'distance']}\nknn_cv = GridSearchCV(knn, parameters, cv=5, n_jobs=-1)\n\nknn_cv.fit(X_train, y_train)\nprint('KNN Cross Validation Parameters:', knn_cv.best_params_)\nprint('KNN Best Score:', knn_cv.best_score_)","808e336f":"knndf = pd.DataFrame(knn_cv.cv_results_)\nknndf = knndf.sort_values('rank_test_score')\nknndf = knndf[['rank_test_score', 'params', 'mean_test_score', 'std_test_score']]\nknndf.head(12)","39b29285":"# make predictions\nexpectedTune = y_test\npredictedTune = knn_cv.predict(X_test)\n\n# summarize the fit of the model\nprint(classification_report(expectedTune, predictedTune))\nprint(confusion_matrix(expectedTune, predictedTune))\nknntune_score = knn_cv.score(X_test, y_test)","6455bef3":"# fit a Naive Bayes model to the data\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\nprint(model)\n\n# make predictions\nexpected = y_test\npredicted = model.predict(X_test)\n\n# summarize the fit of the model\nprint(classification_report(expected, predicted))\nprint(confusion_matrix(expected, predicted))\nnb_score = model.score(X_test, y_test)","9b9d8af9":"# fit model to data\nnb = GaussianNB()\nparameters = {'var_smoothing': np.logspace(0,-9, num=100)}\nnb_cv = GridSearchCV(nb, parameters, cv=5, n_jobs=-1)\n\nnb_cv.fit(X_train, y_train)\nprint('Naive Bayes Cross Validation Parameters:', nb_cv.best_params_)\nprint('Naive Bayes Best Score:', nb_cv.best_score_)","10a63ed3":"nbdf = pd.DataFrame(nb_cv.cv_results_)\nnbdf = nbdf.sort_values('rank_test_score')\nnbdf = nbdf[['rank_test_score', 'params', 'mean_test_score', 'std_test_score']]\nnbdf.head()","8506d193":"# make predictions\nexpectedTune = y_test\npredictedTune = nb_cv.predict(X_test)\n\n# summarize the fit of the model\nprint(classification_report(expectedTune, predictedTune))\nprint(confusion_matrix(expectedTune, predictedTune))\nnbtune_score = nb_cv.score(X_test, y_test)","886f223a":"# fit a logistic regression model to the data\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\nprint(model)\n\n# make predictions\nexpected = y_test\npredicted = model.predict(X_test)\n\n# summarize the fit of the model\nprint(classification_report(expected, predicted))\nprint(confusion_matrix(expected, predicted))\nlogreg_score = model.score(X_test, y_test)","f6a40b15":"# fit model to data\nlogreg = LogisticRegression(max_iter=10000, solver='liblinear')\nparameters = {'penalty': ['l1', 'l2'], 'C': np.logspace(-4, 4, 20)}\nlogreg_cv = GridSearchCV(logreg, parameters, cv=5, n_jobs=-1)\n\nlogreg_cv.fit(X_train, y_train)\nprint('Logistic Regression Cross Validation Parameters:', logreg_cv.best_params_)\nprint('Logistic Regression Best Score:', logreg_cv.best_score_)","f89b5757":"logregdf = pd.DataFrame(logreg_cv.cv_results_)\nlogregdf = logregdf.sort_values('rank_test_score')\nlogregdf = logregdf[['rank_test_score', 'params', 'mean_test_score', 'std_test_score']]\nlogregdf.head(8)","77452e44":"# make predictions\nexpectedTune = y_test\npredictedTune = logreg_cv.predict(X_test)\n\n# summarize the fit of the model\nprint(classification_report(expectedTune, predictedTune))\nprint(confusion_matrix(expectedTune, predictedTune))\nlogregtune_score = logreg_cv.score(X_test, y_test)","9d66e75d":"# fit a CART model to the data\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\nprint(model)\n\n# make predictions\nexpected = y_test\npredicted = model.predict(X_test)\n\n# summarize the fit of the model\nprint(classification_report(expected, predicted))\nprint(confusion_matrix(expected, predicted))\ncart_score = model.score(X_test, y_test)","3befcab7":"# fit model to data\ncart = DecisionTreeClassifier()\nparameters = {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random'], 'max_features': np.arange(1, 10)}\ncart_cv = GridSearchCV(cart, parameters, cv=5, n_jobs=-1)\n\ncart_cv.fit(X_train, y_train)\nprint('CART Cross Validation Parameters:', cart_cv.best_params_)\nprint('CART Best Score:', cart_cv.best_score_)","3be69b9b":"cartdf = pd.DataFrame(cart_cv.cv_results_)\ncartdf = cartdf.sort_values('rank_test_score')\ncartdf = cartdf[['rank_test_score', 'params', 'mean_test_score', 'std_test_score']]\ncartdf.head()","7cb8f032":"# make predictions\nexpectedTune = y_test\npredictedTune = cart_cv.predict(X_test)\n\n# summarize the fit of the model\nprint(classification_report(expectedTune, predictedTune))\nprint(confusion_matrix(expectedTune, predictedTune))\ncarttune_score = cart_cv.score(X_test, y_test)","1b0fe87e":"# fit a SVM model to the data\nmodel = SVC()\nmodel.fit(X_train, y_train)\nprint(model)\n\n# make predictions\nexpected = y_test\npredicted = model.predict(X_test)\n\n# summarize the fit of the model\nprint(classification_report(expected, predicted))\nprint(confusion_matrix(expected, predicted))\nsvc_score = model.score(X_test, y_test)","620c4fd8":"# fit model to data\nsvc = SVC()\nparameters = {'C': [0.1, 1, 10, 100, 1000], 'gamma': [0.1, 1, 10, 100]}\nsvc_cv = GridSearchCV(svc, parameters, cv=5, n_jobs=-1)\n\nsvc_cv.fit(X_train, y_train)\nprint('SVC Cross Validation Parameters:', svc_cv.best_params_)\nprint('SVC Best Score:', svc_cv.best_score_)","2c699e8c":"svcdf = pd.DataFrame(svc_cv.cv_results_)\nsvcdf = svcdf.sort_values('rank_test_score')\nsvcdf = svcdf[['rank_test_score', 'params', 'mean_test_score', 'std_test_score']]\nsvcdf.head()","af3a9466":"# make predictions\nexpectedTune = y_test\npredictedTune = svc_cv.predict(X_test)\n\n# summarize the fit of the model\nprint(classification_report(expectedTune, predictedTune))\nprint(confusion_matrix(expectedTune, predictedTune))\nsvctune_score = svc_cv.score(X_test, y_test)","690157cb":"classification_summary = pd.DataFrame([[knn_score, knntune_score, knntune_score-knn_score], [nb_score, nbtune_score, nbtune_score-nb_score],  [logreg_score, logregtune_score, logregtune_score-logreg_score], [cart_score, carttune_score, carttune_score-cart_score], [svc_score, svctune_score, svctune_score-svc_score]], columns=['Default Model', 'Tuned Model', 'Difference'], index=['k-Nearest Neighbor', 'Naive Bayes', 'Logistic Regression', 'Decision Tree Classifier', 'Support Vector Classifier'])\nclassification_summary","811093b2":"regDf = pd.read_csv('..\/input\/aiml-ca1-datasets\/kc_house_data.csv')\nregDf","7e306e7e":"regDf = regDf.drop(['id', 'date'], axis='columns')","104e0317":"regDf.info()","9ae3a4db":"regDf.describe()","bcbdc236":"g = plt.figure(figsize=(30,10))\nfor i in range(0, 19):\n    g.add_subplot(3,7,i+1)\n    sns.histplot(x=regDf.iloc[:, i], bins=25)\nplt.tight_layout()","31441f3f":"plt.figure(figsize=(10, 10))\nsns.heatmap(regDf.corr(), square=True, cmap='Blues')\nplt.show()","1e13bdca":"target = regDf.iloc[:, 0]\ntarget","ec0fe758":"features = regDf.drop('price', axis='columns')\nfeatures","33f00e18":"from sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n# Regression Models\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR","f860bc4a":"X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3)","b66f36e7":"# extract 'sqft_living' feature\nlinregX = np.array(features['sqft_living']).reshape(-1, 1)\nlinregY = target\nlinregX_train, linregX_test, linregy_train, linregy_test = train_test_split(linregX, linregY, test_size=0.3)\n\n# fit a linear regression model to the data\nmodel = LinearRegression()\nmodel.fit(linregX_train, linregy_train)\nprint(model)\n\n# make predictions\nexpected = linregy_test\npredicted = model.predict(linregX_test)\n\n# summarize the fit of the model\nrmse = (np.mean((predicted-expected)**2))**0.5\n\nprint('RMSE =', rmse)\nstupid_score = model.score(linregX_test, linregy_test)\nprint('R2 =', stupid_score)","90ad41c8":"prediction_space = np.linspace(min(linregX_test), max(linregX_test)).reshape(-1, 1)\n\nplt.figure(figsize=(10, 5))\nplt.scatter(linregX_test, linregy_test)\nplt.plot(prediction_space, model.predict(prediction_space), color='red')\nplt.ylabel('price')\nplt.xlabel('sqft_living')\nplt.show()","408b23cb":"# fit model to data\nlinreg = LinearRegression()\nstupid_cv = cross_val_score(linreg, linregX_train, linregy_train, cv=5, n_jobs=-1)\n\nstupidtune_score = np.mean(stupid_cv)\nprint('Stupid Baseline CV Mean:', stupidtune_score)\nprint('Stupid Baseline CV STD:', np.std(stupid_cv))","8bdf2e57":"# fit model to the data\nlinregpipe = Pipeline([('scaler', StandardScaler()), ('linreg', LinearRegression())])\nlinregpipe.fit(X_train, y_train)\nprint(linregpipe)\n\n# make predictions\nexpected = y_test\npredicted = linregpipe.predict(X_test)\n\n# summarize the fit of the model\nrmse = (np.mean((predicted-expected)**2))**0.5\n\nprint('RMSE =', rmse)\nlinreg_score = linregpipe.score(X_test, y_test)\nprint('R2 =', linreg_score)","29b3c4e4":"# fit model to data\nlinreg_cv = cross_val_score(linregpipe, X_train, y_train, cv=5, n_jobs=-1)\n\nlinregtune_score = np.mean(linreg_cv)\nprint('Linear Regression CV Mean:', linregtune_score)\nprint('Linear Regression CV STD:', np.std(linreg_cv))","bd5ac4fb":"# fit model to the data\nlassopipe = Pipeline([('scaler', StandardScaler()), ('lasso', Lasso(max_iter=100000))])\nlassopipe.fit(X_train, y_train)\nprint(lassopipe)\n\n# make predictions\nexpected = y_test\npredicted = lassopipe.predict(X_test)\n\n# summarize the fit of the model\nrmse = (np.mean((predicted-expected)**2))**0.5\nprint('RMSE =', rmse)\nlasso_score = lassopipe.score(X_test, y_test)\nprint('R2 =', lasso_score)","91678bf8":"# fit model to data\nparameters = {'lasso__alpha': np.arange(0, 1, 0.01)}\nlasso_cv = GridSearchCV(lassopipe, parameters, cv=5, n_jobs=-1)\n\nlasso_cv.fit(X_train, y_train)\nprint('Lasso Cross Validation Parameters:', lasso_cv.best_params_)\nprint('Lasso Best Score:', lasso_cv.best_score_)","20cb4f14":"lassodf = pd.DataFrame(lasso_cv.cv_results_)\nlassodf = lassodf.sort_values('rank_test_score')\nlassodf = lassodf[['rank_test_score', 'params', 'mean_test_score', 'std_test_score']]\nlassodf.head()","d8bafc8f":"# make predictions\nexpected = y_test\npredicted = lasso_cv.predict(X_test)\n\n# summarize the fit of the model\nrmse = (np.mean((predicted-expected)**2))**0.5\n\nprint('RMSE =', rmse)\nlassotune_score = lasso_cv.score(X_test, y_test)\nprint('R2 =', lassotune_score)","19dfe6a9":"# fit model to the data\nridgepipe = Pipeline([('scaler', StandardScaler()), ('ridge', Ridge())])\nridgepipe.fit(X_train, y_train)\nprint(ridgepipe)\n\n# make predictions\nexpected = y_test\npredicted = ridgepipe.predict(X_test)\n\n# summarize the fit of the model\nrmse = (np.mean((predicted-expected)**2))**0.5\n\nprint('RMSE =', rmse)\nridge_score = ridgepipe.score(X_test, y_test)\nprint('R2 =', ridge_score)","221b9aef":"# fit model to data\nparameters = {'ridge__alpha': np.arange(0, 1, 0.01)}\nridge_cv = GridSearchCV(ridgepipe, parameters, cv=5, n_jobs=-1)\n\nridge_cv.fit(X_train, y_train)\nprint('Ridge Cross Validation Parameters:', ridge_cv.best_params_)\nprint('Ridge Best Score:', ridge_cv.best_score_)","edf325af":"ridgedf = pd.DataFrame(ridge_cv.cv_results_)\nridgedf = ridgedf.sort_values('rank_test_score')\nridgedf = ridgedf[['rank_test_score', 'params', 'mean_test_score', 'std_test_score']]\nridgedf.head()","e4128998":"# make predictions\nexpected = y_test\npredicted = ridge_cv.predict(X_test)\n\n# summarize the fit of the model\nrmse = (np.mean((predicted-expected)**2))**0.5\n\nprint('RMSE =', rmse)\nridgetune_score = ridge_cv.score(X_test, y_test)\nprint('R2 =', ridgetune_score)","1d86cd3f":"# fit model to the data\nelasticpipe = Pipeline([('scaler', StandardScaler()), ('elastic', ElasticNet())])\nelasticpipe.fit(X_train, y_train)\nprint(elasticpipe)\n\n# make predictions\nexpected = y_test\npredicted = elasticpipe.predict(X_test)\n\n# summarize the fit of the model\nrmse = (np.mean((predicted-expected)**2))**0.5\n\nprint('RMSE =', rmse)\nelastic_score = elasticpipe.score(X_test, y_test)\nprint('R2 =', elastic_score)","3e382dc6":"# fit model to data\nparameters = {'elastic__l1_ratio': np.arange(0, 1, 0.01), 'elastic__alpha': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]}\nelastic_cv = GridSearchCV(elasticpipe, parameters, cv=5, n_jobs=-1)\n\nelastic_cv.fit(X_train, y_train)\nprint('ElasticNet Cross Validation Parameters:', elastic_cv.best_params_)\nprint('ElasticNet Best Score:', elastic_cv.best_score_)","9dfaa376":"elasticdf = pd.DataFrame(elastic_cv.cv_results_)\nelasticdf = elasticdf.sort_values('rank_test_score')\nelasticdf = elasticdf[['rank_test_score', 'params', 'mean_test_score', 'std_test_score']]\nelasticdf.head()","439d56a4":"# test tuned model on unseen data\nexpected = y_test\npredicted = elastic_cv.predict(X_test)\n\n# summarize the fit of the model\nrmse = (np.mean((predicted-expected)**2))**0.5\n\nprint('RMSE =', rmse)\nelastictune_score = elastic_cv.score(X_test, y_test)\nprint('R2 =', elastictune_score)","5784138a":"# fit model to the data\ntreeregpipe = Pipeline([('scaler', StandardScaler()), ('treereg', DecisionTreeRegressor())])\ntreeregpipe.fit(X_train, y_train)\nprint(treeregpipe)\n\n# make predictions\nexpected = y_test\npredicted = treeregpipe.predict(X_test)\n\n# summarize the fit of the model\nrmse = (np.mean((predicted-expected)**2))**0.5\n\nprint('RMSE =', rmse)\ntreereg_score = treeregpipe.score(X_test, y_test)\nprint('R2 =', treereg_score)","cb837667":"# fit model to data\nparameters = {'treereg__max_depth': np.arange(1, 11), 'treereg__min_samples_split': np.arange(10, 51, 10)}\ntreereg_cv = GridSearchCV(treeregpipe, parameters, cv=5, n_jobs=-1)\n\ntreereg_cv.fit(X_train, y_train)\nprint('Decision Tree Regression Cross Validation Parameters:', treereg_cv.best_params_)\nprint('Decision Tree Regression Best Score:', treereg_cv.best_score_)","145f5467":"treeregdf = pd.DataFrame(treereg_cv.cv_results_)\ntreeregdf = treeregdf.sort_values('rank_test_score')\ntreeregdf = treeregdf[['rank_test_score', 'params', 'mean_test_score', 'std_test_score']]\ntreeregdf.head()","3ad7b7af":"# test tuned model on unseen data\nexpected = y_test\npredicted = treereg_cv.predict(X_test)\n\n# summarize the fit of the model\nmse = (np.mean((predicted-expected)**2))**0.5\n\nprint('RMSE =', rmse)\ntreeregtune_score = treereg_cv.score(X_test, y_test)\nprint('R2 =', treeregtune_score)","592d377c":"# fit model to the data\nsvrpipe = Pipeline([('scaler', StandardScaler()), ('svr', SVR())])\nsvrpipe.fit(X_train, y_train)\nprint(svrpipe)\n\n# make predictions\nexpected = y_test\npredicted = svrpipe.predict(X_test)\n\n# summarize the fit of the model\nrmse = (np.mean((predicted-expected)**2))**0.5\n\nprint('RMSE =', rmse)\nsvr_score = svrpipe.score(X_test, y_test)\nprint('R2 =', svr_score)","5bfddd84":"# fit model to data\nparameters = {'svr__kernel': ['linear', 'poly', 'rbf'], 'svr__C': [0.1, 1, 10, 100, 1000], 'svr__epsilon': [0.1, 1, 10, 100]}\nsvr_cv = GridSearchCV(svrpipe, parameters, cv=5, n_jobs=-1)\n\nsvr_cv.fit(X_train, y_train)\nprint('SVR Cross Validation Parameters:', svr_cv.best_params_)\nprint('SVR Best Score:', svr_cv.best_score_)","2b436f43":"svrdf = pd.DataFrame(svr_cv.cv_results_)\nsvrdf = svrdf.sort_values('rank_test_score')\nsvrdf = svrdf[['rank_test_score', 'params', 'mean_test_score', 'std_test_score']]\nsvrdf.head()","403273f9":"# test tuned model on unseen data\nexpected = y_test\npredicted = svr_cv.predict(X_test)\n\n# summarize the fit of the model\nrmse = (np.mean((predicted-expected)**2))**0.5\n\nprint('RMSE =', rmse)\nsvrtune_score = svr_cv.score(X_test, y_test)\nprint('R2 =', svrtune_score)","b04e0e41":"regression_summary = pd.DataFrame([[stupid_score, stupidtune_score, '<- CV Score'], [linreg_score, linregtune_score, '<- CV Score'],  [lasso_score, lassotune_score, lassotune_score-lasso_score], [ridge_score, ridgetune_score, ridgetune_score-ridge_score], [elastic_score, elastictune_score, elastictune_score-elastic_score], [treereg_score, treeregtune_score, treeregtune_score-treereg_score], [svr_score, svrtune_score, svrtune_score-svr_score]], columns=['Default Model', 'Tuned Model', 'Difference'], index=['Stupid Baseline', 'Linear Regression', 'Lasso Regression', 'Ridge Regression', 'Elastic Net Regression', 'Decision Tree Regressor', 'Support Vector Regression'])\nprint('Tuned Model refers to cross-validated scores for Stupid Baseline and Linear Regression')\nregression_summary","97d54367":"## Support Vector Regressor","e2eed3b3":"### Hyperparameter Tuning","fdb3e10b":"## Lasso Regression (L1)","61d195cb":"## Classification Tree","7ddcf96b":"Prediction task is to predict the sale price of a house in King County.\n\n'price' is the sale price of the houses in the dataset\n\nScaling of features is needed as seen from the histograms and .describe()","89d91a78":"### Cross Validation","e8e2198c":"***\n# Regression","c27aa153":"Data is encoded numerically using dummy variables ","f8b4355a":"## Decision Tree Regressor","f6fc8b76":"### Hyperparameter Tuning","62335d59":"#### Hyperparameter Tuning","90ce1f84":"##  Logistic Regression","93f6065e":"#### Hyperparameter Tuning","ce3f2c36":"## SVC","d259274f":"## KNN","5f67d619":"Can drop 'id', 'date' as it should not affect house prices","d9590517":"### Hyperparameter Tuning","7f25fe14":"Prediction task is to classify mushroom into edible or not edible based on the features of the mushroom. \n\n'edibility_p' = 1 means that mushroom is poisonous and is not edible <br>\n'edibility_p' = 0 means that mushroom is not poisonous and is edible\n\nThere is no need to normalize or scale data as data are all binary.","121b00c4":"There is also no class imbalance present as seen from the first graph on 'edibility' so accuracy would be a good metric for measuring model performance","12a73d24":"### Hyperparameter Tuning","4d748ed8":"# Classification","aca65c43":"## Stupid Baseline","5a4281fb":"### Hyperparameter Tuning","12d783a5":"#### Hyperparameter Tuning","20859780":"## Elastic Net","8e916455":"### Hyperparameter Tuning","31ea2288":"## Naive Bayes","bd3e554a":"## Linear Regression","6c75fb52":"## Ridge Regression (L2)","0df348c2":"Bennett Ng Teng Seng P2011831 DAAA\/FT\/2A\/02","147e16d8":"### Hyperparameter Tuning"}}