{"cell_type":{"e065e17c":"code","75dabb3e":"code","058576c2":"code","208d3065":"code","0bc5dc2b":"code","b6bc3b7f":"code","86fb4564":"code","cb187514":"code","ed8b70c6":"code","f6d98bd9":"code","a2643612":"code","992385c7":"code","b458af1b":"code","c3ce5391":"code","25236d6c":"code","7438ea3a":"code","f247c0b0":"code","7decff39":"code","a474c78c":"code","71d22b2c":"code","99ab520b":"code","19c72667":"code","4125bb68":"code","b25130c6":"code","dd7e4439":"code","7732a2d5":"code","475b449f":"code","779d8bc0":"code","0cd12807":"code","a653d4b7":"code","c2a234b8":"code","2ffef5ac":"code","186621a5":"code","af4a1d94":"code","ed6b1186":"code","4c08da8b":"code","509a5f16":"code","6525ba2b":"code","6a7bbe55":"code","d1e69792":"code","f90aa384":"code","717a6d08":"code","378ef215":"markdown","fcc96035":"markdown","9effbf98":"markdown","f67c1b77":"markdown","e7b94657":"markdown","cb59a4ab":"markdown","7bc8147f":"markdown","2485d1b0":"markdown","4b82c560":"markdown","30423d1c":"markdown","bfc3b247":"markdown","79cec7c7":"markdown","529d0aa4":"markdown","b074303e":"markdown"},"source":{"e065e17c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","75dabb3e":"#Purpose: Importing Libraries\nimport seaborn as sn\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore, chi2_contingency\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_validate, learning_curve\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import RepeatedKFold\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.decomposition import PCA","058576c2":"#Purpose: Reading data in from Kaggle\ntraining = \"\/kaggle\/input\/\/tabular-playground-series-feb-2021\/train.csv\"\ntraining_df = pd.read_csv(training)\n\ntesting = \"\/kaggle\/input\/\/tabular-playground-series-feb-2021\/test.csv\"\ntesting_df = pd.read_csv(testing)","208d3065":"#Creating a dataframe for ID (to be used while making submissions)\ntesting_df_id = pd.DataFrame(testing_df['id'])","0bc5dc2b":"#QC: Displaying head\ntraining_df.head(10)","b6bc3b7f":"#Describe function\ntraining_df.describe()","86fb4564":"#Correlation Matrix \nfig, scatter = plt.subplots(figsize = (16,9))\ncorrMatrix = training_df.corr()\nscatter = sn.heatmap(corrMatrix, annot = True)\nscatter","cb187514":"#Purpose: Plotting a histogram to see the distribution\nfig = plt.figure(figsize = (15,20))\nax = fig.gca()\ntraining_df.hist(ax = ax, bins = 50)","ed8b70c6":"#Creating a dummy dataframe and removing id and target \ntraining_wo_id_target = training_df.drop(columns = [\"id\", \"target\"]) ","f6d98bd9":"#Purpose: Plotting a boxplot to see the outlier distribution\nfig = plt.figure(figsize = (15,5))\nax = fig.gca()\ntraining_wo_id_target.boxplot(ax = ax)","a2643612":"#Calculating the score\nnumeric_cols = training_wo_id_target.select_dtypes(include=[np.number]).columns\nscores = training_wo_id_target[numeric_cols].apply(zscore)  ","992385c7":"#Calculating the upper whisker outliers \nscore_max = pd.DataFrame(scores>3)\nfor col in score_max:\n    print(score_max[col].value_counts())","b458af1b":"#Calculating the lower whisker outliers \nscore_min = pd.DataFrame(scores<-3)\nfor col in score_min:\n    print(score_min[col].value_counts())","c3ce5391":"columns = ['cat0','cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9']\nfor column in columns:\n    print(column)\n    print(training_df[column].value_counts())\n    print(\"======\")","25236d6c":"#Splitting data into X(features) and Y(target variable) \ny = training_df['target']\nx = training_df.drop(['target','id'], axis = 1)","7438ea3a":"import pandas\nimport numpy\nfrom sklearn.preprocessing import OneHotEncoder \nfrom sklearn.compose import ColumnTransformer \n\ncolumns = ['cat0','cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9']\n\ncolumn_set = ColumnTransformer(transformers=[['oe',OrdinalEncoder(),columns]],remainder='passthrough')\nx[columns] = pd.DataFrame(column_set.fit_transform(x[columns]), dtype = numpy.str)","f247c0b0":"#converting the data type of all cols\nx = x.astype(float)\nx.dtypes","7decff39":"#defining the model\nmodel_one = XGBRegressor(tree_method='gpu_hist')\n\n# fitting the model \nmodel_one.fit(x, y)\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model_one, x, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n\n# report performance\nprint('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))","a474c78c":"model_one.predict(x)","71d22b2c":"testing_df_model_one = testing_df.drop(['id'], axis = 1)\ntesting_df_model_one","99ab520b":"#Encoding the categorical cols in the test data\ncolumns = ['cat0','cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9']\ncolumn_set = ColumnTransformer(transformers=[['oe',OrdinalEncoder(),columns]],remainder='passthrough')\ntesting_df_model_one[columns] = pd.DataFrame(column_set.fit_transform(testing_df_model_one[columns]), dtype = numpy.str)","19c72667":"#Changing the datatype of all cols on the test data to float\ntesting_df_model_one = testing_df_model_one.astype(float)","4125bb68":"testing_df_model_one = pd.DataFrame(model_one.predict(testing_df_model_one), columns = ['target'])\ntesting_df_model_one","b25130c6":"#combining the target output field with the ID for submssion\noutput_model_one = testing_df_id.join(testing_df_model_one)\noutput_model_one","dd7e4439":"output_model_one.to_csv(\"TBS_output_model_one.csv\", index = False)","7732a2d5":"# defining the model\nmodel_two = xgb.XGBRegressor(tree_method='gpu_hist',random_state=11,n_jobs=-1,max_depth=4,n_estimators=200,reg_lambda=100)\n\n#fitting the model\nmodel_two.fit(x, y)\n\n#evaluating the model\ncv = cross_validate(estimator=model_two,X=x,y=y,scoring='neg_mean_absolute_error',cv=5,n_jobs=-1,return_train_score=True)","475b449f":"cv","779d8bc0":"model_two.predict(x)","0cd12807":"testing_df_model_two = testing_df.drop(['id'], axis = 1)\ntesting_df_model_two","a653d4b7":"#Encoding the categorical cols in the test data\ncolumns = ['cat0','cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9']\ncolumn_set = ColumnTransformer(transformers=[['oe',OrdinalEncoder(),columns]],remainder='passthrough')\ntesting_df_model_two[columns] = pd.DataFrame(column_set.fit_transform(testing_df_model_two[columns]), dtype = numpy.str)","c2a234b8":"#Changing the datatype of all cols on the test data to float\ntesting_df_model_two = testing_df_model_two.astype(float)","2ffef5ac":"testing_df_model_two = pd.DataFrame(model_two.predict(testing_df_model_two), columns = ['target'])\ntesting_df_model_two","186621a5":"#combining the target output field with the ID for submssion\noutput_model_two = testing_df_id.join(testing_df_model_two)\noutput_model_two","af4a1d94":"output_model_two.to_csv(\"TBS_output_model_two.csv\", index = False)","ed6b1186":"#creating a df with all the continious fields\nx_cont = x.drop(columns, axis = 1)\nx_cont","4c08da8b":"#Implementing PCA\npca = PCA(n_components=1)\nprincipalComponents = pca.fit_transform(x_cont)","509a5f16":"principal_df = pd.DataFrame(data = principalComponents, columns = ['pc1'])","6525ba2b":"x_pc1  = x[columns].join(principal_df)","6a7bbe55":"x_pc1","d1e69792":"# fitting model_one of the whole dataset \nmodel_one.fit(x_pc1, y)\n\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model_one, x_pc1, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n\n# report performance\nprint('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))","f90aa384":"# fitting model_two of the whole dataset \nmodel_two.fit(x_pc1, y)\n\n#evaluating the model\ncv_pca = cross_validate(estimator=model_two,X=x_pc1,y=y,scoring='neg_mean_absolute_error',cv=5,n_jobs=-1,return_train_score=True)","717a6d08":"cv_pca","378ef215":"We see that quite a few of these fields follow normal distribution (only by the looks of it) and we can also detect some of these fields having outliers! \n\n**Lets explore the outliers.**","fcc96035":"We do observe in a few of these fields one or two categorical values are very high in number as compared to others (eg cat6 and cat4). \n","9effbf98":"## **Outlier Detection using Z Score!** \n\n\nA negative Z-score means an observation is below the mean, while a positive one means it above it. The further away from 0 the Z-Score is, the further away from the mean your observation is.\n\nOne way to identify outliers is to determine which points have a z-score that's far from 0.","f67c1b77":"### **EDA on the categorical fields**","e7b94657":"### Running XGB with different parameters as model_two","cb59a4ab":"## **Box Plots to detect outliers**","7bc8147f":"## PCA on the Data to create principal vectors","2485d1b0":"## Xtreme Gradient Boosting (XGB)","4b82c560":"### Encoding the categorical variables to input into the model","30423d1c":"### Running model_one on test data","bfc3b247":"**Findings: (pearson corr p value)**\n1. cont0 has a +ve corr with cont5(0.58) cont8(0.58) cont9(0.52)\n2. cont5 has a +ve corr with cont8(0.61) cont9(0.62) cont11(0.51) cont12(0.63)\n3. cont8 has a +ve corr with cont9(0.56) cont12(0.53)\n4. cont9 has a +ve corr with cont11(0.52) cont12(0.54)\n5. cont10 has a +ve corr with cont11(0.56)\n\n6. cont2 has a -ve corr with cont0 cont3 cont5 cont6 cont7 cont8 cont9 cont10 cont11 cont12 cont13","79cec7c7":"### Running model_two on test data","529d0aa4":"cont0, con2, cont6 and cont8 have outliers!","b074303e":"## Splitting the training data in to target and features"}}