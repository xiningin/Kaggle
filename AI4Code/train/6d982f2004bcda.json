{"cell_type":{"747136bb":"code","50aa17ae":"code","55704bde":"code","7ff4281e":"markdown","6a6dab3e":"markdown"},"source":{"747136bb":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom scipy.optimize import minimize","50aa17ae":"# Let's generate some labels and 3 prediction sets.\nL = (np.random.rand(1800, 206) > 0.5).astype(int)\np1 = np.random.rand(1800, 206)\np2 = np.random.rand(1800, 206)\np3 = np.random.rand(1800, 206)","55704bde":"# Helpers\ndef individual_scores(label, preds: list):\n    for i, pred in enumerate(preds):\n        print('LogLoss for p%d: %.5f'  % (i, tf.keras.losses.binary_crossentropy(L, pred).numpy().mean()))\n\ndef show_weights(result):\n    print('\\n')\n    for i, w in enumerate(result.x):\n        print('Weight_%d: %f' % (i, w))\n\ndef sanity_check():\n    # All probabilities have to be between 0 and 1.\n    if ((blend_func(res.x) > 0) & (blend_func(res.x) < 1)).all():\n        print('\\nAll probabilities are between 0 and 1. \\n    Good to go!')\n    else:\n        print('\\nProbabilities are not between 0 and 1! \\nS    Something is wrong!')\n        \n# Optimization\ndef objective_func(x):\n    newp = blend_func(x)\n    return tf.keras.losses.binary_crossentropy(L, newp).numpy().mean()\n\ndef blend_func(x):\n    return p1*x[0] + p2*x[1] + p3*x[2]\n\n\nindividual_scores(L, [p1, p2, p3])\ninit_guess = [0.1,0.5,0.4]  # Initial guesses for the weights\nbounds = tuple((0,1) for x in init_guess)  # All weights will be between 0 and 1!\ncons = ({'type': 'eq', 'fun': lambda x:  1-x[0]-x[1]-x[2]})  # Constraints - the sum of weights will be 0!\n\nres = minimize(objective_func,\n               init_guess,\n               constraints=cons,\n               bounds=[(0, 1)] * 3,\n               method='SLSQP',\n               options={'disp': True,\n                        'maxiter': 100000}) # Minimize!\nprint('\\nBlend LogLoss: %.5f' % (res.fun))\nshow_weights(res)\nsanity_check()","7ff4281e":"# Blending Multilabeled Models with Scipy\n\nThis notebook demonstrates how to blend 3 different models with scipy to have an improved score.\n\nGiven the average cross entropy for N samples and M targets,\n\n\\begin{equation*}\nBCE(y, p) = -\\frac{1}{M} \\sum\\limits_{m=1}^{M} \\frac{1}{N} \\sum\\limits_{i=1}^{N} \\left( y_{i,m} ln(p_{i,m}) + (1-y_{i,m}) ln(1-p_{i,m})\\right),\n\\end{equation*}\n\nthe blending is based on the minimization of the following objective function:\n\n\\begin{equation*}\nOBJ = BCE(y, w_1 p_1 + w_2 p_2 + w_3 p_3).\n\\end{equation*}\n\nwhere, p1, p2, and p3 is the probability predictions (e.g. OOF predictions) from 3 different models; w1, w2, w3 are the respective blending weights; and y denotes the multilabel targets.\n\nThe optimization is subject to the following conditions:\n* The sum of the weights are required to be 1: w1 + w2 + w3 = 1\n* Each weight can only have values between 0 and 1:  0 < w$_i$ <1\n\n\nThe code below can be easily modified to blend more models.\n\n**If you find this notebook useful, please don't forget to upvote!**","6a6dab3e":"The final predictions can be calculated using the weights estimated above. A psuedo code to do this would be the following:\n\n`Submission_result = Model_1.predict(x_test) * w1 + Model_2.predict(x_test) * w2 + Model_3.predict(x_test) * w3`\n"}}