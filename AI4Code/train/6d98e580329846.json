{"cell_type":{"6aaeb9ea":"code","45507f67":"code","5cc2b83b":"code","cb698cab":"code","37bc08ce":"code","8d5f5f13":"code","ffa573b4":"code","b234dd86":"code","f7eedefe":"code","56de3d91":"code","7448eeb0":"code","c726b92c":"code","f925e9fd":"code","86c376e5":"code","09bf0b65":"code","5676c84b":"code","732df9c3":"code","3fba8688":"code","e5b52dd5":"code","872d45c0":"code","c0dfd931":"code","4231b248":"code","eda1d613":"code","6f2a7a7c":"code","d042b258":"code","b307a065":"code","3b186f59":"code","5fa28880":"code","d9a3addb":"code","b7d03403":"code","44d37b33":"code","bdf2fe0c":"code","332a3f9f":"code","2ba5dbcf":"code","e1f7bab9":"code","b7553b61":"markdown","6739ba11":"markdown","16ac182f":"markdown","dddd32da":"markdown","8655a034":"markdown","df8dae26":"markdown","5b6d2def":"markdown","a88c2843":"markdown","a2e86023":"markdown","ac980d0a":"markdown","0fe44d46":"markdown","d4ddae14":"markdown","e19f9abb":"markdown","c5465b14":"markdown","7386fb2a":"markdown","f9e8d5c8":"markdown","4ad8f468":"markdown","4162fd2b":"markdown","6c5ede2e":"markdown","321dd0f4":"markdown","db3037cd":"markdown","684dba80":"markdown","e3576877":"markdown","e679fb7d":"markdown"},"source":{"6aaeb9ea":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns # Another data visualisation (prettier than matplotlib)\n\nimport math # We need this module to use mathematical function such as EXP()\n\nfrom sklearn.model_selection import train_test_split # Because we will split the training data into a training set and a validation set.\nfrom sklearn.metrics import confusion_matrix # Usefull at the end to see where the model makes mistakes\n\nfrom keras.utils.np_utils import to_categorical # Because we will convert lebels to one-hot-encoding\nfrom keras.models import Sequential # The type of keras model we'll be using\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, Activation # All the different layers that we need to build the model\nfrom keras.optimizers import RMSprop # Optimizer for the model\nfrom keras.preprocessing.image import ImageDataGenerator # This will help to make data augmentation and batches of images\nfrom tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping # To make a model a bit more sophisticated\n# Import to loas the previous callbacks from TensorFlow and not from keras !\n\nsns.set(palette='muted') # To build pretty graphs","45507f67":"!ls \/kaggle\/input\/ # The dataset is already attached to this notebook, we can just load the train and test sets form there","5cc2b83b":"# If you are not using a kaggle notebook with the MNIST attached to it:\n\n# from keras.datasets import mnist\n# (X_train, y_train), (X_test, y_test) = mnist.load_data()","cb698cab":"df_train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ndf_test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\n\nprint('train shape is: ' + str(df_train.shape))\nprint('test shape is: ' + str(df_test.shape))","37bc08ce":"y_train = df_train['label'] # y_train will contains the labels\nX_train = df_train.drop('label', axis = 1) # X_train contains the data to build the images","8d5f5f13":"print('shape of the training label dataset: ' + str(y_train.shape))\nprint('shape of the training image dataset: ' + str(X_train.shape))","ffa573b4":"X_train.loc[0].value_counts().head() # Looks like pixels values range from 0 to 255. We'll need to normalize these. Moreover, their type is *int64* we need to convert them to float.\n# On the fisrt picture we can see that 687 out of the 784 pixels are completely black\n# And that around 30 are very bright\n# The others should be due to some kind of halo effect where pixels close to the bright ones are a bit ligthen up","b234dd86":"X_train \/= 255.0\ndf_test \/= 255.0 # Test data should always be normalized the exact same way the training data have been normalized.","f7eedefe":"X_train = X_train.values.reshape(-1,28,28,1) # (nbr of samples, height, width, channel) Since these are not colored images, there's only one channel\ndf_test = df_test.values.reshape(-1,28,28,1)","56de3d91":"y_label = y_train # Just keeping a copy of the original labels before the one-hot-encoding\ny_train = to_categorical(y_train, num_classes = 10)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state=42) # Here we are creating the training set and the validation set","7448eeb0":"def decode(label): # Since we have encoded the labels,we need to decode it to access to the original number.\n    maxi = max(label)\n    for i in range(len(label)+1):\n        if label[i] == maxi:\n            return(i)\n    \nplt.figure()\nfor i in range(0,9):\n    plt.subplot(330 + 1 + i) # Create the 3x3 image grid\n    plt.axis('off')\n    plt.imshow(X_train[i][:,:,0], cmap='gray')\n    plt.title('number = ' + str(decode(y_train[i])))\nplt.show()","c726b92c":"datagen = ImageDataGenerator(rotation_range=10, # Rotating randomly the images up to 25\u00b0\n                             width_shift_range=0.05, # Moving the images from left to right\n                             height_shift_range=0.05, # Then from top to bottom\n                             shear_range=0.10, \n                             zoom_range=0.05, # Zooming randomly up to 20%\n                             zca_whitening=False,\n                             horizontal_flip=False, \n                             vertical_flip=False,\n                            fill_mode = 'nearest')\n\ndatagen.fit(X_train) # Very important to fit the Generator on the data\n\nfor X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n    for i in range(0,9):\n        plt.subplot(330 + 1 + i)\n        plt.axis('off')\n        plt.imshow(X_batch[i][:,:,0], cmap='gray')\n        plt.title('number = ' + str(decode(y_batch[i])))\n    break\n# Since we are now batching, we won't get the exact same images from the last exemple.","f925e9fd":"def scheduler(epoch, lr):\n    if epoch < 15: # For the first 10 epochs, the learning rate is not changed\n        return(lr)\n    elif 15 < epoch < 30: # After ten is decreases exponentially\n        return(lr*math.exp(-0.1))\n    else:\n        return(lr*math.exp(-0.2)) # And then decreases even more, still exponentially\n\nLRScheduler = LearningRateScheduler(scheduler)","86c376e5":"earlystopper = EarlyStopping(monitor='val_loss', min_delta =0, patience=6, verbose=1, mode='min',restore_best_weights=True) # If after 5 epochs (*patience=5*), the validation loss haven't decreased at all (*min_delta=0), the training stage is stopped","09bf0b65":"model = Sequential([\n    Conv2D(filters = 32, kernel_size = (5,5), activation ='relu', input_shape = (28 ,28 ,1)), # Important to specify the shape of the input data in the first layer.\n    Conv2D(filters = 32, kernel_size = (5,5), activation ='relu'), # The kernel_size is the grid that will stop at every possible location to extract a patch of surrounding features\n    BatchNormalization(),\n    Activation('relu'),\n    MaxPool2D(pool_size=(2,2)),\n    Dropout(0.05), # We are shunting down 25% of the nodes randomly\n    \n    Conv2D(filters = 64, kernel_size = (5,5), activation ='relu'), # Same as block 1 but with 64 nodes\n    Conv2D(filters = 64, kernel_size = (5,5), activation ='relu'),\n    BatchNormalization(),\n    Activation('relu'),\n    MaxPool2D(pool_size=(2,2)),\n    Dropout(0.05),\n    Flatten(), # Important to start using the 1D fully connected part of the layer\n    \n    Dense(256, activation='relu'), # Creating a layer with 256 nodes\n    BatchNormalization(),\n    Activation('relu'),\n    \n    Dense(128, activation='relu'),\n    BatchNormalization(),\n    Activation('relu'),\n    \n    Dense(84, activation='relu'),\n    BatchNormalization(),\n    Activation('relu'),\n    Dropout(0.05),\n    \n    Dense(10, activation='softmax') # We need to end the model with a Dense layer composed of 10 nodes (because 10 numbers from 0 to 9) and with a softmax activation to get a probability\n])\n\nmodel.summary()","5676c84b":"model.compile(optimizer =RMSprop(lr=1e-4) , loss = \"categorical_crossentropy\", metrics=[\"acc\"])","732df9c3":"# Hyperparameters:\n\nEPOCHS = 50 # Number of time the model will see the data\nBATCH_SIZE = 86 # Number of images per batch\nSTEPS_PER_EPOCH = X_train.shape[0] \/\/ BATCH_SIZE # Since we are batching, we need to specify when the model should consider that one epoch has been processed.\n# A proper way to fix this parameter is to have a number of step equal to the number of data\nCALLBACKS = [LRScheduler, earlystopper] # List of the previously defined callbacks\n\n# Fitting the model:\nhistory = model.fit_generator(datagen.flow(X_train,y_train, batch_size=BATCH_SIZE),\n                              epochs = EPOCHS, \n                              validation_data = (X_val,y_val),\n                              verbose = 2,\n                              steps_per_epoch=STEPS_PER_EPOCH,\n                              callbacks=CALLBACKS)","3fba8688":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show() ","e5b52dd5":"# Predict the values from the validation dataset\nY_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n\nsns.heatmap(confusion_mtx, annot=True, fmt='d', cmap='BuPu')\nplt.title('Confusion matrix MNIST')","872d45c0":"df_train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ndf_test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\n\ny_train = df_train['label'] # y_train will contains the labels\nX_train = df_train.drop('label', axis = 1) # X_train contains the data to build the images\n\nX_train \/= 255.0\ndf_test \/= 255.0\n\nX_train = X_train.values.reshape(-1,28,28,1) # (nbr of samples, height, width, channel) Since these are not colored images, there's only one channel\ndf_test = df_test.values.reshape(-1,28,28,1)\n\ny_train = to_categorical(y_train, num_classes = 10)\n\nwith strategy.scope():\n\n    model = Sequential([\n        Conv2D(filters = 32, kernel_size = (3,3), activation ='relu', input_shape = (28 ,28 ,1)), # Important to specify the shape of the input data in the first layer.\n        Conv2D(filters = 32, kernel_size = (3,3), activation ='relu'), # The kernel_size is the grid that will stop at every possible location to extract a patch of surrounding features\n        BatchNormalization(),\n        Activation('relu'),\n        MaxPool2D(pool_size=(2,2)),\n        Dropout(0.05), # We are shunting down 25% of the nodes randomly\n\n        Conv2D(filters = 64, kernel_size = (3,3), activation ='relu'), # Same as block 1 but with 64 nodes\n        Conv2D(filters = 64, kernel_size = (3,3), activation ='relu'),\n        BatchNormalization(),\n        Activation('relu'),\n        MaxPool2D(pool_size=(2,2)),\n        Dropout(0.05),\n        Flatten(), # Important to start using the 1D fully connected part of the layer\n\n        Dense(256, activation='relu'), # Creating a layer with 256 nodes\n        BatchNormalization(),\n        Activation('relu'),\n\n        Dense(128, activation='relu'),\n        BatchNormalization(),\n        Activation('relu'),\n\n        Dense(64, activation='relu'),\n        BatchNormalization(),\n        Activation('relu'),\n        Dropout(0.05),\n\n        Dense(10, activation='softmax') # We need to end the model with a Dense layer composed of 10 nodes (because 10 numbers from 0 to 9) and with a softmax activation to get a probability\n    ])\n\nmodel.compile(optimizer =RMSprop(lr=1e-4) , loss = \"categorical_crossentropy\", metrics=[\"acc\"])\n\n\n# Hyperparameters:\n\nEPOCHS = 50 # Number of time the model will see the data\nBATCH_SIZE = 86 # Number of images per batch\nSTEPS_PER_EPOCH = X_train.shape[0] \/\/ BATCH_SIZE # Since we are batching, we need to specify when the model should consider that one epoch has been processed.\n# A proper way to fix this parameter is to have a number of step equal to the number of data\nCALLBACKS = [LRScheduler, earlystopper] # List of the previously defined callbacks\n\n# Fitting the model:\nhistory = model.fit_generator(datagen.flow(X_train,y_train, batch_size=BATCH_SIZE),\n                              epochs = EPOCHS,\n                              verbose = 2,\n                              steps_per_epoch=STEPS_PER_EPOCH,\n                              callbacks=CALLBACKS)","c0dfd931":"# predict results\nresults = model.predict(df_test)\n\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"cnn_mnist_datagen.csv\",index=False)","4231b248":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns # Another data visualisation (prettier than matplotlib)\n\nimport math # We need this module to use mathematical function such as EXP()\n\nfrom sklearn.model_selection import train_test_split # Because we will split the training data into a training set and a validation set.\nfrom sklearn.metrics import confusion_matrix # Usefull at the end to see where the model makes mistakes\n\nfrom keras.utils.np_utils import to_categorical # Because we will convert lebels to one-hot-encoding\nfrom keras.models import Sequential # The type of keras model we'll be using\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, Activation # All the different layers that we need to build the model\nfrom keras.optimizers import RMSprop # Optimizer for the model\nfrom keras.preprocessing.image import ImageDataGenerator # This will help to make data augmentation and batches of images\nfrom tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping # To make a model a bit more sophisticated\n# Import to loas the previous callbacks from TensorFlow and not from keras !\n\nsns.set(palette='muted') # To build pretty graphs","eda1d613":"import tensorflow as tf\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\ndf_train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ndf_test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\n\ny_train = df_train['label'] # y_train will contains the labels\nX_train = df_train.drop('label', axis = 1) # X_train contains the data to build the images\n\nX_train \/= 255.0\ndf_test \/= 255.0\n\nX_train = X_train.values.reshape(-1,28,28,1) # (nbr of samples, height, width, channel) Since these are not colored images, there's only one channel\ndf_test = df_test.values.reshape(-1,28,28,1)\n\ny_train = to_categorical(y_train, num_classes = 10)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state=42) # Here we are creating the training set and the validation set\n\ndatagen = ImageDataGenerator(rotation_range=10, # Rotating randomly the images up to 25\u00b0\n                             width_shift_range=0.05, # Moving the images from left to right\n                             height_shift_range=0.05, # Then from top to bottom\n                             shear_range=0.10, \n                             zoom_range=0.05, # Zooming randomly up to 20%\n                             zca_whitening=False,\n                             horizontal_flip=False, \n                             vertical_flip=False,\n                            fill_mode = 'nearest')\n\ndatagen.fit(X_train) # Very important to fit the Generator on the data\n\ndropout_list = [0.06,0.10,0.15,0.20,0.25,0.30,0.35]\ndict_acc = {}\ndict_val_acc = {}\ndict_loss = {}\ndict_val_loss = {}\n\ndef scheduler(epoch, lr):\n    if epoch < 15: # For the first 10 epochs, the learning rate is not changed\n        return(lr)\n    elif 15 < epoch < 30: # After ten is decreases exponentially\n        return(lr*math.exp(-0.1))\n    else:\n        return(lr*math.exp(-0.2)) # And then decreases even more, still exponentially\n\nLRScheduler = LearningRateScheduler(scheduler)\n\nearlystopper = EarlyStopping(monitor='val_loss', min_delta =0, patience=6, verbose=1, mode='min', restore_best_weights=True) # If after 5 epochs (*patience=5*), the validation loss haven't decreased at all (*min_delta=0), the training stage is stopped\n\ndef create_model(dp):\n    with strategy.scope():\n        \n        model = Sequential([\n            Conv2D(filters = 32, kernel_size = (3,3), activation ='relu', input_shape = (28 ,28 ,1)), # Important to specify the shape of the input data in the first layer.\n            Conv2D(filters = 32, kernel_size = (3,3), activation ='relu'), # The kernel_size is the grid that will stop at every possible location to extract a patch of surrounding features\n            BatchNormalization(),\n            Activation('relu'),\n            MaxPool2D(pool_size=(2,2)),\n            Dropout(dp), # We are shunting down 25% of the nodes randomly\n\n            Conv2D(filters = 64, kernel_size = (3,3), activation ='relu'), # Same as block 1 but with 64 nodes\n            Conv2D(filters = 64, kernel_size = (3,3), activation ='relu'),\n            BatchNormalization(),\n            Activation('relu'),\n            MaxPool2D(pool_size=(2,2)),\n            Dropout(dp),\n            Flatten(), # Important to start using the 1D fully connected part of the layer\n\n            Dense(256, activation='relu'), # Creating a layer with 256 nodes\n            BatchNormalization(),\n            Activation('relu'),\n\n            Dense(128, activation='relu'),\n            BatchNormalization(),\n            Activation('relu'),\n\n            Dense(64, activation='relu'),\n            BatchNormalization(),\n            Activation('relu'),\n            Dropout(dp),\n\n            Dense(10, activation='softmax') # We need to end the model with a Dense layer composed of 10 nodes (because 10 numbers from 0 to 9) and with a softmax activation to get a probability\n        ])\n\n    model.compile(optimizer =RMSprop(lr=1e-4) , loss = \"categorical_crossentropy\", metrics=[\"acc\"])\n    \n    return(model)\n\nEPOCHS = 50 # Number of time the model will see the data\nBATCH_SIZE = 86 # Number of images per batch\nSTEPS_PER_EPOCH = X_train.shape[0] \/\/ BATCH_SIZE # Since we are batching, we need to specify when the model should consider that one epoch has been processed.\n# A proper way to fix this parameter is to have a number of step equal to the number of data\nCALLBACKS = [LRScheduler, earlystopper] # List of the previously defined callbacks\n\nfor dp in dropout_list:\n\n    # Fitting the model:\n    history = create_model(dp).fit_generator(datagen.flow(X_train,y_train, batch_size=BATCH_SIZE),\n                              validation_data = (X_val,y_val),\n                              epochs = EPOCHS,\n                              verbose = 2,\n                              steps_per_epoch=STEPS_PER_EPOCH,\n                              callbacks=CALLBACKS)\n\n    dict_acc['acc_for_dp_'+ str(dp)] = history.history['acc']\n    dict_val_acc['val_acc_for_dp_'+ str(dp)] = history.history['val_acc']\n    dict_loss['loss_for_dp_'+ str(dp)] = history.history['loss']\n    dict_val_loss['val_loss_for_dp_'+ str(dp)] = history.history['val_loss']","6f2a7a7c":"for key_acc, key_val_acc in zip(dict_acc.keys(), dict_val_acc.keys()):\n    plt.figure()\n    epochs = range(1, len(dict_acc[key_acc])-2)\n    plt.plot(epochs, dict_acc[key_acc][3:], 'bo', label='Training acc')\n    plt.plot(epochs, dict_val_acc[key_val_acc][3:],'b', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.show()","d042b258":"plt.figure()\nepochs = range(1, 16)\nfor key_val_acc in dict_val_acc.keys():\n    plt.plot(epochs, dict_val_acc[key_val_acc][len(dict_val_acc[key_val_acc])-15:], label=key_val_acc)\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.show()","b307a065":"import tensorflow as tf\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\ndf_train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ndf_test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\n\ny_train = df_train['label'] # y_train will contains the labels\nX_train = df_train.drop('label', axis = 1) # X_train contains the data to build the images\n\nX_train \/= 255.0\ndf_test \/= 255.0\n\nX_train = X_train.values.reshape(-1,28,28,1) # (nbr of samples, height, width, channel) Since these are not colored images, there's only one channel\ndf_test = df_test.values.reshape(-1,28,28,1)\n\ny_train = to_categorical(y_train, num_classes = 10)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state=42) # Here we are creating the training set and the validation set\n\nrot_list = [0,5,10,15,20,25,30,35,40,45]\n\ndef create_datagen(rot):\n    datagen = ImageDataGenerator(rotation_range=rot, # Rotating randomly the images up to 25\u00b0\n                             width_shift_range=0.05, # Moving the images from left to right\n                             height_shift_range=0.05, # Then from top to bottom\n                             shear_range=0.10, \n                             zoom_range=0.05, # Zooming randomly up to 20%\n                             zca_whitening=False,\n                             horizontal_flip=False, \n                             vertical_flip=False,\n                            fill_mode = 'nearest')\n    return(datagen)\n\n\n\ndropout_list = 0.15\ndict_acc = {}\ndict_val_acc = {}\ndict_loss = {}\ndict_val_loss = {}\n\ndef scheduler(epoch, lr):\n    if epoch < 15: # For the first 10 epochs, the learning rate is not changed\n        return(lr)\n    elif 15 < epoch < 30: # After ten is decreases exponentially\n        return(lr*math.exp(-0.1))\n    else:\n        return(lr*math.exp(-0.2)) # And then decreases even more, still exponentially\n\nLRScheduler = LearningRateScheduler(scheduler)\n\nearlystopper = EarlyStopping(monitor='val_loss', min_delta =0, patience=8, verbose=1, mode='min', restore_best_weights=True) # If after 5 epochs (*patience=5*), the validation loss haven't decreased at all (*min_delta=0), the training stage is stopped\n\ndef create_model(rot):\n    \n    datagen=create_datagen(rot)\n    datagen.fit(X_train) # Very important to fit the Generator on the data\n    \n    with strategy.scope():\n        \n        model = Sequential([\n            Conv2D(filters = 32, kernel_size = (3,3), activation ='relu', input_shape = (28 ,28 ,1)), # Important to specify the shape of the input data in the first layer.\n            Conv2D(filters = 32, kernel_size = (3,3), activation ='relu'), # The kernel_size is the grid that will stop at every possible location to extract a patch of surrounding features\n            BatchNormalization(),\n            Activation('relu'),\n            MaxPool2D(pool_size=(2,2)),\n            Dropout(0.15), # We are shunting down 25% of the nodes randomly\n\n            Conv2D(filters = 64, kernel_size = (3,3), activation ='relu'), # Same as block 1 but with 64 nodes\n            Conv2D(filters = 64, kernel_size = (3,3), activation ='relu'),\n            BatchNormalization(),\n            Activation('relu'),\n            MaxPool2D(pool_size=(2,2)),\n            Dropout(0.15),\n            Flatten(), # Important to start using the 1D fully connected part of the layer\n\n            Dense(256, activation='relu'), # Creating a layer with 256 nodes\n            BatchNormalization(),\n            Activation('relu'),\n\n            Dense(128, activation='relu'),\n            BatchNormalization(),\n            Activation('relu'),\n\n            Dense(64, activation='relu'),\n            BatchNormalization(),\n            Activation('relu'),\n            Dropout(0.15),\n\n            Dense(10, activation='softmax') # We need to end the model with a Dense layer composed of 10 nodes (because 10 numbers from 0 to 9) and with a softmax activation to get a probability\n        ])\n\n    model.compile(optimizer =RMSprop(lr=1e-4) , loss = \"categorical_crossentropy\", metrics=[\"acc\"])\n    \n    return(model)\n\nEPOCHS = 50 # Number of time the model will see the data\nBATCH_SIZE = 86 # Number of images per batch\nSTEPS_PER_EPOCH = X_train.shape[0] \/\/ BATCH_SIZE # Since we are batching, we need to specify when the model should consider that one epoch has been processed.\n# A proper way to fix this parameter is to have a number of step equal to the number of data\nCALLBACKS = [LRScheduler, earlystopper] # List of the previously defined callbacks\n\nfor rot in rot_list:\n\n    # Fitting the model:\n    history = create_model(rot).fit_generator(datagen.flow(X_train,y_train, batch_size=BATCH_SIZE),\n                              validation_data = (X_val,y_val),\n                              epochs = EPOCHS,\n                              verbose = 2,\n                              steps_per_epoch=STEPS_PER_EPOCH,\n                              callbacks=CALLBACKS)\n\n    dict_acc['acc_for_rot_'+ str(rot)] = history.history['acc']\n    dict_val_acc['val_acc_for_rot_'+ str(rot)] = history.history['val_acc']\n    dict_loss['loss_for_rot_'+ str(rot)] = history.history['loss']\n    dict_val_loss['val_loss_for_rot_'+ str(rot)] = history.history['val_loss']","3b186f59":"plt.figure()\nepochs = range(1, 16)\nfor key_val_acc in dict_val_acc.keys():\n    plt.plot(epochs, dict_val_acc[key_val_acc][len(dict_val_acc[key_val_acc])-15:], label=key_val_acc)\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.show()","5fa28880":"import tensorflow as tf\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","d9a3addb":"df_train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ndf_test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\n\ndf_train.sample(frac=1)\n\ny_train = df_train.loc[:df_train.shape[0]\/\/10,['label']] # y_train will contains the labels\nX_train = df_train.loc[:df_train.shape[0]\/\/10].drop('label', axis = 1) # X_train contains the data to build the images\ny_test = df_train.loc[df_train.shape[0]\/\/10:,['label']]\nX_test = df_train.loc[df_train.shape[0]\/\/10:].drop('label', axis = 1)","b7d03403":"X_train \/= 255.0\nX_test \/= 255.0\ndf_test \/= 255.0\n\nX_train = X_train.values.reshape(-1,28,28,1) # (nbr of samples, height, width, channel) Since these are not colored images, there's only one channel\ndf_test = df_test.values.reshape(-1,28,28,1)\nX_test = X_test.values.reshape(-1,28,28,1)\n\ny_label = y_train\ny_train = to_categorical(y_train, num_classes = 10)\ny_test = to_categorical(y_test, num_classes = 10)","44d37b33":"unique, counts = np.unique(y_label, return_counts=True)\nD = dict(zip(unique, counts))\n\nplt.bar(range(len(D)), list(D.values()), align='center')\n#plt.xticks(range(len(D)), list(D.keys()))\nplt.legend()","bdf2fe0c":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state=42) # Here we are creating the training set and the validation set","332a3f9f":"y_val_label = np.argmax(y_val,axis = 1)\ny_train_label = np.argmax(y_train,axis = 1)","2ba5dbcf":"unique, counts = np.unique(y_val_label, return_counts=True)\ndic_val = dict(zip(unique, counts))\n\nunique, counts = np.unique(y_train_label, return_counts=True)\ndic_train = dict(zip(unique, counts))\n\nplt.figure()\nplt.bar(np.arange(len(dic_val))-.2, list(dic_val.values()), align='center', width=0.4)\nplt.bar(np.arange(len(dic_train))+.2, list(dic_train.values()), align='center', width=0.4)\n#plt.xticks(range(len(D)), list(D.keys()))\nplt.legend()","e1f7bab9":"rot_list = [0.5,10,20]\nw_list = [0,0.05,0.1]\nh_list = [0,0.05,0.1]\nsh_list = [0,0.1,0.2]\nzoom_list = [0,0.05,0.1]\n\ndef create_datagen(rot,w,h,sh,zoom):\n    datagen = ImageDataGenerator(rotation_range=rot, # Rotating randomly the images up to 25\u00b0\n                             width_shift_range=w, # Moving the images from left to right\n                             height_shift_range=w, # Then from top to bottom\n                             shear_range=sh, \n                             zoom_range=zoom, # Zooming randomly up to 20%\n                             zca_whitening=False,\n                             horizontal_flip=False, \n                             vertical_flip=False,\n                            fill_mode = 'nearest')\n    return(datagen)\n\ndict_acc = {}\ndict_val_acc = {}\ndict_loss = {}\ndict_val_loss = {}\ndict_test_acc = {}\nk = 3\nnum_val_samples = len(X_train) \/\/ k\n\ndf = pd.DataFrame(columns=['rotation', 'width', 'height', 'shear', 'zoom', 'val_acc', 'test_loss', 'test_acc'])\n\ndef scheduler(epoch, lr):\n    if epoch < 10: # For the first 10 epochs, the learning rate is not changed\n        return(lr)\n    elif 10 < epoch < 20: # After ten is decreases exponentially\n        return(lr\/2)\n    else:\n        return(lr*math.exp(-0.1)) # And then decreases even more, still exponentially\n\nLRScheduler = LearningRateScheduler(scheduler)\n\nearlystopper = EarlyStopping(monitor='val_loss', min_delta =0, patience=10, verbose=1, mode='min', restore_best_weights=False) # If after 5 epochs (*patience=5*), the validation loss haven't decreased at all (*min_delta=0), the training stage is stopped\n\ndef create_model(rot,w,h,sh,zoom):\n    \n    datagen=create_datagen(rot,w,h,sh,zoom)\n    datagen.fit(X_train) # Very important to fit the Generator on the data\n    \n    with strategy.scope():\n        \n        model = Sequential([\n            Conv2D(filters = 32, kernel_size = (3,3), activation ='relu', input_shape = (28 ,28 ,1)), # Important to specify the shape of the input data in the first layer.\n            Conv2D(filters = 32, kernel_size = (3,3), activation ='relu'), # The kernel_size is the grid that will stop at every possible location to extract a patch of surrounding features\n            BatchNormalization(),\n            Activation('relu'),\n            MaxPool2D(pool_size=(2,2)),\n            Dropout(0.15), # We are shunting down 25% of the nodes randomly\n\n            Conv2D(filters = 64, kernel_size = (3,3), activation ='relu'), # Same as block 1 but with 64 nodes\n            Conv2D(filters = 64, kernel_size = (3,3), activation ='relu'),\n            BatchNormalization(),\n            Activation('relu'),\n            MaxPool2D(pool_size=(2,2)),\n            Dropout(0.15),\n            Flatten(), # Important to start using the 1D fully connected part of the layer\n\n            Dense(256, activation='relu'), # Creating a layer with 256 nodes\n            BatchNormalization(),\n            Activation('relu'),\n\n            Dense(128, activation='relu'),\n            BatchNormalization(),\n            Activation('relu'),\n\n            Dense(64, activation='relu'),\n            BatchNormalization(),\n            Activation('relu'),\n            Dropout(0.15),\n\n            Dense(10, activation='softmax') # We need to end the model with a Dense layer composed of 10 nodes (because 10 numbers from 0 to 9) and with a softmax activation to get a probability\n        ])\n\n    model.compile(optimizer = RMSprop(lr=1e-3) , loss = \"categorical_crossentropy\", metrics=[\"acc\"])\n    \n    return(model)\n\nEPOCHS = 50 # Number of time the model will see the data\nBATCH_SIZE = 86 # Number of images per batch\nSTEPS_PER_EPOCH = X_train.shape[0] \/\/ BATCH_SIZE # Since we are batching, we need to specify when the model should consider that one epoch has been processed.\n# A proper way to fix this parameter is to have a number of step equal to the number of data\nCALLBACKS = [LRScheduler, earlystopper] # List of the previously defined callbacks\n\nfor rot in rot_list:\n    for w in w_list:\n        for sh in sh_list:\n            for zoom in zoom_list:\n                for h in h_list:\n                    all_scores = []\n                    for i in range(k):\n                        val_data = X_train[i*num_val_samples:(i+1)*num_val_samples]\n                        val_targets = y_train[i*num_val_samples:(i+1)*num_val_samples]\n\n                        partial_train_data = np.concatenate(\n                        [X_train[:i*num_val_samples],\n                        X_train[(i+1)*num_val_samples:]],\n                        axis=0)\n\n                        partial_train_targets = np.concatenate(\n                        [y_train[:i*num_val_samples],\n                        y_train[(i+1)*num_val_samples:]],\n                        axis=0)\n\n                        model = create_model(rot,w,h,sh,zoom)\n                        model.fit_generator(datagen.flow(X_train,y_train, batch_size=BATCH_SIZE),\n                                                                  validation_data = (X_val,y_val),\n                                                                  epochs = EPOCHS,\n                                                                  verbose = 0,\n                                                                  steps_per_epoch=STEPS_PER_EPOCH,\n                                                                  callbacks=CALLBACKS)\n                        val_mse, val_mae = model.evaluate(X_val, y_val, verbose = 0)\n                        all_scores.append(val_mae)\n                    val_score = np.mean(all_scores)\n                    test_loss, test_score = model.evaluate(X_test, y_test)\n                    \n                    df = df.append({'rotation': rot, 'width':w, 'height':h, 'shear':sh, 'zoom':zoom, 'val_acc':val_score, 'test_loss':test_loss, 'test_acc':test_score}, ignore_index=True)\n                    \n                    print('acc_'+ str(rot) + ' width_' + str(w)+ ' height_' + str(h)+ ' shear_' + str(sh)+ ' zoom_' + str(zoom) + str(test_score))","b7553b61":"Thanks to *history* we can access some very usefull information on how the model learned through all the epochs and display the very usefull validation curves and loss curves.","6739ba11":"## 2.1 Load the dataset","16ac182f":"# 5. Running the model","dddd32da":"## 4.2 CNN\nThis is where we start building the model. Many different layers have been used here to reach a high accuracy but it is possible to reach very good accuracy (98%) with less. Let's cover quicly what these layer do:\n- Conv2D: This is a very powerful layer that is able to learn local patterns (edges, circles etc...) and that is translation invariant. It opperates as a filter to detect those patterns.\n- MaxPooling2D: This layer is used to make downsampling. The aim is to reduce the number of feature-map coefficient to process while applying spatial-filter hierarchies via a *max* tensor operator\n- BatchNormalization: This is a way to normalize the activation (the output) of the previous layers\n- Activation: Just an activation function layer\n- Dropout: This is another way to mitigate overfitting. The aim of this layer is to randomly shut down a speficied proportion of nodes.\n- Flatten: As the name says, this layer is used to go from 2D activations to 1D. This is mandatory to use Dense layers.\n- Dense: Basic fully connected layer.\n\nThis model in first structured with a stack of Conv2D and MaxPool2D layers and then there is a densely connected part","8655a034":"# 1. Import the relevant libraries\nHere we load everything we need to use later","df8dae26":"# 2. Access the dataset\nBecause this notebook is created for a kaggle competition, the dataset is already attached to it","5b6d2def":"## 2.4 Reshaping the data\nThis is where we will create the images from the previous 784 long arrays and encode the labels","a88c2843":"# 3. Data visualisation\nOne important part of this notebook is dedicated to the technique of *Data augmentation*. This is one of the best and easiest way to mitigate overfitting. In this section it is proposed to see the impact of *data augmentation* on the images.\n## 3.1 Without augmentation\nLet's have a look at those pictures without any transformation:","a2e86023":"## 3.2 With augmentation\nThanks to Keras *ImageDataGenerator* classe, data augmentation is made very easy. It takes a bunch of arguments that changes the images randomly in the specified range. Some where used here, for instance to rotate a bit the images, to shift them from left to right, up and down, to zoom etc... Another important function of the *ImageDataGenerator* is that it allows to create batch of images. The previous modifications will affect only some pictures and will be fed immediatly to the model. This is a way to save a lot of memory !","ac980d0a":"les valeurs sont bien r\u00e9parties.","0fe44d46":"Thanks to the confusion matrix it was possible to adjust the data augmentation parameter to maximize the validation accuracy. For instance the model was missclassifying a lot of 4 as 9 because images where shifted to much to the top.","d4ddae14":"## 6.2 Confusion matrix\nThis will help us to understand where the model is making mistakes.","e19f9abb":"### We take 10% of the original dataset","c5465b14":"# 4. Build the model\n## 4.1 Creating the callbacks\n*From Keras website* \"These are object that can perform actions at various stages of training (at the start or end of an epoch, before or after a batch etc.)\" There are not mandatory but they will help to create a more complete model. Here we'll create one that decreases the learning rate after a specific number of epochs to have a very accurate learning and one to stop the training session if the model stops improving\n### 4.1.1 LearningRateScheduler","7386fb2a":"# Data Augmentation on the MNIST Dataset\nThis Dataset was made to present how to simply build a very effective convolutionnal network for the MNIST digit classification problem. It adresses these particular points:\n- Data visualisation\n- how to make data augmentation\n- Building a convolutional model from scartch\n- Evaluating a model\n\nThis will be the structuration of the following notebook:\n- 1. Import the relevant libraries\n- 2. Access the data\n    - 2.1. Load the Dataset\n    - 2.2 Data exploration\n    - 2.3 Data normalization\n    - 2.4 Reshaping the data\n- 3. Data Visualisation\n    - 3.1 Without data augmentation\n    - 3.2 With data augmentation\n- 4. Build the model\n    - 4.1 Creating the callbacks\n        - 4.1.1 LearningRateScheduler\n        - 4.1.2 EarlyStopping\n    - 4.2 CNN\n- 5. Running the model\n- 6. Evaluation\n    - 6.1 Accuracy and loss through epochs\n    - 6.2 Confusion matrix\n- 7. Submission\n\nWhile reading this, please keep in mind that I am not an expert myself. I am just trying to share what I learned from other kaggle competition notebooks, comments, and external sources.","f9e8d5c8":"### 4.1.2 EarlyStopping","4ad8f468":"## 2.3 Data normalization\nIt's important, before feeding data to the model, to have them in the same range. Having heterogeneous data would lead to make the learning stage harder. Here we'll just divide each pixels value by 255.0 (the *.0* is important to create float numbers). However, one very common way to normalize data is to standardize it by substracting the mean of the feature and divide by the standard deviation.","4162fd2b":"This is no surprise that the model performs better on the validation set since we are unsing data augmentation technique. The model trains on more difficult data than the one we use to evaluate it.","6c5ede2e":"We can clearly identify the images to the corresponding numbers. However, without modifying it, the model will only see the same pictures again and gain. As a result, it will be very efficient classifying these images but will do poorly on new images since it lost its ability to generalize (this is overfitting). By creating artificially new images from the original ones, we are challenging the model to learn in a more general way. It will take more time for the model to learn but it will be way more accurate on classifying new images.","321dd0f4":"For now we just have created two huge datasets but we need to set apart the images and their labels for the train set. One can also notice that for now, datasets are not composed of images but of arrays of 784 pixels.","db3037cd":"It still looks pretty balanced","684dba80":"The train dataset is composed of 42000 samples and 785 rows. Each row corresponds to one pixel of the 28x28 image execpt the firt column that is dedicated to the label. The test dataset is composed of 784 rows because we can't access the label since it is for submission.\nLet's normalize these dataset and reshape them.","e3576877":"## 2.2 Data Exploration","e679fb7d":"# 6. Evaluation\n## 6.1 Accuracy and loss through epochs"}}