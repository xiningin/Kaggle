{"cell_type":{"9d02dd9c":"code","0e606bbf":"code","21b848da":"code","bf51636a":"code","09e9c97e":"code","16756f4f":"code","ddb5c351":"code","b1c2d6ba":"code","eb3a9604":"code","de967a45":"code","9345d4d9":"code","68e7f8ac":"code","fb489d8a":"code","4527b05c":"code","8b5406aa":"code","2d8afc73":"code","a59fe4a8":"code","92e4b958":"code","ce0bb5aa":"code","4a885c33":"markdown","1f60fe0f":"markdown","c68eb976":"markdown","d09c6396":"markdown","ecb460a8":"markdown","0b8e392c":"markdown","f4926d72":"markdown","583f5f82":"markdown","aa9746cd":"markdown","47f9e734":"markdown","26513cfb":"markdown","63e2cfcd":"markdown"},"source":{"9d02dd9c":"import os\nimport numpy as np \nimport pandas as pd\nimport string\nfrom collections import Counter\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(12,9)})\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Input, Embedding, Dropout, Activation, BatchNormalization, CuDNNLSTM, concatenate, Bidirectional,\\\nTimeDistributed\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport re\n\n!pip install num2words\nfrom num2words import num2words\n\nSEQUENCE_LENGTH = 35\nEMBEDDING_DIM = 200","0e606bbf":"train = pd.read_csv('..\/input\/train.csv')\n#Removing the rows that have null values\ntrain = train.dropna(how='any',axis=0) ","21b848da":"misspelled_words = {\"automattic\": \"automatic\", \"sweetpotato\": \"sweet potato\", \"statuscode\": \"status code\", \"applylayer\": \"apply layer\", \"aligator\": \"alligator\", \"downloands\": \"download\", \"dowloand\": \"download\", \"thougths\": \"thoughts\", \"helecopter\": \"helicopter\", \"telugul\": \"telugu\", \"unconditionaly\": \"unconditionally\", \"coompanies\": \"companies\", \"lndigenous\": \"indigenous\", \"evluate\": \"evaluate\", \"suggstion\": \"suggestion\", \"thinkning\": \"thinking\", \"concatinate\": \"concatenate\", \"constitutionals\": \"constitutional\", \"moneyback\": \"money back\", \"civilazation\": \"civilization\", \"paranoria\": \"paranoia\", \"rightside\": \"right side\", \"methamatics\": \"mathematics\", \"natual\": \"natural\", \"brodcast\": \"broadcast\", \"pleasesuggest\": \"please suggest\", \"intitution\": \"institution\", \"experinces\": \"experiences\", \"reallyreally\": \"really\", \"testostreone\": \"testosterone\", \"musceles\": \"muscle\", \"bacause\": \"because\", \"peradox\": \"paradox\", \"probabity\": \"probability\", \"collges\": \"college\", \"diciplined\": \"disciplined\", \"completeted\": \"completed\", \"lunchshould\": \"lunch should\", \"battlenet\": \"battle net\", \"dissapoint\": \"disappoint\", \"resultsnew\": \"results new\", \"indcidents\": \"incidents\", \"figuire\": \"figure\", \"protonneutron\": \"proton neutron\", \"tecnical\": \"technical\", \"patern\": \"pattern\", \"unenroll\": \"un enroll\", \"proceedures\": \"procedures\", \"srategy\": \"strategy\", \"mordern\": \"modern\", \"prepartion\": \"preparation\", \"throuhout\": \"throught\", \"academey\": \"academic\", \"instituitions\": \"institutions\", \"abadon\": \"abandon\",\"compitetive\": \"competitive\", \"hypercondriac\": \"hypochondriac\", \"spiliting\": \"splitting\", \"physchic\": \"psychic\", \"flippingly\": \"flipping\", \"likelyhood\": \"likelihood\", \"armsindustry\": \"arms industry\", \" turorials\": \"tutorials\", \"photostats\": \"photostat\", \"sunconcious\": \"subconscious\", \"chemistryphysics\": \"chemistry physics\", \"secondlife\": \"second life\", \"histrorical\": \"historical\", \"disordes\": \"disorders\", \"differenturl\": \"differential\", \"councilling\": \" counselling\", \"sugarmill\": \"sugar mill\", \"relatiosnhip\": \"relationship\", \"fanpages\": \"fan pages\", \"agregator\": \"aggregator\", \"switc\": \"switch\", \"smatphones\": \"smartphones\", \"headsize\": \"head size\", \"pendrives\": \"pen drives\", \"biotecnology\": \"biotechnology\", \"borderlink\": \"border link\", \"furnance\": \"furnace\", \"competetion\": \"competition\", \"distibution\": \"distribution\", \"ananlysis\": \" analysis\", \"textile\uff1f\": \"textile\", \"howww\": \"how\", \"strategybusiness\": \"strategy business\", \"spectrun\": \"spectrum\", \"propasal\": \"proposal\", \"appilcable\": \"applicable\", \"accountwhat\": \" account what\", \"algorithems\": \" algorithms\", \"protuguese\": \" Portuguese\", \"exatly\": \"exactly\", \"disturbence\": \"disturbance\", \"govrnment\": \"government\", \"requiremnt\": \"requirement\", \"vargin\": \"virgin\", \"lonleley\": \"lonely\", \"unmateralistic\": \"materialistic\", \"dveloper\": \"developer\", \"dcuments\": \"documents\", \"techonologies\": \"technologies\", \"morining\": \"morning\", \"samsing\": \"Samsung\", \"engeeniring\": \"engineering\", \"racetrac\": \"racetrack\", \"physian\": \"physician\", \"theretell\": \"there tell\", \"tryto\": \"try to\", \"teamfight\": \"team fight\", \"recomend\": \"recommend\", \"spectables\": \"spectacles\", \"emtional\": \"emotional\", \"engeenerring\": \"engineering\", \"optionsgood\": \"options good\", \"primarykey\": \"primary key\", \"foreignkey\": \"foreign key\", \"concieved\": \"conceived\", \"leastexpensive\": \"least expensive\", \"foodtech\": \"food tech\", \"electronegetivity\": \"electronegativity\", \"polticians\": \"politicians\", \"distruptive\": \"disruptive\", \"currrent\": \"current\", \"hidraulogy\": \"hydrology\", \"californa\": \"California\", \"electrrical\": \"electrical\", \"navigationally\": \"navigation\", \"whwhat\": \"what\", \"bcos\": \"because\", \"vaccancies\": \"vacancies\", \"articels\": \"articles\", \"boilng\": \"boiling\", \"hyperintensity\": \"hyper intensity\", \"rascism\": \"racism\", \"messenging\": \"messaging\", \"cleaniness\": \"cleanliness\", \"vetenary\": \"veterinary\", \"investorswhat\": \"investors what\", \"chrestianity\": \"Christianity\", \"apporval\": \"approval\", \"repaire\": \"repair\", \"biggerchance\": \"bigger chance\", \"manufacturering\": \"manufacturing\", \"buildertrend\": \"builder trend\", \"allocatively\": \"allocative\", \"subliminals\": \"subliminal\", \"mechnically\": \"mechanically\", \"binaurial\": \"binaural\", \"naaked\": \"naked\", \"aantidepressant\": \"antidepressant\", \"geunine\": \"genuine\", \"quantitaive\": \"quantitative\", \"paticipated\": \"participated\", \"repliedjesus\": \"replied Jesus\", \"baised\": \"biased\",\"worldreport\": \"world report\", \"eecutives\": \"executives\", \"paitents\": \"patients\", \"telgu\": \"Telugu\", \"nomeniculature\": \"nomenclature\", \"crimimaly\": \"criminally\", \"resourse\": \"resource\", \"procurenent\": \"procurement\", \"improvemet\": \"improvement\", \"metamers\": \"metamer\", \"tautomers\": \"tautomer\", \"knowwhen\": \"know when\",\"whatdoes\": \"what does\", \"pletelets\": \"platelets\", \"pssesive\": \"possessive\", \"oxigen\": \"oxygen\", \"ethniticy\": \"ethnicity\", \"situatiation\": \"situation\", \"ecoplanet\": \"eco planet\", \"situatio\": \"situation\", \"dateing\": \"dating\", \"hostress\": \"hostess\", \"initialisation\": \"initialization\", \"hydrabd\": \"Hyderabad\", \"deppresed\": \"depressed\", \"dwnloadng\": \"downloading\", \"expirey\": \"expiry\", \"engeenering\": \"engineering\", \"hyderebad\": \"Hyderabad\", \"automatabl\": \"automatable\", \"architetureocasions\": \"architectureoccasions\", \"restaraunts\": \"restaurants\", \"recommedations\": \"recommendations\", \"intergrity\": \"integrity\", \"reletively\": \"relatively\", \"priceworthy\": \"price worthy\", \"princples\": \"principles\", \"reconigze\": \"recognize\", \"paticular\": \"particular\", \"musictheory\": \"music theory\", \"requied\": \"required\", \"netural\": \"natural\", \"fluoresent\": \"fluorescent\", \"girlfiend\": \"girlfriend\", \"develpment\": \"development\", \"eridicate\": \"eradicate\", \"techologys\": \"technologies\", \"hybridyzation\": \"hybridization\", \"ideaa\": \"ideas\", \"tchnology\": \"technology\", \"appropiate\": \"appropriate\", \"respone\": \"response\", \"celebreties\": \"celebrities\", \"exterion\": \"exterior\", \"uservoice\": \"user voice\", \"effeciently\": \"efficiently\", \"torquise\": \"turquoise \", \"governmentand\": \"government and\", \"eletricity\": \"electricity\", \"coulums\": \"columns\", \"nolonger\": \"no longer\", \"wheras\": \"whereas\", \"infnite\": \"infinite\", \"decolourised\": \"no color\", \"onepiece\": \"one piece\", \"assignements\": \"assignments\", \"celebarted\": \"celebrated\", \"pharmacistical\": \"pharmaceutical\", \"jainsingle\": \"Jain single\", \"asssistance\": \"assistance\", \"glases\": \"glasses\", \"polymorpism\": \"polymorphism\", \"amerians\": \"Americans\", \"masquitos\": \"mosquitoes\", \"interseted\": \"interested\", \"thehighest\": \"the highest\", \"etnicity\": \"ethnicity\", \"anopportunity\": \"anopportunity\", \"multidiscipline\": \"multi discipline\", \"smartchange\": \"smart change\", \"collegefest\": \"college fest\", \"disdvantages\": \"disadvantages\", \"successfcators\": \"success factors\", \"sustitute\": \"substitute\",\"caoching\": \"coaching\", \"bullyed\": \"bullied\", \"comunicate\": \"communicate\", \"prisioner\": \"prisoner\", \"tamilnaadu\": \"Tamil Nadu\", \"methodologyies\": \"methodologies\", \"tranfers\": \"transfers\", \"truenorth\": \"true north\", \"backdonation\": \"back donation\", \"oreals\": \"ordeals\", \"browsec\": \"browser\", \"solarwinds\": \"solar winds\", \"susten\": \"sustain\", \"carnegi\": \"Carnegie\", \"doesent\": \"doesn't\", \"automtotive\": \"automotive\", \"nimuselide\": \"nimesulide\", \"subsciption\": \"subscription\", \"quatrone\": \"Quattrone\", \"qatalyst\": \"catalyst\", \"vardamana\": \"Vardaman\", \"suplements\": \"supplements\", \"repore\": \"report\", \"pikettys\": \"Piketty\", \"paramilltary\":\"paramilitary\", \"aboutlastnight\": \"about last night\", \"vidyapeth\": \"Vidyapeeth\", \"extraterrestial\": \"extraterrestrial\", \"powerloom\": \"power loom\", \"zonbie\": \"zombie\", \"cococola\": \"Coca Cola\", \"hameorrhage\": \"hemorrhage\", \"abhayanand\": \"Abhay Anand\", \"romedynow\": \"remedy now\", \"couster\": \"counter\", \"encouaged\": \"encouraged\", \"toprepare\": \"to prepare\", \"eveteasing\": \"eve teasing\", \"roulete\": \"roulette\", \"sorkar\": \"Sarkar\", \"waveboard\": \"wave board\", \"acclerate\": \"accelerate\", \"togrow\": \"to grow\", \"felatio\": \"fellatio\", \"baherain\": \"Bahrain\", \"teatment\": \"treatment\", \"iwitness\": \"eye witness\", \"autoplaying\": \"autoplay\", \"twise\": \"twice\", \"timeskip\": \"time skip\", \"disphosphorus\": \"diphosphorus\", \"implemnt\": \"implement\", \"proview\": \"preview\", \"pinshoppr\": \"pin shoppe\", \"protestng\": \"protesting\", \"chromatographymass\": \"chromatography mass\", \"ncache\": \"cache\", \"dowloands\": \"downloads\", \"biospecifics\": \"bio specifics\", \"conforim\": \"conform\", \"dreft\": \"draft\", \"sinhaleseand\": \"Sinhalese\", \"swivl\": \"swivel\", \"officerjms\": \"officers\", \"refrigrant\": \"refrigerant\", \"kendras\": \"Kendra\", \"alchoholism\": \"alcoholism\", \"dollor\": \"dollar\", \"jeyalalitha\": \"Jayalalitha\", \"bettner\": \"better\", \"itemstream\": \"timestream\", \"notetaking\": \"note taking\", \"cringworthy\": \"cringeworthy\", \"easyday\": \"easy day\", \"scenessex\": \"scenes sex\", \"vivavideo\": \"via video\", \"washboth\": \"wash both\", \"textout\": \"text out\", \"createwindow\": \"create window\", \"calsium\": \"calcium\", \"biofibre\": \"bio fibre\", \"emailbesides\": \"email besides\", \"kathhi\": \"Kathi\", \"cenre\": \"center\", \"polyarmory\": \"polyamory\", \"superforecasters\": \"super forecasters\", \"blogers\": \"bloggers\", \"medicalwhich\": \"medical which\", \"iiving\": \"living\", \"pronouciation\": \"pronunciation\", \"youor\": \"you or\", \"thuderbird\": \"Thunderbird\", \"oneside\": \"one side\",\"spearow\": \"Spearow\", \"aanythign\": \"anything\", \"inmaking\": \"in making\", \"datamining\": \"data mining\", \"greybus\": \"grey bus\", \"onmeter\": \"on meter\", \"biling\": \"billing\", \"fidlago\": \"Fidalgo\", \"edfice\": \"edifice\", \"microsolutions\": \"micro solutions\", \"easly\": \"easily\", \"eukarotic\": \"eukaryotic\", \"accedental\": \"accidental\", \"intercasts\": \"interests\", \"oppresive\": \"oppressive\", \"generalizably\": \"generalizable\", \"tacometer\": \"tachometer\", \"loking\": \"looking\", \"scrypt\": \"script\", \"usafter\": \"us after\", \"everyweek\": \"every week\", \"hopesthe\": \"hopes the\", \"openflow\": \"OpenFlow\", \"checkride\": \"check ride\", \"springdrive\": \"spring drive\", \"emobile\": \"mobile\", \"dermotology\": \"dermatology\", \"somatrophin\": \"somatropin\", \"saywe\": \"say we\", \"multistores\": \"multistory\", \"bolognaise\": \"Bolognese\", \"hardisk\": \"harddisk\", \"penisula\": \"peninsula\", \"refferring\": \"referring\", \"freshere\": \"fresher\", \"pokemkon\": \"Pokemon\", \"nuero\": \"neuro\", \"whosampled\": \"who sampled\", \"researchkit\": \"research kit\", \"speach\": \"speech\", \"acept\": \"accept\", \"indiashoppe\": \"Indian shoppe\",\"todescribe\": \"to describe\", \"hollywod\": \"Hollywood\", \"whastup\": \"whassup\", \"kjedahls\": \"Kjeldahl\", \"lancher\": \"launcher\", \"stalkees\": \"stalkers\", \"baclinks\": \"backlinks\", \"instutional\": \"institutional\", \"wassap\": \"Wassup\", \"methylethyl\": \"methyl ethyl\", \"fundbox\": \"fund box\", \"keypoints\": \"key points\", \"particually\": \"particularly\", \"loseit\": \"lose it\", \"gowipe\": \"go wipe\", \"autority\": \"authority\", \"prinicple\": \"principle\", \"complaince\": \"compliance\", \"itnormal\": \"it normal\", \"forpeople\": \"for people\", \"chaces\": \"chances\",\"yearhow\": \"year how\", \"fastcomet\": \"fast comet\", \"withadd\": \"with add\", \"omnicient\": \"omniscient\", \"tofeel\": \"to feel\", \"becauseof\": \"because of\", \"laungauage\": \"language\", \"combodia\": \"Cambodia\", \"bhuvneshwer\": \"Bhubaneshwar\", \"cognito\": \"Cognito\", \"thaelsemia\": \"thalassemia\", \"meritstore\": \"merit store\", \"masterbuate\": \"masturbate\", \"planethere\": \"planet here\", \"mostof\": \"most of\", \"shallowin\": \"shallow in\", \"wordwhen\": \"word when\", \"biodesalination\": \"desalination\", \"tendulkars\": \"Tendulkar\", \"kerja\": \"Kerja\", \"sertifikat\": \"certificate\", \"indegenous\": \"indigenous\", \"lowpage\": \"low page\", \"asend\": \"ascend\", \"leadreship\": \"leadership\", \"openlab\": \"open lab\", \"foldinghome\": \"folding home\", \"sachins\": \"Sachin\", \"pleatue\": \"plateau\", \"passwor\": \"password\", \"manisfestation\": \"manifestation\", \"valryian\": \"valerian\", \"chemotaxic\": \"chemotaxis\", \"condesending\": \"condescending\", \"spiltzvilla\": \"splitsville\", \"mammaliaforme\": \"mammaliaform\", \"instituteagra\": \"institute agra\", \"learningand\": \"learning and\", \"ramamurthynagar\": \"Ramamurthy Nagar\", \"glucoses\": \"glucose\", \"imitaion\": \"imitation\", \"awited\": \"awaited\", \"realvision\": \"real vision\", \"simslot\": \"sim slot\", \"yourr\": \"your\", \"pacjage\": \"package\", \"branchth\": \"branch\", \"magzin\": \"magazine\", \"frozon\": \"frozen\", \"codescomputational\": \"code computational\", \"tempratures\": \"temperatures\", \"neurophaphy\": \"neuropathy\", \"freezone\": \"free zone\", \"speices\": \"species\", \"compaitable\": \"compatible\", \"sensilization\": \"sensitization\", \"tuboscope\": \"tube scope\", \"gamechangers\": \"game changer\", \"windsheild\": \"windshield\", \"explorerie\": \"explorer\", \"cuccina\": \"Cucina\", \"earthstone\": \"hearthstone\", \"vocabs\": \"vocab\", \"previouse\": \"previous\", \"oneview\": \"one view\", \"relance\": \"reliance\", \"waterstop\": \"water stop\", \"imput\": \"input\", \"survivers\": \"survivors\", \"benedryl\": \"Benadryl\", \"requestparam\": \"request param\", \"typeadd\": \"type add\", \"autists\":\"artists\", \"forany\": \"for any\", \"inteview\": \"interview\", \"aphantasia\": \"Phantasia\", \"lisanna\": \"Lisanne\",\"civilengineering\": \"civil engineering\", \"austrailia\": \"Australia\", \"alchoholic\": \"alcoholic\", \"adaptersuch\": \"adapter such\", \"sphilosopher\": \"philosopher\", \"calenderisation\": \"calendarization\", \"smooking\": \"smoking\", \"pemdulum\": \"pendulum\", \"analsyis\": \"analysis\", \"psycholology\": \"psychology\", \"ubantu\": \"ubuntu\", \"emals\": \"emails\", \"questionth\": \"questions\", \"jawarlal\": \"Jawaharlal\", \"svaldbard\": \"Svalbard\", \"prabhudeva\": \"Prabhudeva\", \"robtics\": \"robotics\", \"umblock\": \"unblock\", \"professionaly\": \"professionally\", \"biovault\": \"bio vault\", \"bibal\": \"bible\", \"higherstudies\": \"higher studies\", \"lestoil\": \"less oil\", \"biteshow\": \"bike show\", \"humanslike\": \"humans like\", \"purpse\": \"purpose\", \"barazilian\": \"Brazilian\", \"gravitional\": \"gravitational\", \"cylinderical\": \"cylindrical\", \"peparing\": \"preparing\", \"healthequity\": \"health equity\", \"appcleaner\": \"app cleaner\", \"instantq\": \"instant\", \"abolisihed\": \"abolished\", \"kwench\": \"quench\", \"prisamatic\": \"prismatic\", \"bhubneshwar\": \"Bhubaneshwar\", \"liscense\": \"license\", \"cyberbase\": \"cyber base\", \"safezone\": \"safe zone\", \"deactivat\": \"deactivate\", \"salicyclic\": \"salicylic\", \"cocacola\": \"coca cola\", \"noice\": \"noise\", \"examinaton\": \"examination\", \"pharmavigilance\": \"pharmacovigilance\", \"sixthsense\": \"sixth sense\",\"musiclly\": \"musically\", \"khardushan\": \"Kardashian\", \"chandragupt\": \"Chandragupta\", \"bayesians\": \"bayesian\", \"engineeringbut\": \"engineering but\", \"caretrust\": \"care trust\", \"girlbut\": \"girl but\", \"aviations\": \"aviation\", \"joinee\": \"joiner\", \"tutior\": \"tutor\", \"tylenal\": \"Tylenol\", \"neccesity\": \"necessity\", \"kapsule\": \"capsule\", \"prayes\": \"prayers\", \"depositmobile\": \"deposit mobile\", \"settopbox\": \"set top box\", \"meotic\":\"meiotic\", \"accidentially\": \"accidentally\", \"offcloud\": \"off cloud\", \"keshavam\": \"Keshava\", \"domaincentral\": \"domain central\", \"onetaste\": \"one taste\", \"lumpsum\": \"lump sum\", \"medschool\": \"med school\", \"digicard\": \"Digi card\", \"abroadus\": \"abroad\", \"campusexcept\": \"campus except\", \"aptittude\": \"aptitude\", \"neutrions\": \"neutrinos\", \"onepaper\": \"one paper\", \"remidies\": \"remedies\", \"convinient\": \"convenient\", \"financaily\":\"financially\", \"postives\": \"positives\", \"nikefuel\": \"Nike fuel\", \"ingrediants\": \"ingredients\", \"aspireat\": \"aspirate\", \"firstand\": \"first\", \"mohammmad\": \"Mohammad\", \"mutliple\": \"multiple\", \"dimonatization\": \"demonization\", \"cente\": \"center\", \"marshmellow\": \"marshmallow\", \"citreon\": \"Citroen\", \"theirony\": \"the irony\", \"slienced\": \"silenced\", \"identifiy\": \"identify\", \"energ\": \"energy\", \"distribuiton\": \"distribution\", \"devoloping\": \"developing\", \"maharstra\": \"Maharastra\", \"siesmologist\": \"seismologist\", \"geckoos\": \"geckos\", \"placememnt\": \"placement\", \"introvercy\": \"introvert\", \"nuerosurgeon\": \"neurosurgeon\", \"realsense\": \"real sense\", \"congac\": \"cognac\", \"plaese\": \"please\", \"addicition\": \"addiction\", \"othet\": \"other\", \"howwill\": \"how will\", \"betablockers\": \"beta blockers\", \"phython\": \"Python\", \"concelling\": \"counseling\", \"einstine\": \"Einstein\", \"takinng\": \"taking\", \"birtday\": \"birthday\", \"prefessor\": \"professor\", \"dreamscreen\": \"dream screen\", \"satyabama\": \"Satyabhama\", \"faminism\": \"feminism\", \"noooooooooo\": \"no\", \"certifaction\": \"certification\",\"smalll\": \"small\", \"sterlization\": \"sterilization\", \"athelete\": \"athlete\", \"comppany\": \"company\", \"handlebreakup\": \"handle a breakup\", \"wellrounded\": \"well rounded\", \"breif\": \"brief\", \"engginering\": \"engineering\", \"genrally\": \"generally\", \"forgote\": \"forgot\", \"compuny\": \"the company\", \"wholeseller\": \"wholesaler\", \"conventioal\": \"conventional\", \"healther\": \"healthier\", \"realitic\": \"realistic\", \"israil\": \"Israel\", \"morghulis\": \"Margulis\", \"begineer\": \"beginner\", \"unwaiveringly\": \"unwavering\", \"writen\": \"written\", \"gastly\": \"ghastly\", \"obscurial\": \"obscure\", \"permanetly\": \"permanently\", \"bday\": \"birthday\", \"studing\": \"studying\", \"blackcore\": \"black core\", \"macbok\": \"MacBook\", \"realted\": \"related\", \"resoning\": \"reasoning\", \"servicenow\": \"service now\", \"medels\": \"medals\", \"hairloss\": \"hair loss\", \"messanger\": \"messenger\", \"masterbate\": \"masturbate\", \"oppurtunities\": \"opportunities\", \"newzealand\": \"new zealand\", \"offcampus\": \"off campus\", \"lonliness\": \"loneliness\", \"percentilers\": \"percentiles\", \"caccount\": \"account\", \"imrovement\": \"improvement\", \"cashbacks\": \"cashback\", \"inhand\": \"in hand\", \"baahubali\": \"bahubali\", \"diffrent\": \"different\", \"strategywho\": \"strategy who\", \"meetme\": \"meet me\", \"wealthfront\": \"wealth front\", \"masterbation\": \"masturbation\", \"successfull\": \"successful\", \"lenght\": \"length\", \"increse\": \"increase\", \"mastrubation\": \"masturbation\", \"intresting\": \"interesting\", \"quesitons\": \"questions\", \"fullstack\": \"full stack\", \"harambe\": \"Harambee\", \"criterias\": \"criteria\", \"rajyasabha\": \"Rajya Sabha\", \"techmahindra\": \"tech Mahindra\", \"messeges\": \"messages\", \"intership\": \"internship\", \"benifits\": \"benefits\", \"dowload\": \"download\", \"dellhi\": \"Delhi\", \"traval\": \"travel\", \"prepration\": \"preparation\", \"engineeringwhat\": \"engineering what\", \"habbit\": \"habit\", \"diference\": \"difference\", \"permantley\": \"permanently\", \"doesnot\": \"does not\", \"thebest\": \"the best\", \"addmision\": \"admission\", \"gramatically\": \"grammatically\", \"dayswhich\": \"days which\", \"intrest\": \"interest\", \"seperatists\":\"separatists\", \"plagarism\": \"plagiarism\", \"demonitize\": \"demonetize\", \"explaination\": \"explanation\", \"numericals\": \"numerical\", \"defination\": \"definition\", \"inmortal\": \"immortal\", \"elasticsearch\": \"elastic search\"}\n\ncontraction_mapping = {'What\u201ds':'what is', 'What\"s':'what is',\"its\":\"it is\",\"What's\":\"what is\", \"'ll\": \"will\", \"n't\": \"not\", \"'re\":\"are\", \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"'they're\":\"they are\",\"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n\ncontractions = set(contraction_mapping)\nmisspelled = set(misspelled_words)","bf51636a":"def pre_processing(sentence):\n    \n    out = sentence.replace('\\n', ' ').replace('\\t', ' ').replace('\\xa0',' ')\\\n    .replace(\"\uff1f\", \"?\").replace(\"\u2026\", \" \").replace(\"\u00e9\", \"e\")\n    \n    for punct in \".\/-'_\u201c\u201d\":\n        out = out.replace(punct, ' ')\n    for punct in '&':\n        out = out.replace(punct, f'{punct} ')\n    for punct in \"\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&\u00a9^\u00ae<\u2192\u00b0\u20ac\u2122\u203a\u2665\u2190\u00d7\u00a7\u00c2\u2588\u00bd\u00e0\u2026\u2605\u2013\u25cf\u00e2\u25ba\u2212\u00a2\u00b2\u00ac\u2591\u00b6\u2191\u00b1\u00bf\u25be\u2550\u00a6\u2551\u2015\u00a5\u2593\u2014\u2039\u2500\u2592\uff1a\u00bc\u2295\u25bc\u25aa\u2020\u25a0\u2019\u2580\u00a8\u2584\u266b\u2606\u00e9\u00af\u2666\u00a4\u25b2\u00e8\u00b8\u00be\u00c3\u22c5\u2018\u221e\u2219\uff09\u2193\u3001\u2502\uff08\u00bb\uff0c\u266a\u2569\u255a\u00b3\u30fb\u2566\u2563\u2554\u2557\u25ac\u2764\u00ef\u00d8\u00b9\u2264\u2021\u221a\u30b7\u3057\":\n        out = out.replace(punct, ' ')\n    out = out.translate(str.maketrans('', '', string.punctuation))\n    #out = out.translate(str.maketrans('', '', string.digits))\n    out = out.lower()\n    return out\n\n#Removing the left right and in between empty spaces.\ndef left_right_spaces(sentence):\n    new_sentence = []\n    for word in sentence.split(' '):\n        if word!='':\n            new_sentence.append(word)\n    return \" \".join(new_sentence)\n            \n#These words are decided after exploring dataset\nstop_words = [\"the\",\"a\",\"is\",\"to\",\"be\",\"are\",\"what\"]\ndef own_stop_words(sentence, stop_words = stop_words):\n    new_sentence = []\n    for word in sentence.split(' '):\n        if word not in stop_words:\n            new_sentence.append(word)\n    return \" \".join(new_sentence)\n\n#Contractions like \"aren't\" are replaced with \"are not\"\ndef known_contractions(sentence):\n    sentence = sentence.replace(\"\u2019\", \"'\").replace(\"`\", \"'\").replace(\"\u00b4\", \"'\")\n    new_sentence = []\n    for word in sentence.split(' '):\n        if word in contractions:\n            new_sentence.append(contraction_mapping[word])\n        else:\n            new_sentence.append(word)\n    return \" \".join(new_sentence)\n\n#Spelling corrections reduces the unnecessary vocabulary also improves the dataset\ndef spelling_correction(sentence):\n    new_sentence = []\n    for word in sentence.split(' '):\n        if word in misspelled:\n            new_sentence.append(misspelled_words[word])\n        else:\n            new_sentence.append(word)\n    return \" \".join(new_sentence)\n\n#url's have been replaced with url token\ndef check_urls(sentence):\n    flag=0\n    out = sentence\n    for word in sentence.split(' '):\n        if len(word)>3:\n            url = re.findall('((ftp|http|https):\\\/\\\/)?(www.)?(?!.*(ftp|http|https|www.))[a-zA-Z0-9_-]+(\\.[a-zA-Z]+)+((\\\/)[\\w#]+)*(\\\/\\w+\\?[a-zA-Z0-9_]+=\\w+(&[a-zA-Z0-9_]+=\\w+)*)?$', word)\n            if len(url)>0:\n                out = out.replace(word, ' url ')\n                flag=1\n    if flag==1:\n        return out\n    return sentence\n\n#Words like india's economy, america's economy are replaced with just india economy, america economy etc. \ndef smart_words(sentence):\n    try:\n        new_sentence = []\n        for word in sentence.split(' '):\n            if word==\"'s\":\n                word=\"\"\n            new_sentence.append(re.sub('[\\w]+\\s*[\\\u2019\\\u00b4\\'\\\u201d\\\u2019\\\"\\\u201d]\\s*[s]',word[:-2], word))            \n        return \" \".join(new_sentence)\n    except:\n        return np.NaN\n\n#Some common country names are replaced with a common word to reduce vocabulary.\namerica_names = [\"USA\",\"U.S\",\"U.S.A\",\"U.S.\",\"usa\",\"US\",\"U.S,\",\"U.S.A,\",\"U.S.\",\"usa,\",\"US,\",\"US?\",\"U.S?\",\"U.S.A?\",\"U.S.?\",\"usa?\",\"U.S.A?\"]\nuk = [\"U.K\",\"U.K?\",\"U.K,\",\"U.K.\"]\namerica_names = set(america_names)\nuk = set(uk)\ndef countries(sentence):\n    new_sentence = []\n    for word in sentence.split(' '):\n        if word in america_names:\n            new_sentence.append('america')\n        elif word in uk:\n            new_sentence.append('united kingdom')\n        else:\n            new_sentence.append(word)\n    return \" \".join(new_sentence)\n\n\n#Symbols are replaced with the textual information to retain important information.\ndef replace_symbols(sentence):\n    sentence = sentence.replace(\"%\", \" percent \").replace(\"\u20b9\", \" rupee \").replace(\"$\", \" dollar \")\\\n    .replace(\"\u20ac\", \" euro\").replace(\"?\",'').replace(\"INR\",\" rupee \").replace(\"Rs.\",\" rupee \").replace(\"Rs\",\" rupee \")\n    \n    return sentence\n\n#Numbers like 100k is replaced with 100 thousand, 21yrs is replaced with 21 years, 1990's with 1990 and so on.\ndef preprocessing_numbers(row):\n    row = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", row) #removing ,(comma) between the numbers\n    new_sentence = []\n    for word in row.split(' '):\n        match = re.match(r\"(^[0-9]+)(\\'*\\\u2019*k{1}$)\", word)\n        if match:\n            items = match.groups()\n            new_sentence.append(items[0])\n            new_sentence.append('thousand')\n        else:\n            new_sentence.append(word)\n            \n    sentence = \" \".join(new_sentence)\n    new_sentence = []\n    for word in sentence.split(' '):\n        match = re.match(r\"(^[0-9]+)(\\'*\\\u2019*y{1}r?s?$)\", word)\n        if match:\n            items = match.groups()\n            new_sentence.append(items[0])\n            new_sentence.append('year')\n        else:\n            new_sentence.append(word)\n\n    sentence = \" \".join(new_sentence)\n    new_sentence = []\n    for word in sentence.split(' '):\n        match = re.match(r\"(^[0-9]+)(\\'*\\\u2019*s{1}$)\", word)\n        if match:\n            items = match.groups()\n            new_sentence.append(items[0])\n        else:\n            new_sentence.append(word)\n\n    sentence = \" \".join(new_sentence)\n    new_sentence = []\n    for word in sentence.split(' '):\n        match = re.match(r\"([0-9]+)([a-zA-Z]+)\", word)\n        if match:\n            items = match.groups()\n            new_sentence.append(items[0])\n            new_sentence.append(items[1])\n        else:\n            new_sentence.append(word)\n    return \" \".join(new_sentence)\n\n#Numbers are replaced with textual information to retain the value of sentence.\ndef num_to_text(sentence):\n    sentence = sentence.replace(\"%\", \" percent \").replace(\"\u20b9\", \" rupee \").replace(\"$\", \" dollar \")\\\n    .replace(\"\u20ac\", \" euro\").replace(\"?\",'').replace(\"INR\",\" rupee \").replace(\"Rs.\",\" rupee \")\n    new_sentence = []\n    for word in sentence.split(' '):\n        try:\n            word = int(word)\n            if word<=1000 and word>0:\n                text = num2words(word)\n                new_sentence.append(text)\n            else:\n                new_sentence.append(str(word))\n        except:\n            new_sentence.append(str(word))\n    return \" \".join(new_sentence)","09e9c97e":"#Processing training data\ntrain['question1'] = train['question1'].apply(countries)\ntrain['question2'] = train['question2'].apply(countries)\n\ntrain['question1'] = train['question1'].apply(known_contractions)\ntrain['question2'] = train['question2'].apply(known_contractions)\n\ntrain['question1'] = train['question1'].apply(smart_words)\ntrain['question2'] = train['question2'].apply(smart_words)\ntrain = train.dropna(how='any',axis=0) \n\ntrain['question1'] = train['question1'].apply(replace_symbols)\ntrain['question2'] = train['question2'].apply(replace_symbols)\n\ntrain['question1'] = train['question1'].apply(preprocessing_numbers)\ntrain['question2'] = train['question2'].apply(preprocessing_numbers)\n\ntrain['question1'] = train['question1'].apply(num_to_text)\ntrain['question2'] = train['question2'].apply(num_to_text)\n\ntrain['question1'] = train['question1'].apply(check_urls)\ntrain['question2'] = train['question2'].apply(check_urls)\n\ntrain['question1'] = train['question1'].apply(num_to_text)\ntrain['question2'] = train['question2'].apply(num_to_text)\n\ntrain['question1'] = train['question1'].apply(pre_processing)\ntrain['question2'] = train['question2'].apply(pre_processing)\n\ntrain['question1'] = train['question1'].apply(own_stop_words)\ntrain['question2'] = train['question2'].apply(own_stop_words)\n\ntrain['question1'] = train['question1'].apply(spelling_correction)\ntrain['question2'] = train['question2'].apply(spelling_correction)\n\ntrain['question1'] = train['question1'].apply(preprocessing_numbers)\ntrain['question2'] = train['question2'].apply(preprocessing_numbers)\n\ntrain['question1'] = train['question1'].apply(num_to_text)\ntrain['question2'] = train['question2'].apply(num_to_text)\n\ntrain['question1'] = train['question1'].apply(left_right_spaces)\ntrain['question2'] = train['question2'].apply(left_right_spaces)","16756f4f":"duplicate_dataset = train[train[\"is_duplicate\"] == 1]\nnon_duplicate_dataset = train[train[\"is_duplicate\"] == 0]","ddb5c351":"#Calculating the vocab size and creating a dictionary to check the frequency of those words.\nwords_1 = []\nfor doc in train['question1']:\n    for word in doc.split(' '):\n        if word!='':\n            words_1.append(word)\n\nwords_2 = []\nfor doc in train['question2']:\n    for word in doc.split(' '):\n        if word!='':\n            words_2.append(word)\n\nwords_1_count = Counter(words_1)\nwords_2_count = Counter(words_2)\n\ntotal_words = words_1_count + words_2_count\nvocab_size = len(total_words)\n\nprint(\"Total vocab size:\", vocab_size)\nprint(words_1_count.most_common(10))\nprint(words_2_count.most_common(10))\n\ndel words_1\ndel words_2\ndel words_1_count\ndel words_2_count\n","b1c2d6ba":"def n_grams(count, dataset, name):\n    d1 = dataset['question1']\n    d2 = dataset['question2']\n    frames = [d1,d2]\n    result = pd.concat(frames)\n    n_grams_1 = []\n    for row in result:\n        total_words = row.split(' ')\n        for i in range(0,len(total_words)-(count-1)):\n            n_grams_1.append(' '.join(total_words[i:count+i]))\n    n_grams_1 = Counter(n_grams_1)\n    df = pd.DataFrame(list(n_grams_1.items()), columns=['words', 'count'])\n    df.sort_values(by=['count'], ascending=False, inplace=True)\n    \n    fig = sns.barplot(x = df[\"words\"][:20], y= df[\"count\"][:20])\n    plt.xticks(rotation=90)\n    plt.title(name)\n    return fig","eb3a9604":"for k in range(1,4):\n    fig = plt.figure(figsize=(12, 11))\n    for i in range(1, 2):\n        plt.subplot(2, 2, i)\n        print(n_grams(k, duplicate_dataset, \"duplicate\"))\n        plt.subplot(2, 2, i+1)\n        print(n_grams(k, non_duplicate_dataset, \"non_duplicate\"))","de967a45":"#Checking the word length of all rows.\nvalues = []\nfor row in train['question1']:\n    values.append(len(row.split(' ')))\nvalues = np.array(values)\n\nsns.distplot(values, kde = False)","9345d4d9":"#Checking the word length of all rows.\nvalues = []\nfor row in train['question2']:\n    values.append(len(row.split(' ')))\nvalues = np.array(values)\n\nsns.distplot(values, kde = False)\n\n#We can see that most of the questions have word length in between 0 and 150.","68e7f8ac":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n\n!unzip glove*.zip\n\n!rm glove.6B.zip\n!rm glove.6B.100d.txt\n!rm glove.6B.300d.txt\n!rm glove.6B.50d.txt\n\ntotal_words = set(total_words)\nprint('Indexing word vectors.')\n\nembeddings_index = {}\nf = open('glove.6B.200d.txt', encoding='utf-8')\n\nfor line in f:\n    values = line.split()\n    word = values[0]\n    if word in total_words:\n        coefs = np.asarray(values[-200:], dtype='float32')\n        embeddings_index[word] = coefs\n    if len(embeddings_index)==len(total_words):\n        print(\"done\")\nf.close()\nprint('Found %s word vectors.' % len(embeddings_index))\n\n!rm glove.6B.200d.txt","fb489d8a":"print(\"Total number of words (vocabulary size): \", len(total_words))\nprint(\"Embeddings found for the words: \", len(embeddings_index))\nprint(\"Percentage of words having embeddings: \", (len(embeddings_index)\/len(total_words))*100)\nprint(\"Percentage of words not having embeddings: \", 100 - (len(embeddings_index)\/len(total_words))*100)\ndel total_words","4527b05c":"train = train.sample(frac=1) #Shuffles the entire dataset. argument frac is set in range [0,1]. 1 indicates returning all the rows after shuffling.\ntarget = train['is_duplicate']\ntrain.drop(['is_duplicate','qid1','qid2','id'], axis = 1, inplace = True)","8b5406aa":"X_train, X_val, y_train, y_val = train_test_split(train, target, stratify = target, test_size=0.2)\ndel train\ndel duplicate_dataset \ndel non_duplicate_dataset","2d8afc73":"#Converting the words in the sentence to respective Glove word emebddings.\ndef embeddings(sentence):\n    new_sentence = []\n    for word in sentence.split(' '):\n        if word in embeddings_index:\n            new_sentence.append(embeddings_index[word])\n        else:\n            new_sentence.append(np.zeros(EMBEDDING_DIM))\n    return new_sentence","a59fe4a8":"#Designing the architecture\nsequence_1_input = Input(shape = (SEQUENCE_LENGTH , EMBEDDING_DIM))\nx1 = Bidirectional(CuDNNLSTM(200))(sequence_1_input)\n\n\nsequence_2_input = Input(shape = (SEQUENCE_LENGTH , EMBEDDING_DIM))\ny1 = Bidirectional(CuDNNLSTM(200))(sequence_2_input)\n\n\nmerged = concatenate([x1, y1])\n\nmerged = BatchNormalization()(merged)\nmerged = Dense(200, activation='relu')(merged)\nmerged = Dropout(0.3)(merged)\n\nmerged = BatchNormalization()(merged)\nmerged = Dense(200, activation='relu')(merged)\nmerged = Dropout(0.3)(merged)\n\nmerged = BatchNormalization()(merged)\nmerged = Dense(200, activation='relu')(merged)\nmerged = Dropout(0.3)(merged)\n\nmerged = BatchNormalization()(merged)\nmerged = Dense(200, activation='relu')(merged)\nmerged = Dropout(0.3)(merged)\n\npreds = Dense(1, activation='sigmoid')(merged)\n\nmodel = Model(inputs=[sequence_1_input, sequence_2_input], outputs=preds)\n\nmodel.compile(loss='binary_crossentropy', optimizer = 'RMSprop')\nmodel.summary()","92e4b958":"final_epochs = 1\ncount = 30000\n\ncheckpoint = ModelCheckpoint('saved_model.hdf5', monitor='val_loss', verbose=1, save_weights_only=True, save_best_only=True, mode='min')\ncallbacks_list = [checkpoint]\n\nfor k in range(0, final_epochs):\n    print(\"Epoch no: \", k)\n    for i in range(len(X_train)\/\/count):\n        data1 = X_train['question1'][i*count:(i+1)*count]\n        data2 = X_train['question2'][i*count:(i+1)*count]\n        data1 = data1.apply(embeddings)\n        data2 = data2.apply(embeddings)\n\n        data1 = pad_sequences(data1, maxlen=SEQUENCE_LENGTH, padding = 'pre', dtype=\"float32\")\n        data2 = pad_sequences(data2, maxlen=SEQUENCE_LENGTH, padding = 'pre', dtype=\"float32\")\n        \n        val1 = X_val['question1']\n        val2 = X_val['question2']\n        val1 = val1.apply(embeddings)\n        val2 = val2.apply(embeddings)\n\n        val1 = pad_sequences(val1, maxlen=SEQUENCE_LENGTH, padding = 'pre', dtype=\"float32\")\n        val2 = pad_sequences(val2, maxlen=SEQUENCE_LENGTH, padding = 'pre', dtype=\"float32\")\n        \n\n        labels_train = np.array(y_train[i*count:(i+1)*count])\n        labels_val = np.array(y_val)\n\n        model.fit([data1, data2], labels_train,\n                  validation_data=([val1, val2], labels_val), epochs = 1,\n                  batch_size = 1024, callbacks = callbacks_list)\n","ce0bb5aa":"del data1, data2, val1,val2\ndel labels_train, labels_val\ndel model\nimport gc\ngc.collect()","4a885c33":"**Classifying the dataset into duplicate and non-duplicate to explore the data.**","1f60fe0f":"**Below is the data pre-processing. The following pre-processing like deciding own stop words, punctuation, correcting misspelled words, replacing contractions, replacing numbers have been identified after exploring the dataset. The Exploration techniques are there in the below cells.**","c68eb976":"**Below is the plot of occurences of n-grams in the duplicate and non-duplicate sentences.**","d09c6396":"**Applying the above steps using .apply() function.**","ecb460a8":"**Data Exploration**","0b8e392c":"**Now, after all the pre-processing is done, words are replaced with word embeddings. You can also train your own word embeddings. Here, I am using GloVe embeddings.**\n\nThese are 200 dimensional vectors, that means every word is represented using 200 length vector. This length is tunable and we should try multiple length vectors and choose the best one.","f4926d72":"**Import the necessary libraries**","583f5f82":"**Splitting the dataset into trainng and validation to test our model's predictions on unseen data.**","aa9746cd":"Here, I am training the dataset in batches since the RAM cannot handle the entire dataset having 200 dimensional word vectors all at once. So I am training 30,000 samples in iteration. A single epoch will undergo training of all the samples in a dataset (one after another batch). 1 epoch will have length(dataset)\/30000 number of iterations.  ","47f9e734":"**n-grams are the continuous sequence of words. They can be single words if n is equal to 1 or continuous sequence of two words if n is equal to 2 and so on.**\n\nFor instance, suppose the sentence is \"Hello, how are you\".\n\nIf n = 1 then the list would be [Hello, how, are, you]\n\nIf n = 2 then the list would be [Hello how, how are, are you]\n\nIf n = 3 then the list would be [Hello how are, how are you]","26513cfb":"**Checking the word length of each sentence in both the columns, question1 and question2. This will help us decide the sequence length during model training.**","63e2cfcd":"**Loading the training data**"}}