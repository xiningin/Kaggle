{"cell_type":{"c08ba784":"code","ee5625dc":"code","bba23604":"code","896ef2b0":"code","269b4e0e":"code","63528538":"code","a50e969d":"code","e5623fbc":"code","f7e3011f":"code","b0171432":"code","730b6be8":"code","b05fcc22":"code","00edf294":"code","8235274d":"code","06fed98b":"code","9c07451b":"code","248466a9":"code","9941a325":"code","38156d2f":"code","5beee6d5":"code","bc0e0e5d":"code","974b849e":"code","af58c1e1":"code","8b5dfbb5":"code","b5ff097a":"code","ad7ca363":"code","dfff7256":"code","a51b2760":"code","9913040f":"code","cce56720":"code","17eab43a":"code","cffd07a7":"code","6565ba19":"markdown","75cc0cd2":"markdown","3565d1f4":"markdown","c35bd795":"markdown","91a02554":"markdown","bff21e58":"markdown","ce71974c":"markdown","1c84f27d":"markdown","05d79bc9":"markdown","ab0d4d78":"markdown","edded8d7":"markdown","36dce24f":"markdown"},"source":{"c08ba784":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport itertools\nimport matplotlib.pyplot as plt\nimport string\nimport re\nimport collections\nfrom sklearn import  preprocessing\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nfrom sklearn.metrics import make_scorer, f1_score, accuracy_score, mean_absolute_error, confusion_matrix\nimport optuna\nfrom lofo import LOFOImportance, Dataset, plot_importance ## to install !pip install lofo-importance\n%matplotlib inline\nimport itertools\n","ee5625dc":"# READ DATA \ntrain_df = pd.read_json('..\/input\/two-sigma-connect-rental-listing-inquiries\/train.json.zip')\ntest_df = pd.read_json('..\/input\/two-sigma-connect-rental-listing-inquiries\/test.json.zip')","bba23604":"train_df.info()","896ef2b0":"train_df['target'] = train_df['interest_level'].apply(lambda x: 0 if x=='low' \n                                                      else 1 if x=='medium' \n                                                      else 2) \n# train_df['low'] = train_df['interest_level'].apply(lambda x: 1 if x=='low' else 0)\n# train_df['medium'] = train_df['interest_level'].apply(lambda x: 1 if x=='medium' else 0)\n# train_df['high'] = train_df['interest_level'].apply(lambda x: 1 if x=='high' else 0)","269b4e0e":"train_df['description'].iloc[0]","63528538":"# REMOVE UNNECESSARY WORDS FROM DESCRIPTION\ntrain_df['description'] = train_df['description'].apply(lambda x: x.replace(\"<br \/>\", \"\"))\ntrain_df['description'] = train_df['description'].apply(lambda x: x.replace(\"br\", \"\"))\ntrain_df['description'] = train_df['description'].apply(lambda x: x.replace(\"<p><a\", \"\"))","a50e969d":"print(train_df['description'].iloc[0])","e5623fbc":"#basic features\ntrain_df['rooms'] = train_df['bedrooms'] + train_df['bathrooms'] \n\n# count of photos #\ntrain_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n\n# count of \"features\" #\ntrain_df[\"num_features\"] = train_df[\"features\"].apply(len)\n\n# count of words present in description column #\ntrain_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n\n# description contains email\nregex = r'[\\w\\.-]+@[\\w\\.-]+'\ntrain_df['has_email'] = train_df['description'].apply(lambda x: 1 if re.findall(regex, x) else 0)\n\n# description contains phone\n# description contains phone\ntrain_df['has_phone'] = train_df['description'].apply(lambda x:re.sub('['+string.punctuation+']', '', x).split())\\\n        .apply(lambda x: [s for s in x if s.isdigit()])\\\n        .apply(lambda x: len([s for s in x if len(str(s))==10]))\\\n        .apply(lambda x: 1 if x>0 else 0)\n\n# CONVERT LOWER ALL OF WORDS\ntrain_df[[\"features\"]] = train_df[[\"features\"]].apply(\n    lambda _: [list(map(str.strip, map(str.lower, x))) for x in _])","f7e3011f":"# REMOVE UNNECESSARY WORDS FROM DESCRIPTION\ntest_df['description'] = test_df['description'].apply(lambda x: x.replace(\"<br \/>\", \"\"))\ntest_df['description'] = test_df['description'].apply(lambda x: x.replace(\"br\", \"\"))\ntest_df['description'] = test_df['description'].apply(lambda x: x.replace(\"<p><a\", \"\"))\n\n# FEATURE ENGINEERING\n#basic features\ntest_df['rooms'] = test_df['bedrooms'] + test_df['bathrooms'] \n\n# count of photos #\ntest_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n\n# count of \"features\" #\ntest_df[\"num_features\"] = test_df[\"features\"].apply(len)\n\n# count of words present in description column #\ntest_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n\n# description contains email\nregex = r'[\\w\\.-]+@[\\w\\.-]+'\ntest_df['has_email'] = test_df['description'].apply(lambda x: 1 if re.findall(regex, x) else 0)\n\n# description contains phone\ntest_df['has_phone'] = test_df['description'].apply(lambda x:re.sub('['+string.punctuation+']', '', x).split())\\\n        .apply(lambda x: [s for s in x if s.isdigit()])\\\n        .apply(lambda x: len([s for s in x if len(str(s))==10]))\\\n        .apply(lambda x: 1 if x>0 else 0)\n\n# CONVERT LOWER ALL OF WORDS\ntest_df[[\"features\"]] = test_df[[\"features\"]].apply(\n    lambda _: [list(map(str.strip, map(str.lower, x))) for x in _])","b0171432":"feature_value_train = train_df['features'].tolist()\nfeature_value_test = test_df['features'].tolist()\n\nfeature_value_train\nfeature_value_test\n\nfeature_lst_train = []\nfeature_lst_test = []\n\nfor i in range(len(feature_value_train)):\n    feature_lst_train += feature_value_train[i]\n    \nfor i in range(len(feature_value_test)):\n    feature_lst_test += feature_value_test[i]\n# print(len(feature_lst)) # all features\n\nuniq_feature_train = list(set(feature_lst_train))\nuniq_feature_test = list(set(feature_lst_test))\n\n# print(uniq_feature) #all unique features\nlen(uniq_feature_train)\nlen(uniq_feature_test)","730b6be8":"# see the frequency of each feature\ndef most_common(lst):\n    features = collections.Counter(lst)\n    feature_value = features.keys()\n    frequency = features.values()\n    data = [('feature_value', feature_value),\n            ('frequency', frequency),]    \n    df = pd.DataFrame.from_dict(dict(data))\n    return df.sort_values(by = 'frequency', ascending = False)\n\ndf_features_train = most_common(feature_lst_train)\ndf_features_test = most_common(feature_lst_test)\n\ndf_features_train\ndf_features_test","b05fcc22":"def newColumn(name, df, series):\n    feature = pd.Series(0,df.index,name = name)# data : 0\n    for row,word in enumerate(series):\n        if name in word:\n            feature.iloc[row] = 1\n    df[name] = feature # feature : series ; value in series : 1 or 0\n    return df\n\n# select features based on frequency\nfacilities = ['elevator', 'cats allowed', 'hardwood floors', 'dogs allowed', 'doorman', 'dishwasher', 'no fee', 'laundry in building', 'fitness center']\nfor name in facilities:\n    train_df = newColumn(name, train_df, train_df['features'])\n    test_df = newColumn(name, test_df, test_df['features'])","00edf294":"print(train_df['features'].iloc[0])","8235274d":"plt.figure(figsize=(8,4))\ncolors = ['lightcoral','gold','lightblue']\nsns.countplot(train_df['interest_level'], alpha=0.8)\nplt.title(\"INTEREST LEVEL COMPARE\")\nplt.xlabel('Interest level', fontsize=12)\nplt.show()","06fed98b":"plt.style.use(\"seaborn-whitegrid\")\nplt.figure(figsize=(12,8))\nplt.title(\"CORRELATION BETWEEN NUMERICAL VALUES\")\nnum_col = [\"rooms\", \"num_photos\", \"num_features\", \"has_email\", \"has_phone\", \"price\", \"target\"]\nsns.heatmap(train_df[num_col].corr(), annot = True, fmt = \".2f\")\nplt.show()","9c07451b":"plt.style.use(\"seaborn-whitegrid\")\nplt.figure(figsize=(12,8))\nplt.title(\"CORRELATION BETWEEN NUMERICAL VALUES\")\nnum_col = ['elevator', 'cats allowed', 'hardwood floors', 'dogs allowed', 'doorman', 'dishwasher', 'no fee', 'laundry in building', 'fitness center', 'target']\nsns.heatmap(train_df[num_col].corr(), annot = True, fmt = \".2f\")\nplt.show()","248466a9":"### Rent interest graph of New-York\nsns.lmplot(x=\"longitude\", y=\"latitude\", fit_reg=False, hue='interest_level',\n           hue_order=['low', 'medium', 'high'], size=9, scatter_kws={'alpha':0.4,'s':30},\n           data=train_df[(train_df.longitude>train_df.longitude.quantile(0.1))\n                        &(train_df.longitude<train_df.longitude.quantile(0.9))\n                        &(train_df.latitude>train_df.latitude.quantile(0.1))                           \n                        &(train_df.latitude<train_df.latitude.quantile(0.9))]);\nplt.xlabel('Longitude');\nplt.ylabel('Latitude');","9941a325":"### Price exploration\nprices=train_df.groupby('interest_level', as_index=False)['price'].mean()\ncolors = ['lightcoral','gold','lightblue']\n\nfig=plt.figure(figsize=(8,6))\nplt.bar(prices.interest_level, prices.price, color=colors, width=0.5, alpha=0.8)\n#set titles\nplt.xlabel('Interest level')\nplt.ylabel('Average price')\nplt.title('Average price across interest level')\nplt.show()","38156d2f":"train_df.groupby(['building_id', 'manager_id', 'interest_level']).count()","5beee6d5":"#WORDCLOUD FOR DESCRIPTION AND DISPLAY ADDRESS\n#Preprocessing\ntext = ''\ntext_da = ''\ntext_desc = ''\ntext_str = ''\nfor ind, row in train_df.iterrows():\n    for feature in row['features']:\n        text = \" \".join([text, \"_\".join(feature.strip().split(\" \"))])\n    text_da = \" \".join([text_da,\"_\".join(row['display_address'].strip().split(\" \"))])\n    text_desc = \" \".join([text_desc, row['description']])\n    text_str = \" \".join([text_str, row['street_address']])\ntext = text.strip()\ntext_da = text_da.strip()\ntext_desc = text_desc.strip()\ntext_str = text_str.strip()\n\n\n# wordcloud for features\nplt.figure(figsize=(12,6))\nwordcloud = WordCloud(background_color='white', width=600, height=300, max_font_size=50, max_words=40).generate(text)\nwordcloud.recolor(random_state=0)\nplt.imshow(wordcloud)\nplt.title(\"Wordcloud for features\", fontsize=30)\nplt.axis(\"off\")\nplt.show()\n\n\n# wordcloud for display address\nplt.figure(figsize=(12,6))\nwordcloud = WordCloud(background_color='white', width=600, height=300, max_font_size=50, max_words=40).generate(text_da)\nwordcloud.recolor(random_state=0)\nplt.imshow(wordcloud)\nplt.title(\"Wordcloud for Display Address\", fontsize=30)\nplt.axis(\"off\")\nplt.show()\n\n\n# wordcloud for description\nplt.figure(figsize=(12,6))\nwordcloud = WordCloud(background_color='white', width=600, height=300, max_font_size=50, max_words=40).generate(text_desc)\nwordcloud.recolor(random_state=0)\nplt.imshow(wordcloud)\nplt.title(\"Wordcloud for Description\", fontsize=30)\nplt.axis(\"off\")\nplt.show()\n\n# wordcloud for street address\nplt.figure(figsize=(12,6))\nwordcloud = WordCloud(background_color='white', width=600, height=300, max_font_size=50, max_words=40).generate(text_str)\nwordcloud.recolor(random_state=0)\nplt.imshow(wordcloud)\nplt.title(\"Wordcloud for Street Address\", fontsize=30)\nplt.axis(\"off\")\nplt.show()","bc0e0e5d":"# TRAINING DATASET\ntrain_df.drop('interest_level', axis=1, inplace=True)\ntrain_df.drop('created', axis=1, inplace=True)\ntrain_df.drop('description', axis=1, inplace=True)\ntrain_df.drop('features', axis=1, inplace=True)\ntrain_df.drop('photos', axis=1, inplace=True)\n\n# TEST DATASET\ntest_df.drop('created', axis=1, inplace=True)\ntest_df.drop('description', axis=1, inplace=True)\ntest_df.drop('features', axis=1, inplace=True)\ntest_df.drop('photos', axis=1, inplace=True)","974b849e":"categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\nfor f in categorical:\n        if train_df[f].dtype=='object':\n            lbl = preprocessing.LabelEncoder()\n            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n            train_df[f] = lbl.transform(list(train_df[f].values))\n            test_df[f] = lbl.transform(list(test_df[f].values))","af58c1e1":"X = train_df.drop(['target'], axis = 1)\ny = train_df.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = .3,\n                                                    random_state = 5,\n                                                   stratify = y)\n\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndvalid = xgb.DMatrix(X_test, label=y_test)","8b5dfbb5":"kf = KFold(n_splits=5, shuffle=False)\n\nX_train = X_train.values\ny_train = y_train.values\nscores = []\n\nfor train, test in kf.split(X_train, y_train):\n    model = XGBClassifier(n_estimators=1000, learning_rate=0.05, max_depth = 10)\n    model.fit(X_train[train], y_train[train])\n    scores.append(model.score(X_train[test], y_train[test]))","b5ff097a":"def objective(trial):\n    params = {\n        'booster':trial.suggest_categorical('booster', ['gbtree', 'dart', 'gblinear']),\n        'learning_rate':trial.suggest_loguniform(\"learning_rate\", 0.01, 0.1),\n        'max_depth':trial.suggest_int(\"max_depth\", 3, 11),\n        'subsample':trial.suggest_uniform(\"subsample\", 0.0, 1.0),\n        'colsample_bytree':trial.suggest_uniform(\"colsample_bytree\", 0.0, 1.0),\n    }\n\n    model = XGBClassifier(**params)\n    cv = KFold(n_splits=3, shuffle=True, random_state=None)\n    scorer = make_scorer(f1_score, greater_is_better=True)\n    \n    bst = xgb.train(params, dtrain)\n    preds = bst.predict(dvalid)\n    pred_labels = np.rint(preds)\n    f1_scores = f1_score(y_test, pred_labels, average='micro')\n    return f1_scores","ad7ca363":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=100, timeout=600)","dfff7256":"new_params = study.best_params\n\nnew_model = XGBClassifier(**new_params)\nnew_model.fit(X, y)\npreds = new_model.predict(X_test)\n\nprint('Optimized SuperLearner accuracy: ', accuracy_score(y_test, preds))\nprint('Optimized SuperLearner f1-score: ', f1_score(y_test, preds, average='micro'))","a51b2760":"print(\"Number of finished trials: \", len(study.trials))\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","9913040f":"print(\"All of accuracies\")\nprint(scores)\n\nprint(\"Mean of accuracies\")\nprint(np.mean(scores))","cce56720":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","17eab43a":"y_pred = new_model.predict(X)\ncm = confusion_matrix(train_df['target'], y_pred)\nnp.set_printoptions(precision=2)\n\nclass_names = ['low', 'medium', 'high']\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cm, classes=class_names,\n                      title='Confusion matrix')","cffd07a7":"# define the validation scheme\ncv = KFold(n_splits=4, shuffle=True, random_state=0)\nscorer = make_scorer(mean_absolute_error, greater_is_better=False)\n# define the binary target and the features\ntarget = \"target\"\nfeatures = [col for col in train_df.columns if col != target]\ndataset = Dataset(df=train_df, target=target, features=features)\n# define the validation scheme and scorer. The default model is LightGBM\nlofo_imp = LOFOImportance(dataset, scoring=scorer, model=new_model, cv=cv)\n\n# get the mean and standard deviation of the importances in pandas format\nimportance_df = lofo_imp.get_importance()\n\n# plot the means and standard deviations of the importances\nplot_importance(importance_df)","6565ba19":"### FEATURE IMPORTANCE BY LOFO","75cc0cd2":"**Our target 'INTEREST LEVEL' is an object as we can see above.\n\n**Let's convert to the numeric to analyze easily\n\n* 0 : low\n* 1 : medium\n* 2 : high","3565d1f4":"### MOST FREQUENT FEATURES EXTRACTION","c35bd795":"### BASIC FEATURES","91a02554":"## XGBOOST","bff21e58":"- As we can see low level is highly more than other interest levels","ce71974c":"### DATA VISUALIZATION","1c84f27d":"### LABEL ECONDING FOR CATEGORICAL VARIABLES","05d79bc9":"### DROP UNNECESSARY COLUMNS","ab0d4d78":"### APPLY SAME OPERATIONS TO THE TEST DATA","edded8d7":"- WORDCLOUD SHOWS US MOST FREQUENT WORDS IN THE DATASET, DEPENDS ON THE FREQUENCY WORDS SIZE IS GETTING BIGGER","36dce24f":"### Features after extraction"}}