{"cell_type":{"2577b8f0":"code","9cff8987":"code","253b3a98":"code","1916f8fe":"code","8b6e1d85":"code","8d6994fa":"code","e774f1e5":"code","1f027d54":"code","c21f4601":"code","0f087c51":"code","041d1b45":"code","4c04e68e":"code","f056f5ca":"code","69cbe4bc":"code","c59b19a7":"code","daef2370":"code","4719d1de":"code","73ffec8c":"code","e663dcea":"code","92ddf3d8":"code","4389e58e":"code","94c2b372":"code","5cbb2ab1":"code","89f443ec":"code","9b20a41f":"code","7caf6bd0":"code","3860eb4d":"code","91b9ba3f":"code","7d4dc284":"code","be6833c3":"code","b689b141":"code","46acf648":"code","f446c884":"code","cb308510":"code","ff20a997":"code","5232a062":"code","83d16c59":"code","898f8b64":"code","411bd181":"code","9eab2119":"code","3118fe7d":"code","856b32b7":"code","55ff1f94":"code","ef16e5af":"markdown","969591af":"markdown","345399a8":"markdown","26cb85b7":"markdown","d5514498":"markdown","a5b20882":"markdown","35c6813a":"markdown","427b60ee":"markdown","445cc6e7":"markdown","6366d7dd":"markdown","c104af4d":"markdown","b6d2ce8e":"markdown","e5e1268b":"markdown","f8ebb082":"markdown","26b9a62a":"markdown","c5488c12":"markdown","f1bd730e":"markdown","9fb32d74":"markdown","b122f5cc":"markdown","fe0f421b":"markdown","fb1e46fc":"markdown","76be4df1":"markdown","a91df3ae":"markdown","ef517d66":"markdown","4c55ffef":"markdown","696ff444":"markdown","4e2da5b0":"markdown","68c2ecb5":"markdown","ee503797":"markdown","b7543e55":"markdown","b617d6bd":"markdown","68b4f2ac":"markdown","9acdc463":"markdown","291b0dcc":"markdown"},"source":{"2577b8f0":"!pip install langdetect \n!pip install pyspark\n!pip install tqdm","9cff8987":"import numpy as np \nimport pandas as pd \nfrom pandas import DataFrame\nimport os, json\nimport glob\nimport csv\nfrom sklearn.feature_extraction import text\nimport string\nfrom tqdm import tqdm\nfrom langdetect import detect\nfrom langdetect import DetectorFactory\nimport sys\nimport string\nstring.punctuation\nfrom pprint import pprint","253b3a98":"metadataDF = pd.read_csv('..\/input\/CORD-19-research-challenge\/metadata.csv')","1916f8fe":"metadataDF.head(2)","8b6e1d85":"metadataDF.info()","8d6994fa":"metadataDF.head(3)","e774f1e5":"len(metadataDF)","1f027d54":"#drop rows that have empty pdf_json cels\nmetadataDF.dropna(subset=['pdf_json_files'], inplace=True)\n\n#create a new df with first 1000 rows\ncovid_df = metadataDF.sample(int(len(metadataDF) *0.03))\ncovid_df=metadataDF\nprint('Sample Data Rows:', len(covid_df))","c21f4601":"covid_df.info()","0f087c51":"#covid_df['pdf_json_files'] = covid_df['pdf_json_files'].str.rstrip('.json ')\ncovid_df[\"pdf_json_files\"] = covid_df[\"pdf_json_files\"].str.split(\";\").str[0]\ncovid_df.head(3)","041d1b45":"def format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    for section, text in texts:\n        texts_di[section] += text\n    body = \"\"\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    body.encode('utf-8')\n    return body","4c04e68e":"path_to_json ='..\/input\/CORD-19-research-challenge\/'\nbody_text = []\nfor filename in covid_df['pdf_json_files']:\n    filename = path_to_json + filename \n    my_json_file = json.load(open(filename, 'r'))\n    body_text.append(format_body(my_json_file['body_text']))\ncovid_df['body_text'] = body_text\ncovid_df.head(3)","f056f5ca":"covid_df.info()","69cbe4bc":"SelectedFeature = covid_df[['cord_uid','title','abstract','body_text','doi', 'authors', 'journal', 'publish_time']]\nSelectedFeature.info()\nlen(SelectedFeature) ","c59b19a7":"SelectedFeature=SelectedFeature.drop_duplicates(['cord_uid'])\nlen(SelectedFeature)","daef2370":"SelectedFeature.dropna(inplace = True)\nlen(SelectedFeature)","4719d1de":"# set seed\nDetectorFactory.seed = 0\n# hold label - language\nlanguages = []\n# go through each text\nfor ii in tqdm(range(0,len(SelectedFeature))):\n    # split by space into list, take the first x intex, join with space\n    text = SelectedFeature.iloc[ii]['body_text'].split(\" \")\n    lang = \"en\"\n    try:\n        if len(text) > 50:\n            lang = detect(\" \".join(text[:50]))\n        elif len(text) > 0:\n            lang = detect(\" \".join(text[:len(text)]))\n    # ught... beginning of the document was not in a good format\n    except Exception as e:\n        all_words = set(text)\n        try:\n            lang = detect(\" \".join(all_words))\n        # what!! :( let's see if we can find any text in abstract...\n        except Exception as e: \n            try:\n                # let's try to label it through the abstract then\n                lang = detect(df.iloc[ii]['abstract_summary'])\n            except Exception as e:\n                lang = \"unknown\"\n                pass\n    \n    # get the language    \n    languages.append(lang)","73ffec8c":"languages_dict = {}\nfor lang in set(languages):\n    languages_dict[lang]= languages.count(lang)\n    \nprint(\"Total: {}\\n\".format(len(languages)))\npprint(languages_dict)","e663dcea":"SelectedFeature['language'] = languages","92ddf3d8":"SelectedFeature.head(3)","4389e58e":"SelectedFeatureEN = SelectedFeature[SelectedFeature['language'] == 'en']\nSelectedFeatureEN = SelectedFeatureEN[['cord_uid','title','abstract','body_text', 'doi', 'authors', 'journal', 'publish_time']]\nSelectedFeatureEN.info()","94c2b372":"def remove_punctuations(text):\n    for punctuation in string.punctuation:\n        text = text.replace(punctuation, '')\n    return text\n# Apply to the DF series\nSelectedFeatureEN['title'] = SelectedFeatureEN['title'].apply(remove_punctuations)\nSelectedFeatureEN['abstract'] = SelectedFeatureEN['abstract'].apply(remove_punctuations)\nSelectedFeatureEN['body_text'] = SelectedFeatureEN['body_text'].apply(remove_punctuations)\nSelectedFeatureEN.head(3) ","5cbb2ab1":"SelectedFeatureEN['title']=SelectedFeatureEN['title'].str.lower()\nSelectedFeatureEN['abstract']=SelectedFeatureEN['abstract'].str.lower()\nSelectedFeatureEN['body_text']=SelectedFeatureEN['body_text'].str.lower()\nSelectedFeatureEN.head(3) ","89f443ec":"from sklearn.feature_extraction import text\nimport string\nSTOP_WORDS = text.ENGLISH_STOP_WORDS\npunctuations = string.punctuation\nStopWords = list(STOP_WORDS)\n\nCustomStopWords = [\n    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n    'al.', 'Elsevier', 'PMC', 'CZI', 'www'\n]\n\nfor word in CustomStopWords:\n    if word not in StopWords:\n        StopWords.append(word)\n        \nSelectedFeatureEN['title'] = SelectedFeatureEN['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (StopWords)]))\nSelectedFeatureEN['abstract'] = SelectedFeatureEN['abstract'].apply(lambda x: ' '.join([word for word in x.split() if word not in (StopWords)]))\nSelectedFeatureEN['body_text'] = SelectedFeatureEN['body_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (StopWords)]))\n\n\nSelectedFeatureEN.head(3)","9b20a41f":"SelectedFeatureEN.to_parquet(\".\/Newdf.parquet\")","7caf6bd0":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import * \nfrom pyspark.ml.feature import StringIndexer ,OneHotEncoder,VectorAssembler\nfrom pyspark.ml import Pipeline\n\n\nspark = SparkSession \\\n    .builder \\\n    .appName('Covid') \\\n    .getOrCreate()","3860eb4d":"spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")\nprint(\"spark.sql.shuffle.partitions\",spark.conf.get('spark.sql.shuffle.partitions'))\nprint(\"spark.sparkContext.defaultParallelism\",spark.sparkContext.defaultParallelism)\nprint(\"spark.sql.files.maxPartitionBytes\" , spark.conf.get('spark.sql.files.maxPartitionBytes'))\nprint(\"spark.sql.files.maxRecordsPerFile\",spark.conf.get('spark.sql.files.maxRecordsPerFile'))","91b9ba3f":"#DataSet =spark.read.parquet(\"..\/input\/dataoutput\/Newdf.parquet\").repartition(4)\nDataSet =spark.read.parquet(\".\/Newdf.parquet\").repartition(4)\nDataSet.printSchema()","7d4dc284":"print(\"Number of Partitions\",DataSet.rdd.getNumPartitions())\nDataSet.select(spark_partition_id().alias(\"partitionId\")).groupBy(\"partitionId\").count().show()","be6833c3":"DataSet = DataSet.select('cord_uid','title','abstract','body_text', 'doi', 'authors', 'journal', 'publish_time')\\\n                .toDF(\"Id\",\"Title\",\"Abstract\",\"Body\",'Doi', 'Authors', 'Journal', 'PublishTime')\n\n#'doi', 'authors', 'journal', 'publish_time'\nDataSet.limit(3).toPandas()","b689b141":"for thisCol in DataSet.columns :\n    print('The Count of Null in Column %s ' %thisCol , DataSet.where(col(thisCol).isNull()).count() )","46acf648":"'''\n\nTF: HashingTF is a Transformer which takes sets of terms and converts those sets into fixed-length feature vectors. \nIn text processing, a \u201cset of terms\u201d might be a bag of words. The algorithm combines Term Frequency (TF) counts with the hashing \ntrick for dimensionality reduction.\n\nIDF: IDF is an Estimator which fits on a dataset and produces an IDFModel. The IDFModel takes feature vectors (generally \ncreated from HashingTF) and scales each column. Intuitively, it down-weights columns which appear frequently in a corpus.\n\nPlease refer to the MLlib user guide on TF-IDF for more details on Term Frequency and Inverse Document Frequency. For API \ndetails, refer to the HashingTF API docs and the IDF API docs.\n\nIn the following code segment, we start with a set of sentences. We split each sentence into words using Tokenizer. For each \nsentence (bag of words), we use HashingTF to hash the sentence into a feature vector. We use IDF to rescale the feature vectors; \nthis generally improves performance when using text as features. Our feature vectors could then be passed to a learning algorithm.\n'''\n\n\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer\nrescaledData=DataSet\nfor thisCol in rescaledData.select('Title','Abstract','Body').columns :\n    OutCol =thisCol+'Vector'\n    tokenizer = Tokenizer(inputCol=thisCol , outputCol=\"words\")\n    wordsData = tokenizer.transform(rescaledData)\n\n    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n    featurizedData = hashingTF.transform(wordsData)\n\n    idf = IDF(inputCol=\"rawFeatures\", outputCol=OutCol)\n    idfModel = idf.fit(featurizedData)\n    rescaledData = idfModel.transform(featurizedData)\n\n    rescaledData=rescaledData.drop('words','rawFeatures')  \n    ","f446c884":"rescaledData=rescaledData.withColumn('Day_PublishTime' ,  substring('PublishTime', 9,2).cast(IntegerType()))\\\n                        .withColumn('Month_PublishTime' ,  substring('PublishTime', 6,2).cast(IntegerType()))\\\n                        .withColumn('Year_PublishTime' ,  substring('PublishTime', 0,4).cast(IntegerType()))\n    \nrescaledData.limit(3).toPandas()","cb308510":"for thisCol in rescaledData.select('Doi','Authors','Journal').columns :\n    rescaledData = StringIndexer(\n    inputCol=thisCol, \n    outputCol=thisCol+'Index', \n    handleInvalid='skip').fit(rescaledData).transform(rescaledData)\n    \nrescaledData.limit(3).toPandas()","ff20a997":"'''\nrequiredFeatures = ['TitleVector',\n                    'AbstractVector',\n                    'BodyVector',\n                    'Day_PublishTime',\n                    'Month_PublishTime',\n                    'Year_PublishTime',\n                    'DoiIndex',\n                    'AuthorsIndex',\n                    'JournalIndex']\n'''\n\nrequiredFeatures = ['TitleVector',\n                    'AbstractVector',\n                    'BodyVector',\n                    'Year_PublishTime',\n                    'AuthorsIndex',\n                    'JournalIndex']","5232a062":"from pyspark.ml.feature import VectorAssembler\nassembler = VectorAssembler(inputCols=requiredFeatures, outputCol='features', handleInvalid = \"skip\")","83d16c59":"transformed_data = assembler.transform(rescaledData)\ntransformed_data.toPandas().head(3)","898f8b64":"from pyspark.ml.clustering import KMeans\n\nkmeans = KMeans(k=2, seed=3)\nmodel = kmeans.fit(transformed_data)","411bd181":"clusterdData = model.transform(transformed_data)","9eab2119":"from pyspark.ml.evaluation import ClusteringEvaluator\nimport pylab as pl\n\nevaluator = ClusteringEvaluator()\nsilhouette = evaluator.evaluate(clusterdData)\n","3118fe7d":"print('Silhouette with squared euclidean distance = ', silhouette)","856b32b7":"cost = np.zeros(10)\n\nfor k in range(2,10):\n    kmeans = KMeans(k=k, seed=3)\n    model = kmeans.fit(transformed_data)\n    clusterdData = model.transform(transformed_data)\n    evaluator = ClusteringEvaluator()\n    cost[k] = evaluator.evaluate(clusterdData)\n\n# Plot the cost\ndf_cost = pd.DataFrame(cost[2:])\ndf_cost.columns = [\"cost\"]\nnew_col = [2,3,4,5,6,7,8,9]\ndf_cost.insert(0, 'cluster', new_col)","55ff1f94":"pl.plot(df_cost.cluster, df_cost.cost)\npl.xlabel('Number of Clusters')\npl.ylabel('Score')\npl.title('Silhoutee curve')\npl.show()","ef16e5af":"#### Drop Null values of the Column pdf_json_files \n#### Take 3% as a sample of the Data to Proceed withit ","969591af":"#### Set all words in lower case","345399a8":"# Clustering","26cb85b7":"#### Transform texts to Vector","d5514498":"#### Create the clusters using the model","a5b20882":"# Exploratory the Data ","35c6813a":"#### Create a VectorAssembler that transform the combines a given list of columns into a single vector column","427b60ee":"#### Transform string columns to index","445cc6e7":"#### Saving Trasformed Data as Parquet","6366d7dd":"# Preparation and Cleaning the data","c104af4d":"#### Rename the Dataframe Columns","b6d2ce8e":"##### Select columns that will be used in the trasformation","e5e1268b":"#### Divide Date column into three column Day ,Month , Year","f8ebb082":"##### Clean Values of Column pdf_json_files to extract the path of the json file","26b9a62a":"#### Drop Duplicated values","c5488c12":"#### Filter the Article in English Languge","f1bd730e":"#### Use ClusteringEvaluator to evaluate the clusters","9fb32d74":"# Import Required Library","b122f5cc":"# Read Metadata File","fe0f421b":"#### Define the function to remove the punctuation","fb1e46fc":"# Install Required Library","76be4df1":"#### Drop Null values","a91df3ae":"#### Transorm our dataset for use in our clustering algorithm","ef517d66":"##### Function to format body_text into block of text","4c55ffef":"#### Select Features that will be part of model","696ff444":"#### Check the Null values","4e2da5b0":"# Vectorization","68c2ecb5":"##### Load json files and extract the body then add it as a column in the dataset","ee503797":"#### Determine the Languge of each article","b7543e55":"#### Remove Comman\/Custom Stop Words","b617d6bd":"#### Choose number of clusters k","68b4f2ac":"#### Load trasformed Data in spark Data frame in 4 Partitions","9acdc463":"#### Add Column Languge to the Dataframe ","291b0dcc":"##### Showing the count of each languge article exist in dataset"}}