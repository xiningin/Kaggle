{"cell_type":{"713e1dd7":"code","1767f3c0":"code","62122544":"code","0de59702":"code","9b010783":"code","58b550f4":"code","586ed81e":"code","8248a1b8":"code","b41f4d1e":"code","822839c9":"code","bcf99d03":"code","afb89a1f":"code","9250fb3f":"code","335d22c9":"code","86801772":"code","c8ae9093":"code","fdbf2b78":"markdown","8dda95c8":"markdown","18937743":"markdown","dcdfb566":"markdown","0f754c01":"markdown","0a4aa32c":"markdown","a384464f":"markdown","e3a0c094":"markdown","854418be":"markdown"},"source":{"713e1dd7":"import os\nfrom time import time\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nimport warnings\nwarnings.simplefilter(action='ignore')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline","1767f3c0":"# config\nDATA_DIR = '..\/input\/tweet-sentiment-extraction'\nTRAIN_DATA_FILE = 'train.csv'\nTEST_DATA_FILE = 'test.csv'\nSUBMISSION_FILE = 'submission.csv'\n\nRANDOM_STATE = 0","62122544":"train_data = pd.read_csv(os.path.join(DATA_DIR, TRAIN_DATA_FILE)).fillna('')\ntest_data = pd.read_csv(os.path.join(DATA_DIR, TEST_DATA_FILE)).fillna('')","0de59702":"train_data = train_data[['textID', 'text', 'sentiment', 'selected_text']]\ntrain_data[17:22]","9b010783":"test_data.head()","58b550f4":"def create_model():\n    clf = Pipeline([\n        ('vect', CountVectorizer(tokenizer=lambda x: x.split())\n        ),\n        ('clf', LogisticRegression(\n                                   max_iter=1000,\n                                   random_state=RANDOM_STATE\n                                  )\n        )\n    ])\n    return clf\n\nX = pd.concat([train_data['text'], test_data['text']], axis=0)\nY = pd.concat([train_data['sentiment'], test_data['sentiment']], axis=0)\nY = Y.map({'negative': -1, 'positive': 1, 'neutral': 0})\nprint(X.shape, Y.shape)\n\nmodel = create_model()\n\nmodel.fit(X, Y)\n\npred = model.predict(X)\nprint(classification_report(y_true=Y, y_pred=pred))","586ed81e":"# Grid search with CV doesn't seem to make sense here because the model\n# is not used to make predictions on the test data\n# parameters = {\n#     'clf__C': [0.001, 0.009, 0.01, 0.09, 1, 5, 10, 25],\n#     'clf__class_weight': [None, 'balanced']\n# }\n\n# gs_clf = GridSearchCV(model,\n#                       parameters,\n#                       cv=5,\n#                       verbose=2,\n#                       n_jobs=-1\n#                      )\n\n# gs_clf = gs_clf.fit(X, Y)\n\n# gs_clf.best_params_","8248a1b8":"VOCAB = model.named_steps['vect'].vocabulary_\nCOEF = model.named_steps['clf'].coef_\n\nclass WordSelector(BaseEstimator, TransformerMixin):\n    \"\"\"Provide a class to select words supporing the sentiment.\"\"\"\n    \n    def __init__(self, pos_class_std=2.2, neg_class_std=2.2):\n        # number of standard deviations to select words \n        # with unusual weights from a tweet\n        self.pos_class_std = pos_class_std\n        self.neg_class_std = neg_class_std\n        \n        self.vocabulary_ = VOCAB  # word vocab from CountVectorizer\n        self.coef_ = COEF         # word importance weight from LogRegression\n        \n        self.weights_by_classes = {\n            'negative': list(enumerate(self.coef_[0])),\n            'neutral':  list(enumerate(self.coef_[1])),\n            'positive': list(enumerate(self.coef_[2]))\n        }\n        \n        # Translation dict from vocab indexes to words\n        # Only used for plotting\n        self.index_to_word = {\n            ind: word\n            for (word, ind)\n            in self.vocabulary_.items()\n            }\n    \n    ##### PLOTTING-RELATED METHODS###############################\n    def get_words_from_idx(self, indexes):\n        \"\"\"Return a list of words from indexes for plotting.\"\"\"\n        return [self.index_to_word[index] for index in indexes]\n\n    \n    def plot_top_features(self, class_label, max_top_feat):\n        \"\"\"Plot a bar chart of top features for a given label.\"\"\"\n        idx_coef_list = sorted(self.weights_by_classes[class_label],\n                               key=lambda pair: pair[1], \n                               reverse=True\n                              )\n        idx, coef = zip(*idx_coef_list)\n        top_words = self.get_words_from_idx(idx[:max_top_feat])\n        plt.figure(figsize=(12,4))\n        plt.bar(top_words, coef[:max_top_feat])\n        plt.title(f'Top-{max_top_feat} features in category {class_label}')\n        plt.xlabel('Features')\n        plt.ylabel('Weights')\n        plt.xticks(rotation = '45')\n        plt.show()\n    ############################################################\n    \n    \n    def get_weights(self, text_list, class_weights):\n        \"\"\"Return a list of weights for text.\"\"\"\n        text_idx = [self.vocabulary_[tok] for tok in text_list]\n\n        return [class_weights[idx][1] for idx in text_idx]\n\n    def get_top_words(self, words_list, weights_list, num_std):\n        \"\"\"Return a string of words with unusually high weights.\"\"\"\n        mean = np.mean(weights_list)\n        std = np.std(weights_list)\n        top_words = []\n        for word, weight in zip(words_list, weights_list):\n            if weight > (mean +  num_std * std):\n                top_words.append(word)\n        return ' '.join(top_words)\n\n\n    def select_words(self, text_sentiment):\n        \"\"\"Select words given sentiment label by calling get_top_words().\"\"\"\n        text, sentiment = text_sentiment\n\n        if sentiment == 'neutral':\n            return text\n\n        text = text.lower().split()\n        weights = self.get_weights(text,\n                              self.weights_by_classes[sentiment]  \n                             )\n\n        if sentiment == 'positive':\n            res = self.get_top_words(text, weights, num_std=self.pos_class_std)\n\n        if sentiment == 'negative':\n            res = self.get_top_words(text, weights, num_std=self.neg_class_std)\n\n        if res == '':\n            res = ' '.join(text)\n\n        return res\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def predict(self, X):\n        \"\"\"Return predicted 'selected_text' as a Pandas series.\n        \n        X: Pandas dataframe with columns 'text', sentiment'\n        \"\"\"\n\n        res = pd.DataFrame()\n        res['sentiment'] = X['sentiment']\n        res['text'] = X['text'].map(lambda s: s.lower())\n        res['selected_text'] = X[['text', 'sentiment']].apply(self.select_words, axis=1)\n\n        return res['selected_text']\n    \n    def jaccard(self, predicted_selected):\n        \"\"\"Provide an evaluation metric.\n        \n        predicted_selected: a tuple (predicted text, true selected text)\n        \"\"\"\n        str1, str2 = predicted_selected\n        a = set(str1.lower().split()) \n        b = set(str2.lower().split())\n        if (len(a) == 0) & (len(b) == 0):\n            return 0.5\n        c = a.intersection(b)\n        return float(len(c)) \/ (len(a) + len(b) - len(c))\n\n    def score(self, X, y):\n        \"\"\"Return a mean Jaccard score.\n        \n        X: Pandas dataframe with columns 'text', sentiment'\n        y: Pandas series containing true 'selected_text'\n        \"\"\"\n        \n        res = pd.DataFrame()\n        \n        res['selected_text'] = y\n        res['sentiment'] = X['sentiment']\n        res['text'] = X['text']\n\n        res['predictions'] = self.predict(res[['text', 'sentiment']])\n        \n        res['score'] = res[['predictions', 'selected_text']].apply(self.jaccard, axis=1)\n\n        return res['score'].mean()","b41f4d1e":"word_selector = WordSelector(pos_class_std=1.8, neg_class_std=2.4)\n\nprint('Jaccard score for training data:')\nword_selector.score(train_data[['text', 'sentiment']], train_data['selected_text'])\n\n# [0. , 0.8, 1.6, 2.4, 3.2, 4. ]\n# pos_class_std=1.6, neg_class_std=2.4\n# 0.6535060973409171\n\n# [1. , 1.4, 1.8, 2.2, 2.6, 3. ]\n# {'neg_class_std': 2.2, 'pos_class_std': 1.8}\n# 0.6552466246349189\n\n#     'pos_class_std': [1.6, 1.7, 1.8, 1.9],  \n#     'neg_class_std': [2.1, 2.2, 2.3, 2.4]\n# {'neg_class_std': 2.4, 'pos_class_std': 1.8}\n# 0.6553365154317554","822839c9":"# Jaccard score by sentiment\ntemp_df = pd.DataFrame()\ntemp_df['sentiment'] = train_data['sentiment']\ntemp_df['text'] = train_data['text']  #.map(lambda s: s.lower())\n\ntemp_df['predicted_text'] = word_selector.predict(train_data[['text', 'sentiment']])\n\ntemp_df['selected_text'] = train_data['selected_text']\n\ntemp_df['score'] = temp_df[['predicted_text', 'selected_text']].apply(word_selector.jaccard, axis=1)\n\nprint('Jaccard score by sentiment:')\nprint(temp_df.groupby('sentiment')['score'].mean())\nprint('\\nTotal score:', temp_df['score'].mean())","bcf99d03":"for sentiment in ['negative', 'neutral', 'positive']:\n    word_selector.plot_top_features(sentiment, 20)","afb89a1f":"# print('Performing GridSearch for WordSelect parameters...')\n# parameters = {\n#     'pos_class_std': [1.6, 1.7, 1.8, 1.9],  \n#     'neg_class_std': [2.1, 2.2, 2.3, 2.4]\n# }\n\n# gs = GridSearchCV(WordSelector(\n#                               ),\n#                   parameters,\n#                   cv=5,\n#                   verbose=1,\n#                   n_jobs=-1\n#                  )\n\n\n# gs = gs.fit(train_data[['text', 'sentiment']], train_data['selected_text'])\n\n# gs.best_params_","9250fb3f":"submission_df = pd.DataFrame() \nsubmission_df['textID'] = test_data['textID']\n\nsubmission_df['selected_text']= word_selector.predict(test_data[['text', 'sentiment']])\nsubmission_df.to_csv(SUBMISSION_FILE, index = False)","335d22c9":"pd.set_option('max_colwidth', 80)\ntest_data.head()","86801772":"submission_df.head(5)","c8ae9093":"plt.hist(train_data['selected_text'].map(len), alpha=0.8, bins=40)\nplt.title('Distribution of Lengths of TRUE Substrings in Training Data')\nplt.xlabel('Char length')\nplt.ylabel('How often')\nplt.show()\n\nplt.hist(temp_df['predicted_text'].map(len), alpha=0.8, bins=40)\nplt.title('Distribution of Lengths of PREDICTED Substrings in Training Data')\nplt.xlabel('Char length')\nplt.ylabel('How often')\nplt.show()\n\nplt.hist(submission_df['selected_text'].map(len), alpha=0.8, bins=40)\nplt.title('Distribution of Lengths of PREDICTED Substrings in Testing Data')\nplt.xlabel('Char length')\nplt.ylabel('How often')\nplt.show()","fdbf2b78":"\n                                Count           Logistic\n                                Vectorizer      Regression\n\n    +----------------------+   +-----------+   +-----------+         +------------+\n    | Features  |  Targets |   |           |   |           |  word   |    Most    |\n    +----------------------+-->| Vectorizer|-->| Classifier|-------->| important  |\n    | Tweets    | Sentiment|   |           |   |           | weights |    words   |\n    +----------------------+   +-----------+   +-----------+         +------------+\n","8dda95c8":"## Grid Search for Text-Sentiment Classifier","18937743":"## Word Selector Model\n\nImplementing the word selection process as a custom estimator class allows using GridSearchCV to find best parameters ","dcdfb566":"## Load Data","0f754c01":"## Length Distributions -- True and Predicted Substrings in Training Data & Predicted Substrings in Testing Data","0a4aa32c":"## Kaggle submission","a384464f":"# Sentiment Extraction -- Assign Weights & Select Most Important Words","e3a0c094":"## Text-Sentiment Classification Model","854418be":"## Grid Search for Word Selector Model"}}