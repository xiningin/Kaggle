{"cell_type":{"fc9a277c":"code","d4fb2688":"code","49c09382":"code","c7b2c080":"code","53a1b039":"code","c2a1c0cf":"code","78717cf9":"code","d0a8210e":"code","61c94092":"code","7fa27a70":"code","c9717b03":"code","fa87c7af":"code","ab3efa6e":"code","6e4f7d1a":"code","f2c8db11":"code","33f0787a":"code","248ed522":"code","f7b3cbee":"code","8a4b7ab8":"code","02b81876":"code","29f90a1d":"code","de505d17":"code","511ebd09":"code","cb95823d":"code","4d3ad6dd":"code","0a77a2ab":"code","8bcd7688":"code","113687df":"code","33402b9c":"code","3f822856":"code","bd84880b":"code","42d91f5c":"code","b3fe420a":"code","aa61a158":"code","1bbd7032":"code","79d9a15d":"code","212389f4":"code","c639c857":"code","8465be4d":"code","c8cfed0c":"code","904968a2":"code","ab33e835":"code","7f567f20":"code","1198a456":"code","679c702c":"code","a5c2839b":"code","f8726169":"code","ffd8eb82":"code","0cc9004f":"code","e6958cff":"code","d0ff956b":"code","72cbab63":"code","daa3e923":"code","40eb42b1":"code","989265fe":"code","7012dd82":"code","bf423f93":"code","6d2d2095":"code","c26d5360":"code","4d2b517e":"code","b78a5f03":"markdown","9af23dc1":"markdown","fd499369":"markdown","aa4650d3":"markdown","94140720":"markdown","e304d6b4":"markdown","a2859d38":"markdown"},"source":{"fc9a277c":"import numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn import tree\nfrom IPython.display import Image as PImage\nfrom subprocess import check_call\nfrom PIL import Image, ImageDraw, ImageFont\nimport re\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, learning_curve, train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (accuracy_score, log_loss, classification_report,f1_score,confusion_matrix)\nimport xgboost\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,Imputer,LabelEncoder,OneHotEncoder\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score,roc_curve\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nimport scipy\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier","d4fb2688":"df = pd.read_csv('\/kaggle\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')","49c09382":"df.columns","c7b2c080":"df.head()","53a1b039":"df.info()","c2a1c0cf":"df.describe()","78717cf9":"#count of categories in categorial columns\ndef printCategoryCounts():\n    for col, value in df.iteritems():\n        if value.dtype == 'object':\n            print(df[col].value_counts())\n            print(\"=========\")\n            \nprintCategoryCounts()","d0a8210e":"#distribution plot for numerical features\nfig,ax = plt.subplots(8,3, figsize=(20,35))\ni = 0\nj = 0\nfor col, value in df.iteritems():\n        if value.dtype != 'object' and col != 'EmployeeCount' and col != 'StandardHours' :\n            sns.distplot(df[col], ax = ax[i,j],color='orange')\n            j = j +1\n            if j==3:\n                j = 0\n                i = i + 1\n\nplt.show()","61c94092":"sns.set(style=\"darkgrid\")\nplt.figure(figsize=(10,8))\ntotal = float(len(df)) \nax = sns.countplot(x=\"JobLevel\", hue=\"Attrition\", data=df) # for Seaborn version 0.7 and more\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format((height\/total)*100),\n            ha=\"center\") \nplt.show()","7fa27a70":"fig,ax = plt.subplots(5,2, figsize=(25,35))\ni = 0\nj = 0\nfor col, value in df.iteritems():\n        if value.dtype == 'object':\n            ax1 = sns.countplot(data=df,x= col,hue=\"Attrition\", ax = ax[i,j])\n            for p in ax1.patches:\n                height = p.get_height()\n                ax1.text(p.get_x()+p.get_width()\/2.,height + 3,\n                '{:1.2f}%'.format((height\/total)*100),\n                ha=\"center\") \n            j = j +1\n            if j==2:\n                j = 0\n                i = i + 1\ni = 0\nfor ax in fig.axes:\n    if i == 2 or i==3 or i==5:\n        plt.sca(ax)\n        plt.xticks(rotation=90)\n    i = i +1\nplt.subplots_adjust(bottom=-0.2)\nplt.show()","c9717b03":"#Pair Plot\ncont_col= ['Attrition','Age','MonthlyIncome', 'JobLevel','DistanceFromHome']\nsns.pairplot(df[cont_col],  kind=\"reg\", diag_kind = \"kde\"  , hue = 'Attrition' )\nplt.show()","fa87c7af":"#box plot\nfig,ax = plt.subplots(2,2, figsize=(10,10))                       \nsns.boxplot(df['Attrition'], df['MonthlyIncome'], ax = ax[0,0]) \nsns.boxplot(df['Gender'], df['MonthlyIncome'], ax = ax[0,1])\nplt.xticks( rotation=90)\nsns.boxplot(df['Department'], df['MonthlyIncome'], ax = ax[1,0]) \nplt.xticks( rotation=90)\nsns.boxplot(df['JobRole'], df['MonthlyIncome'], ax = ax[1,1])\nplt.show() ","ab3efa6e":"plt.figure(figsize=(12,8))\nsns.heatmap(data=df.corr(),cmap=\"YlGnBu\")","6e4f7d1a":"#convert object columns with hotencoding\ncategorical = []\nfor col, value in df.iteritems():\n    if value.dtype == 'object':\n        categorical.append(col)\nnumerical = df.columns.difference(categorical)\nattrition_cat = df[categorical]\nattrition_cat = attrition_cat.drop(['Attrition'], axis=1)\nattrition_cat = pd.get_dummies(attrition_cat)\nattrition_num = df[numerical]\ndf_final = pd.concat([attrition_num, attrition_cat], axis=1)","f2c8db11":"#Check for outlier in numerical\nQ1 = attrition_num.quantile(0.25)\nQ3 = attrition_num.quantile(0.75)\nIQR = Q3 - Q1\n((attrition_num < (Q1 - 1.5 * IQR)) | (attrition_num > (Q3 + 1.5 * IQR))).sum()","33f0787a":"#Encode target\ntarget_map = {'Yes':1, 'No':0}\ntarget = df[\"Attrition\"].apply(lambda x: target_map[x])\ntarget.head(3)","248ed522":"#dropping columns which are not very significant\ndf_final.drop(columns=['StandardHours','Over18_Y','EmployeeCount'],inplace=True)","f7b3cbee":"#Scaling\nfrom imblearn.over_sampling import SMOTE\nscaler=StandardScaler()\nscaled_df=scaler.fit_transform(df_final)\nX=scaled_df\nY=target\nSMOTE().fit_resample(X, Y)\nX,Y = SMOTE().fit_resample(X, Y)\n#split data\ntrain, test, target_train, target_val = train_test_split(X, \n                                                         Y, \n                                                         train_size= 0.80,\n                                                         random_state=0);","8a4b7ab8":"#Using multiple classifiers\nModel = []\nAccuracy= []\nF1Score = []\nSen = []\nSpe = []\nFPR = []\nFNR = []","02b81876":"def calculateScore(confMat):\n    TP = confMat[0][0]\n    TN = confMat[1][1]\n    FP = confMat[0][1]\n    FN = confMat[1][0]\n    Sen.append(TP \/ (TP + FN))\n    Spe.append(TN \/ (FP + TN))\n    FPR.append(FP \/ (FP + TN))\n    FNR.append(FN \/ (FN + TP))","29f90a1d":"LR = LogisticRegression(multi_class='auto')\nLR.fit(train,target_train)\nlr_pred = LR.predict(test)\nModel.append('Logistic Regression')\nAccuracy.append(accuracy_score(target_val,lr_pred))\nF1Score.append(f1_score(target_val,lr_pred,average=None))","de505d17":"data = confusion_matrix(target_val,lr_pred)\ncalculateScore(data)\ndf_cm = pd.DataFrame(data, columns=np.unique(target_val), index = np.unique(target_val))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.4)\nsns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})","511ebd09":"LR.coef_","cb95823d":"seed = 0\nparams = {\n    'n_estimators':range(10,100,10),\n    'criterion':['gini','entropy'],\n    'max_depth':range(2,10,1),\n    'max_leaf_nodes':range(2,10,1),\n    'max_features':['auto','log2'],\n    'verbose':[0]\n}\nrf = RandomForestClassifier()\nrs = RandomizedSearchCV(rf, param_distributions=params, scoring='accuracy', n_jobs=-1, cv=5, random_state=42)\nrs.fit(X,Y)","4d3ad6dd":"rs.best_params_","0a77a2ab":"rf = RandomForestClassifier(**rs.best_params_)\nrf.fit(train, target_train)\nrf_pred = rf.predict(test)","8bcd7688":"features = df_final.columns\nimportance = rf.feature_importances_\nindices = np.argsort(importance)\nplt.figure(1,figsize=(10,20))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importance[indices], color='lightblue', align='center')\nplt.yticks(range(len(indices)), features[indices])\nplt.xlabel('Relative Importance')","113687df":"Model.append('Random Forrest')\nAccuracy.append(accuracy_score(target_val,rf_pred))\nF1Score.append(f1_score(target_val,rf_pred,average=None))","33402b9c":"data = confusion_matrix(target_val,rf_pred)\ncalculateScore(data)\ndf_cm = pd.DataFrame(data, columns=np.unique(target_val), index = np.unique(target_val))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.4)\nsns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})","3f822856":"params = {\n    \n    'criterion':['gini','entropy'],\n    'splitter':['best','random'],\n    'max_depth':range(1,10,1),\n    'max_leaf_nodes':range(2,10,1),\n    'max_features':['auto','log2']\n    \n}\ndt = DecisionTreeClassifier()\nrs = RandomizedSearchCV(dt, param_distributions=params, scoring='accuracy', n_jobs=-1, cv=5, random_state=42)\nrs.fit(X,Y)","bd84880b":"rs.best_params_","42d91f5c":"dt = DecisionTreeClassifier()\ndt.fit(train, target_train)\ndt_pred = dt.predict(test)","b3fe420a":"features = df_final.columns\nimportance = dt.feature_importances_\nindices = np.argsort(importance)\nplt.figure(1,figsize=(10,20))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importance[indices], color='lightblue', align='center')\nplt.yticks(range(len(indices)), features[indices])\nplt.xlabel('Relative Importance')","aa61a158":"Model.append('Decision Tree')\nAccuracy.append(accuracy_score(target_val,dt_pred))\nF1Score.append(f1_score(target_val,dt_pred,average=None))","1bbd7032":"data = confusion_matrix(target_val,dt_pred)\ncalculateScore(data)\ndf_cm = pd.DataFrame(data, columns=np.unique(target_val), index = np.unique(target_val))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.4)\nsns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})","79d9a15d":"# Export our trained model as a .dot file\nwith open(\"tree1.dot\", 'w') as f:\n     f = tree.export_graphviz(dt,\n                              out_file=f,\n                              max_depth = 4,\n                              impurity = False,\n                              feature_names = df_final.columns.values,\n                              class_names = ['No', 'Yes'],\n                              rounded = True,\n                              filled= True )\ncheck_call(['dot','-Tpng','tree1.dot','-o','tree1.png'])\nimg = Image.open(\"tree1.png\")\ndraw = ImageDraw.Draw(img)\nimg.save('sample-out.png')\nPImage(\"sample-out.png\", height=2000, width=1900)","212389f4":"gb_params ={\n    'n_estimators': 1500,\n    'max_features': 0.9,\n    'learning_rate' : 0.25,\n    'max_depth': 4,\n    'min_samples_leaf': 2,\n    'subsample': 1,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}","c639c857":"gb = GradientBoostingClassifier(**gb_params)\ngb.fit(train, target_train)\ngb_pred = gb.predict(test)","8465be4d":"features = df_final.columns\nimportance = gb.feature_importances_\nindices = np.argsort(importance)\nplt.figure(1,figsize=(10,20))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importance[indices], color='lightblue', align='center')\nplt.yticks(range(len(indices)), features[indices])\nplt.xlabel('Relative Importance')","c8cfed0c":"Model.append('Gradient Boosting')\nAccuracy.append(accuracy_score(target_val,gb_pred))\nF1Score.append(f1_score(target_val,gb_pred,average=None))","904968a2":"data = confusion_matrix(target_val,gb_pred)\ncalculateScore(data)\ndf_cm = pd.DataFrame(data, columns=np.unique(target_val), index = np.unique(target_val))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.4)\nsns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})","ab33e835":"xgb_cfl = xgb.XGBClassifier(n_jobs = -1)\n\n\nparams = {\n        'n_estimators' : [100, 200, 500],\n        'learning_rate' : [0.05, 0.1],\n        'min_child_weight': [1, 5, 7],\n        'gamma': [1, 1.5, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }\n\nrs = RandomizedSearchCV(xgb_cfl, param_distributions=params, scoring='accuracy', n_jobs=-1)\nrs.fit(X,Y)","7f567f20":"rs.best_params_","1198a456":"xgcl = xgb.XGBClassifier(**rs.best_params_)\nxgcl.fit(train, target_train)\nxg_pred = xgcl.predict(test)","679c702c":"features = df_final.columns\nimportance = xgcl.feature_importances_\nindices = np.argsort(importance)\nplt.figure(1,figsize=(10,20))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importance[indices], color='lightblue', align='center')\nplt.yticks(range(len(indices)), features[indices])\nplt.xlabel('Relative Importance')","a5c2839b":"Model.append('XG Boost')\nAccuracy.append(accuracy_score(target_val,xg_pred))\nF1Score.append(f1_score(target_val,xg_pred,average=None))","f8726169":"data = confusion_matrix(target_val,xg_pred)\ncalculateScore(data)\ndf_cm = pd.DataFrame(data, columns=np.unique(target_val), index = np.unique(target_val))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.4)\nsns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})","ffd8eb82":"params = {\n    \n    'n_neighbors': range(1,25),\n    'weights': ['uniform','distance'],\n    'algorithm': ['ball_tree','kd_tree','brute','auto'],\n    'p': [1,2,3]\n}\n\nknn = KNeighborsClassifier()\n\ngs = GridSearchCV(estimator=knn,n_jobs=-1,cv=5,param_grid=params)\ngs.fit(X,Y)","0cc9004f":"gs.best_params_","e6958cff":"knn = KNeighborsClassifier(**gs.best_params_)\nknn.fit(train, target_train)\nknn_pred = knn.predict(test)","d0ff956b":"Model.append('KNN')\nAccuracy.append(accuracy_score(target_val,knn_pred))\nF1Score.append(f1_score(target_val,knn_pred,average=None))","72cbab63":"data = confusion_matrix(target_val,knn_pred)\ncalculateScore(data)\ndf_cm = pd.DataFrame(data, columns=np.unique(target_val), index = np.unique(target_val))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.4)\nsns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})","daa3e923":"# Create ROC Graph\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(target_val, LR.predict_proba(test)[:,1])\nrf_fpr, rf_tpr, rf_thresholds = roc_curve(target_val, rf.predict_proba(test)[:,1])\ndt_fpr, dt_tpr, dt_thresholds = roc_curve(target_val, dt.predict_proba(test)[:,1])\ngb_fpr, gb_tpr, gb_thresholds = roc_curve(target_val, gb.predict_proba(test)[:,1])\nxgb_fpr, xgb_tpr, xgb_thresholds = roc_curve(target_val, xgcl.predict_proba(test)[:,1])\nknn_fpr, knn_tpr, knn_thresholds = roc_curve(target_val, knn.predict_proba(test)[:,1])\nplt.figure(figsize=(10,8))\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.plot(rf_fpr, rf_tpr, label='Random Forest)')\nplt.plot(dt_fpr, dt_tpr, label='Decision Tree')\nplt.plot(gb_fpr, gb_tpr, label='Gradient boosting')\nplt.plot(xgb_fpr, xgb_tpr, label='XGBoost')\nplt.plot(knn_fpr, knn_tpr, label='KNN')\nplt.plot([0,1], [0,1],label='Base Rate')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Graph')\nplt.legend(loc=\"lower right\")\nplt.show()","40eb42b1":"print('Classification report for LinearRegression')\nprint(classification_report(target_val, lr_pred))","989265fe":"print('Classification report for Random Forrest')\nprint(classification_report(target_val, rf_pred))","7012dd82":"print('Classification report for Decsion Tree')\nprint(classification_report(target_val, dt_pred))","bf423f93":"print('Classification report for GradientBoosting')\nprint(classification_report(target_val, gb_pred))","6d2d2095":"print('Classification report for XGB')\nprint(classification_report(target_val, xg_pred))","c26d5360":"print('Classification report for KNN')\nprint(classification_report(target_val, knn_pred))","4d2b517e":"result = pd.DataFrame({'Model':Model,'Accuracy':Accuracy,'F1Score':F1Score,'Sensitivity':Sen,'Specificity':Spe,'FPR':FPR,'FNR':FNR})\nresult","b78a5f03":"LogisticRegression","9af23dc1":"XGboost","fd499369":"**EDA","aa4650d3":"KNN****","94140720":"Random Forrest Classifier****","e304d6b4":"Gradient Boosting****","a2859d38":"Decision Tree Classifier****"}}