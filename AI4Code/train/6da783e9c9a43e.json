{"cell_type":{"fdfcc10d":"code","6d6b60f8":"code","bf4d620b":"code","08665a61":"code","2245d3c3":"code","c2884e09":"code","f4cdb1b1":"code","d166a88f":"code","18625d6e":"code","0d8fa819":"code","fa2bd775":"code","ee710d81":"code","d65c9d21":"code","780be493":"code","19ff8052":"code","31dd2949":"code","66440494":"code","ce38c9ed":"code","ae036c34":"code","a0428d05":"code","c1b29e75":"code","f843d1e5":"code","8e1997e6":"code","10d79762":"code","19822112":"code","ecb1db5f":"markdown","55760d7c":"markdown","8d986c58":"markdown","279d4ea1":"markdown","e4add4aa":"markdown","363de16a":"markdown","10988cd4":"markdown","765e9c18":"markdown","e2ee7b43":"markdown","ce1cc943":"markdown","0c372cf7":"markdown","7b8afcd3":"markdown","d6118f7c":"markdown","930f344d":"markdown","1574d412":"markdown","fbb7cb61":"markdown","353370d3":"markdown","23a9172c":"markdown"},"source":{"fdfcc10d":"import torch","6d6b60f8":"\"\"\"\n\n!pip install pyyaml==5.1\n\n\nTORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\nCUDA_VERSION = torch.__version__.split(\"+\")[-1]\nprint(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n# Install detectron2 that matches the above pytorch version\n# See https:\/\/detectron2.readthedocs.io\/tutorials\/install.html for instructions\n!pip install detectron2 -f https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/$CUDA_VERSION\/torch$TORCH_VERSION\/index.html\n# If there is not yet a detectron2 release that matches the given torch + CUDA version, you need to install a different pytorch.\n\n# exit(0)  # After installation, you may need to \"restart runtime\" in Colab. This line can also restart runtime\n\n\"\"\"\n","bf4d620b":"!pip install 'git+https:\/\/github.com\/facebookresearch\/detectron2.git'","08665a61":"# Some basic setup:\n# Setup detectron2 logger\nimport detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\n# import some common libraries\nimport numpy as np\nimport os, json, cv2, random\n\n# import some common detectron2 utilities\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog","2245d3c3":"# download, decompress the data\n!wget https:\/\/github.com\/matterport\/Mask_RCNN\/releases\/download\/v2.1\/balloon_dataset.zip\n!unzip balloon_dataset.zip > \/dev\/null\n\n\n# if your dataset is in COCO format, this cell can be replaced by the following three lines:\n# from detectron2.data.datasets import register_coco_instances\n# register_coco_instances(\"my_dataset_train\", {}, \"json_annotation_train.json\", \"path\/to\/image\/dir\")\n# register_coco_instances(\"my_dataset_val\", {}, \"json_annotation_val.json\", \"path\/to\/image\/dir\")\n\nfrom detectron2.structures import BoxMode\n\ndef get_balloon_dicts(img_dir):\n    json_file = os.path.join(img_dir, \"via_region_data.json\")\n    with open(json_file) as f:\n        imgs_anns = json.load(f)\n\n    dataset_dicts = []\n    for idx, v in enumerate(imgs_anns.values()):\n        record = {}\n        \n        filename = os.path.join(img_dir, v[\"filename\"])\n        height, width = cv2.imread(filename).shape[:2]\n        \n        record[\"file_name\"] = filename\n        record[\"image_id\"] = idx\n        record[\"height\"] = height\n        record[\"width\"] = width\n      \n        annos = v[\"regions\"]\n        objs = []\n        for _, anno in annos.items():\n            assert not anno[\"region_attributes\"]\n            anno = anno[\"shape_attributes\"]\n            px = anno[\"all_points_x\"]\n            py = anno[\"all_points_y\"]\n            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n            poly = [p for x in poly for p in x]\n\n            obj = {\n                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n                \"bbox_mode\": BoxMode.XYXY_ABS,\n                \"segmentation\": [poly],\n                \"category_id\": 0,\n            }\n            objs.append(obj)\n        record[\"annotations\"] = objs\n        dataset_dicts.append(record)\n    return dataset_dicts\n\nfor d in [\"train\", \"val\"]:\n    DatasetCatalog.register(\"balloon_\" + d, lambda d=d: get_balloon_dicts(\"balloon\/\" + d))\n    MetadataCatalog.get(\"balloon_\" + d).set(thing_classes=[\"balloon\"])\nballoon_metadata = MetadataCatalog.get(\"balloon_train\")\n\ndataset_dicts = get_balloon_dicts(\"balloon\/train\")\n\"\"\"\nfor d in random.sample(dataset_dicts, 3):\n    img = cv2.imread(d[\"file_name\"])\n    visualizer = Visualizer(img[:, :, ::-1], metadata=balloon_metadata, scale=0.5)\n    out = visualizer.draw_dataset_dict(d)\n    cv2_imshow(out.get_image()[:, :, ::-1])\n\"\"\"","c2884e09":"import math\nimport fvcore.nn.weight_init as weight_init\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom detectron2.modeling.backbone.build import BACKBONE_REGISTRY","f4cdb1b1":"from detectron2.modeling.backbone import Backbone\n\nfrom detectron2.layers import (\n    CNNBlockBase,\n    Conv2d,\n    DeformConv,\n    ModulatedDeformConv,\n    ShapeSpec,\n    get_norm,\n)","d166a88f":"!pip install timm\nimport timm\nfrom timm.models.efficientnet import *\nfrom timm.models.resnet import *","18625d6e":"cfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation\/mask_rcnn_R_50_FPN_3x.yaml\"))\ncfg.DATASETS.TRAIN = (\"balloon_train\",)\ncfg.DATASETS.TEST = ()\ncfg.DATALOADER.NUM_WORKERS = 2\n# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation\/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\ncfg.MODEL.WEIGHTS = \"\"\ncfg.SOLVER.IMS_PER_BATCH = 2\ncfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\ncfg.SOLVER.MAX_ITER = 3000   # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\ncfg.SOLVER.STEPS = []        # do not decay learning rate\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon). (see https:\/\/detectron2.readthedocs.io\/tutorials\/datasets.html#update-the-config-for-new-datasets)\n# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n\n\n#cfg.MODEL.FPN.IN_FEATURES = [\"b0\", \"b1\", \"b2\", \"b3\", \"b4\", \"b5\"]\ncfg.MODEL.ANCHOR_GENERATOR.SIZES: [[[16]], [32], [64], [128], [256], [512]]  # One size for each in feature map\ncfg.MODEL.FPN.IN_FEATURES = [\"b0\", \"b1\", \"b2\", \"b3\", \"b5\"]\n\ncfg.MODEL.RPN.IN_FEATURES: [\"p1\", \"p2\", \"p3\", \"p4\", \"p5\", \"p6\"]\ncfg.MODEL.ROI_HEADS.IN_FEATURES: [\"p1\", \"p2\", \"p3\", \"p4\", \"p5\"]\n    \nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)","0d8fa819":"class EfficientnetV2S(Backbone):\n    def __init__(self):\n        super().__init__()\n\n        e = efficientnetv2_rw_s(pretrained=True, drop_path_rate=0.2)\n        \n        all_features = [\"stem\", \"b0\", \"b1\", \"b2\", \"b3\", \"b4\", \"b5\"] \n        out_features = [\"b0\", \"b1\", \"b2\", \"b3\", \"b5\"]\n        # _stride = [2, 1, 2, 2, 2, 1, 2]\n        self._out_feature_strides = {\"stem\": 2, \"b0\": 2, \"b1\": 4, \"b2\": 8, \"b3\": 16, \"b4\": 16, \"b5\": 32}\n        self._out_feature_channels = {\"stem\": 24, \"b0\": 24, \"b1\": 48, \"b2\": 64, \"b3\": 128, \"b4\": 160, \"b5\": 272}\n\n        \n        self.stem = nn.Sequential(\n            e.conv_stem,\n            e.bn1,\n            e.act1,\n        )\n        \n        self.b0 = e.blocks[0]\n        self.b1 = e.blocks[1]\n        self.b2 = e.blocks[2]\n        self.b3 = e.blocks[3]\n        self.b4 = e.blocks[4]\n        self.b5 = e.blocks[5]\n\n        # shall B7 conv_head be used ? \n\n        self.stages = [e.blocks[n] for n in range(0, 6)]\n        self.stage_names = all_features[1:]\n        self._out_features = out_features\n\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Tensor of shape (N,C,H,W). H, W must be a multiple of ``self.size_divisibility``.\n\n        Returns:\n            dict[str->Tensor]: names and the corresponding features\n        \"\"\"\n        assert x.dim() == 4, f\"ResNet takes an input of shape (N, C, H, W). Got {x.shape} instead!\"\n        outputs = {}\n        x = self.stem(x)\n        if \"stem\" in self._out_features:\n            outputs[\"stem\"] = x\n        for name, stage in zip(self.stage_names, self.stages):\n            x = stage(x)\n            if name in self._out_features:\n                outputs[name] = x\n        return outputs\n\n    def output_shape(self):\n        return {\n            name: ShapeSpec(\n                channels=self._out_feature_channels[name], stride=self._out_feature_strides[name]\n            )\n            for name in self._out_features\n        }","fa2bd775":"del BACKBONE_REGISTRY._obj_map[\"build_resnet_backbone\"] \n@BACKBONE_REGISTRY.register()\ndef build_resnet_backbone(cfg, input_shape):    \n    return EfficientnetV2S()","ee710d81":"#e1 = resnet50()\n#print(e1)","d65c9d21":"# bottom_up","780be493":"#e1 = efficientnetv2_rw_s()\n#print(e1)","19ff8052":"input_shape = ShapeSpec(channels=3, height=None, width=None, stride=None)\nbottom_up = build_resnet_backbone(cfg, input_shape)\nprint(\"_out_feature_channels\", bottom_up._out_feature_channels)\nprint(\"_out_feature_strides\", bottom_up._out_feature_strides)\nprint(\"output_shape\", bottom_up.output_shape())","31dd2949":"from detectron2.modeling.backbone.fpn import _assert_strides_are_log2_contiguous, LastLevelMaxPool\n\nclass FPN(Backbone):\n    \"\"\"\n    This module implements :paper:`FPN`.\n    It creates pyramid features built on top of some input feature maps.\n    \"\"\"\n\n    _fuse_type: torch.jit.Final[str]\n\n    def __init__(\n        self, bottom_up, in_features, out_channels, norm=\"\", top_block=None, fuse_type=\"sum\"\n    ):\n        \"\"\"\n        Args:\n            bottom_up (Backbone): module representing the bottom up subnetwork.\n                Must be a subclass of :class:`Backbone`. The multi-scale feature\n                maps generated by the bottom up network, and listed in `in_features`,\n                are used to generate FPN levels.\n            in_features (list[str]): names of the input feature maps coming\n                from the backbone to which FPN is attached. For example, if the\n                backbone produces [\"res2\", \"res3\", \"res4\"], any *contiguous* sublist\n                of these may be used; order must be from high to low resolution.\n            out_channels (int): number of channels in the output feature maps.\n            norm (str): the normalization to use.\n            top_block (nn.Module or None): if provided, an extra operation will\n                be performed on the output of the last (smallest resolution)\n                FPN output, and the result will extend the result list. The top_block\n                further downsamples the feature map. It must have an attribute\n                \"num_levels\", meaning the number of extra FPN levels added by\n                this block, and \"in_feature\", which is a string representing\n                its input feature (e.g., p5).\n            fuse_type (str): types for fusing the top down features and the lateral\n                ones. It can be \"sum\" (default), which sums up element-wise; or \"avg\",\n                which takes the element-wise mean of the two.\n        \"\"\"\n        super(FPN, self).__init__()\n        assert isinstance(bottom_up, Backbone)\n        assert in_features, in_features\n\n        # Feature map strides and channels from the bottom up network (e.g. ResNet)\n        input_shapes = bottom_up.output_shape()\n        strides = [input_shapes[f].stride for f in in_features]\n        in_channels_per_feature = [input_shapes[f].channels for f in in_features]\n        \n        _assert_strides_are_log2_contiguous(strides)\n        lateral_convs = []\n        output_convs = []\n\n        use_bias = norm == \"\"\n        for idx, in_channels in enumerate(in_channels_per_feature):\n            lateral_norm = get_norm(norm, out_channels)\n            output_norm = get_norm(norm, out_channels)\n\n            lateral_conv = Conv2d(\n                in_channels, out_channels, kernel_size=1, bias=use_bias, norm=lateral_norm\n            )\n            output_conv = Conv2d(\n                out_channels,\n                out_channels,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n                bias=use_bias,\n                norm=output_norm,\n            )\n            weight_init.c2_xavier_fill(lateral_conv)\n            weight_init.c2_xavier_fill(output_conv)\n            stage = int(math.log2(strides[idx]))\n            self.add_module(\"fpn_lateral{}\".format(stage), lateral_conv)\n            self.add_module(\"fpn_output{}\".format(stage), output_conv)\n\n            lateral_convs.append(lateral_conv)\n            output_convs.append(output_conv)\n        # Place convs into top-down order (from low to high resolution)\n        # to make the top-down computation in forward clearer.\n        self.lateral_convs = lateral_convs[::-1]\n        self.output_convs = output_convs[::-1]\n        self.top_block = top_block\n        self.in_features = tuple(in_features)\n        self.bottom_up = bottom_up\n        # Return feature names are \"p<stage>\", like [\"p2\", \"p3\", ..., \"p6\"]\n        self._out_feature_strides = {\"p{}\".format(int(math.log2(s))): s for s in strides}\n        # top block output feature maps.\n        \n        if self.top_block is not None:\n            for s in range(stage, stage + self.top_block.num_levels):\n                self._out_feature_strides[\"p{}\".format(s + 1)] = 2 ** (s + 1)\n\n        self._out_features = list(self._out_feature_strides.keys())\n        self._out_feature_channels = {k: out_channels for k in self._out_features}\n        self._size_divisibility = strides[-1]\n        assert fuse_type in {\"avg\", \"sum\"}\n        self._fuse_type = fuse_type\n\n    @property\n    def size_divisibility(self):\n        return self._size_divisibility\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            input (dict[str->Tensor]): mapping feature map name (e.g., \"res5\") to\n                feature map tensor for each feature level in high to low resolution order.\n\n        Returns:\n            dict[str->Tensor]:\n                mapping from feature map name to FPN feature map tensor\n                in high to low resolution order. Returned feature names follow the FPN\n                paper convention: \"p<stage>\", where stage has stride = 2 ** stage e.g.,\n                [\"p2\", \"p3\", ..., \"p6\"].\n        \"\"\"\n        bottom_up_features = self.bottom_up(x)\n        results = []\n        prev_features = self.lateral_convs[0](bottom_up_features[self.in_features[-1]])\n        results.append(self.output_convs[0](prev_features))\n\n        # Reverse feature maps into top-down order (from low to high resolution)\n        for idx, (lateral_conv, output_conv) in enumerate(\n            zip(self.lateral_convs, self.output_convs)\n        ):\n            # Slicing of ModuleList is not supported https:\/\/github.com\/pytorch\/pytorch\/issues\/47336\n            # Therefore we loop over all modules but skip the first one\n            if idx > 0:\n                features = self.in_features[-idx - 1]\n                features = bottom_up_features[features]\n                top_down_features = F.interpolate(prev_features, scale_factor=2.0, mode=\"nearest\")\n                lateral_features = lateral_conv(features)\n                prev_features = lateral_features + top_down_features\n                if self._fuse_type == \"avg\":\n                    prev_features \/= 2\n                results.insert(0, output_conv(prev_features))\n\n        if self.top_block is not None:\n            if self.top_block.in_feature in bottom_up_features:\n                top_block_in_feature = bottom_up_features[self.top_block.in_feature]\n            else:\n                top_block_in_feature = results[self._out_features.index(self.top_block.in_feature)]\n            results.extend(self.top_block(top_block_in_feature))\n        assert len(self._out_features) == len(results)\n        return {f: res for f, res in zip(self._out_features, results)}\n\n    def output_shape(self):\n        return {\n            name: ShapeSpec(\n                channels=self._out_feature_channels[name], stride=self._out_feature_strides[name]\n            )\n            for name in self._out_features\n        }\n\ndel BACKBONE_REGISTRY._obj_map[\"build_resnet_fpn_backbone\"] \n@BACKBONE_REGISTRY.register()\ndef build_resnet_fpn_backbone(cfg, input_shape: ShapeSpec):\n\n    \"\"\"\n    Args:\n        cfg: a detectron2 CfgNode\n\n    Returns:\n        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.\n    \"\"\"\n    bottom_up = build_resnet_backbone(cfg, input_shape)\n    in_features = cfg.MODEL.FPN.IN_FEATURES\n    out_channels = cfg.MODEL.FPN.OUT_CHANNELS\n    backbone = FPN(\n        bottom_up=bottom_up,\n        in_features=in_features,\n        out_channels=out_channels,\n        norm=cfg.MODEL.FPN.NORM,\n        top_block=LastLevelMaxPool(),\n        fuse_type=cfg.MODEL.FPN.FUSE_TYPE,\n    )\n    return backbone","66440494":"bb = build_resnet_fpn_backbone(cfg, ShapeSpec(channels=3))\nprint(bb.output_shape())","ce38c9ed":"from prettytable import PrettyTable\nimport re\ndef count_parameters(model):\n    table = PrettyTable([\"Modules\", \"Parameters\"])\n    total_params = 0\n    resnet_params = 0\n    for name, parameter in model.named_parameters():\n        if not parameter.requires_grad: continue\n        param = parameter.numel()\n        table.add_row([name, param])\n        if re.search(\"backbone.bottom_up\", name):\n          resnet_params += param\n        total_params+=param\n    print(table)\n    print(f\"Total Trainable Params: {total_params}\")\n    print(f\"Resnet Params: {resnet_params}\")\n    return total_params","ae036c34":"from detectron2.engine.defaults import create_ddp_model, default_writers\nfrom detectron2.engine import hooks\n\nfrom detectron2.data import (\n    build_detection_test_loader,\n    build_detection_train_loader,\n)\n\nfrom detectron2.engine.train_loop import AMPTrainer, SimpleTrainer, TrainerBase\nfrom detectron2.checkpoint import DetectionCheckpointer\nfrom detectron2.modeling import build_model\nfrom detectron2.solver import build_lr_scheduler, build_optimizer\n\n\nfrom detectron2.evaluation import (\n    DatasetEvaluator,\n    inference_on_dataset,\n    print_csv_format,\n    verify_results,\n)\n\nimport weakref\nimport logging\nfrom detectron2.utils import comm\nfrom collections import OrderedDict\n\nclass CustomizeTrainer(TrainerBase):\n    \"\"\"\n    A trainer with default training logic. It does the following:\n\n    1. Create a :class:`SimpleTrainer` using model, optimizer, dataloader\n       defined by the given config. Create a LR scheduler defined by the config.\n    2. Load the last checkpoint or `cfg.MODEL.WEIGHTS`, if exists, when\n       `resume_or_load` is called.\n    3. Register a few common hooks defined by the config.\n\n    It is created to simplify the **standard model training workflow** and reduce code boilerplate\n    for users who only need the standard training workflow, with standard features.\n    It means this class makes *many assumptions* about your training logic that\n    may easily become invalid in a new research. In fact, any assumptions beyond those made in the\n    :class:`SimpleTrainer` are too much for research.\n\n    The code of this class has been annotated about restrictive assumptions it makes.\n    When they do not work for you, you're encouraged to:\n\n    1. Overwrite methods of this class, OR:\n    2. Use :class:`SimpleTrainer`, which only does minimal SGD training and\n       nothing else. You can then add your own hooks if needed. OR:\n    3. Write your own training loop similar to `tools\/plain_train_net.py`.\n\n    See the :doc:`\/tutorials\/training` tutorials for more details.\n\n    Note that the behavior of this class, like other functions\/classes in\n    this file, is not stable, since it is meant to represent the \"common default behavior\".\n    It is only guaranteed to work well with the standard models and training workflow in detectron2.\n    To obtain more stable behavior, write your own training logic with other public APIs.\n\n    Examples:\n    ::\n        trainer = DefaultTrainer(cfg)\n        trainer.resume_or_load()  # load last checkpoint or MODEL.WEIGHTS\n        trainer.train()\n\n    Attributes:\n        scheduler:\n        checkpointer (DetectionCheckpointer):\n        cfg (CfgNode):\n    \"\"\"\n\n    def __init__(self, cfg):\n        \"\"\"\n        Args:\n            cfg (CfgNode):\n        \"\"\"\n        super().__init__()\n        logger = logging.getLogger(\"detectron2\")\n        if not logger.isEnabledFor(logging.INFO):  # setup_logger is not called for d2\n            setup_logger()\n        cfg = self.auto_scale_workers(cfg, comm.get_world_size())\n\n        # Assume these objects must be constructed in this order.\n        model = self.build_model(cfg)\n        optimizer = self.build_optimizer(cfg, model)\n        data_loader = self.build_train_loader(cfg)\n\n        model = create_ddp_model(model, broadcast_buffers=False)\n\n        #pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n        #print(\"parameters\", sum(dict((p.data_ptr(), p.numel()) for p in model.parameters()).values()))\n        \n        #print(\"pytorch_total_params\", pytorch_total_params)\n        count_parameters(model)\n            \n        self._trainer = (AMPTrainer if cfg.SOLVER.AMP.ENABLED else SimpleTrainer)(\n            model, data_loader, optimizer\n        )\n\n        self.scheduler = self.build_lr_scheduler(cfg, optimizer)\n        self.checkpointer = DetectionCheckpointer(\n            # Assume you want to save checkpoints together with logs\/statistics\n            model,\n            cfg.OUTPUT_DIR,\n            trainer=weakref.proxy(self),\n        )\n        self.start_iter = 0\n        self.max_iter = cfg.SOLVER.MAX_ITER\n        self.cfg = cfg\n\n        self.register_hooks(self.build_hooks())\n\n    def resume_or_load(self, resume=True):\n        \"\"\"\n        If `resume==True` and `cfg.OUTPUT_DIR` contains the last checkpoint (defined by\n        a `last_checkpoint` file), resume from the file. Resuming means loading all\n        available states (eg. optimizer and scheduler) and update iteration counter\n        from the checkpoint. ``cfg.MODEL.WEIGHTS`` will not be used.\n\n        Otherwise, this is considered as an independent training. The method will load model\n        weights from the file `cfg.MODEL.WEIGHTS` (but will not load other states) and start\n        from iteration 0.\n\n        Args:\n            resume (bool): whether to do resume or not\n        \"\"\"\n        self.checkpointer.resume_or_load(self.cfg.MODEL.WEIGHTS, resume=resume)\n        if resume and self.checkpointer.has_checkpoint():\n            # The checkpoint stores the training iteration that just finished, thus we start\n            # at the next iteration\n            self.start_iter = self.iter + 1\n\n    def build_hooks(self):\n        \"\"\"\n        Build a list of default hooks, including timing, evaluation,\n        checkpointing, lr scheduling, precise BN, writing events.\n\n        Returns:\n            list[HookBase]:\n        \"\"\"\n        cfg = self.cfg.clone()\n        cfg.defrost()\n        cfg.DATALOADER.NUM_WORKERS = 0  # save some memory and time for PreciseBN\n\n        ret = [\n            hooks.IterationTimer(),\n            hooks.LRScheduler(),\n            hooks.PreciseBN(\n                # Run at the same freq as (but before) evaluation.\n                cfg.TEST.EVAL_PERIOD,\n                self.model,\n                # Build a new data loader to not affect training\n                self.build_train_loader(cfg),\n                cfg.TEST.PRECISE_BN.NUM_ITER,\n            )\n            if cfg.TEST.PRECISE_BN.ENABLED and get_bn_modules(self.model)\n            else None,\n        ]\n\n        # Do PreciseBN before checkpointer, because it updates the model and need to\n        # be saved by checkpointer.\n        # This is not always the best: if checkpointing has a different frequency,\n        # some checkpoints may have more precise statistics than others.\n        if comm.is_main_process():\n            ret.append(hooks.PeriodicCheckpointer(self.checkpointer, cfg.SOLVER.CHECKPOINT_PERIOD))\n\n        def test_and_save_results():\n            self._last_eval_results = self.test(self.cfg, self.model)\n            return self._last_eval_results\n\n        # Do evaluation after checkpointer, because then if it fails,\n        # we can use the saved checkpoint to debug.\n        ret.append(hooks.EvalHook(cfg.TEST.EVAL_PERIOD, test_and_save_results))\n\n        if comm.is_main_process():\n            # Here the default print\/log frequency of each writer is used.\n            # run writers in the end, so that evaluation metrics are written\n            ret.append(hooks.PeriodicWriter(self.build_writers(), period=20))\n        return ret\n\n    def build_writers(self):\n        \"\"\"\n        Build a list of writers to be used using :func:`default_writers()`.\n        If you'd like a different list of writers, you can overwrite it in\n        your trainer.\n\n        Returns:\n            list[EventWriter]: a list of :class:`EventWriter` objects.\n        \"\"\"\n        return default_writers(self.cfg.OUTPUT_DIR, self.max_iter)\n\n    def train(self):\n        \"\"\"\n        Run training.\n\n        Returns:\n            OrderedDict of results, if evaluation is enabled. Otherwise None.\n        \"\"\"\n        super().train(self.start_iter, self.max_iter)\n        if len(self.cfg.TEST.EXPECTED_RESULTS) and comm.is_main_process():\n            assert hasattr(\n                self, \"_last_eval_results\"\n            ), \"No evaluation results obtained during training!\"\n            verify_results(self.cfg, self._last_eval_results)\n            return self._last_eval_results\n\n    def run_step(self):\n        self._trainer.iter = self.iter\n        self._trainer.run_step()\n\n    def state_dict(self):\n        ret = super().state_dict()\n        ret[\"_trainer\"] = self._trainer.state_dict()\n        return ret\n\n    def load_state_dict(self, state_dict):\n        super().load_state_dict(state_dict)\n        self._trainer.load_state_dict(state_dict[\"_trainer\"])\n\n    @classmethod\n    def build_model(cls, cfg):\n        \"\"\"\n        Returns:\n            torch.nn.Module:\n\n        It now calls :func:`detectron2.modeling.build_model`.\n        Overwrite it if you'd like a different model.\n        \"\"\"\n        model = build_model(cfg)\n        print(\"Model\", model)\n        logger = logging.getLogger(__name__)\n        logger.info(\"Model:\\n{}\".format(model))\n        return model\n\n    @classmethod\n    def build_optimizer(cls, cfg, model):\n        \"\"\"\n        Returns:\n            torch.optim.Optimizer:\n\n        It now calls :func:`detectron2.solver.build_optimizer`.\n        Overwrite it if you'd like a different optimizer.\n        \"\"\"\n        return build_optimizer(cfg, model)\n\n    @classmethod\n    def build_lr_scheduler(cls, cfg, optimizer):\n        \"\"\"\n        It now calls :func:`detectron2.solver.build_lr_scheduler`.\n        Overwrite it if you'd like a different scheduler.\n        \"\"\"\n        return build_lr_scheduler(cfg, optimizer)\n\n    @classmethod\n    def build_train_loader(cls, cfg):\n        \"\"\"\n        Returns:\n            iterable\n\n        It now calls :func:`detectron2.data.build_detection_train_loader`.\n        Overwrite it if you'd like a different data loader.\n        \"\"\"\n        return build_detection_train_loader(cfg)\n\n    @classmethod\n    def build_test_loader(cls, cfg, dataset_name):\n        \"\"\"\n        Returns:\n            iterable\n\n        It now calls :func:`detectron2.data.build_detection_test_loader`.\n        Overwrite it if you'd like a different data loader.\n        \"\"\"\n        return build_detection_test_loader(cfg, dataset_name)\n\n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name):\n        \"\"\"\n        Returns:\n            DatasetEvaluator or None\n\n        It is not implemented by default.\n        \"\"\"\n        raise NotImplementedError(\n            \"\"\"\nIf you want DefaultTrainer to automatically run evaluation,\nplease implement `build_evaluator()` in subclasses (see train_net.py for example).\nAlternatively, you can call evaluation functions yourself (see Colab balloon tutorial for example).\n\"\"\"\n        )\n\n    @classmethod\n    def test(cls, cfg, model, evaluators=None):\n        \"\"\"\n        Evaluate the given model. The given model is expected to already contain\n        weights to evaluate.\n\n        Args:\n            cfg (CfgNode):\n            model (nn.Module):\n            evaluators (list[DatasetEvaluator] or None): if None, will call\n                :meth:`build_evaluator`. Otherwise, must have the same length as\n                ``cfg.DATASETS.TEST``.\n\n        Returns:\n            dict: a dict of result metrics\n        \"\"\"\n        logger = logging.getLogger(__name__)\n        if isinstance(evaluators, DatasetEvaluator):\n            evaluators = [evaluators]\n        if evaluators is not None:\n            assert len(cfg.DATASETS.TEST) == len(evaluators), \"{} != {}\".format(\n                len(cfg.DATASETS.TEST), len(evaluators)\n            )\n\n        results = OrderedDict()\n        for idx, dataset_name in enumerate(cfg.DATASETS.TEST):\n            data_loader = cls.build_test_loader(cfg, dataset_name)\n            # When evaluators are passed in as arguments,\n            # implicitly assume that evaluators can be created before data_loader.\n            if evaluators is not None:\n                evaluator = evaluators[idx]\n            else:\n                try:\n                    evaluator = cls.build_evaluator(cfg, dataset_name)\n                except NotImplementedError:\n                    logger.warn(\n                        \"No evaluator found. Use `DefaultTrainer.test(evaluators=)`, \"\n                        \"or implement its `build_evaluator` method.\"\n                    )\n                    results[dataset_name] = {}\n                    continue\n            results_i = inference_on_dataset(model, data_loader, evaluator)\n            results[dataset_name] = results_i\n            if comm.is_main_process():\n                assert isinstance(\n                    results_i, dict\n                ), \"Evaluator must return a dict on the main process. Got {} instead.\".format(\n                    results_i\n                )\n                logger.info(\"Evaluation results for {} in csv format:\".format(dataset_name))\n                print_csv_format(results_i)\n\n        if len(results) == 1:\n            results = list(results.values())[0]\n        return results\n\n    @staticmethod\n    def auto_scale_workers(cfg, num_workers: int):\n        \"\"\"\n        When the config is defined for certain number of workers (according to\n        ``cfg.SOLVER.REFERENCE_WORLD_SIZE``) that's different from the number of\n        workers currently in use, returns a new cfg where the total batch size\n        is scaled so that the per-GPU batch size stays the same as the\n        original ``IMS_PER_BATCH \/\/ REFERENCE_WORLD_SIZE``.\n\n        Other config options are also scaled accordingly:\n        * training steps and warmup steps are scaled inverse proportionally.\n        * learning rate are scaled proportionally, following :paper:`ImageNet in 1h`.\n\n        For example, with the original config like the following:\n\n        .. code-block:: yaml\n\n            IMS_PER_BATCH: 16\n            BASE_LR: 0.1\n            REFERENCE_WORLD_SIZE: 8\n            MAX_ITER: 5000\n            STEPS: (4000,)\n            CHECKPOINT_PERIOD: 1000\n\n        When this config is used on 16 GPUs instead of the reference number 8,\n        calling this method will return a new config with:\n\n        .. code-block:: yaml\n\n            IMS_PER_BATCH: 32\n            BASE_LR: 0.2\n            REFERENCE_WORLD_SIZE: 16\n            MAX_ITER: 2500\n            STEPS: (2000,)\n            CHECKPOINT_PERIOD: 500\n\n        Note that both the original config and this new config can be trained on 16 GPUs.\n        It's up to user whether to enable this feature (by setting ``REFERENCE_WORLD_SIZE``).\n\n        Returns:\n            CfgNode: a new config. Same as original if ``cfg.SOLVER.REFERENCE_WORLD_SIZE==0``.\n        \"\"\"\n        old_world_size = cfg.SOLVER.REFERENCE_WORLD_SIZE\n        if old_world_size == 0 or old_world_size == num_workers:\n            return cfg\n        cfg = cfg.clone()\n        frozen = cfg.is_frozen()\n        cfg.defrost()\n\n        assert (\n            cfg.SOLVER.IMS_PER_BATCH % old_world_size == 0\n        ), \"Invalid REFERENCE_WORLD_SIZE in config!\"\n        scale = num_workers \/ old_world_size\n        bs = cfg.SOLVER.IMS_PER_BATCH = int(round(cfg.SOLVER.IMS_PER_BATCH * scale))\n        lr = cfg.SOLVER.BASE_LR = cfg.SOLVER.BASE_LR * scale\n        max_iter = cfg.SOLVER.MAX_ITER = int(round(cfg.SOLVER.MAX_ITER \/ scale))\n        warmup_iter = cfg.SOLVER.WARMUP_ITERS = int(round(cfg.SOLVER.WARMUP_ITERS \/ scale))\n        cfg.SOLVER.STEPS = tuple(int(round(s \/ scale)) for s in cfg.SOLVER.STEPS)\n        cfg.TEST.EVAL_PERIOD = int(round(cfg.TEST.EVAL_PERIOD \/ scale))\n        cfg.SOLVER.CHECKPOINT_PERIOD = int(round(cfg.SOLVER.CHECKPOINT_PERIOD \/ scale))\n        cfg.SOLVER.REFERENCE_WORLD_SIZE = num_workers  # maintain invariant\n        logger = logging.getLogger(__name__)\n        logger.info(\n            f\"Auto-scaling the config to batch_size={bs}, learning_rate={lr}, \"\n            f\"max_iter={max_iter}, warmup={warmup_iter}.\"\n        )\n\n        if frozen:\n            cfg.freeze()\n        return cfg\n\n# Access basic attributes from the underlying trainer\nfor _attr in [\"model\", \"data_loader\", \"optimizer\"]:\n    setattr(\n        CustomizeTrainer,\n        _attr,\n        property(\n            # getter\n            lambda self, x=_attr: getattr(self._trainer, x),\n            # setter\n            lambda self, value, x=_attr: setattr(self._trainer, x, value),\n        ),\n    )","a0428d05":"trainer = CustomizeTrainer(cfg) \ntrainer.resume_or_load(resume=False)\n#trainer.train()","c1b29e75":"# ResNet + FPN + RNN \n# Resnet 23 232 512\n# total 43 695 638\n\n# Efficientnetv2s \n\n# efficient 21657288\n# total 41864926","f843d1e5":"trainer.train()","8e1997e6":"# Inference should use the config with parameters that are used in training\n# cfg now already contains everything we've set previously. We changed it a little bit for inference:\ncfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\npredictor = DefaultPredictor(cfg)","10d79762":"from matplotlib import pyplot as plt","19822112":"from detectron2.utils.visualizer import ColorMode\ndataset_dicts = get_balloon_dicts(\"balloon\/val\")\nfor d in random.sample(dataset_dicts, 3):    \n    im = cv2.imread(d[\"file_name\"])\n    outputs = predictor(im)  # format is documented at https:\/\/detectron2.readthedocs.io\/tutorials\/models.html#model-output-format\n    v = Visualizer(im[:, :, ::-1],\n                   metadata=balloon_metadata, \n                   scale=0.5, \n                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n    )\n    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n    plt.figure(figsize=(16, 16))\n    plt.imshow(out.get_image()[:, :, ::-1])\n    #cv2.imshow(out.get_image()[:, :, ::-1])","ecb1db5f":"# CFG","55760d7c":"### Resnet50 Architecture\n\n| name | func | stride | out_channel |\n| ---- | ---- | ------ | ------ |\n| stem | conv2d + maxpooling | 2 * 2 | 64 |\n| res2(layer1) | bconv | 1 | 256 |\n| res3(layer2) | bconv | 2 | 512 |\n| res4(layer3) | bconv | 2 | 1024 |\n| res5(layer4) | bconv | 2 | 2048 |","8d986c58":"### Efficientnetv2s architecture\n\n\n| name | func | stride | out_channel |\n| ---- | ---- | ------ | ------ |\n| stem | conv2d | 2 | 24 |\n| b0 | ConvBnAct | 1 | 24 |\n| b1 | EdgeResidual | 2 | 48 |\n| b2 | EdgeResidual | 2 | 64 |\n| b3 | InvertedResidual | 2 | 128 |\n| b4 | InvertedResidual | 1 | 160 |\n| b5 | InvertedResidual | 2 | 256(original: 272) |\n| b6 | conv_head | - | 1792 |","279d4ea1":"## Check Backbone","e4add4aa":"## Sannity Check ","363de16a":"### Bottom_up architecture\n\nAs same as Timm Resnet50 implement","10988cd4":"# Prepare Dataset","765e9c18":"# Run","e2ee7b43":"# Check","ce1cc943":"# FPN BACKBONE","0c372cf7":"# Dependences","7b8afcd3":"# Working in progress\n\nDeeply modified Detectron2\n\nSeems Not working yet","d6118f7c":"# Install detectron2","930f344d":"## Stages","1574d412":"# CNN BACKBONE","fbb7cb61":"### Sanity Test","353370d3":"# Cutomize Default trainer","23a9172c":"## ResNet"}}