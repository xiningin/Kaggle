{"cell_type":{"a0103d94":"code","e477e5fe":"code","91ba97a6":"code","af553697":"code","eb6bd0e4":"code","c1219832":"code","daa4333b":"code","1819a631":"code","fa472c98":"code","771aeaab":"code","987b27b1":"code","2b05caa7":"code","690116fa":"code","d857c7de":"code","2e0a2c69":"code","9141e7de":"code","06a145d7":"code","0e9d4c8f":"code","94666f87":"markdown"},"source":{"a0103d94":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e477e5fe":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.svm import SVC\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","91ba97a6":"df = pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\ndf.head()","af553697":"df.dtypes","eb6bd0e4":"df.isnull().sum()","c1219832":"\nsns.countplot(df['output'])","daa4333b":"sns.distplot(df['age'])","1819a631":"\nplt.figure(figsize = (10, 5))\nplt.bar(df['age'], df['output'])","fa472c98":"plt.figure(figsize=(12, 12))\nsns.heatmap(df.corr(), annot=True)","771aeaab":"df.shape","987b27b1":"X = df.iloc[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]]\ny = df['output']","2b05caa7":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True, random_state = 5)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","690116fa":"\n# Logistic Regression Model\nlogistic_model = LogisticRegression().fit(X_train, y_train)\nlogistic_y_pred = logistic_model.predict(X_test)\nprint(\"Logistic Regression accuracy: \", accuracy_score(logistic_y_pred, y_test))\n# Random Forest Model\nrandomForest_model = RandomForestClassifier().fit(X_train, y_train)\nrandomForest_y_pred = randomForest_model.predict(X_test)\nprint(\"Random Forest Classifier: \", accuracy_score(randomForest_y_pred, y_test))\n# Navie Bayes Model\ngaussian_model = GaussianNB().fit(X_train, y_train)\ngaussian_y_pred = gaussian_model.predict(X_test)\nprint(\"Gaussian Navie Bayes accuracy: \", accuracy_score(gaussian_y_pred, y_test))\n# XGB Model\nxgb_model = XGBClassifier().fit(X_train, y_train)\nxgb_y_pred = xgb_model.predict(X_test)\nprint(\"XGB Classifier accuracy: \", accuracy_score(xgb_y_pred, y_test))\n# LGBM Model\nlight_model = LGBMClassifier().fit(X_train, y_train)\nlight_y_pred = light_model.predict(X_test)\nprint(\"LGBM classifier accuracy: \", accuracy_score(light_y_pred, y_test))\n#Support Vector model\nsvc_model = SVC().fit(X_train, y_train)\nsvc_y_pred = svc_model.predict(X_test)\nprint(\"Support Vector Classifier accuracy: \", accuracy_score(svc_y_pred, y_test))","d857c7de":"print(classification_report(logistic_y_pred, y_test))","2e0a2c69":"from keras.layers import Dense, Dropout\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.losses import binary_crossentropy\n\nmodel = Sequential()\nmodel.add(Dense(1024, activation='relu', input_shape=(X.shape[1],)))\n# model.add(Dense(128, activation='relu'))\n# model.add(Dense(32, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['accuracy'])\nmodel.summary()","9141e7de":"history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30)","06a145d7":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","0e9d4c8f":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","94666f87":"We got the highest acuracy using Logistic Regression (0.918)"}}