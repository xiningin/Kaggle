{"cell_type":{"51197222":"code","6293eb7e":"code","d2ff49e7":"code","1c52c970":"code","b28a3b4b":"code","2c0d151d":"code","0aa8512e":"code","9f721756":"code","bb1325f5":"code","b8afc43d":"markdown","f9b70220":"markdown","1d24d423":"markdown","ae9ef1ea":"markdown","fc79ed4f":"markdown","0f9f59c0":"markdown","0fa8a2ec":"markdown","e8761610":"markdown","00ef2f05":"markdown","e978e656":"markdown","5c75e213":"markdown","937f5539":"markdown","a6ff1c27":"markdown","d802917d":"markdown","bbc4e078":"markdown"},"source":{"51197222":"!pip install xlrd\n\n# Modules for data processing\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nimport sys\nfrom datetime import datetime\nimport calendar\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# REFERENCE: https:\/\/www.kaggle.com\/dmitryuarov\/eda-covid-19-impact-on-digital-learning\nSTATE_ABBR = {\n    'Alabama': 'AL', 'Alaska': 'AK', 'American Samoa': 'AS', 'Arizona': 'AZ', 'Arkansas': 'AR',\n    'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', 'District of Columbia': 'DC', 'District Of Columbia': 'DC',\n    'Florida': 'FL', 'Georgia': 'GA', 'Guam': 'GU', 'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL',\n    'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME',\n    'Maryland': 'MD', 'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n    'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH',\n    'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP', 'Ohio': 'OH', 'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR', 'Rhode Island': 'RI', 'South Carolina': 'SC', 'South Dakota': 'SD', 'Tennessee': 'TN',\n    'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT', 'Virgin Islands': 'VI', 'Virginia': 'VA', 'Washington': 'WA',\n    'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY'\n}\nSTATE_NAME = dict([(y, x) for x, y in STATE_ABBR.items()])\n\n# ########################################################################\n# ### analyze_dataset\n# ########################################################################\n# #\n# # Simple function to help quickly analyze information & usability of\n# # a dataset. Provides information about shape, null values, unique\n# # values & basic statistical features.\n# #\n# # Inputs:\n# #   1.  df_path (string) -> Dataset path (if available)\n# #   2.  df (pandas dataframe) -> Dataset (if available)\n# #   3.  direct_df (boolean) -> Whether dataset path or dataset is\n# #       being provided\n# #   4.  processing_func (function) -> If dataset needs to be processed\n# #       before analyzing\n# #   5.  Other arguments for pd.read_csv(...) if dataset path is being\n# #       provided\n# #\n# # Return:   Either dataframe itself (if path provided) or head of\n# #           dataframe (if dataframe provided)\n# #\n# ########################################################################\n\n# def analyze_dataset(df_path = None, df = None, direct_df = False, processing_func = lambda x: x, **read_csv_args):\n    \n#     if(direct_df == False):\n#         df = pd.read_csv(df_path, **read_csv_args)\n#     df = processing_func(df)\n    \n#     num_rows, num_cols = df.shape\n#     dtypes = dict(df.dtypes.items())\n#     print(\"*****************\")\n#     print(\"Basic Info:\")\n#     print(\"*****************\\n\")\n#     print(f\"Shape of Dataset: {num_rows} rows, {num_cols} cols\")\n#     print(\"Columns:\")\n#     for col_idx, col in enumerate(df.columns):\n#         print(f\"\\t{col_idx+1}. {col}\\n\\t\\t\\t\\t\\t\\t\\t\\t{dtypes[col]}\")\n    \n#     print(\"\\n\\n\\n*****************\")\n#     print(\"Null Values:\")\n#     print(\"*****************\\n\")\n#     nulls = pd.isnull(df).sum()\n#     print(f\"Total Nulls: {nulls.sum()}\")\n#     nulls = nulls[nulls > 0]\n#     nulls = list(sorted(nulls.items(), key = lambda x: x[1], reverse = True))\n#     print(\"Columns with missing values:\")\n#     for col_idx, (col_name, col_missin_num) in enumerate(nulls):\n#         print(f\"\\t{col_idx + 1}. {col_name}\\n\\t\\t\\t\\t\\t\\t\\t\\t{col_missin_num} missing ({col_missin_num \/ num_rows * 100:.1f}%)\")\n    \n#     print(\"\\n\\n\\n*****************\")\n#     print(\"Column-specific:\")\n#     print(\"*****************\\n\")\n#     print(\"Unique values in columns:\")\n#     idx = 1\n#     for col in df.columns:\n#         nunique = df[col].nunique()\n#         if(nunique < 10):\n#             unique_vals = [\"'\" + str(x) + \"'\" for x in df[col].unique()]\n#             print(f\"{idx}. {col} has {nunique} unique values\")\n#             idx += 1\n#             print(f\"\\t[ {', '.join(unique_vals)} ]\")\n#     print(\"\\n\\nStatistical Features:\")\n#     print(df.describe())\n    \n#     print(\"\\n\\n\")\n#     if(direct_df == True):\n#         return df.head()\n#     else:\n#         return df\n\n########################################################################\n### load_main_dataset\n########################################################################\n#\n# Function to load the main processed & merged dataset for\n# engagement. Has options to merge selected datasets. The processing\n# has been done separately in another function `process_main_dataset`\n#\n# Inputs:\n#   1.  whether_merge_district (boolean) -> Whether to merge districts\n#       data\n#   2.  whether_merge_products (boolean) -> Whether to merge products\n#       data\n#   3.  whether_merge_dates (boolean) -> Whether to merge dates data\n#\n# Return:   Engagement data merged with other relevant datasets\n#\n########################################################################\n\ndef load_main_dataset(whether_merge_districts = True, whether_merge_products = True, whether_merge_dates = True):\n    def reduce_dtype_size(df):\n        numeric_cols = [x for x in df.columns if (df[x].dtype != object) & ('datetime' not in str(df[x].dtype))]\n        for numeric_col in numeric_cols:\n            if('float' in str(df[numeric_col].dtype)):\n                df[numeric_col] = pd.to_numeric(df[numeric_col], downcast = 'float')\n            elif(('uint' in str(df[numeric_col].dtype)) | ('bool' in str(df[numeric_col].dtype))):\n                df[numeric_col] = pd.to_numeric(df[numeric_col], downcast = 'unsigned')\n            else:\n                df[numeric_col] = pd.to_numeric(df[numeric_col], downcast = 'signed')\n        return df\n    \n    def merge_districts_data(engagement_data):\n        districts_data = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/districts_data.csv')\n        districts_data = reduce_dtype_size(districts_data)\n        merged_engagement_data = pd.merge(engagement_data, districts_data, how = 'left', on = 'district_id')\n        return merged_engagement_data\n\n    def merge_products_data(engagement_data):\n        products_data = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/products_data.csv')\n        products_data = reduce_dtype_size(products_data)\n        merged_engagement_data = pd.merge(engagement_data, products_data, how = 'left', left_on = 'lp_id', right_on = 'LP ID')\n        merged_engagement_data = merged_engagement_data.drop('LP ID', axis = 1)\n        return merged_engagement_data\n\n    def merge_dates_data(engagement_data):\n        dates_data = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/dates_data.csv', parse_dates = ['date'])\n        dates_data = reduce_dtype_size(dates_data)\n        merged_engagement_data = pd.merge(engagement_data, dates_data, how = 'left', left_on = 'time', right_on = 'date')\n        merged_engagement_data = merged_engagement_data.drop('date', axis = 1)\n        return merged_engagement_data\n\n    engagement_data = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/engagement_data.csv', parse_dates = ['time'])\n    engagement_data = reduce_dtype_size(engagement_data)\n    if(whether_merge_districts == True):\n        engagement_data = merge_districts_data(engagement_data)\n    if(whether_merge_products == True):\n        engagement_data = merge_products_data(engagement_data)\n    if(whether_merge_dates == True):\n        engagement_data = merge_dates_data(engagement_data)\n    \n    return engagement_data\n\n\n# ########################################################################\n# ### process_main_dataset\n# ########################################################################\n# #\n# # Function to process the main processed & save it for\n# # loading later from another function `load_main_dataset`.\n# #\n# # Inputs:\n# #   1.  whether_load_url_html_data (boolean) -> Whether to process\n# #       and save URL's HTML data\n# #\n# # Return:   None\n# #\n# ########################################################################\n\n# def process_main_dataset(whether_load_url_html_data = False):\n#     def get_all_na_idx(df):\n#         all_na_idx = df.isnull().all(axis=1)\n#         return all_na_idx[all_na_idx == True].keys()\n\n#     def add_dummys(df, dummy_cols, remove_orig_dummy_cols = False):\n#         dummy_df = df[dummy_cols]\n#         dummy_df = pd.get_dummies(dummy_df)\n        \n#         df = pd.concat([df, dummy_df], axis = 1)\n#         if(remove_orig_dummy_cols == True):\n#             df = df.drop(dummy_cols, axis = 1)\n        \n#         return df\n\n#     def reduce_dtype_size(df):\n#         numeric_cols = [x for x in df.columns if (df[x].dtype != object) & ('datetime' not in str(df[x].dtype))]\n#         for numeric_col in numeric_cols:\n#             if('float' in str(df[numeric_col].dtype)):\n#                 df[numeric_col] = pd.to_numeric(df[numeric_col], downcast = 'float')\n#             elif(('uint' in str(df[numeric_col].dtype)) | ('bool' in str(df[numeric_col].dtype))):\n#                 df[numeric_col] = pd.to_numeric(df[numeric_col], downcast = 'unsigned')\n#             else:\n#                 df[numeric_col] = pd.to_numeric(df[numeric_col], downcast = 'signed')\n#         return df\n    \n#     def load_districts_data():\n        \n#         def districts_data_preprocessing(districts_data):\n            \n#             def process_lower_upper_bounds(df_series):\n#                 processed_lower_series = []\n#                 processed_upper_series = []\n                \n#                 for row in df_series:                \n#                     if(pd.isnull(row) == True):\n#                         processed_lower_series.append(row)\n#                         processed_upper_series.append(row)\n#                     else:\n#                         assert(len(row[1:-1].split(', ')) == 2)\n#                         lower_val, upper_val = row[1:-1].split(', ')\n#                         lower_val = float(lower_val)\n#                         upper_val = float(upper_val)\n#                         processed_lower_series.append(lower_val)\n#                         processed_upper_series.append(upper_val)\n                \n#                 return processed_lower_series, processed_upper_series\n            \n#             for col in ['pct_black\/hispanic', 'pct_free\/reduced', 'county_connections_ratio', 'pp_total_raw']:\n#                 lower_series, upper_series = process_lower_upper_bounds(districts_data[col])\n#                 districts_data[col + '_lower_bound'] = pd.Series(lower_series, index = districts_data.index)\n#                 districts_data[col + '_upper_bound'] = pd.Series(upper_series, index = districts_data.index)\n#                 districts_data[col + '_bound_avg'] = pd.Series(np.add(lower_series, upper_series) \/ 2.0, index = districts_data.index)\n            \n#             districts_data = districts_data.drop(['pct_black\/hispanic', 'pct_free\/reduced', 'county_connections_ratio', 'pp_total_raw'], axis = 1)\n#             return districts_data\n        \n#         districts_data = pd.read_csv('\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv')\n        \n#         districts_data = districts_data_preprocessing(districts_data)\n#         districts_data = districts_data.drop(get_all_na_idx(districts_data.drop('district_id', axis = 1))).reset_index(drop = True)\n#         districts_data = add_dummys(districts_data, ['locale'], remove_orig_dummy_cols = False)\n        \n#         #all_states = districts_data['state'].unique()\n#         #district_id_state_map = dict(districts_data[['district_id', 'state']].values)\n#         #state_district_id_map = dict([(x, [y for y in district_id_state_map if district_id_state_map[y] == x]) for x in all_states])\n#         #districts_data = districts_data.drop('state', axis = 1)\n        \n#         districts_data = reduce_dtype_size(districts_data)\n#         return districts_data\n\n#     districts_data = load_districts_data()\n    \n#     # URL Information Extraction\n#     #   For July 2021\n#     #   Using similarweb.com\n#     #   Avg Duration - in seconds\n#     #   Total Visits - in 1000s\n#     def load_url_html_data():\n#         url_html_dict = {}\n\n#         all_html_content = \"\"\n#         with open(f'.\/Data\/url_info\/combined_url_info_data.txt', 'r') as html_file:\n#             all_html_content = html_file.read()\n\n#         tot_num_files = len([x for x in all_html_content.split('---') if len(x.strip()) != 0])\n#         print(f\"Total No. of files: {tot_num_files}\\n\\n\")\n\n#         for html_content_idx, html_content in enumerate(all_html_content.split('---')):\n            \n#             html_content = html_content.strip()\n#             if(len(html_content) == 0):\n#                 print(\"ERROR: URL not found\")\n#                 sys.exit(\"\")\n            \n#             url_name = html_content.split('<')[0].strip()\n#             html_content = '<'.join(html_content.split('<')[1:])\n#             print(f\"{html_content_idx + 1}. File: {url_name}\\n\")\n#             if(url_name in url_html_dict):\n#                 print(\"ERROR: Name already exists\")\n#                 sys.exit(\"\")\n#             url_html_dict[url_name] = {}\n            \n#             # Global Rank\n#             global_rank = re.findall('\\\"GlobalRank\":\\[\\d+,\\d+,-?\\d+,\\d+\\]', html_content)\n#             if(len(global_rank) != 1):\n#                 print(\"ERROR: Global Rank\")\n#                 print(global_rank)\n#                 sys.exit(\"\")\n#             global_rank = int(global_rank[0].split('[')[1].split(',')[0])\n#             url_html_dict[url_name]['global_rank'] = global_rank\n\n#             # Country\n#             if('<img class=\"websiteRanks-titleIconImg\" src=\"\/images\/flags-svg\/flag-icon-us.svg\">' in html_content):\n#                 country = 'USA'\n#             else:\n#                 print(\"Country: Not USA!\")\n#                 country = 'Not_USA'\n#             url_html_dict[url_name]['country'] = country\n            \n#             # Country Rank\n#             country_rank = re.findall('\\\"CountryRanks\":\\{\"\\d+\":\\[\\d+,\\d+,-?\\d+,\\d+\\]\\}', html_content)\n#             if(len(country_rank) != 1):\n#                 print(\"ERROR: Country Rank\")\n#                 print(country_rank)\n#                 sys.exit(\"\")\n#             country_rank = int(country_rank[0].split('[')[1].split(',')[0])\n#             url_html_dict[url_name]['country_rank'] = country_rank\n            \n#             # Category\n#             category = re.findall('<a class=\"websiteRanks-nameText\" data-analytics-category=\"Internal Link\" data-analytics-label=\"Category Rank\/.+\" href=\"\/top-websites\/category\/.+\" itemprop=\"significantLink\">.+<\/a>', html_content)\n#             if(len(category) != 1):\n#                 print(\"ERROR: Category\")\n#                 print(category)\n#                 url_html_dict[url_name]['main_category'] = np.nan\n#                 url_html_dict[url_name]['sub_category'] = np.nan\n#             else:\n#                 category = category[0].split('Category Rank\/')[1].split('\"')[0]\n#                 main_category = category.split('\/')[0]\n#                 url_html_dict[url_name]['main_category'] = main_category\n#                 if(len(category.split('\/')) != 1):\n#                     sub_category = category.split('\/')[-1]\n#                 else:\n#                     sub_category = \"\"\n#                 url_html_dict[url_name]['sub_category'] = sub_category\n\n#             # Category Rank\n#             category_rank = re.findall('\\\"CategoryRank\\\":\\[\\d+,\\d+,-?\\d+,\\d+\\]', html_content)\n#             if(len(category_rank) != 1):\n#                 print(\"ERROR: Category Rank\")\n#                 print(category_rank)\n#                 sys.exit(\"\")\n#             category_rank = int(category_rank[0].split('[')[1].split(',')[0])\n#             url_html_dict[url_name]['category_rank'] = category_rank\n\n#             # Total Visits\n#             total_visits = re.findall('<span class=\"engagementInfo-valueNumber js-countValue\">.*\\d+[KMB]<\/span>', html_content)\n#             if(len(total_visits) != 1):\n#                 print(\"ERROR: Total Visits\")\n#                 print(total_visits)\n#                 url_html_dict[url_name]['total_visits'] = np.nan\n#             else:\n#                 total_visits = total_visits[0].split('>')[1].split('<')[0]\n#                 units = total_visits[-1]\n#                 total_visits = float(''.join([x for x in total_visits if x.isdigit()]))\n#                 if(units == 'K'):\n#                     total_visits = total_visits * 1\n#                 elif(units == 'M'):\n#                     total_visits = total_visits * 1000\n#                 elif(units == 'B'):\n#                     total_visits = total_visits * 1000000\n#                 url_html_dict[url_name]['total_visits'] = total_visits\n\n#             # Avg Duration\n#             avg_duration = re.findall('<span class=\"engagementInfo-valueNumber js-countValue\">\\d+:\\d+:\\d+<\/span>', html_content)\n#             if(len(avg_duration) != 1):\n#                 print(\"ERROR: Avg Duration\")\n#                 print(avg_duration)\n#                 url_html_dict[url_name]['avg_duration'] = np.nan\n#             else:\n#                 avg_duration = avg_duration[0].split('>')[1].split('<')[0]\n#                 hr_val, min_val, sec_val = avg_duration.split(':')\n#                 avg_duration = 3600 * int(hr_val) + 60 * int(min_val) + int(sec_val)\n#                 url_html_dict[url_name]['avg_duration'] = avg_duration\n\n#             # Page Visits\n#             page_visits = re.findall('<span class=\"engagementInfo-valueNumber js-countValue\">\\d+\\.\\d+<\/span>', html_content)\n#             if(len(page_visits) != 1):\n#                 print(\"ERROR: Page Visits\")\n#                 print(page_visits)\n#                 url_html_dict[url_name]['page_visits'] = np.nan\n#             else:\n#                 page_visits = float(page_visits[0].split('>')[1].split('<')[0])\n#                 url_html_dict[url_name]['page_visits'] = page_visits\n\n#             # Bounce Rate\n#             bounce_rate = re.findall('<span class=\"engagementInfo-valueNumber js-countValue\">\\d+.\\d+%<\/span>', html_content)\n#             if(len(bounce_rate) != 1):\n#                 print(\"ERROR: Bounce Rate\")\n#                 print(bounce_rate)\n#                 url_html_dict[url_name]['bounce_rate'] = np.nan\n#             else:\n#                 bounce_rate = float(bounce_rate[0].split('>')[1].split('%')[0])\n#                 url_html_dict[url_name]['bounce_rate'] = bounce_rate\n            \n#             # Description\n#             description = re.findall('<p itemprop=\"description\" class=\"websiteHeader-companyDescription js-companyDescription\">.+<\/p>', html_content)\n#             if(len(description) != 1):\n#                 url_html_dict[url_name]['description'] = np.nan\n#             else:\n#                 description = description[0].split('>')[1].split('<')[0]\n#                 url_html_dict[url_name]['description'] = description\n\n#         url_html_df = pd.DataFrame.from_dict(url_html_dict, orient = 'index').reset_index(drop = False)\n#         url_html_df.columns = ['URL'] + [*url_html_df.columns][1:]\n\n#         url_html_df['global_rank'] = url_html_df['global_rank'].replace({0: np.nan})\n#         url_html_df['country_rank'] = url_html_df['country_rank'].replace({0: np.nan})\n#         url_html_df['category_rank'] = url_html_df['category_rank'].replace({0: np.nan})\n\n#         additional_data = pd.read_csv('.\/Data\/url_info\/url_info_mobile.csv')\n#         url_html_df = pd.concat([url_html_df, additional_data], axis = 0)\n\n#         def find_subpage_level(url):\n#             level = url.split(':\/\/')[1]\n#             level = level.split('\/')\n#             level = [x for x in level if len(x.strip()) > 0]\n#             return len(level) - 1\n#         url_html_df['URL_subpage_level'] = url_html_df['URL'].apply(find_subpage_level)\n#         url_html_df['URL_subpage_visits'] = url_html_df.apply(lambda x: x['total_visits'] * (((100 - x['bounce_rate']) \/ 100) ** x['URL_subpage_level']), axis = 1)\n#         url_html_df['URL_page_duration'] = url_html_df['avg_duration'] \/ url_html_df['page_visits']\n#         url_html_df['URL_subpage_total_browsing_days'] = url_html_df['URL_subpage_visits'] * url_html_df['URL_page_duration'] \/ 60 \/ 60 \/ 24\n#         url_html_df['URL_subpage_avg_browsing_days'] = url_html_df['URL_subpage_total_browsing_days'] \/ 31\n#         url_html_df.to_csv('.\/Data\/url_info\/final_url_data.csv', index = False)\n    \n#     def load_products_data():\n    \n#         def products_data_preprocessing(products_data):\n\n#             products_data['Primary Category'] = products_data['Primary Essential Function'].apply(lambda x: x.split(' - ')[0] if pd.isna(x) == False else np.nan)\n#             products_data['Primary Category'] = products_data['Primary Category'].map({'LC': 'LC', 'CM': 'CM', 'SDO': 'SDO', 'LC\/CM\/SDO': 'Other'})\n#             products_data['Primary Essential Function'] = products_data['Primary Essential Function'].apply(lambda x: x.split(' - ')[1] if pd.isna(x) == False else np.nan)\n            \n#             def sector_map(sectors):\n                \n#                 sector_prek12 = []\n#                 sector_higher_ed = []\n#                 sector_corporate = []\n                \n#                 for sector in sectors:\n#                     if(pd.isna(sector) == True):\n#                         sector_prek12.append(0)\n#                         sector_higher_ed.append(0)\n#                         sector_corporate.append(0)\n#                     elif(sector == 'PreK-12'):\n#                         sector_prek12.append(1)\n#                         sector_higher_ed.append(0)\n#                         sector_corporate.append(0)\n#                     elif(sector == 'PreK-12; Higher Ed'):\n#                         sector_prek12.append(1)\n#                         sector_higher_ed.append(1)\n#                         sector_corporate.append(0)\n#                     elif(sector == 'PreK-12; Higher Ed; Corporate'):\n#                         sector_prek12.append(1)\n#                         sector_higher_ed.append(1)\n#                         sector_corporate.append(1)\n#                     elif(sector == 'Corporate'):\n#                         sector_prek12.append(0)\n#                         sector_higher_ed.append(0)\n#                         sector_corporate.append(1)\n#                     elif(sector == 'Higher Ed; Corporate'):\n#                         sector_prek12.append(0)\n#                         sector_higher_ed.append(1)\n#                         sector_corporate.append(1)\n#                     else:\n#                         print(f\"***\\nUnknown sector detected! {sector}\\n***\")\n                \n#                 return sector_prek12, sector_higher_ed, sector_corporate\n            \n#             sector_prek12, sector_higher_ed, sector_corporate = sector_map(products_data['Sector(s)'])\n#             products_data = products_data.assign(Sector_prek12 = sector_prek12)\n#             products_data = products_data.assign(Sector_higher_ed = sector_higher_ed)\n#             products_data = products_data.assign(Sector_corporate = sector_corporate)\n            \n#             products_data['Primary Essential Function'] = products_data['Primary Essential Function'].replace({\"Sites, Resources & References\": \"Sites, Resources & Reference\"})\n            \n#             # Correcting small mistakes\n#             products_data['URL'] = products_data['URL'].replace({'https:\/\/fligprid.com': 'https:\/\/flipgrid.com'})\n\n#             if(whether_load_url_html_data == True):\n#                 load_url_html_data()\n#             url_info_data = pd.read_csv('.\/Data\/url_info\/final_url_data.csv')\n#             url_info_data.columns = ['mainURL_' + x if x != 'URL' else x for x in url_info_data.columns]\n#             products_data = pd.merge(products_data, url_info_data, how = 'left', on = 'URL')\n            \n#             products_data['mainURL_country_rank'] = products_data.apply(lambda x: x if x['mainURL_country'] == 'USA' else np.nan, axis = 1)\n            \n#             return products_data\n        \n#         products_data = pd.read_csv('.\/Data\/learnplatform-covid19-impact-on-digital-learning\/products_info.csv')\n        \n#         products_data = products_data_preprocessing(products_data)\n#         products_data = add_dummys(products_data, ['Sector(s)', 'Primary Category', 'Primary Essential Function', 'mainURL_main_category'], remove_orig_dummy_cols = False)\n        \n#         products_data = reduce_dtype_size(products_data)\n#         return products_data\n\n#     products_data = load_products_data()\n\n#     # Not focusing on vacation dates since time-analysis is not priority\n\n#     def load_dates_data():\n#         days_2020 = pd.date_range(datetime(2020, 1, 1), datetime(2020, 12, 31))\n#         dates_data = pd.DataFrame.from_dict({'date': days_2020})\n        \n#         dates_data['month'] = dates_data['date'].apply(lambda x: x.month)\n#         dates_data['day'] = dates_data['date'].apply(lambda x: x.day)\n#         dates_data['day_of_week'] = dates_data['date'].apply(lambda x: calendar.day_name[x.weekday()])\n#         dates_data['is_weekend'] = dates_data['day_of_week'].apply(lambda x: 1 if (x == 'Saturday') | (x == 'Sunday') else 0)\n        \n#         us_holidays = pd.read_csv('.\/Data\/US Holiday Dates (2004-2021).csv', usecols = ['Date', 'Holiday'], parse_dates = ['Date'])\n#         dates_data = pd.merge(dates_data, us_holidays, how = 'left', left_on = 'date', right_on = 'Date')\n#         dates_data = dates_data.drop('Date', axis = 1)\n#         dates_data['is_holiday'] = dates_data['Holiday'].apply(lambda x: 0 if pd.isnull(x) == True else 1)\n        \n#         dates_data = add_dummys(dates_data, ['day_of_week', 'Holiday'], remove_orig_dummy_cols = False)\n#         dates_data = reduce_dtype_size(dates_data)\n#         return dates_data\n\n#     dates_data = load_dates_data()\n\n#     def load_engagement_data():\n\n#         engagement_data = pd.DataFrame()\n#         districts = []\n        \n#         for x in os.listdir('.\/Data\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/'):\n#             data_x = pd.read_csv(f'.\/Data\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/{x}', parse_dates = ['time'])\n#             engagement_data = pd.concat([engagement_data, data_x], axis = 0)\n#             districts.extend([int(x.split('.')[0])] * data_x.shape[0])\n        \n#         engagement_data['district_id'] = pd.Series(districts, index = engagement_data.index)\n        \n#         top_products_id = list(products_data['LP ID'].unique())\n#         districts_id = list(districts_data['district_id'].unique())\n#         engagement_data = engagement_data[engagement_data['lp_id'].isin(top_products_id)]\n#         engagement_data = engagement_data[engagement_data['district_id'].isin(districts_id)]\n        \n#         same_url_map = {\n#             33562: 75206,\n#             87841: 35971\n#         }\n#         engagement_data['lp_id'] = engagement_data['lp_id'].replace(same_url_map)\n#         engagement_data = engagement_data.groupby(['time', 'lp_id', 'district_id'])[['pct_access', 'engagement_index']].aggregate(np.nansum).reset_index()\n        \n#         engagement_data = reduce_dtype_size(engagement_data)\n#         return engagement_data\n\n#     engagement_data = load_engagement_data()\n\n#     # # Saving all datasets\n#     districts_data.to_csv('.\/Data\/Processed_Dataset\/districts_data.csv', index = False)\n#     products_data.to_csv('.\/Data\/Processed_Dataset\/products_data.csv', index = False)\n#     dates_data.to_csv('.\/Data\/Processed_Dataset\/dates_data.csv', index = False)\n#     engagement_data.to_csv('.\/Data\/Processed_Dataset\/engagement_data.csv', index = False)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\nCOLOR_WHITE = '#F8F1FF'\nCOLOR_BLACK = '#231942'\nCOLOR_DARK_BLUE = '#156BB7'\nCOLOR_LIGHT_BLUE = '#63D1DF'\nCOLOR_GREEN = '#30DB8D'\nCOLOR_DARK_GREEN = '#0DAB6C'\nCOLOR_ORANGE = '#FBAB60'\nCOLOR_YELLOW = '#F8E16C'\nCOLOR_RED = '#DA4167'\n\nPLOT_THEME_LIGHT = {\n    'text': COLOR_BLACK,\n    'axis': COLOR_BLACK,\n    'subtitle': COLOR_DARK_BLUE,\n    'color+1': COLOR_DARK_GREEN,\n    'color+2': COLOR_YELLOW,\n    'color+3': COLOR_ORANGE,\n    'color+4': COLOR_DARK_BLUE,\n    'color-1': COLOR_RED,\n    'bg': COLOR_LIGHT_BLUE,\n    'inv': COLOR_WHITE,\n    'color+1_lower': '#064B30',\n    'color+1_higher': '#2EEFA2',\n    'gray': '#676076',\n}\nPLOT_THEME_LIGHT['groups'] = [PLOT_THEME_LIGHT[x] for x in ['color+1', 'color-1', 'color+3', 'color+4', 'color+2']]\n\nPLOT_THEME_DARK = {\n    'text': COLOR_WHITE,\n    'axis': COLOR_WHITE,\n    'subtitle': COLOR_LIGHT_BLUE,\n    'color+1': COLOR_GREEN,\n    'color+2': COLOR_YELLOW,\n    'color+3': COLOR_ORANGE,\n    'color+4': COLOR_LIGHT_BLUE,\n    'color-1': COLOR_RED,\n    'bg': COLOR_DARK_BLUE,\n    'inv': COLOR_BLACK,\n    'color+1_lower': '#188B57',\n    'color+1_higher': '#86EABD',\n    'gray': '#D6CFDB',\n}\nPLOT_THEME_DARK['groups'] = [PLOT_THEME_DARK[x] for x in ['color+1', 'color-1', 'color+3', 'color+4', 'color+2']]\n\ndef create_fig(nrows = 1, ncols = 1, width = 10, height = 5):\n    fig, ax = plt.subplots(nrows, ncols, figsize = (width, height))\n    return fig, ax\n\ndef remove_spines(ax, theme = {}):\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_color(theme['axis'])\n    ax.spines['left'].set_color(theme['axis'])\n    return ax\n\ndef remove_all_spines(ax):\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    return ax\n\ndef set_titles_and_labels(fig, ax, fig_title = \"\", title = \"\", xlabel = \"\", ylabel = \"\", theme = {}):\n    fig.suptitle(fig_title, fontsize = 30, color = theme['text'])\n    ax.set_title(title, fontsize = 20, color = theme['subtitle'])\n    ax.set_xlabel(xlabel, fontsize = 15, color = theme['text'])\n    ax.set_ylabel(ylabel, fontsize = 15, color = theme['text'])\n    return fig, ax\n\ndef set_ticks(ax, theme):\n    ax.tick_params(axis = 'x', colors = theme['axis'])\n    ax.tick_params(axis = 'y', colors = theme['axis'])\n    return ax\n\ndef set_xticklabels(ax, labels, rotate_x = 0, theme = {}):\n    ax.set_xticks(np.arange(len(labels)))\n    ax.set_xticklabels(labels, color = theme['text'], rotation = rotate_x)\n    return ax\n\ndef set_yticklabels(ax, labels, rotate_y = 0, theme = {}):\n    ax.set_yticks(np.arange(len(labels)))\n    ax.set_yticklabels(labels, color = theme['text'], rotation = rotate_y)\n    return ax\n\ndef set_bg(fig, ax, theme):\n    fig.set_facecolor(theme['bg'])\n    ax.set_facecolor(theme['bg'])\n    return fig, ax\n\ndef select_theme(theme):\n    if(theme == 'DARK'):\n        return PLOT_THEME_DARK\n    else:\n        return PLOT_THEME_LIGHT\n\ndef set_legend(ax, theme):\n    ax.legend(loc = 'best')\n    return ax\n\ndef plot_decoration():\n    return \"\"\"\n    fig, ax = set_bg(fig, ax, theme); ax = set_ticks(ax, theme); ax = remove_spines(ax, theme); fig, ax = set_titles_and_labels(fig, ax, suptitle, title, xlabel, ylabel, theme);\n    \"\"\".strip()\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_lineplot(x_vals, y_vals, width = 15, height = 7, labels = [], suptitle = \"Lineplot\", title = \"Demo\", xlabel = \"\", ylabel = \"\", theme = 'DARK', x_names = [], rotate_x = 0):\n    theme = select_theme(theme)\n    fig, ax = create_fig(1, 1, width, height)\n    if(len(y_vals) > len(theme['groups'])):\n        group_colors = [(np.random.random(), np.random.random(), np.random.random()) for _ in range(len(y_vals))]\n    else:\n        group_colors = theme['groups'][:len(y_vals)]\n    for y_val_idx, y_val in enumerate(y_vals):\n        if(len(labels) == 0):\n            ax.plot(x_vals, y_val, lw = 3, color = group_colors[y_val_idx], label = f'line #{y_val_idx + 1}')\n        else:\n            ax.plot(x_vals, y_val, lw = 3, color = group_colors[y_val_idx], label = labels[y_val_idx])\n    ax = set_legend(ax, theme)\n    ax = set_xticklabels(ax, x_names, rotate_x = rotate_x, theme = theme)\n    exec(plot_decoration())\n    plt.show()\n\ndef plot_barplot(x_names, y_vals, cats = [], width = 15, height = 7, suptitle = \"Barplot\", title = \"Demo\", xlabel = '', ylabel = '', theme = 'DARK', rotate_x = 0, rotate_y = 0):\n    theme = select_theme(theme)\n    fig, ax = create_fig(1, 1, width, height)\n    x_vals = np.arange(len(x_names))\n    if(len(cats) > 0):\n        uniq_cats = list(sorted(pd.Series(cats).unique()))\n        if(len(uniq_cats) > len(theme['groups'])):\n            group_colors = [(np.random.random(), np.random.random(), np.random.random()) for _ in range(len(uniq_cats))]\n        else:\n            group_colors = theme['groups'][:len(uniq_cats)]\n        group_colors = [group_colors[uniq_cats.index(x)] for x in cats]\n    else:\n        if(len(y_vals) > len(theme['groups'])):\n            group_colors = [(np.random.random(), np.random.random(), np.random.random()) for _ in range(len(y_vals))]\n        else:\n            group_colors = theme['groups'][:len(y_vals)]\n    ax.bar(x_vals, y_vals, color = group_colors)\n    ax = set_xticklabels(ax, x_names, rotate_x = rotate_x, theme = theme)\n    exec(plot_decoration())\n    plt.show()\n\ndef plot_scatterplot(x_vals, y_vals, cats = [1], width = 15, height = 7, suptitle = \"Scatterplot\", title = 'Demo', xlabel = '', ylabel = '', theme = 'DARK', annotate = False, annotate_texts = []):\n    theme = select_theme(theme)\n    fig, ax = create_fig(1, 1, width, height)\n    no_cats_passed = False\n    if(len(cats) == 1):\n        cats = np.ones(len(x_vals))\n        no_cats_passed = True\n    uniq_cats = pd.Series(cats).unique()\n    if(len(uniq_cats) > len(theme['groups'])):\n        group_colors = [(np.random.random(), np.random.random(), np.random.random()) for _ in range(len(uniq_cats))]\n    else:\n        group_colors = theme['groups'][:len(uniq_cats)]\n    for cat_idx, cat in enumerate(uniq_cats):\n        ax.scatter(x_vals[cats == cat], y_vals[cats == cat], color = group_colors[cat_idx], label = cat)\n    if(annotate == True):\n        for idx in range(len(x_vals)):\n            ax.annotate(annotate_texts[idx], (x_vals[idx], y_vals[idx]), color = theme['text'])\n    if(no_cats_passed == False):\n        ax = set_legend(ax, theme)\n    exec(plot_decoration())\n    plt.show()\n\ndef plot_us_map(state_vals_df, title, val_col, val_label, range_min_val = 0, range_max_val = 1, theme = 'DARK', state_col = 'STATE_ABBR'):\n    \n    theme = select_theme(theme)\n\n    layout = dict(\n        font_family = 'Source Sans Pro',\n        font_color = theme['text'],\n        title_text = title,\n        # To change\n        title_font = dict(\n            family = \"Source Sans Pro\",\n            size = 25,\n            color = theme['axis']\n        ),\n        geo_scope = 'usa',\n        paper_bgcolor = theme['bg'],\n        geo_bgcolor = theme['bg'],\n        geo = dict(\n            landcolor = theme['inv'],\n            subunitcolor = theme['gray'],\n            lakecolor = theme['bg'],\n        ),\n    )\n\n    fig = px.choropleth(\n        state_vals_df,\n        locations = state_col,\n        color = val_col,\n        color_continuous_scale = [theme['color-1'], theme['color+1_higher']],\n        range_color = (range_min_val, range_max_val),\n        locationmode = \"USA-states\",\n        labels = {val_col : val_label, state_col: 'State'},\n    )\n\n    fig.update_layout(layout)\n    fig.update_layout(margin = {\"r\": 0, \"l\": 0, \"b\": 15})\n    fig.show()\n    \nDATA = load_main_dataset()","6293eb7e":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport sys\n\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import pearsonr\n\ndata = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/SVI2018_US.csv')\ndata = data.replace(-999, np.nan)\ndata = data.dropna()\ndata = data[data['ST_ABBR'] != 'AK'].reset_index(drop = True)\n\nsocioeconomic_cols = [\n    'E_POV', 'E_UNEMP', 'E_PCI', 'E_NOHSDP', 'EP_POV',\n    'EP_UNEMP', 'EP_PCI', 'EP_NOHSDP', 'EPL_UNEMP',\n    'EPL_PCI', 'EPL_NOHSDP', 'SPL_THEME1', 'RPL_THEME1', 'F_POV', 'F_UNEMP', 'F_PCI', 'F_NOHSDP', 'F_THEME1',\n]\n\nhousehold_cols = [\n    'E_AGE65', 'E_AGE17', 'E_DISABL', 'E_SNGPNT', 'EP_AGE65',\n    'EP_AGE17', 'EP_DISABL', 'EP_SNGPNT', 'EPL_AGE65',\n    'EPL_AGE17', 'EPL_DISABL', 'EPL_SNGPNT', 'SPL_THEME2', 'RPL_THEME2', 'F_AGE65', 'F_AGE17', 'F_DISABL',\n    'F_SNGPNT', 'F_THEME2',\n]\n\nminority_cols = [\n    'E_MINRTY', 'E_LIMENG', 'EP_MINRTY', 'EP_LIMENG',\n    'EPL_MINRTY', 'EPL_LIMENG', 'SPL_THEME3', 'RPL_THEME3', 'F_MINRTY', 'F_LIMENG', 'F_THEME3',\n]\n\nhousing_cols = [\n    'E_MUNIT', 'E_MOBILE', 'E_CROWD', 'E_NOVEH', 'E_GROUPQ',\n    'EP_MUNIT', 'EP_MOBILE', 'EP_CROWD', 'EP_NOVEH',\n    'EP_GROUPQ', 'EPL_POV', 'EPL_MUNIT', 'EPL_MOBILE', 'EPL_CROWD', 'EPL_NOVEH', 'EPL_GROUPQ',\n    'SPL_THEME4', 'RPL_THEME4', 'F_MUNIT', 'F_MOBILE', 'F_CROWD', 'F_NOVEH', 'F_GROUPQ', 'F_THEME4', 'F_TOTAL',\n]\n\n# IMPORTANT:\n# Alaska not considered in analysis since the extremely large area of Alaska dominates over other features\n\nfrom sklearn.decomposition import PCA\n\nsocial_vulnerability_metrics_names = []\nsocial_vulnerability_metrics = []\n\nfor feat_idx, feats in enumerate([socioeconomic_cols, household_cols, minority_cols, housing_cols]):\n\n    data_for_pca = data[feats]\n    for col in data_for_pca.columns:\n        data_for_pca[col] = (data_for_pca[col] - data_for_pca[col].mean()) \/ (data_for_pca[col].std() + 0.0000000000001)\n\n    pca = PCA(n_components = 10)\n    pca_orig_data = pca.fit_transform(data_for_pca)\n    for pca_idx in range(10):\n        pca_data = pca.components_[pca_idx]\n        expl_var = np.sum(pca.explained_variance_ratio_[:pca_idx + 1]) * 100\n        if(pca_data.sum() < 0):\n            if(feat_idx not in [0]):\n                pca_data = -pca_data\n                social_vulnerability_metrics.append(-pca_orig_data[:, pca_idx])\n            else:\n                social_vulnerability_metrics.append(pca_orig_data[:, pca_idx])\n        else:\n            if(feat_idx in [0]):\n                pca_data = -pca_data\n                social_vulnerability_metrics.append(-pca_orig_data[:, pca_idx])\n            else:\n                social_vulnerability_metrics.append(pca_orig_data[:, pca_idx])\n        \n        if(feat_idx == 0):\n            social_vulnerability_metrics_names.append(f'socioeconomic_{pca_idx+1}')\n            col_name = \"Socio-Economic Indicators\"\n            cats = [1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0]\n        elif(feat_idx == 1):\n            social_vulnerability_metrics_names.append(f'household_{pca_idx+1}')\n            col_name = \"Household Composition \/ Disability\"\n            cats = [1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1]\n        elif(feat_idx == 2):\n            social_vulnerability_metrics_names.append(f'minority_{pca_idx+1}')\n            col_name = \"Minority Status \/ Language\"\n            cats = [1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1]\n        elif(feat_idx == 3):\n            social_vulnerability_metrics_names.append(f'housing_{pca_idx+1}')\n            col_name = \"Housing Type \/ Transportation\"\n            cats = [1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1]\n\n        if(pca_idx == 0):\n            plot_barplot(\n                [x for x in feats], pca_data,\n                width = 15, height = 7,\n                suptitle = f\"{col_name}\", title = f\"PCA Component {pca_idx + 1}, Explained Variance = {expl_var:.2f}%\",\n                xlabel = \"Features\", ylabel = \"PCA Weight\",\n                rotate_x = 60,\n                theme = 'DARK',\n                cats = cats,\n            )\n        \n        if(expl_var >= 70):\n            break","d2ff49e7":"social_vulnerability_metrics = pd.DataFrame(np.asarray(social_vulnerability_metrics).transpose(), columns = social_vulnerability_metrics_names)\n#print(data.shape, social_vulnerability_metrics.shape)\nsocial_vulnerability_data = pd.concat([data[['ST_ABBR']], social_vulnerability_metrics], axis = 1)\nsocial_vulnerability_data = social_vulnerability_data.groupby('ST_ABBR')[social_vulnerability_metrics_names].aggregate(np.nanmean).reset_index(drop = False)\n\nplot_us_map(social_vulnerability_data, \"Socioeconomic Indicators\", \"socioeconomic_1\", \"PCA Component 1\", -3, 3, state_col = 'ST_ABBR', theme = 'LIGHT')\nplot_us_map(social_vulnerability_data, \"Household Indicators\", \"household_1\", \"PCA Component 1\", -3, 3, state_col = 'ST_ABBR', theme = 'LIGHT')\nplot_us_map(social_vulnerability_data, \"Minority Indicators\", \"minority_1\", \"PCA Component 1\", -3, 3, state_col = 'ST_ABBR', theme = 'LIGHT')\nplot_us_map(social_vulnerability_data, \"Housing Indicators\", \"housing_1\", \"PCA Component 1\", -3, 3, state_col = 'ST_ABBR', theme = 'LIGHT')","1c52c970":"engagement_data = DATA.groupby('state')[['pct_access', 'engagement_index']].aggregate(np.nanmean).reset_index(drop = False)\nengagement_data['state'] = engagement_data['state'].replace(STATE_ABBR)\n\nsocial_vulnerability_engagement_data = pd.merge(engagement_data, social_vulnerability_data, how = 'inner', left_on = 'state', right_on = 'ST_ABBR')\n\ntarget_cols = ['pct_access', 'engagement_index']\ncorrs_table = [\n    ['FEATURE', 'CORRELATION WITH PCT_ACCESS', 'CORRELATION WITH ENGAGEMENT_INDEX']\n]\nfor col_idx, col in enumerate(social_vulnerability_metrics_names):\n    row = [col]\n    for target_col_idx, target_col in enumerate(target_cols):\n        corr, corr_pval = pearsonr(social_vulnerability_engagement_data[target_cols[target_col_idx]], social_vulnerability_engagement_data[social_vulnerability_metrics_names[col_idx]])\n        row.append(f\"{corr:.2f} (pval = {corr_pval:.2f})\")\n    corrs_table.append(row)\n\nfrom IPython.display import HTML, display\nimport tabulate\ndisplay(HTML(tabulate.tabulate(corrs_table, tablefmt='html')))","b28a3b4b":"fig, ax = plt.subplots(1, 2, figsize = (25, 10))\nsns.regplot(\n    x = social_vulnerability_engagement_data['socioeconomic_3'],\n    y = social_vulnerability_engagement_data['pct_access'],\n    ax = ax[0],\n    color = COLOR_GREEN,\n)\nsns.regplot(\n    x = social_vulnerability_engagement_data['socioeconomic_3'],\n    y = social_vulnerability_engagement_data['engagement_index'],\n    ax = ax[1],\n    color = COLOR_GREEN,\n)\n\ntheme = select_theme('DARK')\nfor a in ax:\n    fig, a = set_bg(fig, a, theme)\n    a = set_ticks(a, theme)\n    a = remove_spines(a, theme)\nfig, ax[0] = set_titles_and_labels(fig, ax[0], \"\", \"Correlation with pct_access\", \"Feature\", \"pct_access\", theme);\nfig, ax[1] = set_titles_and_labels(fig, ax[1], \"Socioeconomic PCA Component #3\", \"Correlation with engagement_index\", \"Feature\", \"engagement_index\", theme);\n\nplt.show()","2c0d151d":"data_for_pca = data[socioeconomic_cols]\nfor col in data_for_pca.columns:\n    data_for_pca[col] = (data_for_pca[col] - data_for_pca[col].mean()) \/ (data_for_pca[col].std() + 0.0000000000001)\n\npca = PCA(n_components = 10)\npca_orig_data = pca.fit_transform(data_for_pca)\npca_idx = 2\npca_data = pca.components_[pca_idx]\nexpl_var = np.sum(pca.explained_variance_ratio_[:pca_idx + 1]) * 100\nif(pca_data.sum() < 0):\n    pca_data = -pca_data\n\ncats = [1,0,1,1,1,1,1,1,0,1,1,1,1,0,1,0,0,0]\nplot_barplot(\n    [x for x in socioeconomic_cols], pca_data,\n    width = 20, height = 10,\n    suptitle = f\"PCA Component {pca_idx + 1} for Socioeconomic Indicators\", title = \"\",\n    xlabel = \"Features\", ylabel = \"Weight\",\n    rotate_x = 60,\n    theme = 'DARK',\n    cats = cats,\n)","0aa8512e":"corr_check_cols = ['E_UNEMP', 'EPL_UNEMP', 'F_POV', 'F_PCI', 'F_NOHSDP', 'F_THEME1']\n#corr_check_cols = socioeconomic_cols\ngrouped_data = data.groupby('ST_ABBR')[corr_check_cols].aggregate(np.nanmean).reset_index(drop = False)\ndata2 = pd.merge(engagement_data, grouped_data, how = 'inner', left_on = 'state', right_on = 'ST_ABBR')\n\ntarget_cols = ['pct_access', 'engagement_index']\ncorrs_table = [\n    ['FEATURE', 'CORRELATION WITH PCT_ACCESS', 'CORRELATION WITH ENGAGEMENT_INDEX']\n]\nfor col_idx, col in enumerate(corr_check_cols):\n    row = [col]\n    for target_col_idx, target_col in enumerate(target_cols):\n        corr, corr_pval = pearsonr(data2[target_cols[target_col_idx]], data2[corr_check_cols[col_idx]])\n        row.append(f\"{corr:.2f} (pval = {corr_pval:.2f})\")\n    corrs_table.append(row)\ndisplay(HTML(tabulate.tabulate(corrs_table, tablefmt='html')))","9f721756":"fig, ax = plt.subplots(1, 2, figsize = (25, 10))\nsns.regplot(\n    x = data2['E_UNEMP'],\n    y = data2['pct_access'],\n    ax = ax[0],\n    color = COLOR_GREEN,\n)\nsns.regplot(\n    x = data2['E_UNEMP'],\n    y = data2['engagement_index'],\n    ax = ax[1],\n    color = COLOR_GREEN,\n)\n\ntheme = select_theme('DARK')\nfor a in ax:\n    fig, a = set_bg(fig, a, theme)\n    a = set_ticks(a, theme)\n    a = remove_spines(a, theme)\nfig, ax[0] = set_titles_and_labels(fig, ax[0], \"\", \"Correlation with pct_access\", \"Feature\", \"pct_access\", theme);\nfig, ax[1] = set_titles_and_labels(fig, ax[1], \"Unemployment Feature\", \"Correlation with engagement_index\", \"Feature\", \"engagement_index\", theme);\n\nplt.show()","bb1325f5":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport sys\nimport time\n\nimport matplotlib.pyplot as plt\n\nfrom pandas.plotting import parallel_coordinates\nfrom scipy.stats import pearsonr\n\ndata = pd.read_excel('\/kaggle\/input\/learnplatform-analysis-data\/Education (copy).xls')\n\nlatest_2015_19_total_cols = [\n    'Less than a high school diploma, 2015-19',\n    'High school diploma only, 2015-19',\n    \"Some college or associate's degree, 2015-19\",\n    \"Bachelor's degree or higher, 2015-19\",\n]\n\nlatest_data = data.groupby('State')[latest_2015_19_total_cols].aggregate(np.sum).reset_index(drop = False)\n# latest_data['Total'] = latest_data.apply(lambda x: np.sum([x[y] for y in latest_2015_19_total_cols]), axis = 1)\n\n# HS = HS only + C\/A + B = HS only + C\/A + ~0.7 * HS\n# HS = (HS only + C\/A) \/ ~0.3\n# REFERENCE: https:\/\/educationdata.org\/college-enrollment-statistics\n\nlatest_data['has no High School Diploma'] = latest_data['Less than a high school diploma, 2015-19']\nlatest_data['has High School Diploma only'] = latest_data['High school diploma only, 2015-19']\nlatest_data['has College\/Associate Degree'] = latest_data['Some college or associate\\'s degree, 2015-19']\nlatest_data['has Bachelors Degree'] = latest_data['Bachelor\\'s degree or higher, 2015-19']\nlatest_data['has High School Diploma'] = latest_data.apply(lambda x: (x['has High School Diploma only'] + x['has College\/Associate Degree']) \/ 0.3, axis = 1)\n\nuse_cols = ['State', 'has no High School Diploma', 'has High School Diploma', 'has College\/Associate Degree', 'has Bachelors Degree']\nlatest_data = latest_data[use_cols]\nlatest_data['Total People'] = latest_data['has no High School Diploma'] + latest_data['has High School Diploma']\n\nfor col in use_cols[1:]:\n    latest_data[col] = latest_data[col] \/ latest_data['Total People']\n\nlatest_data['Avg Yrs of Study'] = (\n    latest_data['has no High School Diploma'] * 0 +\n    latest_data['has High School Diploma'] * 12 +\n    latest_data['has College\/Associate Degree'] * 2 +\n    latest_data['has Bachelors Degree'] * 4\n)\n\nengagement_data = DATA.groupby('state')[['pct_access', 'engagement_index']].aggregate(np.nanmean).reset_index(drop = False)\nengagement_data['state'] = engagement_data['state'].replace(STATE_ABBR)\n\neducation_engagement_data = pd.merge(engagement_data, latest_data, how = 'inner', left_on = 'state', right_on = 'State')\ncorr, corr_pval = pearsonr(education_engagement_data['engagement_index'], education_engagement_data['Avg Yrs of Study'])\n\nplot_us_map(latest_data, f\"Average Education: Correlation={corr:.2f}, pval={corr_pval:.2f}\", \"Avg Yrs of Study\", \"Avg Yrs\", 10, 12.5, state_col = 'State', theme = 'DARK')","b8afc43d":"<div style=\"background-color: #F8F1FF; padding: 20px 50px;\">\n<span style=\"color:#156BB7;\">\n<font size='+2.5'>\n    <b>4:30 PM:<\/b>\n<\/font>\n<br>\n<br>\n<font size='+1.5'>\n    Finally, it was over. Her battle was won. At least for today. Grace's last class, <b>Foreign Languages<\/b>, got over at 4 PM and now the prisoner was free at last. She could do whatever she wanted, until tomorrow morning of course.\n    <br><br>\n    I was also nearing the completion of my analysis, with just a few finishing touches required. I had managed to analyze student engagement with a lot of different factors at a lot of different levels. To complete my project, I wanted to have one more analysis. And like usual, I went to Grace for inspiration.\n    <br><br>\n    \"How was the Spanish class?\" I asked her, closing her bedroom's door behind me. She had opted to learn Spanish, since some of her friends were from countries like Mexico, Colombia & Spain. She felt she would mingle better with them through Spanish. \"It was nice,\" she replied while playing some game on her mobile phone. \"I think I now know enough to be friends with any person who speaks Spanish.\"\n<\/font>\n<\/span>\n<\/div>","f9b70220":"<div>\n<font size='+1'>\n    As can be seen, unemployment and a few poverty & education related features dominate the third PCA component. Upon analyzing the individual correlations, we realize that actually unemployment was the factor which was highly correlated with student engagement.\n    <br><br>\n    It is also worth noting that the PCA component had more correlation than the individual features, meaning that applying PCA helped in finding the right combination of features.\n<\/font>\n<\/div>","1d24d423":"# Thank You!\n## Analysis by Sakshat Rao","ae9ef1ea":"<div style=\"background-color: #F8F1FF; padding: 20px 50px;\">\n<span style=\"color:#156BB7;\">\n<font size='+2.5'>\n    <b>9 PM:<\/b>\n<\/font>\n<br>\n<br>\n<font size='+1.5'>\n    Little Grace is sleeping in her bedroom now. In a single day, she has inadvertently given so many ideas to think about. She has made me realize that a child's mind works in ways much different than an adult. People like me have spent years to understand what we speak and do. But curious little minds like her make no hesitation in tackling fundamental issues which we arrogant adults miss.\n    <br><br>\n    Which is why it becomes so important to ensure that their innocent curiousness today translates into revolutionary breakthroughs tomorrow. Every organization involved in the Education sector strives to make sure that students do not miss out on the education that would propel them to much greater heights tomorrow. In a world where people are still being divided on several basis, it is important that Education stays united in front of all students.\n    <br><br>\n    <b>This is why Educational Equity is not an option, it is an essential.<\/b>\n<\/font>\n<\/span>\n<\/div>","fc79ed4f":"<div align='center'>\n    <font size='+3' color='#75D345'>\n        <b>A Day of Digital Learning<\/b>\n    <\/font>\n    <br>\n    <font size='+2.5' color='#FBAB60'>\n        <b>Part 4: Finally Done!<\/b>\n    <\/font>\n    <br>\n    <br>\n    <a href=\"https:\/\/www.kaggle.com\/sakshatrao\/a-day-of-digital-learning-part-1-wake-up\">\n        <font size='+1'>\n            &#9202; Wake Up!\n        <\/font>\n    <\/a>\n    &emsp;|&emsp;\n    <a href=\"https:\/\/www.kaggle.com\/sakshatrao\/a-day-of-digital-learning-part-2-morning-mania\">\n        <font size='+1'>\n            &#127748; Morning Mania\n        <\/font>\n    <\/a>\n    &emsp;|&emsp;\n    <a href=\"https:\/\/www.kaggle.com\/sakshatrao\/a-day-of-digital-learning-part-3-sleepy-noons\">\n        <font size='+1'>\n            &#127774; Sleepy Noons\n        <\/font>\n    <\/a>\n    &emsp;|&emsp;\n    <a href=\"https:\/\/www.kaggle.com\/sakshatrao\/a-day-of-digital-learning-part-4-finally-done\">\n        <font size='+1'>\n            <b>&#127769; Finally Done!<\/b>\n        <\/font>\n    <\/a>\n<\/div>","0f9f59c0":"<div>\n<font size='+1'>\n    Upon analyzing the correlations, it was found that the third socioeconomic PCA component had statistically significant correlation with student engagement! The PCA wights have again been plotted for the third PCA component of Socioeconomic indicators to understand why this component is highly correlated.\n<\/font>\n<\/div>","0fa8a2ec":"<div>\n    <font size='+2' color='#0DAB6C'>\n        <b>Detailed Analysis<\/b>\n    <\/font>\n<\/div>","e8761610":"<div>\n<font size='+1'>\n    Now let me go to sleep before my wife realizes I forgot to take the garbage out...\n<\/font>\n<\/div>","00ef2f05":"<div>\n<font size='+1'>\n    The first PCA components were then plotted on the map. The main highlights are -\n    <ul>\n        <li>Northern US seems to be more socio-economically prosperous<\/li>\n        <li>California seems to have much higher minority percentage<\/li>\n        <li>New York seems to have more clustered housing with many not having vehicles<\/li>\n    <\/ul>\n<\/font>\n<\/div>","e978e656":"### Data Sources\n1. Oxford Covid-19 Government Response Tracker (OxCGRT) - https:\/\/www.kaggle.com\/ruchi798\/oxford-covid19-government-response-tracker & https:\/\/www.nature.com\/articles\/s41562-021-01079-8\n2. Educational attainment for the U.S. - https:\/\/www.ers.usda.gov\/webdocs\/DataFiles\/48747\/Education.xls?v=4171.5\n3. Broadband Access - https:\/\/www.imls.gov\/data\/data-catalog\/imls-indicators-workbook-economic-status-and-broadband-availability-and-adoption, https:\/\/github.com\/BroadbandNow\/Open-Data\n4. EdWeek School Reopening - https:\/\/www.edweek.org\/leadership\/map-where-are-schools-closed\/2020\/07\n5. US State Population - https:\/\/www.census.gov\/programs-surveys\/popest\/technical-documentation\/research\/evaluation-estimates\/2020-evaluation-estimates\/2010s-state-total.html\n6. US State Area - https:\/\/www.census.gov\/geographies\/reference-files\/2010\/geo\/state-area.html\n7. US State Polygons - https:\/\/www2.census.gov\/geo\/tiger\/TIGER2020\/STATE\/\n8. US States - Red\/Blue - https:\/\/en.wikipedia.org\/wiki\/2020_United_States_presidential_election#cite_note-FEC-7\n9. COVID-19 US State Policy database - https:\/\/en.wikipedia.org\/wiki\/2020_United_States_presidential_election#cite_note-FEC-7\n10. ACS 1-Year Estimates Data Profiles - https:\/\/data.census.gov\/cedsci\/table?d=ACS 1-Year Estimates Data Profiles&tid=ACSDP1Y2019.DP02&hidePreview=false\n\n\n### References\n1. https:\/\/www.kaggle.com\/dmitryuarov\/eda-covid-19-impact-on-digital-learning\n2. https:\/\/educationdata.org\/college-enrollment-statistics\n3. https:\/\/www.reneshbedre.com\/blog\/anova.html\n4. https:\/\/towardsdatascience.com\/exploratory-spatial-data-analysis-esda-spatial-autocorrelation-7592edcbef9a\n5. https:\/\/www.geeksforgeeks.org\/how-to-plot-andrews-curves-using-pandas-in-python\/\n6. https:\/\/wenr.wes.org\/2018\/06\/education-in-the-united-states-of-america\n7. https:\/\/educationdata.org\/college-enrollment-statistics\n8. https:\/\/www.machinelearningplus.com\/plots\/top-50-matplotlib-visualizations-the-master-plots-python\/\n9. https:\/\/www.similarweb.com\/\n10. https:\/\/www.python-graph-gallery.com\/\n11. https:\/\/github.com\/BroadbandNow\/Open-Data\n12. https:\/\/www.worldbank.org\/en\/topic\/edutech\/brief\/how-countries-are-using-edtech-to-support-remote-learning-during-the-covid-19-pandemic\n13. https:\/\/splot.readthedocs.io\/en\/stable\/users\/tutorials\/autocorrelation.html\n14. https:\/\/www.edweek.org\/","5c75e213":"<div>\n<font size='+1'>\n    To analyze the Social Vulnerability features, I used a dataset which already had categorized its features into 'Socioeconomic Status', 'Household Composition & Disability', 'Minority Status & Language' & 'Housing Type & Transportation'. Again, since there were a lot of features to handle, I used PCA to reduce dimensions till explained variance reached 70%. Shown below are the first PCA components for each category.\n    <br><br>\n    A similar approach can be used to understand what each of these components mean. The first PCA component for Socioeconomic factors increases with Per Capita Income & decreases with poverty rates. Hence it is clear that higher this value, the better a state is in terms of socioeconomic prosperity. The other categories, though not important, can have their first PCA component analyzed in a similar way.\n<\/font>\n<\/div>","937f5539":"<div>\n    <font size='+2' color='#0DAB6C'>\n        <b>References<\/b>\n    <\/font>\n<\/div>","a6ff1c27":"![LearnPlatform_Experimental(9).png](attachment:94011719-3095-45bf-b92d-0bcf2065a410.png)","d802917d":"<div>\n<font size='+1'>\n    Finally, let us now analyze the average education in US states. The idea is that if some states seem to have more education than others, then perhaps students in such states naturally study more.\n    <br><br>\n    To find the average education, we use information about how much population has a high school diploma, college degree or bachelor's degree. Assuming a high school diploma requires 12 years of study, college requires 2 and Bachelor's requires 4, we can then approximate the average number of years of study for a state.\n    <br><br>\n    Finally, when checking for any correlation with Student Engagement, no statistically significant correlation was observed.\n<\/font>\n<\/div>","bbc4e078":"<div>\n<font size='+1'>\n    Learning a new language just to be friends with a person? Now that is admirable, I thought. Grace was anyways the kind of person who cherished her time around friends and was always socially available. No wonder she misses school.\n    <br><br>\n    But that made me realize a gaping hole in my analysis of students. Some part of my analysis was based on EdTech products. Some of it was based on the impact of a pandemic. But have I really based my analysis on the students themselves? On their family condition? On their financial circumstances? On their social life? On their education?\n    <br><br>\n    Now, I don't know any of the students whose data I had been provided with. I don't even know their families or city. Due to de-identification of the data, I don't even know their counties. So the best that I can do is to generalize certain aspects of their life at the state-level. Here is what I found -\n<\/font>\n<\/div>"}}