{"cell_type":{"a676960a":"code","12b99a1a":"code","ce0dc8c6":"code","a001017f":"code","e8e7443d":"code","065c4687":"code","21d42053":"code","e939ed0b":"code","f38d7d64":"code","98f536d7":"code","4e741c2a":"code","cd9cd0b8":"code","6406c683":"code","84bf8a4a":"code","6dc871a6":"code","f069464e":"code","989ebc54":"code","87e65742":"code","778af5a1":"code","c3bccddc":"code","c0bd33d5":"code","fcf1719a":"code","b2eba028":"code","243ce5e3":"code","21791a15":"code","cac499b5":"code","b3cd8de0":"code","8d3ad7da":"code","2f0f1855":"code","915c773c":"code","b671c414":"code","d94b24d4":"code","3c64502a":"code","34fc5194":"code","2a80dfce":"code","22956a38":"code","186e84c5":"code","265a2c8a":"code","3f3e5cf5":"code","45478109":"code","c285ac4c":"code","95269dfd":"code","6cc0fe32":"code","04b89075":"code","c79e8067":"code","95119bd2":"code","38b7ee19":"code","680cc8b0":"code","352b5e39":"code","5342b025":"code","15869eb6":"code","107df14c":"code","696aa634":"code","65361bc1":"code","ba1fd439":"code","922c053a":"code","8ed20fa7":"code","3e6fd013":"code","73be21cc":"code","95fbd8e6":"code","c6301d77":"code","9627df40":"markdown","dfaf31c1":"markdown","fc0b03ab":"markdown","bafd15fe":"markdown"},"source":{"a676960a":" \nimport numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        #print(os.path.join(dirname, filename))\n        if(filename=='train_features.csv'):\n            train_features = pd.read_csv(os.path.join(dirname, filename))\n            print('train_features')\n        if(filename=='test_features.csv'):\n            test_features = pd.read_csv(os.path.join(dirname, filename))\n            print('test_features')    \n        if(filename=='train_targets_scored.csv'):\n            train_targets = pd.read_csv(os.path.join(dirname, filename))\n            print('train_targets')   \n        if(filename=='sample_submission.csv'):\n            sample_submissions = pd.read_csv(os.path.join(dirname, filename))\n            print('sample submissions')       \n            ","12b99a1a":"train_features.info()","ce0dc8c6":"train_targets.info() #159+37=196 MB","a001017f":"train_targets.describe()","e8e7443d":"#print(sample_submissions.head())\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(100,100))\n#print(train_targets)\ntrain_targets. acat_inhibitor.hist(bins=30,alpha=0.5)\ntrain_targets. acetylcholine_receptor_agonist.hist(bins=30,alpha=0.5)\ntrain_targets. adenylyl_cyclase_activator.hist(bins=30,alpha=0.5)\ntrain_targets. trpv_agonist .hist(bins=30,alpha=0.5)\ntrain_targets. trpv_antagonist.hist(bins=30,alpha=0.5)\ntrain_targets. tubulin_inhibitor.hist(bins=30,alpha=0.5)\nplt.show()","065c4687":"train_features","21d42053":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')","e939ed0b":"col=train_features.columns\nreq_train_col = col[1:]\ncol=train_targets.columns\nreq_train_targets_col = col[1:]\nreq_train_num_cols = train_features._get_numeric_data().columns\nreq_train_cat_cols =  (list(set(req_train_col) - set(req_train_num_cols)))\n","f38d7d64":"All_dataset = train_features[req_train_col]\nAll_dataset[req_train_targets_col]  = train_targets[req_train_targets_col] \nAll_dataset","98f536d7":"#training set\ntrain_features[req_train_col]\n","4e741c2a":"#numeric features in the training set\ntrain_features[req_train_num_cols]","cd9cd0b8":"#importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n#Using Pearson Correlation\ntraining_features = train_features[req_train_num_cols[:20]]\ntarget_col = train_targets[req_train_targets_col]\n\n#plt.figure(figsize=(12,10))\n#cor = training_features.corr()\n#sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n#plt.show()","6406c683":"train_targets[req_train_targets_col]","84bf8a4a":"#Categorical features in the trainig set\ntrain_features[req_train_cat_cols]","6dc871a6":"from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder()\nohe.fit(train_features[req_train_cat_cols])\ncat_features = ohe.transform(train_features[req_train_cat_cols])\ncat_features = cat_features.toarray()\ncat_features.shape","f069464e":"cat_features_test = ohe.transform(test_features[req_train_cat_cols])\ncat_features_test = cat_features_test.toarray()\ncat_features_test.shape","989ebc54":"num_features = np.array(train_features[req_train_num_cols])\nnum_features.shape","87e65742":"num_features_test = np.array(test_features[req_train_num_cols])\nnum_features_test.shape","778af5a1":"prepro_training_set =np.hstack((num_features,cat_features))\nX_train = prepro_training_set \nX_train = np.array(prepro_training_set,dtype = 'float')\nX_train.shape","c3bccddc":"prepro_testing_set =np.hstack((num_features_test,cat_features_test))\nX_test = prepro_testing_set \nX_test = np.array(prepro_testing_set,dtype = 'float')\nX_test.shape","c0bd33d5":"Y_train = train_targets[req_train_targets_col].to_numpy()\nY_train = np.array(Y_train,dtype = 'float')\nY_train.shape","fcf1719a":"from skmultilearn.model_selection import iterative_train_test_split\nX_train_part1,Y_train_part1, X_test_part2,Y_test_part2 =  iterative_train_test_split(X_train,Y_train, test_size=0.3)","b2eba028":"X_train_part1.shape","243ce5e3":"from skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.svm import SVC\n\n\nclassifier1 = BinaryRelevance(RandomForestClassifier(random_state=0,n_estimators=30,class_weight=\"balanced_subsample\",n_jobs=-1),require_dense=[False, True])\nclassifier2 = BinaryRelevance(RandomForestClassifier(random_state=0,n_estimators=200,class_weight=\"balanced_subsample\",n_jobs=-1),require_dense=[False, True])\nclassifier3 = BinaryRelevance(BaggingClassifier(LogisticRegression(random_state = 1,n_jobs=-1),n_estimators=5,bootstrap = True,n_jobs=-1),require_dense=[False, True])\nclassifier4 = BinaryRelevance(MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(600,500,400,206), random_state=1),require_dense=[False, True])\n\n#classifier = GridSearchCV(BinaryRelevance(), parameters, scoring='accuracy')\n# train\n#classifier.fit(X_train, Y_train)\n\n# predict\n#predictions = classifier.predict(X_test)","21791a15":"from sklearn import preprocessing\n\nX_NN_scaled_without_Binary_features = preprocessing.scale(X_train_part1[:,:-4])\nX_NN_scaled_with_Binary_features = np.concatenate((X_NN_scaled_without_Binary_features,X_train_part1[:,-4:]),axis=1)\n\nX_NN_Val_scaled_without_Binary_features = preprocessing.scale(X_test_part2[:,:-4])\nX_NN_Val_with_Binary_features = np.concatenate((X_NN_Val_scaled_without_Binary_features,X_test_part2[:,-4:]),axis=1)\n\nX_NN_test_scaled_without_Binary_features = preprocessing.scale(X_test[:,:-4])\nX_NN_test_with_Binary_features = np.concatenate((X_NN_test_scaled_without_Binary_features,X_test[:,-4:]),axis=1)\n\n\n\n   \n\n","cac499b5":"pred4_on_test = np.zeros((3982,),float)\npred4_on_train_Part1 = np.zeros((16669,),float)\npred4_on_test_Part2 = np.zeros((7145,),float)\n","b3cd8de0":"import keras\nimport numpy as np\nfrom keras.models import Sequential\nfrom sklearn.utils import class_weight\nimport random as python_random\nimport tensorflow as tf\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom tensorflow.keras import regularizers\nmodels = []\nfor i in range(0,206,1):\n    np.random.seed(1337)\n    model = Sequential()\n    model.add(Dense(600,input_shape=(877,),activation=\"relu\"))\n    model.add(Dense(500,activation=\"relu\"))\n    model.add(Dense(400,activation=\"relu\"))\n    model.add(Dense(350,activation=\"relu\"))\n    model.add(Dense(300,activation=\"relu\"))\n    model.add(Dense(250,activation=\"relu\"))\n    model.add(Dense(200,activation=\"relu\"))\n    model.add(Dense(150,activation=\"relu\"))\n    model.add(Dense(100,activation=\"relu\"))\n    model.add(Dropout(0.25,seed=0))\n    model.add(Dense(2,activation=\"softmax\"))\n    my_callbacks = [\n    keras.callbacks.EarlyStopping(patience=5)\n    ]\n    class_weights = class_weight.compute_class_weight('balanced',np.unique(Y_train_part1[:,i]),Y_train_part1[:,i])\n    SGD = keras.optimizers.SGD(learning_rate=0.1,decay=0.1\/40,momentum=0.9,nesterov=True)\n    model.compile(loss=\"binary_crossentropy\",optimizer=SGD,metrics=[\"accuracy\"])\n    print(\"trainig model \",format(i))\n    \n    model.fit(X_NN_scaled_with_Binary_features ,Y_train_part1[:,i],callbacks=my_callbacks,class_weight={0:class_weights[0],1:class_weights[1]},validation_split=0.3,epochs=100,batch_size=64,verbose=True)\n    \n    pred4_on_test_star = model.predict(X_NN_test_with_Binary_features,batch_size=64)\n    pred4_on_train_Part1_star = model.predict(X_NN_scaled_with_Binary_features,batch_size=64)\n    pred4_on_test_Part2_star = model.predict(X_NN_Val_with_Binary_features,batch_size=64)\n    \n    pred4_on_test_yet= np.where(pred4_on_test_star[:,1] >= 0.5, 1, 0)\n    pred4_on_train_Part1_yet= np.where(pred4_on_train_Part1_star[:,1] >= 0.5, 1, 0)\n    pred4_on_test_Part2_yet= np.where(pred4_on_test_Part2_star[:,1] >= 0.5, 1, 0)\n    \n    pred4_on_test = np.c_[pred4_on_test, pred4_on_test_yet]\n    pred4_on_test_Part2 = np.c_[pred4_on_test_Part2, pred4_on_test_Part2_yet]\n    pred4_on_train_Part1 = np.c_[pred4_on_train_Part1, pred4_on_train_Part1_yet]\n    \n    if(i==0):\n        pred4_on_test = np.delete(pred4_on_test, 0, 1)\n        pred4_on_test_Part2 = np.delete(pred4_on_test_Part2, 0, 1)\n        pred4_on_train_Part1 = np.delete(pred4_on_train_Part1, 0, 1)\n    \n\n    \n  ","8d3ad7da":"classifier1.fit(X_train_part1, Y_train_part1)","2f0f1855":"X_scaled_for_Logis_without_Binary_features = preprocessing.scale(X_train_part1[:,:-4])\nX_scaled_for_Logis_with_Binary_features = np.concatenate((X_scaled_for_Logis_without_Binary_features,X_train_part1[:,-4:]),axis=1)\n\nX_Val_scaled_for_Logis_without_Binary_features = preprocessing.scale(X_test_part2[:,:-4])\nX_Val_scaled_for_Logis_with_Binary_features = np.concatenate((X_Val_scaled_for_Logis_without_Binary_features,X_test_part2[:,-4:]),axis=1)\n\nX_test_scaled_for_Logis_without_Binary_features = preprocessing.scale(X_test[:,:-4])\nX_test_scaled_for_Logis_with_Binary_features = np.concatenate((X_test_scaled_for_Logis_without_Binary_features,X_test[:,-4:]),axis=1)\n\n\n\nclassifier3.fit(X_scaled_for_Logis_with_Binary_features, Y_train_part1)","915c773c":"X_scaled_for_SVC_with_Binary_features=X_scaled_for_Logis_with_Binary_features\n#classifier4.fit(X_scaled_for_SVC_with_Binary_features, Y_train_part1)","b671c414":"pred1_on_test = classifier1.predict(X_test)\npred1_on_test = pred1_on_test.toarray()","d94b24d4":"pred1_on_train_Part1 = classifier1.predict(X_train_part1)#\u062a\u0642\u064a\u064a\u0645 \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u0627\u0644\u062b\u0627\u0646\u064a\npred1_on_train_Part1 = pred1_on_train_Part1.toarray()","3c64502a":"pred1_on_test_Part2 = classifier1.predict(X_test_part2)#\u062a\u0642\u064a\u0645\u0645 \u0627\u0644\u0646\u0645\u0648\u0630\u062c  \u0627\u0644\u0627\u0648\u0644\npred1_on_test_Part2 = pred1_on_test_Part2.toarray()","34fc5194":"                                              \npred2_on_test = classifier3.predict(X_test_scaled_for_Logis_with_Binary_features)\npred2_on_test = pred2_on_test.toarray()#\u0644\u0644\u0634\u0631\u0643\u0647","2a80dfce":"pred2_on_train_Part1 = classifier3.predict(X_scaled_for_Logis_with_Binary_features)\npred2_on_train_Part1 = pred2_on_train_Part1.toarray() #\u062a\u0642\u064a\u064a\u0645 \u0627\u0644\u0646\u0645\u0648\u0630\u062c \u0627\u0644\u062b\u0627\u0646\u064a\n","22956a38":"\npred2_on_test_Part2 = classifier3.predict(X_Val_scaled_for_Logis_with_Binary_features)#\u062a\u0642\u064a\u0645\u0645 \u0627\u0644\u0646\u0645\u0648\u0630\u062c  \u0627\u0644\u0627\u0648\u0644\npred2_on_test_Part2 = pred2_on_test_Part2.toarray()","186e84c5":"#pred3_on_test = classifier4.predict(X_test)\n#pred3_on_test = pred2_on_test.toarray()#","265a2c8a":"#pred3_on_train_Part1 = classifier4.predict(X_train_part1)#\n#pred3_on_train_Part1 = pred2_on_train_Part1.toarray() #","3f3e5cf5":"#pred3_on_test_Part2 = classifier4.predict(X_test_part2)\n#pred3_on_test_Part2 = pred2_on_test_Part2.toarray()","45478109":"X_train_DT_on_70 = np.concatenate((pred1_on_train_Part1,pred2_on_train_Part1),axis=1)\nX_train_DT_on_70 = np.concatenate((X_train_DT_on_70,pred4_on_train_Part1),axis=1)\n","c285ac4c":"classifier2.fit(X_train_DT_on_70, Y_train_part1)","95269dfd":"test_input_to_DT  = np.concatenate((pred1_on_test,pred2_on_test),axis=1)\ntest_input_to_DT  = np.concatenate((test_input_to_DT,pred4_on_test),axis=1)\nVal_Input_to_DT   = np.concatenate((pred1_on_test_Part2,pred2_on_test_Part2),axis=1)\nVal_Input_to_DT   = np.concatenate((Val_Input_to_DT,pred4_on_test_Part2),axis=1)\ntrain_Input_to_DT = np.concatenate((pred1_on_train_Part1,pred2_on_train_Part1),axis=1)\ntrain_Input_to_DT = np.concatenate((train_Input_to_DT,pred4_on_train_Part1),axis=1)","6cc0fe32":"Val_pred_DT = classifier2.predict(Val_Input_to_DT)\ntrain_pred_DT = classifier2.predict(train_Input_to_DT)","04b89075":"Val_pred_DT = Val_pred_DT.toarray()\ntrain_pred_DT = train_pred_DT.toarray()\n","c79e8067":"pred4_on_test.shape","95119bd2":"Y_test_part2[:,0].shape","38b7ee19":"pred4_on_train_Part1","680cc8b0":"from sklearn.metrics import log_loss\nmetric = log_loss(y_pred=pred4_on_train_Part2,y_true=Y_test_part2)\nmetric","352b5e39":" parameters = [{\n              \"classifier\" : [DecisionTreeClassifier()],\n              \"classifier__max_depth\": [6,8]\n              }]\n\n#classifier3 = GridSearchCV(BinaryRelevance(), parameters, scoring='accuracy', cv = 5,n_jobs=-1)\n\n","5342b025":"#classifier2.get_params().keys()","15869eb6":"#classifier3.fit(X_train_part1, Y_train_part1)\n#print (classifier3.best_params_, classifier3.best_score_)","107df14c":"Final_pred_on_test = classifier2.predict(test_input_to_DT)","696aa634":"#from sklearn.model_selection import KFold\n#from sklearn.metrics import log_loss\n#kf = KFold(n_splits=5, random_state=42, shuffle=True)\n#fold_metrics = []\n#for train_index , Val_index in  kf.split(X_train):\n#    X_train, X_Val = X_train[train_index], X_train[Val_index]\n#    Y_train, Y_Val = Y_train[train_index], Y_train[Val_index]\n#    classifier.fit(X_train, Y_train)\n#    predictions = classifier.predict(X_Val)\n#    predictions = predictions.toarray()\n#    metric = log_loss(y_pred=predictions,y_true=Y_Val)\n#    fold_metrics.append(metric)\n#mean_score = np.mean(fold_metrics)    \n#overall_score_minimizing = np.mean(fold_metrics) + np.std(fold_metrics)\n#overall_score_maximizing = np.mean(fold_metrics) - np.std(fold_metrics)","65361bc1":"pred = Final_pred_on_test.toarray()\npred","ba1fd439":" sample_submissions","922c053a":"features = train_features.columns\ntargets  = train_targets.columns[1:]\npred = np.array(pred,dtype='float')","8ed20fa7":"pred = pd.DataFrame(pred,columns = targets)","3e6fd013":"pred","73be21cc":"pred['sig_id'] = test_features['sig_id']\n\n","95fbd8e6":"submission = pred[train_targets.columns]\nsubmission","c6301d77":"submission.to_csv(\"\/kaggle\/working\/submission.csv\",index = False)","9627df40":"# Training model","dfaf31c1":"# Testing","fc0b03ab":"# EDA\n","bafd15fe":"# submission"}}