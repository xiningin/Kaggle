{"cell_type":{"1d3ee162":"code","63a7a727":"code","3295e7c7":"code","0ca9b0c2":"code","7e7ef24d":"code","bd6c1a5d":"code","d688b766":"code","e2dc3ed1":"code","e196ca93":"code","010afb84":"code","9fcccdde":"code","0188735b":"markdown","b92bdcf2":"markdown","95506e9a":"markdown","9e1d7f78":"markdown","78b76c16":"markdown","a523cbe1":"markdown","04966df6":"markdown","9290c4d3":"markdown","b70bb1af":"markdown","99b289ea":"markdown"},"source":{"1d3ee162":"from IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }<\/style>\"))\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","63a7a727":"#Importing all the neccessary libraries\n%matplotlib inline\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, CountVectorizer\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc, precision_score, recall_score, f1_score\nfrom nltk.stem.porter import PorterStemmer\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom gensim.models import Word2Vec, KeyedVectors\nimport pickle\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn import datasets, neighbors\nfrom sklearn.cross_validation import train_test_split, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import cross_validation\nfrom collections import Counter\nfrom matplotlib.colors import ListedColormap\nimport scikitplot.metrics as sciplot\nfrom sklearn.metrics import accuracy_score\nimport math\nimport nltk\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import TimeSeriesSplit, GridSearchCV","3295e7c7":"'''Loading the Amazon dataset, Remove duplicate data.'''\n#Data Source: https:\/\/www.kaggle.com\/snap\/amazon-fine-food-reviews.\nconnection_sqlobject = sqlite3.connect('..\/input\/database.sqlite') \n\n#Filter only positive and negative reviews. Do nbot consider reviews with score = 3.\n#not taking into consideration those reviews with Score=3\nfiltered_data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3 \"\"\", connection_sqlobject)\n\n#Give reviews with Score > 3 a 'Positive' tag, and reviews with a score < 3 a 'Negative' tag.\nfiltered_data['SentimentPolarity'] = filtered_data['Score'].apply(lambda x : 'Positive' if x > 3 else 'Negative')\nfiltered_data['Class_Labels'] = filtered_data['SentimentPolarity'].apply(lambda x : 1 if x == 'Positive' else 0)\n\n#Display information about the dataset before the removal of duplicate data.\nprint(\"The shape of the filtered matrix : {}\".format(filtered_data.shape))\nprint(\"The median score values : {}\".format(filtered_data['Score'].mean()))\nprint(\"The number of positive and negative reviews before the removal of duplicate data.\")\nprint(filtered_data[\"SentimentPolarity\"].value_counts())\n\n#Removing duplicate entries based on past knowledge.\nfiltered_duplicates=filtered_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\n\n#Removing the entries where HelpfulnessNumerator > HelpfulnessDenominator.\nfinal_data=filtered_duplicates[filtered_duplicates.HelpfulnessNumerator <= filtered_duplicates.HelpfulnessDenominator]\n\n#Display information about the dataset after the removal of duplicate data.\nprint(\"\\nThe shape of the data matrix after deduplication : {}\".format(final_data.shape))\nprint(\"The median score values after deduplication : {}\".format(final_data['Score'].mean()))\nprint(\"The number of positive and negative reviews after the removal of duplicate data.\")\nprint(final_data[\"SentimentPolarity\"].value_counts())\n\n#Checking to see how much % of data still remains.\nprint(\"\\nChecking to see how much percentage of data still remains.\")\nretained_per = (final_data['SentimentPolarity'].size*1.0)\/(filtered_data['SentimentPolarity'].size*1.0)*100\nremoved_per = 100 - retained_per\nprint(\"Percentage of redundant data removed : {}\".format(removed_per))\nprint(\"Percentage of original data retained : {}\".format(retained_per))\n\n#Display the first 5 rows of the final data matrix after de-duplication and intial processing of the original dataset.\nprint(\"\\nFirst 5 rows of the final data matrix after de-duplication and intial processing of the original dataset.\")\nfinal_data.head(5)","0ca9b0c2":"'''Creating a sampled dataset dropping the unwanted columns that we don't need for this problem, from the actual dataset.'''\n#Creating a subset of the \"final_data\" table with randomly selecting 360000 samples\n#sampled_dataset = final_data.sample(n=360000, replace=False, random_state=0).reset_index()\n#print(\"\\nThe shape of the sampled dataset : \", sampled_dataset.shape)\n\n#Dropping unwanted columns for now.\nsampled_dataset=final_data.drop(labels=['Id','ProductId', 'UserId', 'Score', 'ProfileName','HelpfulnessNumerator', 'HelpfulnessDenominator','Summary'], axis=1)\nprint(\"The shape of the sampled dataset after dropping unwanted columns : \", sampled_dataset.shape)\n\n#Sorting data according to Time in ascending order => Time Based Splitting Step 1.\nsampled_dataset=sampled_dataset.sort_values('Time', axis=0, ascending=False, inplace=False, kind='quicksort', na_position='last')\nsampled_dataset = sampled_dataset.reset_index()\nsampled_dataset=sampled_dataset.drop(labels=['index'], axis=1)\n\n#Display distribution of Postive and Negative reviews in a bar graph\nsampled_dataset[\"SentimentPolarity\"].value_counts().plot(kind='bar',color=['green','red'],title='Distribution Of Positive and Negative reviews after De-Duplication.',figsize=(5,5))\nsampled_dataset.head(5)\n\n#Display the first 10 rows of the sampled_dataset (All the reviews are arranged according to time)\nprint(\"\\nFirst 10 rows of the sampled_dataset.\")\nsampled_dataset.head(10)","7e7ef24d":"'''Data Cleaning Stage. Clean each review from the sampled Amazon Dataset.'''\n#Data Cleaning Stage. Clean each review from the sampled Amazon Dataset\n\n#Function to clean html tags from a sentence\ndef removeHtml(sentence): \n    pattern = re.compile('<.*?>')\n    cleaned_text = re.sub(pattern,' ',sentence)\n    return cleaned_text\n\n#Function to keep only words containing letters A-Z and a-z. This will remove all punctuations, special characters etc.\ndef removePunctuations(sentence):\n    cleaned_text  = re.sub('[^a-zA-Z]',' ',sentence)\n    return cleaned_text\n\n#Stemming and stopwords removal\nimport re\nfrom nltk.stem.snowball import SnowballStemmer\nsno = SnowballStemmer(language='english')\n\n#Removing the word 'not' from stopwords\ndefault_stopwords = set(stopwords.words('english'))\nremove_not = set(['not'])\ncustom_stopwords = default_stopwords - remove_not\n\n#Building a data corpus by removing all stopwords except 'not'. Because 'not' can be an important estimator to differentiate between positive and negative reviews.    \ncount=0                   #Iterator to iterate through the list of reviews and check if a given review belongs to the positive or negative class\nstring=' '    \ndata_corpus=[]\nall_positive_words=[] #Store all the relevant words from Positive reviews\nall_negative_words=[] #Store all the relevant words from Negative reviews\nstemed_word=''\nfor review in sampled_dataset['Text'].values:\n    filtered_sentence=[]\n    sentence=removeHtml(review) #Remove HTMl tags\n    for word in sentence.split():\n        for cleaned_words in removePunctuations(word).split():\n            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)): #Checking if a word consists of only alphabets + word length is greater than 2.    \n                if(cleaned_words.lower() not in custom_stopwords):\n                    stemed_word=(sno.stem(cleaned_words.lower()))\n                    filtered_sentence.append(stemed_word)\n                    if (sampled_dataset['SentimentPolarity'].values)[count] == 'Positive': \n                        all_positive_words.append(stemed_word) #List of all the relevant words from Positive reviews\n                    if(sampled_dataset['SentimentPolarity'].values)[count] == 'Negative':\n                        all_negative_words.append(stemed_word) #List of all the relevant words from Negative reviews\n                else:\n                    continue\n            else:\n                continue \n    string = \" \".join(filtered_sentence) #Final string of cleaned words    \n    data_corpus.append(string) #Data corpus contaning cleaned reviews from the whole dataset\n    count+=1\n    \n    \nprint(\"The length of the data corpus is : {}\".format(len(data_corpus)))\n\n#Building a data corpus by removing only the Punctuations and HTML tags. Stopwords are preserved. Words are not stemmed. This is useful for Average Word2Vec, TF-IDF W2V.\nstring=' '    \nnot_stemmed_corpus=[]\nfor review in sampled_dataset['Text'].values:\n    filtered_sentence=[]\n    sentence=removeHtml(review) #Remove HTMl tags\n    for word in sentence.split():\n        for cleaned_words in removePunctuations(word).split():\n            if(cleaned_words.isalpha()): #Checking if a word consists of only alphabets    \n                filtered_sentence.append(cleaned_words.lower())\n            else:\n                continue \n    string = \" \".join(filtered_sentence)    \n    not_stemmed_corpus.append(string) \n    \n#Adding a column of CleanedText to the table final which stores the data_corpus after pre-processing the reviews \nsampled_dataset['CleanedText']=data_corpus \n\n#Adding a column of PreserveStopwords to the table final which stores the data corpus in which stopwords are preserved and words are not stemmed.  \nsampled_dataset['PreserveStopwords']=not_stemmed_corpus \n\nprint(\"Printing the number of positive and negative reviews after data cleaning.\")\nprint(sampled_dataset['SentimentPolarity'].value_counts())\n\n#Store final table into an SQlLite table for future.\nconnection_sqlobject = sqlite3.connect('sampled_dataset_all_reviews.sqlite')\nc=connection_sqlobject.cursor()\nconnection_sqlobject.text_factory = str\nsampled_dataset.to_sql('Reviews', connection_sqlobject, schema=None, if_exists='replace', index=True, index_label=None, chunksize=None, dtype=None)\n\n# Finding most frequently occuring Positive and Negative words \nfreq_positive=nltk.FreqDist(all_positive_words)\nfreq_negative=nltk.FreqDist(all_negative_words)\nprint(\"Most Common Positive Words : \",freq_positive.most_common(20))\nprint(\"Most Common Negative Words : \",freq_negative.most_common(20))\n\n#Display the first 10 rows of the sampled_dataset.\nprint(\"\\nFirst 10 rows of the sampled_dataset.\")\nsampled_dataset.head(10)","bd6c1a5d":"def standardize(X_train_vectors, X_test_vectors):\n    '''Function used to column standardize any given matrix'''\n    from sklearn.preprocessing import StandardScaler\n    scalar = StandardScaler(with_mean=False)\n    scalar.fit(X_train_vectors)\n    X_train_vectors = scalar.transform(X_train_vectors)\n    X_test_vectors = scalar.transform(X_test_vectors)\n    print(\"The shape of the X_train_vectors is : {}\".format(X_train_vectors.shape))\n    print(\"The shape of the X_test_vectors is : {}\".format(X_test_vectors.shape))\n    return (X_train_vectors, X_test_vectors)\n\ndef top_features(nb_classifier, vectorizer_object):\n    '''Get top 50 features displayed from both the negative and the positive review classes.'''\n    neg_class_prob_sorted = (-nb_classifier.feature_log_prob_[0, :]).argsort()               #Note : Putting a - sign indicates the indexes will be sorted in descending order.\n    pos_class_prob_sorted = (-nb_classifier.feature_log_prob_[1, :]).argsort()\n    neg_class_features = np.take(vectorizer_object.get_feature_names(), neg_class_prob_sorted[:50])\n    pos_class_features = np.take(vectorizer_object.get_feature_names(), pos_class_prob_sorted[:50])\n    print(\"The top 50 most frequent words from the positive class are :\\n\")\n    print(pos_class_features)\n    print(\"\\nThe top 50 most frequent words from the negative class are :\\n\")\n    print(neg_class_features)\n    del(neg_class_prob_sorted, pos_class_prob_sorted, neg_class_features, pos_class_features)\n\ndef performance(nb_classifier, vectorizationType, X_train, y_train, X_test, y_test, optimal_alpha, mse): #MSE : Mean Squared Loss\n    '''Function to measure the various performance metrics for a given model.'''\n    print(\"\\n'''PERFORMANCE EVALUATION'''\")\n    print(\"\\n\\nDetailed report for the {} Vectorization.\".format(vectorizationType))\n\n    #Predict the labels for the test set.\n    y_pred = nb_classifier.predict(X_test)\n    \n    #Evaluate the accuracy of the model on test set\n    test_accuracy = accuracy_score(y_test, y_pred, normalize=True) * 100\n    points = accuracy_score(y_test, y_pred, normalize=False)\n    print('\\nThe number of accurate predictions out of {} data points on unseen data is {}'.format(X_test.shape[0], points))\n    print('Accuracy of the {} model on unseen data is {} %'.format(vectorizationType, np.round(test_accuracy,2)))\n    \n    #Get the precision, recall and F1 score for this model.\n    print(\"Precision of the {} model on unseen data is {}\".format(vectorizationType, np.round(metrics.precision_score(y_test ,y_pred),4)))\n    print(\"Recall of the {} model on unseen data is {}\".format(vectorizationType, np.round(metrics.recall_score(y_test ,y_pred),4)))\n    print(\"F1 score of the {} model on unseen data is {}\".format(vectorizationType, np.round(metrics.f1_score(y_test ,y_pred),4)))\n    \n    #Classification Report\n    print ('\\nClasification report for {} model : \\n'.format(vectorizationType))\n    print(metrics.classification_report(y_test,y_pred))\n    \n    #Print the Conclusions on the trained dataset\n    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n    p = tp+fn #p = total number of actual postives\n    n = fp+tn #p = total number of actual negatives\n    TPR = tp\/p ; TNR = tn\/n ; FPR = fp\/n ; FNR = fn\/p\n    print(\"\\nThe True Positives Rate is : {}\".format(TPR))\n    print(\"The True Negatives Rate is : {}\".format(TNR))\n    print(\"The False Positives Rate is : {}\".format(FPR))\n    print(\"The False Negatives Rate is : {}\".format(FNR))\n    \n    #Inference\n    print(\"\\nOf all the reviews that the model has predicted to be positive, {}% of them are actually positive.\".format(np.round(metrics.precision_score(y_test ,y_pred)*100,2)))\n    print(\"Of all the reviews that are actually positive, the model has predicted {}% of them to be positive.\".format(np.round(metrics.recall_score(y_test ,y_pred)*100,2)))\n      \n    #Save the below list for later use to display model information\n    info_model_NB = [vectorizationType, optimal_alpha, np.round(np.array(mse).mean(),4), np.round(1-metrics.accuracy_score(y_test, y_pred),4), np.round(metrics.f1_score(y_test ,y_pred),4), points]\n    with open('info_model_NB.txt', 'a') as filehandle:  \n        filehandle.writelines(\"%s \" % iterator for iterator in info_model_NB)\n        filehandle.writelines(\"\\n\")\n        \n    #Get the confusion matrix for the running model\n    print(\"\\nFind below the confusion matrix for {} model.\".format(vectorizationType))\n    sciplot.plot_confusion_matrix(y_test ,y_pred)\n    \n    #Free memory allocations\n    del(X_train, y_train, X_test, y_test, vectorizationType, y_pred, nb_classifier)\n       \ndef get_GridSearchCV_estimator(vectorizationType, X_train, y_train, X_test, y_test):\n    '''This function will determine the best hyperparameters using TimeSeriesSplit CV and Grid Search, using 10 fold cross validation. '''\n    from sklearn.model_selection import TimeSeriesSplit\n    alphas = np.logspace(-5, 4, 100)\n    tuned_parameters = [{'alpha': alphas}]\n    n_folds = 10\n    cv_timeSeries = TimeSeriesSplit(n_splits=n_folds)\n    model = MultinomialNB()\n    my_cv = TimeSeriesSplit(n_splits=n_folds).split(X_train)\n    gsearch_cv = GridSearchCV(estimator=model, param_grid=tuned_parameters, cv=my_cv, scoring='f1', n_jobs=6)\n    gsearch_cv.fit(X_train, y_train)\n    print(\"\\nGridSearchCV completed for {} model!\".format(vectorizationType))\n    print(\"Best estimator for {} model : \".format(vectorizationType), gsearch_cv.best_estimator_)\n    print(\"Best Score for {} model : \".format(vectorizationType), gsearch_cv.best_score_)\n    return gsearch_cv\n    \ndef plot_errors(gsearch_cv):\n    '''This function is used to plot the curve for mean squared errors vs alpha values'''\n    #Get cross validation scores. Here we obtain the alpha values and their corresponding mean test scores.\n    cv_result = gsearch_cv.cv_results_         \n    mts = cv_result[\"mean_test_score\"]        #list that will hold the mean of cross validation accuracy scores for each alpha\n    alphas = cv_result[\"params\"]\n\n    alpha_values = []                         #list that will hold all the alpha values that the grid search cross validator tried.\n    for i in range(0,len(alphas)):\n        alpha_values.append(alphas[i][\"alpha\"])\n\n    #Changing accuracy to mean squared error. **error = 1 - accuracy ; error = Cross Validation Errors, accuracy = Cross Validation accuracy\n    mse = [1 - x for x in mts]\n\n    #Determining best alpha from errors. 'alpha' will be best for the lowest value for error\n    optimal_alpha = alpha_values[mse.index(min(mse))] #Laplace smoothing\n    print('The optimal value of alpha is : {}'.format(optimal_alpha))     \n\n    #Plot error vs alpha values\n    plt.figure(figsize=(35,8))\n    plt.plot(alpha_values , mse, color='green', linestyle='dashed', linewidth=2, marker='o', markerfacecolor='red', markersize=10)\n    for xy in zip(alpha_values, np.round(mse,3)):\n        plt.annotate('(%s, %s)' % xy, xy=xy, textcoords='data')\n    plt.title('Plot for Errors vs Alpha Values')\n    plt.xlabel('Values of Alpha')\n    plt.ylabel('Errors')\n    plt.show()\n    \n    return (optimal_alpha,mse)\n    \ndef naive_bayes_algorithm(X_train, y_train, X_test, y_test, vectorizationType, vectorizer_object):\n    '''This function splits the dataset into training set and test sets. The test data remains untouched.\n    A time series 10 fold cross validation is performed on the train data and the value of optimal alpha is calculated. \n    The dataset is then trained with this value of optimal alpha. \n    Finally the Naive Bayes model is used to predict its accuracy on the future unseen test set.'''\n    \n    #Perform 10-fold cross validation on the train set\n    print(\"Starting Cross Validation steps...\")\n    gsearch_cv = get_GridSearchCV_estimator(vectorizationType, X_train, y_train, X_test, y_test)\n    \n    #Plot the graphical representation of the mean squared error vs the alpha values obtained during cross validation.\n    optimal_alpha, mse = plot_errors(gsearch_cv)\n\n    #Initialize the Naive Bayes constructor using alpha = optimal_alpha\n    nb_classifier = gsearch_cv.best_estimator_\n\n    #Fit the model to the train set using optimal alpha\n    nb_classifier.fit(X_train, y_train)\n    \n    #Display the top features of both the negative and positive reviews\n    top_features(nb_classifier, vectorizer_object)\n    \n    #Evaluate the model's performance\n    performance(nb_classifier, vectorizationType, X_train, y_train, X_test, y_test, optimal_alpha, mse)","d688b766":"'''Creating the Bag of Words vector for the cleaned reviews.'''\n#Bag of Words model creation using cleaned text \n\n#Using the SQLite Table to read data from a previosuly saved table.\nconnection_sqlobject = sqlite3.connect('sampled_dataset_all_reviews.sqlite') \nsampled_dataset = pd.read_sql_query(\"\"\" SELECT * FROM Reviews \"\"\", connection_sqlobject)\n\n#Split the data set into train and test sets. The test set should be unseen. Time Based Splitting Step 2.\n#The top old 80% data will get into the train set. The latest 20% data will get into the test set.\nX = sampled_dataset['CleanedText']\ny = sampled_dataset['Class_Labels']\nsplit = math.floor(0.8*len(X))\nX_train = X[0:split,] ; y_train = y[0:split,]\nX_test = X[split:,] ; y_test = y[split:,]\n\n#Initializing the BOW constructor\ncv_object = CountVectorizer().fit(X_train)\n\n#Creating the BOW matrix from cleaned data corpus. Only 'not' is preserved from stopwords. This is done for both train and test Vectors.\nprint(\"\\nCreating the BOW vectors using the cleaned corpus\")\nX_train_vectors = cv_object.transform(X_train)\nX_test_vectors = cv_object.transform(X_test)\n\n#Colum Standardization of the Bag of Words vector created using cleaned data. This is done for both train and test Vectors.\nX_train_vectors, X_test_vectors = standardize(X_train_vectors, X_test_vectors)\n\n#Free memory allocations. \ndel(sampled_dataset, X, y, X_train, X_test)\n\n#Fitting the Naive Bayes to the BOW model\nnaive_bayes_algorithm(X_train_vectors, y_train, X_test_vectors, y_test, \"Bag-of-Words\", cv_object)","e2dc3ed1":"'''TF-IDF model creation using text reviews. HTML tags and punctuations are removed. All stopwords are preserved.'''\n\n#Using the SQLite Table to read data from a previosuly saved table.\nconnection_sqlobject = sqlite3.connect('sampled_dataset_all_reviews.sqlite') \nsampled_dataset = pd.read_sql_query(\"\"\" SELECT * FROM Reviews \"\"\", connection_sqlobject)\n\n#Split the data set into train and test sets. The test set should be unseen. Time Based Splitting Step 2.\n#The top old 80% data will get into the train set. The latest 20% data will get into the test set.\nX = sampled_dataset['CleanedText']\ny = sampled_dataset['Class_Labels']\nsplit = math.floor(0.8*len(X))\nX_train = X[0:split,] ; y_train = y[0:split,]\nX_test = X[split:,] ; y_test = y[split:,]\n\n#Initializing the TF-IDF contructor\ntf_idf_object = TfidfVectorizer(ngram_range=(1,1)).fit(X_train)\n\n#Creating the BOW matrix from cleaned data corpus. Only 'not' is preserved from stopwords. This is done for both train and test Vectors.\nprint(\"\\nCreating the BOW vectors using the cleaned corpus\")\nX_train_vectors = tf_idf_object.transform(X_train)\nX_test_vectors = tf_idf_object.transform(X_test)\n\n#Colum Standardization of the TF-IDF vector created using cleaned data. This is done for both train and test Vectors.\nX_train_vectors, X_test_vectors = standardize(X_train_vectors, X_test_vectors)\n\n#Free memory allocations.\ndel(sampled_dataset, X, y)\n\n#Fitting the Naive Bayes to the BOW model\nnaive_bayes_algorithm(X_train_vectors, y_train, X_test_vectors, y_test, \"TF-IDF\", tf_idf_object)","e196ca93":"'''Bi Grams model creation using text reviews. HTML tags and punctuations are removed. All stopwords are preserved.'''\n\n#Using the SQLite Table to read data from a previosuly saved table.\nconnection_sqlobject = sqlite3.connect('sampled_dataset_all_reviews.sqlite') \nsampled_dataset = pd.read_sql_query(\"\"\" SELECT * FROM Reviews \"\"\", connection_sqlobject)\n\n#Split the data set into train and test sets. The test set should be unseen. Time Based Splitting Step 2.\n#The top old 80% data will get into the train set. The latest 20% data will get into the test set.\nX = sampled_dataset['PreserveStopwords']\ny = sampled_dataset['Class_Labels']\nsplit = math.floor(0.8*len(X))\nX_train = X[0:split,] ; y_train = y[0:split,]\nX_test = X[split:,] ; y_test = y[split:,]\n\n#Initializing the TF-IDF contructor\nbigrams_object = TfidfVectorizer(ngram_range=(1,2)).fit(X_train)\n\n#Creating the BOW matrix from cleaned data corpus. Only 'not' is preserved from stopwords. This is done for both train and test Vectors.\nprint(\"\\nCreating the BOW vectors using PreserveStopwords corpus\")\nX_train_vectors = bigrams_object.transform(X_train)\nX_test_vectors = bigrams_object.transform(X_test)\n\n#Colum Standardization of the T-gram vector created using preserved stopwords data. This is done for both train and test Vectors.\nX_train_vectors, X_test_vectors = standardize(X_train_vectors, X_test_vectors)\n\n#Free memory allocations.\ndel(sampled_dataset, X, y)\n\n#Fitting the Naive Bayes to the BOW model\nnaive_bayes_algorithm(X_train_vectors, y_train, X_test_vectors, y_test, \"Bi-Grams\", bigrams_object)","010afb84":"'''Tri-Grams model creation using text reviews. HTML tags and punctuations are removed. All stopwords are preserved.\nThis model won't contain single words. This will mostly try to figure out the relationships between consecutive words in a sentences '''\n\n#Using the SQLite Table to read data from a previosuly saved table.\nconnection_sqlobject = sqlite3.connect('sampled_dataset_all_reviews.sqlite') \nsampled_dataset = pd.read_sql_query(\"\"\" SELECT * FROM Reviews \"\"\", connection_sqlobject)\n\n#Split the data set into train and test sets. The test set should be unseen. Time Based Splitting Step 2.\n#The top old 80% data will get into the train set. The latest 20% data will get into the test set.\nX = sampled_dataset['PreserveStopwords']\ny = sampled_dataset['Class_Labels']\nsplit = math.floor(0.8*len(X))\nX_train = X[0:split,] ; y_train = y[0:split,]\nX_test = X[split:,] ; y_test = y[split:,]\n\n#Initializing the TF-IDF contructor\ntri_grams_object = TfidfVectorizer(ngram_range=(2,3)).fit(X_train)\n\n#Creating the BOW matrix from cleaned data corpus. Only 'not' is preserved from stopwords. This is done for both train and test Vectors.\nprint(\"\\nCreating the BOW vectors using PreserveStopwords corpus\")\nX_train_vectors = tri_grams_object.transform(X_train)\nX_test_vectors = tri_grams_object.transform(X_test)\n\n#Colum Standardization of the tri-gram vector created using preserved stopwords data. This is done for both train and test Vectors.\nX_train_vectors, X_test_vectors = standardize(X_train_vectors, X_test_vectors)\n\n#Free memory allocations.\ndel(sampled_dataset, X, y)\n\n#Fitting the Naive Bayes to the BOW model\nnaive_bayes_algorithm(X_train_vectors, y_train, X_test_vectors, y_test, \"Tri-Grams\", tri_grams_object)","9fcccdde":"'''#Compare performance and display it on a pretty table.\nfrom prettytable import PrettyTable\ntable = PrettyTable()\ntable.field_names = [\"Model\", \" Hyper-Parameter Value (alpha=)\", \"Train Error\", \"Test Error\", \"F1-Score\", \"No. Of accurate predictions\"]\n\nprint(\"Please find below the important metrics for all the models below.\\n\")\nfile = open('info_model_NB.txt', 'r')\nfile.seek(0)\nfor line in file:\n    table.add_row(line.split())\nprint(table)'''","0188735b":"### Naive Bayes on the TF-IDF model created using 'CleanedText' texts. \n\nIn information retrieval, tf\u2013idf or TFIDF, short for term frequency\u2013inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n\nLet's assume we have data corpus D, which contains N reviews {r1,r2,r3,r4...rN}. Let's say our review r1 contains the following words {w1,w2,w3,w1,w9,w6,w7,w9,w9}.\n\nTF or Term Frequency for a word is basically the number of times a word occurs in a review divided by the total number of words present in that same review.\nFor example, in the text corpus that we have considered in the above example, the TF for word w1 is (2\/9) and for word w9 is (1\/3). Intuitively, higher the occurence of a word in a text is, greater will be its TF value. TF values lies between 0 and 1.\n\nIDF or Inverse Document Frequency for a word is given by the formula log(N\/n), where 'N' is equal to the total number of reviews in the corpus 'D' and 'n' refers to the number of reviews in 'D' which contains that specific word. Intuitively, IDF will be higher for words which occur rarely and will be less for words which occurs more frequently. IDF values are more than 0.\n\nSo for each word in each review we will consider the product of (TF x IDF), and represent it in a d dimensional vector. \n\nTF-IDF basically doesn't consider the semantic meaning of words. But what is does is that it gives more importance to words which occurs less frequently in the whole data corpus and also gives much importance to the most frequent words that occurs in each review.","b92bdcf2":"###  Naive Bayes on the Bigrams model created using 'PreserveStopwords':\n\nA bigram or digram is a sequence of two adjacent elements from a string of tokens, which are typically letters, syllables, or words. A bigram is an n-gram for n=2. The frequency distribution of every bigram in a string is commonly used for simple statistical analysis of text in many applications, including in computational linguistics, cryptography, speech recognition, and so on. In a bigram representation of sentences the relationship between two adjacent words are taken into consideration along with single words.","95506e9a":"### Naive Bayes on the Tri-grams model created using 'PreserveStopwords':\n\nTrigrams are a special case of the n-gram, where n is 3. They are often used in natural language processing for performing statistical analysis of texts and in cryptography for control and use of ciphers and codes. For this below code, the relationship between two and three adjacent words are taken into consideration. Single words are omitted.","9e1d7f78":"# Amazon Fine Food Reviews Analysis using Naive Bayes Classifier.\n\nData Source: https:\/\/www.kaggle.com\/snap\/amazon-fine-food-reviews\n\n#### Basic information about the downloaded dataset<br>\n\nNumber of reviews: 568,454<br>\nNumber of users: 256,059<br>\nNumber of products: 74,258<br>\nTimespan: Oct 1999 - Oct 2012<br>\nNumber of Attributes\/Columns in data: 10 \n\n#### Attribute Information:\n\n1. Id\n2. ProductId - unique identifier for the product\n3. UserId - unqiue identifier for the user\n4. ProfileName\n5. HelpfulnessNumerator - number of users who found the review helpful\n6. HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n7. Score - rating between 1 and 5\n8. Time - timestamp for the review\n9. Summary - brief summary of the review\n10. Text - text of the review\n\n\n#### Objective:\nOur main objective for this analysis is to train a model which can seperate the postive and negative reviews.\nIn this problem we will apply classification techniques called Naive Bayes to get an idea if the data can be seperated based on its polarity, i.e. if the review is positive or negative. By looking at the Score column we can make out that the review is positive or not. But we don't need to implement any ML here. A simple if-else condition will make us do this. So for this problem, we will put our focus on to the Review text. The text is the most important feature here if you may ask. Based on the review text we will build a prediction model and determine if a future review is positive or negative.\n\n#### While pre-processing the original dataset we have taken into consideration the following points.\n\n1. We will classify a review to be positive if and only if the corresponding Score for the given review is 4 or 5.\n2. We will classify a review to be negative if and only if the corresponding Score for the given review is 1 or 2.\n3. We will ignore the reviews for the time being which has a Score rating of 3. Because 3 can be thought of as a neutral review. It's neither negative nor positive.\n4. We will remove the duplicate entries from the dataset.\n5. For this problem we will consider a sample size of 50000 reviews sampled randomly from the original dataset. I have done this because I don't have a huge RAM size (12 GB to be specific). \n6. We will train our final mdel using four featurizations -> bag of words model, tf-idf model, average word-to-vec model and tf-idf weighted word-to-vec model.\n7. So at end of the training the model will be trained on the above four featurizations to determine if a given review is positive or negative (Determining the sentiment polarity of the Amazon reviews)","78b76c16":"#### In this code block :\n\n1. I am creating a copy of the final_data dataset called 'sampled_dataset' by dropping the unwanted columns that we don't need for this problem.\n2. Sorting the data according to time, such that the oldest reviews are displayed at the top and the latest reviews are displayed at the bottom.\n3. Displaying information about the number of postive and negative reviews in the sampled dataset, using a Histogram.","a523cbe1":"#### The immediate code block below does the following things :\n\n1. Load the Amazon dataset.\n2. Classify the reviews initially based on their score rating and give them a 'Positve' or a 'Negative' tag.\n3. Remove duplicate\/redundant datas.\n4. Get an idea of how much percentage data were actually duplicates.\n5. Plot a histogram which will display the distribution of the number of positive and negative reviews after de-duplication.\n\n###### NOTE : If we dont' clean the data and feed them to an ML system, it basically means we are throwing in a lot of garbage data to the ML system. If we give it garbage, it will give us garbage back. So it's utmost important to clean the data before proceeding.","04966df6":"### Naive Bayes on the Bag of Words model created using 'CleanedText'.\n\nA bag-of-words model, or BoW for short, is a way of extracting features from text for use in modeling, such as with machine learning algorithms.\nThe approach is very simple and flexible, and can be used in a myriad of ways for extracting features from documents. Suppose we have N reviews in our dataset and we want to convert the words in our reviews to vectors. We can use BOW as a method to do this. What it does is that for each unique word in the data corpus, it creates a dimension. Then it counts how many number of times a word is present in a review. And then this number is placed under that word for a corresponding review. We will get a Sparse Matrix representation for all the worods inthe review.\n\nLet's look at this example of 2 reviews below :\n\nr1 = {\"The food is great, ambience is great\"} and  <\/br>r2 = {\"I love this food\"}\n\nAt first the words will be extracted from r1 and r2.\n\nr1' = {\"The\", \"food\", \"is\", \"great\", \"ambience\", \"is\", \"great\"} and r2' = {\"I\", \"love\", \"this\", \"food\"}\n\nNow using r1' and r2' we will create a vector of unique words -> V =  {\"The\", \"food\", \"is\", \"great\", \"ambience\", \"I\", \"love\", \"this\"}\n\nNow here's how the vector representation will look like for each reviews r1 and r2, when we make use of the vector 'V' created above.\n\nr1_vector = [1,1,2,2,1,0,0,0] and r2_vector = [0,1,0,0,0,1,1,1]\n\nIn r1 since, \"great\" and \"is\" occurs twice, we have set the count to 2. If a words doesn't occur in a review we will set the count to 0. Although \"is\" a stopword, the example above is intended to make you understand how bag of words work.","9290c4d3":"### In this code block : \n\n1. We define a function which is used to perform column standardization on any give input matrix.\n2. We define a function which is used to get the top 50 features from both the negative and the positive review classes.\n3. We define a function which is used to measure the various performance metrics for a given model. We will use accuracy as a metric to evaluate this models performance on unseen data.\n4. We define a function which is used to obtain the optima value of alpha along with the best mnodel estimator, using time series cross validation along with grid search CV.\n5. We define a function which is used to plot and visually represent the errors vs hyperparameter plot.\n6. We fit the naive base classifier to our training data and make the final model.","b70bb1af":"### Conclusion : \n\nFrom the below comparison chart we can see get an idea aboout how the different vectorizers perform.","99b289ea":"#### In this code block :\n\n1. We define two functions which will remove the HTML tags and punctuations from each review.\n2. At the end of this code block, each review will contain texts which will only contain alphabetical strings. \n3. We will apply techniques such as stemming and stopwords removal.\n3. We will create two columns in the sampled dataset - 'CleanedText' and 'RemovedHTML'.\n4. 'CleanedText' column will basically contain the data corpus after stemming the each reviews and removing stopwords from each review. We will use this for our Bag of Word model.\n5. 'RemovedHTML' column will contain the data corpus from which only the HTML tags and punctuations are removed. We will use this column for our TF-IDF model, Average Word2Vec model and TF-IDF weighted average Word2Vec model.\n6. Store the final table in a dataset called 'sampled_dataset' for future use."}}