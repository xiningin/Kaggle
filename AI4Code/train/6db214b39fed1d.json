{"cell_type":{"865d5072":"code","c357ddf0":"code","b9f2f003":"code","ae17935f":"code","3b893602":"code","7e8e3869":"code","0e496db3":"code","2095c6a6":"code","5e94e96b":"code","420c3fb1":"code","71a6b639":"code","31da88a6":"code","fdcc6693":"code","41e6a230":"code","d96c3e79":"code","443e6662":"code","9ee826fa":"code","ed665003":"code","3b6fcba9":"code","61eccdf5":"code","06fde01c":"code","dbff3027":"code","c9a308d5":"code","86acf438":"code","4830d27d":"code","bcd59431":"code","503c9f43":"code","ca6b9ece":"markdown","11b682ff":"markdown","55c09b62":"markdown","15586127":"markdown","34a17995":"markdown","8459df46":"markdown","536fcbd6":"markdown","981be923":"markdown","2e722749":"markdown","3aa5c968":"markdown","6c709a46":"markdown","51876566":"markdown","985808ff":"markdown","14bbfcf8":"markdown","08b84298":"markdown","804d104f":"markdown","afebf510":"markdown","ccf20750":"markdown","e774b14f":"markdown","bf802d19":"markdown","d0621d76":"markdown","9b21ded4":"markdown","be8a2661":"markdown","623de9a9":"markdown","d0176894":"markdown"},"source":{"865d5072":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nimport string\nimport emoji\nimport nltk\nfrom nltk import bigrams\nfrom nltk.tokenize import regexp_tokenize\nimport re\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import CountVectorizer\n\npd.set_option('display.max_rows', 100)\nplt.style.use('seaborn-whitegrid')\n\n#https:\/\/coolors.co\/ef476f-ffd166-06d6a0-118ab2-073b4c\ncolor_disaster = '#EF476F'\ncolor_none = '#118ab2'\ncolor_default = '#06D6A0'\n\n%matplotlib inline","c357ddf0":"df_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv') ","b9f2f003":"df_train.head(20)","ae17935f":"df_train.info()","3b893602":"count = df_train.isnull().sum()\nprecentage = df_train.isnull().mean().round(4) * 100\n\npd.DataFrame({'count': count, 'precentage': precentage}).sort_values('count', ascending=False)","7e8e3869":"sns.countplot(x='target', data=df_train, palette=[color_disaster, color_none])\n\nplt.suptitle('Class Distribution')\nplt.show()","0e496db3":"df_train['keyword'].value_counts()","2095c6a6":"#top 15 keywords\ndisaster_count = df_train[df_train['target']==1]['keyword'].value_counts()\nnone_disaster_count = df_train[df_train['target']==0]['keyword'].value_counts()\n\ndisaster_count = disaster_count[:15]\nnone_disaster_count = none_disaster_count[:15]\n\nplt.figure(figsize=(10, 5))\n\nfig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(20,10))\n\nsns.barplot(x=none_disaster_count.values, y=none_disaster_count.index, ax=ax1, color=color_none)\nax1.set(xlabel='count', ylabel='keyword', title='Non disaster tweets')\n\nsns.barplot(x=disaster_count.values, y=disaster_count.index, ax=ax2, color=color_disaster)\nax2.set(xlabel='count', title='Disaster tweets')\n\nplt.suptitle('Top 15 Keywords')\nplt.show()","5e94e96b":"#top 15 locations\ndisaster_count = df_train[df_train['target']==1]['location'].value_counts()\nnone_disaster_count = df_train[df_train['target']==0]['location'].value_counts()\n\ndisaster_count = disaster_count[:15]\nnone_disaster_count = none_disaster_count[:15]\n\nplt.figure(figsize=(10, 5))\n\nfig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(20,10))\n\nsns.barplot(x=none_disaster_count.values, y=none_disaster_count.index, ax=ax1, color=color_none)\nax1.set(xlabel='count', ylabel='keyword', title='Non disaster tweets')\n\nsns.barplot(x=disaster_count.values, y=disaster_count.index, ax=ax2, color=color_disaster)\nax2.set(xlabel='count', title='Disaster tweets')\n\nplt.suptitle('Top 15 Locations')\nplt.show()","420c3fb1":"#lowercase the tweets\ndf_train['text'] = df_train['text'].apply(lambda tw: tw.lower())","71a6b639":"#number of characters in tweets\nplt.figure(figsize=(10, 5))\n\nbins = [20,40,60,80,100,120,140,160,180]\n\ntext_len_none = df_train[df_train['target']==0]['text'].str.len()\ntext_len_disaster = df_train[df_train['target']==1]['text'].str.len()\n\nplt.hist(text_len_none, label=['Non disaster tweets'], alpha=1, bins=bins, color=color_none, edgecolor='black')\nplt.hist(text_len_disaster, label=['Disaster tweets'], alpha=0.9, bins=bins, color=color_disaster, edgecolor='black')\n\nplt.legend(loc='upper left')\nplt.suptitle('Character Distribution')\nplt.show()","31da88a6":"#word count\nplt.figure(figsize=(10, 5))\n\nbins = [0,5,10,15,20,25,30,35]\n\nword_count_none = df_train[df_train['target']==0]['text'].apply(lambda tw: len(str(tw).split()))\nword_count_disaster = df_train[df_train['target']==1]['text'].apply(lambda tw: len(str(tw).split()))\n\nplt.hist(word_count_none, label=['Non disaster tweets'], alpha=1, bins=bins, color=color_none, edgecolor='black')\nplt.hist(word_count_disaster, label=['Disaster tweets'], alpha=0.9, bins=bins, color=color_disaster, edgecolor='black')\n\nplt.legend(loc='upper right')\nplt.suptitle('Word Distribution')\nplt.show()","fdcc6693":"#unique word\nplt.figure(figsize=(10, 5))\n\nbins = [0,5,10,15,20,25,30,35]\n\nword_count_none = df_train[df_train['target']==0]['text'].apply(lambda tw: len(set(str(tw).split())))\nword_count_disaster = df_train[df_train['target']==1]['text'].apply(lambda tw: len(set(str(tw).split())))\n\nplt.hist(word_count_none, label=['Non disaster tweets'], bins=bins, alpha=1, color=color_none, edgecolor='black')\nplt.hist(word_count_disaster, label=['Disaster tweets'], bins=bins, alpha=0.9, color=color_disaster, edgecolor='black')\n\nplt.legend(loc='upper right')\nplt.suptitle('Unique Word Distribution')\nplt.show()","41e6a230":"def create_corpus(target):\n    corpus = []\n    for x in df_train[df_train['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    \n    return corpus\n\ncorpus_disaster = create_corpus(1)\ncorpus_nondisaster = create_corpus(0)","d96c3e79":"#most common words\nstop_words = stopwords.words('english')\ncommon_words_disaster = []\ncommon_words_nondisaster = []\n\nfor word in corpus_disaster:\n    if word not in stop_words:\n        common_words_disaster.append(word)\n        \nfor word in corpus_nondisaster:\n    if word not in stop_words:\n        common_words_nondisaster.append(word)\n        \ncounter_disaster = Counter(common_words_disaster)\ncounter_nondisaster = Counter(common_words_nondisaster)\n\nx_disaster, y_disaster = map(list, zip(*counter_disaster.most_common()[:20]))\nx_nondisaster, y_nondisaster = map(list, zip(*counter_nondisaster.most_common()[:20]))\n\nfig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(15,10))\n\nsns.barplot(x=y_disaster, y=x_disaster, ax=ax2, color=color_disaster)\nax2.set_title('Disaster tweets')\n\nsns.barplot(x=y_nondisaster, y=x_nondisaster, ax=ax1, color=color_none)\nax1.set_title('Non disaster tweets')\n\nfig.suptitle('Most Common Words')\nplt.show()","443e6662":"#stopword counts\nplt.figure(figsize=(10, 5))\n\nbins = [0,2.5,5,7.5,10,12.5,15,17.5,20]\n\nstopword_count_none = df_train[df_train['target']==0]['text'].apply(lambda tw: len([w for w in str(tw).split() if w in stopwords.words('english')]))\nstopword_count_disaster = df_train[df_train['target']==1]['text'].apply(lambda tw: len([w for w in str(tw).split() if w in stopwords.words('english')]))\n\nplt.hist(stopword_count_none, label=['Non disaster tweets'], bins=bins, alpha=1, color=color_none, edgecolor='black')\nplt.hist(stopword_count_disaster, label=['Disaster tweets'], bins=bins, alpha=0.9, color=color_disaster, edgecolor='black')\n\nplt.legend(loc='upper right')\nplt.suptitle('Stopword Distribution')\nplt.show()","9ee826fa":"#most common stopwords\nstop_words = stopwords.words('english')\nstop_words_disaster = []\nstop_words_nondisaster = []\n\nfor word in corpus_disaster:\n    if word in stop_words:\n        stop_words_disaster.append(word)\n        \nfor word in corpus_nondisaster:\n    if word in stop_words:\n        stop_words_nondisaster.append(word)\n        \ncounter_disaster = Counter(stop_words_disaster)\ncounter_nondisaster = Counter(stop_words_nondisaster)\n\nx_disaster, y_disaster = list(zip(*counter_disaster.most_common()[:15]))\nx_nondisaster, y_nondisaster = list(zip(*counter_nondisaster.most_common()[:15]))\n\nfig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(20,5))\n\nax1.bar(x_nondisaster, y_nondisaster, color=color_none)\nax1.set_title('Non disaster tweets')\n\nax2.bar(x_disaster, y_disaster, color=color_disaster)\nax2.set_title('Disaster tweets')\n\nfig.suptitle('Most Common Stopwords')\nplt.show()","ed665003":"#punctuations\npunctuation_disaster = []\npunctuation_nondisaster = []\n\nfor word in corpus_disaster:\n    if word in string.punctuation:\n        punctuation_disaster.append(word)\n        \nfor word in corpus_nondisaster:\n    if word in string.punctuation:\n        punctuation_nondisaster.append(word)\n        \ncounter_disaster = Counter(punctuation_disaster)\ncounter_nondisaster = Counter(punctuation_nondisaster)\n\nx_disaster, y_disaster = list(zip(*counter_disaster.most_common()[:15]))\nx_nondisaster, y_nondisaster = list(zip(*counter_nondisaster.most_common()[:15]))\n\nfig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(15,5))\n\nax1.bar(x_nondisaster, y_nondisaster, color=color_none)\nax1.set_title('Non disaster tweets')\n\nax2.bar(x_disaster, y_disaster, color=color_disaster)\nax2.set_title('Disaster tweets')\n\nfig.suptitle('Punctuations')\nplt.show()","3b6fcba9":"def find_emoji(string):\n    emoji_pattern = re.compile(\"([\"\n        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        \"\\U0001F600-\\U0001F64F\"  # emoticons\n        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n        \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n        \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n        \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n        \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n        \"\\U00002702-\\U000027B0\"  # Dingbats\n    \"])\") \n\n    return re.findall(emoji_pattern, string)\n\n#https:\/\/gist.github.com\/Alex-Just\/e86110836f3f93fe7932290526529cd1\n\n#find_emoji(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14 as \ud83d\ude0c if \ud83d\udc95\ud83d\udc6d now \ud83d\udc59\")","61eccdf5":"#emojis\nemoji_disaster = []\nemoji_nondisaster = []\n\nfor x in df_train[df_train['target']==1]['text']:\n    emojis = find_emoji(x)\n    if emojis:\n        for e in emojis:\n            emoji_disaster.append(e)\n            \nfor x in df_train[df_train['target']==0]['text']:\n    emojis = find_emoji(x)\n    if emojis:\n        for e in emojis:\n            emoji_nondisaster.append(e)\n            \nprint(\"Emojis count in disaster tweets: {}\".format(len(emoji_disaster)))\nprint(\"Emojis count in non disaster tweets: {}\".format(len(emoji_nondisaster)))","06fde01c":"def find_hashtags(string):\n    return re.findall(r\"#(\\w+)\", string)\n\n#https:\/\/stackoverflow.com\/questions\/2527892\/parsing-a-tweet-to-extract-hashtags-into-an-array\n#https:\/\/stackoverflow.com\/questions\/6331497\/an-elegant-way-to-get-hashtags-out-of-a-string-in-python\n\n#find_hashtags(\"3,000 people receive #wildfires evacuation or #fire\")","dbff3027":"#hashtags \nplt.figure(figsize=(10, 5))\n\nbins = [0,2,4,6,8]\n\nhashtag_count_none = df_train[df_train['target']==0]['text'].apply(lambda tw: len(find_hashtags(tw)))\nhashtag_count = df_train[df_train['target']==1]['text'].apply(lambda tw: len(find_hashtags(tw)))\n\nplt.hist(hashtag_count_none, label='Non disaster tweets', bins=bins, alpha=1, color=color_none, edgecolor='black')\nplt.hist(hashtag_count, label='Disaster tweets', bins=bins, alpha=0.9, color=color_disaster, edgecolor='black')\n\nplt.legend(loc='upper right')\nplt.suptitle('Hashtags Distribution')\nplt.show()","c9a308d5":"#hashtags wordcloud\nhashtag_disaster = []\nhashtag_nondisaster = []\n\nfor x in df_train[df_train['target']==1]['text']:\n    hashtags = find_hashtags(x)\n    if hashtags:\n        for e in hashtags:\n            hashtag_disaster.append(e)\n\nfor x in df_train[df_train['target']==0]['text']:\n    hashtags = find_hashtags(x)\n    if hashtags:\n        for e in hashtags:\n            hashtag_nondisaster.append(e)\n            \nfig, axes = plt.subplots(ncols=2, figsize=(20, 8))\n\ntokens_disaster = ' '.join(hashtag_disaster)\ntokens_nondisaster = ' '.join(hashtag_nondisaster)\n\nwc_disaster = WordCloud(max_words=30, background_color='white', max_font_size=100, colormap='plasma').generate(tokens_disaster)\nwc_nondisaster = WordCloud(max_words=30, background_color='white', max_font_size=100, colormap='plasma').generate(tokens_nondisaster)\n\naxes[0].axis(\"off\")\naxes[0].imshow(wc_nondisaster, interpolation=\"bilinear\")\n\naxes[1].axis(\"off\")\naxes[1].imshow(wc_disaster, interpolation=\"bilinear\")\n\naxes[0].set_title('Non disaster tweets')\naxes[1].set_title('Disaster tweets')\n\nplt.show()","86acf438":"#https:\/\/www.kaggle.com\/bavalpreet26\/nlp-with-disaster-tweets\n#https:\/\/www.analyticsvidhya.com\/blog\/2021\/09\/what-are-n-grams-and-how-to-implement-them-in-python\/\n#https:\/\/practicaldatascience.co.uk\/machine-learning\/how-to-use-count-vectorization-for-n-gram-analysis\n#https:\/\/medium.com\/@cristhianboujon\/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\n\ndef get_n_gram(corpus, r, n):\n    vec = CountVectorizer(ngram_range=r).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n\n    return words_freq[:n]\n\ncorpus = list(df_train['text'].values)\n\nbigrams = get_n_gram(corpus, (2,2), 15)\nbigrams_df = pd.DataFrame(bigrams)\nbigrams_df.columns=['Bigram', 'Frequency']\nbigrams_df","4830d27d":"#top 15 bigrams\nplt.figure(figsize=(10, 5))\n\nsns.barplot(x='Frequency', y='Bigram', data=bigrams_df, color=color_default)\n\nplt.suptitle('Top 15 bigrams')\nplt.show()","bcd59431":"trigrams = get_n_gram(corpus, (3,3), 15)\ntrigrams_df = pd.DataFrame(trigrams)\ntrigrams_df.columns=['Trigram', 'Frequency']\ntrigrams_df","503c9f43":"#top 15 trigrams\nplt.figure(figsize=(10, 5))\n\nsns.barplot(x='Frequency', y='Trigram', data=trigrams_df, color=color_default)\n\nplt.suptitle('Top 15 trigrams')\nplt.show()","ca6b9ece":"### N-grams\n\nN-gram (sometimes also called Q-gram) is a contiguous sequence of n items from a given sample of text or speech. N-grams with N=1 are called unigrams. Similarly, bigrams (N=2), trigrams (N=3) and so on can also be used.\n\n#### Top 15 bigrams","11b682ff":"#### Top 15 trigrams","55c09b62":"### Stopwords\n\nStop words are those words in natural language that do not have significant contextual meaning in a sentence, such as \"is\", \"an\", \"the\", and so on.","15586127":"### Loading the dataset","34a17995":"Let's see the first 20 rows.","8459df46":"What's next?\n\nI hope, now you have some idea about the data. As a next step, you can do the data pre-processing. Once you are satisfied with data pre-processing, you can do the modelling. \n\nAll the best!. ","536fcbd6":"There are more tweets with class 0 than class 1.\n\n### Keyword and Location\n\nBoth keyword and location varaibles are categorical data.","981be923":"According to word distribution plots, we can see that disaster tweets contain low distribution when compared with non disaster tweets.\n\n### Most common words","2e722749":"### Words distribution\n","3aa5c968":"#### Top 15 locations","6c709a46":"Let's plot data on bar chart.","51876566":"### What's in this notebook?\n\nThis notebook tries to provide essential EDA for competition starters.\n\n### Import required libraries","985808ff":"There are five variables available including the target variable. Let's see how many records are available in the dataset.","14bbfcf8":"Let's plot data on bar chart.","08b84298":"### Emojis\n\nEmoji is a small digital image or icon used to express an idea or emotion. Sometimes, they can give strong information about a text such as feeling expression. Therefore, emojis might be a helpful resource in data modeling.\n","804d104f":"We can see that there are duplicate locations are available in the dataset such as USA, United States. ","afebf510":"#### Top 15 keywords","ccf20750":"### Characters distribution","e774b14f":"Emojis are not available in the dataset.\n\n### Hashtags\n\nLet's try to understand hashtags. Hashtags will be helpful to discriminate between disaster and none disaster.","bf802d19":"### Unique words distribution","d0621d76":"There are two varaibles which contain missing values: location and keyword. \n\n### Class distribution\n\nLet's check class distributin. There are only two classes 1(disaster) and 0(none disaster).\n* 1 = Disaster tweets\n* 0 = Non disaster tweets","9b21ded4":"### Punctuations\n\nPunctuations don't add valuable information to the data but it's worth seeing the distribution.","be8a2661":"#### Hashtags wordclud\n\nLet's draw wordcloud for both disaster and none disaster hashtags. For those who are new to wordcloud here is a small brief:\n\n> Word Cloud is a data visualization technique used for representing text data in which the size of each word indicates its frequency or importance. Significant textual data points can be highlighted using a word cloud.","623de9a9":"![0_z9jqZsQ7JSTZGSZz.jpg](attachment:d566a667-4963-4c66-8e71-af67d4bf3fdf.jpg)","d0176894":"There are 7612 records available in the dataset.\n\n### Missing values"}}