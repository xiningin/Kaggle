{"cell_type":{"220d1e45":"code","4f87ccf3":"code","bdd147c7":"code","4a4dddc2":"code","c2d72665":"code","ee9faf76":"code","810e22d4":"code","13ab84dc":"code","84919151":"code","cbc7de2b":"code","c6f11ffc":"code","fcbcb196":"code","9bf9eeac":"code","bfaa46b2":"code","b4ee1fe2":"code","e5ea71ea":"code","aaa39cc4":"code","92604817":"code","63f99dd0":"code","5e5aa763":"code","673cd26e":"code","50e361fd":"code","64771ab0":"code","828814ed":"code","bb42b4d4":"code","09978039":"code","8aeef5b5":"code","dc32c2f9":"code","b3f1a694":"code","0e00231c":"code","fbe2ae2c":"code","ff30bd49":"code","645d7a1f":"code","bf2b226c":"code","be39b8ef":"code","5bab5d28":"code","22bb93de":"code","9a497466":"code","5db08498":"code","ab3750ef":"code","305b11a8":"code","5694f28d":"code","6cba3bc1":"code","0a2b28d9":"code","b77a948f":"code","03a1b042":"code","da572c09":"code","c078d82b":"code","0728eadf":"code","17c9042e":"code","635af611":"code","d4d6644a":"code","fe061cd7":"code","a1ef1bc3":"code","d8c492b7":"markdown","bdf4a07d":"markdown","c12a980e":"markdown","9143f115":"markdown","a5d662c4":"markdown","a9ce912a":"markdown","26e59ca4":"markdown","fe461e9b":"markdown","d1361b23":"markdown","0cd7e4ba":"markdown","36e83b7d":"markdown","f18e72dd":"markdown","00870a10":"markdown","e6be073d":"markdown","8941fc12":"markdown","cc80262c":"markdown","d304318b":"markdown","2b062785":"markdown","8ec682f6":"markdown","efa4cb21":"markdown","c48817c9":"markdown","d33b70d8":"markdown","c233a222":"markdown","21dfab54":"markdown","37db4aed":"markdown","229e24ea":"markdown","74fe0289":"markdown","c7611f30":"markdown","f5455c0d":"markdown","6ca30b46":"markdown","da416502":"markdown","7dc05161":"markdown","f010ab1b":"markdown","2d068253":"markdown","bbbb46db":"markdown","aff2ec89":"markdown"},"source":{"220d1e45":"#Import libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom collections import Counter\n\n%matplotlib inline","4f87ccf3":"# Importing the dataset\ntrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/\/titanic\/test.csv')\ngender_sub = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","bdd147c7":"#Analaysis of data\ntrain_df.head()","4a4dddc2":"#checking for null values\ntrain_df.isnull().sum()","c2d72665":"sns.set_style('whitegrid')\nsns.countplot(x='Survived', hue='Sex',data=train_df)","ee9faf76":"sns.countplot(x='Survived',hue='Pclass',data=train_df,palette='rainbow')","810e22d4":"plt.figure(figsize =(12,6))\nsns.heatmap(train_df.corr(),annot =True)\n","13ab84dc":"sns.countplot(x='SibSp',hue='Survived',data=train_df,palette='rainbow')","84919151":"sns.countplot(x='Parch',hue='Survived',data=train_df,palette='rainbow')","cbc7de2b":"# Explore Age vs Survived\ng = sns.FacetGrid(train_df, col='Survived')\ng = g.map(sns.distplot, \"Age\")","c6f11ffc":"train_df['Fare'].hist(color='green',bins=40,figsize=(8,4))","fcbcb196":"def detect_outliers(df,n,features):\n    outlier_indices = []\n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   ","9bf9eeac":"# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train_df, 2 ,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\n","bfaa46b2":"train_df.loc[Outliers_to_drop] # Show the outliers rows","b4ee1fe2":"# Drop outliers\ntrain_df = train_df.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","e5ea71ea":"combine = pd.concat([train_df,test_df],axis=0)","aaa39cc4":"plt.figure(figsize=(12, 7))\ng = sns.heatmap(train_df[[\"Age\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(),cmap=\"BrBG\",annot=True)","92604817":"combine['Age'] = combine.groupby(\"Pclass\")['Age'].transform(lambda x: x.fillna(x.median()))","63f99dd0":"combine.isnull().sum()","5e5aa763":"combine['Embarked'] = combine['Embarked'].fillna(value=combine.Embarked.mode()[0])","673cd26e":"combine['Fare'] = combine['Fare'].fillna(value=combine['Fare'].median())\ncombine.loc[ combine['Fare'] <= 7.91, 'Fare'] = 0\ncombine.loc[(combine['Fare'] > 7.91) & (combine['Fare'] <= 14.454), 'Fare'] = 1\ncombine.loc[(combine['Fare'] > 14.454) & (combine['Fare'] <= 31), 'Fare']   = 2\ncombine.loc[ combine['Fare'] > 31, 'Fare'] = 3\ncombine['Fare'] = combine['Fare'].astype(int)","50e361fd":"combine['Fare'].unique()","64771ab0":"plt.figure(figsize=(11, 6))\nsns.heatmap(combine.corr(),annot =True)","828814ed":"combine =combine.drop(['SibSp','Parch'],axis=1)","bb42b4d4":"dataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in combine[\"Name\"]]\ncombine['Title'] = pd.Series(dataset_title)\ncombine['Title'] = combine['Title'].replace(['Rev', 'Dr', 'Col', 'Ms', 'Mlle', 'Major', 'the Countess', \n                                     'Capt', 'Dona', 'Jonkheer', 'Lady', 'Sir', 'Mme', 'Don'], 'Other')\ntitle_category = {'Mr':1, 'Miss':2, 'Mrs':3, 'Master':4, 'Other':5}\ncombine['Title'] = combine['Title'].map(title_category)\n","09978039":"combine =combine.drop(['Name','Ticket'],axis=1)","8aeef5b5":"combine['Cabin']=combine['Cabin'].fillna('Missing')\n","dc32c2f9":"combine['Cabin']=combine['Cabin'].str.get(0)\n","b3f1a694":"combine['Cabin'].unique()","0e00231c":"cabin_category = {'A':1,'B':2, 'C':3, 'D':4, 'E':5,'F':6,'G':7,'T':8,'M':9}","fbe2ae2c":"\ncombine['Cabin'] = combine['Cabin'].map(cabin_category)\n","ff30bd49":"combine.info()","645d7a1f":"#converting categorical feature using pd_dummies and also dropping first column in conversion to reduce unnecessary features\nsex = pd.get_dummies(combine['Sex'],drop_first=True)\nembark = pd.get_dummies(combine['Embarked'],drop_first=True)","bf2b226c":"combine.drop(['Sex','Embarked'],axis=1,inplace=True)\n","be39b8ef":"combine = pd.concat([combine,sex,embark],axis=1)","5bab5d28":"combine.head()","22bb93de":"dataset_train = combine[combine['PassengerId']<=891]\ndataset_test = combine[combine['PassengerId']>891]\ndataset_train = dataset_train.drop('PassengerId',axis = 1)\ndataset_test = dataset_test.drop('PassengerId',axis = 1)\n","9a497466":"dataset_test.isnull().sum()","5db08498":"#asssigning values to X and y\nX = dataset_train.drop('Survived',axis = 1)\ny = dataset_train['Survived']\nX_test = dataset_test.drop('Survived',axis=1)","ab3750ef":"y","305b11a8":"\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 0)","5694f28d":"X_train.isnull().sum()","6cba3bc1":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\nclassifier.fit(X_train,y_train)","0a2b28d9":"y_pred =classifier.predict(X_val)\nacc_log = round(classifier.score(X_train, y_train) * 100, 2)\nacc_log","b77a948f":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_val, y_pred)\ncm","03a1b042":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(accuracies.mean())\nprint(accuracies.std())\n","da572c09":"y_test=classifier.predict(X_test)\n","c078d82b":"from sklearn.metrics import classification_report","0728eadf":"print(classification_report(y_val,y_pred))\n","17c9042e":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators=500)\nclassifier.fit(X_train,y_train)","635af611":"y_pred =classifier.predict(X_val)\nacc_log = round(classifier.score(X_train, y_train) * 100, 2)\nacc_log","d4d6644a":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_val, y_pred)\ncm","fe061cd7":"from sklearn.metrics import classification_report\nprint(classification_report(y_val,y_pred))","a1ef1bc3":"y_test=classifier.predict(X_test)\ngender_sub['Survived']=y_test\ngender_sub.dtypes\ngender_sub.Survived=gender_sub.Survived.astype(int)\ngender_sub.head()\ngender_sub.dtypes #to check type of dataframe\ngender_sub.to_csv(\"gender_sub_final1.csv\",index=False)","d8c492b7":"## Combine both test and train data","bdf4a07d":"### Extracting data for cabin","c12a980e":"## Converting Categorical Features","9143f115":"- It seems that very young passengers have more chance to survive","a5d662c4":"## splitting dataset ","a9ce912a":"## Extraction of train and test data from combine","26e59ca4":"## Building a model using Logistic Regression","fe461e9b":"- persons having no siblings or less siblings have more chances of survival","d1361b23":"## Building a model using Random forest model","0cd7e4ba":"## Get the data","36e83b7d":"Let's continue on by visualizing some more of the data by help of plots.In this we analyze train_df (train data) using \nplots and cleaning process will be done on combine data(having both train & test data)","f18e72dd":"Now,next step is to taking care of missing value of 'Embarked'which contain 2 missing values","00870a10":"#  Comparison of Titanic problem solution using logistic regression and Random forest\n\nThe Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we will build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\nIn this code we are analysing titanic probelm using and logistic regression and random forest and compare there perforamance\n\n## Variable Definition Key\n- Survival 0 = No, 1 = Yes\n- pclass Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd\n- Sex:Male\/Female\n- Age: Age in years\n- sibsp : siblings \/ spouses aboard the Titanic\n- parch :parents \/ children aboard the Titanic\n- Ticket :Ticket number\n- Fare: Passenger fare\n- Cabin Cabin number\n- Embarked Port : type of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton","e6be073d":"We detect 10 outliers. The 28, 89 and 342 passenger have an high Ticket Fare\nThe 7 others have very high values of SibSP. \nNext step is to drop outliers","8941fc12":"## comparison between logistic regression and Random forest","cc80262c":"## Exploratory Data Analysis\n\nLet's begin some exploratory data analysis! We'll start by checking out missing data!","d304318b":"Since outliers can have a dramatic effect on the prediction (espacially for regression problems), i choosed to manage them.\noulier is detected by using (IQR) which defines an interquartile range comprised between the 1st and 3rd quartile of the distribution values . An outlier is a row that have a feature value outside the (IQR +- an outlier step).\n\nI decided to detect outliers from the numerical values features (Age, SibSp, Sarch and Fare). Then, i considered outliers as rows that have at least two outlied numerical values.","2b062785":"Great! Our data is ready for our model!","8ec682f6":"|**Model**|Score|cm    |\n|:----|:----|:-----|\n|LR.  |80.02|86 19 | \n|     |     |13 59 |           \n|RF.  |95.74|98 12 | \n|     |     |13 59 | ","efa4cb21":"### extarct title from name and map it ","c48817c9":"- By seeing this count plot we can analyze that people of class 3 have less chances of survival  compare to other 2     classes","d33b70d8":"- By analysizing this heatmap,we can indentify that maximum correlation of survival is with Pclass(taking absolute       value)then Fare seems to have significant correlation","c233a222":"### Predicting a new result ","21dfab54":"- Small families have more chance to survive, more than single (Parch 0), medium (Parch 3,4) and large families \n (Parch 5,6 ).","37db4aed":"By Taking this heamap in account we can identify that sibsp and parch correlatiobn factor with survival is \nvery less so we can drop these","229e24ea":"### k-Fold Cross Validation\n","74fe0289":"Splitting the dataset into the Training set and Test set","c7611f30":"We need to take care of missing values of Fare and also we already analyzed that maximum passengers purchased lower price tickets we assign levels to each of fare category","f5455c0d":"## DATA CLEANING","6ca30b46":"### Missing values in Embarked","da416502":"### Confusion matrix","7dc05161":"combine is combine data of both test and train data .we are taking care of missing values of both dataset","f010ab1b":"### Outlier Detection","2d068253":"Roughly 20 percent of the Age data is missing. The proportion of Age missing is likely small enough for reasonable replacement with some form of imputation. Looking at the Cabin column, it looks like we are just missing too much of that data to do something useful with at a basic level","bbbb46db":"- less price tickets are more purchased one","aff2ec89":"- By seeing this count plot we can analyze that number of female survival is more than no of male surviver so 'SEX' is   an Important feature"}}