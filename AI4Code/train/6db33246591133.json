{"cell_type":{"a865855c":"code","cb5721cd":"code","5f0dcf0c":"code","6b92c429":"code","f34d4d14":"code","2acc62bf":"code","f27f53c4":"code","210ff722":"code","5c1a6d7b":"code","e4df950d":"code","716881ee":"code","51ea9b86":"code","bc1c7994":"code","2f22b53a":"code","c57000b3":"code","ac6395cb":"code","5cc5f356":"code","ca4015c1":"code","f1906191":"code","58b9b924":"code","6003771d":"code","11c81f71":"code","97ccdff6":"code","5b257edb":"code","5475bed8":"code","8cecc707":"code","5d3d3445":"code","8c8c6396":"code","0a9864c2":"code","bc0abddb":"code","4c255ac3":"code","69b73362":"code","46e9d6f1":"code","c7742865":"code","859b82f4":"code","c1b9ab36":"code","e1be6e08":"code","ab62589b":"code","ee229fda":"code","00ea6f9b":"code","8a70e742":"code","b9bbe4c0":"code","dc557d59":"code","9762cdd5":"code","beb14cea":"code","d22c5b25":"code","6d22c50f":"code","534f9fcc":"code","fc572210":"code","d1ac1277":"code","73b45c91":"code","602dbe23":"code","688967c8":"code","0757b225":"code","d318ad7f":"code","b74563f4":"code","0db82844":"code","42b48570":"code","62066f49":"code","a912fc9d":"code","215539bc":"code","c4b4d80d":"code","0e482fe6":"code","e1f832c3":"code","bad25f41":"code","8453a28a":"code","c453e899":"code","56e97c51":"code","5801b871":"code","e0777346":"markdown","bd92dfb7":"markdown","0c2f5d13":"markdown","40c329c1":"markdown","a0bc6c91":"markdown","cbf22e03":"markdown","82d2a3d4":"markdown","962e41c1":"markdown","260327bb":"markdown","10c169eb":"markdown","57ecafbc":"markdown","696c1df1":"markdown","be0f9618":"markdown","daa0c531":"markdown","db7eeac3":"markdown","5450cf01":"markdown","14a5f762":"markdown","bcd4c140":"markdown","09c53e6c":"markdown","c8f73422":"markdown","6c4e20e4":"markdown"},"source":{"a865855c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cb5721cd":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom wordcloud import WordCloud\nimport nltk\nfrom nltk.corpus import stopwords\nfrom textblob import Word, TextBlob\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom nltk.sentiment import SentimentAnalyzer\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom warnings import filterwarnings\n\nfilterwarnings('ignore')\npd.set_option('display.max_columns', None)\npd.set_option('display.float_format', lambda x: '%.2f' % x)\npd.set_option('display.width', 200)","5f0dcf0c":"df = pd.read_csv('..\/input\/df-sub\/df_sub.csv')\ndf.head()","6b92c429":"df.info()","f34d4d14":"df['reviewText'].head(10)","2acc62bf":"# Normalizing Case Folding\n\ndf['reviewText'] = df['reviewText'].str.lower()","f27f53c4":"# Deleting punctuations with regex\n\ndf['reviewText'] = df['reviewText'].str.replace('[^\\w\\s]', '')","210ff722":"# Deleting numbers with regex\n\ndf['reviewText'] = df['reviewText'].str.replace('\\d', '')\n","5c1a6d7b":"# Stopwords\n\nsw = stopwords.words('english')\nprint(sw)","e4df950d":"df['reviewText'] = df['reviewText'].apply(lambda x: \" \".join(x for x in str(x).split() if x not in sw))","716881ee":"# Rarewords\n\ndelete_ = pd.Series(' '.join(df['reviewText']).split()).value_counts()[-1000:]\ndelete_","51ea9b86":"df['reviewText'] = df['reviewText'].apply(lambda x: \" \".join(x for x in x.split() if x not in delete_))","bc1c7994":"# Tokenization\n\ndf[\"reviewText\"].apply(lambda x: TextBlob(x).words).head()\n\n# no assignment, just to see","2f22b53a":"df[\"reviewText\"].head(15)","c57000b3":"# Calculating items' frequency\n\ntf = df[\"reviewText\"].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis=0).reset_index()\ntf.shape","ac6395cb":"# Lemmatization (Finding words with the same root)\n\ndf['reviewText'] = df['reviewText'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\ndf[\"reviewText\"].head(15)","5cc5f356":"# Calculating items' frequency\n\ntf = df[\"reviewText\"].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis=0).reset_index()\ntf.shape","ca4015c1":"# As seen above number of words has decreased ","f1906191":"tf.columns = [\"words\", \"tf\"]\ntf.head()","58b9b924":"tf[tf[\"tf\"] > 500].plot.bar(x=\"words\", y=\"tf\", figsize = (20,7))\nplt.show()","6003771d":"text =  \" \".join(i for i in df.reviewText)","11c81f71":"wordcloud = WordCloud().generate(text)","97ccdff6":"plt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","5b257edb":"# some small arrangments\n\nwordcloud = WordCloud(max_font_size=50,\n                      max_words=100,\n                      background_color=\"white\").generate(text)\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","5475bed8":"# Changing backround for word cloud\nvbo_mask = np.array(Image.open(\"..\/input\/pcturee\/download.png\"))","8cecc707":"wc = WordCloud(background_color=\"white\",\n               max_words=1000,\n               mask=vbo_mask,\n               contour_width=3,\n               contour_color=\"firebrick\")\n\nwc.generate(text)\nplt.figure(figsize=[7, 7])\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","5d3d3445":"df.head()","8c8c6396":"sia = SentimentIntensityAnalyzer()","0a9864c2":"# some examples\n\nsia.polarity_scores(\"The film was awesome\")","bc0abddb":"sia.polarity_scores(\"I liked this music but it is not good as the other one\") ","4c255ac3":"df[\"reviewText\"][0:10].apply(lambda x: sia.polarity_scores(x))","69b73362":"df[\"reviewText\"][0:10].apply(lambda x: sia.polarity_scores(x)['compound'])","46e9d6f1":"df[\"sentiment_label\"] = df[\"reviewText\"].apply(lambda x: \"pos\" if sia.polarity_scores(x)[\"compound\"] > 0 else \"neg\")\ndf[\"sentiment_label\"]","c7742865":"df.head()","859b82f4":"df.groupby('sentiment_label').agg({'overall':'mean'})","c1b9ab36":"from sklearn.feature_extraction.text import CountVectorizer\ncorpus = ['This is the first document.',\n          'This document is the second document.',\n          'And this is the third one.',\n          'Is this the first document?']","e1be6e08":"vectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\nvectorizer.get_feature_names()","ab62589b":"X.toarray()","ee229fda":"vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\nX2 = vectorizer2.fit_transform(corpus)\nvectorizer2.get_feature_names()","00ea6f9b":"X2.toarray()","8a70e742":"# TF-IDF = TF(t) * IDF(t)","b9bbe4c0":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(analyzer='word')\nX = vectorizer.fit_transform(corpus)\nvectorizer.get_feature_names()","dc557d59":"X.toarray()","9762cdd5":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(ngram_range=(2, 3))\nX = vectorizer.fit_transform(corpus)\nvectorizer.get_feature_names()","beb14cea":"X.toarray()","d22c5b25":"df1 = df[['reviewText','sentiment_label']]\ndf1.head()","6d22c50f":"train_x, test_x, train_y, test_y = train_test_split(df[\"reviewText\"],\n                                                    df[\"sentiment_label\"],\n                                                    random_state=17)","534f9fcc":"df.head()","fc572210":"train_x[0:5]","d1ac1277":"train_y[0:5]","73b45c91":"encoder = preprocessing.LabelEncoder()\ntrain_y = encoder.fit_transform(train_y)\ntest_y = encoder.fit_transform(test_y)","602dbe23":"train_y","688967c8":"vectorizer = CountVectorizer()\nvectorizer.fit(train_x)","0757b225":"x_train_count = vectorizer.transform(train_x)","d318ad7f":"x_test_count = vectorizer.transform(test_x)","b74563f4":"vectorizer.get_feature_names()[0:10]","0db82844":"x_train_count.toarray()","42b48570":"tf_idf_word_vectorizer = TfidfVectorizer().fit(train_x)\nx_train_tf_idf_word = tf_idf_word_vectorizer.transform(train_x)\nx_test_tf_idf_word = tf_idf_word_vectorizer.transform(test_x)\ntf_idf_word_vectorizer.get_feature_names()[0:10]","62066f49":"x_train_tf_idf_word.toarray()","a912fc9d":"tf_idf_ngram_vectorizer = TfidfVectorizer(ngram_range=(2, 3)).fit(train_x)\nx_train_tf_idf_ngram = tf_idf_ngram_vectorizer.transform(train_x)\nx_test_tf_idf_ngram = tf_idf_ngram_vectorizer.transform(test_x)","215539bc":"x_train_tf_idf_ngram.toarray()","c4b4d80d":"tf_idf_chars_vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=(2, 3)).fit(train_x)\nx_train_tf_idf_chars = tf_idf_chars_vectorizer.transform(train_x)\nx_test_tf_idf_chars = tf_idf_chars_vectorizer.transform(test_x)","0e482fe6":"x_train_tf_idf_chars.toarray()","e1f832c3":"log_model = LogisticRegression().fit(x_train_tf_idf_word, train_y)\ny_pred = log_model.predict(x_test_tf_idf_word)\nprint(classification_report(y_pred, test_y))","bad25f41":"cross_val_score(log_model, x_test_tf_idf_word, test_y, cv=5).mean()","8453a28a":"rf_model = RandomForestClassifier().fit(x_train_tf_idf_word, train_y)\ncross_val_score(rf_model, x_test_tf_idf_word, test_y, cv=5, n_jobs=-1).mean()","c453e899":"rf_model = RandomForestClassifier().fit(x_train_tf_idf_ngram, train_y)\ncross_val_score(rf_model, x_test_tf_idf_ngram, test_y, cv=5, n_jobs=-1).mean()","56e97c51":"rf_model = RandomForestClassifier().fit(x_train_tf_idf_chars, train_y)\ncross_val_score(rf_model, x_test_tf_idf_chars, test_y, cv=5, n_jobs=-1).mean()","5801b871":"rf_model = RandomForestClassifier().fit(x_train_count, train_y)\ncross_val_score(rf_model, x_test_count, test_y, cv=5).mean()","e0777346":"### TF-IDF N-GRAM","bd92dfb7":"## TF-IDF Word-Level Logistic Regression\n","0c2f5d13":"### Count Vectors","40c329c1":"### TF-IDF Word Level","a0bc6c91":"###  Word tf-idf","cbf22e03":"### TF-IDF CHARLEVEL","82d2a3d4":"##  Count Vectors","962e41c1":"## Wordcloud","260327bb":"### TF-IDF Characters Level","10c169eb":"# MODELING (SENTIMENT MODELING)\n","57ecafbc":"### n-gram tf-idf","696c1df1":"# FEATURE ENGINEERING","be0f9618":"## RandomForestClassifier\n## TF-IDF Word-Level","daa0c531":"# SENTIMENT ANALYSIS","db7eeac3":"## 2.TF-IDF\n\n","5450cf01":"### Word Frequency","14a5f762":"### TF-IDF N-Gram Level\n","bcd4c140":"### n-gram Frequency","09c53e6c":"# TEXT VISUALIZATION","c8f73422":"## Barplot","6c4e20e4":"# Vectorization\n# 1.Count Vectors"}}