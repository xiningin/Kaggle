{"cell_type":{"2ddd2dca":"code","48fe039b":"code","4a94436e":"code","55e70444":"code","2faaa0c1":"code","0ac9f252":"code","5619deb7":"code","2c50552d":"code","b4aaf9b6":"code","eef66a38":"code","5ac06d50":"code","72b42401":"code","d5ddef60":"code","1640e0b4":"code","86c51e93":"code","f8f5561a":"code","f1a3d982":"code","4af6f41b":"markdown","3a13cf45":"markdown","9bfc1b52":"markdown","91e93cfa":"markdown","179c467c":"markdown"},"source":{"2ddd2dca":"!pip install simpletransformers\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom simpletransformers.language_representation import RepresentationModel\n\n#SMOTEs\nfrom imblearn.over_sampling import SMOTE\n\n\n# IMPORTS FOR TRAINING\nfrom sklearn.utils import shuffle\nfrom sklearn.neural_network import MLPClassifier\n\n#RESULTS\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report, confusion_matrix\n","48fe039b":"train_data = pd.read_csv(\"..\/input\/datasets-for-natural-language-processing\/data\/sarcasm\/train.csv\")\ntest_data = pd.read_csv('..\/input\/datasets-for-natural-language-processing\/data\/sarcasm\/test.csv')","4a94436e":"print(train_data.shape)\nprint(test_data.shape)","55e70444":"train = train_data[\"text\"]\ntest = test_data[\"text\"]","2faaa0c1":"# without CUDA=true it takes to remarkable time to vectorize\n# TRAIN SET\n\nsentences = train\nmodel = RepresentationModel(model_type=\"bert\", model_name='bert-base-uncased', use_cuda=True)\nword_vectors_train = model.encode_sentences(sentences, combine_strategy=\"mean\")","0ac9f252":"word_vectors_train.shape","5619deb7":"# TEST SET\n\nsentences = test\nmodel = RepresentationModel(model_type=\"bert\", model_name='bert-base-uncased', use_cuda=True)\nword_vectors_test = model.encode_sentences(sentences, combine_strategy= \"mean\")","2c50552d":"def HybridSMOTE(x, y):\n    smote = SMOTE(sampling_strategy = new_dict, random_state = 42)\n    x, y = smote.fit_resample(x, y)\n\n    return x, y","b4aaf9b6":"def train_model(x_train, y_train, x_test):\n  clf = MLPClassifier(max_iter=10, hidden_layer_sizes = (50,50)).fit(x_train, y_train)\n  y_pred = clf.predict(x_test)\n  return y_pred","eef66a38":"def print_metrics(y_test, y_pred):\n  print('Precision: %.4f' % precision_score(y_test, y_pred, average='weighted'))\n  print('Recall: %.4f' % recall_score(y_test, y_pred, average='weighted'))\n  print('Accuracy: %.4f' % accuracy_score(y_test, y_pred))\n  print('F1 Score: %.4f' % f1_score(y_test, y_pred, average='weighted'))\n  print(classification_report(y_test, y_pred))","5ac06d50":"x_train, y_train, x_test, y_test = word_vectors_train, train_data['Y'], word_vectors_test, test_data['Y']","72b42401":"# CREATING DICTIONARY TO BE ABLE TO AUGMENT NECESSARY CLASSES\n\ndf_c = pd.DataFrame(y_train)\nvalue_counts = df_c.value_counts()\ndictionary = dict()\nfor (i,), j in value_counts.items():\n  dictionary[i] = j\n\ndictionary","d5ddef60":"# ASSIGN NEW DICTIONARY TO ADJUST AMOUNT OF AUGMENTATION\n\naugment_zero = int(dictionary[0] * 2)  ## Augmenting %100\naugment_one = int(dictionary[1] * 2) ## Augmenting %100\n#augment_two = int(dictionary[2] * 2)\n\nnew_dict = {0 : augment_zero, 1: augment_one}\nnew_dict","1640e0b4":"#AUGMENTATION\nx_new, y_new = HybridSMOTE(x_train, y_train)","86c51e93":"# Check the augmented label count\n\ndf_c = pd.DataFrame(y_new)\nvalue_counts = df_c.value_counts()\ndictionary = dict()\nfor (i,), j in value_counts.items():\n  dictionary[i] = j\n\ndictionary","f8f5561a":"from sklearn.utils import shuffle\nx_new, y_new = shuffle(x_new, y_new)\nx_test, y_test = shuffle(x_test, y_test)","f1a3d982":"#Training augmented data\ny_pred = train_model(x_new, y_new, x_test)\nprint_metrics(y_test, y_pred)","4af6f41b":"For more implementation -> [github](https:\/\/github.com\/Toygarr\/synthetic-text-data-augmentation)","3a13cf45":"# Introduction\nWe use this notebook to vectorize the text data and train after augmented by numerical method.\n\nAs mentioned in [simpletransformers](https:\/\/simpletransformers.ai\/docs\/text-rep-model\/):\n> \"The RepresentationModel class is used for generating (contextual) word or sentence embeddings from a list of text sentences, You can then feed these vectors to any model or downstream task.\"\n\nAfter we vectorize the data, it will be used for data augmentation or any other task phases in other notebooks.","9bfc1b52":"# Train the Model\nI used MLPClassifier to get possible highest scores in shortest time. Both train and metrics are functionalized.","91e93cfa":"We vectorize both train and test set seperately and \"bert-base-uncased\" will be using in most of our cases. Sentences will become numerical text in 768 length through BERT.","179c467c":"# Augmentation Method\nSMOTE has package in imblearn library. It basically designed for imbalanced binary data sets to increase the amount of lower label to bigger one. It is working on numerical representations so we will be using our vectorized text data.\n\nIn ctweet data set, we have 3 labels which it is not allowed for default sampling_strategy in SMOTE. To solve the problem, we use dictionary to increase the amount of 2 other lower labels to biggest one like it is a Hybrid version. I named HybridSMOTE of using dictionary to increase both labels as much as we desire rather than augmenting only one lower label."}}