{"cell_type":{"ad41f207":"code","78230bf5":"code","f4da8893":"code","53a781e5":"code","c4541682":"code","5c2a50cf":"code","49582ecc":"code","3414a286":"code","824e1f87":"code","04f0d240":"code","a59164f4":"code","1c5d18f2":"code","cea05a7a":"code","bd644906":"code","bac522f9":"code","bd5a7ffb":"code","c80a4274":"code","01039d57":"code","53b25687":"markdown","5f003ea6":"markdown","a86a4e46":"markdown"},"source":{"ad41f207":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","78230bf5":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","f4da8893":"train.head()\ntrain.columns\ntest.head","53a781e5":"\n[train.shape, test.shape]","c4541682":"# Looking for missing values in the train dataset\ntrain.isna().sum()\ntest.head()","5c2a50cf":"# Finding out the number of people who survived and did not survive using a pie chart\nfrom matplotlib import pyplot as plt\nfig, ax = plt.subplots(figsize=(8,6))\nplt.pie(train.Survived.value_counts(),\n       labels=[\"Survived\",\"Did not Survive\"],\n       shadow = True, \n        explode = (0, 0.1))\nplt.show()","49582ecc":"train_gender=train[['Sex','Survived']]\ntrain_gender = train_gender[train_gender.Survived != 0]\ntrain_gender.head\nplt.subplot(1,2,1)\ntrain['Sex'].value_counts().plot(kind='bar',figsize=(8, 6))\nplt.title(\"No. of passengers who were present on the ship\", y=1.01)\nplt.subplot(1,2,2)\ntrain_gender['Sex'].value_counts().plot(kind='bar',figsize=(12, 6))\nplt.title(\"No. of passengers who survived by the Gender\", y=1.02);","3414a286":"from sklearn.model_selection import train_test_split\n\n# Only taking the columns with numerical values\ntrain_integer= train.select_dtypes(exclude=['object'])\ntest_integer = test.select_dtypes(exclude = ['object'])\ntrain_y = train.Survived\ntrain_integer.drop(['Survived'], axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(train_integer, train_y,\n                                                      train_size=0.8, test_size=0.2,\n                                                      random_state=0)\n","824e1f87":"# Imputing the missing values\nfrom sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer(strategy='median')\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\nimputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns\nimputed_X_train.head\n","04f0d240":"# Building a Random Forest model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nmodel = RandomForestRegressor(n_estimators=10, random_state=0)\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model.fit(X_train, y_train)\n    preds = np.around(model.predict(X_valid),decimals=0)\n    mae= mean_absolute_error(y_valid, preds)\n    score = model.score(X_train,y_train)\n    print(f'Accuracy of the Random Forrest Model: {score*100} and MAE: {mae}')\n    return mae","a59164f4":"y_val = score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid)","1c5d18f2":"train_x = train.drop(['Survived'], axis =1)\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(train_x, train_y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()","cea05a7a":"X_train.head()","bd644906":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='median')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","bac522f9":"model2 = RandomForestRegressor(n_estimators=100, random_state=0)","bd5a7ffb":"# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model2)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(X_valid)\n\n# Evaluate the model\nmae = mean_absolute_error(y_valid, preds)\nprint('MAE:', mae)\nprint('Score:', my_pipeline.score(X_train,y_train))","c80a4274":"# # Fill in the line below: preprocess test data\n# final_X_test = pd.DataFrame(my_imputer.transform(test_integer))\n\n#  get test predictions\npreds_test = np.around(my_pipeline.predict(test))\n","01039d57":"output = pd.DataFrame({'PassengerId':test.PassengerId,\n                       'Survived': preds_test})\nprint(output)\noutput.to_csv('submission.csv', index=False)","53b25687":"***As we can see using imputers we are able to generate more accurate results. The score is much higher than what we obtained when we dropped the object columns. The MAE is also lower.***","5f003ea6":"***Improving on the model by using Pipelines(Makes the whole process smoother) and imputing the missing values in the categorical columns instead of dropping them***:","a86a4e46":"# Testing it on the test data now:"}}