{"cell_type":{"9f6c21eb":"code","c7046b49":"code","bed944d6":"code","a9294846":"code","a3a68ffd":"code","325a76a4":"code","c4516ab5":"code","361317a6":"code","5d162339":"code","ab6f7a49":"code","48ac4e66":"code","8a8cff25":"code","1b15e859":"code","6aa79c99":"code","2e72dba7":"code","9e889ab3":"code","990414e7":"code","f54422e5":"code","34967309":"code","8cc8382b":"code","be27f571":"code","6cd7d282":"code","fefcb027":"code","418e60ed":"code","3bf67c9b":"code","7f8e9502":"markdown","8d42b5aa":"markdown","472d9f41":"markdown","b7b4bb6c":"markdown"},"source":{"9f6c21eb":"%matplotlib inline\nimport numpy as np \nimport pandas as pd \nimport regex as re\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import *\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nfrom sklearn.impute import KNNImputer\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom datetime import timedelta\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c7046b49":"# These helper and data cleaning functions are from the old fast.ai course\n# The repository is here: https:\/\/github.com\/fastai\/fastai\/tree\/master\/old\ndef display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n        display(df)\n        \ndef make_date(df, date_field:str):\n    \"Make sure `df[field_name]` is of the right date type.\"\n    field_dtype = df[date_field].dtype\n    if isinstance(field_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        field_dtype = np.datetime64\n    if not np.issubdtype(field_dtype, np.datetime64):\n        df[date_field] = pd.to_datetime(df[date_field], infer_datetime_format=True)\n        \n\ndef add_datepart(df, fldnames, drop=True, time=False, errors=\"raise\"):\n    # add_datepart converts a column of df from a datetime64 to many columns containing the information from the date. \n    # This applies changes inplace.\n    if isinstance(fldnames,str): \n        fldnames = [fldnames]\n    for fldname in fldnames:\n        fld = df[fldname]\n        fld_dtype = fld.dtype\n        if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n            fld_dtype = np.datetime64\n\n        if not np.issubdtype(fld_dtype, np.datetime64):\n            df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True, errors=errors)\n        targ_pre = re.sub('[Dd]ate$', '', fldname)\n        attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n                'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n        if time: attr = attr + ['Hour', 'Minute', 'Second']\n        for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n        df[targ_pre + 'Elapsed'] = fld.astype(np.int64) \/\/ 10 ** 9\n        if drop: df.drop(fldname, axis=1, inplace=True)\n        \n        \ndef ifnone (a,b): #(a:Any,b:Any)->Any:\n    \"`a` if `a` is not None, otherwise `b`.\"\n    return b if a is None else a\n\n# Function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=10, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)\n\n\ndef train_cats(df):    \n    for n,c in df.items():\n        if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()\n\ndef apply_cats(df, trn):\n    for n,c in df.items():\n        if (n in trn.columns) and (trn[n].dtype.name=='category'):\n            df[n] = c.astype('category').cat.as_ordered()\n            df[n].cat.set_categories(trn[n].cat.categories, ordered=True, inplace=True)\n\ndef numericalize(df, col, name, max_n_cat):\n    if not is_numeric_dtype(col) and ( max_n_cat is None or len(col.cat.categories)>max_n_cat):\n        df[name] = pd.Categorical(col).codes+1\n\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)      \n\n#\n# End fast.ai funcitons...\n#\n\n# This function I believe came from this guy: https:\/\/www.kaggle.com\/siavrez\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    #return df","bed944d6":"df_raw = pd.read_csv('..\/input\/covid19-case-surveillance-public-use-dataset\/COVID-19_Case_Surveillance_Public_Use_Data.csv')\ndf_raw.shape","a9294846":"df_raw.tail()","a3a68ffd":"make_date(df_raw, 'cdc_report_dt')\nmake_date(df_raw, 'pos_spec_dt')\nmake_date(df_raw, 'onset_dt')","325a76a4":"df_processed = df_raw","c4516ab5":"add_datepart(df_processed, 'cdc_report_dt', drop=False)\ndf_processed.head()","361317a6":"cols_with_missing = df_processed.columns[df_processed.isnull().any()].tolist() #Get a list of all columns with null values\n\n#Add a column and field marking that where null values were and that it was missing\nfor col in cols_with_missing: \n    df_processed[col + '_was_missing'] = df_processed[col].isnull()  \ndf_processed.head()","5d162339":"df_processed['pos_spec_dt'][df_processed.pos_spec_dt.notnull()]","ab6f7a49":"# getting the difference in days between CDC report and onset and first positive specimen.\n# making sure to leave missing values missing in the new column\n# The new column will be used for KNN imputation below and therefore to guess at the missing data in the positive specimin and onset columns\ndf_processed['pos_difference'] = (df_processed['cdc_report_dt'] -df_processed['pos_spec_dt'][df_processed.pos_spec_dt.notnull()]).dt.days\ndf_processed['onset_difference'] = (df_processed['cdc_report_dt'] -df_processed['onset_dt'][df_processed.onset_dt.notnull()]).dt.days\ndf_processed.head()","48ac4e66":"cols_for_dummies = ['current_status', 'sex', 'age_group', 'Race and ethnicity (combined)', 'hosp_yn', 'icu_yn', 'death_yn', 'medcond_yn']\ndf_processed = pd.get_dummies(df_processed, columns=cols_for_dummies)\ndf_processed.shape\ndf_processed.head()","8a8cff25":"pd.options.display.max_rows = 100\npd.DataFrame.from_records([(col, df_processed[col].nunique(), df_processed[col].dtype) for col in df_processed.columns],\n                          columns=['Column_Name', 'Num_Unique', 'Dtype']).sort_values(by=['Num_Unique'])","1b15e859":"%who DataFrame","6aa79c99":"del df_raw\nreduce_mem_usage(df_processed)\ngc.collect()","2e72dba7":"df_processed['pos_difference'].hist(bins=100, grid=False, xlabelsize=12, ylabelsize=12)\nplt.xlabel(\"Days between Pos sample & CDC report\", fontsize=15)\nplt.ylabel(\"Frequency\",fontsize=15)\n#plt.xlim([-25,75])\n","9e889ab3":"df_processed['onset_difference'].hist(bins=100)\nplt.xlabel(\"Days between Onset & CDC report\", fontsize=15)\nplt.ylabel(\"Frequency\",fontsize=15)\n#plt.xlim([-25,75])\n","990414e7":"#df_processed['onset_difference'].argmax()\ndf_processed.nlargest(10, ['onset_difference'])","f54422e5":"df_processed.nlargest(10, ['pos_difference'])","34967309":"# getting the difference in days between CDC report and onset and first positive specimen.\n# making sure to leave missing values missing in the new column\n# The new column will be used for KNN imputation below and therefore to guess at the missing data in the positive specimin and onset columns\ndf_processed['onset_pos_difference'] = (df_processed['onset_dt'] -df_processed['pos_spec_dt'][df_processed.pos_spec_dt.notnull()]).dt.days\n\ndf_processed['onset_pos_difference'].hist(bins=100)\nplt.xlabel(\"Days between Onset & Pos sample\", fontsize=15)\nplt.ylabel(\"Frequency\",fontsize=15)\n#plt.xlim([-25,75])","8cc8382b":"df_processed['onset_pos_difference'].mean(), df_processed['onset_pos_difference'].median(), df_processed['pos_difference'].mean(), df_processed['pos_difference'].median(),df_processed['onset_difference'].mean(), df_processed['pos_difference'].median()","be27f571":"df_processed.head()","6cd7d282":"df_processed['onset_dt'].fillna(df_processed.cdc_report_dt + timedelta(days=2), inplace = True)\ndf_processed['pos_spec_dt'].fillna(df_processed.cdc_report_dt + timedelta(days=2), inplace = True)\ndf_processed.tail()","fefcb027":"add_datepart(df_processed, 'pos_spec_dt', drop=False)\nadd_datepart(df_processed, 'onset_dt', drop=False)","418e60ed":"df_processed.head()","3bf67c9b":"df_processed.info","7f8e9502":"# Data cleaning\nThis notebook goes through some pretty typical grunt work of getting the data into a format that's close to useable for ML. You'll have to drop the date columns, but at the end everything is encoded as needed.\n\nThe only bit of trouble I had was deciding how to impute the missing dates for the first positive specimine and onset dates. There are a lot of blanks in those columns. After making a failed attempt at using KNN imputer I decided to go with a simpler route and just take the median days between CDC report and onset and positive sample... I'm not sure if this was a good decision or not, but it's the best guess I could come up with.","8d42b5aa":"### To deal with the outliers let's take the median instead of the mean to calculate the dates for the missing values... As per below it's 2 days.","472d9f41":"### Most of the filled in dates seems to be normal\/possion, that is around zero. But there are lots of outliers...","b7b4bb6c":"<img src =\"https:\/\/snappygoat.com\/b\/e4dcb09d666964fb88c7349ba21417f5952a287c\" width = 250 align = right>\n\n**To impute missing values from the date columns, I was trying to use KNN, but I couldn't get it to work, the process just kept spinning and wouldn't finish. I can get it to work with 1000 or so records, but I then tried to do 400k, I left it alone for a few hours and it still wasn't done. Next code blocks I do some EDA to figure out a simple imputation method that makes sense.**\n\nIf someone wants to mess around with KNN, here's the code I tried. I was just guessing at features and hyperparamaters to put in.\n```\nimputer = KNNImputer(n_neighbors=3)\nimputer.fit_transform(\n    df_processed[['cdc_report_dtWeek',\n                  'cdc_report_dtDayofweek', \n                  'pos_difference','onset_difference',\n                  'Race and ethnicity (combined)_White, Non-Hispanic', \n                  'hosp_yn_Yes'\n    ]])\n```"}}