{"cell_type":{"bd52be39":"code","c965340f":"code","b1361652":"code","d9b60ac9":"code","8da9d44a":"code","22a6ce9c":"code","487c3992":"code","4d94ab46":"code","48bdcf50":"code","74fbc283":"code","71c2115e":"code","6a31610f":"code","df474a16":"code","f2828cfd":"code","fb4430cc":"code","114099f9":"markdown","8d702e66":"markdown","5b85cd2b":"markdown","9de1ae8e":"markdown","703ba709":"markdown","38e5ffcc":"markdown","7dcd110b":"markdown","67f92fca":"markdown","53cae33d":"markdown","083bd64c":"markdown"},"source":{"bd52be39":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c965340f":"import os\n\nfrom glob import glob\nfrom tqdm import tqdm as print_progress\nfrom datetime import datetime, timedelta, date\nimport datetime\nimport dateutil\n\nimport math\nimport numpy as np\nimport pandas as pd\n\nimport time\nimport warnings\nimport random\n\nfrom joblib import Parallel, delayed\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom itertools import combinations\nfrom joblib import Parallel, delayed\nfrom collections import Counter\nimport scipy","b1361652":"import featuretools as ft\nfrom featuretools.variable_types import *\nfrom featuretools.utils.gen_utils import make_tqdm_iterator\n\nfrom featuretools.primitives.base import *\nfrom featuretools.utils.gen_utils import Library\nfrom featuretools.primitives.standard.binary_transform import (\n    AddNumeric, AddNumericScalar,\n    SubtractNumeric, SubtractNumericScalar, ScalarSubtractNumericFeature,\n    MultiplyNumeric, MultiplyNumericScalar,\n    DivideNumeric, DivideNumericScalar, DivideByFeature,\n    ModuloNumeric, ModuloNumericScalar, ModuloByFeature,\n    GreaterThan, GreaterThanScalar, GreaterThanEqualTo, GreaterThanEqualToScalar,\n    LessThan, LessThanScalar, LessThanEqualTo, LessThanEqualToScalar, \n    Equal, EqualScalar, NotEqual, NotEqualScalar\n)\n\nfrom featuretools.variable_types.variable import *\nfrom featuretools.variable_types import Id, Numeric, Categorical, Datetime","d9b60ac9":"from sklearn.utils import class_weight\nfrom sklearn.cluster import KMeans, MeanShift, DBSCAN\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\nfrom sklearn.impute import SimpleImputer as sk_imputer\n\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix, \n    silhouette_score, homogeneity_score, completeness_score, v_measure_score,\n    auc, plot_roc_curve\n  )\n\nfrom sklearn.model_selection import (\n    KFold, StratifiedKFold, cross_val_score, \n    GridSearchCV, \n    train_test_split\n  )\n\nfrom xgboost import XGBClassifier\n\nfrom sklearn.base import BaseEstimator\nimport sklearn.linear_model as models\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted","8da9d44a":"def to_datetime(text: str):\n    try:\n        dt_format = str(parse_to_datetime(text))\n        dt_object = datetime.strptime(dt_format,'%Y-%m-%d %H:%M:%S')\n    except Exception:\n        return None\n    return dt_objec\n\ndef month_to_quarter(month: int) -> int:\n    if 1 <= month <= 3:\n        return 1\n    elif 4 <= month <= 6:\n        return 2\n    elif 7 <= month <= 9:\n        return 3\n    elif 10 <= month <= 12:\n        return 4\n    else:\n        raise ValueError(f'input must be between 1 and 12')\n\ndef load_csv(filepath: str) -> pd.DataFrame:\n  if not os.path.isfile(filepath):\n      raise FileNotFoundError(f\"Cannot find {filepath}\")\n  return pd.read_csv(filepath)\n\nLUCKY_NUMBER=4444\ndef find_best_k_clustering(x: np.array, \n                           max_clusters: int=10,\n                           max_iterations: int=1000, \n                           n_samples: int=169, \n                           lucky_number: int=LUCKY_NUMBER,\n                           verbose: bool=False):\n    scores = {}\n    for k in range(2, max_clusters-1):\n        if k < 2:\n            continue\n        try:\n            kmeans = KMeans(n_clusters=k, max_iter=max_iterations, random_state=lucky_number).fit(x)\n        except:\n            continue\n        # scores[k] = kmeans.inertia_\n        try:\n            scores[k] = silhouette_score(x, kmeans.labels_, metric='euclidean', sample_size=n_samples)\n        except ValueError:\n            continue\n\n    best_k = max(scores, key=scores.get)\n    if verbose:\n        print(f\"Best k is {best_k}\")\n    return best_k\n\ndef reorder_cluster(cluster_field_name: str, \n                    target_field_name: str, \n                    df: pd.DataFrame, \n                    ascending: bool=True):\n    new_cluster_field_name = 'new_' + cluster_field_name\n    df_new = df.groupby(cluster_field_name)[target_field_name].mean().reset_index()\n    df_new = df_new.sort_values(by=target_field_name, ascending=ascending).reset_index(drop=True)\n    df_new['index'] = df_new.index\n    df_final = pd.merge(df, df_new[[cluster_field_name, 'index']], on=cluster_field_name)\n    df_final = df_final.drop([cluster_field_name], axis=1)\n    df_final = df_final.rename(columns={\"index\": cluster_field_name})\n    df_final[cluster_field_name] = df_final[cluster_field_name] + 1\n    return df_final\n\ndef cdf(x):\n    \"\"\"\n    Cumulative Density Function (with epsilon)\n    \"\"\"\n    x = np.sort(x)\n    u, c = np.unique(x, return_counts=True)\n    n = len(x)\n    y = (np.cumsum(c)-0.5) \/ n\n    \n    def interpolate_(x_):\n        y_interp = np.interp(x_, u, y, left=0.0, right=1.0)\n        return y_interp\n    \n    return interpolate_\n\ndef cumulative_kl(x, y, fraction: float=0.5): \n    \"\"\"\n    Cumulative Method to calculate Kullback\u2013Leibler divergence\n    \"\"\"\n    dx = np.diff(np.sort(np.unique(x)))\n    dy = np.diff(np.sort(np.unique(y)))\n    ex = np.min(dx)\n    ey = np.min(dy)\n    e = np.min([ex, ey]) * fraction\n    n = len(x)\n    P = cdf(x)\n    Q = cdf(y)\n    \n    divergence = (1.\/n) * np.sum(np.log((P(x)-P(x-e)) \/ (Q(x)-Q(x-e)+1e-11)))\n    return np.abs(divergence)\n\ndef preprocess_for_classifier(df: pd.DataFrame, target_name: str,\n                              id_cols: list=[], train_size: float=0.69):\n    # Split to train and validation\n    dset, X, Y = dict(), dict(), dict()\n\n    # If the minimum number of groups for any class less than 2\n    try:\n        dset['train'], dset['test'] = train_test_split(df, train_size=train_size, stratify=df[target_name])\n        for ds_name, ds in dset.items():\n            Y[ds_name] = ds[target_name]    \n            X[ds_name] = ds.copy()\n            X[ds_name].drop(columns=id_cols+[target_name], errors='ignore', inplace=True)\n    except:\n        X['train'] = df.copy()\n        X['train'].drop(columns=id_cols+[target_name], errors='ignore', inplace=True)\n        X['test'] = X['train'].copy()\n\n        Y['train'] = df[target_name]\n        Y['test'] = Y['train'].copy()\n    \n    # Compute class weights for target\n    target_weights = Y['train']\n    target_classes = target_weights.unique()\n    class_weights = list(\n        class_weight.compute_class_weight('balanced', target_classes, target_weights)\n    )\n    target_weights = target_weights.map({clss_i+1: clss_w for clss_i, clss_w in enumerate(class_weights)})\n \n    return X, Y, target_weights\n\ndef visualize_results(classifier, X, Y):\n    results = classifier.evals_result()\n\n    epochs = len(results['validation_0']['mlogloss'])\n    x_axis = range(0, epochs)\n\n    viz_df = pd.DataFrame(classifier.feature_importances_, \n                          index=X['train'].columns, \n                          columns=['feature_importance'])\n    viz_df.sort_values(by=['feature_importance'], inplace=True)\n    # viz_df[viz_df.feature_importance>0.011].plot(kind='barh', alpha=0.75)\n\n    print('\\n\\nAccuracy of XGB classifier on training: {:.2f}'\n                .format(classifier.score(X['train'], Y['train'])))\n    y_pred = classifier.predict(X['train'])\n    print(classification_report(Y['train'], y_pred))\n\n    print('\\n\\nAccuracy of XGB classifier on testing: {:.2f}'\n                .format(classifier.score(X['test'], Y['test'])))\n    y_pred = classifier.predict(X['test'])\n    print(classification_report(Y['test'], y_pred))\n\ndef filter_opposite_features(df: pd.DataFrame, verbose: bool=False):\n    features_before = list(df.columns)\n    features_after = deepcopy(features_before)\n    feature_id = 0\n    while feature_id < len(features_after):\n        feature_1 = features_after[feature_id]\n        if verbose:\n            print(f\"Checking {feature_1}\")\n        feature_id += 1\n\n        is_separable = False\n        for op in [' = ', ' - ', ' + ', ' > ', ' < ']:\n            if op in feature_1:\n                is_separable = True\n                break\n\n        if not is_separable:\n            continue\n\n        obj_a, obj_b = feature_1.split(op)\n        feature_2 = obj_b + op + obj_a\n        if feature_2 in features_after:\n            if verbose:\n                print(f\"Remove {feature_2} because of oppositing {feature_1}\")\n            features_after.remove(feature_2)\n\n    features_removed = list(set(features_before).difference(set(features_after)))\n    if verbose:\n        print(f\"\\n\\n\\nFeatures removed:\\n\\t\", features_removed)\n    return features_removed\n\ndef feature_clustering(df: pd.DataFrame, \n                       feature_name: str,\n                       cluster_name: str=None,\n                       ascending: bool=True, \n                       reorder_by_feature: str=None,\n                       best_k: int=None) -> pd.DataFrame:\n    # Preprocess\n    cluster_df = df[\n        ['GuestID', feature_name] if not reorder_by_feature else \\\n        ['GuestID', feature_name, reorder_by_feature]\n    ]\n    x = cluster_df[feature_name].values.reshape(-1, 1)\n    if not cluster_name:\n        cluster_name = feature_name + '_Cluster'\n\n    # Segmentation\n    if not best_k:\n        best_k = find_best_k_clustering(x)\n    model = KMeans(n_clusters=best_k, random_state=LUCKY_NUMBER)\n    model.fit(x)\n\n    # Predict\n    cluster_df[cluster_name] = model.predict(x)\n\n    # Sorting\n    if not reorder_by_feature:\n        reorder_by_feature = feature_name\n    cluster_df = reorder_cluster(cluster_name, reorder_by_feature, cluster_df, ascending)\n    df = df.merge(cluster_df[['GuestID', cluster_name]], how='outer', on='GuestID')\n\n    return df, best_k, model\n\n","22a6ce9c":"class Age(Variable):\n    _default_pandas_dtype = int\n    _name = 'Age'\n    \n    \nclass Price(Variable):\n    _default_pandas_dtype = float\n    _name = 'Price'\n\n\nclass Weight(Variable):\n    _default_pandas_dtype = float\n    _name = 'Weight'\n\n\nclass Size(Variable):\n    _default_pandas_dtype = str\n    _name = 'Size'\n    \n    \nclass MeasurementUnit(Variable):\n    _default_pandas_dtype = str\n    _name = 'MeasurementUnit'\n\n\nclass Currency(Variable):\n    _default_pandas_dtype = str\n    _name = 'Currency'\n\nclass People(Variable):\n    _default_pandas_dtype = int\n    _name = 'People'\n\nCUSTOM_VARIABLES = [\n    Currency, MeasurementUnit, Size, Weight, Price, Age, People\n  ]\n\nclass TimeDelta(TransformPrimitive):\n    \n    name = \"scalar_subtract_numeric_feature\"\n    input_types = [Datetime, Datetime]\n    return_type = Numeric\n    compatibility = [Library.PANDAS, Library.DASK, Library.KOALAS]\n\n    def __init__(self):\n        self.description_template = \"the result of {} minus {}\"\n\n    def get_function(self):\n        def time_delta_between(x_vals, y_vals):\n            return (x_vals - y_vals) \/ pd.Timedelta(days=1)\n        return time_delta_between\n\n    def generate_name(self, base_feature_names):\n        return \"%s - %s\" % (base_feature_names[0], base_feature_names[1])\n\nclass Quarter(TransformPrimitive):\n    \"\"\"\n    Determines the quarter value of a datetime.\n    \"\"\"\n    name = \"quarter\"\n    input_types = [Datetime]\n    return_type = Ordinal\n    compatibility = [Library.PANDAS, Library.DASK, Library.KOALAS]\n    description_template = \"the quarter of {}\"\n\n    def get_function(self):\n        def quarterize(vals):\n            m_vals = vals.dt.month\n            q_vals = m_vals.copy()\n            q_vals.loc[m_vals <= 3] = 1 # 1st quarter of the year\n            q_vals.loc[m_vals > 3] = 2 \n            q_vals.loc[m_vals > 6] = 3 \n            q_vals.loc[m_vals > 9] = 4 \n            return q_vals\n        return quarterize\n\nclass FeatureSelector(BaseEstimator):\n\n    def __init__(self, problem_type: int=\"regression\", \n                       n_runs: int=5,\n                       corr_threshold: float=0.9,\n                       retain_features: list=[],\n                       n_workers: int=1,\n                       verbose: bool=True):\n        \"\"\"\n        Multi-step cross-validated feature selection\n        \n        Inputs:\n            - problem_type: str, either \"regression\" or \"classification\" (default: \"regression\")\n            - n_runs: number of times to perform feature selection with a random fraction of data points (int; default: 5)\n            - corr_threshold: threshold to deliminate correlated features\n            - retain_features: list of features that must be retained\n            - n_jobs: how many jobs to run when selecting the features in parallel (int; default: 1)\n            - n_workers: the number of workers in parallel\n            - verbose: verbosity level (boolean; default: False)\n        \n        Attributes:\n            - important_features_: list of important features (to select via pandas.DataFrame columns)\n            - original_features_: list of original features of X when calling fit\n            - return_df_: whether to return a pandas.DataFrame; if False, return a numpy.array\n        \"\"\"\n        self.retain_features = retain_features\n        self.corr_threshold = corr_threshold\n        self.problem_type = problem_type\n        self.n_workers = n_workers\n        self.verbose = verbose\n        self.n_runs = n_runs\n\n    def fit(self, X: pd.DataFrame or np.array, \n                  y: pd.Series or np.array):\n        # Verify data types\n        if isinstance(X, pd.DataFrame):\n            self.return_df_ = True\n            self.original_features_ = list(X.columns)\n        else:\n            self.return_df_ = False\n            self.original_features_ = [f\"x_{i}\" for i in range(X.shape[1])]\n            X = pd.DataFrame(X, columns=self.original_features_)\n        if not isinstance(y, pd.Series):\n            y = pd.Series(y, name='target')\n        \n        # Perform multi-step feature selection\n        self.important_features_ = feature_screening(\n            X=X, y=y, \n            n_runs=self.n_runs, \n            verbose=self.verbose, \n            n_workers=self.n_workers, \n            problem_type=self.problem_type,\n            corr_threshold=self.corr_threshold, \n            retain_features=self.retain_features\n        )\n        return self                      \n\n    def transform(self, X: pd.DataFrame or np.array):\n        # Validate attributes\n        check_is_fitted(self, [\"important_features_\"])\n        if len(self.important_features_) == 0:\n            if self.verbose:\n                print(\"WARNING: No important features found; returning data unchanged.\")\n            return X\n\n        # Validate data types\n        if isinstance(X, pd.DataFrame):\n            features = list(X.columns)\n        else:\n            features = [f\"x_{i}\" for i in range(X.shape[1])]\n        X = check_array(X, force_all_finite=\"allow-nan\")\n        if sorted(self.original_features_) != sorted(features):\n            raise ValueError(\"Features are different from calling `fit`\")\n\n        # Get selected features only\n        X = pd.DataFrame(X, columns=features)\n        X_selected = X[self.important_features_]\n        if self.return_df_:\n            return X_selected\n        return X_selected.values\n\n    def fit_transform(self, X: pd.DataFrame or np.array, \n                            y: pd.Series or np.array):\n        self.fit(X, y)\n        X_transformed = self.transform(X)\n        return X_transformed\n\n# Define functions to duplicate\nnumeric_functions = [\n    AddNumeric(),\n    SubtractNumeric(),\n    # MultiplyNumeric(),\n    # DivideNumeric(), DivideByFeature(value=1),\n    # ModuloNumeric(), ModuloByFeature(value=1),\n    GreaterThan(), GreaterThanScalar(value=0), \n    LessThan(), LessThanScalar(value=0), \n    Equal(), EqualScalar(value=0), \n  ]\n\n\n# Duplicate Numeric functions for customized Variables\ndef duplicate_functions(functions: list or tuple):\n    duplicated_functions = []\n    duplicated_variables = [Age, Price, Weight]\n    for dup_func in functions:\n        for var_type in duplicated_variables:\n            # print(f\"\\nDuplicating function {dup_func.name} for variable {var_type._name}\")\n\n            # Define nearly-duplicated primitive\n            dup_func.name = dup_func.name.replace('numeric', var_type._name.lower())\n            input_types = dup_func.input_types\n            if isinstance(input_types[0], (list, tuple)):\n                n_inputs = len(input_types[0])\n                dup_func.compatibility = [Library.PANDAS]\n                dup_func.input_types = [[var_type] * n_inputs]\n            else:\n                n_inputs = len(input_types)\n                dup_func.input_types = [var_type] * n_inputs\n            \n            # Append to list\n            duplicated_functions.append(dup_func)\n    return duplicated_functions\n\n\n# Define default primitives\ndefault_aggregation_primitives = [\n    \"sum\", \"max\", \"min\", \"mode\", \n    \"mean\", \"std\", \"skew\",\n    \"count\", \"percent_true\", \"num_unique\",\n  ]\n\ndefault_transformation_primitives = [\n    # Datetime\n    \"month\", Quarter, \"weekday\", \"is_weekend\", TimeDelta,\n\n    # LatLong\n    \"haversine\", \n\n    # NaturalLanguage\n    # \"num_words\", \"num_characters\", \n\n    # Age\n    # GroupOf\n  ] + numeric_functions + duplicate_functions(numeric_functions)\n\n#fill na\nclass DataImputer:\n    \n    def __init__(df: pd.DataFrame, \n                 feature_types: dict, \n                 imputation_methods: dict={\n                     'categorical': ['constant', 'most_frequent', 'random_prob', 'knn', 'mice', 'em', 'deep_learning'],\n                     'numerical': ['mode', 'mean', 'median', 'random_prob', 'knn', 'mice', 'em'],\n                     'datetime': ['current_date', '1st_date']\n                 }):\n        self.df = df\n        self.feature_types = {\n            feature: feature_type.lower() for feature, feature_type in feature_types.items()\n        }\n        self.imputation_methods = imputation_methods\n        self.imputed_features = dict()\n        \n    def fill_na(self):\n        for feature_name in self.df.columns:\n            print(f'\\nImputing {feature_name} ...')\n            if feature_name not in self.feature_types.keys():\n                print(f'\\tType of {feature_name} is not declared!')\n                continue\n            elif self.feature_types[feature_name] not in self.imputation_methods.keys():\n                print(f'\\tType {self.feature_types[feature_name]} is not supported!')\n                continue\n            feature = self.df[feature_name]\n            \n    def fill_na_by_constant(self, feature, const_type: str):\n        \"\"\"\n        Constant value could be 1 among [0, mean, median, mode]\n        Pros:\n            . easy and fast\n        Cons:\n            . not factor the correlations between features \n            . give poor results on encoded categorical features (with MEAN, MEDIAN or MODE)\n            . not very accurate\n            . not account for the uncertainty in the imputations\n        \"\"\"\n        pass\n\n    def fill_na_by_most_frequent(self, feature):\n        \"\"\"\n        Pros:\n            . works well with categorical features\n        Cons:\n            . not factor the correlations between features \n            . can introduce bias in the data\n        \"\"\"\n        pass\n\n    def fill_na_by_random_prob(self, feature):\n        pass\n\n    def fill_na_by_kNN(self, feature):\n        \"\"\"\n        This function creates a basic MEAN impute, then uses the resulting list to construct a KDTree. \n        Then, it uses the resulting KDTree to compute nearest neighbours (NN). \n        After it finds the k-NNs, it takes the weighted average of them.\n        Pros:\n            . much more accurate than the mean, median or most frequent imputation methods\n        Cons:\n            . computationally expensive\n            . quite sensitive to outliers\n        \"\"\"\n        pass\n\n    def fill_na_by_MICE(self, feature):\n        \"\"\"\n        Multivariate Imputation by Chained Equations\n        \"\"\"\n        pass\n\n    def fill_na_by_EM(self, feature):\n        pass\n\n    def fill_na_by_deep_learning(self, feature):\n        \"\"\"\n        Pros:\n            . quite accurate compared to other methods\n            . works very well with categorical and non-numerical features\n        Cons:\n            . single-column imputation\n            . quite slow with large datasets\n        \"\"\"\n        pass\n\n    def fill_na_by_date(self, feature, date_type: str):\n        pass\n\n    ","487c3992":"DEFAULT_TOP_N = 20\ndef auto_feature_engineering(entity_set: ft.EntitySet,\n                             table_name: str,\n                             max_depth: int=2,\n                             max_features: int=-1,\n                             seed_features: list=None,\n                             encoding_categorical: bool=True,\n                             verbose: bool=False) -> pd.DataFrame:\n\n    # Deep Feature Synthesis \n    #   (Categorical features are not processed, on default)\n    feature_matrix, feature_definitions = ft.dfs(\n        entityset=entity_set,\n        target_entity=table_name,\n        agg_primitives=default_aggregation_primitives,\n        trans_primitives=default_transformation_primitives,\n        max_depth=max_depth,\n        max_features=max_features,\n        verbose=verbose,\n        return_variable_types=[Numeric, Discrete, Ordinal, Categorical, Boolean]+CUSTOM_VARIABLES\n    )\n\n    # 1-hot encoding for Categorical features, including generated ones\n    if encoding_categorical:\n        features_before = set(feature_matrix.columns.values.tolist())\n        feature_matrix, feature_definitions = encode_categorical_features(feature_matrix, feature_definitions, include_unknown=False, verbose=verbose)\n        if verbose:\n            features_after = set(feature_matrix.columns.values.tolist())\n            features_encoded = list(features_before.difference(features_after))\n            print(\"\\n\\nFeatures being encoded:\\n\\t\", \"\\n\\t\".join(features_encoded))\n\n    return feature_matrix.astype(float), feature_definitions\n\n\ndef encode_categorical_features(feature_matrix: pd.DataFrame, features, \n                                top_n=DEFAULT_TOP_N, include_unknown=True,\n                                to_encode=None, inplace=False, \n                                drop_first=False, verbose=False):\n    \"\"\"\n    Encode categorical features\n        Args:\n            feature_matrix (pd.DataFrame): Dataframe of features.\n            features (list[PrimitiveBase]): Feature definitions in feature_matrix.\n            top_n (int or dict[string -> int]): Number of top values to include.\n                If dict[string -> int] is used, key is feature name and value is\n                the number of top values to include for that feature.\n                If a feature's name is not in dictionary, a default value of 10 is used.\n            include_unknown (pd.DataFrame): Add feature encoding an unknown class.\n                defaults to True\n            to_encode (list[str]): List of feature names to encode.\n                features not in this list are unencoded in the output matrix\n                defaults to encode all necessary features.\n            inplace (bool): Encode feature_matrix in place. Defaults to False.\n            drop_first (bool): Whether to get k-1 dummies out of k categorical\n                    levels by removing the first level.\n                    defaults to False\n            verbose (str): Print progress info.\n        Returns:\n            (pd.Dataframe, list) : encoded feature_matrix, encoded features\n    \"\"\"\n    if not isinstance(feature_matrix, pd.DataFrame):\n        raise TypeError(\"feature_matrix must be a Pandas DataFrame\")\n\n    X = feature_matrix if inplace else feature_matrix.copy()\n\n    old_feature_names = set()\n    for feature in features:\n        for fname in feature.get_feature_names():\n            assert fname in X.columns, (f\"Feature {fname} not found in feature matrix\")\n            old_feature_names.add(fname)\n\n    pass_through = [col for col in X.columns if col not in old_feature_names]\n\n    iterator = make_tqdm_iterator(iterable=features,\n                                  total=len(features),\n                                  desc=\"Encoding pass 1\",\n                                  unit=\"feature\") if verbose else features\n    new_feature_list = []\n    new_columns = []\n    encoded_columns = set()\n\n    for f in iterator:\n        # TODO: features with multiple columns are not encoded by this method,\n        # which can cause an \"encoded\" matrix with non-numeric vlaues\n        is_categorical = issubclass(f.variable_type, Categorical)\n        if (f.number_output_features > 1 or not is_categorical):\n            if f.number_output_features > 1:\n                print(f\"[WARNING] Feature {f} has multiple columns and will not be encoded. This may result in a matrix with non-numeric values.\")\n            new_feature_list.append(f)\n            new_columns.extend(f.get_feature_names())\n            continue\n\n        if to_encode is not None and f.get_name() not in to_encode:\n            new_feature_list.append(f)\n            new_columns.extend(f.get_feature_names())\n            continue\n\n        val_counts = X[f.get_name()].value_counts().to_frame()\n        index_name = val_counts.index.name\n        if index_name is None:\n            if 'index' in val_counts.columns:\n                index_name = 'level_0'\n            else:\n                index_name = 'index'\n        val_counts.reset_index(inplace=True)\n        val_counts = val_counts.sort_values([f.get_name(), index_name],\n                                            ascending=False)\n        val_counts.set_index(index_name, inplace=True)\n        select_n = top_n\n        if isinstance(top_n, dict):\n            select_n = top_n.get(f.get_name(), DEFAULT_TOP_N)\n        if drop_first:\n            select_n = min(len(val_counts), top_n)\n            select_n = max(select_n - 1, 1)\n        unique = val_counts.head(select_n).index.tolist()\n        for label in unique:\n            add = f == label\n            add_name = add.get_name()\n            new_feature_list.append(add)\n            new_columns.append(add_name)\n            encoded_columns.add(add_name)\n            X[add_name] = (X[f.get_name()] == label)\n\n        if include_unknown:\n            unknown = f.isin(unique).NOT().rename(f.get_name() + \" is unknown\")\n            unknown_name = unknown.get_name()\n            new_feature_list.append(unknown)\n            new_columns.append(unknown_name)\n            encoded_columns.add(unknown_name)\n            X[unknown_name] = (~X[f.get_name()].isin(unique))\n\n        X.drop(f.get_name(), axis=1, inplace=True)\n\n    new_columns.extend(pass_through)\n    new_X = X[new_columns]\n    iterator = new_X.columns\n    if verbose:\n        iterator = make_tqdm_iterator(iterable=new_X.columns,\n                                      total=len(new_X.columns),\n                                      desc=\"Encoding pass 2\",\n                                      unit=\"feature\")\n    for c in iterator:\n        if c in encoded_columns:\n            try:\n                new_X[c] = pd.to_numeric(new_X[c], errors='raise')\n            except (TypeError, ValueError):\n                pass\n\n    return new_X, new_feature_list\n\ndef filter_out_minor_categoricals(df: pd.DataFrame, \n                                  column_names: list or tuple,\n                                  top_k: int=None,\n                                  min_percent: float=0.01, \n                                  verbose: bool=False):\n    for col in column_names:\n        if verbose:\n            print(f\"\\n\\nFiltering {col} ...\")\n        if col not in df.columns:\n            if verbose:\n                print(f\"\\tCannot find {col} in your dataframe!\")\n            continue\n            \n        # Check top values\n        df[col] = df[col].astype(str).str.lower()\n        all_values = df[col].value_counts(normalize=True)\n        if top_k:\n            top_values = list(all_values.index)[:top_k]\n        else:\n            top_values = list(all_values.loc[all_values>=min_percent].index)\n            \n        if verbose:\n            print(f\"\\tAll values\")\n            print(all_values)\n            print(f\"\\tTop values:\", top_values)\n            \n        # Filter-out minority\n        def filter_minority(val):\n            if not val or val not in top_values:\n                val = 'others'\n            return val\n        \n        df[col] = df[col].apply(filter_minority)\n        if verbose:\n            print(f\"\\tFilter values\")\n            print(df[col].value_counts(normalize=True))\n            \n    return df","4d94ab46":"START_DATE = pd.to_datetime('2000-01-01', format='%Y-%m-%d')\nEND_DATE = pd.to_datetime(datetime.date(2021, 1, 1),format='%Y-%m-%d') \n# R_value = InactiveDays = Datetime(1\/1\/2021) - guest_df.DepartureDate.max()\n# that's why we use 1\/1\/2021 is end date","48bdcf50":"def guest_type_detector(folder_path: str,folder_out_path:str):\n  # Load data\n  DF = dict()\n  DF['guest_ltv'] = pd.read_csv(str(folder_path) + \"bookings.csv\")\n\n  # Preprocess\n  time_cols = [col for col in DF['guest_ltv'].columns if 'date' in col.lower()]\n  DF['guest_ltv'][time_cols] = DF['guest_ltv'][time_cols].apply(pd.to_datetime, errors='ignore')\n  \n  # --create one matrix for copy and use  DF['guest_ltv'] because we don't want to have a problem on  DF['guest_ltv']\n  visitors_df = DF['guest_ltv'].copy()\n\n  # --Count the value of ArrivalDate on guest id. so we can know who is the fist and return\n  #How can I determine who are returning \/ first-time customers?\n  #--> Guest Type Detection: Using bookings list, you can group-by GuestID and then, \n  #count number of ArrivalDate to determine type of a guest: first-time (only 1 arrival date), or returning (many arrival dates).\n  visits_counter = visitors_df[['GuestID', 'ArrivalDate']].groupby(by=['GuestID']).agg({'ArrivalDate': 'nunique'})\n  visits_counter.reset_index(inplace=True)\n  visits_counter.rename(columns={'ArrivalDate': 'NumberOfArrivals'}, inplace=True) #rename ready for merge \n  # --this time we have values of NumberOfArrivals, we merge to the database for detect\n  DF['guest_ltv'] = DF['guest_ltv'].merge(visits_counter, how='outer', on='GuestID', sort=False)\n\n  #ready for RFM \n  rooms_counter = visitors_df[['GuestID', 'ArrivalDate']].groupby(by=['GuestID', 'ArrivalDate']).agg({'ArrivalDate': 'count'})\n  rooms_counter.rename(columns={'ArrivalDate': 'NumberOfRooms'}, inplace=True)\n  rooms_counter.reset_index(inplace=True)\n  DF['guest_ltv'] = DF['guest_ltv'].merge(rooms_counter, how='outer', on=['GuestID', 'ArrivalDate'], sort=False)\n\n  # Build DataFrame for 1st Reservations\n  DF['1st_reservation'] = DF['guest_ltv'].groupby(by=['GuestID']).first()\n  DF['1st_reservation'].reset_index(inplace=True)\n  DF['1st_reservation']['is1stVisit'] = DF['1st_reservation'].NumberOfArrivals == 1\n  DF['1st_reservation'].drop(columns=['NumberOfArrivals'],inplace=True) # every first-time cus have NumberOfArrivals=1 so we do not need this\n  DF['1st_reservation'].to_csv(folder_out_path + \"1st_reservation.csv\", index=False)\n\n  # Build DataFrame for Returning Guests\n  DF['frequent_guests'] = DF['guest_ltv'][DF['guest_ltv'].NumberOfArrivals > 1]\n  DF['frequent_guests'].to_csv(folder_out_path + \"returning_guests.csv\", index=False)\n","74fbc283":"# this def for check data \ndef data_impute(df: pd.DataFrame) -> pd.DataFrame:\n  df = filter_out_minor_categoricals(df=df,column_names=['Channel', 'Country'])\n\n  # Drop missing-valued samples\n  df.dropna(subset=['TotalPayment', 'ArrivalDate', 'DepartureDate'], inplace=True)\n\n  # Normalize\n  df.Status = df.Status.str.upper()\n\n  return df\n\n# def for Recency\ndef generate_features_recency(guest_data: pd.DataFrame, END_DATE):\n    most_recent_date = guest_data.DepartureDate.max()\n    inactive_days = int((END_DATE-most_recent_date) \/ pd.Timedelta(days=1))\n    active_months = list(guest_data.ArrivalDate.dt.month) + \\\n                    list(guest_data.DepartureDate.dt.month)\n\n    most_active_month = max(set(active_months), key=active_months.count)\n    most_active_quarter = month_to_quarter(most_active_month)\n    least_active_month = min(set(active_months), key=active_months.count)\n    least_active_quarter = month_to_quarter(least_active_month)\n    \n    return inactive_days, most_active_month, least_active_month, most_active_quarter, least_active_quarter\n\n# def for Frequency\ndef generate_features_frequency(guest_data: pd.DataFrame, customer_lifetime,\n                                date_1st_booking, END_DATE, MAM_data):\n    n_orders = len(guest_data.groupby(by=['ArrivalDate']))\n    average_orders = int(n_orders\/customer_lifetime) if customer_lifetime>1 else n_orders\n    n_orders_in_MAM = len(MAM_data)\n    average_orders_in_MAM = int(n_orders_in_MAM\/customer_lifetime) if customer_lifetime>1 else n_orders_in_MAM\n    \n    return n_orders, average_orders, average_orders_in_MAM\n\n# def for monetary\ndef generate_features_monetary(guest_data: pd.DataFrame, customer_lifetime, MAM_data):\n    total_revenue = guest_data.TotalPayment.sum()\n    average_revenue = total_revenue\/customer_lifetime if customer_lifetime>1 else total_revenue\n    revenue_in_MAM = MAM_data.TotalPayment.sum()\n    average_revenue_in_MAM = revenue_in_MAM\/int(customer_lifetime) if customer_lifetime>1 else revenue_in_MAM\n    \n    return total_revenue, average_revenue, average_revenue_in_MAM\n\n\ndef generate_features_nonRFM(guest_data: pd.DataFrame, n_orders, total_revenue):\n    nights_in_house = int(np.mean(guest_data.Nights))\n    arrdates = guest_data.ArrivalDate.sort_values()\n    days_between_orders = (arrdates.values[1:] - arrdates.values[:-1]) \/ pd.Timedelta(days=1)\n    days_between_orders = np.mean(days_between_orders) if len(days_between_orders)>0 \\\n                            else (START_DATE-END_DATE) \/ pd.Timedelta(days=1)\n    days_between_orders = int(days_between_orders)\n\n    n_rooms = len(guest_data)\n    average_rooms = int(n_rooms\/n_orders) \n    average_revenue_per_order = total_revenue \/ n_orders\n    average_revenue_per_room = total_revenue \/ n_rooms\n\n    return days_between_orders, nights_in_house, n_rooms, \\\n                average_rooms, average_revenue_per_order, average_revenue_per_room\n\n\ndef feature_engineering_manual_for_LTV(df: pd.DataFrame) -> pd.DataFrame:\n    global START_DATE, END_DATE\n\n    features = [\n        # Recency features\n        'InactiveDays', 'MostActiveMonth', 'LeastActiveMonth', 'MostActiveQuarter', 'LeastActiveQuarter',\n        \n        # Frequency features\n        'NumberOfOrders', 'AverageOrdersPerYear', 'AverageOrdersInMostActiveMonth',\n        \n        # Monetary features\n        'TotalRevenue', 'AverageRevenuePerYear', 'AverageRevenueInMostActiveMonth',\n        \n        # Non-RFM features\n        'AverageDaysBetweenOrders', 'AverageNightsInHouse', 'NumberOfRooms', \n        'AverageRoomsPerOrder', 'AverageRevenuePerOrder', 'AverageRevenuePerRoom',\n        \n        # Personal features\n        'Lifetime', 'Country',\n    ]\n    guests_df = pd.DataFrame(columns=features)\n\n    for guest_id, guest_data in print_progress(df.groupby(by=['GuestID'])):\n    \n        # Filter by time window\n        # date_1st_booking = guest_data.CreatedDate.min()\n        date_1st_booking = guest_data.ArrivalDate.min()\n        customer_lifetime = (END_DATE-date_1st_booking).days \/ 365.25\n        \n        # Generate features for RECENCY\n        inactive_days, most_active_month, least_active_month, \\\n                    most_active_quarter, least_active_quarter = generate_features_recency(guest_data, END_DATE)\n        \n        # Filter Most Active Month\n        MAM_data = guest_data.loc[\n            # (guest_data.CreatedDate.dt.month == most_active_month) |\n            (guest_data.ArrivalDate.dt.month == most_active_month) |\n            (guest_data.DepartureDate.dt.month == most_active_month)\n        ]\n        \n        # Generate features for FREQUENCY\n        n_orders, average_orders, average_orders_in_MAM = generate_features_frequency(\n            guest_data, customer_lifetime, date_1st_booking, END_DATE, MAM_data\n        )\n\n        # Generate features for MONETARY\n        total_revenue, average_revenue, average_revenue_in_MAM = generate_features_monetary(\n            guest_data, customer_lifetime, MAM_data\n        )\n        \n        # Generate features for LTV classification\n        days_between_orders, nights_in_house, n_rooms, \\\n        average_rooms, average_revenue_per_order, average_revenue_per_room = generate_features_nonRFM(\n            guest_data, n_orders, total_revenue\n        )\n        \n        # Feed generated features into DataFrame\n        guests_df.loc[guest_id] = [\n            inactive_days, most_active_month, least_active_month, most_active_quarter, least_active_quarter,\n            n_orders, average_orders, average_orders_in_MAM,\n            total_revenue, average_revenue, average_revenue_in_MAM,\n            days_between_orders, nights_in_house, n_rooms, average_rooms, average_revenue_per_order, average_revenue_per_room,\n            customer_lifetime, guest_data.Country.unique()[0]\n        ]\n\n    guests_df['GuestID'] = list(guests_df.index)\n    guests_df.reset_index(drop=True, inplace=True)\n\n    for col in guests_df.columns:\n        if col == 'Country':\n            continue\n        guests_df[col] = guests_df[col].astype(float)\n\n    return guests_df\n\n#def for LTV_returning_guests\ndef feature_engineering_auto_for_LTV(df: pd.DataFrame) -> pd.DataFrame:\n    df_name = 'returning_guests'\n    EntitySet = ft.EntitySet(id=df_name)\n    EntitySet = EntitySet.entity_from_dataframe(\n        entity_id=df_name,\n        dataframe=df,\n        make_index=False, \n        index='GuestID',\n        variable_types={\n            'GuestID': Id,\n            'Country': Categorical,\n            'Lifetime': Age,\n            \n            'TotalRevenue': Price,\n            'AverageRevenuePerYear': Price,\n            'AverageRevenueInMostActiveMonth': Price,\n            \n            'InactiveDays': Numeric,\n            'NumberOfRooms': Numeric,\n            'NumberOfOrders': Numeric,\n            'AverageNightsInHouse': Numeric, \n            'AverageDaysBetweenOrders': Numeric, \n            'AverageRevenuePerOrder': Numeric,\n            'AverageRevenuePerRoom': Numeric,\n            'AverageRoomsPerOrder': Numeric,\n            'AverageOrdersPerYear': Numeric,\n            'AverageOrdersInMostActiveMonth': Numeric,\n            \n            'MostActiveMonth': Categorical,\n            'LeastActiveMonth': Categorical,\n            'MostActiveQuarter': Categorical,\n            'LeastActiveQuarter': Categorical,\n        }\n    )\n    \n    feature_matrix, feature_definitions = auto_feature_engineering(\n        entity_set=EntitySet, table_name=df_name, verbose=True\n    )\n    \n    return feature_matrix\n\n#def for 1stBooking\ndef feature_engineering_auto_for_1stBooking(df: pd.DataFrame) -> pd.DataFrame:\n  df_name = '1st_reservation'\n  EntitySet = ft.EntitySet(id=df_name)\n  EntitySet = EntitySet.entity_from_dataframe(\n        entity_id=df_name,\n        dataframe=df,\n        make_index=False, \n        index='GuestID',\n        variable_types={\n            'GuestID': Id,\n            \n            'Adults': People,\n            'Children': People,\n            'TotalPayment': Price,\n            'Nights': Numeric,\n            'RoomPrice': Numeric,\n            'NumberOfRooms': Numeric,\n            \n            'Channel': Categorical,\n            'Status': Categorical,\n            'RoomGroupID': Categorical,\n            \n            # 'CreatedDate': Datetime,\n            'ArrivalDate': Datetime,\n            'DepartureDate': Datetime,\n        }\n    )\n    \n  feature_matrix, feature_definitions = auto_feature_engineering(\n        entity_set=EntitySet, table_name=df_name, verbose=True\n    )\n    \n  return feature_matrix\n\n# this after for all \n# pipeline for load data -> progressing data(impute-> Feature)\ndef feature_engineering_pipeline(folder_path: str,folder_out_path:str, current_year: int = 0):\n  global END_DATE\n\n  if current_year != 0:\n        input_date = str(int(current_year)+1) + \"-01-01\"\n        END_DATE = pd.to_datetime(input_date, format='%Y-%m-%d')\n  else:\n        input_date = str(int(datetime.now().year)+1) + \"-01-01\"\n        END_DATE = pd.to_datetime(input_date, format='%Y-%m-%d')\n\n  # Load data\n  DF = dict()\n  df_names = ['1st_reservation', 'returning_guests']\n  for df_name in df_names:\n        DF[df_name] = load_csv(folder_out_path+f\"{df_name}.csv\")\n        for col in DF[df_name].columns:\n            if 'date' in col.lower():\n                DF[df_name][col] = pd.to_datetime(DF[df_name][col])\n\n  # Preprocess for raw features\n  for df_name in df_names:   \n        # Data Imputation\n        DF[df_name] = data_impute(DF[df_name])\n\n        # Create feature `Nights`\n        DF[df_name]['Nights'] = (DF[df_name].DepartureDate - DF[df_name].ArrivalDate) \/ pd.Timedelta(days=1)\n        DF[df_name]['Nights'] = DF[df_name]['Nights'].astype(int)\n\n        # Save\n        DF[df_name].to_csv(folder_out_path+f\"{df_name}_fillna.csv\", index=False)\n\n  # Feature Engineering\n  DF['returning_guests'] = feature_engineering_manual_for_LTV(DF['returning_guests'])\n  DF['returning_guests'] = feature_engineering_auto_for_LTV(DF['returning_guests'])\n  DF['returning_guests'].to_csv(folder_out_path+\"returning_guests_processed.csv\")\n\n  dsets = DF['1st_reservation']['is1stVisit']\n  dsets.index = DF['1st_reservation'].GuestID\n  Xs = DF['1st_reservation'].drop(columns=['is1stVisit'], axis=1)\n  Xs = feature_engineering_auto_for_1stBooking(Xs)\n  DF['1st_reservation'] = pd.concat([Xs, dsets.to_frame()], axis=1)\n  DF['1st_reservation'].to_csv(folder_out_path+\"1st_reservation_processed.csv\")","71c2115e":"def boxcox(x, lmbda=None, alpha=None):\n    \n    x = np.asarray(x)\n    if x.ndim != 1:\n        raise ValueError(\"Data must be 1-dimensional.\")\n\n    if x.size == 0:\n        return x\n\n    if np.all(x == x[0]):\n        raise ValueError(\"Data must not be constant.\")\n\n    if any(x <= 0):\n        raise ValueError(\"Data must be positive.\")\n\n    if lmbda is not None:  # single transformation\n        return special.boxcox(x, lmbda)\n\n    # If lmbda=None, find the lmbda that maximizes the log-likelihood function.\n    lmax = boxcox_normmax(x, method='mle')\n    y = boxcox(x, lmax)\n\n    if alpha is None:\n        return y, lmax\n    else:\n        # Find confidence interval\n        interval = _boxcox_conf_interval(x, lmax, alpha)\n        return y, lmax, interval\n","6a31610f":"# def for LTV\ndef split_ltv(folder_path: str,folder_out_path:str):\n  \n  DF = pd.read_csv(folder_out_path+\"returning_guests_processed.csv\")\n\n  MIN_LIFETIME = DF.Lifetime.describe().loc['50%']\n  # --with k=12 month we cannot detect anything beacause of min of lifetime is 1.23 it's mean >12 mounth \n  # --split data \n  df = dict() \n  df['longlife']= DF[DF.Lifetime >= MIN_LIFETIME]\n  df['shortlife']= DF[DF.Lifetime < MIN_LIFETIME]\n\n  \n  fig, ax = plt.subplots(1, 3, figsize=(12,4))\n  sns.distplot(df['longlife']['InactiveDays'], ax = ax[0]) #R\n  sns.distplot(df['longlife']['NumberOfOrders'], ax = ax[1]) #f\n  sns.distplot(df['longlife']['TotalRevenue'], ax = ax[2]) #M\n  plt.show()\n\n  data_t=pd.DataFrame()\n  data_t['GuestID']=df['longlife']['GuestID']\n  data_t['InactiveDays_tran'] = scipy.stats.boxcox(df['longlife']['InactiveDays'])[0]\n  data_t['NumberOfOrders_tran'] = scipy.stats.boxcox(df['longlife']['NumberOfOrders'])[0]\n  data_t['TotalRevenue_tran'] = scipy.stats.boxcox(df['longlife']['TotalRevenue'])[0]\n\n  scaler = StandardScaler()\n  scaler.fit(data_t[['InactiveDays_tran','NumberOfOrders_tran','TotalRevenue_tran']])\n\n  data_t[['InactiveDays_tran','NumberOfOrders_tran','TotalRevenue_tran']]= scaler.transform(data_t[['InactiveDays_tran','NumberOfOrders_tran','TotalRevenue_tran']])\n  # -- this steps, i'll normalize the data for make three of RFM can be normal distribution\n  # -- of couse this need a vision of person for have a good normallize(boxcox, sqrt, log) \n  # -- but I hope when i go to team I can develop more on this automatically and give more accurate results\n  df['longlife']=df['longlife'].merge(data_t,how='outer', on='GuestID')\n  df['longlife'].drop(columns=[\"InactiveDays\",\"NumberOfOrders\",\"TotalRevenue\"],inplace=True)\n  df['longlife'].rename(columns={\"InactiveDays_tran\":\"InactiveDays\",\"NumberOfOrders_tran\":\"NumberOfOrders\",\"TotalRevenue_tran\":\"TotalRevenue\"},inplace=True)\n                                      \n  print(df['longlife'].head())\n  CLUSTEROR = dict()\n  K_CLUSTERS = dict()\n  WEIGHTS = {'Recency': 1, 'Frequency': 3, 'Monetary': 5}\n  for feature, cluster, ascending in zip(['InactiveDays', 'NumberOfOrders', 'TotalRevenue'],\n                                           ['Recency', 'Frequency', 'Monetary'], \n                                           [False, True, True]):\n    df['longlife'], best_k, model = feature_clustering(df['longlife'], feature, cluster+'_Cluster', ascending)\n    CLUSTEROR[cluster] = model\n    K_CLUSTERS[cluster] = best_k\n\n  df['longlife']['RFM_Score'] = WEIGHTS['Recency'] \/ K_CLUSTERS['Recency'] * df['longlife']['Recency_Cluster'] \\\n                               + WEIGHTS['Frequency'] \/ K_CLUSTERS['Frequency'] * df['longlife']['Frequency_Cluster'] \\\n                                + WEIGHTS['Monetary'] \/ K_CLUSTERS['Monetary'] * df['longlife']['Monetary_Cluster']\n  \n  df['longlife']['Estimated_LTV'] = (df['longlife'].TotalRevenue \/ df['longlife'].NumberOfOrders) ** (1 \/ (1+df['longlife'].InactiveDays\/365.25))\n  \n  df['longlife'], _, model = feature_clustering(df['longlife'], \n                                                   feature_name='RFM_Score', \n                                                   cluster_name='LTV_Cluster', \n                                                   ascending=True, \n                                                   reorder_by_feature='Estimated_LTV', \n                                                   best_k=3)\n  \n  df['longlife']['LTV_Class'] = df['longlife']['LTV_Cluster'].map(\n        {1: 'CLV_Low_Prob', 2: 'CLV_Mid_Prob', 3: 'CLV_High_Prob'}\n    )\n  df['longlife'] = pd.concat([df['longlife'], \n                                 pd.get_dummies(df['longlife'].LTV_Class)], axis=1)\n  df['longlife'][\n        ['GuestID', 'LTV_Class', 'CLV_Low_Prob', 'CLV_Mid_Prob', 'CLV_High_Prob']\n    ].to_csv(folder_out_path+'LTV_class_of_long_time_guests.csv', index=False)\n\n    # Model Classification\n  df['longlife'].drop(columns=['Estimated_LTV', 'RFM_Score', \n                                  'InactiveDays', 'NumberOfOrders', 'TotalRevenue', \n                                  'Recency_Cluster', 'Frequency_Cluster', 'Monetary_Cluster',\n                                  'LTV_Class', 'CLV_Low_Prob', 'CLV_Mid_Prob', 'CLV_High_Prob'],\n                         errors='ignore', \n                         inplace=True)\n  \n  X, Y, target_weights = preprocess_for_classifier(df['longlife'], \n                                                  target_name='LTV_Cluster', \n                                                  id_cols=['GuestID', 'Lifetime'])\n  \n  classifier = XGBClassifier(max_depth=5, \n                               learning_rate=0.1, \n                               objective='multi:softprob', \n                               n_jobs=-1)\n  classifier.fit(X['train'], Y['train'], \n                   sample_weight=target_weights.values, \n                   eval_metric=[\"mlogloss\"], \n                   eval_set=[(X['train'], Y['train']), (X['test'], Y['test'])], \n                   early_stopping_rounds=7\n                   )\n\n  visualize_results(classifier, X, Y)\n\n  print(df['longlife'].head())\n  \n  X['infer'] = df['shortlife'].copy()\n  unused_features = []\n  for col in X['infer'].columns:\n        if col not in list(X['train'].columns):\n          unused_features.append(col)\n\n  X['infer'].drop(columns=unused_features, inplace=True)\n  df['shortlife'][['CLV_Low_Prob', 'CLV_Mid_Prob', 'CLV_High_Prob']] = classifier.predict_proba(X['infer'])\n  df['shortlife'].sort_values(by=['GuestID'], inplace=True)\n  df['shortlife']['LTV_Cluster'] = df['shortlife'][['CLV_Low_Prob', 'CLV_Mid_Prob', 'CLV_High_Prob']].apply(np.argmax, axis=1)\n  df['shortlife']['LTV_Cluster'] += 1\n  df['shortlife']['LTV_Class'] = df['shortlife']['LTV_Cluster'].map(\n        {1: 'CLV_Low_Prob', 2: 'CLV_Mid_Prob', 3: 'CLV_High_Prob'}\n    )\n  df['shortlife'][\n        ['GuestID', 'LTV_Class', 'CLV_Low_Prob', 'CLV_Mid_Prob', 'CLV_High_Prob']\n    ].to_csv(folder_out_path+'LTV_class_of_short_time_guests.csv', index=False)\n","df474a16":"guest_type_detector('..\/input\/hcg-test-data-services-dept\/','.\/')\n","f2828cfd":"feature_engineering_pipeline('..\/input\/hcg-test-data-services-dept\/','.\/',2021)","fb4430cc":"split_ltv('..\/input\/hcg-test-data-services-dept\/','.\/')","114099f9":"featuretool","8d702e66":"**utils**","5b85cd2b":"# **LIB I USE**","9de1ae8e":"# **CODE DETECT THE CUSTOMERS**","703ba709":"detect new cus and returning cus","38e5ffcc":"# **all def and class for feature** ","7dcd110b":"# **MODELLING**","67f92fca":"bassic lib","53cae33d":"we can see that with my normal distribution the result have the higher accuracy on training and testing data.\n","083bd64c":"**lib of ML**"}}