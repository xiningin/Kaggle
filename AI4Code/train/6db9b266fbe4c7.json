{"cell_type":{"62c0d86a":"code","a69fba0f":"code","62841544":"code","a45bb711":"code","b3f439fe":"code","09b48df0":"code","d4ad55c8":"code","8a58ce1f":"code","d4718da1":"code","31c3ecf2":"code","81b13f56":"code","19c46602":"code","dcc0ccab":"code","8fe71e68":"code","cdc925d5":"code","1171603b":"code","dda567cf":"code","54bd166f":"code","23042a61":"code","d9de1c72":"code","e8cdf421":"code","89d79258":"code","3bd5e497":"code","5964abfc":"code","fc6dc073":"code","2e3177a6":"code","549fed50":"code","379b2e68":"code","57810d74":"code","47a79611":"code","fa34db2b":"markdown","9b7c6c8c":"markdown","2e2ee373":"markdown","9ac2a838":"markdown","6d3cc0f3":"markdown","6de40d4d":"markdown","1c72c6eb":"markdown","5f150d71":"markdown","7e6621d8":"markdown","2bddc331":"markdown","e6320096":"markdown","75dcc60a":"markdown","5bd480d8":"markdown","f4f8a74c":"markdown","607c5d8f":"markdown","1da7c616":"markdown","5844bf60":"markdown","9e530ac9":"markdown","21eea32f":"markdown","e5d39424":"markdown","e608bbf7":"markdown","434444b8":"markdown"},"source":{"62c0d86a":"%load_ext autoreload\n%autoreload 2","a69fba0f":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport tensorflow as tf\nfrom glob import glob","62841544":"import sys\nprint(sys.version)","a45bb711":"print(tf.__version__)","b3f439fe":"# This is needed to display the images.\n%matplotlib inline","09b48df0":"# What model to download.\nMODEL_NAME = 'ssd_mobilenet_v1_coco_11_06_2017'\nMODEL_FILE = MODEL_NAME + '.tar.gz'\nDOWNLOAD_BASE = 'http:\/\/download.tensorflow.org\/models\/object_detection\/'\n\n# Path to frozen detection graph. This is the actual model that is used for the object detection.\nmodel_path = \".\/\"\nPATH_TO_CKPT = model_path + MODEL_NAME + '\/frozen_inference_graph.pb'\n\n\ndef download_model():\n    import six.moves.urllib as urllib\n    import tarfile\n\n    opener = urllib.request.URLopener()\n    opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n    tar_file = tarfile.open(MODEL_FILE)\n    for file in tar_file.getmembers():\n        file_name = os.path.basename(file.name)\n        if 'frozen_inference_graph.pb' in file_name:\n            tar_file.extract(file, os.getcwd())","d4ad55c8":"def load_graph():\n    if not os.path.exists(PATH_TO_CKPT):\n        download_model()\n\n    detection_graph = tf.Graph()\n    with detection_graph.as_default():\n        od_graph_def = tf.GraphDef()\n        with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n            serialized_graph = fid.read()\n            od_graph_def.ParseFromString(serialized_graph)\n            tf.import_graph_def(od_graph_def, name='')\n\n    return detection_graph\n\ndef select_boxes(boxes, classes, scores, score_threshold=0, target_class=10):\n    \"\"\"\n\n    :param boxes:\n    :param classes:\n    :param scores:\n    :param target_class: default traffic light id in COCO dataset is 10\n    :return:\n    \"\"\"\n\n    sq_scores = np.squeeze(scores)\n    sq_classes = np.squeeze(classes)\n    sq_boxes = np.squeeze(boxes)\n\n    sel_id = np.logical_and(sq_classes == target_class, sq_scores > score_threshold)\n\n    return sq_boxes[sel_id]\n\nclass TLClassifier(object):\n    def __init__(self):\n\n        self.detection_graph = load_graph()\n        self.extract_graph_components()\n        self.sess = tf.Session(graph=self.detection_graph)\n\n        # run the first session to \"warm up\"\n        dummy_image = np.zeros((100, 100, 3))\n        self.detect_multi_object(dummy_image,0.1)\n        self.traffic_light_box = None\n        self.classified_index = 0\n\n    def extract_graph_components(self):\n        # Definite input and output Tensors for detection_graph\n        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')\n        # Each box represents a part of the image where a particular object was detected.\n        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')\n        # Each score represent how level of confidence for each of the objects.\n        # Score is shown on the result image, together with the class label.\n        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')\n        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')\n        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')\n    \n    def detect_multi_object(self, image_np, score_threshold):\n        \"\"\"\n        Return detection boxes in a image\n\n        :param image_np:\n        :param score_threshold:\n        :return:\n        \"\"\"\n\n        # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n        image_np_expanded = np.expand_dims(image_np, axis=0)\n        # Actual detection.\n\n        (boxes, scores, classes, num) = self.sess.run(\n            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],\n            feed_dict={self.image_tensor: image_np_expanded})\n\n        sel_boxes = select_boxes(boxes=boxes, classes=classes, scores=scores,\n                                 score_threshold=score_threshold, target_class=10)\n\n        return sel_boxes\n","8a58ce1f":"%time\ntest_file = \"\/kaggle\/input\/lisa-traffic-light-dataset\/sample-dayClip6\/sample-dayClip6\/frames\/dayClip6--00332.jpg\"\n\nfrom PIL import Image\nim = Image.open(test_file)\nimage_np = np.asarray(im)","d4718da1":"plt.imshow(image_np)","31c3ecf2":"tlc=TLClassifier()","81b13f56":"def crop_roi_image(image_np, sel_box):\n    im_height, im_width, _ = image_np.shape\n    (left, right, top, bottom) = (sel_box[1] * im_width, sel_box[3] * im_width,\n                                  sel_box[0] * im_height, sel_box[2] * im_height)\n    cropped_image = image_np[int(top):int(bottom), int(left):int(right), :]\n    return cropped_image","19c46602":"%time\nboxes=tlc.detect_multi_object(image_np,score_threshold=0.2)\ncropped_image=crop_roi_image(image_np,boxes[0])\nplt.imshow(cropped_image)","dcc0ccab":"from skimage.color import rgb2grey,rgb2hsv\nhsv_test_image=rgb2hsv(cropped_image)","8fe71e68":"plt.imshow(hsv_test_image[:,:,1])","cdc925d5":"s_val_1d=hsv_test_image[:,:,1].ravel()\nplt.hist(s_val_1d,bins=50)\nplt.xlabel(\"saturation\")\nplt.ylabel(\"occurences\")\nplt.show()","1171603b":"saturation_threshold=0.8\nidx = hsv_test_image[:, :, 1] <=saturation_threshold\nsat_mask = np.ones_like(hsv_test_image[:, :, 1])\nsat_mask[idx] = 0\nfig,ax=plt.subplots(nrows=1,ncols=2)\nax[0].imshow(hsv_test_image[:,:,1])\nax[1].imshow(sat_mask)","dda567cf":"def high_value_region_mask(hsv_image, v_thres=0.6):\n    if hsv_image.dtype == np.int:\n        idx = (hsv_image[:, :, 2].astype(np.float) \/ 255.0) < v_thres\n    else:\n        idx = (hsv_image[:, :, 2].astype(np.float)) < v_thres\n    mask = np.ones_like(hsv_image[:, :, 2])\n    mask[idx] = 0\n    return mask","54bd166f":"v_thres_val=0.9\nval_mask=high_value_region_mask(hsv_test_image,v_thres=v_thres_val)\nplt.imshow(val_mask)","23042a61":"def get_masked_hue_image(hsv_test_image):\n\n    s_thres_val = channel_percentile(hsv_test_image[:, :, 1], percentile=70)\n    v_thres_val = channel_percentile(hsv_test_image[:, :, 2], percentile=70)\n    val_mask = high_value_region_mask(hsv_test_image, v_thres=v_thres_val)\n    sat_mask = high_saturation_region_mask(hsv_test_image, s_thres=s_thres_val)\n    masked_hue_image = hsv_test_image[:, :, 0]\n    return masked_hue_image","d9de1c72":"def high_saturation_region_mask(hsv_image, s_thres=0.6):\n    if hsv_image.dtype == np.int:\n        idx = (hsv_image[:, :, 1].astype(np.float) \/ 255.0) < s_thres\n    else:\n        idx = (hsv_image[:, :, 1].astype(np.float)) < s_thres\n    mask = np.ones_like(hsv_image[:, :, 1])\n    mask[idx] = 0\n    return mask\n\n\ndef channel_percentile(single_chan_image, percentile):\n    sq_image = np.squeeze(single_chan_image)\n    assert len(sq_image.shape) < 3\n\n    thres_value = np.percentile(sq_image.ravel(), percentile)\n\n    return float(thres_value) \/ 255.0","e8cdf421":"hue_image=hsv_test_image[:,:,0]\nfig,ax=plt.subplots(nrows=1,ncols=2)\nax[0].imshow(hue_image,cmap='hsv')\nax[0].set_title(\"hue image\")\nax[1].imshow(np.logical_and(sat_mask,val_mask))\nax[1].set_title(\"mask to be applied\")\n\n#TODO!!!!! redraw this image. Use hue color map, make zero values to \"black","89d79258":"hue_1d=hue_image[np.logical_and(val_mask, sat_mask)].ravel()\n\nplt.hist(hue_1d,bins=50)\nplt.xlabel(\"hue\")\nplt.ylabel(\"occurences\")\nplt.show()","3bd5e497":"def get_masked_hue_values(rgb_image):\n    \"\"\"\n    Get the pixels in the RGB image that has high saturation (S) and value (V) in HSV chanels\n\n    :param rgb_image: image (height, width, channel)\n    :return: a 1-d array\n    \"\"\"\n\n    hsv_test_image = rgb2hsv(rgb_image)\n    s_thres_val = channel_percentile(hsv_test_image[:, :, 1], percentile=30)\n    v_thres_val = channel_percentile(hsv_test_image[:, :, 2], percentile=70)\n    val_mask = high_value_region_mask(hsv_test_image, v_thres=v_thres_val)\n    sat_mask = high_saturation_region_mask(hsv_test_image, s_thres=s_thres_val)\n    masked_hue_image = hsv_test_image[:, :, 0] * 180\n    # Note that the following statement is not equivalent to\n    # masked_hue_1d= (maksed_hue_image*np.logical_and(val_mask,sat_mask)).ravel()\n    # Because zero in hue channel means red, we cannot just set unused pixels to zero.\n    masked_hue_1d = masked_hue_image[np.logical_and(val_mask, sat_mask)].ravel()\n\n    return masked_hue_1d\n\ndef convert_to_hue_angle(hue_array):\n    \"\"\"\n    Convert the hue values from [0,179] to radian degrees [-pi, pi]\n\n    :param hue_array: array-like, the hue values in degree [0,179]\n    :return: the angles of hue values in radians [-pi, pi]\n    \"\"\"\n\n    hue_cos = np.cos(hue_array * np.pi \/ 90)\n    hue_sine = np.sin(hue_array * np.pi \/ 90)\n\n    hue_angle = np.arctan2(hue_sine, hue_cos)\n\n    return hue_angle\n\n","5964abfc":"def get_rgy_color_mask(hue_value, from_01=False):\n    \"\"\"\n    return a tuple of np.ndarray that sets the pixels with red, green and yellow matrices to be true\n\n    :param hue_value:\n    :param from_01: True if the hue values is scaled from 0-1 (scikit-image), otherwise is -pi to pi\n    :return:\n    \"\"\"\n\n    if from_01:\n        n_hue_value = conver_to_hue_angle_from_01(hue_value)\n    else:\n        n_hue_value = hue_value\n\n    red_index = np.logical_and(n_hue_value < (0.125 * np.pi), n_hue_value > (-0.125 * np.pi))\n\n    green_index = np.logical_and(n_hue_value > (0.66 * np.pi), n_hue_value < np.pi)\n\n    yellow_index = np.logical_and(n_hue_value > (0.25 * np.pi), n_hue_value < (5.0 \/ 12.0 * np.pi))\n\n    return red_index, green_index, yellow_index\n\n\ndef classify_color_by_range(hue_value):\n    \"\"\"\n    Determine the color (red, yellow or green) in a hue value array\n\n    :param hue_value: hue_value is radians\n    :return: the color index ['red', 'yellow', 'green', '_', 'unknown']\n    \"\"\"\n\n    red_index, green_index, yellow_index = get_rgy_color_mask(hue_value)\n\n    color_counts = np.array([np.sum(red_index) \/ len(hue_value),\n                             np.sum(yellow_index) \/ len(hue_value),\n                             np.sum(green_index) \/ len(hue_value)])\n\n    color_text = ['red', 'yellow', 'green', '_', 'unknown']\n\n    min_index = np.argmax(color_counts)\n\n    return min_index, color_text[min_index]\n\ndef classify_color_cropped_image(rgb_image):\n    \"\"\"\n    Full pipeline of classifying the traffic light color from the traffic light image\n\n    :param rgb_image: the RGB image array (height,width, RGB channel)\n    :return: the color index ['red', 'yellow', 'green', '_', 'unknown']\n    \"\"\"\n\n    hue_1d_deg = get_masked_hue_values(rgb_image)\n\n    if len(hue_1d_deg) == 0:\n        return 4, 'unknown'\n\n    hue_1d_rad = convert_to_hue_angle(hue_1d_deg)\n\n    return classify_color_by_range(hue_1d_rad)","fc6dc073":"classify_color_cropped_image(cropped_image)","2e3177a6":"boxes=tlc.detect_multi_object(image_np,score_threshold=0.1)","549fed50":"def classify_all_boxes_in_image(image_np, boxes):\n    result_index_array = np.zeros(boxes.shape[0], dtype=np.int)\n    for i, box in enumerate(boxes):\n        cropped_image = crop_roi_image(image_np, box)\n        result_color_index, _ = classify_color_cropped_image(cropped_image)\n        result_index_array[i] = result_color_index\n\n    return result_index_array","379b2e68":"results_index=classify_all_boxes_in_image(image_np,boxes)","57810d74":"from vis_util import draw_result_on_image\ndef draw_results_on_image(image_np, boxes, tl_results_array):\n    for i, box in enumerate(boxes):\n        draw_result_on_image(image_np, box, tl_results_array[i])","47a79611":"n_image_np=np.copy(image_np)\ndraw_results_on_image(n_image_np, boxes, results_index)\nplt.imshow(n_image_np)","fa34db2b":"'''python\nfrom detect_traffic_light import \\\n    high_saturation_region_mask, high_value_region_mask,\\\nget_mean_hue_value,channel_percentile,low_saturation_region_mask,low_value_region_mask,\\\nconvert_to_hue_angle,classify_color_by_range\n'''","9b7c6c8c":"We perform similar operation for Value channle. This time I used my written function:","2e2ee373":"By performing these two masks, we reach the following mask for selecting the region to calculate the average hue values","9ac2a838":"### Download pretrained model","6d3cc0f3":"### Select the high-saturation region for further classification","6de40d4d":"Note that the following statement is not equivalent to\n```\nmasked_hue_1d= (maksed_hue_image*np.logical_and(val_mask,sat_mask)).ravel()\n```\nBecause zero in hue channel means red, we cannot just set unused pixels to zero.","1c72c6eb":"We then check the histogram to decide the filtering threshold","5f150d71":"The following codes detects the color by hue values of an image.","7e6621d8":"# Identify the traffic lights in an image","2bddc331":"The hue values in this region is near 0.5, which is green.","e6320096":"References:\n- [Tensorflow object detection tutorial](https:\/\/github.com\/tensorflow\/models\/blob\/master\/research\/object_detection\/object_detection_tutorial.ipynb)","75dcc60a":"# Put all things together to classify an image","5bd480d8":"# Detect and classify traffic light with pre-trained object detection model (and without re-training the model)\n\nIn this work I show a work flow of traffic light detection and classification by using a pretrained object detection model. This model has the two steps:\n\n1. Locate the object detection boxes of traffic lights using a pretrained object detection model\n2. In each detected image box, classify the color of traffic light by human intelligence, or aka. classical computer vsion approach.\n\nThis is a \"lazy\" solution because I didn't train any new machine learning model, just use some classical algorithmic away to classify the color of traffic lights.","f4f8a74c":"# Classify the color by human intelligence","607c5d8f":"## Filter out the region of interest","1da7c616":"### Show saturation values","5844bf60":"### Code for doing detection","9e530ac9":"# Further work\nIn this work I just some algorithmic way to classify the traffic light color. But this may be replaced by using the training the a machine learning model to do this. An alternative is training this CNN pretrained model to classify the traffic lights.","21eea32f":"The following function labels the traffic lights. The color of frames correspond to the traffic light colors.","e5d39424":"The traffic light is very \"light\". That is, it has high saturation values in HSV space. We can filter the high saturation area, and then classify the color by hue values.","e608bbf7":"### Convert the cropped image into HSV space","434444b8":"show python version"}}