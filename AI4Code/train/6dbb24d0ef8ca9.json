{"cell_type":{"c6ba200c":"code","ed6201ce":"code","df4cbb3e":"code","ea55e546":"code","e630156d":"code","f13d6aba":"code","2b0b06a7":"code","e38b846a":"code","1cd32570":"code","906c7bad":"code","6bf3ab2e":"code","ea837212":"markdown","5c261bcb":"markdown","89ba3ac5":"markdown","503b3cf2":"markdown","a858c0fb":"markdown","2be5de6d":"markdown"},"source":{"c6ba200c":"import os\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport seaborn as sns \nfrom sklearn import metrics\nimport torch\nimport torch.nn as nn \nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport transformers\nfrom transformers import BertForSequenceClassification\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.simplefilter('ignore')","ed6201ce":"#config\nTRAIN_PATH = '..\/input\/commonlitreadabilityprize\/train.csv'\nTEST_PATH = '..\/input\/commonlitreadabilityprize\/test.csv'\nEPOCHS = 10\nLR = 3e-4\nMAX_LEN = 200\nBERT_MODEL = '..\/input\/distilbertbaseuncased'\nTOKENIZER = transformers.BertTokenizer.from_pretrained(\n    BERT_MODEL,\n    do_lower_case=True\n)\nTRAIN_BS = 16\nVALID_BS = 32\nCOLUMNS = ['excerpt', 'target']\nDEVICE = device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nDEVICE","df4cbb3e":"#lets have a quick glance at the data\ntrain = pd.read_csv(TRAIN_PATH)\ntrain.head()","ea55e546":"#we need to check if the data is clean and complete\ntrain.info()\n#we will see the all required columns contains no NaN values","e630156d":"#to check how many unique values are there in each columns\n#this also gives us an idea of what sort of a problem we are going to face\nfor col in train.columns:\n    print(f\"{col}: {len(train[col].unique())}\")\n    \n#from this we can see that the target is not a categorical field cause it has \n#same number of unique values as the number of rows\n#So its kind of a regression problem ","f13d6aba":"class CommonLitDataset(Dataset):\n    def __init__(self, excerpt, target=None, test=False):\n        self.excerpt = excerpt\n        self.target = target\n        self.test = test \n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n        \n    def __len__(self):\n        return len(self.excerpt)\n    \n    def __getitem__(self, index):\n        excerpt = str(self.excerpt[index])\n        excerpt = ' '.join(excerpt.split())\n        \n        inputs = self.tokenizer.encode_plus(\n            excerpt,\n            None,\n            truncation=True,\n            add_special_tokens=True,\n            max_length = self.max_len,\n            pad_to_max_length = True\n        )\n        \n        ids = torch.tensor(inputs['input_ids'], dtype=torch.long)\n        mask = torch.tensor(inputs['attention_mask'], dtype=torch.long)\n        token_type_ids = torch.tensor(inputs['token_type_ids'], dtype=torch.long)\n        \n        if self.test:\n            return {\n                'ids': ids,\n                'mask': mask,\n                'token_type_ids': token_type_ids\n            }\n        else:\n            targets = torch.tensor(self.target[index], dtype=torch.float)\n            return {\n                'ids': ids,\n                'mask': mask,\n                'token_type_ids': token_type_ids,\n                'targets': targets\n            }      ","2b0b06a7":"class Engine:\n    def __init__(self,model,optimizer,train_dataloader,\n                 valid_dataloader,device):\n        self.model = model\n        self.optimizer = optimizer\n        self.train_data = train_dataloader\n        self.valid_data = valid_dataloader\n        self.device = device\n        \n    def loss_fn(self,outputs, targets):\n        return torch.sqrt(nn.MSELoss()(outputs, targets))\n    \n    def train_fn(self):\n        self.model.train()\n        i = 0\n        size = len(self.train_data)\n        for data in self.train_data:\n            ids = data['ids'].to(self.device,dtype=torch.long)\n            token_type_ids = data['token_type_ids'].to(self.device,dtype=torch.long)\n            mask = data['mask'].to(self.device,dtype=torch.long)\n            targets = data['targets'].to(self.device,dtype=torch.float)\n            \n            self.optimizer.zero_grad()\n            outputs = model(\n                    ids=ids,\n                    mask=mask,\n                    token_type_ids = token_type_ids\n            )\n            loss = self.loss_fn(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            \n            return loss\n    \n    def eval_fn(self):\n        self.model.eval()\n        _targets = []\n        _outputs = []\n        with torch.no_grad():\n            for data in self.valid_data:\n                ids = data['ids'].to(self.device,dtype=torch.long)\n                token_type_ids = data['token_type_ids'].to(self.device,dtype=torch.long)\n                mask = data['mask'].to(self.device,dtype=torch.long)\n                targets = data['targets'].to(self.device,dtype=torch.float)\n            \n                self.optimizer.zero_grad()\n                outputs = model(\n                        ids=ids,\n                        mask=mask,\n                        token_type_ids = token_type_ids\n                        )\n                val_loss = self.loss_fn(outputs, targets)\n                targets = targets.cpu().detach()\n                _targets.extend(targets.numpy().tolist())\n                outputs = outputs.cpu().detach()\n                _outputs.extend(outputs.numpy().tolist())\n                \n            return val_loss, _outputs, _targets\n        \n    def inference_fn(self, test_dl, infer_model):\n        outputs = []\n        infer_model.eval()\n        with torch.no_grad():\n            for i,data in enumerate(test_dl):\n                ids = data['ids'].to(self.device,dtype=torch.long)\n                token_type_ids = data['token_type_ids'].to(self.device,dtype=torch.long)\n                mask = data['mask'].to(self.device,dtype=torch.long)\n            \n                self.optimizer.zero_grad()\n                out = infer_model(\n                        ids=ids,\n                        mask=mask,\n                        token_type_ids = token_type_ids\n                        )\n                out = out.cpu().detach().numpy()\n                if i==0:\n                    outputs = out\n                else:\n                    outputs = np.concatenate((outputs,out), axis=None)\n                \n        return outputs","e38b846a":"#model\nclass DistilBERT(nn.Module):\n    def __init__(self):\n        super(DistilBERT, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(BERT_MODEL)\n        self.dropout1 = nn.Dropout(0.3)\n        self.out = nn.Linear(768, 1)\n        \n    def forward(self, ids, mask, token_type_ids):\n        _, output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n        output = self.dropout1(output)\n        output = self.out(output)\n        return output","1cd32570":"#Model training\ndata = pd.read_csv(TRAIN_PATH)\ndata = data.sample(frac=1).reset_index(drop=True)\ndata = data[COLUMNS]\n\ntrain_df = data[:2500].sample(frac=1).reset_index(drop=True)\nvalid_df = data[2500:].sample(frac=1).reset_index(drop=True)\nprint(f\"Train : {train_df.shape}\\nValidation: {valid_df.shape}\")\n\ntrain_ds = CommonLitDataset(\n    excerpt=train_df['excerpt'].values,\n    target = train_df['target'].values\n)\nvalid_ds = CommonLitDataset(\n    excerpt=valid_df['excerpt'].values,\n    target = valid_df['target'].values\n)\n\ntrain_dl = DataLoader(\n    train_ds,\n    batch_size = TRAIN_BS,\n    shuffle = True,\n    num_workers = 4\n)\nvalid_dl = DataLoader(\n    valid_ds,\n    batch_size= VALID_BS,\n    shuffle= True,\n    num_workers= 4 \n)\nmodel = DistilBERT().to(DEVICE);\noptimizer = transformers.AdamW(model.parameters(),lr=LR)\nengine = Engine(model=model,optimizer=optimizer,\n                train_dataloader=train_dl,\n                valid_dataloader=valid_dl,\n                device=DEVICE\n               )\nbest_loss = 10\nfor epoch in range(EPOCHS):\n    train_loss = engine.train_fn()\n    val_loss, outputs, targets = engine.eval_fn()\n    print(f\"epoch: {epoch}, train loss: {train_loss}, val_loss: {val_loss}\")\n    if val_loss < best_loss:\n        print(f\"saving model with loss: {val_loss}\")\n        torch.save(model.state_dict(),f\"CommonLit_{val_loss}.bin\")\n        best_loss = val_loss\n        \nprint(f\"final Report\\nValidation RMSE Loss: {best_loss}\")","906c7bad":"infer_model = DistilBERT()\ninfer_model.load_state_dict(torch.load(f\"CommonLit_{best_loss}.bin\"))\ninfer_model.to(device)\ninfer_model.eval()\n\ntest_df = pd.read_csv(TEST_PATH)\ntest_dataset = CommonLitDataset(\n    excerpt=test_df['excerpt'].values,\n    test=True\n)\ntest_dl = DataLoader(test_dataset,batch_size=16,shuffle=False,num_workers=4)\noutput = engine.inference_fn(test_dl,infer_model)\nsubmission_df = pd.DataFrame({'id': test_df.id, 'target': output.reshape(-1).tolist()})\nsubmission_df.head()","6bf3ab2e":"submission_df.to_csv('\/kaggle\/working\/submission.csv', index=False)\nsubmission_df","ea837212":"### **If you find it useful please upvote** :) ","5c261bcb":"### DistilBERT Model","89ba3ac5":"## **CommonLit Readability Prize**\n\nIn this competition, you\u2019ll build algorithms to rate the complexity of reading passages for grade 3-12 classroom use. \n\nThe dataset that includes readers from a wide variety of age groups and a large collection of texts taken from various domains. Winning models will be sure to incorporate text cohesion and semantics.\n\nInstead using the regular BERT implementation lets try out DistilBERT\n\nDistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a self-supervised fashion, using the BERT base model as a teacher. \n\n![BERT](https:\/\/blog.rasa.com\/content\/images\/2019\/09\/pruning_bert.png)","503b3cf2":"### Model Training","a858c0fb":"### Train and Evaluation Engine","2be5de6d":"### Inference and Submission"}}