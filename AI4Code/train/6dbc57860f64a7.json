{"cell_type":{"ad246ac6":"code","4fccc56a":"code","d3c18908":"code","aca6c013":"code","679d4d74":"code","81cddf32":"code","763ec9f5":"code","5d7044fd":"code","62c4971e":"code","58f2252b":"code","d7818f65":"code","a5139c23":"code","b3084e81":"code","3e6ef33b":"code","7be61dd8":"code","f29a8357":"code","0104febb":"code","00fd0a57":"code","fef42bc8":"code","c4e66ba8":"code","ad3fc599":"code","5aa291a0":"markdown","dd2ea996":"markdown","63dd60ea":"markdown","0e59bb88":"markdown","381a7b75":"markdown","a7f44d7c":"markdown","85892d68":"markdown","49325275":"markdown","32880572":"markdown"},"source":{"ad246ac6":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns","4fccc56a":"#Opening the data\noriginalTrain = pd.read_csv(\"..\/input\/predict-demand\/train.csv\")\noriginalTest = pd.read_csv(\"..\/input\/predict-demand\/test.csv\")","d3c18908":"#Lets see the shapes of the data so we know what we are dealing with\noriginalTrain.shape, originalTest.shape","aca6c013":"originalTrain.head()","679d4d74":"originalTest.head()","81cddf32":"originalTrain.describe()","763ec9f5":"originalTest.describe()","5d7044fd":"train = originalTrain.copy()\ntest = originalTest.copy()\n\n#Dropping unnecessary Id column.\n\ntrain.drop('id', axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)\n\n#Dropping rows without quantity\n\ntrain.dropna(axis=0, subset=['quantity'], inplace=True)\ntest.dropna(axis=0, subset=['quantity'],inplace=True)\n\n#Backing up target variables and dropping them from train data.\ny_train = train['quantity']\nX_train = train\nX_train.drop(columns=[\"quantity\"], inplace=True)\n\ny_test = test['quantity']\nX_test = test\nX_test.drop(columns=[\"quantity\"], inplace=True)","62c4971e":"# Display numerical correlations between features.\n\nsns.set(font_scale=1.2)\ncorrelation_train = train.corr()\nmask = np.triu(correlation_train.corr())\nplt.figure(figsize=(8, 8))\nsns.heatmap(correlation_train,\n            annot=True,\n            fmt='.1f',\n            cmap='coolwarm',\n            square=True,\n            mask=mask,\n            linewidths=1,\n            cbar=False)\n\nplt.show()","58f2252b":"features = pd.concat([X_train, X_test]).reset_index(drop=True)\n#Lets see the new shape of the features dataframe\nprint(features.shape)","d7818f65":"def missing_percentage(df):\n    \n    #Defining a function for returning missing ratios\n    \n    total = df.isnull().sum().sort_values(\n        ascending=False)[df.isnull().sum().sort_values(ascending=False) != 0]\n    percent = (df.isnull().sum().sort_values(ascending=False) \/ len(df) *\n              100)[(df.isnull().sum().sort_values(ascending=False) \/ len(df) *\n                   100) != 0]\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n","a5139c23":"#Checking 'NaN' values.\n\nmissing = missing_percentage(features)\n\nfig, ax = plt.subplots(figsize=(20, 5))\nsns.barplot(x=missing.index, y='Percent', data=missing, palette='Reds_r')\nplt.xticks(rotation=90)\n\ndisplay(missing.T.style.background_gradient(cmap='Reds', axis=1))\n","b3084e81":"#Import neccesary packages to create the pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline","3e6ef33b":"#Import the BaseEstimator\nfrom sklearn.base import BaseEstimator\n\n#Define pre-processor classes\nclass DateProcessor(BaseEstimator):\n\n    def __init__(self):\n        pass\n\n    def fit(self, documents, y=None):\n        return self\n\n    def transform(self, df):\n        new_df = df.copy()\n        new_df['date'] = pd.to_datetime(new_df['date'], errors=\"coerce\")\n        #df.dropna(axis=1, subset=['date'], inplace=True)\n        #format=\"%d%m%Y\",errors=\"ignore\"\n        new_df['day'] = new_df['date'].dt.day\n        new_df['month'] = new_df['date'].dt.month\n        new_df['year'] = new_df['date'].dt.year\n        \n        new_df.drop(inplace=True, columns='date')\n        return new_df\n\nclass CategoricalProcessor(BaseEstimator):\n    def __init__(self):\n        pass\n    \n    def fit(self, documents, y=None):\n        return self\n    \n    def _getDictionary(self, df):\n        new_df = df.copy()\n        cat_columns = new_df.select_dtypes(include=['object']).columns\n        dict = {}\n        for col in cat_columns:\n            tempMode = new_df.mode()[col][0]\n            dict[col] = tempMode\n        \n        return dict\n    \n    def transform(self, df):\n        new_df = df.copy()\n        imputer = self._getDictionary(new_df)\n        new_df = new_df.fillna(imputer)\n        new_df = pd.get_dummies(new_df)\n        \n        return new_df\n    \nclass NumericalProcessor(BaseEstimator):\n    def __init__(self):\n        pass\n    \n    def fit(self, documents, y=None):\n        return self\n    \n    def _getDictionary(self, df):\n        new_df = df.copy()\n        num_columns = new_df.select_dtypes(include=['float64', 'int64']).columns\n        dict = {}\n        for col in num_columns:\n            tempMean = new_df[col].mean()\n            dict[col] = tempMean\n        \n        return dict\n    \n    def transform(self, df):\n        new_df = df.copy()\n        imputer = self._getDictionary(new_df)\n        new_df = new_df.fillna(imputer)\n        \n        return new_df","7be61dd8":"#Import the model and GridSearch for Hyperparameter Optimization\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV","f29a8357":"#Defining the pipeline\nxgb = xgb.XGBRegressor()\nmodel_pipeline = Pipeline(steps=[\n                                ('process_date', DateProcessor()),\n                                ('num_process', NumericalProcessor()),\n                                ('cat_process', CategoricalProcessor()),\n                                ('XGBoost', xgb)\n                                ])","0104febb":"param_xgb = {'XGBoost__nthread':[4], #when use hyperthread, xgboost may become slower\n              'XGBoost__objective':['reg:linear'],\n              'XGBoost__learning_rate': [.03, 0.05, .07], #so called `eta` value\n              'XGBoost__max_depth': [5, 6, 7],\n              'XGBoost__min_child_weight': [4],\n              'XGBoost__silent': [1],\n              'XGBoost__subsample': [0.7],\n              'XGBoost__colsample_bytree': [0.7],\n              'XGBoost__tree_method': ['gpu_hist'],\n              'XGBoost__n_estimators': [500]}","00fd0a57":"grid_search_xgb = GridSearchCV(estimator = model_pipeline, param_grid = param_xgb, n_jobs = -1, verbose = 2, cv = 5)\ngrid_search_xgb.fit(X_train, y_train)","fef42bc8":"#Lets observe the best parameters for our XGBRegressor\nprint(\"Best parameter (CV score=%0.3f):\" % grid_search_xgb.best_score_)\nprint(grid_search_xgb.best_params_)","c4e66ba8":"#Score of our model after Hyperparameter optimization\ngrid_search_xgb.score(X_test, y_test)","ad3fc599":"#Just to see\ngrid_search_xgb.score(X_train, y_train)","5aa291a0":"<h4>Observations<\/h4>\n<li>We can see there's a negative correlation between quantity and price,\nindicating, that quantity tends to get lower as price increases <\/li>","dd2ea996":"<li>Id column looks useless, so we can safely drop it from both. I'm going to save our target (quantity) on a different variable so we can use it in future.<\/li>\n<li>Lets first make a copy of the dataframes so we can keep the originals intact<\/li>","63dd60ea":"<p>We can see that we have 7560 rows on the train dataframe, and 1080 rows on the test dataframe, both with 12 columns.\n\nWith that information, we can already calculate the distribution of train - test data:\npercentage_train_rows = 7560*100\/(7560+1080) = 87.5%\npercentage_test_rows = 100% - 87.5% = 12.5%\n7\/8 of the dataset belongs to train data and the remaining 1\/8 belongs to test data\n\nNow lets observe some of their elements<\/p>","0e59bb88":"<h1>Introduction<\/h1>\n<p>Hello all! In this notebook I'm going to analyze different products data and implement multiple \n    Machine Learning algorithms to predict the Demand of each Product<\/p>\n<h3>My main objectives on this project are:<\/h3>   \n<ul>\n    <li>Applying exploratory data analysis and trying to get some insights about our dataset<\/li>\n    <li>Getting data in better shape by transforming and feature engineering to help us in building better models<\/li>\n    <li>Building and tuning a XGBRegressor to get some results on predicting Demand<\/li>\n<\/ul>","381a7b75":"<h2>Missing Data<\/h2>\n<ul>\n    <li>Merge the datasets to see how many missing values there are and visualize them<\/li>\n<\/ul>","a7f44d7c":"<h2>Meeting the data<\/h2>\n<p>Lets open the data and see what we have<\/p>","85892d68":"<h2>Pipeline<\/h2>\n<p>Steps:<\/p>\n<ol>\n    <li>Extract year, month and day from date so we can use them as numerical features<\/li>\n    <li>Add Year, Month and Day columns to the dataset<\/li>\n    <li>Eliminate date column from the dataset<\/li>\n    <li>\n        <ol>\n            <li>Fill long, lat, price and pop columns with their mean values<\/li>\n            <li>Fill capacity, brand, shop, container, city, year, month, day with their most-repeated values<\/li>\n        <\/ol>\n    <\/li>\n    <li>One Hot Encode capacity, brand, shop, container, city, year, month and day<\/li>\n    <li>Fit the model<\/li>\n<\/ol>","49325275":"<h2>Importing Libraries<\/h2>\n<p>Lets start by importing some packages we are going to need<\/p>","32880572":"<h2>EDA<\/h2>\n<p>Exploratory Data Analysis<\/p>\n\n<p>We're going to start with basic correlation table here. I dropped the top part since it's just mirror of the other part below. With this table we can understand some linear relations between different features.<\/p>"}}