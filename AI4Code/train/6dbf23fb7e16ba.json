{"cell_type":{"e43ae8b2":"code","215838f2":"code","970b4dc2":"code","1dde0c09":"code","8b9ca5d0":"code","c517d195":"code","3e7537ee":"code","a533ca70":"code","7625866b":"code","0f9a67d3":"code","786c9a17":"code","53d1d520":"code","79299ffd":"code","fc1feee1":"code","fe3a4f1f":"code","1c0206ad":"code","14f8cbd1":"code","98fd6d24":"code","aa53b0af":"code","fe98f117":"code","cd24903d":"code","4434ae11":"code","d3783914":"markdown","04e4de58":"markdown","fcb52b2c":"markdown","2f6cba27":"markdown","b65f752d":"markdown","045eb4a5":"markdown","eba7fd52":"markdown","8ece917a":"markdown","8b81fae2":"markdown","32ba46d9":"markdown","78fd0fb5":"markdown","bdf9e0d2":"markdown","3ca6ad91":"markdown","bece05e4":"markdown","1a582d3a":"markdown","97ebde8f":"markdown","244e794d":"markdown","a0df6184":"markdown","9b098227":"markdown","6485691a":"markdown","687edeef":"markdown"},"source":{"e43ae8b2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","215838f2":"# save filepath to variable for easier access\nheard_file_path = '..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv'\n# read the data and store data in DataFrame titled melbourne_data\nheard_failure_data = pd.read_csv(heard_file_path) \n\n","970b4dc2":"heard_failure_data.dtypes","1dde0c09":"pd.isnull(heard_failure_data).sum()","8b9ca5d0":"from sklearn.model_selection import train_test_split\n\n# Read the data\n\nfeatures_all = ['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes',\n       'ejection_fraction',  'platelets','high_blood_pressure',\n       'serum_creatinine', 'serum_sodium', 'sex', 'smoking'\n       ]\n\n\n\n\nfeatures = ['age', 'anaemia',  'diabetes','creatinine_phosphokinase',\n       'ejection_fraction', 'platelets','high_blood_pressure',\n        'serum_sodium', 'sex','serum_creatinine', 'smoking'\n       ]\n\nnumeric = ['age', 'creatinine_phosphokinase', \n           'ejection_fraction', 'platelets', \n           'serum_creatinine']\ncategorical = ['anaemia', 'diabetes', 'high_blood_pressure', \n               'sex', 'smoking']\n\n\n# Separate target from predictors\ny = heard_failure_data.DEATH_EVENT\nX = heard_failure_data[features]\n\n# Divide data into training and validation subsets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# print a summary of the data in Melbourne data\nheard_failure_data.describe()","c517d195":"labels = heard_failure_data['DEATH_EVENT'].value_counts().index\nvalues = heard_failure_data['DEATH_EVENT'].value_counts()","3e7537ee":"plt.subplot(title='distrubution DEATH_EVENT' )\nplt.pie(values, labels=labels,autopct='%1.1f%%')\nplt.show()","a533ca70":"for feature in numeric:\n    bplot=sns.boxplot(y=feature, x='DEATH_EVENT', \n                     data=heard_failure_data, \n                     width=0.5,\n                     palette=\"colorblind\")\n    plt.show()\n\n#bplot=sns.stripplot(y='serum_creatinine', \n#                    x='DEATH_EVENT',\n#                   data=heard_failure_data, \n#                   jitter=True, \n#                   marker='o', \n#                   alpha=0.5,\n#                   color='black')\n\n","7625866b":"for feature in categorical:\n    fig, (ax0, ax1) = plt.subplots(1, 2)\n    fig.suptitle('Distribution of '+feature + '\\n' \n                 + '____________________')\n    \n    for i in range(0,2):\n        val = heard_failure_data.loc[heard_failure_data['DEATH_EVENT']==i][feature].value_counts()\n        lab = heard_failure_data.loc[heard_failure_data['DEATH_EVENT']==i][feature].value_counts().index\n        if i == 0:\n            ax0.pie(val,labels=lab,autopct='%1.1f%%')\n            ax0.set_title('DEATH_EVENT = False')\n        else:\n            ax1.pie(val,labels=lab,autopct='%1.1f%%')\n            ax1.set_title('DEATH_EVENT = True')\n        \n","0f9a67d3":"heard_failure_data['age'].describe()","786c9a17":"def map_aged(age):\n\n    if age <= 51:\n        return 1\n    elif age <= 60:\n        return 2\n    elif age <= 70:\n        return 3\n    else:\n        return 4\n\nheard_failure_data['Age_category'] = heard_failure_data['age'].apply(map_aged)","53d1d520":"colours = {1: 'C0',\n           2: 'C1',\n           3: 'C2',\n           4: 'C3'}\n\nfor feature in categorical:\n    fig, (ax0, ax1) = plt.subplots(1, 2)\n    fig.suptitle('Distribution of age Categories '+feature + ' and DEATH_EVENT = TRUE' '\\n' \n                 + '____________________')\n    \n    for i in range(0,2):\n        val = heard_failure_data.loc[(heard_failure_data['DEATH_EVENT']==1) & (heard_failure_data[feature]==i) ]['Age_category'].value_counts()\n        lab = heard_failure_data.loc[(heard_failure_data['DEATH_EVENT']==1) & (heard_failure_data[feature]==i) ]['Age_category'].value_counts().index\n        if i == 0:\n            ax0.pie(val,labels=lab,autopct='%1.1f%%',\n                   colors=[colours[key] for key in lab])\n            ax0.set_title(feature + ' = False')\n        else:\n            ax1.pie(val,labels=lab,autopct='%1.1f%%',\n                   colors=[colours[key] for key in lab])\n            ax1.set_title(feature +  '= True')","79299ffd":"colours = {1: 'C0',\n           2: 'C1',\n           3: 'C2',\n           4: 'C3'}\n\nfor feature in categorical:\n    fig, (ax0, ax1) = plt.subplots(1, 2)\n    fig.suptitle('Distribution of age Categories '+feature + ' and DEATH_EVENT = FALSE' '\\n' \n                 + '____________________')\n    \n    for i in range(0,2):\n        val = heard_failure_data.loc[(heard_failure_data['DEATH_EVENT']==0) & (heard_failure_data[feature]==i) ]['Age_category'].value_counts()\n        lab = heard_failure_data.loc[(heard_failure_data['DEATH_EVENT']==0) & (heard_failure_data[feature]==i) ]['Age_category'].value_counts().index\n        if i == 0:\n            ax0.pie(val,labels=lab,autopct='%1.1f%%',\n                   colors=[colours[key] for key in lab])\n            ax0.set_title(feature + ' = False')\n        else:\n            ax1.pie(val,labels=lab,autopct='%1.1f%%',\n                   colors=[colours[key] for key in lab])\n            ax1.set_title(feature +  '= True')","fc1feee1":"plt.figure(figsize=(10,8))\ncorrMatrix = heard_failure_data.corr()\nsns.heatmap(corrMatrix, annot=True)\nplt.show()","fe3a4f1f":"from sklearn.pipeline import Pipeline\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import LinearSVC\n\nmodels = []\n\nw = 3\n\n#RF = RandomForestClassifier(class_weight={0: 1, 1: w})\n\nmodels.append(RandomForestClassifier(class_weight={0: 1, 1: w}))\nmodels.append(GradientBoostingClassifier())\nmodels.append(DecisionTreeClassifier())\nmodels.append(LinearSVC(C=1.0))\n\n\n","1c0206ad":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.metrics import classification_report\n\n\n\n# Bundle preprocessing and modeling code in a pipeline\n\nfor model in models:\n\n    my_pipeline = Pipeline(steps=[\n                                  ('model', model)\n                                 ])\n\n    # Preprocessing of training data, fit model \n    my_pipeline.fit(X_train, y_train\n             )\n    \n    \n\n    # Preprocessing of validation data, get predictions\n    preds = my_pipeline.predict(X_valid)\n\n    # Evaluate the model\n    score = classification_report(y_valid, preds)\n    #f1    = f1_score(y_valid, preds)\n    print(str(model).split('(')[0],'\\n', score)\n    #print(str(model).split('(')[0],': F1:', f1)\n    \n","14f8cbd1":"model = RandomForestClassifier()\n\n\n\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = {'n_estimators':[5,50,100,150,200,250],\n              'max_depth':[2,5,10,15,20,100,200],\n             'criterion': ['gini', 'entropy'],\n             'random_state': [0],\n             'class_weight':[{0: 1, 1: 3},{0: 1, 1: 2},{0: 1, 1: 4}]\n             # 'class_weight': ['balanced\"']\n             }\n\nclf = GridSearchCV(model, parameters,scoring='f1')\n\nclf.fit(X_train, y_train)\npreds = clf.predict(X_valid)\n\n        # Evaluate the model\nscore = classification_report(y_valid, preds)\n#f1    = f1_score(y_valid, preds)\n#print(str(model).split('(')[0],': Accurancy:', score)\nprint(score)\n#print(str(model).split('(')[0],': F1:', f1)\nprint(clf.best_params_)\n    ","98fd6d24":"clf.best_params_","aa53b0af":"No lets take the best parameters","fe98f117":"model = RandomForestClassifier(class_weight= {0: 1, 1: 3}, \n                               criterion= 'entropy', \n                               max_depth= 2, \n                               n_estimators= 100, \n                               random_state = 0)\n\nmodel.fit(X_train, y_train\n             )\n    \n    \n\n    # Preprocessing of validation data, get predictions\npreds = model.predict(X_valid)\n\n    # Evaluate the model\nscore = classification_report(y_valid, preds)\nf1    = f1_score(y_valid, preds)\nprint(str(model).split('(')[0],': Accurancy:\\n', score)","cd24903d":"perm = PermutationImportance(model, random_state=1).fit(X_valid, y_valid)\neli5.show_weights(perm, feature_names = X_valid.columns.tolist())","4434ae11":"import shap  # package used to calculate Shap values\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(model)\n\n# calculate shap values. This is what we will plot.\n# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\nshap_values = explainer.shap_values(X_valid)\n\n# Make plot. Index of [1] is explained in text below.\nshap.summary_plot(shap_values[1], X_valid)","d3783914":"So we make a age categorie each quartile.","04e4de58":"check collumns with null values","fcb52b2c":"We do some paramter tuning with the RandomForestClassifiert since it scored best","2f6cba27":"At the first glance it seems that smoking, sex and diabets don't have any influece on a Death event. That not what we expected. ","b65f752d":"# EDA\n\nTo get a bedder understunding of feature we do some EDA. ","045eb4a5":"It seems that feature can increase death probabilty at a higher age.","eba7fd52":"# Understundig Features\n\n\nBefore starting to build a model it seems reasonble to make a quick research of feature\n\n## Anaemia\n\nAccording to Wikipedia: Anemia (also spelled anaemia) is a decrease in the total amount of red blood cells (RBCs) or hemoglobin in the blood, or a lowered ability of the blood to carry oxygen.\n\n## creatinine_phosphokinase\n\nThe data describes the Level of the CPK enzyme in the blood (mcg\/L). I found some paper (f.e. https:\/\/pubmed.ncbi.nlm.nih.gov\/22760499\/) that show correlation between level of creatine kinase and heard failer.\n\n\n## diabetes\n\nFrom Wikipedia: Diabetes mellitus (DM), commonly known as diabetes, is a group of metabolic disorders characterized by a high blood sugar level over a prolonged period of time.\n\n## ejection_fraction\n\nFrom Wikipedia: An ejection fraction (EF) is the volumetric fraction (or portion of the total) of fluid (usually blood) ejected from a chamber (usually the heart) with each contraction (or heartbeat).\n\n## high_blood_pressure\n\nFrom Wikipedia: Hypertension (HTN or HT), also known as high blood pressure (HBP), is a long-term medical condition in which the blood pressure in the arteries is persistently elevated\n\n## platelets\n\nFrom Wikipedia: Platelets, also called thrombocytes (from Greek \u03b8\u03c1\u03cc\u03bc\u03b2\u03bf\u03c2, \"clot\" and \u03ba\u03cd\u03c4\u03bf\u03c2, \"cell\"), are a component of blood whose function (along with the coagulation factors) is to react to bleeding from blood vessel injury by clumping, thereby initiating a blood clot.\n\n## serum_creatinine\n\nFrom Wikipedia: Creatinine (\/kri\u02c8\u00e6t\u026an\u026an\/ or \/kri\u02c8\u00e6t\u026ani\u02d0n\/; from Greek: \u03ba\u03c1\u03ad\u03b1\u03c2, romanized: kreas, lit. 'flesh') is a breakdown product of creatine phosphate from muscle and protein metabolism. It is released at a constant rate by the body (depending on muscle mass).\n\n## serum_sodium\n\nSodium ions (Na+) are necessary in small amounts for some types of plants, but sodium as a nutrient is more generally needed in larger amounts by animals, due to their use of it for generation of nerve impulses and for maintenance of electrolyte balance and fluid balance. In animals, sodium ions are necessary for the aforementioned functions and for heart activity and certain metabolic functions.\n\n## time\n\nFollow-up period (days). I think follow-up periode is highly correlate to death, but doesnt really explaine a mortality. \n","8ece917a":"299 rows could be a low number of rows to make a good model for prediction in practice. ","8b81fae2":"The classes seems to be Slighty Imbalance. I am going to adress that later.","32ba46d9":"At the first glance it seems to be a certain correlation between the features and DEATH_EVENT","78fd0fb5":"### Exploring categorical variable with DEATH_EVENT = TRUE and age categorie","bdf9e0d2":"# Models\n\nNow we try to select a model to further improve it.","3ca6ad91":"# Correlation Matrix","bece05e4":"Now let's look at the features\n\n### ejection fraction\n\nPeople with a low value have a higher change to die\n\n### serum creatin\n\nHigh value of serum creatine encrease the probiliy to die\n\n### serum sodium\n\nLow value of serum sodium encrease the probiliy to die\n\n### age\n\nolder people have a higher change to die\n\n### creatinine_phosphokinase\n\n\n\n\nSmoking, gender, diabetes and high blood pressure seem to have littel importance on predicting a DEATH_EVENT which is not intuitiv. Eventuelly there are to littel amount of abservations and feautures to predict a DEATH_EVENT","1a582d3a":"### Exploring categorical variable with DEATH_EVENT = FALSE and age categorie","97ebde8f":"No null values in the dataset","244e794d":"## Explore numeric feature\n","a0df6184":"# Heart failure\n\nAccording to Wikipedia: Heart failure (HF), also known as congestive heart failure (CHF), decompensatio cordis (DC), and congestive cardiac failure (CCF), is when the heart is unable to pump sufficiently to maintain blood flow to meet the body tissue's needs for metabolism.\n\n..\nHeart failure is a common, costly, and potentially fatal condition. In 2015, it affected about 40 million people globally. Overall around 2% of adults have heart failure and in those over the age of 65, this increases to 6\u201310%. Rates are predicted to increase.\n\n..\n\nThe risk of death is about 35% the first year after diagnosis, while by the second year the risk of death is less than 10% for those who remain alive. This degree of risk of death is similar to some cancers. In the United Kingdom, the disease is the reason for 5% of emergency hospital admissions\n\nSo having a model that predict accuratly heart cout save lives. A early ","9b098227":"## Explore categorical feature","6485691a":"Time is highly correlated to a DEATH_EVENT, but we cant use it for our model, because is not usefull when a patient shows up the first team. ","687edeef":"## Permutation Importance\n\nTest importance feature"}}