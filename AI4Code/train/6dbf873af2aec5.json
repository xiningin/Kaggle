{"cell_type":{"50f454ef":"code","11b1fc66":"code","8cb76c28":"code","59b3d83a":"code","d60e6327":"code","27a4f4c7":"code","46198e56":"code","2939c37c":"code","75423352":"code","526c34cc":"code","6f61c84c":"code","e50b0e68":"markdown","6b6d2040":"markdown","523fe637":"markdown","33f2c1e3":"markdown","95f62b78":"markdown","5890fce2":"markdown","580dd7b6":"markdown","e2ce30c5":"markdown","47ff350b":"markdown","6831564f":"markdown","309e8f1f":"markdown","1cb94463":"markdown","05371fbc":"markdown"},"source":{"50f454ef":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n#from joypy import joyplot for matplotlib\nimport tensorflow as tf\nfrom tqdm import tqdm\nimport optuna\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.preprocessing import RobustScaler, normalize, StandardScaler\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold\n\ntqdm.pandas()","11b1fc66":"save_modified_data = True","8cb76c28":"path = '..\/input\/ventilator-pressure-prediction'\ntrain = pd.read_csv(f\"{path}\/train.csv\")\ntest = pd.read_csv(f\"{path}\/test.csv\")\nsubmission = pd.read_csv(f'{path}\/sample_submission.csv')\n\ntrain = train.astype({'time_step': float, 'pressure': float, 'u_in' : float})\ntest = test.astype({'time_step': float, 'u_in' : float})","59b3d83a":"def data_clean(df):\n    ## pickup ignore breath id\n    ignore_breath_ids = set()\n    \n    time_step_diff_limit = 0.04\n    for k, grp in tqdm(df.groupby(\"breath_id\")):\n        \n        ## ignore non liner time_step data\n        diff_se = grp[\"time_step\"].diff()\n        diff_chk = diff_se[diff_se > time_step_diff_limit]\n        if len(diff_chk) != 0:\n            ignore_breath_ids.add(k)\n            \n        ## ignor negative pressure data\n        #mi = grp[\"pressure\"].min()\n        #if mi < 0:\n        #    ignore_breath_ids.add(k)\n            \n        ## ignore len(u_out == 0) =< 28 \n        u_out_0_len = len(grp[grp[\"u_out\"] == 0])\n        if u_out_0_len < 29:\n            ignore_breath_ids.add(k)\n            \n        ## ignore pressure max == 64.8209917386395\n        ma = grp[\"pressure\"].max()\n        if ma == 64.8209917386395:\n            ignore_breath_ids.add(k)\n    \n    df = df[~df[\"breath_id\"].isin(np.array(list(ignore_breath_ids)))]\n    return df\n\ndef change_type(df):\n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    return df\n\ndef add_features(df):\n    df['u_in_cumsum'] = df.groupby('breath_id')[\"u_in\"].cumsum()\n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n    #df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n    #df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n    #df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n    #df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n    df = df.fillna(0)\n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    df = pd.get_dummies(df)\n    return df\n\ndef tf_tpu_or_gpu_or_cpu():\n    tpu = None\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        return \"tpu\"\n\n    elif tf.test.is_gpu_available():\n        strategy = tf.distribute.get_strategy()\n        print('Running on GPU')\n        return \"gpu\"\n\n    else:\n        strategy = tf.distribute.get_strategy()\n        print('Running on CPU')\n        return \"cpu\"","d60e6327":"def u_out_0_df(df):\n    grp_len = int(32)\n    new_df = pd.DataFrame()\n    for k, grp in tqdm(df.groupby(\"breath_id\")):\n        tmp_df = grp[grp[\"u_out\"] == 0]\n        rowno  = tmp_df.shape[0]\n        for i in range(grp_len - rowno):\n            row_df = tmp_df.tail(1).copy()\n            time_diff = tmp_df.tail(2).diff().tail(1)[\"time_step\"]\n            row_df[\"time_step\"] = row_df[\"time_step\"] + time_diff * (i + 1)\n            row_df[\"id\"] = row_df[\"id\"] + int(1)\n            tmp_df = tmp_df.append(row_df,ignore_index=True)\n        new_df = new_df.append(tmp_df,ignore_index=True)\n    return new_df","27a4f4c7":"print(\"**Info : Data clean of train.\")\ntrain = data_clean(train)\nprint(\"**Info : pick up u_out == 0 of train.\")\ntrain = u_out_0_df(train)\ntrain.to_csv(\".\/train_u_out_0.csv\")\nprint(\"**Info : add features of train.\")\ntrain = add_features(train)\n\nprint(\"**Info : pick up u_out == 0 of test.\")\ntest = u_out_0_df(test)\ntest.to_csv(\".\/test_u_out_0.csv\")\nprint(\"**Info : add features of test.\")\ntest = add_features(test)","46198e56":"train.to_csv(\".\/train_mod.csv\")\ntest.to_csv(\".\/test_mod.csv\")","2939c37c":"train = pd.read_csv(f\"{path}\/train.csv\")\ntrain[\"time_step_id\"] = list(range(1,81,1)) * int(len(train)\/80)\nrange_bins = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65,70,75,80,85,90,95,100]\nbins_name = ['~5', '~10', '~15','~20','~25', '~30', '~35', '~40','~45', '~50', '~55', '~60','~65','~70','~75','~80','~85','~90','~95','~100']\ntrain[\"u_in_range\"] = pd.cut(train[\"u_in\"],bins=range_bins,labels=bins_name)\ntmp_df = pd.DataFrame()\nfor k,grp in train.groupby(\"time_step_id\"):\n    tmp_df = tmp_df.append(grp[\"u_in_range\"].value_counts())\ntmp_df = tmp_df.reset_index(drop=True)\ntmp_df.columns = tmp_df.columns.astype(str)\ntmp_df = tmp_df.reindex(columns=bins_name)\nax = sns.heatmap(tmp_df)\nax.set_xlabel(\"u_in_range\", fontsize = 20)\nax.set_ylabel(\"time_step_id\", fontsize = 20)\nplt.show()","75423352":"train = pd.read_csv(f\"{path}\/train.csv\")\ntrain[\"time_step_id\"] = list(range(1,81,1)) * int(len(train)\/80)\nrange_bins = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65]\nbins_name = ['~5', '~10', '~15','~20','~25', '~30', '~35', '~40','~45', '~50', '~55', '~60', '~65']\ntrain[\"pressure_range\"] = pd.cut(train[\"pressure\"],bins=range_bins,labels=bins_name)\ntmp_df = pd.DataFrame()\nfor k,grp in train.groupby(\"time_step_id\"):\n    tmp_df = tmp_df.append(grp[\"pressure_range\"].value_counts())\ntmp_df = tmp_df.reset_index(drop=True)\ntmp_df.columns = tmp_df.columns.astype(str)\ntmp_df = tmp_df.reindex(columns=bins_name)\nax = sns.heatmap(tmp_df)\nax.set_xlabel(\"pressure_range\", fontsize = 20)\nax.set_ylabel(\"time_step_id\", fontsize = 20)\nplt.show()","526c34cc":"train = pd.read_csv(f\"{path}\/train.csv\")\ntrain = u_out_0_df(train)\ncolumns = train.columns\n#scaler = RobustScaler()\nscaler = StandardScaler()\ntrain = scaler.fit_transform(train)\ntrain = pd.DataFrame(train,columns= columns)\ntrain[\"time_step_id\"] = list(range(1,33,1)) * int(len(train)\/32)\nrange_bins = [-1.8, -1.6, -1.4, -1.2, -1.0, -0.8, -0.6, -0.4,-0.2,0,0.2,0.4,0.6,0.8,1.0,1.2,1.4,1.6,1.8]\nbins_name = ['~-1.6', '~-1.4', '~-1.2','~-1.0','~-0.8','~-0.6','~-0.4','~-0.2','~0','~0.2','~0.4','~0.6','~0.8','~1.0','~1.2','~1.4','~1.6','~1.8']\n#train[\"pressure_range\"] = pd.cut(train[\"pressur\"],bins=range_bins,labels=bins_name)\ntrain[\"u_in_range\"] = pd.cut(train[\"u_in\"],bins=range_bins,labels=bins_name)\ntmp_df = pd.DataFrame()\nfor k,grp in train.groupby(\"time_step_id\"):\n    tmp_df = tmp_df.append(grp[\"u_in_range\"].value_counts())\ntmp_df = tmp_df.reset_index(drop=True)\ntmp_df.columns = tmp_df.columns.astype(str)\ntmp_df = tmp_df.reindex(columns=bins_name)\nax = sns.heatmap(tmp_df)\nax.set_xlabel(\"u_in_range\", fontsize = 20)\nax.set_ylabel(\"time_step_id\", fontsize = 20)\nplt.show()","6f61c84c":"train = pd.read_csv(f\"{path}\/train.csv\")\ntrain = u_out_0_df(train)\ncolumns = train.columns\nscaler = RobustScaler()\n#scaler = StandardScaler()\ntrain = scaler.fit_transform(train)\ntrain = pd.DataFrame(train,columns= columns)\ntrain[\"time_step_id\"] = list(range(1,33,1)) * int(len(train)\/32)\nrange_bins = [-0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2,-0.1,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nrange_bins = [-9, -8, -7, -6, -5, -4, -3, -2,-1,0,1,2,3,4,5,6,7,8,9,10]\nbins_name = ['~-8', '~-7','~-6','~-5','~-4','~-3','~-2','~-1','~0','~1','~2','~3','~4','~5','~6','~7','~8','~9','10']\n#train[\"pressure_range\"] = pd.cut(train[\"pressure\"],bins=range_bins,labels=bins_name)\ntrain[\"u_in_range\"] = pd.cut(train[\"u_in\"],bins=range_bins,labels=bins_name)\ntmp_df = pd.DataFrame()\nfor k,grp in train.groupby(\"time_step_id\"):\n    #tmp_df = tmp_df.append(grp[\"pressure_range\"].value_counts())\n    tmp_df = tmp_df.append(grp[\"u_in_range\"].value_counts())\ntmp_df = tmp_df.reset_index(drop=True)\ntmp_df.columns = tmp_df.columns.astype(str)\ntmp_df = tmp_df.reindex(columns=bins_name)\nax = sns.heatmap(tmp_df)\nax.set_xlabel(\"u_in_range\", fontsize = 20)\nax.set_ylabel(\"time_step_id\", fontsize = 20)\nplt.show()","e50b0e68":"# Read data","6b6d2040":"# apply utilitys for data","523fe637":"# Try and Error","33f2c1e3":"# Settings","95f62b78":"# Visualize u_in hist with StandardScaler() for each time step id","5890fce2":"# Visualize original data u_in histgram for each time step id","580dd7b6":"# Visualize original data pressure histgram for each time step id","e2ce30c5":"# Specify library","47ff350b":"# Visualize u_in hist with RobustScaler() for each time step id","6831564f":"# reference\n\n","309e8f1f":"# pickup u_out == 0","1cb94463":"# Save modified train\/test data","05371fbc":"# Utilitys"}}