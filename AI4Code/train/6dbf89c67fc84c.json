{"cell_type":{"99eaf12e":"code","d73af95a":"code","03e8f7a5":"code","5b24b9a0":"code","2b6be490":"code","6a70746f":"code","83b803fd":"code","415c3285":"code","a628e778":"code","a5b45b7a":"code","747052c5":"code","25fbb898":"code","7a5b0a9a":"code","73f7206b":"code","82858ee7":"code","d00fd4a5":"code","001c23ea":"code","5d939505":"code","0f9e7888":"code","b92b17f2":"code","34ee1c6d":"code","a975c07c":"code","f6b36330":"code","f0cbad57":"code","e197e300":"code","2dac0106":"code","a2f39931":"code","06ee162e":"code","9dd589ce":"code","50577058":"code","1feeeba4":"code","d96d85f7":"code","d6f07cc2":"code","2efc1b46":"code","b3e351ba":"code","59065f63":"code","e474b007":"code","2d718e5d":"code","1f955152":"markdown","3e7a652e":"markdown","55d838ef":"markdown","e1f505b9":"markdown","38340778":"markdown","275db82d":"markdown","7e7fd6c0":"markdown","7f294137":"markdown","9462efe3":"markdown","19a992a4":"markdown","7c7ae9ed":"markdown","92a7b0c2":"markdown","6c48a2f0":"markdown","edad5b3f":"markdown","b7b7026a":"markdown","83c914e2":"markdown","6caeac26":"markdown","f85f327d":"markdown"},"source":{"99eaf12e":"# an\u00e1lise e transforma\u00e7\u00e3o dos dados\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualiza\u00e7\u00e3o\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.base import clone ","d73af95a":"# importando os datasets\n\nimport os\n\ntrain_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nall_data = train_df.append(test_df, sort=True);","03e8f7a5":"train_df.head(8)","5b24b9a0":"train_df.describe()","2b6be490":"train_df.describe(include=['O'])","6a70746f":"# N\u00famero de \"NaN\"s em cada feature\nprint(all_data.shape)\nall_data.isnull().sum().drop('Survived')","83b803fd":"# Extraindo os t\u00edtulos de \"Name\"\nall_data['Title'] = all_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(all_data['Title'], all_data['Sex']).T","415c3285":"# Transformando a feature \"Cabin\" em bin\u00e1ria\nall_data['Cabin'].fillna(0, inplace=True)\nall_data.loc[all_data['Cabin'] != 0, 'Cabin'] = 1","a628e778":"# Descartando dados irrelevantes\n\nId_test = test_df['PassengerId'] # necess\u00e1rio para submiss\u00e3o do projeto.\n\nall_data = all_data.drop(['PassengerId', 'Ticket', 'Name'], axis=1)","a5b45b7a":"# Criando uma vari\u00e1vel rand\u00f4mica para testar algumas hip\u00f3teses\n\nnp.random.seed(2020)\nall_data['random'] = np.random.randint(0,2,len(all_data))\n\n#Split\ntrain_df = all_data[:len(train_df)]\ntest_df = all_data[len(train_df):]","747052c5":"def pivota_feature_com_sobrev(feature_analisada):\n    \n    #Essa fun\u00e7\u00e3o cria um pequeno DataFrame com a taxa de sobreviv\u00eancia \n    #e o n\u00famero de indiv\u00edduos de cada elemento de uma feature.\n    \n    df_pivot = pd.concat([train_df[feature_analisada].rename('# ind').value_counts(), \n                          train_df[[feature_analisada, \"Survived\"]].groupby(feature_analisada, as_index=True).mean()], \n                         axis=1, sort=True)\n    \n    df_pivot.index.name = feature_analisada\n    \n    return round(df_pivot, 3)","25fbb898":"display(pivota_feature_com_sobrev('Sex'), \n        pivota_feature_com_sobrev('Embarked'), \n        pivota_feature_com_sobrev('Pclass'),\n        pivota_feature_com_sobrev('Cabin'))","7a5b0a9a":"# Convertendo a feature \"sex\" de nominal para bin\u00e1ria\nall_data['Sex'].replace(['female','male'], [0, 1],inplace=True) \n\n# Preenchendo 2 valores nulos com o porto de embarque mais comum\nall_data['Embarked'].fillna('S', inplace=True)\n\n# Convertendo a feature \"Embarked\" de nominal para discreta\nall_data['Embarked'].replace(['S', 'Q', 'C'], [0, 1, 2],inplace=True)","73f7206b":"display(pivota_feature_com_sobrev('Title').T, \n        pivota_feature_com_sobrev('Parch').T, \n        pivota_feature_com_sobrev('SibSp').T)","82858ee7":"# Transformando a feature \"Title\"\nall_data[\"Title\"] = all_data['Title'].replace(['Mrs', 'Miss', 'Mr'], 0)\nall_data.loc[all_data['Title'] != 0, 'Title'] = 1\n\n# Criando a feature \"Family\"\nall_data['Family'] =  all_data[\"Parch\"] + all_data[\"SibSp\"]\nall_data.loc[all_data['Family'] > 0, 'Family'] = 1\n\n# Descartando\nall_data.drop(['SibSp', 'Parch'], axis=1, inplace=True)\n\n#Split\ntrain_df = all_data[:len(train_df)]\ntest_df = all_data[len(train_df):]","d00fd4a5":"display(pivota_feature_com_sobrev('Title'), pivota_feature_com_sobrev('Family'))","001c23ea":"# Checando a feature rand\u00f4mica\npivota_feature_com_sobrev('random')","5d939505":"# Split\ntrain_df = all_data[:len(train_df)]\ntest_df = all_data[len(train_df):]\n\ntrain_df.corr().style.background_gradient(cmap='Blues').set_precision(2)","0f9e7888":"kde_age= sns.FacetGrid(train_df, col='Pclass', row='Sex', hue='Survived')\nkde_age.add_legend().set(xlim=(0, 100))\nkde_age = kde_age.map(sns.kdeplot, 'Age', shade=True)","b92b17f2":"plt.figure(figsize = (10, 7))\nplt.title('Idade em fun\u00e7\u00e3o da classe e se o passageiro viajou com a familia')\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Family', data = all_data, split = True, inner=\"quartile\")\nplt.show()","34ee1c6d":"matriz_de_medianas = np.zeros((2,3))\n\nfor classe in range(1, 4):\n    for familia in range(0, 2):\n        matriz_de_medianas[familia, classe - 1] = all_data.loc[(all_data['Pclass'] == classe) & (all_data['Family'] == familia)]['Age'].median()\n        \n        all_data.loc[(np.isnan(all_data['Age'])) \n                     & (all_data['Family'] == familia) \n                     & (all_data['Pclass'] == classe), 'Age'] = matriz_de_medianas[familia, classe - 1]\n\nmedianas = pd.DataFrame(matriz_de_medianas, columns=[1, 2, 3]); medianas.index.name = 'Family'; medianas.columns.name = 'Pclass'\nmedianas","a975c07c":"# O mesmo \u00e9 realizado para preencher o \u00fanico valor nulo da feature \"Fare\":\nall_data.loc[np.isnan(all_data['Fare'])]","f6b36330":"# Preenchendo o valor nulo com a mediana das tarifas deste grupo de indiv\u00edduos\nall_data.loc[np.isnan(all_data['Fare'])] = all_data.loc[(all_data['Pclass'] == 3) & (all_data['Sex'] == 1) & (all_data['Family'] == 0)]['Fare'].median()","f0cbad57":"# Nossos dados est\u00e3o finalmente organizados, limpos e transformados:\nall_data.head()","e197e300":"# Split\ntrain_df = all_data[:len(train_df)]\ntest_df = all_data[len(train_df):]\n\ntrain_data = train_df.drop(\"Survived\", axis=1)\ntrain_target = train_df[\"Survived\"]\nX_test  = test_df.drop(\"Survived\", axis=1)\ntrain_data.shape, train_target.shape, X_test.shape","2dac0106":"def treina_e_testa_modelo(modelo, dados_de_treino, rotulo, cross_validation_folders):\n    \n    modelo.fit(dados_de_treino,rotulo)\n    \n    score = cross_val_score(modelo,\n                            dados_de_treino,\n                            rotulo,\n                            cv=cross_validation_folders).mean()\n    \n    model_name = str(modelo).split('(')[0]\n    \n    print(model_name + ' accuracy: '+str(round(score.mean() * 100, 2))+'%')\n    \n    return score","a2f39931":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn_score = treina_e_testa_modelo(knn, train_data, train_target, 5)","06ee162e":"random_forest = RandomForestClassifier(n_estimators=100, random_state=2020)\nrandom_forest_score = treina_e_testa_modelo(random_forest, train_data, train_target, 5)","9dd589ce":"logreg = LogisticRegression(solver='newton-cg', random_state=2020)\nlogreg_score = treina_e_testa_modelo(logreg, train_data, train_target, 5)","50577058":"svc = SVC(gamma='scale', random_state=2020)\nsvc_score = treina_e_testa_modelo(svc, train_data, train_target, 5)","1feeeba4":"def importance_plot(x_name,y_name, dados, graph_title):\n    sns.barplot(x = x_name, y = y_name, data = dados, orient = 'h', color = 'royalblue').set_title(graph_title, fontsize = 20);\n    plt.show()","d96d85f7":"feature_importance_df = pd.DataFrame({'Feature': train_data.columns, 'feature_importance': random_forest.feature_importances_})\\\n                          .sort_values('feature_importance', ascending = False)\n\ncoeff_df = pd.DataFrame({'Feature': train_data.columns, 'feature correlation': pd.Series(logreg.coef_[0])})\\\n             .sort_values('feature correlation', ascending = False)\n\nimportance_plot('feature_importance', 'Feature', feature_importance_df, 'RForest feature importance_defaut')\nimportance_plot('feature correlation', 'Feature', coeff_df, 'logistic regression feature correlation')","d6f07cc2":"rforest_clone = clone(random_forest)\nlogreg_clone = clone(logreg)\nrf_impact = [] #rf = random forest\nlr_impact = [] #lr = logistic regression\n\nfor feature in train_data.columns:\n    \n    rf_new_score = cross_val_score(rforest_clone, train_data.drop(feature, axis=1), train_target, cv=5).mean()\n    lr_new_score = cross_val_score(logreg_clone, train_data.drop(feature, axis=1), train_target, cv=5).mean()\n    \n    rf_impact.append(random_forest_score - rf_new_score)\n    lr_impact.append(logreg_score - lr_new_score)\n\nrforest_impact = pd.DataFrame(data={'Feature': train_data.columns, 'Impact': rf_impact}).sort_values('Impact', ascending = False)\nlogreg_impact = pd.DataFrame(data={'Feature': train_data.columns, 'Impact': lr_impact}).sort_values('Impact', ascending = False)","2efc1b46":"importance_plot('Impact', 'Feature', rforest_impact, 'Random Forest feature impact')\nimportance_plot('Impact', 'Feature', logreg_impact, 'Logistic Regression feature impact')","b3e351ba":"random_forest_score = treina_e_testa_modelo(random_forest, train_data.drop(['Title', 'random', 'Family'], axis=1), train_target, 10)","59065f63":"logreg_forest_score = treina_e_testa_modelo(logreg, train_data.drop(['Cabin', 'random', 'Fare'], axis=1), train_target, 10)","e474b007":"predict = logreg.predict(X_test.drop(['Cabin', 'random', 'Fare'], axis=1)).astype(int)\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = Id_test\n#get predictions\nsubmission['Survived'] = predict\nsubmission.head(15).T","2d718e5d":"submission.to_csv('logreg_submission.csv', index=False)","1f955152":"**<font size=\"5\">Compreendendo os resultados<\/font>**\n\nRandom forest e logistic regression foram os modelos mais assertivos, com aproximadamente 80% de acur\u00e1cia, o pr\u00f3ximo passo \u00e9 utilizar alguns m\u00e9todos para tentar compreender quais s\u00e3o as features mais importantes para cada um e entender seu funcionamento.","3e7a652e":"Uma possibilidade para completar as informa\u00e7\u00f5es de idade \u00e9 preench\u00ea-las com a mediana das idades, mas este valor pode variar em fun\u00e7\u00e3o de diferentes grupos de pessoas. Vale checar as features \"Family\" e \"Pclass\", que s\u00e3o bem correlacionadas com \"Age\":","55d838ef":"**<font size=\"5\">Eliminando features e submetendo predi\u00e7\u00f5es<\/font>**","e1f505b9":"**<font size=\"5\">Importando dados e bibliotecas necess\u00e1rias<\/font>**","38340778":"**<font size=\"5\">Treino com os dados do Titanic<\/font>**\n\nUm dos naufr\u00e1gios mais infames da hist\u00f3ria \u00e9 o do Titanic, que afundou ap\u00f3s colidir com um iceberg. Infelizmente, n\u00e3o havia botes salva-vidas suficientes para todos a bordo, resultando na morte de 1502 dos 2224 passageiros e tripulantes.\n\nEmbora houvesse algum elemento de sorte envolvido na sobreviv\u00eancia, parece que alguns grupos de pessoas eram mais propensos a sobreviver do que outros. **O desafio aqui \u00e9 construir um modelo preditivo que discrimine os grupos de pessoas com maiores chances de sobreviver.**\n\n**<font size=\"3\">Principais objetivos<\/font>**: \n- Me familiarizar com a plataforma Kaggle e suas competi\u00e7\u00f5es.\n- Desenvolver e colocar em pr\u00e1tica t\u00e9cnicas de EDA\n- Resolver o problema e buscar melhorar a pontua\u00e7\u00e3o baseando-se em metodos e ideias que vi em notebooks compartilhados por cientistas de dados da comunidade mais experientes.","275db82d":"**<font size=\"5\">Checando correla\u00e7\u00f5es com a sobreviv\u00eancia<\/font>**","7e7fd6c0":"Todas estas features aparentam impactar nas chances de sobreviv\u00eancia; devem ser preenchidas e convertidas para vari\u00e1veis num\u00e9ricas.","7f294137":"Nota-se que grande parte dos indiv\u00edduos que tiveram companhia para a viagem ou T\u00edtulos raros tiveram mais de 50% de chance de sobreviver. Entretanto muitos desses elementos n\u00e3o possuem um n\u00famero de indiv\u00edduos alto o suficiente para serem representativos do todo, transformar esta feature em uma vari\u00e1vel ordinal pode incorrer em problemas de amostragem para determinados valores, fazendo mais sentido criar as seguintes vari\u00e1veis bin\u00e1rias: \n\n- \"Family\": 0 para indiv\u00edduos sozinhos e 1 para acompanhados.\n- \"Title\": 1 para indiv\u00edduos com t\u00edtulos raros e 0 para t\u00edtulos comuns\n\n","9462efe3":"**<font size=\"3\">Observa\u00e7\u00f5es<\/font>**:\n- Para o algoritmo de regress\u00e3o log\u00edstica a feature 'random' foi apontada como mais correlacionada com a sobreviv\u00eancia do que a idade e a tarifa, sabemos que isso n\u00e3o faz sentido, mas j\u00e1 era esperado pelo fato de que este modelo n\u00e3o lida com vari\u00e1veis cont\u00ednuas como estas.\n\n- J\u00e1 para o random forest, notamos que 'random' \u00e9 apontada como mais importante que a fam\u00edlia e o t\u00edtulo, o que tamb\u00e9m n\u00e3o faz muito sentido. Frente a isso existem duas possibilidade; estas tr\u00eas features n\u00e3o contribuem muito para o modelo e devem ser descartadas, ou o m\u00e9todo \".feature_importances_\" n\u00e3o \u00e9 muito acurado. Este \u00faltimo ponto \u00e9 confirmado pela literatura, que o aponta como enviesado, pois infla o impacto de features cont\u00ednuas e de alta cardinalidade.\n\n- Uma possibilidade \u00e9 testar m\u00e9todos recursivos, que s\u00e3o muito mais assertivos e computacionalmente caros, o que n\u00e3o representa um problema para um conjunto de dados pequeno como este.","19a992a4":"**Boa parte dos insights mais relevantes para a resolu\u00e7\u00e3o do problema n\u00e3o partiram de mim. Tenho muito a agradecer pelo trabalho de v\u00e1rios DSs mais experientes da comunidade que compartilharam suas ideias, principalmente atrav\u00e9s destes notebooks e links:**\n\n- https:\/\/www.kaggle.com\/alexisbcook\/titanic-tutorial\n\n- https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n\n- https:\/\/www.kaggle.com\/omarelgabry\/a-journey-through-titanic\n\n- https:\/\/www.kaggle.com\/tuckerarrants\/titanic-ml-top-10\n\n- https:\/\/towardsdatascience.com\/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e","7c7ae9ed":"**<font size=\"5\">Preenchendo missing values<\/font>**\n\nAssim como esperado, diferentes grupos possuem diferentes distribui\u00e7\u00f5es de idades, e uma vez que possu\u00edmos estas informa\u00e7\u00f5es, \u00e9 melhor fazer o preenchimento de forma condicionada:","92a7b0c2":"**<font size=\"5\">An\u00e1lise explorat\u00f3ria<\/font>**","6c48a2f0":"A correla\u00e7\u00e3o entre a idade e a sobreviv\u00eancia \u00e9 baixa (-0,07), entretanto, se analisarmos as curvas de kde encontramos faixas et\u00e1rias mais propensas a sobreviverem. Isso sugere que a feature \u00e9 relevante para o modelo e deve ser completada.","edad5b3f":"**<font size=\"3\">Observa\u00e7\u00f5es<\/font>**:\n- Cinco features s\u00e3o nominais, das quais apenas \"Sex\" e \"Embarked\" possuem poucos valores \u00fanicos e podem ser facilmente convertidas em vari\u00e1veis num\u00e9ricas ordinais\/discretas.\n\n- Propor\u00e7\u00e3o de \"Missing values\" encontrados: \"Cabin\" 77%  >>> \"Age\" 20% >> \"Embarked\" apenas 2 > \"Fare\" apenas 1.\n\n- A chance de sobreviv\u00eancia m\u00e9dia do conjunto de dados \u00e9 de 38% e est\u00e1 pr\u00f3xima da taxa real de 32% (1-1502\/2224). Tamb\u00e9m pode ser usado como par\u00e2metro de compara\u00e7\u00e3o para o impacto de determinadas features.\n\n**<font size=\"3\">Especula\u00e7\u00f5es<\/font>**:\n- PassangerId, Ticket e Name provavelmente n\u00e3o contribuem para a sobreviv\u00eancia, descart\u00e1-los logo no in\u00edcio \u00e9 conveniente pois reduz o volume de dados, acelera o processamento do c\u00f3digo e simplifica a an\u00e1lise, mas vale ressaltar que o nome tamb\u00e9m possui informa\u00e7\u00f5es sobre o t\u00edtulo do passageiro, extrair isso em uma nova feature pode beneficiar a acur\u00e1cia do modelo.\n\n- Uma possibilidade para haver tantos valores nulos para a cabine \u00e9 que isso pode representar uma lux\u00faria de alguns poucos passageiros, possivelmente um indicador de sua influ\u00eancia. Transform\u00e1-la em uma vari\u00e1vel bin\u00e1ria pode beneficiar o modelo. \n\n- \u00c9 importante saber quais features se correlacionam com a sobreviv\u00eancia e entre s\u00ed logo no in\u00edcio do projeto, pois isso guia a tomada de decis\u00e3o sobre quais delas manter e transformar; alguns dados como o valor da tarifa (Fare) e classe (Pclass) podem acabar dizendo a mesma coisa ficando redundantes\n\n- Se queremos fazer correla\u00e7\u00f5es logo de in\u00edcio, \u00e9 importante converter features potencialmente relevantes como  \"Sex\" e \"Embarked\" para vari\u00e1veis ordinais\/discretas antes.\n\n- Por fim, se a an\u00e1lise apontar que features incompletas como \"Age\" e \"Embarked\" impactam na sobreviv\u00eancia, devem ser completadas ao inv\u00e9s de descartadas.","b7b7026a":"- M\u00e9todos recursivos s\u00e3o mais acurados e permitem analisar com mais clareza se determinadas features s\u00e3o relevantes para o modelo.\n\n- Para o random forest, fare e age parecem menos relevantes do que o antigo m\u00e9todo sugeria e as features \"Title\", \"random\" e \"Family\" contribuem muito pouco ou at\u00e9 o atrapalham, remov\u00ea-las pode beneficiar o modelo deixando-o mais r\u00e1pido e evitando overfiting.\n\n- O mesmo vale para a regress\u00e3o log\u00edstica, mas em rela\u00e7\u00e3o as features \"Cabin\", \"random\" e \"Fare\".","83c914e2":"Como esperado, esta feature n\u00e3o ajuda a discriminar quem tem as melhores chances de sobreviv\u00eancia, mas a manteremos para futuras compara\u00e7\u00f5es","6caeac26":"**<font size=\"5\">Modelagem e predi\u00e7\u00e3o<\/font>**","f85f327d":"**<font size=\"5\">Matriz de correla\u00e7\u00f5es<\/font>**"}}