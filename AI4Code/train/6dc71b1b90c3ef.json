{"cell_type":{"84b24feb":"code","a45bdaad":"code","28ce2244":"code","8b5faa39":"code","74ee6691":"code","e678f1e8":"code","3d4a8acc":"code","cd50c059":"code","d87de0f4":"code","6cc09210":"code","7c538906":"code","ab2acf85":"code","2877ca20":"code","90ed486b":"code","a9dae3f7":"code","1db2569c":"code","db219c39":"code","4ded82bd":"code","55449bd9":"code","d3b0edb4":"code","2ab33a04":"code","34de25f0":"code","c662b73b":"code","520afc54":"code","7725989c":"code","6c5eef71":"code","f6ca4570":"code","ff359409":"code","a621fc5f":"code","ae770d2a":"code","3bdff9e9":"code","16981a6e":"code","3234b24f":"code","86fd1d99":"code","2a63efd7":"code","38a8da5a":"code","f8018848":"code","59e85de8":"code","e38fb16b":"code","c031b0e5":"code","93e24b48":"code","5c7dd695":"code","913f1652":"code","3dc0c9d2":"code","cf782283":"code","5cb6f595":"code","11882faf":"code","45ac13c5":"code","7ca45edf":"code","c7416f27":"code","274a5fe5":"code","3e2323df":"code","b20a9736":"code","88460af5":"code","c4aea327":"code","8d02196f":"code","a6dd9a3e":"code","4a02ebc2":"code","fceed0ac":"code","dd801e30":"code","3fadad13":"code","11ebb9e1":"code","1dee7e8b":"code","242cc47f":"code","dd0eec9b":"code","b4df9c08":"code","84d4502f":"code","6101b85d":"code","2a608e73":"code","0f3c52c1":"code","0209a20e":"code","c154a131":"code","c0bb700e":"code","05275899":"code","dcb82fb0":"code","6a6d279d":"code","fc38afb2":"code","82b5a05d":"markdown","cb426c89":"markdown","fdaaa752":"markdown","39a52710":"markdown","1742a58a":"markdown","0936cc31":"markdown","61414b07":"markdown","12a4f509":"markdown","3cfb4ff1":"markdown","feb1f90e":"markdown","93a67dbd":"markdown","a62176b9":"markdown","8568d37d":"markdown","6bde17c9":"markdown","2f659d58":"markdown","22c38a49":"markdown","871c6553":"markdown","425002e3":"markdown","2d50f2d0":"markdown","2e6e318c":"markdown","ce24238d":"markdown","1cc2e153":"markdown","e802f905":"markdown","9f252a72":"markdown","a93ea808":"markdown","b18ba606":"markdown"},"source":{"84b24feb":"import numpy as np\nimport pandas as pd\nimport json\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches as patches\nimport cv2\n\nimport os\nimport shutil as sh\n\nfrom pathlib import Path\nimport re\nfrom tqdm import tqdm\n\nimport torch\n\nimport warnings \nwarnings.simplefilter(\"ignore\")\n\nimport seaborn as sns\nimport sys\nsys.path.insert(0, \"..\/input\/weightedboxesfusion\")\nfrom ensemble_boxes import *\n\n\nfrom PIL import Image\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport torchvision\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\n\nfrom sklearn.model_selection import train_test_split\n\nimport io\nimport base64\n\nimport time\nfrom IPython.display import clear_output\nfrom IPython.display import HTML\n\n\ncustom_colors = ['#35FCFF', '#FF355A', '#96C503', '#C5035B', '#28B463', '#35FFAF', '#8000FF', '#F400FF']\nhex2rgb = lambda hx: (int(hx[1:3],16),int(hx[3:5],16),int(hx[5:7],16))\nrgbcolors = list(map(hex2rgb, custom_colors))[1:]\nsns.palplot(sns.color_palette(custom_colors))","a45bdaad":"# install any fonts\n!wget -O bitwise.zip https:\/\/www.1001freefonts.com\/d\/8190\/bitwise.zip\n!unzip bitwise.zip","28ce2244":"def plot_data_example(df: pd.DataFrame,\n                      root_dir: str,\n                      img_path: str, \n                      colors: dict):\n\n    image = cv2.imread(os.path.join(root_dir, img_path)).astype(\"uint8\")\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    boxes = np.zeros(image.shape, dtype=\"uint8\")\n\n    fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(25, 8))\n    ax[1].imshow(image, aspect=\"auto\")\n    ax[1].set_title(\"Original Image\", fontsize=22, fontweight='bold', y=1.07)\n    ax[0].set_title(\"Image with Bounding Boxes\", fontsize=22, fontweight='bold', y=1.07)\n\n\n    bb_info = df.loc[df[\"pathname\"] == img_path, [\"x_min\", \"x_max\", \"y_min\", \"y_max\", \"category\"]].values\n    for i_bb in bb_info:\n\n        cmin, cmax, rmin, rmax = i_bb[:-1].astype('int')\n        label = i_bb[-1]\n        bbox = patches.Rectangle((cmin,rmin),cmax-cmin,rmax-rmin,linewidth=1, \n                                 edgecolor=label2hex[label], facecolor='none')\n        boxes[rmin:rmax, cmin:cmax] = label2rgb[label]\n\n        ax[0].add_patch(bbox)\n        ax[0].text(cmin, rmin, label, bbox=dict(fill=True, color=label2hex[label]))\n        ax[0].imshow(image, aspect=\"auto\")\n        ax[0].imshow(boxes,  alpha=0.3, aspect=\"auto\")\n        ax[0].text(cmin, rmin, label, bbox=dict(fill=True, color=label2hex[label]))\n\n    plt.tight_layout()\n    fig.savefig(\"data_exemple1.svg\", format='svg', bbox_inches='tight', pad_inches=0.2)\n    fig.savefig(\"data_exemple1.png\", format='png', bbox_inches='tight', pad_inches=0.2)\n    plt.show()\n\n\ndef plot_image_with_bb(df: pd.DataFrame,\n                       root_dir: str,\n                       img_path: str,\n                       path_to_save: str, \n                       dpi: float=100, \n                       write: bool=True):\n    \n    image = cv2.imread(os.path.join(root_dir, img_path)).astype(\"uint8\")\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    boxes = np.zeros(image.shape, dtype=\"uint8\")\n    h, w, _ = image.shape\n    figsize = (w \/ dpi), (h \/ dpi)\n\n    fig, ax = plt.subplots(sharey=True, figsize=(figsize))\n    fig.add_axes([0, 0, 1, 1])\n \n    bb_info = df.loc[df[\"pathname\"] == img_path, [\"x_min\", \"x_max\", \"y_min\", \"y_max\", \"category\"]].values\n    for i_bb in bb_info:\n\n        cmin, cmax, rmin, rmax = i_bb[:-1].astype('int')\n        label = i_bb[-1]\n        bbox = patches.Rectangle((cmin,rmin),cmax-cmin,rmax-rmin,linewidth=1, \n                                 edgecolor=label2hex[label], facecolor='none')\n        boxes[rmin:rmax, cmin:cmax] = label2rgb[label]\n        ax = plt.gca()\n        ax.add_patch(bbox)\n        plt.text(cmin, rmin, label, bbox=dict(fill=True, color=label2hex[label]))\n        plt.imshow(image, aspect=\"auto\")\n        plt.imshow(boxes,  alpha=0.3, aspect=\"auto\")\n        plt.text(cmin, rmin, label, bbox=dict(fill=True, color=label2hex[label]))\n\n    plt.tight_layout()\n    #plt.axis(\"off\")                                            ## dont work in kaggle kernel\n    fig.axes[0].get_xaxis().set_visible(False)                  ## work in kaggle kernel\n    fig.axes[0].get_yaxis().set_visible(False)\n\n    if write:\n        img_name = img_path.replace('\/', '.').split('.')[-2]\n        #fig.savefig(f\"{path_to_save}\/{img_name}.svg\", format='svg', bbox_inches='tight', pad_inches=0.2)\n        fig.savefig(f\"{path_to_save}\/{img_name}.png\", format='png', bbox_inches='tight', pad_inches=0.0)\n        plt.close()\n    else:\n        plt.show()\n  \n\ndef plot_sample(image: torch.Tensor,\n                boxes: torch.Tensor, \n                labels: torch.Tensor):\n    boxes = boxes.cpu().numpy().astype(np.int32)\n    labels = labels.cpu().numpy().astype(np.int32)\n    image = image.permute(1,2,0).cpu().numpy()\n    image = (image * 255).astype(np.uint8)\n    masks = np.zeros_like(image)\n\n    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\n    for label, box in zip(labels, boxes):\n        color = label2rgb[label]\n        text = label2category[label]\n        cmin, rmin, cmax, rmax = box\n\n        cv2.rectangle(image,\n                    (cmin, rmin),\n                    (cmax, rmax),\n                    color=color, thickness=3,)\n        cv2.putText(image, text, (cmin, rmin), cv2.FONT_HERSHEY_SIMPLEX, 1,  \n                    color, 3, cv2.LINE_AA, False,) \n        \n        masks[rmin:rmax, cmin:cmax] = color\n\n    image_with_masks = cv2.addWeighted(image, 0.7, masks, 0.7, 0)\n    ax.set_axis_off()\n    ax.imshow(image_with_masks)\n\n    \ndef create_video(path_to_imgs: str, video_name: str, framerate: int):\n    \"\"\"\n    Create video from images.\n    Params:\n        path_to_imgs: path to dir with images.\n        video_name: name for saving video.\n        framerate: num frames per sec in video.\n    \"\"\"\n    frame_width, frame_height = 1600, 1200\n    img_names = sorted(os.listdir(path_to_imgs))\n    img_path = os.path.join(path_to_imgs, img_names[0])\n\n    fourc = cv2.VideoWriter_fourcc(*'XVID') \n    video = cv2.VideoWriter(video_name + \".avi\", \n                            fourc, \n                            framerate, \n                            (frame_width, frame_height))\n\n    for img_name in img_names:\n        img_path = os.path.join(path_to_imgs, img_name)\n        image = cv2.imread(img_path)\n        image = cv2.resize(image, (frame_width, frame_height))\n        video.write(image)\n            \n    cv2.destroyAllWindows()\n    video.release()\n    \ndef show_video(video_path: str):\n    \"\"\"\n    show video in jupyter notebook, agent interaction in environment.\n    Takes - path to video file.\n    Returns - html video player in jupyter notebook.\n    \"\"\"  \n    video = io.open(video_path, 'r+b').read()\n    encoded = base64.b64encode(video)\n\n    return HTML(data='''<video alt=\"test\" width=\"800\" height=\"400\" controls> \n    <source src=\"data:video\/mp4;base64,{0}\" type=\"video\/mp4\" \/\"> <\/video>'''\n    .format(encoded.decode('ascii')))","8b5faa39":"# Global veriables\n\nclass GlobalConfig:\n        root_dir = '..\/input\/malaria-bounding-boxes\/malaria\/'\n        train_json_path = '..\/input\/malaria-bounding-boxes\/malaria\/training.json'\n        test_json_path = '..\/input\/malaria-bounding-boxes\/malaria\/test.json'\n        yolo_weights = '..\/input\/malaria\/yolov5x_malaria.pt'\n        fasterrcnn_weights = '..\/input\/malaria\/fasterrcnn_resnet50_fpn.pth'\n        seed = 33\n\ndef seed_everything(seed: int):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    #torch.cuda.manual_seed(seed)\n\nconfig = GlobalConfig()\nseed_everything(config.seed)","74ee6691":"train_df = pd.read_json(config.train_json_path)\ntest_df = pd.read_json(config.test_json_path)\nprint(train_df.shape, test_df.shape)","e678f1e8":"df = pd.concat([train_df, test_df]).reset_index(drop=True)\ndf.head()","3d4a8acc":"print(df['image'].sample().values, \"\\n\",#\n      df['objects'].sample().values)","cd50c059":"df[\"checksum\"] = df['image'].apply(lambda x: x[\"checksum\"])\ndf[\"pathname\"] = df['image'].apply(lambda x: x[\"pathname\"][1:])\ndf[\"shape\"] = df['image'].apply(lambda x: x[\"shape\"])\ndf.head()","d87de0f4":"df = pd.DataFrame(\n    [\n     dict(pathname=row['pathname'], \n          shape=row['shape'],\n          checksum=row['checksum'],\n          **bb_info) \n     for _, row in df.iterrows() \n     for bb_info in row['objects']\n     ]\n     )\ndf","6cc09210":"def extract_nested_list(it):\n    if isinstance(it, list):\n        for sub_it in it:\n            yield from extract_nested_list(sub_it)\n    elif isinstance(it, dict):\n        for value in it.values():\n            yield from extract_nested_list(value)\n    else:\n        yield it","7c538906":"df['x_min'] = -1\ndf['y_min'] = -1\ndf['x_max'] = -1\ndf['y_max'] = -1\n\ndf[['y_min', 'x_min', 'y_max', 'x_max']] = np.stack(df['bounding_box'].apply(lambda x: np.array(list(extract_nested_list(x)))))\ndf.drop(columns=['bounding_box'], inplace=True)\ndf['x_min'] = df['x_min'].astype(np.float)\ndf['y_min'] = df['y_min'].astype(np.float)\ndf['x_max'] = df['x_max'].astype(np.float)\ndf['y_max'] = df['y_max'].astype(np.float)\n\ndf['w'] = df['x_max'] - df['x_min']\ndf['h'] = df['y_max'] - df['y_min']\n\ndf['x_center'] = df['x_min'] + df['w'] \/ 2\ndf['y_center'] = df['y_min'] + df['h'] \/ 2","ab2acf85":"df.sample(5)","2877ca20":"df['img_width'] = -1\ndf['img_height'] = -1\ndf['channels'] = -1\ndf['pixels'] = -1\n\ndf[['img_height','img_width', 'channels']] = np.stack(df['shape'].apply(lambda x: np.array(list(extract_nested_list(x)))))\ndf.drop(columns=['shape'], inplace=True)\ndf['pixels'] = df['img_width'] * df['img_height'] * df['channels']","90ed486b":"df.head()","a9dae3f7":"cat_dict = {v:k for k,v in enumerate(df['category'].value_counts().index, 0)}\ndf[\"label\"] = df[\"category\"].map(cat_dict)\n\ndf.head()","1db2569c":"df['category'].value_counts()","db219c39":"percentages = [c \/ df.shape[0] * 100 for c in df['category'].value_counts()]\ndf['Are there red blood cells in the Image?'] = df['category'] == 'red blood cell'\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(18, 4))\nsns.countplot(df['category'], ax=ax[0],  palette=custom_colors[1:])\nsns.countplot(df['Are there red blood cells in the Image?'], ax=ax[1], palette=custom_colors)\n\nax[0].set_xticklabels(df['category'].value_counts().index, fontsize=12, rotation=15)\nax[0].set_xlabel(ax[0].get_xlabel(), fontsize=15)\nax[0].set_ylabel(ax[0].get_ylabel(), fontsize=15)\n\nax[1].set_xticklabels(['other classes', 'red blood cell'], fontsize=12, rotation=0)\nax[1].set_xlabel('category', fontsize=15)\nax[1].set_ylabel(ax[1].get_ylabel(), fontsize=15)\n\nfor percentage, p in zip(percentages, ax[0].patches):\n    percentage = f'{np.round(percentage, 2)}%'\n    x = p.get_x() + p.get_width() \/ 2 - 0.2\n    y = p.get_y() + p.get_height()\n    ax[0].annotate(percentage, (x, y), fontsize=12, fontweight='bold')\n\nfor percentage, p in zip([3001, 83034], ax[1].patches):\n    percentage = f'{percentage} bounding-boxes'\n    x = p.get_x() + p.get_width() \/ 2 - 0.3\n    y = p.get_y() + p.get_height()\n    ax[1].annotate(percentage, (x, y), fontsize=12, fontweight='bold')\n\nplt.suptitle(\"Distribution of \u0421lasses\", y=1.08, fontsize=20, fontweight='bold')\nplt.tight_layout()\nplt.savefig(f\"class_dist.svg\", format=\"svg\", bbox_inches='tight', pad_inches=0.2)\nplt.savefig(f\"class_dist.png\", format=\"png\", bbox_inches='tight', pad_inches=0.2)\n\ndf.drop(columns=['Are there red blood cells in the Image?'], inplace=True)","4ded82bd":"fig, ax = plt.subplots(figsize=(15, 8))\nsns.countplot(df.groupby('pathname')['checksum'].count(), ax=ax, palette='rocket')\nax.set_xticks(ax.get_xticks()[::3]);\nax.set_ylabel('number of images', fontsize=16)\nax.set_xlabel('number of bounding boxes', fontsize=16)\nax.set_title(\"Distribution of Bounding Boxes in the Image\", fontsize=20, fontweight='bold');\nfig.savefig(\"bb-dist.svg\", format='svg', bbox_inches='tight', pad_inches=0.2)\nfig.savefig(\"bb-dist.png\", format='png', bbox_inches='tight', pad_inches=0.2)","55449bd9":"image_dimensions = df.drop_duplicates(subset=['pathname']).groupby(['img_width', 'img_height', 'channels']).count()['pathname'].to_dict()\nimage_dimensions","d3b0edb4":"fig, ax = plt.subplots(figsize=(10, 6))\nax.bar(np.arange(len(image_dimensions)), image_dimensions.values(), color=custom_colors)\nax.set_xticks(np.arange(len(image_dimensions)))\nax.set_xticklabels(image_dimensions, fontsize=12)\nax.set_xlabel(\"Image dimensions\", fontsize=15)\nax.set_ylabel(\"count\", fontsize=15)\nax.set_title(\"Distribution of Image Dimensions\", fontsize=18)\n\nfor percentage, p in zip(image_dimensions.values(), ax.patches):\n    percentage = f'{percentage} images'\n    x = p.get_x() + p.get_width() \/ 2 - 0.15\n    y = p.get_y() + p.get_height()\n    ax.annotate(str(percentage), (x, y), fontsize=12, fontweight='bold')","2ab33a04":"img_path = df[\"pathname\"].sample(1).values[0]\n\nlabel2hex = {k:v for k, v in \n               zip(df['category'].value_counts().index.tolist(), custom_colors[1:])}\nlabel2rgb = {k:v for k, v in \n               zip(df['category'].value_counts().index.tolist(), rgbcolors)}\n\nplot_data_example(df, config.root_dir, img_path, colors=label2hex)","34de25f0":"%%bash\ngit clone -b v1.0 https:\/\/github.com\/ultralytics\/yolov5 -qq\nmv yolov5\/* .\/\npip install -r requirements.txt -qq","c662b73b":"df.head()","520afc54":"df['w_yolo'] = df[['w', 'img_width']].apply(lambda x: x[0] \/ x[1], axis=1)\ndf['h_yolo'] = df[['h', 'img_height']].apply(lambda x: x[0] \/ x[1], axis=1)\ndf['x_center_yolo'] = df[['x_center', 'img_width']].apply(lambda x: x[0] \/ x[1], axis=1)\ndf['y_center_yolo'] = df[['y_center', 'img_height']].apply(lambda x: x[0] \/ x[1], axis=1)","7725989c":"yolov5_df = df.copy()\nyolov5_df = yolov5_df[yolov5_df[\"pathname\"].apply(lambda x: x[-4:] == '.png')].reset_index(drop=True)\nyolov5_df[\"name\"] = yolov5_df[\"pathname\"].apply(lambda x: x.split(\"\/\")[-1].split(\".\")[0])\nyolov5_df.head()","6c5eef71":"index = list(set(yolov5_df.name)) ","f6ca4570":"source = \"images\"\nif True:\n    for fold in [0]:\n        val_index = index[len(index)*fold\/\/5:len(index)*(fold+1)\/\/5]\n        for name, mini in tqdm(yolov5_df.groupby('name')):\n            if name in val_index:\n                path2save = 'val2017\/'\n                #print(\"val\")\n            else:\n                path2save = 'train2017\/'\n            if not os.path.exists('convertor\/fold{}\/labels\/'.format(fold)+path2save):\n                os.makedirs('convertor\/fold{}\/labels\/'.format(fold)+path2save)\n            with open('convertor\/fold{}\/labels\/'.format(fold)+path2save+name+\".txt\", 'w+') as f:\n                row = mini[['label','x_center_yolo', 'y_center_yolo', 'w_yolo', 'h_yolo']].astype(float).values\n                #print(row)\n                #row = row[0:]\/1024\n                row = row.astype(str)\n                for j in range(len(row)):\n                    text = ' '.join(row[j])\n                    f.write(text)\n                    f.write(\"\\n\")\n            if not os.path.exists('convertor\/fold{}\/images\/{}'.format(fold,path2save)):\n                os.makedirs('convertor\/fold{}\/images\/{}'.format(fold,path2save))\n            sh.copy(\"..\/input\/malaria-bounding-boxes\/malaria\/{}\/{}.png\".format(source,name),'convertor\/fold{}\/images\/{}\/{}.png'.format(fold,path2save,name))","ff359409":"!python \\\n train.py \\\n --img 512 \\\n --batch 12 \\\n --epochs 2 \\\n --data '..\/input\/malaria\/malaria_conf.yaml' \\\n --cfg '..\/input\/malaria\/yolov5x.yaml' \\\n --weights '' \\\n --name yolov5x_malaria ","a621fc5f":"import argparse\n\nfrom utils.datasets import *\nfrom utils.utils import *\n\n\ndef detect(save_img=False):\n    weights, imgsz = opt.weights,opt.img_size\n    source = 'convertor\/fold0\/images\/val2017'\n    \n    # Initialize\n    device = torch_utils.select_device(opt.device)\n    half = False\n    # Load model\n\n    model = torch.load(weights, map_location=device)['model'].to(device).float().eval()\n\n    dataset = LoadImages(source, img_size=1024)\n\n    t0 = time.time()\n    img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n    all_path=[]\n    all_bboxex =[]\n    all_score =[]\n    all_labels = []\n    for path, img, im0s, vid_cap in dataset:\n        print(im0s.shape)\n        img = torch.from_numpy(img).to(device)\n        img = img.half() if half else img.float()  # uint8 to fp16\/32\n        img \/= 255.0  # 0 - 255 to 0.0 - 1.0\n        if img.ndimension() == 3:\n            img = img.unsqueeze(0)\n\n        # Inference\n        t1 = torch_utils.time_synchronized()\n        bboxes_2 = []\n        score_2 = []\n        label_2 = []\n        if True:\n            pred = model(img, augment=opt.augment)[0]\n            pred = non_max_suppression(pred, 0.4, opt.iou_thres, classes=None, agnostic=False)\n            t2 = torch_utils.time_synchronized()\n\n            bboxes = []\n            score = []\n            label = []\n            # Process detections\n            for i, det in enumerate(pred):  # detections per image\n                p, s, im0 = path, '', im0s\n                gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  #  normalization gain whwh\n                if det is not None and len(det):\n                    det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n                    for c in det[:, -1].unique():\n                        n = (det[:, -1] == c).sum()  # detections per class\n\n                    for *xyxy, conf, cls in det:\n                        if True:  # Write to file\n                            xywh = torch.tensor(xyxy).view(-1).numpy()  # normalized xywh\n#                             xywh[2] = xywh[2]-xywh[0]\n#                             xywh[3] = xywh[3]-xywh[1]\n                            bboxes.append(xywh)\n                            score.append(conf)\n                            label.append(cls)\n            bboxes_2.append(bboxes)\n            score_2.append(score)\n            label_2.append(label)\n        all_path.append(path)\n        all_score.append(score_2)\n        all_bboxex.append(bboxes_2)\n        all_labels.append(label_2)\n    return all_path,all_score,all_bboxex, all_labels\n\n\nif __name__ == '__main__':\n    class opt:\n        weights = config.yolo_weights\n        img_size = 1024\n        conf_thres = 0.4\n        iou_thres = 0.5\n        augment = True\n        device = '0'\n        classes=None\n        agnostic_nms = True\n        \n    opt.img_size = check_img_size(opt.img_size)\n    print(opt)\n\n    with torch.no_grad():\n        res = detect()","ae770d2a":"def run_wbf(boxes,scores,labels0, image_size=512, iou_thr=0.5, skip_box_thr=0.4, weights=None):\n#     boxes =boxes\/(image_size-1)\n    #labels0 = [scores for idx in range(len(scores))]\n    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels0, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n#     boxes = boxes*(image_size-1)\n#     boxes = boxes\n    return boxes, scores, labels","3bdff9e9":"all_path,all_score,all_bboxex, all_labels = res","16981a6e":"def format_prediction(boxes, scores, labels):\n    result = np.zeros((len(boxes), 6))\n\n    for i, (j) in enumerate(zip(scores, boxes, labels)):\n        result[i, 0] = j[0]\n        result[i, 1] = j[1][0]\n        result[i, 2] = j[1][1]\n        result[i, 3] = j[1][2]\n        result[i, 4] = j[1][3]\n        result[i, 5] = j[2]\n\n    return result#\nids = []\nresults = []\nfor row in range(len(all_path)):\n    image_id = all_path[row].split(\"\/\")[-1].split(\".\")[0]\n    boxes = all_bboxex[row]\n    scores = all_score[row]\n    labels = all_labels[row]\n    boxes, scores, labels = run_wbf(boxes,scores, labels)\n    #boxes = (boxes*512\/512).astype(np.int32).clip(min=0, max=511)\n    #boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n    #boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n\n    result = format_prediction(boxes, scores, labels)\n    id_ = [image_id] * result.shape[0]\n\n    results.append(result)\n    ids.append(id_)\n","3234b24f":"label2rgb = {k:v for k, v in \n               zip(yolov5_df['label'].value_counts().index.tolist(), rgbcolors)}\n                  \nlabel2category = dict(zip(yolov5_df['label'].value_counts().index.values, \n                          yolov5_df['category'].value_counts().index.values))","86fd1d99":"test_df = pd.DataFrame(np.concatenate(results, axis=0), columns=['score', \"x_min\", \"y_min\", \"x_max\", \"y_max\", 'label'])\ntest_df['pathname'] = sum(ids, [])\ntest_df['category'] = test_df['label'].apply(lambda x: label2category[x])\ntest_df['pathname'] = test_df['pathname'].apply(lambda x: 'images\/'+ x + '.png')\ntest_df.to_csv('yolov5x.csv', index=False)\n\nvalid_df = yolov5_df[yolov5_df['pathname'].isin(test_df.pathname)]\ntest_df","2a63efd7":"test_df['category'].value_counts()","38a8da5a":"hex2rgb = lambda hx: (int(hx[1:3],16),int(hx[3:5],16),int(hx[5:7],16))\nlabel2rgb = list(map(hex2rgb, custom_colors))[1:]\nlabel2rgb = {k:v for k, v in \n               zip(df['category'].value_counts().index.tolist(), label2rgb)}\nlabel2hex = {k:v for k, v in \n               zip(df['category'].value_counts().index.tolist(), custom_colors[1:])}","f8018848":"test_df['pathname'].sample()","59e85de8":"img_path = 'images\/a054a8e2-55f2-4508-97df-207f76b89be2.png'\nlabel2hex = {k:v for k, v in \n               zip(df['category'].value_counts().index.tolist(), custom_colors[1:])}\n\nplot_data_example(df, config.root_dir, img_path, colors=label2hex)","e38fb16b":"img_path = 'images\/a054a8e2-55f2-4508-97df-207f76b89be2.png'\nlabel2hex = {k:v for k, v in \n               zip(df['category'].value_counts().index.tolist(), custom_colors[1:])}\n\nplot_data_example(test_df, config.root_dir, img_path, colors=label2hex)","c031b0e5":"import matplotlib\nprint(\"default backend: \", matplotlib.get_backend())\nmatplotlib.use('Agg')\n\nids = np.random.choice(list(set(test_df.pathname)), 5) # ids for testing","93e24b48":"%%time\nPATH_TO_SAVE = \"yolo_ground_truth\"\n\nif not os.path.exists(PATH_TO_SAVE):\n    os.mkdir(PATH_TO_SAVE)\n    print(f\"Folder {PATH_TO_SAVE} created.\")\n\n_= [\n    plot_image_with_bb(valid_df, config.root_dir, img_path, path_to_save=PATH_TO_SAVE)\n\n    for img_path in ids\n    ]","5c7dd695":"%%time\ncreate_video(path_to_imgs=PATH_TO_SAVE, video_name=\"yolo_ground_truth\", framerate=1)","913f1652":"# to play in google colab had to recode\n#https:\/\/ottverse.com\/ffmpeg-drawtext-filter-dynamic-overlays-timecode-scrolling-text-credits\/\n!ffmpeg -i 'yolo_ground_truth.avi' -vf \"drawtext=text='Ground Truth':x=695:y=8:fontsize=50:fontfile='.\/Bitwise.ttf':fontcolor='#000000'\" -strict -2 'yolo_transcoded_video1.mp4'","3dc0c9d2":"%%time\nPATH_TO_SAVE = \"yolo_predictions\"\n\nif not os.path.exists(PATH_TO_SAVE):\n    os.mkdir(PATH_TO_SAVE)\n    print(f\"Folder {PATH_TO_SAVE} created.\")\n\n_= [\n    plot_image_with_bb(test_df, config.root_dir, img_path, path_to_save=PATH_TO_SAVE)\n\n    for img_path in ids\n    ]","cf782283":"%%time\ncreate_video(path_to_imgs=PATH_TO_SAVE, video_name=\"yolo_predictions\", framerate=1)","5cb6f595":"!ffmpeg -i 'yolo_predictions.avi' -vf \"drawtext=text='Prediction':x=695:y=8:fontsize=50:fontfile='.\/Bitwise.ttf':fontcolor='#000000'\" -strict -2 'yolo_transcoded_video2.mp4'","11882faf":"%%bash\n#  https:\/\/unix.stackexchange.com\/questions\/233832\/merge-two-video-clips-into-one-placing-them-next-to-each-other\nffmpeg \\\n  -i yolo_transcoded_video1.mp4 \\\n  -i yolo_transcoded_video2.mp4 \\\n  -filter_complex '[0:v]pad=iw*2:ih[int];[int][1:v]overlay=W\/2:0[vid]' \\\n  -map [vid] \\\n  -c:v libx264 \\\n  -crf 23 \\\n  -preset veryfast \\\n  yolo_result.mp4","45ac13c5":"show_video('.\/yolo_result.mp4')","7ca45edf":"fastercnn_df = df[['pathname','x_min', 'y_min', 'x_max', 'y_max','w', 'h', 'label', 'category']].copy()\nremap_labels = {0:1, 1:2, 2:3, 3:4, 4:5, 5:6, 6:7}\nremap_category = {v:k for v,k in enumerate(fastercnn_df['category'].value_counts().index, 1)} \nfastercnn_df['label'] = fastercnn_df['label'].apply(lambda x: remap_labels[x])      # remap because 0 its bg\nfastercnn_df['category'] = fastercnn_df['label'].apply(lambda x: remap_category[x]) # remap because 0 its bg","c7416f27":"fastercnn_df.sample()","274a5fe5":"image_ids = fastercnn_df['pathname'].unique()\ntrain_ids, valid_ids = train_test_split(image_ids,test_size=0.12, shuffle=True, random_state=22)\n\nvalid_df = fastercnn_df[fastercnn_df['pathname'].isin(valid_ids)].reset_index(drop=True)\ntrain_df = fastercnn_df[fastercnn_df['pathname'].isin(train_ids)].reset_index(drop=True)","3e2323df":"class MalariaDataset(Dataset):\n\n    def __init__(self, dataframe, root_dir, transforms=None):\n        super().__init__()\n\n        self.image_ids = dataframe['pathname'].unique()\n        self.df = dataframe\n        self.root_dir = root_dir\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        records = self.df[self.df['pathname'] == image_id]\n\n        image = cv2.imread(f'{self.root_dir}\/{image_id}')\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n\n        boxes = records[['x_min', 'y_min', 'x_max', 'y_max']].values\n\n        area = records['w'].values * records['h'].values\n        area = torch.as_tensor(area, dtype=torch.float32)\n\n        # there is only one class\n        labels = records['label'].values\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        # target['masks'] = None\n        target['image_id'] = torch.tensor([index])\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0).float()\n        return image, target, image_id\n\n    def __len__(self):\n        return self.image_ids.shape[0]","b20a9736":"# Albumentations\ndef get_train_transform():\n    return A.Compose([\n        A.Flip(0.5),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef get_valid_transform():\n    return A.Compose([\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n","88460af5":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_dataset = MalariaDataset(train_df, config.root_dir, get_train_transform())\nvalid_dataset = MalariaDataset(valid_df, config.root_dir, get_valid_transform())\n\n\n# split the dataset in train and test set\nindices = torch.randperm(len(train_dataset)).tolist()\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=12,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size=8,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)","c4aea327":"images, targets, ids = next(iter(train_data_loader))","8d02196f":"label2rgb = {k:v for k, v in \n               zip(fastercnn_df['label'].value_counts().index.tolist(), rgbcolors)}\n               \nlabel2category = dict(zip(fastercnn_df['label'].value_counts().index.values, \n                          fastercnn_df['category'].value_counts().index.values))","a6dd9a3e":"for i_image, i_targets in zip(images[:3], targets[:3]):\n    plot_sample(i_image, i_targets['boxes'], i_targets['labels'])","4a02ebc2":"model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\nnum_classes = 8  # 7 classes + background\n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n# Load the trained weights\n#model.load_state_dict(torch.load('\/content\/drive\/My Drive\/fasterrcnn_resnet50_fpn_80_epoch.pth'))","fceed0ac":"model.to('cuda')\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.004, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\",\n                                           patience=3, verbose=True)\n#lr_scheduler = None\n\nnum_epochs = 1","dd801e30":"class Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total \/ self.iterations\n\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0","3fadad13":"loss_hist = Averager()\nitr = 1\ndevice= 'cuda'\nfor epoch in range(num_epochs):\n    loss_hist.reset()\n    \n    for images, targets, image_ids in train_data_loader:\n        \n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        loss_hist.send(loss_value)\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        if itr % 50 == 0:\n            print(f\"Iteration #{itr} loss: {loss_value}\")\n\n        itr += 1\n    \n    # update the learning rate\n    if lr_scheduler is not None:\n        lr_scheduler.step(loss_hist.value)\n\n    print(f\"Epoch #{epoch} loss: {loss_hist.value}\")   ","11ebb9e1":"def format_prediction(boxes, scores, labels):\n    result = np.zeros((len(boxes), 6))\n\n    for i, (j) in enumerate(zip(scores, boxes, labels)):\n        result[i, 0] = j[0]\n        result[i, 1] = j[1][0]\n        result[i, 2] = j[1][1]\n        result[i, 3] = j[1][2]\n        result[i, 4] = j[1][3]\n        result[i, 5] = j[2]\n\n    return result","1dee7e8b":"# Load the trained weights\nmodel.load_state_dict(torch.load(config.fasterrcnn_weights))\nmodel.eval()\n\ndetection_threshold = 0.5\nresults = []\nids = []\ndevice = 'cuda'\nmodel.to(device)\nfor images, targets, image_ids in valid_data_loader:\n\n    images = list(image.to(device) for image in images)\n    outputs = model(images)\n\n    for i, image in enumerate(images):\n\n        boxes = outputs[i]['boxes'].data.cpu().numpy()\n        scores = outputs[i]['scores'].data.cpu().numpy()\n        labels = outputs[i]['labels'].data.cpu().numpy()\n\n        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n        scores = scores[scores >= detection_threshold]\n        image_id = image_ids[i]\n        \n        result = format_prediction(boxes, scores, labels)\n        id_ = [image_id] * result.shape[0]\n        \n        results.append(result)\n        ids.append(id_)","242cc47f":"test_df = pd.DataFrame(np.concatenate(results, axis=0), columns=['score', \"x_min\", \"y_min\", \"x_max\", \"y_max\", 'label'])\ntest_df['pathname'] = sum(ids, [])\ntest_df['category'] = test_df['label'].apply(lambda x: label2category[x])\ntest_df","dd0eec9b":"test_df['category'].value_counts()","b4df9c08":"hex2rgb = lambda hx: (int(hx[1:3],16),int(hx[3:5],16),int(hx[5:7],16))\nlabel2rgb = list(map(hex2rgb, custom_colors))[1:]\nlabel2rgb = {k:v for k, v in \n               zip(fastercnn_df['category'].value_counts().index.tolist(), label2rgb)}\nlabel2hex = {k:v for k, v in \n               zip(fastercnn_df['category'].value_counts().index.tolist(), custom_colors[1:])}","84d4502f":"img_path = test_df[\"pathname\"].sample(1).values[0]\n\nlabel2hex = {k:v for k, v in \n               zip(df['category'].value_counts().index.tolist(), custom_colors[1:])}\n\nplot_data_example(valid_df, config.root_dir, img_path, colors=label2hex)","6101b85d":"label2hex = {k:v for k, v in \n               zip(df['category'].value_counts().index.tolist(), custom_colors[1:])}\n\nplot_data_example(test_df, config.root_dir, img_path, colors=label2hex)","2a608e73":"import matplotlib\nprint(\"default backend: \", matplotlib.get_backend())\nmatplotlib.use('Agg')\n\nids = np.random.choice(list(set(test_df.pathname)), 5) # ids for testing","0f3c52c1":"%%time\nPATH_TO_SAVE = \"fasterrcnn_ground_truth\"\n\nif not os.path.exists(PATH_TO_SAVE):\n    os.mkdir(PATH_TO_SAVE)\n    print(f\"Folder {PATH_TO_SAVE} created.\")\n\n_= [\n    plot_image_with_bb(valid_df, config.root_dir, img_path, path_to_save=PATH_TO_SAVE)\n\n    for img_path in ids\n    ]","0209a20e":"%%time\ncreate_video(path_to_imgs=PATH_TO_SAVE, video_name=\"fasterrcnn_ground_truth\", framerate=1)","c154a131":"# to play in google colab had to recode\n#https:\/\/ottverse.com\/ffmpeg-drawtext-filter-dynamic-overlays-timecode-scrolling-text-credits\/\n!ffmpeg -i 'fasterrcnn_ground_truth.avi' -vf \"drawtext=text='Ground Truth':x=695:y=8:fontsize=50:fontfile='.\/Bitwise.ttf':fontcolor='#000000'\" -strict -2 'fasterrcnn_transcoded_video1.mp4'","c0bb700e":"%%time\nPATH_TO_SAVE = \"fasterrcnn_predictions\"\n\nif not os.path.exists(PATH_TO_SAVE):\n    os.mkdir(PATH_TO_SAVE)\n    print(f\"Folder {PATH_TO_SAVE} created.\")\n\n_= [\n    plot_image_with_bb(test_df, config.root_dir, img_path, path_to_save=PATH_TO_SAVE)\n\n    for img_path in ids\n    ]","05275899":"%%time\ncreate_video(path_to_imgs=PATH_TO_SAVE, video_name=\"fasterrcnn_predictions\", framerate=1)","dcb82fb0":"!ffmpeg -i 'fasterrcnn_predictions.avi' -vf \"drawtext=text='Prediction':x=695:y=8:fontsize=50:fontfile='.\/Bitwise.ttf':fontcolor='#000000'\" -strict -2 'fasterrcnn_transcoded_video2.mp4'","6a6d279d":"%%bash\n#  https:\/\/unix.stackexchange.com\/questions\/233832\/merge-two-video-clips-into-one-placing-them-next-to-each-other\nffmpeg \\\n  -i fasterrcnn_transcoded_video1.mp4 \\\n  -i fasterrcnn_transcoded_video2.mp4 \\\n  -filter_complex '[0:v]pad=iw*2:ih[int];[int][1:v]overlay=W\/2:0[vid]' \\\n  -map [vid] \\\n  -c:v libx264 \\\n  -crf 23 \\\n  -preset veryfast \\\n  fasterrcnn_result.mp4","fc38afb2":"show_video('.\/fasterrcnn_result.mp4')","82b5a05d":"Ground Truth","cb426c89":"Distribution of Bounding Boxes Image","fdaaa752":"### Result","39a52710":"extract and create bb-info in coordinates","1742a58a":"Distribution of Image Dimensions","0936cc31":"# Data Preparation","61414b07":"How the images looks like?","12a4f509":"create columns with image shape information","3cfb4ff1":"Distribution of \u0421lasses","feb1f90e":"helper functions","93a67dbd":"Ground Truth","a62176b9":"Prediction","8568d37d":"# FasterRCNN","6bde17c9":"create numerical label for categories","2f659d58":"Prediction","22c38a49":"Prediction","871c6553":"Prediction","425002e3":"# YoloV5","2d50f2d0":"Distribution by classes","2e6e318c":"# EDA","ce24238d":"convert bb-coordinates for yolo format (center and normalize bounding box coordinates)","1cc2e153":"Prediction","e802f905":"extract information about images and bbx into separate columns","9f252a72":"Ground Truth","a93ea808":"Ground Truth","b18ba606":"Results"}}