{"cell_type":{"741c707a":"code","e6b0b9f5":"code","bbcc17ea":"code","d4a4ce7d":"code","0e66788f":"code","457f2c32":"code","bd801ffc":"code","378e3fbf":"code","17cd820a":"code","87e83cbc":"code","dad72ac8":"code","884e74f2":"code","04949658":"markdown","1017ecc1":"markdown","5ca57b05":"markdown","ee4d1722":"markdown"},"source":{"741c707a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e6b0b9f5":"import pandas as pd\nimport numpy as np\nimport sklearn\nimport statsmodels.api as sm\nimport scipy.stats as st\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, f1_score, make_scorer\nimport matplotlib.mlab as mlab\n%matplotlib inline\n\nheart_dis_df = pd.read_csv(\"..\/input\/framingham-heart-study-dataset\/framingham.csv\")\n\nprint('The number of samples into the train data is {}.'.format(heart_dis_df.shape[0]))\n\nheart_dis_df.head()","bbcc17ea":"heart_dis_df.isnull().sum()","d4a4ce7d":"#heart_dis_df[\"glucose\"].fillna((heart_dis_df[\"glucose\"].mean()), inplace=True)\nheart_dis_df.drop(['education'],axis=1,inplace=True)\nheart_dis_df.dropna(axis=0,inplace=True)\nprint('The number of samples after drop NA data is {}.'.format(heart_dis_df.shape[0]))\n\nheart_dis_df.isnull().sum()","0e66788f":"X=heart_dis_df.iloc[:,:-1]\ny=heart_dis_df.iloc[:,-1]\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.20,random_state=5)","457f2c32":"def draw_histograms(dataframe, features, rows, cols):\n    fig=plt.figure(figsize=(20,20))\n    for i, feature in enumerate(features):\n        ax=fig.add_subplot(rows,cols,i+1)\n        dataframe[feature].hist(bins=20,ax=ax,facecolor='midnightblue')\n        ax.set_title(feature+\" Distribution\",color='DarkRed')\n        \n    fig.tight_layout()  \n    plt.show()\ndraw_histograms(heart_dis_df,heart_dis_df.columns,6,3)","bd801ffc":"\nsns.countplot(x='TenYearCHD',data=heart_dis_df)\nheart_dis_df.TenYearCHD.value_counts()","378e3fbf":"heart_dis_df.describe()","17cd820a":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFECV\n# Create the RFE object and compute a cross-validated score.\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nimport warnings\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    rfecv = RFECV(estimator=LogisticRegression(), step=1, cv=10, scoring='accuracy')\n    rfecv.fit(X, y)\n\nprint(\"Optimal number of features: %d\" % rfecv.n_features_)\nprint('Selected features: %s' % list(X.columns[rfecv.support_]))\n\n# Plot number of features VS. cross-validation scores\nplt.figure(figsize=(10,6))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","87e83cbc":"Selected_features = ['male', 'age', 'currentSmoker', 'cigsPerDay', 'BPMeds', 'prevalentStroke', 'prevalentHyp', 'diabetes', 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']\n#Selected_features = ['male', 'age', 'currentSmoker', 'BPMeds', 'prevalentStroke', 'prevalentHyp', 'diabetes', 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']\n\nX = X[Selected_features]\n\nplt.subplots(figsize=(15, 5))\nsns.heatmap(X.corr(), annot=True, cmap=\"RdYlGn\")\nplt.show()","dad72ac8":"from sklearn.dummy import DummyClassifier\n\nlogreg=LogisticRegression()\nlogreg.fit(X_train,y_train)\ny_pred=logreg.predict(X_test)\n\nsklearn.metrics.accuracy_score(y_test,y_pred)\nprint(\"Accuracy_score : \", sklearn.metrics.accuracy_score(y_test,y_pred))\nprint(\"F1_score : \", f1_score(y_test, y_pred, average=\"macro\"))\n\ndummy = DummyClassifier(strategy=\"constant\", random_state=0, constant=1)\ndummy.fit(X_train,y_train)\n\nprint(\"Dummy f1 score:\", f1_score(y_test, dummy.predict(X_test), average=\"micro\"))","884e74f2":"from sklearn.model_selection import GridSearchCV\n\nf1 = make_scorer(f1_score , average='macro')\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    grid={\"C\":np.logspace(-30,30,1), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n    logreg=LogisticRegression()\n    logreg_cv=GridSearchCV(logreg,grid,cv=10, scoring=f1)\n    logreg_cv.fit(X_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\nprint(\"F1_score : \",logreg_cv.best_score_)","04949658":"**3. Exploratory Data Analysis**","1017ecc1":"**1. Import Data & Python Packages**","5ca57b05":"**4. Logistic Regression and Results**","ee4d1722":"**2. Data Quality & Missing Value Assessment**"}}