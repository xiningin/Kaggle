{"cell_type":{"a6c2369a":"code","4f240b8b":"code","c399bfaf":"code","9f66832a":"code","05935328":"code","5a42be7e":"code","5b9d5355":"code","f8ef4f87":"code","0f38df9e":"code","620009a6":"code","8f1fa62f":"code","b93e7afd":"code","c63c066f":"code","a3a89e6f":"code","2e168b51":"code","9239b62f":"markdown","b82967eb":"markdown","8dd96982":"markdown","dc646c75":"markdown","dce17ae1":"markdown","edfa5997":"markdown","858d88bb":"markdown","3479476a":"markdown"},"source":{"a6c2369a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","4f240b8b":"df_train = pd.read_csv(\"..\/input\/socc-ai-competition-1\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/socc-ai-competition-1\/test.csv\")","c399bfaf":"df_train.shape","9f66832a":"df_train.head()","05935328":"df_train.drop(['url'],1,inplace=True)\ndf_train.head()","5a42be7e":"df_train.describe()\n#df_train.describe().T\n\n#if you want a tranposed output use the above one","5b9d5355":"df_train.info()\n#to check the datatypes of the columns","f8ef4f87":"df_train.isnull().sum()\n# to check the null values\n\n#df_train.isnull(),any() --- Returns true if any null value in column else False","0f38df9e":"import missingno as msno \nmsno.matrix(df_train)\n#plots the null values in dataframe\n\n#msno.bar(df)-- for bar plot of null values\n#msno.heatmap(df)--for heatmap of null values","620009a6":"plt.figure(figsize=(50,40))\nsns.heatmap(data=df_train.corr(),annot=True)","8f1fa62f":"df_train.hist(figsize=(30,30))\nplt.show()","b93e7afd":"for i in df_train.columns:\n    sns.boxplot(df_train[i])\n    plt.show()","c63c066f":"X_train=df_train.drop(['shares'],1)\ny_train=df_train['shares']\nX_test=df_test.drop(['url'],1)\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor()\nrf.fit(X_train,y_train)\nrf_pred = rf.predict(X_test)","a3a89e6f":"features = df_train.columns\nimp= rf.feature_importances_\ni= np.argsort(imp)\nplt.figure(1,figsize=(10,20))\nplt.title('Feature Importances')\nplt.barh(range(len(i)), imp[i], color='lightblue', align='center')\nplt.yticks(range(len(i)), features[i])\nplt.xlabel('Relative Importance')","2e168b51":"from sklearn.datasets import make_classification\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nX, y = make_classification(random_state=0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nclf = SVC(random_state=0)\nclf.fit(X_train, y_train)\nplot_confusion_matrix(clf, X_test, y_test)\nplt.show()","9239b62f":"## Data Analysis\n**Correlation plot**","b82967eb":"**Hist plot**","8dd96982":"**Confusion matrix plot**\neasy way to plot confusion matrix is using sklearn as shown below","dc646c75":"**Feature Importance plot**","dce17ae1":"Feature importance scores play an important role in a predictive modeling project, including providing insight into the data, insight into the model, and the basis for dimensionality reduction and feature selection that can improve the efficiency and effectiveness of a predictive model on the problem.like for \n1. **Decision Tree,Random Forest,XGboost** - importance = model.feature_importances_\n2. **Logistic Regression,linear regression** - importance = model.coef_","edfa5997":"## Data Exploration","858d88bb":"## Outlier Analysis\n**Detecting outliers using boxplot for each column**","3479476a":"## In Kaggle competitions y_test is something we do not have.\n**Then I guess the best thing to do is to use cross validation.**\n\nReal certainty if the model suffers from overfitting or not, we only have when applying the model either to test data or to future production data. So, it is difficult to say just by looking the score on the train data if the model overfits or not.\n>If you have the real values of y (target feature) on the test data, you can measure the accuracy on the test predictions.\n\n1. You train one model some regressor reg.train(X_train,y_train)\n2. The resulting model does not explain the training data 100% well (that would probably be overfitting).\n3. If you use now the X_train to make predictions you will not get exactly y_train, but Y_pred, different somehow y_pred = reg.predict(X_train)\n4. reg.score(X_train,y_train) is calculating the difference between y_train and y_pred (an accuracy measure), but you did not need to explicitly calculate y_pred. The library does this internally.\n\nIf you try this (once the model trained with the train data):\n>y_pred = reg.predict(X_test)\n reg.score(X_test,Y_pred)\n\nthis score will always give 1.0\n\n(because it compares Y_pred' (which the library calculates internally as Y_pred'= logreg.predict(X_test) ) with Y_pred; but Y_pred is also logreg.predict(X_test), because the code we wrote)\n> logreg.score(X_test, Y_test) \n\nis comparing the predictions of the model against the real labels.this gives model accuracy which is not possible beacuse we dont have y_test as said before\n\n**Mainly remember!!**\n1. predictor.score(X,Y) internally calculates Y'=predictor.predict(X) and then compares Y' against Y to give an accuracy measure. This applies not only to logistic regression but to any other model.\n2. reg.score(X_train,Y_train) is measuring the accuracy of the model against the training data. (How well the model explains the data it was trained with). <-- But note that this has nothing to do with test data.\n3. reg.score(X_test, Y_test) is equivalent to your print(classification_report(Y_test, Y_pred)). But you do not need to calculate Y_pred; that is done internally by the library"}}