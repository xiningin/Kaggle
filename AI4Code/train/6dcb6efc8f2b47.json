{"cell_type":{"6b2c0552":"code","3577c162":"code","94a2a17e":"code","6343a86d":"code","8d531891":"code","56e08223":"code","9f4f0f88":"code","72830576":"code","15a37d0a":"code","31d71428":"code","3356dbf7":"code","09cf9b64":"code","5c1f70d1":"code","d7f85926":"code","1efcc024":"code","572ff9d8":"code","22bfd4db":"code","a3f5ca74":"code","95fdd800":"code","ddeb77bb":"code","70bead22":"code","9f520631":"code","7523f23f":"code","e53cc39c":"code","d5907b4b":"code","407986df":"code","19f623a2":"code","39a1c83f":"code","2edd5406":"code","9add40c9":"code","d348526e":"code","088e2ab0":"code","c95befa3":"code","8d492e97":"code","c6f66567":"code","0d6bd092":"code","fdc9b133":"code","2283c43b":"code","5c5c0c68":"code","e25cba02":"code","dcf311dd":"code","11010c58":"code","5c73a344":"code","3f89fafb":"code","0023c9e6":"code","cd15d5c9":"code","6fb7d9ad":"code","d5c75c0f":"code","2bdad0e6":"code","70bb85ba":"code","66df2c87":"code","4902eeb5":"code","322328fc":"code","645d28b1":"markdown","afa4f4e1":"markdown","fd667add":"markdown","64c70c6b":"markdown","02d12b30":"markdown","f508ad1f":"markdown","adc7c0da":"markdown","da14e809":"markdown","c8e2575d":"markdown","2c5edb6a":"markdown","1baef079":"markdown","9030df8a":"markdown","fc4ed32e":"markdown","b6e44a87":"markdown","8dc8b6f2":"markdown","135d3d0c":"markdown","8b697501":"markdown","1114437c":"markdown","c853bffc":"markdown","362db5df":"markdown","2362da3f":"markdown","b17ca6e7":"markdown","6adad0c8":"markdown","36154b5f":"markdown","2df5d11a":"markdown","2369a662":"markdown","b7e4d9fe":"markdown","d877e6be":"markdown","aeae10fc":"markdown","bd6e2ee3":"markdown","a18fa2fd":"markdown","fc2bfc43":"markdown","7ffd9def":"markdown","4b4d5d71":"markdown","19eed8cf":"markdown"},"source":{"6b2c0552":"## load the libraries \nfrom keras.layers import Dense, Input, Conv2D, LSTM, MaxPooling2D, UpSampling2D , Flatten ,MaxPool2D ,BatchNormalization , Dropout\nfrom keras.optimizers import RMSprop ,Adam\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import to_categorical\nfrom numpy import argmax, array_equal\nimport matplotlib.pyplot as plt\nfrom keras.models import Model\nfrom imgaug import augmenters\nfrom random import randint\nimport pandas as pd\nfrom keras.utils.np_utils import to_categorical\n\nimport numpy as np\n","3577c162":"### read dataset \ntrain = pd.read_csv(\"..\/input\/fashion-mnist_train.csv\")\ntrain_x = train[list(train.columns)[1:]].values\ntrain_y = train['label'].values\n\n## normalize and reshape the predictors  \ntrain_x = train_x \/ 255\n\n## create train and validation datasets\ntrain_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2)\n\n## reshape the inputs\ntrain_x = train_x.reshape(-1, 784)\nval_x = val_x.reshape(-1, 784)","94a2a17e":"## input layer\ninput_layer = Input(shape=(784,))\n\n## encoding architecture\nencode_layer1 = Dense(1500, activation='relu')(input_layer)\nencode_layer2 = Dense(1000, activation='relu')(encode_layer1)\nencode_layer3 = Dense(500, activation='relu')(encode_layer2)\n\n## latent view\nlatent_view   = Dense(10, activation='sigmoid')(encode_layer3)\n\n## decoding architecture\ndecode_layer1 = Dense(500, activation='relu')(latent_view)\ndecode_layer2 = Dense(1000, activation='relu')(decode_layer1)\ndecode_layer3 = Dense(1500, activation='relu')(decode_layer2)\n\n## output layer\noutput_layer  = Dense(784)(decode_layer3)\n\nmodel = Model(input_layer, output_layer)","6343a86d":"model.summary()","8d531891":"model.compile(optimizer='adam', loss='mse')\nearly_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\nmodel.fit(train_x, train_x, epochs=20, batch_size=2048, validation_data=(val_x, val_x), callbacks=[early_stopping])","56e08223":"val_x.shape","9f4f0f88":"preds = model.predict(val_x)","72830576":"from PIL import Image \nf, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5):\n    ax[i].imshow(val_x[i].reshape(28, 28))\nplt.show()","15a37d0a":"f, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5):\n    ax[i].imshow(preds[i].reshape(28, 28))\nplt.show()","31d71428":"## recreate the train_x array and val_x array\ntrain_x = train[list(train.columns)[1:]].values\ntrain_x,valid_x,train_ground,valid_ground = train_test_split(train_x, train_x,test_size=0.2,random_state=13)\n\n## normalize and reshape\ntrain_x = train_x\/255.\nvalid_x = valid_x\/255.\ntrain_ground = train_ground\/255.\nvalid_ground = valid_ground\/255.","3356dbf7":"train_x = train_x.reshape(-1, 28, 28, 1)\nvalid_x = valid_x.reshape(-1, 28, 28, 1)\ntrain_ground = train_ground.reshape(-1, 28, 28, 1)\nvalid_ground = valid_ground.reshape(-1, 28, 28, 1)","09cf9b64":"# Lets add sample noise - Salt and Pepper\nnoise = augmenters.SaltAndPepper(0.1)\nseq_object = augmenters.Sequential([noise])\n\ntrain_x_n = seq_object.augment_images(train_x * 255) \/ 255\nval_x_n = seq_object.augment_images(valid_x * 255) \/ 255","5c1f70d1":"f, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5,10):\n    ax[i-5].imshow(train_x[i].reshape(28, 28))\nplt.show()","d7f85926":"f, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5,10):\n    ax[i-5].imshow(train_x_n[i].reshape(28, 28))\nplt.show()","1efcc024":"# input layer\ninput_layer = Input(shape=(28, 28, 1))\n\n# encoding architecture\nencoded_layer1 = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)\nencoded_layer1 = MaxPool2D( (2, 2), padding='same')(encoded_layer1)\nencoded_layer2 = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded_layer1)\nencoded_layer2 = MaxPool2D( (2, 2), padding='same')(encoded_layer2)\nencoded_layer3 = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded_layer2)\nlatent_view    = MaxPool2D( (2, 2), padding='same')(encoded_layer3)\n\n# decoding architecture\ndecoded_layer1 = Conv2D(16, (3, 3), activation='relu', padding='same')(latent_view)\ndecoded_layer1 = UpSampling2D((2, 2))(decoded_layer1)\ndecoded_layer2 = Conv2D(32, (3, 3), activation='relu', padding='same')(decoded_layer1)\ndecoded_layer2 = UpSampling2D((2, 2))(decoded_layer2)\ndecoded_layer3 = Conv2D(64, (3, 3), activation='relu')(decoded_layer2)\ndecoded_layer3 = UpSampling2D((2, 2))(decoded_layer3)\noutput_layer   = Conv2D(1, (3, 3), padding='same')(decoded_layer3)\n\n# compile the model\nmodel_2 = Model(input_layer, output_layer)\nmodel_2.compile(optimizer='adam', loss='mse')","572ff9d8":"model_2.summary()","22bfd4db":"early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=5, mode='auto')\nhistory = model_2.fit(train_x_n, train_x, epochs=10, batch_size=2048, validation_data=(val_x_n, valid_x), callbacks=[early_stopping])","a3f5ca74":"preds = model_2.predict(val_x_n)","95fdd800":"f, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5):\n    ax[i].imshow(val_x_n[i].reshape(28, 28))\nplt.show()","ddeb77bb":"f, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5):\n    ax[i].imshow(preds[i].reshape(28, 28))\nplt.show()","70bead22":"### read dataset \ntrain = pd.read_csv(\"..\/input\/fashion-mnist_train.csv\")\ntest  = pd.read_csv(\"..\/input\/fashion-mnist_test.csv\")\ntrain_x = train[list(train.columns)[1:]].values\ntrain_y = train['label'].values\ntest_y=test[\"label\"].values\ntest=test[list(test.columns)[1:]].values\n\n\n## normalize and reshape the predictors  \ntrain_x = train_x \/ 255\ntest= test\/255\n\n## create train and validation datasets\n#train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2)\n\n## reshape the inputs\ntrain_x = train_x.reshape(-1, 28,28,1)\ntest = test.reshape(-1, 28,28,1)","9f520631":"train_x.shape , test.shape","7523f23f":"label_dict = {\n 0: 'A',\n 1: 'B',\n 2: 'C',\n 3: 'D',\n 4: 'E',\n 5: 'F',\n 6: 'G',\n 7: 'H',\n 8: 'I',\n 9: 'J',\n}","e53cc39c":"\nplt.figure(figsize=[5,5])\n\n# Display the first image in training data\nplt.subplot(121)\ncurr_img = np.reshape(train_x[0], (28,28))\ncurr_lbl = train_y[0]\nplt.imshow(curr_img, cmap='gray')\nplt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")\n\n# Display the first image in testing data\nplt.subplot(122)\ncurr_img = np.reshape(test[0], (28,28))\ncurr_lbl = test_y[0]\nplt.imshow(curr_img, cmap='gray')\nplt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")","d5907b4b":"from sklearn.model_selection import train_test_split\ntrain_X,valid_X,train_ground,valid_ground = train_test_split(train_x,\n                                                             train_x,\n                                                             test_size=0.2,\n                                                             random_state=13)","407986df":"batch_size = 64\nepochs = 200\ninChannel = 1\nx, y = 28, 28\ninput_img = Input(shape = (x, y, inChannel))\nnum_classes = 10","19f623a2":"def encoder(input_img):\n    #encoder\n    #input = 28 x 28 x 1 (wide and thin)\n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32\n    conv1 = BatchNormalization()(conv1)\n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n    conv1 = BatchNormalization()(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) #14 x 14 x 32\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1) #14 x 14 x 64\n    conv2 = BatchNormalization()(conv2)\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n    conv2 = BatchNormalization()(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) #7 x 7 x 64\n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2) #7 x 7 x 128 (small and thick)\n    conv3 = BatchNormalization()(conv3)\n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n    conv3 = BatchNormalization()(conv3)\n    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3) #7 x 7 x 256 (small and thick)\n    conv4 = BatchNormalization()(conv4)\n    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n    conv4 = BatchNormalization()(conv4)\n    return conv4\n\ndef decoder(conv4):    \n    #decoder\n    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv4) #7 x 7 x 128\n    conv5 = BatchNormalization()(conv5)\n    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv5)\n    conv5 = BatchNormalization()(conv5)\n    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv5) #7 x 7 x 64\n    conv6 = BatchNormalization()(conv6)\n    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv6)\n    conv6 = BatchNormalization()(conv6)\n    up1 = UpSampling2D((2,2))(conv6) #14 x 14 x 64\n    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(up1) # 14 x 14 x 32\n    conv7 = BatchNormalization()(conv7)\n    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv7)\n    conv7 = BatchNormalization()(conv7)\n    up2 = UpSampling2D((2,2))(conv7) # 28 x 28 x 32\n    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2) # 28 x 28 x 1\n    return decoded\n","39a1c83f":"autoencoder = Model(input_layer, decoder(encoder(input_layer)))\nautoencoder.compile(loss='mean_squared_error', optimizer = 'rmsprop')","2edd5406":"autoencoder.summary()","9add40c9":"autoencoder_train = autoencoder.fit(train_X, train_ground, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_ground))","d348526e":"loss = autoencoder_train.history['loss']\nval_loss = autoencoder_train.history['val_loss']\nepochs = range(200)\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","088e2ab0":"autoencoder.save_weights('autoencoder.h5')","c95befa3":"# Change the labels from categorical to one-hot encoding\ntrain_Y_one_hot = to_categorical(train_y)\ntest_Y_one_hot = to_categorical(test_y)\n\n# Display the change for category label using one-hot encoding\nprint('Original label:', train_y[1000])\nprint('After conversion to one-hot:', train_Y_one_hot[0])","8d492e97":"train_X,valid_X,train_label,valid_label = train_test_split(train_x,train_Y_one_hot,test_size=0.2,random_state=13)\n","c6f66567":"train_X.shape , valid_X.shape , train_label.shape , valid_label.shape","0d6bd092":"def fc(enco):\n    flat = Flatten()(enco)\n    x = Dense(50, activation='relu')(flat)\n    x = Dropout(0.1)(x)\n    x = Dense(30, activation='relu')(x)\n    x = Dropout(0.1)(x)\n\n    out = Dense(10, activation='softmax')(x)\n    return out","fdc9b133":"encode = encoder(input_layer)\nfull_model = Model(input_layer,fc(encode))\n","2283c43b":"for l1,l2 in zip(full_model.layers[:19],autoencoder.layers[0:19]):\n    l1.set_weights(l2.get_weights())\n","5c5c0c68":"autoencoder.get_weights()[0][1]\n","e25cba02":"full_model.get_weights()[0][1]\n","dcf311dd":"for layer in full_model.layers[0:19]:\n    layer.trainable = False\n","11010c58":"full_model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=1e-5),metrics=['accuracy'])\n","5c73a344":"full_model.summary()","3f89fafb":"classify_train = full_model.fit(train_X, train_label, batch_size=64,epochs=40,verbose=1,validation_data=(valid_X, valid_label))\n","0023c9e6":"accuracy = classify_train.history['acc']\nval_accuracy = classify_train.history['val_acc']\nloss = classify_train.history['loss']\nval_loss = classify_train.history['val_loss']\nepochs = range(len(accuracy))\nplt.plot(epochs, accuracy, 'bo', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","cd15d5c9":"test_eval = full_model.evaluate(test, test_Y_one_hot, verbose=0)","6fb7d9ad":"print('Test loss:', test_eval[0])\nprint('Test accuracy:', test_eval[1])\n","d5c75c0f":"predicted_classes = full_model.predict(test)\n","2bdad0e6":"predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n","70bb85ba":"predicted_classes.shape, test_y.shape","66df2c87":"correct = np.where(predicted_classes==test_y)[0]\nprint (\"Found %d correct labels\" % len(correct))\nfor i, correct in enumerate(correct[:9]):\n    plt.subplot(3,3,i+1)\n    plt.imshow(test[correct].reshape(28,28), cmap='gray', interpolation='none')\n    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], test_y[correct]))\n    plt.tight_layout()","4902eeb5":"incorrect = np.where(predicted_classes!=test_y)[0]\nprint (\"Found %d incorrect labels\" % len(incorrect))\nfor i, incorrect in enumerate(incorrect[:9]):\n    plt.subplot(3,3,i+1)\n    plt.imshow(test[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], test_y[incorrect]))\n    plt.tight_layout()","322328fc":"from sklearn.metrics import classification_report\ntarget_names = [\"Class {}\".format(i) for i in range(num_classes)]\nprint(classification_report(test_y, predicted_classes, target_names=target_names))","645d28b1":"## but again question arises how we will do it ? but before that why we are doing so ? what is the need ? CNN is not efficient do so ?\n## Previously i have made kernel how we can use PCA with CNN now the reason behind to use that is just an expriment that if we can use most important feautres of images by converting images in nxn matrix and can achieve near about accuracy with less features \n## one more reason in kaggle world people do ensemble , without taking care scalibilty of application so might be model is producing good number but not good in production environment \n## with pca-cnn our goal is accuracy and making model more optimized , so i achieved it \n\n## so now we are taking autoencoders for following reasons:\n### 1> firt itself is neural network architecture it will learn more complex features than PCA which learnt only linearities.\n### 2> second pca was usefull for grayscale images but not for RGB data reason behind non-linearities and if data is complex PCA alone can't do make it possible here we are using fashion mnist but in next commit RGB data will be there to classify \n### 3>model optimization , making the use less number of parameters and achieve a good accuracy  ","afa4f4e1":"Generate the predictions on validation data. ","fd667add":"Since the predictions you get are floating point values, it will not be feasible to compare the predicted labels with true test labels. So, you will round off the output which will convert the float values into an integer. Further, you will use np.argmax() to select the index number which has a higher value in a row.\n\nFor example, let's assume a prediction for one test image to be [0 1 0 0 0 0 0 0 0 0], the output for this should be a class label 1.","64c70c6b":"## i have changed the neural architecture from taking dense (128) to two stacked dense (50) and dense(30) with dropout(0.1) to reduce overfitting and make the model generalized for fashion mnist dataset ","02d12b30":"Here is the model summary","f508ad1f":"## Predicted Labels","adc7c0da":"## Model Evaluation on the Test Set\nFinally, let's also evaluate your model on test data and see how it performs!","da14e809":"let's first train our autoencoder then we can use it for classification  ","c8e2575d":"Next, we will train the model with early stopping callback.","2c5edb6a":"# Encoder Architecture , first we will train our autoencoder then we will use our encoder layer after to pass it's output to Dense layers","1baef079":"\n## 2. Implementation\n\n## 2.1 UseCase 1 : Image Reconstruction\n\n1. Load the required libraries\n","9030df8a":"In this autoencoder network, we will add convolutional layers because convolutional networks works really well with the image inputs. To apply convolutions on image data, we will reshape our inputs in the form of 28 * 28 matrix. ","fc4ed32e":"# image resconstructed from noisefull images","b6e44a87":"Here is the summary of our autoencoder architecture.","8dc8b6f2":"# noisefull images in validation set","135d3d0c":"## it is very simple most the people will understand by seeing which layer of autoencoder we will be using pass to the CNN ? \n\n## answer is simple that encoder layer we will be using passing to top layer of CNN \n## we will freeze the layers before CNN becuse encoder is already trained . ","8b697501":"Train the model with early stopping callback. Increase the number of epochs to a higher number for better results. ","1114437c":"## Note: The next step is pretty important. In order to be sure whether the weights of the encoder part of the autoencoder are similar to the weights you loaded to the encoder function of the classification model, you should always print any one of the same layers weights of both the models. If they are not similar, then there is no use in using the autoencoder classification strategy.","c853bffc":"Lets plot the original and predicted image\n\n**Inputs: Actual Images**","362db5df":"## Let's visualize the layers that you created in the above step by using the summary function. This will show a number of parameters (weights and biases) in each layer and also the total parameters in your model.","2362da3f":"### 3. Create Autoencoder architecture\n\nIn this section, lets create an autoencoder architecture. The encoding part comprises of three layers with 2000, 1200, and 500 nodes. Encoding architecture is connected to latent view space comprising of 10 nodes which is then connected to decoding architecture with 500, 1200, and 2000 nodes. The final layer comprises of exact number of nodes as the input layer.","b17ca6e7":"## so both are having same weights","6adad0c8":"small dictionary for showing pics in dataset ","36154b5f":"Before adding noise","2df5d11a":"Lets now create the model architecture for the autoencoder. Lets understand what type of network needs to be created for this problem. \n\n**Encoding Architecture:**   \n\nThe encoding architure is composed of 3 Convolutional Layers and 3 Max Pooling Layers stacked one by one. Relu is used as the activation function in the convolution layers and padding is kept as \"same\". Role of max pooling layer is to downsample the image dimentions. This layer applies a max filter to non-overlapping subregions of the initial representation.  \n\n**Decoding Architecture:**   \n\nSimilarly in decoding architecture, the convolution layers will be used having same dimentions (in reverse manner) as the encoding architecture. But instead of 3 maxpooling layers, we will be adding 3 upsampling layers. Again the activation function will be same (relu), and padding in convolution layers will be same as well.  Role of upsampling layer is to upsample the dimentions of a input vector to a higher resolution \/ dimention. The max pooling operation is non-invertible, however an approximate inverse can be obtained by recording the locations of the maxima within each pooling region. Umsampling layers make use of this property to project the reconstructions from a low dimentional feature space.   \n\n","2369a662":"### 2. Dataset Prepration \n\nLoad the dataset, separate predictors and target, normalize the inputs.","b7e4d9fe":"### Noisy Images \n\nWe can intentionally introduce the noise in an image. I am using imaug package which can be used to augment the images with different variations. One such variation can be introduction of noise. Different types of noises can be added to the images. For example: \n\n- Salt and Pepper Noise  \n- Gaussian Noise  \n- Periodic Noise  \n- Speckle Noise  \n\nLets introduce salt and pepper noise to our data which is also known as impulse noise. This noise introduces sharp and sudden disturbances in the image signal. It presents itself as sparsely occurring white and black pixels. \n","d877e6be":"## so shape of our data -> then build encoder layer ->pass it to CNN let's see how we can do it ","aeae10fc":"So we can see that an autoencoder trained with 20 epoochs is able to reconstruct the input images very well. Lets look at other use-case of autoencoders - Image denoising or removal of noise from the image.  \n\n## 2.2 UseCase 2 - Image Denoising\n\nAutoencoders are pretty useful, lets look at another application of autoencoders - Image denoising. Many a times input images contain noise in the data, autoencoders can be used to get rid of those images. Lets see it in action. First lets prepare the train_x and val_x data contianing the image pixels. \n\n![](https:\/\/www.learnopencv.com\/wp-content\/uploads\/2017\/11\/denoising-autoencoder-600x299.jpg)","bd6e2ee3":"## Classification Report\nClassification report will help you in identifying the misclassified classes in more detail. You will be able to observe for which class the model performed bad out of the given ten classes.\n\n","a18fa2fd":"## Till now we have learnt how to develop autoencoder from scrath not the basic but also a deep one , we understand architecture required to build that \n## So now from here we will start how we can use Auto Encoder we can use to classify images in 10 categories","fc2bfc43":"After adding noise","7ffd9def":"**Predicted : Autoencoder Output**","4b4d5d71":"![](https:\/\/www.spiedigitallibrary.org\/ContentImages\/Journals\/JEIME5\/25\/2\/023018\/FigureImages\/JEI_25_2_023018_f001.png)","19eed8cf":"## How Autoencoders work - Understanding the math and implementation\n\n### Contents \n\n<ul>\n<li>1. Introduction<\/li>\n<ul>\n    <li>1.1 What are Autoencoders ? <\/li>\n    <li>1.2 How Autoencoders Work ? <\/li>\n<\/ul>\n<li>2. Implementation and UseCases<\/li>\n<ul>\n    <li>2.1 UseCase 1: Image Reconstruction <\/li>\n    <li>2.2 UseCase 2: Noise Removal <\/li>\n<\/ul>\n<\/ul>\n\n<br>\n\n## 1. Introduction\n## 1.1 What are Autoencoders \n\nAutoencoders are a special type of neural network architectures in which the output is same as the input. Autoencoders are trained in an unsupervised manner in order to learn the exteremely low level repersentations of the input data. These low level features are then deformed back to project the actual data. An autoencoder is a regression task where the network is asked to predict its input (in other words, model the identity function). These networks has a tight bottleneck of a few neurons in the middle, forcing them to create effective representations that compress the input into a low-dimensional code that can be used by the decoder to reproduce the original input.\n\nA typical autoencoder architecture comprises of three main components: \n\n- **Encoding Architecture :** The encoder architecture comprises of series of layers with decreasing number of nodes and ultimately reduces to a latent view repersentation.  \n- **Latent View Repersentation :** Latent view repersents the lowest level space in which the inputs are reduced and information is preserved.  \n- **Decoding Architecture :** The decoding architecture is the mirro image of the encoding architecture but in which number of nodes in every layer increases and ultimately outputs the similar (almost) input.  \n\n![](https:\/\/i.imgur.com\/Rrmaise.png)\n\nA highly fine tuned autoencoder model should be able to reconstruct the same input which was passed in the first layer. In this kernel, I will walk you through the working of autoencoders and their implementation.  Autoencoders are widly used with the image data and some of their use cases are: \n\n- Dimentionality Reduction   \n- Image Compression   \n- Image Denoising   \n- Image Generation    \n- Feature Extraction  \n\n\n\n## 1.2 How Autoencoders work \n\nLets understand the mathematics behind autoencoders. The main idea behind autoencoders is to learn a low level repersenation of a high level dimentional data. Lets try to understand the encoding process with an example.  \nData compression is a big topic that\u2019s used in computer vision, computer networks, computer architecture, and many other fields. The point of data compression is to convert our input into a smaller representation that we recreate, to a degree of quality. This smaller representation is what would be passed around, and, when anyone needed the original, they would reconstruct it from the smaller representation.\n\nConsider a ZIP file. When we create a ZIP file, we compress our files so that they take up fewer bytes. Then we pass around that ZIP file. If we wanted to access the contents, we can uncompress the ZIP file, and reconstruct the contents from the ZIP file.\n\nIn another example, consider a JPEG image file. This is an example of a lossy format: when we compress a JPEG, we lose information about the original. If we uncompress it, then our reconstruction isn\u2019t perfect. However, for JPEG, we can compress it down to a tenth of the original data without any noticeable loss in image quality!\n\nMany techniques in the past have been hard-coded or use clever algorithms. Autoencoders are unsupervised neural networks that use machine learning to do this compression for us. There are many different kinds of autoencoders that we\u2019re going to look at: vanilla autoencoders, deep autoencoders, deep autoencoders for vision. Finally, we\u2019ll apply autoencoders for removing noise from images.\n\nVanilla Autoencoder\nWe\u2019ll first discuss the simplest of autoencoders: the standard, run-of-the-mill autoencoder. Essentially, an autoencoder is a 2-layer neural network that satisfies the following conditions.\n\nThe hidden layer is smaller than the size of the input and output layer.\nThe input layer and output layer are the same size.\n\n\nThe hidden layer is compressed representation, and we learn two sets of weights (and biases) that encode our input data into the compressed representation and decode our compressed representation back into input space.\n\nNotice that there are no labels! Our input and output are the same! But then what is our loss function? We have a simple Euclidean distance loss: ||\\mathbf{x} - \\mathbf{\\hat{x}}||^2 called the reconstruction error, where the input is \\mathbf{x} and the reconstruction is \\mathbf{\\hat{x}}. We want to minimize this error. In other words, this error represents how close our reconstruction was to the true input data. We won\u2019t expect a perfect reconstruction since the number of hidden neurons is less than the number of input neurons, but we want the parameters to give us the best possible reconstruction.\n\nMathematically, our above autoencoder can be thought of as two separate things: an encoder and decoder.\n# #\nwhere the superscripts correspond to the encoder and decoder and the input is \\mathbf{x}. Hence, our loss function will be the squared Euclidean error.\n\nWhen we train our autoencoder, we\u2019re trying to minimize \\mathcal{L}. We won\u2019t see the backpropagation derivation of the update rules, but they\u2019re identical to a standard neural network.\n# #\nThe big catch with autoencoders is that they only work for the data we train them on! If we train our autoencoder on images of cats, then it won\u2019t work too well for images of dogs!\n\nThis is all there is to autoencoders! They\u2019re simple neural networks but also very powerful! Let\u2019s code up an autoencoder."}}