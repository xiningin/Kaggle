{"cell_type":{"831c1ad3":"code","08a692e3":"code","4a0cec11":"code","20b74877":"code","487a3146":"code","a420a35d":"code","2f3fa5c4":"code","cdee3423":"code","3d16a666":"code","edf96340":"code","442875e0":"code","b2722797":"code","3bee7397":"code","d59b3422":"code","97c20be3":"code","567ba9f8":"code","cdb27527":"code","193b6c60":"code","3f2579eb":"code","9c8c1a02":"code","6aeead24":"code","c82ad10b":"code","e9f6ac59":"code","18e9b2d6":"code","d4487fb7":"code","7e4360d9":"code","3d2103d3":"code","2ad60efe":"code","bd8ca1c7":"code","dc971a5a":"code","6589e10d":"code","2fb3ba51":"code","b92dfd3c":"code","17ce842a":"markdown","1075d0e8":"markdown","332da8e0":"markdown","14e359b7":"markdown","b6095649":"markdown","3c6360e2":"markdown","1f3afb75":"markdown","8c66bdc6":"markdown","d190fc88":"markdown","376c9067":"markdown","ee5620e2":"markdown","97427f28":"markdown","7bb3479e":"markdown","00e6a643":"markdown","6519eee4":"markdown","ba5e5b98":"markdown","1a0b4c49":"markdown","4cf97f66":"markdown","84b66957":"markdown","b3356046":"markdown","5125f309":"markdown","e9a04df7":"markdown","852bf49d":"markdown"},"source":{"831c1ad3":"import numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom math import sqrt\n\nimport gc\nimport time\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\n","08a692e3":"import geojson\nimport geopandas as gpd\nfrom fiona.crs import from_epsg\nimport os, json\nfrom shapely.geometry import shape, Point, Polygon, MultiPoint\nfrom geopandas.tools import sjoin\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt # plotting\nimport seaborn as sns; sns.set()\n\nfrom IPython.display import Image\n\nimport folium\n\nfrom branca.colormap import  linear\nimport json\nimport branca.colormap as cm","4a0cec11":"from numpy.random import seed\n\n# Reproducability\ndef set_seed(seed=31415):\n    \n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    \nset_seed(31415)","20b74877":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","487a3146":"df_belgium = gpd.read_file('\/kaggle\/input\/belgium-obu\/Belgium_streets.json')\n\nm = folium.Map([50.85045, 4.34878], zoom_start=9, tiles='cartodbpositron')\nfolium.GeoJson(df_belgium).add_to(m)\nm","a420a35d":"# BXL_timeseries_kaggle.csv may have more rows in reality, but we are only loading\/previewing the first 1000 rows\nnew_table = pd.read_csv('..\/input\/obu-data-preprocessing\/Flow_BEL_street_30min.csv')\nnRow, nCol = new_table.shape\nprint(f'There are {nRow} rows and {nCol} columns')","2f3fa5c4":"mean_value = 10","cdee3423":"table_index = new_table.iloc[:,1:]\nALL_STREETS = list(table_index.columns.values)\n\nmean_flow =[]\nnew_street=[]\n\n\nfor street in ALL_STREETS:\n    single_street=table_index[street]\n    mean = np.mean(single_street)\n    mean_flow.append(mean)\n    new_street.append(street)\n    \n    \ndf_mean_flow = pd.DataFrame({'street_index':new_street, 'mean_flow': mean_flow})\nprint('')\nprint(df_mean_flow.head())\nprint('')\n\nSTREETS = df_mean_flow[(df_mean_flow['mean_flow']>= mean_value)] \nSTREETS = STREETS.sort_values(by=['street_index'])\nSTREETS = list(STREETS.street_index)\n\nprint('considering a average traffic flow of ' + str(mean_value)+' per street')\nprint('')\nprint('mean traffic flow '+str(mean_value)+ ' ---> number of street segments: ' + str(len(STREETS)))\n","3d16a666":"new_table['Datetime'] = pd.to_datetime(new_table['datetime'])\n\nDATAFRAME = new_table\nDATAFRAME = DATAFRAME.drop(['datetime'],axis=1) \nDATAFRAME = DATAFRAME[DATAFRAME.columns.intersection(STREETS)]\n\n# Auxiliary\n\nDATAFRAME['minutes'] = new_table['Datetime'].dt.minute\nDATAFRAME['hour'] = new_table['Datetime'].dt.hour\n\nDATAFRAME['hour_x']=np.sin(DATAFRAME.hour*(2.*np.pi\/23))\nDATAFRAME['hour_y']=np.cos(DATAFRAME.hour*(2.*np.pi\/23))\n\nDATAFRAME['day'] = new_table['Datetime'].dt.day\n\nDATAFRAME['DayOfWeek'] = new_table['Datetime'].dt.dayofweek\nDATAFRAME['WorkingDays'] = DATAFRAME['DayOfWeek'].apply(lambda y: 2 if y < 5 else y)\nDATAFRAME['WorkingDays'] = DATAFRAME['WorkingDays'].apply(lambda y: 1 if y == 5 else y)\nDATAFRAME['WorkingDays'] = DATAFRAME['WorkingDays'].apply(lambda y: 0 if y == 6 else y)\n\nDATAFRAME = DATAFRAME.drop(['minutes','hour','day'],axis=1)\n\n# temporal features = 4\nfeat_time = 4\n\nDATAFRAME.head()\n\n","edf96340":"STREETS = [int(float(s)) for s in STREETS]\n\n\ndf_belgium = df_belgium[df_belgium.index.isin(STREETS)]\ndf_belgium['Trucks_Flow'] =  DATAFRAME.iloc[2182,:-4].astype(float).values\n\nnbh_count_colormap = linear.YlOrRd_09.scale(0,200)\n\ncolormap_dept = cm.StepColormap(\n    colors=['#00ae53', '#86dc76', '#daf8aa',\n            '#ffe6a4', '#ff9a61', '#ee0028'],\n    vmin = 0,\n    vmax = 200,\n    index=[0, 20, 50, 80, 110, 150, 180])\n\npolygons = df_belgium\nm = folium.Map([50.85045, 4.34878], zoom_start= 9, tiles='cartodbpositron')\n\nstyle_function = lambda x: {\n    'fillColor': colormap_dept(x['properties']['Trucks_Flow']),\n    'color': colormap_dept(x['properties']['Trucks_Flow']),\n    'weight': 1.5,\n    'fillOpacity': 1\n}\nfolium.GeoJson(polygons,\n    style_function=style_function).add_to(m)\n\n\ncolormap_dept.caption = 'Traffic Flow (N#Trucks\/30min) at (not real) 12:00 a.m.'\ncolormap_dept.add_to(m)\n\nm","442875e0":"test_step = 168*2*2 # 1 WEEK\n\n# ATTENTION: anything you learn and is not known in advance, must be learnt only from training data!\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler_aux = MinMaxScaler(feature_range=(0, 1))\n\n# TRAINING --- (scaler\/scaler_aux).fit_transform()\n# TESTING --- (scaler\/scaler_aux).transform()\n\n# TRAINING SET\nTRAIN = DATAFRAME[: -test_step ]\ntrain_feat = scaler.fit_transform(TRAIN.values[:,:-feat_time])\n\n\n# TESTING SET\nTEST = DATAFRAME[-test_step:]\ntest_feat = scaler.transform(TEST.values[:,:-feat_time])\n\n\n# AUX are known in advance\nAUX = scaler_aux.fit_transform(DATAFRAME.values[:,-feat_time:])\ntrain_aux = AUX[: -test_step ]\ntest_aux = AUX[-test_step:]\n\n\n# concate final results\ntrain_feat = np.hstack([train_feat, train_aux])\ntest_feat = np.hstack([test_feat, test_aux])","b2722797":"def inverse_transform(forecasts, scaler):\n    # invert scaling\n    inv_pred = scaler.inverse_transform(forecasts)\n    return inv_pred","3bee7397":"nRow, nCol = DATAFRAME.shape\n\nplt.figure(figsize=(20,10))\nplt.plot(np.mean(TEST.iloc[:,:-feat_time],axis=1))\nplt.title('TESTING SET')\nplt.show()\n\nprint(f'Consider {nRow} instances (rows) and {nCol} streets segments (columns)')\nprint('')\nprint('TRAIN SIZE: '+ str(TRAIN.shape))\nprint('')\nprint('TEST SIZE: '+ str(TEST.shape))\n\n","d59b3422":"Image(\"\/kaggle\/input\/image-lstm\/Autoregressive.png\")","97c20be3":"Image(\"\/kaggle\/input\/image-lstm\/DATAPREP.png\")","567ba9f8":"def prep_data(dataframe, INPUT, BATCH):\n    \n    OUTPUT = INPUT\n    TOTAL = INPUT + OUTPUT\n    \n    dataset = tf.data.Dataset.from_tensor_slices(dataframe)\n    \n#     dataset = dataset.window(TOTAL,  shift=1,  stride=1,  drop_remainder=True)\n#     dataset = dataset.flat_map(lambda window: window.batch(TOTAL))   \n#     dataset = dataset.map(lambda window:( window[:-OUTPUT], window[-OUTPUT:])) #((window[:-OUTPUT],window[-OUTPUT:,-AUX:]), window[-OUTPUT:]))\n#     dataset = dataset.batch(BATCH).prefetch(tf.data.experimental.AUTOTUNE)\n    \n\n    # features\n    feat = dataset.window(INPUT,  shift=1,  stride=1,  drop_remainder=True)\n    feat = feat.flat_map(lambda window: window.batch(INPUT))\n\n    # targets\n    label = dataset.window(OUTPUT, shift=1,  stride=1,  drop_remainder=True).skip(INPUT)\n    label = label.flat_map(lambda window: window.batch(OUTPUT))\n    \n    dataset = tf.data.Dataset.zip((feat, label))\n    dataset = dataset.batch(BATCH).cache().prefetch(tf.data.experimental.AUTOTUNE) #,  drop_remainder=True\n\n    return dataset","cdb27527":"total_features = len(DATAFRAME.columns) \n\nsize_input = 12\nbatch_size = 256\nbatch_train = batch_size\nbatch_test = 1\n\n\nwindowed_train = prep_data(train_feat, size_input, batch_train)\nwindowed_test = prep_data(test_feat, size_input, batch_test)\n\nlatent_dim = 356\n","193b6c60":"Image(\"\/kaggle\/input\/image-lstm\/The-structure-of-the-LSTM-unit.png\")","3f2579eb":"Image(\"\/kaggle\/input\/image-lstm\/LSTM.jpg\")","9c8c1a02":"from tensorflow.keras import regularizers\n\nclass FeedBack_LSTM(tf.keras.Model):\n    \n    def __init__(self, units, sz_input, tot_feat):\n        \n        super(FeedBack_LSTM, self).__init__()\n        self.tot_feat = tot_feat\n        self.units = units\n        self.inp = sz_input\n        \n        \n        self.cell = tf.keras.layers.LSTMCell(self.units, \n                                        kernel_initializer='glorot_uniform',\n                                        recurrent_initializer='glorot_uniform',\n                                        kernel_regularizer=regularizers.l2(0.001),\n                                        bias_initializer='zeros') #, dropout=0.1, recurrent_dropout=0.1)\n        #dropout #recurrent_dropout\n        \n        self.lstm = tf.keras.layers.RNN(self.cell, return_state = True)\n        \n        self.dense_0 = tf.keras.layers.Dense(150, activation='relu',\n                                           kernel_regularizer=regularizers.l2(0.001))\n    \n                \n        self.dense = tf.keras.layers.Dense(self.tot_feat,\n                                           kernel_regularizer=regularizers.l2(0.001))\n        \n    def warmup(self, x):\n        \n        output_lstm, *state = self.lstm(x)\n        \n        dense_0 = self.dense_0(output_lstm)\n        \n        output_dense = self.dense(dense_0)\n        \n        return output_dense, state\n    \n    def call(self, inputs, targs):\n        \n        # Use a TensorArray to capture dynamically unrolled outputs.\n        predictions = []\n\n        # Initialize the lstm state\n        prediction, state = self.warmup(inputs)\n        \n        # Insert the first prediction\n        predictions.append(prediction)\n\n        \n        # Run the rest of the prediction steps\n        for n in range(1, self.inp):\n            \n            # Use the last prediction as input.\n            x =  prediction #targs[:, n-1, :] #\n            \n            # Execute one lstm step.\n            x, state = self.cell(x, states=state, training=True)\n            \n            dense_0 = self.dense_0(x)\n        \n            prediction = self.dense(dense_0)\n        \n            # Add the prediction to the output\n            predictions.append(prediction)\n\n\n        # predictions.shape => (time, batch, features)\n        predictions = tf.stack(predictions)\n\n        # predictions.shape => (batch, time, features)\n        predictions = tf.transpose(predictions, [1, 0, 2])\n    \n        return predictions\n    \n    def inference(self, inputs):\n        \n        # Use a TensorArray to capture dynamically unrolled outputs.\n        predictions = []\n\n        # Initialize the lstm state\n        prediction, state = self.warmup(inputs)\n        \n\n        # Insert the first prediction\n        predictions.append(prediction)\n\n        # Run the rest of the prediction steps\n        for n in range(1, self.inp):\n            \n            # Use the last prediction as input.\n            x = prediction\n            \n            # Execute one lstm step.\n            x, state = self.cell(x, states=state, training=False)\n            \n            dense_0 = self.dense_0(x)\n        \n            prediction = self.dense(dense_0)\n        \n            # Add the prediction to the output\n            predictions.append(prediction)\n\n\n        # predictions.shape => (time, batch, features)\n        predictions = tf.stack(predictions)\n\n        # predictions.shape => (batch, time, features)\n        predictions = tf.transpose(predictions, [1, 0, 2])\n    \n        return predictions\n    \n    \n","6aeead24":"# input for LSTM\ninputs = tf.keras.Input(shape=(size_input, total_features), name='inputs')\nprint(' INPUT SHAPE for LSTM: { batch size, input sequence, features size}')\ninputs","c82ad10b":"FEEDBACK_lstm = FeedBack_LSTM(latent_dim, size_input, total_features)\n\noutput = FEEDBACK_lstm(inputs, inputs)\n\nprint('-----------')\nprint('Model Summary')\nprint('-----------')\nFEEDBACK_lstm.summary()\n\n\nprint(' OUTPUT SHAPE for LSTM: { batch size, output sequence, features size}')\noutput\nprint('----')\n\n\n","e9f6ac59":"optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n\n# comment for now\n# checkpoint_dir = '.\/training_checkpoints'\n# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n# checkpoint = tf.train.Checkpoint(optimizer = optimizer, lstm = lstm )","18e9b2d6":"loss_object = tf.keras.losses.MeanAbsoluteError()\n\ndef loss_function(real, pred):\n    \n    loss_ = loss_object(real, pred)\n\n    return tf.reduce_mean(loss_)","d4487fb7":"\n@tf.function\ndef train_step(inp, targ):\n\n    loss = 0\n\n    with tf.GradientTape() as tape:\n        \n        predictions = FEEDBACK_lstm(inp, targ)\n\n        loss += loss_function(targ, predictions)\n   \n    batch_loss = loss \n    \n    variables = FEEDBACK_lstm.trainable_variables \n\n    gradients = tape.gradient(loss, variables)\n\n    optimizer.apply_gradients(zip(gradients, variables))\n    \n    return batch_loss","7e4360d9":"EPOCHS = 200\n\nsteps_per_epoch = len(TRAIN) \/\/ batch_size\n\n# Keep results for plotting\ntrain_loss_results = []\ntrain_rmse_accuracy_results = []\n\nprint('')\nprint('TRAINING')\nprint('')\n\nfor epoch in range(EPOCHS):\n    \n    start = time.time()\n    \n    epoch_loss_avg = tf.keras.metrics.Mean()\n    epoch_rmse = tf.keras.metrics.MeanSquaredError()\n\n    total_loss = 0\n\n    for (batch, (inp, targ)) in enumerate(windowed_train.take(steps_per_epoch)):\n\n        batch_loss = train_step(inp, targ)\n\n        # Track progress\n        epoch_loss_avg.update_state(batch_loss)  # Add current batch loss\n        epoch_rmse.update_state(targ, FEEDBACK_lstm(inp, targ))\n        \n    # End epoch\n    train_loss_results.append(epoch_loss_avg.result())\n    train_rmse_accuracy_results.append(epoch_rmse.result())\n    \n    \n\n    if epoch % 10 == 0:\n        print(\"Epoch {}: Loss MAE: {:.3f}, Accuracy RMSE: {:.3f}\".format(epoch, epoch_loss_avg.result(),\n                                                                epoch_rmse.result()))\n\n          \n          \nprint('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","3d2103d3":"fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\nfig.suptitle('Training Metrics')\n\naxes[0].set_ylabel(\"Loss (MAE)\", fontsize=14)\naxes[0].plot(train_loss_results)\n\naxes[1].set_ylabel(\"Accuracy (RMSE)\", fontsize=14)\naxes[1].set_xlabel(\"Epoch\", fontsize=14)\naxes[1].plot(train_rmse_accuracy_results)\nplt.show()","2ad60efe":"# input for LSTM\ninputs_test = tf.keras.Input(shape=(size_input, total_features), name='inputs')\nprint(' INPUT SHAPE for LSTM: { batch size, input sequence, features size}')\ninputs_test\n","bd8ca1c7":"def evaluate_forecasts(targets, forecasts, n_seq):\n    \n    list_rmse = []\n    list_mae = []\n    \n    for i in range(n_seq):\n        true = np.vstack([target[i] for target in targets])\n        predicted = np.vstack([forecast[i] for forecast in forecasts])\n        \n        rmse = np.sqrt((np.square(true - predicted)).mean(axis=0))\n        mae = np.absolute(true - predicted).mean(axis=0)\n        \n        list_rmse.append(rmse)\n        list_mae.append(mae)\n        \n    list_rmse = np.vstack(list_rmse)\n    list_mae = np.vstack(list_mae)\n    \n    return list_rmse, list_mae","dc971a5a":"forecasts = []\ntargets = []\n\nrmse_list = []\nmae_list = []\n\n    \nfor (step, (inp, targ)) in enumerate(windowed_test):\n\n        pred  = FEEDBACK_lstm.inference(inp)\n        \n        truth = inverse_transform(targ[0][:,:-feat_time],  scaler)\n        pred = inverse_transform(pred[0][:,:-feat_time],  scaler)\n        \n        forecasts.append(pred)\n        targets.append(truth)\n        \n        rmse, mae = evaluate_forecasts(targets, forecasts, 12)\n           \n        rmse_list.append(rmse)\n        mae_list.append(mae)\n           \n        plt.plot(np.sum(pred, axis=1), label='Prediction') \n        plt.plot(np.sum(truth, axis=1), label='Truth') \n#         plt.ylim(-1, 150)\n        plt.title('Average Prediction on all highways in Belgium')\n        plt.legend()\n        plt.show()\n        \n        print('* Time step '+str(step))\n        print('* Prediction Accuracy (MAE) '+ str(np.absolute(truth - pred).mean()))\n        print('----')\n        print('* After prediction UPDATE model with new streets observations')\n        \n        new_instance = test_feat[step,:].reshape(1,-1)\n    \n        train_feat = np.vstack([train_feat, new_instance])\n    \n        windowed_new = prep_data(train_feat, size_input, batch_size) \n\n        update_steps_per_epoch = len(train_feat)\/\/batch_size\n        \n        UPDATE = 2\n        \n        for epoch in range(UPDATE):\n            \n            # resetting the hidden state at the start of every epoch if state_train = True\n#             lstm.reset_states()\n            \n            for (batch, (inp_new, targ_new)) in enumerate(windowed_new.take(update_steps_per_epoch )):\n\n                batch_loss = train_step(inp_new, targ_new)\n\n                # Track progress\n                epoch_loss_avg.update_state(batch_loss)  # Add current batch loss\n                epoch_rmse.update_state(targ_new, FEEDBACK_lstm(inp_new, targ_new))\n                \n            # End epoch\n            train_loss_results.append(epoch_loss_avg.result())\n            train_rmse_accuracy_results.append(epoch_rmse.result())\n\n\n            if epoch % UPDATE == 0:\n                print(\"UPDATE - Epoch {}: Loss MAE: {:.3f}, Accuracy RMSE: {:.3f}\".format(epoch,\n                                                                        epoch_loss_avg.result(),\n                                                                        epoch_rmse.result()))\n\n","6589e10d":"RMSE_MEAN = np.mean(rmse_list,axis=0).mean(axis=1)\nRMSE_STD =  np.std(rmse_list,axis=0).std(axis=1)\n\nfor i in range(len(RMSE_MEAN)):\n    print('t+'+str(i+1)+' RMSE MEAN ' +str(np.round(RMSE_MEAN[i],3))+' +- '+str(np.round(RMSE_STD[i],3)))\n    print('')","2fb3ba51":"MAE_MEAN = np.mean(mae_list,axis=0).mean(axis=1)\nMAE_STD =  np.std(mae_list,axis=0).std(axis=1)\n\nfor i in range(len(MAE_MEAN)):\n    print('t+'+str(i+1)+' MAE MEAN ' +str(np.round(MAE_MEAN[i],3))+' +- '+str(np.round(MAE_STD[i],3)))\n    print('')","b92dfd3c":"import pickle\n\n# Saving the objects:\nwith open('save_predictions_results.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n    pickle.dump([rmse_list, mae_list], f)","17ce842a":"### loss function","1075d0e8":"### optimizer","332da8e0":"# LSTM model - Multivariate Multiple-step ahead Prediction Model","14e359b7":"### - *@tf.function decorator* - to speed-up training","b6095649":"### SEED","3c6360e2":"# Plot Training Progress","1f3afb75":"# SPLITTING Training\/Testing","8c66bdc6":"# ADD Auxiliary Temporal Features","d190fc88":"# TEST and UPDATE MODEL","376c9067":"## Check files","ee5620e2":"# Visualize Traffic Flow at particular time","97427f28":"## General Import","7bb3479e":"# Visualize Streets Network","00e6a643":"# TRAIN MODEL","6519eee4":"## LSTM Architecture","ba5e5b98":"### Define LSTM for training\n\nthe batch size for training the lstm model is 32. (different from testing as we will see below)","1a0b4c49":"## LSTM Cell","4cf97f66":"# Traffic Flow Predictions with LSTM model\n\n## The goal is to predict traffic flow for multiple steps ahead for all the highways in Belgium","84b66957":"## Autoregressive Approach","b3356046":"# SELECT STREETS BASED ON AVERAGE TRAFFIC FLOW","5125f309":"### Define LSTM for prediction\n\nwe define the same lstm model for prediction: the only difference here is the size, batch_test = 1","e9a04df7":"## PARAMETERS","852bf49d":"## DATA PREPARATION\n### * {BATCH_SIZE, INPUT_SEQUENCE (OUTPUT), FEATURES_SIZE}"}}