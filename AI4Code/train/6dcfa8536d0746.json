{"cell_type":{"38d8080f":"code","ee98eaa2":"code","510047eb":"code","043197ec":"code","a3a09ce8":"code","59c44c71":"code","90e58cb6":"code","2b68cb81":"code","0bf0b50e":"code","e7b6311a":"code","f89ef8dd":"code","adee0418":"code","64ef5faf":"code","04854557":"code","9fcd0e69":"code","e8c371f3":"code","93a32534":"code","2ec491eb":"code","d7256b0d":"code","246a94e2":"code","4949b318":"code","1ca697e8":"code","3581d845":"code","103ca2a0":"code","7108cf9e":"code","ea53d654":"code","993b3684":"code","fce40b27":"code","e3eaf9f7":"code","34136103":"code","39ffbe52":"code","c8b77c42":"code","3147fb58":"code","5cdc5f28":"code","c89b6a81":"code","df189505":"code","bd4a1b4c":"code","d41fef55":"code","d374d2f2":"code","bc0c0dd3":"code","b8878bad":"code","f8294648":"markdown","00d5da73":"markdown","6eaa452d":"markdown","4afa649e":"markdown","f080e359":"markdown","00e9b79d":"markdown","d3a9c2ad":"markdown","71baa86a":"markdown","5080e658":"markdown","c6fd9462":"markdown","47daa132":"markdown","e7cffeaf":"markdown","18e47be7":"markdown","4a953bff":"markdown","ef5b4a87":"markdown","8942b678":"markdown","109c293e":"markdown","e1021eea":"markdown","a95a2d54":"markdown","595d2876":"markdown","4e87ab28":"markdown","0ae400a5":"markdown","8720e5bb":"markdown","7bff829f":"markdown","0a6870de":"markdown","8876b37b":"markdown","2fe9073e":"markdown","50444540":"markdown","865edaab":"markdown","3fa71061":"markdown","db691669":"markdown","ef40b78f":"markdown","ea5e654c":"markdown","1643a700":"markdown","6ffe3267":"markdown","7422a0a3":"markdown"},"source":{"38d8080f":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","ee98eaa2":"df  = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf.head()","510047eb":"df.columns","043197ec":"df.shape","a3a09ce8":"df.isnull().sum()","59c44c71":"df['id'].value_counts()","90e58cb6":"y = df.diagnosis\ndf.drop(['id','diagnosis','Unnamed: 32'],inplace=True,axis=1)","2b68cb81":"df.info()","0bf0b50e":"y.value_counts()","e7b6311a":"sns.countplot(x=y,data =df)","f89ef8dd":"## lets analyse for 10 features\ndiagnosis =y\ndata_stan = (df - df.mean())\/df.std() #standardisation\ndata = pd.concat([y,data_stan.iloc[:,:10]],axis=1) #taking first 10 values\ndata =pd.melt(data,id_vars='diagnosis',var_name='features',value_name='value') # melting the values in a single column to plot easily\nplt.figure(figsize=(10,10))\nsns.violinplot(x ='features',y='value',hue='diagnosis',data=data, split=True,inner='quart')\nplt.xticks(rotation =90)\n","adee0418":"## to understand what pd.melt does\npd.melt(data,id_vars='diagnosis',var_name='features',value_name='value')","64ef5faf":"# next 10 features\n\ndata = pd.concat([y,data_stan.iloc[:,10:20]],axis=1)\ndata =pd.melt(data,id_vars='diagnosis',var_name='features',value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x ='features',y='value',hue='diagnosis',data=data,split=True,inner='quart')\nplt.xticks(rotation =90)","04854557":"# next 10 features\n\ndata = pd.concat([y,data_stan.iloc[:,20:31]],axis=1)\ndata =pd.melt(data,id_vars='diagnosis',var_name='features',value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x ='features',y='value',hue='diagnosis',data=data,split=True,inner='quart')\nplt.xticks(rotation =90)","9fcd0e69":"data = pd.concat([y,data_stan.iloc[:,10:20]],axis=1)\ndata =pd.melt(data,id_vars='diagnosis',var_name='features',value_name='value')\nplt.figure(figsize=(10,10))\nsns.boxplot(x = 'features',y ='value',hue='diagnosis',data =data)\nplt.xticks(rotation = 90)","e8c371f3":"## lets use Joint plot\n\nsns.jointplot(df.loc[:,'radius_se'],df.loc[:,'perimeter_se'],kind='reg')","93a32534":"###  you can use pair plot for ploting major values\ng = sns.PairGrid(data_stan.iloc[:,10:15],diag_sharey=False)\n\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.map_upper(plt.scatter)\ng.map_diag(sns.kdeplot, lw=3)","2ec491eb":"import time\nsns.set(style=\"whitegrid\", palette=\"muted\")\ndiag = y\ndata = df\ndata_std = (data-data.mean())\/data.std()\ndata = pd.concat([diag,data_std.iloc[:,:10]],axis =1)\ndata = pd.melt(data,id_vars = 'diagnosis',value_name='value',var_name='features')\nplt.figure(figsize=(10,10))\ntic = time.time()\nsns.swarmplot(data=data,x='features',y='value',hue='diagnosis')\nplt.xticks(rotation=90)","d7256b0d":"# next 10 values\ndata = pd.concat([diag,data_std.iloc[:,10:20]],axis =1)\ndata = pd.melt(data,id_vars = 'diagnosis',value_name='value',var_name='features')\nplt.figure(figsize=(10,10))\ntic = time.time()\nsns.swarmplot(data=data,x='features',y='value',hue='diagnosis')\nplt.xticks(rotation=90)","246a94e2":"# next 10 values\ndata = pd.concat([diag,data_std.iloc[:,20:31]],axis =1)\ndata = pd.melt(data,id_vars = 'diagnosis',value_name='value',var_name='features')\nplt.figure(figsize=(10,10))\ntic = time.time()\nsns.swarmplot(data=data,x='features',y='value',hue='diagnosis')\nplt.xticks(rotation=90)","4949b318":"f,ax = plt.subplots(figsize =(18,18))\nsns.heatmap(df.corr(),annot= True)","1ca697e8":"df.columns","3581d845":"col_drop = ['perimeter_mean','area_mean','perimeter_worst', 'area_worst','perimeter_se','area_se','texture_worst','concavity_mean','concave points_mean','concavity_se','concavity_worst','fractal_dimension_se']","103ca2a0":"df.drop(col_drop,axis=1,inplace= True)","7108cf9e":"df.shape","ea53d654":"f,ax =plt.subplots(figsize=(14,14))\nsns.heatmap(df.corr(),annot=True)","993b3684":"df.drop('radius_worst',axis=1,inplace= True)","fce40b27":"df.shape","e3eaf9f7":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\nx_tr,x_te,y_tr,y_te = train_test_split(df,y,test_size=0.3,random_state=42)\n\nclf = RandomForestClassifier()\nclf = clf.fit(x_tr,y_tr)\nac = accuracy_score(y_te,clf.predict(x_te))\ncm= confusion_matrix(y_te,clf.predict(x_te))\nprint(ac)\nsns.heatmap(cm,annot=True,fmt='d')","34136103":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nsele_five = SelectKBest(chi2,k=5).fit(x_tr,y_tr)","39ffbe52":"print(sele_five.scores_)\nfor i in range(len(x_tr.columns)):\n    print(x_tr.columns[i], ':' ,sele_five.scores_[i])","c8b77c42":"X_tr = sele_five.transform(x_tr)\nX_te = sele_five.transform(x_te)","3147fb58":"RDclass = RandomForestClassifier()","5cdc5f28":"RDclass.fit(X_tr,y_tr)","c89b6a81":"print(accuracy_score(y_te,RDclass.predict(X_te)))","df189505":"sns.heatmap(confusion_matrix(y_te,RDclass.predict(X_te)),annot=True)","bd4a1b4c":"from sklearn.feature_selection import RFE\n\nRF_class_2 = RandomForestClassifier()\nSelect_5 = RFE(estimator=RF_class_2,n_features_to_select=5,step=1)\nSelect_5 = Select_5.fit(x_tr,y_tr)","d41fef55":"print(x_tr.columns[Select_5.support_])","d374d2f2":"from sklearn.feature_selection import RFECV\n\nRF_Class_3 = RandomForestClassifier()\n\nSelect_RFECV = RFECV(estimator=RF_Class_3, step = 1, cv = 5, scoring='accuracy')\n\nSelect_RFECV.fit(x_tr,y_tr)\n\nprint(Select_RFECV.n_features_)","bc0c0dd3":"print(x_tr.columns[Select_RFECV.support_])","b8878bad":"plt.figure()\nplt.xlabel('Features')\nplt.ylabel('Accuracy')\nplt.plot(range(1,len(Select_RFECV.grid_scores_)+1),Select_RFECV.grid_scores_)","f8294648":"we can see radious worst is corelated with radious mean so removing one feature\n\n","00d5da73":"The main objective of the data set is to find who is diagnosed with cancer","6eaa452d":"Lets Plot the cross validation scores ","4afa649e":"we can see at  7 the highest accuracy can be achived ","f080e359":"we can see id column has 569 values and it only refers the id in of the row. that doen't impact the dataset","00e9b79d":"lets find score for this five values","d3a9c2ad":"we have chossen our features but we dont know it is correct so lets do random forest","71baa86a":"Finding the Null Values","5080e658":"#### Feature Selection","c6fd9462":"We can see it has choosen the same features","47daa132":"we don't know much about this data, in order to understand the data, we have to do Feature Analysis, using mean,variance,standard deviation etc. \n\nUsually we check is there any correlation between the features,by analysing mean meadian of the values. \n\nfor this we can use voilin plot which helps to understand the data easily. but since there may be outliers we can use scaling meh=thods like standerdization, Min Max scaling to analyse the data ","e7cffeaf":"Here we can see texture_se,smoothness_se, symmetry_se are not suitable for classification","18e47be7":"we can see 5 features can give accuracy of 96%","4a953bff":"similary we can do for all the features to understand the features in the data","ef5b4a87":"we are droping unwanted columns from the table","8942b678":"unnamed 32 is a null value column so we can drop it","109c293e":"lets see the columns having categorical values","e1021eea":"radius_mean,texture_mean,radius_se, compactness_worst, concave points_worst","a95a2d54":"From the above we can see that we have increased the accuracy from 95% to 96%.because of few wrong prediction. \n\nlet see another selection method.\n","595d2876":"from above plot we can say radius_se,perimeter_se have similar box plot they might be coorelated, lets doa histogram to analyse these two data","4e87ab28":"we have seen many plots let use swam plot to get into next level","0ae400a5":"From this we can be sure about our feature selection, that we have made correct choice.\n\nLets do Recursive feature elimination with cross validation \n\n### Recursive feature elimination with cross validation and random forest classification","8720e5bb":"In previous method we found how many features we needed most by our own choice, \n\nBut in this method we can find how many features can give best accuracy and the choice is made by cross validation","7bff829f":"where (M = malignant, B = benign) Benign means not dangerous, Malignant means dangerous","0a6870de":"### Univariate feature Selection.\n\nUnivariate Analysis means Selecting features by comparing single feature to the target and finding the result","8876b37b":" We can see it selected  7 features out of all ","2fe9073e":"we have got accuracy arround .95 which is good but we can optimize more, by choosing correct feature","50444540":"We can also use box plot to analyse, but it is mostly used to analyse outliers in the features","865edaab":"we can see all are numerical value","3fa71061":"we can see in  fractal_dimension_mean meadian of Malignant and Bening are equal which says its not good for classification\n\nBut if you see other values there is huge diffrence in the median values of the categories which are good for classification","db691669":"in this radius_worst,perimeter_worst,area_worst,concavity_worst,concave points_worst can be easily classified, other features can't be classified \n","ef40b78f":"values near by 1 indicates higly corelated so we can drop those values.\n\nfor eg:\n    > radius_mean,perimeter_mean,area_mean are highly corelated so taking only one feature radius_mean\n    \n    > radius_worst,perimeter_worst, area_worst are corelated so taking radius_worst alone\n    \n    \n    > radius_se,perimeter_se,area_se are corelated so taking radius_se alone\n    \n    \n    > texture_mean and texture_worst are corelated so taking texture_mean alone\n    \n    \n    > Compactness_mean, concavity_mean and concave points_mean are corelated so taking  Compactness_mean\n    \n    \n    > Compactness_worst, concavity_worst and concave points_worst are corelated so taking  Compactness_worst\n    \n    > concavity_se, concave points_se are co realated \n   \n    > fractal_dimension_se,compactness_se ","ea5e654c":"Let understand the coorelation by classic method using heatmap, we could have done this directly, but it is important to understand the data set before getting into feature selection  ","1643a700":"### Recursive Feature Elemination Method\n\nThis methods uses the machine learning algoritham giiven by us, and assing weigghts to each feature, where smallest weights are cut down from the dataset by recursively going through all the features.","6ffe3267":"We select first 5 columns having max values ","7422a0a3":"from the above plot we can easily calssify area_mean,concave points_mean as M and D, but we can't classify fractal_dimension_mean, since D and M are scatterd all over the plot"}}