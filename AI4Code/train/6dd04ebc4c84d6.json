{"cell_type":{"c924f3ae":"code","deac9a00":"code","3f3e99df":"code","b00c57fd":"code","bafa11c7":"code","0586320c":"code","3f0f43c3":"code","d5804dc4":"code","d97aac42":"code","26ba61e5":"code","7198ace9":"code","e8a00826":"code","34eef88d":"code","3aef7cb6":"code","dd82366c":"code","0a8a989e":"code","2e71e99c":"code","83b346bf":"code","772cd7ce":"code","ba5baebd":"code","ad7a37a2":"code","3ce62abb":"code","b95942f1":"code","7f1c7e2d":"code","0f528469":"code","0b2c564c":"code","decf8422":"code","188906b8":"code","ee2a1345":"code","89770308":"code","c761c28b":"code","7a18266a":"code","ad60503e":"code","ca54d1d3":"code","4e2d7679":"code","ae5db2f3":"code","2f2225c8":"code","9d627a2b":"code","21e718ca":"code","94f6ff5b":"code","058ef013":"code","fabd7f6a":"code","a332b513":"code","309f7d4e":"markdown","7bd6db9e":"markdown","a5c1eb97":"markdown","e93d473e":"markdown","400d8e3c":"markdown","24c86392":"markdown","4835f2bc":"markdown","becdc758":"markdown","4e6657fb":"markdown","cb202afc":"markdown","70bc1d54":"markdown","b82b1b69":"markdown","793c06b7":"markdown","44db03ce":"markdown","0f00850d":"markdown","b8f7dfd4":"markdown","d8813c59":"markdown","f09bb8d8":"markdown","e1192155":"markdown","ca17f585":"markdown","859d2056":"markdown","62c19d44":"markdown","2bcfffef":"markdown","f7e97c3e":"markdown"},"source":{"c924f3ae":"#hide\n!pip install timm -q","deac9a00":"#collapse-hide\nfrom fastai.vision.all import *\nfrom fastai.tabular.all import *\nfrom fastai.medical.imaging import *\n\nfrom fastai.data.load import _FakeLoader, _loaders\nfrom timm import create_model\n\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import convert_color_space\n\nimport albumentations\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n","3f3e99df":"path_img = Path('..\/input\/siim-isic-melanoma-classification\/train')\npath_csv = Path('..\/input\/siim-isic-melanoma-classification\/train.csv')","b00c57fd":"df = pd.read_csv(path_csv)\ndf.head(5)","bafa11c7":"df['target'].value_counts()","0586320c":"dcmfiles = get_dicom_files(path_img);dcmfiles","3f0f43c3":"dcm = dcmfiles[0].dcmread();dcm","d5804dc4":"before = PILDicom.create(dcmfiles[0])\nshow_image(before)","d97aac42":"after = convert_color_space(np.array(before), 'YBR_FULL_422', 'RGB'); show_image(PILImage.create(after))","26ba61e5":"df0 = df[df['target'] == 0].reset_index(drop=True)\ndf1 = df[df['target'] == 1].reset_index(drop=True)\nlen(df0), len(df1)","7198ace9":"sel = np.random.choice(df0.index, len(df1)*4)\ndf0 = df0.loc[sel,:]","e8a00826":"df_small = pd.concat([df0,df1]).reset_index(drop=True)","34eef88d":"df_small['target'].value_counts()","3aef7cb6":"N_FOLDS = 3\nSEED = 101\nBS=32\ndf_small['fold'] = -1\n\nstrat_kfold = StratifiedKFold(n_splits=N_FOLDS, random_state=SEED, shuffle=True)\nfor i, (_, test_index) in enumerate(strat_kfold.split(df_small.image_name.values, df_small['target'].values)):\n    df_small.iloc[test_index, -1] = i\n    \ndf_small['fold'] = df_small['fold'].astype('int')","dd82366c":"fold = 0\nsplitter = [L(list(df_small.loc[df_small.fold!=fold].index)), L(list(df_small.loc[df_small.fold==fold].index))]\nprocs = [Categorify, FillMissing, Normalize]\ncat_names  = ['sex', 'anatom_site_general_challenge']\ncont_names = ['age_approx']\nto = TabularPandas(df_small, procs, \n                   cat_names, cont_names,\n                   y_names='target', y_block=CategoryBlock(),\n                   splits=splitter)","0a8a989e":"tab_dl = to.dataloaders(bs=BS)","2e71e99c":"tab_dl.show_batch()","83b346bf":"class AlbumentationsTransform(RandTransform):\n    \"A transform handler for multiple `Albumentation` transforms\"\n    split_idx, order = None, 2\n    def __init__(self, train_aug, valid_aug): store_attr()\n    \n    def before_call(self, b, split_idx):\n        self.idx = split_idx\n    \n    def encodes(self, img:PILImage):\n        if self.idx == 0:\n            aug_img = self.train_aug(image=np.array(img))['image']\n        else:\n            aug_img = self.valid_aug(image=np.array(img))['image']\n\n        return PILImage.create(aug_img)","772cd7ce":"def convert_tfm(img:PILDicom): \n    np_img = np.array(img)\n    converted_img = convert_color_space(np_img, 'YBR_FULL_422', 'RGB')\n    return PILImage.create(converted_img)","ba5baebd":"sz = 256\n\ndef get_train_aug(sz): return albumentations.Compose(\n    [\n        albumentations.Transpose(p=0.5),\n        albumentations.VerticalFlip(p=0.5),\n        albumentations.HorizontalFlip(p=0.5),\n        albumentations.RandomBrightness(limit=0.2, p=0.75),\n        albumentations.RandomContrast(limit=0.2, p=0.75),\n        \n        albumentations.OneOf([\n            albumentations.MotionBlur(blur_limit=5),\n            albumentations.MedianBlur(blur_limit=5),\n            albumentations.GaussianBlur(blur_limit=5),\n            albumentations.GaussNoise(var_limit=(5.0, 30.0)),\n        ], p=0.7),\n\n        albumentations.OneOf([\n            albumentations.OpticalDistortion(distort_limit=1.0),\n            albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n            albumentations.ElasticTransform(alpha=3),\n        ], p=0.7),\n\n        albumentations.CLAHE(clip_limit=4.0, p=0.7),\n        albumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n        albumentations.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85),\n        albumentations.Resize(sz, sz),\n        albumentations.Cutout(max_h_size=int(sz * 0.375), max_w_size=int(sz * 0.375), num_holes=1, p=0.7),\n    ])\n\ndef get_valid_aug(sz): return  albumentations.Compose(\n    [\n        albumentations.Resize(sz, sz),\n    ])","ad7a37a2":"item_tfms = [Resize(sz*2), Transform(convert_tfm), AlbumentationsTransform(get_train_aug(sz), get_valid_aug(sz))]\n#item_tfms = [Resize(128)]\nbatch_tfms = [Normalize.from_stats(*imagenet_stats)]","3ce62abb":"blocks = (ImageBlock(cls=PILDicom), CategoryBlock(vocab=[0,1]))\nget_x = ColReader('image_name', pref=path_img, suff='.dcm')\nmelanoma = DataBlock(blocks=blocks,\n                   get_x=get_x,\n                   splitter=IndexSplitter(df_small.loc[df_small.fold==fold].index),\n                   item_tfms=item_tfms,\n                   get_y=ColReader('target'),\n                   batch_tfms=batch_tfms)","b95942f1":"vis_dl = melanoma.dataloaders(df_small, bs=BS)","7f1c7e2d":"vis_dl.show_batch()","0f528469":"vis_dl.train.show_batch()","0b2c564c":"vis_dl.valid.show_batch()","decf8422":"class MixedDL():\n    def __init__(self, *dls, device='cuda:0'):\n        \"Accepts any number of `DataLoaders` and a device\"\n        self.device = device\n        for dl in dls: dl.shuffle_fn = self.shuffle_fn\n        self.dls = dls\n        self.count = 0\n        self.fake_l = _FakeLoader(self, False, 0, 0, 0)\n        self._get_idxs()\n        self.n_inp = 1\n        \n    def __len__(self): return len(self.dls[0])\n    \n    def _get_vals(self, x):\n        \"Checks for duplicates in batches\"\n        idxs, new_x = [], []\n        for i, o in enumerate(x): x[i] = o.cpu().numpy().flatten()\n        for idx, o in enumerate(x):\n            if not _arrayisin(o, new_x):\n                idxs.append(idx)\n                new_x.append(o)\n        return idxs\n    \n    def _get_idxs(self):\n        \"Get `x` and `y` indicies for batches of data\"\n        dl_dict = dict(zip(range(0,len(self.dls)), [dl.n_inp for dl in self.dls]))\n        inps = L([])\n        outs = L([])\n        for key, n_inp in dl_dict.items():\n            b = next(iter(self.dls[key]))\n            inps += L(b[:n_inp])\n            outs += L(b[n_inp:])\n        self.x_idxs = self._get_vals(inps)\n        self.y_idxs = self._get_vals(outs)\n    \n    def __iter__(self):\n        z = zip(*[_loaders[i.fake_l.num_workers==0](i.fake_l) for i in self.dls])\n        for b in z:   \n            inps = []\n            outs = []\n            if self.device is not None: \n                b = to_device(b, self.device)\n            for batch, dl in zip(b, self.dls):\n                batch = dl.after_batch(batch)\n                inps += batch[:dl.n_inp]\n                outs += batch[dl.n_inp:]\n            inps = L(inps)[self.x_idxs]\n            outs = L(outs)[self.y_idxs]\n            yield (inps, outs[0])\n                \n    def one_batch(self):\n        \"Grab one batch of data\"\n        with self.fake_l.no_multiproc(): res = first(self)\n        if hasattr(self, 'it'): delattr(self, 'it')\n        return res\n    \n    def shuffle_fn(self, idxs):\n        \"Shuffle the internal `DataLoaders`\"\n        if self.count == 0:\n            self.rng = self.dls[0].rng.sample(idxs, len(idxs))\n            self.count += 1\n            return self.rng\n        if self.count == 1:\n            self.count = 0\n            return self.rng\n\n        \n    def show_batch(self):\n        \"Show a batch of data\"\n        for dl in self.dls:\n            dl.show_batch()\n            \n    def to(self, device): self.device = device\n        \n\ndef _arrayisin(arr, arr_list):\n    \"Checks if `arr` is in `arr_list`\"\n    for a in arr_list:\n        if np.array_equal(arr, a):\n            return True\n    return False","188906b8":"mixed_train = MixedDL(tab_dl[0], vis_dl[0])\nmixed_valid = MixedDL(tab_dl[1], vis_dl[1])\ndls = DataLoaders(mixed_train, mixed_valid)","ee2a1345":"dls.show_batch()","89770308":"dls.to('cuda')\nbatch = dls.one_batch()","c761c28b":"# tabular data - cat\nbatch[0][0]","7a18266a":"# tabular data - cont\nbatch[0][1]","ad60503e":"# image data \nbatch[0][2]","ca54d1d3":"#target\nbatch[1]","4e2d7679":"def get_tabular_model(to, cont_names, out_sz=100, layers=[100,250]):\n    return TabularModel(emb_szs = get_emb_sz(to),\n                        n_cont = len(cont_names),\n                        out_sz = out_sz,\n                        layers = layers)","ae5db2f3":"def get_timm_vis_model(arch:str, out_sz=100, pretrained=True, cut=None):\n    model = create_model(arch, pretrained=pretrained)\n    if cut is None:\n        ll = list(enumerate(model.children()))\n        cut = next(i for i,o in reversed(ll) if has_pool_type(o))\n    model =  nn.Sequential(*list(model.children())[:cut])\n    \n    return model","2f2225c8":"class MelModel(Module):\n    def __init__(self, to, cont_names, out_sz, arch, *args, **kwargs):\n        self.tab_model = get_tabular_model(to, cont_names, out_sz=100)\n        self.vis_model = get_timm_vis_model(arch, out_sz)\n        self.vis_head  = create_head(num_features_model(self.vis_model), out_sz)\n        self.final_head = nn.Sequential(\n                            nn.BatchNorm1d(200),\n                            nn.Dropout(0.25),\n                            nn.Linear(200, 100, bias=False),\n                            nn.ReLU(inplace=True),\n                            nn.BatchNorm1d(100),\n                            nn.Dropout(0.5),\n                            nn.Linear(100, 2)\n                                )        \n    def forward(self, x):\n        tab = self.tab_model(x[0], x[1])\n        vis = self.vis_head(self.vis_model(x[2]))\n        return self.final_head(torch.cat([tab, vis], dim=1))","9d627a2b":"def model_splitter(m): return L(m.vis_model, nn.Sequential(m.tab_model, m.vis_head, m.final_head)).map(params)","21e718ca":"model = MelModel(to, cont_names, out_sz=100, arch='seresnext50_32x4d')","94f6ff5b":"def rocauc_metric(preds, targs, labels=range(2)):\n    # One-hot encode targets\n    targs = np.eye(2)[targs].reshape(targs.shape[0], -1)\n    return np.mean([roc_auc_score(targs[:,i], preds[:,i]) for i in labels])","058ef013":"learn = Learner(dls, model, \n                loss_func=LabelSmoothingCrossEntropyFlat(),\n                metrics=[AccumMetric(rocauc_metric, flatten=False), accuracy],\n                splitter=model_splitter).to_fp16()","fabd7f6a":"learn.freeze()\nlearn.summary()","a332b513":"learn.fine_tune(2, 1e-2)","309f7d4e":"Now, the image look more natural! ","7bd6db9e":"# Imports and basic EDA","a5c1eb97":"Here, we will make use of `TabularPandas` to build a general tabular dataloader.  ","e93d473e":"# Image Dataloader","400d8e3c":"The image does not look natural. Let's use pydicom's `convert_color_space` to convert the image to `RGB` and see if the image looks better. \n ","24c86392":"\nFor this part, we will make heavy use of this [notebook](https:\/\/www.kaggle.com\/muellerzr\/fastai2-tabular-vision-starter-kernel) by Zach. Discussion on `MixedDL` can be found [here](https:\/\/forums.fast.ai\/t\/combining-tabular-images-in-fastai2-and-should-work-with-almost-any-other-type\/73197\/13). \n\nNow, we will make `MixedDL` to load both `tab_dl` and `vis_dl` together. The main idea is to override the shuffle function in `tab_dl` and `vis_dl` with a common shuffle function so both the dataloaders will shuffle to contain the same data.  ","4835f2bc":"`convert_tfm` is a custom transform to convert the `YBR_FULL_422` format into `RGB`. ","becdc758":"# Building the model","4e6657fb":"The image files are in `dicom` format. Let's read one of the `.dcm` file.","cb202afc":"We will select a smaller dataset size since the full dataset is big and takes very long to train. ","70bc1d54":"The `model_splitter` fuction splits the model into layer groups for fine_tuning. ","b82b1b69":"Resources\/references:\n\nhttps:\/\/www.kaggle.com\/muellerzr\/fastai2-tabular-vision-starter-kernel\n\nhttps:\/\/forums.fast.ai\/t\/combining-tabular-images-in-fastai2-and-should-work-with-almost-any-other-type\/73197\/13\n\nhttps:\/\/walkwithfastai.com\/Multimodal_Head_and_Kaggle\n\nhttps:\/\/github.com\/haqishen\/SIIM-ISIC-Melanoma-Classification-1st-Place-Solution\n\nhttps:\/\/www.kaggle.com\/c\/siim-isic-melanoma-classification","793c06b7":"As can be seen, the data is imabalanced. I found this [discussion](https:\/\/www.kaggle.com\/c\/siim-isic-melanoma-classification\/discussion\/160410) on different loss fuctions that could be used for imbalanced data very helpful. We could definitely try  different loss functions but for this we will follow a simple `CrossEntropy` loss as per the winning solution.  ","44db03ce":"# Training","0f00850d":"In this blog, we will loosely follow the [winning solution](https:\/\/github.com\/haqishen\/SIIM-ISIC-Melanoma-Classification-1st-Place-Solution) to tackle the Melanoma Classification [Kaggle competition](https:\/\/github.com\/haqishen\/SIIM-ISIC-Melanoma-Classification-1st-Place-Solution). The solution combines an output from a CNN model with output from tabular model before the final output. The method is summarised in the picture below.  \n\n![image.png](attachment:image.png)\nSource: https:\/\/github.com\/haqishen\/SIIM-ISIC-Melanoma-Classification-1st-Place-Solution\n\nWhat will we be doing here:\n\n1. We will build a dataloader to combine vision data and tabular data. We will make use of [Zach Mueller's](https:\/\/twitter.com\/TheZachMueller) MixedDL. Discussion on MixedDL can be found [here](https:\/\/forums.fast.ai\/t\/combining-tabular-images-in-fastai2-and-should-work-with-almost-any-other-type\/73197\/13).\n2. We will also use albumentation library for our transform as was used in the winning solution. \n3. We will build a model that classfies melanoma or no melanoma using both the image data and the tabular data. For image, we will use timm's `seresnext50_32x4d` and for the tabular data, we will make use of fastai's `TabularModel`. This is different from the winning solution. \n\n","b8f7dfd4":"Let's check if `MixedDL` is working fine.  ","d8813c59":"We will make three folds using `StratifiedKFold`.","f09bb8d8":"# Tabular Dataloader","e1192155":"An important thing to note is the `Photometric Interpretation`. The images are stored in `YBR_FULL_422` format. We might have to convert them to `RBG` to make use of transfer learning.\n\nLet's view one of the image to see how it looks. ","ca17f585":"Next, we will build the vision dataloader. First, we will define `AlbumentationTransform` that allows fastai's `DataBlock` to work albumentation library. ","859d2056":"Let's make sure the augmentation are working properly. `vis_dl.train` should show augmentations while `vis_dl.valid` should only `Resize`. As we can see below, the augmentations are working properly. ","62c19d44":"# Tackling the Kaggle Melanoma Classification Competition\n> Combining tabular and vision data to classify Melanoma  \n\n- toc:true\n- branch: master\n- badges: true\n- comments: true\n- author: Mohamed Arshath\n- categories: [image classification, fastai.medical]","2bcfffef":"`MelModel` combines both the model to output 1 or 0.","f7e97c3e":"Let's build fuction to build `Tabular Model` and `Vision Model`. "}}