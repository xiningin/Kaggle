{"cell_type":{"83be954b":"code","c3236d96":"code","a1874eda":"code","b2e60385":"code","780af309":"code","3bfc782c":"code","9e7c87bb":"code","8d7708a6":"code","aa4e30f3":"code","302bc635":"code","c282fc7d":"code","51f6f2d8":"code","c58f13d9":"code","66e830b1":"code","95baa5ea":"code","a5cbbc9c":"code","6a3bf756":"code","34f614c8":"code","930084cb":"code","801ad32e":"code","27310e05":"code","b3382c7d":"code","89ccb350":"code","09465457":"code","b7f0bd79":"code","7287159e":"code","8b6619aa":"code","1ee603bf":"code","7e7a80d2":"code","4ab6139a":"code","50a4c58a":"code","0bda5a8f":"code","c9a6084b":"code","1cf0f2dc":"code","e98678b1":"code","7228a112":"code","59fa1653":"code","874161ef":"code","4883da9f":"code","74e70625":"code","42abb908":"code","9426b3f0":"code","ba3816b8":"code","3d2f32ee":"code","f6c03a59":"code","dec06e2e":"code","52475cc6":"code","9428a37d":"code","4feacc6f":"code","3a8b03a8":"code","eca334eb":"code","68f348aa":"code","5ce14b41":"code","8a1b6bc0":"code","a03c8d14":"code","71f15dea":"code","15a92e61":"code","815a3812":"code","409dbb0e":"code","96792925":"code","8c277262":"code","c116dc88":"code","310f6295":"code","107b155b":"code","6eac5de7":"code","8ddba8c5":"code","4b0e4a32":"code","943c7615":"code","92ef71fb":"code","10e61f14":"code","f4309cc1":"code","65b5251e":"code","5b62fb1d":"code","595f10ad":"code","16fa1b9c":"code","e13936b3":"markdown","2146690f":"markdown","09ccfcbf":"markdown","89e70b74":"markdown","d158b4ae":"markdown","b62c67e8":"markdown","84ab3104":"markdown","a5f2cda8":"markdown","d3fe0d81":"markdown","eb27533f":"markdown","caa259dd":"markdown","6e3290dd":"markdown","1f81e6e6":"markdown","9777b025":"markdown","373d2f06":"markdown","297580f7":"markdown","2f1f1fa3":"markdown"},"source":{"83be954b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c3236d96":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","a1874eda":"data= pd.read_csv('\/kaggle\/input\/malware-analysis-datasets-top1000-pe-imports\/top_1000_pe_imports.csv')","b2e60385":"data.info()","780af309":"data.columns.tolist()","3bfc782c":"pd.set_option('display.max_columns', None)\ndata.head(10)","9e7c87bb":"innocent= data.loc[data['malware']==0]\ninnocent.head(10)\n","8d7708a6":"counts= data['malware'].value_counts()\n\nprint(\"Malicious Samples: \", counts[1])\nprint(\"Benign Samples: \", counts[0])","aa4e30f3":"%matplotlib inline\n\nlabels=['Malicious', 'Benign']\nsizes=[data['malware'].value_counts()[1],\n     data['malware'].value_counts()[0]]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels,)\nax1.axis('equal')\nplt.show()","302bc635":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel","c282fc7d":"y=data['malware']\nx=data.drop(['malware','hash'],axis=1)\n#dropping hash as it is a (mosty unique) character value, cannot be mapped into feature space.\n\n#x.head()\n#y.head()","51f6f2d8":"model=ExtraTreesClassifier(n_estimators=50, random_state=15372)\n#Setting random state, else predictions vary everytime\nmodel.fit(x,y)","c58f13d9":"selections= SelectFromModel(model,prefit=True)\nx_new=selections.transform(x)\nprint('Number of features: %d' %(x_new.shape[1]))","66e830b1":"nb_features= x_new.shape[1]\nindices = np.argsort(model.feature_importances_)[::-1][:nb_features]\nfor f in range(nb_features):\n    number = f+1\n    feature_name = ''.join(x.columns[indices[f]])\n    feature_importance = model.feature_importances_[indices[f]]\n    print('   %d.\\t%s \\t%f' % (number, feature_name, (feature_importance * 100)))","95baa5ea":"#Testing Purposes \n#print(indices)\n#print(x.iloc[[1],[957]])","a5cbbc9c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","6a3bf756":"data= pd.read_csv('\/kaggle\/input\/malware-analysis-datasets-top1000-pe-imports\/top_1000_pe_imports.csv')","34f614c8":"data=data.drop(['hash'],axis=1)\ndata.info() #dropping hash; to get all integer values","930084cb":"flag=0\nfor column in data.columns.tolist():\n    if data[column].nunique()!=2:\n        print(column+ \" has more than 2 unique values\")\n        flag=1\n\nif flag==0:\n    print(\"All columns have 2 values only\")","801ad32e":"mal= data.loc[data['malware']==1]\nbenign= data.loc[data['malware']==0]","27310e05":"index= data.columns.tolist()\ncolumns=['Benign=0', 'Benign=1', 'Malicious=0', 'Malicious=1', 'Difference']","b3382c7d":"df = pd.DataFrame(index=index, columns=columns)\ndf = df.fillna(0)\n#df.head()","89ccb350":"#Creating a dataframe with the percentage of benign and malicious samples corresponding to 0 and 1 respectively for each column\n\nfor column in data.columns.tolist():\n    df.loc[[column],['Benign=1']]= np.round((benign[column].sum()\/benign.shape[0]*100),2)\n    df.loc[[column],['Benign=0']]= np.round(((benign.shape[0]-benign[column].sum())\/benign.shape[0]*100),2)\n    df.loc[[column],['Malicious=1']]= np.round((mal[column].sum()\/mal.shape[0]*100),2)\n    df.loc[[column],['Malicious=0']]= np.round(((mal.shape[0]-mal[column].sum())\/mal.shape[0]*100),2)\n    df.loc[[column],['Difference']]= abs(np.round((((benign[column].sum()\/benign.shape[0])- (mal[column].sum()\/mal.shape[0]))*100 ),2))","09465457":"df.head()","b7f0bd79":"# Sorting with respect to differences, to get the most different values\n\ndf=df.sort_values('Difference', ascending=False)\ndf=df.drop(['malware']) #Obviously, this column will be different\n#df.head(20)","7287159e":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score","8b6619aa":"data= pd.read_csv('\/kaggle\/input\/malware-analysis-datasets-top1000-pe-imports\/top_1000_pe_imports.csv')","1ee603bf":"def create_data(dataframe):\n    y= dataframe['malware']\n    X= dataframe.drop(['malware'], axis=1)\n    print(\"X Shape: \", X.shape)\n    print(\"Y Shape: \", y.shape)\n    X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=2)\n    \n    return (X, y, X_train, X_test, y_train, y_test)","7e7a80d2":"def evaluation(model, X_val, y_val, predictions):\n    print(\"Confusion Matrix: \\n\", confusion_matrix(y_val, predictions))\n    print(\"Precision:\", precision_score(y_val, predictions))\n    print(\"Recall:\",recall_score(y_val, predictions))\n    print(\"F1 Score:\", f1_score(y_val, predictions))\n    return","4ab6139a":"df= data","50a4c58a":"y= df['malware']\nX= df.drop(['hash','malware'], axis=1)\n#Remove hash, it is always a unique character string- MODEL WILL NOT TRAIN","0bda5a8f":"X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=2)","c9a6084b":"rf = RandomForestClassifier(n_estimators=50, random_state=3)\nrf.fit(X_train,y_train)","1cf0f2dc":"#Find Important features\nimportances= pd.DataFrame({'Feature': X_train.columns, 'Importance': np.round((rf.feature_importances_ * 100),3)})\nimportances= importances.sort_values('Importance', ascending=False).set_index('Feature')\n\nimportances.loc[importances['Importance']> 0.2]","e98678b1":"features= importances.loc[importances['Importance']> 0.2].index.to_list()\nmal= ['malware']\ncolumns= features + mal\n\ncolumns","7228a112":"#Keep only important features\n\ndf=data[columns]\n\n#df.info()","59fa1653":"X, y, X_train, X_test, y_train, y_test= create_data(df)\n","874161ef":"rf = RandomForestClassifier(n_estimators=100, \n                            random_state=3, \n                            bootstrap= True,\n                            max_depth= 110,\n                            max_features= 3,\n                            min_samples_leaf= 2,\n                            min_samples_split= 3)\nrf.fit(X_train,y_train)","4883da9f":"y_pred= rf.predict(X_test)\n\nprint(\"Train Accuracy:\",rf.score(X_train, y_train) )#accuracy\nprint( \"Test Accuracy:\", accuracy_score(y_test, y_pred, normalize=True))","74e70625":"scores= cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy') #5 fold\n\nprint(\"Scores: \", scores)\nprint(\"Mean: \", scores.mean())\nprint(\"Standard Deviation: \", scores.std())","42abb908":"#Cross Validation Evaluation\n\nprint(\"Cross Validation Evaluation:\\n\")\npredictions = cross_val_predict(rf, X_train, y_train, cv=3)\nevaluation(rf, X_train, y_train, predictions)","9426b3f0":"#Test Set Evaulation\n\nprint(\"\\nTest Set Evaluation:\\n\")\ny_pred = rf.predict(X_test)\nevaluation(rf, X_test, y_test, y_pred)","ba3816b8":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier","3d2f32ee":"data= pd.read_csv('\/kaggle\/input\/malware-analysis-datasets-top1000-pe-imports\/top_1000_pe_imports.csv')","f6c03a59":"df= data[['OpenProcess', 'GetCurrentProcess', 'GetProcessHeap','ReadFile', 'CreateFileW', 'WriteFile', 'FindFirstFileW', 'FindNextFileW', 'SetWindowsHookExW', 'GetAsyncKeyState', 'GetForegroundWindow', 'GetKeyState', 'MapVirtualKeyW', 'VirtualAlloc', 'VirtualProtect', 'GetModuleHandleA', 'ExitProcess', 'RegCloseKey','GetCurrentProcessId', 'malware']]\n#df.columns.to_list()\n","dec06e2e":"y= df['malware']\nX= df.drop(['malware'], axis=1)\nprint(X.shape, y.shape)","52475cc6":"X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=2)\n#print(X_train.shape, X_test.shape)\n#print(y_train.shape, y_test.shape)","9428a37d":"#Random Forest Classifier\n\nrf = RandomForestClassifier(n_estimators=50, random_state=3)\nrf.fit(X_train,y_train)\n","4feacc6f":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import cross_val_score, cross_val_predict","3a8b03a8":"y_pred= rf.predict(X_test)\n\nprint(\"Train Accuracy:\",rf.score(X_train, y_train) )\nprint( \"Test Accuracy:\", accuracy_score(y_test, y_pred, normalize=True))","eca334eb":"#since accuracy is pretty high (97.8%), will check on cross validation\n\nscores= cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy') #5 fold\n\nprint(\"Scores: \", scores)\nprint(\"Mean: \", scores.mean())\nprint(\"Standard Deviation: \", scores.std())","68f348aa":"#Still pretty good accuracy, so checking Out of Bag score\n\nrf = RandomForestClassifier(n_estimators=50, oob_score = True, random_state=3)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\n\nprint(\"OOB score:\", round(rf.oob_score_, 4)*100, \"%\")","5ce14b41":"def evaluation(model, X_val, y_val, predictions):\n    print(\"Confusion Matrix: \\n\", confusion_matrix(y_val, predictions))\n    print(\"Precision:\", precision_score(y_val, predictions))\n    print(\"Recall:\",recall_score(y_val, predictions))\n    print(\"F1 Score:\", f1_score(y_val, predictions))\n    return","8a1b6bc0":"#Cross Val Evaluation\n\nprint(\"Cross Validation:\\n\")\npredictions = cross_val_predict(rf, X_train, y_train, cv=3)\nevaluation(rf, X_train, y_train, predictions)\n\n\n#Confusion matrix: [[TN, FP], [FN,TP]]","a03c8d14":"#Testing Evaluation\n\nprint(\"Test Set:\\n\")\ny_pred = rf.predict(X_test)\nevaluation(rf, X_test, y_test, y_pred)","71f15dea":"#Checking what Features are deamed important by the classifier; cross check with how it relates to Bhat's Stuff\n\nimportances= pd.DataFrame({'Feature': X_train.columns, 'Importance': np.round(rf.feature_importances_,3)})\nimportances= importances.sort_values('Importance', ascending=False).set_index('Feature')\n\nimportances\n","15a92e61":"# Current parameters in use\nfrom pprint import pprint\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\npprint(rf.get_params())","815a3812":"#Testing diff values: each iteration, algo chooses a different combination of features\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","409dbb0e":"rf = RandomForestClassifier() #Base Model\nrf_random = RandomizedSearchCV(estimator = rf, \n                               param_distributions = random_grid, \n                               n_iter = 100, \n                               cv = 3, \n                               verbose=2, \n                               random_state=42,\n                               n_jobs = -1)\n\n# 3 folds of cross validation, more cv reduces overfitting but increases run time.\n# n_jobs=-1 uses all processors in parallel\n# Verbosity helps log the output. https:\/\/stats.stackexchange.com\/questions\/153823\/what-is-verbose-in-scikit-learn-package-of-python","96792925":"#Will take a while to run\nrf_random.fit(X_train, y_train)","8c277262":"#Show best parameters\nrf_random.best_params_\n","c116dc88":"from sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [1, 2, 3],\n    'min_samples_split': [8,10],\n    'n_estimators': [1400, 1600, 1800]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","310f6295":"grid_search.fit(X_train, y_train)","107b155b":"grid_search.best_params_","6eac5de7":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score","8ddba8c5":"data= pd.read_csv('\/kaggle\/input\/malware-analysis-datasets-top1000-pe-imports\/top_1000_pe_imports.csv')","4b0e4a32":"df= data[['OpenProcess', 'LoadLibraryA', 'GetProcessHeap', 'ShellExecuteW', 'VirtualFree', 'GetDC', 'IsDebuggerPresent', 'malloc', 'FindNextFileA', 'free', 'GetAsyncKeyState', 'GetTickCount', 'exit', '_cexit', 'VirtualAlloc', 'VirtualProtect', 'GetModuleHandleA', 'ExitProcess', 'RegCloseKey','GetCurrentProcessId', 'malware']]","943c7615":"y= df['malware']\nX= df.drop(['malware'], axis=1)\nprint(X.shape, y.shape)","92ef71fb":"X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=2)\n#print(X_train.shape, X_test.shape)\n#print(y_train.shape, y_test.shape)","10e61f14":"rf = RandomForestClassifier(n_estimators=1600, \n                            random_state=3, \n                            bootstrap= True,\n                            max_depth= 110,\n                            max_features= 3,\n                            min_samples_leaf= 2,\n                            min_samples_split= 10)\nrf.fit(X_train,y_train)\n\n#If Bootstrap is False then OOB is not available","f4309cc1":"y_pred= rf.predict(X_test)\n\nprint(\"Train Accuracy:\",rf.score(X_train, y_train) )#accuracy\nprint( \"Test Accuracy:\", accuracy_score(y_test, y_pred, normalize=True))","65b5251e":"scores= cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy') #5 fold\n\nprint(\"Scores: \", scores)\nprint(\"Mean: \", scores.mean())\nprint(\"Standard Deviation: \", scores.std())","5b62fb1d":"def evaluation(model, X_val, y_val, predictions):\n    print(\"Confusion Matrix: \\n\", confusion_matrix(y_val, predictions))\n    print(\"Precision:\", precision_score(y_val, predictions))\n    print(\"Recall:\",recall_score(y_val, predictions))\n    print(\"F1 Score:\", f1_score(y_val, predictions))\n    return","595f10ad":"#Cross Validation Evaluation\n\nprint(\"Cross Validation Evaluation:\\n\")\npredictions = cross_val_predict(rf, X_train, y_train, cv=3)\nevaluation(rf, X_train, y_train, predictions)","16fa1b9c":"#Test Set Evaulation\n\nprint(\"\\nTest Set Evaluation:\\n\")\ny_pred = rf.predict(X_test)\nevaluation(rf, X_test, y_test, y_pred)","e13936b3":"## Model Evaluation","2146690f":"## Exploratory Data Analysis","09ccfcbf":"# Model Comparision\n\nComparing the performance of our model with all features of the dataset, and important features selected automatically in sk-learn.","89e70b74":"Found Best Parameters:\n\n1. n_estimators: 1600\n2. min_samples_split: 10\n3. min_samples_leaf: 2\n4. max_features: sqrt\n5. max_depth: 80\n6. bootstrap: True","d158b4ae":"Parameters I want to work on:\n\n1. bootstrap = method for sampling data points (with or without replacement) (False= entire dataset used to make trees)\n2. max_depth = max number of levels in each decision tree (More depth may lead to single leaf nodes in each tree, less depth may not be sufficient enough to capture the info)\n3. max_features = max number of features considered for splitting a node\n4. n_estimators = number of trees in the forest\n5. min_samples_split = min number of data points placed in a node before the node is split (how many samples are needed to split the node)\n6. min_samples_leaf = min number of data points allowed in a leaf node (helps smoothen the model)","b62c67e8":"## Feature Analysis: Malicious v. Benign Samples","84ab3104":"Best Parameters:\n\n1. 'bootstrap': True,\n2. 'max_depth': 110,\n3. 'max_features': 3,\n4. 'min_samples_leaf': 2,\n5. 'min_samples_split': 10,\n6. 'n_estimators': 1600","a5f2cda8":"High Precision= Low false positive rate\nHigh Recall= Low false negative rate\n\nSo I am seeing some false positives. But good news is that we have VVV high true positives and VVV low false negatives, which is required for a Malware Detector","d3fe0d81":"# **Data Feature Analysis**","eb27533f":"## Feature Importance","caa259dd":"## Automatically Selected Important Features","6e3290dd":"## Hyperparameter Testing","1f81e6e6":"# Testing and Tuning the Model\nUsing our selected features","9777b025":"## Automatic Feature Selection","373d2f06":"## Our Analysis: Top 1000 PE Headers\n\nmsvcrt.dll -\n\n1. _cexit 8.280818\n2. _controlfp 3.025105\n3. _getmainargs 1.754543\n4. _initterm 1.724222\n5. _p__fmode 1.655268\n6. exit 1.607496\n7. _p__commode 1.430197\n8. _set_app_type 1.413838\n9. _exit 1.263665\n10. _onexit 0.981152\n11. _XcptFilter 0.709312\n12. _setusermatherr 0.688940\n\nImportant Indicators while doing analysis:(my opinion based on my experience)\n\n1. VirtualAlloc\n2. VirtualProtect\n3. GetModuleHandleA\n4. ExitProcess\n5. RegCloseKey\n6. GetCurrentProcessId\n7. TerminateProcess\n8. LoadLibraryA\n9. GetCurrentProcess\n10. malloc\n11. VirtualFree\n12. free\n13. MapVirtualKeyA\n14. MapVirtualKeyW\n15. SetWindowsHookExA\n16. SetWindowsHookExW\n\nThere a couple of more interestung dll's that are used by malwares like WSOC which is used to communicate with the command and control center","297580f7":"# Malware Detector (Final)\n\n(After Tuning)","2f1f1fa3":"## Model Evaluation"}}