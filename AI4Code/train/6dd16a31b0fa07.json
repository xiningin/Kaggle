{"cell_type":{"c74f6fb1":"code","4700f24a":"code","dcdf6a43":"code","e5219eae":"code","d4a87e10":"code","72b6914f":"code","3330b7e2":"code","ecffd43c":"code","c348634f":"code","4ae3cef3":"code","df08a120":"code","b8ff9a82":"code","e5bc69ee":"code","c92cf60d":"code","c5d1fd35":"code","bf4b77b0":"code","a59db332":"code","d59e70ed":"code","5db0fd7b":"code","7f1fe28f":"code","eea03ed9":"code","adceb216":"code","b0ba95b6":"code","7b267418":"code","88d0f437":"code","6babec6a":"code","266f4e74":"code","f4a70580":"code","3ceace45":"code","832e99d5":"code","4a34ad59":"code","e879a82d":"code","6900899f":"code","5694b33e":"code","64ef7a72":"code","93e8f894":"code","2f65be00":"code","13606c0a":"code","bd78fc35":"markdown","c7c951db":"markdown","9e468b15":"markdown","b7b7b04d":"markdown","c9a091a2":"markdown","ef7f18a1":"markdown","ec68051f":"markdown","b802a5df":"markdown","9f9135fc":"markdown","9389c990":"markdown","1e6c8722":"markdown","1b9edf95":"markdown","d9ede1e3":"markdown","bf735d8f":"markdown","2e7eb238":"markdown"},"source":{"c74f6fb1":"import rkn_module_benford_law as rkn_benford\n\nimport sys\nimport csv\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport matplotlib.image as mpimg","4700f24a":"path = \"..\/input\/inputbenfordcovid19\/time_series_covid19_deaths_global.csv\"\n\ndata = pd.read_csv(path, encoding='ISO-8859-1', delimiter=',' , low_memory=False)\n\ndata_mod = data.copy()\n\ndf = pd.DataFrame(data_mod)","dcdf6a43":"# df to store the analysis\ndf_analysis = pd.DataFrame(columns=['var_name', 'var_NaN', 'var_not_NaN', \"var_min\", \"var_max\" , \"var_mean\" , 'var_type', 'var_categ'])","e5219eae":"# Column names in dataSet\ncoluna_name_list = list(df.columns.values)","d4a87e10":"# This routine will store on df_analise the number os NaN each variable from dataset has. Alto the variable type and its categories.\nfor i in coluna_name_list:\n    if(df[i].dtypes == \"object\"):\n        lista=[i, df[i].isna().sum(), df[i].count(), \"NA\", \"NA\", \"NA\", df[i].dtypes, \"numerical variable\"]\n        df_length = len(df_analysis)\n        df_analysis.loc[df_length] = lista\n    else:\n        lista=[i, df[i].isna().sum(), df[i].count(), df[i].min(), df[i].max(), df[i].mean(), df[i].dtypes, \"numerical variable\"]\n        df_length = len(df_analysis)\n        df_analysis.loc[df_length] = lista","72b6914f":"# Set var_name as index\ndf_analysis.set_index('var_name', inplace=True)","3330b7e2":"# For each non numerical variable assign its possible values\nfor i in coluna_name_list:\n    if(df_analysis.loc[i, \"var_type\"] == \"object\"):\n        df_analysis.loc[i, \"var_categ\"] = list(df[i].unique())\n    else:\n        pass","ecffd43c":"df_analysis.sort_values(\"var_NaN\", ascending=False)","c348634f":"# Create a new column with country names. If a country has no provinces\/state desaggregation, so it will show the term: Single Unity\ndf[\"Province\/State\"] = df[\"Province\/State\"].replace(np.NaN, \"Single Unity\")\ndf[\"Country_State\"] = df[\"Country\/Region\"] + \"_\" + df[\"Province\/State\"]","4ae3cef3":"# Remove unecessary columns\ndel df[\"Province\/State\"]\ndel df[\"Country\/Region\"]\ndel df[\"Lat\"]\ndel df[\"Long\"]","df08a120":"df","b8ff9a82":"# Send columns to rows\ndf_melt = df.melt(id_vars=[\"Country_State\"],\n       var_name=\"Date\",\n       value_name=\"Deaths_Accumulated\")","e5bc69ee":"# Assign date type\ndf_melt['Date'] = pd.to_datetime(df_melt['Date'])","c92cf60d":"# Sort by country and date\ndf_melt = df_melt.sort_values([\"Country_State\", \"Date\"], ascending = (True, True))","c5d1fd35":"# New column to receive the new cases for each day\ndf_melt[\"Deaths_New_Day\"] = 0\n# This function will assign to each country\/day the number of new cases confirmed, based on the difference among accumulated cases of the actual and last day.\ncountry_before = df_melt.iloc[0,0]\n\nfor row in range(1, df_melt.shape[0], 1):\n    country_actual = df_melt.iloc[row,0]\n    if(country_actual == country_before):\n        df_melt.iloc[row,3] = df_melt.iloc[row,2] - df_melt.iloc[row-1,2]\n    else:\n        df_melt.iloc[row,3] = df_melt.iloc[row,2]\n    country_before = country_actual","bf4b77b0":"# Reset index: drop = False\ndf_melt.reset_index(inplace = True, drop = True)","a59db332":"# Create column to store de number of the week\ndf_melt['Date_week'] = pd.DatetimeIndex(df_melt['Date']).week","d59e70ed":"# New dataFrame grouped by country and week. The column value represents the number of new cases in each week per country.\ndf_agg_week = (df_melt.groupby(['Country_State', 'Date_week']).sum()).copy()","5db0fd7b":"# Remove columns && rename columns\ndel df_agg_week[\"Deaths_Accumulated\"]\ndf_agg_week = df_agg_week.rename(columns = {\"Deaths_New_Day\": \"Deaths_New_Week\"})","7f1fe28f":"# Set index to columns: drop = False\ndf_agg_week.reset_index(inplace = True, drop = False)","eea03ed9":"# IMPORTANT: This is the data that will be used on our analysis. However, will keep on the code to create a more robust database\ndf_week = df_agg_week.copy()\ndel df_week[\"Date_week\"]\ndf_week.to_excel(\"df_week_deaths.xlsx\")","adceb216":"df_week","b0ba95b6":"# Create key-column (Country_week) to join both dataFrames\ndf_melt[\"Country_week\"] = df_melt[\"Country_State\"] + \"-\" + df_melt[\"Date_week\"].astype(str)\ndf_agg_week[\"Country_week\"] = df_agg_week[\"Country_State\"] + \"-\" + df_agg_week[\"Date_week\"].astype(str)","7b267418":"# Reorder column\ndf_agg_week = df_agg_week[['Country_week', 'Deaths_New_Week']]\ndf_melt = df_melt[[\"Country_week\", 'Country_State', 'Date', \"Date_week\", \"Deaths_Accumulated\", \"Deaths_New_Day\"]]","88d0f437":"# Create the final dataFrame with number of accumulated cases, daily cases and weekly cases. Key-column: Country_week\ndf_merge = df_melt.merge(df_agg_week, on=\"Country_week\")","6babec6a":"# Remove key column\ndel df_merge[\"Country_week\"]","266f4e74":"# Show final data and send to excell\ndf_merge.to_excel(\"df_merge_deaths.xlsx\")","f4a70580":"# Note that we have few weeks of information per country. Insufficient for an analysis ungrouped per country.\ndf_merge","3ceace45":"# Note that it is possible to have a negative new number of cases. Meaning that in such a week the government corrected the numbers informed in the previous week.\ndf_week.describe()","832e99d5":"# Some deeper analysis\ndf_analysis_desc = (df_week.groupby(['Country_State']).describe()).copy()\ndf_analysis_desc","4a34ad59":"# Getting hints (1 for aggregated analysis)\nrkn_benford.hints(df_week, 1)","e879a82d":"# df_week: data set with the values to be analyzed\n# 1: Aggregated analysis (Since the sample per country is very small, we are only interested to analyze frequencies of the entire data set as a whole.)\n# 10: Number of rounds we will run the code in order to produced an averaged chi-squared value.\n# 600: Sample size for the first digit analysis.\n# 150: Sample size for the second digit analysis.\n# 76: Sample size for the third digit analysis.\n# 1: Number of graphs to produce with the best chi-sq values.\n# 1: Number of graphs to produce with the worst chi-sq values. Same as the graph before.\ntable_app = rkn_benford.benford(df_week, 1, 10, 600, 150, 76, 1, 1, \"output_deaths.xlsx\", \"\")","6900899f":"# Order by city name\ntable_app[0].sort_values(by=['units'], inplace=True)\n# Format table values\nresults_d1 = table_app[0].style.format({\n    'N0': '{:,.2%}'.format, 'N1': '{:,.2%}'.format, 'N2': '{:,.2%}'.format, 'N3': '{:,.2%}'.format, 'N4': '{:,.2%}'.format, 'N5': '{:,.2%}'.format,\n    'N6': '{:,.2%}'.format, 'N7': '{:,.2%}'.format, 'N8': '{:,.2%}'.format, 'N8': '{:,.2%}'.format, 'N9': '{:,.2%}'.format, \n    'chi_sq': '{:,.2f}'.format, 'chi_sq 10 rounds': '{:,.2f}'.format,\n    })","5694b33e":"fig = plt.figure(figsize=(8,6), dpi=250)\n\na = fig.add_subplot(1, 1, 1)\nimgplot = plt.imshow(mpimg.imread('..\/input\/inputbenfordcovid19\/D1__Aggregated_deaths_table.png'))\n\nplt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]);","64ef7a72":"fig = plt.figure(figsize=(8,6), dpi=250)\n\na = fig.add_subplot(1, 1, 1)\nimgplot = plt.imshow(mpimg.imread('..\/input\/inputbenfordcovid19\/D1__Aggregated_deaths.png'))\n\nplt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]);","93e8f894":"# Order by city name\ntable_app[1].sort_values(by=['units'], inplace=True)\n# Format table values\nresults_d2 = table_app[1].style.format({\n    'N0': '{:,.2%}'.format, 'N1': '{:,.2%}'.format, 'N2': '{:,.2%}'.format, 'N3': '{:,.2%}'.format, 'N4': '{:,.2%}'.format, 'N5': '{:,.2%}'.format,\n    'N6': '{:,.2%}'.format, 'N7': '{:,.2%}'.format, 'N8': '{:,.2%}'.format, 'N8': '{:,.2%}'.format, 'N9': '{:,.2%}'.format, \n    'chi_sq': '{:,.2f}'.format, 'chi_sq 10 rounds': '{:,.2f}'.format,\n    })","2f65be00":"fig = plt.figure(figsize=(8,6), dpi=250)\n\na = fig.add_subplot(1, 1, 1)\nimgplot = plt.imshow(mpimg.imread('..\/input\/inputbenfordcovid19\/D2__Aggregated_deaths_table.png'))\n\nplt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]);","13606c0a":"fig = plt.figure(figsize=(8,6), dpi=250)\n\na = fig.add_subplot(1, 1, 1)\nimgplot = plt.imshow(mpimg.imread('..\/input\/inputbenfordcovid19\/D2__Aggregated_deaths.png'))\n\nplt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]);","bd78fc35":"# 2. Loading the DataSet","c7c951db":"### 5.2.3. Third digit results\n\n\nFor the third digit we have a very small sample size. That way no conclusion can be drawn.","9e468b15":"# 4. Creating modified data sets\n\nFor this section we want to aggregate the daily numbers into weekly numbers. This aggregation is important since many cases that happen in one day may only be informed some days later. \n\nWe also gonna concatenate the country name and the country state into a unique column named: Country_State\n\nThen, we will produce to excel files:\n\n* **df_merge_deaths.xlsx**: This file contains the number of new cases per day, per week and the accumulated up to the date. These numbers are shown per country and per day (from 22\/jan\/2020 to 11\/apr\/2020). This file will not be used on the work, but may be useful for researchers.\n\n* **df_week_deaths.xlsx**: This file contains in each row the name of the Country\/State plus the number of confirmed new cases of Covid-19 for each week from 22\/jan\/2020 to 11\/apr\/2020. This is the data that will be used further for the Benford's analysis.","b7b7b04d":"# 3. Analysis on the Original Data Set\n\nIn this section we want to analyze the data for confirmed deaths took from [Data Repository by Johns Hopkins CSSE](https:\/\/github.com\/CSSEGISandData\/COVID-19). This dataset shows the number of confirmed deaths of Covid-19 per country and per country's state when possible. The period of analysis goes from 22\/jan\/2020 to 11\/apr\/2020.\n\nThen, let's check for:\n\n* Number of NaN in each variable\n* Variable types\n* Variable categories","c9a091a2":"## Usefull Data for researchers","ef7f18a1":"### 5.2.2. Second digit results\n\nFor the second digit we find a very low chi-squared value (5.4) which means that for the second digit the values informed by governments around the world adhere to the Benford's Law.\n\nFurther analysis per country still needed.","ec68051f":"## 5.2 Running the script\n\n* This work uses the following utility script: \"RKN Module - Benford Law\". For more on how to use this script, please access:\nhttps:\/\/www.kaggle.com\/rafaelknunes\/rkn-module-benford-law-tutorial\/notebook","b802a5df":"### 5.2.1. First digit results\n\nIn this application we will use a sample size of 600 out of 662 possibles. The chi-squared obtained 71.67 is very high. In such case there is evidence of data manipulation.\n\nHowever, our analysis focused only on the entire data. Maybe an analysis per country would reveal that some countries follow and others do not follow Benford's law. Unfortunately there is not enough data to make this investigation at the present moment.","9f9135fc":"## Data to be used further on our analysis","9389c990":"# 5. Application: Confirmed deaths of Covid-19 per week\n\nFor this analysis we are taking our treated data set with only the number of new cases per week. \n\nThe idea is to select a sample size from this data set and analyze the frequency in which appears numbers from 1 to 9 in the first position of the values selected. Then we compare these frequencies to those predicted by Benford's theory. We will be using a chi-squared test for statistical significance. \n\nSince the sample per country is very small (only 15 weeks of information per country), we will be only interested to analyze frequencies of the entire data set as a whole. Later, in the end of the year when we have more data per country, then we may analyze frequencies per country too.","1e6c8722":"<span style='color:Red ; font-size: 250%'> Benford's Law - A tool to detect data manipulation on confirmed deaths of Covid-19 (Part 3 of 3) <\/span> \n#### Author: [Rafael Klanfer Nunes](https:\/\/www.linkedin.com\/in\/rafaelknunes\/)\n#### **Date**: 12\/apr\/2020\n#### **Data Source**: [Data Repository by Johns Hopkins CSSE](https:\/\/github.com\/CSSEGISandData\/COVID-19\/tree\/master\/csse_covid_19_data\/csse_covid_19_time_series)\n#### **Disclaimer**: In this first part we will be analyzing the numbers of confirmed cases of Covid-19. Look for parts 1 and 2 for an investigation on the number of confirmed and recovered cases of Covid-19.\n#### **KAGGLE Notebook (Part 1: confirmed)**: https:\/\/www.kaggle.com\/rafaelknunes\/benford-law-to-detect-covid-19-manipulation-1of3\n#### **KAGGLE Notebook (Part 2: recovered)**: https:\/\/www.kaggle.com\/rafaelknunes\/benford-law-to-detect-covid-19-manipulation-2of3\n#### **KAGGLE Notebook (Part 3: deaths)**: https:\/\/www.kaggle.com\/rafaelknunes\/benford-law-to-detect-covid-19-manipulation-3of3","1b9edf95":"# 1. Notebook Goals\n\n* 1) Using Benford's Law theory, test whether the numbers of deaths of Covid-19 informed by governments are experiencing some kind of manipulation.\n* 2) This work uses the following utility script: \"RKN Module - Benford Law\". For more on how to use this script, please access:\nhttps:\/\/www.kaggle.com\/rafaelknunes\/rkn-module-benford-law-tutorial\/notebook","d9ede1e3":"## 5.1. Analysis of the data set","bf735d8f":"# THANKS FOR READING!","2e7eb238":"# 6. Final Remarks\n\nAlong this work we tested the values of new confirmed deaths of Covid-19 per week informed by governments around the world. We took data from 22\/jan\/2020 to 11\/apr\/2020.\n\nFor the first digit we found evidences of data manipulation. We expected to see the number 1 in 30.1% of times. However, the actual percentage is far higher: 43.8%. This, However, does not necessary imply any kind of fraud, but that further analysis is necessary.\n\n\nFor the second digit we found no evidence of data manipulation. And for the third digit there was not enough data for the analysis."}}