{"cell_type":{"73dc3af7":"code","b8f48889":"markdown","c863c99c":"markdown","e940c978":"markdown","6dbeca0f":"markdown","df3b8f69":"markdown","4a15e671":"markdown","cccca60c":"markdown","5942589b":"markdown","6bda2e23":"markdown","087db583":"markdown","05ed9c01":"markdown","37ec2bba":"markdown","b52a2a1f":"markdown","b77b3c5a":"markdown","42f2b9fc":"markdown","e7d432e8":"markdown","74d9cffa":"markdown","ba11d0be":"markdown","ec906f81":"markdown"},"source":{"73dc3af7":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"..\/input\/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b8f48889":"### The Usage:\n**class sklearn.svm.SVR(*, kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=- 1)class sklearn.svm.SVR(*, kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=- 1)**\n\n\n#### Parameters\n* **kernel**: {\u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019, \u2018precomputed\u2019}, default=\u2019rbf\u2019, specifies the kernel type to be used in the algorithm. It must be one of \u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019, \u2018precomputed\u2019 or a callable. If none is given, \u2018rbf\u2019 will be used. If a callable is given it is used to precompute the kernel matrix.\n\n* **degree**: int, default=3, degree of the polynomial kernel function (\u2018poly\u2019). Ignored by all other kernels.\n\n* **gamma**: {\u2018scale\u2019, \u2018auto\u2019} or float, default=\u2019scale\u2019, kernel coefficient for \u2018rbf\u2019, \u2018poly\u2019 and \u2018sigmoid\u2019.\n    * f gamma='scale' (default) is passed then it uses 1 \/ (n_features * X.var()) as value of gamma,\n    * if \u2018auto\u2019, uses 1 \/ n_features.\n\n\n* **coef0**: float, default=0.0, independent term in kernel function. It is only significant in \u2018poly\u2019 and \u2018sigmoid\u2019.\n\n* **tol**: float, default=1e-3, tolerance for stopping criterion.\n\n* **C**: float, default=1.0, regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.\n\n* **epsilon**: float, default=0.1, epsilon in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value.\n\n\n\n#### Attributes\n\n* class_weight_: ndarray of shape (n_classes,), multipliers of parameter C for each class. Computed based on the class_weight parameter.\n\n* **coef_**: ndarray of shape (1, n_features), weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel.\n\n* **dual_coef_**: ndarray of shape (1, n_SV), coefficients of the support vector in the decision function.\n\n* **fit_status_**: int, 0 if correctly fitted, 1 otherwise (will raise warning)\n\n* **intercept_**: ndarray of shape (1,), constants in decision function.\n\n* **n_support_**: ndarray of shape (n_classes,), dtype=int32, number of support vectors for each class.\n\n* **shape_fit_**: tuple of int of shape (n_dimensions_of_X,), array dimensions of training vector X.\n\n* **support_**: ndarray of shape (n_SV,), indices of support vectors.\n\n* **support_vectors_**: ndarray of shape (n_SV, n_features)\n\n\n#### Methods\n\n* **fit(X, y[, sample_weight])**: Fit the SVM model according to the given training data.\n\n* **get_params([deep])**: Get parameters for this estimator.\n\n* **predict(X)**: Perform regression on samples in X.\n\n* **score(X, y[, sample_weight])**: Return the coefficient of determination of the prediction.\n\n* **set_params(\\**params)**: Set the parameters of this estimator.","c863c99c":"#### Splitter {'Best '}:\n\nIn the \u201cbest\u201d splitter it evaluate all splits using the criterion before splitting whereas the \u201crandom\u201d splitter uses a random uniform function with min_feature_value, max_feature_value and random_state as inputs. We will look into what these are below but for now, let\u2019s see how the splitter will affect the model.\n\nLet\u2019s say you have hundreds of features, then \u201cbest\u201d splitter would be ideal because it will calculate the best features to split based on the impurity measure and use that to split the nodes, whereas if you choose \u201crandom\u201d you have a high chance of ending up with features that don\u2019t really give you that much information, which would lead to a more deeper less precise tree.\n\nOn the other hand, the \u201crandom\u201d splitter has some advantages, specifically, since it selects a set of features randomly and splits, it doesn\u2019t have the computational overhead of computing the optimal split. Next, it is also less prone to overfitting because you are not essentially calculating the best split before each split and the additional randomness will help you here, so if your model is overfitting, then you can change the splitter to \u201crandom\u201d and retrain.\n\nSo for a tree with few features without any overfitting, I would go with the \u201cbest\u201d splitter to be safe so that you get the best possible model architecture.","e940c978":"### Why are decision trees not enough?\n\nDecision trees have a couple of problems:\n\n* The main problem is that they tend to overfit very easily. This causes high variance, which can be seen as high test errors on the test dataset, despite high accuracy on the training dataset. In other words, decision trees do not generalize well to novel data.\n* Decision trees are easily swayed by data that splits the attributes well. Imagine that there\u2019s a single feature, whose values almost deterministically split the dataset. Let's say that when X1 = 0, then Y will always be below 10, while when X1 = 1, then Y will always be equal or greater to 10. Almost every decision tree will use this feature in its split criteria, making the trees overly correlated with each other.\n\nThe ensemble of decision trees introduces randomness, which mitigates the issues above.","6dbeca0f":"### Multiple Linear Regression\nMultiple linear regression is used to estimate the relationship between two or more independent variables and one dependent variable. You can use multiple linear regression when you want to know:\n\n1. How strong the relationship is between two or more independent variables and one dependent variable.\n2. The value of the dependent variable at a certain value of the independent variables (e.g. the expected yield of a crop at certain levels of rainfall, temperature, and fertilizer addition).\n\n#### The Formula:\n\n![image.png](attachment:51fa4a78-bc95-4614-b74c-65dc9470bfcc.png)\n\n* y = the predicted value of the dependent variable\n* B0 = the y-intercept (value of y when all other parameters are set to 0)\n* B1X1= the regression coefficient (B1) of the first independent variable (X1) (a.k.a. the effect that increasing the value of the independent variable has on the predicted y value)\n* \u2026 = do the same for however many independent variables you are testing\n* BnXn = the regression coefficient of the last independent variable\n* e = model error (a.k.a. how much variation there is in our estimate of y)","df3b8f69":"Comparison of Regression Equations\n\n* Simple Linear Regression   ===>        **y = b0+b1x**\n\n* Multiple Linear Regression ===>        **y = b0+b1x1+ b2x2+ b3x3+... bnxn**\n\n* Polynomial Regression      ===>        **y = b0+a1x+a2x^2+a3x^3+... anx^n**","4a15e671":"## Polynominal regression\n\nPolynomial Regression is a special case of linear regression in which a polynomial equation with a specified (n) degree is fit on the non-linear data which forms a curvilinear relationship between the dependent and independent variables.\n\nThe equation of the polynomial regression having an nth degree can be written as:\n\n![image.png](attachment:d1ba58bc-6f33-4e70-b9bd-9b0d92e31c81.png)\n\nGenerally, it is used when the points in the data set are scattered and the linear model is not able to describe the result clearly. As the degree of the polynomial equation (n) becomes higher, the polynomial equation becomes more complicated and there is a possibility of the model tending to overfit.\n\nThere are two techniques which are used in deciding the degree of the equation:\n\n* Forward Selection: It is the method of increasing the degree until it is significant enough to define the model.\n* Backward Selection: It is the method of decreasing the degree until it is significant enough to define the model.","cccca60c":"#### **Regularization**\n\nThe Regularization Parameter tells the SVM optimization how much you want to avoid miss classifying each training example.\n\nIf the C is higher, the optimization will choose smaller margin hyperplane, so training data miss classification rate will be lower.\n\nOn the other hand, if the C is low, then the margin will be big, even if there will be miss classified training data examples. This is shown in the following two diagrams:\n\n![image.png](attachment:8878fe31-fac1-407e-951e-eeff65aa6004.png)\nAs you can see in the image, when the C is low, the margin is higher (so implicitly we don\u2019t have so many curves, the line doesn\u2019t strictly follows the data points) even if two apples were classified as lemons. When the C is high, the boundary is full of curves and all the training data was classified correctly. Don\u2019t forget, even if all the training data was correctly classified, this doesn\u2019t mean that increasing the C will always increase the precision (because of overfitting).","5942589b":"## Support Vector Regression(SVR):\nSupport Vector Regression (SVR) uses the same principle as SVM, but for regression problems.\n\n![image.png](attachment:cb9407b0-fe9a-4eb4-bee6-167461236770.png)\n\nConsider these two red lines as the decision boundary and the green line as the hyperplane. Our objective, when we are moving on with SVR, is to basically consider the points that are within the decision boundary line. Our best fit line is the hyperplane that has a maximum number of points.\n\nThe first thing that we\u2019ll understand is what is the decision boundary (the danger red line above!). Consider these lines as being at any distance, say \u2018a\u2019, from the hyperplane. So, these are the lines that we draw at distance \u2018+a\u2019 and \u2018-a\u2019 from the hyperplane. This \u2018a\u2019 in the text is basically referred to as epsilon.\n\nAssuming that the equation of the hyperplane is as follows:\n\n            Y = wx+b (equation of hyperplane)\n\nThen the equations of decision boundary become:\n\n* wx+b= +a\n\n* wx+b= -a\n\nThus, any hyperplane that satisfies our SVR should satisfy:\n\n* -a < Y- wx+b < +a\n\n\nOur main aim here is to decide a decision boundary at \u2018a\u2019 distance from the original hyperplane such that data points closest to the hyperplane or the support vectors are within that boundary line. Hence, we are going to take only those points that are within the decision boundary and have the least error rate, or are within the Margin of Tolerance. This gives us a better fitting model. SVR gives us the flexibility to define how much error is acceptable in our model and will find an appropriate line (or hyperplane in higher dimensions) to fit the data.","6bda2e23":"#### **Kernel Functions**\n\n* **Linear Kernel**\n\n    It is the most basic type of kernel, usually one dimensional in nature. It proves to be the best function when there are lots of features. The linear kernel is mostly preferred for text-classification problems as most of these kinds of classification problems can be linearly separated.\n\n    Linear kernel functions are faster than other functions. \n    Linear Kernel Formula\n\n    * F(x, xj) = sum( x.xj)\n\n        Here, x, xj represents the data you\u2019re trying to classify.\n\n* **Polynomial Kernel** \n\n    It is a more generalized representation of the linear kernel. It is not as preferred as other kernel functions as it is less efficient and accurate.\n    Polynomial Kernel Formula\n\n    * F(x, xj) = (x.xj+1)^d\n\n        Here \u2018.\u2019 shows the dot product of both the values, and d denotes the degree. \n\n        F(x, xj) representing the decision boundary to separate the given classes. \n\n* **Gaussian Radial Basis Function (RBF)** \n\n    It is one of the most preferred and used kernel functions in svm. It is usually chosen for non-linear data. It helps to make proper separation when there is no prior knowledge of data.\n    Gaussian Radial Basis Formula\n\n    * F(x, xj) = exp(-gamma * ||x - xj||^2)\n\n        The value of gamma varies from 0 to 1. You have to manually provide the value of gamma in the code. The most preferred value for gamma is 0.1.\n\n* **Sigmoid Kernel**\n\n    It is mostly preferred for neural networks. This kernel function is similar to a two-layer perceptron model of the neural network, which works as an activation function for neurons.\n\n    It can be shown as,\n    Sigmoid Kenel Function\n    * F(x, xj) = tanh(\u03b1xay + c)\n","087db583":"#### **Gamma**\n\nThe next important parameter is Gamma. The gamma parameter defines how far the influence of a single training example reaches. This means that high Gamma will consider only points close to the plausible hyperplane and low Gamma will consider points at greater distance.\n![image.png](attachment:d3639813-15ff-4e03-978f-d53ea111849d.png)\n\nAs you can see, decreasing the Gamma will result that finding the correct hyperplane will consider points at greater distances so more and more points will be used (green lines indicates which points were considered when finding the optimal hyperplane).","05ed9c01":"* Assumptions of both simple and multiple linear regression:\n    1. Homogeneity of variance (homoscedasticity): the size of the error in our prediction doesn\u2019t change significantly across the values of the independent variable.\n    2.Independence of observations: the observations in the dataset were collected using statistically valid sampling methods, and there are no hidden relationships among observations.\n    3. Normality: The data follows a normal distribution.\n    4.The relationship between the independent and dependent variable is linear: the line of best fit through the data points is a straight line (rather than a curve or some sort of grouping factor).","37ec2bba":"# Linear Regression\nIt is both a statistical algorithm and a machine learning algorithm. linear regression is studied as a model for understanding the relationship between input and output numerical variables.\nThe overall idea of regression is to examine two things:\n   1. Does a set of predictor variables do a good job in predicting an outcome (dependent) variable?\n   2. Which variables in particular are significant predictors of the outcome variable, and in what way do they\u2013indicated by the magnitude and sign of the beta estimates\u2013impact the outcome variable?\n\nThe simplest form of the regression equation with one dependent and one independent variable is defined by the formula:\n\n                       y = c + b*x\nwhere:\n* y = estimated dependent variable score\n    \n* c = constant, b = regression coefficient\n\n* x = score on the independent variable.\n\n## Types of Linear Regression\n \n\n* ****Simple linear regression:****\n    1 dependent variable (interval or ratio), 1 independent variable (interval or ratio or dichotomous)\n\n* ****Multiple linear regression:****\n    1 dependent variable (interval or ratio) , 2+ independent variables (interval or ratio or dichotomous)\n\n* ****Logistic regression:****\n    1 dependent variable (dichotomous), 2+ independent variable(s) (interval or ratio or dichotomous)\n\n* ****Ordinal regression****\n    1 dependent variable (ordinal), 1+ independent variable(s) (nominal or dichotomous)\n\n* ****Multinomial regression****\n    1 dependent variable (nominal), 1+ independent variable(s) (interval or ratio or dichotomous)\n\n* ****Discriminant analysis****\n    1 dependent variable (nominal), 1+ independent variable(s) (interval or ratio)","b52a2a1f":"## Random Forest Regression\n\nRandom Forest Regression is a supervised learning algorithm that uses ensemble learning method for regression. Ensemble learning method is a technique that combines predictions from multiple machine learning algorithms to make a more accurate prediction than a single model.\n\n![image.png](attachment:f7a31c65-093e-4236-92c6-67a5cc291419.png)\n\nThe diagram above shows the structure of a Random Forest. Trees run in parallel with no interaction amongst them. A Random Forest operates by constructing several decision trees during training time and outputting the mean of the classes as the prediction of all the trees. To get a better understanding of the Random Forest algorithm, let\u2019s walk through the steps:\n1. Pick at random k data points from the training set.\n2. Build a decision tree associated to these k data points.\n3. Choose the number N of trees you want to build and repeat steps 1 and 2.\n4. For a new data point, make each one of your N-tree trees predict the value of y for the data point in question and assign the new data point to the average across all of the predicted y values.\n\nA Random Forest Regression model is powerful and accurate. It usually performs great on many problems, including features with non-linear relationships. Disadvantages, however, include the following: there is no interpretability, overfitting may easily occur, we must choose the number of trees to include in the model. The ensemble of decision trees has high accuracy because it uses randomness on two levels:\n\n1. The algorithm randomly selects a subset of features, which can be used as candidates at each split. This prevents the multitude of decision trees from relying on the same set of features, which automatically solves Problem 2 above and decorrelates individual trees.\n2. Each tree draws a random sample of data from the training dataset when generating its splits. This introduces a further element of randomness, which prevents the individual trees from overfitting the data. Since they cannot see all of the data, they cannot overfit it.\n\n### The Usage:\n class sklearn.ensemble.RandomForestRegressor(n_estimators=100, *, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)\n*  **Parameters**\n\n    * **criterion: {\u201cmse\u201d, \u201cmae\u201d, \u201cpoisson\u201d}, default=\u201dmse\u201d.**\n        The function to measure the quality of a split. Supported criteria are \u201cmse\u201d for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, \u201cmae\u201d for the mean absolute error, which minimizes the L1 loss using the median of each terminal node. When the outliers are present in the dataset, then the mse does not perform well.\n    \n    * **max_depth: int, default=None**\n\n        The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n    \n    * **min_samples_split: int or float, default=2**\n\n        The minimum number of samples required to split an internal node:\n        If int, then consider min_samples_split as the minimum number.\n        If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n\n    * **min_samples_leaf: int or float, default=1**\n\n        The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n        If int, then consider min_samples_leaf as the minimum number.\n        If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n    \n    * **max_features: int, float or {\u201cauto\u201d, \u201csqrt\u201d, \u201clog2\u201d}, default=None**\n\n        The number of features to consider when looking for the best split:\n        If int, then consider max_features features at each split.\n        If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split.\n        If \u201cauto\u201d, then max_features=n_features.\n        If \u201csqrt\u201d, then max_features=sqrt(n_features).\n        If \u201clog2\u201d, then max_features=log2(n_features).\n        If None, then max_features=n_features.\n\n        Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features.\n    \n    * **max_leaf_nodes: int, default=None**\n\n        Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n\n    * **min_impurity_decrease: float, default=0.0**\n\n        A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n        The weighted impurity decrease equation is the following:\n\n        * N_t \/ N * (impurity - N_t_R \/ N_t * right_impurity - N_t_L \/ N_t * left_impurity)\n\n        where N is the total number of samples, N_t is the number of samples at the current node, N_t_L is the number of samples in the left child, and N_t_R is the number of samples in the right child.\n\n    * **min_impurity_split: float, default=0**\n\n        Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.\n\n    * **bootstrap: bool, default=True**\n        Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.\n\n    * **oob_score: bool, default=False**\n        Wether to use out-of-bag samples to estimate the generalization score. Only available if bootstrap=True.\n    \n    * **n_jobs: int, default=None**\n        The number of jobs to run in parallel. fit, predict, decision_path and apply are all parallelized over the trees. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.\n\n    * **warm_start: bool, default=False**\n        When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest.\n\n\n* **Attributes:**\n\n    * **base_estimator_(DecisionTreeRegressor)**: The child estimator template used to create the collection of fitted sub-estimators.\n    * **estimators_(list of DecisionTreeRegressor)**: The collection of fitted sub-estimators.\n    * **feature_importances_(ndarray of shape (n_features,))**: The impurity-based feature importances.\n    * **n_features_(int)**: The number of features when fit is performed.\n    * **n_outputs_(int)**: The number of outputs when fit is performed.\n\n\n* **Methods:**\n\n    * **apply(X)**: Apply trees in the forest to X, return leaf indices.\n    * **decision_path(X)**: Return the decision path in the forest.\n    * **fit(X, y[, sample_weight])**: Build a forest of trees from the training set (X, y).\n    * **get_params([deep])**: Get parameters for this estimator.\n    * **predict(X)**: Predict regression target for X.\n    * **score(X, y[, sample_weight])**: Return the coefficient of determination of the prediction.\n    * **set_params(\\**params)**: Set the parameters of this estimator.","b77b3c5a":"### Types of Ensemble Learning:\n\n1. **Boosting**: Boosting refers to a group of algorithms that utilize weighted averages to make weak learners into stronger learners. Boosting is all about \u201cteamwork\u201d. Each model that runs, dictates what features the next model will focus on. In boosting as the name suggests, one is learning from other which in turn boosts the learning.\n\n2. **Bootstrap Aggregation (Bagging)**: Bootstrap refers to random sampling with replacement. Bootstrap allows us to better understand the bias and the variance with the dataset. Bootstrap involves random sampling of small subset of data from the dataset. With bagging we are not subsetting the training data into smaller chunks and training each tree on a different chunk. Rather, if we have a sample of size N, we are still feeding each tree a training set of size N (unless specified otherwise). But instead of the original training data, we take a random sample of size N with replacement.","42f2b9fc":"### Simple Linear Regression\nWith simple linear regression when we have a single input, we can use statistics to estimate the coefficients.\n\nThis requires that you calculate statistical properties from the data such as means, standard deviations, correlations and covariance. All of the data must be available to traverse and calculate statistics.\n\n#### The formula:\n\n![image.png](attachment:d9dde8b1-19e0-4d9b-9cd1-1840ab4cfc16.png)\n    \n* y = estimated dependent variable score\n* B0 is the intercept, the predicted value of y when the x is 0.\n* B1 is the regression coefficient \u2013 how much we expect y to change as x increases.\n* x is the independent variable ( the variable we expect is influencing y).\n* e is the error of the estimate, or how much variation there is in our estimate of the regression coefficient.\n\n","e7d432e8":"## Support Vector Machine (SVM)\n\nIt tries to find a line\/hyperplane (in multidimensional space) that separates these two classes. Then it classifies the new point depending on whether it lies on the positive or negative side of the hyperplane depending on the classes to predict. SVM tries to find a line\/hyperplane (in multidimensional space) that separates these two classes. Then it classifies the new point depending on whether it lies on the positive or negative side of the hyperplane depending on the classes to predict.\n\n#### Hyperparameters of the Support Vector Machine (SVM) Algorithm\n* Kernel: A kernel helps us find a hyperplane in the higher dimensional space without increasing the computational cost. Usually, the computational cost will increase if the dimension of the data increases. This increase in dimension is required when we are unable to find a separating hyperplane in a given dimension and are required to move in a higher dimension:\n\n![image.png](attachment:0c0a02b0-8548-47bb-aeb7-1f59cf9064ae.png)\n\n* Hyperplane: This is basically a separating line between two data classes in SVM. But in Support Vector Regression, this is the line that will be used to predict the continuous output\n* Decision Boundary: A decision boundary can be thought of as a demarcation line (for simplification) on one side of which lie positive examples and on the other side lie the negative examples. On this very line, the examples may be classified as either positive or negative. This same concept of SVM will be applied in Support Vector Regression as well\n","74d9cffa":"### The usage:\n**class sklearn.preprocessing.PolynomialFeatures(degree=2, *, interaction_only=False, include_bias=True, order='C')**\n\n* #### Parameters\n    * **degree**: int, default=2, the degree of the polynomial features.\n    * **interaction_only**: bool, default=False, if true, only interaction features are produced: features that are products of at most degree distinct input features (so not x[1] ** 2, x[0] * x[2] ** 3, etc.).\n\n    * **include_bias**: bool, default=True, if True (default), then include a bias column, the feature in which all polynomial powers are zero (i.e. a column of ones - acts as an intercept term in a linear model).\n\n    * **order**: {\u2018C\u2019, \u2018F\u2019}, default=\u2019C\u2019 ,order of output array in the dense case. \u2018F\u2019 order is faster to compute, but may slow down subsequent estimators. C-style order, with the last axis index changing fastest, back to the first axis index changing slowest. \u2018F\u2019 means to index the elements in column-major, Fortran-style order, with the first index changing fastest, and the last index changing slowest. Note that the \u2018C\u2019 and \u2018F\u2019 options take no account of the memory layout of the underlying array, and only refer to the order of axis indexing.\n  \n* #### Attributes\n    * **powers_**: ndarray of shape (n_output_features, n_input_features), powers_[i, j] is the exponent of the jth input in the ith output.\n\n    * **n_input_features_**: int, the total number of input features.\n\n    * **n_output_features_**: int, the total number of polynomial output features. The number of output features is computed by iterating over all suitably sized combinations of input features.\n\n* #### Methods\n    * **fit(X, y=None)**: Compute number of output features.\n      \n        * **Parameters**\n          * X{array-like, sparse matrix} of shape (n_samples, n_features) The data.\n          * y: None, ignored.\n\n        * **Returns**\n          * self: object, Fitted transformer.\n\n    * **fit_transform(X, y=None, **fit_params)**: Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.\n\n        * **Parameters**\n            * **X**: array-like of shape (n_samples, n_features), input samples.\n            * **y**: array-like of shape (n_samples,) or (n_samples, n_outputs), default=None, target values (None for unsupervised transformations).\n\n            * **fit_params**: dict, additional fit parameters.\n\n        * **Returns**\n            * **X_new**: ndarray array of shape (n_samples, n_features_new), transformed array.\n\n    * **get_feature_names(input_features=None)**: Return feature names for output features\n\n        * **Parameters**\n            * **input_features**: list of str of shape (n_features,), default=None, string names for input features if available. By default, \u201cx0\u201d, \u201cx1\u201d, \u2026 \u201cxn_features\u201d is used.\n\n        * **Returns**\n            * **output_feature_names**: list of str of shape (n_output_features,)\n\n    * **get_params(deep=True)**: Get parameters for this estimator.\n\n        * **Parameters**\n            * **deep**: bool, default=True, if True, will return the parameters for this estimator and contained subobjects that are estimators.\n\n        * **Returns**\n            * **params**: dict, parameter names mapped to their values.self**: estimator instance\n\n    * **set_params(**params)**: Set the parameters of this estimator.\n\n        * **Parameters**\n            * ****paramsdict**: Estimator parameters.\n\n        * **Returns**\n            * **self**: estimator instance\n        \n    * **transform(X)**: Transform data to polynomial features\n        * **Parameters**\n             * X: {array-like, sparse matrix} of shape (n_samples, n_features), the data to transform, row by row.\n        * **Returns**\n            * XP: {ndarray, sparse matrix} of shape (n_samples, NP), the matrix of features, where NP is the number of polynomial features generated from the combination of inputs.","ba11d0be":"### The Usage:\n\n**class sklearn.linear_model.LinearRegression(*, fit_intercept=True, normalize=False, copy_X=True, n_jobs=None, positive=False)**\n\n* **Parameters**\n    - **fit_intercept**: bool, default=True\n      Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).\n\n    - **normalize**: bool, default=False\n      This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use StandardScaler before calling fit on an estimator with normalize=False.\n\n    - **copy_X**: bool, default=True\n      If True, X will be copied; else, it may be overwritten.\n\n    - **n_jobs**: int, default=None\n      The number of jobs to use for the computation. This will only provide speedup for n_targets > 1 and sufficient large problems. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.\n\n    - **positive**: bool, default=False\n      When set to True, forces the coefficients to be positive. This option is only supported for dense arrays.\n\n* **Attributes:**\n    - **coef_**: array of shape (n_features, ) or (n_targets, n_features)\n      Estimated coefficients for the linear regression problem. If multiple targets are passed during the fit (y 2D), this is a 2D array of shape (n_targets, n_features), while if only one target is passed, this is a 1D array of length n_features.\n\n    - **rank_**: int\n      Rank of matrix X. Only available when X is dense.\n\n    - **singular_**: array of shape (min(X, y),)\n      Singular values of X. Only available when X is dense.\n\n    - **intercept_**: float or array of shape (n_targets,)\n      Independent term in the linear model. Set to 0.0 if fit_intercept = False.\n      \n* **Methods:**\n    - **fit(X, y, sample_weight=None)**\n      Fit linear model, returns an instance of self.\n    \n        **Parameters**\n        * X: {array-like, sparse matrix} of shape (n_samples, n_features)\n          Training data\n\n        * y: array-like of shape (n_samples,) or (n_samples, n_targets)\n          Target values. Will be cast to X\u2019s dtype if necessary.\n\n        * sample_weight: array-like of shape (n_samples,), default=None\n          Individual weights for each sample.\n\n    - **get_params(deep=True)**\n      Get parameters for this estimator.\n\n        **Parameters**\n         * deep: bool, default=True\n           If True, will return the parameters for this estimator and contained subobjects that are estimators.\n\n           Returns\n           params: dict, parameter names mapped to their values.\n\n    - **predict(X)**\n      Predict using the linear model.\n\n        **Parameters**\n            - X: array-like or sparse matrix, shape (n_samples, n_features)\n        Samples.\n\n        Returns\n        C: array, shape (n_samples,), returns predicted values.","ec906f81":"## Decision tree Regression\n\nDecision tree breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node (e.g., Outlook) has two or more branches (e.g., Sunny, Overcast and Rainy), each representing values for the attribute tested. Leaf node (e.g., Hours Played) represents a decision on the numerical target. The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data.\n\n![image.png](attachment:052af2a9-58d8-4d48-bf8f-b4fe83315b03.png)\n\n### The Usage:\n**class sklearn.tree.DecisionTreeRegressor(*, criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, ccp_alpha=0.0)**\n*  **Parameters**\n\n    * **criterion: {\u201cmse\u201d, \u201cmae\u201d, \u201cpoisson\u201d}, default=\u201dmse\u201d.**\n        The function to measure the quality of a split. Supported criteria are \u201cmse\u201d for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, \u201cmae\u201d for the mean absolute error, which minimizes the L1 loss using the median of each terminal node. When the outliers are present in the dataset, then the mse does not perform well.\n\n    * **splitter{\u201cbest\u201d, \u201crandom\u201d}, default=\u201dbest\u201d**\n\n        The strategy used to choose the split at each node. Supported strategies are \u201cbest\u201d to choose the best split and \u201crandom\u201d to choose the best random split.\n    \n    * **max_depth: int, default=None**\n\n        The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n    \n    * **min_samples_split: int or float, default=2**\n\n        The minimum number of samples required to split an internal node:\n        If int, then consider min_samples_split as the minimum number.\n        If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n\n    * **min_samples_leaf: int or float, default=1**\n\n        The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n        If int, then consider min_samples_leaf as the minimum number.\n        If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n    \n    * **max_features: int, float or {\u201cauto\u201d, \u201csqrt\u201d, \u201clog2\u201d}, default=None**\n\n        The number of features to consider when looking for the best split:\n        If int, then consider max_features features at each split.\n        If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split.\n        If \u201cauto\u201d, then max_features=n_features.\n        If \u201csqrt\u201d, then max_features=sqrt(n_features).\n        If \u201clog2\u201d, then max_features=log2(n_features).\n        If None, then max_features=n_features.\n\n        Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features.\n    \n    * **max_leaf_nodes: int, default=None**\n\n        Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n\n    * **min_impurity_decrease: float, default=0.0**\n\n        A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n        The weighted impurity decrease equation is the following:\n\n        * N_t \/ N * (impurity - N_t_R \/ N_t * right_impurity - N_t_L \/ N_t * left_impurity)\n\n        where N is the total number of samples, N_t is the number of samples at the current node, N_t_L is the number of samples in the left child, and N_t_R is the number of samples in the right child.\n\n    * **min_impurity_split: float, default=0**\n\n        Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.\n\n* **Attributes:**\n\n    * **feature_importances_**(ndarray of shape (n_features,)): Return the feature importances.\n    * **max_features_**(int): The inferred value of max_features.\n    * **n_features_**(int): The number of features when fit is performed.\n    * **n_outputs_**(int): The number of outputs when fit is performed.\n    * **tree_**(Tree instance): The underlying Tree object.\n\n\n* **Methods:**\n\n    * **apply(X[, check_input])**: Return the index of the leaf that each sample is predicted as. \n\n    * **decision_path(X[, check_input])**: Return the decision path in the tree.\n\n    * **fit(X, y[, sample_weight, check_input, \u2026])**: Build a decision tree regressor from the training set (X, y).\n\n    * **get_depth()**: Return the depth of the decision tree.\n\n    * **get_n_leaves()**: Return the number of leaves of the decision tree.\n\n    * **get_params([deep])**: Get parameters for this estimator.\n\n    * **predict(X[, check_input])**: Predict class or regression value for X.\n\n    * **score(X, y[, sample_weight])**: Return the coefficient of determination of the prediction.\n\n    * **set_params(**params)**: Set the parameters of this estimator."}}