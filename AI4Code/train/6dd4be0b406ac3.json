{"cell_type":{"48a6149b":"code","9b83566c":"code","00bbb4b9":"code","91059007":"code","973d0d49":"code","46ab97a4":"code","5555b044":"code","59a26642":"code","58838607":"code","206750ed":"code","b6a9008e":"code","f2aa7858":"code","68f2efbd":"code","16c92824":"code","d15cd6cd":"code","8d81b01d":"code","7baad986":"code","5b2c1de7":"code","d4762506":"code","5bddf2da":"code","edc3f20a":"code","f28fd667":"code","752d51e6":"code","8633509e":"code","0efa785d":"code","5b472458":"code","764e1fac":"code","e26357d6":"code","9dfb9b8c":"code","03472c70":"code","79ff945f":"code","fabe90bc":"code","b4a4f0d8":"code","dacc29f7":"code","ae231a6b":"code","13059dbe":"code","a698fd00":"code","ebe4a50e":"code","0209d937":"code","9d2e814a":"code","8434cf13":"code","00ac684b":"code","f50852f8":"code","f793a87c":"code","f8fdf0fe":"code","b7a96a7f":"code","b64efb18":"code","967a5f53":"code","6fa4c91a":"code","ab99c440":"code","98d1066e":"code","cba90992":"code","f7e4948b":"code","3f8836f5":"code","57569cfc":"code","1709e4c5":"code","500b83b0":"code","449a2392":"code","86e0a006":"code","5362f3f1":"code","9ad94ba4":"code","d472ea04":"code","c1b4a90c":"code","28c4c9fd":"code","f29a48e1":"code","9cade189":"code","4e5e4efa":"code","94a54fe9":"code","da635ade":"code","587f4f09":"code","0f184898":"code","4514edb1":"code","d04678d4":"code","7517a667":"code","301b1016":"code","24b68810":"code","259ca611":"code","edd0f08c":"code","736276f0":"code","4a262cb9":"code","47208475":"code","85c4ec60":"code","325a8642":"code","fdc5f089":"code","502049d6":"code","089f8070":"code","c3741a5b":"code","58263733":"code","7d86ae7e":"code","0bf8be9b":"code","87ad25bf":"code","0aca26ca":"code","47f4de47":"code","1a38486d":"code","4f1b419d":"code","8ec508aa":"code","0f8fe296":"code","acf0559c":"code","8b87e2eb":"code","88a0a635":"code","26467daf":"code","bb4b8a42":"code","b61989ee":"code","e5608e3a":"code","f1e0dbd3":"code","d0256bd5":"code","5b997a89":"code","9783172f":"code","5c9cf958":"code","ce8c5ba4":"code","05dd4577":"code","8a9d577a":"code","29d682c8":"code","db7b0236":"markdown","8d7d0f2f":"markdown","8c824571":"markdown","9a4f3b45":"markdown","b806ce9e":"markdown","6a84b8a2":"markdown","6fa6a4b7":"markdown","dd844eaa":"markdown","293c50fb":"markdown","ea71142d":"markdown","41d1223a":"markdown","4bb1d4aa":"markdown","50b7f862":"markdown","6c66062c":"markdown","27d88c4f":"markdown","c38f1e32":"markdown","e11db82b":"markdown","dd50a8b7":"markdown","81438a16":"markdown","4bb21502":"markdown","8c05ad65":"markdown","ca4c2cd6":"markdown","76c31e20":"markdown","59f3a45f":"markdown","7c81a918":"markdown","1c9f9db8":"markdown","7220fb71":"markdown","6297725c":"markdown","67e60367":"markdown","42a1611f":"markdown","29065106":"markdown","233bd00e":"markdown","59887b3d":"markdown","ebccd6ec":"markdown","e08a97e0":"markdown","e853abd5":"markdown","be21b41e":"markdown","031e9fce":"markdown","a634a8ea":"markdown","cc765f48":"markdown","d47b4237":"markdown","8e4bdc68":"markdown","d4245e7f":"markdown","e27bb84e":"markdown","029886a5":"markdown","e276430c":"markdown","8b211b65":"markdown"},"source":{"48a6149b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current sessionq`","9b83566c":"# importing all the libraries\nfrom sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV , GridSearchCV\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, PowerTransformer\nimport xgboost as xgb \nfrom xgboost import XGBRegressor, cv, DMatrix\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.compose import ColumnTransformer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nimport matplotlib\nfrom sklearn.ensemble import RandomForestRegressor\nimport scipy as sp\nimport time\nfrom xgboost import plot_tree\nfrom yellowbrick.regressor import residuals_plot\nfrom yellowbrick.regressor import prediction_error\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.feature_selection import RFE\nimport statsmodels.stats.diagnostic as dg\nimport matplotlib.gridspec as gridspec\nimport optuna\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nimport glob\nfrom functools import partial\nfrom scipy.optimize import fmin\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nxgb.set_config(verbosity=1)","00bbb4b9":"# Reading the train and test data\ndf_train = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv', index_col='id')\ndf_test = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv', index_col='id')\n\n# for the submission part\ndf_submission = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv', index_col = 'id')","91059007":"# Checking some details of the Data set\nprint(f'For Training Dataset: Number of rows: {df_train.shape[0]} and columns: {df_train.shape[1]}')\nprint(f'For Test Dataset: Number of rows: {df_test.shape[0]} and columns: {df_test.shape[1]}')","973d0d49":"# Lets check some details about the columns\ndf_train.info()","46ab97a4":"# How does the data looks like lets have a quick glance\ndf_train.head()","5555b044":"# Lets check some statistical details about the dataset\ndf_train.describe().T.style.bar().background_gradient()","59a26642":"# Lets check whether any missing values is present \ndf_train.isnull().sum()\n# As it can seebn below all for all the columns there is no missing values","58838607":"df_test.isnull().sum()\n# Same is the case fo the Test also","206750ed":"# To check whether any columns is there with Duplicated Values or not\n# Here the Car ID is removed as that is unique\nprint(f'is Duplicate present in Training Dataset: { not df_train.duplicated().sum() == 0 } ')\nprint(f'is Duplicate present in Test Dataset: { not df_test.duplicated().sum() == 0 } ')\n# As we can see below there is no row which has dulicated values","b6a9008e":"# Fetching all the columns \ncolumns_list = df_train.columns.to_list()\ncolumns_list.remove('target')\n\nnumeric_columns = [col for col in columns_list if df_train[col].dtypes == np.float]\ncategorical_columns = list(set(columns_list) - set(numeric_columns))\n\nprint(f'Numeric Columns {numeric_columns}')\nprint(f'Categorical Columns {categorical_columns}')","f2aa7858":"# Splitting into Train and Test Split\nX = df_train.drop('target', axis = 1)\ny = df_train.target\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,train_size=0.8, test_size=0.2, random_state=42)","68f2efbd":"# Lets for now convert all the Categorical Columns to Ordinal Values\nordinal_encoder = OrdinalEncoder()\nX_train[categorical_columns] = ordinal_encoder.fit_transform(X_train[categorical_columns])\nX_valid[categorical_columns] = ordinal_encoder.transform(X_valid[categorical_columns])\ndf_test[categorical_columns] = ordinal_encoder.transform(df_test[categorical_columns])","16c92824":"# Building First Base Model\ndata_dmatrix = xgb.DMatrix(data=X_train,label=y_train)\nmy_model = XGBRegressor(n_estimators=100, learning_rate=0.05, n_jobs=-1,random_state = 42 , objective='reg:squarederror', tree_method = 'gpu_hist')\nmy_model.fit(X_train, y_train, early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)],\n             eval_metric = 'rmse',\n             verbose=False)","d15cd6cd":"print(f\"Model's Prediction power is : {mean_squared_error(y_valid, my_model.predict(X_valid), squared=False)}\")","8d81b01d":"params = {\"objective\":\"reg:squarederror\",\n          'learning_rate': 0.05,\n          'n_estimators': 100, \n          'n_jobs':-1,\n          'random_state': 42, \n          'tree_method': 'gpu_hist'}\n\nxgb_cv = cv(dtrain=data_dmatrix, params=params, nfold=5,\n                    num_boost_round=50, early_stopping_rounds= 5, metrics=\"rmse\", seed=42, verbose_eval= True, shuffle=True)","7baad986":"xgb_cv.head()","5b2c1de7":"# Lets see the Feature Importance as per the model\nxgb.plot_importance(my_model)\nplt.show()","d4762506":"# As it can be seen above that the model is behaving fine and we can see all the features not putting heavy importance \n# and it can be seen all the numerical columns are putting more importance\n\n# So for an instance we remove all the categorical value and just build the model using the numerical value\n\nmy_model_num = XGBRegressor(n_estimators=100, learning_rate=0.05, n_jobs=-1,random_state = 42 , objective='reg:squarederror', tree_method = 'gpu_hist')\nmy_model_num.fit(X_train[numeric_columns], y_train, early_stopping_rounds=5, \n             eval_set=[(X_valid[numeric_columns], y_valid)],\n             eval_metric = 'rmse',\n             verbose=False)\nprint(f\"Model's Prediction power is : {mean_squared_error(y_valid, my_model_num.predict(X_valid[numeric_columns]), squared=False)}\")","5bddf2da":"# Lets first Check the distribution of Target Variable\n# Let's first see the Distribution of target variable\n# As this is our Final Outcome, and let's see the Dataset Distribution\n\nplt.figure(figsize=(18,5))\nplt.suptitle('Plot of Target')\n# Histogram \nsns.distplot(y, ax = plt.subplot(1,3,1))\nplt.ylabel('target')\n\n# To check the spread of data\nsns.boxplot(y = y, ax = plt.subplot(1,3,2))\nsns.violinplot(x = y, ax = plt.subplot(1,3,3))\nplt.ylabel('Distribution')\nplt.show()\n\n\n# So it can seen the mostly there is a skewness present and mostly the data is present after 6,\n# And the target variable looks more like a multimodel distribution\nprint(f'Skew Value of around {round(y.skew(),2)}')","edc3f20a":"# Lets see the distribution of each bracket\ntarget_data = pd.cut(y, bins = np.arange(0,11,2), right = True)\nplt.figure(1,figsize=(10, 5))\n\ndf = (pd.DataFrame(target_data.value_counts())).reset_index()\n# Plot the Frequency plot\nsns.barplot(x = df['index'], y = df.target, ax = plt.subplot(1,2,1))\nplt.xticks(rotation = 90)\nplt.title('Frequency Distribution of target' )\nprint(f'{y[y<6].shape[0] \/ y.shape[0] }% of Data are below 6')\nplt.show()\ndf","f28fd667":"df_train.target.describe().iloc[1:].plot.barh(figsize=(10,5))\nplt.title('Target data statistics',fontsize=16)\nplt.xticks(np.arange(0,df_train.target.max() + .5,.5))\nplt.show()","752d51e6":"df_processed_train = df_train[df_train.target > 6 ]\n# Lets try to fit the model with these set of data and lets see the performance of the model\nX = df_processed_train.drop('target', axis = 1)\ny = df_processed_train.target\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,train_size=0.8, test_size=0.2, random_state=42)\n\n# Doing the ordinal encoding\nordinal_encoder = OrdinalEncoder()\nX_train[categorical_columns] = ordinal_encoder.fit_transform(X_train[categorical_columns])\nX_valid[categorical_columns] = ordinal_encoder.transform(X_valid[categorical_columns])\ndf_test = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv', index_col='id')\ndf_test[categorical_columns] = ordinal_encoder.transform(df_test[categorical_columns])\n\nmy_model = XGBRegressor(n_estimators=100, learning_rate=0.05, n_jobs=-1,random_state = 42 , objective='reg:squarederror', tree_method = 'gpu_hist')\nmy_model.fit(X_train, y_train, early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)],\n             eval_metric = 'rmse',\n             verbose=False)\n\nprint(f\"Model's Prediction power is : {mean_squared_error(y_valid, my_model.predict(X_valid), squared=False)}\")","8633509e":"def plot_learning_curves(model, X_train, y_train, X_valid, y_valid):\n    train_errors, val_errors = [], []\n    for m in range(1, X_train.shape[0], 5000):\n        model.fit(X_train.iloc[:m], y_train.iloc[:m])\n        y_train_predict = model.predict(X_train.iloc[:m])\n        y_val_predict = model.predict(X_valid)\n        train_errors.append(mean_squared_error(y_train[:m], y_train_predict, squared=False))\n        val_errors.append(mean_squared_error(y_valid, y_val_predict, squared=False))\n\n    plt.plot(list(range(1,X_train.shape[0], 5000)), np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n    plt.plot(list(range(1,X_train.shape[0], 5000)), np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n    plt.legend(loc=\"upper right\", fontsize=14)   \n    plt.xlabel(\"Training set size\", fontsize=14) \n    plt.ylabel(\"RMSE\", fontsize=14)              ","0efa785d":"plot_learning_curves(my_model, X_train, y_train, X_valid, y_valid)","5b472458":"# Lets take the test data again\ndf_test = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv', index_col='id')","764e1fac":"# Lets quickly check whether any outliers are present or not\nplt.figure(figsize=(20,10))\nax= sns.violinplot(data=df_train[numeric_columns],inner=None, palette=\"hls\")\nplt.title('Numerical features distribution');","e26357d6":"# Display of Distribution of each numerical feature\nplt.figure(1,figsize=(15,100))\nfig_idx= 0\nfor index,col in enumerate(numeric_columns):\n    fig_idx += 1 \n    plt.subplot(14,2,fig_idx)\n    # Distribution plot of the featrue\n    sns.distplot(df_train[col], color= 'r')\n    plt.xticks(rotation = 90)\n    plt.title('Frequency Distribution of ' + col )\n    plt.ylabel('Density')\n    \n    fig_idx += 1\n    plt.subplot(14,2,fig_idx)\n    # Data Distribution ( Scatter ) of the data points for the numerical feature\n    # And to check the distribution wrt to target\n    sns.scatterplot(x = df_train[col], y=df_train.target)\n    plt.xticks(rotation = 90)\n    plt.xlabel(col)\n    plt.ylabel('Target')\n    plt.title(col + 'Data Distribution w.r.t ' + 'target' )","9dfb9b8c":"# Combined dataframe containing numerical features only\ndf = pd.concat([df_train[numeric_columns], df_test[numeric_columns]], axis=0)\ncolumns = df.columns.values\n\n# Calculating required amount of rows to display all feature plots\ncols = 3\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(20,20), sharex=False)\n\n# Adding some distance between plots\nplt.subplots_adjust(hspace = 0.3)\n\n# Plots counter\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns): # If there is no more data columns to make plots from\n            axs[r, c].set_visible(False) # Hiding axes so there will be clean background\n        else:\n            # Train data histogram\n            sns.histplot(df_train[columns[i]], bins=40, ax = axs[r,c])\n            sns.histplot(df_test[columns[i]], bins=40, ax = axs[r,c], color = 'black', cbar_kws = {'alpha': 0.1})\n            axs[r, c].tick_params(axis=\"y\", labelsize=13)\n            if c > 0:\n                axs[r, c].get_yaxis().set_visible(False)\n            axs[r, c].grid(axis=\"y\")\n                                  \n        i+=1\nfig.legend(labels=['train','test'])\nplt.show();","03472c70":"fig = plt.figure(figsize=(15,5))\nplt.suptitle('Number of Unique values per category',fontsize=13);\n\nplt.subplot(1,2,1)\nsns.barplot(y = df_train[categorical_columns].nunique().sort_values().values, x=df_train[categorical_columns].nunique().sort_values().index)\nplt.title('Train')\n\nplt.subplot(1,2,2)\nsns.barplot(y = df_test[categorical_columns].nunique().sort_values().values, x=df_test[categorical_columns].nunique().sort_values().index)\nplt.title('Test')\nplt.show()","79ff945f":"#Plot Categorical Variable\nplt.figure(1,figsize=(20,60))\nfig_idx= 0\nfor index,col in enumerate(categorical_columns):\n    fig_idx += 1 \n    df = (pd.DataFrame(df_train[col].value_counts())).reset_index()\n    # Plot the Frequency plot\n    sns.barplot(x = df['index'], y = df[col], ax = plt.subplot(10,3,fig_idx))\n    plt.title('Frequency Distribution of ' + col )\n    fig_idx += 1\n    # Box plot for the spread of data for the categorical analysis \n    sns.boxplot(x = df_train[col], y=df_train.target,ax = plt.subplot(10,3,fig_idx))\n    plt.title(col + 'Data Spread w.r.t ' + 'target' )\n    \n    fig_idx += 1\n    sns.kdeplot(df_train.target, hue=df_train[col], ax = plt.subplot(10,3,fig_idx))\n    plt.title('Target Distribution')","fabe90bc":"# Combined dataframe containing categorical features only for both test and train\ntemp_df_train = df_train.copy()\ntemp_df_test = df_test.copy()\n\ntemp_df_train['dataset_type'] = 'train'\ntemp_df_test['dataset_type'] = 'test'\n\ncategorical_cols = categorical_columns + ['dataset_type'] \ndf = pd.concat([temp_df_train[categorical_columns + ['dataset_type']], temp_df_test[categorical_columns + ['dataset_type']]], axis=0)\ncolumns = df.columns.values\n\n# Calculating required amount of rows to display all feature plots\ncols = 3\nrows = (len(columns) - 1) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(20,20), sharex=False)\n\n# Adding some distance between plots\nplt.subplots_adjust(hspace = 0.3)\n\n# Plots counter\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= (len(columns) - 1): # If there is no more data columns to make plots from\n            axs[r, c].set_visible(False) # Hiding axes so there will be clean background\n        else:\n            # Train data histogram\n            sns.histplot(data=df, x=columns[i], hue=\"dataset_type\", palette=[\"C1\", \"k\"], ax = axs[r,c])\n            axs[r, c].tick_params(axis=\"y\", labelsize=13)\n            if c > 0:\n                axs[r, c].get_yaxis().set_visible(False)\n            axs[r, c].grid(axis=\"y\")\n                                  \n        i+=1\nfig.legend(labels=['train','test'])\nplt.show();","b4a4f0d8":"background_color = \"#f6f5f5\"\n\nfig = plt.figure(figsize=(18, 8), facecolor=background_color)\ngs = fig.add_gridspec(1, 2)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\ncolors = [\"#2f5586\", \"#f6f5f5\",\"#2f5586\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nax0.set_facecolor(background_color)\nax0.text(0, -1, 'Features Correlation on Train Dataset', fontsize=20, fontweight='bold')\nax0.text(0, -0.4, 'Highest correlation in the dataset is 0.5', fontsize=13, fontweight='light')\n\nax1.set_facecolor(background_color)\nax1.text(-0.1, -1, 'Features Correlation on Test Dataset', fontsize=20, fontweight='bold', fontfamily='serif')\nax1.text(-0.1, -0.4, 'Features in test dataset resemble features in train dataset ', \n         fontsize=13, fontweight='light', fontfamily='serif')\n\nsns.heatmap(df_train[numeric_columns].corr(), ax=ax0, vmin=-1, vmax=1, annot=True, square=True, \n            cbar_kws={\"orientation\": \"horizontal\"}, cbar=False, cmap=colormap, fmt='.1f')\n\nsns.heatmap(df_test[numeric_columns].corr(), ax=ax1, vmin=-1, vmax=1, annot=True, square=True, \n            cbar_kws={\"orientation\": \"horizontal\"}, cbar=False, cmap=colormap, fmt='.1f')\n\nplt.show()","dacc29f7":"sns.pairplot(df_train[numeric_columns], corner=True, diag_kind='kde');","ae231a6b":"# Lets check whether if there are some continous features which are correlated with the Feature Columns\nbackground_color = \"#f6f5f5\"\n\nfig = plt.figure(figsize=(12, 8), facecolor=background_color)\nplt.text(-1.1, 0.0675, 'Correlation of Continuous Features with Target', fontsize=15, weight='bold')\nchart_df = pd.DataFrame(df_train[numeric_columns].corrwith(df_train.target))\nchart_df.columns = ['corr']\nsns.barplot(x=chart_df.index, y=chart_df['corr'], zorder=3, edgecolor='black', linewidth=1.5)\nplt.show()","13059dbe":"# Lets assign the target Variable\ny = df_train['target']\nX = df_train.drop('target', axis = 1)\n\n# Lets perform Feature Scalling, hence all the numerical features will be on the same scale\nX_test = df_test.copy()\n\n\n# For the Numerical Features, Standard Scalling Operation will be performed \n\nsc = StandardScaler()\nX[numeric_columns] = sc.fit_transform(X[numeric_columns])\n\n\n# For the Categorical Column we would go with the Ordinal Encoder\nordinal_encoder = OrdinalEncoder()\nX[categorical_columns] = ordinal_encoder.fit_transform(X[categorical_columns])","a698fd00":"# Next, we break off a validation set from the training data.\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=42, test_size=0.2)","ebe4a50e":"# Define the model \nmodel_rf = RandomForestRegressor(criterion='mse', max_depth=30,\n           max_features='sqrt',\n           min_samples_leaf=2, min_samples_split=5,n_estimators=200, n_jobs=-1,\n           oob_score=False, random_state= 42, verbose=1, warm_start=True)\n# Train the model\nmodel_rf.fit(X_train, y_train)\npreds_valid = model_rf.predict(X_valid)\nprint(f'MSE from Random Forest Model {mean_squared_error(y_valid, preds_valid, squared=False)}')\n\n\n# Calculate rmse\nrmse_new = np.mean(100 * (mean_squared_error(y_valid, preds_valid, squared=False) \/ y_valid))\n# Calculate and display accuracy\naccuracy = 100 - rmse_new\nprint('Accuracy:', round(accuracy, 2), '%.')","0209d937":"def plot_probability_pot(model, X_train, y_train, X_valid, y_valid):\n    \"\"\"\n     Probability plot Same as a Q-Q plot which is Quantile-Quantile plot Compares the sample and theoretical quantiles, \n     however probabilities are shown in the scale of the theoretical distribution (x-axis) \n     and the y-axis contains unscaled quantiles of the sample data.\n    \"\"\"\n    plt.rcParams[\"axes.labelsize\"] = 12\n    rf_prob_train = model.predict(X_train) - y_train\n    plt.figure(figsize=(6,6))\n    sp.stats.probplot(rf_prob_train, plot=plt, fit=True)\n    plt.title('Train Probability Plot for Random Forest', fontsize=10)\n    plt.show()\n\n    rf_prob_test = model.predict(X_valid) - y_valid\n    plt.figure(figsize=(6,6))\n    sp.stats.probplot(rf_prob_test, plot=plt, fit=True)\n    plt.title('Test Probability Plot for Random Forest', fontsize=10)\n    plt.show()","9d2e814a":"plot_probability_pot(model_rf, X_train, y_train, X_valid, y_valid)","8434cf13":"# Lets Try to Find out which feature is putting more importance \n\nplt.figure(figsize=(12,5)) \n# Calculate feature importances\nimportances = model_rf.feature_importances_\n# Sort feature importances in descending order\nindices = np.argsort(importances)[::-1]\n\n# Rearrange feature names so they match the sorted feature importances\nnames = [X_train.columns.tolist()[i] for i in indices]\n# Barplot: Add bars\nplt.bar(range(X_train.shape[1]), importances[indices])\n# Add feature names as x-axis labels\nplt.xticks(range(X.shape[1]), names, rotation=20, fontsize = 8)\n# Create plot title\nplt.title(\"Feature Importance\")\n# Show plot\nplt.show()","00ac684b":"# Lets check the cummulative importance of each feature and above a certain threshold we can skip the rest of the feature\n# List of features sorted from most to least important\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in list(zip(names, importances[indices]))]\nsorted_importances = [importance[1] for importance in feature_importances]\nsorted_features = [importance[0] for importance in feature_importances]\n\n# Cumulative importances\ncumulative_importances = np.cumsum(sorted_importances)\n# Make a line graph\nx_values = list(range(len(importances)))\nplt.plot(x_values, cumulative_importances, 'g-')\n# Draw line at 95% of importance retained\n# So this value can be tweaked later based on the model performance\nplt.hlines(y = 0.95, xmin=0, xmax=len(sorted_importances), color = 'r', linestyles = 'dashed')\n# Format x ticks and labels\nplt.xticks(x_values, sorted_features, rotation = 'vertical')\n# Axis labels and title\nplt.xlabel('Variable'); plt.ylabel('Cumulative Importance'); plt.title('Cumulative Importances');","f50852f8":"# Find number of features for cumulative importance of 95%\nprint(f'Number of features for 95% importance: { np.where(cumulative_importances > 0.95)[0][0] + 1 } ')","f793a87c":"# Lets try to fit the model which these 18 Parameters\n# Lets check whether these parameters are really improving the model performance or not\nimportant_feature_names = [feature[0] for feature in feature_importances[0:17]]\n\n\nmodel_rf = RandomForestRegressor(criterion='mse', max_depth=30,\n           max_features='sqrt',\n           min_samples_leaf=2, min_samples_split=5,n_estimators=200, n_jobs=-1,\n           oob_score=False, random_state= 42, verbose=1, warm_start=True)\n# Train the expanded model on only the important features\nmodel_rf.fit(X_train[important_feature_names], y_train)\n# Make predictions on test data\nnew_predictions = model_rf.predict(X_valid[important_feature_names])\n# Performance metrics\nerrors = mean_squared_error(y_valid, new_predictions, squared=False)\nprint(f'RMSE: {errors}')\n\n# Calculate rmse\nrmse_new = np.mean(100 * (mean_squared_error(y_valid, preds_valid, squared=False) \/ y_valid))\n# Calculate and display accuracy\naccuracy = 100 - rmse_new\nprint('Accuracy:', round(accuracy, 2), '%.')","f8fdf0fe":"model_xgb = XGBRegressor(n_estimators=1000, learning_rate=0.25, n_jobs=-1,random_state = 42 , objective='reg:squarederror', tree_method = 'gpu_hist')\nmodel_xgb.fit(X_train, y_train, early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)],\n             eval_metric = 'rmse',\n             verbose=False)\n# Make predictions on test data\nnew_predictions = model_xgb.predict(X_valid)\nerrors = mean_squared_error(y_valid, new_predictions, squared=False)\nprint(f'RMSE: {errors}')\n\nplot_probability_pot(model_xgb, X_train, y_train, X_valid, y_valid)","b7a96a7f":"# Lets add some more sets of parameter, So as to check the model performance, whether its really improving or not\nxgb_params = {'objective': 'reg:squarederror',\n              'n_estimators': 10000,\n              'learning_rate': 0.25,\n              'subsample': 0.926,\n              'colsample_bytree': 0.84,\n              'grow_policy':'lossguide',\n              'max_depth': 2,\n              'booster': 'gbtree', \n              'reg_lambda': 45.1,\n              'reg_alpha': 34.9,\n              'random_state': 42,\n              'n_jobs': -1,\n              'tree_method': 'gpu_hist',\n              'max_delta_step':10,\n              'gamma' :0.01,\n              'min_child_weight' :110\n             }\n\nmodel_xgb = XGBRegressor(**xgb_params)\nmodel_xgb.fit(X_train, y_train, early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)],\n             eval_metric = 'rmse',\n             verbose=False)\nnew_predictions = model_xgb.predict(X_valid)\nerrors = mean_squared_error(y_valid, new_predictions, squared=False)\nprint(f'RMSE: {errors}')\n\nplot_probability_pot(model_xgb, X_train, y_train, X_valid, y_valid)","b64efb18":"parameters = {\n    'max_depth': [2, 4, 6],\n    'subsample': [0.623, 0.625, 0.627],\n    'colsample_bytree': [ 0.625, 0.63, 0.635],\n    'learning_rate' : [0.18, 0.192, 0.194],\n    'n_estimators' : [1450 , 2000, 5000 ],\n    'max_depth': [2, 3, 4, 6],\n    'reg_lambda': [71.5, 72, 72.5],\n    'reg_alpha': [12, 13, 14, 15]\n#     ,\n#     'max_delta_step': [0.49, 0.5, 0.51, 0.52],\n#     'gamma' : [0.00835, 0.0084, 0.00845]\n#     ,\n#     'min_child_weight' :[31, 32, 33, 34]\n}\n\nxgb_params = {'objective': 'reg:squarederror',\n              'grow_policy':'lossguide',\n              'booster': 'gbtree', \n              'random_state': 42,\n              'n_jobs': -1,\n              'tree_method': 'gpu_hist', \n              'verbose': 1\n             }\nmodel_clf = XGBRegressor(**xgb_params)\nclf = RandomizedSearchCV (model_clf, parameters, n_jobs=-1, cv=5, return_train_score=True, scoring='neg_root_mean_squared_error')\nstart = time.time()\nclf.fit(X_train, y_train)\nend = time.time()\nprint('time elapsed: ' + str(end-start)) ","967a5f53":"print(f'Finding out the best score: {-1 * clf.best_score_}')\nprint(f'Best set of Parameters are : {clf.best_params_}')","6fa4c91a":"#Check the final model details\nclf.best_estimator_","ab99c440":"final_model_xgb = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.625, gamma=0, gpu_id=0,\n             grow_policy='lossguide', importance_type='gain',\n             interaction_constraints='', learning_rate=0.194, max_delta_step=0,\n             max_depth=2, min_child_weight=1, missing=np.nan,\n             monotone_constraints='()', n_estimators=2000, n_jobs=-1,\n             num_parallel_tree=1, random_state=42, reg_alpha=14,\n             reg_lambda=72.5, scale_pos_weight=1, subsample=0.625,\n             tree_method='gpu_hist', validate_parameters=1,\n             verbosity=None, objective= 'reg:squarederror',\n             )\n\nfinal_model_xgb.fit(X_train, y_train, early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)],\n             eval_metric = 'rmse',\n             verbose=False)\nnew_predictions = final_model_xgb.predict(X_valid)\nerrors = mean_squared_error(y_valid, new_predictions, squared=False)\nprint(f'RMSE: {errors}')\n\nplot_probability_pot(final_model_xgb, X_train, y_train, X_valid, y_valid)","98d1066e":"# Lets try to check the Feature Importance Data of each Variable\nxgb_final_model_imp = pd.Series(final_model_xgb.feature_importances_, index = X_train.columns).sort_values(ascending=False)\n\nplt.figure(figsize=(20,8))\nplt.subplot(1,2,1)\n# Barplot: Add bars\nplt.bar(range(X_train.shape[1]), xgb_final_model_imp.values.tolist())\n# Add feature names as x-axis labels\nplt.xticks(range(X.shape[1]), xgb_final_model_imp.index.to_list(), rotation=90, fontsize = 8)\n# Create plot title\nplt.title(\"Feature Importance\")\n\nplt.subplot(1,2,2)\n# Cumulative importances\ncumulative_importances = np.cumsum(xgb_final_model_imp.values.tolist())\n# Make a line graph\nx_values = list(range(xgb_final_model_imp.shape[0]))\nplt.plot(x_values, cumulative_importances, 'g-')\n# Draw line at 95% of importance retained\n# So this value can be tweaked later based on the model performance\nplt.hlines(y = 0.95, xmin=0, xmax=xgb_final_model_imp.shape[0], color = 'r', linestyles = 'dashed')\n# Format x ticks and labels\nplt.xticks(x_values, xgb_final_model_imp.index.to_list(), rotation = 'vertical')\n# Axis labels and title\nplt.xlabel('Variable'); plt.ylabel('Cumulative Importance'); plt.title('Cumulative Importances');\n\n# Find number of features for cumulative importance of 95%\nprint(f'Number of features contributed to 95% importance: { np.where(cumulative_importances > 0.95)[0][0] + 1 } ')","cba90992":"# Lets try to build the model with the 21 model parameters and lets see if there is any significant improvement in model\nfeatures_to_consider = xgb_final_model_imp.index.tolist()[:21]\n\nfinal_model_xgb_sel_param = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.625, gamma=0, gpu_id=0,\n             grow_policy='lossguide', importance_type='gain',\n             interaction_constraints='', learning_rate=0.194, max_delta_step=0,\n             max_depth=2, min_child_weight=1, missing=np.nan,\n             monotone_constraints='()', n_estimators=2000, n_jobs=-1,\n             num_parallel_tree=1, random_state=42, reg_alpha=14,\n             reg_lambda=72.5, scale_pos_weight=1, subsample=0.625,\n             tree_method='gpu_hist', validate_parameters=1,\n             verbosity=None, objective= 'reg:squarederror',\n             )\n\nfinal_model_xgb_sel_param.fit(X_train[features_to_consider], y_train, \n             eval_set=[(X_valid[features_to_consider], y_valid)],\n             eval_metric = 'rmse',\n             verbose=False)\nnew_predictions = final_model_xgb_sel_param.predict(X_valid[features_to_consider])\nerrors = mean_squared_error(y_valid, new_predictions, squared=False)\nprint(f'RMSE: {errors}')","f7e4948b":"# Just to Support the statement whether the model performance have \nR_squared = r2_score(y_valid, final_model_xgb.predict(X_valid))\nN=X_train.shape[0]\np=X_train.shape[1]\nx = (1-R_squared)\ny = (N-1) \/ (N-p-1)\nadj_rsquared = (1 - (x * y))\nprint(\"\\nR-Squared: \", np.round(R_squared, 4))\nprint(\"\\nAdjusted R-Square\", adj_rsquared )","3f8836f5":"R_squared = r2_score(y_valid, new_predictions)\nN=X_train[features_to_consider].shape[0]\np=X_train[features_to_consider].shape[1]\nx = (1-R_squared)\ny = (N-1) \/ (N-p-1)\nadj_rsquared = (1 - (x * y))\nprint(\"\\nRMSE: \", np.round(errors, 4))\nprint(\"\\nR-Squared: \", np.round(R_squared, 4))\nprint(\"\\nAdjusted R-Square\", adj_rsquared )\n\n# Making the Prediction Error Plot\nprint(\"\\nPrediction Error Plot\")\nprint(prediction_error(final_model_xgb_sel_param, X_train[features_to_consider], y_train, X_valid[features_to_consider], y_valid))\n\n# Making the Residuals Plot\nprint(\"\\nResiduals Plot\")\nprint(residuals_plot(final_model_xgb_sel_param, X_train[features_to_consider], y_train, X_valid[features_to_consider], y_valid))","57569cfc":"def plot_multiple_values(imputed_column, images_per_row = 2):\n    column_list = imputed_column.columns.tolist()\n    images_per_row = min(len(column_list), images_per_row)\n    n_rows = (len(column_list) - 1) \/\/ images_per_row + 1\n    axes = []\n    fig = plt.figure(figsize=(20, 8))\n    outer = gridspec.GridSpec(n_rows, images_per_row, wspace =  0.2, hspace=0.1)\n    for i in range(len(column_list)):\n        inner = gridspec.GridSpecFromSubplotSpec(2, 1,subplot_spec=outer[i], wspace=0.1, hspace=0.1, height_ratios = (0.2, 1))\n        ax_box = plt.Subplot(fig, inner[0])\n        column = column_list[i]\n        mean=imputed_column[column].mean()\n        median=imputed_column[column].median()\n        mode=imputed_column[column].mode().values.tolist()[0]\n        sns.boxplot(imputed_column[column], ax=ax_box)\n        ax_box.axvline(mean, color='r', linestyle='--')\n        ax_box.axvline(median, color='g', linestyle='-')\n        ax_box.axvline(mode, color='b', linestyle='-')\n        ax_box.set(xlabel='')\n        fig.add_subplot(ax_box)\n        \n        ax_hist = plt.Subplot(fig, inner[1], sharex = ax_box)\n        sns.distplot(imputed_column[column], ax=ax_hist)\n        ax_hist.axvline(mean, color='r', linestyle='--', label = 'Mean')\n        ax_hist.axvline(median, color='g', linestyle='-', label = 'Median')\n        ax_hist.axvline(mode, color='b', linestyle='-', label = 'Mode')\n        ax_hist.legend()\n        fig.add_subplot(ax_hist)\n        \n    plt.suptitle(f\"To see the Data Distribution\")\n    for ax in fig.get_axes():\n        ax.label_outer()\n    plt.show()\n    \ndef plot_error_term(model, X, y):\n    \"\"\"\n    This is do analysis on the error terms\n    1. Histogram of the residual, so as to check whether it is Normal and whether it is centered around zero\n    2. To plot the Scatter plot and to check whether it is having a cone shaped or not\n    3. It plots the QQ PLots to checks whether the residual is a Normal Distribution\n    4. This also prints the R Squared value \n    5. To check the Mean Squared Error value\n    6. It's checks the Hetroscedacity using het_goldfeldquandt approach, this checks whether the two features have same variance\n        i. This gives Two Values F Statistics\n        ii. P Values, if the P Values is > 0.05 then the variance is same \n    \"\"\"\n    # To check the distribution of the error term for the Train \n    plt.figure(figsize=(20, 5))\n    y_pred = model.predict(X)\n    error_data = y.values - y_pred\n    sns.distplot(error_data,ax = plt.subplot(1,3,1), hist= False)\n    plt.title('Error Distribution')\n    # The error term doesn't looks like nearly normally distributed\n\n    sns.scatterplot(x = y, y = y_pred,ax = plt.subplot(1,3,2) )\n    plt.xlabel('Actual Value')\n    plt.ylabel('Predicted Values')\n    plt.title('Actual vs Predicted Value')\n    \n    sm.qqplot(error_data, ax = plt.subplot(1,3,3), line='s')\n    plt.title('QQ Plot to check the Normality')\n    plt.show()\n    \n    print(\"R2 Score {}\".format(r2_score(y, y_pred)))\n    print(\"Root Mean Squared Error is {}\".format(mean_squared_error(y, y_pred, squared=False)))\n    pred_df = pd.DataFrame({'Actual': y, 'Predicted': y_pred, 'errors': error_data})\n    plot_multiple_values(pred_df, 3)\n    print(pred_df.describe())","1709e4c5":"# Train Data Residual Analysis\nplot_error_term(final_model_xgb_sel_param, X_train[features_to_consider], y_train)","500b83b0":"# But as it can be seen above the R Squared Values is very less, we need to identify what can be the reason\n# Lets check out the reason , lets check the PValue of each Variable and also the VIF Score of each Params.\n# Lets try first try out with each parameter\n\nX_train_sm = sm.add_constant(X_train)\nlr_model_1 = sm.OLS(y_train, X_train_sm).fit()\nlr_model_1.summary()","449a2392":"def _deter_vif(X):\n    \"\"\"\n    This Function Determines the VIFs for the set of column which is present in the X Data Frame\n    Then it will calculate the VIF Scores for each columns and then\n    it will print all the VIFs for which VIF score is not INF\n    \"\"\"\n    vif = pd.DataFrame()\n    vif['columns'] = X.columns\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    # There are certain colums for which VIF is coming as inf\n    # as per https:\/\/pvanb.wordpress.com\/2016\/04\/25\/vif-stepwise-variable-selection\/\n    # This basically comes when the features nearly does a perfect fit (R2 = 1) which results in an \n    # undefined VIF (denoted by inf in the table with results).\n    print(vif[vif['VIF'] != np.inf])","86e0a006":"_deter_vif(X_train_sm)\n# So as it can be seen here for each and every columns the VIF Score is less than 5 except for the const param\n# So we can build the model with each and every param ","5362f3f1":"# It seems all the data looks fine or we need to have a look into target variable again \n# We need to check whether the model performance is improving or not if we delete those extreme values\n\nplt.figure(figsize=(12,6))\nsns.boxplot(x=df_train.target, width=.4);\nplt.axvline(np.percentile(df_train.target,.1), label='.1%', c='orange', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(df_train.target,.15), label='.5%', c='darkblue', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(df_train.target,10), label='10%', c='green', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(df_train.target,99.9), label='99.9%', c='blue', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(df_train.target,99), label='99%', c='gold', linestyle=':', linewidth=3)\nplt.legend()\nplt.title('Box plot of target data', fontsize=16)\nplt.xticks(np.arange(0,df_train.target.max()+0.5,.5));","9ad94ba4":"# Lets the percentile .1, .15, 10 and < 99\n\ntrain_01_trunc = df_train[df_train.target > np.percentile(df_train.target, .1)]\ntrain_015_trunc = df_train[df_train.target > np.percentile(df_train.target, .15)]\ntrain_10_trunc = df_train[df_train.target > np.percentile(df_train.target, 10)]\ntrain_015_99trunc = df_train[(df_train.target > np.percentile(df_train.target, .15)) & (df_train.target < np.percentile(df_train.target, 99))]","d472ea04":"def plot_y_data(y, fr_which_chunk):\n    \"\"\"\n    Check how the target Values Looks like\n    And lets see the distribution of each\n    \"\"\"\n    plt.figure(figsize=(18,5))\n    plt.suptitle(f'Plot for Target for {fr_which_chunk}')\n    # Histogram \n    sns.distplot(y, ax = plt.subplot(1,3,1))\n    plt.ylabel('target')\n\n    # To check the spread of data\n    sns.boxplot(y = y, ax = plt.subplot(1,3,2))\n    sns.violinplot(x = y, ax = plt.subplot(1,3,3))\n    plt.ylabel('Distribution')\n    plt.show()","c1b4a90c":"plot_y_data(train_01_trunc.target, '>.1 Percentile')\nplot_y_data(train_015_trunc.target, '>.15 Percentile')\nplot_y_data(train_10_trunc.target, '>10 Percentile')\nplot_y_data(train_015_99trunc.target, '>.15 & <99 Percentile')","28c4c9fd":"# Splitting into Train and Test Split\nX_01 = train_01_trunc.drop('target', axis = 1)\ny_01 = train_01_trunc.target\nX_train_01, X_valid_01, y_train_01, y_valid_01 = train_test_split(X_01, y_01,train_size=0.8, test_size=0.2, random_state=42)\n\n# Encoding the Data\nordinal_encoder_01 = OrdinalEncoder()\nX_train_01[categorical_columns] = ordinal_encoder_01.fit_transform(X_train_01[categorical_columns])\nX_valid_01[categorical_columns] = ordinal_encoder_01.transform(X_valid_01[categorical_columns])\n\n# Feature Scaling\nsc_01 = StandardScaler()\nX_train_01[numeric_columns] = sc_01.fit_transform(X_train_01[numeric_columns])\nX_valid_01[numeric_columns] = sc_01.transform(X_valid_01[numeric_columns])\n\n# Fitting the model and check the performance\n\nxgb_model_01_percentile = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.625, gamma=0, gpu_id=0,\n             grow_policy='lossguide', importance_type='gain',\n             interaction_constraints='', learning_rate=0.194, max_delta_step=0,\n             max_depth=2, min_child_weight=1, missing=np.nan,\n             monotone_constraints='()', n_estimators=2000, n_jobs=-1,\n             num_parallel_tree=1, random_state=42, reg_alpha=14,\n             reg_lambda=72.5, scale_pos_weight=1, subsample=0.625,\n             tree_method='gpu_hist', validate_parameters=1,\n             verbosity=None, objective= 'reg:squarederror',\n             )\n\nxgb_model_01_percentile.fit(X_train_01, y_train_01, \n             eval_set=[(X_valid_01, y_valid_01)],\n             eval_metric = 'rmse',\n             verbose=False)\nnew_predictions = xgb_model_01_percentile.predict(X_valid_01)\nerrors = mean_squared_error(y_valid_01, new_predictions, squared=False)\nprint(f'RMSE: {errors}')","f29a48e1":"# Splitting into Train and Test Split\nX_015 = train_015_trunc.drop('target', axis = 1)\ny_015 = train_015_trunc.target\nX_train_015, X_valid_015, y_train_015, y_valid_015 = train_test_split(X_015, y_015,train_size=0.8, test_size=0.2, random_state=42)\n\n# Encoding the Data\nordinal_encoder_015 = OrdinalEncoder()\nX_train_015[categorical_columns] = ordinal_encoder_015.fit_transform(X_train_015[categorical_columns])\nX_valid_015[categorical_columns] = ordinal_encoder_015.transform(X_valid_015[categorical_columns])\n\n# Feature Scaling\nsc_015 = StandardScaler()\nX_train_015[numeric_columns] = sc_015.fit_transform(X_train_015[numeric_columns])\nX_valid_015[numeric_columns] = sc_015.transform(X_valid_015[numeric_columns])\n\n# Fitting the model and check the performance\n\nxgb_model_015_percentile = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.625, gamma=0, gpu_id=0,\n             grow_policy='lossguide', importance_type='gain',\n             interaction_constraints='', learning_rate=0.194, max_delta_step=0,\n             max_depth=2, min_child_weight=1, missing=np.nan,\n             monotone_constraints='()', n_estimators=2000, n_jobs=-1,\n             num_parallel_tree=1, random_state=42, reg_alpha=14,\n             reg_lambda=72.5, scale_pos_weight=1, subsample=0.625,\n             tree_method='gpu_hist', validate_parameters=1,\n             verbosity=None, objective= 'reg:squarederror',\n             )\n\nxgb_model_015_percentile.fit(X_train_015, y_train_015, \n             eval_set=[(X_valid_015, y_valid_015)],\n             eval_metric = 'rmse',\n             verbose=False)\nnew_predictions = xgb_model_015_percentile.predict(X_valid_015)\nerrors = mean_squared_error(y_valid_015, new_predictions, squared=False)\nprint(f'RMSE: {errors}')","9cade189":"# Splitting into Train and Test Split\nX_10 = train_10_trunc.drop('target', axis = 1)\ny_10 = train_10_trunc.target\nX_train_10, X_valid_10, y_train_10, y_valid_10 = train_test_split(X_10, y_10,train_size=0.8, test_size=0.2, random_state=42)\n\n# Encoding the Data\nordinal_encoder_10 = OrdinalEncoder()\nX_train_10[categorical_columns] = ordinal_encoder_10.fit_transform(X_train_10[categorical_columns])\nX_valid_10[categorical_columns] = ordinal_encoder_10.transform(X_valid_10[categorical_columns])\n\n# Feature Scaling\nsc_10 = StandardScaler()\nX_train_10[numeric_columns] = sc_10.fit_transform(X_train_10[numeric_columns])\nX_valid_10[numeric_columns] = sc_10.transform(X_valid_10[numeric_columns])\n\n# Fitting the model and check the performance\n\nxgb_model_10_percentile = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.625, gamma=0, gpu_id=0,\n             grow_policy='lossguide', importance_type='gain',\n             interaction_constraints='', learning_rate=0.194, max_delta_step=0,\n             max_depth=2, min_child_weight=1, missing=np.nan,\n             monotone_constraints='()', n_estimators=2000, n_jobs=-1,\n             num_parallel_tree=1, random_state=42, reg_alpha=14,\n             reg_lambda=72.5, scale_pos_weight=1, subsample=0.625,\n             tree_method='gpu_hist', validate_parameters=1,\n             verbosity=None, objective= 'reg:squarederror',\n             )\n\nxgb_model_10_percentile.fit(X_train_10, y_train_10, \n             eval_set=[(X_valid_10, y_valid_10)],\n             eval_metric = 'rmse',\n             verbose=False)\nnew_predictions = xgb_model_10_percentile.predict(X_valid_10)\nerrors = mean_squared_error(y_valid_10, new_predictions, squared=False)\nprint(f'RMSE: {errors}')","4e5e4efa":"# Splitting into Train and Test Split\nX_015_99 = train_015_99trunc.drop('target', axis = 1)\ny_015_99 = train_015_99trunc.target\nX_train_015_99, X_valid_015_99, y_train_015_99, y_valid_015_99 = train_test_split(X_015_99, y_015_99,train_size=0.8, test_size=0.2, random_state=42)\n\n# Encoding the Data\nordinal_encoder_015_99 = OrdinalEncoder()\nX_train_015_99[categorical_columns] = ordinal_encoder_015_99.fit_transform(X_train_015_99[categorical_columns])\nX_valid_015_99[categorical_columns] = ordinal_encoder_015_99.transform(X_valid_015_99[categorical_columns])\n\n# Feature Scaling\nsc_015_99 = StandardScaler()\nX_train_015_99[numeric_columns] = sc_015_99.fit_transform(X_train_015_99[numeric_columns])\nX_valid_015_99[numeric_columns] = sc_015_99.transform(X_valid_015_99[numeric_columns])\n\n# Fitting the model and check the performance\n\nxgb_model_015_99_percentile = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.625, gamma=0, gpu_id=0,\n             grow_policy='lossguide', importance_type='gain',\n             interaction_constraints='', learning_rate=0.194, max_delta_step=0,\n             max_depth=2, min_child_weight=1, missing=np.nan,\n             monotone_constraints='()', n_estimators=2000, n_jobs=-1,\n             num_parallel_tree=1, random_state=42, reg_alpha=14,\n             reg_lambda=72.5, scale_pos_weight=1, subsample=0.625,\n             tree_method='gpu_hist', validate_parameters=1,\n             verbosity=None, objective= 'reg:squarederror',\n             )\n\nxgb_model_015_99_percentile.fit(X_train_015_99, y_train_015_99, \n             eval_set=[(X_valid_015_99, y_valid_015_99)],\n             eval_metric = 'rmse',\n             verbose=False)\nnew_predictions = xgb_model_015_99_percentile.predict(X_valid_015_99)\nerrors = mean_squared_error(y_valid_015_99, new_predictions, squared=False)\nprint(f'RMSE: {errors}')","94a54fe9":"def perform_kfold(model, X, y, fitted_oe, fitted_sc, fold = 10):\n    kf = KFold(n_splits=fold, shuffle=True, random_state=42)\n\n    oof_preds = np.zeros((X.shape[0],))\n    start = time.time()\n    mean_rmse = 0\n    predictions = 0\n    model_fi = 0\n    final_rmse_per_fold = {}\n    X_test = df_test.copy()\n    X_test[numeric_columns] = fitted_sc.transform(X_test[numeric_columns])\n    X_test[categorical_columns] = fitted_oe.transform(X_test[categorical_columns])\n    for num, (train_id, valid_id) in enumerate(kf.split(X, y)):\n\n        X_train, X_valid = X.iloc[train_id, :], X.iloc[valid_id, :]\n        y_train, y_valid = y.iloc[train_id], y.iloc[valid_id]\n\n\n        # Preprocessing of training data, fit model \n        model.fit(X_train, y_train,\n                 verbose = False,\n                 eval_set = [(X_train, y_train),(X_valid, y_valid)],\n                 eval_metric = \"rmse\",\n                 early_stopping_rounds = 100)\n\n        #Mean of the predictions\n        predictions += model.predict(X_test) \/ fold # Splits\n\n        #Mean of feature importance\n        model_fi += model.feature_importances_ \/ fold #splits\n\n        #Out of Fold predictions\n        oof_preds[valid_id] = model.predict(X_valid)\n        fold_rmse = mean_squared_error(y_valid, oof_preds[valid_id], squared=False)\n#         print(f\"Fold {num} | RMSE: {fold_rmse}\")\n        final_rmse_per_fold[num+1] = fold_rmse\n\n        mean_rmse += fold_rmse \/ fold\n\n    end = time.time()\n\n    print(f\"\\nOverall RMSE: {mean_rmse}\")\n\n    # total time taken    \n    print(f\"Runtime of the program is {end - start} seconds\")\n    sns.lineplot(x = pd.Series(final_rmse_per_fold).index, y =pd.Series(final_rmse_per_fold).values )\n    plt.axhline(mean_rmse, label='Mean RMSE', c='orange', linestyle=':', linewidth=3)\n    plt.legend()\n    plt.show()\n    return predictions","da635ade":"print('='*5, \"Trying KFold Validation for >.1 Percentile\", '='*5)\nX_01_temp = X_01.copy()\nX_01_temp[categorical_columns] = ordinal_encoder_01.transform(X_01_temp[categorical_columns])\nX_01_temp[numeric_columns] = sc_01.transform(X_01_temp[numeric_columns])\nfold_01_pred = perform_kfold(xgb_model_01_percentile, X_01_temp, train_01_trunc.target, ordinal_encoder_01, sc_01)\n\nprint('='*5, \"Trying KFold Validation for >.15 Percentile\", '='*5)\nX_015_temp = X_015.copy()\nX_015_temp[categorical_columns] = ordinal_encoder_015.transform(X_015_temp[categorical_columns])\nX_015_temp[numeric_columns] = sc_015.transform(X_015_temp[numeric_columns])\nfold_015_pred = perform_kfold(xgb_model_015_percentile, X_015_temp, train_015_trunc.target, ordinal_encoder_015, sc_015)\n\nprint('='*5, \"Trying KFold Validation for >10 Percentile\", '='*5)\nX_10_temp = X_10.copy()\nX_10_temp[categorical_columns] = ordinal_encoder_10.transform(X_10_temp[categorical_columns])\nX_10_temp[numeric_columns] = sc_10.transform(X_10_temp[numeric_columns])\nfold_10_pred = perform_kfold(xgb_model_10_percentile, X_10_temp, train_10_trunc.target, ordinal_encoder_10, sc_10)\n\nprint('='*5, \"Trying KFold Validation for >.15 & <99 Percentile\", '='*5)\nX_015_99_temp = X_015_99.copy()\nX_015_99_temp[categorical_columns] = ordinal_encoder_015_99.transform(X_015_99_temp[categorical_columns])\nX_015_99_temp[numeric_columns] = sc_015_99.transform(X_015_99_temp[numeric_columns])\nfold_015_99_pred = perform_kfold(xgb_model_015_99_percentile, X_015_99_temp, train_015_99trunc.target, ordinal_encoder_015_99, sc_015_99)","587f4f09":"output_01 = pd.DataFrame({'Id': X_test.index,\n                       'target': fold_01_pred})\noutput_01.to_csv('submission_01.csv', index=False)\n\noutput_015 = pd.DataFrame({'Id': X_test.index,\n                       'target': fold_015_pred})\noutput_015.to_csv('submission_015.csv', index=False)\n\noutput_10 = pd.DataFrame({'Id': X_test.index,\n                       'target': fold_10_pred})\noutput_10.to_csv('submission_10.csv', index=False)\n\noutput_015_99 = pd.DataFrame({'Id': X_test.index,\n                       'target': fold_015_99_pred})\noutput_015_99.to_csv('submission_015_99.csv', index=False)","0f184898":"def objective(trial):\n    \n    param = {\n        'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n        'reg_lambda': trial.suggest_loguniform('reg_lambda',1e-3,80.0),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha',1e-3,16.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.1,0.12,0.2,0.5,0.6,0.7,0.8,0.9,1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.5,0.6,0.7,0.8, 0.96, 1.0]),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.5), \n        'n_estimators': trial.suggest_categorical('n_estimators', [1000,2000,5000,6000,8000, 10000]),\n        'max_depth': trial.suggest_categorical('max_depth', [2, 5,7,9,11,13,15,17,20]),\n        'random_state': trial.suggest_categorical('random_state', [24,40,2020]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1,300),\n        'use_label_encoder': trial.suggest_categorical('use_label_encoder',[False]),\n        'booster': trial.suggest_categorical('booster',['gbtree']),\n        'objective' : 'reg:squarederror',\n        'random_state' : 42\n    }\n    model = xgb.XGBRegressor(**param, n_jobs = -1)  \n    \n    model.fit(X_train_015,y_train_015,eval_set=[(X_valid_015,y_valid_015)],early_stopping_rounds=100,verbose=False, eval_metric = \"rmse\")\n    \n    preds = model.predict(X_valid_015)\n    \n    rmse = mean_squared_error(y_valid_015, preds,squared=False)\n    \n    return rmse","4514edb1":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=250, show_progress_bar=True)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","d04678d4":"# This is the completed o\/p of each trial run\nstudy.trials_dataframe()","7517a667":"#plot_optimization_histor: shows the scores from all trials as well as the best score so far at each point.\noptuna.visualization.plot_optimization_history(study)","301b1016":"#plot_parallel_coordinate: interactively visualizes the hyperparameters and scores\noptuna.visualization.plot_parallel_coordinate(study)","24b68810":"'''plot_slice: shows the evolution of the search. Y\nou can see where in the hyperparameter space your search\nwent and which parts of the space were explored more.'''\noptuna.visualization.plot_slice(study)","259ca611":"#Visualize parameter importances.\noptuna.visualization.plot_param_importances(study)","edd0f08c":"# tuned_hyper_param = {'tree_method':'gpu_hist', 'reg_lambda': 40.59386754695031, 'reg_alpha': 0.022824894471675152, 'colsample_bytree': 0.12, 'subsample': 0.96, 'learning_rate': 0.019628694971577185, 'n_estimators': 6000, 'max_depth': 5, 'random_state': 40, 'min_child_weight': 252, 'use_label_encoder': False, 'booster': 'gbtree'}\ntuned_hyper_param = {'tree_method':'gpu_hist', 'reg_lambda': 0.23607301912644416, 'reg_alpha': 0.08085311808601214, 'colsample_bytree': 0.2, 'subsample': 0.96, 'learning_rate': 0.10703076565484673, 'n_estimators': 10000, \n 'max_depth': 2, 'random_state': 2020, 'min_child_weight': 217, 'use_label_encoder': False, 'booster': 'gbtree'}\nxgb_model_015_percentile_tuned = XGBRegressor(**tuned_hyper_param, n_jobs = -1)\nxgb_model_015_percentile_tuned.fit(X_train_015, y_train_015, \n             eval_set=[(X_valid_015, y_valid_015)],\n             eval_metric = 'rmse',\n             early_stopping_rounds = 100,\n             verbose=False)\nnew_predictions = xgb_model_015_percentile.predict(X_valid_015)\nerrors = mean_squared_error(y_valid_015, new_predictions, squared=False)\nprint(f'RMSE: {errors}')\n\noutput_015_tuned = perform_kfold(xgb_model_015_percentile_tuned, X_train_015, y_train_015, ordinal_encoder_015, sc_015)\noutput_015_tuned = pd.DataFrame({'Id': X_test.index, 'target': output_015_tuned})\noutput_015_tuned.to_csv('submission_015_tuned.csv', index=False)","736276f0":"# Lets start with some baseline model with some rough set of values\nlgbm_parameters = {\n    'metric': 'RMSE',\n    'feature_pre_filter': False,\n    'lambda_l1': 0.45,\n    'lambda_l2': 4.8,\n    'learning_rate': 0.005,\n    'num_trees': 80000,\n    'num_leaves': 10, \n    'feature_fraction': 0.4, \n    'bagging_fraction': 1.0, \n    'bagging_freq': 0, \n    'min_child_samples': 100,\n    'num_threads': -1,\n    'device' : 'gpu'\n}\n\nlgbm_model = LGBMRegressor(**lgbm_parameters, random_state=42, n_jobs = -1)\nlgbm_model.fit(X_train, y_train, eval_set = [(X_valid,y_valid)], \n               early_stopping_rounds = 500,categorical_feature=categorical_columns, eval_metric = 'rmse',  verbose = -1) \npredictions_LGBM = lgbm_model.predict(X_valid)\nprint(f'RMSE: {mean_squared_error(y_valid, predictions_LGBM, squared=False)}')","4a262cb9":"#  probability plot of LGBM \nplot_probability_pot(lgbm_model, X_train, y_train, X_valid, y_valid)","47208475":"# Lets check the feature Importances of each variable\nplt.figure(figsize=(12,5)) \n# Calculate feature importances\nimportances = lgbm_model.feature_importances_\n# Sort feature importances in descending order\nindices = np.argsort(importances)[::-1]\n\n# Rearrange feature names so they match the sorted feature importances\nnames = [X_train.columns.tolist()[i] for i in indices]\n# Barplot: Add bars\nplt.bar(range(X_train.shape[1]), importances[indices])\n# Add feature names as x-axis labels\nplt.xticks(range(X.shape[1]), names, rotation=20, fontsize = 8)\n# Create plot title\nplt.title(\"Feature Importance\")\n# Show plot\nplt.show()","85c4ec60":"def objective_lgbm(trial):\n    \n    param = {\n        'metric': 'rmse', \n        'random_state': 42,\n        'n_estimators': 20000,\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.006,0.008,0.01,0.014,0.017,0.02]),\n        'max_depth': trial.suggest_categorical('max_depth', [10,20,100]),\n        'num_leaves' : trial.suggest_int('num_leaves', 1, 1000),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n        'cat_smooth' : trial.suggest_int('min_data_per_groups', 1, 100)\n    }\n    model = LGBMRegressor(**param, n_jobs = -1, device = 'gpu')  \n    \n    model.fit(X_train,y_train,eval_set=[(X_valid,y_valid)],early_stopping_rounds=100,verbose=False, eval_metric = \"rmse\")\n    \n    preds = model.predict(X_valid)\n    \n    rmse = mean_squared_error(y_valid, preds,squared=False)\n    \n    return rmse","325a8642":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective_lgbm, n_trials=20, show_progress_bar=True)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","fdc5f089":"#plot_optimization_histor: shows the scores from all trials as well as the best score so far at each point.\noptuna.visualization.plot_optimization_history(study)","502049d6":"#Visualize parameter importances.\noptuna.visualization.plot_param_importances(study)","089f8070":"# Lets build the model with the params\nparams=study.best_params   \nparams['random_state'] = 42\nparams['n_estimators'] = 20000 \nparams['metric'] = 'rmse'\nparams['device'] = 'gpu'\nparams['cat_smooth'] = params.pop('min_data_per_groups')\nparams['n_jobs'] = -1\n\nlgbm_model_tuned = LGBMRegressor(**params)\nlgbm_model_tuned.fit(X_train, y_train, eval_set = [(X_valid,y_valid)], \n               early_stopping_rounds = 500,categorical_feature=categorical_columns, eval_metric = 'rmse',  verbose = -1) \npredictions_LGBM = lgbm_model_tuned.predict(X_valid)\nprint(f'RMSE: {mean_squared_error(y_valid, predictions_LGBM, squared=False)}')","c3741a5b":"# As we have seen for the model with .15 percentile the model was performing better, lets try to validate the same again\noutput_015_lgbm_tuned = perform_kfold(lgbm_model_tuned, X_train_015, y_train_015, ordinal_encoder_015, sc_015)\noutput_015_lgbm_tuned = pd.DataFrame({'Id': X_test.index, 'target': output_015_lgbm_tuned})\noutput_015_lgbm_tuned.to_csv('submission_015_lgbm_tuned.csv', index=False)","58263733":"cat_parameters_1 = {    \n    'iterations':1600,\n    'learning_rate':0.024,\n    'l2_leaf_reg':20,\n    'random_strength':1.5,\n    'grow_policy':'Depthwise',\n    'leaf_estimation_method':'Newton', \n    'bootstrap_type':'Bernoulli',\n    'verbose':False,\n    'loss_function':'RMSE',\n    'eval_metric':'RMSE',\n    'od_type':'Iter',\n    'depth': 12,\n    'task_type' : 'GPU', \n    'devices': '0:1'\n}\n\ncat_model_1 = CatBoostRegressor(**cat_parameters_1)\ncat_model_1.fit(X_train, y_train, verbose = 400, eval_set=(X_valid,y_valid), use_best_model=True) \npredictions_cat_1 = cat_model_1.predict(X_valid)\nprint(mean_squared_error(y_valid, predictions_cat_1, squared=False))","7d86ae7e":"# Lets check the Feature Importance\nfea_imp = pd.DataFrame({'imp': cat_model_1.feature_importances_, 'col': X.columns})\nfea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc[-30:]\n_ = fea_imp.plot(kind='barh', x='col', y='imp', figsize=(10, 8))\nplt.title('Feature Importance')\nplt.show()","0bf8be9b":"def objective(trial):\n    \n    param = {\n        'iterations':1000,\n        'learning_rate':trial.suggest_discrete_uniform(\"learning_rate\", 0.001, 0.02, 0.001),\n        'use_best_model':True,\n        'od_type' : \"Iter\",\n        'od_wait' : 100,\n        \"depth\": trial.suggest_int(\"max_depth\", 2, 10),\n        \"l2_leaf_reg\": trial.suggest_loguniform(\"lambda\", 1e-8, 100),\n        'eval_metric':trial.suggest_categorical(\"loss_function\",['RMSE']),\n        'random_state': 42,\n        'min_child_samples' : trial.suggest_categorical('min_child_samples', [1, 4, 8, 16, 32]),\n        'grow_policy' : 'Depthwise',\n        'use_best_model': True\n    }\n    \n    regressor = CatBoostRegressor(**param)\n\n    regressor.fit(X_train, y_train,\n                  eval_set=[(X_valid, y_valid)],\n                  early_stopping_rounds=100, verbose = 400)\n    loss = mean_squared_error(y_valid, regressor.predict(X_valid), squared=False)\n    return loss","87ad25bf":"%%time\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=20, n_jobs=-1, show_progress_bar=True)","0aca26ca":"print('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","47f4de47":"#plot_optimization_histor: shows the scores from all trials as well as the best score so far at each point.\noptuna.visualization.plot_optimization_history(study)","1a38486d":"#Visualize parameter importances.\noptuna.visualization.plot_param_importances(study)","4f1b419d":"# Lets build the model after fine Tuning\nparam = {\n        'iterations':1000,\n        'learning_rate': 0.02,\n        'use_best_model':True,\n        'od_type' : \"Iter\",\n        'od_wait' : 100,\n        'max_depth': 8,\n        'l2_leaf_reg': 0.0005929377140741253,\n        'loss_function': 'RMSE',\n        'random_state': 42,\n        'min_child_samples': 16,\n        'grow_policy' : 'Depthwise',\n        'use_best_model': True\n    }\n\ncat_model_tuned = CatBoostRegressor(**param)\ncat_model_tuned.fit(X_train, y_train, verbose = 400, eval_set=(X_valid,y_valid), use_best_model=True) \npredictions_cat_tuned = cat_model_tuned.predict(X_valid)\nprint(mean_squared_error(y_valid, predictions_cat_tuned, squared=False))","8ec508aa":"# As we have seen earlier as well the target variable is skewed so for all the below model, so have to do some manipulation\nplt.figure(figsize=(10,5))\nsns.distplot(y_train_015, ax = plt.subplot(1,2,1))\nsns.distplot(y_valid_015, ax = plt.subplot(1,2,2))\nplt.suptitle('Distribution of Target Variable')\nplt.show()\n\nfig = sm.qqplot(y_train_015,line='s')\nplt.show()\n\n# It can be seen from QQ Plot that the target variable is not having normal distribution\n# Its a multinomial Distribution","0f8fe296":"lr_model = LinearRegression(n_jobs = -1) \nlr_model.fit(X_train_015, y_train_015)\npredictions = lr_model.predict(X_valid_015)\nprint(f'RMSE on Validation Set: {mean_squared_error(y_valid_015, predictions, squared=False)}')\n\n# Train Data Residual Analysis\nplot_error_term(lr_model, X_train_015, y_train_015)","acf0559c":"# list of alphas to tune\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ],\n         'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}\n\n# set up cross validation scheme\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 42)\n\nridge = Ridge(random_state = 42)\nmodel_cv = GridSearchCV(estimator = ridge, \n                        param_grid = params, \n                        scoring= 'neg_root_mean_squared_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1,\n                        n_jobs = -1) \nmodel_cv.fit(X_train_015, y_train_015) \n\nprint(f'Best Param {model_cv.best_params_}')\nprint(f'Best Score {-1 * model_cv.best_score_}')","8b87e2eb":"ridge_final = Ridge(alpha =  1000, solver= 'sparse_cg', random_state = 42)\nridge_final.fit(X_train_015, y_train_015)\npredictions = ridge_final.predict(X_valid_015)\nprint(f'RMSE : {mean_squared_error(y_valid_015, predictions, squared=False)}')","88a0a635":"params = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ],\n        'precompute' : [True,False], \n        'positive' : [True,False], \n        'warm_start' : [True,False],\n        'selection': ['random', 'cyclic']}\nlasso = Lasso(random_state= 42)\nmodel_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_root_mean_squared_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1,\n                        n_jobs = -1) \nmodel_cv.fit(X_train_015, y_train_015) \n\nprint(f'Best Param {model_cv.best_params_}')\nprint(f'Best Score {-1 * model_cv.best_score_}')","26467daf":"lasso_final = Lasso(alpha= 0.0001, positive= False, precompute= True, selection= 'cyclic', warm_start = True, random_state=42)\nlasso_final.fit(X_train_015, y_train_015)\npredictions = lasso_final.predict(X_valid_015)\nprint(f'RMSE : {mean_squared_error(y_valid_015, predictions, squared=False)}')","bb4b8a42":"# Lets First Create the Data with Fold Numbers then using that data All the model will be trained\ndf_train_fold = df_train.copy()\ndf_train_fold.loc[:, 'kfold'] = -1\ndf_train_fold = df_train_fold.sample(frac=1).reset_index(drop=True)\n\ny = df_train_fold.target.values\n\nkf = KFold(n_splits=5, random_state=42)\nfor f, (t_, v_) in enumerate(kf.split(X=df_train_fold, y = y)):\n    df_train_fold.loc[v_, \"kfold\"] = f","b61989ee":"# To Check the Fold Count distribution\ndf_train_fold.kfold.value_counts()","e5608e3a":"def run_train_rf(fold):\n    \"\"\"\n    Getting the Predictions for RandomForest Model\n    \"\"\"\n    df = df_train_fold.copy()\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    sc = StandardScaler()\n    df_train[numeric_columns] = sc.fit_transform(df_train[numeric_columns]) \n    df_valid[numeric_columns] = sc.transform(df_valid[numeric_columns])\n    \n    oe = OrdinalEncoder()\n    df_train[categorical_columns] = oe.fit_transform(df_train[categorical_columns])\n    df_valid[categorical_columns] = oe.transform(df_valid[categorical_columns])\n    \n    xtrain = df_train.drop('target', axis = 1)\n    xvalid = df_valid.drop('target', axis = 1)\n    \n    ytrain = df_train.target.values\n    yvalid = df_valid.target.values\n    \n    rf_model = RandomForestRegressor(criterion='mse', max_depth=30,\n           max_features='sqrt',\n           min_samples_leaf=2, min_samples_split=5,n_estimators=200, n_jobs=-1,\n           oob_score=False, random_state= 42, verbose=1, warm_start=True)\n    \n    rf_model.fit(xtrain, ytrain)\n    pred = rf_model.predict(xvalid)\n    rmse = mean_squared_error(yvalid, pred, squared=False)\n    \n    print(f'Fold= {fold}, RMSE = {rmse}')\n\n    df_valid.loc[:, 'rf_cnt_pred'] = pred\n    return df_valid.reset_index()[['index','target', 'kfold', 'rf_cnt_pred']]\n\ndfs =  []\nfor j in range(5):\n    temp_df = run_train_rf(j)\n    dfs.append(temp_df)\nfin_valid_df = pd.concat(dfs)\nprint(fin_valid_df.head())\nfin_valid_df.to_csv('rf_cnt.csv', index=False)","f1e0dbd3":"def run_train_xgb(fold):\n    \"\"\"\n    Getting the Predictions for XGBBoost Model\n    \"\"\"\n    df = df_train_fold.copy()\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    sc = StandardScaler()\n    df_train[numeric_columns] = sc.fit_transform(df_train[numeric_columns]) \n    df_valid[numeric_columns] = sc.transform(df_valid[numeric_columns])\n    \n    oe = OrdinalEncoder()\n    df_train[categorical_columns] = oe.fit_transform(df_train[categorical_columns])\n    df_valid[categorical_columns] = oe.transform(df_valid[categorical_columns])\n    \n    xtrain = df_train.drop('target', axis = 1)\n    xvalid = df_valid.drop('target', axis = 1)\n    \n    ytrain = df_train.target.values\n    yvalid = df_valid.target.values\n    \n    tuned_hyper_param = {'tree_method':'gpu_hist', 'reg_lambda': 0.23607301912644416, 'reg_alpha': 0.08085311808601214, 'colsample_bytree': 0.2, 'subsample': 0.96, 'learning_rate': 0.10703076565484673, 'n_estimators': 10000, \n     'max_depth': 2, 'random_state': 2020, 'min_child_weight': 217, 'use_label_encoder': False, 'booster': 'gbtree'}\n    \n    xgb_model = XGBRegressor(**tuned_hyper_param, n_jobs = -1)\n    xgb_model.fit(xtrain, ytrain, \n                 eval_set=[(xvalid, yvalid)],\n                 eval_metric = 'rmse',\n                 early_stopping_rounds = 100,\n                 verbose=False)\n    pred = xgb_model.predict(xvalid)\n    rmse = mean_squared_error(yvalid, pred, squared=False)\n    \n    print(f'Fold= {fold}, RMSE = {rmse}')\n\n    df_valid.loc[:, 'xgb_cnt_pred'] = pred\n    return df_valid.reset_index()[['index','target', 'kfold', 'xgb_cnt_pred']]\n\ndfs =  []\nfor j in range(5):\n    temp_df = run_train_xgb(j)\n    dfs.append(temp_df)\nfin_valid_df = pd.concat(dfs)\nprint(fin_valid_df.head())\nfin_valid_df.to_csv('xgb_cnt.csv', index=False)","d0256bd5":"def run_train_lgbm(fold):\n    \"\"\"\n    Getting the Predictions for LGBM Model\n    \"\"\"\n    df = df_train_fold.copy()\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    sc = StandardScaler()\n    df_train[numeric_columns] = sc.fit_transform(df_train[numeric_columns]) \n    df_valid[numeric_columns] = sc.transform(df_valid[numeric_columns])\n    \n    oe = OrdinalEncoder()\n    df_train[categorical_columns] = oe.fit_transform(df_train[categorical_columns])\n    df_valid[categorical_columns] = oe.transform(df_valid[categorical_columns])\n    \n    xtrain = df_train.drop('target', axis = 1)\n    xvalid = df_valid.drop('target', axis = 1)\n    \n    ytrain = df_train.target.values\n    yvalid = df_valid.target.values\n    \n    params = {'reg_alpha': 0.007524492391650151, 'reg_lambda': 7.302151455638408, 'colsample_bytree': 0.3, 'subsample': 0.6, 'learning_rate': 0.008, 'max_depth': 20, 'num_leaves': 10, 'min_child_samples': 283, 'min_data_per_groups': 39}\n    params['random_state'] = 42\n    params['n_estimators'] = 20000 \n    params['metric'] = 'rmse'\n    params['device'] = 'gpu'\n    params['cat_smooth'] = params.pop('min_data_per_groups')\n    params['n_jobs'] = -1\n\n    lgbm_model = LGBMRegressor(**params)\n    lgbm_model.fit(xtrain, ytrain, eval_set = [(xvalid,yvalid)],categorical_feature=categorical_columns, eval_metric = 'rmse',  verbose = -1) \n    pred = lgbm_model.predict(xvalid)\n    rmse = mean_squared_error(yvalid, pred, squared=False)\n    \n    print(f'Fold= {fold}, RMSE = {rmse}')\n\n    df_valid.loc[:, 'lgbm_cnt_pred'] = pred\n    return df_valid.reset_index()[['index','target', 'kfold', 'lgbm_cnt_pred']]\n\ndfs =  []\nfor j in range(5):\n    temp_df = run_train_lgbm(j)\n    dfs.append(temp_df)\nfin_valid_df = pd.concat(dfs)\nprint(fin_valid_df.head())\nfin_valid_df.to_csv('lgbm_cnt.csv', index=False)","5b997a89":"def run_train_catboost(fold):\n    \"\"\"\n    Getting the Predictions for CatBoost Model\n    \"\"\"\n    df = df_train_fold.copy()\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    sc = StandardScaler()\n    df_train[numeric_columns] = sc.fit_transform(df_train[numeric_columns]) \n    df_valid[numeric_columns] = sc.transform(df_valid[numeric_columns])\n    \n    oe = OrdinalEncoder()\n    df_train[categorical_columns] = oe.fit_transform(df_train[categorical_columns])\n    df_valid[categorical_columns] = oe.transform(df_valid[categorical_columns])\n    \n    xtrain = df_train.drop('target', axis = 1)\n    xvalid = df_valid.drop('target', axis = 1)\n    \n    ytrain = df_train.target.values\n    yvalid = df_valid.target.values\n    \n    param = {\n        'iterations':1000,\n        'learning_rate': 0.02,\n        'use_best_model':True,\n        'od_type' : \"Iter\",\n        'od_wait' : 100,\n        'max_depth': 8,\n        'l2_leaf_reg': 0.0005929377140741253,\n        'loss_function': 'RMSE',\n        'random_state': 42,\n        'min_child_samples': 16,\n        'grow_policy' : 'Depthwise',\n        'use_best_model': True\n    }\n\n    cat_model = CatBoostRegressor(**param)\n    cat_model.fit(xtrain, ytrain, verbose = 500, eval_set=(xvalid,yvalid), use_best_model=True)  \n    pred = cat_model.predict(xvalid)\n    rmse = mean_squared_error(yvalid, pred, squared=False)\n    \n    print(f'Fold= {fold}, RMSE = {rmse}')\n\n    df_valid.loc[:, 'cat_cnt_pred'] = pred\n    return df_valid.reset_index()[['index','target', 'kfold', 'cat_cnt_pred']]\n\ndfs =  []\nfor j in range(5):\n    temp_df = run_train_catboost(j)\n    dfs.append(temp_df)\nfin_valid_df = pd.concat(dfs)\nprint(fin_valid_df.head())\nfin_valid_df.to_csv('cat_cnt.csv', index=False)","9783172f":"def run_train_lr(fold):\n    \"\"\"\n    Getting the Predictions for Linear Regression Model\n    \"\"\"\n    df = df_train_fold.copy()\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    sc = StandardScaler()\n    df_train[numeric_columns] = sc.fit_transform(df_train[numeric_columns]) \n    df_valid[numeric_columns] = sc.transform(df_valid[numeric_columns])\n    \n    oe = OrdinalEncoder()\n    df_train[categorical_columns] = oe.fit_transform(df_train[categorical_columns])\n    df_valid[categorical_columns] = oe.transform(df_valid[categorical_columns])\n    \n    xtrain = df_train.drop('target', axis = 1)\n    xvalid = df_valid.drop('target', axis = 1)\n    \n    ytrain = df_train.target.values\n    yvalid = df_valid.target.values\n    \n    lr_model = LinearRegression(n_jobs = -1) \n    lr_model.fit(xtrain, ytrain) \n    pred = lr_model.predict(xvalid)\n    rmse = mean_squared_error(yvalid, pred, squared=False)\n    \n    print(f'Fold= {fold}, RMSE = {rmse}')\n\n    df_valid.loc[:, 'lr_cnt_pred'] = pred\n    return df_valid.reset_index()[['index','target', 'kfold', 'lr_cnt_pred']]\n\ndfs =  []\nfor j in range(5):\n    temp_df = run_train_lr(j)\n    dfs.append(temp_df)\nfin_valid_df = pd.concat(dfs)\nprint(fin_valid_df.head())\nfin_valid_df.to_csv('lr_cnt.csv', index=False)","5c9cf958":"def run_train_ridge(fold):\n    \"\"\"\n    Getting the Predictions for Ridge Regression Model\n    \"\"\"\n    df = df_train_fold.copy()\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    sc = StandardScaler()\n    df_train[numeric_columns] = sc.fit_transform(df_train[numeric_columns]) \n    df_valid[numeric_columns] = sc.transform(df_valid[numeric_columns])\n    \n    oe = OrdinalEncoder()\n    df_train[categorical_columns] = oe.fit_transform(df_train[categorical_columns])\n    df_valid[categorical_columns] = oe.transform(df_valid[categorical_columns])\n    \n    xtrain = df_train.drop('target', axis = 1)\n    xvalid = df_valid.drop('target', axis = 1)\n    \n    ytrain = df_train.target.values\n    yvalid = df_valid.target.values\n    \n    ridge_model = Ridge(alpha =  1000, solver= 'sparse_cg', random_state = 42)\n    ridge_model.fit(xtrain, ytrain)\n    pred = ridge_model.predict(xvalid)\n    rmse = mean_squared_error(yvalid, pred, squared=False)\n    \n    print(f'Fold= {fold}, RMSE = {rmse}')\n\n    df_valid.loc[:, 'ridge_cnt_pred'] = pred\n    return df_valid.reset_index()[['index','target', 'kfold', 'ridge_cnt_pred']]\n\ndfs =  []\nfor j in range(5):\n    temp_df = run_train_ridge(j)\n    dfs.append(temp_df)\nfin_valid_df = pd.concat(dfs)\nprint(fin_valid_df.head())\nfin_valid_df.to_csv('ridge_cnt_pred.csv', index=False)","ce8c5ba4":"def run_train_lasso(fold):\n    \"\"\"\n    Getting the Predictions for Lasso Regression Model\n    \"\"\"\n    df = df_train_fold.copy()\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    sc = StandardScaler()\n    df_train[numeric_columns] = sc.fit_transform(df_train[numeric_columns]) \n    df_valid[numeric_columns] = sc.transform(df_valid[numeric_columns])\n    \n    oe = OrdinalEncoder()\n    df_train[categorical_columns] = oe.fit_transform(df_train[categorical_columns])\n    df_valid[categorical_columns] = oe.transform(df_valid[categorical_columns])\n    \n    xtrain = df_train.drop('target', axis = 1)\n    xvalid = df_valid.drop('target', axis = 1)\n    \n    ytrain = df_train.target.values\n    yvalid = df_valid.target.values\n    \n    lasso_model = Lasso(alpha= 0.0001, positive= False, precompute= True, selection= 'cyclic', warm_start = True, random_state=42)\n    lasso_model.fit(xtrain, ytrain)\n    pred = lasso_model.predict(xvalid)\n    rmse = mean_squared_error(yvalid, pred, squared=False)\n    \n    print(f'Fold= {fold}, RMSE = {rmse}')\n\n    df_valid.loc[:, 'lasso_cnt_pred'] = pred\n    return df_valid.reset_index()[['index','target', 'kfold', 'lasso_cnt_pred']]\n\ndfs =  []\nfor j in range(5):\n    temp_df = run_train_lasso(j)\n    dfs.append(temp_df)\nfin_valid_df = pd.concat(dfs)\nprint(fin_valid_df.head())\nfin_valid_df.to_csv('lasso_cnt_pred.csv', index=False)","05dd4577":"# # Lets prepare the test for prediction\nX_test = df_test.copy()\nX_test[categorical_columns] = ordinal_encoder.transform(X_test[categorical_columns])\nX_test[numeric_columns] = sc.transform(X_test[numeric_columns])","8a9d577a":"# Lets get All the Predictions and we would do averaging\n\nprint('Running Random Forest')\nrf_model = RandomForestRegressor(criterion='mse', max_depth=30,\n       max_features='sqrt',\n       min_samples_leaf=2, min_samples_split=5,n_estimators=200, n_jobs=-1,\n       oob_score=False, random_state= 42, verbose=1, warm_start=True)\n\nrf_model.fit(X_train, y_train)\nrf_pred= rf_model.predict(X_test)\n\n\nprint('Running XGBBost Forest')\ntuned_hyper_param = {'tree_method':'gpu_hist', 'reg_lambda': 0.23607301912644416, 'reg_alpha': 0.08085311808601214, 'colsample_bytree': 0.2, 'subsample': 0.96, 'learning_rate': 0.10703076565484673, 'n_estimators': 10000, \n 'max_depth': 2, 'random_state': 2020, 'min_child_weight': 217, 'use_label_encoder': False, 'booster': 'gbtree'}\n\nxgb_model = XGBRegressor(**tuned_hyper_param, n_jobs = -1)\nxgb_model.fit(X_train, y_train, \n             eval_set=[(X_valid, y_valid)],\n             eval_metric = 'rmse',\n             early_stopping_rounds = 100,\n             verbose=False)\nxgb_pred = xgb_model.predict(X_test)\n\nprint('Running for LGBM')\nparams = {'reg_alpha': 0.007524492391650151, 'reg_lambda': 7.302151455638408, 'colsample_bytree': 0.3, 'subsample': 0.6, 'learning_rate': 0.008, 'max_depth': 20, 'num_leaves': 10, 'min_child_samples': 283, 'min_data_per_groups': 39}\nparams['random_state'] = 42\nparams['n_estimators'] = 20000 \nparams['metric'] = 'rmse'\nparams['device'] = 'gpu'\nparams['cat_smooth'] = params.pop('min_data_per_groups')\nparams['n_jobs'] = -1\n\nlgbm_model = LGBMRegressor(**params)\nlgbm_model.fit(X_train, y_train, eval_set = [(X_valid,y_valid)],categorical_feature=categorical_columns, eval_metric = 'rmse',  verbose = -1) \nlgbm_pred = lgbm_model.predict(X_test)\n\nprint('Running for Catboost')\nparam = {\n        'iterations':1000,\n        'learning_rate': 0.02,\n        'use_best_model':True,\n        'od_type' : \"Iter\",\n        'od_wait' : 100,\n        'max_depth': 8,\n        'l2_leaf_reg': 0.0005929377140741253,\n        'loss_function': 'RMSE',\n        'random_state': 42,\n        'min_child_samples': 16,\n        'grow_policy' : 'Depthwise',\n        'use_best_model': True\n    }\n\ncat_model = CatBoostRegressor(**param)\ncat_model.fit(X_train, y_train, verbose = 500, eval_set=(X_valid,y_valid), use_best_model=True)  \ncat_model_pred = cat_model.predict(X_test)\n\nprint('Running for Linear Regression')\nlr_model = LinearRegression(n_jobs = -1) \nlr_model.fit(X_train, y_train) \nlr_pred = lr_model.predict(X_test)\n\nprint('Running for  Ridge regression')\nridge_model = Ridge(alpha =  1000, solver= 'sparse_cg', random_state = 42)\nridge_model.fit(X_train, y_train)\nridge_pred = ridge_model.predict(X_test)\n\nprint('Running for  Lasso regression')\nlasso_model = Lasso(alpha= 0.0001, positive= False, precompute= True, selection= 'cyclic', warm_start = True, random_state=42)\nlasso_model.fit(X_train, y_train)\nlasso_pred = lasso_model.predict(X_test)","29d682c8":"# # # Making the first set of prediction using Average blending\npreds = (rf_pred + xgb_pred + lgbm_pred + cat_model_pred + lr_pred + lasso_pred + ridge_pred) \/ 7\noutput = pd.DataFrame({'Id': X_test.index,\n                       'target': preds})\noutput.to_csv('submission.csv', index=False)","db7b0236":"**Conclusion** - As it can seen around 0.00124% of Data are below 6 hence we can for now ignore the these data as it makes the target variable right skewed and it looks more like an outlier.\nHence dropping all these row data where the y is less than 6","8d7d0f2f":"As it can be seen above that we can't just proceed with the model building we need to do some EDA and Data Preprocessing and then build the model\n\n## EDA","8c824571":"**Conclusion** - As it can be seen here there is a little bit improvement in the model performance. \n1. R Squared have increased \n2. As well as Adjusted R Square Values increased quite significantly \n3. As we can see model is able to able to predict value starting from Range >=7 to <=9","9a4f3b45":"### Data Preparation Steps","b806ce9e":"**Conclusion**\nAs it can be seen here there is no significant improvement on RMSE \/ Accuracy hence lets proceed with the next steps","6a84b8a2":"> #### Model Building with Target variable >.15 Percentile","6fa6a4b7":"### Lets Try to Build Models with different Percentile Values","dd844eaa":"**Conclusion** - It can be seen here with all the categorical columns removed the model is behaving even bad than before, so even if the model's parameter prediction power is less, but still it contributing to some extend. Hence we can't remove this part","293c50fb":"**Conclusion** - \n* As it can be seen here for most of the categorical data, 25% to 75% data lies between the range 7-9 of the target variable, as it can be seen when we tried to do the bivariate analysis of the data \n* And it can also be seen that there are some categorical Value, where mostly they are dominating in the whole data , like cat7, cat2, cat4 ..., so an imbalance is present in the dataset\n* And it can seen above for most of the Categories the KDE Plot is flat compared to other values and this can also be seen in the categorical plot as well","ea71142d":"> #### Model Building with Target variable >10 Percentile","41d1223a":"**Conclusion** - As it can be seen most of the train and test data are overlapping, lets proceed with the categorical values.","4bb1d4aa":"### Ridge Regression","50b7f862":"**Conclusion**\n1. As it can seen here most of the coefficient is near to zero\n2. And the R Squared Value is not so significant\n3. There are also some feature for which there is a high P Value i.e. >0.05, so lets check if there is any need to remove any features ","6c66062c":"It can be seen above that after fine tuning the Hyperparameter the RMSE improved little bit.","27d88c4f":"## Importing Necessary Libraries","c38f1e32":"## Building the Model with Random Forest Model","e11db82b":"## Reading Data","dd50a8b7":"## Lets try to see how the Model is behaving while applying the KFold Validation","81438a16":"### Lets Perform Some Cross Validation, to find out the ideal set of model parameters","4bb21502":"### Lets Do some more EDA so as to increase the model performance","8c05ad65":"**Conclusion** - \n* It can be seen above that the categorical value trend for the train and the test dataset is almost same \n* And the Number of Distinct Categories in Train and Test Dataset is also same","ca4c2cd6":"## Building the Model with XGBoost","76c31e20":"### Lets try to tweak the performance of LGBM using Optuna","59f3a45f":"**Conclusion** - It seems the hyperparameter needs some tuning, lets do the same using Optuna","7c81a918":"**Conclusion** As it can be seen the range value differs a lot, some feature have min values in -ve as well. We will below how the features will be looking","1c9f9db8":"## Analyzing the Data","7220fb71":"**Conclusion** - As it can be seen here most of the data doesn't have much outliers and but the range of the data is varying so we need some scalling","6297725c":"**Conclusion** - As it can be seen the data for each of the continous data have a multimodel distribution except a few, and for each one of them, as we can see when we did a bivariate analysis there are very less points which has target value < 6 ","67e60367":"### Lasso Regression","42a1611f":"**Conclusion Driven**\nIt can be seen above that the Target Variable is positively skewed and nearly 75% of data is more than 6 ","29065106":"**Conclusion**: As it can be seen here most of the categories have less categories except for cat9","233bd00e":"**Conclusion** - It seems there are around 15 Numeric Columns and 10 Categorical Columns","59887b3d":"## Lets Build Model using LGBM ( Light Gradient Boosting Algo )","ebccd6ec":"**Conclusion** - So it seems Test Score best came when we took the data > .15 percentile\n\n### Next is to Tune Some Hyperparameter using Optuna on the same data set where target >.15 Percentile","e08a97e0":"**Conclusion**\nThese stats definitely prove that some variables are much more important to our problem than others! Given that there are so many variables with zero importance (or near-zero due to rounding). But it seems like we should be able to get rid of some of them without impacting performance.\nAnd this is the same insight we got in our baseline model building part as well","e853abd5":"## Lets Build Some more Models\n\n### Linear Regression\n\nAs we can seen above that there are many target variable which are less than target 6 and we have seenin above when we took target variable >.15 percentile the model performed significantly better, hence we would proceed with that ","be21b41e":"**Conclusion**\nHighest correlation between features is 0.5.\nCorrelation between features on train and test dataset are quite similar.\nIt seems the target value is weakly correlated with most of the data\n\nAnd also it can be seen in the pair plot there is no string correlation between variables and as it can idenfied earlier also most of the numerical variables also have a multinominal distribution","031e9fce":"**Conclusion** - This RMSE is definetly better than the RMSE we got when we have build the XGB model with all features","a634a8ea":"**Conclusion** - As it can be seen here most of the features and not even crossing 0.06 mark and qll th features are weakly correlated with the target variable","cc765f48":"**Conclusion** - As it can be seen here the model performance is almost same, so there is a very less improvement in the model's RMSE,\nlets do some more analysis.\nWe can try to do do the KFold Approach and lets see how it behaves","d47b4237":"**Conclusion** - Model's prediction power increased a bit lets try to find a better score","8e4bdc68":"**Conclusion**\n- So for each of these split a Model is required to check whether removing the data really improves model quality or not\n- For Each of Histogram is looks more like a multimodal distribution and we can clearly observe a step like structure\n\n### Lets Start with the Model Building Part\n\n> #### Model Building with Target variable >.1 Percentile","d4245e7f":"## Lets Build Model using CatBoost ","e27bb84e":"> #### Model Building with Target variable >15 & <99 Percentile","029886a5":"## Creating a BaseLine Model","e276430c":"**Conclusion** - For the above Models, i.e. Linear Regression, Ridge and Lasso Regression the model is not performing better.\n\nLets try some way to combine all these models and lets see the prediction.\nUsing the Model Ensembling, Blending and Stacking\n\n**Pre-requisite** - All the model needs to be trained on the same Folds and then we will perform the rest of the operation for Each Models, which includes,\n\n*RandomForest, XGBosst, LGBM, Catboost, LinearRegression, Lasso & Ridge*","8b211b65":"### Lets Apply KFold and lets really see how the model is behaving for the above 4 Models"}}