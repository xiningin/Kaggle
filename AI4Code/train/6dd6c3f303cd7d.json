{"cell_type":{"75e35d3d":"code","3e67567d":"code","c8244e45":"code","da9a69a4":"code","b7c51f0a":"code","82171a0a":"code","823cc064":"code","6c31a63d":"code","60f2e6c4":"code","c0e43766":"code","1780ac25":"code","eb6e6dbe":"code","5062b09e":"code","e7e097f1":"code","d77ed7bc":"code","6516e8d7":"code","1620ad06":"code","a1c4db31":"code","d16853b0":"code","f7905f2d":"code","7b3a5de0":"code","94ab51ad":"code","6a33ac32":"markdown","71050daf":"markdown","6c644f5b":"markdown","3b732cde":"markdown","d93e4fb6":"markdown","dba083fb":"markdown"},"source":{"75e35d3d":"import numpy as np \nimport pandas as pd \nimport lightgbm as lgbm\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import cross_val_score,train_test_split,RandomizedSearchCV\nfrom sklearn.metrics import fbeta_score,make_scorer,recall_score,precision_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","3e67567d":"data = pd.read_csv(\"\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")","c8244e45":"data","da9a69a4":"pd.DataFrame(data.groupby(\"stroke\").count()[\"id\"])","b7c51f0a":"fig1 = plt.figure(figsize=(16,5))\nsns.heatmap(data.isna().T)\nplt.title(\"Null Values\")","82171a0a":"def age_groups(age):\n    if age >= 0 and age <=18:\n        age_group=\"to 18 years\"\n    elif age > 18 and age <=29:\n        age_group=\"18-29 years\"\n    elif age>29 and age <=40:\n        age_group=\"30-40 years\"\n    elif age>40 and age <=50:\n        age_group = \"40-50 years\"\n    elif age >50 and age <= 60:\n        age_group =\"50-60 years\"\n    elif age >60:\n        age_group=\"more than 60 years\"\n    return age_group    ","823cc064":"data[\"age_group\"]=data[\"age\"].apply(age_groups)","6c31a63d":"data.groupby([\"gender\",\"age_group\",\"hypertension\",\"smoking_status\"]).mean()","60f2e6c4":"data['bmi'] = data['bmi'].fillna(data.groupby([\"gender\",\"age_group\",\"hypertension\"])['bmi'].transform('mean'))","c0e43766":"data.drop([\"age_group\",\"id\"],axis=1,inplace=True)","1780ac25":"data.isna().sum()","eb6e6dbe":"data","5062b09e":"enc = OrdinalEncoder()","e7e097f1":"objects = list(data.select_dtypes(\"object\").columns)\ndata_objects = pd.DataFrame(enc.fit_transform(data[objects]))\ndata_objects.columns=objects\n\nfor column in objects:\n    data[column] = data_objects[column]","d77ed7bc":"X = data[data.columns[:10]]\nY = data[\"stroke\"]\n\nX_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.15,random_state=101)","6516e8d7":"f2_scorer = make_scorer(fbeta_score, beta=2.0)\nrecall_scorer = make_scorer(recall_score)","1620ad06":"random_model = lgbm.LGBMClassifier()\n\nparams = {\n    'n_estimators': np.random.randint(50,120,80),\n    'learning_rate': np.random.uniform(0.01,0.2,20),\n    'num_leaves': np.random.randint(25,60,80),\n    'boosting_type' : ['gbdt'],\n    'objective' : ['binary'],\n    'max_depth' : np.random.randint(3,11,20),\n    'random_state' : [101],\n    \"scale_pos_weight\": np.random.randint(15,500,1000)\n    }\n\ngrid = RandomizedSearchCV(random_model,params,verbose=1,cv=10,n_jobs = -1,n_iter=150,scoring=recall_scorer)\ngrid.fit(X,Y)","a1c4db31":"grid.best_params_","d16853b0":"model = lgbm.LGBMClassifier(\n    n_estimators = grid.best_params_.get(\"n_estimators\"),\n    learning_rate=grid.best_params_.get(\"learning_rate\"),\n    objective = \"binary\",\n    num_leaves = grid.best_params_.get(\"num_leaves\"),\n    max_depth = grid.best_params_.get(\"max_depth\"),\n    boosting_type = \"gbdt\",\n    scale_pos_weight=grid.best_params_.get(\"scale_pos_weight\")\n)\n\nmodel.fit(X_train,y_train)","f7905f2d":"predictions = model.predict(X_test)\nrecall_results = recall_score(y_test,predictions)\nf2_results = fbeta_score(y_test,predictions,beta=2.0)\nprecision_results = precision_score(y_test,predictions)\nprint('Precision Score: %.3f' % (precision_results))\nprint('F2 Score: %.3f' % (f2_results))\nprint('Recall Score: %.3f' % (recall_results))","7b3a5de0":"beta_score_list = []\nfor i in range(1,101):\n    fbeta_results = fbeta_score(y_test,predictions,beta=i)\n    beta_score_list.append(fbeta_results)","94ab51ad":"fig2 = plt.figure(figsize=(10,6))\nsns.lineplot(x=range(1,101),y=beta_score_list)\nplt.title(\"Fbeta scores with beta 0 to 100\")","6a33ac32":"Here we will fill NaN with mean from our particular groups and later we will drop age_group featue","71050daf":"So generally our model (Gradient Boosting Machines) can deal with null values, however I not know how it deals with those null values.<br>\nAs we can see we are missing BMI values for our patients and it might be beneficial to fill those values. However we can not just fill them with zeros or avg\/median <br>values as it wouldn't make much of a sense (quite possibly model would also either treat nulls as zeros or something close to that).<br>\n\nIn this case we need to use what we have, and we have other features with in real-life are related to our bmi value. E.g. Hypertension can be caused by Obesity (Obesity-Induced Hypertension).<br>\n\nI feel important features that we should groupby in that case are: <br>\n1) Gender<br>\n2) Age (We will create age group)<br>\n3) Hypertension<br>","6c644f5b":"## Modeling","3b732cde":"# SUMMARY\n\nWe have build model using Gradient Boosting Classifier from Microsoft LightGBM. It helped us with a lot of preprocessing work. <br>\nFor encoding we have used ordinal encoder. <br>\nAs a way to deal with class imbalanced, we have used scale_pos_weight parameter in our model.<br>\nHyperparameter tuning was done with RandomizedCV (due to better speed than GridSearchCV).<br>\n\nWe measured our model performance with three metrics.<br>\n- FBeta<br>\n- Recall<br>\n- Precision<br>\n\nBest obtained results<br>\nPrecision Score: 0.094<br>\nF2 Score: 0.333<br>\nRecall Score: 0.909<br>\n\nOur conclusion is that with this particular dataset it is really difficult to predict stroke. <br>\nRecall score indicates that we can successfully detect true positive values, so our model is pretty good at this.<br>\nUnfortunately however Precision is sitting at only 9.4% which means that majority of our positive cases are misses.<br>\nOur final score is F2 score which acts as balancing act between Precision and Recall and in very short way it indicates overall performance of our model.<br>\n\nNow what can we do with this model. With this kind of knowledge we can consider application. <br>\nIf we achieved a little more precision and recall (let's say recall at 96% and precision at 15-20%) then this model would be quite good for indicating which patients<br> are in group of higher risk of stroke. Other than that, it is pretty poor performing model.<br>\nWe can always search for better parameters, but I doubt we will get much more out of this dataset (if you have any ideas how to improve it, let me know).<br>\n\nI have seen tons of people measuring accuracy here. They do not take into account class imbalance or other metrics and it makes these models just unrealistic.<br>\n\nAs always thank you for attention,<br>\nIf you have noticed any mistakes or room for improvements, please leave a comment.<br>","d93e4fb6":"# Stroke Risk - Fbeta IS THE KEY (LightGBM)\n**Notebook Author** <br>\n**Aleksander Jakubowski**\n\n**Context** <br>\nAccording to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.\nThis dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient.\n\n**Attribute Information** <br>\n1) id: unique identifier<br>\n2) gender: \"Male\", \"Female\" or \"Other\"<br>\n3) age: age of the patient<br>\n4) hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension<br>\n5) heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease<br>\n6) ever_married: \"No\" or \"Yes\"<br>\n7) work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"<br>\n8) Residence_type: \"Rural\" or \"Urban\"<br>\n9) avg_glucose_level: average glucose level in blood<br>\n10) bmi: body mass index<br>\n11) smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*<br>\n12) stroke: 1 if the patient had a stroke or 0 if not<br>\n*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient<br>\n\n**Acknowledgements** <br>\n(Confidential Source) - Use only for educational purposes\nIf you use this dataset in your research, please credit the author.\n","dba083fb":"## Dataset and Objective\n\nWe are not going to go into in-depth analysis with this dataset. I feel like BI tools (e.g Tabelau, Power BI, Qlik), would do much better job in that case.<br>\n\n**Target Variable** <br>\nWe need to look if our y (stroke classes) are imbalanced (Very likely case) <br>\n\n**Independent Variables** <br>\nIn our datasets we have couple of issues to address. First one are categorical features. We have to encode those. When we encode we need to pick encoding type (e.g One Hot or Ordinal) and we base our decision on the model we are going to train. If our model is sensitive to non standardized\/normalized values in our data, then we have to deal with those too. <br>\nIn majority of problems like this Gradient boosting models are among the best in the field since they can deal with non-standardized data, categorical features, missing values and class imbalance. <br>\nSince it is not competition, I will not stack many models to get extremely accurate result. The library and model I pick is Microsoft LightGBM with its Gradient Boosting Machine Classifier.<br>\n\n### What is important to take into account is evaluation metric. What I saw in a lot of other works here on Kaggle,is that people that keep repeating the same mistake. They measure accuracy, precision, best case scenerio f1 score and it is just WRONG.<br>\n\nWhat you need to address is what you don't want to happen. We don't want to have people who might have a stroke to be predicted as healthy. So we want to see how our model is predicting Positive value, and we want to punish our score when model doesn't detect person that is positive. Which means that we want to go with Fbeta Score which is good metrics when cost of False Positive is smaller than False Negative in other words. <br>\n\n**If our patient goes onto list of potentially at risk of stroke without being at risk then not much will change for him\/her. However for person that is at risk it is important to make sure that person has the best care and knows of his\/her state and implement treatment.**<br>\n\n\n\nFor more information regarding Fbeta Metrics I highly encourage these sources.<br>\n[deep ai F2 Score](https:\/\/deepai.org\/machine-learning-glossary-and-terms\/f-score) <br>\n[Sklearn Fbeta Score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.fbeta_score.html)<br>\n\n\nApart from Fbeta, I suggest to also focus on Recall and Precision. We might have good recall and Fbeta but our model might not be precise (detect most of things as positive).\n\nGoing back to dataset as we can see below, we indeed work with imbalanced dataset."}}