{"cell_type":{"f6de182c":"code","5ac49086":"code","bebd516c":"code","1611d83c":"code","5e7c7a18":"code","87d1a69f":"code","fe2181a9":"code","36952d6a":"code","6ce9202a":"code","148e65fe":"code","5d0caf84":"code","335c0455":"code","2a8ac71d":"code","b93785af":"code","ed792e7c":"code","8bf4461b":"code","962df2d6":"code","6a26fc87":"code","c909342b":"code","e246dbea":"code","8fa2b9c9":"code","a44d4aab":"code","f40b2cc2":"code","577fd021":"code","df266006":"code","b5ae9d4a":"code","10aa9872":"code","b953c549":"code","31218491":"code","1df6628f":"code","f02eaa23":"code","e148e984":"code","15976e49":"code","86ce5c62":"code","1d65a700":"code","46db0703":"code","53bb1d44":"code","4c443337":"code","74325b8f":"code","cf69cfa0":"code","4757598d":"code","502ddb1a":"code","475dfc5f":"code","56c63fbd":"code","29822a43":"code","b68d05fb":"code","74e9ee7d":"code","21dfe19a":"code","06a7903b":"code","3eebbc90":"code","cb9f5f7e":"code","6e8ea4b2":"code","5d9c1834":"code","a8539c12":"code","3c124997":"code","f149faeb":"code","fcdfc5c2":"code","f4dc3886":"code","24c148d4":"code","edb3cfbc":"code","7bc59bea":"code","ab75f91d":"code","464f668b":"code","7a9c9712":"code","af1fa36a":"code","688199c0":"code","60936c25":"code","3e1a1ce5":"code","66d51c95":"code","f0837e24":"markdown","8396a011":"markdown","4ff5afb3":"markdown","e3712327":"markdown","f93f7337":"markdown","3d3cf125":"markdown","ef6e8e27":"markdown","06c4a12d":"markdown","31fc11ac":"markdown","075e5878":"markdown","91e5fb19":"markdown","d2664f71":"markdown","92cf6327":"markdown","343fa3ea":"markdown","5539c41a":"markdown","0161f782":"markdown","877e6480":"markdown","2cac3f53":"markdown","0191f215":"markdown","50deba1b":"markdown","29678e34":"markdown","0378075a":"markdown","dda32960":"markdown","714719de":"markdown","36dd5421":"markdown","e3665579":"markdown","ee7243f7":"markdown","7ea6b7f2":"markdown","6c74eb37":"markdown","beb8b240":"markdown","f2e9eebf":"markdown","0684bcd4":"markdown","1f5e193c":"markdown","7be868eb":"markdown","6071a376":"markdown","8cc32dab":"markdown","e31b722b":"markdown","0ca16ecd":"markdown","f343d8b8":"markdown","23a64d3d":"markdown","da5d33ea":"markdown","3113d253":"markdown","c225de30":"markdown","eda92a0a":"markdown","be7fe8da":"markdown","5d1fb96d":"markdown","3bae9ad2":"markdown","b2d408c5":"markdown","0a152399":"markdown","697f3c99":"markdown","3dc8c66e":"markdown","dd4541ab":"markdown","81872ad1":"markdown","15931902":"markdown","050aa8b9":"markdown","6014ecca":"markdown","f038ade5":"markdown","73696600":"markdown","19ad924d":"markdown","843a9ed0":"markdown","7b955788":"markdown","12bb8056":"markdown","81114a2f":"markdown","77f5c9a4":"markdown","8cd714c6":"markdown","20790aa6":"markdown","b7a552f6":"markdown","4ba4e943":"markdown","9775c545":"markdown","db153f68":"markdown","61f773c8":"markdown","552e930d":"markdown","775d5362":"markdown","3c549ba1":"markdown","c1f1fd99":"markdown","5c533f24":"markdown"},"source":{"f6de182c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","5ac49086":"# Loading the training raw data.\ndata = pd.read_csv('..\/input\/titanic-machine-learning-from-disaster\/train.csv')","bebd516c":"data.head()","1611d83c":"data.info()\nprint(f\"Shape of the dataset is {data.shape}\")\nprint(data.columns)","5e7c7a18":"# Checking overall summary of souce data.\ndata.describe()","87d1a69f":"# Checking for missing values.\nimport missingno as msno  # This would be used to visualize missing values in the data.\n\n# Visualizing missing values as a matrix.\nmsno.matrix(data)\n\n","fe2181a9":"# Visualizing missing values in bar plot\nmsno.bar(data)","36952d6a":"# Visualizing missing values as heatmaps.\nmsno.heatmap(data)","6ce9202a":"# checking for total missing values.\ndata.isnull().sum()\n","148e65fe":"f, ax = plt.subplots(1, 2, figsize = (20, 10))\ndata['Survived'].value_counts().plot.pie(explode = [0, 0.1], autopct = '%1.1f%%', ax = ax[0], shadow = True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\n\n\nsns.countplot('Survived', data = data, ax = ax[1])\nax[1].set_title('Survived')\nplt.show()","5d0caf84":"data.groupby(['Sex', 'Survived'])['Survived'].count()","335c0455":"# Plotting above resluts.\nf, ax = plt.subplots(1, 2, figsize = (20, 10))\ndata[['Sex', 'Survived']].groupby(['Sex']).mean().plot.bar(ax = ax[0])\nax[0].set_title('Survived vs Sex')\n\nsns.countplot('Sex', hue = 'Survived', data = data, ax = ax[1])\nax[1].set_title('Survived vs Dead')\n\nplt.show()","2a8ac71d":"pd.crosstab(data['Pclass'], data['Survived'], margins = True).style.background_gradient(cmap = 'summer_r')","b93785af":"f, ax = plt.subplots(1, 2, figsize = (20, 10))\ndata['Pclass'].value_counts().plot.bar(color = ['#CD7F32','#FFDF00', '#D3D3D3'], ax = ax[0])\nax[0].set_title('Number of passengers by Pclass')\nax[0].set_ylabel('Count')\n\n\nsns.countplot('Pclass', hue = 'Survived', data = data, ax = ax[1])\nax[1].set_title('Pclass: Survived vs Dead')\n\nplt.show()","ed792e7c":"print(f\"Oldest passenger is of age: {data['Age'].max()} years.\")\nprint(f\"Youngest passenger is of age: {data['Age'].min()} years.\")\nprint(f\"Average age on the ship is: {data['Age'].mean()} years.\")","8bf4461b":"data['Initials'] = 0\nfor i in data:\n    data['Initials'] = data['Name'].str.extract('([A-Za-z]+)\\.')  # Extracting the salutations.\n    \n# a-Za-z + . => it looks for the string between A to z followed by dot(.) . Inshort it checks for salutations present or not.","962df2d6":"data['Initials']","6a26fc87":"# Now checking of different salutations according to sex of a passenger.\npd.crosstab(data['Initials'], data['Sex']).T.style.background_gradient(cmap = 'summer_r')","c909342b":"data['Initials'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],\n                         ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'], inplace = True)","e246dbea":"# Now we will be requiring mean aage of each category for the imputation.\ndata.groupby('Initials')['Age'].mean()\n","8fa2b9c9":"data.loc[(data['Age'].isnull()) & (data['Initials'] == 'Mr'), 'Age'] = 33 # Assigning Ceil of age only.\ndata.loc[(data['Age'].isnull()) & (data['Initials'] == 'Mrs'), 'Age'] = 36 # Assigning Ceil of age only.\ndata.loc[(data['Age'].isnull()) & (data['Initials'] == 'Master'), 'Age'] = 5 # Assigning Ceil of age only.\ndata.loc[(data['Age'].isnull()) & (data['Initials'] == 'Miss'), 'Age'] = 22 # Assigning Ceil of age only.\ndata.loc[(data['Age'].isnull()) & (data['Initials'] == 'Other'), 'Age'] = 46 # Assigning Ceil of age only.","a44d4aab":"# Again checking for any null value in 'Age' column.\ndata['Age'].isnull().any()","f40b2cc2":"# Now plotting 'survived' according to age.\n# 0 => Not survived\n# 1 => survived\nf, ax = plt.subplots(1, 2, figsize = (20, 10))\ndata[data['Survived'] == 0]['Age'].plot.hist(ax = ax[0], bins = 20, edgecolor = 'black', color = 'red')\nax[0].set_title('Passengers Not Survived (0)')\nx1 = list(range(0, 85, 5))  # x-coordinates.\nax[0].set_xticks(x1)\n\ndata[data['Survived'] == 1]['Age'].plot.hist(ax = ax[1], bins = 20, edgecolor = 'black', color = 'green')\nax[1].set_title('Passengers Survived (1)')\nx2 = list(range(0, 85, 5))  # x-coordinates.\nax[1].set_xticks(x2)\n\nplt.show()","577fd021":"pd.crosstab([data['Embarked'], data['Pclass']], [data['Sex'], data['Survived']], margins = True).style.background_gradient(cmap = 'summer_r')","df266006":"sns.factorplot('Embarked', 'Survived', data = data)\nfig = plt.gcf()  # gcf => get current figure\nfig.set_size_inches(10, 6)\nplt.show()","b5ae9d4a":"f, ax = plt.subplots(2, 2, figsize = (30, 20))\n\nsns.countplot('Embarked', data = data, ax = ax[0, 0])\nax[0, 0].set_title('No. of passengers boarded the Titanic')\n\nsns.countplot('Embarked', hue = 'Sex', data = data, ax = ax[0, 1])\nax[0, 1].set_title('Male-Female Vs Embarked')\n\nsns.countplot('Embarked', hue = 'Survived', data = data, ax = ax[1, 0])\nax[1, 0].set_title('Embarked Vs Survived')\n\nsns.countplot('Embarked', hue = 'Pclass', data = data, ax = ax[1, 1])\nax[1, 1].set_title('Embarked Vs Pclass')","10aa9872":"data['Embarked'].fillna('S', inplace = True)","b953c549":"data['Embarked'].isnull().any()","31218491":"pd.crosstab(data['SibSp'], data['Survived']).style.background_gradient(cmap = 'summer_r')","1df6628f":"f, ax = plt.subplots(1, 2, figsize = (20, 8))\nsns.barplot('SibSp', 'Survived', data = data, ax = ax[0])\nax[0].set_title('SibSp Vs survived')\n\nsns.factorplot('SibSp', 'Survived', data = data, ax = ax[1])\nax[1].set_title('SibSp Vs survived')\n\n# plt.close(2)\nplt.show()","f02eaa23":"pd.crosstab(data['SibSp'],data['Pclass']).style.background_gradient(cmap='summer_r')","e148e984":"print(f\"Highest Fare was: {data['Fare'].max()}\")\nprint(f\"Lowest Fare was: {data['Fare'].min()}\")\nprint(f\"Average Fare was: {data['Fare'].mean()}\")","15976e49":"f, ax = plt.subplots(1, 3, figsize = (20, 10))\n\nsns.distplot(data[data['Pclass'] == 1]['Fare'], ax = ax[0])\nax[0].set_title('Pclass 1 Fares')\n\nsns.distplot(data[data['Pclass'] == 2]['Fare'], ax = ax[1])\nax[1].set_title('Pclass 2 Fares')\n\nsns.distplot(data[data['Pclass'] == 3]['Fare'], ax = ax[2])\nax[2].set_title('Pclass 3 Fares')\n","86ce5c62":"sns.heatmap(data.corr(), annot = True, cmap = 'RdYlGn', linewidth = 0.2)\n# data.corr() => it gives us the correlation matrix.\n\nfig = plt.gcf()\nfig.set_size_inches(10, 10)\nplt.show()","1d65a700":"data['Age_band'] = 0\n\ndata.loc[data['Age'] <= 16, 'Age_band'] = 0\ndata.loc[(data['Age'] > 16) & (data['Age'] <= 32), 'Age_band'] = 1\ndata.loc[(data['Age'] > 32) & (data['Age'] <= 48), 'Age_band'] = 2\ndata.loc[(data['Age'] > 48) & (data['Age'] <= 64), 'Age_band'] = 3\ndata.loc[data['Age'] > 64, 'Age_band'] = 4\n\n","46db0703":"data.head()","53bb1d44":"# To check the number of passengers in each band created.\ndata['Age_band'].value_counts().to_frame().style.background_gradient(cmap = 'summer_r')","4c443337":"data['Family_size'] = 0\ndata['Family_size'] = data['SibSp'] + data['Parch']\n\ndata['Alone'] = 0\ndata.loc[data['Family_size'] == 0, 'Alone'] = 1","74325b8f":"data['Fare_range'] = pd.qcut(data['Fare'], 4)\ndata.groupby(['Fare_range'])['Survived'].mean().to_frame().style.background_gradient(cmap = 'summer_r')","cf69cfa0":"data['Fare_cat'] = 0\ndata.loc[data['Fare'] <= 7.91, 'Fare_cat'] = 0\ndata.loc[(data['Fare'] > 7.91) & (data['Fare'] <= 14.454), 'Fare_cat'] = 1\ndata.loc[(data['Fare'] > 14.454) & (data['Fare'] <= 31.0), 'Fare_cat'] = 2\ndata.loc[data['Fare'] > 31.0, 'Fare_cat'] = 3","4757598d":"data.head()","502ddb1a":"data['Sex'].replace(['male', 'female'], [0, 1], inplace = True)\ndata['Embarked'].replace(['S', 'C', 'Q'], [0, 1, 2], inplace = True)\ndata['Initials'].replace(['Mr', 'Mrs', 'Miss', 'Master', 'Other'], [0, 1, 2, 3, 4], inplace = True)","475dfc5f":"data.head()","56c63fbd":"data.drop(['Name', 'Age', 'Ticket', 'Fare', 'Cabin', 'Fare_range', 'PassengerId'], axis = 1, inplace = True)","29822a43":"data.head()","b68d05fb":"data.isnull().sum()","74e9ee7d":"# Importing all the required libraries.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix","21dfe19a":"train, test = train_test_split(data, test_size = 0.2, random_state = 0, stratify = data['Survived'])\n# It is desirable to split the dataset into train and test sets in a way that preserves the same proportions of examples in each class as observed in the original dataset. \n# This is called a stratified train-test split.\n\ntrain_X = train[train.columns[1:]]\ntrain_Y = train[train.columns[:1]]\ntest_X = test[test.columns[1:]]\ntest_Y = test[test.columns[:1]]\n\nX = data[data.columns[1:]]\ny = data['Survived']","06a7903b":"model = svm.SVC(kernel = 'linear', C = 0.1, gamma = 0.1)\nmodel.fit(train_X, train_Y)\nprediction_svm_linear = model.predict(test_X)\n\n# Printing the accuracy\nprint(f\"Accuracy for Linear SVM is {metrics.accuracy_score(prediction_svm_linear, test_Y)}\")","3eebbc90":"model = svm.SVC(kernel = 'rbf', C = 0.1, gamma = 0.1)\nmodel.fit(train_X, train_Y)\nprediction_svm_radial = model.predict(test_X)\n\n# Printing the accuracy\nprint(f\"Accuracy for Radial SVM is {metrics.accuracy_score(prediction_svm_radial, test_Y)}\")","cb9f5f7e":"model = LogisticRegression()\nmodel.fit(train_X, train_Y)\nprediction_LR = model.predict(test_X)\n\n# Printing the Accuracy\nprint(f\"Accuracy for Logistic Regression classification is {metrics.accuracy_score(prediction_LR, test_Y)}\")","6e8ea4b2":"model = DecisionTreeClassifier()\nmodel.fit(train_X, train_Y)\nprediction_DT = model.predict(test_X)\n\n# Printing the Accuracy\nprint(f\"Accuracy for Decision Trees classification is {metrics.accuracy_score(prediction_DT, test_Y)}\")","5d9c1834":"model = KNeighborsClassifier()\nmodel.fit(train_X, train_Y)\nprediction_KNN = model.predict(test_X)\n\n# Printing the Accuracy\nprint(f\"Accuracy for KNNs classification is {metrics.accuracy_score(prediction_KNN, test_Y)}\")","a8539c12":"model = GaussianNB()\nmodel.fit(train_X, train_Y)\nprediction_NB = model.predict(test_X)\n\n# Printing the Accuracy\nprint(f\"Accuracy for Gaussian Naive Bayes classification is {metrics.accuracy_score(prediction_NB, test_Y)}\")","3c124997":"model = RandomForestClassifier(n_estimators = 200)\nmodel.fit(train_X, train_Y)\nprediction_RF = model.predict(test_X)\n\n# Printing the Accuracy\nprint(f\"Accuracy for Random Forests classification is {metrics.accuracy_score(prediction_RF, test_Y)}\")","f149faeb":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nkfold = KFold(n_splits = 10, random_state = 22)\nxyz = []  # To store the mean of each classifier.\naccuracy = []\nstd = [] # To store standard deviation\n\nclassifiers = ['Linear SVM', 'Radial SVM', 'Logistic Regression',\n               'KNN', 'Decision Trees', 'Naive Bayes', 'Random Forests']\nmodels = [svm.SVC(kernel = 'linear'), svm.SVC(kernel = 'rbf'),\n          LogisticRegression(), KNeighborsClassifier(n_neighbors = 9), DecisionTreeClassifier(),\n         GaussianNB(), RandomForestClassifier(n_estimators = 100)]\n\nfor i in models:\n    model = i\n    cv_result = cross_val_score(model, X, y, cv = kfold, scoring = 'accuracy')\n    \n    cv_result = cv_result\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\n    \nnew_models_df = pd.DataFrame({'CV Mean': xyz, 'Std': std}, index = classifiers)\n\nnew_models_df","fcdfc5c2":"# PLotting above dataframe as boxplots to get some clear insights.\nplt.subplots(figsize = (12, 6))\nbox_plot = pd.DataFrame(accuracy, index = [classifiers])  # [classfiers] was created in above cell.\nbox_plot.T.boxplot()","f4dc3886":"f, ax = plt.subplots(3, 3, figsize = (20, 18))\n\ny_pred =  cross_val_predict(svm.SVC(kernel = 'rbf'), X, y, cv = 10)\nsns.heatmap(confusion_matrix(y, y_pred), ax = ax[0, 0], annot = True, fmt = '2.0f')  # fmt => adding text to each cell\nax[0, 0].set_title('Matrix for SVM-Radial(rbf)')\n\ny_pred =  cross_val_predict(svm.SVC(kernel = 'linear'), X, y, cv = 10)\nsns.heatmap(confusion_matrix(y, y_pred), ax = ax[0, 1], annot = True, fmt = '2.0f') \nax[0, 1].set_title('Matrix for SVM-Linear')\n\ny_pred =  cross_val_predict(KNeighborsClassifier(n_neighbors = 9), X, y, cv = 10)\nsns.heatmap(confusion_matrix(y, y_pred), ax = ax[0, 2], annot = True, fmt = '2.0f') \nax[0, 2].set_title('Matrix KNN Classifier')\n\ny_pred =  cross_val_predict(RandomForestClassifier(n_estimators = 100), X, y, cv = 10)\nsns.heatmap(confusion_matrix(y, y_pred), ax = ax[1, 0], annot = True, fmt = '2.0f') \nax[1, 0].set_title('Matrix for Random forest Classifier')\n\ny_pred =  cross_val_predict(LogisticRegression(), X, y, cv = 10)\nsns.heatmap(confusion_matrix(y, y_pred), ax = ax[1, 1], annot = True, fmt = '2.0f') \nax[1, 1].set_title('Matrix for Logistic Regression Classifier')\n\ny_pred =  cross_val_predict(DecisionTreeClassifier(), X, y, cv = 10)\nsns.heatmap(confusion_matrix(y, y_pred), ax = ax[1, 2], annot = True, fmt = '2.0f') \nax[1, 2].set_title('Matrix for Decision Trees Classifier')\n\ny_pred =  cross_val_predict(GaussianNB(), X, y, cv = 10)\nsns.heatmap(confusion_matrix(y, y_pred), ax = ax[2, 0], annot = True, fmt = '2.0f') \nax[2, 0].set_title('Matrix for Naive Bayes Classifier')\n\n","24c148d4":"from sklearn.model_selection import GridSearchCV\n\nC = [0.05, 0.1, 0.2, 0.3, 0.25, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\ngamma = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\nkernel = ['rbf', 'linear']\n\nhyper = {'kernel': kernel, 'C':C, 'gamma': gamma}\n\ngd = GridSearchCV(estimator = svm.SVC(), param_grid = hyper, verbose = True)\ngd.fit(X, y)\n\nprint(gd.best_score_)\nprint(gd.best_estimator_)","edb3cfbc":"n_estimators = range(100, 1000, 100)\nhyper = {'n_estimators': n_estimators}\ngd = GridSearchCV(estimator = RandomForestClassifier(random_state = 0),\n                 param_grid = hyper, verbose = True)\ngd.fit(X, y)\n\nprint(gd.best_score_)\nprint(gd.best_estimator_)","7bc59bea":"from sklearn.ensemble import VotingClassifier\n\nensemble_model = VotingClassifier(estimators = [('KNN', KNeighborsClassifier(n_neighbors = 10)),\n                                               ('RBF', svm.SVC(probability = True, kernel = 'rbf', C = 0.5, gamma = 0.1)),\n                                               ('RFor', RandomForestClassifier(n_estimators = 500, random_state = 0)),\n                                               ('LR', LogisticRegression(C = 0.05)),\n                                               ('DT', DecisionTreeClassifier(random_state = 0)),\n                                               ('NB', GaussianNB()),\n                                               ('svm', svm.SVC(kernel = 'linear', probability = True))],\n                                 voting = 'soft').fit(train_X, train_Y)\n\nprint(f\"The accuracy for ensembled model is: {ensemble_model.score(test_X, test_Y)}\")\ncross = cross_val_score(ensemble_model, X, y, cv = 10, scoring = 'accuracy')\nprint(f\"The cross validated score is {cross.mean()}\")","ab75f91d":"from sklearn.ensemble import BaggingClassifier\n\nmodel = BaggingClassifier(base_estimator = KNeighborsClassifier(n_neighbors = 3),\n                          random_state = 0,\n                          n_estimators = 700)\n\nmodel.fit(train_X, train_Y)\nprediction = model.predict(test_X)\n\nprint(f\"The accuracy for bagged KNN is: {metrics.accuracy_score(prediction, test_Y)}\")\n\nresult = cross_val_score(model, X, y, cv = 10, scoring = 'accuracy')\nprint(f\"The cross validation score for bagged KNN is: {result.mean()}\")","464f668b":"model = BaggingClassifier(base_estimator = DecisionTreeClassifier(),\n                         random_state = 0,\n                         n_estimators = 200)\nmodel.fit(train_X, train_Y)\nprediction = model.predict(test_X)\n\nprint(f\"The accuracy for bagged Decision Tree is: {metrics.accuracy_score(prediction, test_Y)}\")\n\nresult = cross_val_score(model, X, y, cv = 10, scoring = 'accuracy')\nprint(f\"The cross validation score for bagged Decision Tree is: {result.mean()}\")","7a9c9712":"from sklearn.ensemble import AdaBoostClassifier\nada_boost = AdaBoostClassifier(n_estimators = 200, random_state = 0, learning_rate = 0.1)\n\nresult = cross_val_score(ada_boost, X, y, cv = 10, scoring = 'accuracy')\nprint(f\"The cross validated score for AdaBoost classifier is: {result.mean()}\")","af1fa36a":"from sklearn.ensemble import GradientBoostingClassifier\nmodel = GradientBoostingClassifier(n_estimators = 500,\n                                  random_state = 0,\n                                  learning_rate = 0.1)\n\nresult = cross_val_score(model, X, y, cv = 10, scoring = 'accuracy')\n\nprint(f\"The cross-validated score for Gradient Boosting is: {result.mean()}\")","688199c0":"import xgboost as xg\nmodel = xg.XGBClassifier(n_estimators = 900, learning_rate = 0.1)\n\nresult = cross_val_score(model, X, y, cv = 10, scoring = 'accuracy')\n\nprint(f\"The cross-validated score for XGBoost is: {result.mean()}\")","60936c25":"n_estimators = list(range(100, 1200, 100))\nlearn_rate = [0.05, 0.1, 0.2, 0.3, 0.25, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n\nhyper = {'n_estimators': n_estimators,\n        'learning_rate': learn_rate}\n\ngd = GridSearchCV(estimator = AdaBoostClassifier(),\n                 param_grid = hyper,\n                 verbose = True)\n\ngd.fit(X, y)\n\nprint(gd.best_score_)\nprint(gd.best_estimator_)","3e1a1ce5":"ada = AdaBoostClassifier(n_estimators = 100,\n                         random_state = 0,\n                         learning_rate = 0.1)\n\nresult = cross_val_predict(ada, X, y, cv = 10)\n\n# Creating confusion matrix now.\nsns.heatmap(confusion_matrix(y, result),\n            cmap = 'summer',\n            annot = True,\n            fmt = '2.0f')\n\nplt.show()","66d51c95":"f, ax = plt.subplots(2, 2, figsize = (20, 15))\n\n# Random Forest\nmodel = RandomForestClassifier(n_estimators = 500, random_state = 0)\nmodel.fit(X, y)\n\n## pd.Series() => 1-Dimensional array which is capable of holding any type of data.\npd.Series(model.feature_importances_, X.columns).sort_values(ascending = True).plot.barh(width = 0.7, ax = ax[0, 0])\nax[0, 0].set_title('Feature importance in Random Forests')\n\n\n\n# AdaBoost\nmodel = AdaBoostClassifier(n_estimators = 100, random_state = 0, learning_rate = 0.1)\nmodel.fit(X, y)\n\npd.Series(model.feature_importances_, X.columns).sort_values(ascending = True).plot.barh(width = 0.7, ax = ax[0, 1])\nax[0, 1].set_title('Feature importance in AdaBoost')\n\n\n\n# Stochastic Gradient Boosting\nmodel = GradientBoostingClassifier(n_estimators = 500, random_state = 0, learning_rate = 0.1)\nmodel.fit(X, y)\n\npd.Series(model.feature_importances_, X.columns).sort_values(ascending = True).plot.barh(width = 0.7, ax = ax[1, 0])\nax[1, 0].set_title('Feature importance in Gradient Boosting')\n\n\n\n# XGBoost\nmodel = xg.XGBClassifier(n_estimators = 900, random_state = 0, learning_rate = 0.1)\nmodel.fit(X, y)\n\npd.Series(model.feature_importances_, X.columns).sort_values(ascending = True).plot.barh(width = 0.7, ax = ax[1, 1])\nax[1, 1].set_title('Feature importance in XGBoost')","f0837e24":"### For SVM","8396a011":"# Part 4. Ensembling\nIt is a combination of various simple models to create a single powerful model.\n\nWays of doing ensembling are:-\n1. Voting classifier\n2. Bagging \n3. Boosting","4ff5afb3":"#### Age => Continuous Feature","e3712327":"#### AdaBoost (Adaptive Boosting)\n","f93f7337":"##### With this we came to end of this notebook. I hope you got enough basic learnings of different Classification Algorithms and how to implement them. But always remember that **Clean Data always beats Fancy Algorithms**, so try to spend most of the time in data pre-processing and feature engineering.\n\n#### If you found this notebook helpful, kindly **UPVOTE**\n","3d3cf125":"### Feature *Fare*  => Continuous Feature","ef6e8e27":"***From above matrices we can say that...***\n1. **NaiveBayes** has a higher chance in correctly predicting passengers **who survived**.\n2. **SVM-Radial** has a higher chance in correctly predicting passengers **who died**.","06c4a12d":"### Creating *Family_size* and *Alone* feature.\nWe will just integrate the *SibSp* and *Parch* feature.","31fc11ac":"**INSIGHTS from the above cross tab**\n- There are 3 ports, namely C, Q and S.\n- 1st class passengers mainly used port C and port S.\n- 3rd class passengers mainly used port S.\n- 2nd classs passesngers mainly used port S.\n\nNow let us check, do chances of survival depends upon Port??","075e5878":"Clearly as fare range increases, **survival also increases**. \n\nBut we need a specific category for each fare_range.\nSo creating ***Fare_cat*** feature\n","91e5fb19":"### For Random Forest Classifier","d2664f71":"The number of men on the ship is a lot more than the number of women. Still the number of women saved is almost twice the number of males saved.\n\nSurvival of female is around 75% and that of male is 18%","92cf6327":"#### Plotting how many Survived?\n","343fa3ea":"Now we have **clean data** with **correct features** and **without any missing value**, which would be required to create the machine learning model.","5539c41a":"### Dealing with Age (Creating *Age_band* feature]\nAs we have *Age*, a contnuous variable. It can be problematic of using continuos feature in a machine learning model. We should use categorical feature. So, we need to convert *Age* feature into categorical feature. \n\nWe can use Normalization or binning to perform this task.\nWe will use **binning**.\n- We will categorize *Age* into 5 categories of 16 width each. \n- Highest age is 80, so 80\/5 = 16","0161f782":"**Lowest Fare was zero(0).....Wow!! free death ride in luxorious Titanic (:P**\n\nNow plotting some distribution plots.\n\n","877e6480":"- As highest positive correlation is 0.41, so we do not have any highly correlated features. Hence no redundant features. We have to continue with all the features.\n- If we had any 2 highly correlated features, we could have dropped one of them.\n","2cac3f53":"**Useful Insights**\n- The graph of survival decreases with increase in number of siblings.\n- Families with siblings 5-8 are all dead. They also belonged to Pclass 3 which had very low survival rate(can be seen from crosstab plot).","0191f215":"#### The main objective of this notebook is to give an idea how is the workflow in any predictive modeling problem. \n#### How do we check features, how do we add new features and some Machine Learning Concepts? \n\n\n#### This notebook contains:-\n1. EDA on Titanic Dataset\n2. Feature Engineering and Data Cleaning\n3. Predictive Modelling\n    1. Basic Classification Algorithms\n        1. Logistic Regression\n            - Radial\n            - Linear\n        2. Support Vector Machines\n        3. Random Forests\n        4. K-Nearest Neighbours\n        5. Naive Bayes\n        6. Decision Trees\n    \n    2. Cross Validation\n    3. Hyper-parameter Tuning to get best features\n    4. Ensembling\n        1. Voting Classifier\n        2. Bagging\n            - Bagged KNN\n            - Bagged Decision Tree\n        3. Boosting\n            - AdaBoost\n            - Stochastic Gradient Boosting\n            - XgBoost\n    5. Creating Confusion Matrix for the best model\n    6. Feature Importance\n- ***This Notebook contains all the basic classification algorithms with proper explanation and implementation. If you find it helpful, please ----UPVOTE----***","50deba1b":"#### Hyper-parameter Tuning for AdaBoost","29678e34":"**NOTE:-**\nAccuracy alone cannot be a measure for the performance of the classifier. In order to check for robustness of the model we need to do **cross-validation**.\n\n**K-Fold Cross Validation**\n1. First divide the dataset into **k-equal subsets**.\n2. Lets say k = 10, then we do trainig on 9 subsets and testing on 1 subset.\n3. We cahange the testing subset iteratively and check accuracy for each combination. At last, **average** of all the accuracy is taken for performance measure.\n4. There may be underfit for one subset and overfit for some other. So, k-fold cross validation helps achieve a **generalised model**.","0378075a":"- Pclass 1 has large distribution of fares.\n- We can convert these continuous fares in discrete values using binning.","dda32960":"#### Now we need to perform imputation for missing values.","714719de":"**Insights from above histogram**\n1. Children below age 5 were saved in large number. Previously we had also seen that women were also saved in large numbers. Hence, women and children were on priority list of rescue.\n2. Also, old passengers (age > 75) were saved.\n3. Maximum deaths occur for age group of 30-35.","36dd5421":"#### Stochastic Gradient Boosting","e3665579":"**Creating CONFUSION MATRIX**\n- It gives us the number of correct and incorrect classifications made by the classifier.","ee7243f7":"**So no null values left in *Age* column finally.**","7ea6b7f2":"### 1 b. Radial Support Vector Machines(rbf-SVM)","6c74eb37":"#### Analyzing *Sex* Feature","beb8b240":"### 1. Voting Classifier\n- It gives us an average prediction result based in the prediction of all submodels.\n- The submodels\/basemodels are of different types.","f2e9eebf":"Now visualizing the missing number in the form of heatmap.\n- Heatmap shows the correlation of missingness between every 2 columns.\n- A value near **-1** means if one value appears, another value is likely to be missing.\n- A value near **0** means there is **no dependance** between the occcurence of missing values of two variables.\n- A value near **1** means, if one variable appears another is likely to be present.","0684bcd4":"### 2. Bagging \n- Similar classifiers are applied on small partitions of the dataset. \n- Then average of all predictions is taken.\n- There is reduction in variance as we are taking average of similar classifiers.","1f5e193c":"- We have 891 rows and 12 columns in our source dataset.\n- Coulmns => 'PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp','Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'","7be868eb":"### 1 a. Linear Support Vector Machines(Linear-SVM)","6071a376":"### 6. Random Forests","8cc32dab":"### Correaltion between the features.\nCorrelation means how one variable is changing with respect to another.\nIt lies between -1 and 1. \n","e31b722b":"# PART 1. Exploratory Data Analysis\n1. Analysis of the features.\n2. Finding relations or trends among features.","0ca16ecd":"#### Analyzing *PClass* (Ordinal Feature)","f343d8b8":"### SibSp feature (Discrete Feature)\nSibSip => represents whether a passenger is with family or not.","23a64d3d":"### Handling *Embarked* Feature (Categorcal Feature)\nAlso remember that it also contains missing values which needs to be handled.","da5d33ea":"**The best Score for SVM is 82.82% with optimal parameters:-**\n- **C = 0.4**\n- **gamma = 0.3**","3113d253":"**Some more insights about *Embarked* Feature**\n1. Port S has maximum boarding of passengers, followed by Port C and Port Q.\n2. Majority of passengers from Port S belongs to Class 3.\n3. The Embark S looks to the port from where majority of the rich people boarded. Still the chances for survival is low here, that is because many passengers from Pclass3 around 81% didn't survive.\n4. Port Q has around 95% passengers of Pclass 3.","c225de30":"**INSIGHTS:-**\n- Some of the common important features are *Initials*, *Pclass*, *Family_size* and *Fare_cat*.\n- Feature *Sex* is only important for Random Forests.\n","eda92a0a":"## Confusion Matrix for the Best Model (AdaBoost)","be7fe8da":"Hence, no null values in 'Embarked' feature now.","5d1fb96d":"**The chances for survival at *Port C* is highest i.e. around 55%**\n\nNow plotting sum more plots for more clear insights about *Embarked* feature.\n1. Count of Embarked for all 3 ports.\n2. Male-female presence for embarked.\n3. Embarked vs Survived\n4. Embarked vs Pclass.","3bae9ad2":"# Part 3. Creating the ML Models.\nWe will be implementing different ML Classification algorithms:\n1. Logistic Regression\n2. Support Vector Machines (Linear as well as radial)\n3. Random Forest\n4. K-Nearest Neighbours\n5. Naive Bayes\n6. Decision Trees\n7. Logistic Regression","b2d408c5":"**Results for AdaBoost (Hyper-tuned)**\n- Accuracy = 82.93%\n- n_estimators = 100\n- learning_rate = 0.1","0a152399":"### 2. Logistic Regression","697f3c99":"# Part 2: Feature Engineering","3dc8c66e":"- Salutations for females => Countess, Lady, Miss, Mlle, Mme, Mrs, Ms\n- Salutations for males => Capt, Col, Don, Dr, Jonkheer, Major, Master, Mr, Rev, Sir\n\nNow we will convert all salutations into 5 categories\n1. Master\n2. Miss\n3. Mr\n4. Mrs\n5. Other","dd4541ab":"### 3. Boosting\n- In this we use **sequential learning** of classifiers.\n- Weak model is enhanced step wise.\n- Steps:- \n    1. A model is first trained on the complete dataset.\n    2. Model would get some instances right and some wrong.\n    3. For next iteration, more weightage would be given to wrongly predicted instances, and hence improving it.\n    4. This iterative process is continued and new classifiers are added to the model until we reach the significant accuracy.","81872ad1":"# PART 1. Exploratory Data Analysis\n1. Analysis of the features.\n2. Finding relations or trends among features.","15931902":"## Feature Importance\nTo check which features are the most essential in predicting the survival of passenger, we will plot feature importance for:-\n- *Random Forest* \n- *AdaBoost* \n- *Gradient Boosting* \n- *XgBoost*.","050aa8b9":"#### Bagged Decision Tree\n","6014ecca":"### 4. K-Nearest Neighbours (KNNs)","f038ade5":"## Hyper-parameter Tuning for SVM and Random Forest Classifier\n\n**NOTE about GridSearchCV:-**\nGridSearchCV is a library function that is a member of sklearn's model_selection package. It helps to loop through predefined hyperparameters and fit our estimator (model) on our training set. So, in the end, we can select the best parameters from the listed hyperparameters.","73696600":"**Note:-**\nAccuracy of KNNs classification can be improved if we can change the values of n_nighbours. For now, we had use default value of n_neighbours i.e. 5.","19ad924d":"Clearly there are 3 columns which contain missing values i.e. 'Age', 'Cabin' and 'Embarked'\n- **Age** => we have 714 values only out of 891, so remaining(177) can be imputed.\n- **Cabin** => we just have 204 values out of 891, so it is better to drop this column becuase filling large number of missing values may cause data leakage.\n- **Embarked** => Just 2 values are missing, we can impute this with mean or mode.\n\n***We will handle these missing values later on...***","843a9ed0":"**It is clear that not many passengers survived in the accident.**\n- Out of **891** passengers, only around **350**(38.4%) survived the accident.\n- We need ti check the features to better understand about whuch categories of passengers survived.","7b955788":"#### Now filling missing values of *Embarked* column.\nAs maximum number of passengers boarded from Port S, we replace NaN values with S.\n\n","12bb8056":"### Data Profiling\n**Data profiling is the process of reviewing source data, understanding structure, content and inter-relationships, and identifying potential for data projects.**\n- Getting data info.\n- Checking for missing values.","81114a2f":"**Most** of the passengers were alone or had just 1 family member.\n\n","77f5c9a4":"**As of now, out of AdaBoost, Stochastic Gradient boosting and XGBoost, we got the highest accuracy for AdaBoost. So now performing hyper-parameter tuning for Adaboost to get optimal results.**\n- AdaBoost Accuracy = 82.49%\n- Stochastic Gradient Boosting Accuracy = 81.15%\n- XGBoost Accuracy = 81.60%","8cd714c6":"**Now dropping unwanted features i.e. *Name*, *Age*, *Ticket*, *Fare*, *Cabin*, *Fare_range*, *PassengerId***\n1. Name => This cannot be converted into categorical feature, so we need to drop it.\n2. Age => We already created *Age_band*.\n3. Ticket => It cannot be categorised.\n4. Fare => *Fare_cat* is already created.\n5. Cabin => It has lots of missing values and many passengers have multiple cabins. So this feature is useless.\n6. Fare_range => *Fare_cat* already created.\n7. PassengerId => It cannot be categorised.","20790aa6":"### Creating *Fare_range* feature\nAs Fare was also a continuous feature, we need to convert it to categorical feature.\n\n**We will be using pandas qcut method. It will split fares equally among number of bins passed.**\n","b7a552f6":"#### XGBoost","4ba4e943":"**The best Score for Random Forests is 81.93% with optimal parameters:-**\n- **n_estimators = 300**\n","9775c545":"#### Bagged KNN","db153f68":"Previously we have also seen that *Age* column had 127 missing values. We can impute the missing values by mean of ages.\n**But we caanot mean as it is not beneficial to assign age 29 to a 5 year boy or to 70 years old person. This would decrease the accuracy of the model.**\n\nSo we need to find specific categories of people and then impute mean of those specific categories into missing values.\n\nFor example:- We will search for Mrs. before a name, this means she is a lady and then we can impute the mean age of ladies to the missing age of any lady.","61f773c8":"**Preparing training and testing data**\n- Training data = 80%\n- Testinf data = 20%","552e930d":"## Features in our training dataset.\n1. Categorical\/Nominal Features => Sex, Embarked\n2. Ordinal Features => PClass\n3. Continuous Features => Age","775d5362":"- Even the number of passengers in class 3 is a lot higher than the number of passengers in class 1, still rescue of passengers of class 1 is more than that of class 3.\n- There are a lot of deaths from class 3.\n","3c549ba1":"### 3. Decision Trees","c1f1fd99":"### Now dealing with string feature like *Sex*, *Embarked*, *Initials* etc\nWe need to convert these into numerical features.","5c533f24":"### 5. Gaussian Naive-Bayes"}}