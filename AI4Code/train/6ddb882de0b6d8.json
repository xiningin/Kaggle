{"cell_type":{"8956d3f2":"code","ad0187b9":"code","7a62cfb6":"code","fcaa60ad":"code","575c783d":"code","5294a52f":"code","5fa66583":"code","25b1caa2":"code","90d1cf27":"code","851803b9":"code","f2aff3b2":"code","b2b0cdae":"code","2a3b8546":"code","7eaad715":"code","0cd8bd11":"code","c71851f0":"code","313600b0":"code","15b614d2":"code","96fa76f7":"code","f8cb8b0e":"markdown","b8971ade":"markdown","cc08d66d":"markdown","4a16b5dd":"markdown","f90a2059":"markdown","1427c912":"markdown","e339e9f5":"markdown","fe2089c1":"markdown","66d9ca62":"markdown","56232c7b":"markdown","247b20e4":"markdown","20a2a559":"markdown","92fbe00f":"markdown","3f899bfb":"markdown"},"source":{"8956d3f2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\nfrom catboost.datasets import amazon\ntrain, test = amazon()\nprint(train.shape, test.shape)\ntarget = \"ACTION\"\ncol4train = [x for x in train.columns if x not in [target, \"ROLE_TITLE\"]]\ny = train[target].values","ad0187b9":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_validate\n\n# returns model instance\ndef get_model(): \n    params = {\n        \"n_estimators\":300, \n        \"n_jobs\": 3,\n        \"random_state\":5436,\n    }\n    return ExtraTreesClassifier(**params)\n\n# validate model on given dataset and report CV score\ndef validate_model(model, data):\n    skf = StratifiedKFold(n_splits=5, random_state = 4141, shuffle = True)\n    stats = cross_validate(\n        model, data[0], data[1], \n        groups=None, scoring='roc_auc', \n        cv=skf, n_jobs=None, return_train_score = True\n    )\n    stats = pd.DataFrame(stats)\n    return stats.describe().transpose()\n\n# transforms given train and test datasets using provided function, \n# function parameters can be passed as a dict\ndef transform_dataset(train, test, func, func_params = {}):\n    dataset = pd.concat([train, test], ignore_index = True)\n    dataset = func(dataset, **func_params)\n    if isinstance(dataset, pd.DataFrame):\n        new_train = dataset.iloc[:train.shape[0],:].reset_index(drop = True)\n        new_test =  dataset.iloc[train.shape[0]:,:].reset_index(drop = True)\n    else:\n        new_train = dataset[:train.shape[0]]\n        new_test =  dataset[train.shape[0]:]\n    return new_train, new_test","7a62cfb6":"MJTCP = 32292 #Michael Jordan total career points\n#for each column in dataset creates N column with random integers\ndef assign_rnd_integer(dataset, number_of_times = 5, seed = MJTCP):\n    new_dataset = pd.DataFrame()\n    np.random.seed(seed)\n    for c in dataset.columns:\n        for i in range(number_of_times):\n            col_name = c+\"_\"+str(i)\n            unique_vals = dataset[c].unique()\n            labels = np.array(list(range(len(unique_vals))))\n            np.random.shuffle(labels)\n            mapping = pd.DataFrame({c: unique_vals, col_name: labels})\n            new_dataset[col_name] = (dataset[[c]]\n                                     .merge(mapping, on = c, how = 'left')[col_name]\n                                    ).values\n    return new_dataset","fcaa60ad":"new_train, new_test = transform_dataset(\n    train[col4train], test[col4train], \n    assign_rnd_integer, {\"number_of_times\":5}\n)\nprint(new_train.shape, new_test.shape)\nnew_train.head(5)","575c783d":"validate_model(\n    model = get_model(), \n    data = [new_train.values, y]\n)","5294a52f":"new_train, new_test = transform_dataset(\n    train[col4train], test[col4train], \n    assign_rnd_integer, {\"number_of_times\":1}\n)\nprint(new_train.shape, new_test.shape)\nvalidate_model(\n    model = get_model(), \n    data = [new_train.values, y]\n)","5fa66583":"new_train, new_test = transform_dataset(\n    train[col4train], test[col4train], \n    assign_rnd_integer, {\"number_of_times\":10}\n)\nprint(new_train.shape, new_test.shape)\nvalidate_model(\n    model = get_model(), \n    data = [new_train.values, y]\n)","25b1caa2":"from sklearn.preprocessing import OneHotEncoder\n# transforms given dataset to OHE representation\ndef one_hot(dataset):\n    ohe = OneHotEncoder(sparse=True, dtype=np.float32, handle_unknown='ignore')\n    return ohe.fit_transform(dataset.values)","90d1cf27":"new_train, new_test = transform_dataset(\n    train[col4train], test[col4train], \n    one_hot)\nprint(new_train.shape, new_test.shape)","851803b9":"#Warning!!! Long run, better skip it.\nvalidate_model(\n    model = get_model(), \n    data = [new_train, y]\n)","f2aff3b2":"from sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\ndef extract_col_interaction(dataset, col1, col2, tfidf = True):\n    data = dataset.groupby([col1])[col2].agg(lambda x: \" \".join(list([str(y) for y in x])))\n    if tfidf:\n        vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(\" \"))\n    else:\n        vectorizer = CountVectorizer(tokenizer=lambda x: x.split(\" \"))\n    \n    data_X = vectorizer.fit_transform(data)\n    dim_red = TruncatedSVD(n_components=1, random_state = 5115)\n    data_X = dim_red.fit_transform(data_X)\n    \n    result = pd.DataFrame()\n    result[col1] = data.index.values\n    result[col1+\"_{}_svd\".format(col2)] = data_X.ravel()\n    return result\n\nimport itertools\ndef get_col_interactions_svd(dataset, tfidf = True):\n    new_dataset = pd.DataFrame()\n    for col1,col2 in itertools.permutations(dataset.columns, 2):\n        data = extract_col_interaction(dataset, col1,col2, tfidf)\n        col_name = [x for x in data.columns if \"svd\" in x][0]\n        new_dataset[col_name] = dataset[[col1]].merge(data, on = col1, how = 'left')[col_name]\n    return new_dataset","b2b0cdae":"new_train, new_test = transform_dataset(\n    train[col4train], test[col4train], \n    get_col_interactions_svd\n)\nprint(new_train.shape, new_test.shape)\nnew_train.head(5)","2a3b8546":"validate_model(\n    model = get_model(), \n    data = [new_train.values, y]\n)","7eaad715":"def get_freq_encoding(dataset):\n    new_dataset = pd.DataFrame()\n    for c in dataset.columns:\n        data = dataset.groupby([c]).size().reset_index()\n        new_dataset[c+\"_freq\"] = dataset[[c]].merge(data, on = c, how = \"left\")[0]\n    return new_dataset","0cd8bd11":"new_train, new_test = transform_dataset(\n    train[col4train], test[col4train], \n    get_freq_encoding\n)\nprint(new_train.shape, new_test.shape)\nnew_train.head(5)","c71851f0":"validate_model(\n    model = get_model(), \n    data = [new_train.values, y]\n)","313600b0":"new_train1, new_test1 = transform_dataset(\n    train[col4train], test[col4train], get_freq_encoding\n)\nnew_train2, new_test2 = transform_dataset(\n    train[col4train], test[col4train], get_col_interactions_svd\n)\nnew_train3, new_test3 = transform_dataset(\n    train[col4train], test[col4train], \n    assign_rnd_integer, {\"number_of_times\":10}\n)\n\nnew_train = pd.concat([new_train1, new_train2, new_train3], axis = 1)\nnew_test = pd.concat([new_test1, new_test2, new_test3], axis = 1)\nprint(new_train.shape, new_test.shape)","15b614d2":"validate_model(\n    model = get_model(), \n    data = [new_train.values, y]\n)","96fa76f7":"model = get_model()\nmodel.fit(new_train.values, y)\npredictions = model.predict_proba(new_test)[:,1]\n\nsubmit = pd.DataFrame()\nsubmit[\"Id\"] = test[\"id\"]\nsubmit[\"ACTION\"] = predictions\n\nsubmit.to_csv(\"submission.csv\", index = False)","f8cb8b0e":"Nice, AUC is 0.8782. Let's see how we perform on leaderboard.","b8971ade":"## SVD Encoding\n\nIn this embedding we will try to incorporate information about data interaction.\n\nTake 2 columns A and B, and present them as 2D matrix where rows are unique values of column A and columns are unique values of column B, in a cell of this matrix you'll put a number of times these 2 unique values can be found in dataset.\n\nYou also can apply IDF (inverse document transform) in order to assign bigger values rare pairs. \n\nAfter that you apply dimensionality reduction technique to this 2D matrix (Truncated SVD in our case) and reduce this matrix to a vector. That vector will be embedding of column A based on column B. \n\nRepeat this process for all columns in your dataset.\n\n**Advantages** \n* could provide rich and multidimensional representations\n\n**Disadvantages**\n* Not really easy to understand what it's actually represents","cc08d66d":"Of course all this encodings can be combined.","4a16b5dd":"Ok, let's run the same transformation but with 1 and 10 columns instead of 5.","f90a2059":"## Frequency encoding\n\nThe final way to encode our categorical features is frequency encoding - just count how many time this unique value is presented in your data.\n\n\n**Advantages** \n* easy to implement\n* easy to understand\n\n**Disadvantages**\n* Can't be used in case if you categorical values are balanced","1427c912":"## Unsupervised categorical encodings\n\nWe will start with \"unsupervised\" categorical encodings. By \u201cunsupervised\u201d here I mean we are not going to use information about the target in any way. \n\nOne of the advantages of this approach - you can use all data you have, for example in case of Kaggle competition you can use both files, `train.csv` and `test.csv`. \n\nIn first cell we load all data we need.","e339e9f5":"So far the worst results - AUC is 0.8209. Linear model on OHE gives us AUC of 0.86","fe2089c1":"As you can see for 10 columns we've got AUC score of 0.8663, while for 1 column score was only 0.7843.","66d9ca62":"And results are not very satisfying - AUC is 0.8616.","56232c7b":"## One-hot encoding\n\nNow let get back to one-hot encoding and check how it works for tree-based model.\n\n**Advantages** \n* easy to implement\n* easy to understand\n* for tree-based models like RF helps to build slightly more robust models\n\n**Disadvantages**\n* Working with very sparse matrices can be hard\n* Tree-based models can drop in performance on this type of encoding","247b20e4":"As you can see for each of 8 columns we've made 5 columns with assigned random integers, which gives us 40 columns in total.","20a2a559":"## Label Encoding\n\nFirst way to encode your categorical feature is slightly counterintuitive, you just randomly assign unique integer value for each category level. \n\nAnd because it is randomly assigned why limit ourselves to just one column? Let do arbitrary amount of integers assigned. \n\n**Advantages** - because values are randomly assigned, they are unbiased. \n\n**Disadvantages** - hard to explain to business users why did you do that.","92fbe00f":"As you can see results are good. But running time is bigger too. Unfortunately, one-hot encoding creates a very sparse data, and all tree-based models have trouble with that type of data which could lead to lower performance scores or longer time to train (not true for libraries such as XGBoost or LightGBM, they can handle sparse data pretty good).","3f899bfb":"Here some helper functions."}}