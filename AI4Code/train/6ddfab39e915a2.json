{"cell_type":{"ed60237f":"code","cd3539e4":"code","5482aa76":"code","354c2212":"code","b5b5a1ad":"code","044004e7":"code","a48a1ec6":"code","bce6c824":"code","936390c9":"code","7aafd8b9":"code","d8fcd49a":"code","2d962dbc":"code","a236698b":"code","39c85ddb":"code","be142894":"code","48ce25cc":"code","65c8fda0":"code","8d882c5a":"code","92e01c7d":"code","bfe8bca4":"code","b0e334bf":"code","201f75af":"code","fc378aa7":"code","51b74a3c":"code","5b0aefba":"code","2179e306":"code","6e7376c2":"code","516163de":"code","1b8f68aa":"code","ada755c0":"markdown","4be9afb6":"markdown","bb0a27e1":"markdown","992e051e":"markdown","9b02bb3b":"markdown","c4d9a786":"markdown","bc185445":"markdown","7a735b95":"markdown","cd16aa16":"markdown","28c8b220":"markdown","cbd04062":"markdown","684e0520":"markdown","10a25fc4":"markdown","073fa49a":"markdown","b1074bea":"markdown","40461936":"markdown","14aef397":"markdown","0d8dcab3":"markdown","725bd8e1":"markdown","60fb1588":"markdown","0060601d":"markdown","710fab3a":"markdown","84b4f8da":"markdown","e014e11c":"markdown","7475dada":"markdown","c550f3be":"markdown","9bd2360f":"markdown","3884ab2a":"markdown","ae21e5c5":"markdown","b3de6a34":"markdown"},"source":{"ed60237f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","cd3539e4":"data = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')","5482aa76":"data.head()","354c2212":"data.tail()","b5b5a1ad":"data.sample(5)","044004e7":"data.shape,data.size","a48a1ec6":"data.describe()","bce6c824":"data.info()","936390c9":"data.isnull().sum()","7aafd8b9":"fig,ax=plt.subplots(1,1,figsize=(16,7))\nax.boxplot(data)\nplt.show()","d8fcd49a":"data['BMI'].max(),data['BMI'].min(),data['BMI'].mean(),data['BMI'].mode()","2d962dbc":"# First quartile (Q1) \nQ1 = np.percentile(data['BMI'], 15, interpolation = 'midpoint') \n  \n# Third quartile (Q3) \nQ3 = np.percentile(data['BMI'], 85, interpolation = 'midpoint') \n  \n# Interquaritle range (IQR) \nIQR = Q3 - Q1 \nprint(\"Q1 = \",Q1)\nprint(\"Q3 = \",Q3)\nprint(IQR) ","a236698b":"(data['BMI']>Q3).sum(),(data['BMI']<Q1).sum()","39c85ddb":"np.percentile(data['BMI'], 98.5, interpolation = 'midpoint') ","be142894":"val = data['BMI'].sort_values(ascending=False)","48ce25cc":"print(val[:8])","65c8fda0":"data = data[(data['BMI']>50)==False]","8d882c5a":"data.shape","92e01c7d":"plt.boxplot(data['BMI'])\nplt.show()","bfe8bca4":"xs = data['Outcome'].value_counts().index\nys = data['Outcome'].value_counts().values\n\nax = sns.barplot(xs, ys)\nax.set_xlabel(\"Outcome\")\nplt.show()","b0e334bf":"data.plot(kind= 'kde' , subplots=True, layout=(3,3), sharex=False, sharey=False, figsize=(15,10))\nplt.show()","201f75af":"# Age vs BloodPressure with hue = Outcome\nplt.figure(figsize=(12,8))\nax = sns.scatterplot(x=\"BloodPressure\", y=\"Age\", alpha=0.4, data=data[data['Outcome'] == 0])\nsns.scatterplot(x=\"BloodPressure\", y=\"Age\", alpha=1, data=data[data['Outcome'] == 1], ax=ax)\nplt.show()","fc378aa7":"# BloodPressure vs BMI with hue = Outcome\nplt.figure(figsize=(12,8))\nax = sns.scatterplot(y=\"BMI\", x=\"BloodPressure\", alpha=0.4, data=data[data['Outcome'] == 0])\nsns.scatterplot(y=\"BMI\", x=\"BloodPressure\", alpha=1, data=data[data['Outcome'] == 1], ax=ax)\nplt.show()","51b74a3c":"# BloodPressure vs BMI with hue = Outcome\nplt.figure(figsize=(12,8))\nax = sns.scatterplot(y=\"Glucose\", x=\"BloodPressure\", alpha=0.4, color=\"blue\", label=\"0\", data=data[data['Outcome'] == 0])\nsns.scatterplot(x=\"BloodPressure\", y=\"Glucose\", alpha=1, color=\"red\", label=\"1\", data=data[data['Outcome'] == 1], ax=ax)\nplt.show()","5b0aefba":"# Splitting into features and value to be predicted\nX = data.drop(columns=['Outcome'])\ny = data['Outcome']\nfig, ax = plt.subplots(1,2 ,figsize = (10,5))\n\nsns.barplot(x=['0', '1'], y =[sum(y == 0), sum(y == 1)], ax = ax[0])\nax[0].set_title(\"Before Oversampling\")\nax[0].set_xlabel('Outcome')\n\n#Using SMOTE to balance the Data\nfrom imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state = 2) \nX, y = sm.fit_resample(X, y) \n\nsns.barplot(x=['0', '1'], y =[sum(y == 0), sum(y == 1)], ax = ax[1])\nax[1].set_title(\"After Oversampling\")\nax[1].set_xlabel('Outcome')\n\nplt.tight_layout()\nplt.show()","2179e306":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=40)\n\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","6e7376c2":"from sklearn import model_selection\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('DT', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10)\n    pipeline = make_pipeline(StandardScaler(), RandomForestClassifier())\n    pipeline.fit(X_train, y_train)\n    cv_results = model_selection.cross_val_score(pipeline, X, y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n    \n# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","516163de":"from sklearn.metrics import accuracy_score, classification_report, plot_confusion_matrix\n\npipeline = make_pipeline(StandardScaler(), RandomForestClassifier())\npipeline.fit(X_train, y_train)\nprediction = pipeline.predict(X_test)\n\nprint(f\"Accuracy Score : {round(accuracy_score(y_test, prediction) * 100, 2)}%\")","1b8f68aa":"print(classification_report(y_test, prediction))","ada755c0":"    So our data have 768 rows and 9 columns, with a total size of 6912 cells in it.","4be9afb6":"    So we have our outliars removed and also we are clear from missing data. \n    \n    It's time for some visualizations. \n<hr \/>","bb0a27e1":"### 1.1 Top 5 rows of data ","992e051e":"\nOur Data is clearly not balanced. We will balance this data using **SMOTE technique** after some visualizations.\n\n**SMOTE** is an oversampling technique where the synthetic samples are generated for the minority class. This algorithm helps to overcome the **overfitting problem posed by random oversampling**. It focuses on the feature space to generate new instances with the help of interpolation between the positive instances that lie together.\n<hr \/>","9b02bb3b":"     These are the top values which I am counting as outliers. ","c4d9a786":"### 2.2 Searching for Outliers.","bc185445":"<hr \/>\nNow let's apply SMOTE for balancing our data.\n\nBalancing data is important for better modeling results.\n<hr \/>","7a735b95":"    From above we can see that none of our column in object or text. (only int and float)\n    \n    We can compute and visualize this data easily.","cd16aa16":"# 2. Imputations","28c8b220":"### 1.2 Last 5 rows of data","cbd04062":"### 2.1 Check For Null\/Missing values","684e0520":"## Insights\n<hr>\n<h4> - A younger person with high blood pressure level have more chances of getting diabetic positive than a elder person with high blood pressure\n<br><br>\n- An average person with BMI more than 35 have more chances of getting diabetic positive inspite of having a normal blood pressure also.\n<br><br>\n- A person with high glucose level and high blood pressure have more chances of getting diabetic positive.\n<br><br>\n- A person with high glucose level and high BMI can also come diabetic positive.\n<\/h4>","10a25fc4":"### 1.4 Feature overview","073fa49a":"We will use the cross validation technique for testing different models on our data.\n\n\n<h4>What is Cross-validation?<\/h4>\nCross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. \nThe procedure has k number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation.(k is the only parameter given)","b1074bea":"Here I am using RandomForest Classifier as base and for described modeling.\n\nI am using the **pipeline** feature to make a pipeline for standardising and then only applying proper algorithm. \n<hr \/>\n","40461936":"# Let's Start !\n# 1. Import Libraries and Data","14aef397":"Wow, we got accuracy score above 80%. Thats great.\n\nNow let's check for the model report.","0d8dcab3":"# Overview \n[source](https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database)\n\n## Context\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n## Content\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n<hr \/>\n\n<h2>Table of Content:<\/h2>\n\n### 1. Import Libraries and Data \n    1.1 Top 5 rows of data\n    1.2 Last 5 rows of data\n    1.3 Some Random Values from our data \n    1.4 Feature overview\n    \n### 2. Imputations <br \/>\n\n    2.1 Check For Null\/Missing values<br>\n    2.2 Check For Outliers<br>\n    \n### 3. Exploratory data analysis <br \/>\n    \n### 4. Modeling<br \/>\n\n<hr \/>    \n<hr \/>","725bd8e1":"    So after removing outliers we are left with 760 rows. \n    \n    Lets check removal of Outliers by plotting a boxplot.","60fb1588":"    So we do not have any null\/missing value. Wow, thats great, now lets check for some outliers.","0060601d":"     I will be using Interquartile range method for pointing out the outliers, \n     as in this we choose all the data between 15% and 85% rest data we can drop.","710fab3a":"We have our maximim columns normally distributed","84b4f8da":"     So we have some outliers in column 5 i.e BMI column.\n     Come let's fix this.","e014e11c":"    Here we can see that we have alot data left from these quaritile values.(204 and we can not ignore such large data)\n    \n    So I manually checked for the outliers and got that we have most outliers after 99% of data,\n    means only 1% data contains outliers so lets drop them.  ","7475dada":"Let's first split the data into X,y using sklearn functions","c550f3be":"# Thanks for your time :)\n\n## If you like this kernel an Upvote would be appreciated.","9bd2360f":"#### Firstly lets check that if our data is balanced or not.\n**Balanced data** : If there are two classes, then balanced data would mean 50% points for each of the class.)\n#### Then we will plot a Distribution plot and other visualizations.","3884ab2a":"# 4. Modeling ","ae21e5c5":"# 3. Exploratory data analysis","b3de6a34":"### 1.3 Some Random Values from our data"}}