{"cell_type":{"92f40e17":"code","b83dd0c8":"code","7966014e":"code","86233c18":"code","3bc3c81a":"code","3294ecbc":"code","43e7e89d":"code","1f607bc7":"code","30f8bbaf":"code","ac468659":"code","26076346":"code","d5169a87":"code","e3d8757f":"code","067d7ec9":"code","7576e908":"code","03cf59e9":"code","e05e9578":"code","214e317d":"code","55dcc199":"code","58de5740":"code","5cdb7ecc":"code","94ae5f25":"code","fa9f6422":"code","c77c89fe":"code","658cc2ea":"code","c2cbff65":"code","af04d96e":"code","1b69af9b":"code","4f490695":"code","8cb86ae2":"code","7c8d2be0":"code","ca825138":"code","b8631b27":"code","d21da399":"code","aee25938":"code","53bd5396":"code","3410ca1f":"code","c34783a8":"code","0d51a668":"code","323b12da":"code","44770cb6":"code","ea129d0f":"code","a6dea532":"code","cc5ac9d8":"code","da88bdd6":"code","cd5444f2":"code","8e98c19b":"code","32040a2e":"code","5043abf0":"code","21c2f26a":"code","9d4ae577":"code","32ba5303":"code","c92422bd":"code","9dac3b50":"markdown","53af9d10":"markdown","ea02b4d7":"markdown","9a3d3eb9":"markdown","03b448be":"markdown","f1bb0f3d":"markdown","32617049":"markdown","f491189a":"markdown","03dcc4fb":"markdown","820ba258":"markdown","4cc973e2":"markdown","6bdcfd85":"markdown","ec434695":"markdown","ab36116d":"markdown","ab4c17f4":"markdown","81396c97":"markdown","1b89c7f5":"markdown","372215c4":"markdown","dc624d52":"markdown","4ac4405d":"markdown","ec921995":"markdown","c8a6600a":"markdown","9843e204":"markdown","0cb4b1c8":"markdown","f5afb2ff":"markdown","3cedb3b6":"markdown","056daefc":"markdown","7f6d0de1":"markdown","ad2d6af0":"markdown","c56a7c32":"markdown","42936ba6":"markdown","874cbc4e":"markdown","bf6f671d":"markdown"},"source":{"92f40e17":"!pip install keras_metrics","b83dd0c8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datetime as dt\nimport matplotlib.pylab as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom sklearn.model_selection import train_test_split\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom tabulate import tabulate\nimport seaborn as sns\n# Any results you write to the current directory are saved as output.","7966014e":"payments = pd.read_csv(\"..\/input\/payments.csv\")","86233c18":"sum_missing = payments.isnull().sum()\npercent_missing = payments.isnull().sum() * 100 \/ len(payments)\n\nmissing_value_df = pd.DataFrame({'column_name': payments.columns,\n                                 'total_missing':sum_missing,\n                                 'percent_missing': percent_missing})","3bc3c81a":"missing_value_df","3294ecbc":"dep_payment = payments[payments['DEPARTMENT NAME'].notnull()]\ndept_df = dep_payment.groupby('DEPARTMENT NAME').size().reset_index(name='counts').sort_values(by=['counts'], ascending=False)","43e7e89d":"#fig = plt.figure(figsize=(10,4))\nax = dept_df.plot.bar(x='DEPARTMENT NAME', y='counts', rot=0, figsize=(10,7))\nax.set_xticklabels(dept_df['DEPARTMENT NAME'], rotation=90)\nplt.show()","1f607bc7":"payments[payments['CONTRACT NUMBER'] == 'DV'].shape[0] \/ len(payments)","30f8bbaf":"payments['check_date'] = pd.to_datetime(payments['CHECK DATE'])\ncheckDate_df = payments.groupby(payments['check_date'].map(lambda x: x.year)).size().reset_index(name='counts').sort_values(by=['check_date'], ascending=True)","ac468659":"ax = checkDate_df.plot.bar(x='check_date', y='counts',figsize=(10,5))\nax.set_xticklabels(checkDate_df['check_date'], rotation=90)\nplt.show()","26076346":"payments[payments['VENDOR NAME'].isnull()]","d5169a87":"per_vendor_pay_count_df = payments.groupby('VENDOR NAME').size().reset_index(name='counts').sort_values(by=['counts'], ascending=False)","e3d8757f":"len(per_vendor_pay_count_df[per_vendor_pay_count_df['counts'] < 2]) \/ len(per_vendor_pay_count_df)","067d7ec9":"len(per_vendor_pay_count_df[per_vendor_pay_count_df['counts'] >= 100]) \/ len(per_vendor_pay_count_df)\n","7576e908":"len(per_vendor_pay_count_df[per_vendor_pay_count_df['counts'] >= 500]) \/ len(per_vendor_pay_count_df)","03cf59e9":"per_vendor_pay_count_500 = per_vendor_pay_count_df[per_vendor_pay_count_df['counts'] >= 500]","e05e9578":"fig = plt.figure(figsize=(10,4))\nax = per_vendor_pay_count_500.plot.bar(x='VENDOR NAME', y='counts', rot=0, figsize=(10,7))\nax.set_xticklabels(per_vendor_pay_count_500['VENDOR NAME'], rotation=90)\nplt.show()","214e317d":"top_100_vendors = per_vendor_pay_count_df[per_vendor_pay_count_df['counts'] >= 100]\npayment_subset = pd.merge(payments, top_100_vendors, on='VENDOR NAME')","55dcc199":"print('vendor percent', len(top_100_vendors) \/ len(per_vendor_pay_count_df))\nprint('amount percent', payment_subset['AMOUNT'].sum() \/ payments['AMOUNT'].sum())","58de5740":"per_vendor_pay_count_1 = per_vendor_pay_count_df[per_vendor_pay_count_df['counts'] < 2]\npayment_1 = pd.merge(payments, per_vendor_pay_count_1, on='VENDOR NAME')","5cdb7ecc":"print('vendor percent', len(per_vendor_pay_count_1) \/ len(per_vendor_pay_count_df))\nprint('amount percent', payment_1['AMOUNT'].sum() \/ payments['AMOUNT'].sum() * 100)","94ae5f25":"#per_vendor_pay_count_between = per_vendor_pay_count_df[2 <= per_vendor_pay_count_df['counts'] <= 99 ]\n\nper_vendor_pay_count_between = per_vendor_pay_count_df[(per_vendor_pay_count_df['counts'] >= 2) & (per_vendor_pay_count_df['counts'] <= 99)]\npayment_between = pd.merge(payments, per_vendor_pay_count_between, on='VENDOR NAME')","fa9f6422":"print('vendor percent', len(per_vendor_pay_count_between) \/ len(per_vendor_pay_count_df))\nprint('amount percent', payment_between['AMOUNT'].sum() \/ payments['AMOUNT'].sum())\n","c77c89fe":"print(tabulate([['=1', '72%', '1.27%'], ['>=100','0.15%', '38%'], ['2-99','28%','62%']], \n               headers=['Range', 'Vendor %', 'Amount %']))","658cc2ea":"len(payment_subset)\/len(payments)","c2cbff65":"len(payment_subset[payment_subset['DEPARTMENT NAME'].isnull()])\/len(payment_subset)","af04d96e":"len(payment_subset[payment_subset['CONTRACT NUMBER'] == 'DV'])\/len(payment_subset)","1b69af9b":"# for predicting Department Name \n# train = payments[payments['DEPARTMENT NAME'].notnull()]\n# test = payments[payments['DEPARTMENT NAME'].isnull()]\n# test = test[test['VENDOR NAME'].notnull()]\n# #train_df = train.sample(50000, replace=True).reset_index()\n# #test_df = test.sample(50000, replace=True).reset_index()\n# train_df = train_df[['AMOUNT','VENDOR NAME', 'CASHED', 'DEPARTMENT NAME']]\n# test_df =  test_df[['AMOUNT','VENDOR NAME', 'CASHED', 'DEPARTMENT NAME']]","4f490695":"per_vendor_pay_count_df = payments.groupby('VENDOR NAME').size().reset_index(name='counts').sort_values(by=['counts'], ascending=False)\nper_vendor_pay_count_df['high_vendor'] = [1 if x >= 1500 else 0 for x in per_vendor_pay_count_df['counts']]\nper_vendor_pay_count_df['medium_vendor'] = [1 if 700 <= x < 1500 else 0 for x in per_vendor_pay_count_df['counts']] \nper_vendor_pay_count_df['low_vendor'] = [1 if x < 700 else 0 for x in per_vendor_pay_count_df['counts']] \nper_vendor_pay_count_df.columns = ['VENDOR NAME', 'vendor_count', 'high_vendor','medium_vendor','low_vendor']","8cb86ae2":"payments = pd.merge(payments, per_vendor_pay_count_df, on='VENDOR NAME')","7c8d2be0":"payments.head(10)","ca825138":"payment_data = payments[['AMOUNT','DEPARTMENT NAME', 'vendor_count', 'high_vendor', 'medium_vendor', \n                         'low_vendor','CASHED']]","b8631b27":"payment_data.CASHED = [1 if i == True else 0 for i in payment_data.CASHED]","d21da399":"payment_data[\"AMOUNT\"] = (payment_data[\"AMOUNT\"] - payment_data[\"AMOUNT\"].min()) \/ (payment_data[\"AMOUNT\"].max() - payment_data[\"AMOUNT\"].min())\npayment_data[\"vendor_count\"] = (payment_data[\"vendor_count\"] - payment_data[\"vendor_count\"].min()) \/ (payment_data[\"vendor_count\"].max()- payment_data[\"vendor_count\"].min())","aee25938":"payment_data_encoded = pd.concat([payment_data,pd.get_dummies(payment_data['DEPARTMENT NAME'],prefix='DEPT', drop_first=True, dummy_na=True)],axis=1)\n","53bd5396":"payment_data_encoded.drop(['DEPARTMENT NAME'],axis=1, inplace=True)","3410ca1f":"payment_data_encoded.head(5)","c34783a8":"payment_data_encoded.groupby('CASHED').size()","0d51a668":"735174\/len(payment_data_encoded)","323b12da":"Y = payment_data_encoded.pop('CASHED').to_frame().reset_index(drop=True)\nX = payment_data_encoded","44770cb6":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)","ea129d0f":"#fig = plt.figure(figsize=(10,4))\n# dept_df = y_train.groupby('DEPARTMENT NAME').size().reset_index(name='counts').sort_values(by=['counts'], ascending=False)\n# ax = dept_df.plot.bar(x='DEPARTMENT NAME', y='counts', rot=0, figsize=(10,7))\n# ax.set_xticklabels(dept_df['DEPARTMENT NAME'], rotation=90)\n# plt.show()","a6dea532":"#from sklearn.preprocessing import OneHotEncoder\n#enc = OneHotEncoder(handle_unknown = 'ignore')\n#enc.fit(y_train)\n#y_train_encoded = enc.transform(y_train).toarray()\n#y_val_encoded = enc.transform(y_val).toarray()","cc5ac9d8":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nclf=RandomForestClassifier(random_state=0, n_estimators=100, max_depth=3, class_weight=\"balanced\")\nclf.fit(X_train,y_train)","da88bdd6":"clf_predict = clf.predict(X_test) \naccuracy = clf.score(X_test, y_test)\nprint('accuracy',accuracy)\ncm = confusion_matrix(y_test, clf_predict)\nprint(cm)","cd5444f2":"feature_imp = pd.Series(clf.feature_importances_,index=list(X_train.columns)).sort_values(ascending=False)[:10]\nfeature_imp","8e98c19b":"# Creating a bar plot\nsns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","32040a2e":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import np_utils\nimport keras_metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nnp.random.seed(7)","5043abf0":"# For Department Name Prediction\n#encode class values as integers\n#encoder = LabelEncoder()\n#encoder.fit(y_train)\n#encoded_train_Y = encoder.transform(y_train)\n#encoded_val_Y = encoder.transform(y_test)\n# convert integers to dummy variables (i.e. one hot encoded)\n#dummy_train_y = np_utils.to_categorical(encoded_train_Y)\n#dummy_val_y = np_utils.to_categorical(encoded_val_Y)","21c2f26a":"def baseline_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(20, input_dim=56, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras_metrics.precision(), keras_metrics.recall()])\n    return model","9d4ae577":"#estimator = KerasClassifier(build_fn=baseline_model, epochs=10, batch_size=32, verbose=1)\n#results = cross_val_score(estimator, X_train, y_train )","32ba5303":"#kfold = KFold(n_splits=2, shuffle=True, random_state=seed)","c92422bd":"model = baseline_model()\nmodel.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test), verbose=1)","9dac3b50":"Because there are about 349107 unqiue vendors, it might not be efficient to one hot encode that column to use in the datatset. \nSo, I divided vendors in three categories (High, medium, low). High Vendor meaning vendors having at least 1500 payments each, medium \nvendor: between 700 and 1500 payments and low vendor means payments less than 700. I also included a column vendor_count that basically tells the total count of each vendor. ","53af9d10":"we can see \"department name\" is  85% missing. Let's see which department has the highest number of payments.","ea02b4d7":"From a simple neural network, we get validation set precision of 94.7% and recall of 100%. ","9a3d3eb9":"- Vendor names Office Depot has the highest number of payments (>5000) followed by Hecktoen Institute of Medical Research with just below 3000 payments.","03b448be":"# Classification via Random Forest Classifier:","f1bb0f3d":"There are only 10 payments where vendor name is missing. We also see that for all these payments, departname is also missing and contract number is DV.","32617049":"- About 51% of the subset data, department name is null!","f491189a":"- **percent of missingness in each column in payment dataset**","03dcc4fb":"**Let's plot the top vendors with at least 500 payments.**","820ba258":"> - About 0.1% of the vendors have been paid 100 or more times.","4cc973e2":"# Exploratory Data Analysis","6bdcfd85":"- About 72% of the vendors have been paid once in this dataset. That means there is only one payment corresponding to each of 72% of the vendors.","ec434695":"Vendor Count seems to be the most important feature for RF classifier. Followed by Amount and High Vendor.","ab36116d":"- The payments made by city of chicago are the highest in 2018 as compared to anyother year and growing in 2019. What can be the possible reason behind that?\n","ab4c17f4":"0.15% the vendors that have been paid at least 100 times account for about 38% of the total amount.","81396c97":"### Normalizing Amount and Vendor Count columns","1b89c7f5":"> And 28% the vendors that have been paid between 2 and 99 times account for about 62% of the total amount.","372215c4":"Random forest results in 88% of the accuracy. But most of the accuracy is driven by the majority class (ie: Cashed =1)\nRecall = 14%\nPrecision = 25%","dc624d52":"## There are 50 unique departments. I hot encoded that column in our dataset.","4ac4405d":"If we consider vendors having at least 100 payments made to them, we will be left with 19% of the overall data.","ec921995":"## Some Insights about dataset\n- About 20% of the values in voucher number column is missing\n- Voucher number is not always specific format so parsing might be an issue\n- Check Date format is not consistent - sometimes it is MM\/DD\/YYYY format and other times it is only YYYY\n- Department name is 85% missing - It is empty when Contract number is DV but also empty at other places, so does not seem like a pattern \n- Very small percentage of vendorname is missing 0.0001% (10 payments)\n- For all the payments where vendorname is missing, department name is also missing and Contract number is DV for all of those payments\n- 1.31% of values in Cashed column are missing. All missing values in cashed also corresponds to DV payments\n- 73% of all payments are Direct Vendor (DV) payments\n- power law - does 20% of the people keep 80% of the cash? No, not exactly power law","c8a6600a":"> Only about 0.001% of the vendors have been paid 500 or more times.","9843e204":"department of family and support services has the highest number of payments.","0cb4b1c8":"### There is class imbalanced issue in our data. 95% of the payments are cashed and only 5% of the non-cashed.","f5afb2ff":"- About 16% the vendors in subset data are direct vendors! (DV)","3cedb3b6":"**Let's now look at the yearly payment distribution**","056daefc":"**NEURAL NETWORK**","7f6d0de1":"# Vendor Frequency Analysis","ad2d6af0":"**Let's now try to predict whether a payment is cashed or not. **","c56a7c32":"- About 73% of all payments are Direct Vendor (DV) payments","42936ba6":"This shows that 72% of the vendors (who have been paid only once) account of only 1.27% of the amount","874cbc4e":"Visualizing top 10 most important features:","bf6f671d":"### Above is how our final data looks like"}}