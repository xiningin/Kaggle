{"cell_type":{"26a5b0be":"code","f77ff80c":"code","61d6ea1d":"code","f1aed5a4":"code","066608a7":"code","5f001365":"code","69d9f858":"code","77adef2a":"code","ee741c7b":"code","2d821dff":"code","df413d5d":"code","87a56ac9":"code","3b679b76":"code","d51b0a09":"code","9cc3b27d":"code","e0127d7c":"code","77db244b":"code","e86ed5de":"code","e0316d61":"code","598fc42d":"code","6bebe81d":"code","9b57212d":"code","cf1b0c7e":"code","2a4ba327":"code","3c22bcfd":"code","d72f54c8":"code","3b050165":"code","2e2e9b0e":"code","6eb1b441":"code","d49e5f5e":"code","54d56935":"code","448fc658":"code","a9e5fdb5":"code","d924a6ae":"markdown","1792b0d4":"markdown","ce9b01eb":"markdown","f2135e6e":"markdown","1933c977":"markdown","35ad3518":"markdown","684ae3c6":"markdown","bafb5b4c":"markdown","a6ee2502":"markdown","14abb0f9":"markdown","81871bbb":"markdown","5f23db23":"markdown","7513e34b":"markdown","40419834":"markdown","782ec43b":"markdown","ef7c361c":"markdown","49f6747f":"markdown","c1e09b99":"markdown","81d6cb4d":"markdown","fcfc3287":"markdown","6cad6df8":"markdown","3780025a":"markdown","b9f0fa1c":"markdown","2495a422":"markdown"},"source":{"26a5b0be":"!pip install dabl","f77ff80c":"import numpy as np\nimport pandas as pd\n\nimport dabl\n\nfrom pandas_profiling import ProfileReport #For generating profile reports\n\nimport matplotlib.pyplot as plt  #data visualization\nimport seaborn as sns\nimport plotly.graph_objects as go\n\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n\nfrom sklearn.experimental import enable_iterative_imputer   # For imputing missing values\nfrom sklearn.impute import IterativeImputer, SimpleImputer\n\nfrom sklearn.metrics import roc_auc_score, classification_report, confusion_matrix  #metrics\nfrom scipy import stats\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')  #silence warnings","61d6ea1d":"df = pd.read_csv(\"..\/input\/widsdatathon2021\/TrainingWiDS2021.csv\")\ntest = pd.read_csv(\"..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv\")\ndata_dict = pd.read_csv(\"..\/input\/widsdatathon2021\/DataDictionaryWiDS2021.csv\")\ntemplate = pd.read_csv(\"..\/input\/widsdatathon2021\/SolutionTemplateWiDS2021.csv\")\nsample_sub = pd.read_csv(\"..\/input\/widsdatathon2021\/SampleSubmissionWiDS2021.csv\")","f1aed5a4":"df.head()","066608a7":"test.head()","5f001365":"print(df.shape)\nprint(test.shape)","69d9f858":"data_dict.head() # data_dict gives information about the columns in the main data","77adef2a":"df.describe() # summary statistics for numeric features","ee741c7b":"missing_cnt = df.isna().sum() #column wise sum of missing values\n\n#display only the columns with missing values with count and proportion of missing values by column \nmissing_df = pd.concat([missing_cnt.rename('Missing Count'), \n                        missing_cnt.div(len(df)\/100).rename('Missing Ratio')], axis=1).loc[missing_cnt.ne(0)]\nmissing_df","2d821dff":"# dabl clean method removes near-constant columns\ndabl.clean(df)","df413d5d":"dabl.plot(df, \"diabetes_mellitus\")","87a56ac9":"train_profile = ProfileReport(df, 'EDA')\ntrain_profile","3b679b76":"df.drop(['Unnamed: 0', 'encounter_id'], axis=1, inplace=True) #drop columns with all unique values\ntest_ids = test.pop('encounter_id')  # ids required for submission\ntest.drop(['Unnamed: 0'], axis=1, inplace=True) \n\ntarget = df['diabetes_mellitus']\n\ndf_non_null = df.dropna(axis=1) #drop all columns with missing values\ndf_non_null.drop('diabetes_mellitus', axis=1, inplace=True)","d51b0a09":"class LabelEncoderExt(object):\n    def __init__(self):\n        \"\"\"\n        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n        \"\"\"\n        self.label_encoder = LabelEncoder()\n        # self.classes_ = self.label_encoder.classes_\n\n    def fit(self, data_list):\n        \"\"\"\n        This will fit the encoder for all the unique values and introduce unknown value\n        :param data_list: A list of string\n        :return: self\n        \"\"\"\n        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n        self.classes_ = self.label_encoder.classes_\n\n        return self\n\n    def transform(self, data_list):\n        \"\"\"\n        This will transform the data_list to id list where the new values get assigned to Unknown class\n        :param data_list:\n        :return:\n        \"\"\"\n        new_data_list = list(data_list)\n        for unique_item in np.unique(data_list):\n            if unique_item not in self.label_encoder.classes_:\n                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n\n        return self.label_encoder.transform(new_data_list)","9cc3b27d":"le = LabelEncoderExt()  \nle.fit(df_non_null['icu_stay_type']) #fit on train set column \ndf_non_null['icu_stay_type'] = le.transform(df_non_null['icu_stay_type']) #transform train set column \ntest['icu_stay_type'] = le.transform(test['icu_stay_type']) #transform test set column\n\nle.fit(df_non_null['icu_type'])\ndf_non_null['icu_type'] = le.transform(df_non_null['icu_type'])\ntest['icu_type'] = le.transform(test['icu_type'])","e0127d7c":"cols_5 = missing_df[missing_df['Missing Ratio']<5].index # get names of columns with less than 5% missing values\ndf_5 = pd.concat([df_non_null,df[cols_5]], axis=1) # concatenate new table with existing non-null values table\ndf_5.shape","77db244b":"pd.DataFrame(df_5).isna().sum().sum()","e86ed5de":"df_5.select_dtypes('object').columns # columns with object dtype","e0316d61":"df_5['ethnicity'] = df_5['ethnicity'].fillna(np.nan) # fill missing values with np.nan\ndf_5['gender'] = df_5['gender'].fillna(np.nan)\ndf_5['icu_admit_source'] = df_5['icu_admit_source'].fillna(np.nan)\n\ntest['ethnicity'] = test['ethnicity'].fillna(np.nan) \ntest['gender'] = test['gender'].fillna(np.nan)\ntest['icu_admit_source'] = test['icu_admit_source'].fillna(np.nan)","598fc42d":"simple_imp = SimpleImputer(strategy='most_frequent')# instantiate SimpleImputer \nsimple_imp.fit_transform(df_5['ethnicity'].values.reshape(-1,1))\nsimple_imp.transform(test['ethnicity'].values.reshape(-1,1))# SimpleImputer only accepts arrays\ndf_5['ethnicity'] = df_5['ethnicity'].astype('str') # convert data to string type\ntest['ethnicity'] = test['ethnicity'].astype('str')\n\nsimple_imp.fit_transform(df_5['gender'].values.reshape(-1,1))\nsimple_imp.transform(test['gender'].values.reshape(-1,1))\ndf_5['gender'] = df_5['gender'].astype('str')\ntest['gender'] = test['gender'].astype('str')\n\nsimple_imp.fit_transform(df_5['icu_admit_source'].values.reshape(-1,1))\nsimple_imp.transform(test['icu_admit_source'].values.reshape(-1,1))\ndf_5['icu_admit_source'] = df_5['icu_admit_source'].astype('str')\ntest['icu_admit_source'] = test['icu_admit_source'].astype('str')","6bebe81d":"# Encode string values into int values\nle1 = LabelEncoder()\nle = LabelEncoderExt()\n\nle.fit(df_5['ethnicity'])\ndf_5['ethnicity'] = le.transform(df_5['ethnicity'])\ntest['ethnicity'] = le.transform(test['ethnicity'])\n\nle1.fit(df_5['gender'])\ndf_5['gender'] = le1.transform(df_5['gender'])\ntest['gender'] = le1.transform(test['gender'])\n\nle1.fit(df_5['icu_admit_source'])\ndf_5['icu_admit_source'] = le1.transform(df_5['icu_admit_source'])\ntest['icu_admit_source'] = le1.transform(test['icu_admit_source'])","9b57212d":"df_5.select_dtypes('object').columns # columns with object dtype","cf1b0c7e":"iter_imp = IterativeImputer(random_state=0) # instantiate IterativeImputer\ndf_5_new = iter_imp.fit_transform(df_5) # fit and transform the data\ndf_5_new = pd.DataFrame(df_5_new, columns=df_5.columns)","2a4ba327":"def compute_roc_auc(train_test_split, train_set, target):\n    X_train, X_test, y_train, y_test = train_test_split(train_set, target, test_size=0.2, random_state=42)\n    xgb = XGBClassifier(n_estimators=400, colsample_bytree=0.7, reg_lambda=120, subsample=0.75) #parameters set after tuning\n    xgb.fit(X_train, y_train)\n    y_test_pred = xgb.predict(X_test)\n    y_train_pred = xgb.predict(X_train)\n    print('train auc',roc_auc_score(y_train, y_train_pred))\n    print('test auc',roc_auc_score(y_test, y_test_pred))    ","3c22bcfd":"compute_roc_auc(train_test_split, df_non_null, target)","d72f54c8":"compute_roc_auc(train_test_split, df_5, target)","3b050165":"def add_features(lower_limit, upper_limit, df, missing_df, df_to_merge):\n    df_ = df[missing_df[(missing_df['Missing Ratio']>=lower_limit) & (missing_df['Missing Ratio']<upper_limit)].index]\n    df_new = pd.concat([pd.DataFrame(df_to_merge), df_], axis=1) # concatenate new table with existing table\n    df_new = df_new.select_dtypes(include=['int', 'float']) # exclude columns with string datatype, if any\n    df_new_ = iter_imp.fit_transform(df_new)\n    df_new_ = pd.DataFrame(df_new_, columns=df_new.columns) # return table with column names intact\n    return df_new_","2e2e9b0e":"df_10 = add_features(5, 10, df, missing_df, df_5)\n\ncompute_roc_auc(train_test_split, df_10, target)","6eb1b441":"df_15 = add_features(10, 15, df, missing_df, df_10)\n\ncompute_roc_auc(train_test_split, df_15, target)","d49e5f5e":"df_20 = add_features(15, 20, df, missing_df, df_15)\n\ncompute_roc_auc(train_test_split, df_20, target)","54d56935":"test_ = test[df_20.columns] # columns for test set\ntest_df = iter_imp.fit_transform(test_) # impute missing values in test set before prediction\ntest_df = pd.DataFrame(test_df, columns = test_.columns)","448fc658":"xgb = XGBClassifier(n_estimators=400, colsample_bytree=0.7, reg_lambda=120, subsample=0.75) #parameters set after tuning\nxgb.fit(df_20, target) # train on entire dataset for best results\ntest_pred = xgb.predict(test_df)","a9e5fdb5":"my_submission = pd.DataFrame()\nmy_submission['encounter_id'] = test_ids\nmy_submission['diabetes_mellitus'] = test_pred\nmy_submission.to_csv('My_submission.csv', index=False)","d924a6ae":"# Exploratory Data Analysis","1792b0d4":"46 new features have been added to the initial data after imputing the missing values. Let's see if the model is able to do a better job at target classification with the addition of new features.","ce9b01eb":"Test auc score seems to have saturated at 0.708, even with addition of more features I did not see an improvement in auc score. \n\nI used another strategy wherein I selected columns with less than 20% null values and eliminated the samples containing any missing values. I used this reduced and non-null dataset for training and prediction on test set. However this did not lead to a better score than the previous approach. Hence, I am not showing the code and output for the second strategy here.\n\nI will take the same set of features in test set as used in df_20 and use them for training and prediction. This would be my first submission. I will make more attempts in future to improve upon the present score. ","f2135e6e":"Adding new features helped achieve a better classification of the target variable. \n\nIn order to simplify addition of features at every 5 percentage intervals, I will define another function. This function will accept the lower and upper percentage limits we wish to add, the existing table to which the new features are to be added, table containing missing columns and percentages, initial table which contains all data, it returns the final table after adding the desired columns.","1933c977":"All features are required to be of int or float datatype for the purpose of imputing or modeling. Hence, convert icu_stay_type and icu_type from string to int dytpe. LabelEncoder is a useful method for this purpose, however in this case there are some new labels in test set which are not present in train set. So if we try to fit the encoder on train set and use it to transform the test set using sklearn's LabelEncoder class, we will see a value error. To overcome this issue, I'm defining a new label encoder class that will append a new label 'Unknown' to the existing labels at the time of fitting. When the encoder encounters a previously unseen value, it simply assigns the Unknown label to it.  ","35ad3518":"Now the data is ready for applying IterativeImputer","684ae3c6":"First, train the model on only non null features and compute roc auc score","bafb5b4c":"46 new columns have been added to the initial table","a6ee2502":"Out of 181 columns, 160 have missing values. The proportion of missing values in some columns is more than 80%. Clearly, missing values is a major issue with this data which needs to be fixed.","14abb0f9":"**Select columns with upto 10% missing values**","81871bbb":"**Install dabl which stands for Data Analysis Baseline library. Dabl can be used to automate many of the tasks related to data exploration and analysis**","5f23db23":"I will impute string values using SimpleImputer with 'most_frequent' strategy which simply identifies the most frequent value column wise and fills the missing values with it","7513e34b":"# Missing values\n\nThe data size is quite large, it has 130157 X 181 values. Let's see how many columns have missing values in terms of percentage.","40419834":"# Function to compute roc auc score\n\nI will define a function for computing roc auc score as the score will computed for several datasets. The function will accept training set, test set and train_test_split method. It will split the training set into a sub training set (80%) and test set(20%). A simple XGBClassifier will be trained on the sub training set, evaluated on test set. Finally the function prints roc auc score on full training set and the original test set.","782ec43b":"First submission yielded auc score of 0.7048 on Kaggle Public Leaderbard which is quite close to the auc score on my internal test set.","ef7c361c":"# Submission","49f6747f":"# Load data","c1e09b99":"# Problem Overview\n\nThis challenge is hosted by WiDS Datathon Committee. It focuses on patient health, with an emphasis on the chronic condition of diabetes, through data from MIT\u2019s GOSSIS (Global Open Source Severity of Illness Score) initiative.\n\n**Background**\n\nGetting a rapid understanding of the context of a patient\u2019s overall health has been particularly important during the COVID-19 pandemic as healthcare workers around the world struggle with hospitals overloaded by patients in critical condition. Intensive Care Units (ICUs) often lack verified medical histories for incoming patients. A patient in distress or a patient who is brought in confused or unresponsive may not be able to provide information about chronic conditions such as heart disease, injuries, or diabetes. Medical records may take days to transfer, especially for a patient from another medical provider or system. Knowledge about chronic conditions such as diabetes can inform clinical decisions about patient care and ultimately improve patient outcomes.\n\n**Overview**\n\nIn this challenge, participants are required to build a model to determine whether a patient admitted to an ICU has been diagnosed with a particular type of diabetes, Diabetes Mellitus.\n\n**Data Files**\n\n**TrainingWiDS2021.csv** - the training data. You should see 130,157 encounters represented here. Please view the Data Dictionary file for more information about the columns.\n\n**UnlabeledWiDS2021.csv** - the unlabeled data (data without diabetes_mellitus provided). You are being asked to predict the diabetes_mellitus variable for these encounters.\n\n**SampleSubmissionWiDS2021.csv** - a sample submission file in the correct format.\n\n**SolutionTemplateWiDS2021.csv** - a list of all the rows (and encounters) that should be in your submissions.****\n\n**DataDictionaryWiDS2021.csv** - supplemental information about the data.\n\n**Evaluation metric**\n\nModel will be evaluated on ethe Area under the Receiver Operating Characteristic (ROC) curve between the predicted and the observed target (diabetes_mellitus_diagnosis).","81d6cb4d":"There has been a significant improvement in test roc auc score after adding columns with 5% - 10% null values. ","fcfc3287":"**Select columns with upto 20% missing values**","6cad6df8":"# Import Libraries","3780025a":"**Select columns with upto 15% missing values**","b9f0fa1c":"Total missing values in the new table","2495a422":"# Addressing null values\n\nThe data has 160 columns with null values in varying proportions. However, it is important to retain as many features as possible for building a good predictive model. In order to minimize loss of data or introduction of arbitrary values, I will use sklearn's iterative imputer to impute missing values based on existing non-null features. I will do this in a phased manner as mentioned below.\n* Start with only non-null features, train model on these features, evaluate roc-auc score on test data\n* Add features with less than 5% missing values to initial non-null data, impute missing values using iterative imputer, train model and evaluate roc-auc score on test data\n* Add features with missing values between 5% - 10% to latest data (result of previous step), impute missing values, train model and evaluate\n* Add features with additional 5% missing data to existing data at each stage, impute, train and evaluate. Repeat these steps until there is no further improvement in roc-auc score of test data. At this point it is probable that a lot of data has been added through imputation and adding further data doesn't help the model learn relationships between the independent variables and the dependent variable"}}