{"cell_type":{"c66b837a":"code","1f848bc5":"code","c4dacc4d":"code","fe0f4e9a":"code","00654457":"code","0c63b4ad":"code","707f6a91":"code","1587f670":"code","9b578bda":"code","4c5f0932":"code","635c36b1":"code","74f6cd36":"markdown","76f5f410":"markdown","426fe64c":"markdown"},"source":{"c66b837a":"import os, collections, random, itertools\n\nimport tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","1f848bc5":"# load data\ndf = pd.read_csv(\"\/kaggle\/input\/quora-question-pairs\/train.csv.zip\")\ndf[\"question1\"] = df[\"question1\"].astype(str)  # resolve nan\ndf[\"question2\"] = df[\"question2\"].astype(str)\ndf[\"qid1\"] -= 1  #  index\ndf[\"qid2\"] -= 1","c4dacc4d":"# all questions are identified with its qid\nqid_to_question = {}\nfor qid1, qid2, question1, question2 in zip(df[\"qid1\"], df[\"qid2\"], df[\"question1\"], df[\"question2\"]):\n    qid_to_question[qid1] = question1\n    qid_to_question[qid2] = question2\nquestions_by_idx = [qid_to_question[qid] for qid in range(max(qid_to_question) + 1)]\nassert len(questions_by_idx) == len(qid_to_question)","fe0f4e9a":"!pip install sentence-transformers > \/dev\/null","00654457":"from sentence_transformers import SentenceTransformer","0c63b4ad":"model_name = 'bert-base-nli-stsb-mean-tokens'\nmodel = SentenceTransformer(model_name)","707f6a91":"sentence_vectors_by_idx = model.encode(questions_by_idx)","1587f670":"np.save(\"sentence_vectors_{}.npy\".format(model_name), sentence_vectors_by_idx)\nsentence_vectors_by_idx = np.load(\"sentence_vectors_{}.npy\".format(model_name))\nsentence_vectors_by_idx.shape","9b578bda":"model_name = 'bert-large-nli-stsb-mean-tokens'\nmodel = SentenceTransformer(model_name)","4c5f0932":"sentence_vectors_by_idx = model.encode(questions_by_idx)","635c36b1":"np.save(\"sentence_vectors_{}.npy\".format(model_name), sentence_vectors_by_idx)\nsentence_vectors_by_idx = np.load(\"sentence_vectors_{}.npy\".format(model_name))\nsentence_vectors_by_idx.shape","74f6cd36":"# Obtain BERT sentence embeddings\n\nVarious options for sentence transformers here: https:\/\/huggingface.co\/sentence-transformers\n\nWe are using `bert-base-nli-stsb-mean-tokens` and `bert-large-nli-stsb-mean-tokens`\n\nReference: https:\/\/arxiv.org\/pdf\/1908.10084.pdf\n\n- BERT is better at semantic textual similarity (STS) task compared to its variants (Table 2)\n- Mean pooling is better (Table 6)\n- Training on STSb (STS benchmark) improves performance on STS task (Table 2)\n- I do not think it has been trained on Quora dataset (to ensure no data leakage)","76f5f410":"#### Using `bert-large-nli-stsb-mean-tokens`","426fe64c":"#### Using `bert-base-nli-stsb-mean-tokens`"}}