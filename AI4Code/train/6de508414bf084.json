{"cell_type":{"02de7a74":"code","5dabaf47":"code","928219d0":"code","704bb5ef":"code","ec155822":"code","35c82c17":"code","889381e2":"code","cded033d":"code","64ea2556":"code","4c7ad305":"code","c32fce20":"code","d8e79133":"code","be39ca5c":"code","c013e5fa":"code","95b71e3a":"code","3d983bb8":"code","b31e5acd":"code","e82d18d1":"code","a76c6716":"code","021c7f0c":"code","ad701c07":"code","424755a8":"code","a30d099a":"code","4dbeb821":"code","2bb12a72":"code","bcb65e6e":"code","119ba657":"code","f34333cd":"code","80264c81":"code","abe8d1ab":"code","bd84a46d":"code","6e3f8df2":"code","df95501e":"code","91578d41":"code","45de10e4":"code","d3fa01c1":"code","7580b470":"code","f62b489f":"code","af508436":"code","b40f639a":"code","dc8792db":"code","0bf69f01":"code","6b6d53dd":"code","6779af12":"code","b5fdbf59":"code","fcd322c8":"code","af32c285":"code","301fb935":"code","9026bf95":"code","7b0a791d":"code","8b316c24":"code","bea79de2":"code","ef8fda0e":"code","a9a84c65":"code","d7401fde":"code","2d0daec9":"code","e0284a78":"code","a7752d52":"code","1e346261":"code","637a1b29":"code","062514e8":"code","88eb7642":"code","c0c667c8":"code","313113a5":"code","632a4531":"code","347328af":"code","acbcf0d9":"code","072d13db":"code","36f62c2e":"code","7cd1edc5":"code","3f9f0e48":"code","03f1c9b8":"code","b63edc6f":"code","35959476":"code","37e287b4":"code","87f48e6d":"code","546d70f1":"code","47142e80":"code","1fe6bb25":"code","b152a274":"code","035cce08":"code","70026882":"code","65fc2d3f":"code","483a7756":"code","5f17a798":"code","54250f35":"code","6f18fddb":"code","aa1d10d9":"code","5c81121e":"code","ac82d7a2":"code","72aa767f":"code","56d487f7":"code","a97e9b8e":"markdown","defc37d4":"markdown","46935101":"markdown","2fb54e75":"markdown","9a33ee67":"markdown","1f42d757":"markdown","3f42d6a3":"markdown","70037ab4":"markdown","2712aacd":"markdown","1cac7f8a":"markdown","6a6b55f0":"markdown","71695023":"markdown","d62dcc17":"markdown","1304e378":"markdown","781ab8e5":"markdown","733e140b":"markdown","2bf97bd5":"markdown","2d07ef02":"markdown","7d51b71b":"markdown","52c612b8":"markdown","911af037":"markdown","2747101a":"markdown","6c7a4d8f":"markdown","eece4ff0":"markdown","f61a8179":"markdown","2868cdc4":"markdown","bc28b2c0":"markdown","76663231":"markdown","ae1e1c83":"markdown","14555e11":"markdown","6bf3fcde":"markdown","f771af33":"markdown","8a7c54de":"markdown","77b3ad06":"markdown","802d27ca":"markdown","a5f17eff":"markdown","6de1b984":"markdown","71bc5492":"markdown","fb6423e0":"markdown","8650c360":"markdown","643a394d":"markdown","78f8932d":"markdown","79d4c3e3":"markdown","7990d3b3":"markdown","f3d4b503":"markdown","c7abbede":"markdown","17777779":"markdown","9913211b":"markdown"},"source":{"02de7a74":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n###################################################################\n# import warnings\n# warnings.simplefilter(action='error', category=FutureWarning)\n###################################################################\n\n###################################################################\n# Skip to End to see the Nueral Network Implementation\n###################################################################\n\n#################################################################################################################################################################################################\n# EDA is based on this article by Gokul S Kumar \n# https:\/\/towardsdatascience.com\/exploratory-data-analysis-of-kaggle-datasets-9a293886f644?source=friends_link&sk=f1eb21ee172069717e083c3c2f560099\n#################################################################################################################################################################################################\n\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import impute\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5dabaf47":"train_df = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/test.csv\")","928219d0":"train_df","704bb5ef":"test_df","ec155822":"submissions_index = test_df.Id # this will be used as index for submission csv file, ignore it now","35c82c17":"train_df.drop('Id', axis=1, inplace=True)\ntest_df.drop('Id', axis=1, inplace=True)","889381e2":"pd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 1000)\npd.options.display.float_format = '{:.2f}'.format","cded033d":"train_df.head(2)","64ea2556":"train_df.info()","4c7ad305":"test_df.info()","c32fce20":"train_df.shape","d8e79133":"train_df.shape # SalePrice dropped already","be39ca5c":"train_df.isnull().mean().sort_values(ascending=False)*100 # gives percent missing values\n\n# PoolQC, MiscF, Alley, Fence have 80 percent or more missing values","c013e5fa":"train_missing = train_df.isnull().sum()","95b71e3a":"train_missing = train_missing[train_missing > 0].sort_values()","3d983bb8":"# # https:\/\/github.com\/code-aesthete\/-Kaggle-House-Price-dataset\/blob\/master\/EDA%20housing%20prices.ipynb\n\n# plt.figure(figsize=(20,15))\n# sns.set_style('whitegrid')\n# ax = sns.barplot(x=train_missing.index, y=train_missing, palette ='rocket_r')\n# ax.set_xticklabels(ax.get_xticklabels(), rotation=90);","b31e5acd":"# df having only numerical columns\n\nfeatures_numerical = train_df.select_dtypes(exclude='object').drop('SalePrice', axis='columns')","e82d18d1":"features_numerical.columns","a76c6716":"features_numerical.shape # 37 numerical columns","021c7f0c":"features_categorical = train_df.select_dtypes(include='object')","ad701c07":"features_categorical.columns","424755a8":"features_categorical.shape # 43 numerical columns","a30d099a":"# fig = plt.figure(figsize=(20,25))\n# sns.set_style('whitegrid')\n# for i in range(len(features_numerical.columns)):\n#     fig.add_subplot(9, 4, i+1)\n#     sns.displot(features_numerical.iloc[:, i].dropna(), rug=True, kde_kws = {'bw':0.1}, color = 'b')\n#     plt.xlabel(features_numerical.columns[i])\n\n# plt.tight_layout()","4dbeb821":"# fig = plt.figure(figsize = (20,25))\n# sns.set(style = 'darkgrid')\n# for i in range(len(features_numerical.columns)):\n#     fig.add_subplot(9, 4, i+1)\n#     sns.boxplot(y = features_numerical.iloc[:,i].dropna())\n# plt.tight_layout()","2bb12a72":"# fig = plt.figure(figsize = (20,25))\n# sns.set(style = 'whitegrid')\n# for i in range(len(features_numerical.columns)):\n#     fig.add_subplot(9, 4, i+1)\n#     sns.scatterplot(features_numerical.iloc[:,i].dropna(), train_df.SalePrice)\n# plt.tight_layout()\n# plt.show()","bcb65e6e":"# by looking at above visualizations, we can see following columns have outliers in them\noutliers = train_df[['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', '1stFlrSF',\n                    'LowQualFinSF', 'GrLivArea', 'WoodDeckSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']]","119ba657":"# fig = plt.figure(figsize = (20,30))\n# for i in range(len(outliers.columns)):\n#     fig.add_subplot(5, 3, i+1)\n#     sns.regplot(outliers.iloc[:,i], train_df.SalePrice)\n# plt.tight_layout()","f34333cd":"# https:\/\/towardsdatascience.com\/exploratory-data-analysis-of-kaggle-datasets-9a293886f644\n# We can now confirm that the features are having outliers. The next step will be to remove them. \n# We can get the limits to remove them(from respective features) from the boxplots and regression plots.","80264c81":"train_df.drop(train_df[train_df.LotFrontage > 200].index, inplace = True)\ntrain_df.drop(train_df[train_df.LotArea > 100000].index, inplace = True)\ntrain_df.drop(train_df[train_df.MasVnrArea > 1250].index, inplace = True)\ntrain_df.drop(train_df[train_df.BsmtFinSF1 > 3000].index, inplace = True)\ntrain_df.drop(train_df[train_df.BsmtFinSF2 > 1250].index, inplace = True)\ntrain_df.drop(train_df[train_df.TotalBsmtSF > 4000].index, inplace = True)\ntrain_df.drop(train_df[train_df['1stFlrSF'] > 4000].index, inplace = True)\ntrain_df.drop(train_df[(train_df.LowQualFinSF > 500) & train_df.SalePrice > 600000].index, inplace = True)\ntrain_df.drop(train_df[(train_df.GrLivArea > 4000) & (train_df.SalePrice < 200000)].index, inplace = True)\ntrain_df.drop(train_df[train_df.WoodDeckSF > 800].index, inplace = True)\ntrain_df.drop(train_df[train_df.EnclosedPorch > 400].index, inplace = True)\ntrain_df.drop(train_df[train_df['3SsnPorch'] > 200].index, inplace = True)\ntrain_df.drop(train_df[(train_df.ScreenPorch > 100) & (train_df.SalePrice > 450000)].index, inplace = True)\ntrain_df.drop(train_df[train_df.PoolArea > 400].index, inplace = True)\ntrain_df.drop(train_df[train_df.MiscVal > 5000].index, inplace = True)","abe8d1ab":"# correlation = train_df.select_dtypes(exclude=object).corr()","bd84a46d":"# thresh = 0.8 # threhold currently set to 80 percent\n# plt.figure(figsize=(20,15))\n# sns.heatmap(correlation > thresh, annot=True, square=True)\n","6e3f8df2":"# correlation.loc[:, 'SalePrice'].sort_values(ascending=False)","df95501e":"# train_df.drop(columns = ['1stFlrSF', 'TotRmsAbvGrd', 'GarageYrBlt', 'GarageArea'], axis = 1, inplace = True)\n# test_df.drop(columns = ['1stFlrSF', 'TotRmsAbvGrd', 'GarageYrBlt', 'GarageArea'], axis = 1, inplace = True)\n","91578d41":"train_df.isnull().mean().sort_values(ascending = False)","45de10e4":"train_df.drop(columns = ['PoolQC', 'MiscFeature', 'Alley'], axis = 1, inplace = True)\ntest_df.drop(columns = ['PoolQC', 'MiscFeature', 'Alley'], axis = 1, inplace = True)","d3fa01c1":"train_df.loc[:, 'PoolArea'].value_counts()","7580b470":"train_df.drop(columns = ['PoolArea'], axis = 1, inplace = True)\ntest_df.drop(columns = ['PoolArea'], axis = 1, inplace = True)","f62b489f":"train_df.select_dtypes(exclude = 'object').isna().mean().sort_values(ascending = False)","af508436":"# We can see that the features LotFrontage and MasVnrArea having missing features.\n# We can replace them with their mean value using the inbuilt reputation functions from sklearn.","b40f639a":"imp = impute.SimpleImputer()\ntrain_df.LotFrontage = imp.fit_transform(np.array(train_df.LotFrontage).reshape(-1,1))\n\nimp = impute.SimpleImputer()\ntrain_df.MasVnrArea = imp.fit_transform(np.array(train_df.MasVnrArea).reshape(-1,1))\n\nimp = impute.SimpleImputer()\ntrain_df.GarageYrBlt = imp.fit_transform(np.array(train_df.GarageYrBlt).reshape(-1,1))\n\n# imp = impute.SimpleImputer()\n# train_df.MasVnrArea = imp.fit_transform(np.array(train_df.MasVnrArea).reshape(-1,1))\n\ntrain_df.select_dtypes(exclude = 'object').isna().mean().sort_values(ascending = False)\n","dc8792db":"test_df.select_dtypes(exclude = 'object').isna().mean().sort_values(ascending = False)","0bf69f01":"imp = impute.SimpleImputer()\ntest_df.LotFrontage = imp.fit_transform(np.array(test_df.LotFrontage).reshape(-1,1))\n\nimp = impute.SimpleImputer()\ntest_df.MasVnrArea = imp.fit_transform(np.array(test_df.MasVnrArea).reshape(-1,1))\n\n\ncol = ['BsmtFullBath', 'BsmtHalfBath', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageYrBlt', 'GarageArea']\nfor i in col:\n    imp = impute.SimpleImputer()\n    test_df[i] = imp.fit_transform(np.array(test_df[i]).reshape(-1,1))\n    \ntest_df.select_dtypes(exclude = 'object').isna().mean().sort_values(ascending = False)\n","6b6d53dd":"# So, we have finished dealing with all the missing values in the numerical features in both the train and test dataset.","6779af12":"features_categorical = train_df.select_dtypes(include = 'object')","b5fdbf59":"len(features_categorical.columns)","fcd322c8":"# fig = plt.figure(figsize = (20,50))\n# for i in range(len(features_categorical.columns)):\n#     fig.add_subplot(10, 4, i+1)\n#     ax = sns.countplot(features_categorical.iloc[:,i].dropna())\n#     plt.xticks(rotation = 90)\n# plt.tight_layout()","af32c285":"features_categorical.shape","301fb935":"# for categorical variables, pandas describe() method will return count, unique, top, freq\n# count = number of records\n# unique = unqiue categorical values for each column\n# top = top occuring value\n# freq = frequency of top occuring value\n\n# features_categorical.describe()\n\n\n# .T will transpose (interchage rows with columns)\n# .freq will return frequency of top value\n# sort_values will sort the values accordingly\nfeatures_categorical.describe().T.freq.sort_values(ascending = False)\n\n\n\n# Utilities 1427 -> There are 1427 values FOR top occuring value (we dont care right now what the top occuring value is, we just care for disribution)\n# if 1427 out of 1428 values for Utilities are just for one unqiue categorical value for Utilities, it means Utilities is highly skewed","9026bf95":"train_df.drop(columns = ['Utilities', 'Street', 'Condition2', 'RoofMatl'], axis = 1, inplace = True)\ntest_df.drop(columns = ['Utilities', 'Street', 'Condition2', 'RoofMatl'], axis = 1, inplace = True)","7b0a791d":"features_categorical.isnull().sum().sort_values(ascending=False).head()","8b316c24":"train_df.isnull().sum().sort_values(ascending=False).head()","bea79de2":"train_df.fillna('None', inplace = True)\n# train_df.dropna(inplace=True)","ef8fda0e":"train_df.isnull().sum().sort_values(ascending=False).head()","a9a84c65":"train_df.select_dtypes(include = 'object').isna().mean().sort_values(ascending = False)","d7401fde":"# test_df.dropna(inplace = True)\ntest_df.fillna('None', inplace = True)","2d0daec9":"test_df.select_dtypes(include = 'object').isna().mean().sort_values(ascending = False).head()","e0284a78":"train_df.shape","a7752d52":"test_df.shape","1e346261":"print(train_df.shape)\nprint(test_df.shape)","637a1b29":"train_features_categorical = train_df.select_dtypes(include='object')\ntest_features_categorical = train_df.select_dtypes(include='object')","062514e8":"from sklearn.preprocessing import LabelEncoder     ### way much more elegant than get_dummies <3\n\nle = LabelEncoder()\n\nfor col in train_features_categorical:\n    train_df.loc[:, col] = le.fit_transform(train_df.loc[:, col])\n    \n    \nfor col in test_features_categorical:\n    test_df.loc[:, col] = le.fit_transform(test_df.loc[:, col])","88eb7642":"print(train_df.shape)\nprint(test_df.shape)","c0c667c8":"train_data = train_df.iloc[:, :-1] # all but last column (SalePrice)\ntrain_targets = train_df.iloc[:, -1] # last column\n\ntest_data = test_df","313113a5":"mean = train_data.mean(axis=0)\nstd = train_data.std(axis=0)\n\ntrain_data -= mean\ntrain_data \/= std\n\ntest_data-=mean\ntest_data \/= std","632a4531":"print(train_data.shape)\nprint(train_targets.shape)\nprint(test_data.shape)","347328af":"# corr = train_df.corr()\n\n# plt.figure(figsize=(100,100))\n# sns.heatmap(corr, annot=True, cmap='RdYlGn') # not visible properly\n\n# programmatic approch\n# no_corr_cols are the columns which have almost no correlation with SalePrice, so we can drop them\n\n# filt = (corr.loc[:, 'SalePrice'] <= 0.2)&(corr.loc[:, 'SalePrice'] >= -0.2) # either less than +0.2 or greater than -0.2\n\n# no_corr_cols = train_df.loc[:, filt]\n\n# train_data = train_data.drop(no_corr_cols, axis=1)\n# test_data = test_data.drop(no_corr_cols, axis=1)","acbcf0d9":"print(train_data.shape)\nprint(test_data.shape)","072d13db":"# # https:\/\/stackoverflow.com\/a\/6769460\/10934636\n\n# from math import ceil, log\n\n# def scale_target(max_value):\n#     number = log(max_value, 10)\n#     number = ceil(number)\n#     return 10**number\n\n\n# scale_value = scale_target(np.max(train_targets))","36f62c2e":"# train_targets \/= scale_value","7cd1edc5":"# train_targets.head(2)","3f9f0e48":"!pip install keras-tuner","03f1c9b8":"# def build_model(hp):\n#     model = models.Sequential()\n#     for i in range(hp.Int('num_layers', 2, 20)):\n#         model.add(layers.Dense(units=hp.Int('units_' + str(i),\n#                                             min_value=8,\n#                                             max_value=512,\n#                                             step=32),\n#                                activation='relu'))\n#     model.add(layers.Dense(1, activation='linear'))\n#     model.compile(\n#         optimizer=keras.optimizers.Adam(\n#             hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n#         loss='mae',\n#         metrics=['mae'])\n#     return model","b63edc6f":"# from kerastuner.tuners import RandomSearch\n# from keras import layers\n# from keras import models\n# from keras import regularizers\n# import keras\n# import tensorflow as tf\n\n# tuner = RandomSearch(\n#     build_model,\n#     objective='val_mae',\n#     max_trials=5,\n#     executions_per_trial=3,\n#     directory='project',\n#     project_name='House Price Preediction')","35959476":"# tuner.search_space_summary()","37e287b4":"# tuner.search(train_data, train_targets, epochs=150,  validation_split=0.20)","87f48e6d":"# tuner.results_summary()","546d70f1":"from keras import layers\nfrom keras import models\nfrom keras import regularizers\nimport tensorflow as tf\n\n# Trial summary\n# Hyperparameters:\n# num_layers: 4\n# units_0: 392\n# units_1: 40\n# learning_rate: 0.001\n# units_2: 72\n# units_3: 104\n# units_4: 72\n# units_5: 168\n# units_6: 232\n# units_7: 328\n# units_8: 456\n# units_9: 8\n# units_10: 488\n# units_11: 424\n# units_12: 456\n# Score: 15753.533528645834\n\ndef build_model():\n    model = models.Sequential()\n\n    model.add(layers.Dense(392, activation='relu', input_shape=(train_data.shape[1],)))\n\n    model.add(layers.Dense(40, activation='relu'))\n    \n    model.add(layers.Dense(104, activation='relu'))\n    \n    model.add(layers.Dense(72, activation='relu'))\n    \n    model.add(layers.Dense(168, activation='relu'))\n    \n    model.add(layers.Dense(232, activation='relu'))\n    \n    model.add(layers.Dense(328, activation='relu'))\n    \n    \n    \n    model.add(layers.Dense(456, activation='relu'))\n    \n    model.add(layers.Dense(8, activation='relu'))\n    \n    model.add(layers.Dense(488, activation='relu'))\n    \n    model.add(layers.Dense(424, activation='relu'))\n    \n    model.add(layers.Dense(456, activation='relu'))\n    \n    \n    model.add(layers.Dense(1))\n    \n    op = tf.keras.optimizers.Adam(learning_rate=0.001)\n    \n    # COMPILING\n    model.compile(optimizer='adam', loss='mae', metrics=['mae'])\n    \n    return model","47142e80":"# model.summary()","1fe6bb25":"# COMPILING\n\n# model.compile(optimizer='adam', loss='mse', metrics=['mae'])","b152a274":"model = build_model()\n# Early Stopping\n# from keras.callbacks import EarlyStopping\n# es = EarlyStopping(monitor='val_rmse', verbose=1, patience=10)\n\n\n# FITTING\nhistory = model.fit(train_data, train_targets, epochs=12, validation_split=0.20)","035cce08":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\nmae = history.history['mae']\nval_mae = history.history['val_mae']\nepochs = range(1, len(mae) + 1)\nplt.plot(epochs, mae, 'b--', label='Training mae', linewidth=12)\nplt.plot(epochs, val_mae, 'r', label='Validation mae', linewidth=5)\nplt.title('Training and validation Root Mean Absolute Error')\nplt.xlabel('Epochs')\nplt.ylabel('MAE')\nplt.legend()\nplt.show()","70026882":"# num_epochs = es.stopped_epoch","65fc2d3f":"# num_epochs","483a7756":"# min(history.history['val_mae'])","5f17a798":"# print(history.history['val_mae'][0:4])","54250f35":"print(history.history['val_loss'][0:4])\nplt.plot(epochs, history.history['loss'], linewidth=12, label='loss')\nplt.plot(epochs, history.history['val_loss'], linewidth=5, label='validation loss')\nplt.legend()\nplt.show()","6f18fddb":"# model = build_model()\n\n# # FITTING\n# history = model.fit(train_data, train_targets, epochs=150, batch_size=16, verbose=0)\n\n# Generating test predictions\npreds = model.predict(test_data)","aa1d10d9":"preds = preds.reshape(1459) # ValueError: Data must be 1-dimensional","5c81121e":"# preds *= scale_value","ac82d7a2":"# preds[0:5]","72aa767f":"test_data.index = submissions_index\n\n# Save predictions in format used for competition scoring\noutput = pd.DataFrame({'Id': test_data.index,\n                       'SalePrice': preds})\noutput.to_csv('submission.csv', index=False)","56d487f7":"!pip","a97e9b8e":"## Splitting Data","defc37d4":"# **Data Preprocessing**","46935101":"# Checking for missing values","2fb54e75":"# EDA Ends Here","9a33ee67":"# Final Model --- Trying to submit predictions","1f42d757":"####### Dropping Feautures - NOT DROPPING ","3f42d6a3":"# Removing Outliers","70037ab4":"# Learning Curves","2712aacd":"From the plot, we can see that the following features are highly correlated with each other:\n- 1stFlrSF & TotalBsmtSF\n- TotRmsAbvGrd & GrdLivArea\n- GarageYrBlt & YearBuilt\n- GarageArea & GarageCars\n\n**Removing any one feature in each of these four sets would be sufficient.** \n\nWe can decide on which one to remove by looking into its contribution to the target variable SalePrice. \n\nThis can be done by checking its correlation with SalePrice.","1cac7f8a":"Now, numerical features have no missing values\n- only missing values in train and test dfs are now from categorical variable\n","6a6b55f0":"As shown above, the features \u2018Utilities\u2019, \u2018Street\u2019, \u2018Condition2\u2019, \u2018RoofMatl\u2019, \u2018Heating\u2019 are highly skewed (since they are having a single entry around 1400 times out of 1428 examples). \n- It is better we remove those features.\n- remove all the features having more than 1400 missing features as the total traing examples is 1428","71695023":"## Normalization","d62dcc17":"# Categorical Features","1304e378":"## **Checking for testing data**","781ab8e5":"- We can also plot featuers with target variable for bivariate analysis to see how each feature is related to the prediction target","733e140b":"# Compiling Model","2bf97bd5":"# **Data Insights**","2d07ef02":"# Correlation\n## Checking correlation of each column in training data with prediction output (SalePrice) and removing columns which have almost no (~0) correlation","7d51b71b":"# Univariate Distribution - Box Plots","52c612b8":"# Keras Tuner","911af037":"* # --- NEW STUFF STARTS HERE---","2747101a":"As we can see the feature PoolArea is pretty much unimportant as it is reporting pool area for all the training examples as zero, therefore we can drop it as well.","6c7a4d8f":"- We could plot the scatterplots once again to see if any more useless features.\n","eece4ff0":"# Removing columns with >= 90% missing values","f61a8179":"######## Removing highly correlated features --- NOT REMOVING","2868cdc4":"- First, looking at distributions","bc28b2c0":"- We can\u2019t decide upon the outliers on distribution plots alone.\n- Box plots will give more information related to missing values","76663231":"# Filling missing values for categorical columns ------ this might be the reason for errors later ----","ae1e1c83":"## taking a closer look at these features by plotting regression plots","14555e11":"- we have to create the features_categorical dataframe again, because we dropped some numerical columns in the above explorartory analysis","6bf3fcde":"-  lets see how data in numerical features are distributed by making distplot for each column\n-  these plots give information about : 1. Skewness of data   2. Outliers (if any)","f771af33":"- drop the features (among the pairs shown above) having less correlation with the target variable.","8a7c54de":"# Bivariate Analysis","77b3ad06":"# Scaling Prediction Target","802d27ca":"## One Hot Encoding ","a5f17eff":"![image.png](attachment:image.png)","6de1b984":"# Building Model","71bc5492":"# Preparing Data for Neural Network","fb6423e0":"# Filling missing values - Numerical Features","8650c360":"# Univariate Distribution plots","643a394d":"# Inverse Scaling","78f8932d":"# Plotting missing values","79d4c3e3":"# Model Summary","7990d3b3":"- Filling missing values with mean values of all columns\n- Missing values in test dataset are also imputed by mean of *train datset* as we dont want the model to know anything about test dataset","f3d4b503":"# Feature segregation","c7abbede":"- features with high correlation can be dropped to avoid overfitting\n- Multiple features having a high correlation with each other may cause over-fitting.\n- high correlation among input features -> drop\n- high correlation of a feature with prediction target -> Don't drop","17777779":"## outliers are removed only from training data because we cant most likely fight with outliers\n## real world predictions after deploying model\nhttps:\/\/www.quora.com\/Should-we-exclude-outliers-form-testing-set","9913211b":"# Fitting Model --- Setting up EarlyStopping"}}