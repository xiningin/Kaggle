{"cell_type":{"c11a1a82":"code","ff17e2d5":"code","65dda738":"code","e9457ba6":"code","7d409382":"code","df78f6d1":"code","409ee7a8":"code","8d7c2eb7":"code","a3164c8b":"code","14bb711a":"code","39942cae":"code","1171822e":"code","89f46ed7":"code","8231f3ef":"code","81b5bb7c":"code","48f2f213":"code","1ce4436f":"markdown","65b54404":"markdown","793ab26d":"markdown","c1a62a3b":"markdown","367603e3":"markdown","7f298ae8":"markdown","7c41cd0d":"markdown","be2a77dc":"markdown"},"source":{"c11a1a82":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ff17e2d5":"!cp -r '..\/input\/keras-bert-by-cyberzhg-github\/keras_bert\/keras_bert' '\/kaggle\/working'\n!cp -r '..\/input\/keras-bert-by-cyberzhg-github\/keras_bert\/keras_pos_embd' '\/kaggle\/working'\n!cp -r '..\/input\/keras-bert-by-cyberzhg-github\/keras_bert\/keras_embed_sim' '\/kaggle\/working'\n!cp -r '..\/input\/keras-bert-by-cyberzhg-github\/keras_bert\/keras_position_wise_feed_forward' '\/kaggle\/working'\n!cp -r '..\/input\/keras-bert-by-cyberzhg-github\/keras_bert\/keras_layer_normalization' '\/kaggle\/working'\n!cp -r '..\/input\/keras-bert-by-cyberzhg-github\/keras_bert\/keras_self_attention' '\/kaggle\/working'\n!cp -r '..\/input\/keras-bert-by-cyberzhg-github\/keras_bert\/keras_multi_head' '\/kaggle\/working'\n!cp -r '..\/input\/keras-bert-by-cyberzhg-github\/keras_bert\/keras_transformer' '\/kaggle\/working'","65dda738":"import tensorflow as tf\nimport keras as keras\nimport keras.backend as K\nfrom keras.models import load_model\n\nfrom keras_bert import load_trained_model_from_checkpoint, load_vocabulary\nfrom keras_bert import Tokenizer\nfrom keras_bert import AdamWarmup, calc_train_steps\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport gc","e9457ba6":"SEQ_LEN = 64\nBATCH_SIZE = 128\nEPOCHS = 1\nLR = 1e-4\n\npretrained_path = '..\/input\/pretrained-bert-including-scripts\/uncased_l-12_h-768_a-12\/uncased_L-12_H-768_A-12'\nconfig_path = os.path.join(pretrained_path, 'bert_config.json')\ncheckpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\nvocab_path = os.path.join(pretrained_path, 'vocab.txt')\n\nDATA_COLUMN = 'comment_text'\nLABEL_COLUMN = 'target'","7d409382":"token_dict = load_vocabulary(vocab_path)\ntokenizer = Tokenizer(token_dict)","df78f6d1":"def convert_data(data_df):\n    global tokenizer\n    indices, targets = [], []\n    for i in tqdm(range(len(data_df))):\n        ids, segments = tokenizer.encode(data_df[DATA_COLUMN][i], max_len=SEQ_LEN)\n        indices.append(ids)\n        targets.append(data_df[LABEL_COLUMN][i])\n    items = list(zip(indices, targets))\n    np.random.shuffle(items)\n    indices, targets = zip(*items)\n    indices = np.array(indices)\n    return [indices, np.zeros_like(indices)], np.array(targets)","409ee7a8":"def load_data(path):\n    data_df = pd.read_csv(path, nrows=10000)\n    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n    data_x, data_y = convert_data(data_df)\n    return data_x, data_y","8d7c2eb7":"train_x, train_y = load_data('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\ngc.collect()","a3164c8b":"model = load_trained_model_from_checkpoint(\n    config_path,\n    checkpoint_path,\n    training=True,\n    trainable=True,\n    seq_len=SEQ_LEN,\n)","14bb711a":"inputs = model.inputs[:2]\ndense = model.layers[-3].output\noutputs = keras.layers.Dense(1, activation='sigmoid', kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n                             name = 'real_output')(dense)\n\ndecay_steps, warmup_steps = calc_train_steps(\n    train_y.shape[0],\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n)\n\nmodel = keras.models.Model(inputs, outputs)\nmodel.compile(\n    AdamWarmup(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=LR),\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)","39942cae":"model.summary()","1171822e":"sess = K.get_session()\nuninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.report_uninitialized_variables())])\ninit_op = tf.variables_initializer(\n    [v for v in tf.global_variables() if v.name.split(':')[0] in uninitialized_variables]\n)\nsess.run(init_op)","89f46ed7":"model.fit(\n        train_x,\n        train_y,\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n    )","8231f3ef":"def convert_test(test_df):\n    global tokenizer\n    indices = []\n    for i in tqdm(range(len(test_df))):\n        ids, segments = tokenizer.encode(test_df[DATA_COLUMN][i], max_len=SEQ_LEN)\n        indices.append(ids)\n    indices = np.array(indices)\n    return [indices, np.zeros_like(indices)]\n\ndef load_test(path):\n    data_df = pd.read_csv(path, nrows=5000)\n    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n    data_x = convert_test(data_df)\n    return data_x","81b5bb7c":"test_x = load_test('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\ngc.collect()","48f2f213":"prediction = model.predict(test_x)\nsubmission = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv', index_col='id', nrows=5000)\nsubmission['prediction'] = prediction\nsubmission.reset_index(drop=False, inplace=True)\nsubmission.to_csv('submission.csv', index=False)","1ce4436f":"**Predict and Submit**","65b54404":"**Create Keras Model and compile it**","793ab26d":"**Copy All the files to working directory**","c1a62a3b":"**Get Tokenizer**","367603e3":"**Introduction**\n\nHello All,\n\nI'm a beginner in NLP and machine learning that I have recently started to explore. I have learned Keras and tensorflow and I wanted to impliment a bert kernal in keras, so I found out this github repository https:\/\/github.com\/CyberZHG\/keras-bert which is also mentioned in https:\/\/www.kaggle.com\/httpwwwfszyc\/bert-keras-with-warmup-and-excluding-wd-parameters\/notebook, I packed this in my dataset https:\/\/www.kaggle.com\/gauravs90\/keras-bert-by-cyberzhg-github, I have to create this dataset as existing ones did not include the tokenizer and AdamWarmup.\n\nThanks for Jon Mischo (https:\/\/www.kaggle.com\/supertaz) for uploading BERT Models + Scripts :)","7f298ae8":"**Load the test data**","7c41cd0d":"**Load Model from Checkpoint**","be2a77dc":"**Load and Convert to data that BERT understand**"}}