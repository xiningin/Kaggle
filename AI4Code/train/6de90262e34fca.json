{"cell_type":{"17a68e85":"code","5e9a3851":"code","572a49cf":"code","791c1f7d":"code","75253ba1":"code","d1f53844":"code","567f03c9":"code","1aa08548":"code","14ae3545":"code","8569b20e":"code","812c59aa":"code","cc8e12bd":"code","b26bf35c":"code","d221e061":"code","7242081a":"code","c1436d9a":"code","2ff4bac7":"code","856016f1":"code","17542089":"code","ef5a8b60":"code","fcb26823":"code","1bfcc933":"code","e862ea55":"code","461369d5":"code","c3f2fc2e":"code","9813d43c":"code","f8da5e24":"code","cf15d906":"code","e5e41fb5":"code","997b49b1":"code","5d0d398f":"code","cad6e4e3":"code","370bb276":"code","94d094bb":"code","b96abbf8":"code","c2861523":"code","a211cd12":"code","be3aeb34":"code","fcf061f2":"code","8ecdc697":"code","516103b0":"code","629936b9":"code","53df87bb":"code","f541ee6d":"markdown","c4b245ea":"markdown","42d2b37c":"markdown","d8144447":"markdown","ee2d75f4":"markdown","76297eb6":"markdown","852ab121":"markdown","2f58dbcf":"markdown","88619c16":"markdown","f58103b8":"markdown","4adc7a5e":"markdown","cfda4fbb":"markdown","d0fe573d":"markdown","9051e863":"markdown","1ea97cc8":"markdown","4a863dfa":"markdown","7b64e8aa":"markdown"},"source":{"17a68e85":"# IMporta\u00e7\u00e3o das bibliotecas basicas\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\n# Abaixo listamos os arquivos da base\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","5e9a3851":"#Leitura do arquivo\ndf = pd.read_csv('\/kaggle\/input\/hmeq-data\/hmeq.csv')","572a49cf":"# Abaixo poemos veriricar o conte\u00fado da base\ndf","791c1f7d":"#Verificamos agora os tipos de dados\ndf.info()","75253ba1":"# Verificando os valores nulos\ndf.isnull().sum()","d1f53844":"#Imputa\u00e7\u00e3o de dados nas colunas\ndf.loc[df['MORTDUE'].isnull(),'MORTDUE'] = 0\ndf.loc[df['VALUE'].isnull(),'VALUE'] = 0\ndf.loc[df['JOB'].isnull(),'JOB'] = 'None'\ndf.loc[df['REASON'].isnull(),'REASON'] = 'Other'\ndf.loc[df['YOJ'].isnull(),'YOJ'] = 0\ndf.loc[df['DEROG'].isnull(),'DEROG'] = 0\ndf.loc[df['DELINQ'].isnull(),'DELINQ'] = 0\ndf.loc[df['CLAGE'].isnull(),'CLAGE'] = 0\ndf.loc[df['NINQ'].isnull(),'NINQ'] = 0\ndf.loc[df['CLNO'].isnull(),'CLNO'] = 0\ndf.loc[df['DEBTINC'].isnull(),'DEBTINC'] = 0","567f03c9":"#Listagem das classes de JOB\ndf['JOB'].unique()","1aa08548":"#Listagem das classes de REASON\ndf['REASON'].unique()","14ae3545":"#Defini\u00e7\u00e3o da fun\u00e7\u00e3o para transformar a informa\u00e7\u00e3o textual da coluna REASON em codificacao numerica\ndef REASONN (row):\n   if row['JOB'] == 'Other':\n      return 0\n   if row['JOB'] == 'HomeImp':\n      return 1\n   if row['JOB'] == 'DebtCon':\n      return 2\n   return 3\ndf['REASONN'] = df.apply (lambda row: REASONN(row), axis=1)","8569b20e":"#Defini\u00e7\u00e3o da fun\u00e7\u00e3o para transformar informar textual da coluna JOB em codificacao numerica\ndef JOBN (row):\n   if row['JOB'] == 'Other':\n      return 0\n   if row['JOB'] == 'Office':\n      return 1\n   if row['JOB'] == 'Sales':\n      return 2\n   if row['JOB'] == 'Mgr':\n      return 3\n   if row['JOB'] == 'ProfExe':\n      return 4\n   if row['JOB'] == 'Self':\n      return 5\n   return 6\ndf['JOBN'] = df.apply (lambda row: JOBN(row), axis=1)","812c59aa":"#Contagem de valores nulos na base\ndf.isnull().sum()","cc8e12bd":"# Separando as colunas para a constru\u00e7\u00e3o do modelo\nfeats = [c for c in df.columns if c not in ['BAD','REASON','JOB']]","b26bf35c":"#Conteudo da base de dados\ndf.T","d221e061":"#Dividindo a base em treino e teste\nfrom sklearn.model_selection import train_test_split\n\ntrain,test = train_test_split(df, test_size=0.20, random_state=42)\n\ntrain.shape,valid.shape,test.shape","7242081a":"# Instanciando o random forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_jobs=-1, n_estimators=200, oob_score=True, random_state=42)","c1436d9a":"#Aplicando o RandomForest \u00e0 base\nrf.fit(train[feats], train['BAD'])","2ff4bac7":"# Prevendo o BAD de teste usando o modelo treinado\ny_test_pred = rf.predict(test[feats]).astype(int)","856016f1":"#Importa\u00e7\u00e3o do pacote para aferi\u00e7\u00e3o d acuracia\nfrom sklearn.metrics import accuracy_score","17542089":"# Medida da acur\u00e1cia\naccuracy_score(test['BAD'], y_test_pred)","ef5a8b60":"#Vamos usar agora um m\u00e9todo de medida mais confi\u00e1vel - AUROC - Area sob a curva ROC\n#Geramos as probabilidades das classes na previs\u00e3o (necess\u00e1rio para a rotina de medida AUROC)\ny_test_prob = rf.predict_proba(test[feats])","fcb26823":"#Pega so uma coluna para efetuar o teste\ny_test_prob = [p[1] for p in y_test_prob]","1bfcc933":"#Importando o pacote para a medida da acuracia AUROC\nfrom sklearn.metrics import roc_auc_score","e862ea55":"#Medida da acur\u00e1cia  usando a area sob a curva ROC - AUROC\nroc_auc_score(test['BAD'], y_test_prob) ","461369d5":"#Importa\u00e7\u00e3o de paaotes para plotagem de graficos\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt","c3f2fc2e":"#Curva ROC para os dados originais\n\n\nfpr, tpr, threshold = metrics.roc_curve(test['BAD'], y_test_prob)\nroc_auc = metrics.auc(fpr, tpr)\n\n# method I: plt\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n\n#plt.plot(np.linspace(0,1,10), np.linspace(0,1,10), label=\"diagonal\")\n    \nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","9813d43c":"#Prevendo os dados para a matriz de confus\u00e3o\ncnf_matrix = metrics.confusion_matrix(test['BAD'], y_test_pred)\ncnf_matrix","f8da5e24":"# import required modules\n# is scikit's classifier.predict() using 0.5 by default?\n\n#In probabilistic classifiers, yes. It's the only sensible threshold from a mathematical viewpoint, as others have explained.\nimport seaborn as sns \n\n%matplotlib inline\nfig, ax = plt.subplots()\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Matriz Confus\u00e3o', y=1.1)\nplt.ylabel('Real')\nplt.xlabel('Predito')","cf15d906":"#Por ultimo geramos o relatorio com medidas da qualidade das predi\u00e7\u00f5es\nclassific = metrics.classification_report(test['BAD'], y_test_pred)","e5e41fb5":"print(classific)","997b49b1":"#Importa\u00e7\u00e3o da biblioteca para reamostragem\nfrom imblearn.over_sampling import SMOTE, ADASYN   #reamostragem com a rotina SMOTE","5d0d398f":"X_resampled, y_resampled = SMOTE().fit_resample(train[feats], train['BAD'])","cad6e4e3":"y_resampled.value_counts()","370bb276":"\nrf2 = RandomForestClassifier(n_jobs=-1, n_estimators=200, oob_score=True, random_state=42)","94d094bb":"#Aplicando o RandomForest \u00e0 base\nrf2.fit(X_resampled, y_resampled)","b96abbf8":"# Prevendo o BAD para a base com reamostragem SMOTE\ny_test_pred_2 = rf2.predict(test[feats]).astype(int)","c2861523":"# Medida da acur\u00e1cia\naccuracy_score(test['BAD'], y_test_pred_2)","a211cd12":"#Vamos usar agora um m\u00e9todo de medida mais confi\u00e1vel - AUROC - Area sob a curva ROC\n#Geramos as probabilidades das classes na previs\u00e3o (necess\u00e1rio para a rotina de medida AUROC)\ny_test_prob_2 = rf2.predict_proba(test[feats])","be3aeb34":"#Pega so uma coluna para efetuar o teste\ny_test_prob_2 = [p[1] for p in y_test_prob_2]","fcf061f2":"#Medida da acur\u00e1cia  usando a area sob a curva ROC - AUROC\nroc_auc_score(test['BAD'], y_test_prob_2) ","8ecdc697":"#Curva ROC para os dados reamostrados\n\n\nfpr, tpr, threshold = metrics.roc_curve(test['BAD'], y_test_prob_2)\nroc_auc = metrics.auc(fpr, tpr)\n\n# method I: plt\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n\n#plt.plot(np.linspace(0,1,10), np.linspace(0,1,10), label=\"diagonal\")\n    \nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","516103b0":"#Prevendo os dados para a matriz de confus\u00e3o\ncnf_matrix = metrics.confusion_matrix(test['BAD'], y_test_pred_2)\ncnf_matrix","629936b9":"%matplotlib inline\nfig, ax = plt.subplots()\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Matriz Confus\u00e3o', y=1.1)\nplt.ylabel('Real')\nplt.xlabel('Predito')","53df87bb":"#Geramos o relatorio com medidas da qualidade das predi\u00e7\u00f5es\nclassific = metrics.classification_report(test['BAD'], y_test_pred_2)\nprint(classific)","f541ee6d":"Podemos veriricar que a base se encontra livre de espa\u00e7os vazios. \nVamos agora excluir as colunas que n\u00e3o ser\u00e3o utilzadas em nosso modelo.\nA variavel resposta - BAD - tamb\u00e9m ser\u00e1 removida porque iremos separar as vari\u00e1veis de entrada, ou seja, as variaveis independentes.\nUtilizaremos as vari\u00e1veis numericas e excluiremos as variaveis com innforma\u00e7\u00e3o textual.","c4b245ea":"Pudemos observar que o nosso modelo comporta-se muito bem pois obteve uma medida de acur\u00e1cia elevada.\nPara corroborar os nossos resultdos \u00e9 conveniente utiliar tamb\u00e9m uma outra medida, a da \u00e1rea sob a curva ROC.","42d2b37c":"Para completar a nossa an\u00e1lise, construiremos agora a nossa matriz confus\u00e3o, ela tamb\u00e9m permite aferir a qualidade da resposta do modelo.","d8144447":"Ao comparar os grafico percebemos algumas pequenas diferencas. A linha da curva se afasta mais cedo do eixo vertical nesse ultimo gr\u00e1fico, por\u00e9m a curva se aproxima mais rapidamente do limite superior.\nAS \u00e1rea da curva \u00e9 ligeiramente superior.","ee2d75f4":"A medida baseada na area da curva ROC tamb\u00e9m melhorou.","76297eb6":"Observamos que a base cont\u00e9m muitos valores nulos. Para que possamos prosseguir com o nosso trabalho de construir um modelo \u00e9 necess\u00e1rio corrigir a base preenchendo as informa\u00e7\u00f5es faltantes.\n\nAbaixoi a defini\u00e7\u00e3o dos valores a serem inseridos para cada vari\u00e1vel:\n\nMORTDUE-  Montante devido na hipoteca existente - Assumido como 0\nVALUE - Valor da propriedade atual -  Assumido como 0\nREASON DebtCon = consolida\u00e7\u00e3o da d\u00edvida HomeImp = melhoria da casa - 'Other' (Outro)\nJOB Categorias profissionais JOBSix = 'None' (Nenhum)\nYOJ Anos no emprego atual - Assumido 0\nDEROG N\u00famero de principais relat\u00f3rios depreciativos - Assumido como 0\nDELINQ N\u00famero de linhas de cr\u00e9dito inadimplentes - Assumido como 0\nCLAGE Idade da linha comercial mais antiga em meses - Assumido como 0\nNINQ N\u00famero de linhas de cr\u00e9dito recentes - Assumido como 0\nCLNO N\u00famero de linhas de cr\u00e9dito - Assumido como 0\nDEBTINC Raz\u00e3o D\u00edvida \/ Renda - Assumido como 0\n","852ab121":"# Trabalho final da disciplina Data Mining e Machine Learning II\n## Prof: Marcos Guimar\u00e3es\n## Aluno: Walter Soares Malta","2f58dbcf":"A medida de acur\u00e1cia calculada com base na \u00e1rea sob a curva ROC obteve um resultado melhor ainda.\nAbaixo podemos verificar graficamente o formato da curva ROC que explica esse valor de acur\u00e1cia elevada.","88619c16":"No relat\u00f3rio abaixo temos mais algumas informa\u00e7\u00f5es sobre o nosso modelo.\n\nO relat\u00f3rio mostra as principais m\u00e9tricas de classifica\u00e7\u00e3o com base nas classes de resposta. Ele d\u00e1 uma percep\u00e7\u00e3o mais global sobre a acur\u00e1cia do modelo e dexa mais claro se ele tem uma performance menor para classes minorit\u00e1rias.","f58103b8":"# Conclus\u00e3o\n\n\n\nO modelo construido utilizando RandomForest mostrou-se bastate eficaz para o caso. De fato conseguimos obter uma grande acur\u00e1cia, seja pelo m\u00e9todo comum, seja pelo m\u00e9todo da \u00e1rea da curva ROC, indicado para classes desbalanceadas.\nPercebemos que, ainda que pequena, a reamostragem efetuada utilzando o m\u00e9todo SMOTE, que insere linhas com dados caculados por interpola\u00e7\u00e3o, provocou algumas mudan\u00e7as na performance do modelo.\nA acur\u00e1cia pelo m\u00e9todo comum que verifica a raz\u00e3o de acertos teve uma ligeira melhora. Entretanto, a \u00e1rea sob a curva ROC foi ligeiramente menor. Ao observar os resultados do relat\u00f3rio de classificao, percebemos que caso se queira um maior rigor na concess\u00e3o de cr\u00e9dito, o segundo modelo se mostra mais adequado, pois a medida \"precision\" \u00e9 ligeiramente superior, 0.86 contra 0.82, para o caso de clientes com historico ruim quanto ao cr\u00e9dito, o que indica que ele melhor prev\u00ea esses casos. O lado negativo \u00e9 que a previsibilidade de bons clientes piora, 0.92 contra 0.94 do modelo sem reamostragem, o que pode levar a perda de oportunidde de oferecer credito a alguns clientes bons pagadores.\nPor fim, concluimos que os resultados obtidos com o nosso modelo viabilizam a sua utiliza\u00e7\u00e3o como medida para disponibiliza\u00e7\u00e3o de credito ao cliente.","4adc7a5e":"O objeto de nossa an\u00e1lise consiste em verificar a base de dados de clientes com o prop\u00f3sito de automatizar o processo de tomada de decis\u00e3o para aprova\u00e7\u00e3o das linhas de cr\u00e9dito. Torna-se necess\u00e1rio criar um modelo de pontua\u00e7\u00e3o de cr\u00e9dito baseado em dados coletados de solicitantes recentes de cr\u00e9dito.\n\nUtilizaremos ferramentas de modelagem preditiva, entretnato o modelo dever\u00e1 ser interpret\u00e1vel, de forma a permitir que se explique de forma comprovada eobjetiva e se tenha subsidios para uma rejei\u00e7\u00e3o ao cr\u00e9dito.\n\nA base de dados de Home Equity (HMEQ) cont\u00e9m informa\u00e7\u00f5es de desempenho de empr\u00e9stimos para 5.960 clientes. A nossa vari\u00e1vel resposta (BAD) \u00e9 uma vari\u00e1vel bin\u00e1ria que indica se o requerente \u00e9 inadimplente ou n\u00e3o. Casos de clientes inadimplentes ocorreram em 1.189 casos (20%). Para cada cliente, foram registradas 12 vari\u00e1veis descritas abaixo:\n\n* BAD 1 = cliente inadimplente no empr\u00e9stimo 0 = empr\u00e9stimo reembolsado\n* LOAN Montante do pedido de empr\u00e9stimo\n* MORTDUE Montante devido na hipoteca existente\n* VALUE Valor da propriedade atual\n* REASON DebtCon = consolida\u00e7\u00e3o da d\u00edvida HomeImp = melhoria da casa\n* JOB Categorias profissionais JOBSix\n* YOJ Anos no emprego atual\n* DEROG N\u00famero de principais relat\u00f3rios depreciativos\n* DELINQ N\u00famero de linhas de cr\u00e9dito inadimplentes\n* CLAGE Idade da linha comercial mais antiga em meses\n* NINQ N\u00famero de linhas de cr\u00e9dito recentes\n* CLNO N\u00famero de linhas de cr\u00e9dito\n* DEBTINC Raz\u00e3o D\u00edvida \/ Renda\n","cfda4fbb":"## Introdu\u00e7\u00e3o","d0fe573d":"Podemos observar uma melhora na medida de acuracia.","9051e863":"Novamente os nossos resultados se mostraram satisfat\u00f3rios. A medida \"support\" indica um desbalanceamento.\nAbaixo efetuaremos uma ramostrgem utilziando a rotina SMOTE, que efetua uma interpola\u00e7\u00e3o para inserir novas linha d eforma a igualar as classes.","1ea97cc8":"Abaixo descrevemos as medidas do relat\u00f3rio:\n\nPrecision - \u00e9 a habilidade do modelo em n\u00e3o prever um resultado positivo se o valor \u00e9 realmente negativo. Em cada classe ele \u00e9 definido como a raz\u00e3o entre os poditivos verdadeiros e a soma dos positivos verdadeiros e falsos.\n\nRecall - \u00e9 a habilidade do classificador encontrar todas as instancias positivas. Em cada classe \u00e9 definido como a raz\u00e3o entre os postivos verddeiros e asoma dos verdadeiros positivos e falsos negativos.\n\nF1 score - \u00e9 a medida harmonica de precision e recall tal que o melhor score \u00e9 1.0 e o pior 0.0. Normalmente, F1 score \u00e9 menor que a acuracia medida pois combina precision e recall. \u00c9 util para comparar modelos, n\u00e3o como medida global de acuracia.\n\nSupport - n\u00famero de ocorrencias da classe na base. Desbalanceamento no treinamento da bse pode levar a fraquezas no modelo n\u00e3o reveladas pelo score de acuracia, o que pode justificar uma amostragem estratificada, igualando a propor\u00e7\u00e3o de classes  e rebalanceamento. O suporte n\u00e3o muda com o modelo mas d\u00e1 um diagnostico do processo de avalia\u00e7\u00e3o do processo.","4a863dfa":"Percebemos que a reamostragem n\u00e3o efetuou grandes mudan\u00e7as no modelo mas podemos perceber algumas diferen\u00e7as.\nA quantidade de negativos verdadeiros, celula em azul, teve uma ligeira queda, mas compesnada pelo aumento de verdadeiros positivos.","7b64e8aa":"## Desenvolvimento"}}