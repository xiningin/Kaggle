{"cell_type":{"19e276ff":"code","30e6dbf5":"code","1274bfad":"code","c8763c18":"code","28f019be":"code","2f4c31a9":"code","c71164e1":"code","f71786ff":"code","4ccc5f7c":"code","7509e222":"code","141ac96f":"code","cd7332ca":"code","4848d7ed":"code","7b705f4f":"code","5bcbb7d5":"code","4e115122":"code","a84a1043":"code","a03ebc75":"code","7833216d":"code","4988df51":"code","488207bd":"code","276803d4":"code","a58f1fb3":"code","98125808":"code","e26c71f2":"code","fd466863":"code","55f01480":"code","3ef1aedf":"code","e3d13548":"code","86eb73af":"code","a27ef568":"code","2caf0baa":"code","0a97f20a":"code","e717683c":"code","883e3f34":"code","59268f30":"code","957bc51f":"code","5cd479af":"code","5cfbbba9":"code","a206220b":"code","8e875e5a":"code","1eb45624":"code","72d65c39":"code","29716ebc":"code","dfebeed7":"code","82dce6a0":"markdown","939eefed":"markdown","d4feb0c0":"markdown","3ec3c4e7":"markdown","a3f8d03e":"markdown","785b48a2":"markdown","7e94f7c1":"markdown","c464f03e":"markdown","fc4577ee":"markdown","133a1fe0":"markdown","2ef2d91c":"markdown","9b9be6ae":"markdown","650e1dc1":"markdown","18c6a1af":"markdown","b80022ee":"markdown","69aec5d4":"markdown","1511dcd8":"markdown","b7c7fcd1":"markdown","1e06e2e1":"markdown","1ef99378":"markdown","191d672d":"markdown","fcf3e1ec":"markdown","2c3641a4":"markdown","0a233515":"markdown","f72f3750":"markdown","f43e421c":"markdown","f36fe6a6":"markdown","a4e94500":"markdown","04d0f130":"markdown","ff86e166":"markdown","58a95053":"markdown","80250509":"markdown","186113c3":"markdown","57a41bae":"markdown"},"source":{"19e276ff":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nimport random\nimport itertools\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport tensorflow as tf\nfrom tensorflow.keras import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.optimizers import *\nfrom keras.utils import to_categorical\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report","30e6dbf5":"def import_image_to_array(directory_path, size):\n    \n    \"\"\"\n    Argument:\n        directory_path - The path to the directory of the images to be imported.\n        size - a tuple that describes the desired size to be imported \n    \n    Task(s) this function carry out:\n        Import images from the specified dirctories and convert them to arrays\/tensors\n    \n    Returns:\n        image_arrays - arrays of the images imported\n    \n    \"\"\"\n    # Define empty lists to store the images array and their respective labels\n    image_arrays = []\n    \n    directory = os.listdir(directory_path)\n\n    for image_name in directory:\n        \n        # Load an image from the specified directory\n        image = cv2.imread(directory_path + image_name)\n            \n        # Resize image to the specified size\n        image = cv2.resize(image, size)\n\n        # Update the image dataset and labels lists respectively\n        image_arrays.append(image)\n            \n    return image_arrays","1274bfad":"NORMAL_DIR = \"\/kaggle\/input\/covid-cxr-image-dataset-research\/COVID_IEEE\/normal\/\"  # Normal CXR Image Dataset\nVIRAL_DIR = \"\/kaggle\/input\/covid-cxr-image-dataset-research\/COVID_IEEE\/virus\/\"    # Viral Pneumonia CXR Image Dataset\nCOVID_DIR = \"\/kaggle\/input\/covid-cxr-image-dataset-research\/COVID_IEEE\/covid\/\"    # COVID-19 CXR Image Dataset\n\nnormal_dataset = import_image_to_array(NORMAL_DIR, (224,224))\nviral_dataset = import_image_to_array(VIRAL_DIR, (224,224))\ncovid_dataset = import_image_to_array(COVID_DIR, (224,224))","c8763c18":"print(\"The Dataset which is made up of {} Image Arrays has: \".format(len(normal_dataset) + len(viral_dataset) + len(covid_dataset)))\nprint('{} Normal CXR Image Arrays'.format(len(normal_dataset)))\nprint('{} Viral Pneumonia CXR Image Arrays'.format(len(viral_dataset)))\nprint('{} COVID-19 CXR Image Arrays'.format(len(covid_dataset)))\n","28f019be":"class_names = {0: \"Normal CXR\", 1 : \"Viral Pneumonia CXR\", 2: \"COVID-19 CXR\"}\n\n# Normal CXR Sampling\nplt.figure(figsize=(10, 10))\nfor images in range(3):\n    ax = plt.subplot(3, 3, images + 1)\n    plt.imshow(normal_dataset[images], cmap = \"gray\")\n    plt.title(class_names[0])\n    plt.axis(\"off\")\n    \n# Viral Pneumonia CXR Sampling\nplt.figure(figsize=(10, 10))\nfor images in range(3):\n    ax = plt.subplot(3, 3, images + 1)\n    plt.imshow(viral_dataset[images], cmap = \"gray\")\n    plt.title(class_names[1])\n    plt.axis(\"off\")\n    \n# COVID-19 CXR Sampling\nplt.figure(figsize=(10, 10))\nfor images in range(3):\n    ax = plt.subplot(3, 3, images + 1)\n    plt.imshow(covid_dataset[images], cmap = \"gray\")\n    plt.title(class_names[2])\n    plt.axis(\"off\")","2f4c31a9":"def white_balance(channel, perc = 0.05):\n    mi, ma = (np.percentile(channel, perc), np.percentile(channel,100.0-perc))\n    channel = np.uint8(np.clip((channel-mi)*255.0\/(ma-mi), 0, 255))\n    return channel","c71164e1":"def clahe():\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n    return clahe","f71786ff":"def image_enhancer(image_arrays):\n    \n    \"\"\"\n    Argument: \n    image_arrays: a list containing arrays of images to be normalized\n    \n    returns:\n    enhanced_images: a list of the enhanced images arrays\n    \"\"\"\n    enchanced_images = []\n    \n    for image in image_arrays:\n        \n        # White Balance\n        image_WB  = np.dstack([white_balance(channel, 0.05) for channel in cv2.split(image)] )\n        gray_image = cv2.cvtColor(image_WB, cv2.COLOR_RGB2GRAY)\n\n        # CLAHE\n        clahe_function = clahe()\n        image_clahe = clahe_function.apply(gray_image)\n        image = cv2.cvtColor(image_clahe, cv2.COLOR_GRAY2RGB)\n        \n        enchanced_images.append(image)\n    \n    return enchanced_images","4ccc5f7c":"normal_dataset_enhanced = image_enhancer(normal_dataset) \nviral_dataset_enhanced = image_enhancer(viral_dataset)\ncovid_dataset_enhanced = image_enhancer(covid_dataset)","7509e222":"# Normal CXR Sampling\nplt.figure(figsize=(10, 10))\nax = plt.subplot(2, 2, 1)\nplt.imshow(normal_dataset[100], cmap = \"gray\")\nplt.title(\"Normal CXR - Before Image Enhancement\")\nplt.axis(\"off\")\n\nax = plt.subplot(2, 2, 2)\nplt.imshow(normal_dataset_enhanced[100], cmap = \"gray\")\nplt.title(\"Normal CXR - After Image Enhancement\")\nplt.axis(\"off\")\n    \n# Viral Pneumonia CXR Sampling\nplt.figure(figsize=(10, 10))\nax = plt.subplot(2, 2, 1)\nplt.imshow(viral_dataset[100], cmap = \"gray\")\nplt.title(\"Viral CXR - Before Image Enhancement\")\nplt.axis(\"off\")\n\nax = plt.subplot(2, 2, 2)\nplt.imshow(viral_dataset_enhanced[100], cmap = \"gray\")\nplt.title(\"Viral CXR - After Image Enhancement\")\nplt.axis(\"off\")\n    \n# COVID-19 CXR Sampling\nplt.figure(figsize=(10, 10))\nax = plt.subplot(2, 2, 1)\nplt.imshow(covid_dataset[100], cmap = \"gray\")\nplt.title(\"COVID-19 CXR - Before Image Enhancement\")\nplt.axis(\"off\")\n\nax = plt.subplot(2, 2, 2)\nplt.imshow(covid_dataset_enhanced[100], cmap = \"gray\")\nplt.title(\"COVID-19 CXR - After Image Enhancement\")\nplt.axis(\"off\")","141ac96f":"def normalizer(image_arrays):\n\n    \"\"\"\n    Argument: \n        image_arrays: array of images to be normalized\n    \n    returns:\n        normalized_images_array: array of the normalized images' arrays\n    \"\"\"\n    # Create an empty list to store normalized arrays\n    norm_image_arrays = []\n    \n    # Iterate over all the image arrays and normalize them before storing them into our predefined list\n    for image_array in image_arrays:\n        norm_image_array = image_array \/ 255.0\n        norm_image_arrays.append(norm_image_array)\n    \n    return norm_image_arrays","cd7332ca":"normal_dataset_normalized = normalizer(normal_dataset)\nviral_dataset_normalized = normalizer(viral_dataset)\ncovid_dataset_normalized = normalizer(covid_dataset)","4848d7ed":"# Preview the normal and normalized verisions of a sample dataset\n# normal_dataset[0][], normal_dataset_normalized[0][]","7b705f4f":"def split_and_merge_function(image_arrays, split_factor = [0.7, 0.15, 0.15]):\n    \n    \"\"\"\n    Argument:\n        image_arrays - A list or tuple of the images to be imported.\n        data_labels - A list of the data labels\n        split_factor - A list or tuple that determines how the dataset is splitted into training, validation and test dataset\n    \n    Task(s) carried out:\n        Split the dataset into training, validation, and test sets\n        Merge the sets of each categories\n    \n    Returns:\n    datasets - a dictionary containing the training, validation and test datasets (arrays of the images imported \n                and their respective labels)\n    \n    \"\"\"\n    # Define an empty dictionary to hold the training, validation and test datasets\n    datasets = {}\n    # Calculate the number of image categories in the list argument passed to this function\n    number_of_categories = len(image_arrays)   \n    \n    train_dataset, validation_dataset, test_dataset, train_labels, validation_labels, test_labels = [], [], [], [], [], []\n    \n    for image_array_id in range(number_of_categories):\n        image_array = image_arrays[image_array_id]\n        \n        # Update the data and labels lists, respectively\n        dataset = [[image_array[image_id], [image_array_id]] for image_id in range(len(image_array))]\n            \n        # Split the data and labels into the train, validation, and test datasets\n        train_dataset.extend(np.array(dataset[ : int(np.around(len(dataset) * split_factor[0]))]))\n        validation_dataset.extend(np.array(dataset[int(np.around(len(dataset) * split_factor[0])) : int(np.around(len(dataset) * (split_factor[0] + split_factor[1])))]))\n        test_dataset.extend(np.array(dataset[int(np.around(len(dataset) * (split_factor[0] + split_factor[1]))) : ]))\n        \n        # Randomize the train, validation and test datasets\n        random.seed(42) # Define a random state parameter to ensure the dataset generated is the same regardless of how many iterations we run\n        random.shuffle(train_dataset), random.shuffle(validation_dataset), random.shuffle(test_dataset)\n        \n        # Split the data and label pairs and add them to the data and labels lists\n        train_data = [dataset[0] for dataset in train_dataset]\n        train_labels = [dataset[1] for dataset in train_dataset]\n        validation_data = [dataset[0] for dataset in validation_dataset]\n        validation_labels = [dataset[1] for dataset in validation_dataset]\n        test_data = [dataset[0] for dataset in test_dataset]\n        test_labels = [dataset[1] for dataset in test_dataset]\n        \n    \n    # Store train, validation and test datasets into the datasets dictionary\n    datasets['train_dataset'] = np.array(train_data)\n    datasets['validation_dataset'] = np.array(validation_data)\n    datasets['test_dataset'] = np.array(test_data)\n    \n    # Convert labels from label-encoding to one-hot encoding and store in the datasets dictionary     \n    datasets['train_labels'] = to_categorical(np.array(train_labels))\n    datasets['validation_labels'] = to_categorical(np.array(validation_labels))\n    datasets['test_labels'] = to_categorical(np.array(test_labels))\n        \n    return datasets","5bcbb7d5":"image_arrays = [normal_dataset_normalized, viral_dataset_normalized, covid_dataset_normalized]\ndatasets = split_and_merge_function(image_arrays, split_factor = [0.7, 0.15, 0.15])","4e115122":"train_dataset = datasets['train_dataset']\nvalidation_dataset = datasets['validation_dataset']\ntest_dataset = datasets['test_dataset']\ntrain_labels = datasets['train_labels'] \nvalidation_labels = datasets['validation_labels']\ntest_labels = datasets['test_labels']","a84a1043":"print(\"The Dataset which is made up of {} Image Arrays has been splitted into:\".format(len(train_dataset) + len(validation_dataset) + len(test_dataset)))\nprint('{} Training Image Arrays'.format(len(train_dataset)))\nprint('{} Validation Image Arrays'.format(len(validation_dataset)))\nprint('{} Test Image Arrays'.format(len(test_dataset)))","a03ebc75":"def data_augmenter():\n    \"\"\"\n    Create a simple function to augment training images\n    Returns:\n        tf.keras.Sequential\n\n    \"\"\"\n    \n    data_augmentation = tf.keras.Sequential()\n    data_augmentation.add(RandomFlip('horizontal'))\n    data_augmentation.add(RandomRotation(0.1))\n    \n    return data_augmentation","7833216d":"data_augmentation = data_augmenter()\n\nplt.figure(figsize=(10, 10))\nfirst_image = train_dataset[6]\nfor i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    exp = tf.expand_dims(first_image, 0)\n    augmented_image = data_augmentation(exp)\n    plt.imshow(augmented_image[0])\n    plt.axis('off')","4988df51":"def make_mobilenet_model(image_size, num_classes, data_augmentation = data_augmenter()):\n    \n    input_shape = image_size + (3,)\n    \n    base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape,\n                                                   include_top=False, # Do not include the dense prediction layer\n                                                   weights=\"imagenet\") # Load imageNet parameters\n    \n    # Freeze the base model by making it non trainable\n    base_model.trainable = False \n\n    # create the input layer (Same as the imageNetv2 input size)\n    inputs = tf.keras.Input(shape=input_shape) \n    \n    # apply data augmentation to the inputs\n    x = data_augmentation(inputs)\n     \n    # set training to False to avoid keeping track of statistics in the batch norm layer\n    x = base_model(x, training=False) \n    \n    # Add the new Binary classification layers\n    # use global avg pooling to summarize the info in each channel\n    x = GlobalAveragePooling2D()(x) \n    #include dropout with probability of 0.2 to avoid overfitting\n    x = Dropout(0.2)(x)\n        \n    # create a prediction layer\n    if num_classes == 2:\n        activation = \"sigmoid\"\n        units = 1\n    else:\n        activation = \"softmax\"\n        units = num_classes\n\n    x = layers.Dropout(0.5)(x)\n    \n    prediction_layer = Dense(units, activation=activation)\n    \n    outputs = prediction_layer(x)\n    \n    model = Model(inputs, outputs)\n    \n    return model","488207bd":"# Define a model using the make_model function\nimage_size = (224,224)\nmobilenet_model = make_mobilenet_model(image_size, num_classes = 3)\n\n# Preview the Model Summary\nmobilenet_model.summary()","276803d4":"base_learning_rate = 0.001\noptimizer = Adam(learning_rate = base_learning_rate)\ninitial_epochs = 50\nbatch_size = 64\nloss = 'categorical_crossentropy'\nmetrics = ['accuracy']\ncallback = EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy',factor=1e-1, patience=8, verbose=1, min_lr = 2e-6)\n\nmobilenet_model.compile(optimizer = optimizer,\n              loss = loss,\n              metrics = metrics)","a58f1fb3":"mobilenet_history_freeze = mobilenet_model.fit(train_dataset, train_labels,\n                                               batch_size = batch_size, \n                                               epochs = initial_epochs, \n                                               validation_data = (validation_dataset, validation_labels), \n                                               callbacks = [callback, reduce_lr], \n                                               shuffle = True)","98125808":"acc = [0.] + mobilenet_history_freeze.history['accuracy']\nval_acc = [0.] + mobilenet_history_freeze.history['val_accuracy']\n\nloss = mobilenet_history_freeze.history['loss']\nval_loss = mobilenet_history_freeze.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","e26c71f2":"base_mobilenet_model = mobilenet_model.layers[2] # MobileNetV2 Architecture\nbase_mobilenet_model.trainable = True\n\n# The MobileNet Model has 155 layers (the prediction layer inclusive)\n# Fine-tune from this layer onwards\nfine_tune_at = 120\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in base_mobilenet_model.layers[:fine_tune_at]:\n    layer.trainable = True\n\n\noptimizer = Adam(learning_rate = 0.1 * base_learning_rate)\nbatch_size = 64\nloss = 'categorical_crossentropy'\nmetrics = ['accuracy']\ncallback = EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy',factor=1e-1, patience=8, verbose=1, min_lr = 2e-6)\n\nmobilenet_model.compile(optimizer = optimizer,\n                        loss = loss,\n                        metrics = metrics)","fd466863":"fine_tune_epochs = 30\ntotal_epochs =  initial_epochs + fine_tune_epochs\n\nmobilenet_history_fine = mobilenet_model.fit(train_dataset, train_labels,\n                                             epochs = total_epochs,\n                                             batch_size = batch_size,\n                                             initial_epoch = mobilenet_history_freeze.epoch[-1],\n                                             callbacks = [callback, reduce_lr], \n                                             validation_data = (validation_dataset, validation_labels),\n                                             shuffle = True)","55f01480":"acc = [0.] + mobilenet_history_fine.history['accuracy']\nval_acc = [0.] + mobilenet_history_fine.history['val_accuracy']\n\nloss = mobilenet_history_fine.history['loss']\nval_loss = mobilenet_history_fine.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","3ef1aedf":"mobilenet_model.save('mobilenet-covid-19-cxr-classification-model.h5')","e3d13548":"# Load model\nmobilenet_model = load_model('mobilenet-covid-19-cxr-classification-model.h5')","86eb73af":"training_predictions = mobilenet_model.predict(train_dataset, batch_size = 64)\n\n# The predictions for the each image come as an array of number of classes in the model. e.g [0.05, 0.20, 0.75] for a model predicting three classes\n# In this case, the model predict the probabilties that an x-ray image is normal, covid-19 or pnemonia, and store all of these values in an array\n# Hence for us to know the predicted class, we have to identify the label with corresponding largest predicted probability\n# The np.argmax returns the index of the largest probability. e.g it will return 2 for an array [0.1, 0.15, 0.75] \ntraining_predictions_classes = np.argmax(training_predictions, axis=1)\ntraining_labels_classes = np.argmax(train_labels, axis=1) # Position of the actual label\n\n# Return a nicely formatted classification report\n    \nprint(classification_report(training_labels_classes, training_predictions_classes, target_names=['normal','covid','virus']))","a27ef568":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues,\n                          target_names = ['Normal','Covid-19','Viral Pneumonia']):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    target_names = target_names\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n    \n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","2caf0baa":"# Predict the values from the training dataset\ntraining_predictions = mobilenet_model.predict(train_dataset, batch_size=64)\n# Convert predictions classes from one hot vectors to label encodings\ntraining_predictions_classes = np.argmax(training_predictions, axis = 1)\ntraining_labels_classes = np.argmax(train_labels, axis =1)\n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(training_labels_classes, training_predictions_classes)\n\n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(3)) ","0a97f20a":"validation_predictions = mobilenet_model.predict(validation_dataset, batch_size = 64)\n\n# The predictions for the each image come as an array of number of classes in the model. e.g [0.05, 0.20, 0.75] for a model predicting three classes\n# In this case, the model predict the probabilties that an x-ray image is normal, covid-19 or pnemonia, and store all of these values in an array\n# Hence for us to know the predicted class, we have to identify the label with corresponding largest predicted probability\n# The np.argmax returns the index of the largest probability. e.g it will return 2 for an array [0.1, 0.15, 0.75] \nvalidation_predictions_classes = np.argmax(validation_predictions, axis=1)\nvalidation_labels_classes = np.argmax(validation_labels, axis=1) # Position of the actual label\n\n# Return a nicely formatted classification report\n    \nprint(classification_report(validation_labels_classes, validation_predictions_classes, target_names=['normal','covid','virus']))","e717683c":"# Predict the values from the validation dataset\nvalidation_predictions = mobilenet_model.predict(validation_dataset, batch_size=64)\n# Convert predictions classes to one hot vectors \nvalidation_predictions_classes = np.argmax(validation_predictions, axis = 1)\nvalidation_labels_classes = np.argmax(validation_labels, axis = 1)\n# Convert validation observations to one hot vectors\n# compute the confusion matrix\nconfusion_mtx_2 = confusion_matrix(validation_labels_classes, validation_predictions_classes)\n\n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx_2, classes = range(3)) ","883e3f34":"test_predictions = mobilenet_model.predict(test_dataset, batch_size = 64)\n\n# The predictions for the each image come as an array of number of classes in the model. e.g [0.05, 0.20, 0.75] for a model predicting three classes\n# In this case, the model predict the probabilties that an x-ray image is normal, covid-19 or pnemonia, and store all of these values in an array\n# Hence for us to know the predicted class, we have to identify the label with corresponding largest predicted probability\n# The np.argmax returns the index of the largest probability. e.g it will return 2 for an array [0.1, 0.15, 0.75] \ntest_predictions_classes = np.argmax(test_predictions, axis=1)\ntest_labels_classes = np.argmax(test_labels, axis=1) # Position of the actual label\n\n# Return a nicely formatted classification report\n    \nprint(classification_report(test_labels_classes, test_predictions_classes, target_names=['normal','covid','virus']))","59268f30":"# Predict the values from the validation dataset\ntest_predictions = mobilenet_model.predict(test_dataset, batch_size=64)\n# Convert predictions classes to one hot vectors \ntest_predictions_classes = np.argmax(test_predictions, axis = 1)\ntest_labels_classes = np.argmax(test_labels, axis =1)\n# Convert validation observations to one hot vectors\n# compute the confusion matrix\nconfusion_mtx_3 = confusion_matrix(test_labels_classes, test_predictions_classes)\n\n \n\n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx_3, classes = range(3)) ","957bc51f":"merged_dataset = np.concatenate((train_dataset, validation_dataset), axis=0)\nmerged_dataset = np.concatenate((merged_dataset, test_dataset), axis=0)\nmerged_labels = np.concatenate((train_labels, validation_labels), axis=0)\nmerged_labels = np.concatenate((merged_labels, test_labels), axis=0)\nmerged_dataset.shape, merged_labels.shape","5cd479af":"base_mobilenet_model = mobilenet_model.layers[2] # MobileNetV2 Architecture\nbase_mobilenet_model.trainable = True\n\n# The MobileNet Model has 155 layers (the prediction layer inclusive)\n# Fine-tune from this layer onwards\nfine_tune_at = 120\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in base_mobilenet_model.layers[:fine_tune_at]:\n    layer.trainable = True\n\n\noptimizer = Adam(learning_rate = 0.1 * base_learning_rate)\nbatch_size = 64\nloss = 'categorical_crossentropy'\nmetrics = ['accuracy']\ncallback = EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy',factor=1e-1, patience=8, verbose=1, min_lr = 2e-6)\n\nmobilenet_model.compile(optimizer = optimizer,\n                        loss = loss,\n                        metrics = metrics)","5cfbbba9":"fine_tune_epochs = 20\ntotal_epochs =  initial_epochs + fine_tune_epochs\n\nmobilenet_history_fine_final = mobilenet_model.fit(merged_dataset, merged_labels,\n                                             epochs = total_epochs,\n                                             batch_size = batch_size,\n                                             initial_epoch = mobilenet_history_fine.epoch[-1],\n                                             callbacks = [callback, reduce_lr], \n                                             shuffle = True)","a206220b":"mobilenet_model.save('mobilenet-covid-19-cxr-classification-final-model.h5')","8e875e5a":"# Load model\nmobilenet_model = load_model('mobilenet-covid-19-cxr-classification-final-model.h5')","1eb45624":"merged_dataset_predictions = mobilenet_model.predict(merged_dataset, batch_size = 64)\n\n# The predictions for the each image come as an array of number of classes in the model. e.g [0.05, 0.20, 0.75] for a model predicting three classes\n# In this case, the model predict the probabilties that an x-ray image is normal, covid-19 or pnemonia, and store all of these values in an array\n# Hence for us to know the predicted class, we have to identify the label with corresponding largest predicted probability\n# The np.argmax returns the index of the largest probability. e.g it will return 2 for an array [0.1, 0.15, 0.75] \nmerged_dataset_predictions_classes = np.argmax(merged_dataset_predictions, axis=1)\nmerged_dataset_labels_classes = np.argmax(merged_labels, axis=1) # Position of the actual label\n\n# Return a nicely formatted classification report\n    \nprint(classification_report(merged_dataset_labels_classes, merged_dataset_predictions_classes, target_names=['normal','covid','virus']))","72d65c39":"merged_dataset_predictions = mobilenet_model.predict(merged_dataset, batch_size=64)\nmerged_dataset_predictions_classes = np.argmax(merged_dataset_predictions, axis = 1)\nmerged_dataset_labels_classes = np.argmax(merged_labels, axis =1)\nconfusion_mtx_4 = confusion_matrix(merged_dataset_labels_classes, merged_dataset_predictions_classes)\n\n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx_4, classes = range(3)) ","29716ebc":"predictions = mobilenet_model.predict(merged_dataset, batch_size = 64)\nprediction_classes = np.argmax(predictions, axis = 1)\ntrue_classes = np.argmax(merged_labels, axis = 1)\n\nclass_names = {0: \"Normal CXR\", 1 : \"Viral Pneumonia CXR\", 2: \"COVID-19 CXR\"}","dfebeed7":"plt.figure(figsize=(12, 12))\nfor image in range(9):\n    ax = plt.subplot(3, 3, image + 1)\n    plt.imshow(merged_dataset[image], cmap = \"gray\")\n    plt.title(\"Predicted Class: \" + str(class_names[prediction_classes[image]]) + \"\\n\" +\"Actual Class: \" + str(class_names[true_classes[image]]))\n    plt.axis(\"off\")\n","82dce6a0":"##### 4.4.2. Fine-tune the Model using the merged datasets and labels","939eefed":"#### 2.4. Split datasets into training, validation, and test sets <a class=\"anchor\" id=\"2-4\"><\/a>\n\n***Why do we need three sets?***\n\nMachine learning is an iterative process where the dataset is passed through series of networks. The probality to obtain optimal result at first trial is extremely small. Hence, to avoid most common shortfalls often encountered (model underfitting or overfitting), the best approach to track model performance during and after training is to split the data into the training, validation, and test sets.\nThe training dataset will be used in developing several models while the validation set will be used to select the best model. However, since the validation set is used to reengineer the models, it is also subjected to overfitting depending on how large it is and also how  many times it is exposed to the training process (used to validate model). Hence, the need for another dataset to ensure both the model is not overfitted to both the training and validation sets. That is the test set. \nThe test set should only be used on the selected model and it should only be used once.\n\nWhat if the model performs poorly on the test set? Should I re-architect my model and train again?\nYes, but with a new training, validation and test sets.\n\n***How do we split our datasets three sets?***\n\nThere is no standard rule of how these sets should be form. Understanding how to derive the best out of the data available is how one can achieve great success within a short period.\nWe have three classes of dataset - normal, covid and viral pneumonia. We'll split each of the classes in a ratio of *70%:15%:15%*. 70% of each class will be merged together to form the training set while the other two 15% will be used for validation and test sets respectively.","d4feb0c0":"#### 2.2. Image Enhancement uisng White Balance and Contrast Limited Adaptive Histogram Equalization <a class=\"anchor\" id=\"2-2\"><\/a>\n\nThis data preprocessing step is inspired from  [COVIDLite](https:\/\/arxiv.org\/pdf\/2006.13873.pdf) Paper. In this paper, the authors used White Balance (WB) and Contrast Limited Adaptive Histogram Equalization (CLAHE) as a image preprocessing step for detection of COVID-19 cases.\n\n##### 2.2.1. White Balance\n\nWhite Balance is the image processing operation applied to adjust proper color fidelity in a digital image. Due to low lighting conditions in medical images, some of the parts of the image appeared dark and the image capturing equipment does not detect light precisely as the human eye does. Due to this, image processing or correction help to ensure that the final image represents the colors of the natural image. The objective of this operation is to enhance the visibility of the image so that DCNNs could extract useful features from the image. The white balance algorithm adjusts the colors of the active layers of the image by stretching red, green, and blue channels independently. For doing this, pixel colors discarded, which are at the end of the three channels and are used by only 0.05% of the pixels in the image, while stretching is performed for the remaining color range. After this operation, pixel colors infrequently present at the end of the channel could not negatively influence the upper and lower bound values while stretching. In this solution, we have implemented a white balance algorithm in python language using NumPy and OpenCV library.\n\nThe steps of the White balance algorithm can be summarized as:\n\n![](https:\/\/i.ibb.co\/rxty2DP\/white-balance.png)\n\nwhere Pi(C) represents the taking the ith percentile of channel C, and Clip(., min, max) operation depicts performing saturation operation within min and max values. C, Cupd denotes the input and updated channels pixel values after the operation respectively.\n\n##### 2.2.2. CLAHE (Contrast Limited Adaptive Histogram Equalization)\n\nIt is an effective contrast enhancement method that effectively increases the contrast of the image. CLAHE is an improved version of the adaptive histogram equation (AHE). Histogram equalization is the simple method for enhancing the contrast of the image by spreading out the intensity range of the image or stretching out the most frequent intensity value of the image. Stretching the intensity values changes the natural brightness of the input image and introduces some undesirable noise in the image. In AHE, the input image split into several small images, also known as tiles. In this method, the histogram of each tile computed, which corresponds to different sections of the image and uses them to derive intensity remapping function for each tile. This method introduces noise in the image due to over amplification. CLAHE works precisely the same as AHE, but it clips the histogram at specific values for limiting the amplification before computing the cumulative distributive function. The overamplified part of the histogram is further redistributed over the histogram. In one of the previous studies, CLAHE showed exceptional results in enhancing chest CT images and considered useful in examining a wide variety of medical images. The computation of CLAHE is performed as:\n\n![](https:\/\/i.ibb.co\/MVvpzq7\/clahe-1.png)\n\nwhere, p represents pixel value after applying CLAHE, pmax , pmin represents maximum and minimum pixel value of an image respectively and P(f) represents cumulative probability distribution function.\n\n![](https:\/\/i.ibb.co\/X3cpY0t\/clahe-2.png)","3ec3c4e7":"##### 2.1.2. Initialize the function to create dataset\n","a3f8d03e":"## 1. Import Required Packages <a class=\"anchor\" id=\"1\"><\/a>","785b48a2":"#### 4.2. Model Evaluation on the validation set <a class=\"anchor\" id=\"4-2\"><\/a>\n\n##### 4.2.1. Classification Report 2","7e94f7c1":"#### 4.5. Model Evaluation on all datasets and labels <a class=\"anchor\" id=\"4-5\"><\/a>\n##### 4.5.1. Classification Report 4","c464f03e":"## 4. Model Evaluation <a class=\"anchor\" id=\"4\"><\/a>\n\nAfter training our model on several iterations and tuning the training hyper-parameters, we have succesfully identified a model with a pretty decent accuracy on both the training and the data validation. However, it is important to know classification accuracy alone can be misleading if you have an unequal number of observations in each class or if you have more than two classes in your dataset. Hence, a good accuracy is not always sufficient to conclude that the model is great. The model can perform well at classifying either normal, covid or pneumonia x-ray images and perform poorly on others. This may not reflect on the model accuracy, hence using other metrics can yield additional gains. We'll explore the use of precision and recall as additional metrics to evaluate the model performance.\n\nIn addition, while the model performs well on both the training and validation dataset, it is also imparative to know that we designed the model and tuned its hyper-parameters to perform well on both datasets. This implies there's a probability the model has overfitted on both datasets and perform poorly on datasets not used for training or validation. Therefore, we'll also introduce another dataset that is new to the model - the test dataset. \n\nIn this section, we'll be exploring the model performance on the training, validation and test datasets. We'll be evaluating the model accuracy, precision, and recall using **classification reports** and **confusion matrix**. But before we dive right in, let us quickly understand what these terms are.\n\n**1. Accuracy**: classification accuracy is the ratio of correct predictions to total predictions made. Model accuracy is independent of how many categories\/classes in our dataset. It is often represented in percentages and it can be calculated by:\n>                 Classification accuracy = (correct predictions \/ total predictions) * 100\n\n**2. Precision**: classification precision is the measure of the extent to which our model makes correct predictions. It measures how many correct classification our model predicts are actually correct. For example, if our model predicts 100 x-ray images as normal, precision measures how many of the 100 x-ray images are actually normal.\n\n**3. Recall**: recall is the measure of the extent to which our model can make correct prediction. It measures how many of the correct predictions our model can make. For example, if we have 100 viral pneumonia x-ray images, recall is the measure of how many of the 100 x-ray images can our model can correctly classify as viral pneumonia x-ray images.\n\n**4. F-1 Score**: this combines the precision and recall scores and helps us make a sound judgment to determine if we will stick to the model currently being evaluated or retrain our model. It is often used as a satisfiscing metric with accuracy as the optimizing metric. \n\n**5. Classification Report**: This is a summary of the classification accuracy, precision, recall and F-1 Score.\n\n**6. Confusion Matrix**: While the classification report gives us the high level details of our model's accuracy, precision and recall. Understanding the lower level details of what our model performs makes us understand the model more and how we can further improve it. In summary, a confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix. The confusion matrix shows the ways in which our classification model is confused when it makes predictions. It also gives us insight not only into the errors being made by your classifier but more importantly the types of errors that are being made. It is this breakdown that overcomes the limitation of using classification accuracy alone.\n\nRead more on Confusion Matrix at [**Machinelearningmastery**](https:\/\/machinelearningmastery.com\/confusion-matrix-machine-learning\/)","fc4577ee":"#### 3.3. Save Model <a class=\"anchor\" id=\"3-3\"><\/a>","133a1fe0":"## 5. Key Observations and conclusion <a class=\"anchor\" id=\"5\"><\/a>\n<font size=\"3\">\n\n1. Our model performs pretty well on all classes. However, despite achieving above the pre-defined minimum optimizing and satisficing metrics of 95% accuracy, 95% precision, 95% recall and 95% F-1 score, the model still makes significant error while predicting COVID-19 CXR as Normal and otherwise. (this can be verified from the Confusion Matrices 1 to 4). We can attribute this error to lack of sufficient dataset for our model to learn more high level differences between both classes of the CXR images. <br\/>\n2. The MobileNetV2 pre-trained parameters performs pretty well on the CXR dataset, even though it was pre-trained on ImageNet which does not include medical dataset.<br\/>\n3. The MobileNetV2 architecture (97%) achieved a superior performance compared to the recent state-of-the-art methods employed for the detection of COVID-19 from radiology images in terms of accuracy. The COVIDLite model achieved the highest accuracy of 96%. Other Model accuracy are detailed in the [**COVIDLite Paper**](https:\/\/arxiv.org\/pdf\/2006.13873.pdf) <br\/>\n  \nIn conclusion, our model despite showing significant performance can't be trusted for deployment because we do not have sufficient dataset for our model to learn more high-level complex features. Also, the chest x-ray classification is a critical problem that should be done with utmost preparation without any room for error. \nHand-engineering desired features is an approach future researchers can explore to achieving a more robust and explainable model.\n\n    \n<\/font>","2ef2d91c":"## **Project Overview**: \n\n| <br\/><font size=\"3\"><b> Focus <\/b><\/font><br\/><br\/>  | <br\/><font size=\"3\"><b> Description <\/b><\/font><br\/><br\/> |\n| :-- | :-- |\n| <br\/><font size=\"3\"> <b>Project Title<\/b> <\/font>  <br\/><br\/>| <br\/><font size=\"3\"> Classification of Posteroanterior (PA) view of Chest X-ray (CXR) images into Normal, COVID-19, or Viral Pneumonia using MobileNetV2 Classification Model<\/font> <br\/><br\/>|\n| <br\/><font size=\"3\"> <b>Project Type<\/b> <\/font> <br\/><br\/>| <br\/><font size=\"3\">Image Classification. <\/font> <br\/><br\/>|\n| <br\/><font size=\"3\"><b>Project Objectives<\/b> <\/font>  <br\/><br\/>| <br\/><font size=\"3\">1. Fine-tune the MobileNetV2 Model to classify Posteroanterior (PA) view of Chest X-ray images into Normal, Covid-19, or Viral Pneumonia<br\/><br\/> 2. Evaluate the model perfomance on the training and validation sets <\/font> <br\/><br\/>|\n| <br\/><font size=\"3\"> <b>Dataset Overview <\/b> <\/font> <br\/><br\/>| <br\/><font size=\"3\">The dataset is made up of <b>1823<\/b> annotated posteroanterior (PA) view of Chest X-ray images. Labeled Optical Coherence Tomography (OCT) and CXR Images used for viral pneumonia and non-pneumonia or normal cases. The dataset consists of 536 images of COVID-19, 619 images of viral pneumonia, and 668 images of normal cases. The age range of COVID-19 cases in the dataset is 18-75 years. <\/font> <br\/><br\/>|\n| <br\/><font size=\"3\"> <b> Model Evaluation Metrics<\/b> <\/font> <br\/><br\/>| <br\/><font size=\"3\">1. Optimizing Metric: Accuracy (95%) <br\/>2. Satisficing Metrics: Precision (95%), Recall (95%), F-1 Score (95%)<\/font> <br\/><br\/>|\n| <font size=\"3\"> <b>Classification Model Type<\/b> <\/font> | <br\/><font size=\"3\"> <a href=\"https:\/\/arxiv.org\/pdf\/1801.04381.pdf\"> <b> MobileNetV2<\/b><\/a> <\/font> <br\/><br\/>|\n| <br\/><font size=\"3\"> <b>Major Libraries Used<\/b> <\/font> <br\/><br\/>| <br\/><font size=\"3\"> <a href=\"https:\/\/keras.io\"><b>Keras<\/b><\/a>, <a href=\"https:\/\/opencv.org\"><b>OpenCV<\/b><\/a>, <a href=\"https:\/\/scikit-learn.org\"><b> Scikit-Learn <\/b><\/a>, <a href=\"https:\/\/numpy.org\"><b>Numpy<\/b><\/a>, <a href=\"https:\/\/matplpotlib.org\"><b>Matplotlib<\/b><\/a><\/font> <br\/><br\/>|\n\n<br\/>","9b9be6ae":"##### 4.1.2. Confusion Matrix 1","650e1dc1":"##### 2.2.1. Enhance the chest x-ray images using the predefined function","18c6a1af":"## **Table of Contents**: \n<font size=\"3\">\n    \n- [**1 - Import Required Packages**](#1)\n\n- [**2 - Data Preparation**](#2)\n    - [2.1. Import Images from Directory into Arrays](#2-1)\n    - [2.2. Image Enhancement uisng White Balance and Contrast Limited Adaptive Histogram Equalization](#2-2)\n    - [2.3. Normalize Dataet](#2-3)\n    - [2.4. Split Dataset into training, validation and test sets](#2-4)\n    - [2.5. Augment Dataset](#2-5)\n\n\n- [**3 - Transfer Learning with MobileNetV2**](#3)\n    - [3.1. - Layer Freezing with the Functional API](#3-1)\n    - [3.2. - Fine-tuning the Model](#3-2)    \n    - [3.3. - Save the Model](#3-3)\n\n\n- [**4 - Model Evaluation**](#4)\n    - [4.1. - Model Evaluation on the training set](#4-1)\n    - [4.2. - Model Evaluation on the validation set](#4-2)\n    - [4.3. - Model Evaluation on the test set](#4-3)\n    - [4.4. - Model Retraining using all the datasets (training, validation and test sets)](#4-4)\n    - [4.5. - Model Evaluation on all datasets (merged training, validation and test sets)](#4-5)\n    - [4.6. - Predict selected images using the trained Model](#4-6)    \n    \n\n- [**5 - Key Obsevations and Conclusion**](#5)\n  \n<font>","b80022ee":"## 3. Transfer Learning Using MobileNetV2 <a class=\"anchor\" id=\"3\"><\/a>\n\n##### **What is Transfer Learning?**\n\nTransfer learning is the re-use of a pre-trained model on a new problem. In transfer learning, a machine exploits the knowledge gained from a previous task to improve generalization about another. For example, in training a classifier to predict whether an image contains food, you could use the knowledge it gained during training to recognize drinks.\n\n##### **Why Transfer Learning?**\n\nTransfer learning has several benefits, but the main advantages are  saving training time, better performance of neural networks (in most cases), and not needing a lot of data. \nUsually, a lot of data is needed to train a neural network from scratch but access to that data isn't always available \u2014 this is where transfer learning comes in handy. With transfer learning a solid machine learning model can be built with comparatively little training data because the model is already pre-trained. This is especially valuable in natural language processing because mostly expert knowledge is required to create large labeled datasets. Additionally, training time is reduced because it can sometimes take days or even weeks to train a deep neural network from scratch on a complex task.\n\n##### **When to use Transfer Learning?**\n\nWhile there are no general rules that guide the use of pretrained models, here are some guidelines on when transfer learning might come handy:\n\n1. There isn't enough labeled training data to train your network from scratch.\n2. There already exists a network that is pre-trained on a similar task, which is usually trained on massive amounts of data.\n3. When task 1 and task 2 have the same input.\n\n\n##### **What is MobileNetV2?**\n\nThe [**MobileNetV2**](https:\/\/arxiv.org\/pdf\/1801.04381.pdf) is a light-weight image classification model\/algorithm developed by *Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen*. The model is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer.\nThe model was trained on Imagenet, and its performance was measured on Imagenet classification, COCO object detection, and VOC image segmentation.\n\n***Note***: \nYou can read more on transfer learning on [**Bulletin**](https:\/\/builtin.com\/data-science\/transfer-learning).\n\nYou can read more on MobileNetV2 on [**Towardsdatascience**](https:\/\/towardsdatascience.com\/review-mobilenetv2-light-weight-model-image-classification-8febb490e61c), [**Arxiv**](https:\/\/arxiv.org\/pdf\/1801.04381.pdf)\n\n\n##### **How to use MobileNetV2**\n\nWe'll use the MobileNetV2's pre-trained parameters to modify our classifier task so that it's able to classify chest x-ray images to normal, covid, and pneumonia. We can achieve this in two steps:\n\n1. Freeze the model and use the pre-trained parameters to classify chest x-ray images to normal, covid, and pneumonia.\n2. Unfreeze the model and fine-tune to improve model parameters\n","69aec5d4":"##### 4.2.2. Confusion Matrix 2","1511dcd8":"##### 2.2.2. Preview the differences between the normal images and the enhanced images","b7c7fcd1":"## 2. Data Preparation <a class=\"anchor\" id=\"2\"><\/a>","1e06e2e1":"#### 4.3. Model Evaluation on the test set <a class=\"anchor\" id=\"4-3\"><\/a>\n\nWhy evaluate a third set when we've already evaluated the training and test set?\nJust to avoid model overfitting. We want to ensure our model works well on all relevant dataset and not only those involved in its training.\n\n##### 4.3.1. Classification Report 3","1ef99378":"#### 2.5. Augment dataset <a class=\"anchor\" id=\"2-5\"><\/a>\n\n***Why do we need data augmentation?***\n\nOverfitting is caused by having too few samples to learn from, rendering us unable to train a model able to generalize to new data. Given infinite data, our model would be exposed to every possible aspect of the data distribution at hand: we would never overfit. Data augmentation takes the approach of generating more training data from existing training samples, by \"augmenting\" the samples via a number of random transformations that yield believable-looking images. The goal is that at training time, our model would never see the exact same picture twice. This helps the model get exposed to more aspects of the data and generalize better.","191d672d":"##### 4.3.2. Confusion Matrix 3","fcf3e1ec":"#### 3.2. Fine-tuning the Model by re-running the optimizer in the last layers to improve accuracy <a class=\"anchor\" id=\"3-2\"><\/a>\n\nWe could try fine-tuning the model by re-running the optimizer in the last layers to improve accuracy. In transfer learning, the way we achieve this is by unfreezing the layers at the end of the network, and then re-training our model on the final layers with a very low learning rate. Adapting the learning rate to go over these layers in smaller steps can yield more fine details - and higher accuracy.\n\n**The intuition for what's happening**: when the network is in its earlier stages, it trains on low-level features, like edges. In the later layers, more complex, high-level features like arrows begin to emerge. For transfer learning, the low-level features can be kept the same, as they have common features for most images. When adding new data, we generally want the high-level features to adapt to it, which is rather like letting the network learn to detect features more related to our data. \n\nTo achieve this, just unfreeze the final layers and re-run the optimizer with a smaller learning rate, while keeping all the other layers frozen. After then, we will run it again for another few epochs, and see if our accuracy improved!","2c3641a4":"#### 2.3. Normalize Dataset <a class=\"anchor\" id=\"2-3\"><\/a>\n\nNormalization is a data preprocessing technique  in machine learning whose goal is to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values. The aim of normalization is to ensure data is in a great state to speed up our machine learning training. Hence, not every dataset require normalization. It is required only when features have different ranges.\nEach value of a digital image array is always between 0 and 255. This great distortion could slow down how our model process and learn from the data (especially when our weights are often between 0 and 1). Hence, rescaling our model by dividing all of the array values by 255 (which is the highest achievable value of a digital image) will ensure all the values end up between 0 and 1. This speeds up our model training and also helps us achieve better results.\n\nRead more on Normalization at this [Medium Blog Post](https:\/\/medium.com\/@urvashilluniya\/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029#:~:text=Normalization%20is%20a%20technique%20often,dataset%20does%20not%20require%20normalization.)","0a233515":"##### 2.4.1. Split our datasets using the predefined function","f72f3750":"##### 2.5.1. Apply the data augmentation function on an image from the training dataset ","f43e421c":"#### 2.1. Import images from directory into arrays <a class=\"anchor\" id=\"2-1\"><\/a>\n\nThe chest x-ray images are stored in three different directories, each of which corresponds to their classes\/categories. We will import Images in each of the folders to different lists\/arrays to ensure we have more flexibility in handling the data (especially during splitting into the training, validation and test sets). \n\nOne of the goals of data preprocessing is to eliminate data skeweness in either of the training, validation and test sets. To achieve that, we have to ensure all the training, validation and test sets have considerable datasets from all the three classes (normal, viral pneumonia, and covid-19) we're training. Merging all the images into an array before splitting into the training, validation and test sets isn't an effecient approach as we can't outrightly control the dataset order after it has been merged and shuffled.\nHence, we'll split each class into the training, validation and test sets before merging respective sets together.\n\n##### 2.1.1. Create a function to import images from a specified directory and convert them to array","f36fe6a6":"#### 4.4. Model retraining and fine-tuning using all the dataset <a class=\"anchor\" id=\"4-4\"><\/a>\n\n##### 4.4.1. Merge all the datasets and labels","a4e94500":"#### 4.1. Model evaluation on the training set <a class=\"anchor\" id=\"4-1\"><\/a>\n\n***Why evaluate the training set?***\n\nEvaluating the training set isn't something you'll see on a regular machine learning project. However, doing so helps us to further understand how our model handles the data it learnt from (Does it struggle to learn a class because data isn't sufficient or the images are blurry? etc).  Also, this helps us make better decision whether to stick to the model or not after evaluating the validation and test sets. \n\n***But how does this make any difference?***\n\nAssuming a model is biased towards a class of the training set, but performs well on the same class in the validation and test set.  This flags that our model has one or more hidden issues and can't be trusted to make sound judgement in the future. Definately, that isn't the regular underfitting and overfitting problem we could see from accuracy or a problem we could have flagged without evaluating the training set.  \n\n##### 4.1.1. Classification Report 1","04d0f130":"##### 4.4.3. Save the Model","ff86e166":"##### 4.5.2. Confusion Matrix 4","58a95053":"#### 3.1. Layer Freezing with Functional API  <a class=\"anchor\" id=\"3-1\"><\/a>\n\n1. Delete the top layer (the classification layer)\n    * Set `include_top` in `base_model` as False\n2. Add a new classifier layer\n    * Train only one layer by freezing the rest of the network\n3. Freeze the base model and train the newly-created classifier layer\n    * Set `base model.trainable=False` to avoid changing the weights and train *only* the new layer\n    * Set training in `base_model` to False to avoid keeping track of statistics in the batch norm layer","80250509":"##### 2.1.4 Key Observations from the above sample images :\n\n1. Normal CXR Images have clear lungs \n2. Viral Pneumonia CXR Images have slight congestion in lungs\n3. COVID-19 images have serious congestion in lungs","186113c3":"#### 4.6. Predict Random Images from the merged dataset <a class=\"anchor\" id=\"4-6\"><\/a>","57a41bae":"##### 2.1.3. Preview some of the images from the created datasets"}}