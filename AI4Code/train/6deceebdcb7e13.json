{"cell_type":{"12bfe6a0":"code","2f019d75":"code","0b93ea0e":"code","6f7281d7":"code","6d5933ee":"code","3a009fe2":"code","0103681b":"code","34c8a30f":"code","59a67cc7":"code","eda3cf77":"code","9cafdd57":"markdown","1ddf4d2b":"markdown","a348c2ff":"markdown","f8c8d644":"markdown","24f1f6af":"markdown","29db0005":"markdown","6fc74d95":"markdown","c46ac05c":"markdown","8f965f95":"markdown","b8901f97":"markdown","f9bea36c":"markdown","64c5bc66":"markdown"},"source":{"12bfe6a0":"pip install wordcloud","2f019d75":"import requests\n\ndef wikipedia_page(title):\n    '''\n    This function returns the raw text of a wikipedia page \n    given a wikipedia page title\n    '''\n    params = { \n        'action': 'query', \n        'format': 'json', # request json formatted content\n        'titles': title, # title of the wikipedia page\n        'prop': 'extracts', \n        'explaintext': True\n    }\n    # send a request to the wikipedia api \n    response = requests.get(\n         'https:\/\/en.wikipedia.org\/w\/api.php',\n         params= params\n     ).json()\n\n    # Parse the result\n    page = next(iter(response['query']['pages'].values()))\n    # return the page content \n    if 'extract' in page.keys():\n        return page['extract']\n    else:\n        return \"Page not found\"\n\n# We lowercase the text to avoid having to deal with uppercase and capitalized words\ntext = wikipedia_page('Earth').lower()\nprint(text) ","0b93ea0e":"import requests\n# this is the url for Alice in Wonderland\nresult = requests.get('http:\/\/www.gutenberg.org\/files\/11\/11-0.txt')\nprint(result.text)","6f7281d7":"# import the wordcloud library\nfrom wordcloud import WordCloud\n# Instantiate a new wordcloud.\nwordcloud = WordCloud(random_state = 8,\n        normalize_plurals = False,\n        width = 600, height= 300,\n        max_words = 300,\n        stopwords = [])\n# Apply the wordcloud to the text.\nwordcloud.generate(text)","6d5933ee":"import matplotlib.pyplot as plt\n# create a figure\nfig, ax = plt.subplots(1,1, figsize = (9,6))\n# add interpolation = bilinear to smooth things out\nplt.imshow(wordcloud, interpolation='bilinear')\n# and remove the axis\nplt.axis(\"off\")","3a009fe2":"import requests\ntext = wikipedia_page('Earth').lower()\nprint(text[:200] + '...')","0103681b":"from collections import Counter\n# we transform the text into a list of words \n# by splitting over the space character ' '\nword_list = text.split(' ')\n# and count the words\nword_counts = Counter(word_list)","34c8a30f":"for w in word_counts.most_common(20):\n    print(f\"{w[0]}:  \\t{w[1]} \")\n","59a67cc7":"# transform the text into a list of words\nwords_list = text.split(' ')\n# define the list of words you want to remove from the text\nstopwords = ['the', 'of', 'and', 'is','to','in','a','from','by','that', 'with', 'this', 'as', 'an', 'are','its', 'at', 'for']\n# use a python list comprehension to remove the stopwords from words_list\nwords_without_stopwords = [ word for word in words_list if word not in stopwords ]\n","eda3cf77":"Counter(words_without_stopwords).most_common(20)","9cafdd57":"### Create the Word Cloud\nLet's create a word cloud. Remember, the goal is to understand better what Wikipedia's Earth page is about without reading it.","1ddf4d2b":"The top 20 now contains multiple meaningful words such as million, surface, life, solar, and sun. Although there are still some stop words we could get rid of (on, it, has, etc.), you can see that removing only a handful improves the value of the word cloud.","a348c2ff":"# Natural Language Processing :\n\notherwise known as NLP, is the technology behind Siri, autocorrect, chatbots, and Google Suggest. It's what helps you translate text, filter spam, and detect fake news. In short, it's the technology that allows a machine to understand and process human language. \n\nBut how does it work under the hood? How can you use NLP to transform human language into something that a computer can understand? Look no further; this course has the answer!\n\n* In Part 1 of this course, we will explore how to preprocess text data and get it ready for further processing.\n\n* In Part 2, we will explore a text vectorization technique called bag-of-words and solve text classification problems such as sentiment analysis. \n\n* In Part 3, you will learn another vectorization technique called word embeddings and apply it to text exploration.\n\nBy the end of the course, you will have a basic understanding of how NLP models work and how you can use them in machine learning projects. You will also be introduced to the spaCy 2.3 and NLTK 3.5 libraries in Python 3.8!\n\nReady to dive into one of the most innovative domains in artificial intelligence? Then let's get started!\t\n## Partie 1 - Preprocess Text Data\n\n1. Build Your First Word Cloud\n2. Remove Stop Words From a Block of Text\n3. Apply Tokenization Techniques\n4. Create a Unique Word Form With SpaCy\n5. Extract Information With Regular Expression\n\n## Partie 2 - Vectorize Text for Classification Using Bag-of-Words\n\n1. Apply a Simple Bag-of-Words Approach\n2. Apply the TF-IDF Vectorization Approach\n3. Apply Classifier Models for Sentiment Analysis\n\n## Partie 3 - Vectorize Text for Exploration Using Word Embeddings\n\n1. Discover The Power of Word Embeddings\n2. Compare Embedding Models\n3. Train Your First Embedding Models\n4. Bonus! Doing More with SpaCy\n","f8c8d644":"# Build Your First Word Cloud\n![](https:\/\/user.oc-static.com\/upload\/2021\/01\/06\/16099626797038_P1C1.png)\n\n**Hello and welcome to the course!**\n\nMy name is salah sammari, and I am a data scientist  expert. \n\nI am excited to share some of the tools and methods I use on the job. By the end of this course, you will also be able to apply them to your text analysis projects!\n\nBefore getting into the technique, let's take a moment to discuss natural language processing as a whole.\n\n## What Is Natural Language Processing?\nLanguage is everywhere. It\u2019s how I am communicating with you right now!  There is even a whole scientific discipline, called linguistics, dedicated to the study of human language, including syntax, morphology, and phonetics, whose goal is to uncover the psychological and societal working of language. Ambitious, right? ^^\n\nOkay...but how does this relate to computer science?\n\nWell, in the late 1950s, classic linguistics gave birth to computational linguistics by adding statistics and computers into the mix. The goal then became to build automated processes that could understand and transform text. \n\nToday, natural language processing (NLP) is a direct evolution of computational linguistics that leverages artificial intelligence and machine learning.\n\nThe purpose of NLP is to transform (process) human language (natural language) into language a machine can understand and use.\n\nTo deepen your understanding, let\u2019s look at some real-world applications of NLP in everyday life. You will recognize a few!\n\nDiscover Real-Life Use Cases of NLP\nInformation Extraction\nInformation extraction is identifying a specific piece of information from a block of text such as a name, a location, or a hashtag.\n\nFor example, in human resources, natural language processing can help match the right candidate to the right job. Imagine that you have a bank of thousands of CVs. That's thousands and thousands of words. The ability to extract a specific job title from all those words makes job matching a lot more efficient.\n\n### Speech-to-Text Conversion\nHave you ever used the dictation tool on your phone or spoken to Siri or Alexa? NLP helps convert spoken word to numerical vectors, which computers can understand and process.\n\n### Text Classification\nThis type of NLP is typically referred to as text tagging, assigning a specific sentence or word to an appropriate category such as positive or negative, spam or not spam. It can help with spam filtering, sentiment analysis, topic inference, and hate speech detection!\n\nFor example, in digital marketing, a brand's social media account may have hundreds of comments a day. These comments bring valuable insights to the company but can be tedious to sift through. NLP can help identify positive or negative aspects of customers' comments!\n\n![](http:\/\/)### Text Generation\nGoogle\u2019s Smart Compose, which helps write emails, is a good example. But text generation goes beyond email composition. Recent models (GTP-3) can write coherent stories across multiple paragraphs paving the way for automated news content creation!\n\nNLP is evolving at breathtaking speed. Recent models can write text indistinguishable from human prose, while automatic translation has become commonplace, and conversational chatbots are increasingly efficient. This course will guide you through the foundations of creating an NLP model. Consider this the first step before going on to making the world\u2019s next new speech detection technology. ^^\n\nLet\u2019s kick things off with some hands-on practice. The goal is to give you a taste of what it\u2019s like to build a natural language processing model. You will also learn about stop words, which will come in handy in the coming chapters!","24f1f6af":"## Remove Stop Words","29db0005":"### Let's Recap!\n\n* Natural language processing (NLP) lies at the intersection between linguistics and computer science. Its purpose is to transform (process) human language (natural language) into language a machine can understand and use.\n\n* Some common NLP use-cases include information extraction, text classification, unsupervised exploration, text generation, and many others.\n\n* A word cloud is a snapshot of a text. It helps you explore and understand text at a glance.\n\n* To generate word clouds in Python, use the word cloud library. You can read more about it on the project description of the Python Package Index (PyPI) page. \n\n* Stop words do not provide any useful information to infer a text\u2019s content. They are either prepositions and conjunctions (i.e., the, of, is, are, and, that, from), or non-specific words that occur too frequently.\n\n* All the scripts in the course are available as Jupyter Notebooks in this GitHub repo.\n\nIn the next chapter, we'll start the pre-processing phase of any NLP project by removing stop words.","6fc74d95":"# Remove Stop Words From a Block of Text\n![](https:\/\/user.oc-static.com\/upload\/2021\/01\/06\/16099626487943_P1C2.png)\n\nThis chapter will guide you through removing stop words, and by the end, you will have created a much clearer version of the previous chapter's word cloud!\n\n## Count Word Frequencies\nFirst let's download the Wikipedia Earth page's content in a friendly text format using the wikipedia_page(title)function defined in the previous chapter.","c46ac05c":"## Install the Word Cloud Library\n\nTo generate word clouds in Python, use the word cloud library. You can read more about it on this Python Package Index (PyPI) page. Install it by running:\n\n* In a terminal: pip install wordcloudor conda install -c conda-forge wordcloud \n\n* In a Google Colab notebook:!pip install wordcloud\n\n### Import the Text Material\n\nFor this example, we will use content from a Wikipedia page, which you can download as raw text in a few lines of code without the inherent markup. Copy-paste the code below to get started. We will reuse the  wikipedia_page()  function later in the course.\n\nSince we're all earthlings, we will use the article on planet Earth, but I strongly encourage you to experiment with other topics and content as you follow along, such as Harry Potter, dogs,  Star Trek, or coffee.","8f965f95":"Then use  matplotlib  to display the word cloud as an image:","b8901f97":"The result:  word_counts  is a dictionary whose keys are all the text's different words and whose values are the number of times each word is present in the text.\n\nHere is the list of 20 most common words with  word_counts.most_common(20)  .","f9bea36c":"### Stop words\nare words that do not provide any useful information to infer content or nature. It may be either because they don't carry any meaning (prepositions, conjunctions, etc.) or because they are too frequent.\n\nCounting word frequencies is a three-step process:\n\n* The first step is to create a list of the most frequent words by splitting the text over the whitespace character ' ' with the function text.split(' ').\n\n* Then, count how many times each word appears in the text using the  Counter  function.","64c5bc66":"If Wikipedia isn't your thing, you can also go classic with Project Gutenberg. It is a library that holds thousands of free public domain texts that you can download in three lines of code:"}}