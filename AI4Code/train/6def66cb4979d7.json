{"cell_type":{"b18f6baf":"code","7451323b":"code","e26580bf":"code","232cb45c":"code","8f27cf72":"code","5ca97bf0":"code","ee457c58":"code","798055b0":"code","bf522df9":"code","a4e54400":"code","62cd787e":"code","ba241f5d":"code","8f3fea48":"code","63bd1151":"code","59a2cc6b":"code","1b7f7dfb":"code","8db43c23":"code","c61b7aeb":"code","50d7578c":"code","6bdcc861":"code","5a2bdfc5":"code","c56a6362":"code","66e9d3c9":"code","4e83c70c":"code","0f2f0a06":"code","6014f694":"code","d880339e":"code","74182e0a":"code","82e122e2":"code","1cef4fff":"code","33676d30":"code","9336b5bd":"code","02c2a2ac":"code","38a88b8c":"code","fe66d5d1":"code","d9af425d":"code","db72a245":"code","5a917d23":"code","6c43cfad":"code","27b007b6":"code","7772ebaa":"code","a30e4636":"code","d1518687":"code","336e53f9":"code","0178086b":"code","de0d7141":"code","4edaa160":"code","220318fb":"code","456d2e2b":"code","ed192d7a":"code","4ec3d75a":"code","8c1c4c47":"code","c830df91":"code","bcb9bb7c":"code","86ebc4dd":"code","7f9ebc44":"code","71312673":"code","5e788fe2":"code","a89f877d":"code","2a5c8c4e":"code","e7dad19d":"code","55ffddba":"code","6852dc9a":"code","6430ed7a":"code","8c63afb5":"code","271b3b82":"code","91d9e30d":"code","e92415c0":"code","36e30d9b":"code","48c1ad8c":"code","442e6d58":"code","fb55b988":"code","2596834d":"code","27aebcdc":"code","49bfd153":"code","49d9789d":"code","1909021e":"code","feb3b6ff":"code","78e54db4":"code","174c2033":"code","cafdcead":"code","aa5092cf":"code","8d68f5ef":"code","ce24bab0":"code","aa2dacfe":"code","88c04233":"code","ae54de9e":"code","d132a2ec":"code","90d842aa":"code","71fa2d2d":"code","1c91fae4":"code","f4aca180":"code","d5c4be0d":"code","7eddcddd":"code","e8369ea4":"code","20a2d099":"code","2a7abf97":"code","d58b6bd3":"code","85cb4477":"code","bca3902f":"code","8034ea11":"code","6e0206a9":"code","d812c453":"code","796605b6":"code","6a1ec727":"code","632cd99d":"code","85c9bb45":"code","bc9f478b":"code","57bbb607":"code","14b20772":"code","5d693a23":"code","866b1c63":"code","bbbeaba6":"code","4cd3bbf7":"code","3af6c896":"code","a0c2714c":"code","f2e6b037":"code","7080b261":"code","5e52ed87":"code","1fd5edfa":"code","454f9277":"code","192b54a7":"code","0ee26e52":"code","824915ef":"code","3c54c322":"code","4f594f06":"code","a5beaad1":"code","194359e5":"code","9ac50918":"code","cd2f1750":"code","da2d5af3":"code","9180061c":"code","2abab774":"code","f4842899":"code","f0b60edc":"code","4dcd5c06":"code","ea4437d9":"code","210315fd":"code","f49cf805":"code","468722b5":"code","c656d4bd":"code","775256db":"code","99e02996":"code","1854ac78":"code","14b4e2d0":"code","80ee04ed":"code","6318799e":"code","f00cac60":"code","418cd1a0":"code","45ffae84":"code","2f05d7e9":"code","597a8bef":"code","ac36cf64":"code","1b68c399":"markdown","2b72509b":"markdown","b397bac1":"markdown","2ab1fc51":"markdown","dbdeb37c":"markdown","f9ea7c27":"markdown","315a38a8":"markdown","de3637e5":"markdown","a3bd5417":"markdown","829cbc6b":"markdown","489a3d2a":"markdown","d6e2e8c9":"markdown","ce63f645":"markdown","7273cfbf":"markdown","c129ae03":"markdown","80b05551":"markdown","baeab1bd":"markdown","a6e53d18":"markdown","f1210231":"markdown","fbd29790":"markdown","acb7c781":"markdown","02903c87":"markdown","99d4d9fd":"markdown","9f461cbe":"markdown","bd4b123d":"markdown","3380780c":"markdown","b2de9361":"markdown","3e290c4c":"markdown","49f5991e":"markdown","af343a4f":"markdown","ac286f20":"markdown","0c6a1d6c":"markdown","819093ee":"markdown","f21aa30c":"markdown","a67fc68c":"markdown"},"source":{"b18f6baf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7451323b":"import pandas as pd\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport datetime\nimport seaborn as sns","e26580bf":"with open('..\/input\/car-crashes-severity-prediction\/holidays.xml') as file:\n    content = file.read()\n\ncontent","232cb45c":"content_splited = content.split('<date>')[1:]\ncontent_splited = \"\".join(content_splited).split('<\/date>')[:-1]\ncontent_splited","8f27cf72":"final = []\nfor record in content_splited:\n    final.append(record[-10:])\n\nfinal","5ca97bf0":"holidays_df = pd.DataFrame({'Holiday' : final})\nholidays_df","ee457c58":"holidays_df['Year'] = holidays_df['Holiday'].str.split('-').str[0]\nholidays_df['Month'] = holidays_df['Holiday'].str.split('-').str[1]\nholidays_df['Day'] = holidays_df['Holiday'].str.split('-').str[2]\n\nholidays_df","798055b0":"holidays_df['Month'] = holidays_df['Month'].astype(int).astype(str)\nholidays_df['Day'] = holidays_df['Day'].astype(int).astype(str)\n\nholidays_df","bf522df9":"holidays_df['Date'] = holidays_df['Month'] + str('\/') + holidays_df['Day'] + str('\/') + holidays_df['Year']\nholidays_df","a4e54400":"holidays = list(holidays_df['Date'])\nprint(holidays)","62cd787e":"Weather = pd.read_csv(\"..\/input\/car-crashes-severity-prediction\/weather-sfcsv.csv\", index_col='Weather_Condition')","ba241f5d":"Weather","8f3fea48":"# Description of weather file columns\nWeather.info()","63bd1151":"# Some statistical information about weather file\nWeather.describe()","59a2cc6b":"# Checking if there is null values in the weather file or not\n#Weather['Weather_Condition'].isnull().sum()","1b7f7dfb":"Weather_Duplicates = Weather.copy()","8db43c23":"Weather_Duplicates.duplicated(subset = ['Year', 'Day', 'Month', 'Hour']).sum()","c61b7aeb":"Duplicates = Weather_Duplicates.duplicated(subset = ['Year', 'Day', 'Month', 'Hour'])","50d7578c":"Weather_Duplicates[Duplicates].sort_values(by= ['Year', 'Month', 'Day', 'Hour'], axis =0)","6bdcc861":"#check\nwh = Weather_Duplicates[Duplicates]\nwh1= wh[wh['Year'] == 2020]\nwh2= wh1[wh1['Month'] == 5]\nwh2[wh2['Day'] ==17]","5a2bdfc5":"Weather_No_Duplicates = Weather_Duplicates.drop_duplicates(subset = ['Year', 'Day', 'Month', 'Hour'], inplace = False)","c56a6362":"Weather_No_Duplicates.duplicated(subset = ['Year', 'Day', 'Month', 'Hour']).sum()","66e9d3c9":"Data_no_Duplicates_copy = Weather_No_Duplicates.copy()","4e83c70c":"Weather_No_Duplicates.reset_index(inplace = True)","0f2f0a06":"# Checking if there is missing values in the \"Weather_Condition\" column\n\nWeather_No_Duplicates[Weather_No_Duplicates[\"Weather_Condition\"].isnull()]","6014f694":"# Remove rows with missing values in 'Weather_Condition' column\nWeather_No_Duplicates.dropna(axis=0, subset=['Weather_Condition'], inplace=True)","d880339e":"# Double checking if there is missing values in the \"Weather_Condition\" column\n\nWeather_No_Duplicates[Weather_No_Duplicates[\"Weather_Condition\"].isnull()]","74182e0a":"# Calculating the median of each column in the data\nWeather_Median = Weather_No_Duplicates.groupby(['Weather_Condition']).median()","82e122e2":"Weather_Median","1cef4fff":"# Checking if there is null values or nit\nWeather_Median.isnull().sum()","33676d30":"# exploring the records of the \"Squalls\" category\nWeather_No_Duplicates[Weather_No_Duplicates[\"Weather_Condition\"] == \"Squalls\"]","9336b5bd":"# exploring the records of the \"Light Thunderstorms and Rain\" category\n\nWeather_No_Duplicates[Weather_No_Duplicates[\"Weather_Condition\"] == \"Light Thunderstorms and Rain\"]","02c2a2ac":"# exploring the records of the \"Light Drizzle\" category\n\nWeather_No_Duplicates[Weather_No_Duplicates[\"Weather_Condition\"] == \"Light Drizzle\"]","38a88b8c":"# exploring the records of the \"Clear\" category\n\nWeather_No_Duplicates[Weather_No_Duplicates[\"Weather_Condition\"] == \"Clear\"]","fe66d5d1":"# Make a copy of Weather_Median data frame\nWeather_Median_copy = Weather_Median.copy()","d9af425d":"# Split both 'Wind_Chill(F)' & 'Precipitation(in)' in different DF\nWind_Chill = Weather_Median_copy['Wind_Chill(F)']\nPrecipitation = Weather_Median_copy['Precipitation(in)']","db72a245":"(Wind_Chill)","5a917d23":"print(Precipitation)","6c43cfad":"Wind_Chill.dropna(axis =0)","27b007b6":"Precipitation.dropna(axis = 0)","7772ebaa":"print(Wind_Chill.median())\nprint(Precipitation.median())","a30e4636":"Weather_Median['Wind_Chill(F)'].fillna(Wind_Chill.median(), inplace = True)\nWeather_Median['Precipitation(in)'].fillna(Precipitation.median(), inplace = True)","d1518687":"Weather_Median","336e53f9":"Weather_Median.isnull().sum()","0178086b":"# converting to dict\nWeather_Median_dict = Weather_Median.to_dict()\n  \n# display\nWeather_Median_dict","de0d7141":"# checking null values\nData_no_Duplicates_copy.isnull().sum()","4edaa160":"Weather_Imputed = Data_no_Duplicates_copy.fillna(value=Weather_Median_dict)","220318fb":"Weather_Imputed","456d2e2b":"# checking null values again\nWeather_Imputed.isnull().sum()","ed192d7a":"# exploring columns with missing records\nWeather_Imputed[Weather_Imputed['Wind_Chill(F)'].isnull()]","4ec3d75a":"# reset index column to the default indexing\nWeather_Imputed.reset_index(inplace = True)","8c1c4c47":"Weather_Imputed","c830df91":"Weather_Imputed.dropna(axis=0, subset=['Weather_Condition'], inplace=True)","bcb9bb7c":"Weather_Imputed.isnull().sum()","86ebc4dd":"Weather_Imputed","7f9ebc44":"Weather_Imputed['Selected'] = pd.factorize(Weather_Imputed['Selected'])[0].reshape(-1, 1)","71312673":"Weather_Imputed","5e788fe2":"Weather_Imputed.info()","a89f877d":"# Creating 'Date & Hour' column to merge the 2 DataFrames through\nWeather_Imputed['Date & Hour'] = Weather_Imputed['Month'].astype(str) + str('\/') + Weather_Imputed['Day'].astype(str) + str('\/') + Weather_Imputed['Year'].astype(str) + str('-') + Weather_Imputed['Hour'].astype(str)\nWeather_Imputed.head(5)","2a5c8c4e":"Weather_Imputed_final = Weather_Imputed.copy()\nWeather_Imputed_final= Weather_Imputed_final.drop(['Year', 'Day', 'Month', 'Hour'], axis=1)\nWeather_Imputed_final.info()","e7dad19d":"Weather_Imputed_final.head()","55ffddba":"train_df = pd.read_csv('..\/input\/car-crashes-severity-prediction\/train.csv', index_col='ID')\ntrain_df.head()","6852dc9a":"test_df = pd.read_csv('..\/input\/car-crashes-severity-prediction\/test.csv', index_col='ID')\ntest_df.head()","6430ed7a":"train_df.info()","8c63afb5":"train_df.describe()","271b3b82":"train_df['Bump'].value_counts()","91d9e30d":"train_df['Distance(mi)'].hist(bins=10, range=[0.0, 1]);\nplt.title('Distances of Traffic Jam Provoked by an Accident (mi)');","e92415c0":"train_df['Severity'].hist(bins=4);\nplt.title('Accidents Severity Degree');","36e30d9b":"train_df['Severity'].value_counts()","48c1ad8c":"train_df['Crossing'].value_counts().plot(kind='barh');\nplt.title('If Crossing the Road');","442e6d58":"train_df.groupby(['Crossing'])['Severity'].value_counts()","fb55b988":"train_df['Give_Way'].value_counts()","2596834d":"train_df[train_df['Give_Way'] == True]","27aebcdc":"train_df['Junction'].value_counts().plot(kind='barh');\nplt.title('Accident happend on Junction or Not');","49bfd153":"train_df.groupby(['Junction'])['Severity'].value_counts()","49d9789d":"train_df['No_Exit'].value_counts()","1909021e":"train_df['Railway'].value_counts().plot(kind='barh');\nplt.title('Accident happend on Railway or Not');","feb3b6ff":"train_df['Railway'].value_counts()","78e54db4":"train_df['Roundabout'].value_counts()","174c2033":"train_df['Stop'].value_counts().plot(kind='barh');\nplt.title('Accident happend on road with Stop Sign or Not');","cafdcead":"train_df['Stop'].value_counts()","aa5092cf":"train_df['Amenity'].value_counts().plot(kind='barh');","8d68f5ef":"train_df['Amenity'].value_counts()","ce24bab0":"df_train_full_copy = train_df.copy()","aa2dacfe":"df_test_full_copy = test_df.copy()","88c04233":"#Converting the boolean columns into binary columns\nfor col in train_df.columns: \n    if train_df[col].dtype == 'bool':\n        df_train_full_copy[col] = train_df[col].astype(int)\n\ndf_train_full_copy.head()","ae54de9e":"#Converting the boolean columns into binary columns\nfor col in test_df.columns: \n    if test_df[col].dtype == 'bool':\n        df_test_full_copy[col] = test_df[col].astype(int)\n\ndf_test_full_copy.head()","d132a2ec":"df_train_full_copy['Side'] = df_train_full_copy['Side'].map({'L': 1, 'R': 0})\ndf_train_full_copy.head()","90d842aa":"df_test_full_copy['Side'] = df_test_full_copy['Side'].map({'L': 1, 'R': 0})\ndf_test_full_copy.head()","71fa2d2d":"timestamp = (df_train_full_copy['timestamp'].values).reshape(-1, 1)\ndate_list = []\ntime_list = []\nfor ts in timestamp:\n    ts = str(ts)\n    ts = ts.replace('[', '')\n    ts = ts.replace(']', '')\n    ts = ts.replace(' ', '')\n    ts = ts.replace(\"'\", \"\")\n    d = ts[:10]\n    t = ts[10:]\n    date_list.append(d)\n    time_list.append(t)\n    \n    \nyear_list = []\nmonth_list = []\nday_list = []\nfull_date_list = []\nfor d in date_list:\n    y = d[:4]\n    m = d[5:7]\n    day = d[8:]\n    full_date = str(int(m)) +'\/'+ str(int(day)) +'\/'+ y\n    year_list.append(y)\n    month_list.append(m)\n    day_list.append(day)\n    full_date_list.append(full_date)\n    \n    \ntime_stamp = []\nfor i in range(len(year_list)):\n    d = month_list[i]+day_list[i]+year_list[i]\n\n    ts = pd.to_datetime(d, format='%m%d%Y')\n    time_stamp.append(ts)\n    \n    \nday_name_list = []\nfor ts in time_stamp:\n    day_name_list.append(ts.day_name())\n   \n\ndf_train_full_copy['year'] = year_list\ndf_train_full_copy['month'] = month_list\ndf_train_full_copy['day'] = day_list\ndf_train_full_copy['time'] = time_list\ndf_train_full_copy['day_name'] = day_name_list\ndf_train_full_copy['full_date'] = full_date_list\n\ndf_train_full_copy","1c91fae4":"timestamp2 = (df_test_full_copy['timestamp'].values).reshape(-1, 1)\ndate_list2 = []\ntime_list2 = []\nfor ts in timestamp2:\n    ts = str(ts)\n    ts = ts.replace('[', '')\n    ts = ts.replace(']', '')\n    ts = ts.replace(' ', '')\n    ts = ts.replace(\"'\", \"\")\n    d = ts[:10]\n    t = ts[10:]\n    date_list2.append(d)\n    time_list2.append(t)\n    \n    \nyear_list2 = []\nmonth_list2 = []\nday_list2 = []\nfull_date_list2 = []\nfor d in date_list2:\n    y = d[:4]\n    m = d[5:7]\n    day = d[8:]\n    full_date = str(int(m)) +'\/'+ str(int(day)) +'\/'+ y\n    year_list2.append(y)\n    month_list2.append(m)\n    day_list2.append(day)\n    full_date_list2.append(full_date)\n    \n    \ntime_stamp2 = []\nfor i in range(len(year_list2)):\n    d = month_list2[i]+day_list2[i]+year_list2[i]\n\n    ts = pd.to_datetime(d, format='%m%d%Y')\n    time_stamp2.append(ts)\n    \n    \nday_name_list2 = []\nfor ts in time_stamp2:\n    day_name_list2.append(ts.day_name())\n   \n\ndf_test_full_copy['year'] = year_list2\ndf_test_full_copy['month'] = month_list2\ndf_test_full_copy['day'] = day_list2\ndf_test_full_copy['time'] = time_list2\ndf_test_full_copy['day_name'] = day_name_list2\ndf_test_full_copy['full_date'] = full_date_list2\n\ndf_test_full_copy","f4aca180":"df_train_full_copy['Hour'] = df_train_full_copy['time'].str[:2]\ndf_train_full_copy = df_train_full_copy.drop('time', axis=1)\ndf_train_full_copy","d5c4be0d":"df_test_full_copy['Hour'] = df_test_full_copy['time'].str[:2]\ndf_test_full_copy = df_test_full_copy.drop('time', axis=1)\ndf_test_full_copy","7eddcddd":"# day_of_week_dummy = pd.get_dummies(df_train_full_copy['day_name'])\n# df_train_full_copy = df_train_full_copy.join(day_of_week_dummy)\n# df_train_full_copy","e8369ea4":"df_train_full_copy['day_name'] = pd.factorize(df_train_full_copy['day_name'])[0].reshape(-1, 1)","20a2d099":"df_train_full_copy.head()","2a7abf97":"# day_of_week_dummy2 = pd.get_dummies(df_test_full_copy['day_name'])\n# df_test_full_copy = df_test_full_copy.join(day_of_week_dummy2)\n# df_test_full_copy","d58b6bd3":"df_test_full_copy['day_name'] = pd.factorize(df_test_full_copy['day_name'])[0].reshape(-1, 1)\ndf_test_full_copy.head()","85cb4477":"df_train_full_copy.info()","bca3902f":"df_train_full_copy['year'] = df_train_full_copy['year'].astype(int)\ndf_train_full_copy['month'] = df_train_full_copy['month'].astype(int)\ndf_train_full_copy['day'] = df_train_full_copy['day'].astype(int)\ndf_train_full_copy['Hour'] = df_train_full_copy['Hour'].astype(int)","8034ea11":"df_test_full_copy['year'] = df_test_full_copy['year'].astype(int)\ndf_test_full_copy['month'] = df_test_full_copy['month'].astype(int)\ndf_test_full_copy['day'] = df_test_full_copy['day'].astype(int)\ndf_test_full_copy['Hour'] = df_test_full_copy['Hour'].astype(int)","6e0206a9":"df_train_full_copy.info()","d812c453":"df_train_full_copy['is_holiday'] = df_train_full_copy['full_date'].apply(lambda x: 1 if x in holidays else 0)\ndf_train_full_copy['week_end'] = df_train_full_copy['day_name'].apply(lambda x: 1 if x == 'Saturday' or x == 'Sunday' else 0)\n\n# # df_train_full_copy['is_holiday'] = df_train_full_copy['is_holiday'] + df_train_full_copy['week_end']\n# df_train_full_copy['is_holiday'] = df_train_full_copy['is_holiday'].map({0:0, 1:1, 2:1})\n# df_train_full_copy = df_train_full_copy.drop('week_end', axis=1)\n\ndf_train_full_copy['is_holiday'].value_counts()","796605b6":"df_test_full_copy['is_holiday'] = df_test_full_copy['full_date'].apply(lambda x: 1 if x in holidays else 0)\ndf_test_full_copy['week_end'] = df_test_full_copy['day_name'].apply(lambda x: 1 if x == 'Saturday' or x == 'Sunday' else 0)\n\n# df_test_full_copy['is_holiday'] = df_test_full_copy['is_holiday'] + df_test_full_copy['week_end']\n# df_test_full_copy['is_holiday'] = df_test_full_copy['is_holiday'].map({0:0, 1:1, 2:1})\n# df_test_full_copy = df_test_full_copy.drop('week_end', axis=1)\n\ndf_test_full_copy['is_holiday'].value_counts()","6a1ec727":"df_train_full_copy.info()","632cd99d":"df_train_full_copy['day_name'].value_counts().plot(kind='barh');\nplt.title('Accidents on Every Day');","85c9bb45":"df_train_full_copy.groupby(['day_name'])['Severity'].value_counts()","bc9f478b":"plt.figure(figsize=(8,8))\ndf_train_full_copy['Hour'].value_counts().plot(kind='barh');\nplt.title('Accidents on Every Hour');","57bbb607":"df_train_full_copy.columns","14b20772":"df_train_full_copy['is_holiday'].value_counts().plot(kind='barh');\nplt.title('Accidents in Holidays');","5d693a23":"df_train_full_copy['is_holiday'].value_counts()\/df_train_full_copy.shape[0]","866b1c63":"df_train_full_copy.groupby(['is_holiday'])['Severity'].value_counts()","bbbeaba6":"df_train_full_copy.head()","4cd3bbf7":"df_train_full_copy['Date & Hour'] = df_train_full_copy['full_date'] + str('-') + df_train_full_copy['Hour'].astype(str)","3af6c896":"df_test_full_copy['Date & Hour'] = df_test_full_copy['full_date'] + str('-') + df_test_full_copy['Hour'].astype(str)","a0c2714c":"df_train_full_copy['Date & Hour']","f2e6b037":"df_train_full_copy.head()","7080b261":"Weather_Imputed_final.info()","5e52ed87":"full_data = df_train_full_copy.merge(Weather_Imputed_final,how= 'left', on='Date & Hour')","1fd5edfa":"full_data.isnull().sum()","454f9277":"full_data_test = df_test_full_copy.merge(Weather_Imputed_final, how='left', on='Date & Hour')","192b54a7":"full_data.info()","0ee26e52":"full_data_clean = full_data.drop(['full_date', 'timestamp', 'Date & Hour'], axis=1)\nfull_data_clean","824915ef":"full_data_test_clean = full_data_test.drop(['full_date', 'timestamp', 'Date & Hour'], axis=1)\nfull_data_test_clean","3c54c322":"full_data_clean.info()","4f594f06":"full_data_clean['Temperature(F)'].value_counts()","a5beaad1":"full_data_test_clean.info()","194359e5":"full_data_clean['Weather_Condition'].value_counts()","9ac50918":"full_data_clean['Wind_Chill(F)'].hist(bins=15);\nplt.title('Accidents Wind Chhill Temperature in F');","cd2f1750":"full_data_clean['Precipitation(in)'].value_counts()","da2d5af3":"full_data_clean['Weather_Condition'] = pd.factorize(full_data_clean['Weather_Condition'])[0].reshape(-1,1)\nfull_data_clean.info()","9180061c":"full_data_test_clean['Weather_Condition'] = pd.factorize(full_data_test_clean['Weather_Condition'])[0].reshape(-1,1)\nfull_data_test_clean.info()","2abab774":"full_data_clean.dropna(inplace=True)","f4842899":"full_data_test_clean.dropna(inplace=True)","f0b60edc":"full_data_clean.isnull().sum()","4dcd5c06":"# full_data_clean.columns","ea4437d9":"# full_data_clean.columns","210315fd":"# full_data_clean_2, full_data_test_clean_2 = full_data_clean.align(full_data_test_clean, join='inner', axis=1)","f49cf805":"# full_data_clean_2.columns","468722b5":"df_test_full_copy['is_holiday'] = df_test_full_copy['full_date'].apply(lambda x: 1 if x in holidays else 0)\n","c656d4bd":"full_data_clean['is_rush'] = full_data_clean['Hour'].apply(lambda x: 1 if x  in [14, 15, 16, 17, 18] else 0)","775256db":"full_data_test_clean['is_rush'] = full_data_test_clean['Hour'].apply(lambda x: 1 if x  in [14, 15, 16, 17, 18] else 0)","99e02996":"full_data_clean['Severity'].value_counts()","1854ac78":"full_data_test_clean.columns","14b4e2d0":"# submission_columns = ['Lat', 'Lng', 'Distance(mi)', 'Crossing',\n#        'Junction', 'Stop', 'Amenity',\n#        'Side', 'year', 'month', 'day', 'Hour', 'Friday', 'Monday',\n#        'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', 'is_holiday',\n#        'Weather_Condition', 'Wind_Chill(F)', 'Temperature(F)', 'Humidity(%)', 'Wind_Speed(mph)']\n\n# submission_columns = ['Lat', 'Lng', 'Distance(mi)', 'Crossing', 'Junction', \n#                       'Railway', 'Stop', 'Amenity', 'Side',\n#                       'year', 'month', 'day', 'Hour', 'Friday', 'Monday',\n#                        'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', 'is_holiday',\n#                        'Weather_Condition', 'Wind_Chill(F)',  'Visibility(mi)', 'week_end']\n\n# submission_columns = ['Lat', 'Lng', 'Bump', 'Distance(mi)', 'Crossing', 'Give_Way',\n#        'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Stop', 'Amenity',\n#        'Side', 'year', 'month', 'day', 'day_name', 'Hour']\n\n# submission_columns = ['Lat', 'Lng',\n#                        'Side','day_name', 'year', 'month', 'day',  'Hour', 'is_holiday',\n#                        'week_end', 'Weather_Condition', 'Wind_Chill(F)', 'Precipitation(in)',\n#                        'Temperature(F)', 'Humidity(%)', 'Wind_Speed(mph)', 'Visibility(mi)',\n#                        'Selected']\n\n\n\n# submission_columns = ['Lat', 'Lng', 'Distance(mi)', 'Crossing',\n#        'Junction', 'Railway', 'Amenity',\n#        'Side', 'year', 'month', 'day_name', 'is_holiday',\n#        'week_end', 'Weather_Condition', 'Precipitation(in)',\n#        'Temperature(F)', 'Humidity(%)', 'Wind_Speed(mph)', 'Visibility(mi)', 'is_rush'] \n\n\n\n\n\nsubmission_columns = [ 'Lat', 'Lng', 'is_rush',\n                       'Side','day_name', 'year', 'month',  'Hour', 'is_holiday',\n                       'week_end', 'Weather_Condition', 'Wind_Chill(F)', 'Precipitation(in)', 'Visibility(mi)','Distance(mi)']","80ee04ed":"test_columns = submission_columns.copy()\ntest_columns.append('Severity')\ndf = full_data_clean[test_columns]\nfull_data_clean","6318799e":"from sklearn.model_selection import train_test_split\n\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[['Severity']]) # Try adding `stratify` here\n\nX_train = train_df.drop(columns=['Severity'])\ny_train = train_df['Severity']\n\nX_val = val_df.drop(columns=['Severity'])\ny_val = val_df['Severity']","f00cac60":"from sklearn.ensemble import RandomForestClassifier\n\n# Create an instance of the classifier\nclassifier = RandomForestClassifier(max_depth=2, random_state=0)\n\n# Train the classifier\nclassifier = classifier.fit(X_train, y_train)","418cd1a0":"print(\"The accuracy of the classifier on the validation set is \", (classifier.score(X_val, y_val)))\n\n","45ffae84":"# X_test = test_df.drop(columns=['ID'])\nX_test = full_data_test_clean.copy()\n\n# You should update\/remove the next line once you change the features used for training\nX_test = X_test[submission_columns]\ny_test_predicted = classifier.predict(X_test)\ntest_df['Severity'] = y_test_predicted\ntest_df.head()\n\ny_test_predicted","2f05d7e9":"sub_df = test_df['Severity']\n\nsub_df = pd.DataFrame(sub_df)","597a8bef":"sub_df['ID'] = sub_df.index","ac36cf64":"sub_df[['ID', 'Severity']].to_csv('\/kaggle\/working\/submission.csv', index=False)","1b68c399":"- Most of accident `doesn't happen on Railways`, but inside the town.\n- This feature will have `weak impact` on modeling.","2b72509b":"### Train & Holidays Data Set Manupilations","b397bac1":"### Predicting ","2ab1fc51":"### Duplicates","dbdeb37c":"'Friday',### Merging DataFrames together","f9ea7c27":"- Almost all the records of 'Give_Way' column has the same value, so it's `impact` will be `nothing` to model prediction, `we can drop it`.","315a38a8":"- Most of accident `doesn't happen on Junctions`, but in entire road.\n- It seems that accidents `numbers and severity decreases` on Junctions.","de3637e5":"### Weather Data Set Transformation","a3bd5417":"- Number of accidents `decreases` in `weekend` and `increases` in the `middle` of the week.","829cbc6b":"- Most of accidents `not caued on crossing` the road, but during driving on it.\n- It seems that accidents `number and severity decreases` during crossing.","489a3d2a":"- Almost all the records of 'No_Exit' column has the same value, so it's `impact` will be `nothing` to model prediction, `we can drop it`.","d6e2e8c9":"- It seems that number of accidents `increases in working days 4 times than holidays`.","ce63f645":"### Predicting on the Test dataset","7273cfbf":"- Most accidents occurs in `Partly and Mostly Cloudy` weather.\n- This feature will be `valuable`.","c129ae03":"### Extra EDA","80b05551":"### Train Data Set Transformation","baeab1bd":"- All records of 'Roundabout' column has the same value so it will have `no impact` in model prediction, so we will `drop it`.","a6e53d18":"### As we saw a lot of missing values exist in the data file, So we need to impute all of them by:\n#### 1) we will group the data file by \"Weather_Condition\" column\n#### 2) we will calculate the median of each column in the data file, to impute the missing records with these values","f1210231":"### Dropping duplicates","fbd29790":"## Import the libraries\n\nWe'll use `pandas` to load and manipulate the data. Other libraries will be imported in the relevant sections.","acb7c781":"- Most of accidents cause `traffic jam` of distances in `range 0.1 mi`, about `150 meter`.","02903c87":"# - Most Accedents happens in range `60F degree`.","99d4d9fd":"### Holidays File","9f461cbe":"- Now, there is `no missing values` in the \"Weather_Median\" data frame, so it's `ready now` to be in use. ","bd4b123d":"- Most of accidents happen with cars with `no Amenity`.\n- This feature will have `weak impact` on modeling.","3380780c":"- Most of accidents happens with `severity degree of 2 or 3`.","b2de9361":"### Exploring the missing records in the \"Weather_Median\" data frame","3e290c4c":"- It seems that number of accidents increases between `14 to 18`","49f5991e":"- All records of 'Bump' column has the same value so it will have `no impact` in model prediction, so we will `drop it`.","af343a4f":"### Extra EDA","ac286f20":"### Grouping by 'Weather_Condition' column and calculating median","0c6a1d6c":"- Most of accidents `doesn't happen` on roads with `Stop Sign`.","819093ee":"### Imputing the missing values in Weather_Median data frame","f21aa30c":"### Weather data (main file) imputation","a67fc68c":"### EDA"}}