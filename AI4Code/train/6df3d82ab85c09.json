{"cell_type":{"277b9e44":"code","4292c969":"code","8338376d":"code","e8c0e0bb":"code","a2b912cc":"code","efc1ffb1":"code","427ff0ec":"code","2328900a":"code","867f6a01":"code","94ab98e5":"code","16b25d02":"code","113de7b5":"code","259fe2a2":"code","c27e9f4c":"code","e31289b0":"code","9db80709":"code","9a2ba108":"code","a8df9474":"code","a5099d8f":"code","9e2f0a26":"code","98f034f8":"code","3df82d51":"code","edb38f10":"code","0d9c286d":"code","a8ae5057":"code","cf2d05ad":"code","dc32bb13":"code","4b4587bc":"code","cee8482e":"code","069016a6":"code","1ca56210":"code","b2ee6dd3":"code","9164dcc8":"code","8db17a4e":"code","42ec7511":"code","5ce0f283":"code","db7faa69":"code","2be9e929":"code","d24eff02":"code","0cee2bca":"code","39de4488":"code","0b9afe3d":"code","f95d4aa5":"code","20e676c3":"code","b1733760":"code","e8d87a18":"code","253e1c13":"code","e2b39bca":"code","67dcf374":"code","0286b8b1":"code","b031713e":"code","ea657e48":"code","ed68d10c":"code","6307ae49":"code","05772ab5":"code","0f194f3b":"code","b0038bcd":"code","40409f65":"code","99672f44":"code","f475f299":"code","186cf273":"code","d629dedf":"code","04973aae":"code","aa606d93":"code","7ed8973e":"code","bdde1355":"code","10600e42":"code","64e9f94f":"code","aab2d7f5":"code","b5fbbe33":"code","2a507af6":"code","306e7926":"markdown","301c287f":"markdown","fe022994":"markdown","b15866b5":"markdown","16cc10eb":"markdown","dd2f4c6a":"markdown","803fdc11":"markdown","32d28c91":"markdown","71404ee0":"markdown","db030108":"markdown","e40dac80":"markdown","fdec5b61":"markdown","0722a5e5":"markdown","5b93b3f6":"markdown","21792fef":"markdown","f80b436c":"markdown","7a37900d":"markdown","386ba9ad":"markdown","7de4ce30":"markdown","6824695d":"markdown","855fba05":"markdown","a5176bf8":"markdown","48c0bd03":"markdown","19cb6816":"markdown"},"source":{"277b9e44":"import pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport datetime\nimport math","4292c969":"rsv = pd.read_csv('\/kaggle\/input\/goodreadsbooks\/books.csv', error_bad_lines=False)","8338376d":"rsv.head()","e8c0e0bb":"fn = rsv.authors.unique()\nthresh = 0\nwriters = dict()\nfor f in fn:\n    fc = rsv[rsv.authors == f].bookID.count() \n    if fc >= thresh: \n        writers[f] = fc","a2b912cc":"writers = dict(sorted(writers.items(), key=lambda item: item[1], reverse=True)[:10])","efc1ffb1":"plt.figure(figsize=(40, 40))\nplt.barh(range(len(writers)), list(writers.values()), align='center')\nplt.yticks(range(len(writers)), list(writers.keys()), fontsize=30)\nplt.show();","427ff0ec":"title_arr = []\nwriters_arr = []\nsingle_writer = []\nfor f in fn:\n    if '\/' in f:\n        try:\n            book = rsv[rsv.authors == f].title.values[0]\n            if book in title_arr:\n                if len(writers_arr[title_arr.index(book)]) < len(f.split('\/')):\n                    writers_arr[title_arr.index(book)] = f.split('\/')\n            else:\n                title_arr.append(book)\n                writers_arr.append(f.split('\/'))\n        except IndexError:\n            title_arr.append(rsv[rsv.authors == f].title.values[0])\n            writers_arr.append(f.split('\/'))","2328900a":"len(title_arr)","867f6a01":"len(set(title_arr))","94ab98e5":"unique_writes = set()\ncount = 0\nmax_len = 0\nfor x,y  in zip(title_arr, writers_arr):\n    if len(y) > max_len:\n        max_len = len(y)\n    for z in y:\n        unique_writes.add(z)\n        count+=1","16b25d02":"for k in writers.keys():\n    if '\/' not in k:\n        unique_writes.add(k)\n        count+=1","113de7b5":"len(unique_writes)","259fe2a2":"count","c27e9f4c":"get_books = dict.fromkeys(unique_writes, 0)","e31289b0":"for x,y in zip(title_arr, writers_arr):\n    for z in y:\n        get_books[z]+=1","9db80709":"for x in writers.items():\n    if '\/' not in x[0]:\n        get_books[x[0]]+=x[1]","9a2ba108":"rsv.sort_values(by=['ratings_count'],\n                ascending=False).loc[:,(\"title\",\"average_rating\")][:10]","a8df9474":"get_books = dict(sorted(get_books.items(), key=lambda item: item[1], reverse=True))","a5099d8f":"plt.figure(figsize=(40, 40));\nplt.barh(range(10), list(get_books.values())[:10], align='center')\nplt.yticks(range(10), list(get_books.keys())[:10], fontsize=30)\nplt.xticks(range(100), range(100), fontsize=25, rotation=90)\nplt.show();","9e2f0a26":"dates = []\nfor x in rsv.publication_date.values:\n    try:\n        dates.append(pd.to_datetime(datetime.datetime.strptime(x,\"%m\/%d\/%Y\")))\n    except:\n        dates.append(pd.to_datetime(datetime.datetime.now()))","98f034f8":"rsv[\"New dates\"] = dates","3df82d51":"rsv.sort_values(by='New dates',ascending=False).loc[:,(\"title\",\"New dates\")]","edb38f10":"rsv[\"Rating_Interval\"] = pd.DataFrame(pd.cut(rsv.average_rating, 5, [0.0,1.0,2.0,3.0,4.0,5.0]))","0d9c286d":"ratings = dict()\nfor i in range(0,5,1):\n    ratings[str(i)+\" to \"+str(i+1)] = (rsv[\"Rating_Interval\"] == pd.Interval(left = float(i), right = float(i+1))).sum()","a8ae5057":"del ratings['0 to 1']","cf2d05ad":"plt.figure(figsize=(40, 40));\nplt.barh(range(len(ratings)), list(ratings.values()), align='center')\nplt.yticks(range(len(ratings)), list(ratings.keys()), fontsize=30)\n# plt.yticks(range(50), range(50), fontsize=30)\nplt.show();","dc32bb13":"lang = dict()","4b4587bc":"rsv.language_code.unique()","cee8482e":"lang[\"eng\"] = 0","069016a6":"skip = [\"eng\",\"en-US\",\"en-CA\",\"en-GB\"]","1ca56210":"unq_lang = rsv.language_code.unique()","b2ee6dd3":"for x in unq_lang:\n    if x not in skip:\n        lang[x] = rsv[rsv.language_code == x].bookID.count()\n    if x in skip:\n        lang[\"eng\"]+=rsv[rsv.language_code == x].bookID.count()","9164dcc8":"plt.figure(figsize=(40, 40))\nplt.barh(range(len(lang)), width=sorted(list(lang.values()))[::-1], align='center')\nplt.yticks(range(len(lang)), list(lang.keys()), fontsize=30)\nplt.show();","8db17a4e":"del lang[\"eng\"]","42ec7511":"plt.figure(figsize=(40, 40))\nplt.barh(range(len(lang)), width = sorted(list(lang.values()))[::-1], align='center')\nplt.yticks(range(len(lang)), list(lang.keys()), fontsize=30)\nplt.show();","5ce0f283":"rsv = rsv.set_index('bookID')","db7faa69":"rsv = rsv.drop([45531, 31373])","2be9e929":"l = rsv.publisher.value_counts()\nl = l[l >= 20]","d24eff02":"plt.figure(figsize=(20, 20))\nplt.barh(range(10), width=l[:10])\nplt.yticks(range(10), l.index.values[:10], fontsize= 25)\nplt.show()","0cee2bca":"def calculate(word):\n    return ord(word)","39de4488":"def encode(strs):\n    if '\/' in strs:\n        final_sum = 0\n        intr = strs.split('\/')\n        for s in intr:\n            noramlize = len(s)\n            summation = sum([calculate(x) for x in s])\n        final_sum+=(summation\/noramlize)\n        return final_sum\n    else:\n        return sum([calculate(x) for x in strs])\/len(strs)","0b9afe3d":"rsv.insert(2,'Encoded authors', rsv.authors.apply(encode))","f95d4aa5":"encode(rsv.authors.values[0])","20e676c3":"def encode_title(tt):\n    total = 0\n    for w in tt.split(' '):\n        if len(w) == 0:\n            continue\n        total+= sum([calculate(x)*math.log2(lg + len(w)) for lg,x in enumerate(w)])\/ len(w)\n    return total\/len(tt)","b1733760":"l = []\nfor x in rsv.title.values:\n    l.append(encode(x))","e8d87a18":"len(l), len(set(l))","253e1c13":"rsv.insert(1,'Encoded_titles',rsv.title.apply(encode_title)\/rsv.average_rating.values)","e2b39bca":"rsv.head()","67dcf374":"rsv.columns","0286b8b1":"x = rsv[['Encoded_titles', 'Encoded authors', 'average_rating', '  num_pages','ratings_count','text_reviews_count', 'New dates']]","b031713e":"x","ea657e48":"x['New dates'] = x['New dates'].astype(np.int64)","ed68d10c":"x['New dates'].astype(np.float64)","6307ae49":"x = x.reset_index()","05772ab5":"x = x.drop('bookID',axis=1)","0f194f3b":"x['New dates'] = x['New dates'] \/\/  10**12","b0038bcd":"x.dtypes","40409f65":"x = x[x['New dates'] > 0]","99672f44":"x = x[x['Encoded_titles'] != np.inf]","f475f299":"wcss = [] \nfor i in range(1,11):\n    clusters = KMeans(n_clusters = i, random_state = 42) \n    clusters.fit(x.values)\n    wcss.append(clusters.inertia_)","186cf273":"plt.plot(range(1,11),wcss,'b')\nplt.title('This is Clustering')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.plot(4,wcss[3],'.',mew = 4, ms =8,color = 'r')\nplt.annotate(xy = [4,wcss[3]],s='(%.2f , %.1f) Seems to be Optimal'%(4,wcss[3]))\nplt.vlines(4,0,wcss[3],linestyle='dashed')\nplt.hlines(wcss[3],0,4,linestyle='dashed')\nplt.xlim(0,None)\nplt.ylim(0,None)","d629dedf":"clusters = KMeans(n_clusters=4, random_state=42)","04973aae":"x['cluster'] = clusters.fit_predict(x)","aa606d93":"x","7ed8973e":"def find_mins(df, thresh):\n    indexes = dict()\n    for i in range(df.shape[0]):\n        indexes[df.iloc[i,1]] = i\n    one = []\n    two = []\n    for x,y in indexes.items():\n        if x <= thresh:\n            one.append(x)\n        else:\n            two.append(x)\n    one = sorted(one, reverse=True)[:10]\n    two = sorted(two)[:10]\n    get_one = [df.iloc[indexes[val]] for val in one] \n    get_two = [df.iloc[indexes[val]] for val in two]\n    see = pd.DataFrame((get_one + get_two))\n    return see","bdde1355":"def find_inarr(df,n):\n    lol = []\n    for i in range(df.shape[0]):\n        lol.append(rsv[rsv['Encoded_titles'] == df.iloc[i, 0]])\n    lol = pd.concat(lol)\n    if n > lol.shape[0]:\n        n = lol.shape[0]\n    return lol.sort_values([\"  num_pages\"], ascending=False).iloc[:n]","10600e42":"def suggest(df,n=1):\n    selector = x[x['cluster'] == df.iloc[-1]]\n    finals = selector[selector['Encoded authors'] == df['Encoded authors']]\n    if finals.shape[0] == 1:\n        finals = find_mins(selector, df['Encoded authors'])\n    middle = finals - df\n    suggest = middle.abs().sort_values([\"Encoded_titles\",\"average_rating\",\"New dates\"]).iloc[:-1,:]\n    suggest = finals.loc[suggest.index]\n#     print(suggest.iloc[:, 0])\n    return find_inarr(suggest,n)","64e9f94f":"rsv[rsv['Encoded_titles'] == x.iloc[1311,0]]","aab2d7f5":"suggest(x.iloc[1311], 2)","b5fbbe33":"rsv[rsv['Encoded_titles'] == x.iloc[5,0]]","2a507af6":"suggest(x.iloc[5])","306e7926":"## Find correct values\n* This function tries to find the colums which have similiarty with Encoded titles\n* Also if n is greater than the total space of the selected items it returns all selected\n* Sorting with **number of Pages** so that we get most fat book","301c287f":"# Encoding titles\nThis function was made to stand out with log and bias to length of the title,****Because**** as follows:->\n* A title depends on it's length(talking syntaxically)\n* And moreover it depends upon arrangement of words **such  as(\"a after p\" or \"p after a\") are different things**.","fe022994":"## Main Suggestion Function\n**Before any of below a clustering is done already, so we do this within the same cluster**\n* This function is the actual suggestion function\n* It first finds the same author as the book passed\n* IF yes, returns the minium **diff value** that is matematically the nearest point\n* IF no, we construct a new dataframe for final according to above functions\n* When these are parse the values are sorted with **Titles** **Rating** and **Date Published**\n* And final is to returned according to number of pages","b15866b5":"## Observing Above graph\nEnglish is most dominating language,\nbut in the next bar graph Down below if\nEnglish is removed we see graph looks normal\nand not totally dominant by a single language","16cc10eb":"# Suggested","dd2f4c6a":"## Incorrect dates\nAs you can see only 2 dates are incorrect in the dateset which can effect the anaylsis","803fdc11":"# Question mark \nON the Authors columns, Clearly multiple authors are seprated by a \"\/\" which is \"incorrect\" for direct\ndata analysis. So after figuring out a \"function\" which does the following:->\n* It looks for '\/' in the authors\n* Seprates them if normally acessed\n* But if the book is already seen, maximum number of writers are considered\n* Hence we find that actual count was incorrect\n* Also includes single writers\n","32d28c91":"# Adjusting\n* Dates were < 0 in timestamp format\n* The Encoded Titles had an infinity Thanks to [This guy](https:\/\/www.kaggle.com\/carlosdg) for helping me.","71404ee0":"## Comparing the unique writers\n* We can see there are 7123 unique writers among 10464\n* Thus giving us a way to categorize","db030108":"# Wrong dates\nBookId\n45531\n31373","e40dac80":"# Constructing x\n* For KMeans Clustering we need only numerical type data\n* And for that we need to eliminate string or textual data","fdec5b61":"# The dtypes\nAll of them are numeric","0722a5e5":"## Looking into Titles\nWe find that twilight has maximum reviews and rating of 3.59\nAlthough max rating among top 10 is of Harry Potter and Half blood Prince","5b93b3f6":"# Doing elbow method for n\n* Looking for wcss which is optimal and hence obtain correct amount of categoires.\n* **Weighted Cumlatice shared sum** is the formula which takes the weights of all the features in account\n* This weight is minimized with optimum value by looking at an elbow(shape in graph)","21792fef":"## Simple Counting of writers\nThus obtained graph is shown as follows\nshowing that P.G. Woodhouse and Stephen King are equally dominating","f80b436c":"## Find Mins\n* The function finds 10 minimum from a threshold value\n* and then get the indexes of them from dataframe\n* So the encoded values from a text based entry is matched\n* The logic bieng that the similiar texts are encoded equally","7a37900d":"# Suggested","386ba9ad":"## The correct visualization\nThe visualization is perfect as everything is taken into account, hence Seeing Stephen King as a dominant","7de4ce30":"# Real","6824695d":"## Ratings\nFrom the above graph we can clearly see that the dominating rating is\n3 to 4, moreover the average lies between 3 to 4.\nThis is skewed data, as well an indication that the \ndataset given has more 3 to 5 star books within the sample","855fba05":"# Encoding\nThis function is for encoding Authors, This function was made after a lot of thought such that to have less than 5 % clashes","a5176bf8":"# Date conversion","48c0bd03":"# Real","19cb6816":"# Suggestion Logic"}}