{"cell_type":{"d4b11ac6":"code","2ce9e89d":"code","4e7c5bd4":"code","3a764733":"code","bb3fe983":"code","e49dc107":"code","89084bf1":"code","203fe59b":"code","e75ce303":"code","c87bcc52":"code","beab83e8":"code","3204fe5d":"code","e6fde920":"code","03eaa5c4":"code","44600801":"code","7f9756a2":"code","6e434d0d":"code","461a3fa2":"code","3a320540":"code","42c88e61":"code","7b436c3b":"markdown"},"source":{"d4b11ac6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2ce9e89d":"import pandas as pd\nimport numpy as np\nimport random\n\n#split dataset\nfrom sklearn.model_selection import train_test_split\n\n#precision,recall,f1-score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n\n#visualize results\nimport matplotlib.pyplot as plt","4e7c5bd4":"import torch\nimport torchvision\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms","3a764733":"#global variables\n#seed\nnp.random.seed(123)\ntorch.manual_seed(123)\nrandom.seed(123)","bb3fe983":"data=pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\ndata.head(10)","e49dc107":"X = data.drop(['Time',\"Class\"], axis=1)\ny = data[\"Class\"].values\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,shuffle=False)\n\nprint(f'Positive Samples in Training Set:{sum(y_train)}')\nprint(f'Positive Samples in Testing Set:{sum(y_test)}')","89084bf1":"#\u5f52\u4e00\u5316\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler=MinMaxScaler().fit(X_train)\n\nX_train=scaler.transform(X_train)\nX_test=scaler.transform(X_test)","203fe59b":"#\u8bad\u7ec3\u96c6\u4e2d\u53ea\u5305\u542bnegative samples\n#\u6ce8\u610f\u6b64\u5904\u5f52\u4e00\u5316\u4e0e\u63d0\u53d6negative samples\u7684\u5148\u540e\u987a\u5e8f\nprint(f'Before Extract-X_train Shape:{X_train.shape}')\nprint(f'Before Extract-y_train Length:{len(y_train)}')\n\nX_train=X_train[y_train==0]\ny_train=y_train[y_train==0]\n\nprint(f'After Extract-X_train Shape:{X_train.shape}')\nprint(f'After Extract-y_train Length:{len(y_train)}')","e75ce303":"batch_size=128\n\n#transform numpy to pytorch tensor\nX_train_tensor=torch.from_numpy(X_train).float()\nX_test_tensor=torch.from_numpy(X_test).float()\n\n#fitting by batches (using dataloader)\nX_train_dataloader=DataLoader(X_train_tensor,batch_size=batch_size,shuffle=False,drop_last=True)","c87bcc52":"class generator(nn.Module):\n  def __init__(self,input_size,act_fun,deep=False):\n    super(generator,self).__init__()\n\n    #deeper neural network or not?\n    if not deep:\n      hidden_size=input_size\/\/2\n\n      self.encoder_1=nn.Sequential(\n        nn.Linear(input_size,hidden_size),\n        act_fun\n        )\n\n      self.decoder_1=nn.Sequential(\n        nn.Linear(hidden_size,input_size)\n        )\n\n      self.encoder_2=nn.Sequential(\n        nn.Linear(input_size,hidden_size),\n        act_fun\n        )\n      \n    elif deep:\n      hidden_size_1=input_size\/\/2\n      hidden_size_2=hidden_size_1\/\/2\n\n      self.encoder_1=nn.Sequential(\n        nn.Linear(input_size,hidden_size_1),\n        act_fun,\n        nn.Linear(hidden_size_1,hidden_size_2),\n        act_fun\n        )\n\n      self.decoder_1=nn.Sequential(\n        nn.Linear(hidden_size_2,hidden_size_1),\n        act_fun,\n        nn.Linear(hidden_size_1,input_size)\n        )\n\n      self.encoder_2=nn.Sequential(\n        nn.Linear(input_size,hidden_size_1),\n        act_fun,\n        nn.Linear(hidden_size_1,hidden_size_2),\n        act_fun\n        )\n\n  def forward(self,input):\n    z=self.encoder_1(input)\n    X_hat=self.decoder_1(z)\n    z_hat=self.encoder_2(X_hat)\n    return z,X_hat,z_hat","beab83e8":"class discriminator(nn.Module):\n  def __init__(self,input_size,act_fun,deep=False):\n    super(discriminator,self).__init__()\n    \n    if not deep:\n      hidden_size=input_size\/\/2\n\n      self.encoder=nn.Sequential(\n        nn.Linear(input_size,hidden_size),\n        act_fun\n        )\n\n      self.classifier=nn.Sequential(\n        nn.Linear(hidden_size,1),\n        nn.Sigmoid()\n        )\n    \n    elif deep:\n      hidden_size_1=input_size\/\/2\n      hidden_size_2=hidden_size_1\/\/2\n\n      self.encoder=nn.Sequential(\n        nn.Linear(input_size,hidden_size_1),\n        act_fun,\n        nn.Linear(hidden_size_1,hidden_size_2),\n        act_fun\n        )\n\n      self.classifier=nn.Sequential(\n        nn.Linear(hidden_size_2,1),\n        nn.Sigmoid()\n        )\n      \n  def forward(self,input):\n    latent_vector=self.encoder(input)\n    output=self.classifier(latent_vector)\n\n    return latent_vector,output","3204fe5d":"net_generator=generator(input_size=X_train_tensor.size(1),act_fun=nn.Tanh(),deep=True)\nnet_discriminator=discriminator(input_size=X_train_tensor.size(1),act_fun=nn.Tanh(),deep=False)","e6fde920":"print(net_generator)\nprint(net_discriminator)","03eaa5c4":"L1_criterion=nn.L1Loss()\nL2_criterion=nn.MSELoss()\n\nBCE_criterion=nn.BCELoss()","44600801":"optimizer_G=torch.optim.SGD(net_generator.parameters(),lr=0.01,momentum=0.5)\noptimizer_D=torch.optim.SGD(net_discriminator.parameters(),lr=0.01,momentum=0.5)","7f9756a2":"running_loss_G_vis=np.array([])\nrunning_loss_D_vis=np.array([])\n\nfor epoch in range(20):\n  running_loss_G=0\n  running_loss_D=0\n\n  for X in X_train_dataloader:\n    #training the discriminator with real sample\n    net_discriminator.zero_grad()\n\n    X=Variable(X)\n    y_real=torch.FloatTensor(batch_size).fill_(0)#real label=0,size=batch_size\n    y_real=Variable(y_real)\n\n    _,output=net_discriminator(X)\n\n    loss_D_real=BCE_criterion(output,y_real)\n\n    #training the discriminator with fake sample\n    _,X_hat,_=net_generator(X)\n    y_fake=torch.FloatTensor(batch_size).fill_(1)#fake label=1,size=batch_size\n\n    _,output=net_discriminator(X_hat)\n\n    loss_D_fake=BCE_criterion(output,y_fake)\n\n    #entire loss in discriminator\n    loss_D=loss_D_real+loss_D_fake\n    running_loss_D+=loss_D\n    loss_D.backward()\n\n    optimizer_D.step()\n\n    #training the generator based on the result from the discriminator\n    net_generator.zero_grad()\n\n    z,X_hat,z_hat=net_generator(X)\n\n    #loss_1\n    latent_vector_real,_=net_discriminator(X)\n    latent_vector_fake,_=net_discriminator(X_hat)\n\n    loss_G_1=L2_criterion(latent_vector_fake,latent_vector_real)\n    \n    #loss_2\n    loss_G_2=L1_criterion(X,X_hat)\n\n    #loss_3\n    loss_G_3=L1_criterion(z,z_hat)\n\n    #entire loss in generator\n    loss_G=loss_G_1+loss_G_2+loss_G_3\n    running_loss_G+=loss_G\n    loss_G.backward()\n\n    optimizer_G.step()\n\n  running_loss_G=running_loss_G\/len(X_train_dataloader)\n  running_loss_D=running_loss_D\/len(X_train_dataloader)\n\n  running_loss_G_vis=np.append(running_loss_G_vis,running_loss_G.detach().numpy())\n  running_loss_D_vis=np.append(running_loss_D_vis,running_loss_D.detach().numpy())\n\n  print(f'Generator Loss in Epoch {epoch}: {running_loss_G:.{4}}')\n  print(f'Discriminator Loss in Epoch {epoch}: {running_loss_D:.{4}}\\n')","6e434d0d":"plt.plot(running_loss_G_vis,'b',label='Generator Loss',linewidth=2)\nplt.plot(running_loss_D_vis,'r',label='Discriminator Loss',linewidth=2)\n\nplt.title('Generator and Discriminator Loss in Training')\nplt.legend()\nplt.show()","461a3fa2":"#testing\n#the scores of test samples\nscore=np.array([])\nfor i in range(len(X_test_tensor)):\n  z,_,z_hat=net_generator(X_test_tensor[i])\n  score=np.append(score,L1_criterion(z,z_hat).detach().item())","3a320540":"for ratio in [0.0005,0.001,0.0015,0.002,0.0025,0.003,0.0035,0.004]:\n  y_pred=np.repeat(0,len(y_test))\n\n  y_pred[np.argsort(-score)[:round(ratio*len(y_test))]]=1\n\n  print(f'Precision: {precision_score(y_pred,y_test):.{4}}')\n  print(f'Recall: {recall_score(y_pred,y_test):.{4}}')\n  print(f'F1-score: {f1_score(y_pred,y_test):.{4}}\\n')","42c88e61":"threshold_vis=0.001\nthreshold_score=score[np.argsort(-score)][round(threshold_vis*len(y_test))]\n\nplt.scatter(range(len(score)),score,c='b',label='Normal')\nplt.scatter(np.array(range(len(score)))[y_test==1],score[y_test==1],c='r',label='Anomaly')\n\nplt.hlines(threshold_score,xmin=0,xmax=len(score),colors='y',linewidth=4)\n\nplt.title(f'Threshold = {threshold_vis}')\nplt.legend()\nplt.show()","7b436c3b":"Codes are also provided in my github repo: https:\/\/github.com\/jmq19950824\/Credit-Card-Fraud-Detection"}}