{"cell_type":{"ff44328c":"code","1d55e828":"code","4c16a5c6":"code","8713f3c0":"code","7ca3d25a":"code","c11a5f5c":"code","88c5dce6":"code","fa59aaf4":"code","c4d40fb0":"code","360a2ec7":"code","7566428a":"code","13d98a20":"code","c59a7484":"code","a14eccc3":"code","1f6013d8":"code","00864d4c":"code","960389f3":"code","7a78f5a8":"code","437d6a74":"code","f967bd48":"code","f4bafd03":"code","bef26ebf":"code","8d2fb93e":"code","6af48ad7":"code","ca8d9ce8":"code","09061fcc":"code","7f1e82f7":"code","6c82363d":"code","599b2001":"code","415de144":"code","7bee42ef":"code","76c553f8":"code","e5648103":"code","5f5a03a9":"code","0068fd26":"markdown","f9210bc6":"markdown","a79d97b0":"markdown","91763ce8":"markdown","385838cd":"markdown","13e2084e":"markdown","b92201fe":"markdown","7fb5787b":"markdown","2ba16e26":"markdown","96531ce5":"markdown","3ab8d41b":"markdown","408fafd0":"markdown","fffcf299":"markdown","94ef2d28":"markdown","b8063e74":"markdown","0922cef6":"markdown","d273c1a9":"markdown","08dfb350":"markdown","e499b501":"markdown","55823258":"markdown","474ac80f":"markdown","c8711abc":"markdown","11fa78f0":"markdown","020a8de6":"markdown","93bc8685":"markdown","d83fbce5":"markdown","56bc28e8":"markdown","bbf21725":"markdown","e01a3713":"markdown","1e3160f9":"markdown","0cdb7467":"markdown","bf2004ea":"markdown","af985292":"markdown"},"source":{"ff44328c":"!pip install tweepy","1d55e828":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport re\nimport time\nimport string\nimport warnings\n\n# for all NLP related operations on text\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import *\nfrom nltk.classify import NaiveBayesClassifier\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\n\n# To consume Twitter's API\nimport tweepy\nfrom tweepy import OAuthHandler \n\n# To identify the sentiment of text\nfrom textblob import TextBlob\nfrom textblob.sentiments import NaiveBayesAnalyzer\nfrom textblob.np_extractors import ConllExtractor\n\n# ignoring all the warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n# downloading stopwords corpus\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('vader_lexicon')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('movie_reviews')\nnltk.download('punkt')\nnltk.download('conll2000')\nnltk.download('brown')\nstopwords = set(stopwords.words(\"english\"))\n\n# for showing all the plots inline\n%matplotlib inline","4c16a5c6":"# keys and tokens to access Twitter API\nconsumer_key = 'Sec3MvclRIx2RVlgu9l0SJX6D'\nconsumer_secret = 'ayoPNWtBm7fWpMBoK6EwRmegu3SW8Rw9mzJkottkv97quPe941'\naccess_token = '736550752760406018-so5CPJrEbJKb3c3Pq8va3VFr0yk4S0E'\naccess_token_secret = 'Cgr8tz0h6FTU7kxAjDzpHnjffNTHxWsBytXnu4Ihd1TFb'","8713f3c0":"class TwitterClient(object): \n    def __init__(self): \n        #Initialization method. \n        try: \n            # create OAuthHandler object \n            auth = OAuthHandler(consumer_key, consumer_secret) \n            # set access token and secret \n            auth.set_access_token(access_token, access_token_secret) \n            # create tweepy API object to fetch tweets \n            # add hyper parameter 'proxy' if executing from behind proxy \"proxy='http:\/\/172.22.218.218:8085'\"\n            self.api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n            \n        except tweepy.TweepError as e:\n            print(f\"Error: Tweeter Authentication Failed - \\n{str(e)}\")\n\n    def get_tweets(self, query, maxTweets = 1000):\n        #Function to fetch tweets. \n        # empty list to store parsed tweets \n        tweets = [] \n        sinceId = None\n        max_id = -1\n        tweetCount = 0\n        tweetsPerQry = 100\n\n        while tweetCount < maxTweets:\n            try:\n                if (max_id <= 0):\n                    if (not sinceId):\n                        new_tweets = self.api.search(q=query, count=tweetsPerQry)\n                    else:\n                        new_tweets = self.api.search(q=query, count=tweetsPerQry,\n                                                since_id=sinceId)\n                else:\n                    if (not sinceId):\n                        new_tweets = self.api.search(q=query, count=tweetsPerQry,\n                                                max_id=str(max_id - 1))\n                    else:\n                        new_tweets = self.api.search(q=query, count=tweetsPerQry,\n                                                max_id=str(max_id - 1),\n                                                since_id=sinceId)\n                if not new_tweets:\n                    print(\"No more tweets found\")\n                    break\n\n                for tweet in new_tweets:\n                    parsed_tweet = {} \n                    parsed_tweet['tweets'] = tweet.text \n\n                    # appending parsed tweet to tweets list \n                    if tweet.retweet_count > 0: \n                        # if tweet has retweets, ensure that it is appended only once \n                        if parsed_tweet not in tweets: \n                            tweets.append(parsed_tweet) \n                    else: \n                        tweets.append(parsed_tweet) \n                        \n                tweetCount += len(new_tweets)\n                print(\"Downloaded {0} tweets\".format(tweetCount))\n                max_id = new_tweets[-1].id\n\n            except tweepy.TweepError as e:\n                # Just exit if any error\n                print(\"Tweepy error : \" + str(e))\n                break\n        \n        return pd.DataFrame(tweets)","7ca3d25a":"twitter_client = TwitterClient()\n\n# calling function to get tweets\ntweets_df = twitter_client.get_tweets('Machine Learning', maxTweets=10000)\nprint(f'tweets_df Shape - {tweets_df.shape}')\ntweets_df.head(10)","c11a5f5c":"def underlying_atmosphere_textblob(text):\n    analysis = TextBlob(text)\n    if analysis.sentiment.polarity > 0: \n        return 'positive'\n    elif analysis.sentiment.polarity == 0: \n        return 'neutral'\n    else: \n        return 'negative'","88c5dce6":"sentiments_using_textblob = tweets_df.tweets.apply(lambda tweet: underlying_atmosphere_textblob(tweet))\npd.DataFrame(sentiments_using_textblob.value_counts())","fa59aaf4":"tweets_df['sentiment'] = sentiments_using_textblob\ntweets_df.head()","c4d40fb0":"def remove_pattern(text, pattern_regex):\n    r = re.findall(pattern_regex, text)\n    for i in r:\n        text = re.sub(i, '', text)\n    \n    return text ","360a2ec7":"# We are keeping cleaned tweets in a new column called 'tidy_tweets'\ntweets_df['tidy_tweets'] = np.vectorize(remove_pattern)(tweets_df['tweets'], \"@[\\w]*: | *RT*\")\ntweets_df.head(10)","7566428a":"cleaned_tweets = []\n\nfor index, row in tweets_df.iterrows():\n    # Here we are filtering out all the words that contains link\n    words_without_links = [word for word in row.tidy_tweets.split() if 'http' not in word]\n    cleaned_tweets.append(' '.join(words_without_links))\n\ntweets_df['tidy_tweets'] = cleaned_tweets\ntweets_df.head(10)","13d98a20":"tweets_df = tweets_df[tweets_df['tidy_tweets']!='']\ntweets_df.head()","c59a7484":"tweets_df.drop_duplicates(subset=['tidy_tweets'], keep=False)\ntweets_df.head()","a14eccc3":"tweets_df = tweets_df.reset_index(drop=True)\ntweets_df.head()","1f6013d8":"tweets_df['absolute_tidy_tweets'] = tweets_df['tidy_tweets'].str.replace(\"[^a-zA-Z# ]\", \"\")","00864d4c":"stopwords_set = set(stopwords)\ncleaned_tweets = []\n\nfor index, row in tweets_df.iterrows():\n    \n    # filerting out all the stopwords \n    words_without_stopwords = [word for word in row.absolute_tidy_tweets.split() if not word in stopwords_set and '#' not in word.lower()]\n    \n    # finally creating tweets list of tuples containing stopwords(list) and sentimentType \n    cleaned_tweets.append(' '.join(words_without_stopwords))\n    \ntweets_df['absolute_tidy_tweets'] = cleaned_tweets\ntweets_df.head(10)","960389f3":"tokenized_tweet = tweets_df['absolute_tidy_tweets'].apply(lambda x: x.split())\ntokenized_tweet.head()","7a78f5a8":"word_lemmatizer = WordNetLemmatizer()\n\ntokenized_tweet = tokenized_tweet.apply(lambda x: [word_lemmatizer.lemmatize(i) for i in x])\ntokenized_tweet.head()","437d6a74":"for i, tokens in enumerate(tokenized_tweet):\n    tokenized_tweet[i] = ' '.join(tokens)\n\ntweets_df['absolute_tidy_tweets'] = tokenized_tweet\ntweets_df.head(10)","f967bd48":"textblob_key_phrases = []\nextractor = ConllExtractor()\n\nfor index, row in tweets_df.iterrows():\n    # filerting out all the hashtags\n    words_without_hash = [word for word in row.tidy_tweets.split() if '#' not in word.lower()]\n    \n    hash_removed_sentence = ' '.join(words_without_hash)\n    \n    blob = TextBlob(hash_removed_sentence, np_extractor=extractor)\n    textblob_key_phrases.append(list(blob.noun_phrases))\n\ntextblob_key_phrases[:10]","f4bafd03":"tweets_df['key_phrases'] = textblob_key_phrases\ntweets_df.head(10)","bef26ebf":"def generate_wordcloud(all_words):\n    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=100, relative_scaling=0.5, colormap='Dark2').generate(all_words)\n\n    plt.figure(figsize=(14, 10))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis('off')\n    plt.show()","8d2fb93e":"all_words = ' '.join([text for text in tweets_df['absolute_tidy_tweets'][tweets_df.sentiment == 'positive']])\ngenerate_wordcloud(all_words)","6af48ad7":"all_words = ' '.join([text for text in tweets_df['absolute_tidy_tweets'][tweets_df.sentiment == 'negative']])\ngenerate_wordcloud(all_words)","ca8d9ce8":"all_words = ' '.join([text for text in tweets_df['absolute_tidy_tweets'][tweets_df.sentiment == 'neutral']])\ngenerate_wordcloud(all_words)","09061fcc":"# function to collect hashtags\ndef hashtag_extract(text_list):\n    hashtags = []\n    # Loop over the words in the tweet\n    for text in text_list:\n        ht = re.findall(r\"#(\\w+)\", text)\n        hashtags.append(ht)\n\n    return hashtags\n\ndef generate_hashtag_freqdist(hashtags):\n    a = nltk.FreqDist(hashtags)\n    d = pd.DataFrame({'Hashtag': list(a.keys()),\n                      'Count': list(a.values())})\n    # selecting top 15 most frequent hashtags     \n    d = d.nlargest(columns=\"Count\", n = 25)\n    plt.figure(figsize=(16,7))\n    ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n    plt.xticks(rotation=80)\n    ax.set(ylabel = 'Count')\n    plt.show()","7f1e82f7":"hashtags = hashtag_extract(tweets_df['tidy_tweets'])\nhashtags = sum(hashtags, [])","6c82363d":"generate_hashtag_freqdist(hashtags)","599b2001":"\ntweets_df_full_key_phrases = tweets_df[tweets_df['key_phrases'].str.len()>0]","415de144":"# TF-IDF\ntfidf_word_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, stop_words='english')\ntfidf_word_feature = tfidf_word_vectorizer.fit_transform(tweets_df_full_key_phrases['tidy_tweets'])","7bee42ef":"phrase_sents = tweets_df_full_key_phrases['key_phrases'].apply(lambda x: ' '.join(x))\n\n# TF-IDF for key phrases\ntfidf_phrase_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2)\ntfidf_phrase_feature = tfidf_phrase_vectorizer.fit_transform(phrase_sents)","76c553f8":"target_variable = tweets_df_full_key_phrases['sentiment'].apply(lambda x: 0 if x==\"negative\" else (1 if x==\"neutral\" else 2))","e5648103":"def naive_model(X_train, X_test, y_train, y_test):\n    naive_classifier = LogisticRegression()\n    naive_classifier.fit(X_train.toarray(), y_train)\n\n    # predictions over test set\n    predictions = naive_classifier.predict(X_test.toarray())\n\n    # calculating Accuracy Score\n    print(f'Accuracy Score - {accuracy_score(y_test, predictions)}')\n    ","5f5a03a9":"X_train, X_test, y_train, y_test = train_test_split(tfidf_word_feature, target_variable, test_size=0.2, random_state=123)\nnaive_model(X_train, X_test, y_train, y_test)","0068fd26":"### <a id='4'>4. Key Phrases<\/a> ","f9210bc6":"a) is done \"@anyname\"","a79d97b0":"## <a id='7'>7. Modelling<\/a>\nNow we can discuss our model. Using already categorical sentiment variable we can convert it into multi-classification (3) problem. In other words map the column \"sentiment\" to the values 0 1 2 and a build a basic model just to see the performance. Nothing fancy but example of labeling our data and making predictions. ","91763ce8":"As expected, AI, DeepLearning and MachineLearning are most common hashtags.","385838cd":"Data Science, Machine Learning, AI, Big Data... It is everywhere. Everybody is talking about it. But what are they really saying? Lets use Machine Learning to see whats the fuss with Machine Learning","13e2084e":"### <a id='2B'> Get sentiments<\/a>\nNow sentiment analysis, i.e. we want to know underlying atmosphere in the text in order to label the data. We are going to use TextBlob package to achieve that.","b92201fe":"## <a id='1'>1. Libraries<\/a>","7fb5787b":"As I already said these key phrases are in essence just a feature-engineering step from TextBlob lib. To achieve that we will be using [Conllextractor](https:\/\/textblob.readthedocs.io\/en\/dev\/advanced_usage.html)","2ba16e26":"Visualise the new features with worldcloud. Then see (twiter specific but we can generalise it to most common starter words) what are the most common hashtag words used in these tweets!","96531ce5":"Just empty text should also be removed","3ab8d41b":"![](https:\/\/cdn-images-1.medium.com\/max\/1200\/1*70m-GNQPgCr5TMVeWlA5iA.jpeg)","408fafd0":"Most common words in negative tweets","fffcf299":"After we remove duplicate rows we should reset index since after removal of some rows, some index values are missing, which may cause problem in future operations.","94ef2d28":"Build a class that will get us the tweets. Pythonically!","b8063e74":"b) is done \"websites http and https\"","0922cef6":"In a reality when we want to perform some analysis, data set my not be even present. We would have to scrape it or find it in some other way. There are a couple of libraries to perform webscraping in Python: BeautifulSoup, Selenium... but for twitter data there is a special library called tweepy which makes the whole process much faster. I already did some scraping with selenium where data was dynamically scraped (as the script was running) and sent to a desired E-Mail adress. [Selenium Scraping](https:\/\/www.kaggle.com\/zikazika\/extracting-information-from-yahoo-finance). Here I will proceede with tweepy only.","d273c1a9":"g) is done (stop words)","08dfb350":"## <a id='2'>2. Scraping the tweets<\/a>\n\n__. Fetch from twitter using 'tweepy' <br\/>__\n\nHow? check this out [access keys](https:\/\/themepacific.com\/how-to-generate-api-key-consumer-token-access-key-for-twitter-oauth\/994\/)","e499b501":"When we fetch the tweets from twitter than the question becomes (besides initial analysis) what can I do with it. I would really like to build a model but the usual problem with unstructured data is that it is not labeled. Luckily there is a whole sub-discipline of NLP called [sentiment-analysis](https:\/\/monkeylearn.com\/sentiment-analysis\/). Article is great and I really advise you to read it but in essence what we are trying to achieve is to extract some context from text. Than depending on that we can feature-engineer a categorical variable. Building upon that we can make certain predictions. More specifically we are going to perform **Polarity Classification** where we basically say: this tweet is negative, positive or neutral. Than we can perform multi-classification. Another possibility is to classify emotions, angry, sad ,happy etc... and than proceede. I think polarity captures the best current atmosphere concerning Machine Learning.","55823258":"**REMEMBER** we have mutliple text columns and we need to quantify all of them.","474ac80f":"Tokenization, alternatively with NLTK library... ","c8711abc":"## <a id='3'>3. Text Pre-processing<\/a> \n\nIt is a standard procedure that needs to be done usually with regex lib. Some good notebooks concerning this can be found at [NLP](https:\/\/www.kaggle.com\/zikazika\/natural-language-processing-theory-and-practice), [NLP-pre processing](https:\/\/www.kaggle.com\/deffro\/text-pre-processing-techniques) basic premise is as in standard ML problems, clean your data and prepare it for further work.\n\n\n\nLet us think. What should we drop\/modify in the text that gives us no infromation what so ever?\n\na) \u2018@anyname\u2019 are of no use\n\nb) removing links (https,http)\n\nc) removing duplicate rows\n\nd) removing punctuations, numbers and special characters, (posssibly also: bad-words, contractions etc)\n\ne) stop words, as always I, am ,a etc... are not really helping\n\nf) Tokenization and lemming&stemming (here we avoid word2vec and embeddings)\n\ng) Etc... It is really problem specific some additional pre-processing could always be done but we are not in a competition....","11fa78f0":"Let us inpect coomonly used hashtags","020a8de6":"Join tokens coherently in a single, tidy tweet,","93bc8685":"### Actions I am going to perform are:\n\n\n1. __<a href='#1' target='_self'>Libraries<\/a>__\n1. __<a href='#2' target='_self'>Scraping the tweets with tweepy<\/a>__\n1. __<a href='#3' target='_self'>Text Pre-processing<\/a>__\n1. __<a href='#4' target='_self'>Key-phrases as new feature<\/a>__\n1. __<a href='#5' target='_self'>Visualisation<\/a>__\n\n1. __<a href='#6' target='_self'>Numerical Representation of text<\/a>__\n\n1. __<a href='#7' target='_self'>Model Building of tf-idf feature<\/a>__\n","d83fbce5":"## <a id='6'>6. Numerical representation of text<\/a>\n\nThere are a couple of models that can be used, here I will be using TF-IDF representation (which is built upon BoW) to represent text.","56bc28e8":"Most common words in neutral tweets","bbf21725":"Lemmatize","e01a3713":"Using tf-idf encoding of text to make predictions, remember its basic just to understand how one would proceede in complex scenario!","1e3160f9":"Alright, I know what are you thinking. But how do we come up with this classification, that is how does it work behind the scenes with TextBlob, i.e. with Sentiment Analysis. Well some digging around needs to be done. First of all if one consults [Everything you need to know](https:\/\/monkeylearn.com\/sentiment-analysis\/) under Algorithms it seems that the most logical approach is the hybrid one, i.e. rule based and model based (its a classification model) to confirm that let us look at the source code in this library [source](https:\/\/github.com\/sloria\/TextBlob\/tree\/dev\/textblob).\n\n","0cdb7467":"Most common words in positive tweets","bf2004ea":"# IMPORTANT\n\nNow before we do this final step of removing punctuation, numbers etc one needs to understand another functionality of Semantic Analysis. We can also, using TextBlob, create key phrases that are detrimental for individual tweets. But in order for that to be succesfull we also need to make sure that there are certain semantic connections in the text given (in other words removing stop sign will be a negative thing to do if we want **key phrases**) Because of that we will create additional column where we do this pre-processing step!","af985292":"## <a id='5'>5. Visualisation"}}