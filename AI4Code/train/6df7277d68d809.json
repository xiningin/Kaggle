{"cell_type":{"6ecc4fd6":"code","bbda5072":"code","0e41fe1f":"code","3535276c":"code","e8d7fdee":"code","f088c5af":"code","7e93d115":"code","14a933dc":"code","074b603e":"code","65d67e5f":"code","48e10b27":"code","3f6f4f0f":"code","6f7fe535":"code","4ed94bb7":"code","7aee6b93":"code","0401ad6e":"code","8e0b404e":"code","31e91d81":"code","9e7dd860":"code","449f000c":"code","db048263":"code","dabebd9e":"code","76544811":"code","f7128fa9":"code","329685f2":"code","43ba3076":"code","b06f0030":"code","ee31b109":"code","2f136a59":"code","eaed9aad":"code","6b687b33":"code","7e46e786":"code","fcf2c0c9":"code","0be6e445":"code","bafdfe34":"markdown","a2da71c1":"markdown","311d9f24":"markdown","7ce00fd4":"markdown","4051e75e":"markdown","bda91802":"markdown","e53858e9":"markdown","5587da57":"markdown"},"source":{"6ecc4fd6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bbda5072":"import cv2\nimport keras\nimport random as rn\nfrom PIL import Image\nfrom tqdm import tqdm\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom IPython.display import SVG\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport gc\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom tensorflow.python.keras import backend as K\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.python.keras.utils.vis_utils import model_to_dot\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array, save_img\nfrom tensorflow.python.keras.layers import Dense, Flatten,MaxPooling2D, GlobalAveragePooling2D,BatchNormalization,Dropout,Conv2D,MaxPool2D\nfrom sklearn.utils import shuffle\nfrom sklearn import metrics\nfrom sklearn.metrics import plot_confusion_matrix\nfrom keras.utils.vis_utils import plot_model","0e41fe1f":"train_data_dir='..\/input\/skin-cancer-malignant-vs-benign\/train\/'\ntrain_data_dir1='..\/input\/skin-cancer-malignant-vs-benign\/train\/malignant'\ntrain_data_dir2='..\/input\/skin-cancer-malignant-vs-benign\/train\/benign'","3535276c":"name_malignant = os.listdir(train_data_dir1)\nname_benign = os.listdir(train_data_dir2)","e8d7fdee":"name_malignant = name_malignant[:65]\nname_benign = name_benign[:195]  ","f088c5af":"new_malignant = []\nnew_benign = []\nfor i in name_malignant:\n    new_malignant.append('..\/input\/skin-cancer-malignant-vs-benign\/train\/malignant\/'+i)\nfor j in name_benign:\n    new_benign.append('..\/input\/skin-cancer-malignant-vs-benign\/train\/benign\/'+j)\n","7e93d115":"len(new_benign)","14a933dc":"import pandas as pd\ndf1 = pd.DataFrame()\ndf2 = pd.DataFrame()\ndf = pd.DataFrame()","074b603e":"df1['Skin_Type'] = new_malignant\ndf2['Skin_Type'] = new_benign\ndf1['Target'] = 'malignant'\ndf2['Target'] = 'benign'","65d67e5f":"df= pd.concat([df1, df2], axis = 0)","48e10b27":"df = df.sample(frac=1).reset_index(drop=True)\ndf","3f6f4f0f":"img_height= 224\nimg_width =224\n\ntrain_datagen = ImageDataGenerator(rescale=1.\/255,\n                                   zoom_range=0.1,\n                                   validation_split=0.3)\ntrain_datagen_flow = train_datagen.flow_from_dataframe(\n    dataframe=df,\n    directory=None,\n    x_col='Skin_Type',\n    y_col='Target',\n    target_size=(img_height, img_width),\n    batch_size=32,\n    subset='training',\n    shuffle = True,\n    class_mode='binary'\n)\nvalid_datagen_flow = train_datagen.flow_from_dataframe(\n    dataframe=df,\n    directory=None,\n    x_col='Skin_Type',\n    y_col='Target',\n    target_size=(img_height, img_width),\n    batch_size=32,\n    subset='validation',\n    class_mode = 'binary',\n    shuffle = True\n)","6f7fe535":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(224,224,3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])","4ed94bb7":"from tensorflow.keras.optimizers import RMSprop\nmodel.compile(loss='binary_crossentropy',\n              optimizer=RMSprop(lr=1e-4),\n              metrics=['accuracy'])","7aee6b93":"History = model.fit_generator(train_datagen_flow,\n                         steps_per_epoch = 5,\n                         epochs = 28,\n                         validation_data = valid_datagen_flow,\n                         validation_steps = 2,verbose=1)","0401ad6e":"fig1 = plt.gcf()\nplt.plot(History.history['accuracy'])\nplt.plot(History.history['val_accuracy'])\nplt.axis(ymin=0.4,ymax=1)\nplt.grid()\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'validation'])\nplt.show()","8e0b404e":"fig1 = plt.gcf()\nplt.plot(History.history['loss'])\nplt.plot(History.history['val_loss'])\nplt.grid()\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\n\nplt.legend(['train', 'validation'])\nplt.show()\nfig1.savefig('loss_plot.jpg')","31e91d81":"epochs= 10\nmodel1 = Sequential()\n\npretrained_model= InceptionV3(include_top=False,\n                   input_shape=(224,224,3),\n                   pooling='avg',classes=2,\n                   weights='imagenet')\nprint(len(pretrained_model.layers))\n\nfor layer in pretrained_model.layers:\n        layer.trainable=False\n\nmodel1.add(pretrained_model)\nmodel1.add(Flatten())\nmodel1.add(Dense(512, activation='relu'))\nmodel1.add(Dense(256, activation='relu'))\nmodel1.add(Dense(128, activation='relu'))\nmodel1.add(Dense(64, activation='relu'))\nmodel1.add(Dense(1, activation='sigmoid'))","9e7dd860":"model1.compile(optimizer=Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])","449f000c":"History = model1.fit_generator(train_datagen_flow,\n                              steps_per_epoch = 5,\n                              validation_data = valid_datagen_flow, \n                              validation_steps = 1,\n                              epochs=epochs,\n                              verbose=1\n                              )","db048263":"fig1 = plt.gcf()\nplt.plot(History.history['accuracy'])\nplt.plot(History.history['val_accuracy'])\nplt.axis(ymin=0.4,ymax=1)\nplt.grid()\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'validation'])\nplt.show()","dabebd9e":"fig1 = plt.gcf()\n\nplt.plot(History.history['loss'])\nplt.plot(History.history['val_loss'])\nplt.grid()\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\n\nplt.legend(['train', 'validation'])\nplt.show()\nfig1.savefig('loss_plot.jpg')","76544811":"model2 = Sequential()\n\npretrained_model = tf.keras.applications.VGG16(include_top=False,\n                   input_shape=(224,224,3),\n                   \n                   weights='imagenet')\nprint(len(pretrained_model.layers))\n\nfor layer in pretrained_model.layers:\n        layer.trainable=False\nmodel2.add(pretrained_model)\nmodel2.add(Flatten())\nmodel2.add(Dense(512, activation='relu'))\nmodel2.add(Dense(256, activation='relu'))\nmodel2.add(Dense(128, activation='relu'))\nmodel2.add(Dense(64, activation='relu'))\nmodel2.add(Dense(1, activation='sigmoid'))","f7128fa9":"model2.compile(optimizer=Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])","329685f2":"History = model2.fit_generator(train_datagen_flow,\n                              steps_per_epoch = 5,\n                              validation_data = valid_datagen_flow, \n                              validation_steps = 1,\n                              epochs=10,\n                              verbose=1\n                              )","43ba3076":"fig1 = plt.gcf()\nplt.plot(History.history['accuracy'])\nplt.plot(History.history['val_accuracy'])\nplt.axis(ymin=0.4,ymax=1)\nplt.grid()\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'validation'])\nplt.show()","b06f0030":"fig1 = plt.gcf()\n\nplt.plot(History.history['loss'])\nplt.plot(History.history['val_loss'])\nplt.grid()\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\n\nplt.legend(['train', 'validation'])\nplt.show()\nfig1.savefig('loss_plot.jpg')","ee31b109":"model3 = Sequential()\n\npretrained_model = tf.keras.applications.ResNet50(include_top=False,\n                   input_shape=(224,224,3),\n                   \n                   weights='imagenet')\nprint(len(pretrained_model.layers))\n\nfor layer in pretrained_model.layers:\n        layer.trainable=False\nmodel3.add(pretrained_model)\nmodel3.add(Flatten())\nmodel3.add(Dense(512, activation='relu'))\nmodel3.add(Dense(256, activation='relu'))\nmodel3.add(Dense(128, activation='relu'))\nmodel3.add(Dense(64, activation='relu'))\nmodel3.add(Dense(1, activation='sigmoid'))","2f136a59":"model3.compile(optimizer=Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])","eaed9aad":"History = model3.fit_generator(train_datagen_flow,\n                              steps_per_epoch = 5,\n                              validation_data = valid_datagen_flow, \n                              validation_steps = 1,\n                              epochs=10,\n                              verbose=1\n                              )","6b687b33":"fig1 = plt.gcf()\nplt.plot(History.history['accuracy'])\nplt.plot(History.history['val_accuracy'])\nplt.axis(ymin=0.4,ymax=1)\nplt.grid()\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'validation'])\nplt.show()","7e46e786":"fig1 = plt.gcf()\nplt.plot(History.history['loss'])\nplt.plot(History.history['val_loss'])\nplt.grid()\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['train', 'validation'])\nplt.show()\nfig1.savefig('loss_plot.jpg')","fcf2c0c9":"image_path = os.path.join('..\/input\/skin-cancer-malignant-vs-benign\/train\/malignant\/102.jpg')\nimage = tf.keras.preprocessing.image.load_img(image_path)\nresized_image = image.resize((224,224))\nnumpied_image = np.expand_dims(resized_image, 0)\ntensored_image = tf.cast(numpied_image, tf.float32)\nlabel = model1.predict_classes(tensored_image\/255)\nlabel","0be6e445":"Y_pred = model2.predict(valid_datagen_flow)\ny_pred=[]\nfor i in range(len(Y_pred)):\n    if Y_pred[i][0]>0.5:\n        y_pred.append(1)\n    else:\n        y_pred.append(0)\n        \nprint('Confusion Matrix')\nprint(metrics.confusion_matrix(valid_datagen_flow.classes, y_pred))\nprint ()\n\nprint('Classification Report')\ntarget_names = ['Non nevus', 'Nevus']\nprint(metrics.classification_report(valid_datagen_flow.classes, y_pred, target_names=target_names))\n\nprint(valid_datagen_flow.class_indices)","bafdfe34":"***Basic Model***","a2da71c1":"***Confusion Matrix. Here also you can change model name for checking for different models.***","311d9f24":"***In below cell , you can change according to your requirement of dataset***","7ce00fd4":"***ResNet50***","4051e75e":"***Code for predicting the label of image . You can check for different model by just changing model name in 2nd last line***","bda91802":"***made dataframe and in skin type instead of just name of images , we have taken whole path of image because we had given directory = None in flow from directory method***","e53858e9":"***InceptionV3***","5587da57":"***VGG16***"}}