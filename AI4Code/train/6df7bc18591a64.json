{"cell_type":{"6161aff5":"code","328fef34":"code","52695383":"code","e2d8b766":"code","e8b6e4f2":"code","c53c403f":"code","c23f89e7":"code","df03af2e":"code","7047080e":"code","b62e2631":"code","87a81001":"code","58e56e9a":"code","0af8c532":"code","445cd4df":"code","eccf9a2e":"code","a1032338":"code","7ac7683d":"code","f3ce5af4":"code","67fcf8c5":"code","6edc2595":"code","3026357f":"code","32a3a335":"code","2d921317":"code","d8a9495c":"code","4c4d9317":"code","ea0231c4":"code","8bc2e7f9":"code","354c83b2":"code","e44e6a57":"code","6d3582d1":"code","e7a541a4":"code","a4138c5c":"code","02b380c6":"code","a4519bd4":"code","993b6f83":"code","4116fa12":"code","72352846":"code","415a805f":"code","01fa4747":"code","4ca120fb":"code","a2a72bcd":"code","8ac56044":"code","b67e003b":"markdown","cd7d3a19":"markdown","2bb135d3":"markdown","35e0d209":"markdown","e8834d8c":"markdown","1539edb5":"markdown","33826437":"markdown","9550326f":"markdown","f235d0b8":"markdown","d5f462b4":"markdown","c293d629":"markdown","912de807":"markdown","0297bb58":"markdown","885f1b8e":"markdown","6fdd704b":"markdown","5b393d20":"markdown","f91654f5":"markdown","2306ae4d":"markdown","c04b6fe7":"markdown","bbe264fb":"markdown"},"source":{"6161aff5":"import pandas as pd # for dataframes\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns # for data visualization\n\n%matplotlib inline","328fef34":"train = pd.read_csv(\"..\/input\/titanic-train\/titanic_train.csv\")","52695383":"train.head()","e2d8b766":"train.isnull().sum()","e8b6e4f2":"# using seaborn\nsns.heatmap(train.isnull(), yticklabels = False, cmap = 'viridis', cbar = False)","c53c403f":"# countplot for death and survival rate\nsns.set_style(\"whitegrid\")\nsns.countplot(x = \"Survived\", data = train, palette = 'rainbow')","c23f89e7":"# countplot for male and female\nsns.set_style('darkgrid')\nsns.countplot(x = \"Sex\", data = train, palette = 'rocket')","df03af2e":"# countplot for people died against Pclass\nsns.set_style('whitegrid')\nsns.countplot(x = \"Survived\", hue = 'Pclass', data = train)","7047080e":"# getting the count of Age person\ntrain['Age'].hist(bins=40, color='darkred', alpha=0.5)","b62e2631":"train['Fare'].hist(bins=20, color='purple', alpha = 0.7, figsize = (10, 5))","87a81001":"plt.figure(figsize = (10, 5))\nsns.boxplot(x = 'Pclass', y = 'Age', data = train, palette = 'winter')","58e56e9a":"# making a method for imputing age\ndef impute_age(cols):\n    # we will pass 2 cols as arguments, col at 0 index will be for age and col at 1 index will be for Pclass\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    # getting null values\n    if pd.isnull(Age):\n        # returning avg. age (37) for 1st Class\n        if Pclass == 1:\n            return 37\n        # for 2nd class age (28)\n        elif Pclass == 2:\n            return 28\n        # for 3rd class age (25)\n        else:\n            return 25\n    else:\n        return Age\n            ","0af8c532":"# applying above function\n# col[0] = 'Age' & col[1] = 'Plcass'\ntrain['Age'] = train[['Age', 'Pclass']].apply(impute_age, axis = 1)","445cd4df":"sns.heatmap(train.isnull(), yticklabels = False, cmap = 'viridis', cbar = False)","eccf9a2e":"# cabin\ntrain.drop('Cabin', inplace = True, axis = 1)","a1032338":"sns.heatmap(train.isnull())","7ac7683d":"train.shape # 1 column is dropped now","f3ce5af4":"train.head()","67fcf8c5":"train.info() # 'object' are strings in python","6edc2595":"sex = pd.get_dummies(train['Sex'], drop_first = True)\nembark = pd.get_dummies(train['Embarked'], drop_first = True)","3026357f":"sex # into numerical values","32a3a335":"embark # into numerical","2d921317":"# dropping extra cols\ntrain.drop(['Sex', 'Embarked', 'Name', 'Ticket'], axis = 1, inplace = True)","d8a9495c":"train # categorical values dropped here!","4c4d9317":"train = pd.concat([train, sex, embark], axis = 1)","ea0231c4":"train # all values in numercial form! yay!!","8bc2e7f9":"from sklearn.model_selection import train_test_split","354c83b2":"# we required 'Survived' values on Y-Axis\nY = train['Survived'] # Y == Survived column","e44e6a57":"# all other features will be on X-axis. So, we are dropping 'Survived' column and storing all others\nX = train.drop(['Survived'], axis=1) # X == all cols, excluding Survived column","6d3582d1":"X","e7a541a4":"# splitting Testing and Training data with 20-80 margine\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 101)","a4138c5c":"from sklearn.linear_model import LogisticRegression","02b380c6":"logReg = LogisticRegression()","a4519bd4":"# training our Logistic Regression Model here\nlogReg.fit(X_train, Y_train)","993b6f83":"# making predictions on Testing data\npredictions = logReg.predict(X_test)","4116fa12":"from sklearn.metrics import classification_report","72352846":"# using actual testing data and the predictions our Model just made\nprint(classification_report(Y_test, predictions)) # getting accuracy","415a805f":"# for making a DataFrame shape must be same\npred = logReg.predict(X)","01fa4747":"pred.shape","4ca120fb":"X.shape # same number of Rows","a2a72bcd":"# making our own DataFrame with 'Submission' as Name\nsubmission = pd.DataFrame({\n    'PassengerId' : X['PassengerId'],\n    'Survived' : pred\n})","8ac56044":"submission.head","b67e003b":"## Evalutaion","cd7d3a19":"---\n\nAs we have **Object(4)** so it means there are 4 string type of categorical values that needs to be converted into numercial values. But we don't need **Names** and **PassengerID** as it doesn't effect whether a person lives or die.\n\n---\n","2bb135d3":"### Dropping unnecessary columns\nSuch that: Name, Cabin, PassengerID etc","35e0d209":"## Making a .csv File of the Predictions\nThis file can be submitted on <a href=\"http:\/\/kaggle.com\/\">kaggle<\/a> or you for any other purpose where you want to show your predicitons","e8834d8c":"## Data cleaning\nThat is we are going to fill the missing values, as we have seen using heatmap that we cannot afford to drop those missing values as we may lost a huge amount of data.\n\nReplacing values is known as imputation. So, we are going to impute average values.\n\n---\nGetting the average Age","1539edb5":"## Concat Numerical Values\nNow the next step is to combine or concatenate the features that we just converted from categorical to numerical.","33826437":"## Training and Prediction via Model","9550326f":"Now we have the data, what's our problem? We will be using features like Name, Sex, Age, Pclass etc. To predict whether the person **Survived** or not.\n\nObviously we will be doing data preprocessing to see which part of data is necessary and which is not. So, we will be dropping some features that doesn't effect our prediction such as Name, PassengerID and others.\n\nAnd this will be done after data visualization :)","f235d0b8":" # Building Logistic Regression Model\n ## Machine Learning Model\n As the concept is same, we will split the data into two parts i.e. Train and Test","d5f462b4":"### Fetching the data from Data set","c293d629":"#### Note:\nWhenever you are going to show this null or missing values. *Always go for **Data Visualization** here*","912de807":"## Heatmap","0297bb58":"Let's say we have average age as:\n- 1st class ==> 37\n- 2nd class ==> 28\n- 3rd class ==> 25\n\n---\nNow we will make a function that will replace all the null values for specific PClass with average age in that class\n\n## Imputing Average Age","885f1b8e":"## Exploratory Data Analysis (EDA)\nLet's search for missing values!","6fdd704b":"Now, here the missing values can be visualized very clearly. Hence, this method is more proficient for presentations stuff at industry level.","5b393d20":"# Titanic Survial Predicions via Logistic Regression Model\nWe will be using <a href=\"https:\/\/www.kaggle.com\/tedllh\/titanic-train\">Titanic Data Set from kaggle<\/a>. We will classify two classes here, dead and survived people.","f91654f5":"### Importing Libraries","2306ae4d":"## Histogram","c04b6fe7":"## Countplot","bbe264fb":"## Converting Categorcial Features\nAs we know that ML model only works for numerical not categorical values. "}}