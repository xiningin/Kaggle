{"cell_type":{"c537bd31":"code","e8157d67":"code","73144c5c":"code","13cd6176":"code","54ec1a58":"code","3fc070f1":"code","7f4ff306":"code","d9bbeb76":"code","1428590e":"code","2c700dc5":"code","7bb22388":"code","40392122":"code","592448af":"code","805bd14e":"code","f6a56f1f":"code","3b06524f":"code","ab3ee363":"code","4e3ceaad":"code","5e23da15":"code","e7d9fc80":"markdown","01b9f587":"markdown","47aac999":"markdown","513b8d07":"markdown","62df7897":"markdown","6672a58f":"markdown","a6817115":"markdown","5a557696":"markdown","ed7790a1":"markdown","0deeac1e":"markdown","b1f91c18":"markdown","2ea19a4d":"markdown"},"source":{"c537bd31":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom scipy.misc import imread\n\nimport os\nfrom IPython import display\nimport PIL\nimport time\n\nimport glob\n#import imageio\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n# Any results you write to the current directory are saved as output.\n\nimport gc\n\nimport tensorflow as tf\ntf.enable_eager_execution()\n","e8157d67":"train= pd.read_csv('\/kaggle\/input\/mnist_train.csv')\ntest= pd.read_csv('\/kaggle\/input\/mnist_test.csv')\ntrain = pd.concat([train, test])\ntrain.drop(train.columns[0], axis=1, inplace=True)\ntrain = np.array(train)\ntrain = train.reshape(train.shape[0],28,28,1).astype('float32')\ntrain = (train -127.5) \/ 127.5\ndel test\ngc.collect()\nprint(train.shape)\nplt.imshow(train[0].reshape(28,28), cmap='gray')\nplt.show()","73144c5c":"BUFFER_S = 70000\nBATCH_S = 256\n# Create batches and shuffle the dataset\ntraindata = tf.data.Dataset.from_tensor_slices(train).shuffle(BUFFER_S).batch(BATCH_S)","13cd6176":"def Generator():\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(7*7*256, use_bias=False, input_shape=(100,))) # input the random noise to the dense layer \n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Reshape((7,7,256)))\n    \n    assert model.output_shape == (None,7,7,256)\n    model.add(tf.keras.layers.Convolution2DTranspose(128, (5,5), strides=(1,1), padding='same', use_bias=False)) #[[outshape = (n-f+2p)\/s + 1]]\n    assert model.output_shape ==(None,7,7,128)\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Convolution2DTranspose(64,(5,5), strides=(2,2), padding='same', use_bias=False))\n    assert model.output_shape == (None, 14,14,64)\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU())\n    \n    model.add(tf.keras.layers.Convolution2DTranspose(1, (5,5), strides=(2,2), padding='same', use_bias=False))\n    assert model.output_shape == (None, 28,28,1)\n    \n    return model\n    ","54ec1a58":"def Discriminator():\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Conv2D(64, (5,5), strides=(2,2), padding='same'))\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Dropout(0.3))\n    \n    model.add(tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Dropout(0.3))\n    \n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(1))\n    \n    return model","3fc070f1":"# instantiate the models\ngenerator = Generator()\ndiscriminator = Discriminator()","7f4ff306":"\ndef gLoss(generated_output):\n    return tf.losses.sigmoid_cross_entropy(tf.ones_like(generated_output), generated_output)","d9bbeb76":"def dLoss(real_output, generated_output):\n    real_loss = tf.losses.sigmoid_cross_entropy(tf.ones_like(real_output), real_output)\n    gen_loss = tf.losses.sigmoid_cross_entropy(tf.zeros_like(generated_output), generated_output)\n    \n    total_loss = real_loss + gen_loss\n    return total_loss\n    ","1428590e":"gOptimizer = tf.train.AdamOptimizer(1e-4)\ndOptimizer = tf.train.AdamOptimizer(1e-4)","2c700dc5":"chkpt_dir = '.\/trng_chkpts'\nchkpt_prefix = os.path.join(chkpt_dir,'ckpt')\ncheckpt = tf.train.Checkpoint(generator=generator, discriminator=discriminator, generator_optimizer=gOptimizer, discriminator_optimizer=dOptimizer)","7bb22388":"noise_dim = 100\nEPOCHS = 50\nresults = 10\nrandom_vector = tf.random_normal([results, noise_dim])","40392122":"def gantraining(images):\n    noise = tf.random_normal([BATCH_S, noise_dim])\n    \n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated_images = generator(noise, training=True)\n        \n        #Outputs from models\n        real_output = discriminator(images, training=True)\n        generated_output = discriminator(generated_images, training=True)\n        \n        #respective losses\n        gen_loss = gLoss(generated_output)\n        disc_loss = dLoss(real_output, generated_output)\n    \n    #calculate gradients\n    grad_gen = gen_tape.gradient(gen_loss, generator.variables)\n    grad_disc = disc_tape.gradient(disc_loss, discriminator.variables)\n    \n    #Apply gradients from optimizer\n    gOptimizer.apply_gradients(zip(grad_gen,generator.variables))\n    dOptimizer.apply_gradients(zip(grad_disc, discriminator.variables))","592448af":"def image_processing(model, epoch, test_input):\n    #predictions at each epoch using the input random_vector. Note the model (generator) training is set 'False'\n    predictions = model(test_input,training=False)\n    \n    fig = plt.figure(figsize=(4,4))\n    print('Epoch: ' + str(epoch))\n    for i in range(predictions.shape[0]):\n        plt.subplot(4,4,i+1)\n        plt.imshow(predictions[i,:,:,0]*127.5 + 127.5, cmap='gray')\n        plt.axis('off')\n    \n    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n    plt.show()","805bd14e":"gantraining = tf.contrib.eager.defun(gantraining)","f6a56f1f":"def theGAN(dataset, epochs):\n    for epoch in range(epochs):\n        for images in dataset:\n            gantraining(images)\n        \n        display.clear_output(wait=True)\n        image_processing(generator, epoch+1, random_vector)\n        \n        # saving (checkpoint) the model every 15 epochs\n        if (epoch + 1) % 15 == 0:\n            checkpt.save(file_prefix = chkpt_prefix)\n    \n    display.clear_output(wait=True)\n    image_processing(generator, epoch, random_vector)","3b06524f":"theGAN(traindata, EPOCHS)","ab3ee363":"checkpt.restore(tf.train.latest_checkpoint(chkpt_dir))","4e3ceaad":"def display_image(epoch_no):\n  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))","5e23da15":"display_image(EPOCHS)","e7d9fc80":"**The Discriminator loss** is a sum of losses generated by the Generator n\/w:- i.e. real_loss from the real_output and generated_loss from generated fake output.","01b9f587":"**Setting Up *Dataset* batch to feed into the network**","47aac999":"# **GAN - Generative Adversarial Network**<br\/>\nThis Kernel is a Toy example for GAN implementation using TensorFlow. <br\/>\nThis example is implemented on MNIST dataset for learning the GAN concepts. ","513b8d07":"## **Create Models**:  The Generator and The Discriminator<br\/>\n***Generator***<br\/>\nHere we are doing upsampling (double the size of original image) using Convolution2DTranspose(). Remember that we started the image generation from random noise (100,0)<br\/>\n<br\/>\n***Discriminator***<br\/>\nSimilar to the regular CNN Model","62df7897":"### **Define loss functions for Generator and Discriminator**","6672a58f":"## **Import and preprocess the MNIST DataSet**","a6817115":"**The Generator loss** is \"SigmoidCrossEntropyLoss\" from the generated image and array of Ones. this is because generator want to learn as much fake images it can generate.","5a557696":"**Define Optimizers for the models**","ed7790a1":"### **TRAINING THE GAN **","0deeac1e":"**SETUP TRAINING **","b1f91c18":"### Resultant Output","2ea19a4d":"**CheckPoints: ** Definition"}}