{"cell_type":{"c3b47f9c":"code","c2aa0282":"code","e7aaa227":"code","627f2a66":"code","ab735c59":"code","e6b7dd34":"code","759a0826":"code","760d50e9":"code","a64092d2":"code","3e91de7a":"code","1edd139e":"code","f478076e":"code","9c4800f3":"code","2768e379":"code","fae0645b":"code","b858b006":"code","346e9653":"code","e7c566db":"code","45051b88":"code","685324bd":"code","8e980be5":"code","5e7babd2":"code","fc6b0c59":"code","00807d2e":"code","88f5c6de":"markdown","d2d97087":"markdown","22f15579":"markdown","12684f3b":"markdown","414bb3d5":"markdown","b023be75":"markdown","158af687":"markdown","28b56538":"markdown","79bdb835":"markdown","486fbb6d":"markdown","194b4e35":"markdown","394b6e22":"markdown","b6cb44ba":"markdown","6e75593b":"markdown","7b65d512":"markdown","62a0605d":"markdown","8f32fa4d":"markdown"},"source":{"c3b47f9c":"!pip install -q --upgrade wandb","c2aa0282":"import pandas as pd\nimport numpy as np\n\n#Data Visualization\nimport matplotlib.pyplot as plt\n\n#Text Color\nfrom termcolor import colored\n\n#Train Test Split\nfrom sklearn.model_selection import train_test_split\n\n#Model Evaluation\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\nfrom mlxtend.plotting import plot_confusion_matrix\n\n#Deep Learning\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GlobalMaxPooling1D, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.utils import plot_model","e7aaa227":"import wandb\nfrom wandb.keras import WandbCallback\n\n# Login to wandb\nwandb.login()","627f2a66":"# wandb config\nWANDB_CONFIG = {\n     'competition': 'AG News Classification Dataset', \n              '_wandb_kernel': 'neuracort'\n    }","ab735c59":"#File Path\nTRAIN_FILE_PATH = '..\/input\/ag-news-classification-dataset\/train.csv'\nTEST_FILE_PATH = '..\/input\/ag-news-classification-dataset\/test.csv'\n\n#Load Data\ndata = pd.read_csv(TRAIN_FILE_PATH)\ntestdata = pd.read_csv(TEST_FILE_PATH)\n\n#Set Column Names \ndata.columns = ['ClassIndex', 'Title', 'Description']\ntestdata.columns = ['ClassIndex', 'Title', 'Description']\n\n#Combine Title and Description\nX_train = data['Title'] + \" \" + data['Description'] # Combine title and description (better accuracy than using them as separate features)\ny_train = data['ClassIndex'].apply(lambda x: x-1).values # Class labels need to begin from 0\n\nx_test = testdata['Title'] + \" \" + testdata['Description'] # Combine title and description (better accuracy than using them as separate features)\ny_test = testdata['ClassIndex'].apply(lambda x: x-1).values # Class labels need to begin from 0\n\n#Max Length of sentences in Train Dataset\nmaxlen = X_train.map(lambda x: len(x.split())).max()\ndata.head()","e6b7dd34":"data.shape, testdata.shape","759a0826":"#Checking Value counts to determine class balance\ndata.ClassIndex.value_counts()","760d50e9":"testdata.ClassIndex.value_counts()","a64092d2":"#Train Data\ndata.isnull().sum()","3e91de7a":"#Test Data\ntestdata.isnull().sum()","1edd139e":"vocab_size = 10000 # arbitrarily chosen\nembed_size = 32 # arbitrarily chosen\n\n# Create and Fit tokenizer\ntok = Tokenizer(num_words=vocab_size)\ntok.fit_on_texts(X_train.values)\n\n# Tokenize data\nX_train = tok.texts_to_sequences(X_train)\nx_test = tok.texts_to_sequences(x_test)\n\n# Pad data\nX_train = pad_sequences(X_train, maxlen=maxlen)\nx_test = pad_sequences(x_test, maxlen=maxlen)","f478076e":"run = wandb.init(project='ag-news', config= WANDB_CONFIG)","9c4800f3":"model = Sequential()\nmodel.add(Embedding(vocab_size, embed_size, input_length=maxlen))\nmodel.add(Bidirectional(LSTM(128, return_sequences=True))) \nmodel.add(Bidirectional(LSTM(64, return_sequences=True)))\nmodel.add(GlobalMaxPooling1D()) #Pooling Layer decreases sensitivity to features, thereby creating more generalised data for better test results.\nmodel.add(Dense(1024))\nmodel.add(Dropout(0.25)) #Dropout layer nullifies certain random input values to generate a more general dataset and prevent the problem of overfitting.\nmodel.add(Dense(512))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(256))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(128))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(64))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(4, activation='softmax')) #softmax is used as the activation function for multi-class classification problems where class membership is required on more than two class labels.\nmodel.summary()","2768e379":"callbacks = [\n    EarlyStopping(     #EarlyStopping is used to stop at the epoch where val_accuracy does not improve significantly\n        monitor='val_accuracy',\n        min_delta=1e-4,\n        patience=4,\n        verbose=1\n    ),\n    ModelCheckpoint(\n        filepath='weights.h5',\n        monitor='val_accuracy', \n        mode='max', \n        save_best_only=True,\n        save_weights_only=True,\n        verbose=1\n    ),\n    WandbCallback()\n]","fae0645b":"#Compile and Fit Model\n\nmodel.compile(loss='sparse_categorical_crossentropy', #Sparse Categorical Crossentropy Loss because data is not one-hot encoded\n              optimizer='adam', \n              metrics=['accuracy']) \n\nmodel.fit(X_train, \n          y_train, \n          batch_size=256, \n          validation_data=(x_test, y_test), \n          epochs=20, \n          callbacks=callbacks)\n\n# Close W&B run\nwandb.finish()","b858b006":"model.load_weights('weights.h5')\nmodel.save('model.hdf5')","346e9653":"def modelDemo(news_text):\n\n  #News Labels\n  labels = ['World News', 'Sports News', 'Business News', 'Science-Technology News']\n\n  test_seq = pad_sequences(tok.texts_to_sequences(news_text), maxlen=maxlen)\n\n  test_preds = [labels[np.argmax(i)] for i in model.predict(test_seq)]\n\n  for news, label in zip(news_text, test_preds):\n      # print('{} - {}'.format(news, label))\n      print('{} - {}'.format(colored(news, 'yellow'), colored(label, 'blue')))\n","e7c566db":"modelDemo(['New evidence of virus risks from wildlife trade'])","45051b88":"modelDemo(['Coronavirus: Bank pumps \u00a3100bn into UK economy to aid recovery'])","685324bd":"modelDemo(['Trump\\'s bid to end Obama-era immigration policy ruled unlawful'])","8e980be5":"modelDemo(['David Luiz\u2019s future with Arsenal to be decided this week'])","5e7babd2":"modelDemo(['Indian Economic budget supports the underprivileged sections of society'])","fc6b0c59":"labels = ['World News', 'Sports News', 'Business News', 'Science-Technology News']\npreds = [np.argmax(i) for i in model.predict(x_test)]\ncm  = confusion_matrix(y_test, preds)\nplt.figure()\nplot_confusion_matrix(cm, figsize=(16,12), hide_ticks=True, cmap=plt.cm.Blues)\nplt.xticks(range(4), labels, fontsize=12)\nplt.yticks(range(4), labels, fontsize=12)\nplt.show()","00807d2e":"print(\"Recall of the model is {:.2f}\".format(recall_score(y_test, preds, average='micro')))\nprint(\"Precision of the model is {:.2f}\".format(precision_score(y_test, preds, average='micro')))\nprint(\"Accuracy of the model is {:.2f}\".format(accuracy_score(y_test, preds)))","88f5c6de":"## Libraries","d2d97087":"## Data Preprocessing","22f15579":"## Exploratory Data Analysis","12684f3b":"### Confusion Matrix","414bb3d5":"> ### <span style=\"color:green\">Kindly UpVote the Notebook to support my work and motivate me to create better notebooks<\/span>.\n\n#### You can connect with me on [LinkedIn](https:\/\/www.linkedin.com\/in\/ishandutta0098)","b023be75":"# About the Data\n\n![](https:\/\/images.unsplash.com\/photo-1529243856184-fd5465488984?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=1069&q=80)\n\n### Origin\nAG is a collection of more than 1 million news articles. News articles have been gathered from more than 2000 news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004. The dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc), information retrieval (ranking, search, etc), xml, data compression, data streaming, and any other non-commercial activity.\n\nThe AG's news topic classification dataset is constructed by Xiang Zhang from the dataset above. It is used as a text classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).\n\n### Description\n\nThe AG's news topic classification dataset is constructed by choosing 4 largest classes from the original corpus. Each class contains 30,000 training samples and 1,900 testing samples. The total number of training samples is 120,000 and testing 7,600.\n\nThe file classes.txt contains a list of classes corresponding to each label.\n\nThe files train.csv and test.csv contain all the training samples as comma-sparated values. There are 3 columns in them, corresponding to class index (1 to 4), title and description. The title and description are escaped using double quotes (\"), and any internal double quote is escaped by 2 double quotes (\"\"). New lines are escaped by a backslash followed with an \"n\" character, that is \"\\n\".","158af687":"### LSTM - \nTo solve the problem of Vanishing and Exploding Gradients in a deep Recurrent Neural Network, many variations were developed. One of the most famous of them is the Long Short Term Memory Network(LSTM). In concept, an LSTM recurrent unit tries to \u201cremember\u201d all the past knowledge that the network is seen so far and to \u201cforget\u201d irrelevant data. This is done by introducing different activation function layers called \u201cgates\u201d for different purposes. Each LSTM recurrent unit also maintains a vector called the Internal Cell State which conceptually describes the information that was chosen to be retained by the previous LSTM recurrent unit. A Long Short Term Memory Network consists of four different gates for different purposes as described below:-\n\n1. **Forget Gate(f)**: It determines to what extent to forget the previous data.\n\n2. **Input Gate(i)**: It determines the extent of information to be written onto the Internal Cell State.\n\n3. **Input Modulation Gate(g**): It is often considered as a sub-part of the input gate and many literatures on LSTM\u2019s do not even mention it and assume it inside the Input gate. It is used to modulate the information that the Input gate will write onto the Internal State Cell by adding non-linearity to the information and making the information Zero-mean. This is done to reduce the learning time as Zero-mean input has faster convergence. Although this gate\u2019s actions are less important than the others and is often treated as a finesse-providing concept, it is good practice to include this gate into the structure of the LSTM unit.\n\n4. **Output Gate(o)**: It determines what output(next Hidden State) to generate from the current Internal Cell State.\n\nThe basic work-flow of a Long Short Term Memory Network is similar to the work-flow of a Recurrent Neural Network with only difference being that the Internal Cell State is also passed forward along with the Hidden State.\n\n### BiDirectional LSTM - \nUsing bidirectional will run our inputs in two ways, one from past to future and one from future to past and what differs this approach from unidirectional is that in the LSTM that runs backwards we preserve information from the future and using the two hidden states combined we are able in any point in time to preserve information from both past and future.","28b56538":"## Load Weights with Best val Accuracy","79bdb835":"### Data Shape","486fbb6d":"## Model","194b4e35":"### Value Counts","394b6e22":"## Tokenize and Pad Data\n\n### Tokenization\n\nTokenization is a common task in Natural Language Processing (NLP). It\u2019s a fundamental step in both traditional NLP methods like Count Vectorizer and Advanced Deep Learning-based architectures like Transformers.\n\nTokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types \u2013 word, character, and subword (n-gram characters) tokenization.\n\nFor example, consider the sentence: \u201cNever give up\u201d.\n\nThe most common way of forming tokens is based on space. Assuming space as a delimiter, the tokenization of the sentence results in 3 tokens \u2013 Never-give-up. As each token is a word, it becomes an example of Word tokenization.\n\nSimilarly, tokens can be either characters or subwords. For example, let us consider \u201csmarter\u201d:\n\n1. Character tokens: s-m-a-r-t-e-r\n2. Subword tokens: smart-er\n\n### Padding\nAll the neural networks require to have inputs that have the same shape and size. However, when we pre-process and use the texts as inputs for our model e.g. LSTM, not all the sentences have the same length. In other words, naturally, some of the sentences are longer or shorter. We need to have the inputs with the same size, this is where the padding is necessary.","b6cb44ba":"### Null Values","6e75593b":"## Model Evaluation","7b65d512":"### Recall, Precision and Accuracy","62a0605d":"## Compile and Fit Model","8f32fa4d":"## Model Demo"}}