{"cell_type":{"5b9ef332":"code","842a11a7":"code","69ada909":"code","11d3cc18":"code","c96db032":"code","1e56e1f0":"code","b38515d7":"code","d5e822a6":"code","861003c6":"code","5d9bbd5e":"code","da55c5a2":"code","49525d9d":"code","634ae8a1":"code","0b242d89":"code","52b54a79":"code","4a8f13c4":"code","9eb77917":"code","11b36fd7":"code","d780b7f5":"code","cb4b2fee":"code","69fa92f6":"code","c55836b8":"code","10be5ef2":"code","20697252":"code","399f11db":"code","20476751":"markdown","6ca8b1b6":"markdown","f346e5ba":"markdown","0cae41fd":"markdown","3bac7e0a":"markdown","e80b2021":"markdown","51e85bb7":"markdown","7de8e33f":"markdown","e3eec6fe":"markdown","8d85a781":"markdown","5dce6ca7":"markdown","85692521":"markdown","d0428e9a":"markdown"},"source":{"5b9ef332":"%config Completer.use_jedi = False\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(16, 9))\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","842a11a7":"data = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv', index_col='id')\ndata.head()","69ada909":"data.describe()","11d3cc18":"data.info()","c96db032":"data.isnull().sum()","1e56e1f0":"percent = ((data['bmi'].isnull().sum() \/ data.shape[0]) * 100).round(2)\npercent","b38515d7":"data_filled = data.fillna(method='bfill')\n#data_filled = data.fillna(data.mean())\n#data_filled = data.fillna(data.median())\ndata_filled.isnull().sum()","d5e822a6":"s = (data_filled.dtypes == 'object')\ncat_cols = list(s[s].index)\n\n#num_cols = list(data_filled.select_dtypes(exclude=['object']))\nnum_cols = ['age', 'bmi', 'avg_glucose_level']\n\ncat_cols","861003c6":"sns.pairplot(data[num_cols])","5d9bbd5e":"low_cardinality_cols = [col for col in cat_cols if data_filled[col].nunique() < 10]\nlow_cardinality_cols","da55c5a2":"from sklearn.preprocessing import OneHotEncoder\n\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_data = pd.DataFrame(OH_encoder.fit_transform(data_filled[cat_cols]))\n\nOH_cols_data.index = data_filled.index\nOH_cols_data\n\ndata_num = data_filled.drop(cat_cols, axis=1)\nOH_data = pd.concat([data_num, OH_cols_data], axis=1)\nOH_data","49525d9d":"plt.figure(figsize=(16, 10))\nsns.boxplot(data=OH_data[num_cols])","634ae8a1":"Q1 = OH_data[num_cols].quantile(0.25)\nQ3 = OH_data[num_cols].quantile(0.75)\nIQR = Q3 - Q1\n\ndata_out = OH_data[~((OH_data[num_cols] < (Q1 - 1.5 * IQR)) |(OH_data[num_cols] > (Q3 + 1.5 * IQR))).any(axis=1)]\n\nplt.figure(figsize=(16, 10))\nsns.boxplot(data=data_out)","0b242d89":"data_out","52b54a79":"X = data_out.drop('stroke', axis=1)\ny = data_out.loc[:, 'stroke']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","4a8f13c4":"from sklearn.preprocessing import Normalizer\nscaler = Normalizer()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nX_train","9eb77917":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nk_range = range(1, 26)\nscores_list = []\n\nfor k in k_range:\n    knn = KNeighborsClassifier(k)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    scores_list.append(accuracy_score(y_test, y_pred))","11b36fd7":"scores_list","d780b7f5":"plt.figure(figsize=(16, 10))\nplt.plot(k_range, scores_list)\nplt.xlabel('Values of K')\nplt.ylabel('Accuracy score')\nplt.show()","cb4b2fee":"max(scores_list)","69fa92f6":"scores_list.index(max(scores_list))","c55836b8":"knn = KNeighborsClassifier(4)\nknn.fit(X_train, y_train)","10be5ef2":"from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(random_state=1)\n\ntree.fit(X_train, y_train)\ny_pred = tree.predict(X_test)\nscore = accuracy_score(y_test, y_pred)\nscore","20697252":"from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(n_estimators = 1000, random_state=1)\n\nforest.fit(X_train, y_train)\ny_pred = forest.predict(X_test)\nscore = accuracy_score(y_test, y_pred)\nscore","399f11db":"from sklearn.linear_model import LogisticRegression\nlog_model = LogisticRegression()\nlog_model.fit(X_train, y_train)\ny_pred = log_model.predict(X_test)\nscore = accuracy_score(y_test, y_pred)\nscore","20476751":"In the next step we will use a for loop to find the best hyperparameter.\nIn the future, we can use gridsearch for that.","6ca8b1b6":"Decision Tree","f346e5ba":"Logistic regression","0cae41fd":"****Scaling****","3bac7e0a":"# Read and check informations about data","e80b2021":"**Removing outliers**","51e85bb7":"Random forest","7de8e33f":"**As we can see 4% of bmi data is missing**","e3eec6fe":"The best K value is 4 (3 + 1 because loop started from 1 and not 0)","8d85a781":"Check outliers","5dce6ca7":"**I don't want to drop missing rows or column, therefore I fill these NA places with values**","85692521":"**Now let's see missing values**","d0428e9a":"**Splitting the data to train set and test set**"}}