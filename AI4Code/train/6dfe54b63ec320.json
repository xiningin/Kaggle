{"cell_type":{"c5d169b8":"code","a8135439":"code","3f7baa23":"code","889eaab1":"code","11d7b0f6":"code","352b4e29":"code","fe4fa1cc":"code","0e05e91b":"code","2ea49d37":"code","88c138c2":"code","c404fc78":"code","6993bc19":"code","00a84b7f":"code","2ece5f8a":"code","44992652":"code","81309af3":"code","03e45933":"code","23a3c400":"code","beb45a30":"code","47c4a546":"code","1b2ff315":"code","20bc0208":"code","fc44cc8c":"code","71878c7e":"code","7616c41d":"code","5a6f66da":"code","f3df9747":"code","3fbc06dc":"code","8fb1b488":"code","80db80b6":"code","597a8195":"markdown","1f63fd70":"markdown","9f6a5f1d":"markdown","7d33cd87":"markdown","bc2c46af":"markdown","202ed465":"markdown","2b3f238f":"markdown","f0f31340":"markdown","dbbc1871":"markdown","e0496323":"markdown","c28c24f9":"markdown","ed3a8013":"markdown","2f34257a":"markdown","61b82d39":"markdown","b70bcd74":"markdown","4bc81df3":"markdown","4106d994":"markdown","e81fb47e":"markdown"},"source":{"c5d169b8":"import os\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom PIL import Image","a8135439":"content_path = '..\/input\/style-cnn\/Origin\/img'\nstyle_path = '..\/input\/style-transfer-and-object-detection'\ncontent_path_images = [os.path.join(content_path,image) for image in os.listdir(content_path)]\nstyle_path_images = [ os.path.join(style_path, image)for image in os.listdir(style_path)]","3f7baa23":"#To have an idea about the image shapes (content images & style images)\n#as we see, in the result, below: all the images have the shape (400,500)\n\nshape0=[]\nshape1=[]\nshape0_2=[]\nshape1_2=[]\nfor path in content_path_images:\n    img= Image.open(path)\n    img_arr= np.array(img)\n    shape0.append(img_arr.shape[0])\n    shape1.append(img_arr.shape[1])\nfor path in style_path_images:\n    img= Image.open(path)\n    img_arr= np.array(img)\n    shape0_2.append(img_arr.shape[0])\n    shape1_2.append(img_arr.shape[1])\n\nprint(\"The unique values of content image width are: {}\".format(set(shape0)))\nprint(\"The unique values of content image height are: {}\".format(set(shape1)))\nprint('*********************************************************************')\nprint(\"The unique values of style image width are: {}\".format(set(shape0_2)))\nprint(\"The unique values of style image height are: {}\".format(set(shape1_2)))","889eaab1":"input_shape= (400,500)\nvgg19= tf.keras.applications.VGG19(include_top= False,\n                                  input_shape = input_shape+(3,),\n                                  weights= \\\n                                   '..\/input\/vgg19\/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5')\n#Freeze the pretrained layers\nvgg19.trainable= False\n\nprint(vgg19.inputs)\nprint(vgg19.outputs)","11d7b0f6":"def content_cost(content_image_model_output, generated_image_model_output):\n    \"\"\"\n    Computes the content cost: the difference between the content of the content image C & the content of\n    the generated image G.\n    \n    Arguments:\n        content_image_model_output -- the list of the encoding tensors, computed using the pretrained model,\\\n        the Content image and the layer that have been chosen to represent the Content.The last encoding\\\n        tensor is the result of the activation layer that have been chosen to extract the content.\n  \n        generated_image_model_output -- the list of the encoding tensors, computed using the pretrained model,\\\n        the Generated image and the layer that have been chosen to represent the Content.The last encoding\\\n        tensor is the result of the activation layer that have been chosen to extract the content.\n\n        \n    Returns:\n        J_content: a scalar: result of computing content cost.\n    \n    \"\"\"\n    a_content= content_image_model_output[-1]\n    a_generated= generated_image_model_output[-1]\n    \n    #Retrieve the dimensions from the tensors\n    m,n_H,n_W,n_C= a_generated.get_shape().as_list()\n    \n    #Reshape the tensors so that they have the dimensions (m,n_H*n_W,n_C)\n    #we will need this shape later (when computing style cost)\n    a_content_unrolled = tf.reshape(a_content,[m,-1,n_C])\n    a_generated_unrolled = tf.reshape(a_generated,[m,-1,n_C])\n    \n    #Compute the cost (retrieved from the paper mentioned above & adjusted to dimensions)\n    J_content= (1\/(4*n_H*n_W*n_C))* tf.reduce_sum(tf.square(tf.subtract(a_content_unrolled,\\\n                                                                       a_generated_unrolled)))\n    return J_content","352b4e29":"# I Choose The layer to represent the content of the image\nlayers_vgg19=[]\nfor layer in vgg19.layers:\n    layers_vgg19.append(layer.name)\nprint(layers_vgg19)\n","fe4fa1cc":"vgg19.get_layer('block5_conv4').output","0e05e91b":"print(layers_vgg19)","2ea49d37":"# I Choose the layers to represent the Style of the image (with weights)\nstyle_layers= [('block1_conv1',0.2),('block2_conv1',0.2),('block3_conv1',0.2), ('block4_conv1',0.2),\n              ('block5_conv1',0.2)]","88c138c2":"#Define the gram_matrix:\n\ndef gram_matrix(M):\n    \n    \"\"\"\n    Computes the gram_matrix of a given matrix\n    \n    Argument:\n        M --  matrix of shape(n_C, n_H*n_W)\n    \n    Returns:\n        Gram_matrix: Gram matrix of M of shape (n_C,n_C)\n    \"\"\"\n    \n    Gram_matrix= tf.matmul(M, tf.transpose(M))\n    return Gram_matrix","c404fc78":"# Define the function :one_layer_style_cost,that computes the Style cost for only one layer\n\ndef one_layer_style_cost(a_style_layer, a_generated_layer):\n    \"\"\"\n    Computes the Style cost for one hidden layer: it computes the difference between two gram_matrix\\\n    gram_matrix of Style image S and gram_matrix of Generated image G.\n    \n    Arguments:\n        a_style_layer -- a tensor of dimensions (1, n_H, n_W, n_C): hidden layer activations of the\\\n        style image\n    \n        a_generated_layer -- a tensor of dimensions (1, n_H, n_W, n_C): hidden layer activations of the\\\n        generated image\n    \n    Returns: \n        J_style_layer: tensor representing a scalar value(the difference between the gram_matrix\\\n        of Style image and the gram_matrix of the Generated image)  \n    \"\"\"\n    \n    #Retrieve the dimensions from the generated image\n    m, n_H, n_W, n_C= a_generated_layer.get_shape().as_list()\n    \n    #Reshape the images to the shape (n_C, n_H*n_W)\n    a_style_layer= tf.transpose(tf.reshape(a_style_layer,[-1,n_C]),[1,0])\n    a_generated_layer= tf.transpose(tf.reshape(a_generated_layer,[-1,n_C]), [1,0])\n    \n    #Compute the gram matrix of the two layers\n    gram_style = gram_matrix(a_style_layer)\n    gram_gen= gram_matrix(a_generated_layer)\n    \n    #Compute the cost from the original paper mentioned above (in the beginning)\n    J_style_layer= (1\/(4*n_C**2*(n_H*n_W)**2))*tf.reduce_sum(tf.square(tf.subtract(gram_style,gram_gen)))\n    \n    return J_style_layer","6993bc19":"# Define the function : style_cost, that computes the style_cost for several layers and merges them...\n\ndef style_cost(style_image_model_output, generated_image_model_output, style_layers= style_layers):\n    \"\"\"\n    Computes the \n    Arguments:\n    \n        style_image_model_output -- the list of the encoding tensors, computed using the pretrained model,\\\n        the Style image and the layers that have been chosen to extract the Style.\n\n        generated_image_model_output -- the list of the encoding tensors, computed using the pretrained\\\n        model, the Generated image and the layers that have been chosen to extract the Style.\n    \n        Style_layers -- list containing the tuples of the layers that have been chosen to extract\\\n        the Style and the weights of each of them.\n    \n    Returns:\n        J_style: tensor representing a scalar value (coming form computing the style_cost for each\\\n        layer that have been chosen to extract the Style.\n    \"\"\"\n    J_style= 0\n    #I exclude the first element, because it is the array of the input image (of the model)\n    style_activations= style_image_model_output[1:]\n    generated_activations= generated_image_model_output[1:]\n    \n    #I retrieve the number of activations layers, chosen to extract the Style\n    num_layers = len(style_activations)\n    \n    for i, tuple_name_weight in zip(range(num_layers), style_layers):\n        style_cost_layer= one_layer_style_cost(style_activations[i],generated_activations[i])\n        style_cost_layer *= tuple_name_weight[1]\n        J_style += style_cost_layer\n    return J_style","00a84b7f":"def total_cost(J_content,J_style,alpha=10, beta=40):\n    \"\"\"\n    Computes the total cost function.\n    \n    Arguments:\n        J_content -- content cost computed above by the function content_cost.\n        J_style -- style cost computed by the function style cost.\n        alpha: hyperparameter weighting the importance of the content cost.\n        beta: hyperparameter weighting the importance of the style cost\n    \n    Returns:\n        J_total: the total cost.\n    \n    \"\"\"\n    J_total= alpha* J_content + beta* J_style\n    return J_total","2ece5f8a":"image= Image.open(os.path.join(content_path,'22.png'))\ncontent_image_arr= np.array(image)\ncontent_image_arr_reshaped= np.reshape(content_image_arr, ((1,)+ content_image_arr.shape))\nprint(content_image_arr_reshaped.shape)\n\n#The tensor of the content image\ncontent_image= tf.constant(content_image_arr_reshaped)\n\nplt.imshow(content_image_arr)","44992652":"image= Image.open(os.path.join(style_path,'Wallpapers-famous-painting-artist-painter-brush-oil-on-.jpg' ))\nstyle_image_arr= np.array(image)\nplt.imshow(style_image_arr)","81309af3":"arr_style_image =tf.image.random_crop(style_image_arr,[400, 500, 3]) \n#The tensor of the style image\nstyle_image= tf.constant(np.reshape(arr_style_image.numpy(),(1,)+(400,500,3)))\nprint(style_image.shape)","03e45933":"plt.imshow(style_image[0].numpy())","23a3c400":"print(\"Content image, dtype: {}\".format(content_image[0].numpy().dtype))\nprint(\"Style image, dtype: {}\".format(style_image[0].numpy().dtype))","beb45a30":"#I initialize the generated image as a noisy image \ngenerated_image = tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32))","47c4a546":"def mini_vgg19(layers):\n    \"\"\"\n    Creates a mini model from the Vgg19 model, given some hidden layers\n    Argument:\n        layers -- a list composed of tuples :(layer_name, weight)\n    \"\"\"\n    mini_model_outputs= [vgg19.get_layer(layer_weight[0]).output for layer_weight in layers]\n    model= tf.keras.Model(inputs= vgg19.input, outputs= mini_model_outputs)\n    return model","1b2ff315":"content_layer= [('block5_conv4',1)]\nmini_vgg19_model= mini_vgg19(style_layers+ content_layer)","20bc0208":"content_image= tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32))\nstyle_image= tf.Variable(tf.image.convert_image_dtype(style_image, tf.float32))\n\nC_list_vgg19_encoders= mini_vgg19_model(content_image)\nS_list_vgg19_encoders= mini_vgg19_model(style_image)\nG_list_vgg19_encoders= mini_vgg19_model(generated_image)","fc44cc8c":"#I need these  function for visualization\n\ndef tensor_to_image(tensor):\n    \"\"\"\n    Converts the given tensor into a PIL image\n    \n    Arguments:\n    tensor -- Tensor\n    \n    Returns:\n    Image: A PIL image\n    \"\"\"\n    tensor = tensor * 255\n    tensor = np.array(tensor, dtype=np.uint8)\n    if np.ndim(tensor) > 3:\n        assert tensor.shape[0] == 1\n        #the array\n        tensor = tensor[0]\n    return Image.fromarray(tensor)","71878c7e":"#The optimizer\nopt = tf.optimizers.Adam(learning_rate=0.001, beta_1=0.99, epsilon=1e-1)","7616c41d":"#This function is needed for training the model\ndef clip_0_1(image):\n    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)","5a6f66da":"#Define train_step\n\ndef train_step_one(image):\n    with tf.GradientTape() as tape:\n        aciv_G = mini_vgg19_model(image)\n        J_content= content_cost(C_list_vgg19_encoders,aciv_G)\n        J_style= style_cost(S_list_vgg19_encoders,aciv_G)\n        J_total= total_cost(J_content, J_style)\n   \n    grad = tape.gradient(J_total, image)\n    opt.apply_gradients([(grad, image)])\n    image.assign(clip_0_1(image))\n","f3df9747":"#Execute the optimization \nimport time \nstart= time.time()\nepochs= 40\nsteps_per_epoch= 100\nfor i in range(epochs):\n    for j in range(steps_per_epoch):\n        step +=1\n        train_step_one(generated_image)\nprint(\"Train step: {}\".format(step))\nimage = tensor_to_image(generated_image)\nend= time.time()\nprint(\"Total time: {:.1f}\".format(end-start))\n","3fbc06dc":"print(\"*********Generated image*********\\n\")\nimage\n","8fb1b488":"# Show the 3 images together\n\nlist_names=['Content_image','Style image','Generated image']\nimages= {'Content_image':content_image[0],\n         'Style image':style_image[0],\n         'Generated image':image  }\nplt.figure(figsize=(20,20))\nfor i in range(3):\n    plt.subplot(1,3,i+1)\n    plt.imshow(images[list_names[i]])\n    plt.title(list_names[i])\n  \n    ","80db80b6":"list_names=['Content_image','Style image','Generated image']\nimages= {'Content_image':content_image[0],\n         'Style image':style_image[0],\n         'Generated image':image  }\nplt.figure(figsize=(20,20))\nfor i in range(3):\n    plt.subplot(3,1,i+1)\n    plt.imshow(images[list_names[i]])\n    plt.title(list_names[i])\n  ","597a8195":"- **Create a Model from the Vgg19 model that gives the outputs of some chosen hidden layers**","1f63fd70":"\n#### **2-2 Computing the Style Cost**","9f6a5f1d":"- **First, we explore our datasets to retrieve the images shape**","7d33cd87":"- **Load a Style Image**","bc2c46af":"#### **2-3 Computing the Total Cost**","202ed465":"### **Step_2: Neural Style Transfer: Computing the Content_Cost & the Style_Cost**\n\n#### **2-1 Computing the Content Cost**","2b3f238f":"- **Get the list of encoders for content, style and generated images**","f0f31340":"### **Step_3: Implementing Neural Style Transfer**","dbbc1871":"- **Second: Loading the VGG19 Model**","e0496323":"### **Step_1: Neural Style Transfer Concept & Transfer Learning (VGG19)**\n\n- Neural Style Transfer merges two images: **\"Content\"** image(C) and **\"Style\"** image (S) to create a **Generated image (G).**\n- Neural Style Transfer, uses a **pretrained** convolutional neural network and builds on top of it**(Transfer Learning)**.\n- I will use the **VGG** network (published by the Visual Geometry Group at University of Oxford in 2014). Here, you find the link of the **original paper**: https:\/\/arxiv.org\/abs\/1508.06576, this model has already be trained on a very **large ImageNet database**, and has learned to recognize, **low level features** (at shallow layers), and **high level features** (at deeper layers)\n","c28c24f9":"#### **Steps**\n- I need the **gram_matrix** (representing the **Style** of a given image). The **gram_matrix** measures how **similar** the activations of **filter i** are to the activations of **filter j**: in other words it measures the **correlation between two filters.** with the unrolled version, mentioned above (in the function of computing the content cost), the gram_matrix is the multiplication of the **unrolled filter** by **its transpose** (the diagonal elements measure **how active a filter is**(if it is a filter of **detecting horizontal edges**, so large diagonal elements show that the image has a lot of horizontal textures, and the rest of elements measure the correlation bteween the two filters)\n\n- Then,I compute the **Style cost** for one chosen layer (we retieve the formula from the original paper, mentioned above). it uses the two gram_matrix: **gram_matrix** of the **style hidden layer activations**(representing the Style of the image S: Style image) & the **gram_matrix**  of the **generated hidden layer activations** (representing the Style of G: Generated image).\n- The difference between computing Content cost & Style cost, that with Content Cost, we only use **one chosen layer**, but with **Style cost**, we need **to capture the Style** from **several layers** and **merge them** (not only one layer)in order to get better results. We use weighted layers to reflect how much each layer contibute to the **Style**(I give equal weights to layers)\n","ed3a8013":"- **Load a Content Image**","2f34257a":"## **Table of Contents**\n#### **Step_0: Little Introduction: I am already a painter since childhood**\n#### **Step_1: Neural Style Transfer Concept & Transfer Learning (VGG19)**\n#### **Step_2: Neural Style Transfer: Computing the Content_Cost & the Style_Cost**\n#### **Step_3: Implementing Neural Style Transfer**\n#### **Step_4: Train the model & Generate the final images**","61b82d39":"### **Step_4: Train the model & Generate the final images**","b70bcd74":"**=> I choose the Conv layer before the last pool layer: block5_conv4 (because it captures the high level features of the Content image)**","4bc81df3":"### **Step_0: Little Introduction: I am already a painter since childhood**\nJust a little introduction before starting: In fact, I have a **natural gift for painting** since **my childhood (since the age of 8)** and people often say that **I'm talented...** I have done a lot of canvases (I feel fulfilled when I paint). Now, using **Artificial intelligence** in order to transfer styles of **great painters** to ordinary images and generate very beautiful paintings: I really liked it (it further enriches my imagnation as a painter: **how to be really inspired by the styles of great painters ...): amazing!** ","4106d994":"- **Initialize the image to be Generated**","e81fb47e":"##### **Make the Generated Image match the Content Image**\n\n- Neural Style Transfer aims to produce a **\"Generated image G\"**, having a **content** that matches the **content** of the **\"Content image C\"**. This content comes from a chosen **layer level.**\n- Ideally, I should choose this layer level between **shallow layers(that detect low-level features)** and **deep layers (that detect high-level features)**.\n\n"}}