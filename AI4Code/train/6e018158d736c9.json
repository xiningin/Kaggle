{"cell_type":{"3c92ae4b":"code","53d3ef01":"code","2dd9ca89":"code","d75209e6":"code","05a84b98":"code","1b39cbfe":"code","81aeab0c":"code","2b3684af":"code","874be4c0":"code","c1b03efa":"code","52d8bc15":"code","885f64de":"code","4f5753b2":"code","0773bf37":"code","82e246af":"code","9fc227de":"code","180bea45":"code","bfd0a633":"code","99c64899":"code","b7755b69":"code","f0722328":"code","d5382095":"code","776f9d01":"code","4b271abe":"code","0ff248ea":"code","2b57f86b":"code","8b2fbacb":"code","d8f571c2":"code","ea7a0c80":"markdown","6eb37c82":"markdown","b9208ab9":"markdown","3dec30d9":"markdown","d419759a":"markdown","f98bdbac":"markdown","7532fbeb":"markdown","64c68a38":"markdown","c9c41b83":"markdown","c72fae57":"markdown","75e94a99":"markdown","e562fd94":"markdown","c09b155e":"markdown","ed625d05":"markdown","cdc739b2":"markdown","18c61b8b":"markdown","fae5225a":"markdown","d085b515":"markdown","c1663e29":"markdown","00a150bb":"markdown","dd3b987c":"markdown","608f1660":"markdown","17b11c33":"markdown","fb9e5453":"markdown","27be9ccf":"markdown","363f32cb":"markdown","b514296d":"markdown","f1f34e44":"markdown","cfb69a92":"markdown","90709471":"markdown","f015cb19":"markdown","05ead821":"markdown","74098d42":"markdown","750eb958":"markdown","f13320b6":"markdown"},"source":{"3c92ae4b":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.pipeline import Pipeline\nfrom copy import deepcopy\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn.svm import LinearSVC","53d3ef01":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Reading data from csv files:\ntrain= pd.read_csv('..\/input\/titanic\/train.csv',nrows=100000)\ntest = pd.read_csv('..\/input\/titanic\/test.csv',nrows=100000)","2dd9ca89":"train= train.drop(['PassengerId','Name','Ticket','Cabin'],axis=1)\nprint(train.dtypes)\ntrain","d75209e6":"train[\"Sex\"].value_counts()","05a84b98":"train[\"Embarked\"].value_counts()","1b39cbfe":"cleanup_nums = { \"Embarked\": {\"S\": 0, \"C\": 1, \"Q\": 2 },\"Sex\":     {\"male\": 0, \"female\": 1}}\n","81aeab0c":"train.replace(cleanup_nums, inplace=True)\ntrain.head()","2b3684af":"train=train.dropna()\ntrain","874be4c0":"\ndef get_best_model_and_accuracy(model, params, X, y):\n    grid = GridSearchCV(model,params,error_score=0., verbose=0, n_jobs=2)\n    grid.fit(X, y) # fit the model and parameters\n    s= \"Best Accuracy: {}\".format(grid.best_score_)+ '\\n'+\\\n       \"Best Parameters: {}\".format(grid.best_params_) +'\\n'+\\\n       \"Average Time to Fit (s):{}\".format(round(grid.cv_results_['mean_fit_time'].mean(), 3)) +'\\n'+\\\n       \"Average Time to Score (s):{}\".format(round(grid.cv_results_['mean_score_time'].mean(), 3))\n    return s\n","c1b03efa":"# Next we need to separate all columns from the target column to allow our models to make predictions. \nX = train.drop('Survived', axis=1)\n# create our response variable\ny = train['Survived']\n# here we will write our machine learning paramters \ntree_params = {'max_depth':[None,1, 3, 5,7]}\n# decision tree is the classifier thta we will use. \nd_tree = DecisionTreeClassifier()\n\nprint('Results for training set:','\\n',get_best_model_and_accuracy(d_tree,tree_params,X, y))\n\n","52d8bc15":"train.corr()\n# here is a table that shows the correlation between each feature with the rest of the features. ","885f64de":"train.corr()['Survived'] \n# we will hone on the correlation values between the target feature since that's what we are investigating. ","4f5753b2":"all_corr=train.corr()['Survived'].abs() >= .2\nhighly_correlated =all_corr[all_corr==True].index\n# Only the features that have a correlation value of 0.05 or above with the target will be selected. \nhighly_correlated= highly_correlated.tolist() \nhighly_correlated.remove('Survived')\nprint('The number of features removed out of', all_corr.size, 'is', all_corr.size- len(highly_correlated),', leaving',len(highly_correlated),'selected features.')\nhighly_correlated","0773bf37":"#Only colums with features that have a correlation value 0.2 or above as in the highly_correlated list \n#will be used to make predictions for the 'Survived' column.\nX_subsetted = X[highly_correlated]\nprint(get_best_model_and_accuracy(d_tree, tree_params, X_subsetted, y))\n","82e246af":"# keep only the best 4 features according to p-values of ANOVA test\nk_best = SelectKBest(f_classif, k=4)","9fc227de":"# fit the data and then tranform it.\nk_best.fit_transform(X, y)\nk_best","180bea45":"# get the p values of columns\nk_best.pvalues_\n# make a dataframe of features and p-values\n# sort that dataframe by p-value\np_values = pd.DataFrame({'column': X.columns, 'p_value': k_best.pvalues_}).sort_values('p_value')\n# show the top 4 features\np_values.head(4)","bfd0a633":"# features with a low p value\np_values[p_values['p_value'] < .01]","99c64899":"k_best = SelectKBest(f_classif)\n# setting the paramters\ntree_pipe_params = {'classifier__max_depth': [None, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]}\n# The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters.\nselect_k_pipe = Pipeline([('k_best', k_best),('classifier', d_tree)])\nselect_k_best_pipe_params = deepcopy(tree_pipe_params)\nselect_k_best_pipe_params.update({'k_best__k':[1,2,3,4,5,6] + ['all']})\nprint(select_k_best_pipe_params,'\\n')\n\nprint('Results for training set:','\\n',get_best_model_and_accuracy(select_k_pipe, select_k_best_pipe_params, X, y),'\\n')","b7755b69":"# by default, 75% goes to the training set while 25% goes to the test set. \nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nprint('train_test_ split is used to split the dataset into two pieces, so that the model can be trained and tested on different data. This is a better method for evaluating the model performance rather than testing it on the training data only. ')","f0722328":"tree = DecisionTreeClassifier()\ntree.fit(X, y)\nimportances = pd.DataFrame({'importance': tree.feature_importances_,'feature':X.columns}).sort_values('importance', ascending=False)\nimportances.head()","d5382095":"\nselect_from_model = SelectFromModel(DecisionTreeClassifier(),threshold=.05)\nselected_X = select_from_model.fit_transform(X, y)\n# this shows the features that are selected by the model. \nselected_X.shape","776f9d01":"# create a SelectFromModel that is tuned by a DecisionTreeClassifier\nselect = SelectFromModel(DecisionTreeClassifier())\nselect_from_pipe = Pipeline([('select', select),('classifier', d_tree)])\nselect_from_pipe_params = deepcopy(tree_pipe_params)\nselect_from_pipe_params.update({'select__threshold': [.01, .05, .1, .2, .25, .3, .4, .5, .6, \"mean\",\"median\", \"2.*mean\"],'select__estimator__max_depth': [None, 1, 3, 5, 7]})\nprint(select_from_pipe_params,'\\n')\nprint('Results for training set:','\\n', get_best_model_and_accuracy(select_from_pipe,select_from_pipe_params,X_train, y_train), '\\n')\nprint('Results for test set:','\\n',get_best_model_and_accuracy(select_from_pipe,select_from_pipe_params,X_test, y_test))","4b271abe":"select_from_pipe.set_params(**{'select__threshold': 0.01,'select__estimator__max_depth': None,'classifier__max_depth': 3})\n# fit our pipeline to our data\nselect_from_pipe.steps[0][1].fit(X, y)\n# this presents the name of the featurs that are selected from the model.\nX.columns[select_from_pipe.steps[0][1].get_support()]","0ff248ea":"logistic_selector = SelectFromModel(LogisticRegression(max_iter = 1500))\nregularization_pipe = Pipeline([('select', logistic_selector),('classifier', tree)])\nregularization_pipe_params = deepcopy(tree_pipe_params)\nregularization_pipe_params.update({'select__threshold': [.01, .05, .1, \"mean\", \"median\", \"2.*mean\"],'select__estimator__penalty': ['l1', 'l2'],})\n# l1 and l2 are regularization methods.\n\nprint(regularization_pipe_params) \nprint('Results for training set:','\\n',get_best_model_and_accuracy(regularization_pipe,regularization_pipe_params,X_train, y_train),'\\n')\nprint('Results for test set:','\\n',get_best_model_and_accuracy(regularization_pipe,regularization_pipe_params,X_test, y_test))","2b57f86b":"regularization_pipe.set_params(**{'select__threshold': 0.01,'classifier__max_depth': 5,'select__estimator__penalty': 'l2'})\n# fit our pipeline to our data\nregularization_pipe.steps[0][1].fit(X, y)\n# list the columns  selected by calling the get_support() method from SelectFromModel\nX.columns[regularization_pipe.steps[0][1].get_support()]","8b2fbacb":"svc_selector = SelectFromModel(LinearSVC(max_iter=100000,dual=False))\nsvc_pipe = Pipeline([('select', svc_selector),('classifier', tree)])\nsvc_pipe_params = deepcopy(tree_pipe_params)\nsvc_pipe_params.update({'select__threshold': [.01, .05, .1, \"mean\", \"median\", \"2.*mean\"],'select__estimator__penalty': ['l1', 'l2'],'select__estimator__loss': ['squared_hinge', 'hinge'],'select__estimator__dual': [True, False]})\nprint(svc_pipe_params,'\\n') # 'select__estimator__loss': ['squared_hinge','hinge'], 'select__threshold': [0.01, 0.05, 0.1, 'mean', 'median','2.*mean'], 'select__estimator__penalty': ['l1', 'l2'],'classifier__max_depth': [1, 3, 5, 7], 'select__estimator__dual': [True,False]}\nprint('Results for training set:','\\n',get_best_model_and_accuracy(svc_pipe,svc_pipe_params,X_train, y_train),'\\n')\nprint('Results for test set:','\\n',get_best_model_and_accuracy(svc_pipe,svc_pipe_params,X_test, y_test))","d8f571c2":"svc_pipe.set_params(**{'select__estimator__loss': 'squared_hinge','select__threshold': 0.01,'select__estimator__penalty': 'l1','classifier__max_depth': 5,'select__estimator__dual': False})\n# fit our pipeline to our data\nsvc_pipe.steps[0][1].fit(X, y)\n# list the columns that the SVC selected by calling the get_support() method from SelectFromModel\nX.columns[svc_pipe.steps[0][1].get_support()]","ea7a0c80":"# Hypothesis testing using KBest <a id=\"4.2\"><\/a>\nThis is a method that involves calculating p-value to determine whether a hypothesis can be rejected or not.<br\/> P-value stands for 'probablity value', it indicates how likely it is that a result occured by chance alone. <br\/>\nSelectKBest scores the features against the target variable using a function (in this case f_classif but could be others) and then keeps the most significant features i.e the features that have the highest p-value. ","6eb37c82":"# 2- Model-Based Selection <a id=\"5\"><\/a>\nModel selection is the process of choosing between different machine learning approaches or choosing between different hyperparameters or sets of features for the same machine learning approach. <br\/>\nThe two main machine learning models that we will use in this section for the purposes of feature selection are tree-based models and linear models.They both have a notion of feature ranking that are useful when subsetting feature sets.<br\/>  In this dataset, we will use decision tree classifier for tree-based model and logistic regression and SVM for linear models. \n","b9208ab9":"The SVM module (SVC, NuSVC, etc) is a **wrapper** around the libsvm library and supports different kernels while LinearSVC is based on liblinear and only supports a linear kernel. ","3dec30d9":"# Feature Selection Techniques tutorial ","d419759a":"# iii) LinearSVC <a id=\"5.3\"><\/a>\n\nSVC is a linear model that uses linear supports to seperate classes in euclidean space. This model can only work for binary classification tasks. ","f98bdbac":"# ii) Logistic Regression <a id=\"5.2\"><\/a>\nA linear regression model predicts the target as a weighted sum of the feature inputs. \nLogistic Regression measures the relationship between the dependent variable (our label, what we want to predict) and the one or more independent variables (our features), by estimating probabilities using it\u2019s underlying logistic function.<br\/>\nLinear models work by placing coefficients next to features that tells how much it affects the response when the feature is changed. <br\/>\nYou can read more about it's technicality here https:\/\/christophm.github.io\/interpretable-ml-book\/limo.html#limo.","7532fbeb":"# Tips on when to use each technique: \nIf your features are mostly categorical, you should start by trying to implement a SelectKBest with a Chi2 ranker or a tree-based model selector ( like decision tree). <br\/>\nIf your features are largely quantitative, using linear models as model-based selectors and relying on correlations tends to yield greater results.<br\/>\nIf you are solving a binary classification problem, using a Support Vector Classification model along with a SelectFromModel selector will probably fit nicely, as the SVC tries to find coefficients to optimize for binary classification tasks.","64c68a38":"Check https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html?highlight=pipeline#sklearn.pipeline.Pipeline to read about Pipeline function.","c9c41b83":"# Feature Selection Techniques <a id=\"3\"><\/a>\nWe will explore three main general techniques and illustrate each one by implementing code on the given dataset. ","c72fae57":"# When do we use logistic Regression? \n**You should think about using logistic regression when your Y variable takes on only two values (e.g when you are facing a classification problem)**\n\n# When is using any type of regression not suitable?\nThe two most common kinds of issues are (1) when your data contain major violations of regression assumptions and (2) when you don't have enough data (or of the right kinds).\n\nCore assumptions behind regression include\n\n- That there is in fact a relationship between the outcome variable and the predictor variables.\n\n- That observations are independent.\n\n- That the residuals are **normally distributed** and **independent** of the values of variables in the model.\n\n- That each predictor variable is not a linear combination of any others and is not extremely correlated with any others.\n\n- Additional assumptions depend on the nature of your dependent variable; for example whether it is measured on a continuous scale or is categorical yes\/no etc. The form of regression you use (linear, logistic, etc.) must match the type of data.\n\nNot having enough data means having very few cases at all or having large amounts of missing values for the variables you want to analyze. If you don't have enough observations, your model either will not be able to run or else the estimates could be so imprecise (with large standard errors) that they aren't useful.\n***\nsource: https:\/\/www.answers.com\/Q\/When_regression_is_not_applicable?#slide=1","75e94a99":"Some columns will be removed in this dataset. The tutorial focuses on implementing feature selection techniques, so we can deal with the removed features ( 'Cabin' and 'Ticket' ) in another version for now. ","e562fd94":"Note: <br\/>\nWe have a come across the term 'wrapper' several times in this tutorial. There are three classes for feature selection: filter, wrapper, and embedded methods. Wrapper methods evaluate the importance of features based on the classifier performance. Filter methods measure evaluates the features via univariate statistics instead of cross-validation performance.Finally, embedded methods, are quite similar to wrapper methods, however, the difference is that an intrinsic model building metric is used during learning. Please refer back to this to learn more https:\/\/sebastianraschka.com\/faq\/docs\/feature_sele_categories.html.","c09b155e":"# Selecting features using Pearson's Correlation <a id=\"4.1\"><\/a>\nPearson correlation is a statistic that measures linear correlation between two variables X and Y. It has a value between +1 and \u22121.","ed625d05":"\n# i) Decision Trees <a id=\"5.1\"><\/a>\nA decision tree is a decision support tool that uses a tree-like model of decisions. Decision Trees provide an effective means for decision making because they consider all possible branches \/ scenarios as well as their outcome. ","cdc739b2":"# References: <a id=\"6\"><\/a>\n* Introduction to machine learning by Andreas C. M\u00fcller and Sarah Guido\n* Feature Engineering made easy by Sinan Ozdemir and Divya Susarla\n* https:\/\/christophm.github.io\/interpretable-ml-book\/limo.html#limo\n* https:\/\/www.datacamp.com\/community\/tutorials\/svm-classification-scikit-learn-python\n* https:\/\/dataschool.com\/fundamentals-of-analysis\/correlation-and-p-value\/#:~:text=The%20two%20most%20commonly%20used,an%20experiment%20is%20statistically%20significant.\n* https:\/\/lumina.com\/causal-hypothesis-testing\/#:~:text=The%20framework%20defines%20a%20p,is%20used%20in%20classic%20statistics.\n* https:\/\/www.coursehero.com\/file\/p4jtjlo\/The-disadvantages-of-the-Pearson-r-correlation-method-are-It-assumes-that-there\/#:~:text=The%20disadvantages%20of%20the%20Pearson%20r%20correlation%20method%20are%3B%E2%9D%96,does%20not%20necessarily%20mean%20very\n* https:\/\/www.wisdomjobs.com\/e-university\/research-methodology-tutorial-355\/limitations-of-the-tests-of-hypotheses-11539.html\n* And, ofcourse Qoura and stackOverflow. ","18c61b8b":"This models a statistical test known as ANOVA. <br\/>\nANNOVA: Analysis of variance is a collection of statistical models and their associated estimation procedures used to analyze the differences among group means in a sample","fae5225a":"# Measuring the effect of feature selection on machine learning performance<a id=\"2\"><\/a>\nWe need a function to evaluate the performance of each feature selection technique we use in this tutorial. To optimise the performance of our machine learning model, we will tune its hyperparameters using gridsearch. Check this https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html to read about gridSearchCV.\n","d085b515":"Only the selected features in the list 'highly_correlated' will be used to make predictions for the 'Survived' column.","c1663e29":"# What is feature selection<a id=\"1\"><\/a>\n\nFeature selection is a subset of feature engineering and it aims at excluding features that are not important, leaving only the better features i.e the features that are the best when it comes to model prediction. \n***\n# **Advantages of feature selection:**\n1. results in a better performing model.\n2. creates an easier to understand model. \n3. results in a model that runs faster. \n4. reduces the chance of overfitting. ","00a150bb":"This is an introductory tutorial on some famous feature selection techniques. A large portion of the techniques are studied from\n'Feature Engineering made easy' by Sinan Ozdemir and Divya Susarla. You can use this tutorial as a support while reading from the book and feel free to play with the code. I have used a famous dataset 'Titanic' since it is pretty easy to understand, and our main goal is to predict who will survive ('Survived' column).A brief description for each feature selection technique is provided, in addition to its pros and cons. ","dd3b987c":"Check this to have a better understanding of train_test_split https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html?highlight=train_test_split#sklearn.model_selection.train_test_split","608f1660":"For the machine learning models to understand the data, we need to change labels into numerical categories.","17b11c33":"In linear models, regularization is a method for imposing additional constraints to a\nlearning model, where the goal is to prevent overfitting and improve the generalization of\nthe data. l1 and l2 are regularization methods.","fb9e5453":"# Pros and Cons of Logistic Regression:\n**Pros:** <br\/>\n1- Logistic regression can be a good (and effective) choice, provided that your dataset is fit for it.[Click here for explanation](http:\/\/) <br\/>\n2 Logistic regression is less prone to over-fitting but it can overfit in high dimensional datasets. You should consider Regularization (L1 and L2) techniques to avoid over-fitting in these scenarios. Regularization is a method for imposing additional constraints to a learning model.<br\/>\n3- In addition to giving a measure of how relevant a coefficient size, logistic regression also provides information about the predictor's association ( positive or negative). \n<br\/><br\/>\n**Cons:** <br\/>\n1- can only be used to predict discrete functions.<br\/>\n2- should not be used when the number of observations is less than the number of features. <br\/>\n3- assumes that there is linearity between dependent and independent variables which is rarely the case as data is usually a unorganized. \n","27be9ccf":"#  Now, having gone through the three model-based selection techniques, which one do you think is the most efficient in this dataset? ","363f32cb":"# Data Preparation <a id=\"0\"><\/a>","b514296d":"SelectfromModel is a skikit wrapper that captures the top k most importance features by considering a machine learning interal metric for feature importance. SelectFromModel is similar to SelectKBest as it picks the top k most important features. However, it measures the importance of a feature based on a model's internal metric for feature importance rather than the p-value. You can read more about SelectFromModel here https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.SelectFromModel.html?highlight=selectfrom%20mode#sklearn.feature_selection.SelectFromModel","f1f34e44":"# Pros and Cons of Pearsn's and Hypothesis Testing: \n**Correlation coefficient like Pearson's: <br \/>**\n**Pros:** <br\/>\n1- It gives an idea about how well the variables are related to each other.<br\/>\n**Cons:** <br\/>\n1- It assumes that there is always of linear relationship between the variables which might not be the case at all time.<br\/>\n2- It does not determine causation between any two variables.\n<br \/>\n<br \/>\n**Hypthesis Testing:** <br \/>\n**Pros:** <br\/>\n1- It aids in reaching a conclusion by examining a sample. <br \/>\n**Cons:** <br\/>\n1- Results of significance tests are based on probabilities and as such cannot be expressed with full certainty. <br \/>\n2- Statistical inferences based on the significance tests cannot be said to be entirely correct evidences concerning the truth of the hypothesis.\n***\nIt is worthy to note the correlation and  p-value are the most commoon statistical tests for identifying relationship between variables. \n\n**Difference between correlation and P value:**<br \/>\n\nCorrelation is used to measure *how strong a relationship is between variables*. <br \/>\nFamous types of correlations: <br \/>\n1- Pearson <br \/>\n2- Kendall <br \/>\n3- Spearman <br \/>\n\nOn the other hand, p-value measures *how well your data rejects the null hypothesis*, which claims that the two compared have no relationship. \n\nBoth correlation and p-value give a measure about the relationship but correlation does not imply causation while p-value provides some support for causation as is present in the dataset. You can read more about this here https:\/\/lumina.com\/causal-hypothesis-testing\/#:~:text=The%20framework%20defines%20a%20p,is%20used%20in%20classic%20statistics.\n***\n**Bottom line:** <br \/>\nIf you want to measure the statistical significance between two continuous variables, use correlation techniques like Pearson's. \n\nIf you want to draw conclusions about the population using sample data, go for hypothesis testing. A hypothesis test basically helps us in making a decision about the population supported by sample data. \n","cfb69a92":"# Decision trees pros and cons: \n\n**Pros:** <br\/>\n1- Compared to other algorithms, decision trees require **less pre-processing**.Decision trees are definitely more robust to Outliers and missing values than regression techniques. <br\/>\n2- **No normalization** required. <br\/>\n3- **No scaling** required. <br\/>\n4- **Not necessary to deal with missing values**. Missing values data does not affect the process of building decision tree. This is because they work on segmentation of population and treat all missing values as a different class itself.<br\/>\n5- Desicion trees are **easy to understand**.Decision trees require no complex formulas. They consider all of the decision alternatives for quick comparisons in a format that is comprehensible.<br\/>\n\n**Cons:**<br\/>\n1- Mathematical calculation of decsion tree requires **more memory and time**.<br\/>\n2- A small change in the data can result in large change in the tree structure, thus this can have a **large effect on the tree model sensitivity**. \n<br\/>\n3-**Higher space and time complexity** for a decision tree. \n***\nAs a side note, a single decision tree is often a weak learner so random forect is usually used instead for better prediction. ","90709471":"# Pros and cons of SVC: \n**Pros:** <br\/>\n1- Effective in high dimensional spaces. <br\/>\n2- Memory efficient because it uses a subset of training points in the decision vectore ( support vectors).<br\/>\n\n**Cons:**<br\/>\n1- Similar to linear regression, SVC should not be used when the number of observations is less than the number of features ( this is known as overfitting). <br\/>\n2- SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation.\n","f015cb19":"We could continue onward by trying several other tree-based models, such as RandomForest, ExtraTreesClassifier.","05ead821":"You can read more about fit_transform here https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html.","74098d42":"Rows with null values will be removed to avoid errors while using the dataset in the machine learning models.","750eb958":"# 1- Univariate Statistics <a id=\"4\"><\/a>\n\nUnivariate statistics involves checking for statistically significant relationship between each feature and the target. We will focus on two techniques under this heading: Pearson's Correlation and Hypothesis Testing. \n","f13320b6":"# Table of contents:\n0. [Data Preparation](#0)\n1. [What is feature selection](#1)\n2. [Measuring the effect of feature selection on machine learning performance](#2)\n3. [Feature Selection Techniques](#3)\n     - [Univariate Statistics](#4)\n          * [Selecting features using Pearson's Correlation](#4.1)   \n          * [Hypthesis Testing using selectKBest](#4.2) \n     - [Model-Based Feature Selection](#5)\n          * [Decision Trees](#5.1)\n          * [Logistic Regression](#5.2)\n          * [LinearSVM](#5.3)\n4. [References](#6)"}}