{"cell_type":{"24b8e662":"code","34f2321c":"code","7847d1b4":"code","5fd59baf":"code","ee4c9c5f":"code","3046386e":"code","87a16474":"code","e8e52513":"code","d42f05ad":"code","302b90f9":"code","ae637e95":"code","d9e8d746":"code","611a321e":"code","92fd2416":"code","4158a827":"code","49e65f38":"code","55f17913":"code","66dbe598":"code","10ac077b":"code","6f8ae2f7":"code","0d4b2bb9":"code","18683d9a":"code","cd4eef80":"code","da498bef":"code","075d9296":"code","2034c93c":"code","80b65de5":"code","bd7b6026":"code","f8743ad5":"code","23fa4ec1":"code","5f39cd89":"code","6cb1548d":"code","02304ba5":"code","286e51fb":"code","982ffc12":"code","122b6aa4":"code","eb9d2348":"code","16b9520b":"code","6c0cc378":"code","84ab5a8a":"code","3c625949":"code","f7bf7a39":"code","7f04fe8b":"code","4f9c4828":"code","83161a1c":"code","3debba40":"code","87834aaf":"code","7ca9b46c":"code","67b6ddd9":"code","f4a784a2":"code","2ac60de1":"code","c2cf9d8b":"code","7a907b55":"code","e0a11aaf":"code","1a3a1ae7":"code","7405d0ae":"code","07a9a630":"code","5244ffec":"code","7530bb3e":"code","2b854fed":"code","d7bd8f41":"code","630eb9ef":"code","1458c7c6":"code","228fe43d":"code","8d30e27c":"code","21dbac8a":"code","1b6bc670":"code","78f51a1f":"code","f8fb5f3b":"code","4a790ffc":"code","9d3308fa":"code","d1175dd1":"code","213cf9f1":"code","7ae62e2d":"code","863c6756":"code","24830fa2":"code","0f7316f3":"code","77164f1f":"code","564b04af":"code","e0b0aa99":"code","05230ebf":"code","66ace7f6":"code","894ad5b0":"code","21e20c98":"code","217b611d":"code","c1feeb8d":"code","9a7defe0":"code","58b13b39":"code","ceb91ff7":"code","4c4a2cea":"code","4f3305aa":"code","da189969":"code","0217fe4d":"code","39ea0408":"code","bd22a2e1":"code","4ff9c363":"code","0765a21c":"code","b6fd6b8a":"code","35e3cbd0":"code","da46bee9":"code","b3cdbf65":"code","88607706":"code","7776dcf1":"code","fca5fe9d":"code","4ae1fc49":"code","d353e1c6":"code","736406ac":"code","594b0b69":"code","7ceea625":"code","03378125":"code","650d5026":"code","6a2769a1":"code","d92618dd":"code","ac983f48":"code","e58cc66a":"code","b0eb7272":"code","17e6cb65":"code","bc78d5ac":"code","2cb09707":"code","faac4fb8":"code","959a0a11":"code","8177d83f":"code","5e03e058":"code","726c5f71":"code","62e4d08f":"code","c8668ff3":"code","f8223080":"code","dd54c0fd":"code","7c96c4fa":"markdown","d0c39966":"markdown","22288460":"markdown","dc55e750":"markdown","3cea96d4":"markdown","1431639a":"markdown","e99e9d63":"markdown","e7a04359":"markdown","bd02988a":"markdown","fbd41aa8":"markdown","d49c1828":"markdown","b9078b00":"markdown","5b58290f":"markdown","2172b42a":"markdown","7b4e5ebb":"markdown","46ba82ea":"markdown","44fd2ab4":"markdown","391795a8":"markdown","418706f8":"markdown","1538d4d1":"markdown","61908d8e":"markdown","f484ceaa":"markdown","6df3caa2":"markdown","a5fe6f17":"markdown","0701faaa":"markdown","c0027305":"markdown","a9698e07":"markdown","50d3483e":"markdown","65e0ed7f":"markdown","aa5f3801":"markdown","66587d4b":"markdown","f51076c7":"markdown","43012b6b":"markdown","2a7ffabe":"markdown","63e271a3":"markdown","2d5bff9c":"markdown","040898c6":"markdown","68413f0a":"markdown","e00ce0f7":"markdown","3d39c703":"markdown","ff8d21d7":"markdown","ba0c635f":"markdown","6955093c":"markdown","646e3710":"markdown","b82a4148":"markdown","153e10b2":"markdown","06fe48d8":"markdown","db425789":"markdown","e2c36905":"markdown","484bd863":"markdown","8f88203b":"markdown","48662960":"markdown","4f525792":"markdown"},"source":{"24b8e662":"!pip install ecg-plot\nimport physionet_challenge_utility_script as pc\nimport ecg_plot\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow_addons as tfa\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.utils import plot_model\nfrom keras.preprocessing.sequence import pad_sequences\n%load_ext autoreload\n%autoreload\n%reload_ext autoreload\n\ndef plot_ecg(path):\n    ecg_data = pc.load_challenge_data(path)\n    ecg_plot.plot(ecg_data[0]\/1000, sample_rate=500, title='')\n    ecg_plot.show()\nplot_ecg(\"\/kaggle\/input\/china-12lead-ecg-challenge-database\/Training_2\/Q0948.mat\")","34f2321c":"gender, age, labels, ecg_filenames = pc.import_key_data(\"\/kaggle\/input\/\")\necg_filenames = np.asarray(ecg_filenames)","7847d1b4":"import os\nsignal_lenght=[]\nfor subdir, dirs, files in sorted(os.walk(\"\/kaggle\/input\/\")):\n    for filename in files:\n        filepath = subdir + os.sep + filename\n        if filepath.endswith(\".mat\"):\n            data, header_data = pc.load_challenge_data(filepath)\n            splitted = header_data[0].split()\n            signal_lenght.append(splitted[3])\nsignal_lenght_df = pd.DataFrame(signal_lenght)\nsignal_count=signal_lenght_df[0].value_counts()\nplt.figure(figsize=(20,10))\n#plt.title(title,fontsize =36)\nsns.barplot(signal_count[:10,].index, signal_count[:10,].values)","5fd59baf":"signal_count","ee4c9c5f":"pc.get_signal_lengths(\"\/kaggle\/input\/\", \"Distribution of signal lengths of the ECGs\")","3046386e":"age, gender = pc.import_gender_and_age(age, gender)","87a16474":"SNOMED_scored=pd.read_csv(\"\/kaggle\/input\/physionet-snomed-mappings\/SNOMED_mappings_scored.csv\", sep=\";\")\nSNOMED_unscored=pd.read_csv(\"\/kaggle\/input\/physionet-snomed-mappings\/SNOMED_mappings_unscored.csv\", sep=\";\")\ndf_labels = pc.make_undefined_class(labels,SNOMED_unscored)","e8e52513":"y , snomed_classes = pc.onehot_encode(df_labels)","d42f05ad":"snomed_abbr = []\nfor j in range(len(snomed_classes)):\n    for i in range(len(SNOMED_scored.iloc[:,1])):\n        if (str(SNOMED_scored.iloc[:,1][i]) == snomed_classes[j]):\n            snomed_abbr.append(SNOMED_scored.iloc[:,2][i])\n            \nsnomed_abbr = np.asarray(snomed_abbr)","302b90f9":"pc.plot_classes(snomed_classes, SNOMED_scored,y)","ae637e95":"y_all_comb = pc.get_labels_for_all_combinations(y)\nprint(\"Total number of unique combinations of diagnosis: {}\".format(len(np.unique(y_all_comb))))","d9e8d746":"folds = pc.split_data(labels, y_all_comb)","611a321e":"pc.plot_all_folds(folds,y,snomed_classes)","92fd2416":"order_array = folds[0][0]","4158a827":"def shuffle_batch_generator_demo(batch_size, gen_x,gen_y, gen_z): \n    np.random.shuffle(order_array)\n    batch_features = np.zeros((batch_size,5000, 12))\n    batch_labels = np.zeros((batch_size,snomed_classes.shape[0])) #drop undef class\n    batch_demo_data = np.zeros((batch_size,2))\n    while True:\n        for i in range(batch_size):\n\n            batch_features[i] = next(gen_x)\n            batch_labels[i] = next(gen_y)\n            batch_demo_data[i] = next(gen_z)\n\n        X_combined = [batch_features, batch_demo_data]\n        yield X_combined, batch_labels\n        \ndef shuffle_batch_generator(batch_size, gen_x,gen_y): \n    np.random.shuffle(order_array)\n    batch_features = np.zeros((batch_size,5000, 12))\n    batch_labels = np.zeros((batch_size,snomed_classes.shape[0])) #drop undef class\n    while True:\n        for i in range(batch_size):\n\n            batch_features[i] = next(gen_x)\n            batch_labels[i] = next(gen_y)\n            \n        yield batch_features, batch_labels\n\ndef generate_y_shuffle(y_train):\n    while True:\n        for i in order_array:\n            y_shuffled = y_train[i]\n            yield y_shuffled\n\n\ndef generate_X_shuffle(X_train):\n    while True:\n        for i in order_array:\n                #if filepath.endswith(\".mat\"):\n                    data, header_data = pc.load_challenge_data(X_train[i])\n                    X_train_new = pad_sequences(data, maxlen=5000, truncating='post',padding=\"post\")\n                    X_train_new = X_train_new.reshape(5000,12)\n                    yield X_train_new\n\ndef generate_z_shuffle(age_train, gender_train):\n    while True:\n        for i in order_array:\n            gen_age = age_train[i]\n            gen_gender = gender_train[i]\n            z_train = [gen_age , gen_gender]\n            yield z_train","49e65f38":"new_weights=pc.calculating_class_weights(y)","55f17913":"keys = np.arange(0,27,1)\nweight_dictionary = dict(zip(keys, new_weights.T[1]))\nweight_dictionary","66dbe598":"reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_AUC', factor=0.1, patience=1, verbose=1, mode='max',\n    min_delta=0.0001, cooldown=0, min_lr=0\n)\n\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_AUC', mode='max', verbose=1, patience=2)","10ac077b":"from scipy import optimize\ndef thr_chall_metrics(thr, label, output_prob):\n    return -pc.compute_challenge_metric_for_opt(label, np.array(output_prob>thr))","6f8ae2f7":"model = pc.residual_network_1d()","0d4b2bb9":"model.load_weights(\"\/kaggle\/input\/physionet-challenge-models\/resnet_model.h5\")","18683d9a":"#batchsize = 30\n#model.fit(x=shuffle_batch_generator(batch_size=batchsize, gen_x=generate_X_shuffle(ecg_filenames), gen_y=generate_y_shuffle(y)), epochs=100, steps_per_epoch=(len(order_array)\/batchsize), validation_data=pc.generate_validation_data(ecg_filenames,y,folds[0][1]), validation_freq=1, class_weight=weight_dictionary, callbacks=[reduce_lr,early_stop])","cd4eef80":"y_pred = model.predict(x=pc.generate_validation_data(ecg_filenames,y,folds[0][1])[0])","da498bef":"init_thresholds = np.arange(0,1,0.05)","075d9296":"all_scores = pc.iterate_threshold(y_pred, ecg_filenames, y ,folds[0][1] )","2034c93c":"new_best_thr = optimize.fmin(thr_chall_metrics, args=(pc.generate_validation_data(ecg_filenames,y,folds[0][1])[1],y_pred), x0=init_thresholds[all_scores.argmax()]*np.ones(27))","80b65de5":"print(pc.compute_challenge_metric_for_opt(pc.generate_validation_data(ecg_filenames,y,folds[0][1])[1],(y_pred>new_best_thr)*1))","bd7b6026":"pc.plot_normalized_conf_matrix(y_pred, ecg_filenames, y, folds[0][1], new_best_thr, snomed_classes, snomed_abbr)\nplt.savefig(\"confusion_matrix_resnet.png\", dpi=100)","f8743ad5":"model = pc.encoder_model()","23fa4ec1":"model.load_weights(\"\/kaggle\/input\/physionet-challenge-models\/encoder_model.h5\")","5f39cd89":"#batchsize = 30\n\n#model.fit(x=shuffle_batch_generator(batch_size=batchsize, gen_x=generate_X_shuffle(ecg_filenames), gen_y=generate_y_shuffle(y)), epochs=50, steps_per_epoch=(len(order_array)\/batchsize), validation_data=pc.generate_validation_data(ecg_filenames,y,folds[0][1]), validation_freq=1, class_weight=weight_dictionary, callbacks=[reduce_lr,early_stop])","6cb1548d":"y_pred = model.predict(x=pc.generate_validation_data(ecg_filenames,y,folds[0][1])[0])","02304ba5":"init_thresholds = np.arange(0,1,0.05)","286e51fb":"all_scores = pc.iterate_threshold(y_pred, ecg_filenames, y ,folds[0][1])","982ffc12":"new_best_thr = optimize.fmin(thr_chall_metrics, args=(pc.generate_validation_data(ecg_filenames,y,folds[0][1])[1],y_pred), x0=init_thresholds[all_scores.argmax()]*np.ones(27))#","122b6aa4":"print(pc.compute_challenge_metric_for_opt(pc.generate_validation_data(ecg_filenames,y,folds[0][1])[1],(y_pred>new_best_thr)*1))","eb9d2348":"pc.plot_normalized_conf_matrix(y_pred, ecg_filenames, y, folds[0][1], new_best_thr, snomed_classes, snomed_abbr)\nplt.savefig(\"confusion_matrix_encoder.png\",dpi=100)","16b9520b":"model = pc.FCN()","6c0cc378":"model.load_weights(\"\/kaggle\/input\/physionet-challenge-models\/fcn_model.h5\")","84ab5a8a":"#batchsize = 30\n\n#model.fit(x=shuffle_batch_generator(batch_size=batchsize, gen_x=generate_X_shuffle(ecg_filenames), gen_y=generate_y_shuffle(y)), epochs=30, steps_per_epoch=(len(order_array)\/batchsize), validation_data=pc.generate_validation_data(ecg_filenames,y,folds[0][1]), validation_freq=1, class_weight=weight_dictionary, callbacks=[reduce_lr,early_stop])","3c625949":"y_pred = model.predict(x=pc.generate_validation_data(ecg_filenames,y,folds[0][1])[0])","f7bf7a39":"init_thresholds = np.arange(0,1,0.05)","7f04fe8b":"all_scores = pc.iterate_threshold(y_pred, ecg_filenames, y ,folds[0][1])","4f9c4828":"new_best_thr = optimize.fmin(thr_chall_metrics, args=(pc.generate_validation_data(ecg_filenames,y,folds[0][1])[1],y_pred), x0=init_thresholds[all_scores.argmax()]*np.ones(27))#","83161a1c":"print(pc.compute_challenge_metric_for_opt(pc.generate_validation_data(ecg_filenames,y,folds[0][1])[1],(y_pred>new_best_thr)*1))#","3debba40":"pc.plot_normalized_conf_matrix(y_pred, ecg_filenames, y, folds[0][1], new_best_thr, snomed_classes,snomed_abbr)\nplt.savefig(\"confusion_matrix_fcn.png\", dpi = 100)","87834aaf":"model = pc.residual_network_1d_demo()","7ca9b46c":"model.load_weights(\"\/kaggle\/input\/physionet-challenge-models\/resnet_gender_age_model.h5\")","67b6ddd9":"#batchsize = 30\n\n#history = model.fit(x=shuffle_batch_generator_demo(batch_size=batchsize, gen_x=generate_X_shuffle(ecg_filenames), gen_y=generate_y_shuffle(y), gen_z=generate_z_shuffle(age, gender)), epochs=50, steps_per_epoch=(len(order_array)\/batchsize), validation_data=pc.generate_validation_data_with_demo_data(ecg_filenames,y, gender, age, folds[0][1]), validation_freq=1, class_weight=weight_dictionary, callbacks=[reduce_lr,early_stop])","f4a784a2":"y_pred = model.predict(x=pc.generate_validation_data_with_demo_data(ecg_filenames,y, gender, age,folds[0][1])[0])","2ac60de1":"init_thresholds = np.arange(0,1,0.05)","c2cf9d8b":"all_scores = pc.iterate_threshold(y_pred, ecg_filenames, y ,folds[0][1] )","7a907b55":"new_best_thr = optimize.fmin(thr_chall_metrics, args=(pc.generate_validation_data_with_demo_data(ecg_filenames,y, gender, age,folds[0][1])[1],y_pred), x0=init_thresholds[all_scores.argmax()]*np.ones(27))","e0a11aaf":"print(pc.compute_challenge_metric_for_opt(pc.generate_validation_data_with_demo_data(ecg_filenames,y, gender, age,folds[0][1])[1],(y_pred>new_best_thr)*1))","1a3a1ae7":"pc.plot_normalized_conf_matrix(y_pred, ecg_filenames, y, folds[0][1], new_best_thr, snomed_classes,snomed_abbr)\nplt.savefig(\"confusion_matrix_resnet_age_gender.png\", dpi = 100)","7405d0ae":"model = pc.encoder_model_demo()","07a9a630":"model.load_weights(\"\/kaggle\/input\/physionet-challenge-models\/encoder_gender_age_model.h5\")","5244ffec":"#batchsize = 30\n\n#history = model.fit(x=shuffle_batch_generator_demo(batch_size=batchsize, gen_x=generate_X_shuffle(ecg_filenames), gen_y=generate_y_shuffle(y), gen_z=generate_z_shuffle(age, gender)), epochs=50, steps_per_epoch=(len(order_array)\/batchsize), validation_data=pc.generate_validation_data_with_demo_data(ecg_filenames,y, gender, age, folds[0][1]), validation_freq=1, class_weight=weight_dictionary, callbacks=[reduce_lr,early_stop])","7530bb3e":"y_pred = model.predict(x=pc.generate_validation_data_with_demo_data(ecg_filenames,y, gender, age,folds[0][1])[0])","2b854fed":"init_thresholds = np.arange(0,1,0.05)","d7bd8f41":"all_scores = pc.iterate_threshold(y_pred, ecg_filenames, y ,folds[0][1] )","630eb9ef":"new_best_thr = optimize.fmin(thr_chall_metrics, args=(pc.generate_validation_data_with_demo_data(ecg_filenames,y, gender, age,folds[0][1])[1],y_pred), x0=init_thresholds[all_scores.argmax()]*np.ones(27))","1458c7c6":"print(pc.compute_challenge_metric_for_opt(pc.generate_validation_data_with_demo_data(ecg_filenames,y, gender, age,folds[0][1])[1],(y_pred>new_best_thr)*1))","228fe43d":"pc.plot_normalized_conf_matrix(y_pred, ecg_filenames, y, folds[0][1], new_best_thr, snomed_classes, snomed_abbr)\nplt.savefig(\"confusion_matrix_encoder_age_gender.png\", dpi=100)","8d30e27c":"model = pc.FCN_demo()","21dbac8a":"model.load_weights(\"\/kaggle\/input\/physionet-challenge-models\/fcn_gender_age_model.h5\")","1b6bc670":"#batchsize = 30\n\n#history = model.fit(x=shuffle_batch_generator_demo(batch_size=batchsize, gen_x=generate_X_shuffle(ecg_filenames), gen_y=generate_y_shuffle(y), gen_z=generate_z_shuffle(age, gender)), epochs=50, steps_per_epoch=(len(order_array)\/batchsize), validation_data=pc.generate_validation_data_with_demo_data(ecg_filenames,y, gender, age, folds[0][1]), validation_freq=1, class_weight=weight_dictionary, callbacks=[reduce_lr,early_stop])","78f51a1f":"y_pred = model.predict(x=pc.generate_validation_data_with_demo_data(ecg_filenames,y, gender, age,folds[0][1])[0])","f8fb5f3b":"init_thresholds = np.arange(0,1,0.05)","4a790ffc":"all_scores = pc.iterate_threshold(y_pred, ecg_filenames, y ,folds[0][1] )","9d3308fa":"new_best_thr = optimize.fmin(thr_chall_metrics, args=(pc.generate_validation_data_with_demo_data(ecg_filenames,y, gender, age,folds[0][1])[1],y_pred), x0=init_thresholds[all_scores.argmax()]*np.ones(27))","d1175dd1":"print(pc.compute_challenge_metric_for_opt(pc.generate_validation_data_with_demo_data(ecg_filenames,y, gender, age,folds[0][1])[1],(y_pred>new_best_thr)*1))","213cf9f1":"pc.plot_normalized_conf_matrix(y_pred, ecg_filenames, y, folds[0][1], new_best_thr, snomed_classes, snomed_abbr)\nplt.savefig(\"confusion_matrix_fcn_gender_age.png\", dpi=100)","7ae62e2d":"model = pc.FCN_Encoder()","863c6756":"model.load_weights(\"\/kaggle\/input\/physionet-challenge-models\/fcn_and_encoder_model.h5\")","24830fa2":"#batchsize = 30\n\n#model.fit(x=shuffle_batch_generator(batch_size=batchsize, gen_x=generate_X_shuffle(ecg_filenames), gen_y=generate_y_shuffle(y)), epochs=5, steps_per_epoch=(len(order_array)\/batchsize), validation_data=pc.generate_validation_data(ecg_filenames,y,folds[0][1]), validation_freq=1, class_weight=weight_dictionary, callbacks=[reduce_lr,early_stop])","0f7316f3":"y_pred = model.predict(x=pc.generate_validation_data_with_demo_data(ecg_filenames,y, gender, age,folds[0][1])[0])","77164f1f":"init_thresholds = np.arange(0,1,0.05)","564b04af":"all_scores = pc.iterate_threshold(y_pred, ecg_filenames, y ,folds[0][1] )","e0b0aa99":"new_best_thr = optimize.fmin(thr_chall_metrics, args=(pc.generate_validation_data_with_demo_data(ecg_filenames,y, gender, age,folds[0][1])[1],y_pred), x0=init_thresholds[all_scores.argmax()]*np.ones(27))","05230ebf":"print(pc.compute_challenge_metric_for_opt(pc.generate_validation_data_with_demo_data(ecg_filenames,y, gender, age,folds[0][1])[1],(y_pred>new_best_thr)*1))","66ace7f6":"pc.plot_normalized_conf_matrix(y_pred, ecg_filenames, y, folds[0][1], new_best_thr, snomed_classes, snomed_abbr)\nplt.savefig(\"confusion_matrix_fcn_and_encoder.png\", dpi = 100)","894ad5b0":"binary_prediction = y_pred > new_best_thr\nbinary_prediction = binary_prediction * 1","21e20c98":"rb_pred = pc.rule_based_predictions(ecg_filenames,folds[0][1],binary_prediction)","217b611d":"pc.plot_normalized_conf_matrix_rule(y,folds[0][1], binary_prediction, snomed_classes)","c1feeb8d":"print(pc.compute_challenge_metric_for_opt(pc.generate_validation_data_with_demo_data(ecg_filenames,y, gender, age,folds[0][1])[1],rb_pred))","9a7defe0":"model = pc.FCN_Encoder_demo()","58b13b39":"model.load_weights(\"\/kaggle\/input\/physionet-challenge-models\/fcn_encoder_and_gender_age_model.h5\")","ceb91ff7":"#batchsize = 30\n\n#history = model.fit(x=shuffle_batch_generator_demo(batch_size=batchsize, gen_x=generate_X_shuffle(ecg_filenames), gen_y=generate_y_shuffle(y), gen_z=generate_z_shuffle(age, gender)), epochs=30, steps_per_epoch=(len(order_array)\/batchsize), validation_data=pc.generate_validation_data_with_demo_data(ecg_filenames,y, gender, age, folds[0][1]), validation_freq=1, class_weight=weight_dictionary, callbacks=[reduce_lr,early_stop])","4c4a2cea":"y_pred = model.predict(x=pc.generate_validation_data_with_demo_data(ecg_filenames,y, gender, age,folds[0][1])[0])","4f3305aa":"init_thresholds = np.arange(0,1,0.05)","da189969":"all_scores = pc.iterate_threshold(y_pred, ecg_filenames, y ,folds[0][1] )","0217fe4d":"new_best_thr = optimize.fmin(thr_chall_metrics, args=(pc.generate_validation_data_with_demo_data(ecg_filenames,y, gender, age,folds[0][1])[1],y_pred), x0=init_thresholds[all_scores.argmax()]*np.ones(27))","39ea0408":"print(pc.compute_challenge_metric_for_opt(pc.generate_validation_data_with_demo_data(ecg_filenames,y, gender, age,folds[0][1])[1],(y_pred>new_best_thr)*1))","bd22a2e1":"pc.plot_normalized_conf_matrix(y_pred, ecg_filenames, y, folds[0][1], new_best_thr, snomed_classes, snomed_abbr)\nplt.savefig(\"confusion_matrix_fcn_and_encoder_an_demo.png\", dpi = 100)","4ff9c363":"binary_prediction = y_pred > new_best_thr\nbinary_prediction = binary_prediction * 1","0765a21c":"rb_pred = pc.rule_based_predictions(ecg_filenames,folds[0][1],binary_prediction)","b6fd6b8a":"pc.plot_normalized_conf_matrix_rule(y,folds[0][1], binary_prediction, snomed_classes, snomed_abbr)\nplt.savefig(\"confusion_matrix_fcn_and_encoder_an_demo_rulebased.png\", dpi = 100)","35e3cbd0":"print(pc.compute_challenge_metric_for_opt(pc.generate_validation_data_with_demo_data(ecg_filenames,y, gender, age,folds[0][1])[1],rb_pred))","da46bee9":"def sepres():\n    n_feature_maps = 64\n    input_shape = (5000,12)\n    input_layer = keras.layers.Input(input_shape)\n\n        # BLOCK 1\n\n    conv_x = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=8, padding='same', data_format='channels_last')(input_layer)\n    conv_x = keras.layers.BatchNormalization()(conv_x)\n    conv_x = keras.layers.Activation('relu')(conv_x)\n\n    conv_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=5, padding='same', data_format='channels_last')(conv_x)\n    conv_y = keras.layers.BatchNormalization()(conv_y)\n    conv_y = keras.layers.Activation('relu')(conv_y)\n\n    conv_z = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same', data_format='channels_last')(conv_y)\n    conv_z = keras.layers.BatchNormalization()(conv_z)\n\n        # expand channels for the sum\n    shortcut_y = keras.layers.SeparableConv1D(filters=n_feature_maps, kernel_size=1, padding='same',data_format='channels_last')(input_layer)\n    shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n\n    output_block_1 = keras.layers.add([shortcut_y, conv_z])\n    output_block_1 = keras.layers.Activation('relu')(output_block_1)\n\n        # BLOCK 2\n\n    conv_x = keras.layers.SeparableConv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same', data_format='channels_last')(output_block_1)\n    conv_x = keras.layers.BatchNormalization()(conv_x)\n    conv_x = keras.layers.Activation('relu')(conv_x)\n\n    conv_y = keras.layers.SeparableConv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same',data_format='channels_last')(conv_x)\n    conv_y = keras.layers.BatchNormalization()(conv_y)\n    conv_y = keras.layers.Activation('relu')(conv_y)\n\n    conv_z = keras.layers.SeparableConv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same',data_format='channels_last')(conv_y)\n    conv_z = keras.layers.BatchNormalization()(conv_z)\n\n        # expand channels for the sum\n    shortcut_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same', data_format='channels_last')(output_block_1)\n    shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n\n    output_block_2 = keras.layers.add([shortcut_y, conv_z])\n    output_block_2 = keras.layers.Activation('relu')(output_block_2)\n\n        # BLOCK 3\n\n    conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same', data_format='channels_last')(output_block_2)\n    conv_x = keras.layers.BatchNormalization()(conv_x)\n    conv_x = keras.layers.Activation('relu')(conv_x)\n\n    conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same', data_format='channels_last')(conv_x)\n    conv_y = keras.layers.BatchNormalization()(conv_y)\n    conv_y = keras.layers.Activation('relu')(conv_y)\n\n    conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same', data_format='channels_last')(conv_y)\n    conv_z = keras.layers.BatchNormalization()(conv_z)\n\n        # no need to expand channels because they are equal\n    shortcut_y = keras.layers.SeparableConv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same',data_format='channels_last')(output_block_2)\n    shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n\n    output_block_3 = keras.layers.add([shortcut_y, conv_z])\n    output_block_3 = keras.layers.Activation('relu')(output_block_3)\n\n\n        # Block 4\n\n    conv_x = keras.layers.SeparableConv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same',data_format='channels_last', depth_multiplier=12)(output_block_3)\n    conv_x = keras.layers.BatchNormalization()(conv_x)\n    conv_x = keras.layers.Activation('relu')(conv_x)\n\n    conv_y = keras.layers.SeparableConv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same',data_format='channels_last')(conv_x)\n    conv_y = keras.layers.BatchNormalization()(conv_y)\n    conv_y = keras.layers.Activation('relu')(conv_y)\n\n    conv_z = keras.layers.SeparableConv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same',data_format='channels_last')(conv_y)\n    conv_z = keras.layers.BatchNormalization()(conv_z)\n\n          # expand channels for the sum\n    shortcut_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same',data_format='channels_last')(output_block_1)\n    shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n\n    output_block_4 = keras.layers.add([shortcut_y, conv_z])\n    output_block_4 = keras.layers.Activation('relu')(output_block_4)\n\n          # BLOCK 5\n\n    conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same', data_format='channels_last')(output_block_4)\n    conv_x = keras.layers.BatchNormalization()(conv_x)\n    conv_x = keras.layers.Activation('relu')(conv_x)\n\n    conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same', data_format='channels_last')(conv_x)\n    conv_y = keras.layers.BatchNormalization()(conv_y)\n    conv_y = keras.layers.Activation('relu')(conv_y)\n\n    conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same', data_format='channels_last')(conv_y)\n    conv_z = keras.layers.BatchNormalization()(conv_z)\n\n        # no need to expand channels because they are equal\n    shortcut_y = keras.layers.SeparableConv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same',data_format='channels_last')(output_block_2)\n    shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n\n    output_block_5 = keras.layers.add([shortcut_y, conv_z])\n    output_block_5 = keras.layers.Activation('relu')(output_block_5)\n\n        # FINAL\n\n    gap_layer = keras.layers.GlobalAveragePooling1D()(output_block_5)\n\n    output_layer = keras.layers.Dense(27, activation='softmax')(gap_layer)\n\n    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n\n    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=[tf.keras.metrics.BinaryAccuracy(\n    name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n                    tf.keras.metrics.AUC(\n        num_thresholds=200,\n        curve=\"ROC\",\n        summation_method=\"interpolation\",\n        name=\"AUC\",\n        dtype=None,\n        thresholds=None,\n        multi_label=True,\n        label_weights=None,\n    )])\n\n    #@title Plot model for better visualization\n    #plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n    return model","b3cdbf65":"model = sepres()","88607706":"#batchsize = 30\n\n#model.fit(x=shuffle_batch_generator(batch_size=batchsize, gen_x=generate_X_shuffle(ecg_filenames), gen_y=generate_y_shuffle(y)), epochs=20, steps_per_epoch=(len(order_array)\/batchsize), validation_data=pc.generate_validation_data(ecg_filenames,y,folds[0][1]), validation_freq=1, class_weight=weight_dictionary, callbacks=[reduce_lr,early_stop])","7776dcf1":"model.load_weights(\"\/kaggle\/input\/physionet-challenge-models\/sep_resnet_model.h5\")","fca5fe9d":"y_pred = model.predict(x=pc.generate_validation_data(ecg_filenames,y,folds[0][1])[0])","4ae1fc49":"init_thresholds = np.arange(0,1,0.05)","d353e1c6":"all_scores = pc.iterate_threshold(y_pred, ecg_filenames, y ,folds[0][1] )","736406ac":"new_best_thr = optimize.fmin(thr_chall_metrics, args=(pc.generate_validation_data_with_demo_data(ecg_filenames,y, gender, age,folds[0][1])[1],y_pred), x0=init_thresholds[all_scores.argmax()]*np.ones(27))","594b0b69":"print(pc.compute_challenge_metric_for_opt(pc.generate_validation_data_with_demo_data(ecg_filenames,y, gender, age,folds[0][1])[1],(y_pred>new_best_thr)*1))","7ceea625":"from scipy.io import loadmat\nimport os","03378125":"def load_challenge_data(filename):\n    x = loadmat(filename)\n    data = np.asarray(x['val'], dtype=np.float64)\n    new_file = filename.replace('.mat','.hea')\n    input_header_file = os.path.join(new_file)\n    with open(input_header_file,'r') as f:\n        header_data=f.readlines()\n    return data, header_data","650d5026":"def generate_validation_data(ecg_filenames, y,test_order_array):\n    y_train_gridsearch=y[test_order_array]\n    ecg_filenames_train_gridsearch=ecg_filenames[test_order_array]\n\n    ecg_train_timeseries=[]\n    for names in ecg_filenames_train_gridsearch:\n        data, header_data = load_challenge_data(names)\n        data = pad_sequences(data, maxlen=5000, truncating='post',padding=\"post\")\n        ecg_train_timeseries.append(data)\n    X_train_gridsearch = np.asarray(ecg_train_timeseries)\n\n    X_train_gridsearch = X_train_gridsearch.reshape(ecg_filenames_train_gridsearch.shape[0],5000,12)\n\n    return X_train_gridsearch, y_train_gridsearch","6a2769a1":"def compute_modified_confusion_matrix(labels, outputs):\n    # Compute a binary multi-class, multi-label confusion matrix, where the rows\n    # are the labels and the columns are the outputs.\n    num_recordings, num_classes = np.shape(labels)\n    A = np.zeros((num_classes, num_classes))\n\n    # Iterate over all of the recordings.\n    for i in range(num_recordings):\n        # Calculate the number of positive labels and\/or outputs.\n        normalization = float(max(np.sum(np.any((labels[i, :], outputs[i, :]), axis=0)), 1))\n        # Iterate over all of the classes.\n        for j in range(num_classes):\n            # Assign full and\/or partial credit for each positive class.\n            if labels[i, j]:\n                for k in range(num_classes):\n                    if outputs[i, k]:\n                        A[j, k] += 1.0\/normalization\n\n    return A","d92618dd":"def plot_normalized_conf_matrix_dev(y_pred, ecg_filenames, y, val_fold, threshold, snomedclasses):\n    df_cm = pd.DataFrame(compute_modified_confusion_matrix(generate_validation_data(ecg_filenames,y,val_fold)[1], (y_pred>threshold)*1), columns=snomedclasses, index = snomedclasses)\n    df_cm = df_cm.fillna(0)\n    df_cm.index.name = 'Actual'\n    df_cm.columns.name = 'Predicted'\n    df_norm_col=(df_cm-df_cm.mean())\/df_cm.std()\n    plt.figure(figsize = (36,14))\n    sns.set(font_scale=1.4)\n    sns.heatmap(df_norm_col, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16},fmt=\".2f\",cbar=False)# font size","ac983f48":"plot_normalized_conf_matrix_dev(y_pred, ecg_filenames, y, folds[0][1], new_best_thr, snomed_classes)","e58cc66a":"pc.plot_normalized_conf_matrix(y_pred, ecg_filenames, y, folds[0][1], new_best_thr, snomed_classes, snomed_abbr)\nplt.savefig(\"confusion_matrix_separable_resnet.png\", transparent=True,dpi = 500, bbox_inches=\"tight\" )","b0eb7272":"def scheduler(epoch, lr):\n    if epoch < 6:\n        lr = 0.001\n        return lr\n    else:\n        return lr * 0.1\n\n\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)","17e6cb65":"'''\nscore_array=[]\nfor i in range(len(folds)):\n    order_array = folds[i][0]\n    model = pc.FCN()\n    batchsize = 30\n    model.fit(x=shuffle_batch_generator(batch_size=batchsize, gen_x=generate_X_shuffle(ecg_filenames), gen_y=generate_y_shuffle(y)), epochs=10, steps_per_epoch=(len(order_array)\/batchsize), validation_data=pc.generate_validation_data(ecg_filenames,y,folds[i][1]), validation_freq=1, class_weight=weight_dictionary, callbacks=[lr_schedule])\n    y_pred = model.predict(x=pc.generate_validation_data(ecg_filenames,y,folds[0][1])[0])\n    init_thresholds = np.arange(0,1,0.05)\n    all_scores = pc.iterate_threshold(y_pred, ecg_filenames, y ,folds[0][1])\n    new_best_thr = optimize.fmin(thr_chall_metrics, args=(pc.generate_validation_data(ecg_filenames,y,folds[0][1])[1],y_pred), x0=init_thresholds[all_scores.argmax()]*np.ones(27))\n    print(pc.compute_challenge_metric_for_opt(pc.generate_validation_data(ecg_filenames,y,folds[0][1])[1],(y_pred>new_best_thr)*1))\n    score_array.append(pc.compute_challenge_metric_for_opt(pc.generate_validation_data(ecg_filenames,y,folds[0][1])[1],(y_pred>new_best_thr)*1))\nscore_array = np.asarray(score_array)\n'''","bc78d5ac":"#np.savetxt(\"10fold_score_FCN.txt\",score_array, fmt=\"%f\")","2cb09707":"'''\nscore_array=[]\nfor i in range(len(folds)):\n    order_array = folds[i][0]\n    model = pc.encoder_model()\n    batchsize = 30\n    model.fit(x=shuffle_batch_generator(batch_size=batchsize, gen_x=generate_X_shuffle(ecg_filenames), gen_y=generate_y_shuffle(y)), epochs=10, steps_per_epoch=(len(order_array)\/batchsize), validation_data=pc.generate_validation_data(ecg_filenames,y,folds[i][1]), validation_freq=1, class_weight=weight_dictionary, callbacks=[lr_schedule])\n    y_pred = model.predict(x=pc.generate_validation_data(ecg_filenames,y,folds[0][1])[0])\n    init_thresholds = np.arange(0,1,0.05)\n    all_scores = pc.iterate_threshold(y_pred, ecg_filenames, y ,folds[0][1])\n    new_best_thr = optimize.fmin(thr_chall_metrics, args=(pc.generate_validation_data(ecg_filenames,y,folds[0][1])[1],y_pred), x0=init_thresholds[all_scores.argmax()]*np.ones(27))\n    print(pc.compute_challenge_metric_for_opt(pc.generate_validation_data(ecg_filenames,y,folds[0][1])[1],(y_pred>new_best_thr)*1))\n    score_array.append(pc.compute_challenge_metric_for_opt(pc.generate_validation_data(ecg_filenames,y,folds[0][1])[1],(y_pred>new_best_thr)*1))\nscore_array = np.asarray(score_array)\n'''","faac4fb8":"#np.savetxt(\"10fold_score_encoder.txt\",score_array, fmt=\"%f\")","959a0a11":"fcn10fold = pd.read_csv('\/kaggle\/input\/10fold-scores\/10fold_score_FCN.txt', header = None)\nfcn10fold.set_axis(['FCN'], axis=1, inplace=True)","8177d83f":"encoder10fold = pd.read_csv('\/kaggle\/input\/10fold-scores\/10fold_score_encoder.txt', header = None)\nencoder10fold.set_axis(['Encoder'], axis=1, inplace=True)","5e03e058":"all_10folds = pd.concat([fcn10fold, encoder10fold], axis=1)\n","726c5f71":"all_10folds","62e4d08f":"plt.figure(figsize=(20,8))\nplt.style.use('ggplot')\nboxplot = all_10folds.boxplot(fontsize=20)","c8668ff3":"from zipfile import ZipFile\nimport os","f8223080":"zipObj = ZipFile('ConfusionMatrixes.zip', 'w')","dd54c0fd":"for filename in os.listdir(\"\/kaggle\/working\"):\n    if filename.endswith(\".png\"):\n        zipObj.write(filename)\nzipObj.close()","7c96c4fa":"##### or train it your self by uncomment the code below","d0c39966":"#### Residual Network with Separable Convolution","22288460":"##### or train it your self by uncomment the code below","dc55e750":"#### Imbalanced data\nTo compensate for the imbalaced data we calculate a weight for each label. The weight decides how much the Neural Network will learn from the different data labels","3cea96d4":"##### or train it your self by uncomment the code below","1431639a":"Make conf.matrix","e99e9d63":"load a pre-trained model","e7a04359":"Conf matrix","bd02988a":"#### To be able to feed the labels to a Neural Network we need to OneHot encode the labels","fbd41aa8":"# Citation\n## Please cite [this article](https:\/\/ieeexplore.ieee.org\/document\/9344421) if you reuse some of this content:\n### B. -J. Singstad and C. Tronstad, \"Convolutional Neural Network and Rule-Based Algorithms for Classifying 12-lead ECGs,\" 2020 Computing in Cardiology, 2020, pp. 1-4, doi: 10.22489\/CinC.2020.227.\n\n### or\n\n### Bibtex:\n`\n@INPROCEEDINGS{9344421,\n  author={Singstad, Bj\u00f8rn-Jostein and Tronstad, Christian},\n  booktitle={2020 Computing in Cardiology}, \n  title={Convolutional Neural Network and Rule-Based Algorithms for Classifying 12-lead ECGs}, \n  year={2020},\n  volume={},\n  number={},\n  pages={1-4},\n  doi={10.22489\/CinC.2020.227}}\n `","d49c1828":"#### Encoder + Gender and Age","b9078b00":"#### 10-fold crossvalidation","5b58290f":"#### Residual Neural Network","2172b42a":"#### All diagnoses are encoded with SNOMED-CT codes. We need a CSV-file to decode them:","7b4e5ebb":"##### or train it your self by uncomment the code below","46ba82ea":"load a pre-trained model","44fd2ab4":"#### Fully Convolutional Network","391795a8":"##### or train it your self by uncomment the code below","418706f8":"# <center>Physionet\/Cinc Challenge 2020<\/center>\n## <center>This study is a part of the PhysioNet\/Computation in Cardiology (CinC) Challenge 2020. Our objective was to classify 27 cardiac abnormalities based on a provided dataset of 43101 12-lead ECG recordings. We developed a hybrid model combining a rule-based algorithm with different Deep Learning architectures<\/center>\n\n### Introduction\nThe electrocardiogram (ECG) reflects the electrical activity of the heart, and the interpretation of this recording can reveal numerous pathologies of the heart. An ECG is recorded using an electrocardiograph, where modern clinical devices usually contain automatic interpretation software that interprets the ECGs directly after recording. Although automatic ECG interpretation started in the 1950s, there are still some limitations. Because of the errors they make, doctors have to read over the ECGs . This is time consuming for the doctors and requires high degree of expertise. There is clearly a need for better ECG interpretation algorithms.\n\nThe recent years has shown a rapid improvement in the field of machine learning. A sub-field of machine learning is called Deep Learning, where more complex architectures of neural networks are better able to scale with the amount of data in terms of performance. This type of machine learning has shown promising performance in many fields including medicine, and in this study, we have explored the usefulness of deep learning in classifying 12-lead ECGs. \n\nAs a starting point for our model architecture we chose to use the two best performing Convolutional Neural Networks (CNN) used on ECG data in [Fawaz HI et al 2019](https:\/\/link.springer.com\/article\/10.1007\/s10618-019-00619-1?shared-article-renderer). They reported that Fully Convolutional Networks (FCN) outperformed eight other CNN architectures compared. We also wanted to test the second-best architecture which was an Encoder Network. We also assessed the integration of a rule-based algorithm within these models in order to test the performance of a CNN and rule-based hybrid classifier. \n\nThis study is a part of the PhysioNet\/Computing in Cardiology Challenge 2020, where the aim was to develop an automated interpretation algorithm for identification of clinical diagnoses from 12-lead ECG recordings.","1538d4d1":"Make conf.matrix","61908d8e":"### Further work:","f484ceaa":"load a pre-trained model","6df3caa2":"##### or train it your self by uncomment the code below","a5fe6f17":"load a pre-trained model","0701faaa":"#### FCN and Encoder + Rule-based model","c0027305":"#### We will split the data using a 10-fold split with Shuffle=True and random_seed = 42. \nThe distribution of Training and Val data in each fold is now:\n(in this study we only use the first fold for hold out validation)","a9698e07":"##### or train it your self by uncomment the code below","50d3483e":"#### Make Batch generators\nTo feed the Neural Network with a dataset that is too large for our RAM set we need a batch generator to get data into the RAM in batches.\nWe start by making a \"order array\" so we can shuffle the order of the data during the training process","65e0ed7f":"### Results","aa5f3801":"#### FCN and Encoder + Gender and Age + Rule-based","66587d4b":"#### FCN and Encoder","f51076c7":"#### The distribution of diagnoses accross the dataset\nIn the figure under we can see the same SNOMED CT codes decoded into human readable diagnoses on the X-axis. On the Y-axis we have the number of the given diagnoses in the dataset","43012b6b":"#### FCN + Gender and Age","2a7ffabe":"#### Since this is a multiclass multi-label classification there will be a lot of different combinations of the 27 diagnoses in this study","63e271a3":"Conf.matrix","2d5bff9c":"##### or train it your self by uncomment the code below","040898c6":"### Methods\n\n\n#### Data\nThe training data in this study contains 43.101 Electrocariographic recordings from 4 different sources. \n1. Southeast University, China, including the data from the China Physiological Signal Challenge 2018 (2 datasets from this source)\n2. St. Petersburg Institute of Cardiological Technics, St. Petersburg, Russia.\n3. The Physikalisch Technische Bundesanstalt, Brunswick, Germany. (2 datasets from this source)\n4. Georgia 12-Lead ECG Challenge Database, Emory University, Atlanta, Georgia, USA.\n\nThe data is given in the form of native Python waveform-database-format [WFDB](https:\/\/wfdb.readthedocs.io\/en\/latest\/). The dataset contains two file types:\n\n\n1.   Header files (.hea)\n2.   Signal files (.mat)\n\nWe have 43.101 Signal file with a corresponding header file. Each file are named with a patient number starting with ***A0001*** and goes all the way up to ***A6877***\n","68413f0a":"#### FCN and Encoder + Gender and Age","e00ce0f7":"#### ResNet + Gender and Age","3d39c703":"#### To find the optimal threshold we will use Downhill simplex method","ff8d21d7":"load a pre-trained model","ba0c635f":"Conf.matrix","6955093c":"load a pre-trained model","646e3710":"load a pre-trained model","b82a4148":"Make conf.matrix","153e10b2":"#### Encoder Network","06fe48d8":"#### From the figure under we can se that the signals varies, but most of the signals are around 5000 samples long","db425789":"#### From the header file we have access to gender and age from each patient","e2c36905":"Conf.matrix","484bd863":"Conf.matrix","8f88203b":"load a pre-trained model","48662960":"Conf.matrix","4f525792":"#### Learning rate reduction\nTo controll the learning rate we use learning rate reduction and early stopping to prevent overfitting"}}