{"cell_type":{"ffe78a2a":"code","ef42dfb9":"code","19422651":"code","9f682493":"code","75201c5d":"code","61836693":"code","e73aa64d":"code","32a70696":"code","1bb19ffd":"code","abaa1225":"code","00610125":"code","33ca22eb":"code","5dce78ca":"code","924e2c1b":"code","28529712":"code","d46f5240":"code","e75f04ed":"code","75ac71a3":"code","3a3c9475":"code","e92d2a48":"code","9d76dbe4":"code","64714f2a":"code","256a851a":"code","20c3d9cd":"code","0b989044":"code","6a769326":"code","c856d524":"code","faf2b0ee":"code","6be789d3":"code","e6b353a9":"code","835943a5":"code","196531f0":"code","46dc8afc":"code","9c594fa5":"code","bc3f3fdc":"code","d7d2d081":"code","641a9e1a":"code","38bbbec5":"code","fe2bbebb":"code","5cfc612e":"code","1ea395a6":"code","52fc919d":"code","168bbda3":"code","8145ccd6":"markdown","96bdcda6":"markdown","1bbe879e":"markdown","7fc9f00e":"markdown","37cece54":"markdown","6d4bff9c":"markdown","06f51a61":"markdown","9e670a47":"markdown","713768b4":"markdown","04fd7e2d":"markdown","d09a8eac":"markdown","432130b8":"markdown","95c1d735":"markdown","80871c7d":"markdown","501e34fb":"markdown","c344baed":"markdown","43a543b5":"markdown","c2b575a1":"markdown","eaaaa198":"markdown","c2535a89":"markdown","b4d22ade":"markdown","9dc254be":"markdown","a21d28f8":"markdown","f976b11b":"markdown","64672ff0":"markdown","ebc01d50":"markdown","77bae3f8":"markdown","f3793988":"markdown","a803c263":"markdown","96f9d12f":"markdown","60cfbc2d":"markdown","116eacc2":"markdown","945d4596":"markdown","90b70598":"markdown","5054fff2":"markdown","4f015b75":"markdown","cb081b91":"markdown","13099209":"markdown","f7c869b2":"markdown","ee5f543d":"markdown","fda31f07":"markdown","89339051":"markdown"},"source":{"ffe78a2a":"%%capture\n!pip install catalyst","ef42dfb9":"import numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.utils.data as data_utils \nfrom torchvision import datasets, transforms\nimport plotly.express as px\nfrom sklearn.model_selection import train_test_split\nfrom IPython.display import clear_output\nfrom skimage.transform import resize\nfrom skimage.io import imread\nimport scipy.stats as stats\nfrom tqdm.autonotebook import tqdm, trange\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.neighbors import NearestNeighbors\nimport warnings\nfrom catalyst.utils import set_global_seed\nfrom copy import deepcopy\nfrom sklearn.preprocessing import QuantileTransformer\nimport plotly.express as px\nsns.set_style('darkgrid')\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\nset_global_seed(42)","19422651":"device = 'cuda:0' if torch.cuda.is_available() else 'cpu'","9f682493":"current_palette = sns.color_palette('mako', 40)\nsns.palplot(current_palette)","75201c5d":"%%capture\nbatch_size = 128\ntrain_dataset = (datasets\n                 .MNIST(root='.\/mnist_data\/', train=True, transform=transforms\n                        .ToTensor(), download=True)\n                 )\ntest_dataset = (datasets\n                .MNIST(root='.\/mnist_data\/', train=False, transform=transforms\n                       .ToTensor(), download=False)\n                )\n\ntrain_loader = (torch.utils.data.DataLoader(dataset=train_dataset, \n                                            batch_size=batch_size, \n                                            shuffle=True)\n)\ntest_loader = (torch.utils.data.DataLoader(dataset=test_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=False)\n)","61836693":"dim_code = 4","e73aa64d":"class VAE(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # encoder\n\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size = 4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size = 4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU()\n\n        )\n\n        self.flatten_mu = nn.Linear(128 * 7 * 7, out_features=dim_code)\n        self.flatten_logsigma = nn.Linear(128 * 7 * 7, out_features=dim_code)\n\n        # decoder\n\n        self.decode_linear = nn.Linear(4, 128 * 7 * 7)\n        self.decode_2 = nn.ConvTranspose2d(in_channels=128, out_channels=64, \n                                        kernel_size=4, stride=2, padding=1)\n        self.decode_1 = nn.ConvTranspose2d(in_channels=64, out_channels=1, \n                                        kernel_size=4, stride=2, padding=1)\n\n\n\n\n    def encode(self, x):\n        x = self.encoder(x)\n        x = x.view(x.size(0), -1)\n        mu, logsigma = self.flatten_mu(x), self.flatten_logsigma(x)\n        z = self.gaussian_sampler(mu, logsigma)\n        return z\n    \n    def gaussian_sampler(self, mu, logsigma):\n        if self.training:\n            std = torch.exp(logsigma \/ 2)\n            eps = torch.empty_like(std).normal_()\n            return eps.mul(std).add_(mu)\n        else:\n            return mu\n\n\n    def decode(self, x):\n        x = self.decode_linear(x)\n        x = x.view(x.size(0), 128, 7, 7)\n        x = F.relu(self.decode_2(x))\n        reconstruction = F.sigmoid(self.decode_1(x))\n        return reconstruction\n    \n    def forward(self, x):\n        x = self.encoder(x)\n        x = x.view(x.size(0), -1)\n        mu, logsigma = self.flatten_mu(x), self.flatten_logsigma(x)\n        z = self.gaussian_sampler(mu, logsigma)\n        x = self.decode_linear(z)\n        x = x.view(x.size(0), 128, 7, 7)\n        x = F.relu(self.decode_2(x))\n        reconstruction = F.sigmoid(self.decode_1(x))\n        return mu, logsigma, reconstruction","32a70696":"def KL_divergence(mu, logsigma):\n    loss = -0.5 * torch.sum(1 + logsigma - mu.pow(2) - logsigma.exp())\n    return loss\n\ndef log_likelihood(x, reconstruction):\n    loss = nn.BCELoss(reduction='sum')\n    return loss(reconstruction, x)\n\ndef loss_vae(x, mu, logsigma, reconstruction):\n    return KL_divergence(mu, logsigma) + log_likelihood(x, reconstruction)","1bb19ffd":"def train_epoch(model, criterion, optimizer, data_loader):\n    train_losses_per_epoch = []\n    model.train()\n    for x_batch, _ in data_loader:\n\n        x_batch = x_batch.to(device)    \n        mu, logsigma, reconstruction = model(x_batch)\n        loss = criterion(x_batch.to(device).float(), mu, logsigma, reconstruction)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_losses_per_epoch.append(loss.item())\n\n    return np.mean(train_losses_per_epoch), mu, logsigma, reconstruction","abaa1225":"def eval_epoch(model, criterion, optimizer, data_loader):\n    val_losses_per_epoch = []\n    model.eval()\n    with torch.no_grad():\n        for x_val, _ in data_loader:\n            x_val = x_val.to(device)\n            mu, logsigma, reconstruction = model(x_val)\n            loss = criterion(x_val.to(device).float(), mu, logsigma, reconstruction)\n            val_losses_per_epoch.append(loss.item())\n    return np.mean(val_losses_per_epoch), mu, logsigma, reconstruction","00610125":"def plot_output(model, epoch, epochs, train_loss, val_loss, size = 5):\n    clear_output(wait=True)\n    plt.figure(figsize=(18, 6))\n    for k in range(size):\n        ax = plt.subplot(2, size, k + 1)\n        img = test_dataset[k][0].unsqueeze(0).to(device)\n\n        model.eval()\n        with torch.no_grad():\n             mu, logsigma, reconstruction  = model(img)\n\n        plt.imshow(img.cpu().squeeze().numpy(), cmap='gray')\n        plt.axis('off')\n        if k == size\/\/2:\n            ax.set_title('Real')\n        ax = plt.subplot(2, size, k + 1 + size)\n        plt.imshow(reconstruction.cpu().squeeze().numpy(), cmap='gray')\n        plt.axis('off')\n\n        if k == size\/\/2:\n            ax.set_title('Output')\n    plt.suptitle('%d \/ %d - loss: %f val_loss: %f' % (epoch+1, epochs, train_loss, val_loss))\n    plt.show()","33ca22eb":"criterion = loss_vae\nautoencoder_vae = VAE().to(device)\noptimizer = torch.optim.Adam(autoencoder_vae.parameters(), lr=1e-3)","5dce78ca":"epochs = 30\nloss = {'train_loss':[],'val_loss':[]}\nwith tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n    for epoch in range(epochs):\n        print('* Epoch %d\/%d' % (epoch+1, epochs))\n        train_loss, mu, logsigma, reconstruction = (train_epoch(autoencoder_vae, \n                                                             criterion, \n                                                             optimizer, \n                                                             train_loader\n                                                             )\n        )\n\n        val_loss, mu, logsigma, reconstruction = (eval_epoch(autoencoder_vae, \n                                                          criterion, \n                                                          optimizer, \n                                                          test_loader)\n        )\n        pbar_outer.update(1)\n\n        loss['train_loss'].append(train_loss)\n        loss['val_loss'].append(val_loss)\n        plot_output(autoencoder_vae, epoch, epochs, train_loss, val_loss, size = 5)","924e2c1b":"plt.figure(figsize=(15, 6))\nplt.semilogy(loss['train_loss'], label='Train')\nplt.semilogy(loss['val_loss'], label='Valid')\nplt.xlabel('Epoch')\nplt.ylabel('Average Loss')\nplt.legend()\nplt.title('Loss_vae')\nplt.show()","28529712":"inputs = np.array([np.random.normal(0, 1, 4) for i in range(12)])","d46f5240":"with torch.no_grad():\n    autoencoder_vae.eval()\n    output = autoencoder_vae.decode(torch.FloatTensor(inputs).to(device))","e75f04ed":"plt.figure(figsize=(18, 6))\nfor k in range(12):\n    plt.subplot(2, 6, k+1)\n    res = output[k].cpu().squeeze().detach().numpy()\n    plt.imshow(res, cmap='gray')\n    plt.title(f'Sampling \u2116 {k + 1}')\n    plt.axis('off')","75ac71a3":"latent_space = []\nfor digits in tqdm(test_dataset):\n    img = digits[0].unsqueeze(0).to(device)\n    label = digits[1]\n\n    autoencoder_vae.eval()\n    with torch.no_grad():\n        latent = autoencoder_vae.encode(img)\n\n    latent = latent.flatten().cpu().numpy()\n    sample = {f\"Encoded_{i}\": encoded for i, encoded in enumerate(latent)}\n    sample['label'] = label\n    latent_space.append(sample)","3a3c9475":"latent_space = pd.DataFrame(latent_space)\nlatent_space['label'] = latent_space['label'].astype(str)","e92d2a48":"latent_space.head()","9d76dbe4":"fig = px.scatter(latent_space, x='Encoded_0', y='Encoded_1', \n           color='label', opacity=0.42,\n           \n                 labels={\n                     'label': \"Digit \"\n                 },\n                title=\"Latent space without t-SNE\")\\\n                .for_each_trace(lambda t: t.update(name=t.name.replace(\"=\",\": \"))\n                )\n           \n           \n\nfig.update_traces(marker=dict(size=10,\n                              line=dict(width=2,\n                                        color='DarkSlateGrey')),\n                selector=dict(mode='markers'))\nfig.update_yaxes(visible=False, showticklabels=False)\nfig.update_xaxes(visible=False, showticklabels=False)\nfig.show();","64714f2a":"tsne = TSNE(n_components=2)\ndigits_embedded = tsne.fit_transform(latent_space.drop(['label'],axis=1))","256a851a":"digits_embedded[:5]","20c3d9cd":"fig = px.scatter(digits_embedded, x=0, y=1, \n           color=latent_space['label'], opacity=0.7,\n           \n                 labels={\n                     'color': \"Digit \"\n                 },\n                title=\"Latent space with t-SNE\")\\\n                .for_each_trace(lambda t: t.update(name=t.name.replace(\"=\",\": \"))\n                )\n           \n           \n\nfig.update_traces(marker=dict(size=10,\n                              line=dict(width=2,\n                                        color='DarkSlateGrey')),\n                selector=dict(mode='markers'))\nfig.update_yaxes(visible=False, showticklabels=False)\nfig.update_xaxes(visible=False, showticklabels=False)\nfig.show();","0b989044":"dim_code = 4","6a769326":"class cVAE(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # encoder\n\n        self.label = nn.Embedding(10, dim_code)\n\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size = 4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size = 4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU()\n\n        )\n\n        self.flatten_mu = nn.Linear(128 * 7 * 7, out_features=dim_code)\n        self.flatten_logsigma = nn.Linear(128 * 7 * 7, out_features=dim_code)\n\n        # decoder\n\n        self.decode_linear = nn.Linear(8, 128 * 7 * 7)\n        self.decode_2 = nn.ConvTranspose2d(in_channels=128, out_channels=64, \n                                        kernel_size=4, stride=2, padding=1)\n        self.decode_1 = nn.ConvTranspose2d(in_channels=64, out_channels=1, \n                                        kernel_size=4, stride=2, padding=1)\n\n\n\n\n    def encode(self, x, y):\n        y = self.label(y)\n        x = self.encoder(x)\n        x = x.view(x.size(0), -1)\n        mu, logsigma = self.flatten_mu(x), self.flatten_logsigma(x)\n        x = self.gaussian_sampler(mu, logsigma)\n        z = torch.cat((x, y), dim = 1)\n        return z\n    \n    def gaussian_sampler(self, mu, logsigma):\n        if self.training:\n            std = torch.exp(logsigma \/ 2)\n            eps = torch.empty_like(std).normal_()\n            return eps.mul(std).add_(mu)\n        else:\n            return mu\n\n\n    def decode(self, x, y):\n        y = self.label(y)\n        z = torch.cat((x, y), dim = 1)\n        x = self.decode_linear(z)\n        x = x.view(x.size(0), 128, 7, 7)\n        x = F.relu(self.decode_2(x))\n        reconstruction = F.sigmoid(self.decode_1(x))\n        return reconstruction\n    \n    def forward(self, x, y):\n        y = self.label(y)\n        x = self.encoder(x)\n        x = x.view(x.size(0), -1)\n        mu, logsigma = self.flatten_mu(x), self.flatten_logsigma(x)\n        x = self.gaussian_sampler(mu, logsigma)\n        z = torch.cat((x, y), dim = 1)\n        x = self.decode_linear(z)\n        x = x.view(x.size(0), 128, 7, 7)\n        x = F.relu(self.decode_2(x))\n        reconstruction = F.sigmoid(self.decode_1(x))\n        return mu, logsigma, reconstruction","c856d524":"def train_epoch_cvae(model, criterion, optimizer, data_loader):\n\n    train_losses_per_epoch = []\n    model.train()\n    for x_batch, y in data_loader:\n        y = y.to(device)\n        x_batch = x_batch.to(device)    \n        mu, logsigma, reconstruction = model(x_batch, y)\n        loss = criterion(x_batch.to(device).float(), mu, logsigma, reconstruction)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_losses_per_epoch.append(loss.item())\n\n    return np.mean(train_losses_per_epoch), mu, logsigma, reconstruction","faf2b0ee":"def eval_epoch_cvae(model, criterion, optimizer, data_loader):\n\n    val_losses_per_epoch = []\n    model.eval()\n    with torch.no_grad():\n        for x_val, y in data_loader:\n            y = y.to(device)\n            x_val = x_val.to(device)\n            mu, logsigma, reconstruction = model(x_val, y)\n            loss = criterion(x_val.to(device).float(), mu, logsigma, reconstruction)\n            val_losses_per_epoch.append(loss.item())\n    return np.mean(val_losses_per_epoch), mu, logsigma, reconstruction","6be789d3":"def plot_output_cvae(model, epoch, epochs, train_loss, val_loss, size = 5):\n\n\n    clear_output(wait=True)\n    plt.figure(figsize=(18, 6))\n    for k in range(size):\n        ax = plt.subplot(2, size, k + 1)\n        img, label = next(iter(test_loader))\n        img = img.to(device)\n        label = label.to(device)\n        model.eval()\n        with torch.no_grad():\n            mu, logsigma, reconstruction  = model(img, label)\n\n        plt.imshow(img[k].cpu().squeeze().numpy(), cmap='gray')\n        plt.axis('off')\n        plt.title(f'Real val {label[k].item()}')\n        ax = plt.subplot(2, size, k + 1 + size)\n        plt.imshow(reconstruction[k].cpu().squeeze().numpy(), cmap='gray')\n        plt.axis('off')\n\n        if k == size\/\/2:\n            ax.set_title('Output')\n    plt.suptitle('%d \/ %d - loss: %f val_loss: %f' % (epoch+1, epochs, train_loss, val_loss))\n    plt.show()","e6b353a9":"criterion = loss_vae\nautoencoder_cvae = cVAE().to(device)\noptimizer = torch.optim.Adam(autoencoder_cvae.parameters(), lr=1e-3)","835943a5":"epochs = 30\nloss = {'train_loss':[],'val_loss':[]}\nwith tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n    for epoch in range(epochs):\n        print('* Epoch %d\/%d' % (epoch+1, epochs))\n        train_loss, mu, logsigma, reconstruction = (train_epoch_cvae(autoencoder_cvae, \n                                                             criterion, \n                                                             optimizer, \n                                                             train_loader\n                                                             )\n        )\n\n        val_loss, mu, logsigma, reconstruction = (eval_epoch_cvae(autoencoder_cvae, \n                                                          criterion, \n                                                          optimizer, \n                                                          test_loader)\n        )\n        pbar_outer.update(1)\n      \n        loss['train_loss'].append(train_loss)\n        loss['val_loss'].append(val_loss)\n        plot_output_cvae(autoencoder_cvae, epoch, epochs, train_loss, val_loss, size = 5)","196531f0":"plt.figure(figsize=(15, 6))\nplt.semilogy(loss['train_loss'], label='Train')\nplt.semilogy(loss['val_loss'], label='Valid')\nplt.xlabel('Epoch')\nplt.ylabel('Average Loss')\nplt.legend()\nplt.title('Loss_vae')\nplt.show()","46dc8afc":"inputs = np.array([np.random.normal(0, 1, dim_code) for i in range(12)])\nlabel_4 = np.array([4 for i in range(12)])\nlabel_2 = np.array([2 for i in range(12)])","9c594fa5":"with torch.no_grad():\n    autoencoder_cvae.eval()\n    output = autoencoder_cvae.decode(torch.FloatTensor(inputs).to(device), \n                                   torch.IntTensor(label_4).to(device)\n                                   )","bc3f3fdc":"plt.figure(figsize=(18, 6))\nfor k in range(12):\n    plt.subplot(2, 6, k+1)\n    res = output[k].cpu().squeeze().detach().numpy()\n    plt.imshow(res, cmap='gray')\n    plt.title(f'Sampling for 4 \u2116 {k + 1}')\n    plt.axis('off')","d7d2d081":"with torch.no_grad():\n    autoencoder_cvae.eval()\n    output = autoencoder_cvae.decode(torch.FloatTensor(inputs).to(device), \n                                   torch.IntTensor(label_2).to(device)\n                                   )","641a9e1a":"plt.figure(figsize=(18, 6))\nfor k in range(12):\n    plt.subplot(2, 6, k+1)\n    res = output[k].cpu().squeeze().detach().numpy()\n    plt.imshow(res, cmap='gray')\n    plt.title(f'Sampling for 2 \u2116 {k + 1}')\n    plt.axis('off')","38bbbec5":"latent_space = []\n\ntest_loader = (torch.utils.data.DataLoader(dataset=test_dataset, \n                                           batch_size=1, \n                                           shuffle=False)\n)\nfor X, y in tqdm(test_loader):\n    img = X.to(device)\n    label = y.to(device)\n    autoencoder_cvae.eval()\n    with torch.no_grad():\n        latent = autoencoder_cvae.encode(img, label)\n\n    latent = latent.flatten().cpu().numpy()\n    sample = {f\"Encoded_{i}\": encoded for i, encoded in enumerate(latent)}\n    sample['label'] = label.item()\n    latent_space.append(sample)","fe2bbebb":"latent_space = pd.DataFrame(latent_space)\nlatent_space['label'] = latent_space['label'].astype(str)","5cfc612e":"latent_space.head()","1ea395a6":"tsne = TSNE(n_components=2)\ndigits_embedded = tsne.fit_transform(latent_space.drop(['label'],axis=1))","52fc919d":"digits_embedded[:5]","168bbda3":"fig = px.scatter(digits_embedded, x=0, y=1, \n           color=latent_space['label'], opacity=0.7,\n           \n                 labels={\n                     'color': \"Digit \"\n                 },\n                title=\"Latent space with t-SNE\")\\\n                .for_each_trace(lambda t: t.update(name=t.name.replace(\"=\",\": \"))\n                )\n           \n           \n\nfig.update_traces(marker=dict(size=10,\n                              line=dict(width=2,\n                                        color='DarkSlateGrey')),\n                selector=dict(mode='markers'))\nfig.update_yaxes(visible=False, showticklabels=False)\nfig.update_xaxes(visible=False, showticklabels=False)\nfig.show();","8145ccd6":"<a href=\"#footer\"> To contents <\/a>\n<footer id = \"footer\"> <\/footer>","96bdcda6":"The `t-SNE` representation allows you to separate one digit from another. Some points intersect, but nevertheless - better than the usual representation - we observe groups of points","1bbe879e":"Let's try to sample from `cVAE`. You can take the same latent vector and ask `cVAE` to restore pictures of different classes from it\nFor `MNIST`, you can ask` cVAE` to recover from one latent vector, for example, pictures of the numbers `4` and` 2`.","7fc9f00e":"Let's start teaching variational autoencoders. We will train on the dataset `MNIST`, containing handwritten digits from` 0` to `9`","37cece54":"### Sampling\n","6d4bff9c":"\n<p align=\"center\"><b>Autoencoders. PyTorch<\/b>\n\n\n\n","06f51a61":"the loss is smaller, let's see the learning curves","9e670a47":"In this case, many digits fall into one cluster. Let's try using `t-SNE` and compare the difference","713768b4":"Let's try for `VAE` the same as with a regular autoencoder - show` decoder` from `VAE` random vectors from normal distribution and see what pictures are obtained:","04fd7e2d":"### Model architecture and training\n\nWe implement VAE. At the encoder, we will make two convolutions with normalization, then a line layer will be added. Next, the encoder has an `Unflatten` layer and a` ConTransposed2d` layer","d09a8eac":"We import the libraries necessary for work in the future. For convenience, we will collect imports in the first cell","432130b8":"# Importing Libraries","95c1d735":"### Latent Representations","80871c7d":"<a href=\"#footer\"> To contents <\/a>\n<footer id = \"footer\"> <\/footer>","501e34fb":"The loss of `VAE` consists of two parts:` KL` and `log-likelihood`.\n\nThe total loss will look like this:\n\n$$\\mathcal{L} = -D_{KL}(q_{\\phi}(z|x)||p(z)) + \\log p_{\\theta}(x|z)$$\n\nFormula for `KL-divergence`:\n\n$$D_{KL} = -\\frac{1}{2}\\sum_{i=1}^{dimZ}(1+log(\\sigma_i^2)-\\mu_i^2-\\sigma_i^2)$$\n\nLet's take the cross-entropy we are used to as the `log-likelihood`.","c344baed":"The numbers are perfectly separated - we are already seeing clear clusters, in comparison with the `VAE` - this is due to the fact that the latent space contains information about the target variable (` Encoded_4: Encoded_7`) - `data leaks` and that is ok here)","43a543b5":"<a id='early'><\/a>","c2b575a1":"Thus, when generating a new random picture, we will have to pass the concatenated latent vector and picture class to the decoder.","eaaaa198":"<a href=\"#footer\"> To contents <\/a>\n<footer id = \"footer\"> <\/footer>","c2535a89":"[**The problem of classic autoencoders**](https:\/\/neurohive.io\/ru\/osnovy-data-science\/variacionnyj-avtojenkoder-vae\/).\nClassic autoencoders are trained to encode and recover input data. However, the area of \u200b\u200btheir application is limited mainly to the creation of noise-canceling encoders.\n","b4d22ade":"To our `VAE` add the function` give me a random number from this class here`, where there are ten classes (numbers from 0 to 9 form ten classes). `Conditional VAE` is the name of the type of autoencoder that provides such an opportunity.","9dc254be":"An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning).The encoding is validated and refined by attempting to regenerate the input from the encoding. The autoencoder learns a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore insignificant data (\u201cnoise\u201d)[wiki](https:\/\/en.wikipedia.org\/wiki\/Autoencoder)","a21d28f8":"Let's see what the image latency space looks like in `cVAE` ([earlier](#early) we did it for` VAE`)","f976b11b":"And let's train the model for 30 epochs:","64672ff0":"But before the `t-SNE` representation, let's try to draw the latent space without the` t-SNE` for comparison. In order to display the numbers by color - we need labels, so we will collect the `DataFrame` from the latent space and labels denoting this or that digit","ebc01d50":"Let's change the functions for training a little - we need to transfer information about the value of the target variable for the object","77bae3f8":"<a href=\"#footer\"> To contents <\/a>\n<footer id = \"footer\"> <\/footer>","f3793988":"<a href=\"#footer\"> To contents <\/a>\n<footer id = \"footer\"> <\/footer>","a803c263":"<a href=\"#footer\"> To contents <\/a>\n<footer id = \"footer\"> <\/footer>","96f9d12f":"## Conditional VAE\n","60cfbc2d":"<a href=\"#footer\"> To contents <\/a>\n<footer id = \"footer\"> <\/footer>","116eacc2":"Pretty good. Net draws handwritten numbers","945d4596":"<a href=\"#footer\"> To contents <\/a>\n<footer id = \"footer\"> <\/footer>","90b70598":"<footer id=\"footer\"><\/footer>","5054fff2":"### Architecture\n\n\nIn fact, the only difference from the usual one is that together with the picture in the first layer of the encoder and decoder we also transfer information about the picture class.\n\nThat is, the concatenation of the picture and information about the class (for example, a vector of nine zeros and one one) is fed to the first (input) layer of the encoder. The concatenation of the latent vector and class information is fed into the first layer of the decoder.","4f015b75":"# Variational Autoencoder ","cb081b91":"Let's see how latent vectors look in space.\nYour task is to depict the latent vectors of pictures with points in a two-dimensional space.\n\nThis will make it possible to estimate how densely the latent vectors of digit images are distributed in space.\n\n1. Get latent representations of test dataset pictures\n2.Using `TSNE` (available in` sklearn`), compress these representations to a dimension of 2 (so that you can visualize them as points in space)\n3. Render the resulting two-dimensional representations using `plotly.express` \/ [documentation](https:\/\/plotly.com\/python-api-reference\/generated\/plotly.express.scatter), paint the points corresponding to pictures of different numbers with different colors","13099209":"### Latent Representation","f7c869b2":"Now for the number `2`","ee5f543d":"## VAE","fda31f07":"Let's define the loss and its components for the `VAE`:","89339051":"Thank you for reading."}}