{"cell_type":{"7d39549a":"code","3493f786":"code","14a77f58":"code","00487a27":"code","d0ea8703":"code","4fa846a8":"code","77ee9069":"code","8f53eb11":"code","231a1cb1":"code","1523d197":"code","52ee93bf":"code","a9151a66":"code","cb156c76":"code","0acfee99":"code","b36f2b4f":"code","39d4e277":"code","5a1f2908":"code","dc24b20b":"code","4c847297":"code","6547fede":"code","ef9ca060":"code","86eb9dd8":"code","d3fb27e0":"code","66c455ad":"markdown","c7b12268":"markdown","ea988a72":"markdown","f619d9fc":"markdown"},"source":{"7d39549a":"import re\nimport json\nimport string\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nimport gc\nimport pickle\nfrom tqdm import tqdm\ntqdm.pandas()\n\n%matplotlib inline\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport lightgbm as lgb\n\nimport time\nfrom contextlib import contextmanager\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999","3493f786":"@contextmanager\ndef timer(task_name=\"timer\"):\n    # a timer cm from https:\/\/www.kaggle.com\/lopuhin\/mercari-golf-0-3875-cv-in-75-loc-1900-s\n    print(\"----> {} started\".format(task_name))\n    t0 = time.time()\n    yield\n    print(\"----> {} done in {:.0f} seconds\".format(task_name, time.time() - t0))","14a77f58":"with timer(\"reading_data\"):\n    train_df = pd.read_csv(\"..\/input\/train.csv\")\n    test_df = pd.read_csv(\"..\/input\/test.csv\")\n    df = pd.concat([train_df ,test_df]).reset_index(drop=True)\n\n    y_train = train_df[\"target\"].values\n\n    print('Total:', df.shape)\n    print('Train:', train_df.shape)\n    print('Test:', test_df.shape)\n    print(\"Number of texts: \", df.shape[0])","00487a27":"print(train_df['target'].value_counts())\nsns.countplot(train_df['target'])\nplt.show()","d0ea8703":"import psutil\nfrom multiprocessing import Pool\n\nnum_cores = psutil.cpu_count()  # number of cores on your machine\nnum_partitions = num_cores  # number of partitions to split dataframe\n\nprint('number of cores:', num_cores)\ndef df_parallelize_run(df, func):\n    df_split = np.array_split(df, num_partitions)\n    pool = Pool(num_cores)\n    df = pd.concat(pool.map(func, df_split))\n    pool.close()\n    pool.join()\n    return df\n","4fa846a8":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', \n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', \n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', \n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\n\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n    return mispellings_re.sub(replace, text)\n\ndef text_cleaning(text):\n    text = clean_text(text)\n    text = clean_numbers(text)\n    text = replace_typical_misspell(text)\n    return text\n\ndef text_clean_wrapper(df):\n    df[\"question_text\"] = df[\"question_text\"].progress_apply(text_cleaning)\n    return df","77ee9069":"with timer(\"basic_feature_engineering\"):\n    from nltk.corpus import stopwords\n    STOPWORDS = list(set(stopwords.words('english')))\n\n    def count_regexp_occ(regexp=\"\", text=None):\n        \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n        return len(re.findall(regexp, text))\n\n    # Count number of \\n\n    df[\"ant_slash_n\"] = df['question_text'].progress_apply(lambda x: count_regexp_occ(r\"\\n\", x))\n    # Get length in words and characters\n    df[\"raw_word_len\"] = df['question_text'].progress_apply(lambda x: len(x.split()))\n    df[\"raw_char_len\"] = df['question_text'].progress_apply(lambda x: len(x))\n    # Check number of upper case, if you're angry you may write in upper case\n    df[\"nb_upper\"] = df['question_text'].progress_apply(lambda x: count_regexp_occ(r\"[A-Z]\", x))\n    df[\"nb_title\"] = df[\"question_text\"].progress_apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n    # stopwords count\n    df[\"num_stopwords\"] = df[\"question_text\"].progress_apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n","8f53eb11":"# text cleaning\nwith timer(\"text_cleaning\"):\n    df = df_parallelize_run(df, text_clean_wrapper)","231a1cb1":"train_df = df.loc[:train_df.shape[0] - 1, :]\ntest_df = df.loc[train_df.shape[0]:, :]\ndel test_df['target']; del df\ngc.collect()\n\ntrain_df.reset_index(drop=True, inplace=True)\ntest_df.reset_index(drop=True, inplace=True)\n\nprint('Train:', train_df.shape)\nprint('Test:', test_df.shape)","1523d197":"handcraft_feature_names = [f_ for f_ in test_df.columns if f_ not in [\"question_text\", \"qid\"]]\ntrain_handcraft_features = train_df[handcraft_feature_names]\ntest_handcraft_features = test_df[handcraft_feature_names]\nprint('handcraft feature count:', len(handcraft_feature_names))","52ee93bf":"all_text = pd.concat([train_df['question_text'], test_df['question_text']], axis =0)","a9151a66":"with timer(\"word_vectorizer\"):\n    word_vectorizer = TfidfVectorizer(\n                    ngram_range=(1,4),\n                    token_pattern=r'\\w{1,}',\n                    min_df=3,\n                    max_df=0.9,\n                    strip_accents='unicode',\n                    use_idf=True,\n                    smooth_idf=True,\n                    sublinear_tf=True,\n                    max_features=100000\n                    )\n    \n    word_vectorizer.fit(all_text)\n    train_word_tfidf_features  = word_vectorizer.transform(train_df['question_text'])\n    test_word_tfidf_features  = word_vectorizer.transform(test_df['question_text'])","cb156c76":"def prepare_for_char_n_gram(text):\n    \"\"\"\n    The word hashing method described here aim to reduce the\n    dimensionality of the bag-of-words term vectors. It is based on\n    letter n-gram, and is a new method developed especially for our\n    task. Given a word (e.g. good), we first add word starting and\n    ending marks to the word (e.g. #good#). Then, we break the word\n    into letter n-grams (e.g. letter trigrams: #go, goo, ood, od#).\n    Finally, the word is represented using a vector of letter n-grams. \n    \"\"\"\n    text = re.sub(\" \", \"# #\", text)  # Replace space\n    text = \"#\" + text + \"#\"  # add leading and trailing #\n    return text\n\ntrain_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: prepare_for_char_n_gram(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: prepare_for_char_n_gram(x))","0acfee99":"with timer(\"char_vectorizer\"):\n    def char_analyzer(text):\n        \"\"\"\n        This is used to split strings in small lots\n        Word Hashing: https:\/\/www.microsoft.com\/en-us\/research\/wp-content\/uploads\/2016\/02\/cikm2013_DSSM_fullversion.pdf\n        \"\"\"\n        tokens = text.split()\n        return [token[i: i + 3] for token in tokens for i in range(len(token) - 2)]\n\n    char_vectorizer = TfidfVectorizer(\n                        ngram_range=(1,1),\n                        tokenizer=char_analyzer,\n                        min_df=3,\n                        max_df=0.9,\n                        strip_accents='unicode',\n                        use_idf=True,\n                        smooth_idf=True,\n                        sublinear_tf=True,\n                        max_features=50000\n                        )\n\n    char_vectorizer.fit(all_text)\n    train_char_tfidf_features = char_vectorizer.transform(train_df['question_text'])\n    test_char_tfidf_features = char_vectorizer.transform(test_df['question_text'])","b36f2b4f":"from scipy.sparse import hstack, csr_matrix\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass NBTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    https:\/\/nlp.stanford.edu\/pubs\/sidaw12_simple_sentiment.pdf\n    \"\"\"\n    def __init__(self, alpha=1):\n        self.r = None\n        self.alpha = alpha\n\n    def fit(self, X, y):\n        # store smoothed log count ratio\n        p = self.alpha + X[y==1].sum(0)\n        q = self.alpha + X[y==0].sum(0)\n        self.r = csr_matrix(np.log(\n            (p \/ (self.alpha + (y==1).sum())) \/\n            (q \/ (self.alpha + (y==0).sum()))\n        ))\n        return self\n\n    def transform(self, X, y=None):\n        return X.multiply(self.r)","39d4e277":"with timer(\"transform_Naive_Bayes\"):\n    # transform to Naive Bayes feature\n    nb_transformer = NBTransformer(alpha=1).fit(train_word_tfidf_features, y_train)\n    train_word_tfidf_features = nb_transformer.transform(train_word_tfidf_features)\n    test_word_tfidf_features = nb_transformer.transform(test_word_tfidf_features)\n    \n    nb_transformer = NBTransformer(alpha=1).fit(train_char_tfidf_features, y_train)\n    train_char_tfidf_features = nb_transformer.transform(train_char_tfidf_features)\n    test_char_tfidf_features = nb_transformer.transform(test_char_tfidf_features)","5a1f2908":"with timer(\"feature_concate\"):\n    feature_names = word_vectorizer.get_feature_names() + char_vectorizer.get_feature_names() + handcraft_feature_names\n    del all_text; gc.collect()\n\n    X_train = hstack(\n            [\n                train_word_tfidf_features,\n                train_char_tfidf_features,\n                train_handcraft_features\n            ]\n        ).tocsr()\n\n    del train_word_tfidf_features; del train_char_tfidf_features; del train_handcraft_features\n    gc.collect()\n\n    X_test = hstack(\n        [\n            test_word_tfidf_features,\n            test_char_tfidf_features,\n            test_handcraft_features\n        ]\n    ).tocsr()\n    del test_word_tfidf_features; del test_char_tfidf_features; del test_handcraft_features\n    gc.collect()","dc24b20b":"print('Train:', X_train.shape)\nprint('Test:', X_test.shape)\nprint('feature count:', len(feature_names))","4c847297":"import lightgbm as lgb\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nparam = {\n        \"objective\": \"binary\",\n        'metric': {'auc'},\n        \"boosting_type\": \"gbdt\",\n        \"num_threads\": -1,\n        \"bagging_fraction\": 0.8,\n        \"feature_fraction\": 0.8,\n        \"learning_rate\": 0.1,\n        \"num_leaves\": 31,\n        \"min_split_gain\": .1,\n        \"reg_alpha\": .1,\n        \n        \"scale_pos_weight\": 15,\n    }","6547fede":"with timer(\"run-lightgbm-out-of-fold\"):\n    n_folds = 5\n    sk_folds = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=2019)\n\n    oof = np.zeros(X_train.shape[0])\n    predictions = np.zeros(X_test.shape[0])\n    feature_importance_df = pd.DataFrame()\n\n    train_aucs = []\n    valid_aucs = []\n    for fold_, (trn_idx, val_idx) in enumerate(sk_folds.split(X_train, y_train)):\n        with timer(\"run fold {}\/{}\".format(fold_ + 1, n_folds)):\n            trn_data = lgb.Dataset(X_train[trn_idx], label=y_train[trn_idx])\n            val_data = lgb.Dataset(X_train[val_idx], label=y_train[val_idx])\n            num_round = 10000\n            clf = lgb.train(param, trn_data, num_round, \n                            valid_sets = [trn_data, val_data], \n                            verbose_eval=100, early_stopping_rounds = 200)\n            oof[val_idx] = clf.predict(X_train[val_idx], num_iteration=clf.best_iteration)\n            predictions += clf.predict(X_test, num_iteration=clf.best_iteration) \/ sk_folds.n_splits\n\n            train_aucs.append(clf.best_score['training']['auc'])\n            valid_aucs.append(clf.best_score['valid_1']['auc'])\n\n            # \u5f53\u524d fold \u8bad\u7ec3\u7684 feature importance\n            fold_importance_df = pd.DataFrame()\n#             fold_importance_df[\"feature\"] = used_features\n            fold_importance_df[\"feature\"] = feature_names\n            fold_importance_df[\"importance\"] = clf.feature_importance()\n            fold_importance_df[\"fold\"] = fold_ + 1\n            feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    print('cv_auc')\n    print('-'*50)\n    print('Mean train-auc: {:<8.5f}, valid-auc: {:<8.5f}'.format(np.mean(train_aucs), np.mean(valid_aucs)))\n    print('-'*50)","ef9ca060":"def threshold_searching(y_true, y_proba, verbose=True):\n    from sklearn.metrics import roc_curve, precision_recall_curve, f1_score\n    from sklearn.model_selection import RepeatedStratifiedKFold\n\n    def threshold_search(y_true, y_proba):\n        precision , recall, thresholds = precision_recall_curve(y_true, y_proba)\n        thresholds = np.append(thresholds, 1.001) \n        F = 2 \/ (1\/precision + 1\/recall)\n        best_score = np.max(F)\n        best_th = thresholds[np.argmax(F)]\n        return best_th \n\n\n    rkf = RepeatedStratifiedKFold(n_splits=5, n_repeats=2)\n\n    scores = []\n    ths = []\n    for train_index, test_index in rkf.split(y_true, y_true):\n        y_prob_train, y_prob_test = y_proba[train_index], y_proba[test_index]\n        y_true_train, y_true_test = y_true[train_index], y_true[test_index]\n\n        # determine best threshold on 'train' part \n        best_threshold = threshold_search(y_true_train, y_prob_train)\n\n        # use this threshold on 'test' part for score \n        sc = f1_score(y_true_test, (y_prob_test >= best_threshold).astype(int))\n        scores.append(sc)\n        ths.append(best_threshold)\n\n    best_th = np.mean(ths)\n    score = np.mean(scores)\n\n    if verbose: print(f'Best threshold: {np.round(best_th, 4)}, Score: {np.round(score,5)}')\n\n    return best_th, score","86eb9dd8":"with timer(\"search-threshold\"):\n    best_threshold, cv_f1_score = threshold_searching(y_train, oof)","d3fb27e0":"pred_test_y = (predictions > best_threshold).astype(int)\nsub = test_df[['qid']]\nsub['prediction'] = pred_test_y\nsub.to_csv(\"submission.csv\", index=False)","66c455ad":"# Text Preprocess","c7b12268":"# Optimal threshold","ea988a72":"## Word&Char TFIDF Vectorizer","f619d9fc":"## Features concate"}}