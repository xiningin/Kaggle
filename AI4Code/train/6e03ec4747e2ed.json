{"cell_type":{"1b105ad4":"code","3f97b5e3":"code","c42357bf":"code","358ee72d":"code","55427d82":"code","d4cbf20b":"code","9f65cc4f":"code","bd1fe65b":"code","8ab26293":"code","7d56694e":"code","71b7fcf2":"code","2ae13ca2":"code","9eb4cb66":"code","6028d04b":"code","b4458e32":"code","d747ba79":"code","c322d160":"code","6b3b1e33":"code","890e3364":"code","53e24260":"code","975925a2":"code","a25afcbe":"markdown","7d0c1b88":"markdown","8940544c":"markdown","9257bb99":"markdown","c4e7a286":"markdown","f25a8352":"markdown","dd7edd92":"markdown","4da7a7c5":"markdown","44898ebc":"markdown","3958213c":"markdown"},"source":{"1b105ad4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings \nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns',None)\npd.set_option('display.max_rows',None)\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Sequential\n\nfrom keras.layers import *\nfrom datetime import datetime\nimport kerastuner as kt\nfrom kerastuner import HyperModel\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\n","3f97b5e3":"train=pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ntestdf=pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\nsub=pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")","c42357bf":"train.head(2)","358ee72d":"testdf.head(2)","55427d82":"sub.head(2)","d4cbf20b":"features = train.columns[1:-1]\ndf=train[features].copy()","9f65cc4f":"si=SimpleImputer(strategy='median',copy=False)\nsi.fit_transform(df)\nidf=pd.DataFrame(data=df,columns=features)","bd1fe65b":"st=StandardScaler(copy = False)\nst.fit_transform(idf)\nstidf=pd.DataFrame(data=idf,columns=features)","8ab26293":"X=stidf.values\nY=train[['claim']].values\ntestdf.drop(columns=\"id\",inplace=True)","7d56694e":"x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.20,  random_state=21)","71b7fcf2":"class ANNHyperModel(HyperModel):\n    \n    def build(self, hp):\n        model = tf.keras.Sequential()\n        # Tune the number of units in the first Dense layer\n        # Choose an optimal value between 32-512\n        hp_units1 = hp.Int('units1', min_value=32, max_value=512, step=32)\n        hp_units2 = hp.Int('units2', min_value=32, max_value=512, step=32)\n        hp_units3 = hp.Int('units3', min_value=32, max_value=512, step=32)\n        model.add(Dense(units=hp_units1, activation='relu'))\n        model.add(tf.keras.layers.Dense(units=hp_units2, activation='relu'))\n        model.add(tf.keras.layers.Dense(units=hp_units3, activation='relu'))\n        model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n\n        # Tune the learning rate for the optimizer\n        # Choose an optimal value from 0.01, 0.001, or 0.0001\n        hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n\n        model.compile(\n            optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n            loss='binary_crossentropy',\n            metrics=[tf.keras.metrics.AUC()]\n        )\n\n        return model\n\nhypermodel = ANNHyperModel()\n\ntuner = kt.Hyperband(\n    hypermodel,\n    objective=kt.Objective(\"auc\", direction=\"max\"),\n    max_epochs=10,\n    factor=3,\n    directory='keras_tuner_dir',\n    project_name=\"Hyperband_TPS\"\n)\n\ntuner.search(x_train, y_train, epochs=10, validation_split=0.2)","2ae13ca2":"best_model = tuner.get_best_models()[0]\nbest_model.build(x_train.shape)\nbest_model.summary()","9eb4cb66":"best_model.fit(\n    x_train, \n    y_train,\n    epochs=50,\n    batch_size=1024\n)","6028d04b":"y_pred=best_model.predict(x_test)\nprint(y_pred.shape,y_test.shape)","b4458e32":"roc_auc_score(y_test,y_pred)","d747ba79":"si.transform(testdf)\nst.transform(testdf)","c322d160":"test = testdf.values","6b3b1e33":"pred = best_model.predict(test)\nprint(pred.shape)\npred","890e3364":"sub['claim'] = pred","53e24260":"sub.head()","975925a2":"sub.to_csv(\"submission_ann2.csv\",index=False)","a25afcbe":"#### In this notebook my attempt is to create an ANN model using HyperModel from keras tuner.","7d0c1b88":"## ANN Algorithm Tuning using Hypermodel","8940544c":"###  Submission","9257bb99":"## Import Libraries","c4e7a286":"###  Finalize Model\n###  Predictions on Validation dataset","f25a8352":"#### + In an earlier notebook we did the [EDA](http:\/\/www.kaggle.com\/saileshnair\/tps202109-normal-and-quick-eda)\n#### + In the next we loooked at machine learning algorithms and their comparison after feature transformations. [Algorithm Comparion+Transformation](https:\/\/www.kaggle.com\/saileshnair\/tps2021-feature-transformation)","dd7edd92":"### Scaling Values","4da7a7c5":"### Imputing missing values","44898ebc":"###  Predictions on Test dataset\n","3958213c":"### Splitting the dataset"}}