{"cell_type":{"ca9c04c9":"code","e045b4f6":"code","1210a453":"code","56aaa824":"code","f9f74844":"code","b0c9c034":"code","22302d19":"code","90683795":"code","e103d771":"code","4de2a10d":"code","2e07e905":"code","ff4bf091":"code","732ad84f":"code","b458f840":"code","182725e1":"code","a12ed5f9":"code","e373a724":"code","738be6f3":"code","e4bf6946":"code","a995cd51":"code","39c56af8":"code","c3cc26d4":"code","5b840530":"code","89bdd686":"code","69c98ffc":"code","f94c3ce7":"code","e91534a6":"markdown","8bf5e0eb":"markdown","272423b7":"markdown","9498953c":"markdown","b43d3c07":"markdown","c1e8963f":"markdown","7e0a424a":"markdown","bc732e9e":"markdown","91ad585c":"markdown","d5fdafd2":"markdown","180f0f95":"markdown","e7f68192":"markdown","06fff745":"markdown","dff9eb9e":"markdown","9a8729d6":"markdown"},"source":{"ca9c04c9":"#https:\/\/github.com\/FrozenWolf-Cyber\/Brain-Tumor-Classification","e045b4f6":"import torch\nimport torch.nn as nn\nfrom torchvision import models\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom torch.utils.data import Dataset as Dataset\nfrom torch.utils.data import DataLoader as DataLoader\nfrom sklearn import cluster\nfrom zipfile import ZipFile\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom PIL import Image\nimport os\nimport random\nimport pickle\n\ntorch.cuda.empty_cache()\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\nsns.set_style(\"darkgrid\")\n\n","1210a453":"path = '..\/input\/brain-tumor-classification-mri-images\/brain_tumor_mri\/new_dataset\/'\nprint(os.listdir(path))","56aaa824":"training_data = pickle.load(open(path+'training_data.pickle','rb'))\nrandom.shuffle(training_data)","f9f74844":"total_data_len = len(training_data)\nclasses = ['Meningioma', 'Glioma', 'Pitutary']\nn_classes = len(classes)\n\nx_raw = []\ny_raw = []\n\nfor i in training_data:\n    x_raw.append(i[0])\n    y_raw.append(i[1])\n\ny_raw = list(np.asarray(y_raw)-np.ones(total_data_len,dtype=np.int))","b0c9c034":"print(f'Total number of images : {total_data_len}')","22302d19":"plt.imshow(x_raw[0])\nprint(classes[y_raw[0]])","90683795":"each_class = [0,0,0]\nfor i in y_raw:\n    for j in range(n_classes):\n        if i == j:\n            each_class[j] = each_class[j] + 1\nprint(f'Number of scans with Meningioma : {each_class[0]} Glioma : {each_class[1]} Pitutary : {each_class[2]}')            \nax, fig = plt.subplots(figsize=[7,7])\nsns.barplot(x=classes, y=each_class)\nplt.show()","e103d771":"sample = []\nsample_size = 4\nfor i in range(n_classes):\n    temp = []\n    key = 0\n    while True:\n        index = y_raw.index(i,key,total_data_len-1)\n        key = index+1\n        temp.append(x_raw[index])\n        \n        if len(temp) == sample_size:\n            break\n            \n    sample.append(temp)\nsample = np.asarray(sample)\nprint(f'sample contains {len(sample)} classes with {len(sample[0])} samples in each class')","4de2a10d":"for i in range(n_classes):\n    fig = plt.figure(figsize=(15,15))\n    for j in range(sample_size):\n        fig.add_subplot(i+1,sample_size ,j+1)\n        plt.title(classes[i])\n        plt.imshow(sample[i][j])\n","2e07e905":"def clustered_img(x):\n    kmeans = cluster.KMeans(5)\n    dims = np.shape(x)\n    pixel_matrix = np.reshape(x, (dims[0] * dims[1], dims[2]))\n    clustered = kmeans.fit_predict(pixel_matrix)\n\n\n    clustered_img = np.reshape(clustered, (dims[0], dims[1]))\n    return clustered_img","ff4bf091":"for i in range(n_classes):\n    fig = plt.figure(figsize=(15,15))\n    for j in range(sample_size):\n        fig.add_subplot(i+1,sample_size ,j+1)\n        plt.title(classes[i])\n        plt.imshow(clustered_img(sample[i][j]))\n","732ad84f":"n_train = int(0.95 *total_data_len)\nn_validation = int(0.021*total_data_len)\n\ntrain_image = x_raw[:n_train]\ntrain_label = y_raw[:n_train]\n\nvalidation_image = x_raw[n_train:n_train+n_validation]\nvalidation_label = y_raw[n_train:n_train+n_validation]\n\ntest_image = x_raw[n_train:n_train+n_validation:]\ntest_label = y_raw[n_train:n_train+n_validation:]","b458f840":"class Dataset(Dataset):\n    def __init__(self,x,y,transform = 'T.Resize((224,224))'):\n        self.transform = transform\n        self.x = x\n        self.y = y \n        self.len = len(x)\n   \n    def __len__(self):\n        return self.len\n    \n    def __getitem__(self,i):\n        return torch.FloatTensor(np.asarray(self.transform(Image.fromarray(self.x[i])))),torch.LongTensor([self.y[i]])\n\nbatch_size = 16\n\ntransforms_train = T.Compose([\n        T.Resize((224,224)),\n        T.RandomRotation(degrees=(-45, 45)),\n        T.RandomHorizontalFlip(),\n        T.RandomVerticalFlip(),\n        T.ToTensor(),\n        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\ntransforms_test = T.Compose([\n        T.Resize((224,224)),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\ntrain_dataset = Dataset(train_image,train_label,transform =transforms_train)\nvalidation_dataset = Dataset(validation_image,validation_label,transform =transforms_test)\ntest_dataset = Dataset(test_image,test_label,transform =transforms_test)\n\ntrain_dataloader = DataLoader(train_dataset , batch_size = batch_size , shuffle = True )\nvalidation_dataloader = DataLoader(validation_dataset , batch_size = batch_size , shuffle = True )\ntest_dataloader = DataLoader(test_dataset , batch_size = batch_size , shuffle = True )","182725e1":"next(iter(train_dataloader))[0].shape","a12ed5f9":"model = models.resnet50(pretrained=True)\nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, n_classes)\n","e373a724":"lr = 0.0015\nepoch = 15\ntrain_dataset_size = train_dataset.__len__()\nvalidation_dataset_size = validation_dataset.__len__()\ntrain_n_minibatches = train_dataloader.__len__()\nvalidation_n_minibatches = validation_dataloader.__len__()\n\ndevice = 'cuda'\nmodel.to(device)\nloss_history = [[],[]] #[[train], [validation]]\naccuracy_history = [[],[]] #[[train], [validation]]\n\ncriterion = nn.CrossEntropyLoss()  \noptimizer = torch.optim.SGD(model.parameters(),lr = lr)\n\n","738be6f3":"for e in range(epoch):\n    print(f'---------------------------------------EPOCH {e+1}-------------------------------------------')\n    for batch_idx , (x ,y) in enumerate(train_dataloader):\n        model.train() # Setting mode to train\n        optimizer.zero_grad()\n        x , y = x.to(device) , y.to(device)\n        y_pred = model(x).to(device)\n        \n        # Calculating Loss\n        loss = criterion(y_pred,y.reshape(x.shape[0]))\n        loss.backward()\n        optimizer.step()\n        loss_history[0].append(float(loss.detach()))\n        \n        #Calaculating Accuracy\n        correct = 0\n        y_pred = y_pred.cpu().detach().numpy().tolist()\n        y = y.cpu().detach().numpy().tolist()\n        for i in range(x.shape[0]):\n            n = 0\n            n = y_pred[i].index(max(y_pred[i]))\n            if n == y[i][0]:\n                correct = correct + 1\n        accuracy_history[0].append((correct\/x.shape[0])*100)\n        \n        if batch_idx % 30 == 0:\n            # Printing Log\n            print(f'LOSS for EPOCH {e+1} BATCH {batch_idx}\/{train_n_minibatches} TRAIN LOSS : {loss_history[0][-1]}',end = ' ')\n            print(f'TRAIN ACCURACY : {accuracy_history[0][-1]}',end = ' ')\n            with torch.no_grad():\n                # Calculating loss and accuracy for validation\n                model.eval()\n                for _batch_idx_ , (x ,y) in enumerate(validation_dataloader):\n                    x , y = x.to(device) , y.to(device)\n                    y_pred = model(x).to(device)\n                    validation_loss = criterion(y_pred,y.reshape(x.shape[0]))\n                    loss_history[1].append(float(validation_loss.detach()))\n                    \n                    correct = 0\n                    y_pred = y_pred.cpu().detach().numpy().tolist()\n                    y = y.cpu().detach().numpy().tolist()      \n                    for i in range(x.shape[0]):\n                        n = 0\n                        n = y_pred[i].index(max(y_pred[i]))\n                        if n == y[i][0]:\n                            correct = correct + 1\n                    accuracy_history[1].append((correct\/x.shape[0])*100)\n                        \n                    \n                print(f'VALIDATION LOSS : {sum(loss_history[1][-1:-validation_n_minibatches-1:-1])\/validation_n_minibatches}',end = ' ')\n                print(f'VALIDATION ACCURACY : {sum(accuracy_history[1][-1:-validation_n_minibatches-1:-1])\/validation_n_minibatches}')\n    \n    # Saving the model progress\n    torch.save(model.state_dict(),'resnet50_v1')\n    \n    #Log for e+1th epoch\n    print(f'--------------------------------------------------------------------------------------------')\n    print(f'Loss for EPOCH {e+1}  TRAIN LOSS : {sum(loss_history[0][-1:-train_n_minibatches-1:-1])\/train_n_minibatches}',end = ' ')\n    print(f'TRAIN ACCURACY : {sum(accuracy_history[0][-1:-train_n_minibatches-1:-1])\/train_n_minibatches}')\n    n_validation_losses = int(train_n_minibatches\/30)*validation_n_minibatches\n    print(f'VALIDATION LOSS for EPOCH {e+1} : {sum(loss_history[1][-1:-1*n_validation_losses-1:-1])\/n_validation_losses}',end = ' ')\n    print(f'VALIDATION ACCURACY : {sum(accuracy_history[1][-1:-1*n_validation_losses-1:-1])\/n_validation_losses}')\n    print('---------------------------------------------------------------------------------------------')","e4bf6946":"avg_loss_history = [[],[]]\navg_accuracy_history = [[],[]]\n\nfor i in range(0,len(loss_history[0]),30):\n    avg_loss_history[0].append(sum(loss_history[0][i:i+30])\/30)\n    \nfor i in range(0,len(loss_history[1]),4):\n    avg_loss_history[1].append(sum(loss_history[1][i:i+4])\/4)\n    \nfor i in range(0,len(accuracy_history[0]),30):\n    avg_accuracy_history[0].append(sum(accuracy_history[0][i:i+30])\/30)\n    \nfor i in range(0,len(accuracy_history[1]),4):\n    avg_accuracy_history[1].append(sum(accuracy_history[1][i:i+4])\/4)\n\n","a995cd51":"sns.lineplot(x=range(len(avg_loss_history[0])),y=avg_loss_history[0])\nsns.lineplot(x=range(len(avg_loss_history[1])),y=avg_loss_history[1])\nplt.show()\n","39c56af8":"sns.lineplot(x=range(len(avg_accuracy_history[0])),y=avg_accuracy_history[0])\nsns.lineplot(x=range(len(avg_accuracy_history[1])),y=avg_accuracy_history[1])\nplt.show()","c3cc26d4":"# Plotting Loss per epoch\nloss_per_epoch = [[],[]]\nfor i in range(epoch):\n    temp = 0\n    for j in loss_history[0][i*train_n_minibatches:(i+1)*train_n_minibatches]:\n        temp = temp + j\n    loss_per_epoch[0].append(temp\/train_n_minibatches)\n    temp = 0\n    for j in loss_history[1][i*n_validation_losses:(i+1)*n_validation_losses]:\n        temp = temp + j\n    loss_per_epoch[1].append(temp\/n_validation_losses)    \n\nsns.lineplot(x=range(len(loss_per_epoch[0])),y=loss_per_epoch[0])\nsns.lineplot(x=range(len(loss_per_epoch[1])),y=loss_per_epoch[1])\nplt.show()","5b840530":"# Plotting Accuracy per epoch\naccuracy_per_epoch = [[],[]]\nfor i in range(epoch):\n    temp = 0\n    for j in accuracy_history[0][i*train_n_minibatches:(i+1)*train_n_minibatches]:\n        temp = temp + j\n    accuracy_per_epoch[0].append(temp\/train_n_minibatches)\n    temp = 0\n    for j in accuracy_history[1][i*n_validation_losses:(i+1)*n_validation_losses]:\n        temp = temp + j\n    accuracy_per_epoch[1].append(temp\/n_validation_losses)    \n\nsns.lineplot(x=range(len(accuracy_per_epoch[0])),y=accuracy_per_epoch[0])\nsns.lineplot(x=range(len(accuracy_per_epoch[1])),y=accuracy_per_epoch[1])\nplt.show()","89bdd686":"#Loading the saved model\nmodel = models.resnet50(pretrained=True)\nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, n_classes)\nmodel.load_state_dict(torch.load('resnet50_v1', map_location='cpu'))","69c98ffc":"test_loss_history = []\ntest_accuracy_history = []","f94c3ce7":"#Testing the model on test dataset\nmodel = model.to('cpu')\nmodel.eval()\nfor _batch_idx_ , (x ,y) in enumerate(test_dataloader):\n    y_pred = model(x)\n    test_loss = criterion(y_pred,y.reshape(x.shape[0]))\n    test_loss_history.append(float(test_loss.detach()))\n    correct = 0\n    y_pred = y_pred.detach().numpy().tolist()\n    y = y.detach().numpy().tolist()      \n    for i in range(x.shape[0]):\n        n = 0\n        n = y_pred[i].index(max(y_pred[i]))\n        if n == y[i][0]:\n            correct = correct + 1\n    test_accuracy_history.append((correct\/len(y))*100)\n                        \nprint(f'LOSS : {sum(test_loss_history)\/len(test_loss_history)}  ACCURACY : {sum(test_accuracy_history)\/len(test_accuracy_history)}')                    ","e91534a6":"### 4.Training <a class=\"anchor\" id=\"4\"><\/a>","8bf5e0eb":"### 5.Plotting Graphs<a class=\"anchor\" id=\"5\"><\/a>","272423b7":"### 1.Importing Libraries <a class=\"anchor\" id=\"1\"><\/a>","9498953c":"#### 1.Plotting Loss vs MiniBatch<a class=\"anchor\" id=\"5.1\"><\/a>","b43d3c07":"#### 3.Plotting Loss vs Epoch<a class=\"anchor\" id=\"5.3\"><\/a>","c1e8963f":"#### 2.Visualizing Dataset <a class=\"anchor\" id=\"2.2\"><\/a>","7e0a424a":"### 6.Loading and Testing<a class=\"anchor\" id=\"6\"><\/a>","bc732e9e":"## Contents\n-  [1.Importing Libraries](#1)\n-  [2.Dataset Managament](#2)\n    -  [2.1.Downloading and Extracting Dataset](#2.1)\n    -  [2.2.Visualizing Dataset](#2.2)\n    -  [2.3.Splitting Dataset](#2.3)\n    -  [2.4.Pytorch DataLoaders](#2.4)\n-  [3.Initializing pre-trained model](#3)\n-  [4.Training](#4)\n-  [5.Plotting Graphs](#5)\n    -  [5.1.Plotting Loss vs MiniBatch](#5.3)\n    -  [5.2.Plotting Accuracy vs MiniBatch](#5.4)\n    -  [5.3.Plotting Loss vs Epoch](#5.3)\n    -  [5.4.Plotting Accuracy vs Epoch](#5.4)\n-  [6.Loading and Testing](#6)","91ad585c":"#### 4.Pytorch DataLoaders <a class=\"anchor\" id=\"2.4\"><\/a>","d5fdafd2":"#### 1.Downloading and Extracting Dataset <a class=\"anchor\" id=\"2.1\"><\/a>","180f0f95":"#### 4.Plotting Accuracy vs Epoch<a class=\"anchor\" id=\"5.4\"><\/a>","e7f68192":"### 3.Initializing pre-trained model <a class=\"anchor\" id=\"3\"><\/a>","06fff745":"### 2.Dataset Managament <a class=\"anchor\" id=\"2\"><\/a>","dff9eb9e":"#### 3.Splitting Dataset <a class=\"anchor\" id=\"2.3\"><\/a>","9a8729d6":"#### 2.Plotting Accuracy vs MiniBatch<a class=\"anchor\" id=\"5.2\"><\/a>"}}