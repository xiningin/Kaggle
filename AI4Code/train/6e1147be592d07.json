{"cell_type":{"88209ae6":"code","224affea":"code","67081109":"code","bd175b2e":"code","e13f684b":"code","75e89f18":"code","0ca03544":"code","5350e9c0":"code","3e9d3277":"code","6cd5aaca":"code","f2c8854e":"code","da796783":"code","14b35474":"code","dab0dcf9":"code","fbd227d2":"code","583e4baf":"code","c3c28ff6":"code","401f71c5":"code","3a785dfd":"code","66cffb12":"code","53b00fdd":"code","95cb1692":"code","00a2515a":"code","f784fcb0":"code","376363b3":"code","04baab85":"code","bcdd2b67":"code","34f04ea9":"code","9cac6af3":"code","2498f9e5":"code","c4d2b8c9":"code","2aef8ea6":"code","9fca281a":"code","58318eb6":"code","8bb38b75":"code","af37f89c":"code","145c6398":"code","c0a671bc":"code","3a4fd668":"code","64e19fd5":"code","e958cca5":"code","f9ea938e":"code","0c0333fc":"code","b1683273":"code","dd9e9abe":"code","b2352fb2":"code","fc964785":"code","3af7f09a":"code","3c78afad":"code","19d1f9d8":"code","936a360b":"code","1599b1b4":"code","a8566a3a":"code","b2c0c745":"code","1c9bb61b":"code","50935863":"code","e57ef598":"code","f64a8b70":"code","6a1a90d4":"code","00ebb229":"code","7d2429c0":"code","a7afc882":"code","bb96a114":"code","50e93a5b":"code","1d252815":"code","7b7371cd":"code","c46797b2":"code","05fff7ca":"code","51327a76":"code","bd18a9fd":"code","44891fc9":"code","05e2a265":"code","cb20b91c":"code","9c35ba0b":"code","c1560702":"code","76de649a":"code","e6331f27":"markdown","a8ab76cd":"markdown","603f289f":"markdown","f741f979":"markdown","bed9f91e":"markdown","ee68be0f":"markdown","de8c4ff7":"markdown","dfeb1b4e":"markdown","ef51d5ca":"markdown","e5e0d692":"markdown","cab628ac":"markdown","a440e942":"markdown","f5c8181f":"markdown","400c368f":"markdown","f10e135c":"markdown","edc02a8f":"markdown","1b104b10":"markdown","e49dc917":"markdown","61e4abe2":"markdown","ff6c2d53":"markdown","603a20ec":"markdown","e0460c8e":"markdown","207b41db":"markdown","80d1b8c3":"markdown","c44a3ea7":"markdown","60523730":"markdown","a85d9c33":"markdown","2b72c453":"markdown","b1327806":"markdown","f065c5eb":"markdown","23aec3fd":"markdown","19c35e63":"markdown","3156042a":"markdown","215ec418":"markdown","ebd8dabc":"markdown","89a0a79b":"markdown","ecb29f2c":"markdown","76485ca2":"markdown","c63eea22":"markdown","754c9afd":"markdown","a2ea150b":"markdown","52926f05":"markdown","c3a0c197":"markdown","7f2ed5c1":"markdown","7e17ea9a":"markdown","bb7e53cd":"markdown","0cf3497a":"markdown","549de8a5":"markdown","377d0867":"markdown","8b1235ba":"markdown","5d8ad3e5":"markdown","863e2e6c":"markdown","3a28f85f":"markdown","e97d5653":"markdown","e65cd577":"markdown","f6ed5137":"markdown","1942b9c9":"markdown","9d5c055c":"markdown","744ae378":"markdown","ce4f5b29":"markdown","de9f865a":"markdown","28edbb03":"markdown","eb3345b7":"markdown","05f308ae":"markdown","2de99477":"markdown","d5755f69":"markdown","ec6f38ed":"markdown","36b4214f":"markdown","c892dbc7":"markdown","64a7f62e":"markdown","e56d7a58":"markdown","f980099a":"markdown","9e11263a":"markdown","b225e5ad":"markdown","a8082c7c":"markdown","df041061":"markdown","6c4d5901":"markdown","45265b76":"markdown","450289f8":"markdown","5f2ed0a4":"markdown","770d5b0d":"markdown","83c34eb9":"markdown","e638c101":"markdown","d438ee35":"markdown","a7eaa095":"markdown","9c7780ea":"markdown","0e6871e3":"markdown","eeaf1a44":"markdown","305218a4":"markdown","8af15001":"markdown","70fba1b7":"markdown","f7b1baff":"markdown","f145c5c0":"markdown","182bf148":"markdown","e04c57a3":"markdown","2b784729":"markdown","1d82c0ae":"markdown"},"source":{"88209ae6":"# Necessary librarys\nimport os # it's a operational system library, to set some informations\nimport random # random is to generate random values\n\nimport pandas as pd # to manipulate data frames \nimport numpy as np # to work with matrix\nfrom scipy.stats import kurtosis, skew # it's to explore some statistics of numerical values\n\nimport matplotlib.pyplot as plt # to graphics plot\nimport seaborn as sns # a good library to graphic plots\nimport squarify # to better understand proportion of categorys - it's a treemap layout algorithm\n\n# Importing librarys to use on interactive graphs\nfrom plotly.offline import init_notebook_mode, iplot, plot \nimport plotly.graph_objs as go \n\nimport json # to convert json in df\nfrom pandas.io.json import json_normalize # to normalize the json file\n\n# to set a style to all graphs\nplt.style.use('fivethirtyeight')\ninit_notebook_mode(connected=True)","224affea":"columns = ['device', 'geoNetwork', 'totals', 'trafficSource'] # Columns that have json format\n\ndir_path = \"..\/input\/\" # you can change to your local \n\n# p is a fractional number to skiprows and read just a random sample of the our dataset. \np = 0.07 # *** In this case we will use 50% of data set *** #\n\n#Code to transform the json format columns in table\ndef json_read(df):\n    #joining the [ path + df received]\n    data_frame = dir_path + df\n    \n    #Importing the dataset\n    df = pd.read_csv(data_frame, \n                     converters={column: json.loads for column in columns}, # loading the json columns properly\n                     dtype={'fullVisitorId': 'str'}, # transforming this column to string\n                     skiprows=lambda i: i>0 and random.random() > p)# Number of rows that will be imported randomly\n    \n    for column in columns: #loop to finally transform the columns in data frame\n        #It will normalize and set the json to a table\n        column_as_df = json_normalize(df[column]) \n        # here will be set the name using the category and subcategory of json columns\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns] \n        # after extracting the values, let drop the original columns\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n        \n    # Printing the shape of dataframes that was imported     \n    print(f\"Loaded {os.path.basename(data_frame)}. Shape: {df.shape}\")\n    return df # returning the df after importing and transforming","67081109":"%%time \n# %%time is used to calculate the timing of code chunk execution #\n\n# We will import the data using the name and extension that will be concatenated with dir_path\ndf_train = json_read(\"train.csv\") \n# The same to test dataset\n#df_test = json_read(\"test.csv\") ","bd175b2e":"# This command shows the first 5 rows of our dataset\ndf_train.head()","e13f684b":"# code chunk that I saw in Gabriel Preda kernel\ndef missing_values(data):\n    total = data.isnull().sum().sort_values(ascending = False) # getting the sum of null values and ordering\n    percent = (data.isnull().sum() \/ data.isnull().count() * 100 ).sort_values(ascending = False) #getting the percent and order of null\n    df = pd.concat([total, percent], axis=1, keys=['Total', 'Percent']) # Concatenating the total and percent\n    print(\"Total columns at least one Values: \")\n    print (df[~(df['Total'] == 0)]) # Returning values of nulls different of 0\n    \n    print(\"\\n Total of Sales % of Total: \", round((df_train[df_train['totals.transactionRevenue'] != np.nan]['totals.transactionRevenue'].count() \/ len(df_train['totals.transactionRevenue']) * 100),4))\n    \n    return ","75e89f18":"# calling the missing values function\nmissing_values(df_train) ","0ca03544":"print(df_train.info())","5350e9c0":"# library of datetime\nfrom datetime import datetime\n\n# This function is to extract date features\ndef date_process(df):\n    df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y%m%d\") # seting the column as pandas datetime\n    df[\"_weekday\"] = df['date'].dt.weekday #extracting week day\n    df[\"_day\"] = df['date'].dt.day # extracting day\n    df[\"_month\"] = df['date'].dt.month # extracting day\n    df[\"_year\"] = df['date'].dt.year # extracting day\n    df['_visitHour'] = (df['visitStartTime'].apply(lambda x: str(datetime.fromtimestamp(x).hour))).astype(int)\n    \n    return df #returning the df after the transformations","3e9d3277":"### Calling the function ","6cd5aaca":"df_train = date_process(df_train) #calling the function that we created above\n\ndf_train.head(n=2) #printing the first 2 rows of our dataset","f2c8854e":"def FillingNaValues(df):    # fillna numeric feature\n    df['totals.pageviews'].fillna(1, inplace=True).astype(int) #filling NA's with 1\n    df['totals.newVisits'].fillna(0, inplace=True).astype(int) #filling NA's with 0\n    df['totals.bounces'].fillna(0, inplace=True).astype(int)   #filling NA's with 0\n    df[\"totals.transactionRevenue\"] = df[\"totals.transactionRevenue\"].fillna(0.0).astype(float) #filling NA with zero\n    df['trafficSource.isTrueDirect'].fillna(False, inplace=True) # filling boolean with False\n    df['trafficSource.adwordsClickInfo.isVideoAd'].fillna(True, inplace=True) # filling boolean with True\n    df_train.loc[df_train['geoNetwork.city'] == \"(not set)\", 'geoNetwork.city'] = np.nan\n    df_train['geoNetwork.city'].fillna(\"NaN\", inplace=True)\n\n    return df #return the transformed dataframe","da796783":"def NumericalColumns(df):    # fillna numeric feature\n    df['totals.pageviews'].fillna(1, inplace=True) #filling NA's with 1\n    df['totals.newVisits'].fillna(0, inplace=True) #filling NA's with 0\n    df['totals.bounces'].fillna(0, inplace=True)   #filling NA's with 0\n    df['trafficSource.isTrueDirect'].fillna(False, inplace=True) # filling boolean with False\n    df['trafficSource.adwordsClickInfo.isVideoAd'].fillna(True, inplace=True) # filling boolean with True\n    df[\"totals.transactionRevenue\"] = df[\"totals.transactionRevenue\"].fillna(0.0).astype(float) #filling NA with zero\n    df['totals.pageviews'] = df['totals.pageviews'].astype(int) # setting numerical column as integer\n    df['totals.newVisits'] = df['totals.newVisits'].astype(int) # setting numerical column as integer\n    df['totals.bounces'] = df['totals.bounces'].astype(int)  # setting numerical column as integer\n    df[\"totals.hits\"] = df[\"totals.hits\"].astype(float) # setting numerical to float\n    df['totals.visits'] = df['totals.visits'].astype(int) # seting as int\n\n    return df #return the transformed dataframe","14b35474":"from sklearn import preprocessing\n\ndef Normalizing(df):\n    # Use MinMaxScaler to normalize the column\n    df[\"totals.hits\"] =  (df['totals.hits'] - min(df['totals.hits'])) \/ (max(df['totals.hits'])  - min(df['totals.hits']))\n    # normalizing the transaction Revenue\n    df['totals.transactionRevenue'] = df_train['totals.transactionRevenue'].apply(lambda x: np.log1p(x))\n    # return the modified df\n    return df ","dab0dcf9":"# We will takeoff all columns where we have a unique value (constants)\n# It is useful because this columns don't give us none information\ndiscovering_consts = [col for col in df_train.columns if df_train[col].nunique() == 1]\n\n# printing the total of columns dropped and the name of columns \nprint(\"Columns with just one value: \", len(discovering_consts), \"columns\")\nprint(\"Name of constant columns: \\n\", discovering_consts)","fbd227d2":"#Here are all columns that the unique value is 'not available in demo dataset'\n\nnot_aval_cols = ['socialEngagementType','device.browserSize','device.browserVersion', 'device.flashVersion', \n                 'device.language' ,'device.mobileDeviceBranding', 'device.mobileDeviceInfo','device.mobileDeviceMarketingName',\n                 'device.mobileDeviceModel', 'device.mobileInputSelector' , 'device.operatingSystemVersion','device.screenColors',\n                 'device.screenResolution', 'geoNetwork.cityId', 'geoNetwork.latitude' ,'geoNetwork.longitude',\n                 'geoNetwork.networkLocation','trafficSource.adwordsClickInfo.criteriaParameters']","583e4baf":"# seting the function to show \ndef knowningData(df, data_type=object, limit=3): #seting the function with df, \n    n = df.select_dtypes(include=data_type) #selecting the desired data type\n    for column in n.columns: #initializing the loop\n        print(\"##############################################\")\n        print(\"Name of column \", column, ': \\n', \"Uniques: \", df[column].unique()[:limit], \"\\n\",\n              \" | ## Total nulls: \", (round(df[column].isnull().sum() \/ len(df[column]) * 100,2)),\n              \" | ## Total unique values: \", df_train.nunique()[column]) #print the data and % of nulls)\n        # print(\"Percentual of top 3 of: \", column)\n        # print(round(df[column].value_counts()[:3] \/ df[column].value_counts().sum() * 100,2))\n        print(\"#############################################\")","c3c28ff6":"# calling our function: object is default\nknowningData(df_train)","401f71c5":"knowningData(df_train, data_type=int)","3a785dfd":"knowningData(df_train, data_type=float)","66cffb12":"to_drop = [\"socialEngagementType\",'device.browserVersion', 'device.browserSize', 'device.flashVersion', 'device.language', \n           'device.mobileDeviceBranding', 'device.mobileDeviceInfo', 'device.mobileDeviceMarketingName', 'device.mobileDeviceModel',\n           'device.mobileInputSelector', 'device.operatingSystemVersion', 'device.screenColors', 'device.screenResolution', \n           'geoNetwork.cityId', 'geoNetwork.latitude', 'geoNetwork.longitude','geoNetwork.networkLocation', \n           'trafficSource.adwordsClickInfo.criteriaParameters', 'trafficSource.adwordsClickInfo.gclId', 'trafficSource.campaign',\n           'trafficSource.adwordsClickInfo.page', 'trafficSource.referralPath', 'trafficSource.adwordsClickInfo.slot',\n           'trafficSource.adContent', 'trafficSource.keyword']\n\n\n","53b00fdd":"df_train.drop(to_drop, axis=1, inplace=True)","95cb1692":"print(\"Total features dropped: \", len(to_drop))\nprint(\"Shape after dropping: \", df_train.shape)","00a2515a":"# call the function to transform the numerical columns\ndf_train = NumericalColumns(df_train)\n\n# Call the function that will normalize some features\ndf_train = Normalizing(df_train)","f784fcb0":"# We will takeoff all columns where we have a unique value\n# It is useful because this columns don't give us none information\nclean_consts = [col for col in df_train.columns if df_train[col].nunique() == 1]\n\n\n# this function drop all constant columns, inplacing the data \ndf_train.drop('trafficSource.adwordsClickInfo.adNetworkType', axis=1, inplace=True) \n\n# printing the total of columns dropped and the name of columns \nprint(\"This useful action will drop: \", len(clean_consts), \"columns\")\nprint(\"All dropped columns: \\n\", clean_consts)","376363b3":"df_train.nunique()","04baab85":"'trafficSource.adwordsClickInfo.adNetworkType'","bcdd2b67":"dummy_feaures =['channelGrouping', 'device.browser', 'device.deviceCategory', 'geoNetwork.city', 'device.operatingSystem', \n                'trafficSource.medium', 'trafficSource.source',\n                'geoNetwork.continent', 'geoNetwork.country', 'geoNetwork.metro', 'geoNetwork.networkDomain', 'geoNetwork.region', \n                'geoNetwork.subContinent']\n\n\nnumericals = ['totals.visits', '_visitHour', '_day', '_month', '_weekday']","34f04ea9":"# Printing some statistics of our data\nprint(\"Transaction Revenue Min Value: \", \n      df_train[df_train['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"].min()) # printing the min value\nprint(\"Transaction Revenue Mean Value: \", \n      df_train[df_train['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"].mean()) # mean value\nprint(\"Transaction Revenue Median Value: \", \n      df_train[df_train['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"].median()) # median value\nprint(\"Transaction Revenue Max Value: \", \n      df_train[df_train['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"].max()) # the max value\n\n# It I did to plot the quantiles but are not working\n#print(round(df_train['totals.transactionRevenue'].quantile([.025,.25,.5,.75,.975]),2))\n\n# seting the figure size of our plots\nplt.figure(figsize=(14,5))\n\n# Subplot allow us to plot more than one \n# in this case, will be create a subplot grid of 2 x 1\nplt.subplot(1,2,1)\n# seting the distribuition of our data and normalizing using np.log on values highest than 0 and + \n# also, we will set the number of bins and if we want or not kde on our histogram\nax = sns.distplot(np.log(df_train[df_train['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"] + 0.01), bins=40, kde=True)\nax.set_xlabel('Transaction RevenueLog', fontsize=15) #seting the xlabel and size of font\nax.set_ylabel('Distribuition', fontsize=15) #seting the ylabel and size of font\nax.set_title(\"Distribuition of Revenue Log\", fontsize=20) #seting the title and size of font\n\n# setting the second plot of our grid of graphs\nplt.subplot(1,2,2)\n# ordering the total of users and seting the values of transactions to understanding \nplt.scatter(range(df_train.shape[0]), np.sort(df_train['totals.transactionRevenue'].values))\nplt.xlabel('Index', fontsize=15) # xlabel and size of words\nplt.ylabel('Revenue value', fontsize=15) # ylabel and size of words\nplt.title(\"Revenue Value Distribution\", fontsize=20) # Setting Title and fontsize\n\nplt.show()","9cac6af3":"print('Excess kurtosis of normal distribution (should be 0): {}'.format(\n    kurtosis(df_train[df_train['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"])))\nprint( 'Skewness of normal distribution (should be 0): {}'.format(\n    skew((df_train[df_train['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"]))))","2498f9e5":"def CalcOutliers(df_num): \n    '''\n    \n    Leonardo Ferreira 20\/10\/2018\n    Set a numerical value and it will calculate the upper, lower and total number of outliers\n    It will print a lot of statistics of the numerical feature that you set on input\n    \n    '''\n    # calculating mean and std of the array\n    data_mean, data_std = np.mean(df_num), np.std(df_num)\n\n    # seting the cut line to both higher and lower values\n    # You can change this value\n    cut = data_std * 3\n\n    #Calculating the higher and lower cut values\n    lower, upper = data_mean - cut, data_mean + cut\n\n    # creating an array of lower, higher and total outlier values \n    outliers_lower = [x for x in df_num if x < lower]\n    outliers_higher = [x for x in df_num if x > upper]\n    outliers_total = [x for x in df_num if x < lower or x > upper]\n\n    # array without outlier values\n    outliers_removed = [x for x in df_num if x > lower and x < upper]\n    \n    print('Identified lowest outliers: %d' % len(outliers_lower)) # printing total number of values in lower cut of outliers\n    print('Identified upper outliers: %d' % len(outliers_higher)) # printing total number of values in higher cut of outliers\n    print('Identified outliers: %d' % len(outliers_total)) # printing total number of values outliers of both sides\n    print('Non-outlier observations: %d' % len(outliers_removed)) # printing total number of non outlier values\n    print(\"Total percentual of Outliers: \", round((len(outliers_total) \/ len(outliers_removed) )*100, 4)) # Percentual of outliers in points\n    \n    return\n","c4d2b8c9":"CalcOutliers(df_train['totals.transactionRevenue']) # Call ","2aef8ea6":"CalcOutliers(df_train['totals.pageviews']) # Call ","9fca281a":"# the top 10 of browsers represent % of total\nprint(\"Percentual of Browser usage: \")\nprint(df_train['device.browser'].value_counts()[:7] ) # printing the top 7 percentage of browsers\n\n# seting the graph size\nplt.figure(figsize=(14,6))\n\n# Let explore the browser used by users\nsns.countplot(df_train[df_train['device.browser']\\\n                       .isin(df_train['device.browser']\\\n                             .value_counts()[:10].index.values)]['device.browser'], palette=\"hls\") # It's a module to count the category's\nplt.title(\"TOP 10 Most Frequent Browsers\", fontsize=20) # Adding Title and seting the size\nplt.xlabel(\"Browser Names\", fontsize=16) # Adding x label and seting the size\nplt.ylabel(\"Count\", fontsize=16) # Adding y label and seting the size\nplt.xticks(rotation=45) # Adjust the xticks, rotating the labels\n\nplt.show() #use plt.show to render the graph that we did above","58318eb6":"plt.figure(figsize=(13,6)) #figure size\n\n#It's another way to plot our data. using a variable that contains the plot parameters\ng1 = sns.boxenplot(x='device.browser', y='totals.transactionRevenue', \n                   data=df_train[(df_train['device.browser'].isin((df_train['device.browser'].value_counts()[:10].index.values))) &\n                                  df_train['totals.transactionRevenue'] > 0])\ng1.set_title('Browsers Name by Transactions Revenue', fontsize=20) # title and fontsize\ng1.set_xticklabels(g1.get_xticklabels(),rotation=45) # It's the way to rotate the xticks when we use variable to our graphs\ng1.set_xlabel('Device Names', fontsize=18) # Xlabel\ng1.set_ylabel('Trans Revenue(log) Dist', fontsize=18) #Ylabel\n\nplt.show()","8bb38b75":"# the top 10 of browsers represent % of total\nprint(\"Percentual of Channel Grouping used: \")\nprint((df_train['channelGrouping'].value_counts()[:5])) # printing the top 7 percentage of browsers\n\n# seting the graph size\nplt.figure(figsize=(14,7))\n\n# let explore the browser used by users\nsns.countplot(df_train[\"channelGrouping\"], palette=\"hls\") # It's a module to count the category's\nplt.title(\"Channel Grouping Count\", fontsize=20) # seting the title size\nplt.xlabel(\"Channel Grouping Name\", fontsize=18) # seting the x label size\nplt.ylabel(\"Count\", fontsize=18) # seting the y label size\n\nplt.show() #use plt.show to render the graph that we did above","af37f89c":"## I will use the crosstab to explore two categorical values\n\n# At index I will use set my variable that I want analyse and cross by another\ncrosstab_eda = pd.crosstab(index=df_train['channelGrouping'], normalize=True,\n                           # at this line, I am using the isin to select just the top 5 of browsers\n                           columns=df_train[df_train['device.browser'].isin(df_train['device.browser']\\\n                                                                            .value_counts()[:5].index.values)]['device.browser'])\n# Ploting the crosstab that we did above\ncrosstab_eda.plot(kind=\"bar\",    # select the bar to plot the count of categoricals\n                 figsize=(14,7), # adjusting the size of graphs\n                 stacked=True)   # code to unstack \nplt.title(\"Channel Grouping % for which Browser\", fontsize=20) # seting the title size\nplt.xlabel(\"The Channel Grouping Name\", fontsize=18) # seting the x label size\nplt.ylabel(\"Count\", fontsize=18) # seting the y label size\nplt.xticks(rotation=0)\nplt.show() # rendering","145c6398":"# the top 5 of browsers represent % of total\nprint(\"Percentual of Operational System: \")\nprint(df_train['device.operatingSystem'].value_counts()[:5]) # printing the top 7 percentage of browsers\n\n# seting the graph size\nplt.figure(figsize=(14,7))\n\n# let explore the browser used by users\nsns.countplot(df_train[\"device.operatingSystem\"], palette=\"hls\") # It's a module to count the category's\nplt.title(\"Operational System used Count\", fontsize=20) # seting the title size\nplt.xlabel(\"Operational System Name\", fontsize=16) # seting the x label size\nplt.ylabel(\"OS Count\", fontsize=16) # seting the y label size\nplt.xticks(rotation=45) # Adjust the xticks, rotating the labels\n\nplt.show() #use plt.show to render the graph that we did above","c0a671bc":"\n# At index I will use isin to substitute the loop and get just the values with more than 1%\ncrosstab_eda = pd.crosstab(index=df_train[df_train['device.operatingSystem']\\\n                                          .isin(df_train['device.operatingSystem']\\\n                                                .value_counts()[:6].index.values)]['device.operatingSystem'], \n                           \n                           # at this line, I am using the isin to select just the top 5 of browsers\n                           columns=df_train[df_train['device.browser'].isin(df_train['device.browser']\\\n                                                                            .value_counts()[:5].index.values)]['device.browser'])\n# Ploting the crosstab that we did above\ncrosstab_eda.plot(kind=\"bar\",    # select the bar to plot the count of categoricals\n                 figsize=(14,7), # adjusting the size of graphs\n                 stacked=True)   # code to unstack \nplt.title(\"Most frequent OS's by Browsers of users\", fontsize=22) # adjusting title and fontsize\nplt.xlabel(\"Operational System Name\", fontsize=19) # adjusting x label and fontsize\nplt.ylabel(\"Count OS\", fontsize=19) # adjusting y label and fontsize\nplt.xticks(rotation=0) # Adjust the xticks, rotating the labels\n\nplt.show() # rendering","3a4fd668":"(sns.FacetGrid(df_train[(df_train['device.operatingSystem']\\\n                        .isin(df_train['device.operatingSystem']\\\n                              .value_counts()[:6].index.values)) & df_train['totals.transactionRevenue'] > 0],\n               hue='device.operatingSystem', height=5, aspect=2)\n  .map(sns.kdeplot, 'totals.transactionRevenue', shade=True)\n .add_legend()\n)\nplt.show()","64e19fd5":"# the top 5 of browsers represent % of total\nprint(\"Percentual of Operational System: \")\nprint(round(df_train['device.deviceCategory'].value_counts() \/ len(df_train['device.deviceCategory']) * 100, 2)) # printing the top 7 percentage of browsers\n\n# seting the graph size\nplt.figure(figsize=(14,5))\n\nplt.subplot(1,2,1)\n# let explore the browser used by users\nsns.countplot(df_train[\"device.deviceCategory\"], palette=\"hls\") # It's a module to count the category's\nplt.title(\"Device Category Count\", fontsize=20) # seting the title size\nplt.xlabel(\"Device Category\", fontsize=18) # seting the x label size\nplt.ylabel(\"Count\", fontsize=16) # seting the y label size\nplt.xticks(fontsize=18) # Adjust the xticks, rotating the labels\n\nplt.subplot(1,2,2)\nsns.boxenplot(x=\"device.deviceCategory\", y = 'totals.transactionRevenue', \n              data=df_train[df_train['totals.transactionRevenue'] > 0], palette=\"hls\") # It's a module to count the category's\nplt.title(\"Device Category Revenue Distribuition\", fontsize=20) # seting the title size\nplt.xlabel(\"Device Category\", fontsize=18) # seting the x label size\nplt.ylabel(\"Revenue(Log)\", fontsize=16) # seting the y label size\nplt.xticks(fontsize=18) # Adjust the xticks, rotating the labels\n\nplt.subplots_adjust(hspace = 0.9, wspace = 0.5)\n\nplt.show() #use plt.show to render the graph that we did above","e958cca5":"(sns.FacetGrid(df_train[df_train['totals.transactionRevenue'] > 0],\n               hue='device.deviceCategory', height=5, aspect=2)\n  .map(sns.kdeplot, 'totals.transactionRevenue', shade=True)\n .add_legend()\n)\nplt.show()","f9ea938e":"# At index I will use isin to substitute the loop and get just the values with more than 1%\ncrosstab_eda = pd.crosstab(index=df_train['device.deviceCategory'], # at this line, I am using the isin to select just the top 5 of browsers\n                           columns=df_train[df_train['device.operatingSystem']\\\n                                            .isin(df_train['device.operatingSystem']\\\n                                                  .value_counts()[:6].index.values)]['device.operatingSystem'])\n# Ploting the crosstab that we did above\ncrosstab_eda.plot(kind=\"bar\",    # select the bar to plot the count of categoricals\n                 figsize=(14,7), # adjusting the size of graphs\n                 stacked=True)   # code to unstack \nplt.title(\"Most frequent OS's by Device Categorys of users\", fontsize=22) # adjusting title and fontsize\nplt.xlabel(\"Device Name\", fontsize=19)                # adjusting x label and fontsize\nplt.ylabel(\"Count Device x OS\", fontsize=19)                               # adjusting y label and fontsize\nplt.xticks(rotation=0)                                            # Adjust the xticks, rotating the labels\n\n\nplt.show() # rendering","0c0333fc":"# the top 8 of browsers represent % of total\nprint(\"Description of SubContinent count: \")\nprint(df_train['geoNetwork.subContinent'].value_counts()[:8]) # printing the top 7 percentage of browsers\n\n# seting the graph size\nplt.figure(figsize=(16,7))\n\n# let explore the browser used by users\nsns.countplot(df_train[df_train['geoNetwork.subContinent']\\\n                       .isin(df_train['geoNetwork.subContinent']\\\n                             .value_counts()[:15].index.values)]['geoNetwork.subContinent'], palette=\"hls\") # It's a module to count the category's\nplt.title(\"TOP 15 most frequent SubContinents\", fontsize=20) # seting the title size\nplt.xlabel(\"subContinent Names\", fontsize=18) # seting the x label size\nplt.ylabel(\"SubContinent Count\", fontsize=18) # seting the y label size\nplt.xticks(rotation=45) # Adjust the xticks, rotating the labels\n\nplt.show() #use plt.show to render the graph that we did above","b1683273":"## I will use the crosstab to explore two categorical values\n\n# At index I will use isin to substitute the loop and get just the values with more than 1%\ncrosstab_eda = pd.crosstab(index=df_train[df_train['geoNetwork.subContinent']\\\n                                          .isin(df_train['geoNetwork.subContinent']\\\n                                                .value_counts()[:10].index.values)]['geoNetwork.subContinent'], \n                           \n                           # at this line, I am using the isin to select just the top 5 of browsers\n                           columns=df_train[df_train['device.browser'].isin(df_train['device.browser']\\\n                                                                            .value_counts()[:5].index.values)]['device.browser'])\n# Ploting the crosstab that we did above\ncrosstab_eda.plot(kind=\"bar\",    # select the bar to plot the count of categoricals\n                 figsize=(16,7), # adjusting the size of graphs\n                 stacked=True)   # code to unstack \nplt.title(\"TOP 10 Most frequent Subcontinents by Browsers used\", fontsize=22) # adjusting title and fontsize\nplt.xlabel(\"Subcontinent Name\", fontsize=19) # adjusting x label and fontsize\nplt.ylabel(\"Count Subcontinent\", fontsize=19) # adjusting y label and fontsize\nplt.xticks(rotation=45) # Adjust the xticks, rotating the labels\nplt.legend(loc=1, prop={'size': 12}) # to \n\nplt.show() # rendering","dd9e9abe":"print('train date:', min(df_train['date']), 'to', max(df_train['date']))","b2352fb2":"year = df_train['_year'].value_counts()         # counting the Year with value counts\nmonth = df_train['_month'].value_counts()      # coutning months\nweeday = df_train['_weekday'].value_counts()    # Couting weekday\nday = df_train['_day'].value_counts()              # counting Day\ndate = df_train['date'].value_counts()           # Counting date","fc964785":"# I saw and take a lot of inspiration to this interactive plots in kernel: \n# https:\/\/www.kaggle.com\/jsaguiar\/complete-exploratory-analysis-all-columns\n# I learned a lot in this kernel and I will implement and adapted some ideas\n\n#seting some static color options\ncolor_op = ['#5527A0', '#BB93D7', '#834CF7', '#6C941E', '#93EAEA', '#7425FF', '#F2098A', '#7E87AC', \n            '#EBE36F', '#7FD394', '#49C35D', '#3058EE', '#44FDCF', '#A38F85', '#C4CEE0', '#B63A05', \n            '#4856BF', '#F0DB1B', '#9FDBD9', '#B123AC']\n\n# Visits by time train\n\n# couting all entries by date to get number of visits by each date\ndates_temp = df_train['date'].value_counts().to_frame().reset_index().sort_values('index') \n# renaming the columns to apropriate names\ndates_temp = dates_temp.rename(columns = {\"date\" : \"visits\"}).rename(columns = {\"index\" : \"date\"})\n\n# creating the first trace with the necessary parameters\ntrace = go.Scatter(x=dates_temp.date.astype(str), y=dates_temp.visits,\n                    opacity = 0.8, line = dict(color = color_op[3]), name= 'Visits by day')\n\n# Below we will get the total values by Transaction Revenue Log by date\ndates_temp_sum = df_train.groupby('date')['totals.transactionRevenue'].sum().to_frame().reset_index()\n\n# using the new dates_temp_sum we will create the second trace\ntrace1 = go.Scatter(x=dates_temp_sum.date.astype(str), line = dict(color = color_op[1]), name=\"RevenueLog by day\",\n                        y=dates_temp_sum['totals.transactionRevenue'], opacity = 0.8)\n\n# Getting the total values by Transactions by each date\ndates_temp_count = df_train[df_train['totals.transactionRevenue'] > 0].groupby('date')['totals.transactionRevenue'].count().to_frame().reset_index()\n\n# using the new dates_temp_count we will create the third trace\ntrace2 = go.Scatter(x=dates_temp_count.date.astype(str), line = dict(color = color_op[5]), name=\"Sellings by day\",\n                        y=dates_temp_count['totals.transactionRevenue'], opacity = 0.8)\n\n#creating the layout the will allow us to give an title and \n# give us some interesting options to handle with the outputs of graphs\nlayout = dict(\n    title= \"Informations by Date\",\n    xaxis=dict(\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1, label='1m', step='month', stepmode='backward'),\n                dict(count=3, label='3m', step='month', stepmode='backward'),\n                dict(count=6, label='6m', step='month', stepmode='backward'),\n                dict(step='all')\n            ])\n        ),\n        rangeslider=dict(visible = True),\n        type='date'\n    )\n)\n\n# creating figure with the both traces and layout\nfig = dict(data= [trace, trace1, trace2], layout=layout)\n\n#rendering the graphs\niplot(fig) #it's an equivalent to plt.show()","3af7f09a":"# Setting the first trace\ntrace1 = go.Histogram(x=df_train[\"_year\"],\n                      name='Year Count')\n\n# Setting the second trace\ntrace2 = go.Histogram(x=df_train[\"_month\"],\n                name='Month Count')\n\n# Setting the third trace\ntrace3 = go.Bar(y=day.values,\n                x=day.index.values, \n                name='Day Count')\n\n# Setting the fourth trace\ntrace4 = go.Bar(y=weeday.values,\n                x=weeday.index.values,\n                name='Weekday Count')\n\n# puting all traces in the same \"array of graphics\" to we render it below\ndata = [trace1, trace2, trace4, trace3]\n\n#Creating the options to be posible we use in our \nupdatemenus = list([\n    dict(active=-1,\n         x=-0.15,\n         buttons=list([  \n             dict(\n                 label = 'Years Count',\n                 method = 'update',\n                 args = [{'visible': [True, False, False, False,False]}, \n                         {'title': 'Count of Year'}]),\n             dict(\n                 label = 'Months Count',\n                 method = 'update',\n                 args = [{'visible': [False, True, False, False,False]},\n                         {'title': 'Count of Months'}]),\n             dict(\n                 label = 'WeekDays Count',\n                 method = 'update',\n                 args = [{'visible': [False, False, True, False, False]},\n                         {'title': 'Count of WeekDays'}]),\n            dict(\n                label = 'Days Count ',\n                method = 'update',\n                args = [{'visible': [False, False, False, True,False]},\n                        {'title': 'Count of Day'}]) ])\n    )\n])\n\n\nlayout = dict(title='The percentual Distribuitions of Date Features (Select from Dropdown)',\n              showlegend=False,\n              updatemenus=updatemenus,\n#              xaxis = dict(\n#                  type=\"category\"\n#                      ),\n              barmode=\"group\"\n             )\nfig = dict(data=data, layout=layout)\nprint(\"SELECT BELOW: \")\niplot(fig)","3c78afad":"date_sales = ['_visitHour', '_weekday'] #seting the desired \n\ncm = sns.light_palette(\"green\", as_cmap=True)\npd.crosstab(df_train[date_sales[0]], df_train[date_sales[1]], \n            values=df_train[\"totals.transactionRevenue\"], aggfunc=[np.sum]).style.background_gradient(cmap = cm)\n\n# tab.columns.levels[1] = [\"Sun\", \"Mon\", \"Thu\", \"wed\", \"Thi\",\"Fri\",\"Sat\"]","19d1f9d8":"number_of_colors = 20 # total number of different collors that we will use\n\n# Here I will generate a bunch of hexadecimal colors \ncolor = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n             for i in range(number_of_colors)]","936a360b":"country_tree = df_train[\"geoNetwork.country\"].value_counts() #counting the values of Country\n\nprint(\"Description most frequent countrys: \")\nprint(country_tree[:15]) #printing the 15 top most \n\ncountry_tree = round((df_train[\"geoNetwork.country\"].value_counts()[:30] \\\n                       \/ len(df_train['geoNetwork.country']) * 100),2)\n\nplt.figure(figsize=(14,5))\ng = squarify.plot(sizes=country_tree.values, label=country_tree.index, \n                  value=country_tree.values,\n                  alpha=.4, color=color)\ng.set_title(\"'TOP 30 Countrys - % size of total\",fontsize=20)\ng.set_axis_off()\nplt.show()","1599b1b4":"df_train.loc[df_train[\"geoNetwork.city\"] == \"not available in demo dataset\", 'geoNetwork.city'] = np.nan","a8566a3a":"city_tree = df_train[\"geoNetwork.city\"].value_counts() #counting \n\nprint(\"Description most frequent Citys: \" )\nprint(city_tree[:15])\n\ncity_tree = round((city_tree[:30] \/ len(df_train['geoNetwork.city']) * 100),2)\n\nplt.figure(figsize=(14,5))\ng = squarify.plot(sizes=city_tree.values, label=city_tree.index, \n                  value=city_tree.values,\n                  alpha=.4, color=color)\ng.set_title(\"'TOP 30 Citys - % size of total\",fontsize=20)\ng.set_axis_off()\nplt.show()","b2c0c745":"def PieChart(df_colum, title, limit=15):\n    \"\"\"\n    This function helps to investigate the proportion of visits and total of transction revenue \n    by each category\n    \"\"\"\n\n    count_trace = df_train[df_colum].value_counts()[:limit].to_frame().reset_index()\n    rev_trace = df_train.groupby(df_colum)[\"totals.transactionRevenue\"].sum().nlargest(10).to_frame().reset_index()\n\n    trace1 = go.Pie(labels=count_trace['index'], values=count_trace[df_colum], name= \"% Acesses\", hole= .5, \n                    hoverinfo=\"label+percent+name\", showlegend=True,domain= {'x': [0, .48]}, \n                    marker=dict(colors=color))\n\n    trace2 = go.Pie(labels=rev_trace[df_colum], \n                    values=rev_trace['totals.transactionRevenue'], name=\"% Revenue\", hole= .5, \n                    hoverinfo=\"label+percent+name\", showlegend=False, domain= {'x': [.52, 1]})\n\n    layout = dict(title= title, height=450, font=dict(size=15),\n                  annotations = [\n                      dict(\n                          x=.25, y=.5,\n                          text='Visits', \n                          showarrow=False,\n                          font=dict(size=20)\n                      ),\n                      dict(\n                          x=.80, y=.5,\n                          text='Revenue', \n                          showarrow=False,\n                          font=dict(size=20)\n                      )\n        ])\n\n    fig = dict(data=[trace1, trace2], layout=layout)\n    iplot(fig)","1c9bb61b":"PieChart(\"device.deviceCategory\", \"Device Category\")","50935863":"# call the function\nPieChart(\"geoNetwork.city\", \"Top Cities by Accesses and Revenue\", limit=12)","e57ef598":"PieChart(\"channelGrouping\", \"Channel Grouping Visits and Revenues\")","f64a8b70":"PieChart('geoNetwork.networkDomain', \"Network Domain\")","6a1a90d4":"PieChart(\"device.deviceCategory\", \"Device Category\")","00ebb229":"PieChart(\"trafficSource.medium\", \"Trafic Source - Medium\")","7d2429c0":"PieChart('trafficSource.source', \"Visits and Revenue by TOP Sources\", limit=8)","a7afc882":"df_train.corr()['totals.transactionRevenue']","bb96a114":"country_repayment = ['channelGrouping', '_weekday'] #seting the desired \n\ncm = sns.light_palette(\"green\", as_cmap=True)\npd.crosstab(df_train[country_repayment[0]], df_train[country_repayment[1]], \n            values=df_train[\"totals.transactionRevenue\"], aggfunc=[np.sum]).style.background_gradient(cmap = cm)\n\n# tab.columns.levels[1] = [\"Sun\", \"Mon\", \"Thu\", \"wed\", \"Thi\",\"Fri\",\"Sat\"]","50e93a5b":"# Counting total visits by countrys\ncountMaps = pd.DataFrame(df_train['geoNetwork.country'].value_counts()).reset_index()\ncountMaps.columns=['country', 'counts'] #renaming columns\ncountMaps = countMaps.reset_index().drop('index', axis=1) #reseting index and droping the column\n\ndata = [ dict(\n        type = 'choropleth',\n        locations = countMaps['country'],\n        locationmode = 'country names',\n        z = countMaps['counts'],\n        text = countMaps['country'],\n        autocolorscale = False,\n        marker = dict(\n            line = dict (\n                color = 'rgb(180,180,180)',\n                width = 0.5\n            ) ),\n        colorbar = dict(\n            autotick = False,\n            tickprefix = '',\n            title = 'Number of Visits'),\n      ) ]\n\nlayout = dict(\n    title = 'Couting Visits Per Country',\n    geo = dict(\n        showframe = False,\n        showcoastlines = True,\n        projection = dict(\n            type = 'Mercator'\n        )\n    )\n)\n\nfigure = dict( data=data, layout=layout )\niplot(figure, validate=False, filename='map-countrys-count')","1d252815":"# I will crete a variable of Revenues by country sum\nsumRevMaps = df_train[df_train['totals.transactionRevenue'] > 0].groupby(\"geoNetwork.country\")[\"totals.transactionRevenue\"].count().to_frame().reset_index()\nsumRevMaps.columns = [\"country\", \"count_sales\"] # renaming columns\nsumRevMaps = sumRevMaps.reset_index().drop('index', axis=1) #reseting index and drop index column\n\ndata = [ dict(\n        type = 'choropleth',\n        locations = sumRevMaps['country'],\n        locationmode = 'country names',\n        z = sumRevMaps['count_sales'],\n        text = sumRevMaps['country'],\n        autocolorscale = False,\n        marker = dict(\n            line = dict (\n                color = 'rgb(180,180,180)',\n                width = 0.5\n            ) ),\n        colorbar = dict(\n            autotick = False,\n            tickprefix = '',\n            title = 'Count of Sales'),\n      ) ]\n\nlayout = dict(\n    title = 'Total Sales by Country',\n    geo = dict(\n        showframe = False,\n        showcoastlines = True,\n        projection = dict(\n            type = 'Mercator'\n        )\n    )\n)\n\nfigure = dict( data=data, layout=layout )\n\niplot(figure, validate=False, filename='map-countrys-total')","7b7371cd":"df_train['month_unique_user_count'] = df_train.groupby('_month')['fullVisitorId'].transform('nunique')\ndf_train['day_unique_user_count'] = df_train.groupby('_day')['fullVisitorId'].transform('nunique')\ndf_train['weekday_unique_user_count'] = df_train.groupby('_weekday')['fullVisitorId'].transform('nunique')\n\n\ndf_train['traf_sourc_browser_count'] = df_train.groupby(['trafficSource.medium', 'device.browser'])['totals.pageviews'].transform('nunique')\ndf_train['Id_browser_pageviews_sumprod'] = df_train.groupby(['fullVisitorId', 'device.browser'])['totals.pageviews'].transform('cumprod')\ndf_train['Id_browser_hits_sumprod'] = df_train.groupby(['fullVisitorId', 'device.browser'])['totals.hits'].transform('cumprod')\ndf_train['Id_browser_hits_sumprod'] = df_train.groupby(['fullVisitorId', 'device.browser'])['totals.hits'].transform('cumprod')\ndf_train['Id_browser_hits_sumprod_mob'] = df_train.groupby(['fullVisitorId', 'device.browser', 'device.isMobile'])['totals.hits'].transform('sum')\n\ndf_train['Id_networkDomain_hits'] = df_train.groupby(['fullVisitorId', 'geoNetwork.networkDomain'])['totals.hits'].transform('var')\ndf_train['Id_networkDomain_country_hits'] = df_train.groupby(['fullVisitorId', 'geoNetwork.networkDomain', 'geoNetwork.country'])['totals.hits'].transform('unique')\n\n","c46797b2":"df_train[[\"totals.transactionRevenue\", 'Id_browser_hits_sumprod', 'Id_networkDomain_hits','Id_networkDomain_country_hits', 'Id_browser_hits_sumprod_mob']].corr()","05fff7ca":"aggs = {\n    'date': ['min', 'max'],\n    'totals.hits': ['sum', 'min', 'max', 'mean', 'median'],\n    'totals.pageviews': ['sum', 'min', 'max', 'mean', 'median'],\n    'totals.bounces': ['sum', 'mean', 'median'],\n    'totals.newVisits': ['sum', 'mean', 'median']\n}\n\n# Previous applications categorical features\ncat_aggregations = {}\n\nfor cat in dummy_feaures:\n    cat_aggregations[cat] = ['min', 'max', 'mean']\n\nprev_agg = df_train.groupby('fullVisitorId').agg({**aggs})\n\nprev_agg.columns = pd.Index(['Agg_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])","51327a76":"prev_agg","bd18a9fd":"new_columns = [\n        k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n    ]\nnew_columns","44891fc9":"dummy_feaures","05e2a265":"### Testing some grouping approaches","cb20b91c":"df_train['cumcount'] = df_train.groupby('fullVisitorId').cumcount() + 1","9c35ba0b":"aggs = {\n    'date': ['min', 'max'],\n    'totals.transactionRevenue': ['sum', 'size'],\n    'totals.hits': ['sum', 'min', 'max', 'count', 'median'],\n    'totals.pageviews': ['sum', 'min', 'max', 'mean', 'median'],\n    'totals.bounces': ['sum', 'mean', 'median'],\n    'totals.newVisits': ['sum', 'mean', 'median']\n}\n\n# Previous applications categorical features\ncat_aggregations = {}\n\nfor cat in dummy_feaures:\n    cat_aggregations[cat] = ['min', 'max', 'mean']\n\nprev_agg = df_train.groupby('fullVisitorId').agg({**aggs})\n\nprev_agg.head()\n\n","c1560702":"prev_agg.columns = [\"_\".join(x) for x in prev_agg.columns.ravel()]","76de649a":"prev_agg.head()","e6331f27":"## Some columns are in Json format so it will be necessary to handle with this problem.\n\nI will use a chunk code inspiration that almost all kernels are using I dont know who did first, but I got on SRK kernel and I did some modifications","a8ab76cd":"We have We can see the distribuition of ","603f289f":"Nice, this graph is very insightful.\nThe North America have a low ratio of Safari x Chrome... I thought that it was the contrary\n\nFirefox have a relative high presence in North America too.","f741f979":"### Function that I created to find the map outlier values\n- Click on \"code\" to see the function","bed9f91e":"### Cool! <br>\n\nIt's visually clear to see that chrome is the most used in all OS, less in iOS, that is a mobile OS. \n\nI will see if we can see a diference between the Revenues of transactions are different ","ee68be0f":"Seeing the crosstab with heatmap","de8c4ff7":"Very interesting, we can see that from 17  to 20 hour we have the highest numbers of ","dfeb1b4e":"The TOP 5 Grouping Channels represents 97% of total values. \nRespectivelly: \n- TOP 1 => Organic Search - 42.99%\n- TOP 2 => Social - 24.39%\n- TOP 3 => Direct - 15.42%\n- TOP 4 => Referral - 11.89%\n- TOP 5 => Paid Search - 2.55%\n\nI have a new insight that I will explore furthuer. How wich channel are distributed by browsers? \n","ef51d5ca":"Let's take a look on Mobile and Browser proportions","e5e0d692":"## I will apply the Prie Chart in Country's again","cab628ac":"## Crossing Channel Grouping x Browsers","a440e942":"First of all, the data are becoming in Json format, so we will need to handle with it and I will use a chunk that I saw in almost all kernel of this competition.","f5c8181f":"#### Let's investigate the VisitHour and weekday to see if we can find some interesting patterns","400c368f":"# I will continue this notebook! Votes up the kernel and stay tuned to next updates","f10e135c":"Excellent. Now we don't have more constant values","edc02a8f":"### Creating an Sofistcated interactive graphics to better understanding of date features\n\nTo see the code click in \"code\". \n## SELECT THE OPTION: ","1b104b10":"## Let's see the difference distribution between Devices","e49dc917":"## Let's investigate the Device Category","61e4abe2":"### Let's see the unique values in our dataset. <br>\n\n- if you want see click in \"output\"","ff6c2d53":"### Creating a function with plotly to better investigate the dataset\n\n- Click in \"code\" to see the commented code","603a20ec":"## I will explore the distribuition of transaction Revenue by each OS","e0460c8e":"Nice distribuition... We have very high values on the Transactions Revenue. \n","207b41db":"Wow, another very cool information. \n\n- (not set) domain have almost 50% of total visits and 62% of Revenues.\n- Unknown is responsible by 28% of visits but just 2.70% of Revenues\n- comcast.net have 5.5% of visits and 7.4% Revenues.","80d1b8c3":"### Months in pizza graph","c44a3ea7":"# First, let see the distribuition of transactions Revenues\n\nI will start exploring the quantile ","60523730":"The output show us totals.visits and trafficSource.adwordsClickInfo.adNetworkType, but totals,visits can be useful, so I will drop just trafficSource feature","a85d9c33":"\n### I will by object data Type. <br>\nClick on \"Output\" to see the result ","2b72c453":"## Nice! \n\nData Types contained in our dataframe: <br>\n- bool(1) <br>\n- int64(4) <br>\n- object(49) <br>","b1327806":"____________________","f065c5eb":"Defining some functions that I will use to call clean the data\n- If you want see, click in \"code\"","23aec3fd":"# looking if we have any mistake on \n\nfor c in dummy_feaures:\n    if c in to_drop: print(c)","19c35e63":"# SubContinent ","3156042a":"## What if we cross the Revenue and Browser?\n","215ec418":"### Some tests that I am doing to try find interesting feature engineering approaches","ebd8dabc":"Nice. <br>\nAfter the import and transformation, we have 54 columns. <br>\nNow, let's see our data and handle with problemns that we will find","89a0a79b":"## First I will explore revenue and number of visits by day","ecb29f2c":"Our data are fairly symmetrical skewed and have a High Kurtosis.\n\nI will see how many outliers we have on this dataset.\n","76485ca2":"# Objectives: \n\n- I will explore if we have some difference between the browser and if browser is significant to predict sells. <br>\n- Which countrys and continents have more acesses and sales ? How it's distributed?!  <br>\n- Which type of device are most normal in our dataset?<br>\n- What's the mobile % of accesses? <br>\n- Which is the most frequent Operational System? <br>\n- What's the most frequent channelGrouping ?<br> \n- Whats the most frequent Weekdays, months, days, year with highest accesses and revenue? \n\nAnd another bunch of ideas that I will have when start exploring.\n\n\n","c63eea22":"## Before look the unique values in each column, I will drop the constant values that is not useful and will make the df lighter\nfor it, I will need to give some attention to numerical values","754c9afd":"******* *How can I set order to my year, months and days?* *******","a2ea150b":"Some tests to feature engineering","52926f05":"- New York is responsible by 14% of visits and 31% of revenues.\n- Montain view have 19% in visists but just 16% of revenues\n- Chicago have just 3.5% of visits but have a high significance in revenues","c3a0c197":"## Knowing the missing values","7f2ed5c1":"## Let's see the NetWork Domain\n- I will plot visits and revenues by each category, including the non-set and unknown accesses and revenues\n","7e17ea9a":"## Exploring Countrys","bb7e53cd":"### Very Cool graphs.\n\nWE can see that the number of access are clearly downing by the through the time. \n\n- The months with highest accesses are October and November.\n- On the Weekend the trafic is lower than other days.\n- The 5 days with highest number of accesses is 1 and 5\n- Considering the full count of dates, we can see that the days with highest accesses are almost all in november\/2016\n","0cf3497a":"# Importing the datasets","549de8a5":"It's interesting to note that Referral have a less number of Visits but is responsible for almost 40% of revenues****","377d0867":"## Creating the function to handle with date ","8b1235ba":"It's interesting because we can see that <b>SessionId<\/b> has the <b>fullVisitorId<\/b> and <b>VisitStartTime<\/b> and <b>visitId<\/b>\n\nAlso, the date column we need to transform in datetime format and extract another datetime informations contained in the columns that I quoted above","5d8ad3e5":"### In our top 5 browsers we have more than 94% of total\n- TOP 1 - CHROME - 69,08%\n- TOP 2 - SAFARI - 20,04%\n- TOP 3 - FIREFOX - 3,77%\n\nNothing new under the sun... Chrome is the most used followed by Safari and firefox.\n","863e2e6c":"- If you want see the code click in \"code\"\n- If you want see the ouput click in \"output\"","3a28f85f":"Very cool! Interesting patterns","e97d5653":"Cool, we can have a better understanding of the distribution of Revenue by OS","e65cd577":"## I will see the kurtosis and Skewness of Transaction Revenue\nSkew and Kurtosis: 2 Important Statistics terms you need to know \n\n\n## Skewness\nIt is the degree of distortion from the symmetrical bell curve or the normal distribution. It measures the lack of symmetry in data distribution.<br>\nIt differentiates extreme values in one versus the other tail. A symmetrical distribution will have a skewness of 0.\n\n<b>Positive Skewness <\/b>means when the tail on the right side of the distribution is longer or fatter. The mean and median will be greater than the mode.<br>\n<b>Negative Skewness <\/b> is when the tail of the left side of the distribution is longer or fatter than the tail on the right side. The mean and median will be less than the mode.\n\n#### So, when is the skewness too much?\nThe rule of thumb seems to be:<br>\nIf the skewness is between -0.5 and 0.5, the data are fairly symmetrical. <br>\nIf the skewness is between -1 and -0.5(negatively skewed) or between 0.5 and 1(positively skewed), the data are moderately skewed.<br>\nIf the skewness is less than -1(negatively skewed) or greater than 1(positively skewed), the data are highly skewed.<br>\n\n\n## Kurtosis\nKurtosis is all about the tails of the distribution\u200a\u2014\u200anot the peakedness or flatness. It is used to describe the extreme values in one versus the other tail. <br>It is actually the measure of outliers present in the distribution.\n\n<b>High kurtosis<\/b> in a data set is an indicator that data has heavy tails or outliers. If there is a high kurtosis, then, we need to investigate why do we have so many outliers. It indicates a lot of things, maybe wrong data entry or other things. Investigate! <br>\n<b>Low kurtosis<\/b> in a data set is an indicator that data has light tails or lack of outliers. If we get low kurtosis(too good to be true), then also we need to investigate and trim the dataset of unwanted results","f6ed5137":"# Device Browsers","1942b9c9":"The TOP 5 of Operational System corresponds to 96%.\n\nTOP 1 => Windows - 38.75% <br>\nTOP 2 => Macintosh - 28.04% <br>\nTOP 3 => Android - 14.15% <br>\nTOP 4 => iOS - 11.75% <br>\nTOP 5 => Linux - 3.91% <br>\n\nIt's very interestign to me. In my country macbook isn't the most common SO. I will investigate further the SO by Country's","9d5c055c":"## I will drop some of this features and fillna or missing in some of them","744ae378":"## Presenting the initial data: \n\n<b>Data Fields: <\/b>\n<b>fullVisitorIdv<\/b> - A unique identifier for each user of the Google Merchandise Store. <br>\n<b>channelGrouping<\/b> - The channel via which the user came to the Store.<br>\n<b>date<\/b> - The date on which the user visited the Store.<br>\n<b>device <\/b>- The specifications for the device used to access the Store.<br>\n<b>geoNetwork<\/b> - This section contains information about the geography of the user.<br>\n<b>sessionId<\/b> - A unique identifier for this visit to the store.<br>\n<b>socialEngagementType<\/b> - Engagement type, either \"Socially Engaged\" or \"Not Socially Engaged\".<br>\n<b>totals<\/b> - This section contains aggregate values across the session.<br>\n<b>trafficSource<\/b> - This section contains information about the Traffic Source from which the session originated.<br>\n<b>visitId<\/b> - An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the user. For a completely unique ID, you should use a combination of fullVisitorId and visitId.<br>\n<b>visitNumber<\/b> - The session number for this user. If this is the first session, then this is set to 1.<br>\n<b>visitStartTime<\/b> - The timestamp (expressed as POSIX time).<br>","ce4f5b29":"## Let's see the Channel Grouping\n - The channel via which the user came to the Store.","de9f865a":"## Importing necessary librarys","28edbb03":"## I will use a interesting graphic called Squarify \n- I will apply it in feature Country to discovery where the user access the store","eb3345b7":"## Now, I will look on City feature and see the principal cities in the dataset","05f308ae":"If you want see another interesting Kernels please check here https:\/\/www.kaggle.com\/kabure\/kernels\n\n### *Please, don't forget to upvote this Kernel to keep me motivated ! *","2de99477":"Now I will take a look on trafficSource section, the Source to access the store","d5755f69":"# Google Analytics Customer Revenue Prediction","ec6f38ed":"## Now let's investigate the most used brower by Operational System","36b4214f":"## Total Revenues by Country ","c892dbc7":"## Let's cross the SubContinent by Browser","64a7f62e":"## Trafic Source Medium","e56d7a58":"English is not my native language, so sorry for any error.","f980099a":"Very interesting values.","9e11263a":"WoW, We have a very high number of users from North America. \n\nTOP 5 regions are equivalent of almost 70% +-  of total \n\nTOP 1 => Northern America - 44.18% <br>\nTOP 2 => Southeast Asia - 8.29% <br>\nTOP 3 => Northern Europe - 6.73% <br>\nTOP 4 => Southern Asia - 6.33% <br>\nTOP 5 => Western Europe - 6.23% <br>","b225e5ad":"## Now, lets investigate the Device Category by Browsers","a8082c7c":"## Seeing again Channel Grouping more specified","df041061":"- Organic have highest number of visits but is the third in revenues\n- Referral have almost 40% in both Visits and Revenues\n- The none category have almost 16% of visits but almost 40% of revenues","6c4d5901":"### Based on this output I will select and set a variable with all features by category","45265b76":"We haven't float datatype yet. <br>","450289f8":"### Let's investigate some constant columns","5f2ed0a4":"##  Geolocation plot to visually understand the data","770d5b0d":"## Preprocessing the fulldataset and creating new features","83c34eb9":"# Operational System ","e638c101":"Nicelly distributed clients that accessed the store. \n(non set) have 3.81% of total, so I dont will consider in top five, but it was the top 2 most frequent. \n\nThe top 5 are:\n- Montain View\n- New York \n- San Francisco\n- Sunnyvale \n- London\n\nAnd in terms of money, how the Countrys and Cities are ? \n","d438ee35":"# I will continue working on this kernel, stay tuned\n\n******** Please, if you liked this kernel don't forget to votes up and give your feedback ******** ","a7eaa095":"Normalize","9c7780ea":"The absolutelly high part of revenues are from Desktop Devices","0e6871e3":"We have a high number of visits from youtube but 0 sales. <br>\nthe mall.googleplex is have a low number of access but have the highest value in revenues","eeaf1a44":"Printing Integers","305218a4":"USA have a very highest value than another countrys. \n\nBelow I will take a look on cities and find for the highest revenues from them\n","8af15001":"### Nice. We can see that we have: <br>\nOur target have just 1.3% of non-null values  <br> \n6 columns with 97%+ of missing values  <br>\n4 columns with 50%+ of missing values <br>\n1 column with 22.22%  <br>\n1 column with 0.004% <br> \n\nWe will explore to understand what it looks like\n                                          ","70fba1b7":"Printing Float","f7b1baff":"## Let's take a look on datatypes of all columns","f145c5c0":"## INTERACTIVE DATE FEATURES","182bf148":"In percentual, we can see that :\n- desktop represents 73.5%\n- mobile represents 23.12%\n- tablet represents 3.38%\n\nI thought that the Revenue is almost all did by desktops. Let's explore it further.","e04c57a3":"# It's useul to we have notion that might we have 23 constant columns\n\n- Below I will set a function to better investigate our data and correctly categorize them\n","2b784729":"## Device Category feature","1d82c0ae":"I think that it's very insightful information.\n\nChrome have highest values in general but the highest value of transactions was did on Firefox.<br>\nWe can see a \"small\" but consistent sells in Safari. Also IE and Edge give some results to Google;"}}