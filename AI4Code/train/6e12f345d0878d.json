{"cell_type":{"ee88b265":"code","a3ea240e":"code","afec3849":"code","e7716627":"code","c3048877":"code","6aa4ab6e":"code","cf4ce8c2":"code","b44f7f4b":"code","785e7a44":"code","f3475c92":"code","c5e8f18f":"code","2999edb8":"code","2fa487e7":"code","9e9f5a23":"code","4595da01":"code","a4e38e39":"code","ad6303ff":"code","01761cfe":"code","79dfe0d6":"code","f3c68abf":"code","5f983093":"code","88a418c6":"code","ca4128fa":"code","842c0792":"code","ab537cba":"code","59c5328b":"code","2e6fbf42":"code","5ec29de9":"code","920659d3":"code","0215f482":"code","aaa57b4f":"code","9bd2f217":"code","eb068dda":"code","d2585aeb":"code","1b7a4b60":"code","d5f9801c":"code","8e043c0f":"code","9cd285ef":"code","37211fb0":"code","d261ea2f":"code","1e1b10e2":"code","ef20f2f1":"code","20e612ff":"code","fed7164c":"code","d9ccdcdb":"code","8ace105f":"code","7753b71d":"code","71ab28a3":"code","9222dfff":"code","d4bb670b":"code","965a6149":"code","c544dbe3":"code","eaf735a9":"code","3c6e159a":"code","53c8e932":"code","d9c8d918":"code","71fa17f7":"code","f385b0f7":"code","c39462b4":"markdown","21efd971":"markdown","78c652d1":"markdown","5781cd22":"markdown","dd7af3b6":"markdown","44cbf351":"markdown","e5660b5c":"markdown","e23d468d":"markdown","b30d124a":"markdown","c113cb2b":"markdown","66bec37b":"markdown","4e9a4ed7":"markdown","4c8cae3b":"markdown","b64584f5":"markdown","05f2d3fc":"markdown","c77f5eb0":"markdown","141d33c8":"markdown","c87ca16f":"markdown","c5dd4ba6":"markdown","a4c99e07":"markdown","1bc4015c":"markdown","c6a6db4b":"markdown","05a2fa13":"markdown","97c4d497":"markdown","2d13e83c":"markdown","a32e2f33":"markdown","8883ffa0":"markdown","2009ab16":"markdown","78a65582":"markdown","ca59d729":"markdown"},"source":{"ee88b265":"from numpy.random import seed\nseed(101)\nfrom tensorflow import set_random_seed\nset_random_seed(101)\n\nimport pandas as pd\nimport numpy as np\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\n\nimport os\nimport cv2\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline","a3ea240e":"\nIMAGE_SIZE = 96\nIMAGE_CHANNELS = 3\n\nSAMPLE_SIZE = 80000 # the number of images we use from each of the two classes\n","afec3849":"os.listdir('..\/input')","e7716627":"\nprint(len(os.listdir('..\/input\/train')))\nprint(len(os.listdir('..\/input\/test')))","c3048877":"df_data = pd.read_csv('..\/input\/train_labels.csv')\n\n# removing this image because it caused a training error previously\ndf_data[df_data['id'] != 'dd6dfed324f9fcb6f93f46f32fc800f2ec196be2']\n\n# removing this image because it's black\ndf_data[df_data['id'] != '9369c7278ec8bcc6c880d99194de09fc2bd4efbe']\n\n\nprint(df_data.shape)","6aa4ab6e":"df_data['label'].value_counts()","cf4ce8c2":"# source: https:\/\/www.kaggle.com\/gpreda\/honey-bee-subspecies-classification\n\ndef draw_category_images(col_name,figure_cols, df, IMAGE_PATH):\n    \n    \"\"\"\n    Give a column in a dataframe,\n    this function takes a sample of each class and displays that\n    sample on one row. The sample size is the same as figure_cols which\n    is the number of columns in the figure.\n    Because this function takes a random sample, each time the function is run it\n    displays different images.\n    \"\"\"\n    \n\n    categories = (df.groupby([col_name])[col_name].nunique()).index\n    f, ax = plt.subplots(nrows=len(categories),ncols=figure_cols, \n                         figsize=(4*figure_cols,4*len(categories))) # adjust size here\n    # draw a number of images for each location\n    for i, cat in enumerate(categories):\n        sample = df[df[col_name]==cat].sample(figure_cols) # figure_cols is also the sample size\n        for j in range(0,figure_cols):\n            file=IMAGE_PATH + sample.iloc[j]['id'] + '.tif'\n            im=cv2.imread(file)\n            ax[i, j].imshow(im, resample=True, cmap='gray')\n            ax[i, j].set_title(cat, fontsize=16)  \n    plt.tight_layout()\n    plt.show()\n    ","b44f7f4b":"IMAGE_PATH = '..\/input\/train\/' \n\ndraw_category_images('label',4, df_data, IMAGE_PATH)","785e7a44":"df_data.head()","f3475c92":"# take a random sample of class 0 with size equal to num samples in class 1\ndf_0 = df_data[df_data['label'] == 0].sample(SAMPLE_SIZE, random_state = 101)\n# filter out class 1\ndf_1 = df_data[df_data['label'] == 1].sample(SAMPLE_SIZE, random_state = 101)\n\n# concat the dataframes\ndf_data = pd.concat([df_0, df_1], axis=0).reset_index(drop=True)\n# shuffle\ndf_data = shuffle(df_data)\n\ndf_data['label'].value_counts()","c5e8f18f":"df_data.head()","2999edb8":"# train_test_split\n\n# stratify=y creates a balanced validation set.\ny = df_data['label']\n\ndf_train, df_val = train_test_split(df_data, test_size=0.10, random_state=101, stratify=y)\n\nprint(df_train.shape)\nprint(df_val.shape)","2fa487e7":"df_train['label'].value_counts()","9e9f5a23":"df_val['label'].value_counts()","4595da01":"# Create a new directory\nbase_dir = 'base_dir'\nos.mkdir(base_dir)\n\n\n#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n\n# now we create 2 folders inside 'base_dir':\n\n# train_dir\n    # a_no_tumor_tissue\n    # b_has_tumor_tissue\n\n# val_dir\n    # a_no_tumor_tissue\n    # b_has_tumor_tissue\n\n\n\n# create a path to 'base_dir' to which we will join the names of the new folders\n# train_dir\ntrain_dir = os.path.join(base_dir, 'train_dir')\nos.mkdir(train_dir)\n\n# val_dir\nval_dir = os.path.join(base_dir, 'val_dir')\nos.mkdir(val_dir)\n\n\n\n# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\n# Inside each folder we create seperate folders for each class\n\n# create new folders inside train_dir\nno_tumor_tissue = os.path.join(train_dir, 'a_no_tumor_tissue')\nos.mkdir(no_tumor_tissue)\nhas_tumor_tissue = os.path.join(train_dir, 'b_has_tumor_tissue')\nos.mkdir(has_tumor_tissue)\n\n\n# create new folders inside val_dir\nno_tumor_tissue = os.path.join(val_dir, 'a_no_tumor_tissue')\nos.mkdir(no_tumor_tissue)\nhas_tumor_tissue = os.path.join(val_dir, 'b_has_tumor_tissue')\nos.mkdir(has_tumor_tissue)\n\n","a4e38e39":"# check that the folders have been created\nos.listdir('base_dir\/train_dir')","ad6303ff":"# Set the id as the index in df_data\ndf_data.set_index('id', inplace=True)","01761cfe":"\n\n# Get a list of train and val images\ntrain_list = list(df_train['id'])\nval_list = list(df_val['id'])\n\n\n\n# Transfer the train images\n\nfor image in train_list:\n    \n    # the id in the csv file does not have the .tif extension therefore we add it here\n    fname = image + '.tif'\n    # get the label for a certain image\n    target = df_data.loc[image,'label']\n    \n    # these must match the folder names\n    if target == 0:\n        label = 'a_no_tumor_tissue'\n    if target == 1:\n        label = 'b_has_tumor_tissue'\n    \n    # source path to image\n    src = os.path.join('..\/input\/train', fname)\n    # destination path to image\n    dst = os.path.join(train_dir, label, fname)\n    # copy the image from the source to the destination\n    shutil.copyfile(src, dst)\n\n\n# Transfer the val images\n\nfor image in val_list:\n    \n    # the id in the csv file does not have the .tif extension therefore we add it here\n    fname = image + '.tif'\n    # get the label for a certain image\n    target = df_data.loc[image,'label']\n    \n    # these must match the folder names\n    if target == 0:\n        label = 'a_no_tumor_tissue'\n    if target == 1:\n        label = 'b_has_tumor_tissue'\n    \n\n    # source path to image\n    src = os.path.join('..\/input\/train', fname)\n    # destination path to image\n    dst = os.path.join(val_dir, label, fname)\n    # copy the image from the source to the destination\n    shutil.copyfile(src, dst)\n    \n\n\n   ","79dfe0d6":"# check how many train images we have in each folder\n\nprint(len(os.listdir('base_dir\/train_dir\/a_no_tumor_tissue')))\nprint(len(os.listdir('base_dir\/train_dir\/b_has_tumor_tissue')))\n","f3c68abf":"# check how many val images we have in each folder\n\nprint(len(os.listdir('base_dir\/val_dir\/a_no_tumor_tissue')))\nprint(len(os.listdir('base_dir\/val_dir\/b_has_tumor_tissue')))\n","5f983093":"# End of Data Preparation\n### ===================================================================================== ###\n# Start of Model Building\n","88a418c6":"train_path = 'base_dir\/train_dir'\nvalid_path = 'base_dir\/val_dir'\ntest_path = '..\/input\/test'\n\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = 10\nval_batch_size = 10\n\n\ntrain_steps = np.ceil(num_train_samples \/ train_batch_size)\nval_steps = np.ceil(num_val_samples \/ val_batch_size)","ca4128fa":"datagen = ImageDataGenerator(rescale=1.0\/255)\n\ntrain_gen = datagen.flow_from_directory(train_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=train_batch_size,\n                                        class_mode='categorical')\n\nval_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=val_batch_size,\n                                        class_mode='categorical')\n\n# Note: shuffle=False causes the test dataset to not be shuffled\ntest_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=1,\n                                        class_mode='categorical',\n                                        shuffle=False)","842c0792":"\n\nkernel_size = (3,3)\npool_size= (2,2)\nfirst_filters = 32\nsecond_filters = 64\nthird_filters = 128\n\ndropout_conv = 0.3\ndropout_dense = 0.3\n\n\nmodel = Sequential()\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu', input_shape = (96, 96, 3)))\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size)) \nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(dropout_dense))\nmodel.add(Dense(2, activation = \"softmax\"))\n\nmodel.summary()\n","ab537cba":"model.compile(Adam(lr=0.0001), loss='binary_crossentropy', \n              metrics=['accuracy'])","59c5328b":"# Get the labels that are associated with each index\nprint(val_gen.class_indices)","2e6fbf42":"filepath = \"model.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n                             save_best_only=True, mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \n                              \ncallbacks_list = [checkpoint, reduce_lr]\n\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n                    validation_data=val_gen,\n                    validation_steps=val_steps,\n                    epochs=20, verbose=1,\n                   callbacks=callbacks_list)","5ec29de9":"# get the metric names so we can use evaulate_generator\nmodel.metrics_names","920659d3":"# Here the best epoch will be used.\n\nmodel.load_weights('model.h5')\n\nval_loss, val_acc = \\\nmodel.evaluate_generator(test_gen, \n                        steps=len(df_val))\n\nprint('val_loss:', val_loss)\nprint('val_acc:', val_acc)","0215f482":"# display the loss and accuracy curves\n\nimport matplotlib.pyplot as plt\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()","aaa57b4f":"# make a prediction\npredictions = model.predict_generator(test_gen, steps=len(df_val), verbose=1)","9bd2f217":"predictions.shape","eb068dda":"# This is how to check what index keras has internally assigned to each class. \ntest_gen.class_indices","d2585aeb":"# Put the predictions into a dataframe.\n# The columns need to be oredered to match the output of the previous cell\n\ndf_preds = pd.DataFrame(predictions, columns=['no_tumor_tissue', 'has_tumor_tissue'])\n\ndf_preds.head()","1b7a4b60":"# Get the true labels\ny_true = test_gen.classes\n\n# Get the predicted labels as probabilities\ny_pred = df_preds['has_tumor_tissue']\n","d5f9801c":"from sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_true, y_pred)","8e043c0f":"# Source: Scikit Learn website\n# http:\/\/scikit-learn.org\/stable\/auto_examples\/\n# model_selection\/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n# selection-plot-confusion-matrix-py\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","9cd285ef":"# Get the labels of the test images.\n\ntest_labels = test_gen.classes","37211fb0":"test_labels.shape","d261ea2f":"# argmax returns the index of the max value in a row\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))","1e1b10e2":"# Print the label associated with each class\ntest_gen.class_indices","ef20f2f1":"# Define the labels of the class indices. These need to match the \n# order shown above.\ncm_plot_labels = ['no_tumor_tissue', 'has_tumor_tissue']\n\nplot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')","20e612ff":"from sklearn.metrics import classification_report\n\n# Generate a classification report\n\n# For this to work we need y_pred as binary labels not as probabilities\ny_pred_binary = predictions.argmax(axis=1)\n\nreport = classification_report(y_true, y_pred_binary, target_names=cm_plot_labels)\n\nprint(report)\n","fed7164c":"# Delete base_dir and it's sub folders to free up disk space.\n\nshutil.rmtree('base_dir')\n","d9ccdcdb":"\n#[CREATE A TEST FOLDER DIRECTORY STRUCTURE]\n\n# We will be feeding test images from a folder into predict_generator().\n# Keras requires that the path should point to a folder containing images and not\n# to the images themselves. That is why we are creating a folder (test_images) \n# inside another folder (test_dir).\n\n# test_dir\n    # test_images\n\n# create test_dir\ntest_dir = 'test_dir'\nos.mkdir(test_dir)\n    \n# create test_images inside test_dir\ntest_images = os.path.join(test_dir, 'test_images')\nos.mkdir(test_images)","8ace105f":"# check that the directory we created exists\nos.listdir('test_dir')","7753b71d":"# Transfer the test images into image_dir\n\ntest_list = os.listdir('..\/input\/test')\n\nfor image in test_list:\n    \n    fname = image\n    \n    # source path to image\n    src = os.path.join('..\/input\/test', fname)\n    # destination path to image\n    dst = os.path.join(test_images, fname)\n    # copy the image from the source to the destination\n    shutil.copyfile(src, dst)","71ab28a3":"# check that the images are now in the test_images\n# Should now be 57458 images in the test_images folder\n\nlen(os.listdir('test_dir\/test_images'))","9222dfff":"test_path ='test_dir'\n\n\n# Here we change the path to point to the test_images folder.\n\ntest_gen = datagen.flow_from_directory(test_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=1,\n                                        class_mode='categorical',\n                                        shuffle=False)","d4bb670b":"num_test_images = 57458\n\n# make sure we are using the best epoch\nmodel.load_weights('model.h5')\n\npredictions = model.predict_generator(test_gen, steps=num_test_images, verbose=1)\n","965a6149":"# Are the number of predictions correct?\n# Should be 57458.\n\nlen(predictions)","c544dbe3":"# Put the predictions into a dataframe\n\ndf_preds = pd.DataFrame(predictions, columns=['no_tumor_tissue', 'has_tumor_tissue'])\n\ndf_preds.head()","eaf735a9":"\n# This outputs the file names in the sequence in which \n# the generator processed the test images.\ntest_filenames = test_gen.filenames\n\n# add the filenames to the dataframe\ndf_preds['file_names'] = test_filenames\n\ndf_preds.head()","3c6e159a":"# Create an id column\n\n# A file name now has this format: \n# test_images\/00006537328c33e284c973d7b39d340809f7271b.tif\n\n# This function will extract the id:\n# 00006537328c33e284c973d7b39d340809f7271b\n\n\ndef extract_id(x):\n    \n    # split into a list\n    a = x.split('\/')\n    # split into a list\n    b = a[1].split('.')\n    extracted_id = b[0]\n    \n    return extracted_id\n\ndf_preds['id'] = df_preds['file_names'].apply(extract_id)\n\ndf_preds.head()","53c8e932":"# Get the predicted labels.\n# We were asked to predict a probability that the image has tumor tissue\ny_pred = df_preds['has_tumor_tissue']\n\n# get the id column\nimage_id = df_preds['id']","d9c8d918":"submission = pd.DataFrame({'id':image_id, \n                           'label':y_pred, \n                          }).set_index('id')\n\nsubmission.to_csv('patch_preds.csv', columns=['label']) ","71fa17f7":"submission.head()","f385b0f7":"# Delete the test_dir directory we created to prevent a Kaggle error.\n# Kaggle allows a max of 500 files to be saved.\n\nshutil.rmtree('test_dir')","c39462b4":"\n### Reference Kernels\n\nI found these kernels very helpful:\n\n1. Gabriel Preda, Honey Bee Subspecies Classification <br>\nhttps:\/\/www.kaggle.com\/gpreda\/honey-bee-subspecies-classification<br>\n\n2. Beluga, Black and White CNN<br>\nhttps:\/\/www.kaggle.com\/gaborfodor\/black-white-cnn-lb-0-77\n\n3.  Francesco Marazzi, Baseline Keras CNN<br>\nhttps:\/\/www.kaggle.com\/fmarazzi\/baseline-keras-cnn-roc-fast-5min-0-8253-lb\n","21efd971":"### How many images are in each folder?","78c652d1":"> 1. It helps to turn on the GPU before creating any folders. If you create the folder structure and then turn on the GPU or Internet - all the folders you created will disappear.\n> 2. I've used tensorflow.keras. If you use native Keras then you may get different results.\n> ","5781cd22":"### Make a prediction on the test images","dd7af3b6":"### Create a Confusion Matrix","44cbf351":"### Labels as per csv file\n\n0 = no tumor tissue<br>\n1 =   has tumor tissue. <br>\n","e5660b5c":"Thank you for reading.","e23d468d":"### A note on Keras class index values\n\nKeras assigns it's own index value (here 0 and 1) to the classes.\nIt infers the classes based on the folder structure.<br>\nImportant: These index values may not match the index values we were given in the **train_labels.csv** file.\n\nI've used 'a' and 'b' folder name pre-fixes to get keras to assign index values to match what\nwas in the train_labels.csv file - I guessed that keras is assigning the index value based on\nfolder name alphabetical order.","b30d124a":"### Make a prediction on the val set\nWe need these predictions to calculate the AUC score, print the Confusion Matrix and calculate the F1 score.","c113cb2b":"### Create a submission file","66bec37b":"### Evaluate the model using the val set","4e9a4ed7":"### What files are available?","4c8cae3b":"### Create a Classification Report","b64584f5":"### Create the Model Architecture\u00b6","05f2d3fc":"### Transfer the images into the folders","c77f5eb0":"**Recall **= Given a class, will the classifier be able to detect it?<br>\n**Precision **= Given a class prediction from a classifier, how likely is it to be correct?<br>\n**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.\n\nFrom the confusion matrix and classification report we see that our model is equally good at detecting both classes.","141d33c8":"### Set up the generator","c87ca16f":"### Create a Dataframe containing all images","c5dd4ba6":"### What is the AUC Score?","a4c99e07":"### MAKE A TEST SET PREDICTION","1bc4015c":"**Introduction**\n\nIn this kernel I will describe a workflow that  allows 160,000 full size images to be used without crashing the kaggle kernel. \n\nThis is made possible by setting up a directory structure and then using generators to feed the data into the model for training, validation and for prediction.\n\nWe will train the model using 144,000 images and validate on 16,000 images.\n","c6a6db4b":"### Display a random sample of train images  by class","05a2fa13":"### Plot the Training Curves","97c4d497":"### Set Up the Generators","2d13e83c":"### Create the Train and Val Sets","a32e2f33":"### Train the Model","8883ffa0":"### Create a Directory Structure","2009ab16":"### Check the class distribution","78a65582":"I've used the CNN architecture presented by @fmarazzi in this kernel:<br>\nhttps:\/\/www.kaggle.com\/fmarazzi\/baseline-keras-cnn-roc-fast-5min-0-8253-lb","ca59d729":"#### Balance the target distribution\nWe will reduce the number of samples in class 0."}}