{"cell_type":{"5dfdbf41":"code","2aec2b44":"code","3a6acf87":"code","33f8a38f":"code","58336697":"code","84e22c97":"code","0227f2fd":"code","f5960899":"code","d509ea51":"code","83152b74":"code","dcd807cb":"code","4c3caabf":"code","9b922c13":"code","95525151":"markdown"},"source":{"5dfdbf41":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport json\nimport math\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom transformers import BertModel, BertTokenizer,RobertaModel,RobertaTokenizer\nfrom transformers import AdamW, get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2aec2b44":"#Calculate Benchmarks\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\n\nclf_svm = LinearSVC(loss='hinge', penalty='l2')\nclf_mlp = MLPClassifier(hidden_layer_sizes=(150,100,50), max_iter=300,activation = 'relu',solver='adam',random_state=1)\n","3a6acf87":"#Read Train and Test Set\ntrain_set = pd.read_csv(\"\/kaggle\/input\/helpdesk-tickets\/train.csv\",encoding = \"ISO-8859-1\")\ntest_set = pd.read_csv(\"\/kaggle\/input\/helpdesk-tickets-test\/test.csv\",encoding = \"ISO-8859-1\")\ntrain_set['description']=train_set['description'].astype('str')\ntrain_set['pool']=train_set['pool'].astype('str')\ntest_set['description']=test_set['description'].astype('str')\ntest_set['pool']=test_set['pool'].astype('str')\nx_train, y_train = train_set['description'], train_set['pool']\nx_test, y_test = test_set['description'], test_set['pool']","33f8a38f":"# SVM Benchmark for Accuracy\nbaseline= make_pipeline(CountVectorizer(ngram_range=(1,2), max_df=0.8, min_df=5),TfidfTransformer(),clf_svm)\nbaseline.fit(x_train, y_train)\nte_str=f'Test accuracy (SVM tf-idf baseline): {100*baseline.score(x_test, y_test):.2f} %'\nprint(te_str)","58336697":"#MLP Benchmark for Accuracy\nbaseline_mlp= make_pipeline(CountVectorizer(ngram_range=(1,2), max_df=0.8, min_df=5),TfidfTransformer(),clf_mlp)\nbaseline_mlp.fit(x_train, y_train)\nte_str=f'Test accuracy (MLP tf-idf baseline): {100*baseline_mlp.score(x_test, y_test):.2f} %'\nprint(te_str)","84e22c97":"#RoBerta Encoder to encode text data for prototypical networks\nclass BERTEncoder(nn.Module):\n    \"\"\"Encoder indices of sentences in BERT last hidden states.\"\"\"\n    def __init__(self, model_shortcut_name,max_length):\n        super(BERTEncoder, self).__init__()\n        self.max_length = max_length\n        self.bert = RobertaModel.from_pretrained(\n            model_shortcut_name\n        )\n        self.bert_tokenizer = RobertaTokenizer.from_pretrained(\n            model_shortcut_name\n        )\n           \n    def forward(self, tokens, mask):\n        \"\"\"BERT encoder forward.\n        Args:\n            tokens: torch.Tensor, [-1, max_length]\n            mask: torch.Tensor, [-1, max_length]\n            \n        Returns:\n            sentence_embedding: torch.Tensor, [-1, hidden_size]\"\"\"\n       \n        last_hidden_state = self.bert(tokens, attention_mask=mask)\n        return last_hidden_state[0][:, 0, :]  # The last hidden-state of <CLS>\n\n    def tokenize(self, text):\n        \"\"\"BERT tokenizer.\n        \n        Args:\n            text: str\n        \n        Returns:\n            ids: list, [max_length]\n            mask: list, [max_length]\"\"\"\n        ids = self.bert_tokenizer.encode(text, add_special_tokens=True,\n                                         max_length=self.max_length)\n        # attention mask: 1 for tokens that are NOT MASKED, 0 for MASKED tokens.\n        mask = [1] * len(ids)\n        # Padding\n        while len(ids) < self.max_length:\n            ids.append(0)\n            mask.append(0)\n        # truncation\n        ids = ids[:self.max_length]\n        mask = mask[:self.max_length]\n        return ids, mask\n    ","0227f2fd":"# Load train and test dataset in the form of  N way K shot tasks. \nimport os\nimport json\nimport logging\nimport random\nimport math\nimport pandas as pd\nimport torch\nimport torch.utils.data as data\nclass GeneralDataset(data.Dataset):\n    \"\"\"\n    Returns:\n        support: torch.Tensor, [N, K, max_length]\n        support_mask: torch.Tensor, [N, K, max_length]\n        query: torch.Tensor, [totalQ, max_length]\n        query_mask: torch.Tensor, [totalQ, max_length]\n        label: torch.Tensor, [totalQ]\"\"\"\n    def __init__(self, path, name, tokenizer, N, K, Q):\n        file_path = os.path.join(path, name)\n        if not os.path.exists(file_path):\n            raise Exception(\"File {} does not exist.\".format(file_path))\n        self.train_df = pd.read_csv(file_path)\n        self.train_X = self.train_df['description']\n        self.train_Y = self.train_df['pool']\n        self.classes = list(set(self.train_Y))\n        self.nb_classes = len(self.classes)\n        self.N, self.K, self.Q = N, K, Q\n        self.tokenizer = tokenizer\n\n    def __getitem__(self, index):\n        support, support_mask = [], []\n        query, query_mask = [], []\n        query_label = []\n\n        target_classes = random.sample(self.classes, self.N)  # Sample N class name\n        #print(target_classes)\n        for class_idx, class_name in enumerate(target_classes):\n\n            # Add [] for each class\n            support.append([])\n            support_mask.append([])\n            dataset = self.train_X[self.train_df['pool'] == class_name].values.tolist()\n            samples = random.sample(dataset, self.K + self.Q)\n         #   print(samples)\n            for idx, sample in enumerate(samples):\n                # Tokenize. Senquences to indices.\n                indices, mask = self.tokenizer(sample)\n                \n\n                if idx < self.K:\n                    support[class_idx].append(indices)\n                    support_mask[class_idx].append(mask)\n                    \n                else:\n                    query.append(indices)\n                    query_mask.append(mask)\n            query_label += [class_idx] * self.Q\n        #print(support)\n        return (torch.tensor(support, dtype=torch.long),\n                torch.tensor(support_mask, dtype=torch.long),\n                torch.tensor(query, dtype=torch.long),\n                torch.tensor(query_mask, dtype=torch.long),\n                torch.tensor(query_label, dtype=torch.long))\n\n    def __len__(self):\n        return 10000\n\n\ndef get_general_data_loader(path, name, tokenizer, N, K, Q, batch_size,\n                            num_workers=4, sampler=False):\n    dataset = GeneralDataset(path, name, tokenizer, N, K, Q)\n    if sampler:\n        sampler = data.distributed.DistributedSampler(dataset)\n    else:\n        sampler = None\n    data_loader = data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        num_workers=0,\n        sampler=sampler\n    )\n    return iter(data_loader)","f5960899":"# Calculates Euclidean distance between class protoypes and query samples\nclass L2Distance(nn.Module):\n    def __init__(self):\n        super(L2Distance, self).__init__()\n\n    def forward(self, support, query):\n        \"\"\"\n        Args:\n            support: torch.Tensor, [B, totalQ, N, D]\n            query: torch.Tensor, [B, totalQ, N, D]\n            \n        Returns:\n            relation_score: torch.Tensor, [B, totalQ, N]\"\"\"\n        l2_distance = torch.pow(support - query, 2).sum(-1, keepdim=False)  # [B, totalQ, N]\n        return F.softmax(-l2_distance, dim=-1)","d509ea51":"#Model for prototypical network\nclass PrototypeNetwork(nn.Module):\n    def __init__(self, encoder, relation_module, hidden_size, max_length,\n                 current_device=torch.device(\"cpu\")):\n        super(PrototypeNetwork, self).__init__()\n        self.encoder = encoder\n        self.relation_module = relation_module\n        self.hidden_size = hidden_size  # D\n        self.max_length = max_length\n        self.current_device = current_device\n\n    def loss(self, predict_proba, label):\n        # CE loss\n        N = predict_proba.size(-1)\n        return F.cross_entropy(predict_proba.view(-1, N), label.view(-1))\n    \n    def mean_accuracy(self, predict_label, label):\n        return torch.mean((predict_label.view(-1) == label.view(-1)).type(torch.FloatTensor))\n\n    def forward(self, support, support_mask, query, query_mask):\n        \"\"\"Prototype Networks forward.\n        Args:\n            support: torch.Tensor, [-1, N, K, max_length]\n            support_mask: torch.Tensor, [-1, N, K, max_length]\n            query: torch.Tensor, [-1, totalQ, max_length]\n            query_mask: torch.Tensor, [-1, totalQ, max_length]\n            \n        Returns:\n            relation_score: torch.Tensor, [B, totalQ, N]\n            predict_label: torch.Tensor, [B, totalQ]\"\"\"\n        B, N, K = support.size()[:3]\n        totalQ = query.size()[1]  # Number of query instances for each batch\n        \n        # 1. Encoder\n        support = support.view(-1, self.max_length)  # [B * N * K, max_length]\n        support_mask = support_mask.view(-1, self.max_length)\n        query = query.view(-1, self.max_length)  # [B * totalQ, max_length]\n        query_mask = query_mask.view(-1, self.max_length)\n\n        support = self.encoder(support, support_mask)  # [B * N * K, D]\n        query = self.encoder(query, query_mask)  # [B * totalQ, D]\n        support = support.view(-1, N, K, self.hidden_size)  # [B, N, K, D]\n        query = query.view(-1, totalQ, self.hidden_size)  # [B, totalQ, D]\n\n        # 2. Prototype\n        support = support.mean(2, keepdim=False)  # [B, N, D]\n\n        # 3. Relation\n        support = support.unsqueeze(1).expand(-1, totalQ, -1, -1)  # [B, totalQ, N, D]\n        query = query.unsqueeze(2).expand(-1, -1, N, -1)  # [B, totalQ, N, D]\n        relation_score = self.relation_module(support, query)  # [B, totalQ, N]\n\n        predict_label = relation_score.argmax(dim=-1, keepdims=False)  # [B, totalQ]\n\n        return relation_score, predict_label","83152b74":"# Trains the model\ndef train_prototype(data_name, train_data, val_data, tokenizer, model, B, N, K, Q, optimizer,\n          train_episodes, val_episodes, val_steps, grad_steps, lr,\n          warmup, weight_decay, save_checkpoint, cuda=False, fp16=False):\n   \n    data_name == \"helpdesk-tickets\"\n    train_data_loader = get_general_data_loader(\"\/kaggle\/input\/helpdesk-tickets\/\",train_data,\n                                                    tokenizer, N, K, Q, B)\n    \n\n    parameter_list = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]  # Do not use weight decay.\n    optimizer = optimizer(\n        [\n            {\n                \"params\": [param for name, param in parameter_list\n                if not any(nd in name for nd in no_decay)],\n                \"weight_decay\": weight_decay\n            },\n            {\n                \"params\": [param for name, param in parameter_list\n                if any(nd in name for nd in no_decay)],\n                \"weight_decay\": 0.0\n            }\n        ], lr=lr\n    )\n\n    # A schedule with a learning rate that decreases linearly after linearly\n    # increasing during a warmup period.\n    # https:\/\/huggingface.co\/transformers\/main_classes\/optimizer_schedules.html#transformers.get_linear_schedule_with_warmup\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=int(warmup * train_episodes),\n        num_training_steps=train_episodes\n    )\n    \n\n    if cuda:\n        model.cuda()\n    if fp16:\n        from apex import amp\n        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n    model.train()  # Set model to train mode.\n\n    # Add model graph to tensorboard.\n    dummy_support, dummy_support_mask, dummy_query, dummy_query_mask, _ = next(train_data_loader)\n    if cuda:\n        dummy_support = dummy_support.cuda()\n        dummy_support_mask = dummy_support_mask.cuda()\n        dummy_query = dummy_query.cuda()\n        dummy_query_mask = dummy_query_mask.cuda()\n  \n\n    total_samples = 0  # Count 'total' samples\n    total_loss = 0.0\n    total_acc_mean = 0.0\n    best_val_acc = 0.0\n\n\n    for episode in range(1, train_episodes + 1):\n        support, support_mask, query, query_mask, label = next(train_data_loader)\n        if cuda:\n            support = support.cuda()\n            support_mask = support_mask.cuda()\n            query = query.cuda()\n            query_mask = query_mask.cuda()\n            label = label.cuda()\n\n        relation_score, predict_label = model(support, support_mask, query, query_mask)\n        loss = model.loss(relation_score, label) \/ grad_steps\n        acc_mean = model.mean_accuracy(predict_label, label)\n\n        if fp16:\n            with amp.scale_loss(loss, optimizer) as scaled_loss:\n                scaled_loss.backward()\n        else:\n            loss.backward()\n        \n     \n\n        if episode % grad_steps == 0:\n            # Update params\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n        \n        total_loss += loss.item()\n        total_acc_mean += acc_mean.item()\n        total_samples += 1\n        print(\"Total loss: \" + str(total_loss))    \n        if val_data is not None and episode % val_steps == 0:\n            eval_loss, eval_acc = eval(data_name, val_data, tokenizer, model,\n                                           1, 5, 4, 1, val_episodes, cuda=cuda)\n            print(\"Eval Loss: \" + str(eval_loss))\n            print(\"Eval acc: \" + str(eval_acc))\n            model.train()  # Reset model to train mode.\n            if eval_acc > best_val_acc:\n                # Save model\n                torch.save(model.state_dict(), save_checkpoint)\n                best_val_acc = eval_acc\n            total_samples = 0\n            total_loss = 0.0\n            total_acc_mean = 0.0\n    print(best_val_acc)        \n    return best_val_acc","dcd807cb":"#Evaluates the model\ndef eval(data_name, val_data, tokenizer, model, B, N, K, Q, val_episodes,\n         load_checkpoint=None, is_test=False, cuda=False):\n        \n    if cuda:\n       model.cuda()\n\n    model.eval()  # Set model to eval mode.\n\n    if data_name == \"helpdesk-tickets\":\n        data_loader = get_general_data_loader(\"\/kaggle\/input\/helpdesk-tickets-test\/\",val_data, tokenizer, N, K, Q, B)\n    \n    out_mark = \"Test\" if is_test else \"Val\"\n\n    total_loss = 0.0\n    total_acc_mean = 0.0\n    with torch.no_grad():\n        for episode in range(1, val_episodes + 1):\n            support, support_mask, query, query_mask, label = next(data_loader)\n            if cuda:\n                support = support.cuda()\n                support_mask = support_mask.cuda()\n                query = query.cuda()\n                query_mask = query_mask.cuda()\n                label = label.cuda()\n\n            relation_score, predict_label = model(support, support_mask, query, query_mask)\n            loss = model.loss(relation_score, label)\n            acc_mean = model.mean_accuracy(predict_label, label)\n            total_loss += loss.item()\n            total_acc_mean += acc_mean.item()\n\n    loss_mean, acc_mean = total_loss \/ val_episodes, 100 * total_acc_mean \/ val_episodes\n    return loss_mean, acc_mean","4c3caabf":"def main():\n    \n    data_name = \"helpdesk-tickets\"\n    max_length = 100\n    \n    if torch.cuda.is_available():\n        current_cuda = True\n        current_device = torch.device(\"cuda\")\n    else:\n        current_cuda = False\n        current_device = torch.device(\"cpu\")\n    \n    \n    encoder = BERTEncoder(\"roberta-base\",max_length)\n    tokenizer = encoder.tokenize\n    relation_module = L2Distance()\n   \n    model = PrototypeNetwork(\n            encoder,\n            relation_module,\n            768,\n            max_length,\n            current_device=current_device\n        )\n\n    optimizer = torch.optim.Adam\n    val_data = 'test.csv'  \n    train_data = 'train.csv'\n    N=7\n    batch_size = 2\n    K=3\n    Q=2\n    train_episodes = 300\n    val_episodes = 30\n    val_steps = 10\n    grad_steps = 1\n    lr = 1e-5\n    warmup = 0.06\n    weight_decay = 0.01\n    save_checkpoint = 'checkpoint.pt'\n    best_val_acc = train_prototype(\n            data_name, train_data, val_data, tokenizer, model, batch_size,\n            N, K, Q, optimizer, train_episodes, val_episodes,\n            val_steps, grad_steps, lr, warmup, weight_decay,\n            save_checkpoint='checkpoint.pt', cuda=current_cuda, fp16=False\n        )\n","9b922c13":"if __name__ == \"__main__\":\n    main()","95525151":"**Introduction:**\nIn this notebook we are going to explore prototypical networks for few shot learning. This technique is quite useful when we have very less data available or where labeling of data is quite expensive.  The main idea behind prototypical network is to create an overall representation of each class and classify the unknown point based on the distance between its representation and class representation. \nWe will be using Roberta base model to encode the text as it performs the best.\nI explored other techniques like Matching networks, relation networks, siamese network but prototypical networks worked the best for me.\nI have taken SVM and MLP as a benchmark to compare against prototypical networks. I found out that prototypical networks performed extremely well as compared to SVM and MLP.\nI have evaluated this on helpdesk tickets where training data has 20 classes and 10 examples for each class. Test dataset consists of 15 classes with examples each."}}