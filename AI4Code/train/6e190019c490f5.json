{"cell_type":{"eab2d0a9":"code","00df23fc":"code","cf05dba5":"code","2ff027d1":"code","ca873ba0":"code","fb2302c5":"code","3a3dee98":"code","00336c6f":"code","f538be3f":"code","769783d9":"code","7755731f":"code","fd4eaf79":"code","c2005762":"code","271ce424":"code","5e487015":"code","cefadb0c":"code","3914ef7d":"code","e5e60679":"code","27934098":"code","406ed529":"code","283cf197":"code","8925a25c":"code","a4926a91":"code","6664880f":"code","c9be5fee":"markdown","2a17e7c8":"markdown","da2927ed":"markdown","36118405":"markdown","1782c8c6":"markdown","3b795d22":"markdown","92cdd9a3":"markdown","237a4a88":"markdown"},"source":{"eab2d0a9":"import numpy as np\nimport pandas as pd\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gensim\nfrom nltk.corpus import brown\nimport random\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport gc\nfrom keras.callbacks.callbacks import EarlyStopping\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.callbacks.callbacks import EarlyStopping\nfrom scipy.stats import spearmanr\nfrom nltk.corpus import wordnet as wn\nimport tqdm\nfrom sklearn.model_selection import StratifiedKFold","00df23fc":"train = pd.read_csv(\"..\/input\/google-quest-challenge\/train.csv\")\ntest = pd.read_csv(\"..\/input\/google-quest-challenge\/test.csv\")","cf05dba5":"sample_sub = pd.read_csv(\"..\/input\/google-quest-challenge\/sample_submission.csv\")","2ff027d1":"sample_sub ","ca873ba0":"target_cols = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']","fb2302c5":"train.head()","3a3dee98":"def char_count(s):\n    return len(s)\n\ndef word_count(s):\n    return s.count(' ')\n\ntrain['question_title_n_chars'] = train['question_title'].apply(char_count)\ntrain['question_title_n_words'] = train['question_title'].apply(word_count)\ntrain['question_body_n_chars'] = train['question_body'].apply(char_count)\ntrain['question_body_n_words'] = train['question_body'].apply(word_count)\ntrain['answer_n_chars'] = train['answer'].apply(char_count)\ntrain['answer_n_words'] = train['answer'].apply(word_count)\n\ntest['question_title_n_chars'] = test['question_title'].apply(char_count)\ntest['question_title_n_words'] = test['question_title'].apply(word_count)\ntest['question_body_n_chars'] = test['question_body'].apply(char_count)\ntest['question_body_n_words'] = test['question_body'].apply(word_count)\ntest['answer_n_chars'] = test['answer'].apply(char_count)\ntest['answer_n_words'] = test['answer'].apply(word_count)\n\ntrain['question_body_n_chars'].clip(0, 5000, inplace=True)\ntest['question_body_n_chars'].clip(0, 5000, inplace=True)\ntrain['question_body_n_words'].clip(0, 1000, inplace=True)\ntest['question_body_n_words'].clip(0, 1000, inplace=True)\n\ntrain['answer_n_chars'].clip(0, 5000, inplace=True)\ntest['answer_n_chars'].clip(0, 5000, inplace=True)\ntrain['answer_n_words'].clip(0, 1000, inplace=True)\ntest['answer_n_words'].clip(0, 1000, inplace=True)\n","00336c6f":"num_question = train['question_user_name'].value_counts()\nnum_answer = train['answer_user_name'].value_counts()\n\ntrain['num_answer_user'] = train['answer_user_name'].map(num_answer)\ntrain['num_question_user'] = train['question_user_name'].map(num_question)\ntest['num_answer_user'] = test['answer_user_name'].map(num_answer)\ntest['num_question_user'] = test['question_user_name'].map(num_question)\n\n# map is done by train data, we need to fill value for user which does not appear in train data...\ntest['num_answer_user'].fillna(1, inplace=True)\ntest['num_question_user'].fillna(1, inplace=True)","f538be3f":"simple_feature_cols = [\n    'question_title_n_chars', 'question_title_n_words', 'question_body_n_chars', 'question_body_n_words',\n    'answer_n_chars', 'answer_n_words', 'num_answer_user', 'num_question_user'\n]\nsimple_engineered_feature = train[simple_feature_cols].values\nsimple_engineered_feature_test = test[simple_feature_cols].values","769783d9":"from sklearn.preprocessing import MaxAbsScaler\n\nscaler = MaxAbsScaler()\nsimple_engineered_feature = scaler.fit_transform(simple_engineered_feature)\nsimple_engineered_feature_test = scaler.transform(simple_engineered_feature_test)","7755731f":"def simple_prepro(s):\n    return [w for w in s.replace(\"\\n\",\" \").replace(\",\",\" , \").replace(\"(\",\" ( \").replace(\")\",\" ) \").\n            replace(\".\",\" . \").replace(\"?\",\" ? \").replace(\":\",\" : \").replace(\"n't\",\" not\").\n            replace(\"'ve\",\" have\").replace(\"'re\",\" are\").replace(\"'s\",\" is\").split(\" \") if w != \"\"]","fd4eaf79":"def simple_prepro_tfidf(s):\n    return \" \".join([w for w in s.lower().replace(\"\\n\",\" \").replace(\",\",\" , \").replace(\"(\",\" ( \").replace(\")\",\" ) \").\n            replace(\".\",\" . \").replace(\"?\",\" ? \").replace(\":\",\" : \").replace(\"n't\",\" not\").\n            replace(\"'ve\",\" have\").replace(\"'re\",\" are\").replace(\"'s\",\" is\").split(\" \") if w != \"\"])","c2005762":"qt_max = max([len(simple_prepro(l)) for l in list(train[\"question_title\"].values)])\nqb_max = max([len(simple_prepro(l))  for l in list(train[\"question_body\"].values)])\nan_max = max([len(simple_prepro(l))  for l in list(train[\"answer\"].values)])\nprint(\"max lenght of question_title is\",qt_max)\nprint(\"max lenght of question_body is\",qb_max)\nprint(\"max lenght of question_answer is\",an_max)","271ce424":"w2v_model = gensim.models.Word2Vec(brown.sents())","5e487015":"def get_word_embeddings(text):\n    np.random.seed(abs(hash(text)) % (10 ** 8))\n    words = simple_prepro(text)\n    vectors = np.zeros((len(words),100))\n    if len(words)==0:\n        vectors = np.zeros((1,100))\n    for i,word in enumerate(simple_prepro(text)):\n        try:\n            vectors[i]=w2v_model[word]\n        except:\n            vectors[i]=np.random.uniform(-0.01, 0.01,100)\n            #np.array([len(text)\/5000,len(words)\/1000,text.count(\"\\n\")\/10])]\n    return np.max(np.array(vectors), axis=0)\n                           ","cefadb0c":"question_title = [get_word_embeddings(l) for l in tqdm.tqdm(train[\"question_title\"].values)]\nquestion_title_test = [get_word_embeddings(l) for l in tqdm.tqdm(test[\"question_title\"].values)]\n\nquestion_body = [get_word_embeddings(l) for l in tqdm.tqdm(train[\"question_body\"].values)]\nquestion_body_test = [get_word_embeddings(l) for l in tqdm.tqdm(test[\"question_body\"].values)]\n\nanswer = [get_word_embeddings(l) for l in tqdm.tqdm(train[\"answer\"].values)]\nanswer_test = [get_word_embeddings(l) for l in tqdm.tqdm(test[\"answer\"].values)]","3914ef7d":"gc.collect()\ntfidf = TfidfVectorizer(ngram_range=(1, 3))\ntsvd = TruncatedSVD(n_components = 50)\ntfidf_question_title = tfidf.fit_transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(train[\"question_title\"].values)])\ntfidf_question_title_test = tfidf.transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(test[\"question_title\"].values)])\ntfidf_question_title = tsvd.fit_transform(tfidf_question_title)\ntfidf_question_title_test = tsvd.transform(tfidf_question_title_test)\n\ntfidf_question_body = tfidf.fit_transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(train[\"question_body\"].values)])\ntfidf_question_body_test = tfidf.transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(test[\"question_body\"].values)])\ntfidf_question_body = tsvd.fit_transform(tfidf_question_body)\ntfidf_question_body_test = tsvd.transform(tfidf_question_body_test)\n\ntfidf_answer = tfidf.fit_transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(train[\"answer\"].values)])\ntfidf_answer_test = tfidf.transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(test[\"answer\"].values)])\ntfidf_answer = tsvd.fit_transform(tfidf_answer)\ntfidf_answer_test = tsvd.transform(tfidf_answer_test)","e5e60679":"type2int = {type:i for i,type in enumerate(list(set(train[\"category\"])))}\ncate = np.identity(5)[np.array(train[\"category\"].apply(lambda x:type2int[x]))].astype(np.float64)\ncate_test = np.identity(5)[np.array(test[\"category\"].apply(lambda x:type2int[x]))].astype(np.float64)","27934098":"train_features = np.concatenate([question_title, question_body, answer,\n                                 tfidf_question_title, tfidf_question_body, tfidf_answer, \n                                 cate, simple_engineered_feature\n                                ], axis=1)\ntest_features = np.concatenate([question_title_test, question_body_test, answer_test, \n                               tfidf_question_title_test, tfidf_question_body_test, tfidf_answer_test,\n                                cate_test, simple_engineered_feature_test\n                                ], axis=1)","406ed529":"num_folds = 10\nfold_scores = []\nkf = KFold(n_splits = num_folds, shuffle = True, random_state = 42)\ntest_preds = np.zeros((len(test_features), len(target_cols)))\nfor train_index, val_index in kf.split(train_features):\n    gc.collect()\n    train_X = train_features[train_index, :]\n    train_y = train[target_cols].iloc[train_index]\n    \n    val_X = train_features[val_index, :]\n    val_y = train[target_cols].iloc[val_index]\n    \n    model = Sequential([\n        Dense(512, input_shape=(train_features.shape[1],)),\n        Activation('relu'),\n        Dense(128),\n        Activation('relu'),\n        Dense(len(target_cols)),\n        Activation('sigmoid'),\n    ])\n    \n    es = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy')\n    \n    model.fit(train_X, train_y, epochs = 100, validation_data=(val_X, val_y), callbacks = [es])\n    preds = model.predict(val_X)\n    overall_score = 0\n    for col_index, col in enumerate(target_cols):\n        overall_score += spearmanr(preds[:, col_index], val_y[col].values).correlation\/len(target_cols)\n        print(col, spearmanr(preds[:, col_index], val_y[col].values).correlation)\n    fold_scores.append(overall_score)\n    print(overall_score)\n\n    test_preds += model.predict(test_features)\/num_folds\n    \nprint(fold_scores)","283cf197":"sub = pd.read_csv(\"..\/input\/google-quest-challenge\/sample_submission.csv\")\nfor col_index, col in enumerate(target_cols):\n    sub[col] = test_preds[:, col_index]\nsub.to_csv(\"submission.csv\", index = False)","8925a25c":"test_preds","a4926a91":"sub.isna().sum()","6664880f":"import seaborn as sns\n\nfig, axes = plt.subplots(6, 5, figsize=(18, 15))\naxes = axes.ravel()\nbins = np.linspace(0, 1, 20)\n\nfor i, col in enumerate(target_cols):\n    ax = axes[i]\n    sns.distplot(train[col], label=col, bins=bins, ax=ax, color='blue')\n    sns.distplot(sub[col], label=col, bins=bins, ax=ax, color='orange')\n    # ax.set_title(col)\n    ax.set_xlim([0, 1])\nplt.tight_layout()\nplt.show()\nplt.close()","c9be5fee":"# Introduction\n\nI will add some simple features I checked in my **data analysis kernel [Google QUEST: First data introduction](https:\/\/www.kaggle.com\/corochann\/google-quest-first-data-introduction)**.\n\nExcept that, most of the code is just copy from [https:\/\/www.kaggle.com\/hukuda222\/tfidf-swem-approach](https:\/\/www.kaggle.com\/hukuda222\/tfidf-swem-approach) by @hukuda222.\nThis kernel is based on TFIDF+NN model(https:\/\/www.kaggle.com\/ryches\/tfidf-benchmark ).\n\n> I will add new information to TFIDF+NN model(https:\/\/www.kaggle.com\/ryches\/tfidf-benchmark ).<br>\n> TFIDF can create features based on actual vocabulary, but it can't handle well when there is another word of close meaning.<br>\n> Therefore, I thought that adding SWEM(https:\/\/arxiv.org\/abs\/1805.09843) using learned word2vec as a feature value would increase the score.\n\nSince I only added some codes from forked kernel, **please upvote original kernel as well :)**.","2a17e7c8":"From here on, I'm quite referring to https:\/\/www.kaggle.com\/ryches\/tfidf-benchmark.","da2927ed":"# Adding simple feature\n\n","36118405":"# Check prediction\n\nCompare train ground truth and test prediction for the distribution.","1782c8c6":"# other feature engineering\n\nBelow is just a copy of https:\/\/www.kaggle.com\/hukuda222\/tfidf-swem-approach","3b795d22":"This is basic preprocessing. This time, symbols and words are attached, so they are separated here.","92cdd9a3":"Here we use a trained word2vec model that is easily available with nltk.<br>\nWe used SWEM with max pooling.<br>","237a4a88":"The text is so long that it is difficult to apply RNN to all series."}}