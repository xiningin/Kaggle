{"cell_type":{"44e7d47b":"code","b7b0f25c":"code","0c76ac3e":"code","975ec8bc":"code","0c1d9db0":"code","0e1ba708":"code","d2f71b22":"code","8421ac14":"code","39582705":"code","3a3972ae":"code","884482a6":"code","d99ba745":"code","f56e71b3":"code","94926fcd":"code","16f91229":"code","f72dd9df":"code","7b838675":"code","df77afa8":"code","3f2ae7fb":"code","2178afb9":"code","3baafc02":"code","cbd8bfe2":"code","130a9541":"code","b2134dc7":"code","3a5236e2":"code","7568a490":"code","bd182220":"code","8b3eca63":"code","a62a7cf6":"code","8c303394":"code","f0fa2362":"markdown"},"source":{"44e7d47b":"import pandas as pd\nfrom tqdm import tqdm_notebook\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nimport gc\nfrom xgboost import XGBRegressor\n\ndef browser_mapping(x):\n    browsers = ['chrome','safari','firefox','internet explorer','edge','opera','coc coc','maxthon','iron']\n    if x in browsers:\n        return x.lower()\n    elif  ('android' in x) or ('samsung' in x) or ('mini' in x) or ('iphone' in x) or ('in-app' in x) or ('playstation' in x):\n        return 'mobile browser'\n    elif  ('mozilla' in x) or ('chrome' in x) or ('blackberry' in x) or ('nokia' in x) or ('browser' in x) or ('amazon' in x):\n        return 'mobile browser'\n    elif  ('lunascape' in x) or ('netscape' in x) or ('blackberry' in x) or ('konqueror' in x) or ('puffin' in x) or ('amazon' in x):\n        return 'mobile browser'\n    elif '(not set)' in x:\n        return x\n    else:\n        return 'others'\n    \n    \ndef adcontents_mapping(x):\n    if  ('google' in x):\n        return 'google'\n    elif  ('placement' in x) | ('placememnt' in x):\n        return 'placement'\n    elif '(not set)' in x or 'nan' in x:\n        return x\n    elif 'ad' in x:\n        return 'ad'\n    else:\n        return 'others'\n    \ndef source_mapping(x):\n    if  ('google' in x):\n        return 'google'\n    elif  ('youtube' in x):\n        return 'youtube'\n    elif '(not set)' in x or 'nan' in x:\n        return x\n    elif 'yahoo' in x:\n        return 'yahoo'\n    elif 'facebook' in x:\n        return 'facebook'\n    elif 'reddit' in x:\n        return 'reddit'\n    elif 'bing' in x:\n        return 'bing'\n    elif 'quora' in x:\n        return 'quora'\n    elif 'outlook' in x:\n        return 'outlook'\n    elif 'linkedin' in x:\n        return 'linkedin'\n    elif 'pinterest' in x:\n        return 'pinterest'\n    elif 'ask' in x:\n        return 'ask'\n    elif 'siliconvalley' in x:\n        return 'siliconvalley'\n    elif 'lunametrics' in x:\n        return 'lunametrics'\n    elif 'amazon' in x:\n        return 'amazon'\n    elif 'mysearch' in x:\n        return 'mysearch'\n    elif 'qiita' in x:\n        return 'qiita'\n    elif 'messenger' in x:\n        return 'messenger'\n    elif 'twitter' in x:\n        return 'twitter'\n    elif 't.co' in x:\n        return 't.co'\n    elif 'vk.com' in x:\n        return 'vk.com'\n    elif 'search' in x:\n        return 'search'\n    elif 'edu' in x:\n        return 'edu'\n    elif 'mail' in x:\n        return 'mail'\n    elif 'ad' in x:\n        return 'ad'\n    elif 'golang' in x:\n        return 'golang'\n    elif 'direct' in x:\n        return 'direct'\n    elif 'dealspotr' in x:\n        return 'dealspotr'\n    elif 'sashihara' in x:\n        return 'sashihara'\n    elif 'phandroid' in x:\n        return 'phandroid'\n    elif 'baidu' in x:\n        return 'baidu'\n    elif 'mdn' in x:\n        return 'mdn'\n    elif 'duckduckgo' in x:\n        return 'duckduckgo'\n    elif 'seroundtable' in x:\n        return 'seroundtable'\n    elif 'metrics' in x:\n        return 'metrics'\n    elif 'sogou' in x:\n        return 'sogou'\n    elif 'businessinsider' in x:\n        return 'businessinsider'\n    elif 'github' in x:\n        return 'github'\n    elif 'gophergala' in x:\n        return 'gophergala'\n    elif 'yandex' in x:\n        return 'yandex'\n    elif 'msn' in x:\n        return 'msn'\n    elif 'dfa' in x:\n        return 'dfa'\n    elif '(not set)' in x:\n        return '(not set)'\n    elif 'feedly' in x:\n        return 'feedly'\n    elif 'arstechnica' in x:\n        return 'arstechnica'\n    elif 'squishable' in x:\n        return 'squishable'\n    elif 'flipboard' in x:\n        return 'flipboard'\n    elif 't-online.de' in x:\n        return 't-online.de'\n    elif 'sm.cn' in x:\n        return 'sm.cn'\n    elif 'wow' in x:\n        return 'wow'\n    elif 'baidu' in x:\n        return 'baidu'\n    elif 'partners' in x:\n        return 'partners'\n    else:\n        return 'others'\n\ndef make_mapping(data_df):\n    data_df['device.browser'] = data_df['device.browser'].map(lambda x:browser_mapping(str(x).lower())).astype('str')\n    data_df['trafficSource.adContent'] = data_df['trafficSource.adContent'].map(lambda x:adcontents_mapping(str(x).lower())).astype('str')\n    data_df['trafficSource.source'] = data_df['trafficSource.source'].map(lambda x:source_mapping(str(x).lower())).astype('str')\n    return data_df\n\n\ndef process_device(data_df):\n    print(\"process device ...\")\n    data_df['source.country'] = data_df['trafficSource.source'] + '_' + data_df['geoNetwork.country']\n    data_df['campaign.medium'] = data_df['trafficSource.campaign'] + '_' + data_df['trafficSource.medium']\n    data_df['browser.category'] = data_df['device.browser'] + '_' + data_df['device.deviceCategory']\n    data_df['browser.os'] = data_df['device.browser'] + '_' + data_df['device.operatingSystem']\n    return data_df\n\n\ndef custom(data):\n    print('custom..')\n    data['device_deviceCategory_channelGrouping'] = data['device.deviceCategory'] + \"_\" + data['channelGrouping']\n    data['channelGrouping_browser'] = data['device.browser'] + \"_\" + data['channelGrouping']\n    data['channelGrouping_OS'] = data['device.operatingSystem'] + \"_\" + data['channelGrouping']\n    \n    for i in ['geoNetwork.city', 'geoNetwork.continent', 'geoNetwork.country','geoNetwork.metro', 'geoNetwork.networkDomain', 'geoNetwork.region','geoNetwork.subContinent']:\n        for j in ['device.browser','device.deviceCategory', 'device.operatingSystem', 'trafficSource.source']:\n            data[i + \"_\" + j] = data[i] + \"_\" + data[j]\n    \n    data['content.source'] = data['trafficSource.adContent'] + \"_\" + data['source.country']\n    data['medium.source'] = data['trafficSource.medium'] + \"_\" + data['source.country']\n    return data\n\ndef process(data):\n    return custom(process_device(make_mapping(data)))\n    \n","b7b0f25c":"target_key = 'totals.transactionRevenue'","0c76ac3e":"train = pd.read_csv('..\/input\/create-extracted-json-fields-dataset\/extracted_fields_train.gz', \n                    dtype={'date': str, 'fullVisitorId': str, 'sessionId':str}, nrows=None)\ntest = pd.read_csv('..\/input\/create-extracted-json-fields-dataset\/extracted_fields_test.gz', \n                   dtype={'date': str, 'fullVisitorId': str, 'sessionId':str}, nrows=None)\ntrain.shape, test.shape","975ec8bc":"train['totals.transactionRevenue'] = train['totals.transactionRevenue'].fillna(0)","0c1d9db0":"#Loading external data\ntrain_store_1 = pd.read_csv('..\/input\/exported-google-analytics-data\/Train_external_data.csv',\n                            low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\ntrain_store_2 = pd.read_csv('..\/input\/exported-google-analytics-data\/Train_external_data_2.csv',\n                            low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\ntest_store_1 = pd.read_csv('..\/input\/exported-google-analytics-data\/Test_external_data.csv',\n                           low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\ntest_store_2 = pd.read_csv('..\/input\/exported-google-analytics-data\/Test_external_data_2.csv',\n                           low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})","0e1ba708":"for df in [train_store_1, train_store_2, test_store_1, test_store_2]:\n    df[\"visitId\"] = df[\"Client Id\"].apply(lambda x: x.split('.', 1)[1]).astype(str)\n\ntrain_exdata = pd.concat([train_store_1, train_store_2], sort=False)\ntest_exdata = pd.concat([test_store_1, test_store_2], sort=False)","d2f71b22":"for df in [train, test, train_exdata, test_exdata]:\n    df[\"visitId\"] = df[\"visitId\"].astype(float).astype(int)\n\n# Merge with train\/test data\ntrain_new = train.merge(train_exdata, how=\"left\", on=\"visitId\")\ntest_new = test.merge(test_exdata, how=\"left\", on=\"visitId\")\n\n# Drop Client Id\nfor df in [train_new, test_new]:\n    df.drop(\"Client Id\", 1, inplace=True)\n    \n#Cleaning Revenue\nfor df in [train_new, test_new]:\n    df[\"Revenue\"].fillna('$', inplace=True)\n    df[\"Revenue\"] = df[\"Revenue\"].apply(lambda x: x.replace('$', '').replace(',', ''))\n    df[\"Revenue\"] = pd.to_numeric(df[\"Revenue\"], errors=\"coerce\")\n    df[\"Revenue\"].fillna(0.0, inplace=True)\n    df[\"Revenue\"] = np.log1p(df[\"Revenue\"])\n    df['is_high_hits'] = np.logical_or(df[\"totals.hits\"]>4,df[\"totals.pageviews\"]>4).astype(np.int32)\n    df['views\/hits']=df[\"totals.pageviews\"]\/df[\"totals.hits\"].dropna(0)","8421ac14":"%%time\ntrain = process(train_new)\ntest = process(test_new)\ndel train_new, test_new","39582705":"excluded_features = [\n    'date', 'fullVisitorId', 'sessionId', 'totals.transactionRevenue', \n    'visitId', 'visitStartTime'\n]\n\ncategorical_features = [\n    _f for _f in train.columns\n    if (_f not in excluded_features) & (train[_f].dtype == 'object')\n]","3a3972ae":"def freq_encoding(column):\n    counter = column.value_counts()\n    counter \/= counter.sum()\n    return counter","884482a6":"def mean_encoding(column, target, alpha = 0.01):\n    global_mean = target.mean()\n    gb = target.groupby(column)\n    mean = gb.mean()\n    count = gb.count()\n    mean_adj = (mean * count + global_mean * alpha) \/ (count + alpha)\n    return mean_adj","d99ba745":"from sklearn.model_selection import GroupKFold\ndef get_folds(df=None, n_splits=5):\n    \"\"\"Returns dataframe indices corresponding to Visitors Group KFold\"\"\"\n    # Get sorted unique visitors\n    unique_vis = np.array(sorted(df['fullVisitorId'].unique()))\n\n    # Get folds\n    folds = GroupKFold(n_splits=n_splits)\n    fold_ids = []\n    ids = np.arange(df.shape[0])\n    for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n        fold_ids.append(\n            [\n                ids[df['fullVisitorId'].isin(unique_vis[trn_vis])],\n                ids[df['fullVisitorId'].isin(unique_vis[val_vis])]\n            ]\n        )\n\n    return fold_ids","f56e71b3":"def get_subfolds(train, fold_idx, n_subfolds):\n    subfolds = get_folds(df=train.loc[fold_trn_idx], n_splits=n_subfolds)\n    for i in range(len(subfolds)):\n        for j in range(len(subfolds[i])):\n            subfolds[i][j] = fold_trn_idx[subfolds[i][j]]\n    return subfolds","94926fcd":"n_folds = 5\nn_subfolds = 3\n\nsubfolds = []\n\nfolds = get_folds(df=train, n_splits=n_folds)\nfor fold_trn_idx, fold_val_idx in tqdm_notebook(folds):\n    subfolds_cur = get_subfolds(train, fold_val_idx, n_subfolds)\n    subfolds.append(subfolds_cur)\n    '''for subfold_train_idx, subfold_val_idx in subfolds_cur:\n        for cat_name in categorical_features:\n            target = train.loc[subfold_train_idx, target_key]\n            column = train.loc[subfold_train_idx, cat_name].fillna(0)\n            mapping = mean_encoding(column, target)\n            column_to = train.loc[subfold_val_idx, cat_name].fillna(0)\n            train.loc[subfold_val_idx, 'mean_' + cat_name] = mapping[column_to].values'''\n            ","16f91229":"'''for cat_name in tqdm_notebook(categorical_features, leave=False):            \n    target = train[target_key]\n    column = train[cat_name].fillna(0)\n    mapping = mean_encoding(column, target)\n    column_to = test[cat_name].fillna(0)\n    test['mean_' + cat_name] = mapping[column_to].values'''","f72dd9df":"for column in tqdm_notebook(categorical_features):\n    train_col = train[column].fillna(0)\n    test_col = test[column].fillna(0)\n    mapping =  freq_encoding(pd.concat([train_col,test_col]))\n    train['freq_' + column] = mapping[train_col].values\n    test['freq_' + column] = mapping[test_col].values","7b838675":"for f in tqdm_notebook(categorical_features):\n    train[f], indexer = pd.factorize(train[f])\n    test[f] = indexer.get_indexer(test[f])","df77afa8":"y_reg = train[target_key]","3f2ae7fb":"train['target'] = y_reg\nfor df in [train, test]:\n    df['vis_date'] = pd.to_datetime(df['visitStartTime'], unit='s')\n    df['sess_date_dow'] = df['vis_date'].dt.dayofweek\n    df['sess_date_hours'] = df['vis_date'].dt.hour\n    df['sess_date_dom'] = df['vis_date'].dt.day\n    df.sort_values(['fullVisitorId', 'vis_date'], ascending=True, inplace=True)\n    df['next_session_1'] = (\n        df['vis_date'] - df[['fullVisitorId', 'vis_date']].groupby('fullVisitorId')['vis_date'].shift(1)\n    ).astype(np.int64) \/\/ 1e9 \/\/ 60 \/\/ 60\n    df['next_session_2'] = (\n        df['vis_date'] - df[['fullVisitorId', 'vis_date']].groupby('fullVisitorId')['vis_date'].shift(-1)\n    ).astype(np.int64) \/\/ 1e9 \/\/ 60 \/\/ 60\n    \n    df['nb_pageviews'] = df['date'].map(\n        df[['date', 'totals.pageviews']].groupby('date')['totals.pageviews'].sum()\n    )\n        \ny_reg = train['target']\ndel train['target']","2178afb9":"del train['vis_date']","3baafc02":"train_features = [_f for _f in train.columns if _f not in excluded_features]\n\nimportances = pd.DataFrame()\noof_reg_preds = np.zeros(train.shape[0])\nsub_reg_preds = np.zeros(test.shape[0])\nfor fold_, (trn_, val_) in enumerate(tqdm_notebook(folds)):\n    trn_x, trn_y = train[train_features].iloc[trn_], y_reg.iloc[trn_]\n    val_x, val_y = train[train_features].iloc[val_], y_reg.iloc[val_]\n    \n    reg = lgb.LGBMRegressor(\n        num_leaves=31,\n        learning_rate=0.03,\n        n_estimators=600,\n        subsample=.9,\n        colsample_bytree=.9,\n        random_state=1\n    )\n    reg.fit(\n        trn_x, np.log1p(trn_y),\n        eval_set=[(val_x, np.log1p(val_y))],\n        early_stopping_rounds=50,\n        verbose=100\n    )\n    imp_df = pd.DataFrame()\n    imp_df['feature'] = train_features\n    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n    \n    imp_df['fold'] = fold_ + 1\n    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n    \n    oof_reg_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n    oof_reg_preds[oof_reg_preds < 0] = 0\n    _preds = reg.predict(test[train_features], num_iteration=reg.best_iteration_)\n    _preds[_preds < 0] = 0\n    sub_reg_preds += np.expm1(_preds) \/ len(folds)\n    \nmean_squared_error(np.log1p(y_reg), oof_reg_preds) ** .5","cbd8bfe2":"mean_squared_error(np.log1p(y_reg), oof_reg_preds) ** .5","130a9541":"train['predictions'] = np.expm1(oof_reg_preds)\ntest['predictions'] = sub_reg_preds\ntrn_data = train[train_features + ['fullVisitorId']].groupby('fullVisitorId').mean()\n\ntrn_pred_list = train[['fullVisitorId', 'predictions']].groupby('fullVisitorId')\\\n    .apply(lambda df: list(df.predictions))\\\n    .apply(lambda x: {'pred_'+str(i): pred for i, pred in enumerate(x)})\n\ntrn_all_predictions = pd.DataFrame(list(trn_pred_list.values), index=trn_data.index)\ntrn_feats = trn_all_predictions.columns\ntrn_all_predictions['t_mean'] = np.log1p(trn_all_predictions[trn_feats].mean(axis=1))\ntrn_all_predictions['t_std'] = np.log1p(trn_all_predictions[trn_feats].std(axis=1))\ntrn_all_predictions['t_median'] = np.log1p(trn_all_predictions[trn_feats].median(axis=1))\ntrn_all_predictions['t_sum_log'] = np.log1p(trn_all_predictions[trn_feats]).sum(axis=1)\ntrn_all_predictions['t_sum_act'] = np.log1p(trn_all_predictions[trn_feats].fillna(0).sum(axis=1))\ntrn_all_predictions['t_nb_sess'] = trn_all_predictions[trn_feats].isnull().sum(axis=1)\nfull_data = pd.concat([trn_data, trn_all_predictions], axis=1)\ndel trn_data, trn_all_predictions\ngc.collect()","b2134dc7":"%%time\nsub_pred_list = test[['fullVisitorId', 'predictions']].groupby('fullVisitorId')\\\n    .apply(lambda df: list(df.predictions))\\\n    .apply(lambda x: {'pred_'+str(i): pred for i, pred in enumerate(x)})\nsub_data = test[train_features + ['fullVisitorId']].groupby('fullVisitorId').mean()\ndel test\nsub_all_predictions = pd.DataFrame(list(sub_pred_list.values), index=sub_data.index)\n\nfor f in trn_feats:\n    if f not in sub_all_predictions.columns:\n        sub_all_predictions[f] = np.nan\nsub_all_predictions['t_mean'] = np.log1p(sub_all_predictions[trn_feats].mean(axis=1))\nsub_all_predictions['t_std'] = np.log1p(sub_all_predictions[trn_feats].std(axis=1))\nsub_all_predictions['t_median'] = np.log1p(sub_all_predictions[trn_feats].median(axis=1))\nsub_all_predictions['t_sum_log'] = np.log1p(sub_all_predictions[trn_feats]).sum(axis=1)\nsub_all_predictions['t_sum_act'] = np.log1p(sub_all_predictions[trn_feats].fillna(0).sum(axis=1))\nsub_all_predictions['t_nb_sess'] = sub_all_predictions[trn_feats].isnull().sum(axis=1)\nsub_full_data = pd.concat([sub_data, sub_all_predictions], axis=1)\ndel sub_data, sub_all_predictions\ngc.collect()\nsub_full_data.shape","3a5236e2":"train['target'] = y_reg\ntrn_user_target = train[['fullVisitorId', 'target']].groupby('fullVisitorId').sum()\ndel train\ngc.collect()","7568a490":"len(full_data)","bd182220":"xgb_params = {\n        'objective': 'reg:linear',\n        'booster': 'gbtree',\n        'learning_rate': 0.02,\n        'max_depth': 22,\n        'min_child_weight': 57,\n        'gamma' : 1.45,\n        'alpha': 0.0,\n        'lambda': 0.0,\n        'subsample': 0.67,\n        'colsample_bytree': 0.054,\n        'colsample_bylevel': 0.50,\n        'n_jobs': -1,\n        'random_state': 456\n    }","8b3eca63":"%%time\n#I don't use xgboost in kernel, because i have some problems with kernel commit\nfolds = get_folds(df=full_data[['totals.pageviews']].reset_index(), n_splits=5)\n\noof_preds = np.zeros(full_data.shape[0])\noof_preds1 = np.zeros(full_data.shape[0])\nboth_oof = np.zeros(full_data.shape[0])\nsub_preds = np.zeros(sub_full_data.shape[0])\nvis_importances = pd.DataFrame()\n\nfor fold_, (trn_, val_) in enumerate(tqdm_notebook(folds)):\n    trn_x, trn_y = full_data.iloc[trn_], trn_user_target['target'].iloc[trn_]\n    val_x, val_y = full_data.iloc[val_], trn_user_target['target'].iloc[val_]\n    \n    #xg = XGBRegressor(**xgb_params, n_estimators=1000)\n    \n    reg = lgb.LGBMRegressor(\n        num_leaves=31,\n        learning_rate=0.03,\n        n_estimators=100,\n        subsample=.9,\n        colsample_bytree=.9,\n        random_state=1\n    )\n    reg.fit(\n        trn_x, np.log1p(trn_y),\n        eval_set=[(trn_x, np.log1p(trn_y)), (val_x, np.log1p(val_y))],\n        eval_names=['TRAIN', 'VALID'],\n        early_stopping_rounds=50,\n        eval_metric='rmse',\n        verbose=1000\n    )\n    #xg.fit(\n    #    trn_x, np.log1p(trn_y),\n    #    eval_set=[(trn_x, np.log1p(trn_y)), (val_x, np.log1p(val_y))],\n    #    early_stopping_rounds=50,\n    #    eval_metric='rmse',\n    #    verbose=100\n    #)\n    \n    imp_df = pd.DataFrame()\n    imp_df['feature'] = trn_x.columns\n    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n\n    imp_df['fold'] = fold_ + 1\n    vis_importances = pd.concat([vis_importances, imp_df], axis=0, sort=False)\n\n    oof_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n    #oof_preds1[val_] = xg.predict(val_x)\n\n    oof_preds[oof_preds < 0] = 0\n    #oof_preds1[oof_preds1 < 0] = 0\n\n    both_oof[val_] = oof_preds[val_]# * 0.6 + oof_preds1[val_] * 0.4\n\n    # Make sure features are in the same order\n    _preds = reg.predict(sub_full_data[full_data.columns], num_iteration=reg.best_iteration_)\n    _preds[_preds < 0] = 0\n\n    #pre = xg.predict(sub_full_data[full_data.columns])\n    #pre[pre<0]=0\n\n    sub_preds += (_preds \/ len(folds))# * 0.6 + (pre \/ len(folds)) * 0.4\n    \nmean_squared_error(np.log1p(trn_user_target['target']), oof_preds) ** .5","a62a7cf6":"mean_squared_error(np.log1p(trn_user_target['target']), oof_preds) ** .5","8c303394":"sub_full_data['PredictedLogRevenue'] = sub_preds\n\nidx = list(set(sub_full_data.index).intersection(set(full_data.index)))\na = full_data.loc[idx]['Revenue']\ndiff = np.expm1(a[a>-10]) - trn_user_target['target'].loc[idx][a>-10] \/ 5e5\ndiff[diff < 0] = 0\nsub_full_data['PredictedLogRevenue'].loc[diff.index] = np.log1p(diff * 5e5)\n\nsub_full_data[['PredictedLogRevenue']].to_csv('new_test_leak_xgb.csv', index=True)","f0fa2362":"Use dataleak + categorical features encoded with frequency encoding, mean encoding and OHE, lightgbm on session level, lightgbm+xgb on visitior level\nI use https:\/\/www.kaggle.com\/ogrellier\/i-have-seen-the-future, https:\/\/www.kaggle.com\/satian\/story-of-a-leak"}}