{"cell_type":{"7c8ab026":"code","fbbcac57":"code","e9b58c75":"code","fa397058":"code","54abb558":"code","e1e9b6f3":"code","6ecdeeb7":"code","8d1e73fd":"code","9a9ef376":"code","50549808":"markdown","58884e35":"markdown","d85c7051":"markdown","5382db72":"markdown","f034d4b9":"markdown","e9c773f6":"markdown","247fb08f":"markdown","25e3b582":"markdown"},"source":{"7c8ab026":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","fbbcac57":"heart = pd.read_csv(\"..\/input\/heart.csv\")\nheart.head()","e9b58c75":"heart.info()","fa397058":"plt.figure(figsize = (12,10))\nsns.heatmap(heart.corr(),annot = True)\nplt.show()","54abb558":"x = heart.drop(\"target\",axis = 1)\ny = heart.target.values","e1e9b6f3":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 42)\nprint(\"x_train : \",x_train.shape)\nprint(\"x_test : \",x_test.shape)\nprint(\"y_train : \",y_train.shape)\nprint(\"y_test : \",y_test.shape)","6ecdeeb7":"class LogReg():\n    def forward_backward_propagation(self,w,b,x_train,y_train):\n        x_train = x_train.T #(13,242)\n        y_train = y_train.T #(242,)\n        w = w.T     # if we want to multiply two matrix,first matrix column number and second matrix row number must be equal.\n        z = np.dot(w,x_train)+b # This is our model\n        y_head = 1\/(1+np.exp(-z)) # Sigmuid function\n        loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head) \n        cost = np.sum(loss)\/x_train.shape[1] # cost function\n        \n        dw = np.dot(x_train,(y_head-y_train).T)\/x_train.shape[1] #derivative weight\n        db = np.sum(y_head-y_train)\/x_train.shape[1] #derivative bias\n        \n        gradients = {\"dw\":dw,\"db\":db}\n        \n        return cost,gradients\n    def fit(self,w,b,x_train,y_train,learn_rate,num_it):\n        #To reach most fit model we should update our model\n        self.cost_list = []\n        for i in range(num_it):\n            cost,gradients = self.forward_backward_propagation(w,b,x_train,y_train)\n            self.cost_list.append(cost)\n            \n            w = w - learn_rate*gradients[\"dw\"]# update weight\n            b = b - learn_rate*gradients[\"db\"]#update bias\n        #Last parameters\n        self.w = w\n        self.b = b\n        self.cost = self.cost_list[-1]\n    def predict(self,x_test):\n        x_test = x_test.T\n        z = np.dot(self.w,x_test)+self.b\n        y_head = 1\/(1+np.exp(-z))\n        for i in range(len(y_head)):\n            if y_head[i] <= 0.5:\n                y_head[i] = 0\n            else:\n                y_head[i] = 1\n        return y_head\n    def score(self,x_test,y_test):\n        y_head = self.predict(x_test)\n        return 1-np.mean(np.abs(y_head-y_test))\n\n    \nw = np.full((x_train.shape[1]),0.01)#initial weight values\nb = 0.0 #initial bias value\n\nfrom sklearn.preprocessing import minmax_scale\nx_train = minmax_scale(x_train)#Normalize data\nx_test = minmax_scale(x_test)# (x-min(x))\/(max(x)-min(x))\n\nlog_reg = LogReg()\nlog_reg.fit(w,b,x_train,y_train,2,300)\npredict = log_reg.score(x_test,y_test)\npredict2 = log_reg.score(x_train,y_train)\nprint(\"Test Accurary : {}\".format(predict))\nprint(\"Train Accurary : {}\".format(predict2))","8d1e73fd":"plt.figure(figsize = (10,7))\nplt.plot(np.arange(len(log_reg.cost_list)),np.array(log_reg.cost_list))#Change of cost function\nplt.title(\"Change of Cost Function\")\nplt.ylabel(\"Value of Cost Function\")\nplt.xlabel(\"Number of Iteration\")\nplt.show()","9a9ef376":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\npredict = lr.score(x_test,y_test)\npredict2 = lr.score(x_train,y_train)\nprint(\"Test Accurary : {}\".format(predict))\nprint(\"Train Accurary : {}\".format(predict2))","50549808":"# Introduction\nAim : Predict person has heart disease or not<br\/>\n      Study and Learn Logistic Regression\n* EDA\n* Logistic Regression\n* Logistic Regression with Sklearn","58884e35":"# Split Data","d85c7051":"* Data has target column\n* This is input (dependent variable)\n* Target has two unique value 1 and 0\n* 1 -> person has heart disease\n* 0 -> person has no heart disease","5382db72":"Okey,let's write logistic regression with own code.","f034d4b9":"# Logistic Regression with Sklearn","e9c773f6":"# EDA","247fb08f":"You see this is very easy with sklearn library","25e3b582":"This is good because there is no missing value."}}