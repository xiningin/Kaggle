{"cell_type":{"e83a55fc":"code","2061577a":"code","866e2ad4":"code","47ade31c":"code","f8819c61":"code","d1d39ebe":"code","0110ac9a":"code","f7880fea":"code","313c5e07":"code","532213a4":"code","71c41a81":"code","42c4b350":"code","61729172":"code","493296d8":"code","7d40f968":"code","a0a89bb1":"code","6cbd220b":"code","94fee9ef":"code","b5f94fa1":"code","2d150449":"code","0d3558f6":"code","edf4237d":"code","ce1948e1":"code","abf196ac":"code","4c39c267":"code","94c727b7":"code","e17587fe":"code","ed1c6ce7":"markdown","673ab664":"markdown","cc5989aa":"markdown","e2c7b587":"markdown","9347105a":"markdown","9c792587":"markdown"},"source":{"e83a55fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2061577a":"data_path = '..\/input\/tabular-playground-series-aug-2021'\ntrain = pd.read_csv(os.path.join(data_path + '\/train.csv'))\ntest = pd.read_csv(os.path.join(data_path + '\/test.csv'))\nsample_submission = pd.read_csv(os.path.join(data_path + '\/sample_submission.csv'))\nprint(train.shape, test.shape)","866e2ad4":"train.head()","47ade31c":"# num unique value\nnum_unique = {}\nfor i in train.columns:\n    num_unique[i] = train[i].nunique()\n#num_unique","f8819c61":"num_unique = dict(sorted(num_unique.items(), key = lambda x: x[1]))\nlist(num_unique.items())[:3]","d1d39ebe":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.kdeplot(train.loss)","0110ac9a":"train.describe()","f7880fea":"y = train.pop('loss')","313c5e07":"from sklearn.model_selection import train_test_split\nx_train,x_valid,y_train,y_valid = train_test_split(train,y, train_size = 0.8)\n","532213a4":"from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(x_train,y_train)","71c41a81":"from sklearn.metrics import mean_squared_error\npred = lin_reg.predict(x_train)\nnp.sqrt(mean_squared_error(pred,y_train))\n","42c4b350":"pred = lin_reg.predict(x_valid)\nnp.sqrt(mean_squared_error(pred,y_valid))","61729172":"%%time\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import KFold, cross_validate\n\nreg = XGBRegressor(n_estimators=5000,\n                   max_depth=5,\n                   objective=\"reg:squarederror\",\n                   tree_method=\"gpu_hist\",\n                   learning_rate=0.1,\n                   gamma=10)\n\neval_set = [(x_valid,y_valid)]\nfit_params = {\n    \"eval_set\": eval_set,\n    \"eval_metric\": \"rmse\",\n    \"early_stopping_rounds\": 100,\n    \"verbose\": False,\n}\n\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 42)\nscores = cross_validate(\n    reg,\n    x_train,\n    y_train,\n    cv=kfold,\n    scoring=\"neg_mean_squared_error\",\n    return_estimator=True,\n    n_jobs=-1,\n)","493296d8":"rmse = np.sqrt(-scores[\"test_score\"].mean())\nprint(f\"Base RMSE: {rmse:.5f}\")","7d40f968":"final_xgb = (\n    pd.DataFrame(scores).sort_values(\"test_score\", ascending=False)[\"estimator\"].iloc[0]\n)\nfinal_xgb","a0a89bb1":"final_xgb.fit(x_train,y_train)","6cbd220b":"np.sqrt(mean_squared_error(final_xgb.predict(x_valid), y_valid))","94fee9ef":"importance = final_xgb.get_booster().get_score(importance_type = 'gain')","b5f94fa1":"list(dict(sorted(importance.items(), key = lambda x:x[1])))[-3:]","2d150449":"import lightgbm as lgbm","0d3558f6":"def calc_model_importance(model, feature_names=None, importance_type='gain'):\n    importance_df = pd.DataFrame(model.feature_importance(importance_type=importance_type),\n                                 index=feature_names,\n                                 columns=['importance']).sort_values('importance')\n    return importance_df\ndef plot_importance(importance_df, title='',\n                    save_filepath=None, figsize=(8, 12)):\n    fig, ax = plt.subplots(figsize=figsize)\n    importance_df.plot.barh(ax=ax)\n    if title:\n        plt.title(title)\n    plt.tight_layout()\n    if save_filepath is None:\n        plt.show()\n    else:\n        plt.savefig(save_filepath)\n    plt.close()","edf4237d":"params = {\n      \"objective\": \"rmse\", \n      \"metric\": \"rmse\", \n      \"boosting_type\": \"gbdt\",\n      'early_stopping_rounds': 30,\n      'learning_rate': 0.1,\n      #'lambda_l1': 1,\n      'lambda_l2': 1,\n      'feature_fraction': 0.8,\n      'bagging_fraction': 0.8,\n       'n_jobs':-1,\n        'verbose':-1\n    \n  }","ce1948e1":"kfold = KFold(n_splits = 5, shuffle = True, random_state = 43)\nmodels = []\nscores = 0.0\ngain_importance_list = []\n","abf196ac":"\nfor fold, (train_idx,val_idx) in enumerate(kfold.split(train,y)):\n    print('Fold: ',fold)\n    x_train, y_train = train.loc[train_idx], y[train_idx]\n    x_val, y_val = train.loc[val_idx], y[val_idx]\n    lgbm_train = lgbm.Dataset(x_train,y_train)\n    lgbm_val = lgbm.Dataset(x_val,y_val,reference = lgbm_train)\n    model = lgbm.train(params=params,\n                      train_set=lgbm_train,\n                      valid_sets=[lgbm_train, lgbm_val],\n                      num_boost_round=5000,         \n                      #feval=feval_RMSPE,\n                      verbose_eval=100,\n                      categorical_feature = ['id']                \n                     )\n    y_pred = model.predict(x_val, num_iteration=model.best_iteration)\n    RMSE = np.sqrt(mean_squared_error(y_true = y_val, y_pred = y_pred))\n    print('RMSE:',RMSE)\n    scores += RMSE \/ 5\n    models.append(model)\n    print(\"*\" * 100)\n   \n    feature_names = x_train.columns.values.tolist()\n    gain_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='gain')\n    gain_importance_list.append(gain_importance_df)\n","4c39c267":"def calc_mean_importance(importance_df_list):\n    mean_importance = np.mean(\n        np.array([df['importance'].values for df in importance_df_list]), axis=0)\n    mean_df = importance_df_list[0].copy()\n    mean_df['importance'] = mean_importance\n    return mean_df\n\nmean_gain_df = calc_mean_importance(gain_importance_list)\nplot_importance(mean_gain_df, title='Model feature importance by gain')\nmean_gain_df = mean_gain_df.reset_index().rename(columns={'index': 'feature_names'})\nmean_gain_df.to_csv('gain_importance_mean.csv', index=False)","94c727b7":"targets = np.zeros(len(test))\nfor model in models:\n    pred = model.predict(test, num_iteration = model.best_iteration)\n    targets += pred\/len(models)","e17587fe":"sample_submission['loss'] = targets\nsample_submission.to_csv('submission.csv', index = False)","ed1c6ce7":"## XGBoost","673ab664":"## Observe feature importance","cc5989aa":"### LightGBM\n","e2c7b587":"## Linear Regression","9347105a":"## Make prediction","9c792587":"## Modeling"}}