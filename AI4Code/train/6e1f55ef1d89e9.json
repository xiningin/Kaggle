{"cell_type":{"6516016a":"code","69e4c333":"code","4d9cdb72":"code","c06249cc":"code","6bc1ea58":"code","2b691b34":"code","bbdf74ed":"code","c377e66d":"code","224be825":"code","10fc97e0":"code","89dea0bf":"code","d69dd642":"code","aebb364c":"code","bbd27fd9":"code","71bf324a":"code","ea06545e":"code","0cc65799":"code","d4b35105":"code","819de911":"code","156fa3ac":"code","6f2a2b20":"code","78eebb71":"code","eaca63ec":"code","901950fe":"code","ef36e5c1":"code","cc73c7ce":"code","a4e4c405":"code","fc5e0350":"code","55fabdb4":"code","fe9bb7b4":"code","4e97047b":"code","f72d7145":"code","f44b83f2":"code","ae5abbfb":"code","75338f15":"code","53c73ea4":"code","43acd098":"code","c3155c70":"code","83bbeea1":"code","f78dae1e":"code","aed5e284":"code","917d53eb":"code","e4db792f":"code","cf21b823":"code","c6e57379":"code","f94760ad":"code","dc39a046":"code","a624bc55":"code","cb8945d5":"code","769cbae7":"code","af69c638":"code","bad26a17":"markdown","f1a52c51":"markdown","9d719f00":"markdown"},"source":{"6516016a":"# Need to install pyspark bc it is not available\n!pip install pyspark","69e4c333":"# Libraries\nimport pandas as pd\nimport math\nfrom pyspark import SparkContext, SparkConf\n#import os\n#print(os.listdir(\"..\/input\"))","4d9cdb72":"def perform_check(result,print_limit):\n    \"\"\"\n    Takes result of Mapping to a RDD\n    and prints a certain amount of lines\n    \"\"\"\n    limit = print_limit\n    count = 0\n    for x in result.collect():\n        count = count + 1\n        print(x)\n        if count == limit:\n            break","c06249cc":"def get_length(result):\n    \"\"\"\n    Takes result of Mapping to a RDD\n    and prints a certain amount of lines\n    \"\"\"\n    count = 0\n    for x in result.collect():\n        count = count + 1\n    print(count)\n    return count","6bc1ea58":"# A Quick Look At Our Data\nsource = pd.read_table(\"..\/input\/project2_data.txt\",header=None)\nsource[0] = source[0].str.split()\nsource[\"doc\"] = source[0].apply(lambda x: x[0])\nsource[\"terms\"] = source[0].apply(lambda x: x[1:])\nsource.head()","2b691b34":"# Set app name and master for spark\nappName = \"TFIDF\"\nmaster = \"local\"\nconf = SparkConf().setAppName(appName).setMaster(master)\nsc = SparkContext(conf=conf)","bbdf74ed":"# Convert our given text file for spark\ndata_file_path = \"..\/input\/project2_data.txt\"\nproject2_data = sc.textFile(data_file_path)","c377e66d":"key_value = project2_data.map(lambda x: x.split())","224be825":"# Get the total number of documents in corpus\n# Necessary for our TF-IDF calculation\nTOTAL_DOCS = get_length(key_value)","10fc97e0":"perform_check(key_value,1)","89dea0bf":"def Filter_Terms(x):\n    \"\"\"\n    Filters all extraneous terms within the list\n    of the terms that we have to consider\n    In this case we want to get the words\n    with the following formats:\n    (1) gene_word_gene\n    (2) disease_word_disease\n    \"\"\"\n    relevant_terms = []\n    for word in x[1:]:\n        if (word.startswith(\"gene_\") and word.endswith(\"_gene\")):\n            relevant_terms.append((word,x[0]))\n        if (word.startswith(\"disease_\") and word.endswith(\"_disease\")):\n            relevant_terms.append((word,x[0]))\n    return relevant_terms","d69dd642":"word_count = key_value.flatMap(lambda x: Filter_Terms(x))\\\n.map(lambda x: (x, 1))\\\n.reduceByKey(lambda x, y: x + y)","aebb364c":"perform_check(word_count,5)","bbd27fd9":"def Get_All_Pairs(x):\n    \"\"\"\n    Gathers all word doc pairs and\n    resturctures so we have all words\n    followed by their counts for each doc\n    \"\"\"\n    docid_word_pair = x[0]\n    word_count = x[1]\n    unique_word = docid_word_pair[0]\n    docid = docid_word_pair[1]\n    return (docid, list((unique_word,word_count)))","71bf324a":"doc_word_counts = word_count.map(lambda x: Get_All_Pairs(x))\\\n.cache()\\\n.reduceByKey(lambda x, y: x + y)","ea06545e":"perform_check(doc_word_counts,5)","0cc65799":"def CreateTuple(x):\n    \"\"\"\n    Gathers word and their counts\n    from the list and puts\n    them in tuples with the format:\n    (word,word_count)\n    This way a word and it's count\n    is explicit\n    \"\"\"\n    docid = x[0]\n    converted_list = []\n    tuple_list = x[1]\n    for i in range(0, len(tuple_list), 2):\n        converted_list.append((tuple_list[i], tuple_list[i+1]))\n    return (docid, converted_list)","d4b35105":"tuple_result = doc_word_counts.map(lambda x: CreateTuple(x))","819de911":"perform_check(tuple_result,1)","156fa3ac":"def WordCountPerDoc(x):\n    \"\"\"\n    Gets word and its document pair and reports \n    the occurences of the word in the document and the total\n    number of words in the document in the following\n    format:\n    ((word,doc),(word occurences,total # of words in doc))\n    \"\"\"\n    list_ = []\n    docid = x[0]\n    list_of_tuples = x[1]\n    number_of_terms_in_doc = 0\n    for each_tuple in list_of_tuples:\n        number_of_terms_in_doc += each_tuple[1]\n    for each in list_of_tuples:\n        unique_word = each[0]\n        word_occurences = each[1]\n        list_.append(\n            (\n                (unique_word, docid),\n                (word_occurences, number_of_terms_in_doc)\n            )\n        )\n    return list_","6f2a2b20":"word_count_per_doc= tuple_result.flatMap(lambda x: WordCountPerDoc(x))","78eebb71":"perform_check(word_count_per_doc,5)","eaca63ec":"def All_Doc_Word_Count_Pairs(x):\n    \"\"\"\n    Get all word counts and the total\n    counts for each document within our\n    database\n    \"\"\"\n    word_and_doc = x[0]\n    word = word_and_doc[0]\n    docid = word_and_doc[1]\n    word_count_and_total_word_in_doc = x[1]\n    word_count = word_count_and_total_word_in_doc[0]\n    total_word_count = word_count_and_total_word_in_doc[1]\n    return (word, (docid, word_count, total_word_count)) ","901950fe":"word_per_doc = word_count_per_doc.map(lambda x: All_Doc_Word_Count_Pairs(x))\\\n.cache()\\\n.reduceByKey(lambda x, y: x + y)","ef36e5c1":"perform_check(word_per_doc,1)","cc73c7ce":"def All_Word_Count_Pairs(x):\n    \"\"\"\n    From list forms tuples\n    of word and every document it appears\n    and the total number words in that document\n    \"\"\"\n    list_ = []\n    word = x[0]\n    tuple_list = x[1]\n    for i in range(0,len(tuple_list),3):\n        list_.append((tuple_list[i], tuple_list[i+1], tuple_list[i+2]))\n    return (word, list_)","a4e4c405":"all_doc_word_counts= word_per_doc.map(lambda x: All_Word_Count_Pairs(x))","fc5e0350":"perform_check(all_doc_word_counts,1)","55fabdb4":"def CountDocsPerWord(x):\n    \"\"\"\n    Determines the number of documents\n    a term appears in\n    \"\"\"\n    list_ = []\n    docsPerWord = 0\n    word = x[0]\n    tuple_list = x[1]\n    for each in tuple_list:\n        docsPerWord += 1\n    for each in tuple_list:\n        docid = each[0]\n        word_count = each[1]\n        total_w_count = each[2]\n        list_.append(\n            (\n                (word, docid),\n                (word_count, total_w_count, docsPerWord)\n            )\n        )\n    return list_","fe9bb7b4":"docs_per_word_result = all_doc_word_counts.flatMap(lambda x: CountDocsPerWord(x))","4e97047b":"perform_check(docs_per_word_result,5)","f72d7145":"def TFIDF(x,total_docs):\n    \"\"\"\n    Calculates the term-frequency inverse document frequency\n    for each term\n    \"\"\"\n    term_name = x[0][0]\n    second_tuple = x[1]\n    term_word_count = second_tuple[0]\n    all_word_count = second_tuple[1]\n    docs_with_term = second_tuple[2]\n    term_frequency = term_word_count \/ all_word_count\n    inverse_doc_frequency = math.log(total_docs\/docs_with_term)\n    tfidf = term_frequency * inverse_doc_frequency\n    return (term_name, tfidf)    ","f44b83f2":"tfidf_result = docs_per_word_result.map(lambda x: TFIDF(x,TOTAL_DOCS)).groupByKey()","ae5abbfb":"perform_check(tfidf_result,3)","75338f15":"# Gathers all calculated tfidf by\n# term\ntfidf = tfidf_result.cache().map(lambda x: (x[0], list(x[1])))","53c73ea4":"perform_check(tfidf,1)","43acd098":"def GetQueryVector(x, query_term):\n    \"\"\"\n    Will return the vector for the query term\n    \"\"\"\n    if x[0] == query_term:\n        return True\n    return False","c3155c70":"query_term = 'gene_nmdars_gene'","83bbeea1":"query_vector = tfidf.filter(lambda x: GetQueryVector(x, query_term))","f78dae1e":"perform_check(query_vector,1)","aed5e284":"def FilterOutQuery(x):\n    \"\"\"\n    Filter out query terms\n    \"\"\"\n    if x[0] == query_term:\n        return False\n    else:\n        return True","917d53eb":"cartesian_filter = tfidf.filter(lambda x: FilterOutQuery(x))\\\n.cache()\\\n.cartesian(query_vector)\\","e4db792f":"perform_check(cartesian_filter,1)","cf21b823":"def SemanticSimilarity(x):\n    \"\"\"\n    Calculates the Semantic Similarity\n    for all term-term pairs\n    \"\"\"\n    A_vector = x[0][1]\n    B_vector = x[1][1]\n    A_denominator = 0\n    B_denominator = 0\n    A_B_denominator = 0\n    A_B_numerator = 0\n    semantic_similarity = 0\n    \n    #calculates the denominator part for the A vector \n    for i in range(0, len(A_vector), 1):\n        A_denominator += A_vector[i] * A_vector[i]\n\n    A_denominator = math.sqrt(A_denominator)\n    \n    #calculates the denominator part for the B vector\n    for i in range(0, len(B_vector), 1):\n        B_denominator += B_vector[i] * B_vector[i]\n\n    B_denominator = math.sqrt(B_denominator)\n    \n    #makes the vectors equal sized in order \n    #to allow multiplication of both vectors\n    if len(B_vector) <= len(A_vector):\n        difference = len(A_vector) - len(B_vector)\n        for i in range(difference):\n            B_vector.append(0)\n    elif len(A_vector) <= len(B_vector):\n        difference = len(B_vector) - len(A_vector)\n        for i in range(difference):\n            A_vector.append(0)\n\n    #multiplies each element of A and B to find the numerator \n    #of the semantic similarity formula\n    for i in range(len(A_vector)):\n        A_B_numerator += A_vector[i] * B_vector[i]\n    \n    #calculates the denominator of the semantic similarity formula\n    A_B_denominator = A_denominator * B_denominator\n    \n    #output is ((A-term, B-term), semantic similarity)\n    return (x[1][0], x[0][0]), A_B_numerator\/A_B_denominator","c6e57379":"semantic_result = cartesian_filter.map(lambda x: SemanticSimilarity(x))\\\n.map(lambda x: (x[1], x[0]))\\\n.sortByKey(False)\\\n.map(lambda x: (x[1], x[0]))\\","f94760ad":"perform_check(semantic_result,1)","dc39a046":"final_output = semantic_result.collect()","a624bc55":"len(final_output)","cb8945d5":"final_output[:5]","769cbae7":"def Write_To_File(x):\n    \"\"\"\n    Taking our final output\n    and writes it file\n    \"\"\"\n    list_ = x\n    file = open(\"Final_Output.txt\", \"w\")\n    file.write('\\n'.join('%s %s' % x for x in list_))","af69c638":"Write_To_File(final_output)","bad26a17":"# Term Frequency Inverse Document Frequency\n![Term Frequency functions](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/0\/05\/Plot_IDF_functions.png)\nTF-IDF is a numerial statistic which symbolizes how important a word is in a document from\na collection of documents. In our context, we are using it first to determine each term's relevance within our collection of documents then using it again to determine similarity scores between term pairs","f1a52c51":"# Task\nGiven M documents compute the term-term relevance and for output return the term pairs and their similarity score descending\n\nTo calculate term-term relevance:\n1.  Calculate tfidf of every term\n2.  Compute and sort term-term relevance between a term and other terms\n","9d719f00":"Authors:\n# [Gael Blanchard](https:\/\/github.com\/gaelblanchard)\n# [Lloyd Massiah](https:\/\/github.com\/lazypassion)"}}