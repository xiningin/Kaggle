{"cell_type":{"57539175":"code","23c833d7":"code","60642c17":"code","3e83bfab":"code","44cdb251":"code","d64ed1be":"code","670f5dc3":"code","6859f9e8":"code","2a3d4c86":"code","ca071cf6":"code","32ff9404":"code","7a044a98":"code","331644a2":"code","6a3fcd96":"code","3eb1df4c":"code","48fb6f30":"code","47c75d40":"code","85fc61d0":"code","c9308e8f":"code","544860b0":"code","87da7428":"code","08a89946":"code","d39fef9c":"code","b3285b42":"code","6b313b68":"code","a4cfcb1e":"markdown","a044ee69":"markdown","7f2dd1b4":"markdown","24cf5c00":"markdown","a620d00f":"markdown","77e5f543":"markdown","88d9bf94":"markdown","d4b96b91":"markdown","14194204":"markdown","175753b8":"markdown","fa0d2202":"markdown","7e6d74ce":"markdown","d5e8d687":"markdown","5f00c19a":"markdown","60c2d790":"markdown","772cfdb4":"markdown","e3276c69":"markdown","796f6852":"markdown","6640d343":"markdown","47daba4f":"markdown","8cfbc1d2":"markdown","f225bfa5":"markdown","ca50d67d":"markdown","cdc5686e":"markdown","d87f3802":"markdown","47d47e55":"markdown","8941b804":"markdown","9289c687":"markdown","26185519":"markdown"},"source":{"57539175":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport pandas as pd\n\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.datasets import fashion_mnist\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.models import load_model","23c833d7":"\n((X_train, Y_train), (X_test, Y_test)) = fashion_mnist.load_data()","60642c17":"class_labels = pd.Series(['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Code', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot'])\nlabels_dict = class_labels.to_dict()\nlabels_dict","3e83bfab":"np.random.seed(40)\nfor rand_num in np.random.randint(0, len(X_train), 5):\n    plt.figure()\n    plt.imshow(X_train[rand_num]), plt.axis('off')\n    plt.title(labels_dict[Y_train[rand_num]])","44cdb251":"X_train_reshaped = X_train.reshape(len(X_train), -1)   \nX_test_reshaped = X_test.reshape(len(X_test), -1)\n\nX_train_norm = X_train_reshaped\/255            \nX_test_norm = X_test_reshaped\/255","d64ed1be":"n_features = X_train_norm.shape[1]\nn_classes =  len(class_labels)\n\nprint('Number of input features (image pixels) : ', n_features)\nprint('Number of target classes (fashion categories) : ', n_classes)","670f5dc3":"Y_train_onehot = to_categorical(Y_train, num_classes = n_classes)\nY_test_onehot = to_categorical(Y_test, num_classes = n_classes)","6859f9e8":"X_train_final, X_valid, Y_train_final, Y_valid = train_test_split(X_train_norm, Y_train_onehot, \n                                                                  test_size=0.16666)\n\nprint('Shape of data used for training, and shape of training targets : \\n ', X_train.shape, ',', Y_train.shape)\nprint('Shape of data used for validation, and shape of validation targets: \\n ', X_valid.shape, ',', Y_valid.shape)","2a3d4c86":"model = Sequential()\nmodel.add(Dense(1000, input_dim = n_features, activation='relu', use_bias=False))\nmodel.add(Dense(1000, activation='relu', use_bias=False))\nmodel.add(Dense(500, activation='relu', use_bias=False))\nmodel.add(Dense(200, activation='relu', use_bias=False))\nmodel.add(Dense(n_classes, activation='softmax', use_bias=False))\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam(0.0001), metrics=['accuracy'])\n\nmodel.summary()","ca071cf6":"save_at = \"\/kaggle\/working\/model.hdf5\"\nsave_best = ModelCheckpoint (save_at, monitor='val_accuracy', verbose=0, \n                             save_best_only=True, save_weights_only=False, mode='max')","32ff9404":"history = model.fit( X_train_final, Y_train_final, \n                    epochs = 12, batch_size = 20, \n                    callbacks=[save_best], verbose=1, \n                    validation_data = (X_valid, Y_valid) )\n\nplt.figure(figsize=(6, 5))\nplt.plot(history.history['accuracy'], color='r')\nplt.plot(history.history['val_accuracy'], color='b')\nplt.title('Model Accuracy', weight='bold', fontsize=16)\nplt.ylabel('accuracy', weight='bold', fontsize=14)\nplt.xlabel('epoch', weight='bold', fontsize=14)\nplt.ylim(0.5, 1)\nplt.xticks(weight='bold', fontsize=12)\nplt.yticks(weight='bold', fontsize=12)\nplt.legend(['train', 'val'], loc='upper left', prop={'size': 14})\nplt.grid(color = 'y', linewidth='0.5')\nplt.show()","7a044a98":"score = model.evaluate(X_test_norm, Y_test_onehot, verbose=0)\nprint('Accuracy over the test set: \\n ', round((score[1]*100), 2), '%')","331644a2":"Y_pred = np.round(model.predict(X_test_norm))\n\nnp.random.seed(50)\nfor rand_num in np.random.randint(0, len(Y_test_onehot), 5):\n    plt.figure()\n    plt.imshow(X_test[rand_num]), plt.axis('off')\n    if np.where(Y_pred[rand_num] == 1)[0].sum() == np.where(Y_test_onehot[rand_num] == 1)[0].sum():\n        plt.title(labels_dict[np.where(Y_pred[rand_num] == 1)[0].sum()], color='g')\n    else :\n        plt.title(labels_dict[np.where(Y_pred[rand_num] == 1)[0].sum()], color='r')","6a3fcd96":"trained_model = load_model(\"\/kaggle\/working\/model.hdf5\")\n\ntrained_model.layers","3eb1df4c":"total_no_layers = len(trained_model.layers)\nprint(total_no_layers)","48fb6f30":"#Pruning percentages\n\nK = [0, 25, 50, 60, 70, 80, 90, 95, 97, 99]   ","47c75d40":"all_weights = {}\n\nfor layer_no in range(total_no_layers - 1):         #All except the final layer                                                                          #only the first four dense layers are to be pruned\n    layer_weights = (pd.DataFrame(trained_model.layers[layer_no].get_weights()[0]).stack()).to_dict() \n    layer_weights = { (layer_no, k[0], k[1]): v for k, v in layer_weights.items() }\n    all_weights.update(layer_weights)","85fc61d0":"all_weights_sorted = {k: v for k, v in sorted(all_weights.items(), key=lambda item: abs(item[1]))}","c9308e8f":"total_no_weights = len(all_weights_sorted) \ntotal_no_weights ","544860b0":"weight_pruning_scores = []\n\nfor pruning_percent in K:\n\n    new_model = load_model(\"\/kaggle\/working\/model.hdf5\")\n    new_weights = trained_model.get_weights().copy()\n\n    prune_fraction = pruning_percent\/100\n    number_of_weights_to_be_pruned = int(prune_fraction*total_no_weights)\n    weights_to_be_pruned = {k: all_weights_sorted[k] for k in list(all_weights_sorted)[ :  number_of_weights_to_be_pruned]}     \n\n    for k, v in weights_to_be_pruned.items():\n        new_weights[k[0]][k[1], k[2]] = 0\n\n    for layer_no in range(total_no_layers - 1) :\n        new_layer_weights = new_weights[layer_no].reshape(1, new_weights[layer_no].shape[0], new_weights[layer_no].shape[1])\n        new_model.layers[layer_no].set_weights(new_layer_weights)\n    \n    new_score  = new_model.evaluate(X_test_norm, Y_test_onehot, verbose=0)\n    weight_pruning_scores .append(new_score[1])","87da7428":"all_neurons = {}\n\nfor layer_no in range(total_no_layers - 1):         \n\n    layer_neurons = {}\n    layer_neurons_df = pd.DataFrame(trained_model.layers[layer_no].get_weights()[0])\n\n    for i in range(len(layer_neurons_df.columns)):\n        layer_neurons.update({ i : np.array( layer_neurons_df.iloc[:,i] ) })    \n                                                                 \n    layer_neurons = { (layer_no, k): v for k, v in layer_neurons.items() }\n    all_neurons.update(layer_neurons)","08a89946":"all_neurons_sorted = {k: v for k, v in sorted(all_neurons.items(), key=lambda item: np.linalg.norm(item[1], ord=2, axis=0))}","d39fef9c":"total_no_neurons = len(all_neurons_sorted) \ntotal_no_neurons ","b3285b42":"neuron_pruning_scores = []\n\nfor pruning_percent in K:\n\n    new_model = load_model(\"\/kaggle\/working\/model.hdf5\")\n    new_weights = trained_model.get_weights().copy()\n\n    prune_fraction = pruning_percent\/100\n    number_of_neurons_to_be_pruned = int(prune_fraction*total_no_neurons)\n    neurons_to_be_pruned = {k: all_neurons_sorted[k] for k in list(all_neurons_sorted)[ : number_of_neurons_to_be_pruned]}     \n\n    for k, v in neurons_to_be_pruned.items():\n        new_weights[k[0]][:, k[1]] = 0\n\n    for layer_no in range(total_no_layers - 1) :\n        new_layer_weights = new_weights[layer_no].reshape(1, new_weights[layer_no].shape[0], new_weights[layer_no].shape[1])\n        new_model.layers[layer_no].set_weights(new_layer_weights)\n    \n    new_score  = new_model.evaluate(X_test_norm, Y_test_onehot, verbose=0)\n    neuron_pruning_scores.append(new_score[1])","6b313b68":"plt.figure(figsize=(8, 4))\nplt.plot(pd.DataFrame(weight_pruning_scores).set_index(pd.Series(K), drop=True) , color='r')\nplt.plot(pd.DataFrame(neuron_pruning_scores).set_index(pd.Series(K), drop=True) , color='b')\nplt.title('Effect of Pruning on accuracy', weight='bold', fontsize=16)\nplt.ylabel('Score', weight='bold', fontsize=14)\nplt.xlabel('Pruning Percentage (K)', weight='bold', fontsize=14)\nplt.xticks(weight='bold', fontsize=12)\nplt.yticks(weight='bold', fontsize=12)\nplt.legend(['Weight Pruning', 'Neuron Pruning'], loc='best', prop={'size': 14})\nplt.grid(color = 'y', linewidth='0.5')\nplt.show()","a4cfcb1e":"# 1. What is Pruning?","a044ee69":"Pruning is a way to reduce the size of a neural network by using compression. After a network is trained, it's weights are ranked according to their importance. A percentage of the weights are pruned away (as in set to zero), in accordance with their importance. We test the pruned model on a test set and if the performance still holds out, we prune more. \n\nWe keep pruning until the performance remains satisfactory. The aim is to find the level of pruining that provides us with an acceptable compromise in performance for a decent reduction in the network size.\n\nThere are two basic approaches of Pruning:\n\n* Weight Pruning\n\nHere the individual weights are ranked are ranked according to their magnitudes and the ones with small values are deemed less important. \n\n* Neuron pruining\n\nHere we rank entire neurons (units) according to their [L2 norm](https:\/\/en.wikipedia.org\/wiki\/Norm_(mathematics). The weights accociated with any neuron exist as columns in the weight matrices of a network. By setting entire columns to zero, we can essentially prune away entire neurons.\n\nFollowing figure is a good repesentative diagram. Note that a synapse refers to the connection between any two neurons. When a weight is pruned, essentially this connection disappears. And when all weights associated with a neuron are pruned, the neuron gets a sum total input of zero, so basically it does nothing, i.e disappears.","7f2dd1b4":"# 6. Weight Pruning","24cf5c00":"### Libraries and Modules","a620d00f":"We will now plot 5 images at random from test set, but with titles as classified by the model, with every correct classification titled in 'green' color, and every incorrect classification titles in 'red' color.","77e5f543":"### Reshaping input data, and normalizing it","88d9bf94":"We will start with zero pruning and graudally increase the level of pruning up unit 99%, and keep checking the pruned model's performance after every increase.","d4b96b91":"### One-Hot transformation of Target Data","14194204":"Fetch 5 images at random from the training data, plot them and fetch their corresponding labels from the training targets.","175753b8":"![](https:\/\/i.imgur.com\/ya0PWGI.jpg)","fa0d2202":"We will reshape the image data from 28x28 to 784x1, and normalize all values between 0 and 1.","7e6d74ce":"Create a dictionary of all classes in the target","d5e8d687":"# 4. Neural Network set-up","5f00c19a":"# 7. Neuron Pruning","60c2d790":"### Testing \nLet's check the model's score on the test set","772cfdb4":"### Training of Neural Network","e3276c69":"### Saving the model while it trains","796f6852":"# 9. Observations\n\nWe are able to prune upto 60% of the weights with a tolerable dip in accuracy.\n\nWe are able to prune upto 15% of the neurons with a tolerable dip in accuracy.\n\nFor weight pruining as high a 80%, the accuracy of the pruned model is still decent. \n\nThis suggests that Neural Networks are highly robust. Even if a large protion of the less significant weights are removed, the other significant weights are still able to generate a good enough result.","6640d343":"We need to separate out a validation set from the training set, that is to be used while traing the network to keep an eye on overfitting ot underfitting.","47daba4f":"### Creation of a Validation set","8cfbc1d2":"We will use five dense layers. We will set 'bias' to false for now, since that will make the weight matrices easier to work with. The weights of the final dense layers (the one leading to the output nodes) will not be pruned. But first, let's create the model and train it.","f225bfa5":"By using 'Callback' and 'ModelCheckpoint'utilities of Keras, we can save the model with the best weights. By 'best', I mean the ones that gave the best performance over the validation set while training. It checks if the performance of model with updated weights after every epoch is better than the performance of the saved model.","ca50d67d":"### Fashion MNIST dataset","cdc5686e":"# 8. Pruning Results","d87f3802":"# 2. Importing Utilities","47d47e55":"# 3. Data preprocessing","8941b804":"The strategy is simple. The weights of the network will be accessed layer wise. Each layer has two weight matrices - first one for link weights, and the second one for bias weights of every neuron of that layer. Since we are not using biases, we only have to worry about one matrix per layer for now.\n\nFor weight pruning, we will create a dictionary of the weights, with every key being a three valued tuple. The first value having the layer number, second and third values having the row and column number from that layer's weight matrix.\n\nEach row number is in fact the neuron number from previous layer, and each column number is the neuron number of that particular layer. So, for neuron pruning, unlike weight pruining where we want to create a weights dictionary, here we will have a dictionary that contains column arrays. The key will be a two valued tuple. First value representing the layer number, and the second one representing the column number.\n\nWe will sort the weights in the weights dictionary according to their absolute values, and the weight vectors in the neuron dictionary according to their L2 norm.\n\nWe will then map the weights (or columns) from the sorted dictionaries on to the trained neural network and set the weights (or columns) to zero to obtain our compresed Neural Networks (with the set pruing level).","9289c687":"We will use the Fashion-MNIST dataset and train a vanilla Neural Network on it. Then we will apply various levels of pruning on it, and then test the pruned versions on the test set.\n\nNote - Trun on the internet in the settings of the Kernel.","26185519":"# 5. Pruning Strategy"}}