{"cell_type":{"9d764206":"code","a324fea1":"code","01f6a0b5":"code","d0259bbd":"code","0f7bba60":"code","fdf95e37":"code","1bf87344":"code","9f6ea8e8":"code","0929e582":"code","de63ab39":"code","da49e3e5":"code","a857cf6c":"code","0695a019":"code","56e1a2fa":"code","ae4f2e4c":"code","38f4bd49":"code","47641ec0":"code","f32b5529":"code","c4b05994":"code","54813599":"code","ddb3541d":"code","f47019b2":"code","8ec4c5be":"code","8427be3b":"code","328bb438":"code","048a7f3d":"code","d3fa5aa8":"code","46f312f4":"code","528f99c8":"markdown","a00adb7e":"markdown","20cba7d9":"markdown","d8e9b1be":"markdown","378a913d":"markdown","6ed99415":"markdown","aa416dfe":"markdown","b1894e12":"markdown","9bb7f474":"markdown","647cb151":"markdown","57fafcd5":"markdown","f5e8b6c8":"markdown","eede79cd":"markdown","5e2893c8":"markdown","b127336b":"markdown","34632d06":"markdown","056690d1":"markdown","573771ef":"markdown","87aeea62":"markdown","b543c535":"markdown","14039509":"markdown","af615203":"markdown","57b920b8":"markdown","d528a9a6":"markdown","0e015b1b":"markdown","7e8d6bd0":"markdown"},"source":{"9d764206":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a324fea1":"#Visualization\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap \n#Libraries for ML\nfrom sklearn.preprocessing import StandardScaler #Standardizasyon i\u00e7in\nfrom sklearn.model_selection import train_test_split, GridSearchCV #GridSearchCV: KNN ile ilgili en iyi parametreleri belirlemek\nfrom sklearn.metrics import accuracy_score ,confusion_matrix #Sonu\u00e7 de\u011ferlendirme\nfrom sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis, LocalOutlierFactor #Trainin algoritmas\u0131 ve NCA ve Outlier de\u011ferler i\u00e7in\nfrom sklearn.decomposition import PCA #PCA i\u00e7in\n#Others\nimport warnings\nwarnings.filterwarnings('ignore') #Uyar\u0131lar\u0131 kapatmak","01f6a0b5":"data = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\ndata.drop(['Unnamed: 32', 'id'],inplace=True,axis=1)\n\ndata = data.rename(columns={'diagnosis':'target'})","d0259bbd":"sns.countplot(data['target']) \nprint(data.target.value_counts())\n\ndata['target'] = [1 if i.strip() == 'M' else 0 for i in data.target]\n\nprint(len(data))\nprint('Data Shape',data.shape)\nprint(data.info()) \ndescribe = data.describe()","0f7bba60":"corr_data = data.corr() \nsns.clustermap(corr_data,annot= True,fmt = '.2f')\nplt.title('Correlation Between Features')\nplt.show();","fdf95e37":"threshold = 0.75 #Bu e\u015fik de\u011feri ile sadece bu de\u011ferin \u00fcst\u00fcndeki korelasyonlar\u0131 de\u011ferlendirece\u011fiz yeni grafikte\n\nfiltre = np.abs(corr_data['target']) > threshold # Burada corelasyon de\u011ferleri negatifde olaca\u011f\u0131ndan mutlak de\u011ferini al\u0131p tresholddan b\u00fcy\u00fcklar\u0131 filtreledik.\ncorr_feature = corr_data.columns[filtre].tolist()\n#Bu de\u011fi\u015fkene ise korelasyon matrisi s\u00fctunlar\u0131na filtrenin uygulanmas\u0131yla \u00e7\u0131kan \u00e7\u0131kt\u0131lar\u0131 listeye \u00e7evirip atad\u0131k.\n\nsns.clustermap(data[corr_feature].corr(),annot= True,fmt = '.2f')\n#Buradan e\u015fik de\u011ferine uygun olarak elde edilen featurelar\u0131n uyguland\u0131\u011f\u0131 corr matr. olu\u015fturduk.\n\nplt.title('Correlation Between Features with threshold 0.75')\nplt.show();","1bf87344":"#Box p. \u00f6ncesi bir melted i\u015flemi gerekitor.\ndata_melted = pd.melt(data,id_vars='target',\n                      var_name='Features',\n                      value_name='Value')\n\nplt.figure()\nsns.boxplot(x='Features',y='Value',hue='target',data=data_melted) #Featureslar target'a g\u00f6re ayr\u0131ld\u0131.\nplt.xticks(rotation=75) #Feature isimleri 90 derece dik g\u00f6r\u00fclecek.\nplt.show()\n\n\"\"\"\nWe will need standardization.\n\"\"\"","9f6ea8e8":"sns.pairplot(data[corr_feature],diag_kind='kde',markers='+',hue='target')\nplt.show()\n\n\"\"\"\nData has skewness. We will handle it.\n\"\"\"","0929e582":"x = data.drop(['target'],axis=1) \ny = data.target\ncolumns = x.columns.tolist() # Featurelar\u0131n isimlerini bir listede toplad\u0131k.\n\nclf = LocalOutlierFactor() #KNN de\u011feri gerekiyor. Tan\u0131ml\u0131 de\u011feri 20 ve bizde 20 kullanaca\u011f\u0131z. O nedenle bir\u015fey yazmayaca\u011f\u0131z.\ny_pred = clf.fit_predict(x) #LOF uygulay\u0131o negetif outlier f. al\u0131yoruz.\n\nx_score = clf.negative_outlier_factor_\noutlier_score = pd.DataFrame()\noutlier_score['score'] = x_score\n\nthreshold_outliers = -2\nfiltre = outlier_score['score'] < threshold_outliers\noutlier_index = outlier_score[filtre].index.tolist() #outlier de\u011ferlerine threshold uygulanm\u0131\u015f olanlar\u0131 bir listeye att\u0131k\n\n\nplt.figure()\nplt.scatter(x.iloc[outlier_index,0], x.iloc[outlier_index,1],color = 'blue',s=50,label='outliers')\nplt.scatter(x.iloc[:,0]\n            ,x.iloc[:,1],color='k',s=3,label='data_point') #s : boyut\n\nradius = (x_score.max() - x_score ) \/ (x_score.max() - x_score.min() ) #De\u011ferleri normalize ederek bias\u0131 \u00f6nledik\noutlier_score['radius '] = radius\n\nplt.scatter(x.iloc[:,0], x.iloc[:,1], s=1000*radius, edgecolors='r',facecolor='none',label='Outlier skores')\nplt.legend()\nplt.show();","de63ab39":"x = x.drop(outlier_index) #outliers remove\ny = y.drop(outlier_index).values #outliers remove","da49e3e5":"test_size = 0.3\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=test_size,random_state=42)","a857cf6c":"scaler = StandardScaler()\nx_train = scaler.fit_transform(x_train) #scaler isimli bir de\u011fi\u015fkene Standartscaler metodunu atay\u0131p sonra bu de\u011fi\u015fken arac\u0131l\u0131\u011f\u0131yla xtraindeki verileri standardize ettik\n\nx_test = scaler.transform(x_test)\n\nx_train_df = pd.DataFrame(x_train,columns=columns)\nx_train_df_describe = x_train_df.describe()\nx_train_df['target'] = y_train","0695a019":"data_melted = pd.melt(x_train_df,id_vars='target',\n                      var_name='Features',\n                      value_name='Value')\n\nplt.figure()\nsns.boxplot(x='Features',y='Value',hue='target',data=data_melted) #Featureslar target'a g\u00f6re ayr\u0131ld\u0131.\nplt.xticks(rotation=75) #Feature isimleri 90 derece dik g\u00f6r\u00fclecek.\nplt.show()\n","56e1a2fa":"\nsns.pairplot(x_train_df[corr_feature],diag_kind='kde',markers='+',hue='target')\nplt.show()","ae4f2e4c":"knn = KNeighborsClassifier(n_neighbors=2)\nknn.fit(x_train, y_train) #Calculation (In the supervise learning this section = training)\ny_predict = knn.predict(x_test) #Test Section\ncm = confusion_matrix(y_test, y_predict) #Plooting\nacc = accuracy_score(y_test, y_predict) #Accuracy Score\nscore = knn.score(x_test, y_test) #acc ile buras\u0131n\u0131n sonucu ayn\u0131 \u00e7\u0131kacak. Do\u011frulama ama\u00e7l\u0131 yap\u0131l\u0131yor.\n\nprint(\"Score:\",score)\nprint(\"CM:\",cm)\nprint(\"Basic KNN Acc:\",acc)","38f4bd49":"def KNN_best_parameters(x_train,x_test,y_train,y_test):\n    \n    k_range = list(range(1,51)) #En uygun k de\u011feri buluma\n    weight_options = ['uniform','distance'] #En uygun weighti buluma\n    #manhattan_distance = 1\n    #euclidean_distance = 2\n    distance_options = [1,2] #En uygun distance type buluma\n    print()\n    param_grid = dict(n_neighbors=k_range,weights=weight_options,p=distance_options) #Aranacak parametreleri bir s\u00f6zl\u00fckte toplad\u0131k.\n\n    knn =KNeighborsClassifier() #Parametrelerin denenece\u011fi knn olu\u015fturuldu.\n    grid = GridSearchCV(knn,param_grid,cv=10,scoring='accuracy') #Parametrelerin aranmas\u0131 i\u00e7in method\n    grid.fit(x_train, y_train) #fitting ile best parm. elde edildi\n    \n    print('Best training score: {} with parametres: {}'.format(grid.best_score_,grid.best_params_))\n    print()\n    \n    knn = KNeighborsClassifier(**grid.best_params_) #Test setinde deneme i\u015flemi i\u00e7in\n    knn.fit(x_train, y_train)\n    \n    y_predict_test = knn.predict(x_test)\n    y_predict_train = knn.predict(x_train)\n\n    cm_test = confusion_matrix(y_test,y_predict_test)\n    cm_train = confusion_matrix(y_train,y_predict_train)\n\n    acc_test = accuracy_score(y_test,y_predict_test)  \n    acc_train = accuracy_score(y_train,y_predict_train)\n\n    print('Test Score: {}, Train Score: {}'.format(acc_test,acc_train))\n    print()\n    print('CM Test:',cm_test)\n    print('CM Train:',cm_train)\n    \n    return grid","47641ec0":"grid = KNN_best_parameters(x_train,x_test,y_train,y_test)","f32b5529":"basic_best_acc =grid.best_score_\nbasic_best_acc","c4b05994":"#%% PCA\n\nscale = StandardScaler()\nx_scaled = scaler.fit_transform(x) #x verisi b\u00f6l\u00fcnmeden tam bir \u015fekilde PCA i\u00e7in scale edildi. \n\npca = PCA(n_components=2) #2 componentli bir PCA olu\u015fturduk.\npca.fit(x_scaled)\nx_reduce_pca = pca.transform(x_scaled) #2feature'a yani boyuta d\u00fc\u015f\u00fcr\u00fclm\u00fc\u015f x \npca_data =pd.DataFrame(x_reduce_pca,columns=['p1','p2']) #reduce datadan incelemek i\u00e7in bir dataframe olu\u015fturuldu\npca_data['target'] = y #buna target eklendi. G\u00f6rselle\u015ftirmek i\u00e7in gerekli.\n\nsns.scatterplot(x='p1',y='p2',hue='target',data=pca_data) # targeta g\u00f6re renklendirilmi\u015f grafik\nplt.title('PCA: P1 Vs P2')","54813599":"x_train_pca, x_test_pca, y_train_pca, y_test_pca = train_test_split(x_reduce_pca,y,test_size=test_size,random_state=42)\n\ngrid_pca = KNN_best_parameters(x_train_pca, x_test_pca, y_train_pca, y_test_pca)\n#en iyi parametreleri elde etti\u011fimiz metodu PCA i\u00e7in \u00e7al\u0131\u015ft\u0131r\u0131yorum.","ddb3541d":"pca_best_acc = grid_pca.best_score_\npca_best_acc","f47019b2":"cmap_light = ListedColormap(['orange',  'cornflowerblue'])\ncmap_bold = ListedColormap(['darkorange', 'darkblue'])\n\nh = .05 # step size in the mesh\nX = x_reduce_pca\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = grid_pca.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure()\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n            edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(\"%i-Class classification (k = %i, weights = '%s')\"\n          % (len(np.unique(y)),grid_pca.best_estimator_.n_neighbors, grid_pca.best_estimator_.weights))","8ec4c5be":"nca = NeighborhoodComponentsAnalysis(n_components=2,random_state=42)\n#NCA PCA'in aksine unsupervise learning de\u011fildir y'lere yani targetlara ihtiya\u00e7 duyar. \nnca.fit(x_scaled,y)\nx_reduce_nca = nca.transform(x_scaled)\nnca_data = pd.DataFrame(x_reduce_nca,columns=['p1','p2'])\nnca_data['target'] = y\n\nsns.scatterplot(x='p1',y='p2',hue='target',data=nca_data)\nplt.title('NCA: P1 vs P2')\n","8427be3b":"x_train_nca, x_test_nca, y_train_nca, y_test_nca = train_test_split(x_reduce_nca,y,test_size=test_size,random_state=42)\n\ngrid_nca = KNN_best_parameters(x_train_nca, x_test_nca, y_train_nca, y_test_nca)","328bb438":"nca_best_acc = grid_nca.best_score_","048a7f3d":"cmap_light = ListedColormap(['orange',  'cornflowerblue'])\ncmap_bold = ListedColormap(['darkorange', 'darkblue'])\n\nh = .2 # step size in the mesh\nX = x_reduce_nca\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = grid_nca.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure()\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n            edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(\"%i-Class classification (k = %i, weights = '%s')\"\n          % (len(np.unique(y)),grid_nca.best_estimator_.n_neighbors, grid_nca.best_estimator_.weights))","d3fa5aa8":"# plotly\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport itertools\nplt.style.use('fivethirtyeight')","46f312f4":"AlgorthmsName = ['KNN_Training_Accuracy_with_Best_Parameters','KNN-PCA_Training_Accuracy_with_Best_Parameters','KNN-NCA_Training_Accuracy_with_Best_Parameters']\nscoresf1=[basic_best_acc,pca_best_acc,nca_best_acc]\n#create traces\n\ntrace1 = go.Scatter(\n    x = AlgorthmsName,\n    y= scoresf1,\n    name='Algortms Name',\n    marker =dict(color='rgba(225,126,0,0.5)',\n               line =dict(color='rgb(0,0,0)',width=2)),\n                text=AlgorthmsName\n)\ndata = [trace1]\n\nlayout = go.Layout(barmode = \"group\", \n                  xaxis= dict(title= 'Traning Type',ticklen= 5,zeroline= False),\n              yaxis= dict(title= 'Prediction Scores(Acc)',ticklen= 5,zeroline= False))\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","528f99c8":"## Wrong classification Visualization in PCA_KNN","a00adb7e":"## Feautre Visualization with Pair plot [Before Standardization]","20cba7d9":"# If you like this kernel, Please Upvote :) Thanks","d8e9b1be":"## KNN Implementation & Obtaining First Accuracy Score","378a913d":"## After PCA, obtaining KNN score the best parameters ","6ed99415":"## Outlier Detection","aa416dfe":"## KNN Algorithms PCA & NCA's Best Parameters for Prediction\n\n----Content\n\n1-Import Dataset\n\n2-Investigation Dataset Visualizaiton\n\n3-Exploratory Data Analysis\n\n4-Feature Investigation \n\n5-Drop Outliers with LocalOutlierFactor\n\n6-Standardization\n\n7- KNN Prediction with Best Parameters\n\n8- After the PCA, accuracy detection\n\n9- After the NCA, accuracy detection\n\n10- Results Evaluation","b1894e12":"## After NCA, obtaining KNN score the best parameters ","9bb7f474":"## Standardizasyon","647cb151":"## Choose Best Parameters with GridSearchCrossValidation (Function)","57fafcd5":"## Exploratory Data Analysis","f5e8b6c8":"## Wrong classification in NCA_KNN","eede79cd":"## Box Plot Visualization after the Standardization","5e2893c8":"## Train - Tespt Split","b127336b":"## Feautre Visualization with Box plot [Before Standardization]","34632d06":"## Pair Plot Visualization after the Standardization","056690d1":"# Results Evaluation","573771ef":"## Data Reading & Pre-editing","87aeea62":"## Import Libraries","b543c535":"## Correlation Matrix with threshold","14039509":"## Neighborhood Components Analysis","af615203":"**Burak Kahveci**\n\n* My Linkedin Account: https:\/\/www.linkedin.com\/in\/kahveciburak\/\n* My Twitter Account: https:\/\/twitter.com\/ImpartialBrain","57b920b8":"## Drop Outliers","d528a9a6":"## Target Class Visualization & Data Description","0e015b1b":"## Obtaining KNN score the best parameters ","7e8d6bd0":"## Principal Component Analysis"}}