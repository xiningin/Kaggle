{"cell_type":{"b926e6a5":"code","25d3c7cd":"code","4d05ad08":"code","67f8e7e8":"code","72347b4a":"code","bd3f82c2":"code","a984c5e5":"code","242932a5":"code","692f4fa5":"markdown","cdec0856":"markdown","7606a07d":"markdown"},"source":{"b926e6a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","25d3c7cd":"#imports\nfrom bs4 import BeautifulSoup\nimport urllib.request \nimport pandas as pd","4d05ad08":"#######\n### Configurable items\n\ndomain = 'https:\/\/dallas.2backpage.com'\n\n#target\ntarget_url = domain + '\/post\/escorts'\n\n#limits how many sub urls will be scanned\nlimiter = 5\n\n#list of search terms: replace this with phone numbers or any other search term\nsearch_terms = ['asian', 'Older Women \ud83d\udc9b\ud83c\udf37\ud83d\udc99 L', 'bitcoin']\n#######","67f8e7e8":"#variables \n# the page to be scraped\ntarget_url_str = str(target_url)\n# open the target page\nr = urllib.request.urlopen(target_url)\n#create the soup object\nsoup = BeautifulSoup(r, \"lxml\")\n# create an empty set\ncomp_set = [] # a placeholder for the urls found within the primary domain\nurl_collection = []","72347b4a":"def scan_for_urls(url_to_scan):\n    #compare url_collection size start to end size to determine if more were found\n    \n    for link in soup.find_all('a'):\n        if type(link.get('href')) == str and len(link.get('href')) > 1 and link.get('href')[0] != '#':\n            clean_url = (\n                link.get('href')[0: -1] \n            if link.get('href')[-1] == '\/' \n            else link.get('href'))\n        \n        focus_url = clean_url if clean_url[0:8] == 'https:\/\/' and domain in clean_url else domain + clean_url\n        if focus_url not in url_collection:\n            url_collection.append(focus_url)\n            \n    print(len(url_collection))\n    \nscan_for_urls('asd')","bd3f82c2":"         \n\n\"\"\"\nWe find web links by iterating over each anchor tag in the html code\n\"\"\"\nfor link in soup.find_all('a'):\n    \n    \"\"\"\n    We then extract the href attribute, ensure it isn't simply '\/' meaning \n    'this domain' which would diplicate of the domain. We also do not want to \n    collect hashtags, which are present for page navigation and organization.\n    An additional check for type of str is present to avoid None types.\n    \"\"\"\n    if type(link.get('href')) == str and len(link.get('href')) > 1 and link.get('href')[0] != '#':\n        \n        \"\"\"\n        Before we add any urls to the python set we should remove trailing \/ \n        to ensure that links are not duplicated. home.html\/ and home.html would\n        otherwise be viewed as two urls when they point to the same web page.\n        \"\"\"\n        clean_url = (\n            link.get('href')[0: -1] \n            if link.get('href')[-1] == '\/' \n            else link.get('href'))\n        \n        \n        \"\"\"\n        Add the clean url to the set, concat the domain where it is not present\n        \"\"\"\n        \n        #check that the url contains 'https:\/\/dallas.2backpage.com\/view\/escorts' these are the targets to open\n        if '\/view\/escorts' in clean_url:\n            comp_set.append(clean_url if clean_url[0:8] == 'https:\/\/' else domain + clean_url)\n    \n        \n# Can display the extracted links, ex) https:\/\/dallas.2backpage.com\/view\/escorts\/2519703.html\n#for rec in comp_set:\n    #print(rec)","a984c5e5":"\"\"\"\n1. Open each link in the list\n2. Check if any item in our phone list is found on the open page\n3. If a phone number is found then add that tuple {phone: url} to a suspect list\n4. move to the next item\n5. export the suspect list to csv locally\n\"\"\"\nfinal_set =[]\n\ndf = pd.DataFrame(final_set, columns=['term', 'url'])\nfor sub_url in comp_set[:limiter]:\n    r = urllib.request.urlopen(sub_url)\n    #create the soup object\n    soup = BeautifulSoup(r, \"lxml\")\n    for item in search_terms:\n        if item in soup.body.text:\n            new_row = {'term':item, 'url':sub_url}\n            #append row to the dataframe\n            df = df.append(new_row, ignore_index=True)\n\nprint(df)","242932a5":"df.to_csv('web_findings.csv', sep=',', encoding='utf-8')","692f4fa5":"# Scan a target domain and collect sub urls","cdec0856":"# Parse the sub urls found","7606a07d":"# Save the results to file"}}