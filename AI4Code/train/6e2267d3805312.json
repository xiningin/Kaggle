{"cell_type":{"f7bb780b":"code","cbdd4977":"code","b4203a69":"code","1edd12a1":"code","312e3b76":"code","bf3a25b4":"code","00ce9081":"code","4ea2a681":"code","ba9b8803":"code","7529811e":"code","2fc6989f":"code","1fdc5f64":"code","56d63781":"code","3e234e2d":"code","009ec591":"code","5b3ae11e":"code","d5c2626e":"code","e70c00cc":"code","fff98b7f":"code","ea1caebc":"code","4056bf3f":"markdown","a3661e33":"markdown","f3d42fb1":"markdown","01a8320e":"markdown","3bfcf3bd":"markdown","a68f106f":"markdown","2c770147":"markdown","becb945e":"markdown","bf5e8e6c":"markdown","cc5d10aa":"markdown","70207db0":"markdown","f0e0c711":"markdown"},"source":{"f7bb780b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport itertools\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n#train_data.head(10)","cbdd4977":"train_data[\"Name\"] = train_data[\"Name\"].str.split(',').str[1]\ntrain_data[\"Name\"] = train_data[\"Name\"].str.split('.').str[0]\ntrain_data[\"Name\"] = train_data[\"Name\"].str.strip()\nx = train_data.groupby('Name').agg(['count']).index.get_level_values('Name')\nx","b4203a69":"train_data[\"Age\"] = train_data.groupby(\"Name\").transform(lambda x: x.fillna(x.mean()))['Age']\n#changing sex to be 0 or 1 for female & male\ntrain_data['Sex'].replace({'female':0,'male':1},inplace=True)\ntrain_data.head()","1edd12a1":"train_data_tree = train_data.iloc[:,[False,False,True, False,True,True,True,True,False,True,False,False]]\ntrain_labels_tree = train_data.iloc[:,1]\ntrain_data_tree.describe()","312e3b76":"#Need to create dummy variable columns for the Pclass variable. The other variables are either binary or numeric\ntrain_data_tree_dummy = pd.concat([train_data_tree,pd.get_dummies(train_data_tree['Pclass'], prefix='Pclass')],axis=1)\n\ntrain_data_tree_dummy.drop([\"Pclass\"],axis=1,inplace=True)\nsib_sp = pd.cut(train_data_tree_dummy[\"SibSp\"], 3,labels=[0,1,2]).tolist()\nparch = pd.cut(train_data_tree_dummy[\"Parch\"], 3,labels=[0,1,2]).tolist()\n\ntrain_data_tree_dummy.drop([\"Parch\"],axis=1,inplace=True)\ntrain_data_tree_dummy[\"SibSp2\"] = np.where(train_data_tree_dummy.SibSp==0,0,1)\ntrain_data_tree_dummy.drop([\"SibSp\"],axis=1,inplace=True)\n\n\n\ntrain_data_tree_dummy.describe()","bf3a25b4":"def gini_calc(train,labels):\n    gini_list = []\n    numeric_indices = []\n    numeric_best_split = []\n    counter = -1\n    for i in train:\n        counter +=1\n        if train[i].dtype == \"int64\" or train[i].dtype ==\"uint8\":\n            split_1 = train[train[i] == 0]\n            split_2 = train[train[i] == 1]\n            split_1_index = split_1.index\n            split_2_index = split_2.index\n            labels_split_1 = labels[split_1_index]\n            labels_split_2 = labels[split_2_index]\n\n            val1 = (labels_split_1==0).sum()\n            val2 = (labels_split_1==1).sum()\n            val3 = (labels_split_2==0).sum()\n            val4 = (labels_split_2==1).sum()\n\n            gini_one = 1 - (val1\/(val1+val2))**2 - (val2\/(val1+val2))**2\n            gini_two = 1 - (val3\/(val3+val4))**2 - (val4\/(val3+val4))**2                \n            weighted_gini = (gini_one * (val1 + val2)\/(len(train_labels_tree))) + (gini_two * (val3 + val4)\/(len(train_labels_tree))) \n            gini_list.append(weighted_gini)\n        elif train[i].dtype == \"float64\":\n            numeric_indices.append(counter)\n            numeric_gini_lst = []\n            numeric_vals = np.array(train.sort_values([i])[i].reset_index(drop=True))\n            averages = (numeric_vals[0:len(numeric_vals)-1] + numeric_vals[1:len(numeric_vals)])\/2\n            zeros = np.zeros(len(labels))\n            ones = zeros + 1 \n            \n            for val in averages:\n                vals_array = ones*val\n                split_1 = train[train[i] <= vals_array]\n                split_2 = train[train[i] >= vals_array]\n                split_1_index = split_1.index\n                split_2_index = split_2.index\n                labels_split_1 = labels[split_1_index]\n                labels_split_2 = labels[split_2_index]\n                \n                val1 = (labels_split_1==0).sum()\n                val2 = (labels_split_1==1).sum()\n                val3 = (labels_split_2==0).sum()\n                val4 = (labels_split_2==1).sum()\n                \n                gini_one = 1 - (val1\/(val1+val2))**2 - (val2\/(val1+val2))**2\n                gini_two = 1 - (val3\/(val3+val4))**2 - (val4\/(val3+val4))**2                \n                weighted_gini = (gini_one * (val1 + val2)\/(len(train_labels_tree))) + (gini_two * (val3 + val4)\/(len(train_labels_tree)))     \n\n                numeric_gini_lst.append(weighted_gini)\n            \n            index_min = np.argmin(numeric_gini_lst)\n            numeric_best_split.append(numeric_vals[index_min])\n            gini_list.append(min(numeric_gini_lst))\n            \n    return gini_list, numeric_best_split, numeric_indices\nginis, numeric_splits, numeric_index = gini_calc(train_data_tree_dummy,train_labels_tree)\nprint(ginis,numeric_splits, numeric_index)","00ce9081":"numeric_splits, numeric_index","4ea2a681":"def splitter(gini_array, data,labels, numeric_splits, numeric_index):\n    \"\"\"Splits a dataframe based on the best gini value determined from the gini function\"\"\"\n    \n    best_split = np.argmin(gini_array)\n    best_gini = min(gini_array)\n    best_var = data.columns[best_split]\n    is_numeric = False\n    tester = 0\n    numeric_splitter = \"Null\"\n    \n    for i in numeric_index:\n        if best_split == i:\n            tester +=1\n    \n    if tester == 1:\n        is_numeric = True\n        numeric_splitter = numeric_splits[best_split]\n        splitter = numeric_splits[best_split]\n        combined = pd.concat([data, labels], axis=1, sort=False)\n        df1 = combined[combined.iloc[:,best_split] <= splitter]\n        df2 = combined[combined.iloc[:,best_split] > splitter]\n        df1 = df1.drop(df1.columns[best_split], axis=1)\n        df2 = df2.drop(df2.columns[best_split], axis=1)\n        \n        \n        df1_labels = df1.iloc[:,-1]\n        df1 = df1.iloc[:,:-1]\n\n        df2_labels = df2.iloc[:,-1]\n        df2 = df2.iloc[:,:-1]\n        majority_class = 0\n        if sum(df1_labels) > sum(df2_labels):\n            majority_class = 1\n        else:\n            majority_class = 0\n      \n    else:\n        combined = pd.concat([data, labels], axis=1, sort=False)\n\n        df1 = combined[combined.iloc[:,best_split] == 0]\n        df2 = combined[combined.iloc[:,best_split] == 1]\n        df1 = df1.drop(df1.columns[best_split], axis=1)\n        df2 = df2.drop(df2.columns[best_split], axis=1)   \n\n\n        df1_labels = df1.iloc[:,-1]\n        df1 = df1.iloc[:,:-1]\n\n        df2_labels = df2.iloc[:,-1]\n        df2 = df2.iloc[:,:-1]\n        \n        if sum(df1_labels) > sum(df2_labels):\n            majority_class = 1\n        else:\n            majority_class = 0\n    \n\n    return df1, df2, df1_labels, df2_labels, best_gini, best_var, is_numeric, numeric_splitter, majority_class","ba9b8803":"def decision_tree(data,labels):\n    \"\"\"Takes a dataframe and labels and applied the Gini and splitter functions to it\"\"\"\n    ginis, numeric_splits, numeric_index = gini_calc(data,labels)\n    df_1, df_2, labs_one, labs_two, best_gini, best_var, is_numeric, numeric_splitter, majority_class = splitter(ginis,data,labels, numeric_splits, numeric_index)\n    \n\n    return best_gini, df_1, df_2, labs_one, labs_two, best_var, is_numeric, numeric_splitter, majority_class\n\n\nbest_gini, df_1, df_2, labs_one, labs_two, best_var, is_numeric, numeric_splitter,majority_class = decision_tree(train_data_tree_dummy,train_labels_tree)\n\n","7529811e":"def recursive_tree(data,labels,max_depth = 5):\n    \"\"\"Function that takes original data and labels and iteratively splits each dataframe for the full max-depth\"\"\"\n    \n    best_gini, df_1, df_2, labs_one, labs_two, best_var, is_numeric, numeric_splitter, majority_class = decision_tree(data,labels)\n    \n    data_frame_splits = []\n    ginis = []\n    labs_list = []\n    best_var_list = []\n    is_numeric_lst = []\n    numer_splitter_lst = []\n    majority_class_list = []\n    data_frame_splits.append(data)\n    ginis.append(.5)\n    labs_list.append(labels)\n    best_var_list.append(\"NA\")\n    is_numeric_lst.append(\"NA\")\n    numer_splitter_lst.append(\"NA\")\n    majority_class_list.append(\"NA\")\n    \n    counter = 0\n    for i in range(max_depth*2):\n        if counter == 0:\n            best_gini, df_1, df_2, labs_one, labs_two, best_var, is_numeric, numeric_splitter, majority_class = decision_tree(data,labels)\n        else:\n            best_gini, df_1, df_2, labs_one, labs_two, best_var, is_numeric, numeric_splitter,majority_class = decision_tree(data_frame_splits[counter],labs_list[counter])\n        counter +=1\n        data_frame_splits.append(df_1)\n        data_frame_splits.append(df_2)\n        ginis.append(best_gini)\n        ginis.append(best_gini)\n        labs_list.append(labs_one)\n        labs_list.append(labs_two)\n        best_var_list.append(best_var)\n        best_var_list.append(best_var)\n        is_numeric_lst.append(is_numeric)\n        is_numeric_lst.append(is_numeric)\n        numer_splitter_lst.append(numeric_splitter)\n        numer_splitter_lst.append(numeric_splitter)\n        majority_class_list.append(majority_class)\n        if majority_class == 1:\n            majority_class_list.append(majority_class-1)\n        else:\n            majority_class_list.append(majority_class+1)\n        \n    return data_frame_splits, ginis,labs_list,best_var_list, is_numeric_lst,numer_splitter_lst,majority_class_list\n\ndata_frame_splits, ginis,labs_list,best_var_list, is_numeric_lst,numer_splitter_lst,majority_class_list = recursive_tree(train_data_tree_dummy,train_labels_tree)","2fc6989f":"def create_rules(ginis, is_numeric_lst):\n    \"\"\"Returns indices of best tree as well as relavent information indexed by these best indices\"\"\"\n    counter = 0\n    include_list = []\n    for i in range(len(ginis)):\n        if i == 0:\n            include_list.append(counter)\n        \n        elif is_numeric_lst[i] == False and counter % 2 != 0:\n            if ginis[i] <= ginis[int((counter-1)\/2)]:\n                include_list.append(counter)\n        elif is_numeric_lst[i] == False and counter % 2 == 0:\n            if ginis[i] <= ginis[int((counter-2)\/2)]:\n                 include_list.append(counter)\n        elif is_numeric_lst[i] == True and counter % 2 != 0:\n            if ginis[i] <= ginis[int((counter-1)\/2)]:\n                include_list.append(counter)\n        elif is_numeric_lst[i] == True and counter % 2 == 0:\n            if ginis[i] <= ginis[int((counter-2)\/2)]:\n                include_list.append(counter)\n        counter +=1\n    \n    best_variables = []\n    for i in include_list:\n        best_variables.append(best_var_list[i])\n    \n    is_numeric_final = []\n    for i in include_list:\n        is_numeric_final.append(is_numeric_lst[i])\n    \n    majority_class_list_final = []\n    for i in include_list:\n        majority_class_list_final.append(majority_class_list[i])\n    \n    numer_splitter_lst_final = []\n    for i in include_list:\n        numer_splitter_lst_final.append(numer_splitter_lst[i])\n    \n    \n    return include_list, is_numeric_final, best_variables, majority_class_list_final, numer_splitter_lst_final\n    \ninclusions, numeric_final, best_vars, majority_class_lst, numer_splitter_lst_final = create_rules(ginis, is_numeric_lst)\nprint(inclusions)","1fdc5f64":"def yarf(data,is_numeric_final,best_variables, inclusions,majority_class_list_final,numer_splitter_lst_final, counter):\n    \"\"\"yet another recursive function... go through the data and set a column equal to a value determined by the rules\"\"\"\n    if is_numeric_final[counter] == False:\n        x = data[data[best_variables[counter]] == 0].copy()\n        if majority_class_list_final[counter] == 1:\n            x[\"Survived\"] = 0\n        elif majority_class_list_final[counter] == 0:\n            x[\"Survived\"] = 1\n            \n        y = data[data[best_variables[counter]] == 1].copy()\n        if majority_class_list_final[counter] == 0:\n            y[\"Survived\"] = 0\n        elif majority_class_list_final[counter] == 1:\n            y[\"Survived\"] = 1\n        return x, y\n        \n    elif is_numeric_final[counter] == True:\n        x = data[data[best_variables[counter]] <= numer_splitter_lst_final[counter]].copy()\n        if majority_class_list_final[counter] == 1:\n            x[\"Survived\"] = 0\n        elif majority_class_list_final[counter] == 0:\n            x[\"Survived\"] = 1\n        y = data[data[best_variables[counter]] > numer_splitter_lst_final[counter]].copy()\n        if majority_class_list_final[counter] == 0:\n            y[\"Survived\"] = 0\n        elif majority_class_list_final[counter] == 1:\n            y[\"Survived\"] = 1\n        return x, y\n    \n\n","56d63781":"def recursive_through_yarf(data,is_numeric_final,best_variables, inclusions,majority_class_list_final,numer_splitter_lst_final, max_depth = 5):\n    \"\"\"Using the yarf function, iteratively save the dataframes with the predictions as a column\"\"\"\n    x, y = yarf(data,is_numeric_final,best_variables, inclusions,majority_class_list_final,numer_splitter_lst_final,1)\n    \n    list_of_data_frames = []\n    list_of_data_frames.append(x)\n    list_of_data_frames.append(y)\n    counter = 2\n    frames_counter = 0\n    \n    for i in range(max_depth):\n        x, y = yarf(list_of_data_frames[frames_counter],is_numeric_final,best_variables, inclusions,majority_class_list_final,numer_splitter_lst_final,counter)\n        list_of_data_frames.append(x)\n        list_of_data_frames.append(y)\n        counter+=1\n        frames_counter +=1\n        x, y = yarf(list_of_data_frames[frames_counter],is_numeric_final,best_variables, inclusions,majority_class_list_final,numer_splitter_lst_final,counter)\n        list_of_data_frames.append(x)\n        list_of_data_frames.append(y)\n        counter+=1\n        frames_counter +=1\n    \n    return list_of_data_frames\n\n\n#testing = recursive_through_yarf(train_data_tree_dummy,is_numeric_final,best_variables, inclusions,majority_class_list_final,numer_splitter_lst_final,max_depth = 3)\n    \n","3e234e2d":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data[\"Name\"] = test_data[\"Name\"].str.split(',').str[1]\ntest_data[\"Name\"] = test_data[\"Name\"].str.split('.').str[0]\ntest_data[\"Name\"] = test_data[\"Name\"].str.strip()\ntest_data[\"Age\"] = test_data.groupby(\"Name\").transform(lambda x: x.fillna(x.mean()))['Age']\n#changing sex to be 0 or 1 for female & male\ntest_data['Sex'].replace({'female':0,'male':1},inplace=True)\ntest_data_tree = test_data.iloc[:,[False,True,False, True,True,True,True,False,True,False,False]]\ntest_data_tree.head()\n\ntest_data_tree_dummy = pd.concat([test_data_tree,pd.get_dummies(test_data_tree['Pclass'], prefix='Pclass')],axis=1)\n\ntest_data_tree_dummy.drop([\"Pclass\"],axis=1,inplace=True)\nsib_sp = pd.cut(test_data_tree_dummy[\"SibSp\"], 3,labels=[0,1,2]).tolist()\nparch = pd.cut(test_data_tree_dummy[\"Parch\"], 3,labels=[0,1,2]).tolist()\ntest_data_tree_dummy[\"SibSp2\"] = np.where(test_data_tree_dummy.SibSp==0,0,1)\n\ntest_data_tree_dummy.drop([\"Parch\"],axis=1,inplace=True)\ntest_data_tree_dummy.drop([\"SibSp\"],axis=1,inplace=True)\ntest_data_tree_dummy.head()\n","009ec591":"testing = recursive_through_yarf(test_data_tree_dummy,numeric_final,best_vars, inclusions,majority_class_lst,numer_splitter_lst_final,max_depth = 5)\n","5b3ae11e":"def make_predictions(testing_data, inclusions):\n    \"\"\"When merging dataframes, only want to merge leaf nodes. This function takes all the leaf nodes and merges together\"\"\"\n    \n    for i in inclusions:\n        if i %2 !=0:\n            try:\n                inclusions.remove(int((i-1)\/2))\n            except:\n                pass\n        elif i %2 ==0:\n            try:\n                inclusions.remove(int((i-2)\/2))\n            except:\n                pass\n    inclusions2 = []\n    for i in inclusions:\n        inclusions2.append(i-1)\n        \n    dataframes_to_keep = []\n    for i in inclusions2:\n        dataframes_to_keep.append(testing_data[i])\n\n\n    preds = pd.concat(dataframes_to_keep, axis=0).sort_index(axis = 0)\n\n    return preds\n    \npreds = make_predictions(testing, inclusions)","d5c2626e":"preds.shape","e70c00cc":"test_data.shape","fff98b7f":"test_data.head()","ea1caebc":"data = {'PassengerId': test_data[\"PassengerId\"].values, 'Survived':preds[\"Survived\"].values}\n\ndf_submission = pd.DataFrame(data)\n\ndf_submission.to_csv(\"submission_decision_tree.csv\",index=False)","4056bf3f":"Final score on testing Set: 0.76076","a3661e33":"Below I am just creating the testing data so it is in the same format as the training data.","f3d42fb1":"# 3.Impurity \nFor the measure of impurity, I'm using Gini. Lower values indicate better splits, so this function calculates Gini for each column to determine the best variable to split.","01a8320e":"# 4.Splitting \n\nOnce we have a function that calculates Gini scores, we can write a function that splits data based on whichever variable has the lowest Gini. The Splitter function checks if the variable is binary or numeric and takes different operations depending on which. It then returns two split dataframes with the indices of the original dataframe in tact","3bfcf3bd":"# 5.Recursion \n\nThe next two functions basically do recursion to iteratively go through the data splitting each split to a specified amount. I am specifiying a max depth of 5 but this parameter is changeable (anything from 1-6 should work). I took a list approach to contain all of the information needed to develop the set of rules that are determined later.\n\nNote: The approach I used basically does the following: The main dataframe would be index 0. That splits into two dataframes with indices 1 and 2. Dataframe 1 then splits into two dataframes with indices 3 and 4 while Dataframe two splits into 2 dataframes with indices 5 and 6. This continues for the max depth. So there will be a full tree, then I determined which splits were actually less informative than the previous ones and created rules based on this, collecting the indices of the dataframes that actually reduced Gini.[](http:\/\/)","a68f106f":"# 8.Submission ","2c770147":"For the categorical variables, I'm transforming the columns into columns of dummy variables.","becb945e":"# 6.Pruning \nNow that I have the full tree, I want to find the indices of the dataframes that make up the best tree\n","bf5e8e6c":"# 2.Preprocessing \nThe age variable has missing data but all of the other numeric columns are fine. An approach is to group data by another variable and find the average age for each group and impute the average into the missing values. I will take group by the title of each person since people with similar title may have similar ages.","cc5d10aa":"# 1.Introduction \n\n![](http:\/\/)This is my attempt at creating a decision tree without using only numpy and pandas libraries. I did not reference other people's code or approaches; rather I watched the StatQuest video on Decision Trees (https:\/\/www.youtube.com\/watch?v=7VeUPuFGJHk) to understand the algorithm and took my own approach to code it. This was simply meant as a challenge and a learning exercise for me.","70207db0":"# 7.Predictions \nThe next function goes through the data with all of the information determined by the rules of the best tree and sets a labels column equal to value determined by the tree.\n\n","f0e0c711":"# Decision Tree: Classifying the Titanic Dataset Without Machine Learning Libraries\n\n[1. Introduction](#1.Introduction)\n\n[2. Preprocessing](#2.Preprocessing)\n\n[3. Impurity Measure](#3.Impurity)\n\n[4. Splitting Data](#4.Splitting )\n\n[5. Getting Full Tree With Recursion](#5.Recursion)\n\n[6. Pruning Full Tree](#6.Pruning)\n\n[7. Making Predictions With Pruned Tree](#7.Predictions)\n\n[8. Submission](#8.Submission)\n\n\n"}}