{"cell_type":{"cf7a17c6":"code","b03a3dbd":"code","0048c02f":"code","28d491c6":"code","e1e205bf":"code","1d2e0f41":"code","833fbf18":"code","bb65eef6":"code","378f13bc":"code","e3ab39cc":"code","eaa3cd27":"code","a41fc03f":"code","50ca674e":"code","38d294b6":"code","07a394fe":"code","d307dfc8":"code","4e7a386b":"code","0ae4f982":"code","6e09caf7":"code","aba36b12":"code","03bf453e":"code","19f1b773":"code","f2915827":"code","78368afa":"code","9134f184":"code","350ffc4e":"code","5c9b631f":"code","9b80bf9b":"code","8fa1ecb8":"code","7836be28":"code","0886463e":"code","dfb004c8":"code","2609928a":"code","99e273d1":"code","3814a629":"code","6455ea62":"code","ae2f15f0":"code","30afdcdc":"code","6db29f9d":"code","0ffe7fb3":"code","c5cd9d12":"code","0c5b88e1":"code","858f9d63":"code","3744c800":"code","2457071e":"code","72067b44":"code","19f0dadf":"code","dc5fff30":"code","6f79cefc":"code","e2f9f7f5":"code","cd885031":"code","45bfe29c":"code","09f5f77f":"markdown","f0ee9037":"markdown","8698a1a7":"markdown","bccfd8eb":"markdown","be7959f9":"markdown","c5f85e2b":"markdown","164c4f58":"markdown","90cfdccc":"markdown","abc4f73b":"markdown","5547d2e0":"markdown","ae6effb1":"markdown","7a635a02":"markdown","06c081b7":"markdown","1716cca6":"markdown","945abfd2":"markdown","2bda4158":"markdown","37183771":"markdown","70161db7":"markdown","8f9635ac":"markdown","52bca315":"markdown","eb0230ef":"markdown","5becb734":"markdown","e6525bc1":"markdown","95dc7f60":"markdown","9a96918a":"markdown"},"source":{"cf7a17c6":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport optuna\nfrom lightgbm import LGBMClassifier","b03a3dbd":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")","0048c02f":"train_data.head()","28d491c6":"test_data.head()","e1e205bf":"#submission sample\nsample_submission.head()","1d2e0f41":"train_data.info()","833fbf18":"test_data.info()","bb65eef6":"print(\" train_data num of unique\")\nprint(train_data.apply(lambda x: len(x.unique())))\n\nprint(\"\\ntest_data num of unique\")\ntest_data.apply(lambda x: len(x.unique()))","378f13bc":"df = train_data.copy()\ndf.head(2)","e3ab39cc":"#1-1\ndf[\"Survived\"].groupby(df[\"Pclass\"]).mean()","eaa3cd27":"#1-2\nsns.countplot(data=df, x=\"Pclass\", hue=\"Survived\")\nplt.show()","a41fc03f":"#2-1\ndf[\"Survived\"].groupby(df[\"Sex\"]).mean()","50ca674e":"#2-2\nsns.countplot(data=df, x=\"Sex\", hue=\"Survived\")\nplt.show()","38d294b6":"#3-1\ndf[\"Age\"].groupby(df[\"Survived\"]).mean()","07a394fe":"#3-2\nsns.displot(data=df, x=\"Age\", hue=\"Survived\")\nplt.show()","d307dfc8":"df_Age = df.loc[:,[\"Survived\",\"Age\"]]\ndf_Age.dropna(inplace=True)\ndf_Age[\"Age_s\"] = df_Age[\"Age\"].map(lambda x:str(x)[0] if x>=10 else str(0))\n\n#As in 3-1.\nprint(df_Age[\"Survived\"].groupby(df_Age[\"Age_s\"]).mean())\n#As in 3-2\nplt.figure(figsize=(12,4))\nsns.countplot(data=df_Age, x=\"Age_s\", hue=\"Survived\",order=[str(x) for x in range(9)])\nplt.show()","4e7a386b":"#3-4\ndf_Age = df.loc[:,[\"Survived\",\"Age\"]]\ndf_Age.dropna(inplace=True)\ndf_Age[\"Age_s\"] = df_Age[\"Age\"].map(lambda x:str(x)[0] if x>=10 else str(0))\ndf_Age[\"Age_s\"] = df_Age[\"Age_s\"].map(lambda x:x if int(x)<=6 else \"7<=\")\n\n#As in 3-1.\nprint(df_Age[\"Survived\"].groupby(df_Age[\"Age_s\"]).mean())\n#As in 3-2\nplt.figure(figsize=(12,4))\nsns.countplot(data=df_Age, x=\"Age_s\", hue=\"Survived\",order=[str(x) for x in range(7)]+[\"7<=\"])\nplt.show()","0ae4f982":"#4-1\ndf[\"Survived\"].groupby(df[\"SibSp\"]).mean()","6e09caf7":"#4-2\nplt.figure(figsize=(12,4))\nsns.countplot(data=train_data, x=\"SibSp\", hue=\"Survived\")\nplt.show()","aba36b12":"#4-3\ndf_SibSP = df.loc[:,[\"Survived\",\"SibSp\"]]\ngroup_ids = []\nfor num in df_SibSP[\"SibSp\"]:\n    if num==0:\n        group_ids.append(\"alone\")\n    elif (num==1) or (num==2):\n        group_ids.append(\"small\")\n    elif num >= 3:\n        group_ids.append(\"large\")\ndf_SibSP[\"Family_size\"] = group_ids\n#As in 4-1.\nprint(df_SibSP[\"Survived\"].groupby(df_SibSP[\"Family_size\"]).mean())\n#As in 4-2\nplt.figure(figsize=(12,4))\nsns.countplot(data=df_SibSP, x=\"Family_size\", hue=\"Survived\")\nplt.show()","03bf453e":"#5-1\ndf[\"Survived\"].groupby(df[\"Parch\"]).mean()","19f1b773":"#5-2\nsns.countplot(data=df, x=\"Parch\", hue=\"Survived\")\nplt.show()","f2915827":"#5-3\ndf_Parch = df.loc[:,[\"Survived\",\"Parch\"]]\ngroup_ids = []\nfor num in df_Parch[\"Parch\"]:\n    if num==0:\n        group_ids.append(\"alone\")\n    elif (num==1) or (num==2):\n        group_ids.append(\"small\")\n    elif num >= 3:\n        group_ids.append(\"large\")\n        \ndf_Parch[\"Family_size\"] = group_ids\n#As in 5-1.\nprint(df_SibSP[\"Survived\"].groupby(df_Parch[\"Family_size\"]).mean())\n#As in 5-2\nplt.figure(figsize=(12,4))\nsns.countplot(data=df_Parch, x=\"Family_size\", hue=\"Survived\")\nplt.show()","78368afa":"#6-1\ndf[\"Fare\"].groupby(df[\"Survived\"]).mean()","9134f184":"#6-2\nsns.displot(data=df, x=\"Fare\", hue=\"Survived\")\nplt.show()","350ffc4e":"#6-3\ndf_Fare = df.loc[:,[\"Survived\",\"Fare\"]]\nbins_per_10 = np.arange(0, max(df_Fare[\"Fare\"]), 10)\nids = np.digitize(df_Fare[\"Fare\"], bins_per_10)\ndf_Fare[\"Fare_bin\"] = ids\nmean_Fare_bin = df_Fare[\"Survived\"].groupby(df_Fare[\"Fare_bin\"]).mean()\nprint(\"---Fare bins per_10 value_counts---\\n\",df_Fare[\"Fare_bin\"].value_counts(),\"\\n---end---\\n\")\nprint(\"---Fare bins per_10 survived rate---\\n\",mean_Fare_bin,\"\\n---end---\\n\")\nplt.bar(x=mean_Fare_bin.index, height=mean_Fare_bin)\nplt.xlabel(\"Fare bin per_10\")\nplt.ylabel(\"Survived\")\nplt.show()","5c9b631f":"#6-4\ndf_Fare = df.loc[:,[\"Survived\",\"Fare\"]]\nnew_bins = np.array([0, 10, 30, 100, 1000])\nids = np.digitize(df_Fare[\"Fare\"], new_bins)\ndf_Fare[\"Fare_bin\"] = ids\nmean_Fare_bin = df_Fare[\"Survived\"].groupby(df_Fare[\"Fare_bin\"]).mean()\nprint(\"---Fare bins per_10 value_counts---\\n\",df_Fare[\"Fare_bin\"].value_counts(),\"\\n---end---\\n\")\nprint(\"---Fare bins per_10 survived rate---\\n\",mean_Fare_bin,\"\\n---end---\\n\")\nplt.bar(x=mean_Fare_bin.index, height=mean_Fare_bin)\nplt.xlabel(\"Fare bin [0, 10, 30, 100, 1000]\")\nplt.ylabel(\"Survived\")\nplt.show()","9b80bf9b":"#7-1\ndf[\"Survived\"].groupby(df[\"Embarked\"]).mean()","8fa1ecb8":"#7-2\nsns.countplot(data=df, x=\"Embarked\", hue=\"Survived\")\nplt.show()","7836be28":"#8-1\nsplit_num = pd.Series([len(x.split())-1 for x in df[\"Ticket\"]])\nprint(\"---split_num_length---\\n\",pd.Series([len(x.split())-1 for x in df[\"Ticket\"]]).value_counts(), \"\\n\")\nprint(\"---split_0---\\n\", df[\"Ticket\"][split_num==0].sample(10), \"\\n\")\nprint(\"---split_1---\\n\", df[\"Ticket\"][split_num==1].sample(10), \"\\n\")\nprint(\"---split_2---\\n\", df[\"Ticket\"][split_num==2].sample(10), \"\\n\")","0886463e":"#8-2\nwords = []\nfor ticket in df[\"Ticket\"]:\n    for word in ticket.split():\n        words.append(word.replace(\".\",\"\"))\ncount_df = pd.Series(words).value_counts().to_frame()\ncount_df.head(10)","dfb004c8":"#8-3\nwords_num_over5 = list(count_df[count_df[0]>=5].index)\nprint(\"over5num words\\n\",words_num_over5)\n\ndf_Ticket = df.loc[:,[\"Survived\",\"Ticket\"]]\nfor word in words_num_over5:\n    df_Ticket[word] = df_Ticket[\"Ticket\"].map(lambda x:1 if word in x.replace(\".\",\"\").split() else 0)\ndf_Ticket[\"No_target\"] = pd.Series(df_Ticket.iloc[:,2:].sum(axis=1)).map(lambda x:1 if x==0 else 0)\nprint(\"\\n---over5num words_df---\")\ndisplay(df_Ticket.head(3))\n\ncompare_ticket = pd.DataFrame(index=words_num_over5, columns=[\"Survived_0\",\"Survived_1\"])\nfor word in words_num_over5+[\"No_target\"]:\n    tmp = df_Ticket[\"Survived\"].groupby(df_Ticket[word]).mean()\n    compare_ticket.at[word,\"Survived_0\"] = tmp[0]\n    compare_ticket.at[word,\"Survived_1\"] = tmp[1]\nprint(\"\\n---over5num words_compare_df---\")\ndisplay(compare_ticket)","2609928a":"print(\"---sample Cabin---\\n\", df[\"Cabin\"][df[\"Cabin\"].isnull()==False].sample(20))\ncabin_value_counts_df = pd.Series(df[\"Cabin\"][df[\"Cabin\"].isnull()==False].map(lambda x: x[0])).value_counts()\nprint(\"---head word counts---\\n\", cabin_value_counts_df)","99e273d1":"df_Cabin = df.loc[:,[\"Survived\",\"Cabin\"]]\ndf_Cabin = df_Cabin.fillna(\"unknown\")\nfor word in cabin_value_counts_df.index:\n    df_Cabin[word] = df_Cabin[\"Cabin\"][df_Cabin[\"Cabin\"].isnull()==False].map(lambda x:1 if x[0]==word else 0)\ndf_Cabin[\"unknown\"] = df_Cabin[\"Cabin\"].map(lambda x:1 if x==\"unknown\" else 0)\n\ncompare_cabin = pd.DataFrame(index=list(cabin_value_counts_df.index)+[\"unknown\"], columns=[\"Survived_0\",\"Survived_1\"])\nfor word in compare_cabin.index:\n    tmp = df_Cabin[\"Survived\"].groupby(df_Cabin[word]).mean()\n    compare_cabin.at[word,\"Survived_0\"] = tmp[0]\n    compare_cabin.at[word,\"Survived_1\"] = tmp[1]\n\ncompare_cabin","3814a629":"#10-1\ndf_Name = df.loc[:,[\"Survived\",\"Name\"]]\n\ndf_Name[\"Title\"] = df_Name['Name'].map(lambda x: x.split(', ')[1].split('. ')[0])\ndf_Name['Title'].replace(['Capt', 'Col', 'Major', 'Dr', 'Rev'], 'Officer', inplace=True)\ndf_Name['Title'].replace(['Don', 'Sir',  'the Countess', 'Lady', 'Dona'], 'Royalty', inplace=True)\ndf_Name['Title'].replace(['Mme', 'Ms'], 'Mrs', inplace=True)\ndf_Name['Title'].replace(['Mlle'], 'Miss', inplace=True)\ndf_Name['Title'].replace(['Jonkheer'], 'Master', inplace=True)\ncount_df = df_Name[\"Title\"].value_counts().to_frame()\nprint(\"---Title counts---\\n\",count_df)\n\ndf_Name = pd.get_dummies(df_Name, columns=[\"Title\"], prefix='', prefix_sep='')\n\ncompare_title = pd.DataFrame(index=list(count_df.index), columns=[\"Survived_0\",\"Survived_1\"])\nfor word in compare_title.index:\n    tmp = df_Name[\"Survived\"].groupby(df_Name[word]).mean()\n    compare_title.at[word,\"Survived_0\"] = tmp[0]\n    compare_title.at[word,\"Survived_1\"] = tmp[1]\ncompare_title","6455ea62":"# Name\u304b\u3089Surname(\u82d7\u5b57)\u3092\u62bd\u51fa\ndf_Name['Surname'] = df['Name'].map(lambda name:name.split(',')[0].strip())\ncount_df_over2 = df_Name[\"Surname\"].value_counts().to_frame()\ncount_df_over2 = count_df_over2[count_df_over2[\"Surname\"]>1]\nprint(\"---Surname counts---\\n\",count_df_over2)\ndf_Name['Family_flag'] = df_Name['Surname'].map(lambda x:1 if x in list(count_df_over2.index) else 0)\nprint(\"\\n---compere survived in family flag---\")\ndf_Name[\"Survived\"].groupby(df_Name[\"Family_flag\"]).mean()","ae2f15f0":"count_df_over2","30afdcdc":"def age_preprocessing(age):\n    age = age.fillna(age.mean())\n    age_s = age.map(lambda x:str(x)[0] if x>=10 else str(0))\n    age_s = age_s.map(lambda x:x if int(x)<=6 else \"7<=\")\n    return age_s\n\ndef sibsp_preprocessing(sibsp):\n    group_ids = []\n    for num in sibsp:\n        if num==0:\n            group_ids.append(\"alone\")\n        elif (num==1) or (num==2):\n            group_ids.append(\"small\")\n        elif num >= 3:\n            group_ids.append(\"large\")\n    return group_ids\n\ndef parch_preprocessing(parch):\n    group_ids = []\n    for num in parch:\n        if num==0:\n            group_ids.append(\"alone\")\n        elif (num==1) or (num==2):\n            group_ids.append(\"small\")\n        elif num >= 3:\n            group_ids.append(\"large\")\n    return group_ids\n\ndef fare_preprocessing(fare):\n    fare = fare.fillna(fare.mean())\n    new_bins = np.array([0, 10, 30, 100, 1000])\n    ids = np.digitize(fare, new_bins)\n    return ids\n\ndef ticket_preprocessing(ticket_df):\n    words = []\n    for ticket in ticket_df:\n        for word in ticket.split():\n            words.append(word.replace(\".\",\"\"))\n    count_df = pd.Series(words).value_counts().to_frame()\n    words_num_over5 = list(count_df[count_df[0]>=5].index)\n    print(\"---ticket over5num words---\\n\",words_num_over5,\"\\n\")\n    df_Ticket = pd.DataFrame(index=ticket_df.index, data=ticket_df.values, columns=[\"Ticket\"])\n    for word in words_num_over5:\n        df_Ticket[\"T_\" + word] = df_Ticket[\"Ticket\"].map(lambda x:1 if word in x.replace(\".\",\"\").split() else 0)\n    df_Ticket[\"T_No_target\"] = pd.Series(df_Ticket.iloc[:,2:].sum(axis=1)).map(lambda x:1 if x==0 else 0)\n    return df_Ticket.drop(\"Ticket\",axis=1)\n\ndef cabin_preprocessing(cabin):\n    cabin = cabin.fillna(\"-\")\n    cabin = cabin.map(lambda x:x[0])\n    return cabin\n\ndef name_preprocessing(name):\n    df_Name = pd.DataFrame(index=name.index)\n    #Title\n    df_Name[\"Title\"] = name.map(lambda x: x.split(', ')[1].split('. ')[0])\n    df_Name['Title'].replace(['Capt', 'Col', 'Major', 'Dr', 'Rev'], 'Officer', inplace=True)\n    df_Name['Title'].replace(['Don', 'Sir',  'the Countess', 'Lady', 'Dona'], 'Royalty', inplace=True)\n    df_Name['Title'].replace(['Mme', 'Ms'], 'Mrs', inplace=True)\n    df_Name['Title'].replace(['Mlle'], 'Miss', inplace=True)\n    df_Name['Title'].replace(['Jonkheer'], 'Master', inplace=True)\n    #Family_flag\n    Surname = name.map(lambda name:name.split(',')[0].strip())\n    count_df_over2 = Surname.value_counts().to_frame()\n    count_df_over2 = count_df_over2[count_df_over2[\"Name\"]>1]\n    df_Name['Family_flag'] = Surname.map(lambda x:1 if x in list(count_df_over2.index) else 0)\n    df_Name = pd.get_dummies(df_Name)\n    return df_Name\n\n###load data\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\").set_index(\"PassengerId\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\").set_index(\"PassengerId\")\n\n###concat train_data and test_data\ndata = pd.concat([train_data.drop(\"Survived\",axis=1), test_data])\n\n###check null data\nprint(\"---null counts---\\n\",data.apply(lambda x: sum(x.isnull())),\"\\n\")\n\n### 1.Pclass   ### 2.Sex   ### 7.Embarked \n# No preprocessing. using get_dummies.\ndata = pd.get_dummies(data, columns=[\"Pclass\",\"Sex\",\"Embarked\"])\n#Sex is binary... better one variable than dummies..?\ndata[\"Sex\"] = data[\"Sex_female\"]\ndata = data.drop([\"Sex_male\",\"Sex_female\"], axis=1)\n\n### 3.Age\ndata[\"Age\"] = age_preprocessing(data[\"Age\"])\ndata = pd.get_dummies(data, columns=[\"Age\"])\n\n### 4.SibSp\ndata[\"SibSp\"] = sibsp_preprocessing(data[\"SibSp\"])\ndata = pd.get_dummies(data, columns=[\"SibSp\"])\n\n### 5.Parch  \ndata[\"Parch\"] = sibsp_preprocessing(data[\"Parch\"])\ndata = pd.get_dummies(data, columns=[\"Parch\"])\n\n### 6.Fare  \ndata[\"Fare\"] = fare_preprocessing(data[\"Fare\"])\ndata = pd.get_dummies(data, columns=[\"Fare\"])\n\n### 8.Ticket\ndf = ticket_preprocessing(data[\"Ticket\"])\ndata = pd.concat([data, df],axis=1)\ndata = data.drop(\"Ticket\",axis=1)\n\n### 9.Cabin  \ndata[\"Cabin\"] = cabin_preprocessing(data[\"Cabin\"])\ndata = pd.get_dummies(data, columns=[\"Cabin\"])\n\n### 10.Name   \ndf = name_preprocessing(data[\"Name\"])\ndata = pd.concat([data, df],axis=1)\ndata = data.drop(\"Name\",axis=1)\n\ndata.head()","6db29f9d":"train_processed = data.loc[train_data.index,:]\ntest_processed = data.loc[test_data.index,:]","0ffe7fb3":"model = RandomForestClassifier()\nmodel.fit(train_processed.values, train_data[\"Survived\"].values)\npred_df = pd.DataFrame({\"column\":train_processed.columns, \"feature_importance\":model.feature_importances_})\npred_df = pred_df.sort_values(by=\"feature_importance\", ascending=True)\nplt.figure(figsize=(15,20))\nplt.barh(pred_df[\"column\"], pred_df[\"feature_importance\"])","c5cd9d12":"X_train, X_test, y_train, y_test = train_test_split(train_processed.values, train_data[\"Survived\"].values, train_size=0.9, stratify=train_data[\"Survived\"], random_state=0)","0c5b88e1":"rn = RandomForestClassifier()\nsvc = SVC()\nbooster = LGBMClassifier()\n\nrn.fit(X_train, y_train)\nsvc.fit(X_train, y_train)\nbooster.fit(X_train, y_train)\n\nno_params_rnc_score = rn.score(X_test, y_test)\nno_params_svc_score = svc.score(X_test, y_test)\nno_params_booster_score = booster.score(X_test, y_test)\n\nprint(\"benchmark\")\nprint(\"RandomForest_score\", no_params_rnc_score)\nprint(\"SVC_score\", no_params_svc_score)\nprint(\"lightGBM_score\", no_params_booster_score)","858f9d63":"def objective(trial):\n    params_rfc = {\n        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 20),\n        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 15),\n        'min_samples_split': trial.suggest_int(\"min_samples_split\", 5, 10),\n        \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n        'max_features': trial.suggest_int(\"max_features\", 2, 50),\n        \"random_state\": 2021\n    }\n\n    model = RandomForestClassifier(**params_rfc)\n    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n    scores = cross_validate(model, X=X_train, y=y_train, cv=kf)\n    \n    return scores['test_score'].mean()\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)\nprint(study.best_params)\nprint(study.best_value)\nrfc_best_param = study.best_params","3744c800":"def objective(trial):\n    \n    params_svc = {\n        'C' : trial.suggest_int(\"C\", 50, 200),\n        'gamma': trial.suggest_loguniform(\"gamma\", 1e-4, 1.0),\n        \"random_state\": 0,\n        'kernel': 'rbf'\n    }\n\n    model = SVC(**params_svc)\n    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n    scores = cross_validate(model, X=X_train, y=y_train, cv=kf)\n    return scores['test_score'].mean()\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=20)\nprint(study.best_params)\nprint(study.best_value)\nsvc_best_param = study.best_params","2457071e":"def objective(trial):\n    \n    params_lgb = {\n        'num_leaves': trial.suggest_int(\"num_leaves\", 3, 20),\n        'learning_rate': trial.suggest_loguniform(\"learning_rate\", 1e-8, 1.0),\n        'max_depth': trial.suggest_int(\"max_depth\", 3, 20),\n        \"random_state\": 0\n    }\n\n    model = LGBMClassifier(**params_lgb)\n    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n    scores = cross_validate(model, X=X_train, y=y_train, cv=kf)\n    return scores['test_score'].mean()\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\nprint(study.best_params)\nprint(study.best_value)\nlgb_best_param = study.best_params","72067b44":"rn = RandomForestClassifier(**rfc_best_param)\nsvc = SVC(**svc_best_param)\nbooster = LGBMClassifier(**lgb_best_param)\n\nrn.fit(X_train, y_train)\nsvc.fit(X_train, y_train)\nbooster.fit(X_train, y_train)\n\noptuna_rnc_score = rn.score(X_test, y_test)\noptuna_svc_score = svc.score(X_test, y_test)\noptuna_booster_score = booster.score(X_test, y_test)\n\nprint(\"best_params_score\")\nprint(\"RandomForest_score\", optuna_rnc_score)\nprint(\"SVC_score\", optuna_svc_score)\nprint(\"lightGBM_score\", optuna_booster_score)","19f0dadf":"print(\"-----RandomForest-----\")\nprint(\"no_param\", no_params_rnc_score.round(3))\nprint(\"optuna\", optuna_rnc_score.round(3))\nprint(\"point spread\", abs(optuna_rnc_score-no_params_rnc_score).round(3))\n\nprint(\"-----SVC-----\")\nprint(\"no_param\", no_params_svc_score.round(3))\nprint(\"optuna\", optuna_svc_score.round(3))\nprint(\"point spread\", abs(optuna_svc_score-no_params_svc_score).round(3))\n\nprint(\"-----booster-----\")\nprint(\"no_param\", no_params_booster_score.round(3))\nprint(\"optuna\", optuna_booster_score.round(3))\nprint(\"point spread\", abs(optuna_booster_score-no_params_booster_score).round(3))","dc5fff30":"rn = RandomForestClassifier(**rfc_best_param)\nrn.fit(train_processed.values, train_data[\"Survived\"].values)","6f79cefc":"submission = sample_submission.copy()\nsubmission.Survived = rn.predict(test_processed)\nsubmission.to_csv(\"submission.csv\",index=False)","e2f9f7f5":"#check Survived counts\nprint(submission[\"Survived\"].value_counts())\nprint(\"\\nper_1\", sum(submission[\"Survived\"]==1) \/ len(submission))","cd885031":"submission","45bfe29c":"import os\nfor dirname, _, filenames in os.walk('.\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","09f5f77f":"# First\n##### I will provide analysis from a beginner's perspective!  \n##### I'm not very good at English, so I use DeepL to translate( \uff1b\u2200\uff1b)","f0ee9037":"### test_data no have target column[\"Survived\"]","8698a1a7":"# Confirmation of loaded data\n##### 1. check table\n##### 2. check null and NaN","bccfd8eb":"### I decision RandomForest model!","be7959f9":"# Read datasets\n##### The data for the kaggle competition exists in the following locations ( ..)\u03c6\n\/kaggle\/input\/\"competition name\"\/train.csv","c5f85e2b":"## 10.Name\nThere are many things to check before processing a name, and I won't list them all here, but I will briefly introduce them.  \nI wrote it in my own way, using kagler's notebook as a reference.  \n\n10-1   \nmake Title columns.  \nHonorifics include gender, title, etc.  \n\n10-2  \nmale Family_flag columns.  \nIf the count of Surname is 2 or more, it is assumed to be family.","164c4f58":"# 8.Ticket\nTicket looks featureless at first glance, but let's check it out.\n\n8-1  \nI checked to see if there were any blanks in the ticket.  \nThe ones that are split contain English characters.\n\n8-2  \nwords and numbers included in multiple Ticket have been identified.\nMany of them were split with and without commas, so I replece'd them.\n\n8-3  \nThis time, we focused on words that appear more than five times in the training data and compared the survival rates.\nIt appears that there is a reasonable difference.","90cfdccc":"# 1.Pclass \nThis column Ticket Class  \n  \n1-1  \nI've confirmed the Pclass bias.  \nThe closer you are to one, the better your chances of survival.  \n1-2  \ndraw it on a diagram.  \n  \n    \nThe closer the value is to 1, the higher the survival rate, so this item is likely to be beneficial.","abc4f73b":"### Take an unadjusted benchmark first.","5547d2e0":"# Check the Feature Importance!","ae6effb1":"# 5.Parch\nNumber of parents\/children riding together on the Titanic\n  \n5-1, 5-2  \nApparently, the survival rate is lower when it goes above 4.\n\n5-3  \nGroup items to narrow them down.  \n0   _alone  (Survive:0.34)   \n1-2 _small  (Survive:0.53)     \n3-  _large  (Survive:0.26)   \n\nParch and SibSp show a similar trend.","7a635a02":"# Thank you for reading to the end!\n### Sorry for the messy code and translated English( \uff1b\u2200\uff1b)\nI'd be happy to VOTE if it helps!","06c081b7":"### Return train data and test data","1716cca6":"# Preprocessing\uff08Processing in one go\uff09\n### preprocessing should be done to reflect the trend well!","945abfd2":"# 3.Age\n\n3-1  \nFirst, check the average age of the survivors and non-survivors.  \nbut I don't see any major bias.\n  \n3-2  \nNo significant features can be seen when drawing the histogram.\nI would venture to say that people younger than 10 years have a higher survival rate.\n\n3-3  \nI checked the survival rate by age.\nThe survival rate is still high for those under 10 years old, and I don't think there is much difference between 20 and 60.\nbut 70s and 80s sample size is so small that it feels dangerous to continue this way.\nTherefore, people in their 70s and older are in one group.\n\n3-4\nGroup and check again as in 3-3","2bda4158":"### check dataframe infomation\n##### Check the \"Non-Null Count\" and \"dtype\" carefully.\n##### Pay attention to the number of unique per column.","37183771":"# Model Create!\n### for RandomForest, lightGBM, SVC\n### use optuna(An auto-tuning library for beginners)","70161db7":"# 7.Embarked\nPlace of departure\n\n7-1,7-2  \nC has a higher survival rate than S and Q.","8f9635ac":"# Now let's look at the traindata trends!\uff08EDA\uff09\n---easy---  \n1.Pclass  \n2.Sex      \n3.Age     \n4.SibSp  \n5.Parch  \n6.Fare  \n7.Embarked  \n---complicated---  \n8.Ticket  \n9.Cabin  \n10.Name   ","52bca315":"# Final step!\n## submission data creat!!","eb0230ef":"# 9. Cabin\n\n8-1  \nMost of the Cabin has no data in it.  \nHowever, if you look at the acronym, you can see something that could be grouped according to English letters.\n\n8-2  \nLet's check the survival rate.  \nAs with the head word, it seems that survival rates are characterized by the presence or absence of data.","5becb734":"# 6.Fare\n\n6-1  \nSurvival seems to have a higher Fare average.\u3000\u3000but..  \n\n6-2  \nIf you look at the figure, you can see that there are outliers, so it is considered to have fluctuated greatly.\n\n6-3  \nGroup by 10 and check the survival rate.\nIt seems that the larger the Fare, the higher the survival rate.\n\n6-4  \nSort new groups based on 6-3.  \n1:0-10  \n2:10-30  \n3:30-100  \n4:100-1000  \nIt's a nice grouping i think.","e6525bc1":"# Library import\n##### In this notebook, we will only use the basic libraries!","95dc7f60":"# 4. SibSp\nNumber of siblings\/spouses riding together on the Titanic\n\n4-1, 4-2  \nApparently, the survival rate is lower when it goes above 3.\n\n4-3  \nGroup items to narrow them down.  \n0   _alone  (Survive:0.34)   \n1-2 _small  (Survive:0.52)     \n3-  _large  (Survive:0.15)   ","9a96918a":"# 2. Sex\n  \n2-1  \nA very big bias. It can be said that women had a very high survival rate compared to men.  \n2-2  \nI drew it just in case.\n\nWe might as well focus on the Male survivor and Female no survivor."}}