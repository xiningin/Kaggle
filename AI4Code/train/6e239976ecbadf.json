{"cell_type":{"545dfa49":"code","38178006":"code","1483eae1":"code","673e279a":"code","51d3d112":"code","0d2b4913":"code","a693108b":"code","b85d1a4a":"code","c5893625":"code","a2f016ce":"code","d0b7c6cd":"code","255e7ef6":"code","73ebcc73":"code","170b62a3":"code","d5301b5b":"code","cb570556":"code","c5ad03be":"code","2394d857":"code","40309b86":"code","ebf1f3a8":"code","202fac85":"code","49a4da44":"code","f43b301a":"code","b1a3e2c2":"code","a3e9819c":"code","8c58cbad":"code","70611aaf":"code","4a49d383":"code","e3f10555":"code","7cbf184e":"code","c98a4970":"code","8d77258b":"code","7b1c418b":"code","5c7d4332":"code","e5c83804":"code","6a5bb4ae":"code","97957b39":"markdown","6e6cbd1d":"markdown","401b9c03":"markdown","c685795f":"markdown","cb7b79f1":"markdown","3d03f36c":"markdown","20213daa":"markdown","8c0f5619":"markdown","7ec0ba6d":"markdown","02d2b47a":"markdown","304f9e92":"markdown","94f284da":"markdown","a372ff23":"markdown","414f4480":"markdown","596fff19":"markdown","33ad4494":"markdown","cfb69f61":"markdown","f9fbebd0":"markdown","4a27aef4":"markdown","f5483022":"markdown","86b3f4e3":"markdown","6fb41a6e":"markdown","17ed8171":"markdown"},"source":{"545dfa49":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")","38178006":"df = pd.read_csv(\"\/kaggle\/input\/trip-advisor-hotel-reviews\/tripadvisor_hotel_reviews.csv\")\ndf.head()","1483eae1":"# shape of data\ndf.shape","673e279a":"# info of data\ndf.info()","51d3d112":"# Precentage of null values\n(df.isna().sum()\/len(df))*100","0d2b4913":"df.Rating.value_counts()","a693108b":"plt.figure(figsize=(8,6))\nsns.countplot(df[\"Rating\"])\nplt.title(\"Ratings of Hotels\")","b85d1a4a":"# Calculate review lengths\nreview_len = pd.Series([len(review.split()) for review in df['Review']])\n\n# The distribution of review text lengths\nreview_len.plot(kind='box')","c5893625":"sns.set_theme(\n    context='notebook',\n    style='darkgrid',\n    palette='deep',\n    font='sans-serif',\n    font_scale=1,\n    color_codes=True,\n    rc=None,\n)\nplt.figure(figsize=(12,8))\nsns.histplot(review_len)","a2f016ce":"fig = plt.figure(figsize=(14,7))\ndf[\"Length\"] = df.Review.str.split().apply(len)\nax1 = fig.add_subplot(122)\nsns.histplot(df[df['Rating']==5]['Length'], ax=ax1,color='green')\nfig.suptitle('Distribution of text length for 5 Star Rating', fontsize=16)\ndisplay(df.Length[df.Rating==5].describe())","d0b7c6cd":"fig2 = plt.figure(figsize=(14,8))\nax2 = fig2.add_subplot(122)\nsns.histplot(df[df[\"Rating\"]==1][\"Length\"],ax=ax2,color='r')\nfig2.suptitle(\"Distribution of text length for 1 Star Rating\",fontsize=16)\ndisplay(df.Length[df.Rating==1].describe())","255e7ef6":"from wordcloud import WordCloud\nplt.figure(figsize=(20,20))\nwc1 = WordCloud(max_words=2000, min_font_size=10, \n                height=800,width=1600,background_color=\"white\").generate(\" \".join(df[df[\"Rating\"]==1].Review))\nplt.imshow(wc1)","73ebcc73":"plt.figure(figsize=(20,20))\nwc2 = WordCloud(max_words=2000, min_font_size=10, \n                height=800,width=1600,background_color=\"white\").generate(\" \".join(df[df[\"Rating\"]==2].Review))\nplt.imshow(wc2)","170b62a3":"plt.figure(figsize=(20,20))\nwc3 = WordCloud(max_words=2000, min_font_size=10, \n                height=800,width=1600,background_color=\"white\").generate(\" \".join(df[df[\"Rating\"]==3].Review))\nplt.imshow(wc3)","d5301b5b":"plt.figure(figsize=(20,20))\nwc4 = WordCloud(max_words=2000, min_font_size=10, \n                height=800,width=1600,background_color=\"white\").generate(\" \".join(df[df[\"Rating\"]==4].Review))\nplt.imshow(wc4)","cb570556":"plt.figure(figsize=(20,20))\nwc5 = WordCloud(max_words=2000, min_font_size=10, \n                height=800,width=1600,background_color=\"white\").generate(\" \".join(df[df[\"Rating\"]==5].Review))\nplt.imshow(wc5)","c5ad03be":"# function for cleaning Review\ndef standardize_text(df, field):\n    df[field] = df[field].str.replace(r\"http\\S+\", \"\")\n    df[field] = df[field].str.replace(r\"http\",\"\")\n    df[field] = df[field].str.replace(r\"@\/S+\",\"\")\n    df[field] = df[field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n    df[field] = df[field].str.replace(r\"@\",\" at \")\n    df[field] = df[field].str.lower()\n    return df","2394d857":"standardize_text(df,\"Review\")","40309b86":"import re\nimport string\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk import word_tokenize","ebf1f3a8":"lemmatizer = WordNetLemmatizer()\ncorpus = []\nfor i in range(0, len(df)):\n    review = re.sub('[^a-zA-Z]', ' ', df['Review'][i])\n    review = review.split()\n    review = [word for word in review if not word in set(stopwords.words('english'))]\n    review = [lemmatizer.lemmatize(word) for word in review]\n    review = ' '.join(review)\n    corpus.append(review)","202fac85":"corpus[:1]","49a4da44":"def sentiment(review):\n    if review>=3:\n        return 1\n    else:\n        return 0\ndf['Sentiment']= df['Rating'].apply(sentiment)","f43b301a":"df.head(10)","b1a3e2c2":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(ngram_range=(1, 3), max_features=10000, tokenizer = word_tokenize)\nX = tfidf.fit_transform(corpus)\ny = df['Sentiment']","a3e9819c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25, random_state=24)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","8c58cbad":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nlr.score(X_test, y_test)","70611aaf":"from xgboost import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(X_train,y_train)","4a49d383":"xgb.score(X_test, y_test)","e3f10555":"from lightgbm import LGBMClassifier\nlgb = LGBMClassifier()\nlgb.fit(X_train, y_train)\nlgb.score(X_test, y_test)","7cbf184e":"from sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(verbose=True)\nmlp.fit(X_train, y_train)","c98a4970":"mlp.score(X_test, y_test)","8d77258b":"y_pred = mlp.predict(X_test)","7b1c418b":"from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nacc = accuracy_score(y_pred, y_test)\nreport = classification_report(y_pred, y_test)\nprint(report)\ncm = confusion_matrix(y_pred, y_test)\n#np.set_printoptions(precision=0.01)\nprint(\"Accuracy of MLP Model: {}%\".format(acc*100))\nsns.heatmap(cm, annot=True, fmt = \".1f\",cmap=\"RdBu\")\nplt.title(\"Confusion Matrix for MLP Model\")","5c7d4332":"from sklearn.metrics import roc_auc_score, roc_curve\ny_pred_proba = mlp.predict_proba(X_test)\npos_proba = y_pred_proba[:,1]","e5c83804":"fpr, tpr, thersholds = roc_curve(y_test, pos_proba)\nplt.plot(fpr, tpr, \"*-\")\nplt.plot([0,1],[0,1],'r--')\nplt.legend(['MLP', 'Random chance'])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC curve for MLP Classifier')","6a5bb4ae":"roc_auc_score(y_test, pos_proba)","97957b39":"# **Hotel Reviews**","6e6cbd1d":"* WordCloud for 2 Star Rating","401b9c03":"<div class=\"alert alert-box alert-warning\">\nOut of all the Models we used, MLP model performes best with Accuracy of 92.5%.\n\nSo, we are considering MLP Classifier Model.\n<\/div>","c685795f":"* Values closer to 1 in roc_auc_score shows that classifier is efficient and gives better performance.","cb7b79f1":"### Multilayer Perceptron(MLP) Classifier","3d03f36c":"* WordCloud for 3 Star Rating","20213daa":"# Importing Libraries","8c0f5619":"## WordCloud","7ec0ba6d":"* WordCloud for 1 Star Rating","02d2b47a":"# NLP Approach:\n1. Cleaning\n2. Lemmatization\n3. TF-IDF","304f9e92":"### Applying TF-IDF\n*Concept*: Tfidf is meant for rendering more importance to the rare words. It so happens that if you rely on word counts alone, the unimportant words like \u2018the\u2019 , \u2018and\u2019 etc. will get more importance because they tend to get used more often.\n\n* For better understanding of **Term Frequency - Inverse Document Frequency(TF-IDF)** refer to [this](https:\/\/www.quora.com\/How-are-TF-IDF-vectorizers-with-n-gram-features-created). ","94f284da":"# Model Training...\ud83d\udeb4","a372ff23":"### Logistic Regression","414f4480":"## ROC_AUC Score and Curve\n* The receiver operating characteristic (ROC) curve is a plot of the pairs of true positive rates (y-axis) and false positive rates (x-axis) that result from lowering the threshold down from 1, all the way to 0.","596fff19":"### LightGBM","33ad4494":"### Applying Lemmmatizer to remove tenses from texts.","cfb69f61":"* WordCloud for 5 Star Rating","f9fbebd0":"<div class=\"alert alert-box alert-warning\">\nIf you find this notebook insightful, Please UPVOTE!\n\nThank you:)\n<\/div>","4a27aef4":"### Splitting data into Training and Testing Set","f5483022":"# Prediction and Accuracy","86b3f4e3":"### XGBoost","6fb41a6e":"* WordCloud for 4 Star Rating","17ed8171":"# EDA"}}