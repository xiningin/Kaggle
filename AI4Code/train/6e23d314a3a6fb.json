{"cell_type":{"53bb4f4e":"code","49741fc0":"code","e793829b":"code","202f6fbf":"code","f3bd23a6":"code","9ce8e102":"code","617f7ca6":"code","d6f42021":"code","ffee1739":"code","e25e3cde":"code","7347a078":"code","5558db06":"code","47ac79a6":"code","74e679c8":"code","9d2c5450":"code","4b0f946c":"code","6d1378fc":"code","76d090d4":"code","193b9b53":"code","8cc8f042":"code","0d617d70":"code","67a4565a":"code","6041bd84":"code","dbc91b84":"code","22293669":"code","44e2be03":"code","4a8d663d":"code","7344a30d":"code","1cb13db5":"code","766bfea8":"code","8afa677f":"code","88b4964a":"code","d6e5b78c":"code","e6bed221":"code","ee901998":"code","c3992047":"code","75d65966":"code","95af5ea6":"code","e39c68e2":"code","6b2d0f5a":"code","eb8da818":"code","cfd5d61e":"code","d334d6e9":"code","f0e83252":"code","be3b5835":"code","f772f5be":"code","6e2dcbaf":"code","80802c97":"code","a87600a3":"code","a3ebc863":"code","0f7f6ffe":"code","44e08f9d":"code","af1cabb1":"code","b1556c86":"code","b602b712":"code","bdd458b4":"code","4537f01d":"code","328ede4b":"code","d11baa4b":"code","21daaf1e":"code","59bb3e6d":"code","6c36739f":"code","9dc0af32":"code","f32f610c":"code","2e4e695c":"code","cf85f6b3":"code","3dd4b21b":"code","7636665a":"code","02ce37bb":"code","22fab5bd":"code","2fcb7566":"code","eb4b630f":"code","6550edeb":"code","1bbd389d":"code","53e15a49":"code","c7388532":"code","1c90ea49":"code","3dd85b06":"code","8985d4f1":"code","d34c8269":"code","673d6d8b":"code","5b284eed":"code","a34ace0d":"code","c51dab4a":"code","3d7dcacf":"code","086ca791":"code","671be7da":"code","9287b94e":"code","f4a632d7":"code","cbf38d6d":"code","345f97f5":"code","6477fe28":"code","c3818db0":"code","ecfb86ba":"code","fdbb7f19":"code","4565c705":"code","049d49bd":"code","26b729a7":"code","8fd02333":"code","10802cc4":"code","2b75712d":"code","4d639dab":"code","01593791":"code","7cf63aed":"code","f5680b5e":"code","e85a447c":"code","027f7acc":"code","70b6148e":"code","c0a4bb17":"code","6eac64fd":"code","61bd7540":"code","7f4f3c9e":"code","500b20ba":"code","385b49ea":"code","9d2ebfe5":"code","3b923ca6":"code","7e2ded68":"code","88240236":"code","fd7beeed":"code","e7dea0d6":"code","caa37d1c":"markdown","8ca96448":"markdown","4ea7d43b":"markdown","f0d8fced":"markdown","b8d526bb":"markdown","b54a48ee":"markdown","11d6cc6b":"markdown","0624e3ae":"markdown","65c57010":"markdown","74b49d4a":"markdown","c9fc5514":"markdown","3da291c8":"markdown","a9818000":"markdown","847a7e02":"markdown","769b4bea":"markdown","1633fc05":"markdown","c985396e":"markdown","839c94d0":"markdown","abbdcae9":"markdown","2a97d120":"markdown","7301cef9":"markdown","3fc3e6db":"markdown","4d092730":"markdown","0023ac8a":"markdown","fa887d8b":"markdown","e8e6028a":"markdown","e4cd1b18":"markdown","81146f99":"markdown","7a876db8":"markdown","8d9a1105":"markdown","7af98a5d":"markdown","16707184":"markdown","4d581be1":"markdown","b8527b96":"markdown","99e59626":"markdown","e06893b3":"markdown","a53ce658":"markdown","ab7fa84b":"markdown","bedc7e0d":"markdown","521e917d":"markdown","3bf16e46":"markdown","9e36af68":"markdown","3b6b7c7d":"markdown","f335358a":"markdown","8b28d8f9":"markdown","3d03ef58":"markdown","65fdb34b":"markdown","9ba08ccb":"markdown","8f70de8b":"markdown","4efb3756":"markdown","0b239f73":"markdown"},"source":{"53bb4f4e":"import pandas as pd","49741fc0":"# Meta data :\nmeta = pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\")","e793829b":"meta.head(3)","202f6fbf":"meta.shape","f3bd23a6":"meta.columns","9ce8e102":"meta.dtypes","617f7ca6":"# Charge all .Json files:\nimport glob\npapers = glob.glob(f'\/kaggle\/input\/CORD-19-research-challenge\/**\/*.json', recursive=True)","d6f42021":"len(papers)","ffee1739":"# import json\n# papers_data = pd.DataFrame(columns=['PaperID','Title','Section','Text','Affilations'], index=range(len(papers)*50))\n\n# # Remove duplicates in a list:\n# def my_function(x):\n#     return list(dict.fromkeys(x))\n\n# i=0\n# for j in range(len(papers)):\n#     with open(papers[j]) as file:\n#         content = json.load(file)\n        \n#         # ID and Title:\n#         pap_id = content['paper_id']\n#         title =  content['metadata']['title']\n        \n#         # Affiations:\n#         affiliation = []\n#         for sec in content['metadata']['authors']:\n#             try:\n#                 affiliation.append(sec['affiliation']['institution'])\n#             except:\n#                 pass\n#         affiliation = my_function(affiliation)\n        \n#         # Abstract\n#         for sec in content['abstract']:\n#             papers_data.iloc[i, 0] = pap_id\n#             papers_data.iloc[i, 1] = title\n#             papers_data.iloc[i, 2] = sec['section']\n#             papers_data.iloc[i, 3] = sec['text']\n#             papers_data.iloc[i, 4] = affiliation\n#             i = i + 1\n            \n#         # Body text\n#         for sec in content['body_text']:\n#             papers_data.iloc[i, 0] = pap_id\n#             papers_data.iloc[i, 1] = title\n#             papers_data.iloc[i, 2] = sec['section']\n#             papers_data.iloc[i, 3] = sec['text']\n#             papers_data.iloc[i, 4] = affiliation\n#             i = i + 1\n\n# papers_data.dropna(inplace=True)\n# papers_data = papers_data.astype(str).drop_duplicates() \n\n# # Text processing:\n# import nltk\n# nltk.download('punkt')\n# # Lowercase:\n# for i in range(len(papers_data)):\n#     try:\n#         papers_data.iloc[i, 1] = papers_data.iloc[i, 1].lower()\n#         papers_data.iloc[i, 2] = papers_data.iloc[i, 2].lower()\n#         papers_data.iloc[i, 3] = papers_data.iloc[i, 3].lower()\n#         papers_data.iloc[i, 4] = papers_data.iloc[i, 4].lower()\n#     except:\n#         pass\n    \n# # Tokenization:\n\n# from nltk.tokenize import word_tokenize, sent_tokenize , RegexpTokenizer\n\n# tokenizer = RegexpTokenizer(r'\\w+') # remove punctuation\n# papers_data[\"Title_Tokens_words\"] = [list() for x in range(len(papers_data.index))]\n# papers_data[\"Text_Tokens_words\"] = [list() for x in range(len(papers_data.index))]\n\n# for i in range(len(papers_data)):\n#     try:\n#         papers_data.iloc[i, 5] = tokenizer.tokenize(papers_data.iloc[i, 1])\n#         papers_data.iloc[i, 6] = tokenizer.tokenize(papers_data.iloc[i, 3])\n#     except:\n#         pass\n    \n# # Remove stopwords:\n# nltk.download('stopwords')\n# from nltk.corpus import stopwords\n# stop_words = set(stopwords.words('english')) \n\n# for i in range(len(papers_data)):\n#     try:\n#         papers_data.iloc[i, 5] = [w for w in papers_data.iloc[i, 5] if not w in stop_words] \n#         papers_data.iloc[i, 6] = [w for w in papers_data.iloc[i, 6] if not w in stop_words]\n#     except:\n#         pass\n    \n# # Words count:  \n# papers_data[\"Words_count\"] = 0\n\n# # for i in range(len(papers_data)):\n# #     try:\n# #         papers_data.iloc[i, 7] = len(papers_data.iloc[i, 6])\n# #     except:\n# #         pass\n    \n# # Lemmatization :\n# nltk.download('wordnet')\n\n# from nltk.stem import WordNetLemmatizer\n\n# wordnet_lemmatizer = WordNetLemmatizer()\n\n# papers_data[\"Text_Lem_words\"] = [list() for x in range(len(papers_data))]\n\n# for i in range(len(papers_data)):\n#     for j in range(len(papers_data.iloc[i, 6])):\n#         papers_data.iloc[i, 8].append(wordnet_lemmatizer.lemmatize(papers_data.iloc[i, 6][j]))\n        \n# papers_data[\"Title_Lem_words\"] = [list() for x in range(len(papers_data))]\n\n# for i in range(len(papers_data)):\n#     for j in range(len(papers_data.iloc[i, 5])):\n#         papers_data.iloc[i, 9].append(wordnet_lemmatizer.lemmatize(papers_data.iloc[i, 5][j]))\n        \n# papers_data.to_csv(\"\/kaggle\/input\/processed-researches-data\/papers_data_final.csv\")\nprint(\"Preprocessing done!\")","e25e3cde":"papers_data = pd.read_csv(\"\/kaggle\/input\/processed-researches-data\/papers_data_final.csv\")\ndel papers_data['Unnamed: 0']\nimport ast\npapers_data['Affilations'] = papers_data['Affilations'].apply(lambda x: ast.literal_eval(x))\npapers_data['Text_Lem_words'] = papers_data['Text_Lem_words'].apply(lambda x: ast.literal_eval(x))","7347a078":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nwordnet_lemmatizer = WordNetLemmatizer()\ndef my_function(x):\n    return list(dict.fromkeys(x))\n\n\nkeywords =['widespread','exposure', 'policy', 'recommendation','mitigation','measures','denominator','mechanism','test','sharing','demographic','asymptomatic','disease','serosurvey','convalescent','detection','screening','neutralizing','antibodies','elisas']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","5558db06":"# At least 11 of words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 11: \n        return(True)  \n    return(False)    \n  \ntask6_1 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_1.append(i)\n    \nlen(task6_1)","47ac79a6":"## Results for task 6.1 :\nfor i in task6_1:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","74e679c8":"# Task 6.1:\nprint(task6_1)","9d2c5450":"task6_1_rank = papers_data.iloc[task6_1, :]\ntask6_1_rank.reset_index(inplace=True,drop=True)\n\n# Grab the publish year from the meta data file:\nmeta = meta.rename(columns={\"title\": \"Title\"})\nfor i in range(len(meta)):\n    meta.iloc[i, 2] = str(meta.iloc[i, 2]).lower()\nmeta['Title'] = meta['Title'].astype(str) \ntask6_1_rank['Title'] = task6_1_rank['Title'].astype(str) \n\ntask6_1_rank = pd.merge(task6_1_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_1_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\nimport dateutil.parser as parser\ntask6_1_rank['publish_time'] = task6_1_rank['publish_time'].apply(lambda x:  str(x).replace('Winter',''))\ntask6_1_rank['publish_time'] = task6_1_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_1_rank['publish_time'] = pd.to_numeric(task6_1_rank['publish_time'])\ntask6_1_rank = task6_1_rank.sort_values(by='publish_time', ascending=False)\ntask6_1_rank.reset_index(inplace=True,drop=True)","4b0f946c":"rank = pd.read_csv(\"\/kaggle\/input\/shanghai-ranking\/rank-univ.csv\")\ndel rank['Unnamed: 0']\nrank.head(5)","6d1378fc":"# Extract the affiliations score to the task's results:\ntask6_1_rank['Aff_Score'] = 0\nfor i in range(len(task6_1_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_1_rank.iloc[i, 4]:\n            task6_1_rank.iloc[i, 11] = rank.iloc[j, 3]","76d090d4":"task6_1_rank[\"Ranking_Score\"] = task6_1_rank[\"publish_time\"]*0.8 + task6_1_rank[\"Aff_Score\"]*0.2","193b9b53":"task6_1_rank = task6_1_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_1_rank.reset_index(inplace=True,drop=True)\ntask6_1_rank","8cc8f042":"## 20 - Ranked Results for task 1.1 :\n\nfor i in range(len(task6_1_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_1_rank.iloc[i, 0])\n    print(\"Title: \", task6_1_rank.iloc[i, 1])\n    print(\"Section: \", task6_1_rank.iloc[i, 2])\n    print(\"Text: \", task6_1_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","0d617d70":"keywords =['efforts','capacity', 'diagnostic', 'platforms','surveillance','covid19','covid-19']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","67a4565a":"# At least 5 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 4: \n        return(True)  \n    return(False)    \n  \ntask6_2 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_2.append(i)\n    \nlen(task6_2)","6041bd84":"## Results for task 6.2 :\nfor i in task6_2:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","dbc91b84":"# Task 6.2:\nprint(task6_2)","22293669":"task6_2_rank = papers_data.iloc[task6_2, :]\ntask6_2_rank.reset_index(inplace=True,drop=True)\n\n# Grab the publish year from the meta data file:\n\ntask6_2_rank['Title'] = task6_2_rank['Title'].astype(str) \n\ntask6_2_rank = pd.merge(task6_2_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_2_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_2_rank['publish_time'] = task6_2_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_2_rank['publish_time'] = pd.to_numeric(task6_2_rank['publish_time'])\ntask6_2_rank = task6_2_rank.sort_values(by='publish_time', ascending=False)\ntask6_2_rank.reset_index(inplace=True,drop=True)","44e2be03":"# Extract the affiliations score to the task's results:\ntask6_2_rank['Aff_Score'] = 0\nfor i in range(len(task6_2_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_2_rank.iloc[i, 4]:\n            task6_2_rank.iloc[i, 11] = rank.iloc[j, 3]","4a8d663d":"task6_2_rank[\"Ranking_Score\"] = task6_2_rank[\"publish_time\"]*0.8 + task6_2_rank[\"Aff_Score\"]*0.2\ntask6_2_rank = task6_2_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_2_rank.reset_index(inplace=True,drop=True)\ntask6_2_rank","7344a30d":"## 20 - Ranked Results for task 1.2 :\n\nfor i in range(len(task6_2_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_2_rank.iloc[i, 0])\n    print(\"Title: \", task6_2_rank.iloc[i, 1])\n    print(\"Section: \", task6_2_rank.iloc[i, 2])\n    print(\"Text: \", task6_2_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","1cb13db5":"keywords =['recruitment', 'support','coordination','local','expertise','capacity','public','private','commercial','non-profit','academic','legal','ethical','communication','operational','issues']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","766bfea8":"# At least 12 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 11: \n        return(True)  \n    return(False)    \n  \ntask6_3 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_3.append(i)\n    \nlen(task6_3)","8afa677f":"## Results for task 6.3 :\nfor i in task6_3:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","88b4964a":"# Task 6.3:\nprint(task6_3)","d6e5b78c":"task6_3_rank = papers_data.iloc[task6_3, :]\ntask6_3_rank.reset_index(inplace=True,drop=True)\n\n# Grab the publish year from the meta data file:\ntask6_3_rank['Title'] = task6_3_rank['Title'].astype(str) \n\ntask6_3_rank = pd.merge(task6_3_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_3_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_3_rank['publish_time'] = task6_3_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_3_rank['publish_time'] = pd.to_numeric(task6_3_rank['publish_time'])\ntask6_3_rank = task6_3_rank.sort_values(by='publish_time', ascending=False)\ntask6_3_rank.reset_index(inplace=True,drop=True)","e6bed221":"# Extract the affiliations score to the task's results:\ntask6_3_rank['Aff_Score'] = 0\nfor i in range(len(task6_3_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_3_rank.iloc[i, 4]:\n            task6_3_rank.iloc[i, 11] = rank.iloc[j, 3]","ee901998":"task6_3_rank[\"Ranking_Score\"] = task6_3_rank[\"publish_time\"]*0.8 + task6_3_rank[\"Aff_Score\"]*0.2\ntask6_3_rank = task6_3_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_3_rank.reset_index(inplace=True,drop=True)\ntask6_3_rank","c3992047":"## 20 - Ranked Results for task 1.3 :\n\nfor i in range(len(task6_3_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_3_rank.iloc[i, 0])\n    print(\"Title: \", task6_3_rank.iloc[i, 1])\n    print(\"Section: \", task6_3_rank.iloc[i, 2])\n    print(\"Text: \", task6_3_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","75d65966":"keywords =['national', 'guidance','guidelines','practices','states','universities','private','laboratories','test','public','health','officials','public','covid19','covid-19','virus']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","95af5ea6":"# At least 12 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 11: \n        return(True)  \n    return(False)    \n  \ntask6_4 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_4.append(i)\n    \nlen(task6_4)","e39c68e2":"## Results for task 6.4 :\nfor i in task6_4:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","6b2d0f5a":"# Task 6.4:\nprint(task6_4)","eb8da818":"task6_4_rank = papers_data.iloc[task6_4, :]\ntask6_4_rank.reset_index(inplace=True,drop=True)\n\n# Grab the publish year from the meta data file:\n\ntask6_4_rank['Title'] = task6_4_rank['Title'].astype(str) \n\ntask6_4_rank = pd.merge(task6_4_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_4_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_4_rank['publish_time'] = task6_4_rank['publish_time'].apply(lambda x:  str(x).replace('May 8 Summer',''))\ntask6_4_rank['publish_time'] = task6_4_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_4_rank['publish_time'] = pd.to_numeric(task6_4_rank['publish_time'])\ntask6_4_rank = task6_4_rank.sort_values(by='publish_time', ascending=False)\ntask6_4_rank.reset_index(inplace=True,drop=True)","cfd5d61e":"# Extract the affiliations score to the task's results:\ntask6_4_rank['Aff_Score'] = 0\nfor i in range(len(task6_4_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_4_rank.iloc[i, 4]:\n            task6_4_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_4_rank[\"Ranking_Score\"] = task6_4_rank[\"publish_time\"]*0.8 + task6_4_rank[\"Aff_Score\"]*0.2\ntask6_4_rank = task6_4_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_4_rank.reset_index(inplace=True,drop=True)\ntask6_4_rank","d334d6e9":"## 20 - Ranked Results for task 1.4 :\n\nfor i in range(len(task6_4_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_4_rank.iloc[i, 0])\n    print(\"Title: \", task6_4_rank.iloc[i, 1])\n    print(\"Section: \", task6_4_rank.iloc[i, 2])\n    print(\"Text: \", task6_4_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","f0e83252":"keywords =['development', 'point-of-care','covid19','covid-19','test','influenza','bed-side','recognizing','dischtradeoffs','speed','accessibility','accuracy']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","be3b5835":"# At least 7 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 6: \n        return(True)  \n    return(False)    \n  \ntask6_5 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_5.append(i)\n    \nlen(task6_5)","f772f5be":"## Results for task 6.5 :\nfor i in task6_5:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","6e2dcbaf":"# Task 6.5 \nprint(task6_5)","80802c97":"task6_5_rank = papers_data.iloc[task6_5, :]\ntask6_5_rank.reset_index(inplace=True,drop=True)\n\n# Grab the publish year from the meta data file:\n\ntask6_5_rank['Title'] = task6_5_rank['Title'].astype(str) \n\ntask6_5_rank = pd.merge(task6_5_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_5_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_5_rank['publish_time'] = task6_5_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_5_rank['publish_time'] = pd.to_numeric(task6_5_rank['publish_time'])\ntask6_5_rank = task6_5_rank.sort_values(by='publish_time', ascending=False)\ntask6_5_rank.reset_index(inplace=True,drop=True)","a87600a3":"# Extract the affiliations score to the task's results:\ntask6_5_rank['Aff_Score'] = 0\nfor i in range(len(task6_5_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_5_rank.iloc[i, 4]:\n            task6_5_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_5_rank[\"Ranking_Score\"] = task6_5_rank[\"publish_time\"]*0.8 + task6_5_rank[\"Aff_Score\"]*0.2\ntask6_5_rank = task6_5_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_5_rank.reset_index(inplace=True,drop=True)\ntask6_5_rank","a3ebc863":"## 20 - Ranked Results for task 1.5 :\n\nfor i in range(len(task6_5_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_5_rank.iloc[i, 0])\n    print(\"Title: \", task6_5_rank.iloc[i, 1])\n    print(\"Section: \", task6_5_rank.iloc[i, 2])\n    print(\"Text: \", task6_5_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","0f7f6ffe":"keywords =['rapid', 'design','execution','target','surveillance','experiments','testers','pcr','area','report','entity','aid','longitudinal','sample','critical','impact','ad-hoc','intervention','local','recorded','covid19','covidd-19','virus']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","44e08f9d":"# At least 16 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 15: \n        return(True)  \n    return(False)    \n  \ntask6_6 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_6.append(i)\n    \nlen(task6_6)","af1cabb1":"## Results for task 6.6 :\nfor i in task6_6:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","b1556c86":"# Task 6.6:\nprint(task6_6)","b602b712":"task6_6_rank = papers_data.iloc[task6_6, :]\ntask6_6_rank.reset_index(inplace=True,drop=True)\n\ntask6_6_rank['Title'] = task6_6_rank['Title'].astype(str) \n\ntask6_6_rank = pd.merge(task6_6_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_6_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_6_rank['publish_time'] = task6_6_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_6_rank['publish_time'] = pd.to_numeric(task6_6_rank['publish_time'])\ntask6_6_rank = task6_6_rank.sort_values(by='publish_time', ascending=False)\ntask6_6_rank.reset_index(inplace=True,drop=True)","bdd458b4":"# Extract the affiliations score to the task's results:\ntask6_6_rank['Aff_Score'] = 0\nfor i in range(len(task6_6_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_6_rank.iloc[i, 4]:\n            task6_6_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_6_rank[\"Ranking_Score\"] = task6_6_rank[\"publish_time\"]*0.8 + task6_6_rank[\"Aff_Score\"]*0.2\ntask6_6_rank = task6_6_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_6_rank.reset_index(inplace=True,drop=True)\ntask6_6_rank","4537f01d":"## 20 - Ranked Results for task 1.6 :\n\nfor i in range(len(task6_6_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_6_rank.iloc[i, 0])\n    print(\"Title: \", task6_6_rank.iloc[i, 1])\n    print(\"Section: \", task6_6_rank.iloc[i, 2])\n    print(\"Text: \", task6_6_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","328ede4b":"keywords =['separation', 'assay','virus','development','issues','instruments','role','private','sector','devices','covid19','coronavirus','covid-19']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","d11baa4b":"# At least 13 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 12: \n        return(True)  \n    return(False)    \n  \ntask6_7 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_7.append(i)\n    \nlen(task6_7)","21daaf1e":"## Results for task 6.7 :\nfor i in task6_7:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","59bb3e6d":"task6_7_rank = papers_data.iloc[task6_7, :]\ntask6_7_rank.reset_index(inplace=True,drop=True)\n\ntask6_7_rank['Title'] = task6_7_rank['Title'].astype(str) \n\ntask6_7_rank = pd.merge(task6_7_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_7_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_7_rank['publish_time'] = task6_7_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_7_rank['publish_time'] = pd.to_numeric(task6_7_rank['publish_time'])\ntask6_7_rank = task6_7_rank.sort_values(by='publish_time', ascending=False)\ntask6_7_rank.reset_index(inplace=True,drop=True)","6c36739f":"# Extract the affiliations score to the task's results:\ntask6_7_rank['Aff_Score'] = 0\nfor i in range(len(task6_7_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_7_rank.iloc[i, 4]:\n            task6_7_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_7_rank[\"Ranking_Score\"] = task6_7_rank[\"publish_time\"]*0.8 + task6_7_rank[\"Aff_Score\"]*0.2\ntask6_7_rank = task6_7_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_7_rank.reset_index(inplace=True,drop=True)\ntask6_7_rank","9dc0af32":"## 20 - Ranked Results for task 1.7 :\n\nfor i in range(len(task6_7_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_7_rank.iloc[i, 0])\n    print(\"Title: \", task6_7_rank.iloc[i, 1])\n    print(\"Section: \", task6_7_rank.iloc[i, 2])\n    print(\"Text: \", task6_7_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","f32f610c":"keywords =['efforts', 'track','evolution','virus','genetic','mutation','locking','reagent','surveillance','detection','scheme','covid19','covid-19','coronavirus']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","2e4e695c":"# At least 9 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 9: \n        return(True)  \n    return(False)    \n  \ntask6_8 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_8.append(i)\n    \nlen(task6_8)","cf85f6b3":"## Results for task 6.8 :\nfor i in task6_8:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","3dd4b21b":"task6_8_rank = papers_data.iloc[task6_8, :]\ntask6_8_rank.reset_index(inplace=True,drop=True)\n\ntask6_8_rank['Title'] = task6_8_rank['Title'].astype(str) \n\ntask6_8_rank = pd.merge(task6_8_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_8_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\nimport dateutil.parser as parser\ntask6_8_rank['publish_time'] = task6_8_rank['publish_time'].apply(lambda x:  str(x).replace('Winter',''))\ntask6_8_rank['publish_time'] = task6_8_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_8_rank['publish_time'] = pd.to_numeric(task6_8_rank['publish_time'])\ntask6_8_rank = task6_8_rank.sort_values(by='publish_time', ascending=False)\ntask6_8_rank.reset_index(inplace=True,drop=True)","7636665a":"# Extract the affiliations score to the task's results:\ntask6_8_rank['Aff_Score'] = 0\nfor i in range(len(task6_8_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_8_rank.iloc[i, 4]:\n            task6_8_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_8_rank[\"Ranking_Score\"] = task6_8_rank[\"publish_time\"]*0.8 + task6_8_rank[\"Aff_Score\"]*0.2\ntask6_8_rank = task6_8_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_8_rank.reset_index(inplace=True,drop=True)\ntask6_8_rank","02ce37bb":"## 20 - Ranked Results for task 1.8 :\n\nfor i in range(len(task6_8_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_8_rank.iloc[i, 0])\n    print(\"Title: \", task6_8_rank.iloc[i, 1])\n    print(\"Section: \", task6_8_rank.iloc[i, 2])\n    print(\"Text: \", task6_8_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","22fab5bd":"keywords =['latency', 'issues','viral','detect','pathogen', 'needed','biological','environment','sample']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","2fcb7566":"# At least 11 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 10: \n        return(True)  \n    return(False)    \n  \ntask6_9 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_9.append(i)\n    \nlen(task6_9)","eb4b630f":"## Results for task 6.9 :\nfor i in task6_9:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","6550edeb":"task6_9_rank = papers_data.iloc[task6_9, :]\ntask6_9_rank.reset_index(inplace=True,drop=True)\n\ntask6_9_rank['Title'] = task6_9_rank['Title'].astype(str) \n\ntask6_9_rank = pd.merge(task6_9_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_9_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_9_rank['publish_time'] = task6_9_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_9_rank['publish_time'] = pd.to_numeric(task6_9_rank['publish_time'])\ntask6_9_rank = task6_9_rank.sort_values(by='publish_time', ascending=False)\ntask6_9_rank.reset_index(inplace=True,drop=True)","1bbd389d":"# Extract the affiliations score to the task's results:\ntask6_9_rank['Aff_Score'] = 0\nfor i in range(len(task6_9_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_9_rank.iloc[i, 4]:\n            task6_9_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_9_rank[\"Ranking_Score\"] = task6_9_rank[\"publish_time\"]*0.8 + task6_9_rank[\"Aff_Score\"]*0.2\ntask6_9_rank = task6_9_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_9_rank.reset_index(inplace=True,drop=True)\ntask6_9_rank","53e15a49":"## 20 - Ranked Results for task 1.9 :\n\nfor i in range(len(task6_9_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_9_rank.iloc[i, 0])\n    print(\"Title: \", task6_9_rank.iloc[i, 1])\n    print(\"Section: \", task6_9_rank.iloc[i, 2])\n    print(\"Text: \", task6_9_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","c7388532":"keywords =['diagnostics', 'host','response','markers','cytokines','detect','early','disease','predict','progression','clinical','practice','efficacy','therapeutic','interventions','covid-19','covid19','virus']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","1c90ea49":"# At least 11 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 11: \n        return(True)  \n    return(False)    \n  \ntask6_10 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_10.append(i)\n    \nlen(task6_10)","3dd85b06":"## Results for task 6.10 :\nfor i in task6_10:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","8985d4f1":"task6_10_rank = papers_data.iloc[task6_10, :]\ntask6_10_rank.reset_index(inplace=True,drop=True)\n\ntask6_10_rank['Title'] = task6_10_rank['Title'].astype(str) \n\ntask6_10_rank = pd.merge(task6_10_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_10_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_10_rank['publish_time'] = task6_10_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_10_rank['publish_time'] = pd.to_numeric(task6_10_rank['publish_time'])\ntask6_10_rank = task6_10_rank.sort_values(by='publish_time', ascending=False)\ntask6_10_rank.reset_index(inplace=True,drop=True)","d34c8269":"# Extract the affiliations score to the task's results:\ntask6_10_rank['Aff_Score'] = 0\nfor i in range(len(task6_10_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_10_rank.iloc[i, 4]:\n            task6_10_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_10_rank[\"Ranking_Score\"] = task6_10_rank[\"publish_time\"]*0.8 + task6_10_rank[\"Aff_Score\"]*0.2\ntask6_10_rank = task6_10_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_10_rank.reset_index(inplace=True,drop=True)\ntask6_10_rank","673d6d8b":"## 20 - Ranked Results for task 1.10 :\n\nfor i in range(len(task6_10_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_10_rank.iloc[i, 0])\n    print(\"Title: \", task6_10_rank.iloc[i, 1])\n    print(\"Section: \", task6_10_rank.iloc[i, 2])\n    print(\"Text: \", task6_10_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","5b284eed":"keywords =['policies','protocol','coronavirus','corona','covid','testing','screening','covid19','covid-19']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","a34ace0d":"# At least 5 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 4: \n        return(True)  \n    return(False)    \n  \ntask6_11 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_11.append(i)\n    \nlen(task6_11)","c51dab4a":"## Results for task 6.11 :\nfor i in task6_11:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","3d7dcacf":"task6_11_rank = papers_data.iloc[task6_11, :]\ntask6_11_rank.reset_index(inplace=True,drop=True)\n\ntask6_11_rank['Title'] = task6_11_rank['Title'].astype(str) \n\ntask6_11_rank = pd.merge(task6_11_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_11_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_11_rank['publish_time'] = task6_11_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_11_rank['publish_time'] = pd.to_numeric(task6_11_rank['publish_time'])\ntask6_11_rank = task6_11_rank.sort_values(by='publish_time', ascending=False)\ntask6_11_rank.reset_index(inplace=True,drop=True)","086ca791":"# Extract the affiliations score to the task's results:\ntask6_11_rank['Aff_Score'] = 0\nfor i in range(len(task6_11_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_11_rank.iloc[i, 4]:\n            task6_11_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_11_rank[\"Ranking_Score\"] = task6_11_rank[\"publish_time\"]*0.8 + task6_11_rank[\"Aff_Score\"]*0.2\ntask6_11_rank = task6_11_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_11_rank.reset_index(inplace=True,drop=True)\ntask6_11_rank","671be7da":"## 20 - Ranked Results for task 1.11 :\n\nfor i in range(len(task6_11_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_11_rank.iloc[i, 0])\n    print(\"Title: \", task6_11_rank.iloc[i, 1])\n    print(\"Section: \", task6_11_rank.iloc[i, 2])\n    print(\"Text: \", task6_11_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","9287b94e":"keywords =['policies', 'effects','control','supplies','mass','testing','reagents','swabs','covid19','virus','covid-19']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","f4a632d7":"# At least 10 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 10: \n        return(True)  \n    return(False)    \n  \ntask6_12 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_12.append(i)\n    \nlen(task6_12)","cbf38d6d":"## Results for task 6.12 :\nfor i in task6_12:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","345f97f5":"task6_12_rank = papers_data.iloc[task6_12, :]\ntask6_12_rank.reset_index(inplace=True,drop=True)\n\ntask6_12_rank['Title'] = task6_12_rank['Title'].astype(str) \n\ntask6_12_rank = pd.merge(task6_12_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_12_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\nimport dateutil.parser as parser\ntask6_12_rank['publish_time'] = task6_12_rank['publish_time'].apply(lambda x:  str(x).replace('May 8 Summer',''))\ntask6_12_rank['publish_time'] = task6_12_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_12_rank['publish_time'] = pd.to_numeric(task6_12_rank['publish_time'])\ntask6_12_rank = task6_12_rank.sort_values(by='publish_time', ascending=False)\ntask6_12_rank.reset_index(inplace=True,drop=True)","6477fe28":"# Extract the affiliations score to the task's results:\ntask6_12_rank['Aff_Score'] = 0\nfor i in range(len(task6_12_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_12_rank.iloc[i, 4]:\n            task6_12_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_12_rank[\"Ranking_Score\"] = task6_12_rank[\"publish_time\"]*0.8 + task6_12_rank[\"Aff_Score\"]*0.2\ntask6_12_rank = task6_12_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_12_rank.reset_index(inplace=True,drop=True)\ntask6_12_rank","c3818db0":"## 20 - Ranked Results for task 1.12 :\n\nfor i in range(len(task6_12_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_12_rank.iloc[i, 0])\n    print(\"Title: \", task6_12_rank.iloc[i, 1])\n    print(\"Section: \", task6_12_rank.iloc[i, 2])\n    print(\"Text: \", task6_12_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","ecfb86ba":"keywords =['technology', 'roadmap','diagnostics','covid19','virus','covid-19']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","fdbb7f19":"# At least 3 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 2: \n        return(True)  \n    return(False)    \n  \ntask6_13 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_13.append(i)\n    \nlen(task6_13)","4565c705":"## Results for task 6.13 :\nfor i in task6_13:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","049d49bd":"task6_13_rank = papers_data.iloc[task6_13, :]\ntask6_13_rank.reset_index(inplace=True,drop=True)\n\ntask6_13_rank['Title'] = task6_13_rank['Title'].astype(str) \n\ntask6_13_rank = pd.merge(task6_13_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_13_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_13_rank['publish_time'] = task6_13_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_13_rank['publish_time'] = pd.to_numeric(task6_13_rank['publish_time'])\ntask6_13_rank = task6_13_rank.sort_values(by='publish_time', ascending=False)\ntask6_13_rank.reset_index(inplace=True,drop=True)","26b729a7":"# Extract the affiliations score to the task's results:\ntask6_13_rank['Aff_Score'] = 0\nfor i in range(len(task6_13_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_13_rank.iloc[i, 4]:\n            task6_13_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_13_rank[\"Ranking_Score\"] = task6_13_rank[\"publish_time\"]*0.8 + task6_13_rank[\"Aff_Score\"]*0.2\ntask6_13_rank = task6_13_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_13_rank.reset_index(inplace=True,drop=True)\ntask6_13_rank","8fd02333":"## 20 - Ranked Results for task 1.13 :\n\nfor i in range(len(task6_13_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_13_rank.iloc[i, 0])\n    print(\"Title: \", task6_13_rank.iloc[i, 1])\n    print(\"Section: \", task6_13_rank.iloc[i, 2])\n    print(\"Text: \", task6_13_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","10802cc4":"keywords =['barriers','scaling','diagnostic','market','coalition','accelerator','model','epidemic','preparedness','innovation','critical','funding','opportunities','streamlined','regulatory','environment']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","2b75712d":"# At least 9 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 8: \n        return(True)  \n    return(False)    \n  \ntask6_14 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_14.append(i)\n    \nlen(task6_14)","4d639dab":"## Results for task 6.14 :\nfor i in task6_14:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")\n    ","01593791":"task6_14_rank = papers_data.iloc[task6_14, :]\ntask6_14_rank.reset_index(inplace=True,drop=True)\n\ntask6_14_rank['Title'] = task6_14_rank['Title'].astype(str) \n\ntask6_14_rank = pd.merge(task6_14_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_14_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\nimport dateutil.parser as parser\ntask6_14_rank['publish_time'] = task6_14_rank['publish_time'].apply(lambda x:  str(x).replace('May 8 Summer',''))\ntask6_14_rank['publish_time'] = task6_14_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_14_rank['publish_time'] = pd.to_numeric(task6_14_rank['publish_time'])\ntask6_14_rank = task6_14_rank.sort_values(by='publish_time', ascending=False)\ntask6_14_rank.reset_index(inplace=True,drop=True)","7cf63aed":"# Extract the affiliations score to the task's results:\ntask6_14_rank['Aff_Score'] = 0\nfor i in range(len(task6_14_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_14_rank.iloc[i, 4]:\n            task6_14_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_14_rank[\"Ranking_Score\"] = task6_14_rank[\"publish_time\"]*0.8 + task6_14_rank[\"Aff_Score\"]*0.2\ntask6_14_rank = task6_14_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_14_rank.reset_index(inplace=True,drop=True)\ntask6_14_rank","f5680b5e":"## 20 - Ranked Results for task 6.14 :\n\nfor i in range(len(task6_14_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_14_rank.iloc[i, 0])\n    print(\"Title: \", task6_14_rank.iloc[i, 1])\n    print(\"Section: \", task6_14_rank.iloc[i, 2])\n    print(\"Text: \", task6_14_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","e85a447c":"keywords =['platforms','technology','response','time','employ','holistic','approaches','epidemic','covid19','didease','covid-19','future']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","027f7acc":"# At least 8 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 7: \n        return(True)  \n    return(False)    \n  \ntask6_15 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_15.append(i)\n    \nlen(task6_15)","70b6148e":"## Results for task 6.15 :\nfor i in task6_15:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")\n    ","c0a4bb17":"task6_15_rank = papers_data.iloc[task6_15, :]\ntask6_15_rank.reset_index(inplace=True,drop=True)\n\ntask6_15_rank['Title'] = task6_15_rank['Title'].astype(str) \n\ntask6_15_rank = pd.merge(task6_15_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_15_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_15_rank['publish_time'] = task6_15_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_15_rank['publish_time'] = pd.to_numeric(task6_15_rank['publish_time'])\ntask6_15_rank = task6_15_rank.sort_values(by='publish_time', ascending=False)\ntask6_15_rank.reset_index(inplace=True,drop=True)","6eac64fd":"# Extract the affiliations score to the task's results:\ntask6_15_rank['Aff_Score'] = 0\nfor i in range(len(task6_15_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_15_rank.iloc[i, 4]:\n            task6_15_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_15_rank[\"Ranking_Score\"] = task6_15_rank[\"publish_time\"]*0.8 + task6_15_rank[\"Aff_Score\"]*0.2\ntask6_15_rank = task6_15_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_15_rank.reset_index(inplace=True,drop=True)\ntask6_15_rank","61bd7540":"## 20 - Ranked Results for task 6.14 :\n\nfor i in range(len(task6_15_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_15_rank.iloc[i, 0])\n    print(\"Title: \", task6_15_rank.iloc[i, 1])\n    print(\"Section: \", task6_15_rank.iloc[i, 2])\n    print(\"Text: \", task6_15_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","7f4f3c9e":"keywords =['coupling','genomic','diagnostic','test','large-scale','covid19','covid-19','epidemic','didease']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   \n\n# At least 5 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 4: \n        return(True)  \n    return(False)    \n  \ntask6_16 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_16.append(i)\n    \nlen(task6_16)","500b20ba":"task6_16_rank = papers_data.iloc[task6_16, :]\ntask6_16_rank.reset_index(inplace=True,drop=True)\n\ntask6_16_rank['Title'] = task6_16_rank['Title'].astype(str) \n\ntask6_16_rank = pd.merge(task6_16_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_16_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_16_rank['publish_time'] = task6_16_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_16_rank['publish_time'] = pd.to_numeric(task6_16_rank['publish_time'])\ntask6_16_rank = task6_16_rank.sort_values(by='publish_time', ascending=False)\ntask6_16_rank.reset_index(inplace=True,drop=True)\n\n## Ranking by affiliations\n\n# Extract the affiliations score to the task's results:\ntask6_16_rank['Aff_Score'] = 0\nfor i in range(len(task6_16_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_16_rank.iloc[i, 4]:\n            task6_16_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_16_rank[\"Ranking_Score\"] = task6_16_rank[\"publish_time\"]*0.8 + task6_16_rank[\"Aff_Score\"]*0.2\ntask6_16_rank = task6_16_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_16_rank.reset_index(inplace=True,drop=True)\n\n\n## 20 - Ranked Results for task 6.14 :\n\nfor i in range(len(task6_16_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_16_rank.iloc[i, 0])\n    print(\"Title: \", task6_16_rank.iloc[i, 1])\n    print(\"Section: \", task6_16_rank.iloc[i, 2])\n    print(\"Text: \", task6_16_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","385b49ea":"keywords =['enhance','capabilities','sequencing','bioinformatics','target','region','genome','variant','covid19','covid-19','epidemic','didease']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   \n\n# At least 7 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 9: \n        return(True)  \n    return(False)    \n  \ntask6_17 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_17.append(i)\n    \nlen(task6_17)","9d2ebfe5":"task6_17_rank = papers_data.iloc[task6_17, :]\ntask6_17_rank.reset_index(inplace=True,drop=True)\n\ntask6_17_rank['Title'] = task6_17_rank['Title'].astype(str) \n\ntask6_17_rank = pd.merge(task6_17_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_17_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_17_rank['publish_time'] = task6_17_rank['publish_time'].apply(lambda x: str(x).replace('Feb 5 Nov-Dec',''))\n\ntask6_17_rank['publish_time'] = task6_17_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_17_rank['publish_time'] = pd.to_numeric(task6_17_rank['publish_time'])\ntask6_17_rank = task6_17_rank.sort_values(by='publish_time', ascending=False)\ntask6_17_rank.reset_index(inplace=True,drop=True)\n\n## Ranking by affiliations\n\n# Extract the affiliations score to the task's results:\ntask6_17_rank['Aff_Score'] = 0\nfor i in range(len(task6_17_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_17_rank.iloc[i, 4]:\n            task6_17_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_17_rank[\"Ranking_Score\"] = task6_17_rank[\"publish_time\"]*0.8 + task6_17_rank[\"Aff_Score\"]*0.2\ntask6_17_rank = task6_17_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_17_rank.reset_index(inplace=True,drop=True)\n\n\n## 20 - Ranked Results for task 6.14 :\n\nfor i in range(len(task6_17_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_17_rank.iloc[i, 0])\n    print(\"Title: \", task6_17_rank.iloc[i, 1])\n    print(\"Section: \", task6_17_rank.iloc[i, 2])\n    print(\"Text: \", task6_17_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","3b923ca6":"keywords =['enhance','capacity','people','technology','data','sequencing','analytics','pathogens','covid19','covid-19','epidemic','didease','capabilities','natural','intentional']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   \n\n# At least 7 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 6: \n        return(True)  \n    return(False)    \n  \ntask6_18 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_18.append(i)\n    \nlen(task6_18)","7e2ded68":"task6_18_rank = papers_data.iloc[task6_18, :]\ntask6_18_rank.reset_index(inplace=True,drop=True)\n\ntask6_18_rank['Title'] = task6_18_rank['Title'].astype(str) \n\ntask6_18_rank = pd.merge(task6_18_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_18_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_18_rank['publish_time'] = task6_18_rank['publish_time'].apply(lambda x: str(x).replace('Feb 5 Nov-Dec',''))\n\ntask6_18_rank['publish_time'] = task6_18_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_18_rank['publish_time'] = pd.to_numeric(task6_18_rank['publish_time'])\ntask6_18_rank = task6_18_rank.sort_values(by='publish_time', ascending=False)\ntask6_18_rank.reset_index(inplace=True,drop=True)\n\n## Ranking by affiliations\n\n# Extract the affiliations score to the task's results:\ntask6_18_rank['Aff_Score'] = 0\nfor i in range(len(task6_18_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_18_rank.iloc[i, 4]:\n            task6_18_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_18_rank[\"Ranking_Score\"] = task6_18_rank[\"publish_time\"]*0.8 + task6_18_rank[\"Aff_Score\"]*0.2\ntask6_18_rank = task6_18_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_18_rank.reset_index(inplace=True,drop=True)\n\n\n## 20 - Ranked Results for task 6.14 :\n\nfor i in range(len(task6_18_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_18_rank.iloc[i, 0])\n    print(\"Title: \", task6_18_rank.iloc[i, 1])\n    print(\"Section: \", task6_18_rank.iloc[i, 2])\n    print(\"Text: \", task6_18_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","88240236":"keywords =['health','surveillance','humans','sources','spillover','future','exposure','pathogens','covid19','covid-19','epidemic','didease','organism','evolutionary','host','bats','transmission','trafficked','farm','wildlife','domestic','food','companion','species','environment','demographic','occupation','risk','factor']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   \n\n# At least 15 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 14: \n        return(True)  \n    return(False)    \n  \ntask6_19 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_19.append(i)\n    \nlen(task6_19)","fd7beeed":"task6_19_rank = papers_data.iloc[task6_19, :]\ntask6_19_rank.reset_index(inplace=True,drop=True)\n\ntask6_19_rank['Title'] = task6_19_rank['Title'].astype(str) \n\ntask6_19_rank = pd.merge(task6_19_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_19_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_19_rank['publish_time'] = task6_19_rank['publish_time'].apply(lambda x: str(x).replace('Feb 5 Nov-Dec',''))\n\ntask6_19_rank['publish_time'] = task6_19_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_19_rank['publish_time'] = pd.to_numeric(task6_19_rank['publish_time'])\ntask6_19_rank = task6_19_rank.sort_values(by='publish_time', ascending=False)\ntask6_19_rank.reset_index(inplace=True,drop=True)\n\n## Ranking by affiliations\n\n# Extract the affiliations score to the task's results:\ntask6_19_rank['Aff_Score'] = 0\nfor i in range(len(task6_19_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_19_rank.iloc[i, 4]:\n            task6_19_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_19_rank[\"Ranking_Score\"] = task6_19_rank[\"publish_time\"]*0.8 + task6_19_rank[\"Aff_Score\"]*0.2\ntask6_19_rank = task6_19_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_19_rank.reset_index(inplace=True,drop=True)\n\n\n## 20 - Ranked Results for task 6.14 :\n\nfor i in range(len(task6_19_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_19_rank.iloc[i, 0])\n    print(\"Title: \", task6_19_rank.iloc[i, 1])\n    print(\"Section: \", task6_19_rank.iloc[i, 2])\n    print(\"Text: \", task6_19_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","e7dea0d6":"task6_1_rank.to_csv(\"task6_1_rank.csv\")\ntask6_2_rank.to_csv(\"task6_2_rank.csv\")\ntask6_3_rank.to_csv(\"task6_3_rank.csv\")\ntask6_4_rank.to_csv(\"task6_4_rank.csv\")\ntask6_5_rank.to_csv(\"task6_5_rank.csv\")\ntask6_6_rank.to_csv(\"task6_6_rank.csv\")\ntask6_7_rank.to_csv(\"task6_7_rank.csv\")\ntask6_8_rank.to_csv(\"task6_8_rank.csv\")\ntask6_9_rank.to_csv(\"task6_9_rank.csv\")\ntask6_10_rank.to_csv(\"task6_10_rank.csv\")\ntask6_11_rank.to_csv(\"task6_11_rank.csv\")\ntask6_12_rank.to_csv(\"task6_12_rank.csv\")\ntask6_13_rank.to_csv(\"task6_13_rank.csv\")\ntask6_14_rank.to_csv(\"task6_14_rank.csv\")\ntask6_15_rank.to_csv(\"task6_15_rank.csv\")\ntask6_16_rank.to_csv(\"task6_16_rank.csv\")\ntask6_17_rank.to_csv(\"task6_17_rank.csv\")\ntask6_18_rank.to_csv(\"task6_18_rank.csv\")\ntask6_19_rank.to_csv(\"task6_19_rank.csv\")","caa37d1c":"### Ranking by the most recent:","8ca96448":"# COVID-19: Open Research Dataset Challenge (CORD-19)\n# Task 06: What do we know about non-pharmaceutical interventions?\n\n\n### Purpose:\n\n- Our purpose is to extract relevant text sections related to the proposed questions from all the research dataset. \n\n### Description of the methodology steps:\n\n- Upload and structure all researches paper's sections separately.\n- Process the sections' text using the nltk package (lowercase, remove punctuations, tokenization, Lemmatization).\n- To search for the relevant results about each task:   \n    - Select the keywords and their synonyms.   \n    - Search in the dataset those keywords (or most of them) on the lemmatized tokens of the sections.\n- While the obtained results are very large, a ranking of the resulted papers' sections is recommended using both: the freshness degree and the rank score of the papers' institutions, according to the following Formula: \n    \n    ***Score = Affiliation Score * 0.2 + Publication Year * 2020***\n    \n- The obtained results now are ranked, and ready to get used!\n","4ea7d43b":"### By afffiliation scores:","f0d8fced":"### Ranking by the most recent:","b8d526bb":"### Ranking by most Recent:","b54a48ee":"# Search for papers sections containing task's keywords:","11d6cc6b":"### Ranking by the most recent:","0624e3ae":"### Task 6.5: ***Development of a point-of-care test (like a rapid influenza test) and rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy.***","65c57010":"### Task 06 : What do we know about diagnostics and surveillance?\n\n#### Task Details\n\n- What do we know about diagnostics and surveillance? What has been published concerning systematic, holistic approach to diagnostics (from the public health surveillance perspective to being able to predict clinical outcomes)?\n\n- Specifically, we want to know what the literature reports about:\n\n    - How widespread current exposure is to be able to make immediate policy recommendations on mitigation measures. Denominators for testing and a mechanism for rapidly sharing that information, including demographics, to the extent possible. Sampling methods to determine asymptomatic disease (e.g., use of serosurveys (such as convalescent samples) and early detection of disease (e.g., use of screening of neutralizing antibodies such as ELISAs).\n    - Efforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms.\n    - Recruitment, support, and coordination of local expertise and capacity (public, private\u2014commercial, and non-profit, including academic), including legal, ethical, communications, and operational issues.\n    - National guidance and guidelines about best practices to states (e.g., how states might leverage universities and private laboratories for testing purposes, communications to public health officials and the public).\n    - Development of a point-of-care test (like a rapid influenza test) and rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy.\n    - Rapid design and execution of targeted surveillance experiments calling for all potential testers using PCR in a defined area to start testing and report to a specific entity. These experiments could aid in collecting longitudinal samples, which are critical to understanding the impact of ad hoc local interventions (which also need to be recorded).\n    - Separation of assay development issues from instruments, and the role of the private sector to help quickly migrate assays onto those devices.\n    - Efforts to track the evolution of the virus (i.e., genetic drift or mutations) and avoid locking into specific reagents and surveillance\/detection schemes.\n    - Latency issues and when there is sufficient viral load to detect the pathogen, and understanding of what is needed in terms of biological and environmental sampling.\n    - Use of diagnostics such as host response markers (e.g., cytokines) to detect early disease or predict severe disease progression, which would be important to understanding best clinical practice and efficacy of therapeutic interventions.\n    - Policies and protocols for screening and testing.\n    - Policies to mitigate the effects on supplies associated with mass testing, including swabs and reagents.\n    - Technology roadmap for diagnostics.\n    - Barriers to developing and scaling up new diagnostic tests (e.g., market forces), how future coalition and accelerator models (e.g., Coalition for Epidemic Preparedness Innovations) could provide critical funding for diagnostics, and opportunities for a streamlined regulatory environment.\n    - New platforms and technology (e.g., CRISPR) to improve response times and employ more holistic approaches to COVID-19 and future diseases.\n    - Coupling genomics and diagnostic testing on a large scale.\n    - Enhance capabilities for rapid sequencing and bioinformatics to target regions of the genome that will allow specificity for a particular variant.\n    - Enhance capacity (people, technology, data) for sequencing with advanced analytics for unknown pathogens, and explore capabilities for distinguishing naturally-occurring pathogens from intentional.\n    - One Health surveillance of humans and potential sources of future spillover or ongoing exposure for this organism and future pathogens, including both evolutionary hosts (e.g., bats) and transmission hosts (e.g., heavily trafficked and farmed wildlife and domestic food and companion species), inclusive of environmental, demographic, and occupational risk factors.","74b49d4a":"### By affilitions Scores:","c9fc5514":"### By affiliations Scores:","3da291c8":"### By affiliations scores:","a9818000":"### By affiliations Score:","847a7e02":"### By affiliation scores:","769b4bea":"### Task 6.6: ***Rapid design and execution of targeted surveillance experiments calling for all potential testers using PCR in a defined area to start testing and report to a specific entity. These experiments could aid in collecting longitudinal samples, which are critical to understanding the impact of ad hoc local interventions (which also need to be recorded).***","1633fc05":"### Ranking by the most recent:","c985396e":"### Task 6.8: ***Efforts to track the evolution of the virus (i.e., genetic drift or mutations) and avoid locking into specific reagents and surveillance\/detection schemes.***","839c94d0":"### Task 6.9: ***Latency issues and when there is sufficient viral load to detect the pathogen, and understanding of what is needed in terms of biological and environmental sampling.***","abbdcae9":"### Ranking by the most recent:","2a97d120":"### Task 6.12: ***Policies to mitigate the effects on supplies associated with mass testing, including swabs and reagents.***","7301cef9":"### Task 6.19: ***One Health surveillance of humans and potential sources of future spillover or ongoing exposure for this organism and future pathogens, including both evolutionary hosts (e.g., bats) and transmission hosts (e.g., heavily trafficked and farmed wildlife and domestic food and companion species), inclusive of environmental, demographic, and occupational risk factors.***","3fc3e6db":"### Task 6.1: ***How widespread current exposure is to be able to make immediate policy recommendations on mitigation measures. Denominators for testing and a mechanism for rapidly sharing that information, including demographics, to the extent possible. Sampling methods to determine asymptomatic disease (e.g., use of serosurveys (such as convalescent samples) and early detection of disease (e.g., use of screening of neutralizing antibodies such as ELISAs).***","4d092730":"### Task 6.15: ***New platforms and technology (e.g., CRISPR) to improve response times and employ more holistic approaches to COVID-19 and future diseases.***","0023ac8a":"### By affiliations score:","fa887d8b":"### Ranking by teh most recent:","e8e6028a":"### By affiliations scores:","e4cd1b18":"### Ranking by most Recent:","81146f99":"### By affiliations Scores:","7a876db8":"### Ranking by the most recent:","8d9a1105":"### By affiliation scores:","7af98a5d":"### By affiliations Scores:\n","16707184":"### Task 6.18: ***Enhance capacity (people, technology, data) for sequencing with advanced analytics for unknown pathogens, and explore capabilities for distinguishing naturally-occurring pathogens from intentional.***","4d581be1":"### Task 6.13: ***Technology roadmap for diagnostics.***","b8527b96":"### Save all sub-tasks results:","99e59626":"### Ranking by the most recent:","e06893b3":"### Ranking by the most recent:","a53ce658":"### Task 6.7: ***Separation of assay development issues from instruments, and the role of the private sector to help quickly migrate assays onto those devices.***","ab7fa84b":"### By affiliations Scores:\n\n- Ranking using the : http:\/\/www.shanghairanking.com\/arwu2019.html","bedc7e0d":"### Task 6.4: ***National guidance and guidelines about best practices to states (e.g., how states might leverage universities and private laboratories for testing purposes, communications to public health officials and the public).***","521e917d":"### Task 6.11: ***Policies and protocols for screening and testing.***","3bf16e46":"### Task 6.3: ***Recruitment, support, and coordination of local expertise and capacity (public, private\u2014commercial, and non-profit, including academic), including legal, ethical, communications, and operational issues.***","9e36af68":"### Ranking by the most recent:","3b6b7c7d":"### Task 6.17: ***Enhance capabilities for rapid sequencing and bioinformatics to target regions of the genome that will allow specificity for a particular variant.***","f335358a":"### Task 6.16: ***Coupling genomics and diagnostic testing on a large scale.***","8b28d8f9":"### By the effiliations scores:","3d03ef58":"### Task 6.2: ***Efforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms.***","65fdb34b":"### Ranking by the most recent:","9ba08ccb":"### By affiliations scores:","8f70de8b":"### Task 6.10: ***Use of diagnostics such as host response markers (e.g., cytokines) to detect early disease or predict severe disease progression, which would be important to understanding best clinical practice and efficacy of therapeutic interventions.***","4efb3756":"### Task 6.14: ***Barriers to developing and scaling up new diagnostic tests (e.g., market forces), how future coalition and accelerator models (e.g., Coalition for Epidemic Preparedness Innovations) could provide critical funding for diagnostics, and opportunities for a streamlined regulatory environment.***","0b239f73":"### Ranking by the most recent:"}}