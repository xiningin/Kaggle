{"cell_type":{"23557584":"code","dba6c713":"code","26297b2e":"code","a1de50e0":"code","9b33784c":"code","5e0a708e":"code","0faff023":"code","3b950a0c":"code","30527e16":"code","818873f4":"code","20e5f155":"code","9e3f6cb5":"code","d246bd50":"code","6a180322":"code","cacc4102":"code","8dcb57f1":"code","0971014e":"code","7c359b72":"code","b3e92054":"code","38ca659b":"code","939d2ac3":"code","cbe3fa1a":"code","29b97425":"code","8ffeb9eb":"code","43f86360":"code","b9efe87f":"code","5e3854fc":"code","79f63fe2":"code","dbb424fb":"code","1d8c3237":"markdown","8e04c69c":"markdown","833de38d":"markdown","0a07e377":"markdown","da8566d3":"markdown","8a14aca1":"markdown","dd7f2c1d":"markdown","2f61cb74":"markdown","d0ed001e":"markdown","00c33e25":"markdown","58ebf28a":"markdown","9e13ce83":"markdown","b2ebdbc0":"markdown","ac4e0dfc":"markdown"},"source":{"23557584":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os","dba6c713":"data=pd.read_csv(\"..\/input\/train.csv\")\ndata.head()","26297b2e":"#looking for null values\n(len(data)-data.count())\/len(data)","a1de50e0":"#Visualizing the Data\ndata.groupby(['airline_sentiment']).size()","9b33784c":"data.groupby(['airline']).size()","5e0a708e":"import seaborn as sns\nimport matplotlib.pyplot as plt","0faff023":"sns.countplot(x='airline_sentiment',data=data,order=['negative','neutral','positive'])\nplt.show()","3b950a0c":"#Visualizing 'airline_sentiment' and 'airline'\nsns.factorplot(x = 'airline_sentiment',data=data,\n               order = ['negative','neutral','positive'],kind = 'count',col_wrap=3,col='airline',size=4,aspect=0.6,sharex=False,sharey=False)\nplt.show()","30527e16":"# Visualizing 'airlinee_sentiment' and 'tweet_count'\nsns.factorplot(x= 'airline_sentiment',data=data,\n              order=['negative','neutral','positive'],kind = 'count',col_wrap=3,col='retweet_count',size=4,aspect=0.6,sharex=False,sharey=False)\nplt.show()","818873f4":"#Visualizing 'negativereason' and 'airline'\nsns.factorplot(x = 'airline',data=data,\n               order = ['Virgin America','United'],kind ='count',hue='negativereason',size=6,aspect=0.9)\nplt.show()","20e5f155":"import re\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer","9e3f6cb5":"data=data.drop([\"tweet_id\",\n           \"airline\",\n           \"name\",\n           \"retweet_count\",\n           \"tweet_created\",\n           \"tweet_location\",\n           \"user_timezone\"],axis=1)\n","d246bd50":"#remove words which are starts with @ symbols\ndata['text'] = data['text'].map(lambda x:re.sub('@\\w*','',str(x)))\n#remove link starts with https\ndata['text'] = data['text'].map(lambda x:re.sub('http.*','',str(x)))\n#removing data and time (numeric values)\ndata['text'] = data['text'].map(lambda x:re.sub('[0-9]','',str(x)))\n#removing special characters\ndata['text'] = data['text'].map(lambda x:re.sub('[#|*|$|:|\\\\|&]','',str(x)))\n","6a180322":"data.head()","cacc4102":"#appending negative reason to text\ndata=data.values\nfor i in range(3339):\n    if not str(data[i][2])==\"nan\":\n        data[i][4]=str(data[i][4])+\" \"+ str(data[i][2])","8dcb57f1":"#Getting important numeric data \nfor i in range(3339):\n    if str(data[i][2])==\"nan\":\n        data[i][2]=0\n    if str(data[i][3])==\"nan\":\n        data[i][3]=0.3\nfor i in range(3339):\n    if not str(data[i][2])=='0':\n        data[i][2]=1\n","0971014e":"data=pd.DataFrame(data=data,columns=[\"airline_sentiment\",\"airline_sentiment_confidence\",\"negativereason\",\"negativereason_confidence\",\"text\"])\ndata.head()","7c359b72":"#preparing train data\n#removing stopwords and tokenizing it.\nstop=stopwords.words('english')\ntext=[]\nnone=data['text'].map(lambda x:text.append(' '.join\n       ([word for word in str(x).strip().split() if not word in set(stop)])))\ntfid=TfidfVectorizer(strip_accents=None,lowercase=False,preprocessor=None)\nx_features=tfid.fit_transform(text).toarray()","b3e92054":"#preparing target variable\ny=data['airline_sentiment']\ny=pd.DataFrame(y,columns=['airline_sentiment'])\ny = y['airline_sentiment'].map({'neutral':1,'negative':2,'positive':0})","38ca659b":"#training with Logistic Regression\nfrom sklearn.linear_model import LogisticRegression as lg\nfrom sklearn.model_selection import cross_val_score","939d2ac3":"clf=lg()\nacc=cross_val_score(estimator=clf,X=x_features,y=y,cv=5)\nacc","cbe3fa1a":"#calculating accuracy after adding three more numerical parameters 'negativereason','negativereason_confidence', and 'airline_sentiment_confidence'.\n#Note that we have transformed that earlier\n#emmbading numerical data in x_features\nx_features=pd.DataFrame(x_features)\nx_features.loc[:,'a']=data.iloc[:,1].values\nx_features.loc[:,'b']=data.iloc[:,2].values\nx_features.loc[:,'c']=data.iloc[:,3].values","29b97425":"#training our new data\nclf=lg()\nacc=cross_val_score(estimator=clf,X=x_features,y=y,cv=5)\nacc","8ffeb9eb":"#lets dig deeper and apply Deep learning for better accuracy\nfrom keras.models import Sequential\nfrom keras.layers import Flatten, Dense\nfrom keras import regularizers\nfrom keras.layers import Dropout","43f86360":"# Transforming our target vatiable\nfrom sklearn.preprocessing import OneHotEncoder","b9efe87f":"onehotencoder=OneHotEncoder()\ntarget=y.values\ntarget=target.reshape(-1,1)\ntarget=onehotencoder.fit_transform(target).toarray()","5e3854fc":"target=pd.DataFrame(data=target,columns=['positive','neutral','negative'])\ntarget.head()","79f63fe2":"clf=Sequential()\n#adding layers to ANN\nclf.add(Dense(units=2048,activation=\"relu\",kernel_initializer=\"uniform\",kernel_regularizer=regularizers.l2(0.001),input_dim=6212))\nclf.add(Dropout(0.5))\n#adding two more hidden layer to ANN\nclf.add(Dense(units=2048,activation=\"relu\",kernel_initializer=\"uniform\",kernel_regularizer=regularizers.l2(0.001)))\nclf.add(Dropout(0.5))\nclf.add(Dense(units=2048,activation=\"relu\",kernel_initializer=\"uniform\",kernel_regularizer=regularizers.l2(0.001)))\nclf.add(Dropout(0.5))\n#adding output layer\nclf.add(Dense(units=3,activation=\"softmax\",kernel_initializer=\"uniform\"))\n#compiling ANN\nclf.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n\n#fitting ANN\nhist=clf.fit(x_features,target,batch_size=32,epochs=10)\n","dbb424fb":"# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1)\nax[0].plot(hist.history['loss'], color='b', label=\"Training loss\")\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(hist.history['acc'], color='r', label=\"Training accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","1d8c3237":"## Logistic Regression","8e04c69c":"## Content\n1. Introduction\n2. Data Injection\n3. Data Visualisation\n4. Preprocessing\n5. Training                                                                                                                                                                                                                                                            \n5.1. Logistic Regression                                                                                                                                                                                                                                      \n5.2.Artificial Neural Network\n6. Evaluation with graph","833de38d":"### Thank you for your visit and plzz upvote it if like it. ","0a07e377":"#### As you can clearly see the accuracy is increased by a desent margin","da8566d3":"# Training","8a14aca1":"# Introduction","dd7f2c1d":"# Preprocessing","2f61cb74":"# Data Injection","d0ed001e":"# Vizualizing Data","00c33e25":"##  Artificial Neural Network","58ebf28a":"## Visualizing with Graph","9e13ce83":"# Evaluation with Graph","b2ebdbc0":"### Problem Description\nGiven dataset contains data of tweets on various airline\u2019s twitter handles.\n\nIt contains a total of 12 columns, out of which one column specifies the sentiment of the tweet. All other columns provide various information related to what was the tweet, where was it posted from, when was it posted, it's retweeted; etc.\n\nMy task was to build a machine learning \/ deep learning model to predict the sentiment of the tweet using all or some of the other given columns\n\n### Data Description\nDescription of columns of the dataset is given below -\n\ntweet_id -- Id of the tweet\n\nairline_sentiment -- Sentiment of the tweet (Target variable)\n\nairline_sentiment_confidence -- Confidence with which the given sentiment was determined\n\nnegativereason_confidence -- Confidence with which the negative reason of tweet was predicted\n\nname -- Name of the person who tweeted\n\nretweet_count -- Number of retweets\n\ntext -- Text of the tweet whose sentiment has to be predicted\n\ntweet_created -- Time at which the tweet was created\n\ntweet_location -- Location from where the tweet was posted\n\nuser_timezone -- Time zone from where the tweet was posted\n\nnegativereason -- Reason for which user posted a negative tweet\n\nairline -- Airline for which the tweet was posted","ac4e0dfc":"## Getting important numeric and non numeric data\n1. Appending negative reason to text\n2. For data['negativereason'] i have removed the NaN values by 0 in 'negativereason' and placed 1 in place of vaild negative reason.\n3. For data['negativereason_confidence'] the values are between 0 to 1 higher the values more its chances to be a 'negative' tweet lower the values more its chances to be 'positive' or 'neutral' tweet.\nso replacing the NaN by value near to zero"}}