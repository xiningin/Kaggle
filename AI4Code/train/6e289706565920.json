{"cell_type":{"383f475d":"code","b3ec0f19":"code","ccacc43e":"code","de268545":"code","16a6be54":"code","0a7e6e24":"code","b73a997e":"code","e0cc987c":"code","3a3d7670":"code","542688c9":"code","19602faa":"code","bd6b3076":"code","6e72b5b1":"code","cef4601a":"code","34c2bd88":"code","83c4a321":"code","76ab496b":"code","fa47e2e5":"code","c947cdf0":"code","f7102461":"code","d8616658":"markdown","dae03c5c":"markdown","bcb41584":"markdown","f1f796ba":"markdown","f1a2b71e":"markdown","ffb4c500":"markdown"},"source":{"383f475d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))","b3ec0f19":"# https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        #else: df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","ccacc43e":"## Note: Kernel unstable when using all data\ndf = reduce_mem_usage(pd.read_csv(\"..\/input\/checkouts-by-title.csv\",\n                 nrows=15123456,\n                 keep_date_col=True,infer_datetime_format=True\n                 ,parse_dates=[[\"CheckoutYear\",\"CheckoutMonth\"]]).rename(columns={\"CheckoutYear_CheckoutMonth\":\"CheckoutDate\"}).set_index(\"CheckoutDate\"))\n\nprint(df.shape)\ndf.head()","de268545":"### assuming all publication years are 4 digit years, we could parse with this. \n### It's not, so we get erros. We could coerce, but let's just leave them as numbers for now:\n# df.PublicationYear = pd.to_datetime(df.PublicationYear.str.replace(r\"[^0-9]\",''),format=\"%Y\")\n\ndf.PublicationYear = df.PublicationYear.str.replace(r\"[^0-9]\",'')#.astype(int) # astype int fails due to nans. Parsing as floats would give us \"ugly\" .0 per number","16a6be54":"df.nunique()","0a7e6e24":"df[\"title_sum_total\"] = df.groupby(\"Title\")[\"Checkouts\"].transform(\"sum\")\ndf[\"title_MonthCounts_total\"] = df.groupby(\"Title\")[\"Checkouts\"].transform(\"count\")\n\ndf.describe()","b73a997e":"df[\"title_sum_total\"].hist(bins=30)","e0cc987c":"df[\"title_MonthCounts_total\"].hist()","3a3d7670":"df[\"total_checkouts_all_monthly_sum\"] = df.groupby([\"CheckoutYear\",\"CheckoutMonth\"])[\"Checkouts\"].transform(\"sum\")\n\ndf[\"total_checkouts_ByMaterial_monthly_sum\"] = df.groupby([\"MaterialType\",\"CheckoutYear\",\"CheckoutMonth\"])[\"Checkouts\"].transform(\"sum\")","542688c9":"# we see a lot of overlap for some \"mixed\" categories\ndf.groupby(\"MaterialType\")[\"total_checkouts_ByMaterial_monthly_sum\"].plot()","19602faa":"df[\"total_checkouts_all_monthly_sum\"].plot()","bd6b3076":"print(\"Original shape: %i\" %(df.shape[0]))\ndf = df.loc[(df[\"title_MonthCounts_total\"]>8) & (df[\"title_sum_total\"]>40)]\nprint(\"We keep items that were checked out at least once in 8+ different months, AND checked out at least 30 times in aggregate\")\nprint(\"New shape: %i\" %(df.shape[0]))","6e72b5b1":"# Describe new counts\/sum distribution for our \"popular\" subset\ndf.describe()","cef4601a":"df.nlargest(10, columns=[\"title_MonthCounts_total\"])","34c2bd88":"df.nlargest(15, columns=[\"total_checkouts_all_monthly_sum\"])","83c4a321":"df = df.loc[df.Title !=\"<Unknown Title>\"]\nprint(df.shape)","76ab496b":"df.nlargest(10, columns=[\"title_MonthCounts_total\"])","fa47e2e5":"df.tail()","c947cdf0":"df.columns","f7102461":"df.to_csv(\"LibraryCheckouts_WithLeaks_v1.csv.gz\",compression=\"gzip\")\n# df.drop([])","d8616658":"# Let's look at some of the popular items\n\n* We find a data issue: \"<Unknown Title>\" is very common (although we could probably create a new ID using the concatenation of subject+publisher, we may wish to drop it for now, as it creates outliers)\n    \n    * It seems that the popular titles are mostly not what we might expect! This could be a data issue in our process... or just the result of popular romance and historical novels being popular or taken out in batch :) ","dae03c5c":"# Let's filter out the \"less popular\" items. \n* While interesting for many uses cases (and specific users, e.g. research textbook), we want to get at least moderately popular items\n\n* We could also look at subsets by subject: i.e get total monthly checkouts per publisher (for popular subjects or publishers) , then  see which checked-out items have a larger proportional share of this. \n\n* the number of distinct months an item was checked out in, is also important for building a model to predict monthly checkouts per item: Our model will \"see\" each item only once per month, so # occurences is going to affect this strongly! ","bcb41584":"Let's get the most popular items over all the time-period covered:\n* Ideally we would also normalize titles' text and versions of the same product, but I'll ignore for now.\n* Could also look at distinct counts, (i.e not just # checkouts, but count of distinct months when it was checked out). \n* We could also look at the quarterly level. Or even better, look at items as a % of total checkouts for that month per library. Or reaggregate per quarter","f1f796ba":"## Add total library checkouts for the library per month.\n* Relevant: \n    * Distinct checkouts (proxy for # visitors)\n    * Checkouts per MaterialType (books vs music..) ","f1a2b71e":"* Unsurprisingly, the distribution is very biased with a super long tail, and a \"bump\" for the very popular items! ","ffb4c500":"* ## Drop *Unknown Title* for simplicity\n    * Ideally we would impute this or create a hummy ID. Easier to ignore for now"}}