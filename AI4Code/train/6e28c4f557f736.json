{"cell_type":{"d3b16075":"code","817172f8":"code","0d7490b6":"code","ab1bc3be":"code","1595f1ca":"code","8603456b":"code","f4732ce9":"code","b82c5302":"code","1222322c":"code","cf7186c4":"code","e8b3ff5a":"code","d0233c39":"code","5a0290bd":"code","3587b940":"code","093a2d67":"code","a8efa428":"code","839570c9":"code","08336fd0":"code","8afd0f8c":"code","0fabf91e":"code","e9f0d0a4":"code","2387e606":"code","b574a6b2":"code","6204f2cd":"code","d2058a76":"code","e944a849":"code","36414691":"code","38404e17":"code","4d69ad3b":"code","79e9803b":"code","71168ee7":"markdown","56f755cd":"markdown","3b1ba59a":"markdown","e8ea2ea2":"markdown","affb92a1":"markdown","9c211a4b":"markdown","e3486ecf":"markdown","fdd4a883":"markdown","4019a113":"markdown","7da72468":"markdown"},"source":{"d3b16075":"#Load packages\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom collections import defaultdict\nfrom collections import  Counter\nfrom matplotlib import pyplot as plt\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam","817172f8":"df_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', sep=',')\ndf_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv',sep=',')\ndf_train.shape\n","0d7490b6":"ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n\ntrain_orig = df_train \ntrain_orig[train_orig['id'].isin(ids_with_target_error)]\n\ntrain_orig.at[train_orig['id'].isin(ids_with_target_error),'target'] = 0\n\ntrain_orig[train_orig['id'].isin(ids_with_target_error)]\n\ndf_train = train_orig","ab1bc3be":"#### Check missing values\ndf_train.isnull().sum()","1595f1ca":"### Check distribution of class labels. \nx = df_train.target.value_counts()\ncountplt = sns.barplot(x.index,x)\ncountplt.set_xticklabels(['0: Not Disaster (4342)', '1: Disaster (3271)'])\n#plt.gca().set_ylabel('samples')\n#Ref: https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove","8603456b":"### Plot tweets distribution over locations, that are aggregated in to a country's level\ntrain = df_train\ntrain['location'].replace({'United States':'USA',\n                           'New York':'USA',\n                            \"London\":'UK',\n                            \"Los Angeles, CA\":'USA',\n                            \"Washington, D.C.\":'USA',\n                            \"California\":'USA',\n                             \"Chicago, IL\":'USA',\n                             \"Chicago\":'USA',\n                            \"New York, NY\":'USA',\n                            \"California, USA\":'USA',\n                            \"FLorida\":'USA',\n                            \"Nigeria\":'Africa',\n                            \"Kenya\":'Africa',\n                            \"Everywhere\":'Worldwide',\n                            \"San Francisco\":'USA',\n                            \"Florida\":'USA',\n                            \"United Kingdom\":'UK',\n                            \"Los Angeles\":'USA',\n                            \"Toronto\":'Canada',\n                            \"San Francisco, CA\":'USA',\n                            \"NYC\":'USA',\n                            \"Seattle\":'USA',\n                            \"Earth\":'Worldwide',\n                            \"Ireland\":'UK',\n                            \"London, England\":'UK',\n                            \"New York City\":'USA',\n                            \"Texas\":'USA',\n                            \"London, UK\":'UK',\n                            \"Atlanta, GA\":'USA',\n                            \"Mumbai\":\"India\"},inplace=True)\n\nsns.barplot(y=train['location'].value_counts()[:5].index,x=train['location'].value_counts()[:5],\n            orient='h')\n\n# Ref: https:\/\/www.kaggle.com\/alex094495\/getting-started-with-nlp-a-general-intro\/edit","f4732ce9":"## examine the \"keyword\" distribution\ndf_train['keyword'].value_counts()","b82c5302":"print('keywords for disaster tweets:','\\n', df_train[df_train.target==1].keyword.value_counts().head(10), '\\n')\nprint('keywords for non-disaster tweets:','\\n',df_train[df_train.target==0].keyword.value_counts().head(10))","1222322c":"tweet = df_train\nfig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n\nword=tweet[tweet['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\n#ax1.set_title('disaster')\nword=tweet[tweet['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\n#ax2.set_title('Not disaster')\n#fig.suptitle('Average word length in each tweet')\n\n### As we can see, tweet with disasters have greater average lenth of text. \n### Our assumptions is that description of real disasters are more formal.","cf7186c4":"### Tweet lenghths distributions comparison between real disaster tweet and non-disaster ones\ndef length(text):    \n    '''a function which returns the length of text'''\n    return len(text)\ntweet = df_train\ntweet['length'] = tweet['text'].apply(length)\n\nplt.rcParams['figure.figsize'] = (18.0, 6.0)\nbins = 150\nplt.hist(tweet[tweet['target'] == 0]['length'], alpha = 0.6, bins=bins, label='Not')\nplt.hist(tweet[tweet['target'] == 1]['length'], alpha = 0.8, bins=bins, label='Real')\nplt.xlabel('length')\nplt.ylabel('numbers')\nplt.legend(loc='upper right')\nplt.xlim(0,150)\nplt.grid()\nplt.show()\n\n##Ref: https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert","e8b3ff5a":"df=pd.concat([df_train,df_test])","d0233c39":"df","5a0290bd":"!pip install pyspellchecker","3587b940":"\n\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\ndf['text']=df['text'].apply(lambda x : remove_URL(x))\n\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\ndf['text']=df['text'].apply(lambda x : remove_html(x))\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndf['text']=df['text'].apply(lambda x: remove_emoji(x))\n\ndef clean_text(text):\n    text = re.sub(r'https?:\/\/\\S+', '', text) # Remove link\n    text = re.sub(r'\\n',' ', text) # Remove line breaks\n    text = re.sub('\\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces\n    return text\ndf['text']=df['text'].apply(lambda x: clean_text(x))\n\n\nfrom spellchecker import SpellChecker\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n        \ntext = \"corect me plese\"\ncorrect_spellings(text)\n\ndef clean_text(text):\n    text = re.sub(r'https?:\/\/\\S+', '', text) # Remove link\n    text = re.sub(r'\\n',' ', text) # Remove line breaks\n    text = re.sub('\\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces\n    return text\ndf['text']=df['text'].apply(lambda x: clean_text(x))\n# Ref: https:\/\/www.kaggle.com\/aaroha33\/disaster-tweets-evaluation-with-nlp \n","093a2d67":"### Turn cleaned tweets into corpus (lower case, non stop, alphabetical words)\nfrom tqdm import tqdm ### This is for a progress bar\ndef create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus\ncorpus=create_corpus(df)\n","a8efa428":"### Generate embedding dict from the GloVe txt file. So in the dictionary, every word is \n### associated with the GloVe representation of them\nimport numpy as np\nembedding_dict={}\n\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.50d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","839570c9":"MAX_LEN=50\ntokenizer_obj=Tokenizer() ## Initialize tokenizer\ntokenizer_obj.fit_on_texts(corpus) \nsequences=tokenizer_obj.texts_to_sequences(corpus)### Convert each tweet in the corpus into sequence of numbers\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')### Pad the sequences so that they have the same length\n\nword_index=tokenizer_obj.word_index\nword_index\n#Now each word in the corpus is associated with a number representing the location it is in the sequence","08336fd0":"### Generate embedding matrix using the dictionary \"embedding_dict={}\" from GloVe\nnum_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,50))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec","8afd0f8c":"### Train test split\ntrain=tweet_pad[:tweet.shape[0]]\ntest=tweet_pad[tweet.shape[0]:]\nX_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)","0fabf91e":"### Neural Network\nmodel=Sequential() ### Initiate neural network\n\nembedding=Embedding(num_words,50,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False) ### First layer is the embedding layer\n\nmodel.add(embedding) ## Add the GloVe embedding layer\n\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2)) ### RNN\nmodel.add(Dense(1, activation='sigmoid')) ### Sigmoid for binary classification\n\n\noptimzer=Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\n","e9f0d0a4":"\n\nmodel.summary() \n\n","2387e606":"\n## Train the model\nhistory=model.fit(X_train,y_train,batch_size=4,epochs=5,validation_data=(X_test,y_test),verbose=True)\n\n","b574a6b2":"\n\npredictions=model.predict(test)\npredictions=np.round(predictions).astype(int).reshape(3263)\n","6204f2cd":"sample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","d2058a76":"sample_submission[\"target\"] = predictions","e944a849":"sample_submission.tail(50)","36414691":"sample_submission.to_csv(\"submission.csv\", index=False)","38404e17":"from tensorflow.keras import metrics\nMETRICS = [\n      metrics.TruePositives(name='tp'),\n      metrics.FalsePositives(name='fp'),\n      metrics.TrueNegatives(name='tn'),\n      metrics.FalseNegatives(name='fn'), \n      metrics.BinaryAccuracy(name='accuracy'),\n      metrics.Precision(name='precision'),\n      metrics.Recall(name='recall'),\n      metrics.AUC(name='auc')]","4d69ad3b":"import matplotlib.gridspec as gridspec\ndef plot_model_eval(history):\n\n    string = ['loss', 'accuracy']  \n    cnt = 0\n    ncols, nrows = 2, 1  \n    fig = plt.figure(constrained_layout=True, figsize = (10,10))\n    gs = gridspec.GridSpec(ncols = 3, nrows = 2, figure = fig)\n    for i in range(nrows):\n        for j in range(ncols):\n            ax = plt.subplot(gs[i,j]) \n            ax.plot(history.history[string[cnt]])\n            ax.plot(history.history['val_'+string[cnt]]) \n            ax.set_xlabel(\"Epochs\")\n            ax.set_ylabel(string[cnt])\n            ax.legend([string[cnt], 'val_'+string[cnt]])\n            cnt +=1\n        \nplot_model_eval(history)\n\n","79e9803b":"## Getting Started : https:\/\/www.kaggle.com\/alex094495\/getting-started-with-nlp-a-general-intro\/edit\n## (\u5f88\u7b80\u5355\u5f88\u57fa\u7840\u7684\u4e1c\u897f\uff09","71168ee7":"## Model Building","56f755cd":"### Make Prediction","3b1ba59a":"# NLP - EDA, Bag of Words, TF IDF, GloVe, BERT: https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert ","e8ea2ea2":"## Data cleaning\nremove urls, html, emoji, punctuation\n\nspell checker\n\nhttps:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove","affb92a1":"### Data Prep Completed HERE","9c211a4b":"### Predicting real disaster tweet using GloVe and LSTM\nTo determine whether a person\u2019s words are actually announcing a disaster is important: \nDataset description:","e3486ecf":"### Fixing data source error\nRef: https:\/\/www.kaggle.com\/wrrosa\/keras-bert-using-tfhub-modified-train-data\nThere are several tweets in the dataset are incorrectly labeled. In fact, they are not disaster related by they are labeled so. This will actually harm the model performance. ","fdd4a883":"To do: \n- Use crawled tweets outside of the dataset","4019a113":"### EDA","7da72468":"### Model Evaluation\nhttps:\/\/www.kaggle.com\/mjvakili\/glove-bidirectional-lstm"}}