{"cell_type":{"398b423b":"code","5df87d0b":"code","7771ed84":"code","fba69312":"code","ec1f10a7":"code","c81290ff":"code","6cb73361":"code","341814b7":"code","a80a6094":"code","2eeed2ed":"markdown","f5d233e9":"markdown","5686f283":"markdown","5d34a76d":"markdown","2320c0d3":"markdown","0ed12363":"markdown","ac5ff0ed":"markdown","5a0d1501":"markdown","9c4edbca":"markdown","74e1f51d":"markdown","e6dc707e":"markdown","ed6b60cf":"markdown"},"source":{"398b423b":"import pandas as pd \nimport numpy as np\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold \nimport gc \n\nimport warnings \nwarnings.filterwarnings('ignore')","5df87d0b":"#There are too many files, so just try 10,000. It will take too long to turn the whole things around.\n\n# na_values to get set a list of values to be read as NaN or NA while creating DataFrame, and nrows = 10000 to make dataframe small \nprint('loading files...')\ntrain = pd.read_csv('..\/input\/train.csv', na_values=-1, nrows =10000)  \ntest = pd.read_csv('..\/input\/test.csv', na_values=-1, nrows = 10000)","7771ed84":"#Drop useless data.\ncol_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\ntrain = train.drop(col_to_drop, axis=1)\ntest = test.drop(col_to_drop, axis=1)","fba69312":"train.info()","ec1f10a7":"for c in train.select_dtypes(include=['float64']).columns:\n    train[c] = train[c].astype(np.float32)\n    test[c] = test[c].astype(np.float32)\n\nfor c in train.select_dtypes(include=['int64']).columns[2:]: # \uc65c [2:] \ud558\ub294\uac70 \ub178\uc774\ud574 \n    train[c] = train[c].astype(np.int8)\n    test[c] = test[c].astype(np.int8)\n    \nprint(train.shape, test.shape)","c81290ff":"# custom objective function (similar to auc)\n\ndef gini(y, pred):\n    g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n    g = g[np.lexsort((g[:,2], -1*g[:,1]))]\n    gs = g[:,0].cumsum().sum() \/ g[:,0].sum()\n    gs -= (len(y) + 1) \/ 2.\n    return gs \/ len(y)\n\ndef gini_xgb(pred, y):\n    y = y.get_label()\n    return 'gini', gini(y, pred) \/ gini(y, y)\n\ndef gini_lgb(preds, dtrain):\n    y = list(dtrain.get_label())\n    score = gini(y, preds) \/ gini(y, y)\n    return 'gini', score, True","6cb73361":"#xgb \nparams = {'eta': 0.02, \n          'max_depth': 4,\n          'subsample': 0.9,\n          'colsample_bytree':0.9, #dive the fold in 0.1 : 0.9\n          'objective': 'binary:logistic', #logistic the binary classifiaction \n          'eval_metric': 'auc',\n          'silent': True}\n\nX = train.drop(['id', 'target'], axis =1)\nfeatures = X.columns\nX = X.values\ny = train['target'].values\nsub = test['id'].to_frame()\nsub['target'] = 0\n\nnrounds = 200\nkfold = 2\nskf = StratifiedKFold(n_splits=kfold, random_state=0, shuffle =True) #StratifiedkFold since the dataset is inbalance. ","341814b7":"for i, (train_index, test_index) in enumerate(skf.split(X,y)): #You have to put 'y' value in to show how to divide it\n    print(' xgb kfold: {} of {}: '.format(i+1,kfold))\n    X_train, X_valid = X[train_index], X[test_index]\n    y_train, y_valid = y[train_index], y[test_index]\n    d_train = xgb.DMatrix(X_train, y_train)  #xgb is a DMatrix data structure, so you have to make it if you have to make it faster.\n    d_valid = xgb.DMatrix(X_valid, y_valid)\n    \n    watchlist = [(d_train, 'train'), (d_valid, 'valid')] #watchlistis to check whether our learning is going well or not. If it's not going well, it can stop it\n    xgb_model = xgb.train(params, \n                          d_train, \n                          nrounds, \n                          watchlist, \n                          early_stopping_rounds=100, #standard for 'early_stooping_rounds' to stop is to check and stop the watch list.\n                          feval=gini_xgb, #Outputs a function to be executed the 'gini' values\n                          maximize=True,\n                          verbose_eval=100)\n    \n    #sub['target'] += xgb_model.predict(xgb.DMatrix(test[features].values),ntree_limit = xgb_model.best_ntree_limit+50 \/ (2 * kfold))\n    #ntree_limit specify the value when to stop. \n    #In other words, make a prediction only 50 more rounds from the highest Gini coefficient. \n    #We gave it to Kfold here. Because it was divided into two, it was divided into four quarters because each of them made a submission.\ngc.collect()\nsub.head(2)","a80a6094":"# lgb. Same code as xgb\nparams = {'metric': 'auc', \n          'learning_rate' : 0.01, \n          'max_depth':10, \n          'max_bin':10,  \n          'objective': 'binary', \n          'feature_fraction': 0.8,\n          'bagging_fraction':0.9,\n          'bagging_freq':10,  \n          'min_data': 500}\n\nskf = StratifiedKFold(n_splits=kfold, random_state=1, shuffle=True)\nfor i, (train_index, test_index) in enumerate(skf.split(X,y)):\n    print(' lgb kfold: {} of {} : '.format(i+1,kfold))\n    X_train, X_eval = X[train_index], X[test_index]\n    y_train, y_eval = y[train_index], y[test_index]\n    \n    lgb_model = lgb.train(params, \n                          lgb.Dataset(X_train, label=y_train), \n                          nrounds, \n                          lgb.Dataset(X_eval, label=y_eval), \n                          verbose_eval=100, \n                          feval=gini_lgb, \n                          early_stopping_rounds=100)  \n    \nsub.to_csv('sub10.csv', index=False, float_format='%.5f') \ngc.collect()\nsub.head(2)    \n    ","2eeed2ed":"# Conclusion\n**XGBoost is currently the dominant algorithm for building accurate models on conventional data (also called tabular or strutured data). Go apply it to improve your models!**","f5d233e9":"# XGB_Parameters\n\nYou should know the parameters of the models well. If you search Google, it is shown in documentation. How to tune parameters and their meanings are shown. \n\nThe parameters are different xgb and lgb model, but the parameters are almost the same. If you look at these, they have different names, but there are some that say the same things. You need to understand the algorithm a little to understand the meaning of these parameters. \n\nThat's why we have to study algorithms.\n\n\n#### **1) learning_rate** [default=0.3, allas: eta]: range[0,1]\n* Step size shrinkage used in update to prevents overfitting. After each boosting step, we can diresctly get the weights of new features. \n* 'learning_rate' shrinks the feature weights to make the boosting process more conservative. \n*  It is easier to tune other parameters if it is set to 0.05 to 0.1 or higher. \n*  If you want fine accuracy adjustment, leave it. Once the other parameters are fixed, there is no need to change them. \n*  Also, tuning to unnecessarily long decimal places is useless.\n \n#### **2) min_split_loss** [defalut=0, alas: gamma]: range[0,\u221e]\n\n*  Minimum loss reduction required to make a further partition on a leaf node of the tree. \n* The larger gamma is, the more conservative the algorithm will be.\n  \n#### **3) max_depth** [default=6]: range: [0,\u221e] \n* Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. \n* 0 is only accepted in lossguided growing policy when tree_method is set as hist or gpu_hist and it indicates no limit on depth. \n* Beware that XGBoost aggressively consumes memory when training a deep tree.\n* If set to -1, branch out without restriction. If there are many features, set it higher, and set it first when setting parameters.\n \n#### **4) subsample** [default=1]: range: (0,1]\n* Subsample ratio of the training instances.\n* Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \n* Subsampling will occur once in every boosting iteration.\n \n#### **5) colsample_bytree** [default=1]: range (0, 1]\n* It specify the fraction of columns to be subsampled.\n* Increase the diversity of each column by sampling it. \n* It was a function in a random forest, usually with higher accuracy.\n* 1 without column sampling is the default, but it is common to set it to 0.7 to 0.9.","5686f283":"### What is Gini index?\nGini index or Gini impurity measures the degree or probability of a particular variable being wrongly classified when it is randomly chosen.\n\nThe degree of Gini index varies between 0 and 1, where 0 denotes that all elements belong to a certain class or if there exists only one class, and 1 denotes that the elements are randomly distributed across various classes. \n\nA Gini Index of 0.5 denotes equally distributed elements into some classes.","5d34a76d":"## StratifiedKFold\n\n* We used the **StratifiedKFlod method**, since the data is unbalanced. \n![image.png](attachment:image.png)\n\nFor example, in the financial transaction fraud classification model, the number of normal transactions out of the total data is 95% fraudulent, and the number of transactions is 5%. When data is divided by the general cross-validation described earlier, the number of fraudulent transactions may not be divided evenly and may be concentrated in one segment.\n\nAt this point, considering the distribution by data class, a method of creating a data fold set is layer-wise k-layer cross-validation.\nSplit method considering distribution by data class so that normal transactions and fraudulent transactions can be included in each data fold.\n\nThis method should be used in situations where data by data class is very unbalanced.","2320c0d3":"The target values is not appropriate since we used much less data than we should use.","0ed12363":"## Xgb: \n**XGBoost is an Ensemble algorithm that uses a combination of several Decision trees.**\n* It has excellent efficiency, flexibility, and portability.\n* Flexible Learning System - You can adjust multiple parameters to create an optimal model.\n* Overfitting can be prevented.\n* Compared to neural networks, visualization is easier and more intuitive than understanding.\n* It supports Cross validation.\n* It represents high performance. More than 90% of the datasets showed higher performance than GBMs. Also, as the results of actually writing XGBoost in Kaggle began to sweep the top ranks, people began to use it, both for you and me.\n\nBecause Stacking is taught separately by dividing each fold, it is necessary to predict the value of the test test with the model from here and combine it, so if a folder with only zero is divided, it is meaningless to give it out.","ac5ff0ed":"# LGB_Parameters\n**The hyperparameters of lgb are similar, but because lgb uses leaf-wise algorithms, the value of the leaf-wise hyperparameters is added.** I explained the parameter that are not explained in the XGB parameter. \n\n**1)** **num_iterations [default=100]**: Specifies the number of trees that you want to repeat. Setting too large will result in overfitting.\n\n**2)** **min_data_in_leaf [default=20]**: Parameters such as min_samples_leaf in the decision tree. It is used as a parameter to control overfitting.\n\n**3)** **num_leaves [default=3]**: Indicates the maximum number of leaves a tree can have.\n\n**4)** **bagging_fraction [default=1.0]**: Specifies the percentage at which data is sampled. Used to control overfitting.\n\n**5)** **feature_fraction\t [default=1.0]**: The percentage of features randomly selected when learning an individual tree.\n\n**6)** **lambda_l1**: Value for L1 regulation control.\n\n**7)** **lambda_l2**: Value for L2 regulation control.\n\n","5a0d1501":"# Introduction\n\nIn this notebook, we are going to use the  **Stacking Ensemble Machine Learning** method. Stacking or Stacked Generalization is an ensemble machine learning algorithm.\n\nIt uses a meta-learning algorithm to learn how to best combine the predictions from two or more base machine learning algorithms.\n\nThe benefit of stacking is that it can harness the capabilities of a range of well-performing models on a classification or regression task and make predictions that have better performance than any single model in the ensemble.\n\nThank you @Rudolph. I used his notebook to study. I made more detailed about it and plus description about the models it use. \n\n\n\n\n### **After studying this notebook, you will learn**:\n\n* Stacking is an ensemble machine learning algorithm that learns how to best combine the predictions from multiple well-performing machine learning models.\n* The scikit-learn library provides a standard implementation of the stacking ensemble in Python.\n* How to use stacking ensembles for regression and classification predictive modeling.","9c4edbca":"# Import Packages & Data ","74e1f51d":"If you want more details about parameters. Go to https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html","e6dc707e":"<a id='1'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\"> \ud83d\udccaStacking_ XGB & LGB  \ud83d\udcda<\/p>","ed6b60cf":" ## lgb: \n####  **LightGBM is a gradient boosting framework that uses tree based learning algorithms.**\n\n* Faster training speed and higher efficiency.\n* Lower memory usage.\n* Better accuracy.\n* Support of parallel, distributed, and GPU learning.\n* Capable of handling large-scale data."}}