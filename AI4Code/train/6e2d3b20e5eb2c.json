{"cell_type":{"adacdc24":"code","b5bb3ec9":"code","5d4d3952":"code","dfff87d0":"code","30196a79":"code","00ee24db":"code","3259a493":"code","cfd39f50":"code","57a29cfb":"code","b4face06":"code","aebc79e3":"code","ec2891ec":"code","a44a4f9b":"code","26df5ec6":"code","10d92d55":"code","97850227":"code","26853c91":"code","a4ce6897":"code","f384125e":"code","d3534693":"code","65700c32":"code","9a46a1c6":"code","b219817a":"code","2dba43d2":"code","58a304a0":"code","a63f45f4":"code","1639ff2b":"code","69652b5a":"code","f6743b1c":"code","c1e8f0cf":"code","2a4bf74d":"code","20f91c3c":"code","a50b1255":"code","abdbcfcd":"code","5cb49028":"code","9a157042":"code","1322dfeb":"code","56aaf60d":"code","d4d4c995":"code","29e30e37":"code","bbd1ab63":"code","e090368f":"code","c1f8ff3f":"code","d6ca4ce9":"code","78bef1e0":"code","14571012":"code","5e14779f":"code","cafe4892":"code","01a343d4":"code","c35ed62b":"code","4abb4b6b":"code","b53ac959":"code","c4cbd078":"code","ce9af7b9":"code","6d98471d":"code","0ef9ad15":"code","909e0295":"code","0cd1b0dd":"code","51490649":"markdown","d3365afe":"markdown","7f6a6abf":"markdown","945ab3f6":"markdown","9d897e49":"markdown","43acd60f":"markdown","1f4f54c9":"markdown","a4878f36":"markdown","cc3a8f34":"markdown","39f8d4ba":"markdown","a6ee8fda":"markdown","6c05a710":"markdown","346dc465":"markdown","7587fee1":"markdown","70086420":"markdown","93486917":"markdown","cc4d3186":"markdown","223e25e5":"markdown","85a17335":"markdown","10832c85":"markdown","a7bb6663":"markdown","c282a26c":"markdown","11da116d":"markdown","25c083b0":"markdown","503a98dc":"markdown","b5433705":"markdown","7b63a128":"markdown","cadb0a4a":"markdown","e1e066ad":"markdown","5ed4ebfa":"markdown","f93fc181":"markdown","a2151d4d":"markdown","bf4b1118":"markdown","01178792":"markdown","4e46b024":"markdown","757455cc":"markdown","71c69a3e":"markdown","7cea70f5":"markdown","15626224":"markdown","176ee839":"markdown","e0b6e09f":"markdown","9bb6d580":"markdown","6db7fdc7":"markdown","5effe11d":"markdown","d430c336":"markdown","b0b6af3d":"markdown","8d6448d3":"markdown","611ed584":"markdown","e27c9f19":"markdown","91174307":"markdown","388c097f":"markdown","64d3ca30":"markdown","abacf961":"markdown","6bbd1ad3":"markdown"},"source":{"adacdc24":"import numpy as np\nimport pandas as pd\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport torch\nimport copy\nimport math\nimport gc\nfrom tqdm import tqdm\nimport torch.utils.data as D\nimport random\nimport os\nfrom transformers import AutoModelWithLMHead, AutoTokenizer,RobertaConfig, RobertaModel,AutoModelForSequenceClassification,AutoModelForMaskedLM\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch import nn\nfrom torch import optim\nimport time\nimport torch.nn.functional as F\nfrom transformers import (\n    AutoModel,\n    CONFIG_MAPPING,\n    MODEL_MAPPING,\n    AdamW,\n    AutoConfig,\n    AutoModelForMaskedLM,\n    AutoTokenizer,\n    DataCollatorForLanguageModeling,\n    SchedulerType,\n    get_scheduler,\n    set_seed,\n)\nfrom sklearn.linear_model import Ridge,Lasso\nimport lightgbm as lgb\nimport time\nimport seaborn as sns","b5bb3ec9":"class CFG:\n    seed=12345\n    test_avg_n=5\n    val_avg_n=5\n    max_len=256\n    batch_size=24\n    dropout_p=0.1\n    folds=5\n    cv_shuffle=False\n    pad_token_id=1\n    device=torch.device('cuda:0')\n    dtype=torch.float32","5d4d3952":"train_df=pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\ntest_df=pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')\nres_df=pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv')","dfff87d0":"class RobertaDataset(D.Dataset):\n    def __init__(self, token, target):\n        self.token = token\n        self.target = target\n        \n    def __len__(self):\n        return self.token.shape[0]\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.token[idx].input_ids), \\\n                torch.tensor(self.token[idx].attention_mask), self.target[idx]\n    \ndef collate(batch):\n    ids, attns, targets = zip(*batch)\n    ids = pad_sequence(ids, batch_first=True,padding_value=CFG.pad_token_id).to(CFG.device)\n    attns = pad_sequence(attns, batch_first=True,padding_value=CFG.pad_token_id).to(CFG.device)\n    targets = torch.tensor(targets).float().to(CFG.device)\n    return ids, attns, targets\n\ndef CV_split(m,k=5,shuffle=False,seed=7):\n    index=np.arange(m)\n    if shuffle:\n        np.random.seed(seed)\n        np.random.shuffle(index)\n    test_size=math.ceil(m\/k)\n    split_indices=[]\n    for i in range(k):\n        bool_index=np.zeros(m)\n        bool_index[test_size*i:test_size*(i+1)]=1\n        bool_index=bool_index.astype('bool')\n        val_index=index[bool_index]\n        train_index=index[~bool_index]\n        split_indices.append((train_index,val_index))\n    return split_indices\n\ndef rmse(y1,y2):\n    score=np.sqrt(((y1-y2)**2).mean())\n    return score\n\ndef score_test(model,test_ldr,mode='train',avg_n=1):\n    if mode=='eval':\n        model.eval()\n    elif mode=='train':\n        model.train()\n    avg_pred=pd.DataFrame()\n    for i in range(avg_n):\n        preds=[]\n        for texts, attns, idx in test_ldr:\n            with torch.no_grad():\n                pred = model(texts,attns)\n                preds.append(pred)\n        preds=torch.cat(preds,axis=0)\n        preds=preds.to('cpu').numpy().reshape(-1)\n        avg_pred[f'pred{i+1}']=preds\n    #print(avg_pred.corr())\n    return avg_pred.values.mean(axis=1)\n\ndef tokenize(tokenizer,texts):\n    tokens=[]\n    for text in texts:\n        token=tokenizer(text,max_length=CFG.max_len,truncation=True, padding='max_length',add_special_tokens=True)\n        tokens.append(token)\n    return tokens","30196a79":"class MyModel1(nn.Module):\n    def __init__(self, model):\n        super(MyModel1, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\n    \nF1_CFG={}\nF1_CFG['max_len']=256\nF1_CFG['seed']=12345\nF1_CFG['batch_size']=24\nF1_CFG['dropout_p']=0.1","00ee24db":"class MyModel2(nn.Module):\n    def __init__(self, model):\n        super(MyModel2, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\n    \nF2_CFG={}\nF2_CFG['max_len']=256\nF2_CFG['seed']=7\nF2_CFG['batch_size']=8\nF2_CFG['dropout_p']=0.1","3259a493":"class MyModel3(nn.Module):\n    def __init__(self, model):\n        super(MyModel3, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\n    \nF3_CFG={}\nF3_CFG['max_len']=256\nF3_CFG['seed']=33\nF3_CFG['batch_size']=24\nF3_CFG['dropout_p']=0.1","cfd39f50":"class MyModel5(nn.Module):\n    def __init__(self, model):\n        super(MyModel5, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\n    \nF5_CFG={}\nF5_CFG['max_len']=256\nF5_CFG['seed']=12345\nF5_CFG['batch_size']=24\nF5_CFG['dropout_p']=0.1","57a29cfb":"class MyModel6(nn.Module):\n    def __init__(self, model):\n        super(MyModel6, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\n    \nF6_CFG={}\nF6_CFG['max_len']=256\nF6_CFG['seed']=12345\nF6_CFG['batch_size']=24\nF6_CFG['dropout_p']=0.1","b4face06":"class MyModel7(nn.Module):\n    def __init__(self,model):\n        super(MyModel7,self).__init__()\n        self.roberta = model             \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        return self.regressor(context_vector)\n    \nF7_CFG={}\nF7_CFG['max_len']=256\nF7_CFG['seed']=666\nF7_CFG['batch_size']=24\nF7_CFG['dropout_p']=0.1","aebc79e3":"class MyModel8(nn.Module):\n    def __init__(self, model):\n        super(MyModel8, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.output_layer(text_emb)\n        return x2\nF8_CFG={}\nF8_CFG['max_len']=256\nF8_CFG['seed']=54321\nF8_CFG['batch_size']=24\nF8_CFG['dropout_p']=0.1","ec2891ec":"class MyModel9(nn.Module):\n    def __init__(self, model):\n        super(MyModel9, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.output_layer(text_emb)\n        return x2\nF9_CFG={}\nF9_CFG['max_len']=256\nF9_CFG['seed']=54321\nF9_CFG['batch_size']=24\nF9_CFG['dropout_p']=0.1","a44a4f9b":"class MyModel10(nn.Module):\n    def __init__(self, model):\n        super(MyModel10, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(768,768)\n        self.linear2 = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF10_CFG={}\nF10_CFG['max_len']=256\nF10_CFG['seed']=12345\nF10_CFG['batch_size']=24\nF10_CFG['dropout_p']=0.1","26df5ec6":"class MyModel11(nn.Module):\n    def __init__(self, model):\n        super(MyModel11, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1024)\n        self.linear2 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\nF11_CFG={}\nF11_CFG['max_len']=256\nF11_CFG['seed']=7\nF11_CFG['batch_size']=8\nF11_CFG['dropout_p']=0.1","10d92d55":"class MyModel12(nn.Module):\n    def __init__(self, model):\n        super(MyModel12, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\n    \nF12_CFG={}\nF12_CFG['max_len']=256\nF12_CFG['seed']=12345\nF12_CFG['batch_size']=24\nF12_CFG['dropout_p']=0.1","97850227":"class MyModel13(nn.Module):\n    def __init__(self, model):\n        super(MyModel13, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(768,768)\n        self.linear2 = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF13_CFG={}\nF13_CFG['max_len']=256\nF13_CFG['seed']=12345\nF13_CFG['batch_size']=24\nF13_CFG['dropout_p']=0.1","26853c91":"class MyModel14(nn.Module):\n    def __init__(self, model):\n        super(MyModel14, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\n    \nF14_CFG={}\nF14_CFG['max_len']=256\nF14_CFG['seed']=12345\nF14_CFG['batch_size']=24\nF14_CFG['dropout_p']=0.1","a4ce6897":"class MyModel15(nn.Module):\n    def __init__(self, model):\n        super(MyModel15, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1024)\n        self.linear2 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\nF15_CFG={}\nF15_CFG['max_len']=256\nF15_CFG['seed']=7\nF15_CFG['batch_size']=8\nF15_CFG['dropout_p']=0.1","f384125e":"class MyModel16(nn.Module):\n    def __init__(self, model):\n        super(MyModel16, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1024)\n        self.linear2 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\nF16_CFG={}\nF16_CFG['max_len']=256\nF16_CFG['seed']=7\nF16_CFG['batch_size']=8\nF16_CFG['dropout_p']=0.1","d3534693":"class MyModel17(nn.Module):\n    def __init__(self, model):\n        super(MyModel17, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\n    \nF17_CFG={}\nF17_CFG['max_len']=256\nF17_CFG['seed']=12345\nF17_CFG['batch_size']=24\nF17_CFG['dropout_p']=0.1","65700c32":"class MyModel18(nn.Module):\n    def __init__(self,model):\n        super(MyModel18,self).__init__()\n        self.model = model             \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.model(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        return self.regressor(context_vector)\n    \nF18_CFG={}\nF18_CFG['max_len']=256\nF18_CFG['seed']=755\nF18_CFG['batch_size']=24\nF18_CFG['dropout_p']=0.1","9a46a1c6":"class MyModel19(nn.Module):\n    def __init__(self, model):\n        super(MyModel19, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\nF19_CFG={}\nF19_CFG['max_len']=256\nF19_CFG['seed']=755\nF19_CFG['batch_size']=24\nF19_CFG['dropout_p']=0.1","b219817a":"class MyModel20(nn.Module):\n    def __init__(self, model):\n        super(MyModel20, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\nF20_CFG={}\nF20_CFG['max_len']=256\nF20_CFG['seed']=755\nF20_CFG['batch_size']=8\nF20_CFG['dropout_p']=0.1","2dba43d2":"class MyModel22(nn.Module):\n    def __init__(self, model):\n        super(MyModel22, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1024)\n        self.linear2 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF22_CFG={}\nF22_CFG['max_len']=256\nF22_CFG['seed']=7\nF22_CFG['batch_size']=8\nF22_CFG['dropout_p']=0.1","58a304a0":"class MyModel23(nn.Module):\n    def __init__(self, model):\n        super(MyModel23, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1024)\n        self.linear2 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF23_CFG={}\nF23_CFG['max_len']=256\nF23_CFG['seed']=7\nF23_CFG['batch_size']=8\nF23_CFG['dropout_p']=0.1","a63f45f4":"class MyModel24(nn.Module):\n    def __init__(self, model):\n        super(MyModel24, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(768,768)\n        self.linear2 = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF24_CFG={}\nF24_CFG['max_len']=256\nF24_CFG['seed']=721\nF24_CFG['batch_size']=24\nF24_CFG['dropout_p']=0.1","1639ff2b":"class MyModel25(nn.Module):\n    def __init__(self, model):\n        super(MyModel25, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(768,768)\n        self.linear2 = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF25_CFG={}\nF25_CFG['max_len']=256\nF25_CFG['seed']=721\nF25_CFG['batch_size']=24\nF25_CFG['dropout_p']=0.1","69652b5a":"class MyModel26(nn.Module):\n    def __init__(self, model):\n        super(MyModel26, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1024)\n        self.linear2 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF26_CFG={}\nF26_CFG['max_len']=256\nF26_CFG['seed']=721\nF26_CFG['batch_size']=8\nF26_CFG['dropout_p']=0.1","f6743b1c":"class MyModel27(nn.Module):\n    def __init__(self, model):\n        super(MyModel27, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1024)\n        self.linear2 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF27_CFG={}\nF27_CFG['max_len']=256\nF27_CFG['seed']=755\nF27_CFG['batch_size']=8\nF27_CFG['dropout_p']=0.1","c1e8f0cf":"class MyModel29(nn.Module):\n    def __init__(self, model):\n        super(MyModel29, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(768,768)\n        self.linear2 = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\nF29_CFG={}\nF29_CFG['max_len']=256\nF29_CFG['seed']=2\nF29_CFG['batch_size']=24\nF29_CFG['dropout_p']=0.1","2a4bf74d":"class MyModel30(nn.Module):\n    def __init__(self, model):\n        super(MyModel30, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1024)\n        self.linear2 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF30_CFG={}\nF30_CFG['max_len']=256\nF30_CFG['seed']=721\nF30_CFG['batch_size']=8\nF30_CFG['dropout_p']=0.1","20f91c3c":"class MyModel31(nn.Module):\n    def __init__(self, model):\n        super(MyModel31, self).__init__()\n        self.model = model\n        self.linear1 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\nF31_CFG={}\nF31_CFG['max_len']=256\nF31_CFG['seed']=7212\nF31_CFG['batch_size']=8\nF31_CFG['dropout_p']=0.1","a50b1255":"class MyModel32(nn.Module):\n    def __init__(self, model):\n        super(MyModel32, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF32_CFG={}\nF32_CFG['max_len']=256\nF32_CFG['seed']=20134\nF32_CFG['batch_size']=8\nF32_CFG['dropout_p']=0.1","abdbcfcd":"class MyModel33(nn.Module):\n    def __init__(self, model):\n        super(MyModel33, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF33_CFG={}\nF33_CFG['max_len']=256\nF33_CFG['seed']=20134\nF33_CFG['batch_size']=8\nF33_CFG['dropout_p']=0.1","5cb49028":"class MyModel34(nn.Module):\n    def __init__(self, model):\n        super(MyModel34, self).__init__()\n        self.model = model\n        self.linear1 = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['last_hidden_state']\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n\nF34_CFG={}\nF34_CFG['max_len']=256\nF34_CFG['seed']=201\nF34_CFG['batch_size']=24\nF34_CFG['dropout_p']=0.1","9a157042":"class MyModel35(nn.Module):\n    def __init__(self, model):\n        super(MyModel35, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(768,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF35_CFG={}\nF35_CFG['max_len']=256\nF35_CFG['seed']=75412\nF35_CFG['batch_size']=24\nF35_CFG['dropout_p']=0.1","1322dfeb":"class MyModel36(nn.Module):\n    def __init__(self, model):\n        super(MyModel36, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF36_CFG={}\nF36_CFG['max_len']=256\nF36_CFG['seed']=75412\nF36_CFG['batch_size']=8\nF36_CFG['dropout_p']=0.1","56aaf60d":"class MyModel37(nn.Module):\n    def __init__(self, model):\n        super(MyModel37, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(768,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF37_CFG={}\nF37_CFG['max_len']=256\nF37_CFG['seed']=900\nF37_CFG['batch_size']=24\nF37_CFG['dropout_p']=0.1","d4d4c995":"class MyModel38(nn.Module):\n    def __init__(self, model):\n        super(MyModel38, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(768,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF38_CFG={}\nF38_CFG['max_len']=300\nF38_CFG['seed']=900\nF38_CFG['batch_size']=24\nF38_CFG['dropout_p']=0.1","29e30e37":"class MyModel40(nn.Module):\n    def __init__(self, model):\n        super(MyModel40, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF40_CFG={}\nF40_CFG['max_len']=256\nF40_CFG['seed']=901\nF40_CFG['batch_size']=8\nF40_CFG['dropout_p']=0.1","bbd1ab63":"class MyModel41(nn.Module):\n    def __init__(self, model):\n        super(MyModel41, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF41_CFG={}\nF41_CFG['max_len']=256\nF41_CFG['seed']=111\nF41_CFG['batch_size']=8\nF41_CFG['dropout_p']=0.1","e090368f":"class MyModel42(nn.Module):\n    def __init__(self, model):\n        super(MyModel42, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF42_CFG={}\nF42_CFG['max_len']=256\nF42_CFG['seed']=1112\nF42_CFG['batch_size']=8\nF42_CFG['dropout_p']=0.1","c1f8ff3f":"class MyModel43(nn.Module):\n    def __init__(self, model):\n        super(MyModel43, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF43_CFG={}\nF43_CFG['max_len']=256\nF43_CFG['seed']=1112\nF43_CFG['batch_size']=8\nF43_CFG['dropout_p']=0.1","d6ca4ce9":"class MyModel50(nn.Module):\n    def __init__(self, model):\n        super(MyModel50, self).__init__()\n        self.model = model\n        self.linear1 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF50_CFG={}\nF50_CFG['max_len']=256\nF50_CFG['seed']=711223\nF50_CFG['batch_size']=8\nF50_CFG['dropout_p']=0.1","78bef1e0":"class MyModel51(nn.Module):\n    def __init__(self, model):\n        super(MyModel51, self).__init__()\n        self.model = model\n        self.linear1 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF51_CFG={}\nF51_CFG['max_len']=256\nF51_CFG['seed']=91\nF51_CFG['batch_size']=8\nF51_CFG['dropout_p']=0.1","14571012":"def inference_wrapper(model_name,model_class,model_CFG,test_df,train_df):\n    # moad model cfg\n    for key,val in model_CFG.items():\n        setattr(CFG,key,val)\n    # fix random seed    \n    random.seed(CFG.seed)\n    os.environ['PYTHONHASHSEED'] = str(CFG.seed)\n    np.random.seed(CFG.seed)\n    torch.manual_seed(CFG.seed)\n    torch.cuda.manual_seed(CFG.seed)\n    torch.cuda.manual_seed_all(CFG.seed)\n    torch.backends.cudnn.deterministic = True\n    # load model\n    notebook=model_name+'-train'\n    path='..\/input\/'+notebook+'\/model_init\/'\n    config = AutoConfig.from_pretrained(path, output_hidden_states=True,attention_probs_dropout_prob=CFG.dropout_p,hidden_dropout_prob=CFG.dropout_p)\n    tokenizer = AutoTokenizer.from_pretrained(path,model_max_length=CFG.max_len)\n    CFG.pad_token_id=tokenizer.pad_token_id\n    if int(model_name[1:])<=23:\n        model =AutoModelForMaskedLM.from_pretrained(path,config=config)\n    else:\n        model =AutoModel.from_pretrained(path,config=config)\n    model=model_class(model)\n    # load state dicts\n    state_dicts=[]\n    for i in range(CFG.folds):\n        checkpoint=torch.load('..\/input\/'+notebook+f'\/fold_{i+1}_model',map_location=CFG.device)\n        state_dicts.append(checkpoint['model_state_dict'])\n    # create data loader\n    \n    # test df\n    if test_df is not None:\n        test_df['token'] = tokenize(tokenizer,test_df.excerpt)\n        test_dataset = RobertaDataset(test_df.token, test_df.index)\n        # shuffle = False !!\n        test_dataloader = D.DataLoader(test_dataset, batch_size=CFG.batch_size,\n                                     shuffle=False, collate_fn = collate,num_workers=0)\n        test_df['test\/'+model_name]=0\n        for i in range(CFG.folds):\n            model.load_state_dict(state_dicts[i])\n            model.to(CFG.device)\n            preds=score_test(model,test_dataloader,mode='train',avg_n=CFG.test_avg_n)\n            test_df['test\/'+model_name]+=preds\/CFG.folds\n    # train\n    if train_df is not None:\n        print('bug?')\n        train_df['val\/'+model_name]=0\n        train_df['token'] = tokenize(tokenizer,train_df.excerpt)\n        train_dataset = RobertaDataset(train_df.token, train_df.index)\n        split_indices=CV_split(len(train_df),k=CFG.folds,shuffle=CFG.cv_shuffle,seed=7)\n        for i in range(CFG.folds):\n            train_index,val_index=split_indices[i]\n            val_dataset = D.Subset(train_dataset, val_index)\n            val_dataloader = D.DataLoader(val_dataset, batch_size=CFG.batch_size,\n                                          shuffle=False, collate_fn = collate,num_workers=0)\n            model.load_state_dict(state_dicts[i])\n            model.to(CFG.device)\n            preds=score_test(model,val_dataloader,mode='train',avg_n=CFG.val_avg_n)\n            train_df.loc[val_index,'val\/'+model_name]=preds\n    del model\n    del state_dicts\n    del checkpoint\n    del test_dataloader\n    torch.cuda.empty_cache()\n    gc.collect()\n    return ","5e14779f":"inference_tasks=[\n                 ('f2',MyModel2,F2_CFG),\n                 #('f14',MyModel14,F14_CFG),\n                 ('f17',MyModel17,F17_CFG),\n                 #('f20',MyModel20,F20_CFG),\n                 ('f23',MyModel23,F23_CFG),\n                 #('f30',MyModel30,F30_CFG),\n                 ('f32',MyModel32,F32_CFG),\n                 #('f33',MyModel33,F33_CFG),\n                 #('f36',MyModel36,F36_CFG),\n                 #('f40',MyModel40,F40_CFG),\n                 ('f41',MyModel41,F41_CFG),\n                ('f50',MyModel50,F50_CFG),\n                ('f51',MyModel51,F51_CFG),\n                ]\n\nfor i,(model_name,model_class,model_CFG) in tqdm(enumerate(inference_tasks)):\n    model_CFG['seed']=i\n    model_CFG['batch_size']*=6\n    inference_wrapper(model_name,model_class,model_CFG,test_df=test_df,train_df=None)\n","cafe4892":"berts_val=[]\nberts_test=[]\nfor model_name,_,_ in inference_tasks:\n    berts_val.append('val\/'+model_name)\n    berts_test.append('test\/'+model_name)","01a343d4":"train_df=pd.read_csv('..\/input\/f-inference-1-1\/ensemble_train.csv')","c35ed62b":"def feature_generate(df):\n    new_features=[]\n    # count of char\n    # rmk: as it is excerpt, so lengh may not matter\n    df['char_count']=df['excerpt'].apply(len)\n    new_features.append('char_count')\n    # count of words\n    df['word_count']=df['excerpt'].apply(lambda s:len(s.split(' ')))\n    new_features.append('word_count')\n    # count of sentence\n    # rmk: what if end with '!' '?' ...\n    df['sentence_count']=df['excerpt'].apply(lambda s:len(s.split('.')))\n    new_features.append('sentence_count')\n    # average char per word\n    df['c2w']=df['char_count']\/df['word_count']\n    new_features.append('c2w')\n    # avg word per sentence\n    df['w2s']=df['word_count']\/df['sentence_count']\n    new_features.append('w2s')\n    # count the unique words\n    df['unique_word_count'] = df['excerpt'].apply(lambda s: len(set( s.split(' ') )))\n    new_features.append('unique_word_count')\n    # text diversity\n    df['word_diversity'] = df['unique_word_count'] \/ df['word_count']\n    new_features.append('word_diversity')\n    # word lengths\n    words = df['excerpt'].apply(lambda s: s.split(' '))\n    word_lengths = words.apply(lambda s: [len(f) for f in s ])\n    df['longest_w_len'] = word_lengths.apply(max)\n    new_features.append('longest_w_len')\n    df['avg_w_len'] = word_lengths.apply(np.mean)\n    new_features.append('avg_w_len')\n#     # word freq\n#     word_freqs = words.apply(lambda s: [word_freq(f) for f in s ])\n#     df['avg_freq']=word_freqs.apply(np.mean)\n#     df['min_freq']=word_freqs.apply(min)\n#     new_features.append('avg_freq')\n#     new_features.append('min_freq')\n    return new_features","4abb4b6b":"extra_features=feature_generate(train_df)\n_=feature_generate(test_df)","b53ac959":"val_cols=berts_val\ntest_cols=berts_test","c4cbd078":"train_df['ridge_pred']=0\ntest_df['ridge_pred']=0\nmodel_weight=0\nbias=0\nfor i,(train_index,val_index) in enumerate(CV_split(len(train_df),k=CFG.folds,shuffle=False,seed=7)):\n    model=Ridge(alpha=0.025,fit_intercept=True,normalize=True)\n    model.fit(train_df.loc[train_index,val_cols],train_df.loc[train_index,'target'])\n    val_preds=model.predict(train_df.loc[val_index,val_cols])\n    test_preds=model.predict(test_df[test_cols])\n    train_df.loc[val_index,'ridge_pred']=val_preds\n    test_df['ridge_pred']+=test_preds\/CFG.folds\n    model_weight+=model.coef_\/CFG.folds\n    bias+=model.intercept_\/CFG.folds\nprint('ensemble cv score is:',rmse(train_df['target'],train_df['ridge_pred']))\nprint('################################################################')\nfor i in range(len(val_cols)):\n    print(val_cols[i],' weight is:',np.round(model_weight[i],3))\nprint('ensemble bias is:',bias)\nprint('################################################################')\nprint(train_df[val_cols].corr())","ce9af7b9":"params=\\\n  {'n_estimators':3000,\n   'boosting_type': 'gbdt',\n   'objective':'regression',\n   'metric':'rmse',\n   'subsample': 0.7, \n   'subsample_freq': 1,\n   #'num_leaves':124,\n   'min_data_in_leaf':40,\n   'feature_fraction_bynode':np.sqrt(0.6),\n   'feature_fraction': np.sqrt(0.6),            \n   'learning_rate': 0.005,\n   'max_bin':255,\n   #'cat_l2':10,\n   #'max_depth':5,\n   'boost_from_average':True,\n   'nthread' : 8,\n    'lambda_l1': 2,  \n    'lambda_l2': 20,\n  #'min_gain_to_split':0.0001\n   'early_stopping_rounds':200,\n   'verbose':-1\n    }","6d98471d":"val_cols=berts_val+extra_features\ntest_cols=berts_test+extra_features","0ef9ad15":"split_indices=CV_split(len(train_df),k=5,shuffle=False,seed=7)\ntrain_df['lgb_pred']=0\ntest_df['lgb_pred']=0\nfor fold,(train_index,val_index) in enumerate(split_indices):\n    X_train=train_df.loc[train_index,val_cols].values\n    X_val=train_df.loc[val_index,val_cols].values\n    Y_train=train_df.loc[train_index,'target'].values\n    Y_val=train_df.loc[val_index,'target'].values\n    \n    lgb_train= lgb.Dataset(X_train,Y_train,feature_name=val_cols)\n    lgb_val= lgb.Dataset(X_val,Y_val,feature_name=val_cols)\n    model=lgb.train(params,\n                   lgb_train,\n                   valid_sets=(lgb_val),\n                   valid_names=('fold '+str(fold+1)+' val set'),\n                   verbose_eval=100)\n\n    val_preds=model.predict(X_val)\n    test_preds=model.predict(test_df[test_cols])\n    train_df.loc[val_index,'lgb_pred']=val_preds\n    test_df['lgb_pred']+=test_preds\/CFG.folds\nprint('################################################################')\nprint('ensemble cv score is:',rmse(train_df['target'],train_df['lgb_pred']))\n","909e0295":"final_features=['ridge_pred','lgb_pred']\nX=train_df[final_features].values\nY=train_df['target'].values.reshape(-1,1)\nw=np.linalg.inv(X.T@X)@X.T@Y\nfor i,feature in enumerate(final_features):\n    print(feature,' weight: ',w[i])\ntrain_df['final_pred']=(train_df[final_features].values@w).reshape(-1)\ntest_df['final_pred']=(test_df[final_features].values@w).reshape(-1)\nprint('final cv: ',rmse(train_df['target'],train_df['final_pred']))","0cd1b0dd":"res_df['target']=test_df['final_pred']\nres_df.to_csv('submission.csv',index=False)\nres_df","51490649":"* F13\n* CV 0.4816 LB 0.484\n* roberta base \n* two linear output layer\n* 3 layer re-init\n* seed 777 to compare with F1\n* lr diff don't apply to re-init layers\n* seems roberta base (not large) with re init do poorly in the LB test fold...","d3365afe":"* F33\n* CV 0.4835 LB 0.464\n* roberta large with ITPT\n* seed 555666\n* re init 5 layers\n* 1 linear layer","7f6a6abf":"* F7\n* CV 0.4781 LB 0.473","945ab3f6":"* F50\n* CV 0.480 LB 0.465\n* funnel large\n* bs 16\n* lr 3e-5","9d897e49":"* F12\n* CV 0.4773 LB 0.473\n* roberta base\n* seed 777 to compare with F1\n* 10% warm up","43acd60f":"* F30\n* CV 0.47575  LB 0.462\n* deberta large\n* seed 5311\n* re init 5 layers\n* bs 4\n* grad accum 2 lr 2","1f4f54c9":"* F42\n* CV 0.4883 LB \n* roberta large roberta-large-mnli\n* seed 5551\n* re init 5 layers\n* bs 8 lr 2e-5\n* 1 linear layer","a4878f36":"# Inference","cc3a8f34":"* F14\n* CV 0.4778 LB 0.468\n* roberta base \n* seed 777 to compare with F1\n* 10% warm up\n* batch size 32 and lr 8e-5","39f8d4ba":"* F34\n* CV LB\n* bart base\n* seed 9999\n* one linear layer\n","a6ee8fda":"* F24\n* CV 0.4815 LB 0.488\n* deberta base \n* seed 5311\n* bs 24","6c05a710":"* F29\n* CV 0.50150967 LB\n* bert base cased\n* seed 2334\n* bs 32 lr 8","346dc465":"* F27\n* CV 0.486 LB 0.464\n* simcse large\n* seed 754\n* re init 5 layers\n* grad accum 2 lr 2.5","7587fee1":"* F39\n* CV LB\n* xlnet-base\n* seed 900\n* 1 linear layer\n","70086420":"* F43\n* CV 0.4970 LB \n* roberta-large-openai-detector\n* seed 5551\n* re init 5 layers\n* bs 8 lr 2e-5\n* 1 linear layer","93486917":"* F25\n* CV 0.4831 LB 0.488\n* deberta base \n* seed 5311\n* bs 24\n* re init 2 layers","cc4d3186":"* F3\n* CV 0.487 LB 0.486\n* SimCSE roberta base","223e25e5":"* F9\n* CV 0.4799 LB 0.475\n* roberta base\n* one linear output layer\n* 2 layer re-init","85a17335":"* F11\n* CV 0.4811 LB 0.466\n* roberta large\n* two linear output layer\n* 5 layer re init","10832c85":"### 2.Lightgbm","a7bb6663":"* some feature engineering","c282a26c":"* model predictions on trainning set","11da116d":"* F38\n* CV 0.4782 LB 0.477\n* roberta base\n* seed 900\n* 1 linear layer\n* len 300\n* bs 24","25c083b0":"* F28\n* CV LB","503a98dc":"* F35\n* CV 0.4986 LB\n* electra base\n* seed 75412\n* 1 linear layer","b5433705":"### Validation","7b63a128":"* F15\n* CV 0.4839 LB\n* roberta large\n* two linear output layer\n* 5 layer re init\n* lr with warm up","cadb0a4a":"* F31\n* CV LB\n* bert large cased\n* seed 2334\n* bs 32 lr 8","e1e066ad":"* F5\n* CV 0.4773 LB 0.474\n* roberta base train with ITPT weights\n* ITPT model from: Maunish's pre-trained model","5ed4ebfa":"# Validation & Ensemble","f93fc181":"* F19\n* CV 0.4845 LB 0.483\n* Simcse base \n* seed 754\n* 2 layer re init","a2151d4d":"* F2\n* CV 0.4839 LB 0.470\n* roberta large\n* mv state dict to cpu\n\n","bf4b1118":"* F18\n* CV 0.4874678 LB 0.488\n* Simcse base \n* seed 754\n* with attention head","01178792":"* F36\n* CV 0.4931 LB 0.480\n* electra large\n* seed 75412\n* 1 linear layer\n* re init 5 layers","4e46b024":"# parameters for this notebook","757455cc":"* F51\n* CV 0.479 LB 0.469\n* electra large\n* bs 16\n* lr 3.5e-5\n* no reinit","71c69a3e":"* F17\n* CV 0.47581798 LB 0.468\n* roberta base train with ITPT weights\n* bs 32 \n* fix oom\n* fix possible token id change\n* add reinit code","7cea70f5":"* F6\n* CV 0.4855 LB 0.471\n* almost no diversity same seed and model and pretrain","15626224":"* F4","176ee839":"* F23\n* CV 0.4792 LB 0.465\n* roberta large\n* gradient accumulation\n* batch size 16 lr 2.5e-5","e0b6e09f":"* F8\n* CV 0.4815  LB 0.472","9bb6d580":"### 1.Ridge Regression","6db7fdc7":"* F10\n* CV 0.479 LB 0.479\n\n* roberta base\n* two linear output layer\n* 2 layer re-init\n* seed 777 to compare with F1","5effe11d":"* F22\n* CV LB\n* roberta large\n* gradient accumulation\n* batch size 16 lr 4e-5","d430c336":"* F32\n* CV 0.4749 LB 0.469\n* deberta large\n* seed 12222\n* no re init\n* bs 4\n* grad accum 2 lr 2\n* 1 linear layer","b0b6af3d":"* F41\n* CV 0.4796 LB 0.475\n* deberta-large-mnli\n* seed 5311\n* re init 5 layers\n* bs 4\n* grad accum 2 lr 2\n* 1 linear layer","8d6448d3":"* F20\n* CV 0.4796 LB 0.464\n* Simcse large\n* seed 754\n* two linear output layer\n* 5 layer re init","611ed584":"# Models","e27c9f19":"* remeber that different models has different mask token id\n* use different seed in dataloader for diversity","91174307":"* F1\n* CV 0.477 LB 0.469\n* roberta base train with ITPT weights\n","388c097f":"* F16\n* CV 0.4824 LB 0.468\n* roberta large\n* two linear output layer\n* 5 layer re init\n* 6 epochs (=large lr in 5epochs)","64d3ca30":"* F40\n* CV 0.4836 LB 0.463\n* Simcse large unsup\n* seed 754\n* re init 5 layers\n* bs 8 lr 2e-5\n* 1 linear layer","abacf961":"* F37\n* CV LB\n* xlm-roberta base\n* seed 900\n* 1 linear layer","6bbd1ad3":"* F26\n* CV 0.4757 LB 0.465\n* deberta large\n* seed 5311\n* re init 5 layers\n* bs 4\n* grad accum 4 lr 2.5"}}