{"cell_type":{"ce333cf2":"code","10f2bc3b":"code","5bdbf33f":"code","7deae9d8":"code","75c320bd":"code","9c1d587e":"code","af98e675":"code","78c47ab4":"code","28bcbb7f":"code","fa4cba46":"code","f6246bb2":"code","7d19ceaa":"code","2c8e92b5":"code","04903818":"code","fa102719":"markdown","a455697a":"markdown","82a8f536":"markdown","a4f18c39":"markdown","483d9ab9":"markdown","c6adacd9":"markdown","6ee99829":"markdown","d8e355a9":"markdown"},"source":{"ce333cf2":"import numpy as np\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping","10f2bc3b":"train = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\n\ntrain.head(3)","5bdbf33f":"def text_preprocessing(text):\n    tokenized = word_tokenize(text)\n    text = [i for i in tokenized if i not in stopwords.words('english')]\n    text = ' '.join(text)\n    \n    return text","7deae9d8":"x = train.excerpt.apply(text_preprocessing)\ny = train.target","75c320bd":"tokenizer = Tokenizer()\n\n# fit only train set\ntokenizer.fit_on_texts(x)\n\n# in case you have validation\/test dataset don't forget to transform val\/test data on already fitted train data\nx = tokenizer.texts_to_sequences(x)","9c1d587e":"# find out the longest sequence length for padding\nlen_seq_list = [len(s) for s in x]\nmax_seq_len = np.max(len_seq_list)\n\n# pad sequence for first embedding layer\nx = tf.keras.preprocessing.sequence.pad_sequences(\n    x,\n    padding='post',\n    truncating='post',\n    maxlen=max_seq_len\n)","af98e675":"# hyperparameters\nvoc_size = len(tokenizer.index_word) + 1\nepochs = 100\nbatch_size = 1024\nembedding_dim = 512\ndropout_rate = .5","78c47ab4":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(voc_size, embedding_dim, input_length=max_seq_len),\n#     gap1d layer shows better accuracy than flatten layer\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dropout(dropout_rate),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dropout(dropout_rate),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(dropout_rate),\n    tf.keras.layers.Dense(1)\n])","28bcbb7f":"model.summary()","fa4cba46":"early_stopping = EarlyStopping(patience=8, restore_best_weights=True, verbose=1)\nlr_reduce = ReduceLROnPlateau(patience=3, verbose=1)\n\ncallbacks = [early_stopping, lr_reduce]","f6246bb2":"model.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss='mean_squared_error'\n)\n\nhistory = model.fit(\n    x,\n    y,\n    epochs=epochs,\n    batch_size=batch_size,\n    validation_split=.2,\n    callbacks=callbacks\n)","7d19ceaa":"sub = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\ntest = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')","2c8e92b5":"test_data = test.excerpt.apply(text_preprocessing)\ntest_data = tokenizer.texts_to_sequences(test_data)\ntest_data = tf.keras.preprocessing.sequence.pad_sequences(\n    test_data,\n    padding='post',\n    truncating='post',\n    maxlen=max_seq_len\n)\n\ntest_pred = model.predict(test_data)","04903818":"sub['target'] = test_pred\nsub.to_csv('submission.csv', index=False)","fa102719":"During all experiments I found out the best way to preprocess the text. Somehow non alphabet characters doens't need to be deleted.","a455697a":"# Training","82a8f536":"# Make a submission file","a4f18c39":"# Build model","483d9ab9":"**Here is my notebook to solve this task using simple dnn (dense neural network)!**","c6adacd9":"# Import libs","6ee99829":"Decent callbacks to control train\/val loss to not let the model to overfit.","d8e355a9":"Thanks for watching!\n\nIf you have any *questions* or *suggestions* feel free to write 'em down below!"}}