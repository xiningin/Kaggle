{"cell_type":{"f71a55bf":"code","36025922":"code","2a25cac2":"code","04573150":"code","3eabd23a":"code","5ba9064e":"code","cee44c7f":"code","3ee784f3":"code","641afc11":"code","8761c8a1":"code","97447a64":"code","a4c11ae7":"code","e684193e":"code","258b6339":"code","2cee75f7":"code","f2676cf2":"code","15dcb0e4":"code","e8eeed53":"code","98fa0667":"code","cadbb9d6":"code","d830e251":"code","05ceb006":"code","1af4f896":"code","9f6ff31c":"code","3f98799a":"code","34c333e4":"code","c9a4699d":"code","48c4dacb":"code","d1a2140e":"code","ed045dfc":"code","8d39f542":"code","2a860e27":"code","98e9b45b":"code","f154e651":"code","2e900efc":"code","1c20a3c6":"code","4077c495":"code","1e9b1ba8":"markdown","e7e798a1":"markdown","39af3c9f":"markdown","2cfa296a":"markdown","e2a2b79d":"markdown","fb406255":"markdown","64c381a5":"markdown","30c5f327":"markdown","fccde27f":"markdown"},"source":{"f71a55bf":"# for basic operations\nimport numpy as np\nimport pandas as pd\n\n# for visualizations\nimport matplotlib.pyplot as plt\nimport squarify\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport networkx as nx\n\n# for market basket analysis\nfrom mlxtend.frequent_patterns import apriori, association_rules, fpgrowth, fpmax, fpcommon \n","36025922":"# reading the dataset\n\ndata = pd.read_csv('..\/input\/market-basket-optimization\/Market_Basket_Optimisation.csv', header = None)\n\n# let's check the shape of the dataset\ndata.shape","2a25cac2":"# checking the head of the data\n\ndata.head()","04573150":"# checkng the tail of the data\n\ndata.tail()","3eabd23a":"# checking the random entries in the data\n\ndata.sample(10)","5ba9064e":"# let's describe the dataset\n\ndata.describe()","cee44c7f":"plt.rcParams['figure.figsize'] = (15, 15)\nwordcloud = WordCloud(background_color = 'white', width = 1200,  height = 1200, max_words = 121).generate(str(data[0]))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('Most Popular Items',fontsize = 20)\nplt.show()","3ee784f3":"# looking at the frequency of most popular items \n\nplt.rcParams['figure.figsize'] = (18, 7)\ncolor = plt.cm.copper(np.linspace(0, 1, 40))\ndata[0].value_counts().head(40).plot.bar(color = color)\nplt.title('frequency of most popular items', fontsize = 20)\nplt.xticks(rotation = 90 )\nplt.grid()\nplt.show()","641afc11":"y = data[0].value_counts().head(50).to_frame()\ny.index","8761c8a1":"# plotting a tree map\n\nplt.rcParams['figure.figsize'] = (20, 20)\ncolor = plt.cm.cool(np.linspace(0, 1, 50))\nsquarify.plot(sizes = y.values, label = y.index, alpha=.8, color = color)\nplt.title('Tree Map for Popular Items')\nplt.axis('off')\nplt.show()","97447a64":"data['food'] = 'Food'\nfood = data.truncate(before = -1, after = 15)\n\nfood = nx.from_pandas_edgelist(food, source = 'food', target = 0, edge_attr = True)","a4c11ae7":"import warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['figure.figsize'] = (20, 20)\npos = nx.spring_layout(food)\ncolor = plt.cm.Wistia(np.linspace(0, 15, 1))\nnx.draw_networkx_nodes(food, pos, node_size = 15000, node_color = color)\nnx.draw_networkx_edges(food, pos, width = 3, alpha = 0.6, edge_color = 'black')\nnx.draw_networkx_labels(food, pos, font_size = 20, font_family = 'sans-serif')\nplt.axis('off')\nplt.grid()\nplt.title('Top 15 First Choices', fontsize = 40)\nplt.show()","e684193e":"data['secondchoice'] = 'Second Choice'\nsecondchoice = data.truncate(before = -1, after = 15)\nsecondchoice = nx.from_pandas_edgelist(secondchoice, source = 'food', target = 1, edge_attr = True)","258b6339":"import warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['figure.figsize'] = (20, 20)\npos = nx.spring_layout(secondchoice)\ncolor = plt.cm.Blues(np.linspace(0, 15, 1))\nnx.draw_networkx_nodes(secondchoice, pos, node_size = 15000, node_color = color)\nnx.draw_networkx_edges(secondchoice, pos, width = 3, alpha = 0.6, edge_color = 'brown')\nnx.draw_networkx_labels(secondchoice, pos, font_size = 20, font_family = 'sans-serif')\nplt.axis('off')\nplt.grid()\nplt.title('Top 15 Second Choices', fontsize = 40)\nplt.show()","2cee75f7":"data['thirdchoice'] = 'Third Choice'\nsecondchoice = data.truncate(before = -1, after = 10)\nsecondchoice = nx.from_pandas_edgelist(secondchoice, source = 'food', target = 2, edge_attr = True)","f2676cf2":"import warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['figure.figsize'] = (20, 20)\npos = nx.spring_layout(secondchoice)\ncolor = plt.cm.Reds(np.linspace(0, 15, 1))\nnx.draw_networkx_nodes(secondchoice, pos, node_size = 15000, node_color = color)\nnx.draw_networkx_edges(secondchoice, pos, width = 3, alpha = 0.6, edge_color = 'pink')\nnx.draw_networkx_labels(secondchoice, pos, font_size = 20, font_family = 'sans-serif')\nplt.axis('off')\nplt.grid()\nplt.title('Top 10 Third Choices', fontsize = 40)\nplt.show()","15dcb0e4":"# making each customers shopping items an identical list\ntrans = []\nfor i in range(0, 7501):\n    trans.append([str(data.values[i,j]) for j in range(0, 20)])\n\n# conveting it into an numpy array\ntrans = np.array(trans)\n\n# checking the shape of the array\nprint(trans.shape)","e8eeed53":"import pandas as pd\nfrom mlxtend.preprocessing import TransactionEncoder\n\nte = TransactionEncoder()\ndata = te.fit_transform(trans)\ndata = pd.DataFrame(data, columns = te.columns_)\n\n# getting the shape of the data\ndata.shape","98fa0667":"import warnings\nwarnings.filterwarnings('ignore')\n\n# getting correlations for 121 items would be messy \n# so let's reduce the items from 121 to 50\n\ndata = data.loc[:, list(y.index)]\n\n# checking the shape\ndata.shape","cadbb9d6":"# getting the head of the data\n\ndata.head()","d830e251":"frequent_itemsets = apriori(data, min_support = 0.01, use_colnames=True)\nfrequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\nfrequent_itemsets","05ceb006":"rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.3).iloc[:,:-3]\nrules[\"antecedents_length\"] = rules[\"antecedents\"].apply(lambda x: len(x))\nrules[\"consequents_length\"] = rules[\"consequents\"].apply(lambda x: len(x))\nrules.sort_values(\"confidence\")","1af4f896":"frequent_itemsets = apriori(data, min_support = 0.03, use_colnames=True)\nfrequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\nfrequent_itemsets","9f6ff31c":"rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.25).iloc[:,:-3]\nrules[\"antecedents_length\"] = rules[\"antecedents\"].apply(lambda x: len(x))\nrules[\"consequents_length\"] = rules[\"consequents\"].apply(lambda x: len(x))\nrules.sort_values(\"confidence\")","3f98799a":"frequent_itemsets = apriori(data, min_support = 0.05, use_colnames=True)\nfrequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\nfrequent_itemsets","34c333e4":"rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.2).iloc[:,:-3]\nrules[\"antecedents_length\"] = rules[\"antecedents\"].apply(lambda x: len(x))\nrules[\"consequents_length\"] = rules[\"consequents\"].apply(lambda x: len(x))\nrules.sort_values(\"confidence\")","c9a4699d":"frequent_itemsets = fpgrowth(data, min_support = 0.01, use_colnames=True)\nfrequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\nfrequent_itemsets","48c4dacb":"rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.3).iloc[:,:-3]\nrules[\"antecedents_length\"] = rules[\"antecedents\"].apply(lambda x: len(x))\nrules[\"consequents_length\"] = rules[\"consequents\"].apply(lambda x: len(x))\nrules.sort_values(\"confidence\")","d1a2140e":"frequent_itemsets = fpgrowth(data, min_support = 0.03, use_colnames=True)\nfrequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\nfrequent_itemsets","ed045dfc":"rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.25).iloc[:,:-3]\nrules[\"antecedents_length\"] = rules[\"antecedents\"].apply(lambda x: len(x))\nrules[\"consequents_length\"] = rules[\"consequents\"].apply(lambda x: len(x))\nrules.sort_values(\"confidence\")","8d39f542":"frequent_itemsets = fpgrowth(data, min_support = 0.05, use_colnames=True)\nfrequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\nfrequent_itemsets","2a860e27":"rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.2).iloc[:,:-3]\nrules[\"antecedents_length\"] = rules[\"antecedents\"].apply(lambda x: len(x))\nrules[\"consequents_length\"] = rules[\"consequents\"].apply(lambda x: len(x))\nrules.sort_values(\"confidence\")","98e9b45b":"from timeit import repeat","f154e651":"testcases = [''' \ndef fn(): \n    return apriori(data, min_support = 0.01, use_colnames=True)\n''',\n''' \ndef fn(): \n    return fpgrowth(data, min_support = 0.01, use_colnames=True)\n''']\n\n\nres_apriori1 = repeat(stmt=testcases[0], repeat=5)\n\nres_fpgrowth1 = repeat(stmt=testcases[1], repeat=5)\n\nresults1 = [res_apriori1,res_fpgrowth1]\n\nplt.figure(figsize=(10,6))\nplt.boxplot(results1, labels=[\"Apriori\", \"FP Growth\"], showmeans=True)\nplt.title(\"Execution time at 1% Min. Support\")\nplt.xlabel(\"Algorithm\")\nplt.ylabel(\"Time (in sec)\")\nplt.show()","2e900efc":"testcases = [''' \ndef fn(): \n    return apriori(data, min_support = 0.03, use_colnames=True)\n''',\n''' \ndef fn(): \n    return fpgrowth(data, min_support = 0.03, use_colnames=True)\n''']\n\nres_apriori3 = repeat(stmt=testcases[0], repeat=5)\n\nres_fpgrowth3 = repeat(stmt=testcases[1], repeat=5)\n\nresults3 = [res_apriori3,res_fpgrowth3]\n\nplt.figure(figsize=(10,6))\nplt.boxplot(results3, labels=[\"Apriori\", \"FP Growth\"], showmeans=True)\nplt.title(\"Execution time at 3% Min. Support\")\nplt.xlabel(\"Algorithm\")\nplt.ylabel(\"Time (in sec)\")\nplt.show()","1c20a3c6":"testcases = [''' \ndef fn(): \n    return apriori(data, min_support = 0.05, use_colnames=True)\n''',\n''' \ndef fn(): \n    return fpgrowth(data, min_support = 0.05, use_colnames=True)\n''']\n\nres_apriori5 = repeat(stmt=testcases[0], repeat=5)\n\nres_fpgrowth5 = repeat(stmt=testcases[1], repeat=5)\n\n\nresults5 = [res_apriori5,res_fpgrowth5]\n\nplt.figure(figsize=(10,6))\nplt.boxplot(results5, labels=[\"Apriori\", \"FP Growth\"], showmeans=True)\nplt.title(\"Execution time at 5% Min. Support\")\nplt.xlabel(\"Algorithm\")\nplt.ylabel(\"Time (in sec)\")\nplt.show()","4077c495":"plt.figure(figsize=(10,6))\nsns.lineplot([\"1%\",\"3%\",\"5%\"],[np.mean(results1[0]),np.mean(results3[0]),np.mean(results5[0])], label=\"Apriori\")\nsns.lineplot([\"1%\",\"3%\",\"5%\"],[np.mean(results1[1]),np.mean(results3[1]),np.mean(results5[1])], label=\"FP Growth\")\nplt.title(\"Execution time comparison of Apriori and FP Growth\")\nplt.xlabel(\"Min. Support\")\nplt.ylabel(\"Time (in sec)\")\nplt.show()","1e9b1ba8":"## Using Apriori Algorithm","e7e798a1":"## Data Preprocessing","39af3c9f":"## Data Visualizations","2cfa296a":"## Time Comparison","e2a2b79d":"**Importing the required libraries.**","fb406255":"**Importing the dataset**","64c381a5":"**Statistical description of the dataset.**","30c5f327":"## Using FP Growth","fccde27f":"## Using Apriori Algorithm and FP Growth"}}