{"cell_type":{"bea4bc16":"code","b15bcb5f":"code","e0af664a":"code","bce63b64":"code","7f6b7a93":"code","d422f60c":"code","0adf5315":"code","e8f26822":"code","4399223e":"code","9dfeee74":"code","5f18721d":"code","c92ab4fb":"code","49dde176":"code","f4b5121d":"code","2329e332":"code","b1a34aad":"code","9d7a1e9b":"code","fe781e74":"code","830a4d0f":"code","cf09e0e3":"code","6adcf46f":"code","af80a3f5":"code","57b9f949":"code","d7175e02":"code","a13810a7":"code","9ec22352":"code","524b2e24":"code","c9a623ea":"code","c48d07ba":"code","433121c3":"markdown","4307169a":"markdown","b0aafd70":"markdown","68875e2a":"markdown","4a226e86":"markdown","37e16fe3":"markdown","869cbf4b":"markdown","aafa82de":"markdown","87b2cca7":"markdown","54beadd5":"markdown","a8d71924":"markdown","03144a9c":"markdown","5205cb07":"markdown","1758df27":"markdown","dfd5d5e2":"markdown","dc9455be":"markdown","d7eb8567":"markdown","cac05d90":"markdown","cc26eced":"markdown","46068b61":"markdown","f4d3c5e8":"markdown","db7844d8":"markdown","27989c57":"markdown"},"source":{"bea4bc16":"import time\nstart_time = time.time()\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\nfrom sklearn.utils import shuffle\nfrom sklearn.svm import SVC\nfrom sklearn import model_selection\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport re\n\n%matplotlib inline\nimport matplotlib.pyplot as plt","b15bcb5f":"!pip install tensorflow-text==2.0.0 --user","e0af664a":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as textb","bce63b64":"#print full tweet , not a part\npd.set_option('display.max_colwidth', -1)\npd.set_option('display.max_rows', 100)","7f6b7a93":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nlength_train = len(train.index)\nlength_train","d422f60c":"# the code in the cell is taken from \n# https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert\ndf_mislabeled = train.groupby(['text']).nunique().sort_values(by='target', ascending=False)\ndf_mislabeled = df_mislabeled[df_mislabeled['target'] > 1]['target']\nindex_misl = df_mislabeled.index.tolist()\n\nlenght = len(index_misl)\n\nprint(f\"There are {lenght} equivalence classes with mislabelling\")","0adf5315":"index_misl","e8f26822":"train_nu_target = train[train['text'].isin(index_misl)].sort_values(by = 'text')\ntrain_nu_target.head(60)","4399223e":"num_records = train_nu_target.shape[0]\nlength = len(index_misl)\nprint(f\"There are {num_records} records in train set which generate {lenght} equivalence classes with mislabelling (raw text, no cleaning)\") ","9dfeee74":"copy = train_nu_target.copy()\nclasses = copy.groupby('text').agg({'keyword':np.size, 'target':np.mean}).rename(columns={'keyword':'Number of records in train set', 'target':'Target mean'})\n\nclasses.sort_values('Number of records in train set', ascending=False).head(20)","5f18721d":"majority_df = train_nu_target.groupby(['text'])['target'].mean()\n#majority_df.index","c92ab4fb":"def relabel(r, majority_index):\n    ind = ''\n    if r['text'] in majority_index:\n        ind = r['text']\n#        print(ind)\n        if majority_df[ind] <= 0.5:\n            return 0\n        else:\n            return 1\n    else: \n        return r['target'] ","49dde176":"train['target'] = train.apply( lambda row: relabel(row, majority_df.index), axis = 1)","f4b5121d":"new_df = train[train['text'].isin(majority_df.index)].sort_values(['target', 'text'], ascending = [False, True])\nnew_df.head(15)","2329e332":"# the code in the cell is taken from \n# https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert\ndf_mislabeled = train.groupby(['text']).nunique().sort_values(by='target', ascending=False)\ndf_mislabeled = df_mislabeled[df_mislabeled['target'] > 1]['target']\nindex_misl = df_mislabeled.index.tolist()\n#index_dupl[0:50]\nlen(index_misl)","b1a34aad":"use = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-multilingual-large\/3\")","9d7a1e9b":"X_train = []\nfor r in tqdm(train.text.values):\n  emb = use(r)\n  review_emb = tf.reshape(emb, [-1]).numpy()\n  X_train.append(review_emb)\n\nX_train = np.array(X_train)\ny_train = train.target.values\n\nX_test = []\nfor r in tqdm(test.text.values):\n  emb = use(r)\n  review_emb = tf.reshape(emb, [-1]).numpy()\n  X_test.append(review_emb)\n\nX_test = np.array(X_test)","fe781e74":"train_arrays, test_arrays, train_labels, test_labels = train_test_split(X_train,\n                                                                        y_train,\n                                                                        random_state =42,\n                                                                        test_size=0.20)","830a4d0f":"def svc_param_selection(X, y, nfolds):\n    Cs = [1.07]\n    gammas = [2.075]\n    param_grid = {'C': Cs, 'gamma' : gammas}\n    grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=nfolds, n_jobs=8)\n    grid_search.fit(X, y)\n    grid_search.best_params_\n    return grid_search\n\nmodel = svc_param_selection(train_arrays,train_labels, 5)","cf09e0e3":"model.best_params_","6adcf46f":"pred = model.predict(test_arrays)","af80a3f5":"cm = confusion_matrix(test_labels,pred)\ncm","57b9f949":"accuracy = accuracy_score(test_labels,pred)\naccuracy","d7175e02":"test_pred = model.predict(X_test)\nsubmission['target'] = test_pred.round().astype(int)\n#submission.to_csv('submission.csv', index=False)","a13810a7":"train_df_copy = train\ntrain_df_copy = train_df_copy.fillna('None')\nag = train_df_copy.groupby('keyword').agg({'text':np.size, 'target':np.mean}).rename(columns={'text':'Count', 'target':'Disaster Probability'})\n\nag.sort_values('Disaster Probability', ascending=False).head(20)","9ec22352":"count = 2\nprob_disaster = 0.9\nkeyword_list_disaster = list(ag[(ag['Count']>count) & (ag['Disaster Probability']>=prob_disaster)].index)\n#we print the list of keywords which will be used for prediction correction \nkeyword_list_disaster","524b2e24":"ids_disaster = test['id'][test.keyword.isin(keyword_list_disaster)].values\nsubmission['target'][submission['id'].isin(ids_disaster)] = 1","c9a623ea":"submission.to_csv(\"submission.csv\", index=False)\nsubmission.head(10)","c48d07ba":"print(\"--- %s seconds ---\" % (time.time() - start_time))","433121c3":"Below the encoding is applied to every sentence in train.text and test.text columns and the resulting vectors are saved to lists.<br>","4307169a":"### Support Vector Machine prediction","b0aafd70":"### Model 1\n\n#### Transformer (Multilingual Universal Sentence Encoder)  + Support Vector Machine\n\nIt gives the public score 0.83742   (Version 1 in this notebook)<br>\nNot a bad result taking into account we just supply the raw text to the Transformer,  which transform every string to 512 dimentional vector, supply the vectors to Support Vector Machine and receive the result. \nThe idea to use Multilingual Encoder is from here: https:\/\/www.kaggle.com\/gibrano\/disaster-universal-sentences-encoder-svm","68875e2a":"### Load the Multilingual Encoder module ","4a226e86":"### Model 3\n\n#### Transformer + Support Vector Machine + Filtering basing on keywords + Majority voting  for semantically equivalent but mislabelled tweets\n\nIt gives the score 0.84049 (this version)<br> \nThe idea is from here: https:\/\/www.kaggle.com\/atpspin\/same-tweet-two-target-labels","37e16fe3":"### Majority voting\n\nIf Target mean is lower or equal 0.5 , I relabel it to 0, otherwise to 1.","869cbf4b":"Please, upvote if you like.","aafa82de":"### Equivalence classes with mislabelling. ","87b2cca7":"### Data loading","54beadd5":"#### Accuracy and confusion matrix","a8d71924":"The 18 'mislabelled tweets' (each of them respresent a class with min 2 elements).","03144a9c":"### Model 2\n#### Transformer + Support Vector Machine + Filtering basing on keywords \n\nIt gives the public score 0.83946 (Version 3 of the notebook)<br>\nThe idea for the filtering is from here: https:\/\/www.kaggle.com\/bandits\/using-keywords-for-prediction-improvement","5205cb07":"### Training and Evaluating","1758df27":"Here I follow https:\/\/www.kaggle.com\/bandits\/using-keywords-for-prediction-improvement The idea is that some keywords with very high probability (sometimes = 1) signal about disaster (or usual) tweets. It is possible to add the extra 'keyword' feature to the model, but the simple approach also works. I make correction for the disaster tweets prediction to the model basing on the \"disaster\" keywords.","dfd5d5e2":"The 'target' for mislabelled tweets is recalculated. \nThe number of mislabelled tweets is 0 after recalculation. ","dc9455be":"Many notebooks in the competition show the Support Vector Machine works quite well for the classificaion. In https:\/\/www.kaggle.com\/gibrano\/disaster-universal-sentences-encoder-svm the Multilingual Universal Sentence Encoder is used for sentence encoding. Here I follow the work and use  the Multilingual Universal Sentence Encoder (from tensorflow_hub).<br>\n<br>\nThe approach from https:\/\/www.kaggle.com\/bandits\/using-keywords-for-prediction-improvement is applied for final filtering of the results basing on the 'keywords'.<br>\n<br>\nIn the training data there are many 'semantically equivalent' tweets. For example some tweets differ only in the URLs at the tail of string. It is reasonable to expect the URL tails are not very important for prediction of target and the tweets are semantically equal. To find such 'only URL different' tweets some cleaning of the 'text' strings is to be done. After the cleaning the tweets become equal as strings. Such semantically equivalent records in train set generate equivalence classes. What is important, there are classes, where tweets have 'mislabelling'. We can find 1 and 0 labels in the same class. But all tweets in a class are considered as semantically equal and as such must be all 0 xor all 1 labelled.<br>\nIn raw 'text' (without a cleaning) in the train set we can find 55 records with 'mislabelling' (the records generate 18 equivalence classes).<br>\n<br>\nIn https:\/\/www.kaggle.com\/atpspin\/same-tweet-two-target-labels (there is text cleaning in that model) 79 equivalence classes were detected with mislabelled tweets (here we have only 18). I do here (in model 3) same as in: https:\/\/www.kaggle.com\/atpspin\/same-tweet-two-target-labels. The mean for the 'target' is calculated for each class with mislabelling and the 'target' for the corresponfing records in train set is recalculated depending on the mean value (the majority voting).<br>\n<br>\n","d7eb8567":"### Pipeline of ideas in the baseline model(s)","cac05d90":"Let us check how the records with 'mislabelled' tweets looks like in train set. I print the long list here intentionally. Please, check the behaviour of 'location' variable within the classes. ","cc26eced":"The Transformer and  The Support Vector Machine are very complex and very powerfool tools. Here the tools are used as 'black boxes', and the code to combine the tools in the pipeline is simple.<br>\nIdeas from other notebooks in the competition help to reach good results.<br> \n<br>\nNO TEXT CLEANING is used in the model(s), the row text is supplied to the Transformer.<br>\n<br>The model(s) is a 'baseline' for more complex versions.<br>\n<br>\nThe execution time for the model(s) is about 700...800 seconds without a GPU.","46068b61":"### Using keywords for better prediction","f4d3c5e8":"### Some words about Universal Sentence Encoders and the Transformer","db7844d8":"Let us calculate some statistic for each class. Below in table the target mean + number of records in train set for each class are calculated. As we can see there are from 2 to 6 elements in equivalence class.  ","27989c57":"A Universal Sentence Encoders encode sentencies to fixed length vectors (The size is 512 in the case of the Multilingual Encoder). The encoders are pre trained on several different tasks: (research article) https:\/\/arxiv.org\/pdf\/1803.11175.pdf. And a use case: https:\/\/towardsdatascience.com\/use-cases-of-googles-universal-sentence-encoder-in-production-dd5aaab4fc15<br>\nTwo architectures are in use in the encoders: Transformer and Deep Averaging Networks. Transformer use \"self attention mechanism\" that learns contextual relations between words and (depending on model) even subwords in a sentence. Not only a word , but it position in a sentence is also taking into account (like positions of other words). There are different ways to implement the intuitive notion of \"contextual relation between words in a sentence\" ( so, different ways to construct \"representation space\" for the contextual words relation). If the several \"ways\" are implemented in a model in the same time: the term \"multi head attention mechanism\" is used.<br>\nTransformers have 2 steps. Encoding: read the text and transform it in vector of fixed length, and decoding: decode the vector (produce prediction for the task). For example: take sentence in English, encode, and translate (decode) in sentence in German.<br>\nFor our model we need only encoding mechanism: sentencies are encoded in vectors and supplied for classification to Support Vector Machine.<br>\nGood and intuitive explanation of the Transformer: http:\/\/jalammar.github.io\/illustrated-transformer\/ ; The original and quite famous now paper \"Attention is all you need\": (research article) https:\/\/arxiv.org\/pdf\/1706.03762.pdf. More about multi head attention: (research article) https:\/\/arxiv.org\/pdf\/1810.10183.pdf. How Transformer is used in BERT:<br> https:\/\/towardsdatascience.com\/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270.\n\nThe Multilingual Universal Sentence Encoder:(research articles) https:\/\/arxiv.org\/pdf\/1810.12836.pdf; https:\/\/arxiv.org\/pdf\/1810.12836.pdf; Example code: https:\/\/tfhub.dev\/google\/universal-sentence-encoder-multilingual-large\/3 The Multilingual Encoder uses very interesting Sentence Piece tokenization to make a pretrained vocabulary: (research articles) https:\/\/www.aclweb.org\/anthology\/D18-2012.pdf; https:\/\/www.aclweb.org\/anthology\/P18-1007.pdf.<br>\n\nAbout the text preprocessing and importance of its coherence with the text preprocessing that is conducted for pretraining + about the different models of text tokeniation:<br>\nvery good article: https:\/\/mlexplained.com\/2019\/11\/06\/a-deep-dive-into-the-wonderful-world-of-preprocessing-in-nlp\/.<br>\n\nFor deep understanding of the Transormer:  http:\/\/nlp.seas.harvard.edu\/2018\/04\/03\/attention.html ; <br>\nhttps:\/\/towardsdatascience.com\/how-to-code-the-transformer-in-pytorch-24db27c8f9ec ; <br> ;\nhttps:\/\/towardsdatascience.com\/how-to-use-torchtext-for-neural-machine-translation-plus-hack-to-make-it-5x-faster-77f3884d95 ; https:\/\/github.com\/SamLynnEvans\/Transformer\/blob\/master\/Models.py"}}