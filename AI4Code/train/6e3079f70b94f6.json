{"cell_type":{"eaad0671":"code","8ed3c10c":"code","7e58dd62":"code","19e7bdce":"code","2a08b29c":"code","609fe43f":"code","756817f7":"code","9f870f94":"code","57886c0a":"code","be549dd7":"code","dfc88ed4":"code","13621314":"code","6f204c5d":"code","43630608":"code","ff0a9dc2":"code","42054fe3":"markdown","b6d93a37":"markdown","ccba1a46":"markdown","b1785bb4":"markdown","b6684b6c":"markdown","5f829017":"markdown","f3a13cf6":"markdown","53af52d8":"markdown","61e8921c":"markdown","9aecb500":"markdown","633ea993":"markdown","8a01e42a":"markdown","cbcb6d67":"markdown","86d0f4a5":"markdown","9ad2f962":"markdown","c24ab6ca":"markdown","e9699813":"markdown","86d02b36":"markdown"},"source":{"eaad0671":"# Importing packages\nimport pandas as pd\nimport numpy as np\n\nfrom scipy import stats as sps\nfrom scipy.interpolate import interp1d\n\nfrom pathlib import Path \n\nfrom matplotlib.patches import Patch\nfrom matplotlib import pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.dates import date2num, num2date\nfrom matplotlib import dates as mdates\nfrom matplotlib import ticker\n\nfrom datetime import date, timedelta\n\nfrom IPython.display import clear_output\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","8ed3c10c":"# Importing data\nsdate = date(2020, 2, 24) + timedelta(days=1)  # start date\nedate = date.today() - timedelta(days=1)  # last date\n\ndelta = edate - sdate + timedelta(days=1)  # number of reports available until today\n\nurl = \"https:\/\/raw.githubusercontent.com\/pcm-dpc\/COVID-19\/master\/dati-regioni\/dpc-covid19-ita-regioni-\" \\\n      + str(edate.year) + '{:02d}'.format(edate.month) + '{:02d}'.format(edate.day) + \".csv\"\n\ndateparser = lambda x: pd.to_datetime(x).date  # .date cut off time\nregions_new = pd.read_csv(url,\n                     usecols=['data', 'denominazione_regione', 'nuovi_positivi'],\n                     parse_dates=['data'],\n                     date_parser = dateparser,\n                     index_col=['denominazione_regione', 'data'],\n                     squeeze=True).sort_index()\n\nfor i in range(delta.days):\n    day = edate - timedelta(days=i + 1)\n    url = \"https:\/\/raw.githubusercontent.com\/pcm-dpc\/COVID-19\/master\/dati-regioni\/dpc-covid19-ita-regioni-\" \\\n          + str(day.year) + '{:02d}'.format(day.month) + '{:02d}'.format(day.day) + \".csv\"\n    regions = pd.read_csv(url,\n                     usecols=['data', 'denominazione_regione', 'nuovi_positivi'],\n                     parse_dates=['data'],\n                     date_parser = dateparser,\n                     index_col=['denominazione_regione', 'data'],\n                     squeeze=True).sort_index()\n    regions_new = pd.concat([regions, regions_new], axis=0, ignore_index=False)\nregions = regions_new.copy()\n\nregions.head()","7e58dd62":"# Gaussian smooth function\ndef prepare_cases(cases, cutoff = 5): \n  # cutoff: minimum number of new cases to consider each day\n    smoothed = cases.rolling(7, # mobile window of a week\n        win_type = 'gaussian',\n        min_periods = 1, # windows step\n        center=True).mean(std=2).round()\n    idx_start = np.searchsorted(smoothed, cutoff) # indexes of days with more than cutoff new cases\n    smoothed = smoothed.iloc[idx_start:] # smoothed values\n    original = cases.loc[smoothed.index] # original values \n    return original, smoothed","19e7bdce":"region_name = 'Lombardia' # as example\n\ncases = regions.xs(region_name).rename(f\"Casi in {region_name}\")\n\noriginal, smoothed = prepare_cases(cases, cutoff = 25)\n\nfig, ax = plt.subplots()\n\noriginal.plot(title = f\"Nuovi casi per giorno del {region_name}\",\n               c = 'k',\n               linestyle = ':',\n               alpha = .5,\n               label = 'Reale',\n               legend = True,\n               figsize = (10, 7))\n\nax = smoothed.plot(label='Smoothed',\n                   legend=True)\n\nax.get_figure().set_facecolor('w')","2a08b29c":"GAMMA = 1\/7\n# We create an array for every possible value of R_t\nR_T_MAX = 12\nr_t_range = np.linspace(0, R_T_MAX, R_T_MAX*100+1)\n\ndef get_posteriors(sr, sigma=0.15): \n    \n    # (1) Calculate Lambda\n    lam = sr[:-1].values*np.exp(GAMMA*(r_t_range[:, None]-1)) \n    \n    # (2) Calculate each day's likelihood\n    likelihoods = pd.DataFrame(\n        data = sps.poisson.pmf(sr[1:].values, lam),\n        index = r_t_range,\n        columns = sr.index[1:])\n    \n    # (3) Create the Gaussian Matrix \n    process_matrix = sps.norm(loc = r_t_range,\n                              scale = sigma\n                             ).pdf(r_t_range[:, None]) \n\n    # (3a) Normalize all rows to sum to 1\n    process_matrix \/= process_matrix.sum(axis=0)\n    \n    # (4) Calculate the initial prior\n    prior0 = sps.gamma(a = 4).pdf(r_t_range)\n    prior0 \/= prior0.sum()\n\n    # Create a DataFrame that will hold our posteriors for each day\n    # Insert our prior as the first posterior.\n    posteriors = pd.DataFrame(\n        index = r_t_range,\n        columns = sr.index,\n        data = {sr.index[0]: prior0}\n    )\n    \n    # We keep track of the sum of the log of the probability\n    # of the data for maximum likelihood calculation.\n    log_likelihood = 0.0\n\n    # (5) Iteratively apply Bayes' rule\n    for previous_day, current_day in zip(sr.index[:-1], sr.index[1:]):\n\n        #(5a) Calculate the new prior\n        current_prior = process_matrix @ posteriors[previous_day]\n        \n        #(5b) Calculate the numerator of Bayes' Rule: P(k|R_t)*P(R_t)\n        numerator = likelihoods[current_day]*current_prior\n        \n        #(5c) Calculuate the denominator of Bayes' Rule P(k)\n        denominator = np.sum(numerator)\n        \n        # Execute full Bayes' Rule\n        posteriors[current_day] = numerator\/denominator\n        \n        # Add to the running sum of log likelihoods\n        log_likelihood += np.log(denominator)\n    \n    return posteriors, log_likelihood\n\n# Note that we're fixing sigma to a value just for the example\nposteriors, log_likelihood = get_posteriors(smoothed, sigma=.25)","609fe43f":"ax = posteriors.plot(title=f'{region_name} - Posteriori giornaliere per $R_t$',\n           legend = False, \n           lw = 1,\n           alpha = 1,\n           colormap = 'Blues',\n           xlim = (0,4))\n\nax.set_xlabel('$R_t$');","756817f7":"def highest_density_interval(pmf, p=.9):\n    if(isinstance(pmf, pd.DataFrame)):\n        return pd.DataFrame([highest_density_interval(pmf[col], p=p) for col in pmf],\n                            index=pmf.columns)\n    \n    cumsum = np.cumsum(pmf.values)\n    \n    # N x N matrix of total probability mass for each low, high\n    total_p = cumsum - cumsum[:, None]\n\n    # Return all indices with total_p > p\n    lows, highs = (total_p > p).nonzero()\n\n    # Find the smallest range (highest density)\n    best = (highs-lows).argmin()\n    \n    low = pmf.index[lows[best]]\n    high = pmf.index[highs[best]]\n    \n    return pd.Series([low, high],\n                     index=[f'Low_{p*100:.0f}',\n                            f'High_{p*100:.0f}'])","9f870f94":"# This takes a while to execute - it's not the most efficient algorithm\nhdis = highest_density_interval(posteriors, p=.9)\nmost_likely = posteriors.idxmax().rename('ML')\n\n# Look into why you shift -1\nresult = pd.concat([most_likely, hdis], axis=1)\n\nresult.tail()","57886c0a":"def plot_rt(result, ax, region_name):\n    \n    ax.set_title(f\"{region_name}\")\n    \n    # Colors\n    ABOVE = [1,0,0]\n    MIDDLE = [1,1,1]\n    BELOW = [0,0,0]\n    cmap = ListedColormap(np.r_[\n        np.linspace(BELOW,MIDDLE,25),\n        np.linspace(MIDDLE,ABOVE,25)\n    ])\n    color_mapped = lambda y: np.clip(y, .5, 1.5)-.5\n    \n    index = result['ML'].index.get_level_values('data')\n\n    values = result['ML'].values\n    \n    # Plot dots and line\n    ax.plot(index, values, c='k', zorder=1, alpha=.25)\n    ax.scatter(index,\n               values,\n               s=40,\n               lw=.5,\n               c=cmap(color_mapped(values)),\n               edgecolors='k', zorder=2)\n    \n    # Aesthetically, extrapolate credible interval by 1 day either side\n    lowfn = interp1d(date2num(index),\n                     result['Low_90'].values,\n                     bounds_error=False,\n                     fill_value='extrapolate')\n    \n    highfn = interp1d(date2num(index),\n                      result['High_90'].values,\n                      bounds_error=False,\n                      fill_value='extrapolate')\n    \n    extended = pd.date_range(start=pd.Timestamp('2020-02-24'),\n                             end=index[-1]+pd.Timedelta(days=1))\n\n    ax.fill_between(extended,\n                    lowfn(date2num(extended)),\n                    highfn(date2num(extended)),\n                    color='k',\n                    alpha=.1,\n                    lw=0,\n                    zorder=3)\n\n    ax.axhline(1.0, c='k', lw=1, label='$R_t=1.0$', alpha=.25);\n    \n    # Formatting\n    ax.xaxis.set_major_locator(mdates.MonthLocator())\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n    ax.xaxis.set_minor_locator(mdates.DayLocator())\n    \n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_formatter(ticker.StrMethodFormatter(\"{x:.1f}\"))\n    ax.yaxis.tick_right()\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.margins(0)\n    ax.grid(which='major', axis='y', c='k', alpha=.1, zorder=-2)\n    ax.margins(0)\n    ax.set_ylim(-1.0, 5.0)\n    ax.set_xlim(pd.Timestamp('2020-02-20'), pd.Timestamp(date.today()-timedelta(days=1)))\n    fig.set_facecolor('w')","be549dd7":"fig, ax = plt.subplots(figsize=(10,7))\n\nplot_rt(result, ax, region_name)\nax.set_title(f'$R_t$ giornaliero in {region_name}')\nax.xaxis.set_major_locator(mdates.WeekdayLocator())\nax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))","dfc88ed4":"sigmas = np.linspace(1\/20, 1, 20)\n\nresults = {}\n\nfor region_name, cases in regions.groupby(level='denominazione_regione'):\n    \n    print(region_name)\n    new, smoothed = prepare_cases(cases)\n    \n    result = {}\n    \n    # Holds all posteriors with every given value of sigma\n    result['posteriors'] = []\n    \n    # Holds the log likelihood across all k for each value of sigma\n    result['log_likelihoods'] = []\n    \n    for sigma in sigmas:\n        posteriors, log_likelihood = get_posteriors(smoothed, sigma=sigma)\n        result['posteriors'].append(posteriors)\n        result['log_likelihoods'].append(log_likelihood)\n    \n    # Store all results keyed off of regions name\n    results[region_name] = result\n    clear_output(wait=True)\n\nprint('Fatto.')","13621314":"# Each index of this array holds the total of the log likelihoods for\n# the corresponding index of the sigmas array.\ntotal_log_likelihoods = np.zeros_like(sigmas)\n\n# Loop through each regions's results and add the log likelihoods to the running total.\nfor region_name, result in results.items():\n    total_log_likelihoods += result['log_likelihoods']\n\n# Select the index with the largest log likelihood total\nmax_likelihood_index = total_log_likelihoods.argmax()\n\n# Select the value that has the highest log likelihood\nsigma = sigmas[max_likelihood_index]\n\n# Plot it\nfig, ax = plt.subplots()\nax.set_title(f\"Valore massimo della log-verosomiglianza per $\\sigma$ = {sigma:.2f}\");\nax.plot(sigmas, total_log_likelihoods)\nax.axvline(sigma, color='k', linestyle=\":\")","6f204c5d":"final_results = None\n\nfor region_name, result in results.items():\n    print(region_name)\n    posteriors = result['posteriors'][max_likelihood_index]\n    hdis_90 = highest_density_interval(posteriors, p=.9)\n    hdis_50 = highest_density_interval(posteriors, p=.5)\n    most_likely = posteriors.idxmax().rename('ML')\n    result = pd.concat([most_likely, hdis_90, hdis_50], axis=1)\n    if final_results is None:\n        final_results = result\n    else:\n        final_results = pd.concat([final_results, result])\n    clear_output(wait=True)\n\nprint('Fatto.')","43630608":"ncols = 3\nnrows = int(np.ceil(len(results)\/ncols))\n\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, nrows*3))\nprint(axes.flat[1])\nfor i, (region_name, result) in enumerate(final_results.groupby('denominazione_regione')):\n    plot_rt(result, axes.flat[i], region_name)\n\nfig.tight_layout()\nfig.set_facecolor('w')","ff0a9dc2":"FULL_COLOR = [.7,.7,.7]\nNONE_COLOR = [179\/255,35\/255,14\/255]\nPARTIAL_COLOR = [.5,.5,.5]\nERROR_BAR_COLOR = [.3,.3,.3]\n\nmr = final_results.groupby(level=0)[['ML', 'High_90', 'Low_90']].last()\n\ndef plot_standings(mr, figsize=None, title='Pi\u00f9 recenti $R_t$ per regione'):\n    if not figsize:\n        figsize = ((15.9\/50)*len(mr)+.1,2.5)\n        \n    fig, ax = plt.subplots(figsize=figsize)\n\n    ax.set_title(title)\n    err = mr[['Low_90', 'High_90']].sub(mr['ML'], axis=0).abs()\n    bars = ax.bar(mr.index,\n                  mr['ML'],\n                  width=.825,\n                  color=FULL_COLOR,\n                  ecolor=ERROR_BAR_COLOR,\n                  capsize=2,\n                  error_kw={'alpha':.5, 'lw':1},\n                  yerr=err.values.T)\n\n    labels = mr.index.to_series()\n    ax.set_xticklabels(labels, rotation=90, fontsize=11)\n    ax.margins(0)\n    ax.set_ylim(0,2.)\n    ax.axhline(1.0, linestyle=':', color='k', lw=1)\n\n    leg = ax.legend(handles=[\n                        Patch(label='Full', color=FULL_COLOR),\n                    ],\n                    title='Lockdown',\n                    ncol=3,\n                    loc='upper left',\n                    columnspacing=.75,\n                    handletextpad=.5,\n                    handlelength=1)\n\n    leg._legend_box.align = \"left\"\n    fig.set_facecolor('w')\n    return fig, ax\n\n\nmr.sort_values('ML', inplace=True)\nplot_standings(mr)","42054fe3":"## 2. Applicazione del modello alle regioni italiane\nI dati utilizzati sono quelli forniti dal Ministero della Salute ed elaborati dal Dipartimento della Protezione Civile, che quotidianamente fornisce il [Monitoraggio della situazione](http:\/\/opendatadpc.maps.arcgis.com\/apps\/opsdashboard\/index.html#\/b0c68bce2cce478eaac82fe38d4138b1).","b6d93a37":"### 2.2 Esecuzione dell'algoritmo\nPer calcolare i valori *a posteriori* di $R_t$ seguiamo questi passaggi:\n\n1. Calcolo $\\lambda$: il rate di arrivo previsto per il processo di Poisson per ogni giorno\n2. Calcolo la funzione di *verosomiglianza* giornaliera su tutti i possibili valori di $R_t$\n3. Calcolo la matrice del processo in base al valore di $\\sigma$ \n4. Calcolo il valore *a priori* iniziale: il  primo giorno non ha un giorno precedente da cui prendere la distribuzione *a posteriori*\n  * In base alle [informazioni del cdc](https:\/\/wwwnc.cdc.gov\/eid\/article\/26\/7\/20-0282_article) sceglieremo $\\gamma=\\frac{1}{7}$\n5. Itero dal primo giorno all'ultimo disponibile, procedendo come segue:\n  * Calcolo il valore *a priori* di $R_t$ per il giorno $t$ in considerazione, applicando la Gaussiana al valore *a posteriori* del giorno precedente $t-1$\n  * Applico la regola di Bayes moltiplicando il valore *a priori* ottenuto e il valore della funzione di verosomiglianza calcolato nel punto 2\n  * Divido per la probabilit\u00e0 dei dati per normalizzare\n","ccba1a46":"### 3.1 Andamento per ogni regione","b1785bb4":"Poich\u00e9 i nostri risultati includono incertezza, teniamo conto sia del valore stimato (il pi\u00f9 probabile) di $R_t$ che del suo intervallo di confidenza al 90%.","b6684b6c":"Mostriamo un esempio di quanto ottenuto sui dati della Lombardia.","5f829017":"Vediamo l'esempio sempre per la regione Lombardia.","f3a13cf6":"# **Stima giornaliera del valore $R_t$ del COVID-19 nelle regioni italiane**\n*Lucia Innocenti, Gianluigi Lopardo - 20 Aprile 2020*","53af52d8":"Il grafico mostra l'andamento delle distribuzioni a posteriori tracciate simultaneamente. Il colore delle curve diventa pi\u00f9 scuro nel tempo. I valori *a posteriori* iniziano senza molta \"fiducia\" (curve ampie) e diventano progressivamente pi\u00f9 \"sicuri\" (curve pi\u00f9 ristrette) sul valore reale di $R_t$.","61e8921c":"Vediamo ad esempio i valori per gli ultimi 4 giorni giorni per la Lombardia.","9aecb500":"### 2.4 Scelta del $\\sigma$ ottimale\nFinora abbiamo assunto solo un valore arbitrario di $\\sigma$. A questo punto per\u00f2 possiamo per\u00f2 stimare il valore ottimale di $\\sigma$ per ogni regione, cio\u00e8 quello che massimizza la probabilit\u00e0 $P(k)$. \n\nPer farlo, sommiamo tutte le *log-verosomiglianza* per regione per ogni valore di sigma, quindi scegliamo il massimo.","633ea993":"Ora che abbiamo le *log-verosomiglianze* di tutte le regioni, possiamo sommarle per ogni valore di sigma e quindi scegliere il massimo.","8a01e42a":"Utilizziamo quindi il **Teorema di Bayes**: $$ P(R_t|k)=\\frac{P(k|R_t)\\cdot P(R_t)}{P(k)} $$\nQuesto dice che, avendo osservato $k$ nuovi casi, la distribuzione di $R_t$ sia uguale a:\n*  la probabilit\u00e0 di osservare $k$ nuovi casi dato $R_t$, moltiplicata per\n*  la stima *a priori* $P(R_t)$  del valore $R_T$ di senza i dati \n*  diviso per la probabilit\u00e0 di vedere $k$ nuovi casi in generale.\n\nQuesto \u00e8 per un solo giorno. Iterando: ogni giorno che passa, usiamo la distribuzione *a posteriori* di ieri $P(R_{t-1}|k)$ per stimare il valori *a priori* $P(R_t)$ di oggi. Assumiamo che la distribuzione di $R_t$ sia una Gaussiana centrata in $R_{t-1}$, quindi $P(R_t|R_{t-1})=\\mathcal{N}(R_{t-1},\\sigma) $, dove la deviazione standard $\\sigma$ \u00e8 un iperparametro, ovvero un valore che verr\u00e0 stimato di volta in volta.\n\nQuindi, il primo giorno:\n$$P(R_1|k_1)\\propto P(R_1)\\cdot\\mathcal{L}(R_1|k_1)$$\n\nIl secondo giorno:\n$$P(R_2|k_1,k_2) \\propto P(R_2)\\cdot\\mathcal{L}(R_2|k_2) = {P(R_1|k_1)\\cdot P(R_2|R_1) \\cdot \\mathcal{L}(R_2|k_2)}$$\ne cos\u00ec via.\n\nCome funzione di *verosomiglianza*, usiamo la [distribuzione di Poisson](https:\/\/it.wikipedia.org\/wiki\/Distribuzione_di_Poisson), definita come: $$P(k|\\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$$\n\nLa relazione tra $\\lambda$ e $R_t$ ([derivata qui](https:\/\/journals.plos.org\/plosone\/article?id=10.1371\/journal.pone.0002185)) \u00e8: $$ \\lambda = k_{t-1}e^{\\gamma(R_t-1)}$$\ndove $k_{t-1}$ \u00e8 il numero di nuovi casi nel giorno $t-1$ e $\\gamma$ \u00e8 il reciproco dell'intervallo seriale, stimato in [circa 7 giorni](https:\/\/wwwnc.cdc.gov\/eid\/article\/26\/7\/20-0282_article). \n","cbcb6d67":"Sappiamo che la segnalazione dei casi nel tempo \u00e8 molto irregolare. I nuovi casi vengono registrati quando si ha a disposizione l'esito del tampone. Il tempo impiegato per ottenere i risultati dipene dalle singole regioni, cos\u00ec come i criteri adottati per decidere chi sottoporre al test. Questo causa forti variabilit\u00e0 e imprecisioni nel conteggio dei nuovi casi giornalieri.\n\nPer ottenere la migliore visualizzazione possibile dei dati \"reali\", \u00e8 opportuno applicare un filtro gaussiano alle serie temporali, in modo da regolarizzare la curva dei contagi, riducendo il rumore sui dati.","86d0f4a5":"## 1. Introduzione\n\nBasandoci sul [lavoro svolto](https:\/\/github.com\/k-sys\/covid-19\/blob\/master\/Realtime%20R0.ipynb) per gli USA da [Kevin Systrom](https:\/\/it.wikipedia.org\/wiki\/Kevin_Systrom) per la stima del cosidetto valore $R_0$ (ovvero il [numero di riproduzione di base](https:\/\/www.iss.it\/primo-piano\/-\/asset_publisher\/o4oGR9qmvUz9\/content\/id\/5268851)) del **COVID-19**, abbiamo provato ad applicare la stessa analisi alle regioni italiane, sulla base dei [dati forniti](http:\/\/opendatadpc.maps.arcgis.com\/apps\/opsdashboard\/index.html#\/b0c68bce2cce478eaac82fe38d4138b1) dal Dipartimento della Protezione Civile.\n\nI risultati possono essere utili per valutare l'effettiva efficacia delle misure di contenimento sia nazionali che regionali e, in base ad essi, definire i successivi passi nella gestione della pandemia.\n\nQuella presentata \u00e8 una versione modificata di una soluzione sviluppata da [Bettencourt & Ribeiro 2008](https:\/\/journals.plos.org\/plosone\/article?id=10.1371\/journal.pone.0002185) per stimare $R_t$ in tempo reale usando un **approccio bayesiano**. L'articolo di Kevin Systrom migliora la stima del valore statico di $R$, introducendo un modello di processo con rumore gaussiano per stimare un $R_t$ variabile nel tempo.\n","9ad2f962":"## 3. Risultati finali\nUna volta selezionato il $\\sigma$ ottimale, valutiamo le distribuzioni *a posteriori* calcolate in precedenza in quel valore di $\\sigma$ per ogni regione. Calcoliamo anche gli intervalli di confidenza del 90% e 50% e anche il valore pi\u00f9 probabile.","c24ab6ca":"### 2.3 Visualizzazione\nDefiniamo la funzione che useremo per la visualizzazione dei risultati ottenuti, includendo sia il valore stimato di $R_t$ che l'area di confidenza.","e9699813":"### 3.2 Valore pi\u00f9 recente","86d02b36":"### 2.1 Setup\nPreparazione dei dati e delle strutture dati utilizzate nell'algoritmo."}}