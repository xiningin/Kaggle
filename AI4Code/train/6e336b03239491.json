{"cell_type":{"8e289996":"code","74b46ec6":"code","60503bdf":"code","ecabf108":"code","6d4dbe72":"code","f85929b5":"code","8e10f024":"code","4cff7093":"code","ad938c5a":"code","f8ffefd9":"code","b15500b2":"code","5e94c758":"code","20694e09":"code","1d844e42":"code","3b229567":"code","374769b1":"code","54917c0c":"code","289f73d0":"code","d9f6f7cd":"code","8e763b01":"code","923e0447":"code","0e95aa1e":"code","bab6b9e7":"code","2bef5a23":"code","880c7e0b":"code","af27822d":"code","170921d1":"code","20a11ced":"code","0be75206":"code","e279a279":"code","49f6c3ee":"code","5adcc9f7":"code","a647a445":"code","3f2575b5":"code","126ade3d":"code","7bd35227":"code","43d4dc73":"code","5087e06a":"code","55029bc9":"code","a1bdcd15":"code","d763272e":"code","c15fa454":"code","b049ee0b":"code","a2d14ab8":"code","85c26623":"code","3eab803b":"code","dd2503ed":"code","574f5472":"code","776d708c":"code","83a816ef":"code","a3a2bef9":"code","84048952":"code","268c8668":"code","f241f476":"code","5d1f0178":"code","61725341":"code","371da4f1":"code","586f733f":"code","1739f31b":"markdown","274ef70d":"markdown","84b5802e":"markdown","db52e222":"markdown","043e286c":"markdown","d328a591":"markdown","72e4161f":"markdown","3d864349":"markdown","abcb6ca0":"markdown","cd931946":"markdown","b7544b8d":"markdown","7ae4c0d4":"markdown","3d741175":"markdown","e91b616e":"markdown","3033bcaa":"markdown","b6bc4469":"markdown","4840d296":"markdown","450d7194":"markdown","1ebf4cce":"markdown","7001da29":"markdown","83a6589f":"markdown","fde038d0":"markdown","caac5fc1":"markdown","67def569":"markdown","82216308":"markdown","9d969907":"markdown","9ab85123":"markdown","4378b9fd":"markdown","b7331cf1":"markdown","4a6b97b8":"markdown","fdefb2b4":"markdown","9df240f1":"markdown","2befe17d":"markdown","fe5e52d4":"markdown","fefc6e8a":"markdown","4c1fc40a":"markdown","fb6150e3":"markdown","daa41d50":"markdown","ba310842":"markdown","6b3e77b2":"markdown","e43f47cd":"markdown","ee1f43e7":"markdown","0475870f":"markdown","5153faa5":"markdown","e27d6a42":"markdown","3668cfb7":"markdown","945182ae":"markdown","1ba93d69":"markdown","bebb40e7":"markdown","e05eb97e":"markdown","9a6a9dc2":"markdown","76f1ef96":"markdown","a7bb7981":"markdown","b929e2ae":"markdown","7015298b":"markdown","91d0bbfc":"markdown","fb91210f":"markdown","02b50596":"markdown","f08a5a04":"markdown","6a36e377":"markdown","dfd404a9":"markdown","87be5b0d":"markdown","9c093255":"markdown","4bb4fd08":"markdown","d721c59e":"markdown","99ea3cdb":"markdown","7667636e":"markdown","91d42eb7":"markdown"},"source":{"8e289996":"#@title Current Date\nToday = '2021-11-15' #@param {type:\"date\"}\n","74b46ec6":"#@markdown ---\n#@markdown ### Enter your details here:\nTeam_Number = \"3\" #@param {type:\"string\"}\nTeam_Name = \"TeamThree\" #@param {type:\"string\"}\nStudent_ID = \"12152854\" #@param {type:\"string\"}\nStudent_full_name = \"Ashley Bentley\" #@param {type:\"string\"}\nStudent_ID = \"19133359\" #@param {type:\"string\"}\nStudent_full_name = \"Eva Radescu\" #@param {type:\"string\"}\nStudent_ID = \"21026416\" #@param {type:\"string\"}\nStudent_full_name = \"Deepika V Chhatpar\" #@param {type:\"string\"}\nStudent_ID = \"21041725\" #@param {type:\"string\"}\nStudent_full_name = \"Johnathan Fernandes\" #@param {type:\"string\"}\nStudent_ID = \"21033951\" #@param {type:\"string\"}\nStudent_full_name = \"Tejaswini Javagal\" #@param {type:\"string\"}\n#@markdown ---","60503bdf":"#@title Notebook information\nNotebook_type = 'Assignment' #@param [\"Example\", \"Lab\", \"Practice\", \"Etivity\", \"Assignment\", \"Exam\"]\nVersion = \"Final\" #@param [\"Draft\", \"Final\"] {type:\"raw\"}\nSubmission = True #@param {type:\"boolean\"}","ecabf108":"# Import packages\n\n# Data processing\nimport numpy as np\nimport pandas as pd\n\n# Data visualization\nimport folium # Create maps\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom folium.plugins import MarkerCluster\n\n# Text Processing\nimport nltk # Natural Language ToolKit\nimport re # Regular expressions\nimport string # To detect punctuation\nfrom nltk.corpus import stopwords # Import stop words list\nfrom nltk.stem import WordNetLemmatizer # Lemmatization\nfrom nltk.tokenize import word_tokenize # tokenization\n\n# Machine learning\nfrom sklearn.preprocessing import OneHotEncoder # Encode categorical variables\nfrom sklearn.preprocessing import StandardScaler # Scale numerical data\n\n# File handling\n#from google.colab import files # Enables direct file uploads in colab","6d4dbe72":"# Other setup\nsns.set_theme(style=\"whitegrid\", \n              palette=\"colorblind\")","f85929b5":"# Upload data to Google Colab\n#uploaded = files.upload()","8e10f024":"# Import data\n#df = pd.read_csv(\"HousePrice_Train.csv\")","4cff7093":"# Import data from kaggle\ndf = pd.read_csv(\"..\/input\/ul-cs6501-sem1-2021-2\/HousePrice_Train.csv\")","ad938c5a":"# View data\ndf","f8ffefd9":"df.info() # Data types of columns","b15500b2":"df.describe() # Statistical properties of dataset","5e94c758":"df.shape # Number of (rows, columns)","20694e09":"df.head() # First few rows","1d844e42":"df.tail() # Last few rows","3b229567":"df.isnull().sum() # Check number of missing \/ Nan \/ NULL values per column","374769b1":"# Draw histograms for data\ncolumn_list = ['Num_Bathrooms', 'Num_Beds', 'BER_class', 'Type','Surface', 'Price']\nfor column in column_list: # Iterate through dataset columns from list\n    plt.figure(figsize=(20,5)) # Set figure size\n    sns.histplot(df[column]) # Create histogram\n    plt.show() # Show histogram","54917c0c":"# Draw up scatterplot to examine the relationship between numerical variables and price\ncolumn_list = ['Num_Bathrooms', 'Num_Beds', 'Latitude', 'Longitude','Surface']\nfor column in column_list:\n    plt.figure(figsize=(20,5)) \n    sns.scatterplot(df[column],df[\"Price\"])\n    plt.show()","289f73d0":"# Box plots to look for outliers\ncolumn_list = ['BER_class','Type']\nfor column in column_list:\n    plt.figure(figsize=(20,5)) \n    sns.boxplot(df[column],df[\"Price\"])\n    plt.show()","d9f6f7cd":"# View correlation of variables\nplt.figure(figsize= (10, 10)) # Define plot size, increase if the graph is crowded\nsns.heatmap(df.iloc[:,2:].corr(),square=True, annot=True)","8e763b01":"# Visualize locations \n\nlocations = df[['Latitude', 'Longitude']] # Store locations as a set of coordinates\nlocationlist = locations.values.tolist() # convert to a list\n\n# Set map parameters.\nmap = folium.Map(location=[df['Latitude'].median(), df['Longitude'].median()], # Center the default map location around our data\n                 zoom_start=12) \nmarker_cluster = MarkerCluster().add_to(map) # condense functions to create clusters when zoomed out\n\nfor point in range(0, len(locationlist)): # Iterate through list of coordinates\n    folium.Marker(locationlist[point], popup=df['Location'][point]).add_to(marker_cluster) # Add each to map\nmap # Display the map. Takes a while to load!","923e0447":"df[df[\"Latitude\"] < 52.6]","0e95aa1e":"df[df[\"Longitude\"] > -2]","bab6b9e7":"outlier_index = df[df[\"ID\"] == 12270559].index # Store index of row where ID = 12270559\ndf.drop(outlier_index, inplace=True) # Delete it","2bef5a23":"df[df[\"ID\"] == 12270559] # find rows where ID = 12270559","880c7e0b":"df[df[\"Num_Bathrooms\"] > 10]","af27822d":"df[df[\"Num_Beds\"] > 10]","170921d1":"fancy_house_IDs = [\"12381836\",\"11780612\",\"12085770\"]\nfor houseID in fancy_house_IDs:\n    outlier_index = df[df[\"ID\"] == houseID].index\n    df.drop(outlier_index, inplace=True)","20a11ced":"outlier_index = df[df[\"Surface\"] > 5000].index \ndf.drop(outlier_index, inplace=True)","0be75206":"plt.figure(figsize= (10, 10))\nsns.heatmap(df.iloc[:,2:].corr(),square=True, annot=True)","e279a279":"df_numeric = df[[\"Location\",\"Num_Bathrooms\",\"Num_Beds\",\"BER_class\",\"Latitude\",\"Longitude\",\"Type\",\"Surface\",\"Price\"]]","49f6c3ee":"# Initialize (set up) encoder to do the heavy lifting for us\nencoder = OneHotEncoder(drop=\"first\", # Remove the first column\n                        sparse=False) # Set to return an array instead of a matrix\n                        ","5adcc9f7":"temp = pd.DataFrame(encoder.fit_transform(df_numeric[[\"Location\"]])) # Run encoder on \"Location\" and store in \"temp\"\ntemp.columns = \"Location_\"+encoder.categories_[0][1:] # Get column names from encoder and set them in temp\ntemp.columns.tolist() # View columns","a647a445":"df_numeric = df_numeric.drop(['Location'] ,axis=1) # Remove the \"Location\" column\ndf_numeric = pd.concat([temp,df_numeric],axis=1) # Concatenate (join) the dataframes along the columns\ndf_numeric.columns.tolist() # View column names of data","3f2575b5":"Type_column = df_numeric[\"Type\"].dropna()\nType_column = Type_column.values.reshape((-1,1))","126ade3d":"temp = pd.DataFrame(encoder.fit_transform(Type_column))\ntemp.columns = \"Type_\"+encoder.categories_[0][1:]\ntemp.head()","7bd35227":"df_numeric = df_numeric.drop(['Type'] ,axis=1)\ndf_numeric = pd.concat([temp,df_numeric],axis=1)\ndf_numeric.columns.tolist()","43d4dc73":"BER_column = df_numeric[\"BER_class\"].dropna()\nBER_column = BER_column.replace(\"SINo666of2006exempt\", \"Exempt\")\nBER_column = BER_column.values.reshape((-1,1))","5087e06a":"temp = pd.DataFrame(encoder.fit_transform(BER_column))\ntemp.columns = \"BER_Rating_\"+encoder.categories_[0][1:]\ntemp.head() # We can also use head() to view row names for smaller data","55029bc9":"df_numeric = df_numeric.drop(['BER_class'] ,axis=1)\ndf_numeric = pd.concat([temp,df_numeric],axis=1)\ndf_numeric.columns.tolist()","a1bdcd15":"scaler = StandardScaler() # Initialize scaler ","d763272e":"for column in df_numeric.columns:\n    df_numeric[column]=pd.DataFrame(scaler.fit_transform(df_numeric[column].values.reshape(-1,1))) # Apply scaler","c15fa454":"df_numeric.describe()","b049ee0b":"df_textual = df[[\"Description\",\"Services\",\"Features\",\"Price\"]]","a2d14ab8":"lemmatizer = WordNetLemmatizer() # Initialize Lemmatizer\nnltk.download('wordnet')# Download lemmatization database\nnltk.download('stopwords') # Download list of stop words\nnltk.download('punkt') # download list of punctuation symbols\nstopwords_set = set(stopwords.words('english')) # Define list of stop words","85c26623":"stopwords_set","3eab803b":"def clean(row):\n    input_data = str(row)\n\n    lowertext = input_data.lower() # convert to lower case\n    tokens = word_tokenize(lowertext) # Tokenize\n    df_stopwords=[word for word in tokens if word not in stopwords_set] # Remove stopwords\n    df_punctuation=[re.sub(r'['+string.punctuation+']+', ' ', i) for i in df_stopwords] # Remove Punctuation and split 's, 't, 've with a space for filter\n    df_whitespace = ' '.join(df_punctuation) # Remove multiple whitespace\n    lemmatizer = WordNetLemmatizer() # Initialize lemmatizer\n    df_lemma = lemmatizer.lemmatize(df_whitespace) # Lemmatize\n    df_lemma_tokenized = word_tokenize(df_lemma) # Tokenize again\n    df_lemma_shortwords = [re.sub(r'^\\w\\w?$', '', i) for i in df_lemma_tokenized] # Remove short words\n    df_lemma_whitespace =' '.join(df_lemma_shortwords) # Join whitespace again\n    df_lemma_multiplewhitespace = re.sub(r'\\s\\s+', ' ', df_lemma_whitespace) # Join multiple white spaces\n    df_clean = df_lemma_multiplewhitespace.lstrip(' ') #Remove any whitespace at the front of the sentence\n\n    return df_clean","dd2503ed":"df_textual[\"Description\"][0] # Original","574f5472":"clean(df_textual[\"Description\"][0]) # Cleaned","776d708c":"df_textual_cleaned = df_textual.iloc[:,:-1].applymap(clean) # Apply cleaning to entire dataset","83a816ef":"df_textual_cleaned = pd.concat([df_textual_cleaned,df_textual[\"Price\"]],axis=1) # concatenate price","a3a2bef9":"df_textual_cleaned.head() # View cleaned dataset","84048952":"# TODO","268c8668":"# TODO","f241f476":"# TODO","5d1f0178":"# TODO","61725341":"# TODO","371da4f1":"# TODO","586f733f":"# TODO","1739f31b":"The describe() function gives us a little more insight about the statistical properties of the data. It conveniently excludes the text based data, giving us stats about numerical data only.","274ef70d":"## Vectorization","84b5802e":"Taking it step by step:\n* Lowercase: easier to process everything when you don\u2019t have to worry about case sensitivity.\n* Tokenize: Split sentences into words, with each word being a \u201ctoken\u201d.\n* Remove stopwords: Scans through the tokens of each sentence, checks them against the big list of stopwords, and only keeps them if the aren\u2019t present in the stopwords list.\n* Remove punctuation: Punctuation usually doesn\u2019t add much to the context of a sentence, and can be very ambiguous depending on usage. It\u2019s better to remove them altogether.\n* Whitespace: A lot of these operations don\u2019t actually remove the offending characters, but rather replace them with a space. We join multiple spaces together quite often here.\n* Lemmatization: Reduces words to their base form.\n\nThe odd symbols you see around the re.sub() functions are known as \u201cregular expressions\u201d. They are a way to scan through and identify patterns in blocks of text.","db52e222":"Now we work on removing the other outliers.","043e286c":"# INTRODUCTION","d328a591":"We can also directly check the condition and drop the outliers by index as such:","72e4161f":"## Predicting","3d864349":"Looks like this is it!","abcb6ca0":"We now do the same with the \u201cType\u201d and \u201cBER score\u201d columns, taking care to drop the missing values (they would become empty columns anyways) before adding the prefix and concatenating them with our main dataset.","cd931946":"# SUMMARY","b7544b8d":"# Modelling","7ae4c0d4":"The above code gets rid of 2 houses with an oddly high surface area.","3d741175":"The \u201cfolium\u201d package allows us to visualize a set of coordinates on an interactive map on interactive python notebooks, so we use it here to draw up a map of our houses. This should also help us investigate the outlier we observed earlier.","e91b616e":"Textual data","3033bcaa":"We now define a cleaning function that should be able to reduce entire sentences down to their base forms.","b6bc4469":"### Removing outliers","4840d296":"After the ordeal we just went through with categorical data, processing full blown textual data seems like an even bigger hurdle. We can\u2019t possibly encode each and every word from each and every review, can we?\n\nWell actually, we can! The process follows the same principle, transforming words into numbers to we can feed them into our traditional machine learning algorithms. A major difference here is that we initially have to clean the data and extract the most significant words, before assigning values to them, a process known as \u201cvectorizing\u201d.\n\nLet\u2019s walk through cleaning first. \n","450d7194":"To start off, we grab a subset of the original dataframe containing only numerical and categorical data.","1ebf4cce":"Histograms are usually the primary go-to when working on data viz. They depict the frequency of data points, enabling us to quickly check for outliers.\n\nHistograms obviously aren\u2019t applicable to textual data, so we define a custom list of columns to plot them for:","7001da29":"Going further, we will need to process the data separately. For the next section, we actually separate the data into two sections \u2013 number\/categorical, and textual.","83a6589f":"## Predicting","fde038d0":"There seem to be a few extravagant houses in here that's throwing off our model.","caac5fc1":"# LSTM","67def569":"To start off, we initialize our lemmatizer object.\nLemmatizing is the process of reducing a word to its base form. It is an advanced form of stemming, taking into consideration the context of the word instead of blindly removing suffixes.\n\nFor example:\n\n|   Word   | Stemming | Lemmatizing |\n|:--------:|:--------:|:-----------:|\n|    was   |    wa    |      be     |\n|  studies |   studi  |    study    |\n| studying |   study  |    study    |\n\n\nAs we see from the example, lemmatizing is far superior to stemming when reducing words to their base form. The fewer words we have, the smaller our data will be and the faster our models will learn.\n\nWe also download a few sets:\n* Wordnet: the Lemmatization dataset.\n* Stopwords: Common \u201cstop\u201d words that do not add much to the context of a sentence.\n* Punkt: Punctuation marks.\n","82216308":"The aim of this experiment is to design and develop a system to predict the price of a house based on features given in the dataset. The predictions from this system will simultaneously be submitted to a ranked Kaggle competition in addition to the usual submission on SULIS. We are expected to implement the machine learning concepts studied during this week, i.e. Natural Language Processing and Long Short Term Memory. We are also presented with the opportunity to apply prior knowledge to design an effective model.","9d969907":"While the preprocessing steps  technically count as NLP, I chose to include them in the general preprocessing section, leaving this one just for the vectorization and modelling. While we studied multiple vectorization techniques in class, namely \u201cCount Vectorization\u201d and \u201cTerm Frequency Inverse Document Frequency\u201d (TF-IDF), multiple other methods exist, with the industry standards today being \u201cWord2Vec\u201d and \u201cGlobal Vectors for Word Representation\u201d (GloVe). In this section, we will attempt to use a pure NLP approach (textual data only!) to predict house price.","9ab85123":"Now that our categorical features are all encoded, we move on to scaling. Scaling multiplies the data points by a specific value so that the resulting data has a mean of 0 ad a standard deviation of 1. Doing so makes the model much more computationally efficient. Other methods of scaling also exist, applying different operations for specific use cases.\n\n[Scaler documentation here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html)","4378b9fd":"# NLP","b7331cf1":"TODO","4a6b97b8":"As mentioned earlier, this experiment also encourages us to attempt building our own systems by considering different approaches. Would another regression technique perform better (in terms of accuracy and\/or speed) when compared to the LSTM? Do any of these numerical methods stand a chance against NLP vectorization? Would it be possible to combine the numerical, categorical and textual data? What about using the base model outputs to train another model on top? Will XGBoost outclass all the other algorithms again?! ","fdefb2b4":"Machine learning algorithms work exclusively with numbers. In order to apply them to categorical data, we must first transform it. \n\nThe na\u00efve approach would be to so something like the following:\n\nBefore:\n\n| ID | Letter | Target |\n|:--:|:------:|:------:|\n|  1 |    A   |    2   |\n|  2 |    B   |    3   |\n|  3 |    C   |    4   |\n|  4 |    B   |    5   |\n\nAfter:\n\n| ID | Letter | Target |\n|:--:|:------:|:------:|\n|  1 |    1   |    2   |\n|  2 |    2   |    3   |\n|  3 |    3   |    4   |\n|  4 |    2   |    5   |\n\nThis method ids known as \u201cLabel encoding\u201d\n\nHowever, simply assigning a number to a value is impossible, as we cannot associate words with numbers. While 1 > 2 is true, saying \u201cA = 1\u201d or \u201cA>B\u201d makes no sense when speaking in general terms.\n\nThe correct method is to treat each category as a column, and indicating the presence of the feature using a binary value.\n\nBefore:\n\n| ID | Letter | Target |\n|:--:|:------:|:------:|\n|  1 |    A   |    2   |\n|  2 |    B   |    3   |\n|  3 |    C   |    4   |\n|  4 |    B   |    5   |\n\nAfter:\n\n| ID | A | B | C | Target |\n|:--:|:-:|:-:|---|:------:|\n|  1 | 1 | 0 | 0 |    2   |\n|  2 | 0 | 1 | 0 |    3   |\n|  3 | 0 | 0 | 1 |    4   |\n|  4 | 0 | 1 | 0 |    5   |\n\nHOWEVER, we cannot directly toss this into a model and call it a day. Observing the table, it is fairly easy to infer to value of column \u201cC\u201d based on the values of column \u201cA\u201d and \u201cB\u201d. This is essentially a problem with machine learning known as \u201cmulticollinearity\u201d, and can cause issues when building the model. So to prevent any issues, we remove one column and finally end up with a table as:\n\n| ID | A | B | Target |\n|:--:|:-:|:-:|:------:|\n|  1 | 1 | 0 |    2   |\n|  2 | 0 | 1 |    3   |\n|  3 | 0 | 0 |    4   |\n|  4 | 0 | 1 |    5   |\n\nThis process is known as \u201cencoding\u201d, specifically \u201cone-hot encoding\u201d.\n","9df240f1":"An alternative to our scatter plot shenanigans is to calculate a correlation matrix and plot it using a heatmap.","2befe17d":"We also rename the \u201cSINo666of2006exempt\u201d column here, replacing it with \u201cExempt\u201d, which is much easier to remember.","fe5e52d4":"Our numerical data is all scaled and ready for the model now.","fefc6e8a":"## Exploratory Data Analysis (EDA)","4c1fc40a":"The first thing we do is remove the outlier we just spotted. We do this by simply finding the ID of the specific point, and simply dropping it. ","fb6150e3":"The info() function presents us with a quick glance at the data, showcasing the features and their data types. The numerical data is either integer (int64) or floating point (float64), while the categorical and text data are simply stored as objects.","daa41d50":"## Preprocessing","ba310842":"We note that the \u201cBER_class\u201d and \u201cServices\u201d columns have several missing values, which we shall deal with when processing this data.","6b3e77b2":"Much better! Removing the outliers greatly enhances the correlation of our data, allowing us to build a better model for the reasonably prices houses.","e43f47cd":"As insightful as those functions were, staring at raw numbers can only get us so far. Now we start visualizing the data to get a better understanding of its statistical properties, and also hunt down a few outliers, if present.","ee1f43e7":"<div>\n<img src=\"https:\/\/drive.google.com\/uc?export=view&id=1vK33e_EqaHgBHcbRV_m38hx6IkG0blK_\" width=\"350\"\/>\n<\/div> \n\n#**Artificial Intelligence - MSc**\n##CS6501 - MACHINE LEARNING APPLICATIONS \n\n###Instructor: Enrique Naredo\n###CS6501_Kaggle","0475870f":"Note the empty dataframe returned.","5153faa5":"Viewing the data on the map shows us that most of the data is in and around Dublin. Zooming out, however, reveals our outlier point. A singular point, all the way over in the United Kingdom! Considering we\u2019re looking at Ireland housing data, it\u2019s safe to say this point was included in our dataset accidently. We\u2019ll exclude it from our data before processing any further.","e27d6a42":"Let\u2019s apply this to the first description and compare the results:","3668cfb7":"# Modelling","945182ae":"We can verify that this point is gone by searching for the same ID again.","1ba93d69":"Graphs plotted and outliers identified, we move on to pre processing our data.","bebb40e7":"So far so good! Now all we have next is to vectorize the data and apply a model!","e05eb97e":"The correlation heatmap shows us at a glance that price is highly correlated to the number of bedrooms and bathrooms, while not very correlated with location and surface are, although this could be caused due to outliers. This is why even though it might be faster,  we observe the relationship between variables using scatter plots.","9a6a9dc2":"We can validate our oulier removal by running the plots again. Let's check the heatmap again.","76f1ef96":"\u201cNull\u201d or missing values are infamous for messing up prediction models, and should be processed very carefully. We use built in functions to check for null values.","a7bb7981":"Looking through the scatterplots, we note down;\n* There is a slight correlation between number of bathrooms and price, with a few houses having an exorbitant number of them. While technically feasible in real life, it might affect our model adversely if not removed.\n* The same can be said for number of bedrooms.\n* Plotting Latitude and Longitude shows a very slight correlation between location and price (which we know usually happens), but interestingly enough, contains a singular point all by its lonesome in the \u201cLongitude\u201d plot. We can look into this later in more detail.\n* And finally, there is a strong positive correlation between price and surface area. Not surprising.\n","b929e2ae":"While slightly skewed, the histograms do not immediately signal anything too dangerous. We do note that the \u201cBER_class\u201d column has a value \u201cSINo666of2006exempt\u201d, which is quite a mouthful. ","7015298b":"Taking a look at our data, we note that it has a mix of numerical, categorical and text data. Each of these will have to be processed separately. We start by looking through the data as best as we can, checking for missing values and analyzing for outliers.","91d0bbfc":"* The box plots for BER class showcases quite a few outliers, namely a house with an exorbitantly high price with a BER rating of \u201cB2\u201d\n* There are plenty of outliers for the \u201cdetached\u201d home too, but there might be other reasons for these houses to have such high prices, like location. We shouldn\u2019t be too quick to discard these data points. We might come back to take care of them if they heavily affect the model performance.\n","fb91210f":"The function that removes missing values, dropna(), also converts the data into a list of dimension (x,). The encoder expects an array of dimensions (x,1), so we use the reshape() function to transform the data accordingly. A tiny technicality, but the cause of much debate in the community.","02b50596":"The One Hot Encoder from the scikit -learn library [(documentation available here)]( https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html) handles most of the work for us. It doesn\u2019t, however, retain column names, leaving us to fetch them from the \u201cencoder.categories_\u201d variable (grabbing the **second** name onwards, since we drop the first column), and apply them to our new dataframe. This allows us to attach a prefix to our new column names, however. So we prefix all the location columns with \u201cLocation_\u201d. ","f08a5a04":"The other technique studied during this week is \u201cLong Short Term Memory\u201d (LSTM). This is a neural network based model which works best on time series data, but should perform sufficiently on normal numerical data as well. We look forward to observing the time and accuracy tradeoff when using an LSTM based model on non-time series data. Currently, we intend to apply this technique solely on the numerical data. ","6a36e377":"While scatter plots are also applicable to categorical variables, they're more useful in spotting outliers than anything. Box plots are more suited to this.","dfd404a9":"Since we primarily use Google Colab, this allows us to select our data and easily upload it.","87be5b0d":"### Numerical and categorical data","9c093255":"## Imports","4bb4fd08":"Coming up next, we have scatterplots. We use these to examine the relationship between the predictor and target variables. A high correlation (data points forming a line) indicated that that feature would be a very good predictor.","d721c59e":"We can actually view the list of stop words!","99ea3cdb":"# OTHERS","7667636e":"We can, observe the results of scaling by using the describe() function again.","91d42eb7":"Looking good so far! It\u2019s boiled down the description to key terms, removed useless punctuation and pesky escape characters. We can apply the function to every cell of the dataset using the applymap() function.\n\nNote that since our dataset also contains the price data, we must be careful to exclude this column and then concatenate it with our cleaned dataset later. Our cleaning function may or may not affect the price rows, and I really don\u2019t want to find out right now.\n"}}