{"cell_type":{"a42261d4":"code","a7450a2f":"code","52af491d":"code","7e3d0e2a":"code","d7a4a54e":"code","2a0f179d":"code","db3050b6":"code","78c0dcf3":"code","48146ff8":"code","8b56a264":"code","4fa98ba3":"code","6fcdb91c":"code","a9daf764":"code","da71efae":"code","bc7033bf":"code","c5d1a5a5":"code","0eee4058":"code","07a7b10b":"code","b49e033c":"code","b507051b":"code","ec801ff6":"code","1e2b6e69":"code","6ddf8f1f":"code","93cbf440":"code","12d5912c":"code","ea0d5227":"code","4c306edc":"code","5c435c59":"code","3d989f43":"code","b7e6819a":"code","dd3a618b":"code","c03f4494":"code","b2eeeb1a":"code","1d5aec54":"code","fe9c0277":"code","9ed2be58":"markdown","c9356303":"markdown","a0e764f7":"markdown","f1c90919":"markdown","f8415cab":"markdown","6c9af266":"markdown","392f2989":"markdown","2bd57aee":"markdown","f0139e9d":"markdown","7a0b89af":"markdown","a8cfa144":"markdown","01c07c96":"markdown","90220349":"markdown","b80bcfd4":"markdown","1e796281":"markdown","f0ef43e8":"markdown","f446a58c":"markdown","d6b4338c":"markdown"},"source":{"a42261d4":"import numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport re # Regex\nimport string # String manipulation\nimport spacy # NLP tooling\n\n# IMPORTS FOR TRAINING\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import  Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom sklearn.utils import shuffle\n\n\n#Preprocessing\nfrom sklearn.preprocessing import LabelEncoder # Encode label into numerical format\nfrom sklearn.model_selection import train_test_split # Split train and validation set\n\n#RESULTS\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report, confusion_matrix\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a7450a2f":"nlp = spacy.load('en_core_web_sm')  \ndf_document = pd.read_csv('..\/input\/emotion-detection-from-text\/tweet_emotions.csv')","52af491d":"df_document","7e3d0e2a":"def clean_text(text):\n    regex_html = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    remove_digits = str.maketrans('', '', string.digits + string.punctuation)\n    text = re.sub(regex_html, '', text)\n    text = text.translate(remove_digits)\n    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\", \" \", text).split()).lower()\n\ndf_document['text'] = (df_document.content).apply(clean_text)\n#\\.apply(nlp).apply(lambda x: \" \".join(token.lemma_ for token in x if not token.is_stop).lower())","d7a4a54e":"df_document","2a0f179d":"df_document.sentiment.value_counts()","db3050b6":"import seaborn as sns\ncountplt, ax = plt.subplots(figsize = (12,6))\nax = sns.countplot(df_document.sentiment, palette='Set3')","78c0dcf3":"df_document['sentiment'] = df_document['sentiment'].apply(lambda x : x if x in ['happiness', 'sadness', 'worry', 'neutral', 'love', 'hate'] else 'other') ","48146ff8":"col = 'sentiment'\nfig, (ax1, ax2)  = plt.subplots(nrows=1, ncols=2, figsize=(12,8))\nexplode = list((np.array(list(df_document[col].dropna().value_counts()))\/sum(list(df_document[col].dropna().value_counts())))[::-1])[:]\nlabels = list(df_document[col].dropna().unique())[:]\nsizes = df_document[col].value_counts()[:]\nax2.pie(sizes,  explode=explode, startangle=60, labels=labels, autopct='%1.0f%%', pctdistance=0.8)\nax2.add_artist(plt.Circle((0,0),0.4,fc='white'))\nsns.countplot(y =col, data = df_document, ax=ax1)\nax1.set_title(\"Count of each emotion\")\nax2.set_title(\"Percentage of each emotion\")\nplt.show()","8b56a264":"le = LabelEncoder().fit(df_document['sentiment'])\ndf_document['label'] = le.transform(df_document['sentiment'])\ntrain, val = train_test_split(df_document, test_size=0.15, random_state=42, stratify=df_document.label)\ntrain, test = train_test_split(train, test_size=0.15, random_state=42, stratify=train.label)","4fa98ba3":"df_document[['text', 'label']]","6fcdb91c":"!pip install simpletransformers\nfrom simpletransformers.language_representation import RepresentationModel","a9daf764":"# use this block if new bert vectors needed\n# without CUDA=true it takes to much time to vectorize\n\n# TRAIN SET\nsentences = train.text\nmodel = RepresentationModel(model_type=\"bert\", model_name='bert-base-uncased', use_cuda=True)\nword_vectors_document_train = model.encode_sentences(sentences, combine_strategy=\"mean\")","da71efae":"word_vectors_document_train.shape","bc7033bf":"# use this block if new bert vectors needed\n\n# VAL SET\nsentences = val.text\nmodel = RepresentationModel(model_type=\"bert\", model_name='bert-base-uncased', use_cuda=True)\nword_vectors_document_val = model.encode_sentences(sentences, combine_strategy= \"mean\")","c5d1a5a5":"word_vectors_document_val.shape","0eee4058":"# use this block if new bert vectors needed\n\n# TEST SET\nsentences = test.text\nmodel = RepresentationModel(model_type=\"bert\", model_name='bert-base-uncased', use_cuda=True)\nword_vectors_document_test = model.encode_sentences(sentences, combine_strategy= \"mean\")","07a7b10b":"word_vectors_document_test.shape","b49e033c":"x_train, y_train, x_val, y_val, x_test, y_test = word_vectors_document_train, train.label,\\\n                                                 word_vectors_document_val, val.label, \\\n                                                 word_vectors_document_test, test.label","b507051b":"class MIXUP:\n    \"\"\"\n    This class implements the mixup algorithm [1] for natural language processing.\n    [1] Zhang, Hongyi, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. \"mixup: Beyond empirical risk\n    minimization.\" in International Conference on Learning Representations (2018).\n    https:\/\/openreview.net\/forum?id=r1Ddp1-Rb\n    \"\"\"\n\n    @staticmethod\n    def validate(**kwargs):\n        \"\"\"Validate input data\"\"\"\n\n        if 'data' in kwargs:\n            if isinstance(kwargs['data'], list):\n                kwargs['data'] = np.array(kwargs['data'])\n            if not isinstance(kwargs['data'], np.ndarray):\n                raise TypeError(\"data must be numpy array. Found \" + str(type(kwargs['data'])))\n        if 'labels' in kwargs:\n            if isinstance(kwargs['labels'], (list, type(None))):\n                kwargs['labels'] = np.array(kwargs['labels'])\n            if not isinstance(kwargs['labels'], np.ndarray):\n                raise TypeError(\"labels must be numpy array. Found \" + str(type(kwargs['labels'])))\n        if 'batch_size' in kwargs:\n            if not isinstance(kwargs['batch_size'], int):\n                raise TypeError(\"batch_size must be a valid integer. Found \" + str(type(kwargs['batch_size'])))\n        if 'shuffle' in kwargs:\n            if not isinstance(kwargs['shuffle'], bool):\n                raise TypeError(\"shuffle must be a boolean. Found \" + str(type(kwargs['shuffle'])))\n        if 'runs' in kwargs:\n            if not isinstance(kwargs['runs'], int):\n                raise TypeError(\"runs must be a valid integer. Found \" + str(type(kwargs['runs'])))\n\n    def __init__(self, random_state=1, runs=1):\n        self.random_state = random_state\n        self.runs = runs\n        if isinstance(self.random_state, int):\n            random.seed(self.random_state)\n            np.random.seed(self.random_state)\n        else:\n            raise TypeError(\"random_state must have type int\")\n\n    def mixup_data(self, x, y=None, alpha=0.2):\n        \"\"\"This method performs mixup. If runs = 1 it just does 1 mixup with whole batch, any n of runs\n        creates many mixup matches.\n        :type x: Numpy array\n        :param x: Data array\n        :type y: Numpy array\n        :param y: (optional) labels\n        :type alpha: float\n        :param alpha: alpha\n        :rtype: tuple\n        :return: Returns mixed inputs, pairs of targets, and lambda\n        \"\"\"\n        if self.runs is None:\n            self.runs = 1\n        output_x = []\n        output_y = []\n        batch_size = x.shape[0]\n        for i in range(self.runs):\n            lam_vector = np.random.beta(alpha, alpha, batch_size)\n            index = np.random.permutation(batch_size)\n            mixed_x = (x.T * lam_vector).T + (x[index, :].T * (1.0 - lam_vector)).T\n            output_x.append(mixed_x)\n            if y is None:\n                return np.concatenate(output_x, axis=0)\n            mixed_y = (y.T * lam_vector).T + (y[index].T * (1.0 - lam_vector)).T\n            output_y.append(mixed_y)\n        return np.concatenate(output_x, axis=0), np.concatenate(output_y, axis=0)\n\n    def flow(self, data, labels=None, batch_size=32, shuffle=True, runs=1):\n        \"\"\"This function implements the batch iterator and specifically calls mixup\n        :param data: Input data. Numpy ndarray or list of lists.\n        :param labels: Labels. Numpy ndarray or list of lists.\n        :param batch_size: Int (default: 32).\n        :param shuffle: Boolean (default: True).\n        :param runs: Int (default: 1). Number of augmentations\n        :rtype:   array or tuple\n        :return:  array or tuple of arrays (X_data array, labels array).\"\"\"\n\n        self.validate(data=data, labels=labels, batch_size=batch_size, shuffle=shuffle, runs=runs)\n\n        self.runs = runs\n\n        num_batches_per_epoch = int((len(data) - 1) \/ batch_size) + 1\n\n        def data_generator():\n            data_size = len(data)\n            while True:\n                # Shuffle the data at each epoch\n                if shuffle:\n                    shuffle_indices = np.random.permutation(np.arange(data_size))\n                    shuffled_data = data[shuffle_indices]\n                    if labels is not None:\n                        shuffled_labels = labels[shuffle_indices]\n                else:\n                    shuffled_data = data\n                    if labels is not None:\n                        shuffled_labels = labels\n                for batch_num in range(num_batches_per_epoch):\n                    start_index = batch_num * batch_size\n                    end_index = min((batch_num + 1) * batch_size, data_size)\n                    X = shuffled_data[start_index: end_index]\n                    if labels is None:\n                        X = self.mixup_data(X, y=None)\n                        yield X\n                    else:\n                        y = shuffled_labels[start_index: end_index]\n                        X, y = self.mixup_data(X, y)\n                        yield X, y\n\n        return data_generator(), num_batches_per_epoch","ec801ff6":"#HYPERPARAMETERS\nbatch_size = 32 #default\nruns = 1 #default\nepochs = 8","1e2b6e69":"y_train = tf.keras.utils.to_categorical(y_train)\ny_test = tf.keras.utils.to_categorical(y_test)","6ddf8f1f":"mixup = MIXUP()\ngenerator, step = mixup.flow(x_train, y_train, batch_size=batch_size, runs=runs)","93cbf440":"def BasicModel():\n    \n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.Input(shape=(768,)))\n    model.add(tf.keras.layers.Dense(50, activation='relu'))\n    model.add(tf.keras.layers.Dense(50, activation='relu'))\n    model.add(tf.keras.layers.Dense(7, activation='softmax'))\n\n    model.compile(\n        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n        )\n    return model","12d5912c":"model = BasicModel()\nhistory_org = model.fit(x_train, y_train,\n                        validation_data = (x_test, y_test),\n                        batch_size = batch_size,\n                        verbose=1,\n                        epochs=epochs)","ea0d5227":"pd.DataFrame(history_org.history)[['loss','val_loss']].plot(title=\"Without mixup\")","4c306edc":"model = BasicModel()\nhistory_mixup = model.fit(generator, steps_per_epoch=step,\n                          epochs=epochs,\n                          validation_data=(x_test, y_test))","5c435c59":"pd.DataFrame(history_mixup.history)[['loss','val_loss']].plot(title=\"With mixup\")","3d989f43":"def plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n\n    cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=20)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90, fontsize=16)\n    plt.yticks(tick_marks, classes, fontsize=16)\n\n    fmt = '.2f'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label', fontsize=12)\n    plt.xlabel('Predicted label', fontsize=12)","b7e6819a":"predicted_documents = model.predict(x_test, batch_size=batch_size)\ny_pred =  np.argmax(predicted_documents, axis=1)\n#y_pred = y_pred[:,1].reshape(686).flatten()  # use for pure cnn output","dd3a618b":"y_test = np.argmax(y_test, axis=1)","c03f4494":"import itertools\nprint('Confusion Matrix')\nprint(confusion_matrix(y_test, y_pred))","b2eeeb1a":"print('Precision: %.4f' % precision_score(y_test, y_pred, average=\"weighted\"))\nprint('Recall: %.4f' % recall_score(y_test, y_pred, average=\"weighted\"))\nprint('Accuracy: %.4f' % accuracy_score(y_test, y_pred))\nprint('F1 Score: %.4f' % f1_score(y_test, y_pred, average=\"weighted\"))","1d5aec54":"df_c = pd.DataFrame(y_test)\nvalue_counts = df_c.value_counts()\ndictionary = dict()\nfor (i,), j in value_counts.items():\n    dictionary[i] = j\n\ndictionary","fe9c0277":"target_names = ['happiness',\n'hate',     \n'love',\n'neutral',\n'other',\n'sadness',\n'worry']\nprint(classification_report(y_test, y_pred, target_names=target_names))","9ed2be58":"# Traditions...","c9356303":"## Confusion Matrix","a0e764f7":"## Precision, Recall, Accuracy, F1","f1c90919":"# Introduction\n\nIn this notebook, we will be investigating MIXUP Text Data Augmentation method by using simpletransformers via BERT.","f8415cab":"#### So we undertand from basic exploratory analysis, we have very imbalanced dataset. We can easily guess that the score will be really bad because the classes like boredom, anger and enthusiasm do not let the model learn properly. \n\n#### Our model will be prone to memorize neutral and worry because of their amount of samples. \n**Recap** <br\/>\n'empty': 827 <br\/>\n'enthusiasm': 759 <br\/>\n'boredom': 179 <br\/>\n'anger': 110 <br\/>\n'fun': 1776 <br\/>\n'surprise': 2187 <br\/>\n'relief' : 1526 <br\/>\n\n#### As a last pre-processing step we will remove 7 classes which are insufficient and use as 'other'. (boredom, anger, enthusiasm, fun, surprise, relief and empty)\n\n#### We will continue with 7 classes because we will use removed ones as 'other' label.\n\n#### *The reason we will remove specific classes.* <br \/>\n#### Before work, I observed that some of classes have 0 scores due to lack of data amount and sentiment ambiguity. <br \/>\n![image.png](attachment:1c9929de-0e20-4116-93d1-6b5a5490984a.png)\n","6c9af266":"# Vectorization\nWe use this part to vectorize the text data. \n\nAs mentioned in [simpletransformers](https:\/\/simpletransformers.ai\/docs\/text-rep-model\/):\n> \"The RepresentationModel class is used for generating (contextual) word or sentence embeddings from a list of text sentences, You can then feed these vectors to any model or downstream task.\"\n\nAfter we vectorize the data, it will be used for data augmentation or any other task phases in other notebooks.","392f2989":"## Classification Report","2bd57aee":"# Understand Loss Graphs\n\n#### As we can see above, mixup is doing great as regularizer. The loss curves are getting close for train and val sets.","f0139e9d":"# LabelEncoding to give sentiments a numerical corresponds","7a0b89af":"We used ***train.sentiment*** to look sentiments in graph, however, we will use corresponding numbers which we will create by labelencoder under ***train.label*** to use in training phase.","a8cfa144":"# Visualization (EDA)\n[referenced notebook for some EDA stuff](https:\/\/www.kaggle.com\/pashupatigupta\/starter-notebook-a-to-z-emotion-detection)","01c07c96":"# Model","90220349":"# About Data\n\nemotion-detection-from-text dataset has 40.000 samples with 13 classes. <br\/>\n\n* **sentiment**: includes 13 classes as follows, *class name : amount<br\/>*\n('neutral': 8638,<br\/>\n 'worry': 8459,<br\/>\n 'happiness': 5209,<br\/>\n 'sadness': 5165,<br\/>\n 'love': 3842,<br\/>\n 'surprise': 2187,<br\/>\n 'fun': 1776,<br\/>\n 'relief': 1526,<br\/>\n 'hate': 1323,<br\/>\n 'empty': 827,<br\/>\n 'enthusiasm': 759,<br\/>\n 'boredom': 179,<br\/>\n 'anger': 110 )<br\/>\n <br\/>\n* **content**: includes text we will classify<br\/>\n* **tweet_id**: unique id","b80bcfd4":"# Loading data & Preprocessing","1e796281":"# Import Necessary Packages","f0ef43e8":"# Initialize MIXUP","f446a58c":"# TEXT AUGMENTATION BY MIXUP ALGORITHM\nMixup is a neural network training method that generates new samples by linear interpolation of multiple samples and their labels. The mixup training method has better generalization ability than the traditional Empirical Risk Minimization method (ERM). [Understanding Mixup Training Methods](https:\/\/www.researchgate.net\/publication\/328009804_Understanding_Mixup_Training_Methods) <br \/>\nOriginal paper: [mixup: BEYOND EMPIRICAL RISK MINIMIZATION](https:\/\/arxiv.org\/pdf\/1710.09412.pdf) <br \/>\nGithub page where base Mixup function is taken: [textaugment](https:\/\/github.com\/dsfsi\/textaugment) <br \/> \n> @inproceedings{marivate2020improving,<br \/> \n  title={Improving short text classification through global augmentation methods},<br \/> \n  author={Marivate, Vukosi and Sefara, Tshephisho},<br \/> \n  booktitle={International Cross-Domain Conference for Machine Learning and Knowledge Extraction},<br \/> \n  pages={385--399},<br \/> \n  year={2020},<br \/> \n  organization={Springer}\n}\n\nNice paper for text augmentation with Mixup: [Augmenting Data with Mixup for Sentence Classification: An Empirical Study](https:\/\/arxiv.org\/pdf\/1905.08941.pdf) <br \/>\nFrom paper: \n> Mixup, a recent proposed data augmentation method through linearly interpolating inputs and modeling targets of random samples, has demonstrated its capability of significantly improving the predictive accuracy of the state-of-the-art networks for image classification. However, how this technique can be applied to and what is its effectiveness on natural language processing (NLP) tasks have not been investigated. In this paper, we propose two strategies for the adaption of Mixup on sentence classification: one performs interpolation on word embeddings and another on sentence embeddings. We conduct experiments to evaluate our methods using several benchmark datasets. Our studies show that such interpolation strategies serve as an effective, domain independent data augmentation approach for sentence classification, and can result in significant accuracy improvement for both CNN and LSTM models.","d6b4338c":"# Plotting the Results of MIXUP"}}