{"cell_type":{"38cb6e4f":"code","ce806711":"code","3e2fa368":"code","4648a359":"code","fc50471d":"code","73bd89d0":"code","4ccd61ab":"code","01b6ce9b":"code","9efdcc21":"code","289b4a6e":"code","bb0f38ad":"code","91837064":"code","6b22b1de":"code","4fe8837c":"code","fb1ff7a3":"code","88c9b2b8":"code","b9915f8e":"code","d22e7612":"code","b17e605c":"code","ce6d559b":"code","0167aac2":"markdown","9d4d3a2a":"markdown","6ee1d3a2":"markdown","10dd8308":"markdown","0192c525":"markdown","83330783":"markdown","b8797541":"markdown","75916b87":"markdown","53d169ff":"markdown","26af27d7":"markdown","da04a8aa":"markdown","8f6e5094":"markdown","5a4e96b3":"markdown","4de14b59":"markdown","b4999a78":"markdown","050a17f7":"markdown","299fb537":"markdown","2fb67592":"markdown","0165b9a2":"markdown","a570ef9f":"markdown","db65168b":"markdown","0921a44f":"markdown","38e37a9c":"markdown","a294f646":"markdown"},"source":{"38cb6e4f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# import libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandas.tools import plotting\nfrom scipy import stats\nplt.style.use(\"ggplot\")\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom scipy import stats\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ce806711":"# read data as pandas data frame\ndata = pd.read_csv(\"..\/input\/data.csv\")\ndata = data.drop(['Unnamed: 32','id'],axis = 1)","3e2fa368":"data.head()","4648a359":"# quick look to data\ndata.head()\ndata.shape # (569, 31)\ndata.columns ","fc50471d":"m = plt.hist(data[data[\"diagnosis\"] == \"M\"].radius_mean,bins=30,fc = (1,0,0,0.5),label = \"Malignant\")\nb = plt.hist(data[data[\"diagnosis\"] == \"B\"].radius_mean,bins=30,fc = (0,1,0,0.5),label = \"Bening\")\nplt.legend()\nplt.xlabel(\"Radius Mean Values\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of Radius Mean for Bening and Malignant Tumors\")\nplt.show()\nfrequent_malignant_radius_mean = m[0].max()\nindex_frequent_malignant_radius_mean = list(m[0]).index(frequent_malignant_radius_mean)\nmost_frequent_malignant_radius_mean = m[1][index_frequent_malignant_radius_mean]\nprint(\"Most frequent malignant radius mean is: \",most_frequent_malignant_radius_mean)","73bd89d0":"data_bening = data[data[\"diagnosis\"] == \"B\"]\ndata_malignant = data[data[\"diagnosis\"] == \"M\"]\ndesc = data_bening.radius_mean.describe()\nQ1 = desc[4]\nQ3 = desc[6]\nIQR = Q3-Q1\nlower_bound = Q1 - 1.5*IQR\nupper_bound = Q3 + 1.5*IQR\nprint(\"Anything outside this range is an outlier: (\", lower_bound ,\",\", upper_bound,\")\")\ndata_bening[data_bening.radius_mean < lower_bound].radius_mean\nprint(\"Outliers: \",data_bening[(data_bening.radius_mean < lower_bound) | (data_bening.radius_mean > upper_bound)].radius_mean.values)","4ccd61ab":"melted_data = pd.melt(data,id_vars = \"diagnosis\",value_vars = ['radius_mean', 'texture_mean'])\nplt.figure(figsize = (15,10))\nsns.boxplot(x = \"variable\", y = \"value\", hue=\"diagnosis\",data= melted_data)\nplt.show()","01b6ce9b":"print(\"mean: \",data_bening.radius_mean.mean())\nprint(\"variance: \",data_bening.radius_mean.var())\nprint(\"standart deviation (std): \",data_bening.radius_mean.std())\nprint(\"describe method: \",data_bening.radius_mean.describe())","9efdcc21":"plt.hist(data_bening.radius_mean,bins=50,fc=(0,1,0,0.5),label='Bening',normed = True,cumulative = True)\nsorted_data = np.sort(data_bening.radius_mean)\ny = np.arange(len(sorted_data))\/float(len(sorted_data)-1)\nplt.plot(sorted_data,y,color='red')\nplt.title('CDF of bening tumor radius mean')\nplt.show()","289b4a6e":"mean_diff = data_malignant.radius_mean.mean() - data_bening.radius_mean.mean()\nvar_bening = data_bening.radius_mean.var()\nvar_malignant = data_malignant.radius_mean.var()\nvar_pooled = (len(data_bening)*var_bening +len(data_malignant)*var_malignant ) \/ float(len(data_bening)+ len(data_malignant))\neffect_size = mean_diff\/np.sqrt(var_pooled)\nprint(\"Effect size: \",effect_size)","bb0f38ad":"plt.figure(figsize = (15,10))\nsns.jointplot(data.radius_mean,data.area_mean,kind=\"regg\")\nplt.show()","91837064":"# Also we can look relationship between more than 2 distribution\nsns.set(style = \"white\")\ndf = data.loc[:,[\"radius_mean\",\"area_mean\",\"fractal_dimension_se\"]]\ng = sns.PairGrid(df,diag_sharey = False,)\ng.map_lower(sns.kdeplot,cmap=\"Blues_d\")\ng.map_upper(plt.scatter)\ng.map_diag(sns.kdeplot,lw =3)\nplt.show()","6b22b1de":"f,ax=plt.subplots(figsize = (18,18))\nsns.heatmap(data.corr(),annot= True,linewidths=0.5,fmt = \".1f\",ax=ax)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title('Correlation Map')\nplt.savefig('graph.png')\nplt.show()","4fe8837c":"np.cov(data.radius_mean,data.area_mean)\nprint(\"Covariance between radius mean and area mean: \",data.radius_mean.cov(data.area_mean))\nprint(\"Covariance between radius mean and fractal dimension se: \",data.radius_mean.cov(data.fractal_dimension_se))","fb1ff7a3":"p1 = data.loc[:,[\"area_mean\",\"radius_mean\"]].corr(method= \"pearson\")\np2 = data.radius_mean.cov(data.area_mean)\/(data.radius_mean.std()*data.area_mean.std())\nprint('Pearson correlation: ')\nprint(p1)\nprint('Pearson correlation: ',p2)","88c9b2b8":"ranked_data = data.rank()\nspearman_corr = ranked_data.loc[:,[\"area_mean\",\"radius_mean\"]].corr(method= \"pearson\")\nprint(\"Spearman's correlation: \")\nprint(spearman_corr)","b9915f8e":"salary = [1,4,3,2,5,4,2,3,1,500]\nprint(\"Mean of salary: \",np.mean(salary))","d22e7612":"print(\"Median of salary: \",np.median(salary))","b17e605c":"statistic, p_value = stats.ttest_rel(data.radius_mean,data.area_mean)\nprint('p-value: ',p_value)","ce6d559b":"# parameters of normal distribution\nmu, sigma = 110, 20  # mean and standard deviation\ns = np.random.normal(mu, sigma, 100000)\nprint(\"mean: \", np.mean(s))\nprint(\"standart deviation: \", np.std(s))\n# visualize with histogram\nplt.figure(figsize = (10,7))\nplt.hist(s, 100, normed=False)\nplt.ylabel(\"frequency\")\nplt.xlabel(\"IQ\")\nplt.title(\"Histogram of IQ\")\nplt.show()","0167aac2":"* P values is almost zero so we can reject null hypothesis.","9d4d3a2a":"# Introduction\nBasic statistic for beginners\n* [Histogram](#1)\n* [Outliers](#2)\n* [Box Plot ](#3)\n* [Summary Statistics](#4)\n* [CDF](#5)\n* [Effect size](#6)\n* [Relationship Between Variables](#7)\n* [Correlation](#8)\n* [Covariance](#9)\n* [Pearson Correlation](#10)\n* [Spearman's Rank Correlation](#11)\n* [Mean VS Median](#12)\n* [Hypothesis Testing](#13)\n* [Normal(Gaussian) Distribution and z-score](#14) \n","6ee1d3a2":"<a id=\"4\"><\/a> <br>\n## Summary Statistics\n* Mean\n* Variance: spread of distribution\n* Standart deviation square root of variance\n* Lets look at summary statistics of bening tumor radiance mean","10dd8308":"* Now median of the salary is 3 and it is less than 5 and employees will take raise in their sallaries and they are happy and this situation is fair :)","0192c525":"<a id=\"14\"><\/a> <br>\n## Normal(Gaussian) Distribution and z-score\n* Also called bell shaped distribution\n* Instead of making formal definition of gaussian distribution, I want to explain it with an example.\n* The classic example is gaussian is IQ score.\n    * In the world lets say average IQ is 110.\n    * There are few people that are super intelligent and their IQs are higher than 110. It can be 140 or 150 but it is rare.\n    * Also there are few people that have low intelligent and their IQ is lower than 110. It can be 40 or 50 but it is rare.\n    * From these information we can say that mean of IQ is 110. And lets say standart deviation is 20.\n    * Mean and standart deviation is parameters of normal distribution.\n    * Lets create 100000 sample and visualize it with histogram.\n","83330783":"<a id=\"1\"><\/a> <br>\n## Histogram\n* How many times each value appears in dataset. This description is called the distribution of variable\n* Most common way to represent distribution of varible is histogram that is graph which shows frequency of each value.\n* Frequency = number of times each value appears\n* Example: [1,1,1,1,2,2,2]. Frequency of 1 is four and frequency of 2 is three.","b8797541":"<a id=\"13\"><\/a> <br>\n## Hypothesis Testing\n* Classical Hypothesis Testing\n* We want to answer this question: \"given a sample and a apparent effecti what is the probability of seeing such an effect by chance\"\n* The first step is to quantify the size of the apparent effect by choosing a test statistic. Natural choice for the test statistic is the difference in means between two groups.\n* The second step is to define null hypothesis that is model of the system based on the assumption that the apparent effect is not real. A null hypothesis is a type of hypothesis used in statistics that proposes that no statistical significance exists in a set of given observations. The null hypothesis is a hypothesis which people tries to disprove it. Alternative hypothesis is a hypothesis which people want to tries to prove it. \n* Third step is compute p-value that is probablity of seeing the apparent effect if the null hypothesis is true. Suppose we have null hypothesis test. Then we calculate p value. If p value is less than or equal to a threshold, we reject null hypothesis.\n* If the p-value is low, the effect is said to be statistacally significant that means that it is unlikely to have occured by chance. Therefore we can say that the effect is more likely to appear in the larger population.\n* Lets have an example. Null hypothesis: world is flatten. Alternative hypothesis: world is round. Several scientists set out to disprove the null hypothesis. This eventually led to the refection of the null hypothesis and acceptance of the alternative hypothesis.\n* Other example. \"this effect is real\" this is null hypothesis. Based on that assumption we compute the probability of the apparent effect. That is the p-value. If p-value is low, we conclude that null hypothesis is unlikely to be true.\n* Now lets make our example:\n    * I want to learn that are radius mean and area mean related with each other? My null hypothesis is that \"relationship between radius mean and area mean is zero in tumor population'.\n    * Now we need to refute this null hypothesis in order to demonstrate that radius mean and area mean are related. (actually we know it from our previous experiences)\n    * lets find p-value (probability value) ","75916b87":"<a id=\"8\"><\/a> <br>\n## Correlation\n* Strength of the relationship between two variables\n* Lets look at correlation between all features.","53d169ff":"* Mean of salary is 52.5 so the boss thinks that oooo I gave a lot of salary to my employees. And do not makes raise in their salaries. However as you know this is not fair and 500(salary) is outlier for this salary list.\n* Median avoids outliers","26af27d7":"* Lets look at other conclusions\n* From this graph you can see that radius mean of malignant tumors are bigger than radius mean of bening tumors mostly.\n* The bening distribution (green in graph) is approcimately bell-shaped that is shape of normal distribution (gaussian distribution)\n* Also you can find result like that most frequent malignant radius mean is  ","da04a8aa":"<a id=\"7\"><\/a> <br>\n## Relationship Between Variables\n* We can say that two variables are related with each other, if one of them gives information about others\n* For example, price and distance. If you go long distance with taxi you will pay more. There fore we can say that price and distance are positively related with each other.\n* Scatter Plot\n* Simplest way to check relationship between two variables\n* Lets look at relationship between radius mean and area mean\n* In scatter plot you can see that when radius mean increases, area mean also increases. Therefore, they are positively correlated with each other.\n* There is no correlation between area mean and fractal dimension se. Because when area mean changes, fractal dimension se is not affected by chance of area mean","8f6e5094":"<a id=\"9\"><\/a> <br>\n## Covariance\n* Covariance is measure of the tendency of two variables to vary together\n* So covariance is maximized if two vectors are identical\n* Covariance is zero if they are orthogonal.\n* Covariance is negative if they point in opposite direction\n* Lets look at covariance between radius mean and area mean. Then look at radius mean and fractal dimension se\n","5a4e96b3":"<a id=\"12\"><\/a> <br>\n## Mean VS Median\n* Sometimes instead of mean we need to use median. I am going to explain why we need to use median with an example\n* Lets think that there are 10 people who work in a company. Boss of the company will make raise in their salary if their mean of salary is smaller than 5\n","4de14b59":"* As it can be seen from histogram most of the people are cumulated near to 110 that is mean of our normal distribution\n* However what is the \"most\" I mentioned at previous sentence? What if I want to know what percentage of people should have an IQ score between 80 and 140?\n* We will use z-score the answer this question. \n      * z = (x - mean)\/std \n      * z1 = (80-110)\/20 = -1.5\n      * z2 = (140-110)\/20 = 1.5\n      * Distance between mean and 80 is 1.5std and distance between mean and 140 is 1.5std.\n      * If you look at z table, you will see that 1.5std correspond to 0.4332\n <a href=\"https:\/\/ibb.co\/hys6OT\"><img src=\"https:\/\/preview.ibb.co\/fYzWq8\/123.png\" alt=\"123\" border=\"0\"><\/a>\n      * Lets calculate it with 2 because 1 from 80 to mean and other from mean to 140\n      * 0.4332 * 2 = 0.8664\n      * 86.64 % of people has an IQ between 80 and 140.\n  <a href=\"https:\/\/ibb.co\/fhc6OT\"><img src=\"https:\/\/preview.ibb.co\/bUi2xo\/hist.png\" alt=\"hist\" border=\"0\"><\/a>","b4999a78":"<a id=\"5\"><\/a> <br>\n## CDF\n* Cumulative distribution function is the probability that the variable takes a value less than or equal to x. P(X <= x)\n* Lets explain in cdf graph of bening radiues mean\n* in graph, what is P(12 < X)? The answer is 0.5. The probability that the variable takes a values less than or equal to 12(radius mean) is 0.5.\n* You can plot cdf with two different method","050a17f7":"<a id=\"6\"><\/a> <br>\n## Effect size\n* One of the summary statistics.\n* It describes size of an effect. It is simple way of quantifying the difference between two groups.\n* In an other saying, effect size emphasises the size of the difference\n* Use cohen effect size\n* Cohen suggest that if d(effect size)= 0.2, it is small effect size, d = 0.5 medium effect size, d = 0.8 large effect size.\n* lets compare size of the effect between bening radius mean and malignant radius mean\n* Effect size is 2.2 that is too big and says that two groups are different from each other as we expect. Because our groups are bening radius mean and malignant radius mean that are different from each other","299fb537":"<a id=\"3\"><\/a> <br>\n## Box Plot \n* You can see outliers also from box plots\n* We found 3 outlier in bening radius mean and in box plot there are 3 outlier.","2fb67592":"# Conclusion\n* If you have any question or advise, I will be very happy to hear them.","0165b9a2":"<a id=\"10\"><\/a> <br>\n## Pearson Correlation\n* Division of covariance by standart deviation of variables\n* Lets look at pearson correlation between radius mean and area mean\n* First lets use .corr() method that we used actually at correlation part. In correlation part we actually used pearson correlation :) \n* p1 and p2 is the same. In p1 we use corr() method, in p2 we apply definition of pearson correlation (cov(A,B)\/(std(A)*std(B)))\n* As we expect pearson correlation between area_mean and area_mean is 1 that means that they are same distribution\n* Also pearson correlation between area_mean and radius_mean is 0.98 that means that they are positively correlated with each other and relationship between of the is very high.\n* To be more clear what we did at correlation part and pearson correlation part is same.\n","a570ef9f":"<a id=\"2\"><\/a> <br>\n## Outliers\n* While looking histogram as yok can see there are rare values in bening distribution (green in graph)\n* There values can be errors or rare events.\n* These errors and rare events can be called outliers.\n* Calculating outliers: \n    * first we need to calculate first quartile (Q1)(25%)\n    * then find IQR(inter quartile range) = Q3-Q1\n    * finally compute Q1 - 1.5*IQR and Q3 + 1.5*IQR\n    * Anything outside this range is an outlier\n    * lets write the code for bening tumor distribution for feature radius mean","db65168b":"* Spearman's correlation is little higher than pearson correlation\n    * If relationship between distributions are non linear, spearman's correlation tends to better estimate the strength of relationship\n    * Pearson correlation can be affected by outliers. Spearman's correlation is more robust. ","0921a44f":"* Huge matrix that includes a lot of numbers\n* The range of this numbers are -1 to 1. \n* Meaning of 1 is two variable are positively correlated with each other like radius mean and area mean\n* Meaning of zero is there is no correlation between variables like radius mean and fractal dimension se\n* Meaning of -1 is two variables are negatively correlated with each other like radius mean and fractal dimension mean.Actually correlation between of them is not -1, it is -0.3 but the idea is that if sign of correlation is negative that means that there is negative correlation.","38e37a9c":"* What percentage of people should have an IQ score less than 80?\n* z = (110-80)\/20 = 1.5\n* Lets look at table of z score 0.4332. 43.32% of people has an IQ between 80 and mean(110).\n* If we subtract from 50% to 43.32%, we ca n find percentage of people have an IQ score less than 80.\n* 50-43.32 = 6.68. As a result, 6.68% of people have an IQ score less than 80.","a294f646":"<a id=\"11\"><\/a> <br>\n## Spearman's Rank Correlation\n* Pearson correlation works well if the relationship between variables are linear and variables are roughly normal. But it is not robust, if there are outliers\n* To compute spearman's correlation we need to compute rank of each value\n"}}