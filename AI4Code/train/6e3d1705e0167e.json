{"cell_type":{"d67fc99c":"code","e0728905":"code","57cbcf21":"code","660b98d9":"code","52f3a92e":"code","71802ab2":"code","4589d649":"code","700e8ba1":"code","71025ea0":"code","a1c65b0e":"markdown","5a33a671":"markdown","06163062":"markdown","8e8d71e5":"markdown","7aefb50f":"markdown","649b5263":"markdown","434b3c83":"markdown","3ddc9b96":"markdown","55d8c1f8":"markdown"},"source":{"d67fc99c":"%%writefile agent_copy_strat.py\n\nimport random\nrandom.seed(27)\n\nclass Agent:\n    \n    def __init__(self):\n        self.my_moves = []\n        self.opponent_moves = []\n        self.rewards = []\n    \n    def random_move(self, obs, config):\n        '''Make a random move\n        '''\n        move = random.choice(range(config.banditCount))\n        return move\n        \n    def get_last_move(self, moves, obs, config):\n        '''Get the last move of given list of moves (of an agent).\n        '''\n        try:\n            move = moves[-1]\n        except IndexError:\n            move = self.random_move(obs,config)\n        return move\n        \n    def copy_move(self, obs, config):\n        '''Simply copy last move of opponent.\n        '''\n        return self.get_last_move(self.opponent_moves, obs, config)\n    \n    # TODO: Record other information besides moves\n    def record_history(self, obs, conf):\n        '''Record history like list of moves (both agents), rewards, etc.\n        '''\n        # If first turn, don't record anything\n        if obs.step > 0:\n            # Record opponenet last move\n            # TODO: consider if more than two agents\n            my_index = (obs.agentIndex) % 2\n            opponent_index = (obs.agentIndex + 1) % 2\n            my_last_move = obs.lastActions[my_index]        \n            opponent_last_move = obs.lastActions[opponent_index]\n            self.my_moves.append(my_last_move)\n            self.opponent_moves.append(opponent_last_move)\n            # TODO: Record rewards\n        return\n    \n    def use_strategy(self, obs, config, strat='random'):\n        '''Return which bandit to choose given a strategy, observations, & \n        environment configuration.\n        '''\n        if strat.lower() == 'random':\n            move = self.random_move(obs, config)\n        elif strat.lower() == 'copy_move':\n            move = self.copy_move(obs, config)\n        # Default to choosing first lever if not known strategy\n        else:\n            move = 0\n        return move\n    \n\nmy_agent = Agent()\n\ndef agent_run(observation, config):\n    # Record history\n    my_agent.record_history(observation, config)\n    # Simply copy moves\n    bandit = my_agent.use_strategy(observation, config, strat='copy_move')\n    return bandit ","e0728905":"from agent_copy_strat import Agent \n\nrand_agent = Agent()\ndef rand_run(obs,conf):\n    bandit = rand_agent.use_strategy(obs,conf,strat='random')\n    return bandit","57cbcf21":"!pip install kaggle-environments --upgrade -q","660b98d9":"from kaggle_environments import make","52f3a92e":"env = make(\"mab\", debug=True)\n\nenv.run(['agent_copy_strat.py', rand_run])\nenv.render(mode=\"ipython\", width=800, height=300)","71802ab2":"# Import module we'll need to import our custom module\nfrom shutil import copyfile\n\n# Copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src=\"..\/input\/visualizing-reward-outcomes\/SimulationExplorer.py\", \n         dst= \"..\/working\/SimulationExplorer.py\")\n\n# Import SimulationExplorer functions\nimport SimulationExplorer as Explorer","4589d649":"sims = {'test':env}\ntest = Explorer.SimViz(sims)\ntest.plot_total_reward()\n\nfor n,env in sims.items():\n    print(n,env.toJSON().get('rewards'))","700e8ba1":"def print_trial_results(trial, env, start_time):\n    '''Helper function to see how agents compare\n    '''\n    rewards = env.toJSON().get('rewards')\n    print(f'Trial # {trial}:')\n    print(f'\\t{time.time()-start_time:.4} seconds')\n    print(f'\\t{\"W\" if rewards[0]>rewards[1] else \"L\"} \u2192 {rewards}')\n    diff = rewards[0]-rewards[-1]\n    print(f'\\tDifference: {diff}')\n    \n    return diff","71025ea0":"import time\n\n\nsims = {}\ndiffs = []\n\nfor trial in range(20):\n    start_time = time.time()\n    myagent = Agent()\n    \n    env = make(\"mab\", debug=True)\n    env.run(['agent_copy_strat.py', rand_run])\n\n    #\n    name = f'Trial#{trial}'\n    sims[name] = env\n    \n    #\n    diffs.append(print_trial_results(trial, env, start_time))\n\n\ntest = Explorer.SimViz(sims)\ntest.plot_total_reward()\n\nprint(f'Win Percentage: {sum(1 for x in diffs if x>0)\/len(diffs):.2}')","a1c65b0e":"### Multiple runs","5a33a671":"### Checkout the rewards over time","06163062":"Alright, let's see how this does a few many runs. Should be able to sneak out a few lucky wins","8e8d71e5":"# Monkey See, Monkey Do: Copy Opponent Strategy","7aefb50f":"Let's create a random agent to run against","649b5263":"# Agent Runs","434b3c83":"## Evaluation","3ddc9b96":"Obviously not a great strategy by itself but how often does it actually win due to random chance?","55d8c1f8":"## Agent Setup"}}