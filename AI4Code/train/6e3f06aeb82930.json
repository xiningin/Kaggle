{"cell_type":{"d8ffa367":"code","2e812a31":"code","2126d80b":"code","00111afc":"code","2fcb720e":"code","a5b286a2":"code","c4fb22dd":"code","01a85866":"code","94640da3":"code","b590e997":"code","262be95b":"code","5cc236c2":"code","d09ea658":"code","46329e3e":"code","f523899a":"code","80a5dd3e":"code","23121b8c":"code","a959b549":"code","c5e21c00":"code","e8b7997a":"code","095cd806":"code","7df5fd53":"code","352f86b4":"code","f00172f7":"code","3d2b8188":"code","d03f6853":"code","1dd249b8":"code","8d8dadc6":"code","b0b8ce5a":"code","b174a788":"code","b4cbfd18":"code","018ce385":"code","adefdcc6":"code","f1a49b8d":"code","3b3139bd":"code","42466ef1":"code","7712ba69":"code","53ce9235":"code","5c182d65":"markdown","ec6baa5d":"markdown","4f68697e":"markdown","6ccc1de6":"markdown","809a2d66":"markdown","88ccdea2":"markdown","24356811":"markdown","878b4b47":"markdown","3fa466a7":"markdown","d9d89383":"markdown","13df48b3":"markdown","b4746421":"markdown","64639a54":"markdown","9b912368":"markdown","cb82023b":"markdown","9fae3aaf":"markdown","5c270746":"markdown","15b4091f":"markdown","af8804f8":"markdown","663ecaee":"markdown","0f15c667":"markdown","8afd926f":"markdown","0114d14a":"markdown","abac5508":"markdown","7787c271":"markdown","5d2ce416":"markdown","2a8925b8":"markdown","8c4b24b6":"markdown","de2ae7d8":"markdown","89564e04":"markdown","4053e9f0":"markdown"},"source":{"d8ffa367":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\npd.set_option('display.max_columns', None)\nplt.style.use('ggplot')","2e812a31":"glass = pd.read_csv(\"..\/input\/glass\/glass.csv\")\nglass.head()","2126d80b":"# the data columns\ncols = ['RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe']\ntarget = [\"Type\"]","00111afc":"glass.info()","2fcb720e":"glass.describe()","a5b286a2":"sns.countplot(glass[\"Type\"])\nplt.show()","c4fb22dd":"fig,ax = plt.subplots(3,3, figsize=(16, 12))\nax = ax.flatten()\ni = 0\nfor col in cols:\n    skew = glass[col].skew()\n    sns.distplot(glass[col], ax = ax[i], fit= stats.norm, kde=False, label='Skew = %.3f' %(skew))\n    ax[i].legend(loc='best')\n    i += 1\nplt.show()","01a85866":"glass.iloc[:,:-1].boxplot(figsize=(12,6))\nplt.show()","94640da3":"fig,ax = plt.subplots(3,3, figsize=(16, 12))\nax = ax.flatten()\ni = 0\nfor col in cols:\n    sns.boxplot(\"Type\", col, ax = ax[i], data=glass)\n    ax[i].legend([col], loc='best')\n    i += 1\nplt.tight_layout()\nplt.show()","b590e997":"pd.plotting.scatter_matrix(glass.iloc[:,:-1], c=glass.iloc[:,-1], figsize=(20, 20), marker='o')\nplt.legend(glass[\"Type\"].unique())\nplt.show()","262be95b":"sns.pairplot(glass, hue='Type', diag_kind='hist')\nplt.show()","5cc236c2":"plt.figure(figsize=(8,6))\ncorr = glass.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, annot=True, fmt= '.2f', cmap='YlGnBu', mask=mask)\nplt.show()","d09ea658":"glass.groupby(\"Type\")[\"Ca\"].mean()","46329e3e":"glass.groupby(\"Type\")[\"K\"].mean()","f523899a":"glass[\"Ca_morethan9\"] = np.where(glass[\"Ca\"]>9, 1, 0)\nglass[\"K_morethandot7\"] = np.where(glass[\"K\"]>0.7, 1, 0)\nglass[\"K_lessthandot4\"] = np.where(glass[\"K\"]<0.4, 1, 0)","80a5dd3e":"cols.append(\"Ca_morethan9\")\ncols.append(\"K_morethandot7\")\ncols.append(\"K_lessthandot4\")","23121b8c":"import statsmodels.api as sm\nimport statsmodels.stats as sms\n\nfor col in cols:\n    data = sm.formula.ols(col+\"~ Type\", data=glass).fit()\n    pval = sms.anova.anova_lm(data)[\"PR(>F)\"][0]\n    print(f\"Pval for {col}: {pval}\")","a959b549":"seed = 1\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import classification_report, roc_auc_score, roc_curve\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier,\\\n                            BaggingClassifier,VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline, Pipeline","c5e21c00":"# split the data into train and test\ndef split_data(X, Y, seed=1, train_size=0.7):\n    xtrain, xtest, ytrain, ytest = train_test_split(X, Y, train_size=train_size, random_state = seed, stratify=Y)\n    xtrain, xtest = preprocess(xtrain, xtest)\n    return (xtrain, xtest, ytrain, ytest)\n\n# preprocess the data for training\ndef preprocess(x1, x2=None):\n    sc = StandardScaler()\n    x1 = pd.DataFrame(sc.fit_transform(x1), columns=x1.columns)\n    if x2 is not None:\n        x2 = pd.DataFrame(sc.transform(x2), columns=x2.columns)\n        return (x1,x2)\n    return x1\n\n# for model evaluation and training\ndef eval_model(model, X, Y, seed=1):\n    xtrain, xtest, ytrain, ytest = split_data(X, Y)\n    model.fit(xtrain, ytrain)\n    \n    trainpred = model.predict(xtrain)\n    trainpred_prob = model.predict_proba(xtrain)\n    testpred = model.predict(xtest)\n    testpred_prob = model.predict_proba(xtest)\n    \n    print(\"Train ROC AUC : %.4f\"%roc_auc_score(ytrain, trainpred_prob, multi_class='ovr'))\n    print(\"\\nTrain classification report\\n\",classification_report(ytrain, trainpred))\n    \n    ### make a bar chart for displaying the wrong classification of one class coming in which other class\n    \n    print(\"\\nTest ROC AUC : %.4f\"%roc_auc_score(ytest, testpred_prob, multi_class='ovr'))\n    print(\"\\nTest classification report\\n\",classification_report(ytest, testpred))\n    \ndef plot_importance(columns, importance):\n    plt.bar(columns, importance)\n    plt.show()","e8b7997a":"X = glass.drop([\"Type\"], axis=1)\nX_sc = preprocess(X)\nY = glass[\"Type\"]","095cd806":"model_logr = LogisticRegression(random_state=seed,n_jobs=-1)\nmodel_nb = GaussianNB()\nmodel_dt = DecisionTreeClassifier(random_state=seed)\nmodel_dt_bag = BaggingClassifier(model_dt, random_state=seed, n_jobs=-1)\nmodel_ada = AdaBoostClassifier(random_state=seed)\nmodel_gbc = GradientBoostingClassifier(random_state=seed)\nmodel_rf = RandomForestClassifier(random_state=seed, n_jobs=-1)\nmodel_xgb = XGBClassifier(random_state=seed)\nmodel_lgbm = LGBMClassifier(random_state=seed, n_jobs=-1)\nmodel_knn = KNeighborsClassifier(n_jobs=-1)\n\nmodels = []\nmodels.append(('LR',model_logr))\nmodels.append(('NB',model_nb))\nmodels.append(('DT',model_dt))\nmodels.append(('Bag',model_dt_bag))\nmodels.append(('Ada',model_ada))\nmodels.append(('GBC',model_gbc))\nmodels.append(('RF',model_rf))\nmodels.append(('XGB',model_xgb))\nmodels.append(('LGBM',model_lgbm))\nmodels.append(('KNN',model_knn))","7df5fd53":"cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n\nresults = []\nnames = []\n\nfor name, model in models:\n    scores = cross_val_score(model, X_sc, Y, scoring='f1_weighted', cv=cv, n_jobs=-1)\n    accuracy = scores.mean()\n    std = scores.std()\n    print(f\"{name} : Mean ROC {accuracy} STD:({std})\")\n    results.append(scores)\n    names.append(name)","352f86b4":"fig, ax = plt.subplots(figsize=(12,6))\nax.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","f00172f7":"X = glass.drop([\"Type\",\"K\",\"Ca\"], axis=1)\nX_sc = preprocess(X)\nY = glass[\"Type\"]","3d2b8188":"results = []\nnames = []\n\nfor name, model in models:\n    scores = cross_val_score(model, X_sc, Y, scoring='f1_weighted', cv=cv, n_jobs=-1)\n    accuracy = scores.mean()\n    std = scores.std()\n    print(f\"{name} : Mean ROC {accuracy} STD:({std})\")\n    results.append(scores)\n    names.append(name)","d03f6853":"fig, ax = plt.subplots(figsize=(12,6))\nax.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","1dd249b8":"from sklearn.decomposition import PCA\n\nX = glass.drop([\"Type\"], axis=1)\nX_std = preprocess(X)\npca = PCA(n_components=None)\n# None means that we are selecting all the principal components. Once again, WE ARE NOT DROPPING ANY VARIABLES.\npca.fit(X_std)","8d8dadc6":"# the eigenvalues\npca.explained_variance_","b0b8ce5a":"# the % of variance explained\nvar_exp = pca.explained_variance_ratio_\ncum_var = np.cumsum(pca.explained_variance_ratio_)\nprint(\"Cummulative variance:\\n\", cum_var)\nplt.plot(range(1, len(var_exp)+1), cum_var, color='r', marker='^', label=\"Cummulative Variance\")\nplt.bar(range(1, len(var_exp)+1), var_exp, color='r', label=\"Individual Variance\")\nplt.legend(loc='best')\nplt.title(\"PCA components vs Variance Explained\")\nplt.show()","b174a788":"pca8 = PCA(n_components = 8)\nX_pca = pd.DataFrame(pca8.fit_transform(X_std))\nX_pca.head()","b4cbfd18":"results = []\nnames = []\n\nfor name, model in models:\n    scores = cross_val_score(model, X_pca, Y, scoring='f1_weighted', cv=cv, n_jobs=-1)\n    accuracy = scores.mean()\n    std = scores.std()\n    print(f\"{name} : Mean ROC {accuracy} STD:({std})\")\n    results.append(scores)\n    names.append(name)","018ce385":"fig, ax = plt.subplots(figsize=(12,6))\nax.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","adefdcc6":"pca9 = PCA(n_components = 10)\nX_pca = pd.DataFrame(pca9.fit_transform(X_std))\nX_pca.head()","f1a49b8d":"results = []\nnames = []\n\nfor name, model in models:\n    scores = cross_val_score(model, X_pca, Y, scoring='f1_weighted', cv=cv, n_jobs=-1)\n    accuracy = scores.mean()\n    std = scores.std()\n    print(f\"{name} : Mean ROC {accuracy} STD:({std})\")\n    results.append(scores)\n    names.append(name)","3b3139bd":"fig, ax = plt.subplots(figsize=(12,6))\nax.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","42466ef1":"pca9 = PCA(n_components = 0.99)\nX_pca = pd.DataFrame(pca9.fit_transform(X_std))\nX_pca.head()","7712ba69":"results = []\nnames = []\n\nfor name, model in models:\n    scores = cross_val_score(model, X_pca, Y, scoring='f1_weighted', cv=cv, n_jobs=-1)\n    accuracy = scores.mean()\n    std = scores.std()\n    print(f\"{name} : Mean ROC {accuracy} STD:({std})\")\n    results.append(scores)\n    names.append(name)","53ce9235":"fig, ax = plt.subplots(figsize=(12,6))\nax.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","5c182d65":"### Correlation Plot","ec6baa5d":"    K and Ca have no correlation with Type, which means for some type it maybe high for some low causing cancelling effect","4f68697e":"### Statistical Summary","6ccc1de6":"<H3>Random Forest Model has best performance, so we can work further on it and tune to improve performance\nModel Tuning can be performed using RandomGridSearchCV or Bayesian Optimization which I will add further.","809a2d66":"### Dataset Information","88ccdea2":"### Only Significant Variables","24356811":"#### K and Ca are not siginificant, but the new variables we have created are significant. KUDOS!!!","878b4b47":"### Feature Engineering - Based on the mean of K and Ca in classes","3fa466a7":"    None of the features are normally distributed and some have outliers\n\n    Note: Outlier treatment maybe done to check impact on classification","d9d89383":"### Univariate Box Plot","13df48b3":"### Target Countplot","b4746421":"### Dataset Info","64639a54":"    RI: refractive index\n    Na: Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)\n    Mg: Magnesium\n    Al: Aluminum\n    Si: Silicon\n    K: Potassium\n    Ca: Calcium\n    Ba: Barium\n    Fe: Iron\n    \n    Type of glass: \n        1 building_windows_float_processed\n        2 building_windows_non_float_processed\n        3 vehicle_windows_float_processed\n        4 vehicle_windows_non_float_processed (none in this database)\n        5 containers\n        6 tableware\n        7 headlamps","9b912368":"### Running the algorithms","cb82023b":"### Inferences\n    - Refractive index lies between 1.51 and 1.54\n    - Type 6 and 7 have higher Na %\n    - Type 1,2 and 3 have higher Mg %\n    - Type 5 and 7 have higher Al %\n    - Si % is similar in all types\n    - Type 6 has no K composition\n    - Type 5 and 6 have higher Ca composition\n    - Ba is mostly used in Type 7\n    - Fe is used in Type 1,2 and 3","9fae3aaf":"### Features in data","5c270746":"# Application of PCA\n<b>PCA is a statistical method which can help identify the pattern in data and also help in dimensionality reduction. However, there is a misconception that PCA reduces the set of variables but this is not the case.\nPCA transforms the variables into a new coordinate system with variables accounting for the maximum variance in the data.<\/b>\n<p><b> Also remember that PCA requires standardized data as the variable scale can heavily impact the transformation and end up with total garbage<\/b><\/p>\n\n<H3>A small tip, eigenvalues >0.7 indicate a strong variable importance","15b4091f":"### Importing Libraries","af8804f8":"### Bivariate Box plots","663ecaee":"### Statistical Importance Check for Variable","0f15c667":"### Pairplot","8afd926f":"### Scatter Matrix","0114d14a":"### Data Preprocessing & Evaluation Functions","abac5508":"### Comparison of Models","7787c271":"### Separating the X and Y data","5d2ce416":"### Creating array of models","2a8925b8":"First two components represent almost 50% variance in the data.\n\nThe first 8 components represent more than 95% variance in the data.","8c4b24b6":"### Thank You for viewing my Kernel, if you like it or have any suggestions you are welcome. Also please upvote the Kernel.","de2ae7d8":"### Checking distribution of the features","89564e04":"### Observations:\n    - Silicon is the main component of Glass making more than 70% of composition\n    - Combined Silicon, Sodium and Calcium make up around 90%\n    - Iron is the least important component\n\nAbove box plot confirms the outliers\n\n    I prefer to use models without outlier treatment, in many cases it can improve the model performance.\n    But it also leads to change of information which might alter real\/practical situations","4053e9f0":"### Reading the data"}}