{"cell_type":{"cf34adcd":"code","98af2cac":"code","7c7d3cdf":"code","2a4cd85a":"code","dbab3622":"code","464640ba":"code","ff63c01a":"code","ffb1ad70":"code","5659271c":"code","4ac2f3e4":"code","c41e9a76":"code","bbc4cb94":"code","68e10315":"code","43f7fb54":"code","c5a2110e":"code","61e4529c":"code","03d9233d":"code","5b39c4cf":"code","d0c623e8":"code","43dec93a":"code","0014466d":"code","d59cb594":"code","559f9ec5":"code","a2b49fca":"code","d7ecf885":"code","9f96f6ef":"code","b597edf3":"code","4fb6d0cb":"code","6cd8665c":"code","2a8aea80":"code","fbb63651":"code","5e227217":"code","c0b39ee0":"code","ddabe3ce":"code","6ddc429d":"code","86645d19":"code","698e2861":"code","637e64d1":"code","21ce3fd7":"code","30c86655":"code","16debe30":"code","c01db3c2":"code","4a5ee4fc":"code","76780002":"code","3380cf08":"code","c0af796e":"code","59513170":"code","35cb325c":"code","4a2c748d":"code","57f8402f":"code","a9be3cd8":"code","3962f18d":"code","e4bff6c3":"code","e138c982":"code","312f57d1":"code","16ada0b9":"code","3277b053":"code","ac43bd2e":"code","1dcd1f34":"code","01bfcf3f":"code","a407e9da":"code","243ec278":"code","62a38ff3":"code","c5df70d3":"code","37463d90":"code","56694f68":"code","9551784f":"code","e01adf59":"code","5661cbfa":"code","e90dd820":"code","f438acd6":"code","baa851db":"code","ea63dc92":"code","1948e763":"code","89c41dee":"code","86e05c32":"code","bc941886":"markdown","ea22e287":"markdown","a1e95ec6":"markdown","dfb1b5ec":"markdown","d5e4a8cc":"markdown","fd3315b8":"markdown","6c9fe57b":"markdown","b7d1ae68":"markdown","1e504588":"markdown"},"source":{"cf34adcd":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n! pip install empiricaldist\nfrom empiricaldist import Pmf,Cdf\nfrom scipy.stats import linregress\nimport statsmodels.formula.api as smf\nimport missingno as msno\nfrom scipy.stats import chi2_contingency\nfrom scipy.stats import ttest_ind\nimport shap\nimport xgboost as xgb","98af2cac":"def pmf_cdf_plots(dataset):\n    for c in dataset.columns:\n        pmf = Pmf.from_seq(dataset[c].tolist(), normalize=True)\n        cdf = Cdf.from_seq(dataset[c].tolist())\n\n        plt.figure(figsize = [10,5])# larger figure size for subplots \n\n        plt.subplot(1,2,1)\n        pmf.bar()\n        plt.xlabel(c)\n        plt.ylabel(\"PMF\")\n\n        plt.subplot(1,2,2)\n        cdf.plot()\n        plt.xlabel(c)\n        plt.ylabel(\"CDF\")\n\n        plt.show()\n        \ndef scatter_plots(dataset):\n    float_cols = [i for i in dataset.columns if dataset[i].dtypes == 'float64' and i != 'median_house_value']\n    for i in float_cols:\n        i_jitter = dataset[i]+np.random.normal(0,2,size=len(dataset))\n        j_jitter = dataset['median_house_value']+np.random.normal(0,2,size=len(dataset))\n        plt.plot(i_jitter,j_jitter,'o',data=dataset, alpha=0.1, markersize=1)\n        plt.xlabel(i)\n        plt.ylabel('median house value')\n        plt.show()\n        \ndef violin_plots(dataset):\n    float_cols = [i for i in dataset.columns if dataset[i].dtypes == 'float64' and i != 'median_house_value']\n    for i in float_cols+['median_house_value']:\n        my_order = dataset.groupby(by=[\"ocean_proximity\"])[i].mean().sort_values().iloc[::-1].index.tolist()\n        sns.violinplot(x='ocean_proximity',y=i, data=dataset, inner=None, order=my_order)\n        plt.show()\n        \ndef box_plots(dataset):\n    float_cols = [i for i in dataset.columns if dataset[i].dtypes == 'float64' and i != 'median_house_value']\n    for i in float_cols+['median_house_value']:\n        my_order = dataset.groupby(by=[\"ocean_proximity\"])[i].mean().sort_values().iloc[::-1].index.tolist()\n        sns.boxplot(x='ocean_proximity',y=i, data=dataset, whis=10, order=my_order)\n        plt.show()\n        \ndef reg_plots(dataset):\n    float_cols = [i for i in dataset.columns if dataset[i].dtypes == 'float64' and i != 'median_house_value']\n    for i in float_cols:\n        results = smf.ols('median_house_value~'+i, data=dataset).fit()\n        pred12 = results.predict(dataset[i])\n        i_jitter = dataset[i]+np.random.normal(0,2,size=len(dataset))\n        j_jitter = dataset['median_house_value']+np.random.normal(0,2,size=len(dataset))\n        plt.plot(i_jitter,j_jitter,'o',data=dataset, alpha=0.1, markersize=1)\n\n    #    plt.plot(dataset[i],dataset['median_house_value'], 'o', alpha=0.05, markersize=1)\n        plt.plot(dataset[i], pred12, label='median_house_value')\n        plt.xlabel(i)\n        plt.ylabel('predicted median house value')\n        plt.legend()\n        plt.show()","7c7d3cdf":"dataset = pd.read_csv('..\/input\/california-housing-prices\/housing.csv')","2a4cd85a":"print(dataset.head())","dbab3622":"print(dataset.info())","464640ba":"dataset.shape","ff63c01a":"print(dataset.columns)","ffb1ad70":"print(round(dataset.describe(),2))","5659271c":"print(round(dataset.describe(include='object')))","4ac2f3e4":"cormat = dataset.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(round(dataset.corr(),2), annot=True, cmap=\"RdYlGn\")","c41e9a76":"dataset['bedrooms_to_rooms'] = dataset['total_rooms']\/dataset['total_bedrooms']\ndataset['households_to_population'] = dataset['population']\/dataset['households']\n#dataset.reset_index(inplace=True)","bbc4cb94":"dataset['per_capita_income'] = dataset['median_income']\/dataset['population']\ndataset['per_household_income'] = dataset['median_income']\/dataset['households']","68e10315":"drop_cols = ['total_rooms','total_bedrooms','population', 'households', 'median_income','median_house_value']\ndf = dataset.drop(drop_cols, axis=1)","43f7fb54":"cormat = df.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(round(df.corr(),2), annot=True, cmap=\"RdYlGn\")","c5a2110e":"scaled_df=df.groupby('ocean_proximity').transform(lambda x: (x - x.mean()) \/ x.std())","61e4529c":"scaled_df['ocean_proximity'] = dataset['ocean_proximity']\nscaled_df['median_house_value'] = dataset['median_house_value']","03d9233d":"msno.matrix(scaled_df)\nmsno.heatmap(scaled_df)","5b39c4cf":"msno.bar(scaled_df)","d0c623e8":"dataset","43dec93a":"scaled_df","0014466d":"dataset.columns","d59cb594":"pmf_cdf_plots(scaled_df)","559f9ec5":"sns.pairplot(scaled_df, hue = 'ocean_proximity')","a2b49fca":"cormat = scaled_df.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(round(scaled_df.corr(),2), annot=True, cmap=\"RdYlGn\")","d7ecf885":"ocean_proximity_count = scaled_df.ocean_proximity.value_counts().sort_values()\nocean_proximity_count.plot(kind='bar', figsize=(8,8))","9f96f6ef":"print(round(scaled_df.groupby(\"ocean_proximity\").mean(),2))","b597edf3":"float_cols","4fb6d0cb":"scatter_plots(scaled_df)","6cd8665c":"violin_plots(scaled_df)","2a8aea80":"box_plots(scaled_df)","fbb63651":"reg_plots(scaled_df)","5e227217":"print(dataset.columns)","c0b39ee0":"for i in ['bedrooms_to_rooms','households_to_population', 'households_to_population', 'per_capita_income']:\n    results = smf.ols('median_house_value~'+i, data=dataset).fit()\n    pred12 = results.predict(dataset[i])\n#    i_jitter = dataset[i]+np.random.normal(0,2,size=len(dataset))\n#   j_jitter = dataset['median_house_value']+np.random.normal(0,2,size=len(dataset))\n    plt.plot(i,'median_house_value','o',data=dataset, alpha=0.1, markersize=1)\n    plt.plot(dataset[i], pred12, label='median_house_value')\n    plt.xlabel(i)\n    plt.ylabel('predicted median house value')\n    plt.show()","ddabe3ce":"for i in ['bedrooms_to_rooms','households_to_population', 'median_house_value','households_to_population', 'per_capita_income']:\n    dataset[i].hist(bins=50, alpha=0.5)\n    plt.xscale('log')\n    plt.show()","6ddc429d":"for i in ['bedrooms_to_rooms','households_to_population','households_to_population', 'per_capita_income']:\n    splot = sns.regplot(x=i, y='median_house_value', \n                        data=dataset,\n                        scatter_kws={'alpha':0.15},\n                        fit_reg=True)\n    splot.set(xscale=\"log\", yscale='log')\n    plt.show()","86645d19":"data = pd.get_dummies(scaled_df, prefix_sep='_', columns=['ocean_proximity'], drop_first=True)","698e2861":"data","637e64d1":"#data.drop('index',axis=1 ,inplace=True)\ndata['y_response'] = data['median_house_value']\ndata.drop('median_house_value', axis=1, inplace=True)","21ce3fd7":"data","30c86655":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error","16debe30":"#trans = StandardScaler()\n#data = trans.fit_transform(data)","c01db3c2":"#X, y = data[:,1:-1], data[:,-1]","4a5ee4fc":"X, y = data.iloc[:,1:-1], data.iloc[:,-1]","76780002":"#X, y = data[:,:-1], data[:,-1]","3380cf08":"y","c0af796e":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=123)","59513170":"DM_train = xgb.DMatrix(data=X_train, label=y_train)\nDM_test = xgb.DMatrix(data=X_test, label=y_test)","35cb325c":"params = {\n    # Parameters that we are going to tune.\n    'max_depth':6,\n    'min_child_weight': 1,\n    'eta':.3,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    # Other parameters\n    'objective':'reg:linear',\n    'eval_metric':'mae',\n    \n}","4a2c748d":"model = xgb.train(\n                params,\n                DM_train,\n                num_boost_round=999,\n                evals=[(DM_test,\"Test\")],\n                early_stopping_rounds = 10,\n)","57f8402f":"cv_results = xgb.cv(\n        params,\n        DM_train,\n        num_boost_round=999,\n        seed=42,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=10,\n        as_pandas = True\n)","a9be3cd8":"cv_results","3962f18d":"cv_results[['train-mae-mean','test-mae-mean']].plot()\ncv_results[['train-mae-std','test-mae-std']].plot()","e4bff6c3":"gridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in range(9,12)\n    for min_child_weight in range(5,8)\n]","e138c982":"gridsearch_params","312f57d1":"min_mae = float(\"Inf\")\nbest_params = None\n\nfor max_depth, min_child_weight in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}\".format(max_depth, min_child_weight))\n    \n    # Update our parameter\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n    \n    # Run CV\n    cv_results = xgb.cv(\n    params,\n    DM_train,\n    num_boost_round=999,\n    seed=42,\n    nfold=5,\n    metrics={'mae'},\n    early_stopping_rounds = 10\n    )\n    \n    # update best MAE\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].argmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (max_depth, min_child_weight)\n        \nprint(\"Best params: {}, {}, MAE:{}\".format(best_params[0], best_params[1], min_mae))\n    ","16ada0b9":"params['max_depth'] = 10\nparams['min_child_weight'] = 6","3277b053":"gridsearch_params = [\n    (subsample, colsample)\n    for subsample in [i\/10. for i in range(7,11)]\n    for colsample in [i\/10. for i in range(7,11)]\n]\ngridsearch_params","ac43bd2e":"min_mae = float(\"Inf\")\nbest_params = None\n# We start by the largest values and go down to the smallest\nfor subsample, colsample in reversed(gridsearch_params):\n    print(\"CV with subsample={}, colsample={}\".format(\n                             subsample,\n                             colsample))\n    # We update our parameters\n    params['subsample'] = subsample\n    params['colsample_bytree'] = colsample\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        DM_train,\n        num_boost_round=999,\n        seed=42,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=10\n    )\n    # Update best score\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].argmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (subsample,colsample)\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))\n","1dcd1f34":"params['subsample'] = 1.0\nparams['colsample_bytree'] = 1.0","01bfcf3f":"# This can take some time\u2026\nmin_mae = float(\"Inf\")\nbest_params = None\nfor eta in [.3, .2, .1, .05, .01, .005]:\n    print(\"CV with eta={}\".format(eta))\n    # We update our parameters\n    params['eta'] = eta\n    # Run and time CV\n    cv_results = xgb.cv(\n            params,\n            DM_train,\n            num_boost_round=999,\n            seed=42,\n            nfold=5,\n            metrics=['mae'],\n            early_stopping_rounds=10\n          )\n    # Update best score\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].argmin()\n    print(\"\\tMAE {} for {} rounds\\n\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = eta\nprint(\"Best params: {}, MAE: {}\".format(best_params, min_mae))","a407e9da":"params['eta'] = 0.05","243ec278":"params","62a38ff3":"model = xgb.train(\n    params,\n    DM_train,\n    num_boost_round=999,\n    evals=[(DM_test, \"Test\")],\n    early_stopping_rounds=10\n)","c5df70d3":"y_hat = model.predict(DM_test)","37463d90":"y_test","56694f68":"y_hat","9551784f":"i_jitter = y_hat+np.random.normal(0,2,size=len(y_hat))\nj_jitter = y_test.values+np.random.normal(0,2,size=len(y_test))\nplt.plot(i_jitter,j_jitter,'o',data=dataset, alpha=0.1, markersize=1)\nplt.xlabel('Predicted')\nplt.ylabel('median house value')\nplt.show()","e01adf59":"i_jitter = y_hat+np.random.normal(0,2,size=len(y_hat))","5661cbfa":"residuals = y_test.values-y_hat","e90dd820":"plt.plot(y_hat, residuals, 'o',markersize=1)","f438acd6":"sns.distplot(residuals)","baa851db":"sns.distplot(y_hat)","ea63dc92":"sns.distplot(y_test)","1948e763":"xgb.plot_importance(model)","89c41dee":"output_df=pd.DataFrame({\n    'actual_value':y_test.tolist(),\n    'predicted_value': y_hat\n})","86e05c32":"output_df.to_csv('.\/output.csv')","bc941886":"## Plots on the Log Scale","ea22e287":"# Importing Dataset","a1e95ec6":"# Parametric assisment of the response variable with predictor. ","dfb1b5ec":"## Deriving Columns ","d5e4a8cc":"# Understandign each variable in the dataset.","fd3315b8":"# Importing Python Modules ","6c9fe57b":"# Inspecting Dataset","b7d1ae68":"## Identifying Missing values ","1e504588":"# Non Parametric method to check the significance of the response variable with the predictior."}}