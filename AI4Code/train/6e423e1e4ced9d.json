{"cell_type":{"e8a3fe1d":"code","e5e64c20":"code","a25aeb33":"code","0274a52b":"code","a0aac0b2":"code","e8649093":"code","31efae75":"code","0e57f249":"code","ca25c7a2":"code","841cb43c":"code","33249fb8":"code","198a4185":"code","031dcceb":"code","876b1d76":"code","82c9d212":"code","6142dbca":"code","623f0c93":"code","661284a4":"code","bfb19647":"code","f7ecf3ba":"code","926b1e63":"code","2231b756":"code","28ca95a4":"code","615b7790":"code","f21772c4":"code","b9c85737":"code","566a26bf":"code","224f8804":"code","db48c73e":"code","a6368c9f":"code","0c687e78":"code","1c3c5c3d":"code","6a4efbb3":"code","0efde8a9":"code","c80a86fc":"code","9f2a382f":"code","2c3c939b":"code","9aa4f372":"code","e56c190f":"code","76244ae8":"code","20e32867":"code","c77406b5":"code","340e9899":"code","9c9000a3":"code","8dc7103c":"code","ff7312bb":"code","ce18b5bc":"code","fafdc8e8":"markdown","08cbd8db":"markdown","d8a14964":"markdown","abbd879b":"markdown","596ebd7f":"markdown","99a628c1":"markdown","17dd436f":"markdown","61b035e4":"markdown","e0adb193":"markdown","034573d2":"markdown","b966745f":"markdown","dbd56b35":"markdown","d2496a2a":"markdown"},"source":{"e8a3fe1d":"import warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)","e5e64c20":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler,OneHotEncoder,OrdinalEncoder,LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import ExtraTreesClassifier,VotingClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import f1_score,confusion_matrix,plot_confusion_matrix,classification_report,accuracy_score,recall_score,precision_score\nfrom imblearn.over_sampling import RandomOverSampler","a25aeb33":"stroke_data=pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","0274a52b":"stroke_data.head()","a0aac0b2":"stroke_data.info()","e8649093":"stroke_data_replaced=stroke_data.copy()","31efae75":"stroke_data_replaced.fillna(value=stroke_data_replaced.mean(axis=0),axis=0,inplace=True)","0e57f249":"for col in ['hypertension','heart_disease','stroke']:\n    stroke_data_replaced[col]=stroke_data_replaced[col].replace([0,1],['No','Yes'])","ca25c7a2":"stroke_data_replaced.describe()","841cb43c":"stroke_data_replaced.info()","33249fb8":"stroke_data_replaced.head()","198a4185":"categorical_col=[]\nnumerical_col=[]\nfor col in stroke_data_replaced.dtypes.index:\n    if stroke_data_replaced[col].dtype == 'object':\n        categorical_col.append(col)\n    else:\n        numerical_col.append(col)","031dcceb":"print(\"Numerical Columns in DataFrame are :\",numerical_col, '\\n')\nprint(\"Categorical Columns in DataFrame are :\",categorical_col)","876b1d76":"def bar_plot(data,x,y='id',hue='stroke',group='stroke' ,title=None):\n    \"\"\"\n        function use to plot barplot by grouping the data\n\n    \"\"\"\n    try:\n        group=data.groupby([x,group],as_index=False)['id'].count()\n        sns.barplot(data=group,x=x,y=y,hue=hue,)\n        plt.title(\"Stroke count based on \"+ x , fontdict={'size' :16,'color':'red'})\n        plt.ylabel('Count',fontdict={'size':12,'color':'blue'})\n        plt.xlabel(x,fontdict={'size':12,'color':'blue'})\n        print('-'*100)\n        return display(group) ,plt.show()\n    except:\n        pass","82c9d212":"for col in categorical_col:\n    bar_plot(stroke_data_replaced,col)","6142dbca":"Feature_to_drop=['work_type']","623f0c93":"def scatter_plot(data,x,y='stroke',title=None):\n    plt.figure(figsize=(10,7))\n    plt.title(\"Relationship of \"+ y + \" v\/s \"+ x , fontdict={'size':18,'color':'red'})\n    sns.scatterplot(data=data,x=x,y='stroke')\n    plt.xlabel(x,fontdict={'size':12,\"color\":'blue'})\n    plt.ylabel(y,fontdict={'size':12,\"color\":'blue'})\n    print('-'*100)\n    return plt.show()","661284a4":"for col in numerical_col:\n    scatter_plot(stroke_data_replaced,col)","bfb19647":"stroke_data_replaced","f7ecf3ba":"Feature_to_drop.append('id')","926b1e63":"stroke_data_replaced.drop(Feature_to_drop,axis=1,inplace=True)","2231b756":"sns.pairplot(stroke_data_replaced,hue='stroke')","28ca95a4":"sample=stroke_data_replaced.sample(n=100,random_state=72018,)\nsample","615b7790":"def outlier_detection(data,x):\n    plt.figure(figsize=(10,7))\n    plt.title(\"Outliers for \" + x , fontdict={'size':18,'color':'red'})\n    sns.boxplot(data=data,x=x)\n    plt.xlabel(x,fontdict={'size':12,\"color\":'blue'})\n    print('-'*100)\n    return plt.show()","f21772c4":"for i in numerical_col:\n    if i!='id':\n        outlier_detection(stroke_data_replaced,i)","b9c85737":"#this wil masl the categorical columns\nmask = stroke_data_replaced.dtypes == np.object_","566a26bf":"#here we separate the numerical and categorical columns\ncategorical_col = stroke_data_replaced.columns[mask.values]\nnumerical_col = stroke_data_replaced.columns[~mask.values]","224f8804":"#now separate ordinal and onehot columns\n#here by the visulization we know our ordinal column have >2 variables\n\nonehot_categorical_columns=[]\nordinal_categorical_columns=[]\nfor i in categorical_col:\n    if stroke_data_replaced[i].nunique()<3:\n        onehot_categorical_columns.append(i)\n    else:\n        ordinal_categorical_columns.append(i)","db48c73e":"#this version of scikit-learn doesn't support the handle unknown option for ordinal encoder so this will replace ordinal encoder\nconverter={'gender':{'Other':0,'Male':1,'Female':2},'smoking_status':{'formerly smoked':1, 'never smoked':2, 'smokes':3, 'Unknown':4}}\nfor i in ordinal_categorical_columns:\n    stroke_data_replaced[i]=stroke_data_replaced[i].map(converter[i])","a6368c9f":"#Create our transformers that will transform the columns \n#oridnal transformer\n#OD=OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=5)\n\n#min max scaler\nMM=MinMaxScaler()\n\n#label encoder for target variable\nLE=LabelEncoder()\n\n#one hot encoded column\nOHE=OneHotEncoder(handle_unknown='ignore')","0c687e78":"#traget variable\ny=stroke_data_replaced['stroke']\n\n#remove from onehot columns\nonehot_categorical_columns.remove('stroke')","1c3c5c3d":"#create column transformer that will transform the columns respective to the criteria\n#transformer=ColumnTransformer(transformers=[('numerical',MM,numerical_col),\n#                                            ('ordinal',OD,ordinal_categorical_columns),\n#                                            ('onehot',OHE,onehot_categorical_columns)])\n\n#use te transformer above if you are using ordinal_encoder\ntransformer=ColumnTransformer(transformers=[('numerical',MM,numerical_col),('onehot',OHE,onehot_categorical_columns)])","6a4efbb3":"#feature columns\nX=stroke_data_replaced.drop('stroke',axis=1)","0efde8a9":"#used for oversampling the data as it is highly imbalanced\nROS=RandomOverSampler(random_state=42)","c80a86fc":"def transform_and_resample_data(X,y):\n    \"\"\"\n    It accepts feature column and target column.\n    split the data int train and test set in stratify manner.\n    transform the target with label encoder\n    over sample the target set.\n    return train set and test set\n    \"\"\"\n    \n    train_X,test_X,train_y,test_y = train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)\n    train_y = LE.fit_transform(train_y)\n    test_y = LE.transform(test_y)\n    train_X_resampled , train_y_resampled = ROS.fit_resample(train_X,train_y)\n\n\n    return train_X_resampled,train_y_resampled,test_X,test_y","9f2a382f":"def prune_tree(train_X,train_y,transformer):\n    \"\"\"\n    accepts the model and train data.\n    fit to Decision tree return the required parameter. \n    \n    It is used to prune the tree to generate range of the parameters.\n    \"\"\"\n    model = DecisionTreeClassifier(criterion='gini',random_state=42)\n    \n    pipeline=Pipeline(steps=[('transform',transformer),\n                             ('model',model)])\n    \n    pipeline.fit(train_X,train_y)\n    return model.tree_.max_depth,model.tree_.n_features","2c3c939b":"def model_selection(train_X,train_y,transformer,current=None):\n    \"\"\"\n    It is used to select different models for traning purpose based in the value of current.\n    \n    \"\"\"\n    if current == 0:\n        \n        label='Logistic Regression'\n        model = LogisticRegression(solver='liblinear')\n        params = {'model__C':[0.001,0.01,0.1,1,10],\n               'model__penalty':['l1','l2']}\n        \n    elif current == 1:\n        \n        label = 'KNeighborsClassifier'\n        model = KNeighborsClassifier(weights='distance')\n        params = {'model__n_neighbors':list(range(2,20))}\n        \n    elif current == 2:\n        \n        label = 'SVC'\n        model = SVC(kernel='rbf')\n        params = {'model__C':[0.001,0.01,0.1,1,10]}\n        \n    elif current == 3:\n        \n        label = 'DecisionTreeClassifier'\n        model = DecisionTreeClassifier(criterion='entropy',random_state=42)\n        max_depth, max_features = prune_tree(train_X,train_y,transformer)\n        params = {'model__max_depth':list(range(1,max_depth+1,2)),\n                 'model__max_features':list(range(1,max_features))}\n        \n    else:\n        raise ValueError('No value passed for current')\n        \n    return model, params, label","9aa4f372":"def scoring(model , params, true, pred):\n    \"\"\"\n    It is used to score the model based on its preformance \n    Accepts model , parameter, original labels and predicted labels\n    \n    \"\"\"\n    re_Y = recall_score(true, pred, pos_label=1)\n    re_N = recall_score(true, pred, pos_label=0)\n    pr_Y = precision_score(true, pred, pos_label=1)\n    pr_N = precision_score(true, pred, pos_label=0)\n    f1_Y = f1_score(true, pred, pos_label=1)\n    f1_N = f1_score(true, pred, pos_label=0)\n    acc = accuracy_score(true, pred)\n    data = pd.Series({'model':model,\n                      'params': params,\n                      'recall_Yes': re_Y,\n                      'recall_No': re_N,\n                      'precision_Yes':pr_Y,\n                      'precision_No':pr_N,\n                      'f1_Yes':f1_Y,\n                      'f1_No':f1_N,\n                      'accuracy':acc})\n    \n    return data","e56c190f":"#final dataframe which will store the performance of the model\nperformance =  pd.DataFrame(columns=['model','params','recall_Yes','recall_No','precision_Yes','precision_No','f1_Yes','f1_No','accuracy'])\n\n#get train and test set\ntrain_X,train_y,test_X,test_y = transform_and_resample_data(X,y)\n\n#call different model fir training and testing\nfor i in range(4):\n    \n    model, param, label = model_selection(train_X, train_y, transformer, current=i)\n    pipeline = Pipeline(steps = [('transform',transformer),\n                             ('model',model)])\n    \n    #score metric \n    score = {'r':'recall','p':'precision'} \n    grid = GridSearchCV(pipeline,param,cv=4,scoring=score, refit='r')\n    \n    grid.fit(train_X,train_y)\n    \n    pred = grid.predict(test_X)\n    \n    para = grid.best_estimator_.named_steps['model']\n    \n    performance = performance.append(scoring(label, para, test_y, pred),ignore_index=True)\n\n    string = 'This results are for '+label\n    print(19*'* '+string+19*' *')\n    print('----- Classification report -----')\n    print(classification_report(test_y,pred))\n    print('----- Confusion matrix -----')\n    print(confusion_matrix(test_y,pred))","76244ae8":"performance","20e32867":"#getting our model we want to use fr voting\nv_1,v_2,v_3,v_4 = performance['params'].values","c77406b5":"#models for voting\nestimators = [('lr',v_1),('svc',v_3),('dt',v_4)]","340e9899":"#voting model\nvote = VotingClassifier(estimators,voting='hard')\npipeline = Pipeline(steps = [('transform',transformer),\n                             ('model',vote)])","9c9000a3":"#fit the model\npipeline.fit(train_X,train_y)","8dc7103c":"#prediction made my the pipeline\npred=pipeline.predict(test_X)","ff7312bb":"#classification report \nprint(classification_report(test_y,pred))","ce18b5bc":"#plot the confusion matrix\nplot_confusion_matrix(pipeline,test_X,test_y)","fafdc8e8":"Things that can be assumed on the basis of above plots:\n    \n   - Relationship of Stroke and id\n        - this feature id is totally irrelavent for prediction purpose as it add no information to the data\n        - every person will have different id and no pattern will be formed\n   - Relationship of Stroke and age\n        - According to the trend elder people will have high risk of getting stroked\n   - Relationship of Stroke and average glucose level\n        - People having glucose level around 150 mg\/dl are less prone to get stroke\n        - while people having more or less than this are more prone for stroke\n   - Relationship of Stroke and average BMI\n        - we can't really tell if this is a good predictor or not\n        - further investgation is needed","08cbd8db":"From this we can see clearly the Column bmi and average glucose level have a lot of outliers\nwe can handle this with suitable techniques like replacing it with mean or median or dropping in \nin this case we are ieabing these outliers as it is.\n\nIn hope that an extreme behaviour target variable are highly dependent on the pattern of these features","d8a14964":"while KNeighborsClassifier perform the worst.","abbd879b":"##  Model traning and prediction","596ebd7f":"\nobserving recall, precision and accuracy of SVC is higher than everyone else.","99a628c1":"## Voting classifier","17dd436f":"## Visulization","61b035e4":"## Data Transformation","e0adb193":"By above Study it is clear that we are going to drop the featue Work Type as it doesn't contribute to prediction\n\nAnd The Featue Maritial Status need to be further investigated ","034573d2":"#  Stroke prediction","b966745f":"we can see from voting the recall precision and accuray is actualy increased by fine margin.","dbd56b35":"## Data information","d2496a2a":"Things that can be assumed on the basis of above plots:\n\n   - Stroke Count based on gender    \n        * it is around 5% for males and 4% for females those have been struggling with stroke  \n        \n        * this can be the important feature as 1% is a significant drop     \n        \n   - Stroke Count based on hypertension    \n        * It is found to be person tested with hypertension is more likely to be stroked (~ 13%)   \n        \n        * and this is also accepted by having some prior domain knowledge     \n        \n   - Stroke Count based on heart_disease     \n        * It is found to be person with heart disease are at high risk of stroke (~ 17%)\n        \n        * This is also accepted by having some prior domain knowlegde\n        \n   - Stroke Count Based on maritial Status       \n        * 6% of married people have faced stroke         \n        \n        * also is it to be notice that we have more baised data around the married people nearly 2000 more entries married people\n        \n        * This need to be investigated further\n        \n   - Stroke count Based on Work Type\n        * Based on the data it is seen that there are more number of records for private sector jobs\n        \n        * It can be possible that people having govt job are more relived than people with private jobs but on other have people having no job don't have stroke who should probaly be more worried about getting a job\n        \n        * And children are being tested positive for stroke \n        \n        * Conclusively Stroke doesn't depends on Work Type\n        \n   - Stroke based on Residence Type\n        * People living in Urban Areas have !% more Stroke than People Living in Rural areas\n        \n        * Urban areas have more Pollution level and Busy life style which can have great impact on the health of a person\n        \n   - Stroke Count Based on Smoking Status\n        * Similary for people who formerly smoked or smokes have more chances to get Stroke than people who doesn't\n        \n        * It is un predictable for the people with Unknown Status"}}