{"cell_type":{"87168094":"code","409944f4":"code","e8b51a0e":"code","f5c92175":"code","c5affc3b":"code","5369f8ec":"code","3c67eeaf":"code","bbaadbd6":"code","aa752352":"code","5c8d6568":"code","32eb369e":"code","b1e6fa9a":"code","b9cce08e":"code","4ebe898a":"code","f581e750":"code","31edd49c":"code","509d5fdc":"code","95587269":"code","0adb3898":"markdown","94360159":"markdown"},"source":{"87168094":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom glob import glob\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n\nimport keras.backend as K\nfrom keras.layers import Input,Conv2D,MaxPool2D,Dense,Dropout,Flatten,BatchNormalization\nfrom keras.models import Model,Sequential\nfrom keras.optimizers import Adam\nfrom keras.utils import to_categorical\nfrom keras.losses import categorical_crossentropy\nfrom keras.preprocessing import image\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","409944f4":"Labels = pd.read_csv('..\/input\/train.csv')\nLabels.head()","e8b51a0e":"Pics = os.listdir('..\/input\/train\/')\n","f5c92175":"SIZE = 128\ndef ImportImage( filename):\n    img = Image.open('..\/input\/train\/'+filename).convert(\"LA\").resize( (SIZE,SIZE))\n    return np.array(img)[:,:,0]","c5affc3b":"# load pictares and label them , \"MinPicsPerUser\" is the number of pictures requers for class to load.\n# At the begginig for easy training and convargse I will load only those with high number of pics\n\ndef LoadImage_And_MatchLabels(Pics,Labels,Unique_Labels,MinPicsPerUser,MaxPicPerUser = 1000,SIZE= 128):\n    ManyImageIndex = np.array(Unique_Labels['Count']\n                              [Unique_Labels.index[ Unique_Labels['Count'] > MinPicsPerUser ]].tolist())\n    ManyImageIndex_Sum = np.sum(ManyImageIndex[ManyImageIndex<MaxPicPerUser]) \n\n    Train_img_Array = np.zeros((ManyImageIndex_Sum,SIZE,SIZE))\n    PicInd = 0 \n    ImageLabel =  [] \n    for Pic in  Pics : \n        #print(Pic,PicInd)\n        ID = Labels['Id'][Labels.index[Labels['Image']==Pic ].tolist()].tolist()[0]\n        NumImages = Unique_Labels['Count'][Unique_Labels.index[Unique_Labels['Id']== ID].tolist()].tolist()\n        if (NumImages[0] > MinPicsPerUser and NumImages[0] < MaxPicPerUser) : \n            Train_img_Array[PicInd,:,:] = ImportImage(Pic)\n            PicInd += 1\n            ImageLabel.append(ID)\n    return Train_img_Array,ImageLabel","5369f8ec":"# load only clasess with >29 images per class \n\nMinPic = 29 \nUnique_Labels = Labels.drop_duplicates(subset='Id').reset_index()\nUnique_Labels['Count'] = 0\nSumImages = np.zeros(Unique_Labels.shape[0])\nfor i in range(Unique_Labels.shape[0]):\n    SumImages[i] = np.sum(Labels['Id']== Unique_Labels['Id'][i])\n    Unique_Labels['Count'][i] = SumImages[i]\nSumImages_Sort = np.sort(SumImages)\nTrain_Phase_1_Array,ImageLabel = LoadImage_And_MatchLabels(Pics,Labels,Unique_Labels,MinPic,SIZE=SIZE)             \n","3c67eeaf":"# Helper function to Categorical classes\ndef List_To_Categorical(Label_List):\n    LabelsArray = np.zeros(len(Label_List))\n    for j,label in enumerate(set(Label_List)):\n        inds = [i for i,e in enumerate(Label_List) if (e == label)]\n        for i in inds: \n            LabelsArray[i] = j\n    LabelsCategorical = to_categorical(LabelsArray)\n    print(LabelsCategorical.shape)\n    return LabelsCategorical,LabelsArray","bbaadbd6":"CtegoricalLabel,LabelsArray = List_To_Categorical(ImageLabel)\n","aa752352":"\ninputs1 = Input((SIZE,SIZE,1))\nC1 = Conv2D(32,kernel_size=(3,3),activation='relu')(inputs1)\nC1 = BatchNormalization()(C1)\nC1 = MaxPool2D(pool_size=(2,2))(C1)\nC2 = Conv2D(32,kernel_size=(3,3),activation='relu')(C1)\nC2 = BatchNormalization()(C2)\nC2 = MaxPool2D(pool_size=(2,2))(C2)\nC3 = Conv2D(64,kernel_size=(3,3),activation='relu')(C2)\nC3 = MaxPool2D(pool_size=(2,2))(C3)\nC4 = Conv2D(64,kernel_size=(3,3),activation='relu')(C3)\nC4 = MaxPool2D(pool_size=(2,2))(C4)\nC5 = Flatten()(C4)\nDanse1 = Dense(128,activation='relu')(C5)\nDanse1 = Dropout(0.5)(Danse1)\nDanse2 = Dense(128)(Danse1)\nDanse2d = Dropout(0.5)(Danse2)\nDense3 = Dense(CtegoricalLabel.shape[1],activation='softmax')(Danse2d)\n","5c8d6568":"model = Model(inputs1,Dense3)\nmodel.compile(loss=categorical_crossentropy, optimizer=Adam(),metrics=['accuracy'])\nmodel.summary()\n","32eb369e":"model.fit(x=Train_Phase_1_Array.reshape([-1,SIZE,SIZE,1]),y=CtegoricalLabel,batch_size=32,epochs=50,verbose=1,\n          validation_split=0.15)","b1e6fa9a":"\nTriplet_model = Sequential()\nfor layer in model.layers[:-2]:\n    Triplet_model.add(layer)\n    Triplet_model\nTriplet_model.summary()","b9cce08e":"# My Triplet loss, and sorting the data according to the loss definition\ndef TripletLoss_3(yTrue,y_pred):\n    #y_pred_norm = K.l2_normalize(y_pred,axis=0)\n    PosDiff = K.sqrt(K.mean(K.square(y_pred-yTrue[:,:128])))\n    NegDiff = K.sqrt(K.mean(K.square(y_pred-yTrue[:,128:256])))\n    Dist_Pos_Neg = .6  - (NegDiff) + (PosDiff)\n    #loss = K.maximum(0.0,Dist_Pos_Neg)\n    loss = K.log(1 + K.exp(Dist_Pos_Neg))\n    return loss\n\n\ndef SortFeatures(ImArray,Features,LabelsArray):\n    FeaturesOut = np.zeros((Features.shape[0],Features.shape[1]*2))\n    for j in range(ImArray.shape[0]) :\n        Ind_Same_Whale = np.array([i for i,e in enumerate(LabelsArray) if (e == LabelsArray[j]) & (j != i)])\n        Ind_different_Whale = np.array([i for i,e in enumerate(LabelsArray) if (e != LabelsArray[j]) & (j != i)])\n        PosInd = np.random.choice(Ind_Same_Whale)\n        NegInd = np.random.choice(Ind_different_Whale)\n\n        FeaturesOut[j,:128] = Features[PosInd,:]\n        FeaturesOut[j,128:256] = Features[NegInd,:]\n    return FeaturesOut","4ebe898a":"# Optmize the Nearest neighber classification using the Triplet loss: \nCtegoricalLabel = List_To_Categorical(ImageLabel)\nTriplet_model.compile(optimizer='adam',loss=TripletLoss_3)\nPred = Triplet_model.predict(Train_Phase_1_Array.reshape([-1,SIZE,SIZE,1]))","f581e750":"\nSortedPred = SortFeatures(Train_Phase_1_Array,Pred,ImageLabel)\nTriplet_model.fit(x=Train_Phase_1_Array.reshape([-1,SIZE,SIZE,1]),y=SortedPred,batch_size=32,epochs=5,verbose=1)","31edd49c":"# load other set of Classes for test , those with Number of Pics >16 <28\nMinPicsPerUser = 16\nMaxPic = 28\nTrain_Phase_2_Array,ImageLabel_2 = LoadImage_And_MatchLabels(Pics,Labels,Unique_Labels,MinPicsPerUser,MaxPicPerUser=MaxPic,SIZE=SIZE)             \n\n# Extract features\nPred_Features = Triplet_model.predict(Train_Phase_2_Array.reshape([-1,SIZE,SIZE,1]))\nLabelsCategorical_2,LabelsArray_2 = List_To_Categorical(ImageLabel_2)\n","509d5fdc":"#  train and test split\nX_train, X_test, y_train, y_test = train_test_split(Pred_Features, LabelsArray_2, test_size=0.25, random_state=42)\n# Nearest neighbor distance calculation\nDist = np.zeros((len(y_test),len(np.unique(y_train))))\nfor i in range(X_test.shape[0]):\n    DiffMat = np.sum(np.square(np.subtract(X_train,X_test[i,:])),axis=1)\n    for j in range(len(np.unique(y_train))):\n        Dist[i,j] = np.sum(DiffMat[np.where(j==y_train)])\n        ","95587269":"# top 5 nearest neighbor classification \nSumTop5 = 0 \nfor k  in range(len(Dist)):\n    if np.sum(np.argsort(Dist[k,:])[:5]== y_test[k]) == 1 : \n        SumTop5 += 1\n        \nPercentCorrect_top5 = SumTop5\/len(Dist) \nprint(PercentCorrect_top5)","0adb3898":"In this notbook I will use Nearest Neighber Classification using Triplet loss training (FaceNet: A Unified Embedding for Face Recognition and Clustering), in five steps:\n1. Loading first sub-set of data-base (Whales having at least 29 images per whale) and train cross entropy CNN\n2. Optimize the first sub-set using triplet loss to extract 128 features\n3. Load secound sub-set (Whales with 18-28  images per whale) , split it to train and test.\n4. Predict the train set features\n5. Classification of test set using Nearest neighbor features to the train set.\n","94360159":"Define a Triplet network, with the wightes from previse CNN model:"}}