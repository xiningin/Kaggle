{"cell_type":{"23f1a00c":"code","dbde5c2d":"code","a24a425a":"code","786a47cb":"code","6cd66aba":"code","aee1b6ee":"code","7d1d9a9c":"code","532af3d6":"code","5049c06f":"code","075412cb":"code","a764edd4":"code","a59e3acb":"code","aeeef74c":"code","2e0e8f05":"code","2d73e854":"code","de324482":"code","d570e1c5":"code","f5f08c08":"code","d0d1e1da":"code","b552bef4":"code","458f2ff5":"code","f304777f":"code","8364ab27":"code","00b9e069":"code","00256ac7":"code","8ef4171d":"code","2e54feaa":"code","c3c0387c":"code","b600de5a":"code","827b090a":"code","244888e7":"code","8f489f69":"code","08c01dbf":"code","86fba309":"code","1616bc62":"code","4739bc34":"code","e188898b":"code","53afabd4":"code","f7a86459":"code","09729083":"code","8b2a00d9":"markdown","a437e16f":"markdown","176ebada":"markdown","b751886a":"markdown","9c861c44":"markdown","2bd0ac69":"markdown","c12e855b":"markdown","2e989186":"markdown","5763b61b":"markdown","888f86af":"markdown","c2b44fe8":"markdown","adba7cd8":"markdown","24a4e598":"markdown","22ed9e44":"markdown","9cc928fb":"markdown"},"source":{"23f1a00c":"\nimport os\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\n\nfrom transformers import AutoTokenizer\nfrom transformers import TFAutoModel\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow.keras.backend as K\nimport logging\n","dbde5c2d":"df = pd.read_csv('..\/input\/iiitd-abuse-detection-challenge\/eam2021-train-set\/bq-results-20210825-203004-swh711l21gv2.csv')\ndf.head()","a24a425a":"df.info()","786a47cb":"df['length'] = df['commentText'].str.len()\ndf['length'].describe()","6cd66aba":"drop_percentage = len(df[df['length'] > 384]) \/ len(df) * 100\nprint(\"% Amount of data that will be truncated: \", drop_percentage)\n\n#df = df.sample(n=20000).reset_index(drop=True) ## Need to implement generator that can tokenize on the fly, if you have large RAM locally, comment this line\n# df = df[df['length'] < 384]","aee1b6ee":"config = {\n    'seed' : 42,\n    'model': '..\/input\/murilbertcased',\n    'group': 'MURIL',\n    \n    'batch_size': 16,\n    'max_length': 64,\n    \n    'device' : 'GPU',\n    'epochs' : 2,\n\n    'test_size' : 0.1,\n    'lr': 5e-6,\n    'use_transfer_learning' : False,\n    \n    'use_wandb': True,\n    'wandb_mode' : 'online',\n}","7d1d9a9c":"def seed_everything(seed = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n    \n# Creating a logger \ud83d\udcc3\ndef init_logger(log_file:str ='training.log'):\n    \n    # Specify the format \n    formatter = logging.Formatter('%(levelname)s:%(name)s:%(message)s')\n    \n    # Create a StreamHandler Instance\n    stream_handler = logging.StreamHandler()\n    stream_handler.setLevel(logging.DEBUG)\n    stream_handler.setFormatter(formatter)\n    \n    # Create a FileHandler Instance\n    file_handler = logging.FileHandler(log_file)\n    file_handler.setFormatter(formatter)\n    \n    # Create a logging.Logger Instance\n    logger = logging.getLogger('IITD-MURIL')\n    logger.setLevel(logging.DEBUG)\n    logger.addHandler(stream_handler)\n    logger.addHandler(file_handler)\n    \n    return logger\n\n\nLOGGER = init_logger()\nLOGGER.info(\"Logger Initialized\")\n\nseed_everything()\nLOGGER.info(\"Seed Setting done\")\n","532af3d6":"# To make this work with TPU\n\ndef get_device(device):\n    if device == 'TPU':\n        try: # detect TPUs\n            tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n            strategy = tf.distribute.TPUStrategy(tpu)\n        except ValueError: # detect GPUs\n            print('Cannot initialize TPU')\n    if device == 'GPU':\n        strategy = tf.distribute.MirroredStrategy() \n\n    print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n    return strategy\n\nstrategy= get_device(config['device'])\nconfig['batch_size'] = config['batch_size'] * strategy.num_replicas_in_sync\n\nLOGGER.info(\"Effective batch size is \" + str(config['batch_size']))\n","5049c06f":"#Intialize wandb run\n\nif config['use_wandb']:\n    import wandb\n    from wandb.keras import WandbCallback\n    from kaggle_secrets import UserSecretsClient\n\n    if config['wandb_mode'] == 'offline':\n        os.environ[\"WANDB_MODE\"] = \"offline\"\n        key='X'*40\n        wandb.login(key=key)\n    else:\n        user_secrets = UserSecretsClient()\n        wandb_api = user_secrets.get_secret(\"wandb_api\")\n        wandb.login(key=wandb_api)\n\n    run = wandb.init(project='iitd-toxic-comment-challenge', \n                     group =config['group'], \n                     job_type='train',\n                     config = config)\n\n    LOGGER.info(\"Wandb is initialized\")\n","075412cb":"df.language.value_counts(normalize=True)*100","a764edd4":"from sklearn.model_selection import train_test_split\n\ndf_train, df_valid = train_test_split(df, \n                                      shuffle=True, \n                                      random_state=config['seed'], \n                                      test_size=config['test_size'], \n                                      stratify=df['language'].values)\n\ndf_train = df_train.reset_index(drop=True)\ndf_valid = df_valid.reset_index(drop=True)\ndf_train.to_csv('df_train.csv', index=False)\ndf_valid.to_csv('df_valid.csv', index=False)\n\nLOGGER.info(\"Data read from disk\")","a59e3acb":"if config['use_wandb']:\n    artifact =  wandb.Artifact(name=\"folds\", type=\"dataset\")\n    artifact.add_file('.\/df_train.csv')\n    artifact.add_file('.\/df_valid.csv')\n\n    LOGGER.info(\"Logging folds.csv to W&B Artifacts\")\n    wandb.log_artifact(artifact)\n","aeeef74c":"tokenizer = AutoTokenizer.from_pretrained(config['model'])\nprint(tokenizer)\n\nLOGGER.info('Tokenizer loaded')","2e0e8f05":"AUTOTUNE = tf.data.experimental.AUTOTUNE","2d73e854":"\nclass ChaiiDataset:\n    def __init__(self, max_length,  tokenizer):\n        self.max_length = max_length\n        self.pad_on_right = tokenizer.padding_side == \"right\"\n        self.tokenizer = tokenizer\n    \n    def run_tokenizer( self, data):\n        tokenized_data = self.tokenizer(\n            list(data['commentText'].values),\n            max_length = self.max_length,\n            padding='max_length',\n            truncation=True\n        )\n        return tokenized_data\n\n\n\n    def prepare_tf_data_pipeline(self, data, batch_size = 16, type='train'):\n#         tokenized_data = []\n#         for i in tqdm(range(0, len(data), batch_size)):\n#             _data = self.run_tokenizer(data[i:i+batch_size])\n#             tokenized_data.extend(_data)\n        tokenized_data = self.run_tokenizer(data)\n        print(\"tokenization done\")\n        \n\n        def map_func(tokenized_data, label):\n            input_ids = tokenized_data['input_ids']\n            token_type_ids = tokenized_data['token_type_ids']\n            attention_mask = tokenized_data['attention_mask']\n            return {'input_ids':input_ids, \n                    'token_type_ids':token_type_ids, \n                    'attention_mask': attention_mask}, \\\n                    label\n\n        def map_func_eval(tokenized_data):\n            input_ids = tokenized_data['input_ids']\n            token_type_ids = tokenized_data['token_type_ids']\n            attention_mask = tokenized_data['attention_mask']\n            return {'input_ids':input_ids, \n                    'token_type_ids':token_type_ids, \n                    'attention_mask': attention_mask}\n                    \n\n\n        \n        if type=='train':\n            dataset_train_raw = tf.data.Dataset.from_tensor_slices((tokenized_data, data['label']))\n            dataset_train = dataset_train_raw.map(map_func) \\\n                            .shuffle(10) \\\n                            .batch(batch_size, drop_remainder=True) \\\n                            .prefetch(buffer_size=AUTOTUNE) \n            return dataset_train\n        \n        if type =='valid':\n            dataset_valid_raw = tf.data.Dataset.from_tensor_slices((tokenized_data, data['label']))\n            dataset_valid = dataset_valid_raw.map(map_func) \\\n                            .batch(batch_size)\n            return dataset_valid            \n\n\n        if type == 'eval':\n            dataset_eval_raw = tf.data.Dataset.from_tensor_slices((tokenized_data))\n            dataset_eval = dataset_eval_raw.map(map_func_eval) \\\n                            .batch(batch_size)\n                \n            return dataset_eval       \n\n","de324482":"dataset = ChaiiDataset(config['max_length'],  tokenizer)","d570e1c5":"train_dataset = dataset.prepare_tf_data_pipeline(df_train)\nLOGGER.info(\"tf.data pipeline for training dataset is created\")\n","f5f08c08":"valid_dataset = dataset.prepare_tf_data_pipeline(df_valid, type='eval')\nLOGGER.info(\"tf.data pipeline for validation dataset is created\")\n","d0d1e1da":"for x, y in train_dataset.take(1):\n    print(x, y)","b552bef4":"def get_keras_model():\n    pretrained_model = TFAutoModel.from_pretrained(config['model'])\n    \n    input_ids = layers.Input(shape=(config['max_length'],),\n                             name='input_ids', \n                             dtype=tf.int32)\n    token_type_ids = layers.Input(shape=(config['max_length'],),\n                                  name='token_type_ids', \n                                  dtype=tf.int32)\n    attention_mask = layers.Input(shape=(config['max_length'],),\n                                  name='attention_mask', \n                                  dtype=tf.int32)\n    embedding = pretrained_model(input_ids, \n                     token_type_ids=token_type_ids, \n                     attention_mask=attention_mask)[0]\n\n   \n\n    x1 = tf.keras.layers.Dropout(0.1)(embedding) \n    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Dense(1, activation='sigmoid')(x1)\n   \n    \n    \n    model = keras.Model(inputs=[input_ids, \n                                token_type_ids, \n                                attention_mask],\n                        outputs=x1)\n    \n    return model","458f2ff5":"model = get_keras_model()\nLOGGER.info(\"Model Loaded\")","f304777f":"if config['use_transfer_learning']:\n    for layer in model.layers:\n        if 'tf_bert_model' in layer.name:\n            layer.trainable = False\n    Logger.info(\"Transfer learning is enabled\")\n","8364ab27":"#taken from old keras source code\n\ndef f1_score(y_true, y_pred): \n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)\/(precision+recall+K.epsilon())\n    return f1_val","00b9e069":"loss = keras.losses.CategoricalCrossentropy()\noptimizer = keras.optimizers.Adam(lr= config['lr'])\nmodel.compile(loss = 'binary_crossentropy', optimizer=optimizer, metrics=[tf.keras.metrics.AUC(), f1_score])","00256ac7":"model.summary()","8ef4171d":"tf.keras.utils.plot_model(model, show_shapes=True,show_dtype=True)","2e54feaa":"if config['use_wandb']:\n    wandb.log({\"model\": wandb.Image('model.png')})","c3c0387c":"def get_callbacks():\n    bm = tf.keras.callbacks.ModelCheckpoint('best_model.h5',\n                                            verbose=1, \n                                            monitor='val_loss', \n                                            mode='min', \n                                            save_best_only=True, \n                                            save_weights_only=True)\n    lm = tf.keras.callbacks.ModelCheckpoint('last_model.h5',\n                                            verbose=1, \n                                            save_best_only=False, \n                                            save_weights_only=True)\n    \n    callbacks = [bm , lm]\n    \n    if config['use_wandb']:\n        callbacks.append( WandbCallback(save_model=False) )\n    return callbacks","b600de5a":"LOGGER.debug(\"Model Training is starting\")\nhistory = model.fit(train_dataset, \n          epochs=config['epochs'], \n          callbacks=get_callbacks(), \n          verbose = 1, \n          validation_data = dataset.prepare_tf_data_pipeline(df_valid, type='valid'))","827b090a":"def plot_hist(hist):\n    plt.figure(figsize=(15,5))\n    local_epochs = len(hist.history[\"loss\"])\n    plt.plot(np.arange(local_epochs, step=1), hist.history[\"loss\"], '-o', label='Train Loss',color='#ff7f0e')\n    plt.plot(np.arange(local_epochs, step=1), hist.history[\"val_loss\"], '-o',label='Val Loss',color='#1f77b4')\n    plt.xlabel('Epoch',size=14)\n    plt.ylabel('Loss',size=14)\n    plt.legend(loc=2)\n    \n    plt.savefig('loss.png')\n    plt.show()\n    \nplot_hist(history)","244888e7":"K.clear_session()\ndel train_dataset\n\nimport gc\ngc.collect()","8f489f69":"## Load best weights\n\nmodel.load_weights('best_model.h5')\nLOGGER.info(\"Best weights loaded\")","08c01dbf":"df_test = pd.read_csv('..\/input\/iiitd-abuse-detection-challenge\/eam2021-test-set-public\/eam2021-test-set-public.csv')\ndf_test.head()","86fba309":"df_test.info()","1616bc62":"test_dataset = dataset.prepare_tf_data_pipeline(df_test, type='eval')\nLOGGER.info(\"Test dataset loaded\")\n","4739bc34":"preds = model.predict(test_dataset, verbose = 1, workers=4)\n\nLOGGER.info(\"Done with predictions on test set\")","e188898b":"pred_test_df = pd.DataFrame({'Id': df_test['Id']})\npred_test_df['Expected'] = preds>0.5\npred_test_df['Expected'] = pred_test_df['Expected'].astype('int')\npred_test_df.to_csv('submission.csv', index=False)\nLOGGER.info(\"Saving of predictions done\")","53afabd4":"pred_test_df.head(10)","f7a86459":"if config['use_wandb']:\n    run.finish()","09729083":"LOGGER.info(\"Done everything. Finished\")","8b2a00d9":"## Stack\n\n1. Pandas for Data Preprocessing\n\n2. tf.data from Tensorflow for Data Pipelines\n\n3. Hugging Face for pretrained model and tokenizer\n\n4. MURIL as the pretrained model\n\n5. Keras for Model training\n\n6. Weights and biases for experiment tracking\n\n![kaggle.png](attachment:d3c64f59-55c8-4f4f-9eff-d82c4e64779e.png)","a437e16f":"<a id=\"section5\"><\/a>\n## Dataset Pipelines","176ebada":"# Table of Contents\n\n1. [Imports](#section1)\n2. [Configuration](#section2)\n3. [Dataset split](#section3)\n4. [Setting Tokenizer](#section4)\n5. [Dataset Pipeline](#section5)\n6. [Postprocessing Pipeline](#section6)\n7. [Creating Dataset](#section7)\n8. [Creating Model](#section8)\n9. [Model Training](#section9)\n10. [Make Predictions on Test Set](#section10)","b751886a":"## Weights and Biases Link: https:\/\/wandb.ai\/harveenchadha\/iitd-toxic-comment-challenge","9c861c44":"![Screenshot from 2021-09-05 15-27-50.png](attachment:a91d27e6-acff-4189-abf6-24314847de65.png)","2bd0ac69":"<a id=\"section1\"><\/a>\n## Imports + Short EDA\n","c12e855b":"<a id=\"section3\"><\/a>\n## Dataset Split","2e989186":"<a id=\"section2\"><\/a>\n## Configuration","5763b61b":"## Further Reading\n\n1. Learning Rate Scheduler\n2. Tokenization on the fly\n\n\n### If you learnt something from this kernel please don't forget to upvote :) ","888f86af":"<a id=\"section10\"><\/a>\n## Make predictions on test set","c2b44fe8":"<a id=\"section9\"><\/a>\n## Model Training","adba7cd8":"<a id=\"section4\"><\/a>\n## Setting Tokenizer","24a4e598":"<a id=\"section8\"><\/a>\n## Creating Model","22ed9e44":"<a id=\"section7\"><\/a>\n## Creating Dataset","9cc928fb":"# MURIL (Multilingual Representations for Indic Languages)\n\n\n<font size=\"3\">\nIn this notebook I try to train model using MURIL pretrained model combined with Hugging Face and Keras.\n\nBut what is MURIL? MURIL (Khanuja et al)(@simrankhanuja) is a multilingual LM specifically built for Indic Languages. Read the full paper [here](https:\/\/arxiv.org\/pdf\/2103.10730.pdf)\n<\/font>\n"}}