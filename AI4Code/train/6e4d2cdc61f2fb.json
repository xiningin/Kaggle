{"cell_type":{"7b60f531":"code","b4157891":"code","6edbdf97":"code","0f056f7e":"code","0ca1ed0a":"code","cb0683df":"code","72e55f9e":"code","7fb63964":"code","88721113":"code","e6b1ae45":"code","044f4c73":"code","2dd5c351":"markdown","c1839975":"markdown","0938fa01":"markdown","2020fdf7":"markdown","cd0e4588":"markdown","7ac07480":"markdown","4144b0bf":"markdown","8bbdbeef":"markdown","8d154a19":"markdown","5b531b5c":"markdown","24da7e77":"markdown"},"source":{"7b60f531":"import numpy as np\nimport keras\nimport tensorflow as tf\nfrom numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import TimeDistributed\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras_tqdm import TQDMNotebookCallback\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nimport random\nfrom random import randint","b4157891":"# split a univariate sequence into samples\ndef split_sequence(sequence, n_steps):\n    X, y = list(), list()\n    for i in range(len(sequence)):\n        # find the end of this pattern\n        end_ix = i + n_steps\n        # check if we are beyond the sequence\n        if end_ix > len(sequence)-1:\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)\n\ndef plot_multi_graph(xAxis,yAxes,title='',xAxisLabel='number',yAxisLabel='Y'):\n    linestyles = ['-', '--', '-.', ':']\n    plt.figure()\n    plt.title(title)\n    plt.xlabel(xAxisLabel)\n    plt.ylabel(yAxisLabel)\n    for key, value in yAxes.items():\n        plt.plot(xAxis, np.array(value), label=key, linestyle=linestyles[randint(0,3)])\n    plt.legend()","6edbdf97":"# define input sequence\nraw_seq = [i for i in range(100)]\n\n# Try the following if randomizing the sequence:\n# random.seed('sam') # set the seed\n# raw_seq = random.sample(raw_seq, 100)\n\n# choose a number of time steps for sliding window from data start to target start\nsliding_window = 20\n\n# split into samples\nX, y = split_sequence(raw_seq, sliding_window)\n\nprint(X)\nprint(y)","0f056f7e":"# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\nn_features = 1\nn_seq = 20\nn_steps = 1\nX = X.reshape((X.shape[0], n_seq, n_steps, n_features))","0ca1ed0a":"# define model\nmodel = Sequential()\nmodel.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features)))\nmodel.add(TimeDistributed(MaxPooling1D(pool_size=1)))\nmodel.add(TimeDistributed(Flatten()))\nmodel.add(LSTM(50, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse', metrics=['mse'])","cb0683df":"early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n# fit model\nhistory = model.fit(X, y, epochs=100, verbose=1, validation_data=(X,y))","72e55f9e":"#Plot Error\n# Mean Square Error\nyAxes = {}\nyAxes[\"Training\"]=history.history['mean_squared_error']\nyAxes[\"Validation\"]=history.history['val_mean_squared_error']\nplot_multi_graph(history.epoch,yAxes, title='Mean Square Error',xAxisLabel='Epochs')","7fb63964":"# demonstrate prediction\nx_input = array([i for i in range(100,120)])\nprint(x_input)\nx_input = x_input.reshape((1, n_seq, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","88721113":"# demonstrate prediction in data\nyhat = model.predict(X, verbose=0)\nprint(yhat)","e6b1ae45":"print(y)","044f4c73":"xAxis = [i for i in range(len(y))]\nyAxes = {}\nyAxes[\"Data\"]=raw_seq[0:len(raw_seq)-sliding_window]\nyAxes[\"Target\"]=y\nyAxes[\"Prediction\"]=yhat\nplot_multi_graph(xAxis,yAxes,title='')","2dd5c351":"<h3 id=\"4-2\">4-2. Training Model<\/h3>\nDefined early stop, can be used in callbacks param of model fit, not using for now since it's not recommended at first few iterations of experimentation with new data","c1839975":"<h1 id=\"6\">6. Complete Figure<\/h1>\nData, Target, Prediction - all in one single graph","0938fa01":"<h1 id=\"1\">1. Import Packages<\/h1>\nImporting all necessary and useful packages in single cell.","2020fdf7":"<h3 id=\"4-3\">4-3. Evaluating Model<\/h3>\nPlotting Training and Validation mean square error","cd0e4588":"<h1 id=\"5\">5. Prediction<\/h1>\n\n<h3 id=\"5-1\">5-1. Single Value Prediction<\/h3>\nPredicting a single value slided 20 (our provided figure for sliding window above) values ahead","7ac07480":"<h1 id=\"2\">2. Helper Functions<\/h1>\nDefining Some helper functions which we will need later in code","4144b0bf":"<h3 id=\"3-2\">3-2. Reshaping Sequence<\/h3>\nReshaping accordingly for CNN Layer","8bbdbeef":"<h3 id=\"5-2\">5-2. Sequence Prediction<\/h3>\nPredicting complete sequence (determining closeness to target) based on data <br \/>\n<i>change variable for any other sequence though<\/i>","8d154a19":"<h1>Notebook Content<\/h1>\n1. [Import Packages](#1)\n1. [Helper Functions](#2)\n1. [Input](#3)\n1. [Model](#4)\n1. [Prediction](#5)\n1. [Complete Figure](#6)","5b531b5c":"<h1 id=\"4\">4. Model<\/h1>\n\n<h3 id=\"4-1\">4-1. Defining Layers<\/h3>\nAdding 1D Convolution, Max Pooling, LSTM and finally Dense (MLP) layer","24da7e77":"<h1 id=\"3\">3. Input<\/h1>\n\n<h3 id=\"3-1\">3-1. Providing Sequence<\/h3>\nTaking a sample input sequence (a simple 1 to 100 here) in *raw_seq*"}}