{"cell_type":{"707e1ce2":"code","bd55100d":"code","8498dd9c":"code","18f783d5":"code","e2d3d37e":"code","81bef1ae":"code","9368eb71":"code","8cd24cbd":"code","ad0156d7":"code","532febdd":"code","0b923dcd":"code","9dbce46e":"code","35d01e6c":"code","28b4d879":"code","1230ed71":"code","03eeda21":"markdown","ea7b9045":"markdown","07dc3258":"markdown","10c540cb":"markdown","e1745b1d":"markdown","484047a1":"markdown","fe2879bb":"markdown","a525072d":"markdown"},"source":{"707e1ce2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nimport lightgbm as lgb\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore')\n\nplt.style.use('seaborn')\nsns.set(font_scale=1)","bd55100d":"random_state = 42\nnp.random.seed(random_state)\ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')","8498dd9c":"df_train.columns","18f783d5":"print(\"Total values in the dataset : {}\".format(df_train['target'].count()))\nOnes = df_train.groupby('target')['target'].count()\nprint(\"% of 1s in total {}\".format(Ones[1]*100.0\/200000))","e2d3d37e":"# Using the mask to filter out 1s . \ny = df_train['target']\ny.head()\nx = df_train[y > 0].copy()\nx.head()","81bef1ae":"def augment(x,y,t=2):\n    xs,xn = [],[]\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xs.append(x1)\n\n    for i in range(t\/\/2):\n        mask = y==0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn])\n    y = np.concatenate([y,ys,yn])\n    return x,y","9368eb71":"lgb_params = {\n    \"objective\" : \"binary\",\n    \"metric\" : \"auc\",\n    \"boosting\": 'gbdt',\n    \"max_depth\" : 4,\n    \"num_leaves\" : 13,\n    \"learning_rate\" : 0.01,\n    \"bagging_freq\": 5,\n    \"bagging_fraction\" : 0.4,\n    \"feature_fraction\" : 0.05,\n    \"min_data_in_leaf\": 80,\n    #\"min_sum_hessian_in_leaf\" : 10,\n    \"tree_learner\": \"serial\",\n    \"boost_from_average\": \"false\",\n    #\"lambda_l1\" : 5,\n    #\"lambda_l2\" : 5,\n    \"bagging_seed\" : random_state,\n    \"verbosity\" : 1,\n    \"seed\": random_state\n}","8cd24cbd":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\noof = df_train[['ID_code', 'target']]\noof['predict'] = 0\npredictions = df_test[['ID_code']]\nval_aucs = []\nfeature_importance_df = pd.DataFrame()","ad0156d7":"features = [col for col in df_train.columns if col not in ['target', 'ID_code']]\nX_test = df_test[features].values","532febdd":"for fold, (trn_idx, val_idx) in enumerate(skf.split(df_train, df_train['target'])):\n    X_train, y_train = df_train.iloc[trn_idx][features], df_train.iloc[trn_idx]['target']\n    X_valid, y_valid = df_train.iloc[val_idx][features], df_train.iloc[val_idx]['target']\n    \n    N = 5\n    p_valid,yp = 0,0\n    for i in range(N):\n        X_t, y_t = augment(X_train.values, y_train.values)\n        X_t = pd.DataFrame(X_t)\n        X_t = X_t.add_prefix('var_')\n    \n        trn_data = lgb.Dataset(X_t, label=y_t)\n        val_data = lgb.Dataset(X_valid, label=y_valid)\n        evals_result = {}\n        lgb_clf = lgb.train(lgb_params,\n                        trn_data,\n                        100000,\n                        valid_sets = [trn_data, val_data],\n                        early_stopping_rounds=3000,\n                        verbose_eval = 1000,\n                        evals_result=evals_result\n                       )\n        p_valid += lgb_clf.predict(X_valid)\n        yp += lgb_clf.predict(X_test)\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = lgb_clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    oof['predict'][val_idx] = p_valid\/N\n    val_score = roc_auc_score(y_valid, p_valid)\n    val_aucs.append(val_score)\n    \n    predictions['fold{}'.format(fold+1)] = yp\/N","0b923dcd":"#%precision 3","9dbce46e":"print(\"Distribution of 1s in original data : {} \/ {} \".format(np.sum(y_train) , len(y_train)))\nprint(\"Percentage of 1s in original data : {}\".format(np.sum(y_train)*100.0\/len(y_train)))\n\n\nprint(\"Percentage of 1s in augmented data : {}\".format(np.sum(y_t)*100.0\/len(y_t)))\nprint(\"Distribution of 1s in augmented data : {} \/ {} \".format(np.sum(y_t) , len(y_t)))\n","35d01e6c":"mean_auc = np.mean(val_aucs)\nstd_auc = np.std(val_aucs)\nall_auc = roc_auc_score(oof['target'], oof['predict'])\nprint(\"Mean auc: %.9f, std: %.9f. All auc: %.9f.\" % (mean_auc, std_auc, all_auc))","28b4d879":"cols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(14,26))\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('LightGBM Features (averaged over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","1230ed71":"# submission\npredictions['target'] = np.mean(predictions[[col for col in predictions.columns if col not in ['ID_code', 'target']]].values, axis=1)\npredictions.to_csv('lgb_all_predictions.csv', index=None)\nsub_df = pd.DataFrame({\"ID_code\":df_test[\"ID_code\"].values})\nsub_df[\"target\"] = predictions['target']\nsub_df.to_csv(\"lgb_submission.csv\", index=False)\noof.to_csv('lgb_oof.csv', index=False)","03eeda21":"## Hence this technique is more like oversampling , but , here we oversample BOTH classses , rather than just one.      ","ea7b9045":"## As one can see , there is a class imbalance. \n### Now , how do we solve it ? \n\n### Plan 1 : Oversampling \/ Undersampling -> \n* In this strategy , we either increase or decrease the number of samples by duplicating the smaller class or removing the majority class elements to make them equal or similar\n* The risk involved is that we may change the original distribution of the data . \n\n### Plan 2 : Follow below ----->","07dc3258":"## So How the augmentation was done ? \n\n    * X : Original  : 200,000\n    * Xs : Ones     ~  20,000\n    * Xn : Zeros    ~ 180,000  \n\n    ### X_Final = X + 3\\*Xs + 2\\*Xn\n","10c540cb":"<h1><center><font size=\"6\">Santander EDA, PCA and Light GBM Classification Model<\/font><\/center><\/h1>\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/4\/4a\/Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg\/640px-Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg\"><\/img>\n\n<br>\n<b>\nIn this challenge, Santander invites Kagglers to help them identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data they have available to solve this problem. \nThe data is anonimyzed, each row containing 200 numerical values identified just with a number.<\/b>\n\n<b>Inspired by Jiwei Liu's Kernel. I added Data Augmentation Segment to my kernel<\/b>\n\n### I will not be covering EDA in this kernel . I'd keep it short as the data is completely anonimized and all columns are just pure numbers, giving almost no insight . \nhttps:\/\/www.kaggle.com\/roydatascience\/eda-pca-lgbm-santander-transactions  You can check for EDA here","e1745b1d":"This is how we filter using masks ","484047a1":"Proof : ? \n\n1s = 20k(X) + 3\\*20k(Xs) = 80k\nTotal = 20k(X) + 3\\*20k(Xs) + 2\\*180k(Xn)\n\ni.e. , \n    80k \/ 520 k = \n    14.6 % (Approx. , the one we found above)","fe2879bb":"# Let's see what this augmentation does to our data frame","a525072d":"## If you found this kernel useful , please upvote ><"}}