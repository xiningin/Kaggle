{"cell_type":{"4088456f":"code","a4ee97e2":"code","0cf059a2":"code","1ae36cec":"code","55038ec9":"code","20d89daa":"code","ce152eda":"code","2dfc59e3":"code","b430b3c7":"code","ddd8cb3a":"code","395d18e5":"code","faf1cc17":"code","30a16b14":"code","7ed09ffa":"code","98acee3d":"code","1a23b28c":"code","5dfe9df5":"code","bdb6a7a2":"code","29b68a1f":"code","bfe3a7bd":"code","6b4a072f":"code","ed9f6714":"code","40a9555e":"code","2a09390b":"code","aa0bb684":"code","fcdc3a4a":"code","dbd79712":"code","f5148085":"code","a99e731d":"code","55775c23":"code","f229ccf2":"code","cf675358":"code","9ad6f50d":"code","32ea199a":"code","ebc3b0f6":"code","bfe51e5a":"code","f9d9c317":"code","b99f159d":"code","0109c6a1":"code","6d618443":"code","7ae83407":"code","d8159e88":"code","dd733b2c":"code","a3ddc0c9":"code","d247fe09":"code","eba33374":"code","e5b49e1a":"code","225228f1":"code","3c30914a":"code","683240e2":"code","57d614ab":"code","6e528123":"code","be3913a0":"code","b64d5e5c":"code","2fd72f58":"code","96218b74":"code","6c20aa8b":"code","eaf6a0fb":"code","aeb5cbd9":"code","5270e9a3":"code","71540742":"code","00d6f933":"code","5fccd13c":"code","5dfa5127":"code","137e8487":"code","3b89fc86":"code","e451d911":"code","8980aa81":"code","c50a2e2c":"code","0f56b03a":"code","0fd14de2":"code","d3827269":"code","fec04e89":"code","1315373f":"code","fbd4eb80":"code","70e0c1d3":"code","5f6dd912":"code","2af40f15":"code","f964b464":"code","586d5efa":"code","bc55d4c6":"code","2f1c2d53":"code","1dc1476b":"code","73a253e6":"code","128da43b":"code","f8ab1188":"code","31e3f753":"code","6278cf5b":"code","57812244":"code","be4d9b15":"code","30aa3527":"code","66ccdd2e":"code","2d8e04db":"code","62a98eba":"code","17741fec":"code","837ef846":"code","c6837b6d":"code","4ea0787b":"code","c08a26f0":"code","cf85d683":"code","4924430c":"code","bd8f3eed":"code","dbe820bc":"code","6c10a5c3":"code","d2d4954a":"code","e9e1141b":"code","327e1c28":"code","445387b1":"code","bc37a9ae":"code","e00b52ff":"code","0e327471":"code","d165b78d":"code","b4a70284":"code","18e45e11":"code","78cb6243":"code","0b4636a2":"code","31fca224":"code","7b2b5d3b":"code","e692eb8a":"code","b1ee7fc1":"code","688048b9":"code","ab7887e9":"code","1733f36b":"code","86ef4d0e":"code","378e11d0":"code","4b6944c2":"code","2f77eb23":"code","7b6f5202":"code","04fc9bfc":"code","8d1349a8":"code","0fd1208a":"code","33c9ffc4":"code","87a1fb90":"code","b2673d46":"code","3be84a72":"code","30d910ef":"code","ebdc251d":"code","94cabb39":"code","1973007b":"code","e301859a":"code","796816f6":"code","cc7dc9f1":"code","c159e844":"code","dd7c92bf":"code","89291a30":"code","fe1eb6a1":"code","663918b5":"code","c12004cf":"code","c1dd1d6d":"code","2121fdf4":"code","a160b9da":"code","cebc1752":"code","04449b73":"code","6bb4a18a":"code","edaa4570":"code","cfe5f93c":"code","613a0b63":"code","2cbeaf1a":"code","13313807":"code","d3d52fb2":"code","b34af75c":"code","85f4c9b6":"code","56b86ac3":"markdown","ad4efc16":"markdown","c4f0acb5":"markdown","48ee18bb":"markdown","ae80d37d":"markdown","281f62c1":"markdown","18b862b1":"markdown","069941e5":"markdown","ac4a2e00":"markdown","1b4184a3":"markdown","599b9302":"markdown","b32a4fd3":"markdown","ed46eb64":"markdown","48836e2f":"markdown","93688816":"markdown","1775ab0c":"markdown","f96cf945":"markdown","0a00a3e2":"markdown","85a93fdc":"markdown","5ddfa21e":"markdown","3ea762d6":"markdown","03ef7bd6":"markdown","bd9ee626":"markdown","850949e9":"markdown","fa9ca040":"markdown","f511bb7b":"markdown","010aaffd":"markdown","a665498d":"markdown","eb466cbb":"markdown","8e656ccb":"markdown","291d3dc8":"markdown","96e64c1a":"markdown","4aacfb15":"markdown","5c4de61e":"markdown","b5e9dfd4":"markdown","7c0df18e":"markdown","8be717b9":"markdown","fd2d71be":"markdown","e49bf46d":"markdown","30ec49c2":"markdown","212287d6":"markdown","f3d690ee":"markdown","ac75bf07":"markdown","a2cd2da4":"markdown","6a6c50a5":"markdown","461343ae":"markdown","44190e54":"markdown","0d5c4b34":"markdown","bd46b777":"markdown","737a59e3":"markdown","dc1ac193":"markdown","0ef7043a":"markdown","932a9ce4":"markdown","c00f492f":"markdown","f3209b9a":"markdown","00af94a1":"markdown","f1b160ea":"markdown","2290eb87":"markdown","85ee8a12":"markdown","29bdc705":"markdown","336819ff":"markdown","8b66886c":"markdown","15e87b47":"markdown","758788fd":"markdown","af62437c":"markdown","f514904f":"markdown","6a5d4834":"markdown","77ec5dca":"markdown","f17c1e70":"markdown","eea650fa":"markdown","f3c32109":"markdown","71b27158":"markdown","35dbfebb":"markdown","254e9eb9":"markdown","cdcef2f4":"markdown","8c3fdb3b":"markdown","4cd50aa2":"markdown","b5070fa7":"markdown","862f3b87":"markdown","f4e9f179":"markdown","7b9f534b":"markdown","bbde0054":"markdown","cfefb74c":"markdown","14e86810":"markdown","fed9675e":"markdown","bb2aae3b":"markdown","85ec45f8":"markdown","662d28ae":"markdown","c19b467c":"markdown","d798533d":"markdown","561f7d3c":"markdown","db8726e7":"markdown","df1aa471":"markdown"},"source":{"4088456f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a4ee97e2":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","0cf059a2":"data_df=pd.read_csv(r'\/kaggle\/input\/nlp-getting-started\/train.csv')\ndata_df.head()","1ae36cec":"data_df.info()","55038ec9":"data_df.shape","20d89daa":"data_df.drop_duplicates(keep=False,inplace=True)\ndata_df.shape","ce152eda":"data_df['target'].value_counts(normalize=True)","2dfc59e3":"plt.figure(figsize=(20,5))\nplt.subplot(1,2,1)\nsns.countplot(x='target',data=data_df)\nplt.xticks(np.arange(2), ('False Tweets','Real Disaster Tweets'))\nplt.title('Distribution of Tweets')\n\nplt.subplot(1,2,2)\nsizes=[x for x in data_df['target'].value_counts()]\nstatus='False Tweets','Real Disaster Tweets'\nplt.pie(sizes,labels=status,wedgeprops=dict(width=0.5),startangle=90,autopct='%1.2f%%')\nplt.legend()\nplt.title('Distribution of Tweets')\nplt.show()","b430b3c7":"unq_ids=data_df['id'].nunique()\nprint(f\"number of unique ids:{unq_ids}\")","ddd8cb3a":"def grpcount(ftr):\n    temp=pd.DataFrame(data_df.groupby(ftr)['target'].agg(lambda x: x.eq(1).sum())).reset_index()\n    temp['total']=pd.DataFrame(data_df.groupby(ftr)['target'].agg({'total':'count'})).reset_index()['total']\n    temp['Avg']=pd.DataFrame(data_df.groupby(ftr)['target'].agg({'Avg':'mean'})).reset_index()['Avg']\n    temp.rename(columns={'target':'disaster_tweets'},inplace=True)\n    temp.sort_values(by=['Avg'],inplace=True, ascending=False)\n    return temp","395d18e5":"def plotftr(df,ftr,avg_ul,avg_lwl,min_occrnc,title):\n    Imp_ftrs=df.loc[(df['Avg']>avg_ul)&(df['total']>min_occrnc),[ftr,'Avg']][0:20]\n    plt.figure(figsize=(20,5))\n    plt.subplot(1,2,1)\n    sns.barplot(x=Imp_ftrs[ftr],y=temp_df['Avg'],data=temp_df)\n    plt.xticks(rotation=90)\n    #reference->https:\/\/stackoverflow.com\/questions\/3899980\/how-to-change-the-font-size-on-a-matplotlib-plot\n    plt.rc('xtick',labelsize=12)\n    plt.rc('axes', labelsize=15)\n    plt.rc('axes', titlesize=15)\n    plt.title(title+' with highest% of real disaster tweets')\n    \n    Imp_ftrs=df.loc[(df['Avg']<avg_lwl),[ftr,'Avg']][-20:]\n    plt.subplot(1,2,2)\n    sns.barplot(x=Imp_ftrs[ftr],y=temp_df['Avg'],data=temp_df)\n    plt.xticks(rotation=90)\n    plt.rc('xtick',labelsize=12)\n    plt.rc('axes', labelsize=15)\n    plt.rc('axes', titlesize=15)\n    plt.title(title+' with lowest% of real disaster tweets')\n    plt.show()","faf1cc17":"temp_df=grpcount('keyword')\nplotftr(temp_df,'keyword',0.7,0.4,5,'keywords')","30a16b14":"temp_df=grpcount('location')\nplotftr(temp_df,'location',0.7,0.4,5,'locations')","7ed09ffa":"lst=data_df.columns[0:3]\nnull_key=[]\nnull_percnt=[]\nfor i in lst: \n    null_percnt.append((data_df[i].isna().sum()\/data_df.shape[0])*100)\n    null_key.append(i)\nnull_dct=dict((key,val) for key,val in zip(null_key,null_percnt))\nnull_dct_sorted=dict(sorted(null_dct.items(),key=lambda kv:kv[1]))\nplt.bar(null_dct_sorted.keys(),null_dct_sorted.values())\nplt.xlabel('Features')\nplt.ylabel('Null_value percentage')\nplt.title('Features vs Null_values')\nplt.show()","98acee3d":"data_df['keyword']=data_df['keyword'].fillna(data_df['keyword'].mode()[0])\ndata_df['location']=data_df['location'].fillna(data_df['location'].mode()[0])","1a23b28c":"data_df.isna().sum().max()","5dfe9df5":"data_df['location'][200:230]","bdb6a7a2":"data_df.drop(['location'],axis=1,inplace=True)","29b68a1f":"data_df['keyword'].value_counts()[:20]","bfe3a7bd":"#reference-->https:\/\/towardsdatascience.com\/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79\n#https:\/\/medium.com\/@jorlugaqui\/how-to-strip-html-tags-from-a-string-in-python-7cb81a2bbf44","6b4a072f":"import re\ndef htmlremover(data,ftr):\n    clean=[]\n    for sentance in data[ftr].values:\n        #remove html tags\n        pattern=re.compile(r'<.*?>')\n        sentance=re.sub(pattern,'',sentance)\n        #remove urls\n        pattern1=re.compile(r'http\\S+')\n        clean.append(re.sub(pattern1,'',sentance))\n    return clean        ","ed9f6714":"##eg:\ntmp=['<br \/>http:\/\/www.amazon.com\/VICTOR-FLY-MAGNET-BAIT-REFILL\/dp\/B00004RBDY<br \/><br \/>The Victor M380 and M502 traps are unreal, of course -- total fly genocide. Pretty stinky, but only right nearby.']\ntmp_df=pd.DataFrame()\ntmp_df['txt']=tmp\ntmp_df","40a9555e":"cleaned_tmp=htmlremover(tmp_df,'txt')\ntmp_df['txt']=cleaned_tmp\ntmp_df","2a09390b":"import unicodedata\ndef remove_accented_chars(data,ftr):\n    clean=[]\n    for sentance in data[ftr].values:\n        clean_sent = unicodedata.normalize('NFKD', sentance).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n        clean.append(clean_sent)\n    return clean      ","aa0bb684":"##eg:\ntmp=['S\u00f3m\u011b \u00c1cc\u011bnt\u011bd t\u011bxt this is a paragraph.']\ntmp_df=pd.DataFrame()\ntmp_df['txt']=tmp","fcdc3a4a":"clean=remove_accented_chars(tmp_df,'txt')","dbd79712":"clean","f5148085":"#ref:https:\/\/stackoverflow.com\/questions\/33404752\/removing-emojis-from-a-string-in-python\ndef emojiremover(data,ftr):\n    clean=[]\n    for sentance in data[ftr].values:\n        emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n        clean.append(re.sub(emoji_pattern,'',sentance))\n    return clean","a99e731d":"##eg:\ntmp=['Life is \ud83d\udc95 beautiful.Be \ud83d\udc6d happy']\ntmp_df=pd.DataFrame()\ntmp_df['txt']=tmp\ntmp_df","55775c23":"clean=remove_accented_chars(tmp_df,'txt')\nclean","f229ccf2":"# https:\/\/stackoverflow.com\/a\/47091490\/4084039\nimport re\n\ndef decontracted(data,ftr):\n    clean=[]\n    for sentance in data[ftr].values:\n        # specific\n        sentance = re.sub(r\"won't\", \"will not\", sentance)\n        sentance = re.sub(r\"can\\'t\", \"can not\", sentance)\n\n        # general\n        sentance = re.sub(r\"n\\'t\", \" not\",sentance)\n        sentance = re.sub(r\"\\'re\", \" are\",sentance)\n        sentance = re.sub(r\"\\'s\", \" is\", sentance)\n        sentance = re.sub(r\"\\'d\", \" would\",sentance)\n        sentance = re.sub(r\"\\'ll\", \" will\",sentance)\n        sentance = re.sub(r\"\\'t\", \" not\",sentance)\n        sentance = re.sub(r\"\\'ve\", \" have\",sentance)\n        sentance = re.sub(r\"\\'m\", \" am\",sentance)\n        clean.append(sentance)\n    return clean","cf675358":"##eg:\ntmp=[\"We \\'re strong.we \\'ll win\"]\ntmp_df=pd.DataFrame()\ntmp_df['txt']=tmp\ntmp_df","9ad6f50d":"clean=decontracted(tmp_df,'txt')\nclean","32ea199a":"def remove_special_characters(data,ftr):\n    clean=[]\n    for sentance in data[ftr].values:\n        pattern = r'[^a-zA-z0-9\\s]'\n        clean.append(re.sub(pattern, '',sentance))\n    return clean\n\n#remove_special_characters(\"Well this was fun! What do you think? 123#@!\", \n                          #remove_digits=True)","ebc3b0f6":"def lowercase(data,ftr):\n    data[ftr]=data[ftr].map(lambda x:x.lower())\n    return data","bfe51e5a":"def whitespace_remover(data,ftr):\n    clean=[]\n    for sent in data[ftr].values:\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        clean.append(sent)\n    return clean","f9d9c317":"#import nltk\n#nltk.download('wordnet')","b99f159d":"\"\"\"from nltk.tokenize import word_tokenize \nfrom nltk.stem import WordNetLemmatizer\nlemmatizer=WordNetLemmatizer()\ndef lemmatize(data,ftr):\n    clean=[]\n    for para in data[ftr].values:\n        clean_sent=[]\n        sent_lst=[sentance for sentance in para.split('.')]\n        for sentance in sent_lst :\n            wrd_lst=nltk.word_tokenize(sentance)\n            sent=' '.join(lemmatizer.lemmatize(word) for word in wrd_lst)\n            clean_sent.append(sent.strip())\n        para1='.'.join(sentance.strip() for sentance in clean_sent)\n        clean.append(para1)\n    return clean\"\"\"   ","0109c6a1":"from nltk.tokenize import word_tokenize \nfrom nltk.stem import WordNetLemmatizer\nlemmatizer=WordNetLemmatizer()\ndef lemmatize(data,ftr):\n    clean=[]\n    for sentance in data[ftr].values:\n        wrd_lst=word_tokenize(sentance)\n        sent=' '.join(lemmatizer.lemmatize(word) for word in wrd_lst)\n        clean.append(sent)\n    return clean","6d618443":"##eg:\ntmp=[\"rocks fall on his feet\"]\ntmp_df=pd.DataFrame()\ntmp_df['txt']=tmp\ntmp_df","7ae83407":"cln=lemmatize(tmp_df,'txt')\ntmp_df['txt']=cln\ntmp_df","d8159e88":"# https:\/\/gist.github.com\/sebleier\/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"])","dd733b2c":"def stopword_removing(data,ftr):\n    clean=[]\n    for sentance in data[ftr].values:\n        wrd_lst=word_tokenize(sentance)\n        sent=' '.join(word for word in wrd_lst if word not in stopwords)\n        clean.append(sent.strip())\n    return clean","a3ddc0c9":"##eg:\ntmp=[\"this pasta is very tasty\"]\ntmp_df=pd.DataFrame()\ntmp_df['txt']=tmp\ntmp_df","d247fe09":"cln=stopword_removing(tmp_df,'txt')\ntmp_df['txt']=cln\ntmp_df","eba33374":"def text_normalizing(data,ftr):\n    #removing urls and html tags\n    cleaned_column=htmlremover(data,ftr)\n    data[ftr]=cleaned_column\n\n    #removing accented characters\n    cleaned_column=remove_accented_chars(data,ftr)\n    data[ftr]=cleaned_column\n\n    #removing emojis\n    cleaned_column=emojiremover(data,ftr)\n    data[ftr]=cleaned_column\n\n    #decontracting words\n    cleaned_column=decontracted(data,ftr)\n    data[ftr]=cleaned_column\n\n    #remove special characters\n    cleaned_column=remove_special_characters(data,ftr)\n    data[ftr]=cleaned_column\n\n    #convert to lowercase\n    data=lowercase(data,ftr)\n\n    #white-space remover\n    cleaned_column=whitespace_remover(data,ftr)\n    data[ftr]=cleaned_column\n\n    #lemmatization\n    cleaned_column=lemmatize(data,ftr)\n    data[ftr]=cleaned_column\n\n    #stemming\n    #cleaned_column=stemming(data_df,'text')\n    #data_df['text']=cleaned_column\n\n    #removing stopwords\n    cleaned_column=stopword_removing(data,ftr)\n    data[ftr]=cleaned_column\n    \n    return data\n","e5b49e1a":"data_df=text_normalizing(data_df,'text')\ndata_df.head()","225228f1":"data_df.drop(['id'],axis=1,inplace=True)\ndata_df.head(2)","3c30914a":"print(f\"Shape of data matrix after text_normalization is : {data_df.shape}\")","683240e2":"base_data=data_df\nY=data_df['target']\ndata_df.drop(['target'],axis=1,inplace=True)\nX=data_df","57d614ab":"X.head(2)","6e528123":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.33,stratify=Y)","be3913a0":"print(f\"Shape of train data:{X_train.shape}\")\nprint(f\"Shape of test data:{X_test.shape}\")","b64d5e5c":"\"\"\"Encoding tweets-text\"\"\"\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(ngram_range=(2,2),min_df=10)\nvectorizer.fit(X_train['text'].values) \ntrain_text_tf=vectorizer.transform(X_train['text'].values)\ntest_text_tf=vectorizer.transform(X_test['text'].values)","2fd72f58":"print(\"Data after vectorization\")\nprint(f\"Train data-:{train_text_tf.shape},Train data-target-:{Y_train.shape}\")\nprint(f\"Test data-:{test_text_tf.shape},Test data-target-:{Y_test.shape}\")","96218b74":"from sklearn.feature_extraction.text import CountVectorizer\ndef wordvectorizer(train,test):\n    vectorizer=CountVectorizer(lowercase=False,binary=True)\n    vectorizer.fit(train.values)\n    one_hot_train=vectorizer.transform(train.values)\n    one_hot_test=vectorizer.transform(test.values)\n    return one_hot_train,one_hot_test","6c20aa8b":"\"\"\"Vectorizing Keyword -One hot encoding\"\"\"\nkeyword_one_hot_X_train,keyword_one_hot_X_test=\\\nwordvectorizer(X_train['keyword'],X_test['keyword'])\nprint(\"Data after vectorizations\")\nprint(keyword_one_hot_X_train.shape,Y_train.shape)\nprint(keyword_one_hot_X_test.shape,Y_test.shape)","eaf6a0fb":"from scipy.sparse import hstack\nX_tr=hstack((train_text_tf,keyword_one_hot_X_train)).tocsr()\nX_te=hstack((test_text_tf,keyword_one_hot_X_test)).tocsr()\nprint(\"Data after stacking\")\nprint(f\"Train data-:{X_tr.shape},Train data-Target:{Y_train.shape}\")\nprint(f\"Test data- X_data:{X_te.shape},Test data-Target:{Y_test.shape}\")","aeb5cbd9":"from sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve,auc\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5270e9a3":"h_para=[0.001,0.01,0.1,1,10,100]","71540742":"def tuning(dataX,dataY,mdl):\n    train_auc=[]\n    cv_auc=[]\n    if mdl==0:\n        para={'C':[0.001,0.01,0.1,1,10,100],'max_iter':[10]}\n        lr=LogisticRegression()\n        lr_model=GridSearchCV(lr,para,scoring='roc_auc',return_train_score=True)\n        lr_model.fit(dataX,dataY)\n\n        train_auc= lr_model.cv_results_['mean_train_score']\n        cv_auc= lr_model.cv_results_['mean_test_score']\n        return train_auc,cv_auc,lr_model\n    else:\n        if mdl==1:\n            para={'max_depth':[2,3,4,5,6,8,10],'n_estimators':[50,100,200,300,1000]}\n            lr=XGBClassifier(criterion='gini',class_weight='balanced')\n            lr_model=GridSearchCV(lr,para,scoring='roc_auc',return_train_score=True)\n            lr_model.fit(dataX,dataY)\n            #reference-https:\/\/towardsdatascience.com\/using-3d-visualizations-to-tune-hyperparameters-of-ml-models-with-python-ba2885eab2e9\n            scores=pd.DataFrame(lr_model.cv_results_).groupby(['param_max_depth','param_n_estimators']).max().unstack()[['mean_test_score','mean_train_score']]\n            return train_auc,cv_auc,lr_model,scores","00d6f933":"def semiloggraph(train_auc,cv_auc,prfrm):\n    if prfrm==1:\n        plt.semilogx(h_para,train_auc,label='Train AUC')\n        plt.semilogx(h_para,cv_auc,label='CV AUC')\n        plt.xlabel('Hyper-Parameter')\n        plt.ylabel('AUC')\n        plt.title('AUC v\/s Hyper-Parameter')\n        plt.legend()\n        plt.show()","5fccd13c":"def parameters(data_tr,data_te,data_Ytr,data_Yte,best_para,para,mdl):\n    if mdl==0:\n        lr=LogisticRegression(C=best_para)\n        lr.fit(data_tr,data_Ytr)\n        y_trprob=lr.predict_proba(data_tr)[:,1]\n        y_teprob=lr.predict_proba(data_te)[:,1]\n        y_trpred=lr.predict(data_tr)\n        y_tepred=lr.predict(data_te)\n\n        tr_fpr,tr_tpr,tr_thresholds=roc_curve(data_Ytr,y_trprob)\n        te_fpr,te_tpr,te_thresholds=roc_curve(data_Yte,y_teprob)\n        return lr,tr_fpr,tr_tpr,te_fpr,te_tpr,tr_thresholds,y_trprob,y_teprob ,y_trpred,y_tepred\n    else:\n        if mdl==1:\n            lr=XGBClassifier(criterion='gini',class_weight='balanced',max_depth=para['max_depth'],n_estimators=para['n_estimators'])\n            lr.fit(data_tr,data_Ytr)\n            y_trprob=lr.predict_proba(data_tr)[:,1]\n            y_teprob=lr.predict_proba(data_te)[:,1]\n            y_trpred=lr.predict(data_tr)\n            y_tepred=lr.predict(data_te)\n\n            tr_fpr,tr_tpr,tr_thresholds=roc_curve(data_Ytr,y_trprob)\n            te_fpr,te_tpr,te_thresholds=roc_curve(data_Yte,y_teprob)\n            return lr,tr_fpr,tr_tpr,te_fpr,te_tpr,tr_thresholds,y_trprob,y_teprob ,y_trpred,y_tepred\n\n","5dfa5127":"def roc_plot(tr_fpr,tr_tpr,te_fpr,te_tpr):\n    plt.plot(tr_fpr,tr_tpr,label=\"Train AUC=\"+str(auc(tr_fpr,tr_tpr)))\n    plt.plot(te_fpr,te_tpr,label=\"Test AUC=\"+str(auc(te_fpr,te_tpr)))\n    plt.xlabel('FPR')\n    plt.ylabel('TPR')\n    plt.title('AUC')\n    plt.legend()\n    plt.show()","137e8487":"train_auc,cv_auc,lr_model=tuning(X_tr,Y_train,0)","3b89fc86":"semiloggraph(train_auc,cv_auc,1)","e451d911":"param=lr_model.best_params_\nprint(f\"best parameters:{param}\")","8980aa81":"best_para=param['C']","c50a2e2c":"best_para","0f56b03a":"para=0","0fd14de2":"lr,tr_fpr,tr_tpr,te_fpr,te_tpr,tr_thresholds,y_trprob,y_teprob,y_trpred,y_tepred=\\\nparameters(X_tr,X_te,Y_train,Y_test,best_para,para,0) ","d3827269":"roc_plot(tr_fpr,tr_tpr,te_fpr,te_tpr)","fec04e89":"from sklearn.metrics import confusion_matrix\nconfusion_matrix_tr=confusion_matrix(Y_train,y_trpred)\nconfusion_matrix_te=confusion_matrix(Y_test,y_tepred)\nprint(\"****Confusion matrix_Train****\")\nprint(confusion_matrix_tr)","1315373f":"print(\"****Confusion matrix_Test****\")\nprint(confusion_matrix_te)","fbd4eb80":"from sklearn.metrics import f1_score\nmdl1_f1=f1_score(Y_test,y_tepred)\nprint(f\"F1 Score:{mdl1_f1}\")","70e0c1d3":"train_auc,cv_auc,lr_model,scores=tuning(X_tr,Y_train,1)","5f6dd912":"semiloggraph(train_auc,cv_auc,0)","2af40f15":"para=lr_model.best_params_","f964b464":"para","586d5efa":"best_para=0","bc55d4c6":"lr,tr_fpr,tr_tpr,te_fpr,te_tpr,tr_thresholds,y_trprob,y_teprob,y_trpred,y_tepred=\\\nparameters(X_tr,X_te,Y_train,Y_test,best_para,para,1) ","2f1c2d53":"roc_plot(tr_fpr,tr_tpr,te_fpr,te_tpr)","1dc1476b":"from sklearn.metrics import confusion_matrix\nconfusion_matrix_tr=confusion_matrix(Y_train,y_trpred)\nconfusion_matrix_te=confusion_matrix(Y_test,y_tepred)\nprint(\"****Confusion matrix_Train****\")\nprint(confusion_matrix_tr)","73a253e6":"print(\"****Confusion matrix_Test****\")\nprint(confusion_matrix_te)","128da43b":"mdl2_f1=f1_score(Y_test,y_tepred)\nprint(f\"F1 Score:{mdl2_f1}\")","f8ab1188":"\"\"\"Adding Sentiment scores of Keywords\"\"\"\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n#reference-https:\/\/stackoverflow.com\/questions\/42212810\/tqdm-in-jupyter-notebook\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n\n# import nltk\n#nltk.download('vader_lexicon')\ndef sent_analyser(data,ftr):\n    neg=[]\n    neu=[]\n    pos=[]\n    com=[]\n    data_f=pd.DataFrame()\n    sid = SentimentIntensityAnalyzer()\n    for tweet in tqdm(data[ftr].values):\n        ss = sid.polarity_scores(tweet)\n        temp=[]\n        for k in ss:\n            temp.append(ss[k])\n        neg.append(temp[0])\n        neu.append(temp[1])\n        pos.append(temp[2])\n        com.append(temp[3])\n    data_f['neg']=neg\n    data_f['pos']=pos\n    data_f['neu']=neu    \n    data_f['com']=com\n    print(ss)\n    return data_f\n\n# we can use these 4 things as features\/attributes (neg, neu, pos, compound)\n# neg: 0.0, neu: 0.753, pos: 0.247, compound: 0.93\"\"\"","31e3f753":"X_train.head(3)","6278cf5b":"X_train_sent=sent_analyser(X_train,'text')\nX_test_sent=sent_analyser(X_test,'text')\nX_test_sent.head(3)","57812244":"from scipy.sparse import hstack\nX_tr=hstack((train_text_tf,keyword_one_hot_X_train,X_train_sent)).tocsr()\nX_te=hstack((test_text_tf,keyword_one_hot_X_test,X_test_sent)).tocsr()\nprint(f\"Train data-:{X_tr.shape},Train data-Target:{Y_train.shape}\")\nprint(f\"Test data- X_data:{X_te.shape},Test data-Target:{Y_test.shape}\")","be4d9b15":"train_auc,cv_auc,lr_model=tuning(X_tr,Y_train,0)","30aa3527":"semiloggraph(train_auc,cv_auc,1)","66ccdd2e":"param=lr_model.best_params_\nprint(f\"best parameters:{param}\")","2d8e04db":"best_para=param['C']","62a98eba":"best_para","17741fec":"para=0","837ef846":"lr,tr_fpr,tr_tpr,te_fpr,te_tpr,tr_thresholds,y_trprob,y_teprob,y_trpred,y_tepred=\\\nparameters(X_tr,X_te,Y_train,Y_test,best_para,para,0) ","c6837b6d":"roc_plot(tr_fpr,tr_tpr,te_fpr,te_tpr)","4ea0787b":"from sklearn.metrics import confusion_matrix\nconfusion_matrix_tr=confusion_matrix(Y_train,y_trpred)\nconfusion_matrix_te=confusion_matrix(Y_test,y_tepred)\nprint(\"****Confusion matrix_Train****\")\nprint(confusion_matrix_tr)","c08a26f0":"print(\"****Confusion matrix_Test****\")\nprint(confusion_matrix_te)","cf85d683":"mdl3_f1=f1_score(Y_test,y_tepred)\nprint(f\"F1 Score:{mdl3_f1}\")","4924430c":"from sklearn.neighbors import KNeighborsClassifier\ntrain_auc = []\ncv_auc = []\nk_range = [1,11,31,51,71,91]\nparam_grid = dict(n_neighbors = k_range)\n\nneigh = KNeighborsClassifier()\nneigh_model=GridSearchCV(neigh,param_grid,cv=5,scoring='roc_auc',return_train_score=True)\nneigh_model.fit(X_tr,Y_train)\n\ntrain_auc= neigh_model.cv_results_['mean_train_score']\ncv_auc= neigh_model.cv_results_['mean_test_score']\nnn=neigh_model.best_params_\nnn_val=nn['n_neighbors']","bd8f3eed":"neigh = KNeighborsClassifier(n_neighbors = nn_val)\nneigh.fit(X_tr,Y_train)\ny_trpred=neigh.predict(X_tr)\ny_tepred=neigh.predict(X_te)","dbe820bc":"from sklearn.metrics import confusion_matrix\nconfusion_matrix_tr=confusion_matrix(Y_train,y_trpred)\nconfusion_matrix_te=confusion_matrix(Y_test,y_tepred)\nprint(\"****Confusion matrix_Train****\")\nprint(confusion_matrix_tr)","6c10a5c3":"print(\"****Confusion matrix_Test****\")\nprint(confusion_matrix_te)","d2d4954a":"mdl4_f1=f1_score(Y_test,y_tepred)\nprint(f\"F1 Score:{mdl4_f1}\")","e9e1141b":"from sklearn.svm import LinearSVC\ntrain_auc=[]\ncv_auc=[]\npara={'C':[0.0001,0.001,0.01,0.1,1,10,100],'max_iter':[10,20,30,40]}\nsvm=LinearSVC(loss='squared_hinge')\nsvm_model=GridSearchCV(svm,para,scoring='roc_auc')\nsvm_model.fit(X_tr,Y_train)\npara=svm_model.best_params_\npara_alpha=para['C']\npara_iter=para['max_iter']","327e1c28":"print(f\"best parameters:{para}\")","445387b1":"svm = LinearSVC(C=para_alpha,max_iter=para_iter)\nsvm.fit(X_tr,Y_train)\ny_trpred=svm.predict(X_tr)\ny_tepred=svm.predict(X_te)","bc37a9ae":"from sklearn.metrics import confusion_matrix\nconfusion_matrix_tr=confusion_matrix(Y_train,y_trpred)\nconfusion_matrix_te=confusion_matrix(Y_test,y_tepred)\nprint(\"****Confusion matrix_Train****\")\nprint(confusion_matrix_tr)","e00b52ff":"print(\"****Confusion matrix_Test****\")\nprint(confusion_matrix_te)","0e327471":"mdl5_f1=f1_score(Y_test,y_tepred)\nprint(f\"F1 Score:{mdl5_f1}\")","d165b78d":"data = {'F1_SCORE':[mdl1_f1,mdl2_f1,mdl3_f1,mdl4_f1,mdl5_f1]}\nlabels= ['Logistic Regression', 'XGBoost Classification','Logistic Regression+Sentimet analysis','KNN+Sentimet analysis','SVM+Sentimet analysis']\nF1_df=pd.DataFrame(data,index=labels)\nF1_df","b4a70284":"data_df_test=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ndata_df_test.head()","18e45e11":"id_test=[]\nid_test=data_df_test['id'].values","78cb6243":"data_df_test.isna().sum()","0b4636a2":"data_df_test.drop(['id','location'],axis=1,inplace=True)\ndata_df_test.head(3)","31fca224":"data_df_test.shape","7b2b5d3b":"data_df_test.isna().sum()","e692eb8a":"data_df_test['keyword']=data_df_test['keyword'].fillna(data_df_test['keyword'].mode()[0])","b1ee7fc1":"data_df_test.shape","688048b9":"data_df_test.isna().sum()","ab7887e9":"data_df_test=text_normalizing(data_df_test,'text')","1733f36b":"\"\"\"Encoding tweets-text\"\"\"\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(ngram_range=(2,2),min_df=10)\nvectorizer.fit(X_train['text'].values) \ntrain_text_tf=vectorizer.transform(X_train['text'].values)\ndata_test_text_tf=vectorizer.transform(data_df_test['text'].values)","86ef4d0e":"print(f\"Test data-:{data_test_text_tf.shape}\")","378e11d0":"\"\"\"Vectorizing Keyword -One hot encoding\"\"\"\nkeyword_one_hot_X_train,keyword_one_hot_data_df_test=\\\nwordvectorizer(X_train['keyword'],data_df_test['keyword'])\nprint(\"After vectorizations\")\nprint(f\"train data:{keyword_one_hot_X_train.shape,Y_train.shape}\")\nprint(f\"test data:{keyword_one_hot_data_df_test.shape}\")","4b6944c2":"data_df_test_sent=sent_analyser(data_df_test,'text')","2f77eb23":"from scipy.sparse import hstack\ndata_df_X_te=hstack((data_test_text_tf,keyword_one_hot_data_df_test,data_df_test_sent)).tocsr()\nprint(f\"Train data- X_data:{X_tr.shape},Y_data:{Y_train.shape}\")\nprint(f\"Test data- X_data:{data_df_X_te.shape}\")","7b6f5202":"svm = LinearSVC(C=0.1,max_iter=30)\nsvm.fit(X_tr,Y_train)\ny_trpred=svm.predict(X_tr)\ny_tepred=svm.predict(data_df_X_te)","04fc9bfc":"result=pd.DataFrame()\nresult['id']=id_test\nresult['target']=y_tepred","8d1349a8":"print(result.shape)","0fd1208a":"result.head()","33c9ffc4":"#result.to_excel(r\"E:\\projects-ai resume\\kaggle_tweet\\tweet_disaster_results.xlsx\")","87a1fb90":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer=CountVectorizer(binary=True)\nvectorizer.fit(X_train['text'])\ntrain_text_bow=vectorizer.transform(X_train['text'])\ntest_text_bow=vectorizer.transform(X_test['text'])","b2673d46":"from scipy.sparse import hstack\nX_tr=hstack((train_text_bow,keyword_one_hot_X_train,X_train_sent)).tocsr()\nX_te=hstack((test_text_bow,keyword_one_hot_X_test,X_test_sent)).tocsr()\nprint(f\"Train data-:{X_tr.shape},Train data-Target:{Y_train.shape}\")\nprint(f\"Test data- X_data:{X_te.shape},Test data-Target:{Y_test.shape}\")","3be84a72":"from sklearn.svm import LinearSVC\nfrom sklearn.model_selection import GridSearchCV\ntrain_auc=[]\ncv_auc=[]\npara={'C':[0.0001,0.001,0.01,0.1,1,10,100],'max_iter':[10,20,30,40]}\nsvm=LinearSVC(loss='squared_hinge')\nsvm_model=GridSearchCV(svm,para,scoring='roc_auc')\nsvm_model.fit(X_tr,Y_train)\npara=svm_model.best_params_\npara_alpha=para['C']\npara_iter=para['max_iter']","30d910ef":"print(f\"best parameters:{para}\")","ebdc251d":"svm = LinearSVC(C=para_alpha,max_iter=para_iter)\nsvm.fit(X_tr,Y_train)\ny_trpred=svm.predict(X_tr)\ny_tepred=svm.predict(X_te)","94cabb39":"from sklearn.metrics import confusion_matrix\nconfusion_matrix_tr=confusion_matrix(Y_train,y_trpred)\nconfusion_matrix_te=confusion_matrix(Y_test,y_tepred)\nprint(\"****Confusion matrix_Train****\")\nprint(confusion_matrix_tr)","1973007b":"print(\"****Confusion matrix_Test****\")\nprint(confusion_matrix_te)","e301859a":"from sklearn.metrics import f1_score\nmdl5_f1=f1_score(Y_test,y_tepred)\nprint(f\"F1 Score:{mdl5_f1}\")","796816f6":"data_df_test=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ndata_df_test.head()","cc7dc9f1":"id_test=[]\nid_test=data_df_test['id'].values","c159e844":"data_df_test.isna().sum()","dd7c92bf":"data_df_test.drop(['id','location'],axis=1,inplace=True)\ndata_df_test.head(3)","89291a30":"data_df_test.shape","fe1eb6a1":"data_df_test.isna().sum()","663918b5":"data_df_test['keyword']=data_df_test['keyword'].fillna(data_df_test['keyword'].mode()[0])","c12004cf":"data_df_test.shape","c1dd1d6d":"data_df_test.isna().sum()","2121fdf4":"data_df_test=text_normalizing(data_df_test,'text')","a160b9da":"\"\"\"Encoding tweets-text\"\"\"\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(binary=True)\nvectorizer.fit(X_train['text'].values) \ntrain_text_bow=vectorizer.transform(X_train['text'].values)\ndata_test_text_bow=vectorizer.transform(data_df_test['text'].values)","cebc1752":"print(f\"Test data-:{data_test_text_tf.shape}\")","04449b73":"\"\"\"Vectorizing Keyword -One hot encoding\"\"\"\nkeyword_one_hot_X_train,keyword_one_hot_data_df_test=\\\nwordvectorizer(X_train['keyword'],data_df_test['keyword'])\nprint(\"After vectorizations\")\nprint(f\"train data:{keyword_one_hot_X_train.shape,Y_train.shape}\")\nprint(f\"test data:{keyword_one_hot_data_df_test.shape}\")","6bb4a18a":"data_df_test_sent=sent_analyser(data_df_test,'text')","edaa4570":"from scipy.sparse import hstack\ndata_df_X_te=hstack((data_test_text_bow,keyword_one_hot_data_df_test,data_df_test_sent)).tocsr()\nprint(f\"Train data- X_data:{X_tr.shape},Y_data:{Y_train.shape}\")\nprint(f\"Test data- X_data:{data_df_X_te.shape}\")","cfe5f93c":"svm = LinearSVC(C=0.01,max_iter=20)\nsvm.fit(X_tr,Y_train)\ny_trpred=svm.predict(X_tr)\ny_tepred=svm.predict(data_df_X_te)","613a0b63":"result=pd.DataFrame()\nresult['id']=id_test\nresult['target']=y_tepred","2cbeaf1a":"print(result.shape)","13313807":"result.head()","d3d52fb2":"#result.to_excel(r\"E:\\projects-ai resume\\kaggle_tweet\\Tweet_Disaster_files\\12032020\\tweet_disaster_results.xlsx\")","b34af75c":"#os.path.join('\/kaggle\/output\/kaggle\/working','tweet_svm_bow_sent_submission')","85f4c9b6":"result.to_csv('tweet_disaster_subbmission.csv',index=False)","56b86ac3":"##### Stacking Sparse Matrices","ad4efc16":"##### Remove Emojis","c4f0acb5":"#### Word Vectorization","48ee18bb":"#### Model:Logistic Regression +One hot Encoding of Categorical class data+sentiment score","ae80d37d":"##### Tf-idf vectorization","281f62c1":"##### Hyperparameter Tuning","18b862b1":"##### ROC Curve","069941e5":"##### BOW vectorization","ac4a2e00":"- 90%  of the tweets that indicates disastermbelongs to keywords words such as debris,wreckage,derailment,outbreak,oilspill,typhoon,bombing etc..\n- Only 20% of tweets with keywords  aftershock,ruin,electrocute etc. indicate  disaster. ","1b4184a3":"##### Applying SVM for classification","599b9302":"###### Keyword","b32a4fd3":"##### Classification Report","ed46eb64":"##### Loading Original Test data - Kaggle","48836e2f":"##### Confusion Matrix","93688816":"##### Distribution of datapoint among output","1775ab0c":"#### Text Preprocessing","f96cf945":"##### Hyperparameter Tuning","0a00a3e2":"#### Model:XG Boost classifier +One hot Encoding of Categorical class data","85a93fdc":"- The cost of a mis-classification can be  high.\n- Need to classify\/predict for both the positive and negative class correctly.\n- F1 Score is taken as the performance metric for the competition","5ddfa21e":"#### Description","3ea762d6":"from nltk.stem.porter import PorterStemmer\nstemmer=PorterStemmer()\ndef stemming(data,ftr):\n    clean=[]\n    for sentance in data[ftr].values:\n        wrd_lst=word_tokenize(sentance)\n        sent=' '.join(stemmer.stem(word) for word in wrd_lst)\n        clean.append(sent.strip())\n    return clean","03ef7bd6":"##### Remove accented characters","bd9ee626":"#### Basic EDA","850949e9":"##### Lemmatization","fa9ca040":"##### Convert to Lowercase","f511bb7b":"##### Keywords vs Tweets","010aaffd":"#### Data Overview","a665498d":"##### Type of Machine Learning Problem","eb466cbb":"- Among the tweets reported from mumbai\/India as disaster, 90% of them are actuall disaster.\n- The locations are not automatically generated.They are entered by the user.They may not be much helpful in classification.","8e656ccb":"- Locations are not system generated.They are entered by the user themseleves.It is better to avoid location as a feature.","291d3dc8":"Objective of the competition is to ,built a model that classify weather a tweet is really about a disaster or not.","96e64c1a":"##### Classification Report","4aacfb15":"#### Comparing Results & Selecting Best Model","5c4de61e":"It is a binary classification problem, for a given tweet we need to predict if the tweet is really about a disaster or not","b5e9dfd4":"##### Remove Special Characters","7c0df18e":"- Features Keyword and location are having null values.\n- There are no duplicate rows found.","8be717b9":"##### Remove URLs&HTML tags","fd2d71be":"- Null values in both features,keyword and location are below 35%.Retained both the columns and performed imputation with modes of respective columns.","e49bf46d":"##### Remove StopWords","30ec49c2":"#### Mapping the real world problem to an ML Problem","212287d6":"cln=stemming(tmp_df,'txt')\ntmp_df['txt']=cln\ntmp_df","f3d690ee":"#### BOW+ SVM-Tuning for better F1","ac75bf07":"##### Remove whitespaces and newlines","a2cd2da4":"##### Null value Treatment","6a6c50a5":"##### Stacking Sparse Matrices","461343ae":"### TWEET CLASSIFICATION","44190e54":"#### Feature Engineering","0d5c4b34":"##### One-Hot Encoding","bd46b777":"##### TF-idf vectorization","737a59e3":"#### Train-Test split","dc1ac193":"##### ROC Curve","0ef7043a":"- Keyword can be used as a feature.","932a9ce4":"- As the feature id is not uch hepful in classification,droped the feature fro data.","c00f492f":"#### Model:Logistic Regression +One hot Encoding of Categorical class data","f3209b9a":"##### ID vs Tweets","00af94a1":"##### Normmalizing text data","f1b160ea":"##### Hyperparameter Tuning","2290eb87":"##### Classification Report","85ee8a12":"##### Adding Extra Features-Sentiment scores","29bdc705":"##### Sentiment Analyser","336819ff":"#### Model:SVM +One hot Encoding of Categorical class data+sentiment score","8b66886c":"##### Applying SVM for classification","15e87b47":"##### Loading Original Test data - Kaggle","758788fd":"##### Stacking sparse matrices","af62437c":"- 57% tweets are not actually indicating disaster.\n- 43% tweets are indicating disaster.\n- There is not huge differrence between positive and negative class.Hence,no need to apply upsampling.","f514904f":"##### Normmalizing text data","6a5d4834":"##### One-Hot Encoding","77ec5dca":"##### Normalizing Text","f17c1e70":"- Split the data into X_train for training the model and X_test for validating the model.\n- Split the target into Y_train and Y_test.","eea650fa":"#### Classifying Kaggle-Test Data","f3c32109":"##### ROC Curve","71b27158":"##### Stacking sparse matrices","35dbfebb":"###### Location","254e9eb9":"##### Decontracting words","cdcef2f4":"- BOW model is giving better score than tfidf  model","8c3fdb3b":"### Machine Learning Problem","4cd50aa2":"##eg:\ntmp=[\"gaming biking living.\"]\ntmp_df=pd.DataFrame()\ntmp_df['txt']=tmp\ntmp_df","b5070fa7":"##### Null value Treatment","862f3b87":"- Train.csv contains 5 columns : id,keyword,location,text,target.\n- Size of train.csv - 965 KB.\n- Number of rows in train.csv = 7613\n\n- .csv contains 4 columns : id,keyword,location,text.\n- Size of test.csv - 411 KB.\n- Number of rows in Train.csv = 3263","f4e9f179":"##### BOW","7b9f534b":"##### Performance Metric","bbde0054":"##### Null-Value Treatment","cfefb74c":"#### Applying SVM+BOW+Sentiment analysis on kaggle-Test Data","14e86810":"##### Sentiment Analyser","fed9675e":"- Logistic Regression & SVM with  One-hot encoding and Sentiment analysis gives better F1 scores.\n- SV is selected for test data classification","bb2aae3b":"##### Stemming","85ec45f8":"##### Peeking into data- Location & Keyword","662d28ae":"##### Confusion Matrix","c19b467c":"##### One-Hot Encoding","d798533d":"#### Encoding","561f7d3c":"##### Confusion Matrix","db8726e7":"##### Location vs Tweets","df1aa471":"#### Model:KNN +One hot Encoding of Categorical class data+sentiment score"}}