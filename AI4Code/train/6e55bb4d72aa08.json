{"cell_type":{"5ca2107d":"code","89eb95ff":"code","c7507a9b":"code","cd566f0b":"code","f4d88b83":"code","fea594c6":"code","061c0cc4":"code","97772854":"code","4d7b405e":"code","37eac5c5":"code","171d3872":"code","27e4d6e7":"code","f89f6ac8":"code","b8a856f3":"code","cb7d3130":"code","2b6c3715":"code","c7be7787":"code","670d1674":"code","991c18e7":"code","51fbfcff":"code","42d19245":"code","2fb45cb4":"code","eee2b1ff":"code","32e3a123":"code","29226616":"code","5cce3e80":"code","491b105e":"code","a959611d":"code","52537560":"code","bc901023":"code","db2a5dd0":"code","3dac9eba":"code","cec093d7":"code","38379a6e":"code","d73127f3":"code","fb448d41":"code","198707e2":"code","cc35d8f1":"code","855cca07":"code","1f68f4a4":"code","52a0d6f8":"code","1c3ae6a7":"code","d2e73fb2":"code","b11359c7":"code","1654d1b0":"code","185afd6f":"code","264d8b05":"code","3d641319":"code","36762a39":"code","5ad1f4fc":"code","38aa5013":"code","b47a6d4a":"markdown","78d76e97":"markdown","ecdef4bc":"markdown","d1041553":"markdown","12851ed0":"markdown","7baf836d":"markdown","7fe30691":"markdown","6c14424a":"markdown","ffde7c7e":"markdown","c7716e8b":"markdown","f3835574":"markdown","5e3439dd":"markdown","7e90b725":"markdown","e1a117e5":"markdown","b6651c2e":"markdown","534a982a":"markdown","58ef445d":"markdown","901d36ec":"markdown","d424a7b6":"markdown","1f177e7c":"markdown","b4c58d96":"markdown","9fad221b":"markdown","803c5b69":"markdown","a67d9f7d":"markdown"},"source":{"5ca2107d":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.applications import vgg19\nfrom tensorflow.keras.models import load_model,Model\nfrom PIL import Image\nimport time\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport requests\nimport base64\nimport os\nfrom pathlib import Path\nfrom io import BytesIO\nmatplotlib.rcParams['figure.figsize'] = (12,12)\nmatplotlib.rcParams['axes.grid'] = False","89eb95ff":"def load_image(image_path,dim=None,resize=False):\n    img= Image.open(image_path)\n    if dim:\n        if resize:\n            img=img.resize(dim)\n        else:\n            img.thumbnail(dim)\n    img= img.convert(\"RGB\")\n    return np.array(img)","c7507a9b":"def load_url_image(url,dim=None,resize=False):\n    img_request=requests.get(url)\n    img= Image.open(BytesIO(img_request.content))\n    if dim:\n        if resize:\n            img=img.resize(dim)\n        else:\n            img.thumbnail(dim)\n    img= img.convert(\"RGB\")\n    return np.array(img)","cd566f0b":"def array_to_img(array):\n    array=np.array(array,dtype=np.uint8)\n    if np.ndim(array)>3:\n        assert array.shape[0]==1\n        array=array[0]\n    return Image.fromarray(array)","f4d88b83":"def show_image(image,title=None):\n    if len(image.shape)>3:\n        image=tf.squeeze(image,axis=0)\n    plt.imshow(image)\n    if title:\n        plt.title=title","fea594c6":"def plot_images_grid(images,num_rows=1):\n    n=len(images)\n    if n > 1:\n        num_cols=np.ceil(n\/num_rows)\n        fig,axes=plt.subplots(ncols=int(num_cols),nrows=int(num_rows))\n        axes=axes.flatten()\n        fig.set_size_inches((15,15))\n        for i,image in enumerate(images):\n            axes[i].imshow(image)\n    else:\n        plt.figure(figsize=(10,10))\n        plt.imshow(images[0])","061c0cc4":"vgg=vgg19.VGG19(weights='imagenet',include_top=False)\nvgg.summary()","97772854":"content_layers=['block4_conv2']\nstyle_layers=['block1_conv1',\n            'block2_conv1',\n            'block3_conv1',\n            'block4_conv1',\n            'block5_conv1']\ncontent_layers_weights=[1]\nstyle_layers_weights=[1]*5","4d7b405e":"class LossModel:\n    def __init__(self,pretrained_model,content_layers,style_layers):\n        self.model=pretrained_model\n        self.content_layers=content_layers\n        self.style_layers=style_layers\n        self.loss_model=self.get_model()\n\n    def get_model(self):\n        self.model.trainable=False\n        layer_names=self.style_layers + self.content_layers\n        outputs=[self.model.get_layer(name).output for name in layer_names]\n        new_model=Model(inputs=self.model.input,outputs=outputs)\n        return new_model\n    \n    def get_activations(self,inputs):\n        inputs=inputs*255.0\n        style_length=len(self.style_layers)\n        outputs=self.loss_model(vgg19.preprocess_input(inputs))\n        style_output,content_output=outputs[:style_length],outputs[style_length:]\n        content_dict={name:value for name,value in zip(self.content_layers,content_output)}\n        style_dict={name:value for name,value in zip(self.style_layers,style_output)}\n        return {'content':content_dict,'style':style_dict}","37eac5c5":"loss_model=LossModel(vgg,content_layers,style_layers)","171d3872":"def content_loss(placeholder,content,weight):\n    assert placeholder.shape == content.shape\n    return weight*tf.reduce_mean(tf.square(placeholder-content))","27e4d6e7":"def gram_matrix(x):\n    gram=tf.linalg.einsum('bijc,bijd->bcd', x, x)\n    return gram\/tf.cast(x.shape[1]*x.shape[2]*x.shape[3],tf.float32)","f89f6ac8":"def style_loss(placeholder,style,weight):\n    assert placeholder.shape == style.shape\n    s=gram_matrix(style)\n    p=gram_matrix(placeholder)\n    return weight*tf.reduce_mean(tf.square(s-p))","b8a856f3":"def preceptual_loss(predicted_activations,content_activations,style_activations,content_weight,style_weight,content_layers_weights,style_layer_weights):\n    pred_content=predicted_activations[\"content\"]\n    pred_style=predicted_activations[\"style\"]\n    c_loss=tf.add_n([content_loss(pred_content[name],content_activations[name],content_layers_weights[i]) for i,name in enumerate(pred_content.keys())])\n    c_loss=c_loss*content_weight\n    s_loss=tf.add_n([style_loss(pred_style[name],style_activations[name],style_layer_weights[i]) for i,name in enumerate(pred_style.keys())])\n    s_loss=s_loss*style_weight\n    return c_loss+s_loss","cb7d3130":"class ReflectionPadding2D(tf.keras.layers.Layer):\n    def __init__(self, padding=(1, 1), **kwargs):\n        super(ReflectionPadding2D, self).__init__(**kwargs)\n        self.padding = tuple(padding)\n    def call(self, input_tensor):\n        padding_width, padding_height = self.padding\n        return tf.pad(input_tensor, [[0,0], [padding_height, padding_height], [padding_width, padding_width], [0,0] ], 'REFLECT')","2b6c3715":"class InstanceNormalization(tf.keras.layers.Layer):\n    def __init__(self,**kwargs):\n        super(InstanceNormalization, self).__init__(**kwargs)\n    def call(self,inputs):\n        batch, rows, cols, channels = [i for i in inputs.get_shape()]\n        mu, var = tf.nn.moments(inputs, [1,2], keepdims=True)\n        shift = tf.Variable(tf.zeros([channels]))\n        scale = tf.Variable(tf.ones([channels]))\n        epsilon = 1e-3\n        normalized = (inputs-mu)\/tf.sqrt(var + epsilon)\n        return scale * normalized + shift","c7be7787":"class ConvLayer(tf.keras.layers.Layer):\n    def __init__(self,filters,kernel_size,strides=1,**kwargs):\n        super(ConvLayer,self).__init__(**kwargs)\n        self.padding=ReflectionPadding2D([k\/\/2 for k in kernel_size])\n        self.conv2d=tf.keras.layers.Conv2D(filters,kernel_size,strides)\n        self.bn=InstanceNormalization()\n    def call(self,inputs):\n        x=self.padding(inputs)\n        x=self.conv2d(x)\n        x=self.bn(x)\n        return x","670d1674":"class ResidualLayer(tf.keras.layers.Layer):\n    def __init__(self,filters,kernel_size,**kwargs):\n        super(ResidualLayer,self).__init__(**kwargs)\n        self.conv2d_1=ConvLayer(filters,kernel_size)\n        self.conv2d_2=ConvLayer(filters,kernel_size)\n        self.relu=tf.keras.layers.ReLU()\n        self.add=tf.keras.layers.Add()\n    def call(self,inputs):\n        residual=inputs\n        x=self.conv2d_1(inputs)\n        x=self.relu(x)\n        x=self.conv2d_2(x)\n        x=self.add([x,residual])\n        return x","991c18e7":"class UpsampleLayer(tf.keras.layers.Layer):\n    def __init__(self,filters,kernel_size,strides=1,upsample=2,**kwargs):\n        super(UpsampleLayer,self).__init__(**kwargs)\n        self.upsample=tf.keras.layers.UpSampling2D(size=upsample)\n        self.padding=ReflectionPadding2D([k\/\/2 for k in kernel_size])\n        self.conv2d=tf.keras.layers.Conv2D(filters,kernel_size,strides)\n        self.bn=InstanceNormalization()\n    def call(self,inputs):\n        x=self.upsample(inputs)\n        x=self.padding(x)\n        x=self.conv2d(x)\n        return self.bn(x)","51fbfcff":"class StyleTransferModel(tf.keras.Model):\n    def __init__(self,**kwargs):\n        super(StyleTransferModel, self).__init__(name='StyleTransferModel',**kwargs)\n        self.conv2d_1= ConvLayer(filters=32,kernel_size=(9,9),strides=1,name=\"conv2d_1_32\")\n        self.conv2d_2= ConvLayer(filters=64,kernel_size=(3,3),strides=2,name=\"conv2d_2_64\")\n        self.conv2d_3= ConvLayer(filters=128,kernel_size=(3,3),strides=2,name=\"conv2d_3_128\")\n        self.res_1=ResidualLayer(filters=128,kernel_size=(3,3),name=\"res_1_128\")\n        self.res_2=ResidualLayer(filters=128,kernel_size=(3,3),name=\"res_2_128\")\n        self.res_3=ResidualLayer(filters=128,kernel_size=(3,3),name=\"res_3_128\")\n        self.res_4=ResidualLayer(filters=128,kernel_size=(3,3),name=\"res_4_128\")\n        self.res_5=ResidualLayer(filters=128,kernel_size=(3,3),name=\"res_5_128\")\n        self.deconv2d_1= UpsampleLayer(filters=64,kernel_size=(3,3),name=\"deconv2d_1_64\")\n        self.deconv2d_2= UpsampleLayer(filters=32,kernel_size=(3,3),name=\"deconv2d_2_32\")\n        self.deconv2d_3= ConvLayer(filters=3,kernel_size=(9,9),strides=1,name=\"deconv2d_3_3\")\n        self.relu=tf.keras.layers.ReLU()\n    def call(self, inputs):\n        x=self.conv2d_1(inputs)\n        x=self.relu(x)\n        x=self.conv2d_2(x)\n        x=self.relu(x)\n        x=self.conv2d_3(x)\n        x=self.relu(x)\n        x=self.res_1(x)\n        x=self.res_2(x)\n        x=self.res_3(x)\n        x=self.res_4(x)\n        x=self.res_5(x)\n        x=self.deconv2d_1(x)\n        x=self.relu(x)\n        x=self.deconv2d_2(x)\n        x=self.relu(x)\n        x=self.deconv2d_3(x)\n        x = (tf.nn.tanh(x) + 1) * (255.0 \/ 2)\n        return x\n    \n    ## used to print shapes of each layer to check if input shape == output shape\n    ## I don't know any better solution to this right now\n    def print_shape(self,inputs):\n        print(inputs.shape)\n        x=self.conv2d_1(inputs)\n        print(x.shape)\n        x=self.relu(x)\n        x=self.conv2d_2(x)\n        print(x.shape)\n        x=self.relu(x)\n        x=self.conv2d_3(x)\n        print(x.shape)\n        x=self.relu(x)\n        x=self.res_1(x)\n        print(x.shape)\n        x=self.res_2(x)\n        print(x.shape)\n        x=self.res_3(x)\n        print(x.shape)\n        x=self.res_4(x)\n        print(x.shape)\n        x=self.res_5(x)\n        print(x.shape)\n        x=self.deconv2d_1(x)\n        print(x.shape)\n        x=self.relu(x)\n        x=self.deconv2d_2(x)\n        print(x.shape)\n        x=self.relu(x)\n        x=self.deconv2d_3(x)\n        print(x.shape)","42d19245":"input_shape=(256,256,3)\nbatch_size=16","2fb45cb4":"style_model = StyleTransferModel()","eee2b1ff":"style_model.print_shape(tf.zeros(shape=(1,*input_shape)))","32e3a123":"optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)","29226616":"def train_step(dataset,style_activations,steps_per_epoch,style_model,loss_model,optimizer,checkpoint_path=\".\/\",content_weight=1e4,style_weight=1e-2,total_variation_weight=0.004,content_layers_weights=[1],style_layers_weights=[1]*5):\n    batch_losses=[]\n    steps=1\n    save_path=os.path.join(checkpoint_path,f\"model_checkpoint.ckpt\")\n    print(\"Model Checkpoint Path: \",save_path)\n    for input_image_batch in dataset:\n        if steps-1 >= steps_per_epoch:\n            break\n        with tf.GradientTape() as tape:\n            outputs=style_model(input_image_batch)\n            outputs=tf.clip_by_value(outputs, 0, 255)\n            pred_activations=loss_model.get_activations(outputs\/255.0)\n            content_activations=loss_model.get_activations(input_image_batch)[\"content\"] \n            curr_loss=preceptual_loss(pred_activations,content_activations,style_activations,content_weight,\n                                      style_weight,content_layers_weights,style_layers_weights)\n            curr_loss += total_variation_weight*tf.image.total_variation(outputs)\n        batch_losses.append(curr_loss)\n        grad = tape.gradient(curr_loss,style_model.trainable_variables)\n        optimizer.apply_gradients(zip(grad,style_model.trainable_variables))\n        if steps%100==0:\n            print(\"checkpoint saved \",end=\" \")\n            style_model.save_weights(save_path)\n            print(f\"Loss: {tf.reduce_mean(batch_losses).numpy()}\")\n        steps+=1\n    return tf.reduce_mean(batch_losses)","5cce3e80":"input_path = \"..\/input\/gan-getting-started\/photo_jpg\"\nstyle_path = \"..\/input\/gan-getting-started\/monet_jpg\"","491b105e":"class TensorflowDatasetLoader:\n    def __init__(self,dataset_path,batch_size=4, image_size=(256, 256),num_images=None):\n        images_paths = [str(path) for path in Path(dataset_path).glob(\"*.jpg\")]\n        self.length=len(images_paths)\n        if num_images is not None:\n            images_paths = images_paths[0:num_images]\n        dataset = tf.data.Dataset.from_tensor_slices(images_paths).map(\n            lambda path: self.load_tf_image(path, dim=image_size),\n            num_parallel_calls=tf.data.experimental.AUTOTUNE,\n        )\n        dataset = dataset.batch(batch_size,drop_remainder=True)\n        dataset = dataset.repeat()\n        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n        self.dataset=dataset\n    def __len__(self):\n        return self.length\n    def load_tf_image(self,image_path,dim):\n        image = tf.io.read_file(image_path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image= tf.image.resize(image,dim)\n        image= image\/255.0\n        image = tf.image.convert_image_dtype(image, tf.float32)\n        return image","a959611d":"loader=TensorflowDatasetLoader(input_path,batch_size=batch_size)","52537560":"loader.dataset.element_spec","bc901023":"plot_images_grid(next(iter(loader.dataset.take(1))),num_rows=4)","db2a5dd0":"# setting up style image\n\nstyle_image_path =os.path.join(style_path,\"05b493ff42.jpg\")\nstyle_image=load_image(style_image_path,dim=(input_shape[0],input_shape[1]),resize=True)\nstyle_image=style_image\/255.0","3dac9eba":"show_image(style_image)","cec093d7":"style_image=style_image.astype(np.float32)\nstyle_image_batch=np.repeat([style_image],batch_size,axis=0)\nstyle_activations=loss_model.get_activations(style_image_batch)[\"style\"]","38379a6e":"epochs=10\ncontent_weight=1e1\nstyle_weight=1e2\ntotal_variation_weight=0.004","d73127f3":"num_images=len(loader)\nsteps_per_epochs=num_images\/\/batch_size\nprint(steps_per_epochs)","fb448d41":"model_save_path=\"model_checkpoint\"","198707e2":"os.makedirs(model_save_path,exist_ok=True)","cc35d8f1":"try:\n    policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n    tf.keras.mixed_precision.experimental.set_policy(policy) \nexcept:\n    pass","855cca07":"try:\n    tf.config.optimizer.set_jit(True)\nexcept:\n    pass","1f68f4a4":"if os.path.isfile(os.path.join(model_save_path,\"model_checkpoint.ckpt.index\")):\n    style_model.load_weights(os.path.join(model_save_path,\"model_checkpoint.ckpt\"))\n    print(\"resuming training ...\")\nelse:\n    print(\"training scratch ...\")","52a0d6f8":"epoch_losses=[]\nfor epoch in range(1,epochs+1):\n    print(f\"epoch: {epoch}\")\n    batch_loss=train_step(loader.dataset,style_activations,steps_per_epochs,style_model,loss_model,optimizer,\n                          model_save_path,\n                          content_weight,style_weight,total_variation_weight,\n                          content_layers_weights,style_layers_weights)\n    style_model.save_weights(os.path.join(model_save_path,\"model_checkpoint.ckpt\"))\n    print(\"Model Checkpointed at: \",os.path.join(model_save_path,\"model_checkpoint.ckpt\"))\n    print(f\"loss: {batch_loss.numpy()}\")\n    epoch_losses.append(batch_loss)","1c3ae6a7":"plt.plot(epoch_losses)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Process\")\nplt.show()","d2e73fb2":"if os.path.isfile(os.path.join(model_save_path,\"model_checkpoint.ckpt.index\")):\n    style_model.load_weights(os.path.join(model_save_path,\"model_checkpoint.ckpt\"))\n    print(\"loading weights ...\")\nelse:\n    print(\"no weights found ...\")","b11359c7":"plot_images_grid(next(iter(loader.dataset.take(1))),num_rows = 4)","1654d1b0":"styled_images = []\n\nfor images in loader.dataset.take(1):\n    images = images * 255.0\n    images= tf.image.resize(images,(1024,1024))\n    generated_images = style_model(images)\n    generated_images = np.clip(generated_images,0,255)\n    generated_images = tf.image.resize(generated_images,(256,256)).numpy()\n    generated_images = generated_images.astype(np.uint8)\n    for image in generated_images:\n        styled_images.append(image)","185afd6f":"plot_images_grid(styled_images,num_rows = 4)","264d8b05":"os.makedirs(\"images\",exist_ok=True)","3d641319":"i = 1\nfor images in loader.dataset.take(500):\n    images = images * 255.0\n    images= tf.image.resize(images,(1024,1024))\n    generated_images = style_model(images)\n    generated_images = np.clip(generated_images,0,255)\n    generated_images = tf.image.resize(generated_images,(256,256)).numpy()\n    generated_images = generated_images.astype(np.uint8)\n    for image in generated_images:\n        img = Image.fromarray(image)\n        img.save(os.path.join(\"images\",f\"{i}.jpg\"))\n        i=i+1","36762a39":"import shutil\n\nshutil.make_archive('\/kaggle\/working\/images\/', 'zip', 'images')","5ad1f4fc":"shutil.make_archive('\/kaggle\/working\/model_checkpoint\/', 'zip', 'model_checkpoint')","38aa5013":"!rm -rf images\n!rm -rf model_checkpoint","b47a6d4a":"### Defining loss functions","78d76e97":"### creating instance of style model","ecdef4bc":"### Importing all dependencies","d1041553":"### Constructing style transfer model","12851ed0":"### Utility Functions","7baf836d":"## GENERATING ARTWORK","7fe30691":"# FAST STYLE TRANSFER","6c14424a":"### Defining model layers","ffde7c7e":"## TRAINING STYLE MODEL","c7716e8b":"## SETTING UP DATASET\n\ntf.data api from tensorflow is used to set up training data.","f3835574":"### setting up our style image for training","5e3439dd":"### Loading previous saved checkpoints if exists","7e90b725":"<div>\n  <img src='https:\/\/github.com\/tarun-bisht\/fast-style-transfer\/raw\/master\/data\/images\/style.jpg' height=\"346px\">\n  <img src='https:\/\/github.com\/tarun-bisht\/fast-style-transfer\/raw\/master\/data\/images\/content.jpg' height=\"346px\">\n  <img src='https:\/\/github.com\/tarun-bisht\/fast-style-transfer\/raw\/master\/output\/styled.jpg' height=\"512px\">\n<\/div>","e1a117e5":"### enabling mix precision and jit for training optimization","b6651c2e":"## ZIPPING SUBMISSION DATA","534a982a":"## TRAINING UTILITY\n\nThe output from decoder is passed to loss model (VGG) from which we extract features and calculate style loss and content loss whose weighted sum provide perceptual loss. Using loss, gradients are calculated with respect to style model's trainable parameters.","58ef445d":"### Define input shape and batch size","901d36ec":"### Creating Loss model which is used to calcuate perceptual loss by combining style and content loss ","d424a7b6":"### initializing optimizer for backpropogation","1f177e7c":"## SETTING UP","b4c58d96":"### Defining content and style layers from pretrained model's layers","9fad221b":"# FAST STYLE TRANSFER MODEL\n\nIt is a encoder-decoder architecture with residual layers. Input images are passed to encoder part and it propogates to decoder part of same size as input and predict generated image. For training this generated image is passed to our loss model (VGG19) and features from different layers were extracted (content layers and style layers) these features are then used to calculate style loss and content loss, whose weighted sum produce perceptual loss that trains the network. The below image from paper describe it well.\n\n![https:\/\/arxiv.org\/abs\/1603.08155](https:\/\/miro.medium.com\/max\/1574\/1*Um82GJ99gauIOh0U-S11hQ.png)\n\nThe main highlights of network:\n\n- Residual Layers\n- Encoder Decoder Model\n- output from decoder is passed to loss model(VGG) to alculate loss and train","803c5b69":"Stylize any photo or video in style of famous paintings using Neural Style Transfer.\n- This is hundreds of times faster than the optimization-based method presented by [Gatys et al](https:\/\/arxiv.org\/abs\/1508.06576) so called fast style transfer.\n- We train a feedforward network that apply artistic styles to images using loss function defined in [Gatys et al](https:\/\/arxiv.org\/abs\/1508.06576) paper.\n- Feed forward network is a residual autoencoder network that takes content image as input and spits out stylized image.\n- Model also uses instance normalization instead of batch normalization based on the paper [Instance Normalization: The Missing Ingredient for Fast Stylization](https:\/\/arxiv.org\/abs\/1607.08022)\n- Training is done by using perceptual loss defined in paper [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https:\/\/arxiv.org\/abs\/1603.08155).\n- Vgg19 is used to calculate perceptual loss more working described on paper.","a67d9f7d":"## SETTING UP LOSS MODEL\n\nHere pretrained VGG19 model was used to calculate perceptual loss as describe in this [paper](https:\/\/arxiv.org\/abs\/1603.08155). \n\nIf you know about Gatys style transfer then it is same, we need to calculate style and content loss using a pretrained model using them we create total loss. \n\nThis loss is calculated by passing style and content images both into this VGG network and using some layers of network to extract features. Higher layer learns complex features which also preserves some content of image (like higher layers can represent face of dog but lower layers represent lower features like eyes, nose, ears etc.), so for content loss higher layers of networks are used. For style loss lower layers of networks are used since they have learned small features like edges, countours etc. that can imitate brush stroke from a painting. We use multiple layers for style because every layer learns differnt strokes and to look realistic we need some variations in stroke. "}}