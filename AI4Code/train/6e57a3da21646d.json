{"cell_type":{"43b2b8f7":"code","5cba8ac1":"code","2049339d":"code","52cb106b":"code","fb53827d":"code","4c79f841":"code","087fe4dd":"code","7d3dacb3":"code","23a39a7e":"code","d37539e0":"code","56e4ac0e":"code","4e6e7e6d":"code","7d6110ba":"code","d3c20c7b":"code","aaf45092":"code","b9f5a102":"code","0f7276f5":"code","db657807":"code","1785ca73":"code","18156c63":"code","563c99e4":"code","66ebf57f":"code","ae2fafb3":"code","ba705714":"code","cccbcaf2":"code","2334ca76":"markdown","85b8206e":"markdown","f6f1f128":"markdown","44b351ce":"markdown","b8a16610":"markdown","25a827be":"markdown","f116f490":"markdown","0eb562d2":"markdown"},"source":{"43b2b8f7":"import os, cv2 \nimport numpy as np\nimport time","5cba8ac1":"start = time.time()","2049339d":"! mkdir -p \/root\/.kaggle\/\n! cp ..\/input\/api-token\/kaggle.json \/root\/.kaggle\/kaggle.json","52cb106b":"! mkdir -p \/kaggle\/tmp\/pullic_hpa\n! kaggle datasets init -p \/kaggle\/tmp\/pullic_hpa","fb53827d":"! cp -rv ..\/input\/publichpa512x512\/images \/kaggle\/tmp\/pullic_hpa","4c79f841":"%%bash\necho \"{\n  \\\"title\\\": \\\"PUBLIC HPA 512x512\\\",\n  \\\"id\\\": \\\"tchaye59\/PUBLICHPA512x512\\\",\n  \\\"licenses\\\": [\n    {\n      \\\"name\\\": \\\"CC0-1.0\\\"\n    }\n  ]\n}\" > \/kaggle\/tmp\/pullic_hpa\/dataset-metadata.json","087fe4dd":"import io\nimport os\nimport requests\nimport pathlib\nimport gzip\nimport imageio\nimport pandas as pd\n\n\ndef tif_gzip_to_png(tif_path):\n    '''Function to convert .tif.gz to .png and put it in the same folder\n    Eg. for working in local work station\n    '''\n    png_path = pathlib.Path(tif_path.replace('.tif.gz','.png'))\n    tf = gzip.open(tif_path).read()\n    img = imageio.imread(tf, 'tiff')\n    imageio.imwrite(png_path, img)\n    \ndef download_and_convert_tifgzip_to_png(url, target_path):    \n    '''Function to convert .tif.gz to .png and put it in the same folder\n    Eg. in Kaggle notebook\n    '''\n    r = requests.get(url)\n    f = io.BytesIO(r.content)\n    tf = gzip.open(f).read()\n    img = imageio.imread(tf, 'tiff')\n    \n    img = cv2.resize(img, (512,512), interpolation = cv2.INTER_AREA)\n    return img\n    #cv2.imwrite(target_path , img)\n    #imageio.imwrite(target_path, img)","7d3dacb3":"# All label names in the public HPA and their corresponding index. \nall_locations = dict({\n    \"Nucleoplasm\": 0,\n    \"Nuclear membrane\": 1,\n    \"Nucleoli\": 2,\n    \"Nucleoli fibrillar center\": 3,\n    \"Nuclear speckles\": 4,\n    \"Nuclear bodies\": 5,\n    \"Endoplasmic reticulum\": 6,\n    \"Golgi apparatus\": 7,\n    \"Intermediate filaments\": 8,\n    \"Actin filaments\": 9,\n    \"Focal adhesion sites\": 9,\n    \"Microtubules\": 10,\n    \"Mitotic spindle\": 11,\n    \"Centrosome\": 12,\n    \"Centriolar satellite\": 12,\n    \"Plasma membrane\": 13,\n    \"Cell Junctions\": 13,\n    \"Mitochondria\": 14,\n    \"Aggresome\": 15,\n    \"Cytosol\": 16,\n    \"Vesicles\": 17,\n    \"Peroxisomes\": 17,\n    \"Endosomes\": 17,\n    \"Lysosomes\": 17,\n    \"Lipid droplets\": 17,\n    \"Cytoplasmic bodies\": 17,\n    \"No staining\": 18\n})\n\n\ndef add_label_idx(df, all_locations):\n    '''Function to convert label name to index\n    '''\n    df[\"Label_idx\"] = None\n    for i, row in df.iterrows():\n        labels = row.Label.split(',')\n        idx = []\n        for l in labels:\n            if l in all_locations.keys():\n                idx.append(str(all_locations[l]))\n        if len(idx)>0:\n            df.loc[i,\"Label_idx\"] = \"|\".join(idx)\n            \n        print(df.loc[i,\"Label\"], df.loc[i,\"Label_idx\"])\n    return df\n    ","23a39a7e":"public_hpa_df = pd.read_csv('..\/input\/publichpa-withcellline\/kaggle_2021.tsv')\n# Remove all images overlapping with Training set\npublic_hpa_df = public_hpa_df[public_hpa_df.in_trainset == False]\n\n# Remove all images with only labels that are not in this competition\npublic_hpa_df = public_hpa_df[~public_hpa_df.Label_idx.isna()]\n\ncolors = ['blue', 'red', 'green', 'yellow']\ncelllines = ['A-431', 'A549', 'EFO-21', 'HAP1', 'HEK 293', 'HUVEC TERT2', 'HaCaT', 'HeLa', 'PC-3', 'RH-30', 'RPTEC TERT1', 'SH-SY5Y', 'SK-MEL-30', 'SiHa', 'U-2 OS', 'U-251 MG', 'hTCEpi']\npublic_hpa_df_17 = public_hpa_df[public_hpa_df.Cellline.isin(celllines)]\nlen(public_hpa_df), len(public_hpa_df_17)\n","d37539e0":"public_hpa_df = public_hpa_df_17","56e4ac0e":"train_df = pd.read_csv('..\/input\/hpa-single-cell-image-classification\/train.csv')","4e6e7e6d":"# Get targets and remove images with multiple labels\ntarget = []\nidxs = []\nfor i,s in enumerate(train_df.Label):\n    target.append([int(x) for x in s.split('|')])\n    idxs.append(len(target[-1])==1)\ntrain_df['target'] = target\ntrain_df = train_df[idxs]\ntrain_df['target'] = [x[0] for x in train_df['target']]\ndel target","7d6110ba":"# downloaded data\ninfo = pd.read_csv('..\/input\/publichpa512x512\/data.csv',index_col=0)\ninfo.rename(columns={'Label_idx':'target'},inplace=True)\ninfo = info.groupby('target').size()\ninfo","d3c20c7b":"counts = (train_df.groupby('target').size().max()-train_df.groupby('target').size()) - info\ncounts = counts.fillna(0)\ncounts = [(i,v) for i,v in enumerate(counts)]\ncounts = sorted(counts,key=lambda x:x[1],reverse=True)\ndownloaded_urls = set()","aaf45092":"counts","b9f5a102":"# Get targets and remove images with multiple labels\ntarget = []\nidxs = []\nfor i,s in enumerate(public_hpa_df.Label_idx):\n    target.append([int(x) for x in s.split('|')])\n    idxs.append(len(target[-1])==1)\npublic_hpa_df['target'] = target\npublic_hpa_df = public_hpa_df[idxs]\npublic_hpa_df['target'] = [x[0] for x in public_hpa_df['target']]\ndel target","0f7276f5":"public_hpa_df.groupby('target').size()","db657807":"info = pd.read_csv('..\/input\/publichpa512x512\/data.csv',index_col=0)\n\ndownloaded = {'Image':[],'Label':[],'Cellline':[],'Label_idx':[]}\ndownloaded = dict([(c,list(info[c])) for c in info.columns])\ndownloaded_urls = set(downloaded['Image'])","1785ca73":"public_hpa_df.tail()","18156c63":"counts = [(y,min(c,500)) for y,c in counts] # just take a max of 500 for each","563c99e4":"save_dir = '\/kaggle\/tmp\/pullic_hpa\/images\/'\nif not os.path.exists(save_dir):\n    os.makedirs(save_dir)\n    \nk= 1   \nfor target,count in counts:\n    df = public_hpa_df[public_hpa_df.target == target]\n    for i, row in df[:min(count,len(df))].iterrows():\n        try:\n            img = row.Image\n            save_path = os.path.join(save_dir,  f'{os.path.basename(img)}.npz')\n\n            if img in downloaded_urls:\n                print(f'Skipping: {img}')\n                continue\n            \n            arr = []\n            for color in colors:\n                img_url = f'{img}_{color}.tif.gz'\n                #save_path = os.path.join(save_dir,  f'{os.path.basename(img)}_{color}.png')\n                \n                arr.append(download_and_convert_tifgzip_to_png(img_url, save_path))\n            print(f'{k} : Downloaded {img_url} as {save_path}')\n            k+=1\n                \n            image = np.savez_compressed(save_path,blue=arr[0],red=arr[1],green=arr[2],yellow=arr[3])\n            downloaded['Image'].append(row.Image)\n            downloaded['Label'].append(row.Label)\n            downloaded['Cellline'].append(row.Cellline)\n            downloaded['Label_idx'].append(row.Label_idx)\n            downloaded_urls.add(row.Image)\n        except Exception as e:\n            print(f'failed to download: {img}',e)\n        \n        # control the time and don't exceed 9 hours\n        if (time.time()-start)> (7*3600):\n            break\n            ","66ebf57f":"downloaded['Image'] = [os.path.basename(img) for img in downloaded['Image']]","ae2fafb3":"pd.DataFrame(downloaded).to_csv('\/kaggle\/tmp\/pullic_hpa\/data.csv')","ba705714":"! kaggle datasets version -p \/kaggle\/tmp\/pullic_hpa -m \"Update\" --dir-mode tar\n#! kaggle datasets create -p \/kaggle\/tmp\/pullic_hpa -u --dir-mode tar","cccbcaf2":"! rm -rf \/root\/.kaggle\/kaggle.json","2334ca76":"# Downloading HPA public data","85b8206e":"! Warning: These are raw, full size images. Each channel is approximately 8MB. With ~~82495~~ 77668 images * 4 channels, everything amounts to around ~~2.6TB~~ 2.4TB of data.\n\nWe have 17 cell lines in the training set and test set. So if you want to download extra public data for training and downloading is too slow because of big size, you probably want to consider:\n- Downloading just 17 cell lines (67k images * 4 channels, instead of 77.6k images)\n- Sampling according to label (eg. You have a lot of Nucleoplasm and Cytosol in the training set already so maybe you just need more rare labels)\n- Using jpeg images. These were purposely created for visualization on HPA website. They are much smaller in size (you will loose some info compared to tif file, but maybe that's enough for your model. Your call). To download jpeg, simply change `.tif.gz` to `.jpg` in the url. For example: https:\/\/images.proteinatlas.org\/10005\/921_B9_1_blue.jpg\t","f6f1f128":"## Update \n\n**2021\/02\/26**\n\nI indicated which images are in trainset in the column `in_trainset`. Please remove them so you don't have to download twice.\n\nI have also added a column `Label_idx`. As I mentioned in [Single Cell Patterns](https:\/\/www.kaggle.com\/lnhtrang\/single-cell-patterns) notebook, we merged some classes for this competition. The function to convert from label names to indexes is also added if you want to double check.","44b351ce":"##  This dataset only consider image having a single cell type","b8a16610":"## How many cells do we have in the public dataset","25a827be":"Dataset link : [https:\/\/www.kaggle.com\/tchaye59\/PUBLICHPA512x512](https:\/\/www.kaggle.com\/tchaye59\/PUBLICHPA512x512)","f116f490":"## How many of each class do we need to have a balanced dataset","0eb562d2":"# Download"}}