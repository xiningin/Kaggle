{"cell_type":{"2c56e642":"code","83220cc3":"code","10a9d9ac":"code","2e65fc57":"code","4138b96b":"code","45091af7":"code","d80c5ff9":"code","0fc3cefa":"code","f5a8ec95":"code","00c13131":"code","470b0dab":"code","d719e395":"code","a8a6c30a":"code","98e32ca5":"code","be43ad8a":"code","5de1dd8f":"code","f6f06645":"code","17b01464":"code","4a2372f3":"code","30b452e6":"code","926a393a":"code","eacdc30e":"code","ad0fc88a":"code","8728b81b":"code","31c94545":"code","7a9cf856":"code","87fff1c8":"code","ed4fdf88":"code","dcc0a1a8":"code","92292025":"code","117aed6d":"code","b1edd345":"code","ba80b2bd":"code","2525f002":"code","19485559":"code","43f43d4b":"code","2299ed24":"code","69983593":"code","10125b26":"markdown","d409f2aa":"markdown","783b88be":"markdown","ced8df4e":"markdown","775d13eb":"markdown","b2a728f0":"markdown","dcfc544f":"markdown","8737a735":"markdown","27db103f":"markdown","ea9bbeea":"markdown","4fac0e89":"markdown","b5ed74d9":"markdown","4c2b19ef":"markdown","70e4997f":"markdown","6c11e17d":"markdown","15636257":"markdown","22eedd8f":"markdown","9544e9fc":"markdown","888eb3b6":"markdown","7a423526":"markdown","2c51d8cc":"markdown","39b43e50":"markdown","fa4c8b65":"markdown","6c7ac7eb":"markdown","384de958":"markdown"},"source":{"2c56e642":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport os\nimport warnings\nwarnings.simplefilter('ignore')\nimport missingno as msn","83220cc3":"os.listdir('..\/input\/')","10a9d9ac":"data_1 = pd.read_csv('..\/input\/Admission_Predict.csv')\ndata_2 = pd.read_csv('..\/input\/Admission_Predict_Ver1.1.csv')","2e65fc57":"print(\"Data Columns: \",np.array(data_1.columns.tolist()))","4138b96b":"#original columns has trailing sapaces in LOR and Chance of Admit. changed Serial no. with Id\ncols = ['Id', 'GRE Score', 'TOEFL Score', 'University Rating', 'SOP',\n       'LOR', 'CGPA', 'Research', 'Chance of Admit']\ndata_1.columns = cols\ndata_2.columns = cols","45091af7":"data_1.head()","d80c5ff9":"data_2.head()","0fc3cefa":"print(\"data_1 length: \",len(data_1))\nprint(\"data_2 length: \",len(data_2))","f5a8ec95":"(data_1 != data_2[:len(data_1)]).sum()","00c13131":"data = data_2.copy()","470b0dab":"msn.matrix(data,figsize=(10,5))","d719e395":"data.describe()","a8a6c30a":"data.info()","98e32ca5":"## numerical and categorical columns\nnum_cols = ['GRE Score','TOEFL Score', 'CGPA']\ncat_cols = ['University Rating', 'SOP','LOR']\nbin_cols = ['Research']","be43ad8a":"g = sns.pairplot(data,hue='Research',vars=num_cols,diag_kind='kde')","5de1dd8f":"g = sns.catplot(x='Research',y='CGPA',data=data,kind='box')\ng.fig.set_size_inches([10,5])","f6f06645":"plt.figure(figsize=(10,5))\nsns.distplot(data[data['Research'] == 0]['CGPA'],label='0',bins=20)\nsns.distplot(data[data['Research'] == 1]['CGPA'],label='1',bins=20)\nplt.legend()\nplt.show()","17b01464":"print(\"Num of students with research experience: \",len(data[data.Research == 1]))\nprint(\"Num of students with NO research experience: \",len(data[data.Research == 0]))\nprint(\"Avg CGPA of students with research experience: \", data[data.Research == 1]['CGPA'].mean())\nprint(\"Avg CGPA of students with NO research experience: \", data[data.Research == 0]['CGPA'].mean())","4a2372f3":"CGPA_thre = 7.5\nprint(\"Num of students with CGPA >= {}: {}\".format(CGPA_thre,len(data[data.CGPA >= CGPA_thre])))\nprint(\"Num of students with CGPA < {}: {}\".format(CGPA_thre,len(data[data.CGPA < CGPA_thre])))\nprint(\"percentage of students with CGPA >= {} and go for research: {}\".format(CGPA_thre,\n                                                                                   len(data[(data.CGPA>=CGPA_thre)&(data.Research == 1)])\/len(data[data.CGPA >= CGPA_thre])))\nprint(\"percentage of students with CGPA < {} and go for research: {}\".format(CGPA_thre,\n                                                                                   len(data[(data.CGPA<CGPA_thre)&(data.Research == 1)])\/len(data[data.CGPA < CGPA_thre])))","30b452e6":"fig,[axs1,axs2] = plt.subplots(ncols=2,figsize=(20,5))\nsns.scatterplot(x='GRE Score',y='CGPA',data=data,hue='SOP',ax=axs1,size='SOP')\nsns.scatterplot(x='GRE Score',y='CGPA',data=data,hue= 'LOR',ax=axs2,size='LOR')\nfig.show()","926a393a":"fig,axs = plt.subplots(nrows=2,ncols=2,figsize=(20,10))\nsns.countplot(x='LOR',data=data,ax=axs[0,0])\nsns.countplot(x='SOP',data=data,ax=axs[0,1])\nsns.countplot(x='University Rating',data=data,ax=axs[1,0])\nsns.countplot(x='Research',data=data,ax=axs[1,1])\nfig.show()","eacdc30e":"_,axs = plt.subplots(ncols=3,figsize=(30,5))\nsns.scatterplot(x='CGPA',y='Chance of Admit',data=data,hue='Research',ax=axs[0])\nsns.scatterplot(x='GRE Score',y='Chance of Admit',data=data,hue='Research',ax=axs[1])\nsns.scatterplot(x='TOEFL Score',y='Chance of Admit',data=data,hue='Research',ax=axs[2])","ad0fc88a":"sns.set_style('whitegrid')\ng = sns.catplot(x='SOP',y='Chance of Admit',data=data,kind='swarm',col='Research')\ng.fig.set_size_inches([20,5])\ng.set_xticklabels(\"\")\nsns.set_style('whitegrid')\ng = sns.catplot(x='LOR',y='Chance of Admit',data=data,kind='swarm',col='Research')\ng.fig.set_size_inches([20,5])\ng.set_titles(\"\")","8728b81b":"g = sns.relplot(x='CGPA',y='Chance of Admit',data=data,col='Research',hue='University Rating',size='University Rating')\ng.fig.set_size_inches([20,5])\ng = sns.catplot(x='Research',y='Chance of Admit',data=data,kind='box')\ng.fig.set_size_inches([10,5])","31c94545":"cols = ['GRE Score','TOEFL Score','SOP','LOR','CGPA','Chance of Admit']\ng = sns.heatmap(data[cols].corr(),annot=True,xticklabels=cols,yticklabels=cols,fmt=\".2f\")\ng.figure.set_size_inches([10,10])","7a9cf856":"X_cols = ['GRE Score','TOEFL Score','University Rating','SOP','LOR','CGPA','Research']\ntarget = 'Chance of Admit'\nX = data[X_cols]\ny = data['Chance of Admit']","87fff1c8":"from sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nrfc = RandomForestRegressor(max_depth=3)\nxgb = XGBRegressor()","ed4fdf88":"rfc.fit(X,y)\nxgb.fit(X,y)","dcc0a1a8":"df_feature_imp = pd.DataFrame()\ndf_feature_imp['feature_name'] = X.columns\ndf_feature_imp['xgb'] = xgb.feature_importances_\ndf_feature_imp['rfc'] = rfc.feature_importances_\ndf_feature_imp","92292025":"_,axs = plt.subplots(ncols=2,sharey=True,figsize=(10,5))\nsns.barplot(x='xgb',y='feature_name',data=df_feature_imp,ax=axs[0])\nsns.barplot(x='rfc',y='feature_name',data=df_feature_imp,ax=axs[1])","117aed6d":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)","b1edd345":"xgb = XGBRegressor(max_depth=3,learning_rate=0.1)\nxgb.fit(X_train,y_train)","ba80b2bd":"from sklearn.metrics import mean_squared_error\nprint(\"Using XGB\")\nprint(\"RMSE score on training data: \",np.sqrt(mean_squared_error(y_true=y_train,y_pred=xgb.predict(X_train))))\nprint(\"RMSE score on testing data: \",np.sqrt(mean_squared_error(y_true=y_test,y_pred=xgb.predict(X_test))))","2525f002":"rfc = RandomForestRegressor(max_depth=3)\nrfc.fit(X_train,y_train)","19485559":"print(\"Using rfc\")\nprint(\"RMSE score on training data: \",np.sqrt(mean_squared_error(y_true=y_train,y_pred=rfc.predict(X_train))))\nprint(\"RMSE score on testing data: \",np.sqrt(mean_squared_error(y_true=y_test,y_pred=rfc.predict(X_test))))","43f43d4b":"df_eval_train = pd.DataFrame()\ndf_eval_test = pd.DataFrame()\n\ndf_eval_train['rfc'] = rfc.predict(X_train)\ndf_eval_train['xgb'] = xgb.predict(X_train)\ndf_eval_train['target'] = y_train.values\n\ndf_eval_test['rfc'] = rfc.predict(X_test)\ndf_eval_test['xgb'] = xgb.predict(X_test)\ndf_eval_test['target'] = y_test.values","2299ed24":"df_eval_train.sort_values('target',inplace=True)\ndf_eval_train.reset_index(drop=True,inplace=True)\n_,axs = plt.subplots(2,2,figsize=(20,10))\naxs[0,0].plot(df_eval_train['xgb'])\naxs[0,0].plot(df_eval_train['target'])\naxs[0,1].plot(df_eval_train['rfc'])\naxs[0,1].plot(df_eval_train['target'])\n\naxs[1,0].plot(df_eval_test['target'])\naxs[1,0].plot(df_eval_test['xgb'],'o')\naxs[1,1].plot(df_eval_test['target'])\naxs[1,1].plot(df_eval_test['rfc'],'o')\naxs[0,0].legend(['target','XGB'])\naxs[0,1].legend(['target','RFC'])\n\naxs[0,0].set_ylabel('Train')\naxs[1,0].set_ylabel('Test')\n\naxs[1,0].set_xlabel('XGB')\naxs[1,1].set_xlabel('RFC')","69983593":"df_eval_test['rfc_diff'] = np.square(df_eval_test['target']-df_eval_test['rfc'])\ndf_eval_test['xgb_diff'] = np.square(df_eval_test['target']-df_eval_test['xgb'])\nplt.figure(figsize=(10,5))\nplt.plot(df_eval_test['rfc_diff'],'o')\nplt.plot(df_eval_test['xgb_diff'],'o')\nplt.legend(['RFC','XGB'])","10125b26":"### **let's visualize how these models are doing on train and test**","d409f2aa":"### Missing Value Visualization","783b88be":"## 2. Data Exploration and Visualization","ced8df4e":"as data_1 is a subset of data_2, we will continue with data_2 only.","775d13eb":"# Predicting chance of admission in a Masters Program.\nin this notebook, we will be looking at different parameters which influence selection for a Masters Program. we will analyse  [Graduate Admissions](https:\/\/www.kaggle.com\/mohansacharya\/graduate-admissions) data set, build multiple probabilistic models and evaluate them. <br\/>\n**Objective :**\n    *  Data exploration and Visualization     \n    *  feature importance\/selection.\n    *  Build Model     ","b2a728f0":"in this data we don't have any missing values. when we have lot of features  it's a good idea to visualize missing values. this give us an idea about inter correlation between features and will help in feature selection.","dcfc544f":"### Relation with target (chance of Admit)","8737a735":"**Take Away**:\nWe know that both of these statment are very obvious. research guys usually have better CGPA and students with better CGPA (better academics) go for research<br\/>\nBut from this data we can't make a strong claim on the second statement because we have very small sample set with CGPA <7.5 (only 3.6% students). <br\/>\nAsking questions from the data and answering them by giving some stats from the data is really a great way for data analysis. it gives better understanding of data and helps in feature engineering. ","27db103f":"Main things I consider for feature selection\n* feature importance provided by tree based models usually random forest \/ xgboost.\n* Percentage of missing values.\n* correlation with target value\n* insights from data visualization\n* also try dimensionality reduction methods. like PCA\n* Recursive feature elimination<br\/>\nwe will take the full data set and see feature importance. we have very less number of feature so feature selection is not that important. but still we ll have look!","ea9bbeea":"**Take away:** \n* from info() we can see there is no null values in data and also give info about the data types.\n* describe() tells that some students got full marks in GRE and TOEFL and there is no student with 10 CGPA. Chance of selection variable has maximum value 0.97 which also makes sense because max CGPA is 9.92. ","4fac0e89":"### Correlation Matrix","b5ed74d9":" [GRE](https:\/\/en.wikipedia.org\/wiki\/Graduate_Record_Examinations): Graduate Record Examinations \n[TOEFL](https:\/\/en.wikipedia.org\/wiki\/Test_of_English_as_a_Foreign_Language\/): Test of English as a Foreign Language\n[SOP](https:\/\/en.wikipedia.org\/wiki\/Standard_operating_procedure): Statement of Purpose\n[LOR](https:\/\/en.wikipedia.org\/wiki\/Letter_of_recommendation): Letter of Recommendation\n[CGPA](https:\/\/en.wikipedia.org\/wiki\/Grading_in_education): Grades","4c2b19ef":"## 3. Feature Importance Visualization\n","70e4997f":" let's explore **CGPA** and **Research Experience**!\n we will be looking for two things:\n* Does students with **research\nexpereince** **research\nexpereince** have better **CGPA** ?\n* Does students with better **CGPA** go for **research**?","6c11e17d":"### Categorical Features","15636257":"**The most important thing usually people miss is looking at the data on which model is not performing well. doing visualization after building model can give an idea about features because of which model is confusing or performing poorly and help improving model performance.** <br\/>\nthings we should look:\n* seperate out data on which model is performing good and poor.\n* Analyse both of these data set together and look for features which are:\n    1. different while looking at true positive \/ false negative data<br\\\n    2. similar in case of true positive \/ false positive data. ","22eedd8f":"**Take Away:**\nwe can see CGPA, GRE Score and TOEFL Score are the most important features from both the models. Thses three also have highest correlation values with target value 0.88 (CGPA), 0.81 (GRE Score), 0.79 (TOEFL Score)","9544e9fc":"## Model","888eb3b6":"## 1. Loading Data","7a423526":"### It's difficult to interpret plots on testing data. we'll try something else.<br\/>\nlet's plot squared error for individual sample ","2c51d8cc":"Please let me know if you see any thing wrong.\nThanks!","39b43e50":"now let's define 7.5 as better CGPA criterion. and see percentage of students with research experience with CGPA >= 7.5 and < 7.5 ","fa4c8b65":"**Take Away:**\n* both models are having problem while making prediction on samples with low target values (<0.6)\n* XGB is performing better than RFC on testing data <br\/>\n**Sorry for naming RFC(Random forest classifier) it should be RFR :P**","6c7ac7eb":"let's look at some plots!","384de958":"**Success is no accident. It is hard work, perseverance, learning, studying, sacrifice and most of all, love of what you are doing or learning to do - [pele](https:\/\/en.wikipedia.org\/wiki\/Pel%C3%A9)**"}}