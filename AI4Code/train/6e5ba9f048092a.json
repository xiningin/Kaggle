{"cell_type":{"eabde5b4":"code","acbbe76d":"code","6b1c73f3":"code","d3506ea6":"code","4cbf1905":"code","b2deed33":"code","921034ae":"code","73da6766":"code","c5dae29d":"code","c92913a0":"code","89c70619":"code","446dcbeb":"code","a3d6ebc6":"code","17eb7f17":"code","8654edf0":"markdown","c7518579":"markdown","6fe47dec":"markdown","33d3b197":"markdown","ac0d3d04":"markdown","d0f3d6b2":"markdown","81f2b3b7":"markdown","6fbcedcc":"markdown","53dbbfc5":"markdown","9cfbeb02":"markdown","4d2a1842":"markdown","c023c3a4":"markdown","520a8d6e":"markdown","ccf0609b":"markdown","8dd88074":"markdown","6fb582ac":"markdown","26501e01":"markdown","4895ec54":"markdown","fcd3952c":"markdown"},"source":{"eabde5b4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","acbbe76d":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom numpy import expand_dims\nfrom numpy import zeros\nfrom numpy import ones\nfrom numpy import vstack\nfrom numpy.random import randn\nfrom numpy.random import randint\nfrom keras.datasets.mnist import load_data\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Reshape\nfrom keras.layers import Flatten\nfrom keras.layers import Conv2D\nfrom keras.layers import Conv2DTranspose\nfrom keras.layers import LeakyReLU\nfrom keras.layers import Dropout\nfrom matplotlib import pyplot\nfrom tensorflow.keras.datasets import mnist","6b1c73f3":"def define_generator(latent_dim):\n\tmodel = Sequential()\n\t# foundation for 7x7 image\n\tn_nodes = 128 * 7 * 7\n\tmodel.add(Dense(n_nodes, input_dim=latent_dim))\n\tmodel.add(LeakyReLU(alpha=0.2))\n\tmodel.add(Reshape((7, 7, 128)))\n\t# upsample to 14x14\n\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n\tmodel.add(LeakyReLU(alpha=0.2))\n\t# upsample to 28x28\n\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n\tmodel.add(LeakyReLU(alpha=0.2))\n\tmodel.add(Conv2D(1, (7,7), activation='sigmoid', padding='same'))\n\treturn model","d3506ea6":"noise = tf.random.normal([1, 100])\ngenerator = define_generator(100)\ngenerated_image = generator(noise, training=False)\nplt.imshow(generated_image[0, :, :, 0], cmap='gray')\ngenerator.save('\/kaggle\/output')","4cbf1905":"def define_discriminator(in_shape=(28,28,1)):\n\tmodel = Sequential()\n\tmodel.add(Conv2D(64, (3,3), strides=(2, 2), padding='same', input_shape=in_shape))\n\tmodel.add(LeakyReLU(alpha=0.2))\n\tmodel.add(Dropout(0.4))\n\tmodel.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n\tmodel.add(LeakyReLU(alpha=0.2))\n\tmodel.add(Dropout(0.4))\n\tmodel.add(Flatten())\n\tmodel.add(Dense(1, activation='sigmoid'))\n\t# compile model\n\topt = Adam(lr=0.0002, beta_1=0.5)\n\tmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n\treturn model","b2deed33":"def define_gan(g_model, d_model):\n\t# make weights in the discriminator not trainable\n\td_model.trainable = False\n\t# connect them\n\tmodel = Sequential()\n\t# add generator\n\tmodel.add(g_model)\n\t# add the discriminator\n\tmodel.add(d_model)\n\t# compile model\n\topt = Adam(lr=0.0002, beta_1=0.5)\n\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n\treturn model","921034ae":"def load_real_samples():\n\t# load mnist dataset\n\t(trainX, _), (_, _) = mnist.load_data()\n\t# expand to 3d, e.g. add channels dimension\n\tX = expand_dims(trainX, axis=-1)\n\t# convert from unsigned ints to floats\n\tX = X.astype('float32')\n\t# scale from [0,255] to [0,1]\n\tX = X \/ 255.0\n\treturn X","73da6766":"def generate_real_samples(dataset, n_samples):\n\t# choose random instances\n\tix = randint(0, dataset.shape[0], n_samples)\n\t# retrieve selected images\n\tX = dataset[ix]\n\t# generate 'real' class labels (1)\n\ty = ones((n_samples, 1))\n\treturn X, y","c5dae29d":"# generate points in latent space as input for the generator\ndef generate_latent_points(latent_dim, n_samples):\n\t# generate points in the latent space\n\tx_input = randn(latent_dim * n_samples)\n\t# reshape into a batch of inputs for the network\n\tx_input = x_input.reshape(n_samples, latent_dim)\n\treturn x_input","c92913a0":"# use the generator to generate n fake examples, with class labels\ndef generate_fake_samples(g_model, latent_dim, n_samples):\n\t# generate points in latent space\n\tx_input = generate_latent_points(latent_dim, n_samples)\n\t# predict outputs\n\tX = g_model.predict(x_input)\n\t# create 'fake' class labels (0)\n\ty = zeros((n_samples, 1))\n\treturn X, y","89c70619":"def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=40, n_batch=256):\n\tbat_per_epo = int(dataset.shape[0] \/ n_batch)\n\thalf_batch = int(n_batch \/ 2)\n\t# manually enumerate epochs\n\tfor i in range(n_epochs):\n\t\t# enumerate batches over the training set\n\t\tfor j in range(bat_per_epo):\n\t\t\t# get randomly selected 'real' samples\n\t\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n\t\t\t# generate 'fake' examples\n\t\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n\t\t\t# create training set for the discriminator\n\t\t\tX, y = vstack((X_real, X_fake)), vstack((y_real, y_fake))\n\t\t\t# update discriminator model weights\n\t\t\td_loss, _ = d_model.train_on_batch(X, y)\n\t\t\t# prepare points in latent space as input for the generator\n\t\t\tX_gan = generate_latent_points(latent_dim, n_batch)\n\t\t\t# create inverted labels for the fake samples\n\t\t\ty_gan = ones((n_batch, 1))\n\t\t\t# update the generator via the discriminator's error\n\t\t\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n\t\t\t# summarize loss on this batch\n\t\t\tprint('>%d, %d\/%d, d=%.3f, g=%.3f' % (i+1, j+1, bat_per_epo, d_loss, g_loss))\n","446dcbeb":"# size of the latent space\nlatent_dim = 100\n# create the discriminator\nd_model = define_discriminator()\n# create the generator\ng_model = define_generator(latent_dim)\n# create the gan\ngan_model = define_gan(g_model, d_model)\n# load image data\ndataset = load_real_samples()\n# train model\ntrain(g_model, d_model, gan_model, dataset, latent_dim)","a3d6ebc6":"noise_dim = 100\nnum_examples_to_generate = 16\n\n# You will reuse this seed overtime (so it's easier)\n# to visualize progress in the animated GIF)\nseed = tf.random.normal([num_examples_to_generate, noise_dim])\npredictions = g_model(seed, training=False)\n\nfig = plt.figure(figsize=(4, 4))\nfor i in range(predictions.shape[0]):\n      plt.subplot(4, 4, i+1)\n      plt.imshow(predictions[i, :, :, 0] , cmap='gray')\n      plt.axis('off')\n        \ng_model.save('\/kaggle\/output')","17eb7f17":"gmdl_new=keras.models.load_model('\/kaggle\/output')\nseed = tf.random.normal([num_examples_to_generate, noise_dim])\npredictions = gmdl_new(seed, training=False)\n\nfig = plt.figure(figsize=(4, 4))\nfor i in range(predictions.shape[0]):\n      plt.subplot(4, 4, i+1)\n      plt.imshow(predictions[i, :, :, 0] , cmap='gray')\n      plt.axis('off')","8654edf0":"## Creating fake dataset for Discriminator","c7518579":"![image.png](attachment:dd6f6c88-6209-4d48-af09-10e8745d31fe.png)","6fe47dec":"## Discriminator\nIt works as a binary clasifier and returns the probability whether an image is fake or real","33d3b197":"* **Load Real Data: -** Gets the data from MNIST\n* **Generate Real Data: -** Gets a sample from the real dattaset, and apapend class label '1'\n* **Genarate Noise Data:-** Creates random noise as input to generator\n* **Generate Fake Date:-** Gets a sample from generator and append class label '0'","ac0d3d04":"## Loading Libraries","d0f3d6b2":"## Creating a dataset from MNIST Images with class lebel as 1","81f2b3b7":"## Loading a saved model","6fbcedcc":"# Generative Adversarial Network(GAN):\n- These are one of the coolest addition to the deep learning family. The objective of GAN, is to generalte synthetic samples after which are very realistic like other generative models\n- Genearative models learns the trick in an adversarial setting\n- There are two multi layer neural networks one acting as a generator another as an adversary of the same, called the discriminator\n- Both are trainied using regualar back propagation method, although with different and conflicting loss function (Adversarial Setup)\n- Once the training is done, the discriminator is removed and the generator is used to produce the samples","53dbbfc5":"## Testing the Generation","9cfbeb02":"![image.png](attachment:d18de706-1c41-433a-8e76-6b1c22cd8c72.png)","4d2a1842":"## Training mehod GAN","c023c3a4":"## Generator\nIt is going to take a noise vector of dimension 100 and create an image of sige 28*28","520a8d6e":"### Testing the Generator","ccf0609b":"## Load the data and do some pre-processing","8dd88074":"## Creating input for Generator","6fb582ac":"## Creating the Combined Model","26501e01":"## Training","4895ec54":"## General Working","fcd3952c":"* We will create a generator model, which will take a noise vector of k dimension as input and will produce an image of expected size ( Through convolutions and upsampling)\n* We will create a discriminator model which will work as a binary classifier, one side it will be have the real image as class '1' and fake image as class '0'\n* Generator model will want the produced output to be classified as class 1, the loss function will be likewise\n"}}