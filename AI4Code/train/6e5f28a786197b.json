{"cell_type":{"99f1529d":"code","20410ce0":"code","7c03ddfa":"code","2d7a6219":"code","632d1a02":"code","1467fd6b":"code","4ea6b7bb":"code","69f35b06":"code","01590967":"code","4cd17600":"code","6c908bc4":"code","c54437dc":"code","a40f1ab8":"code","d4635c28":"code","89580c09":"code","e362cc41":"code","7682418d":"code","aa30108d":"code","cc509123":"code","57feca1f":"code","d592ba4f":"code","e750d1e6":"code","efe66a68":"code","bc6ec4d5":"code","8aec45f0":"code","7edd191d":"markdown","5f4f0e1b":"markdown","cefdefe3":"markdown","3c0aaebf":"markdown","ec0e334c":"markdown","666e959c":"markdown","88b13a55":"markdown","65a617af":"markdown","eddd42df":"markdown","8dacfa0b":"markdown","b7a47f34":"markdown","ef67faef":"markdown","2be8ab36":"markdown","1787e1b7":"markdown","45c2ce2b":"markdown","e0afbcad":"markdown","3c87ab0b":"markdown","0a2447e5":"markdown","979c1475":"markdown","0508726c":"markdown","99c982fc":"markdown","a3929b6b":"markdown"},"source":{"99f1529d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","20410ce0":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")","7c03ddfa":"train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')","2d7a6219":"train.shape","632d1a02":"test.shape","1467fd6b":"train.head(2)","4ea6b7bb":"test.head(2)","69f35b06":"X = train.drop('label',axis=1)\nY = train['label']","01590967":"X.isnull().any().describe()","4cd17600":"X.shape","6c908bc4":"X = X.values.reshape(-1, 28,28,1)","c54437dc":"fig, ax_arr = plt.subplots(10, 10, figsize=(7, 7))\nfig.subplots_adjust(wspace=.025, hspace=.025)\n\nax_arr = ax_arr.ravel()\nfor i, ax in enumerate(ax_arr):\n    ax.imshow(X[i], cmap=\"gray\")\n    ax.axis(\"off\")\n    \nplt.show()","a40f1ab8":"X.shape #Shape of X_train","d4635c28":"import seaborn as sns\nsns.countplot(Y)","89580c09":"test.isnull().any().describe()","e362cc41":"Y = Y.values\ntype(Y)","7682418d":"X = np.array(X, dtype=\"float\") \/ 255.0 * 0.99 + 0.01","aa30108d":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28,28,1)),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(10, activation='softmax')])\n\nmodel.summary() #model summary\n\n# compile the model\nmodel.compile(\n    optimizer='adam',\n    loss = 'sparse_categorical_crossentropy',\n    metrics = ['acc']) ","cc509123":"history = model.fit(X, Y, validation_split=0.1, epochs=40, batch_size=128, verbose=0)\n\n# list all data in history\nprint(history.history.keys())\n\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","57feca1f":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42)\nmodel.fit(X_train,y_train, epochs=6, validation_data=(X_test,y_test), batch_size=128, verbose=1)\n\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\ny_pred = model.predict(X_train)\n# Convert predictions classes to one hot vectors \ny_pred_classes = np.argmax(y_pred,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(y_train, y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","d592ba4f":"# Predict the values from the validation dataset\ny_pred = model.predict(X_test)\n# Convert predictions classes to one hot vectors \ny_pred_classes = np.argmax(y_pred,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(y_test, y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","e750d1e6":"# Errors are difference between predicted labels and true labels\nerrors = (y_pred_classes - y_test != 0)\n\nY_pred_classes_errors = y_pred_classes[errors]\nY_pred_errors = y_pred[errors]\nY_true_errors = y_test[errors]\nX_val_errors = X_test[errors]\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n\n# Probabilities of the wrong predicted numbers\nY_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 6 errors \nmost_important_errors = sorted_dela_errors[-10:]\n\n# Show the top 6 errors\ndisplay_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)","efe66a68":"test = test.values.reshape(-1,28,28,1)\ntest = np.array(test, dtype=\"float\") \/ 255.0 * 0.99 + 0.01\ntest.shape","bc6ec4d5":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.001, random_state=42)\nmodel.fit(X_train,y_train, epochs=6, validation_data=(X_test,y_test), batch_size=128, verbose=0)\n\npredictions = model.predict(test)\nresults = predictions.argmax(axis=-1)\n#check if your model predicted correctly or not\nprint(\"Prediction result for a score {}\".format(results[22250]))\nplt.imshow(test[22250]) #model predicted correclty ","8aec45f0":"result = pd.DataFrame()\nresult['ImageId'] = list(range(1,28001))\nresult['Label'] = results\nresult.to_csv(\"output.csv\", index = False)","7edd191d":"The MNIST database (Modified National Institute of Standards and Technology database) of handwritten digits consists of a training set of 42,000 examples, and a test set of 28,000 examples. It is a subset of a larger set available from NIST. Additionally, the black and white images from NIST were size-normalized and centered to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels.\n\nThis database is well liked for training and testing in the field of machine learning and image processing. It is a remixed subset of the original NIST datasets. ","5f4f0e1b":"# Now, submit the prediction result","cefdefe3":"# Data normalization","3c0aaebf":"# Data processing","ec0e334c":"Reshape image in 3 dimensions (height = 28px, width = 28px, canal = 1)\n","666e959c":"To better understand what happen\nDisplay some error results","88b13a55":"Let\u2019s create the architecture for our CNN model. The architecture is simple, it has three Convolutional layers and two fully connected layers.","65a617af":"# We have all setup, we now create a convolution model using tensorflow.keras ","eddd42df":"Now, we reshape the data in 3 dimensions to represent an image:\n+ -1 keeps the number of data as it, values convert the dataframe to arrays\n+ 28, 28 is height and width\n+ 1 is grayscale, if we have coloured we should use 3.","8dacfa0b":"# Reading the MNIST data set","b7a47f34":"Note that, the algorithm converges faster on [0 to 1] data than on [0 to 255]. The images of the MNIST dataset are greyscale and the pixels range between 0 and 255 including both bounding values. We will map these values into an interval from [0.01 to 1] by multiplying each pixel by 0.99 \/ 255 and adding 0.01 to the result. This way, we avoid 0 values as inputs, which are capable of preventing weight updates.","ef67faef":"# Check if the dataset is unbalanced?","2be8ab36":"# Train the Model\n\nFinally, let\u2019s train our model and see if the augmentations had any positive impact on the result!","1787e1b7":"# Confusion matrix\u00b6","45c2ce2b":"# Image representation","e0afbcad":"# Introduction","3c87ab0b":"Now, apply the model to predict the test dataset and check the result","0a2447e5":"# Check for null and missing values","979c1475":"The images from the data set have the size 28 x 28. Every line of these files consists of an image, i.e. 785 numbers between 0 and 255. The first number of each line is the label, i.e. the digit which is depicted in the image. The following 784 numbers are the pixels of the 28 x 28 image.","0508726c":"Confusion matrix can be very helpfull to see your model drawbacks. We plot the confusion matrix of the validation results.","99c982fc":"# Now, we convert Y from series type to array type","a3929b6b":"# Process the prediction data set by reshaping & normalizing the data"}}