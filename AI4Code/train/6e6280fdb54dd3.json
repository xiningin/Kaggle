{"cell_type":{"41394aee":"code","1e3c9a9a":"code","262309b8":"code","a5ab7c66":"code","ce0fbe3c":"code","db4e147a":"code","800fc5a1":"code","574a73ee":"code","9afc2410":"code","9dd7084b":"code","e23884ea":"code","a94eb651":"code","54cb7e95":"code","05f5f32c":"code","505e9b6b":"markdown","4839a7f2":"markdown","38c120c3":"markdown","231bd146":"markdown","b0d4a771":"markdown","e63df64b":"markdown","9ead2636":"markdown","7d2e2f0c":"markdown","4b15074a":"markdown","5eb48317":"markdown","900b0bd9":"markdown","f236b3ae":"markdown","f2a98b01":"markdown"},"source":{"41394aee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1e3c9a9a":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport missingno as msno \nfrom sklearn.model_selection import train_test_split","262309b8":"data = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndata.head()","a5ab7c66":"msno.matrix(data)","ce0fbe3c":"#Eliminate NaN value column\ndata = data.iloc[:,:32]\ndata.head()","db4e147a":"data.diagnosis.replace(['M','B'],[0,1],inplace = True)\ndata.head()","800fc5a1":"id_c = data['id'] \ndel data['id']\ndata.head()","574a73ee":"x_data = data.iloc[:,1:]\ny_data = data.iloc[:,:1]\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.1, random_state = 0)","9afc2410":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","9dd7084b":"#Import Keras and its packages.\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier","e23884ea":"classifier = Sequential() #Initialising the ANN\n# Let's add input layer and first hidden layer\nclassifier.add(Dense(units =16, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[1]))\nclassifier.add(Dropout(0.01))\n# Let's add 2 more hidden layer and output layer\n\n#2 hidden layers\nclassifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dropout(0.01))\n\nclassifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dropout(0.01))\n\n# Output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n#Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n#Fitting data\nclassifier.fit(x_train, y_train, epochs = 150, batch_size = 100)","a94eb651":"# Predicting the Test set results\ny_pred = classifier.predict(x_test)\ny_pred = (y_pred > 0.5)","54cb7e95":"# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\nprint(\"Classifier accuracy : {}%\".format(((cm[0][0] + cm[1][1])\/57)*100))","05f5f32c":"sns.heatmap(cm,annot=True)\nplt.savefig('hm.png')","505e9b6b":"### We will create a function for our classifier","4839a7f2":"### Change \"diagnosis\" column values to numeric from string","38c120c3":"About Breast Cancer Wisconsin (Diagnostic) Data Set Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. n the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n\nThis database is also available through the UW CS ftp server: ftp ftp.cs.wisc.edu cd math-prog\/cpo-dataset\/machine-learn\/WDBC\/\n\nAlso can be found on UCI Machine Learning Repository: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+%28Diagnostic%29\n\nAttribute Information:\n\n1) ID number 2) Diagnosis (M = malignant, B = benign) 3-32)\n\nTen real-valued features are computed for each cell nucleus:\n\na) radius (mean of distances from center to points on the perimeter) b) texture (standard deviation of gray-scale values) c) perimeter d) area e) smoothness (local variation in radius lengths) f) compactness (perimeter^2 \/ area - 1.0) g) concavity (severity of concave portions of the contour) h) concave points (number of concave portions of the contour) i) symmetry j) fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n\nAll feature values are recoded with four significant digits.\n\nMissing attribute values: none\n\nClass distribution: 357 benign, 212 malignant","231bd146":"## 2.Explore Dataset","b0d4a771":"# Thank you for reading my notebook. I am open to suggestions if you have any. If you liked my notebook, please support and encourage me for new notebooks with upvoting \ud83d\ude43\ud83d\ude42","e63df64b":"### Save 'id' column with assuming that we will make submission(because of there is no competition with the dataset, we will not make submission) and then drop it because there is need for it in training.","9ead2636":"### output_dim and units are the same. output_dim is 1 as we want only 1 output from the final layer.","7d2e2f0c":"## 3.Split Data into the Training set and Test set","4b15074a":"## 1.Import Libraries","5eb48317":"<a id=\"22\"><\/a> <br>\n## Implementing with keras library\nLets look at some parameters of keras library:\n* units: output dimensions of node\n* kernel_initializer: to initialize weights\n* activation: activation function, we use relu\n* input_dim: input dimension that is number of pixels in our images (4096 px)\n* optimizer: we use adam optimizer\n    * Adam is one of the most effective optimization algorithms for training neural networks.\n    * Some advantages of Adam is that relatively low memory requirements and usually works well even with little tuning of hyperparameters\n* loss: Cost function is same. By the way the name of the cost function is cross-entropy cost function that we use previous parts.\n$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n* metrics: it is accuracy.\n* epochs: number of iteration\n* Batch size defines number of samples that going to be propagated through the network.","900b0bd9":"\n## Now data is ready. It's time to prepare classifier.","f236b3ae":"### Unnamed:32 column is full of NaN values, we will drop it","f2a98b01":"## 4.Feature Scaling"}}