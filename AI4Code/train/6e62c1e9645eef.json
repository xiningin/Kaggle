{"cell_type":{"3f363750":"code","16833149":"code","984dc6ac":"code","686d0494":"code","ddcdf2fb":"code","aae90aca":"code","14345723":"code","7757c274":"code","6a4f4df5":"code","57d63439":"code","3d85cf96":"code","1b6da837":"code","37723246":"code","acf917c5":"code","bf5a0bb0":"code","f7ab1221":"code","1b8fe033":"code","a5bf51a7":"code","6c190989":"code","746b971a":"code","41bb9458":"code","f99a9f51":"code","098339b4":"code","81bdb86d":"code","841be25d":"code","85eea3cf":"code","dec68201":"code","b1b3da1c":"code","f8fbe336":"code","96a3c510":"code","43657eca":"code","eb8e3911":"code","394bc57f":"markdown","c60b5d01":"markdown","bc049382":"markdown","67d47a57":"markdown","81e38d44":"markdown","437a801a":"markdown","da4d842b":"markdown","b17cca32":"markdown","132b0ac6":"markdown","cd686459":"markdown","9cce0821":"markdown","02aa10da":"markdown"},"source":{"3f363750":"# from google.colab import drive\n# drive.mount('\/content\/drive')","16833149":"# !mkdir \/kaggle\n# !ln -s \/content\/drive\/My\\ Drive\/Kaggle \/kaggle\/input\n","984dc6ac":"# D_WORK='\/kaggle\/input\/stanford-covid-vaccine\/work\/kg-openvaccine-ae-v2\/'\nD_WORK='.\/'","686d0494":"# !mkdir {D_WORK}","ddcdf2fb":"# !ls {D_WORK}","aae90aca":"!pip install -q keras-adamw","14345723":"pretrain_dir = None#\"\/kaggle\/input\/covid-v9-no-consis\/\"\n\none_fold = False\n# one_fold = True#False\n# with_ae = False#True\nrun_test = False\n# run_test = True\ndenoise = True\n\n# ae_epochs = 20\n# ae_epochs_each = 5\n# ae_batch_size = 32\n\nae_epochs = 50\nae_epochs_each = 15\nae_batch_size = 16\n\n# epochs_list = [30, 10, 3, 3, 5, 5]\nepochs_list = [60, 20, 5, 3, 3, 5, 5]\nbatch_size_list = [4, 8, 16, 32, 64, 128, 256]\n\n## copy pretrain model to working dir\nimport shutil\nimport glob\nif pretrain_dir is not None:\n    for d in glob.glob(pretrain_dir + \"*\"):\n        shutil.copy(d, \".\")\n    \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline","7757c274":"import json\nimport glob\nfrom tqdm.notebook import tqdm\n\ntrain = pd.read_json(\"\/kaggle\/input\/stanford-covid-vaccine\/train.json\",lines=True)\nif denoise:\n    print('Number of rows', len(train))\n    train = train[train.signal_to_noise > 1].reset_index(drop = True)\n    print('Number of rows after SNR filtering', len(train))\ntest  = pd.read_json(\"\/kaggle\/input\/stanford-covid-vaccine\/test.json\",lines=True)\ntest_pub = test[test[\"seq_length\"] == 107]\ntest_pri = test[test[\"seq_length\"] == 130]\nsub = pd.read_csv(\"\/kaggle\/input\/stanford-covid-vaccine\/sample_submission.csv\")\n\nif run_test:\n    train = train[:30]\n    test_pub = test_pub[:30]\n    test_pri = test_pri[:30]\n","6a4f4df5":"!unzip -qq \/kaggle\/input\/stanford-covid-vaccine\/bpps.zip -d {D_WORK}","57d63439":"As = []\nfor id in tqdm(train[\"id\"]):\n    # a = np.load(f\"\/kaggle\/input\/stanford-covid-vaccine\/bpps\/{id}.npy\")\n    a = np.load(f\"{D_WORK}\/bpps\/{id}.npy\")\n    As.append(a)\nAs = np.array(As)\nAs_pub = []\nfor id in tqdm(test_pub[\"id\"]):\n    # a = np.load(f\"\/kaggle\/input\/stanford-covid-vaccine\/bpps\/{id}.npy\")\n    a = np.load(f\"{D_WORK}\/bpps\/{id}.npy\")\n    As_pub.append(a)\nAs_pub = np.array(As_pub)\nAs_pri = []\nfor id in tqdm(test_pri[\"id\"]):\n    # a = np.load(f\"\/kaggle\/input\/stanford-covid-vaccine\/bpps\/{id}.npy\")\n    a = np.load(f\"{D_WORK}\/bpps\/{id}.npy\")\n    As_pri.append(a)\nAs_pri = np.array(As_pri)","3d85cf96":"print(train.shape)\ntrain.head()","1b6da837":"print(test.shape)\ntest.head()","37723246":"print(sub.shape)\nsub.head()","acf917c5":"targets = list(sub.columns[1:])\nprint(targets)\n\ny_train = []\nseq_len = train[\"seq_length\"].iloc[0]\nseq_len_target = train[\"seq_scored\"].iloc[0]\nignore = -10000\nignore_length = seq_len - seq_len_target\nfor target in targets:\n    y = np.vstack(train[target])\n    dummy = np.zeros([y.shape[0], ignore_length]) + ignore\n    y = np.hstack([y, dummy])\n    y_train.append(y)\ny = np.stack(y_train, axis = 2)\ny.shape","bf5a0bb0":"def get_structure_adj(train):\n    Ss = []\n    for i in tqdm(range(len(train))):\n        seq_length = train[\"seq_length\"].iloc[i]\n        structure = train[\"structure\"].iloc[i]\n        sequence = train[\"sequence\"].iloc[i]\n\n        cue = []\n        a_structures = {\n            (\"A\", \"U\") : np.zeros([seq_length, seq_length]),\n            (\"C\", \"G\") : np.zeros([seq_length, seq_length]),\n            (\"U\", \"G\") : np.zeros([seq_length, seq_length]),\n            (\"U\", \"A\") : np.zeros([seq_length, seq_length]),\n            (\"G\", \"C\") : np.zeros([seq_length, seq_length]),\n            (\"G\", \"U\") : np.zeros([seq_length, seq_length]),\n        }\n        a_structure = np.zeros([seq_length, seq_length])\n        for i in range(seq_length):\n            if structure[i] == \"(\":\n                cue.append(i)\n            elif structure[i] == \")\":\n                start = cue.pop()\n#                 a_structure[start, i] = 1\n#                 a_structure[i, start] = 1\n                a_structures[(sequence[start], sequence[i])][start, i] = 1\n                a_structures[(sequence[i], sequence[start])][i, start] = 1\n        \n        a_strc = np.stack([a for a in a_structures.values()], axis = 2)\n        a_strc = np.sum(a_strc, axis = 2, keepdims = True)\n        Ss.append(a_strc)\n    \n    Ss = np.array(Ss)\n    print(Ss.shape)\n    return Ss\nSs = get_structure_adj(train)\nSs_pub = get_structure_adj(test_pub)\nSs_pri = get_structure_adj(test_pri)","f7ab1221":"def get_distance_matrix(As):\n    idx = np.arange(As.shape[1])\n    Ds = []\n    for i in range(len(idx)):\n        d = np.abs(idx[i] - idx)\n        Ds.append(d)\n\n    Ds = np.array(Ds) + 1\n    Ds = 1\/Ds\n    Ds = Ds[None, :,:]\n    Ds = np.repeat(Ds, len(As), axis = 0)\n    \n    Dss = []\n    for i in [1, 2, 4]:\n        Dss.append(Ds ** i)\n    Ds = np.stack(Dss, axis = 3)\n    print(Ds.shape)\n    return Ds\n\nDs = get_distance_matrix(As)\nDs_pub = get_distance_matrix(As_pub)\nDs_pri = get_distance_matrix(As_pri)","1b8fe033":"## concat adjecent\nAs = np.concatenate([As[:,:,:,None], Ss, Ds], axis = 3).astype(np.float32)\nAs_pub = np.concatenate([As_pub[:,:,:,None], Ss_pub, Ds_pub], axis = 3).astype(np.float32)\nAs_pri = np.concatenate([As_pri[:,:,:,None], Ss_pri, Ds_pri], axis = 3).astype(np.float32)\ndel Ss, Ds, Ss_pub, Ds_pub, Ss_pri, Ds_pri\nAs.shape, As_pub.shape, As_pri.shape","a5bf51a7":"## sequence\ndef return_ohe(n, i):\n    tmp = [0] * n\n    tmp[i] = 1\n    return tmp\n\ndef get_input(train):\n    mapping = {}\n    vocab = [\"A\", \"G\", \"C\", \"U\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_node = np.stack(train[\"sequence\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n\n    mapping = {}\n    vocab = [\"S\", \"M\", \"I\", \"B\", \"H\", \"E\", \"X\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_loop = np.stack(train[\"predicted_loop_type\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n    \n    mapping = {}\n    vocab = [\".\", \"(\", \")\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_structure = np.stack(train[\"structure\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n    \n    \n    X_node = np.concatenate([X_node, X_loop], axis = 2)\n    \n    ## interaction\n    a = np.sum(X_node * (2 ** np.arange(X_node.shape[2])[None, None, :]), axis = 2)\n    vocab = sorted(set(a.flatten()))\n    print(vocab)\n    ohes = []\n    for v in vocab:\n        ohes.append(a == v)\n    ohes = np.stack(ohes, axis = 2)\n    X_node = np.concatenate([X_node, ohes], axis = 2).astype(np.float32)\n    \n    \n    print(X_node.shape)\n    return X_node\n\nX_node = get_input(train)\nX_node_pub = get_input(test_pub)\nX_node_pri = get_input(test_pri)","6c190989":"import tensorflow as tf\nfrom tensorflow.keras import layers as L\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import backend as K\n\n","746b971a":"# # Detect hardware, return appropriate distribution strategy\n# try:\n#     # TPU detection. No parameters necessary if TPU_NAME environment variable is\n#     # set: this is always the case on Kaggle.\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n#     print('Running on TPU ', tpu.master())\n# except ValueError:\n#     tpu = None\n\n# if tpu:\n#     tf.config.experimental_connect_to_cluster(tpu)\n#     tf.tpu.experimental.initialize_tpu_system(tpu)\n#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# else:\n#     # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n#     strategy = tf.distribute.get_strategy()\n\n# print(\"REPLICAS: \", strategy.num_replicas_in_sync)","41bb9458":"import os\nos.environ[\"TF_KERAS\"]=\"1\"\nfrom keras_adamw import AdamW","f99a9f51":"def mcrmse(t, p, seq_len_target = seq_len_target):\n    score = np.mean(np.sqrt(np.mean((p - y_va) ** 2, axis = 2))[:, :seq_len_target])\n    return score\n\ndef mcrmse_loss(t, y, seq_len_target = seq_len_target):\n    t = t[:, :seq_len_target]\n    y = y[:, :seq_len_target]\n    \n    loss = tf.reduce_mean(tf.sqrt(tf.reduce_mean((t - y) ** 2, axis = 1)))\n\n    return loss\n\ndef attention(x_inner, x_outer, n_factor, dropout):\n    x_Q =  L.Conv1D(n_factor, 1, activation='linear', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(x_inner)\n    x_K =  L.Conv1D(n_factor, 1, activation='linear', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(x_outer)\n    x_V =  L.Conv1D(n_factor, 1, activation='linear', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(x_outer)\n    x_KT = L.Permute((2, 1))(x_K)\n    res = L.Lambda(lambda c: K.batch_dot(c[0], c[1]) \/ np.sqrt(n_factor))([x_Q, x_KT])\n#     res = tf.expand_dims(res, axis = 3)\n#     res = L.Conv2D(16, 3, 1, padding = \"same\", activation = \"relu\")(res)\n#     res = L.Conv2D(1, 3, 1, padding = \"same\", activation = \"relu\")(res)\n#     res = tf.squeeze(res, axis = 3)\n    att = L.Lambda(lambda c: K.softmax(c, axis=-1))(res)\n    att = L.Lambda(lambda c: K.batch_dot(c[0], c[1]))([att, x_V])\n    return att\n\ndef multi_head_attention(x, y, n_factor, n_head, dropout):\n    if n_head == 1:\n        att = attention(x, y, n_factor, dropout)\n    else:\n        n_factor_head = n_factor \/\/ n_head\n        heads = [attention(x, y, n_factor_head, dropout) for i in range(n_head)]\n        att = L.Concatenate()(heads)\n        att = L.Dense(n_factor, \n                      kernel_initializer='glorot_uniform',\n                      bias_initializer='glorot_uniform',\n                      \n                     )(att)\n    x = L.Add()([x, att])\n    x = L.LayerNormalization()(x)\n    # x = L.BatchNormalization()(x)\n    if dropout > 0:\n        x = L.Dropout(dropout)(x)\n    return x\n\ndef res(x, unit, kernel = 3, rate = 0.1):\n    h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n    h = L.LayerNormalization()(h)\n    # x = L.BatchNormalization()(x)\n    h = L.LeakyReLU()(h)\n    h = L.Dropout(rate)(h)\n    return L.Add()([x, h])\n\ndef forward(x, unit, kernel = 3, rate = 0.1):\n#     h = L.Dense(unit, None)(x)\n    h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n    h = L.LayerNormalization()(h)\n    # x = L.BatchNormalization()(x)\n    h = L.Dropout(rate)(h)\n    # h = tf.keras.activations.swish(h)\n    h = L.LeakyReLU()(h)\n    h = res(h, unit, kernel, rate)\n    return h\n\ndef adj_attn(x, adj, unit, n = 2, rate = 0.1):\n    x_a = x\n    x_as = []\n    for i in range(n):\n        x_a = forward(x_a, unit)\n        x_a = tf.matmul(adj, x_a)\n        x_as.append(x_a)\n    if n == 1:\n        x_a = x_as[0]\n    else:\n        x_a = L.Concatenate()(x_as)\n    x_a = forward(x_a, unit)\n    return x_a\n\n\ndef get_base(config):\n    node = tf.keras.Input(shape = (None, X_node.shape[2]), name = \"node\")\n    adj = tf.keras.Input(shape = (None, None, As.shape[3]), name = \"adj\")\n    \n    adj_learned = L.Dense(1, \"relu\")(adj)\n    adj_all = L.Concatenate(axis = 3)([adj, adj_learned])\n        \n    xs = []\n    xs.append(node)\n    x1 = forward(node, 128, kernel = 3, rate = 0.0)\n    x2 = forward(x1, 64, kernel = 6, rate = 0.0)\n    x3 = forward(x2, 32, kernel = 15, rate = 0.0)\n    x4 = forward(x3, 16, kernel = 30, rate = 0.0)\n    x = L.Concatenate()([x1, x2, x3, x4])\n    \n    for unit in [64, 32]:\n        x_as = []\n        for i in range(adj_all.shape[3]):\n            x_a = adj_attn(x, adj_all[:, :, :, i], unit, rate = 0.0)\n            x_as.append(x_a)\n        x_c = forward(x, unit, kernel = 30)\n        \n        x = L.Concatenate()(x_as + [x_c])\n        x = forward(x, unit)\n        x = multi_head_attention(x, x, unit, 4, 0.0)\n        xs.append(x)\n        \n    x = L.Concatenate()(xs)\n\n    model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n    return model\n\n\ndef get_ae_model(base, config):\n    node = tf.keras.Input(shape = (None, X_node.shape[2]), name = \"node\")\n    adj = tf.keras.Input(shape = (None, None, As.shape[3]), name = \"adj\")\n\n    x = base([L.SpatialDropout1D(0.3)(node), adj])\n    x = forward(x, 64, rate = 0.3)\n    p = L.Dense(X_node.shape[2], \"sigmoid\")(x)\n    \n    loss = - tf.reduce_mean(20 * node * tf.math.log(p + 1e-4) + (1 - node) * tf.math.log(1 - p + 1e-4))\n    model = tf.keras.Model(inputs = [node, adj], outputs = [loss])\n    \n    opt = get_optimizer()\n    model.compile(optimizer = opt, loss = lambda t, y : y)\n    return model\n\n\ndef get_model(base, config):\n    node = tf.keras.Input(shape = (None, X_node.shape[2]), name = \"node\")\n    adj = tf.keras.Input(shape = (None, None, As.shape[3]), name = \"adj\")\n    \n    x = base([node, adj])\n    x = forward(x, 128, rate = 0.4)\n    x = L.Dense(5, None)(x)\n\n    model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n    \n    opt = get_optimizer()\n    model.compile(optimizer = opt, loss = mcrmse_loss)\n    return model\n\ndef get_optimizer():\n    # sgd = tf.keras.optimizers.SGD(0.05, momentum = 0.9, nesterov=True)\n    # adam = tf.optimizers.Adam()\n    # radam = tfa.optimizers.RectifiedAdam()\n    # lookahead = tfa.optimizers.Lookahead(adam, sync_period=6)\n    # swa = tfa.optimizers.SWA(adam)\n    adam = AdamW(learning_rate=0.0005)\n\n    return adam","098339b4":"import matplotlib.pyplot as plt\nfrom IPython.display import clear_output        \nimport seaborn as sns\nimport pathlib\n\nclass TrainingPlot(tf.keras.callbacks.Callback):\n    def __init__(self, nrows=1, ncols=2, figsize=(10, 5), title=None, save_file=None, old_logs_path=None):\n        self.nrows=nrows\n        self.ncols=ncols\n        self.figsize=figsize\n        self.title=title\n        self.old_logs_path=old_logs_path\n        self.old_logs = []\n        self.old_log_file_names = []\n\n        if self.old_logs_path is not None:\n            p = pathlib.Path(self.old_logs_path)\n            self.old_logs_files = p.parent.glob(p.name)\n\n            for f in self.old_logs_files:\n                try:\n                    self.old_logs.append(pd.read_csv(f))\n                    self.old_log_file_names.append(pathlib.Path(f).stem)\n                except:\n                    continue\n\n        if save_file:\n            self.save_file = save_file\n        \n        self.metrics = []\n        self.logs = []\n        \n    def add(self, row, col, name, color=None, vmin=None, vmax=None, show_min=False, show_max=False):\n        self.metrics.append({'row': row, 'col': col, 'name': name, 'color': color, 'vmin': vmin, 'vmax': vmax, 'show_min': show_min, 'show_max': show_max})\n    \n    def on_train_begin(self, logs={}):\n        self.logs = []\n        \n        for m in self.metrics:\n            m['values'] = []\n\n    def on_epoch_end(self, epoch, logs={}):\n        self.logs.append(logs)\n        \n        clear_output(wait=True)\n        plt.style.use(\"seaborn\")\n        # sns.set_style(\"whitegrid\")\n        fig, ax = plt.subplots(self.nrows, self.ncols, figsize=self.figsize)\n\n        if len(ax.shape) == 1:\n            ax = np.expand_dims(ax, axis=0)\n        \n        if self.title:\n            fig.suptitle(self.title)\n        \n        for m in self.metrics:\n            if logs.get(m['name']) is None:\n                v = m['values']\n                v.append(np.nan)\n                continue\n            \n            a = ax[m['row'], m['col']]\n            \n            if m['name'] == 'off':\n                a.axis('off')\n                continue\n\n            v = m['values']\n            v.append(logs.get(m['name']))\n                                \n            # old logs\n            for i, old_log in enumerate(self.old_logs):\n                if m['name'] not in old_log:\n                    continue\n                old_values = old_log[m['name']].values\n                a.plot(np.arange(len(old_values)), old_values, \n                     '-', \n                     color=m['color'], \n                    #  label=self.old_log_file_names[i], # gets crowded\n                     alpha=0.2, lw=1)\n\n            # new log\n            a.plot(np.arange(len(v)), v, '-o', color=m['color'], label=m['name'], lw=1, markersize=3)\n            a.set_xlabel('Epoch #', size=14)\n            \n            yname = m['name']\n            if yname.startswith('val_'):\n                yname = m['name'][4:]\n            a.set_ylabel(yname, size=14)\n\n            xdist = a.get_xlim()[1] - a.get_xlim()[0]\n            ydist = a.get_ylim()[1] - a.get_ylim()[0]\n            \n            if ydist is not None and xdist is not None:\n                if m['show_max']:\n                    x = np.argmax(v)\n                    y = np.max(v)\n                    a.scatter(x, y, s=200, color=m['color'], alpha=0.5)\n                    a.text(x-0.03*xdist, y-0.13*ydist, f'{round(y, 4)}', size=14)\n                if m['show_min']:\n                    x = np.argmin(v)\n                    y = np.min(v)\n                    a.scatter(x, y, s=200, color=m['color'], alpha=0.5)\n                    a.text(x-0.03*xdist, y+0.05*ydist, f'{round(y, 4)}', size=14)\n\n            if m['vmin'] is not None:\n                a.set_ylim(m['vmin'], m['vmax'])\n\n            a.legend()\n\n        plt.show()\n\n        if self.save_file:\n            fig.savefig(self.save_file)\n\ndef create_plot(label):\n    plot = TrainingPlot(nrows=1, ncols=2, figsize=(20, 5), title=label, \n                      save_file=D_WORK + f'{label}.png', \n                      old_logs_path=D_WORK + f'*.csv'\n            )\n\n    plot.add(0, 0, 'loss', 'green')\n    plot.add(0, 0, 'val_loss', 'red', show_min=True)\n    plot.add(0, 1, 'lr', 'black', vmin=0)\n    \n    return plot","81bdb86d":"def cvs_callback(filename):\n    return tf.keras.callbacks.CSVLogger(filename)","841be25d":"config = {}\n\nif ae_epochs > 0:\n  base = get_base(config)\n  ae_model = get_ae_model(base, config)\n      \n  ## TODO : simultaneous train\n  for i in range(ae_epochs\/\/ae_epochs_each):\n      print(f\"------ {i} ------\")\n      print(\"--- train ---\")\n      ae_model.fit([X_node, As], [X_node[:,0]],\n                epochs = ae_epochs_each,\n                batch_size = ae_batch_size * 8,\n                callbacks=[create_plot(f'train-{i}'), cvs_callback(D_WORK + f'pretrain-train-{i}.csv')])\n      print(\"--- public ---\")\n      ae_model.fit([X_node_pub, As_pub], [X_node_pub[:,0]],\n                epochs = ae_epochs_each,\n                batch_size = ae_batch_size * 8,\n                callbacks=[create_plot(f'public-{i}'), cvs_callback(D_WORK + f'pretrain-public-{i}.csv')])\n      print(\"--- private ---\")\n      ae_model.fit([X_node_pri, As_pri], [X_node_pri[:,0]],\n                epochs = ae_epochs_each,\n                batch_size = ae_batch_size * 8,\n                callbacks=[create_plot(f'private-{i}'), cvs_callback(D_WORK + f'pretrain-private-{i}.csv')])\n      gc.collect()\n  print(\"****** save ae model ******\")\n  base.save_weights(D_WORK + \".\/base_ae.h5\")\n","85eea3cf":"X_node.shape","dec68201":"n = X_node.shape[2]\nfig, ax = plt.subplots(n, 3, figsize=(30, n * 3))\n\n# check if input features in train and test look the same\n\nfor i in range(n):\n    sns.distplot(X_node[:, :, i],color=\"Blue\", ax=ax[i, 0])\n    sns.distplot(X_node_pub[:, :, i],color=\"Green\", ax=ax[i, 1])\n    sns.distplot(X_node_pri[:, :, i],color=\"Red\", ax=ax[i, 2])","b1b3da1c":"from sklearn.model_selection import KFold\nkfold = KFold(5, shuffle = True, random_state = 42)\n\nscores = []\npreds = np.zeros([len(X_node), X_node.shape[1], 5])\n\n\nfor i, (tr_idx, va_idx) in enumerate(kfold.split(X_node, As)):\n    print(f\"------ fold {i} start -----\")\n    print(f\"------ fold {i} start -----\")\n    print(f\"------ fold {i} start -----\")\n    X_node_tr = X_node[tr_idx]\n    X_node_va = X_node[va_idx]\n    As_tr = As[tr_idx]\n    As_va = As[va_idx]\n    y_tr = y[tr_idx]\n    y_va = y[va_idx]\n\n    base = get_base(config)\n    if ae_epochs > 0:\n        print(\"****** load ae model ******\")\n        base.load_weights(D_WORK + \".\/base_ae.h5\")\n    model = get_model(base, config)\n    if pretrain_dir is not None:\n        d = D_WORK + f\".\/model{i}.h5\"\n        print(f\"--- load from {d} ---\")\n        model.load_weights(d)\n    for epochs, batch_size in zip(epochs_list, batch_size_list):\n        print(f\"epochs : {epochs}, batch_size : {batch_size}\")\n        model.fit([X_node_tr, As_tr], [y_tr],\n                  validation_data=([X_node_va, As_va], [y_va]),\n                  epochs = epochs,\n                  batch_size = batch_size, \n                  callbacks=[\n                    create_plot(f'train-F{i}-E{epochs}-B{batch_size}'), \n                    cvs_callback(D_WORK + f'train-F{i}-E{epochs}-B{batch_size}.csv'),\n                    tf.keras.callbacks.ReduceLROnPlateau(factor=0.75, patience=3, min_lr=0.000001)\n                  ],\n                  # validation_freq = 3\n                  )\n\n    model.save_weights(D_WORK + f\".\/model{i}.h5\")\n    p = model.predict([X_node_va, As_va])\n    scores.append(mcrmse(y_va, p))\n    print(f\"fold {i}: mcrmse {scores[-1]}\")\n    preds[va_idx] = p\n    if one_fold:\n        break\n\npd.to_pickle(preds, D_WORK + \"oof.pkl\")","f8fbe336":"print(scores)","96a3c510":"p_pub = 0\np_pri = 0\nfor i in range(5):\n    model.load_weights(D_WORK + f\".\/model{i}.h5\")\n    p_pub += model.predict([X_node_pub, As_pub]) \/ 5\n    p_pri += model.predict([X_node_pri, As_pri]) \/ 5\n    if one_fold:\n        p_pub *= 5\n        p_pri *= 5\n        break\n\nfor i, target in enumerate(targets):\n    test_pub[target] = [list(p_pub[k, :, i]) for k in range(p_pub.shape[0])]\n    test_pri[target] = [list(p_pri[k, :, i]) for k in range(p_pri.shape[0])]","43657eca":"preds_ls = []\nfor df, preds in [(test_pub, p_pub), (test_pri, p_pri)]:\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n\n        single_df = pd.DataFrame(single_pred, columns=targets)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n        preds_ls.append(single_df)\n\npreds_df = pd.concat(preds_ls)\npreds_df.to_csv(D_WORK + \"submission.csv\", index = False)\npreds_df.head()","eb8e3911":"  print(scores)\nprint(np.mean(scores))","394bc57f":"## train","c60b5d01":"## structure adj","bc049382":"## sub","67d47a57":"## predict","81e38d44":"v1 \n* added batch 4, increased epochs\n```\nae_epochs = 20\nae_epochs_each = 5\nae_batch_size = 32\nepochs_list = [50, 10, 5, 3, 3, 5, 5]\nbatch_size_list = [4, 8, 16, 32, 64, 128, 256]\n```\n\nv2 \n* switched to AdamW, increased epochs for AE\n```\nae_epochs = 50\nae_epochs_each = 15\nae_batch_size = 16\nepochs_list = [60, 20, 5, 3, 3, 5, 5]\nbatch_size_list = [4, 8, 16, 32, 64, 128, 256]\n```\n\nFixed bug in the loss\n\nv3\n\n","437a801a":"## model","da4d842b":"Forked and changed version of https:\/\/www.kaggle.com\/mrkmakr\/covid-ae-pretrain-gnn-attn-cnn","b17cca32":"## pretrain","132b0ac6":"## node","cd686459":"## target","9cce0821":"## distance adj","02aa10da":"## load"}}