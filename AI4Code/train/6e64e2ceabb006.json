{"cell_type":{"ccb90936":"code","cbae9c09":"code","97a4b482":"code","fa0ee8c9":"code","7152feed":"code","42eb6079":"code","e35842d2":"code","b74b2f15":"code","a5a7e57a":"code","36dc6bee":"code","c9d2ca2e":"code","5ecf86e9":"code","1ab84eb3":"code","33b054ab":"code","f9ca6bfa":"code","c63dd4f0":"code","4c9bcc3a":"code","ae8dc5c8":"code","40ccf066":"code","234f1422":"code","cef175ce":"code","a7434ce8":"code","9854f672":"code","80d73527":"code","2f792d74":"code","90faca37":"code","4a127693":"code","0d7dd45a":"code","d4c47dbd":"code","a4ae704f":"code","ec070f11":"code","a606a7de":"code","bb43ff59":"code","9522aeb1":"markdown","f63e30e8":"markdown","d5cb0195":"markdown","7b160ffe":"markdown","922d3c1a":"markdown","fc76a37e":"markdown","6e875b0b":"markdown","c301cd78":"markdown","627cc30a":"markdown","7b2fdf29":"markdown","fcb1fa6b":"markdown","2657b252":"markdown","d831a3ed":"markdown","ce7559a1":"markdown","9a93ecb2":"markdown","039f71c9":"markdown","e6c996be":"markdown","31223100":"markdown","3fa8ea2c":"markdown","477cd28c":"markdown","08b7bd9b":"markdown","05ab8f84":"markdown","df2af065":"markdown","be34a9d4":"markdown","4e82707d":"markdown","611a1ef0":"markdown","d78e7d28":"markdown","6cf99e78":"markdown","46861615":"markdown","fbb5e438":"markdown"},"source":{"ccb90936":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport seaborn as sns\nimport gc\n\nimport os\nimport sys\nimport pandas as pd\nimport numpy as np\nimport tqdm\nimport seaborn as sns\n\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\nimport plotly.figure_factory as ff\n\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline  \n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import roc_auc_score\n\nimport catboost as cb\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cbae9c09":"!pip install xlrd\ndf = pd.read_excel('..\/input\/bankcustomers\/Bank_customers.xls', sheet_name='BankCustomers')\ndf.head()","97a4b482":"df.info()","fa0ee8c9":"null_col = df.columns[df.isnull().any()]\ndf[null_col].isnull().mean()","7152feed":"fields = [i for i in df.columns if i not in ['CLIENTNUM','Flag']]\ntarget = [i for i in df.columns if i in ['CLIENTNUM','Flag']]","42eb6079":"print(df.shape)\ndf1 = df.drop_duplicates()\nprint(df1.shape)","e35842d2":"toss = df.Flag.value_counts()\nlabels = (np.array(toss.index))\nsizes = (np.array((toss \/ toss.sum())*100))\ncolors = ['red', 'lightskyblue']\n\na1 = plt.figure(figsize=(10, 5))\na1 = plt.pie(sizes, labels=labels, colors=colors,\n        autopct='%1.1f%%', shadow=True, startangle=90)\na1 = plt.title(\"Flag percentage\")\n\nplt.show()","b74b2f15":"numeric_data = df[fields]._get_numeric_data()\nnum_col = numeric_data.columns\n\nplt.style.use('ggplot')\nfor i in num_col:\n    plt.figure(figsize=(10, 4))\n    plt.xticks(rotation=45)\n    sns.histplot(df[i], color=\"blue\")\n    plt.title(\"Distribution of {}\".format(i))\nplt.show()","a5a7e57a":"plt.style.use('ggplot')\n\nfig1 = plt.figure(figsize=(18, 8))\n#ax1 = plt.hist(df['Review Length'], color = \"red\", bins = 20)\nax1 = plt.subplot2grid((2, 2), (0, 0))\nax1 = plt.xticks(rotation=45)\nax1 = sns.distplot(df['Credit_Limit'], color=\"blue\")\nax1 = plt.title(\"Distribution of Credit Limit\")\n\n\n#fig3 = plt.figure(figsize=(10, 5))\n#ax1 = plt.hist(df['Review Length'], color = \"red\", bins = 20)\nax3 = plt.subplot2grid((2, 2), (0, 1))\nax3 = plt.xticks(rotation=45)\nax3 = sns.distplot(df['Avg_Open_To_Buy'], color=\"blue\")\nax3 = plt.title(\"Distribution of Total Revolving Bal\")\n\n#fig2 = plt.figure(figsize=(10, 5))\n#ax1 = plt.hist(df['Review Length'], color = \"red\", bins = 20)\nax2 = plt.subplot2grid((2, 2), (1, 0))\nax2 = plt.xticks(rotation=45)\nax2 = sns.distplot(df['Total_Trans_Amt'], color=\"blue\")\nax2 = plt.title(\"Distribution of Total transaction amount\")\n\nax4 = plt.subplot2grid((2, 2), (1, 1))\nax4 = plt.xticks(rotation=45)\nax4 = sns.distplot(df['Total_Trans_Ct'], color=\"blue\")\nax4 = plt.title(\"Distribution of Total_Trans_Ct\")\n","36dc6bee":"corrMatrix = df[num_col].corr()\nplt.figure(figsize=(18, 12))\nsns.heatmap(corrMatrix, annot=True)\nplt.show()","c9d2ca2e":"numeric_data = df[fields]._get_numeric_data()\nnum_col = numeric_data.columns\n\n\nplt.style.use('ggplot')\nfor i in num_col:\n    plt.figure(figsize=(10, 4))\n    plt.xticks(rotation=45)\n    sns.boxplot(x = 'Flag', y = i, data = df)\n    plt.title(\"Boxplot of Risk to {}\".format(i))\nplt.show()","5ecf86e9":"plt.style.use('ggplot')\n\nfig1 = plt.figure(figsize=(24, 20))\nax1 = plt.subplot2grid((3, 2), (0, 0))\nax1 = plt.xticks(rotation=45)\nax1 = sns.boxplot(x = 'Education_Level', y = 'Total_Trans_Ct', data = df)\nax1 = plt.title(\"Boxplot of Saving accounts to Age\")\n\n\nax2 = plt.subplot2grid((3, 2), (0, 1))\nax2 = plt.xticks(rotation=45)\nax2 = sns.boxplot(x = 'Education_Level', y = 'Total_Amt_Chng_Q4_Q1', data = df)\nax2 = plt.title(\"Boxplot of Saving accounts to Credit amount\")\n\n\nax3 = plt.subplot2grid((3, 2), (1, 0))\nax3 = plt.xticks(rotation=45)\nax3 = sns.boxplot(x = 'Marital_Status', y = 'Total_Trans_Ct', data = df)\nax3 = plt.title(\"Boxplot of Checking account to Duration\")\n\nax4 = plt.subplot2grid((3, 2), (1, 1))\nax4 = plt.xticks(rotation=45)\nax4 = sns.boxplot(x = 'Marital_Status', y = 'Total_Amt_Chng_Q4_Q1', data = df)\nax4 = plt.title(\"Boxplot of Checking account to Duration\")\n\nax5 = plt.subplot2grid((3, 2), (2, 0))\nax5 = plt.xticks(rotation=45)\nax5 = sns.boxplot(x = 'Card_Category', y = 'Total_Trans_Ct', data = df)\nax5 = plt.title(\"Boxplot of Checking account to Duration\")\n\nax6 = plt.subplot2grid((3, 2), (2, 1))\nax6 = plt.xticks(rotation=45)\nax6 = sns.boxplot(x = 'Card_Category', y = 'Total_Amt_Chng_Q4_Q1', data = df)\nax6 = plt.title(\"Boxplot of Checking account to Duration\")","1ab84eb3":"width = 0.5\nbestump_df = pd.melt(df, id_vars=['Customer_Age'], value_vars=['Education_Level'])\n\nbest_ump = bestump_df.value.value_counts()\nlabels = np.array(best_ump.index)\nind = np.arange(len(labels))\nfig = plt.figure(figsize=(18, 12))\nax = plt.subplot2grid((2, 2), (0, 0))\nrects = ax.bar(ind, np.array(best_ump), width=width, color='r')\nax.set_xticks(ind)\nax.set_xticklabels(labels, rotation=45)\nax.set_ylabel(\"Count\")\nax.set_title(\"Education_Level\")\n\nbestump_df = pd.melt(df, id_vars=['Customer_Age'], value_vars=['Marital_Status'])\n\nbest_ump = bestump_df.value.value_counts()\nlabels = np.array(best_ump.index)\nind = np.arange(len(labels))\nax = plt.subplot2grid((2, 2), (0, 1))\nrects = ax.bar(ind, np.array(best_ump), width=width, color='g')\nax.set_xticks(ind)\nax.set_xticklabels(labels, rotation=45)\nax.set_ylabel(\"Count\")\nax.set_title(\"Marital_Status\")\n\nbestump_df = pd.melt(df, id_vars=['Customer_Age'], value_vars=['Income_Category'])\n\nbest_ump = bestump_df.value.value_counts()\nlabels = np.array(best_ump.index)\nind = np.arange(len(labels))\nax = plt.subplot2grid((2, 2), (1, 1))\nrects = ax.bar(ind, np.array(best_ump), width=width, color='orange')\nax.set_xticks(ind)\nax.set_xticklabels(labels, rotation=45)\nax.set_ylabel(\"Count\")\nax.set_title(\"Income_Category\")\n\nbestump_df = pd.melt(df, id_vars=['Customer_Age'], value_vars=['Card_Category'])\n\nbest_ump = bestump_df.value.value_counts()\nlabels = np.array(best_ump.index)\nind = np.arange(len(labels))\nax = plt.subplot2grid((2, 2), (1, 0))\nrects = ax.bar(ind, np.array(best_ump), width=width, color='b')\nax.set_xticks(ind)\nax.set_xticklabels(labels, rotation=45)\nax.set_ylabel(\"Count\")\nax.set_title(\"Card_Category\")","33b054ab":"d = df.groupby(by=['Months_on_book','Dependent_count']).count()\ndd = pd.DataFrame([d.index[i][0] for i in range(len(d))])\ndd.columns =['Months_on_book']\ndd['Dependent_count'] = pd.DataFrame([d.index[i][1] for i in range(len(d))])\ndd['COUNT'] = pd.DataFrame([d['Gender'].iloc[i] for i in range(len(d))])\n\ndf2 = pd.merge(df, dd, on=['Months_on_book','Dependent_count'], how='left')\n\nplt.style.use('ggplot')\ndf2.plot.scatter(x='Dependent_count', y='Months_on_book',figsize=(25,18),s = df2.COUNT);\nplt.show()\n","f9ca6bfa":"print(df.Gender.value_counts())\ndf['Gender'] = df['Gender'].apply(lambda x: 1 if x == \"M\" else 0)","c63dd4f0":"print(df.Education_Level.value_counts())\ndef parse_ed_level(x):\n    if x == 'Uneducated':\n       return 1\n    elif x == 'Unknown':\n       return 2\n    elif x == 'High School':\n       return 3\n    elif x == 'College':\n       return 4\n    elif x == 'Graduate':\n       return 5\n    elif x == 'Post-Graduate\/others':\n       return 6\n    elif x == 'Doctorate':\n       return 7\n    else:\n       return -1\n\ndf['Education_Level'] = df['Education_Level'].apply(parse_ed_level)","4c9bcc3a":"print(df.Marital_Status.value_counts())\ndef parse_marital_status(x):\n    if x == 'Divorced':\n       return 1\n    elif x == 'Unknown':\n       return 2\n    elif x == 'Single':\n       return 3\n    elif x == 'Married':\n       return 4\n    else:\n       return -1\n\ndf['Marital_Status'] = df['Marital_Status'].apply(parse_marital_status)","ae8dc5c8":"print(df.Income_Category.value_counts())\ndef parse_income_category(x):\n    if x == 'Less than $40K':\n       return 1\n    elif x == '$40K - $60K':\n       return 2\n    elif x == 'Unknown':\n       return 3\n    elif x == '$60K - $80K':\n       return 4\n    elif x == '$80K - $120K':\n       return 5\n    elif x == '$120K +':\n       return 6\n    else:\n       return -1\n\ndf['Income_Category'] = df['Income_Category'].apply(parse_income_category)","40ccf066":"print(df.Card_Category.value_counts())\ndef parse_card_category(x):\n    if x == 'Blue':\n       return 1\n    elif x == 'Silver':\n       return 2\n    elif x == 'Gold':\n       return 3\n    elif x == 'Platinum':\n       return 4\n    else:\n       return -1\n\ndf['Card_Category'] = df['Card_Category'].apply(parse_card_category)\ndf.head()","234f1422":"df.info()","cef175ce":"fields = [i for i in df.columns if i not in ['CLIENTNUM','Flag','Avg_Open_To_Buy']]\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df[fields], df[target], test_size=0.2, random_state=1)\n\nX_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2","a7434ce8":"knn = KNeighborsClassifier(n_neighbors = 4)\nknn.fit(X_train, y_train)","9854f672":"from sklearn.metrics import accuracy_score\ny_predicted = knn.predict(X_train)[:,1]\nprint(accuracy_score(y_train['Flag'], y_predicted))\nprint(roc_auc_score(y_train['Flag'], y_predicted))","80d73527":"from sklearn.metrics import accuracy_score\ny_predicted = knn.predict(X_test)[:,1]\nprint(accuracy_score(y_test['Flag'], y_predicted))\nprint(roc_auc_score(y_test['Flag'], y_predicted))","2f792d74":"logreg = LogisticRegression(max_iter=1000)\nlogreg.fit(X_train, y_train['Flag'])","90faca37":"from sklearn.metrics import accuracy_score\ny_predicted = logreg.predict_proba(X_train)[:,1]\nprint(roc_auc_score(y_train['Flag'], y_predicted))\ny_predicted = logreg.predict(X_train)\nprint(accuracy_score(y_train['Flag'], y_predicted))","4a127693":"from sklearn.metrics import accuracy_score\ny_predicted = logreg.predict_proba(X_test)[:,1]\nprint(roc_auc_score(y_test['Flag'], y_predicted))\ny_predicted = logreg.predict(X_test)\nprint(accuracy_score(y_test['Flag'], y_predicted))","0d7dd45a":"rf = RandomForestClassifier(max_depth=5, random_state=0)\nrf.fit(X_train, y_train['Flag'])","d4c47dbd":"y_predicted = rf.predict_proba(X_train)[:,1]\nprint(roc_auc_score(y_train['Flag'], y_predicted))\ny_predicted = rf.predict(X_train)\nprint(accuracy_score(y_train['Flag'], y_predicted))","a4ae704f":"y_predicted = rf.predict_proba(X_test)[:,1]\nprint(roc_auc_score(y_test['Flag'], y_predicted))\ny_predicted = rf.predict(X_test)\nprint(accuracy_score(y_test['Flag'], y_predicted))","ec070f11":"train1 = pd.concat([X_train, y_train], axis=1, join=\"inner\")\ntrain1.index = [i for i in range(len(train1))]\ntargets = train1.Flag.values\n\ncv = KFold(n_splits=5, random_state=100, shuffle=True)\n\noof = np.zeros(len(train1))\ntrain_preds = np.zeros(len(train1))\n\nmodels = []\n\ntree_params = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'learning_rate': 0.05,\n    'subsample' : 0.7,\n    'is_unbalance' : 'True',\n    'colsample_by_tree' : 1,\n    'max_bin' : 50,\n    'min_child_sample' : 10,\n    'min_child_weight' : 1,\n    'max_depth': 5,\n    'reg_lambda': 10,\n    'reg_alpha': 10,\n    'num_leaves': 8,\n    'n_jobs': 5,\n    'n_estimators': 250\n}\n\nfor fold_, (train_idx, val_idx) in enumerate(cv.split(train1, targets), 1):\n    print(f'Training with fold {fold_} started.')\n    lgb_model = lgb.LGBMClassifier(**tree_params)\n    train, val = train1.iloc[train_idx], train1.iloc[val_idx]\n    \n    lgb_model.fit(train[fields], train.Flag.values, eval_set=[(val[fields], val.Flag.values)],\n              early_stopping_rounds=50, verbose=50)\n\n    \n    oof[val_idx] = lgb_model.predict_proba(val[fields])[:, 1]\n    train_preds[train_idx] += lgb_model.predict_proba(train[fields])[:, 1] \/ (cv.n_splits-1)\n    models.append(lgb_model)\n    print(f'Training with fold {fold_} completed.')","a606a7de":"from sklearn.metrics import accuracy_score\ny_predicted = lgb_model.predict_proba(X_train[fields], num_iteration = lgb_model.best_iteration_)[:,1]\nprint(roc_auc_score(y_train['Flag'], y_predicted))\ny_predicted = lgb_model.predict(X_train[fields], num_iteration = lgb_model.best_iteration_)\nprint(accuracy_score(y_train['Flag'], y_predicted))","bb43ff59":"from sklearn.metrics import accuracy_score\ny_predicted = lgb_model.predict_proba(X_test[fields], num_iteration = lgb_model.best_iteration_)[:,1]\nprint(roc_auc_score(y_test['Flag'], y_predicted))\ny_predicted = lgb_model.predict(X_test[fields], num_iteration = lgb_model.best_iteration_)\nprint(accuracy_score(y_test['Flag'], y_predicted))","9522aeb1":"## The similarity of some features force us consider correlation matrix to check for the presence of linearly dependent of features (so that multicollinearity does not occur) \u00b6","f63e30e8":"### Distribution of categorical parameters by parameters (which will give the most information gain","d5cb0195":"# Building model","7b160ffe":"## Distribution of parameters via boxplot for each class","922d3c1a":"Adv: Simple and intuitive model that works quickly.\n\nDisadv: Works great with simple dependencies. And with a complicated dependence, it can no longer cope with the classification. Also not suitable for multi label classification, since here, as an activation function with a choice of a threshold, it can return 1 or 0.","fc76a37e":"### Barplot of categorical features by sorted count","6e875b0b":"### Scatterplot by the ratio of Months_on_book to Dependent_count with point sizes corresponding to the number of objects","c301cd78":"Here you can already note the following parameters, which are likely to give the most information gain:\n\n1) Total_Amt_Chng_Q4_Q1  \n2) Total_Trans_Amt  \n3) Total_Trans_Ct  \n4) Total_Ct_Chng_Q4_Q1  ","627cc30a":"# EDA","7b2fdf29":"### Dividing features and target columns","fcb1fa6b":"## Based on the results of the models, it is obvious that the choice falls on LightGBM. (On Roc-auc test: 99%, Accuracy: 95.5%) \n## In addition, the model is not overfitted, since the same results come out on train and test.","2657b252":"# Checking for duplicates","d831a3ed":"# Checking for missing values","ce7559a1":"# Reading dataframe","9a93ecb2":"As a result, the parameters Credit Limit and Avg_open_to_buy turned out to be completely correlated (as it was suspected). One of these parameters must be excluded when building the model.","039f71c9":"\nAdvantages: The algorithm is resistant to abnormal outliers. The result of the algorithm is easy to interpret.\n\nDisadvantages: The data used for the algorithm must be representative. Second, the model cannot be separated from the data; all examples must be used to classify a new example. This severely limits the use of the algorithm","e6c996be":"## KNN Classifier","31223100":"## Train, test, validation split","3fa8ea2c":"#### First of all, distribution of credit limit and total removing bal look similar. And same situation with distribution of total transaction amount and total trans cnt\n\n### Distribution of credit limit and total removing bal are lognormal. Consequently if we want lead distribution to normal we need just to logarithm them. But, I am going to use tree model then it's not necessary.","477cd28c":"## Ratio of two classes of target\nThere is a slight imbalance, but generally an acceptable ratio","08b7bd9b":"# RandomForestClassifier","05ab8f84":"Adv: Works much better than other models. This strong classifier is creating, we can say on the basis of weak ones, since each subsequent tree is built on the basis of the previous tree, that is, it takes into account the errors of the previous one and tries to correct them.\n\nDisadv: This algorithm requires a large amount of data, in this case there are only 10,000 objects in the dataset (not so much). But even under such circumstances, he showed the best results. (and I also tweaked the hyperparameters in this model)","df2af065":"# LightGBM\nLightGBM Classifier","be34a9d4":"### Due to a slight imbalance as a metric, the use of accuracy may be questionable, but it can still be used.   Also, let's consider roc auc metric","4e82707d":"# Logistic Regression","611a1ef0":"## Distribution of numeric data by loop","d78e7d28":"Even we have there some high correlated features, I decided to try build model with them first. ","6cf99e78":"### I decided to once again consider the distribution of suspicious parameters, which seemed to me to be similar to each other\u00b6","46861615":"## Let's start converting categorical features to numeric","fbb5e438":"Could be converted in a number of ways. I decided to do it in a simple way, similar to the logic of getdummies or labelencoder, but through the function described below for each parameter."}}