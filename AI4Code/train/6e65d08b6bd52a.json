{"cell_type":{"6b0c5dc1":"code","61bbc7dc":"code","d8c78c21":"code","5263c246":"code","1ebcaf94":"code","f672b6ce":"code","5fc60dde":"code","8cad344f":"code","275a1cc4":"code","5c080eb2":"code","a8b7dd3d":"code","afa06165":"code","8f261b55":"code","5a0092d3":"code","ab9c86df":"code","ea81194d":"code","5467d648":"code","d9b0088b":"code","f875aa7f":"code","2276d93c":"code","9e6338f8":"code","e1bf0cd2":"code","36ac0187":"code","7c41d274":"code","6a63ea56":"code","24125687":"code","ebbccda2":"code","0539b721":"code","5933d328":"code","5ffce589":"code","56ed4210":"code","9ae00ad6":"code","d6c7d7cc":"code","232af606":"code","2894fde6":"code","0ccf5a64":"code","989cf4fe":"code","59ae83a5":"code","495f89aa":"code","fa0d29ea":"code","2dbcea77":"code","f0174f4d":"code","cb0763f5":"code","40abfd16":"code","0e365137":"code","15ed8817":"code","39c469d5":"code","6333c042":"code","28597189":"code","c021487d":"code","5918a7e1":"code","d9241d7a":"code","76a40f7e":"markdown","9b1d874d":"markdown","aaf6ef2a":"markdown","ca6a5ffb":"markdown","28db56cb":"markdown","6cc8297f":"markdown","1ff2f0f5":"markdown","4f9a3541":"markdown","511b8ba1":"markdown","57205f3f":"markdown","3813a888":"markdown","dd6f27e6":"markdown","a55a44f1":"markdown","951142c8":"markdown","f0c8536c":"markdown","23fe6985":"markdown","94b87add":"markdown","288b186a":"markdown","d775d130":"markdown","88bca752":"markdown","ef852aef":"markdown","dc6c20ae":"markdown","8210b349":"markdown","15a44ad4":"markdown","6a90fc17":"markdown","9c3b8be3":"markdown","7b4843e1":"markdown","11ef00f9":"markdown","103a067d":"markdown","1e553bf5":"markdown","1c0c2544":"markdown","e27e912f":"markdown","fc3aa60a":"markdown","025b53a2":"markdown","30804118":"markdown","79103933":"markdown","42627195":"markdown","32c6607c":"markdown","acf970e8":"markdown","f6a5b53b":"markdown","4c4d0002":"markdown","cc74783c":"markdown","d54929a7":"markdown","69249285":"markdown","20d25531":"markdown","c2634a6d":"markdown","5ef0f22b":"markdown","44f68007":"markdown","48d94c58":"markdown","df91f5ae":"markdown","853b325f":"markdown","81452f0c":"markdown","65f7b6aa":"markdown","629aee4e":"markdown","91848b2d":"markdown","a720a323":"markdown","cb1a6d28":"markdown","a1b3ba64":"markdown","4718add3":"markdown","71229f1d":"markdown","598eb0a0":"markdown","57412352":"markdown","35bca7e7":"markdown","1dbcc7ce":"markdown","c07be10a":"markdown","4e4f04b8":"markdown"},"source":{"6b0c5dc1":"import time\nimport pandas as pd\nimport warnings\nfrom datetime import datetime\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import pearsonr\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.legend_handler import HandlerLine2D\n%matplotlib inline\n\nfrom sklearn.model_selection import KFold, cross_val_score, StratifiedKFold, GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, ExtraTreesClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.feature_selection import SelectPercentile, RFE, RFECV\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics","61bbc7dc":"df = pd.read_csv('\/kaggle\/input\/kobe-bryant-shot-selection\/data.csv', header=0, index_col=\"shot_id\", parse_dates=['game_date'])\nprint(\"Size of data loaded:\", len(df))\n\ndf.head()","d8c78c21":"df.dtypes","5263c246":"df[\"period\"] = df[\"period\"].astype('category')\ndf[\"season\"] = df[\"season\"].astype('category')\ndf[\"team_id\"] = df[\"team_id\"].astype('category')\ndf[\"game_id\"] = df[\"game_id\"].astype('category')\ndf[\"opponent\"] = df[\"opponent\"].astype('category')\ndf[\"playoffs\"] = df[\"playoffs\"].astype('category')\ndf[\"shot_type\"] = df[\"shot_type\"].astype('category')\ndf[\"action_type\"] = df[\"action_type\"].astype('category')\ndf[\"game_event_id\"] = df[\"game_event_id\"].astype('category')\ndf[\"shot_zone_area\"] = df[\"shot_zone_area\"].astype('category')\ndf[\"shot_zone_basic\"] = df[\"shot_zone_basic\"].astype('category')\ndf[\"shot_zone_range\"] = df[\"shot_zone_range\"].astype('category')\ndf[\"combined_shot_type\"] = df[\"combined_shot_type\"].astype('category')","1ebcaf94":"df.isnull().sum()","f672b6ce":"plt.figure(figsize=(20,5))\nsns.countplot('shot_zone_range',hue='shot_made_flag',data=df[df.shot_made_flag.notnull()])\nplt.title('misses and baskets from each zone_range')\nplt.show()","5fc60dde":"plt.figure(figsize=(20,5))\nsns.countplot('shot_zone_basic',hue='shot_made_flag',data=df[df.shot_made_flag.notnull()])\nplt.title('misses and baskets from each zone_basic')\nplt.show()","8cad344f":"cols = [\"combined_shot_type\", \"period\", \"playoffs\", \"season\", \"shot_type\", \"shot_zone_area\", \"shot_zone_basic\", \"shot_zone_range\", \"team_id\", \"team_name\", \"opponent\"]\n\nfor c in cols:\n    plt.figure(figsize=(20, 8), dpi=80, facecolor='w', edgecolor='k')\n    ax = plt.axes()\n    sns.countplot(x=c, data=df, ax=ax);\n    ax.set_title(c)\n    plt.xticks(rotation=90)\n    plt.show()","275a1cc4":"sns.set(style=\"white\")\n\n# Compute the correlation matrix\ncorr = df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(10, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\nplt.show()","5c080eb2":"print(\"Pearson correlation between lat and loc_y variables: %.2f\" % pearsonr(df.lat, df.loc_y)[0])\ndf.drop('lat', axis=1, inplace=True)\n\nprint(\"Pearson correlation between lon and loc_x variables: %.2f\" % pearsonr(df.lon, df.loc_x)[0])\ndf.drop('lon', axis=1, inplace=True)","a8b7dd3d":"df.describe(include=['object', 'category'])","afa06165":"print(\"Different values for team_name variable\", set(df.team_name))\ndf.drop('team_name', axis=1, inplace=True)\n\nprint(\"Different values for team_id variable\", set(df.team_id))\ndf.drop('team_id', axis=1, inplace=True)\n\ndf.drop('game_event_id', axis=1, inplace=True)\ndf.drop('game_id', axis=1, inplace=True)","8f261b55":"df[[\"action_type\", \"combined_shot_type\"]].head(25)","5a0092d3":"print(\"Uniques values for action_type:\",format(str(len(set(df.action_type)))))\nprint(\"Uniques values for combined_shot_type:\",format(str(len(set(df.combined_shot_type)))))","ab9c86df":"#df.drop('action_type', axis=1, inplace=True)\ndf.head(1)","ea81194d":"matchups = list(set(df.matchup.str[-3:]))\nprint(\"Number of teams by matches column:\", len(matchups))\nopponent = list(set(df.opponent))\nprint(\"Number of teams by opponent column:\", len(opponent))\n\nmain_list = list(set(matchups).difference(opponent))\nprint(\"\\nThere are\", len(main_list), \"teams incongruous:\")\nprint(main_list)","5467d648":"df[df[\"matchup\"].str.endswith(main_list[0])].head(2) # PHO == PHX --> Phoenix Suns\ndf[df[\"matchup\"].str.endswith(main_list[1])].head(2) # SAN == SAS --> San Antonio Spurs\ndf[df[\"matchup\"].str.endswith(main_list[2])].head(2) # CHH == CHA --> Charlotte Horets\ndf[df[\"matchup\"].str.endswith(main_list[3])].head(2) # UTH == UTA --> Utah Jazz\ndf[df[\"matchup\"].str.endswith(main_list[4])].head(2) # NOK == NOP --> New Orleans Pelicans https:\/\/stats.nba.com\/game\/0020500903\/scoring\/","d9b0088b":"df[\"home\"] = pd.np.where(df.matchup.str.contains(\"@\"), 0, 1)\ndf[\"home\"] = df[\"home\"].astype('category')\ndf.drop('matchup', axis=1, inplace=True)","f875aa7f":"df['remain_time'] = 60*df['seconds_remaining'] + df['minutes_remaining']\ndf.drop('minutes_remaining', axis=1, inplace=True)\ndf.drop('seconds_remaining', axis=1, inplace=True)","2276d93c":"df.head()","9e6338f8":"df['year'] = df.game_date.dt.year\ndf['month'] = df.game_date.dt.month\ndf['day'] = df.game_date.dt.day","e1bf0cd2":"df.drop('game_date', axis=1, inplace=True)","36ac0187":"df.head(5)","7c41d274":"categorial_cols = df.select_dtypes(include='category').columns\n\nfor cc in categorial_cols:\n    dummies = pd.get_dummies(df[cc])\n    dummies = dummies.add_prefix(\"{}#\".format(cc))\n    df.drop(cc, axis=1, inplace=True)\n    df = df.join(dummies)\n","6a63ea56":"df.head(1)","24125687":"# Splitting data into train-test\ndata = df[~df.shot_made_flag.isna()]\nsubmit = df[df.shot_made_flag.isna()]\n\nprint(\"Split dataframe into data-submit: Data:\", len(data), \"; Submit:\", len(submit))\nprint(\"\\nPercentage for every class:\\n\", data.shot_made_flag.value_counts()\/len(data))","ebbccda2":"plt.figure(figsize=(8, 5), dpi=80, facecolor='w', edgecolor='k')\nax = plt.axes()\nsns.countplot(x='shot_made_flag', data=data, ax=ax);\nax.set_title('Target class distribution')\nplt.show()","0539b721":"X_train, X_test, y_train, y_test = train_test_split(\n    data.loc[:, data.columns != 'shot_made_flag'], data.shot_made_flag, \n    test_size=0.2, random_state=0, stratify=data.shot_made_flag)\n\nprint(\"Split data into train-test:\\nTrain:\", len(X_train), \"\\nTest:\", len(X_test),\"\\n\\n\")\n\nprint(\"Percentage for train set:\\n\",y_train.value_counts()\/len(y_train),\"\\n\")\nprint(\"Percentage for train set:\\n\",y_test.value_counts()\/len(y_test))","5933d328":"#sc = StandardScaler()\nsc = MinMaxScaler()\n\nX_train_sc = pd.DataFrame(sc.fit_transform(X_train.values), \n                          index=X_train.index, \n                          columns=X_train.columns)\nX_test_sc = pd.DataFrame(sc.transform(X_test.values), \n                          index=X_test.index, \n                          columns=X_test.columns)\n\n","5ffce589":"X_train_sc","56ed4210":"# Create the RFE object and compute a cross-validated score.\nranker = GradientBoostingClassifier()\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nrfecv = RFECV(estimator=ranker, step=1, cv=StratifiedKFold(2), n_jobs = 1, scoring='neg_log_loss')\nrfecv.fit(X_train_sc, y_train)\n\nprint(\"Optimal number of features based on Gradient Boosting : %d\" % rfecv.n_features_)\n","9ae00ad6":"# Plot number of features VS. cross-validation scores\nplt.figure(figsize=(13, 6.5))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation -Log_Loss\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","d6c7d7cc":"feature_names = X_train_sc.columns\nselected_features_gb = feature_names[rfecv.support_].tolist()\nselected_features_gb\n\nprint(\"\\nNumber of main features by Linear Discriminant Analysis: {}\\n\".format(len(selected_features_gb)))\n#selected_features_gb","232af606":"ranker = LinearDiscriminantAnalysis()\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nrfecv = RFECV(estimator=ranker, step=1, cv=StratifiedKFold(2), scoring='neg_log_loss')\nrfecv.fit(X_train_sc, y_train)\n\nprint(\"Optimal number of features based on LDA : %d\" % rfecv.n_features_)\n\n# Plot number of features VS. cross-validation scores\nplt.figure(figsize=(13, 6.5))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","2894fde6":"feature_names = X_train_sc.columns\nselected_features_lda = feature_names[rfecv.support_].tolist()\nselected_features_lda\n\nprint(\"\\nNumber of main features by Linear Discriminant Analysis: {}\\n\".format(len(selected_features_lda)))","0ccf5a64":"# Create the RFE object and compute a cross-validated score.\nranker = LogisticRegression()\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nrfecv = RFECV(estimator=ranker, step=1, cv=StratifiedKFold(2),\n              scoring='accuracy')\nrfecv.fit(X_train_sc, y_train)\n\nprint(\"Optimal number of features based on Logistic Regression : %d\" % rfecv.n_features_)\n\n# Plot number of features VS. cross-validation scores\nplt.figure(figsize=(13, 6.5))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","989cf4fe":"feature_names = X_train_sc.columns\nselected_features_lr = feature_names[rfecv.support_].tolist()\n\nprint(\"\\nNumber of main features by Logistic regression: {}\\n\".format(len(selected_features_lr)))","59ae83a5":"num_folds = 5\nkfold = KFold(n_splits=num_folds, shuffle = True)\n\nmodels = []\nmodels.append((\"LDA\", LinearDiscriminantAnalysis()))\nmodels.append(('Logistic regression', LogisticRegression()))\nmodels.append(('Random Forest', RandomForestClassifier()))\nmodels.append(('Ada Boost', AdaBoostClassifier()))\nmodels.append(('Gradient Boosting', GradientBoostingClassifier()))\nmodels.append(('XGBoost', XGBClassifier()))\nmodels.append((\"Bagging\", BaggingClassifier()))\nmodels.append((\"KNN\", KNeighborsClassifier()))\nmodels.append((\"MLP\", MLPClassifier()))\nmodels.append((\"Gauss\", GaussianNB()))\nmodels.append((\"Voting\", VotingClassifier(estimators=[\n                                                    ('lr', GradientBoostingClassifier()), \n                                                    ('rf', AdaBoostClassifier()), \n                                                    ('xgb', XGBClassifier())], voting='soft')))\n\n\n\nstart_time = time.time()\n# Evaluate each model in turn\nresults = []\nnames = []\nstds = []\nmeans =[]\nfor name, model in models:\n    cv_results = cross_val_score(model, X_train_sc[selected_features_gb], y_train, cv=kfold, scoring='neg_log_loss', n_jobs=2)\n    print(\"Cross validation results for {0}: {1}\".format(name, cv_results))\n    print(\"{0}: ({1:.4f}) +\/- ({2:.4f})\".format(name, cv_results.mean(), cv_results.std()),\"\\n\")\n    results.append(cv_results)\n    names.append(name)\n    stds.append(cv_results.std())\n    means.append(abs(cv_results.mean()))\n    \n    \nprint(\"--- %s seconds ---\" % (time.time() - start_time))","495f89aa":"pd.DataFrame({\"Name\":names, \"Log Loss\":means, \"Standar Deviation\": stds}).sort_values(by=\"Log Loss\")","fa0d29ea":"learning_rates = [0.0001, 0.001,0.005, 0.01, 0.05, 0.1, 0.15, 0.5, 1]\ntrain_results = []\ntest_results = []\nfor eta in learning_rates:\n    model = GradientBoostingClassifier(learning_rate=eta)\n    model.fit(X_train_sc[selected_features_gb], y_train)\n    train_pred = model.predict(X_train_sc[selected_features_gb])\n    false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_train, train_pred)\n    roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n    train_results.append(roc_auc)\n    y_pred = model.predict(X_test_sc[selected_features_gb])\n    false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_test, y_pred)\n    roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n    test_results.append(roc_auc)\n\nplt.figure(figsize=(15,8))\nline1, = plt.plot(learning_rates, train_results, 'b', label='Train AUC')\nline2, = plt.plot(learning_rates, test_results, 'r', label='Test AUC')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.xscale(\"log\")\nplt.ylabel('AUC score')\nplt.xlabel('learning rate')\nplt.show()","2dbcea77":"n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200, 500, 1000]\ntrain_results = []\ntest_results = []\nfor estimator in n_estimators:\n    model = GradientBoostingClassifier(n_estimators=estimator)\n    model.fit(X_train_sc[selected_features_gb], y_train)\n    train_pred = model.predict(X_train_sc[selected_features_gb])\n    false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_train, train_pred)\n    roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n    train_results.append(roc_auc)\n    y_pred = model.predict(X_test_sc[selected_features_gb])\n    false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_test, y_pred)\n    roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n    test_results.append(roc_auc)\n    \nplt.figure(figsize=(15,8))\nline1, = plt.plot(n_estimators, train_results, 'b', label='Train AUC')\nline2, = plt.plot(n_estimators, test_results, 'r', label='Test AUC')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('n_estimators')\nplt.show()","f0174f4d":"min_samples_splits = np.linspace(0.0001, 1.0, 10, endpoint=True)\ntrain_results = []\ntest_results = []\nfor min_samples in min_samples_splits:\n    model = GradientBoostingClassifier(min_samples_split = min_samples)\n    model.fit(X_train_sc[selected_features_gb], y_train)\n    train_pred = model.predict(X_train_sc[selected_features_gb])\n    false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_train, train_pred)\n    roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n    train_results.append(roc_auc)\n    y_pred = model.predict(X_test_sc[selected_features_gb])\n    false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_test, y_pred)\n    roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n    test_results.append(roc_auc)\n\nplt.figure(figsize=(15,8))\nline1, = plt.plot(min_samples_splits, train_results, 'b', label='Train AUC')\nline2, = plt.plot(min_samples_splits, test_results, 'r', label='Test AUC')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.xscale(\"log\")\nplt.ylabel('AUC score')\nplt.xlabel('Min. samples split')\nplt.show()","cb0763f5":"min_samples_leafs = [0.00001, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 2, 4, 8, 15]\ntrain_results = []\ntest_results = []\nfor min_samples in min_samples_leafs:\n    model = GradientBoostingClassifier(min_samples_leaf = min_samples)\n    model.fit(X_train_sc[selected_features_gb], y_train)\n    train_pred = model.predict(X_train_sc[selected_features_gb])\n    false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_train, train_pred)\n    roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n    train_results.append(roc_auc)\n    y_pred = model.predict(X_test_sc[selected_features_gb])\n    false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_test, y_pred)\n    roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n    test_results.append(roc_auc)\n\nplt.figure(figsize=(15,8))\nline1, = plt.plot(min_samples_leafs, train_results, 'b', label='Train AUC')\nline2, = plt.plot(min_samples_leafs, test_results, 'r', label='Test AUC')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.xscale(\"log\")\nplt.ylabel('AUC score')\nplt.xlabel('Min. samples leaf')\nplt.show()","40abfd16":"max_features = list(range(1,X_train_sc[selected_features_gb].shape[1]))\ntrain_results = []\ntest_results = []\nfor max_feature in max_features:\n    model = GradientBoostingClassifier(max_features = max_feature)\n    model.fit(X_train_sc[selected_features_gb], y_train)\n    train_pred = model.predict(X_train_sc[selected_features_gb])\n    false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_train, train_pred)\n    roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n    train_results.append(roc_auc)\n    y_pred = model.predict(X_test_sc[selected_features_gb])\n    false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_test, y_pred)\n    roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n    test_results.append(roc_auc)\n\nplt.figure(figsize=(15,8))\nline1, = plt.plot(max_features, train_results, 'b', label='Train AUC')\nline2, = plt.plot(max_features, test_results, 'r', label='Test AUC')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('Max. features')\nplt.show()","0e365137":"classifiers = {}\nclassifiers.update({\"Gradient Boosting\": GradientBoostingClassifier()})","15ed8817":"parameters = {}\nparameters.update({\"Gradient Boosting\": { \n                                        \"classifier__learning_rate\":[0.1,0.05,0.01,0.005], \n                                        \"classifier__n_estimators\": [500],\n                                        \"classifier__max_depth\": [2,3,4,5,6],\n                                        \"classifier__min_samples_split\": [0.005, 0.01, 0.05, 0.10],\n                                        \"classifier__min_samples_leaf\": [0.005, 0.01, 0.05, 0.10],\n                                        \"classifier__subsample\": [0.8, 0.9, 1]\n                                         }})\n","39c469d5":"results = {}\nstart_time = time.time()\n# Tune and evaluate classifiers\nfor classifier_label, classifier in classifiers.items():\n    \n    # Print message to user\n    print(f\"Now tuning {classifier_label}.\")\n\n    # Initialize Pipeline object\n    pipeline = Pipeline([(\"classifier\", classifier)])\n\n    # Define parameter grid\n    param_grid = parameters[classifier_label]\n    \n    # Initialize GridSearch object\n    rscv = RandomizedSearchCV(pipeline, param_grid, cv = 5,  n_jobs= -1, verbose = 1, scoring = 'neg_log_loss')\n\n    # Fit gscv\n    rscv.fit(X_train_sc[selected_features_gb], np.ravel(y_train))  \n\n    # Get best parameters and score\n    best_params = rscv.best_params_\n    best_score = rscv.best_score_\n\n    # Update classifier parameters and define new pipeline with tuned classifier\n    tuned_params = {item[12:]: best_params[item] for item in best_params}\n    classifier.set_params(**tuned_params)\n\n    # Make predictions\n    y_pred = rscv.predict_proba(X_test_sc[selected_features_gb])\n\n    # Evaluate model\n    log_loss = metrics.log_loss(y_test, y_pred)\n\n    # Save results\n    result = {\"Classifier\": rscv,\n              \"Best Parameters\": best_params,\n              \"Training Log Loss\": (-1) * best_score,\n              \"Test Log Loss\": log_loss\n             }\n\n    results.update({classifier_label: result})\n\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","6333c042":"log_scores = {\n              \"Classifier\": [],\n              \"Log Loss\": [],\n              \"Log Loss Type\": []\n              }\n\n# Get AUC scores into dictionary\nfor classifier_label in results:\n    log_scores.update({\"Classifier\": [classifier_label] + log_scores[\"Classifier\"],\n                       \"Log Loss\": [results[classifier_label][\"Training Log Loss\"]] + log_scores[\"Log Loss\"],\n                       \"Log Loss Type\": [\"Training\"] + log_scores[\"Log Loss Type\"]})\n    \n    log_scores.update({\"Classifier\": [classifier_label] + log_scores[\"Classifier\"],\n                       \"Log Loss\": [results[classifier_label][\"Test Log Loss\"]] + log_scores[\"Log Loss\"],\n                       \"Log Loss Type\": [\"Test\"] + log_scores[\"Log Loss Type\"]})\n    \n\n\n# Dictionary to PandasDataFrame\nlog_scores = pd.DataFrame(log_scores)\n\n# Set graph style\nsns.set(font_scale = 1.75)\nsns.set_style({\"axes.facecolor\": \"1.0\", \"axes.edgecolor\": \"0.85\", \"grid.color\": \"0.85\",\n               \"grid.linestyle\": \"-\", 'axes.labelcolor': '0.4', \"xtick.color\": \"0.4\",\n               'ytick.color': '0.4'})\n\n    \n# Colors\ntraining_color = sns.color_palette(\"RdYlBu\", 10)[1]\ntest_color = sns.color_palette(\"RdYlBu\", 10)[-2]\ncolors = [training_color, test_color]\n\n# Set figure size and create barplot\nf, ax = plt.subplots(figsize=(10, 5))\n\nsns.barplot(x=\"Log Loss\", y=\"Classifier\", hue=\"Log Loss Type\", palette = colors, data=log_scores)\n\n# Generate a bolded horizontal line at y = 0\nax.axvline(x = 0, color = 'black', linewidth = 4, alpha = .7)\n\n# Turn frame off\nax.set_frame_on(False)\n\n# Tight layout\nplt.tight_layout()\n","28597189":"best_params","c021487d":"log_scores","5918a7e1":"train = pd.DataFrame(sc.fit_transform(data.loc[:, data.columns != 'shot_made_flag'].values), \n                          index=data.loc[:, data.columns != 'shot_made_flag'].index, \n                          columns=data.loc[:, data.columns != 'shot_made_flag'].columns)\n\nobject_variable = data.shot_made_flag\n\nsubmit_ = pd.DataFrame(sc.transform(submit.loc[:, submit.columns != 'shot_made_flag'].values), \n                          index=submit.loc[:, submit.columns != 'shot_made_flag'].index, \n                          columns=submit.loc[:, submit.columns != 'shot_made_flag'].columns)\n\nmodel = classifiers[\"Gradient Boosting\"]\nmodel.fit(train[selected_features_gb], object_variable)\ny_pred = model.predict_proba(submit_[selected_features_gb])","d9241d7a":"predictions = pd.DataFrame({'shot_made_flag' : y_pred[:,1]},\n                           index = df[df.shot_made_flag.isnull()].index)\npredictions.index.name = 'shot_id'\npredictions.to_csv('\/kaggle\/working\/submission_{}.csv'.format(datetime.now().strftime('%Y_%m_%d')))","76a40f7e":"## 2.1. Casting types of variables\n\n<a id='casting'><\/a>","9b1d874d":"## 4.4. First modelling\n\n<a id='first'><\/a>","aaf6ef2a":"Veamos gr\u00e1ficamente los resultados obtenidos","ca6a5ffb":"## 2.2. Check missing values\n\n<a id='missing'><\/a>","28db56cb":"### Logistic Regression","6cc8297f":"# 1.1. Import libraries\n\n<a id='libraries'><\/a>","1ff2f0f5":"Esto demuestra que no por generar miles de \u00e1rboles obtendremos mejores resultados. Con unos 50 estimadores nos servir\u00e1 para que el algoritmo no cometa overfitting.","4f9a3541":"## 3.3. Date\n\n<a id='date'><\/a>","511b8ba1":"### Drop continuous variables","57205f3f":"# 2. Data Processing\n\n<a id='data_processing'><\/a>","3813a888":"# 1. Data Selection\n\n<a id='data_selection'><\/a>","dd6f27e6":"### Min. samples splits","a55a44f1":"## 4.2. Scaler data\n\n<a id='scaler'><\/a>","951142c8":"Finalmente elegiremos el algoritmo Gradient Boosting que a priori mejor resultados nos habian dado. En este caso, aplicaremos un grid de hiperparametros para cada parametros y los iremos seleccionando de forma aleatoria.","f0c8536c":"### Number estimators","23fe6985":"## 3.4. One hot encoding\n\n<a id='hot_encoding'><\/a>","94b87add":"\n# Table of Contents\n\n\n$\\;\\;\\;$ 1. [Data Selection](#data_selection)<br>\n$\\;\\;\\;\\;\\;\\;$ 1.1. [Import libraries](#libraries)<br>\n$\\;\\;\\;\\;\\;\\;$ 1.2. [Load data](#load_data)<br>\n$\\;\\;\\;$ 2. [Data Processing](#data_processing)<br>\n$\\;\\;\\;\\;\\;\\;$ 2.1. [Casting types of variables](#casting)<br>\n$\\;\\;\\;\\;\\;\\;$ 2.2. [Check missing values](#missing)<br>\n$\\;\\;\\;\\;\\;\\;$ 2.3. [Exploratory analysis](#exploratory)<br>\n$\\;\\;\\;\\;\\;\\;$ 2.4. [Processing and selecting features](#select_features)<br>\n$\\;\\;\\;\\;\\;\\;\\;\\;\\;$ 2.4.1. [Quantitative features](#select_quantitatives)<br>\n$\\;\\;\\;\\;\\;\\;\\;\\;\\;$ 1.4.2. [Qualitarive features](#select_qualitatives)<br>\n$\\;\\;\\;$ 3. [Data transformation](#data_transformation)<br>\n$\\;\\;\\;\\;\\;\\;$ 3.1. [Opponents](#opponent)<br>\n$\\;\\;\\;\\;\\;\\;$ 3.2. [Time](#time)<br>\n$\\;\\;\\;\\;\\;\\;$ 3.3. [Date](#date)<br>\n$\\;\\;\\;\\;\\;\\;$ 3.4. [One hot encoding](#hot_encoding)<br>\n$\\;\\;\\;$ 4. [Data Mining](#modelling)<br>\n$\\;\\;\\;\\;\\;\\;$ 4.1. [Splitting data](#splitting)<br>\n$\\;\\;\\;\\;\\;\\;$ 4.2. [Scaler](#scaler)<br>\n$\\;\\;\\;\\;\\;\\;$ 4.3. [Feature selecion](#feat_sel)<br>\n$\\;\\;\\;\\;\\;\\;$ 4.4. [First modelling](#first)<br>\n$\\;\\;\\;\\;\\;\\;$ 4.5. [Second modelling](#second)<br>\n$\\;\\;\\;$ 5. [Predictions](#prediction)<br>\n$\\;\\;\\;$ 6. [Anexo](#anexo)<br>\n\n\n","288b186a":"# 1.2. Load data\n\n<a id='load_data'><\/a>","d775d130":"Vemos como de balanceada esta la clase a predecir:","88bca752":"Con el fin de no extendernos demasiado en el tiempo de computo, vamos a tratar de averiguar cuales son los mejores hiperparametros para este conjunto de datos y el algoritmo Gradient Boosting","ef852aef":"## 2.3. Exploratory analysis\n\n<a id='exploratory'><\/a>","dc6c20ae":"Implementaremos un m\u00e9todo que escale los datos aplicando la diferencia el m\u00ednimo y cada punto del dataset, dividiendo eso por el rango.","8210b349":"Gradient boosting permite elegir como numero maximo de caracteristicas para optimizar la division del nodo la ra\u00edz cuadrada del numero de caracteristicas. Como contamos con unas 35 carateristicas, la raiz ser\u00e1 casi 6, y por tanto parece un valor bastante razonable para elegir.","15a44ad4":"En cuanto a las variables cualitativas, como no podemos estudiar la correlacion de Pearson, vamos a indagar en como est\u00e1n distribuidos los valores en cada variables con el fin de encontrar la forma de disminuir el n\u00famero de variables.","6a90fc17":"A pesar de que los resultados son muy parejos, Gradient Boost se ejecuto 4 veces mas r\u00e1pido que XGBoost. Finalmente, vamos a ejecutar un grid search para este modelo en busca de los mejores hiperparametros.","9c3b8be3":"## 3.1. Opponents\n\n<a id='opponent'><\/a>","7b4843e1":"# 2.4. Processing and selecting features\n\n<a id='select_features'> <\/a>","11ef00f9":"## 4.3. Features selection\n\n<a id='feat_sel'><\/a>","103a067d":"Como podemos ver en esta tabla que tenemos encima, parece que la variable 'action_type' aporta mas granularidad aun sobre la variable 'combined_shot_type', es decir, va m\u00e1s aun al detalle. Al introducir tantas cas\u00faisticas diferentes, seg\u00fan el modelo que se utilice ser\u00eda recomendable no usarla o tratar de preprocesarla.","1e553bf5":"Omitimos la seleccion de variables y mantenemos la variable ActionType","1c0c2544":"Una vez hemos entrenado y validado el modelo, se le ha asignado los mejores hiperparametros posible. En este momento vamos a realizar las predicciones sobre el conjunto de datos 'submit'","e27e912f":"Exportamos los resultados en el formato que Kaggle necesita para poder evaluarlo.","fc3aa60a":"Nuevamente el modelo produce overfitting para valores superiores a 0.5","025b53a2":"### LDA","30804118":"Como vemos, existe una inconsistencia entre el n\u00famero de equipos encontrados en una columna y en otra. Para avergiguar cual es la mejor soluci\u00f3n, recurriremos a la web oficial de la NBA y comprobaremos cuales son las siglas reales para los equipos que participan en dicha competici\u00f3n.","79103933":"Un paso fundamental que debemos realizar es el realizar un 'encoding' sobre las variables categoricas. La mayoria de algoritmos trabajan mejor con variables num\u00e9ricas. La idea de esta t\u00e9cnica se basa en, dada una variable predictora que puede tomar n valores distintos, generaremos a partir de ella un vector de n dimensiones, cada una de ellas correspondiente a los posibles valores. De forma que si para una instancia, esta variable predictora tomaba un valor \"X\", entonces el vector estar\u00e1 completamente relleno de ceros, excepto en la posicion de la columna \"X\".","42627195":"A la hora de entrenar nuestros modelos, puede ser computacionalmente muy complejo contar con mas de 150 variables. Es cierto que nuestro dataset no contiene demasiadas instancias, pero aun as\u00ed es conveniente seleccionar las variables mas importantes.","32c6607c":"## 4.1. Splitting data\n\n<a id='splitting'><\/a>","acf970e8":"Las variables con las que contamos hasta el momento son tanto binarias como multietiqueta. No parecen presentar demasiado problema a priori, salvo que quiz\u00e1 tengamos que discretizarlas num\u00e9ricamente m\u00e1s adelante. La \u00fanica variable categ\u00f3rica que no esta incluida en las gr\u00e1ficas anteriores es 'action_type'. Tendremos que examinarla con mas detalle, ya que cuenta con 57 etiquetas diferentes.","f6a5b53b":"El mejor learing rate rondar\u00e1 el valor 0.05, ya que a partir de 0.1 el algoritmo empieza a cometer overfitting.","4c4d0002":"Elegiremos valores que ronden 0.01","cc74783c":"### Gradient Boosting","d54929a7":"Hemos almacenado cuales han sido las variables mas relevantes aplicando este algoritmo. Probemos ahora cambiando el modelo, en este caso aplicando una regresi\u00f3n log\u00edstica, y comparemos resultados.","69249285":"# 2.4.1. Quantitative features\n\n<a id='select_quantitatives'> <\/a>","20d25531":"### Min. samples leaf","c2634a6d":"Nos encontramos que tenemos dos pares de variables que est\u00e1n completamente correlacionadas entre s\u00ed, lon-loc_x y lat-loc_y. Con el fin de no introducir informaci\u00f3n redundante en nuestro modelo, procederemos a eliminar de nuestro conjunto de datos dichas variables.","5ef0f22b":"### Learning rates","44f68007":"En este momento, toca implementar diferentes modelos y t\u00e9cnicas. Para ello, aplicaremos un cross validation con 5 folds para diferendes modelos y las principales variables.","48d94c58":"Se supone que los datos contenidos en la columna 'matchup' hacen referencia a los partidos que se disputaron entre Los Angeles Lakers y sus contrincantes. Esta columna nos denotaria \u00fanicamente cual es los dos equipos era local, ya que el oponente viene especificado nuevamente en la columna 'opponent'. De tal forma que los equipos contrarios deben ser los mismos en una columna y en otra. Veamos que pasa:","df91f5ae":"# 4. Data Mining\n\n<a id='modelling'><\/a>","853b325f":"### Matrix correlation","81452f0c":"# 2.4.2. Qualitative features\n\n<a id='select_qualitatives'> <\/a>","65f7b6aa":"El resultado obtenido, debe ser un dataframe que solo contenga variables predictoras num\u00e9ricas.","629aee4e":"## 3.2. Time\n\n<a id='time'><\/a>","91848b2d":"Los resultados mostrados son propios de mas de 10 modelos en los que hemos dejado por defecto los hiperparametros. Tiene sentido que habiendo seleccionado las mejores variables segun LDA y Linear Regression , sean dos de los modelos que mejores resultados obtienen. Pero no solo tenemos esos dos, si no tambi\u00e9n han obtenido muy buenos resultados Gradient Boosting y XGboots, ya que es una versi\u00f3n modificada del anterior. Lo que haremos ahora ser\u00e1, seleccionar esos 3 o 4 modelos mas importantes, y tratar de optimizar los hiperparametros.","a720a323":"En primer lugar vamos a dividir todo el conjunto de datos en dos. Por un lado, los datos que nos servir\u00e1n para realizar el aprendizaje supervisado, es decir, aquellos registros para los cuales tenemos el valor de la variable objetivo. Por otro, los registros a los que aplicaremos nuestro modelo predictivo y enviaremos a Kaggle.","cb1a6d28":"Otra de las variables que parecen estar muy relacionadas entre si son 'minutes_remaining' y 'seconds_remaining'. en lugar de tener ambas por separado, al deberse cada una de ellas a espacios temporales relacionados, podemos tratar de aunarlas en una sola. Por ejemplo de la siguiente forma: ","a1b3ba64":"Como vemos, hemos dividido nuestros datos en conjunto de entrenamiento y de validacion manteniendo las proporciones de clase originales.","4718add3":"# 5. Predictions\n\n<a id='prediction'><\/a>","71229f1d":"# 3. Data Transformation\n\n<a id='data_transformation'><\/a>","598eb0a0":"## 4.5 Second modelling\n\n<a id='second'><\/a>","57412352":"Como vemos, existen algunas variables que solo cuentan con un \u00fanico valor. A fines pr\u00e1cticos, poco podr\u00e1 aprender nuestro modelo de este tipo de caracteristicas.","35bca7e7":"### Max. features","1dbcc7ce":"Comenzaremos el an\u00e1lisis de nuestras caracter\u00edsticas por las variables cuantitativas. En este sentido, vamos a analizar la correlaci\u00f3n que existe entre cada una de esas variables con el fin de poder descartar alguna de ellas si es que poseen alta correlaci\u00f3n. Esto es importante para posteriormente no introducir informaci\u00f3n redundante a la hora de entrenar nuestro modelo.","c07be10a":"Hemos querido realizar un print tanto del n\u00famero de registros que han sido repartidos a cada conjunto, como del porcentaje de elementos de cada clase. A pesar de que hemos obtenido que un 55% de las instancias pertenecen a la clase 0 y un 45% a la clase 0, es decir, los porcentajes son bastante similares, cuando dividamos nuestro conjunto de datos en train-test, intentaremos mantener las mismas proporciones.","4e4f04b8":"Se ha comprobado manualmente que la columna 'matchup' posee errores en las siglas usadas par denotar a los equipos, por lo que para saber con que equipo se enfrentaban los LAL, usaremos la columna 'opponent' y descartaremos 'matchup'. Sin embargo, antes de eliminarla vamos a crear una nueva columna a partir de ella en la que indiquemos con 1 LAL jugaba como local y con 0 si jugaba como visitante. Si vamos a esa columna, vemos que existen dos formatos para expresar el enfrentamiento, el simbolo '@' y el 'vs.'. Tras una serie de comprobaciones tales como: buscar en google 2006-03-26 Lakers vs. new orleans pelicans y ver que LAL jugaron en casa a la vez que en los datos se denoto con vs. Asimismo se ha comprobado con el encuentro 2000-10-31 lakers vs partland, donde LAL jug\u00f3 como visitante y en nuestros datos aparece con un @. Finalmente la logica ser\u00e1, si aparece '@' en dicha columna, asigno un 0, si aparece un 'vs.', asigno un 1."}}