{"cell_type":{"932c0fc2":"code","9f534dbd":"code","7a4bdeb9":"code","7a15e40e":"code","d8b4d66c":"code","84a1b9b0":"code","dbaab55d":"code","676961f3":"code","f96fced7":"code","ba00aaab":"code","a3a92d7b":"code","6e54e1eb":"code","664f5654":"code","fa52ebf5":"code","ed8f2746":"code","5513609f":"code","77d2b0da":"markdown","60867a16":"markdown","fd0a903f":"markdown","52b1c3f3":"markdown","f3727ae0":"markdown"},"source":{"932c0fc2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9f534dbd":"#Adding all possible machine learning techniques, not going to use all.\nimport catboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn import svm\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.layers.advanced_activations import ReLU\nfrom keras.models import Sequential, Model\nfrom keras.layers import Activation, Convolution2D, MaxPooling2D, BatchNormalization, Flatten,Dense, Dropout, Conv2D,MaxPool2D, ZeroPadding2D\nimport keras\nimport cv2\nimport tensorflow\nfrom keras.utils import np_utils\nfrom keras.callbacks import ModelCheckpoint\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom sklearn.metrics import accuracy_score, plot_confusion_matrix, confusion_matrix","7a4bdeb9":"data = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndata.head()","7a15e40e":"#import pandas_profiling\n#from pandas_profiling import ProfileReport\n\n#profile = ProfileReport(data, title='Pandas Profiling Report')\n#profile","d8b4d66c":"import pandas as pd\nimport numpy as npo\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn","84a1b9b0":"plt.subplots(figsize=(11, 11)) \nsns.heatmap(data.corr(),annot=True)","dbaab55d":"#data=data.drop(['time'],axis=1)\n#data=data.drop(['ejection_fraction'],axis=1)","676961f3":"X=data.drop(['DEATH_EVENT'],axis=1)\ny=data[['DEATH_EVENT']]","f96fced7":"X.shape","ba00aaab":"y.shape","a3a92d7b":"#preprocess\n#from sklearn.preprocessing import MinMaxScaler\n#scaler = MinMaxScaler()\n#scaler.fit(data)\n#scaled_features = scaler.transform(data)\n#Convert to table format - MinMaxScaler\n#df_MinMax = pd.DataFrame(data=scaled_features, columns=[\"age\", \"serum_creatinine\",\"anaemia\",\"creatinine_phosphokinase\",\"diabetes\",\"high_blood_pressure\",\"platelets\"\n#                                                        ,\"serum_sodium\",\"time\",\"ejection_fraction\",\"sex\",\"smoking\",\"DEATH_EVENT\"])","6e54e1eb":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n\ntrain_set, test_set = train_test_split(data, train_size=0.8, test_size=0.2, random_state = 42)\nprint(len(train_set), ' Training Set +', len(test_set), ' Testing Set')\n","664f5654":"#Same for train set(Target is purchase)\nX_train = train_set.drop(\"DEATH_EVENT\", axis=1) # drop labels for training set\ny_train = train_set[\"DEATH_EVENT\"].copy()\n\n#same for test set\nX_test = test_set.drop(\"DEATH_EVENT\", axis=1)\ny_test = test_set[\"DEATH_EVENT\"].copy()","fa52ebf5":"from catboost import CatBoostClassifier\ncat = CatBoostClassifier(iterations=48,learning_rate=0.315,depth=4)\n\ncat.fit(X_train, y_train,eval_set=(X_test, y_test))","ed8f2746":"from sklearn.metrics import accuracy_score\npredict = cat.predict(X_test)\nprint(accuracy_score(y_test, predict))","5513609f":"#load up xgboost\nimport xgboost as xgb\n\ntrain = xgb.DMatrix(X_train, label = y_train)\ntest = xgb.DMatrix(X_test, label = y_test)\n\nparam = {'max_depth': 4, 'eta': 0.3, 'objective': 'multi:softmax', 'num_class':2}\nparam['nthread'] = 4\nparam['eval_metric'] = 'auc'\n\n#epochs or iterations.\niters = 20\nmodel = xgb.train(param, train, iters)\n\npredictions = model.predict(test)\nprint(accuracy_score(y_test, predictions))","77d2b0da":"# CATBOOST","60867a16":"Max-Min Normalization will be used as we will obtain smaller standard deviations which could further increase the accuracy, but proved not to increase it at all.","fd0a903f":"As shown below time and ejection_fraction are strongly negatively correlated to Death_event.","52b1c3f3":"from looking below upon the iterations, we can clearly narrow down the learning rate and iterations to get an accuracy score of 0.7833333333333333","f3727ae0":"# XGBOOST "}}