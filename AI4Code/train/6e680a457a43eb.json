{"cell_type":{"96fea16f":"code","f0595cca":"code","594459f5":"code","93749c7a":"code","0e632acd":"code","720f751a":"code","d452bdb0":"code","f46f2204":"code","9a67faf3":"code","42773b22":"code","7acd9f0b":"code","bdb3678a":"code","a1820c4c":"code","6104e0e5":"code","f7b935a7":"code","7ad50df4":"code","cf253b15":"code","55fe705a":"code","1e5b9ca9":"code","4266f5c5":"code","969bef00":"code","c64a3d8e":"code","cfda52a6":"code","a8944c80":"code","a256475b":"code","a03de9ee":"code","459e5f9d":"code","ad7cad01":"code","5d3f384b":"markdown","c41bc839":"markdown","ea9107d6":"markdown","35a34cb4":"markdown","47e1b4b7":"markdown","c362d133":"markdown","8db87ea7":"markdown","a83feef8":"markdown","39febcc2":"markdown","369340da":"markdown","6353617a":"markdown","ae8ba86c":"markdown","34728c98":"markdown","0d6352a5":"markdown","fa690bad":"markdown","f7f31403":"markdown","6f2da282":"markdown","e9d3093f":"markdown","a1a37bc0":"markdown","bf204929":"markdown","0fe40ca1":"markdown","1ff25eb1":"markdown","756700e2":"markdown","53ac505b":"markdown","90bf37a5":"markdown","887c68fb":"markdown","29a75dbe":"markdown","be3d2f7f":"markdown"},"source":{"96fea16f":"import os\nimport pandas as pd\nimport numpy as np\nfrom functools import reduce\nimport matplotlib.pyplot as plt\nimport time\nfrom IPython.display import display\npd.options.display.max_colwidth = 120\n%matplotlib inline","f0595cca":"input_root_dir = '..\/input\/cord-19-eda-parse-json-and-generate-clean-csv'\ninput_files_list = [f for f in os.listdir(input_root_dir) if os.path.splitext(f)[1] == '.csv']\ndf_list = []\nfor input_file in input_files_list:\n    df_next = pd.read_csv(os.path.join(input_root_dir, input_file))\n    df_next['source_dataset'] = input_file\n    df_list.append(df_next)\ndf_all_papers = pd.concat(df_list).reset_index(drop=True)","594459f5":"df_all_papers.head(3)","93749c7a":"df_all_papers.describe()","0e632acd":"print(f\"Number of missing texts: {df_all_papers['text'].isnull().sum()}\")","720f751a":"print(f\"Number of missing titles: {df_all_papers['title'].isnull().sum()}\")","d452bdb0":"print(f\"Number of missing authors: {df_all_papers['authors'].isnull().sum()}\")","f46f2204":"source_datasets_of_duplicates = list(df_all_papers.loc[df_all_papers['text'].duplicated(keep=False), 'source_dataset'].values)\nduplicated_text_bool_idx = df_all_papers['text'].duplicated()\ndf_all_papers = df_all_papers[np.logical_not(duplicated_text_bool_idx)]\nprint(f'{sum(duplicated_text_bool_idx)} papers with duplicated text were removed.')","9a67faf3":"removed_duplicates_total_count = 10","42773b22":"missing_abstract_bool_idx = df_all_papers['abstract'].isnull()\nprint(f\"Number of missing abstracts: {missing_abstract_bool_idx.sum()}\")","7acd9f0b":"indices = df_all_papers.loc[~df_all_papers['abstract'].isnull() & df_all_papers['abstract'].duplicated(keep=False),\n                  'abstract'].sort_values().index\n\ndef get_word_count(text):\n    # when missing\n    if isinstance(text, float):\n        return 0\n    return len(text.replace('\\n', ' ').split())\n    \ndef check_duplicates_by_sorted_indices(indices, duplication_field, max_number_of_groups=50, max_group_size=20, return_shown_indices=False):\n    cols = ['title', 'authors', 'abstract', 'text']\n    if not duplication_field in cols:\n        cols += [duplication_field]\n    duplicates_to_check = df_all_papers.loc[indices, cols]    \n    duplication_field_i = duplicates_to_check.columns.to_list().index(duplication_field)    \n    row_i = 0\n    duplication_count = 0\n    group_i = 0\n    if return_shown_indices:\n        indices_shown = []\n    while row_i + duplication_count + 1 < len(duplicates_to_check) and group_i < max_number_of_groups:\n        while (row_i + duplication_count + 1 < len(duplicates_to_check) and \n               duplicates_to_check.iloc[row_i, duplication_field_i] == duplicates_to_check.iloc[row_i + duplication_count + 1, duplication_field_i]):\n            duplication_count += 1 \n        group_i += 1\n        if duplication_count + 1 > max_group_size:\n            continue\n        \n        print(f'Group {group_i + 1} of potential duplicates')\n        dups_group = duplicates_to_check.iloc[row_i: row_i + duplication_count + 1]\n        if return_shown_indices:\n            indices_shown.extend(dups_group.index)\n        display(dups_group)\n        text_lens = dups_group['text'].map(get_word_count).values\n        if reduce(lambda x, y: x == y, text_lens):\n            print('Text have equal length.')\n        else:\n            print(f\"Text lengths are {', '.join(map(str, text_lens))}.\")\n        print('='*90)\n        row_i += duplication_count + 1\n        duplication_count = 0\n    if return_shown_indices:\n        return indices_shown\n                  \ncheck_duplicates_by_sorted_indices(indices, 'abstract')","bdb3678a":"missing_affiliations_count = df_all_papers['affiliations'].isnull().sum()\nprint(f\"Number of missing affiliations: {missing_affiliations_count}\")\nnon_missing_affiliations = df_all_papers.loc[np.logical_not(df_all_papers['affiliations'].isnull()), 'affiliations']\nduplicated_affiliations = non_missing_affiliations[non_missing_affiliations.duplicated()].unique()\nprint(f\"Number of duplicated affiliations: {len(duplicated_affiliations)}\")","a1820c4c":"duplicated_affiliations_bool_idx = df_all_papers['affiliations'].isin(duplicated_affiliations)\ntitle_affilation_series = (df_all_papers['title'].map(lambda x: [x]) +\n                           df_all_papers['affiliations'].map(lambda x: [x]))\nremoval_candidates_title_affiliation = title_affilation_series[duplicated_affiliations_bool_idx & \n                                                               title_affilation_series.map(tuple).duplicated(keep=False)]","6104e0e5":"print(f'Number of duplicates: {len(removal_candidates_title_affiliation)}.')","f7b935a7":"indices = (removal_candidates_title_affiliation\n           .map(lambda x: ', '.join(filter(lambda i: not isinstance(i, float), x)))\n           .sort_values().index)\n\ncheck_duplicates_by_sorted_indices(indices, 'affiliations')","7ad50df4":"merged_to_parent_papers_total_count = 0\nsource_datasets_of_merged_supps = []\nsource_datasets_of_parents_for_merged_supps = []\n\ndef drop_strategy_developed_for_affiliations(indices):\n    global merged_to_parent_papers_total_count\n    global removed_duplicates_total_count\n    global source_datasets_of_duplicates\n    global source_datasets_of_merged_supps\n    global source_datasets_of_parents_for_merged_supps\n    \n    merged_to_parent_papers_total_count_before = merged_to_parent_papers_total_count\n    indices_to_drop = []\n    duplicates_to_check = df_all_papers.loc[indices, ['title', 'abstract', 'text']]\n    for row_i in range(0, len(duplicates_to_check), 2):\n        dups_pair = duplicates_to_check.iloc[row_i: row_i + 2]\n        text_lens = dups_pair['text'].map(get_word_count).values\n        missing_abstracts = dups_pair['abstract'].map(lambda x: isinstance(x, float)).values\n        if text_lens[0] == text_lens[1]:\n            if missing_abstracts[0]:\n                indices_to_drop.append(duplicates_to_check.index[row_i])\n            else:\n                indices_to_drop.append(duplicates_to_check.index[row_i + 1]) # including the case when both abstracts are present\n            source_datasets_of_duplicates.extend(df_all_papers.loc[dups_pair.index, 'source_dataset'].values)\n        else:\n            missing_titles = dups_pair['title'].map(lambda x: isinstance(x, float)).values\n            shorter_paper_i = np.argmin(text_lens)\n            if not missing_titles[0] and not missing_titles[1]:\n                shorter_paper_index = duplicates_to_check.index[row_i + shorter_paper_i]\n                indices_to_drop.append(shorter_paper_index)\n                merged_to_parent_papers_total_count += 1\n                source_datasets_of_merged_supps.append(df_all_papers.loc[shorter_paper_index, 'source_dataset'])\n                if not missing_abstracts[(shorter_paper_i+1) % 2] and missing_abstracts[shorter_paper_i]:\n                    longer_paper_index = duplicates_to_check.index[row_i + (shorter_paper_i+1) % 2]\n                    df_all_papers.loc[longer_paper_index, 'text'] += ' ' + df_all_papers.loc[shorter_paper_index, 'text']\n                    source_datasets_of_parents_for_merged_supps.append(df_all_papers.loc[longer_paper_index, 'source_dataset'])\n    df_all_papers.drop(indices_to_drop, inplace=True)\n    number_of_merged_papers = merged_to_parent_papers_total_count - merged_to_parent_papers_total_count_before\n    removed_duplicates_total_count += len(indices_to_drop) - number_of_merged_papers\n    print(f'{len(indices_to_drop) - number_of_merged_papers} items were removed.')\n    print(f'{number_of_merged_papers} items were merged to parents.')\n\ndrop_strategy_developed_for_affiliations(indices)","cf253b15":"non_missing_authors = df_all_papers.loc[np.logical_not(df_all_papers['authors'].isnull()), 'authors']\nduplicated_authors = non_missing_authors[non_missing_authors.duplicated()].unique()\nduplicated_authors_bool_idx = df_all_papers['authors'].isin(duplicated_authors)\ntitle_authors_series = (df_all_papers['title'].map(lambda x: [x]) +\n                        df_all_papers['authors'].map(lambda x: [x]))\nremoval_candidates_title_author = title_authors_series[duplicated_authors_bool_idx & \n                                                       title_authors_series.map(tuple).duplicated(keep=False)]\nprint(f'Number of duplicates: {len(removal_candidates_title_author)}.')","55fe705a":"indices = (removal_candidates_title_author\n           .map(lambda x: ', '.join(filter(lambda i: not isinstance(i, float), x)))\n           .sort_values().index)\n\ncheck_duplicates_by_sorted_indices(indices, 'authors')","1e5b9ca9":"drop_strategy_developed_for_affiliations(indices)","4266f5c5":"indices = df_all_papers.loc[~df_all_papers['title'].isnull() & df_all_papers['title'].duplicated(keep=False),\n                            'title'].sort_values().index\nprint(f'Number of papers with non-unique title: {len(indices)}.')              ","969bef00":"check_duplicates_by_sorted_indices(indices, 'title', max_number_of_groups=20)","c64a3d8e":"indices = df_all_papers.loc[~df_all_papers['title'].isnull() & \n                            df_all_papers['title'].duplicated(keep=False) & \n                            (df_all_papers['title'].map(get_word_count) > 11),\n                            'title'].sort_values().index\nindices = check_duplicates_by_sorted_indices(indices, 'title', max_group_size=2, return_shown_indices=True)","cfda52a6":"drop_strategy_developed_for_affiliations(indices)","a8944c80":"fig, ax = plt.subplots(figsize=(7, 7))\n\n# credits: https:\/\/stackoverflow.com\/questions\/6170246\/how-do-i-use-matplotlib-autopct\ndef make_autopct(values):\n    def my_autopct(pct):\n        total = sum(values)\n        val = int(round(pct*total\/100.0))\n        return '{p:.0f}%  ({v:d})'.format(p=pct,v=val)\n    return my_autopct\n\nprocessed_duplicates_counts = [removed_duplicates_total_count, merged_to_parent_papers_total_count]\nax.pie(processed_duplicates_counts, labels=['Duplicates removed', 'Merged to parent as supplementary material'], \n       autopct=make_autopct(processed_duplicates_counts), shadow=True, startangle=90)\nax.axis('equal')\n_ = ax.set_title('How the discovered duplicates were processed', fontsize=20)","a256475b":"duplicate_source_dataset_counts = pd.Series(source_datasets_of_duplicates).value_counts()\nduplicate_source_dataset_counts.plot(kind='barh',\n                                     title='Counts of removed duplicates per dataset', \n                                     color=['dodgerblue'])","a03de9ee":"merged_source_dataset_counts = pd.Series(source_datasets_of_merged_supps).value_counts()\nmerged_source_dataset_counts.plot(kind='barh',\n                                  title='Counts of the merged shorter papers per dataset', \n                                  color=['dodgerblue'])","459e5f9d":"parent_for_merged_source_dataset_counts = pd.Series(source_datasets_of_parents_for_merged_supps).value_counts()\nparent_for_merged_source_dataset_counts.plot(kind='barh',\n                                             title='Counts of the merged longer papers per dataset', \n                                             color=['dodgerblue'])","ad7cad01":"output_file_name = f\"master_dataset_cleaned_{time.strftime('%Y%m%d_%H%M')}.csv\"\ndf_all_papers.to_csv(output_file_name, index=None)","5d3f384b":"Affiliations provide detailed info on authors. Therefore, when possible let's use `affiliations` to find duplicates. Otherwise, let's use the `authors` field.\n\n*a technical note: to speed up processing, I'm going to avoid pandas apply and use map instead. Whenever more columns would be required simultaneously, I'll just create a helping pandas series containing the required fields*","c41bc839":"## Counts of removed duplicates per source dataset","ea9107d6":"## Counts of merged duplicates per source dataset","35a34cb4":"## Using *affiliations* field","47e1b4b7":"*Observations*: \n* the first group seems to correspond to supplementary materials of the same paper,\n* the second group of the \"dummy\" short abstract contain different papers. The same holds for groups 6, 7.\n\nThe remaining true duplicates could have been detected when focusing on authors.\nTo sum up, instead of cleaning starting with duplicated abstracts we should start from the more reliable field.","c362d133":"Ideally, out of the remaining duplicates we'd like to leave the most informative entries. E.g., if a duplicated paper has entries with and without abstract, we'd prefer to leave the one with abstract.","8db87ea7":"# Discovering and cleaning duplicated papers\nI'm going to leverage output of the nice [xhlulu's extraction work](https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv).","a83feef8":"Not too many, let's verify the duplicates manually by text.","39febcc2":"By comparing `count` and `unique` we can see symptoms of duplicated entries. However, `unique` counts differ for different fields. Based on the main `text` field it seems like if almost all papers were unique. Yet, the `title` and `authors` fields suggest otherwise. \n","369340da":"No paper has missing text, it's great.","6353617a":"# Libraries","ae8ba86c":"## Using *text* field\nFirst, let's drop duplicated `text`.","34728c98":"We can see that quite a few papers have abstract missing. Let's check groups of papers with duplicated abstracts. Let's count words in the `text` field of the papers.","0d6352a5":"Except for the last group all observations made during cleaning based on affiliations hold. Let's repeat the cleaning.","fa690bad":"Let's keep track of the performed changes.","f7f31403":"# Summary of duplicates cleaning","6f2da282":"# Storing the results for further reuse","e9d3093f":"Regarding the last group from initial manual investigation, even though similar lengths suggested that we might be dealing with the same paper, manual check verified that those are two different papers.","a1a37bc0":"## Using *title* field\nAfter cleaning based on more reliable combinations of authors(affiliations) + title, let's check if some duplicates can be still spotted based on `title` field solely.","bf204929":"Based on the gained intuition from the inspection above, let's check only groups of size 2 with titles longer than 11 words.","0fe40ca1":"## Using *authors* field\nLet's repeat the same for the `authors` field, which is a less-informative version of the `affiliations`. \n\n*In the vain of Andrej Karpathy's advice not to generalize too early, I'm not creating general functions straight away for the one-time cleaning (even though I'm in fact copy-pasting.. but I'm not going to couple any other field with title, so no practical reason to generalize). Btw, if by any chance some of us hasn't read the Karpathy's [Recipe for Training Neural Networks](http:\/\/karpathy.github.io\/2019\/04\/25\/recipe\/) yet, I'd recommend to do so, it's priceless!*","1ff25eb1":"# Plan\n1. [Libraries](#Libraries)\n2. [Discovering and cleaning duplicated papers](#Discovering-and-cleaning-duplicated-papers)\n  * [Using *text* field](#Using-text-field)\n  * [Using *abstract* field](#Using-abstract-field)\n  * [Using *affiliations* field](#Using-affiliations-field)\n  * [Using *authors* field](#Using-authors-field)\n  * [Using *title* field](#Using-title-field)\n3. [Summary of duplicates cleaning](#Summary-of-duplicates-cleaning)\n  * [How the discovered duplicates were processed](#How-the-discovered-duplicates-were-processed)\n  * [Counts of removed duplicates per source dataset](#Counts-of-removed-duplicates-per-source-dataset)\n  * [Counts of merged duplicates per source dataset](#Counts-of-merged-duplicates-per-source-dataset)\n4. [Storing the results for further reuse](#Storing-the-results-for-further-reuse)","756700e2":"I'll do the following: if the text lengths are the same, then I'll drop the either the 2nd one or the one without abstract; if the the text lengths are different, we clean data only if both titles are not missing. In this case if both abstracts are present, then I'll drop the shorter paper. When a shorter paper has no abstract, I'll concatenate the texts.","53ac505b":"# Intro\nIn this notebook I'd like to focus on discovering and cleaning duplicated papers in the dataset, in hope to help avoid garbage-in-garbage-out scenario for all the awesome analytics being developed by our community.","90bf37a5":"Observations: some article pairs have the same number of words, it's obvious that they are duplicates with some insignificant differences in text (otherwise they would have been dropped as duplicated `text` already). Some papers have slight difference in word counts, suggesting different versions. However, in some cases it seems that we have supplementary materials stored as a separate paper. I've manually examined these cases, let me summarize some insights.\n\n*Observation 1*: the supplementary materials might contain important info, like the stats etc., but unfortunately we might not have the data itself, as in the case of the item above. Might be worth going through the most relevant papers manually.\n\n*Observation 2*: supplementary materials have abstract missing.\nAs supplementary materials can obviously provide important clues about values pieces of information (like the one below without actual figures yet with mentions of contacts\/contact durations ect.), let's keep it as well. It might indicate to download the full paper and add information from the tables.","887c68fb":"## How the discovered duplicates were processed","29a75dbe":"## Using *abstract* field","be3d2f7f":"Let's focus on the duplicated affiliations. Either those are just papers from the perfectly same team, or potential duplicates."}}