{"cell_type":{"528cb518":"code","c121a2d9":"code","49822dd8":"code","af22c32c":"code","3b451dee":"code","12a3bb6e":"code","f77d8dd2":"code","d4da7e85":"code","15b15a10":"code","93c516b4":"code","c9571812":"code","8b958d37":"code","399f14aa":"code","97dab60c":"code","0a87f9ef":"code","e2e36593":"code","38922fc0":"code","bd74c5bf":"code","18c8e634":"code","819f4e33":"code","a1412a32":"code","cad7f749":"code","912e159a":"code","aa77b610":"code","fdb4d541":"code","a0739761":"code","0f43b4cf":"code","2d434b98":"code","24135f54":"code","1bdecf1a":"code","3a123ca3":"code","f8c5558a":"code","14c65f76":"code","e39d57e3":"markdown","433b74f5":"markdown","bcab7cad":"markdown","ee13bdb7":"markdown","814448ff":"markdown","416a32c5":"markdown","137fec9d":"markdown","843fb207":"markdown","4fda13b6":"markdown","2a29fc5c":"markdown","26b340bf":"markdown","465fd80c":"markdown","6509a28d":"markdown"},"source":{"528cb518":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c121a2d9":"import pandas as pd\nimport numpy as np\nimport re\nimport math\nfrom bs4 import BeautifulSoup\nimport random","49822dd8":"pip install bert-for-tf2","af22c32c":"import tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers\nimport bert","3b451dee":"cols=[\"sentiment\",\"id\",\"date\",\"query\",'user',\"text\"]\ndata=pd.read_csv(\"\/kaggle\/input\/stanford-140-for-nlp\/training.1600000.processed.noemoticon.csv\",\n                header=None,\n                names=cols,\n                engine=\"python\",\n                encoding=\"latin1\")","12a3bb6e":"# Dropping some use less columns\ndata.drop([\"id\",\"date\",'query',\"user\"],axis=1,inplace=True)","f77d8dd2":"data.head()","d4da7e85":"def clean_tweet(tweet):\n    tweet = BeautifulSoup(tweet,\"lxml\").get_text()\n    tweet = re.sub(r\"@[A-Za-z0-9]+\",\" \",tweet) # like replace for string\n    tweet = re.sub(r\"https?:\/\/[A-Za-z0-9.\/]+\",' ',tweet) # replacing https and ? as s is not conformed\n    tweet = re.sub(r\"[^a-zA-Z.!?']\",\" \",tweet) # removing everything other than these\n    tweet = re.sub(r\" +\",\" \",tweet)\n    return tweet","15b15a10":"%%time\ndata_clean = [clean_tweet(tweet) for tweet in data.text]","93c516b4":"data_clean[0]","c9571812":"# Changing the value of labels\ndata_labels=data.sentiment.values\ndata_labels[data_labels==4] = 1","8b958d37":"FullTokenizer=bert.bert_tokenization.FullTokenizer\n# at hub all the pre-trained models are present\nbert_layer=hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\", \n                          trainable=False)\nvocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case=bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer=FullTokenizer(vocab_file,do_lower_case)","399f14aa":"def encode_sent(sent):\n    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))","97dab60c":"%%time\ndata_inputs=[encode_sent(sent) for sent in data_clean]","0a87f9ef":"# import time \n# while True:\n#     print(\"5\")\n#     time.sleep(80)","e2e36593":"data_with_len=[[sent, data_labels[i],len(sent)]\n                for i ,sent in enumerate(data_inputs)]\nrandom.shuffle(data_with_len)\n# As our data was sorted by labels to make batches of different sizes have all the data types\n# Now sorting the data by length of the sentence\ndata_with_len.sort(key=lambda x: x[2])\nsorted_all=[(sent_lab[0],sent_lab[1])\n            for sent_lab in data_with_len if sent_lab[2] > 7]\n\n## only using long sentences greater than 7 for better understanding of the sentencev by the model","38922fc0":"## Creating a Tensordata set to be used by Tensorflow\n## needed a Gerator to give to tensorflow so using list as it is also a generator\nall_dataset=tf.data.Dataset.from_generator(lambda :sorted_all ,\n                                          output_types=(tf.int32,tf.int32))","bd74c5bf":"next(iter(all_dataset))","18c8e634":"BATCH_SIZE = 32\nall_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None,),()))\n## Padding the tensors to make it of equal size","819f4e33":"next(iter(all_batched))","a1412a32":"## The numbers of input we have\/batch size = no of batches \nNB_BATCHES = math.ceil(len(sorted_all)\/BATCH_SIZE)\n## get 1\/10 of it to create testing set\nNB_BATCHES_TEST = NB_BATCHES\/\/10\n## Need to shuffle the data set as shaorter are in the start and longer at the end\nall_batched.shuffle(NB_BATCHES)\n## If size is large use less than No of batches in buffer\n\n## Creating testing and training dataset\n\ntest_dataset=all_batched.take(NB_BATCHES_TEST)\ntrain_dataset=all_batched.skip(NB_BATCHES_TEST)","cad7f749":"## nb_filters == no of filter\n##CNN filters for ecah size 50 size 4 , 3 , 2\n\n## FFN_units == No of hidden units used in dense layer in the end ..\n## Between two dense layers the number of hideen layer \n\n## nb_classes : no of classes for classification : 2 by default in this case","912e159a":"class DCNN(tf.keras.Model):\n    \n    def __init__(self,vocab_size, emb_dim=128, nb_filters=50, FFN_units=512,\n                    nb_classes=2, dropout_rate=0.1, training=False, name=\"dcnn\"):\n        \n        ## inherits from class its comming from, which allows us use the methods from the class which we are inheriting from\n        \n        super(DCNN,self).__init__(name=name)\n        ## Creating the embedding layer\n        ## creating vectors\n        \n        self.embedding = layers.Embedding(vocab_size,emb_dim)\n        \n        # creating the CNN layer\n        ## using padding valid : stride out feature detecture by 1\n        self.bigram = layers.Conv1D(filters=nb_filters,\n                                    kernel_size=2,\n                                    padding=\"valid\",\n                                    activation=\"relu\")\n        \n        self.trigram = layers.Conv1D(filters=nb_filters,\n                                     kernel_size=3,\n                                     padding=\"valid\",\n                                     activation=\"relu\")\n        \n        self.fourgram = layers.Conv1D(filters=nb_filters,\n                                      kernel_size=4,\n                                      padding=\"valid\",\n                                      activation=\"relu\")\n        \n        ## Creating a layer which takes the max of all the outputs\n        \n        self.pool=layers.GlobalAveragePooling1D()\n        \n        ## Creatign two dense layers with hidden number of units between the two dense layers\n        \n        self.dense1=layers.Dense(units=FFN_units,\n                                 activation=\"relu\")\n        ## Create a sense of gerenalarity using Dropout\n        \n        self.dropout=layers.Dropout(dropout_rate)\n        \n        if nb_classes==2:\n            \n            self.dense2=layers.Dense(units=1,\n                                     activation=\"sigmoid\")\n        else :\n            self.dense2=layers.Dense(units=nb_classes,\n                                     activation=\"softmax\")\n        \n    \n    def call(self,inputs,training):\n        ## training Bool:if trainign false not use of fropout\n        x=self.embedding(inputs)\n        x_1=self.bigram(x)\n        x_1=self.pool(x_1)\n        ## each of 50 feature detector of size 2 we get 1 number which has max values\n        \n        x_2=self.trigram(x)\n        x_2=self.pool(x_2)\n        \n        x_3=self.fourgram(x)\n        x_3=self.pool(x_3)   ## (batch_szie, nb_filters)\n        \n        ## now concat all the results and apply dense layers\n        \n        merged=tf.concat([x_1,x_2,x_3],axis=1) ## (batch_size,3*nb_flters)\n        merged=self.dense1(merged)\n        \n        merged=self.dropout(merged,training)\n        output=self.dense2(merged)\n        \n        return output","aa77b610":"VOCAB_SIZE=len(tokenizer.vocab)\nEMB_DIM=200\nNB_FILTERS=100\nFFN_UNITS=256\nNB_CLASSES=2\n\nDROPOUT_RATE=0.2\n\nNB_EPOCHS=5","fdb4d541":"Dcnn = DCNN(vocab_size=VOCAB_SIZE,\n            emb_dim=EMB_DIM,\n            nb_filters=NB_FILTERS,\n            FFN_units=FFN_UNITS,\n            nb_classes=NB_CLASSES,\n            dropout_rate=DROPOUT_RATE)","a0739761":"if NB_CLASSES==2:\n    Dcnn.compile(loss=\"binary_crossentropy\",\n                optimizer=\"adam\",\n                metrics=[\"accuracy\"])\nelse:\n    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n                optimizer=\"adam\",\n                metrics=[\"sparse_categorcal_accuracy\"])","0f43b4cf":"## Creating Checkpoint in order to save the model weights of model\ncheckpoint_path=\".\/ckpt_bert_tok\"\n\nckpt=tf.train.Checkpoint(Dcnn=Dcnn)\nckpt_manager=tf.train.CheckpointManager(ckpt,checkpoint_path,max_to_keep=1)\n\nif ckpt_manager.latest_checkpoint:\n        ckpt.restore(ckpt_manager.latest_checkpoint)\n        print(\"Latest chekcpoint restored\")","2d434b98":"class MyCustomCallback(tf.keras.callbacks.Callback):\n    \n    def on_epoch_ends(self,epoch,logs=\"None\"):\n        ckpt_manager.save()\n        print(f\"Checkpoint is saved at {checkpoint_path}\")","24135f54":"Dcnn.fit(train_dataset,\n        epochs=NB_EPOCHS,\n        callbacks=[MyCustomCallback()])","1bdecf1a":"results=Dcnn.evaluate(test_dataset)\nprint(results)","3a123ca3":"def get_prediction(sent):\n    tokens = encode_sent(sent)\n    ## As tensorflow works with batches need to send batches even if we have only 1 senteance\n    inputs = tf.expand_dims(tokens,0)\n    output = Dcnn(inputs,training=False)\n    ## *2 as we have two classes \n    sentiment = math.floor(output*2)\n    \n    if sentiment ==0:\n        print(f\"Output of the model : {sentiment}\\npredicted sentiment : negative\")\n    elif sentiment ==1:\n        print(f\"Output of the model : {sentiment}\\npredicted sentiment : positive\")","f8c5558a":"get_prediction(\"Pls help me, i am stuck\")","14c65f76":"Dcnn.save(\"BERT_sentiment_token.h5\") ","e39d57e3":"## Training our model","433b74f5":"<img src=\"data:image\/jpeg;base64,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\" width=75% height=36%>\n\n# Importing libraries","bcab7cad":"> #### As we are using BERT is trained on a bigger corpus of words, so it gives better acccuracy, if we train it on out own model, as out courspus is not very large the data might not get as good accuracy as we are getting when using BERT","ee13bdb7":"### using Small BERT","814448ff":"## As the data is in XML","416a32c5":"# Model Building","137fec9d":"# Data Creation\n### We will create padded batches (so wee pad sentences for each batch independently), this way we add the minimum of padding tokens possible. For that, we sort sentences by length , apply padded_batches and then shuffle","843fb207":"### Here we can see how the tokenizer tokenizes the values","4fda13b6":"# Tokenization\n## we need to create a BERT layer to have access to meta data for tokenizer(like vocab size)","2a29fc5c":"# Cleaning Data","26b340bf":"## Creating Callbacks","465fd80c":"## Here we get batch of 32 sentences, and the corresponding labels","6509a28d":"# Evaluating the model"}}