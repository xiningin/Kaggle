{"cell_type":{"ec0fffc0":"code","09b6bbb8":"code","f5f8c87e":"code","94051031":"code","187c961c":"code","6df815f6":"code","c4aaf12d":"code","6ea1897d":"code","c05c7c45":"code","8eed0416":"code","a5f3a51e":"code","c2cf4640":"code","9809d68d":"code","1d146d47":"code","8c9af0cd":"code","8c978102":"code","5a5e6cf4":"code","7e3845c9":"code","75dadbce":"code","799b6625":"code","1a014c6b":"code","b171e637":"code","f861f3c4":"code","105082e2":"code","f8460d99":"code","db4d34ab":"code","5129f6fb":"code","df7fe32a":"code","72a9b25d":"code","24295067":"code","250fb01b":"code","615f9005":"code","ce74f883":"code","7d4d5cc6":"code","1cd9e6e3":"code","9f948d41":"markdown","c3840977":"markdown","ea57aec6":"markdown","0c52fe88":"markdown","a3dccdc5":"markdown","029be424":"markdown","abec880e":"markdown","51202b83":"markdown","bf122759":"markdown","7ad2b5eb":"markdown","12e1ae6b":"markdown","1a3a674d":"markdown","2bd6b9e9":"markdown","27350ba4":"markdown","80210b3e":"markdown","84de18c5":"markdown","196ebe39":"markdown","327f3492":"markdown","c6db05a9":"markdown"},"source":{"ec0fffc0":"!git clone https:\/\/github.com\/JulienAu\/Anomaly_Detection_Tuto.git","09b6bbb8":"!rm -r Anomaly_Detection_Tuto\/*.ipynb Anomaly_Detection_Tuto\/.git Anomaly_Detection_Tuto\/Data\/*.gif","f5f8c87e":"from IPython.display import Image\nImage(filename=\".\/Anomaly_Detection_Tuto\/Data\/Autoencoder_structure.png\")","94051031":"import random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\nimport warnings\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline","187c961c":"import logging\nhandler=logging.basicConfig(level=logging.INFO)\nlgr = logging.getLogger(__name__)\n\n#Utilisation des GPUs\n#use_cuda = torch.cuda.is_available()\nuse_cuda = False\n\nFloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\nLongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\nTensor = FloatTensor\n\n#lgr.info(\"USE CUDA=\" + str (use_cuda))","6df815f6":"def Rand(num):\n    res = []\n    for j in range(num):\n        res.append(random.random())\n    return res\n\n\n\n##### series 0 - half sine, half linear\nnb_periods=10\nnb_points=2600\nx = np.linspace(-nb_periods*np.pi, nb_periods*np.pi, nb_points)\nx2 = np.linspace(int(-nb_periods*np.pi\/3), int(nb_periods*np.pi\/3), int(nb_points\/3))\naa=list(np.sin(x))\nbb=list(np.zeros(int(nb_points\/4)))\ncc=list(np.sin(x2))\ndd=list(np.arange(int(nb_points\/4))\/int(nb_points\/-4))\nee=list(np.arange(int(nb_points\/4))\/int(nb_points\/4))\ny=list(np.arange(len(aa+aa+bb+cc+dd+aa)))\n# On joint nos 3 patterns\nserie_simple=np.array(aa+aa+bb+cc+ee+cc+dd+aa)\n\n# Visualisation \nfig=plt.figure(figsize=(12,4))\nplt.plot(serie_simple)\nplt.title(\"A simple series composed of 4 patterns\")\nplt.show()","c4aaf12d":"fig=plt.figure(figsize=(12,4))\ndataframe = pd.DataFrame(serie_simple.astype('float32'))\ndf_total=dataframe\nplt.plot(df_total)\n\ndf= dataframe[:6500]\nplt.plot(df,label=\"Apprentissage\")\n\ndf_test= dataframe[6500:10000]\nplt.plot(df_test,label=\"Test\")\nplt.legend()\nplt.suptitle(\"D\u00e9coupage de la s\u00e9rie temporelle\")\nplt.show()","6ea1897d":"dataset = df.values\ndataset_test = df_test.values","c05c7c45":"def normalize_test(dataset,dataset_test):\n    \"\"\"\n    Permet de normaliser les valeurs de la time-series [0;1] par rapport aux valeurs de la p\u00e9riode d'apprentissage\n    \n    Inputs : \n        data = np array 1D correspondant \u00e0 une s\u00e9rie temporelle\n    Output : np.array 1D normalis\u00e9\n    \"\"\"\n    max_value = np.max(dataset)\n    min_value = np.min(dataset)\n    scalar = max_value - min_value\n    dataset = list(map(lambda x: (x-min_value)   \/ scalar, dataset))\n    dataset_test = list(map(lambda x: (x-min_value)   \/ scalar, dataset_test))\n    return dataset , dataset_test\n\ndataset,dataset_test=normalize_test(dataset,dataset_test)\n","8eed0416":"def rolling_window(data,look_back,pattern_indices=[]):\n    \"\"\"\n    Inputs : \n        data = np array 1D correspondant \u00e0 une s\u00e9rie temporelle\n        pattern_indices = quand ils sont connus, les indices de changements de patterns\n        look_back : taille de la fen\u00eatre glissante\n    Output : un np.array 2D avec \"look_back\" columns \n    \"\"\"\n    dataX=[]\n    for i in range(len(data)-look_back-1):\n        a = data[i:(i+look_back)]\n        dataX.append(list(a))\n\n    dataX=np.array(dataX)\n    return dataX","a5f3a51e":"look_back=128\ndata_X=rolling_window(dataset,look_back)\n#print(data_X.shape)\ndata_X=data_X.reshape(-1, 1,look_back)\n#print(data_X.shape)\n\nif use_cuda:\n        #lgr.info (\"Using the GPU\")    \n        train_x = torch.from_numpy(data_X).float().cuda() # Note the conversion for pytorch    \nelse:\n        #lgr.info (\"Using the CPU\")\n        train_x = torch.from_numpy(data_X).float() # Note the conversion for pytorch\n","c2cf4640":"class autoencoder(nn.Module):\n    def __init__(self ,input_size):\n        super(autoencoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_size, 128),\n            nn.ReLU(True),\n            nn.Linear(128, 32),\n            nn.ReLU(True),\n            nn.Linear(32, 12),\n            nn.ReLU(True), \n            nn.Linear(12, 3))\n        self.decoder = nn.Sequential(\n            nn.Linear(3, 12),\n            nn.ReLU(True),\n            nn.Linear(12, 32),\n            nn.ReLU(True),\n            nn.Linear(32, 128),\n            nn.ReLU(True),\n            nn.Linear(128, input_size), \n            nn.Tanh())\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x","9809d68d":"net = autoencoder(look_back)","1d146d47":"criterion = nn.MSELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)","8c9af0cd":"if use_cuda:\n    lgr.info (\"Using the GPU\")    \n    net.cuda()\n    criterion.cuda()","8c978102":"from IPython.display import Image\nImage(filename=\".\/Anomaly_Detection_Tuto\/Data\/mse.png\")","5a5e6cf4":"net_loss=[]\nfor e in range(500):\n    var_x = Variable(train_x)\n    out = net(var_x)\n    loss = criterion(out, var_x)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    net_loss.append(loss.item())\n    if (e + 1) % 50 == 0:\n        print('Epoch: {}, Loss: {:.5f}'.format(e + 1, loss.item()))     ","7e3845c9":"plt.suptitle(\"MSE during training \")\nplt.plot(net_loss)\nplt.show()","75dadbce":"net = net.eval()\nif use_cuda:\n    lgr.info (\"Using the GPU\")    \n    net.cuda()","799b6625":"var_data = Variable(train_x)\npred_test = net(var_data)\npred_test = pred_test.view(-1,128)","1a014c6b":"def reconstruction_viz(train_x,autoencoder,lookback,index=0):\n    var_data = Variable(train_x)\n    pred_test = net(var_data)\n    pred_test = pred_test.view(-1,lookback).data.cpu().numpy()\n    data = train_x.view(-1,lookback).data.cpu().numpy()\n    fig=plt.figure(figsize=(12,4))\n    plt.plot(data[:,1],color=\"blue\",label=\"Real series\")\n    plt.plot(pred_test[:,1],color=\"green\",label=\"Reconstruction\")\n    plt.legend()\n    plt.suptitle(\"A piece of a curve and its reconstruction by the auto encoder\")\n    plt.show()\n\nreconstruction_viz(train_x,net,look_back,index=0)","b171e637":"def reconstruction_viz2(train_x,autoencoder,lookback,index=0):\n    var_data = Variable(train_x)\n    pred_test = net(var_data)\n    pred_test = pred_test.view(-1,lookback).data.cpu().numpy()\n    data = train_x.view(-1,lookback).data.cpu().numpy()\n    ecart= np.abs(np.subtract(data[:,1],pred_test[:,1]))\n    \n    fig, ax = plt.subplots(nrows=4, sharey=True,figsize=(12,12))\n    ax[0].plot(pred_test[:,1], 'r', label='reconstruction')\n    ax[0].legend(loc='best')\n    ax[1].plot(data[:,1], 'b', label='real')\n    ax[1].legend(loc='best')\n    ax[2].plot(pred_test[:,1], 'r', label='reconstruction')\n    ax[2].plot(data[:,1], 'b', label='real')\n    ax[2].legend(loc='best')\n    ax[3].plot(ecart, 'g', label='ecart')\n    ax[3].legend(loc='best')\n    plt.legend()\n    plt.suptitle(\"Result of the autoencoder\")\n    plt.show()\n\nreconstruction_viz2(train_x,net,look_back,index=0)","f861f3c4":"data_X_test=rolling_window(dataset_test,look_back)\n\ndata_X_test=data_X_test.reshape(-1, 1,look_back)\n\n\nif use_cuda:\n        #lgr.info (\"Using the GPU\")    \n        test_x = torch.from_numpy(data_X_test).float().cuda() # Note the conversion for pytorch    \nelse:\n        #lgr.info (\"Using the CPU\")\n        test_x = torch.from_numpy(data_X_test).float() # Note the conversion for pytorch\n\n\n\nvar_data_test = Variable(test_x)\npred_test = net(var_data_test)\npred_test = pred_test.view(-1,128)\n\nreconstruction_viz2(test_x,net,look_back,index=0)","105082e2":"df_total= pd.read_json('.\/Anomaly_Detection_Tuto\/Data\/serie2.json', lines=True)[['val']]\nplt.plot(df_total)\n\ndf= pd.read_json('.\/Anomaly_Detection_Tuto\/Data\/serie2.json', lines=True)[['val']][3000:6000]\nplt.plot(df,label=\"Train\")\n\ndf_test= pd.read_json('.\/Anomaly_Detection_Tuto\/Data\/serie2.json', lines=True)[['val']][7000:12000]\nplt.plot(df_test,label=\"Test\")\n\nplt.legend()\nplt.suptitle(\"The time series\")\nplt.show()","f8460d99":"dataset = df.values\ndataset = dataset.astype('float32')\n\ndataset_test = df_test.values\ndataset_test = dataset_test.astype('float32')\n","db4d34ab":"dataset,dataset_test=normalize_test(dataset,dataset_test)\ndata_X=rolling_window(dataset,look_back)\n\ndata_X=data_X.reshape(-1, 1,look_back)\nif use_cuda:\n        #lgr.info (\"Using the GPU\")    \n        train_x = torch.from_numpy(data_X).float().cuda() # Note the conversion for pytorch    \nelse:\n        #lgr.info (\"Using the CPU\")\n        train_x = torch.from_numpy(data_X).float() # Note the conversion for pytorch\n","5129f6fb":"net = autoencoder(look_back)","df7fe32a":"criterion = nn.MSELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=0.005)\n\nif use_cuda:\n    #lgr.info (\"Using the GPU\")    \n    net.cuda()\n    criterion.cuda()","72a9b25d":"net_loss=[]\nfor e in range(1200):\n    var_x = Variable(train_x)\n    out = net(var_x)\n    loss = criterion(out, var_x)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    net_loss.append(loss.item())\n    if (e + 1) % 100 == 0:\n        print('Epoch: {}, Loss: {:.5f}'.format(e + 1, loss.item()))","24295067":"plt.suptitle(\"MSE during training\")\nplt.plot(net_loss)\nplt.show()","250fb01b":"net = net.eval()\nif use_cuda:\n    #lgr.info (\"Using the GPU\")    \n    net.cuda()","615f9005":"var_data = Variable(train_x)\npred_test = net(var_data)\n#print(pred_test.shape)\n\npred_test = pred_test.view(-1,128)\n#print(pred_test.shape)","ce74f883":"reconstruction_viz(train_x,net,look_back,index=0)","7d4d5cc6":"reconstruction_viz2(train_x,net,look_back,index=0)","1cd9e6e3":"data_X_test=rolling_window(dataset_test,look_back)\n\ndata_X_test=data_X_test.reshape(-1, 1,look_back)\n\n\nif use_cuda:\n        #lgr.info (\"Using the GPU\")    \n        test_x = torch.from_numpy(data_X_test).float().cuda() # Note the conversion for pytorch    \nelse:\n        #lgr.info (\"Using the CPU\")\n        test_x = torch.from_numpy(data_X_test).float() # Note the conversion for pytorch\n\n\nvar_data_test = Variable(test_x)\npred_test = net(var_data_test)\npred_test = pred_test.view(-1,128)\n\nreconstruction_viz2(test_x,net,look_back,index=0)","9f948d41":"An auto encoder is a neural network that learns a representation of data in a smaller dimension. In concrete terms, it is given a vector and is expected to return the same vector, in other words, it rebuilds its entry.\n\nThe structure of a simple auto encoder is shown in the following image.\n\nThe auto encoder is composed of an encoder, which takes an X vector as input and returns a smaller Z vector.\n\nThe second part of the auto encoder, the decoder, takes as input the small vector returned by the encoder (Z) and returns a vector of the same size as X.\n\n\n* X is the input vector\n* Z is an \"encoding\" of X\n* X' is a \"reconstruction\" of X","c3840977":"The autoencoder learns well how to reconstruct the curve. Indeed, the gap between the real curve and the reconstruction is quite small.\n\nLet's now try to apply the autoencoder over our test period.\n","ea57aec6":"**Let's train our autoencoder :  **","0c52fe88":"The auto encoder is a powerful model that can learn the structure of time series.\n\nHowever, one point remains to be studied to produce a robust and fully automated approach from an auto encoder.\n\n* an automatic and intelligent choice of the anomaly threshold.\n","a3dccdc5":"First of all, it is a matter of training the auto encoder so that it understands the characteristics of the series over a so-called \"normal\" period. \n\nTo generate the training data, a sliding window of a limited size will be applied to the series, but it will still capture the characteristics of the series.\n\n**In practice, the size of the sliding window is equal to the minimum size of a pattern.**\n\nLet's take a window size of 128.","029be424":"The autoencoder always learns how to reconstruct the curve well. \n\nIndeed, the gap between the real curve and the reconstruction remains rather small.\n\nWe can already identify some anomalies in our learning period that did not impact the learning of our autoencoder.\n\n\nLet's now try to apply the autoencoder over our test period.\n","abec880e":"The purpose of this notebook is to detect anomalies in time series using autoencoders.","51202b83":"# Application to complex series <a name=\"astrolog\"><\/a>","bf122759":"**Let's train our autoencoder :  **","7ad2b5eb":"It is hoped that the neural layers of the encoder will be able to extract characteristic features of the input vector. For example, if the input vector is a sinusoid with a length of 2000 points, it is hoped that the auto encoder will \"understand it\" and be able to encode it with a very small vector.","12e1ae6b":"**Same result as before! Our anomaly detector works fine!**\n\nHowever, it can be observed that the choice of threshold is slightly more complex. ","1a3a674d":"# Conclusion <a name=\"conclusion\"><\/a>","2bd6b9e9":"# Example of application on a simple series <a name=\"simple\"><\/a>","27350ba4":"In the context of anomaly detection (unsupervised), the classic approach is to train a model, at each moment, to predict the next point. If the observed point is too far from the prediction, the point is declared an anomaly.\n\nIn the case of an autoencoder, the objective is to train a network of neurons to learn the patterns of a curve over normal periods. Thereafter, the network will be able to reproduce the curve as long as the patterns of the curve do not change. If the patterns change, the reproduction made by the autoencoder will be less accurate and we will be able to detect this change.\n","80210b3e":"We can observe that the difference is much greater over periods that do not correspond to the \"normal\" behaviour of the curve! \n\nA threshold could now be defined to select abnormal periods. \n\n**Our anomaly detector works fine !**\n\nLet's now try a more complex curve.","84de18c5":"The metric allowing us to calculate the performance of the autoencoder is **Mean Squared Error (MSE)**","196ebe39":"# What is an auto encoder? ? <a name=\"introduction\"><\/a>","327f3492":"# How to use it in anomaly detection?<a name=\"app\"><\/a>","c6db05a9":"## Contents\n1. [What is an auto encoder?](#introduction)\n2. [How to use it in anomaly detection?](#app)\n3. [Example of application on a simple series](#simple)\n4. [Application to complex series](#astrolog)\n5. [Conclusion](#conclusion)\n"}}