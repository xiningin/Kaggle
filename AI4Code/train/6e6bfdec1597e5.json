{"cell_type":{"48e02305":"code","e3a7bdcd":"code","096e239b":"code","e713b6c5":"code","bc15b2b7":"code","b3d286c0":"code","979e76d4":"code","892267a2":"code","7655ac0c":"code","299c287f":"code","d22822f0":"code","abf02a9d":"code","eebf5c2d":"code","8b41f1cf":"code","13f26fb6":"code","10c7e88f":"code","80c18512":"code","f33ec657":"code","e6ebeb1a":"code","af6f3099":"code","e9fe50cb":"code","7ce36a79":"code","88ae56ba":"code","e89f3b09":"code","7b8efc2a":"code","94fbb077":"code","c3081658":"code","34f9cf49":"code","20fb7a02":"code","37a19936":"code","3107f2e9":"code","a3407f47":"code","0288df0c":"code","44f719e7":"code","f4d97177":"code","2e799e1d":"code","2697e601":"code","92b71674":"code","72ea826a":"code","38ffb953":"code","9ba0def2":"code","9cb1b971":"code","20f0e4a1":"code","7f26c26c":"code","f15d414e":"code","d011241f":"code","d755378b":"code","ea6dd6f4":"code","8980b276":"code","7b733aae":"code","23e8c48a":"markdown","33327e34":"markdown","5d2b7526":"markdown","76f337f5":"markdown","5c3dc791":"markdown","41f57482":"markdown","b8ec7f45":"markdown","9609a9c6":"markdown","8b4fc454":"markdown","3e16ec05":"markdown","2da2282a":"markdown","2cc580dd":"markdown","fbe86541":"markdown","5b505ae1":"markdown","81937889":"markdown","46f59f7b":"markdown","631221f7":"markdown","177a01a1":"markdown","55fbd70c":"markdown","dbcdda55":"markdown","67027fa6":"markdown","d9a8eb84":"markdown","5ac381f0":"markdown","68ba5079":"markdown","f7cbbee5":"markdown","7803696d":"markdown"},"source":{"48e02305":"#Importing libraries\nimport nltk, re, pprint\nimport numpy as np\nimport pandas as pd\nimport requests\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pprint, time\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize","e3a7bdcd":"# reading the Treebank tagged sentences\nnltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))","096e239b":"# first few tagged sentences\nprint(nltk_data[:4])\nprint(len(nltk_data))","e713b6c5":"# Splitting into train and test\nrandom.seed(1234)\ntrain_set, test_set = train_test_split(nltk_data,test_size=0.05)\n\nprint(len(train_set))\nprint(len(test_set))\n#print(train_set[:40])","bc15b2b7":"# Getting list of tagged words\ntrain_tagged_words = [tup for sent in train_set for tup in sent]\nlen(train_tagged_words)","b3d286c0":"# number of unique POS tags in the training corpus\n# we can use the set() function on the list of tags to get a unique set of tags, \n# and compute its length\ntags = [pair[1] for pair in train_tagged_words]\nunique_tags = set(tags)\nlen(unique_tags)","979e76d4":"# Which is the most frequent tag in the corpus\n# to count the frequency of elements in a list, the Counter() class from collections\n# module is very useful, as shown below\n\nfrom collections import Counter\ntag_counts = Counter(tags)\ntag_counts","892267a2":"# the most common tags can be seen using the most_common() method of Counter\ntag_counts.most_common(12)","7655ac0c":" #how many words with the tag 'VERB' (verb) end with 'ed'\npast_tense_verbs = [pair for pair in train_tagged_words if pair[1]=='VERB']\ned_verbs = [pair for pair in past_tense_verbs if pair[0].endswith('ed')]\nprint(len(ed_verbs) \/ len(past_tense_verbs))\ned_verbs[:20]","299c287f":"# how many words with the tag 'VERB' end with 'ing'\nparticiple_verbs = [pair for pair in train_tagged_words if pair[1]=='VERB']\ning_verbs = [pair for pair in participle_verbs if pair[0].endswith('ing')]\nprint(len(ing_verbs) \/ len(participle_verbs))\ning_verbs[:20]","d22822f0":"tag_counts","abf02a9d":"# what fraction of adjectives ADJ are followed by a noun NOUN\n\n# create a list of all tags (without the words)\ntags = [pair[1] for pair in train_tagged_words]\n\n# create a list of ADJ tags\nADJ_tags = [t for t in tags if t == 'ADJ']\n\n# create a list of (ADJ, NOUN) tags\nADJ_NOUN_tags = [(t, tags[index+1]) for index, t in enumerate(tags) \n              if t=='ADJ' and tags[index+1]=='NOUN']\n\nprint(len(ADJ_tags))\nprint(len(ADJ_NOUN_tags))\nprint(len(ADJ_NOUN_tags) \/ len(ADJ_tags))","eebf5c2d":"# what fraction of determiners DET are followed by a noun NOUN\ndet_tags = [t for t in tags if t == 'DET']\ndet_nn_tags = [(t, tags[index+1]) for index, t in enumerate(tags) \n              if t=='DET' and tags[index+1]=='NOUN']\n\nprint(len(det_tags))\nprint(len(det_nn_tags))\nprint(len(det_nn_tags) \/ len(det_tags))","8b41f1cf":"# Lexicon (or unigram tagger)\nunigram_tagger = nltk.UnigramTagger(train_set)\nunigram_tagger.evaluate(test_set)","13f26fb6":"# specify patterns for tagging\n# example from the NLTK book\npatterns = [\n    (r'.*ing$', 'VBG'),              # gerund\n    (r'.*ed$', 'VBD'),               # past tense\n    (r'.*es$', 'VBZ'),               # 3rd singular present\n    (r'.*ould$', 'MD'),              # modals\n    (r'.*\\'s$', 'NN$'),              # possessive nouns\n    (r'.*s$', 'NNS'),                # plural nouns\n    (r'^-?[0-9]+(.[0-9]+)?$', 'NUM'), # cardinal numbers\n    (r'.*', 'NOUN')                    # nouns\n]","10c7e88f":"regexp_tagger = nltk.RegexpTagger(patterns)\n# help(regexp_tagger)","80c18512":"regexp_tagger.evaluate(test_set)","f33ec657":"# rule based tagger\nrule_based_tagger = nltk.RegexpTagger(patterns)\n\n# lexicon backed up by the rule-based tagger\nlexicon_tagger = nltk.UnigramTagger(train_set, backoff=rule_based_tagger)\n\nlexicon_tagger.evaluate(test_set)","e6ebeb1a":"# tokens \ntokens = [pair[0] for pair in train_tagged_words]\ntokens[:10]","af6f3099":"# vocabulary\nV = set(tokens)\nprint(len(V))","e9fe50cb":"# number of tags\nT = set([pair[1] for pair in train_tagged_words])\nlen(T)","7ce36a79":"print(T)","88ae56ba":"# computing P(w\/t) and storing in T x V matrix\nt = len(T)\nv = len(V)\nw_given_t = np.zeros((t, v))","e89f3b09":"print(w_given_t)","7b8efc2a":"# compute word given tag: Emission Probability\ndef word_given_tag(word, tag, train_bag = train_tagged_words):\n    tag_list = [pair for pair in train_bag if pair[1]==tag]\n    count_tag = len(tag_list)\n    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n    count_w_given_tag = len(w_given_tag_list)\n    \n    return (count_w_given_tag, count_tag)","94fbb077":"# examples\n\n# large\nprint(\"\\n\", \"arrest\")\nprint(word_given_tag('arrest', 'PRON'))\nprint(word_given_tag('arrest', 'PRT'))\nprint(word_given_tag('arrest', 'X'))\nprint(word_given_tag('arrest', 'NOUN'), \"\\n\")\n\n# will\nprint(\"\\n\", \"will\")\nprint(word_given_tag('will', 'VERB'))\nprint(word_given_tag('will', '.'))\nprint(word_given_tag('will', 'ADP'))\n\n# book\nprint(\"\\n\", \"book\")\nprint(word_given_tag('book', 'NUM'))\nprint(word_given_tag('book', 'ADV'))","c3081658":"# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n\ndef t2_given_t1(t2, t1, train_bag = train_tagged_words):\n    tags = [pair[1] for pair in train_bag]\n    count_t1 = len([t for t in tags if t==t1])\n    count_t2_t1 = 0\n    for index in range(len(tags)-1):\n        if tags[index]==t1 and tags[index+1] == t2:\n            count_t2_t1 += 1\n    return (count_t2_t1, count_t1 ,(count_t2_t1\/count_t1)*100)","34f9cf49":"# examples\nprint(t2_given_t1(t2='PRON', t1='PRT'))\nprint(t2_given_t1('PRT', 'X'))\nprint(t2_given_t1('X', 'NOUN'))\nprint(t2_given_t1('ADJ', 'CONJ'))","20fb7a02":"#Please note P(tag|start) is same as P(tag|'.')\nprint(t2_given_t1('PRON', '.'))\nprint(t2_given_t1('PRT', '.'))\nprint(t2_given_t1('X', '.'))\nprint(t2_given_t1('ADJ', '.'))","37a19936":"# creating t x t transition matrix of tags\n# each column is t2, each row is t1\n# thus M(i, j) represents P(tj given ti)\n\ntags_matrix = np.zeros((len(T), len(T)), dtype='float32')\nfor i, t1 in enumerate(list(T)):\n    for j, t2 in enumerate(list(T)): \n        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]\/t2_given_t1(t2, t1)[1]","3107f2e9":"tags_matrix","a3407f47":"# convert the matrix to a df for better readability\ntags_df = pd.DataFrame(tags_matrix, columns = list(T), index=list(T))","0288df0c":"tags_df","44f719e7":"tags_df.loc['.', :]","f4d97177":"# heatmap of tags matrix\n# T(i, j) means P(tag j given tag i)\nplt.figure(figsize=(18, 12))\nsns.heatmap(tags_df)\nplt.show()","2e799e1d":"# frequent tags\n# filter the df to get P(t2, t1) > 0.5\ntags_frequent = tags_df[tags_df>0.5]\nplt.figure(figsize=(18, 12))\nsns.heatmap(tags_frequent)\nplt.show()","2697e601":"#Viterbi Algorithm\n\nlen(train_tagged_words)","92b71674":"# Viterbi Heuristic\ndef Viterbi(words, train_bag = train_tagged_words):\n    state = []\n    T = list(set([pair[1] for pair in train_bag]))\n    \n    for key, word in enumerate(words):\n        #initialise list of probability column for a given observation\n        p = [] \n        for tag in T:\n            if key == 0:\n                transition_p = tags_df.loc['.', tag]\n            else:\n                transition_p = tags_df.loc[state[-1], tag]\n                \n            # compute emission and state probabilities\n            emission_p = word_given_tag(words[key], tag)[0]\/word_given_tag(words[key], tag)[1]\n            state_probability = emission_p * transition_p    \n            p.append(state_probability)\n            \n        pmax = max(p)\n        # getting state for which probability is maximum\n        state_max = T[p.index(pmax)] \n        state.append(state_max)\n    return list(zip(words, state))","72ea826a":"# Running on entire test dataset would take more than 3-4hrs. \n# Let's test our Viterbi algorithm on a few sample sentences of test dataset\n\nrandom.seed(1234)\n\n# choose random 5 sents\nrndom = [random.randint(1,len(test_set)) for x in range(5)]\n\n# list of sents\ntest_run = [test_set[i] for i in rndom]\n\n# list of tagged words\ntest_run_base = [tup for sent in test_run for tup in sent]\n\n# list of untagged words\ntest_tagged_words = [tup[0] for sent in test_run for tup in sent]\ntest_run","38ffb953":"# tagging the test sentences\nstart = time.time()\ntagged_seq = Viterbi(test_tagged_words)\nend = time.time()\ndifference = end-start","9ba0def2":"print(\"Time taken in seconds: \", difference)\nprint(tagged_seq)\n#print(test_run_base)","9cb1b971":"# accuracy\ncheck = [i for i, j in zip(tagged_seq, test_run_base) if i == j] ","20f0e4a1":"accuracy = len(check)\/len(tagged_seq)","7f26c26c":"accuracy","f15d414e":"incorrect_tagged_cases = [[test_run_base[i-1],j] for i, j in enumerate(zip(tagged_seq, test_run_base)) if j[0]!=j[1]]","d011241f":"incorrect_tagged_cases","d755378b":"## Testing\nsentence_test = 'Twitter is the best networking social site. Man is a social animal. Data science is an emerging field. Data science jobs are high in demand.'\nwords = word_tokenize(sentence_test)\n\nstart = time.time()\ntagged_seq = Viterbi(words)\nend = time.time()\ndifference = end-start","ea6dd6f4":"print(tagged_seq)\nprint(difference)","8980b276":"## Testing\nsentence_test = 'Donald Trump is the current President of US. Before entering politics, he was a domineering businessman and television personality.'\nwords = word_tokenize(sentence_test)\n\nstart = time.time()\ntagged_seq = Viterbi(words)\nend = time.time()\ndifference = end-start","7b733aae":"print(tagged_seq)\nprint(difference)","23e8c48a":"Even a simple unigram tagger seems to perform fairly well.","33327e34":"### Solve the problem of unknown words","5d2b7526":"POS Tagging Algorithm ","76f337f5":"## 2. Exploratory Analysis Continued\n\nLet's now try observing some tag patterns using the fact the some tags are more likely to apper after certain other tags. For e.g. most nouns are usually followed by determiners DT (\"The\/DET constitution\/NOUN\"), adjectives usually precede a noun NOUN (\" A large\/AJ building\/NN\"), etc. \n\nTry answering the following questions:\n1. What fraction of adjectives ADJ are followed by a noun NOUN? \n2. What fraction of determiners DET are followed by a noun NOUN?\n","5c3dc791":"{'PRON', 'PRT', 'X', 'NOUN', 'ADJ', 'CONJ', 'DET', 'VERB', '.', 'ADP', 'NUM', 'ADV'}","41f57482":"4. Evaluating on Test Set\u00b6","b8ec7f45":"Lexicon and Rule-Based Models for POS Tagging","9609a9c6":"Now let's build a rule-based, or regular expression based tagger. In NLTK, the RegexpTagger() can be provided with handwritten regular expression patterns, as shown below.\n\nIn the example below, we specify regexes for gerunds and past tense verbs (as seen above), 3rd singular present verb (creates, moves, makes etc.), modal verbs MD (should, would, could), possesive nouns (partner's, bank's etc.), plural nouns (banks, institutions), cardinal numbers CD and finally, if none of the above rules are applicable to a word, we tag the most frequent tag NOUN.","8b4fc454":"# Rule-Based (Regular Expression) Tagger","3e16ec05":" Exploratory Analysis","2da2282a":"Thus, NOUN is the most common tag followed by VERB, ., ADP, X etc. we can read the exhaustive list of tags using the NLTK documentation as shown below.","2cc580dd":"# Lexicon (Unigram) Tagger","fbe86541":"### Data Preparation","5b505ae1":"Thus, we see that the probability of certain tags appearing after certain other tags is quite high, and this fact can be used to build quite efficient POS tagging algorithms. ","81937889":"VERB - verbs (all tenses and modes) NOUN - nouns (common and proper) PRON - pronouns ADJ - adjectives ADV - adverbs ADP - adpositions (prepositions and postpositions) CONJ - conjunctions DET - determiners NUM - cardinal numbers PRT - particles or other function words X - other: foreign words, typos, abbreviations . - punctuation","46f59f7b":"### Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm","631221f7":"Emission Probabilities\u00b6","177a01a1":"Combining Taggers\nLet's now try combining the taggers created above. We saw that the rule-based tagger by itself is quite ineffective since we've only written a handful of rules. However, if we could combine the lexicon and the rule-based tagger, we can potentially create a tagger much better than any of the individual ones.\n\nNLTK provides a convenient way to combine taggers using the 'backup' argument. In the following code, we create a regex tagger which is used as a backup tagger to the lexicon tagger, i.e. when the tagger is not able to tag using the lexicon (in case of a new word not in the vocabulary), it uses the rule-based tagger.\n\nAlso, note that the rule-based tagger itself is backed up by the tag 'NOUN'","55fbd70c":"### List down cases which were incorrectly tagged by original POS tagger and got corrected by your modifications","dbcdda55":"### Build the vanilla Viterbi based POS tagger","67027fa6":"## POS tagging using modified Viterbi","d9a8eb84":"We now have a list of about 95395 (word, tag) tuples in our training data. Let's now do some exploratory analyses","5ac381f0":"Transition Probabilities","68ba5079":"#### Evaluating tagging accuracy","f7cbbee5":"{'PRON', 'PRT', 'X', 'NOUN', 'ADJ', 'CONJ', 'DET', 'VERB', '.', 'ADP', 'NUM', 'ADV'}","7803696d":"\"\"\"\nInterface for converting POS tags from various treebanks\nto the universal tagset of Petrov, Das, & McDonald.\n\nThe tagset consists of the following 12 coarse tags:\n\nVERB - verbs (all tenses and modes)\nNOUN - nouns (common and proper)\nPRON - pronouns\nADJ - adjectives\nADV - adverbs\nADP - adpositions (prepositions and postpositions)\nCONJ - conjunctions\nDET - determiners\nNUM - cardinal numbers\nPRT - particles or other function words\nX - other: foreign words, typos, abbreviations\n. - punctuation\n\n@see: http:\/\/arxiv.org\/abs\/1104.2086 and http:\/\/code.google.com\/p\/universal-pos-tags\/\n\n\"\"\""}}