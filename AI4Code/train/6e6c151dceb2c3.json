{"cell_type":{"e20e3212":"code","a4d170f5":"code","9921a4e1":"code","9dc19e70":"code","cc68383f":"code","3e126d87":"code","e4dcd4f4":"code","fb2ce399":"code","5a460f3f":"code","8b2a5d52":"code","0ce46218":"code","e52b0c2a":"code","1ce160d9":"code","29f0362d":"code","64587566":"code","8facb75d":"code","fed7d25b":"code","ac12a81d":"code","c497174d":"code","12ffa8ce":"code","c4e49323":"code","465ed952":"code","0c685aaf":"code","6430d692":"code","e5887182":"code","e72bdb07":"code","e5c960cf":"code","82527fba":"code","cfd55d36":"code","faac3d73":"code","467a5e04":"code","6ffa382b":"markdown","39dc3a39":"markdown","a57b22a9":"markdown","f0051c5a":"markdown","92ddc7b6":"markdown"},"source":{"e20e3212":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a4d170f5":"import sys\n!cp ..\/input\/rapids\/rapids.0.15.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/\n\n# Rapids Imports\nimport cudf\nimport cupy # CuPy is an open-source array library accelerated with NVIDIA CUDA.","9921a4e1":"%%time\n#get the data\nimport numpy as np\nimport pandas as pd\ndtypes = {\n    \"row_id\": \"int64\",\n    \"timestamp\": \"int64\",\n    \"user_id\": \"int32\",\n    \"content_id\": \"int16\",\n    \"content_type_id\": \"boolean\",\n    \"task_container_id\": \"int16\",\n    \"user_answer\": \"int8\",\n    \"answered_correctly\": \"int8\",\n    \"prior_question_elapsed_time\": \"float32\", \n    \"prior_question_had_explanation\": \"boolean\"\n}\n\n\ntrain = pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/train.csv',dtype=dtypes)\ntrain = train.iloc[:1500000]\nprint('Loaded dataset!')","9dc19e70":"train.answered_correctly.value_counts()","cc68383f":"train.user_answer.value_counts()","3e126d87":"train.prior_question_elapsed_time.isna().sum()","e4dcd4f4":"train.isna().sum()","fb2ce399":"del_ids = ((train.user_answer == -1) & (train.answered_correctly == -1) & \n (train['prior_question_elapsed_time'].isna() == True) ).sum()","5a460f3f":"print('percentage Ids that have been erased {:.4f}%'.format(del_ids\/len(train)*100))","8b2a5d52":"print('Train length : ', len(train))\n#remove the -1 values in the data it affects the algorithms ability to learn\ntrain = train.drop(train[(train.user_answer == -1) & (train.answered_correctly == -1) & \n (train['prior_question_elapsed_time'].isna() == True)].index)\nprint(\"We erased  {:.3}% of all data.\".format(del_ids\/len(train)*100))\n","0ce46218":"train.shape  ","e52b0c2a":"train.user_answer.value_counts()","1ce160d9":"print('Train shape: ' ,train.shape)","29f0362d":"# Find Missing Data if any\ntotal = len(train)\n\nfor column in train.columns:\n    if train[column].isna().sum() != 0:\n        print(\"{} has: {:,} ({:.2}%) missing values.\".format(column, train[column].isna().sum(), \n                                                             (train[column].isna().sum()\/total)*100))","64587566":"# Fill in missing values \ntrain[\"prior_question_elapsed_time\"] = train[\"prior_question_elapsed_time\"].fillna(np.float32(train[\"prior_question_elapsed_time\"].mean()))\ntrain[\"prior_question_had_explanation\"] = train[\"prior_question_had_explanation\"].fillna(train[\"prior_question_had_explanation\"].value_counts().index[0])\ntrain[\"prior_question_had_explanation\"] = train[\"prior_question_had_explanation\"].astype(int)\ntrain[\"content_type_id\"] = train[\"content_type_id\"].astype(int)\ntrain.head()","8facb75d":"print(\"{} has: {:,} unique user ids,\\n{:.0f} is the average number of times user_id appears.\".format(train.columns[2], (train['user_id'].value_counts().unique()).sum(), \n                                                             train['user_id'].value_counts().mean()))\n#see if dropping users with < 15 appearences is better than dropping users with < mean_id\n#mean_id = train['user_id'].value_counts().mean()","fed7d25b":"# Select ids to erase\n# user_ids with less than 5 appearences where most-likely jerking off so, we remove them\nids_to_erase = train[\"user_id\"].value_counts().reset_index()[train[\"user_id\"].value_counts().reset_index()[\"user_id\"] < 15]\\\n                                                                                                                [\"index\"].values\n# Erase the ids\nnew_train = train[~train['user_id'].isin(ids_to_erase)]\n\nprint(\"We erased {} rows meaning {:.3}% of all data.\".format(len(train)-len(new_train), (1 - len(new_train)\/len(train))*100))\ndel ids_to_erase\n# del train\n","ac12a81d":"new_train.shape","c497174d":"del train","12ffa8ce":"new_train.columns","c4e49323":"new_train.head()","465ed952":"new_train.drop('row_id',1,inplace=True)\nnew_train.drop('user_answer',1,inplace=True)\nnew_train.drop('user_id',1,inplace=True)","0c685aaf":"y = new_train['answered_correctly']\nnew_train.drop('answered_correctly',1,inplace=True)","6430d692":"from sklearn.model_selection import train_test_split\n\n","e5887182":"x_train,x_test,y_train,y_test = train_test_split(new_train,y,test_size=0.3)","e72bdb07":"import lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score","e5c960cf":"from sklearn.model_selection import TimeSeriesSplit, KFold\n\nn_folds = 5\nfolds = TimeSeriesSplit(n_splits=n_folds)\nfolds = KFold(n_splits=5)","82527fba":"columns = x_train.columns\nsplits = folds.split(x_train,y_train)\n\ny_preds = np.zeros(x_test.shape[0])\ny_oof = np.zeros(x_train.shape[0])\n\nscore_auc = 0\n \nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = columns","cfd55d36":"params = {\n        'num_leaves': 64,\n        'min_child_weight' : 0.03,\n        'max_depth': -1,\n        'feature_fraction': 0.04,\n        'learning_rate': 0.006,\n        'min_data_in_leaf': 80,\n        'metric': 'auc',\n        'bagging_fraction': 0.33,\n        'boosting_type': 'gbdt',\n        'reg_alpha' : 0.3,\n        'reg_lambda' : 0.6,\n        'verbosity': -1,\n        'random_sate' : 0\n        }","faac3d73":"%%time\nfor fold_n, (train_index, valid_index) in enumerate(splits):\n    x_tr , x_val = x_train[columns].iloc[train_index], x_train[columns].iloc[valid_index]\n    y_tr, y_val = y_train.iloc[train_index], y_train.iloc[valid_index]\n    \n    dtrain = lgb.Dataset(x_tr, label=y_tr)\n    dvalid = lgb.Dataset(x_val, label=y_val)\n    \n    clf = lgb.train(params, dtrain,10000,valid_sets = [dtrain, dvalid], verbose_eval=200, early_stopping_rounds=100)\n    \n    feature_importances[f'fold_{fold_n + 1} '] = clf.feature_importance()\n    \n    y_pred_val = clf.predict(x_val)\n    y_oof[valid_index] = y_pred_val\n    print(f\"Fold {fold_n + 1} | AUC : {roc_auc_score(y_val,y_pred_val)}\")\n    \n    score_auc += roc_auc_score(y_val, y_pred_val) \/ n_folds\n    \n    y_preds += clf.predict(x_test)  \/ n_folds\n    \n    del x_tr, x_val, y_tr, y_val","467a5e04":"%%time\nprint('Loading riiid!')\nimport riiideducation\n#Create the env\nenv = riiideducation.make_env()\nprint('Loaded riiideducation!')\n\n#Create the iterator\niter_test = env.iter_test()\n\n#Iter and predict\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df['prior_question_elapsed_time'] = test_df['prior_question_elapsed_time'].fillna(np.float32(test_df['prior_question_elapsed_time'].mean()))\n    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(test_df['prior_question_had_explanation'].value_counts().index[0])\n    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].astype(int)\n    test_df['answered_correctly'] = clf.predict(np.array(test_df[['timestamp', 'content_id', 'content_type_id',\n       'task_container_id', 'prior_question_elapsed_time',\n       'prior_question_had_explanation']]))\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","6ffa382b":"## do the same for user_answer ","39dc3a39":"### -1 values are lectures (do not need them for trainning)\n### therefore drop them ","a57b22a9":"### how many nan values are there in prior_question_elapsed_time","f0051c5a":"# preprocessing the data","92ddc7b6":"## look at the values in answered_correctly\n### reasons will be explained later"}}