{"cell_type":{"fc1132cc":"code","25a62636":"code","f0afbf92":"code","b9abd2c2":"code","a7df241c":"code","ed4cf87d":"code","2954ce63":"code","dbe5f826":"code","3ae24203":"code","9d1b956e":"code","d7ac01dd":"code","31501432":"code","bef2f294":"code","a2b3be31":"code","b7a8b5b3":"code","a4e63a4e":"code","d461e589":"code","58c3bc64":"code","d710d265":"code","d728e2ac":"code","8a5953f9":"code","c1863119":"code","a0cdbb82":"code","072feb30":"code","6ad52a2a":"code","123f5e5e":"code","a889349e":"markdown","4d8290f4":"markdown","4a50c85b":"markdown","90a0490d":"markdown","92eee651":"markdown","eb976033":"markdown","8066992f":"markdown","9706ae72":"markdown","35934658":"markdown","7bbf8ebc":"markdown","251c3661":"markdown"},"source":{"fc1132cc":"#IMPORT THE LIBRARIES....\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\npd.set_option(\"display.max_columns\",None)","25a62636":"train = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/train.csv\")","f0afbf92":"train.isnull().sum()[train.isnull().sum() != 0]","b9abd2c2":"train_df = train.drop(['id', 'target'], axis = 1 )","a7df241c":"train_df.iloc[:, 0:,].describe()","ed4cf87d":"from tqdm import tqdm\nfeatures = train_df.columns.tolist()\nlen(features)\n\n\nfor col in tqdm(features):\n    train_df[col+'_bin'] = train_df[col].apply(lambda x: 1 if np.cbrt(x)>0 else 0)\n## The cube root of the variable x is 1 if it is greater than 0 and 0 if it is less than 0.\n\n\nprint(f\"train_df: {train_df.shape}\")\ntrain_df.head(20)","2954ce63":"!pip install pytorch-tabnet","dbe5f826":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom matplotlib import pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom pytorch_tabnet.tab_model import TabNetClassifier","3ae24203":"train_y = train['target']","9d1b956e":"train = pd.concat([train_df, train_y], axis =1)","d7ac01dd":"train_df.describe()","31501432":"target = 201    # 203\ubc88\uc9f8 \uceec\ub7fc\nif \"Set\" not in train.columns:\n    train[\"Set\"] = np.random.choice([\"train\", \"valid\"] ,p =[.8, .2], size=(train.shape[0],))\n\ntrain_indices = train[train.Set==\"train\"].index\nvalid_indices = train[train.Set==\"valid\"].index","bef2f294":"nunique = train.nunique()\ntypes = train.dtypes\n\ncategorical_columns = []\ncategorical_dims =  {}\nfor col in train.columns:\n    if types[col] == 'object' or nunique[col] < 200:\n        print(col, train[col].nunique())\n        l_enc = LabelEncoder()\n        train[col] = train[col].fillna(\"VV_likely\")\n        train[col] = l_enc.fit_transform(train[col].values)\n        categorical_columns.append(col)\n        categorical_dims[col] = len(l_enc.classes_)\n    else:\n        train.fillna(train.loc[train_indices, col].mean(), inplace=True)","a2b3be31":"# Contains dimension and idxs of categorical variable for categorical embedding.\nunused_feat = ['Set']\nfeatures = [ col for col in train.columns if col not in unused_feat+[target]] \ncat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\ncat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]","b7a8b5b3":"test = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/test.csv\")\ntest = test.drop(['id'], axis = 1)","a4e63a4e":"from tqdm import tqdm\nfeatures = test.columns.tolist()\nlen(features)\n\nfor col in tqdm(features):\n    test[col+'_bin'] = test[col].apply(lambda x: 1 if np.cbrt(x)>0 else 0)\n\nprint(f\"test_df: {test.shape}\")\ntest.head(20)","d461e589":"if \"Set\" not in test.columns:\n    test[\"Set\"] = np.random.choice([ \"test\"] ,p =[1.0], size=(test.shape[0],))\n\n\ntest_indices = test[test.Set==\"test\"].index","58c3bc64":"nunique = test.nunique()\ntypes = test.dtypes\n\ncategorical_columns = []\ncategorical_dims =  {}\nfor col in test.columns:\n    if types[col] == 'object' or nunique[col] < 200:\n        print(col, test[col].nunique())\n        l_enc = LabelEncoder()\n        test[col] = test[col].fillna(\"VV_likely\")\n        test[col] = l_enc.fit_transform(test[col].values)\n        categorical_columns.append(col)\n        categorical_dims[col] = len(l_enc.classes_)\n    else:\n        test.fillna(test.loc[test_indices, col].mean(), inplace=True)","d710d265":"# Contains dimension and idxs of categorical variable for categorical embedding.\nunused_feat = ['Set']\nfeatures = [ col for col in test.columns if col not in unused_feat] \ncat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\ncat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]","d728e2ac":"X_test = test[features].values[test_indices]","8a5953f9":"X_train = train[features].values[train_indices]\ny_train = train['target'].values[train_indices]\n\nX_valid = train[features].values[valid_indices]\ny_valid = train['target'].values[valid_indices]\n\nX_test = test[features].values[test_indices]","c1863119":"clf = TabNetClassifier(cat_idxs=cat_idxs,\n                       cat_dims=cat_dims,\n                       cat_emb_dim=10,\n                       optimizer_fn=torch.optim.Adam,\n                       optimizer_params=dict(lr=1e-2),\n                       scheduler_params={\"step_size\":50,\n                                         \"gamma\":0.9},\n                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n                       mask_type='sparsemax' # \"sparsemax\", entmax\n                      )","a0cdbb82":"max_epochs = 50\n\nclf.fit(\n    X_train=X_train, y_train=y_train,\n    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n    eval_name=['train', 'valid'],\n    eval_metric=['auc'],\n    max_epochs=max_epochs , patience=20,\n    batch_size=1024, virtual_batch_size=128,\n    num_workers=0,\n    weights=1,\n    drop_last=False,\n)","072feb30":"prediction = clf.predict(X_test)","6ad52a2a":"test = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/test.csv\")","123f5e5e":"submission = pd.DataFrame({'Id':test['id'], 'target':prediction})\nsubmission.to_csv(\".\/submission.csv\", index =False)\nsubmission.head(15)","a889349e":" **Reference thesis** : *TABNET: Attentive Interpretable Tabular Learning (2019)*","4d8290f4":"# Modeling","4a50c85b":"![\ucea1\ucc982.PNG](attachment:fc0d9b1d-d12c-4e3f-9c31-849a0bec028f.PNG)","90a0490d":"Create a derived variable in the same way as train.","92eee651":"The author says that TABNET is a high-performance interpretable deep learning structured data network.  \nTabNet uses a sequential attention mechanism that smoothly selects a sensible shape at each decision-making  \nstep and then aggregates the processed information to make a final predictive decision.  \nBy explicitly selecting the sparse feature, TabNet learns very efficiently at each decision-making step to utilize  \nthe relevant variables that give high-performance model results to the fully relevant variables in the model capacity.","eb976033":"![\ucea1\ucc98.PNG](attachment:dc306422-40a6-4d94-a02c-3a34618c7b5f.PNG)","8066992f":"# ABSTRACT","9706ae72":"# Import Data","35934658":"**Split data**","7bbf8ebc":"# EDA + Data processing","251c3661":"# Model architecture"}}