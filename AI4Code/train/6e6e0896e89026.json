{"cell_type":{"9a5d29c7":"code","529cab59":"code","f1fc3c2a":"code","fec260ad":"code","f5d0ec41":"code","21cbb148":"code","1f818e25":"code","a9b1f27f":"code","e12b6326":"code","fd623620":"code","65882369":"code","a6611464":"code","ef59b0f5":"code","21485403":"code","50b3a8e1":"code","a938d369":"code","3ccefe72":"code","3cbd9d56":"code","3ea67733":"code","fb506fe4":"code","57be7b2f":"code","553b4042":"code","46a07124":"code","9672dce9":"code","d31dc54d":"code","bf8177ad":"code","0138df1e":"code","0ca9570d":"code","6aaabe6e":"code","ac15c0f7":"code","3cdc8ea9":"code","c5dfd40a":"code","4552165b":"code","a6f84471":"code","c960532b":"code","dc777c91":"code","b6599c45":"code","84f53d89":"code","572d6342":"code","5b10b9fa":"code","8ce7f861":"code","9504c9cc":"code","8ddba54e":"code","064adf9a":"code","96281bcd":"code","60117b34":"code","e507d4ef":"code","e758e5ab":"code","c9ee3b65":"code","a3aa8bd9":"code","b976bf34":"code","88fc977a":"code","a7006775":"code","3f54d701":"code","de2d01a1":"code","2f79f9e3":"code","30881235":"code","c7e88e49":"code","4abf6ab8":"code","4b2c7991":"code","009cb665":"code","085d780a":"code","97f0a6c2":"code","1ec6b63f":"code","f57ddc7d":"code","e5440034":"code","bda6647d":"code","a974e395":"code","61358889":"code","c7968752":"code","5b563eec":"code","03673efc":"code","4ae4ead6":"code","290eeae5":"markdown","a54a94c6":"markdown","fa23f1b5":"markdown","4feeafcf":"markdown","66838a57":"markdown","214db6d0":"markdown","d8054be8":"markdown","584118d2":"markdown","3671a53c":"markdown","f3def8fd":"markdown","bda32d97":"markdown","b29e38da":"markdown","7a22f339":"markdown","1b6c622a":"markdown","8b2f2d00":"markdown","109f3a69":"markdown","f4aa7cbd":"markdown","eb2b2fb8":"markdown","e86ea575":"markdown","bd15591d":"markdown","beaf1371":"markdown","811ee109":"markdown","64939a55":"markdown","4910ad28":"markdown","79379ffc":"markdown","5c872818":"markdown","0f4e5213":"markdown","bbd2c9f7":"markdown","b8ceb2d1":"markdown","07b31b6d":"markdown","b814fa24":"markdown","36385744":"markdown","ccf7dd05":"markdown","734abb56":"markdown","8c569bb0":"markdown","da319171":"markdown","d7f0c707":"markdown","b19560f7":"markdown","6e86e93c":"markdown","4eb31683":"markdown","a7a0e22c":"markdown","2eeee5b2":"markdown","b278f1ce":"markdown","59ade021":"markdown","25a09466":"markdown","409e2884":"markdown","c9f002a2":"markdown","0fbc0b4d":"markdown","66c1829a":"markdown","b434fbf4":"markdown","89f826fd":"markdown","4ff592db":"markdown","9e799234":"markdown","dbb47909":"markdown","fec9553d":"markdown","5403a573":"markdown","427217a6":"markdown","f3f28ad6":"markdown","e651a2db":"markdown","a2a35e73":"markdown","47a64955":"markdown","b68f2bb6":"markdown","66a0ab2d":"markdown","c70b64ad":"markdown","2d28cd79":"markdown","cd7724a0":"markdown","50f5d50a":"markdown","3e8a1400":"markdown","e2ad2a3c":"markdown","80e6dead":"markdown","66940ddb":"markdown","fd4437e1":"markdown","de4896dd":"markdown","e9f18fc5":"markdown","dd7e5e93":"markdown","c47799cf":"markdown","ae44946d":"markdown","54e73b6c":"markdown","78b73c76":"markdown","dcb86a20":"markdown","fc49d2bd":"markdown","66f30936":"markdown","1257b1d6":"markdown","1b032444":"markdown","e161c6a5":"markdown","5df0809d":"markdown"},"source":{"9a5d29c7":"#\n# set seeds to ensure repeatability of results\nfrom numpy.random import seed\nseed(101)\nfrom tensorflow import set_random_seed\nset_random_seed(101)\n\nimport pandas as pd\nimport numpy as np\nimport os\n\nimport cv2\nimport tensorflow\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nimport itertools\nimport shutil\nimport pickle\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\n\n\nimport tensorflow\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, ZeroPadding2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.metrics import binary_accuracy\n\n\n# Don't Show Warning Messages\nimport warnings\nwarnings.filterwarnings('ignore')","529cab59":"#\nIMAGE_HEIGHT = 96\nIMAGE_WIDTH = 96\n\nNUM_HOLDOUT_IMAGES = 200\n\nNUM_EPOCHS = 10\nNUM_FOLDS = 5\n\nPADDING = 10\nBATCH_SIZE = 10\n\n\nNUM_FINAL_MODEL_EPOCHS = 10","f1fc3c2a":"#\npath_uninfected = '..\/input\/cell_images\/cell_images\/Uninfected'\npath_parasitized = '..\/input\/cell_images\/cell_images\/Parasitized'\n\nuninfected_list = os.listdir(path_uninfected)\nparasitized_list = os.listdir(path_parasitized)\n\nprint('Uninfected: ', len(uninfected_list))\nprint('Parasitized: ', len(parasitized_list))","fec260ad":"#\npath_uninfected = '..\/input\/cell_images\/cell_images\/Uninfected\/'\npath_parasitized = '..\/input\/cell_images\/cell_images\/Parasitized\/'\n\n\n# set up the canvas for the subplots\nplt.figure(figsize=(20,10))\n\n# Our subplot will contain 2 rows and 4 columns\n# plt.subplot(nrows, ncols, plot_number)\nplt.subplot(2,4,1)\n\n# plt.imread reads an image from a path and converts it into an array\n\n# starting from 1 makes the code easier to write\nfor i in range(1,9):\n    \n    plt.subplot(2,4,i)\n    \n    # get a random image\n    image = uninfected_list[i]\n    \n    # display the image\n    plt.imshow(plt.imread(path_uninfected + image))\n    \n    plt.xlabel('uninfected', fontsize=20)","f5d0ec41":"#\npath_uninfected = '..\/input\/cell_images\/cell_images\/Uninfected\/'\npath_parasitized = '..\/input\/cell_images\/cell_images\/Parasitized\/'\n\n\n# set up the canvas for the subplots\nplt.figure(figsize=(20,10))\n\n# Our subplot will contain 2 rows and 4 columns\n# plt.subplot(nrows, ncols, plot_number)\nplt.subplot(2,4,1)\n\n# plt.imread reads an image from a path and converts it into an array\n\n# starting from 1 makes the code easier to write\nfor i in range(1,9):\n    \n    plt.subplot(2,4,i)\n    \n    # get a random image\n    image = parasitized_list[i]\n    \n    # display the image\n    plt.imshow(plt.imread(path_parasitized + image))\n    \n    plt.xlabel('parasitized', fontsize=20)","21cbb148":"#\n# Check if any non image files are present in the folder\n\n# sample image name: C140P101ThinF_IMG_20151005_211735_cell_159.png\n\nfor item in uninfected_list:\n    # split the filename into a list\n    file_list = item.split('.')\n    \n    # check if the file extension is not png\n    if file_list[1] != 'png':\n        print('Uninfected folder: ', item)","1f818e25":"#\n# Check if any non image files are present in the folder\n\nfor item in parasitized_list:\n    # split the filename into a list\n    file_list = item.split('.')\n    \n    # check if the file extension is not png\n    if file_list[1] != 'png':\n        print('Parasitized folder: ',item)","a9b1f27f":"#\n# create the dataframe\ndf_uninfected = pd.DataFrame(uninfected_list, columns=['image_id'])\n\n# remove the non image file\ndf_uninfected = df_uninfected[df_uninfected['image_id'] != 'Thumbs.db']\n\n# add a target column\ndf_uninfected['target'] = 0\n\n\n# create the dataframe\ndf_parasitized = pd.DataFrame(parasitized_list, columns=['image_id'])\n\n# remove the non image file\ndf_parasitized = df_parasitized[df_parasitized['image_id'] != 'Thumbs.db']\n\n# add a target column\ndf_parasitized['target'] = 1\n\n#print(df_uninfected.shape)\n#print(df_parasitized.shape)\n\n# Combine the two dataframes\n\ndf_combined = pd.concat([df_uninfected, df_parasitized], axis=0).reset_index(drop=True)\n\n#df_combined.shape","e12b6326":"df_combined.head()","fd623620":"# Check the shape.\n# There should be 27558 rows.\n\ndf_combined.shape","65882369":"# Check if the image names are unique.\n# The output should be 27558\n\ndf_combined['image_id'].nunique()","a6611464":"#\ndef read_image_sizes(file_name):\n    \"\"\"\n    1. Get the shape of the image\n    2. Get the min and max pixel values in the image.\n    Getting pixel values will tell if any pre-processing has been done.\n    3. This info will be added to the original dataframe.\n    \"\"\"\n    \n    path_uninfected = '..\/input\/cell_images\/cell_images\/Uninfected\/'\n    path_parasitized = '..\/input\/cell_images\/cell_images\/Parasitized\/'\n     \n    if file_name in uninfected_list:\n        \n        path = path_uninfected\n        \n    else:\n        path = path_parasitized\n    \n    \n    image = cv2.imread(path + file_name)\n    max_pixel_val = image.max()\n    min_pixel_val = image.min()\n    img_format = file_name.split('.')[1]\n    output = [image.shape[0], image.shape[1], image.shape[2], max_pixel_val, min_pixel_val, img_format]\n    return output\n\nm = np.stack(df_combined['image_id'].apply(read_image_sizes))\ndf = pd.DataFrame(m,columns=['w','h','c','max_pixel_val','min_pixel_val', 'image_format'])\n\ndf_combined = pd.concat([df_combined,df],axis=1, sort=False)\n\ndf_combined.head(10)","ef59b0f5":"# Check if all images have 3 channels\ndf_combined['c'].value_counts()","21485403":"# Check if all images are in png format\ndf_combined['image_format'].value_counts()","50b3a8e1":"# Check for all black images\nlen(df_combined[(df_combined['max_pixel_val'] == 0) & (df_combined['max_pixel_val'] == 0)])","a938d369":"# Check for all white images\nlen(df_combined[(df_combined['max_pixel_val'] == 255) & (df_combined['max_pixel_val'] == 255)])","3ccefe72":"#\npath_uninfected = '..\/input\/cell_images\/cell_images\/Uninfected\/'\npath_parasitized = '..\/input\/cell_images\/cell_images\/Parasitized\/'\n\n\n# set up the canvas for the subplots\nplt.figure(figsize=(20,10))\n\n# Our subplot will contain 2 rows and 4 columns\n# plt.subplot(nrows, ncols, plot_number)\nplt.subplot(2,4,1)\n\n# plt.imread reads an image from a path and converts it into an array\n\n# starting from 1 makes the code easier to write\nfor i in range(1,9):\n    \n    plt.subplot(2,4,i)\n    \n    # get a random image\n    image_list = list(df_uninfected['image_id'].sample(1))\n    image = image_list[0]\n    \n    # display the image\n    plt.imshow(plt.imread(path_uninfected + image))\n    \n    plt.xlabel('uninfected', fontsize=20)\n    \n","3cbd9d56":"#\npath_uninfected = '..\/input\/cell_images\/cell_images\/Uninfected\/'\npath_parasitized = '..\/input\/cell_images\/cell_images\/Parasitized\/'\n\n\n# set up the canvas for the subplots\nplt.figure(figsize=(20,10))\n\n# Our subplot will contain 2 rows and 4 columns\n# plt.subplot(nrows, ncols, plot_number)\nplt.subplot(2,4,1)\n\n# plt.imread reads an image from a path and converts it into an array\n\n# starting from 1 makes the code easier to write\nfor i in range(1,9):\n    \n    plt.subplot(2,4,i)\n    \n    # get a random image\n    image_list = list(df_parasitized['image_id'].sample(1))\n    image = image_list[0]\n    \n    # display the image\n    plt.imshow(plt.imread(path_parasitized + image))\n    \n    plt.xlabel('parasitized', fontsize=20)","3ea67733":"#\n# shuffle\ndf_combined = shuffle(df_combined, random_state=101)\n\n# create a holdout set with 200 samples\ndf_holdout = df_combined.sample(NUM_HOLDOUT_IMAGES, random_state=101)\n\n# create a list of holdout images\nholdout_images_list = list(df_holdout['image_id'])\n\n\n# Select only rows that are not part of the holdout set.\n# Note the use of ~ to execute 'not in'.\ndf_data = df_combined[~df_combined['image_id'].isin(holdout_images_list)]\n","fb506fe4":"df_holdout.head()","57be7b2f":"df_data.head()","553b4042":"# Check the shapes.\n# The ouput should be:\n# (200, 8)\n# (27358, 8)\n\nprint(df_holdout.shape)\nprint(df_data.shape)","46a07124":"# Check the target distribution in the holdout set.\n# 0 = uninfected\n# 1 = parasitized\n\ndf_holdout['target'].value_counts()","9672dce9":"\nbase_dir = 'base_dir'\nos.mkdir(base_dir)\n\n\n#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n\n# now we create 2 folders inside 'base_dir':\n\n# train\n    # a_uninfected\n    # b_parasitized\n\n# val\n    # a_uninfected\n    # b_parasitized\n\n\n# create a path to 'base_dir' to which we will join the names of the new folders\n# train_dir\ntrain_dir = os.path.join(base_dir, 'train_dir')\nos.mkdir(train_dir)\n\n# val_dir\nval_dir = os.path.join(base_dir, 'val_dir')\nos.mkdir(val_dir)\n\n\n# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\n# Inside each folder we create seperate folders for each class\n\n# create new folders inside train_dir\na_uninfected = os.path.join(train_dir, 'a_uninfected')\nos.mkdir(a_uninfected)\nb_parasitized = os.path.join(train_dir, 'b_parasitized')\nos.mkdir(b_parasitized)\n\n\n# create new folders inside val_dir\na_uninfected = os.path.join(val_dir, 'a_uninfected')\nos.mkdir(a_uninfected)\nb_parasitized = os.path.join(val_dir, 'b_parasitized')\nos.mkdir(b_parasitized)","d31dc54d":"# check if base_dir has been created\n!ls","bf8177ad":"# see what's inside base_dir\nos.listdir('base_dir')","0138df1e":"# select the column that we will use for stratification\ny = df_data['target']\n\ndf_train, df_val = train_test_split(df_data, test_size=0.15, random_state=101, stratify=y)\n\nprint(df_train.shape)\nprint(df_val.shape)","0ca9570d":"# Check the target distribution of the val set.\n# The target should be approx balanced.\n\ndf_val['target'].value_counts()","6aaabe6e":"# Set the image_id as the index in df_data\ndf_data.set_index('image_id', inplace=True)","ac15c0f7":"df_data.head()","3cdc8ea9":"#\npath_uninfected = '..\/input\/cell_images\/cell_images\/Uninfected'\npath_parasitized = '..\/input\/cell_images\/cell_images\/Parasitized'\n\n# Get a list of images in each of the two folders\nfolder_1 = os.listdir(path_uninfected)\nfolder_2 = os.listdir(path_parasitized)\n\n# Get a list of train and val images\ntrain_list = list(df_train['image_id'])\nval_list = list(df_val['image_id'])\n\n\n\n# Transfer the train images\n\nfor image in train_list:\n    \n    fname = image\n    target = df_data.loc[image,'target']\n    \n    if target == 0:\n        label = 'a_uninfected'\n    else:\n        label = 'b_parasitized'\n    \n    if fname in folder_1:\n        # source path to image\n        src = os.path.join(path_uninfected, fname)\n        # destination path to image\n        dst = os.path.join(train_dir, label, fname)\n        \n        image = cv2.imread(src)\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n        # save the image at the destination\n        cv2.imwrite(dst, image)\n        #shutil.copyfile(src, dst)\n        \n    if fname in folder_2:\n        # source path to image\n        src = os.path.join(path_parasitized, fname)\n        # destination path to image\n        dst = os.path.join(train_dir, label, fname)\n        \n        image = cv2.imread(src)\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n        # save the image at the destination\n        cv2.imwrite(dst, image)\n        \n        # copy the image from the source to the destination\n        #shutil.copyfile(src, dst)\n        \n\n\n# Transfer the val images\n\nfor image in val_list:\n    \n    fname = image\n    target = df_data.loc[image,'target']\n    \n    \n    if target == 0:\n        label = 'a_uninfected'\n    else:\n        label = 'b_parasitized'\n        \n    \n    if fname in folder_1:\n        # source path to image\n        src = os.path.join(path_uninfected, fname)\n        # destination path to image\n        dst = os.path.join(val_dir, label, fname)\n        \n        image = cv2.imread(src)\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n        # save the image at the destination\n        cv2.imwrite(dst, image)\n        \n        # copy the image from the source to the destination\n        #shutil.copyfile(src, dst)\n\n        \n    if fname in folder_2:\n        # source path to image\n        src = os.path.join(path_parasitized, fname)\n        # destination path to image\n        dst = os.path.join(val_dir, label, fname)\n        \n        image = cv2.imread(src)\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n        # save the image at the destination\n        cv2.imwrite(dst, image)\n        \n        # copy the image from the source to the destination\n        #shutil.copyfile(src, dst)","c5dfd40a":"# Print the number of images in each folder\n\n# train\nprint(len(os.listdir('base_dir\/train_dir\/a_uninfected')))\nprint(len(os.listdir('base_dir\/train_dir\/b_parasitized\/')))\n\n# val\nprint(len(os.listdir('base_dir\/val_dir\/a_uninfected')))\nprint(len(os.listdir('base_dir\/val_dir\/b_parasitized\/')))","4552165b":"train_path = 'base_dir\/train_dir'\nvalid_path = 'base_dir\/val_dir'\n\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = BATCH_SIZE\nval_batch_size = BATCH_SIZE\n\n\ntrain_steps = np.ceil(num_train_samples \/ train_batch_size)\nval_steps = np.ceil(num_val_samples \/ val_batch_size)","a6f84471":"# Note that here we are normalizing the images inside the generator.\n# If you wanted to add some data augmentation you could do it here.\ndatagen = ImageDataGenerator(rescale=1.0\/255)\n\ntrain_gen = datagen.flow_from_directory(train_path,\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\n                                        batch_size=train_batch_size,\n                                        class_mode='categorical')\n\nval_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\n                                        batch_size=val_batch_size,\n                                        class_mode='categorical')\n\n# Note: shuffle=False causes the test dataset to not be shuffled\ntest_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\n                                        batch_size=val_batch_size,\n                                        class_mode='categorical',\n                                        shuffle=False)","c960532b":"# source: https:\/\/www.kaggle.com\/fmarazzi\/baseline-keras-cnn-roc-fast-10min-0-925-lb\n\nkernel_size = (3,3)\npool_size= (2,2)\nfirst_filters = 32\nsecond_filters = 64\nthird_filters = 128\n\ndropout_conv = 0.3\ndropout_dense = 0.3\n\n\nmodel = Sequential()\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu', \n                 input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n\nmodel.add(ZeroPadding2D(padding=(PADDING, PADDING), data_format=None))\n\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size)) \nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(dropout_dense))\nmodel.add(Dense(2, activation = \"softmax\"))\n\nmodel.summary()","dc777c91":"model.compile(Adam(lr=0.0001, decay=1e-6), loss='binary_crossentropy', \n              metrics=['accuracy'])","b6599c45":"filepath = \"model.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n                             save_best_only=True, mode='max')\n                              \n                              \ncallbacks_list = [checkpoint]\n\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n                            validation_data=val_gen,\n                            validation_steps=val_steps,\n                            epochs=NUM_EPOCHS, verbose=1,\n                           callbacks=callbacks_list)","84f53d89":"# get the metric names so we can use evaulate_generator\nmodel.metrics_names","572d6342":"# Here the best epoch will be used.\n\nmodel.load_weights('model.h5')\n\nval_loss, val_acc = \\\nmodel.evaluate_generator(test_gen, \n                        steps=val_steps)\n\nprint('val_loss:', val_loss)\nprint('val_acc:', val_acc)","5b10b9fa":"#\n# display the loss and accuracy curves\n\nimport matplotlib.pyplot as plt\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()","8ce7f861":"# Get the labels of the test images.\n\ntest_labels = test_gen.classes","9504c9cc":"# We need these to plot the confusion matrix.\ntest_labels","8ddba54e":"# Print the label associated with each class\ntest_gen.class_indices","064adf9a":"# make a prediction\npredictions = model.predict_generator(test_gen, steps=val_steps, verbose=1)","96281bcd":"# If you wanted to get the image_id's to match them to predictions this\n# is how to do it.\n\n# test_gen.filenames","60117b34":"# check the number of predictions\npredictions.shape","e507d4ef":"#\n# Source: Scikit Learn website\n# http:\/\/scikit-learn.org\/stable\/auto_examples\/\n# model_selection\/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n# selection-plot-confusion-matrix-py\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","e758e5ab":"test_labels.shape","c9ee3b65":"# argmax returns the index of the max value in a row\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))","a3aa8bd9":"test_gen.class_indices","b976bf34":"# Define the labels of the class indices. These need to match the \n# order shown above.\ncm_plot_labels = ['uninfected', 'parasitized']\n\nplot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')","88fc977a":"# Get the filenames, labels and associated predictions\n\n# This outputs the sequence in which the generator processed the test images\ntest_filenames = test_gen.filenames\n\n# Get the true labels\ny_true = test_gen.classes\n\n# Get the predicted labels\ny_pred = predictions.argmax(axis=1)","a7006775":"# Generate a classification report\n\nreport = classification_report(y_true, y_pred, target_names=cm_plot_labels)\n\nprint(report)","3f54d701":"#\n# put the val image_id, labels and predictions into a dataframe\n\nval_pred_dict = {\n    'image_id': test_gen.filenames,\n    'val_labels': test_gen.classes,\n    'val_preds': predictions.argmax(axis=1)\n}\n\ndf_val_preds = pd.DataFrame(val_pred_dict)\n\n\n# Adjust the file names\n\n# sample image name: a_uninfected\/C100P61ThinF_IMG_20150918_144104_...\n# we want just this part: C100P61ThinF_IMG_20150918_144104_...\n\ndef adjust_file_names(x):\n    # split into a list based on '\/'\n    fname = x.split('\/')\n    # chose the second item in the list which is the image name\n    fname = fname[1]\n    \n    return fname\n\ndf_val_preds['image_id'] = df_val_preds['image_id'].apply(adjust_file_names)\n\n\n# savedf_val_preds so we can analyze the results later\npickle.dump(df_val_preds,open('df_val_preds.pickle','wb'))\n\n# code to load the dataframe\n# df_val_preds = pickle.load(open('df_val_preds','rb'))\n\n\n#df_val_preds.head()","de2d01a1":"df_val_preds.head()","2f79f9e3":"# filter out those rows where the model made correct predictions\ndf_correct = df_val_preds[df_val_preds['val_labels'] == df_val_preds['val_preds']]\n\n# filter out those rows where the model made wrong predictions\ndf_wrong = df_val_preds[df_val_preds['val_labels'] != df_val_preds['val_preds']]\n\nprint(df_correct.shape)\nprint(df_wrong.shape)","30881235":"df_correct.head()","c7e88e49":"#\ndf_1 = df_correct[df_correct['val_labels'] == 1]\n\n# we see diffrent images each time the code is run.\n\npath_uninfected = '..\/input\/cell_images\/cell_images\/Uninfected\/'\npath_parasitized = '..\/input\/cell_images\/cell_images\/Parasitized\/'\n\n\n# set up the canvas for the subplots\nplt.figure(figsize=(20,10))\n\n# Our subplot will contain 2 rows and 4 columns\n# plt.subplot(nrows, ncols, plot_number)\nplt.subplot(2,4,1)\n\n# plt.imread reads an image from a path and converts it into an array\n\n# starting from 1 makes the code easier to write\nfor i in range(1,9):\n    \n    plt.subplot(2,4,i)\n    \n    # get a random image\n    image_list = list(df_1['image_id'].sample(1))\n    image = image_list[0]\n    \n    # display the image\n    plt.imshow(plt.imread(path_parasitized + image))\n    \n    plt.tight_layout()\n    \n    plt.xlabel('true: 1, pred: 1', fontsize=20)","4abf6ab8":"#\ndf_0 = df_correct[df_correct['val_labels'] == 0]\n\n# we see diffrent images each time the code is run.\n\npath_uninfected = '..\/input\/cell_images\/cell_images\/Uninfected\/'\npath_parasitized = '..\/input\/cell_images\/cell_images\/Parasitized\/'\n\n\n# set up the canvas for the subplots\nplt.figure(figsize=(20,10))\n\n# Our subplot will contain 2 rows and 4 columns\n# plt.subplot(nrows, ncols, plot_number)\nplt.subplot(2,4,1)\n\n# plt.imread reads an image from a path and converts it into an array\n\n# starting from 1 makes the code easier to write\nfor i in range(1,9):\n    \n    plt.subplot(2,4,i)\n    \n    # get a random image\n    image_list = list(df_0['image_id'].sample(1))\n    image = image_list[0]\n    \n    # display the image\n    plt.imshow(plt.imread(path_uninfected + image))\n    \n    plt.xlabel('true: 0, pred: 0', fontsize=20)\n    ","4b2c7991":"#\ndf_1 = df_wrong[df_wrong['val_labels'] == 1]\n\n\n# Note that this chooses random images. Therefore,\n# we see diffrent images each time the code is run.\n\npath_uninfected = '..\/input\/cell_images\/cell_images\/Uninfected\/'\npath_parasitized = '..\/input\/cell_images\/cell_images\/Parasitized\/'\n\n\n# set up the canvas for the subplots\nplt.figure(figsize=(20,10))\n\n# Our subplot will contain 2 rows and 4 columns\n# plt.subplot(nrows, ncols, plot_number)\nplt.subplot(2,4,1)\n\n# plt.imread reads an image from a path and converts it into an array\n\n# starting from 1 makes the code easier to write\nfor i in range(1,9):\n    \n    plt.subplot(2,4,i)\n    \n    # get a random image\n    image_list = list(df_1['image_id'].sample(1))\n    image = image_list[0]\n    \n    # display the image\n    plt.imshow(plt.imread(path_parasitized + image))\n    \n    plt.tight_layout()\n    \n    plt.xlabel('true: 1, pred: 0', fontsize=20)\n    \n","009cb665":"#\ndf_0 = df_wrong[df_wrong['val_labels'] == 0]\n\n# Note that this chooses random images. Therefore,\n# we see diffrent images each time the code is run.\n\npath_uninfected = '..\/input\/cell_images\/cell_images\/Uninfected\/'\npath_parasitized = '..\/input\/cell_images\/cell_images\/Parasitized\/'\n\n\n# set up the canvas for the subplots\nplt.figure(figsize=(20,10))\n\n# Our subplot will contain 2 rows and 4 columns\n# plt.subplot(nrows, ncols, plot_number)\nplt.subplot(2,4,1)\n\n# plt.imread reads an image from a path and converts it into an array\n\n# starting from 1 makes the code easier to write\nfor i in range(1,9):\n    \n    plt.subplot(2,4,i)\n    \n    # get a random image\n    image_list = list(df_0['image_id'].sample(1))\n    image = image_list[0]\n    \n    # display the image\n    plt.imshow(plt.imread(path_uninfected + image))\n    \n    plt.xlabel('true: 0, pred: 1', fontsize=20)\n    ","085d780a":"#\n# ==============================\n# Create the 5 Folds\n# ==============================\n\n# shuffle df_combined and change the name to df_data\ndf_data = shuffle(df_combined.copy())\n\n# train_test_split\ny = df_data['target']\n\n# initialize kfold\nkf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=101)\n\n# define y for stratification\ny = df_data['target']\n\n# Note:\n# Each fold is a tuple ([train_index_values], [val_index_values])\n# fold_0, fold_1, fold_2, fold_3, fold_5 = kf.split(df_train, y)\n\n# Put the folds into a list. This is a list of tuples.\n# y was set above.\nfold_list = list(kf.split(df_data, y))\n\n\n# ==============================\n# Loop Through the Folds\n# ==============================\n\n# create a list to store the predictions\nval_pred_list = []\n\n# create a list to store the scores\nval_acc_list = []\nval_loss_list = []\nval_auc_list = []\n\n\nfor i, fold in enumerate(fold_list):\n\n    # Delete the image data directory we created to prevent a Kaggle error.\n    # Kaggle allows a max of 500 files to be saved.\n    \n    if os.path.isdir('base_dir') == True: # return true if the directory exists\n    \n        shutil.rmtree('base_dir')\n        \n        \n    \n    # set df_data\n    df_data = df_combined.copy()\n    \n    print('=== Fold_' + str(i) + ' ===')\n    print('\\n')\n\n    # map the train and val index values to dataframe rows\n    df_train = df_data[df_data.index.isin(fold[0])]\n    df_val = df_data[df_data.index.isin(fold[1])]\n    \n\n\n\n    # ==============================\n    # Create a Directory Structure\n    # ==============================\n\n    # Create a new directory\n    base_dir = 'base_dir'\n    os.mkdir(base_dir)\n\n\n    #[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n\n    # now we create 2 folders inside 'base_dir':\n\n    # train\n        # a_uninfected\n        # b_parasitized\n\n    # val\n        # a_uninfected\n        # b_parasitized\n\n\n    # create a path to 'base_dir' to which we will join the names of the new folders\n    # train_dir\n    train_dir = os.path.join(base_dir, 'train_dir')\n    os.mkdir(train_dir)\n\n    # val_dir\n    val_dir = os.path.join(base_dir, 'val_dir')\n    os.mkdir(val_dir)\n\n\n    # [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\n    # Inside each folder we create seperate folders for each class\n\n    # create new folders inside train_dir\n    a_uninfected = os.path.join(train_dir, 'a_uninfected')\n    os.mkdir(a_uninfected)\n    b_parasitized = os.path.join(train_dir, 'b_parasitized')\n    os.mkdir(b_parasitized)\n\n\n    # create new folders inside val_dir\n    a_uninfected = os.path.join(val_dir, 'a_uninfected')\n    os.mkdir(a_uninfected)\n    b_parasitized = os.path.join(val_dir, 'b_parasitized')\n    os.mkdir(b_parasitized)\n\n\n\n    # =================================\n    # Transfer the Images into Folders\n    # =================================\n\n    # Set the image_id as the index in df_data\n    df_data.set_index('image_id', inplace=True)\n\n    # Get a list of images in each of the two folders\n\n    path_uninfected = '..\/input\/cell_images\/cell_images\/Uninfected'\n    path_parasitized = '..\/input\/cell_images\/cell_images\/Parasitized'\n\n    folder_1 = os.listdir(path_uninfected)\n    folder_2 = os.listdir(path_parasitized)\n\n    # Get a list of train and val images\n    train_list = list(df_train['image_id'])\n    val_list = list(df_val['image_id'])\n\n\n\n    # Transfer the train images\n\n    for image in train_list:\n\n        fname = image\n        target = df_data.loc[image,'target']\n\n        if target == 0:\n            label = 'a_uninfected'\n        else:\n            label = 'b_parasitized'\n\n        if fname in folder_1:\n            # source path to image\n            src = os.path.join(path_uninfected, fname)\n            # destination path to image\n            dst = os.path.join(train_dir, label, fname)\n\n            image = cv2.imread(src)\n            image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n            # save the image at the destination\n            cv2.imwrite(dst, image)\n            #shutil.copyfile(src, dst)\n\n        if fname in folder_2:\n            # source path to image\n            src = os.path.join(path_parasitized, fname)\n            # destination path to image\n            dst = os.path.join(train_dir, label, fname)\n\n            image = cv2.imread(src)\n            image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n            # save the image at the destination\n            cv2.imwrite(dst, image)\n\n            # copy the image from the source to the destination\n            #shutil.copyfile(src, dst)\n\n\n\n    # Transfer the val images\n\n    for image in val_list:\n\n        fname = image\n        target = df_data.loc[image,'target']\n\n\n        if target == 0:\n            label = 'a_uninfected'\n        else:\n            label = 'b_parasitized'\n\n\n        if fname in folder_1:\n            # source path to image\n            src = os.path.join(path_uninfected, fname)\n            # destination path to image\n            dst = os.path.join(val_dir, label, fname)\n\n            image = cv2.imread(src)\n            image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n            # save the image at the destination\n            cv2.imwrite(dst, image)\n\n            # copy the image from the source to the destination\n            #shutil.copyfile(src, dst)\n\n        if fname in folder_2:\n            # source path to image\n            src = os.path.join(path_parasitized, fname)\n            # destination path to image\n            dst = os.path.join(val_dir, label, fname)\n\n            image = cv2.imread(src)\n            image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n            # save the image at the destination\n            cv2.imwrite(dst, image)\n\n            # copy the image from the source to the destination\n            #shutil.copyfile(src, dst)\n\n    # Print the number of images in each folder\n\n    # train\n    #print(len(os.listdir('base_dir\/train_dir\/a_uninfected')))\n    #print(len(os.listdir('base_dir\/train_dir\/b_parasitized\/')))\n\n    # val\n    #print(len(os.listdir('base_dir\/val_dir\/a_uninfected')))\n    #print(len(os.listdir('base_dir\/val_dir\/b_parasitized\/')))\n    #print('\\n')\n\n\n    # ==============================\n    # Set Up the Generators\n    # ==============================\n\n    train_path = 'base_dir\/train_dir'\n    valid_path = 'base_dir\/val_dir'\n\n    num_train_samples = len(df_train)\n    num_val_samples = len(df_val)\n    train_batch_size = BATCH_SIZE\n    val_batch_size = BATCH_SIZE\n\n\n    train_steps = np.ceil(num_train_samples \/ train_batch_size)\n    val_steps = np.ceil(num_val_samples \/ val_batch_size)\n\n\n    datagen = ImageDataGenerator(rescale=1.0\/255)\n\n    train_gen = datagen.flow_from_directory(train_path,\n                                            target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\n                                            batch_size=train_batch_size,\n                                            class_mode='categorical')\n\n    val_gen = datagen.flow_from_directory(valid_path,\n                                            target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\n                                            batch_size=val_batch_size,\n                                            class_mode='categorical')\n\n    # Note: shuffle=False causes the test dataset to not be shuffled\n    test_gen = datagen.flow_from_directory(valid_path,\n                                            target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\n                                            batch_size=val_batch_size,\n                                            class_mode='categorical',\n                                            shuffle=False)\n    \n    print('\\n')\n\n    # ==============================\n    # Set Up the Model Architecture\n    # ==============================\n\n\n\n    kernel_size = (3,3)\n    pool_size= (2,2)\n    first_filters = 32\n    second_filters = 64\n    third_filters = 128\n\n    dropout_conv = 0.3\n    dropout_dense = 0.3\n\n\n    model = Sequential()\n    model.add(Conv2D(first_filters, kernel_size, activation = 'relu', \n                     input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n    \n    model.add(ZeroPadding2D(padding=(PADDING, PADDING), data_format=None))\n    \n    model.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\n    model.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\n    model.add(MaxPooling2D(pool_size = pool_size)) \n    model.add(Dropout(dropout_conv))\n\n    model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n    model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n    model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n    model.add(MaxPooling2D(pool_size = pool_size))\n    model.add(Dropout(dropout_conv))\n\n    model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n    model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n    model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n    model.add(MaxPooling2D(pool_size = pool_size))\n    model.add(Dropout(dropout_conv))\n\n    model.add(Flatten())\n    model.add(Dense(256, activation = \"relu\"))\n    model.add(Dropout(dropout_dense))\n    model.add(Dense(2, activation = \"softmax\"))\n\n    #model.summary()\n\n\n\n    # ==============================\n    # Train the Model\n    # ==============================\n\n\n    model.compile(Adam(lr=0.0001, decay=1e-6), loss='binary_crossentropy', \n                  metrics=['accuracy'])\n\n    filepath = \"model.h5\"\n    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n                                 save_best_only=True, mode='max')\n\n    reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, \n                                       verbose=1, mode='max', min_lr=0.00001)\n\n\n    callbacks_list = [checkpoint, reduce_lr]\n\n    history = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n                                validation_data=val_gen,\n                                validation_steps=val_steps,\n                                epochs=NUM_EPOCHS, verbose=1,\n                               callbacks=callbacks_list)\n\n\n\n    # ==================================\n    # Evaluate the Model on the Val Set\n    # ==================================\n\n    model.load_weights('model.h5')\n\n    val_loss, val_acc = \\\n    model.evaluate_generator(test_gen, \n                            steps=val_steps)\n    \n    # append the acc score val_scores_list\n    val_acc_list.append(val_acc)\n    val_loss_list.append(val_loss)\n    \n    \n    # ==================================\n    # Calculate the AUC Score\n    # ==================================\n\n    test_labels = test_gen.classes\n\n    # make a prediction\n    predictions = model.predict_generator(test_gen, steps=val_steps, verbose=1)\n    \n    # append the predictions to a list\n    val_pred_list.append(predictions)\n    \n    val_auc = roc_auc_score(test_labels, predictions.argmax(axis=1))\n    \n    val_auc_list.append(val_auc)\n    \n    \n    \n    # ==================================\n    # Print the Fold Scores\n    # ==================================\n    \n    \n    #print('\\n')\n    #print('Fold_' + str(i) + ' scores:\\n')\n    #print('val_loss:', val_loss)\n    #print('val_acc:', val_acc)\n    #print('val_auc:', val_auc)\n    #print('\\n')\n\n    \n    \n    \n# Calc the average score over the 5 folds\navg_acc = sum(val_acc_list)\/len(val_acc_list)\navg_loss = sum(val_loss_list)\/len(val_loss_list)\navg_auc = sum(val_auc_list)\/len(val_auc_list)\n\n#print('\\n')\n#print('Average for of all 5 folds:\\n')\n#print('Average Accuracy: ', avg_acc)\n#print('Average Loss: ', avg_loss)\n#print('Average AUC: ', avg_auc)","97f0a6c2":"#\n# Print the scores\n\nprint('Val Acc')\nfor item in val_acc_list:\n    print(item)\n    \nprint('\\n')\n\nprint('Val Loss')\nfor item in val_loss_list:\n    print(item)\n    \nprint('\\n')\n\nprint('Val AUC')\nfor item in val_auc_list:\n    print(item)\n\n    \n    \n# Calc the average score over the 5 folds\navg_acc = sum(val_acc_list)\/len(val_acc_list)\navg_loss = sum(val_loss_list)\/len(val_loss_list)\navg_auc = sum(val_auc_list)\/len(val_auc_list)\n\nprint('\\n')\nprint('Average for of all 5 folds:\\n')\nprint('Average Accuracy: ', avg_acc)\nprint('Average Loss: ', avg_loss)\nprint('Average AUC: ', avg_auc)","1ec6b63f":"#\n# Delete the image data directory we created to prevent a Kaggle error.\n# Kaggle allows a max of 500 files to be saved.\n\nif os.path.isdir('base_dir') == True: # return true if the directory exists\n    \n        shutil.rmtree('base_dir')\n\n\n\n# ==============================\n# Set df_train\n# ==============================\n\n# This variable was set above. Just setting it here again for clarity.\nholdout_images_list = list(df_holdout['image_id'])\n\n# Select only rows that are not part of the holdout set.\n# Note the use of ~ to execute 'not in'.\ndf_data = df_combined[~df_combined['image_id'].isin(holdout_images_list)]\n\ndf_train = df_data.copy()\ndf_val = df_holdout.copy()\n\n\n\n# ==============================\n# Create a Directory Structure\n# ==============================\n\n# Create a new directory\nbase_dir = 'base_dir'\nos.mkdir(base_dir)\n\n\n#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n\n# now we create 2 folders inside 'base_dir':\n\n# train\n    # a_uninfected\n    # b_parasitized\n\n# val\n    # a_uninfected\n    # b_parasitized\n\n\n# create a path to 'base_dir' to which we will join the names of the new folders\n# train_dir\ntrain_dir = os.path.join(base_dir, 'train_dir')\nos.mkdir(train_dir)\n\n# val_dir\nval_dir = os.path.join(base_dir, 'val_dir')\nos.mkdir(val_dir)\n\n\n# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\n# Inside each folder we create seperate folders for each class\n\n# create new folders inside train_dir\na_uninfected = os.path.join(train_dir, 'a_uninfected')\nos.mkdir(a_uninfected)\nb_parasitized = os.path.join(train_dir, 'b_parasitized')\nos.mkdir(b_parasitized)\n\n\n# create new folders inside val_dir\na_uninfected = os.path.join(val_dir, 'a_uninfected')\nos.mkdir(a_uninfected)\nb_parasitized = os.path.join(val_dir, 'b_parasitized')\nos.mkdir(b_parasitized)\n\n\n\n# =================================\n# Transfer the Images into Folders\n# =================================\n\n# Set the image_id as the index in df_data\ndf_data.set_index('image_id', inplace=True)\n\n# Set the image_id as the index in df_data\ndf_holdout.set_index('image_id', inplace=True)\n\n\n\n# Get a list of images in each of the two folders\n\npath_uninfected = '..\/input\/cell_images\/cell_images\/Uninfected'\npath_parasitized = '..\/input\/cell_images\/cell_images\/Parasitized'\n\nfolder_1 = os.listdir(path_uninfected)\nfolder_2 = os.listdir(path_parasitized)\n\n# Get a list of train and val images\ntrain_list = list(df_train['image_id'])\nval_list = list(df_val['image_id'])\n\n\n\n# Transfer the train images\n\nfor image in train_list:\n\n    fname = image\n    target = df_data.loc[image,'target']\n\n    if target == 0:\n        label = 'a_uninfected'\n    else:\n        label = 'b_parasitized'\n\n    if fname in folder_1:\n        # source path to image\n        src = os.path.join(path_uninfected, fname)\n        # destination path to image\n        dst = os.path.join(train_dir, label, fname)\n\n        image = cv2.imread(src)\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n        # save the image at the destination\n        cv2.imwrite(dst, image)\n        #shutil.copyfile(src, dst)\n\n    if fname in folder_2:\n        # source path to image\n        src = os.path.join(path_parasitized, fname)\n        # destination path to image\n        dst = os.path.join(train_dir, label, fname)\n\n        image = cv2.imread(src)\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n        # save the image at the destination\n        cv2.imwrite(dst, image)\n\n        # copy the image from the source to the destination\n        #shutil.copyfile(src, dst)\n\n\n\n# Transfer the val images\n\nfor image in val_list:\n\n    fname = image\n    target = df_holdout.loc[image,'target']\n\n\n    if target == 0:\n        label = 'a_uninfected'\n    else:\n        label = 'b_parasitized'\n\n\n    if fname in folder_1:\n        # source path to image\n        src = os.path.join(path_uninfected, fname)\n        # destination path to image\n        dst = os.path.join(val_dir, label, fname)\n\n        image = cv2.imread(src)\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n        # save the image at the destination\n        cv2.imwrite(dst, image)\n\n        # copy the image from the source to the destination\n        #shutil.copyfile(src, dst)\n\n    if fname in folder_2:\n        # source path to image\n        src = os.path.join(path_parasitized, fname)\n        # destination path to image\n        dst = os.path.join(val_dir, label, fname)\n\n        image = cv2.imread(src)\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n        # save the image at the destination\n        cv2.imwrite(dst, image)\n\n        # copy the image from the source to the destination\n        #shutil.copyfile(src, dst)\n\n# Print the number of images in each folder\n\n# train\n#print(len(os.listdir('base_dir\/train_dir\/a_uninfected')))\n#print(len(os.listdir('base_dir\/train_dir\/b_parasitized\/')))\n\n# val\n#print(len(os.listdir('base_dir\/val_dir\/a_uninfected')))\n#print(len(os.listdir('base_dir\/val_dir\/b_parasitized\/')))\n#print('\\n')\n\n\n# ==============================\n# Set Up the Generators\n# ==============================\n\ntrain_path = 'base_dir\/train_dir'\nvalid_path = 'base_dir\/val_dir'\n\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = BATCH_SIZE\nval_batch_size = BATCH_SIZE\n\n\ntrain_steps = np.ceil(num_train_samples \/ train_batch_size)\nval_steps = np.ceil(num_val_samples \/ val_batch_size)\n\n\ndatagen = ImageDataGenerator(rescale=1.0\/255)\n\ntrain_gen = datagen.flow_from_directory(train_path,\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\n                                        batch_size=train_batch_size,\n                                        class_mode='categorical')\n\n\n# Note: shuffle=False causes the test dataset to not be shuffled\ntest_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\n                                        batch_size=val_batch_size,\n                                        class_mode='categorical',\n                                        shuffle=False)\n\nprint('\\n')\n\n# ==============================\n# Set Up the Model Architecture\n# ==============================\n\n\nkernel_size = (3,3)\npool_size= (2,2)\nfirst_filters = 32\nsecond_filters = 64\nthird_filters = 128\n\ndropout_conv = 0.3\ndropout_dense = 0.3\n\n\nmodel = Sequential()\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu', \n                 input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n\nmodel.add(ZeroPadding2D(padding=(PADDING, PADDING), data_format=None))\n\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size)) \nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(dropout_dense))\nmodel.add(Dense(2, activation = \"softmax\"))\n\n#model.summary()\n\n\n\n# ==============================\n# Train the Model\n# ==============================\n\n\nmodel.compile(Adam(lr=0.0001, decay=1e-6), loss='binary_crossentropy', \n              metrics=['accuracy'])\n\n# we are saving the model based on training accuracy\nfilepath = \"final_model.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, \n                             save_best_only=True, mode='max')\n\n\ncallbacks_list = [checkpoint]\n\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n                            epochs=NUM_FINAL_MODEL_EPOCHS, verbose=1,\n                           callbacks=callbacks_list)\n\n\n# ==================================\n# Evaluate the Model on the Val Set\n# ==================================\n\nmodel.load_weights('final_model.h5')\n\nval_loss, val_acc = \\\nmodel.evaluate_generator(test_gen, \n                        steps=val_steps)\n\n\n# ==================================\n# Calculate the AUC Score\n# ==================================\n\ntest_labels = test_gen.classes\n\n# make a prediction\npredictions = model.predict_generator(test_gen, steps=val_steps, verbose=1)\n\n\nval_auc = roc_auc_score(test_labels, predictions.argmax(axis=1))\n\n\n\n\n# ==================================\n# Print the Scores\n# ==================================\n\n#print('\\n')\n#print('Accuracy: ', val_acc)\n#print('Loss: ', val_loss)\n#print('AUC: ', val_auc)\n","f57ddc7d":"# ==================================\n# Print the Scores\n# ==================================\n\nprint('\\n')\nprint('Accuracy: ', val_acc)\nprint('Loss: ', val_loss)\nprint('AUC: ', val_auc)","e5440034":"# argmax returns the index of the max value in a row\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))\n\ntest_gen.class_indices","bda6647d":"# Define the labels of the class indices. These need to match the \n# order shown above.\ncm_plot_labels = ['uninfected', 'parasitized']\n\nplot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')","a974e395":"# Get the filenames, labels and associated predictions\n\n# This outputs the sequence in which the generator processed the test images\ntest_filenames = test_gen.filenames\n\n# Get the true labels\ny_true = test_gen.classes\n\n# Get the predicted labels\ny_pred = predictions.argmax(axis=1)\n\n\n# Generate a classification report\n\nreport = classification_report(y_true, y_pred, target_names=cm_plot_labels)\n\nprint(report)","61358889":"# --ignore-installed is added to fix an error.\n\n# https:\/\/stackoverflow.com\/questions\/49932759\/pip-10-and-apt-how-to-avoid-cannot-uninstall\n# -x-errors-for-distutils-packages\n\n!pip install tensorflowjs --ignore-installed","c7968752":"# Use the command line conversion tool to convert the model\n\n!tensorflowjs_converter --input_format keras final_model.h5 tfjs\/model","5b563eec":"# check that the folder containing the tfjs model files has been created\n!ls","03673efc":"# check that the tfjs files exist\nos.listdir('tfjs\/model')","4ae4ead6":"# Delete the image data directory we created to prevent a Kaggle error.\n# Kaggle allows a max of 500 files to be saved.\n\nif os.path.isdir('base_dir') == True: # return true if the directory exists\n    \n        shutil.rmtree('base_dir')","290eeae5":"### Conclusion","a54a94c6":"### Create a Classification Report\u00b6","fa23f1b5":"We're not using validation data in this final model. Therefore, we won't be able to set up the model to save the best epoch. We'll need to determine the number of training epochs before starting.\n\nTo determine this number I would normally look at the training curves (see train-test-split model) and determine the epoch at which the model started to overfit i.e. the training and validation accuracy curves start to diverge. However, with this data and architecture it appears that overfitting is not a huge problem because the training and validation curves do not diverge significantly. Therefore choosing 10 epochs seems to give a good balance between training time and model quality.\n\nIt's also important to keep in mind that we can train on all data because we set the learning rate to decay at each epoch i.e. the learning rate was scheduled. If we had used a dynamic learning rate (e.g. ReduceLROnPlateau) then we'll need to have some way of replicating the learning rate changes that happened automatically during 5 fold cross validation. Therefore, using a scheduled learning rate keeps things simple.","4feeafcf":"\n- Pre-trained convolutional neural networks as feature extractors toward improved Malaria parasite detection in thin blood smear images. PeerJ6:e4568<br>\nRajaraman S, Antani SK, Poostchi M, Silamut K, Hossain MA, Maude, RJ, Jaeger S, Thoma GR. (2018)<br>\nhttps:\/\/peerj.com\/articles\/4568\/\n","66838a57":"## Introduction","214db6d0":"The images are randomly selected. Therefore, different images will be dispayed each time the code is run. Also you'll see that parasitized images seem to have a blue-ish area that's not as common in uninfected images.","d8054be8":"#### Display images that are parasitized that the model predicted correctly","584118d2":"| <a id='Reference_Kernels'><\/a>","3671a53c":"We now have a trained model that can be incorporated into the web app. The metrics we calculated above all look very good meaning that the model should perform well on unseen data.\n\nAlso, because we've used a simple architecture the model size will be less than 10 MB. This means that it will download quickly. Therefore, the web page will load fast and the overall user experience will be good.","f3def8fd":"### Create the Train and Val Sets\n\nWe will create a stratified val set. This means that the val set will have the same target distribution as the train set. Actually doing stratification isn't essential here because the target is balanced. Because of this there's a good chance that randomly selecting rows will still result in the val set having a fairly balanced target distribution. Stratification is more applicable for data where the target is unbalanced.","bda32d97":"Put the val image_id, labels and predictions into a dataframe.","b29e38da":"### Print the scores for each fold and the average scores","7a22f339":"### Display random images from each of the target classes","1b6c622a":"In this section we're not going to use any fancy statistical gymnastics. We're simply going to look at the images that the model predicted correctly and those it got wrong. We want to see if there are any patterns or issues that could've caused the model to make mistakes.","8b2f2d00":"Please note that we've set the learning rate to reduce (decay) at each epoch.","109f3a69":"### Create a Directory Structure","f4aa7cbd":"## 4. Train-Test-Split Model","eb2b2fb8":"### Transfer the images into the folders","e86ea575":"### Create the Model Architecture","bd15591d":"<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/2O3YrdUZQ5U?rel=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>","beaf1371":"Here we will simply apply the same workflow that we used for the train-test-split model to 5 folds. For each fold we'll get the loss, accuracy and auc. Then we'll average the results of the 5 folds to get the final scores.","811ee109":"The data description says there should be 27,558 cell images i.e. 13779 files per folder. We see that each folder has an extra file. Later we'll check what kind of file this is.","64939a55":"As we continue to train, validate and modify a model we could over time start to overfit the validation data without realizing it. This means that the model will perform well during training but it will perform poorly on unseen data i.e. the app will perform badly in production.\n\nHere we will create a holdout set containing 200 images. We will keep this holdout set aside and only use it at the end to check how the final model performs on unseen data.","4910ad28":"## 7. Train the Final Model using all the data","79379ffc":"### Check if any non image files are in the folders\n\nWe see that there are actually 13780 files in each folder and not 13779 as expected. Folders can sometimes contain files that are automatically created during processing. These files can cause errors in the code if we don't know they exist - like the code we wrote above to display the images. Let's check if there are any non image files in the folders. ","5c872818":"The goal of this kernel is to build a model that can detect malaria parasites in a cell image. The model will analyze a segmented red blood cell image and classify it as either \"uninfected\" or \"parasitized\". Once trained the model will be deployed online as a Tensorflow.js web app. \n\nMalaria diagnosis is currently a manual process. This prototype app is capable of automatically batch analyzing cell images. Therefore, it can help speed up a doctor's diagnostic workflow and reduce diagnostic errors. It can also help medical staff to triage patients by allowing them to quickly assess the severity of each patient's infection.\n\nWe will do basic EDA, build a Keras cnn model, do 5 fold cross validation, train the model using all the data and finally evaluate the model on a holdout set.\n\n\nThis is the link to the live app. The html, css, and javascript code is available on Github. I recommend using the latest version of the Chrome browser. When using Safari for example, you may see a message saying that the model is loading but the app may actually be frozen. To see the app in action simply feed it some images from this dataset.\n\n> Web App<br>\n> http:\/\/malaria.test.woza.work\/<br>\n> \n> Github<br>\n> https:\/\/github.com\/vbookshelf\/Malaria-Cell-Analyzer\n\n\nBecause this app relies on segmented images (Fig. 3 below) this isn't a true end to end solution. In practice a glass slide image has many cells (see Fig. 2 below). A segmentation algorithm will need to be used to isolate (segment out) each cell before it can be fed into this app.\n\nThe app is based on Tensorflow.js, a technology that Google developed that allows machine learning models to run in a web browser. You'll notice that the app can process multiple images in about one second. It's fast because the analysis is done locally - on the user's pc or mobile phone. There's no need to upload images to an external server thus ensuring patient privacy and data security.\n\nI hope that this project will help you see how easy it is to build and deploy an Ai based solution.","0f4e5213":"<img src=\"http:\/\/malaria.test.woza.work\/assets\/children.jpg\" width=\"600\"><\/img>\n\n<h5 align=\"center\">Malaria killed more than 261,000 children in 2017<\/h5>","bbd2c9f7":"| <a id='Error_Analysis'><\/a>","b8ceb2d1":"## Contents","07b31b6d":"## 2. EDA","b814fa24":"- Excellent tutorial series by deeplizard on how to use Tensorflow.js to build a web app.<br>\nhttps:\/\/www.youtube.com\/watch?v=HEQDRWMK6yY\n\n- Tutorial by Minsuk Heo on Accuracy, Precision and F1 Score<br>\nhttps:\/\/www.youtube.com\/watch?v=HBi-P5j0Kec\n\n- Tutorial by Data School on how to evaluate a classifier<br>\nhttps:\/\/www.youtube.com\/watch?v=85dtiMz9tSo\n\n- Tensorflow.js gallery of projects<br>\nhttps:\/\/github.com\/tensorflow\/tfjs\/blob\/master\/GALLERY.md","36385744":"### Create a Classification Report","ccf7dd05":"## 9. Convert the final model from Keras to Tensorflow.js\n\nThis conversion needs to be done so that the model can be loaded into the web app.","734abb56":"To reduce RAM use and prevent this kernel from crashing we will batch feed the images when training the model. We will use generators to do this. In order to use this approach Keras requires that a particular directory structure be set up. Keras uses this structure to automatically infer the class (target) of each image.","8c569bb0":"## 1. Domain Knowledge","da319171":"<a href='#Domain_Knowledge'>1. Domain Knowledge<\/a><br>\n<a href='#EDA'>2. EDA<\/a><br>\n<a href='#Create_a_Holdout-Set'>3. Create a Holdout Set<\/a><br>\n<a href='#Train-Test-Split_Model'>4. Train-Test-Split Model<\/a><br>\n<a href='#Error_Analysis'>5. Error Analysis<\/a><br>\n<a href='#Cross_Validation'>6. 5 Fold Cross Validation<\/a><br>\n<a href='#Train'>7. Train the Final Model on all data<\/a><br>\n<a href='#Evaluate'>8. Evaluate the Final Model on the Holdout Set<\/a><br>\n<a href='#Convert'>9. Convert the Final Model to Tensorflow.js<\/a><br>\n\n<a href='#Citations'>Citations<\/a><br>\n<a href='#Reference_Kernels'>Reference Kernels<\/a><br>\n<a href='#Helpful_Resources'>Helpful Resources<\/a><br>\n<a href='#Conclusion'>Conclusion<\/a><br>\n","d7f0c707":"<hr>","b19560f7":"#### Display images that are parasitized but the model predicted uninfected","6e86e93c":"### Reference Kernels","4eb31683":"### Citations","a7a0e22c":"### How many files are in each folder?","2eeee5b2":"### EDA Summary\n\n- There are 27,558 segmented cell images.\n- Each folder contains 13,779 images.\n- The images are of various sizes.\n- All images are in png format.\n- All images have 3 channels i.e. all are colour images.\n- There are no all black or all white images.\n- The target distribution is balanced i.e. each target class has the same number of images.\n- All cells were segmented from thin blood smear slides.\n- The parasite specie on all parasitized images is P. falciparum.\n- Each folder contains a non image file called Thumbs.db.\n- The presence of a blue-ish area inside the cell appears to be a common (but not definitive) indicator that a cell is parasitized.\n\n","b278f1ce":"<hr>","59ade021":"### Observations and Comments\n\nIn cases where the label was parasitized but the model predicted uninfected I noticed that the blue-ish area was located close to the edge of the image. Could it be that the model was not detecting important patterns that are located at the edge of images? To address this issue I decided to add a padding layer to the model arcitecture. This layer added a 10 pixel zero padding around each image. I found that the cross validation scores improved after this change.\n\n\nWhen doing this error analysis our premise has been that if there is a mismatch between the label and the prediction then the model has made a mistake. However, there's another possibility - the model prediction is correct. There are 27,558 images in this dataset. All images were examined and labeled by the same expert. This must've been a tedious and time consuming process so it's possible that some images were incorrectly labeled. \n\nAt this point it would be helpful to collaborate with a domain expert in order to discuss possible reasons for the mistakes this model is making. Are they the result of model weakness, incorrect labels or maybe damaged images? Is the correct label easy for a human to classify or would a human struggle to make a correct diagnosis? What are the main indicators that an expert looks for when examining cell images? Is the model seeing things that an expert didn't notice?","25a09466":"#### Display images that are uninfected that the model predicted correctly","409e2884":"### Delete the images that were moved around","c9f002a2":"### Helpful Resources","0fbc0b4d":"| <a id='EDA'><\/a>","66c1829a":"We now have a holdout set containing 200 images.","b434fbf4":"### Train the Model","89f826fd":"- Gabriel Preda, Honey Bee Subspecies Classification<br>\nhttps:\/\/www.kaggle.com\/gpreda\/honey-bee-subspecies-classification\n\n- Francesco Marazzi, Baseline Keras CNN<br>\nhttps:\/\/www.kaggle.com\/fmarazzi\/baseline-keras-cnn-roc-fast-5min-0-8253-lb\n\n- Kimmo S\u00e4\u00e4skilahti, Image processing with scikit-image<br>\nhttps:\/\/www.kaggle.com\/ksaaskil\/image-processing-with-scikit-image\n\n- Marsh, Skin Lesion Analyzer<br>\nhttps:\/\/www.kaggle.com\/vbookshelf\/skin-lesion-analyzer-tensorflow-js-web-app","4ff592db":"## 5. Error Analysis","9e799234":"| <a id='Evaluate'><\/a>","dbb47909":"I found this cnn architecture a while ago in a kernel created by @fmarazzi. I've since used this on several projects. It really is a good multi-purpose architecture. Here I've modified it slightly by adding a ZeroPadding layer. Later, in the error analysis section, I'll explain why I added this layer.","fec9553d":"### What are the image sizes and how many channels does each have?\n\nHere we will add the following info on each image to the df_combined dataframe:\n\nw = width<br>\nh = height<br>\nc = number of channels<br>\nmax_pixel_value<br>\nmin_pixel_value<br>\nimage_format","5403a573":"**Recall** = Given a class, will the classifier be able to detect it?<br>\n**Precision** = Given a class prediction from a classifier, how likely is it to be correct?<br>\n**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.<br>\n","427217a6":"| <a id='Create_a_Holdout-Set'><\/a>","f3f28ad6":"The output above matches the number of rows. This confirms that each image has a unique name. This is important to verify because duplicate image file names will cause errors in the processing code that we will write later.","e651a2db":"| <a id='Convert'><\/a>","a2a35e73":"### Plot the Training Curves","47a64955":"> *Malaria exacts a massive toll on human health and imposes a heavy social\nand economic burden in low and middle income countries, particularly in Sub-Saharan Africa and South Asia. An estimated 219 million people suffered from the disease in 2017 and about 435,000 died. More than 90 percent of the deaths were in Africa, and over 60 percent were among children under 5.<br>*\n ~ [gatesfoundation.org](https:\/\/www.gatesfoundation.org\/What-We-Do\/Global-Health\/Malaria)","b68f2bb6":"### Evaluate the model using the val set","66a0ab2d":"\n\n### Quick Facts\n\n- Malaria is a disease that infects and destroys red blood cells. \n- Doctors analyze thick and thin blood smears to diagnose malaria and determine how severe it is.\n- The parasite that causes malaria is called Plasmodium.\n- The female Anopheles mosquito is the only mosquito that transmits malaria.\n\n\n### Background\n\nMicroscopic examination of thick and thin blood smears is the easiest and most reliable test for malaria. Blood smears are often taken from a finger prick.\n\n<br>\n\n<img src=\"http:\/\/malaria.test.woza.work\/assets\/glassslide.jpg\" width=\"400\"><\/img>\n\n<h5 align=\"center\">Fig 1. Microscope and glass slide<\/h5>\n\n<br>\n<br>\n\n<img src=\"http:\/\/malaria.test.woza.work\/assets\/raw_image.jpg\" width=\"400\"><\/img>\n\n<h5 align=\"center\">Fig 2. Cells seen under a microscope. <br>\nThin blood smear.<\/h5>\n\n<br>\n<br>\n\n<img src=\"http:\/\/malaria.test.woza.work\/assets\/parasitized.png\" width=\"200\"><\/img>\n\n<h5 align=\"center\">Fig 3. Segmented parasitized cell<\/h5>\n\n<br>\n<br>\n\n*Malaria diagnosis involves the following*:<br>\n> - Determine if the malaria parasite is present.\n> - Determine the parasite species.\n> - Determine parasite density by counting the number of cells that are infected.\n\n<br>\n\n*Thick Blood Smear*<br>\n\nA thick blood smear is a drop of blood on a glass slide. It's used to determine if there are any parasites in the blood. It's a larger blood sample because there could only be a few parasites present so a larger sample is needed to detect them. If no parasites are found the patient will have repeated blood smears every 8 hours for a few of days to confirm that there's no malaria infection.\n\n<br>\n\n*Thin Blood Smear*<br>\n\nA thin blood smear is a drop of blood that's spread over a large area of the glass slide. This blood smear helps doctor's discover what species of parasite is causing the infection.\n\n<br>\n\n*Plasmodium*<br>\n\nThis is the parasite that causes malaria. There are many species of Plasmodium. Five species cause malaria - P. vivax, P. ovale, P. malariae, P. knowlesi and P. falciparum. Most deaths are caused by P. falciparum. The other species usually cause a milder form of malaria.\n\n<br>\n\n*Parasitemia*<br>\n\nThis is the percentage of red blood cells that are infected by malaria (parasite density). This is computed by counting the number of infected cells visible under a microscope. This number helps doctors determine how severe the disease is. They then prescribe a treatment based on the severity. For example, if a large percentage of blood cells is infected medicine may be given directly into a vein instead of by mouth.\n\n<br>\n**Sources**\n\n- uofmhealth.org<br>\nhttps:\/\/www.uofmhealth.org\/health-library\/hw118744\n\n- Wikipedia Malaria<br>\nhttps:\/\/en.wikipedia.org\/wiki\/Malaria\n\n- Pre-trained convolutional neural networks as feature extractors toward improved malaria parasite detection in thin blood smear images<br>\nhttps:\/\/peerj.com\/articles\/4568\/\n","c70b64ad":"| <a id='Helpful_Resources'><\/a>","2d28cd79":"| <a id='Conclusion'><\/a>","cd7724a0":"## Malaria Cell Analyzer\nby Marsh [ @vbookshelf ] <br>\n24 May 2019","50f5d50a":"#### Display images that are uninfected but the model predicted parasitized","3e8a1400":"### Create a Confusion Matrix","e2ad2a3c":"| <a id='Train'><\/a>","80e6dead":"### Analyze the wrong predictions","66940ddb":"| <a id='Cross_Validation'><\/a>","fd4437e1":"## 3. Create a Holdout Set\n","de4896dd":"## 8. Evaluate the Final Model on the Holdout Set","e9f18fc5":"## 5 Fold Cross Validation","dd7e5e93":"### Put the image names into dataframes\n\nHere we will create a dataframe called df_combined that includes both uninfected and parasitized images. This new dataframe will have a column showing the target class of each image.","c47799cf":"| <a id='Citations'><\/a>","ae44946d":"| <a id='Train-Test-Split_Model'><\/a>","54e73b6c":"### Set Up the Generators","78b73c76":"### Let's take a quick look at some images from each class\n\nThere are two classes\n- uninfected\n- parasitized.","dcb86a20":"| <a id='Domain_Knowledge'><\/a>","fc49d2bd":"### Create a Confusion Matrix","66f30936":"We see that each folder has one non image file called Thumbs.db. Now that we know that these non image files exist we will be sure exclude them later.","1257b1d6":"Train-test-split is not the ideal way to assess model perfromance. However, it's a good starting point because it's simple to set up and runs 5 times faster than 5 fold cross validation. This helps us to:\n\n- Quickly get a feel for what kind of performance we can expect from this data.\n- Quickly test different architectures and parameters.\n- Establish the workflow that will later be used in cross validation.\n- Check the training curves to see if the model is overfitting.\n- Check the training curves to establish the number of epochs we will use when training the final model on all data.\n- Perform error analysis.\n\nA train-test-split model is a rough guide. When deciding if a particular change actually improved model performance we will base that decision on cross validation results. This is important. If we don't use cross validation we may think we are improving but in reality we may be getting nowhere.","1b032444":"This is the Foldscope Origami Microscope.<br>\nhttps:\/\/youtu.be\/ky-cqSI5mwE\n\nThe microscope image could perhaps be photographed using phone camera and then processed using a web app.\n\n\nThis was a software solution. However, there could be some who are reading this who may be interested in combining the power of Ai with electronics - by building robots, analyzers and other devices. You probably don't know where to start. The good news is that if you can code then you can also build electronic devices. The principles are the same. It's just that one uses software and the other uses physical components. \n\nThese two practical Udemy courses are a great place to start your journey towards world domination. They are geared towards young students and, most importantly, the instructor is an excellent teacher. The courses were developed with deaf students in mind.\n\nElectronics and Robotics<br>\nhttps:\/\/www.udemy.com\/analog-electronics-robotics-learn-by-building\/\n\nDigital Electronics and Robotics<br>\nhttps:\/\/www.udemy.com\/digital-electronics-robotics-learn-by-building-module-ii\/\n\n\nMany thanks to Arunava for making this interesting dataset available on Kaggle.\n\nThank you for reading.","e161c6a5":"### Analyze the correct predictions","5df0809d":"**Recall** = Given a class, will the classifier be able to detect it?<br>\n**Precision** = Given a class prediction from a classifier, how likely is it to be correct?<br>\n**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.<br>"}}