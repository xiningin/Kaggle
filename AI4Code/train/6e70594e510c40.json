{"cell_type":{"1d73372f":"code","c1c34824":"code","dbd7a336":"code","848d3155":"code","33f0c51a":"code","2d5a30cd":"code","ac398bd6":"code","e0bca5f3":"code","fa5c2b85":"code","46d218e6":"code","c3856388":"code","3b876fae":"code","657bb066":"markdown","024c0d86":"markdown","9702c1c6":"markdown","8d78f8b1":"markdown","08781c8f":"markdown","457a335c":"markdown","28ee9627":"markdown","9fd5b86d":"markdown"},"source":{"1d73372f":"!pip install rank_bm25","c1c34824":"!pip install transformers==2.3.0","dbd7a336":"import os\nimport re\nimport json\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport glob\nfrom nltk import tokenize\nimport torch\nfrom transformers import BertTokenizer, BertForQuestionAnswering\nfrom rank_bm25 import BM25Okapi\nfrom collections import Counter\nimport pickle\n\nbase_dir = '\/kaggle\/input\/cord19researchchallenge\/CORD-19-research-challenge\/'","848d3155":"def format_name(author):\n    middle_name = \" \".join(author['middle'])\n    \n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n    \n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n    \n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:\n            name_ls.append(name)\n    \n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n    \n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'], \n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n\n    return \"; \".join(formatted)\n\ndef format_body_text(body_text):\n    \n    body = \"\"\n\n    for di in body_text:\n        text = di['text']\n        body += text\n    return body\n    \ndef format_corpus_text(body_text, min_len=18, max_len=128):\n    junk_text = \"copyright\"\n    \n    def remove_braces_brackets(body_text):\n        body_text = re.sub(r'\\([0-9]+\\)', '', body_text)\n        body_text = re.sub(r'\\[[^)]*\\]', '', body_text)\n        return(body_text)\n        \n    body_text = remove_braces_brackets(body_text)\n    text_lines = []\n    token_lines = tokenize.sent_tokenize(body_text)\n    for line in token_lines:\n      \n        words = line.split()\n        if junk_text not in words:\n             max_word_len = len(max(words, key=len))\n             if (len(words) > min_len) and (len(words) < max_len) and max_word_len > 5:\n                 text_lines.append(line)\n    \n    return(text_lines)\n\n\ndef find_filenames(folder):\n    all_files = glob.glob(f'{folder}\/**\/*.json', recursive=True)\n    print(\"Number of articles retrieved from the folder:\", len(all_files))\n    files = []\n\n    for filename in all_files:\n        with open(filename) as f:\n            file = json.load(open(filename))\n            files.append(file)\n    return(files) \n\n\ndef find_file_index(folder):\n    all_files = glob.glob(f'{folder}\/**\/*.json', recursive=True)\n    path_name = []\n    path_dict = {}\n    path_dict_inv = {}\n    file_index = []\n\n\n    for filename in all_files:\n        filename_split = filename.split('\/')\n        last = filename_split[-1]\n        first = filename_split[-4]+'\/'+filename_split[-3]+'\/'+filename_split[-2]+'\/'\n  \n        if first not in path_name:\n            path_name.append(first)\n            path_dict[first] = len(path_name)-1\n            path_dict_inv[len(path_name)-1] = first\n        file_index.append((path_dict[first], last))   \n        \n    print(len(file_index))\n    return file_index, path_dict_inv ","33f0c51a":"def generate_clean_data(files):\n    cleaned_text = []\n\n    for file in tqdm(files):\n        body_text = format_body_text(file['body_text'])\n        body_text = body_text.replace('\\n',' ')\n\n        features = [\n           file['metadata']['title'],\n           format_authors(file['metadata']['authors'], with_affiliation=True),\n           body_text]\n        cleaned_text.append(features)\n    \n    col_names = [\n       'title',\n       'authors',\n       'paragraphs']\n\n    clean_df = pd.DataFrame(cleaned_text, columns=col_names)\n    return(clean_df)\n\ndef find_index_text(base_dir, file_index, path_dict, index):\n    indexed_files = []\n    \n    for i in index:\n        filename = base_dir+path_dict[file_index[i][0]]+file_index[i][1]\n\n        with open(filename) as f:\n            file = json.load(open(filename))\n            indexed_files.append(file)\n        \n    frame = generate_clean_data(indexed_files)\n    return(frame)","2d5a30cd":"class BM25Retriever(BM25Okapi):\n    def __init__(self, lowercase=True, tokenizer=None, top_n=10, k1=1.5, b=0.75, epsilon=0.25):\n        super().__init__(\"dummy\", tokenizer=None, k1=k1, b=b, epsilon=epsilon)\n        self.lowercase = lowercase\n        self.top_n = top_n\n        self.doc_freqs = []\n        self.idf = {}\n        self.doc_len = []\n        self.tokenizer = tokenizer\n        self.num_doc = 0\n        self.corpus_size = 0\n        self.nd = Counter({})\n        \n    def fit_retriever(self, documents):\n        doc_list = [document for document in documents]\n        #print(len(doc_list))\n        if self.tokenizer:\n            tokenized_text = [self.tokenizer(document) for document in doc_list]\n        else:\n            tokenized_text = [document.split(\" \") for document in doc_list]\n   \n        #print(tokenized_text[0])\n        self.corpus_size = self.corpus_size+len(tokenized_text)\n        num_doc = 0\n        for doc_tokens in tokenized_text:\n            num_doc += len(doc_tokens)\n        self.num_doc = self.num_doc+num_doc   \n        self.avgdl = self.num_doc\/self.corpus_size\n        \n        #print(self.corpus_size, self.num_doc, self.avgdl)\n        nd = Counter(self._initialize(tokenized_text))\n        self.nd = self.nd + nd      \n        \n        \n    def compute_params(self):    \n        self._calc_idf(self.nd)\n        \n    def compute_scores(self, query):\n        if(self.tokenizer == None):\n           tokenized_query = query.split(\" \")\n        else:\n           tokenizer = self.tokenizer\n           tokenized_query = tokenizer(query)\n      \n        doc_scores = self.get_scores(tokenized_query)\n\n        #return top_n indices and scores as list\n        sorted_scores = np.argsort(doc_scores)\n        top_n = self.top_n\n        out = zip(sorted_scores[-1:-top_n-1:-1],doc_scores[sorted_scores[-1:-top_n-1:-1]])\n        return list(out)   \n                ","ac398bd6":"from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\nclass DocReader():\n    \"\"\"\n    Uses Hungging Face DistilBertForQuestionAnswering\n    Parameters\n    ----------\n    model     : path to folder containing pytorch.bin, bert_config.json and vocab.txt\n                or pretrained model\n    lowercase : boolean\n        Convert all characters to lowercase before tokenizing. (default is True)\n    tokenizer : default is DistilBertTokenizer\n    \"\"\"\n    \n    \n    def __init__(self, model:str=None, lowercase=True, tokenizer=DistilBertTokenizer):\n\n        self.lowercase = lowercase\n        self.tokenizer = tokenizer.from_pretrained(model)\n        self.model = DistilBertForQuestionAnswering.from_pretrained(model)\n        if torch.cuda.is_available():\n            self.device =torch.device(\"cuda\")\n            self.model.cuda()\n            print(\"PyTorch on CUDA\")\n        else:\n            self.device =torch.device(\"cpu\")\n        \n    def predict(self, \n                df: pd.DataFrame = None,\n                query: str = None,\n                n_best: int =3):\n    \n        doc_text = df['paragraphs']\n        self.n_best = n_best\n        \n        if(self.lowercase):\n            query = query.lower()\n        \n        # num docs_index must be equal to top_n\n        doc_index = list(doc_text.index)\n        \n        #prepare the model for validation\n        self.model.eval()\n        answers = []\n        for df_index in doc_index:      \n            if(self.lowercase):\n                doc_lines = doc_text[df_index].lower()\n            else:\n                doc_lines = doc_text[df_index]\n                 \n            #doc_lines = tokenize.sent_tokenize(doc_lines)\n            doc_lines = format_corpus_text(doc_lines)\n            doc_answers = []\n            for lines in doc_lines:\n                input_ids  = self.tokenizer.encode(query, lines)\n              \n                input_ids_device = torch.tensor([input_ids]).to(self.device)\n                with torch.no_grad():\n                    start_scores, end_scores = self.model(input_ids_device)\n                   \n                all_tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n                answer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1])\n                entry = {}   \n                start_scores = start_scores.detach().cpu().numpy()\n                end_scores = end_scores.detach().cpu().numpy()\n                entry['answer'] = answer\n                entry['score'] = (max(start_scores[0]), max(end_scores[0]))\n                entry['index'] = df_index\n                entry['sentence'] = lines\n                doc_answers.append(entry)\n                \n            best_doc_ans = [entry['score'][0]+entry['score'][1]  for entry in doc_answers]\n            ans_index = np.argsort(best_doc_ans)    \n            # take n_best answers per document based on max(start_scores+end_scores)\n            #it is possible to improve by taking different metric\n            for ans in range(1,self.n_best+1):\n                answers.append(doc_answers[ans_index[-ans]])    \n                \n                \n        best_ans = [entry['score'][0]+entry['score'][1]  for entry in answers]\n        ans_index = np.argsort(best_ans)     \n        \n        n_best_answers = []\n        for ans in range(1,self.n_best+1):\n            n_best_answers.append(answers[ans_index[-ans]])\n        \n        return(n_best_answers)          \n    \n    \n    \n    def best_answer(self, answers):\n        ans_dict = {}\n        final_answer = {}\n        ANS_THRESH = 2.0\n        max_score = answers[0]['score'][0]+answers[0]['score'][1]\n    \n        for ans in answers:\n           score = ans['score'][0]+ans['score'][1]\n           if score > max_score - ANS_THRESH:\n              start_end = ans['answer'].split()\n              if(len(start_end)>0):\n                 ans_key = (start_end[0], start_end[-1])\n                 ans_dict[ans_key] = ans_dict.get(ans_key,0)+1\n                \n        inverse = [value for key, value in ans_dict.items()]\n        inverse.sort()\n        \n        ans_list = [key for key, value in ans_dict.items() if(value == inverse[-1])]\n    \n        max_score = float('-inf')\n        for ans in answers:\n           start_end = ans['answer'].split()\n           ans_key = (start_end[0], start_end[-1])\n           for item in ans_list:\n              if(ans_key == item):\n                 score = ans['score'][0]+ans['score'][1]\n                 if(score > max_score):\n                    ans_ids = self.tokenizer.convert_tokens_to_ids(start_end)\n                    final_answer['answer'] = self.tokenizer.decode(ans_ids)\n                    final_answer['index'] = ans['index']\n                    final_answer['sentence'] = ans['sentence']\n        \n        \n        return(final_answer)\n            \n            ","e0bca5f3":"if 0:\n  # Use Wordpiece tokenizer\n    bert_tokenizer =  BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n    retriever = BM25Retriever(tokenizer=bert_tokenizer.tokenize)\n\n    base_dir = '\/kaggle\/input\/CORD-19-research-challenge\/'\n    sub_folders = glob.glob(base_dir)\n    for folder in sub_folders:\n        files = find_filenames(folder)\n        if(len(files) > 0):\n            frame = generate_clean_data(files)\n            retriever.fit_retriever(frame['paragraphs'])\n   \n    #Compute TF-IDF paramas\n    retriever.compute_params()\n    with open('\/kaggle\/output\/retriever.pkl', 'wb') as f:\n        pickle.dump(retriever, f, pickle.HIGHEST_PROTOCOL)\n        \n    #Save file index in a dictionary    \n    file_index, path_dict_inv = find_file_index(base_dir)    \n    datafile_dict['file_index'] = file_index\n    datafile_dict['path_dict_inv'] = path_dict_inv\n    \n    with open('\/kaggle\/output\/datafile_dict.pkl', 'wb') as f:\n        pickle.dump(datafile_dict, f, pickle.HIGHEST_PROTOCOL)\n    \n        ","fa5c2b85":"# Use Wordpiece tokenizer\nbert_tokenizer =  BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\nretriever = BM25Retriever(tokenizer=bert_tokenizer.tokenize)\n\nretriever_pkl_file = '\/kaggle\/input\/bm25-dictionary\/retriever.pkl'\nwith open(retriever_pkl_file, 'rb') as file_in:\n    retriever = pickle.load(file_in)\n\nprint(retriever.corpus_size, retriever.num_doc, retriever.avgdl)    ","46d218e6":"datafile_pkl_file = '\/kaggle\/input\/datafile-dict\/datafile_dict.pkl'\nwith open(datafile_pkl_file, 'rb') as file_in:\n    datafile_dict = pickle.load(file_in)\n\nfile_index = datafile_dict['file_index']\npath_dict_inv = datafile_dict['path_dict_inv']\n\n#Initialize Distilbert based document reader.\nreader = DocReader('distilbert-base-uncased-distilled-squad')","c3856388":"# Find top_n documents based on BM250 for a given query \nquery = \"what is covid-19\"\ndoc_scores = retriever.compute_scores(query)\n\n#Select top_n documents\nindex = [score[0] for score in doc_scores]\n\n#Retrieve document texts for top_n documents\n#text['paragraphs'] = Entire text in the document\n#text['title'] = title of the document\n#text['authors'] = authors\ntext = find_index_text(base_dir, file_index, path_dict_inv, index)","3b876fae":"#find best answer from n_best documents returned by the document retriever\nans = reader.predict(df=text, query=query, n_best=5)\nb_answer = reader.best_answer(ans)\n\nprint('query: {}\\n'.format(query))\nprint('answer: {}\\n'.format(b_answer['answer']))\nprint('title: {}\\n'.format(text['title'][b_answer['index']]))\nprint('paragraph: {}\\n'.format(b_answer['sentence']))","657bb066":"<p> Created a new class using BM25Okapi at  https:\/\/pypi.org\/project\/rank-bm25\/ to fit large corpus by splitting into smaller datasets <p>\n    \nTokenizing entire CORD19 dataset requires large memory. So, BM25Okapi class is modified to feed few documents (one folder) at a time and generate word frequencies. ","024c0d86":"Load BM25 model from pickle file.  The TF-IDF based document retriever uses wordpiece tokenizer.\nThe use of wordpiece tokenizer helps\n1. To minimize the memory foot-print\n2. Improved performance when tokenizer of retriever and bert reader are matched.\n\nThe fit_retriever() function uses WordPiece tokenizer to generate word frequencies of entire CORD19 database.  The compute_params() function computes idf parameters.\nTakes an hour to generate idf parameters and saved as pickle file.\nThe idf parameters needed to computed only once and loaded from the pickle file to initialize the document retriever","9702c1c6":"Use Distilbert model from Transformers that is pretrained on SQUAD 1.1","8d78f8b1":"The initial work on my native machine was done using bert-large-uncased and same vocabulary\/tokenizer is used for document retriever. \nNote that distibert uses bert-base-uncased vocabulary and this small difference in retriever and reader vocabulary does not impact the performance","08781c8f":"### It is slow and computationally expensive to run Bert on every document in the CORD19 dataset for QA application.\n\n### The CORD19 QA is implemented as \n1. Document Retrieval based on TF-IDF of the query \n2. Select N_best documents prioritized by TF-IDF scores\n3. Run QA on N_best docuements using Distilbert \n4. Select best answer\/span based on start-end logits\n\n#### The computation of TF-IDF on entire CORD19 dataset is intense and it can be pre-computed\/stored. The CORD19 database is updated regularly and currently, CORD19 dataset dated April 20th 2020 is used to generate TF-IDF for the retriever. The file index corresponding to this dataset is stored in a file.  So, the implementation relies on two files - one for TF-IDF of dataset words and another for file index\n\n#### The HuggingFace DistilBertForQuestionAnswering on PyTorch is used. It is available in transformers library. The BM25 TF-IDF module in rank-bm25 is used for document retriever","457a335c":"# CORD19 QA Based on DistilBert","28ee9627":"Re-using some functions from https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv to read paragraphs, title and authors from tha database","9fd5b86d":"<p>The CORD19 dataset is updated periodically. So, file indexing of the document retriever model (with idf parameters) loaded from the pickle file might not match the dataset when dataset changes.<p>\nThe CORD19 dataset is fixed (currently set to April 20th) and file indexing is stored in a pickle file. The idf computation and file index generation steps much match."}}