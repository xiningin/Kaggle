{"cell_type":{"7abaacfd":"code","940f4a08":"code","c6f4e8b7":"code","508a91aa":"code","21e66389":"code","eef540fb":"code","2422ded8":"code","4656d8eb":"code","5d3db1e2":"code","15317383":"code","7fa061ea":"code","2014cc35":"code","65f9a1cb":"code","a1e6cbb4":"code","539c2f0f":"code","f6b019dd":"code","d71d8c9d":"code","922130e1":"code","9b1f326a":"code","1c1a9e98":"code","80d5acf3":"code","f0ab6333":"code","543a1c89":"code","c293783d":"code","d6022a6c":"code","0291371a":"code","fcd88711":"code","75e24dd4":"code","fce776f8":"code","3b19e73b":"code","93cb960e":"code","58839c06":"code","8da2be58":"code","ead69491":"code","91976623":"code","78d1aaaa":"markdown","9ec6fb3c":"markdown","7bde1eaa":"markdown","54bd3ce0":"markdown","d3e4a408":"markdown","99b63461":"markdown","861f31ba":"markdown","fe176ab2":"markdown","16a04e33":"markdown","4b2fcb12":"markdown","2bad2dba":"markdown","b48c2f94":"markdown","88d3661a":"markdown","454f1918":"markdown","7c07d225":"markdown"},"source":{"7abaacfd":"#Importing Data manipulation and plotting modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport time","940f4a08":"#Importing libraries for performance measures\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import auc, roc_curve","c6f4e8b7":"#Importing libraries For data splitting\nfrom sklearn.model_selection import train_test_split\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier","508a91aa":"#Import libraries for data balancing\nfrom imblearn.over_sampling import SMOTE, ADASYN","21e66389":"data = pd.read_csv(\"..\/input\/creditcard.csv\")","eef540fb":"pd.options.display.max_columns = 200","2422ded8":"data.head()","4656d8eb":"data.shape","5d3db1e2":"sns.countplot(data['Class'])","15317383":"f, axes = plt.subplots(1, 2, figsize=(14, 5))\nsns.distplot(data['Amount'].values,ax=axes[0])\naxes[0].set_title(\"Distribution of Transaction Amount\")\nsns.distplot(data['Time'].values,ax=axes[1])\naxes[1].set_title(\"Distribution of Transaction Time (in seconds)\")\nplt.show()","7fa061ea":"data.drop(['Time'], inplace = True, axis =1)","2014cc35":"\ndata['Class'].value_counts()[1]\/data.shape[0]","65f9a1cb":"y = data.iloc[:,29]\nX = data.iloc[:,0:29]\nX.shape","a1e6cbb4":"y.shape","539c2f0f":"X_train, X_test, y_train, y_test =   train_test_split(X,\n                                                      y,\n                                                      test_size = 0.3,\n                                                      stratify = y\n                                                      )","f6b019dd":"X_train.shape","d71d8c9d":"y_train.shape","922130e1":"max_delta_step= [1,2,3,4,5,6,7,8,9,10]\nscale_pos_weight= [1,2,3,4,5,6,7,8,9,10]\nnum_zeros = (data['Class'] == 0).sum()\nnum_ones = (data['Class'] == 1).sum()\nsp_weight = num_zeros \/ num_ones\nfor i in max_delta_step:\n    print('--------------------')\n    print('Iteration ', i)\n    print('--------------------')\n    print('scale_pos_weight = {} '.format(i))\n    print('max_delta_step = {} '.format(i))\n    xgb = XGBClassifier(scale_pos_weight = i,max_delta_step=i)\n    xgb.fit(X_train,y_train)\n    xgb_predict = xgb.predict(X_test)\n    xgb_proba = xgb.predict_proba(X_test)\n    xgb_cm = confusion_matrix(y_test, xgb_predict)\n    p_xg,r_xg,f_xg,_ = precision_recall_fscore_support(y_test,xgb_predict)\n    print('Accuracy',accuracy_score(y_test, xgb_predict))\n    print('Confusion Matrix: \\n', xgb_cm)\n    print('Precision: ',p_xg)\n    print('Recall: ',r_xg)\n    print('F score: ',f_xg)","9b1f326a":"rf = RandomForestClassifier(n_estimators=100,random_state=0, n_jobs=-1, class_weight=\"balanced\")","1c1a9e98":"rf1 = rf.fit(X_train,y_train)\ny_pred_rf = rf1.predict(X_test)\ny_pred_rf_prob = rf1.predict_proba(X_test)","80d5acf3":"accuracy_score(y_test,y_pred_rf)","f0ab6333":"confusion_matrix(y_test,y_pred_rf)","543a1c89":"p_rf,r_rf,f_rf,_  = precision_recall_fscore_support(y_test,y_pred_rf)","c293783d":"print('Precision:',p_rf , '\\nRecall',r_rf,'\\nFscore',f_rf,_ )","d6022a6c":"sm = SMOTE(random_state=42)","0291371a":"X_smote, y_smote = sm.fit_sample(X_train, y_train)","fcd88711":"X_smote.shape","75e24dd4":"y_smote.shape","fce776f8":"np.sum(y_smote)\/len(y_smote)\n\n#We can see now the data is balanced","3b19e73b":"y_smote = y_smote.reshape(y_smote.size, 1)\ny_smote.shape ","93cb960e":"xg_smote = XGBClassifier(learning_rate=0.1,\n                   reg_alpha= 0,\n                   reg_lambda= 1,\n                   )\nrf_smote = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n\ncolumns = X_train.columns\nX_smote = pd.DataFrame(data = X_smote, columns = columns)\n\nxg_fit = xg_smote.fit(X_smote,y_smote)\nrf_fit = rf_smote.fit(X_smote,y_smote)\n\ny_pred_xgb = xg_fit.predict(X_test)\ny_pred_rfb = rf_fit.predict(X_test)\n\ny_pred_xgb_prob = xg_fit.predict_proba(X_test)\ny_pred_rfb_prob = rf_fit.predict_proba(X_test)\n\np_rfb,r_rfb,f_rfb,_  = precision_recall_fscore_support(y_test,y_pred_rfb)\np_xgb,r_xgb,f_xgb,_  = precision_recall_fscore_support(y_test,y_pred_xgb)\n\n\nprint('Random Forest:\\n')\nprint('Accuracy - ',accuracy_score(y_test,y_pred_rfb))\nprint('\\nPrecision - ',p_rfb , '\\nRecall - ',r_rfb,'\\nFscore - ',f_rfb,_ )\nprint('Confusion Matrix -\\n',confusion_matrix(y_test,y_pred_rfb))\n\nprint('XGBoost:\\n')\nprint('Accuracy - ',accuracy_score(y_test,y_pred_xgb))\nprint('\\nPrecision - ',p_xgb , '\\nRecall - ',r_xgb,'\\nFscore - ',f_xgb,_ )\nprint('Confusion Matrix - \\n',confusion_matrix(y_test,y_pred_xgb))\n\n","58839c06":"adasyn = ADASYN(random_state=42)\nX_ada, y_ada = adasyn.fit_sample(X_train, y_train)","8da2be58":"X_ada.shape","ead69491":"y_ada.shape","91976623":"np.sum(y_ada)\/len(y_ada)\n\ny_ada = y_ada.reshape(y_ada.size, 1)\ny_ada.shape \n\nxg_ada = XGBClassifier(learning_rate=0.5,\n                   reg_alpha= 0.1,\n                   reg_lambda= 1,\n                   )\n\n\nrf_ada = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n\ncolumns = X_train.columns\nX_ada = pd.DataFrame(data = X_ada, columns = columns)\n\nxg_fit = xg_ada.fit(X_ada,y_ada)\n\n\nrf_fit = rf_ada.fit(X_ada,y_ada)\n\ny_pred_xgb = xg_fit.predict(X_test)\n\n\ny_pred_rfb = rf_fit.predict(X_test)\n\ny_pred_xgb_prob = xg_fit.predict_proba(X_test)\ny_pred_rfb_prob = rf_fit.predict_proba(X_test)\n\np_rfb,r_rfb,f_rfb,_  = precision_recall_fscore_support(y_test,y_pred_rfb)\n\np_xgb,r_xgb,f_xgb,_  = precision_recall_fscore_support(y_test,y_pred_xgb)\n\n\nprint('Random Forest:\\n')\nprint('Accuracy - ',accuracy_score(y_test,y_pred_rfb))\nprint('Precision - \\n',p_rfb , 'Recall - \\n',r_rfb,'Fscore - \\n',f_rfb,_ )\nprint('Confusion Matrix - \\n',confusion_matrix(y_test,y_pred_rfb))\n\nprint('XGBoost:\\n')\nprint('Accuracy XGBoost',accuracy_score(y_test,y_pred_xgb))\nprint('Precision XGBoost:',p_xgb , 'Recall',r_xgb,'Fscore',f_xgb,_ )\nprint('Confusion Matrix - \\n',confusion_matrix(y_test,y_pred_xgb))\n","78d1aaaa":"'scale_pos_weight' is used for imbalanced dataset wherein it balancese the Negative and Positive weights. With the default value of '1', it implies that the positive class has a weight equal to the negative class. \n'max_delta_step' specifies maximum delta step to be allowed for each leaf output. It helps in logistic regression when data is highly imbalanced.\nHere is defined a range of scale_pos_weight & max_delta_step. The best combination of these parameters which will give a better accuracy, precision, recall and F score. At the same time keeping FN as minimum as possible.","9ec6fb3c":"<font color=blue size = 4.8>Loading Dataset<\/font>","7bde1eaa":"<font color=blue size = 4.8>ADAptive SYNthetic (ADASYN)<\/font>","54bd3ce0":"<font color=green>For Adasyn as well it is observed that Random Forest gives better results as compared to XGBoost<\/font>","d3e4a408":"<font color=green>It is observed that Random Forest gives better results as compared to XGBoost<\/font>","99b63461":"#**Credit Card Fraud Detection**","861f31ba":"<font color=blue size = 4.8>Modeling on imbalanced data<\/font>","fe176ab2":"<font color=blue size = 4.8>Synthetic Minority Over sampling Technique (SMOTE)<\/font>","16a04e33":"The dataset consists of credit cards transactions in September 2013 by European cardholders. It presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly imbalanced, the positive class (frauds) account for 0.172% of all transactions.\nIt consists of 28 Principle Components Analysis transformed features from V1 to V28.\nDue to confidentiality issue, no additional data of original features is provided; hence pre-analysis of features cannot be done.\n\nIn this kernel I have attempted to get the model which gives best performance while handling imbalanced data.\nModeling algorithms for balancing data used in the kernel (click on below links to go to respective sections)-\n - Modeling using Imbalanced data\n - SMOTE\n - ADASYN","4b2fcb12":"<font color=orange size =3>Modeling using balanced weights in Random Forest<\/font>","2bad2dba":"<font color=orange size = 3>Modeling using weights<\/font>","b48c2f94":"<font color=green>As we can see the data is highly imbalanced. Out of 2 lakh cases only 492 are fraud cases.<\/font>","88d3661a":"Significance of confusion matrixx that can be seen for this dataset -\n - True Positive: Number of fraud cases the model predicted as 'fraud'.\n - False Positive: Number of non-fraud cases that the model predicted as 'fraud'.\n - True Negative: Number of non-fraud cases that the model predicted as 'non-fraud'.\n - False Negative: Number of fraud cases that the model predicted as 'non-fraud'.\n - Precision: Precision is the ratio of correctly predicted fraud cases to total predicted fraud cases.\n\nHence we can focus on the model that gives least False Negative values.","454f1918":"<font color=green>We can observe that the combination of scale_pos_weight=10 and max_delta_step=10 gives us the best values for Precision, Recall and Fscore. Also the False negatives is also minimum : 29 <\/font>","7c07d225":"<font color=blue size = 4.8>Import necessary libraries<\/font>"}}