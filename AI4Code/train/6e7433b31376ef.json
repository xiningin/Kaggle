{"cell_type":{"8d486502":"code","408b99bd":"code","a5e3c2aa":"code","7454751a":"code","0248246e":"code","f8d9f210":"code","d9a27f59":"code","ca8a82ae":"code","310ef9a5":"code","4521cafc":"code","d84a989b":"code","7719c864":"code","0474b478":"code","13c2c5b7":"code","c8ad5383":"code","da943ab7":"code","0531f4e6":"code","fb6c4278":"code","83aa1080":"code","85097da9":"code","ab0cc101":"code","bbd97d51":"code","85860c62":"code","d3d3987a":"code","994aad6c":"code","eda61dbe":"code","c4c96990":"code","f097a2a7":"code","a07466cf":"code","bb3fd70a":"code","68c3ea0c":"code","a9d9e080":"code","db0df5b8":"code","48477576":"code","d892b0d8":"code","e9385f64":"code","7cc91719":"code","9704e54c":"code","c46dcf34":"markdown","de628c23":"markdown","98bdfa75":"markdown","1cf07161":"markdown","79c2a19f":"markdown","d9f04ce2":"markdown","d56b2d33":"markdown","e6171ae5":"markdown","43361d3f":"markdown","3fc3b355":"markdown","23d679cd":"markdown","b7fa3e8d":"markdown","848a968d":"markdown","5f8f8202":"markdown","eec56710":"markdown"},"source":{"8d486502":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd# data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","408b99bd":"data_raw=pd.read_csv('..\/input\/mushrooms.csv')","a5e3c2aa":"print(data_raw.shape)\ndata_raw.head()","7454751a":"le=preprocessing.LabelEncoder()\nfor column in data_raw.columns:\n    data_raw[column]=le.fit_transform(pd.Series(data_raw[column]))","0248246e":"#Shuffling the dataset\nnp.random.shuffle(data_raw.as_matrix())","f8d9f210":"data_raw.head()","d9a27f59":"X=data_raw.iloc[:,1:]\ny=data_raw['class'].values\n","ca8a82ae":"print(X.shape)\nprint(y.shape)","310ef9a5":"enc=OneHotEncoder()\nX=enc.fit_transform(X).toarray()","4521cafc":"X.shape","d84a989b":"#Count of each class\nprint(pd.Series(y).value_counts())","7719c864":"#Univariate Analysis\ndata_raw['cap-shape'].value_counts().sort_index().plot.bar()\nplt.xlabel('Cap-Shape')","0474b478":"data_raw['cap-surface'].value_counts().sort_index().plot.bar()\nplt.xlabel('Cap-Surface')","13c2c5b7":"data_raw['cap-color'].value_counts().sort_index().plot.bar()\nplt.xlabel('Cap-Color')","c8ad5383":"data_raw['bruises'].value_counts().sort_index().plot.bar()\nplt.xlabel('bruises')","da943ab7":"data_raw['odor'].value_counts().sort_index().plot.bar()\nplt.xlabel('Odour')","0531f4e6":"data_raw['gill-attachment'].value_counts().sort_index().plot.bar()\nplt.xlabel('Gill-Attachment')","fb6c4278":"data_raw['gill-spacing'].value_counts().sort_index().plot.bar()\nplt.xlabel('Gill-Spacing')","83aa1080":"data_raw['gill-size'].value_counts().sort_index().plot.bar()\nplt.xlabel('Gill-Size')","85097da9":"data_raw['gill-color'].value_counts().sort_index().plot.bar()\nplt.xlabel('Gill-Color')","ab0cc101":"data_raw['stalk-shape'].value_counts().sort_index().plot.bar()\nplt.xlabel('Stalk-Shape')","bbd97d51":"data_raw['stalk-root'].value_counts().sort_index().plot.bar()\nplt.xlabel('Stalk-Root')","85860c62":"#Checking for Duplicates\nX_df=pd.DataFrame(X)\ndup=X_df.duplicated()\ng=X_df[dup==True].index\nprint(g)","d3d3987a":"#Checking for missing values\ndata_raw.isnull().sum()","994aad6c":"#Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nestimators=[]\nestimators.append(('standardize',StandardScaler()))\nestimators.append(('logreg',LogisticRegression()))\nmodel=Pipeline(estimators)\nseed=4\nkfold=StratifiedKFold(n_splits=5,shuffle=True,random_state=seed)\nresults=cross_val_score(model,X,y,cv=kfold,scoring='accuracy')\nprint(results.mean())","eda61dbe":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=6)\nlogreg=LogisticRegression()\nfrom sklearn.metrics import confusion_matrix\nlogreg.fit(X_train,y_train)\ny_pred=logreg.predict(X_test)\nconfusion_matrix(y_test,y_pred)","c4c96990":"#Kneighbors Classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nresults=[]\nfor i in range(1,31):\n    estimators=[]\n    estimators.append(('standardize',StandardScaler()))\n    estimators.append(('kneighbors',KNeighborsClassifier(n_neighbors=i)))\n    model=Pipeline(estimators)\n    seed=1\n    kfold=StratifiedKFold(n_splits=5,random_state=seed)\n    results.append(cross_val_score(model,X,y,cv=kfold,scoring='accuracy').mean())\n    print('Result of k='+str(i)+\"  : \"+str(results[-1]))\n","f097a2a7":"import matplotlib.pyplot as plt\nplt.plot(np.arange(1,31),results)\nplt.show()","a07466cf":"#Naive Bayes Classifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.model_selection import GridSearchCV\nestimators=[]\nresults=0\nfrom sklearn.model_selection import StratifiedKFold\nestimators.append(('standardize',StandardScaler()))\nestimators.append(('naive_bayes',BernoulliNB()))\nmodel=Pipeline(estimators)\nseed=1\nparam_grid=[{'naive_bayes__alpha':[0.0001,0.001,0.003,0.01,0.03,0.1,0.3,1]}]\nkfold=StratifiedKFold(n_splits=5,shuffle=True,random_state=seed)\ngrid=GridSearchCV(model,param_grid,cv=kfold,scoring='accuracy')\ngrid.fit(X_train,y_train)\nprint(grid.best_score_)\nprint(grid.best_params_)\nprint(grid.best_estimator_)\nprint(accuracy_score(y_test,grid.predict(X_test)))\n","bb3fd70a":"model=BernoulliNB(alpha=0.00001)\nmodel.fit(X_train,y_train)\ny_test=model.predict(X_test)\naccuracy_score(y_test,y_pred)","68c3ea0c":"#SVM\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold\nestimators=[]\nresults=0\nestimators.append(('standardize',StandardScaler()))\nestimators.append(('svm',SVC(kernel='linear')))\nmodel=Pipeline(estimators)\nseed=1\nkfold=StratifiedKFold(n_splits=5,shuffle=True,random_state=seed)\nresults=cross_val_score(model,X,y,cv=kfold,scoring='accuracy')\nresults.mean()","a9d9e080":"svm=SVC(kernel='linear')\nsvm.fit(X_train,y_train)\ny_pred=svm.predict(X_test)\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred)","db0df5b8":"#LDA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nestimators=[]\nresults=0\nestimators.append(('standardize',StandardScaler()))\nestimators.append(('svm',LinearDiscriminantAnalysis()))\nmodel=Pipeline(estimators)\nseed=1\nkfold=StratifiedKFold(n_splits=5,shuffle=True,random_state=seed)\nresults=cross_val_score(model,X,y,cv=kfold,scoring='accuracy')\nresults.mean()","48477576":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nld=LinearDiscriminantAnalysis()\nld.fit(X_train,y_train)\ny_pred=ld.predict(X_test)\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred)","d892b0d8":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nestimators=[]\nresults=0\nfrom sklearn.model_selection import StratifiedKFold\n#estimators.append(('standardize',StandardScaler()))\nestimators.append(('dec_tree',DecisionTreeClassifier(random_state=23)))\nmodel=Pipeline(estimators)\nseed=1\nparam_grid=[{'dec_tree__max_depth':[2,3,5,7,9],'dec_tree__min_samples_leaf':[1,2,3,4,5,6],'dec_tree__max_features':['auto','sqrt',None]}]\nkfold=StratifiedKFold(n_splits=5,shuffle=True,random_state=seed)\ngrid=GridSearchCV(model,param_grid,cv=kfold,scoring='accuracy')\ngrid.fit(X_train,y_train)\nprint(grid.best_score_)\nprint(grid.best_params_)\nprint(grid.best_estimator_)\nprint(accuracy_score(y_test,grid.predict(X_test)))","e9385f64":"model=DecisionTreeClassifier()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\naccuracy_score(y_test,y_pred)","7cc91719":"#Random forest with Grid Search Cv\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nestimators=[]\nresults=0\nfrom sklearn.model_selection import StratifiedKFold\n#estimators.append(('standardize',StandardScaler()))\nestimators.append(('ran_for',RandomForestClassifier(random_state=23)))\nmodel=Pipeline(estimators)\nseed=1\nparam_grid=[{'ran_for__max_depth':[2,3,5,7,9],'ran_for__min_samples_leaf':[1,2,3,4,5,6],'ran_for__max_features':['auto','sqrt',None]}]\nkfold=StratifiedKFold(n_splits=5,shuffle=True,random_state=seed)\ngrid=GridSearchCV(model,param_grid,cv=kfold,scoring='accuracy')\ngrid.fit(X_train,y_train)\nprint(grid.best_score_)\nprint(grid.best_params_)\nprint(grid.best_estimator_)\nprint(accuracy_score(y_test,grid.predict(X_test)))","9704e54c":"#Random forest with Grid Search Cv\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nestimators=[]\nresults=0\nfrom sklearn.model_selection import StratifiedKFold\n#estimators.append(('standardize',StandardScaler()))\nestimators.append(('xgboost',GradientBoostingClassifier(random_state=23)))\nmodel=Pipeline(estimators)\nseed=1\nparam_grid=[{'xgboost__max_depth':[2,3,5],'xgboost__min_samples_leaf':[1,2,3,4,5,6],'xgboost__max_features':['auto','sqrt',None]}]\nkfold=StratifiedKFold(n_splits=5,shuffle=True,random_state=seed)\ngrid=GridSearchCV(model,param_grid,cv=kfold,scoring='accuracy')\ngrid.fit(X_train,y_train)\nprint(grid.best_score_)\nprint(grid.best_params_)\nprint(grid.best_estimator_)\nprint(accuracy_score(y_test,grid.predict(X_test)))","c46dcf34":"**BERNOULLI  NAIVE BAYES**","de628c23":"> DATA EXPLORATION","98bdfa75":"*There are no missing values in the dataset*","1cf07161":"**LOGISTIC REGRESSION**","79c2a19f":"**SVM**","d9f04ce2":"**KNN**","d56b2d33":"> DATA CLEANING","e6171ae5":"**RANDOM FOREST**","43361d3f":"**XGBOOST**","3fc3b355":"**LINEAR DISCRIMINANT ANALYSIS**","23d679cd":"**DECISION TREE**","b7fa3e8d":"> DEPLOYING DIFFERENT MODELS","848a968d":"*Thus, each model performs with an accuracy of almost 99 % . It is mainly because the data is too clean is too organised*","5f8f8202":"*So there are no duplicates*","eec56710":"*Since the classes are not skewed,so we may choose to use accuracy as our evaluation metric*"}}