{"cell_type":{"0d116447":"code","dc9ccf64":"code","1354c625":"code","50088ded":"code","bf06288d":"code","8a590a19":"code","1a888b78":"code","95301503":"code","20eff24b":"code","7862f3ae":"code","a3c3ae1f":"code","f869b43f":"code","77dc6125":"code","379fe345":"code","d2c2c362":"code","e11e3273":"code","1db9701f":"code","7f60e67c":"code","1a8ffdf7":"code","eb6206a0":"code","9635524a":"code","54adbe58":"code","680b491a":"code","78b5c1f5":"code","f39f66a3":"code","d6c4cbc6":"code","c87983c8":"code","37eb966b":"code","5ce32f8c":"code","33c2ca6b":"markdown","b3b002ef":"markdown","32ec4762":"markdown","b9fe610e":"markdown","83b2967a":"markdown","5f64d21c":"markdown","1117f927":"markdown","8f31818f":"markdown","27bb3aa3":"markdown","282fa674":"markdown","d27f667a":"markdown","8612b0f6":"markdown","95eb1020":"markdown","d9dcd5ff":"markdown","9402e18e":"markdown","e5cfb1b7":"markdown","b5a11909":"markdown","c5c3d9b9":"markdown","367a7c9f":"markdown","9d4679b4":"markdown"},"source":{"0d116447":"# Import libraires \nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","dc9ccf64":"data = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\") #Load the dataset","1354c625":"data.describe()","50088ded":"data.head(10)","bf06288d":"data.tail(10) ","8a590a19":"data.isnull().sum()\n#There are no missing values in the dataset but there are 0 values.","1a888b78":"plt.figure(figsize=(10,8))\nsns.heatmap(data.corr(),annot=True,linewidths=3)\n\nplt.show()","95301503":"for col in data.columns:\n    if col !=\"Pregnancies\" and col !=\"DiabetesPedigreeFunction\" and col !=\"Outcome\":\n        data[col] = data[col].replace(0,np.nan)\n\n#We have replaced the inaccure values with nan values.Now let's look for the missing values        \ntotal_missing_values = pd.DataFrame(data.isnull().sum()) # count the percentage of misisng values\n\npercentage_mising_values = pd.DataFrame(data.isnull().sum() \/ data.shape[0] * 100) #Compute percentage of missing values\n\nmissing_df = pd.DataFrame(index=data.columns,columns = ['Total missing values','Total missing values %'])\nmissing_df['Total missing values'] = total_missing_values\nmissing_df['Total missing values %'] = percentage_mising_values \nmissing_df","20eff24b":"data.drop(['Insulin','SkinThickness'],inplace=True,axis=1)","7862f3ae":"data.hist(figsize=(10,10),grid=[3,3])\nplt.show()","a3c3ae1f":"data['BMI'].fillna(data['BMI'].mean(),inplace=True)\ndata['Glucose'].fillna(data['Glucose'].mean(),inplace=True)\ndata['BloodPressure'].fillna(data['BloodPressure'].mean(),inplace=True)","f869b43f":"data.hist(figsize=(10,10),grid=[4,4])\nplt.show()","77dc6125":"sns.distplot(data['Age'])\nplt.show()","379fe345":"sns.boxplot(x='Outcome',y='Age',data=data)\nplt.show()","d2c2c362":"data[data['Age'] > 68]","e11e3273":"data.drop(data[data['Age'] > 68].Age,inplace=True)","1db9701f":"sns.boxplot(y='DiabetesPedigreeFunction',x='Outcome',data=data)\nplt.show()","7f60e67c":"sns.boxplot(y='Pregnancies',x='Outcome',data=data)\nplt.show()","1a8ffdf7":"data.drop(data[data['Pregnancies'] >10].index,inplace=True)","eb6206a0":"data['Outcome'].value_counts()","9635524a":"sns.distplot(data['Outcome'])\nplt.show()","54adbe58":"from sklearn.utils import resample\nfrom imblearn.over_sampling import SMOTE\n\nimbalanced = data.copy(deep=True)\ndownsampled = data.copy(deep=True)\nsynthetic_dataset = data.copy(deep=True)\n\n#Before we down sample the data we should split the data based on class labels.\nnot_diabetic = downsampled[downsampled.Outcome==0]\ndiabetic = downsampled[downsampled.Outcome==1]\n\nnot_diabetic_downsampled = resample(not_diabetic,\n                                replace = False, # sample without replacement\n                                n_samples = len(diabetic), # match minority n\n                                random_state = 27) # reproducible results\n\n# combine minority and downsampled majority\ndownsampled  = pd.concat([not_diabetic_downsampled, diabetic])\n\n#split the data into x and y\nsynthetic_dataset_X = synthetic_dataset.iloc[:,:-1]\nsynthetic_dataset_y = synthetic_dataset.iloc[:,-1]\nsm = SMOTE(random_state = 10) \n#Fit the SMOTE model to the data\nsynthetic_X, synthetic_y = sm.fit_sample(synthetic_dataset_X, synthetic_dataset_y.ravel()) \ndf_synthetic_dataset_y = pd.DataFrame(columns=['Outcome'])\ndf_synthetic_dataset_y['Outcome'] = synthetic_y\nsynthetic_dataset_X = pd.DataFrame(synthetic_X, columns=synthetic_dataset.columns[:-1])\n#Concat X and Y again into singel dataset\nsynthetic_dataset = pd.concat([synthetic_dataset_X,df_synthetic_dataset_y],axis=1) \n\nprint(\"Class labels of imbalanced dataset has {} 0s and {} 1s.\\n\".format(imbalanced['Outcome'].value_counts()[0],imbalanced['Outcome'].value_counts()[1]))\nprint(\"Class labels of downsampled dataset has {} 0s and {} 1s.\\n\".format(downsampled['Outcome'].value_counts()[0],downsampled['Outcome'].value_counts()[1]))\nprint(\"Class labels of synthetic dataset has {} 0s and {} 1s.\\n\".format(synthetic_dataset['Outcome'].value_counts()[0],synthetic_dataset['Outcome'].value_counts()[1]))","680b491a":"plt.figure(figsize=(10,5))\nsns.pairplot(data=data,hue='Outcome')\nplt.show()","78b5c1f5":"plt.figure(figsize=(10,8))\nsns.heatmap(data.corr(),annot=True,linewidths=3)\n\nplt.show()","f39f66a3":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,f1_score,confusion_matrix,classification_report\n\ndatasets = [imbalanced,downsampled,synthetic_dataset]\ndataset_names = ['imbalanced','downsampled','synthetic_dataset']\nmodels = [KNeighborsClassifier,LogisticRegression,RandomForestClassifier]\n\ncolumns = ['Dataset','Model','accuray_score','f1_score','TN','FP','FN','TP']\nModel_details = pd.DataFrame(columns=columns)\n\n\n\nfor dataset_name in dataset_names:\n    index_dataset = dataset_names.index(dataset_name)\n    frame = {}\n    for model in models:\n        frame['Dataset'] = dataset_name\n        frame['Model'] = model.__name__\n        dataset = datasets[index_dataset]\n        X= dataset.iloc[:,:-1]\n        y = dataset.iloc[:,-1]\n        \n        #scale the values\n        sc_X = StandardScaler()\n        X = sc_X.fit_transform(X)\n        #split the dataset into train and test\n        X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42, stratify=y)\n        clf = model()\n        clf.fit(X_train,y_train)\n        \n        y_pred = clf.predict(X_test)\n        \n        frame['f1_score'] = f1_score(y_test,y_pred)\n        frame['accuray_score'] = accuracy_score(y_test,y_pred)\n        frame['TN'] = confusion_matrix(y_test,y_pred)[0][0]\n        frame['FP'] = confusion_matrix(y_test,y_pred)[0][1]\n        frame['FN'] = confusion_matrix(y_test,y_pred)[1][0]\n        frame['TP'] = confusion_matrix(y_test,y_pred)[1][1]\n        Model_details = Model_details.append(frame,ignore_index=True)","d6c4cbc6":"Model_details","c87983c8":"from sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\"n_neighbors\": np.arange(1, 25),'weights':['uniform','distance']}\n\n\nX= downsampled.iloc[:,:-1]\ny = downsampled.iloc[:,-1]\n        \n#scale the values\nsc_X = StandardScaler()\nX = sc_X.fit_transform(X)\n#split the dataset into train and test\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42, stratify=y)\n\nclf = KNeighborsClassifier()\nclf_cv= GridSearchCV(clf,param_grid,cv=3,n_jobs=-1)\nclf_cv.fit(X,y)","37eb966b":"clf_cv.best_params_","5ce32f8c":"clf_cv.best_score_","33c2ca6b":"## Bivariate analysis","b3b002ef":"All the feautres in the data are in different scale. So we have to scale the values.","32ec4762":"BMI,Glucose and BloodPressure columns have missing values. From above plot we can see that these features follow so what a normal distribution. We can impute the missing values with the mean of the columns","b9fe610e":"We can see that 12 is the optimal value for \u2018n_neighbors\u2019 and distance is the weight metric. We can use the \u2018best_score_\u2019 function to check the accuracy of our model when \u2018n_neighbors\u2019 is 12. \u2018best_score_\u2019 outputs the mean accuracy of the scores obtained through cross-validation.","83b2967a":"From above result clearly KNeighborsClassifier is the better model.\nObserve that though accuracy score is high for imbalanced dataset but the f1_score is low.\nSynthetic_dataset and downsampled dataset achived high accuray score as well as f1 score.","5f64d21c":"When we look at the data.describe method we can see Insulin,Gluscose,Blood Pressure,SkinThickness and BMI are 0 \nBut they can't be zero. They zero values could be be becuase of missing values\/data corruption\nBeofer we jump to any conclusion let see the count of missing values in the datset","1117f927":"Let's take a look at basic statisics of the data like mean,min,max,standard deviation,percentile of the data","8f31818f":"Before imputing the missing values let's the distribtion of the cloumns","27bb3aa3":"From the above table we can see that min values for most of the columns are 0. That's highlhy unlikely. Let's look at some of the top and bottom rows in the data","282fa674":"From above heat map we can see Gloucose and BMI are one the feautre which is highly corelated with Outcome","d27f667a":"## Modle Selection","8612b0f6":"As a rule of thumb if any feature have more than 30 % of missing data we can drop it becuase imputing the missing values for these columns will not add much value while using mean,median or mode methods. Having said that we can use some advnaced technquies like KNN or Random forest regressor to predict the missing values (Here the column with missing values will become our dependant variable). But in our cases the correlation between SkinThickness,Insulin is very less so we can drop them.","95eb1020":"Breif summary about this in Notebook.Below are few topcis we will cover in this notebook.\n* Step by Step EDA\n* Handel imbalanced dataset\n* Model Selection\n* Hyperparameter tuning\n    \n","d9dcd5ff":"### Hyper parameter tuning","9402e18e":"\nThis dataset is imblanced . That means there are more rows with 0 label than 1.This will bias the prediction model towards the more common class.\nTake a look at https:\/\/towardsdatascience.com\/methods-for-dealing-with-imbalanced-data-5b761be45a18 to under more on imbalanced \ndataset and how it will effect the model performance\nIn this note book we will measure the model performance on Orginal, downsampled and syntheticaly generated sampled datasets","e5cfb1b7":"Let's replace the 0 values in datasent  with numpy Nan.\nIn later part of the notebook will see how to repalce the incorrect\/missing values","b5a11909":"## Univariate analysis\n\nLet's see the distribution of the feautes once again after imputation","c5c3d9b9":"Having more than 10 pregnancies is higlhy unlikely.This could be beacuse of typo error.Let's remove them","367a7c9f":"Age,DiabetesPedigreeFunction and Pregnanices featues seems to be swked. Let anaylse these feautes","9d4679b4":"Let's take a closer look at the dependant variable"}}