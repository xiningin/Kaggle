{"cell_type":{"0614da95":"code","404d4368":"code","194d8185":"code","e21be1ae":"code","e2a7a581":"code","00cc4970":"code","1f8eb202":"code","b2da12ae":"code","ccbcf611":"code","74978178":"code","28ed04da":"code","39d6373a":"code","86b44e54":"code","cf639c3f":"code","6c3dba65":"code","d220bbfb":"code","c37fdddc":"code","8e4eff78":"code","574763c5":"code","a7439f0d":"code","4e555c3b":"code","2679df6d":"code","2bb5af4c":"code","e484a8d1":"code","755605ee":"code","fd6ecdaf":"code","1f1310a4":"code","16095866":"code","345f12bd":"code","06ec32c4":"code","a0492826":"code","36de2ab6":"code","100ca47b":"code","54eeb1fd":"code","5170cb18":"code","b808f754":"code","c87014c5":"code","8f723ecd":"code","8821a87d":"code","c42bca2d":"code","b89dfd23":"code","de9e7924":"code","1897f7fe":"code","854d9fa6":"code","18c31cc7":"code","849fbabc":"code","dda4293e":"code","077a52e8":"code","107f624f":"code","2dc31abd":"code","8a50cd4c":"code","3c751471":"code","7d6aafd5":"code","1a766a45":"code","d7fd8fd1":"code","6742e881":"code","3ef995a8":"code","e4ccb47c":"code","30e5b30d":"code","7029f962":"code","813a9ace":"code","2211c9e2":"code","e50455fc":"code","2ea96fb7":"code","1d59d854":"code","db8f4c3a":"code","8f5ff74a":"code","c9fda1f0":"code","b1cc86eb":"markdown","59f06346":"markdown","af8f3bd1":"markdown","3b6747d1":"markdown","7be62bc3":"markdown","fe62069b":"markdown","2a7121d9":"markdown","36838123":"markdown","5f85643b":"markdown","51267121":"markdown","8de551cb":"markdown","66bc93f0":"markdown","2f89ce05":"markdown","2fd79013":"markdown","9ee9274c":"markdown","c7a26c67":"markdown","cdfdae60":"markdown","cc8ca763":"markdown","fdefa64e":"markdown","6217e70a":"markdown","a650a3cc":"markdown","376c2d64":"markdown","25c07574":"markdown","70a0db35":"markdown","0d3e649d":"markdown","3fc36932":"markdown","f4d227ce":"markdown","dc817525":"markdown","b784c751":"markdown","2dad27b2":"markdown","dd341e60":"markdown","4811b2ab":"markdown","4141996a":"markdown","74857d5d":"markdown","65a50848":"markdown","9c152cc6":"markdown","c870861e":"markdown","f847708e":"markdown","2bcea67b":"markdown","f37363a3":"markdown","9778505e":"markdown","94473e13":"markdown","e63cff39":"markdown","191bb0a9":"markdown","3347921b":"markdown","6a41da7c":"markdown","cb3ed5ae":"markdown","d1e869d7":"markdown","fc22f6cb":"markdown","43e77e02":"markdown","e28bcd6a":"markdown","a0cedba7":"markdown","2cf384ea":"markdown","c4c0a1a2":"markdown","56a76b09":"markdown"},"source":{"0614da95":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport matplotlib.pyplot as plt\n%matplotlib inline","404d4368":"from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,BaggingClassifier,ExtraTreesClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score\nfrom sklearn import metrics\nfrom datetime import datetime\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import StratifiedKFold","194d8185":"data = pd.read_csv(\"..\/input\/adult.csv\")\ndata.head()","e21be1ae":"data.shape","e2a7a581":"attrib, counts = np.unique(data['workclass'], return_counts = True)\nmost_freq_attrib = attrib[np.argmax(counts, axis = 0)]\ndata['workclass'][data['workclass'] == '?'] = most_freq_attrib \n\nattrib, counts = np.unique(data['occupation'], return_counts = True)\nmost_freq_attrib = attrib[np.argmax(counts, axis = 0)]\ndata['occupation'][data['occupation'] == '?'] = most_freq_attrib \n\nattrib, counts = np.unique(data['native-country'], return_counts = True)\nmost_freq_attrib = attrib[np.argmax(counts, axis = 0)]\ndata['native-country'][data['native-country'] == '?'] = most_freq_attrib ","00cc4970":"data.head()","1f8eb202":"hs_grad = ['HS-grad','11th','10th','9th','12th']\nelementary = ['1st-4th','5th-6th','7th-8th']\n\n# replace elements in list.\ndata['education'].replace(to_replace = hs_grad,value = 'HS-grad',inplace = True)\ndata['education'].replace(to_replace = elementary,value = 'elementary_school',inplace = True)\n\ndata['education'].value_counts()","b2da12ae":"married= ['Married-spouse-absent','Married-civ-spouse','Married-AF-spouse']\nseparated = ['Separated','Divorced']\n\n#replace elements in list.\ndata['marital-status'].replace(to_replace = married ,value = 'Married',inplace = True)\ndata['marital-status'].replace(to_replace = separated,value = 'Separated',inplace = True)\n\ndata['marital-status'].value_counts()","ccbcf611":"self_employed = ['Self-emp-not-inc','Self-emp-inc']\ngovt_employees = ['Local-gov','State-gov','Federal-gov']\n\n#replace elements in list.\ndata['workclass'].replace(to_replace = self_employed ,value = 'Self_employed',inplace = True)\ndata['workclass'].replace(to_replace = govt_employees,value = 'Govt_employees',inplace = True)\n\ndata['workclass'].value_counts()","74978178":"del_cols = ['relationship','educational-num']\ndata.drop(labels = del_cols,axis = 1,inplace = True)","28ed04da":"# drop rows with age 90\nprint(\"Number of observation before removing:\",data.shape)\nindex_age = data[data['age'] == 90].index\ndata.drop(labels = index_age,axis = 0,inplace =True)\nprint(\"Number of observation after removing:\",data.shape)","39d6373a":"print(\"Number of observation before removing:\",data.shape)\nindex_gain = data[data['capital-gain'] == 99999].index\ndata.drop(labels = index_gain,axis = 0,inplace =True)\nprint(\"Number of observation after removing:\",data.shape)","86b44e54":"num_col_new = ['age','capital-gain', 'capital-loss',\n       'hours-per-week','fnlwgt']\ncat_col_new = ['workclass', 'education', 'marital-status', 'occupation',\n               'race', 'gender', 'income']","cf639c3f":"from sklearn.pipeline import Pipeline\nfrom sklearn.base import TransformerMixin\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\n\nscaler = MinMaxScaler()\npd.DataFrame(scaler.fit_transform(data[num_col_new]),columns = num_col_new).head(5)","6c3dba65":"class DataFrameSelector(TransformerMixin):\n    def __init__(self,attribute_names):\n        self.attribute_names = attribute_names\n                \n    def fit(self,X,y = None):\n        return self\n    \n    def transform(self,X):\n        return X[self.attribute_names]\n    \n    \nclass num_trans(TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        df = pd.DataFrame(X)\n        df.columns = num_col_new \n        return df\n        \n    \n    \npipeline = Pipeline([('selector',DataFrameSelector(num_col_new)),  \n                     ('scaler',MinMaxScaler()),\n                    ('transform',num_trans())])","d220bbfb":"num_df = pipeline.fit_transform(data)\nnum_df.shape","c37fdddc":"# columns which I don't need after creating dummy variables dataframe\ncols = ['workclass_Govt_employess','education_Some-college',\n        'marital-status_Never-married','occupation_Other-service',\n        'race_Black','gender_Male','income_>50K']","8e4eff78":"class dummies(TransformerMixin):\n    def __init__(self,cols):\n        self.cols = cols\n    \n    def fit(self,X,y = None):\n        return self\n    \n    def transform(self,X):\n        df = pd.get_dummies(X)\n        df_new = df[df.columns.difference(cols)] \n#difference returns the original columns, with the columns passed as argument removed.\n        return df_new\n\npipeline_cat=Pipeline([('selector',DataFrameSelector(cat_col_new)),\n                      ('dummies',dummies(cols))])\ncat_df = pipeline_cat.fit_transform(data)\ncat_df.shape","574763c5":"cat_df['id'] = pd.Series(range(cat_df.shape[0]))\nnum_df['id'] = pd.Series(range(num_df.shape[0]))","a7439f0d":"final_df = pd.merge(cat_df,num_df,how = 'inner', on = 'id')\nprint(f\"Number of observations in final dataset: {final_df.shape}\")","4e555c3b":"y = final_df['income_<=50K']\nfinal_df.drop(labels = ['id','income_<=50K','fnlwgt'],axis = 1,inplace = True)\nX = final_df","2679df6d":"sns.countplot(x=\"income\", data= data)\nplt.show()\ndata[\"income\"].value_counts()","2bb5af4c":"from imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler()\nX_rus, y_rus = rus.fit_sample(X, y)","e484a8d1":"X_rus = pd.DataFrame(X_rus)\nX_rus.columns = ['education_Assoc-acdm', 'education_Assoc-voc', 'education_Bachelors',\n       'education_Doctorate', 'education_HS-grad', 'education_Masters',\n       'education_Preschool', 'education_Prof-school',\n       'education_elementary_school', 'gender_Female',\n       'marital-status_Married', 'marital-status_Separated',\n       'marital-status_Widowed', 'occupation_Adm-clerical',\n       'occupation_Armed-Forces', 'occupation_Craft-repair',\n       'occupation_Exec-managerial', 'occupation_Farming-fishing',\n       'occupation_Handlers-cleaners', 'occupation_Machine-op-inspct',\n       'occupation_Priv-house-serv', 'occupation_Prof-specialty',\n       'occupation_Protective-serv', 'occupation_Sales',\n       'occupation_Tech-support', 'occupation_Transport-moving',\n       'race_Amer-Indian-Eskimo', 'race_Asian-Pac-Islander', 'race_Other',\n       'race_White', 'workclass_Govt_employees', 'workclass_Never-worked',\n       'workclass_Private', 'workclass_Self_employed', 'workclass_Without-pay',\n       'age', 'capital-gain', 'capital-loss', 'hours-per-week']\ny_rus = pd.DataFrame(y_rus)\ny_rus.columns = [\"income\"]","755605ee":"X_train,X_test,y_train,y_test = train_test_split(X_rus,y_rus,test_size =0.25,random_state = 42)","fd6ecdaf":"X_train.shape","1f1310a4":"# Spot-Check Algorithms\ndef GetBasedModel():\n    basedModels = []\n    basedModels.append(('LR'   , LogisticRegression()))\n    basedModels.append(('LDA'  , LinearDiscriminantAnalysis()))\n    basedModels.append(('KNN'  , KNeighborsClassifier()))\n    basedModels.append(('CART' , DecisionTreeClassifier()))\n    basedModels.append(('NB'   , GaussianNB()))\n    basedModels.append(('AB'   , AdaBoostClassifier()))\n    basedModels.append(('GBM'  , GradientBoostingClassifier()))\n    basedModels.append(('RF'   , RandomForestClassifier()))\n    basedModels.append(('ET'   , ExtraTreesClassifier()))\n\n    \n    return basedModels","16095866":"def BasedLine2(X_train, y_train,models):\n    # Test options and evaluation metric\n    num_folds = 3\n    scoring = 'accuracy'\n\n    results = []\n    names = []\n    for name, model in models:\n        kfold = StratifiedKFold(n_splits=num_folds, random_state=10)\n        cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n        \n    return names,results","345f12bd":"class PlotBoxR(object):\n    \n    \n    def __Trace(self,nameOfFeature,value): \n    \n        trace = go.Box(\n            y=value,\n            name = nameOfFeature,\n            marker = dict(\n                color = 'rgb(0, 128, 128)',\n            )\n        )\n        return trace\n\n    def PlotResult(self,names,results):\n        \n        data = []\n\n        for i in range(len(names)):\n            data.append(self.__Trace(names[i],results[i]))\n\n\n        py.iplot(data)","06ec32c4":"models = GetBasedModel()\nnames,results = BasedLine2(X_train, y_train,models)","a0492826":"py.init_notebook_mode(connected=True)\nPlotBoxR().PlotResult(names,results)","36de2ab6":"def ScoreDataFrame(names,results):\n    def floatingDecimals(f_val, dec=3):\n        prc = \"{:.\"+str(dec)+\"f}\" \n    \n        return float(prc.format(f_val))\n\n    scores = []\n    for r in results:\n        scores.append(floatingDecimals(r.mean(),4))\n\n    scoreDataFrame = pd.DataFrame({'Model':names, 'Score': scores})\n    return scoreDataFrame\nbasedLineScore = ScoreDataFrame(names,results)\nbasedLineScore.sort_values(by='Score', ascending=False)","100ca47b":"from sklearn.model_selection import GridSearchCV\nlr = LogisticRegression(class_weight='balanced',random_state=42)\nparam_grid = { \n    'C': [0.1,0.2,0.3,0.4],\n    'penalty': ['l1', 'l2'],\n    'class_weight':[{0: 1, 1: 1},{ 0:0.67, 1:0.33 },{ 0:0.75, 1:0.25 },{ 0:0.8, 1:0.2 }]}\nCV_rfc = GridSearchCV(estimator=lr, param_grid=param_grid, cv= 5)\nCV_rfc.fit(X_train, y_train)\nCV_rfc.best_params_","54eeb1fd":"#fitting the model\nlr1=LogisticRegression(C=0.4, random_state=4 ,penalty='l1', class_weight={0:1,1:1})\nlr1.fit(X_train,y_train)","5170cb18":"# predict \ny_pred1=lr1.predict(X_test)","b808f754":"cf_matrix = confusion_matrix(y_test, y_pred1)\nsns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","c87014c5":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred1))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred1))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred1))\nprint(\"F1 score:\",metrics.f1_score(y_test, y_pred1))","8f723ecd":"fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred1)\nauc = metrics.roc_auc_score(y_test, y_pred1)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","8821a87d":"from sklearn.model_selection import RandomizedSearchCV\nlr = LogisticRegression(class_weight='balanced',random_state=42)\nparam_grid = { \n    'C': [0.1,0.2,0.3,0.4, .5],\n    'penalty': ['l1', 'l2'],\n    'class_weight':[{0: 1, 1: 1},{ 0:0.67, 1:0.33 },{ 0:0.75, 1:0.25 },{ 0:0.8, 1:0.2 }]}\nCV_rfc = RandomizedSearchCV(estimator=lr, param_distributions=param_grid, cv= 5)\nCV_rfc.fit(X_train, y_train)\nCV_rfc.best_params_","c42bca2d":"#fitting the model\nlr2=LogisticRegression(C=0.4, random_state=4 ,penalty='l2', class_weight={0:1,1:1})\nlr2.fit(X_train,y_train)","b89dfd23":"# predict \ny_pred2=lr2.predict(X_test)\n","de9e7924":"cf_matrix = confusion_matrix(y_test, y_pred2)\nsns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","1897f7fe":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred2))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred2))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred2))\nprint(\"F1 score:\",metrics.f1_score(y_test, y_pred2))","854d9fa6":"fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred2)\nauc = metrics.roc_auc_score(y_test, y_pred2)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","18c31cc7":"from sklearn.feature_selection import RFE\n\n# feature extraction\nlr = LogisticRegression()\nrfe = RFE(lr, 15)\nlr3 = rfe.fit(X_train, y_train)\n\nprint(\"Num Features: \", lr3.n_features_)\nprint(\"Selected Features: \",  lr3.support_)\nprint(\"Feature Ranking: \", lr3.ranking_)","849fbabc":"feature = list(X_train.columns.values) \nprint(sorted(zip(map(lambda x: round(x, 4), lr3.ranking_), feature)))","dda4293e":"X_train_f = X_train[['education_Bachelors','education_Doctorate','education_Masters','education_Preschool',\n                     'education_Prof-school','education_elementary_school','marital-status_Married',\n                     'marital-status_Separated','marital-status_Widowed','occupation_Exec-managerial',\n                     'occupation_Prof-specialty','occupation_Protective-serv','occupation_Sales','occupation_Tech-support']]\n\nX_test_f = X_test[['education_Bachelors','education_Doctorate','education_Masters','education_Preschool',\n                     'education_Prof-school','education_elementary_school','marital-status_Married',\n                     'marital-status_Separated','marital-status_Widowed','occupation_Exec-managerial',\n                  'occupation_Prof-specialty','occupation_Protective-serv','occupation_Sales','occupation_Tech-support']]\n\nlr4=LogisticRegression(C=0.4, random_state=4 ,penalty='l2', class_weight={0:1,1:1})\nlr4.fit(X_train_f,y_train)","077a52e8":"# predict \ny_pred4=lr4.predict(X_test_f)\ncf_matrix = confusion_matrix(y_test, y_pred4)\nsns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred4))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred4))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred4))\nprint(\"F1 score:\",metrics.f1_score(y_test, y_pred4))\n\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred4)\nauc = metrics.roc_auc_score(y_test, y_pred4)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","107f624f":"from sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X_train)\n\n\nfrom sklearn.decomposition import PCA as sklearnPCA\n\nsklearn_pca = sklearnPCA(n_components=39)\nY_sklearn = sklearn_pca.fit_transform(X_std)\n\ncum_sum = sklearn_pca.explained_variance_ratio_.cumsum()\n\nsklearn_pca.explained_variance_ratio_[:10].sum()\n\ncum_sum = cum_sum*100\n\nfig, ax = plt.subplots(figsize=(8,8))\nplt.bar(range(39), cum_sum, label='Cumulative _Sum_of_Explained _Varaince', color = 'b',alpha=0.5)","2dc31abd":"#Cumulative explained variance\nfrom sklearn.decomposition import PCA\npca = PCA(39)\npca_full = pca.fit(X)\n\nplt.plot(np.cumsum(pca_full.explained_variance_ratio_))\nplt.xlabel('# of components')\nplt.ylabel('Cumulative explained variance')","8a50cd4c":"# 26 Principal Components seems good \n\npca = PCA(n_components=26)\nX_transformed = pca.fit_transform(X_train)\n\nX_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split( \n    X_transformed, y_train, test_size=0.2, random_state=13)\n\nlr5=LogisticRegression(C=0.4, random_state=4 ,penalty='l1', class_weight={0:1,1:1})\nlr5.fit(X_train_pca, y_train_pca)\n\n# predict \ny_pred =lr5.predict(X_test_pca)\n\ncf_matrix = confusion_matrix(y_test_pca, y_pred)\nsns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test_pca, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test_pca, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test_pca, y_pred))\nprint(\"F1 score:\",metrics.f1_score(y_test_pca, y_pred))\n\nfpr, tpr, _ = metrics.roc_curve(y_test_pca,  y_pred)\nauc = metrics.roc_auc_score(y_test_pca, y_pred)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","3c751471":"data1 = pd.DataFrame(X,y)\n\nclf = ExtraTreesClassifier(n_estimators=250,\n                              random_state=2)\n\nclf.fit(X_train, y_train)\n\n\n# Plot feature importance\nfeature_importance = clf.feature_importances_\n# make importances relative to max importance\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.figure(figsize=(10,10))\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, data1.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()","7d6aafd5":"X_train_ef = X_train[['race_White','education_Doctorate', 'marital-status_Separated','education_elementary_school', 'occupation_Prof-specialty',\n                      'education_Prof-school', 'capital-loss', 'education_Masters','education_Bachelors', 'education_HS-grad','occupation_Exec-managerial',\n                    'gender_Female', 'capital-gain','marital-status_Married', 'hours-per-week', 'age']]\nX_test_ef = X_test[['race_White','education_Doctorate', 'marital-status_Separated','education_elementary_school', 'occupation_Prof-specialty',\n                      'education_Prof-school', 'capital-loss', 'education_Masters','education_Bachelors', 'education_HS-grad','occupation_Exec-managerial',\n                    'gender_Female', 'capital-gain','marital-status_Married', 'hours-per-week', 'age']]","1a766a45":"lr5=LogisticRegression(C=0.4, random_state=4 ,penalty='l1', class_weight={0:1,1:1})\nlr5.fit(X_train_ef, y_train)\n\n# predict \ny_pred =lr5.predict(X_test_ef)\n\ncf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\nprint(\"F1 score:\",metrics.f1_score(y_test, y_pred))\n\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred)\nauc = metrics.roc_auc_score(y_test, y_pred)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","d7fd8fd1":"class GridSearch(object):\n    \n    def __init__(self,X_train,y_train,model,hyperparameters):\n        \n        self.X_train = X_train\n        self.y_train = y_train\n        self.model = model\n        self.hyperparameters = hyperparameters\n        \n    def GridSearch(self):\n        # Create randomized search 10-fold cross validation and 100 iterations\n        cv = 10\n        clf = GridSearchCV(self.model,\n                                 self.hyperparameters,\n                                 cv=cv,\n                                 verbose=0,\n                                 n_jobs=-1\n                                 )\n        # Fit randomized search\n        best_model = clf.fit(self.X_train, self.y_train)\n        message = (best_model.best_score_, best_model.best_params_)\n        print(\"Best: %f using %s\" % (message))\n\n        return best_model,best_model.best_params_\n    \n    def Best_Model_Predict(self,X_test):\n        \n        best_model,_ = self.GridSearch()\n        pred = best_model.predict(X_test)\n        return pred","6742e881":"from scipy.stats import uniform\n\nclass RandomSearch(object):\n    \n    def __init__(self,X_train,y_train,model,hyperparameters):\n        \n        self.X_train = X_train\n        self.y_train = y_train\n        self.model = model\n        self.hyperparameters = hyperparameters\n        \n    def RandomSearch(self):\n        # Create randomized search 10-fold cross validation and 100 iterations\n        cv = 10\n        clf = RandomizedSearchCV(self.model,\n                                 self.hyperparameters,\n                                 random_state=1,\n                                 n_iter=100,\n                                 cv=cv,\n                                 verbose=0,\n                                 n_jobs=-1\n                                 )\n        # Fit randomized search\n        best_model = clf.fit(self.X_train, self.y_train)\n        message = (best_model.best_score_, best_model.best_params_)\n        print(\"Best: %f using %s\" % (message))\n\n        return best_model,best_model.best_params_\n    \n    def Best_Model_Predict(self,X_test):\n        \n        best_model,_ = self.RandomSearch()\n        pred = best_model.predict(X_test)\n        return pred","3ef995a8":"k_range = list(range(2,15))\nd_metric = ['euclidean','minkowski']\n\nparam_grid = dict(n_neighbors = k_range, metric =d_metric)\n\nknn = KNeighborsClassifier()\n\nKNN_GridSearch = GridSearch(X_train_f, y_train, knn ,param_grid)\ny_pred = KNN_GridSearch.Best_Model_Predict(X_test_f)","e4ccb47c":"cf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\nprint(\"F1 score:\",metrics.f1_score(y_test, y_pred))\n\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred)\nauc = metrics.roc_auc_score(y_test, y_pred)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","30e5b30d":"param_grid = [{'gamma': [ 0.1, 1, 10],'C': [ 0.10, 10, 100]}]\n\nsvm = SVC()\n\nsvm_GridSearch = GridSearch(X_train_f, y_train, svm,param_grid )\ny_pred = svm_GridSearch.Best_Model_Predict(X_test_f)\n","7029f962":"cf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\nprint(\"F1 score:\",metrics.f1_score(y_test, y_pred))\n\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred)\nauc = metrics.roc_auc_score(y_test, y_pred)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","813a9ace":"param_grid = [{'n_components': [1,2,3,4]}]\n\nlda = LinearDiscriminantAnalysis()\n\nlda_GridSearch = GridSearch(X_train, y_train, lda , param_grid )\ny_pred = lda_GridSearch.Best_Model_Predict(X_test)","2211c9e2":"cf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\nprint(\"F1 score:\",metrics.f1_score(y_test, y_pred))\n\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred)\nauc = metrics.roc_auc_score(y_test, y_pred)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","e50455fc":"from scipy.stats import randint\nmax_depth_value = [4, 5,6,7,8,9,10,11,12,13]\nmax_features_value =  randint(1, 7)\nmin_samples_leaf_value = randint(1, 4)\ncriterion_value = [\"gini\", \"entropy\"]\n\nparam_grid = dict(max_depth = max_depth_value,\n                  max_features = max_features_value,\n                  min_samples_leaf = min_samples_leaf_value,\n                  criterion = criterion_value)\n\nCART = DecisionTreeClassifier(random_state=1)\n\nCART_RandSearch = RandomSearch(X_train_f, y_train, CART, param_grid)\nPrediction_CART = CART_RandSearch.Best_Model_Predict(X_test_f)\n","2ea96fb7":"cf_matrix = confusion_matrix(y_test, Prediction_CART)\nsns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, Prediction_CART))\nprint(\"Precision:\",metrics.precision_score(y_test, Prediction_CART))\nprint(\"Recall:\",metrics.recall_score(y_test, Prediction_CART))\nprint(\"F1 score:\",metrics.f1_score(y_test, Prediction_CART))\n\nfpr, tpr, _ = metrics.roc_curve(y_test,  Prediction_CART)\nauc = metrics.roc_auc_score(y_test, Prediction_CART)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","1d59d854":"param_grid = [\n{'n_estimators': [10, 25], 'max_features': [5, 10], \n 'max_depth': [10, 20, None], 'bootstrap': [True, False]}\n]\n\nrf = RandomForestClassifier()\n\nrf_GridSearch = GridSearch(X_train, y_train, rf ,param_grid )\ny_pred = rf_GridSearch.Best_Model_Predict(X_test)","db8f4c3a":"cf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\nprint(\"F1 score:\",metrics.f1_score(y_test, y_pred))\n\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred)\nauc = metrics.roc_auc_score(y_test, y_pred)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","8f5ff74a":"learning_rate_ = [.01,.05,.1,.5,1]\nn_estimators_ = [50,100,150,200,250,300]\n\nparam_grid = dict(learning_rate=learning_rate_, n_estimators=n_estimators_)\n\nGB = GradientBoostingClassifier()\n\nGB_GridSearch = GridSearch(X_train, y_train, GB, param_grid)\nPrediction_GB = GB_GridSearch.Best_Model_Predict(X_test)","c9fda1f0":"cf_matrix = confusion_matrix(y_test, Prediction_GB)\nsns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, Prediction_GB))\nprint(\"Precision:\",metrics.precision_score(y_test, Prediction_GB))\nprint(\"Recall:\",metrics.recall_score(y_test, Prediction_GB))\nprint(\"F1 score:\",metrics.f1_score(y_test, Prediction_GB))\n\nfpr, tpr, _ = metrics.roc_curve(y_test,  Prediction_GB)\nauc = metrics.roc_auc_score(y_test, Prediction_GB)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()\n#.712","b1cc86eb":"# Feature Importance","59f06346":"### Updating the columns","af8f3bd1":"## 6.6.2 Boosting","3b6747d1":"# 3. Data Cleaning","7be62bc3":"### Implemention of Decision Tree","fe62069b":"# 6.6.2.1 GradientBoosting\n\nA special case of boosting where errors are minimized by gradient descent algorithm e.g. the strategy consulting firms leverage by using case interviews to weed out less qualified candidates.","2a7121d9":"# 6.2 KNN","36838123":"# 6. Tuning Machine Learning Models","5f85643b":"Education\n\n    9th, 10th, 11th, 12th comes under HighSchool Grad but it has mentioned separately\n    Create Elementary object for 1st-4th, 5th-6th, 7th-8th\n\nMarital Status\n\n    \n    Married-civ-spouse,Married-spouse-absent,Married-AF-spouse comes under category Married\n    Divorced, separated again comes under category separated.\n\nWorkclass\n\n    Self-emp-not-inc, Self-emp-inc comes under category self employed\n    Local-gov,State-gov,Federal-gov comes under category goverment emloyees\n","51267121":"## 5.2 Split the dataset\u00b6","8de551cb":"## 5.3 Take a look to Income class distribution","66bc93f0":"Lets look the data it again :","2f89ce05":"## 5.4 Resampling\n\nThe main idea of sampling classes is to either increasing the samples of the minority class or decreasing the samples of the majority class. This is done in order to obtain a fair balance in the number of instances for both the classes.\n\nThere can be two main types of sampling:\n\n    You can add copies of instances from the minority class which is called over-sampling (or more formally sampling with replacement), or\n    You can delete instances from the majority class, which is called under-sampling.\n\n\nA widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and \/ or adding more examples from the minority class (over-sampling).\n\nDespite the advantage of balancing classes, these techniques also have their weaknesses (there is no free lunch). The simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfitting. In under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information.\n\n**Under-sampling**\n\nAdvantages of this approach:\n\n    It can help improve the runtime of the model and solve the memory problems by reducing the number of training data samples when the training data set is enormous.\n","2fd79013":"* The Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain. \n* It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.\n* As the name suggests, this method eliminates worst performing features on a particular model one after the other until the best subset of features are known.\n![vnb.png](attachment:vnb.png)","9ee9274c":"**Features sorted by their rank:**","c7a26c67":"## Recursive Feature Elimination","cdfdae60":"### ROC Curve","cc8ca763":"### Evaluation of logistic regression(Grid Search)\n**Confusion Matrix**","fdefa64e":"### Deleting the unuseful features and observations","6217e70a":"**C : Inverse of regularization strength**\n\nwe use paramter C as our regularization parameter. Parameter C = 1\/\u03bb.\n\nLambda (\u03bb) controls the trade-off between allowing the model to increase it's complexity as much as it wants with trying to keep it simple. For example, if \u03bb is very low or 0, the model will have enough power to increase it's complexity (overfit) by assigning big values to the weights for each parameter. If, in the other hand, we increase the value of \u03bb, the model will tend to underfit, as the model will become too simple.\n\n* Parameter C will work the other way around. For small values of C, we increase the regularization strength which will create simple models which underfit the data. For big values of C, we low the power of regularization which imples the model is allowed to increase it's complexity, and therefore, overfit the data.\n\n**L2 Regularization or Ridge Regularization**\n\n* Ridge regression adds \u201csquared magnitude\u201d of coefficient as penalty term to the loss function. Here the highlighted part represents L2 regularization element.\n![ewr.png](attachment:ewr.png)\n\n**L1 Regularization or Lasso**\n\n* Lasso Regression (Least Absolute Shrinkage and Selection Operator) adds \u201cabsolute value of magnitude\u201d of coefficient as penalty term to the loss function.\n![tre.png](attachment:tre.png)\n\nThe key difference between these techniques is that Lasso shrinks the less important feature\u2019s coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features.\n\n**Class weight**\n\n* If we have highly imbalanced classes and have no addressed it during preprocessing, we have the option of using the class_weight parameter to weight the classes to make certain we have a balanced mix of each class. Class weights will be given by n_samples \/ (n_classes * np.bincount(y))","a650a3cc":"**Kernel**\n\nkernel parameters selects the type of hyperplane used to separate the data. Using \u2018linear\u2019 will use a linear hyperplane (a line in the case of 2D data). \u2018rbf\u2019 and \u2018poly\u2019 uses a non linear hyper-plane.\n\n**gamma**\n\ngamma is a parameter for non linear hyperplanes. The higher the gamma value it tries to exactly fit the training data set.  Increasing gamma leads to overfitting as the classifier tries to perfectly fit the training data.\n\n**C**\n\nC is the penalty parameter of the error term. It controls the trade off between smooth decision boundary and classifying the training points correctly.","376c2d64":"**Logistic Regression is used when the dependent variable(target) is categorical.**\n\n**Model**\n\nOutput = 0 or 1\n\nHypothesis => Z = WX + B\n\nh\u0398(x) = sigmoid (Z)\n\n**Sigmoid Function**\n![logistic.png](attachment:logistic.png)\n\nIf \u2018Z\u2019 goes to infinity, Y(predicted) will become 1 and if \u2018Z\u2019 goes to negative infinity, Y(predicted) will become 0.\n\n**Cost Function**\n![sfd.png](attachment:sfd.png)","25c07574":"# 2. Import packages","70a0db35":"**Set of hyperparameters:  **\n\n*     n_estimators = number of trees in the foreset\n*     max_features = max number of features considered for splitting a node\n*     max_depth = max number of levels in each decision tree\n*     bootstrap = method for sampling data points (with or without replacement)","0d3e649d":"Fixing the common nan values\n\n    Nan values were as ? in data. Hence we fix this with most frequent element(mode) in the entire dataset. It generalizes well, as we will see with the accuracy of our classifiers","3fc36932":"## 5.5 Baseline models","f4d227ce":"**What is a Support Vector Machine**\nA Support Vector Machine is a supervised machine learning algorithm which can be used for both classification and regression problems. It follows a technique called the kernel trick to transform the data and based on these transformations, it finds an optimal boundary between the possible outputs.\n\n**How does it work?**\nThe main idea is to identify the optimal separating hyperplane which maximizes the margin of the training data. \n\nThe goal of SVMs is to find the optimal hyperplane because it not only classifies the existing dataset but also helps predict the class of the unseen data. And the optimal hyperplane is the one which has the biggest margin.\n\n","dc817525":"**kNN is non-parametric, instance based, lazy algorithm and used in the supervised setting.**\n\n* Non-parametric :\n      \n      It means that algorithm has no pre assumptions about the functional form of the model, to avoid mismodeling .\n\n* Instance based :\n      \n      It means that our algorithm does not explicitly learn a model.\n      Instead, it memorize the training instances which are subsequently used as \u201cknowledge\u201d for the prediction.\n\n* Lazy algorithm :\n\n      It means that it does not use the training data for the Generalization i.e. these algorithm has no explicit training phase or it is minimal. Training is very fast.\n      \n**kNN Algorithm for Classification**\n\nTraining element {xi, yi} , Testing point(x)\n\n    Compute the Distance D(x,xi) to every training element xi.\n    Select k closest instance xi1,xi2,\u2026\u2026.., xik and their labels yi1, yi2 \u2026, yik.\n    Output the class y* which is most frequent in yi1,yi2 \u2026\u2026yik.\n    \n\n\n**Significant of \u201ck\u201d**\n\n    Value of k has strong effect on kNN performance.\n    k act as controller to decide the shape of decision boundary.\n    Large value of k has following properties:\n\n 1. Smoother decision boundary\n 2. It provide more voters for prediction, it implies less affect from outliers.\n 3. As a result has Lower Variance and High Bias.\n \n ![gh.jpeg](attachment:gh.jpeg)\n \n**How to Select k**\n\n    The simplest solution is Cross Validation.\n    Best method is to try many k values and use Cross-Validation to see which k value is giving the best result.\n","b784c751":"### Grid Search vs Random search\n\n**Grid search** is a traditional way to perform hyperparameter optimization. It works by searching exhaustively through a specified subset of hyperparameters.\n\n**Random search** differs from grid search mainly in that it searches the specified subset of hyperparameters randomly instead of exhaustively. The major benefit being decreased processing time.\n\n* There is a tradeoff to decreased processing time, however. We aren\u2019t guaranteed to find the optimal combination of hyperparameters.\n\n![HPO1.png](attachment:HPO1.png)","2dad27b2":"## 1.2 Features Description\n**1. Categorical Attributes**\n * **workclass**:  Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n  -  Individual work category  \n * **education**: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n  -  Individual's highest education degree  \n * **marital-status**: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n  -  Individual marital status  \n * **occupation**:  Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n  -  Individual's occupation  \n * **relationship**:  Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n  -  Individual's relation in a family   \n * **race**:  White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n  -  Race of Individual   \n * **sex**:  Female, Male.\n * **native-country**:  United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n  -  Individual's native country   \n  \n**2. Continuous Attributes**\n * **age**: continuous.\n  -  Age of an individual  \n * **fnlwgt**: final weight, continuous. \n * The weights on the CPS files are controlled to independent estimates of the civilian noninstitutional population of the US.  These are prepared monthly for us by Population Division here at the Census Bureau.\n * **capital-gain**: continuous.\n * **capital-loss**: continuous.\n * **hours-per-week**: continuous.\n  -  Individual's working hour per week   ","dd341e60":"### Random search","4811b2ab":"**Precision:**\nPrecision is about being precise, i.e., how accurate your model is. In other words, you can say, when a model makes a prediction, how often it is correct.\nPrecision can be thought of as a measure of a classifier's exactness.\n![ret.png](attachment:ret.png)\n\n**Recall:**\nOut of all the positive classes, how much we predicted correctly. It should be high as possible.\nRecall can be thought of as a measure of a classifier's completeness.\n![ytu.png](attachment:ytu.png)\n\n**F1 score**\nIt is difficult to compare two models with low precision and high recall or vice versa. So to make them comparable, we use F-Score.\n\nF-score helps to measure Recall and Precision at the same time. It uses Harmonic Mean in place of Arithmetic Mean by punishing the extreme values more.\n![uyt.png](attachment:uyt.png)","4141996a":"## 1.1  Data description\nThis data was extracted from the 1994 Census bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics).  The prediction task is to determine whether a person makes over $50K a year.","74857d5d":"## 1.3 Objective of this project\nThe goal of this machine learning project is to predict whether a person makes over 50K a year or not given their demographic variation. This is a classification problem.","65a50848":"# 6.5 Decision Tree","9c152cc6":"# 5. Pipeline","c870861e":"## PCA analysis\nPrincipal Component Analysis (or PCA) uses linear algebra to transform the dataset into a compressed form.\n\nGenerally this is called a data reduction technique. A property of PCA is that you can choose the number of dimensions or principal component in the transformed result.\n\nIn this case we will use it to analyse the feature importanace","f847708e":"## Load Data","2bcea67b":"# 4. Feature Engineering","f37363a3":"## 6.1 Logistic Regression","9778505e":"### Grid Search","94473e13":"### Evaluation of logistic regression(Random Search)","e63cff39":"**Introduction to Decision Trees**\n\n* A decision tree is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.\n\n**How does Decision Tree works ?**\n\n* Decision tree is a type of supervised learning algorithm (having a pre-defined target variable) that is mostly used in classification problems. It works for both categorical and continuous input and output variables. In this technique, we split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter \/ differentiator in input variables.\n\n![asdwq.png](attachment:asdwq.png)\n\nThe core algorithm for building decision trees called ID3 by J. R. Quinlan which employs a top-down, greedy search through the space of possible branches with no backtracking. ID3 uses Entropy and Information Gain to construct a decision tree. \n\n**The popular attribute selection measures:**\n\n    Information gain\n    Gini index\n\n\n**Advantages of CART**\n\n    Simple to understand, interpret, visualize.\n    Decision trees implicitly perform variable screening or feature selection.\n    Can handle both numerical and categorical data. Can also handle multi-output problems.\n    Decision trees require relatively little effort from users for data preparation.\n    Nonlinear relationships between parameters do not affect tree performance.\n    \n**Disadvantages of CART**\n\n    Decision-tree learners can create over-complex trees that do not generalize the data well.This is called overfitting.\n    Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This is called variance, which needs to be lowered by methods like bagging and boosting.","191bb0a9":"# 6.4 LDA\n\n* Linear Discriminant Analysis (LDA) is most commonly used as dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications. \n* The goal is to project a dataset onto a lower-dimensional space with good class-separability in order avoid overfitting (\u201ccurse of dimensionality\u201d) and also reduce computational costs.\n\nIn general, dimensionality reduction does not only help reducing computational costs for a given classification task, but it can also be helpful to avoid overfitting by minimizing the error in parameter estimation (\u201ccurse of dimensionality\u201d).","3347921b":"# 6.3 SVM (Support Vector Machine)","6a41da7c":"# 1. Introduction\nA census is the procedure of systematically acquiring and recording information about the members of a given population.\nThe census is a special, wide-range activity, which takes place once a decade in the entire country. The purpose is to gather information about the general population, in order to present a full and reliable picture of the population in the country - its housing conditions and demographic, social and economic characteristics. The information collected includes data on age, gender, country of origin, marital status, housing conditions, marriage, education, employment, etc.","cb3ed5ae":"# 6.6.1.1 Random Forest\n","d1e869d7":"# 6.6 Ensemble methods\n\n**What is an ensemble method?**\n\nEnsemble is a Machine Learning concept in which the idea is to train multiple models using the same learning algorithm. The ensembles take part in a bigger group of methods, called multiclassifiers, where a set of hundreds or thousands of learners with a common objective are fused together to solve the problem.\nWhen we try to predict the target variable using any machine learning technique, the main causes of difference in actual and predicted values are noise, variance, and bias. Ensemble helps to reduce these factors (except noise, which is irreducible error).\n\n**Techniques to perform ensemble decision trees:**\n\n**1. Bagging**\n\nBagging is used when the goal is to reduce the variance of a decision tree classifier. Here the objective is to create several subsets of data from training sample chosen randomly with replacement. Each collection of subset data is used to train their decision trees. As a result, we get an ensemble of different models. Average of all the predictions from different trees are used which is more robust than a single decision tree classifier.\n\nBagging Steps:\n\n    Suppose there are N observations and M features in training data set. A sample from training data set is taken randomly with replacement.\n    A subset of M features are selected randomly and whichever feature gives the best split is used to split the node iteratively.\n    The tree is grown to the largest.\n    Above steps are repeated n times and prediction is given based on the aggregation of predictions from n number of trees.\n\nAdvantages:\n\n    Reduces over-fitting of the model.\n    Handles higher dimensionality data very well.\n    Maintains accuracy for missing data.\n\nDisadvantages:\n\n    Since final prediction is based on the mean predictions from subset trees, it won\u2019t give precise values for the classification and regression model.\n\n\n**2. Boosting**\n\nBoosting is used to create a collection of predictors. In this technique, learners are learned sequentially with early learners fitting simple models to the data and then analysing data for errors. Consecutive trees (random sample) are fit and at every step, the goal is to improve the accuracy from the prior tree. When an input is misclassified by a hypothesis, its weight is increased so that next hypothesis is more likely to classify it correctly. This process converts weak learners into better performing model.\n\nBoosting Steps:\n\n    Draw a random subset of training samples d1 without replacement from the training set D to train a weak learner C1\n    Draw second random training subset d2 without replacement from the training set and add 50 percent of the samples that were previously falsely classified\/misclassified to train a weak learner C2\n    Find the training samples d3 in the training set D on which C1 and C2 disagree to train a third weak learner C3\n    Combine all the weak learners via majority voting.\n\nAdvantages:\n\n    Supports different loss function (we have used \u2018binary:logistic\u2019 for this example).\n    Works well with interactions.\n\nDisadvantages:\n\n    Prone to over-fitting.\n    Requires careful tuning of different hyper-parameters.\n\n*     Bagging to decrease the model\u2019s variance;\n*     Boosting to decreasing the model\u2019s bias, and;\n*     Stacking to increasing the predictive force of the classifier.","fc22f6cb":"## 6.6.1 Bagging","43e77e02":"### Grid Search","e28bcd6a":"An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n\n    True Positive Rate\n    False Positive Rate\n![gfh.png](attachment:gfh.png)","a0cedba7":"## Hyperparameter Tuning \nA hyperparameter is a parameter whose value is set before the learning process begins.\nTuning Strategies\n\nWe will explore two different methods for optimizing hyperparameters:\n\n    Grid Search\n    Random Search","2cf384ea":"## 5.6 Models Scores","c4c0a1a2":"### Random Search","56a76b09":"#### Confusion Matrix"}}