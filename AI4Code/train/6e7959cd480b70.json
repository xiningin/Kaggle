{"cell_type":{"ffa184c1":"code","6ebe807e":"code","118fb955":"code","ac37ae50":"code","7ab36ed5":"code","d8426c71":"code","78fb6a07":"code","fbc7cb46":"code","3adb5bdc":"code","cc881051":"code","a512d430":"code","1a1a1ed5":"code","013761ea":"code","886c3a63":"code","48545253":"code","f6876f6d":"code","bd0b515f":"code","683a0685":"code","d3a836a2":"code","6f35adcd":"code","2494d3ee":"markdown","e607054e":"markdown","a2d588b4":"markdown","e7bda51f":"markdown","e9248dba":"markdown","b14ce0f9":"markdown","a6138e17":"markdown","0f29194c":"markdown","d81a9ff0":"markdown","701d56ab":"markdown","41397379":"markdown","0e073458":"markdown","3f72b873":"markdown","15718dc5":"markdown","52ab1b55":"markdown","9ab9cc99":"markdown","49798fe2":"markdown","6005703e":"markdown","19305ece":"markdown","851012c9":"markdown","23ce86a3":"markdown","c2bf9092":"markdown","1f3c8e0e":"markdown","49e4bdde":"markdown","71aa31e2":"markdown","5dd9a210":"markdown","4c6fd9ca":"markdown","e36eeab5":"markdown","13579319":"markdown","9d0a1ff3":"markdown","1f8dc4ea":"markdown","741879e4":"markdown"},"source":{"ffa184c1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport warnings\nwarnings.filterwarnings('ignore')","6ebe807e":"X = np.matrix([[1, 1], \n               [1, 2],\n              [1, 3],\n              [1, 4]])\nX","118fb955":"XT = np.matrix.transpose(X)\nXT","ac37ae50":"y = np.matrix([[1], \n               [3],\n              [3],\n              [5]])\ny","7ab36ed5":"XT_X = np.matmul(XT, X)\nXT_X","d8426c71":"XT_y = np.matmul(XT, y)\nXT_y","78fb6a07":"betas = np.matmul(np.linalg.inv(XT_X), XT_y)\nbetas","fbc7cb46":"from sklearn.linear_model import LinearRegression\n\nregressor = LinearRegression().fit(X = np.array([1, 2, 3, 4]).reshape(-1, 1), y = [1, 3, 3, 5])\nprint(\"The intercept is: \", str(regressor.intercept_), \". Which is almost 0.\")\nprint(\"The coefficient is: \", str(regressor.coef_))","3adb5bdc":"data_vw = pd.read_csv(\"\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/vw.csv\")\ndata_vw = data_vw[:300]\ndata_vw[\"Intercept\"] = 1\ndata_vw = data_vw[[\"Intercept\", \"year\", \"mileage\", \"tax\", \"mpg\", \"engineSize\", \"price\"]]\nprint(data_vw.shape)\ndata_vw.head()","cc881051":"np.set_printoptions(formatter={'float_kind':'{:f}'.format})\ncross_tab = np.matmul(np.matrix.transpose(data_vw.values), data_vw.values)\ncross_tab","a512d430":"X = data_vw[[\"Intercept\", \"year\", \"mileage\", \"tax\", \"mpg\", \"engineSize\"]].values\ny = data_vw[[\"price\"]].values","1a1a1ed5":"XT = np.matrix.transpose(X)","013761ea":"XT_X = np.matmul(XT, X)\nXT_X","886c3a63":"XT_X_inv = np.linalg.inv(XT_X)\nXT_X_inv","48545253":"XT_y = np.matmul(XT, y)\nXT_y","f6876f6d":"betas = np.matmul(XT_X_inv, XT_y)\nbetas","bd0b515f":"import statsmodels.api as sm\n\nregressor = sm.OLS(y, X).fit()\nprint(regressor.summary())","683a0685":"yT_y = cross_tab[-1:, -1:]\nn = cross_tab[:1, :1]\ny_bar_square = np.square(cross_tab[:1, -1:])\n\nSST = yT_y - (y_bar_square \/ n)\nSST","d3a836a2":"n = cross_tab[:1, :1]\ny_bar_square = np.square(cross_tab[:1, -1:])\n\nSSR = np.sum(np.multiply(betas, XT_y)) - (y_bar_square \/ n)\nSSR","6f35adcd":"r_square = SSR \/ SST\nr_square","2494d3ee":"#### Step 4: Find the inverse of this matrix","e607054e":"![image.png](attachment:image.png)","a2d588b4":"![image.png](attachment:image.png)","e7bda51f":"Consider linear regression equation: ![image.png](attachment:image.png)","e9248dba":"Muliplying by transpose of the X matrix on both the sides: ![image.png](attachment:image.png)","b14ce0f9":"![image.png](attachment:image.png)","a6138e17":"![image.png](attachment:image.png)","0f29194c":"I have imported the data and taken only the first 300 rows for this analysis. Though this will work with any number of rows, since I did this first on excel worksheets before converting into python program, I felt comfortable to work with 300 records on the excel.\n\n#### Step 1: Adding a column with intercept","d81a9ff0":"The calculstion for SST is:\n![image.png](attachment:image.png)\n\nWe can take these values from the cross_tab variable above","701d56ab":"The best mean that we have to reduce the error (\u03b5) which is a perpendicular line between the regression line and the data point (x)\n\n![image.png](attachment:image.png)\n\nOLS gives us the closed from solution in the form of the normal equations. Minimizing this sum of squared deviations is why the problem is called the Least Squares problem.","41397379":"#### Step 3: Multiply X transpose multipled X matrices\n\nWhat we get from excel is:\n![image.png](attachment:image.png)","0e073458":"# Part 1: Math behind solving Linear Regression using Matrix Multiplication:\n\nFind the best line through a set of data points: ![image.png](attachment:image.png)\n","3f72b873":"So, with the above equations we can calculate the intercept and coefficients with the equation:\n![image.png](attachment:image.png)","15718dc5":"Which is the value that we get out of the sm.ols() method.\n\n<hr>\n\n# Conclusion\n\nIn this notebook, I have demonstrated how to solve a simple and multiple linear regression using just numpy and linear regression.\nThough most of them would never need to use this with availability of sophisticated packages, it is always cool to solve something from scratch to learn the intution behind the algorithms\n\n[Link to medium article of this notebook.](https:\/\/towardsdatascience.com\/building-linear-regression-least-squares-with-linear-algebra-2adf071dd5dd)\n\nPlease upvote this notebook if you found this useful or informative for you!\n\n## Please visit my other notebooks on statistics here:\n\n1. [Sampling and sample distribution](https:\/\/www.kaggle.com\/gireeshs\/learn-statistics-1-sampling-sample-distribution)\n2. [Confidence intervals](https:\/\/www.kaggle.com\/gireeshs\/learn-statistics-2-confidence-interval)\n3. [WIP] Hypothesis testing\n4. [WIP] Comparison of two populations\n5. [WIP] Analysis of variance (ANOVA)\n6. [This notebook]  [Linear regression basics with matrix multiplication](https:\/\/www.kaggle.com\/gireeshs\/diy-from-scratch-linearregression-only-using-numpy)","52ab1b55":"#### Step 5: Multiply X transpose multipled y matrix","9ab9cc99":"#### Step 2: Taking the transpose of X matrix","49798fe2":"<hr>\n\n# Part 3: Multiple linear regression\n\nAs we have seen for the simple linear regression part, the multiple linear regression is similar to that of the simple linear regression, but with more X variables, and hence we will have as many \u03b2 as there are number of X variables.\n\nThe procedure for calculation for \u03b2 is:\n1. Add a first column with all 1's for the intercept\n2. Take the transpose of X matrix\n3. Multiply  X transpose multipled X matrices\n4. Find the inverse of this matrix\n5. Multiply X transpose multipled y matrix\n6. Multiply both the matrices to find the intercept and the coefficient\n\n### First I have done all the steps in an excel worksheet. The excel is available on [Google Drive here](https:\/\/docs.google.com\/spreadsheets\/d\/10zoOfiQLryz0Y7T1Zbd1grvOSdDGEgEEyECZ5sVyKJ8\/edit?usp=sharing).\n\nPlease download the google sheet, since there are some formatting issues on google drive that does not display the formulas correctly.","6005703e":"![image.png](attachment:image.png)","19305ece":"# Introduction\n\nWhen I did my course work for Business Analytics course, the statistics course involved a lecture on showing how to solve Linear Regression using just excel and matrix multiplication using linear algebra which solves for oridinary least squares (OLS).\n\nSince then I have forgotten how to solve it using matrix multiplication and I wanted learn how it is done as well demonstrate to others.\n\n### This is a series of notebooks which will go through the techniques that is essential for the statistics and will be useful for all machine learning practitioners. It is a series of 6 notebooks:\n\n1. [Sampling and sample distribution](https:\/\/www.kaggle.com\/gireeshs\/learn-statistics-1-sampling-sample-distribution)\n2. [Confidence intervals](https:\/\/www.kaggle.com\/gireeshs\/learn-statistics-2-confidence-interval)\n3. [WIP] Hypothesis testing\n4. [WIP] Comparison of two populations\n5. [WIP] Analysis of variance (ANOVA)\n6. [This notebook]  [Linear regression basics with matrix multiplication](https:\/\/www.kaggle.com\/gireeshs\/diy-from-scratch-linearregression-only-using-numpy)\n\n### How to use this notebook:\n1. See this [youtube video](https:\/\/www.youtube.com\/watch?v=Lx6CfgKVIuE) which gives the intution for how this is solved. I have followed the equations and the solutions given in that youtube video and solved it using numpy\n2. Fork this notebook and follow along each step with understanding\n\nSorry for not usng Latex for math equations, I had my notes on one note, so it was easier for me to put screenshots in this notebook.\n\n### Sections:\n1. [Math behind solving Linear Regression using Matrix Multiplication](https:\/\/www.kaggle.com\/gireeshs\/diy-build-linear-regression-with-linear-algebra?scriptVersionId=39808155#Part-1:-Math-behind-solving-Linear-Regression-using-Matrix-Multiplication:)\n2. [Solving simple linear regression](https:\/\/www.kaggle.com\/gireeshs\/diy-build-linear-regression-with-linear-algebra?scriptVersionId=39808155#Part-2:-Solving-a-simple-linear-regression:)\n3. [Multiple linear regression](https:\/\/www.kaggle.com\/gireeshs\/diy-build-linear-regression-with-linear-algebra?scriptVersionId=39808155#Part-3:-Multiple-linear-regression)\n4. [Calculation for r-square value](https:\/\/www.kaggle.com\/gireeshs\/diy-build-linear-regression-with-linear-algebra?scriptVersionId=39808155#Part-4:-Calculation-for-r-square-value:)\n5. Conclusion\n6. References\n\nPlease upvote this notebook if you found useful and\/or informative!","851012c9":"### What does best mean?\n\n![image.png](attachment:image.png)","23ce86a3":"<hr>\n\n# Part 2: Solving a simple linear regression:\n\nLet us find the best line using least square for the set of data points: (1, 1), (2, 3), (3, 3), (4, 5)\n\nHere the X = [1, 2, 3, 4] and y = [1, 3, 3, 5]\n\n\nBut we have to convert the X into a matrix so that we can have the \u03b20 which is the intercept\n![image.png](attachment:image.png)","c2bf9092":"![image.png](attachment:image.png)","1f3c8e0e":"To formulate this as a matrix problem, we can write it as:\n\n![image.png](attachment:image.png)","49e4bdde":"# References:\n1. https:\/\/www.youtube.com\/watch?v=Lx6CfgKVIuE\n2. Complete business statistics book\n3. My course work for [ISB CBA](https:\/\/www.isb.edu\/en\/study-isb\/advanced-management-programmes\/ampba.html)\n4. https:\/\/towardsdatascience.com\/qr-matrix-factorization-15bae43a6b2\n5. https:\/\/en.wikipedia.org\/wiki\/Ordinary_least_squares\n\nImage\nhttp:\/\/onlinestatbook.com\/2\/regression\/intro.html","71aa31e2":"![image.png](attachment:image.png)\n\nThe best line is given by the equation y = 0 + 1.2 X\n\nOfcourse we can verify this with fitting it into a model as below.","5dd9a210":"We can verify this with SM OLS below:","4c6fd9ca":"<hr>\n\n# Part 4: Calculation for r-square value:\n\nIf you have followed along till untill here, what follows will also be quite simple.\n\nFor any regression problem, finding just the interecept and the co-efficients is never good enough, we need to know how good the fit is.\n\n**The multiple coefficient of determination R2 measures the proportion of the variation in the dependent variable that is explained by the combination of the independent variables in the multiple regression model:**\n\n![image.png](attachment:image.png)","e36eeab5":"### Let us interpret what that matrix is:\n\n![image.png](attachment:image.png)","13579319":"![image.png](attachment:image.png)","9d0a1ff3":"#### Step 6: Multiply both the matrices to find the intercept and the coefficient","1f8dc4ea":"Such that: ![image.png](attachment:image.png)","741879e4":"SSR is given by:\n![image.png](attachment:image.png)"}}