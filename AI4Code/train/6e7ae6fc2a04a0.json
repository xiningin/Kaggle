{"cell_type":{"e15caad3":"code","35bd235b":"code","9f318854":"code","4efda9c7":"code","ffe8953d":"code","b7665203":"code","5322ebd3":"code","3e4be470":"code","bd1fb750":"code","04eb863f":"code","87606d79":"code","531b6ab4":"code","f77c1de9":"code","0bdc8074":"code","651ea064":"code","83ef6b58":"code","7f1fa9c5":"code","e8fa7181":"code","68ffa43f":"code","8ccdb08b":"code","6977fa66":"code","fa8cd38b":"code","f92c4942":"code","b884be98":"code","25c1134b":"code","4a9f62ba":"code","a958b5c0":"code","8fc9ce89":"code","bcbd5aa1":"code","8bb4e863":"code","8434e146":"code","3e9ba9f1":"code","20a0006c":"code","a9ea4ac9":"code","80f8b6df":"code","9dcfab28":"code","fe844ccc":"code","cef5ebcf":"code","322856ba":"code","8d2562a5":"code","5ce325b7":"code","ac376519":"code","3a4c06c4":"code","9fdb7775":"code","ae6bb315":"code","bb12ad20":"code","d27f27b8":"code","0b7c451b":"code","6fb6e28d":"code","c52cf603":"code","d2980606":"code","c1c6f475":"code","e5da8b1f":"code","e49fb6f9":"code","fc232aba":"code","2b1ff357":"markdown","1500f5b4":"markdown","b671d47f":"markdown","02164150":"markdown","208dc528":"markdown","10f8be9f":"markdown","be37e434":"markdown","2eb9fb6b":"markdown","09d796f7":"markdown","79e6e87e":"markdown","fd43117e":"markdown","04a123fe":"markdown","5efac163":"markdown","ccf36546":"markdown","ffbe4b15":"markdown","2422a816":"markdown","f7d3b451":"markdown","d7ee91d2":"markdown","ccc6dd71":"markdown","bab8aab6":"markdown","315db33b":"markdown","9c5ec8a5":"markdown","32382ef9":"markdown","d65b3e3b":"markdown","449f161e":"markdown","f94cb7e0":"markdown","712c79b2":"markdown","ac702d8b":"markdown","8a574459":"markdown","98dc47ed":"markdown","a8a0328e":"markdown","4233520c":"markdown","bb7c1d57":"markdown","38576b35":"markdown","45b27ade":"markdown","6f23d0b6":"markdown","707f469e":"markdown","810df0da":"markdown","d931ec53":"markdown","3cb77fc9":"markdown","29c49906":"markdown","2a7bbe9f":"markdown","f8467152":"markdown","0010bd7b":"markdown","de43dd39":"markdown","a15425ab":"markdown","300593fc":"markdown","9c1b352a":"markdown","f45b209f":"markdown","4c1cf98a":"markdown","a8b8e4a1":"markdown"},"source":{"e15caad3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","35bd235b":"!pip install pyspark","9f318854":"import os\nimport pandas as pd\nimport numpy as np\n\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession, SQLContext\n\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import udf, col\n\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.mllib.evaluation import RegressionMetrics\n\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\nfrom pyspark.ml.evaluation import RegressionEvaluator","4efda9c7":"import seaborn as sns\nimport matplotlib.pyplot as plt","ffe8953d":"# Visualization\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_colwidth', 400)\n\nfrom matplotlib import rcParams\nsns.set(context='notebook', style='whitegrid', rc={'figure.figsize': (18,4)})\nrcParams['figure.figsize'] = 18,4\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'","b7665203":"# setting random seed for notebook reproducability\nrnd_seed=23\nnp.random.seed=rnd_seed\nnp.random.set_state=rnd_seed","5322ebd3":"spark=SparkSession.builder.master(\"local[2]\").appName(\"Linear-Regression-California-Housing\").getOrCreate()  ","3e4be470":"spark","bd1fb750":"sc = spark.sparkContext\nsc","04eb863f":"sqlContext = SQLContext(spark.sparkContext)\nsqlContext","87606d79":"HOUSING_DATA = '..\/input\/hausing-data\/cal_housing.data'","531b6ab4":"# define the schema, corresponding to a line in the csv data file.\nschema = StructType([\n    StructField(\"long\", FloatType(), nullable=True),\n    StructField(\"lat\", FloatType(), nullable=True),\n    StructField(\"medage\", FloatType(), nullable=True),\n    StructField(\"totrooms\", FloatType(), nullable=True),\n    StructField(\"totbdrms\", FloatType(), nullable=True),\n    StructField(\"pop\", FloatType(), nullable=True),\n    StructField(\"houshlds\", FloatType(), nullable=True),\n    StructField(\"medinc\", FloatType(), nullable=True),\n    StructField(\"medhv\", FloatType(), nullable=True)]\n)","f77c1de9":"# Load housing data\nhousing_df = spark.read.csv(path=HOUSING_DATA, schema=schema).cache()","0bdc8074":"housing_df.take(5)","651ea064":"housing_df.head()","83ef6b58":"housing_df.show(5)","7f1fa9c5":"housing_df.columns","e8fa7181":"housing_df.printSchema()","68ffa43f":"#run sample selection\nhousing_df.select('pop','totrooms').show(10)","8ccdb08b":"# group by housingmedianage and see the distribution\nresult_df=housing_df.groupBy(\"medage\").count().sort(\"medage\",ascending=False)\nresult_df.show()","6977fa66":"result_df.toPandas().plot.bar(x='medage', y='count',figsize=(14,6))","fa8cd38b":"housing_df.describe().select(\n                    \"summary\",\n                    F.round(\"medage\", 4).alias(\"medage\"),\n                    F.round(\"totrooms\", 4).alias(\"totrooms\"),\n                    F.round(\"totbdrms\", 4).alias(\"totbdrms\"),\n                    F.round(\"pop\", 4).alias(\"pop\"),\n                    F.round(\"houshlds\", 4).alias(\"houshlds\"),\n                    F.round(\"medinc\", 4).alias(\"medinc\"),\n                    F.round(\"medhv\", 4).alias(\"medhv\")).show()","f92c4942":"# Adjust the values of `medianHouseValue'\nhousing_df=housing_df.withColumn(\"medhv\",col('medhv')\/100000)\nhousing_df.show(3)","b884be98":"housing_df.columns","25c1134b":"%config Completer.use_jedi = False","4a9f62ba":"# Add the new columns to `df`\nhousing_df=(housing_df.withColumn('rmsperhh',F.round(col('totrooms')\/col('houshlds'),2))\n            .withColumn(\"popperhh\", F.round(col(\"pop\")\/col(\"houshlds\"), 2))\n                       .withColumn(\"bdrmsperrm\", F.round(col(\"totbdrms\")\/col(\"totrooms\"), 2)))","a958b5c0":"# Inspect the result\nhousing_df.show(5)","8fc9ce89":"housing_df=housing_df.select(\"medhv\",\"totbdrms\", \n                              \"pop\", \n                              \"houshlds\", \n                              \"medinc\", \n                              \"rmsperhh\", \n                              \"popperhh\", \n                              \"bdrmsperrm\")\nhousing_df.show(5)","bcbd5aa1":"featureCols=[\"totbdrms\", \"pop\",\"houshlds\",\"medinc\",\"rmsperhh\",\"popperhh\",\"bdrmsperrm\"]","8bb4e863":"assembler=VectorAssembler(inputCols=featureCols, outputCol=\"features\")","8434e146":"assembled_df=assembler.transform(housing_df)\nassembled_df.show(5, truncate= False )","3e9ba9f1":"# Initialize the `standardScaler`\nstandardScaler=StandardScaler(inputCol=\"features\",outputCol='features_scaled')","20a0006c":"# Fit the DataFrame to the scaler\nscaled_df = standardScaler.fit(assembled_df).transform(assembled_df)","a9ea4ac9":"# Inspect the result\nscaled_df.select(\"features\", \"features_scaled\").show(10, truncate=False)","80f8b6df":"# Split the data into train and test sets\ntrain_data, test_data = scaled_df.randomSplit([.8,.2], seed=rnd_seed)","9dcfab28":"train_data.columns","fe844ccc":"# Initialize `lr`\nlr = (LinearRegression(featuresCol='features_scaled', labelCol=\"medhv\", predictionCol='predmedhv', \n                               maxIter=10, regParam=0.3, elasticNetParam=0.8, standardization=False))","cef5ebcf":"# Fit the data to the model\nlinearModel = lr.fit(train_data)","322856ba":"# Coefficients for the model\nlinearModel.coefficients","8d2562a5":"featureCols","5ce325b7":"# Intercept for the model\nlinearModel.intercept","ac376519":"coeff_df = pd.DataFrame({\"Feature\": [\"Intercept\"] + featureCols, \"Co-efficients\": np.insert(linearModel.coefficients.toArray(), 0, linearModel.intercept)})\ncoeff_df = coeff_df[[\"Feature\", \"Co-efficients\"]]","3a4c06c4":"coeff_df","9fdb7775":"# Generate predictions\npredictions = linearModel.transform(test_data)\n# Extract the predictions and the \"known\" correct labels\npredandlabels = predictions.select(\"predmedhv\", \"medhv\")","ae6bb315":"predandlabels.show()","bb12ad20":"# Get the RMSE\nprint(\"RMSE: {0}\".format(linearModel.summary.rootMeanSquaredError))","d27f27b8":"print(\"MAE: {0}\".format(linearModel.summary.meanAbsoluteError))","0b7c451b":"# Get the R2\nprint(\"R2: {0}\".format(linearModel.summary.r2))","6fb6e28d":"evaluator = RegressionEvaluator(predictionCol=\"predmedhv\", labelCol='medhv', metricName='rmse')\nprint(\"RMSE: {0}\".format(evaluator.evaluate(predandlabels)))","c52cf603":"evaluator = RegressionEvaluator(predictionCol=\"predmedhv\", labelCol='medhv', metricName='mae')\nprint(\"MAE: {0}\".format(evaluator.evaluate(predandlabels)))","d2980606":"evaluator = RegressionEvaluator(predictionCol=\"predmedhv\", labelCol='medhv', metricName='r2')\nprint(\"R2: {0}\".format(evaluator.evaluate(predandlabels)))","c1c6f475":"# mllib is old so the methods are available in rdd\nmetrics = RegressionMetrics(predandlabels.rdd)\nprint(\"RMSE: {0}\".format(metrics.rootMeanSquaredError))","e5da8b1f":"print(\"MAE: {0}\".format(metrics.meanAbsoluteError))","e49fb6f9":"print(\"R2: {0}\".format(metrics.r2))","fc232aba":"spark.stop()","2b1ff357":"<a id=\"8p3\"><\/a> <br>\n### 8.3 Inspect the Metrics  \nLooking at predicted values is one thing, but another and better thing is looking at some metrics to get a better idea of how good your model actually is.  \n\nUsing the LinearRegressionModel.summary attribute:  \n\nNext, we can also use the summary attribute to pull up the rootMeanSquaredError and the r2.","1500f5b4":"<a id=\"5p1\"><\/a> <br>\n### 5.1 Preprocessing The Target Values\nFirst, let's start with the medianHouseValue, our dependent variable. To facilitate our working with the target values, we will express the house values in units of 100,000. That means that a target such as 452600.000000 should become 4.526:","b671d47f":"<a id=\"7p2\"><\/a> <br>\n### 7.2 Ridge Regression Theory \nFrom the discussion so far we have concluded that we would like to decrease the model complexity, that is the number of predictors. We could use the forward or backward selection for this, but that way we would not be able to tell anything about the removed variables' effect on the response. Removing predictors from the model can be seen as settings their coefficients to zero. Instead of forcing them to be exactly zero, let's penalize them if they are too far from zero, thus enforcing them to be small in a continuous way. This way, we decrease model complexity while keeping all variables in the model. This, basically, is what Ridge Regression does.","02164150":"All the features have transformed into a Dense Vector.","208dc528":"[Table of Contents](#TOC) Click here to go back to 'Table of Contents'","10f8be9f":"[Table of Contents](#TOC) Click here to go back to 'Table of Contents'","be37e434":"                                                                    --By Partha Chowdhury \n                                                        If it helps, please give upvote.","2eb9fb6b":"Note that the argument elasticNetParam corresponds to  \u03b1  or the vertical intercept and that the regParam or the regularization paramater corresponds to  \u03bb .","09d796f7":"[Table of Contents](#TOC) Click here to go back to 'Table of Contents'","79e6e87e":"Pyspark is faster than others as it does in-memory calculations. It has many other aspects and its highlighted below. \n\nIn this we'll make use of the California Housing data set. Note, of course, that this is actually 'small' data and that using Spark in this context might be overkill; This notebook is for educational purposes only and is meant to give us an idea of how we can use PySpark to build a machine learning model.","fd43117e":"<a id=\"3\"><\/a> <br>\n# 3. Load The Data From a File Into a Dataframe","04a123fe":"<a id=\"8\"><\/a> <br>\n## 8. Evaluating the Model  \nWith our model in place, we can generate predictions for our test data: use the transform() method to predict the labels for our test_data. Then, we can use RDD operations to extract the predictions as well as the true labels from the DataFrame.","5efac163":"<a id=\"6p1\"><\/a> <br>\n### 6.1 Feature Extraction  \nNow that we have re-ordered the data, we're ready to normalize the data. We will choose the features to be normalized.","ccf36546":"<a id=\"2\"><\/a> <br>\n# 2. Creating the Spark Session","ffbe4b15":"<a id=\"8p1\"><\/a> <br>\n### 8.1 Inspect the Model Co-efficients","2422a816":"**Longitude**:refers to the angular distance of a geographic place north or south of the earth\u2019s equator for each block group <br>\n**Latitude** :refers to the angular distance of a geographic place east or west of the earth\u2019s equator for each block group <br>\n**Housing Median Age**:is the median age of the people that belong to a block group. Note that the median is the value that lies at the midpoint of a frequency distribution of observed values <br>\n**Total Rooms**:is the total number of rooms in the houses per block group <br>\n**Total Bedrooms**:is the total number of bedrooms in the houses per block group <br>\n**Population**:is the number of inhabitants of a block group <br>\n**Households**:refers to units of houses and their occupants per block group <br>\n**Median Income**:is used to register the median income of people that belong to a block group <br>\n**Median House Value**:is the dependent variable and refers to the median house value per block group <br>","f7d3b451":"DataFrame.withColumn(colName, col)\nReturns a new DataFrame by adding a column or replacing the existing column that has the same name.","d7ee91d2":"DataFrame.toPandas()  \nReturns the contents of this DataFrame as Pandas pandas.DataFrame.","ccc6dd71":"[Table of Contents](#TOC) Click here to go back to 'Table of Contents'","bab8aab6":"The OLS estimator has the desired property of being unbiased. However, it can have a huge variance. Specifically, this happens when:  \n\nThe predictor variables are highly correlated with each other;  \nThere are many predictors. This is reflected in the formula for variance given above: if m approaches n, the variance approaches infinity.  \nThe general solution to this is: reduce variance at the cost of introducing some bias. This approach is called regularization and is almost always beneficial for the predictive performance of the model. To make it sink in, let's take a look at the following plot.","315db33b":"## This Notebook is created to demonstrate capability of Pyspark with Kaggle California Housing dataset. ","9c5ec8a5":"Research:   \nRegularization: Ridge, Lasso and Elastic Net  \nLink: https:\/\/www.datacamp.com\/community\/tutorials\/tutorial-ridge-lasso-elastic-net","32382ef9":"<a id=\"7\"><\/a> <br>\n## 7. Building A Machine Learning Model With Spark ML","d65b3e3b":"There's definitely some improvements needed to our model! If we want to continue with this model, we can play around with the parameters that we passed to your model, the variables that we included in your original DataFrame.   \nIt gives us a good idea about how Pyspark works. ","449f161e":"<a id=\"6p2\"><\/a> <br>\n### 6.2 Standardization  \nNext, we can finally scale the data using StandardScaler. The input columns are the features, and the output column with the rescaled that will be included in the scaled_df will be named \"features_scaled\":**","f94cb7e0":"Specifying the schema when loading data into a DataFrame will give better performance than schema inference.","712c79b2":"<a id=\"1\"><\/a> <br>\n# 1. Understanding the Data Set","ac702d8b":"<a id=\"4\"><\/a> <br>\n# 4. Data Exploration","8a574459":"[Table of Contents](#TOC) Click here to go back to 'Table of Contents'","98dc47ed":"Import necessary libfraries","a8a0328e":"![](https:\/\/miro.medium.com\/max\/1068\/1*0DduCSleektW3HiuMeYRGQ.png)","4233520c":"Use a VectorAssembler to put features into a feature vector column. THis is the way MLlib works in Pyspark","bb7c1d57":"With all the preprocessing done, it's finally time to start building our Linear Regression model! Just like always, we first need to split the data into training and test sets. Luckily, this is no issue with the randomSplit() method:","38576b35":"[Table of Contents](#TOC) Click here to go back to 'Table of Contents'","45b27ade":"<a id=\"8p2\"><\/a> <br>\n### 8.2 Generating Predictions","6f23d0b6":"<a id=\"6\"><\/a> <br>\n## 6. Feature Engineering  \nNow that we have adjusted the values in medianHouseValue, we will now add the following columns to the data set:  \n\nRooms per household which refers to the number of rooms in households per block group;  \nPopulation per household, which basically gives us an indication of how many people live in households per block group; And  \nBedrooms per room which will give us an idea about how many rooms are bedrooms per block group;    \n\nAs we're working with DataFrames, we can best use the select() method to select the columns that we're going to be working with, namely totalRooms, households, and population.   Additionally, we have to indicate that we're working with columns by adding the col() function to our code. Otherwise, we won't be able to do element-wise operations like the division that we have in mind for these three variables:","707f469e":"> *Partha Chowdhury signing off*","810df0da":"We can see that, for the first row, there are about 6.98 rooms per household, the households in the block group consist of about 2.5 people and the amount of bedrooms is quite low with 0.14:  \n\n\nSince we don't want to necessarily standardize our target values, we'll want to make sure to isolate those in our data set. Note also that this is the time to leave out variables that we might not want to consider in our analysis. In this case, let's leave out variables such as longitude, latitude, housingMedianAge and totalRooms.  \n\nIn this case, we will use the select() method and passing the column names in the order that is more appropriate. In this case, the target variable medianHouseValue is put first, so that it won't be affected by the standardization.","d931ec53":"<a id=\"7p3\"><\/a> <br>\n## 7.3 Create an ElasticNet model:\n\nElasticNet is a linear regression model trained with L1 and L2 prior as regularizer. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. We control the convex combination of L1 and L2 using the l1_ratio parameter.\n\nElastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.\n\nA practical advantage of trading-off between Lasso and Ridge is it allows Elastic-Net to inherit some of Ridge\u2019s stability under rotation.","3cb77fc9":"[Table of Contents](#TOC) Click here to go back to 'Table of Contents'","29c49906":"<a id=\"TOC\"><\/a> <br>\n**Table of Contents:**  \n[1. Understanding the Dataset](#1)  \n[2. Creating the Spark Session](#2)<br> \n[3. Load The Data From a File Into a Dataframe](#3) <br> \n[4. Data Exploration](#4) <br>\n    [4.1 Distribution of the median age of the people living in the area](#4p1)  <br>\n    [4.2 Summary Statistics](#4p2) <br>\n[5. Data Preprocessing](#5) <br>\n    [5.1 Preprocessing The Target Values](#5p1)  <br>\n[6. Feature Engineering](#6) <br>\n    [6.1 Feature Extraction](#6p1)  <br>\n    [6.2 Standardization](#6p2) <br>\n[7. Building A Machine Learning Model With Spark ML](#7) <br>\n    [7.1 Bias and Variance Explanation](#7p1)  <br>\n    [7.2 Ridge Regression Theory](#7p2)<br> \n    [7.3 Create an ElasticNet model](#7p3) <br>\n[8. Evaluating the Model](#8) <br>\n    [8.1 Inspect the Model Co-efficients](#8p1) <br> \n    [8.2 Generating Predictions](#8p2) <br>\n    [8.3 Inspect the Metrics](#8p3) ","2a7bbe9f":"<a id=\"4p2\"><\/a> <br>\n### 4.2 Summary Statistics:  \nSpark DataFrames include some built-in functions for statistical processing. The describe() function performs summary statistics calculations on all numeric columns and returns them as a DataFrame.","f8467152":"This graphic illustrates what bias and variance are. Imagine the bull's-eye is the true population parameter that we are estimating, \u03b2, and the shots at it are the values of our estimates resulting from four different estimators - low bias and variance, high bias and variance, and the combinations thereof.  \n![](https:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1543418451\/bias_vs_variance_swxhxx.jpg)","0010bd7b":"### Using the RegressionMetrics from pyspark.mllib package:","de43dd39":"We see that multiple attributes have a wide range of values: we will need to normalize your dataset.","a15425ab":"SparkContext. Main entry point for Spark functionality. A SparkContext represents the connection to a Spark cluster, and can be used to create RDD and broadcast variables on that cluster.","300593fc":"<a id=\"5\"><\/a> <br>\n## 5. Data Preprocessing  \nWith all this information that we gathered from our small exploratory data analysis, we know enough to preprocess our data to feed it to the model.  \n\nwe shouldn't care about missing values; all zero values have been excluded from the data set.  \nWe should probably standardize our data, as we have seen that the range of minimum and maximum values is quite big.  \nThere are possibly some additional attributes that we could add, such as a feature that registers the number of bedrooms per room or the rooms per household.  \nOur dependent variable is also quite big; To make our life easier, we'll have to adjust the values slightly.","9c1b352a":"* The RMSE measures how much error there is between two datasets comparing a predicted value and an observed or known value. The smaller an RMSE value, the closer predicted and observed values are.  \n   \n* The R2 (\"R squared\") or the coefficient of determination is a measure that shows how close the data are to the fitted regression line. This score will always be between 0 and a 100% (or 0 to 1 in this case), where 0% indicates that the model explains none of the variability of the response data around its mean, and 100% indicates the opposite: it explains all the variability. That means that, in general, the higher the R-squared, the better the model fits our data.  \n\nUsing the RegressionEvaluator from pyspark.ml package:","f45b209f":"<a id=\"7p1\"><\/a> <br>\n### 7.1 Bias and Variance Explanation ","4c1cf98a":"The data contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). In this sample a block group on average includes 1425.5 individuals living in a geographically compact area.\n\nThese spatial data contain 20,640 observations on housing prices with 9 economic variables:","a8b8e4a1":"<a id=\"4p1\"><\/a> <br>\n### 4.1 Distribution of the median age of the people living in the area:"}}