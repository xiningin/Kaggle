{"cell_type":{"04b82cc9":"code","c3642880":"code","51f7aa5a":"code","c5998ead":"code","1b18c4bd":"code","95fae379":"code","979dae8b":"code","a3570074":"code","74d1cbe1":"code","0827bd14":"code","719b9c6d":"code","f7933ec3":"code","7278c6c3":"code","85248cce":"code","208dc45e":"code","65d1801a":"code","09372d35":"code","13d87d3c":"code","40455303":"code","271e2c22":"code","3f718dc4":"code","35b76ab0":"code","f7f0d006":"code","9b9c96b1":"code","bbe5a1ec":"code","0b23f1f4":"code","0807eaee":"code","39ff46be":"code","203d476e":"code","d291ebc8":"code","985652b4":"code","2ea8d7c4":"code","8ecbbf9f":"code","0f548e12":"code","8f8d51a6":"code","f83de78d":"code","524fd42c":"code","ad465ea7":"code","f238219d":"code","f3c36623":"markdown","dbdbf8b1":"markdown","dd610b3a":"markdown","eeff8561":"markdown","5cbd8efb":"markdown","45ee6857":"markdown","acd7e5c0":"markdown","7a33cbe9":"markdown","454d1d69":"markdown","875da4e1":"markdown","bc417dd1":"markdown","9951541b":"markdown","0db10190":"markdown","cd9b3db7":"markdown","31930578":"markdown","0c723b77":"markdown","be450467":"markdown","a8d14e1a":"markdown","97b175b4":"markdown","e4aa46ca":"markdown","e084f27e":"markdown"},"source":{"04b82cc9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","c3642880":"sns.set_style(\"darkgrid\")","51f7aa5a":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F","c5998ead":"sales = pd.read_csv(\"..\/input\/for-simple-exercises-time-series-forecasting\/Alcohol_Sales.csv\",index_col=0,parse_dates=True)","1b18c4bd":"sales.head()","95fae379":"sales.tail()","979dae8b":"sales.plot(figsize=(16,5),grid=True,legend = False);","a3570074":"X = sales.index\nY = sales['S4248SM144NCEN'].values.astype(float)","74d1cbe1":"X = np.array(X)","0827bd14":"X[0]","719b9c6d":"test_size = 12\ntrain_set = Y[:-test_size]\ntest_set = Y[-test_size:]","f7933ec3":"from sklearn.preprocessing import MinMaxScaler","7278c6c3":"scaler = MinMaxScaler(feature_range=(-1,1))","85248cce":"train_norm = scaler.fit_transform(train_set.reshape(-1,1))","208dc45e":"train_norm = train_norm.flatten()","65d1801a":"train_norm = torch.FloatTensor(train_norm)","09372d35":"def get_windows(data,ws):\n    out = []\n    L = len(data)\n    for i in range(L-ws):\n        out.append((data[i:i+ws],data[i+ws:i+ws+1]))\n    return out","13d87d3c":"window_size = 12\ntrain_data = get_windows(train_norm,window_size)","40455303":"class LSTM(nn.Module):\n    def __init__(self,in_size = 1,hidden_size = 100,out_size = 1):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.lstm = nn.LSTM(in_size,hidden_size)\n        self.linear = nn.Linear(hidden_size,out_size)\n        self.hidden = (torch.zeros(1,1,self.hidden_size).cuda(),\n                       torch.zeros(1,1,self.hidden_size).cuda())\n    def forward(self,X):\n        lstm_out,self.hidden = self.lstm(X.view(len(X),1,-1),self.hidden)\n        pred = self.linear(lstm_out.view(len(X),-1))\n        return pred[-1]","271e2c22":"model = LSTM().cuda()","3f718dc4":"optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)\ncriterion = nn.MSELoss()","35b76ab0":"model","f7f0d006":"import time\nstart = time.time()\nepochs = 100\nfor i in range(epochs):\n    for X_train,Y_train in train_data:\n        X_train = X_train.cuda()\n        Y_train = Y_train.cuda()\n        optimizer.zero_grad()\n        model.hidden = (torch.zeros(1,1,model.hidden_size).cuda(),\n                        torch.zeros(1,1,model.hidden_size).cuda())\n        Y_pred = model(X_train)\n        loss = criterion(Y_pred,Y_train)\n        loss.backward()\n        optimizer.step()\n    print(f\"Epoch : {i+1} LOSS : {loss.item():.7f}\")\nend = time.time()\ndur = end-start\nprint(f\"Duration : {int(dur\/60)} minutes and {int(dur%60)} seconds\")","9b9c96b1":"future = 12\npreds = train_norm[-window_size:].tolist()\nmodel.eval()\nfor i in range(future):\n    X_test = torch.FloatTensor(preds[-window_size:]).cuda()\n    with torch.no_grad():\n        model.hidden = (torch.zeros(1,1,model.hidden_size).cuda(),\n                      torch.zeros(1,1,model.hidden_size).cuda())\n        preds.append(model(X_test).item())\n","bbe5a1ec":"preds[-window_size:]","0b23f1f4":"true_predictions = scaler.inverse_transform(np.array(preds[-window_size:]).reshape(-1,1))\ntrue_predictions","0807eaee":"sales['S4248SM144NCEN'][-12:]","39ff46be":"dates = np.arange('2018-02-01', '2019-02-01', dtype='datetime64[M]').astype('datetime64[D]')\ndates","203d476e":"plt.figure(figsize=(20,7))\nplt.grid(True)\nplt.plot(sales['S4248SM144NCEN'],label = 'Original')\nplt.plot(dates,true_predictions,label = 'Predicted')\nplt.legend()\nplt.show()","d291ebc8":"plt.figure(figsize=(20,7))\nplt.grid(True)\nplt.plot(sales['S4248SM144NCEN']['2018-01-01':],label = \"Original\")\nplt.plot(dates,true_predictions,label = \"Predicted\")\nplt.legend()\nplt.show()","985652b4":"Y_norm = scaler.fit_transform(Y.reshape(-1,1))\nY_norm = torch.FloatTensor(Y_norm).view(-1)\nfull_train_data = get_windows(Y_norm,window_size)","2ea8d7c4":"start = time.time()\nepochs = 100\nmodel.train()\nfor i in range(epochs):\n    for X_train,Y_train in full_train_data:\n        X_train = X_train.cuda()\n        Y_train = Y_train.cuda()\n        optimizer.zero_grad()\n        model.hidden = (torch.zeros(1,1,model.hidden_size).cuda(),\n                        torch.zeros(1,1,model.hidden_size).cuda())\n        Y_pred = model(X_train)\n        loss = criterion(Y_pred,Y_train)\n        loss.backward()\n        optimizer.step()\n    print(f\"Epoch {i+1} LOSS : {loss.item():.8f}\")\nend = time.time()\nprint(f\"Train Duration {int((end-start)\/60)} minutes {int((end-start)%60)} seconds\")","8ecbbf9f":"model.eval()\npreds = Y_norm[-window_size:].tolist()\nfor i in range(future):\n    X_test = torch.FloatTensor(preds[-window_size:]).cuda()\n    with torch.no_grad():\n        model.hidden = (torch.zeros(1,1,model.hidden_size).cuda(),\n                      torch.zeros(1,1,model.hidden_size).cuda())\n        preds.append(model(X_test).item())","0f548e12":"preds[-window_size:]","8f8d51a6":"true_predictions = scaler.inverse_transform(np.array(preds[-window_size:]).reshape(-1,1))\ntrue_predictions","f83de78d":"true_predictions = true_predictions.flatten()","524fd42c":"dates = np.arange('2019-02-01', '2020-02-01', dtype='datetime64[M]').astype('datetime64[D]')\ndates","ad465ea7":"plt.figure(figsize=(20,6))\nplt.grid(True)\nplt.plot(sales['S4248SM144NCEN'])\nplt.plot(dates,true_predictions)\nplt.show()","f238219d":"plt.figure(figsize=(20,6))\nplt.grid(True)\nplt.plot(sales['S4248SM144NCEN']['2017-01-01':])\nplt.plot(dates,true_predictions)\nplt.show()","f3c36623":"# Loading Data","dbdbf8b1":"## Testing on the last Window Size\nIn the code given below, we are predicting the values based off of the last window and then adding this predicted value to the previous window thus sliding the window forward","dd610b3a":"## Predicting into the Unknown Future\nUsing the same approach as explained earlier we are going to predict the Sales for the next year","eeff8561":"We'll be diving our Dataset into windows of size 12<br>\nAfter our model has been trained, we will predict the Sales for the next 12 months i.e. an Year","5cbd8efb":"# Alchohol Sales Prediction\n![alchohol](https:\/\/i0.wp.com\/xtalks.com\/wp-content\/uploads\/2020\/04\/drinks-e1585833542715.jpg?resize=1098%2C600&ssl=1)\n\nIn this notebook we are going to predict Alchohol Sales Time Series Data using LSTM Model in PyTorch.\n\n[Dataset Link](https:\/\/www.kaggle.com\/bulentsiyah\/for-simple-exercises-time-series-forecasting)\n\nThanks [@bulentsiyah](https:\/\/www.kaggle.com\/bulentsiyah\/) for the Dataset","45ee6857":"We have stored the Dates in special data type defined in Numpy","acd7e5c0":"## Inverting the normalization","7a33cbe9":"# Standard Imports\n","454d1d69":"We can observe that the Sales has a pattern for each year but overall Sales has increased from 1994 to 2019","875da4e1":"## Defining and Instatiating the LSTM Model,Optimizing and Loss Function","bc417dd1":"## Preparing the Train Data","9951541b":"We can see that our predicted data is normalized. Let's invert the normalization","0db10190":"## Inverting the Normalization","cd9b3db7":"# Training the Model\nHere we are training our model based on the Data excluding the last window. We'll be predicting the last window of our dataset which is also our Test Data","31930578":"In the code above, we are inserting a tuples into an array. Each tuple has Sales data for the given window size which in our case is an Year. We are also inserting the Sales for the next immediate month of the given window ","0c723b77":"Note that the gap here is due to the fact that both data are from different sources! Actually they are continous","be450467":"The predicted Data closely resembles our Original Data","a8d14e1a":"## Normalizing the Train Set","97b175b4":"# Defining Training and Testing Data","e4aa46ca":"## Training with the entire Dataset\nFor predicting the next Year Sales we are going to train our model over the entire Dataset this time! ","e084f27e":"# Intro to LSTMs\n> Long short-term memory (LSTM) is a recurrent neural network (RNN) architecture used in the field of deep learning LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series.\n\n                                                                    Wikipedia\n\nLSTM networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems.\n\nWe'll not be going into details of LSTM. If you're curious follow [this link](https:\/\/machinelearningmastery.com\/gentle-introduction-long-short-term-memory-networks-experts\/)"}}