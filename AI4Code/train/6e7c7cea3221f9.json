{"cell_type":{"87e1bf2d":"code","41061dc1":"code","fd672981":"code","22aa05a8":"code","fc5ed493":"code","44378249":"code","0e85629a":"code","a625f3c2":"code","18a0ef2e":"code","fd563929":"code","95f4351c":"code","c1cd1f7e":"code","03097c52":"code","80dff731":"code","eee1e301":"code","4da861ac":"code","395468f8":"code","79246565":"code","d41cefd1":"code","6fde2134":"code","13d91588":"code","21e13767":"code","9a784d6b":"code","5e5df16d":"code","e45ab39b":"markdown","1b13b78f":"markdown","d871ae07":"markdown","acafd8ca":"markdown","584873cb":"markdown","75385877":"markdown","6f649790":"markdown","755ee6c0":"markdown","5796a78f":"markdown","c37abc57":"markdown","fe5d5042":"markdown","bdc10e34":"markdown","554a0efe":"markdown","5362bf7d":"markdown","b5422958":"markdown","1007839e":"markdown","78ac6848":"markdown","c5c58444":"markdown","2daeb61b":"markdown","69148aef":"markdown","16b0e5d6":"markdown","b67a3e58":"markdown","23ac5e6c":"markdown","008e60ef":"markdown","3693a709":"markdown","0d66811a":"markdown","560fc3c6":"markdown","3d8e022a":"markdown","4f8ca0ef":"markdown","e5f1d797":"markdown","e08f9987":"markdown","16a370b8":"markdown"},"source":{"87e1bf2d":"#Importing libraries.\nimport functools\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nfrom tensorflow import feature_column\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split","41061dc1":"# Creating variables for urls of datasets.\nTRAIN_DATA_URL = \"https:\/\/storage.googleapis.com\/tf-datasets\/titanic\/train.csv\"\nTEST_DATA_URL = \"https:\/\/storage.googleapis.com\/tf-datasets\/titanic\/eval.csv\"","fd672981":"# Creating variables for paths of datasets.\ntrain_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\ntest_file_path = tf.keras.utils.get_file(\"eval.csv\",  TEST_DATA_URL)","22aa05a8":"#Converting train_file_path into pandas dataframe.\ntrain_df = pd.read_csv(train_file_path, header='infer')\ntest_df = pd.read_csv(test_file_path, header='infer')","fc5ed493":"#Take a look titanic dataset.\ntrain_df.head()","44378249":"#Creating the target and featrues variables.\nLABEL_COLUMN = 'survived'\nLABELS = [0, 1]\n# Let's specify file path, batch size, label name, missing value parameters in make_csv_dataset method.\ntrain_ds = tf.data.experimental.make_csv_dataset(\n        train_file_path,\n        batch_size = 3,\n        label_name=LABEL_COLUMN,\n        na_value=\"?\",\n        num_epochs= 1,\n        ignore_errors=True)\n# Let's create test dataset as above.\ntest_ds = tf.data.experimental.make_csv_dataset(\n        test_file_path,\n        batch_size=3,\n        label_name=LABEL_COLUMN,\n        na_value=\"?\",\n        num_epochs=1,\n        ignore_errors=True)","0e85629a":"for batch, label in train_ds.take(1):\n    print(label)\n    for key, value in batch.items():\n        print(f\"{key}: {value.numpy()}\")","a625f3c2":"# Setting numeric columns\nfeature_columns = []\n# numeric columns\nfor header in ['age', 'n_siblings_spouses', 'parch', 'fare']:\n    feature_columns.append(feature_column.numeric_column(header))","18a0ef2e":"titanic_df = pd.read_csv(train_file_path, header='infer')\ntitanic_df.describe()","fd563929":"# Bucketizing age columns\nage = feature_column.numeric_column('age')\nage_buckets = feature_column.bucketized_column(age, boundaries=[23, 28, 35])","95f4351c":"#Deteriming categorical columns\nh = {}\nfor col in titanic_df:\n    if col in ['sex', 'class', 'deck', 'embark_town', 'alone']:\n        print(col, ':', titanic_df[col].unique())\n        h[col] = titanic_df[col].unique()","c1cd1f7e":"# Converting categorical columns and encoding unique categorical values\nsex_type = feature_column.categorical_column_with_vocabulary_list(\n      'Type', h.get('sex').tolist())\nsex_type_one_hot = feature_column.indicator_column(sex_type)\n\nclass_type = feature_column.categorical_column_with_vocabulary_list(\n      'Type', h.get('class').tolist())\nclass_type_one_hot = feature_column.indicator_column(class_type)\n\ndeck_type = feature_column.categorical_column_with_vocabulary_list(\n      'Type', h.get('deck').tolist())\ndeck_type_one_hot = feature_column.indicator_column(deck_type)\n\nembark_town_type = feature_column.categorical_column_with_vocabulary_list(\n      'Type', h.get('embark_town').tolist())\nembark_town_type_one_hot = feature_column.indicator_column(embark_town_type)\n\nalone_type = feature_column.categorical_column_with_vocabulary_list(\n      'Type', h.get('alone').tolist())\nalone_one_hot = feature_column.indicator_column(alone_type)","03097c52":"# Embeding the \"deck\" column and reducing its dimension to 3.\ndeck = feature_column.categorical_column_with_vocabulary_list(\n      'deck', titanic_df.deck.unique())\ndeck_embedding = feature_column.embedding_column(deck, dimension=3)","80dff731":"# Reducing class column\nclass_hashed = feature_column.categorical_column_with_hash_bucket(\n      'class', hash_bucket_size=4)","eee1e301":"cross_type_feature = feature_column.crossed_column(['sex', 'class'], hash_bucket_size=5)","4da861ac":"feature_columns = []\n\n# appending numeric columns\nfor header in ['age', 'n_siblings_spouses', 'parch', 'fare']:\n    feature_columns.append(feature_column.numeric_column(header))\n    \n# appending bucketized columns\nage = feature_column.numeric_column('age')\nage_buckets = feature_column.bucketized_column(age, boundaries=[23, 28, 35])\nfeature_columns.append(age_buckets)\n\n# appending categorical columns\nindicator_column_names = ['sex', 'class', 'deck', 'embark_town', 'alone']\nfor col_name in indicator_column_names:\n    categorical_column = feature_column.categorical_column_with_vocabulary_list(\n        col_name, titanic_df[col_name].unique())\n    indicator_column = feature_column.indicator_column(categorical_column)\n    feature_columns.append(indicator_column)\n    \n# appending embedding columns\ndeck = feature_column.categorical_column_with_vocabulary_list(\n      'deck', titanic_df.deck.unique())\ndeck_embedding = feature_column.embedding_column(deck, dimension=3)\nfeature_columns.append(deck_embedding)\n\n# appending crossed columns\nfeature_columns.append(feature_column.indicator_column(cross_type_feature))","395468f8":"feature_layer = tf.keras.layers.DenseFeatures(feature_columns)  ","79246565":"val_df, test_df = train_test_split(test_df, test_size=0.4)","d41cefd1":"labels = train_df.pop(\"survived\")","6fde2134":"def pandas_to_dataset(dataframe, shuffle=True, batch_size=32):\n    dataframe = dataframe.copy()\n    labels = dataframe.pop('survived')\n    # To transform the DataFrame into a key-value pair. \n    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n    # To shuffle and batch\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.batch(batch_size)\n    return ds","13d91588":"batch_size=32\nval_ds = pandas_to_dataset(val_df, shuffle=False, batch_size=batch_size)\ntest_ds = pandas_to_dataset(test_df, shuffle=False, batch_size=batch_size)","21e13767":"model = tf.keras.Sequential([\n  feature_layer,\n  layers.Dense(128, activation='relu'),\n  layers.Dense(128, activation='relu'),\n  layers.Dropout(.1),\n  layers.Dense(1)\n])","9a784d6b":"model.compile(optimizer='adam', \n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])","5e5df16d":"model.fit(train_ds,\n          validation_data=val_ds,\n          epochs=10)","e45ab39b":"Appliying this function to both validation and test data.","1b13b78f":"Let's take a look columns of train dataset in the first batch.","d871ae07":"To stream the data into the training process with the dataset, I am going to create a function. ","acafd8ca":"To use one-hot encode, I am going to see the distinct values.","584873cb":"Let me split test_df into validation and test datasets. Hyperparameters are fine tuned using validation dataset and model is evaluated using test dataset.  ","75385877":"## Training the Model","6f649790":"That is all. In this tutorail, I am going to showed how to prepare tabular dataset to analyze and deal with multiple data types. ","755ee6c0":"Now I am going to create a feature layer. This layer will serve as the first (input) layer in the model.","5796a78f":"Now that I am going to put together what I've done. Let's create a list to hold all the feature.","c37abc57":"Don't forget to follow on Tirendaz Academy [YouTube-Tr](https:\/\/youtube.com\/c\/tirendazakademi), [YouTube-Eng](https:\/\/www.youtube.com\/channel\/UCFU9Go20p01kC64w-tmFORw), [Twitter](https:\/\/twitter.com\/TirendazAcademy), [Medium](https:\/\/tirendazacademy.medium.com), [GitHub](https:\/\/github.com\/TirendazAcademy) and [LinkedIn](https:\/\/www.linkedin.com\/in\/tirendaz-academy)","fe5d5042":"As you can see above dataseti dataset consist of numeric and categorical columns. You will need to mark \"survived\" columns as the target and mark the rest of the columns as features. To do this I am going to use tf.data.experimental.make_csv_dataset() method. This method reads CSV files into a dataset, where each element of the dataset is a (features, labels) tuple that corresponds to a batch of CSV rows.","bdc10e34":"Tabular data consist of rows and columns. The values of the categorical columns have to encode as one-hot encoding. In this tutorail, I am going to cover how to preparing tabular data. To show this, I'll use Titanic dataset. First of all, let's import libraries. ","554a0efe":"If you want, you can bin age into a bucket. First, let's take a look statistics of age column. To do this, I am going to use Pandas.","5362bf7d":"I am going to use get_files() method which downloads a file from a URL if it not already in the cache.","b5422958":"Now that I loaded train and test datasets. Let me arrange columns by feature types. First of all, I am going to designate numerics columns.","1007839e":"Let me try three bin boundaries for age : 23, 28, and 35.","78ac6848":"\"deck\" column has eight unique values so I am going to embed this column. ","c5c58444":"Pandas is the most popular library of Python. You can manipulate dataset with Pandas. To read these datasets, you can use read_csv () method in Pandas.","2daeb61b":"Let's use categorical_column_with_vocabulary_list since inputs are in string format. Let me keep track of these unique values using h variable.","69148aef":"## Building the Model","16b0e5d6":"Let me take a look the first five rows of train dataset.","b67a3e58":"The Titanic dataset is open source and tabular dataset. This dataset consist of columns as such age, gender, cabin grade, and whether or not they survived. Google provide this dataset. Let me create variables that contain URLs of train and test datasets.","23ac5e6c":"There may be interaction between passenger gender and cabin class. Let's encode those intercations using crossed_column() method.","008e60ef":"- [KC Tung, 2021, TensorFlow 2 Pocket Reference](https:\/\/www.amazon.com\/TensorFlow-Pocket-Reference-Building-Deploying\/dp\/1492089184)\n- [TensorFlow Tutorial](https:\/\/www.tensorflow.org\/tutorials)","3693a709":"## Loading the Dataset","0d66811a":"## Compiling the Model","560fc3c6":"## Resources","3d8e022a":"Let's reduce the dimensions of class columns using a hashed feature column. ","4f8ca0ef":"Take a look summary of the model.","e5f1d797":"Let me specify target variable.","e08f9987":"# Preparing Tabular Data with TensorFlow","16a370b8":"## Preprocessing the Datasets"}}