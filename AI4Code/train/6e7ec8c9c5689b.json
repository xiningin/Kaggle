{"cell_type":{"866ff858":"code","3f4fabbf":"code","b2953065":"code","da5139e4":"code","0aa76f99":"code","8ded9d9d":"code","fe2c46bf":"code","7508d2ec":"code","d952d287":"code","7fb9b60e":"code","7ebf4c6f":"code","fdafef5b":"code","38ae3b0b":"code","d8b8db2f":"code","ac97bbc7":"code","1679a950":"code","913a6b10":"code","35240234":"code","f5ee3e96":"code","16bce8f7":"code","a238f4e3":"code","f76e4f2f":"code","db2a7b9b":"code","ac22d576":"code","294534bb":"code","02815e09":"code","9e8c3d5a":"code","f203866b":"code","338c0f43":"code","4e8f3ac4":"code","ddd4a4df":"code","03cf2b3c":"code","b0557266":"code","86260a52":"code","4ba3d6f4":"code","f4478dd3":"code","ebf3c7d7":"code","59386bb1":"code","d8c91fab":"code","784d57f0":"code","7db9c595":"code","43ac4f58":"code","97bb1f7e":"code","d7d3bf0d":"code","d2eb9970":"code","8b40eaba":"code","ba9a24df":"code","a53b871b":"code","f890540c":"code","1c7ff415":"code","e7dcbf70":"code","48944347":"code","3674890d":"code","9f295fb8":"code","e6abcf4a":"code","c8a28ac1":"code","98e29c20":"code","f3421f2d":"code","42f8a07d":"code","a6ab0572":"code","b9b7dcdf":"code","7e020938":"code","dc363d65":"code","865206fa":"code","52ab1348":"code","a1e09d41":"code","5bc9a728":"code","4d1e1a92":"code","c56f8580":"code","9bcb9ae5":"code","b64b542e":"code","73185163":"code","1d5c26bf":"code","111b0326":"code","ad7c67f7":"code","dd76925d":"code","0ac41e83":"code","c7dcbd3b":"code","28eced98":"code","3a1eae08":"code","d85dc642":"code","b5b7086c":"code","908c19d2":"code","2bc3ce5a":"code","12951269":"code","55bff30d":"code","75e763b1":"code","80f80951":"code","93212c4e":"code","48fb66b8":"code","1aaa8c20":"code","15a8baed":"code","07688226":"code","985a706b":"code","cb99d5f1":"code","94ac740b":"code","209cd888":"code","cc0ba5b1":"code","97b3cacb":"code","7c6e1d2b":"code","889ba11b":"markdown","5f263ed7":"markdown","e24e9ce6":"markdown","1a9261ef":"markdown","5bc6fe49":"markdown","afb6566b":"markdown","44828cba":"markdown","2ea1fcfd":"markdown","9337fe9e":"markdown","746a556e":"markdown","8ea27b52":"markdown","520f92c2":"markdown","e9bf44de":"markdown","615151d2":"markdown","8e70a2ba":"markdown","c478fdec":"markdown","cfe2009b":"markdown","01ef5082":"markdown","ea2e162a":"markdown","db33a2be":"markdown","96c2adf7":"markdown","8d548b02":"markdown","2a9b9a83":"markdown","519d415c":"markdown","4985ab35":"markdown","77b821f4":"markdown","0579fd26":"markdown","368945c8":"markdown","a5b1499d":"markdown","994f94d7":"markdown","ff50255c":"markdown","cde40973":"markdown","35eca3b4":"markdown","a22c2e4c":"markdown","b91f5858":"markdown","f02039f5":"markdown","8c2c8dd1":"markdown","d9833f75":"markdown","61308f07":"markdown","f865ce7c":"markdown","142a1510":"markdown"},"source":{"866ff858":"from warnings import filterwarnings\nfilterwarnings(\"ignore\")","3f4fabbf":"import pandas as pd","b2953065":"import numpy as np","da5139e4":"import matplotlib.pyplot as plt","0aa76f99":"import seaborn as sns","8ded9d9d":"import statsmodels.api as sm","fe2c46bf":"import statsmodels.formula.api as smf","7508d2ec":"from sklearn.linear_model import LinearRegression","d952d287":"from sklearn.metrics import mean_squared_error,r2_score","7fb9b60e":"from sklearn.model_selection import train_test_split,cross_val_score,cross_val_predict,ShuffleSplit,GridSearchCV","7ebf4c6f":"from sklearn.decomposition import PCA","fdafef5b":"from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier","38ae3b0b":"from sklearn.preprocessing import scale","d8b8db2f":"from sklearn import model_selection","ac97bbc7":"from sklearn.neighbors import KNeighborsRegressor","1679a950":"from sklearn.ensemble import BaggingRegressor,RandomForestRegressor,BaseEnsemble,GradientBoostingRegressor","913a6b10":"pip install astor","35240234":"import astor","f5ee3e96":"import time","16bce8f7":"hts = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\nhts.head()","a238f4e3":"hts.dropna(inplace=True)","f76e4f2f":"one_hot_encoded = pd.get_dummies(hts[[\"League\",\"Division\",\"NewLeague\"]])\none_hot_encoded.head()","db2a7b9b":"new_hts = hts.drop([\"League\",\"Division\",\"NewLeague\",\"Salary\"],axis=1).astype(\"float64\")","ac22d576":"X = pd.concat([new_hts,one_hot_encoded[[\"League_N\",\"Division_W\",\"NewLeague_N\"]]],axis=1)\nX.head()","294534bb":"y = hts.Salary # Target-dependent variable","02815e09":"hts.shape","9e8c3d5a":"#Independent Variables\nX.shape","f203866b":"#Dependent Variables\ny.shape","338c0f43":"X_train = X.iloc[:210]\nX_test = X.iloc[210:]\ny_train = y[:210]\ny_test = y[210:]\n\nprint(\"X_train Shape: \",X_train.shape)\nprint(\"X_test Shape: \",X_test.shape)\nprint(\"y_train Shape: \",y_train.shape)\nprint(\"y_test Shape: \",y_test.shape)","4e8f3ac4":"bagging_model = BaggingRegressor(bootstrap_features=True).fit(X_train,y_train)","ddd4a4df":"#Number of Trees\nbagging_model.n_estimators","03cf2b3c":"#10 different tree\nbagging_model.estimators_","b0557266":"#Samples in each tree\nbagging_model.estimators_samples_[:1]","86260a52":"#Independent Variables in each tree\nbagging_model.estimators_features_","4ba3d6f4":"bagging_model","f4478dd3":"y_pred=bagging_model.predict(X_train)","ebf3c7d7":"#Train Error\nnp.sqrt(mean_squared_error(y_train,y_pred))","59386bb1":"r2_score(y_train,y_pred)","d8c91fab":"y_pred=bagging_model.predict(X_test)","784d57f0":"#Test Error\nnp.sqrt(mean_squared_error(y_test,y_pred))","7db9c595":"r2_score(y_test,y_pred)","43ac4f58":"second_tree = bagging_model.estimators_[1].fit(X_train,y_train).predict(X_test)","97bb1f7e":"#Test Error for second tree\nnp.sqrt(mean_squared_error(y_test,second_tree))","d7d3bf0d":"fourth_tree = bagging_model.estimators_[3].fit(X_train,y_train).predict(X_test)","d2eb9970":"#Test Error for fourth tree\nnp.sqrt(mean_squared_error(y_test,fourth_tree))","8b40eaba":"bagging_model","ba9a24df":"bagging_params = {\"n_estimators\":range(1,30)}","a53b871b":"bagging_cv_model = GridSearchCV(bagging_model,bagging_params,cv=15).fit(X_train,y_train)","f890540c":"bagging_cv_model.best_params_","1c7ff415":"tuned_bagging_model = BaggingRegressor(n_estimators=bagging_cv_model.best_params_[\"n_estimators\"]).fit(X_train,y_train)","e7dcbf70":"tuned_bagging_model","48944347":"y_pred=tuned_bagging_model.predict(X_train)","3674890d":"#Train Error\nnp.sqrt(mean_squared_error(y_train,y_pred))","9f295fb8":"r2_score(y_train,y_pred)","e6abcf4a":"y_pred=tuned_bagging_model.predict(X_test)","c8a28ac1":"#Test Error\nnp.sqrt(mean_squared_error(y_test,y_pred))","98e29c20":"r2_score(y_test,y_pred)","f3421f2d":"hts = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\nhts.head()","42f8a07d":"hts.dropna(inplace=True)","a6ab0572":"one_hot_encoded = pd.get_dummies(hts[[\"League\",\"Division\",\"NewLeague\"]])\none_hot_encoded.head()","b9b7dcdf":"new_hts = hts.drop([\"League\",\"Division\",\"NewLeague\",\"Salary\"],axis=1).astype(\"float64\")","7e020938":"X = pd.concat([new_hts,one_hot_encoded[[\"League_N\",\"Division_W\",\"NewLeague_N\"]]],axis=1)\nX.head()","dc363d65":"y = hts.Salary # Target-dependent variable","865206fa":"hts.shape","52ab1348":"#Independent Variables\nX.shape","a1e09d41":"#Dependent Variables\ny.shape","5bc9a728":"X_train = X.iloc[:210]\nX_test = X.iloc[210:]\ny_train = y[:210]\ny_test = y[210:]\n\nprint(\"X_train Shape: \",X_train.shape)\nprint(\"X_test Shape: \",X_test.shape)\nprint(\"y_train Shape: \",y_train.shape)\nprint(\"y_test Shape: \",y_test.shape)","4d1e1a92":"random_forests = RandomForestRegressor(random_state=60).fit(X_train,y_train)","c56f8580":"random_forests","9bcb9ae5":"random_forests.max_features","b64b542e":"#Number of Trees\nrandom_forests.n_estimators","73185163":"random_forests.min_samples_leaf","1d5c26bf":"random_forests.min_samples_split","111b0326":"random_forests.feature_importances_","ad7c67f7":"std= np.std([ tree.feature_importances_ for tree in random_forests.estimators_], axis=0)","dd76925d":"fig, ax = plt.subplots()\npd.Series(random_forests.feature_importances_,index=[X_train.columns]).plot.bar(yerr=std, ax=ax)\nax.set_title(\"Feature importances using MDI\")\nax.set_ylabel(\"Feature Imporances\")\nfig.tight_layout()","0ac41e83":"random_forests","c7dcbd3b":"y_pred=random_forests.predict(X_train)","28eced98":"#Train Error\nnp.sqrt(mean_squared_error(y_train,y_pred))","3a1eae08":"r2_score(y_train,y_pred)","d85dc642":"y_pred=random_forests.predict(X_test)","b5b7086c":"#Test Error\nnp.sqrt(mean_squared_error(y_test,y_pred))","908c19d2":"r2_score(y_test,y_pred)","2bc3ce5a":"random_forests","12951269":"random_forests_params = {\"max_depth\": list(range(1,20)),\n                         \"max_features\":[2,5,8,11,16],\n                         \"n_estimators\":[300,500,1000,1700]}","55bff30d":"random_forests = RandomForestRegressor(random_state=60)","75e763b1":"cv_random_forests = GridSearchCV(random_forests,random_forests_params,cv=7,n_jobs=-1)","80f80951":"#It takes nearly 10 minutes\nstart_time = time.time()\ncv_random_forests.fit(X_train,y_train)\nelapsed_time = time.time() - start_time\n\nprint(f\"Elapsed time for cross validation: \"\n      f\"{elapsed_time:.3f} seconds\")","93212c4e":"cv_random_forests.best_params_","48fb66b8":"random_forests_tuned= RandomForestRegressor(max_depth=cv_random_forests.best_params_[\"max_depth\"],\n                                            max_features=cv_random_forests.best_params_[\"max_features\"],\n                                            n_estimators=cv_random_forests.best_params_[\"n_estimators\"]).fit(X_train,y_train)","1aaa8c20":"random_forests_tuned","15a8baed":"y_pred=random_forests_tuned.predict(X_train)","07688226":"#Train Error\nnp.sqrt(mean_squared_error(y_train,y_pred))","985a706b":"r2_score(y_train,y_pred)","cb99d5f1":"y_pred=random_forests_tuned.predict(X_test)","94ac740b":"#Test Error\nnp.sqrt(mean_squared_error(y_test,y_pred))","209cd888":"r2_score(y_test,y_pred)","cc0ba5b1":"Importances = pd.DataFrame({\"Importance\":random_forests_tuned.feature_importances_*100},index=X_train.columns)","97b3cacb":"Importances","7c6e1d2b":"Importances.sort_values(by=\"Importance\",axis=0,ascending=True).plot(kind=\"barh\",color=\"b\")\nplt.xlabel(\"Feature Importances\")\nplt.ylabel(\"Features\")\nplt.title(\"Feature Importances\");","889ba11b":"### Prediction","5f263ed7":"**Created by Berkay Alan**\n\n**Non-Linear Models - Regression | Ensemble Learning - Bagging - Random Forests**\n\n**31 July 2021**\n","e24e9ce6":"Now we will remove NA values.","1a9261ef":"### Model","5bc6fe49":"We will do **One Hot Encoding** to categorical columns.","afb6566b":"As an example of an Ensemble method, we can train a group of Decision Tree classifiers, each on a different random subset of the training set. To make predictions, we obtain the predictions of all the individual trees, then predict the class that gets the most votes.","44828cba":"Let's look at the importances of features.","2ea1fcfd":"## Importing Libraries","9337fe9e":"In bagging, a random sample of data in a training set is selected with replacement\u2014meaning that the individual data points can be chosen more than once. After several data samples are generated, these weak models are then trained independently, and depending on the type of task\u2014regression or classification, for example\u2014the average or majority of those predictions yield a more accurate estimate. Bagging allows training instances to be sampled several times across multiple predictors.","746a556e":"\n- Ensemble Learning - Bagged Trees(Bagging) (Theory - Model- Tuning)\n- Ensemble Learning - Random Forests (Theory - Model- Tuning)\n","8ea27b52":"### Theory","520f92c2":"Github Repository Including:\n    \n  - K - Nearest Neighbors(KNN) (Theory - Model- Tuning)\n  - Support Vector Regression(SVR) (Theory - Model- Tuning)\n  - Non-Linear Support Vector Regression(SVR) (Theory - Model- Tuning)\n  - Regression(Decision) Trees (CART) (Theory - Model- Tuning)\n  - Gradient Boosting Machines(GBM)  (Theory - Model- Tuning)\n  - Light Gradient Boosting Machines(LGBM)  (Theory - Model- Tuning)\n  - XGBoost(Extreme Gradient Boosting)  (Theory - Model- Tuning)\n  - Catboost  (Theory - Model- Tuning)\n  \nCheck it out: https:\/\/github.com\/berkayalan\/Data-Science-Tutorials\/blob\/master\/Non-Linear%20Models%20-%20Regression.ipynb","e9bf44de":"- **The Elements of  Statistical Learning** - Trevor Hastie,  Robert Tibshirani, Jerome Friedman -  Data Mining, Inference, and Prediction (Springer Series in Statistics) \n\n\n\n- [**Classification And Regression Trees for Machine Learning**](https:\/\/machinelearningmastery.com\/classification-and-regression-trees-for-machine-learning\/)\n\n- [**Regression Trees by Statquest**](https:\/\/www.youtube.com\/watch?v=g9c66TUylZ4&ab_channel=StatQuestwithJoshStarmer)\n\n- [**Decision Tree Algorithm, Explained**](https:\/\/www.kdnuggets.com\/2020\/01\/decision-tree-algorithm-explained.html)\n\n- [**Ensemble methods: bagging, boosting and stacking**](https:\/\/towardsdatascience.com\/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)\n\n- [**Random Forests by Statquest**](https:\/\/www.youtube.com\/watch?v=J4Wdy0Wc_xQ&ab_channel=StatQuestwithJoshStarmer)\n\n- [**Why random forests outperform decision trees?**](https:\/\/towardsdatascience.com\/why-random-forests-outperform-decision-trees-1b0f175a0b5)","615151d2":"### Model Tuning","8e70a2ba":"Bootstrap aggregating(bagging), is a ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.","c478fdec":"### Model","cfe2009b":"Let's look at the importances of features.","01ef5082":"Let's check each tree independently.","ea2e162a":"For this section, I highly recommend you to read this well explained article: [**Ensemble methods: bagging, boosting and stacking**](https:\/\/towardsdatascience.com\/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)","db33a2be":"## Resources","96c2adf7":"For a real world example, we will work with **Hitters** dataset.\n\nIt can be downloaded here: https:\/\/www.kaggle.com\/floser\/hitters","8d548b02":"Now we will remove NA values.","2a9b9a83":"![image-2.png](attachment:image-2.png)\n\nPhoto is cited by: https:\/\/www.google.com\/url?sa=i&url=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2020%2F05%2Fdecision-tree-vs-random-forest-algorithm%2F&psig=AOvVaw2jevf2JFgvEKCBieh5yaHX&ust=1627289101408000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCICY4bvq_fECFQAAAAAdAAAAABAD","519d415c":"### Model Tuning","4985ab35":"Now we will split our dataset as train and test set.","77b821f4":"## Ensemble Learning  - Bagged Trees (Bagging)","0579fd26":"**For more Tutorial:** https:\/\/github.com\/berkayalan","368945c8":"Yet another great quality of Random Forests is that they make it easy to measure the relative importance of each feature. Scikit-Learn measures a feature\u2019s importance by looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest). More precisely, it is a weighted average, where each node\u2019s weight is equal to the number of training samples that are associated with it. ","a5b1499d":"**Content**\n","994f94d7":"Another advantage of sampling over the features is that it makes the decision making process more robust to missing data: observations (from the training dataset or not) with missing data can still be regressed or classified based on the trees that take into account only features where data are not missing. Thus, random forest algorithm combines the concepts of bagging and random feature subspace selection to create more robust models.","ff50255c":"We will do **One Hot Encoding** to categorical columns.","cde40973":"## Ensemble Learning - Random Forests","35eca3b4":"### Prediction","a22c2e4c":"![Screen%20Shot%202021-07-24%20at%2015.43.45.png](attachment:Screen%20Shot%202021-07-24%20at%2015.43.45.png)\n\nPhoto is cited by this book: Hands-On Machine Learning with Scikit-Learn & TensorFlow","b91f5858":"Random Forest is also an example of ensemble learning, in which we combine multiple machine learning algorithms to obtain better predictive performance.\n\nThe random forest algorithm is an extension of the bagging method as it utilizes both bagging and feature randomness to create an uncorrelated forest of decision trees. Feature randomness, also known as feature bagging or \u201cthe random subspace method\u201d, generates a random subset of features, which ensures low correlation among decision trees. This is a key difference between decision trees and random forests. While decision trees consider all the possible feature splits, random forests only select a subset of those features.\n\nLet's try to understand with an example. For example, I want to watch a movie today and I am not sure what to watch. After calling one of my best friends, she recommend a movie to me according to my old preferences that she know. At this point, my old preferences are training set for her. It's a classical decision tree. But if I would get recommendations from my 20 different friends and select most voted movie, that would be **Random Forests**.\n\nRandom forest algorithms have three main hyperparameters, which need to be set before training. These include node size, the number of trees, and the number of features sampled. From there, the random forest classifier can be used to solve for regression or classification problems.","f02039f5":"Now we will split our dataset as train and test set.","8c2c8dd1":"Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors. The aggregation function is typically the statistical mode (the most frequent prediction, just like a hard voting classifier) for classification, or the average for regression. Each individual predictor has a higher bias than if it were trained on the original training set, but aggregation reduces both bias and variance.","d9833f75":"**<span style='color:Blue'> Check out My Github for other Regression Models  <\/span>**","61308f07":"### Theory","f865ce7c":"In order to understand Bagged trees, first we need to get familiar with **Ensemble Learning**.\n\nEnsemble learning gives credence to the idea of the \u201cwisdom of crowds,\u201d which suggests that the decision-making of a larger group of people is typically better than that of an individual expert. Similarly, ensemble learning refers to a group (or ensemble) of base learners, or models, which work collectively to achieve a better final prediction. A single model, also known as a base or weak learner, may not perform well individually due to high variance or high bias. However, when weak learners are aggregated, they can form a strong learner, as their combination reduces bias or variance, yielding better model performance.\n\nEnsemble methods are frequently illustrated using decision trees as this algorithm can be prone to overfitting (high variance and low bias) when it hasn\u2019t been pruned and it can also lend itself to underfitting (low variance and high bias) when it\u2019s very small, like a decision stump, which is a decision tree with one level. Remember, when an algorithm overfits or underfits to its training set, it cannot generalize well to new datasets, so ensemble methods are used to counteract this behavior to allow for generalization of the model to new datasets. ","142a1510":"For a real world example, we will work with **Hitters** dataset.\n\nIt can be downloaded here: https:\/\/www.kaggle.com\/floser\/hitters"}}