{"cell_type":{"1c453320":"code","0c64b253":"code","d2a017df":"code","42218011":"code","d1c645e7":"code","429c5712":"code","55d19004":"code","cb694b19":"code","8c3998dc":"code","76864b5b":"code","75a151f7":"code","5705b392":"code","db7615b1":"code","6d7e309e":"code","f48e4627":"code","2fd175b4":"code","b77e53af":"code","42b6f5ce":"code","35f3f0a5":"code","569f9dfc":"code","596c8d6e":"code","706775ee":"code","486fa556":"code","b8312fdf":"code","f0036191":"markdown","8b50273d":"markdown","9a46ab8a":"markdown","2ff19d66":"markdown","0e68c75d":"markdown","857b9883":"markdown","e6bd9e73":"markdown","a208f1e8":"markdown","4976eeb1":"markdown","07247f15":"markdown","805779bd":"markdown","033bdfe4":"markdown","005b6122":"markdown","2c262c25":"markdown","349a7a53":"markdown","b83bdfcd":"markdown","051019bb":"markdown","1bcd9047":"markdown","a0910249":"markdown","0f4da91c":"markdown","6f71aee0":"markdown","11253cdf":"markdown","72a79a36":"markdown","26b19ae5":"markdown","26d03b51":"markdown","017fb617":"markdown","d6517638":"markdown","e3df3b3f":"markdown","ad66eb80":"markdown","5a46b0a8":"markdown"},"source":{"1c453320":"import torch\nimport torchaudio\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport librosa\nimport librosa.display\n\nimport IPython.display as ipd","0c64b253":"# filename = \"..\/input\/birdsong-recognition\/train_audio\/nutwoo\/XC462016.mp3\"\nfilename = '..\/input\/birdsong-recognition\/train_audio\/balori\/XC101614.mp3'\n\nwaveform, sample_rate = torchaudio.load(filename)\n\nprint(\"Shape of waveform {}\".format(waveform.size()))\nprint(\"Sample rate of wavefor {}\".format(sample_rate))\n\nplt.figure(figsize=(14,5))\nplt.plot(waveform.t()) # transpose\n","d2a017df":"ipd.Audio(waveform, rate=sample_rate)","42218011":"specgram = torchaudio.transforms.Spectrogram()(waveform)\n\nprint(\"Shape of Spectrogram {}\".format(specgram.size()))\n\nplt.figure(figsize=(14,5))\nplt.imshow(specgram.log2()[0,:,:1200].numpy(), cmap='gray')","d1c645e7":"specgram = torchaudio.transforms.MelSpectrogram()(waveform)\n\nprint(\"Shape of MelSpectrogram {}\".format(specgram.size()))\n\nplt.figure(figsize=(14,5))\nplt.imshow(specgram.log2()[0,:,:1000].numpy(), cmap='gray')","429c5712":"new_sample_rate = sample_rate \/ 10\n\nchannel = 0\n\nresampled = torchaudio.transforms.Resample(sample_rate, new_sample_rate)(waveform[channel,:].view(1,-1))\n\nprint(\"Shape of resampled waveform: {}\".format(resampled.size()))\n\nplt.figure(figsize=(14,5))\nplt.plot(resampled[0,:].numpy())","55d19004":"ipd.Audio(resampled, rate=new_sample_rate)","cb694b19":"print(\"Min. of waveform {} \\n Max. of waveform {} \\n Mean of waveform {}\".format(waveform.min(), waveform.max(), waveform.mean()))","8c3998dc":"def normalize(signal):\n    signal_minusmean = signal - signal.mean()\n    return signal_minusmean \/ signal_minusmean.abs().max()\n\n#Normalizing waveform\n# print(\"After normalizing waveform...\")\n# print(\"Min. of waveform {}\".format(normalize(waveform).min().item()))\n# print(\"Max. of waveform {}\".format(normalize(waveform).max().item()))\n# print(\"Mean. of waveform {}\".format(normalize(waveform).mean().item()))","76864b5b":"# Applying Mu Law encoding\nencoded = torchaudio.transforms.MuLawEncoding()(waveform)\n\nprint(\"Shape of encoded waveform {}\".format(encoded.size()))\n\nplt.figure(figsize=(14,5))\nplt.plot(encoded[0,:].numpy())","75a151f7":"ipd.Audio(encoded, rate=sample_rate)","5705b392":"reconstructed = torchaudio.transforms.MuLawDecoding()(encoded)\n\nprint(\"Shape of recovered waveform {}\".format(reconstructed.size()))\n\nplt.figure(figsize=(14,5))\nplt.plot(reconstructed[0,:].numpy())","db7615b1":"ipd.Audio(reconstructed, rate=sample_rate)","6d7e309e":"err = ((waveform-reconstructed).abs() \/ waveform.abs()).median()\n\nprint(\"Median error difference between original waveform and its reconstructed version is {:.2%}\".format(err))","f48e4627":"mu_law_encoding_waveform = torchaudio.functional.mu_law_encoding(waveform, quantization_channels=256)\n\nprint(\"Shape of transformed waveform: {}\".format(mu_law_encoding_waveform.size()))\n\nplt.figure(figsize=(14,5))\nplt.plot(mu_law_encoding_waveform[0,:].numpy())","2fd175b4":"computed = torchaudio.functional.compute_deltas(specgram.contiguous(), win_length=3)\n\nprint(\"Shape of Computed deltas {}\".format(computed.size()))\n\nplt.figure(figsize=(14,5))\nplt.imshow(computed.log2()[0,:,:1000].numpy(), cmap='gray')","b77e53af":"gain_waveform = torchaudio.functional.gain(waveform, gain_db=5.0)\n\nprint(\"Min. of gain_waveform {} \\nMax. of gain_waveform {} \\nMean of gain_waveform {}\".format(gain_waveform.min(), gain_waveform.max(), gain_waveform.mean()))","42b6f5ce":"ipd.Audio(gain_waveform, rate=sample_rate)","35f3f0a5":"dither_waveform = torchaudio.functional.dither(waveform)\nprint(\"Min of dither_waveform: {}\\nMax of dither_waveform: {}\\nMean of dither_waveform: {}\".format(dither_waveform.min(), dither_waveform.max(), dither_waveform.mean()))","569f9dfc":"ipd.Audio(dither_waveform, rate=sample_rate)","596c8d6e":"lowpass_waveform = torchaudio.functional.lowpass_biquad(waveform, sample_rate, cutoff_freq=3000)\n\nprint(\"Min. of lowpass_waveform: {}\\nMax. of lowpass_waveform: {}\\nMean of lowpass_waveform: {}\".format(lowpass_waveform.min(), lowpass_waveform.max(), lowpass_waveform.mean()))\n\nplt.figure(figsize=(14,5))\nplt.plot(lowpass_waveform.t().numpy())","706775ee":"ipd.Audio(lowpass_waveform, rate=sample_rate)","486fa556":"highpass_waveform = torchaudio.functional.highpass_biquad(waveform, sample_rate, cutoff_freq=2000)\n\nprint(\"Min of highpass_waveform: {}\\nMax of highpass_waveform: {}\\nMean of highpass_waveform: {}\".format(highpass_waveform.min(), highpass_waveform.max(), highpass_waveform.mean()))\n\nplt.figure(figsize=(14,5))\nplt.plot(highpass_waveform.t().numpy())","b8312fdf":"ipd.Audio(highpass_waveform, rate=sample_rate)","f0036191":"## Filtered (high-pass) Audio:","8b50273d":"## Now lets decode this waveform","9a46ab8a":"# Applying Filters to our waveform: using torchaudio.functional","2ff19d66":"## gain_waveform Audio:","0e68c75d":"## Decoded Audio:","857b9883":"## Filters : High-pass filter(Second order)","e6bd9e73":"## Functional: gain\n### Applies amplification\/attenuation to the whole waveform****","a208f1e8":"## Functional: dither\n### Increases the perceived dynamic range of audio stored at a particular bit-depth","4976eeb1":"# If you like my kernel, do upvote \ud83d\udd4a\ufe0f\ud83d\udd4a\ufe0f\ud83d\udd4a\ufe0f","07247f15":"## Transformation : Log of spectrogram on log scale","805779bd":"## Original Audio:","033bdfe4":"### Our signal is already between [-1, 1], but had it not been, we would have used the following normalizing function","005b6122":"## Functional: Compute_deltas \n### To compute delta cofficients of a tensor","2c262c25":"## Picking up a random Audiofile for all the operatons moving forward","349a7a53":"# Transformations:\n## ***torchaudio.transforms***\n### Torchaudio supports the [following](https:\/\/pytorch.org\/audio\/transforms.html) transformations. We will be looking at some of them here.","b83bdfcd":"# Functions:\n## All these transformations that we saw till now rely on stateless *functions* for their computations, which are availabe under *torchaudio.functional*\n## torchaudio.functional\n### Functions to perform common audio operations [Link](https:\/\/pytorch.org\/audio\/functional.html)","051019bb":"## Resampled Audio:","1bcd9047":"## Filters : Low-pass filter(Second order)","a0910249":"## Filtered (low-pass) Audio:","0f4da91c":"## Transformation : Resampling the waveform, one channel at a time","6f71aee0":"### Sounds like original only !","11253cdf":"### Update : \n#### Added audio widget to physically analyse the changes\n\n# \ud83e\udd89 Cornell Birdcall Identification:\n![](https:\/\/imgc.artprintimages.com\/img\/print\/a-tawny-frogmouth-owl-podargus-strigoides-at-the-fort-worth-zoo_u-l-pncfu00.jpg?h=550&p=0&w=550&background=fbfbfb)\n\n# Introduction:\n### This notebook aims at analysing various transformations and important functions that can be used to encode\/transform audio data","72a79a36":"### Observe how the output from *torchaudio.functional.mu_law_encoding* is same as output from *torchaudio.transforms.MuLawEncoding *","26b19ae5":"## Mu-Law encoded Audio:","26d03b51":"## Dither_waveform Audio:","017fb617":"### Reference:    torchaudio [tutorial](https:\/\/pytorch.org\/tutorials\/beginner\/audio_preprocessing_tutorial.html#functional)","d6517638":"## Comparing original waveform with its reconstructed version","e3df3b3f":"## Transformation : Mu-Law encoding\n### The signal must be between [-1,1] for Mu-Law encoding","ad66eb80":"## Transformation : MelSpectrogram on log scale","5a46b0a8":"## Functional: Mu Law encoding using functional "}}