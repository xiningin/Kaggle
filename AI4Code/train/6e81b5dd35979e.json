{"cell_type":{"7ba8879a":"code","32375096":"code","0585bb89":"code","d668b0ff":"code","bf5380eb":"code","c3b1dd21":"code","5226d794":"code","5eead3bc":"code","dbafad60":"code","66b1792c":"code","ce7287ff":"code","83175004":"code","e45d312d":"code","90604125":"code","c02dcaf3":"code","b8a3e532":"code","56000793":"code","b327ce7e":"code","c9e689a5":"code","a62459d1":"code","43d11060":"code","7b99c251":"code","469c49fa":"code","ced484bb":"code","49963958":"code","2f968bbc":"code","c470f30b":"code","f650fa26":"code","9cdd3320":"code","7e99df53":"code","659cd9b3":"code","feeb9ea6":"code","5d03b326":"code","405fbd30":"code","2098cebf":"code","2413f6f8":"code","3b215a8e":"code","56e61d89":"code","c27482fc":"code","c6a77982":"code","d1de1f19":"code","8c1a2699":"code","f2832a81":"code","458ad010":"code","f47c95ea":"code","998fdb27":"code","6a0ab402":"code","2ec19fc4":"code","70d8d204":"code","d688bb05":"code","d51b42b2":"code","9c03224c":"code","3ac95fa9":"code","c3100251":"code","2775194a":"code","400881cc":"code","8014b9d4":"code","66c3f7b3":"code","fc47d638":"code","ddd2151d":"code","fb4e7408":"code","4a1d9b04":"code","9d6a1d8f":"code","730973ab":"code","2b4b2a95":"code","d4db0b12":"code","786a84a6":"code","04807728":"code","73292bdf":"code","8a7c52a4":"code","94b99ed5":"code","d6f0b81f":"markdown","75bbd06b":"markdown","9d9a39fb":"markdown","f1b9bf5e":"markdown","485af834":"markdown","3a4b42c1":"markdown","839ff404":"markdown","7dbc8a7f":"markdown","62091a0e":"markdown","2d3ac656":"markdown","09d30e94":"markdown","6c2973a7":"markdown","e0882b23":"markdown","12bfd509":"markdown","82c84982":"markdown","565026d0":"markdown","14d40c73":"markdown","7c575027":"markdown","12ab4c0c":"markdown","e2264e3e":"markdown","97bff705":"markdown","3efee05b":"markdown","69b267f5":"markdown","cd8d7135":"markdown","f3d7f12e":"markdown","43051f76":"markdown","42b798b2":"markdown","803a9b38":"markdown","ce5a9de0":"markdown","361992fa":"markdown","a2e7a7f6":"markdown","f52d8f73":"markdown","347d2f47":"markdown","38afd798":"markdown","9a2a2ab3":"markdown","58da5919":"markdown","985df8c1":"markdown","40e1d3b0":"markdown","00111301":"markdown","f96b35ac":"markdown","908bcac4":"markdown","1b5af58d":"markdown","2266fd48":"markdown","5a45ed6b":"markdown","125295fd":"markdown","47a115f8":"markdown","4bd1b62b":"markdown","dd1cde6e":"markdown","f3ed078c":"markdown","278a4c38":"markdown","3105dc4b":"markdown","27ce04c8":"markdown","b3abedf6":"markdown","f1922013":"markdown","5d3ce538":"markdown","a35c23e9":"markdown","24fc9fe9":"markdown","b2b87632":"markdown","897c49a4":"markdown","204805ef":"markdown","8709dd34":"markdown"},"source":{"7ba8879a":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport matplotlib.pyplot as plt\n%matplotlib inline","32375096":"from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,BaggingClassifier,ExtraTreesClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score\nfrom sklearn import metrics\nfrom datetime import datetime\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import StratifiedKFold","0585bb89":"data = pd.read_csv(\"..\/input\/adult-census-income\/adult.csv\")\ndata.head()","d668b0ff":"data.shape","bf5380eb":"attrib, counts = np.unique(data['workclass'], return_counts = True)\nmost_freq_attrib = attrib[np.argmax(counts, axis = 0)]\ndata['workclass'][data['workclass'] == '?'] = most_freq_attrib \n\nattrib, counts = np.unique(data['occupation'], return_counts = True)\nmost_freq_attrib = attrib[np.argmax(counts, axis = 0)]\ndata['occupation'][data['occupation'] == '?'] = most_freq_attrib \n\nattrib, counts = np.unique(data['native.country'], return_counts = True)\nmost_freq_attrib = attrib[np.argmax(counts, axis = 0)]\ndata['native.country'][data['native.country'] == '?'] = most_freq_attrib ","c3b1dd21":"# for later use\ndata_num = data.copy()\ndata1 = data.copy()\ndata.head(10)","5226d794":"hs_grad = ['HS-grad','11th','10th','9th','12th']\nelementary = ['1st-4th','5th-6th','7th-8th']\n\n# replace elements in list.\ndata1['education'].replace(to_replace = hs_grad,value = 'HS-grad',inplace = True)\ndata1['education'].replace(to_replace = elementary,value = 'elementary_school',inplace = True)\n\ndata1['education'].value_counts()","5eead3bc":"married= ['Married-spouse-absent','Married-civ-spouse','Married-AF-spouse']\nseparated = ['Separated','Divorced']\n\n#replace elements in list.\ndata1['marital.status'].replace(to_replace = married ,value = 'Married',inplace = True)\ndata1['marital.status'].replace(to_replace = separated,value = 'Separated',inplace = True)\n\ndata1['marital.status'].value_counts()","dbafad60":"self_employed = ['Self-emp-not-inc','Self-emp-inc']\ngovt_employees = ['Local-gov','State-gov','Federal-gov']\n\n#replace elements in list.\ndata1['workclass'].replace(to_replace = self_employed ,value = 'Self_employed',inplace = True)\ndata1['workclass'].replace(to_replace = govt_employees,value = 'Govt_employees',inplace = True)\n\ndata1['workclass'].value_counts()","66b1792c":"del_cols = ['education.num']\ndata1.drop(labels = del_cols,axis = 1,inplace = True)","ce7287ff":"num_col_new = ['age','capital.gain', 'capital.loss',\n       'hours.per.week','fnlwgt']\ncat_col_new = ['workclass', 'education', 'marital.status', 'occupation','relationship',\n               'race', 'sex', 'income']","83175004":"from sklearn.pipeline import Pipeline\nfrom sklearn.base import TransformerMixin\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\n\nscaler = MinMaxScaler()\npd.DataFrame(scaler.fit_transform(data1[num_col_new]),columns = num_col_new).head(5)","e45d312d":"class DataFrameSelector(TransformerMixin):\n    def __init__(self,attribute_names):\n        self.attribute_names = attribute_names\n                \n    def fit(self,X,y = None):\n        return self\n    \n    def transform(self,X):\n        return X[self.attribute_names]\n    \n    \nclass num_trans(TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        df = pd.DataFrame(X)\n        df.columns = num_col_new \n        return df\n        \n    \n    \npipeline = Pipeline([('selector',DataFrameSelector(num_col_new)),  \n                     ('scaler',MinMaxScaler()),\n                    ('transform',num_trans())])","90604125":"num_df = pipeline.fit_transform(data1)\nnum_df.shape","c02dcaf3":"# columns which I don't need after creating dummy variables dataframe\ncols = ['workclass_Govt_employess','education_Some-college',\n        'marital-status_Never-married','occupation_Other-service',\n        'race_Black','sex_Male','income_>50K']","b8a3e532":"class dummies(TransformerMixin):\n    def __init__(self,cols):\n        self.cols = cols\n    \n    def fit(self,X,y = None):\n        return self\n    \n    def transform(self,X):\n        df = pd.get_dummies(X)\n        df_new = df[df.columns.difference(cols)] \n#difference returns the original columns, with the columns passed as argument removed.\n        return df_new\n\npipeline_cat=Pipeline([('selector',DataFrameSelector(cat_col_new)),\n                      ('dummies',dummies(cols))])\ncat_df = pipeline_cat.fit_transform(data1)\ncat_df.shape","56000793":"cat_df['id'] = pd.Series(range(cat_df.shape[0]))\nnum_df['id'] = pd.Series(range(num_df.shape[0]))","b327ce7e":"final_df = pd.merge(cat_df,num_df,how = 'inner', on = 'id')\nprint(f\"Number of observations in final dataset: {final_df.shape}\")","c9e689a5":"y = final_df['income_<=50K']\nfinal_df.drop(labels = ['id','income_<=50K','fnlwgt'],axis = 1,inplace = True)\nX = final_df","a62459d1":"sns.countplot(x=\"income\", data= data)\nplt.show()\ndata[\"income\"].value_counts()","43d11060":"X_train1,X_test1,y_train1,y_test1 = train_test_split(X,y,test_size =0.15,random_state = 42)\n#fitting the model\nlr=LogisticRegression()\nlr.fit(X_train1,y_train1)\n# predict \ny_pred4=lr.predict(X_test1)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test1, y_pred4))\nprint(\"Precision:\",metrics.precision_score(y_test1, y_pred4))\nprint(\"Recall:\",metrics.recall_score(y_test1, y_pred4))\nprint(\"F1 score:\",metrics.f1_score(y_test1, y_pred4))\nprint(\"AUC :\",metrics.roc_auc_score(y_test1, y_pred4))","7b99c251":"from imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler()\nX_rus, y_rus = rus.fit_sample(X, y)","469c49fa":"X_rus = pd.DataFrame(X_rus)\nX_rus.columns = ['education_Assoc-acdm', 'education_Assoc-voc', 'education_Bachelors',\n       'education_Doctorate', 'education_HS-grad', 'education_Masters',\n       'education_Preschool', 'education_Prof-school',\n       'education_elementary_school', 'gender_Female',\n       'marital-status_Married', 'marital-status_Separated',\n       'marital-status_Widowed', 'occupation_Adm-clerical',\n       'occupation_Armed-Forces', 'occupation_Craft-repair',\n       'occupation_Exec-managerial', 'occupation_Farming-fishing',\n       'occupation_Handlers-cleaners', 'occupation_Machine-op-inspct',\n       'occupation_Priv-house-serv', 'occupation_Prof-specialty',\n       'occupation_Protective-serv', 'occupation_Sales',\n       'occupation_Tech-support', 'occupation_Transport-moving',\n       'race_Amer-Indian-Eskimo', 'race_Asian-Pac-Islander', 'race_Other',\n       'race_White', 'relationship_Husband', 'relationship_Not-in-family',\n       'relationship_Other-relative', 'relationship_Own-child',\n       'relationship_Unmarried', 'relationship_Wife',\n       'workclass_Govt_employees', 'workclass_Never-worked',\n       'workclass_Private', 'workclass_Self_employed', 'workclass_Without-pay',\n       'age', 'capital-gain', 'capital-loss', 'hours-per-week']\ny_rus = pd.DataFrame(y_rus)\ny_rus.columns = [\"income\"]","ced484bb":"sns.countplot(x=y_rus[\"income\"])\nplt.show()\n","49963958":"X_train,X_test,y_train,y_test = train_test_split(X_rus,y_rus,test_size =0.15,random_state = 42)","2f968bbc":"# Spot-Check Algorithms\ndef GetBasedModel():\n    basedModels = []\n    basedModels.append(('LR'   , LogisticRegression()))\n    basedModels.append(('LDA'  , LinearDiscriminantAnalysis()))\n    basedModels.append(('KNN'  , KNeighborsClassifier()))\n    basedModels.append(('CART' , DecisionTreeClassifier()))\n    basedModels.append(('NB'   , GaussianNB()))\n    basedModels.append(('AB'   , AdaBoostClassifier()))\n    basedModels.append(('GBM'  , GradientBoostingClassifier()))\n    basedModels.append(('RF'   , RandomForestClassifier()))\n    basedModels.append(('ET'   , ExtraTreesClassifier()))\n\n    \n    return basedModels","c470f30b":"def BasedLine2(X_train, y_train,models):\n    # Test options and evaluation metric\n    num_folds = 3\n    scoring = 'accuracy'\n\n    results = []\n    names = []\n    for name, model in models:\n        kfold = StratifiedKFold(n_splits=num_folds, random_state=10)\n        cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n        \n    return names,results","f650fa26":"models = GetBasedModel()\nnames,results = BasedLine2(X_train, y_train,models)","9cdd3320":"def ScoreDataFrame(names,results):\n    def floatingDecimals(f_val, dec=3):\n        prc = \"{:.\"+str(dec)+\"f}\" \n    \n        return float(prc.format(f_val))\n\n    scores = []\n    for r in results:\n        scores.append(floatingDecimals(r.mean(),4))\n\n    scoreDataFrame = pd.DataFrame({'Model':names, 'Score': scores})\n    return scoreDataFrame\nbasedLineScore = ScoreDataFrame(names,results)\nbasedLineScore.sort_values(by='Score', ascending=False)","7e99df53":"%%time\nfrom sklearn.model_selection import GridSearchCV\nlr = LogisticRegression(class_weight='balanced',random_state=42)\nparam_grid = { \n    'C': [0.1,0.2,0.3,0.4],\n    'penalty': ['l1', 'l2'],\n    'class_weight':[{0: 1, 1: 1},{ 0:0.67, 1:0.33 },{ 0:0.75, 1:0.25 },{ 0:0.8, 1:0.2 }]}\nCV_rfc = GridSearchCV(estimator=lr, param_grid=param_grid, cv= 5)\nCV_rfc.fit(X_train, y_train)\nprint(CV_rfc.best_params_)","659cd9b3":"%%time\n#fitting the model\nlr1=LogisticRegression(C=0.4, random_state=4 ,penalty='l1', class_weight={0:1,1:1})\nlr1.fit(X_train,y_train)\n# predict \ny_pred1=lr1.predict(X_test)","feeb9ea6":"cf_matrix = confusion_matrix(y_test, y_pred1)\nsns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","5d03b326":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred1))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred1))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred1))\nprint(\"F1 score:\",metrics.f1_score(y_test, y_pred1))\nprint(\"AUC :\",metrics.roc_auc_score(y_test, y_pred1))","405fbd30":"fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred1)\nauc = metrics.roc_auc_score(y_test, y_pred1)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","2098cebf":"%%time\nfrom sklearn.model_selection import RandomizedSearchCV\nlr = LogisticRegression(class_weight='balanced',random_state=42)\nparam_grid = { \n    'C': [0.1,0.2,0.3,0.4],\n    'penalty': ['l1', 'l2'],\n    'class_weight':[{0: 1, 1: 1},{ 0:0.67, 1:0.33 },{ 0:0.75, 1:0.25 },{ 0:0.8, 1:0.2 }]}\nCV_rfc = RandomizedSearchCV(estimator=lr, param_distributions=param_grid, cv= 5,random_state=1)\nCV_rfc.fit(X_train, y_train)\nprint(CV_rfc.best_params_)\n","2413f6f8":"%%time\n#fitting the model\nlr2=LogisticRegression(C=0.3, random_state=4 ,penalty='l2', class_weight={0:1,1:1})\nlr2.fit(X_train,y_train)\n# predict \ny_pred2=lr2.predict(X_test)","3b215a8e":"cf_matrix = confusion_matrix(y_test, y_pred2)\nsns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","56e61d89":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred2))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred2))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred2))\nprint(\"F1 score:\",metrics.f1_score(y_test, y_pred2))\nprint(\"AUC :\",metrics.roc_auc_score(y_test, y_pred2))","c27482fc":"fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred2)\nauc = metrics.roc_auc_score(y_test, y_pred2)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","c6a77982":"%%time\nfrom sklearn.feature_selection import RFE\n\n# feature extraction\nlr = LogisticRegression()\nrfe = RFE(lr, 15)\nlr3 = rfe.fit(X_train, y_train)\n\nprint(\"Num Features: \", lr3.n_features_)\nprint(\"Selected Features: \",  lr3.support_)\nprint(\"Feature Ranking: \", lr3.ranking_)","d1de1f19":"feature = list(X_train.columns.values) \nprint(sorted(zip(map(lambda x: round(x, 4), lr3.ranking_), feature)))\n","8c1a2699":"\nX_train_f = X_train[['age','capital-gain','capital-loss','education_Bachelors','education_Doctorate','education_Masters',\n                     'education_Preschool','education_Prof-school','education_elementary_school',\n                     'hours-per-week','occupation_Priv-house-serv','relationship_Not-in-family','relationship_Other-relative'\n                     ,'relationship_Own-child','relationship_Unmarried']]\n\nX_test_f = X_test[['age','capital-gain','capital-loss','education_Bachelors','education_Doctorate','education_Masters',\n                     'education_Preschool','education_Prof-school','education_elementary_school',\n                     'hours-per-week','occupation_Priv-house-serv','relationship_Not-in-family','relationship_Other-relative'\n                     ,'relationship_Own-child','relationship_Unmarried']]\n\nlr4=LogisticRegression(C=0.4, random_state=4 ,penalty='l2', class_weight={0:1,1:1})\n%time lr4.fit(X_train_f,y_train)","f2832a81":"# predict \ny_pred4=lr4.predict(X_test_f)\ncf_matrix = confusion_matrix(y_test, y_pred4)\nsns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred4))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred4))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred4))\nprint(\"F1 score:\",metrics.f1_score(y_test, y_pred4))\nprint(\"AUC :\",metrics.roc_auc_score(y_test, y_pred4))\n\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred4)\nauc = metrics.roc_auc_score(y_test, y_pred4)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","458ad010":"from sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X_train)\n\n\nfrom sklearn.decomposition import PCA as sklearnPCA\n\nsklearn_pca = sklearnPCA(n_components=39)\nY_sklearn = sklearn_pca.fit_transform(X_std)\n\ncum_sum = sklearn_pca.explained_variance_ratio_.cumsum()\n\nsklearn_pca.explained_variance_ratio_[:10].sum()\n\ncum_sum = cum_sum*100\n\nfig, ax = plt.subplots(figsize=(8,8))\nplt.bar(range(39), cum_sum, label='Cumulative _Sum_of_Explained _Varaince', color = 'b',alpha=0.5)","f47c95ea":"#Cumulative explained variance\nfrom sklearn.decomposition import PCA\npca = PCA(39)\npca_full = pca.fit(X)\n\nplt.plot(np.cumsum(pca_full.explained_variance_ratio_))\nplt.xlabel('# of components')\nplt.ylabel('Cumulative explained variance')","998fdb27":"%%time\n# 26 Principal Components seems good \npca = PCA(n_components=26)\nX_transformed = pca.fit_transform(X_train)\n\nX_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split( \n    X_transformed, y_train, test_size=0.2, random_state=13)\n\nlr5=LogisticRegression(C=0.4, random_state=4 ,penalty='l1', class_weight={0:1,1:1})\nlr5.fit(X_train_pca, y_train_pca)\n\n# predict \ny_pred =lr5.predict(X_test_pca)\n\ncf_matrix = confusion_matrix(y_test_pca, y_pred)\nsns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test_pca, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test_pca, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test_pca, y_pred))\nprint(\"F1 score:\",metrics.f1_score(y_test_pca, y_pred))\nprint(\"AUC :\",metrics.roc_auc_score(y_test_pca, y_pred))\n\nfpr, tpr, _ = metrics.roc_curve(y_test_pca,  y_pred)\nauc = metrics.roc_auc_score(y_test_pca, y_pred)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","6a0ab402":"data1 = pd.DataFrame(X,y)\n\nclf = ExtraTreesClassifier(n_estimators=250,\n                              random_state=2)\n\nclf.fit(X_train, y_train)\n\n\n# Plot feature importance\nfeature_importance = clf.feature_importances_\n# make importances relative to max importance\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.figure(figsize=(10,10))\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, data1.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()","2ec19fc4":"X_train_ef = X_train[['race_Asian-Pac-Islander','education_Assoc-voc', 'workclass_Govt_employees','occupation_Craft-repair',\n       'education_Doctorate','workclass_Self_employed', 'occupation_Adm-clerical','occupation_Sales', 'education_Prof-school',\n       'workclass_Private','race_White', 'occupation_Prof-specialty', 'relationship_Unmarried','marital-status_Separated', 'education_elementary_school',\n       'education_Masters', 'relationship_Wife', 'education_Bachelors','capital-loss', 'occupation_Exec-managerial', 'gender_Female',\n       'relationship_Not-in-family', 'education_HS-grad','relationship_Own-child', 'capital-gain', 'relationship_Husband',\n       'marital-status_Married', 'hours-per-week', 'age']]\n                      \nX_test_ef = X_test[['race_Asian-Pac-Islander','education_Assoc-voc', 'workclass_Govt_employees',\n       'occupation_Craft-repair', 'education_Doctorate','workclass_Self_employed', 'occupation_Adm-clerical',\n       'occupation_Sales', 'education_Prof-school', 'workclass_Private','race_White', 'occupation_Prof-specialty', 'relationship_Unmarried',\n       'marital-status_Separated', 'education_elementary_school','education_Masters', 'relationship_Wife', 'education_Bachelors',\n       'capital-loss', 'occupation_Exec-managerial', 'gender_Female','relationship_Not-in-family', 'education_HS-grad',\n       'relationship_Own-child', 'capital-gain', 'relationship_Husband','marital-status_Married', 'hours-per-week', 'age']]","70d8d204":"%%time\nlr5=LogisticRegression(C=0.4, random_state=4 ,penalty='l1', class_weight={0:1,1:1})\nlr5.fit(X_train_ef, y_train)\n\n# predict \ny_pred =lr5.predict(X_test_ef)\n\ncf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\nprint(\"F1 score:\",metrics.f1_score(y_test, y_pred))\nprint(\"AUC :\",metrics.roc_auc_score(y_test, y_pred))\n\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred)\nauc = metrics.roc_auc_score(y_test, y_pred)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","d688bb05":"class GridSearch(object):\n    \n    def __init__(self,X_train,y_train,model,hyperparameters):\n        \n        self.X_train = X_train\n        self.y_train = y_train\n        self.model = model\n        self.hyperparameters = hyperparameters\n        \n    def GridSearch(self):\n        # Create randomized search 10-fold cross validation and 100 iterations\n        cv = 10\n        clf = GridSearchCV(self.model,\n                                 self.hyperparameters,\n                                 cv=cv,\n                                 verbose=0,\n                                 n_jobs=-1\n                                 )\n        # Fit randomized search\n        best_model = clf.fit(self.X_train, self.y_train)\n        message = (best_model.best_score_, best_model.best_params_)\n        print(\"Best: %f using %s\" % (message))\n\n        return best_model,best_model.best_params_\n    \n    def Best_Model_Predict(self,X_test):\n        \n        best_model,_ = self.GridSearch()\n        pred = best_model.predict(X_test)\n        return pred","d51b42b2":"from scipy.stats import uniform\n\nclass RandomSearch(object):\n    \n    def __init__(self,X_train,y_train,model,hyperparameters):\n        \n        self.X_train = X_train\n        self.y_train = y_train\n        self.model = model\n        self.hyperparameters = hyperparameters\n        \n    def RandomSearch(self):\n        # Create randomized search 10-fold cross validation and 100 iterations\n        cv = 10\n        clf = RandomizedSearchCV(self.model,\n                                 self.hyperparameters,\n                                 random_state=1,\n                                 n_iter=100,\n                                 cv=cv,\n                                 verbose=0,\n                                 n_jobs=-1\n                                 )\n        # Fit randomized search\n        best_model = clf.fit(self.X_train, self.y_train)\n        message = (best_model.best_score_, best_model.best_params_)\n        print(\"Best: %f using %s\" % (message))\n\n        return best_model,best_model.best_params_\n    \n    def Best_Model_Predict(self,X_test):\n        \n        best_model,_ = self.RandomSearch()\n        pred = best_model.predict(X_test)\n        return pred","9c03224c":"%%time\nk_range = list(range(2,15))\nd_metric = ['euclidean','minkowski']\n\nparam_grid = dict(n_neighbors = k_range, metric =d_metric)\n\nknn = KNeighborsClassifier()\n\nKNN_GridSearch = GridSearch(X_train_f, y_train, knn ,param_grid)\ny_pred = KNN_GridSearch.Best_Model_Predict(X_test_f)","3ac95fa9":"cf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\nprint(\"F1 score:\",metrics.f1_score(y_test, y_pred))\nprint(\"AUC :\",metrics.roc_auc_score(y_test, y_pred))\n\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred)\nauc = metrics.roc_auc_score(y_test, y_pred)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","c3100251":"%%time\nparam_grid = [{'gamma': [ 0.1, 1, 10],'C': [ 0.10, 10, 100]}]\n\nsvm = SVC()\n\nsvm_GridSearch = GridSearch(X_train_f, y_train, svm,param_grid )\ny_pred = svm_GridSearch.Best_Model_Predict(X_test_f)\n","2775194a":"cf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\nprint(\"F1 score:\",metrics.f1_score(y_test, y_pred))\nprint(\"AUC :\",metrics.roc_auc_score(y_test, y_pred))\n\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred)\nauc = metrics.roc_auc_score(y_test, y_pred)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","400881cc":"%%time\nparam_grid = [{'n_components': [1,2,3,4]}]\n\nlda = LinearDiscriminantAnalysis()\n\nlda_GridSearch = GridSearch(X_train, y_train, lda , param_grid )\ny_pred = lda_GridSearch.Best_Model_Predict(X_test)","8014b9d4":"cf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\nprint(\"F1 score:\",metrics.f1_score(y_test, y_pred))\nprint(\"AUC :\",metrics.roc_auc_score(y_test, y_pred))\n\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred)\nauc = metrics.roc_auc_score(y_test, y_pred)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","66c3f7b3":"%%time\nfrom scipy.stats import randint\nmax_depth_value = [4, 5,6,7,8,9,10,11,12,13]\nmax_features_value =  randint(1, 7)\nmin_samples_leaf_value = randint(1, 4)\ncriterion_value = [\"gini\", \"entropy\"]\n\nparam_grid = dict(max_depth = max_depth_value,\n                  max_features = max_features_value,\n                  min_samples_leaf = min_samples_leaf_value,\n                  criterion = criterion_value)\n\nCART = DecisionTreeClassifier(random_state=1)\n\nCART_RandSearch = RandomSearch(X_train_f, y_train, CART, param_grid)\nPrediction_CART = CART_RandSearch.Best_Model_Predict(X_test_f)\n","fc47d638":"# Visualize Decision Tree\nfrom sklearn import tree\nfrom sklearn.tree import export_graphviz\nfrom IPython.display import Image \n\nfeature_names = [i for i in X_train_f.columns]\n\ny_train_str = y_train.astype('str')\ny_train_str[y_train_str == 1] = \"1\"\ny_train_str[y_train_str == 0] =\"0\"\ny_train_str = y_train_str.values\n\nmodel_1 = tree.DecisionTreeClassifier(criterion='gini', max_depth= 13, max_features= 2, min_samples_leaf= 2) \nmodel_1.fit(X_train_f, y_train)\n\nfrom sklearn.externals.six import StringIO\n# Let's give dot_data some space so it will not feel nervous any more\ndot_data = StringIO()\ntree.export_graphviz(model_1, out_file=dot_data,filled=True)\nimport pydotplus\n\ngraph = pydotplus.graphviz.graph_from_dot_data(dot_data.getvalue())\n# make sure you have graphviz installed and set in path\nImage(graph.create_png())","ddd2151d":"cf_matrix = confusion_matrix(y_test, Prediction_CART)\nsns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, Prediction_CART))\nprint(\"Precision:\",metrics.precision_score(y_test, Prediction_CART))\nprint(\"Recall:\",metrics.recall_score(y_test, Prediction_CART))\nprint(\"F1 score:\",metrics.f1_score(y_test, Prediction_CART))\nprint(\"AUC :\",metrics.roc_auc_score(y_test, Prediction_CART))\n\nfpr, tpr, _ = metrics.roc_curve(y_test,  Prediction_CART)\nauc = metrics.roc_auc_score(y_test, Prediction_CART)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","fb4e7408":"%%time\nparam_grid = [\n{'n_estimators': [10, 25,30], 'max_features': ['auto', 'sqrt', 'log2', None], \n 'max_depth': [10, 20, None], 'bootstrap': [True, False]}\n]\n\nrf = RandomForestClassifier()\n\nrf_GridSearch = GridSearch(X_train, y_train, rf ,param_grid )\ny_pred = rf_GridSearch.Best_Model_Predict(X_test)","4a1d9b04":"cf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\nprint(\"F1 score:\",metrics.f1_score(y_test, y_pred))\nprint(\"AUC :\",metrics.roc_auc_score(y_test, y_pred))\n\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred)\nauc = metrics.roc_auc_score(y_test, y_pred)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","9d6a1d8f":"%%time\nparam_grid = [\n{'n_estimators': [10, 25,30], 'max_features': ['auto', 'sqrt', 'log2', None], \n 'max_depth': [10, 20, None], 'bootstrap': [True, False]}\n]\n\nrf = RandomForestClassifier()\n\nrf_GridSearch = GridSearch(X_train_ef, y_train, rf ,param_grid )\ny_pred = rf_GridSearch.Best_Model_Predict(X_test_ef)","730973ab":"cf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\nprint(\"F1 score:\",metrics.f1_score(y_test, y_pred))\nprint(\"AUC :\",metrics.roc_auc_score(y_test, y_pred))\n\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred)\nauc = metrics.roc_auc_score(y_test, y_pred)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","2b4b2a95":"%%time\nlearning_rate_ = [.01,.05,.1,.5,1]\nn_estimators_ = [50,100,150,200,250,300]\n\nparam_grid = dict(learning_rate=learning_rate_, n_estimators=n_estimators_)\n\nGB = GradientBoostingClassifier()\n\nGB_GridSearch = RandomSearch(X_train, y_train, GB, param_grid)\n","d4db0b12":"Prediction_GB = GB_GridSearch.Best_Model_Predict(X_test)","786a84a6":"from xgboost import XGBClassifier\n%%time\nlearning_rate_ = [.01,.05,.1,.5,1]\nn_estimators_ = [50,100,150,200,250,300]\n\nparam_grid = dict(learning_rate=learning_rate_, n_estimators=n_estimators_,n_jobs=-1)\n\nGB =  XGBClassifier()\n\nGB_GridSearch = RandomSearch(X_train, y_train, GB, param_grid)\nPrediction_GB = GB_GridSearch.Best_Model_Predict(X_test)","04807728":"cf_matrix = confusion_matrix(y_test, Prediction_GB)\nsns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, Prediction_GB))\nprint(\"Precision:\",metrics.precision_score(y_test, Prediction_GB))\nprint(\"Recall:\",metrics.recall_score(y_test, Prediction_GB))\nprint(\"F1 score:\",metrics.f1_score(y_test, Prediction_GB))\nprint(\"AUC :\",metrics.roc_auc_score(y_test, Prediction_GB))\n\n\nfpr, tpr, _ = metrics.roc_curve(y_test,  Prediction_GB)\nauc = metrics.roc_auc_score(y_test, Prediction_GB)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()\n#.712","73292bdf":"%%time\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nmodel = Sequential()\n\n# Adding the input layer and the first hidden layer\nmodel.add(Dense(output_dim = 16, activation = 'relu', input_dim = 45))\n# Adding the second hidden layer\nmodel.add(Dense(output_dim = 8,  activation = 'relu'))\n\n# Adding the output layer\nmodel.add(Dense(output_dim = 1, activation = 'sigmoid'))\n\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\nmodel.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)\n\ny_pred1 = model.predict(X_test)\ny_pred1 = (y_pred1 > 0.5)\n\n","8a7c52a4":"cf_matrix = confusion_matrix(y_test, y_pred1)\nsns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred1))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred1))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred1))\nprint(\"F1 score:\",metrics.f1_score(y_test, y_pred1))\nprint(\"AUC :\",metrics.roc_auc_score(y_test, y_pred1))\n\n\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred1)\nauc = metrics.roc_auc_score(y_test, y_pred1)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","94b99ed5":"\nm = { \n\"GradientBoosting\" : {\n    \"Accuracy\": 0.8280,\n    \"Precision\": 0.84150,\n    \"Recall\": 0.8063,\n    \"F1 score\": 0.82352,\n    \"AUC\" : 0.82795,\n    \"RunTime(sec)\" : 481\n    },\n\"Random Forest\" : {\n    \"Accuracy\": 0.82064,\n    \"Precision\": 0.86139,\n    \"Recall\": 0.76217,\n    \"F1 score\": 0.80875,\n    \"AUC\" : 0.8203,\n    \"RunTime(sec)\" : 59.4\n    },\n\"LDA\" : {\n    \"Accuracy\": 0.8001,\n    \"Precision\": 0.8307,\n    \"Recall\": 0.7512,\n    \"F1 score\": 0.7890,\n    \"AUC\" : 0.7998,\n    \"RunTime(sec)\" : 1.2\n    },\n\"Decision Tree\" : {\n    \"Accuracy\": 0.7992,\n    \"Precision\": 0.8607,\n    \"Recall\": 0.7117,\n    \"F1 score\": 0.7791,\n    \"AUC\" : 0.7988,\n    \"RunTime(sec)\" : 2.1\n    },\n\"SVM\" : {\n    \"Accuracy\": 0.8066,\n    \"Precision\": 0.8592,\n    \"Recall\": 0.7312,\n    \"F1 score\": 0.7900,\n    \"AUC\" : 0.8063,\n    \"RunTime(sec)\" : 500.4\n    },\n\"KNN\" : {\n    \"Accuracy\": 0.7924,\n    \"Precision\": 0.8212,\n    \"Recall\": 0.74498,\n    \"F1 score\": 0.7812,\n    \"AUC\" : 0.7921,\n    \"RunTime(sec)\" : 144.1\n    },\n\"LR(ET)\" : {\n    \"Accuracy\": 0.8069,\n    \"Precision\": 0.8156,\n    \"Recall\": 0.7908,\n    \"F1 score\": 0.8030,\n    \"AUC\" : 0.80687,\n    \"RunTime(sec)\" : .688\n    },\n\"LR(PCA)\" : {\n    \"Accuracy\": 0.7866,\n    \"Precision\": 0.8162,\n    \"Recall\": 0.7492,\n    \"F1 score\": 0.7813,\n    \"AUC\" : 0.7872,\n    \"RunTime(sec)\" : .433\n    },\n\"LR(REF)\" : {\n    \"Accuracy\": 0.7984,\n    \"Precision\": 0.8409,\n    \"Recall\": 0.7335,\n    \"F1 score\":0.7835,\n    \"AUC\" : 0.7980,\n    \"RunTime(sec)\" : 1.7\n    },\n\"LR(GridSearch)\" : {\n    \"Accuracy\": 0.81636,\n    \"Precision\": 0.8270,\n    \"Recall\": 0.7977,\n    \"F1 score\": 0.8121,\n    \"AUC\" : 0.8162,\n    \"RunTime(sec)\" : 23.8\n    },\n\"LR(RandomSearch)\":{\n    \"Accuracy\": 0.8120,\n    \"Precision\": 0.8302,\n    \"Recall\": 0.7822,\n    \"F1 score\": 0.8055,\n    \"AUC\" : 0.8119,\n    \"RunTime(sec)\" : 4.9\n    },\n\"Neural Network\" : {\n    \"Accuracy\": 0.8157,\n    \"Precision\": 0.8466,\n    \"Recall\": 0.76905,\n    \"F1 score\": 0.7707,\n    \"AUC\" : 0.8155,\n    \"RunTime(sec)\" : 120\n    },\n}\ndf = pd.DataFrame(m)\ndf.T\n","d6f0b81f":"## 1.1  Data description\nThis data was extracted from the 1994 Census bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics).  The prediction task is to determine whether a person makes over $50K a year.","75bbd06b":"####  How does a tree decide where to split?\n\nDecision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.\n\nLet\u2019s look at the two most commonly used algorithms in decision tree:\n\n#### Gini:\n\nGini  says, if we select two items from a population at random then they must be of same class and probability for this is 1 if population is pure.\nSteps to Calculate Gini for a split\n\n    Calculate Gini for sub-nodes, using formula sum of square of probability for success and failure (p^2+q^2).\n    Calculate Gini for split using weighted Gini score of each node of that split\n\nExample: \u2013 Referring to example used above, where we want to segregate the students based on target variable ( playing cricket or not ). \n\n![dt.PNG](attachment:dt.PNG)\nSplit on Gender:\n\n    Calculate, Gini for sub-node Female = (0.2)*(0.2)+(0.8)*(0.8)=0.68\n    Gini for sub-node Male = (0.65)*(0.65)+(0.35)*(0.35)=0.55\n    Calculate weighted Gini for Split Gender = (10\/30)*0.68+(20\/30)*0.55 = 0.59\n\nSimilar for Split on Class:\n\n    Gini for sub-node Class IX = (0.43)*(0.43)+(0.57)*(0.57)=0.51\n    Gini for sub-node Class X = (0.56)*(0.56)+(0.44)*(0.44)=0.51\n    Calculate weighted Gini for Split Class = (14\/30)*0.51+(16\/30)*0.51 = 0.51\n\nAbove, you can see that Gini score for Split on Gender is higher than Split on Class, hence, the node split will take place on Gender.\n\n#### Information Gain:\n\nInformation gain can be understood as decrease in \u201cuncertainty\u201d of the result.\n \nInformation theory is a measure to define this degree of disorganization in a system known as Entropy. If the sample is completely homogeneous, then the entropy is zero and if the sample is an equally divided (50% \u2013 50%), it has entropy of one.\n\nEntropy can be calculated using formula:-\n![et.PNG](attachment:et.PNG)\n\nHere p and q is probability of success and failure respectively in that node. Entropy is also used with categorical target variable. It chooses the split which has lowest entropy compared to parent node and other splits. The lesser the entropy, the better it is.\n\nSteps to calculate entropy for a split:\n\n    Calculate entropy of parent node\n    Calculate entropy of each individual node of split and calculate weighted average of all sub-nodes available in split.\n\nExample: Let\u2019s use this method to identify best split for student example.\n\n    Entropy for parent node = -(15\/30) log2 (15\/30) \u2013 (15\/30) log2 (15\/30) = 1. Here 1 shows that it is a impure node.\n    Entropy for Female node = -(2\/10) log2 (2\/10) \u2013 (8\/10) log2 (8\/10) = 0.72 and for male node,  -(13\/20) log2 (13\/20) \u2013 (7\/20) log2 (7\/20) = 0.93\n    Entropy for split Gender = Weighted entropy of sub-nodes = (10\/30)*0.72 + (20\/30)*0.93 = 0.86\n    Entropy for Class IX node, -(6\/14) log2 (6\/14) \u2013 (8\/14) log2 (8\/14) = 0.99 and for Class X node,  -(9\/16) log2 (9\/16) \u2013 (7\/16) log2 (7\/16) = 0.99.\n    Entropy for split Class =  (14\/30)*0.99 + (16\/30)*0.99 = 0.99\n\nAbove, you can see that entropy for Split on Gender is the lowest among all, so the tree will split on Gender. We can derive information gain from entropy as 1- Entropy.","9d9a39fb":"# 6.3 SVM (Support Vector Machine)","f1b9bf5e":"**Introduction to Decision Trees**\n\n* A decision tree is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.\n\n**How does Decision Tree works ?**\n\n* Decision tree is a type of supervised learning algorithm (having a pre-defined target variable) that is mostly used in classification problems. It works for both categorical and continuous input and output variables. In this technique, we split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter \/ differentiator in input variables.\n\n![asdwq.png](attachment:asdwq.png)\n\nThe core algorithm for building decision trees called ID3 by J. R. Quinlan which employs a top-down, greedy search through the space of possible branches with no backtracking. ID3 uses Entropy and Information Gain to construct a decision tree. \n\n**The popular attribute selection measures:**\n\n    Information gain\n    Gini index\n\n\n**Advantages of CART**\n\n    Simple to understand, interpret, visualize.\n    Decision trees implicitly perform variable screening or feature selection.\n    Can handle both numerical and categorical data. Can also handle multi-output problems.\n    Decision trees require relatively little effort from users for data preparation.\n    Nonlinear relationships between parameters do not affect tree performance.\n    \n**Disadvantages of CART**\n\n    Decision-tree learners can create over-complex trees that do not generalize the data well.This is called overfitting.\n    Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This is called variance, which needs to be lowered by methods like bagging and boosting.","485af834":"#### Confusion Matrix","3a4b42c1":"# 7. Neural Network\n\n![nn_diagram_1.png](attachment:nn_diagram_1.png)\n\n**The Sequential model is a linear stack of layers.**\n\n**Specifying the input shape**\n\n    The model needs to know what input shape it should expect. For this reason, the first layer in a Sequential model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape.\n    \n**Compilation**\n\nBefore training a model, we need to configure the learning process, which is done via the compile method. It receives three arguments:\n\n    An optimizer. \n    This could be the string identifier of an existing optimizer (such as rmsprop , adam or adagrad).\n    \n    A loss function. \n    This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as categorical_crossentropy or mse), or it can be an objective function. \n    \n    A list of metrics.\n    For any classification problem you will want to set this to metrics=['accuracy']. A metric could be the string identifier of an existing metric or a custom metric function.\n\n**Training**\n\n    Keras models are trained on Numpy arrays of input data and labels. For training a model,we will use the fit function.","839ff404":"# 6.6.2.1 GradientBoosting\n\nA special case of boosting where errors are minimized by gradient descent algorithm e.g. the strategy consulting firms leverage by using case interviews to weed out less qualified candidates.","7dbc8a7f":"## 6.1 Logistic Regression","62091a0e":"**Kernel**\n\nkernel parameters selects the type of hyperplane used to separate the data. Using \u2018linear\u2019 will use a linear hyperplane (a line in the case of 2D data). \u2018rbf\u2019 and \u2018poly\u2019 uses a non linear hyper-plane.\n\n**gamma**\n\ngamma is a parameter for non linear hyperplanes. The higher the gamma value it tries to exactly fit the training data set.  Increasing gamma leads to overfitting as the classifier tries to perfectly fit the training data.\n\n**C**\n\nC is the penalty parameter of the error term. It controls the trade off between smooth decision boundary and classifying the training points correctly.\nA smaller C value leads to a wider street but more margin violations","2d3ac656":"**The results obtained above can be used as a standard point of reference for other comparative studies done in the field of predicting values from census data. This comparative study can further be used as a basis for improving the present classifiers and techniques resulting in making better technologies for accurately predicting income level of an individual.**","09d30e94":"## 6.6.1 Bagging","6c2973a7":"## 5.3 Take a look to Income class distribution","e0882b23":"# 3. Data Cleaning","12bfd509":"## Visualize Decision Tree","82c84982":"### Updating the columns","565026d0":"## 5.6 Models Scores","14d40c73":"### Grid Search","7c575027":"### Grid Search vs Random search\n\n**Grid search** is a traditional way to perform hyperparameter optimization. It works by searching exhaustively through a specified subset of hyperparameters.\n\n**Random search** differs from grid search mainly in that it searches the specified subset of hyperparameters randomly instead of exhaustively. The major benefit being decreased processing time.\n\n* There is a tradeoff to decreased processing time, however. We aren\u2019t guaranteed to find the optimal combination of hyperparameters.\n\n![HPO1.png](attachment:HPO1.png)","12ab4c0c":"## 6.6.2 Boosting","e2264e3e":"Fixing the common nan values\n\n    Nan values were as ? in data. Hence we fix this with most frequent element(mode) in the entire dataset. It generalizes well, as we will see with the accuracy of our classifiers","97bff705":"**Logistic Regression is used when the dependent variable(target) is categorical.**\n\n**Model**\n\nOutput = 0 or 1\n\nHypothesis => Z = WX + B\n\nh\u0398(x) = sigmoid (Z)\n\n**Sigmoid Function**\n![logistic.png](attachment:logistic.png)\n\nIf \u2018Z\u2019 goes to infinity, Y(predicted) will become 1 and if \u2018Z\u2019 goes to negative infinity, Y(predicted) will become 0.\n\n**Cost Function**\n![sfd.png](attachment:sfd.png)","3efee05b":"# 5. Pipeline","69b267f5":"### Random search","cd8d7135":"* The Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain. \n* It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.\n* As the name suggests, this method eliminates worst performing features on a particular model one after the other until the best subset of features are known.\n![vnb.png](attachment:vnb.png)","f3d7f12e":"# 6.6.1.1 Random Forest\n","43051f76":"**kNN is non-parametric, instance based, lazy algorithm and used in the supervised setting.**\n\n* Non-parametric :\n      \n      It means that algorithm has no pre assumptions about the functional form of the model, to avoid mismodeling .\n\n* Instance based :\n      \n      It means that our algorithm does not explicitly learn a model.\n      Instead, it memorize the training instances which are subsequently used as \u201cknowledge\u201d for the prediction.\n\n* Lazy algorithm :\n\n      It means that it does not use the training data for the Generalization i.e. these algorithm has no explicit training phase or it is minimal. Training is very fast.\n      \n**kNN Algorithm for Classification**\n\nTraining element {xi, yi} , Testing point(x)\n\n    Compute the Distance D(x,xi) to every training element xi.\n    Select k closest instance xi1,xi2,\u2026\u2026.., xik and their labels yi1, yi2 \u2026, yik.\n    Output the class y* which is most frequent in yi1,yi2 \u2026\u2026yik.\n    \n\n\n**Significant of \u201ck\u201d**\n\n    Value of k has strong effect on kNN performance.\n    k act as controller to decide the shape of decision boundary.\n    Large value of k has following properties:\n\n 1. Smoother decision boundary\n 2. It provide more voters for prediction, it implies less affect from outliers.\n 3. As a result has Lower Variance and High Bias.\n \n ![gh.jpeg](attachment:gh.jpeg)\n \n**How to Select k**\n\n    The simplest solution is Cross Validation.\n    Best method is to try many k values and use Cross-Validation to see which k value is giving the best result.\n","42b798b2":"**C : Inverse of regularization strength**\n\nwe use paramter C as our regularization parameter. Parameter C = 1\/\u03bb.\n\nLambda (\u03bb) controls the trade-off between allowing the model to increase it's complexity as much as it wants with trying to keep it simple. For example, if \u03bb is very low or 0, the model will have enough power to increase it's complexity (overfit) by assigning big values to the weights for each parameter. If, in the other hand, we increase the value of \u03bb, the model will tend to underfit, as the model will become too simple.\n\n* Parameter C will work the other way around. For small values of C, we increase the regularization strength which will create simple models which underfit the data. For big values of C, we low the power of regularization which imples the model is allowed to increase it's complexity, and therefore, overfit the data.\n\n**L2 Regularization or Ridge Regularization**\n\n* Ridge regression adds \u201csquared magnitude\u201d of coefficient as penalty term to the loss function. Here the highlighted part represents L2 regularization element.\n![ewr.png](attachment:ewr.png)\n\n**L1 Regularization or Lasso**\n\n* Lasso Regression (Least Absolute Shrinkage and Selection Operator) adds \u201cabsolute value of magnitude\u201d of coefficient as penalty term to the loss function.\n![tre.png](attachment:tre.png)\n\nThe key difference between these techniques is that Lasso shrinks the less important feature\u2019s coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features.\n\n**Class weight**\n\n* If we have highly imbalanced classes and have no addressed it during preprocessing, we have the option of using the class_weight parameter to weight the classes to make certain we have a balanced mix of each class. Class weights will be given by n_samples \/ (n_classes * np.bincount(y))","803a9b38":"# 6.2 KNN","ce5a9de0":"## Recursive Feature Elimination","361992fa":"## 5.4 Resampling\n\nThe main idea of sampling classes is to either increasing the samples of the minority class or decreasing the samples of the majority class. This is done in order to obtain a fair balance in the number of instances for both the classes.\n\nThere can be two main types of sampling:\n\n    You can add copies of instances from the minority class which is called over-sampling (or more formally sampling with replacement), or\n    You can delete instances from the majority class, which is called under-sampling.\n\n\nA widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and \/ or adding more examples from the minority class (over-sampling).\n\nDespite the advantage of balancing classes, these techniques also have their weaknesses (there is no free lunch). The simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfitting. In under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information.\n\n**Under-sampling**\n\nAdvantages of this approach:\n\n    It can help improve the runtime of the model and solve the memory problems by reducing the number of training data samples when the training data set is enormous.\n![resampling.png](attachment:resampling.png)","a2e7a7f6":"### Evaluation of logistic regression(Random Search)","f52d8f73":"# 1. Introduction\nA census is the procedure of systematically acquiring and recording information about the members of a given population.\nThe census is a special, wide-range activity, which takes place once a decade in the entire country. The purpose is to gather information about the general population, in order to present a full and reliable picture of the population in the country - its housing conditions and demographic, social and economic characteristics. The information collected includes data on age, gender, country of origin, marital status, housing conditions, marriage, education, employment, etc.","347d2f47":"# 4. Feature Engineering","38afd798":"## Hyperparameter Tuning \nA hyperparameter is a parameter whose value is set before the learning process begins.\nTuning Strategies\n\nWe will explore two different methods for optimizing hyperparameters:\n\n    Grid Search\n    Random Search","9a2a2ab3":"### Grid Search","58da5919":"## Load Data","985df8c1":"An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n\n    True Positive Rate\n    False Positive Rate\n![gfh.png](attachment:gfh.png)","40e1d3b0":"Lets look the data it again :","00111301":"## 5.2 Split the dataset\u00b6","f96b35ac":"**What is a Support Vector Machine**\nA Support Vector Machine is a supervised machine learning algorithm which can be used for both classification and regression problems. It follows a technique called the kernel trick to transform the data and based on these transformations, it finds an optimal boundary between the possible outputs.\n\n**How does it work?**\nThe main idea is to identify the optimal separating hyperplane which maximizes the margin of the training data. \n\nThe goal of SVMs is to find the optimal hyperplane because it not only classifies the existing dataset but also helps predict the class of the unseen data. And the optimal hyperplane is the one which has the biggest margin.\n\n![Margin.png](attachment:Margin.png)","908bcac4":"**Set of hyperparameters:**\n\n*     n_estimators = number of trees in the foreset\n*     max_features = max number of features considered for splitting a node\n*     max_depth = max number of levels in each decision tree\n*     bootstrap = method for sampling data points (with or without replacement)\n\n\n**max_features:** \n\nThese are the maximum number of features Random Forest is allowed to try in individual tree. \n\n1)Auto : This will simply take all the features which make sense in every tree.Here we simply do not put any restrictions on the individual tree.\n\n2)sqrt : This option will take square root of the total number of features in individual run. For instance, if the total number of variables are 100, we can only take 10 of them in individual tree.\n\n3)log2:It is another option which takes log to the base 2 of the features input.\n\nIncreasing max_features generally improves the performance of the model as at each node now we have a higher number of options to be considered.But, for sure, you decrease the speed of algorithm by increasing the max_features. Hence, you need to strike the right balance and choose the optimal max_features.\n\n**n_estimators :** \n\nThis is the number of trees you want to build before taking the maximum voting or averages of predictions. Higher number of trees give you better performance but makes your code slower. You should choose as high value as your processor can handle because this makes your predictions stronger and more stable.\n\n**min_sample_leaf:** \n\nLeaf is the end node of a decision tree. A smaller leaf makes the model more prone to capturing noise in train data. Hence it is important to try different values to get good estimate.","1b5af58d":"# 6.5 Decision Tree","2266fd48":"## 1.2 Features Description\n**1. Categorical Attributes**\n * **workclass**:  Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n  -  Individual work category  \n * **education**: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n  -  Individual's highest education degree  \n * **marital-status**: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n  -  Individual marital status  \n * **occupation**:  Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n  -  Individual's occupation  \n * **relationship**:  Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n  -  Individual's relation in a family   \n * **race**:  White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n  -  Race of Individual   \n * **sex**:  Female, Male.\n * **native-country**:  United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n  -  Individual's native country   \n  \n**2. Continuous Attributes**\n * **age**: continuous.\n  -  Age of an individual  \n * **fnlwgt**: final weight, continuous. \n * The weights on the CPS files are controlled to independent estimates of the civilian noninstitutional population of the US.  These are prepared monthly for us by Population Division here at the Census Bureau.\n * **capital-gain**: continuous.\n * **capital-loss**: continuous.\n * **hours-per-week**: continuous.\n  -  Individual's working hour per week   ","5a45ed6b":"### Deleting the unuseful features and observations","125295fd":"## PCA analysis\nPrincipal Component Analysis (or PCA) uses linear algebra to transform the dataset into a compressed form.\n\nGenerally this is called a data reduction technique. A property of PCA is that you can choose the number of dimensions or principal component in the transformed result.\n\nIn this case we will use it to analyse the feature importanace","47a115f8":"**Precision:**\nPrecision is about being precise, i.e., how accurate your model is. In other words, you can say, when a model makes a prediction, how often it is correct.\nPrecision can be thought of as a measure of a classifier's exactness.\n![ret.png](attachment:ret.png)\n\n**Recall:**\nOut of all the positive classes, how much we predicted correctly. It should be high as possible.\nRecall can be thought of as a measure of a classifier's completeness.\n![ytu.png](attachment:ytu.png)\n\n**F1 score**\nIt is difficult to compare two models with low precision and high recall or vice versa. So to make them comparable, we use F-Score.\n\nF-score helps to measure Recall and Precision at the same time. It uses Harmonic Mean in place of Arithmetic Mean by punishing the extreme values more.\n![uyt.png](attachment:uyt.png)","4bd1b62b":"# 2. Import packages","dd1cde6e":"**Features sorted by their rank:**","f3ed078c":"### Implemention of Decision Tree","278a4c38":"# Feature Importance","3105dc4b":"# 6.6 Ensemble methods\n\n**What is an ensemble method?**\n\nEnsemble is a Machine Learning concept in which the idea is to train multiple models using the same learning algorithm. The ensembles take part in a bigger group of methods, called multiclassifiers, where a set of hundreds or thousands of learners with a common objective are fused together to solve the problem.\nWhen we try to predict the target variable using any machine learning technique, the main causes of difference in actual and predicted values are noise, variance, and bias. Ensemble helps to reduce these factors (except noise, which is irreducible error).\n\n**Techniques to perform ensemble decision trees:**\n\n**1. Bagging**\n\nBagging is used when the goal is to reduce the variance of a decision tree classifier. Here the objective is to create several subsets of data from training sample chosen randomly with replacement. Each collection of subset data is used to train their decision trees. As a result, we get an ensemble of different models. Average of all the predictions from different trees are used which is more robust than a single decision tree classifier.\n\nBagging Steps:\n\n    Suppose there are N observations and M features in training data set. A sample from training data set is taken randomly with replacement.\n    A subset of M features are selected randomly and whichever feature gives the best split is used to split the node iteratively.\n    The tree is grown to the largest.\n    Above steps are repeated n times and prediction is given based on the aggregation of predictions from n number of trees.\n\nAdvantages:\n\n    Reduces over-fitting of the model.\n    Handles higher dimensionality data very well.\n    Maintains accuracy for missing data.\n\nDisadvantages:\n\n    Since final prediction is based on the mean predictions from subset trees, it won\u2019t give precise values for the classification and regression model.\n    \n ![dfg.png](attachment:dfg.png)\n\n\n**2. Boosting**\n\nBoosting is used to create a collection of predictors. In this technique, learners are learned sequentially with early learners fitting simple models to the data and then analysing data for errors. Consecutive trees (random sample) are fit and at every step, the goal is to improve the accuracy from the prior tree. When an input is misclassified by a hypothesis, its weight is increased so that next hypothesis is more likely to classify it correctly. This process converts weak learners into better performing model.\n\nBoosting Steps:\n\n    Draw a random subset of training samples d1 without replacement from the training set D to train a weak learner C1\n    Draw second random training subset d2 without replacement from the training set and add 50 percent of the samples that were previously falsely classified\/misclassified to train a weak learner C2\n    Find the training samples d3 in the training set D on which C1 and C2 disagree to train a third weak learner C3\n    Combine all the weak learners via majority voting.\n\nAdvantages:\n\n    Supports different loss function (we have used \u2018binary:logistic\u2019 for this example).\n    Works well with interactions.\n\nDisadvantages:\n\n    Prone to over-fitting.\n    Requires careful tuning of different hyper-parameters.\n\n*     Bagging to decrease the model\u2019s variance;\n*     Boosting to decreasing the model\u2019s bias, and;\n*     Stacking to increasing the predictive force of the classifier.\n\n![boosting.png](attachment:boosting.png)","27ce04c8":"## 1.3 Objective of this project\nThe goal of this machine learning project is to predict whether a person makes over 50K a year or not given their demographic variation. This is a classification problem.","b3abedf6":"### ROC Curve","f1922013":"# 6.4 LDA\n\n* Linear Discriminant Analysis (LDA) is most commonly used as dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications. \n\n* The goal is to project a dataset onto a lower-dimensional space with good class-separability in order avoid overfitting (\u201ccurse of dimensionality\u201d) and also reduce computational costs.\n\nIn general, dimensionality reduction does not only help reducing computational costs for a given classification task, but it can also be helpful to avoid overfitting by minimizing the error in parameter estimation (\u201ccurse of dimensionality\u201d).\n\n![sdfg.jpg](attachment:sdfg.jpg)","5d3ce538":"## 5.5 Baseline models","a35c23e9":"# 6. Tuning Machine Learning Models","24fc9fe9":"Education\n\n    9th, 10th, 11th, 12th comes under HighSchool Grad but it has mentioned separately\n    Create Elementary object for 1st-4th, 5th-6th, 7th-8th\n\nMarital Status\n\n    \n    Married-civ-spouse,Married-spouse-absent,Married-AF-spouse comes under category Married\n    Divorced, separated again comes under category separated.\n\nWorkclass\n\n    Self-emp-not-inc, Self-emp-inc comes under category self employed\n    Local-gov,State-gov,Federal-gov comes under category goverment emloyees\n","b2b87632":"### Evaluation of logistic regression(Grid Search)\n**Confusion Matrix**","897c49a4":"### Random Search","204805ef":"# Summary\n\nI have choosen two performance criterion for models:\n1. Model Accuracy\n2. Run time\n\n#### 1. Model Accuracy\n\n**GradientBoosting** outperform all the models if we take accuracy under consideration.It has 82.7% accuracy. But it takes nearly 481 sec (8 min) to pick the hyperparameters, train the model and predict the output.\n\nThere is another way to reduce the runtime by a factor of 8 without losing much accuracy if we use Random Forest for modelling.\n**Random Forest** has accuracy 82% but it has nearly 1 min runtime.\n\n#### 2. Run time\n\nIn case if our focus is on runtime then **Logistic Regression** has runtime 4.9 sec with accuracy 81.1%.\n\nBut if we want lowest runtime, we have to diminishing the accuracy. In that case **LDA** is the best choice with nearly 80% accuracy and 1.2 sec of runtime.","8709dd34":"### Implementating model on imbalanced data"}}