{"cell_type":{"df5e5a5c":"code","a20118d0":"code","07350b11":"code","d6420259":"code","5bb39c52":"code","efb4047c":"code","52d2e858":"code","4921ff9e":"code","e9da263e":"code","ac3119a0":"code","7444ab92":"code","1456ae07":"code","f82c2dea":"code","095a79d8":"code","dffa33e8":"code","b11edb7d":"markdown","4092b302":"markdown","a0d7c689":"markdown","0a9493e9":"markdown","cd47bb9a":"markdown","0ea935a3":"markdown","2a215e20":"markdown","6edca51d":"markdown"},"source":{"df5e5a5c":"!pip install facenet_pytorch\n!pip install pretrainedmodels","a20118d0":"import os\nimport pretrainedmodels\nimport pretrainedmodels.utils as utils\nfrom shutil import copyfile\nos.environ['TORCH_HOME'] = '\/kaggle\/working\/pretrained-model-weights-pytorch'\n\ndef copy_weights(model_name):\n    found = False\n    for dirname, _, filenames in os.walk('\/kaggle\/input\/'):\n        for filename in filenames:\n            full_path = os.path.join(dirname, filename)\n            if filename.startswith(model_name):\n                found = True\n                break\n        if found:\n            break\n            \n    base_dir = \"\/kaggle\/working\/pretrained-model-weights-pytorch\/checkpoints\"\n    os.makedirs(base_dir, exist_ok=True)\n    filename = os.path.basename(full_path)\n    copyfile(full_path, os.path.join(base_dir, filename))\n    \ncopy_weights('xception')","07350b11":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pathlib import Path\nfrom facenet_pytorch import MTCNN\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport cv2\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom time import time\nimport shutil","d6420259":"list_files = [str(x) for x in Path('\/kaggle\/input\/deepfake-detection-challenge\/test_videos').glob('*.mp4')] + \\\n             [str(x) for x in Path('\/kaggle\/input\/deepfake-detection-challenge\/train_sample_videos').glob('*.mp4')]\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmtcnn = MTCNN(keep_all=False, select_largest=False, device=device, min_face_size = 60)","5bb39c52":"def save_frame(file, folder):\n    reader = cv2.VideoCapture(file)\n    _, image = reader.read()\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    pilimg = Image.fromarray(image)\n    boxes, probs = mtcnn.detect(pilimg)\n    if boxes is None:\n        return\n    if len(boxes) > 0:\n        try:\n            best_index = probs.argmax()\n            box = [int(x) for x in boxes[best_index].tolist()]\n            face_image = image[box[1]:box[3], box[0]:box[2]]\n            pilface = Image.fromarray(face_image)\n            imgfile = f'{Path(file).stem}.jpg'\n            pilface.save(Path(folder)\/imgfile)\n        except:\n            return","efb4047c":"folder = '\/kaggle\/working\/faces'\nPath(folder).mkdir(parents=True, exist_ok=True)\nfor file in tqdm(list_files):\n    save_frame(file, folder)\n\nface_files = [str(x) for x in Path(folder).glob('*')]","52d2e858":"%%time\nmodel = pretrainedmodels.__dict__['xception'](num_classes=1000, pretrained='imagenet')\nmodel.eval();\nnum_ftrs = model.last_linear.in_features\nmodel.last_linear = nn.Linear(num_ftrs, 2)\nmodel = model.to(device)\n\ns = torch.load('\/kaggle\/input\/deepfakemodels\/london.pt', map_location=device)\nmodel.load_state_dict(s)\nmodel.eval();\n\ntf_img = utils.TransformImage(model)","4921ff9e":"def embeddings(model, input):\n    f = model.features(input)\n    x = nn.ReLU(inplace=True)(f)\n    x = F.adaptive_avg_pool2d(x, (1, 1))\n    x = x.view(x.size(0), -1)\n    return x","e9da263e":"list_embs = []\nfor face in tqdm(face_files):\n    t = tf_img(Image.open(face)).to(device)\n    e = embeddings(model, t.unsqueeze(0)).squeeze().cpu().detach().numpy().tolist()\n    list_embs.append(e)","ac3119a0":"df = pd.DataFrame({'faces': face_files, 'embeddings': list_embs})\ndf['videos'] = df['faces'].apply(lambda x: f'{Path(x).stem}.mp4')\ndf = df[['videos', 'faces', 'embeddings']]\ndf.head()","7444ab92":"from annoy import AnnoyIndex\n\nf = len(df['embeddings'][0])\nt = AnnoyIndex(f, metric='euclidean')\nntree = 50\n\nfor i, vector in enumerate(df['embeddings']):\n    t.add_item(i, vector)\n_  = t.build(ntree)","1456ae07":"def get_similar_images_annoy(img_index):\n    t0 = time()\n    v, f  = df.iloc[img_index, [0, 1]]\n    similar_img_ids = t.get_nns_by_item(img_index, 8)\n    return v, f, df.iloc[similar_img_ids]","f82c2dea":"sample_idx = np.random.choice(len(df))  # 166, # 302\nv, f, s = get_similar_images_annoy(sample_idx)","095a79d8":"fig = plt.figure(figsize=(15, 7))\ngs = fig.add_gridspec(2, 6)\nax1 = fig.add_subplot(gs[0:2, 0:2])\nax2 = fig.add_subplot(gs[0, 2])\nax3 = fig.add_subplot(gs[0, 3])\nax4 = fig.add_subplot(gs[0, 4])\nax5 = fig.add_subplot(gs[0, 5])\nax6 = fig.add_subplot(gs[1, 2])\nax7 = fig.add_subplot(gs[1, 3])\nax8 = fig.add_subplot(gs[1, 4])\nax9 = fig.add_subplot(gs[1, 5])\naxx = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9]\nlist_plot = [face_files[sample_idx]] + s['faces'].values.tolist()\nfor i, ax in enumerate(axx):\n    ax.imshow(plt.imread(list_plot[i]))\n    ax.xaxis.set_visible(False)\n    ax.yaxis.set_visible(False)","dffa33e8":"shutil.rmtree(Path('\/kaggle\/working\/faces'))","b11edb7d":"# All these frames are from different videos!\n# From a given reference (image on the right), the algorithm finds the 8 most similar faces in all samples (8 small images on the left)!","4092b302":"# References:\n- https:\/\/blog.usejournal.com\/fastai-image-similarity-search-pytorch-hooks-spotifys-annoy-9161bf517aaf\n- https:\/\/towardsdatascience.com\/similar-images-recommendations-using-fastai-and-annoy-16d6ceb3b809\n- https:\/\/towardsdatascience.com\/finding-similar-images-using-deep-learning-and-locality-sensitive-hashing-9528afee02f5","a0d7c689":"# 4. Get similar faces using Spotify's Annoy","0a9493e9":"To be continued...","cd47bb9a":"# Embeddings: grouping similar faces automagically\nThis notebook demonstrates how embeddings can be used to group similar faces automagically. The idea is to develop this code into a validation strategy that ensures all actors (similar faces) are all in either train or validation sets.","0ea935a3":"# 2. Calculate embedding vectors from all images\nHere I'm using one of my pre-trained models, as the point of this notebook is not training\/generating fake\/real predictions, but group similar faces using embeddings.","2a215e20":"# 1. Extract first face from all sample videos","6edca51d":"### The magic happens here\nThis cell below calculates a forward pass through almost all NN. It stops in the 2nd last fully connected layer. The output for each image is a vector with 2048 dimensions. Later, the algorithm will find similar faces by grouping these vectors, finding which are closest to each other."}}