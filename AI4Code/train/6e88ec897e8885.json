{"cell_type":{"620d548b":"code","08dd3cfb":"code","f02f975c":"code","fa2cf639":"code","7c4ed1eb":"code","1807be8c":"code","f4548343":"code","bbd725bf":"code","14a6a0d8":"code","7bd1ba46":"code","429ea987":"code","cfd8e851":"code","2c4cf84d":"code","e10bb768":"code","75b1f7ff":"markdown","905dfc1a":"markdown","191d977b":"markdown","80cf33de":"markdown","1d29f8f9":"markdown","855ba35f":"markdown","47c7e93e":"markdown","f1a2a80a":"markdown","b232564e":"markdown"},"source":{"620d548b":"import numpy as np\nimport pandas as pd \nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nstopWords = stopwords.words('english')\nRE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)","08dd3cfb":"data = pd.read_csv('..\/input\/reddit-india-flair-detector\/rindia_ver2.csv')\ndata.head()","f02f975c":"data['Title Length'] = data['Title'].astype(str).apply(len)\ndata.head()","fa2cf639":"data.isnull().sum()","7c4ed1eb":"data.dropna(subset=['Title','Date'],inplace=True)","1807be8c":"fig = go.Figure(data=[go.Bar(\n                x = data['Flair'].value_counts()[:25].index.tolist(),\n                y = data['Flair'].value_counts()[:25].values.tolist())])\n\nfig.show()","f4548343":"fig = px.histogram(data, x=\"Title Length\")\nfig.show()","bbd725bf":"data['Date'] = data['Date'].apply(lambda x: x[:10])","14a6a0d8":"fig = px.histogram(data, x=\"Date\")\nfig.show()","7bd1ba46":"def clean_text(text):\n    text = re.sub('#', '', text)  # remove hashtags\n    text = re.sub('@\\S+', '', text)  # remove mentions\n    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-\/:;<=>?@[\\]^_`{|}~\"\"\"), '', text)  # remove punctuations\n    text = re.sub('\\s+', ' ', text)  # remove extra whitespace\n    text = RE_EMOJI.sub('',text)\n    words = word_tokenize(text)\n    clean_text = []\n    for word in words:\n        if word not in stopWords:\n            clean_text.append(word)\n    cln_txt = ' '.join(clean_text)\n    return cln_txt.lower()","429ea987":"data['Clean Title'] = data['Title'].apply(clean_text)\ndata.head()","cfd8e851":"def get_top_n_words(corpus, n=None):\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_n_words(data['Clean Title'], 20)\ndf1 = pd.DataFrame(common_words, columns = ['TitleText' , 'count'])\nunigram = df1.groupby('TitleText').sum()['count'].sort_values(ascending=False)\nfig = go.Figure(data=[go.Bar(\n                y = unigram.tolist(),\n                x = unigram.index.tolist())])\n\nfig.show()","2c4cf84d":"def get_top_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2),stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_bigram(data['Clean Title'], 20)\ndf2 = pd.DataFrame(common_words, columns = ['TitleText' , 'count'])\nbigram = df2.groupby('TitleText').sum()['count'].sort_values(ascending=False)\nfig2 = go.Figure(data=[go.Bar(\n                y = bigram.tolist(),\n                x = bigram.index.tolist())])\n\nfig2.show()","e10bb768":"def get_top_trigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3),stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_trigram(data['Clean Title'], 20)\ndf3 = pd.DataFrame(common_words, columns = ['TitleText' , 'count'])\ntrigram = df3.groupby('TitleText').sum()['count'].sort_values(ascending=False)\nfig3 = go.Figure(data=[go.Bar(\n                y = trigram.tolist(),\n                x = trigram.index.tolist())])\n\nfig3.show()","75b1f7ff":"## Loading the Data","905dfc1a":"## Trigram count","191d977b":"## Cleaning the Title","80cf33de":"## Flair Distribution","1d29f8f9":"## Bigram count","855ba35f":"## Imports","47c7e93e":"## Title Length Distribution","f1a2a80a":"# N-gram distribution\n**Reference - https:\/\/towardsdatascience.com\/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a**\n## Unigram count","b232564e":"## Date vs No of submissions"}}