{"cell_type":{"e8909b9c":"code","c5bc4f4c":"code","49a5344e":"code","e8ad7128":"code","87ed9b3a":"code","7d493095":"code","3af0557b":"code","64b82ce9":"code","b52596b4":"code","237cf20a":"code","da093158":"code","2564ced9":"code","5020d819":"code","e1488695":"code","ed1d67f3":"code","b9eb3f58":"code","2c3032fb":"code","b7b215da":"code","c040b572":"code","3ecbb2fa":"code","3fe9f2ac":"code","8d14455c":"code","eaee15d3":"code","40f9571e":"code","ca2c5286":"code","66636e74":"code","336422c0":"code","184f97f5":"code","4f3fd8e5":"code","77771fa7":"code","891612d3":"code","3bb25cb5":"code","50a94a68":"code","ff8fab86":"code","9e53c3cb":"code","c722c605":"code","a57c15a0":"code","ea24e860":"code","88ca5f51":"code","4a34d48c":"code","437e2153":"code","ab72b9bf":"code","3eb43a0a":"code","e51caa4a":"code","1e361eef":"code","c9764bc2":"code","d891b5e8":"code","22e95f4d":"code","9d21710b":"code","4ef0e1e8":"code","ffa7eefd":"code","49cde359":"code","7e2b0e35":"code","d7584c12":"code","6771fe3f":"code","7fbf051a":"code","3756272a":"code","dacb5eb3":"markdown","06806811":"markdown","e28aa270":"markdown","333a1ec0":"markdown","a9e62ce5":"markdown","9e067173":"markdown","5d00103c":"markdown","0a7a2c3b":"markdown","dca75838":"markdown","cf412d68":"markdown","958a1421":"markdown","2eac59bb":"markdown","fde07cb2":"markdown","ac6e0f72":"markdown","2b0985b3":"markdown","bf1acd01":"markdown","94f8debb":"markdown","8848b092":"markdown","bb9cb301":"markdown","0ac13b93":"markdown","554d3f54":"markdown","0e23fc34":"markdown","d51d51fd":"markdown","615a5392":"markdown","a1427a7b":"markdown","39a8cc56":"markdown"},"source":{"e8909b9c":"import os\nimport sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","c5bc4f4c":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import RobustScaler,StandardScaler,QuantileTransformer,PowerTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import log_loss\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom scipy.stats import skew,boxcox,boxcox_normmax,kurtosis\n#sns.set_context(\"paper\", font_scale = 1, rc={\"grid.linewidth\": 3})\npd.set_option('display.max_rows', 100, 'display.max_columns', 900)\nfrom matplotlib.pyplot import cm\nfrom sklearn.feature_selection import VarianceThreshold\nimport sys\nfrom torch.utils.data import Dataset, TensorDataset","49a5344e":"data_dir = '..\/input\/lish-moa'\nprint(os.listdir(data_dir))\n","e8ad7128":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\ntrain_drug= pd.read_csv('..\/input\/lish-moa\/train_drug.csv')\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","87ed9b3a":"#train features\ntrain_features.head(3)","7d493095":"# test fetures\ntest_features.head(3)","3af0557b":"# contains an anonymous drug_id for the training set only.\ntrain_drug.head(2)","64b82ce9":"# binary MoA targets that are scored(used for scoring and prediction)\ntrain_targets_scored.head(3)","b52596b4":"# Additional (optional) binary MoA responses for the training data. These are not predicted nor scored.\ntrain_targets_nonscored.head(3)","237cf20a":"genes=[col for col in train_features.columns if col.startswith('g-')]\ncells=[col for col in train_features.columns if col.startswith('c-')]\nprint(genes)\nprint('    <----------------------------------------------------------------------------------------------------------------->')\nprint(cells)\nprint('\\n')\nprint(len(genes))\nprint('\\n')\nprint(len(cells))","da093158":"# fetaures shape\nprint(train_features.shape)\nprint(test_features.shape)\nprint(train_targets_scored.shape)\nprint(train_targets_nonscored.shape)\nprint(train_drug.shape)","2564ced9":"# Checking null\/nan values if any\nprint(train_features.isna().sum().sum())\nprint(test_features.isna().sum().sum())\nprint(train_drug.isna().sum().sum())\nprint(train_targets_scored.isna().sum().sum())\nprint(train_targets_nonscored.isna().sum().sum())","5020d819":"train_features2=train_features.copy()\ntest_features2=test_features.copy()","e1488695":"g_features = [feature for feature in train_features.columns if feature.startswith('g-')]\nc_features = [feature for feature in train_features.columns if feature.startswith('c-')]\nother_features = [feature for feature in train_features.columns if feature not in g_features and \n                                                             feature not in c_features and \n                                                             feature not in train_targets_scored and\n                                                             feature not in train_targets_nonscored]\n\nprint(f'Number of g- Features: {len(g_features)}')\nprint(f'Number of c- Features: {len(c_features)}')\nprint(f'Number of Other Features: {len(other_features)} ({other_features})')","ed1d67f3":"qt = QuantileTransformer(n_quantiles=100,random_state=42,output_distribution='normal')\ndata = pd.concat([pd.DataFrame(train_features[g_features+c_features]), pd.DataFrame(test_features[g_features+c_features])])\ndata2 = qt.fit_transform(data[g_features+c_features])\ntrain_features[g_features+c_features] = pd.DataFrame(data2[:train_features.shape[0]])\ntest_features[g_features+c_features] = pd.DataFrame(data2[-test_features.shape[0]:])","b9eb3f58":"df = pd.concat([train_features, test_features])\ncolumns = g_features + c_features\ncorrelation = list(set([columns[np.random.randint(0, len(columns)-1)] for i in range(200)]))[:40]\ndata = df[correlation]\nf = plt.figure(figsize=(18, 12))\nsns.heatmap(data.corr(),cmap='cividis_r')\nplt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=50)\nplt.yticks(range(data.shape[1]), data.columns, fontsize=14)\nplt.show()","2c3032fb":"color=cm.rainbow(np.linspace(0,3,20))\ncolor_ind=0\nn_row = 6\nn_col = 5\nn_sub = 1 \nplt.rcParams[\"legend.loc\"] = 'upper right'\nfig = plt.figure(figsize=(14,14))\nplt.subplots_adjust(left=-0.3, right=1.3,bottom=-0.3,top=1.3)\nfor i in (np.arange(0,10,1)):\n    plt.subplot(n_row, n_col, n_sub)\n    sns.kdeplot(train_features.loc[:,g_features[i]],color=color[color_ind],shade=True,\n                 label=['mean:'+str('{:.2f}'.format(train_features.loc[:,g_features[i]].mean()))\n                        +'  ''std: '+str('{:.2f}'.format(train_features.loc[:,g_features[i]].std()))])\n    \n    plt.xlabel(g_features[i])\n    plt.legend()                    \n    n_sub+=1\n    color_ind+=1\nplt.show()","b7b215da":"color=cm.rainbow(np.linspace(0,3,20))\ncolor_ind=0\nn_row = 6\nn_col = 5\nn_sub = 1 \nplt.rcParams[\"legend.loc\"] = 'upper right'\nfig = plt.figure(figsize=(14,14))\nplt.subplots_adjust(left=-0.3, right=1.3,bottom=-0.3,top=1.3)\nfor i in (np.arange(0,10,1)):\n    plt.subplot(n_row, n_col, n_sub)\n    sns.kdeplot(train_features.loc[:,c_features[i]],color=color[color_ind],shade=True,\n                 label=['mean:'+str('{:.2f}'.format(train_features.loc[:,g_features[i]].mean()))\n                        +'  ''std: '+str('{:.2f}'.format(train_features.loc[:,g_features[i]].std()))])\n    \n    plt.xlabel(g_features[i])\n    plt.legend()                    \n    n_sub+=1\n    color_ind+=1\nplt.show()","c040b572":"color=cm.rainbow(np.linspace(0,3,20))\ncolor_ind=0\nn_row = 6\nn_col = 5\nn_sub = 1 \nplt.rcParams[\"legend.loc\"] = 'upper right'\nfig = plt.figure(figsize=(14,14))\nplt.subplots_adjust(left=-0.3, right=1.3,bottom=-0.3,top=1.3)\nfor i in (np.arange(0,10,1)):\n    plt.subplot(n_row, n_col, n_sub)\n    sns.kdeplot(test_features.loc[:,g_features[i]],color=color[color_ind],shade=True,\n                 label=['mean:'+str('{:.2f}'.format(train_features.loc[:,g_features[i]].mean()))\n                        +'  ''std: '+str('{:.2f}'.format(train_features.loc[:,g_features[i]].std()))])\n    \n    plt.xlabel(g_features[i])\n    plt.legend()                    \n    n_sub+=1\n    color_ind+=1\nplt.show()","3ecbb2fa":"color=cm.rainbow(np.linspace(0,3,20))\ncolor_ind=0\nn_row = 6\nn_col = 5\nn_sub = 1 \nplt.rcParams[\"legend.loc\"] = 'upper right'\nfig = plt.figure(figsize=(14,14))\nplt.subplots_adjust(left=-0.3, right=1.3,bottom=-0.3,top=1.3)\nfor i in (np.arange(0,10,1)):\n    plt.subplot(n_row, n_col, n_sub)\n    sns.kdeplot(test_features.loc[:,c_features[i]],color=color[color_ind],shade=True,\n                 label=['mean:'+str('{:.2f}'.format(train_features.loc[:,g_features[i]].mean()))\n                        +'  ''std: '+str('{:.2f}'.format(train_features.loc[:,g_features[i]].std()))])\n    \n    plt.xlabel(g_features[i])\n    plt.legend()                    \n    n_sub+=1\n    color_ind+=1\nplt.show()","3fe9f2ac":"for col in (genes + cells):\n\n    #transformer = PowerTransformer(method = 'yeo-johnson')\n    transformer = QuantileTransformer(n_quantiles=100,random_state=42, output_distribution='normal')\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","8d14455c":"color=cm.rainbow(np.linspace(0,3,20))\ncolor_ind=0\nn_row = 6\nn_col = 5\nn_sub = 1 \nplt.rcParams[\"legend.loc\"] = 'upper right'\nfig = plt.figure(figsize=(14,14))\nplt.subplots_adjust(left=-0.3, right=1.3,bottom=-0.3,top=1.3)\nfor i in (np.arange(0,10,1)):\n    plt.subplot(n_row, n_col, n_sub)\n    sns.kdeplot(train_features.loc[:,g_features[i]],color=color[color_ind],shade=True,\n                 label=['mean:'+str('{:.2f}'.format(train_features.loc[:,g_features[i]].mean()))\n                        +'  ''std: '+str('{:.2f}'.format(train_features.loc[:,g_features[i]].std()))])\n    \n    plt.xlabel(g_features[i])\n    plt.legend()                    \n    n_sub+=1\n    color_ind+=1\nplt.show()","eaee15d3":"color=cm.rainbow(np.linspace(0,3,20))\ncolor_ind=0\nn_row = 6\nn_col = 5\nn_sub = 1 \nplt.rcParams[\"legend.loc\"] = 'upper right'\nfig = plt.figure(figsize=(14,14))\nplt.subplots_adjust(left=-0.3, right=1.3,bottom=-0.3,top=1.3)\nfor i in (np.arange(0,10,1)):\n    plt.subplot(n_row, n_col, n_sub)\n    sns.kdeplot(train_features.loc[:,c_features[i]],color=color[color_ind],shade=True,\n                 label=['mean:'+str('{:.2f}'.format(train_features.loc[:,g_features[i]].mean()))\n                        +'  ''std: '+str('{:.2f}'.format(train_features.loc[:,g_features[i]].std()))])\n    \n    plt.xlabel(g_features[i])\n    plt.legend()                    \n    n_sub+=1\n    color_ind+=1\nplt.show()","40f9571e":"color=cm.rainbow(np.linspace(0,3,20))\ncolor_ind=0\nn_row = 6\nn_col = 5\nn_sub = 1 \nplt.rcParams[\"legend.loc\"] = 'upper right'\nfig = plt.figure(figsize=(14,14))\nplt.subplots_adjust(left=-0.3, right=1.3,bottom=-0.3,top=1.3)\nfor i in (np.arange(0,10,1)):\n    plt.subplot(n_row, n_col, n_sub)\n    sns.kdeplot(test_features.loc[:,c_features[i]],color=color[color_ind],shade=True,\n                 label=['mean:'+str('{:.2f}'.format(train_features.loc[:,g_features[i]].mean()))\n                        +'  ''std: '+str('{:.2f}'.format(train_features.loc[:,g_features[i]].std()))])\n    \n    plt.xlabel(g_features[i])\n    plt.legend()                    \n    n_sub+=1\n    color_ind+=1\nplt.show()","ca2c5286":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","66636e74":"#Creating more features with Genes\nn_comp = 650 \ndata = pd.concat([pd.DataFrame(train_features[genes]), pd.DataFrame(test_features[genes])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[genes]))\ntrain2 = data2[:train_features.shape[0]] \ntest2 = data2[-test_features.shape[0]:]\ntrain_gpca = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest_gpca = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntrain_features = pd.concat((train_features, train_gpca), axis=1)\ntest_features = pd.concat((test_features, test_gpca), axis=1)","336422c0":"#Creating more features with Cells\nn_comp = 50\ndata = pd.concat([pd.DataFrame(train_features[cells]), pd.DataFrame(test_features[cells])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[cells]))\ntrain2 = data2[:train_features.shape[0]]\ntest2 = data2[-test_features.shape[0]:]\ntrain_cpca = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest_cpca = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntrain_features = pd.concat((train_features, train_cpca), axis=1)\ntest_features = pd.concat((test_features, test_cpca), axis=1)","184f97f5":"print(train_features.shape)\nprint(test_features.shape)","4f3fd8e5":"# Feature Selection using Variance Encoding\n\nvar_thresh = VarianceThreshold(0.845)\ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\ntrain_features.shape\n\n","77771fa7":"from sklearn.cluster import KMeans\ndef feat_cluster(train, test, n_clusters_g = 22, n_clusters_c = 4, SEED = 42):\n    \n    features_g = g_features\n    features_c = c_features\n    \n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n        train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n        test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\ntrain_features2 ,test_features2=feat_cluster(train_features2,test_features2)","891612d3":"train_pca=pd.concat((train_gpca,train_cpca),axis=1)\ntest_pca=pd.concat((test_gpca,test_cpca),axis=1)","3bb25cb5":"def feat_cluster_pca(train, test,n_clusters=5,SEED = 42):\n        data=pd.concat([train,test],axis=0)\n        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n        train[f'clusters_pca'] = kmeans.labels_[:train.shape[0]]\n        test[f'clusters_pca'] = kmeans.labels_[train.shape[0]:]\n        train = pd.get_dummies(train, columns = [f'clusters_pca'])\n        test = pd.get_dummies(test, columns = [f'clusters_pca'])\n        return train, test\ntrain_cluster_pca ,test_cluster_pca = feat_cluster_pca(train_pca,test_pca)","50a94a68":"train_cluster_pca = train_cluster_pca.iloc[:,700:]\ntest_cluster_pca = test_cluster_pca.iloc[:,700:]","ff8fab86":"train_features_cluster=train_features2.iloc[:,876:]\ntest_features_cluster=test_features2.iloc[:,876:]","9e53c3cb":"gsquarecols=['g-574','g-211','g-216','g-0','g-255','g-577','g-153','g-389','g-60','g-370','g-248','g-167',\\\n             'g-203','g-177','g-301','g-332','g-517','g-6','g-744','g-224','g-162','g-3','g-736','g-486',\\\n             'g-283','g-22','g-359','g-361','g-440','g-335','g-106','g-307','g-745','g-146','g-416','g-298',\\\n             'g-666','g-91','g-17','g-549','g-145','g-157','g-768','g-568','g-396']","c722c605":"def feat_stats(train, test):\n    \n    features_g = g_features\n    features_c = c_features\n    \n    for df in train, test:\n        df['g_sum'] = df[features_g].sum(axis = 1)\n        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n        df['g_skew'] = df[features_g].skew(axis = 1)\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n        df['c52_c42'] = df['c-52'] * df['c-42']\n        df['c13_c73'] = df['c-13'] * df['c-73']\n        df['c26_c13'] = df['c-23'] * df['c-13']\n        df['c33_c6'] = df['c-33'] * df['c-6']\n        df['c11_c55'] = df['c-11'] * df['c-55']\n        df['c38_c63'] = df['c-38'] * df['c-63']\n        df['c38_c94'] = df['c-38'] * df['c-94']\n        df['c13_c94'] = df['c-13'] * df['c-94']\n        df['c4_c52'] = df['c-4'] * df['c-52']\n        df['c4_c42'] = df['c-4'] * df['c-42']\n        df['c13_c38'] = df['c-13'] * df['c-38']\n        df['c55_c2'] = df['c-55'] * df['c-2']\n        df['c55_c4'] = df['c-55'] * df['c-4']\n        df['c4_c13'] = df['c-4'] * df['c-13']\n        df['c82_c42'] = df['c-82'] * df['c-42']\n        df['c66_c42'] = df['c-66'] * df['c-42']\n        df['c6_c38'] = df['c-6'] * df['c-38']\n        df['c2_c13'] = df['c-2'] * df['c-13']\n        df['c62_c42'] = df['c-62'] * df['c-42']\n        df['c90_c55'] = df['c-90'] * df['c-55']\n        \n        \n        for feature in features_c:\n             df[f'{feature}_squared'] = df[feature] ** 2\n        for feature in features_c:\n             df[f'{feature}_cubed'] = df[feature] ** 3         \n                \n        for feature in gsquarecols:\n            df[f'{feature}_squared'] = df[feature] ** 2\n        for feature in gsquarecols:\n            df[f'{feature}_cubed'] = df[feature] ** 3\n        \n    return train, test\n\ntrain_features2,test_features2=feat_stats(train_features2,test_features2)","a57c15a0":"train_features_stats=train_features2.iloc[:,902:]\ntest_features_stats=test_features2.iloc[:,902:]","ea24e860":"train_features = pd.concat((train_features, train_features_cluster,train_cluster_pca,train_features_stats), axis=1)\ntest_features = pd.concat((test_features, test_features_cluster,test_cluster_pca,test_features_stats), axis=1)","88ca5f51":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train.merge(train_targets_nonscored, on='sig_id')\ntrain = train.merge(train_drug, on='sig_id')\ntrain = train[train['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\n#train = train_features.merge(train_targets_scored, on='sig_id')\n#train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n#test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n#target = train[train_targets_scored.columns]","4a34d48c":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)\n#target_cols = target.drop('sig_id', axis=1).columns.values.tolist()\ntarget_cols = [x for x in train_targets_scored.columns if x != 'sig_id']\naux_target_cols = [x for x in train_targets_nonscored.columns if x != 'sig_id']\nall_target_cols = target_cols + aux_target_cols\n\nnum_targets = len(target_cols)\nnum_aux_targets = len(aux_target_cols)\nnum_all_targets = len(all_target_cols)\nprint('num_targets: {}'.format(num_targets))\nprint('num_aux_targets: {}'.format(num_aux_targets))\nprint('num_all_targets: {}'.format(num_all_targets))","437e2153":"# Final Dataframe shapes\nprint(train.shape)\nprint(test.shape)\nprint(sample_submission.shape)","ab72b9bf":"class MoADataset:\n    #it takes whatever arguments needed to build a list of tuples \u2014 it may be the name of a CSV file that will be loaded and processed; it may be two tensors, one for features, another one for labels; or anything else, depending on the task at hand.\n    def __init__(self, features, targets): \n        self.features = features\n        self.targets = targets\n   #it should simply return the size of the whole dataset so, whenever it is sampled, its indexing is limited to the actual size.\n    def __len__(self):\n        return (self.features.shape[0])\n    #There is no need to load the whole dataset in the constructor method (__init__). If your dataset is big (tens of thousands of image files, for instance), loading it at once would not be memory efficient. It is recommended to load them on demand (whenever __get_item__ is called).\n    # it allows the dataset to be indexed, so it can work like a list (dataset[i]) \u2014 it must return a tuple (features, label) corresponding to the requested data point. \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n    ","3eb43a0a":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train() #  set the model to training mode\n    final_loss = 0  # Initialise final loss to zero\n    \n    for data in dataloader:\n        optimizer.zero_grad() #every time we use the gradients to update the parameters, we need to zero the gradients afterwards\n        inputs, targets = data['x'].to(device), data['y'].to(device) #Sending data to GPU(cuda) if gpu is available otherwise CPU\n        outputs = model(inputs) #output \n        loss = loss_fn(outputs, targets) #loss function\n        loss.backward() #compute gradients(work its way BACKWARDS from the specified loss)\n        optimizer.step()  #gradient optimisation\n        scheduler.step() \n        \n        final_loss += loss.item() #Final loss\n        \n    final_loss \/= len(dataloader) #average loss\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval() #  set the model to evaluation\/validation mode\n    final_loss = 0 # Initialise validation final loss to zero\n    valid_preds = [] #Empty list for appending prediction\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device) #Sending data to GPU(cuda) if gpu is available otherwise CPU\n        outputs = model(inputs) #output\n        loss = loss_fn(outputs, targets) #loss calculation\n        \n        final_loss += loss.item() #final validation loss\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy()) # get CPU tensor as numpy array # cannot get GPU tensor as numpy array directly\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds) #concatenating predictions under valid_preds\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():  # need to use NO_GRAD to keep the update out of the gradient computation\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy()) \n        \n    preds = np.concatenate(preds)\n    \n    return preds","e51caa4a":"# Label Smoothing Regularization(LSR) is a trick to overcome overfitting and reduce the ability of the model to adapt.\n# it is similar to CrossEntropyLoss in Tensorflow.\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","1e361eef":"#class LabelSmoothingCrossEntropy(nn.Module):\n#import torch\n#from torch.nn.modules.loss import _WeightedLoss\n#import torch.nn.functional as F\n\n#class SmoothCrossEntropyLoss(_WeightedLoss):\n#    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n#        super().__init__(weight=weight, reduction=reduction)\n#        self.smoothing = smoothing\n#        self.weight = weight\n#        self.reduction = reduction\n#\n#    @staticmethod\n#    def _smooth_one_hot(targets:torch.Tensor, n_classes:int, smoothing=0.0):\n#        assert 0 <= smoothing < 1\n#        with torch.no_grad():\n#            targets = torch.empty(size=(targets.size(0), n_classes),\n#                    device=targets.device) \\\n#                .fill_(smoothing \/(n_classes-1)) \\\n#                .scatter_(1, targets.data.unsqueeze(1), 1.-smoothing)\n#        return targets\n#\n#    def forward(self, inputs, targets):\n#        targets = SmoothCrossEntropyLoss._smooth_one_hot(targets, inputs.size(-1),\n#            self.smoothing)\n#        lsm = F.log_softmax(inputs, -1)\n#\n#        if self.weight is not None:\n#            lsm = lsm * self.weight.unsqueeze(0)\n#\n#        loss = -(targets * lsm).sum(-1)\n#\n#        if  self.reduction == 'sum':\n#            loss = loss.sum()\n#        elif  self.reduction == 'mean':\n#            loss = loss.mean()\n#\n#        return loss","c9764bc2":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super(Model, self).__init__()\n        self.hidden_size = [1500, 1250, 1000, 750]\n        self.dropout_value = [0.5, 0.35, 0.3, 0.25]\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n        \n        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n        self.dropout2 = nn.Dropout(self.dropout_value[0])\n        self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n\n        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n        self.dropout3 = nn.Dropout(self.dropout_value[1])\n        self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n\n        self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n        self.dropout4 = nn.Dropout(self.dropout_value[2])\n        self.dense4 = nn.Linear(self.hidden_size[2], self.hidden_size[3])\n\n        self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n        self.dropout5 = nn.Dropout(self.dropout_value[3])\n        self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = F.leaky_relu(self.dense3(x))\n\n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = F.leaky_relu(self.dense4(x))\n\n        x = self.batch_norm5(x)\n        x = self.dropout5(x)\n        x = self.dense5(x)\n        return x\n    \nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing \/ (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n            \n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    ","d891b5e8":"class FineTuneScheduler:\n    def __init__(self, epochs):\n        self.epochs = epochs\n        self.epochs_per_step = 0\n        self.frozen_layers = []\n\n    def copy_without_top(self, model, num_features, num_targets, num_targets_new):\n        self.frozen_layers = []\n\n        model_new = Model(num_features, num_targets)\n        model_new.load_state_dict(model.state_dict())\n\n        # Freeze all weights\n        for name, param in model_new.named_parameters():\n            layer_index = name.split('.')[0][-1]\n\n            if layer_index == 5:\n                continue\n\n            param.requires_grad = False\n\n            # Save frozen layer names\n            if layer_index not in self.frozen_layers:\n                self.frozen_layers.append(layer_index)\n\n        self.epochs_per_step = self.epochs \/\/ len(self.frozen_layers)\n\n        # Replace the top layers with another ones\n        model_new.batch_norm5 = nn.BatchNorm1d(model_new.hidden_size[3])\n        model_new.dropout5 = nn.Dropout(model_new.dropout_value[3])\n        model_new.dense5 = nn.utils.weight_norm(nn.Linear(model_new.hidden_size[-1], num_targets_new))\n        model_new.to(DEVICE)\n        return model_new\n\n    def step(self, epoch, model):\n        if len(self.frozen_layers) == 0:\n            return\n\n        if epoch % self.epochs_per_step == 0:\n            last_frozen_index = self.frozen_layers[-1]\n            \n            # Unfreeze parameters of the last frozen layer\n            for name, param in model.named_parameters():\n                layer_index = name.split('.')[0][-1]\n\n                if layer_index == last_frozen_index:\n                    param.requires_grad = True\n\n            del self.frozen_layers[-1]  # Remove the last layer as unfrozen","22e95f4d":"# convert categorical columns\ndef process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data","9d21710b":"feature_cols = [c for c in process_data(train).columns if c not in all_target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold', 'sig_id', 'drug_id']]\nnum_features = len(feature_cols)\n","4ef0e1e8":"\n\n# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 24\nBATCH_SIZE = 128\n\nWEIGHT_DECAY = {'ALL_TARGETS': 1e-5, 'SCORED_ONLY': 3e-6}\nMAX_LR = {'ALL_TARGETS': 1e-2, 'SCORED_ONLY': 3e-3}\nDIV_FACTOR = {'ALL_TARGETS': 1e3, 'SCORED_ONLY': 1e2}\nPCT_START = 0.1","ffa7eefd":"# Show model architecture\nmodel = Model(num_features, num_all_targets)\nmodel","49cde359":"from sklearn.model_selection import KFold\n\ndef make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH):\n    vc = train.drug_id.value_counts()\n    vc1 = vc.loc[vc <= DRUG_THRESH].index.sort_values()\n    vc2 = vc.loc[vc > DRUG_THRESH].index.sort_values()\n\n    for seed_id in range(SEEDS):\n        kfold_col = 'kfold_{}'.format(seed_id)\n        \n        # STRATIFY DRUGS 18X OR LESS\n        dct1 = {}\n        dct2 = {}\n\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.groupby('drug_id')[target_cols].mean().loc[vc1]\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n\n        # STRATIFY DRUGS MORE THAN 18X\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.loc[train.drug_id.isin(vc2)].reset_index(drop=True)\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n        # ASSIGN FOLDS\n        train[kfold_col] = train.drug_id.map(dct1)\n        train.loc[train[kfold_col].isna(), kfold_col] = train.loc[train[kfold_col].isna(), 'sig_id'].map(dct2)\n        train[kfold_col] = train[kfold_col].astype('int8')\n        \n    return train\n\nSEEDS = 7\nNFOLDS = 7\nDRUG_THRESH = 18\n\ntrain = make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH)\ntrain.head()","7e2b0e35":"def run_training(fold_id, seed_id):\n    seed_everything(seed_id)\n    \n    train_ = process_data(train)\n    test_ = process_data(test)\n    \n    kfold_col = f'kfold_{seed_id}'\n    trn_idx = train_[train_[kfold_col] != fold_id].index\n    val_idx = train_[train_[kfold_col] == fold_id].index\n    \n    train_df = train_[train_[kfold_col] != fold_id].reset_index(drop=True)\n    valid_df = train_[train_[kfold_col] == fold_id].reset_index(drop=True)\n    \n    def train_model(model, tag_name, target_cols_now, fine_tune_scheduler=None):\n        x_train, y_train  = train_df[feature_cols].values, train_df[target_cols_now].values\n        x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols_now].values\n        \n        train_dataset = MoADataset(x_train, y_train)\n        valid_dataset = MoADataset(x_valid, y_valid)\n\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n        \n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY[tag_name])\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n                                                  steps_per_epoch=len(trainloader),\n                                                  pct_start=PCT_START,\n                                                  div_factor=DIV_FACTOR[tag_name], \n                                                  max_lr=MAX_LR[tag_name],\n                                                  epochs=EPOCHS)\n        \n        loss_fn = nn.BCEWithLogitsLoss()\n        loss_tr = SmoothBCEwLogits(smoothing=0.001)\n\n        oof = np.zeros((len(train), len(target_cols_now)))\n        best_loss = np.inf\n        \n        for epoch in range(EPOCHS):\n            if fine_tune_scheduler is not None:\n                fine_tune_scheduler.step(epoch, model)\n\n            train_loss = train_fn(model, optimizer, scheduler, loss_tr, trainloader, DEVICE)\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n            print(f\"SEED: {seed_id}, FOLD: {fold_id}, {tag_name}, EPOCH: {epoch}, train_loss: {train_loss:.6f}, valid_loss: {valid_loss:.6f}\")\n\n            if np.isnan(valid_loss):\n                break\n            \n            if valid_loss < best_loss:\n                best_loss = valid_loss\n                oof[val_idx] = valid_preds\n                torch.save(model.state_dict(), f\"{tag_name}_FOLD{fold_id}_.pth\")\n\n        return oof\n\n    fine_tune_scheduler = FineTuneScheduler(EPOCHS)\n\n    pretrained_model = Model(num_features, num_all_targets)\n    pretrained_model.to(DEVICE)\n\n    # Train on scored + nonscored targets\n    train_model(pretrained_model, 'ALL_TARGETS', all_target_cols)\n\n    # Load the pretrained model with the best loss\n    pretrained_model = Model(num_features, num_all_targets)\n    pretrained_model.load_state_dict(torch.load(f\"ALL_TARGETS_FOLD{fold_id}_.pth\"))\n    pretrained_model.to(DEVICE)\n\n    # Copy model without the top layer\n    final_model = fine_tune_scheduler.copy_without_top(pretrained_model, num_features, num_all_targets, num_targets)\n\n    # Fine-tune the model on scored targets only\n    oof = train_model(final_model, 'SCORED_ONLY', target_cols, fine_tune_scheduler)\n\n    # Load the fine-tuned model with the best loss\n    model = Model(num_features, num_targets)\n    model.load_state_dict(torch.load(f\"SCORED_ONLY_FOLD{fold_id}_.pth\"))\n    model.to(DEVICE)\n\n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    predictions = np.zeros((len(test_), num_targets))\n    predictions = inference_fn(model, testloader, DEVICE)\n    return oof, predictions","d7584c12":"def run_k_fold(NFOLDS, seed_id):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold_id in range(NFOLDS):\n        oof_, pred_ = run_training(fold_id, seed_id)\n        predictions += pred_ \/ NFOLDS\n        oof += oof_\n        \n    return oof, predictions","6771fe3f":"# Averaging on multiple SEEDS\n\nSEED = [0, 1, 2, 3, 4, 5, 6]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    \n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ \/ len(SEED)\n    predictions += predictions_ \/ len(SEED)\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions\n","7fbf051a":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\n\nfor i in range(len(target_cols)):\n    score += log_loss(y_true[:, i], y_pred[:, i])\n\nprint(\"CV log_loss: \", score \/ y_pred.shape[1])","3756272a":"submission = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsubmission.to_csv('submission.csv', index=False)\n","dacb5eb3":"`Visualising Genes Testing set before applying transformation( Not Normal\/Gaussian)`","06806811":"<a id=\"10\"><\/a>\n# Prediction & Submission","e28aa270":"<a id=\"11\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:lightgray; border:0; color:black' role=\"tab\" aria-controls=\"home\"><center>References<\/center><\/h2>\n\n* [MoA | Pytorch | 0.01859 | RankGauss | PCA | NN](https:\/\/www.kaggle.com\/kushal1506\/moa-pytorch-0-01859-rankgauss-pca-nn)\n* [Pytorch CV|0.0145| LB| 0.01839 |](https:\/\/www.kaggle.com\/riadalmadani\/pytorch-cv-0-0145-lb-0-01839)\n\n","333a1ec0":"<a id=\"5\"><\/a>\n# PCA","a9e62ce5":"<h1><center>THANK YOU :)<\/center><\/h1>","9e067173":"<a id=\"2\"><\/a>\n# Import Libaries","5d00103c":"`Visualising Genes Testing set after transformation(looking Normal\/Gaussian), Mean=0, Std=1`","0a7a2c3b":"`Visualising Cells Testing set before applying transformation( Not Normal\/Gaussian)`","dca75838":"<a id=\"9\"><\/a>\n# Model","cf412d68":"`Applying Seed function to generate same random numbers when using pytorch, GPU and numpy functions.`","958a1421":"* **Learning rate scheduling**: Instead of using a fixed learning rate, we will use a learning rate scheduler, which will change the learning rate after every batch of training. There are many strategies for varying the learning rate during training, and the one we'll use is called the **\"One Cycle Learning Rate Policy\"**, which involves starting with a low learning rate, gradually increasing it batch-by-batch to a high learning rate for about 30% of epochs, then gradually decreasing it to a very low value for the remaining epochs. \n\n* **Weight decay**: We also use weight decay, which is yet another regularization technique which prevents the weights from becoming too large by adding an additional term to the loss function.\n","2eac59bb":"`Visualising Genes training set before applying transformation( Not Normal\/Gaussian)`","fde07cb2":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:lightgray; border:0; color:black' role=\"tab\" aria-controls=\"home\"><center>Table of Contents<\/center><\/h2>\n\n    \n    \n- [Problem Statement](#1)\n- [Import Libaries](#2)\n- [Reading Data](#3)     \n- [Explanatory Data Analysis (EDA)](#4)\n- [PCA](#5)\n- [MultilabelStratifiedKFold CV](#6)\n- [Pytorch Dataset Classes)](#7)\n- [Smoothing](#8)\n- [Model](#9)\n- [Prediction & Submission](#10)\n- [References](#11)\n","ac6e0f72":"`Applying Quantile transformer to make distribution more like gaussian\/Normal.I applied power transformer also but quantile did great job. `","2b0985b3":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:tomato' role=\"tab\" aria-controls=\"home\"><center>If you found this notebook helpful , some upvotes would be very much appreciated - That will keep me motivated :)<\/center><\/h2>\n","bf1acd01":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:plum; border:0; color:black' role=\"tab\" aria-controls=\"home\"><center>Mechanisms of Action (MoA) Prediction<\/center><\/h2>\n<img src=\"https:\/\/www.ledgerinsights.com\/wp-content\/uploads\/2019\/09\/drugs-pharmaceuticals.jpg\" width=\"1500\"><\/center>","94f8debb":"<a id=\"4\"><\/a>\n# Checking Normal distribution (skewness) in training and testing set for Genes and Cells","8848b092":"`Visualising Cells Training set before applying transformation( Not Normal\/Gaussian)`\n","bb9cb301":"### Correlation Heatmap","0ac13b93":"<a id=\"3\"><\/a>\n# Reading Data","554d3f54":"### **Features**\n\n* `sig_id` is the unique sample id\n* Features with `g-` prefix are gene expression features and there are 772 of them (from `g-0` to `g-771`)\n* Features with `c-` prefix are cell viability features and there are 100 of them (from `c-0` to `c-99`)\n* `cp_type` is a binary categorical feature which indicates the samples are treated with a compound or with a control perturbation (`trt_cp` or `ctl_vehicle`)\n* `cp_time` is a categorical feature which indicates the treatment duration (`24`, `48` or `72` hours)\n* `cp_dose` is a binary categorical feature which indicates the dose is low or high (`D1` or `D2`)m","0e23fc34":"<a id=\"1\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:lightgray; border:0; color:black' role=\"tab\" aria-controls=\"home\"><center>Problem Statment<\/center><\/h2>\n<b><i>In this competition, we will have access to a unique dataset that combines gene expression and cell viability data. The data is based on a new technology that measures simultaneously (within the same samples) human cells\u2019 responses to drugs in a pool of 100 different cell types (thus solving the problem of identifying ex-ante, which cell types are better suited for a given drug). In addition, We will have access to MoA annotations for more than 5,000 drugs in this dataset.Hence, Our task is to use the training dataset to develop an algorithm that automatically labels each case in the test set as one or more MoA classes. Note that since drugs can have multiple MoA annotations, the task is formally a multi-label classification problem.<\/b>\n","d51d51fd":"`Visualising Cells Trainig set after transformation(looking Normal\/Gaussian, Mean=0. Std=1)`","615a5392":"`Visualising Genes Trainig set after transformation(looking Normal\/Gaussian, Mean=0. Std=1)`","a1427a7b":"<a id=\"7\"><\/a>\n# Pytorch Dataset Classes","39a8cc56":"<a id=\"8\"><\/a>\n# Smoothing"}}