{"cell_type":{"128e59a3":"code","eecfb5e0":"code","24bac53f":"code","006217d3":"code","6681e6b9":"code","ea97fe2e":"code","eacc2972":"code","49367bb3":"code","0b1e3114":"code","b996bbeb":"code","2f7438f5":"code","9b5de607":"code","f7e5807b":"code","d8c424c9":"code","e9c0e692":"code","6478db06":"code","1df12a57":"code","43095899":"code","83d63278":"code","e3896c59":"code","3bc9f86b":"code","1ae65f1f":"code","3eed4083":"code","cfdb24a3":"code","cbcb656a":"code","d0ecda56":"code","b61d4338":"code","d85c0122":"code","fb388bb5":"code","64f3bf82":"code","1a9c1727":"code","8233fcb0":"code","9eb26e13":"code","739fe9e4":"code","fb411ff8":"code","c2824dc3":"code","56b15565":"code","a4feab3b":"code","87eb5eae":"code","89519c0b":"code","8023cb60":"code","653a4b8e":"code","bad2bbdc":"code","14103d0f":"code","35679b74":"code","dd014682":"code","e83592f2":"code","08bfe42a":"code","3adc1954":"code","b17c51bc":"code","b57fa740":"code","406db4eb":"code","25667734":"code","52d6365a":"code","48732598":"code","bf8a6ef3":"code","8da9276e":"code","c550669b":"code","5acea9aa":"code","6231f156":"code","3da0366e":"code","4b0e2776":"code","e6e93e14":"code","680e09b2":"code","9e9dc619":"code","6edc1aad":"code","7ed7dca3":"code","a82d59f0":"code","ec7cf6fe":"code","9ba72cdd":"code","817b9ba3":"code","3da3e32b":"code","83578deb":"code","4dfa43d1":"code","b1471e66":"code","7279d40b":"code","b92bf7bc":"code","a7e32817":"code","35178eca":"code","91dd9d47":"code","43d95e71":"code","4cae82bd":"code","cc776b3a":"code","326f659b":"code","af0b6e4b":"code","0383ad16":"code","91cd37c7":"code","4ab38367":"code","e6b279dc":"code","0a6ba02c":"code","f9e8931a":"code","cfa954b3":"code","38a22be7":"code","c7bbfe6c":"code","b56d88a5":"code","468d102e":"code","31f3c2c5":"code","c7c16d91":"code","5f3aa056":"code","4d4ffb74":"code","66abbf06":"code","3cbe570a":"code","7e625332":"code","ddd500dd":"code","8a71d502":"markdown","437b5b69":"markdown","68a9fa36":"markdown","524fe80d":"markdown","751a3489":"markdown","7de31760":"markdown","1d55e36f":"markdown","266cae07":"markdown","6d98f372":"markdown","edd74c45":"markdown","23b6d55c":"markdown","a7ccd2dd":"markdown","78be815d":"markdown","252afad5":"markdown","a59325b6":"markdown","3457c3bb":"markdown","f2464781":"markdown","e1cb3fa9":"markdown","9f44dbd1":"markdown","500ce255":"markdown","010908b6":"markdown","376cd89a":"markdown","b0b84110":"markdown","0f613f00":"markdown","c8646908":"markdown","d37eb4db":"markdown","7763593b":"markdown","19c6b1f9":"markdown","1932b5ea":"markdown","e84d4487":"markdown","21f9046b":"markdown","605560b5":"markdown","bb42daca":"markdown","86c6728b":"markdown","3ed67d28":"markdown","953b3f70":"markdown","74ade764":"markdown","2cbddf6a":"markdown","99271dcc":"markdown","8c808a57":"markdown","e4dc1faf":"markdown","b88df094":"markdown","9b50d02f":"markdown","8f7c3b79":"markdown","b94e041f":"markdown","67e7ee65":"markdown","e35e76fa":"markdown","d1a82f0a":"markdown","ff2a77de":"markdown","fe0f00ff":"markdown","2f49cb9b":"markdown","50c7df87":"markdown","2b9447e8":"markdown","41dde44b":"markdown","0abfdc35":"markdown","03da8ecd":"markdown","e3ef3789":"markdown","885f9401":"markdown","320abd2e":"markdown","f11239a0":"markdown","0a2a042a":"markdown","52734ac8":"markdown","743fa06f":"markdown","e143da92":"markdown","6fe09b90":"markdown","55cf1eed":"markdown","d2e06155":"markdown","fc758c89":"markdown","58f04424":"markdown","1e908e14":"markdown","f29173e9":"markdown","bc652991":"markdown","e1576607":"markdown","8a6312a1":"markdown","144de0be":"markdown","bacafc37":"markdown","6b007e22":"markdown","b7e05a2f":"markdown","41aee1fd":"markdown","2ce5c0cc":"markdown","d4dc5380":"markdown","d9aeb203":"markdown","dce4319a":"markdown"},"source":{"128e59a3":"import pandas as pd\npd.options.display.max_columns = None\nimport numpy as np\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore')\nimport matplotlib.pyplot as plt\nimport cpi\nfrom tqdm import tqdm_notebook as tqdm\nimport os\nfrom collections import defaultdict\nfrom functools import reduce\nimport string\nfrom nltk.corpus import stopwords\nstopwords_eng = set(stopwords.words('english'))\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom wordcloud import WordCloud\nfrom scipy import stats\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, GroupKFold\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom sklearn.metrics import roc_auc_score, mean_squared_error\nimport joblib\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.utils import use_named_args\nfrom copy import deepcopy\nimport subprocess\nfrom statsmodels.stats.weightstats import _tconfint_generic\nfrom shutil import rmtree\nfrom catboost import CatBoostRegressor\n\n%matplotlib inline","eecfb5e0":"train = pd.read_hdf('..\/input\/tmdb-preprocessed\/train_kamal.h5')\ntest = pd.read_hdf('..\/input\/tmdb-preprocessed\/test_kamal.h5')","24bac53f":"train['dataset'] = 'Train'\ntest['dataset'] = 'Test'\ntest['revenue'] = -1\ndata = pd.concat((train, test))\n\n# helping function\ndef compare_barplots_categorical(df, first_group_bool_idx, second_group_bool_idx, include_sqrt=False, n_categories_in_figure=40, binned_categories=False):\n    high_cardinality_cols = []\n    for col in df.columns:\n        plt.figure(figsize=(12, 8))\n        counts_train = df.loc[first_group_bool_idx, col].value_counts(dropna=False)\n        counts_train \/= np.sum(counts_train)\n        counts_test = df.loc[second_group_bool_idx, col].value_counts(dropna=False)\n        counts_test \/= np.sum(counts_test)\n        y_max = 1.1 * max(max(counts_train), max(counts_test))\n        all_categories = sorted(df[col].unique()) if not binned_categories else sorted(df[col].unique(), key=lambda x: float(x[1:].split(',')[0]) if x != 'nan' else float('inf'))\n        for category in all_categories:\n            if category not in counts_train.index:\n                counts_train[category] = 0\n            if category not in counts_test.index:\n                counts_test[category] = 0\n        if len(all_categories) > 200:\n            high_cardinality_cols.append([col, len(all_categories)])\n        for pic_i, i in enumerate(range(0, len(all_categories), n_categories_in_figure)):\n            categories_current = all_categories[i: i+n_categories_in_figure]\n            all_counts = pd.DataFrame({'train': counts_train.loc[categories_current], 'test': counts_test.loc[categories_current]},\n                                      index=categories_current)\n            all_counts.plot.bar(rot=90, figsize=(12, 8))\n            name = f' ({categories_current[0]} - {categories_current[-1]})' if len(all_categories) > n_categories_in_figure else ''\n            plt.title(f'{col}' + name, fontsize=19)\n            plt.ylim((0, y_max))\n            if include_sqrt:\n                all_counts = pd.DataFrame({'train': counts_train.loc[categories_current].map(lambda x: np.sqrt(np.sqrt(x))), 'test': counts_test.loc[categories_current].map(lambda x: np.sqrt(np.sqrt(x)))},\n                                          index=categories_current)\n                plt.figure()\n                all_counts.plot.bar(rot=90, figsize=(12, 8))\n                plt.title(f'{col} (sqrt)' + name, fontsize=19)\n                plt.ylim((0, np.sqrt(np.sqrt(y_max))))\n        if len(df.columns) > 1:\n            plt.figure(figsize=(12, 1))\n            plt.plot(range(10), np.ones(10))\n    if len(high_cardinality_cols):\n        print('High cardinality cols: ' + ', '.join(list(map(lambda x: x[0] + f' ({x[1]} vals)', high_cardinality_cols))))\n        \ncompare_barplots_categorical(data[['release_year']], data['dataset']=='Train', data['dataset']=='Test')","006217d3":"def release_date_mapper(str_date):\n    if str(str_date) == 'nan':\n        return str_date\n    month, day, year_candidate = str_date.split('\/')\n    year = int(year_candidate)\n    if year < 100:\n        if year <= 19: #imprecise, but good enough\n            year += 2000\n        else:\n            year += 1900\n    return month + '-' + day + '-' + str(year)\n\ntest_orig = pd.read_csv('..\/input\/tmdb-box-office-prediction\/test.csv')\ntest_orig['release_date'] = test_orig['release_date'].map(release_date_mapper)\n\ntrain_orig = pd.read_csv('..\/input\/tmdb-box-office-prediction\/train.csv')\ntrain_orig['release_date'] = train_orig['release_date'].map(release_date_mapper)\ntrain_orig['dataset'] = 'Train'\ntest_orig['dataset'] = 'Test'\ndata_orig = pd.concat((train_orig, test_orig))\n\nlatest_release_date_test = pd.to_datetime(train_orig['release_date']).max()\n\nprint(f\"Only {100*sum(pd.to_datetime(train_orig['release_date']) > latest_release_date_test)\/len(train_orig):.1f} % of original training movies were release after the latest movie in test set.\")","6681e6b9":"data_orig['release_year'] = data_orig['release_date'].map(lambda x: int(x.split('-')[-1]) if str(x) != 'nan' else np.nan)\n\ndata_orig_latest = data_orig[data_orig['release_year'] >= 2016]\ncompare_barplots_categorical(data_orig_latest[['release_year']], data_orig_latest['dataset']=='Train', data_orig_latest['dataset']=='Test')","ea97fe2e":"# comparing counts of each 5 percentiles\ndata['budget_cat'] = pd.cut(data['budget'], bins=data['budget'].quantile(np.arange(0, 1.01, 0.05)), duplicates='drop', include_lowest=True).astype(str)\ncompare_barplots_categorical(data[['budget_cat']], data['dataset']=='Train', data['dataset']=='Test', n_categories_in_figure=21, binned_categories=True)\nplt.title('Budget train\/test distributions', fontsize=20)","eacc2972":"low_budget_value_counts = data.loc[data['budget'] <= 174, 'budget'].map(int).value_counts().sort_index()\nseries_cum_sum_percentages_low_budget = low_budget_value_counts.cumsum()\/low_budget_value_counts.sum()\n# let's convert to percentages of total\nlow_budget_value_counts = 100*low_budget_value_counts\/len(data)\n# let's leave budget categories responsible for 97% of low-budget bin\nlow_budget_value_counts = low_budget_value_counts[series_cum_sum_percentages_low_budget < 0.97]\nlow_budget_value_counts.plot.bar(figsize=(10, 8))\nplt.xlabel('Budget [$]', fontsize=15)\nplt.ylabel('Percentage of total number of movies [%]', fontsize=15)\nplt.title('Fraction of low-budget movies', fontsize=20)","49367bb3":"%%HTML\n<a id=missing-zeros><\/a>\n<img src=https:\/\/i.imgflip.com\/2wazn1.jpg width=\"250\" align=\"left\">","0b1e3114":"plt.figure(figsize=(12, 8))\nlogical_mask = data['budget'] < 100\nsns.stripplot(x='budget', y='revenue', data=data[logical_mask], jitter=True)\nplt.xlabel('Budget [$]', fontsize=15)\nplt.ylabel('Revenue [$]', fontsize=15)\nplt.title('Revenues of low-budget movies', fontsize=20)","b996bbeb":"%%HTML\n<img src=https:\/\/memegenerator.net\/img\/instances\/84246705\/let-make-missing-values-nan-again.jpg width=\"300\">","2f7438f5":"data.loc[data['budget'] == 0, 'budget'] = np.nan","9b5de607":"data[(data['budget'] < 37) & (data['budget'] > 32)]","f7e5807b":"%%HTML\n<img src=https:\/\/image.tmdb.org\/t\/p\/original\/RdsII0tG6vdF6GTaiHePFaAKVM.jpg width=\"250\">","d8c424c9":"%%HTML\n<a id='low-budget'><\/a>","e9c0e692":"plt.figure(figsize=(7, 4))\ndata.loc[data['budget'] == 35, 'budget'] *= 1e6\ndata['budget_cat'] = pd.cut(data['budget'], bins=data['budget'].quantile(np.arange(0, 1.01, 0.05)), duplicates='drop', include_lowest=True).astype(str)\nsns.swarmplot(x='budget_cat', y='release_year', data=data[data['budget'] < 6000], hue='dataset')\nplt.xlabel('Budget range [$]', fontsize=15)\nplt.ylabel('Year of release', fontsize=15)\nplt.title('Low-budget movies production', fontsize=20)","6478db06":"data_orig['budget_cat'] = pd.cut(data_orig['budget'], bins=data_orig['budget'].quantile(np.arange(0, 1.01, 0.05)), duplicates='drop', include_lowest=True).astype(str)\ncompare_barplots_categorical(data_orig[['budget_cat']], data_orig['dataset']=='Train', data_orig['dataset']=='Test', n_categories_in_figure=21, binned_categories=True)\nplt.title('Budget train\/test distributions, initial dataset', fontsize=20)","1df12a57":"compare_barplots_categorical(data[['budget_cat']], data['dataset']=='Train', data['dataset']=='Test', n_categories_in_figure=21, binned_categories=True)\nplt.title('Budget train\/test distributions', fontsize=20)","43095899":"data.loc[(data['release_year'] == 2018) & (data['budget'].isnull()), 'title'].head(20)","83d63278":"data_enriched_path = '..\/input\/tmdb-preprocessed\/data_tmdb_enriched_with_popularities.h5'\n# in order not to disclose my API key and to save some time I have uploaded the enriched dataset, \n# the dataset will be loaded after the final filling of values using the TMDB API\nif not os.path.exists(data_enriched_path): \n    import tmdbsimple as tmdb\n    tmdb.API_KEY = 'YOUR_KEY_HERE'\n    search = tmdb.Search()\n    hits_cout = 0\n    missing_credits_mask = data['budget'].isnull()\n    total_number_of_movies_to_fill = sum(missing_credits_mask)\n    \n    for index, movie_row in tqdm(data[missing_credits_mask].iterrows(), total=total_number_of_movies_to_fill):\n        response = search.movie(query=movie_row['title']) if np.isnan(movie_row['release_year']) else search.movie(query=movie_row['title'], year=movie_row['release_year'])\n        if response['total_results'] == 1:\n            movie = tmdb.Movies(response['results'][0]['id'])\n            movie_info = movie.info()\n            if movie_info['budget'] != 0 and np.isnan(data.loc[index, 'budget']):\n                hits_cout += 1\n                data.loc[index, 'budget'] = movie_info['budget']\n                \n    # there were 202 filled entries\n    \n    # fixing features engineered based on budget\n    data['budget_adjusted'] = (data['budget'].map(lambda x: [x]) + data['release_year'].map(lambda x: [x])).map(lambda budget_year: \n                                                                                                          cpi.inflate(budget_year[0], budget_year[1], to=2018)) #Inflation simple formula\n    data['_budget_runtime_ratio'] = data['budget_adjusted']\/data['runtime'] \n    data['_budget_popularity_ratio'] = data['budget_adjusted']\/data['popularity']\n    data['_budget_year_ratio'] = data['budget_adjusted']\/(data['release_year']*data['release_year'])\n    data['_budget_rating_ratio'] = data['budget_adjusted']\/data['rating']\n    data['_budget_totalVotes_ratio'] = data['budget_adjusted']\/data['totalVotes']    ","e3896c59":"train_data = data[data['dataset'] == 'Train']\n# let's merge early single-movie years into a joint release_year\nyearly_movie_counts = train_data['release_year'].value_counts()\nsingle_movie_years = set(yearly_movie_counts.index[yearly_movie_counts == 1])\ntrain_data.loc[train_data['release_year'].isin(single_movie_years), 'release_year'] = 1\ntrn_indices_set, val_indices_set = [set(indices) for indices in train_test_split(train_data.index, stratify=train_data['release_year'])]\ntrain_data.loc[train_data.index.isin(trn_indices_set), 'dataset'] = 'Train_train'\ntrain_data.loc[train_data.index.isin(val_indices_set), 'dataset'] = 'Train_val'\ncompare_barplots_categorical(train_data[['budget_cat']], train_data['dataset'] == 'Train_train', train_data['dataset'] == 'Train_val', \n                             n_categories_in_figure=21, binned_categories=True)\nplt.title('Budget train\/val distributions, stratified by release year', fontsize=20)","3bc9f86b":"def nans_proportion(pd_series, logical_index=None):\n    if logical_index is None:\n        logical_index = np.ones_like(pd_series, dtype=np.bool)\n    return pd_series[logical_index].isnull().sum()\/logical_index.sum()\nprint(f\"In train proportion of missing values is: {100*nans_proportion(data['rating'], data['dataset']=='Train'):.2f} %.\")\nprint(f\"In test proportion of missing values is: {100*nans_proportion(data['rating'], data['dataset']=='Test'):.2f} %.\")","1ae65f1f":"bins = data['rating'].quantile(np.arange(0, 1.001, 0.05))\ng = sns.FacetGrid(data, hue='dataset', height=7)\ng = g.map(sns.distplot, 'rating', bins=bins)\ng.add_legend()\nplt.title('Rating train\/test distributions', fontsize=20)","3eed4083":"%%HTML\n<a id='low-rating'><\/a>","cfdb24a3":"plt.figure(figsize=(12, 5))\nsns.swarmplot(x='rating', y='release_year', data=data[data['rating'] < 3], hue='dataset')\nplt.xlabel('Rating', fontsize=15)\nplt.ylabel('Year of release', fontsize=15)\nplt.title('Low-rating movies production', fontsize=20)","cbcb656a":"low_rating_proportion_trn = 100 * data[(data['release_year'] > 1990) & (data['dataset'] == 'Train')].groupby('release_year')['rating'].agg(lambda x: sum(x < 3)\/len(x))\nlow_rating_proportion_tst = 100 * data[(data['release_year'] > 1990) & (data['dataset'] == 'Test')].groupby('release_year')['rating'].agg(lambda x: sum(x < 3)\/len(x))\n\ndef plot_with_dots(ax, series):\n    ax.scatter(series.index, series, s=50)\n    ax.plot(series.index, series)\n\nplt.figure(figsize=(10, 7))\nplot_with_dots(plt, low_rating_proportion_trn)\nplot_with_dots(plt, low_rating_proportion_tst)\n\nplt.xlabel('Year of Movie Release', fontsize=15)\nplt.ylabel('Proportion of yearly produced movies [%]', fontsize=15)\nplt.legend(['Train', 'Test'])\nplt.title('Low-rating movies proportion', fontsize=20)","d0ecda56":"data_orig = data_orig.merge(data[['id', 'rating']], on='id', how='left')\nlow_rating_proportion_trn_orig = 100 * data_orig[(data_orig['release_year'] > 1980) &\n                                                 (data_orig['dataset'] == 'Train')\n                                                ].groupby('release_year')['rating'].agg(lambda x: sum(x < 3)\/len(x))\nlow_rating_proportion_tst_orig = 100 * data_orig[(data_orig['release_year'] > 1980) &\n                                                 (data_orig['dataset'] == 'Test')\n                                                ].groupby('release_year')['rating'].agg(lambda x: sum(x < 3)\/len(x))\n\nplt.figure(figsize=(7, 4))\nplot_with_dots(plt, low_rating_proportion_trn_orig)\nplot_with_dots(plt, low_rating_proportion_tst_orig)\n\nplt.xlabel('Year of Movie Release', fontsize=15)\nplt.ylabel('Proportion of yearly produced movies [%]', fontsize=15)\nplt.legend(['Train orig', 'Test orig'])\nplt.ylim(0, low_rating_proportion_trn.max())\nplt.title('Low-rating movies proportion', fontsize=20)","b61d4338":"# in the year 1990 there were no low-ranking movies in the initial dataset, if we check the figure above. So these movies were definitely added\ndata.loc[(data['dataset'] == 'Train') & (data['release_year'] == 1990) & (data['rating'] < 3), ['title', 'imdb_id']]","d85c0122":"print(f\"Number of duplicated movies: {len(data) - data['imdb_id'].isnull().sum() - data['imdb_id'].nunique()}.\")\ndata = data.iloc[data.astype(str).drop_duplicates().index]\nprint(f\"After the cleaning, number of duplicated movies: {len(data) - data['imdb_id'].isnull().sum() - data['imdb_id'].nunique()}.\")","fb388bb5":"data.loc[(data['dataset'] == 'Train') & (data['release_year'] == 1999) & (data['rating'] < 3), 'title'].iloc[:5]","64f3bf82":"low_rating_proportion = 100 * data[data['release_year'] > 2000].groupby('release_year')['rating'].agg(lambda x: sum(x < 3)\/len(x))\n\nplt.figure(figsize=(7, 5))\nplot_with_dots(plt, low_rating_proportion)\n\nplt.xlabel('Year of Movie Release', fontsize=15)\nplt.ylabel('Proportion of yearly produced movies [%]', fontsize=15)\nplt.title('Low-rating movies proportion', fontsize=20)","1a9c1727":"%%HTML\n<a id='apocalypse'><\/a>","8233fcb0":"low_rating_proportion_latest = data[data['release_year'] > 2008].groupby('release_year', as_index=False)['rating'].agg(lambda x: 100*sum(x < 3)\/len(x))\nlow_rating_proportion_latest.columns = ['release_year', 'low_ranking_proportion']\ncoeffs = np.polyfit(low_rating_proportion_latest['release_year'], low_rating_proportion_latest['low_ranking_proportion'], deg=2)\napocalypse_year = int(np.ceil(np.max(np.roots(np.concatenate((coeffs[:-1], coeffs[-1:] - 100))))))\nextrapolation = [np.polyval(coeffs, year) for year in range(2008, apocalypse_year)]\n\n_, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 10))\nax1.plot(list(range(2008, apocalypse_year)), extrapolation, '--', linewidth=2, c='r')\nax1.scatter(low_rating_proportion_latest['release_year'], low_rating_proportion_latest['low_ranking_proportion'], s=50)\nax1.set_xlabel('Year of Movie Release', fontsize=15)\nax1.set_ylabel('Proportion of yearly produced movies [%]', fontsize=15)\nax1.set_ylim(0, 20)\nax1.set_xlim(2007, 2030)\nax1.legend(['quadratic extrapolation', 'observations'])\nax1.set_title('10-year horizon', fontsize=16)\n\n\nax2.scatter(low_rating_proportion_latest['release_year'], low_rating_proportion_latest['low_ranking_proportion'], s=50)\nax2.scatter([apocalypse_year - 1], extrapolation[-1], s=150, c='r', marker='*')\nax2.plot(list(range(2008, apocalypse_year)), extrapolation, '--', linewidth=2, c='r')\nax2.set_xlabel('Year of Movie Release', fontsize=15)\nax2.set_ylabel('Proportion of yearly produced movies [%]', fontsize=15)\nax2.legend(['quadratic extrapolation', 'observations', 'estimated film-industry apocalypse'])\nax2.set_title('40-year horizon', fontsize=16)\nplt.suptitle('Low-rating movies proportion', fontsize=20)","9eb26e13":"_, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n\nadequate_rating_count = 100 * data[(data['release_year'] > 1990) & (data['release_year'] < 2018)].groupby('release_year')['rating'].agg(lambda x: sum(x > 4))\n\nplot_with_dots(ax1, adequate_rating_count)\nax1.set_xlabel('Year of Movie Release', fontsize=15)\nax1.set_ylabel('Number of yearly produced movies', fontsize=15)\nax1.set_title('Movies with ranking > 4', fontsize=20)\n\ngreat_rating_count = 100 * data[(data['release_year'] > 1990) & (data['release_year'] < 2018)].groupby('release_year')['rating'].agg(lambda x: sum(x >= 8))\n\nplot_with_dots(ax2, great_rating_count)\nax2.set_xlabel('Year of Movie Release', fontsize=15)\nax2.set_ylabel('Number of yearly produced movies', fontsize=15)\nax2.set_title('Movies with ranking >= 8', fontsize=20)","739fe9e4":"train_data = data[data['dataset'] == 'Train']\n# let's merge early single-movie years into a joint release_year\nyearly_movie_counts = train_data['release_year'].value_counts()\nsingle_movie_years = set(yearly_movie_counts.index[yearly_movie_counts == 1])\ntrain_data.loc[train_data['release_year'].isin(single_movie_years), 'release_year'] = 1\ntrn_indices_set, val_indices_set = [set(indices) for indices in train_test_split(train_data.index, stratify=train_data['release_year'])]\ntrain_data.loc[train_data.index.isin(trn_indices_set), 'dataset'] = 'Train_train'\ntrain_data.loc[train_data.index.isin(val_indices_set), 'dataset'] = 'Train_val'\n\nplt.figure(figsize=(8, 7))\nbins = train_data['rating'].quantile(np.arange(0, 1.001, 0.05))\nsns.distplot(train_data.loc[train_data['dataset'] == 'Train_train', 'rating'], bins=bins)\nsns.distplot(train_data.loc[train_data['dataset'] == 'Train_val', 'rating'], bins=bins)\nplt.title('Rating, train\/val distributions', fontsize=18)\nplt.legend(['Train: train', 'Train: val'])","fb411ff8":"importances = pd.DataFrame()\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=21)\n# we already checked budget and ranking, so we omit those as well\ndata_adversarial_validation = data.drop(['Keywords', 'belongs_to_collection', 'homepage', 'budget_cat', 'cast', 'crew', \n                                         'genres', 'imdb_id', 'id', 'original_title', 'overview', 'poster_path', \n                                         'production_companies', 'production_countries', 'spoken_languages', \n                                         'tagline', 'title', 'original_language', 'release_date', 'status', 'revenue'] +\n                                        [col for col in data.columns if 'rating' in col.lower() or 'budget' in col.lower()], axis=1)\n\ndataset_year = data_adversarial_validation['dataset'].map(str) + '_' + data_adversarial_validation['release_year'].map(str)\ndata_adversarial_validation['dataset'] = data_adversarial_validation['dataset'].map(lambda x: 1 if x == 'Test' else 0)\n\nauc_mean = 0\nfor fold_idx, (trn_idx, val_idx) in enumerate(skf.split(dataset_year, dataset_year)):\n\n    df_trn = data_adversarial_validation.iloc[trn_idx]\n    trn_y = df_trn['dataset']\n    df_trn.drop(['dataset'], axis=1, inplace=True)\n    df_val = data_adversarial_validation.iloc[val_idx]\n    val_y = df_val['dataset']\n    df_val.drop(['dataset'], axis=1, inplace=True)\n\n    clf = LGBMClassifier(objective='binary',\n                         boosting_type='gbdt',\n                         n_jobs=-1,\n                         n_estimators=2000,\n                         metric='auc')\n\n    clf.fit(\n        df_trn, trn_y,\n        eval_set=[(df_trn, trn_y), (df_val, val_y)],\n        early_stopping_rounds=50,\n        verbose=0\n    )\n    auc_mean += roc_auc_score(val_y, clf.predict_proba(df_val)[:, 1])\/skf.get_n_splits()\n    imp_df = pd.DataFrame({\n                'feature': df_trn.columns,\n                'gain': clf.feature_importances_,\n                'fold': [fold_idx + 1] * len(df_trn.columns),\n                })\n    importances = pd.concat([importances, imp_df], axis=0, sort=False)\nprint(f'Mean AUC: {auc_mean:.3}.')","c2824dc3":"plt.figure(figsize=(9, 9))\nmean_importances = importances.groupby('feature')['gain'].mean().sort_values()\nplt.barh(y=mean_importances.index, width=mean_importances)","56b15565":"print(f\"In train proportion of missing values is: {100*nans_proportion(data['popularity2'], data['dataset']=='Train'):.2f} %.\")\nprint(f\"In test proportion of missing values is: {100*nans_proportion(data['popularity2'], data['dataset']=='Test'):.2f} %.\")","a4feab3b":"print(f\"\"\"In additional train data from 2018, \nproportion of missing values is: {100*nans_proportion(data['popularity2'], (data['dataset']=='Train') & (data['release_year']==2018)):.2f} %.\"\"\")","87eb5eae":"bins = data['popularity2'].quantile(np.arange(0, 1.001, 0.05))\ng = sns.FacetGrid(data, hue='dataset', height=7)\ng = g.map(sns.distplot, 'popularity2', bins=bins)\ng.add_legend()\nplt.title('Popularity2 train\/test distributions', fontsize=20)","89519c0b":"if not os.path.exists(data_enriched_path): \n    import tmdbsimple as tmdb\n    tmdb.API_KEY = 'YOUR_KEY'\n    search = tmdb.Search()\n    hits_cout = 0\n    data['popularity3'] = np.nan\n    total_number_of_movies_to_fill = len(data)\n    \n    for index, movie_row in tqdm(data.iterrows(), total=total_number_of_movies_to_fill):\n        response = search.movie(query=movie_row['title']) if np.isnan(movie_row['release_year']) else search.movie(query=movie_row['title'], year=movie_row['release_year'])\n        if response['total_results'] == 1:\n            movie = tmdb.Movies(response['results'][0]['id'])\n            movie_info = movie.info()\n            data.loc[index, 'popularity3'] = movie_info['popularity']\n    helping_series = data['popularity'].map(lambda x: [x]) + data['popularity2'].map(lambda x: [x]) + data['popularity3'].map(lambda x: [x])\n    data['popularity_mean'] = helping_series.map(np.nanmean)","8023cb60":"print(f\"From the most recent added movies {100 * nans_proportion(data['overview'], data['release_date'] > latest_release_date_test):.1f} % do not have overview.\")\nprint(f\"In the original dataset {100 * nans_proportion(data_orig['overview']):.1f} % of movies do not have overview\")","653a4b8e":"_, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,10))\ndata_orig['overview_word_count'] = data_orig['overview'].map(lambda x: len(str(x).split()))\nbins = data_orig['overview_word_count'].quantile(np.arange(0, 1.001, 0.05))\nsns.distplot(data_orig.loc[data_orig['dataset'] == 'Train', 'overview_word_count'], bins=bins, ax=ax1)\nsns.distplot(data_orig.loc[data_orig['dataset'] == 'Test', 'overview_word_count'], bins=bins, ax=ax1)\nax1.set_title('Initial train\/test distributions', fontsize=18)\nax1.legend(['Train', 'Test'])\n\n\nbins = data['overview_word_count'].quantile(np.arange(0, 1.001, 0.05))\nsns.distplot(data.loc[data['dataset'] == 'Train', 'overview_word_count'], bins=bins, ax=ax2)\nsns.distplot(data.loc[data['dataset'] == 'Test', 'overview_word_count'], bins=bins, ax=ax2)\nax2.set_title('Enriched train\/test distributions', fontsize=18)\nax2.legend(['Train', 'Test'])\nplt.suptitle('Overview_word_count distributions', fontsize=21)","bad2bbdc":"%%HTML\n<a id='overview-length'><\/a>","14103d0f":"plt.figure(figsize=(8,6))\nplot_with_dots(plt, data[data['release_year'] > 1990].groupby('release_year')['overview_word_count'].median())\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Median overview length', fontsize=12)\nplt.title('Overview length, last three decades', fontsize=15)","35679b74":"pd.set_option('display.max_colwidth', -1)\ndata.loc[(data['release_year'] > 2015) & (data['overview_word_count'] < 20), ['title', 'overview']].sample(5, random_state=2)","dd014682":"print(f\"From originally available movies {100 * nans_proportion(data_orig['tagline']):.1f} % do not have a tagline.\")\nprint(f\"From the most recent added movies {100 * nans_proportion(data['tagline'], data['release_date'] > latest_release_date_test):.1f} % do not have a tagline.\")","e83592f2":"%%HTML\n<a id='tagline-length'><\/a>","08bfe42a":"plt.figure(figsize=(8,6))\nplot_with_dots(plt, data[data['release_year'] > 1990].groupby('release_year')['tagline_word_count'].median())\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Median overview tagline', fontsize=12)\nplt.title('Tagline length, last three decades', fontsize=15)","3adc1954":"%%HTML\n<img src=http:\/\/wist.info\/wp\/wp-content\/uploads\/2016\/04\/Chekhov-brevity-sister-of-talent-wist_info-quote.jpg width='400'>","b17c51bc":"print(f\"From originally available movies {100 * nans_proportion(data_orig['crew']):.1f} % do not have information about crew.\")\nfraction_of_record_with_missing_crew = data.loc[data['release_date'] > latest_release_date_test, 'crew'].map(lambda x: len(x) == 0).sum()\/sum(data['release_date'] > latest_release_date_test)\nprint(f'From the most recent added movies {100 * fraction_of_record_with_missing_crew:.1f} % do not have information about crew.')","b57fa740":"if os.path.exists(data_enriched_path):\n    data = pd.read_hdf(data_enriched_path, 'data')\nelse:\n    import tmdbsimple as tmdb\n    tmdb.API_KEY = 'YOUR_KEY_HERE'\n    search = tmdb.Search()\n    hits_cout = 0\n    attributes_to_fill = ['crew', 'cast', 'Keywords', 'genres', 'production_companies', 'production_countries', 'spoken_languages']\n    missing_credits_mask = (data[attributes_to_fill[0]].map(lambda x: len(x) == 0))\n    for col in attributes_to_fill[1:]:\n        missing_credits_mask = missing_credits_mask | (data[col].map(lambda x: len(x) == 0))\n    total_number_of_movies_to_fill = sum(missing_credits_mask)\n    # substituting dummy empty dicts with lists\n    for col in attributes_to_fill:\n        data.loc[missing_credits_mask, col] = data.loc[missing_credits_mask, col].map(lambda x: [] if isinstance(x, dict) else x)\n    for index, movie_row in tqdm(data[missing_credits_mask].iterrows(), total=total_number_of_movies_to_fill):\n        response = search.movie(query=movie_row['title']) if np.isnan(movie_row['release_year']) else search.movie(query=movie_row['title'], year=movie_row['release_year'])\n        if response['total_results'] == 1:\n            movie = tmdb.Movies(response['results'][0]['id'])\n            movie_info = movie.info(append_to_response='credits,keywords')\n            if len(data.loc[index, 'cast']) == 0:\n                data.loc[index, 'cast'].extend(movie_info['credits']['cast'])\n            if len(data.loc[index, 'crew']) == 0:\n                data.loc[index, 'crew'].extend(movie_info['credits']['crew'])\n            if len(data.loc[index, 'Keywords']) == 0:\n                data.loc[index, 'Keywords'].extend(movie_info['keywords']['keywords'])\n            for col in attributes_to_fill[3:]:\n                if len(data.loc[index, col]) == 0:\n                    data.loc[index, col].extend(movie_info[col])\n            hits_cout += int(len(movie_info['credits']['cast']) > 0 or len(movie_info['credits']['crew']) > 0)\n\n    print(f'Filling in was successful for {hits_cout} movies out of {total_number_of_movies_to_fill}.')\n    data.to_hdf(data_enriched_path, 'data')","406db4eb":"from sklearn.preprocessing import LabelEncoder\n# Thanks to this Kernel for the next 7 features https:\/\/www.kaggle.com\/artgor\/eda-feature-engineering-and-model-interpretation\ndata['genders_0_crew'] = data['crew'].apply(lambda x: sum([i['gender'] == 0 for i in x]) if isinstance(x, list) else np.nan)\ndata['genders_1_crew'] = data['crew'].apply(lambda x: sum([i['gender'] == 1 for i in x]) if isinstance(x, list) else np.nan)\ndata['genders_2_crew'] = data['crew'].apply(lambda x: sum([i['gender'] == 2 for i in x]) if isinstance(x, list) else np.nan)\ndata['_collection_name'] = data['belongs_to_collection'].apply(lambda x: x[0]['name'] if isinstance(x, list) else np.nan)\nle = LabelEncoder()\ndata['_collection_name'] = le.fit_transform(data['_collection_name'].map(str))\ndata.loc[data['_collection_name'].map(str) == 'nan', '_collection_name'] = np.nan\ndata['_num_Keywords'] = data['Keywords'].apply(lambda x: len(x) if isinstance(x, list) else np.nan)\ndata['_num_cast'] = data['cast'].apply(lambda x: len(x) if isinstance(x, list) else np.nan)\n\n#thanks to the cited Kamal's kernel\ndata['production_countries_count'] = data['production_countries'].apply(lambda x : len(x) if isinstance(x, list) else np.nan)\ndata['production_companies_count'] = data['production_companies'].apply(lambda x : len(x) if isinstance(x, list) else np.nan)\ndata['cast_count'] = data['cast'].apply(lambda x : len(x) if isinstance(x, list) else np.nan)\ndata['crew_count'] = data['crew'].apply(lambda x : len(x) if isinstance(x, list) else np.nan)","25667734":"%%HTML\n<a id='revenue-inflation'><\/a>","52d6365a":"mean_yearly_revenues = train.groupby('release_year', as_index=False)['revenue'].agg(np.nanmean)\nmean_yearly_revenues['revenue'] \/= mean_yearly_revenues.iloc[-1]['revenue']\n# let's see what one 2018 dollar cost was throught the history of cinema\ncpi_2018 = cpi.get(year_or_month=2018)\ndollar_costs = [cpi.get(year_or_month=year)\/cpi_2018 for year in mean_yearly_revenues['release_year'].values]\nplt.figure(figsize=(9, 5))\nplt.plot(mean_yearly_revenues['release_year'], mean_yearly_revenues['revenue'], linewidth=3.5)\nplt.plot(mean_yearly_revenues['release_year'], dollar_costs, linewidth=3.5)\nplt.legend(['Average movie revenue', '1$ (2018) value adjusted for inflation'], fontsize=14)\nplt.xlabel('Release Year', fontsize=15)\nplt.ylabel('Normalized scale (1 corresponds to 2018)', fontsize=15)\nplt.title('Investigating effects of inflation', fontsize=20)","48732598":"dollar_costs_df = pd.DataFrame({'year': mean_yearly_revenues['release_year'], 'dollar_2018_cost': dollar_costs})\ndata['dollar_2018_cost'] = data[['release_year']].merge(dollar_costs_df, left_on='release_year', right_on='year')['dollar_2018_cost']","bf8a6ef3":"plt.figure(figsize=(9, 4))\nsns.violinplot(x='release_year', y='revenue', data=train[train['release_date'] >= '2015-01-01'], color='lightgray')\nsns.swarmplot(x='release_year', y='revenue', data=train[train['release_date'] >= '2015-01-01'])\nplt.title('Revenue-based check', fontsize=22)\nplt.xlabel('Year of release', fontsize=16)\nplt.ylabel('Revenue', fontsize=16)","8da9276e":"train.loc[(train['release_date'] >= '2015-01-01') & (train['revenue'] > 2e9), ['title']]","c550669b":"%%HTML\n<img src=https:\/\/media.giphy.com\/media\/xT0xejJnePNcOWoHOo\/giphy.gif width=300>","5acea9aa":"# pandas apply is very slow, so again\ndata['revenue_adjusted'] = (data['revenue'].map(lambda x: [x]) + data['release_year'].map(lambda x: [x])).map(\n    lambda revenue_year: cpi.inflate(value=revenue_year[0], year_or_month=revenue_year[1], to=2018))\n\nmean_yearly_revenues_adjusted = data[data['dataset'] == 'Train'].groupby('release_year', as_index=False)['revenue_adjusted'].agg(np.nanmean)\n\nplt.figure(figsize=(9, 4))\nplt.plot(mean_yearly_revenues_adjusted['release_year'], mean_yearly_revenues_adjusted['revenue_adjusted'], linewidth=3.5)\nplt.xlabel('Release Year', fontsize=15)\nplt.ylabel('Mean yearly revenue [$]', fontsize=15)\nplt.title('Average movie revenue adjusted for inflation', fontsize=20)\ndata.drop('revenue_adjusted', axis=1, inplace=True)","6231f156":"%%HTML\n<img src=http:\/\/filmsplusmovies.com\/wp-content\/uploads\/2014\/06\/317-hollywood.jpg width=\"500\">","3da0366e":"data.drop('budget', axis=1, inplace=True)\ndata['revenue'] = data['revenue'].map(np.log1p)\n\ndata['_budget_runtime_ratio'] = data['budget_adjusted']\/data['runtime'] \ndata['_budget_popularity_ratio'] = data['budget_adjusted']\/data['popularity_mean']\ndata['_budget_year_ratio'] = data['budget_adjusted']\/(data['release_year']*data['release_year'])\ndata['_budget_rating_ratio'] = data['budget_adjusted']\/data['rating']\ndata['_budget_totalVotes_ratio'] = data['budget_adjusted']\/data['totalVotes'] ","4b0e2776":"all_crew_job_movies = pd.Series(reduce(lambda list1, list2: list1 + list2, \n                                        data['crew'].map(lambda x: list(set(map(lambda el: el['job'], x)))).values))\nplt.figure(figsize=(8, 9))\nnumber_of_movies_with_job = all_crew_job_movies.value_counts().sort_values(ascending=False)\npercentege_of_movies_with_job = 100 * number_of_movies_with_job \/ len(data)\ntop_frequent_jobs = percentege_of_movies_with_job[percentege_of_movies_with_job > 50].sort_values()\nplt.barh(top_frequent_jobs.index, top_frequent_jobs)\nplt.title('Crew jobs', fontsize=16)\nplt.xlabel('Percentage of movies having the job in a crew team [%]', fontsize=12)","e6e93e14":"jobs_of_interest_set = set(top_frequent_jobs.index)\n\ndef value_counts_in_list(input_list):\n    counts = defaultdict(int)\n    for val in input_list:\n        counts[val] += 1\n    return list(counts.items())\njob_person_counts = pd.Series(reduce(lambda list1, list2: list1 + list2, \n                                        data['crew'].map(lambda x: value_counts_in_list(\n                                            filter(lambda x: x in jobs_of_interest_set, \n                                                   map(lambda el: el['job'], \n                                                       x)\n                                        ))).values))\njob_head_counts_df = pd.DataFrame({'job': job_person_counts.map(lambda x: x[0]), 'headcount': job_person_counts.map(lambda x: x[1])})\npercentile_95_job_headcount = job_head_counts_df.groupby('job')['headcount'].quantile(0.95).map(int)\npercentile_95_job_headcount[top_frequent_jobs.index]","680e09b2":"columns_len_init = len(data.columns)\n# def return_job_names(crew, job_name):\n#     headcount = percentile_95_job_headcount[job_name]\n#     if not isinstance(crew, list) and np.isnan(crew):\n#         return np.full(headcount, fill_value=np.nan)\n#     names_generator = (el['name'] for el in crew if el['job'] == job_name)\n#     results = np.empty(headcount, dtype='object')\n#     for i in range(headcount):\n#         results[i] = next(names_generator, np.nan)\n#     return results\n\n# for crew_col in top_frequent_jobs.index:\n#     new_col_names = [f'{crew_col}_{i + 1}' for i in range(percentile_95_job_headcount[crew_col])]\n#     df_added_cols = pd.DataFrame(np.column_stack(data['crew'].map(lambda x: return_job_names(x, crew_col))).T, columns=new_col_names, index=data.index)\n#     data = pd.concat((data, df_added_cols), axis=1)","9e9dc619":"# field to use for counting (there should not be any missing values in the feature)\nprint(data.isnull().sum().sort_values()[:1])\ncount_col = data.isnull().sum().sort_values().index[0]","6edc1aad":"def return_joint_entry(crew, job_name, sep='^'):\n    headcount = percentile_95_job_headcount[job_name]\n    if not isinstance(crew, list) and np.isnan(crew):\n        return np.nan\n    names = [el['name'] for el in crew if el['job'] == job_name][:headcount]\n    return sep.join(names)\n\nfor crew_col in top_frequent_jobs.index:\n    data[crew_col] = data['crew'].map(lambda x: return_joint_entry(x, crew_col))\n    \njobs_to_dummify = [job for job in top_frequent_jobs.index if percentile_95_job_headcount[job] > 2]\nfinal_dummy_cols = []\n\ndef get_number_of_movies_with_name(pd_series, sep='^'):\n    names_in_movies = pd.Series(reduce(lambda list1, list2: list1 + list2,\n                                       pd_series.map(lambda x: x.split(sep)).values))\n    number_of_movies_with_name = names_in_movies.value_counts()\n    return number_of_movies_with_name\n    \nfor crew_col in jobs_to_dummify:\n    number_of_movies_with_name = get_number_of_movies_with_name(data[crew_col])\n    number_of_movies_with_name_train = get_number_of_movies_with_name(data.loc[data['dataset'] == 'Train', crew_col])\n    # to have a personal feature, there must be at least 10 movies in total and at least 5 movies in train\n    remaining_names = set(number_of_movies_with_name.index[number_of_movies_with_name >= 10]).intersection(\n        set(number_of_movies_with_name_train.index[number_of_movies_with_name_train >= 5]))\n    df_added_cols = data[crew_col].map(lambda x: '^'.join([name for name in x.split('^') if name in remaining_names])).str.get_dummies(sep='^')\n    df_added_cols.columns = [f'{crew_col}_{value}' for value in df_added_cols.columns]\n    final_dummy_cols.extend(df_added_cols.columns)\n    data = pd.concat((data, df_added_cols), axis=1)\ndata.drop(jobs_to_dummify + ['crew'], axis=1, inplace=True)\n                          \njobs_joint = [job for job in top_frequent_jobs.index if percentile_95_job_headcount[job] <= 2]\n\ndef hide_rare_values(data, column, overall_limit=7, train_limit=3):\n    counts_total = data.groupby(joint_col)[count_col].count()\n    counts_train = data[data['dataset']=='Train'].groupby(joint_col)[count_col].count()\n    names_to_forget = set(counts_total[counts_total < overall_limit].index).union(set(counts_train[counts_train < train_limit].index))\n    data.loc[data[joint_col].isin(names_to_forget), joint_col] = np.nan\n    return data\n\nfor joint_col in jobs_joint:\n    data = hide_rare_values(data, joint_col)","7ed7dca3":"def return_entries(list_dicts, sep='^'):\n    if not isinstance(list_dicts, list) and np.isnan(list_dicts):\n        return np.nan\n    entries = [el['name'] for el in list_dicts]\n    return sep.join(entries)\n\ndata['genres'] = data['genres'].map(return_entries)\ndata = hide_rare_values(data, 'genres', overall_limit=50, train_limit=20)\n\nfor dict_col in ['cast', 'Keywords', 'production_companies', 'production_countries', 'spoken_languages']:\n    data[dict_col] = data[dict_col].map(return_entries)    \n    names_in_movies = pd.Series(reduce(lambda list1, list2: list1 + list2,\n                                       data[dict_col].map(lambda x: x.split('^')).values))\n    number_of_movies_with_name = names_in_movies.value_counts()\n    # minimal number of movies to have a feature\n    min_movie_count = 20 if dict_col == 'cast' else 100 \n    remaining_names = number_of_movies_with_name.index[number_of_movies_with_name >= min_movie_count]\n    df_added_cols = data[dict_col].map(lambda x: '^'.join([name for name in x.split('^') if name in remaining_names])).str.get_dummies(sep='^')\n    df_added_cols.columns = [f'{dict_col}_{value}' for value in df_added_cols.columns]\n    final_dummy_cols.extend(df_added_cols.columns)\n    data = pd.concat((data, df_added_cols), axis=1)\ndata.drop(['cast', 'Keywords', 'production_companies', 'production_countries', 'spoken_languages'], axis=1, inplace=True)","a82d59f0":"porter = PorterStemmer()\n\ndef get_listofwords_series(col):\n    # mapping to str is performed so that missing values become 'nan' strings\n    all_list_of_words_series = data[col].map(str).map(lambda text: [porter.stem(w) if w != 'nan' else np.nan for w in word_tokenize(text.translate(str.maketrans('','',string.punctuation)).lower())])\n    if f'{col}_stopwords_count' not in data.columns:\n        data[f'{col}_stopwords_count'] =  all_list_of_words_series.map(lambda x: sum([w in stopwords_eng for w in x]))\n        data[f'{col}_stopwords_fraction'] = data[f'{col}_stopwords_count']\/all_list_of_words_series.map(len)\n    return all_list_of_words_series.map(lambda all_words: list(filter(lambda x: x not in stopwords_eng, all_words)))","ec7cf6fe":"def get_percentege_of_movies_with_word(col):\n    listofwords_series = get_listofwords_series(col)\n    word_movies_count = pd.Series(reduce(lambda list1, list2: list1 + list2, listofwords_series.map(lambda x: list(set(x))).values)).value_counts()\n    percentege_of_movies_with_word = 100 * word_movies_count.sort_values(ascending=False) \/ len(data)\n    return percentege_of_movies_with_word\n\ndef visualize_rank_freq(col, number_of_top_words_to_check=30):\n    percentege_of_movie_with_word = get_percentege_of_movies_with_word(col)\n    _, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 24))\n    for rank, movies_percentage in enumerate(percentege_of_movie_with_word[:number_of_top_words_to_check]):\n        ax1.scatter(rank + 1, movies_percentage, s=200*len(percentege_of_movie_with_word.index[rank]), \n                    marker=r\"$ {} $\".format(percentege_of_movie_with_word.index[rank]))\n    ax1.set_xlabel('Rank', fontsize=14)\n    ax1.set_ylabel('Percentage of movies [%]', fontsize=14)\n    ax1.set_title(f'{number_of_top_words_to_check} the most frequent words in {col}', fontsize=19)\n\n    for percentile in np.arange(0, 100, 0.1):\n        ax2.scatter(percentile, percentege_of_movie_with_word[int(round(len(percentege_of_movie_with_word)*percentile\/100))])\n    ax2.set_xlabel('Rank percentile', fontsize=14)\n    ax2.set_ylabel('Percentage of movies [%]', fontsize=14)\n    ax2.set_title(f'Words in {col}', fontsize=19)\n    \ndef add_word_columns(data, col, percentege_of_movie_with_word_lb=0.01, percentege_of_movie_with_word_ub=0.4):\n    percentege_of_movie_with_word = get_percentege_of_movies_with_word(col)\n    words_to_use = set(percentege_of_movie_with_word.index[(percentege_of_movie_with_word > percentege_of_movie_with_word_lb) & \n                                                         (percentege_of_movie_with_word < percentege_of_movie_with_word_ub)])\n    listwords_series = get_listofwords_series(col).map(lambda x: [word for word in x if word in words_to_use])\n    word_columns_count = int(listwords_series.map(len).quantile(0.95))\n    listwords_series = listwords_series.map(lambda x: x[:word_columns_count] if len(x) >= word_columns_count else x + np.full(word_columns_count - len(x), np.nan).tolist())\n    added_columns = pd.DataFrame(np.column_stack(listwords_series).T, columns=[f'{col}_word_{i}' for i in range(1, word_columns_count + 1)], index=data.index)\n    added_columns[added_columns == 'nan'] = np.nan # as we used a numpy for the intermediate step\n    data = pd.concat((data, added_columns), axis=1)\n    return data","9ba72cdd":"%%HTML\n<a id='title'><\/a>","817b9ba3":"visualize_rank_freq('title')","3da3e32b":"%%HTML\n<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/_wYtG7aQTHA?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe>","83578deb":"visualize_rank_freq('overview', number_of_top_words_to_check=40)","4dfa43d1":"data = add_word_columns(data, 'overview')","b1471e66":"visualize_rank_freq('tagline')","7279d40b":"%%HTML\n<img src=https:\/\/mindblown.smumn.edu\/wp-content\/uploads\/2018\/02\/zipf-law-meme.jpg width=\"200\">","b92bf7bc":"data = add_word_columns(data, 'tagline')","a7e32817":"data.drop(['title', 'original_title', 'overview', 'tagline'], axis=1, inplace=True)","35178eca":"def compute_stats(val_date, competition_values_all):\n    movie_val = val_date[0]\n    movie_competition_groupy_id_date = val_date[1]\n    if np.isnan(movie_val) or pd.isnull(movie_competition_groupy_id_date):\n        return [np.nan] * 6\n    competition = competition_values_all[movie_competition_groupy_id_date]\n    zscore = (movie_val - np.nanmean(competition))\/np.nanstd(competition)\n    percentile = stats.percentileofscore(competition, movie_val)\n    val_to_max = movie_val \/ np.nanmax(competition)\n    min_to_val = np.nanmin(competition) \/ movie_val  \n    val_to_mean = movie_val \/ np.nanmean(competition)\n    val_to_median = movie_val \/ np.nanmedian(competition)\n    return [zscore, percentile, val_to_max, min_to_val, val_to_mean, val_to_median]\n\ndef add_comparison_stats(data, cols, neighbourhood_in_weeks=1):\n    \n    def get_next_sunday_date(date):\n        # as we use closed='right', group release date is the first Sunday after the period\n        return date + pd.to_timedelta(6 - date.weekday() % 7, unit='D')\n    \n    time_range_groups = None\n    dummy_release_dates = []\n    for i in range(2*neighbourhood_in_weeks):\n        # we add a dummy row with all nans to manipulate resample-like functionality of pandas.Grouper so that groups for each week are introduced. Dummy nans are filtered during aggregation\n        ts_dummy = data.loc[~data['release_date'].isin(dummy_release_dates), 'release_date'].min() - pd.to_timedelta(i * 1, unit='W')\n        dummy_row = pd.DataFrame([np.full_like(data.iloc[0], fill_value=np.nan)], columns=data.columns)\n        dummy_row['release_date'] = ts_dummy\n        if i > 0: # when i == 0, ts_dummy is equal to the earliest actual timestamp\n            dummy_release_dates.append(ts_dummy)\n        data = pd.concat((data, dummy_row))\n        groups_next = (data[['release_date'] + cols]\n                       .groupby(pd.Grouper(key='release_date', freq=f'{2*neighbourhood_in_weeks}W', closed='right'))\n                       .agg(lambda values: list(filter(lambda val: not np.isnan(val), values))))\n        time_range_groups = groups_next if time_range_groups is None else pd.concat((time_range_groups, groups_next))\n\n    for col in cols:   \n        competition_values = time_range_groups[col]\n        # computing date identifying the competition time range for particular movie, and, again, avoiding the slow apply by using map on a series of two-element list\n        val_date_series = data[col].map(lambda x: [x]) + (data['release_date']\n                                                          .map(lambda x: x + pd.to_timedelta(neighbourhood_in_weeks, unit='W'))\n                                                          .map(get_next_sunday_date)\n                                                          .map(lambda x: [x]))\n        column_names = [f\"{col}_competition_{stat}_\u00b1_{neighbourhood_in_weeks}_week{'s' if neighbourhood_in_weeks > 1 else ''}\" for stat in ['zscore', 'percentile', 'val_to_max', 'min_to_val', 'val_to_mean', 'val_to_median']]\n        df_added_cols = pd.DataFrame(np.column_stack(val_date_series.map(lambda val_date: compute_stats(val_date, competition_values))).T, columns=column_names, index=data.index)\n        data = pd.concat((data, df_added_cols), axis=1)\n    return data","91dd9d47":"data = add_comparison_stats(data, cols=['rating', 'popularity_mean'])\n# data = add_comparison_stats(data, cols=['rating', 'popularity_mean'], neighbourhood_in_weeks=2)\n# data = add_comparison_stats(data, cols=['rating', 'popularity_mean'], neighbourhood_in_weeks=3)","43d95e71":"print(f\"Each movie belongs to {data['belongs_to_collection'].map(lambda x: 0 if not isinstance(x, list) else len(x)).max()} collection(s) at most.\")","4cae82bd":"data.drop(['belongs_to_collection', 'popularity', 'popularity2', 'popularity3'], axis=1, inplace=True)\ndata['collection_id'] = data['collection_id'].astype('object')","cc776b3a":"data.drop(['homepage', 'id', 'imdb_id', 'poster_path', 'release_date', 'budget_cat'], axis=1, inplace=True)","326f659b":"%%HTML\n<a id=libffm><\/a>","af0b6e4b":"%%HTML\n<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/1cRGpDXTJC8?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe>","0383ad16":"categorical_columns = []\nnumerical_columns = []\nfor col, col_type in zip(data.columns, data.dtypes):\n    if col == 'dataset':\n        continue\n    if col_type==np.dtype('O') or col in final_dummy_cols:\n        categorical_columns.append(col)\n    else:\n        numerical_columns.append(col)","91cd37c7":"flag_cols = ['has_homepage',\n             'isbelongs_to_collectionNA',\n             'isTaglineNA',\n             'isOriginalLanguageEng',\n             'isTitleDifferent',\n             'isMovieReleased']\nnumerical_columns = [col for col in numerical_columns if col not in flag_cols]\ncategorical_columns.extend(flag_cols)\nnumerical_columns.remove('revenue')","4ab38367":"data_ffm = deepcopy(data)\nfor col in numerical_columns:\n    if col != 'release_year':\n        # + 1 so that in all columns missing value corresponds to 0, \n        # reminder: libffm requires non-negative integers and having non-negative ints here would help us during encoding into libffm format\n        data_ffm[col] = pd.qcut(data_ffm[col], q=200, duplicates='drop', labels=False).factorize()[0] + 1 \nfor col in categorical_columns:\n    data_ffm[col] = data_ffm[col].factorize()[0] + 1\n\n# there is no need to lear embeddings for values not present in the test\n# and as for the test set, value not encounted during training is equivalent to missing\ntrain_logical_indexing = data_ffm['dataset']=='Train'\ntest_logical_indexing = data_ffm['dataset']=='Test'\nfor col in [num_col for num_col in numerical_columns if num_col != 'release_year'] + [cat_col for cat_col in categorical_columns if cat_col != 'dataset']:\n    train_values = set(data_ffm.loc[train_logical_indexing, col].values)\n    test_values = set(data_ffm.loc[test_logical_indexing, col].values)\n    values_to_preserve = train_values.intersection(test_values)\n    data_ffm.loc[~data_ffm[col].isin(values_to_preserve), col] = 0 # corresponds to missing values, see above","e6b279dc":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ncv_folds = [[trn_idx, val_idx] for trn_idx, val_idx in skf.split(list(range((data_ffm['dataset'] == 'Train').sum())), \n                                                                 data_ffm.loc[data_ffm['dataset'] == 'Train', 'release_year'])]\ndata_ffm['release_year'] = data_ffm['release_year'].factorize()[0]\ntrain_ffm = data_ffm[data_ffm['dataset'] == 'Train'].drop('dataset', axis=1)\ntest_ffm = data_ffm[data_ffm['dataset'] == 'Test'].drop('dataset', axis=1)","0a6ba02c":"accumulated_count = 0\ndict_feature = dict()\nfor col in numerical_columns + categorical_columns:\n    for val in data_ffm[col].unique():\n        dict_feature[(col, val)] = accumulated_count + val\n    accumulated_count += data_ffm[col].nunique()\n    \ndef produce_file(file_name, df_to_process, test=False):\n        noofrows = df_to_process.shape[0]\n        noofcolumns = df_to_process.shape[1]\n        currentcode = len(numerical_columns)\n        with open(file_name, 'w') as text_file:\n            for n, r in enumerate(tqdm(range(noofrows), desc=file_name)):\n                datastring = ''\n                datarow = df_to_process.iloc[r]\n                if not test:\n                    datastring += str(datarow['revenue'])\n                else:\n                    datastring += '10'\n\n                for col_i, col in enumerate(numerical_columns + categorical_columns):\n                    datastring = f'{datastring} {col_i}:{dict_feature[(col, datarow[col])]}:1'\n                datastring += '\\n'\n                text_file.write(datastring)\n                \ndef create_train_val_files(train_ffm, val_ffm, test_ffm=test_ffm, fold_idx=0, dict_feature=dict_feature):   \n    produce_file(f'train_ffm_{fold_idx}', train_ffm)                \n    produce_file(f'val_ffm_{fold_idx}', val_ffm) \n\nproduce_file('test_ffm', test_ffm, test=True)","f9e8931a":"# capturing subprocess output line by line: https:\/\/stackoverflow.com\/questions\/4417546\/constantly-print-subprocess-output-while-process-is-running\ndef execute_generator(cmd):\n    popen = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, universal_newlines=True)\n    for stdout_line in iter(popen.stdout.readline, \"\"):\n        yield stdout_line \n    popen.stdout.close()\n    return_code = popen.wait()\n    if return_code:\n        raise subprocess.CalledProcessError(return_code, cmd)\n        \ndef execute(cmd, parse_val_result=False):\n    # for local cross validation we would like to parse validation score\n    all_lines = []\n    for path in execute_generator(cmd):\n        print(path, end=\"\")\n        all_lines.append(path)\n    if parse_val_result:\n        words_in_lines = [line.split() for line in all_lines]\n        the_last_row_to_use = False\n        try:\n            val_score = float(words_in_lines[-1][-1])\n            the_last_row_to_use = True\n        except ValueError: pass\n        if the_last_row_to_use:\n            return val_score\n        # otherwise, the last line must be something like Auto-stop. Use model at 7th iteration. So we need the 3rd line from the end\n        return float(words_in_lines[-3][-1])       \n        \nexecute('git clone https:\/\/github.com\/bobye\/libffm-regression')\nwith open('libffm-regression\/ffm-predict.cpp', 'r') as file:\n    prediction_code = file.readlines()\n# I had problems with ffm-predict command: it was outputting several GB of floats on each run. This modification helped\nprediction_code[84] = '        ffm_float t = ffm_predict(x.data(), x.data()+x.size(), model, false);\\n'\nwith open('libffm-regression\/ffm-predict.cpp', 'w') as file:\n    file.writelines(prediction_code)\nos.chdir('libffm-regression')\nexecute('make')\nos.chdir('..')","cfa954b3":"submission_ffm = pd.read_csv('..\/input\/tmdb-box-office-prediction\/sample_submission.csv')\nsubmission_ffm['revenue'] = 0\ncv_rmsle_scores = []\nfor fold_idx, (trn_idx, val_idx) in enumerate(cv_folds):\n    print('='*40)    \n    print('='*16 + f' FOLD {fold_idx} ' + '='*16)\n    train_ffm_trn = train_ffm.iloc[trn_idx]\n    train_ffm_val = train_ffm.iloc[val_idx]\n    create_train_val_files(train_ffm_trn, train_ffm_val, fold_idx=fold_idx)\n    val_score = execute(f'libffm-regression\/ffm-train -p val_ffm_{fold_idx} --auto-stop -t 1 train_ffm_{fold_idx} model_{fold_idx}', parse_val_result=True)\n    print(f'Val score: {val_score}')\n    cv_rmsle_scores.append(val_score)\n    os.remove(f'val_ffm_{fold_idx}')\n    os.remove(f'train_ffm_{fold_idx}')\n    execute(f'libffm-regression\/ffm-predict test_ffm model_{fold_idx} output_{fold_idx} > \/dev\/null')\n    predictions_current = pd.read_csv(f'output_{fold_idx}',header=None)\n    os.remove(f'output_{fold_idx}')\n    os.remove(f'model_{fold_idx}')\n    predictions_current.columns = ['revenue']\n    submission_ffm['revenue'] += predictions_current['revenue']\/len(cv_folds)\n\n# cleaning up\nrmtree('libffm-regression')\nos.remove('test_ffm')\n\n# performance estimation\nrmsle_mean = np.mean(cv_rmsle_scores)\nrmsle_mean_std = np.std(cv_rmsle_scores, ddof=1)\/np.sqrt(len(cv_rmsle_scores))\ntinteval_lb, tinterval_ub = _tconfint_generic(rmsle_mean, rmsle_mean_std, dof=len(cv_rmsle_scores)-1, alpha=0.05, alternative='two-sided')\nprint(f\"95% confidence interval for RMSLE is: ({tinteval_lb:.2f}, {tinterval_ub:.2f}).\")\n\n# transforming revenues back to $\nsubmission_ffm['revenue'] = submission_ffm['revenue'].map(np.expm1)\n# submission_ffm.to_csv('submission_ffm.csv', index=None)","38a22be7":"%%HTML\n<a id=lgb><\/a>","c7bbfe6c":"%%HTML\n<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/mg7BNAvHeII?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe>","b56d88a5":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\ntrain_data = data[data['dataset'] == 'Train']\ny_train = train_data['revenue']\ncv_folds = [[trn_idx, val_idx] for trn_idx, val_idx in skf.split(list(range(len(train_data))), train_data['release_year'])]\n\ndef hide_rare_vals(data, categorical_columns, min_count=2):\n    for col in categorical_columns:\n        data['count'] = np.arange(len(data))\n        value_counts = data.groupby(col, as_index=False)['count'].count()\n        values_to_discard = set(value_counts.loc[value_counts['count'] < min_count, col].values)\n        data.loc[data[col].isin(values_to_discard), col] = np.nan\n        data.drop('count', axis=1, inplace=True)\n    return data\n\ndef get_train_lgb(data):\n    categorical_columns = [col for col, col_type in zip(data.columns, data.dtypes) if col_type==np.dtype('O') and col != 'dataset']\n    data_for_boosting = hide_rare_vals(data, \n                                       [col for col in categorical_columns if 'word' in col], \n                                       min_count=10)\n\n    data_one_hot_enc = pd.get_dummies(data_for_boosting.drop(['revenue'], axis=1),\n                                      columns=categorical_columns)\n    train_one_hot_enc = data_one_hot_enc[data_one_hot_enc['dataset'] == 'Train'].drop(['dataset'], axis=1)\n    return train_one_hot_enc, data_one_hot_enc\n\ntrain_one_hot_enc, data_one_hot_enc = get_train_lgb(data)","468d102e":"counter = 0\ndef onstep(res):\n    global counter\n    x0 = res.x_iters   # List of input points\n    y0 = res.func_vals # Evaluation of input points\n    print('Current iter: ', counter, \n          ' - Score ', res.fun, \n          ' - Args: ', res.x)\n    joblib.dump((x0, y0), 'checkpoint.pkl') # Saving a checkpoint to disk\n    counter += 1\n              \n# The objective function to be minimized\ndef make_objective(model, dimensions, df=data):\n    model = deepcopy(model)\n\n    # This decorator converts your objective function with named arguments into one that\n    # accepts a list as argument, while doing the conversion automatically.\n    @use_named_args(dimensions) \n    def objective(**params):\n        model.set_params(**params)\n        score = 0\n        for trn_idx, val_idx in cv_folds:\n            X_fit, X_val = df.iloc[trn_idx], df.iloc[val_idx]\n            y_fit, y_val = y_train.iloc[trn_idx], y_train.iloc[val_idx]\n            model.fit(X_fit, y_fit,\n                      eval_set=[(X_val, y_val)],\n                      verbose=False, early_stopping_rounds=30)\n            score += np.sqrt(mean_squared_error(y_val, model.predict(X_val)))\/len(cv_folds)              \n        return score\n    \n    return objective","31f3c2c5":"# result of the offline optimization\nskopt_params = {'learning_rate': 0.005, 'num_leaves': 2558, 'max_depth': 0, 'min_child_samples': 15, 'max_bin': 95637, \n                'subsample': 0.4462883478742228, 'subsample_freq': 0, 'colsample_bytree': 0.12636976300879604, 'min_child_weight': 1, 'subsample_for_bin': 422967, 'reg_lambda': 8.991454050632966e-05, 'reg_alpha': 0.000956883899855844, 'scale_pos_weight': 0.000511057873147196}\n\ndef get_skopt_params_lgb(train_one_hot_enc):\n    clf = LGBMRegressor(boosting_type='gbdt',\n                        objective='rmse',\n                        n_jobs=-1,\n                        verbose=0,\n                        n_estimators=10000)\n\n    dimensions = [Real(0.01, 1.0, 'log-uniform', name='learning_rate'),\n                  Integer(50, 1000, name='num_leaves'),\n                  Integer(0, 500, name='max_depth'),\n                  Integer(0, 200, name='min_child_samples'),\n                  Integer(100, 100000, name='max_bin'),\n                  Real(0.01, 1.0, 'uniform', name='subsample'),\n                  Integer(0, 10, name='subsample_freq'),\n                  Real(0.01, 1.0, 'uniform', name='colsample_bytree'),\n                  Integer(0, 10, name='min_child_weight'),\n                  Integer(100000, 500000, name='subsample_for_bin'),\n                  Real(1e-9, 1000, 'log-uniform', name='reg_lambda'),\n                  Real(1e-9, 1.0, 'log-uniform', name='reg_alpha'),\n                  Real(1e-6, 500, 'log-uniform', name='scale_pos_weight')]\n\n    objective = make_objective(clf, dimensions, train_one_hot_enc)\n\n    gp_round = gp_minimize(func=objective,\n                           dimensions=dimensions,\n                           acq_func='gp_hedge', \n                           n_calls=25,\n                           callback=[onstep],\n                           random_state=42)\n    best_parameters = gp_round.x\n    best_result = gp_round.fun\n    skopt_params = {param.name: value for param, value in zip(dimensions, best_parameters)}\n    print(f'Best RMSLE: {best_result},\\ncorresponding parameters: {skopt_params}')\n    return skopt_params\n\nif skopt_params is None:\n    skopt_params = get_skopt_params_lgb(train_one_hot_enc)","c7c16d91":"def train_lgb(train_one_hot_enc):\n    clfs = []\n    cv_rmsle_scores = []\n    importances = pd.DataFrame()\n    for fold_idx, (trn_idx, val_idx) in enumerate(cv_folds):\n        clf = LGBMRegressor(boosting_type='gbdt',\n                            objective='rmse',\n                            num_leaves=5000,\n                            n_jobs=-1,\n                            verbose=1,\n                            n_estimators=15000, \n                            min_data=50)\n        clf.set_params(**skopt_params)\n\n        X_fit, X_val = train_one_hot_enc.iloc[trn_idx], train_one_hot_enc.iloc[val_idx]\n        y_fit, y_val = y_train.iloc[trn_idx], y_train.iloc[val_idx]\n\n        clf.fit(X_fit, y_fit, eval_metric='rmse', \n                      eval_set=[(X_val, y_val)], \n                      verbose=500, early_stopping_rounds=300)\n\n        imp_df = pd.DataFrame({\n                    'feature': X_fit.columns,\n                    'gain': clf.feature_importances_,\n                    'fold': [fold_idx + 1] * len(X_fit.columns),\n                    })\n        importances = pd.concat([importances, imp_df], axis=0, sort=False)\n        clfs.append(clf)\n        cv_rmsle_scores.append(np.sqrt(mean_squared_error(y_val, clf.predict(X_val))))\n\n    # performance estimation\n    rmsle_mean = np.mean(cv_rmsle_scores)\n    rmsle_mean_std = np.std(cv_rmsle_scores, ddof=1)\/np.sqrt(len(cv_rmsle_scores))\n    tinteval_lb, tinterval_ub = _tconfint_generic(rmsle_mean, rmsle_mean_std, dof=len(cv_rmsle_scores)-1, alpha=0.05, alternative='two-sided')\n    print(f\"95% confidence interval for RMSLE is: ({tinteval_lb:.2f}, {tinterval_ub:.2f}).\")\n    return clfs, importances\n\nclfs, importances = train_lgb(train_one_hot_enc)","5f3aa056":"plt.figure(figsize=(20,20))\nstds = importances.groupby('feature')['gain'].std()\nmean_importances = importances.groupby('feature')['gain'].mean().sort_values(ascending=False)[:50][::-1]\nplt.barh(y=mean_importances.index, width=mean_importances, xerr=mean_importances.index.map(stds))\nplt.title('LightGBM Feature Importances')","4d4ffb74":"%%HTML\n<a id=catboost><\/a>","66abbf06":"def get_data_cat(data):\n    data_catboost = data.drop('revenue', axis=1)\n    train_logical_idx = data_catboost['dataset'] == 'Train'\n    test_logical_idx = data_catboost['dataset'] == 'Test'\n    data_catboost.drop('dataset', axis=1, inplace=True)\n    init_cat_features = {'has_homepage',\n     'isbelongs_to_collectionNA',\n     'isTaglineNA',\n     'isOriginalLanguageEng',\n     'isTitleDifferent',\n     'isMovieReleased'}\n    cat_features=[i for i, (col, col_type) in enumerate(zip(data_catboost.columns, data_catboost.dtypes))\n                  if col_type==np.dtype('O') or col in init_cat_features]\n    for col in cat_features:\n        data_catboost.iloc[:, col] = data_catboost.iloc[:, col].map(str)\n    train_cat = data_catboost[train_logical_idx]\n    test_cat = data_catboost[test_logical_idx]\n    return train_cat, test_cat, cat_features\n\ntrain_data_cat, test_data_cat, cat_features = get_data_cat(data)\n\n\nskopt_params_catboost = {'learning_rate': 0.02, 'depth': 4, 'random_strength': 2.6832935037024834e-08, 'bagging_temperature': 0.650888472948853, 'border_count': 15, 'l2_leaf_reg': 22} #no hyperopt\n\ndef get_skopt_params_cat(train_data_cat):    \n    clf = CatBoostRegressor(loss_function='RMSE', \n                        verbose=False,\n                        cat_features=cat_features,\n                        thread_count=12)\n    dimensions = [Real(0.01, 1.0, 'log-uniform', name='learning_rate'),\n                  Integer(1, 10, name='depth'),\n                  Real(1e-9, 10, 'log-uniform', name='random_strength'),\n                  Real(0.0, 1.0, name='bagging_temperature'),\n                  Integer(1, 255, name='border_count'),\n                  Integer(2, 30, name='l2_leaf_reg')]\n\n    objective = make_objective(clf, dimensions, train_data_cat)\n    gp_round = gp_minimize(func=objective,\n                           dimensions=dimensions,\n                           acq_func='gp_hedge', \n                           n_calls=15,\n                           callback=[onstep],\n                           random_state=42)\n    best_parameters = gp_round.x\n    best_result = gp_round.fun\n    skopt_params_catboost = {param.name: value for param, value in zip(dimensions, best_parameters)}\n    print(f'Best RMSLE: {best_result},\\ncorresponding parameters: {skopt_params_catboost}')\n    return skopt_params_catboost\n\nif skopt_params_catboost is None:\n    skopt_params_catboost = get_skopt_params_cat(train_data_cat)","3cbe570a":"def train_catboost(train_data_cat, cat_features):\n    clfs_cat = []\n    cv_rmsle_scores = []\n    importances_cat = pd.DataFrame()\n    for fold_idx, (trn_idx, val_idx) in enumerate(cv_folds):\n        clf = CatBoostRegressor(loss_function='RMSE', \n                                verbose=False,\n                                cat_features=cat_features,\n                                thread_count=12,\n                                iterations=15000)\n        clf.set_params(**skopt_params_catboost)\n\n        X_fit, X_val = train_data_cat.iloc[trn_idx], train_data_cat.iloc[val_idx]\n        y_fit, y_val = y_train.iloc[trn_idx], y_train.iloc[val_idx]\n\n        clf.fit(X_fit, y_fit,\n                      eval_set=[(X_fit, y_fit), (X_val, y_val)], \n                      verbose=100, early_stopping_rounds=50)\n        imp_df_cat = pd.DataFrame({\n                    'feature': X_fit.columns,\n                    'gain': clf.feature_importances_,\n                    'fold': [fold_idx + 1] * len(X_fit.columns),\n                    })\n        importances_cat = pd.concat([importances_cat, imp_df_cat], axis=0, sort=False)\n        clfs_cat.append(clf)\n        cv_rmsle_scores.append(np.sqrt(mean_squared_error(y_val, clf.predict(X_val))))\n\n    # performance estimation\n    rmsle_mean = np.mean(cv_rmsle_scores)\n    rmsle_mean_std = np.std(cv_rmsle_scores, ddof=1)\/np.sqrt(len(cv_rmsle_scores))\n    tinteval_lb, tinterval_ub = _tconfint_generic(rmsle_mean, rmsle_mean_std, dof=len(cv_rmsle_scores)-1, alpha=0.05, alternative='two-sided')\n    print(f\"95% confidence interval for RMSLE is: ({tinteval_lb:.2f}, {tinterval_ub:.2f}).\")\n    return clfs_cat, importances_cat\n\nclfs_cat, importances_cat = train_catboost(train_data_cat, cat_features)","7e625332":"plt.figure(figsize=(20,20))\nstds = importances_cat.groupby('feature')['gain'].std()\nmean_importances_cat = importances_cat.groupby('feature')['gain'].mean().sort_values(ascending=False)[:50][::-1]\nplt.barh(y=mean_importances_cat.index, width=mean_importances_cat, xerr=mean_importances_cat.index.map(stds))\nplt.title('CatBoost Feature Importances')","ddd500dd":"test_one_hot_enc = data_one_hot_enc[data_one_hot_enc['dataset'] == 'Test'].drop(['dataset'], axis=1)\nsubmission_lgb = pd.read_csv('..\/input\/tmdb-box-office-prediction\/sample_submission.csv')\nsubmission_lgb['revenue'] = 0\nfor clf in clfs:\n    submission_lgb['revenue'] += clf.predict(test_one_hot_enc)\/len(clfs)\n    \n    \nsubmission_cat = pd.read_csv('..\/input\/tmdb-box-office-prediction\/sample_submission.csv')\nsubmission_cat['revenue'] = 0\nfor clf in clfs_cat:\n    submission_cat['revenue'] += clf.predict(test_data_cat)\/len(clfs_cat)\n\ncat_weight = 3\nlgb_weight = 1\n\nsubmission_lgb_cat = pd.read_csv('..\/input\/tmdb-box-office-prediction\/sample_submission.csv')\nsubmission_lgb_cat['revenue'] = (cat_weight*submission_cat['revenue'] + lgb_weight*submission_lgb['revenue'])\/(cat_weight + lgb_weight)\nsubmission_lgb_cat['revenue'] = submission_lgb_cat['revenue'].map(np.expm1)\nsubmission_lgb_cat.to_csv('submission_lgb_cat.csv', index=None)","8a71d502":"#### Tagline\/Cast\nLet's check why tagline and cast count helps distunguish between train and test.","437b5b69":"# Feature engineering","68a9fa36":"So, we can see that the film industry also produces more and more movies with good rankings.\n\nThanks to my brother\u2019s advice, I scanned through [introduction to David Hesmondhalgh's book \"The Cultural Industries\"](https:\/\/www.researchgate.net\/publication\/261554803_The_Cultural_Industries_3rd_Ed). It cites the study by Bettig from 1996 showing that less than 3% of released movies are going to become box office hits. As a result, Hesmondhalgh argures that *\"in the cultural industries, companies tend to offset misses against hits by means of \u2018overproduction\u2019 (Hirsch, 1990), attempting to put together a large catalogue or \u2018cultural repertoire\u2019 (Garnham, 1990) or, to put it another way, \u2018throwing mud\u2019 \u2013 or other similar substances \u2013 \u2018against the wall\u2019 to see what sticks (Laing, 1985: 9; Negus, 1999: 34).\"*\n\nThese tendencies might explain what we observe: the companies tend to produce more and more movies betting on success of just a few hits which should cover all the costs. Good news is that in the figure above we indeed see more and more successful movies with good rankings. Yet, as we've seen and as Hesmondhalgh argues, to get there, companies might need to produce more and more not so successful movies.\n\nTo sum up, I believe we should not be afraid of the film-industry apocalypse \"estimated\" before, and future generations would hopefully have enough great movies :)\n\n______________\n*Please note that I tried to remove the ranking outlier movies as well as to hide zeros and tens under NaNs. Neither improved the score. In our case having more data seem to help the model even if those additional data have distributions of parameters slightly away from the corresponding test distributions. Yet, dealing with the outliers might require more thorough investigation, I performed just a very quick check.*\n\nNow, let us check that train\/val split stratified by release year lead to similar distributions of rating.","524fe80d":"Interestingly, in the added movies there are more short overviews. ","751a3489":"## Budget analysis\nLet us now validate stratification by release year. We would check other features relevant for the prediction. For instance, the organizers' train\/test split cound have been stratified by pair of release year and budget values and stratification by the year only might lead to different budget distributions compared to train vs. test pair. \n\nTo compare train and test distributions of budget, let's reuse the function above.","7de31760":"Let us estimate how many persons might be on a particular position in the same movie.","1d55e36f":"Let's try to fill in the missing values by querying TMDB.\n\n## Filling in missing values using TMDB API\nIn order not to disclose my API key and to save some time, I have uploaded the enriched dataset","266cae07":"### Command-line installation & training\nInstalling [a branch of LIBFFM for regression problems](https:\/\/github.com\/bobye\/libffm-regression).","6d98f372":"Random check of movies with still missing budget confirmed that the budget was known neither on TMDB, nor on IMDB, nor on [boxofficemojo](https:\/\/www.boxofficemojo.com), nor on [www.the-numbers.com](www.the-numbers.com), nor on [kinopoisk](https:\/\/www.kinopoisk.ru).\n\nReasons:\n\"The info on movie budgets is even harder to find: studios are usually very reticent when it comes to discussing how much a film cost, especially when a movie performed poorly at the box-office. Additionally, reported budgets may change over time due to escalating costs (Waterworld and Titanic are two high profile examples) or costs may be difficult to calculate (the salary for a star or director may be part of a sum agreed for a package deal consisting of several films).\" ([source IMDB page](https:\/\/help.imdb.com\/article\/imdb\/discover-watch\/why-are-your-budget-gross-figures-for-some-movies-different-than-those-listed-by-another-source\/GCS37LSKVDMU37LV?ref_=helpart_nav_16#))","edd74c45":"## Overview and tagline analysis","23b6d55c":"## Final Predictions\n### Weighted sum of LightGBM and CatBoost outputs","a7ccd2dd":"### Data Preparation\nLet us inspire with [libffm kernel](https:\/\/www.kaggle.com\/scirpus\/libffm-generator-lb-280) by [Scirpus](https:\/\/www.kaggle.com\/scirpus). Things we are going to do differently:\n* quatile-based discretization of numerical features,\n* we would additionally discard only-train\/only-test values,\n* we would write custom conversion to libffm data format based on the library documentation.","78be815d":"![](https:\/\/media.giphy.com\/media\/Pi2TJNJKu615m\/giphy.gif)\n\n# TL;DR\n## Cherry-picked Insights \nIn case you might be interested in clues to some of the preselected questions, please feel free to follow the corresponding link and start reading from the anchor section.\n\n1. [Let's make missing values NaNs again!](#missing-zeros)\n1. [Low-budget movies: how many are there? Are there more or less of them recently?](#low-budget)\n\n2. [Low-ranking movies: Are there more or less of them recently?](#low-rating)  \n2. [Should we be afraid of film industry apocalypse? ;)](#apocalypse)\n\n3. [Given that brevity is the sister of talent, do more or less people with talent write movie overviews?](#overview-length) [And taglines?](#tagline-length)\n4. [Can revenues reveal which decades were really the greatest in cinema?](#revenue-inflation)\n5. [Are there more sequels or movies with word \"love\" in a title? :)](#title)\n\n## Modeling\n1. [Field-Aware Factorization Machines: how to make the kaggle-champ library libffm up and running, how to prepare data in the required format, how to train models using 5-fold stratified CV and early stopping, calling all command-line functions from Python](#libffm)\n1. [LightGBM, 5-fold stratified CV, Bayesian hyperparameters search](#lgb)\n1. [CatBoost, 5-fold stratified CV, Bayesian hyperparameters search](#catboost)\n\n\n# Kernel overview\n1. To have reliable local validation, it is important to mimic the train\/test split. So, let us focus on train vs. test EDA first. Based on the EDA findings we would clean data, including filling of missing entries by querying TMDB API. Along the way we would explore potential curiosities about film industry. As a helping step, we would perform adversarial validation.\n    * [EDA and data cleaning](#EDA-and-data-cleaning)\n        * [Was train\/test split time-based?](#Was-the-organizers'-train\/test-split-time-based?)\n        * [Budget analysis](#Budget-analysis) \n        * [Rating analysis](#Rating-analysis)\n        * [Adversarial validation](#Adversarial-validation)\n        * [Popularity Analysis](#Popularity-Analysis)\n        * [Overview and tagline analysis](#Overview-and-tagline-analysis)\n        * [Filling in missing values using TMDB API](#Filling-in-missing-values-using-TMDB-API)\n3. Next, we would perform some feature engineering. \n    * [Feature engineering](#Feature-engineering)\n        * [Revenue and inflation](#Revenue-and-inflation)\n        * [Sparse categorical features based on columns like crew and cast](#Sparse-categorical-features-based-on-columns-like-crew-and-cast)\n        * [Text-based features](#Text-based-features)\n        * [Features based on competition with other movies](#Features-based-on-competition-with-other-movies)\n4. The task at hand has quite a few categorical features with high cardinality. Factorization machines have proven itself useful in problems with huge sparsity. We would use [libffm library](https:\/\/github.com\/guestwalk\/libffm) to create a field-aware factorization machine baseline. *FFM turned out not to work well enough, please see previous kernel versions for full training. On the bright side we learned how to work with the library which won a couple of competitions.*\n    * [Modeling](#Modeling)\n        * [Field-Aware Factorization Machines](#Field-Aware-Factorization-Machines)\n5. Create gradient boosting baselines (LightGBM, CatBoost) to predict movie box office. We would tune hyperparameters of the gradient boosting models using Bayesian optimization.\n    * [Modeling](#Modeling)\n        * [LightGBM](#LightGBM)\n        * [CatBoost](#CatBoost)\n        * [Final Predictions](#Final-Predictions)","252afad5":"# Modeling\n\n## Field-Aware Factorization Machines \n*(not promising performance for the task, please check out first kernel versions for full training logs)*\n\nPerhaps, someone might find useful an introductory video by [Yuchin Juan](https:\/\/www.kaggle.com\/guestwalk), one of the authors.","a59325b6":"If we don't do anything about it now, then by year 2030 more than one fifth of all movies might be B-movies and by year 2060 we might end up with B-movies only. The apocalypse of the film-industry we love! ;)\n\nSpeaking seriously, let us check how many movies with good ranking were produced in recent years.","3457c3bb":"Now, let us transform revenues to log scale.","f2464781":"### Quick check of 2018 data","e1cb3fa9":"We have discovered that there are duplicates in the used dataset, likely due to the additionally added movies in the previously published kernels. Let us clean it up.","9f44dbd1":"Converting data to libffm format as specified in [libffm README](https:\/\/github.com\/guestwalk\/libffm\/blob\/master\/README).","500ce255":"We can see that in the train set there tend to be more movies with lower budget. \n\nFirst, let's understand why there are so many low-budget movies in both train and test.","010908b6":"It seems like an opportunity for an extrapolation joke!\n![](https:\/\/imgs.xkcd.com\/comics\/extrapolating.png)","376cd89a":"### Title\n\n\"[Zipf's law](https:\/\/en.wikipedia.org\/wiki\/Zipf%27s_law) states that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table.\" Let us see it ourselves.","b0b84110":"Training with early stopping, then predicting on test.","0f613f00":"Fun isn't something one often considers when cleaning the data. But this... does put a smile on my face.","c8646908":"Again, we observe analogous trend to the one in overview length. Seemingly, we tend to write shorter. Hopefully, it's the sign that we as the humanity are evolving. Let's stay optimistic.","d37eb4db":"*for each position there are only 5% of movies with higher headcounts*\n\n\nLet us now engeer features. ","7763593b":"If we check [the highest movie revenues per years](https:\/\/en.wikipedia.org\/wiki\/List_of_highest-grossing_films#High-grossing_films_by_year) we can see that the Infinity War movie is not an isolated outlier and in test data there might be even higher revenues. I would suggest not to do anything about the picture.\n\nLet us now adjust movie revenues for inflaction and check an adjusted mean revenue graph.","19c6b1f9":"This checks out: the movies have rating 0 on TMDB. \n\nTo sum up, the initial training data seemed to ignore significant number of B-movies. But there are quite some B-movies recently, if we look at their proportion out of the total count.","1932b5ea":"There are more sequels than movies with word \"love\" in the title :) It reminds me of a line from the funny [Epic Rap Battle between movie directors](https:\/\/www.youtube.com\/watch?v=_wYtG7aQTHA): *\"Got no time to read reviews while I'm working on the sequel!\"*","e84d4487":"Finally, let us check if stratification by release year would lead to train\/val split mimicking train\/test split. ","21f9046b":"Some numerical columns are categorical flags.","605560b5":"#### Low-rating movies","bb42daca":"Please note a movie with a budget of $35 and high revenue. Let's take a closer look.","86c6728b":"To conclude, initially there were no differences between train\/test. The observed peculiarity in train vs. test comparison can be explained by additional data. We have also cleaned the data slightly.\n\nNext, let's compare the resulted distributions.","3ed67d28":"We have previously verified that stratification by year leads to reasonable mimicking of the train\/test split. Let's do it.","953b3f70":"We can see that there are almost no low-ranking movies in the test set. Is it true just for the test set or for the whole initial dataset?","74ade764":"# EDA and data cleaning\n\n## Train vs. test\nFor production ML systems it's crucial to know how your production data would look like and mimic it by your validation data. So this way your validation score would tell you more about performance of your system \"in the wild\", i.e. after the deployment. Analogously, for ML competition we should understand train\/test split and mimic it in our validation scheme (train\/val split). So that if we improve our local validation score it also improves our test score.\n### Loading data\nI'm going to reuse data from the [kernel by Kamal Chhirang (ver 103)](https:\/\/www.kaggle.com\/kamalchhirang\/eda-feature-engineering-lgb-xgb-cat?scriptVersionId=11689723). In order not to repeat the same content as in the Kamal's kernel, I stored the data preprocessed by [Kamal](https:\/\/www.kaggle.com\/kamalchhirang). There are insignificant differences in preprocessing, please check previous versions of this kernel for details.","2cbddf6a":"Discretization of numerical features. ","99271dcc":"So, in the initial dataset there were almost no low-ranking movies. Let's see some low-ranking movies which were added (to verify that they indeed have such a low ranking).","8c808a57":"I decided not to do it like in the code cell above. Motivation: both *Green Book* and comedies like *Dumb and Dumber To* would have Peter Farrelly as director 1. However, the solo *Green Book* is completely different from results of Peter Farrelly & Bobby Farrelly tandem. I might be \"overfitted\" based on a small number of cases, but I feel like a tandem of directors is a creative force different from each solo person. I would create concatenated entries.\n\nFor any job with ``percentile_95_job_headcount[job] > 2`` let us just do dummification with each person for the job resulting into a new dummy variable. Motivation to omit ordering from TMDB for high-cardinality positions: there are many actors in a movie. As a fan, I would like to see the movie due to particular actor. However, it doesn't really matter to me if the actor is listed as the first one on TMDB or as the 7th one. Again, this might be overfitted to my \"domain intuition\" and should be experimented with. ","e4dc1faf":"### Feature importance","b88df094":"### CatBoost-specific data preparation and Bayesian hyperparamater search","9b50d02f":"There is an apparent difference in the distributions for very small and very large ratings. Let's check years of release for the low-rating movies (as it corresponds to a significantly higher mode of the distribution).","8f7c3b79":"#### Final cleaning of columns","b94e041f":"There are quite some missing budget entries. Let us check if the movies added by Kaggle community have budget records from TMDB.","67e7ee65":"We can see just the slight difference around the lower mode of distribution. The main issue is that the movies added to train additionally seemingly don't contain ```popularity2```.\n\nI tried to understand what popularity represents and why there are two columns. When I queried TMDB API, I got the third value which equals neither to ```popularity```, nor to ```popularity2```. It turned out that the popularity is \"*really only used for boosting search results and trending results over time*\"([source](https:\/\/www.themoviedb.org\/talk\/5a42cf99c3a368586c04177a)). Based on [the documentation](https:\/\/developers.themoviedb.org\/3\/getting-started\/popularity) we can further confirm that the ```popularity``` is about recent activity on the movie's TMDB webpage. It might be interesting to know web activity just before the movie release. Yet, it turned out that the webpage popularity is helpful for the prediction. Let us get the most recent popularity us TMDB API. ","e35e76fa":"On the TMDB page of the movie we see that in fact the movies' budget was $35 000 000. It seems the Snow Dogs had enough money afterall :) ","d1a82f0a":"I'm not going to tune the number of estimators as a hyperparameter, rather I'll define early stopping inside the objective. Also, for each categorical column based on word let's leave only values which occur at least 10 times.\n### LightGBM-specific data preparation","ff2a77de":"Let us create features from raw columns ``['cast', 'Keywords', 'genres', 'production_companies', 'production_countries', 'spoken_languages']``. I would create a joint feature for genres. For the rest let us perform the dummification.","fe0f00ff":"From the Kamal's kernel we know that some movies were downloaded additionally. Let's check if the difference in the years 2017 and 2018 is given by the additional data.","2f49cb9b":"Indeed, the movies released in 2018 and late 2017 were added by our Kaggle community. \n\nTo conclude, we do not deal with time-based train\/test split. The split could have been just random. Also, it looks like train\/test split might have been stratified by release year.  ","50c7df87":"## LightGBM\n\nFor Bayesian optimization of hyperparameters we're going to reuse some pieces from the great workshop from Luca Massaron and Pietro Marinelli I was lucky to attend during the second edition of KaggleDays, January 2019. ","2b9447e8":"Let us remove rare words. First, we would visualize frequencies of words. ","41dde44b":"\n\n---------\nDue to the observed low percentages of movies with a particular word in the title, I would not introduce any features based on titles.\n\n### Overview and tagline","0abfdc35":"Gaining more and more confidence in stratification by release year.\n\n## Adversarial validation\n\nWe saw that some interesting aspects of our training data (such as zero and max-rating outliers) are not present in test data. In order to discover other potential peculiarities of our train data, let us perform adversarial validation and focus on features which discriminate train vs. test.","03da8ecd":"## Revenue and inflation\nLet us compare trend in movie revenues over years with a trend in inflation. For inflation adjustment we are going to use Consumer Price Index and [this library](https:\/\/github.com\/datadesk\/cpi). ","e3ef3789":"There is some high-revenue outlier released in 2018. Let's check the revenue champ.","885f9401":"Observations:\n1. If we would like to see which decade was the best for the film industry, then we must adjust revenues for inflation.\n2. In 2018 there is a jump in mean revenue.\n\nLet us add information about inflation to the dataset.","320abd2e":"## Sparse categorical features based on columns like crew and cast\nLet's derive crew-based features. First, let us select jobs documented for more than half of movies.","f11239a0":"### Bayesian hyperparamater search","0a2a042a":"### Training","52734ac8":"We have imputed quite some missing values, let's redo feature engineering from the previously available kernels if it was based on the imputed features.","743fa06f":"### Training","e143da92":"Let us get back to the low-rating movies which were added. \n\nWe can see from the figures above that in the initial train data there were no movies having ranking under 3 and released in 1999. Yet, in the final train data more than 10% of all movies have ranking this low. Let us now use year 1999 to check if the movies are indeed low-ranked.","6fe09b90":"I checked that *Johnny English Strikes Again* has known budget on TMDB. Let us try filling missing budget entries by querying TMDB API.","55cf1eed":"## Loading libraries","d2e06155":"Let us calculate an interval estimate for the model performance. Please note that in general there is a rule of thumb that one should not compute t-interval for sample mean unless there are at least 30 randomly picked samples. The reason for this rule of thumb is that due to central limit theorem, sample mean would be normally distributed for large enough sample no matter what population distribution was. And the commonly used threshold of 30 is an empirical way to check that the sample is large enough. In our case we perform 5-fold CV, therefore we have only 5 samples. However, I would assume that distribution of the model rmsle is normal itself, which results in normal distribution of sample mean. Please, let me know if I might be missing some reasons for it not to be true (the folds are shuffled :)).","fc758c89":"Let us now check cast counts.","58f04424":"#### Low-budget movies","1e908e14":"Do we really have more that 25% movies with no budget? \n\nIn case there are indeed movies done without any budget at all, I'd expect them to have very low or no revenues. Let's check it.","f29173e9":"It increases confidence with stratification by release year. Let us analogously check other attributes to validate the release-year stratification.\n\n## Rating analysis\nAnalogously to budget check, let us now check rating and see if stratification by the year would lead to the same rating distributions compared to train vs. test pair. Along the way, let us again explore anything interesting we notice (such attitude helped us to clean data already).","bc652991":"Just as an interesting observation, the gathered dataset suggests that in the 2000s the film industry tend to produce more and more low\/micro-budget movies. Let's hope it means that young filmmakers have more opportunities nowadays :) At the same time, it might be given by the fact that nowadays first-time filmmakers enter info about their work in databases like TMDB, while information about earlier low-budget movies might have been lost.\n\nAs in the 2000s the industry tends to produce more low-budget movies, the difference in budget distributions might be explained by the fact that in train set there are more years from the 2000s. To verify this,","e1576607":"Let's fix this record as well. \n\nLet's now get back to the observation that in the train set there tend to be more movies with lower budget. Let's check during which years the majority of lower-budget movies were created. ","8a6312a1":"Does the film industry produce more and more movies in 2000s and as the consequence there are also more low-rating movies recently? Or does the proportion of low-rating movies tend to increase? Let us quickly check how the proportion of low-ranking movies changed over recent years.","144de0be":"Bingo. As billion-revenue movies are not made with zero budget, missing values in budget were obviously substituted with zeros.","bacafc37":"### Feature importance","6b007e22":"## Text-based features","b7e05a2f":"Now the plot tells completely different story!\n\n\nIt might be interesting to read\n\n[Why The 70's Were The Greatest Decade In America Cinema](https:\/\/medium.com\/@jtesterkamp\/new-hollywood-why-the-70s-were-the-greatest-decade-in-america-cinema-c42676e2170f), and \n\n[Were the 1950s a good decade for Hollywood cinema? (reddit)](https:\/\/www.reddit.com\/r\/TrueFilm\/comments\/39hf7u\/were_the_1950s_a_good_decade_for_hollywood_cinema\/cs3q208\/)","41aee1fd":"## CatBoost\n\nLet us take hyperparameters found in Version 10 of the kernel.","2ce5c0cc":"## Popularity Analysis","d4dc5380":"## Features based on competition with other movies\nLet us derive features which would compare a movie with other pictures likely shown in cinemas simultaneously with the movie.","d9aeb203":"## Was the organizers' train\/test split time-based?","dce4319a":"The train vs. test difference seems to be given by recent trends in overview style and the fact that there are more recent movies in train data. Out of curiosity, let us check some consice overviews."}}