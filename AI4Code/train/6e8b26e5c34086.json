{"cell_type":{"5d98761f":"code","10c7cc97":"code","9759f462":"code","0bce67c9":"code","45a567c4":"code","c3147b4c":"code","8a2cdf4b":"code","6c184147":"code","6cc9110f":"code","09bc448f":"code","e0a6d681":"code","637c1a13":"code","405c1fbd":"code","184c53fd":"markdown"},"source":{"5d98761f":"#Importing all essential libraries \nimport numpy as np\nimport pandas as pd, os\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import roc_auc_score","10c7cc97":"#Setting the directory path\nprint(os.listdir(\"..\/input\"))\n\n#Assigning training and testing dataset to dataframes\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","9759f462":"train.shape, test.shape","0bce67c9":"train.head()","45a567c4":"test.head()","c3147b4c":"train.describe()","8a2cdf4b":"cols = [c for c in train.columns if c not in ['id', 'target']]\nfor i in range(255):\n    histogram = train.hist(cols[i],bins = 10, figsize=(2,2))\n    \n#Notice the distribution of \"wheezy-copper-turtle-magic\" column","6c184147":"#Further checking the number of unique values in \"wheezy-..-magic\" \ntrain['wheezy-copper-turtle-magic'].nunique()","6cc9110f":"#Here it can be seen that the dataset consists of 512 mini datasets \nlen(train.index) \/ train['wheezy-copper-turtle-magic'].nunique()","09bc448f":"#Create arrays with zeros\nans = np.zeros(len(train))\npredictions = np.zeros(len(test))\ncols = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]","e0a6d681":"for i in range(512):\n\n    #Extracting subset of dataset where wheezy-copper-turtle-magic equals i \n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n    \n    #Applying Principal Components Analysis to deduct dimensionality \n    dim_red = PCA(n_components=40).fit(train2[cols])\n    train3 = dim_red.transform(train2[cols])\n    test3 = dim_red.transform(test2[cols])\n    \n    \n    #Using Stratified K-fold cross-validation \n    skf = StratifiedKFold(n_splits=25, random_state=42)\n    for train_index, test_index in skf.split(train3, train2['target']):\n\n        \n        #Data modelling using Quadratic Discriminant Analysis (here I did use SVM, but QDA gave a better score)\n        classification = QuadraticDiscriminantAnalysis()\n        classification.fit(train3[train_index,:],train2.loc[train_index]['target'])\n        ans[idx1[test_index]] = classification.predict_proba(train3[test_index,:])[:,1]\n        predictions[idx2] += classification.predict_proba(test3)[:,1] \/ skf.n_splits \n\n\n        \n    if i==512: print(i)\n\n    #Printing the validation cross-validation area under the curve (The larger the better.)\n    auc = roc_auc_score(train['target'],ans)\n    print('CV score =',round(auc,5))    \n    ","637c1a13":"#Creating a submission file and filling the target column with predicted probabilities for respective ids\nsub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['target'] = predictions\nsub.to_csv('submission.csv', index=False)","405c1fbd":"#The CV score is 0.96238 ","184c53fd":"# This is my first ever kernel. \nI'd like to thank all the people who have posted their kernels for this competition. For me, this was more of a learning opportunity than a competition. I've made use of some insights I saw in a few kernels. \nI am thankful to the noteworthy contribution of Mr. Bakhteev and Mr. Deotte, their kernels helped me get through a few setbacks.   "}}