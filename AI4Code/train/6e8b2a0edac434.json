{"cell_type":{"b726f452":"code","f792bc2c":"code","f86ea863":"code","cef985b5":"code","b4c78e70":"code","94d6ab89":"code","be5595ce":"code","f304315f":"code","354fa00b":"code","1d03281b":"code","49e25b12":"code","94443eee":"code","4515dfc3":"code","f0863580":"code","ea12231b":"code","0330161c":"code","08c774e8":"code","5b21ac40":"code","c632693b":"code","5e302e14":"code","cabe3041":"code","07bfc0f7":"code","1dd7cb1c":"code","65ac4112":"code","69bcce8e":"code","acc80077":"code","e6e2e3b8":"code","fbf708b1":"code","73175969":"code","45fad97b":"code","2b380af2":"code","38cd4e24":"code","0c10784b":"code","7d13f8b8":"code","c310f279":"code","0a600fd3":"code","af372997":"code","d29cf665":"code","3b2e6b6d":"code","adad1312":"code","0dccd7dc":"markdown","3b841b27":"markdown","774275e8":"markdown","c06bff96":"markdown","cf338d3a":"markdown","eb375523":"markdown","29997b83":"markdown","ed51be1b":"markdown","77aa1621":"markdown","0cd98868":"markdown","96e8a824":"markdown","cd5f0479":"markdown","3637b4aa":"markdown","7b91ae3b":"markdown","ec5f7d78":"markdown","50335ac9":"markdown","71f9d90f":"markdown","2efa918f":"markdown","77c42be7":"markdown","0623f3d1":"markdown","e1fb0951":"markdown","9dea23cc":"markdown","a518bdc6":"markdown","6666ed1f":"markdown","0ecfb22e":"markdown","7ab8e25f":"markdown","c31969bc":"markdown","ae1b61e7":"markdown","3cbeadbe":"markdown","88aefa93":"markdown","139177ff":"markdown","0c9c7f31":"markdown","64cfd3f5":"markdown"},"source":{"b726f452":"# pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\n\n# featuretools for automated feature engineering\nimport featuretools as ft\n\n# matplotlit and seaborn for visualizations\nimport matplotlib.pyplot as plt\nplt.rcParams['font.size'] = 22\nimport seaborn as sns\n\n# Suppress warnings from pandas\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# modeling \nimport lightgbm as lgb\n\n# utilities\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n\n# memory management\nimport gc","f792bc2c":"# Read in data\ntrain_bureau = pd.read_csv('..\/input\/home-credit-manual-engineered-features\/train_bureau_raw.csv', nrows = 1000)\ntest_bureau = pd.read_csv('..\/input\/home-credit-manual-engineered-features\/test_bureau_raw.csv', nrows = 1000)\n\ntrain_previous = pd.read_csv('..\/input\/home-credit-manual-engineered-features\/train_previous_raw.csv', nrows = 1000)\ntest_previous = pd.read_csv('..\/input\/home-credit-manual-engineered-features\/test_previous_raw.csv', nrows = 1000)\n\n# All columns in dataframes\nbureau_columns = list(train_bureau.columns)\nprevious_columns = list(train_previous.columns)","f86ea863":"# Bureau only features\nbureau_features = list(set(bureau_columns) - set(previous_columns))\n\n# Previous only features\nprevious_features = list(set(previous_columns) - set(bureau_columns))\n\n# Original features will be in both datasets\noriginal_features = list(set(previous_columns) & set(bureau_columns))\n\nprint('There are %d original features.' % len(original_features))\nprint('There are %d bureau and bureau balance features.' % len(bureau_features))\nprint('There are %d previous Home Credit loan features.' % len(previous_features))","cef985b5":"train_labels = train_bureau['TARGET']\nprevious_features.append('SK_ID_CURR')\n\ntrain_ids = train_bureau['SK_ID_CURR']\ntest_ids = test_bureau['SK_ID_CURR']\n\n# Merge the dataframes avoiding duplicating columns by subsetting train_previous\ntrain = train_bureau.merge(train_previous[previous_features], on = 'SK_ID_CURR')\ntest = test_bureau.merge(test_previous[previous_features], on = 'SK_ID_CURR')\n\nprint('Training shape: ', train.shape)\nprint('Testing shape: ', test.shape)","b4c78e70":"# One hot encoding\ntrain = pd.get_dummies(train)\ntest = pd.get_dummies(test)\n\n# Match the columns in the dataframes\ntrain, test = train.align(test, join = 'inner', axis = 1)\nprint('Training shape: ', train.shape)\nprint('Testing shape: ', test.shape)","94d6ab89":"cols_with_id = [x for x in train.columns if 'SK_ID_CURR' in x]\ncols_with_bureau_id = [x for x in train.columns if 'SK_ID_BUREAU' in x]\ncols_with_previous_id = [x for x in train.columns if 'SK_ID_PREV' in x]\nprint('There are %d columns that contain SK_ID_CURR' % len(cols_with_id))\nprint('There are %d columns that contain SK_ID_BUREAU' % len(cols_with_bureau_id))\nprint('There are %d columns that contain SK_ID_PREV' % len(cols_with_previous_id))\n\ntrain = train.drop(columns = cols_with_id)\ntest = test.drop(columns = cols_with_id)\nprint('Training shape: ', train.shape)\nprint('Testing shape: ', test.shape)","be5595ce":"# Threshold for removing correlated variables\nthreshold = 0.9\n\n# Absolute value correlation matrix\ncorr_matrix = train.corr().abs()\ncorr_matrix.head()","f304315f":"# Upper triangle of correlations\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper.head()","354fa00b":"# Select columns with correlations above threshold\nto_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n\nprint('There are %d columns to remove.' % (len(to_drop)))","1d03281b":"train = train.drop(columns = to_drop)\ntest = test.drop(columns = to_drop)\n\nprint('Training shape: ', train.shape)\nprint('Testing shape: ', test.shape)","49e25b12":"train = pd.read_csv('..\/input\/home-credit-manual-engineered-features\/m_train_combined.csv')\ntest = pd.read_csv('..\/input\/home-credit-manual-engineered-features\/m_test_combined.csv')","94443eee":"print('Training set full shape: ', train.shape)\nprint('Testing set full shape: ' , test.shape)","4515dfc3":"# Train missing values (in percent)\ntrain_missing = (train.isnull().sum() \/ len(train)).sort_values(ascending = False)\ntrain_missing.head()","f0863580":"# Test missing values (in percent)\ntest_missing = (test.isnull().sum() \/ len(test)).sort_values(ascending = False)\ntest_missing.head()","ea12231b":"# Identify missing values above threshold\ntrain_missing = train_missing.index[train_missing > 0.75]\ntest_missing = test_missing.index[test_missing > 0.75]\n\nall_missing = list(set(set(train_missing) | set(test_missing)))\nprint('There are %d columns with more than 75%% missing values' % len(all_missing))","0330161c":"# Need to save the labels because aligning will remove this column\ntrain_labels = train[\"TARGET\"]\ntrain_ids = train['SK_ID_CURR']\ntest_ids = test['SK_ID_CURR']\n\ntrain = pd.get_dummies(train.drop(columns = all_missing))\ntest = pd.get_dummies(test.drop(columns = all_missing))\n\ntrain, test = train.align(test, join = 'inner', axis = 1)\n\nprint('Training set full shape: ', train.shape)\nprint('Testing set full shape: ' , test.shape)","08c774e8":"train = train.drop(columns = ['SK_ID_CURR'])\ntest = test.drop(columns = ['SK_ID_CURR'])","5b21ac40":"# Initialize an empty array to hold feature importances\nfeature_importances = np.zeros(train.shape[1])\n\n# Create the model with several hyperparameters\nmodel = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 10000, class_weight = 'balanced')","c632693b":"# Fit the model twice to avoid overfitting\nfor i in range(2):\n    \n    # Split into training and validation set\n    train_features, valid_features, train_y, valid_y = train_test_split(train, train_labels, test_size = 0.25, random_state = i)\n    \n    # Train using early stopping\n    model.fit(train_features, train_y, early_stopping_rounds=100, eval_set = [(valid_features, valid_y)], \n              eval_metric = 'auc', verbose = 200)\n    \n    # Record the feature importances\n    feature_importances += model.feature_importances_","5e302e14":"# Make sure to average feature importances! \nfeature_importances = feature_importances \/ 2\nfeature_importances = pd.DataFrame({'feature': list(train.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)\n\nfeature_importances.head()","cabe3041":"# Find the features with zero importance\nzero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\nprint('There are %d features with 0.0 importance' % len(zero_features))\nfeature_importances.tail()","07bfc0f7":"def plot_feature_importances(df, threshold = 0.9):\n    \"\"\"\n    Plots 15 most important features and the cumulative importance of features.\n    Prints the number of features needed to reach threshold cumulative importance.\n    \n    Parameters\n    --------\n    df : dataframe\n        Dataframe of feature importances. Columns must be feature and importance\n    threshold : float, default = 0.9\n        Threshold for pruning information about cumulative importances\n        \n    Return\n    --------\n    df : dataframe\n        Dataframe ordered by feature importances with a normalized column (sums to 1)\n        and a cumulative importance column\n    \n    \"\"\"\n    \n    plt.rcParams['font.size'] = 18\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    # Cumulative importance plot\n    plt.figure(figsize = (8, 6))\n    plt.plot(list(range(len(df))), df['cumulative_importance'], 'r-')\n    plt.xlabel('Number of Features'); plt.ylabel('Cumulative Importance'); \n    plt.title('Cumulative Feature Importance');\n    plt.show();\n    \n    importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n    print('%d features required for %0.2f of cumulative importance' % (importance_index + 1, threshold))\n    \n    return df","1dd7cb1c":"norm_feature_importances = plot_feature_importances(feature_importances)","65ac4112":"train = train.drop(columns = zero_features)\ntest = test.drop(columns = zero_features)\n\nprint('Training shape: ', train.shape)\nprint('Testing shape: ', test.shape)","69bcce8e":"def identify_zero_importance_features(train, train_labels, iterations = 2):\n    \"\"\"\n    Identify zero importance features in a training dataset based on the \n    feature importances from a gradient boosting model. \n    \n    Parameters\n    --------\n    train : dataframe\n        Training features\n        \n    train_labels : np.array\n        Labels for training data\n        \n    iterations : integer, default = 2\n        Number of cross validation splits to use for determining feature importances\n    \"\"\"\n    \n    # Initialize an empty array to hold feature importances\n    feature_importances = np.zeros(train.shape[1])\n\n    # Create the model with several hyperparameters\n    model = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 10000, class_weight = 'balanced')\n    \n    # Fit the model multiple times to avoid overfitting\n    for i in range(iterations):\n\n        # Split into training and validation set\n        train_features, valid_features, train_y, valid_y = train_test_split(train, train_labels, test_size = 0.25, random_state = i)\n\n        # Train using early stopping\n        model.fit(train_features, train_y, early_stopping_rounds=100, eval_set = [(valid_features, valid_y)], \n                  eval_metric = 'auc', verbose = 200)\n\n        # Record the feature importances\n        feature_importances += model.feature_importances_ \/ iterations\n    \n    feature_importances = pd.DataFrame({'feature': list(train.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)\n    \n    # Find the features with zero importance\n    zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\n    print('\\nThere are %d features with 0.0 importance' % len(zero_features))\n    \n    return zero_features, feature_importances","acc80077":"second_round_zero_features, feature_importances = identify_zero_importance_features(train, train_labels)","e6e2e3b8":"norm_feature_importances = plot_feature_importances(feature_importances, threshold = 0.95)","fbf708b1":"# Threshold for cumulative importance\nthreshold = 0.95\n\n# Extract the features to keep\nfeatures_to_keep = list(norm_feature_importances[norm_feature_importances['cumulative_importance'] < threshold]['feature'])\n\n# Create new datasets with smaller features\ntrain_small = train[features_to_keep]\ntest_small = test[features_to_keep]","73175969":"train_small['TARGET'] = train_labels\ntrain_small['SK_ID_CURR'] = train_ids\ntest_small['SK_ID_CURR'] = test_ids\n\ntrain_small.to_csv('m_train_small.csv', index = False)\ntest_small.to_csv('m_test_small.csv', index = False)","45fad97b":"def model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = False, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', boosting_type='goss',\n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ \/ k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] \/ k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","2b380af2":"train['TARGET'] = train_labels\ntrain['SK_ID_CURR'] = train_ids\ntest['SK_ID_CURR'] = test_ids\n\nsubmission, feature_importances, metrics = model(train, test)","38cd4e24":"metrics","0c10784b":"submission.to_csv('selected_features_submission.csv', index = False)","7d13f8b8":"submission_small, feature_importances_small, metrics_small = model(train_small, test_small)","c310f279":"metrics_small","0a600fd3":"submission_small.to_csv('selected_features_small_submission.csv', index = False)","af372997":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import Pipeline\n\n# Make sure to drop the ids and target\ntrain = train.drop(columns = ['SK_ID_CURR', 'TARGET'])\ntest = test.drop(columns = ['SK_ID_CURR'])\n\n# Make a pipeline with imputation and pca\npipeline = Pipeline(steps = [('imputer', Imputer(strategy = 'median')),\n             ('pca', PCA())])\n\n# Fit and transform on the training data\ntrain_pca = pipeline.fit_transform(train)\n\n# transform the testing data\ntest_pca = pipeline.transform(test)","d29cf665":"# Extract the pca object\npca = pipeline.named_steps['pca']\n\n# Plot the cumulative variance explained\n\nplt.figure(figsize = (10, 8))\nplt.plot(list(range(train.shape[1])), np.cumsum(pca.explained_variance_ratio_), 'r-')\nplt.xlabel('Number of PC'); plt.ylabel('Cumulative Variance Explained');\nplt.title('Cumulative Variance Explained with PCA');","3b2e6b6d":"# Dataframe of pca results\npca_df = pd.DataFrame({'pc_1': train_pca[:, 0], 'pc_2': train_pca[:, 1], 'target': train_labels})\n\n# Plot pc2 vs pc1 colored by target\nsns.lmplot('pc_1', 'pc_2', data = pca_df, hue = 'target', fit_reg=False, size = 10)\nplt.title('PC2 vs PC1 by Target');","adad1312":"print('2 principal components account for {:.4f}% of the variance.'.format(100 * np.sum(pca.explained_variance_ratio_[:2])))","0dccd7dc":"The smaller featureset scores __0.782__ when submitted to the public leaderboard.  \nTherefore, as pointed out in the abstract above, and in conclusion we can see that the results and accuracy with the Test \"Full\" Dataset (536 features) and the Test \"Small\" Dataset (342 features - with only 95% importance) are very similar ones.","3b841b27":"At this point, we can re-run the model to see if it identifies any more features with zero importance. In a way, we are implementing our own form of recursive feature elimination. Since we are repeating work, we should probably put the zero feature importance identification code in a function.","774275e8":"We only need a few prinicipal components to account for the majority of variance in the data. We can use the first two principal components to visualize the entire dataset. We will color the datapoints by the value of the target to see if using two principal components clearly separates the classes.","c06bff96":"### Identify Correlated Variables","cf338d3a":"#### Drop Correlated Variables","eb375523":"Even though we have accounted for most of the variance, that does not mean the pca decomposition makes the problem of identifying loans repaid vs not repaid any easier. Keep in mind that the differences along PC_1 are more important than that differences along PC_2. PCA does not consider the value of the label when projecting the features to a lower dimension. Feel free to try a classifier on top of this data, but when I have done so, I noticed that it was not very accurate. ","29997b83":"## PCA Example\n\nWe can go through a quick example to show how PCA is implemented. Without going through too many details, PCA finds a new set of axis (the principal components) that maximize the amount of variance captured in the data. The original data is then projected down onto these principal components. The idea is that we can use fewer principal components than the original number of features while still capturing most of the variance. PCA is implemented in Scikit-Learn in the same way as preprocessing methods. We can either select the number of new components, or the fraction of variance we want explained in the data. If we pass in no argument, the number of principal components will be the same as the number of original features. We can then use the `variance_explained_ratio_` to determine the number of components needed for different threshold of variance retained.","ed51be1b":"# Remove Collinear Variables\n\nCollinear variables are those which are highly correlated with one another. These can decrease the model's availablility to learn, decrease model interpretability, and decrease generalization performance on the test set. Clearly, these are three things we want to increase, so removing collinear variables is a useful step. We will establish an admittedly arbitrary threshold for removing collinear variables, and then remove one out of any pair of variables that is above that threshold. \n\nThe code below identifies the highly correlated variables based on the absolute magnitude of the Pearson correlation coefficient being greater than 0.9. Again, this is not entirely accurate since we are dealing with such a limited section of the data. This code is for illustration purposes, but if we read in the entire dataset, it would work (if the kernels allowed it)! \n\nThis code is adapted from [work by Chris Albon](https:\/\/chrisalbon.com\/machine_learning\/feature_selection\/drop_highly_correlated_features\/).","77aa1621":"### Read in Full Dataset\n\nNow we are ready to move on to the full set of features. These were built by applying the above steps to the entire `train_bureau` and `train_previous` files (you can do the same if you want and have the computational resources)!","0cd98868":"# Feature Selection | Credit Default  \n  \n**David Rivas, Ph.D.**    \n  \n# Abstract  \n  \nIn this notebook we apply feature engineering to the manual engineered features built in two previous kernels. We reduce the number of features using several methods and then we test the performance of the features using a fairly basic gradient boosting machine model.   \n  \nThe full set of features was built in [Part One](https:\/\/www.kaggle.com\/willkoehrsen\/introduction-to-manual-feature-engineering) and [Part Two](https:\/\/www.kaggle.com\/willkoehrsen\/introduction-to-manual-feature-engineering-p2) of Manual Feature Engineering.  \n  \nWe employ a number of feature selection methods. These methods are necessary to reduce the number of features to increase model interpretability, decrease model runtime, and increase generalization performance on the test set. The methods of feature selection we use are:  \n\n1. Remove highly collinear variables as measured by a correlation coefficient greater than 0.9\n2. Remove any columns with more than 75% missing values.\n3. Remove any features with a zero importance as determined by a gradient boosting machine.\n4. (Optional) keep only enough features to account for 95% of the importance in the gradient boosting machine.\n\nUsing the first three methods, we reduced the number of features from __1465__ to __536__ with a 5-fold cv AUC ROC score of 0.7838 and a public leaderboard score of 0.783.  \n\nAfter applying the fourth method, we end up with 342 features with a 5-fold cv AUC SCORE of 0.7482 and a public leaderboard score of 0.782.   \n\nWe also take a look at an example of applying PCA although we do not use this method for feature reduction.   ","96e8a824":"Applying this on the entire dataset __results in 538  collinear features__ removed.  \n\nThis has reduced the number of features singificantly, but it is likely still too many. At this point, we'll read in the full dataset after removing correlated variables for further feature selection.\n\nThe full datasets (after removing correlated variables) are available in `m_train_combined.csv` and `m_test_combined.csv`.","cd5f0479":"# Remove Missing Values\n\nA relatively simple choice of feature selection is removing missing values. Well, it seems simple, at least until we have to decide what percentage of missing values is the minimum threshold for removing a column. Like many choices in machine learning, there is no right answer, and not even a general rule of thumb for making this choice. In this implementation, if any columns have greater than 75% missing values, they will be removed. \n\nMost models (including those in Sk-Learn) cannot handle missing values, so we will have to fill these in before machine learning. The Gradient Boosting Machine ([at least in LightGBM](https:\/\/github.com\/Microsoft\/LightGBM\/blob\/master\/docs\/Advanced-Topics.rst)) can handle missing values. Imputing missing values always makes me a little uncomfortable because we are adding information that actually isn't in the dataset. Since we are going to be evaluating several models (in a later notebook), we will have to use some form of imputation. For now, we will focus on removing columns above the threshold.","3637b4aa":"# References Used  \n   \n[Feature Selection](https:\/\/www.kaggle.com\/willkoehrsen\/introduction-to-feature-selection) by Will Koehsen\n\n[Feature Engineering I | Credit Default](https:\/\/www.kaggle.com\/davidrivasphd\/feature-engineering-i-credit-default) by David Rivas, Ph.D.\n\n[Feature Engineering II Functions | Credit Default](https:\/\/www.kaggle.com\/davidrivasphd\/feature-engineering-ii-functions-credit-default) by David Rivas, Ph.D.\n","7b91ae3b":"### Test \"Small\" Dataset\n\nThe small dataset requires one additional step over the ful l dataset:\n\n* Keep only features needed to reach 95% cumulative importance in the gradient boosting machine","ec5f7d78":"After applying this to the full dataset, we end up with __1416 __ features. More features might seem like a good thing, and they can be if they help our model learn. However, irrelevant features, highly correlated features, and missing values can prevent the model from learning and decrease generalization performance on the testing data. Therefore, we perform feature selection to keep only the most useful variables.\n\nWe will start feature selection by focusing on collinear variables.","50335ac9":"Next we want to one-hot encode the dataframes. This doesn't give the full features since we are only working with a sample of the data and this will not create as many columns as one-hot encoding the entire dataset would. Doing this to the full dataset results in 1465 features.\n\nAn important note in the code cell is where we __align the dataframes by the columns.__ This ensures we have the same columns in the training and testing datasets.","71f9d90f":"The full features after feature selection score __0.783__ when submitted to the public leaderboard. ","2efa918f":"We can keep only the features needed for 95% importance. This step seems to me to have the greatest chance of harming the model's learning ability, so rather than changing the original dataset, we will make smaller copies. Then, we can test both versions of the data to see if the extra feature removal step is worthwhile. ","77c42be7":"Standard imports for data science work. The LightGBM library is used for the gradient boosting machine.","0623f3d1":"When we do this to the full dataset, we get __1465__ features. ","e1fb0951":"# Other Options for Dimensionality Reduction\n\nWe only covered a small portion of the techniques used for feature selection\/dimensionality reduction. There are many other methods such as:\n\n* PCA: Principle Components Analysis (PCA)\n* ICA: Independent Components Analysis (ICA)\n* Manifold learning: [also called non-linear dimensionality reduction](https:\/\/stats.stackexchange.com\/questions\/247907\/what-is-the-difference-between-manifold-learning-and-non-linear-dimensionality-r)\n\nPCA is a great method for reducing the number of features provided that you do not care about model interpretability. It projects the original set of features onto a lower dimension, in the process, eliminating any physical representation behind the features. Here's a pretty thorough introduction to the math for anyone interested. PCA also assumes that the data is Gaussian distributed, which may not be the case, especially when dealing with real-world human generated data. \n\nICA representations also obscure any physical meaning behind the variables and presevere the most \"independent\" dimensions of the data (which is different than the dimensions with the most variance). \n\nManifold learning is more often used for low-dimensional visualizations (such as with T-SNE or LLE) rather than for dimensionality reduction for a classifier. These methods are heavily dependent on several hyperparameters and are not deterministic which means that there is no way to apply it to new data (in other words you cannot `fit` it to the training data and then separately `transform` the testing data). The learned representation of a dataset will change every time you apply manifold learning so it is not generally a stable method for feature selection.","9dea23cc":"Let's now remove the features that have zero importance (271 zero importance features out of at total 844 features - knowing that 288 of the latter's top features are necessary to keep 0.9 cumulative importance).","a518bdc6":"# Going Forward  \n\n\nGoing forward, we might actually want to add _more_ features except this time, instead of naively applying aggregations, think about what features are actually important from a domain point of view. There are a number of kernels that have created useful features that we can add to our set here to improve performance. The process of feature engineering - feature selection is iterative, and it may require several more passes before we get it completely right! ","6666ed1f":"Since the LightGBM model does not need missing values to be imputed, we can directly `fit` on the training data. We will use Early Stopping to determine the optimal number of iterations and run the model twice, averaging the feature importances to try and avoid overfitting to a certain set of features.","0ecfb22e":"# Feature Selection through Feature Importances\n\nThe next method we can employ for feature selection is to use the feature importances of a model. Tree-based models (and consequently ensembles of trees) can determine an \"importance\" for each feature by measuring the reduction in impurity for including the feature in the model. I'm not really sure what that means (any explanations would be welcome) and the absolute value of the importance can be difficult to interpret. However, the relative value of the importances can be used as an approximation of the \"relevance\" of different features in a model. Moreover, we can use the feature importances to remove features that the model does not consider important. \n\nOne method for doing this automatically is the [Recursive Feature Elimination method](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFE.html) in Scikit-Learn. This accepts an estimator (one that either returns feature weights such as a linear regression, or feature importances such as a random forest) and a desired number of features. In then fits the model repeatedly on the data and iteratively removes the lowest importance features until the desired number of features is left. This means we have another arbitrary hyperparameter to use in out pipeline: the number of features to keep! \n\nInstead of doing this automatically, we can perform our own feature removal by first removing all zero importance features from the model. If this leaves too many features, then we can consider removing the features with the lowest importance. We will use a Gradient Boosted Model from the LightGBM library to assess feature importances. If you're used to the Scikit-Learn library, the LightGBM library has an API that makes deploying the model very similar to using a Scikit-Learn model. ","7ab8e25f":"### Test \"Full\" Dataset\n\nThis is the expanded dataset. To recap the process to make this dataset we:\n\n* Removed collinear features as measured by the correlation coefficient greater than 0.9\n* Removed any columns with greater than 75% missing values in the train or test set\n* Removed all features with non-zero feature importances","c31969bc":"* `train_bureau` is the training features built manually using the `bureau` and `bureau_balance` data\n* `train_previous` is the training features built manually using the `previous`, `cash`, `credit`, and `installments` data\n\nWe first will see how many features we built over the manual engineering process. Here we use a couple of set operations to find the columns that are only in the `bureau`, only in the `previous`, and in both dataframes, indicating that there are `original` features from the `application` dataframe. Here we are working with a small subset of the data in order to not overwhelm the kernel. This code has also been run on the full dataset (we will take a look at some of the results).","ae1b61e7":"# Test New Featuresets\n\nThe last step of feature removal we did seems like it may have the potential to hurt the model the most. Therefore we want to test the effect of this removal. To do that, we can use a standard model and change the features. \n\nWe will use a fairly standard LightGBM model, similar to the one we used for feature selection. The main difference is this model uses five-fold cross validation for training and we  use it to make predictions. There's a lot of code here, but that's because I included documentation and a few extras (such as feature importances) that aren't strictly necessary. For now, understanding the entire model isn't critical, just know that we are using the same model with two different datasets to see which one performs the best.","3cbeadbe":"There are now no 0 importance features left (I guess we should have expected this). If we want to remove more features, we will have to start with features that have a non-zero importance. One way we could do this is by retaining enough features to account for a threshold percentage of importance, such as 95%. At this point, let's keep enough features to account for 95% of the importance. Again, this is an arbitrary decision! ","88aefa93":"That gives us the number of features in each dataframe. Now we want to combine the data without creating any duplicate rows. ","139177ff":"We see that one of our features made it into the top 5 most important! That's a good sign for all of our hard work making the features. It also looks like many of the features we made have literally 0 importance. For the gradient boosting machine, features with 0 importance are not used at all to make any splits. Therefore, we can remove these features from the model with no effect on performance (except for faster training). Before removing the features with 0 importance, let us plot the following to have a sense of the how many of these features are really important: ","0c9c7f31":"### Admit and Correct Mistakes!\n\nWhen doing manual feature engineering, I accidentally created some columns derived from the client id, `SK_ID_CURR`. As this is a unique identifier for each client, it should not have any predictive power, and we would not want to build a model trained on this \"feature\". Let's remove any columns built on the `SK_ID_CURR`.","64cfd3f5":"Let's drop the columns, one-hot encode the dataframes, and then align the columns of the dataframes."}}