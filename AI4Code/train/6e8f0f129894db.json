{"cell_type":{"5ad37bd1":"code","2bba2acd":"code","2a26b7af":"code","6f5c95de":"code","2904e569":"code","7cfa66d5":"code","fb15545e":"code","6400cc86":"code","e1a7da45":"code","7960e230":"code","0bde8a12":"code","a74ff3e3":"code","18a6869f":"code","a2339875":"code","1421b3ea":"code","39c2c8cb":"code","19e2ef98":"code","01e30bcd":"code","2dac3a12":"code","a009ee80":"code","691a2bea":"code","0e1fe9b4":"code","5a217d67":"code","66184077":"code","8cffd4b8":"code","e19cb95a":"code","ddd9bcdd":"code","937728c0":"code","99460789":"code","76f6385a":"code","780a1c0e":"code","7f33e41e":"code","a26bef86":"code","fdead193":"code","a336da80":"code","1c0deb00":"code","3becdc6a":"code","772d7aab":"code","fbcdb973":"code","2494d3ba":"code","c69bf7c9":"code","df3468ca":"code","4dbc87b3":"code","8fbdec81":"code","fbcfb3f8":"code","9d4d5e93":"code","dd68690e":"code","842d2430":"code","3f6ff1e1":"code","6b13cfb5":"code","b39dacf9":"code","8ac8a815":"code","93f06c24":"code","159d6e55":"code","ed7de7f3":"code","9be6925b":"code","927e85f4":"code","9bc327ff":"code","7b513f55":"code","13a18cea":"code","5075a9cf":"code","89bb7457":"markdown","f5910283":"markdown","61c49595":"markdown","856141c5":"markdown","fbe40df3":"markdown","a6671b7a":"markdown"},"source":{"5ad37bd1":"import pandas as pd\nimport numpy as np\n\n# for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# for the Q-Q plots\nimport scipy.stats as stats\n\n# the dataset for the demo\nfrom sklearn.datasets import load_boston\n\n# for linear regression\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,ElasticNet\n\n# to split and standarize the dataset\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.preprocessing import StandardScaler,RobustScaler\n\n# to evaluate the regression model\nfrom sklearn.metrics import mean_squared_error\n\nfrom feature_engine import outlier_removers as outr\n\nfrom feature_engine import categorical_encoders as ce\n\nfrom sklearn.neighbors import LocalOutlierFactor\n\ndistance = 1.5","2bba2acd":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2a26b7af":"data=pd.read_csv('\/kaggle\/input\/Hitters.csv')\ndata","6f5c95de":"data.info()","2904e569":"data.describe([0.05,0.10,0.25,0.50,0.75,0.80,0.90,0.95,0.99]).T","7cfa66d5":"data.ndim","fb15545e":"data.shape","6400cc86":"# find the variables with missing observations\n\nvars_with_na = [var for var in data.columns if data[var].isnull().mean() > 0]\nvars_with_na","e1a7da45":"# let's find out whether they are numerical or categorical\ndata[vars_with_na].dtypes","7960e230":"# let's have a look at the values of the variables with\n# missing data\n\ndata[vars_with_na].head(10)","0bde8a12":"# let's find out the percentage of observations missing per variable\n\n# calculate the percentage of missing (as we did in section 3)\n# using the isnull() and mean() methods from pandas\ndata_na = data[vars_with_na].isnull().mean()\n\n# transform the array into a dataframe\ndata_na = pd.DataFrame(data_na.reset_index())\n\n# add column names to the dataframe\ndata_na.columns = ['variable', 'na_percentage']\n\n# order the dataframe according to percentage of na per variable\ndata_na.sort_values(by='na_percentage', ascending=False, inplace=True)\n\n# show\ndata_na","a74ff3e3":"#veri setini inceledigimizde , sadece salary bagimli degiskeninde na degerler oldugnu g\u00f6zlemliyoruz.\n#ve bu na degiskenlerinin sistematik degil tamamen rastgle bir sekilde nan atandigini g\u00f6r\u00fcyoruz.\n# bu nedenle prediction \u00f6ncesi mean medyan degerleri atamak yerine siliyoruz \n","18a6869f":"data.dropna(subset = [\"Salary\"], inplace=True)","a2339875":"data.head()","1421b3ea":"#birden fazla independent degiskenin birbirine cok fazla korele olmasi ","39c2c8cb":"\n\n# we calculate the correlations using pandas corr\n# and we round the values to 2 decimals\ncorrelation_matrix = data.corr().round(2) \n\n# plot the correlation matrix usng seaborn\n# annot = True to print the correlation values\n# inside the squares\n\nfigure = plt.figure(figsize=(12, 12))\nsns.heatmap(data=correlation_matrix, annot=True)","19e2ef98":"correlation_matrix = data.corr().round(2)\nthreshold=0.75\nfiltre=np.abs(correlation_matrix['Salary'])>0.50\ncorr_features=correlation_matrix.columns[filtre].tolist()\nsns.clustermap(data[corr_features].corr(),annot=True,fmt=\".2f\")\nplt.title('Correlation btw features')\nplt.show()\n","01e30bcd":"# correlation between RAD (index of accessibility to radial highways)\n# and TAX (full-value property-tax rate per $10,000)\n\nsns.lmplot(x=\"CRuns\", y=\"CHits\", data=data, order=1)","2dac3a12":"sns.lmplot(x=\"CWalks\", y=\"Hits\", data=data, order=1)","a009ee80":"#sns.pairplot(data,diag_kind='kde',markers='+')\n#plt.show()","691a2bea":"plt.figure()\nsns.countplot(data['League'])","0e1fe9b4":"plt.figure()\nsns.countplot(data['NewLeague'])","5a217d67":"plt.figure()\nsns.countplot(data['Division'])","66184077":"# function to create histogram, Q-Q plot and\n# boxplot. We learned this in section 3 of the course\n\n\ndef diagnostic_plots(df, variable):\n    # function takes a dataframe (df) and\n    # the variable of interest as arguments\n\n    # define figure size\n    plt.figure(figsize=(16, 4))\n\n    # histogram\n    plt.subplot(1, 3, 1)\n    sns.distplot(df[variable], bins=30)\n    plt.title('Histogram')\n\n    # Q-Q plot\n    plt.subplot(1, 3, 2)\n    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n    plt.ylabel('Variable quantiles')\n\n    # boxplot\n    plt.subplot(1, 3, 3)\n    sns.boxplot(y=df[variable])\n    plt.title('Boxplot')\n\n    plt.show()","8cffd4b8":"# let's find outliers in Salary\n\ndiagnostic_plots(data, 'Salary')","e19cb95a":"# let's find outliers in Errors\n\ndiagnostic_plots(data, 'Errors')","ddd9bcdd":"# let's find outliers in Assists\n\ndiagnostic_plots(data, 'Assists')","937728c0":"# let's find outliers in PutOuts\n\ndiagnostic_plots(data, 'PutOuts')","99460789":"# let's find outliers in CWalks\n\ndiagnostic_plots(data, 'CWalks')","76f6385a":"# let's find outliers in CRBI\n\ndiagnostic_plots(data, 'CRBI')","780a1c0e":"# let's find outliers in CHmRun\ndiagnostic_plots(data, 'CHmRun')","7f33e41e":"# let's find outliers in Years\ndiagnostic_plots(data, 'Years')","a26bef86":"# let's find outliers in HmRun\ndiagnostic_plots(data, 'HmRun')","fdead193":"# let's find outliers in CAtBat\ndiagnostic_plots(data, 'CAtBat')","a336da80":"# let's find outliers in CHits\ndiagnostic_plots(data, 'CHits')","1c0deb00":"# let's find outliers in CRuns\ndiagnostic_plots(data, 'CRuns')","3becdc6a":"def find_skewed_boundaries(df, variable):\n\n    # Let's calculate the boundaries outside which sit the outliers\n    # for skewed distributions\n\n    # distance passed as an argument, gives us the option to\n    # estimate 1.5 times or 3 times the IQR to calculate\n    # the boundaries.\n\n    IQR = df[variable].quantile(0.75) - df[variable].quantile(0.25)\n\n    lower_boundary = df[variable].quantile(0.25) - (IQR * distance)\n    upper_boundary = df[variable].quantile(0.75) + (IQR * distance)\n\n    return upper_boundary, lower_boundary","772d7aab":"RM_upper_limit, RM_lower_limit = find_skewed_boundaries(data, 'Salary')\nRM_upper_limit, RM_lower_limit\n","fbcdb973":"outliers_salary = np.where(data['Salary'] > RM_upper_limit, True, np.where(data['Salary'] < RM_lower_limit, True, False))","2494d3ba":"# let's trimm the dataset\n\ndata = data.loc[~(outliers_salary)]\ndata[data[\"Salary\"] > 2000]","c69bf7c9":"diagnostic_plots(data, 'Salary')","df3468ca":"#diagnostic_plots(data1, 'Salary')","4dbc87b3":"data = pd.get_dummies(data, columns = ['League', 'Division', 'NewLeague'], drop_first = True)\ndata.head()","8fbdec81":"from sklearn.neighbors import LocalOutlierFactor\nclf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n# use fit_predict to compute the predicted labels of the training samples\n# (when LOF is used for outlier detection, the estimator has no predict,\n# decision_function and score_samples methods).\ny_pred = clf.fit_predict(data)[0:10]\nX_scores = clf.negative_outlier_factor_\n\nX_scores","fbcfb3f8":"threshold=np.sort(X_scores)[10]\nthreshold","9d4d5e93":"data=data.loc[X_scores > threshold]\ndata.shape","dd68690e":"# log transform the variables\ndata['CRuns'] = np.log(data['CRuns'])\ndata['CHits'] = np.log(data['CHits'])\ndata['CAtBat'] = np.log(data['CAtBat'])\ndata['Years'] = np.log(data['Years'])\ndata['CRBI'] = np.log(data['CRBI'])\ndata['CWalks'] = np.log(data['CWalks'])\n","842d2430":"# Separate into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n            data.drop(['Salary'], axis=1),\n            data['Salary'], test_size=0.2, random_state=42)\n\n# set up the capper\ncapper = outr.OutlierTrimmer(\n    distribution='skewed', tail='right', fold=3 , variables=[ \n                                                              'CRuns',\n                                                              'CHits',\n                                                              'CAtBat',\n                                                              'HmRun',\n                                                              'Years',\n                                                              'CHmRun',\n                                                              'CRBI',\n                                                              'CWalks',\n                                                              'PutOuts',\n                                                              'Assists',\n                                                              'Errors'])\n# fit the capper\ncapper.fit(X_train)\n\n# transform the data\ntrain_t= capper.transform(X_train)\ntest_t= capper.transform(X_test)","3f6ff1e1":"train_t[['CRuns', 'CHits']].max()","6b13cfb5":"data[['CRuns', 'CHits']].max()","b39dacf9":"# let's find outliers in CHits\ndiagnostic_plots(data, 'CRuns')","8ac8a815":"# let's find outliers in CHits\ndiagnostic_plots(train_t, 'CRuns')","93f06c24":"# let's find outliers in CHits\ndiagnostic_plots(train_t, 'CHits')","159d6e55":"# let's find outliers in CHits\ndiagnostic_plots(test_t, 'CRuns')","ed7de7f3":"train_t= X_train\ntest_t= X_test","9be6925b":"train_t","927e85f4":"# let's scale the features\nscaler = RobustScaler()\nscaler.fit(train_t)","9bc327ff":"# model build using the natural distributions\n\n# call the model\nlinreg = LinearRegression()\n\n# fit the model\nlinreg.fit(scaler.transform(train_t), y_train)\n\n# make predictions and calculate the mean squared\n# error over the train set\nprint('Train set')\npred = linreg.predict(scaler.transform(train_t))\nprint('Linear Regression mse: {}'.format(mean_squared_error(y_train, pred)))\n\n# make predictions and calculate the mean squared\n# error over the test set\nprint('Test set')\npred = linreg.predict(scaler.transform(test_t))\nprint('Linear Regression mse: {}'.format(mean_squared_error(y_test, pred)))\nprint(np.sqrt(mean_squared_error(y_test, pred)))","7b513f55":"error = y_test - pred\nsns.distplot(error, bins=30)","13a18cea":"np.sqrt(mean_squared_error(y_test, pred))","5075a9cf":"#train_t= encoder.transform(X_train)\n#test_t= encoder.transform(X_test)\n\n\nridge=Ridge(random_state=42,max_iter=30000)\nalphas=np.logspace(-4,-0.5,30)\ntuned_parameters=[ {\"alpha\":alphas} ]\nn_folds=5\n\nclf=GridSearchCV(ridge,tuned_parameters,cv=n_folds,scoring=\"neg_mean_squared_error\",refit=True)\nclf.fit(train_t,y_train)\nscores=clf.cv_results_[\"mean_test_score\"]\nscores_std= clf.cv_results_[\"std_test_score\"]\nclf.best_estimator_.coef_\n\nridge= clf.best_estimator_\nridge\ny_predicted_dummy=clf.predict(test_t)\nmse=mean_squared_error(y_test, y_predicted_dummy)\n\nmse\n\nnp.sqrt(mse)\n","89bb7457":"## multicolinearity  \nTo determine co-linearity, we evaluate the correlation of all the independent variables in the dataframe.","f5910283":"## outlier tespiti ve cikarilmasi","61c49595":"## checking missing values","856141c5":"## One Hot Encoding","fbe40df3":"AtBat - ratio - multi valued discrete\n1986 y\u0131l\u0131nda vurus say\u0131s\u0131\n\nHit - ratio - multi valued discrete\n1986'daki isabet say\u0131s\u0131\n\nHmRun -ratio  - multi valued discrete\n1986 y\u0131l\u0131nda yap\u0131lan ev ko\u015fu say\u0131s\u0131\n\nKo\u015far  - ratio - multi valued discrete\n1986 y\u0131l\u0131nda yap\u0131lan ko\u015fu say\u0131s\u0131\n\nRBI  - ratio - multi valued discrete\n1986 y\u0131l\u0131nda vuru\u015f yap\u0131lan ko\u015fu say\u0131s\u0131\n\nY\u00fcr\u00fcy\u00fc\u015fleri  - ratio - multi valued discrete\n1986 y\u0131l\u0131nda y\u00fcr\u00fcy\u00fc\u015f say\u0131s\u0131\n\ny\u0131l  - ratio -multi valued discrete\nB\u00fcy\u00fck liglerdeki y\u0131l say\u0131s\u0131\n\nCATBat - ratio -multi valued discrete\nKariyeri boyunca yarasa say\u0131s\u0131\n\nchits - ratio -multi valued discrete\nKariyeri boyunca isabet say\u0131s\u0131\n\nCHmRun - ratio -multi valued discrete\nKariyeri boyunca ev ka\u00e7\u0131\u015f\u0131\n\nCRuns - ratio -multi valued discrete\nKariyeri boyunca ko\u015fu say\u0131s\u0131\n\nCRBI - ratio -multi valued discrete\nKariyeri boyunca katledilen ko\u015fu say\u0131s\u0131\n\nCWalks - ratio -multi valued discrete\nKariyeri boyunca y\u00fcr\u00fcy\u00fc\u015f say\u0131s\u0131\n\nLig nominal - string\n1986 sonunda oyuncunun ligini g\u00f6steren A ve N seviyelerine sahip bir fakt\u00f6r\n\ndevision nominal - string\n1986 sonunda oyuncunun b\u00f6l\u00fcnmesini g\u00f6steren E ve W seviyelerine sahip bir fakt\u00f6r\n\nPutOuts ratio -multi valued discrete\n1986 y\u0131l\u0131nda yap\u0131lan sat\u0131\u015f say\u0131s\u0131\n\nasist ratio -multi valued discrete\n1986 y\u0131l\u0131nda asist say\u0131s\u0131\n\nHatalar ratio -multi valued discrete\n1986'daki hata say\u0131s\u0131\n\nMaa\u015f ratio -continous\n1987 a\u00e7\u0131l\u0131\u015f g\u00fcn\u00fcnde y\u0131ll\u0131k binlerce dolar maa\u015f\n\nNewLeague nominal nominal - string\n1987 ba\u015f\u0131nda oyuncunun ligini g\u00f6steren A ve N seviyelerine sahip bir fakt\u00f6rAtbat =","a6671b7a":" mean degeri medyandan farkli ise carpiklik vardir.\n feature lar arasinda min max kiyaslamasi yapilarak \u00f6lcek farkina bakilabilir."}}