{"cell_type":{"db72d2ab":"code","88b27bb0":"code","815c36e3":"code","bafb5e39":"code","22875bb0":"code","94ea91e5":"code","ff004247":"code","e4c626b7":"code","5f2347fc":"code","44be071a":"code","f809e71f":"code","d3532b5c":"code","09e41e33":"markdown","569f9ee2":"markdown","8ea97bf5":"markdown","59cba8c0":"markdown","afe68f3d":"markdown","2c61ff2e":"markdown","d4fac953":"markdown","793127ae":"markdown","9a5d441b":"markdown","e89ea134":"markdown"},"source":{"db72d2ab":"!pip install ranking\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom readability import Readability\nfrom collections import defaultdict\nfrom ranking import Ranking\n\nimport os\nprint(os.listdir(\"..\/input\"))","88b27bb0":"questions = pd.read_csv(\"..\/input\/questions.csv\", delimiter=\",\")\nquestions = questions.drop([\"questions_author_id\", \"questions_date_added\"], axis=1)\n\nscores = pd.read_csv(\"..\/input\/question_scores.csv\")\n\nquestions = pd.merge(\n    questions,\n    scores,\n    left_on='questions_id',\n    right_on='id',\n    how=\"inner\"  # Questions without answers are useless, so we leave them out\n)\n\nquestions = questions.drop(\"id\", axis=1)\n\nquestions.tail()","815c36e3":"professionals = pd.read_csv(\"..\/input\/professionals.csv\", delimiter=\",\")\nprofessionals = professionals.drop([\"professionals_date_joined\", \"professionals_headline\"], axis=1)\nprofessionals.tail()","bafb5e39":"answers = pd.read_csv(\"..\/input\/answers.csv\")\nanswers = answers.drop([\"answers_date_added\", \"answers_id\"], axis=1)\n\nanswers.head()","22875bb0":"data = pd.merge(\n    questions,\n    answers,\n    left_on='questions_id',\n    right_on='answers_question_id',\n    how=\"inner\"\n)\ndata = data.drop([\"questions_id\", \"answers_question_id\"], axis=1)\n\ndata = pd.merge(\n    data, \n    professionals,\n    left_on=\"answers_author_id\",\n    right_on=\"professionals_id\",\n    how=\"inner\"\n)\n\ndata = data.drop([\"answers_author_id\"], axis=1)\ndata.tail()","94ea91e5":"(professionals.professionals_location.unique(), professionals.professionals_industry.unique())","ff004247":"(len(data.professionals_id.unique()), len(data.professionals_location.unique()),\n len(data.professionals_industry.unique()))","e4c626b7":"import string\n# returns that last word in the string, in lowercase\ndef simplify_name(name):\n    line = str(name).replace(\",\", \" \")\n    line = line.split()[-1]\n    line = line.lower()\n    return line \n\n# data['professionals_location']\ndata['professionals_location'] = data['professionals_location'].apply(simplify_name)\ndata['professionals_industry'] = data['professionals_industry'].apply(simplify_name)\ndata.tail()","5f2347fc":"(len(data.professionals_id.unique()), len(data.professionals_location.unique()),\n len(data.professionals_industry.unique()))","44be071a":"writers = list(data.professionals_id.unique())\n\n# These dictionaries map the writer to the words that they engage with\nwriter_words = {writer:{} for writer in writers}\nwriter_tags = {writer:{} for writer in writers}\n\n# for each row, add to the writer's word bucket\nfor _, question in data.iterrows():\n    writer = question[\"professionals_id\"]\n    body = question[\"questions_body\"].split()\n    title = question[\"questions_title\"].split()\n    \n    # Store the tags in one list and words in another\n    # Store as a lower case so that things like Word and word are not held differently\n    def simplify(s):\n        s = s.lower()\n        exclude = set(string.punctuation)\n        s = ''.join(ch for ch in s if ch not in exclude)\n        return s\n    \n    tags = [simplify(word) for word in (body+title) if len(word) > 0 and word[0] == \"#\"]\n    body = [simplify(word) for word in (body+title) if len(word) > 0 and word[0] != \"#\"]\n    \n    # add the words to the writer's word cloud\n    for word in body:\n        if word in writer_words[writer]:\n            writer_words[writer][word] += 1\n        else:\n            writer_words[writer][word] = 1\n        \n    # add the words to the writer's tag cloud\n    for word in tags:\n        if word in writer_tags[writer]:\n            writer_tags[writer][word] += 1\n        else:\n            writer_tags[writer][word] = 1\n\nwriter_tags[data.professionals_id.unique()[1]]","f809e71f":"\ntotal_tags = []\ntotal_words = []\n\ndef normalize_dict(word_to_freq):\n    \"\"\" Maps the counts from 0 to 1, divides all by the most frequent word\/tag \n        Mutates and returns the original dict\n    \"\"\"\n    max_freq = 0\n    for word in word_to_freq:\n        max_freq = max(max_freq, word_to_freq[word])\n    \n    if float(max_freq) <= 10 ** (-8):\n        return dict()\n    else:\n        for word in word_to_freq:\n            word_to_freq[word] \/= float(max_freq)\n    \n    return word_to_freq\n    \n\nfor writer in writers:\n    # get the total number of tags they made\n    totaltag = 0\n    totalword = 0\n    if writer in writer_tags:\n        for tag in writer_tags[writer]:\n            totaltag += writer_tags[writer][tag]\n    if writer in writer_words:\n        for word in writer_words[writer]:\n            totalword += writer_words[writer][word]\n            \n    total_tags.append(totaltag)\n    total_words.append(totalword)\n\n# keep only the people with at least 50 tags associated to them (AKA only recommend users that have already been useful)\nwriter_totals = pd.DataFrame({\"professional_id\": writers, \"total_tags\": total_tags, \"total_words\": total_words})\nwriter_totals = writer_totals[writer_totals[\"total_tags\"] > 50]\n\nprint(writer_totals.head())\n\nwriters = list(writer_totals[\"professional_id\"])\ntotal_tags = list(writer_totals[\"total_tags\"])\n\nnorm_writer_tags = {writer:normalize_dict(writer_tags[writer]) for writer in writers}\n\nnorm_writer_tags['f65c1eac3d2846d1a05206be08477272']","d3532b5c":"def _get_user_ranking_tag_similarity(words, other):\n    \"\"\" (list of string, word bag) -> Error\n    Gets an ordering of users to recommend. First user is most recommended.\n    \"\"\"\n    # Read it into a normalized word frequency dictionary\n    word_bag = defaultdict(int)\n    for word in words:\n        word_bag[word] += 1\n    word_bag = normalize_dict(word_bag)\n    \n    error = 0 \n    for word in word_bag:\n        pw = other[word] if word in other else 0\n        pm = word_bag[word]\n        error += (pw - pm) ** 2\n    \n    return error\n\n\ndef get_user_ranking_tag_similarity(words, tags):\n    \"\"\"\n    Given a list of words and a list of tags, ranks all of the professionals by what kind of questions they answered before\n    \"\"\"\n    ranking = []\n    for writer in norm_writer_tags:\n        error = _get_user_ranking_tag_similarity(tags, norm_writer_tags[writer])\n#         error += _get_user_ranking_tag_similarity(words, norm_writer_words[writer]) # Uncomment when I implement this\n        ranking.append((writer, error))\n    \n    ranking = sorted(ranking, key=lambda x: x[1], reverse=True)\n    \n    # Thanks stack overflow, gets the ranking\n    users, scores = zip(*ranking)\n    scores = [max(scores) - score for score in scores]\n    \n    ranking = list(Ranking(scores, start=1, reverse=True).ranks())\n    \n    ranking_df = pd.DataFrame({\"prof_id\": users, \"error\": scores, \"rank\":ranking})\n    \n    return ranking_df\n    \n    \nresults = get_user_ranking_tag_similarity([\"college\", \"college\", \"professor\"], [\"college\", \"college\", \"professor\"])\nresults.sample(10)\n    ","09e41e33":"# Career Village Predictor\n\nMatching aspiring students to professional help. This works by matching the tags in the question to professionals who have dealt with those tags before.\n\n","569f9ee2":"# Clustering with questions\n\nThe first thing we want to do is to try and get a feel for what kind of words are unique to each professional. To do this, we sum up the occurence of each word for a particular Professional\/Question. We then know a word is unique to the professional if that word uses up significantly more of a person's lexicon than average.\n\nBefore we can look at that though, we need to load in everything. There are a ton of possibly useful features to look at, but for the time being I will ignore them for the sake of simplicity. ","8ea97bf5":"Again, many of these features may be useful on a second pass-through.","59cba8c0":"Much better. There are now 10063 users that fit into 157 locations and 525 industries. Lets try and take a deeper look at the professionals themselves. What kind of words do they respond to? We are going to create a mapping of professional, to their personal word bucket they react to.","afe68f3d":"Great! Now we have a convenient table containing questions, answers, and the professional's industry. Now lets take a look at what kind of industries we have access to.","2c61ff2e":"Now that we have all of it loaded in, we want to merge all of this into one table for easier reading.","d4fac953":"If we aren't careful here, we may end up recommending people who are more active. Rather, we should choose people whose fraction of vocabulary better fits with the target demographic. We will still filter out inactive and new users, since their sample of words is too small to make meaningful predictions.","793127ae":"First things first, we want to load in everything and import all the things.","9a5d441b":"And now, we can begin to make some recommendations. We will parse the words in a new message, check it against all of the authors, once we do that, we return a list, in order of how well we beleive the question matches the writer's experience. We use the following error function to rank them. We sum up over all words that match up between the message and the existing word cloud for the author.\n\n$$ E = \\sum^{words}_n (p^w_i - p^m_i)^2 + \\sum^{tags}_n (p^t_i - p^m_i)^2$$\n\nWhere $p^w$ is the word frequency, $p^t$ is that tag frequency, and $p^m$ is the frequency of the message. If $p^t$ or $p^w$ are missing, they are assumed to be $0$.","e89ea134":"It appears that there are a lot of professionals on this site (unsurprisingly). Lets see if we can improve this."}}