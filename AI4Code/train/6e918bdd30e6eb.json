{"cell_type":{"630fe05a":"code","1fddec7a":"code","479e0044":"code","04ec14dd":"code","d8261089":"code","c1936cdc":"code","cc316bc8":"code","c72d6edd":"code","e9145b45":"code","53b439c9":"code","5b4c42d0":"code","5533c67c":"code","cf9c9128":"code","b110c4b2":"code","7bde4ade":"code","68f3b55b":"code","0e5fae8f":"code","faa1c0b1":"code","c7952d2a":"code","fe3cf3ae":"code","1a11abfc":"code","623d419e":"code","63003947":"code","850d4f2d":"code","5b2e97d1":"code","6b2d0e55":"code","30683905":"code","865434c3":"code","fa0d7117":"code","3b0288c1":"code","7789dbd2":"code","c8ff006a":"code","c0d10335":"code","f41caee7":"code","291f18b1":"code","ff214f04":"code","27461279":"code","c5e02352":"code","ea96e010":"code","43dd166d":"code","cf85e48d":"code","33f6a33e":"code","ad48b602":"code","a6b2f893":"code","92ef141b":"code","9fb4b5f5":"code","3e8c24ab":"code","2d9097a0":"code","ff477d3f":"code","e0c16a9b":"code","8dfa4576":"code","b308709d":"code","c3b4aa97":"code","72bff7b5":"code","bed0b693":"code","8b189212":"code","d46fd818":"code","606d74c1":"code","c9531e17":"code","9e1c690a":"code","c0d29bb2":"code","1b18b70d":"code","14bcd410":"code","1982f59e":"code","344986fb":"code","a85f9e63":"code","f7b54ae4":"code","7fbcf2e3":"code","18707ae5":"code","00c149f8":"code","2056e705":"code","4fca5518":"code","e542925f":"code","0878cb9b":"code","765dd424":"code","ca25c30d":"code","1dee4417":"code","0bb3c288":"code","51d1f862":"code","0167b88a":"code","0f957263":"code","46ddf49f":"code","9cb30463":"markdown","64656b8c":"markdown","4ec2e049":"markdown","338f92eb":"markdown","f8c28a00":"markdown","492918d9":"markdown","444c4611":"markdown","5d9e540c":"markdown","afbc73e1":"markdown","8f212dfc":"markdown","23ae05ac":"markdown","27bec620":"markdown","b96c4346":"markdown","d52e94ab":"markdown","4bd66309":"markdown","9f91ab18":"markdown","aafda9e8":"markdown","b62531a2":"markdown","ada580a3":"markdown","edee36b5":"markdown","ec3aa3fa":"markdown","fd5d30fc":"markdown","ac6d321a":"markdown","3cc3ea75":"markdown","a7209b75":"markdown","16624e11":"markdown","8b01c36e":"markdown","346cba52":"markdown","1ecd3f4e":"markdown","3d363895":"markdown","34029842":"markdown","a0fb152e":"markdown","be13ac49":"markdown","11c7e98e":"markdown","3623ab37":"markdown","7f6926cf":"markdown","8e71c73e":"markdown"},"source":{"630fe05a":"import numpy as np\nimport pandas as pd\nimport datetime\nimport random\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Models\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\n# Stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n# Misc\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\n\npd.set_option('display.max_columns', None)\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000","1fddec7a":"df_train=pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv')\ndf_test=pd.read_csv(\"..\/input\/home-data-for-ml-course\/test.csv\")","479e0044":"corrmat=df_train.corr()\nf, ax = plt.subplots(figsize=(10,12))\nsns.heatmap(corrmat,mask=corrmat<0.75,linewidth=0.5,cmap=\"Blues\", square=True)\n\n","04ec14dd":"corrmat=df_train.corr()\ncorrmat['SalePrice'].sort_values(ascending=False).head(10)\n\n","d8261089":"k = 10 \ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nf, ax = plt.subplots(figsize=(8,10))\nsns.set(font_scale=1.5)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 12}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","c1936cdc":"df_train.plot.scatter(x='GrLivArea', y='SalePrice')","cc316bc8":"df_train.sort_values(by = 'GrLivArea', ascending = False)[:2]\ndf_train=df_train.drop(df_train[df_train['Id']==1299].index)\ndf_train=df_train.drop(df_train[df_train['Id']==524].index)\n\n","c72d6edd":"df_train.plot.scatter(x='TotalBsmtSF', y='SalePrice')","e9145b45":"df_train.sort_values(by='TotalBsmtSF',ascending=False)[:2]\n#df_train.drop(df_train[df_train['Id']==333].index,inplace=True)\ndf_train[df_train['TotalBsmtSF']>2000].sort_values(by='SalePrice',ascending=True)[:2]\ndf_train.drop(df_train[df_train['Id']==1224].index,inplace=True)","53b439c9":"df_train.plot.scatter(x='TotalBsmtSF', y='SalePrice')","5b4c42d0":"f, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=\"OverallQual\", y=\"SalePrice\", data=df_train)\nfig.axis(ymin=0, ymax=800000);","5533c67c":"df_train.drop(df_train[(df_train.OverallQual==4) & (df_train.SalePrice>200000)].index,inplace=True)\n#df_train.drop(df_train[(df_train.OverallQual==8) & (df_train.SalePrice>500000)].index,inplace=True)","cf9c9128":"df_train.plot.scatter(x='LotFrontage', y='SalePrice')","b110c4b2":"df_train.drop(df_train[df_train['LotFrontage'] > 200].index,inplace=True)","7bde4ade":"df_train.plot.scatter(x='LotArea', y='SalePrice')","68f3b55b":"df_train[df_train['LotArea']>100000]['OverallQual']","0e5fae8f":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageArea', 'TotalBsmtSF', 'YearBuilt']\nsns.pairplot(df_train[cols], size = 3)\nplt.show()","faa1c0b1":"#df_train.drop(df_train[df_train['TotalBsmtSF'] > 3000].index,inplace=True)","c7952d2a":"# This outlier is subjective, if you think houses with larger basements than ground level should be rare\n#df_train[df_train['GrLivArea']<1500].sort_values(by='TotalBsmtSF',ascending=False)[:1]\n#df_train.drop(df_train[df_train['Id']==154].index,inplace=True)","fe3cf3ae":"df_train.plot.scatter(y='TotalBsmtSF', x='GrLivArea')","1a11abfc":"df_train.plot.scatter(x='YearBuilt', y='SalePrice')","623d419e":"df_train.drop(df_train[(df_train.YearBuilt < 1900) & (df_train.SalePrice > 200000)].index,inplace=True)\ndf_train.drop(df_train[(df_train.YearBuilt < 2000) & (df_train.SalePrice > 650000)].index,inplace=True)\ndf_train.plot.scatter(x='YearBuilt', y='SalePrice')","63003947":"corrmat1=df_train.corr()\ncorrmat1['SalePrice'].sort_values(ascending=False).head(10)","850d4f2d":"# concatenate training and testing sets to create new features and fill in missing values\ntarget=df_train['SalePrice'].reset_index(drop=True)\ntrainx=df_train.drop(['SalePrice'],1)\nall_features=pd.concat([trainx,df_test]).reset_index(drop=True)","5b2e97d1":"total = all_features.isnull().sum().sort_values(ascending=False)\npercent = (all_features.isnull().sum()\/all_features.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(25)","6b2d0e55":"missing_data['Percent'].head(15).plot.bar()","30683905":"all_features['LotFrontage'] = all_features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\nall_features['MSSubClass'] = all_features['MSSubClass'].apply(str)\nall_features['GarageArea'] = all_features.groupby('Neighborhood')['GarageArea'].transform(lambda x: x.fillna(x.mean()))\nall_features['MSZoning'] = all_features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\nall_features['YrSold'] = all_features['YrSold'].astype(str)\nall_features['MoSold'] = all_features['MoSold'].astype(str)\nall_features['Functional'] = all_features['Functional'].fillna('Typ')\nall_features['Electrical'] = all_features['Electrical'].fillna(\"SBrkr\")\nall_features['KitchenQual'] = all_features['KitchenQual'].fillna(\"TA\")\nall_features['Exterior1st'] = all_features['Exterior1st'].fillna(all_features['Exterior1st'].mode()[0])\nall_features['Exterior2nd'] = all_features['Exterior2nd'].fillna(all_features['Exterior2nd'].mode()[0])\nall_features['SaleType'] = all_features['SaleType'].fillna(all_features['SaleType'].mode()[0])\nall_features['MSZoning'] = all_features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\n#all_features = all_features.drop(['Functional','Electrical','Alley','RoofStyle','RoofMatl','GarageYrBlt',\n#'Street','GarageFinish','BsmtExposure','BsmtFinType2','BsmtFinType1','BsmtFinSF1','BsmtFinSF2','Utilities','LandContour','LotShape'],1)\n","865434c3":"objects = []\nfor i in all_features.columns:\n    if all_features[i].dtype == object:\n        objects.append(i)\nall_features.update(all_features[objects].fillna('None'))\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in all_features.columns:\n    if all_features[i].dtype in numeric_dtypes:\n        numeric.append(i)\nall_features.update(all_features[numeric].fillna(0))\n","fa0d7117":"# Important categorical features\nfrom matplotlib.ticker import MaxNLocator\ncat_feats=['MSZoning','Neighborhood','Condition1','BldgType','MasVnrType','ExterQual','BsmtQual','GarageQual']\ndef srt_box(y='SalePrice', df=df_train):\n    fig, axes = plt.subplots(4, 2, figsize=(20,30))\n    axes = axes.flatten()\n\n    for i, j in zip(cat_feats, axes):\n\n        sortd = df.groupby([i])[y].median().sort_values(ascending=False)\n        sns.boxplot(x=i,\n                    y=y,\n                    data=df,\n                    palette='plasma',\n                    order=sortd.index,\n                    ax=j)\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=14))\n\n        plt.tight_layout()","3b0288c1":"srt_box()","7789dbd2":"# Creating new features\nall_features['YearRemodAdd']=all_features['YearRemodAdd'].astype(int)\nall_features['Years_Since_Remod'] = all_features['YrSold'].astype(int) - all_features['YearRemodAdd'].astype(int)\n    \nall_features['Age']=all_features['YrSold'].astype(int) - all_features['YearBuilt'].astype(int)\nall_features['Newness']=all_features['Age']*all_features['Years_Since_Remod']\n    \nall_features['Total_Home_Quality'] = all_features['OverallQual'] + all_features['OverallCond']\n\nfeats=['2ndFlrSF','GarageArea','TotalBsmtSF','Fireplaces','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch']\nfor col  in feats:\n    name='Has_'+str(col)\n    all_features[name]=all_features[col].apply(lambda x: 1 if x > 0 else 0)\n    \nall_features['Total_porch_sf'] = (all_features['OpenPorchSF'] + all_features['3SsnPorch'] +all_features['EnclosedPorch'] \n                                  + all_features['ScreenPorch'] +all_features['WoodDeckSF'])\n\nall_features['Bsmt_Baths'] = all_features['BsmtFullBath'] + (0.5 * all_features['BsmtHalfBath'])\n\nall_features['Total_BathAbvGrd'] = (all_features['FullBath'] + (0.5 * all_features['HalfBath']))\n# Assume bathrooms are half size of other rooms\nall_features['AvgRoomSize']=all_features['GrLivArea']\/(all_features['TotRmsAbvGrd']+(0.4*all_features['Total_BathAbvGrd']))\n#Also captures size of rooms:\nall_features['BedBath']=all_features['BedroomAbvGr']*all_features['Total_BathAbvGrd']\n\nall_features['TotalLot'] = all_features['LotFrontage'] + all_features['LotArea']\nall_features['sqft_feet_living']=all_features['TotalBsmtSF']+all_features['GrLivArea']\n\nall_features.drop(['Id','BsmtFullBath','BsmtHalfBath'],1,inplace=True)\n\n# Mapping neighborhood unique values according to the shades of the box-plot, nearly same median values\nneigh_map={'None': 0,'MeadowV':1,'IDOTRR':1,'BrDale':1,\n        'OldTown':2,'Edwards':2,'BrkSide':2,\n        'Sawyer':3,'Blueste':3,'SWISU':3,'NAmes':3,\n        'NPkVill':4,'Mitchel':4,'SawyerW':4,\n        'Gilbert':5,'NWAmes':5,'Blmngtn':5,\n        'CollgCr':6,'ClearCr':6,'Crawfor':6,\n        'Somerst':8,'Veenker':8,'Timber':8,\n         'StoneBr':10,'NoRidge':10,'NridgHt':10 } \nall_features['Neighborhood'] = all_features['Neighborhood'].map(neigh_map)\n# Quality maps for external and basement\n\nbsm_map = {'None': 0, 'Po': 1, 'Fa': 4, 'TA': 9, 'Gd': 16, 'Ex': 25}\n#ordinal_map = {'Ex': 10,'Gd': 8, 'TA': 6, 'Fa': 5, 'Po': 2, 'NA':0}\nord_col = ['ExterQual','ExterCond','BsmtQual', 'BsmtCond','HeatingQC','KitchenQual','GarageQual','GarageCond', 'FireplaceQu']\nfor col in ord_col:\n    all_features[col] = all_features[col].map(bsm_map)\nall_features.shape\n","c8ff006a":"all_features[all_features['YrSold'].astype(int) < all_features['YearRemodAdd'].astype(int)]","c0d10335":"all_features.at[2284,'Years_Since_Remod']=0\nall_features.at[2538,'Years_Since_Remod']=0\nall_features.at[2538,'Age']=0\n\n\n\n","f41caee7":"#multi-collinearity\nall_features.drop(['GarageYrBlt','TotRmsAbvGrd','1stFlrSF','LotFrontage'], axis=1, inplace=True)\n\n#Missing values\nall_features.drop(['PoolQC','MiscFeature','Alley'], axis=1, inplace=True)\n\n# weakly correlated\nall_features.drop(['MoSold','YrSold'], axis=1, inplace=True)\n\n#features with same value\noverfit_cat = []\nfor i in all_features.columns:\n    counts = all_features[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(all_features) * 100 > 97:\n        overfit_cat.append(i)\noverfit_cat = list(overfit_cat)\n#all_features.drop(overfit_cat,1,inplace=True)\nprint(overfit_cat)\n\n","291f18b1":"overfit_cat=['Street', 'Utilities', 'Condition2', 'RoofMatl', 'Heating', 'LowQualFinSF', '3SsnPorch', 'Has_TotalBsmtSF', 'Has_3SsnPorch']\nall_features.drop(overfit_cat,1,inplace=True)","ff214f04":"all_features.isnull().sum().sort_values(ascending=True).head()","27461279":"all_features.shape","c5e02352":"numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in all_features.columns:\n    if all_features[i].dtype in numeric_dtypes:\n        numeric.append(i)\n","ea96e010":"skew_features = all_features[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index","43dd166d":"print(skew_features.head(5))\n#skew_features.tail(10)","cf85e48d":"for i in skew_index:\n    all_features[i] = boxcox1p(all_features[i], boxcox_normmax(all_features[i] + 1))\n    \n    \n    ","33f6a33e":"skew_features1 = all_features[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)\nprint(skew_features1.head(3))","ad48b602":"skew_features1.head(10)","a6b2f893":"target = np.log1p(df_train['SalePrice']).reset_index(drop=True)\nsns.distplot(target, fit=norm);\nfig = plt.figure()\nfrom scipy import stats\nres = stats.probplot(target, plot=plt)","92ef141b":"def logs(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values)   \n        res.columns.values[m] = l + '_log'\n        m += 1\n    return res\n\nlog_features = ['BsmtUnfSF', 'TotalBsmtSF', 'GrLivArea', 'FireplaceQu', 'GarageArea', 'OpenPorchSF', 'EnclosedPorch', \n                'ScreenPorch', 'Years_Since_Remod', 'Newness','Total_Home_Quality', 'Total_porch_sf', 'AvgRoomSize', \n                'TotalLot', 'sqft_feet_living','BsmtFinSF1','BedBath']\nall_features1=all_features\nloged_features = logs(all_features, numeric)\nall_features=logs(all_features,log_features)","9fb4b5f5":"feats=['2ndFlrSF','GarageArea','Fireplaces','WoodDeckSF','OpenPorchSF','EnclosedPorch','ScreenPorch']\nfor col  in feats:\n    name1='Has_'+str(col) + '_log'\n    loged_features.drop(name1,1,inplace=True)","3e8c24ab":"for o in ord_col:\n    name=str(o)+'_log'\n    loged_features.drop(name,1,inplace=True)\nloged_features.drop('Neighborhood_log',1,inplace=True)\nloged_features.head()","2d9097a0":"collist=loged_features.columns[-36:]\n","ff477d3f":"qfeat=[]\nfor col in collist:\n    name=str(col)\n    q=name[:-4]\n    qfeat.append(q)","e0c16a9b":"squared_features = ['Years_Since_Remod','Total_Home_Quality','Total_porch_sf','AvgRoomSize',\n                'TotalBsmtSF','GrLivArea','BedBath','YearBuilt',\n                'Fireplaces','GarageArea','MasVnrArea']\nsquared_features=log_features\nsqrd_features=[]\nfor l in squared_features:\n    a=l+str(\"_log\")\n    sqrd_features.append(a)\n\n\ndef squares(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(res[l]*res[l]).values)   \n        res.columns.values[m] = l + '_sq'\n        m += 1\n    return res \n\n\nsq_features = squares(all_features, qfeat)\n\nlog_sq_cols=squares(loged_features,qfeat)\n\nall_features=squares(all_features,sqrd_features)\n\ndef sqrt(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(np.sqrt(res[l])).values)   \n        res.columns.values[m] = l + '_sqroot'\n        m += 1\n    return res \ndef cube(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(res[l]**3).values)   \n        res.columns.values[m] = l + '_cube3'\n        m += 1\n    return res \n\nroot_features = sqrt(sq_features, qfeat)\ncube_features = cube(sq_features, qfeat)\nvar_feats=cube(log_sq_cols,qfeat)\nvar_feats=sqrt(var_feats,qfeat)","8dfa4576":"cube_features.isnull().sum().sort_values(ascending=False).head()\nvar_feats.isnull().sum().sort_values(ascending=False).head()","b308709d":"all_features=pd.get_dummies(all_features).reset_index(drop=True)\nall_features1=pd.get_dummies(all_features1).reset_index(drop=True)\nloged_features=pd.get_dummies(loged_features).reset_index(drop=True)\ncube_features=pd.get_dummies(cube_features).reset_index(drop=True)\nvar_feats=pd.get_dummies(var_feats).reset_index(drop=True)\nlog_sq_cols=pd.get_dummies(log_sq_cols).reset_index(drop=True)","c3b4aa97":"def get_splits(all_features,target): \n    df=pd.concat([all_features,target],1)\n    X_train=df.iloc[:len(target),:]\n    X_test=all_features.iloc[len(target):,:]\n    return X_train,X_test\ndef get_valid(df,target,valid_fraction=0.2):\n    validrows=int(len(df)*valid_fraction)\n    trains=df[:-validrows]\n    valids=df[-validrows:]\n    feature_col=df.columns.drop(target)\n    return trains,valids,feature_col\n\n# Split for feature engineering:\nX_train,X_test=get_splits(all_features,target)\ntrain,valid,feature_col=get_valid(X_train,'SalePrice')\n\nX_train0,X_test0=get_splits(all_features1,target)\ntrain0,valid0,feature_col0=get_valid(X_train0,'SalePrice')\n\nX_train1,X_test1=get_splits(log_sq_cols,target)\ntrain1,valid1,feature_col1=get_valid(X_train1,'SalePrice')\n\nX_tr, X_te=get_splits(var_feats,target)\nfeat_col=X_tr.columns.drop('SalePrice')\n\ntr_log,te_log=get_splits(loged_features,target)\nlog_feats=tr_log.columns.drop('SalePrice')","72bff7b5":"corr1=X_tr.corr()\ncorr1[\"SalePrice\"].sort_values(ascending=False).head(15)","bed0b693":"best_columns=['sqft_feet_living','GarageArea_log_sq','Age','BedBath','AvgRoomSize_log_sq','TotalLot_log_sq','Total_porch_sf_log_sq']\nfig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12,20))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\nsns.color_palette(\"husl\", 8)\nfor i, feature in enumerate(list(X_train[best_columns]), 1):\n    plt.subplot(len(list(best_columns)), 2, i)\n    sns.scatterplot(x=feature, y='SalePrice', hue=None, data=X_train)\n        \n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 10})\n        \nplt.show()\n","8b189212":"from sklearn.linear_model import Lasso","d46fd818":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\nkf = KFold(n_splits=12, random_state=7, shuffle=True)\ndef cv_rmse(model, X=X_train,target=target):\n    rmse = np.sqrt(-cross_val_score(model, X, target, scoring=\"neg_mean_squared_error\", cv=kf,n_jobs=-1))\n    return (rmse)\n\n#ridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 20, 30,40, 50, 100]","606d74c1":"feature_col.shape","c9531e17":"## Function used for submitting predictions in the Kaggle competition scorer\ndef submission(preds,pred):\n    submission=pd.read_csv(\"..\/input\/home-data-for-ml-course\/sample_submission.csv\")\n    submission.iloc[:,1] = np.floor(np.expm1(preds))\n\n    q1 = submission['SalePrice'].quantile(0.0045)\n    q2 = submission['SalePrice'].quantile(0.99)\n    submission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\n    submission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\n    #submission.to_csv(\"%s1.csv\" %pred, index=False)\n    # Scale predictions\n    submission['SalePrice'] *= 1.001619\n    submission.to_csv('%s mycsvfile.csv'%pred,index=False)","9e1c690a":"from sklearn.linear_model import LassoCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\nX,y=X_train1[feature_col1],target\n\nalphas = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008,5e-3,5e-2]\nXscaled=RobustScaler().fit_transform(X)\nlassopipe =LassoCV(max_iter=1e7, alphas=alphas, random_state=42,n_jobs=-1)   \n\nclf=lassopipe.fit(Xscaled,y)\nimportance=np.abs(clf.coef_)\nframe=pd.DataFrame(importance,index=feature_col1)\nframe.sort_values(by=0,ascending=False).head(10)","c0d29bb2":"imps=frame.sort_values(by=0,ascending=False).index","1b18b70d":"log_sq_cols.shape","14bcd410":"# Using all features\nfrom sklearn.pipeline import Pipeline\nalphas=np.linspace(1,stop=50)\npipe=Pipeline([('scaler',RobustScaler()), ('ridgemodel',RidgeCV(alphas=alphas, \n                                                                cv=None,store_cv_values=True ))])\nmodel=pipe.fit(X_train1[feature_col1],target)\nprint('Alpha: %f'%model['ridgemodel'].alpha_)\nprint('Score: %f'%np.sqrt(-model['ridgemodel'].best_score_))\n\n","1982f59e":"# Using selected features\npred=model.predict(X_test1[feature_col1])\n#submission(pred,pred='ridge2319')\n\n","344986fb":"alphas=np.linspace(1,stop=50,num=50)\npipe=Pipeline([('scaler',RobustScaler()), ('ridgemodel',RidgeCV(alphas=alphas, cv=None,store_cv_values=True))])\nnumfeats=[298,290,280,275,270,260,250]\nfor n in numfeats:\n    model=pipe.fit(X_train1[imps[:n]],target)\n    print(n)\n    print('Alpha: %f'%model['ridgemodel'].alpha_)\n    print('Score: %f'%np.sqrt(-model['ridgemodel'].best_score_))\n    print('')","a85f9e63":"model=pipe.fit(X_train1[imps[:280]],target)\nridgepred=model.predict(X_test1[imps[:280]])\n#submission(ridgepred,'ridge1619')\n\n\n","f7b54ae4":"lightgbm = LGBMRegressor(objective='regression', num_leaves=5,learning_rate=0.007, n_estimators=3500,max_bin=163,\n                       bagging_fraction=0.35711,bagging_freq=4, bagging_seed=8,feature_fraction=0.1294, feature_fraction_seed=8,\n                       min_data_in_leaf = 8,  verbose=-1, random_state=42,n_jobs=-1)\nlightgbmod=lightgbm.fit(X_train[feature_col],target)\nlightpred=lightgbmod.predict(X_test[feature_col])\n                   ","7fbcf2e3":"(np.sqrt(-cross_val_score(lightgbm, X_train[feature_col], target, scoring=\"neg_mean_squared_error\", cv=5))).mean()","18707ae5":"#submission(lightpred,'lgpreds258')","00c149f8":"from sklearn.pipeline import make_pipeline\nsvr = make_pipeline(RobustScaler(),\n                    SVR(C=21, epsilon=0.0099, gamma=0.00017, tol=0.000121))\nsvrmodel=svr.fit(X_train[feature_col],target)\nsvrpred=svrmodel.predict(X_test[feature_col])\n\n","2056e705":"(cv_rmse(svr,X_train[feature_col])).mean()","4fca5518":"\n#submission(svrpred,pred='svrpred258')","e542925f":"xgboost = XGBRegressor(\n    learning_rate=0.0139,\n    n_estimators=4500,\n    max_depth=4,\n    min_child_weight=0,\n    subsample=0.7968,\n    colsample_bytree=0.4064,\n    nthread=-1,\n    scale_pos_weight=2,\n    seed=42,\n)\nxgboo=xgboost.fit(X_train[feature_col],target)\nxgboostpred=xgboo.predict(X_test[feature_col])\n\n\n","0878cb9b":"stack_gen = StackingCVRegressor(regressors=(lightgbm, pipe,svr,xgboost),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)\nstack=stack_gen.fit(np.array(X_train[feature_col]),np.array(target))\nstackpreds=stack.predict(np.array(X_test[feature_col]))\n\n\n","765dd424":"(cv_rmse(xgboost,X_train[feature_col])).mean()","ca25c30d":"subm= (0.2 * lightpred)+(0.4 * ridgepred)+(0.1 * stackpreds)+(0.3*svrpred)","1dee4417":"submission(subm,pred='blended1705')","0bb3c288":"# First lets look at importance a naive model will attribute\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfc1=all_features1.drop(['sqft_feet_living','Age','Newness','Years_Since_Remod','Total_Home_Quality','TotalLot',\n                   'Total_porch_sf','Bsmt_Baths','Total_BathAbvGrd','AvgRoomSize','BedBath'],1)\nfc=fc1.columns\n\nalphas=np.linspace(1,stop=50,num=50)\npipe0=Pipeline([('scaler',RobustScaler()), ('ridgemodel',RidgeCV(alphas=alphas, \n                                                                 cv=None,\n                                                                 store_cv_values=True))])\nmodel0=pipe0.fit(X_train0[fc],target)\n\nperm0 = PermutationImportance(model0['ridgemodel'], random_state=1,cv=5).fit(X_train0[fc], \n                                                                             target)\neli5.show_weights(perm0, feature_names = X_train0[fc].columns.tolist())\n\n","51d1f862":"# Importance attributed by best performing ridge model\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nalphas=np.linspace(1,stop=50,num=50)\npipe1=Pipeline([('scaler',RobustScaler()), ('ridgemodel',RidgeCV(alphas=alphas, \n                                                                 cv=None,\n                                                                 store_cv_values=True))])\nx=X_train1[imps[:270]]\nmodel1=pipe1.fit(x,target)\n\nperm1 = PermutationImportance(model1['ridgemodel'], random_state=1,cv=5).fit(x, target)\n\neli5.show_weights(perm1, feature_names =x.columns.tolist())\n","0167b88a":"#.drop(['YearRemodAdd','Newness','Age_sq','Years_Since_Remod_log','2ndFlrSF_log','sqft_feet_living_sq','ScreenPorch_sq'],1)\n","0f957263":"import shap  # package used to calculate Shap values\n\nexplainer = shap.LinearExplainer(model1['ridgemodel'],X_train1[imps[:270]])\nshap_values = explainer.shap_values(X_train1[imps[:270]])\nshap.summary_plot(shap_values, X_train1[imps[:270]],max_display=6)\n","46ddf49f":"x=tr_log[log_feats].drop(['2ndFlrSF','TotalBsmtSF','GarageArea','BsmtFinSF1','BsmtUnfSF'],1)\nmodel1=pipe1.fit(x,target)\nexplainer = shap.LinearExplainer(model1['ridgemodel'],x)\nshap_values = explainer.shap_values(x)\nshap.summary_plot(shap_values, x,max_display=10)\n\n","9cb30463":"XGBoost parameters from reference notebook, were hypertuned using CV but it took long time to run.","64656b8c":"For selecting most important features, lasso regression is performed on various alpha values and optimum features are chosen according to RMSE score on Ridge model. The Ridge alpha is hypertuned by using CV.","4ec2e049":"Thus, the model generalizes best score when the number of features are 270","338f92eb":"First remove outliers from the highly correlated variables and then decide which to keep and create. Manually examining peculiarities in relationships between highly correlated variables.","f8c28a00":"# Missing Data","492918d9":"## Feature Transformation\n\nSquaring and logging some important features to explore degrees of relationships","444c4611":"##### Code this cell to get summary plot for XGboost, it needs python 3.8 and tensorflow package when using GradientExplainer\nimport xgboost\nfrom sklearn.model_selection import train_test_split\nxgb_full = xgboost.DMatrix(X_train[feature_col], label=target)\nXg_train, Xg_test, yg_train, yg_test = train_test_split(X_train[feature_col], target, test_size=0.2, random_state=7)\nxgb_train = xgboost.DMatrix(Xg_train, label=yg_train)\nxgb_test = xgboost.DMatrix(Xg_test, label=yg_test)\nparams = {\n    'learning_rate':0.0139,\n    'n_estimators':4500,\n    'max_depth':4,\n    'min_child_weight':0,\n    'subsample':0.7968,\n    'colsample_bytree':0.4064,\n    'nthread':-1,\n    'scale_pos_weight':2,\n    'seed':42,\n}\nmodelxg = xgboost.train(params, xgb_train, 4500, [(xgb_train, \"train\"),(xgb_test, \"valid\")], early_stopping_rounds=5, verbose_eval=25)\nshap.initjs()\n\nshap_values1 = shap.TreeExplainer(modelxg).shap_values(xgb_test)\n\nshap.summary_plot(shap_values1, xgb_test,max_display=12)","5d9e540c":"Now variables that are highly correlated to SalePrice:","afbc73e1":"As can be seen in the plot between TotalBsmtSF and GrLivArea, there are few observations for basement area being larger than ground level area.","8f212dfc":"### Support Vector Regression","23ae05ac":"The prices above 200k before 1900 and ones more than 600k before 2000 seem like outliers to the relationship","27bec620":"### LIghtGBM Regressor","b96c4346":"# Modelling","d52e94ab":"Dropping outliers in the highly correlated variables","4bd66309":"### XGBoost and Stacking regressor","9f91ab18":"### Normalizing features","aafda9e8":"Submit blended predictions according to various trial and error weightages ","b62531a2":"Naturally the sale price is impacted greatly by house having a large 2nd floor area which also means the house has large ground floor area. However, the squared spacce features also have higher range of absolute values. These model output impact plots can also be plotted for particular features individually.\n\n\n","ada580a3":"Test Score 12284","edee36b5":"Constructing a correlation matrix to find features that are closely related to target variable.","ec3aa3fa":"Naturally, overall quality of the house is strongly correlated with the price. However, it is not defined how this measure was calculated. Other features closely related are space considerations(Gr Liv Area, Total Basement area, Garage area) and how old the house is(Year Built).","fd5d30fc":"# Introduction and objectives\nThis is a notebook by an absolute beginner in data science and any suggestions are appreciated. \n\nThe objective of this notebook is to help beginners develop their intuition for machine learning regression techniques and feature engineering. \nThe high score is obtained partly using a blended model, however the ridge model alone obtains a score of 12,727. The blended obtains a score of 12,284.\n \nSome commands were kept as comments as they made sense at first but did not show a better score. Maybe reader can use these comments. Most of them are in eliminating outliers by examining plots of features.\n\nIt is not recommended to use data leakage for gaining real practical experience and many notebooks found on the competition dashboard use it in between the modelling cells without mentioning. Thus, it should be noted this notebook does not use data leakage and neither do any of the references.\n\nReferences: \n\nFor understanding of features and introduction:\n1. COMPREHENSIVE DATA EXPLORATION WITH PYTHON, Pedro Marcelino - February 2017, link :https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n\nFor other models used and scaling features:\n2. How I made top 0.3% on a Kaggle competition, Lavanya Shukla, link :https:\/\/www.kaggle.com\/lavanyashukla01\/how-i-made-top-0-3-on-a-kaggle-competition\/notebook\n\nFor hypertuning models:\n3. Data Science Workflow TOP 2% (with Tuning), aqx, link :https:\/\/www.kaggle.com\/angqx95\/data-science-workflow-top-2-with-tuning\/notebook#4.-Modeling\n\nFor shap values and permutation importance:\n4. Machine Learning Explainability by Dan Becker, Kaggle Mini Course.","ac6d321a":"Submission of ridge model formed above","3cc3ea75":"Thus,we see that some features are highly skewed on positive side, hence transforming these using box-cox transformation.","a7209b75":"RMSE on test set using all features= 12906","16624e11":"Highly Inter-Correlated variables:\n1. GarageYrBlt and YearBuilt\n2. TotRmsAbvGrd and GrLivArea\n3. 1stFlrSF and TotalBsmtSF\n4. GarageArea and GarageCars","8b01c36e":"log_sq_cols test score 12723","346cba52":"# Ridge model","1ecd3f4e":"# Multivariate analysis","3d363895":"Thus, we see that our ridge model has better score on the test dataset provided owing to greater weighing of the features related to year house was built and renovated.\n\n\n\n","34029842":"Thus, we see that despite having larger Lot Area, these houses have a overall quality of 7, and average price for that quality is bit more than 200k as in the box plot. Thus, these houses may not be a harmful outlier to the model.","a0fb152e":"# Feature Engineering\nPart 1) Creating features deemed to be relevant by subjective analysis of features, such as:\n\n1. Years since Remodelling of the house.\n2. Total surface area=Total basement surface area+ 1st Floor SF + 2nd Floor SF\n3. Bathrooms =full bathrooms + 0.5* half bathrooms\n3. Average Room Size= Ground living area \/ (Number of rooms and bathrooms)\n4. Bedroom Bathroom tradeoff = Bedrooms * Bathrooms\n5. Porch Area = Sum of porch features\n6. Type of Porch = categorical feature\n7. The newness of the house captured by its age taking in factor of renovation. \n\nPart 2) Mapping important categorical features\n\nPart 3) Feature transformation, logaritmic and quadratic relationships","be13ac49":"### Blended model","11c7e98e":"The last two values are out of trend and are outliers.","3623ab37":"Thus, we see the correlations have increased for the variables after removing outliers. Next step would be to create new features that signify the value out of similar ones and eliminate similar features. This will ensure feature importance is not squandered among similar features.","7f6926cf":"Examining highly correlated variables","8e71c73e":"Transforming distribution of SalePrice to a normal distribution."}}