{"cell_type":{"66c09f64":"code","e22a0dfa":"code","30d535ff":"code","5a017742":"code","5a42ea4b":"code","0bde42aa":"code","a20d58ad":"code","aa98016c":"code","645b2c50":"code","8df5d393":"code","4a44977c":"code","64b82a0d":"code","c6ac1533":"code","bb52f3b9":"code","a777b5b5":"code","c2902cdf":"code","af7917ca":"code","18e028d5":"code","19626715":"code","04a14947":"code","df566fa1":"code","46d6a655":"code","c426e4f4":"code","dc370735":"code","622b014c":"code","cea67f27":"code","6c994244":"code","4774ab8e":"code","1160bbda":"code","94c5ba36":"code","1603c5c6":"code","89a2479c":"code","f9bcb564":"code","a9235411":"code","070620fa":"code","0cb1ac89":"code","fb9dcc7a":"code","626b5caa":"code","a7406774":"code","58683ffd":"code","66963cc6":"code","4fef8a4e":"code","907d524d":"code","8d1e4014":"code","061d35d0":"code","59f825a1":"markdown","499470b2":"markdown","6a76c8a8":"markdown","9c6405df":"markdown","fb7ee6ae":"markdown","905f0df8":"markdown","c0878197":"markdown","44da5e30":"markdown","bd2990a3":"markdown","96d6ad3c":"markdown","1e4bb035":"markdown","29890186":"markdown","6ba4c1a7":"markdown","8195c76d":"markdown","91c05547":"markdown","18675689":"markdown","d6f303dd":"markdown","f1b0a95d":"markdown","7c874eb0":"markdown","6594ed6f":"markdown","159273b9":"markdown","00964eca":"markdown","bd0b88d3":"markdown","f79bdfeb":"markdown","f7cd72c7":"markdown","c73d16d3":"markdown","b2bb7287":"markdown","8181276a":"markdown","9d955fd4":"markdown","ec3664a7":"markdown","455c5389":"markdown","bf1629d9":"markdown","06138046":"markdown","829e74a7":"markdown","2732da10":"markdown","cad8160b":"markdown","f4d94c7f":"markdown","67fb5b78":"markdown","b660fd70":"markdown","d60a86fd":"markdown","46916116":"markdown","44b03d8c":"markdown","def373bf":"markdown","a0ffe584":"markdown","8c45df36":"markdown"},"source":{"66c09f64":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.model_selection import KFold,cross_val_score, RepeatedStratifiedKFold,StratifiedKFold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.preprocessing import OneHotEncoder,StandardScaler,PowerTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.dummy import DummyClassifier\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport optuna\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import make_column_transformer\n\nfrom sklearn.model_selection import KFold, cross_val_predict, train_test_split,GridSearchCV,cross_val_score\nfrom sklearn.metrics import accuracy_score,classification_report\n\n#importing plotly and cufflinks in offline mode\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\n\nimport plotly \nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.offline as py\nfrom plotly.offline import iplot\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n\nimport missingno as msno\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\npd.set_option('max_columns',100)\npd.set_option('max_rows',900)\npd.set_option('max_colwidth',200)","e22a0dfa":"data = pd.read_csv(\"..\/input\/gender-classification\/Transformed Data Set - Sheet1.csv\")\ndata.head()","30d535ff":"data_1 = data.copy()","5a017742":"data_1[\"Gender\"] = data_1[\"Gender\"].map(lambda x: 0 if x==\"F\" else 1)","5a42ea4b":"df = data_1.copy()","0bde42aa":"df.head()","a20d58ad":"df.shape","aa98016c":"df.info()","645b2c50":"df.duplicated().sum()","8df5d393":"def missing (df):\n    missing_number = df.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values\n\nmissing(df)","4a44977c":"numerical= df.drop(['Gender'], axis=1).select_dtypes('number').columns\n\ncategorical = df.select_dtypes('object').columns\n\nprint(f'Numerical Columns:  {df[numerical].columns}')\nprint('\\n')\nprint(f'Categorical Columns: {df[categorical].columns}')","64b82a0d":"df[categorical].nunique()","c6ac1533":"y = df['Gender']\nprint(f'Percentage of female respondents:  {round(y.value_counts(normalize=True)[1]*100,2)} %  --> ({y.value_counts()[1]} patient)\\nPercentage of male respondents: {round(y.value_counts(normalize=True)[0]*100,2)}  %  --> ({y.value_counts()[0]} patient)')","bb52f3b9":"df.Gender.iplot(kind=\"hist\");","a777b5b5":"df[categorical].head()","c2902cdf":"df[\"Favorite Color\"].unique()","af7917ca":"df[df[\"Favorite Color\"] == \"Cool\"][\"Gender\"].value_counts(normalize=True)","18e028d5":"df[df[\"Favorite Color\"] == \"Cool\"][\"Gender\"].value_counts()","19626715":"df[df[\"Favorite Color\"] == \"Neutral\"][\"Gender\"].value_counts(normalize=True)","04a14947":"df[df[\"Favorite Color\"] == \"Neutral\"][\"Gender\"].value_counts()","df566fa1":"df[df[\"Favorite Color\"] == \"Warm\"][\"Gender\"].value_counts(normalize=True)","46d6a655":"df[df[\"Favorite Color\"] == \"Warm\"][\"Gender\"].value_counts()","c426e4f4":"print (f'A respondent, whose favorite color is cool, has a probability of {round(df[df[\"Favorite Color\"]==\"Cool\"][\"Gender\"].value_counts(normalize=True)[0], 2)} of being female and {round(df[df[\"Favorite Color\"]==\"Cool\"][\"Gender\"].value_counts(normalize=True)[1], 2)} of being male.')\n\nprint()\n\nprint (f'A respondent, whose favorite color is neutral, has a probability of {round(df[df[\"Favorite Color\"]==\"Neutral\"][\"Gender\"].value_counts(normalize=True)[0], 2)} of being female and {round(df[df[\"Favorite Color\"]==\"Neutral\"][\"Gender\"].value_counts(normalize=True)[1], 2)} of being male.')\n\nprint()\n\nprint (f'A respondent, whose favorite color is warm, has a probability of {round(df[df[\"Favorite Color\"]==\"Warm\"][\"Gender\"].value_counts(normalize=True)[0], 2)} of being female and {round(df[df[\"Favorite Color\"]==\"Warm\"][\"Gender\"].value_counts(normalize=True)[1], 2)} of being male.')","dc370735":"fig = px.histogram(data_frame=df, x=\"Favorite Color\", color=\"Gender\", width=400, height=400)\nfig.show()","622b014c":"df[\"Favorite Music Genre\"].unique()","cea67f27":"round(df.groupby(\"Favorite Music Genre\")[\"Gender\"].mean(), 2).sort_values(ascending=False)","6c994244":"fig = px.histogram(data_frame=df, x=\"Favorite Music Genre\", color=\"Gender\", width=400, height=400)\nfig.show()","4774ab8e":"df[\"Favorite Beverage\"].unique()","1160bbda":"round(df.groupby(\"Favorite Beverage\")[\"Gender\"].mean(), 2).sort_values(ascending=False)","94c5ba36":"fig = px.histogram(data_frame=df, x=\"Favorite Beverage\", color=\"Gender\", width=400, height=400)\nfig.show()","1603c5c6":"df[\"Favorite Soft Drink\"].unique()","89a2479c":"round(df.groupby(\"Favorite Soft Drink\")[\"Gender\"].mean(), 2).sort_values(ascending=False)","f9bcb564":"fig = px.histogram(data_frame=df, x=\"Favorite Soft Drink\", color=\"Gender\", width=400, height=400)\nfig.show()","a9235411":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_transformer\n\nfrom sklearn.pipeline import make_pipeline","070620fa":"accuracy =[]\nmodel_names =[]\n\n\nX= df.drop('Gender', axis=1)\ny= df['Gender']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nohe= OneHotEncoder()\nct= make_column_transformer((ohe,categorical),remainder='passthrough')  \n\n\nmodel = DummyClassifier(strategy='constant', constant=1)\npipe = make_pipeline(ct, model)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\naccuracy.append(round(accuracy_score(y_test, y_pred),4))\nprint (f'model : {model} and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n\nmodel_names = ['DummyClassifier']\ndummy_result_df = pd.DataFrame({'Accuracy':accuracy}, index=model_names)\ndummy_result_df","0cb1ac89":"accuracy =[]\nmodel_names =[]\n\n\nX= df.drop('Gender', axis=1)\ny= df['Gender']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\nohe= OneHotEncoder()\nct= make_column_transformer((ohe,categorical),remainder='passthrough')  \n\n\nlr = LogisticRegression(solver='liblinear')\n# lda= LinearDiscriminantAnalysis()\nsvm = SVC(gamma='scale')\nknn = KNeighborsClassifier()\n\nmodels = [lr,svm,knn]\n\nfor model in models: \n    pipe = make_pipeline(ct, model)\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    accuracy.append(round(accuracy_score(y_test, y_pred),4))\n    print (f'model : {model} and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n\nmodel_names = ['Logistic','SVM','KNeighbors']\nresult_df1 = pd.DataFrame({'Accuracy':accuracy}, index=model_names)\nresult_df1","fb9dcc7a":"accuracy =[]\nmodel_names =[]\n\n\nX= df.drop('Gender', axis=1)\ny= df['Gender']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nohe= OneHotEncoder()\ns= StandardScaler()\nct1= make_column_transformer((ohe,categorical),(s,numerical))  \n\n\nlr = LogisticRegression(solver='liblinear')\n# lda= LinearDiscriminantAnalysis()\nsvm = SVC(gamma='scale')\nknn = KNeighborsClassifier()\n\nmodels = [lr,svm,knn]\n\nfor model in models: \n    pipe = make_pipeline(ct1, model)\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    accuracy.append(round(accuracy_score(y_test, y_pred),4))\n    print (f'model : {model} and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n\nmodel_names = ['Logistic_scl','SVM_scl','KNeighbors_scl']\nresult_df2 = pd.DataFrame({'Accuracy':accuracy}, index=model_names)\nresult_df2","626b5caa":"accuracy =[]\nmodel_names =[]\n\n\nX= df.drop('Gender', axis=1)\ny= df['Gender']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nohe= OneHotEncoder()\nct= make_column_transformer((ohe,categorical),remainder='passthrough')  \n\nada = AdaBoostClassifier(random_state=0)\ngb = GradientBoostingClassifier(random_state=0)\nrf = RandomForestClassifier(random_state=0)\net=  ExtraTreesClassifier(random_state=0)\n\n\n\nmodels = [ada,gb,rf,et]\n\nfor model in models: \n    pipe = make_pipeline(ct, model)\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    accuracy.append(round(accuracy_score(y_test, y_pred),4))\n    print (f'model : {model} and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n\nmodel_names = ['Ada','Gradient','Random','ExtraTree']\nresult_df3 = pd.DataFrame({'Accuracy':accuracy}, index=model_names)\nresult_df3","a7406774":"accuracy =[]\nmodel_names =[]\n\n\nX= df.drop('Gender', axis=1)\ny= df['Gender']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nohe= OneHotEncoder()\nct= make_column_transformer((ohe,categorical),remainder='passthrough')  \n\nxgbc = XGBClassifier(random_state=0)\nlgbmc=LGBMClassifier(random_state=0)\n\n\nmodels = [xgbc,lgbmc]\n\nfor model in models: \n    pipe = make_pipeline(ct, model)\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    accuracy.append(round(accuracy_score(y_test, y_pred),4))\n\nmodel_names = ['XGBoost','LightGBM']\nresult_df4 = pd.DataFrame({'Accuracy':accuracy}, index=model_names)\nresult_df4","58683ffd":"accuracy =[]\nmodel_names =[]\n\n\nX= df.drop('Gender', axis=1)\ny= df['Gender']\ncategorical_features_indices = np.where(X.dtypes != np.float)[0]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nmodel = CatBoostClassifier(verbose=False,random_state=0)\n\nmodel.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_test, y_test))\ny_pred = model.predict(X_test)\naccuracy.append(round(accuracy_score(y_test, y_pred),4))\n\nmodel_names = ['Catboost_default']\nresult_df5 = pd.DataFrame({'Accuracy':accuracy}, index=model_names)\nresult_df5","66963cc6":"def objective(trial):\n    X= df.drop('Gender', axis=1)\n    y= df['Gender']\n    categorical_features_indices = np.where(X.dtypes != np.float)[0]\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n    param = {\n        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\n            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n        ),\n        \"used_ram_limit\": \"3gb\",\n    }\n\n    if param[\"bootstrap_type\"] == \"Bayesian\":\n        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.5, 1)\n\n    cat_cls = CatBoostClassifier(**param)\n\n    cat_cls.fit(X_train, y_train, eval_set=[(X_test, y_test)], cat_features=categorical_features_indices,verbose=0, early_stopping_rounds=100)\n\n    preds = cat_cls.predict(X_test)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(y_test, pred_labels)\n    return accuracy\n\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=50, timeout=600)\n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","4fef8a4e":"accuracy =[]\nmodel_names =[]\n\n\nX= df.drop('Gender', axis=1)\ny= df['Gender']\ncategorical_features_indices = np.where(X.dtypes != np.float)[0]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nmodel = CatBoostClassifier(verbose=False,random_state=0,\n                          objective= 'CrossEntropy',\n    colsample_bylevel= 0.04292240490294766,\n    depth= 10,\n    boosting_type= 'Plain',\n    bootstrap_type= 'MVS')\n\nmodel.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_test, y_test))\ny_pred = model.predict(X_test)\naccuracy.append(round(accuracy_score(y_test, y_pred),4))\nprint(classification_report(y_test, y_pred))\n\nmodel_names = ['Catboost_tuned']\nresult_df6 = pd.DataFrame({'Accuracy':accuracy}, index=model_names)\nresult_df6","907d524d":"feature_importance = np.array(model.get_feature_importance())\nfeatures = np.array(X_train.columns)\nfi={'features':features,'feature_importance':feature_importance}\ndf_fi = pd.DataFrame(fi)\ndf_fi.sort_values(by=['feature_importance'], ascending=True,inplace=True)\nfig = px.bar(df_fi, x='feature_importance', y='features',title=\"CatBoost Feature Importance\",height=500)\nfig.show()","8d1e4014":"result_final = pd.concat([dummy_result_df,result_df1,result_df2,result_df3,result_df4,result_df5,result_df6],axis=0)\nresult_final","061d35d0":"result_final.sort_values(by=['Accuracy'], ascending=True,inplace=True)\nfig = px.bar(result_final, x='Accuracy', y=result_final.index,title='Model Comparison',height=600,labels={'index':'MODELS'})\nfig.show()","59f825a1":"**Purpose**:\n\nTraining and applying models for the classification problems. Provides compatibility with the scikit-learn tools.\n\n**The default optimized objective depends on various conditions**:\n\n**Logloss** \u2014 The target has only two different values or the target_border parameter is not None.\n\n**MultiClass** \u2014 The target has more than two different values and the border_count parameter is None.","499470b2":"**Favorite Color vs. Gender**","6a76c8a8":"# MODEL SELECTION","9c6405df":"- Majority of female respondents prefer Coca Cola\/Pepsi.\n- Majority of male respondents prefer soft drinks other than Coca Cola\/Pepsi.","fb7ee6ae":"- We have developed model to classifiy heart disease cases.\n\n- First, we made the detailed exploratory analysis.\n\n- We have decided which metric to use.\n- We analyzed both target and features in detail.\n- We transform categorical variables into numeric so we can use them in the model.\n- We use pipeline to avoid data leakage.\n- We looked at the results of the each model and selected the best one for the problem on hand.\n- We looked in detail Catboost\n- We made hyperparameter tuning of the Catboost with Optuna to see the improvement\n- We looked at the feature importance.","905f0df8":"# Hi all. \ud83d\ude4b","c0878197":"- With **DummyClassifier** we had quite bad accuracy score.","44da5e30":"### Numerical Features","bd2990a3":"**Context**\n\nGender is a social construct. The way males and females are treated differently since birth moulds their behaviour and personal preferences into what society expects for their gender.\n\n\nThis small dataset is designed to provide an idea about whether a person's gender can be predicted with an accuracy significantly above 50% based on their personal preferences.\n\n**Content**\n\nThe data was collected in Fall 2015 from university students of 21 nationalities studying various majors in various countries using this form:\n\nhttps:\/\/docs.google.com\/forms\/d\/e\/1FAIpQLSduEjDURjTh7-a1ZjjlIYx75ScVETLp_gmoFszypz2J7E0LtQ\/viewform\n\nThe responses were then pre-processed and grouped into categories in order to obtain the final, transformed dataset.\n\n**Inspiration**\n\nWith the rise of feminism, the difference between males and females in terms of their personal preferences has decreased in recent years. For instance, historically in many cultures, warm colors such as red and pink were thought of as feminine colors while cool colors such as blue were considered masculine. Today, such ideas are considered outdated.\n\nDespite the decrease in the influence of gender on people\u2019s personal preferences, can a decent gender classifier be built given a dataset with people\u2019s personal preferences? What does this small dataset suggest?","96d6ad3c":"- Number of men and women, whose favorite music genre is Rock, is almost equal.\n- For other music genres, number of genders differs significantly form each other.","1e4bb035":"- We'll use dummy classifier model as a base model.\n- And then we will use Logistic & Linear Discriminant & KNeighbors, and Support Vector Machine models with and without scaler.\n- And then we will use ensemble models (Adaboost, Randomforest, Gradient Boosting, and Extra Trees).\n- We will see famous trio: XGBoost,LightGBM & Catboost.\n- Finally, we will look in detail to hyperparameter tuning for Catboost.","29890186":"- We have no numerical feature in our dataset.","6ba4c1a7":"**Baseline Modle**","8195c76d":"- So far so good. No zero variance and no extremely high variance.","91c05547":"**Catboost HyperParameter Tuning with Optuna**","18675689":"- We have 66 rows and 5 columns. \n- All columns are object.\n- We have no missing value.","d6f303dd":"**Famous Trio (XGBoost & LightGBM & Catboost)**","f1b0a95d":"### Target Variable","7c874eb0":"### Categorical Features","6594ed6f":"Our dataset consists of the following columns:\n    \n- **Favorite Color**: Favorite color (colors reported by respondents were mapped to either warm, cool or neutral)\n- **Favorite Music Genre**: Favorite broad music genre    \n- **Favorite Beverage**: Favorite alcoholic drink\n- **Favorite Soft Drink**: Favorite fizzy drink\n- **Gender**: Binary gender as reported by the respondents ","159273b9":" # Feature Importance","00964eca":"**CATBOOST**","bd0b88d3":"- Now let's see Catboost","f79bdfeb":"First thing first, let's import the libraries that we will use.","f7cd72c7":"- We have perfectly balanced data in terms of our label feature.\n- We can use 'accuracy' metric as our evaluation metric.","c73d16d3":"**Parameters**:\n\n- **Objective**: Supported metrics for overfitting detection and best model selection\n\n- **colsample_bylevel**: this parameter speeds up the training and usually does not affect the quality.\n\n- **depht**: Depth of the tree.\n\n- **boosting_type**: By default, the boosting type is set to for small datasets. This prevents overfitting but it is expensive in terms of computation. Try to set the value of this parameter to to speed up the training.\n\n- **bootstrap_type**: By default, the method for sampling the weights of objects is set to . The training is performed faster if the method is set and the value for the sample rate for bagging is smaller than 1.\n\nReference: https:\/\/catboost.ai\/\n\n\n\n- Ok let's use our best model with new parameters.","b2bb7287":"#### Today, we are going to continue our Machine Learning series with Gender Classification Dataset.\n\nIn this notebook we will implement new beginner friendly end to end ML model by using **CATBOOST** with **Optuna HyperParameter Tuning**.","8181276a":"- I'll use Catboost alone by using its capability to handle categorical variables without doing any preprocessing.\n\n- Let's first look at the XGBoost and LightGBM","9d955fd4":"- OK. Let's see the very famous trio:\n\nXGBoost,\nLight GBM,\nCatboost","ec3664a7":"- While 28 of female respondents drink alcohol, 5 of them do not drink any type of alcohol.\n- While 24 of male respondents drink alcohol, 9 of them do not drink any type of alcohol.","455c5389":"# What Problem We Have and Which Metric to Use?\n\n\n- Based on the data and data dictionary, we have a classification problem.\n- We wil make classification on the target variable **Gender**.\n- And we will build a model to get best calssification possible on the target variable.\n- For that we will look at the balance of the target variable.\n- As we will see later, our target variable has balanced or balanced like data.\n- Therefore, we will use **Accuracy** score to evaluate our model.","bf1629d9":"**Logistic & Linear Discriminant & SVC & KNN**","06138046":"# DATA","829e74a7":"**Favorite Music Genre vs. Gender**","2732da10":"**Ensemble Models (AdaBoost & Gradient Boosting & Random Forest & Extra Trees)**","cad8160b":"**Overall Insights from the Exploratory Data Analysis**\n\n- Target variable has perfect balanced data.\n- There is no numeric feature.\n- While men prefer cool and neutral colors,\n- women prefer warm colors.\n- Number of men and women, whose favorite music genre is Rock, is almost equal.\n- For other music genres, number of genders differs significantly form each other.\n- While 28 of female respondents drink alcohol, 5 of them do not drink any type of alcohol.\n- While 24 of male respondents drink alcohol, 9 of them do not drink any type of alcohol.\n- Majority of female respondents prefer Coca Cola\/Pepsi.\n- Majority of male respondents prefer soft drinks other than Coca Cola\/Pepsi.","f4d94c7f":"- Let's make some adjustment on the Catboost model to see its' peak performance on the problem.","67fb5b78":"- While men prefer cool and neutral colors,\n- women prefer warm colors.","b660fd70":"- We hav 4 duplicated rows. We will drop those later on.","d60a86fd":"![image.png](attachment:image.png)","46916116":"**Favorite Beverage vs. Gender**","44b03d8c":"**Favorite Soft Drink vs. Gender**","def373bf":"# Conclusion ","a0ffe584":"- Let's see how ensemble models do with the problem at hand.","8c45df36":"# Exploratory Data Analysis"}}