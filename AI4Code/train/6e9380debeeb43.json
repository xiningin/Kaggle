{"cell_type":{"3beec8de":"code","35e58b14":"code","c51dc44e":"code","2a8be61f":"code","8b223eb0":"code","aff05514":"code","8fb003cf":"code","23ed9cb1":"code","7fccb94e":"code","dc879cf1":"code","a2874d15":"code","2a3a7be1":"code","a4bc8de9":"code","3b37ce00":"code","13ce9b81":"code","adef3d9d":"code","3c391aab":"code","d185bc13":"code","f434323c":"code","ca91ba3a":"code","cabb59ff":"code","41bb033b":"code","47aa7064":"code","f37bda9c":"code","1678a71e":"code","3e27d9a9":"code","d2acbaf3":"code","0857c8fa":"code","2885bd7d":"code","3b768972":"code","d53a5b93":"code","114e839b":"markdown","2547e66f":"markdown","309ac319":"markdown","76968c8d":"markdown","8ce2f933":"markdown","de7f4656":"markdown","0f5d839b":"markdown","908cf5cf":"markdown","a51c3cd7":"markdown","5cc6c228":"markdown","60ebdfde":"markdown","6cbdbf4b":"markdown","fead669e":"markdown","b9115a2d":"markdown","bf94bd64":"markdown","bc877243":"markdown","6d91221e":"markdown","b238e85c":"markdown","c823f3bf":"markdown"},"source":{"3beec8de":"#When running the notebook on a fresh kernel, \n#download the stopwords package.\nimport nltk\nnltk.download('stopwords')","35e58b14":"#Standard Python Packages\nimport string\nimport time\nfrom collections import defaultdict, Counter\nimport re\n\n#Data and Visualization\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#SciKit Learn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n#nltk\nfrom nltk.corpus import stopwords\n\n#This is needed for graphs to generate correctly within the notebook\n%matplotlib inline ","c51dc44e":"#Change this variable, as needed, to the correct path of the Disater Tweets datasets \npath = '..\/input\/nlp-getting-started\/'\n\n#Data files\ntrain_data_file = path + 'train.csv'\ntest_data_file = path + 'test.csv'\n\n#Open the testing datafile into a dataframe\ntest_df = pd.read_csv(test_data_file)\n\n#Open the training datafile into a dataframe and display the first few rows\ntweets = pd.read_csv(train_data_file, index_col='id')\ntweets.head()","2a8be61f":"#Print the basic information about the dataframe\ntweets.info()","8b223eb0":"#Create a column to store the length of each tweet, \n#and calculate the tweet length\ntweets['length'] = tweets['text'].apply(len)","aff05514":"#Initial Formatting\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14,6), constrained_layout=True, sharey=True)\nsns_style = sns.set_style('white')\n\n#Graph\nnondisaster_tweetlen_hist = sns.histplot(data=tweets[tweets['target']==0]['length'],ax=axes[0])\ndisaster_tweetlen_hist = sns.histplot(data=tweets[tweets['target']==1]['length'],ax=axes[1])\n\n#Additional Formatting\nfig_title = fig.suptitle('Tweet Length Distribution by Target Class',fontsize=24)\naxeszero_title = axes[0].set_title('Non-Disaster Tweets (Class 0)',fontsize=14)\naxesone_title = axes[1].set_title('Disaster Tweets (Class 1)',fontsize=14)\naxeszero_format = axes[0].set(xlabel='Tweet Length (Characters)',ylabel='Count')\naxesone_format = axes[1].set(xlabel='Tweet Length (Characters)',ylabel=None)","8fb003cf":"#Calculate and print the class distributions of the target labels from the training dataset.\nclass_one_distribution = (tweets['target'].value_counts()[1]\/len(tweets['target']))*100\nclass_zero_distribution = 100 - class_one_distribution\n\nprint(f'Non-Disaster tweets (Class 0) make up {class_zero_distribution:4.2f}% of the training dataset.')\nprint(f'Disaster tweets (Class 1) make up {class_one_distribution:4.2f}% percent of the training dataset.')","23ed9cb1":"#Initial Formatting\nfig, axes = plt.subplots(figsize=(6,6), constrained_layout=True)\nsns_style = sns.set_style('white')\n\n#Graph\nclass_hist = sns.histplot(data=tweets['target'], kde=False, bins=2, ax=axes)\n\n#Additional Formatting\nfig_title = fig.suptitle('Class Distribution',fontsize=24)\naxes_title = axes.set_title('Training Dataset',fontsize=14)\naxes_format = axes.set(xlabel='Target Class',ylabel='Count',xticklabels=[0,1],xticks=[0,1])","7fccb94e":"#Creates a set of English stop words\nstop=set(stopwords.words('english'))","dc879cf1":"#Creates a corpus of words and returns the list\ndef create_corpus(target):\n  corpus=[]\n\n  for x in tweets[tweets['target']==target]['text'].str.split():\n    for i in x:\n      corpus.append(i)\n  return corpus","a2874d15":"#Analyze class 0\nnondisaster_corpus = create_corpus(0)\n\n#Create a dictionary to hold the stop words within the corpus\n#along with a count of the stop word.\nnondisaster_dic=defaultdict(int)\nfor word in nondisaster_corpus:\n  if word in stop:\n    nondisaster_dic[word] +=1\n\n#Get the sorted top 10 stop words of the Class 0 corpus\nnondisaster_top=sorted(nondisaster_dic.items(),key=lambda x:x[1],reverse=True)[:10]\n#Get data ready for a graph\nnondisaster_x,nondisaster_y=zip(*nondisaster_top)\n\n#Analyze class 1\ndisaster_corpus = create_corpus(1)\n\n#Create a dictionary to hold the stop words within the corpus\n#along with a count of the stop word.\ndisaster_dic=defaultdict(int)\nfor word in disaster_corpus:\n  if word in stop:\n    disaster_dic[word] +=1\n\n#Get the sorted top 10 stop words of the Class 1 corpus\ndisaster_top=sorted(disaster_dic.items(),key=lambda x:x[1],reverse=True)[:10]\n#Get data ready for a graph\ndisaster_x,disaster_y=zip(*disaster_top)","2a3a7be1":"#Class 0 Stop Words\n#Initial Formatting\nfig, axes = plt.subplots(figsize=(6,6), constrained_layout=True)\nsns_style = sns.set_style('white')\n\n#Graph\nnondisaster_bar = plt.bar(nondisaster_x,nondisaster_y)\n\n#Additional Formatting\nfig_title = fig.suptitle('Stop Words Distribution',fontsize=24)\naxes_title = axes.set_title('Class 0',fontsize=14)\naxes_format = axes.set(xlabel='Stop Words',ylabel='Count')","a4bc8de9":"#Class 1 Stopwords\n#Initial Formatting\nfig, axes = plt.subplots(figsize=(6,6), constrained_layout=True)\nsns_style = sns.set_style('white')\n\n#Graph\ndisaster_bar = plt.bar(disaster_x,disaster_y)\n\n#Additional Formatting\nfig_title = fig.suptitle('Stop Words Distribution',fontsize=24)\naxes_title = axes.set_title('Class 1',fontsize=14)\naxes_format = axes.set(xlabel='Stop Words',ylabel='Count')","3b37ce00":"#Get top bigrams from corpus\ndef get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","13ce9b81":"#Get top 10 bigrams and prepare them for a graph\ntop_tweet_bigrams=get_top_tweet_bigrams(tweets['text'])[:10]\ntopbigram_y,topbigram_x=map(list,zip(*top_tweet_bigrams))","adef3d9d":"#Top bigrams\n#Initial Formatting\nfig, axes = plt.subplots(figsize=(12,6), constrained_layout=True)\n\n#Graph\ntopbigrams_bar = sns.barplot(x=topbigram_x,y=topbigram_y,ax=axes)\n\n#Additional Formatting\nfig_title = fig.suptitle('Top Bigrams',fontsize=24)\naxes_title = axes.set_title('Training Dataset',fontsize=14)\naxes_format = axes.set(xlabel='Count',ylabel='Bigrams')","3c391aab":"#This function accepts a string and uses regular expressions\n#to remove URLs from the string, before returning the cleaned string.\ndef remove_URL(text):\n  url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n  return url.sub(r'',text)","d185bc13":"#This function accepts a string and uses regular expressions\n#to remove HTML tags from the string, before returning the cleaned string.\ndef remove_html(text):\n  html=re.compile(r'<.*?>')\n  return html.sub(r'',text)","f434323c":"#This function accepts a string and uses regular expressions\n#to remove emojis from the string, before returning the cleaned string.\n\n# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n  emoji_pattern = re.compile(\"[\"\n  u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n  u\"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n  u\"\\U0001F680-\\U0001F6FF\"  # Transport & Map symbols\n  u\"\\U0001F1E0-\\U0001F1FF\"  # IOS Flags\n  u\"\\U00002500-\\U00002BEF\"  # Chinese characters\n  u\"\\U00002702-\\U000027B0\"\n  u\"\\U00002702-\\U000027B0\"\n  u\"\\U000024C2-\\U0001F251\"\n  u\"\\U0001f926-\\U0001f937\"\n  u\"\\U00010000-\\U0010ffff\"\n  u\"\\u2640-\\u2642\"\n  u\"\\u2600-\\u2B55\"\n  u\"\\u200d\"\n  u\"\\u23cf\"\n  u\"\\u23e9\"\n  u\"\\u231a\"\n  u\"\\ufe0f\"  # Dingbats\n  u\"\\u3030\"\n  \"]+\", flags=re.UNICODE)\n  return emoji_pattern.sub(r'', text)","ca91ba3a":"#This function accepts a string and uses regular expressions\n#to remove Twitter mentions from the string, before returning the cleaned string.\ndef remove_mentions(text):\n  return re.sub(\"@[A-Za-z0-9]+\",\"\",text)","cabb59ff":"#This function will be used to process text to\n#create a bag-of-words model.\ndef text_processor(messy_str):\n  \"\"\"\n  Accepts a messy string, cleans it, \n  and then returns the string as list.\n\n  1. Removes URLs and make the string lowercase.\n  2. Removes HTML tags.\n  3. Removes popular emoji symbols.\n  4. Remove Twitter mentions.\n  5. Remove punctionation.\n  7. Remove numbers.\n  8. Remove stopwords.\n  9. Return list of cleaned, lowercase words\n  \"\"\"\n  \n  #Remove URLs from the string\n  cln_str = remove_URL(messy_str).lower()\n\n  #Remove HTML tags from the string\n  cln_str = remove_html(cln_str)\n\n  #Remove emoji from the string\n  cln_str = remove_emoji(cln_str)\n\n  #Remove Twitter mentions from the string\n  cln_str = remove_mentions(cln_str)\n\n  #Removes the punctuation from the string\n  nopunc = [char for char in cln_str if char not in string.punctuation]\n  nopunc = ''.join(nopunc)\n\n  #Removes digits from the string\n  nonum = [char for char in nopunc if char not in string.digits]\n  nonum = ''.join(nonum)\n\n  #Remove the stopwords and return a cleaned list of words\n  return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]","41bb033b":"#Create the features, X, and the target, y,\n#and then split them into training and validation sets.\nX, y = tweets['text'], tweets['target']\nX_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.20,random_state=1010)\n#The validation set will be 20% of the training data, as set by the test_size parameter","47aa7064":"#Create the Naive Bayes NLP pipeline\n#First, use the CountVectorizer with the custom text_processor\n#to create the BoW model. Second, pass the BoW model to the ML classifier. \npipeline = Pipeline([\n                     ('bow', CountVectorizer(analyzer=text_processor)), #Tokenize message strings into bag-of-word\n                     ('nb',MultinomialNB(alpha=1.1)), #Train a classifier on the bag-of-words vectors\n])","f37bda9c":"#Fit the pipeline to the training data\npipeline.fit(X_train, y_train)","1678a71e":"#Use the trained ML model from the pipeline to predict validation data.\ny_pred = pipeline.predict(X_val)","3e27d9a9":"#Print a classification report\nprint(classification_report(y_val,y_pred))","d2acbaf3":"#Create a dataframe of the confusion matrix data\ncm_df = pd.DataFrame(confusion_matrix(y_val,y_pred),columns=np.unique(y_val),index=np.unique(y_val))\ncm_df.index.name = \"Actual\"\ncm_df.columns.name = \"Predicted\"","0857c8fa":"#Create confusion matrix heatmap\n#Initial Formatting\nfig, axes = plt.subplots(figsize=(6,6), constrained_layout=True)\nsns_style = sns.set_style('white')\n\n#Graph\ncnfmtx_pipeline = sns.heatmap(data=cm_df, cmap='Blues',annot=confusion_matrix(y_val,y_pred),annot_kws={\"size\": 16}, fmt='d',ax=axes)\n\n#Additional Formatting\nfig_title = fig.suptitle('Confusion Matrix',fontsize=24)\naxes_title = axes.set_title('Trained Pipeline',fontsize=14)\naxes_Xax = axes.set_xlabel('Predicted', fontsize=16)\naxes_Yax = axes.set_ylabel('Actual', fontsize=16)\naxes_xtick = axes.set_xticklabels(labels=[0,1],fontsize=16)\naxes_ytick = axes.set_yticklabels(labels=[0,1],fontsize=16)","2885bd7d":"#Display a few rows of the test dataset\ntest_df.head()","3b768972":"#Feed the test data into the pipeline and get predictions\ntest_pred = pipeline.predict(test_df['text'])","d53a5b93":"#Save the predictions in the correct format for submission\ntest_df['target'] = test_pred\nsubmission = test_df[['id','target']]\n\n#Save the file as a csv\nsubmission.to_csv('submission.csv',index=False)","114e839b":"### Stop Words\n\nStop words are words that do not add much meaning to a phrase or sentence. Good examples of stop words are \"the\", \"a\", and \"is\". Large amounts of stop words could negatively affect a ML model, so it is vital to understand if this kind of cleansing is needed.\n\nThis dataset has large numbers of stop words, indicating these should probably be removed.","2547e66f":"### Commonly Paired Words\n\nThis analysis of bigrams within the training tweets will show the most commonly paired words, which can help give even more insight into the data cleansing needed.\n\nIt is worth noting that the top bigrams involve 'HTTP', indicating there are a large number of URLs included in the dataset. These URLs may need to be removed.","309ac319":"## Train Test Split\n\nTo avoid overfitting, it can be important to validate the model on data that was not part of the dataset the model was trained on. Train Test Split helps to automate the process of splitting the training dataset into training tweets and validation tweets. A random state seed was also used to allow for the reproducibility of the project results.","76968c8d":"## Class Distribution\n\nA piece of information that is important is the class distribution of the two classes from within the training dataset. This information can be used to guide decisions about the ML model later in the data mining process.\n\nWhile the Non-Disaster tweet class (class 0) represents a larger portion of the dataset than the Disaster tweet class (class 1), the class distribution is not largely imbalanced.","8ce2f933":"# Setup\n\nFirst, package imports will be handled, then the training and testing datasets will be loaded.","de7f4656":"# Further Research Suggestions\n\nThere is a vast amount of information on ML, NLP, data exploration, and many other subjects related to data mining available for research. Below are a handful of suggestions for potential research areas that could be utilized to improve the performance of the ML model that was developed for this project.\n\n* Spellchecking\n * Correcting common misspellings can potentially help increase data quality and model performance through reducing the features space by removing unique misspellings. To save on processing time, spellchecking was not included in this project.\n* Word Stemming\n * Word stemming is the process of converting words to their base form. For example, stemming the list of words [running, ran, runs] would result in all three words being converted to the base word \u2018run\u2019. To save on processing time, word stemming was not included in this project.\n* Term Frequency - Inverse Document Frequency (TF-IDF)\n * This is a statistical method that can be used to evaluate the importance of a specific word within a corpus. Term frequency refers to the frequency of a specific word occurring within a corpus. Inverse document frequency helps measure the importance of a word by providing more weight to words that occur less frequently, by taking the inverse of the frequency of documents the term occurs with. For simplicity, TF-IDF was not utilized for this project.\n* Hyperparameter Tuning\n * Both the ML model and other functions used in the pipeline, such as the CountVectorizer, have a variety of hyperparameters that can be tuned to improve the overall performance of predictions. Hyperparameter tuning, such as using GridsearchCV, can be used to determine the best hyperparameters by exhaustively testing a defined set of possible parameters. To save on processing time, hyperparameter tuning is not presented in this project. However, a simple GridsearchCV analysis was performed during researching this project for tunning. The findings were as follows:\n   * CountVectorizer max features: the optimal maximum number of features was found to be None, meaning there is no limit on the vocabulary of the BoW model.\n   * Na\u00efve Bayes alpha: the optimal smoothing coefficient for the Na\u00efve Bayes classifier was found to be 1.1.\n* Different Classifiers and Other Libraries\n *  While the Naive Bayes classifier provided modest performance (approximately 79% accuracy), there are many other ML models. Additionally, new Python libraries supporting NLP, ML, and data science are regularly released, and existing libraries are updated. This project could be improved by using newer, more relevant tools and libraries to improve performance. This project is aimed at being a basic introduction, so this process is left for future works and research.","0f5d839b":"## Create a Pipeline for the Model\n\nPipelines allow the creation of an automated process for data processing, such as this NLP task.\n\nFor this introductory project the SciKit Learn ML classifier Multinomial Naive Bayes will be used.\n\nThe first step is to clean the text and then converting it into a bag-of-words (BoW) model before sending the BoW model to the classifier. To automate this process a pipeline has been created to create the BoW model and pass it to the classifier. This pipeline object can be used just like a SciKit Learn ML model, providing functionality for quick and robust automation of ML tasks.","908cf5cf":"## Basic information about the dataset\nThere are 7,613 tweets in the training set. Both the keyword and location attributes have null values, while the remaining attributes do not have null values.\n\n* <1% of the Keyword column is missing. This feature could be potentially useful.\n* Approxamately 33% of the Location column is missing. This feature may not be useful given the amount of values missing, and may be dropped.\n\nTo maintain focus on NLP, both the Keyword and Location columns will not be used in the final model.","a51c3cd7":"## Text Processing and Data Cleaning\n\nWith a basic understanding of the dataset, along with tweets in general, a process for cleaning and preparing the tweet text can be developed and implemented.\n\nFrom the analysis, URLs and stop words are major items needing cleansing. Other areas of cleaning, based on techniques for analyzing text and knowledge about tweets, will also be focused on. These areas include punctuation, HTML tags, emoji, numbers, and Twitter mentions. Finally, to make the corpus more uniform, all words will be converted to lowercase.\n\nThis text processing will be used during the NLP to create a bag-of-words model.","5cc6c228":"## Data Cleanning - Common Words, Characters, and Other Features\n\nAn important area to explore is the text itself, to gain a better understanding of the data cleansing that must take place. This exploration, along with subject expert knowledge and other relevant experience can help inform the data analyst to the kinds of cleansing that must be completed on a dataset.","60ebdfde":"### Potential Additional Considerations\n\nBecause this is a dataset of tweets, several other considerations can be made: \n* There will be a large number of punctuation characters.\n* Twitter allows emoji.\n* Tweets typically include mentions of Twitter accounts.\n* Tweets may include numbers.\n\nThe above list represents some of the possible cleaning processes that could be completed to prepare the dataset for the ML process.","6cbdbf4b":"# Data Preparation and Machine Learning","fead669e":"# Create the Kaggle Submission\n\nWith a trained ML model, the pipeline can then be used to make predictions on the test dataset. These predictions can then be formatted, saved, and submitted to the Kaggle competition for scoring.\n\nFor this notebook, when ran on Kaggle.com, the submission file will be found in the output and can be easily submitted from the Kaggle notebook environment.","b9115a2d":"## Tweet Length\n\nAnother possible dimension from this text-based dataset that could be used to train a ML model is the length of the text.\n\nMost tweets from both classes are around 150 characters. This makes sense logically, considering that was the maximum character amount for tweets. People may attempt to use up all the possible characters when making tweets, regardless of the sentiment. However, there may be some correlation when looking at tweets that are less than the old Twitter character maximum. It seems that non-disaster tweets are slightly more likely to use less than the maximum allowed characters.\n\nTo maintain focus on NLP, tweet length will not be used in the final model.","bf94bd64":"## Load the Datasets\n\nUpdate the below section as needed to open the datasets of the Disaster Tweets Kaggle competition into two Pandas dataframes.\n\n**Do not change the names of the dataframes (test_df, tweets).**","bc877243":"# Evaluate the Pipeline Model Performance","6d91221e":"## Imports","b238e85c":"# Data Exploration\n\nExploratory data analysis can provide useful insights on the dataset being analyzed. In the case of NLP, data exploration will help provide an idea of the kinds of data cleansing and text processing that may be needed to prepare the dataset for training a ML model.\n\nThe following is an excerpt of some of the data exploration completed for this project.","c823f3bf":"# Introduction\n\nThis notebook is a brief exploration of the basics of exploratory data analysis, machine learning (ML), and nature language processing (NLP). This notebook will discuss utilizing Python, SciKit Learn, and the nltk libraries to perform sentiment analysis on a sample dataset. A simple pipeline for processing the text data will be implemented, and a supervised ML model will be trained as a simple example of NLP.\n\nThe dataset used in this project is provided from a Kaggle.com competition: [Natural Language Processing with Disaster Tweets](https:\/\/www.kaggle.com\/c\/nlp-getting-started). Kaggle.com is a prominent website within the ML research community that hosts a variety of ML competitions. The linked competition involves analyzing a dataset of labeled tweets to develop a NLP ML model that can determine whether a specific tweet is about a disaster. Competitors can then utilize their model on an unlabeled test dataset and submit the predictions to be scored and ranked against other competitors. Competitors typically share their research utilizing Jupyter Notebooks, allowing others to quickly learn and access new techniques.\n\nAfter initial imports and setup, the training dataset will be briefly analyzed through data exploration. With the information gained from data analysis, exploration, and other knowledge, a process for cleaning and preparing the text data will be presented. With a process for cleaning the dataset in place, a pipeline can be developed to automate processing text and training a ML model. A simple pipeline that utilizes a bag-of-words model and a Na\u00efve Bayes classifier will be presented as an example. The trained Na\u00efve Bayes ML model will be measured for performance. Additionally, preparing a submission for the Kaggle competition will be discussed. Finally, possible steps at advancing the model or complexity of NPL in general will be briefly presented.\n\n* The following open Python notebook on Kaggle was utilized as a reference:\n * [Basic EDA, Cleaning, and GloVe, on Kaggle.com](https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove)"}}