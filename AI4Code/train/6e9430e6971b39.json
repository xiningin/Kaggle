{"cell_type":{"2714a93c":"code","c0794b89":"code","976a0473":"code","505d2df5":"code","10ce50d8":"code","64362dd1":"code","388f174b":"code","76082fad":"code","e3011fc5":"code","fd59fe5b":"code","d12e6888":"code","ac6396d1":"code","f97eb1a8":"code","de400984":"code","ed919934":"code","1acf30bc":"code","bc395e04":"code","3e5ef1f6":"code","356dbbe1":"code","eaccc937":"markdown","8da98536":"markdown","f1d49cce":"markdown","0f8c4a49":"markdown"},"source":{"2714a93c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/digit-recognizer'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c0794b89":"train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\nprint(train.shape)\nprint(test.shape)","976a0473":"y_train = train['label']\nX_train = train.drop(labels = [\"label\"],axis = 1)","505d2df5":"import matplotlib.pyplot as plt\n%matplotlib inline","10ce50d8":"# Normalize the data\n\n\"\"\" the goal of normalization is to change the values of numeric columns in the dataset \n    to a common scale, without distorting differences in the ranges of values.\"\"\"\nX_train = X_train \/ 255.0  \ntest = test \/ 255.0","64362dd1":"#Converting our dataset into (28,28,1) as default image.\nX_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)\n","388f174b":"print(X_train.shape)\nprint(y_train.shape)\nprint(test.shape)\n","76082fad":"import tensorflow as tf\nprint(tf.__version__)\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\ny_train = to_categorical(y_train, num_classes = 10)","e3011fc5":"#Using matplotlib to show a random image in our training data.\nplt.imshow(X_train[50][:,:,0])","fd59fe5b":"from sklearn.model_selection import train_test_split\nX_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size= 0.2,random_state = 101)\n#Splitting data into training and validation to check whether model is not overfitting nor underfitting.","d12e6888":"from tensorflow import keras\nfrom tensorflow.keras import layers, models\n\nmodel = models.Sequential()\n","ac6396d1":"model.add(layers.Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(layers.Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(layers.MaxPool2D(pool_size=(2,2)))\nmodel.add(layers.Dropout(0.25))\n\n\nmodel.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(layers.Dropout(0.25))\n\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(256, activation = \"relu\"))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(10, activation = \"softmax\"))","f97eb1a8":"model.summary()","de400984":"optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n#RMSprop is a gradient-based optimization technique used in training neural networks.\n#Using RMSprop to minimize an error function(loss function).\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","ed919934":"history = model.fit(X_train, y_train, batch_size = 100, epochs = 10, validation_data = (X_val, y_val))","1acf30bc":"plt.plot(history.epoch,history.history['accuracy'],label = \"accuracy\") # Accuracy curve for training set\nplt.plot(history.epoch,history.history['val_accuracy'],label = \"val_accuracy\") # Accuracy curve for validation set\n\nplt.title(\"Accuracy Curve\",fontsize=18)\nplt.xlabel(\"Epochs\",fontsize=15)\nplt.ylabel(\"Accuracy\",fontsize=15)\nplt.grid(alpha=0.3)\nplt.legend()","bc395e04":"##Lets predict our model using random image in test data\nplt.imshow(test[50][:,:,0])\n","3e5ef1f6":"prediction = model.predict(test[50].reshape(-1,28,28,1))\nprediction = np.argmax(prediction,axis = 1)\nprint(\"Our model predicts for the image is Digit:\",prediction)\n#results = pd.Series(results,name=\"Label\")\n","356dbbe1":"## Similarly we will predict for the test and submit it.\nresults = model.predict(test)\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"cnn_mnist.csv\",index=False)","eaccc937":"We have 42000 training data and 28000 Test data","8da98536":"With one-hot, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0 ,<br>\nto those columns. Each integer value is represented as a binary vector.","f1d49cce":"Our random test image shows number 6.Lets test on our model.","0f8c4a49":"Our dataset currently has 42000 rows and 785 column."}}