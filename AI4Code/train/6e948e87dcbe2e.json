{"cell_type":{"2cb8f311":"code","5b5ef7ed":"code","611ed043":"code","862585f5":"code","731ef061":"code","f2287e83":"code","ed77d773":"code","7fa9773e":"code","2cc7b353":"code","6f8417d8":"code","e77ff60a":"code","a93b426d":"code","e27fc44c":"code","608fc36c":"code","e6d9f59f":"code","18658bbc":"code","c01f99de":"code","45be1899":"code","8f1dcf7f":"code","bfc73c2d":"code","d82af12a":"code","e49ca5c6":"markdown","65459df7":"markdown","1d416d6f":"markdown","ad51beac":"markdown","2a678796":"markdown","f6efbcfa":"markdown","95afdb2b":"markdown","6740a92f":"markdown","ca62f2bc":"markdown","1ecba116":"markdown","e8bfbc03":"markdown","54e2be7d":"markdown","5fda0153":"markdown","21cbaaa0":"markdown"},"source":{"2cb8f311":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom ipywidgets import interact,  FloatSlider\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve\n\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.over_sampling import SMOTE","5b5ef7ed":"pd.options.display.max_columns = 100\npd.options.display.max_rows = 100\npd.options.display.width=100\nplt.style.use('ggplot')","611ed043":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","862585f5":"df.shape","731ef061":"df.isnull().sum().sum()","f2287e83":"df.describe()","ed77d773":"df.dtypes","7fa9773e":"diff_class = df['Class'].value_counts()\ndiff_class.plot(kind='bar', color=['m', 'k'], figsize=(5, 5))\nplt.xticks(range(2), ['Normal  [0]', 'Fraud  [1]'], rotation=0)\nfor i, v in enumerate(diff_class):\n    plt.text(i-0.1, v+3000, str(v))\nplt.title('Class Count')\nplt.show()","2cc7b353":"ss = StandardScaler()\ndf['Amount'] = ss.fit_transform(df[['Amount']])\ndf['Time'] = ss.fit_transform(df[['Time']])","6f8417d8":"for var in df.columns[:-1]:\n    \n    sns.boxplot(df[var], hue=df['Class'], palette='Set3')\n    mean = df[var].mean()\n    std = df[var].std()\n    plt.axvline(mean - 3 * std, 0, 1)\n    plt.text(mean - 3 * std, -0.55, 'mean - 3* std', rotation=60)\n    plt.axvline(mean + 3 * std, 0, 1)\n    plt.text(mean + 3 * std, -0.55, 'mean + 3* std', rotation=60)\n    \n\n    plt.show()\n    ","e77ff60a":"X_train, X_test, y_train, y_test = train_test_split(df.drop('Class', axis=1), df[['Class']].values, test_size=0.3,random_state=1997)","a93b426d":"lr = LogisticRegression(max_iter=1000)\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)","e27fc44c":"def plot_confusion_matrix(y_test, y_pred):\n    cf_matrix = confusion_matrix(y_test, y_pred)\n\n    labels_name = ['True Neg', 'False Pos', 'False Neg', 'True Pos']\n    labels_count = [value for value in cf_matrix.flatten()]\n    labels_percentage = [ \"{0:.2%}\".format(value) for value in cf_matrix.flatten()\/ np.sum(cf_matrix)]\n\n    labels = [f'{x}\\n {y} \\n{z}' for x, y, z in zip(labels_name, labels_count, labels_percentage)]\n    labels = np.array(labels, dtype=str).reshape(2,2)\n\n    recall = cf_matrix[1,1]\/(cf_matrix[1,0] + cf_matrix[1,1])\n    precision = cf_matrix[1,1]\/(cf_matrix[0,1] + cf_matrix[1,1])\n    accuracy = (cf_matrix[0, 0] + cf_matrix[1,1])\/ np.sum(cf_matrix)\n    f1_score = (2*precision*recall)\/(precision + recall)\n\n    stats = '\\n\\n Recall:   {0:.03}\\n Precision:   {1:.03}\\n Accuracy:  {2:.03}\\nF1-Score:  {3:.03}'.format(recall, precision, accuracy, f1_score)\n\n    sns.heatmap(cf_matrix, annot=labels, fmt='', center=3, linewidth=3, linecolor='k', cbar=False)\n    plt.title('Confusion matrix\\n', fontsize=20)\n    plt.xlabel('Predicted Label'+stats, fontsize=14)\n    plt.ylabel('True Label', fontsize=14)\n\n    plt.show()\nplot_confusion_matrix(y_test, y_pred)","608fc36c":"y_prob = lr.predict_proba(X_test)\ny_prob = y_prob[:, 1] # Probability of getting the output 1 (Fraud)","e6d9f59f":"fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n# gmeans = np.sqrt(tpr*(1-fpr))\n# ix = np.argmax(gmeans)\n# print(\"Best thresholds=%f, G-Mean=%.3f\" %(thresholds[ix], gmeans[ix]))\n\nplt.plot([0, 1], [0, 1], linestyle='--', label='No Skill')\nplt.plot(fpr, tpr, marker='.', label='Logistic')\n# plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='best', sizes=(200, 100))\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.title('TPR vs FPR', fontsize=20)\nplt.show()","18658bbc":"lr_precision, lr_recall, lr_thresholds = precision_recall_curve(y_test, y_prob)\nno_skill = len(y_test[y_test==1])\/ len(y_test)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\nplt.plot(lr_precision, lr_recall, marker='.', label='Logistic')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.legend()\nplt.title('Precision vs Recall', fontsize=20)\nplt.show()","c01f99de":"print(\"Slide, Range -> (0.001, 0.04)\")\ndef update(var=0.004):\n    print(\"y_prob should be greater than >\", var)\n    predict_mine = np.where(y_prob > var, 1, 0)\n    plot_confusion_matrix(y_test, predict_mine)\n\ninteract(update, var=FloatSlider(min=0.001, max=0.04, step=0.001))","45be1899":"lr_b = LogisticRegression(max_iter=1000, class_weight='balanced')\nlr_b.fit(X_train, y_train)\ny_pred_b = lr_b.predict(X_test)\nplot_confusion_matrix(y_test, y_pred_b)","8f1dcf7f":"tl = TomekLinks(sampling_strategy='majority')\nX_train_tl, y_train_tl = tl.fit_sample(X_train, y_train)\nlr_tl = LogisticRegression(max_iter=1000, class_weight='balanced')\nlr_tl.fit(X_train_tl, y_train_tl)\ny_pred_tl = lr_tl.predict(X_test)\nplot_confusion_matrix(y_test, y_pred_tl)","bfc73c2d":"smote  = SMOTE(sampling_strategy='minority')\nX_train_sm, y_train_sm = smote.fit_sample(X_train, y_train)","d82af12a":"lr_sm = LogisticRegression(max_iter=1000)\nlr_sm.fit(X_train_sm, y_train_sm)\ny_pred_sm = lr_sm.predict(X_test)\nplot_confusion_matrix(y_test, y_pred_sm)","e49ca5c6":"Distribution of different columns.","65459df7":"# 5. Tomek Link","1d416d6f":"In this Kernel, I tried different methods for Credit Card Fraud Detection. \n![image.png](attachment:image.png)\nCheck the last Cell for knowledge sources.","ad51beac":"# 3. Change the Threshold","2a678796":"# 2. Predict Porba","f6efbcfa":" # 1. Logistic Regression","95afdb2b":"1. https:\/\/www.svds.com\/learning-imbalanced-classes\/\n2. https:\/\/stackoverflow.com\/questions\/30972029\/how-does-the-class-weight-parameter-in-scikit-learn-work\n3. https:\/\/stackoverflow.com\/questions\/34831676\/how-to-perform-undersampling-the-right-way-with-python-scikit-learn\n4. https:\/\/www.xenonstack.com\/wp-content\/uploads\/xenonstack-credit-card-fraud-detection.png","6740a92f":"# 6. Synthetic Minority Oversampling Technique( SMOTE)","ca62f2bc":"predict_proba gives you the probabilities for the target (0 and 1 in this case) in array form. The number of probabilities for each row is equal to the number of categories in target variable. ","1ecba116":"\n- __Upper Left Square__: The amount of correctly classified by model of no fraud transactions.\n- __Upper Right Square__: The amount of incorrectly classified transactions as fraud cases, but the actual label is no fraud .\n- __Lower Left Square__: The amount of incorrectly classified transactions as no fraud cases, but the actual label is fraud .\n- __Lower Right Square__: The amount of correctly classified by our model of fraud transactions.\n\n\n- *Recall*: Out of all the positive classes, how much we predicted correctly.  (TP\/TP+FN)\n- *Precision*: Out of all the positive classes we have predicted, how many are actually positive. (TP\/TP+FP)\n- *Accuracy*: Out of all the classes, how much we predicted correctly \n- *F-measure*: 2(Recall) (Precision)\/(Recall + Precision)","e8bfbc03":"# 4. Logistic Regression with balanced class weight","54e2be7d":"![image.png](attachment:image.png)","5fda0153":"Split the data in training and test set. ","21cbaaa0":"![image.png](attachment:image.png)"}}