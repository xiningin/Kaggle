{"cell_type":{"1c0c9265":"code","c64e7b0f":"code","f5ced412":"code","7e9451e4":"code","49c933f5":"code","6d88610f":"code","798eafca":"code","ea57b3e2":"code","8598913d":"code","0bc14bde":"code","831a3af0":"code","06edcbc5":"code","3f1684d3":"code","5f871ede":"code","9c51350a":"code","9ad9d249":"code","c2864f21":"code","ccfcbe41":"code","4b719d7e":"code","3ec7830a":"code","a63063b0":"code","92442172":"code","89043553":"code","5f339c21":"code","43def540":"code","fc93094e":"code","3eef4aba":"code","3971ae8b":"code","54e1daaf":"code","fae83d41":"code","b6117eae":"code","b29a5a45":"code","62c5a160":"code","1d9eacc9":"code","8cd1548d":"code","1cb4975f":"code","c89da705":"code","345d1551":"code","32642a2e":"code","1cb19b35":"code","393d9651":"code","18c95ad0":"markdown","6bb15fdd":"markdown","57a6809b":"markdown","388c75e1":"markdown","5ee46063":"markdown","bc46881c":"markdown","40f66d4d":"markdown","a21657d5":"markdown","59950916":"markdown","5b7695ad":"markdown","f0910698":"markdown"},"source":{"1c0c9265":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c64e7b0f":"#importing the dataset\n\ndf = pd.read_csv('..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf.head()","f5ced412":"df.columns","7e9451e4":"df.shape","49c933f5":"df.info()","6d88610f":"df.describe()","798eafca":"# checking if any null data exists\ndf.isnull().sum()","ea57b3e2":"df = df.drop(columns = ['customerID'])\ndf.head()","8598913d":"#plotting the graph\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.countplot(x = \"Churn\", data = df)\ndf.loc[:, 'Churn'].value_counts()","0bc14bde":"sns.countplot(x = \"SeniorCitizen\", data = df)\ndf.loc[:, 'SeniorCitizen'].value_counts()","831a3af0":"sns.countplot(x = \"InternetService\", data = df)\ndf.loc[:, 'InternetService'].value_counts()","06edcbc5":"sns.countplot(x = \"PhoneService\", data = df)\ndf.loc[:, 'PhoneService'].value_counts()","3f1684d3":"plt.figure()\nCorr=df[df.columns].corr()\nsns.heatmap(Corr,annot=True)","5f871ede":"df['TotalCharges'].value_counts().sort_index().plot.hist()","9c51350a":"df['MonthlyCharges'].value_counts().sort_index().plot.hist()","9ad9d249":"df['tenure'].value_counts().sort_index().plot.hist()","c2864f21":"df['PaymentMethod'].value_counts().plot.pie()\nplt.gca().set_aspect('equal')","ccfcbe41":"sns.kdeplot(df['tenure'], df['MonthlyCharges'])","4b719d7e":"# converting the non-numeric data into numeric data.\nfrom sklearn.preprocessing import LabelEncoder\nencoded = df.apply(lambda x: LabelEncoder().fit_transform(x) if x.dtype == 'object' else x)\nencoded.head()","3ec7830a":"plt.figure(figsize =(20,20))\nCorr=encoded[encoded.columns].corr()\nsns.heatmap(Corr,annot=True)","a63063b0":"sns.violinplot(x='gender', y='InternetService', data=encoded)","92442172":"sns.violinplot(x='PaperlessBilling', y='PaymentMethod', data=encoded)","89043553":"sns.violinplot(encoded['StreamingTV'],encoded['StreamingMovies'])","5f339c21":"sns.violinplot(encoded['Partner'],encoded['Dependents'])","43def540":"sns.violinplot(encoded['MultipleLines'])","fc93094e":"X = encoded.iloc[:, 0:19]\ny = encoded.Churn","3eef4aba":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n\n#print length of X_train, X_test, y_train, y_test\nprint (\"X_train: \", len(X_train))\nprint(\"X_test: \", len(X_test))\nprint(\"y_train: \", len(y_train))\nprint(\"y_test: \", len(y_test))","3971ae8b":"from sklearn.neighbors import KNeighborsClassifier\nknc = KNeighborsClassifier()\nknc.fit(X_train, y_train)\nknc.fit(X_train, y_train)\nprint('Accuracy score of KNN training set: {:.3f}'.format(knc.score(X_train, y_train)))\nprint('Accuracy score of KNN test set: {:.3f}'.format(knc.score(X_test, y_test)))","54e1daaf":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, precision_recall_curve\ny_knc = knc.predict(X_test)\n\nprint('confusion_matrix of KNN: ', confusion_matrix(y_test, y_knc))\nprint('precision_score of KNN: ', precision_score(y_test, y_knc))\nprint('recall_score of KNN: ', recall_score(y_test, y_knc))\nprint('precision_recall_curve of KNN: ', precision_recall_curve(y_test, y_knc))","fae83d41":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nprint(\"linear model intercept (b) :{:.3f}\".format(lr.intercept_))\nprint('linear model coeff (w) :{}'.format(lr.coef_))\nprint('Accuracy score of Linear Regression training set: {:.3f}'.format(lr.score(X_train, y_train)))\nprint('Accuracy score Linear Regression test set: {:.3f}'.format(lr.score(X_test, y_test)))","b6117eae":"from sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor()\nrfr.fit(X_train, y_train)\nprint('Accuracy score (training): {:.3f}'.format(rfr.score(X_train, y_train)))\nprint('Accuracy score (test): {:.3f}'.format(rfr.score(X_test, y_test)))","b29a5a45":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\nprint('Accuracy score Random Forest Classifier training set: {:.3f}'.format(rfc.score(X_train, y_train)))\nprint('Accuracy score Random Forest Classifier test set: {:.3f}'.format(rfc.score(X_test, y_test)))","62c5a160":"y_rfc = rfc.predict(X_test)\n\nprint('confusion_matrix of Random Forest Classifier: ', confusion_matrix(y_test, y_rfc))\nprint('precision_score of Random Forest Classifier: ', precision_score(y_test, y_rfc))\nprint('recall_score of Random Forest Classifier: ', recall_score(y_test, y_rfc))\nprint('precision_recall_curve of Random Forest Classifier: ', precision_recall_curve(y_test, y_rfc))","1d9eacc9":"from sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 42)\nclassifier.fit(X_train, y_train)\n\nprint('accuracy of Decision Tree Classifier training set: {:.3f}'.format(classifier.score(X_train,y_train)))\nprint('accuaracy of Decision Tree Classifier test set: {:.3f}'.format(classifier.score(X_test, y_test)))\n","8cd1548d":"y_dtc = classifier.predict(X_test)\n\nprint('accuracy_score of decesion tree classifier: ', accuracy_score(y_dtc, y_test))\nprint('confusion_matrix of decision tree classifier: ', confusion_matrix(y_dtc, y_test))\nprint('precision_score of decision tree classifier: ', precision_score(y_dtc, y_test))\nprint('recall_score of decision tree classifier: ', recall_score(y_dtc, y_test))\nprint('precision_recall_curve of decision tree classifier: ', precision_recall_curve(y_dtc, y_test))","1cb4975f":"from xgboost import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(X_train, y_train)\nprint('Accuracy XGBclassifier on train set: {:.3f}'.format(xgb.score(X_train, y_train)))\nprint('Accuracy XGBClassifier on test set: {:.3f}'.format(xgb.score(X_test, y_test)))","c89da705":"y_xgbc = xgb.predict(X_test)\n# predicting Confusion matrix, accuracy score,precision score, recall score\nprint('accuracy_score of xgboost: ', accuracy_score(y_test, y_xgbc))\nprint('confusion_matrix of xgboost: ', confusion_matrix(y_test, y_xgbc))\nprint('precision_score of xgboost: ', precision_score(y_test, y_xgbc))\nprint('recall_score of xgboost: ', recall_score(y_test, y_xgbc))\nprint('precision_recall_curve of xgboost: ', precision_recall_curve(y_test, y_xgbc))","345d1551":"# prediction using Naive Bayes Algorithm \nfrom sklearn.naive_bayes import GaussianNB\nnbc = GaussianNB()\nnbc.fit(X_train, y_train)\n\nprint('accuracy of Naive Bayes training set: {:.3f}'.format(nbc.score(X_train,y_train)))\nprint('accuaracy of Naive Bayes test set: {:.3f}'.format(nbc.score(X_test, y_test)))","32642a2e":"y_nb = nbc.predict(X_test)\n\nprint('accuracy_score of Naive Bayes: ', accuracy_score(y_test, y_nb))\nprint('confusion_matrix of Naive Bayes: ', confusion_matrix(y_test, y_nb))\nprint('precision_score of Naive Bayes: ', precision_score(y_test, y_nb))\nprint('recall_score of Naive Bayes: ', recall_score(y_test, y_nb))\nprint('precision_recall_curve of Naive Bayes: ', precision_recall_curve(y_test, y_nb))","1cb19b35":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 15, kernel_initializer = 'uniform', activation = 'relu', input_dim = 19))\n\n# Adding the second hidden layer\nclassifier.add(Dense(units = 15, kernel_initializer = 'uniform', activation = 'relu'))\n\n# Adding the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nclassifier.fit(X_train, y_train, batch_size = 10, epochs = 15)\n","393d9651":"classifier.summary()","18c95ad0":"In this kernel, I tired plotting different graphs then trained the data using different machine learning algorithms.\n\n# Upvote if you like this kernel.","6bb15fdd":"# Random Forest Algorithm\n\nRandom forest algorithm is a supervised classification algorithm. As the name suggest, this algorithm creates the forest with a number of trees.\n\nThe same random forest algorithm or the random forest classifier can use for both classification and the regression task.\nRandom forest classifier will handle the missing values.\nWhen we have more trees in the forest, random forest classifier won\u2019t overfit the model.\nCan model the random forest classifier for categorical values also.","57a6809b":"# Artificial Neural Networks","388c75e1":"# Naive Bayes Algorithm\nIt is a classification technique based on Bayes\u2019 Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. ","5ee46063":"# XGBoost Algorithm\nXGBoost is an implementation of gradient boosted decision trees designed for speed and performance.","bc46881c":"# Telco Customer Churn\n![alt text](https:\/\/miro.medium.com\/max\/1104\/0*6hXwLKd67L__lTsg.png)","40f66d4d":"# K Nearest Neighbors Classification\n\nIn k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.","a21657d5":"# Decision Tree Classification","59950916":"The dataset describes Telco Customer Churn data.  The dataset contains 21 Columns and 7043 Rows which has both numeric and non-numeric data.","5b7695ad":"# Linear Regression\nLinear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable.\n\nA linear regression line has an equation of the form Y = a + bX, \nwhere X is the explanatory variable and\nY is the dependent variable.\nThe slope of the line is b, and a is the intercept (the value of y when x = 0).","f0910698":"# Train Test Spliting\nWe will  split the data into a training and a test part. The models will be trained on the training data set and tested on the test data set."}}