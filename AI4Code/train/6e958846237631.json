{"cell_type":{"cd689ffd":"code","7d14e6d4":"code","32944db2":"code","b7cb1518":"code","9b501ee8":"code","3c9af753":"code","73fcd30a":"code","8f890d01":"code","f8f23911":"code","14368c41":"code","7cc41678":"code","6d42a457":"code","a1c8407c":"code","3af4696d":"code","2c2f234c":"code","a3d59555":"code","cba8a21b":"code","250ba489":"markdown","67410d3d":"markdown","fbe3544c":"markdown","d160a819":"markdown","065d25c1":"markdown","3fd9d902":"markdown","95802bc5":"markdown","d28ab04a":"markdown","6bc9a966":"markdown","14e0790c":"markdown","c9c018d9":"markdown","fd2a8699":"markdown","cbe5043a":"markdown","dea09fe5":"markdown"},"source":{"cd689ffd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7d14e6d4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndf_train = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-1\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-1\/test.csv')\nprint(df_train.shape)\nprint(df_test.shape)","32944db2":"df_train.columns","b7cb1518":"df_train['Province\/State'] = df_train.apply(\n    lambda row: row['Country\/Region'] if pd.isnull(row['Province\/State']) else row['Province\/State'],\n    axis=1\n)\ndf_test['Province\/State'] = df_test.apply(\n    lambda row: row['Country\/Region'] if pd.isnull(row['Province\/State']) else row['Province\/State'],\n    axis=1\n)","9b501ee8":"df_train['Date'] = df_train.apply(\n    lambda row: pd.Timestamp(row['Date']).value\/\/10**9,\n    axis=1\n)\ndf_test['Date'] = df_test.apply(\n    lambda row: pd.Timestamp(row['Date']).value\/\/10**9,\n    axis=1\n)","3c9af753":"import matplotlib.pyplot as plt\ndf = df_train.sort_values('Date')\nplt.plot(df['Date'],np.log2(df['ConfirmedCases'])\/np.log2(1.5))\nplt.show()\nplt.plot(df['Date'],np.log2(df['Fatalities'])\/np.log2(1.5))\nplt.show()","73fcd30a":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import Normalizer\nvectorizer = CountVectorizer(binary=True)\nvectorizer.fit(df_train['Province\/State'])\nstate_train = vectorizer.transform(df_train['Province\/State'])\nstate_test = vectorizer.transform(df_test['Province\/State'])\n\nvectorizer = CountVectorizer(binary=True)\nvectorizer.fit(df_train['Country\/Region'])\ncountry_train = vectorizer.transform(df_train['Country\/Region'])\ncountry_test = vectorizer.transform(df_test['Country\/Region'])","8f890d01":"normalizer = Normalizer()\nnormalizer.fit(df_train['Lat'].values.reshape(1, -1))\nlat_train = normalizer.transform(df_train['Lat'].values.reshape(1, -1))\nlat_test = normalizer.transform(df_test['Lat'].values.reshape(1, -1))\n\nnormalizer = Normalizer()\nnormalizer.fit(df_train['Long'].values.reshape(1, -1))\nlong_train = normalizer.transform(df_train['Long'].values.reshape(1, -1))\nlong_test = normalizer.transform(df_test['Long'].values.reshape(1, -1))\n\nnormalizer = Normalizer()\nnormalizer.fit(df_train['Date'].values.reshape(1, -1))\ndate_train = normalizer.transform(df_train['Date'].values.reshape(1, -1))\ndate_test = normalizer.transform(df_test['Date'].values.reshape(1, -1))","f8f23911":"print(state_train.shape)\nprint(country_train.shape)\nprint(lat_train.reshape(-1, 1).shape)\nprint(long_train.shape)\nprint(date_train.shape)","14368c41":"from scipy.sparse import hstack\ndata_train = hstack([state_train, country_train, lat_train.reshape(-1, 1), long_train.reshape(-1, 1), date_train.reshape(-1, 1)])\ndata_test = hstack([state_test, country_test, lat_test.reshape(-1, 1), long_test.reshape(-1, 1), date_test.reshape(-1, 1)])\n#data_train=data_train.todense()\n#data_test=data_test.todense()","7cc41678":"#print(data_test[0])","6d42a457":"from sklearn import linear_model\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nclf = XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints=None,\n             learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n             min_child_weight=1, monotone_constraints=None,\n             n_estimators=1000, n_jobs=0, num_parallel_tree=1,\n             objective='reg:squarederror', random_state=0, reg_alpha=0,\n             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n             validate_parameters=False, verbosity=None)\nclf.fit(data_train.todense(), df_train['ConfirmedCases'])\n","a1c8407c":"#conf_cased_pred_tr = clf.predict(data_test.todense())\nconf_cased_pred = clf.predict(data_test.todense())\nnormalizer = Normalizer()\nnormalizer.fit(df_train['ConfirmedCases'].values.reshape(1, -1))\nconf_cased_pred_train = normalizer.transform(df_train['ConfirmedCases'].values.reshape(1, -1))\nconf_cased_pred_test = normalizer.transform(conf_cased_pred.reshape(1, -1))\ndata_train_with_conf = hstack([state_train, country_train, lat_train.reshape(-1, 1), long_train.reshape(-1, 1), date_train.reshape(-1, 1), conf_cased_pred_train.reshape(-1,1)])\ndata_test_with_conf = hstack([state_test, country_test, lat_test.reshape(-1, 1), long_test.reshape(-1, 1), date_test.reshape(-1, 1),conf_cased_pred_test.reshape(-1, 1)])","3af4696d":"print(np.mean(conf_cased_pred))","2c2f234c":"clf = XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints=None,\n             learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n             min_child_weight=1, monotone_constraints=None,\n             n_estimators=1000, n_jobs=0, num_parallel_tree=1,\n             objective='reg:squarederror', random_state=0, reg_alpha=0,\n             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n             validate_parameters=False, verbosity=None)\nclf.fit(data_train_with_conf.todense(), np.log(df_train['Fatalities']+0.000000001))\nfatalities_pred = clf.predict(data_test_with_conf.todense())\n#fatalities_pred = np.exp(fatalities_pred)\n","a3d59555":"print(np.mean(fatalities_pred))","cba8a21b":"def make_submission(conf, fat, sub_name):\n    my_submission = pd.DataFrame({'ForecastId':pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-1\/test.csv').ForecastId,'ConfirmedCases':conf, 'Fatalities':fat})\n    my_submission.to_csv('{}.csv'.format(sub_name),index=False)\n    print('A submission file has been made')\nmake_submission(conf_cased_pred,fatalities_pred,'submission')","250ba489":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nNN_model = Sequential()\n\n# The Input Layer :\nNN_model.add(Dense(256, kernel_initializer='normal',input_dim = 494, activation='relu'))\n\n# The Hidden Layers :\nNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\nNN_model.add(Dense(128, kernel_initializer='normal',activation='relu'))\n#NN_model.add(Flatten())\nNN_model.add(Dropout(0.4))\nNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n\n# The Output Layer :\nNN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n\n# Compile the network :\nNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\nNN_model.summary()\nNN_model.fit(data_train.todense(), df_train['ConfirmedCases'], epochs=100,batch_size=64, validation_split = 0.1)","67410d3d":"from sklearn import linear_model\ndata_country={}\nlabel_country={}\nfor index, row in df_train.iterrows():\n    if row['Country\/Region'] not in data_country:\n        data_country[row['Country\/Region']]=[]\n        label_country[row['Country\/Region']]=[]\n    \n    data_country[row['Country\/Region']].append(data_train[index])\n    label_country[row['Country\/Region']].append(row['ConfirmedCases'])\nmodel_country={}\nfor country in data_country.keys():\n    model_country[country]=linear_model.SGDRegressor(max_iter=1000, tol=1e-3)\n    #print(np.array(data_country[country]).reshape(len(label_country[country]),-1).shape)\n    #print(len(label_country[country]))\n    model_country[country].fit(np.array(data_country[country]).reshape(len(label_country[country]),-1),label_country[country])","fbe3544c":"data_country={}\nlabel_country={}\nfor index, row in df_train.iterrows():\n    if row['Country\/Region'] not in data_country:\n        data_country[row['Country\/Region']]=[]\n        label_country[row['Country\/Region']]=[]\n    \n    data_country[row['Country\/Region']].append(data_train_with_conf[index])\n    label_country[row['Country\/Region']].append(row['Fatalities'])\nmodel_country={}\nfor country in data_country.keys():\n    model_country[country]=linear_model.SGDRegressor(max_iter=1000, tol=1e-3)\n    #print(np.array(data_country[country]).reshape(len(label_country[country]),-1).shape)\n    #print(len(label_country[country]))\n    model_country[country].fit(np.array(data_country[country]).reshape(len(label_country[country]),-1),label_country[country])","d160a819":"def make_submission(conf, fat, sub_name):\n    my_submission = pd.DataFrame({'ForecastId':pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-1\/test.csv').ForecastId,'ConfirmedCases':conf, 'Fatalities':fat})\n    my_submission.to_csv('{}.csv'.format(sub_name),index=False)\n    print('A submission file has been made')\nmake_submission(np.ndarray.flatten(conf_cased_pred),np.ndarray.flatten(fatalities_pred),'submission')","065d25c1":"fatalities_pred=[]\nfor index, row in df_test.iterrows():\n    fatalities_pred.append(model_country[row['Country\/Region']].predict(data_test_with_conf[index]))","3fd9d902":"conf_cased_pred=[]\nfor index, row in df_test.iterrows():\n    conf_cased_pred.append(model_country[row['Country\/Region']].predict(data_test[index]))\nnormalizer = Normalizer()\nnormalizer.fit(df_train['ConfirmedCases'].values.reshape(1, -1))\nconf_cased_pred_train = normalizer.transform(df_train['ConfirmedCases'].values.reshape(1, -1))\nconf_cased_pred_test = normalizer.transform(conf_cased_pred)\ndata_train_with_conf = hstack([state_train, country_train, lat_train.reshape(-1, 1), long_train.reshape(-1, 1), date_train.reshape(-1, 1), conf_cased_pred_train.reshape(-1,1)])\ndata_test_with_conf = hstack([state_test, country_test, lat_test.reshape(-1, 1), long_test.reshape(-1, 1), date_test.reshape(-1, 1),conf_cased_pred_test.reshape(-1, 1)])\ndata_train_with_conf = data_train_with_conf.todense()\ndata_test_with_conf = data_test_with_conf.todense()","95802bc5":"print(len(np.ndarray.flatten(conf_cased_pred)))\nprint(fatalities_pred.shape)","d28ab04a":"param_grid = dict(n_estimators=np.array([50,100,200,300,400]))\nmodel = GradientBoostingRegressor(random_state=21)\nkfold = KFold(n_splits=10, random_state=21)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=kfold)\ngrid_result = grid.fit(data_train_with_conf.todense(), df_train['Fatalities'])\nprint(grid_result.best_estimator_)","6bc9a966":"NN_model = Sequential()\n# The Input Layer :\nNN_model.add(Dense(256, kernel_initializer='normal',input_dim = 495, activation='relu'))\n\n# The Hidden Layers :\nNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\nNN_model.add(Dense(128, kernel_initializer='normal',activation='relu'))\n#NN_model.add(Flatten())\nNN_model.add(Dropout(0.4))\nNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n\n# The Output Layer :\nNN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n\n# Compile the network :\nNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\nNN_model.summary()\nNN_model.fit(data_train_with_conf.todense(), df_train['Fatalities'], epochs=100,batch_size=64, validation_split = 0.1)","14e0790c":"import pandas_profiling\npandas_profiling.ProfileReport(df_train)","c9c018d9":"fatalities_pred = NN_model.predict(data_test_with_conf.todense())","fd2a8699":"conf_cased_pred=np.concatenate( conf_cased_pred, axis=0 )\nfatalities_pred= np.concatenate( fatalities_pred, axis=0 )\nprint(conf_cased_pred[:5])\nprint(fatalities_pred[:5])","cbe5043a":"conf_cased_pred = NN_model.predict(data_test.todense())\nnormalizer = Normalizer()\nnormalizer.fit(df_train['ConfirmedCases'].values.reshape(1, -1))\nconf_cased_pred_train = normalizer.transform(df_train['ConfirmedCases'].values.reshape(1, -1))\nconf_cased_pred_test = normalizer.transform(conf_cased_pred.reshape(1, -1))\ndata_train_with_conf = hstack([state_train, country_train, lat_train.reshape(-1, 1), long_train.reshape(-1, 1), date_train.reshape(-1, 1), conf_cased_pred_train.reshape(-1,1)])\ndata_test_with_conf = hstack([state_test, country_test, lat_test.reshape(-1, 1), long_test.reshape(-1, 1), date_test.reshape(-1, 1),conf_cased_pred_test.reshape(-1, 1)])","dea09fe5":"from sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nparam_grid = dict(n_estimators=np.array([50,100,200,300,400]), subsample=np.array([0.5,0.6,0.7,0.8,0.9,1]), max_depth=np.array([3,5,7,9]))\nmodel = GradientBoostingRegressor(random_state=21)\nkfold = KFold(n_splits=10, random_state=21)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=kfold)\ngrid_result = grid.fit(data_train.todense(), df_train['ConfirmedCases'])\nprint(grid_result.best_estimator_)"}}