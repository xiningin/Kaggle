{"cell_type":{"f4bd087f":"code","6021a13b":"code","e2852a94":"code","eff82ed4":"code","f0ebfc1a":"code","ad70cc22":"code","8a77cdd6":"code","122838d1":"markdown"},"source":{"f4bd087f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6021a13b":"import matplotlib.pyplot as plt","e2852a94":"from sklearn import datasets\nX, y = datasets.make_blobs(n_samples=150,n_features=2,\n                           centers=2,cluster_std=1.05,\n                           random_state=2)\n#Plotting\nfig = plt.figure(figsize=(10,8))\nplt.plot(X[:, 0][y == 0], X[:, 1][y == 0], 'r^')\nplt.plot(X[:, 0][y == 1], X[:, 1][y == 1], 'bs')\nplt.xlabel(\"feature 1\")\nplt.ylabel(\"feature 2\")\nplt.title('Random Classification Data with 2 classes')","eff82ed4":"def step_func(z):\n        return 1.0 if (z > 0) else 0.0","f0ebfc1a":"def perceptron(X, y, lr, epochs):\n    \n    # X --> Inputs.\n    # y --> labels\/target.\n    # lr --> learning rate.\n    # epochs --> Number of iterations.\n    \n    # m-> number of training examples\n    # n-> number of features \n    m, n = X.shape\n    \n    # Initializing parapeters(theta) to zeros.\n    # +1 in n+1 for the bias term.\n    theta = np.zeros((n+1,1))\n    \n    # Empty list to store how many examples were \n    # misclassified at every iteration.\n    n_miss_list = []\n    \n    # Training.\n    for epoch in range(epochs):\n        \n        # variable to store #misclassified.\n        n_miss = 0\n        \n        # looping for every example.\n        for idx, x_i in enumerate(X):\n            \n            # Insering 1 for bias, X0 = 1.\n            x_i = np.insert(x_i, 0, 1).reshape(-1,1)\n            \n            # Calculating prediction\/hypothesis.\n            y_hat = step_func(np.dot(x_i.T, theta))\n            \n            # Updating if the example is misclassified.\n            if (np.squeeze(y_hat) - y[idx]) != 0:\n                theta += lr*((y[idx] - y_hat)*x_i)\n                \n                # Incrementing by 1.\n                n_miss += 1\n        \n        # Appending number of misclassified examples\n        # at every iteration.\n        n_miss_list.append(n_miss)\n        \n    return theta, n_miss_list","ad70cc22":"def plot_decision_boundary(X, theta):\n    \n    # X --> Inputs\n    # theta --> parameters\n    \n    # The Line is y=mx+c\n    # So, Equate mx+c = theta0.X0 + theta1.X1 + theta2.X2\n    # Solving we find m and c\n    x1 = [min(X[:,0]), max(X[:,0])]\n    m = -theta[1]\/theta[2]\n    c = -theta[0]\/theta[2]\n    x2 = m*x1 + c\n    \n    # Plotting\n    fig = plt.figure(figsize=(10,8))\n    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"r^\")\n    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\")\n    plt.xlabel(\"feature 1\")\n    plt.ylabel(\"feature 2\")\n    plt.title(\"Perceptron Algorithm\")\n    plt.plot(x1, x2, 'y-')","8a77cdd6":"theta, miss_l = perceptron(X, y, 0.5, 100)\nplot_decision_boundary(X, theta)","122838d1":"There are two classes, red and green, and we want to separate them by drawing a straight line between them. Or, more formally, we want to learn a set of parameters theta to find an optimal hyperplane(straight line for our data) that separates the two classes."}}