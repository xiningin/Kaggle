{"cell_type":{"184a4ae4":"code","0d4b93a9":"code","115d60cf":"code","1da8f789":"code","b945b942":"code","1adbe4cf":"code","5630089e":"code","caadd201":"code","e5b6e859":"code","118c9dd4":"code","7f8f0bbb":"code","86b2956e":"code","950fff96":"code","56c5115b":"code","97b77952":"code","a6085436":"code","da3cb96c":"code","201bdc7d":"code","ebcb2191":"code","d165c94c":"code","9aff3e20":"code","54afbbc3":"markdown","6e58e0b3":"markdown","88ddf3a1":"markdown","5e532122":"markdown","bb6b4c75":"markdown","fdd95c10":"markdown","3b1305ca":"markdown","b86676ca":"markdown","aff5f563":"markdown","da158d68":"markdown","e97b8eb7":"markdown","64c08587":"markdown","c3435c91":"markdown","6752d987":"markdown","f400dc8f":"markdown"},"source":{"184a4ae4":"#for loading data and for performing data analysis operations on it\nimport pandas as pd\nimport numpy as np\n\n#for data visualization\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\n#for PCA (feature engineering)\nfrom sklearn.decomposition import PCA\n\n#for data scaling\nfrom sklearn.preprocessing import StandardScaler\n\n#for splitting dataset\nfrom sklearn.model_selection import train_test_split\n\n#for fitting SVM model\nfrom sklearn.svm import SVC\n\n#for displaying evaluation metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n#for file operations\nimport os\n\nprint(\"All required libraries loaded!\")","0d4b93a9":"#check the files in the given input folder\nprint(os.listdir(\"..\/input\/\"))","115d60cf":"#load dataset into pandas dataframe\ndf = pd.read_csv(\"..\/input\/data.csv\")\ndf.shape","1da8f789":"#check the data types of all the attributes loaded into the dataframe\ndf.dtypes","b945b942":"#see first few rows of the data loaded\ndf.head()","1adbe4cf":"#see last few rows of the data loaded\ndf.tail()","5630089e":"#loading the predictors into dataframe 'X'\n#NOTE: we are not choosing columns - 'id', 'diagnosis', 'Unnamed:32'\nX = df.iloc[:,2:32]\nprint(X.shape)\nX.head()","caadd201":"#loading target values into dataframe 'y'\ny = df.diagnosis\nprint(y.shape)\ny.head()","e5b6e859":"#coverting categorical data to numerical data\ny_num = pd.get_dummies(y)\ny_num.tail()","118c9dd4":"#use only one column for target value\ny = y_num.M\nprint(y.shape)\ny.tail()","7f8f0bbb":"#call corr() on dataframe X\nX.corr()","86b2956e":"plt.figure(figsize=(18, 12))\nsns.heatmap(X.corr(), vmin=0.85, vmax=1, annot=True, cmap='YlGnBu', linewidths=.5)","950fff96":"#reducing the attributes in X dataframe\n\n#1 scale the data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n#2 drop the highly correlated columns which are not useful i.e., area, perimeter, perimeter_worst, area_worst, perimeter_se, area_se \nX_scaled = pd.DataFrame(X_scaled)\nX_scaled_drop = X_scaled.drop(X_scaled.columns[[2, 3, 12, 13, 22, 23]], axis=1)\n\n#3 apply PCA on scaled data\npca = PCA(n_components=0.95)\nx_pca = pca.fit_transform(X_scaled_drop)\nx_pca = pd.DataFrame(x_pca)\n\nprint(\"Before PCA, X dataframe shape = \",X.shape,\"\\nAfter PCA, x_pca dataframe shape = \",x_pca.shape)","56c5115b":"print(pca.explained_variance_ratio_) \nprint(pca.explained_variance_ratio_.sum())","97b77952":"#combine PCA data and target data\n\n#1 set column names for the dataframe\ncolnames = ['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10','PC11','diagnosis']\n\n#target data\ndiag = df.iloc[:,1:2]\n\n#combine PCA and target data\nXy = pd.DataFrame(np.hstack([x_pca,diag.values]),columns=colnames)\n\nXy.head()","a6085436":"#visualize data\nsns.lmplot(\"PC1\", \"PC2\", hue=\"diagnosis\", data=Xy, fit_reg=False, markers=[\"o\", \"x\"])\nplt.show()","da3cb96c":"X=(Xy.iloc[:,0:11]).values\n#75:25 train:test data splitting\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\nprint(\"X_train shape \",X_train.shape)\nprint(\"y_train shape \",y_train.shape)\nprint(\"X_test shape \",X_test.shape)\nprint(\"y_test shape \",y_test.shape)","201bdc7d":"#model fitting\nsvc = SVC()\nsvc.fit(X_train, y_train)","ebcb2191":"#predict values\ny_pred_svc =svc.predict(X_test)\ny_pred_svc.shape","d165c94c":"#print confusion matrix\ncm = confusion_matrix(y_test, y_pred_svc)\nprint(\"Confusion matrix:\\n\",cm)","9aff3e20":"#print classification report\ncreport = classification_report(y_test, y_pred_svc)\nprint(\"Classification report:\\n\",creport)","54afbbc3":"## 4. PCA (Principal component analysis)\/Dimensionality reduction\n\n- We have 30 columns\/attributes in the predictors dataframe. Dealing with so many attributes becomes difficult because it is hard to visualize data in 30 different dimensions.\n- The aim in this step is to reduce the attributes from 30 to ~10 without loosing the key components of the available data.\n\n**What is PCA in a nutshell?**\n- Identifying and choosing the principal components among the various components we have to predict an output efficiently.\n\nThe above explanation may seem very vague. I suggest reading this [blogpost on medium](https:\/\/towardsdatascience.com\/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c) which answers the fundamental questions about PCA.","6e58e0b3":"### Visualize data after PCA","88ddf3a1":"### References\n- https:\/\/www.kaggle.com\/saramille\/bc-collinearity-and-dimensional-reduction\n- https:\/\/www.kaggle.com\/saramille\/breast-cancer-prediction-knn-svc-and-logistic\n- https:\/\/www.geeksforgeeks.org\/classifying-data-using-support-vector-machinessvms-in-python\/\n- https:\/\/towardsdatascience.com\/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c","5e532122":"**What are we trying to solve?**<br>\nUsing patients diagnosis data (***here it is data.csv***) we are trying to predict whether a patient has cancer or not.<br>\n\nFor predicting whether a patient has cancer or not we must separate out the attributes from the dataset into **predictors** and **target**. Additionally, we will also drop the attributes\/columns which we will not be useful for predicting the target value.<br>\n\n**What is the target attribute in this dataset?**<br>\n'diagnosis' column which holds the value M\/B is the target attribute because our aim is to predict whether a patient has cancer or not given the symptoms.\n\n**What are the predictors in this dataset?**<br>\nAll attributes in this dataset except ***'id', 'diagnosis'*** are predictors. Let's see the usefulness of these predictors in further data analysis steps like - PCA, dimensionality reduction.","bb6b4c75":"### Check correlation among predictors","fdd95c10":"**What we can observe from the above correlations heatmap?**<br>\n- It is observed that there are some attributes which are highly correlated with each other whereas other attributes are loosely correlated with each other.\n- Attributes like 'radius_x' , 'perimeter_x' , 'area_x' are highly correlated because perimeter and area depend on radius, so we can use 'radius_x' and ignore perimeter, area attributes in our data analysis.\n\nAfter observing the correlation heatmap we can say what attributes among the predictors will be useful for predictions and which can be ignored.","3b1305ca":"## 5. Split data for training and testing","b86676ca":"- For model fitting we will need numerical data only, but here the 'diagnosis' data is not numerical.\n- The categorical data has to be converted to numerical data.\n\nLet's use [get_dummies()](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.get_dummies.html) pandas method for converting categorical data to numerical data.","aff5f563":"- As we in the above output [get_dummies()](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.get_dummies.html) pandas method creates two numerical columns for categories - 'B', 'M'.\n- The values under column 'M' are 0 and 1. 0 -> for not Malignant, 1 -> for Malignant.\n- We are interested in finding out whether the patient has cancer or not hence let's use 'M' column only.","da158d68":"### Breast cancer prediction using SVM\n- This kernel is for practice purposes. DS\/ML newbies may find this helpful :)\n- Reference links are provided in the end section of this kernel.","e97b8eb7":"## 3. Data exploration","64c08587":"## 6. SVM model fitting","c3435c91":"- It is difficult to identify the attributes which are strongly using the numerical data produced by **corr()** method, so let's use [seaborn heatmap](https:\/\/seaborn.pydata.org\/generated\/seaborn.heatmap.html) to visualize the correlations among the attributes.","6752d987":"## 2. Load the dataset","f400dc8f":"## 1. Import all required libraries"}}