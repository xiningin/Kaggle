{"cell_type":{"87ca718e":"code","1025cc90":"code","0e651cab":"code","18054227":"code","641cea95":"code","4246032a":"code","19ffb99c":"code","a947014c":"code","2e5eb316":"code","54e4663b":"code","bfb3e1e3":"code","8d05d6af":"code","ea3f7c19":"code","a2b1538c":"code","e130ede1":"code","f9a3c562":"code","c8339326":"code","950ef0d3":"code","e5cce106":"code","526f3cd3":"code","6b6e0783":"code","6897eda8":"code","14e7c585":"code","05226b04":"code","1dcc3c15":"code","1c06a9de":"code","ea0eb722":"code","d8648fca":"code","41e87fb8":"code","df3ff535":"markdown","d2d3bfbd":"markdown","60100197":"markdown","ff0dfe02":"markdown","4ff88574":"markdown","21781276":"markdown","2f72e9a8":"markdown","445a28f4":"markdown","a779e488":"markdown","113fa11c":"markdown","ab0fbefd":"markdown","a88ccf33":"markdown","416ed79b":"markdown"},"source":{"87ca718e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sb\nimport matplotlib.pyplot as plt","1025cc90":"\ntrain_df = pd.read_csv('\/kaggle\/input\/song-popularity-prediction\/train.csv', index_col=0)\ntest_df = pd.read_csv('\/kaggle\/input\/song-popularity-prediction\/test.csv', index_col=0)\ntrain_df.head()","0e651cab":"train_df.info()","18054227":"train_df.describe()","641cea95":"# plotting correlation heatmap\ncorr= train_df.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sb.axes_style(\"white\"):\n    fig, ax = plt.subplots(figsize=(15,10))\n    sb.heatmap(corr,mask=mask, vmax=.3, cmap=\"YlGnBu\", annot=True, ax=ax)","4246032a":"train_df.song_popularity.value_counts().plot.pie()","19ffb99c":"train_df.song_popularity.value_counts(normalize=True)*100","a947014c":"train_df.info()","2e5eb316":"feature_col = train_df.columns[:-1]","54e4663b":"fig, axes = plt.subplots(7, 2, figsize=(15, 30))\nplt.tight_layout()\nfig.suptitle('Density plot')\nfor i, col in enumerate(feature_col):\n    sb.kdeplot(ax=axes[i\/\/2, i%2], data=train_df, x=col, hue=\"song_popularity\", multiple=\"stack\")","bfb3e1e3":"fig, axes = plt.subplots(7, 2, figsize=(15, 30))\nplt.tight_layout()\nfig.suptitle('Density plot')\nfor i, col in enumerate(feature_col):\n    sb.boxplot(ax=axes[i\/\/2, i%2], data=train_df, x=col)","8d05d6af":"train_df.columns","ea3f7c19":"train_df.info()","a2b1538c":"filtered_train_df = train_df.query('song_duration_ms>=90000 and song_duration_ms<=3300000 and instrumentalness<=0.4 and liveness<=0.5 and loudness>=-18 and speechiness <=0.3 and tempo<=180')\nfiltered_train_df.info()","e130ede1":"fig, axes = plt.subplots(7, 2, figsize=(15, 30))\nplt.tight_layout()\nfig.suptitle('Density plot')\nfor i, col in enumerate(feature_col):\n    sb.boxplot(ax=axes[i\/\/2, i%2], data=filtered_train_df, x=col)","f9a3c562":"filtered_train_df.info()","c8339326":"test_df.info()","950ef0d3":"X_train_main = filtered_train_df.drop(columns=['song_popularity'])\nY_train_main = filtered_train_df['song_popularity']","e5cce106":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp.fit(X_train_main)\nmy_array = imp.transform(X_train_main)\ntest_array = imp.transform(test_df)\nX_train_full = pd.DataFrame(my_array, columns=X_train_main.columns)\ntest_df_main = pd.DataFrame(test_array,columns=test_df.columns)\nX_train_full.describe()","526f3cd3":"from sklearn.model_selection import train_test_split\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.ensemble  import RandomForestClassifier\n#Create an instance\nfrom sklearn.metrics import roc_auc_score\nfrom catboost import CatBoostClassifier\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_full, Y_train_main, test_size=0.15, random_state=42)","6b6e0783":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\ndef algorithm_pipeline(X_train_data, X_valid_data, y_train_data, y_valid_data, \n                       model, param_grid, cv=5, scoring_fit='roc_auc',\n                       do_probabilities = True, cat_features=None):\n    \n    gs = GridSearchCV(\n        estimator=model,\n        param_grid=param_grid, \n        cv=cv, \n        n_jobs=-1, \n        scoring=scoring_fit,\n        verbose=2\n    )\n    \n    fitted_model = gs.fit(X_train_data, y_train_data)\n    \n    if do_probabilities:\n        pred = fitted_model.predict_proba(X_valid_data)\n    else:\n        pred = fitted_model.predict(X_valid_data)\n    \n    return fitted_model, pred\n\n# random_forest = RandomForestClassifier()\n# param_grid = {\n#     'n_estimators': [80, 100, 120],\n#     'max_depth': [5, 10],\n#     'max_leaf_nodes': [70, 90]\n# }\n\n# estimator, pred = algorithm_pipeline(X_train, X_valid, y_train, y_valid, random_forest, \n#                                  param_grid, cv=5)\n\n# best_random_forest = estimator.best_estimator_\n# print(estimator.best_score_)\n# print(estimator.best_params_)","6897eda8":"# catboostClassifier = CatBoostClassifier(iterations=250, depth=3, learning_rate=0.01, eval_metric='AUC')\n# classifier = BalancedBaggingClassifier(base_estimator=catboostClassifier,\n#                                 sampling_strategy='all',\n#                                 replacement=False,\n#                                 random_state=42)\n# classifier.fit(X_train, y_train)\n# y_predict = model.predict_proba(X_valid)[:, 1]\n# roc_auc_score(y_valid, y_predict)","14e7c585":"rm_forest = RandomForestClassifier(max_depth= 10, max_leaf_nodes=90, n_estimators=120)\nclassifier = BalancedBaggingClassifier(base_estimator=rm_forest,\n                                       n_estimators = 15,\n                                sampling_strategy='all',\n                                replacement=False,\n                                random_state=42)\nclassifier.fit(X_train, y_train)\nroc_auc_score(y_valid, classifier.predict_proba(X_valid)[:, 1])","05226b04":"# print(\"Threshold | Score\")\n# for threshold in range(45, 56):\n#     print(str(threshold\/100), \"|\" ,str(roc_auc_score(y_valid,(classifier.predict_proba(X_valid)[:, 1]>threshold\/100)*1)))","1dcc3c15":"rm_forest = RandomForestClassifier(max_depth= 10, max_leaf_nodes=90, n_estimators=120)\nclassifier = BalancedBaggingClassifier(base_estimator=rm_forest,\n                                       n_estimators = 15,\n                                sampling_strategy='all',\n                                replacement=False,\n                                random_state=42)\nclassifier.fit(X_train_full, Y_train_main)","1c06a9de":"# from sklearn.metrics import roc_auc_score\n# roc_auc_score(y_valid, clf.predict(X_valid))","ea0eb722":"submission_df = pd.read_csv('\/kaggle\/input\/song-popularity-prediction\/sample_submission.csv', index_col=0)\nsubmission_df.head()","d8648fca":"submission_df['song_popularity'] = classifier.predict_proba(test_df_main)[:, 1]\nsubmission_df['song_popularity'].hist()","41e87fb8":"submission_df.to_csv('submission.csv')","df3ff535":"### Outlier Detection:","d2d3bfbd":"* We can say that values are normalized on the scale of 0 and 1\n\n## Step3. Model Training","60100197":"It seems similiar distribution for both target. **This features doesn't seems much impactful**. We can always experiment with dropping few features, There no harm in that","ff0dfe02":"We don't have any missing value as of now","4ff88574":"According to above boxplots there are outliers in  the features like `song_duration`, `energy`, `instrumentalness`, `loudness`, `liveness`, `speechiness`, `tempo` lets filter such records in the train data","21781276":"This shows that we gave **40,000** training records","2f72e9a8":"This shows that our data is having `36.44%` labels with `Popularity` target\n\n### Target Imapct Analysis on Features","445a28f4":"#### 1. Song_Duration","a779e488":"## Observations\n* Above observations shows that columns like `song_duration_ms`, `acousticness`, `danceability`, `energy`, `instrumentalness`, `key`, `liveness`, and `loudness` is having missing values in both train and test set,\n* we are having total 13 features,\n* Our Features are having Numeric value but few colums like `song_duration_ms` is having large range of values so normalization can be helpful,\n* Few features has negative values but it seems resonable since sound can have such feature so **its doesn't have any inappropriate value** more analysis can be done \n* Target `song_popularity` is having uneven distribution. It shows we are having `36.44%` target as 1 means popularity. So **data is imbalance**. Appropriate sampling methods like `upsampling` or `downsampling` can be used to get better result\n* **We are having missing values** in both train and test set we can't drop rows with missing values. Replacing missing values with appropriate column specific`mean` or `meadian` value is one option. We need to do it in both train and test set. \n\nNote: the respective `mean` or `median` values need to get from train set and apply on both train and test set since we in real life sinario we apply same preprocessing in test set like training set without analysing test set","113fa11c":"## Step1. Data Analysis:\nWe will try to understand data in this step","ab0fbefd":"## Step2. Preprocessing:\n\nWe preprocessing steps like,\n1. Handling Missing Values\n2. Normalizing Data\n\n### 1. Handling Missing Values:\n\nWe will replace missing values with meadian value for simplicity to make simple starter notebook","a88ccf33":"# Song Popularity Prediction V1\n\nThis will be a beginner friendly notebook. I will try insightful for someone who is just getting started with ML.\n\n| No | Method | Score |\n| --- | --- | --- |\n| 1 | Logistic Regression + MinMaxScaler + Null Value with Median | 0.50205 |\n| 2 | Random Forest + BalancedBaggingClassifier + MinMaxScaler + Null Value with Median | 0.56374 |\n| 3 | Random Forest + BalancedBaggingClassifier + Null Value with Median | 0.59715 |\n\n## Description About Data\n\nSource - Spotify: \"In Spotify's API is something called Valence, that describes the musical positiveness conveyed by a track. Tracks with high valence sound more positive (happy, cheerful, euphoric), while tracks with low valence sound more negative (sad, depressed, angry).\"\n\nFrom very good article explaining Spotify API What Makes a Song Likeable?- https:\/\/towardsdatascience.com\/what-makes-a-song-likeable-dbfdb7abe404 we can read that:\n\nSpotify Audio Features\nFor every track on their platform, Spotify provides data for thirteen Audio Features.The Spotify Web API developer guide defines them as follows:\n\nDanceability: Describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity.\n\nValence: Describes the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n\nEnergy: Represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale.\n\nTempo: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece, and derives directly from the average beat duration.\n\nLoudness: The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks.\n\nSpeechiness: This detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value.\nInstrumentalness: Predicts whether a track contains no vocals. \u201cOoh\u201d and \u201caah\u201d sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly \u201cvocal\u201d.\n\nLiveness: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live.\n\nAcousticness: A confidence measure from 0.0 to 1.0 of whether the track is acoustic.\n\nKey: The estimated overall key of the track. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C\u266f\/D\u266d, 2 = D, and so on.\n\nMode: Indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\n\nDuration: The duration of the track in milliseconds.\n\nTime Signature: An estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure).","416ed79b":"### 2. Normalization:\n\nFor normalization we will use `sklearn` minmax scaler"}}