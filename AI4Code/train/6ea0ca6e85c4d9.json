{"cell_type":{"085a8629":"code","925f8804":"code","2b11c361":"code","ed7e1dbf":"code","d79cf72c":"code","ec49a86b":"code","9b045988":"code","bc6f08d4":"code","15b25655":"code","09ed3934":"code","37e83329":"code","cc79e498":"code","fd0d9c32":"code","ccac9658":"code","94200e98":"code","88e701f4":"code","47cc6eba":"code","693619d2":"code","e33286a6":"code","65e09506":"code","decbb608":"code","0d0f23bc":"code","d607b581":"markdown","d1764f0a":"markdown","a6c98388":"markdown","0d157dae":"markdown","4378b525":"markdown","4f66d493":"markdown","308ae5da":"markdown","dbadf6eb":"markdown","fe77a01e":"markdown","b9fb2932":"markdown","a8a5c32b":"markdown","fbf54c8e":"markdown","7eccd554":"markdown","81bd90a3":"markdown","7e66b1eb":"markdown"},"source":{"085a8629":"!pip install datasets --no-index --find-links=file:\/\/\/kaggle\/input\/coleridge-packages\/packages\/datasets\n!pip install ..\/input\/coleridge-packages\/seqeval-1.2.2-py3-none-any.whl\n!pip install ..\/input\/coleridge-packages\/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n!pip install ..\/input\/coleridge-packages\/transformers-4.5.0.dev0-py3-none-any.whl","925f8804":"import os\nimport re\nimport json\nimport time\nimport datetime\nimport random\nimport glob\nimport importlib\nfrom functools import partial\nimport string\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, DataCollatorForLanguageModeling, \\\nAutoModelForMaskedLM, Trainer, TrainingArguments, pipeline, AutoModelForSequenceClassification\n\nsns.set()\nrandom.seed(123)\nnp.random.seed(456)\ntorch.manual_seed(2021)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nfrom nltk.corpus import stopwords\nfrom unidecode import unidecode\n\nSTOPWORDS = set(stopwords.words('english'))\nREMOVE_MATCH = True\nMASKEDLM = False","2b11c361":"train_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train.csv'\ntrain_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\ntrain = pd.read_csv(train_path)","ed7e1dbf":"sample_submission_path = '..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv'\nsample_submission = pd.read_csv(sample_submission_path)\n\npaper_test_folder = '..\/input\/coleridgeinitiative-show-us-the-data\/test'\n# paper_test_folder = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\npapers = {}\nfor paper_id in sample_submission['Id']:\n    with open(f'{paper_test_folder}\/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper","d79cf72c":"existing_labels = set(np.load('..\/input\/showdata-labels1\/existing_labels.npy', allow_pickle = True).tolist())\ntrain = pd.read_csv('..\/input\/showdata-labels1\/cleaned_train.csv')\n    \nprint(f'No. different labels: {len(existing_labels)}')\n\ntrain.head()","ec49a86b":"existing_labels","9b045988":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n\ndef totally_clean_text(txt):\n    txt = clean_text(txt)\n    txt = re.sub(' +', ' ', txt)\n    return txt\n\n## Functions for literal matching\n\ndef json_to_text(filename, train_files_path=train_files_path, output='text'):\n    json_path = os.path.join(train_files_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data\n    \ndef text_cleaning(text):\n    text = ''.join([k for k in text if k not in string.punctuation])\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n    text = re.sub(' +', ' ', text)\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    return text\n\ntqdm.pandas()\nsample_submission['text'] = sample_submission['Id'].progress_apply(partial(json_to_text, train_files_path=paper_test_folder))","bc6f08d4":"literal_preds = []\n\ndef predict(sample_sub):\n    id_list = []\n    labels_list = []\n    for index, row in tqdm(sample_sub.iterrows()):\n        sample_text = row['text']\n        row_id = row['Id']\n        temp_df = train[train['text'] == text_cleaning(sample_text)]\n        cleaned_labels = temp_df['cleaned_label'].to_list()\n        for known_label in existing_labels:\n            if known_label in sample_text.lower():\n                if known_label not in STOPWORDS:\n                    if len(known_label)>1:\n                        cleaned_labels.append(clean_text(known_label))\n        cleaned_labels = [clean_text(x) for x in cleaned_labels]\n        cleaned_labels = set(cleaned_labels)\n        labels_list.append('|'.join(cleaned_labels))\n        id_list.append(row_id)\n    return (id_list,labels_list)\n\nZ=predict(sample_submission)\nliteral_preds = Z[1]","15b25655":"#PRETRAINED_PATH = '..\/input\/coleridge-bert-masked-dataset-modeling-edit\/mlm-model'\nPRETRAINED_PATH = '..\/input\/coleridgebertsequenceclassification\/seqClass-model'\nTOKENIZER_PATH = '..\/input\/coleridgebertsequenceclassification\/model_tokenizer'\n\nMAX_LENGTH = 64\nOVERLAP = 20\n\nPREDICT_BATCH = 32 # a higher value requires higher GPU memory usage\n\nDATASET_SYMBOL = '$' # this symbol represents a dataset name\nNONDATA_SYMBOL = '#' # this symbol represents a non-dataset name","09ed3934":"# During training, the suspect phrases (i.e. uppercase\/ connection words) are replaced with either \n# $ denoting dataset or # denoting not dataset, and the phrases are randomly masked - the model is\n# then tasked to predict the masked words. In the inference phase, the suspect phrases (this time # \n# or $ is unknown) are masked and the model infers either # or $. \n\n# The fill-mask pipeline has an argument target_tokens when called which indicates what the valid \n# tokens for prediction are.\n\ntokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=True)\n\n\nif MASKEDLM:\n    model = AutoModelForMaskedLM.from_pretrained(PRETRAINED_PATH)\n    mlm = pipeline(\n        'fill-mask', \n        model=model,\n        tokenizer=tokenizer,\n        device=0 if torch.cuda.is_available() else -1\n        )\nelse:\n    model = AutoModelForSequenceClassification.from_pretrained(PRETRAINED_PATH)\n    snt = pipeline(\n        'sentiment-analysis', \n        model=model,\n        tokenizer=tokenizer,\n        device=0 if torch.cuda.is_available() else -1\n        )","37e83329":"def jaccard_similarity(s1, s2):\n    l1 = s1.split(\" \")\n    l2 = s2.split(\" \")    \n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) \/ union\n\ndef clean_paper_sentence(s):\n    \"\"\"\n    This function is essentially clean_text without lowercasing.\n    \"\"\"\n    s = re.sub('[^A-Za-z0-9]+', ' ', str(s)).strip()\n    s = re.sub(' +', ' ', s)\n    return s\n\ndef shorten_sentences(sentences):\n    \"\"\"\n    Sentences that have more than MAX_LENGTH words will be split\n    into multiple sentences with overlappings.\n    \"\"\"\n    short_sentences = []\n    for sentence in sentences:\n        words = sentence.split()\n        if len(words) > MAX_LENGTH:\n            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n        else:\n            short_sentences.append(sentence)\n    return short_sentences\n\n# connection_tokens = {'s', 'of', 'and', 'in', 'on', 'for', 'data', 'dataset'}\ndef find_mask_candidates_old(sentence):\n    \"\"\"\n    Extract masking candidates for Masked Dataset Modeling from a given $sentence.\n    A candidate should be a continuous sequence of at least 2 words, \n    each of these words either has the first letter in uppercase or is one of\n    the connection words ($connection_tokens). Furthermore, the connection \n    tokens are not allowed to appear at the beginning and the end of the\n    sequence.\n    \"\"\"\n    def candidate_qualified(words):\n        while len(words) and words[0].lower() in connection_tokens:\n            words = words[1:]\n        while len(words) and words[-1].lower() in connection_tokens:\n            words = words[:-1]\n        \n        return len(words) >= 2\n    \n    candidates = []\n    \n    phrase_start, phrase_end = -1, -1\n    for id in range(1, len(sentence)):\n        word = sentence[id]\n        if word[0].isupper() or word in connection_tokens:\n            if phrase_start == -1:\n                phrase_start = phrase_end = id\n            else:\n                phrase_end = id\n        else:\n            if phrase_start != -1:\n                if candidate_qualified(sentence[phrase_start:phrase_end+1]):\n                    candidates.append((phrase_start, phrase_end))\n                phrase_start = phrase_end = -1\n    \n    if phrase_start != -1:\n        if candidate_qualified(sentence[phrase_start:phrase_end+1]):\n            candidates.append((phrase_start, phrase_end))\n    \n    return candidates\n\nconnection_tokens = {'s', 'of', 'and', 'in', 'on', 'for', 'data', 'dataset', 'survey', 'study','sequence'}\nprep_tokens = {'s', 'of', 'and', 'in', 'on', 'for', 'this', 'we', 'their', 'it', 'to'}\ndef find_mask_candidates(sentence):\n    \"\"\"\n    Extract negative samples for Masked Dataset Modeling from a given $sentence.\n    A negative candidate should be a continuous sequence of at least 2 words, \n    each of these words either has the first letter in uppercase or is one of\n    the connection words ($connection_tokens). Furthermore, the connection \n    tokens are not allowed to appear at the beginning and the end of the\n    sequence. Lastly, the sequence must be quite different to any of the \n    ground truth labels (measured by Jaccard similarity).\n    \"\"\"\n    def candidate_qualified(words):\n        # remove beginning words that are connection_tokens except data\/dataset\n        startIdx = 0\n        endIdx = 0\n        while len(words) and words[0].lower() in prep_tokens:\n            words = words[1:]\n            startIdx +=1\n        # remove ending words that are connection_tokens\n        while len(words) and words[-1].lower() in prep_tokens:\n            words = words[:-1]\n            endIdx+=1\n        # comparison without connection_tokens\n        if (len(words) <= 3 or \\\n            sum([1 for word in words if not word.isnumeric()])<=2):\n            return False, []\n        elif len(words)==4 and words[-1].isnumeric() and words[1] == 'and':\n            # to get rid of references, e.g. Johnson and Johnson 2018\n            return False, []\n        else:\n            return True, [startIdx, endIdx]\n    \n    candidates = []\n    \n    phrase_start, phrase_end = -1, -1\n    for id in range(1, len(sentence)):\n        word = sentence[id]\n        # if word is captial or connection token\n        # if word[0].isupper() or word in connection_tokens:\n        if word[0].isupper() or (word[0].isnumeric() and len(word)>2):\n            # set as phrase start if phrase start doesn't exist, if not set as end\n            if phrase_start == -1:\n                phrase_start = phrase_end = id\n            else:\n                phrase_end = id\n        elif word not in connection_tokens:\n            # if current phrase fulfils dissimilarity requirement, reset phrase_start\n            if phrase_start != -1:\n                qualified, tmpidxs = candidate_qualified(sentence[phrase_start:phrase_end+1])\n                if qualified:\n                    candidates.append((phrase_start+tmpidxs[0], phrase_end-tmpidxs[1]))\n                phrase_start = phrase_end = -1\n    \n    # to deal with case where phrase end is last word\n    if phrase_start != -1:\n        qualified, tmpidxs = candidate_qualified(sentence[phrase_start:phrase_end+1])\n        if qualified:\n            candidates.append((phrase_start+tmpidxs[0], phrase_end-tmpidxs[1]))\n    \n    return candidates\n\ndef pre_tokenize(sentence):\n    try:\n        sentence = sentence.split()\n    except:\n        pass\n    wordlist = ['university', 'initiative','international','information']\n    for i in range(len(sentence)):\n        word  = sentence[i]\n        if word.isupper():\n            sentence[i] = '#'\n        elif word[0].isupper() and len(word)>8 and word.lower() not in wordlist:\n            sentence[i] = '$'\n    return sentence","cc79e498":"if MASKEDLM:\n    mask = mlm.tokenizer.mask_token","fd0d9c32":"all_test_data = []\ndN = 5\nfor paper_id in sample_submission['Id']:\n    # load paper\n    paper = papers[paper_id]\n    \n    # extract sentences\n    sentences = set([clean_paper_sentence(sentence) for section in paper \n                     for sentence in section['text'].split('.')\n                    ])\n    sentences = shorten_sentences(sentences) # make sentences short\n    sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n    sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\n    sentences = [sentence.split() for sentence in sentences] # sentence = list of words\n    \n    # mask\n    test_data = []\n    for sentence in sentences:\n        for phrase_start, phrase_end in find_mask_candidates(sentence):\n            if MASKEDLM:\n                dt_point = sentence[:phrase_start] + [mask] + sentence[phrase_end+1:]\n                test_data.append((' '.join(dt_point), ' '.join(sentence[phrase_start:phrase_end+1]))) # (masked text, phrase)\n            else:\n                dt_point = pre_tokenize(sentence[max(phrase_start-dN,0):phrase_end+1+dN])\n                test_data.append((' '.join(dt_point), sentence[phrase_start:phrase_end+1]))\n    \n    all_test_data.append(test_data)","ccac9658":"pred_labels = []\n\npbar = tqdm(total = len(all_test_data))\n# each iteration is one ID, test_data contains a list of tuples, 1st member of tuple\n# is the masked sentence, second member is the phrase\nfor i,test_data in enumerate(all_test_data):\n    pred_bag = set()\n    \n    if len(test_data):\n        texts, phrases = list(zip(*test_data))\n        preds = []\n        # iterate through sentences for that ID\n        for p_id in range(0, len(texts), PREDICT_BATCH):\n            batch_texts = texts[p_id:p_id+PREDICT_BATCH]\n            # since there are two target tokens, batch_pred is a list of two dicts, each\n            # containing the token identity and the corresponding prob score\n            if MASKEDLM:\n                batch_pred = mlm(list(batch_texts), targets=[f' {DATASET_SYMBOL}', f' {NONDATA_SYMBOL}'])\n            else:\n                batch_pred = snt(list(batch_texts))\n            \n            if len(batch_texts) == 1 and MASKEDLM:\n                batch_pred = [batch_pred]\n            \n            preds.extend(batch_pred)\n        \n        if MASKEDLM:\n            # append phrase to pred_bag if probability score corresponding to DATASET_SYMBOL is\n            # sufficiently high - pred_bag contains all predictions for a particular ID\n            for (result1, result2), phrase in zip(preds, phrases):\n                # print(result1)\n                if (result1['score'] > result2['score']*2 and result1['token_str'] == DATASET_SYMBOL) or\\\n                   (result2['score'] > result1['score']*2 and result2['token_str'] == DATASET_SYMBOL):\n                    pred_bag.add(clean_text(phrase))\n        else:\n            for lbl, phrase in zip(preds,phrases):\n                if lbl['label']=='LABEL_1':\n                    pred_bag.add(clean_text(phrase))\n    # filter labels by jaccard score \n    filtered_labels = []\n    # starting with longest labels, label is accepted only if there are no previously\n    # accepted labels or if label is sufficiently dissimilar to other accepted labels\n    for label in sorted(pred_bag, key=len, reverse=True):\n        if len(filtered_labels) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered_labels):\n            if any(jaccard_similarity(label, got_label) > 0.5 for got_label in existing_labels) and REMOVE_MATCH:\n                continue\n            filtered_labels.append(label)\n            \n    pred_labels.append('|'.join(filtered_labels))\n    pbar.update(1)","94200e98":"batch_pred","88e701f4":"preds","47cc6eba":"if MASKEDLM:\n    mlm_pred\nelse:\n    for lbl, phrase in zip(preds,phrases):\n        print('Label: '+ str(lbl['label'])+ '-> '+str(lbl['score']) +'; string:'+ clean_text(phrase))","693619d2":"# pred_labels[:5]","e33286a6":"final_predictions = []\nfor literal_match, mlm_pred in zip(literal_preds, pred_labels):\n    if literal_match and not REMOVE_MATCH:\n        final_predictions.append(literal_match)\n    else:\n        final_predictions.append(mlm_pred)","65e09506":"sample_submission['PredictionString'] = final_predictions\nsample_submission = sample_submission[['Id','PredictionString']]\nsample_submission.to_csv('submission.csv', index=False)","decbb608":"sample_submission.head()","0d0f23bc":"preds","d607b581":"### Predict","d1764f0a":"# Install packages","a6c98388":"# Load data","0d157dae":"### Auxiliary functions","4378b525":"### Transform","4f66d493":"### Create a knowledge bank","308ae5da":"# Literal Matching","dbadf6eb":"# Masked Dataset Modeling","fe77a01e":"# Import","b9fb2932":"This notebook is a template on using Literal Matching + Masked Language Modeling to identify datasets in papers.\n\nThe training of the Bert model was done in another notebook: [\\[Coleridge\\] BERT - Masked Dataset Modeling.](https:\/\/www.kaggle.com\/tungmphung\/coleridge-bert-masked-dataset-modeling)\n\nThe approach is:\n- Locate all the sequences of capitalized words (these sequences may contain some stopwords),\n- Replace each sequence with one of 2 special symbols (e.g. $ and #), implying if that sequence represents a dataset name or not.\n- Have the model learn the MLM task.","a8a5c32b":"### Matching on test data","fbf54c8e":"## Aggregate final predictions and write submission file","7eccd554":"### Paths and Hyperparameters","81bd90a3":"# Transform data to MLM format","7e66b1eb":"### Load model and tokenizer"}}