{"cell_type":{"babdafed":"code","62798a28":"code","286d7bfb":"code","45cc17aa":"code","ece147c6":"code","75e54085":"code","a1865d8e":"code","a68cd491":"code","9ab32951":"code","8ce87639":"code","4309e3db":"code","428f1bdd":"code","22c467d2":"code","ad342cfa":"code","14b1787c":"code","6a55d15c":"code","45458012":"code","bacc4d73":"code","26c5ad23":"code","df8ef0e6":"code","5129edde":"code","835305a7":"code","29cb7020":"code","3ad8ab2f":"code","593adc54":"code","fbf1bb21":"code","c11b2592":"code","5f26a214":"code","868ad2f9":"code","493f6ddc":"code","a0b5cadd":"code","ade8c4bf":"code","f994b5da":"code","e856bc2f":"code","2ef361af":"code","4b7daf84":"code","c1d9d197":"code","9b66b0eb":"code","944d6bfa":"code","d9c0a263":"code","5af5e1c6":"code","bf0e150d":"code","d6c94643":"code","d0d07d0b":"code","1fc5fdf1":"code","74b912f3":"code","63be59d7":"code","7cd3c256":"code","cea87585":"code","f8f587c1":"code","31c115c1":"code","8ae1c4ae":"code","0c0b4d82":"code","ef4f7cb3":"code","973038ac":"code","fe4bc2dc":"code","de20bf7f":"code","b6a299af":"code","7b4a8c01":"code","1a56c71a":"code","6ff80068":"markdown","91da9e2e":"markdown","52a400cb":"markdown","448b0b2d":"markdown","fe8dbad3":"markdown","fb249a5e":"markdown","659b07e7":"markdown","ad98c2cf":"markdown","bf44a0c5":"markdown","dbc0a1a4":"markdown","52a825b7":"markdown","31360903":"markdown","927550ba":"markdown","d464bbb7":"markdown","7fea2345":"markdown"},"source":{"babdafed":"import numpy as np\nimport pandas as pd\nimport re\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom wordcloud import WordCloud\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\n\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.experimental import enable_halving_search_cv\nfrom sklearn.model_selection import HalvingGridSearchCV, RepeatedStratifiedKFold\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Dense, Flatten, LSTM, Bidirectional\nfrom tensorflow.keras.utils import to_categorical, plot_model\nfrom tensorflow.keras import callbacks\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n\n\nimport joblib\nimport warnings\n\n\nwarnings.filterwarnings('ignore')\nsns.set_theme()","62798a28":"Path = '\/content\/drive\/MyDrive\/Projects\/Twitter US Airline Sentiment\/Data\/Tweets.csv'\nraw_data = pd.read_csv(Path)\nraw_data.head(5)","286d7bfb":"raw_data.columns","45cc17aa":"data = raw_data[['text', 'airline_sentiment']]\ndata.head(5)","ece147c6":"data.rename({'text': 'tweet', 'airline_sentiment':'sentiment'}, axis=1, inplace=True)\ndata.head(5)","75e54085":"data['sentiment'].unique()","a1865d8e":"data['sentiment'].value_counts()","a68cd491":"data.isna().sum()","9ab32951":"# clean text\n\ndef preprocesing_text(text):  \n    text = text.lower()    \n    text = re.sub('https?:\/\/[A-Za-z0-9.\/]+', ' ', text)   #remove url links\n    text = re.sub(\"www.[A-Za-z0-9.\/]+\", ' ', text)        #remove url links\n    text = re.sub('@[^\\s]+', ' ', text)     #remove user name\n    text = re.sub('\\d+', ' ', text)         #remove digits\n    text = re.sub(r'[^\\w\\s]+', ' ', text)   #remove punctuations\n    text = re.sub('_+', ' ', text)          #remove _ char\n    text = re.sub('\\n', ' ', text)          #convert to one line only\n    text = re.sub(r'\\b\\w{1,2}\\b', '', text) #remove words < 2\n    text = re.sub(' +', ' ', text)          #convert two or more spaces into one space\n    return text\n","8ce87639":"data['text']=data['tweet'].apply(preprocesing_text)\ndata.head()","4309e3db":"# save preprocessed data to new csv\n# data = data[['text', 'sentiment']]\n# data.to_csv('tweets_processed.csv', index=False)","428f1bdd":"labels = ['negative','neutral','positive']\nvalues = data['sentiment'].value_counts()\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])\nfig.update_layout(height=500, width=500, paper_bgcolor=\"grey\")\nfig.show()","22c467d2":"fig = px.histogram(data, x=data['text'].str.len(), title='Tweet length vs Tweets count ')\nfig.update_layout(height=500, width=800, paper_bgcolor=\"grey\")\nfig.show()","ad342cfa":"# Word cloud \ndef wordCloud(sentiment):\n    df = data[data['sentiment'] == sentiment]\n    text = \" \".join(review for review in df.text)\n    wordcloud = WordCloud(background_color='black',height = 600, width = 800).generate(text)\n    plt.figure(figsize=(10,10))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.show()","14b1787c":"# most positive words\nwordCloud('positive')","6a55d15c":"# most neutral words\nwordCloud('neutral')","45458012":"# most negative words\nwordCloud('negative')","bacc4d73":"Path = '\/content\/drive\/MyDrive\/Projects\/Twitter US Airline Sentiment\/Data\/tweets_processed.csv'\ndf = pd.read_csv(Path)\ndf.head(5)","26c5ad23":"df.shape","df8ef0e6":"X = df.drop('sentiment', inplace=False, axis=1)\ny = df['sentiment']","5129edde":"# split the data into train, validation, and test set\n\ndef train_val_test_split(X, y, train_size, val_size, test_size):\n    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size = test_size, random_state=42)\n\n    relative_train_size = train_size \/ (val_size + train_size)\n\n    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val,\n                                                      train_size = relative_train_size, test_size = 1-relative_train_size,\n                                                      random_state=42)\n    return X_train, X_val, X_test, y_train, y_val, y_test\n","835305a7":"X_train, X_val, X_test, y_train, y_val, y_test=train_val_test_split(X, y, .6, .2, .2)","29cb7020":"print('training features size: '+str(X_train.shape)+\", validation features size: \"+ str(X_val.shape)+\", testing features size: \"+str(X_test.shape))\nprint('training labels size: '+str(y_train.shape)+\", validation labels size: \"+str(y_val.shape)+ \", testing labels size: \"+str(y_test.shape))","3ad8ab2f":"# Text to Vector using tf-idf\n\nvec = TfidfVectorizer(ngram_range=(1, 2), max_df=.8, min_df=1, max_features=1000)","593adc54":"X_train_vec = vec.fit_transform(X_train['text'].apply(lambda x: np.str_(x))).toarray()\nX_val_vec = vec.transform(X_val['text'].apply(lambda x: np.str_(x))).toarray()\nX_test_vec = vec.transform(X_test['text'].apply(lambda x: np.str_(x))).toarray()","fbf1bb21":"# Encode labels\n\nenc = {'positive': 1, 'negative': 0, 'neutral': 2}\n\ny_train_vec = y_train.map(enc).to_numpy()\ny_val_vec = y_val.map(enc).to_numpy()\ny_test_vec = y_test.map(enc).to_numpy()","c11b2592":"print('X_train size:', X_train_vec.shape)\nprint('y_train size:', y_train_vec.shape)\nprint('X_val size:', X_val_vec.shape)\nprint('y_val size:', y_val_vec.shape)\nprint('X_test size:', X_test_vec.shape)\nprint('y_test size:', y_test_vec.shape)","5f26a214":"model_lr = LogisticRegression()\nsolvers = ['newton-cg', 'lbfgs','liblinear']  \npenalty = ['l2']\nmax_iter=[200]\nc_values = [0.001, 0.01, 0.1, 1, 10, 100]\n\ngrid_lr = dict(solver=solvers, penalty=penalty, C=c_values, max_iter=max_iter)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)","868ad2f9":"HGS_lr = HalvingGridSearchCV(estimator=model_lr, param_grid=grid_lr, n_jobs=-1, cv=cv, scoring='accuracy')\nresult_lr = HGS_lr.fit(X_train_vec, y_train_vec)","493f6ddc":"print(result_lr.best_score_)\nprint(result_lr.best_params_)","a0b5cadd":"model_lr = result_lr.best_estimator_","ade8c4bf":"# save model\njoblib.dump(model_lr, \"LR_MODEL.pkl\")","f994b5da":"model_nb = MultinomialNB()\nalpha = [ 0.001, 0.01, 0.1, 1, 1.5, 2.0] \n\ngrid_nb = dict(alpha=alpha)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)","e856bc2f":"HGS_nb = HalvingGridSearchCV(estimator=model_nb, param_grid=grid_nb, n_jobs=-1, cv=cv, scoring='accuracy')\nresult_nb = HGS_nb.fit(X_train_vec, y_train_vec)","2ef361af":"print(result_nb.best_score_)\nprint(result_nb.best_params_)","4b7daf84":"model_nb = result_nb.best_estimator_","c1d9d197":"# save model\njoblib.dump(model_nb, \"NB_MODEL.pkl\")","9b66b0eb":"# copy data\nX_ntrain, X_nval, X_ntest, y_ntrain, y_nval, y_ntest = X_train, X_val, X_test, y_train, y_val, y_test\n\n# squeeze data to be 1-dim\nX_ntrain = np.squeeze(X_ntrain)\nX_nval = np.squeeze(X_nval)\nX_ntest = np.squeeze(X_ntest)","944d6bfa":"# Tokenize our training data\n\nt = Tokenizer(num_words=1000, oov_token='<UNK>')\nt.fit_on_texts(X_ntrain)","d9c0a263":"# Get our training data word index\n\nword_index = t.word_index\n# print(\"Word index:\\n\", word_index)","5af5e1c6":"# Vocab size\n\nvocab_size = len(t.word_index) + 1\nprint(vocab_size)","bf0e150d":"# Encode data sentences into sequences\n\nX_ntrain = t.texts_to_sequences(X_ntrain)\nX_nval = t.texts_to_sequences(X_nval)\nX_ntest = t.texts_to_sequences(X_ntest)","d6c94643":"# Get max training sequence length\n\nmaxlen = max([len(x) for x in X_ntrain])\nprint(maxlen)","d0d07d0b":"# Pad the sequences\nX_ntrain = pad_sequences(X_ntrain, padding=\"post\", maxlen=50)\nX_nval = pad_sequences(X_nval, padding=\"post\", maxlen=50)\nX_ntest = pad_sequences(X_ntest, padding=\"post\", maxlen=50)","1fc5fdf1":"# Encode labels\nlb =LabelEncoder()\n# negative:0, neutral:1, positive:2\n\ny_ntrain = lb.fit_transform(y_ntrain)\ny_nval = lb.transform(y_nval)\ny_ntest = lb.transform(y_ntest)\n","74b912f3":"# One hot encoding\n\ny_ntrain = to_categorical(y_ntrain)\ny_nval = to_categorical(y_nval)\ny_ntest = to_categorical(y_ntest)","63be59d7":"print('X_train size:', X_ntrain.shape)\nprint('y_train size:', y_ntrain.shape)\nprint('X_val size:', X_nval.shape)\nprint('y_val size:', y_nval.shape)\nprint('X_test size:', X_ntest.shape)\nprint('y_test size:', y_ntest.shape)","7cd3c256":"# Embeddings using glove\n\npath = '\/content\/drive\/MyDrive\/Projects\/Twitter US Airline Sentiment\/Glove\/glove.6B.100d.txt'\n\nembeddings_index = {}\nf = open(path, encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","cea87585":"# fill in matrix of embedding for each word in the training dataset.\nembedding_matrix = np.zeros((vocab_size, 100))\nfor word, i in t.word_index.items():  # dictionary\n    embedding_vector = embeddings_index.get(word) # gets embedded vector of word from GloVe\n    if embedding_vector is not None:\n        # add to matrixb\n        embedding_matrix[i] = embedding_vector # each row of matrix","f8f587c1":"# Create model\n\nmodel_lst = Sequential()\nmodel_lst.add(Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix],\n                           input_length = 50, trainable=False))\n\nmodel_lst.add(Bidirectional(LSTM(128)))\nmodel_lst.add(Dense(10, activation='relu'))\nmodel_lst.add(Dense(3, activation='softmax'))\n\nmodel_lst.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n\nmodel_lst.summary()\nplot_model(model_lst, show_shapes=True, show_layer_names=True)","31c115c1":"callback = callbacks.EarlyStopping(monitor='val_acc', patience=5)\n\nhistory = model_lst.fit(X_ntrain, y_ntrain, epochs=20, batch_size=256,\n                       validation_data=(X_nval, y_nval), callbacks=[callback])","8ae1c4ae":"# Draw Training and validation accuracy\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","0c0b4d82":"# save model\njoblib.dump(model_lst, \"LST_MODEL.pkl\")","ef4f7cb3":"# validate 3 models logistic regression, naive bayes and lstm neural network\n# load 3 models \nlr = joblib.load('\/content\/drive\/MyDrive\/Projects\/Twitter US Airline Sentiment\/Models\/LR_MODEL.pkl')\nnb = joblib.load('\/content\/drive\/MyDrive\/Projects\/Twitter US Airline Sentiment\/Models\/NB_MODEL.pkl')\nlstm = joblib.load('\/content\/drive\/MyDrive\/Projects\/Twitter US Airline Sentiment\/Models\/LSTM_MODEL.pkl')","973038ac":"# Fuunction to validate machine learning models\ndef ml_evaluate(model, X_features, y_labels):\n  y_pred = model.predict(X_features)\n\n  accuracy = round(accuracy_score(y_labels, y_pred), 4)\n  precision = round(precision_score(y_labels, y_pred, average='micro'), 4)\n  recall = round(recall_score(y_labels, y_pred, average='micro'), 4)\n  f1 = round(f1_score(y_labels, y_pred, average='micro'), 4)\n\n  return ('accuracy: {} \/ Precision: {} \/ recall: {} \/ f1_score: {}'.format(accuracy, precision, recall, f1))","fe4bc2dc":"# Fuunction to validate deep learning models\ndef dl_evaluate(model, X_features, y_labels):\n  y_pred = model.predict(X_features)\n  y_pred = np.argmax(y_pred, axis=1)\n  y_labels = np.argmax(y_labels, axis=1)\n  \n  accuracy = round(accuracy_score(y_labels, y_pred), 4)\n  precision = round(precision_score(y_labels, y_pred, average='micro'), 4)\n  recall = round(recall_score(y_labels, y_pred, average='micro'), 4)\n  f1 = round(f1_score(y_labels, y_pred, average='micro'), 4)\n\n  return ('accuracy: {} \/ Precision: {} \/ recall: {} \/ f1_score: {}'.format(accuracy, precision, recall, f1))","de20bf7f":"print(\"Logistic regression: \" + ml_evaluate(lr, X_val_vec, y_val_vec))\nprint('--------------------------------------------------------------------')\nprint(\"Naive bayes: \" + ml_evaluate(nb, X_val_vec, y_val_vec))\nprint('--------------------------------------------------------------------')\nprint(\"Neural network: \" + dl_evaluate(lstm, X_nval, y_nval))","b6a299af":"# Classification Report\ny_pred = lstm.predict(X_ntest)\n\ny_pred = np.argmax(y_pred, axis=1)\ny_ntest = np.argmax(y_ntest, axis=1)\n\nprint(classification_report(y_ntest, y_pred, target_names = ['negative','neutral','positive']))","7b4a8c01":"#Get the confusion matrix\ncf_matrix = confusion_matrix(y_ntest, y_pred)\n\nprint(cf_matrix)","1a56c71a":"# Draw the confusion matrix\nax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n\nax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\nax.set_xlabel('\\nPredicted sentiment Category')\nax.set_ylabel('Actual sentiment Category ');\n\n## Ticket labels - List must be in alphabetical order\nax.xaxis.set_ticklabels(['negative','neutral', 'positive'])\nax.yaxis.set_ticklabels(['negative','neutral', 'positive'])\n\n## Display the visualization of the Confusion Matrix.\nplt.show()","6ff80068":"# Overall validation","91da9e2e":"# Data splitting","52a400cb":"# Data anaylsis","448b0b2d":"# Test using the best model","fe8dbad3":"# Train using deep learning models","fb249a5e":"# Data preprocessing","659b07e7":"## 2. Naive Bayes","ad98c2cf":"[![Open In Colab](https:\/\/colab.research.google.com\/assets\/colab-badge.svg)](https:\/\/colab.research.google.com\/github\/\/mohnabilfadl\/Airline_sentiment_analysis\/blob\/main\/Airline_sentiment_analysis.ipynb)\n","bf44a0c5":"# Train traditional machine learning models","dbc0a1a4":"**This notebook for implementing  sentiment anaylsis using different models from A to Z**\n\n**Hint: Open colab to see the result**","52a825b7":"# Data visualization","31360903":"# Import libraries\n","927550ba":"**So We noticed that neural network model is the best model**","d464bbb7":"# End\n# **By Mohamed Fadl**","7fea2345":"## 1. Logistic Regression\n\n"}}