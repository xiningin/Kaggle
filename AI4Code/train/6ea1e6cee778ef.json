{"cell_type":{"b80e44bd":"code","26cea911":"code","01664a2b":"code","5edc9991":"code","0ee66aba":"code","16994f50":"code","9ce338c6":"code","b3a5bc0a":"code","5e05524b":"code","0eb8ef9f":"code","68f89ca0":"code","32dd1e63":"code","04db110b":"code","a84b01c0":"code","aa751c5a":"code","4cfc7a52":"code","0b1cfa61":"code","04cbdad0":"code","93cd2e85":"code","2db830f6":"code","8aaa2bd9":"code","f0be2aae":"code","3869e352":"code","56f5cee8":"code","de0f8f8b":"code","5cba4f0a":"code","9473d751":"code","d943b721":"code","690f4cd8":"code","b23258fa":"code","e62eecb2":"code","4d910d76":"code","4487fc83":"code","894d4136":"code","1d6333ec":"code","d0d7cb44":"code","e007b064":"code","4333db03":"code","0ce7fee8":"code","d7f79540":"code","4005968d":"code","50d865a4":"code","b4be9a76":"code","d833237a":"code","1aec357d":"code","150f0a18":"code","495ba90f":"code","7a11000b":"code","55215363":"code","0268d4c9":"code","a56e9f6a":"code","3f35eea1":"code","92788c36":"code","15d8cfd8":"code","1a5c9349":"code","ce264c93":"code","638ee06b":"code","5daa638d":"code","e15d0b16":"code","c96f679a":"code","ae18a99f":"code","2f995965":"code","00ae8cc2":"code","35fd4c2c":"code","54add98b":"code","e9584596":"code","cbeca089":"code","69614e32":"code","10db6d03":"code","e907a3e0":"code","80ada719":"markdown","7a1c7809":"markdown","4df657f7":"markdown","df91eb36":"markdown","cec387bf":"markdown","74127ed4":"markdown","68031f35":"markdown","cafc3e37":"markdown","57cd088d":"markdown","79e44e4d":"markdown","11cdde99":"markdown","e5c8193d":"markdown","57d849d9":"markdown","c9b9c072":"markdown","145eecfd":"markdown","3d8c3e7a":"markdown","efe91721":"markdown","36b63066":"markdown","c1b66534":"markdown","90dafef1":"markdown","83689c09":"markdown","93df3773":"markdown","ec98d60b":"markdown","1b0dde3d":"markdown","1c5ce8aa":"markdown","9bd1647d":"markdown","bfbba60d":"markdown","db638f0f":"markdown","266f3aac":"markdown","f1c0407c":"markdown","e08d3bc7":"markdown","4767cd5d":"markdown","4ec4ce7e":"markdown","486ed7a4":"markdown","bb1da27d":"markdown","a5fc9e22":"markdown","fab71aa3":"markdown","72a0ea6f":"markdown","e52803aa":"markdown","f33d4eda":"markdown","932ee630":"markdown","0d03fd04":"markdown","a57676c6":"markdown","1036f76e":"markdown","3559117d":"markdown","437ed239":"markdown","645744c7":"markdown","9606a913":"markdown","a0728bb5":"markdown","2a3d66e5":"markdown","4dd41d57":"markdown","f53aa17f":"markdown","87308a5b":"markdown","ea356f27":"markdown","42aa281f":"markdown","a535aecd":"markdown","b469d29c":"markdown","ab8ad13f":"markdown","bab3eb49":"markdown","a2086a02":"markdown","d53d8826":"markdown","066525ab":"markdown","707f8ed4":"markdown","7d3dcc3c":"markdown","454b7a70":"markdown","53f455ca":"markdown","50fa6cfa":"markdown","1cf2fa14":"markdown","3a91d7c4":"markdown","345e25b4":"markdown","a00d448e":"markdown","f4290c3a":"markdown","9d3afeac":"markdown","2aec1884":"markdown","1bfdd227":"markdown","d29e149c":"markdown","7e401d3e":"markdown","5d5202a7":"markdown","156f43d8":"markdown","1548a9ea":"markdown","970f4264":"markdown"},"source":{"b80e44bd":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import train_test_split\npd.set_option('display.max_columns', None)\nsns.set_style('darkgrid')\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom pandas.core.common import SettingWithCopyWarning\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)","26cea911":"stores = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv')\nfeatures = pd.read_csv( '\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/features.csv.zip')\ntest = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/test.csv.zip')\ntrain = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/train.csv.zip')","01664a2b":"print(train.shape)\ntrain.head()","5edc9991":"print(test.shape)\ntest.head()","0ee66aba":"print(stores.shape)\nstores.head()","16994f50":"print(features.shape)\nfeatures.head()","9ce338c6":"# concatenating test and train datasets\ntrain['dataset'] = 'train'\ntest['dataset'] = 'test'\ntrain_test = pd.concat([train, test])\n\n# Merge all data\ntrain_test = train_test.merge(stores, how='left').merge(features, how='left')\n\n# Creating date-time objects and some extra date-time info\ntrain_test['Date'] = pd.to_datetime(train_test['Date'])\ntrain_test['Year'] = pd.to_datetime(train_test['Date']).dt.year\ntrain_test['Month'] = pd.to_datetime(train_test['Date']).dt.month\ntrain_test['Week'] = pd.to_datetime(train_test['Date']).dt.week\ntrain_test['DayOfTheMonth'] = pd.to_datetime(train_test['Date']).dt.day\n\ntrain_test.head()","b3a5bc0a":"# test and train data is seperated by dates\ndisplay(train.Date, test.Date)","5e05524b":"pd.DataFrame(train_test.dtypes).reset_index().rename(columns={'index':'Columns', 0:'Type'})","0eb8ef9f":"train_test.info()","68f89ca0":"train_test.isna().sum()","32dd1e63":"# all numeric columns\nnumeric = train_test.select_dtypes(include=['number']).copy()\n\n# discrete number columns for bar-graphs\ndisc_num_var = ['Year','Month','Week','DayOfTheMonth']\n\n# continious number columns for histograms\ncont_num_var = []\nfor i in numeric:\n    if i not in disc_num_var:\n        cont_num_var.append(i)\n        \nprint('Discrete:', disc_num_var)\nprint('Continious:', cont_num_var)","04db110b":"# all categorical columns\ncategoric = train_test.select_dtypes(exclude=['number']).drop(['Date', 'dataset'], axis=1).copy()\ncategoric.columns","a84b01c0":"fig = plt.figure(figsize=(14,10))\n\nfor index, col in enumerate(cont_num_var): \n    plt.subplot(4,4,index+1) \n    sns.distplot(numeric.loc[:,col].dropna(), kde=False) \n    plt.xlabel(None)\n    plt.title(col, fontsize=12)\nfig.tight_layout(pad=1.0) ","aa751c5a":"fig = plt.figure(figsize=(15,16))\n\nfor index, col in enumerate(cont_num_var):\n    plt.subplot(6,4,index+1)\n    sns.boxplot(y=col, data=numeric.dropna())\n    plt.ylabel(None)\n    plt.title(col, fontsize=12)\nfig.tight_layout(pad=1.0)","4cfc7a52":"fig = plt.figure(figsize=(15,8))\n\nfor index, col in enumerate(disc_num_var):\n    plt.subplot(2, 2, index+1)\n    sns.countplot(x=col, data=numeric.dropna()) \n    plt.ylabel(None)\n    plt.title(col)\nfig.tight_layout(pad=1.0)","0b1cfa61":"# to be able to make an countplot for this boolean variable we've to change the type of it\ncategoric['IsHoliday'] = categoric['IsHoliday'].apply(str)\n\nfig = plt.figure(figsize=(10,6))\nfor index, col in enumerate(categoric):\n    plt.subplot(2,1,index+1)\n    sns.countplot(x=categoric[col], data=categoric.dropna())\n    plt.xlabel(None)\n    plt.title(col, fontsize=14)\nfig.tight_layout(pad=1.0)","04cbdad0":"plt.figure(figsize=(10,7))\n# joining isholiday and type\nnumeric['Type'] = categoric['Type'].replace({'A': 3, 'B': 2, 'C': 1})\nnumeric['IsHoliday'] = categoric['IsHoliday'].replace({'False': 0, 'True': 1})\ncor = numeric.corr()\nsns.heatmap(cor, linewidths=0.2, cmap='Blues') \nplt.show()","93cd2e85":"train_test.corr()[['Weekly_Sales']].apply(abs).sort_values('Weekly_Sales', ascending=False).head(10)","2db830f6":"fig = plt.figure(figsize=(20,20))\nfor index, col in enumerate(numeric):\n    plt.subplot(5,4,index+1)\n    sns.scatterplot(x=col, y='Weekly_Sales', data=numeric.dropna())\n    plt.title(col, fontsize=14)\n    plt.xlabel(None)\n    plt.ylabel('Weekly_Sales')\nfig.tight_layout(pad=1.0)    \nplt.show()","8aaa2bd9":"# Creating a column with markdown_totals\ntrain_test['markdown_totals'] = train_test[['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']].sum(axis=1)\ntrain_test.groupby('IsHoliday')[['markdown_totals']].mean()","f0be2aae":"plt.figure(figsize=(15,5))\n\n# working on xticks labels with dates\nweeks = [str(i) for i in range(1,53)]\nweeks[5] = '6 - superbowl'\nweeks[12] = 'easter - 2010 \/ 13'\nweeks[13] = 'easter - 2012 \/ 14'\nweeks[15] = 'easter - 2011 \/ 16'\nweeks[21] = 'memorial day - 22'\nweeks[26] = 'independence day - 27'\nweeks[35] = '36 - laborday'\nweeks[46] = '47 - thanksgiving'\nweeks[51] = '52 - christmas'\n\n# plotting markdowns 1 to 6\nfor i in range(1,6):\n    markdown_df = train_test[train_test['MarkDown' + str(i)] > 0].groupby(train_test['Date'].dt.week)[['MarkDown' + str(i)]].sum()\n    plt.plot(markdown_df.index, markdown_df[['MarkDown' + str(i)]].values, label='MarkDown' + str(i))\n\nplt.xticks(np.arange(1, 53, step=1), labels=weeks, rotation=90)\nplt.legend(['Super Bowl','Christmas','Thanksgiving','Super Bowl','Unknown'], loc=2)\nplt.title('Weekly markdown summings (of 3 years) per markdown_type', fontsize=16)\nplt.ylabel('markdown_totals', fontsize=12)\nplt.xlabel('Weeks', fontsize=12)\nplt.show()","3869e352":"# plotting\nplt.figure(figsize=(15,6))\n# we drop store 28 since it's an outlier for 1 particular moment which influences the visibality of our plot.\nfor i in train_test.Store.value_counts().sort_index().index.drop(28):\n    mark_per_store = train_test[(train_test['Store'] == i) & (train_test['markdown_totals'] > 0)].groupby('Date')[['markdown_totals']].mean()\n    plt.plot(mark_per_store.index, mark_per_store.values, label='store' + str(i))\nprint(len(train_test.Store.unique()), ' different stores')\nplt.title('Markdown vs Stores')\nplt.ylabel('Markdown Totals')\nplt.xlabel('Date')\nplt.show()","56f5cee8":"plt.figure(figsize=(15,5))\n\n# working on xticks labels with dates\nweeks = [str(i) for i in range(1,53)]\nweeks[4] = 'superbowl - 2013 \/ 5'\nweeks[5] = 'superbowl - 2012 \/ 6'\nweeks[13] = 'easter - 2012 & 2013 \/ 14'\nweeks[21] = 'memorial day - 22'\nweeks[26] = 'independence day - 27'\nweeks[35] = 'laborday - 36'\nweeks[46] = 'thanksgiving - 47'\nweeks[50] = 'christmas - 2010 \/ 51'\nweeks[51] = 'christmas - 2011 \/ 52'\n\n# making datasets for next plot\nmark_totals_2011 = train_test[(train_test['Year'] == 2011) & train_test['Week'].isin(list(range(44,53)))].groupby(train_test['Week'])[['markdown_totals']].sum()\nmark_totals_2012 = train_test[train_test['Year'] == 2012].groupby(train_test['Week'])[['markdown_totals']].sum()\nmark_totals_2013 = train_test[train_test['Year'] == 2013].groupby(train_test['Week'])[['markdown_totals']].sum()\n\n# plotting \nplt.plot(mark_totals_2011.index, mark_totals_2011['markdown_totals'])\nplt.plot(mark_totals_2012.index, mark_totals_2012['markdown_totals'])\nplt.plot(mark_totals_2013.index, mark_totals_2013['markdown_totals'])\n\nplt.xticks(np.arange(1, 53, step=1), labels=weeks, rotation=90)\nplt.legend(['2011', '2012', '2013'], fontsize=16)\nplt.title('Sum of markdown totals - Per week & Year (department\/ stores combined)', fontsize=18)\nplt.ylabel('markdown totals', fontsize=16)\nplt.xlabel('Week', fontsize=16)\nplt.show()","de0f8f8b":"for i in train_test['Type'].unique():\n    local_data = train_test[train_test['Type'] == i]\n    plt.scatter(local_data['Size'], local_data['Weekly_Sales'], label='Type: %s' %i, alpha=0.5)\n    \nplt.title('Size vs. Sales vs. Type')\nplt.legend(loc=2)\nplt.show()","5cba4f0a":"plt.title('Type vs. average Sales')\nsns.barplot(y='Weekly_Sales', x='Type', data=train_test.dropna())\nplt.show()","9473d751":"train_test.groupby('IsHoliday')[['Weekly_Sales']].mean().rename(columns={'Weekly_Sales': 'Average Weekly Sales'})","d943b721":"# making datasets for next plot\nweekly_sales_2010 = train_test[train_test['Date'].dt.year == 2010].groupby(train_test['Date'].dt.week)[['Weekly_Sales']].mean()\nweekly_sales_2011 = train_test[train_test['Date'].dt.year == 2011].groupby(train_test['Date'].dt.week)[['Weekly_Sales']].mean()\nweekly_sales_2012 = train_test[train_test['Date'].dt.year == 2012].groupby(train_test['Date'].dt.week)[['Weekly_Sales']].mean()","690f4cd8":"plt.figure(figsize=(15,5))\n\n# working on xticks labels with dates\nweeks = [str(i) for i in range(1,53)]\nweeks[5] = 'superbowl - 6'\nweeks[12] = 'easter - 2010 \/ 13'\nweeks[13] = 'easter - 2012 & 2013 \/ 14'\nweeks[15] = 'easter - 2011 \/ 16'\nweeks[21] = 'memorial day - 22'\nweeks[26] = 'independence day - 27'\nweeks[35] = 'laborday - 36'\nweeks[46] = 'thanksgiving - 47'\nweeks[50] = 'christmas - 2010 \/ 51'\nweeks[51] = 'christmas - 2011 \/ 52'\n\n# plotting \nplt.plot(weekly_sales_2010.index, weekly_sales_2010['Weekly_Sales'])\nplt.plot(weekly_sales_2011.index, weekly_sales_2011['Weekly_Sales'])\nplt.plot(weekly_sales_2012.index, weekly_sales_2012['Weekly_Sales'])\n\nplt.xticks(np.arange(1, 53, step=1), labels=weeks, rotation=90)\nplt.legend(['2010', '2011', '2012'], fontsize=16)\nplt.title('Average Weekly Sales (department\/ stores combined) - Per Year', fontsize=18)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Week', fontsize=16)\nplt.show()","b23258fa":"plt.figure(figsize=(20,8))\nsns.barplot(x='Store', y='Weekly_Sales', data=train_test)\nplt.title('Average Sales - per Store', fontsize=18)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Store', fontsize=16)\nplt.show()","e62eecb2":"# plotting\nplt.figure(figsize=(15,6))\nfor i in train_test.Store.unique():\n    sale_per_store = train_test[(train_test['Store'] == i)].groupby('Date')[['Weekly_Sales']].mean()\n    plt.plot(sale_per_store.index, sale_per_store.values, label='store' + str(i))\n\nprint(len(train_test.Store.unique()), ' different stores')\nplt.title('Max Sales vs stores')\nplt.ylabel('max of Weekly sales')\nplt.xlabel('date')\nplt.show()","4d910d76":"plt.figure(figsize=(20,8))\nsns.barplot(x='Dept', y='Weekly_Sales', data=train_test)\nplt.title('Average Sales - per Dept', fontsize=18)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Dept', fontsize=16)\nplt.show()","4487fc83":"plt.figure(figsize=(15,6))\nfor i in train_test.Dept.unique():\n    sale_per_dept = train_test[(train_test['Dept'] == i)].groupby('Date')[['Weekly_Sales']].mean()\n    plt.plot(sale_per_dept.index, sale_per_dept.values, label='department: ' + str(i))\n\nprint(len(train_test.Dept.unique()), ' different departments')\nplt.title('Average Sales vs departments', fontsize=14)\nplt.ylabel('Average Weekly sales')\nplt.xlabel('Date')\nplt.show()","894d4136":"# creating datasets for next plot\nweekly_sales_mean = train_test['Weekly_Sales'].groupby(train_test['Date']).mean()\nweekly_sales_median = train_test['Weekly_Sales'].groupby(train_test['Date']).median()\n\n# plotting\nplt.figure(figsize=(15,5))\nplt.plot(weekly_sales_mean.index, weekly_sales_mean.values)\nplt.plot(weekly_sales_median.index, weekly_sales_median.values)\nplt.legend(['Mean', 'Median'], loc='best', fontsize=16)\nplt.title('Weekly Sales - Mean and Median', fontsize=18)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Date', fontsize=16)\nplt.show()","1d6333ec":"train_test[['Weekly_Sales']].describe()","d0d7cb44":"# Again concatenating test and train datasets\ntrain['dataset'] = 'train'\ntest['dataset'] = 'test'\ntrain_test = pd.concat([train, test])\n\n# Merge all data\ntrain_test = train_test.merge(stores, how='left').merge(features, how='left')\n\n# Creating date-time objects and some extra date-time info\ntrain_test['Date'] = pd.to_datetime(train_test['Date'])\ntrain_test['Year'] = pd.to_datetime(train_test['Date']).dt.year\ntrain_test['Month'] = pd.to_datetime(train_test['Date']).dt.month\ntrain_test['Week'] = pd.to_datetime(train_test['Date']).dt.week\ntrain_test['DayOfTheWeek'] = pd.to_datetime(train_test['Date']).dt.dayofweek\ntrain_test['DayOfTheMonth'] = pd.to_datetime(train_test['Date']).dt.day\n\n# labelencoding the type column by order\ntrain_test['Type_encoded'] = train_test['Type'].replace({'A': 3, 'B': 2,'C':1})\n\n# fahrenheit to celcius\ntrain_test['Temperature'] = round((train_test['Temperature'] - 32) * 5\/9,2)\n\n# Creating a column with markdown_totals\ntrain_test['markdown_totals'] = train_test[['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']].sum(axis=1)\n\n# Department and Store should be categorical values\ntrain_test['Dept'] = train_test['Dept'].apply(str)\ntrain_test['Store'] = train_test['Store'].apply(str)","e007b064":"# holidays\ntrain_test.loc[train_test.Week==6, 'IsHoliday'] = 'superbowl'\ntrain_test.loc[train_test.Week==22, 'IsHoliday'] = 'memorial'\ntrain_test.loc[train_test.Week==27, 'IsHoliday'] = 'independence'\ntrain_test.loc[train_test.Week==36, 'IsHoliday'] = 'laborday'\ntrain_test.loc[train_test.Week==47, 'IsHoliday'] = 'thanksgiving'\n# christmas\ntrain_test['week_day'] = np.nan\ntrain_test.loc[(train_test.Year==2010) & (train_test.Week==51), 'IsHoliday'] = 'christmas'\ntrain_test.loc[(train_test.Year==2010) & (train_test.Week==51), 'weekday'] = 'friday'\ntrain_test.loc[(train_test.Year==2011) & (train_test.Week==52), 'IsHoliday'] = 'christmas'\ntrain_test.loc[(train_test.Year==2011) & (train_test.Week==52), 'weekday'] = 'monday'\ntrain_test.loc[(train_test.Year==2012) & (train_test.Week==52), 'IsHoliday'] = 'christmas'\ntrain_test.loc[(train_test.Year==2012) & (train_test.Week==52), 'weekday'] = 'tuesday'\ntrain_test.loc[(train_test.Year==2012) & (train_test.Week==51), 'weekday'] = 'before_CM'\n# easter\ntrain_test.loc[(train_test.Year==2010) & (train_test.Week==13), 'IsHoliday'] = 'easter'\ntrain_test.loc[(train_test.Year==2011) & (train_test.Week==16), 'IsHoliday'] = 'easter'\ntrain_test.loc[(train_test.Year==2012) & (train_test.Week==14), 'IsHoliday'] = 'easter'\ntrain_test.loc[(train_test.Year==2013) & (train_test.Week==13), 'IsHoliday'] = 'easter'\n\n# setting rest of holidays to NaN\nholidays = ['superbowl','laborday','thanksgiving','christmas','easter','independence','memorial']\ntrain_test.loc[~train_test['IsHoliday'].isin(holidays), 'IsHoliday'] = np.nan\n\n# holiday dummies\nholiday_dummies = pd.get_dummies(train_test['IsHoliday'], prefix='Holiday')\n\n# extra holiday column for WMAE\ntrain_test['old_IsHoliday'] =[1 if i != 0 else 0 for i in train_test['IsHoliday'].fillna(0)]\n\n# concat dummies with data\ntrain_test = pd.concat([train_test, holiday_dummies], axis=1)\ntrain_test.head()","4333db03":"# Long functions - Run them ones!\ndef get_affected_depts(holiday):\n    # make dataframe with overall means and means on holiday\n    overall_mean = train_test[~train_test['IsHoliday'].isin(holidays)].groupby('Dept')[['Weekly_Sales']].mean()\n    mean_on_holiday = train_test[train_test['IsHoliday'] == holiday].groupby('Dept')[['Weekly_Sales']].mean()\n    means = pd.concat([overall_mean, mean_on_holiday], axis=1, keys=['overall_mean', '%s_mean' %holiday])\n\n    # select departments with higher mean sales on christmas than overall\n    affected_departments = []\n    for label, row in means.iterrows():\n        if row['overall_mean'][0] < row['%s_mean' %holiday][0]:\n            affected_departments.append(label)\n    return affected_departments\n\n# create feature that returns True if the department is one of the affected ones by holiday_x and whether holiday_x is True\n    # - \"NaN\"\n    # - \"LD\" = (laborday)\n    # - \"SB\" = (superbowl)\n    # - \"TG\" = (thanksgiving)\n    # - \"CM\" = (christmas) \n    # - \"ID\" = (independence day)\n    # - \"EA\" = (easter)    \n    # - \"MD\"= (memorial day)\n    \n# get affected department-numbers\nchrist_depts = get_affected_depts('christmas')\nthanks_depts = get_affected_depts('thanksgiving')\nbowl_depts = get_affected_depts('superbowl')\nlabor_depts = get_affected_depts('laborday')\neaster_depts = get_affected_depts('easter')\nindependence_depts = get_affected_depts('independence')\nmemorial_depts = get_affected_depts('memorial')\n\ndef assign_holidays_to_affected_depts(row):\n    if np.logical_and(row.Dept in labor_depts, row.Holiday_laborday == 1) :\n        return 'LD' \n    elif np.logical_and(row.Dept in bowl_depts, row.Holiday_superbowl == 1) :\n        return 'SB'\n    elif np.logical_and(row.Dept in thanks_depts, row.Holiday_thanksgiving == 1) :\n        return 'TG'\n    elif np.logical_and(row.Dept in christ_depts, row.Holiday_christmas == 1) :\n        return 'CM'\n    elif np.logical_and(row.Dept in easter_depts, row.Holiday_easter == 1) :\n        return 'EA'\n    elif np.logical_and(row.Dept in independence_depts, row.Holiday_independence == 1) :\n        return 'ID'\n    elif np.logical_and(row.Dept in memorial_depts, row.Holiday_memorial == 1) :\n        return 'MD'\n    else :\n        return np.nan\n    \n# this function creates the new feature - (running takes a while)\nIsHoliday_x_and_affected_dept = train_test.apply(lambda i: assign_holidays_to_affected_depts(i), axis=1)","0ce7fee8":"# getting dummies and assign\ntrain_test['IsHoliday_x_and_affected_dept'] = IsHoliday_x_and_affected_dept\nhol_af_dummies = pd.get_dummies(train_test['IsHoliday_x_and_affected_dept'], prefix='Af')\ntrain_test = pd.concat([train_test, hol_af_dummies], axis=1)","d7f79540":"sns.barplot(train_test.IsHoliday, train_test['Weekly_Sales'], order=['superbowl','easter','christmas','memorial','independence','laborday','thanksgiving'])\nplt.xticks(ticks=[0,1,2,3,4,5,6], labels=['SB','EA','CM','MD','ID','LD','TG'])\nplt.title('Old Holiday Feature', fontsize=14)\nplt.ylabel('Mean Weekly_Sales')\nplt.show()","4005968d":"sns.barplot(train_test.IsHoliday_x_and_affected_dept, train_test['Weekly_Sales'])\nplt.title('New Holiday Feature', fontsize=14)\nplt.ylabel('Mean Weekly_Sales')\nplt.show()","50d865a4":"def get_holiday_change(holiday):\n    overall_mean = train_test[train_test['old_IsHoliday'] == False].groupby('Dept')[['Weekly_Sales']].mean()\n    mean_on_holiday = train_test[train_test['Holiday_' + holiday] == True].groupby('Dept')[['Weekly_Sales']].mean()\n    means = pd.concat([overall_mean, mean_on_holiday], axis=1, keys=['overall_mean', '%s_mean' %holiday])\n    means['mean_change_%s' %holiday] = (means['%s_mean' %holiday] - means['overall_mean']) \/ abs(means['overall_mean']) * 100    \n    return means[['mean_change_%s' %holiday]]\n\nholiday_changes = pd.concat([get_holiday_change('thanksgiving'), \n                                 get_holiday_change('christmas'), \n                                 get_holiday_change('laborday'),\n                                 get_holiday_change('superbowl'),\n                                 get_holiday_change('independence'),\n                                 get_holiday_change('easter'),\n                                 get_holiday_change('memorial')], axis=1)\n\ndef assign_change_per_holiday(row):\n    if row.Holiday_thanksgiving == True:\n        return holiday_changes[['mean_change_thanksgiving']].loc[row['Dept']][0]\n    elif row.Holiday_christmas == True:\n        return holiday_changes[['mean_change_christmas']].loc[row['Dept']][0]\n    elif row.Holiday_superbowl == True:\n        return holiday_changes[['mean_change_superbowl']].loc[row['Dept']][0]\n    elif row.Holiday_laborday == True:\n        return holiday_changes[['mean_change_laborday']].loc[row['Dept']][0]\n    elif row.Holiday_easter == True:\n        return holiday_changes[['mean_change_easter']].loc[row['Dept']][0]\n    elif row.Holiday_memorial == True:\n        return holiday_changes[['mean_change_memorial']].loc[row['Dept']][0]\n    elif row.Holiday_independence == True:\n        return holiday_changes[['mean_change_independence']].loc[row['Dept']][0]\n    else:\n        return np.nan","b4be9a76":"# Long function, only run ones!\npercentual_change = train_test.apply(lambda i: assign_change_per_holiday(i), axis=1)","d833237a":"# assign percentual change \ntrain_test['percentual_change'] = percentual_change\n\n# plot\ntrain_test.plot('percentual_change','Weekly_Sales', kind='scatter', alpha=0.5)\nplt.show()","1aec357d":"# creating a feature that combines Affected departments per holicay and types. E.g: Af_CM_TA, Af_CM_TB, Af_CM_Tc, etc...\nfor i in ['Af_CM', 'Af_EA', 'Af_LD', 'Af_SB', 'Af_TG', 'Af_ID', 'Af_MD']:\n    selected_data = train_test[train_test[i] == 1]\n    TA = selected_data[selected_data['Type'] == 'A']['Type'].index\n    TB = selected_data[selected_data['Type'] == 'B']['Type'].index\n    TC = selected_data[selected_data['Type'] == 'C']['Type'].index\n    train_test[i + '_TA'] = 0\n    train_test[i + '_TB'] = 0\n    train_test[i + '_TC'] = 0\n    train_test.loc[TA, i + '_TA'] = 1\n    train_test.loc[TB, i + '_TB'] = 1\n    train_test.loc[TC, i + '_TC'] = 1","150f0a18":"# markdown lag \nmark1_lag = train_test[['MarkDown1']].rename(columns={'MarkDown1':'mark1_lag'})\nmark1_lag.drop(536633, inplace=True)\nmark1_lag = pd.concat([pd.DataFrame([np.nan]).rename(columns={0:'mark1_lag'}), mark1_lag]).reset_index(drop=True)\n\nmark2_lag = train_test[['MarkDown2']].rename(columns={'MarkDown2':'mark2_lag'})\nmark2_lag.drop(536633, inplace=True)\nmark2_lag = pd.concat([pd.DataFrame([np.nan]).rename(columns={0:'mark2_lag'}), mark2_lag]).reset_index(drop=True)\n\nmark3_lag = train_test[['MarkDown3']].rename(columns={'MarkDown3':'mark3_lag'})\nmark3_lag.drop(536633, inplace=True)\nmark3_lag = pd.concat([pd.DataFrame([np.nan]).rename(columns={0:'mark3_lag'}), mark3_lag]).reset_index(drop=True)\n\nmark4_lag = train_test[['MarkDown4']].rename(columns={'MarkDown4':'mark4_lag'})\nmark4_lag.drop(536633, inplace=True)\nmark4_lag = pd.concat([pd.DataFrame([np.nan]).rename(columns={0:'mark4_lag'}), mark4_lag]).reset_index(drop=True)\n\nmark5_lag = train_test[['MarkDown5']].rename(columns={'MarkDown5':'mark5_lag'})\nmark5_lag.drop(536633, inplace=True)\nmark5_lag = pd.concat([pd.DataFrame([np.nan]).rename(columns={0:'mark5_lag'}), mark5_lag]).reset_index(drop=True)\n\n# markdown presence\ntrain_test = train_test.assign(md1_present = train_test.MarkDown1.notnull().astype(int))\ntrain_test = train_test.assign(md2_present = train_test.MarkDown2.notnull().astype(int))\ntrain_test = train_test.assign(md3_present = train_test.MarkDown3.notnull().astype(int))\ntrain_test = train_test.assign(md4_present = train_test.MarkDown4.notnull().astype(int))\ntrain_test = train_test.assign(md5_present = train_test.MarkDown5.notnull().astype(int))\n\n# concat them\ntrain_test = pd.concat([train_test, mark1_lag, mark2_lag, mark3_lag, mark4_lag, mark5_lag], axis=1)","495ba90f":"train_test['IsHoliday'] = train_test['IsHoliday'].replace({np.nan: 'NA'})\nfirst_part = train_test[['Store','Dept', 'IsHoliday', 'Week', 'weekday', 'Weekly_Sales']]\nfirst_part['weekday'] = first_part['weekday'].replace({np.nan: 'NA'})\nfirst_part = first_part.groupby(['Store','Dept', 'IsHoliday', 'Week', 'weekday'])[['Weekly_Sales']].mean()\nprint(first_part.shape)\nfirst_part.reset_index()[(first_part.reset_index()['IsHoliday'] == 'christmas') & (first_part.reset_index()['Week'] == 52)].head()","7a11000b":"second_part = train_test[(train_test.Year == 2012) & (train_test.Week.isin([51,52]))].groupby(['Store','Dept', 'IsHoliday', 'Week', 'weekday'])[['Weekly_Sales']].mean()\nsecond_part = second_part.reset_index()\nsecond_part.head()","55215363":"# finding the right values for the 2012 data for week 51 and 52\nfor label, row in second_part.iterrows():\n    store = row['Store']\n    dept = row['Dept']\n    weekday = row['weekday']\n\n    try:\n        if weekday == 'before_CM': # for when it isn't officially christmas week yet but much purchases are done\n            weekly_sale_before_CM = first_part.loc[store, dept, 'christmas', 51, 'friday'][0] * 0.85 #2010\n            second_part.loc[label, 'Weekly_Sales'] = weekly_sale_before_CM\n\n        elif weekday == 'tuesday': # for when it is officially christmas but because it falls on tuesday the purchases are devided with previous week\n            weekly_sale_CM =  first_part.loc[store, dept, 'christmas', 51, 'friday'][0] * 0.7 #2010\n            second_part.loc[label, 'Weekly_Sales'] = weekly_sale_CM\n            \n    except:\n        # if department wasn't in train data or in train data 2010\n        second_part.loc[label, 'Weekly_Sales'] = np.nan # we'll later impute it with a median of stores per week and holiday","0268d4c9":"# create a df with values for all of the rows excluding the NaN-values\nweekly_dept_mean = pd.merge(first_part, second_part, on=['Store','Dept','IsHoliday','Week','weekday'], how='left')\nprint(weekly_dept_mean.shape)\nweekly_dept_mean.fillna(0, inplace=True)\nweekly_dept_mean['Weekly_Sales'] = weekly_dept_mean['Weekly_Sales_x'] + weekly_dept_mean['Weekly_Sales_y']\nweekly_dept_mean['Weekly_Sales'] = weekly_dept_mean['Weekly_Sales'].replace({0: np.nan})\nprint(weekly_dept_mean.isna().sum())\n\n# departments of which some values are still None. They are not in the train data\ndisplay(weekly_dept_mean[~weekly_dept_mean['Weekly_Sales'].notnull()]['Dept'].value_counts().sort_index(), weekly_dept_mean[~weekly_dept_mean['Weekly_Sales'].notnull()])\nweekly_dept_mean = weekly_dept_mean[['Store','Dept','IsHoliday','Week','weekday','Weekly_Sales']]\n\n# impute feature column where rows are with departments that only excist in test with the median of the store per week and holiday\nstore_medians = train_test[['Store', 'IsHoliday', 'Week', 'Weekly_Sales']]\nstore_medians['IsHoliday'] = store_medians['IsHoliday'].replace({np.nan: 'NA'})\nstore_medians = store_medians.groupby(['Store','Week','IsHoliday'])[['Weekly_Sales']].median()\n\nweekly_dept_mean.fillna(0, inplace=True)\nweekly_dept_mean[['Weekly_Sales']] = weekly_dept_mean.apply(lambda i: store_medians.loc[i['Store'], i['Week'], i['IsHoliday']][0] if i['Weekly_Sales'] == 0 else i['Weekly_Sales'], axis=1)\nweekly_dept_mean = weekly_dept_mean.set_index(['Store','Dept','IsHoliday','Week','weekday'])[['Weekly_Sales']]\nweekly_dept_mean","a56e9f6a":"# Long functions - Run them ones!\ntrain_test['weekday'] = train_test['weekday'].replace({np.nan: 'NA'})\n\n# new - based on functions above\ndept_weekly_mean_per_store = train_test[['Store', 'Dept', 'IsHoliday', 'Week', 'weekday']].apply(lambda i: weekly_dept_mean.loc[i['Store'], i['Dept'], i['IsHoliday'], i['Week'], i['weekday']][0], axis=1)","3f35eea1":"# Long functions - Run them ones!\nweekly_store_max = train_test.groupby(['Store', 'IsHoliday', 'Week', 'Year'])[['Weekly_Sales']].sum()\nweekly_store_max = weekly_store_max[weekly_store_max['Weekly_Sales'] != 0]\nweekly_store_max = weekly_store_max.groupby(['Store', 'IsHoliday', 'Week'])[['Weekly_Sales']].mean()\nweekly_store_max_total = train_test[['Store', 'IsHoliday', 'Week']].apply(lambda i: weekly_store_max.loc[i['Store'], i['IsHoliday'], i['Week']][0], axis=1)","92788c36":"# assign newly created features to the data\ntrain_test['dept_weekly_mean_per_store'] = dept_weekly_mean_per_store\ntrain_test['weekly_store_mean_total'] = weekly_store_max_total\n# getting percentage\ntrain_test['dept_percentage'] = train_test['dept_weekly_mean_per_store'] \/ train_test['weekly_store_mean_total'] * 100\n# resetting IsHoliday column NaN Features\ntrain_test['IsHoliday'] = train_test['IsHoliday'].replace({'NA': np.nan})\ntrain_test['weekday'] = train_test['weekday'].replace({'NA': np.nan})","15d8cfd8":"# get dummies for features that are not machine readable yet\ndept_dummies = pd.get_dummies(train_test['Dept'], drop_first=True, prefix='Dept')\nstore_dummies = pd.get_dummies(train_test['Store'], drop_first=True, prefix='Store')\nmonth_dummies = pd.get_dummies(train_test['Month'], drop_first=True, prefix='Month')\nweek_dummies = pd.get_dummies(train_test['Week'], drop_first=True, prefix='Week')\nyear_dummies = pd.get_dummies(train_test['Year'], drop_first=True, prefix='Year')\ntype_dummies = pd.get_dummies(train_test['Type'], prefix='Type').drop('Type_C',axis=1)\nold_IsHoliday_dummies = pd.get_dummies(train_test['old_IsHoliday'], prefix='old_IsHoliday', drop_first=True)\n\n# concat dummies with data\ntrain_test = pd.concat([train_test, type_dummies, week_dummies, month_dummies, store_dummies, dept_dummies, year_dummies, old_IsHoliday_dummies], axis=1)","1a5c9349":"# pearson correlation \ncheck_corr = train_test.corr()","ce264c93":"check_corr[['Weekly_Sales']].sort_values('Weekly_Sales', ascending=False).head(10)","638ee06b":"# plotting best indicator feature\nplt.scatter(train_test['dept_weekly_mean_per_store'], train_test['Weekly_Sales'])\nplt.title('New Feature', fontsize=14)\nplt.ylabel('Weekly_Sales', fontsize=12)\nplt.xlabel('Assigned weekly average sales', fontsize=12)\nplt.show()","5daa638d":"# Multicolinearity\nplt.figure(figsize=(12,8))\ncor = check_corr.loc[['Weekly_Sales', 'Size','dept_weekly_mean_per_store', 'dept_percentage','Dept_92','weekly_store_mean_total'], ['Weekly_Sales', 'Size','dept_weekly_mean_per_store', 'dept_percentage','Dept_92','weekly_store_mean_total']]\nthreshold = cor < 0.7 \nsns.heatmap(cor, mask=threshold, cmap='Blues') \nplt.show()","e15d0b16":"# all features in lists for feature selection\naffected_features = ['Af_CM', 'Af_EA', 'Af_LD', 'Af_SB', 'Af_TG', 'Af_ID', 'Af_MD']\naffected_features_types = ['Af_CM_TA', 'Af_EA_TA', 'Af_LD_TA', 'Af_SB_TA', 'Af_TG_TA', 'Af_CM_TB', 'Af_EA_TB', 'Af_LD_TB', 'Af_SB_TB', 'Af_TG_TB', 'Af_CM_TC', 'Af_EA_TC', 'Af_LD_TC', 'Af_SB_TC', 'Af_TG_TC', 'Af_ID_TA', 'Af_ID_TB', 'Af_ID_TC', 'Af_MD_TA', 'Af_MD_TB', 'Af_MD_TC']\n\ndept_features = dept_dummies.columns.to_list()\ntype_features = type_dummies.columns.to_list()\nstore_features = store_dummies.columns.to_list()\nweek_features = week_dummies.columns.to_list()\nmonth_features = month_dummies.columns.to_list()\nyear_features = year_dummies.columns.to_list()\n\nmarkpresent_features = ['md1_present', 'md2_present', 'md3_present', 'md4_present', 'md5_present']\nmarklag_features = ['mark1_lag', 'mark2_lag', 'mark3_lag', 'mark4_lag', 'mark5_lag']\nmarkdown_features = ['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']\nother_features = ['Size', 'old_IsHoliday', 'dept_weekly_mean_per_store', 'weekly_store_mean_total', 'dept_percentage']\n\n# The final features-combination-input\nfeature_names = other_features + dept_features\n\n# get data\nX = train_test[train_test['dataset'] == 'train'][feature_names].fillna(0)\ny = train_test[train_test['dataset'] == 'train']['Weekly_Sales']","c96f679a":"# split data\nX_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.6)\n\n# modeling \nrf = RandomForestRegressor(verbose=2, n_estimators=57)\n\n# fitting model\nrf.fit(X_train, y_train)\nnames = train_test[feature_names]\n\nprint(\"Features sorted by their score:\")\nprint(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), names), reverse=True)) # see link 4","ae18a99f":"# neural network\nnn = MLPRegressor(verbose=2)\nnn.fit(X_train,y_train.astype(int))","2f995965":"# scoring\ndef WMAE(data, real, predicted):\n    weights = [5 if i == 1 else 1 for i in data.old_IsHoliday]\n    return np.round(np.sum(weights*abs(real-predicted))\/(np.sum(weights)), 2)","00ae8cc2":"# predicting with RandomForest\ny_pred_rf = rf.predict(X_valid)\nprint('rf-WMAE:', WMAE(X_valid, y_valid, y_pred_rf))\n\n# plotting rf-predictions against y_valid\nplt.scatter(y_pred_rf, y_valid)\nplt.title('Random Forest')\nplt.show()\n\nplt.scatter(X_valid.index, y_valid)\nplt.scatter(X_valid.index, y_pred_rf, c='red', alpha=0.1)\nplt.show()","35fd4c2c":"# predicting with Neural Network\ny_pred_nn = nn.predict(X_valid)\nWMAE(X_valid, y_valid, y_pred_nn)\nprint('nn-WMAE:', WMAE(X_valid, y_valid, y_pred_nn))\n\n# plotting nn-predictions against y_valid\nplt.scatter(y_pred_nn, y_valid)\nplt.title('Neural Net')\nplt.show()\n\nplt.scatter(X_valid.index, y_valid)\nplt.scatter(X_valid.index, y_pred_nn, c='red', alpha=0.1)\nplt.show()","54add98b":"# for now we take X and y to fit the model on all the data\nrf = RandomForestRegressor(verbose=2, n_estimators=57, random_state=42)\nrf.fit(X, y)\nnames = train_test[feature_names]\n\nprint(\"Features sorted by their score:\")\nprint(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), names), reverse=True)) # see link 4","e9584596":"# neural network\nnn = MLPRegressor(verbose=2, random_state=42)\nnn.fit(X,y.astype(int))","cbeca089":"# predicting on test data for submission\ntest_predict = train_test[train_test['dataset'] == 'test'][feature_names].fillna(0)\ntest_y_pred = rf.predict(test_predict)\n\n# # neural_net predicting on test data for submission\n# test_predict = train_test[train_test['dataset'] == 'test'][feature_names].fillna(0)\n# test_y_pred = nn.predict(test_predict)","69614e32":"# submitting\ntestfile = pd.concat([train_test[train_test['dataset'] == 'test'].reset_index(drop=True), pd.DataFrame(test_y_pred)], axis=1).rename(columns={0:'prediction'})\n\nsubmission = pd.DataFrame({'id':pd.Series([''.join(list(filter(str.isdigit, x))) for x in testfile['Store'].astype(str)]).map(str) + '_' +\n                           pd.Series([''.join(list(filter(str.isdigit, x))) for x in testfile['Dept'].astype(str)]).map(str)  + '_' +\n                           testfile['Date'].astype(str).map(str),\n                          'Weekly_Sales':testfile['prediction']})\nsubmission.head()","10db6d03":"submission.to_csv('Walmart_submission.csv',index=False)","e907a3e0":"# making datasets for next plot\nweekly_sales_2012_test = testfile[testfile['Year'] == 2012].groupby(testfile['Week'])[['prediction']].mean()\nweekly_sales_2013_test = testfile[testfile['Year'] == 2013].groupby(testfile['Week'])[['prediction']].mean()\n\nplt.figure(figsize=(15,5))\n\n# working on xticks labels with dates\nweeks = [str(i) for i in range(1,53)]\nweeks[5] = 'superbowl - 6'\nweeks[12] = 'easter - 2010 & 2013 \/ 13'\nweeks[13] = 'easter - 2012 \/ 14'\nweeks[15] = 'easter - 2011 \/ 16'\nweeks[21] = 'memorial day - 22'\nweeks[26] = 'independence day - 27'\nweeks[35] = 'laborday - 36'\nweeks[46] = 'thanksgiving - 47'\nweeks[50] = 'christmas - 2010 \/ 51'\nweeks[51] = 'christmas - 2011 & 2012 \/ 52'\n\n# plotting \nplt.plot(weekly_sales_2010.index, weekly_sales_2010['Weekly_Sales'])\nplt.plot(weekly_sales_2011.index, weekly_sales_2011['Weekly_Sales'])\nplt.plot(weekly_sales_2012.index, weekly_sales_2012['Weekly_Sales'])\n\nplt.plot(weekly_sales_2012_test.index, weekly_sales_2012_test['prediction'], marker='D')\nplt.plot(weekly_sales_2013_test.index, weekly_sales_2013_test['prediction'], marker='D')\n\nplt.xticks(np.arange(1, 53, step=1), labels=weeks, rotation=90)\nplt.legend(['2010','2011','2012','2012_predictions', '2013_predictions'], fontsize=16)\nplt.title('Average predicted Weekly Sales - Per week & year', fontsize=18)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Week', fontsize=16)\nplt.show()","80ada719":"## 3.5 Inspect predictions ","7a1c7809":"Is the markdown policy the same among all stores? Let's make a plot","4df657f7":"Plot markdown by date and store.","df91eb36":"## 3.4 Final Model & Submission","cec387bf":"Let's look at multicolinearity","74127ed4":"#### 5. Stores","68031f35":"The evaluation is based on Weighted Mean Absolute Error (WMAE), with a weight of 5 for Holiday Weeks and 1 otherwise. Turning this formula in a function, we create the following code.","cafc3e37":"In the EDA we saw that in some instances the mardowns preceded the sales. To implement this in the model we can create a feature which puts the relevant markdown values and weekly sales in the correct week. Also we created a feature which indicates whether a certain markdown type is present in a certain period. ","57cd088d":"# 1.2 Bi-variate Analysis","79e44e4d":"Markdowns on average lower or higher during holidays?","11cdde99":"- It seems that the biggest stores also have the highest Weekly_Sales.","e5c8193d":"- Now it's clear that the stores and departments have a big difference in sales among eachother, which probably also influences the mean to differ from the median. We'll confirm this below.","57d849d9":"## 1.1 Imports and First Impressions","c9b9c072":"- Because the markdowns seem to either consist mostyly of 1 value or have a lot of oultiers above 0 they are most likely connected to one of the holidays \n- Furthermore, only the unemployment-rate seems to have some outliers","145eecfd":"Earlier, we noticed multicolinearity between type and size. Also, both features had a small correlation with the Weekly_Sales. Let's again make a plot to verify our assumptions.","3d8c3e7a":"## 2.3 Feature Engineering Idea 2","efe91721":"Let's have a look at the differences in sales between the stores.","36b63066":"## 2.4 Feature Engineering Idea 3","c1b66534":"So far we already created dummy variables for the different holiday types. Since the sales are not affected in all departments. This means our dummy variables are not yet a great indicator for the weekly sales. Therefore we'll try to create a new feature which will only work with the departments that are positively affected by the different holidays.","90dafef1":"- Looking at the y-axis we can see that the new feature is less affected by certain departments. Hereby we can further differentiate between the departments and holidays which seem to contribute to the development of the weekly sales.  ","83689c09":"#### 4. Year","93df3773":"Let's plot boxplots for better overview of the outliers","ec98d60b":"## 2.6 Feature Engineering Idea 5","1b0dde3d":"\n#### Walmart business problem\nFor this project Walmart made data available on Kaggle for (aspiring) data-scientists to help Walmart with their business problem. In this particulair case Walmart provided a few years worth of weekly retail data based on wich they would like a model which could be used to predict their future sales. This model could be very valuable because it would lead to a better understanding of multiple facets of their business such as: Sales fluctuations, warehouse-to-store logistics and their seasonal markdowns.  ","1c5ce8aa":"Something we didn't do now, but we still might want investigate later:\n- Sales are higher when there are holidays. Until what extent is this caused by markdowns?","9bd1647d":"### <i>Points of improvement<\/i>\n1. 1.4 Further EDA on important Features\n    - Sales are higher when there are holidays. We could further explore untill what extent this could be caused by markdowns.\n2. 3.4 Final Model & Submissions\n    - Even though it seems that we found a great explanotry variable for the Weekly_Sales, there are plenty of better scores on Kaggle than ours. Therefor we expect the test data to be slightly different than what we fitted the model on - train data. Herefor, to improve our model and make adjustments for the difference in the test data we could further explore it.\n    - The test data seems to be different from the train data in a couple of aspects:\n        - There are departments we need to predict upon that are not in the train data\n        - Some of the holidays fall on other dates\/ weeks what makes it harder to predict\n    - Also, it seems there are still some more official holidays to add to make better use of the holiday feature","bfbba60d":"Let's plot histograms for distribution on the continious number variables.","db638f0f":"- We see the same for departments as with stores. There seems to be a big difference in sales between departments. \n- Also we see that some departments don't seem to have any sales on average. But it is a False conclusion since these departments actually are departments that are not in the train dataset, but are in the test dataset. This means that with our model we will need to predict on departments that it didn't see before.\n- Let's verify the different sales per department.","266f3aac":"#### Workflow:\n1. Understanding Data\n<ul>\n<li>1.1 Imports and First Impressions<\/li>\n<li>1.2 Univariate Analysis<\/li>\n<li>1.3 Bivariate Analysis<\/li>\n<li>1.4 Further EDA on important features<\/li>\n<\/ul>\n2. Feature Engineering\n<ul>\n<li>2.1 Basic Feature Changes<\/li>\n<li>2.2 Feature Engineering Idea 1<\/li>\n<li>2.3 Feature Engineering Idea 2<\/li>\n<li>2.4 Feature Engineering Idea 3<\/li>\n<li>2.5 Feature Engineering Idea 4<\/li>\n<li>2.6 Feature Engineering Idea 5<\/li>\n<li>2.7 Feature Engineering Idea 6<\/li>\n<\/ul>\n3. Modeling\n<ul>\n<li>3.1 Model Preperation<\/li>\n<li>3.2 Correlation of Prepared Features<\/li>\n<li>3.3 Model- and Feature Selection & Hyperparamater Tuning<\/li>\n<li>3.4 Final Model & Submission<\/li>\n<li>3.5 Visualizing predictions<\/li>\n<\/ul>","f1c0407c":"What is the correlation between Types and Weekly_Sales? Let's plot!","e08d3bc7":"#### 3. IsHoliday","4767cd5d":"## 3.2 Correlation of all Model Features","4ec4ce7e":"### Numeric Features\nImportant for numeric features is the distribution including the mean, median and mode. For this we'll use histograms and boxplots.","486ed7a4":"## 3.1 Model Preperation","bb1da27d":"Let's plot the mean Sales over the Years","a5fc9e22":"# 2. Feature Engineering","fab71aa3":"To further distinguish between holidays and affected departments we want to create a feature which also considers store-type. ","72a0ea6f":"Plotting barplots for categorical values.","e52803aa":"# 1. Understanding Data","f33d4eda":"#### To sum up: \nWe found an x amount of features that seem most relevant to indicate our target variable. These features either came up in our analysis or by our interpretation of their possible meaning. \n1. MarkDowns:\n    - So far, we noticed a linear relationship between MarkDown3 and IsHoliday. \n2. Type & Size:\n    - We'll further investigate this multicolinearity and it's behaviour in regards to the Weekly_Sales\n3. IsHoliday:\n    - We're going to further explore the relation between holidays and sales. Also we'll look whether all holidays are included.\n4. Year:\n    - We further want to explore the Weekly_Sales over 2010 up and till 2012 to see whether a respective development is applicable. \n5. Store:\n    - We are going to further explore the relationship between sales and the individual stores\n6. Dept:\n    - Since we expect the departments to be identical over all the stores by selling the same goods (e.g. an electronical- & groceries department), we like to further investigate the difference within the departments themselves.  ","932ee630":"Let's make scatterplots to confirm the above.","0d03fd04":"- We see that christmas sales are done in the preceding week, so we have to adjust this later on.","a57676c6":"- In our plot with which we investigated the sales over the years we noticed that the IsHoliday feature missed out on some important dates. To make our future model better predict based on the holiday feature we completed the data with more relevant holidays.\n- Also, to let our model interpretet the feature we created dummy variables of the new column with holiday types and kept the old IsHoliday column.","1036f76e":"To be able to create the best possible predictive model we followed a few essential steps. We started with model preperation, where we converted our data to machine language. After a final insight in the linear correlation we selected the best features and model by evaluation (trial and error) of the different feature-combinations and models. Finally we created a submission file based on our best model, so we could achieve our best possible score in the Kaggle competion.  ","3559117d":"Let's create some plots for the discrete number variables","437ed239":"#### 2. Type & Size ","645744c7":"Now, with all of the new features, lets have a look at some correlations insights with the Weekly_Sales.","9606a913":"Let's look at the correlation coefficients between Weekly_Sales and its features. ","a0728bb5":"- All categorical columns are nominal.","2a3d66e5":"At this point we have concluded our EDA. The next step is to look whether we can create more meaningful features which could be used in the final model for sales predictions. ","4dd41d57":"the train and test data is splitted by date. \n- The train data contains rows that go from 2010 till the 26th of October 2012. \n- The test data contains rows that go from November 2nd 2012 till July 26th, 2013.\n\nTo better analyse our data and make preproccesing more convenient we'll combine the datasets till our modelingprocess.","f53aa17f":"### Bibliography\n1. https:\/\/www.kaggle.com\/avelinocaio\/walmart-store-sales-forecasting\n2. https:\/\/www.kaggle.com\/c\/walmart-recruiting-store-sales-forecasting\/notebooks?sortBy=voteCount&group=everyone&pageSize=20&competitionId=3816\n3. https:\/\/www.startpagina.nl\/v\/wetenschap\/wiskunde\/vraag\/660756\/bereken-correlatie-tussen-nominaal-meetniveau\/\n4. https:\/\/blog.datadive.net\/selecting-good-features-part-iii-random-forests\/\n5. https:\/\/www.timeanddate.com\/holidays\/us\/?hol=1\n6. https:\/\/towardsdatascience.com\/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e\n7. https:\/\/explained.ai\/rf-importance\/index.html#5\n8. https:\/\/www.inc.com\/bill-murphy-jr\/heres-crazy-reason-why-holiday-season-is-6-days-shorter-in-2019.html","87308a5b":"- It seems that smaller stores are labeled as type C, medium sized stores as type B, and the biggest ones as type A.","ea356f27":"## 2.2 Feature Engineering Idea 1","42aa281f":"- This gives us an interesting insight in the difference between the sales of the stores. We'll later on look how to make better use of the feature by converting the data. ","a535aecd":"- It seems that with some of the new created features we'll be able to predict quite accurately. Let's see how they really behave in models!","b469d29c":"## 3.3 Model- and Feature Selection & Hyperparameter Tuning","ab8ad13f":"To further confirm the preceding insight, let's plot sales over time per store.","bab3eb49":"The columns below seems to have a high relationship between each other:\n- MarkDown1 - MarkDown4\n- Week - Month\n- Type - Size","a2086a02":"Earlier we found weekly_sales is most affected by 'Store', 'Department', 'week' and 'special events such as holidays. Therefore we created a feature which calculates the average sales combined on these features. Also we created the total weekly_sales per store and week, divided by the years. We did this so we could divide the former created feature by the total weekly_sales of the store to get a percentage which could be assigned to the responisble department. Also, because christmas falls on a Tuesday in 2012 and on a Monday in 2011 we have to adjust the results so that the model won't predict the same sales for christmas 2012 as we're in week 51 and 52 in 2011.","d53d8826":"- The median is much lower than the mean. This is caused by a couple of stores\/departments that have huge weekly sales.","066525ab":"Per holiday type there still is a huge differents between the affected departments and the average weekly_sales. The reason for this is probably that there are departments that affected more than others or relatively more. A possible solution could be finding the percentual change in weekly_sales of the affected departments in order to give a better indication of what the possible sales could be eventualy. ","707f8ed4":"## 2.1 Basic Feature Changes","7d3dcc3c":"Finaly based on the WMAE we were able to retrieve the best model, hyperparameters and data-feature-combination to predict on our given test set. ","454b7a70":"## 1.4 Further EDA on important features","53f455ca":"## Student Project Team\nThe Hague University of Applied Sciences <br><br>\nChristiaan Morreau: https:\/\/www.linkedin.com\/in\/christiaan-morreau-507565187\/ <br>\nMarcus van Gulik: https:\/\/www.linkedin.com\/in\/marcus-van-gulik-7718b91bb\/ <br>\nGerard Draadjer: https:\/\/www.linkedin.com\/in\/gerard-draadjer-635751179\/ <br>","50fa6cfa":"## 2.5 Feature Engineering Idea 4","1cf2fa14":"- Why are markdowns higher during holidays? In the description they mention markdowns PRECEDE holidays.","3a91d7c4":"# 3 Modeling","345e25b4":"- We see that some markdowns do precede holidays and others fall upon the holiday weeks themselves. This has probably to do with the kind of holiday.","a00d448e":"## 1.2 Univariate Analysis","f4290c3a":"#### 6. Departments","9d3afeac":"## 2.7 Feature Engineering Idea 6","2aec1884":"With al our features combined we ran multiple model-configurations to see which features where scoring best according to the feature-importance-attribute of the models. Afterwards we tried different settings and parameters in the models in order to achieve an even better final score. ","1bfdd227":"### Kaggle Leaderboard\n- Public Score: 3348.10600\n- Private Score: 3463.42705","d29e149c":"Let's have a look at the mean weekly sales between holiday periods and regular ones!","7e401d3e":"#### 1. Markdowns","5d5202a7":"- On first sight there doesn't seem to be high correlations with the target variable. To further investigate this, we'll first want to make sure we don't miss out on other correlatoins such ash quadratic or exponential ones.","156f43d8":"Let's plot the new predictions over time and see how they'll differ from the train_data!","1548a9ea":"We'll want to do the same thing for departments as for stores, so let's plot!","970f4264":"- Looking at the resulting plot we have to conclude that this feature will not be able to contribute to a better predictive model."}}