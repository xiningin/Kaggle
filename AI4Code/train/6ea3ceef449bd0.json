{"cell_type":{"10f65151":"code","0364670a":"code","1452aa8c":"code","bbd2f705":"code","71b8abfd":"code","c74ba10b":"code","20752f7e":"code","a6bb3c29":"code","5e55be2f":"code","e857b462":"code","d4fa4a7a":"code","e0c602ea":"code","f46d0985":"code","277aea2e":"code","90924f60":"code","08a34534":"code","a97aa7f7":"code","3f208744":"code","d85aecf9":"code","1d05ff3c":"code","0392807f":"code","2876f58d":"code","b245d5ac":"code","60de5463":"code","7b439ca5":"code","0b2f0251":"code","db079b48":"code","28a2b87c":"code","eb341666":"code","86f5e2f0":"code","96068f88":"markdown","764b7744":"markdown","d878817d":"markdown","b469f921":"markdown","76763998":"markdown","8a738ebb":"markdown","54a76a7f":"markdown","613d335a":"markdown","782dd1db":"markdown","99d9e73f":"markdown","50a26434":"markdown","8acd5382":"markdown","1e0bb4e0":"markdown","b8d886ce":"markdown","6bf61af9":"markdown","d184d4d2":"markdown","8283a68f":"markdown","4bd08452":"markdown","4464661e":"markdown","b3f15c0a":"markdown","267a93f2":"markdown","290d29ef":"markdown","640e8510":"markdown","833de31b":"markdown","2edcca70":"markdown","1575135a":"markdown","22e2be34":"markdown","54c6ee8c":"markdown","8b0daab4":"markdown"},"source":{"10f65151":"%matplotlib notebook\nimport matplotlib.pylab as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nimport scipy.stats as st","0364670a":"# Load data\noriginal_data = pd.read_csv('..\/input\/train.csv')\n\nprint('\\tFeature\\t\\t\\tdtype\\tunique')\n# Inspect type and number of options in each feature (continuous vs discrete variables)\nfor col in original_data.columns: \n    print('%15s\\t\\t%10s\\t%d'%(col,original_data[col].dtype,original_data[col].nunique()))\n\n# Drop Id column, doesn't really contain any info about the houses\noriginal_data = original_data.drop('Id',axis=1)\n#y = original_data.SalePrice\n#original_data = original_data.drop('SalePrice', axis=1)\n\n# Summary\nquantitative_features = original_data.select_dtypes('number').columns\nquantitative_features = quantitative_features[:-1]\nqualitative_features = original_data.select_dtypes('object').columns\nprint('Houses: %d , columns: %d \\n\\n'%(original_data.shape[0],original_data.shape[1]))\nprint('%d numerical features, %d qualitative features'%(len(quantitative_features),len(qualitative_features)))","1452aa8c":"def qualitative_to_quantitative(df):\n    transformed_data = df.copy()\n    for col in qualitative_features:\n        ordering = pd.DataFrame() # it's easier to order values to do it in a data frame \n        ordering['vals'] =  original_data[col].unique()\n        ordering.index = ordering.vals\n        ordering['mean_price'] = original_data[[col,'SalePrice']].groupby(col).mean()['SalePrice']\n        ordering['rank'] = original_data[[col,'SalePrice']].groupby(col).mean()['SalePrice'].rank()\n        ordering_dict = ordering.to_dict()['rank']\n        transformed_data[col] = transformed_data[col].replace(to_replace=ordering_dict)\n        #print(col,ordering_dict)\n    return transformed_data\n\nmassaged_data = qualitative_to_quantitative(original_data)","bbd2f705":"fig, ax = plt.subplots(1,1,figsize=(10,3.5))\nfig.subplots_adjust(bottom=0.2,hspace=0.7,top=0.99)\ncorr_with_price = massaged_data.corr('spearman')['SalePrice']\ncorr_with_price = corr_with_price.drop('SalePrice')\ncorr_with_price = corr_with_price.apply(abs)\ncorr_with_price.sort_values(inplace=True)\n\ncorr_with_price.plot.bar(ax=ax)\nax.axhline(0.5)\nax.axhline(0.4)\n\n\ndef select_features(thr):\n    return corr_with_price.loc[corr_with_price > thr].keys()\n\nprint(select_features(0.5))","71b8abfd":"from sklearn import linear_model\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_log_error\n\ndef error(actual, predicted):\n    actual = np.log(actual)\n    predicted = np.log(predicted)\n    return np.sqrt(np.sum(np.square(actual-predicted))\/len(actual))\n\ndef plot(model,X_train,y_train,X_test,y_test,title,log):\n    fig, ax = plt.subplots(1,2,figsize=(14,4))\n    fig.subplots_adjust(top=0.8)\n    ax[0].scatter(y_train,model.predict(X_train),label='train',alpha=0.8,color='C3')\n    ax[1].plot(y_train-model.predict(X_train),marker='o',linestyle='None',label='train',alpha=0.8,color='C3')\n    ax[0].scatter(y_test,model.predict(X_test),alpha=0.8,color='0.2')\n    ax[1].plot(y_test-model.predict(X_test),marker='o',linestyle='None',label='test',alpha=0.8,color='0.2')\n\n    ax[1].axhline(0,color='k',linestyle='--')\n    \n    ax[0].set_ylabel('Predicted Price')\n    ax[0].set_xlabel('Real Price')\n    ax[1].set_ylabel('Price difference')\n    \n    plt.legend()\n    if log:\n        score = error(np.exp(model.predict(X_test)),np.exp(y_test))\n        ax[0].plot(range(8,17),range(8,17),color='k',linestyle='--')\n\n    else:\n        score = error(model.predict(X_test),y_test)\n        ax[0].plot(range(500000),range(500000),color='k',linestyle='--')\n\n    fig.suptitle('%s\\n score: %0.5f'%(title,score))\n    \n    return score\n    \n    \ndef plot_different_fitting_routines(x,y,xt,yt,log=False,model='linreg'):\n        \n    if 'linreg' in model:\n        regr = linear_model.LinearRegression(normalize=True)\n        regr.fit(x,y)\n        plot(regr,x,y,xt,yt,title='Linear Regression',log=log)\n\n    if 'ridge' in model:\n        ridge = linear_model.RidgeCV()\n        ridge.fit(x,y)\n        plot(ridge,x,y,xt,yt,'RidgeCV',log=log)\n\n    if 'lasso' in model:\n        lasso = linear_model.LassoCV(max_iter=10000,random_state=1)\n        lasso.fit(x,y)\n        plot(lasso,x,y,xt,yt,'Lasso',log=log)\n\n    if 'lassolars' in model:\n        lassolars = linear_model.LassoLarsCV(max_iter=10000)\n        lassolars.fit(x,y)\n        plot(lassolars,x,y,xt,yt,'LassoLars',log=log)\n\n    if 'bayes' in model:\n        bayes = linear_model.BayesianRidge()\n        bayes.fit(x,y)\n        plot(bayes,x,y,xt,yt,'Bayes',log=log)\n    \n    \ndef mse_different_fitting_routines(x,y,xt,yt):\n    \n    regr = linear_model.LinearRegression(normalize=True)\n    regr.fit(x,y)\n    regr_score = mean_absolute_error(regr.predict(X_test),y_test)\n\n    ridge = linear_model.RidgeCV()\n    ridge.fit(x,y)\n    ridge_score = mean_absolute_error(ridge.predict(X_test),y_test)\n\n    lasso = linear_model.LassoCV(max_iter=10000,random_state=1,cv=5)\n    lasso.fit(x,y)\n    lasso_score = mean_absolute_error(lasso.predict(X_test),y_test)\n\n    lassolars = linear_model.LassoLarsCV(max_iter=10000,cv=5)\n    lassolars.fit(x,y)\n    lassolars_score = mean_absolute_error(lassolars.predict(X_test),y_test)\n    \n    bayes = linear_model.BayesianRidge()\n    bayes.fit(x,y)\n    bayes_score = mean_absolute_error(bayes.predict(X_test),y_test)\n    \n    return regr_score,ridge_score,lasso_score,lassolars_score,bayes_score","c74ba10b":"y = massaged_data.SalePrice\nall_data = massaged_data.drop('SalePrice',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(all_data,y,random_state=2)\n\nimputer = SimpleImputer()\nX_train = imputer.fit_transform(X_train)\nX_test = imputer.fit_transform(X_test)\n\nplot_different_fitting_routines(X_train,y_train,X_test,y_test)","20752f7e":"y = massaged_data.SalePrice\nnumerical_data = massaged_data.drop('SalePrice',axis=1)\nnumerical_data = numerical_data[quantitative_features]\nX_train, X_test, y_train, y_test = train_test_split(numerical_data,y,random_state=1)\n\nimputer = SimpleImputer()\nX_train = imputer.fit_transform(X_train)\nX_test = imputer.fit_transform(X_test)\n\nplot_different_fitting_routines(X_train,y_train,X_test,y_test)","a6bb3c29":"# Selected features\nmse = []\nthresholds = np.arange(0.0,0.8,0.1)#[0,0.1,0.3,0.5,0.7]\n\nfor thr in thresholds:\n    \n    y = massaged_data.SalePrice\n    some_data = massaged_data.drop('SalePrice',axis=1)\n    some_data = some_data[select_features(thr)]\n    X_train, X_test, y_train, y_test = train_test_split(some_data,y,random_state=1)\n\n    imputer = SimpleImputer()\n    X_train = imputer.fit_transform(X_train)\n    X_test = imputer.fit_transform(X_test)\n\n    mse.append(mse_different_fitting_routines(X_train,y_train,X_test,y_test))\n    \nmse = np.array(mse).T\nplt.figure(figsize=(14,5))\nplt.plot(thresholds,mse[0], label = 'regression')\nplt.plot(thresholds,mse[1], label = 'ridge')\nplt.plot(thresholds,mse[2], label = 'lasso')\nplt.plot(thresholds,mse[3], label = 'lassolars')\nplt.plot(thresholds,mse[4], label = 'bayes')\nplt.xlabel('Correlation threshold')\nplt.ylabel('MSE')\nplt.yscale('log')\nplt.gca().invert_xaxis()\nplt.legend()","5e55be2f":"fig, ax = plt.subplots(1,3,figsize=(14,3))\nsns.distplot(massaged_data.SalePrice, kde=False, fit=st.lognorm,ax=ax[0])\nsns.distplot(massaged_data.GrLivArea, kde=False, fit=st.norm,ax=ax[1])\nsns.scatterplot('SalePrice','GrLivArea',data=massaged_data,ax=ax[2])\nax[2].axhline(4000,color='r')\n\nmassaged_data = massaged_data[original_data.GrLivArea < 4000]","e857b462":"y = massaged_data.SalePrice\nall_data = massaged_data.drop('SalePrice',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(all_data,y,random_state=2)\n\nimputer = SimpleImputer()\nX_train = imputer.fit_transform(X_train)\nX_test = imputer.fit_transform(X_test)\n\nplot_different_fitting_routines(X_train,y_train,X_test,y_test)","d4fa4a7a":"# Correlation between variables (dependent variables should not be included)\ntest_data = massaged_data.drop('SalePrice',axis=1)\ncorr = test_data.corr('spearman')\ncorr = corr.apply(abs)\n\nfig, ax = plt.subplots(1,2,figsize=(14,4))\nsns.heatmap(corr,cmap='viridis',ax=ax[0])\n\ncorr.mask(corr < 0.80, inplace=True )\ncorr.mask(corr == 1, inplace=True )\nsns.heatmap(corr,cmap='viridis',ax=ax[1])\n\n# Print stronger correlations\nfor col1 in corr.columns:\n    for col2 in corr[col1].keys():\n        if np.isfinite(corr[col1][col2]):\n            print(corr[col1][col2],col1,col2)","e0c602ea":"# Correlation between variables (dependent variables should not be included)\ntest_data = massaged_data.drop('SalePrice',axis=1)\ntest_data= test_data.drop('PoolQC',axis=1)\ncorr = test_data.corr('spearman')\ncorr = corr.apply(abs)\n\nfig, ax = plt.subplots(1,2,figsize=(10,4))\nsns.heatmap(corr,cmap='viridis',ax=ax[0])\n\ncorr.mask(corr < 0.8, inplace=True )\ncorr.mask(corr == 1, inplace=True )\nsns.heatmap(corr,cmap='viridis',ax=ax[1])\n\n# Print stronger correlations\nfor col1 in corr.columns:\n    for col2 in corr[col1].keys():\n        if np.isfinite(corr[col1][col2]):\n            print(corr[col1][col2],col1,col2)","f46d0985":"print(corr_with_price['YearBuilt'],corr_with_price['GarageYrBlt'])\nprint(corr_with_price['Exterior1st'],corr_with_price['Exterior2nd'])\nprint(corr_with_price['MasVnrType'],corr_with_price['MasVnrArea'])\nprint(corr_with_price['GarageCars'],corr_with_price['GarageArea'])","277aea2e":"prunned_data = massaged_data.drop('PoolQC',axis=1) # score: 0.14562\nprunned_data = prunned_data.drop('GarageYrBlt',axis=1) # score  0.14539\n#prunned_data = prunned_data.drop('Exterior2nd',axis=1) # score 0.14655 WORST! so keep this one\n#prunned_data = prunned_data.drop('MasVnrArea',axis=1) # score 0.14938 WORST! so keep this one\n#prunned_data = prunned_data.drop('GarageArea',axis=1) # score 0.14708 WORST! so keep this one","90924f60":"y = prunned_data.SalePrice\nall_data = prunned_data.drop('SalePrice',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(all_data,y,random_state=2)\n\nimputer = SimpleImputer()\nX_train = imputer.fit_transform(X_train)\nX_test = imputer.fit_transform(X_test)\n\nplot_different_fitting_routines(X_train,y_train,X_test,y_test)","08a34534":"import scipy.stats as st\n\ny = massaged_data.SalePrice\nplt.figure(figsize=(10,3))\nsns.distplot(y, kde=False, fit=st.norm)","a97aa7f7":"logy = np.log(y)\nall_data = prunned_data.drop('SalePrice',axis=1)\nX_train, X_test, logy_train, logy_test = train_test_split(all_data,logy,random_state=2)\n\nimputer = SimpleImputer()\nX_train = imputer.fit_transform(X_train)\nX_test = imputer.fit_transform(X_test)\n\nplot_different_fitting_routines(X_train,logy_train,X_test,logy_test,log=True)","3f208744":"for col in prunned_data.columns[:-1]:\n    k2, p  = st.normaltest(np.log(prunned_data[col]))\n    alpha = 1e-6#1e-3\n    if p > alpha:\n        fig, ax = plt.subplots(1,2,figsize=(9,3))\n        fig.suptitle(col)\n        sns.distplot(prunned_data[col],kde=False,ax=ax[0],fit=st.lognorm)\n        sns.distplot(np.log(prunned_data[col]),kde=False,ax=ax[1],fit=st.norm)","d85aecf9":"logy = np.log(y)\nall_data = prunned_data.drop('SalePrice',axis=1)\n\n## Transform variables\nall_data['GrLivArea'] = np.log(all_data['GrLivArea'])\nall_data['1stFlrSF'] = np.log(all_data['1stFlrSF'])\n#all_data['TotRmsAbvGrd'] = np.log(all_data['TotRmsAbvGrd'])\nall_data['ExterQual'] = np.log(all_data['ExterQual'])\n\nX_train, X_test, logy_train, logy_test = train_test_split(all_data,logy,random_state=2)\n\nimputer = SimpleImputer()\nX_train = imputer.fit_transform(X_train)\nX_test = imputer.fit_transform(X_test)\n\nplot_different_fitting_routines(X_train,logy_train,X_test,logy_test,log=True)","1d05ff3c":"corr_with_price = prunned_data.corr()['SalePrice']\ncorr_with_price = corr_with_price.drop('SalePrice')\ncorr_with_price = corr_with_price.apply(abs)\ncorr_with_price.sort_values(inplace=True)\n\nsquared_data = np.power(prunned_data,2)\nsquared_data['SalePrice'] = prunned_data.SalePrice.values\ncorr_sq_with_price = squared_data.corr()['SalePrice']\ncorr_sq_with_price = corr_sq_with_price.drop('SalePrice')\ncorr_sq_with_price = corr_sq_with_price.apply(abs)\ncorr_sq_with_price.sort_values(inplace=True)\ndiff = corr_sq_with_price.values-corr_with_price.values\n\nfig, ax = plt.subplots(1,1)\nsns.scatterplot(range(len(diff)),diff)\nax.axhline(0,color='r')\nax.set_ylabel('squared- normal')\nprint('Possible variables to square %s'%corr_with_price.keys()[diff > 0.02])","0392807f":"logy = np.log(y)\nall_data = prunned_data.drop('SalePrice',axis=1)\n\n## Transform variables\nall_data['GrLivArea'] = np.log(all_data['GrLivArea'])\nall_data['1stFlrSF'] = np.log(all_data['1stFlrSF'])\nall_data['ExterQual'] = np.log(all_data['ExterQual'])\nall_data['GarageCars'] = np.power(all_data['GarageCars'],2)\nall_data['BsmtQual'] = np.power(all_data['BsmtQual'],2)\nall_data['OverallQual'] = np.power(all_data['OverallQual'],2)\n\nX_train, X_test, logy_train, logy_test = train_test_split(all_data,logy,random_state=2)\n\nimputer = SimpleImputer()\nX_train = imputer.fit_transform(X_train)\nX_test = imputer.fit_transform(X_test)\n\nplot_different_fitting_routines(X_train,logy_train,X_test,logy_test,log=True)","2876f58d":"logy = np.log(y)\nall_data = prunned_data.drop('SalePrice',axis=1)\n\n## Transform variables\nall_data['GrLivArea'] = np.log(all_data['GrLivArea'])\nall_data['1stFlrSF'] = np.log(all_data['1stFlrSF'])\nall_data['ExterQual'] = np.log(all_data['ExterQual'])\nall_data['GarageCars'] = np.power(all_data['GarageCars'],2)\nall_data['BsmtQual'] = np.power(all_data['BsmtQual'],2)\nall_data['OverallQual'] = np.power(all_data['OverallQual'],2)\n\nX_train_final, X_test_final, logy_train_final, logy_test_final = train_test_split(all_data,logy,random_state=2)\n\nimputer = SimpleImputer()\nX_train_final = imputer.fit_transform(X_train_final)\nX_test_final = imputer.fit_transform(X_test_final)","b245d5ac":"regr = linear_model.LinearRegression(normalize=False)\nregr.fit(X_train_final,logy_train_final)\nplot(regr,X_train_final,logy_train_final,X_test_final,logy_test_final,title='Linear Regression',log=True)","60de5463":"ridge = linear_model.RidgeCV(alphas=(0.1, 1.0, 10.0),normalize=True) # the default\nridge.fit(X_train_final,logy_train_final)\nplot(ridge,X_train_final,logy_train_final,X_test_final,logy_test_final,title='RidgeCV',log=True)\nprint(ridge.alpha_)\n\nridge2 = linear_model.RidgeCV(alphas=(0.5, 1.0, 2., 5., 8.),normalize=True) # try to fine tune alpha\nridge2.fit(X_train_final,logy_train_final)\nplot(ridge2,X_train_final,logy_train_final,X_test_final,logy_test_final,title='RidgeCV',log=True)\nprint(ridge2.alpha_)","7b439ca5":"lasso = linear_model.LassoCV(normalize=True)\nlasso.fit(X_train_final,logy_train_final)\nplot(lasso,X_train_final,logy_train_final,X_test_final,logy_test_final,title='LassoCV',log=True)\nprint(lasso.alpha_)","0b2f0251":"lassolars = linear_model.LassoLarsCV(normalize=True)\nlassolars.fit(X_train_final,logy_train_final)\nplot(lassolars,X_train_final,logy_train_final,X_test_final,logy_test_final,title='LassoLarsCV',log=True)\nprint(lassolars.alpha_)","db079b48":"bayes = linear_model.BayesianRidge(normalize=True)\nbayes.fit(X_train_final,logy_train_final)\nplot(bayes,X_train_final,logy_train_final,X_test_final,logy_test_final,title='Bayes',log=True)","28a2b87c":"# Load data\nsubmission_data = pd.read_csv('..\/input\/test.csv')\nids = submission_data.Id\nsubmission_data = submission_data.drop('Id',axis=1)\n\n## Transform qualitative into quantitative features\nsubmission_data = qualitative_to_quantitative(submission_data)\n\n## Get rid of not so interesting variables\nsubmission_data = submission_data.drop('PoolQC',axis=1)\nsubmission_data = submission_data.drop('GarageYrBlt',axis=1)\n\n## Transform variables\nsubmission_data['GrLivArea'] = np.log(submission_data['GrLivArea'])\nsubmission_data['1stFlrSF'] = np.log(submission_data['1stFlrSF'])\nsubmission_data['ExterQual'] = np.log(submission_data['ExterQual'])\nsubmission_data['GarageCars'] = np.power(submission_data['GarageCars'],2)\nsubmission_data['BsmtQual'] = np.power(submission_data['BsmtQual'],2)\nsubmission_data['OverallQual'] = np.power(submission_data['OverallQual'],2)\n\n# Fix Nans\nimputer = SimpleImputer()\nX_submission = imputer.fit_transform(submission_data)\n\npredicted_prices_log = lassolars.predict(X_submission)\npredicted_prices = np.exp(predicted_prices_log)\n\nfig, ax = plt.subplots(1,1)\nax.plot(ids.values,predicted_prices,marker='o',linestyle='None',label='train')","eb341666":"np.savetxt('submission_lassolars.csv',np.dstack((ids,predicted_prices)).reshape(1459 , 2),delimiter=',',fmt='%d,%f',header='Id,SalePrice',comments='')","86f5e2f0":"bayes_predicted_prices_log = bayes.predict(X_submission)\nbayes_predicted_prices = np.exp(bayes_predicted_prices_log)\n\nfig, ax = plt.subplots(1,1)\nax.plot(ids.values,bayes_predicted_prices,marker='o',linestyle='None',label='train')\nnp.savetxt('submission_bayes.csv',np.dstack((ids,predicted_prices)).reshape(1459 , 2),delimiter=',',fmt='%d,%f',header='Id,SalePrice',comments='')","96068f88":"Another tiny bit better. Probably there is still a lot of stuff that can be done to the data (combining variables, simplifying others...) but let's focus now on the fitting part. So let's wrap up our final features so that we can play with them.","764b7744":"Now that the data is a bit more clean, we can start to look at what actual information each one has.\nFor that, I'm ploting the spearman rank of each feature with the Sale Price, this is, how monotonous does one feature vary with Sale price , which tell us how correlated it is with house price.","d878817d":"On the left, there's the full correlation map, and on the right a selection of features that have a correlation factor above 0.8 (arbitrary, but we need to start somewhere...)\nPoolQC seems to be correlated with a lot of things, let's remove that one and try to make something out of the above mess.","b469f921":"It does, we are in the right track! Let's continue and...\n\n### Gett rid of (strongly) correlated features","76763998":"Just to recap, correlation thr. of 0 means all features are included and as we move to the left, less and less features are included. And we're plotting the Mean Squared Error (MSE).\n\nA simple regression does a lot worst than other more tweakable algorithms. But the difference between including a lot or a few variables is not huge, so... probably our data is not very good still.\n\nTime for data cleaning!","8a738ebb":"** Laso **\n\n(Docs:) The optimization objective for Lasso is::\n\n    (1 \/ (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1","54a76a7f":"A bit more human-readable. Now we can also see other interesting correlations:\n    Other interesting pairs:\n    \n    0.89, 'YearBuilt', 'GarageYrBlt' -> fair to say that most garages are built with the house\n    0.89, 'Exterior1st', 'Exterior2nd' -> apparently the materials covering the house don't give much more info\n    0.87, 'MasVnrType', 'MasVnrArea' -> weirdly, the massonery type is correlated with its area\n    0.85, 'GarageCars', 'GarageArea' -> pretty understandable\n\nFrom the pair, select the one that has the strongest correlation with SalePrice and drop the other","613d335a":"| Name               | score   |\n|--------------------|---------|\n| Unfancy Regression | 0.11263 |\n| Ridge CV           | 0.11505 |\n| LassoCV            | 0.11229 |\n| LassoLarsCV        | 0.11227 |\n| Bayes              | 0.11179 |\n\n\nSeams like the best option is either the LassoLarcsCV or the Bayes Ridge model.\n\n## Prepare submission!","782dd1db":"Maybe *GrLivArea* and *1stFlrSF*? \n\nAnd eventually *TotRmsAbvGrd* and *ExterQual*?\n\nI'm repeating the same process as above, and run the cell bellow a couple of times to test this.","99d9e73f":"Let's see what we get using those best candidates","50a26434":"### First fits. Hurray! \n\nHere I'll start experimenting with some linear regression algorithms (RidgeCV, Lasso, BayesianRidge...). These are just some functions that will make the tests a bit cleaner.","8acd5382":"### Getting rid of 'special' houses (outliers)\n\nfrom https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset we are reminded that:\n\"More generally, the author of the dataset recommends removing 'any houses with more than 4000 square feet' from the dataset.\"","1e0bb4e0":"Interestingly, the score is actually quite better, which teel us that maybe a lot of the 70ish variables are not super useful (at least, without further massaging).\n\nLet's do something a bit more systematic and calculate the score with when different features are included (and using different minimisation strategies). We order the features by their correlation (spearman rank) with Sales Price, define several correlation thresholds (i.e. only include in the model features with min x correlation with the price, where x is our threshold :)) and plot it all.","b8d886ce":"First, let's just see what happens when we include ALL the features.","6bf61af9":"So let's transform it, using a logarithm. This will make the big differences between very expensive houses not be so big (which makes sense because 50 000 dolars is a huge difference if two houses cost ~100 000 but not so shocking if they cost ~500 000 dolars)","d184d4d2":"And it got another tiny bit better. Cool!\n\nDo we have some quatractic tems (i.e. their correlation with Sales price is stronger with their square)\n","8283a68f":"The score didn't dramatically improve, but still, now we know bad results can't be blamed on redundant (correlated) features","4bd08452":"## More fitting, this time, with a bit more detail.\n\n**Linear Fit, no regularization**","4464661e":"** Lasso Lars **\n\nSame objective funtion as Lasso, but different cross validation algorithm","b3f15c0a":"**Ridge CV, regularization**\n\nTry to optimize alpha. Normalisation does help here (which is not surprising, given there are a looot of features, probably not very useful)","267a93f2":"So, the winners of correlation are:\n\n        PoolQC\n        GarageYrBlt\n        Exterior2nd\n        MasVnrArea\n        GarageArea\n        \nI run the cell bellow a couple of times to calculate the scores and make sure which variables I should remove.","290d29ef":"Let's see if it imediatly and magically improves our previous score of 0.27 when all features are included.","640e8510":"** Bayes**","833de31b":"Load data set and check what variables are there, what is their type and how many different values they take","2edcca70":"Same thing, but only including the (originally) numerical variables.","1575135a":"Now this is a bigger improvement. Can we do it for some other features?","22e2be34":"## Moving away from pure linearity\nManipulate variables (a.k.a. feature engineering). Some numerical features may not follow a normal distribution, which is nicer to predict stuff, so we can transform them until that transformed form approximates a gaussian. For example, the feature we're trying to predict, Price, is actually quite skewed (see here what happens when we try to model it with a gaussian).","54c6ee8c":"House Price Predictions -- already a classic for linear regression training, which is what this kernel is :)","8b0daab4":"### Transform the qualitative variables into quantitative (that can be used in models)\n\n(some inspiration from : https:\/\/www.kaggle.com\/dgawlik\/house-prices-eda)\n\nWith qualitative variables we can implement two methods. First one is to check distribution of SalePrice with respect to variable values and enumerate them (label encoding).  Second to create dummy variable for each possible category (one-hot incoding).\n\nHere I'm using the first, 'ordering' labels by their mean sales price."}}