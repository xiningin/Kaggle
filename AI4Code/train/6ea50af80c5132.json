{"cell_type":{"1bdd986b":"code","c26ce912":"code","70a51739":"code","78fb7725":"code","aa935667":"code","afaccdf5":"code","c240bafc":"code","bdf0bff4":"code","2e73e700":"code","5594f94a":"code","2d5dbfb7":"code","5a09ff84":"code","ff2e8c72":"code","43fbef50":"code","17e5f996":"code","270e750a":"code","a1cef314":"code","5139ae50":"code","adea28b3":"code","bd9ac909":"code","0a00a262":"code","262a4952":"code","13e0cdcf":"code","6895b82a":"markdown","4abc0ab7":"markdown"},"source":{"1bdd986b":"#Importing the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import iqr\n\ngl_df=pd.read_csv('..\/input\/glass\/glass.csv')\n\ngl_df.head()","c26ce912":"gl_df.info()","70a51739":"gl_df.describe()","78fb7725":"col_names=gl_df.columns\ncol_names","aa935667":"#TO check yhe outliers using box-plot\nfor i in range(len(col_names)-1):\n    plt.figure()\n    sns.boxplot(x='Type',y=col_names[i],data=gl_df)","afaccdf5":"#To check the outliers using scatter plot\nfor i in range(len(col_names)-1):\n    plt.figure()\n    plt.scatter(gl_df['Type'],gl_df[col_names[i]])","c240bafc":"#changing the dataframe to numpy array\ndata=gl_df.values","bdf0bff4":"data","2e73e700":"#Extracting the features and label data\nX, y = data[:, :-1], data[:, -1]","5594f94a":"X","2d5dbfb7":"y","5a09ff84":"#To remove the outliers\nfrom sklearn.neighbors import LocalOutlierFactor\nlof=LocalOutlierFactor()\nyhat=lof.fit_predict(X)","ff2e8c72":"#The values with -1 are outliers so we will remove them\nyhat","43fbef50":"mask=yhat!= -1\nmask","17e5f996":"#New feature and label data after removing outliers\nX,y=X[mask,:],y[mask]\ny","270e750a":"#Spliting dataset into train dataset and test dataset\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=21)","a1cef314":"#fitting and predicting using KNN algorithm\nfrom sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train,y_train)\ny_predict=knn.predict(X_test)","5139ae50":"#KNN Score\nknn.score(X_test,y_test)","adea28b3":"# Classification report and confusion metrics\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(y_test,y_predict))","bd9ac909":"print(classification_report(y_test,y_predict,zero_division=0))","0a00a262":"#Using Cross validation for k=3 and checking the value of n for knn for best result\nfrom sklearn.model_selection import cross_val_score\nneighbors=list(range(1,20))\ncv_scores=[]\nfor n in neighbors:\n    knn=KNeighborsClassifier(n_neighbors=n)\n    score=cross_val_score(knn,X,y,cv=3,scoring='accuracy')\n    cv_scores.append(score.mean())\n    ","262a4952":"#To determine the best value for n in KNN\nimport matplotlib.pyplot as plt\nplt.plot(neighbors,cv_scores,marker='o')\nplt.xlabel(\"neighbors\")\nplt.ylabel(\"accuracy_score\")\nplt.xticks(np.arange(1,21),neighbors)\nplt.grid()\nplt.show()","13e0cdcf":"print(f\"The score at n=12 is {cv_scores[11]}.\")","6895b82a":"**So here the optimum value of k is 12**","4abc0ab7":"# KNN Classifier algorithm on glass dataset"}}