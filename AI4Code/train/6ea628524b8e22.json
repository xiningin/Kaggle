{"cell_type":{"572e3fce":"code","4d387b86":"code","67b17a56":"code","bf1e68e7":"code","847a8d00":"code","73557d54":"code","3c83544d":"code","fa697071":"code","b6d70974":"code","229bee58":"code","0c372077":"code","1cff516e":"code","735a54e6":"code","7b98db76":"code","e3ecdaaf":"code","fd43566b":"markdown","d32337aa":"markdown","1dcbfe6e":"markdown","1795cf5a":"markdown","039375a7":"markdown","07156ef5":"markdown","c4c13b09":"markdown","5c1aff8b":"markdown","f1dcc825":"markdown","2c5f4ff6":"markdown"},"source":{"572e3fce":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pickle\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nplt.style.use('ggplot')\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4d387b86":"data = pd.read_csv('\/kaggle\/input\/pe-files-malwares\/dataset_malwares.csv')","67b17a56":"data.head()","bf1e68e7":"data.info()","847a8d00":"used_data = data.drop(['Name', 'Machine', 'TimeDateStamp', 'Malware'], axis=1)","73557d54":"plt.figure(figsize=(8, 6))\nax=sns.countplot(data['Malware'])\nax.set_xticklabels(['Benign', 'Malware'])","3c83544d":"features = ['MajorSubsystemVersion', 'MajorLinkerVersion', 'SizeOfCode', 'SizeOfImage', 'SizeOfHeaders', 'SizeOfInitializedData', \n           'SizeOfUninitializedData', 'SizeOfStackReserve', 'SizeOfHeapReserve', \n            'NumberOfSymbols', 'SectionMaxChar']\ni=1\n\nfor feature in features:\n    plt.figure(figsize=(10, 15))\n    ax1 = plt.subplot(len(features), 2, i)\n    sns.distplot(data[data['Malware']==1][feature], ax=ax1, kde_kws={'bw': 0.1})\n    ax1.set_title(f'Malware', fontsize=10)\n    ax2 = plt.subplot(len(features), 2, i+1)\n    sns.distplot(data[data['Malware']==0][feature], ax=ax2, kde_kws={'bw': 0.1})\n    ax2.set_title(f'Benign', fontsize=10)\n    i= i+2\n    ","fa697071":"X_train, X_test, y_train, y_test = train_test_split(used_data, data['Malware'], test_size=0.2, random_state=0)","b6d70974":"print(f'Number of used features is {X_train.shape[1]}')","229bee58":"rfc = RandomForestClassifier(n_estimators=100, random_state=0, \n                         oob_score = True,\n                         max_depth = 16)\nrfc.fit(X_train, y_train)","0c372077":"y_pred = rfc.predict(X_test)","1cff516e":"print(classification_report(y_test, y_pred, target_names=['Benign', 'Malware']))\n","735a54e6":"cm = confusion_matrix(y_pred, y_test)\nax = sns.heatmap(cm, annot=True, fmt=\"d\", cmap=plt.cm.Blues, cbar=False)\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nplt.title('Confusion matrix')","7b98db76":"pkl_filename = \"rf_model.pkl\"\nwith open(pkl_filename, 'wb') as file:\n    pickle.dump(rfc, file)","e3ecdaaf":"importance = rfc.feature_importances_\nimportance_dict = {used_data.columns.values[i]: importance[i] for i in range (len(importance))}\nsorted_dict = {k: v for k, v in sorted(importance_dict.items(), key=lambda item: item[1])}\nplt.figure(figsize=(10, 20))\nsns.barplot(y=list(sorted_dict.keys())[::-1], x=list(sorted_dict.values())[::-1], palette='mako')\nplt.title('Features importance')","fd43566b":"### Splitting the data","d32337aa":"#### Features Importance","1dcbfe6e":"### Features visualization","1795cf5a":"### Conclusion\n- The randomforest model provide very good results without any preprocessing on the data. \n- The reult is good despite the fact that the data is imbalnced. So, I found that we do not need to use any technique to rebalance it.\n- Scaling is not nessecry, Random forest model is recursive partitioning model depends on data partition because it works on separation of features values and not make calculations on it. ","039375a7":"### Imports","07156ef5":"#### Saving model","c4c13b09":"#### Confusion matrix","5c1aff8b":"### Building the model","f1dcc825":"#### Classification report","2c5f4ff6":"### Classes Distribution"}}