{"cell_type":{"850a04c8":"code","0783d2ee":"code","c6796bcd":"code","e0bccb46":"code","3f32afde":"code","43b91c2c":"code","9996501a":"code","c5b0760d":"code","2d1bea7f":"code","452979dd":"code","85fcf55d":"code","2d903948":"code","94ab1bb1":"code","20fdfb75":"code","eee13e33":"code","f7652b60":"code","81feb3c5":"code","b6d3d66f":"code","059b804c":"code","8da5d2f9":"code","a43d5e0f":"code","e96c112f":"code","a84b2950":"code","2c9e7e9d":"code","bd9e184f":"code","959a4cea":"code","badca136":"code","319a9708":"code","356c9a00":"code","cbcd511d":"code","81c93fae":"code","a14f140b":"code","369e141a":"code","ca5ff2f8":"code","75d54722":"code","1a3ce890":"code","a46a7cf4":"code","60577a6b":"code","484270ef":"code","6b14dc5a":"code","2cec4495":"code","67dd2206":"code","afd790dd":"code","e53f6988":"code","ca8159c4":"code","23ece71a":"code","2c25ae70":"code","4857de39":"code","784a4db8":"code","d286d811":"code","9fa3d91d":"code","9a4b4031":"code","1668b2a8":"code","9d8c8ecf":"code","a1dc1fd0":"code","2ef2a18b":"markdown","41e8fe36":"markdown","01e73a6d":"markdown","0907cc0e":"markdown","23a362fc":"markdown","6c71f330":"markdown","fc235d42":"markdown","d36f3a6a":"markdown","d2d1985c":"markdown","713c7679":"markdown","04bad19d":"markdown","e47a950d":"markdown","32b8340f":"markdown","7f6fb10e":"markdown","00ab4880":"markdown","4f41e28e":"markdown","fc8ffe44":"markdown","78e690e8":"markdown","7f5eb0c9":"markdown","555f0f1d":"markdown","6b81ae4b":"markdown","0f518a4e":"markdown","c49049b3":"markdown","c9ad4dcf":"markdown","b1dec976":"markdown","14acd877":"markdown","1dea4ff9":"markdown","e8aaedfd":"markdown","9985a93a":"markdown","bb1697fc":"markdown","74869343":"markdown"},"source":{"850a04c8":"import warnings\nwarnings.simplefilter('ignore')\n\nimport pandas as pd\nfrom pandas.plotting import scatter_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import roc_auc_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, learning_curve\nfrom sklearn.utils import shuffle\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\nfrom catboost import CatBoostClassifier\nimport xgboost as xgb\n\n\nimport eli5\n\nimport riiideducation\nimport joblib\nimport time\n\n%matplotlib inline\n# for heatmap and other plots\ncolorMap1 = sns.color_palette(\"RdBu_r\")\n# for countplot and others plots\ncolorMap2 = 'Blues_r'\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/riiid-test-answer-prediction'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0783d2ee":"train_path = \"..\/input\/riiid-train-data-multiple-formats\/riiid_train.pkl.gzip\"\nquestions_path = \"..\/input\/riiid-test-answer-prediction\/questions.csv\"\nlectures_path = \"..\/input\/riiid-test-answer-prediction\/lectures.csv\"\n\ntest = \"..\/input\/riiid-test-answer-prediction\/example_test.csv\"","c6796bcd":"\n%%time\n\ndf = pd.read_pickle(train_path)","e0bccb46":"print(f\"Train shape: {df.shape}\")","3f32afde":"df.head(10)","43b91c2c":"df['timestamp_diff'] = df['timestamp'].diff()\ndf['timestamp_diff'].fillna(0, inplace=True)\ndf.loc[df['timestamp_diff'] < 0, 'timestamp_diff'] = np.median(df['timestamp_diff'])","9996501a":"df.memory_usage()","c5b0760d":"df.drop(['row_id', 'timestamp'], axis=1, inplace=True)","2d1bea7f":"df.describe()","452979dd":"print(f'Number of unique users: {len(np.unique(df.user_id))}')","85fcf55d":"print(df.isnull().sum() \/ len(df))","2d903948":"freq_answered_tasks = df['task_container_id'].value_counts().reset_index()\nfreq_answered_tasks.columns = [\n    'task_container_id', \n    'freq'\n]\n\ndf['freq_task_id'] = ''\ndf.loc[df['task_container_id'].isin(freq_answered_tasks[freq_answered_tasks['freq'] < 10000]['task_container_id'].values), 'freq_task_id'] = 'very rare answered'\ndf.loc[df['task_container_id'].isin(freq_answered_tasks[freq_answered_tasks['freq'] >= 10000]['task_container_id'].values), 'freq_task_id'] = 'rare answered'\ndf.loc[df['task_container_id'].isin(freq_answered_tasks[freq_answered_tasks['freq'] >= 50000]['task_container_id'].values), 'freq_task_id'] = 'normal answered'\ndf.loc[df['task_container_id'].isin(freq_answered_tasks[freq_answered_tasks['freq'] >= 200000]['task_container_id'].values), 'freq_task_id'] = 'often answered'\ndf.loc[df['task_container_id'].isin(freq_answered_tasks[freq_answered_tasks['freq'] >= 400000]['task_container_id'].values), 'freq_task_id'] = 'very often answered'","94ab1bb1":"df.sample(5)","20fdfb75":"df.loc[(df['content_type_id'] == 1) & (df['user_answer'] == -1), 'user_answer'] = 0\ndf.loc[(df['content_type_id'] == 1) & (df['answered_correctly'] == -1), 'answered_correctly'] = 0","eee13e33":"df.groupby(['content_type_id', 'answered_correctly']).agg({'answered_correctly': 'count'})","f7652b60":"questions = pd.read_csv(questions_path)\nquestions.head(10)","81feb3c5":"questions.describe().style.background_gradient(cmap='Blues')","b6d3d66f":"print(questions.isnull().sum() \/ len(questions))","059b804c":"lectures = pd.read_csv(lectures_path)\nlectures.head(10)","8da5d2f9":"lectures['type_of'].value_counts()","a43d5e0f":"test_example = pd.read_csv(test)","e96c112f":"test_example.head(10)","a84b2950":"n = int(df.shape[0] * 0.1)\ntrain = df.sample(n=n, random_state=42)","2c9e7e9d":"del questions\ndel lectures\n","bd9e184f":"user_characteristics = df.groupby('user_id').agg({'answered_correctly':\n                                                  ['mean', 'median', 'std', 'skew', 'count']})\nuser_characteristics.columns = [\n    'mean_user_acc',\n    'median_user_acc',\n    'std_user_acc',\n    'skew_user_acc',\n    'number_of_answered_q'\n]","959a4cea":"task_container_characteristics = df.groupby('task_container_id').agg({'answered_correctly':\n                                                                      ['mean', 'median', 'std', 'skew', 'count']})\ntask_container_characteristics.columns = [\n    'mean_task_acc',\n    'median_task_acc',\n    'std_task_acc',\n    'skew_task_acc',\n    'number_of_asked_task_containers'\n]","badca136":"task_container_characteristics.head(5)","319a9708":"content_characteristics = df.groupby('content_id').agg({'answered_correctly':\n                                                        ['mean', 'median', 'std', 'skew', 'count']})\ncontent_characteristics.columns = [\n    'mean_acc',\n    'median_acc',\n    'std_acc',\n    'skew_acc',\n    'number_of_asked_q'\n]","356c9a00":"# content_characteristics.head(5)","cbcd511d":"df = train.copy()\ndel train","81c93fae":"df = df.merge(user_characteristics, how='left', on='user_id')\ndf = df.merge(task_container_characteristics, how='left', on='task_container_id')\ndf = df.merge(content_characteristics, how='left', on='content_id')","a14f140b":"features = [\n    'prior_question_elapsed_time', \n    'prior_question_had_explanation',\n    'timestamp_diff',\n    'mean_user_acc',\n    'median_user_acc',\n    'std_user_acc',\n    'skew_user_acc',\n    'number_of_answered_q',\n    'mean_task_acc',\n    'median_task_acc',\n    'std_task_acc',\n    'skew_task_acc',\n    'number_of_asked_task_containers',\n    'mean_acc',\n    'median_acc',\n    'std_acc',\n    'skew_acc',\n    'number_of_asked_q'\n]\n\ntarget = 'answered_correctly'","369e141a":"col_to_drop = set(df.columns.values.tolist()).difference(features + [target])\nfor col in col_to_drop:\n    del df[col]","ca5ff2f8":"df['prior_question_had_explanation'] = df['prior_question_had_explanation'].fillna(value=False).astype(bool)\ndf = df.fillna(value=0.5)","75d54722":"df = df.replace([np.inf, -np.inf], np.nan)\ndf = df.fillna(0.5)","1a3ce890":"df.head(5)","a46a7cf4":"train_df, test_df, y_train, y_test = train_test_split(df[features], df[target], random_state=777, test_size=0.3)","60577a6b":"# hyper parametre optimization\n# clf = LGBMClassifier(random_state=777)\n\n# params = {\n#     'n_estimators': [50, 150, 300],\n#     'max_depth': [3, 5, 10],\n#     'num_leaves': [5, 15, 30],\n#     'min_data_in_leaf': [5, 50, 100],\n#     'feature_fraction': [0.1, 0.5, 1.],\n#     'lambda': [0., 0.5, 1.],\n# }\n\n# cv = RandomizedSearchCV(clf, param_distributions=params, cv=5, n_iter=50, verbose=2)\n# cv.fit(df[features], df[target])\n\n# print(cv.best_params_)\n# print(cv.best_score_)","484270ef":"params = {\n    'num_leaves': 10, \n    'n_estimators': 100, \n    'min_data_in_leaf': 10, \n    'max_depth': 5, \n    'lambda': 0.0, \n    'feature_fraction': 1.0 , \n    'device': 'gpu',\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0\n}","6b14dc5a":"\nparams_cat = {\n    'loss_function': 'Logloss',\n    'eval_metric': 'AUC',\n    'task_type': 'GPU' ,\n    'grow_policy': 'Lossguide',\n    'iterations': 2500,\n    'learning_rate': 4e-2,\n    'random_seed': 0,\n    'l2_leaf_reg': 1e-1,\n    'depth': 15,\n    'max_leaves': 10,\n    'border_count': 128,\n    'verbose': 50,\n}","2cec4495":"# train_df.prior_question_had_explanation.value_counts()","67dd2206":"# models lgbm and catboost uncomment to train\nmodel_lbgm = LGBMClassifier(**params)\n# model_lbgm.fit(train_df, y_train)\nmodel =  CatBoostClassifier(**params_cat)\n# model_lbgm.fit(train_df, y_train)","afd790dd":"# model.save_model(\"model.cbm\") # save model","e53f6988":"# import joblib\n# joblib.dump(model_lbgm, 'lgb.pkl')\n\n","ca8159c4":"lgb_model = joblib.load('..\/input\/model-cat-riid\/lgb.pkl')# load model","23ece71a":"model.load_model('..\/input\/model-cat-riid\/model.cbm') #load catboost\n\n","2c25ae70":"print('LGB ROC-AUC score: ', roc_auc_score(y_test.values, model.predict_proba(test_df)[:, 1]))","4857de39":"print('LGB ROC-AUC score: ', roc_auc_score(y_test.values, lgb_model.predict_proba(test_df)[:, 1]))","784a4db8":"print('LGB ROC-AUC score: ', roc_auc_score(y_test.values, model.predict_proba(test_df)[:, 1] * 0.8 + lgb_model.predict_proba(test_df)[:, 1] * 0.2))\n#  model.predict_proba(test_df)[:, 1] * 0.8 + lgb_model.predict_proba(test_df)[:, 1] * 0.2","d286d811":"eli5.show_weights(model, top=20)","9fa3d91d":"weights = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\nroc_auc = []\nfor i in weights:\n    roc_auc.append(roc_auc_score(y_test.values, model.predict_proba(test_df)[:, 1] * i + lgb_model.predict_proba(test_df)[:, 1] * (1-i)))\n    \n    ","9a4b4031":"sns.lineplot(x = weights, y = roc_auc  )","1668b2a8":"env = riiideducation.make_env()\n","9d8c8ecf":"iter_test = env.iter_test()","a1dc1fd0":"for (test_df, sample_prediction_df) in iter_test:\n    # new features\n    test_df['timestamp_diff'] = test_df['timestamp'].diff()\n    test_df['timestamp_diff'].fillna(0, inplace=True)\n    test_df.loc[test_df['timestamp_diff'] < 0, 'timestamp_diff'] = np.median(test_df['timestamp_diff'])\n    \n    # merge\n    test_df = test_df.merge(user_characteristics, on = \"user_id\", how = \"left\")\n    test_df = test_df.merge(task_container_characteristics, on = \"task_container_id\", how = \"left\")\n    test_df = test_df.merge(content_characteristics, on = \"content_id\", how = \"left\")\n    \n    # type transformation\n    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(value=False).astype(bool)\n    test_df.fillna(value = 0.5, inplace = True)\n    test_df = test_df.replace([np.inf, -np.inf], np.nan)\n    test_df = test_df.fillna(0.5)\n    \n    # preds\n    test_df['answered_correctly'] = model.predict_proba(test_df[features])[:, 1]* 0.9 + lgb_model.predict_proba(test_df[features])[:, 1] * 0.1\n    cols_to_submission = ['row_id', 'answered_correctly', 'group_num']\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","2ef2a18b":"0.9 seems to be the best ","41e8fe36":"to many useless features work on this","01e73a6d":"Drop features that we are not going to use in our model","0907cc0e":"The notebook can be treated as a infrence notebook i have commented the training part here if you can find the model here - https:\/\/www.kaggle.com\/ash1706\/model-cat-riid.\n\nThe aim is to use LGBM and catboost and then use thier wieghted average to make the predictions.\n\nI just started with the competetion will work on this as i get more ideas.\n\n## please leave a upvote !!!!!!!  \ud83d\ude01\ud83d\ude4f","23a362fc":"Droping **row_id** and **timestamp** ","6c71f330":">**row_id**: (int64) ID code for the row.\n\n>**timestamp**: (int64) the time in milliseconds between this user interaction and the first event completion from that user.\n\n>**user_id**: (int32) ID code for the user.\n\n>**content_id**: (int16) ID code for the user interaction\n\n>**content_type_id**: (int8) 0 if the event was a question being posed to the user, 1 if the event was the user watching a lecture.\n\n>**task_container_id**: (int16) Id code for the batch of questions or lectures. For example, a user might see three questions in a row before seeing the explanations for any of them. Those three would all share a task_container_id.\n\n>**user_answer**: (int8) the user's answer to the question, if any. Read -1 as null, for lectures.\n\n>**answered_correctly**: (int8) if the user responded correctly. Read -1 as null, for lectures.\n\n>**prior_question_elapsed_time**: (float32) The average time in milliseconds it took a user to answer each question in the previous question bundle, ignoring any lectures in between. Is null for a user's first question bundle or lecture. Note that the time is the average time a user took to solve each question in the previous bundle.\n\n>**prior_question_had_explanation**: (bool) Whether or not the user saw an explanation and the correct response(s) after answering the previous question bundle, ignoring any lectures in between. The value is shared across a single question bundle, and is null for a user's first question bundle or lecture. Typically the first several questions a user sees were part of an onboarding diagnostic test where they did not get any feedback.\n","fc235d42":"<a id=\"3\"><\/a>\n\n<div style=\"background:write; border:0; color:black; width: 100%; height: 50px\">\n    <div style=\"vertical-align: middle; text-align:center\"><h1>MODELING<\/h1><\/div>\n<\/div>","d36f3a6a":"Metadata for the lectures watched by users as they progress in their education.\n\n>**lecture_id**: foreign key for the train\/test content_id column, when the content type is lecture (1).\n\n>**part**: top level category code for the lecture.\n\n>**tag**: one tag codes for the lecture. The meaning of the tags will not be provided, but these codes are sufficient for clustering the lectures together.\n\n>**type_of**: brief description of the core purpose of the lecture","d2d1985c":"Let's look at the missing data","713c7679":"# weight optimization","04bad19d":"I am using the features generated by - https:\/\/www.kaggle.com\/ldevyataykina\/riiid-exploratory-data-analysis-baseline <br>\nand thanks to this kernel for helping in loading the huge dataset it was a pain - https:\/\/www.kaggle.com\/rohanrao\/tutorial-on-reading-large-datasets\/","e47a950d":"Merge all of our data","32b8340f":"### TRAIN","7f6fb10e":"The difference between **timestamp** is not equal to **prior_question_elapsed_time**. Therefore we need to create **timestamp** difference and check it importance","00ab4880":"Let's look where is the most part of the incorrect answers","4f41e28e":"### QUESTIONS.CSV","fc8ffe44":"# Riiid! Answer Correctness Prediction","78e690e8":"Let's check the parts distribution","7f5eb0c9":"<a id=\"2\"><\/a>\n\n<div style=\"background:write; border:0; color:black; width: 100%; height: 50px\">\n    <div style=\"vertical-align: middle; text-align:center\"><h1>FEATURE ENGINEERING<\/h1><\/div>\n<\/div>","555f0f1d":"<a id=\"4\"><\/a>\n\n<div style=\"background:write; border:0; color:black; width: 100%; height: 50px\">\n    <div style=\"vertical-align: middle; text-align:center\"><h1>SUBMISSION PREPARATION<\/h1><\/div>\n<\/div>","6b81ae4b":"### LECTURES.CSV","0f518a4e":"We saw earlier some dependencies between **answered_correctly** and the frequency of **task_container_id**. Therefore I want to add some features for the **task_container_id**","c49049b3":"# to do\n1. Throw SAKT in the mix seems like the way to go\n2. Work on feature eginnering","c9ad4dcf":"Change the -1 values to 0 for lectures in **user_answer** and **answered_correctly** as we saw in the columns' description","b1dec976":"#### PATHS","14acd877":"It was the best params. Therefore I will use them","1dea4ff9":"The models are already trained and present in - https:\/\/www.kaggle.com\/ash1706\/model-cat-riid you can add it and play around . the models are basleline though nothing fancy.\nThe params are given in below cells in case you want to train it again","e8aaedfd":"### EXAMPLE-TEST.CSV","9985a93a":"Thanks for the read !!!!<br>\nplease do leave an upvote!!!!!","bb1697fc":"# CV","74869343":">**question_id**: foreign key for the train\/test content_id column, when the content type is question (0).\n\n>**bundle_id**: code for which questions are served together.\n\n>**correct_answer**: the answer to the question. Can be compared with the train user_answer column to check if the user was right.\n\n>**part**: top level category code for the question.\n\n>**tags**: one or more detailed tag codes for the question. The meaning of the tags will not be provided, but these codes are sufficient for clustering the questions together."}}