{"cell_type":{"3b6809a8":"code","b9dbad73":"code","0ac3f057":"code","655c2080":"code","6bb4c65d":"code","843d6cff":"code","ffa5fbad":"code","01d8cb30":"code","59826d81":"code","2d0524b7":"code","393f380d":"code","ed01edcc":"code","22485088":"code","19fbdc5c":"code","f3c27da4":"code","3400d3b2":"code","e07f5e1c":"code","fe1bf0fa":"code","16dd843b":"code","954ed1ab":"code","298ebc64":"code","125220e0":"code","4e8c277c":"code","2ed9060d":"code","10197880":"code","dfd6c5b8":"code","e6713af4":"code","c3a25850":"code","5b7309d4":"code","524fa6b3":"markdown","4a0074ec":"markdown"},"source":{"3b6809a8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b9dbad73":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n\nimport torch\nimport random \nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split","0ac3f057":"df = pd.read_csv(\"\/kaggle\/input\/mushroom-classification\/mushrooms.csv\")\ndf_2 = df.copy()","655c2080":"df.head()","6bb4c65d":"ax = df['class'].value_counts().plot.bar(color= ['darkblue', 'lightblue']);\n\nplt.title('Quantity of each class in the dataset', fontsize = 15)\nplt.xticks(rotation=0)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nfor i in ax.patches:\n    ax.text(i.get_x() + 0.12, i.get_height()-300, i.get_height(), fontsize=16,\n                color='white')\n    \nplt.show()","843d6cff":"df.apply(lambda x: x.nunique())","ffa5fbad":"df = df.drop(['veil-type'], axis=1)\ny = df['class']\nX = df.drop(['class'], axis=1)","01d8cb30":"y = y.apply(lambda x : 1 if x == 'e' else 0)","59826d81":"l_t = LabelEncoder()\nX_label_encoding = X.apply(lambda x : l_t.fit_transform(x))","2d0524b7":"X_label_encoding.head()","393f380d":"X_label_encoding.shape","ed01edcc":"torch.cuda.is_available()\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice","22485088":"# torch.cuda.get_device_name(0)","19fbdc5c":"X_train, X_test,y_train, y_test = train_test_split(X_label_encoding, y, test_size = 0.2, shuffle= True)","f3c27da4":"X_train = torch.FloatTensor(X_train.values)\nX_test = torch.FloatTensor(X_test.values)\ny_train = torch.FloatTensor(y_train.values)\ny_test = torch.FloatTensor(y_test.values)","3400d3b2":"class NeuralNetwork(nn.Module):\n    def __init__(self, first_neurons, n_hidden_neurons):\n        super().__init__()\n        self.fc1 = nn.Linear(first_neurons, n_hidden_neurons)\n        self.activ1 = nn.ReLU()\n        self.fc2 = nn.Linear(n_hidden_neurons, n_hidden_neurons)\n        self.activ2 = nn.ReLU()\n        self.fc3 = nn.Linear(n_hidden_neurons, 2)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.activ1(x)\n      \n        x = self.fc2(x)\n        x = self.activ2(x)\n        \n        x = self.fc3(x)\n        return x ","e07f5e1c":"myNet = NeuralNetwork(21, 50)\nmyNet = myNet.to(device)\nmyNet.parameters()\n# list(myNet.parameters())\nloss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(myNet.parameters(),  lr = 10 **(-4))","fe1bf0fa":"X_test = X_test.to(device)\ny_test = y_test.to(device)","16dd843b":"acc = []\nlosss = []\ndef training(model, batch_size, epochs, loss, optimizer):\n    for epoch in range(epochs):\n        order = np.random.permutation(len(X_train))\n        for start_ind in range(0, len(X_train), batch_size):\n            optimizer.zero_grad()\n            batch_indexes = order[start_ind:start_ind + batch_size]\n            X_batch = X_train[batch_indexes].to(device)\n            y_batch = y_train[batch_indexes].to(device)\n\n            preds = model.forward(X_batch)\n            loss_value = loss(preds, y_batch.long())\n            loss_value.backward()\n            optimizer.step()\n\n        test_preds = model.forward(X_test)\n        losss.append(loss(test_preds, y_test.long()).to('cpu').item())\n\n        accuracy = (test_preds.argmax(dim=1) == y_test).float().mean().to('cpu')\n        acc.append(accuracy)\n        if (epoch + 1) % 5 == 0:\n            print(f'epoch: {epoch + 1}, acc:{accuracy}')","954ed1ab":"epoch = 100\nbatch_size = 200\ntraining(myNet, batch_size, epoch, loss, optimizer)","298ebc64":"x = np.arange(1,101)\ny_v = np.array(acc) * 100\nmax_value = y_v.max().reshape(-1)\ny_values = np.array(acc) * 100\ny_ticks =   np.concatenate((np.arange(y_v[1:].min(), y_v.max(), 5), max_value),axis = 0)\nplt.plot(x, y_v,  color ='darkblue', linewidth=4)\nplt.yticks(y_ticks)\nplt.grid(axis='y', linestyle='-')\nplt.grid(axis='x', linestyle='-')\nplt.title('Accuracy', fontsize = 15 );\nplt.xlabel('epoch', fontsize=12);\nplt.show();","125220e0":"X_one_hot_encoding  =  pd.get_dummies(X)","4e8c277c":"X_one_hot_encoding.shape","2ed9060d":"X_train, X_test,y_train, y_test = train_test_split(X_one_hot_encoding, y, test_size = 0.2, shuffle= True)\nX_train = torch.FloatTensor(X_train.values)\nX_test = torch.FloatTensor(X_test.values)\ny_train = torch.FloatTensor(y_train.values)\ny_test = torch.FloatTensor(y_test.values)","10197880":"myNet_OHE = NeuralNetwork(116, 50)\nmyNet_OHE = myNet_OHE.to(device)","dfd6c5b8":"# list(myNet_OHE.parameters())","e6713af4":"X_test = X_test.to(device)\ny_test = y_test.to(device)\nacc = []","c3a25850":"epoch = 100\nbatch_size = 200\nloss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(myNet_OHE.parameters(),  lr = 10 **(-4))\ntraining(myNet_OHE, batch_size, epoch, loss, optimizer)","5b7309d4":"x = np.arange(1,101)\ny_label_encoding = y_v\ny_one_hot_encoding = np.array(acc) * 100\nmax_value_1 = y_label_encoding.max().reshape(-1)\nmax_value_2 = y_one_hot_encoding.max().reshape(-1)\ny_values = np.array(acc) * 100\ny_ticks =   np.concatenate((np.arange(y_v[1:].min(), y_v.max(), 5), max_value_1,\n                            max_value_2,),axis = 0)\nplt.plot(x, y_label_encoding,  color ='darkblue', linewidth=3, label ='LabelEncoding')\nplt.plot(x, y_one_hot_encoding,  color ='red', linewidth=3, label = 'OneHotEncoding')\nplt.yticks(y_ticks)\nplt.grid(axis='y', linestyle='-')\nplt.grid(axis='x', linestyle='-')\nplt.title('Accuracy', fontsize = 15 );\nplt.xlabel('epoch', fontsize=12);\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\nplt.show();","524fa6b3":"# EXPIRIMENT 1. LABEL ENCODING","4a0074ec":"# EXPIRIMENT 2. ONE HOT ENCODING"}}