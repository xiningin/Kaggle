{"cell_type":{"5a6e72d1":"code","ff6d8e06":"code","e852d26c":"code","e6fdff69":"code","b33d02a6":"code","e8560fbe":"code","7f76d21a":"code","2672dea2":"code","25aee6b1":"code","cd623838":"code","1230477b":"code","2d6b60e9":"code","53a478da":"code","48543e47":"code","7eb7709b":"code","109e53ad":"code","32e813be":"code","25095c8d":"code","c468e86c":"code","eca727f8":"code","b48383bf":"code","25fb98f4":"code","4dee1c58":"code","b186570b":"markdown","9baa3290":"markdown","6c1d471e":"markdown","1233be6f":"markdown","25a2ea4d":"markdown","6301b806":"markdown","e17c7e7d":"markdown","669c5bc7":"markdown","b79fcdb2":"markdown","5cb2b82d":"markdown","0348f1d6":"markdown","f114534e":"markdown","c9c996d8":"markdown","38252b47":"markdown","e3a0fe32":"markdown","3dd9c320":"markdown","716c747b":"markdown","ac264ea5":"markdown","d4f0d505":"markdown","b6e92c92":"markdown","c99e719f":"markdown","10e3bd0b":"markdown","941b98bd":"markdown","b38606e0":"markdown","e8e003c4":"markdown","f867048c":"markdown","fe6b2765":"markdown","e824b681":"markdown","afdbfa9c":"markdown","a99eb95a":"markdown"},"source":{"5a6e72d1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ff6d8e06":"df = pd.read_csv('..\/input\/internet-articles-data-with-users-engagement\/articles_data.csv')\ndf = df.drop('Unnamed: 0', axis=1)\n\ndf['title'] = df['title'].fillna('NaN')\ndf['description'] = df['description'].fillna('NaN')\ndf['content'] = df['content'].fillna('NaN')\ndf['published_at'] = df['published_at'].fillna('NaN')\n\ndf['engagement_reaction_count'] = df['engagement_reaction_count'].fillna(0)\ndf['engagement_comment_count'] = df['engagement_comment_count'].fillna(0)\ndf['engagement_share_count'] = df['engagement_share_count'].fillna(0)\ndf['engagement_comment_plugin_count'] = df['engagement_comment_plugin_count'].fillna(0)","e852d26c":"df.head()","e6fdff69":"def bar_charts(title, x, y, colour, values, keys, figsize=(10, 5), fontsize=12):\n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n    bars = plt.bar(keys, values, color=colour)\n\n    for bar in bars:\n        label = list(count)[list(bars).index(bar)]\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()\/2, height, label, ha='center', va='bottom', \n                 fontsize=fontsize)\n\n    plt.title(title, fontsize=fontsize)\n    plt.xlabel(x, fontsize=fontsize)\n    plt.ylabel(y, fontsize=fontsize)\n    plt.show()","b33d02a6":"def plots(title, x, y, values, keys, figsize=(10, 5), fontsize=12):\n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n    plt.plot(list(keys), list(values))\n    plt.title(title)\n    plt.xlabel(x)\n    plt.ylabel(y)\n    plt.show()","e8560fbe":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 20))\n\nfor ax in [[ax1, 'title'], [ax2, 'description'], [ax3, 'content']]:\n    wordcloud = WordCloud(background_color='white').generate(' '.join(df[ax[1]]))\n    ax[0].set_title(ax[1], fontsize=20)\n    ax[0].imshow(wordcloud)\n    ax[0].axis('off')\n    \nplt.show()","7f76d21a":"count = Counter(df['source_name'])\ncount = pd.Series(count).sort_values(ascending=False)\n\nkeys = list(count.keys())\nkeys[keys.index('The New York Times')] =  'The NY Times'\nkeys[keys.index('Al Jazeera English')] = 'Al Jazeera'\nkeys[keys.index('The Wall Street Journal')] =  'Wall Street Journal'\n\nbar_charts('Articles per source', 'Source name', 'Number of articles', 'blue', count, keys,\n          (20, 13), 18)","2672dea2":"month = [i[5:7] for i in df['published_at']]\ncount = Counter(month)\ncount = pd.Series(count).sort_values(ascending=False)[:2]\n\nbar_charts('Distribution of articles released per month', 'Month number', 'Number of articles',\n          'orange', count, count.keys(), figsize=(15, 10))","25aee6b1":"day = [i[8:10] for i in df['published_at']]\ncount = Counter(day)\ncount = pd.Series(count).sort_values(ascending=False)[:13]\n\nbar_charts('Day that articles were released', 'Day released', 'Number of articles', 'purple', \n           count, count.keys(), figsize=(15, 10))","cd623838":"hour = [i[11:13] for i in df['published_at']]\ncount = Counter(hour)\ncount = pd.Series(count).sort_values(ascending=False)[:20]\n\nbar_charts('Hours that articles were released', 'Hour released', 'Number of articles',\n          'green', count, count.keys(), figsize=(13, 10))","1230477b":"day_and_month = pd.DataFrame([])\nday_and_month['day'] = day\nday_and_month['month'] = month\n\ncount1 = Counter(day_and_month[day_and_month['month']=='09']['day'])\ncount2 = Counter(day_and_month[day_and_month['month']=='10']['day'])\n\nkeys = pd.concat([pd.Series(count1.keys()), pd.Series(count2.keys())[:2]])\nvalues = pd.concat([pd.Series(count1.values()), pd.Series(count2.values())[:2]])\ncount = dict(zip(keys, values))\n\nplots('Articles released over the days', 'Days', 'Number of articles', count.values(),\n      count.keys(), (13, 8))","2d6b60e9":"plots('Engagement reaction count over time', 'Time', 'Engagement reaction count', \n      df['engagement_reaction_count'], df['engagement_reaction_count'].keys(), (13, 8))","53a478da":"plots('Engagement comment count over time', 'Time', 'Engagement comment count', \n      df['engagement_comment_count'], df['engagement_comment_count'].keys(), (13, 8))","48543e47":"plots('Engagement share count over time', 'Time', 'Engagement share count', \n      df['engagement_share_count'], df['engagement_share_count'].keys(), (13, 8))","7eb7709b":"plots('Engagement comment plugin count over time', 'Time', 'Engagement comment plugin count', \n      df['engagement_comment_plugin_count'], df['engagement_comment_plugin_count'].keys(), \n      (13, 8))","109e53ad":"sns.heatmap(df.corr(), annot=True)\nplt.title('Correlation of variables')\nplt.show()","32e813be":"df['engagement_reaction_count'] = df['engagement_reaction_count'][df['engagement_reaction_count']<100000]\ndf['engagement_share_count'] = df['engagement_share_count'][df['engagement_share_count']<20000]","25095c8d":"fontsize=15\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 7))\n\nfor ax in [[ax1, ['engagement_reaction_count', 'engagement_comment_count']],\n           [ax2, ['engagement_reaction_count', 'engagement_share_count']],\n           [ax3, ['engagement_share_count', 'engagement_comment_count']]]:\n    sns.regplot(data=df, x=ax[1][0], y=ax[1][1], ax=ax[0])\n    ax[0].set_xlabel(ax[1][0], fontsize=fontsize)\n    ax[0].set_ylabel(ax[1][1], fontsize=fontsize)\n\nax2.set_title('Correlation of variables', fontsize=30, pad=30)\nplt.show()","c468e86c":"X = df['content']\ny = df['source_name']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","eca727f8":"cv = CountVectorizer()\ntfidf = TfidfTransformer()\n\nX_train = cv.fit_transform(X_train)\nX_test = cv.transform(X_test)\n\nX_train = tfidf.fit_transform(X_train)\nX_test = tfidf.transform(X_test)","b48383bf":"svd = TruncatedSVD(n_components=2000)\nX_train = svd.fit_transform(X_train)\nX_test = svd.transform(X_test)","25fb98f4":"classifiers = [['SGD', SGDClassifier()], ['Random Forest', RandomForestClassifier()],\n              ['Linear SVC', LinearSVC()]]\nscores = []\ncross_vals = []\n\nfor classifier in classifiers:\n    model = classifier[1]\n    model.fit(X_train, y_train)\n\n    score = model.score(X_test, y_test)\n    cross_val = cross_val_score(model, X_test, y_test).mean()\n    scores.append(score)\n    cross_vals.append(cross_val)\n    \n    print(classifier[0])\n    print(score)\n    print(cross_val)\n    if model != classifiers[-1][1]:\n        print('')","4dee1c58":"names = ['SGD', 'Random Forest', 'Linear SVC']\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 9))\n\nfor ax in [[ax1, scores, 'model score'], [ax2, cross_vals, 'cross validation score']]:\n    metric = ax[1]\n    bars = ax[0].bar(names, metric, color='blue')\n    for bar in bars:\n        label = str(metric[list(bars).index(bar)])[:4]\n        height = bar.get_height()\n        ax[0].text(bar.get_x() + bar.get_width()\/2, height, label, ha='center', va='bottom')\n    ax[0].set_title(ax[2])\n    ax[0].set_xlabel('model')\n    ax[0].set_ylabel('accuracy')\n\nplt.show()","b186570b":"Now we will use a bar chart to take a look into which hours are the most popular for releasing a news piece. It seems that around 2-4 pm is the most regular time that people publish their content.","9baa3290":"Now we will reduce the unwanted parts of our data using a TruncatedSVD model, which is basically a PCA for text.","6c1d471e":"We must first convert our data from textual to numerical format in order for us to input it into a predictor. The way this is done is through a 'CountVectorizer' and then a 'TFIDF' model.","1233be6f":"We assign an 'X' variable to the 'content' feature and 'y' to our 'source_name' feature. They then go on to be further split into train and test sets.","25a2ea4d":"# Predicting the data","6301b806":"Lastly, we will scatter the datapoints and create a line of best fit to have a closer look at how the different variables correlate to each other.","e17c7e7d":"## Splitting our dataset","669c5bc7":"The first variable that we will display is the 'source_name' column which describes the originators of the article. As seen below, the 'Reuters', 'BBC News' and 'Irish Times' have written the most articles.","b79fcdb2":"## Using TruncatedSVD","5cb2b82d":"# Preparing the data\nThe first step will be to fill the null values of the data and drop the 'Unnamed: 0' feature which is useless.","0348f1d6":"We remove some outliers in the columns because they could skew our results.","f114534e":"Subsequently, we now show the distribution of how many articles were written in September and October. The amount written in October was less than a third of that written in September.","c9c996d8":"The following cell is a procedure which plots out a bar chart that can tell us the distribution of the different variables.","38252b47":"The following EDA is how many pieces were written per day, only looking at those in September. The most that was written on one day was on the third of September.","e3a0fe32":"The next four line graphs are about the engagement of the reaction, comment, share and comment-plugin.","3dd9c320":"Finally, we use bar charts to visualise how well each classifier has performed in relation to model score and cross val score. We can see that the best predictor for this data is the Linear SVC, followed by SGD Classifier and then the Random Forest.","716c747b":"There are many spikes in these visualisations, which show that there is no steady order of the variables and that these increases in the data are seemingly unexpected.","ac264ea5":"## Using NLP","d4f0d505":"Next, we use a heatmap to check out whether there are correlations between any of the variables. We see that there are three sets of features that do have connections, which means that they have a dependency on each other.","b6e92c92":"## Creating and evaluating classifiers","c99e719f":"# Internet News Prediction & EDA\nWelcome to today's notebook, where we will be visualising and predicting dataset which includes different newspaper articles and their details.","10e3bd0b":"Afterwards, another procedure is defined, though this time in the form of a line graph.","941b98bd":"A very useful technique for visualisation is the WordCloud, which shows what words are the most frequently occurring.","b38606e0":"## Line graphs","e8e003c4":"### Thank you for reading my notebook.\n### If you enjoyed this notebook and found it helpful, please upvote it and give feedback as it will help me make more of these.","f867048c":"# Visualising the data\nNext, we will perform EDA on our features.","fe6b2765":"Furthermore, we switch our attention to line graphs, where we will look at a how many articles were released.","e824b681":"## Bar charts","afdbfa9c":"Next, will train three different classifiers: 'SGD', 'Random Forest' and 'Linear SVC' and then evaluate their performance.","a99eb95a":"## Correlation"}}