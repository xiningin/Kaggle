{"cell_type":{"545c479d":"code","56301dc7":"code","57b54171":"code","9effb86e":"code","e1801a29":"code","ac4e222e":"code","d9124947":"code","67ad3e84":"code","2e79886a":"code","5cb0a053":"code","1ae96536":"code","9b585fda":"code","a99f2884":"code","8209c7a9":"code","c2c918c2":"code","05c64890":"code","2faa0b22":"code","b0171b60":"code","f8f7cc65":"code","f025d87f":"code","3a0af019":"code","5301ef53":"code","7e7e7c2c":"code","498e122b":"code","be758bad":"code","bf3c5898":"code","f4e38abd":"code","e7ecfa0c":"code","f60400b6":"code","972c45ae":"code","5ecbf76e":"code","37403b43":"code","cd899d9d":"code","80be5720":"code","eb42b222":"code","8dc76a4f":"code","86e19163":"code","1e35640d":"markdown","4d940fd6":"markdown","6696a144":"markdown","80e8a4ea":"markdown","dc1da932":"markdown","788bf5c8":"markdown","b1e6abde":"markdown","2a464153":"markdown","ae5f2473":"markdown","5a8b5213":"markdown","e0512df7":"markdown","ed9338ed":"markdown","ff021917":"markdown","ed02cc72":"markdown","25b5686c":"markdown","6c5ff40d":"markdown","3548b990":"markdown","556dfd01":"markdown","a9076c63":"markdown","d0ff2618":"markdown","94cd1525":"markdown","450f6746":"markdown","78628617":"markdown","7e5cafdc":"markdown","3b84e9ba":"markdown","8c40e12b":"markdown","884847e1":"markdown","65a32d6e":"markdown","a5133d0e":"markdown","f8623f3a":"markdown","33e7f8ba":"markdown","41f33b05":"markdown","e352e2dc":"markdown","d4bec61e":"markdown","d8d1bd53":"markdown"},"source":{"545c479d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","56301dc7":"#used libraries\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import pos_tag\nfrom textblob import TextBlob\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n#making sure plots work\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)","57b54171":"import os\nimport glob\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\none = pd.read_csv('\/kaggle\/input\/articles1.csv')\ntwo = pd.read_csv('\/kaggle\/input\/articles2.csv')\nthree = pd.read_csv('\/kaggle\/input\/articles3.csv')\n\n\n#reading data\ndf = one.append([two, three])\n\n\n    \nprint(df)    ","9effb86e":"from sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport re\nimport pandas as pd\n\ndf_nyp = df[df['publication'] == 'New York Post']\ndf_nyp = df_nyp.sample(frac=0.50, random_state=42)","e1801a29":"\ncorpus = []\ndf_nyp['content'].apply(lambda x: corpus.append(x))\n\nstop_words = stopwords.words(\"english\")\nadditional_sw = ['say', 'said', 'one', 'two']\nstop_words += additional_sw\n\ndef tokenize(content):\n    letters_only = re.sub(\"[^a-zA-Z]\",\n                      \" \",         \n                      content )\n    lower_case = letters_only.lower()\n    tokens = word_tokenize(lower_case)\n    words = [w for w in tokens if not w in stop_words]\n    stems = [stemmer.lemmatize(word) for word in words]\n    return(stems)\n\n#create dictionary and incorporating 1 to n-gram \nstemmer = WordNetLemmatizer()\nvectorizer = CountVectorizer(ngram_range = (1,2),\n                             lowercase = True,\n                             tokenizer=tokenize,\n                             preprocessor = None,\n                             max_features=5000)\n\n#bag of words\nbow = vectorizer.fit_transform(corpus)\nvectorizer.vocabulary_\n\n#term document matrix\ntdm = pd.DataFrame(bow.toarray(), columns=vectorizer.get_feature_names())","ac4e222e":"df_nyp['word_count'] = df_nyp['content'].apply(lambda x : len(x.split()))\ndf_nyp['char_count'] = df_nyp['content'].apply(lambda x : len(x.replace(\" \",\"\")))\n\ndf_nyp.describe()\n\ndf_nyp.hist(column='word_count', bins=100)\ndf_nyp.hist(column='char_count', bins=100)\n\nprint('number of articles from the New York Post in set: ', df_nyp.shape[0])","d9124947":"df_content = df_nyp.sample(frac=0.03, random_state=42)","67ad3e84":"#sentiment\ndf_content['polarity'] = df_content['content'].map(lambda text: TextBlob(text).sentiment.polarity)\n\ndf_content['polarity'].iplot(\n    kind='hist',\n    bins=60,\n    xTitle='polarity',\n    linecolor='black',\n    yTitle='count',\n    title='Sentiment Polarity Distribution')","2e79886a":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef get_top_n_words(corpus, n=None):\n    vector = CountVectorizer(tokenizer=tokenize).fit(corpus)\n    bag_of_words = vector.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vector.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_words(df_content['content'], 20)\ndata_content = pd.DataFrame(common_words, columns = ['text' , 'count'])\ndata_content.groupby('text').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar',orientation='h', yTitle='Count', linecolor='black', title='Unigram - top 20 words in comments')","5cb0a053":"def get_top_n_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2), tokenizer=tokenize).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_bigram(df_content['content'], 20)\ndata_content = pd.DataFrame(common_words, columns = ['text' , 'count'])\ndata_content.groupby('text').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar',orientation='h', xTitle='Count', linecolor='black', title='Bigram - top 20 bigrams in comments')","1ae96536":"def get_top_n_trigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3), tokenizer=tokenize).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_trigram(df_content['content'], 20)\ndata_comment = pd.DataFrame(common_words, columns = ['text' , 'count'])\ndata_comment.groupby('text').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar',orientation='h', yTitle='Count', linecolor='black', title='Trigram - top 20 trigrams in comments')","9b585fda":"from sklearn.decomposition import LatentDirichletAllocation\n\nnb_topics = 15\nlda = LatentDirichletAllocation(n_components=nb_topics, max_iter=20,\n                                learning_method='batch')\ndocument_topics = lda.fit_transform(bow)\nsorting = np.argsort(lda.components_, axis=1)[:, ::-1]\nfeature_names = np.array(vectorizer.get_feature_names())\n\ndef print_topics(topics, feature_names, sorting, topics_per_chunk=6,\n                 n_words=20):\n    for i in range(0, len(topics), topics_per_chunk):\n        these_topics = topics[i: i + topics_per_chunk]\n        len_this_chunk = len(these_topics)\n        words = []\n        for i in range(n_words):\n            try:\n                words.append(feature_names[sorting[these_topics, i]])\n            except:\n                pass\n\n    #setting up word dictionary for comparison\n    word_dict = {}\n    for i in topics:\n        word_dict.update({i : [word[i] for word in words]})\n    \n    return word_dict\n\nlda_topics = print_topics(topics=range(nb_topics), feature_names=feature_names, sorting=sorting, topics_per_chunk=nb_topics, n_words=15)\n\nfor i in range(nb_topics):\n    print(i,': ' ,lda_topics[i], '\\n')","a99f2884":"from sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import Normalizer\n\nsvd = TruncatedSVD(n_components=nb_topics)\nlsa = svd.fit_transform(bow)\n\nnormalizer = Normalizer()\nlsa_norm = normalizer.fit_transform(lsa)\n\ndef print_top_words(model, feature_names, n_top_words):\n    word_dict = {}\n    for topic_idx, topic in enumerate(model.components_):\n        message = \"Topic #%d: \" % topic_idx\n        message += \" \".join([feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n        words = [feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]]\n\n        word_dict.update({topic_idx : words})\n#         print(message)\n    return word_dict\n    \nn_top_words = 15\nfeature_names = vectorizer.get_feature_names()\nsvd_topics = print_top_words(svd, feature_names, n_top_words)\n\nfor i in range(nb_topics):\n    print(i,': ' ,svd_topics[i], '\\n')","8209c7a9":"from sklearn.decomposition import NMF\n\nnmf = NMF(n_components=nb_topics)\nnmf_data = nmf.fit_transform(bow)\n\nnormalizer = Normalizer()\nnmf_norm = normalizer.fit_transform(nmf_data)\n\nn_top_words = 15\nfeature_names = vectorizer.get_feature_names()\nnmf_topics = print_top_words(nmf, feature_names, n_top_words)\n\nfor i in range(nb_topics):\n    print(i,': ' ,nmf_topics[i], '\\n')","c2c918c2":"def get_jaccard_sim(str1, str2):\n    #get similairity between two topics (number of words present in both sets)\n    a = set(str1)\n    b = set(str2)\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\n# sim_matrix = []\n# for lda_topic in range(len(lda_topics)):\n#     sim_matrix.append([get_jaccard_sim(lda_topics[lda_topic], svd_topics[svd_topic]) for svd_topic in range(len(svd_topics))])","05c64890":"#the different number of topics\nk_list = [3, 10, 25, 50]\ndict_k = {}\n\nfor k in k_list:\n    #SVD\n    svd = TruncatedSVD(n_components=k)\n    lsa = svd.fit_transform(bow)\n\n    normalizer = Normalizer()\n    lsa_norm = normalizer.fit_transform(lsa)\n    \n    #LDA\n    lda = LatentDirichletAllocation(n_components=k, max_iter=20,\n                                learning_method='batch')\n    document_topics = lda.fit_transform(bow)\n    \n    #get data about topics\n    lda_topics = print_topics(topics=range(k), \n                              feature_names=np.array(vectorizer.get_feature_names()), \n                              sorting=np.argsort(lda.components_, axis=1)[:, ::-1], \n                              topics_per_chunk=k, \n                              n_words=15)\n    svd_topics = print_top_words(svd, vectorizer.get_feature_names(), 15)\n    \n    dict_k.update({k:[lsa_norm, document_topics]})\n    \n    #plot jaccard similairities matrix\n    sim_matrix = []\n    for lda_topic in range(len(lda_topics)):\n        sim_matrix.append([get_jaccard_sim(lda_topics[lda_topic], \n                                           svd_topics[svd_topic]) for svd_topic in range(len(svd_topics))])\n\n    plt.imshow(sim_matrix)\n    plt.xlabel('LDA')\n    plt.ylabel('SVD')\n    plt.colorbar()\n    plt.show()\n    ","2faa0b22":"#select k\nnr_topics = 15 #k\n\n#select topic from LDA\nlda = LatentDirichletAllocation(n_components=nr_topics, max_iter=20,\n                                learning_method='batch')\ndocument_topics = lda.fit_transform(bow)\n\n#find 10 articles with highest topic saturation\nd = {}\nfor i in range(document_topics.shape[1]):\n    d.update({i:[]})\n\nfor doc in document_topics:\n    for i in range(len(doc)):\n        d[i].append(doc[i])\n    \ndf_text = pd.DataFrame(data=d)\ndf_text['text'] = corpus\n\nsorting = np.argsort(lda.components_, axis=1)[:, ::-1]\nfeature_names = np.array(vectorizer.get_feature_names())\ntopics_study = print_topics(topics=range(nr_topics), feature_names=feature_names, sorting=sorting, topics_per_chunk=nr_topics, n_words=10)\n\ntopic_nr = 13\ntopic_print = 0\n\nfor i in range(nr_topics):\n    if 'yankee' in topics_study[i]:\n        topic_print = i\n\ndf_text = df_text.sort_values(by=[topic_print], ascending=False)\ndf_text[:10]['text'].apply(lambda x: print(x, '\\n'))\n\nprint(topics_study[topic_print])","b0171b60":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\n#select another publication (opposing the previes chosen publication)\ndf_cnn = df[df['publication'] == 'CNN']\n# df_cnn = df_cnn.sample(frac=1)\n\ndf_nyp = df[df['publication'] == 'New York Post']\ndf_nyp = df_nyp.sample(frac=0.65, random_state=42)\n\ndf_cnn['label'] = 1\ndf_nyp['label'] = 0\n\ndf_total = pd.concat([df_nyp, df_cnn])\n\ndf_sample = df_total.sample(frac=0.50, random_state=42)\n\nX = df_sample['content']\ny = df_sample['label']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","f8f7cc65":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nimport mglearn\n\npipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression())\nparam_grid = {'logisticregression__C': [0.001, 0.01],\n              \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3)]}\ngrid = GridSearchCV(pipe, param_grid, cv=5, n_jobs=5, verbose=1)\ngrid.fit(X_train, y_train)\n\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid.best_params_))\n\n# extract scores from grid_search\nscores = grid.cv_results_['mean_test_score'].reshape(-1, 3).T\n\n# visualize heat map\nheatmap = mglearn.tools.heatmap(\n    scores, xlabel=\"C\", ylabel=\"ngram_range\", cmap=\"viridis\", fmt=\"%.3f\",\n    xticklabels=param_grid['logisticregression__C'],\n    yticklabels=param_grid['tfidfvectorizer__ngram_range'])\nplt.colorbar(heatmap)","f025d87f":"from sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.model_selection import GridSearchCV\n\n#GridSearchCV on LatentDirichletAllocation Parameters\n#grid search parameters has been optimized already\nsearch_params = {'max_iter': [15], \n                 'n_components': [15]}\n\nlda = LatentDirichletAllocation(\n    learning_method='batch', random_state=0)\n\nmodel = GridSearchCV(lda, param_grid=search_params)\nmodel.fit(bow)","3a0af019":"best_lda_model = model.best_estimator_\n\nprint(\"Best Model's Params: \", model.best_params_)\nprint(\"Best Log Likelihood Score: \", model.best_score_)\nprint(\"Model Perplexity: \", best_lda_model.perplexity(bow))","5301ef53":"from sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\n\nnb_iter = 15\nnb_topics = 15\nlda = LatentDirichletAllocation(n_components=nb_topics, max_iter=nb_iter,\n                                learning_method='batch', random_state=0)\n\nbow_train = vectorizer.fit_transform(X_train)\nbow_test = vectorizer.transform(X_test)\nX_train_lda = lda.fit_transform(bow_train)\nX_test_lda = lda.transform(bow_test)","7e7e7c2c":"pipe = make_pipeline(RandomForestClassifier(max_depth=10, random_state=0))\nparam_grid = {\n#                'randomforestclassifier__n_estimators': [30],\n               'randomforestclassifier__max_features': ['auto', 'sqrt'],\n                 'randomforestclassifier__max_depth': [5, 13, 20],\n               'randomforestclassifier__min_samples_split': [5, 12, 17],\n                'randomforestclassifier__min_samples_leaf': [1, 2, 5]\n}\n\ngrid = GridSearchCV(pipe, param_grid, cv=3)\ngrid.fit(X_train_lda, y_train)\n\nprint(classification_report(grid.predict(X_test_lda), y_test))","498e122b":"from sklearn.linear_model import LogisticRegression\n\npipe = make_pipeline(LogisticRegression(class_weight='balanced'))\nparam_grid = {'logisticregression__C': np.linspace(10, 1000, num=100)}\n\ngrid = GridSearchCV(pipe, param_grid, cv=5)\ngrid.fit(X_train_lda, y_train)\n\nprint(classification_report(grid.predict(X_test_lda), y_test))","be758bad":"from sklearn.svm import LinearSVC\n\npipe = make_pipeline(LinearSVC(loss='hinge', penalty='l2'))\nparam_grid = {'linearsvc__C': np.linspace(10, 1000, num=100)}\n\ngrid = GridSearchCV(pipe, param_grid, cv=3)\ngrid.fit(X_train_lda, y_train)\n\nprint(classification_report(grid.predict(X_test_lda), y_test))","bf3c5898":"from sklearn.naive_bayes import MultinomialNB\n\npipe = make_pipeline(MultinomialNB())\nparam_grid = {'multinomialnb__alpha': np.linspace(0.0001, 1000, num=100)}\n\ngrid = GridSearchCV(pipe, param_grid, cv=3)\ngrid.fit(X_train_lda, y_train)\n\nprint(classification_report(grid.predict(X_test_lda), y_test))","f4e38abd":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1, 2),\n                                   lowercase=True, tokenizer=tokenize,\n                                   preprocessor=None, max_features=10000)\n\nX_train_tf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tf = tfidf_vectorizer.transform(X_test)","e7ecfa0c":"pipe = make_pipeline(RandomForestClassifier(max_depth=10, random_state=0))\nparam_grid = {\n#                'randomforestclassifier__n_estimators': [30],\n               'randomforestclassifier__max_features': ['auto', 'sqrt'],\n                 'randomforestclassifier__max_depth': [5, 13, 20],\n               'randomforestclassifier__min_samples_split': [5, 12, 17],\n                'randomforestclassifier__min_samples_leaf': [1, 2, 5]\n}\n\ngrid = GridSearchCV(pipe, param_grid, cv=3)\ngrid.fit(X_train_tf, y_train)\n\nprint(classification_report(grid.predict(X_test_tf), y_test))","f60400b6":"pipe = make_pipeline(LogisticRegression(class_weight='balanced'))\nparam_grid = {'logisticregression__C': np.linspace(0.001, 1000, num=10)}\n\ngrid = GridSearchCV(pipe, param_grid, cv=5)\ngrid.fit(X_train_tf, y_train)\n\nprint(classification_report(grid.predict(X_test_tf), y_test))","972c45ae":"pipe = make_pipeline(LinearSVC(loss='hinge', penalty='l2'))\nparam_grid = {'linearsvc__C': np.linspace(10, 1000, num=100)}\n\ngrid = GridSearchCV(pipe, param_grid, cv=3)\ngrid.fit(X_train_tf, y_train)\n\nprint(classification_report(grid.predict(X_test_tf), y_test))","5ecbf76e":"pipe = make_pipeline(MultinomialNB())\nparam_grid = {'multinomialnb__alpha': np.linspace(0.0001, 1000, num=100)}\n\ngrid = GridSearchCV(pipe, param_grid, cv=3)\ngrid.fit(X_train_tf, y_train)\n\nprint(classification_report(grid.predict(X_test_tf), y_test))","37403b43":"import tensorflow as tf\nimport json\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.metrics import accuracy_score, classification_report\n\nvocab_size = 10000\nembedding_dim = 16\nmax_length = 500\ntrunc_type='post'\noov_tok = \"<OOV>\"\n\nX = df_total['content']\ny = df_total['label']\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n#optional cleaning of data\n# X_test = [' '.join(tokenize(x)) for x in X_test]\n# X_train = [' '.join(tokenize(x)) for x in X_train]\n\nX_test =  np.array(X_test)\nX_train = np.array(X_train)\ny_train = np.array(y_train)\ny_test = np.array(y_test)","cd899d9d":"tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(X_train)\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(X_train)\npadded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(X_test)\ntesting_padded = pad_sequences(testing_sequences, maxlen=500)","80be5720":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(6, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","eb42b222":"num_epochs = 10\nmodel.fit(padded, y_train, epochs=num_epochs, validation_data=(testing_padded, y_test))","8dc76a4f":"model.evaluate(testing_padded, y_test)","86e19163":"model.metrics_names ","1e35640d":"## Bi-gram","4d940fd6":"This was a fun project to understand different machine learning models and how it could analyse text and make predictions based on media bias and topic modelling.","6696a144":"### TF-IDF Linear Support Vector Classification","80e8a4ea":"## TF-IDF\n\nAllthough we found that (1,1) is the optimal n-gram range, we decided to use (1,2) since it gave the highest model performance.","dc1da932":"## LDA","788bf5c8":"### TF-IDF Random Forest","b1e6abde":"## LDA","2a464153":"# Study corpus\n\nLet's check if our hypothesis earlier is correct, by checking the topics that are created by our LDA model. The value for k we are using was found as having the most distinct non overlapping topics, while not having too much noise.","ae5f2473":"## Neural network for classification","5a8b5213":"## Topic models with different K + comparison\n\nCompare the different topic models to see if the give the same topics. We did this by using the jaccard similarity metric. This metric checks the overlap between topics. We plot the jaccard similarities in a matrix, where a light color indicates a high jaccard similarity. ","e0512df7":"# import","ed9338ed":"### LDA Random Forest","ff021917":"## find best lda parameters ","ed02cc72":"## SVD","25b5686c":"# EDA \n\nPerforming some Exploratoty Data Analysis to see what we can find in the data. Consists of the following steps:\n- Checking the sentiment of the articles, you would expect that the polarity would be neutral, since we are talking about independent journalism.\n- Top 20 words from unigram.\n- Top 20 words from bigram.\n- Top 20 words from trigram.\n\nThis to get a feeling what the articles are about","6c5ff40d":"From these result it's easy to see that the neural network has by far the best performance. That's interseting since model need no additional text cleaning steps, since cleaning the text decreased model performance. The next best model is a Random Forest with the TF-IDF as input, which outperformed the LDA embedding based models.\n\nFurther improvements:\n- Additional model tuning\n- Trying different techniques like a BERT transformer (downside: works on smaller texts)\n- Further experimentation with LDA parameters for optimal number of topics\n- Reduce vocabulary size for Neural Network to make model faster. (reducing vocabulary size has impact on performance)\n","3548b990":"### LDA Naive Bayes classifier for multinomial models","556dfd01":"## Uni-gram","a9076c63":"## Pre-process corpus\n\n- Taking a sample of the data for the sake of making it able to run on a laptop.\n- Adding some additional stopwords, since they don't add any value. These words were present in almost all topics, so we thought it was better to remove them.","d0ff2618":"## Tri-gram","94cd1525":"## Statistics","450f6746":"### LDA Linear Support Vector Classification","78628617":"# Intro\n\nWe chose New York Post as our initial publiction, which contains a Right media bias. https:\/\/www.allsides.com\/media-bias\/media-bias-ratings?field_featured_bias_rating_value=All&field_news_source_type_tid[1]=1&field_news_source_type_tid[2]=2&field_news_source_type_tid[3]=3&field_news_source_type_tid[4]=4&field_news_bias_nid_1[1]=1&field_news_bias_nid_1[2]=2&field_news_bias_nid_1[3]=3&title=new%20york%20post\n\nThe York post is a right wing publications that covers a range of topics like sports, especially the renowned baseball teams like Mets and Yankees. Politics and government where it has a particularly soft stance towards the republicans. Entertainment, where it covers Hollywood movies reviews and well as celebrity scandals. It also has lots of journalistic features which investigates stock market trends, big companies like Apple and Amazon , Fashion empires and reports on small enterprises, with a focus on competitive capitalism and the free market. It also has a section on fashion where it discuses glamour trends.","7e5cafdc":"# Topic Models\n\nBuilding multiple Topic Models like LDA, SVD and NMF. ","3b84e9ba":"## Reading files","8c40e12b":"This topic is intersting since it is about sports and mainly about the teams that are located in New York. This is in line with our hypothesis, since we said the NY Post has a big sports category talking about teams based in the city.","884847e1":"### LDA Logistic Regression","65a32d6e":"### TF-IDF Naive Bayes classifier for multinomial models","a5133d0e":"# Predict publication\nFor the publication prediction we tried several approaches:\n- LDA embeddings\n- TF-IDF\n- Neural network trained on the tokenized text\n\n## setting up data\n\nSince the data is not balanced, we undersample the data so that we have equal classes.\n\nThe other publication we chose was CNN, since its media bias is Left. Which is the opposite of the New York Post.","f8623f3a":"### TF-IDF Logistic Regression","33e7f8ba":"## finding optimal n-gram setting","41f33b05":"## NMF","e352e2dc":"## Sentiment analysis","d4bec61e":"## Model comparison and selection\n\nAccuracy was used a measure, since the data is balanced.\n\n|Classification| accuracy |\n| --- | --- |\n|LDA Random Forest | 0.79 |\n|LDA Logistic Regression | 0.73 |\n|LDA Linear SVC| 0.73 |\n|LDA Naive Bayes| 0.73 |\n| --- | --- |\n|TF-IDF Random Forest | 0.94 |\n|TF-IDF Logistic Regression | 0.93 |\n|TF-IDF Linear SVC| 0.92 |\n|TF-IDF Naive Bayes| 0.82|\n| --- | --- |\n|Neural Network| 0.96 |","d8d1bd53":"From the plots above we can see that there is not a similar topic in SVD for every topic in LDA. This means both models output different topics. However, there are some topics that are present in both models."}}