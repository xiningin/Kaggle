{"cell_type":{"ed083d1a":"code","8bdca0d1":"code","3ef7816c":"code","06253ee7":"code","f7b4b7ab":"code","11f63f9a":"code","1a015a8e":"code","ac9dbb01":"code","2ca56208":"code","3e518a73":"code","fc80cbd1":"code","8bfb9b4c":"code","73e28246":"code","18319157":"code","7aa53e81":"code","a603af67":"code","6d207a17":"code","ac331e0e":"code","f185bfe0":"code","6bffd2e6":"code","93385451":"code","0692d193":"code","3785ae6a":"code","6e94300c":"code","f551b079":"code","8a0e04b4":"code","01826448":"code","485ae5b6":"code","df327ba7":"code","294560ea":"code","bded9e8c":"code","189481fc":"code","e17150e1":"code","9bae5ef9":"markdown","f24728f3":"markdown","2ca54d42":"markdown","a21a9978":"markdown","61aec7e4":"markdown","752dd2ad":"markdown","de2770ff":"markdown","2db1d128":"markdown"},"source":{"ed083d1a":"import numpy as np \nimport pandas as pd \nimport graphviz\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nfrom sklearn.model_selection import train_test_split","8bdca0d1":"# loading the data set\n# dataset = load_iris()\n# df= pd.DataFrame(data= dataset.data)\ndf = pd.read_csv('..\/input\/iris\/Iris.csv')\n\nprint(df.shape)\ndf.head()","3ef7816c":"# Seperating to X and Y \nX = df.iloc[:, 1:5]\ny = df.iloc[:, -1:]\n\n# splitting training and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, shuffle=True, random_state=24)","06253ee7":"X.head()","f7b4b7ab":"y.head()","11f63f9a":"X_test.head()","1a015a8e":"dt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)","ac9dbb01":"dot_data = export_graphviz(dt, out_file=None, \n                           feature_names=X.columns,  \n                           class_names=y.Species,  \n                           filled=True, rounded=True,  \n                           special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph","2ca56208":"sklearn_y_preds = dt.predict(X_test)\nprint(f\"Sklearn Implementation:\\nACCURACY: {accuracy_score(y_test, sklearn_y_preds)}\")","3e518a73":"# Partition the data into left (indicating True), right (inidcating false).\ndef partition(data, column, value):\n    left = data[data[column] <= value].index\n    right = data[data[column] > value].index\n\n    return left, right","fc80cbd1":"# performing a split on the root node\nleft_idx, right_idx = partition(X_train, \"PetalLengthCm\", 2.45)\n\nprint(\"[PetalLengthCm <= 2.45]\")\n\n# print results --> left_idx = 38 setosa | right index = 42 versicolor, 32 virginica \nprint(f\"left_idx: {dict(zip(np.unique(y_train.loc[left_idx], return_counts=True)[0], np.unique(y_train.loc[left_idx], return_counts=True)[1]))}\\n\\\nright_idx: {dict(zip(np.unique(y_train.loc[right_idx], return_counts=True)[0], np.unique(y_train.loc[right_idx], return_counts=True)[1]))}\")","8bfb9b4c":"def gini_impurity(label, label_idx):\n    # the unique labels and counts in the data\n    unique_label, unique_label_count = np.unique(label.loc[label_idx], return_counts=True)\n\n    impurity = 1.0\n    for i in range(len(unique_label)):\n        p_i = unique_label_count[i] \/ sum(unique_label_count)\n        impurity -= p_i ** 2 \n    return impurity","73e28246":"# Gini impurity of the first node\nimpurity = gini_impurity(y_train, y_train.index)\nimpurity","18319157":"def information_gain(label, left_idx, right_idx, impurity): \n    p = float(len(left_idx)) \/ (len(left_idx) + len(right_idx))\n    info_gain = impurity - p * gini_impurity(label, left_idx) - (1 - p) * gini_impurity(label, right_idx)\n    return info_gain","7aa53e81":"# testing info gain of the first split at root node\ninfo_gain = information_gain(y_train, left_idx, right_idx, impurity)\ninfo_gain","a603af67":"X_train.head()","6d207a17":"# testing a random feature and value to see the information gain\nleft_idx, right_idx = partition(X_train, \"PetalWidthCm\", 1.65)\n\nimpurity = gini_impurity(y_train, y_train.index)\n\ninfo_gain = information_gain(y_train, left_idx, right_idx, impurity)\ninfo_gain","ac331e0e":"def find_best_split(df, label, idx):\n    best_gain = 0 \n    best_col = None\n    best_value = None\n    \n    df = df.loc[idx] # converting training data to pandas dataframe\n    label_idx = label.loc[idx].index # getting the index of the labels\n\n    impurity = gini_impurity(label, label_idx) # determining the impurity at the current node\n    \n    # go through the columns and store the unique values in each column (no point testing on the same value twice)\n    for col in df.columns: \n        unique_values = set(df[col])\n        # loop thorugh each value and partition the data into true (left_index) and false (right_index)\n        for value in unique_values: \n\n            left_idx, right_idx = partition(df, col, value)\n            # ignore if the index is empty (meaning there was no features that met the decision rule)\n            if len(left_idx) == 0 or len(right_idx) == 0: \n                continue \n            # determine the info gain at the node\n            info_gain = information_gain(label, left_idx, right_idx, impurity)\n            # if the info gain is higher then our current best gain then that becomes the best gain\n            if info_gain > best_gain:\n                best_gain, best_col, best_value = info_gain, col, value\n                \n    return best_gain, best_col, best_value","f185bfe0":"find_best_split(X_train, y_train, y_train.index)","6bffd2e6":"# helper function to count unique values\ndef count(label, idx):\n    unique_label, unique_label_counts = np.unique(label.loc[idx], return_counts=True)\n    dict_label_count = dict(zip(unique_label, unique_label_counts))\n    return dict_label_count","93385451":"# check counts at first node to check it aligns with scitkit learn\ncount(y_train, y_train.index)","0692d193":"class Leaf:\n    def __init__(self, label, idx):\n        self.predictions = count(label, idx)","3785ae6a":"class Decision_Node:\n    def __init__(self,\n                 column,\n                 value,\n                 true_branch,\n                 false_branch):\n        self.column = column\n        self.value = value\n        self.true_branch = true_branch\n        self.false_branch = false_branch","6e94300c":"def build_tree(df, label, idx):\n    best_gain, best_col, best_value = find_best_split(df, label, idx)\n    \n    if best_gain == 0: \n        return Leaf(label, label.loc[idx].index)\n    \n    left_idx, right_idx = partition(df.loc[idx], best_col, best_value)\n    \n    true_branch = build_tree(df, label, left_idx)\n    \n    false_branch = build_tree(df, label, right_idx)\n    \n    return Decision_Node(best_col, best_value, true_branch, false_branch)","f551b079":"def print_tree(node, spacing=\"\"):\n    # Base case: we've reached a leaf\n    if isinstance(node, Leaf):\n        print (spacing + \"Predict\", node.predictions)\n        return\n\n    # Print the col and value at this node\n    print(spacing + f\"[{node.column} <= {node.value}]\")\n    \n\n    # Call this function recursively on the true branch\n    print (spacing + '--> True:')\n    print_tree(node.true_branch, spacing + \"  \")\n\n    # Call this function recursively on the false branch\n    print (spacing + '--> False:')\n    print_tree(node.false_branch, spacing + \"  \")","8a0e04b4":"my_tree = build_tree(X_train, y_train, X_train.index)","01826448":"print_tree(my_tree)","485ae5b6":"def predict(test_data, tree):\n    \n    # Check if we are at a leaf node\n    if isinstance(tree, Leaf): \n        return max(tree.predictions)\n    \n    # the current feature_name and value \n    feature_name, feature_value = tree.column, tree.value\n    \n    # pass the observation through the nodes recursively\n    if test_data[feature_name] <= feature_value: \n        return predict(test_data, tree.true_branch)\n    \n    else: \n        return predict(test_data, tree.false_branch)","df327ba7":"# taking one instance to test function \nexample, example_target = X_test.iloc[6], y_test.iloc[6]\nexample, example_target","294560ea":"# if working correctly should output setosa\npredict(example, my_tree)","bded9e8c":"# create a new col of predictions\nX_test[\"predictions\"] = X_test.apply(predict, axis=1, args=(my_tree,))\n","189481fc":"sklearn_y_preds == X_test[\"predictions\"]","e17150e1":"# sklearn accuracy vs my implementation accuracy\nprint(f\"Sklearn Implementation:\\nACCURACY: {accuracy_score(y_test, sklearn_y_preds)}\\n\\n\\\nMy Implementation:\\nACCURACY: {accuracy_score(y_test, X_test['predictions'])}\")","9bae5ef9":"## Implementation from SCRATCH!","f24728f3":"Now we find features and values automatically and split the best.","2ca54d42":"A Leaf node classifies data.\nThis holds a dictionary of class (e.g., \"Apple\") -> number of times it appears in the rows from the training data that reach this leaf.","a21a9978":"### Gini Impurity\nA measure of how often a randomly chosen element from the set would be incorrectly labelled if it was randomly labelled according to the distribution of labels in the subset.<br>\n- **Inputs**\n    - label: The class label available at current node\n- **Outputs**\n    - impurity: The gini impurity of the node ","61aec7e4":"## With Scikit Learn","752dd2ad":"## Accuracy","de2770ff":"### Information Gain\nFor each node of the tree, the information gain \"represents the\n    expected amount of information that would be needed to specify whether\n    a new instance should be classified yes or no, given that the example\n    reached that node. (Source: Wikipedia)\n    \n- **Inputs**\n   - left: The values that met the conditions of the current node\n   - right: The values that failed to meet the conditions of the current noode\n   - gini_impurity: the uncertainty at the current node\n    \n- **Outputs**\n    - info_gain: The information gain at the node\n","2db1d128":"## Build Tree\nRecursively Builds the tree until is leaf is pure. \n    \n- Input \n    - df: the training data\n    - label: the target labels\n    - idx: the indexes \n    \n- Output\n    - best_col: the best column \n    - best_value: the value of the column that minimizes impurity \n    - true_branch: the true branch \n    - false_branch: the false branch"}}