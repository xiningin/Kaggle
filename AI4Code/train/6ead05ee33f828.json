{"cell_type":{"8a1a9660":"code","b6e28df6":"code","a7292e65":"code","6d762b83":"code","a0398df0":"code","c8db4466":"code","ed3f3fdb":"code","02e1030b":"code","8bdf7578":"code","6041ef63":"code","c8c478a5":"code","7b82dc72":"code","7d4231c3":"code","0c662e83":"code","e56d764a":"code","f58e02a2":"code","06c70dea":"code","a66ee68e":"code","93e0a97e":"code","9100f08c":"markdown","80570868":"markdown"},"source":{"8a1a9660":"import nltk\nimport numpy as np\nimport pandas as pd\nfrom nltk.tokenize import sent_tokenize","b6e28df6":"#file = open('..\/input\/S08_question_answer_pairs.txt', 'r')\nfile = open('..\/input\/questionanswer-dataset\/S08_question_answer_pairs.txt', 'r')\narray_arquivo = []\nfor line in file:      \n    array_arquivo.append(line.strip().split('\\t'))\nfile = open('..\/input\/questionanswer-dataset\/S09_question_answer_pairs.txt', 'r')\n#array_arquivo = []\nfor line in file:      \n    array_arquivo.append(line.strip().split('\\t'))\nfile = open('..\/input\/questionanswer-dataset\/S10_question_answer_pairs.txt', 'r', encoding = \"ISO-8859-1\")\n#array_arquivo = []\nfor line in file:      \n    array_arquivo.append(line.strip().split('\\t'))\n#import pandas as pd\n#import numpy as np\n#import string\n#df_S08 = pd.read_csv('..\/input\/S08_question_answer_pairs.txt', sep=\"\\t\", header=0)\n#df_S09 = pd.read_csv('..\/input\/S09_question_answer_pairs.txt', sep=\"\\t\", header=0)\n#df_S10 = pd.read_csv('..\/input\/S10_question_answer_pairs.txt', sep=\"\\t\", header=0, encoding = \"ISO-8859-1\")","a7292e65":"texto = ''\nfor a in array_arquivo:\n    texto += a[1] + ' '","6d762b83":"array_arquivo[1][2]","a0398df0":"sents_token = sent_tokenize(texto)\nprint(sents_token)","c8db4466":"sents_array = np.array(sents_token)\nprint(sents_array[10])","ed3f3fdb":"a = pd.DataFrame(sents_array)\nlen(a)\na.dropna()\nlen(a)","02e1030b":"[print(a) for a in sents_array]","8bdf7578":"from nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nimport string\nfrom nltk.corpus import wordnet\n\nstopwords_list = stopwords.words('english')\nlemmatizer = WordNetLemmatizer()","6041ef63":"def my_tokenizer(doc):\n    words = word_tokenize(doc)\n    \n    pos_tags = pos_tag(words)\n    \n    non_stopwords = [w for w in pos_tags if not w[0].lower() in stopwords_list]\n    \n    non_punctuation = [w for w in non_stopwords if not w[0] in string.punctuation]\n    \n    lemmas = []\n    for w in non_punctuation:\n        if w[1].startswith('J'):\n            pos = wordnet.ADJ\n        elif w[1].startswith('V'):\n            pos = wordnet.VERB\n        elif w[1].startswith('N'):\n            pos = wordnet.NOUN\n        elif w[1].startswith('R'):\n            pos = wordnet.ADV\n        else:\n            pos = wordnet.NOUN\n        \n        lemmas.append(lemmatizer.lemmatize(w[0], pos))\n\n    return lemmas","c8c478a5":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(tokenizer=my_tokenizer)\n\ntfs = tfidf_vectorizer.fit_transform(sents_array)\n\nprint(tfs.shape)\nprint(tfs)","7b82dc72":"print([k for k in tfidf_vectorizer.vocabulary_.keys()][:20])","7d4231c3":"from sklearn.decomposition import TruncatedSVD\n\nsvd_transformer = TruncatedSVD(n_components=1000)\n\nsvd_transformer.fit(tfs)\n\nprint(sorted(svd_transformer.explained_variance_ratio_)[::-1][:30])","0c662e83":"cummulative_variance = 0.0\nk = 0\nfor var in sorted(svd_transformer.explained_variance_ratio_)[::-1]:\n    cummulative_variance += var\n    if cummulative_variance >= 0.5:\n        break\n    else:\n        k += 1\n        \nprint(k)","e56d764a":"svd_transformer = TruncatedSVD(n_components=k)\nsvd_data = svd_transformer.fit_transform(tfs)\nprint(sorted(svd_transformer.explained_variance_ratio_)[::-1])","f58e02a2":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer()\ntfidf_matrix = tfidf_vectorizer.fit_transform(sents_array)","06c70dea":"from sklearn.metrics.pairwise import cosine_similarity","a66ee68e":"def retornar_pergunta(index):    \n    return array_arquivo[index + 1][1]\ndef retornar_resposta(index):    \n    if array_arquivo[index + 1][2] == 'NULL':\n        return 'We didn t find an answer, could you ask another question?'\n    else:\n        return array_arquivo[index + 1][2]","93e0a97e":"#Escreva uma pergunta \n#query_vect = tfidf_vectorizer.transform([\"When did J\u00e3o graduate from William and Mary?\"])\n# ou Escolha uma pergunta dentre as existentes\nquery_vect = tfidf_vectorizer.transform([sents_array[3000]])\n#789\n#777\n\nresult_cosine = cosine_similarity(query_vect, tfidf_matrix)\n\nprint(np.amax(result_cosine))\nprint(np.argmax(result_cosine))\nprint(retornar_pergunta(np.argmax(result_cosine)))\nprint(retornar_resposta(np.argmax(result_cosine)))","9100f08c":"<p><b>Exerc\u00edcio 3:<\/b>\n\nEscreva um chatbot que, dado uma pergunta em Ingl\u00eas, encontre uma pergunta mais parecida no corpus de perguntas e respostas dispon\u00edvel no Kaggle (https:\/\/www.kaggle.com\/rtatman\/questionanswer-dataset#S08_question_answer_pairs.txt) e exiba a resposta.","80570868":"<h1 align=\"center\"> Aplica\u00e7\u00f5es em Processamento de Linguagem Natural <\/h1>\n<h2 align=\"center\"> Aula 03 - T\u00e9cnicas de Pr\u00e9-Processamento de Texto e Similaridade<\/h2>\n<h3 align=\"center\"> Prof. Fernando Vieira da Silva MSc.<\/h3>\n\n# Integrantes:\n### Leandro Freitas \t    - RA: 080162\n### Paulo \u00c9rico de Freitas\t- RA: 183545\n### Renan Souza\t\t        - RA: 070487 "}}