{"cell_type":{"d1c9f982":"code","87dd8665":"code","8ee9ccf0":"code","f6d092b9":"code","d16da7c8":"code","3ec095d2":"code","4cbc25aa":"code","dae9e6c0":"code","d61cf540":"code","5a96540a":"code","73398d66":"code","163bdb66":"code","4e71bd26":"code","c7d75d07":"code","82a14e6c":"code","55c02631":"code","c46ea248":"markdown","77d2846e":"markdown","08124153":"markdown"},"source":{"d1c9f982":"import pandas as pd\nimport numpy as np\nimport regex as re\nfrom nltk.corpus import stopwords\nfrom tqdm import tqdm\nimport gc\nfrom string import punctuation\nimport string\ngc.collect()\nfrom spacy.lang.en.lemmatizer import LOOKUP\n\n# replace abbreviations, source: https:\/\/github.com\/xiangzhemeng\/Kaggle-Twitter-Sentiment-Analysis\/blob\/master\/data_preprocessing.py\ndef abbreviation_replacement(text):\n    text = re.sub(r\"i\\'m\", \"i am\", text)\n    text = re.sub(r\"\\'re\", \"are\", text)\n    text = re.sub(r\"he\\'s\", \"he is\", text)\n    text = re.sub(r\"it\\'s\", \"it is\", text)\n    text = re.sub(r\"that\\'s\", \"that is\", text)\n    text = re.sub(r\"who\\'s\", \"who is\", text)\n    text = re.sub(r\"what\\'s\", \"what is\", text)\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n#     text = re.sub(r\",\", \" , \", text)\n#     text = re.sub(r\"!\", \" ! \", text)\n#     text = re.sub(r\"\\.\", \" \\. \", text)\n#     text = re.sub(r\"\\(\", \" \\( \", text)\n#     text = re.sub(r\"\\)\", \" \\) \", text)\n#     text = re.sub(r\"\\?\", \" \\? \", text)\n    return text\n\n# handle emojis, source: https:\/\/github.com\/Carmezim\/crypto-twitter-sentiment-analysis\/blob\/master\/preprocess.py, but updated a bit\ndef handle_emojis(tweet):\n    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' emopos ', tweet)\n    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n    tweet = re.sub(r'(:\\s?3|:\\s?D|:-D|x-?D|X-?D)', ' emopos ', tweet)\n    # Love -- <3, :*, \u2665, ^^, ^_^\n    tweet = re.sub(r'(<3|:\\*|\u2665|^\\s?^)', ' emopos ', tweet)\n    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' emopos ', tweet)\n    # Sad -- :-(, : (, :(, ):, )-:\n    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' emoneg ', tweet)\n    # Cry -- :,(, :'(, :\"(, ;(, );\n    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()|;\\s?\\(|\\(\\s?;', ' emoneg ', tweet)\n    return tweet\n\n\npuntation_dict = str.maketrans('', '', string.punctuation)\ndef drop_punctuation(text):\n    # \u042d\u0442\u043e \u0438\u0441\u0442\u043e\u0440\u0438\u044f \u043e \u0442\u043e\u043c \u043a\u0430\u043a \u043d\u0435\u043b\u044c\u0437\u044f \u043f\u0438\u0441\u0430\u0442\u044c \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043a\u0438 - \u043e\u043d\u0430 \u0437\u0430\u043c\u0435\u043d\u044f\u0435\u0442 \u0432\u0441\u0435 \u043d\u0430 troetoch:\n#     text = re.sub(\"\\.\\.\\.\", \" troetoch \", text)\n    # Convert more than 2 letter repetitions to 2 letter\n    # fuuuuunnnnny --> funny\n    text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n    # convert necc punctuation to special words\n    text = re.sub(r\"!\", \" www \", text)\n    text = re.sub(r\"\\?\", \" mmm \", text)\n    # convert other punctuation to empty string\n    text = text.translate(puntation_dict)\n    \n    return text\n\n\ndef remove_number(text):\n    text = re.sub(r'(0|1|2|3|4|5|6|7|8|9)', '', text)\n    return text\n\n\n# Loading stopwords list from NLTK\nstoplist = set(stopwords.words('english'))\n\ndef get_tokens(text):\n    tokens = text.split()\n    del text\n    return tokens\n\n# get stopwords using nltk corpus, but unfortunatly it doesn't work on this dataset\ndef remove_stopwords(text):\n    tokens = text.split()\n    for word in tokens:\n        if word in stoplist:\n            tokens.remove(word)\n    text = ' '.join(tokens)\n    del tokens\n    return text\n\n# get lemmatization using huge dict of lemmas integrated to spacy, but unfortunatly lemmatization don't work on this dataset\ndef spacy_fast_lemmatizier_stopworder(text, return_tokens=False):\n    tokens = []\n    for word in text.split(' '):\n        if word not in stoplist:\n            tokens.append(LOOKUP.get(word, word))\n    if return_tokens:\n        del text\n        return tokens\n    text = ' '.join(tokens)\n    del tokens\n    return text\n\n\n# Combine all preprocessing method\ndef preprocessing_all_method(tweet):\n    tweet = tweet.strip().lower()\n    tweet = abbreviation_replacement(tweet)\n    tweet = handle_emojis(tweet)\n    tweet = drop_punctuation(tweet)\n    tweet = remove_number(tweet)\n#     tweet = spacy_fast_lemmatizier_stopworder(tweet, return_tokens=False)\n    return tweet\n\n# Combine all preprocessing method and return tokens\ndef tokenizer_all_method(tweet):\n    tweet = tweet.strip().lower()\n    tweet = abbreviation_replacement(tweet)\n    tweet = handle_emojis(tweet)\n    tweet = remove_number(tweet)\n    tweet = drop_punctuation(tweet)\n#     tokens = spacy_fast_lemmatizier_stopworder(tweet, return_tokens=True)\n    tokens = tweet.split()\n    del tweet\n    return tokens\n\n# run all preprocessing for texts\ndef preproc(texts):\n    for i in tqdm(range(len(texts))):\n        texts[i] = preprocessing_all_method(texts[i])\n    return texts","87dd8665":"# load data:\nwith open('..\/input\/x_train.txt', 'r') as f:\n    x_train = f.readlines()\nwith open('..\/input\/x_test.txt', 'r') as f:\n    x_test = f.readlines()","8ee9ccf0":"import pandas as pd\ny_train = np.array(pd.read_csv('..\/input\/y_train.csv')['Probability'])","f6d092b9":"from sklearn.feature_extraction.text import TfidfVectorizer","d16da7c8":"\nvectorizer = TfidfVectorizer(analyzer='word',\n                           ngram_range=(1, 3),\n                           tokenizer=tokenizer_all_method,\n                           # \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043f\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b \u043f\u0440\u043e\u0441\u0442\u043e \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u0442\u0430\u043a \u043b\u0443\u0447\u0448\u0435:\n                           min_df=3,\n                           max_df=0.85,\n                           sublinear_tf=True\n                           )","3ec095d2":"# \u043d\u0435\u043f\u043b\u043e\u0445\u043e\u0439 \u0438\u0434\u0435\u0435\u0439 \u0437\u0432\u0443\u0447\u0438\u0442 \u0442\u0430\u043a \u0436\u0435 \u0437\u0430\u0444\u0438\u0442\u0438\u0442\u044c\u0441\u044f \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 - \u0442\u043e\u0433\u0434\u0430 \u0432\u0441\u0435 n-\u0433\u0440\u0430\u043c\u043c\u044b \u043e\u0442\u0442\u0443\u0434\u0430 \u0431\u0443\u0434\u0443\u0442 \u043d\u0430\u043c \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b, \u043d\u043e \u043c\u044b \u043d\u0435 \u0441\u0434\u043e\u0445\u043d\u0435\u043c \u043f\u043e \u043f\u0430\u043c\u044f\u0442\u0438\nvectorizer = vectorizer.fit(x_test)","4cbc25aa":"# just pray for garbage collector\ngc.collect()","dae9e6c0":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.model_selection import StratifiedKFold","d61cf540":"# import hyperopt - \u043c\u0435\u0447\u0442\u0430\u044e...","5a96540a":"# this is g*kod predictor, enjoy :)\n\nclass Gkpredictor():\n    def __init__(self, model):\n        self.model = model\n    \n    def fit(self, X, y):\n        self.model = self.model.fit(vectorizer.transform(X, copy=False).astype(np.float16), y)\n    \n    def predict_proba(self, X):\n        return self.model.predict_proba(vectorizer.transform(X, copy=False).astype(np.float16))","73398d66":"from sklearn.metrics import roc_auc_score\nfrom operator import itemgetter\nfrom sklearn.model_selection import train_test_split\n\nclass SuperVoting():\n    def __init__(self, clfs):\n        self.clfs = clfs\n    \n    # \u0440\u0430\u0432\u043d\u043e\u043c\u0435\u0440\u043d\u043e \u0434\u0435\u043b\u0438\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043c\u0435\u0436\u0434\u0443 \u0432\u0441\u0435\u043c\u0438 \u043c\u043e\u0434\u0435\u043b\u044f\u043c\u0438\n    def fit(self, X, y, need_pred=False):\n        skf = StratifiedKFold(n_splits=len(self.clfs))\n        i = 0\n        for train_index, test_index in skf.split(X, y):\n            # StratifiedKFold yields indexes of necc elements \n            try:\n                X_train, X_test = X[train_index], X[test_index]\n                y_train, y_test = y[train_index], y[test_index]\n            except:\n                # \u0435\u0441\u043b\u0438 \u043d\u0430 \u0432\u0445\u043e\u0434\u0435 \u0432\u0434\u0440\u0443\u0433 \u043e\u043a\u0430\u0437\u0430\u043b\u0441\u044f list:\n                need_pred = False\n                X_train = itemgetter(*train_index)(X)\n                y_train = y[train_index]\n            self.clfs[i].fit(X_train, y_train)\n            if need_pred:\n                pred = self.clfs[i].predict_proba(X_test)[:, 1]\n                print(roc_auc_score(y_test, pred))\n            gc.collect()\n            i += 1\n    \n    # \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043d\u0430 \u0440\u0430\u043d\u0434\u043e\u043c\u043d\u043e\u043c \u043a\u0443\u0441\u043a\u0435 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0438\n    def fit_tts(self, X, y, need_pred=False, train_size=0.5):\n        for i in range(len(self.clfs)):\n            if need_pred:\n                X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size)\n            else:\n                # \u0431\u0435\u0440\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u043d\u0443\u0436\u043d\u043e\u0435:\n                X_train, _, y_train, _ = train_test_split(X, y, train_size=train_size)\n            self.clfs[i].fit(X_train, y_train)\n            if need_pred:\n                pred = self.clfs[i].predict_proba(X_test)[:, 1]\n                print(roc_auc_score(y_test, pred))\n            gc.collect()\n    \n    # \u043f\u0440\u0435\u0434\u0438\u043a\u0442\u0438\u043c\n    def predict_proba(self, X):\n        try:\n            # \u0443 sparse \u043c\u0430\u0442\u0440\u0438\u0446 \u043d\u0435\u0442\u0443 len, \u0435\u0441\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e shape\n            res = np.zeros((X.shape[0], len(self.clfs)))\n        except:\n            res = np.zeros((len(X), len(self.clfs)))\n        for i in range(len(self.clfs)):\n            res[:, i] = self.clfs[i].predict_proba(X)[:, 1]\n        return res.mean(axis=1)","163bdb66":"from sklearn.linear_model import SGDClassifier, PassiveAggressiveClassifier, LogisticRegressionCV, LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler","4e71bd26":"# \u0438\u043c\u0435\u043d\u043d\u043e \u044d\u0442\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u044f \u0438\u0437\u0431\u0440\u0430\u043b \u0434\u043b\u044f fit-predicta\n# \u043d\u0430 \u043f\u0440\u0438\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0438 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0432 200\u043a \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u043e\u043d\u0438 \u0434\u0430\u0432\u0430\u043b\u0438 \u043b\u0443\u0447\u0448\u0438\u0439 \u0441\u043a\u043e\u0440, \u043e\u043d \u0438\u043d\u043e\u0433\u0434\u0430 \u043f\u043e\u0434\u043f\u0438\u0441\u0430\u043d \u0441\u0431\u043e\u043a\u0443:\n\nmodel1 = Gkpredictor(CalibratedClassifierCV(base_estimator=SGDClassifier(\n                                                       loss='hinge', \n                                                       penalty='elasticnet', \n                                                       alpha=0.00000008,\n                                                       max_iter=3)))\nmodel2 = Gkpredictor(CalibratedClassifierCV(base_estimator=SGDClassifier(\n                                                       loss='hinge', \n                                                       penalty='elasticnet', \n                                                       alpha=0.0000003,\n                                                       max_iter=3)))\nmodel3 = Gkpredictor(CalibratedClassifierCV(base_estimator=SGDClassifier(\n                                                       loss='hinge', \n                                                       penalty='elasticnet', \n                                                       alpha=0.000001,\n                                                       max_iter=3)))\n# model3 = Gkpredictor(CalibratedClassifierCV(base_estimator=PassiveAggressiveClassifier())) #- 0.9704\nmodel4 = Gkpredictor(CalibratedClassifierCV(base_estimator=LinearSVC())) #- 0.971\nmodel5 = Gkpredictor(CalibratedClassifierCV(base_estimator=LogisticRegression(\n                                                        solver='lbfgs',\n                                                        C=10.,\n                                                        max_iter=75)) )#- 0.9713","c7d75d07":"# \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u0448\u0430\u0440\u043c\u0430\u043d\u043a\u0443:\nmodel = SuperVoting(clfs=[model3, model4, model5, model3, model4, model5])\nmodel.fit_tts(x_train, y_train, need_pred=False)\npred = model.predict_proba(x_test)","82a14e6c":"# \u0441\u043e\u0437\u0440\u0430\u043d\u044f\u0435\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b:\ndef saver(path, res):\n    with open(path, 'w') as out:\n        print('Id,Probability', file = out)\n        for i in range(len(res)):\n            print('{i},{res}'.format(i = i + 1, res=res[i]), file = out)","55c02631":"saver('sk_1_3_multi_tts.csv', pred)","c46ea248":"\u0422\u0435\u043f\u0435\u0440\u044c \u043a\u043e\u0433\u0434\u0430 \u043c\u044b \u0441\u0434\u0435\u043b\u0430\u043b\u0438 \u0442\u0430\u043a\u0443\u044e \u043a\u043b\u0430\u0441\u0441\u043d\u0443\u044e \u0442\u0435\u043c\u0443 \u043a\u0430\u043a \u043f\u0440\u0435\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0438\u043d\u0433, \u0445\u043e\u0447\u0435\u0442\u0441\u044f \u0435\u0451 \u0437\u0430\u044e\u0437\u0430\u0442\u044c, \u043e\u0434\u043d\u0430\u043a\u043e \u043d\u0430\u0441 \u0436\u0434\u0435\u0442 \u0440\u0430\u0437\u043e\u0447\u0430\u0440\u043e\u0432\u0430\u043d\u0438\u0435.\n\u041e\u043d\u0430 \u0445\u0430\u0432\u0430\u0435\u0442 \u043f\u0430\u043c\u044f\u0442\u044c.\n\u0417\u043d\u0430\u0435\u0442\u0435 \u043f\u043e\u0447\u0435\u043c\u0443?\n\u041f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u043a\u0430\u0436\u0434\u0430\u044f \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u044f \u043f\u0440\u0435\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0438\u043d\u0433\u0430 \u043d\u0435 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442\u0441\u044f *inplace*, \u0442\u0430\u043a \u043a\u0430\u043a \u0441\u0442\u0440\u043e\u043a\u0438 \u0432 \u043f\u0438\u0442\u043e\u043d\u0435 \u043d\u0435\u0438\u0437\u043c\u0435\u043d\u044f\u0435\u043c\u044b...\n\u041d\u0435\u043e\u0436\u0438\u0434\u0430\u043d\u043d\u043e \u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442\u0441\u044f, \u0447\u0442\u043e \u0435\u0441\u043b\u0438 \u043e\u0442\u043f\u0440\u0430\u0432\u0438\u0442\u044c \u043f\u0440\u0435\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0438\u043d\u0433 \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u0438 \u0432 sklearn, \u0442\u043e \u043f\u0430\u043c\u044f\u0442\u044c \u043d\u0430\u0447\u043d\u0435\u0442 \u043f\u043e\u0434\u0447\u0438\u0449\u0430\u0442\u044c\u0441\u044f \u0438\u043c, \u0434\u0430\u0432\u0430\u0439\u0442\u0435 \u0442\u0430\u043a \u0438 \u0441\u0434\u0435\u043b\u0430\u0435\u043c;","77d2846e":"\u0414\u043b\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432 \u0447\u0430\u0441\u0442\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043d\u044b\u0435 \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u044f. \u0429\u0430 \u043c\u044b \u0438\u043c\u0435\u043d\u043d\u043e \u044d\u0442\u0438\u043c \u0438 \u0437\u0430\u0439\u043c\u0435\u043c\u0441\u044f. \u043e\u0431\u044b\u0447\u043d\u043e \u044d\u0442\u043e \u043e\u0442\u043d\u0438\u043c\u0430\u0435\u0442 \u043c\u043d\u043e\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438, \u043e\u0434\u043d\u0430\u043a\u043e \u0435\u0441\u043b\u0438 \u0445\u043e\u0440\u043e\u0448\u043e \u0433\u0443\u0433\u043b\u0438\u0442\u044c, \u0442\u043e 3\/4 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043e\u043a \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0437\u0434\u0435\u0441\u044c \u0431\u0443\u0434\u0443\u0442 \u0441\u043e\u0431\u0440\u0430\u043d\u044b \u043f\u043e \u0440\u0430\u0437\u043d\u044b\u043c \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0430\u043c, \u0442\u043e\u043f\u043e\u0432\u044b\u043c \u0440\u0435\u0448\u0435\u043d\u0438\u044f\u043c \u0432\u044b\u043b\u043e\u0436\u0435\u043d\u043d\u044b\u043c \u043d\u0430 \u0433\u0438\u0442\u0445\u0430\u0431, \u0438\u043b\u0438 \u0432 public kernel, \u0442\u0430\u043a\u043e\u0439 \u043a\u0430\u043a \u044d\u0442\u043e\u0442.","08124153":"\u0427\u0442\u043e\u0431\u044b \u043d\u0435 \u0441\u0434\u043e\u0445\u043d\u0443\u0442\u044c \u043f\u043e \u043f\u0430\u043c\u044f\u0442\u0438 \u043e\u0442 sparse \u043c\u0430\u0442\u0440\u0438\u0446 \u043e\u0433\u0440\u043e\u043c\u043d\u043e\u0433\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u0430, \u0431\u0443\u0434\u0435\u043c \u043e\u0431\u0443\u0447\u0430\u0442\u044c\u0441\u044f \u043d\u0435 \u0431\u043e\u043b\u0435\u0435 \u0447\u0435\u043c \u043d\u0430 \u043f\u043e\u043b\u043e\u0432\u0438\u043d\u0435 \u0432\u044b\u0431\u043e\u0440\u043a\u0438, \u043d\u0430 \u0441\u0430\u043c\u043e\u043c \u0434\u0435\u043b\u0435 \u044d\u0442\u043e\u0433\u043e \u0432\u043f\u043e\u043b\u043d\u0435 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e, \u0430 \u0434\u0430\u043b\u044c\u0448\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u0443\u0441\u0440\u0435\u0434\u043d\u044f\u0442\u044c:"}}