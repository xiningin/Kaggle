{"cell_type":{"f1983584":"code","5df2f78e":"code","e8488c89":"code","01045b92":"code","30a0c326":"code","d8d5ae61":"code","f0ceec4b":"code","1e932575":"code","f2bc6407":"code","1bbf73d7":"code","95d1e040":"code","9b31a99a":"code","ac679b02":"code","17aa67db":"code","8b0ef9c4":"code","0d3462d2":"code","67b4b7ea":"code","d19d78da":"code","437ce090":"code","3dae53ba":"code","dd97a144":"markdown","f8a62eeb":"markdown","1034fe2f":"markdown","5f34424e":"markdown","22c4fb83":"markdown","5a823a36":"markdown","12c2f03b":"markdown","f929b8cd":"markdown","a35c3594":"markdown"},"source":{"f1983584":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5df2f78e":"# file path\npath_train = '\/kaggle\/input\/titanic\/train.csv'\npath_test = '\/kaggle\/input\/titanic\/test.csv'\n\n# loading CSV\ndata = pd.read_csv(path_train)\ndata_test = pd.read_csv(path_test)","e8488c89":"# checking index and columns\n\ndata.head()","01045b92":"# checking number of records, data types and number of non null elements\n\ndata.info()\nprint('*****')\ndata_test.info()","30a0c326":"# checking number of null elements\n\nprint(data.isnull().sum())\nprint('*****')\nprint(data_test.isnull().sum())","d8d5ae61":"#statistical report of numerical columns\n\ndata.describe()\n# data.describe(include='object')","f0ceec4b":"#removing uninformative columns\ndata = data.drop(['Ticket', 'Name', 'PassengerId', 'Cabin'], axis=1)\n\n# adding new column\ndata['Alone'] = (data.Parch + data.SibSp).map(lambda x : x == 0).astype('float64') # checking if passenger was alone or not\n\ndata.head()","1e932575":"# survival rate for single people vs. people with family\nax = pd.crosstab(data.Alone, data.Survived).plot.bar(stacked=True)","f2bc6407":"# survival rate in different classes\nax = pd.crosstab(data.Pclass, data.Survived).plot.bar(stacked=True)\n\n# class rate in survival groups\nax = pd.crosstab(data.Survived, data.Pclass).plot.bar(stacked=True)","1bbf73d7":"# gender rate in survival groups\nax = pd.crosstab(data.Survived, data.Sex).plot.bar(stacked=True)\n\n# survival rate in gender groups\nax = pd.crosstab(data.Sex, data.Survived).plot.bar(stacked=True)","95d1e040":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline","9b31a99a":"# loading CSV\ndata_train = pd.read_csv(path_train)\ndata_test = pd.read_csv(path_test)\n\n# keeping index of test dataset\nsubmission_index = data_test.PassengerId","ac679b02":"#removing uninformative columns\ndata_train = data_train.drop(['Ticket', 'Name', 'PassengerId', 'Cabin'], axis=1)\ndata_test = data_test.drop(['Ticket', 'Name', 'PassengerId', 'Cabin'], axis=1)\n\n\n\ndata_train.groupby(\"Embarked\").Embarked.count()\n#Most of the Embarked values are from Southampton(S).\ndata_train['Embarked']=data_train['Embarked'].fillna('S')\ndisplay(data_train.isnull().sum())\n\n\n","17aa67db":"#As we can see Age and Pclass have a high corelation.\n#Using median age of the Pclass to missing age will be benefitial rather using median of whole data set. \n\ndata_train.groupby(['Sex', 'Pclass'])['Age'].median()\n\n\ndata_train['Age'] = data_train.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\n\ndisplay(data_train.isnull().sum())\n\n# editing column types\ndata_train['Sex'] = data_train.Sex.map({'male': 1, 'female': 0}).astype('float64') # binary gender representation\ndata_test['Sex'] = data_test.Sex.map({'male': 1, 'female': 0}).astype('float64')\ndata_train['Parch'] = (data_train.Parch >= 1).astype('float64') # binary parch representation\ndata_test['Parch'] = (data_test.Parch >= 1).astype('float64')\ndata_train['SibSp'] = (data_train.SibSp >= 1).astype('float64') # binary sibsp representation\ndata_test['SibSp'] = (data_test.SibSp >= 1).astype('float64')\n\ndata_train.head()","8b0ef9c4":"\n# transformation for categorical column\ntransformer_cat = Pipeline(steps =[('imputer', SimpleImputer(strategy= 'most_frequent')),\n                                   ('onehot', OneHotEncoder(handle_unknown= 'ignore'))\n                                  ])\n# transformation for numerical column\ntransformer_num = Pipeline(steps= [('imputer', SimpleImputer(strategy= 'mean')),\n                                   ('standard', StandardScaler())\n                                  ])\n\n# transformation for age column\ntransformer_age = Pipeline(steps= [('imputer', SimpleImputer(strategy= 'median')),\n                                   ('standard', StandardScaler())\n                                  ])\n# transformation package\npreprocessor = ColumnTransformer(transformers= [('categorical', transformer_cat, ['Pclass', 'Embarked']),\n                                                ('numerical', transformer_num, ['Fare']),\n                                                ('age', transformer_age, ['Age']) ], remainder= 'passthrough')","0d3462d2":"# backup to keep data files unchanged\nX = data_train.copy() # creating a copy\nX_test = data_test.copy()\n\n\n# splitting to dependent and independent features\ny = X.Survived.values # target vector \/ dependent feature\nX = X.drop('Survived', axis=1) # independent features","67b4b7ea":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import confusion_matrix, accuracy_score","d19d78da":"# model definition\nmodels = {'Logistic Regression': LogisticRegression(),\n          'Random Forest': RandomForestClassifier(n_estimators= 50, max_leaf_nodes= 25)}\n\nmodel_pipeline = Pipeline(steps= [('preprocessor', preprocessor),\n                                  ('model', models['Logistic Regression'])])\n\n# cross validation\nscores = cross_validate(model_pipeline, X, y, cv= 5,\n                        scoring= 'accuracy', return_train_score= True)\nprint('Train Accuracy', round(scores['train_score'].mean() * 100, 2), '%')\nprint('Validation Accuracy', round(scores['test_score'].mean() * 100, 2), '%')","437ce090":"#fitting on full data\nmodel_pipeline.fit(X, y)\n\n# predicting the full data to check its result\npred = model_pipeline.predict(X)\nprint(round(accuracy_score(y, pred) * 100, 2), '%')\n\n# confusion matrix \ncm = pd.DataFrame(confusion_matrix(y, pred), index = ['Really Died', 'Really Survived'], columns = ['Died', 'Survived'])\nprint(cm)\n","3dae53ba":"# predicting the test data for submission\nsubmission_prediction = model_pipeline.predict(X_test)\n\n# creating output dataframe in a format of competition\noutput = pd.DataFrame({'PassengerId': submission_index, 'Survived': submission_prediction})\noutput.to_csv('submission.csv', index=False) # exporting CSV file","dd97a144":"**factors which have the most effect on survival :**\n1. age people between 20-30 has the most survivng \n2.p-class also has an effect since class 3 has small amount of survivals \n3. gender**","f8a62eeb":"## Checking Data","1034fe2f":"## Loading Data","5f34424e":"## Model Selection & Validation","22c4fb83":"## Full Training","5a823a36":"## Dataset Modifying","12c2f03b":"## Filling missing values","f929b8cd":"## Data Preproccessing","a35c3594":"## Predicting Test Data"}}