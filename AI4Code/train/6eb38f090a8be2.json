{"cell_type":{"26e88d70":"code","37137345":"code","e5875ffa":"code","2dcd5216":"code","35881503":"code","bcbe2200":"code","163829a7":"code","b73b8785":"code","30a2b0b1":"code","aeac210b":"code","45a10334":"code","6310a7d4":"code","dabec52c":"code","16359a3f":"code","2bd9b019":"code","5a5bbeba":"code","8dbb7bf7":"code","598d90fd":"code","89d59c8b":"code","6a8f5f2b":"code","528c0c0c":"code","c6382c92":"code","7b3f304e":"code","172dd745":"code","3992884b":"code","b98c51fe":"code","fa7e219e":"code","2f082274":"code","d07bb7f3":"code","27741376":"code","bac950ec":"code","fa3a35a5":"code","1f420171":"code","f8eb33ab":"code","bcfa4552":"code","29dcd962":"code","d74d0e04":"code","82bbef4f":"code","10e66165":"code","b52889aa":"code","c8dfce66":"code","f827d383":"code","a38c22da":"code","8ed4a802":"code","1d8b8d97":"code","ddf67770":"code","4dec2f1d":"code","73757d80":"code","15360c1e":"code","3a2560b7":"code","fdf88e17":"code","b6209d19":"code","2403f5db":"code","d2420a30":"code","82334abc":"code","ca65cb44":"code","d5326cbd":"code","7d20cd8c":"code","e4c07430":"code","ede7fe2e":"code","65685db9":"code","9726954c":"code","4b7ad9c9":"code","00dcd0b5":"code","39d86c60":"code","17300d46":"code","eab6cfaa":"code","40420c06":"code","85f9524d":"code","c34f5e48":"code","f1e2260a":"code","67001e41":"code","9c0c4c20":"code","ed7e43db":"code","300b6bf9":"code","dfdb5c33":"code","9529ac25":"code","98b09911":"code","f62307f9":"code","40f71535":"code","017a95ad":"code","c679c4a7":"code","f2bcc6e7":"code","63b0e178":"code","53dab090":"code","af4a0119":"code","ea13c641":"code","8f045148":"code","a94f3cd2":"code","1966f2d3":"code","054671b3":"code","fd9f8b45":"code","f2a0054d":"code","fbe68153":"code","3c44e6bb":"code","e56f4976":"code","8210e85e":"code","63fe2516":"code","c6a4d6a8":"code","ba0b27df":"code","950bee09":"code","d2af22e8":"code","adf549cd":"code","270bf16e":"code","05b3961b":"code","dcce9984":"code","69d72de0":"code","ad278121":"code","31bbb992":"code","ede8b5d5":"code","713558d7":"code","59e55e1f":"code","a2d262a7":"code","6d68887a":"code","f0b0841d":"code","cac20e13":"code","c35703fa":"code","4e8d304a":"code","a09bd03e":"code","59797b76":"code","de22d905":"code","8d31f5e5":"code","61abecc1":"code","dee1bb3b":"code","96340c3a":"code","98853daa":"code","cf47ae09":"code","4742e672":"code","65969208":"code","762f6b70":"code","914b5988":"code","015df509":"code","e3ef2a29":"code","4612a09d":"code","79f2ac6b":"code","862b5bd8":"code","96b7eed0":"code","085f8c07":"code","d30686b2":"code","07c3f9a3":"code","da755f53":"code","89f84560":"code","e992c314":"code","954e07ee":"code","998b2a32":"code","0ecd34d4":"code","932da737":"code","dd674996":"code","f95aec48":"code","6f4cb1f6":"code","5f45e95c":"code","7b4bafab":"code","32d07ffb":"code","2a718938":"code","4f187a32":"markdown","c8e3e6a1":"markdown","5909157c":"markdown","274388fb":"markdown","72d9ecfa":"markdown","b8c7befa":"markdown","e0485ad1":"markdown","47ba68bb":"markdown","021e2526":"markdown","3c7a4bac":"markdown","92e17bd8":"markdown","087bb1ea":"markdown","9c8a1835":"markdown","7075e165":"markdown","d531ec63":"markdown","743b92c3":"markdown","5a06875d":"markdown","19d3f445":"markdown","7092df64":"markdown","03b026c3":"markdown","3fe8da29":"markdown","c80b47f9":"markdown","a17a73eb":"markdown","2f6fe962":"markdown"},"source":{"26e88d70":"# Importing relevant libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\n%matplotlib inline","37137345":"# Loading the data set into a pandas dataframe\ngender=pd.read_csv('..\/input\/gender_submission.csv')\ntest=pd.read_csv('..\/input\/test.csv')\ntrain=pd.read_csv('..\/input\/train.csv')","e5875ffa":"gender.info()","2dcd5216":"train.info()","35881503":"train.describe(include='all')","bcbe2200":"# Assessing the first few columns.\ntrain.head()","163829a7":"train['PassengerId']=train['PassengerId'].astype(str)","b73b8785":"train.info()","30a2b0b1":"train=train.drop(['Cabin'],axis=1)","aeac210b":"df1=train[['Survived','Pclass','Age','SibSp','Parch','Fare']]","45a10334":"df1.head(5)","6310a7d4":"from fancyimpute import KNN","dabec52c":"def knn(t,i):\n    z=t\n    z.loc[0,i]=np.NaN\n    z=pd.DataFrame(KNN(k=3).fit_transform(z),columns=z.columns)\n    return(z.loc[0,i])","16359a3f":"def mean(t,i):\n    z=t\n    z.loc[0,i]=np.NaN\n    z=z.loc[:,i].fillna(z.loc[:,i].mean())\n    return(z[0])","2bd9b019":"def median(t,i):\n    z=t\n    z.loc[0,i]=np.NaN\n    z=z.loc[:,i].fillna(z.loc[:,i].median())\n    return(z[0])","5a5bbeba":"# Function for imputing the missing values.\n# Here we have first stored a non null value of a particular column stored it in a separate variable and replaced it in the dataframe with nan.\n# Then we have imputed the missing value using the mean and median and depending upon which method imputes the value closes to the actual value is used for imputing the missing values in the dataset.\ndef impute(t):\n    for i in miss_val:\n            if(sum(t.loc[:,i].isnull())!=0):\n                p=mean(t,i)\n                q=median(t,i)\n                r=knn(t,i)\n                if(abs(p-t.loc[0,i]) < abs(q-t.loc[0,i]) and abs(p-t.loc[0,i]) < abs(r-t.loc[0,i])):\n                    t.loc[:,i]=t.loc[:,i].fillna(t.loc[:,i].mean())\n                elif(abs(q-t.loc[0,i]) < abs(p-t.loc[0,i]) and abs(q-t.loc[0,i]) < abs(r-t.loc[0,i])):\n                    t.loc[:,i]=t.loc[:,i].fillna(t.loc[:,i].median())\n                else:\n                    t=pd.DataFrame(KNN(k=3).fit_transform(t),columns=t.columns)\n            else:\n                continue\n    return(t)      ","8dbb7bf7":"miss_val=['Age']\ndf=impute(df1)","598d90fd":"df.info()","89d59c8b":"train.loc[:,['Survived','Pclass','Age','SibSp','Parch','Fare']]=df","6a8f5f2b":"train.info()","528c0c0c":"train['Embarked'].value_counts()","c6382c92":"k=train","7b3f304e":"train[train['Embarked'].isnull()]","172dd745":"train.loc[:,'Embarked']=train.loc[:,'Embarked'].fillna('S')","3992884b":"train.info()","b98c51fe":"test.info()","fa7e219e":"test=test.drop(['Cabin'],axis=1)\ntest['PassengerId']=test['PassengerId'].astype(str)","2f082274":"test.info()","d07bb7f3":"miss_val=['Age','Fare']","27741376":"df1=test[['Pclass','Age','SibSp','Parch','Fare']]","bac950ec":"df=impute(df1)","fa3a35a5":"df.info()","1f420171":"test.loc[:,['Pclass','Age','SibSp','Parch','Fare']]=df","f8eb33ab":"test.info()","bcfa4552":"#train=k\ntrain.info()","29dcd962":"train['Survived']=train['Survived'].astype(int)\ntrain['Pclass']=train['Pclass'].astype(int)\ntrain['SibSp']=train['SibSp'].astype(int)\ntrain['Parch']=train['Parch'].astype(int)\ntest['Pclass']=test['Pclass'].astype(int)\ntest['SibSp']=test['SibSp'].astype(int)\ntest['Parch']=test['Parch'].astype(int)","d74d0e04":"train=train.drop(['Ticket'],axis=1)\ntest=test.drop(['Ticket'],axis=1)","82bbef4f":"bins=np.arange(0,train.Age.max()+10,10)\nplt.hist(train['Age'],rwidth=0.6,bins=bins)\nplt.title('Distribution of the dataset according to Age.')\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.show();","10e66165":"bins=np.arange(0,train.Fare.max()+10,50)\nplt.hist(train['Fare'],rwidth=0.6,bins=bins)\nplt.title('Distribution of the dataset according to Fare.')\nplt.xlabel('Fare')\nplt.ylabel('Count')\nplt.show();","b52889aa":"bins=np.arange(0,train.SibSp.max()+1,1)\nplt.hist(train['SibSp'],rwidth=0.6,bins=bins)\nplt.title('Distribution of the dataset according to SibSp.')\nplt.xlabel('SibSp')\nplt.ylabel('Count')\nplt.show();","c8dfce66":"bins=np.arange(0,train.Parch.max()+1,1)\nplt.hist(train['Parch'],rwidth=0.6,bins=bins)\nplt.title('Distribution of the dataset according to Parch.')\nplt.xlabel('Parch')\nplt.ylabel('Count')\nplt.show();","f827d383":"sb.countplot(data=train,x='Survived',hue='Sex');","a38c22da":"sb.countplot(data=train,x='Survived',hue='Pclass');","8ed4a802":"plt.figure(figsize=[12,7])\nsb.violinplot(data=train,x='Pclass',y='Fare')\nplt.show();","1d8b8d97":"# Analysing each continuous variable for presence of outliers using boxplot.\nplt.figure(figsize=[10,5])\nplt.boxplot([train['Age'],train['Fare'],train['SibSp'],train['Parch']])\nplt.xlabel(['1. Age', '2. Fare', '3. SibSp', '4. Parch'])\nplt.title(\"BoxPlot of the continuous Variables\")\nplt.ylabel('Values');","ddf67770":"train.info()","4dec2f1d":"df1=train[['Age','SibSp','Parch','Fare','Survived']]","73757d80":"df1.head(3)","15360c1e":"from scipy import stats\ncnames=['Age','SibSp','Parch','Fare']\nfor i in cnames:\n    f, p = stats.f_oneway(df1[i], df1['Survived'])\n    print(\"P value for variable \"+str(i)+\" is \"+str(p))","3a2560b7":"f, ax = plt.subplots(figsize=(20, 8))\ncorr = df1.corr()\nsb.heatmap(corr, mask=np.zeros_like(corr,dtype=np.bool),cmap=sb.diverging_palette(220, 10, as_cmap=True),annot=True,ax=ax,);","fdf88e17":"df1['logage']=np.log(df1['Age'])","b6209d19":"df1['sqrtfare']=np.sqrt(df1['Fare'])","2403f5db":"df1['sqrtage']=np.sqrt(df1['Age'])","d2420a30":"df1.head(2)","82334abc":"from scipy import stats\ncnames=['Age','SibSp','Parch','Fare','sqrtfare','logage','sqrtage']\nfor i in cnames:\n    f, p = stats.f_oneway(df1[i], df1['Survived'])\n    print(\"P value for variable \"+str(i)+\" is \"+str(p))","ca65cb44":"from scipy.stats import chi2_contingency","d5326cbd":"train['Pclass']=train['Pclass'].astype(str)\ncat_names=['Embarked','Sex','Pclass']\nfor i in cat_names:\n    print(i)\n    chi2,p,dof,ex=chi2_contingency(pd.crosstab(train['Survived'],train[i]))\n    print(p)","7d20cd8c":"train['log_age']=np.log(train['Age'])\ntrain['sqrt_age']=np.sqrt(train['Age'])\ntrain['sqrt_fare']=np.sqrt(train['Fare'])\ntest['log_age']=np.log(test['Age'])\ntest['sqrt_age']=np.sqrt(test['Age'])\ntest['sqrt_fare']=np.sqrt(test['Fare'])","e4c07430":"train.info()","ede7fe2e":"k=train['Embarked'].value_counts()\nk.plot(kind='pie',figsize=(20,10),legend=True)\nplt.legend(loc=0,bbox_to_anchor=(1.5,0.5));","65685db9":"k=train['Sex'].value_counts()\nk.plot(kind='pie',figsize=(20,10),legend=True)\nplt.legend(loc=0,bbox_to_anchor=(1.5,0.5));","9726954c":"k=train['Pclass'].value_counts()\nk.plot(kind='pie',figsize=(20,10),legend=True)\nplt.legend(loc=0,bbox_to_anchor=(1.5,0.5));","4b7ad9c9":"train.head()","00dcd0b5":"df=train[['Survived','Pclass','Sex','Embarked','Age','SibSp','Parch','Fare','log_age','sqrt_age','sqrt_fare']]\ndf=pd.get_dummies(df)","39d86c60":"df.info()","17300d46":"df=df.drop(['Embarked_Q','Sex_female','Pclass_2'],axis=1)","eab6cfaa":"df.columns","40420c06":"bins=np.arange(0,train.sqrt_fare.max()+1,1)\nplt.hist(train['sqrt_fare'],rwidth=0.6,bins=bins)\nplt.title('Distribution of the dataset according to Age.')\nplt.xlabel('sqrt_fare')\nplt.ylabel('Count')\nplt.show();","85f9524d":"bins=np.arange(0,train.sqrt_age.max()+1,1)\nplt.hist(train['sqrt_age'],rwidth=0.6,bins=bins)\nplt.title('Distribution of the dataset according to Age.')\nplt.xlabel('sqrt_age')\nplt.ylabel('Count')\nplt.show();","c34f5e48":"X=df[['Age', 'SibSp', 'sqrt_fare', 'Pclass_1', 'Pclass_3', 'Sex_male','Embarked_C', 'Embarked_S']]\ny=df['Survived']","f1e2260a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)","67001e41":"from sklearn.linear_model import LogisticRegression\nclf_LR = LogisticRegression(random_state = 123,tol=1e-3,C=1,solver='lbfgs')\nclf_LR.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred_LR = clf_LR.predict(X_test)\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred_LR)\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y_test,y_pred_LR))","9c0c4c20":"cm","ed7e43db":"from sklearn.metrics import r2_score\nr2_score(y_test, y_pred_LR)","300b6bf9":"from sklearn.tree import DecisionTreeClassifier","dfdb5c33":"clf_DT=DecisionTreeClassifier(criterion='entropy',random_state=123,)\nclf_DT.fit(X_train,y_train)","9529ac25":"y_pred_DT = clf_DT.predict(X_test)","98b09911":"from sklearn import metrics\nprint(metrics.accuracy_score(y_test,y_pred_DT))","f62307f9":"cm = confusion_matrix(y_test, y_pred_DT)\ncm","40f71535":"r2_score(y_test, y_pred_DT)","017a95ad":"from sklearn.ensemble import RandomForestClassifier","c679c4a7":"clf_RF=RandomForestClassifier(n_estimators=5000,criterion='gini')\nclf_RF.fit(X_train,y_train)","f2bcc6e7":"y_pred_RF = clf_RF.predict(X_test)","63b0e178":"from sklearn import metrics\nprint(metrics.accuracy_score(y_test,y_pred_RF))","53dab090":"cm = confusion_matrix(y_test, y_pred_RF)\ncm","af4a0119":"r2_score(y_test, y_pred_RF)","ea13c641":"from sklearn.neighbors import KNeighborsClassifier","8f045148":"clf_knn=KNeighborsClassifier(n_neighbors=3,algorithm='ball_tree')\nclf_knn.fit(X_train,y_train)","a94f3cd2":"y_pred_knn = clf_knn.predict(X_test)","1966f2d3":"from sklearn import metrics\nprint(metrics.accuracy_score(y_test,y_pred_knn))","054671b3":"cm = confusion_matrix(y_test, y_pred_knn)\ncm","fd9f8b45":"r2_score(y_test,y_pred_knn)","f2a0054d":"from sklearn.svm import SVC","fbe68153":"clf_svm=SVC(kernel='rbf',random_state=123,C=100)\nclf_svm.fit(X_train,y_train)","3c44e6bb":"y_pred_svc = clf_svm.predict(X_test)","e56f4976":"from sklearn import metrics\nprint(metrics.accuracy_score(y_test,y_pred_svc))","8210e85e":"cm = confusion_matrix(y_test, y_pred_svc)\ncm","63fe2516":"r2_score(y_test,y_pred_svc)","c6a4d6a8":"from sklearn.naive_bayes import GaussianNB","ba0b27df":"clf_NB=GaussianNB()\nclf_NB.fit(X_train,y_train)","950bee09":"y_pred_NB = clf_NB.predict(X_test)","d2af22e8":"from sklearn import metrics\nprint(metrics.accuracy_score(y_test,y_pred_NB))","adf549cd":"cm = confusion_matrix(y_test, y_pred_NB)\ncm","270bf16e":"r2_score(y_test,y_pred_NB)","05b3961b":"from xgboost import XGBClassifier\nclf_XGB=XGBClassifier()","dcce9984":"clf_XGB.fit(X_train,y_train)","69d72de0":"y_pred_XGB = clf_XGB.predict(X_test)","ad278121":"from sklearn import metrics\nprint(metrics.accuracy_score(y_test,y_pred_XGB))","31bbb992":"cm = confusion_matrix(y_test, y_pred_XGB)\ncm","ede8b5d5":"r2_score(y_test,y_pred_XGB)","713558d7":"from sklearn.model_selection import RandomizedSearchCV","59e55e1f":"# Creating a dictionary of parameters for tuning Decision Tree\nz={'max_depth':[2,3,5,7,10],'min_samples_leaf':[2,5,7,10,15],'min_samples_split':[2,5,7,10],'max_features':\n   ['auto','sqrt','log2'],'criterion':['entropy','gini']}","a2d262a7":"classifier = RandomizedSearchCV(clf_DT,param_distributions=z, random_state=1)","6d68887a":"best_model = classifier.fit(X_train, y_train)","f0b0841d":"best_model.best_params_","cac20e13":"clf_DT=DecisionTreeClassifier(criterion='gini',min_samples_split=2,min_samples_leaf=5,max_features='log2',\n                              max_depth=3,random_state=123)\nclf_DT.fit(X_train,y_train)","c35703fa":"y_pred_DT = clf_DT.predict(X_test)","4e8d304a":"print(metrics.accuracy_score(y_test,y_pred_DT))","a09bd03e":"cm = confusion_matrix(y_test, y_pred_DT)\ncm","59797b76":"r2_score(y_test,y_pred_DT)","de22d905":"# Creating a dictionary of parameters for tuning Random Forest\nz={'n_estimators':[1000,5000,10000,50000],'max_depth':[2,3,5,7,10,15],'min_samples_leaf':[2,3,5,7,10,15],'min_samples_split':[2,3,5,7,10,15],'max_features':\n   ['auto','sqrt','log2'],'oob_score':[True],'n_jobs':[-1],'criterion':['entropy','gini']}","8d31f5e5":"classifier = RandomizedSearchCV(clf_RF,param_distributions=z, random_state=1)","61abecc1":"best_model = classifier.fit(X_train, y_train)","dee1bb3b":"best_model.best_params_","96340c3a":"clf_RF=RandomForestClassifier(n_estimators=1000,max_depth=15,min_samples_leaf=2,min_samples_split=10,n_jobs=-1,\n                              oob_score=True,max_features='log2',criterion='gini')\nclf_RF.fit(X_train,y_train)","98853daa":"y_pred_RF = clf_RF.predict(X_test)\nprint(metrics.accuracy_score(y_test,y_pred_RF))\ncm = confusion_matrix(y_test, y_pred_RF)\nr2_score(y_test,y_pred_RF)","cf47ae09":"# Creating a dictionary of parameters for tuning Support Vector Machines\nz={'C':[1,10,50,70,100,1000],'gamma':[0.01,0.1,0.05,0.5,1],'kernel':['linear','rbf','poly'],\n   'tol':[0.001,0.05,0.005,0.01]}","4742e672":"classifier = RandomizedSearchCV(clf_svm,param_distributions=z, random_state=123)","65969208":"best_model = classifier.fit(X_train, y_train)","762f6b70":"best_model.best_params_","914b5988":"clf_svm=SVC(n_estimators=1000,max_depth=15,min_samples_leaf=2,min_samples_split=10,n_jobs=-1,\n                              oob_score=True,max_features='log2',criterion='gini')\nclf_svm.fit(X_train,y_train)","015df509":"y_pred_svc = clf_svm.predict(X_test)","e3ef2a29":"print(metrics.accuracy_score(y_test,y_pred_svc))\ncm = confusion_matrix(y_test, y_pred_svc)\nr2_score(y_test,y_pred_svc)","4612a09d":"# Creating a dictionary of parameters for tuning XGBoost Classfier\nz={'n_estimators':[100,500,1000,2500,5000],'learning_rate':[0.001,0.005,0.01,0.05,0.1,0.5]}","79f2ac6b":"classifier = RandomizedSearchCV(clf_XGB,param_distributions=z, random_state=123)","862b5bd8":"best_model = classifier.fit(X_train, y_train)","96b7eed0":"best_model.best_params_","085f8c07":"clf_XGB=XGBClassifier(n_estimators=1000,learning_rate=)\nclf_XGB.fit(X_train,y_train)","d30686b2":"y_pred_XGB = clf_XGB.predict(X_test)\nprint(metrics.accuracy_score(y_test,y_pred_XGB))\ncm = confusion_matrix(y_test, y_pred_XGB)\nr2_score(y_test,y_pred_XGB)","07c3f9a3":"test['Pclass']=test['Pclass'].astype(str)","da755f53":"test.info()","89f84560":"test.columns","e992c314":"df=test[['Age', 'SibSp', 'sqrt_fare', 'Pclass', 'Sex','Embarked']]\ndf=pd.get_dummies(df,drop_first=True)","954e07ee":"df.columns","998b2a32":"X_test=df[['Age', 'SibSp', 'sqrt_fare', 'Pclass_2', 'Pclass_3', 'Sex_male','Embarked_Q', 'Embarked_S']]","0ecd34d4":"predictions=clf_NB.predict(X_test)","932da737":"Y=gender['Survived']","dd674996":"print(metrics.accuracy_score(Y,predictions))","f95aec48":"r2_score(Y,predictions)","6f4cb1f6":"submiss=gender","5f45e95c":"submiss['Survived']=predictions","7b4bafab":"submiss.info()","32d07ffb":"submiss.to_csv('mycsvfile.csv',index=False)","2a718938":"predictions=y_pred.to_csv()","4f187a32":"### Define\n\n> PasseingerId column is of int type rather it should be of str type.","c8e3e6a1":"## Introduction\n\n### Describing the meaning of the features given the both train & test datasets.\n<h4>Variable Definition Key.<\/h4>\n \n> - Survival\n\n> - 0= No\n> - 1= Yes\n\n> - pclass (Ticket class)\n\n> - 1=1st\n> - 2=2nd\n> - 3=3rd\n \n- sex\n<br>\n\n- age\n\n\n- sibsp (# of siblings \/ spouses aboard the Titanic)\n<br>\n- parch (# of parents \/ children aboard the Titanic)\n<br>\n- tickets\n<br>\n- fare\n<br>\n- cabin\n- embarked Port of Embarkation.\n - C = Cherbourg,\n - Q = Queenstown,\n - S = Southampton\n- pclass: A proxy for socio-economic status (SES)\n<br>\n<h4>Passenger Class.<\/h4>\n\n> - 1st = Upper\n> - 2nd = Middle\n> - 3rd = Lower\n","5909157c":"## Preprocessing the for performing logistics regression ","274388fb":"### Dataset Issues\n\n> PassengerId column is of int type.\n\n> Cabin information missing for most of the passengers.\n\n> For the age column also dataset is missing which can prove to be a deciding factor in predicting survival.\n\n> Two passengers having no information of Embarked ports.\n","72d9ecfa":"## Cleaning the Dataset","b8c7befa":"#### The task can be completed either using loops or functions. Functions have been used here to reduce the execution time.","e0485ad1":"> The histogram for age distribution shows that majority of the dataset contains people of age between 20 to 30, followed by those between 30 and 40. \n\n> First class passengers were preferred over other passsengers at the time of rescue,as the number of third class passengers are high among those who did not survive.\n\n> The violinplot shows that distribution of fares of passengers depending upon their socio-economic status (Pclass).From the plot it is clear that passengers belonging to first class have the highest fares along with presence of outliers beyond 500. Those belonging to second class have their fares between 0 to 100.And those belonging to third class majority of their fares are below 50 and the distribution of fare is unimodal. \nThus it can be concluded that fares for passengers of First class are higher than that of other class also they belong to the upper class of society. \n\n> In the above regression analysis firstly regression has been performed on three features namely gender,age and passenger class where the accuracy of training and testing set were 79.6% and 73.9% respectively.\n\n> Here different models have been built using different algorithms.On comparing the accuracies of different models on test set we can see that naive bayes model has the maximum accuracy of about 99.5%.","47ba68bb":"### Decision Tree","021e2526":"#### Test","3c7a4bac":"## Exploring the dataset","92e17bd8":"### K Nearest Neighbors(KNN)","087bb1ea":"> The bar chart above compares the passengers survived or not on gender basis.\n\n> From above it is clear that among those who survived females are high in number as compared to males and among who did not survive males are high in number.","9c8a1835":"> From the above histogram it is clear that majority of the dataset contains people of age between 20 to 30, followed by those between 30 and 40. ","7075e165":"## Linear Model","d531ec63":"## Conclusion","743b92c3":"### Random Forest","5a06875d":"### Naive Bayes","19d3f445":"## Titanic Dataset Analysis\n\n> Here the standard titanic dataset has been analysed and reuslts have been portrayed using visualisations along with statistical inferences. Also different models have been created using different algorithms and their accuracy has been printed. \n## Table of Contents\n<ul>\n<li><a href=\"#intro\">Introduction<\/a><\/li>\n<li><a href=\"#wrangling\">Assessing<\/a><\/li>\n<li><a href=\"#wrangling\">Cleaning<\/a><\/li>\n<li><a href=\"#wrangling\">Feature Engineering<\/a><\/li>    \n<li><a href=\"#eda\">Exploratory Analysis<\/a><\/li>\n<li><a href=\"#eda\">Regression Analysis<\/a><\/li>\n<li><a href=\"#eda\">Predictive Analysis<\/a><\/li>    \n<li><a href=\"#conclusions\">Conclusions<\/a><\/li>    \n<\/ul>","7092df64":"## Assessing the data","03b026c3":"### Cleaning of testing dataset","3fe8da29":"> The bar chart above compares the passengers survived or not on socio economic status.\n\n> The above chart shows that first class passengers were preferred over other passsengers at the time of rescue,which is quite evident from above graph as the number of third class passengers are high among those who did not survive.\n","c80b47f9":"## Logistic Regression using different algorithms and their accuracy on the test set.","a17a73eb":"### SVM Kernel","2f6fe962":"#### Code"}}