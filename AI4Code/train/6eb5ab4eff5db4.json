{"cell_type":{"7243ca52":"code","9402ab3c":"code","a61b3364":"code","0ed7ce54":"code","2b2b5782":"code","cf1bfde1":"code","8e04b1c1":"code","d4ed4481":"code","e3563a37":"code","098e7688":"code","165f18c3":"code","50c295ff":"code","8ee37681":"code","dd189665":"code","b266ad3b":"code","a3d81ec9":"code","17a3cf39":"code","66c023a9":"code","10e032d7":"code","b462f389":"code","49d95f33":"code","19601fc0":"code","660cff85":"code","21ec5726":"code","98e1d4aa":"code","ba78131c":"code","543c3518":"code","91f6bacd":"markdown","df97606e":"markdown","e77028fc":"markdown","532b7b3c":"markdown","c67c8209":"markdown","610edbb5":"markdown","b51fd2ee":"markdown","6da7102a":"markdown","fec5dfb2":"markdown","4d271747":"markdown","9c9b8ddd":"markdown","20ba9ad3":"markdown","acf75a76":"markdown","7d4e09ed":"markdown","512d92cc":"markdown"},"source":{"7243ca52":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport sklearn\nprint (sklearn.__version__)\n# suppress warnings\nimport warnings  \nwarnings.filterwarnings('ignore')","9402ab3c":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df  = pd.read_csv(\"..\/input\/test.csv\")\n# use both datasets for inspection by saving them in a new dataframe df\ndf = pd.concat([train_df,test_df], axis=0, ignore_index=False) \nprint('Size of training set: {:d}'.format(len(train_df)))\nprint('Size of test set: {:d}'.format(len(test_df)))","a61b3364":"df.head(10)","0ed7ce54":"train_df.describe()","2b2b5782":"# print summary of missing fields in the training set\nprint(\"Training set\\n\")\nprint(train_df.isnull().sum(axis=0))\nprint(\"\\n\")\nprint(\"Train and test set\\n\")\nprint(df.isnull().sum(axis=0))\n# drop Cabin \ntrain_df = train_df.drop(['Cabin'], axis=1)\ndf = df.drop(['Cabin'], axis=1)","cf1bfde1":"print('Missing values per column in %')\nprint(((1 - train_df.count()\/len(train_df.index))*100).apply(lambda x: '{:.1f}%'.format(x)))","8e04b1c1":"# look at the row containing awkward minimum value (.42) for Age\ntrain_df.loc[train_df['Age'].idxmin(axis=1)]","d4ed4481":"# look at a few records with missing Age\ntrain_df[train_df.Age.isnull()].head()","e3563a37":"train_df['Age'] = train_df.groupby(['Pclass'])['Age'].transform(lambda x: x.fillna(x.mean()))\ndf['Age'] = df.groupby(['Pclass'])['Age'].transform(lambda x: x.fillna(x.mean()))\n# check -- now there should be no rows with null Age fields\ntrain_df[train_df.Age.isnull()].head()","098e7688":"train_df['Fare'] = train_df.groupby(['Pclass'])['Fare'].transform(lambda x: x.fillna(x.mean()))\ndf['Fare'] = df.groupby(['Pclass'])['Fare'].transform(lambda x: x.fillna(x.mean()))\n# check -- now there should be no rows with null Age fields\ntrain_df[train_df.Fare.isnull()].head()","165f18c3":"# just two records with missing `Embarked` data\nprint('Number of records with missing Embarked data: {}'.format(len(train_df[train_df.Embarked.isnull()])))\n\n# check occurrences of different types of embarkment\ntrain_df['Embarked'].value_counts(dropna=False)","50c295ff":"# fill the two missing values with the most frequent value, which is \"S\".\ntrain_df[\"Embarked\"] = train_df[\"Embarked\"].fillna(\"S\")\ndf['Embarked'].value_counts(dropna=False)","8ee37681":"# any more nans?\nlen(train_df[pd.isnull(train_df).any(axis=1)])","dd189665":"train_df[train_df['Survived']==0]","b266ad3b":"train_df[train_df['Survived']==0]","a3d81ec9":"# add `FirstName` and `LastName` columns that will be needed later\ntrain_df['LastName'],train_df['FirstName'] = train_df['Name'].str.split(',', 1).str\ndf['LastName'],df['FirstName'] = df['Name'].str.split(',', 1).str","17a3cf39":"# foreign names\ntrain_df['Foreign'] = False\ntrain_df['Foreign'] = train_df['LastName'].str.endswith((\"ic\", \"sson\", \"ff\", \"i\", \"o\", \"u\", \"ski\", \"a\"))\ndf['Foreign'] = False\ndf['Foreign'] = df['LastName'].str.endswith((\"ic\", \"sson\", \"ff\", \"i\", \"o\", \"u\", \"ski\", \"a\"))\ntrain_df[train_df['Foreign']].head(10)","66c023a9":"# are names ending in \"ff\" Russian?\n# how many of them survived?\ntrain_df[['FirstName', 'LastName', 'Survived']][train_df['LastName'].str.endswith(\"ff\")].head()","10e032d7":"# none survived\ntrain_df[train_df['LastName'].str.endswith(\"ff\") & train_df['Survived']>0]","b462f389":"# also for names ending in \"ic\" there are very few (1\/20) survivors\ntrain_df[train_df['LastName'].str.endswith(\"ic\")]['Survived'].value_counts()","49d95f33":"train_df['Name_len'] = train_df['Name'].apply(lambda x: len(x)).astype(int)\ntrain_df['Name_end'] = train_df['LastName'].str[-1:]\ndf['Name_len'] = df['Name'].apply(lambda x: len(x)).astype(int)\ndf['Name_end'] = df['LastName'].str[-1:]","19601fc0":"# ADD FEATURES\nprint(df.columns)\ndummy_features=['Age', 'Embarked', 'Fare', 'Name', 'Parch', 'Pclass', 'Sex', 'SibSp',\n       'Survived', 'Ticket', 'LastName', 'FirstName', \n       'Name_len', 'Foreign']\n\ndf_dummies = df[dummy_features]\ndf = pd.get_dummies(df[dummy_features])\ntrain_features = df.iloc[:891,:]\ntrain_labels = train_features.pop('Survived').astype(int)\ntest_features = df.iloc[891:,:].drop('Survived',axis=1)","660cff85":"print(df.columns)","21ec5726":"from sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier","98e1d4aa":"models=[KNeighborsClassifier(), LogisticRegression(), GaussianNB(), SVC(), DecisionTreeClassifier(),\n        RandomForestClassifier(), GradientBoostingClassifier(), AdaBoostClassifier()]\nnames=['KNN', 'LR', 'NB', 'SVM', 'Tree', 'RF', 'GB', 'Ada']\nfor name,model in zip(names, models):\n    score = cross_val_score(model, train_features, train_labels, cv=5)\n    print('{} :: {} , {}'.format(name, score.mean(), score))","ba78131c":"models=[LogisticRegression(),RandomForestClassifier()]\nnames=['LR','RF']\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ntrain_features_scaled = scaler.fit(train_features).transform(train_features)\ntest_features_scaled = scaler.fit(test_features).transform(test_features)\nfor name,model in zip(names,models):\n    score = cross_val_score(model,train_features_scaled,train_labels,cv=5)\n    print('{} :: {} , {}'.format(name,score.mean(),score))","543c3518":"# Initialize the model class\nmodel = RandomForestClassifier()\n\n# Train the algorithm using all the training data\nmodel.fit(train_features,train_labels)\n\n# Make predictions using the test set.\npredictions = model.predict(test_features)\n\n# Create a new dataframe with only the columns Kaggle wants from the dataset.\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df['PassengerId'],\n        \"Survived\": predictions\n    })\n\n# uncomment to save submission file\n# submission.to_csv(\".\/output\/RandomForestClassifierNormName.csv\", index=False)","91f6bacd":"For `Embarked` we look at the occurrences of each value and replace null values with the most frequent one, which is \"S\".","df97606e":"## Load and preview data \nLoad the Titanic train & test csv files into DataFrames `train_df` and `test_df`. \nWe're also creating a DataFrame `df` that contains both train and test data by concatenating both datasets.","e77028fc":"With this kernel I got my currently best score of 80%","532b7b3c":"## Enrich data\nBy looking at non survivors I spotted some foreign looking names. So we're going to look at names. Names ending with \"ff\" sound Russian. There are also other names that sound foreign, namely those ending in \"sson\" (Swedish?) \"ic\" and \"ff\" (Russian?), and some other endings: , \"i\", \"o\", \"u\", \"ski\", \"a\". I'm going to use these name endings to populate a new column `Foreign`.\n\nI'm also going to add a `Name_len` column with the name length and a column `Name_end` with the last character of the family name (not sure how these columns might contribute to the model).","c67c8209":"## Build a predictive model\n\nThis time we're going to use `scikit-learn`, evaluate a few models on our dataset and choose the best for our submission.\n","610edbb5":"## Clean up the data\nLook for null fields in the data. The `Cabin` data seems to be pretty incomplete. Since I can't think of a way to use such information anyway, I'm going to drop that column. \n\nThere are two missing records for `Embarked` in the training set and one missing record for `Fare` in the test set.","b51fd2ee":"We're going to try out a few models and rate them using cross-evaluation.","6da7102a":"Take a look at all data by showing some statistics. ","fec5dfb2":"Take a look at data again -- notice there are many missing values in the `Age` column.\nShow percent of missing values per column.","4d271747":"Replace missing `Age` fields with mean age (computed by class). Same with missing `Fare`.","9c9b8ddd":"Take a look at some data by showing the first 10 lines of `df`. We can already see that a few Cabin values are missing.","20ba9ad3":"Logistic regression and Random Forest seem to produce the best models, so we will use these methods again to build models. But this time we are going to first scale the features using a [`StandardScaler`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html).","acf75a76":"Are there any more `NaN`s in the training set?","7d4e09ed":"Add columns `Name_len` and `Name_end` (last character of the last name).","512d92cc":"## Imports\nImport here all libraries that will be needed for loading, inspecting, and cleaning the data: pandas, numpy, matplotlib.pyplot, and sklearn. Show version of sklearn."}}