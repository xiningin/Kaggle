{"cell_type":{"87440960":"code","739f5259":"code","2a3fd6c2":"code","fd7138d6":"code","9d283796":"code","f37adfdb":"code","638c7999":"code","b8a7b9a0":"code","bdb399af":"code","1b417ff6":"code","0944e70a":"code","8f1b49b0":"code","f8e21cbc":"code","4dd06caf":"code","83ad68de":"code","1f1a2b18":"code","6ae357a2":"code","5c77d64f":"code","e6158e1b":"code","30b665fd":"code","5d6016fc":"code","866cdbe9":"code","4f8af717":"code","fbe88745":"code","3fb1b980":"code","f6e60b14":"code","3c90e35c":"code","a568a697":"code","f203343d":"code","84cc8d00":"code","ba4f1677":"code","bec8209f":"code","16b03b6d":"code","0ca09e03":"code","5fac1382":"markdown","8f45bfb7":"markdown","d6b1082d":"markdown","ec072fba":"markdown","ae5a2252":"markdown","676c608b":"markdown","f791ac95":"markdown","7fdfe49f":"markdown","17112e80":"markdown","826a50d7":"markdown","894a43c5":"markdown","d90731d8":"markdown","cd1f6ddc":"markdown","20a4d388":"markdown","9896ec25":"markdown","a67cef32":"markdown"},"source":{"87440960":"import os\nimport gc\nimport datetime\n\nimport pandas as pd #Analysis \nimport matplotlib.pyplot as plt #Visulization\nimport seaborn as sns #Visulization\nimport numpy as np #Analysis \nfrom scipy.stats import norm #Analysis \nfrom sklearn.preprocessing import StandardScaler #Analysis \nfrom scipy import stats #Analysis \n%matplotlib inline\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nfrom scipy.optimize import minimize\n\nimport lightgbm as lgb\nimport catboost as cb\nimport xgboost as xgb\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nprint(os.listdir('..\/input'))","739f5259":"train = pd.read_csv(\"..\/input\/train.csv\", parse_dates=[\"first_active_month\"])\nprint(\"shape of train : \",train.shape)\n\ntest = pd.read_csv(\"..\/input\/test.csv\", parse_dates=[\"first_active_month\"])\nprint(\"shape of test : \",test.shape)","2a3fd6c2":"def missing_impute(df):\n    for i in df.columns:\n        if df[i].dtype == \"object\":\n            df[i] = df[i].fillna(\"other\")\n        elif (df[i].dtype == \"int64\" or df[i].dtype == \"float64\"):\n            df[i] = df[i].fillna(df[i].mean())\n        else:\n            pass\n    return df\n\n\ndef datetime_extract(df, dt_col='first_active_month'):\n    df['date'] = df[dt_col].dt.date \n    df['day'] = df[dt_col].dt.day \n    df['dayofweek'] = df[dt_col].dt.dayofweek\n    df['dayofyear'] = df[dt_col].dt.dayofyear\n    df['days_in_month'] = df[dt_col].dt.days_in_month\n    df['daysinmonth'] = df[dt_col].dt.daysinmonth \n    df['month'] = df[dt_col].dt.month\n    df['week'] = df[dt_col].dt.week \n    df['weekday'] = df[dt_col].dt.weekday\n    df['weekofyear'] = df[dt_col].dt.weekofyear\n    # df['year'] = train[dt_col].dt.year\n    \n    df['elapsed_time'] = (datetime.date(2018, 2, 1) - df['date']).dt.days\n\n    return df","fd7138d6":"# Do impute missing values for train & test\nfor df in [train, test]:\n    missing_impute(df)\n    \n# Do extract datetime values for train & test\ntrain = datetime_extract(train, dt_col='first_active_month')\ntest = datetime_extract(test, dt_col='first_active_month')","9d283796":"train.shape, test.shape","f37adfdb":"train.head()","638c7999":"ht = pd.read_csv(\"..\/input\/historical_transactions.csv\")\nprint(\"shape of historical_transactions: \", ht.shape)","b8a7b9a0":"ht['authorized_flag'] = ht['authorized_flag'].map({'Y':1, 'N':0})","bdb399af":"# Do impute missing values for history\nht = missing_impute(ht)","1b417ff6":"def aggregate_historical_transactions(history):\n    \n    history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).\\\n                                      astype(np.int64) * 1e-9\n    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'city_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std', 'mean', 'size'],\n        'installments': ['sum', 'median', 'max', 'min', 'std', 'mean', 'size'],\n        'purchase_date': [np.ptp],\n        'month_lag': ['min', 'max']\n        }\n    agg_history = history.groupby(['card_id']).agg(agg_func)\n    agg_history.columns = ['hist_' + '_'.join(col).strip() \n                           for col in agg_history.columns.values]\n    agg_history.reset_index(inplace=True)\n    \n    df = (history.groupby('card_id')\n          .size()\n          .reset_index(name='hist_transactions_count'))\n    \n    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n    \n    return agg_history\n\nhistory = aggregate_historical_transactions(ht)\ndel ht\ngc.collect()","0944e70a":"history.head()","8f1b49b0":"train.shape, test.shape","f8e21cbc":"train = pd.merge(train, history, on='card_id', how='left')\ntest = pd.merge(test, history, on='card_id', how='left')","4dd06caf":"merchant = pd.read_csv(\"..\/input\/merchants.csv\")\nprint(\"shape of merchant: \", merchant.shape)\n\nnew_merchant = pd.read_csv(\"..\/input\/new_merchant_transactions.csv\")\nprint(\"shape of new_merchant_transactions: \", new_merchant.shape)","83ad68de":"new_merchant['authorized_flag'] = new_merchant['authorized_flag'].map({'Y':1, 'N':0})","1f1a2b18":"# Do impute missing values for merchant and new_merchant\nfor df in [merchant, new_merchant]:\n    missing_impute(df)","6ae357a2":"def aggregate_new_transactions(new_trans):    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'city_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std', 'mean', 'size'],\n        'installments': ['sum', 'median', 'max', 'min', 'std', 'mean', 'size'],\n        'month_lag': ['min', 'max']\n        }\n    agg_new_trans = new_trans.groupby(['card_id']).agg(agg_func)\n    agg_new_trans.columns = ['new_' + '_'.join(col).strip() \n                           for col in agg_new_trans.columns.values]\n    agg_new_trans.reset_index(inplace=True)\n    \n    df = (new_trans.groupby('card_id')\n          .size()\n          .reset_index(name='new_transactions_count'))\n    \n    agg_new_trans = pd.merge(df, agg_new_trans, on='card_id', how='left')\n    \n    return agg_new_trans\n\nnew_trans = aggregate_new_transactions(new_merchant)","5c77d64f":"train = pd.merge(train, new_trans, on='card_id', how='left')\ntest = pd.merge(test, new_trans, on='card_id', how='left')","e6158e1b":"# The target\ntarget = train['target']","30b665fd":"excluded_features = ['first_active_month', 'card_id', 'target', 'date']\nuse_cols = [col for col in train.columns if col not in excluded_features]\n\ntrain = train[use_cols]\ntest = test[use_cols]\n\nfeatures = list(train[use_cols].columns)\ncategorical_feats = [col for col in features if 'feature_' in col]","5d6016fc":"from sklearn.preprocessing import LabelEncoder\nfor col in categorical_feats:\n    print(col)\n    lbl = LabelEncoder()\n    lbl.fit(list(train[col].values.astype('str')) + list(test[col].values.astype('str')))\n    train[col] = lbl.transform(list(train[col].values.astype('str')))\n    test[col] = lbl.transform(list(test[col].values.astype('str')))","866cdbe9":"df_all = pd.concat([train, test])\ndf_all = pd.get_dummies(df_all, columns=categorical_feats)\n\nlen_train = train.shape[0]\n\ntrain = df_all[:len_train]\ntest = df_all[len_train:]","4f8af717":"# Check missing again\ntrain.isnull().sum()","fbe88745":"train.info()","3fb1b980":"# Final fill missing values\nfor col in train.columns:\n    for df in [train, test]:\n        if df[col].dtype == \"float64\":\n            print(col)\n            df[col] = df[col].fillna(df[col].mean())","f6e60b14":"lgb_params = {\"objective\" : \"regression\", \"metric\" : \"rmse\", \n               \"max_depth\": 9, \"min_child_samples\": 20, \n               \"reg_alpha\": 1, \"reg_lambda\": 1,\n               \"num_leaves\" : 64, \"learning_rate\" : 0.005, \n               \"subsample\" : 0.8, \"colsample_bytree\" : 0.8, \n               \"verbosity\": -1}\n\nFOLDs = KFold(n_splits=10, shuffle=True, random_state=100)\n\noof_lgb = np.zeros(len(train))\npredictions_lgb = np.zeros(len(test))\n\nfeatures_lgb = list(train.columns)\nfeature_importance_df_lgb = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train)):\n    trn_data = lgb.Dataset(train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx], label=target.iloc[val_idx])\n\n    print(\"LGB \" + str(fold_) + \"-\" * 50)\n    num_round = 10000\n    clf = lgb.train(lgb_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 50)\n    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx], num_iteration=clf.best_iteration)\n\n    fold_importance_df_lgb = pd.DataFrame()\n    fold_importance_df_lgb[\"feature\"] = features_lgb\n    fold_importance_df_lgb[\"importance\"] = clf.feature_importance()\n    fold_importance_df_lgb[\"fold\"] = fold_ + 1\n    feature_importance_df_lgb = pd.concat([feature_importance_df_lgb, fold_importance_df_lgb], axis=0)\n    predictions_lgb += clf.predict(test, num_iteration=clf.best_iteration) \/ FOLDs.n_splits\n    \nprint(np.sqrt(mean_squared_error(oof_lgb, target)))","3c90e35c":"cols = (feature_importance_df_lgb[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features = feature_importance_df_lgb.loc[feature_importance_df_lgb.feature.isin(cols)]\n\nplt.figure(figsize=(14,14))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","a568a697":"FOLDs = KFold(n_splits=10, shuffle=True, random_state=100)\nX = train\ny = target\n\noof_cb = np.zeros(len(train))\npredictions_cb = np.zeros(len(test))\n\nfor n_fold, (trn_idx, val_idx) in enumerate(FOLDs.split(X, y)):\n    X_train, y_train = X.iloc[trn_idx], y.iloc[trn_idx]\n    X_valid, y_valid = X.iloc[val_idx], y.iloc[val_idx]\n    \n    # CatBoost Regressor estimator\n    model = cb.CatBoostRegressor(\n        learning_rate = 0.03,\n        iterations = 1000,\n        eval_metric = 'RMSE',\n        allow_writing_files = False,\n        od_type = 'Iter',\n        bagging_temperature = 0.2,\n        depth = 10,\n        od_wait = 20,\n        silent = True\n    )\n    \n    # Fit\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n        verbose=None,\n        early_stopping_rounds=100\n    )\n    \n    print(\"CB \" + str(n_fold) + \"-\" * 50)\n    \n    oof_cb[val_idx] = model.predict(X_valid)\n    test_preds = model.predict(test)\n    predictions_cb += test_preds \/ FOLDs.n_splits\n\nprint(np.sqrt(mean_squared_error(oof_cb, target)))","f203343d":"xgb_params = {'eta': 0.005, 'max_depth': 9, 'subsample': 0.8, 'colsample_bytree': 0.8, \n          'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True}\n\nFOLDs = KFold(n_splits=10, shuffle=True, random_state=100)\n\noof_xgb = np.zeros(len(train))\npredictions_xgb = np.zeros(len(test))\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train)):\n    trn_data = xgb.DMatrix(data=train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = xgb.DMatrix(data=train.iloc[val_idx], label=target.iloc[val_idx])\n    watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n    print(\"XGB \" + str(fold_) + \"-\" * 50)\n    num_round = 10000\n    xgb_model = xgb.train(xgb_params, trn_data, num_round, watchlist, early_stopping_rounds=50, verbose_eval=1000)\n    oof_xgb[val_idx] = xgb_model.predict(xgb.DMatrix(train.iloc[val_idx]), ntree_limit=xgb_model.best_ntree_limit+50)\n\n    predictions_xgb += xgb_model.predict(xgb.DMatrix(test), ntree_limit=xgb_model.best_ntree_limit+50) \/ FOLDs.n_splits\n\nprint(np.sqrt(mean_squared_error(oof_xgb, target)))","84cc8d00":"def find_best_weight(preds, target):\n    def _validate_func(weights):\n        ''' scipy minimize will pass the weights as a numpy array '''\n        final_prediction = 0\n        for weight, prediction in zip(weights, preds):\n                final_prediction += weight * prediction\n        return np.sqrt(mean_squared_error(final_prediction, target))\n\n    #the algorithms need a starting value, right not we chose 0.5 for all weights\n    #its better to choose many random starting points and run minimize a few times\n    starting_values = [0.5]*len(preds)\n\n    #adding constraints and a different solver as suggested by user 16universe\n    #https:\/\/kaggle2.blob.core.windows.net\/forum-message-attachments\/75655\/2393\/otto%20model%20weights.pdf?sv=2012-02-12&se=2015-05-03T21%3A22%3A17Z&sr=b&sp=r&sig=rkeA7EJC%2BiQ%2FJ%2BcMpcA4lYQLFh6ubNqs2XAkGtFsAv0%3D\n    cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n    #our weights are bound between 0 and 1\n    bounds = [(0, 1)] * len(preds)\n    \n    res = minimize(_validate_func, starting_values, method='Nelder-Mead', bounds=bounds, constraints=cons)\n    \n    print('Ensemble Score: {best_score}'.format(best_score=(1-res['fun'])))\n    print('Best Weights: {weights}'.format(weights=res['x']))\n    \n    return res","ba4f1677":"print('lgb', np.sqrt(mean_squared_error(oof_lgb, target)))\nprint('xgb', np.sqrt(mean_squared_error(oof_xgb, target)))\nprint('cb', np.sqrt(mean_squared_error(oof_cb, target)))","bec8209f":"res = find_best_weight([oof_lgb, oof_cb, oof_xgb], target)","16b03b6d":"total_sum = 0.35864667 * oof_lgb + 0.59360413 * oof_cb + 0.14343413 * oof_xgb\nprint(\"CV score: {:<8.5f}\".format(np.sqrt(mean_squared_error(total_sum, target))))","0ca09e03":"sub_df = pd.read_csv('..\/input\/sample_submission.csv')\nsub_df[\"target\"] = 0.35864667 * predictions_lgb + 0.59360413 * predictions_cb + 0.14343413 * predictions_xgb\nsub_df.to_csv(\"submission_ensemble.csv\", index=False)","5fac1382":"## Merchants Preprocessing","8f45bfb7":"# Load data","d6b1082d":"## Historical Transaction Preprocessing","ec072fba":"## Train and Test Preprocessing","ae5a2252":"# Training","676c608b":"## XGBoost","f791ac95":"# Target","7fdfe49f":"**Merge to train and test**","17112e80":"# Preprocessing","826a50d7":"# Ensembling","894a43c5":"# Notes\n\nThis kernel just referred a very helpful kernel from Thuong Dinh. Thanks a lot!\n\n**References:**\n\n* https:\/\/www.kaggle.com\/thuongdinh\/ensemble-with-best-weights-finding\n* https:\/\/www.kaggle.com\/truocpham\/feature-engineering-and-lightgbm-starter\n\n**Update:**\n\n* Add some aggregate functions\n* Remove **year** feature\n* Do ensemble 3 models (LightGBM, CatBoost, XGBoost) with best weights finding","d90731d8":"# One Hot Encoding","cd1f6ddc":"## CatBoost","20a4d388":"# Import libs","9896ec25":"**Merge to train and test**","a67cef32":"## LightGBM"}}