{"cell_type":{"9bd9cb53":"code","52598efb":"code","463060cb":"code","cc73e678":"code","402afde8":"code","a2f5678b":"code","7e935009":"code","d6b9f478":"code","72e5ea8f":"code","6e34f335":"code","7948f97c":"code","20b979b5":"code","83fe3661":"code","e5a6ff24":"code","de2b719b":"code","268ead8e":"code","c3e124eb":"code","2290ff05":"code","6989f086":"code","4f432a73":"code","dc79e11e":"code","d89f08a9":"code","17bcdb52":"code","80653f42":"code","f5aa46dd":"code","b78cc17c":"code","c40516e8":"code","4207819f":"code","0f028a93":"code","de9783ae":"code","26a6c823":"code","aae8a6e6":"code","fe20b934":"code","7a1eb47a":"code","a4891922":"code","2f73e029":"code","15ebb42b":"code","37bbd326":"code","6282c9f5":"code","a9574fcb":"code","cbc2a5a2":"code","82737b3c":"code","992a261a":"code","0fd15cb2":"code","546454a1":"code","5c07154a":"code","f8070e67":"code","e51c36bb":"code","6998ef56":"code","bc7107d1":"code","652ea294":"code","5b30bf8a":"code","d69033fc":"code","4621c152":"code","3996ff2e":"code","e7c1e003":"code","379be0eb":"code","8be291eb":"code","7ce9c9f7":"code","544b05b2":"code","973f52b1":"code","146c7df2":"code","515a7c49":"code","eacfdb3d":"code","e34b2b60":"code","09f8b095":"code","f8676f56":"code","196bee78":"code","3c86d577":"code","ff9cf8d8":"code","cfeecd4e":"code","9fe6e566":"code","7043c8b5":"code","774e796f":"code","a16657ae":"code","2971dce2":"code","21277291":"code","4e04da52":"code","140c36e0":"code","9ad22265":"code","316147c6":"code","5fa153b6":"code","7eee5dff":"code","6c8e19bc":"code","03fd8c4d":"code","79410d3b":"code","d2d16077":"code","53bdab87":"code","04abd432":"code","a458f9d9":"code","1e09ea64":"code","c8bcbcb5":"code","bc2238a2":"code","d23d59ea":"code","9331b837":"code","cfd0ba78":"code","b6510548":"code","61cc2e18":"code","a04acbc1":"code","fa4e68c5":"code","d098d889":"code","fdf52be2":"code","9fa505c6":"code","47a3c533":"code","9a69570e":"code","d1721387":"code","1cf03b0f":"code","b5892856":"code","a004abf5":"code","4a8a96d5":"code","65964a2a":"code","08795e00":"code","2819cef1":"code","32ac6464":"code","145de9b4":"code","2b2ca609":"code","7a060898":"code","2219bd95":"code","b2209841":"code","02bb0aad":"code","b80111d5":"code","d2032ac2":"code","3e47d43b":"code","5e8616d7":"code","14916e04":"code","ba2edd24":"code","f5f6ff3b":"code","978225a5":"code","151b7f00":"code","2a379998":"code","ecaa973b":"code","95908282":"code","79e8ca52":"code","398cc802":"code","27a0e5b1":"code","44d1e4ad":"code","af6cb1d3":"code","88c7ad26":"code","56c08b3d":"code","d8a02f08":"code","dd97909a":"code","3f0186f9":"code","572c8c2f":"code","edf68328":"code","c80daa30":"code","73cb3dde":"code","ab356297":"code","85203045":"code","a96169c0":"code","6eb3ffc2":"code","5167b1ff":"code","94146c8e":"code","83c43281":"code","51862e90":"code","68536c43":"code","fc108d8b":"code","56ee6edf":"code","bb620a07":"code","035e1979":"code","9e2580ab":"code","f48cae98":"code","ecad28d1":"code","ec2c86d2":"code","b131cd73":"code","59f77f48":"code","004f11f9":"code","376effa1":"code","08b3ff97":"code","22981b28":"code","cac3c400":"code","4c637711":"code","4f9fb2bf":"code","28b49f03":"code","ea2f11df":"code","e73f5013":"code","fee80417":"code","4dfbb920":"code","35cfe31d":"code","17208f38":"code","5d6095c3":"code","6f300a1b":"code","b1113178":"code","9d30a943":"code","4e82a511":"code","0e99b9d4":"code","80beec3a":"code","9e0347d4":"code","193c2db4":"code","983e48ca":"code","44a88e94":"code","7fc9e352":"code","c4321cd8":"code","cf25c25f":"code","c92837d4":"code","5bdd5e6d":"code","c18b1b1b":"code","5ebe81ae":"code","b82480ce":"code","dbb485d5":"code","ed669f40":"code","1c75a54b":"code","f63c6002":"code","924df3d5":"code","c944e52f":"code","ead14d58":"code","f3aa27ba":"code","c80d9f03":"code","7a43dd56":"code","c1db0069":"code","d1f93344":"code","dab6256f":"code","14ec845f":"code","6fd764d2":"code","60b519a3":"code","070c819e":"code","b88d0ba1":"code","eec2bead":"code","929d0369":"code","f18aae66":"code","91a4a838":"code","07fc9711":"code","66d85f3e":"code","a45bd6cd":"code","5d12069f":"code","09c85bdc":"code","baeb0a5c":"code","d4482c1d":"code","e277b777":"code","9c508d2e":"code","c56569a2":"code","356e389b":"code","c097af20":"code","5894e60c":"code","e19f4afc":"code","56c27d8d":"code","e55fd5c6":"markdown","bd35ce75":"markdown","2182421c":"markdown","7a35a4e3":"markdown","8e64ab4d":"markdown","48477e6e":"markdown","4114f7fb":"markdown","1d21f5bf":"markdown","d6aa91b9":"markdown","13de703c":"markdown","d624b242":"markdown","1a14148b":"markdown","607bbcee":"markdown","c2872ef8":"markdown","c7c4a18d":"markdown","fede0e62":"markdown","7ba84721":"markdown","ffe8a0cd":"markdown","3e7b0db7":"markdown","49270031":"markdown","26f29afb":"markdown","64df7d2c":"markdown","d5d6a930":"markdown","99d52329":"markdown","eca1363e":"markdown","6fe5954a":"markdown","7fd8dcb2":"markdown","94caa138":"markdown","181aa2a7":"markdown","98f7f5b6":"markdown","0ae6c534":"markdown","a8161c81":"markdown","8eed0e09":"markdown","ae335d77":"markdown","25d8ad11":"markdown","00e07716":"markdown","a49e3360":"markdown","faae0958":"markdown","d796e3e6":"markdown","7e95581b":"markdown","a78c0f03":"markdown","034c8038":"markdown","4a5811f2":"markdown","5f67e421":"markdown","5a2b6495":"markdown"},"source":{"9bd9cb53":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","52598efb":"# visualization libraries\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(12,8)})\nimport matplotlib.pyplot as plt\nplt.style.use('classic')\n%matplotlib inline\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport cufflinks as cf\ncf.go_offline()\ninit_notebook_mode(connected=True)\nfrom sklearn.impute import KNNImputer\nfrom sklearn.model_selection import train_test_split\n# import function\nfrom sklearn.linear_model import LogisticRegression\n# peformance metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report,roc_auc_score\nfrom sklearn.model_selection import train_test_split,StratifiedKFold,cross_val_score,GridSearchCV,RepeatedStratifiedKFold\nfrom yellowbrick.features import FeatureImportances\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n#Import svm model\nfrom sklearn import svm\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nimport optuna\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nfrom hyperopt import hp, fmin, tpe, Trials, STATUS_OK\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping","463060cb":"import warnings\nwarnings.filterwarnings('ignore')","cc73e678":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_df.sample(10)","402afde8":"train_df.info()","a2f5678b":"train_df.shape","7e935009":"# Any null features\ntrain_df.isnull().sum() # Age, Cabin and Embarked fields have null values","d6b9f478":"# dropping the Id column\ntrain_df = train_df.drop('PassengerId',axis=1)","72e5ea8f":"train_df.head()","6e34f335":"train_df.describe().T","7948f97c":"train_df.Survived.value_counts()","20b979b5":"train_df.Pclass.value_counts()","83fe3661":"train_df.Pclass.value_counts().plot.barh()","e5a6ff24":"sns.histplot(data=train_df, x=train_df[\"Pclass\"], hue=\"Survived\", multiple=\"dodge\", shrink=.8)","de2b719b":"pd.crosstab(train_df[\"Survived\"],train_df[\"Pclass\"])","268ead8e":"pd.crosstab(train_df[\"Pclass\"],train_df[\"Survived\"]).plot(kind=\"bar\", figsize=(10,6),  color=[\"salmon\", \"lightblue\"]);\nplt.title(\"Survived vs Passenger Class\")\nplt.xlabel(\"0 = No Survival, 1 = Survival\")\nplt.ylabel(\"PClass\")\nplt.legend([\"No Survival\", \"Survival\"])\nplt.xticks(rotation=0);","c3e124eb":"train_df.Sex.value_counts()","2290ff05":"train_df.Sex.value_counts().plot.barh()","6989f086":"sns.histplot(data=train_df, x=train_df[\"Sex\"], hue=\"Survived\", multiple=\"dodge\", shrink=.8)","4f432a73":"pd.crosstab(train_df[\"Survived\"],train_df[\"Sex\"])","dc79e11e":"train_df.Age.isnull().sum()","d89f08a9":"train_df.Age.value_counts()","17bcdb52":"sns.histplot(data=train_df, x=train_df[\"Age\"], binwidth=5 , kde=True)","80653f42":"sns.violinplot(\"Survived\", \"Age\", data=train_df, palette=[\"lightblue\", \"lightpink\"]);","f5aa46dd":"sns.displot(data=train_df, x='Age', hue='Survived', kind='kde', fill=True)","b78cc17c":"train_df.SibSp.value_counts()","c40516e8":"train_df.SibSp.value_counts().plot.barh()","4207819f":"pd.crosstab(train_df[\"Survived\"],train_df[\"SibSp\"])","0f028a93":"train_df.Parch.value_counts()","de9783ae":"train_df.Parch.value_counts().plot.barh()","26a6c823":"pd.crosstab(train_df[\"Survived\"],train_df[\"Parch\"])","aae8a6e6":"train_df.Ticket.value_counts()","fe20b934":"train_df = train_df.drop('Ticket',axis=1)","7a1eb47a":"sns.histplot(data=train_df, x=train_df[\"Fare\"], binwidth=10 , kde=True)","a4891922":"LogFare = np.log(train_df.Fare + 1.0) # Adding 1 to accomodate zero fares : log(0) is not defined","2f73e029":"# Histogram of LogFare\nLogFare.plot(kind='hist', color='c', bins=20);","15ebb42b":"sns.violinplot(\"Survived\", \"Fare\", data=train_df, palette=[\"lightblue\", \"lightpink\"]);","37bbd326":"# box-whisker plot\ntrain_df.Fare.plot(kind='box')","6282c9f5":"train_df.Cabin.value_counts()","a9574fcb":"train_df.Cabin.unique()","cbc2a5a2":"train_df.Embarked.value_counts()","82737b3c":"train_df.Embarked.value_counts().plot.barh()","992a261a":"# Function to extract the title from the name \ndef GetTitle(name):\n    first_name_with_title = name.split(',')[1]\n    title = first_name_with_title.split('.')[0]\n    title = title.strip().lower()\n    return title","0fd15cb2":"# use map function to apply the function on each Name value row i\ntrain_df.Name.map(lambda x : GetTitle(x)) ","546454a1":"train_df.Name.map(lambda x : GetTitle(x)).unique()","5c07154a":"train_df.Name.map(lambda x : GetTitle(x)).unique()","f8070e67":"# Function to extract the title from the name \ndef GetTitle(name):\n    title_group = {'mr' : 'Mr', \n               'mrs' : 'Mrs', \n               'miss' : 'Miss', \n               'master' : 'Master',\n               'don' : 'Sir',\n               'rev' : 'Sir',\n               'dr' : 'Officer',\n               'mme' : 'Mrs',\n               'ms' : 'Mrs',\n               'major' : 'Officer',\n               'lady' : 'Lady',\n               'sir' : 'Sir',\n               'mlle' : 'Miss',\n               'col' : 'Officer',\n               'capt' : 'Officer',\n               'the countess' : 'Lady',\n               'jonkheer' : 'Sir',\n               'dona' : 'Lady'\n                 }\n    first_name_with_title = name.split(',')[1]\n    title = first_name_with_title.split('.')[0]\n    title = title.strip().lower()\n    return title_group[title]","e51c36bb":"# create Title feature\ntrain_df['Title'] =  train_df.Name.map(lambda x : GetTitle(x))","6998ef56":"# binning\npd.qcut(train_df.Fare, 4)","bc7107d1":"pd.qcut(train_df.Fare, 4, labels=['very_low','low','high','very_high']) # discretization","652ea294":"# create fare bin feature\ntrain_df['Fare_Bin'] = pd.qcut(train_df.Fare, 4, labels=['very_low','low','high','very_high'])","5b30bf8a":"# AgeState based on Age\ntrain_df['AgeState'] = np.where(train_df['Age'] >= 18, 'Adult','Child')","d69033fc":"# AgeState Counts\ntrain_df['AgeState'].value_counts()","4621c152":"train_df.groupby(['Pclass']).Fare.median()","3996ff2e":"train_df.groupby(['Pclass']).Age.median()","e7c1e003":"train_df.groupby(['Pclass'])['Fare','Age'].median()","379be0eb":"train_df.groupby(['Pclass']).agg({'Fare' : 'mean', 'Age' : 'median'})","8be291eb":"# pivot table\ntrain_df.pivot_table(index='Sex',columns = 'Pclass',values='Age', aggfunc='mean')","7ce9c9f7":"# Family : Adding Parents with Siblings\ntrain_df['FamilySize'] = train_df.Parch + train_df.SibSp + 1 # 1 for self","544b05b2":"# explore the family feature\ntrain_df['FamilySize'].plot(kind='hist', color='c');","973f52b1":"train_df.dtypes","146c7df2":"train_df.plot.scatter(x='Age', y='Fare', color='c', title='scatter plot : Age vs Fare');","515a7c49":"train_df.pivot_table(index='Sex',columns = 'Pclass',values='Age', aggfunc='mean')","eacfdb3d":"train_df.dtypes","e34b2b60":"train_df = pd.get_dummies(train_df,columns=['Sex', 'Pclass','Title', 'Fare_Bin', 'Embarked','AgeState'])","09f8b095":"train_df.info()","f8676f56":"# drop columns\ntrain_df.drop(['Cabin','Name','Parch','SibSp'], axis=1, inplace=True)","196bee78":"#### the KNN Imptuer is a distance-based imputation method and it requires us to normalize our data. \nimputer = KNNImputer()\nscaler = MinMaxScaler()\ntrain_df = pd.DataFrame(scaler.fit_transform(train_df), columns = train_df.columns)\ntrain_df.head()","3c86d577":"train_df.isnull().sum()","ff9cf8d8":"imputer = KNNImputer(n_neighbors=5)\ntrain_df = pd.DataFrame(imputer.fit_transform(train_df),columns = train_df.columns)","cfeecd4e":"train_df.isnull().sum()","9fe6e566":"X = train_df.drop('Survived',axis=1)\ny = train_df['Survived']","7043c8b5":"X.head()","774e796f":"y","a16657ae":"X.shape,y.shape","2971dce2":"# train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, test_size=0.2, random_state=0)\n","21277291":" X_train.shape, y_train.shape,X_test.shape, y_test.shape","4e04da52":"# average survival in train and test\nprint ('mean survival in train : {0:.3f}'.format(np.mean(y_train)))\nprint ('mean survival in test : {0:.3f}'.format(np.mean(y_test)))","140c36e0":"# create model\nmodel = LogisticRegression(random_state=0)","9ad22265":"# train model\nmodel.fit(X_train,y_train)","316147c6":"# evaluate model\nprint ('score for logistic regression - version 1 : {0:.2f}'.format(model.score(X_test, y_test)))","5fa153b6":"y_pred = model.predict(X_test)","7eee5dff":"accuracy_score(y_test,y_pred)","6c8e19bc":"confusion_matrix(y_test,y_pred)","03fd8c4d":"precision_score(y_test, y_pred)","79410d3b":"recall_score(y_test, y_pred)","d2d16077":"print(classification_report(y_test,y_pred))","53bdab87":"# model coefficients\nmodel.coef_","04abd432":"print(X_train.columns)","a458f9d9":"fig, ax = plt.subplots(figsize=(16, 14))\nvisualization = FeatureImportances(model)\nvisualization.fit(X, y)\nvisualization.poof()","1e09ea64":"for model in [LogisticRegression]:\n     skflr = model()\n     skf = StratifiedKFold(n_splits=10, random_state=42)\n     s = cross_val_score(skflr, X, y, scoring=\"roc_auc\", cv=skf)\n     print(\"Accuracy = \", s.mean())","c8bcbcb5":"# Loads test data set\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","bc2238a2":"test.head()","d23d59ea":"test.drop([\"PassengerId\"], axis=1, inplace=True)","9331b837":"test.drop([\"Ticket\"], axis=1, inplace=True)","cfd0ba78":"test.isnull().sum()","b6510548":"# create Title feature\ntest['Title'] =  test.Name.map(lambda x : GetTitle(x))\n\n# create fare bin feature\ntest['Fare_Bin'] = pd.qcut(test.Fare, 4, labels=['very_low','low','high','very_high'])\n\n# AgeState based on Age\ntest['AgeState'] = np.where(test['Age'] >= 18, 'Adult','Child')\n\n# AgeState Counts\ntest['AgeState'].value_counts()\n\n","61cc2e18":"# Family : Adding Parents with Siblings\ntest['FamilySize'] = test.Parch + test.SibSp + 1 # 1 for self\n\n","a04acbc1":"test = pd.get_dummies(test,columns=['Sex', 'Pclass','Title', 'Fare_Bin', 'Embarked','AgeState'])\n\n# drop columns\ntest.drop(['Cabin','Name','Parch','SibSp'], axis=1, inplace=True)","fa4e68c5":"\ntest = pd.DataFrame(scaler.fit_transform(test), columns = test.columns)\ntest.head()\n","d098d889":"test = pd.DataFrame(imputer.fit_transform(test),columns = test.columns)\n\ntest.isnull().sum()\n","fdf52be2":"submission = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")","9fa505c6":"submission.head()","47a3c533":"submission.info()","9a69570e":"# Create StratifiedKFold object.\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state= 40)\nval_acc = []\ntest_predictions = []\nsubmission_predictions = []\nmodel = LogisticRegression(random_state=0)","d1721387":"for fold, (train_index, test_index) in enumerate(skf.split(X_train, y_train)):\n    x_train_fold, x_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n    print('Fold', fold )\n    \n    model.fit(x_train_fold, y_train_fold)\n    \n    print(\"score : \",model.score(x_train_fold, y_train_fold))\n    \n    y_pred = model.predict(x_test_fold)\n    print(\"Validation score : \",accuracy_score(y_test_fold, y_pred))\n    \n    preds = model.predict(X_test)\n    test_predictions.append(preds)\n    \n    submission_preds = model.predict(test)\n    submission_predictions.append(submission_preds)\n    ","1cf03b0f":"y_pred = np.mean(np.column_stack(test_predictions), axis=1)","b5892856":"y_pred = y_pred.astype('int32')\ny_test = y_test.astype('int32')","a004abf5":"print(accuracy_score(y_test, y_pred))","4a8a96d5":"submission_preds = np.mean(np.column_stack(submission_predictions), axis=1)\nsubmission_preds = submission_preds.astype('int32')","65964a2a":"submit_df =  pd.DataFrame({'PassengerId': submission['PassengerId'],\n                          'Survived': submission_preds})","08795e00":"submit_df.head(10)","2819cef1":"submit_df.to_csv('submission.csv',index=False)","32ac6464":"classifier = GaussianNB()\nclassifier.fit(X_train, y_train)","145de9b4":"y_pred = classifier.predict(X_test)","2b2ca609":"accuracy_score(y_test,y_pred)","7a060898":"cm = confusion_matrix(y_test, y_pred)\nprint(cm)","2219bd95":"print(classification_report(y_test,y_pred))","b2209841":"test_preds = classifier.predict(test)\ntest_preds = test_preds.astype('int32')","02bb0aad":"submission[\"Survived\"] = test_preds","b80111d5":"submission","d2032ac2":"submission.to_csv('submission.csv',index=False) # 0.76794","3e47d43b":"knc = KNeighborsClassifier()\nknc.fit(X_train, y_train)\n\n","5e8616d7":"knc.score(X_test, y_test)\n","14916e04":"y_pred = knc.predict(X_test)","ba2edd24":"print(confusion_matrix(y_test,y_pred))","f5f6ff3b":"print(classification_report(y_test,y_pred))","978225a5":"test_preds = knc.predict(test)\ntest_preds = test_preds.astype('int32')","151b7f00":"submission[\"Survived\"] = test_preds","2a379998":"submission","ecaa973b":"submission.to_csv('submission.csv',index=False) # 0.76794","95908282":"# LDA\nlda = LDA()\nlda.fit(X_train,y_train)","79e8ca52":"y_pred=lda.predict(X_test)","398cc802":"lda.score(X_test, y_test)","27a0e5b1":"print(confusion_matrix(y_test,y_pred))","44d1e4ad":"print(classification_report(y_test,y_pred))","af6cb1d3":"test_preds = lda.predict(test)\ntest_preds = test_preds.astype('int32')","88c7ad26":"submission[\"Survived\"] = test_preds","56c08b3d":"submission","d8a02f08":"submission.to_csv('submission.csv',index=False) # 0.77511","dd97909a":"# define model\nmodel = LDA()\n# define model evaluation method\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# define grid\ngrid = dict()\ngrid['solver'] = ['svd', 'lsqr', 'eigen']\n# define search\nsearch = GridSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)\n# perform the search\nresults = search.fit(X, y)\n# summarize\nprint('Mean Accuracy: %.3f' % results.best_score_)\nprint('Config: %s' % results.best_params_)","3f0186f9":"# define model\nmodel =LDA(solver='lsqr')\n# define model evaluation method\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# define grid\ngrid = dict()\ngrid['shrinkage'] = np.arange(0, 1, 0.01)\n# define search\nsearch = GridSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)\n# perform the search\nresults = search.fit(X, y)\n# summarize\nprint('Mean Accuracy: %.3f' % results.best_score_)\nprint('Config: %s' % results.best_params_)","572c8c2f":"svm_model = svm.SVC(kernel='linear', class_weight='balanced') \nsvm_model.fit(X_train, y_train)","edf68328":"y_pred=svm_model.predict(X_test)","c80daa30":"accuracy_score(y_test,y_pred)","73cb3dde":"print(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))","ab356297":"test_preds = svm_model.predict(test)\ntest_preds = test_preds.astype('int32')","85203045":"submission[\"Survived\"] = test_preds\nsubmission","a96169c0":"submission.to_csv('submission.csv',index=False) # 0.77033","6eb3ffc2":"# SVM Classifier model\nsvm_model = svm.SVC(kernel=\"poly\", degree=3, coef0=1, C=5,class_weight='balanced')\nsvm_model.fit(X_train, y_train)","5167b1ff":"y_pred=svm_model.predict(X_test)","94146c8e":"accuracy_score(y_test,y_pred)","83c43281":"print(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))","51862e90":"test_preds = svm_model.predict(test)\ntest_preds = test_preds.astype('int32')","68536c43":"submission[\"Survived\"] = test_preds\nsubmission","fc108d8b":"submission.to_csv('submission.csv',index=False) # 0.73444","56ee6edf":"# SVM Classifier model with RBF kernel for non linear separability\nsvm_model = svm.SVC(kernel=\"rbf\",class_weight='balanced')\nsvm_model.fit(X_train, y_train)","bb620a07":"y_pred=svm_model.predict(X_test)","035e1979":"accuracy_score(y_test,y_pred)","9e2580ab":"print(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))","f48cae98":"test_preds = svm_model.predict(test)\ntest_preds = test_preds.astype('int32')","ecad28d1":"submission[\"Survived\"] = test_preds\nsubmission","ec2c86d2":"submission.to_csv('submission.csv',index=False) #0.75598","b131cd73":"\nparams = { 'C':[0.1,1,10,100,1000],'kernel':['rbf','poly','sigmoid','linear'],'degree':[1,2,3,4,5,6],'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}\n","59f77f48":"svm_model = svm.SVC()","004f11f9":"random_search = RandomizedSearchCV(svm_model, params, n_iter =10, cv=9)","376effa1":"random_search.fit(X_train,y_train)","08b3ff97":"random_search.best_params_","22981b28":"\nrandom_search.best_score_","cac3c400":"svm_model = svm.SVC(kernel=\"poly\",gamma=1,degree=1,C=10,class_weight='balanced')\nsvm_model.fit(X_train, y_train)","4c637711":"y_pred=svm_model.predict(X_test)\naccuracy_score(y_test,y_pred)","4f9fb2bf":"print(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))","28b49f03":"test_preds = svm_model.predict(test)\ntest_preds = test_preds.astype('int32')\nsubmission[\"Survived\"] = test_preds\nsubmission\nsubmission.to_csv('submission.csv',index=False) # 0.77033","ea2f11df":"# with the default hyperparameters setting\nrf_model = RandomForestClassifier()\nrf_model.fit(X_train, y_train)\n","e73f5013":"y_pred = rf_model.predict(X_test)","fee80417":"print(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))","4dfbb920":"# the objective function takes the hyperparameter space as input\ndef objective(trial):\n    n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000)\n    criterion = trial.suggest_categorical(\"criterion\", ['gini', 'entropy'])\n    max_depth = trial.suggest_int(\"max_depth\", 1, 4)\n    min_samples_split = trial.suggest_float(\"min_samples_split\", 0.01, 1)\n\n    model = RandomForestClassifier(\n            n_estimators=n_estimators,\n            criterion=criterion,\n            max_depth=max_depth,\n            min_samples_split=min_samples_split,\n        )\n    \n    score = cross_val_score(model, X_train, y_train, cv=5)\n    accuracy = score.mean()\n    \n    return accuracy\n    ","35cfe31d":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=20)","17208f38":"study.best_params","5d6095c3":"study.best_value","6f300a1b":"study.trials_dataframe()","b1113178":"# training the model with the hyperparameter values\nmodel = RandomForestClassifier(\n            n_estimators=214,\n            criterion='gini',\n            max_depth=4,\n            min_samples_split=0.31977425965640455,\n        )\n    \nmodel.fit(X_train,y_train)","9d30a943":"y_pred = model.predict(X_test)","4e82a511":"print(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))","0e99b9d4":"importances = model.feature_importances_\n#\n# Sort the feature importance in descending order\n#\nsorted_indices = np.argsort(importances)[::-1]","80beec3a":"plt.figure(figsize=(10,8), dpi=80)\nplt.title('Feature Importance')\nplt.bar(range(X_train.shape[1]), importances[sorted_indices], align='center')\nplt.xticks(range(X_train.shape[1]), X_train.columns[sorted_indices], rotation=90)\nplt.tight_layout()\nplt.show()","9e0347d4":"test_preds = model.predict(test)\ntest_preds = test_preds.astype('int32')\nsubmission[\"Survived\"] = test_preds\nsubmission\nsubmission.to_csv('submission.csv',index=False) #0.77","193c2db4":"# Create StratifiedKFold object.\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state= 40)","983e48ca":"# Performs cross validation on XGB Classifier\n\nmodel = XGBClassifier(n_estimators=500,objective='binary:logistic', eval_metric='auc',tree_method='gpu_hist')\nmodel_score = cross_val_score(model, X, y, scoring='roc_auc', cv=skf.split(X, y), n_jobs=-1, verbose=10)","44a88e94":"print(model_score.mean())\n","7fc9e352":"del model_score, model","c4321cd8":"fold_no = 1\nfor train_index, test_index in skf.split(X, y):\n    print('Fold = ',fold_no)\n    y_val = y.iloc[test_index]\n    dtrain = xgb.DMatrix(data=X.iloc[train_index], label=y.iloc[train_index])\n    dval = xgb.DMatrix(data=X.iloc[test_index], label=y.iloc[test_index])\n    fold_no +=1","cf25c25f":"hyperparameter_space = { \n                        'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n                        'max_depth': hp.quniform(\"max_depth\", 2, 6, 1),\n                        'min_child_weight' : hp.quniform('min_child_weight', 1, 8, 1),\n                        'reg_alpha' : hp.uniform('reg_alpha', 1e-8, 100),\n                        'reg_lambda' : hp.uniform('reg_lambda', 1e-8, 100),\n                        'gamma': hp.uniform ('gamma', 0.0, 1.0),\n                        'subsample': hp.uniform(\"subsample\", 0.1, 1.0),\n                        'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0)\n                       }","c92837d4":"def optimize_hyppara(hyperparameter_space):\n    # Converts parameter value to int as required by XGBoost\n    hyperparameter_space[\"max_depth\"] = int(hyperparameter_space[\"max_depth\"])\n    hyperparameter_space[\"objective\"] = \"binary:logistic\"\n    hyperparameter_space[\"eval_metric\"] = \"auc\"\n    hyperparameter_space[\"tree_method\"] = \"gpu_hist\"\n    \n    model = xgb.train(\n        hyperparameter_space, \n        dtrain, \n        num_boost_round=2000, \n        evals=[(dtrain, 'train'), (dval, 'eval')],\n        early_stopping_rounds=50, verbose_eval=False)\n    \n    predictions = model.predict(dval)\n    \n    roc_auc = roc_auc_score(y_val, predictions)\n    \n    del predictions, model, hyperparameter_space\n    \n    return {\"loss\": -roc_auc, \"status\": STATUS_OK}","5bdd5e6d":"# Starts hyperparameters tuning\ntrials = Trials()\nbest_model_params = fmin(fn=optimize_hyppara,space=hyperparameter_space, max_evals=50,algo=tpe.suggest,trials=trials)","c18b1b1b":"best_model_params","5ebe81ae":"del dtrain, dval,y_val","b82480ce":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\ndtrain = xgb.DMatrix(data=X_train, label=y_train)\ndval = xgb.DMatrix(data=X_test, label=y_test)\n#del X_train,y_train,X_test,y_test\nparams = {'colsample_bytree': 0.9912835539334705,\n 'gamma': 0.9947467688258089,\n 'learning_rate': 0.17756388494635836,\n 'max_depth': 6.0,\n 'min_child_weight': 4.0,\n 'reg_alpha': 1.0806687020657577,\n 'reg_lambda': 56.99895595690155,\n 'subsample': 0.7218949376758498}\n\nparams[\"max_depth\"] = int(params[\"max_depth\"])\nparams[\"objective\"] = \"binary:logistic\"\nparams[\"eval_metric\"] = \"auc\"\nparams[\"tree_method\"] = \"gpu_hist\"\n    \nmodel = xgb.train(\n        params, \n        dtrain, \n        num_boost_round=2000, \n        evals=[(dtrain, 'train'), (dval, 'eval')],\n        early_stopping_rounds=50, verbose_eval=200)","dbb485d5":"# Adds other important parameters\nbest_model_params[\"max_depth\"] = int(best_model_params[\"max_depth\"])\nbest_model_params[\"objective\"] = \"binary:logistic\"\nbest_model_params[\"eval_metric\"] = \"auc\"\nbest_model_params[\"tree_method\"] = \"gpu_hist\"","ed669f40":"dtest = xgb.DMatrix(data=test)\npredictions = model.predict(dtest)","1c75a54b":"predictions  = predictions > 0.5  \npredictions = predictions.astype(int)  ","f63c6002":"submission[\"Survived\"] = predictions\nsubmission","924df3d5":"submission.to_csv(\".\/submission.csv\", index=False) # 0.78468","c944e52f":"del model, dtest, predictions","ead14d58":"# Gets the model trained over cross validation and predictions \n# against each iteration is stored\n\ntest_predictions = []\n\ndtest = xgb.DMatrix(data=test)\n\nfor fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n    print(\"fold\", fold)\n\n    dtrain = xgb.DMatrix(data=X.iloc[train_index], label=y.iloc[train_index])\n    dval = xgb.DMatrix(data=X.iloc[val_index], label=y.iloc[val_index])\n    \n    model = xgb.train(\n        best_model_params, \n        dtrain, \n        num_boost_round=2000, \n        evals=[(dtrain, 'train'), (dval, 'eval')],\n        early_stopping_rounds=50, verbose_eval=200)\n    \n    predictions = model.predict(dtest)\n    \n    test_predictions.append(predictions)\n    \n    del predictions, model, dval, dtrain","f3aa27ba":"del dtest","c80d9f03":"submission_preds = np.mean(np.column_stack(submission_predictions), axis=1)\nsubmission_preds  = submission_preds > 0.5  \nsubmission_preds = submission_preds.astype(int)  ","7a43dd56":"submission[\"Survived\"] = submission_preds\nsubmission","c1db0069":"submission.to_csv(\".\/submission.csv\", index=False) # 0.78468","d1f93344":"X_train.shape # 24 features hence we start with the first dense model having 24 neurons ","dab6256f":"keras_model = Sequential()\nkeras_model.add(Dense(units=24,activation='relu'))\nkeras_model.add(Dropout(0.5))\nkeras_model.add(Dense(units=12,activation='relu'))\nkeras_model.add(Dropout(0.5))\nkeras_model.add(Dense(units=1,activation='sigmoid'))\n# For a binary classification problem\nkeras_model.compile(loss='binary_crossentropy', optimizer='adam')","14ec845f":"early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)","6fd764d2":"keras_model.fit(x=X_train, \n          y=y_train, \n          epochs=500,\n          validation_data=(X_test, y_test), verbose=1,callbacks=[early_stop]\n          )","60b519a3":"model_loss = pd.DataFrame(keras_model.history.history)","070c819e":"model_loss.plot()","b88d0ba1":"predictions = keras_model.predict(X_test)","eec2bead":"predictions  = predictions > 0.5  \npredictions = predictions.astype(int)  ","929d0369":"print(classification_report(y_test,predictions))","f18aae66":"test_preds = keras_model.predict(test)","91a4a838":"test_preds  = test_preds > 0.5  \ntest_preds = test_preds.astype(int)  ","07fc9711":"submission[\"Survived\"] = test_preds\nsubmission","66d85f3e":"submission.to_csv(\".\/submission.csv\", index=False) # 0.77751","a45bd6cd":"# logistic regression, random forest and xgboost\nlogreg = LogisticRegression()\nrf = RandomForestClassifier()\nxgbc = XGBClassifier()","5d12069f":"# fit all models on X_train\nlogreg.fit(X_train, y_train)\nrf.fit(X_train, y_train)\nxgbc.fit(X_train, y_train)","09c85bdc":"# predicting all the models on X_test\n# taking the probability for class 1\npred_logreg = logreg.predict_proba(X_test)[:, 1]\npred_rf = rf.predict_proba(X_test)[:, 1]\npred_xgbc = xgbc.predict_proba(X_test)[:, 1]\n","baeb0a5c":"# creating an average of all the predictions\navg_pred = (pred_logreg + pred_rf + pred_xgbc) \/ 3","d4482c1d":"# storing all the predictions in an array\ntest_preds = np.column_stack((\n pred_logreg,\n pred_rf,\n pred_xgbc,\n avg_pred\n))","e277b777":"# calculating and storing individual AUC values\nauc_test = []\nfor i in range(test_preds.shape[1]):\n auc = roc_auc_score(y_test, test_preds[:, i])\n auc_test.append(auc)\nprint(f\"LR AUC = {auc_test[0]}\")\nprint(f\"RF AUC = {auc_test[1]}\")\nprint(f\"XGB AUC = {auc_test[2]}\")\nprint(f\"Average Pred AUC = {auc_test[3]}\")","9c508d2e":"# predict all models on test\n# take probability for class 1\npred_logreg = logreg.predict_proba(test)[:, 1]\npred_rf = rf.predict_proba(test)[:, 1]\npred_xgbc = xgbc.predict_proba(test)[:, 1]","c56569a2":"# create an average of all predictions\n# that is the simplest ensemble\ntest_preds = (pred_logreg + pred_rf + pred_xgbc) \/ 3","356e389b":"test_preds","c097af20":"test_preds  = test_preds > 0.5  \ntest_preds = test_preds.astype(int)  ","5894e60c":"test_preds","e19f4afc":"submission[\"Survived\"] = test_preds\nsubmission","56c27d8d":"submission.to_csv(\".\/submission.csv\", index=False) # 0.76315","e55fd5c6":"# XGBoost","bd35ce75":"#### We can use a random search cross-validation to explore combinations of parameters. \n","2182421c":"# Stratified Crossvalidation","7a35a4e3":"c) Gamma: A lower value of Gamma will loosely fit the training dataset, whereas a higher value of gamma will exactly fit the training dataset, which causes over-fitting","8e64ab4d":"#### Polynomial Kernel can distinguish curved or nonlinear input space.\n#### SVM classifier using a third-degree polynomial kernel. the hyperparameter coef0 controls how much the model is influenced by high degree polynomials versus low degree polynomials","48477e6e":"#### Feature importances","4114f7fb":"#### Fare","1d21f5bf":"#### Cabin","d6aa91b9":"#### Embarked","13de703c":"d) degree: It is the degree of the polynomial kernel function (\u2018poly\u2019) default value is 3.","d624b242":"# Logistic Regression","1a14148b":"# Data","607bbcee":"#### Sex feature","c2872ef8":"# Hyperparameter tuning with Optuna","c7c4a18d":"# KNN","fede0e62":"#### SibSp","7ba84721":"#### Ticket","ffe8a0cd":"# Hyperparameter tuning with RandomSearch","3e7b0db7":"## Tuning with Hyperopt","49270031":"#### Age","26f29afb":"#### Shows the features ranked according to the explained variance each feature contributes to the model. In this case the features are plotted against their relative importance, that is the percent importance of the most important feature","64df7d2c":"### The goal is to predict which passengers survived the Titanic shipwreck.","d5d6a930":"#### training the model with the selectec parameters","99d52329":"#### Parch","eca1363e":"# Classification with Keras Sequential API","6fe5954a":"#### Pclass","7fd8dcb2":"a) Kernel: The main function of the kernel is to transform the given dataset input data into the required form. There are various types of functions such as linear, polynomial, and radial basis function (RBF). Polynomial and RBF are useful for non-linear hyperplane. Polynomial and RBF kernels compute the separation line in the higher dimension. ","94caa138":"#### The linear kernel","181aa2a7":"## Univariate Analysis, Bivariate Analysis","98f7f5b6":"#### Tuning LDA Hyperparameters solver and shrinkage with sklearn GridSearchCV","0ae6c534":"# Model training and evaluation","a8161c81":"# Linear Discriminant Analysis","8eed0e09":"# Naive Bayes","ae335d77":"## Load the needed dataset","25d8ad11":"# Ensembling and Stacking","00e07716":"#### the objective function","a49e3360":"#### Reading ... https:\/\/machinelearningmastery.com\/linear-discriminant-analysis-with-python\/","faae0958":"# Exploratory Data Analysis","d796e3e6":"# Libraries","7e95581b":"# Support Vector Machine ","a78c0f03":"b) Regularization: C is the penalty parameter, which represents misclassification or error term. The misclassification or error term tells the SVM optimization how much error is bearable. This is how you can control the trade-off between decision boundary and misclassification term. A smaller value of C creates a small-margin hyperplane and a larger value of C creates a larger-margin hyperplane.","034c8038":"# Objective","4a5811f2":"# RandomForest","5f67e421":"#### An important hyperparameter is the solver, which defaults to \u2018svd\u2018 but can also be set to other values for solvers that support the shrinkage capability.","5a2b6495":"# Feature Selection"}}