{"cell_type":{"f5f5e171":"code","04a72a62":"code","0dd4b00c":"code","e77c61c7":"code","60b58a54":"code","ff651d1d":"code","b05ebfc7":"code","4b94857f":"code","36aba767":"code","afb073ae":"code","fcbf6062":"code","fb98296a":"code","c8ace862":"code","2f72507a":"code","4ca57835":"code","97dc6cbc":"code","18e456df":"code","4fe8a8ef":"code","8e0b359b":"code","879a4196":"markdown","b56445ae":"markdown","6520ad08":"markdown","89b7a99b":"markdown","722962e2":"markdown","4a381c56":"markdown","cc11b759":"markdown","a735f6f2":"markdown","3786984a":"markdown","59aa6e2c":"markdown","2c2e4566":"markdown","a3c28770":"markdown","696ad251":"markdown","a4b80ddb":"markdown","16450bc1":"markdown","ffd11806":"markdown"},"source":{"f5f5e171":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer, IterativeImputer\nfrom sklearn.linear_model import RidgeClassifier,RidgeClassifierCV\nfrom sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor, RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder,PolynomialFeatures, StandardScaler\nfrom sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.decomposition import PCA,TruncatedSVD\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","04a72a62":"gender_submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\",index_col=[0])\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\",index_col=[0])\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\",index_col=[0])","0dd4b00c":"def initializeFeatureAndTargetFrame(completeData):\n    X = (completeData.copy())[['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']]\n    y = (completeData.copy())['Survived']\n    return X,y\n\ndef write_prediction(model,description=\"\"):\n    SubmissionFileName = 'submission_'+datetime.now().strftime('%Y%m%d%H%M%S')+'.csv'\n    y_pred = model.predict(test)\n    df_y_pred = pd.DataFrame(y_pred, index=test.index, columns=['Survived'])\n    df_y_pred.to_csv(SubmissionFileName)\n    return SubmissionFileName","e77c61c7":"plt.subplots(figsize=(25,10))\nsns.heatmap(train.isnull(), cbar=False).set_title(\"Null values heatmap\")","60b58a54":"train.info()","ff651d1d":"OrdEncoder = OrdinalEncoder()\ntrain_transf = OrdEncoder.fit_transform((train.dropna(subset=['Embarked','Age']))[['Survived','Pclass','Age', 'SibSp','Parch','Fare','Embarked']])\ndf_train_transf = pd.DataFrame(train_transf, columns=['Survived','Pclass','Age', 'SibSp','Parch','Fare','Embarked'])\nfeat_corr = df_train_transf.corr()\n\nax = sns.heatmap(\n    feat_corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True,\n)\n\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","b05ebfc7":"data = np.concatenate([train[['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket',\n       'Fare', 'Cabin', 'Embarked']],test],axis=0)","4b94857f":"X,y = initializeFeatureAndTargetFrame(train)","36aba767":"num_cols = X.select_dtypes(include='number').columns.values\nNumRidgeModel = Pipeline([\n    ('cs', ColumnTransformer([\n        ('num_cols', SimpleImputer(missing_values=np.NaN), num_cols)\n    ])),\n    ('Predictor', RidgeClassifier())\n])\n#NumRidgeModel.fit(X,y)\n#write_prediction(NumRidgeModel)","afb073ae":"ImputEncodeFeature = Pipeline([\n    ('si',SimpleImputer(strategy='most_frequent', missing_values=np.NaN)),\n    ('ohe', OneHotEncoder(handle_unknown='ignore'))\n])\nNumCatRidgeModel =Pipeline([\n    ('cs', ColumnTransformer(transformers=[\n        ('age_transformer', SimpleImputer(strategy='median', missing_values=np.NaN), ['Age']),\n        ('Embarked_transformer',ImputEncodeFeature, ['Embarked']),\n        ('sex_encoder', OneHotEncoder(handle_unknown='ignore'), ['Sex','Parch','Pclass','SibSp']),\n        ('rem_feature', SimpleImputer(strategy='mean', missing_values=np.NaN), ['Fare']),\n    ],remainder='drop')),\n    ('predictor', RidgeClassifier())\n])\n#NumCatRidgeModel.fit(X,y)\n#write_prediction(NumCatRidgeModel)\n#Score : 0.76555","fcbf6062":"NumCatIIRidgeModel =Pipeline([\n    ('cs_ii', ColumnTransformer(transformers=[\n        ('featureExtracter', 'passthrough', ['Sex']),\n        ('si', SimpleImputer(strategy='most_frequent'),['Embarked']),\n        ('ii', IterativeImputer(random_state=42, estimator=ExtraTreesRegressor(), ), ['Pclass','SibSp','Parch','Age','Fare'])\n    ])),\n    ('cs', ColumnTransformer(transformers=[\n        ('featureExtracter', 'passthrough', [5,6]),\n        ('sex_encoder', OneHotEncoder(handle_unknown='ignore'), [0,1,2,3,4])\n    ])),\n    ('pf', PolynomialFeatures(2)),\n    ('svd', TruncatedSVD(n_components=15)),\n    ('predictor', RidgeClassifier())\n])\n#NumCatIIRidgeModel.fit(X,y)\n#write_prediction(NumCatIIRidgeModel)\n#Score : 0.78947\n#Rank : 3571","fb98296a":"class nameExtractor(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X,y=None):\n        return X.reshape(len(X),)\n    \nname_feature_pipe = Pipeline([\n    ('ne', nameExtractor()),\n    ('cv', HashingVectorizer(n_features=3000))\n])\n        \nNumCatTextRidgeModel =Pipeline([\n    ('cs_ii', ColumnTransformer(transformers=[\n        ('featureExtracter', 'passthrough', ['Sex','Name']),\n        ('si', SimpleImputer(strategy='most_frequent'),['Embarked']),\n        ('ii', IterativeImputer(random_state=42, estimator=ExtraTreesRegressor()), ['Pclass','SibSp','Parch','Age','Fare'])\n    ])),\n    ('cs', ColumnTransformer(transformers=[\n        ('ohe_sex', OneHotEncoder(handle_unknown='ignore'), [0]),\n        ('nfp', name_feature_Output, [1]),\n        ('ohe', OneHotEncoder(handle_unknown='ignore'), [2,3,4,5]),\n        ('featureExtracter', 'passthrough', [6,7])\n    ])),\n    ('svd', TruncatedSVD(n_components=3000)),\n    ('predictor', RidgeClassifier())\n])\n#NumCatTextRidgeModel.fit(X,y);\n#write_prediction(NumCatTextRidgeModel)\n#Score : 0.79904\n#Rank : 1728","c8ace862":"class nameExtractor2(BaseEstimator, TransformerMixin):\n    def __init__(self,column):\n        self.column = column\n        pass\n    \n    def fit(self,X,y=None):\n        print(len(X),len(y))\n        return self\n    \n    def transform(self,X,y=None):\n        return [col for col in X[self.column]]\n        #return X_feat","2f72507a":"name_predictor_model = Pipeline([\n    ('ne', nameExtractor2('Name')),\n    ('cv', HashingVectorizer(n_features=3000)),\n    ('predictor', RidgeClassifier())\n])\n\nname_predictor_model.fit(X,y)\n#write_prediction(name_predictor_model)\n#Score: 0.80861\n#Score with 3000 features: 0.80861\n#Rank: 845","4ca57835":"name_predictor_model = Pipeline([\n    ('ne', nameExtractor2('Name')),\n    ('cv', HashingVectorizer()),\n    ('predictor', RidgeClassifier())\n])\ngrid = {'cv__n_features':range(1000,100001,1000),'cv__stop_words':['english','None'],\n        'cv__norm':['l1','l2'],'cv__ngram_range':[(1,1),(1,2)],'predictor__alpha':np.logspace(-3,3,10)}\nname_predictor_modelGS = Pipeline([\n    ('gs',GridSearchCV(estimator=name_predictor_model,param_grid=grid, scoring='average_precision', cv=3, n_jobs=-1))\n])\n#name_predictor_modelGS.fit(X,y);\n#write_prediction(name_predictor_modelGS)","97dc6cbc":"tfidf_name_predictor_model = Pipeline([\n    ('ne', nameExtractor2('Name')),\n    ('tfidf', TfidfVectorizer()),\n    ('predictor', RidgeClassifier())\n])\ngrid = {'tfidf__max_df': (0.25, 0.5, 0.75),'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n        'predictor__alpha':np.logspace(-3,3,10)}\ntfidf_name_predictor_modelGS = Pipeline([\n    ('gs',GridSearchCV(estimator=tfidf_name_predictor_model,param_grid=grid, scoring='average_precision', cv=3, n_jobs=-1))\n])\n#tfidf_name_predictor_modelGS.fit(X,y);\n#write_prediction(tfidf_name_predictor_modelGS)\n#Score: 0.79425","18e456df":"class nameExtractor(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self,X,y=None):\n        print(len(X),len(y))\n        return self\n    \n    def transform(self,X,y=None):\n        return X.reshape(len(X),)\n    \nname_feature_pipe = Pipeline([\n    ('ne', nameExtractor()),\n    ('cv', HashingVectorizer(n_features=3000))\n])\n        \nRidgeCV_NumCatTextRidgeModel =Pipeline([\n    ('cs_ii', ColumnTransformer(transformers=[\n        ('featureExtracter', 'passthrough', ['Sex','Name']),\n        ('si', SimpleImputer(strategy='most_frequent'),['Embarked']),\n        ('ii', IterativeImputer(random_state=42, estimator=ExtraTreesRegressor()), ['Pclass','SibSp','Parch','Age','Fare'])\n    ])),\n    ('cs', ColumnTransformer(transformers=[\n        ('ohe_sex', OneHotEncoder(handle_unknown='ignore'), [0]),\n        ('nfp', name_feature_pipe, [1]),\n        ('ohe', OneHotEncoder(handle_unknown='ignore'), [2,3,4,5]),\n        ('featureExtracter', 'passthrough', [6,7])\n    ])),\n    ('pf', PolynomialFeatures(2)),\n    ('svd', TruncatedSVD(n_components=3000)),\n    ('predictor', RidgeClassifierCV(cv=5))\n])\nRidgeCV_NumCatTextRidgeModel.fit(X,y);\n#write_prediction(RidgeCV_NumCatTextRidgeModel)","4fe8a8ef":"class nameExtractor(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X,y=None):\n        return X.reshape(len(X),)\n\nname_predictor_model = Pipeline([\n    ('ne', nameExtractor()),\n    ('cv', HashingVectorizer(n_features=3000)),\n    ('predictor', RidgeClassifier())\n])\n\nclass name_feature_prediction(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self,X,y=None):\n        name_predictor_model.fit(X,y)\n        return self\n    \n    def transform(self,X,y=None):\n        name_predictions = name_predictor_model.predict(X)\n        return name_predictions.reshape(-1,1)\n    \nStackedNameModel =Pipeline([\n    ('cs_ii', ColumnTransformer(transformers=[\n        ('featureExtracter', 'passthrough', ['Sex','Name']),\n        ('si', SimpleImputer(strategy='most_frequent'),['Embarked']),\n        ('ii', IterativeImputer(random_state=42, estimator=ExtraTreesRegressor()), ['Pclass','SibSp','Parch','Age','Fare'])\n    ])),\n    ('cs', ColumnTransformer(transformers=[\n        ('ohe_sex', OneHotEncoder(handle_unknown='ignore'), [0]),\n        ('nfp', name_feature_prediction(), [1]),\n        ('ohe', OneHotEncoder(handle_unknown='ignore'), [2,3,4,5]),\n        ('featureExtracter', 'passthrough', [6,7])\n    ])),\n    ('pf', PolynomialFeatures(2)),\n    ('svd', TruncatedSVD(n_components=3000)),\n    ('predictor', RidgeClassifierCV(cv=5))\n])","8e0b359b":"StackedNameModel.fit(X,y)\nwrite_prediction(StackedNameModel)","879a4196":"* There is perfect negative correlation between Fare and Pclass\n* There is strong negative correlation between Age and Pclass\n* There is positive correlation between Fare and SibSp, Fare and Parch, Fare and Survived\n* There is relatively stronger positive correlation between Parch and SibSp","b56445ae":"# Titanic Dataset EDA and Prediction Models\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nUsing the patterns we find in the train.csv data, we are predicting whether the other 418 passengers on board (found in test.csv) survived.","6520ad08":"## Prediction model using numeric columns only","89b7a99b":"## A few utilities function","722962e2":"grid = {'cv__n_features': 4000, 'cv__ngram_range': (1, 1), 'cv__norm': 'l1', 'cv__stop_words': 'english', 'predictor__alpha': 2.154434690031882}\nscore: 0.794,\nscorer: default\n    \ngrid = {'cv__n_features': 4000, 'cv__ngram_range': (1, 1), 'cv__norm': 'l1', 'cv__stop_words': 'english',\n'predictor__alpha': 2.154434690031882}\nscore: 0.765,\nscorer: recall\n\ngrid = {'cv__n_features': 62000, 'cv__ngram_range': (1, 2), 'cv__norm': 'l2', 'cv__stop_words': 'english',\n 'predictor__alpha': 0.46415888336127775}\nscore: 0.79425,\nscorer: f1\n\ngrid = {'cv__n_features': 87000, 'cv__ngram_range': (1, 2), 'cv__norm': 'l2', 'cv__stop_words': 'english',\n 'predictor__alpha': 0.46415888336127775}\nscore: 0.78947,\nscorer: average_precision\n","4a381c56":"## Hypertuning Current best : NumCatTextRidgeModel model","cc11b759":"## Visualizing feature correlation","a735f6f2":"## Model with Polynomial features, Iterative Imputer and Hashing Vectorizer\n**Changes to train data**\n* Age,Fare : Iterative Imputing using ExtraForestRegressor\n* Embarked,Parch,Pclass,SibSp : Imputing with most_frequent and Encoding using OneHotEncoder\n* Added Text feature __Name__ using Hashing Vectorizer","3786984a":"## Prediction model after imputing values\n\n**Changes to train data**\n* Age : Imputing with Median value\n* Embarked : Imputing with most_frequent and Encoding using OneHotEncoder\n* Sex,Parch,Pclass,SibSp : Encoding using OneHotEncoder\n* Fare : Imputing with mean values","59aa6e2c":"## Model with Polynomial features and Iterative Imputer\n**Changes to train data**\n* Age,Fare : Iterative Imputing using ExtraForestRegressor\n* Embarked,Parch,Pclass,SibSp : Imputing with most_frequent and Encoding using OneHotEncoder","2c2e4566":"## Visualizing NULL data","a3c28770":"## Importing required python libraries","696ad251":"## Next Steps:\n* prediction model using just the Name column\n* Hyperparameter tuning of Name Predictor model\n* Hyperparameter tuning of the complete model","a4b80ddb":"## Importing test and train data","16450bc1":"### This model shoes that there is a highly positive correlation between Survived and Name feature\n#### Next Step\n* We will hypertune name_predictor_model using GridSearchCV for a better score","ffd11806":"## Concatenating train and test data without submission column"}}