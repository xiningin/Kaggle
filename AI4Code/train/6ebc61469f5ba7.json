{"cell_type":{"57756756":"code","afbdda3c":"code","44f28e6c":"code","7d7c4ba3":"code","57354544":"code","45397a94":"code","d3eea221":"code","67e1097a":"code","fc414d4c":"code","1e98ef82":"code","c1634c82":"code","a2bf55e8":"code","4295a16a":"code","7c46c0b6":"code","cb02ff0f":"code","ebde45dc":"code","b7af1720":"code","e3cec4a3":"code","b9ffb738":"code","91a02813":"code","1b9e0e7f":"code","f5e5b8a0":"code","b7ea3f80":"markdown","36d47765":"markdown","848b3141":"markdown","e27eabe5":"markdown","20260e28":"markdown","db5ffb43":"markdown"},"source":{"57756756":"intents = []\nintents.append(\n        {\"tag\": \"greeting\",\n         \"patterns\": [\"Hi\", \"How are you\", \"Is anyone there?\", \"Hello\", \"Good day\"],\n         \"responses\": [\"Hello, thanks for visiting\", \"Good to see you again\", \"Hi there, how can I help?\"],\n        }\n)","afbdda3c":"intents.append(\n        {\"tag\": \"thanks\",\n         \"patterns\": [\"Thanks\", \"Thank you\", \"That's helpful\"],\n         \"responses\": [\"Happy to help!\", \"Any time!\", \"My pleasure\"]\n        }\n)","44f28e6c":"intents.append(\n        {\"tag\": \"hours\",\n         \"patterns\": [\"What hours are you open?\", \"What are your hours?\", \"When are you open?\" ],\n         \"responses\": [\"We're open every day 9am-9pm\", \"Our hours are 9am-9pm every day\"]\n        }\n)","7d7c4ba3":"intents.append(\n        {\"tag\": \"location\",\n         \"patterns\": [\"What is your location?\", \"Where are you located?\", \"What is your address?\", \"Where is your restaurant situated?\" ],\n         \"responses\": [\"We are on the intersection of London Alley and Bridge Avenue.\", \"We are situated at the intersection of London Alley and Bridge Avenue\", \"Our Address is: 1000 Bridge Avenue, London EC3N 4AJ, UK\"]\n        }\n)","57354544":"intents.append(\n        {\"tag\": \"payments\",\n         \"patterns\": [\"Do you take credit cards?\", \"Do you accept Mastercard?\", \"Are you cash only?\" ],\n         \"responses\": [\"We accept VISA, Mastercard and AMEX\", \"We accept most major credit cards\"]\n        }\n)","45397a94":"intents.append(\n        {\"tag\": \"todaysmenu\",\n         \"patterns\": [\"What is your menu for today?\", \"What are you serving today?\", \"What is today's special?\"],\n         \"responses\": [\"Today's special is Chicken Tikka\", \"Our speciality for today is Chicken Tikka\"]\n        }\n)","d3eea221":"intents.append(\n        {\"tag\": \"deliveryoption\",\n         \"patterns\": [\"Do you provide home delivery?\", \"Do you deliver the food?\", \"What are the home delivery options?\" ],\n         \"responses\": [\"Yes, we provide home delivery through UBER Eats and Zomato?\", \"We have home delivery options through UBER Eats and Zomato\"]\n        }\n)","67e1097a":"intents.append(\n        {\"tag\": \"menu\",\n         \"patterns\": [\"What is your Menu?\", \"What are the main course options?\", \"Can you tell me the most delicious dish from the menu?\", \"What is the today's special?\"],\n         \"responses\": [\"You can visit www.mymenu.com for menu options\", \"You can check out the food menu at www.mymenu.com\", \"You can check various delicacies given in the food menu at www.mymenu.com\"],\n        }\n)","fc414d4c":"intents.append(\n    {'patterns': ['Bye', 'See you later', 'Goodbye'],\n     'responses': ['See you later, thanks for visiting','Have a nice day','Bye! Come back again soon.'],\n     'tag': 'goodbye'\n    }\n)","1e98ef82":"import nltk\nnltk.download(\"punkt\")\nfrom nltk.stem.lancaster import LancasterStemmer\nstemmer = LancasterStemmer()","c1634c82":"import tensorflow as tf\nimport numpy as np\nimport random","a2bf55e8":"import string\nprint(string.punctuation)\n\ndef remove_punc(a):\n  return a.translate(str.maketrans('', '', string.punctuation))\n\nremove_punc(\"string. With. Punctuation?\")","4295a16a":"words = []\nclasses = []\ndocuments = []\n\nfor intent in intents:\n  for pattern in intent[\"patterns\"]:\n    pattern_2 = remove_punc(pattern)\n    w = nltk.word_tokenize(pattern_2)\n    words.extend(w)\n    \n    documents.append((w, intent[\"tag\"]))\n\n    if intent[\"tag\"] not in classes:\n      classes.append(intent[\"tag\"])","7c46c0b6":"words = [stemmer.stem(word.lower())  for word in words]\nwords = list(set(words))\nwords","cb02ff0f":"y = []\nX = []\nfor doc in documents:\n  bag = []\n  pattern_words = doc[0]\n  pattern_words = [stemmer.stem(word.lower())  for word in pattern_words]\n  for w in words:\n    bag.append(1 if w in pattern_words else 0)\n  y.append(classes.index(doc[1]))\n  X.append(bag)","ebde45dc":"X, y = np.array(X), np.array(y)\nfrom tensorflow.keras.utils import to_categorical\ny = to_categorical(y)","b7af1720":"# ANN\n\nmodel = tf.keras.Sequential([\n\n        # input layer\n        tf.keras.layers.Flatten(input_shape=(X.shape[1], )),\n\n        \n        tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n        \n        tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n        \n        \n        tf.keras.layers.Dense(len(classes), activation='softmax')\n    ])\n\noptimiser = tf.keras.optimizers.Adam(learning_rate=0.0001)\nmodel.compile(optimizer=optimiser,\n                  loss='categorical_crossentropy',\n                  metrics=['categorical_accuracy'])\nmodel.summary()\n\nhistory = model.fit(X, y, batch_size=8, epochs=1000, verbose = 0)\n","e3cec4a3":"import matplotlib.pyplot as plt\ndef plot_history(history):\n    plt.plot(history.history['categorical_accuracy'])\n    #plt.plot(history.history['val_categorical_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(history.history['loss'])\n    #plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\nplot_history(history)","b9ffb738":"# Get Bag of Words from giving string\ndef get_bag_of_words(a):\n  a = remove_punc(a)\n  b = nltk.word_tokenize(a)\n  b = [stemmer.stem(word.lower()) for word in b]\n  bag = []\n  for w in words:\n    bag.append(1 if w in b else 0)\n  return np.array(bag)\n\n# Classify string by tag\ndef classify(a):\n  bag = get_bag_of_words(a)\n  pred = model.predict(bag.reshape(1, 56, 1))\n  max = [0, 0]\n  for i in range(pred.shape[1]):\n    if max[0] < pred[0][i]:\n      max[0] = pred[0][i]\n      max[1] = i\n  return classes[max[1]], max[0]\n\n# Generate response for given tag\ndef response(class_name):\n  for intent in intents:\n    if intent[\"tag\"] == class_name:\n      responses = intent[\"responses\"]\n      return responses[random.randint(1, len(responses)) - 1]\n\n# Classify and response for given query\ndef classify_and_response(a):\n  classify_ = classify(a)\n  print(classify_)\n  class_name = classify_[0]\n  print(response(class_name))","91a02813":"classify_and_response('What are you hours of operation?')","1b9e0e7f":"classify_and_response('What is menu for today?')","f5e5b8a0":"classify_and_response('Do you accept Credit Card?')","b7ea3f80":"We will download nltk library for tokenizing and stemming the words","36d47765":"Okay first we will write some patterns and their responses","848b3141":"We will write a function for remove punctuation from string","e27eabe5":"Extract words, classes and documents","20260e28":"And finally let's test our ChatBot","db5ffb43":"For every pattern in document we extract bag of words"}}