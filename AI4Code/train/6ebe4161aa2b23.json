{"cell_type":{"ce279c0b":"code","7f0ec4c3":"code","826a9544":"code","cf3b7516":"code","164f9f71":"code","9d3cceb6":"code","1576c119":"code","22ecc0d0":"code","50fe930a":"code","0e207e1a":"code","2eaa1b9f":"code","553eada0":"code","c7bacd98":"code","deaa4a99":"code","5000ae76":"code","c3820d5f":"code","16d26384":"markdown","3cef3c88":"markdown","df97ca5e":"markdown","7d27662a":"markdown","697d03fb":"markdown","d788dc47":"markdown","984baef9":"markdown","8d06bb9f":"markdown"},"source":{"ce279c0b":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skimage import io, transform\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Sequential\nfrom tqdm import tqdm","7f0ec4c3":"SEED = 42\nEPOCHS = 50\nBATCH_SIZE = 32\nIMG_SIZE = 64\nIMG_ROOT = '..\/input\/chinese-mnist\/data\/data\/'\n\ntrain_df = pd.read_csv('..\/input\/chinese-mnist\/chinese_mnist.csv')","826a9544":"def seed_everything(seed):\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n\nseed_everything(SEED)","cf3b7516":"train_df","164f9f71":"train_df.isnull().sum()","9d3cceb6":"train_df['character'].value_counts()","1576c119":"def create_file_name(x):\n    file_name = f'input_{x[0]}_{x[1]}_{x[2]}.jpg'\n    return file_name\n\n\ndef add_filenames(df, img_root):\n    filenames = list(os.listdir(img_root))\n    df['filenames'] = df.apply(create_file_name, axis=1)\n    return df \n    \ntrain_df = add_filenames(train_df, IMG_ROOT)\ntrain_df","22ecc0d0":"train_df, test_df = train_test_split(train_df, \n                                     test_size=0.2,\n                                     random_state=SEED,\n                                     stratify=train_df['character'].values) \ntrain_df, val_df = train_test_split(train_df,\n                                    test_size=0.1,\n                                    random_state=SEED,\n                                    stratify=train_df['character'].values)","50fe930a":"def create_datasets(df, img_root, img_size, n):\n    imgs = []\n    for filename in tqdm(df['filenames']):\n        img = io.imread(img_root+filename)\n        img = transform.resize(img, (img_size,img_size,n))\n        imgs.append(img)\n        \n    imgs = np.array(imgs)\n    df = pd.get_dummies(df['character'])\n    return imgs, df\n\n\ntrain_imgs, train_df = create_datasets(train_df, IMG_ROOT, IMG_SIZE, 1)\nval_imgs, val_df = create_datasets(val_df, IMG_ROOT, IMG_SIZE, 1)\ntest_imgs, test_df = create_datasets(test_df, IMG_ROOT, IMG_SIZE, 1)","0e207e1a":"input_shape = (IMG_SIZE, IMG_SIZE, 1)\n\nmodel = Sequential()\nmodel.add(Conv2D(16, kernel_size=3, padding='same', input_shape=input_shape, activation='relu'))\nmodel.add(Conv2D(16, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPool2D(3))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(15, activation='softmax'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n\nmodel.summary()","2eaa1b9f":"es_callback = tf.keras.callbacks.EarlyStopping(patience=20, \n                                               verbose=1, \n                                               restore_best_weights=True)\n\n\nhistory = model.fit(train_imgs, \n                    train_df, \n                    batch_size=BATCH_SIZE, \n                    epochs=EPOCHS, \n                    callbacks=[es_callback],\n                    validation_data=(val_imgs, val_df))\n\npd.DataFrame(history.history)[['accuracy', 'val_accuracy']].plot()\npd.DataFrame(history.history)[['loss', 'val_loss']].plot()\nplt.show()","553eada0":"model.evaluate(test_imgs, test_df) ","c7bacd98":"model = Sequential()\nmodel.add(Conv2D(16, kernel_size=3, padding='same', input_shape=input_shape, activation='relu'))\nmodel.add(Conv2D(16, kernel_size=3, padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(3))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(16, kernel_size=3, padding='same', activation='relu'))\nmodel.add(Conv2D(16, kernel_size=3, padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(3))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(15, activation='softmax'))\nopt = tfa.optimizers.LazyAdam()\nloss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.025)\nmodel.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n\n\nmodel.summary()","deaa4a99":"def get_lr_callback(batch_size=32, plot=False):\n    lr_start   = 0.003\n    lr_max     = 0.00125 * batch_size\n    lr_min     = 0.001\n    lr_ramp_ep = 20\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        return lr\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n    if plot == True:\n        rng = [i for i in range(EPOCHS)]\n        y = [lrfn(x) for x in rng]\n        plt.plot(rng, y)\n        plt.xlabel('epoch', size=14); plt.ylabel('learning_rate', size=14)\n        plt.title('Training Schedule', size=16)\n        plt.show()\n    return lr_callback\n\nget_lr_callback(plot=True)","5000ae76":"es_callback = tf.keras.callbacks.EarlyStopping(patience=20, \n                                               verbose=1, \n                                               restore_best_weights=True)\n\n\nhistory = model.fit(train_imgs, \n                    train_df, \n                    batch_size=BATCH_SIZE, \n                    epochs=EPOCHS, \n                    callbacks=[es_callback, get_lr_callback(BATCH_SIZE)],\n                    validation_data=(val_imgs, val_df))\n\npd.DataFrame(history.history)[['accuracy', 'val_accuracy']].plot()\npd.DataFrame(history.history)[['loss', 'val_loss']].plot()\nplt.show()","c3820d5f":"model.evaluate(test_imgs, test_df) ","16d26384":"## Introduction\n\nThis notebook challenges the task of image classification using an interesting datasets published by [Gabriel Preda].\n\nWe will proceed with a simple approach without using image data augmentation or pretrained models.\n\n[Gabriel Preda]: https:\/\/www.kaggle.com\/gpreda","3cef3c88":"## Set Configurations and read metadata.","df97ca5e":"## Build the model (1)\nFirst, build a simple neural network and try it.","7d27662a":"<h1><font size=\"6\">Chinese MNIST Classification<\/font><\/h1>","697d03fb":"## Create datasets","d788dc47":"## Checking metadata.\nCheck for missing values and the balance of the dataset.","984baef9":"## Import libraries","8d06bb9f":"## Build the model (2)\nMake some changes and try again.\n1. Add some layers.\n2. Change the optimization function.\n3. Apply label smoothing.\n4. Add a learning rate scheduler."}}