{"cell_type":{"44fe325d":"code","53a014fa":"code","253b891f":"code","f03ad846":"code","3e49241b":"code","533445dd":"code","107c79d2":"code","fcb5b488":"code","8b4739cf":"code","ef11d648":"code","0485edb6":"code","a9d0463e":"code","a62be04d":"code","461eea7f":"code","1722a36e":"code","4baa0ab9":"code","af609073":"code","0758352c":"code","e2cfff22":"code","2d26e102":"code","38259d78":"code","3713e8c2":"code","b1ba9aa7":"code","fd1d07fc":"code","c7383033":"code","d621749d":"markdown","7a81c7d0":"markdown","6fc2d188":"markdown","ae94a331":"markdown","74572fa7":"markdown","231bb992":"markdown","3bcbba2f":"markdown","e2de4c7c":"markdown","36e15c9a":"markdown","cbacae6a":"markdown","fcfa3b24":"markdown","ed3c3059":"markdown","80eaa5c9":"markdown"},"source":{"44fe325d":"from fastai.tabular import * \nfrom fastai.callbacks import *\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","53a014fa":"PATH = '\/kaggle\/input\/covid19-global-forecasting-week-3\/'\ntrain_df = pd.read_csv(PATH + 'train.csv', parse_dates=['Date'])\ntest_df = pd.read_csv(PATH + 'test.csv', parse_dates=['Date'])\n\nadd_datepart(train_df, 'Date', drop=False)\nadd_datepart(test_df, 'Date', drop=False)","253b891f":"PATH1 = '\/kaggle\/input\/covid19-country-data-wk3-release\/'\nmeta_df = pd.read_csv(PATH1 + 'Data Join - RELEASE.csv', thousands=\",\")\n\nPATH2 = '\/kaggle\/input\/countryinfo\/'\ncountryinfo = pd.read_csv(PATH2 + 'covid19countryinfo.csv', thousands=\",\", parse_dates=['quarantine', 'schools', 'publicplace', 'gathering', 'nonessential'])\ntestinfo = pd.read_csv(PATH2 + 'covid19tests.csv', thousands=\",\")\n\ncountryinfo.rename(columns={'region': 'Province_State', 'country': 'Country_Region'}, inplace=True)\ntestinfo.rename(columns={'region': 'Province_State', 'country': 'Country_Region'}, inplace=True)\ntestinfo = testinfo.drop(['alpha3code', 'alpha2code', 'date'], axis=1)\n\nPATH3 = '\/kaggle\/input\/covid19-forecasting-metadata\/'\ncontinent_meta = pd.read_csv(PATH3 + 'region_metadata.csv')\ncontinent_meta = continent_meta[['Country_Region' ,'Province_State', 'continent']]\n\ndef fill_unknown_state(df):\n    df.fillna({'Province_State': 'Unknown'}, inplace=True)\n    \nfor d in [train_df, test_df, meta_df, countryinfo, testinfo, continent_meta]:\n    fill_unknown_state(d)","f03ad846":"idx_group = ['Country_Region', 'Province_State']\n\ndef day_reached_cases(df, name, no_cases=1):\n    \"\"\"For each country\/province get first day of year with at least given number of cases.\"\"\"\n    gb = df[df['ConfirmedCases'] >= no_cases].groupby(idx_group)\n    return gb.Dayofyear.first().reset_index().rename(columns={'Dayofyear': name})\n\ndef area_fatality_rate(df):\n    \"\"\"Get average fatality rate for last known entry, for each country\/province.\"\"\"\n    gb = df[df['Fatalities'] >= 22].groupby(idx_group)\n    res_df = (gb.Fatalities.last() \/ gb.ConfirmedCases.last()).reset_index()\n    return res_df.rename(columns={0 : 'FatalityRate'})","3e49241b":"def joined_data(df):\n    res = df.copy()\n    \n    fatality = area_fatality_rate(train_df)\n    first_nonzero = day_reached_cases(train_df, 'FirstCaseDay', 1)\n    first_fifty = day_reached_cases(train_df, 'First50CasesDay', 50)\n    \n    # Add external features\n    res = pd.merge(res, continent_meta, how='left')\n    res = pd.merge(res, meta_df, how='left')\n    res = pd.merge(res, countryinfo, how='left')\n    res = pd.merge(res, testinfo, how='left', left_on=idx_group, right_on=idx_group)\n    \n    # Add calculated features\n    res = pd.merge(res, fatality, how='left')\n    res = pd.merge(res, first_nonzero, how='left')\n    res = pd.merge(res, first_fifty, how='left')\n    return res\n\ntrain_df = joined_data(train_df)\ntest_df = joined_data(test_df)","533445dd":"# It turns out any country in train dataset has at least one case.\ntrain_df.FirstCaseDay.isna().sum()","107c79d2":"def with_new_features(df, train=True):\n    res = df.copy()\n    add_datepart(res, 'quarantine', prefix='qua')\n    add_datepart(res, 'schools', prefix='sql')\n    \n    res['DaysSinceFirst'] = res['Dayofyear'] - res['FirstCaseDay']\n    res['DaysSince50'] = res['Dayofyear'] - res['First50CasesDay']\n    res['DaysQua'] = res['Dayofyear'] - res['quaDayofyear']\n    res['DaysSql'] = res['Dayofyear'] - res['sqlDayofyear']\n    \n    # Since we will take log of dependent variable, we won't make it nonzero.\n    if train:\n        res['ConfirmedCases'] += 1\n    return res\n    \ntrain_df = with_new_features(train_df)\ntest_df = with_new_features(test_df, train=False)","fcb5b488":"# Categorical variables - only basic identifiers, some features like continent will be worth adding.\ncat_vars = ['Country_Region', 'Province_State',\n            'continent'\n#             'publicplace', 'gathering', 'nonessential'\n           ]\n\n# Continuous variables - just ones directly connected with time.\ncont_vars = ['DaysSinceFirst', 'DaysSince50', 'Dayofyear',\n            'DaysQua', 'DaysSql',\n            'TRUE POPULATION', \n            'testper1m', 'positiveper1m',\n            'casediv1m', 'deathdiv1m', \n            'FatalityRate',\n#             'density', 'urbanpop', 'medianage', 'hospibed','healthperpop', 'fertility',\n#             'smokers', 'lung', \n#             'continent_gdp_pc', 'continent_happiness', 'continent_Life_expectancy','GDP_region', \n#             'latitude', 'abs_latitude', 'longitude', 'temperature', 'humidity',\n            ]\n\n# We will predict only confirmed cases. \n# For fatalities, one could train another model but we won't do it - multiplying by average fatality in each area is enough for a sample submission.\ndep_var = 'ConfirmedCases'\n\ndf = train_df[cont_vars + cat_vars + [dep_var,'Date']].copy().sort_values('Date')","8b4739cf":"print(test_df.Date.min())\nMAX_TRAIN_IDX = df[df['Date'] < test_df.Date.min()].shape[0]","ef11d648":"procs=[FillMissing, Categorify, Normalize]\npath = '\/kaggle\/working\/'\ndata = (TabularList.from_df(df, path=path, cat_names=cat_vars.copy(), cont_names=cont_vars.copy(), procs=procs,)\n                # Take only rows before beginning of the test set - as explained above.\n                .split_by_idx(list(range(MAX_TRAIN_IDX, len(df))))\n                .label_from_df(cols=dep_var, label_cls=FloatList, log=True)\n                .add_test(TabularList.from_df(test_df, path=path, cat_names=cat_vars, cont_names=cont_vars))\n                .databunch())","0485edb6":"data.show_batch(ds_type=DatasetType.Train, rows=5)","a9d0463e":"learn = tabular_learner(data, layers=[1000, 500], ps=[0.001,0.01], emb_drop=0.04, \n                        metrics=root_mean_squared_error)\n\ncalls = [SaveModelCallback(learn, mode='min', every='improvement', monitor='root_mean_squared_error', name='best')]","a62be04d":"learn.lr_find(start_lr=1e-8, end_lr=1e-2, num_it=300)\nlearn.recorder.plot()","461eea7f":"learn.fit_one_cycle(25, callbacks=calls)","1722a36e":"learn.recorder.plot_losses()","4baa0ab9":"learn.load('best')\nlearn.fit_one_cycle(10, 5e-5, callbacks=calls)","af609073":"learn.load('best')\nlearn.fit_one_cycle(10, 5e-6, callbacks=calls)","0758352c":"learn.load('best')\nlearn.fit_one_cycle(10, 5e-7, callbacks=calls)","e2cfff22":"learn.load('best')","2d26e102":"preds1, y = learn.get_preds(ds_type=DatasetType.Test)","38259d78":"res1 = np.expm1(preds1)\nres2 = list(map(lambda x: x[0], res1.numpy()))\nsubmit = pd.DataFrame({'ConfirmedCases': res2})\nsubmit.index = test_df.ForecastId","3713e8c2":"fatality_series = test_df.FatalityRate.copy()\nfatality_series.index += 1\nfatality_series.fillna(0.02137, inplace=True)\n\nsubmit['Fatalities'] = (submit.ConfirmedCases > 69) * fatality_series * submit.ConfirmedCases","b1ba9aa7":"submit.to_csv('submission.csv')","fd1d07fc":"import seaborn as sns\n\nmin_date = test_df.Date.min()\nmax_date = train_df.Date.max()\n\nf, axes = plt.subplots(10, 1, figsize=(16, 60))\n\ndef plot_preds(country, ax):\n    targets = train_df[(train_df['Country_Region'] == country) & (train_df['Date'] >= min_date)].ConfirmedCases\n    subset = test_df[(test_df['Country_Region'] == country) & (test_df['Date'] <= max_date)]\n    \n    idx = subset.index\n    dates = subset.Date\n    predicted = submit.iloc[idx].ConfirmedCases\n    \n    targets.index = dates\n    predicted.index = dates\n    \n    combined = pd.DataFrame({'real' : targets, 'pred': predicted})\n    \n    sns.lineplot(data=combined, ax=axes[ax]).set_title(country)\n\nplot_preds('Italy', 0)\nplot_preds('Spain', 1)\nplot_preds('Germany', 2)\nplot_preds('Poland', 3)\nplot_preds('Czechia', 4)\nplot_preds('Russia', 5)\nplot_preds('Iran', 6)\nplot_preds('Sweden', 7)\nplot_preds('Japan', 8)\nplot_preds('Belgium', 9)","c7383033":"res = submit.iloc[test_df[(test_df['Country_Region'] == 'Poland')].index]\nres.index = test_df[(test_df['Country_Region'] == 'Poland')].Date\nres","d621749d":"# Fatalities\n\nWe calculated average fatality rate for each country and region. Although it is not the best predictor, we can use it here. \n\nFor countries that don't have any fatalities yet, we provide a magic value as they exceed another magic number of cases.","7a81c7d0":"# [#masks4all](https:\/\/masks4all.co\/why-we-need-mandatory-mask-laws-masks4all\/)\n\n# Introduction\n\nThe goal of this notebook is to provide some basic [fast.ai](https:\/\/www.fast.ai\/) tabular model for COVID-19 dataset.\n\nAlthough it is not the best approach here, it requires reasonably small amount of code and obviously no feature engineering.\n\nThe solution utilizes mostly [fast.ai](https:\/\/www.fast.ai\/) library and stuff included in [this course](https:\/\/course.fast.ai\/)","6fc2d188":"# Add temporal features\nSome basic features like number of days since the first case in each country\/province with analogous feature for 50 days may be worth adding.","ae94a331":"# Load input data","74572fa7":"# Example predictions\n\n","231bb992":"# Data preprocessing for the model\n\nBasically vanilla fast.ai stuff here, including taking log of dependent variable.","3bcbba2f":"# Model\n\nBaseline fast.ai tabular learner with RMSE metrics (we took log before, so it is RMSLE)\n\nAs mentioned before, our dependent variable is number of confirmed cases. We will provide a simple estimate for fatalities later.","e2de4c7c":"# Add external features to input dataframes\nFor each country, we want to extract the first day when it reached at least 1 and 50 cases.\n\nAverage fatality rate will be calculated from last data available, simply taking deaths \/ cases.","36e15c9a":"# Feature selection\nIn fast.ai we can easily select categorical and continuous variables for training.\n\nI decided not to choose any external data in baseline model. Adding numerical values from country data provided in this notebook doesn't seem to improve the validation score much.","cbacae6a":"# Metadata (continent, population, tests per million etc.)","fcfa3b24":"# Avoid leakage - take only non-overlapping values for training\n\nFor now, the only available data to validate our model is in training set. \n\nAs our test set starts on **26.03.2020**, we should take only rows before that date for training to avoid leakage.","ed3c3059":"# Preparing submission","80eaa5c9":"# Conclusion\n\nAs neural networks are thought not to perform well on tabular data, which is true especially for some trivial architectures like the MLP used here, we cannot expect much from this model.\n\nWhat is interesting, additional continuous variables not dependent on time don't seem to provide any improvement in our validation score.\n\nPredictions of confirmed cases look quite legit for short time windows like the one in validation set (up to 10 days). For later dates especially in May, we can see some unreasonable exponential behaviour.\n\n# [#masks4all](https:\/\/masks4all.co\/why-we-need-mandatory-mask-laws-masks4all\/)"}}