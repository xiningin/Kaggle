{"cell_type":{"c248ae14":"code","ef17a58e":"code","f3be872d":"code","f22336f2":"code","f2435765":"code","ea300421":"code","0319b1d3":"code","c6b318f9":"code","fe93bf49":"code","0e104688":"code","4cb88ea9":"code","1a1d8d9f":"code","c4c623cb":"code","6a471b30":"code","a4e0b404":"code","aa43a8dd":"code","ee2b35a3":"code","2705d101":"code","76b94562":"code","c909a2df":"code","17c3314c":"code","c9e36466":"code","85bb38cb":"code","b693eb40":"code","b6b920d4":"code","3be3bd47":"code","b8beb697":"code","317e5a41":"code","0105283b":"code","4e2ff3c6":"code","d5f5d8fa":"code","abe9e96e":"markdown","b323381a":"markdown","786a234d":"markdown","0545e70e":"markdown","200f816e":"markdown","1929d69b":"markdown","5946107c":"markdown","53d603c4":"markdown","0e3b32de":"markdown","7c03d1d0":"markdown","f35e00e7":"markdown","c52650ec":"markdown","9df3470f":"markdown","4c893bb7":"markdown","afaf0ff2":"markdown","1ffbfc93":"markdown","11fc6934":"markdown","76f5b441":"markdown","7de25322":"markdown"},"source":{"c248ae14":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n#To display plots inline; within the notebook window\n%matplotlib inline","ef17a58e":"dataset = pd.read_csv('..\/input\/coronavirus-covid19-tweets\/2020-03-28 Coronavirus Tweets.CSV')","f3be872d":"dataset.columns","f22336f2":"#Countries with most tweets\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\n# count the occurrence of each class \ndata = dataset[\"country_code\"].value_counts() \n# get x and y data \npoints = data.index[:20]\nfrequency = data.values[:20]\n#plot graph\nax.barh(points,frequency)\nplt.title(\"Countries with most tweets\")\nplt.xlabel(\"Number of Tweets\")\nplt.ylabel(\"Country\")\nplt.show()\n","f2435765":"#Tweets from verfied\/ normal account\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\n# count the occurrence of each class \ndata = dataset[\"verified\"].value_counts() \n# get x and y data \npoints = data.index\nfrequency = data.values\n#plot graph\nax.barh(points,frequency)\nax.set_yticks([0,1,2])\nax.set_yticklabels([\"Normal\",\"Verified\"])\nplt.xlabel(\"Number of Tweets\")\nplt.ylabel(\"Type of Account\")\nplt.title(\"Tweets from verfied\/ normal account\")\nplt.show()","ea300421":"#Tweets existing\/ new account\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\n# count the occurrence of each class \ntemp_data = dataset.copy()\ntemp_data[\"account_created_at\"] = [i[:4] for i in temp_data[\"account_created_at\"]]\ndata = temp_data[\"account_created_at\"].value_counts() \n# get x and y data \npoints = data.index\nfrequency = data.values\n#plot graph\nax.barh(points,frequency)\nax.set_yticklabels(data.index)\nplt.xlabel(\"Number of Tweets\")\nplt.ylabel(\"Languages\")\nplt.title(\"Tweets existing\/ new account\")\nplt.show()","0319b1d3":"#Tweets-languages\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\n# count the occurrence of each class \ndata = dataset[\"lang\"].value_counts() \n# get x and y data \npoints = data.index[:20]\nfrequency = data.values[:20]\n#plot graph\nax.barh(points,frequency)\nplt.title(\"Tweets-Languges\")\nplt.show()","c6b318f9":"import nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('vader_lexicon')\nnltk.download('punkt')\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.sentiment.util import *\nfrom nltk.tokenize import word_tokenize","fe93bf49":"#Extracting English Tweets of all countries\ntweets= dataset[[\"text\",\"country_code\"]][dataset['lang'] == 'en'].reset_index()\ntweets.drop([\"index\"],axis=1)\n\n#maintain a copy of original tweets before processing it\ntweets_original = tweets.copy()","0e104688":"#Retaining only alphabets (removing all punctuations and numbers)\ntweets[\"text\"] = [re.sub('[^a-zA-Z]', ' ',i) for i in tweets[\"text\"]]\n\n#Converting into lower case \ntweets[\"text\"] = [i.lower() for i in tweets[\"text\"]]\n\n#Removing Emoticons\ndef deEmojify(inputString):\n    return inputString.encode('ascii', 'ignore').decode('ascii')\ntweets[\"text\"]  = [deEmojify(i) for i in tweets[\"text\"] ]\n\n\n#Removing URLs\ndef removeURLs(str):\n    ans = \"\"\n    clean_tweet1 = re.match('(.*?)http.*?\\s?(.*?)', str)\n    clean_tweet2 = re.match('(.*?)https.*?\\s?(.*?)', str)\n    if clean_tweet1:\n        ans=ans+clean_tweet1.group(1)\n        ans=ans+clean_tweet1.group(2)\n    elif clean_tweet2: \n        ans=ans+clean_tweet2.group(1)\n        ans=ans+clean_tweet2.group(2)\n    else:\n        ans = str\n    return ans\n\n\ntweets[\"text\"] = tweets[\"text\"].apply(lambda tweet: removeURLs(tweet))\n\n\n#Removing Stop Words\ncachedStopWords = set(stopwords.words(\"english\"))\ntweets[\"text\"] = tweets[\"text\"].apply(lambda tweet: ' '.join([word for word in tweet.split() if word not in cachedStopWords]))\n\n#Define words that we do not want to Stem or Lemmatize\nspecialWords = [\"coronavirus\", \"covid\",\"quarantine\",\"coronavirusoutbreak\",\"virus\",\"corona\",\"lockdown\"]\n\n#Stemming\nps = PorterStemmer()\ndef stemWords(word):\n    if word in specialWords:\n            return word\n    else:\n        return ps.stem(word)\n        \ntweets[\"text\"] = tweets[\"text\"].apply(lambda tweet: ' '.join([stemWords(word) for word in tweet.split()]))\n\n\n#Lemmatization: \nwnl = WordNetLemmatizer()\ndef lemmatizeWords(word):\n    if word in specialWords:\n            return word\n    else:\n        return wnl.lemmatize(word)\ntweets[\"text\"] = tweets[\"text\"].apply(lambda tweet: ' '.join([lemmatizeWords(word) for word in tweet.split()]))\n\n\n#Preparing corpus\ncorpus=[]\ncorpus = [word for tweet in tweets[\"text\"] for word in tweet.split()]","4cb88ea9":"#Using Bag of Words\nfrom operator import itemgetter  \nvectorizer = CountVectorizer(max_features = 30)\ncv = vectorizer.fit_transform(tweets[\"text\"]).toarray()\nterms = vectorizer.get_feature_names()\nfreqs = cv.sum(axis=0)\nresult = dict(zip(terms, freqs))\nprint(result)\nfeatures = []\nvals = []\nfor key, value in sorted(result.items(), key = itemgetter(1), reverse = True):\n    features.append(key)\n    vals.append(value)\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\n# get x and y data \npoints = features\nfrequency = vals\n#plot graph\nax.barh(points,frequency)\nplt.title(\"Most Tweeted Words\")\nplt.show()","1a1d8d9f":"#Using Term Frequency (TF-IDF)\n\ntf=TfidfTransformer(smooth_idf=True,use_idf=True)\ntf.fit(cv)\n\n# print idf values\ndata = { \"Word\" :vectorizer.get_feature_names(), \"idf_weights\":tf.idf_}\ndf_idf = pd.DataFrame(data)\n \n# sort ascending\ndf_idf = df_idf.sort_values(by=['idf_weights'])\n\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\n# get x and y data \npoints = df_idf[\"Word\"]\nfrequency = df_idf[\"idf_weights\"]\n#plot graph\nax.barh(points,frequency)\nplt.title(\"IDF weights of Most Tweeted Words -> Least weights = Most Importance\")\nplt.show()\n\n","c4c623cb":"wordcloud = WordCloud(\n    background_color='black',\n    max_words=50,\n    max_font_size=40, \n    scale=5,\n    random_state=1,\n    collocations=False,\n    normalize_plurals=False\n).generate(' '.join(corpus))\n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor=\"None\") \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.title(\"WordCloud of Corona-Tweets\")\nplt.tight_layout(pad = 0) \nplt.show()","6a471b30":"s = SentimentIntensityAnalyzer()\nscores = tweets[\"text\"].apply(lambda tweet: s.polarity_scores(tweet))\nscores_df = pd.DataFrame(list(scores))\nscores_df.head()","a4e0b404":"#Calculating sentiments for all tweets\nscores_df['result'] = scores_df['compound'].apply(lambda res: 'neutral' if res == 0 else ('positive' if res > 0 else 'negative'))\nscores_df['tweet'] = tweets_original[\"text\"]\nscores_df[\"country_code\"] = tweets_original[\"country_code\"]\nscores_df= scores_df.sort_values(by=['compound'])\n\n\n#Calculating sentiments wrt top 5 tweeting countries\nus = scores_df[[\"tweet\",\"result\",\"country_code\"]][scores_df.country_code == 'US']\nus_positive = us[[\"tweet\",\"result\",\"country_code\"]][us.result == 'positive']\nscore_us_positive = us_positive.shape[0]\nus_negative = us[[\"tweet\",\"result\",\"country_code\"]][us.result == 'negative']\nscore_us_negative = us_negative.shape[0]\nus_neutral = us[[\"tweet\",\"result\",\"country_code\"]][us.result == 'neutral']\nscore_us_neutral = us_neutral.shape[0]\n\nindia = scores_df[[\"tweet\",\"result\",\"country_code\"]][scores_df.country_code == 'IN']\nindia_positive = india[[\"tweet\",\"result\",\"country_code\"]][india.result == 'positive']\nscore_india_positive = india_positive.shape[0]\nindia_negative = india[[\"tweet\",\"result\",\"country_code\"]][india.result == 'negative']\nscore_india_negative = india_negative.shape[0]\nindia_neutral = india[[\"tweet\",\"result\",\"country_code\"]][india.result == 'neutral']\nscore_india_neutral = india_neutral.shape[0]\n\nuk = scores_df[[\"tweet\",\"result\",\"country_code\"]][scores_df.country_code == 'GB']\nuk_positive = uk[[\"tweet\",\"result\",\"country_code\"]][uk.result == 'positive']\nscore_uk_positive = uk_positive.shape[0]\nuk_negative = uk[[\"tweet\",\"result\",\"country_code\"]][uk.result == 'negative']\nscore_uk_negative = uk_negative.shape[0]\nuk_neutral = uk[[\"tweet\",\"result\",\"country_code\"]][uk.result == 'neutral']\nscore_uk_neutral = uk_neutral.shape[0]\n\nspain = scores_df[[\"tweet\",\"result\",\"country_code\"]][scores_df.country_code == 'ES']\nspain_positive = spain[[\"tweet\",\"result\",\"country_code\"]][spain.result == 'positive']\nscore_spain_positive = spain_positive.shape[0]\nspain_negative = spain[[\"tweet\",\"result\",\"country_code\"]][spain.result == 'negative']\nscore_spain_negative = spain_negative.shape[0]\nspain_neutral = spain[[\"tweet\",\"result\",\"country_code\"]][spain.result == 'neutral']\nscore_spain_neutral = spain_neutral.shape[0]\n\ncanada = scores_df[[\"tweet\",\"result\",\"country_code\"]][scores_df.country_code == 'CA']\ncanada_positive = canada[[\"tweet\",\"result\",\"country_code\"]][canada.result == 'positive']\nscore_canada_positive = canada_positive.shape[0]\ncanada_negative = canada[[\"tweet\",\"result\",\"country_code\"]][canada.result == 'negative']\nscore_canada_negative = canada_negative.shape[0]\ncanada_neutral = canada[[\"tweet\",\"result\",\"country_code\"]][canada.result == 'neutral']\nscore_canada_neutral = canada_neutral.shape[0]","aa43a8dd":"fig = plt.figure()\nax = fig.add_axes([0,0,1,1])\ndata = scores_df[\"result\"].value_counts()\n# get x and y data \npoints = data.index\nfrequency = data.values\n#plot graph\nax.bar(points,frequency)\nplt.title(\"Sentiment Analysis of Tweets around the world\")\nplt.show()","ee2b35a3":"data = [[score_us_positive,score_india_positive, score_uk_positive, score_spain_positive, score_canada_positive],\n[score_us_negative,score_india_negative, score_uk_negative, score_spain_negative, score_canada_negative],\n[score_us_neutral,score_india_neutral, score_uk_neutral, score_spain_neutral, score_canada_neutral]]\nX = np.arange(5)\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.set_xticks([0,1,2,3,4,5])\nax.set_xticklabels([\"USA\",\"India\",\"UK\",\"Spain\",\"Canada\"])\nax.bar(X + 0.00, data[0], color = 'g', width = 0.25, label=\"positive\")\nax.bar(X + 0.25, data[1], color = 'r', width = 0.25, label=\"negative\")\nax.bar(X + 0.50, data[2], color = 'b', width = 0.25, label=\"neutral\")\nplt.title(\"Sentiment Analysis in top 5 tweeting countries\")\nplt.legend(loc=\"upper right\")\nplt.show()","2705d101":"pos_word_list=[]\nneu_word_list=[]\nneg_word_list=[]\npos_word_weight=[]\nneu_word_weight=[]\nneg_word_weight=[]\n\ndef get_word_sentiment(text):\n    \n    tokenized_text = nltk.word_tokenize(text)\n    #print(tokenized_text)    \n\n    for word in tokenized_text:\n            if (s.polarity_scores(word)['compound']) >= 0.6:\n                pos_word_list.append(word)\n                pos_word_weight.append(s.polarity_scores(word)['compound'])\n            elif (s.polarity_scores(word)['compound']) <= -0.6:\n                neg_word_list.append(word)\n                neg_word_weight.append(s.polarity_scores(word)['compound'])\n            else:\n                neu_word_list.append(word)\n                neu_word_weight.append(s.polarity_scores(word)['compound'])\n\nfor tweet in tweets[\"text\"]:\n    get_word_sentiment(tweet)\n","76b94562":"print('Total Positive Words in Tweets:',len(pos_word_list))\nprint('Total Negative Words in Tweets:',len(neg_word_list))\nprint('Total Neutral Words in Tweets:',len(neu_word_list)) ","c909a2df":"pos_word_list = list(set(pos_word_list))\nneg_word_list = list(set(neg_word_list))\nneu_word_list = list(set(neu_word_list))\n\nresult_pos = dict(zip(pos_word_list, pos_word_weight))\nresult_neg = dict(zip(neg_word_list, neg_word_weight))\nresult_neu = dict(zip(neu_word_list, neu_word_weight))\n\nfeatures_pos = []\nvals_pos = []\n\nfeatures_neg = []\nvals_neg = []\n\nfeatures_neu = []\nvals_neu = []\n\nfor key, value in sorted(result_pos.items(), key = itemgetter(1), reverse = True):\n    features_pos.append(key)\n    vals_pos.append(value)\nfor key, value in sorted(result_neg.items(), key = itemgetter(1), reverse = True):\n    features_neg.append(key)\n    vals_neg.append(value)\nfor key, value in sorted(result_neu.items(), key = itemgetter(1), reverse = True):\n    features_neu.append(key)\n    vals_neu.append(value)\n\ntop_positive_words = features_pos[:10]\ntop_positive_words_freq = vals_pos[:10]\n\ntop_negative_words = features_neg[:10]\ntop_negative_words_freq = vals_neg[:10]\n\ntop_neutral_words = features_neu[:10]\ntop_neutral_words_freq = vals_neu[:10]\n\n\nprint(top_positive_words)\nprint(top_negative_words)\nprint(top_neutral_words)\n","17c3314c":"fig = plt.figure()\nax = fig.add_axes([0,0,1,1])\n# get x and y data \npoints = top_positive_words\nfrequency = top_positive_words_freq\n#plot graph\nax.barh(points,frequency)\nplt.title(\"Most Positive Words vs their Compound Score\")\nplt.xlabel(\"Compound Score\")\nplt.ylabel(\"Positive Words\")\nplt.show()","c9e36466":"fig = plt.figure()\nax = fig.add_axes([0,0,1,1])\n# get x and y data \npoints = top_negative_words\nfrequency = top_negative_words_freq\n#plot graph\nax.barh(points,frequency)\nplt.title(\"Most Negative Words vs their Compound Score\")\nplt.xlabel(\"Compound Score\")\nplt.ylabel(\"Negative Words\")\nplt.show()","85bb38cb":"fig = plt.figure()\nax = fig.add_axes([0,0,1,1])\n# get x and y data \npoints = top_neutral_words\nfrequency = top_neutral_words_freq\n#plot graph\nax.barh(points,frequency)\nplt.title(\"Most Neutral Words vs their Compound Score\")\nplt.xlabel(\"Compound Score\")\nplt.ylabel(\"Neutral Words\")\nplt.show()","b693eb40":"who_dataset = pd.read_csv('..\/input\/who-tweet-data\/WHO__tweet_data.csv')\nwho_dataset.head()","b6b920d4":"#Retaining only alphabetnumerics\nwho_dataset[\"Tweet\"] = [re.sub('[^a-zA-Z0-9]', ' ',i) for i in who_dataset[\"Tweet\"]]\n\n#Converting into lower case \nwho_dataset[\"Tweet\"] = [i.lower() for i in who_dataset[\"Tweet\"]]\n\n#Removing Emoticons\ndef deEmojify(inputString):\n    return inputString.encode('ascii', 'ignore').decode('ascii')\nwho_dataset[\"Tweet\"]  = [deEmojify(i) for i in who_dataset[\"Tweet\"]]\n\n#Removing URLs\ndef removeURLs(str):\n    ans = \"\"\n    clean_tweet1 = re.match('(.*?)http.*?\\s?(.*?)', str)\n    clean_tweet2 = re.match('(.*?)https.*?\\s?(.*?)', str)\n    if clean_tweet1:\n        ans=ans+clean_tweet1.group(1)\n        ans=ans+clean_tweet1.group(2)\n    elif clean_tweet2: \n        ans=ans+clean_tweet2.group(1)\n        ans=ans+clean_tweet2.group(2)\n    else:\n        ans = str\n    return ans\nwho_dataset[\"Tweet\"] = who_dataset[\"Tweet\"].apply(lambda tweet: removeURLs(tweet))\n\n\nsentences = [i for i in who_dataset[\"Tweet\"]]","3be3bd47":"#Create a word frequency table from every sentence.\n\ndef create_frequency_table(text_string) -> dict:\n    \n    #Remove all the stop words\n    stopWords = set(stopwords.words(\"english\"))\n    #Tokenise the sentence into words\n    words = word_tokenize(text_string)\n    #Stem the words to get the root word\n    ps = PorterStemmer()\n    \n    freqTable = dict()\n    for word in words:\n        word = ps.stem(word)\n        if word in stopWords:\n            continue\n        if word in freqTable:\n            freqTable[word] += 1\n        else:\n            freqTable[word] = 1\n\n    return freqTable","b8beb697":"#Score a sentence by its words, adding the frequency of every non-stop word in a sentence.\n#First 10 chars of each sentence is used as key instead of using whole sentence; to prevent memory overload.\n\ndef score_sentences(sentences, freqTable) -> dict:\n    \n    sentenceValue = dict()\n\n    for sentence in sentences:\n        word_count_in_sentence = (len(word_tokenize(sentence)))\n        for word in freqTable:\n            if word in sentence:\n                if sentence[:10] in sentenceValue:\n                    sentenceValue[sentence[:10]] += freqTable[word]\n                else:\n                    sentenceValue[sentence[:10]] = freqTable[word]\n\n        #To prevent large sentences from dominating, we divide the score of a sentences by total number of words in it\n        if word_count_in_sentence > 0:\n            sentenceValue[sentence[:10]] = sentenceValue[sentence[:10]] \/\/ word_count_in_sentence\n\n    return sentenceValue","317e5a41":"#Consider the average score of the sentences as a threshold\n\ndef find_average_score(sentenceValue) -> int:\n    sumValues = 0\n    for entry in sentenceValue:\n        sumValues += sentenceValue[entry]\n\n    # Average value of a sentence from original text\n    average = int(sumValues \/ len(sentenceValue))\n\n    return average","0105283b":"#Select a sentence for summarization, only if the sentence score is more than the threshold score\n\ndef generate_summary(sentences, sentenceValue, threshold):\n    sentence_count = 0\n    summary = []\n\n    for sentence in sentences:\n        if sentence[:10] in sentenceValue and sentenceValue[sentence[:10]] > (threshold):\n            summary.append(sentence)\n            sentence_count += 1\n\n    return summary","4e2ff3c6":"# 1 Create the word frequency table\nfreq_table = create_frequency_table('. '.join(sentences))\n\n# 2 Important Algorithm: score the sentences\nsentence_scores = score_sentences(sentences, freq_table)\n\n# 3 Find the threshold\nthreshold = find_average_score(sentence_scores)\n\n# 4 Important Algorithm: Generate the summary\nsummary = generate_summary(sentences, sentence_scores, 1.5 * threshold)\n\n#5 ignore repetitive sentences in summary\ncountSummary = 0\nsummaryText = []\nfor i in summary:\n    if(summary.count(i)==1):\n        #Trimming extra white spaces\n        i = re.sub(' +', ' ', i)\n        \n        countSummary+=1\n        summaryText.append(i)\n        \n        print(str(countSummary)+\")  \"+i+\"\\n\")\n","d5f5d8fa":"print(\"Number of Tweets by WHO: \"+str(len(sentences)))\nprint(\"Number of Sentences in Summary: \"+str(countSummary))","abe9e96e":"### Extract necessary columns from dataset","b323381a":"### Preprocess the data","786a234d":"### Driver Function for WHO corona-tweet summarizarion","0545e70e":"### Sentiment Analysis","200f816e":"# Part 1: Sentiment Analysis on Corona-Tweets","1929d69b":"### Find the Threshold Score","5946107c":"### Importing the dataset","53d603c4":"### Sentiment Analysis of Tweets around the world","0e3b32de":"### Imporing basic libraries","7c03d1d0":"### Exploring Data","f35e00e7":"### Preprocessing the Data","c52650ec":"### Create the word frequency table","9df3470f":"# Part 2: WHO Corona-Tweets Summarization","4c893bb7":"### Importing the Dataset","afaf0ff2":"### Word Frequency Analysis","1ffbfc93":"### Importing necessary libraries for language processing","11fc6934":"### Generate Summary","76f5b441":"### Term Frequency Method","7de25322":"### Sentiment Analysis in top 5 tweeting countries"}}