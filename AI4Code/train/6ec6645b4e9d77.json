{"cell_type":{"d493d4ee":"code","0dd0e1af":"code","414c18d2":"code","8eeddf32":"code","774e5c9a":"code","6afe3a98":"code","f3ce4752":"code","f834da14":"code","cedae929":"code","c2061df4":"code","9f554894":"code","7d656285":"code","0364f83e":"code","5a1b1bfd":"code","2edb28fb":"code","9b3ddb79":"code","2794a985":"code","7ef850a5":"code","f0eba238":"code","9413f14d":"code","57415e5d":"code","2b3714ed":"code","2c5b81c7":"code","45c49950":"code","55d0bca2":"code","6d0e49a4":"code","42edd1f5":"code","667052de":"code","9b67e51a":"code","7666df0b":"code","f4e7c78a":"code","afad02fb":"code","e86e6a31":"code","256b0ee3":"code","c90d34ae":"code","f7ad9f1a":"code","a6756c93":"code","09081152":"code","f123176a":"code","05ca44a1":"code","5baf7e23":"code","1d97a402":"code","705ce7a1":"code","549c2c02":"code","2dbaaf1f":"code","7f10f71f":"code","32eed2de":"code","d1da5ecb":"code","8a2ff2f1":"code","cf30a8fa":"code","0c1d2209":"code","d860a159":"code","4414960b":"code","a224c8dd":"code","60aeedd1":"code","cf1e74c1":"code","490ea0ea":"code","7a8419e6":"code","7c51841e":"code","0fea6716":"code","2b6b65b4":"code","0d827bf0":"code","406dc312":"code","da525a62":"code","76fdedeb":"code","948d4048":"code","9281070a":"markdown","de40b6ca":"markdown","50dfd58d":"markdown","f7e93d3f":"markdown","4e0cce72":"markdown","3eda22dc":"markdown","46b4f7fc":"markdown","e48aa8ea":"markdown","5e6cf550":"markdown","597f09eb":"markdown","c6c0aa3b":"markdown","2ac6f6d6":"markdown","552ce987":"markdown","961bd3bb":"markdown","cdd78e1e":"markdown","f1e7c54c":"markdown","3944bdcc":"markdown","4b9275ca":"markdown","7639f261":"markdown","b40f8e6d":"markdown","7c9a23de":"markdown","e6297af1":"markdown","049a82ce":"markdown","c0bab229":"markdown","b40209c8":"markdown","237779ea":"markdown","117d1661":"markdown","484c7748":"markdown","6ddcc6f4":"markdown","420f1236":"markdown","e3392831":"markdown","d5d092f7":"markdown","93623fbf":"markdown","b8238e6b":"markdown","5ea339e9":"markdown"},"source":{"d493d4ee":"import numpy as np \nimport pandas as pd \n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\ntweets = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\n#tweets = pd.read_csv(r'C:\\Users\\TeYan\\OneDrive\\Work\\Kaggle\\Tweets_Disaster\\Data\\train.csv')\n#tweets = pd.read_csv('\/Users\/teyang\/OneDrive\/Work\/Kaggle\/Tweets_Disaster\/Data\/train.csv')","0dd0e1af":"tweets.head()\n","414c18d2":"tweets.isnull().sum().plot(kind='bar')","8eeddf32":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ncolor = [sns.xkcd_rgb['pale red'],sns.xkcd_rgb['medium blue']]\nsns.countplot('target',data = tweets, palette = color)\nplt.gca().set_ylabel('Samples')","774e5c9a":"#import nltk\n#nltk.download('punkt')\nfrom nltk import word_tokenize, sent_tokenize\n\n# count number of characters in each tweet\ntweets['char_len'] = tweets.text.str.len()\n\n# count number of words in each tweet\nword_tokens = [len(word_tokenize(tweet)) for tweet in tweets.text]\ntweets['word_len'] = word_tokens\n\n# count number of sentence in each tweet\nsent_tokens = [len(sent_tokenize(tweet)) for tweet in tweets.text]\ntweets['sent_len'] = sent_tokens\n\nplot_cols = ['char_len','word_len','sent_len']\nplot_titles = ['Character Length','Word Length','Sentence Length']\n\nplt.figure(figsize=(20,4))\nfor counter, i in enumerate([0,1,2]):\n    plt.subplot(1,3,counter+1)\n    sns.distplot(tweets[tweets.target == 1][plot_cols[i]], label='Disaster', color=color[1]).set_title(plot_titles[i])\n    sns.distplot(tweets[tweets.target == 0][plot_cols[i]], label='Non-Disaster', color=color[0])\n    plt.legend()\n\n\n","6afe3a98":"# Investigate the Outliers\n\ntweets[tweets.sent_len > 8]\ntweets[tweets.word_len > 50]","f3ce4752":"## Plot most common stopwords\n\n#nltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\n\n# Get all the word tokens in dataframe for Disaster and Non-Disaster\ncorpus0 = [] # Non-Disaster\n[corpus0.append(word.lower()) for tweet in tweets[tweets.target == 0].text for word in word_tokenize(tweet)]\ncorpus1 = [] # Disaster\n[corpus1.append(word.lower()) for tweet in tweets[tweets.target == 1].text for word in word_tokenize(tweet)]\n\n# Function for counting top stopwords in a corpus\ndef count_top_stopwords(corpus):\n    stopwords_freq = {}\n    for word in corpus:\n        if word in stop: \n            if word in stopwords_freq:\n                stopwords_freq[word] += 1\n            else:\n                stopwords_freq[word] = 1\n    topwords = sorted(stopwords_freq.items(), key=lambda item: item[1], reverse=True)[:10] # get the top 10 stopwords\n    x,y = zip(*topwords) # get key and values\n    return x,y\n\nx0,y0 = count_top_stopwords(corpus0)\nx1,y1 = count_top_stopwords(corpus1)\n\n# Plot bar plot of top stopwords for each class\nplt.figure(figsize=(15,4))\nplt.subplot(1,2,1)\nplt.bar(x0,y0, color=color[0])\nplt.title('Top Stopwords for Disaster Tweets')\nplt.subplot(1,2,2)\nplt.bar(x1,y1, color=color[1])\nplt.title('Top Stopwords for  Non-Disaster Tweets')\n\n\n\n\n","f834da14":"## Plot most common punctuations\n\nfrom string import punctuation\n\n# Get all the punctuations in dataframe for Disaster and Non-Disaster\ncorpus0 = [] # Non-Disaster\n[corpus0.append(c) for tweet in tweets[tweets.target == 0].text for c in tweet]\ncorpus0 = list(filter(lambda x: x in punctuation, corpus0)) # use filter to select only punctuations\ncorpus1 = [] # Disaster\n[corpus1.append(c) for tweet in tweets[tweets.target == 1].text for c in tweet]\ncorpus1 = list(filter(lambda x: x in punctuation, corpus1)) \n\nfrom collections import Counter\nx0,y0 = zip(*Counter(corpus0).most_common())\nx1,y1 = zip(*Counter(corpus1).most_common())\n\n# Plot bar plot of top punctuations for each class\nplt.figure(figsize=(15,4))\nplt.subplot(1,2,1)\nplt.bar(x0,y0, color=color[0])\nplt.title('Top Punctuations for Disaster Tweets')\nplt.subplot(1,2,2)\nplt.bar(x1,y1, color=color[1])\nplt.title('Top Punctuations for Non-Disaster Tweets')\n\n\n\n","cedae929":"## Plot most common words\nimport re\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\nstop = ENGLISH_STOP_WORDS.union(stop) # combine stop words from different sources\n\n# function for removing url from text\ndef remove_url(txt):\n    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\", \"\", txt).split())\n\n# Get all the word tokens in dataframe for Disaster and Non-Disaster\n# - remove url, tokenize tweet into words, lowercase words\ncorpus0 = [] # Non-Disaster\n[corpus0.append(word.lower()) for tweet in tweets[tweets.target == 0].text for word in word_tokenize(remove_url(tweet))]\ncorpus0 = list(filter(lambda x: x not in stop, corpus0)) # use filter to unselect stopwords\n\ncorpus1 = [] # Disaster\n[corpus1.append(word.lower()) for tweet in tweets[tweets.target == 1].text for word in word_tokenize(remove_url(tweet))]\ncorpus1 = list(filter(lambda x: x not in stop, corpus1)) # use filter to unselect stopwords\n\n# Create df for word counts to use sns plots\na = Counter(corpus0).most_common()\ndf0 = pd.DataFrame(a, columns=['Word','Count'])\n\na = Counter(corpus1).most_common()\ndf1 = pd.DataFrame(a, columns=['Word','Count'])\n\n# Plot for Disaster and Non-Disaster\nplt.figure(figsize=(15,4))\nplt.subplot(1,2,1)\nsns.barplot(x='Word',y='Count',data=df0.head(10), color=color[1]).set_title('Most Common Words for Non-Disasters')\nplt.xticks(rotation=45)\nplt.subplot(1,2,2)\nsns.barplot(x='Word',y='Count',data=df1.head(10), color=color[0]).set_title('Most Common Words for Disasters')\nplt.xticks(rotation=45)\n\n\n\n\n","c2061df4":"def clean(word):\n    for p in punctuation: word = word.replace(p, '')\n    return word\n\nfrom wordcloud import WordCloud\n\ndef wc_hash(target):\n    hashtag = [clean(w[1:].lower()) for tweet in tweets[tweets.target == target].text for w in tweet.split() if '#' in w and w[0] == '#']\n    hashtag = ' '.join(hashtag)\n    my_cloud = WordCloud(background_color='white', stopwords=stop).generate(hashtag)\n\n    plt.subplot(1,2,target+1)\n    plt.imshow(my_cloud, interpolation='bilinear') \n    plt.axis(\"off\")\n\nplt.figure(figsize=(15,4))\nwc_hash(0)\nplt.title('Non-Disaster')\nwc_hash(1)\nplt.title('Disaster')\n","9f554894":"from textblob import TextBlob\n\n# polarity and subjectivity\ntweets['polarity'] = [TextBlob(tweet).sentiment.polarity for tweet in tweets.text]\ntweets['subjectivity'] = [TextBlob(tweet).sentiment.subjectivity for tweet in tweets.text]\n\n#############################################################################################################################\n# exclaimation and question marks\ntweets['exclaimation_num'] = [tweet.count('!') for tweet in tweets.text]\ntweets['questionmark_num'] = [tweet.count('?') for tweet in tweets.text]\n\n#############################################################################################################################\n# count number of hashtags and mentions\n# Function for counting number of hashtags and mentions\ndef count_url_hashtag_mention(text):\n    urls_num = len(re.findall('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text))\n    word_tokens = text.split()\n    hash_num = len([word for word in word_tokens if word[0] == '#' and word.count('#') == 1]) # only appears once in front of word \n    mention_num = len([word for word in word_tokens if word[0] == '@' and word.count('@') == 1]) # only appears once in front of word \n    return urls_num, hash_num, mention_num\n\nurl_num, hash_num, mention_num = zip(*[count_url_hashtag_mention(tweet) for tweet in tweets.text])\ntweets = tweets.assign(url_num = url_num, hash_num = hash_num, mention_num = mention_num)\n\n#############################################################################################################################\n# count number of contractions\ncontractions = [\"'t\", \"'re\", \"'s\", \"'d\", \"'ll\", \"'ve\", \"'m\"]\ntweets['contraction_num'] = [sum([tweet.count(cont) for cont in contractions]) for tweet in tweets.text]\n","7d656285":"tweets.head()","0364f83e":"## Replace NaNs with 'None'\ntweets.keyword.fillna('None', inplace=True) \n\n#############################################################################################################################\n## Expand Contractions\n\n# Function for expanding most common contractions https:\/\/stackoverflow.com\/questions\/19790188\/expanding-english-language-contractions-in-python\ndef decontraction(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\ntweets.text = [decontraction(tweet) for tweet in tweets.text]\n\n#############################################################################################################################\n## Remove Emojis\n\n# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nprint(remove_emoji(\"OMG there is a volcano eruption!!! \ud83d\ude2d\ud83d\ude31\ud83d\ude37\"))\n\ntweets.text = tweets.text.apply(lambda x: remove_emoji(x))","5a1b1bfd":"#############################################################################################################################\n## Remove URLs\ntweets.text = tweets.text.apply(lambda x: remove_url(x))\n\n#############################################################################################################################\n## Remove Punctuations except '!?'\n\ndef remove_punct(text):\n    new_punct = re.sub('\\ |\\!|\\?', '', punctuation)\n    table=str.maketrans('','',new_punct)\n    return text.translate(table)\n\ntweets.text = tweets.text.apply(lambda x: remove_punct(x))\n\n#############################################################################################################################\n## Replace amp\ndef replace_amp(text):\n    text = re.sub(r\" amp \", \" and \", text)\n    return text\n\ntweets.text = tweets.text.apply(lambda x: replace_amp(x))\n\n#############################################################################################################################\n","2edb28fb":"## Lemmatization\n\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\ndef lemma(text):\n    words = word_tokenize(text)\n    return ' '.join([lemmatizer.lemmatize(w.lower(), pos='v') for w in words])\n\ntweets.text = tweets.text.apply(lambda x: lemma(x))\n","9b3ddb79":"## Ngrams\nfrom nltk.util import ngrams\n\ndef generate_ngrams(text, n):\n    words = word_tokenize(text)\n    return [' '.join(ngram) for ngram in list(ngrams(words, n)) if not all(w in stop for w in ngram)] # exclude if all are stopwords\n\n","2794a985":"# Bigrams\n\nbigrams_disaster = tweets[tweets.target==1].text.apply(lambda x: generate_ngrams(x, 2))\nbigrams_ndisaster = tweets[tweets.target==0].text.apply(lambda x: generate_ngrams(x, 2))\n\nbigrams_d_dict = {}\nfor bgs in bigrams_disaster:\n    for bg in bgs:\n        if bg in bigrams_d_dict:\n            bigrams_d_dict[bg] += 1\n        else:\n            bigrams_d_dict[bg] = 1\n\nbigrams_d_df = pd.DataFrame(bigrams_d_dict.items(), columns=['Bigrams','Count'])\n\nbigrams_nd_dict = {}\nfor bgs in bigrams_ndisaster:\n    for bg in bgs:\n        if bg in bigrams_nd_dict:\n            bigrams_nd_dict[bg] += 1\n        else:\n            bigrams_nd_dict[bg] = 1            \n\nbigrams_nd_df = pd.DataFrame(bigrams_nd_dict.items(), columns=['Bigrams','Count'])","7ef850a5":"# Barplots for bigrams\n\nplt.figure(figsize=(15,10))\nplt.subplot(1,2,1)\nsns.barplot(x='Count',y='Bigrams',data=bigrams_nd_df.sort_values('Count', ascending=False).head(40), color=color[0]).set_title('Most Common Bigrams for Non-Disasters')\nax = plt.gca()\nax.set_ylabel('')\nplt.subplot(1,2,2)\nsns.barplot(x='Count',y='Bigrams',data=bigrams_d_df.sort_values('Count', ascending=False).head(40), color=color[1]).set_title('Most Common Bigrams for Disasters')\nax = plt.gca()\nax.set_ylabel('')\nplt.tight_layout()\nplt.show()","f0eba238":"# Woudcloud for bigrams\n\nplt.figure(figsize=(15,10))\nplt.subplot(1,2,1)\nmy_cloud = WordCloud(background_color='white', stopwords=stop).generate_from_frequencies(bigrams_nd_dict)\nplt.imshow(my_cloud, interpolation='bilinear')\nplt.axis('off')\n\nplt.subplot(1,2,2)\nmy_cloud = WordCloud(background_color='white', stopwords=stop).generate_from_frequencies(bigrams_d_dict)\nplt.imshow(my_cloud, interpolation='bilinear')\nplt.axis('off')\n\nplt.show()","9413f14d":"# Trigrams\n\ntrigrams_disaster = tweets[tweets.target==1].text.apply(lambda x: generate_ngrams(x, 3))\ntrigrams_ndisaster = tweets[tweets.target==0].text.apply(lambda x: generate_ngrams(x, 3))\n\ntrigrams_d_dict = {}\nfor tgs in trigrams_disaster:\n    for tg in tgs:\n        if tg in trigrams_d_dict:\n            trigrams_d_dict[tg] += 1\n        else:\n            trigrams_d_dict[tg] = 1\n\ntrigrams_d_df = pd.DataFrame(trigrams_d_dict.items(), columns=['Trigrams','Count'])\n\ntrigrams_nd_dict = {}\nfor tgs in trigrams_ndisaster:\n    for tg in tgs:\n        if tg in trigrams_nd_dict:\n            trigrams_nd_dict[tg] += 1\n        else:\n            trigrams_nd_dict[tg] = 1            \n\ntrigrams_nd_df = pd.DataFrame(trigrams_nd_dict.items(), columns=['Trigrams','Count'])","57415e5d":"# Barplots for trigrams\n\nplt.figure(figsize=(15,10))\nplt.subplot(1,2,1)\nsns.barplot(x='Count',y='Trigrams',data=trigrams_nd_df.sort_values('Count', ascending=False).head(40), color=color[0]).set_title('Most Common Trigrams for Non-Disasters')\nax = plt.gca()\nax.set_ylabel('')\nplt.subplot(1,2,2)\nsns.barplot(x='Count',y='Trigrams',data=trigrams_d_df.sort_values('Count', ascending=False).head(40), color=color[1]).set_title('Most Common Trigrams for Disasters')\nax = plt.gca()\nax.set_ylabel('')\nplt.tight_layout()\nplt.show()","2b3714ed":"## Remove Stopwords\ndef remove_stopwords(text):\n    word_tokens = word_tokenize(text)\n    return ' '.join([w.lower() for w in word_tokens if not w.lower() in stop])\n\n#tweets_tmp = tweets.copy()\ntweets['text_nostopwords'] = tweets.text.apply(lambda x: remove_stopwords(x))\n","2c5b81c7":"## Plot word cloud for most common words after cleaning\n\nfrom PIL import Image\nmask = np.array(Image.open('..\/input\/twitter-logo\/Twitter-Logo_white.png'))\nreverse = mask[...,::-1,:]\n\ndef wc_words(target, mask=mask):\n    words = [word.lower() for tweet in tweets[tweets.target == target].text_nostopwords for word in tweet.split()]\n    words = list(filter(lambda w: w != 'like', words))\n    words = list(filter(lambda w: w != 'new', words))\n    words = list(filter(lambda w: w != 'people', words))\n    dict = {}\n    for w in words:\n        if w in dict:\n            dict[w] += 1\n        else:\n            dict[w] = 1\n    # plot using frequencies        \n    my_cloud = WordCloud(background_color='white', stopwords=stop, mask=mask, random_state=0).generate_from_frequencies(dict) \n    \n    plt.subplot(1,2,target+1)\n    plt.imshow(my_cloud, interpolation='bilinear') \n    plt.axis(\"off\")\n\nplt.figure(figsize=(15,10))\nwc_words(0)\nplt.title('Non-Disaster')\nwc_words(1, reverse)\nplt.title('Disaster')\nplt.show()","45c49950":"pd.options.display.max_colwidth = 200\ndisplay(tweets['text'].sample(n=20, random_state=0))\npd.reset_option('max_colwidth')","55d0bca2":"pd.reset_option('max_colwidth')\ntweets.drop('text_nostopwords', axis=1, inplace=True)\ntweets.head()","6d0e49a4":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(tweets.drop(['id','keyword','location','target'],axis=1), tweets[['target']], test_size=0.2, stratify=tweets[['target']], random_state=0)\nX_train_text = X_train['text']\nX_val_text = X_val['text']\n\nprint('X_train shape: ', X_train.shape)\nprint('X_val shape: ', X_val.shape)\nprint('y_train shape: ', y_train.shape)\nprint('y_val shape: ', y_val.shape)","42edd1f5":"print('Train Class Proportion:\\n', y_train['target'].value_counts() \/ len(y_train) * 100)\nprint('\\nValidation Class Proportion:\\n', y_val['target'].value_counts() \/ len(y_val) * 100)","667052de":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\ntokenizer_1 = Tokenizer(num_words=5000, oov_token=True)\ntokenizer_1.fit_on_texts(X_train_text)","9b67e51a":"X_train_text = tokenizer_1.texts_to_sequences(X_train_text)\nX_val_text = tokenizer_1.texts_to_sequences(X_val_text)\nprint(X_train_text[:10])\nprint('')\nprint(X_val_text[:10])","7666df0b":"tokenizer_1.sequences_to_texts([X_train_text[3]])","f4e7c78a":"print('Train Set Max Length:', max(len(text) for text in X_train_text))\nmaxlen = 50\n\nX_train_text = pad_sequences(X_train_text, padding='post', maxlen=maxlen)\nX_val_text = pad_sequences(X_val_text, padding='post', maxlen=maxlen)\n\nprint('X_train shape:', X_train_text.shape)\nprint('X_train shape:', X_val_text.shape)","afad02fb":"# Adding 1 because of reserved 0 index\nvocab_size = len(tokenizer_1.word_index) + 1\n\n# load the whole embedding into memory\nembeddings_index = dict()\nf = open('..\/input\/glove-global-vectors-for-word-representation\/glove.twitter.27B.200d.txt')\n\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\n\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))","e86e6a31":"# create a weight matrix for words in training set\nembedding_matrix = np.zeros((vocab_size, 200))\n\nfor word, i in tokenizer_1.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n        \nprint('Embedding Matrix Shape:', embedding_matrix.shape)","256b0ee3":"## Hyperparameters\nnum_epochs=15\ndropout=0.2\nrecurrent_dropout=0.2\nlr=0.0005\nbatch_size=128","c90d34ae":"from keras.models import Sequential\nfrom keras.layers.core import Activation, Dropout, Dense\nfrom keras.layers import Flatten, GlobalMaxPooling1D, LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras import optimizers\nfrom keras.callbacks import ModelCheckpoint\n\nlstm_model = Sequential()\nembedding_layer = Embedding(vocab_size, 200, weights=[embedding_matrix], input_length=maxlen, trainable=False)\nlstm_model.add(embedding_layer)\nlstm_model.add(LSTM(128, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout)) # try adding dropout later\nlstm_model.add(LSTM(128))\n\n#model.add(Flatten())\nlstm_model.add(Dense(1, activation='sigmoid'))\n\nadam = optimizers.Adam(lr=lr)\nlstm_model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['acc'])\nprint(lstm_model.summary())\n\n# best hyperparameters\n# num_epochs=15\n# dropout=0.2\n# recurrent_dropout=0.2\n# lr=0.0005\n# batch_size=128","f7ad9f1a":"def plot_model_performance(history):   \n    plt.figure(figsize=(15,5))\n    plt.plot(range(num_epochs), history.history['acc'],'-o',\n             label='Train ACC',color='#ff7f0e')\n    plt.plot(range(num_epochs),history.history['val_acc'],'-o',\n             label='Val ACC',color='#1f77b4')\n    x = np.argmax( history.history['val_acc'] ); y = np.max( history.history['val_acc'] )\n    xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n    plt.scatter(x,y,s=200,color='#1f77b4')\n    plt.text(x-0.03*xdist,y-0.13*ydist,'max acc\\n%.2f'%y,size=14)\n    plt.ylabel('Accuracy',size=14); plt.xlabel('Epoch',size=14)\n    plt.legend(loc=(0.01,0.75))\n\n    plt2 = plt.gca().twinx()\n    plt2.plot(range(num_epochs),history.history['loss'],'-o',\n              label='Train Loss',color='#2ca02c')\n    plt2.plot(range(num_epochs),history.history['val_loss'],'-o',\n              label='Val Loss',color='#d62728')\n    x = np.argmin( history.history['val_loss'] ); y = np.min( history.history['val_loss'] )\n    ydist = plt.ylim()[1] - plt.ylim()[0]\n    plt.scatter(x,y,s=200,color='#d62728')\n    plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n    plt.ylabel('Loss',size=14)\n    plt.xticks(ticks=list(range(num_epochs)),labels=list(range(1, num_epochs+1)))\n    plt.legend(loc='lower left', bbox_to_anchor=(0.01, 0.1))\n    plt.show()","a6756c93":"checkpoint = ModelCheckpoint('lstm_model.h5', monitor='val_acc', save_best_only=True)\nhistory = lstm_model.fit(X_train_text, y_train, batch_size=batch_size, callbacks=[checkpoint], epochs=num_epochs, \n                         validation_data=(X_val_text, y_val), verbose=1)\nplot_model_performance(history)","09081152":"# from keras.models import Sequential\n# from keras.layers.core import Activation, Dropout, Dense\n# from keras.layers import Flatten, GlobalMaxPooling1D, LSTM, Bidirectional\n# from keras.layers.embeddings import Embedding\n# from keras import optimizers\n\n# model = Sequential()\n# embedding_layer = Embedding(vocab_size, 200, weights=[embedding_matrix], input_length=maxlen, trainable=False)\n# model.add(embedding_layer)\n# model.add(Bidirectional(LSTM(128, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout))) # try adding dropout later\n# model.add(Bidirectional(LSTM(128)))\n\n# #model.add(Flatten())\n# model.add(Dense(1, activation='sigmoid'))\n\n# adam = optimizers.Adam(lr=lr)\n# model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['acc'])\n# print(model.summary())","f123176a":"from keras.layers import Layer\nimport keras.backend as K\n\nclass attention(Layer):\n    def __init__(self,**kwargs):\n        super(attention,self).__init__(**kwargs)\n\n    def build(self,input_shape):\n        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n        super(attention, self).build(input_shape)\n\n    def call(self,x):\n        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n        at=K.softmax(et)\n        at=K.expand_dims(at,axis=-1)\n        output=x*at\n        return K.sum(output,axis=1)\n\n    def compute_output_shape(self,input_shape):\n        return (input_shape[0],input_shape[-1])\n\n    def get_config(self):\n        return super(attention,self).get_config()","05ca44a1":"### Attention\n\n## Hyperparameters\nnum_epochs=15\ndropout=0.3\nrecurrent_dropout=0.3\nlr=0.0005\nbatch_size=128\n\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras import Model\nfrom keras.layers.core import Activation, Dropout, Dense\nfrom keras.layers import Flatten, Input, Layer, GlobalMaxPooling1D, LSTM, Bidirectional, Concatenate\nfrom keras.layers.embeddings import Embedding\nfrom keras import optimizers\n\n## Embedding Layer\nsequence_input = Input(shape=(maxlen,))\nembedded_sequences = Embedding(vocab_size, 200, weights=[embedding_matrix], trainable=False)(sequence_input)\n\n## RNN Layer\nlstm = Bidirectional(LSTM(128, return_sequences = True, dropout=dropout, recurrent_dropout=recurrent_dropout))(embedded_sequences)\n# Getting our LSTM outputs\n(lstm, forward_h, forward_c, backward_h, backward_c) = Bidirectional(LSTM(128, return_sequences=True, return_state=True))(lstm)\n\n## Attention Layer\natt_out=attention()(lstm)\noutputs=Dense(1,activation='sigmoid')(att_out)\nmodel_attn = Model(sequence_input, outputs)\n\nadam = optimizers.Adam(lr=lr)\n#sgd = optimizers.sgd(lr=lr)\nmodel_attn.compile(optimizer=adam, loss='binary_crossentropy', metrics=['acc'])\n\nprint(model_attn.summary())","5baf7e23":"checkpoint = ModelCheckpoint('attn_model.h5', monitor='val_acc', save_best_only=True)\nhistory_attn = model_attn.fit(X_train_text, y_train, batch_size=batch_size, callbacks=[checkpoint], epochs=num_epochs, \n                              validation_data=(X_val_text, y_val), verbose=1)\nplot_model_performance(history_attn)","1d97a402":"# Hyperparameters\nmaxlen = 100\nlr = 5e-5 \nnum_epochs = 5\nbatch_size=16 # batch size cannot be too big for bert","705ce7a1":"# We will use the official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tokenization\n\n\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","549c2c02":"def build_model(bert_layer, max_len=512, lr=1e-5):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=lr), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","2dbaaf1f":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)\n\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","7f10f71f":"train_input = bert_encode(X_train.text.values, tokenizer, max_len=maxlen)\nval_input = bert_encode(X_val.text.values, tokenizer, max_len=maxlen)\ntrain_labels = y_train.target.values\nval_labels = y_val.target.values","32eed2de":"bert_model = build_model(bert_layer, max_len=maxlen, lr=lr)\nbert_model.summary()","d1da5ecb":"checkpoint = ModelCheckpoint('bertmodel.h5', monitor='val_accuracy', save_best_only=True)\n\nbert_history = bert_model.fit(\n    train_input, train_labels,\n    epochs=num_epochs,\n    callbacks=[checkpoint], \n    batch_size=batch_size,\n    validation_data=(val_input, val_labels)\n)","8a2ff2f1":"X_train","cf30a8fa":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nclf = RandomForestClassifier(n_estimators=500, max_depth=15, min_samples_split=20, min_samples_leaf=2, n_jobs=-1, random_state=0)\nclf.fit(X_train.drop('text',axis=1), y_train.target.values)\nclf_pred = clf.predict_proba(X_val.drop('text',axis=1))\n\nprint('Validation Accuracy:', accuracy_score(y_val.target.values, clf_pred.argmax(axis=-1)))","0c1d2209":"clf_pred.max(axis=-1)","d860a159":"clf_pred.max(axis=-1)*0.1","4414960b":"# val = X_val.copy()\n# val = val[['text']]\n# val['target'] = y_val\n# val['pred'] = model.predict(X_val_text)\n# val['pred'] = (val['pred']*0.8) + (clf_pred.max(axis=-1)*0.2)\n# val['pred'] = val['pred'].apply(lambda x: 1 if x >=0.5 else 0)\n# error = val[val['target'] != val['pred']]\n# error.head()\n\nval = X_val.copy()\nval = val[['text']]\nval['target'] = y_val\nval['pred'] = lstm_model.predict_classes(X_val_text)\nerror = val[val['target'] != val['pred']]\nerror.head()","a224c8dd":"from mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\n\n# Plot confusion matrix\ncm  = confusion_matrix(val.target, val.pred)\nplt.figure()\nplot_confusion_matrix(cm,figsize=(12,8),cmap=plt.cm.Blues)\nplt.xticks(range(2), ['Non-Disaster', 'Disaster'], fontsize=16)\nplt.yticks(range(2), ['Non-Disaster', 'Disaster'], fontsize=16)\nplt.xlabel('Predicted Label',fontsize=18)\nplt.ylabel('True Label',fontsize=18)\nplt.show()\n\nprint('Num False Negatives:',sum((val['target'] == 1) & (val['pred'] == 0)))\nprint('Num False Positives:',sum((val['target'] == 0) & (val['pred'] == 1)))","60aeedd1":"for t in error[(error['target'] == 1) & (error['pred'] == 0)]['text']:\n    print(t)","cf1e74c1":"# count number of characters in each tweet\ntest['char_len'] = test.text.str.len()\n\n# count number of words in each tweet\nword_tokens = [len(word_tokenize(tweet)) for tweet in test.text]\ntest['word_len'] = word_tokens\n\n# count number of sentence in each tweet\nsent_tokens = [len(sent_tokenize(tweet)) for tweet in test.text]\ntest['sent_len'] = sent_tokens","490ea0ea":"# polarity and subjectivity\ntest['polarity'] = [TextBlob(tweet).sentiment.polarity for tweet in test.text]\ntest['subjectivity'] = [TextBlob(tweet).sentiment.subjectivity for tweet in test.text]\n\n#############################################################################################################################\n# exclaimation and question marks\ntest['exclaimation_num'] = [tweet.count('!') for tweet in test.text]\ntest['questionmark_num'] = [tweet.count('?') for tweet in test.text]\n\n#############################################################################################################################\n# count number of hashtags and mentions\n# Function for counting number of hashtags and mentions\ndef count_url_hashtag_mention(text):\n    urls_num = len(re.findall('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text))\n    word_tokens = text.split()\n    hash_num = len([word for word in word_tokens if word[0] == '#' and word.count('#') == 1]) # only appears once in front of word \n    mention_num = len([word for word in word_tokens if word[0] == '@' and word.count('@') == 1]) # only appears once in front of word \n    return urls_num, hash_num, mention_num\n\nurl_num, hash_num, mention_num = zip(*[count_url_hashtag_mention(tweet) for tweet in test.text])\ntest = test.assign(url_num = url_num, hash_num = hash_num, mention_num = mention_num)\n\n#############################################################################################################################\n# count number of contractions\ncontractions = [\"'t\", \"'re\", \"'s\", \"'d\", \"'ll\", \"'ve\", \"'m\"]\ntest['contraction_num'] = [sum([tweet.count(cont) for cont in contractions]) for tweet in test.text]","7a8419e6":"## Replace NaNs with 'None'\ntest.keyword.fillna('None', inplace=True) \n\n#############################################################################################################################\n## Expand Contractions\n\n# Function for expanding most common contractions https:\/\/stackoverflow.com\/questions\/19790188\/expanding-english-language-contractions-in-python\ndef decontraction(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\ntest.text = [decontraction(tweet) for tweet in test.text]\n\n#############################################################################################################################\n## Remove Emojis\n\n# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nprint(remove_emoji(\"OMG there is a volcano eruption!!! \ud83d\ude2d\ud83d\ude31\ud83d\ude37\"))\n\ntest.text = test.text.apply(lambda x: remove_emoji(x))","7c51841e":"#############################################################################################################################\n## Remove URLs\ntest.text = test.text.apply(lambda x: remove_url(x))\n\n#############################################################################################################################\n## Remove Punctuations except '!?'\n\ndef remove_punct(text):\n    new_punct = re.sub('\\ |\\!|\\?', '', punctuation)\n    table=str.maketrans('','',new_punct)\n    return text.translate(table)\n\ntest.text = test.text.apply(lambda x: remove_punct(x))\n\n#############################################################################################################################\n## Replace amp\ndef replace_amp(text):\n    text = re.sub(r\" amp \", \" and \", text)\n    return text\n\ntest.text = test.text.apply(lambda x: replace_amp(x))\n\n#############################################################################################################################","0fea6716":"## Lemmatization\n\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\ndef lemma(text):\n    words = word_tokenize(text)\n    return ' '.join([lemmatizer.lemmatize(w.lower(), pos='v') for w in words])\n\ntest.text = test.text.apply(lambda x: lemma(x))","2b6b65b4":"# tokenize\ntest_text = test['text']\ntest_text = tokenizer_1.texts_to_sequences(test_text)\n\n# padding\ntest_text = pad_sequences(test_text, padding='post', maxlen=50)\n\nprint('X_test shape:', test_text.shape)","0d827bf0":"# lstm prediction\n# model.predict(test_text)\nlstm_model.load_weights('lstm_model.h5')\nsubmission = test.copy()[['id']]\nsubmission['target'] = lstm_model.predict_classes(test_text)\nsubmission.to_csv('submission.csv', index=False)\ndisplay(submission.head())\n\n","406dc312":"# bi-lstm attention prediction\nmodel_attn.load_weights('attn_model.h5')\nsubmission_attn = test.copy()[['id']]\nsubmission_attn['target'] = model_attn.predict(test_text)\nsubmission_attn['target'] = submission_attn['target'].apply(lambda x: 1 if x >=0.5 else 0)\nsubmission_attn.to_csv('submission_attn.csv', index=False)\ndisplay(submission_attn.head())","da525a62":"# bert prediction\n\ntest_input = bert_encode(test.text.values, tokenizer, max_len=100)\n\nbert_model.load_weights('bertmodel.h5')\nsubmission_bert = test.copy()[['id']]\nsubmission_bert['target'] = bert_model.predict(test_input)\nsubmission_bert['target'] = submission_bert['target'].apply(lambda x: 1 if x >=0.5 else 0)\nsubmission_bert.to_csv('submission_bert.csv', index=False)\ndisplay(submission_bert.head())","76fdedeb":"# bert + meta-features prediction\n\nclf_testpred = clf.predict_proba(test.drop(['id','keyword','location','text'],axis=1))\nsubmission_bert = test.copy()[['id']]\nsubmission_bert['target'] = (bert_model.predict(test_input)*0.8).ravel() + (clf_testpred.max(axis=1)*0.2)\nsubmission_bert['target'] = submission_bert['target'].apply(lambda x: 1 if x >=0.5 else 0)\nsubmission_bert.to_csv('submission_bert_ensemble.csv', index=False)\ndisplay(submission_bert.head())","948d4048":"submission_bert['target'].plot(kind='hist')","9281070a":"<a id='Hashtags'><\/a>\n## 2.5. Wordcloud for Hashtags","de40b6ca":"<a id='Data_loading_structure'><\/a>\n# 1. Data Loading and Structure","50dfd58d":"<a id='TrainValSplit'><\/a>\n# 5. Train Validation Data Split\n","f7e93d3f":"<a id='WC_Cleaned'><\/a>\n## 4.1. WordCloud of Most Common Words after Cleaning\n\nRemoved some words such as 'new', 'like' and 'people' as they are common between both targets","4e0cce72":"<a id='Top_Stopwords'><\/a>\n## 2.2. Top Most Common Stopwords","3eda22dc":"<a id='Data_Clean'><\/a>\n# 4. Text Data Cleaning\n\nThis is the most important step of the entire project \u2014 text preprocessing\/cleaning. This cleans the text into a more 'suitable' form as inputs into the NLP models. For example, URLs might make the text difficult to understand and should be removed when necessary. The choice of whether to remove\/clean some words or parts-of-speech is an entire process on its own and sometimes this needs to be experimented. Different models are also able to deal with different kinds of parts-of-speech.\n\n* Replace NaNs with 'None'\n* Expand Contractions\n* Remove Emojis\n* Remove URLs\n* Remove Punctuations except '!?' as they convey intensity and tonality of tweet\n* Replace 'amp' with 'and'\n* Lemmatization - reduces inflected words into their root form; verb part-of-speech tag is used here)\n* Ngrams Exploration \n* Remove Stopwords\n* WordCloud of most commmon words (Unigrams)","46b4f7fc":"https:\/\/stackabuse.com\/python-for-nlp-movie-sentiment-analysis-using-deep-learning-in-keras\/\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/03\/pretrained-word-embeddings-nlp\/\n\n### attention\nhttps:\/\/matthewmcateer.me\/blog\/getting-started-with-attention-for-classification\/\n","e48aa8ea":"Next we will create an embedding matrix for our train vocab\/corpus where each row number will correspond to the index of the word in our train vocab\/corpus. The matrix will have 200 columns, each containing the GloVe word embeddings for the words in our train vocab\/corpus.","5e6cf550":"Disaster tweets contain more words related to disasters. But still need more cleaning. And what is the word amp? Will need to expand contractions as well such as 'im'.","597f09eb":"### Ngrams","c6c0aa3b":"<a id='Conclusion'><\/a>\n# 10. Conclusion","2ac6f6d6":"Each list in the `X_train_text` and `X_val_text` is a list of integers, which corresponds to each tweets in the train and validation set respectively. The length of each list is also different as different tweets have different lengths. Therefore, we will need to apply **padding** to make all sequences the same length.\n\nWe can use `tokenizer.word_index` to look at the vocabulary dictionary and `sequences_to_texts` to transform sequences back into texts.\n\n\n**Note:** The Tokenizer stores everything in the `word_index` during `fit_on_texts`. Then, when calling the `texts_to_sequences` method, only the top `num_words` are considered. So `word_index` will actually contain more words than `num_words`.","552ce987":"<a id='Tokenization'><\/a>\n## 6.1. Tokenization","961bd3bb":"<a id='Test'><\/a>\n# 9. Testing","cdd78e1e":"<a id='Top_Words'><\/a>\n## 2.4. Top Most Common Words","f1e7c54c":"<a id='Embedding'><\/a>\n# 6. Embedding Layer\n\n### Word Representation\n\nWord representation refers to representing words as numbers so that a computer can understand it. One way to represent words is to use a one-hot representation (bottom left), where each word in a corpus\/dictionary is a vector of all 0s except the index which it is assigned to. For example, in a 10,000 word dictionary, `a` is usually the first word and so is given a vector of [1,0,0,0,...,0], `aaron` is a vector of [0,1,0,0,...,0] and `zulu`, which might be the last word, is a vector of [0,0,0,0,...,1], all with a shape of (10000, 1). However, this way of representing words have a major weakness \u2014 ***it treats each word as onto itself, so it does not generalize across words.*** For example, the relationship between `apple` and `orange` is not any closer than the relationship between `apple` and `king`. The inner product or Euclidean distance between any 2 words will be 0. Therefore, all word pairs will have a dissimilarity (Euclidean Distance) of 0.\n<br><br>\n\n<img src = 'https:\/\/bn1301files.storage.live.com\/y4mS1q2-u6bjSL9LZ317bVz57HUlCnt3l3du9-iVCE8GiUrMMM4YAuxWQ12iHTImvYXvnLJgCKWZFE7kiurFmRX7jMUINieWGPGLeP9rtszv3GlaEwvhWiDXo3wfS7tC-semwXswn3QOlKZi1Ddsz9VRS9YABa_6lugTftLC_ZLOfv77igv55y_E_3Lq5AgqFus?width=3676&height=1378&cropmode=none' width=800> \n<br>\n\n### Word Embeddings\n\nA better way to represent words is using word embeddings, which can be learned from large corpuses of texts, such as Wikipedia. It is a dense way, compared to the sparse way for word representation, of representing words as well as the relationships between them. A word embedding is a learned representation for text where words that have the **same meaning have a similar representation**. For example, as shown above in the right table, `apple` and `orange` have similar vector values (their euclidean distance is very small) compared to `apple` and `king`. Another way to compare two words is using **cosine similarity**. \n\nEach row of the matrix represents a **feature\/dimension**, such as `gender` or `food` that are attributes of the words. Words that are highly attributed to the feature are given high positive and negative values, while words with no such attributes are given values close to 0s. If we take the vector difference between `man` and `woman`, or `king` and `queen`, both will give a vector close to [-2,0,0,...,0], indicating that each of the pair of words differ highly according to the `gender` attribute. In practice, the features\/dimensions that are learned for word embeddings are more abstract, and sometimes it might not be intuitive as to what attributes they represent, and they might be a combination of different attributes.\n\nWord embeddings can be trained from scratch. Some of the most popular ways include [Word2Vec](http:\/\/jalammar.github.io\/illustrated-word2vec\/), [NegativeSampling](http:\/\/jalammar.github.io\/illustrated-word2vec\/), and [GloVe (Global vectors for word representation)](https:\/\/towardsdatascience.com\/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010). A pre-trained word embedding can also be downloaded and used.\n\nThe graph below shows a simple RNN model for a *many-to-one* classification problem, such as tweet disaster classification or sentiment analysis, with the input words fed into the embedding layer. For each word, its vector representation (`e`) is obtained from the embedding matrix (`E`), and is then fed into the hidden layers.\n<br><br>\n\n<img align=left src = 'https:\/\/bn1301files.storage.live.com\/y4mkNjcljwqSA1Sb6vCIb8YMk8i5mcl-ViArevkMz6kqZVvbi8fW0lJFPwAprRt5DBN3YamG_ooLd_dRT85rIEinIHrPUTcdxLeBHuxLAYmfpxdDT6Hajvhrqmevt1C_XtXMWQnEe1z2-fouUj760K41kfVH2vbzBOr8JNZYWCNte-xVWHuBSFxGCrzTM7bumTs?width=3368&height=1448&cropmode=none' width=800> \n\n<br>\n","3944bdcc":"<a id='Model_Build'><\/a>\n# 7. Model Building & Training\n\ndropout vs recurrent dropout https:\/\/stackoverflow.com\/questions\/44924690\/keras-the-difference-between-lstm-dropout-and-lstm-recurrent-dropout","4b9275ca":"<a id='Feature_Engineer'><\/a>\n# 3. Meta-Feature Engineering\n\nHere, we extract some features from the tweets that might give us some idea about whether it is a disaster or not. The purpose of this is to build a feature-based model and use it as part of an ensemble model to improve the predictions of the sequence model. Although it might not perform well on its own, it can  actually boost the performance when combined with other models.\n\n* polarity - range of \\[-1,1] where 1 denotes positivity and -1 denotes negativity\n* subjectivity - range of \\[0,1] where 1 denotes personal opinions and 0 denotes factual info\n* exclaimation_num - number of exclamation marks in tweet\n* questionmark_num - number of question marks in tweet\n* url_num - number of urls in tweet\n* hash_num - number of hashtags (#) in tweet\n* mention_num - number of mentions (@) in tweet\n* contraction_num - number of contractions (e.g I'm, we're, we've)","7639f261":"To feed the tweets into the model, first we need to split them up. Here we **tokenize** the sentences -- break them up into words and assign them an integer based on the vocabulary dictionary. The maximum vocabulary size is set to 5000, so only the most common `num_words`-1 words will be kept. `oov_token` is set to `True` so that out-of-vocabulary words will be given an index instead of being ignored during `text_to_sequence` call.\n\nUse `fit_on_texts` to create a word-to-index vocabulary dictionary based on the train texts. This creates the vocabulary index based on word frequency, with words that appear more often at the top of the vocabulary.\n\n`texts_to_sequences` transforms each text in texts to a sequence of integers.","b40f8e6d":"There are lots of occurences of stopwords. These should be removed as they do not predict the target.","7c9a23de":"##  TO DO\n* Word and Char vectorizer\n* Remove numbers? Convert numbers to words?\n* Unigrams, Bigrams and Trigrams\n* Glove; remove stopwords, clean before glove?\n* Logistic Regression, BOW, TD IDF, GloVe, BERT?\n* Check Duplicates\n* Twitter mask for word cloud\n* Decaying LR","e6297af1":"<a id='LSTM'><\/a>\n## 7.1. Long Short-Term Memory (LSTM)","049a82ce":"<a id='EDA'><\/a>\n# 2. Exploratory Data Analysis of Tweets\n\n<a id='Frequency_Distribution'><\/a>\n## 2.1. Distribution of Character, Word and Sentence Frequency","c0bab229":"Most common punctuation is the slash, which usually comes from a link ('http:\/\/t.co\/'). URLs should be removed, as well as most punctuations, with the exception of '!?', which signal some kind of intensity or tonality of the tweet.\n","b40209c8":"<a id='Top_Punc'><\/a>\n## 2.3. Top Most Common Punctuations","237779ea":"Location has lots of NaN values and would not be a good\/useful feature, unless we have a priori knowledge of where a disaster occured. Furthermore, some of them are not in the correct format, so it will be quite time consuming to clean it. \n\nKeyword has NaNs as well, but can be imputed with 'None'.","117d1661":"# Tweets Disaster Classification <br>\nAuthor: TeYang, Lau<br>\nCreated: 18\/2\/2020<br>\nLast update: 14\/12\/2020<br>\n\n<img src = 'https:\/\/bn1301files.storage.live.com\/y4m-toxx6sX6SL9zvwtvAbEi9xPKLkgI6kdJ0PJ0uWjzQIR5GouWmvWfMBEuppVlUoFh3eZkKSrveb0QWnLNHPfHVwlBx55CtJMcmqurAYyBv-a2d1rSAmBUxU9CYHY7zZ50XIldgPJMkU7o18TcvrbPJatlu7ioKXMNV0qyev-Z1ise-zNPFjcYmbqz52FSyeW?width=5048&height=1838&cropmode=none' width=\"900\">\n\n<br><br>\n\n\nThe process is as follows:\n1. [Data Loading and Structure](#Data_loading_structure)\n2. [Exploratory Data Analysis of Tweets](#EDA) <br>\n2.1. [Distribution of Character, Word and Sentence Frequency](#Frequency_Distribution) <br>\n2.2. [Top Most Common Stopwords](#Top_Stopwords) <br>\n2.3. [Top Most Common Punctuations](#Top_Punc) <br>\n2.4. [Top Most Common Words](#Top_Words) <br>\n2.5. [Wordcloud for Hashtags](#Hashtags) <br>\n3. [Meta-Feature Engineering](#Feature_Engineer)\n4. [Text Data Cleaning](#Data_Clean) <br>\n4.1. [Ngrams](#Ngrams) <br>\n4.2. [WordCloud of Most Common Words after Cleaning](#WC_Cleaned)\n5. [Train Validation Data Split](#TrainValSplit)    \n6. [Embedding Layer](#Embedding) <br>\n6.1. [Tokenization](#Tokenization) <br>\n6.2. [Padding](#Padding) <br>\n6.3. [Embedding Matrix \u2013 GloVe](#E_Matrix) <br>\n7. [Model Building & Training](#Model_Build) <br>\n7.1. [Long Short-Term Memory (LSTM)](#LSTM) <br>\n7.2. [Bidirectional LSTM with Attention](#Attention) <br>\n7.3. [BERT](#BERT)\n8. [Error Analysis](#Error)\n9. [Testing](#Test)\n10. [Conclusion](#Conclusion)<br><br>","484c7748":"<a id='Meta-data'><\/a>\n## 7.4. Feature-based Model\n","6ddcc6f4":"<a id='E_Matrix'><\/a>\n## 6.3. Embedding Matrix \u2013 GloVe\n\nWe will use the [GloVe embeddings](https:\/\/nlp.stanford.edu\/projects\/glove\/) that were pre-trained on 2 billion tweets to create our feature matrix. First, we will create a dictionary that will contain words as keys and their corresponding embedding list at values. The length of the embedding for each word will be 200, as the GloVe embedding we are using was trained to have 200 dimensions. Refer to [here](https:\/\/github.com\/stanfordnlp\/GloVe) also for more details.\n\n","420f1236":"Some of the outliers such as sentence length > 10 consist of a lot of punctuations. I left it unchanged as I feel that a  tweet with a many sentences, which is indicative of many punctuations, suggest that it is not a serious tweet (about a disaster). Of course there might be some instances where a disaster tweet consists of multiple punctuations (e.g. a volvano just erupted!!!!!!!!!!!!) but that is not very frequent.\n","e3392831":"<a id='Attention'><\/a>\n## 7.2. Bidirectional LSTM with Attention\n","d5d092f7":"<a id='Error'><\/a>\n# 8. Error Analysis","93623fbf":"<a id='BERT'><\/a>\n## 7.3. BERT\n\nBidirectional Encoder Representations from Transformers","b8238e6b":"There appears to be more false negatives than false positives from the validation data, meaning that more tweets are being labelled as `not disaster` when in fact they are.","5ea339e9":"<a id='Padding'><\/a>\n## 6.2. Padding\n\nAfter tokenization, each tweet is represented as a list of tokens. Next, we need to **pad** all lists to the same size, so we can represent the input as one 2-d array, rather than a list of lists (of different lengths). Do this by adding 0s to the end of each sentence in the tokenized form so that each sentence is *now the same length as the longest tweet*. \n\nThe max length for the train set tweets is 32. We will set the `maxlen` to be 50 as tweets from the validation or test set might be longer. This means that texts longer than 50 words will be truncated to the 1st 50 words while texts shorter than 50 will have 0s appended to make them of length 50.\n\nBelow shows a quick example of padding sentences to a length of 5 sequences.\n\n<br>\n\n<img src = 'https:\/\/bn1301files.storage.live.com\/y4ma9N0t0Cjf_JcFdIj5J6W47lKDiMsXBwUwg5KXo6hUlH9PrpNv5b067TNxP7NFrtk1nbM8fxn5HXFs4rOLJ1QZK1omFFHB5Bl-jsoX5T4bZKJ3I76JwZazSPvquBb0aVem8MGLIP2CT8AsnRW1EOeMExc4w1AkzmfJ_p1oNRv506yRZEUEVlbtY780CnoAadD?width=4342&height=494&cropmode=none' width=700 align=left>"}}