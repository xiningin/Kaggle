{"cell_type":{"9fc3ac2f":"code","d2781e45":"code","ba9a77a4":"code","4a6a478e":"code","3acca70f":"code","1e33d03a":"code","58543e22":"code","f961fe5c":"code","88570d84":"code","d067a258":"code","ba70955b":"code","f6ffacc0":"code","efb288b1":"code","08aa667e":"code","5d63a4e2":"code","3b785339":"code","80597a21":"code","9d96fa5d":"code","e985d45c":"code","04cf54c4":"code","ef51f03c":"code","b9e7a79b":"code","10880646":"code","e00aa6a2":"code","24721906":"code","addb8c75":"code","54a9d58e":"code","ef2545f9":"code","3573cd38":"code","4aa4010c":"code","989fae6f":"code","70c64230":"code","a75dad7a":"code","76e92c94":"code","cbd50c25":"code","47ee4424":"code","5b2a85f0":"markdown","ffbd6764":"markdown","58eb8f53":"markdown","ba1f6ba8":"markdown","3bda1f31":"markdown","a4861ad2":"markdown","b1a6c24e":"markdown"},"source":{"9fc3ac2f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d2781e45":"import os\nimport gc\nimport copy\nimport time\nimport random\nimport string\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\n# Utils\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n# Sklearn Imports\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\n# For Transformer Models\nfrom transformers import AutoTokenizer, AutoModel, AdamW\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nb_ = Fore.BLUE\ny_ = Fore.YELLOW\nsr_ = Style.RESET_ALL\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","ba9a77a4":"import wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"w_b\")\n    wandb.login(key=api_key)\n    anony = None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https:\/\/wandb.ai\/authorize')","4a6a478e":"def id_generator(size=12, chars=string.ascii_lowercase + string.digits):\n    return ''.join(random.SystemRandom().choice(chars) for _ in range(size))\n\nHASH_NAME = id_generator(size=12)\nprint(HASH_NAME)","3acca70f":"CONFIG = {\"seed\":42,\n         \"num_epochs\":3,\n         \"train_batch_size\": 16,\n          \"val_batch_size\":32,\n          \"model_name\":\"roberta-base\",\n         \"learning_rate\": 1e-4,\n          \"scheduler\": None,\n          \"min_lr\": 1e-6,\n          \"n_fold\":3,\n          \"weight_decay\":1e-6,\n          \"T_max\": 500,\n          \"num_classes\":1,\n          \"margin\": 0.5,\n          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n          \"hash_name\": HASH_NAME,\n          \"max_length\":256}\n\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"])\nCONFIG[\"group\"] = 'f{HASH_NAME}-Baseline'","1e33d03a":"def set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CONFIG['seed'])","58543e22":"df = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\ndf.head()","f961fe5c":"print(len(df))\nlen(np.unique(np.concatenate([df[\"less_toxic\"], df[\"more_toxic\"]])))","88570d84":"df_ruddit = pd.read_csv(\"\/kaggle\/input\/ruddit-jigsaw-dataset\/Dataset\/ruddit_with_text.csv\")\ndf_ruddit = df_ruddit[df_ruddit[\"txt\"]!=\"[deleted]\"]\nlen(df_ruddit)","d067a258":"df_ruddit[\"offensiveness_score\"] = (df_ruddit[\"offensiveness_score\"] - df_ruddit[\"offensiveness_score\"].min() )\/ (df_ruddit[\"offensiveness_score\"].max() - df_ruddit[\"offensiveness_score\"].min() )","ba70955b":"\ncomment_pairs = []\nfor index, row in df_ruddit.iterrows():\n    low_toxic_df = df_ruddit[df_ruddit[\"offensiveness_score\"]<=(row[\"offensiveness_score\"] - 0.3)]\n#     print(low_toxic_df)\n    if len(low_toxic_df)>=4:\n        low_toxic = low_toxic_df.sample(n=4, random_state = index+1).reset_index(drop=True)\n        comment_pairs.append((low_toxic[\"txt\"][0], row[\"txt\"]))\n        comment_pairs.append((low_toxic[\"txt\"][1], row[\"txt\"]))\n    more_toxic_df= df_ruddit[df_ruddit[\"offensiveness_score\"]>=(row[\"offensiveness_score\"] + 0.3)]\n    if len(more_toxic_df)>=4:\n        more_toxic =  more_toxic_df.sample(n=4, random_state = index+2).reset_index(drop=True)\n        comment_pairs.append(( row[\"txt\"],more_toxic[\"txt\"][0]))\n        comment_pairs.append(( row[\"txt\"],more_toxic[\"txt\"][1]))\n    ","f6ffacc0":"df_ruddit_final = pd.DataFrame(comment_pairs, columns= [\"less_toxic\",\"more_toxic\"])\ndf_ruddit_final","efb288b1":"\n\ndf_classification = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\ndf_classification.head()","08aa667e":"## Overlapping comments\n\n### Total unique comments in severity data\ndf_val = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\nprint(df_val.shape)\ntot_unique_comments = np.unique(np.concatenate([df_val[\"less_toxic\"], df_val[\"more_toxic\"]]))\nprint(\"total unique: \", len(tot_unique_comments))\n\n\n# Find cases already present in toxic data\n\ndf_val_1 = pd.merge(df_val, df_classification.loc[:,['comment_text']], \n                  left_on = 'less_toxic', \n                  right_on = 'comment_text', how='inner')\n# print(df_val_1.shape)\n\ndf_val_2 = pd.merge(df_val, df_classification.loc[:,['comment_text']], \n                  left_on = 'more_toxic', \n                  right_on = 'comment_text', how='inner')\n# print(df_val_2.shape)\n\ntot_unique_common = np.unique(np.concatenate([df_val_1[\"comment_text\"], df_val_2[\"comment_text\"]]))\nprint(\"total common: \", len(tot_unique_common))\n\n# Removing those cases\ndf_classification_u = df_classification[~df_classification[\"comment_text\"].isin(tot_unique_common)]\nprint(\"total uncommon :\", len(df_classification_u) )","5d63a4e2":"df_classification_u[\"neutral\"] = 1 - df_classification_u[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]].max(axis=1)\nmore_toxic = df_classification_u[df_classification_u[[\"severe_toxic\",\"threat\", \"toxic\"]].max(axis=1)>=2][\"comment_text\"]\nless_toxic = df_classification_u[df_classification_u[\"neutral\"]==1].sample(n = 10*len(more_toxic), random_state = CONFIG[\"seed\"])\nlen(less_toxic), len(more_toxic)\n\n","3b785339":"more_toxic = more_toxic.repeat(5)\n\nfor l_t, m_t in zip(less_toxic, more_toxic):\n    comment_pairs.append((l_t,m_t))\n    ","80597a21":"pd.set_option(\"display.max_columns\",500)\ndf_multi = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\")\ndf_multi.head()","9d96fa5d":"df_multi[\"identity_associated\"] = df_multi.iloc[:,8:-13].sum(axis=1)","e985d45c":"df_multi[\"neutral\"] = df_multi[[\"toxic\",\"severe_toxicity\",\"obscene\",\"threat\",\"insult\",\"identity_attack\"]].sum(axis=1)==0\n\nmore_toxic_1 = df_multi[df_multi[[\"severe_toxicity\",\"threat\"]].sum(axis=1)>0.2][\"comment_text\"]\nmore_toxic_2 = df_multi[df_multi[\"toxic\"]>=0.8][\"comment_text\"]\nmore_toxic_3 = df_multi[df_multi[\"identity_attack\"]>=0.8][\"comment_text\"]\nmore_toxic = np.unique(np.concatenate([more_toxic_1, more_toxic_2, more_toxic_3]))\n\nless_toxic_1 = df_multi.loc[((df_multi[\"neutral\"]==1) & (df_multi[\"identity_associated\"]==0)),:].sample(n = 4*len(more_toxic), random_state = CONFIG[\"seed\"])[\"comment_text\"]\nless_toxic_2 = df_multi.loc[((df_multi[\"neutral\"]==1) & (df_multi[\"identity_associated\"]>0)),:].sample(n = len(more_toxic), random_state = CONFIG[\"seed\"])[\"comment_text\"]\n\nless_toxic = np.concatenate([less_toxic_1, less_toxic_2])\nlen(less_toxic), 5*len(more_toxic)","04cf54c4":"more_toxic = more_toxic.repeat(5)\n\nfor l_t, m_t in zip(less_toxic, more_toxic):\n    comment_pairs.append((l_t,m_t))","ef51f03c":"len(comment_pairs)","b9e7a79b":"df_2  = pd.DataFrame(comment_pairs, columns = [\"less_toxic\",\"more_toxic\"])\ncombined_data = pd.concat([df[[\"less_toxic\",\"more_toxic\"]], df_2])\ncombined_data.info()","10880646":"combined_data[\"less_toxic\"].str.len().describe() ","e00aa6a2":"df = combined_data.sample(frac=1).reset_index(drop=True)\ndf[\"target\"]=1","24721906":"\ndef clean(data, col):\n    \n    \n    data[col] = data[col].str.lower()\n    data[col] = data[col].str.replace(r\"what's\", \"what is \")\n    data[col] = data[col].str.replace(r\"\\'s\", \" \")\n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \")\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \")\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \")\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \")\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \")\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \")\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \")\n    data[col] = data[col].str.replace('\\s+', ' ')\n    \n  \n\n    # Clean some punctutations\n    data[col] = data[col].str.replace('\\n', ' \\n ')\n    # Remove ip address\n    data[col] = data[col].str.replace(r'(([0-9]+\\.){2,}[0-9]+)',' ')\n    \n    # Remove website\n    data[col] = data[col].str.replace(r'https?:\/\/\\S+|www\\.\\S+', ' ')\n    \n    \n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([\/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3')\n    # Replace repeating characters more than 3 times to length of 3\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1')\n    # patterns with repeating characters \n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1')\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1')\n    data[col] = data[col].str.replace(r'[ ]{2,}',' ').str.strip()   \n    # Add space around repeating characters\n    data[col] = data[col].str.replace(r'([*!?\\'\"]+)',r' \\1 ')    \n    # Remove multiple white spaces\n    data[col] = data[col].str.replace(r' +', ' ')\n    # Remove html tags\n    data[col] = data[col].str.replace(r'<[^<]+?>', ' ')\n    \n    return data\n                     \n","addb8c75":"df = clean(df, \"less_toxic\")\ndf = clean(df, \"more_toxic\")","54a9d58e":"skf = StratifiedKFold(n_splits = CONFIG[\"n_fold\"], shuffle = True, random_state = CONFIG[\"seed\"])\n\nfor fold, (_, val_ind)  in enumerate(skf.split(X = df,y = df[\"target\"])):\n    df.loc[val_ind,\"val_set\"] = int(fold)\n    \ndf[\"val_set\"] = df[\"val_set\"].astype(int)","ef2545f9":"class JigsawDataset():\n    def __init__(self, df, tokenizer, max_length ):\n        self.df = df\n        self.more_toxic = df[\"more_toxic\"].values\n        self.less_toxic = df[\"less_toxic\"].values\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        more_toxic = self.more_toxic[index]\n        less_toxic = self.less_toxic[index]\n        \n        inputs_more_toxic = self.tokenizer(text = more_toxic , truncation=True, padding= \"max_length\", add_special_tokens = True, max_length = self.max_length)\n        inputs_less_toxic = self.tokenizer(text = less_toxic , truncation=True, padding= \"max_length\", add_special_tokens = True, max_length = self.max_length)\n        \n        more_toxic_ids = inputs_more_toxic[\"input_ids\"]\n        more_toxic_mask = inputs_more_toxic[\"attention_mask\"]\n        \n        less_toxic_ids = inputs_less_toxic[\"input_ids\"]\n        less_toxic_mask = inputs_less_toxic[\"attention_mask\"]\n        \n        target = 1\n        return {\"less_toxic_ids\": torch.tensor(less_toxic_ids),\n               \"less_toxic_mask\": torch.tensor(less_toxic_mask),\n               \"more_toxic_ids\": torch.tensor(more_toxic_ids),\n               \"more_toxic_mask\": torch.tensor(more_toxic_mask),\n                \"target\": torch.tensor(target)}\n                                               \n                                               \n    \n        \n        \n        \n        ","3573cd38":"class JigsawModel(nn.Module):\n    def __init__(self, model_name):\n        super(JigsawModel, self).__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n        self.dropout = nn.Dropout(p=0.2)\n        self.fc = nn.Linear(768, CONFIG[\"num_classes\"])\n        \n    def forward(self, ids, mask):\n        out = self.model(input_ids = ids, attention_mask = mask, output_hidden_states = False)\n        out = self.dropout(out[1])\n        outputs  = self.fc(out)\n        return outputs\n        ","4aa4010c":"### Loss Function\n\ndef criterion(outputs1, outputs2, targets):\n    return nn.MarginRankingLoss(margin = CONFIG[\"margin\"])(outputs1, outputs2, targets)","989fae6f":"def train_one_epoch(model, optimizer, dataloader, scheduler, device, epoch):\n    model.train()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    bar = tqdm(enumerate(dataloader), total = len(dataloader))\n    for step , data in bar:\n        less_toxic_ids = data[\"less_toxic_ids\"].to(device)\n        less_toxic_mask = data[\"less_toxic_mask\"].to(device)\n        more_toxic_ids = data[\"more_toxic_ids\"].to(device)\n        more_toxic_mask= data[\"more_toxic_mask\"].to(device)\n        targets = data[\"target\"].to(device)\n        batch_size = less_toxic_ids.size(0)\n        \n        less_toxic_outputs = model(less_toxic_ids, less_toxic_mask)\n        more_toxic_outputs = model(more_toxic_ids, more_toxic_mask)\n        \n        loss = criterion(more_toxic_outputs, less_toxic_outputs, targets)\n        loss.backward()\n        \n        optimizer.step()\n        optimizer.zero_grad()\n        if scheduler:\n            scheduler.step()\n            \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss \/ dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n    gc.collect()\n    \n    return epoch_loss\n        \n    ","70c64230":"def valid_one_epoch(model, dataloader, device, epoch):\n    model.eval()\n    dataset_size = 0\n    running_loss = 0.0\n    bar = tqdm(enumerate(dataloader), total = len(dataloader))\n    for step, data in bar:\n        with torch.no_grad():\n            less_toxic_ids = data[\"less_toxic_ids\"].to(device)\n            less_toxic_mask = data[\"less_toxic_mask\"].to(device)\n            more_toxic_ids = data[\"more_toxic_ids\"].to(device)\n            more_toxic_mask= data[\"more_toxic_mask\"].to(device)\n            targets = data[\"target\"].to(device)\n            \n            batch_size = less_toxic_ids.size(0)\n            \n            less_toxic_outputs = model(less_toxic_ids, less_toxic_mask)\n            more_toxic_outputs = model(more_toxic_ids, more_toxic_mask) \n            \n            loss = criterion(more_toxic_outputs, less_toxic_outputs, targets)\n            \n            running_loss += (loss.item() * batch_size)\n            dataset_size += batch_size\n        \n            epoch_loss = running_loss \/ dataset_size\n        \n            bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n    gc.collect()\n    \n    return epoch_loss\n        \n            \n","a75dad7a":"def run_training(model, optimizer, scheduler, num_epochs, device, folds, train_loader, val_loader):\n    \n    wandb.watch(model, log_freq=10)\n    \n    if torch.cuda.is_available():\n        print(\"Using GPU :) ==> {}\".format(torch.cuda.get_device_name()))\n    \n    start = time.time()\n    best_model_weights = copy.deepcopy(model.state_dict())\n    best_epoch_loss= np.inf\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs+1):\n        gc.collect()\n        \n        train_epoch_loss =  train_one_epoch(model, optimizer, dataloader = train_loader, scheduler = scheduler, device = CONFIG[\"device\"], epoch= epoch)\n        \n        val_epoch_loss = valid_one_epoch(model,dataloader = val_loader,device = CONFIG[\"device\"], epoch=epoch)\n        \n        history[\"train_loss\"].append(train_epoch_loss)\n        history[\"val_loss\"].append(val_epoch_loss)\n        \n        wandb.log({\"Train Loss\": train_epoch_loss})\n        wandb.log({\"Valid Loss\": val_epoch_loss})\n        \n        if val_epoch_loss <= best_epoch_loss:\n            print(f\"{b_}Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n            best_epoch_loss = val_epoch_loss\n            run.summary[\"Best Loss\"] = best_epoch_loss\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = f\"Loss-Fold-{fold}.bin\"\n            torch.save(model.state_dict(), PATH)\n            # Save a model file from the current directory\n            print(f\"Model Saved{sr_}\")\n            \n        print()\n        \n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 3600, (time_elapsed % 3600) \/\/ 60, (time_elapsed % 3600) % 60))\n    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model, history\n    ","76e92c94":"def prepare_loaders(fold):\n    df_train = df[df[\"val_set\"]!= fold]\n    df_val = df[df[\"val_set\"]==fold]\n    \n    train_dataset = JigsawDataset(df_train, CONFIG[\"tokenizer\"], CONFIG[\"max_length\"])\n    \n    val_dataset = JigsawDataset(df_val, CONFIG[\"tokenizer\"], CONFIG[\"max_length\"])\n    \n    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n                              num_workers=2, shuffle=True, pin_memory=True, drop_last=True)\n    \n    val_loader = DataLoader(val_dataset, batch_size=CONFIG['val_batch_size'], \n                              num_workers=2, shuffle=True, pin_memory=True, drop_last=True)\n    \n    return train_loader, val_loader\n","cbd50c25":"def fetch_scheduler(optimizer):\n    if CONFIG['scheduler'] == 'cosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n                                                   eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == 'cosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], \n                                                             eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == None:\n        return None\n        \n    return scheduler","47ee4424":"for fold in range(0, CONFIG['n_fold']):\n    print(f\"{y_}====== Fold: {fold} ======{sr_}\")\n    run = wandb.init(project='Jigsaw', \n                     config=CONFIG,\n                     job_type='Train',\n                     group=CONFIG['group'],\n                     tags=['roberta-base', f'{HASH_NAME}', 'margin-loss'],\n                     name=f'{HASH_NAME}-fold-{fold}',\n                     anonymous='must')\n    \n    # Create Dataloaders\n    train_loader, valid_loader = prepare_loaders(fold=fold)\n    \n    model = JigsawModel(CONFIG['model_name'])\n    model.to(CONFIG['device'])\n    \n    # Define Optimizer and Scheduler\n    optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n    scheduler = fetch_scheduler(optimizer)\n                        \n    model, history = run_training(model, optimizer, scheduler,\n                                  device=CONFIG['device'],\n                                  num_epochs=CONFIG['num_epochs'],\n                                  folds=fold,\n                                  train_loader  = train_loader, val_loader = valid_loader)\n    \n    run.finish()\n    \n    del model, history, train_loader, valid_loader\n    _ = gc.collect()\n    print()","5b2a85f0":"## Training","ffbd6764":"#### Original work: https:\/\/www.kaggle.com\/debarshichanda\/pytorch-w-b-jigsaw-starter\n#### This notebook applies the same strategy by augmenting the data from other Jigsaw competitions (toxic classification , Ruddit Data, Jigsaw Unintended Bias data) by creating less toxic - more toxic pairs and training ROBERTA on the augmented data\n\n### LB score: 0.834\n","58eb8f53":"### Prepare Ruddit data","ba1f6ba8":"### Prepare Jigsaw unintended Bias data","3bda1f31":"### Combining all Together","a4861ad2":"### Fold prep and training","b1a6c24e":"### Prepare toxic classification data"}}