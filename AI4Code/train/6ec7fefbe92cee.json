{"cell_type":{"98eb3c46":"code","3c178e57":"code","c04d9598":"code","a5c0a636":"code","b87a701a":"code","6624b3ea":"code","ae9161e3":"code","ddb92537":"code","c71afa72":"code","69f7c859":"code","a8634a98":"code","d4bda0b2":"code","3156f9fc":"code","b7f48ecf":"code","7d318c5d":"code","6b81b8d9":"code","6be478c3":"code","f5a33598":"code","0df7bebf":"code","aba77ffd":"markdown","f3d2345c":"markdown","967ff26f":"markdown","3e555717":"markdown","5bd8d209":"markdown","5f61fe72":"markdown","b1e05892":"markdown","9bdf0c1e":"markdown","62dec3a0":"markdown"},"source":{"98eb3c46":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3c178e57":"import numpy as np\nimport pandas as pd\nimport nltk\nimport re\nimport os\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.models import Model\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Activation, Conv2D, Input, Embedding, Reshape, Flatten, Dense, Conv1D, MaxPool1D, Concatenate, TimeDistributed, LSTM\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score\nfrom sklearn.model_selection import KFold","c04d9598":"PATH_DATASET = \"..\/input\/foodreview\/foodreview.csv\"\nPATH_GLOVE = \"..\/input\/glove6b100dtxt\/glove.6B.100d.txt\"","a5c0a636":"def get_word2vec(resource_path):\n    embeddings_index = {}\n    f = open(resource_path, encoding=\"utf8\")\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n    f.close()\n    print('Found %s word vectors.' % len(embeddings_index))\n    return embeddings_index\n\nFOODREVIEW_CSV = pd.read_csv(PATH_DATASET, encoding = \"ISO-8859-1\")\nGLOVE_W2V = get_word2vec(PATH_GLOVE)","b87a701a":"FOODREVIEW_CSV.head()","6624b3ea":"#preprocessing functions\n\ndef lower(input_string):\n    return input_string.lower()\n\ndef remove_html(input_string):\n    html_compile = re.compile('<.*?>')\n    cleantext = re.sub(html_compile, '', str(input_string))\n    return cleantext\n\ndef remove_number(input_string):\n    return re.sub(r'\\d+', '', str(input_string))\n\ndef remove_punc(input_string):\n    return re.sub(r'[^\\w\\s]','', str(input_string))\n\ndef remove_whitespace(input_string):\n    return input_string.strip()\n\ndef tokenize(input_string):\n    return word_tokenize(input_string)\n    \ndef remove_stopwords(input_tokens):\n    stop_words = set(stopwords.words('english'))\n    return [i for i in input_tokens if not i in stop_words]\n\ndef stemming(input_tokens):\n    ps = PorterStemmer()\n    return [ps.stem(i) for i in input_tokens]\n\ndef preprocess_one_text(input_sentence):\n    result = remove_html(input_sentence)\n    result = remove_number(result)\n    result = remove_punc(result)\n    result = lower(result)\n    result = remove_whitespace(result)\n    result = tokenize(result)\n    result = remove_stopwords(result)\n    result = stemming(result)\n    return result\n\ndef preprocess_texts(dataset):\n    return [preprocess_one_text(datum) for datum in dataset]","ae9161e3":"# taro hasil preprocessing di sini ya gan.\nfoodreview_sample = FOODREVIEW_CSV.sample(n=50000, random_state=1)\nfoodreview_sample = foodreview_sample.reset_index()\nmerged_texts = [str(sum_text) + \" \" + str(main_text) for sum_text, main_text in zip(foodreview_sample['Summary'], foodreview_sample['Text'])]\nprepocessed_data = preprocess_texts(merged_texts)\nlabels = foodreview_sample['Score']\n\ndef keras_tokenizer(dataset, num_words=500):\n    tokenizer  = Tokenizer(num_words)\n    tokenizer.fit_on_texts(dataset)\n    sequences =  tokenizer.texts_to_sequences(dataset)\n\n    word_index = tokenizer.word_index\n    print(\"unique words : {}\".format(len(word_index)))\n\n    return tokenizer, word_index, pad_sequences(sequences, maxlen=num_words)\n\n# ini 2 input yang dimasukin ke CNN\ntokenizer_keras, word_idx, keras_input = keras_tokenizer(prepocessed_data)","ddb92537":"foodreview_sample","c71afa72":"def generate_embedded_matrix(embedded_index, word_index, embedding_dim):\n    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n    for word, i in word_index.items():\n        embedding_vector = embedded_index.get(word)\n        if embedding_vector is not None:\n            # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n    \n    return embedding_matrix\n\ndef create_embedded_layer(word_index, embedding_dim, max_seq_length, w2v_model=None):\n    word2vec_index = w2v_model if w2v_model else get_word2vec(PATH_GLOVE)\n    embedding_matrix = generate_embedded_matrix(word2vec_index, word_index, embedding_dim)\n    return Embedding(len(word_index) + 1,\n                            embedding_dim,\n                            weights=[embedding_matrix],\n                            input_length=max_seq_length,\n                            trainable=False)","69f7c859":"class ProposedMultiChannelCNNLSTM:\n    def __init__(self, word_index, epochs=20):\n        \n        self.model = None\n        self.word_idx = word_index\n        self.max_seq_len = 500\n        self.embedding_dim = 100\n        self.epochs = epochs\n        \n    def _build_model(self, label_count):\n        sequence_input = Input(shape=(self.max_seq_len,), dtype='int32')\n        \n        # pembuatan embedded layer\n        embedding_layer = create_embedded_layer(self.word_idx, \n                                                self.embedding_dim, \n                                                self.max_seq_len)\n        embedded_sequences = embedding_layer(sequence_input)\n        \n        # convultion layers. untuk feature extraction\n        conv_1 = Conv1D(200, 3, activation='relu')(embedded_sequences)\n        pool_1 = MaxPool1D(5)(conv_1)\n        \n        conv_2 = Conv1D(200, 4, activation='relu')(embedded_sequences)\n        pool_2 = MaxPool1D(5)(conv_2)\n        \n        conv_3 = Conv1D(200, 5, activation='relu')(embedded_sequences)\n        pool_3 = MaxPool1D(5)(conv_3)\n        \n        concat_layer = Concatenate(axis=1)([pool_1, pool_2, pool_3])\n        \n        x = Conv1D(200, 5, activation='relu')(concat_layer)\n        x = MaxPool1D(293)(x)  # global max pooling\n        x = TimeDistributed(Flatten())(x)\n        x = LSTM(512)(x)\n        x = Dense(128, activation='relu')(x)\n        preds = Dense(label_count, activation='softmax')(x)\n\n        self.model = Model(sequence_input, preds)\n        self.model.compile(loss='categorical_crossentropy',\n                      optimizer='rmsprop',\n                      metrics=['acc'])\n        \n    def fit(self, X, y):\n        print(\"Fitting...\")\n        self._build_model(len(set(y)))\n        y_categ = to_categorical(y)\n        self.model.fit(X, y_categ[:,1:], epochs=self.epochs)\n        print(\"Fitting completed\")\n        print()\n        \n    def predict(self, X):\n        print(\"Predicting...\")\n        return [np.argmax(x_pred)+1 for x_pred in self.model.predict(X)]\n    \n    def summarize(self):\n        print(\"Summarizing...\")\n        if not self.model:\n            print(\"Model hasn't been trained\")\n        else:\n            self.model.summary()\n            print(\"Summarizing completed\")\n        print()","a8634a98":"kf = KFold(n_splits=5)\nproposed_cnn = ProposedMultiChannelCNNLSTM(word_idx, 20)\nX = keras_input\ny = labels\nacc_list = []\nf1_list = []\nconfuse_list = []\n\nfor train_index, test_index in kf.split(X):\n    print(train_index)\n    print(test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    proposed_cnn.fit(X_train, y_train)\n    proposed_preds = proposed_cnn.predict(X_test)\n    acc_list.append(accuracy_score(y_test, proposed_preds))\n    f1_list.append(f1_score(y_test, proposed_preds, average='macro'))\n    confuse_list.append(confusion_matrix(y_test, proposed_preds))","d4bda0b2":"len(set(y))","3156f9fc":"print(\"Cross Validation Results\")\nk = 1\nfor acc, f1_sc in zip(acc_list, f1_list):\n    print(\"Fold \", k)\n    print(\"Accuracy: \", acc)\n    print(\"F1 Score: \", f1_sc)\n    print()\n    k+=1","b7f48ecf":"df_metrics = {\"Fold\": [\"Fold \" + str(k+1) for k in range(5)], \"Accuracy\": acc_list, \"F1 Score\":f1_list}\nprint(\"Cross Validation Evaluation\")\npd.DataFrame(df_metrics)","7d318c5d":"model_to_test = ProposedMultiChannelCNNLSTM(word_idx, 20)\nmodel_to_test.fit(keras_input, labels)","6b81b8d9":"# masukkan test set anda pada variabel ini. Diasumsikan test_set berupa csv\ntest_set = pd.read_csv(\"[INSERT CSV DATASET HERE]\")","6be478c3":"# preprocessing test test\nmerged_test_texts = [str(sum_text) + \" \" + str(main_text) for sum_text, main_text in zip(test_set['Summary'], test_set['Text'])]\npreprocessed_test_set = preprocess_texts(merged_test_texts)\ntest_set_keras_input = pad_sequences(tokenizer_keras.text_to_sequences(preprocessed_test_set), maxlen=num_words)","f5a33598":"# predicting\npreds = model_to_test.predict(test_set_keras_input)","0df7bebf":"# Penghitungan Metrik\nprint(\"Test Accuracy: \", accuracy_score(test_set[\"Score\"], preds))\nprint(\"Test F1 Score: \", f1_score(test_set[\"Score\"], preds))","aba77ffd":"### Validation ###","f3d2345c":"### Proposed Multi-Channel CNN-LSTM (Testing yang ini) ###","967ff26f":"### Validation Result ###","3e555717":"### Resource Paths ###","5bd8d209":"### Import Section ###","5f61fe72":"### Preprocessing ###","b1e05892":"### Model to Test\n\nModel yang dapat digunakan oleh tim pengajar untuk melakukan testing","9bdf0c1e":"### Embedding Layers ###","62dec3a0":"### Resource Variable ###"}}