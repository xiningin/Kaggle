{"cell_type":{"2273e45d":"code","0347d39e":"code","ddad29e0":"code","15d3eb45":"code","2b02f8dc":"code","c5170a18":"code","c925bf00":"code","1cd9da5b":"code","fd4d89e2":"code","b9282854":"code","a7062f90":"code","ab49d0d1":"code","c3895a87":"code","03d8b6f2":"code","13b250be":"code","df797fd7":"code","eed2ed47":"code","e2d7d0d3":"code","ed30fe30":"code","7cb53a44":"code","47882a82":"code","eb4cd111":"code","e363db13":"code","e37f3278":"code","3745f25c":"code","c679f18e":"code","02d57ba0":"code","40bc1e48":"code","1caceccc":"code","0fa8d341":"code","448bedb2":"code","12a65271":"code","50fc1f71":"code","65184307":"markdown","0e86d72c":"markdown","dd569391":"markdown","86dc7f74":"markdown","4d3ece47":"markdown","b9429091":"markdown"},"source":{"2273e45d":"!pip install imblearn","0347d39e":"import pandas as pd\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.combine import SMOTEENN\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom sklearn.tree import plot_tree, export_text\n%matplotlib inline","ddad29e0":"df = pd.read_csv('..\/input\/churnprediction\/churn.csv')\ndf.head()","15d3eb45":"df = df.drop('Unnamed: 0',axis=1)","2b02f8dc":"df.head()","c5170a18":"x = df.drop('Churn',axis=1)\ny = df['Churn']","c925bf00":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)","1cd9da5b":"x_train.shape, x_test.shape , x.shape","fd4d89e2":"model = DecisionTreeClassifier(criterion='gini',random_state=42,max_depth=7,min_samples_leaf=10,max_leaf_nodes=128)","b9282854":"model.fit(x_train,y_train)","a7062f90":"y_pred = model.predict(x_test)","ab49d0d1":"y_pred","c3895a87":"importance_df = pd.DataFrame({\n    'feature': x_train.columns,\n    'importance': model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nimportance_df.head(10)","03d8b6f2":"plt.title('Feature Importance')\nsns.barplot(data=importance_df.head(10), x='importance', y='feature')","13b250be":"# import matplotlib.pyplot as plt\n# from sklearn.tree import plot_tree, export_text\n# %matplotlib inline\n\n# plt.figure(figsize=(80,20))\n# plot_tree(model.estimators_[0], max_depth=2, feature_names=X_train.columns, filled=True, rounded=True, class_names=model.classes_);","df797fd7":"print(classification_report(y_test,y_pred, labels=[0,1]))","eed2ed47":"print(confusion_matrix(y_test,y_pred))","e2d7d0d3":"model.score(x_train, y_train)","ed30fe30":"sm = SMOTEENN()\nx_resampled, y_resampled = sm.fit_resample(x,y)","7cb53a44":"xre_train,xre_test,yre_train,yre_test = train_test_split(x_resampled, y_resampled,test_size=0.2)","47882a82":"model_smote = DecisionTreeClassifier(criterion='gini',random_state=42,max_depth=7,min_samples_leaf=10,max_leaf_nodes=128)","eb4cd111":"model_smote.fit(xre_train,yre_train)","e363db13":"yre_pred = model_smote.predict(xre_test)","e37f3278":"yre_pred","3745f25c":"print(classification_report(yre_test,yre_pred, labels=[0,1]))","c679f18e":"print(confusion_matrix(yre_test,yre_pred))","02d57ba0":"model_rf = RandomForestClassifier(n_jobs=-1, random_state=42,n_estimators=150)","40bc1e48":"xrf_train,xrf_test,yrf_train,yrf_test = train_test_split(x_resampled, y_resampled,test_size=0.2)\nmodel_rf.fit(xrf_train,yrf_train)","1caceccc":"yrf_pred = model_rf.predict(xrf_test)\nyrf_pred","0fa8d341":"print(classification_report(yrf_test,yrf_pred, labels=[0,1]))","448bedb2":"print(confusion_matrix(yrf_test,yrf_pred))","12a65271":"importance_rf = pd.DataFrame({\n    'feature': xrf_train.columns,\n    'importance': model_rf.feature_importances_\n}).sort_values('importance', ascending=False)\n\nimportance_rf.head(10)","50fc1f71":"plt.title('Feature Importance')\nsns.barplot(data=importance_rf.head(10), x='importance', y='feature');","65184307":"### we will use RandomForest technique","0e86d72c":"A random forest works by averaging\/combining the results of several decision trees:\n\n<img src=\"https:\/\/1.bp.blogspot.com\/-Ax59WK4DE8w\/YK6o9bt_9jI\/AAAAAAAAEQA\/9KbBf9cdL6kOFkJnU39aUn4m8ydThPenwCLcBGAsYHQ\/s0\/Random%2BForest%2B03.gif\" width=\"640\">\n","dd569391":"### Now we see that there are huge differenece before and after **resampling data**","86dc7f74":"### Due to imbalanced data the accuracy predition about churners are low so we will fix it with **SMOTE + ENN**\n\n### **SMOTE + ENN** is  technique where more no. of observations are removed from the sample space. ENN is undersampling technique where the nearest neighbors of each of the majority class is estimated. If the nearest neighbors misclassify that particular instance of the majority class, then that instance gets deleted.\n\n### Integrating this technique with oversampled data done by **SMOTE** helps in doing extensive data cleaning. Here on misclassification by NN\u2019s samples from both the classes are removed. This results in a more clear and concise class separation.","4d3ece47":"### we notice that the distribution is less skewed than a single decision tree.","b9429091":"`n_jobs` allow random forest to use mutiple parallel workers to train decision trees(**-1 means using all processors**),<br> `random_state=42` for the same execution results.<br>\n`n_estimators` this argument controls the number of decision trees in the random forest. **The default value is 100**, for larger datasets **(many features)**. As a general rule, try to have as few estimators as needed. "}}