{"cell_type":{"891b9a5f":"code","404b5034":"code","1ac98529":"code","04fdd91e":"code","338ea9bb":"code","9c9d663b":"code","825e297d":"code","2329ae75":"code","31023497":"code","2e40ea0d":"code","91b5b4d3":"code","4db1b0b2":"code","5983d999":"code","75eb15a0":"code","e6c29f82":"code","6d1625b8":"code","b5156504":"code","90c4b088":"code","7278611f":"code","70031dc2":"code","f3a5d145":"code","a2535738":"code","58699d06":"code","d13b99cf":"code","27ed3996":"code","39254676":"code","ea94ffe8":"code","8858f34c":"code","53a00a2f":"code","5dadf406":"code","f3fb3ab4":"code","ff786dd0":"code","e6251753":"code","bbdfa35d":"code","17fcfe43":"code","4e8840fc":"markdown","1aed686c":"markdown","2f242f38":"markdown","9e28f841":"markdown","1290c795":"markdown","4d5606c5":"markdown","b4245ab1":"markdown","97e3dd7d":"markdown","b43c3b23":"markdown","da359e76":"markdown","5228fd5a":"markdown","9268708c":"markdown","49cc1e3e":"markdown","7afe65ee":"markdown","cb858dfd":"markdown","510ec195":"markdown","f7825a50":"markdown","a30bed51":"markdown","e2f44b01":"markdown","91815567":"markdown","1fe6b123":"markdown","fce064df":"markdown","49e3a4a9":"markdown","2c944cba":"markdown","ff459d1a":"markdown","1bfe68c9":"markdown","3e2fa612":"markdown","2e55b587":"markdown","bdf57c26":"markdown","94a7e0b3":"markdown","003d1bb0":"markdown","442d8495":"markdown","b7207120":"markdown","c5cb6560":"markdown","172df864":"markdown","5f0ca607":"markdown","d62ee1f5":"markdown","7faad19f":"markdown","0ad726b8":"markdown","04568020":"markdown","fb47ad32":"markdown","49901476":"markdown","3604f0a5":"markdown","fe799bce":"markdown","c5e63a57":"markdown","6f95f000":"markdown"},"source":{"891b9a5f":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","404b5034":"\ndataset = pd.read_csv('..\/input\/50-startups\/50_Startups.csv')\ndataset.head()","1ac98529":"dataset.isnull().sum()","04fdd91e":"dataset.info()","338ea9bb":"dataset.describe()","9c9d663b":"sns.distplot(dataset['R&D Spend'], color = 'green')","825e297d":"sns.distplot(dataset['Administration'], color = 'red')","2329ae75":"sns.distplot(dataset['Marketing Spend'], color = 'orange')","31023497":"sns.pairplot(dataset)","2e40ea0d":"dataset.corr()","91b5b4d3":"sns.heatmap(dataset.corr(), annot = True)","4db1b0b2":"\nX = dataset.iloc[:,:-1].values\n\ny = dataset.iloc[:,4].values\n\nprint(X)","5983d999":"print(y)","75eb15a0":"\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nlabelencoder_X = LabelEncoder()\nX[:,3] = labelencoder_X.fit_transform(X[:,3])\n\n# Country column\nct = ColumnTransformer([(\"Country\", OneHotEncoder(), [3])], remainder = 'passthrough')\n                                # creating dummy var(for states means 3 diff. column ) \nX = ct.fit_transform(X)\nprint(X)\n\n","e6c29f82":"\nX = X[:,1:]\nprint(X)","6d1625b8":"from sklearn.model_selection import train_test_split\nX_train,X_test, y_train, y_test = train_test_split(X,y,test_size = 1\/3, random_state = 0)","b5156504":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)","90c4b088":"y_pred = regressor.predict(X_test)","7278611f":"\nfor i,j in np.nditer((y_test, y_pred)):\n    print(i,\"      \", j)  # compare actual price vs predicted price\n    ","70031dc2":"print(regressor.score(X_train, y_train))\nprint(regressor.score(X_test,y_test))\n\n","f3a5d145":"\nimport statsmodels.api as sm\n# (bydefault it's not take constant(thetas 0 ,we have to put theta_0 * X0 = 1\n# that's why we are creating col. of 1's and trying to put in the starting of X)\nX = np.append(arr  = np.ones((50,1)).astype(int),values = X, axis = 1)# we are adding 1 extra col. in the starting  of X\nprint(X)\n\n\n# np.append(values = X, np.ones((50,1)), axis = 1) # it will add col. at the last of X dataset","a2535738":"X_opt =X[:, [0,1,2,3,4,5]].astype(float)\nregressor_OLS = sm.OLS(y,X_opt).fit()  \nregressor_OLS.summary()\n","58699d06":"\nX_opt =X[:, [0,1,3,4,5]].astype(float)   # removed X2(dummmy var) cause p>SL(significant level =0.5)\nregressor_OLS = sm.OLS(y,X_opt).fit()           #    p>SL (0.990 > 0.05)\nregressor_OLS.summary()\n","d13b99cf":"X_opt =X[:, [0,3,4,5]].astype(float)   # removed X1(dummy var) cause P>SL(0.953 > 0.05)\nregressor_OLS = sm.OLS(y,X_opt).fit()\nregressor_OLS.summary()\n","27ed3996":"X_opt =X[:, [0,3,5]].astype(float)   #     removed X4(administration) cause P>SL (0.608 > 0.05)\nregressor_OLS = sm.OLS(y,X_opt).fit()\nregressor_OLS.summary()\n","39254676":"X_opt =X[:, [0,3]].astype(float)   # removed X5 (marketing spend) cause P>SL(0.060 > 0.05)\nregressor_OLS = sm.OLS(y,X_opt).fit()\nregressor_OLS.summary()\n","ea94ffe8":"  \nx_BE= dataset.iloc[:,[0]].values  \ny_BE= dataset.iloc[:, -1].values  \n  ","8858f34c":" \nfrom sklearn.model_selection import train_test_split  \nx_BE_train, x_BE_test, y_BE_train, y_BE_test= train_test_split(x_BE, y_BE, test_size= 0.20, random_state=0)  \n  ","53a00a2f":"  \nfrom sklearn.linear_model import LinearRegression  \nregressor= LinearRegression()  \nregressor.fit(np.array(x_BE_train).reshape(-1,1), y_BE_train)  ","5dadf406":"\ny_pred= regressor.predict(x_BE_test)  \n","f3fb3ab4":"    \nprint('Train Score: ', regressor.score(x_BE_train, y_BE_train))  \nprint('Test Score: ', regressor.score(x_BE_test, y_BE_test))  ","ff786dd0":"for i,j in np.nditer((y_BE_test,y_pred)):\n    print(i,\"      \", j)\n    ","e6251753":"plt.plot(dataset.iloc[:,0], dataset.iloc[:, 4], color = 'green')\nplt.xlabel('R&D Spends')\nplt.ylabel('Profits')\nplt.title('Relation b\/w the R&D spend and Profits')\nplt.grid()","bbdfa35d":"plt.scatter(x_BE_train, y_BE_train, color = 'red')\nplt.plot(x_BE_train, regressor.predict(x_BE_train), color = 'blue')\nplt.title('R&D spend vs Profit (Training set)')\nplt.xlabel('R&D spend')\nplt.ylabel('Profit')\nplt.grid(color='gold', linestyle='-.', linewidth=0.7)\nplt.show()\n","17fcfe43":"plt.scatter(x_BE_test, y_BE_test, color = 'red')\nplt.plot(x_BE_test, regressor.predict(x_BE_test), color = 'blue')\nplt.title('R&D spend vs Profit (Test set)')\nplt.xlabel('R&D spend')\nplt.ylabel('Profit')\nplt.grid(color = 'green', linestyle='-.', linewidth=0.7)\nplt.show()","4e8840fc":"#### 5.1 Predicting the Test set results","1aed686c":"### 6. find optimal Model using backward elimination","2f242f38":"## Multiple Linear Regression","9e28f841":"### 4 Modeling","1290c795":"####  7.3 Fitting the MLR model to the training set","4d5606c5":"#### 5.3 evaluate the train & test score performance","b4245ab1":"### 1.2 Load dataset","97e3dd7d":"####  7.5 Cheking the score","b43c3b23":"#### 2.6 find the correlation","da359e76":"\nWe got this result by using one independent variable (R&D spend) only instead of four variables. Hence, now our model is simple and accurate.","5228fd5a":"- There is no null values","9268708c":"#### Backward elimination:\nBackward elimination is a feature selection technique while building a machine learning model. It is used to remove those features that do not have a significant effect on the dependent variable or prediction of output.\n","49cc1e3e":"#### 3.2 Encoding categorical data :","7afe65ee":"#### 8.2 Visualizing the train set result ","cb858dfd":"#### 6.1 applying backward elimination","510ec195":"### 8. Visualizing the final result ","f7825a50":"#### 2.1 identifying  the missing values","a30bed51":"#### 5.2 comparing the actual_price with predicted_price","e2f44b01":"- Here above we can see that __R&D Spend__ is highly correlated to __Profit__.","91815567":"#### 7.6 Comparision b\/w actual price and predicted price ","1fe6b123":"####  8.3 Visualizing the test set result ","fce064df":"# About Dataset:\ndataset has data collected from New York, California and Florida about 50 business Startups \"17 in each state\". The variables used in the dataset are Profit, R&D spending, Administration Spending, and Marketing Spending.\n- We have to  make a Model that can predict the profit based on the comapanies data.","49e3a4a9":"####  3.3  Avoiding the dummy variable trap:","2c944cba":"- Here above we can see that R&D Spend have linear relationship with Profit.\n- So here it's most significant feature compare to others.","ff459d1a":"- Here __state__ column is object type. later we will convert this column  into dummy varible.","1bfe68c9":"### 3. Data Preparing","3e2fa612":"### 1.1 importing Libraries","2e55b587":" To encode the categorical variable into numbers, we will use the LabelEncoder class. But it is not sufficient because it still has some relational order, which may create a wrong model. So in order to remove this problem, we will use OneHotEncoder, which will create the dummy variables. Below is code for it:\n","bdf57c26":"### 2 EDA","94a7e0b3":"####  7.1 Extracting Independent and dependent Variable","003d1bb0":"#### 3.4 Splitting the dataset into the Training set and Test set","442d8495":"#### 2.4  Checking the distribution of  'R&D Spend',    'Administration'   &  'Marketing Spend'","b7207120":"####  2.5 Checking the relation b\/w the features and o\/p variable","c5cb6560":"\n### 5. Making the predictions and evaluating the model","172df864":"####  8.1 Visualizing the R&d spend with Profits","5f0ca607":"#### 7.2 Splitting the dataset into training and test set","d62ee1f5":"#### 4.1 Training the Multiple Linear Regression model on the Training set","7faad19f":"#### 2.3 Descriptive Analysis","0ad726b8":"#### 2.2 checking the datatype","04568020":"If we do not remove the first dummy variable, then it may introduce multicollinearity in the model.","fb47ad32":"### 7. Apply  optimal Multiple Linear Regression model","49901476":"#### 3.1 splitting  data into dependent &  independent varibles ","3604f0a5":"### 9. Conclusion :","fe799bce":"Unnecessary features increase the complexity of the model. Hence it is good to have only the most significant features and keep our model simple to get the better result.","c5e63a57":"R&D independent variable is a significant variable for the prediction. So we  predicted efficiently using this variable.\nWe can see below the relation of R&d spend with Profits.","6f95f000":"####  7.4 Predicting the Test set result"}}