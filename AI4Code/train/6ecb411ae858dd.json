{"cell_type":{"3b672c77":"code","82827715":"code","6f1c93fe":"code","645fba2a":"code","4be9c314":"code","bc0e3e42":"code","7dd06ab6":"code","249dca22":"code","f028b301":"code","b72d75b6":"code","b8e02e1f":"code","2ee974b6":"code","c69093ea":"code","734230b8":"code","e6e157da":"code","70a551b5":"code","441c376a":"code","b873e04f":"code","8bd1b2b6":"code","fa588091":"code","76b33e31":"code","43457f29":"code","663984e5":"code","06122198":"code","276372c1":"code","71d3a8d1":"code","8b3a5663":"code","543f9002":"code","4ffca074":"code","0bbfcdbb":"code","80d441bc":"code","5ad36342":"code","fed0c28f":"code","1d6929fe":"markdown","dd32d940":"markdown","ce7a33ca":"markdown","20887419":"markdown","1de781d6":"markdown","4db4d7d0":"markdown","09da3942":"markdown","c72170b5":"markdown","005579f1":"markdown","51841122":"markdown","6198e6dd":"markdown","17ec82c3":"markdown","dac2d69a":"markdown","ef941479":"markdown","0a431bc8":"markdown","56fdb22d":"markdown","546e49f9":"markdown","7ee92a09":"markdown","c2a7656a":"markdown","bb8f53ec":"markdown","2ed500d1":"markdown","abff932f":"markdown","a62681a7":"markdown","6138e8a0":"markdown","a0752222":"markdown","89f385be":"markdown","746f8992":"markdown","a926ae66":"markdown","cd435ccb":"markdown","8e78ebda":"markdown","5faf651b":"markdown","edb4b5f8":"markdown","15f29c34":"markdown","5b1d4c07":"markdown","5ab5a5bf":"markdown","7bb7c0f4":"markdown","e50957ed":"markdown","7736046a":"markdown","9ea42427":"markdown","005b6e38":"markdown","44ce51de":"markdown","7c4c48c8":"markdown"},"source":{"3b672c77":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.display.float_format = '{:,.3f}'.format\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","82827715":"def Numeric_data_eda(feature_name,feature_name_long,target):\n    '''\n    This function can be used to get general information about a feature and its relationship with a target variable\n    The feature must be numeric and the target must be  binary.\n    The relationship is only visualized, no statistical test are done to examine the relationship.\n    '''\n    #print the descriptive statistics\n    print('Descriptive Statistics of {} \\n'.format(feature_name_long))\n    print(data[feature_name].describe())\n    \n    #print the descriptive statistics by target\n    print('Descriptive Statistics of {} by target\\n'.format(feature_name_long))\n    print(data.groupby(target)[feature_name].describe())\n\n    #Visualize \n    fig=plt.figure(figsize=(21,6))\n    ### first plot-distribution plot\n    ax1=fig.add_subplot(1,1,1)\n    sns.distplot(data[feature_name],kde=False,ax=ax1)\n    plt.xlabel(feature_name_long)\n    plt.title(str(feature_name_long+' Distribution'))\n\n    #Visualize the relationship\n    ##create a filter for histogram\n    filter_target=data[target]==0\n    ##create a figure\n    fig=plt.figure(figsize=(21,6))\n    ### first plot\n    ax1=fig.add_subplot(1,2,1)\n    filter_target=data[target]==0\n    sns.distplot(data[filter_target][feature_name],color='b',label='Healthy',ax=ax1,kde=False)\n    sns.distplot(data[~filter_target][feature_name],color='r',label='Unhealthy',ax=ax1,kde=False)\n    plt.title('{} Distribution by Target'.format(feature_name_long))\n    plt.xlabel(feature_name_long)\n    plt.legend()\n    ### second plot\n    ax2=fig.add_subplot(1,2,2)\n    sns.violinplot(x=target,y=feature_name,data=data,ax=ax2)\n    plt.title('{} Distribution by Target'.format(feature_name_long))\n    plt.xlabel(feature_name_long)\n    \ndef categoric_data_eda(feature_name,feature_name_long,target):\n    '''\n    This function can be used to get general information about a feature and its relationship with a target variable\n    The feature must be categoric and the target must be  binary.\n    The relationship is only visualized, no statistical test are done to examine the relationship.\n    '''\n    #print the number of patients by feature name\n    print(feature_name_long)\n    print(data[feature_name].value_counts())\n    \n    #Plot the data and visualize the relationship between target and feature \n    ##Create a figure\n    fig=plt.figure(figsize=(18,6))\n    ## Add a subplot for feature distribution\n    ax1=fig.add_subplot(1,2,1)\n    sns.countplot(x=feature_name,data=data,ax=ax1)\n    plt.title('{} Distribution'.format(feature_name_long))\n    plt.xlabel(feature_name_long)\n    \n    ## Add another subplot for target ratio and feature \n    ### Create a temp table for visualization\n    temp_data=data.groupby(feature_name,as_index=False).agg({'target':['sum','count']})\n    temp_data['ratio']=temp_data['target']['sum']\/temp_data['target']['count']\n\n    ax2=fig.add_subplot(1,2,2)\n    sns.barplot(x=feature_name,y='ratio',data=temp_data,ax=ax2) \n    plt.ylim((0,1))\n    plt.title('Heart Disease Ratio by {}'.format(feature_name_long))\n    plt.xlabel(feature_name_long)\n    plt.ylabel('Heart Disease Ratio')\n    \ndef dataFrameTrim(dt,col,perc=0.75):\n    '''\n    This function replaces values in column in dataframe with perc. \n    '''\n    upper_limit=dt[col].quantile(q=perc)\n    dt[col][dt[col]>upper_limit]=upper_limit\n    data_frame_standardize(dt=dt,col=col)\n\ndef data_frame_standardize(dt,col):\n    '''\n    This function standardizes a column in a dataframe\n    '''\n    from sklearn.preprocessing import StandardScaler\n    data_Standardize=dt[col].values.reshape(-1,1)\n    dt[col]=StandardScaler(copy=False).fit_transform(data_Standardize)\n    \ndef impute_outliers(df,col_name,upper_limit,lower_limit):\n    '''\n    This function replace the values, which are outside of the lower and upper limit boundiries, with the lower and upper limit values.\n    The values between lower and upper limit boundiries stay the same.\n    '''\n    df[col_name][df[col_name]>upper_lmt]=upper_lmt\n    df[col_name][lower_lmt>df[col_name]]=lower_lmt","6f1c93fe":"data=pd.read_csv('..\/input\/heart.csv')","645fba2a":"print('Number of observations is {}.'.format(data.shape[0]))\nprint('Number of features is {}.'.format(data.shape[1]-1)) #one of them is target variable","4be9c314":"data.head()","bc0e3e42":"data.dtypes","7dd06ab6":"print('Number of features with missing values: {}'.format(data.isnull().any().sum()))","249dca22":"#Age\nNumeric_data_eda(feature_name='age',feature_name_long='Age',target='target')\n# looks like older people gets less heart disease","f028b301":"#extreme values\nupper_lmt=data['age'].mean()+data['age'].std()*2\nlower_lmt=data['age'].mean()-data['age'].std()*2\noutlierfilter=np.logical_or(data['age']>upper_lmt,(lower_lmt>data['age']))\n\nfig=plt.figure(figsize=(15,10))\nax1=fig.add_subplot(1,2,1)\n\nplt.plot(data['age'][outlierfilter],\n         linestyle='', \n         marker='o',\n         color='r',\n         alpha=0.8)\nplt.plot(data['age'][~outlierfilter],\n         linestyle='', \n         marker='o',\n         color='b',\n         alpha=0.1)\nplt.title('Age Distribution-Before Outlier Imputation')\nplt.ylabel('Age')\nplt.xlabel('Patients')\nplt.axhline(upper_lmt, color='r', linestyle='--')\nplt.axhline(lower_lmt, color='r', linestyle='--')\nplt.axhline(data['age'].mean(), color='b', linestyle='--')\n\n#impute outliers   \nprint(data['age'].describe())\nimpute_outliers(df=data,col_name='age',upper_limit=upper_lmt,lower_limit=lower_lmt)\nprint(data['age'].describe())\n\nax2=fig.add_subplot(1,2,2)\nplt.plot(data['age'][outlierfilter],\n         linestyle='', \n         marker='o',\n         color='r',\n         alpha=0.8)\nplt.plot(data['age'][~outlierfilter],\n         linestyle='', \n         marker='o',\n         color='b',\n         alpha=0.1)\nplt.title('Age Distribution-After Outlier Imputation')\nplt.ylabel('Age')\nplt.xlabel('Patients')\nplt.axhline(upper_lmt, color='r', linestyle='--')\nplt.axhline(lower_lmt, color='r', linestyle='--')\nplt.axhline(data['age'].mean(), color='b', linestyle='--')","b72d75b6":"#The Resting Blood Pressure\nNumeric_data_eda(feature_name='trestbps',feature_name_long='Resting Blood Pressure',target='target')\n# looks like there is no relationship between the resting blood pressure and heart disease","b8e02e1f":"#Cholestoral\nNumeric_data_eda(feature_name='chol',feature_name_long='Cholestoral',target='target')\n#looks like there is no relationship between the Cholestoral and the heart disease.\n#there are some observations with heartdisease and high cholestoral values","2ee974b6":"#thalach-maximum heart rate achieved\nNumeric_data_eda(feature_name='thalach',feature_name_long='Maximum Heart Rate Achieved',target='target')\n#Looks like there is a relationship between Maximum Heart Rate Achieved and heart disease.\n#Patients with higher Maximum Heart Rate Achieved are higher rate of heart disease.","c69093ea":"#oldpeak-ST depression induced by exercise relative to rest\nNumeric_data_eda(feature_name='oldpeak',feature_name_long='ST depression induced by exercise relative to rest',target='target')\n#Looks like there is a relationship between oldpeak and heart disease.\n#Patient with lower oldpeak values have higher portion of heart disease.!!","734230b8":"#sex-(1 = male; 0 = female)\ncategoric_data_eda(feature_name='sex',feature_name_long='Gender',target='target')","e6e157da":"#cp-chest pain type\ncategoric_data_eda(feature_name='cp',feature_name_long='Chest Pain Type',target='target')","70a551b5":"#fbs-(fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\ncategoric_data_eda(feature_name='fbs',feature_name_long='Fasting Blood Sugar Flag',target='target')","441c376a":"#restecg-resting electrocardiographic results\ncategoric_data_eda(feature_name='restecg',feature_name_long='Resting Electrocardiographic Results',target='target')","b873e04f":"#exang-exercise induced angina (1 = yes; 0 = no)\ncategoric_data_eda(feature_name='exang',feature_name_long='Exercise Induced Angina ',target='target')","8bd1b2b6":"#slope-the slope of the peak exercise ST segment\ncategoric_data_eda(feature_name='slope',feature_name_long='The Slope of The Peak Exercise ST Segment',target='target')","fa588091":"#ca-number of major vessels (0-3) colored by flourosopy\ncategoric_data_eda(feature_name='ca',feature_name_long='Number of Major Vessels (0-3) Colored by Flourosopy',target='target')","76b33e31":"#thal-3 = normal; 6 = fixed defect; 7 = reversable defect\ncategoric_data_eda(feature_name='thal',feature_name_long='Thal',target='target')","43457f29":"data.columns","663984e5":"sns.pairplot(data=data,vars=[ 'age', 'trestbps',  'chol'],hue='target');","06122198":"sns.pairplot(data=data,vars=['thalach', 'oldpeak',  'slope',  'ca'],hue='target');","276372c1":"#trim and standardize the numeric values\nnum_features=['age', 'trestbps',  'chol','thalach', 'oldpeak']\nfor feature in num_features: \n    dataFrameTrim(dt=data,col=feature,perc=0.9)","71d3a8d1":"#create dummy variables\ndata=pd.get_dummies(data=data,columns=['cp','restecg','slope','ca','thal'])","8b3a5663":"#create a decision tree model\n## split data as x and y\nX=data.drop(columns=['target']).values\ny=data['target'].values\nFeature_Names=data.drop(columns=['target']).columns.values\n##decision tree\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import recall_score,precision_score,accuracy_score\ndt1_cv_scores=[]\ndt1_precision=[]\ndt1_recall=[]\ndt1_accuracy=[]\nfor depth in range(2,20):\n    dt_1=DecisionTreeClassifier(max_depth=depth)#min_samples_split =0.05,min_samples_leaf=0.02)\n    dt_1_cv=cross_val_score(estimator=dt_1,X=X,y=y,cv=5)\n    dt1_cv_scores.append(dt_1_cv)\n    dt1_pred=dt_1.fit(X=X,y=y).predict(X=X)\n    dt1_precision.append(precision_score(y_pred=dt1_pred,y_true=y))\n    dt1_recall.append(recall_score(y_pred=dt1_pred,y_true=y))\n    dt1_accuracy.append(accuracy_score(y_pred=dt1_pred,y_true=y))\n#plot the results\nfig=plt.figure(figsize=(30,6))\nax1=fig.add_subplot(1,2,1)\nsns.boxplot(x=np.arange(2,20),y=dt1_cv_scores,ax=ax1)\nplt.title('Decision Tree Cross Validation Accuracy Results with Different Depth Parameters')\nplt.ylabel('Accuracy')\nplt.ylim(top=0.9,bottom=0.6)\nplt.xlabel('Decision Tree Depth Parameter')\n\nax2=fig.add_subplot(1,2,2)\nsns.lineplot(x=np.arange(2,20),y=[i.mean() for i in dt1_cv_scores])\nplt.title('Decision Tree Accuracy Results with Different Depth Parameters')\nplt.ylabel('Average Accuracy')\nplt.ylim(top=0.9,bottom=0.6)\nplt.xlabel('Decision Tree Depth Parameter')\nplt.xticks(ticks=np.arange(2,20),labels=np.arange(2,20));\n#according to the accuracy and accuracy mean plots, decision tree with max depth parameter 5 has better results\n\n#But before deciding the parameter, accuracy, precision and recall scores should be review together\nfig=plt.figure(figsize=(12,6))\nplt.figure(figsize=(15,6))\nplt.plot(dt1_precision,'r',label='Precision')\nplt.plot(dt1_precision,'ro')\nplt.plot(dt1_recall,'b',label='Recall')\nplt.plot(dt1_recall,'bo')\nplt.plot(dt1_accuracy,'g', alpha=0.5,label='Accuracy')\nplt.xticks(ticks=np.arange(0,20),labels=np.arange(2,20))\nplt.title('Decision Tree Model Results with Different Depth Parameters')\nplt.xlabel('Depth Parameter')\nplt.ylabel('Score')\nplt.legend();\n#looks like a decision tree with max depth parameter 6 is a better than a decision tree with max depth parameter 5.","543f9002":"#Create a decision tree with max depth parameter, 6.\nX=data.drop(columns=['target']).values\ny=data['target'].values\ndt=DecisionTreeClassifier(max_depth=6)\ndt.fit(X=X,y=y)\ndt_pred=dt.predict(X=X)\nfrom sklearn.metrics import classification_report, precision_recall_fscore_support\n\nprint('Classification Report-Decision Tree')\nprint(classification_report(y_true=y, y_pred=dt_pred))\n\nclassifier_report_decision_tree=precision_recall_fscore_support(y_true=y, y_pred=dt_pred) #0:precision, 1: recall","4ffca074":"#logistic regression\nfrom sklearn.linear_model import LogisticRegression\nlr_1=LogisticRegression()\nlr_1_cv=cross_val_score(estimator=lr_1,X=X,y=y,cv=5)\nprint('Logistic Regression Cross Validation Accuracy Score: {:,.2f}'.format(lr_1_cv.mean()))\nplt.figure(figsize=(12,4))\nsns.barplot(y=lr_1_cv,x=np.arange(1,6))\nplt.title('Logistic Regression Cross Validation Accuracy Score Distribution')\nplt.xlabel('Cross Validation Trials')\nplt.ylabel('Accuracy Score');\n\nlr=LogisticRegression()\nlr.fit(X=X,y=y)\nlr_pred=lr.predict(X=X)\nprint('Classification Report-Logistic Regression')\nprint(classification_report(y_true=y, y_pred=lr_pred))\n\nclassifier_report_logistic_regression=precision_recall_fscore_support(y_true=y, \n                                                                      y_pred=lr_pred) #0:precision, 1: recall","0bbfcdbb":"#knn\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nknn_array=[]\nfor n in range(2,20):\n    knn=KNeighborsClassifier(n_neighbors=n)\n    knn_1_cv=cross_val_score(estimator=knn,X=X,y=y,cv=5)\n    #print('{} neighbors:{:,.2f}'.format(n,knn_1_cv.mean()))\n    knn_array.append(knn_1_cv.mean())\n\nplt.figure(figsize=(18,6))\nplt.title('Cross Validation Trial Scores and Number of Neighbors')\nplt.ylim(top=1,bottom=0.5)\nsns.barplot(y=knn_array,\n            x=np.arange(2,20));\n#looks like the optimal neighbor parameter is 5 and 3 is another good option.","80d441bc":"#First try 5 as a neighbor parameter\nknn=KNeighborsClassifier(n_neighbors=5)\n\nknn.fit(X=X,y=y)\nknn_pred=knn.predict(X=X)\nprint('Classification Report-K-Neighbor Classifier')\nprint(classification_report(y_true=y, y_pred=knn_pred))\n\nclassifier_report_knn=precision_recall_fscore_support(y_true=y,y_pred=knn_pred) #0:precision, 1: recall","5ad36342":"#Now try a KNN classifier with neighbor parameter 3.\n#knn-with standardized variables\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler,RobustScaler\nknn_array=[]\n#standardize X \nrobustStandardize=RobustScaler(quantile_range=(10,90))\nrobustStandardize.fit(X)\nX_R_Std=robustStandardize.transform(X)\n\nknn=KNeighborsClassifier(n_neighbors=3)\n\nknn.fit(X=X_R_Std,y=y)\nknn_pred=knn.predict(X=X_R_Std)\nprint('Classification Report-K-Neighbour Classifier')\nprint(classification_report(y_true=y, y_pred=knn_pred))\n\nclassifier_report_knn=precision_recall_fscore_support(y_true=y,y_pred=knn_pred) #0:precision, 1: recall","fed0c28f":"#Visualization of the features and their effect on target variable\nFeature_Names=data.drop(columns=['target']).columns.values\nplt.figure(figsize=(16,12))\nplt.barh(y=Feature_Names,width=dt.feature_importances_)\nplt.title('Decision Tree-Feature Importance Distribution')\nplt.ylabel('Feature Importance');","1d6929fe":"**Preparing The Data for Predictive Modelling**","dd32d940":"Cholestoral","ce7a33ca":"Triming and standardizing the data","20887419":"Logistic Regression","1de781d6":"In this study, I analyzed \"the Heart Disease\" data and tried to find an algorithm that detects the patients with heart disease.\n\n\nOur data set has thirteen features. \n\n\nThese features can be grouped as numeric and categoric variables:\n    * Numeric variables:\n        * Age\n        * Serum cholestoral in mg\/dl\n        * Maximum heart rate achieved\n        * Resting blood pressure\n        * Old peak-ST depression induced by exercise relative to rest \n    * Categoric variables:\n        * Sex\n        * Chest pain type\n        * Fasting blood suger>120 mg\/dl\n        * Exercise induced angina\n        * Resting electrocardiographic results\n        * The slope of the peak exercise ST segment\n        * Number of major vessels (0-3) colored by flourosopy\n        * Thal\nIn this analyze, the following steps are performed:\n    * Warm up\n        * Reading the data\n        * Calculating number of observations and features\n        * Browsing a sample of data\n        * Listing the data types\n        * Checking the missing values\n    * EDA\n        * Numeric features\n            * Checking extreme values only for a feature\n        * Categoric features\n\t* Pair Plots\n    * Prediction\n        * Preparing the data\n            *Triming and standardizing data\n            *Creating dummy variables\n        * Searching for a good classifier\n            * Decision tree\n            * Logistic regression\n            * K-nearest neighbours\n    * Conclusion\n","4db4d7d0":"Import the necessary libraries","09da3942":"Maximum Heart Rate Achieved","c72170b5":"After compering the results of these two KNN algorithms, I chose the one with better results.","005579f1":"Checking the missing values","51841122":"Thal","6198e6dd":"**Numeric Features**","17ec82c3":"Reading the data","dac2d69a":"First X for independent variables and y for dependent variable were created. \n\nI tried to find a model for y by using X,which means detecting the patients with heart disease.","ef941479":"Resting Electrocardiographic Results","0a431bc8":"Creating Dummy Variables","56fdb22d":"Oldpeak-ST Depression Induced by Exercise Relative to Rest","546e49f9":"Exercise Induced Angina","7ee92a09":"**------------------------------------Prediction------------------------------------**","c2a7656a":"KNN with neighbor parameter 3.","bb8f53ec":"Browsing on a sample of data","2ed500d1":"Age-Checking extreme values","abff932f":"Calculating number of observations and features","a62681a7":"The Resting Blood Pressure","6138e8a0":"Visualizing the feature importance.","a0752222":"Decision Tree","89f385be":"Number of Major Vessels (0-3) Colored by Flourosopy","746f8992":"**Searching for a Good Classifier**","a926ae66":"Listing the data types","cd435ccb":"**---------------------------Explanatory Data Analysis---------------------------**","8e78ebda":"User defined functions","5faf651b":"After trying different depth parameter, the decision tree with max depth parameter 6 performed better than the other decision trees.","edb4b5f8":"**Categoric Features**","15f29c34":"**Conclusion**\n\nIn this analyze, I used \"the heart disease\" data and tried to find the signals of disease and created a classsifier to detect the patients who have problems with their hearts.\n\n\nAfter making EDA and having clues about the features and their effects on the disease, I tried to find a successfull classifier for disease detection. \n\n\nDecision tree, logistic regression and K-nearest neighbor algorithms were tested with various parameters.\n\n\nK-nearest neighbor algorithm had the best results but other algorithms were also nearly as successful as the K-nearest neighbors. \n\nAs a disadvantege of the K-nearest neighbor algorithm, features and their effects on the target variable can not be measured. \n\nI used the decision tree algoritm feature importances and visualize them and finalize the analyze.\n\nThank you.","5b1d4c07":"Age","5ab5a5bf":"**--------------------------------------Pair Plots-----------------------------------**","7bb7c0f4":"Chest Pain Type","e50957ed":"Fasting Blood Sugar","7736046a":"K-Nearest Neighbors","9ea42427":"Sex","005b6e38":"The graph above shows that KNN algorithm with 3 and 5 might be good choices for neighbor parameter.\n\nI created 2 KNN classifier with different parameters and compared the results.","44ce51de":"The Slope of The Peak Exercise ST Segment","7c4c48c8":"KNN with neighbor parameter 5."}}