{"cell_type":{"ba1cc55a":"code","8e763cd8":"code","2cd275d8":"code","08c9f6b0":"code","8a5a9046":"code","95251f13":"code","d09ab4c2":"code","ce400f51":"code","d87c6760":"code","4a76bc55":"code","b4497a27":"code","dcc25478":"code","1614a294":"code","1e36e062":"code","16cd795d":"code","8e7e948b":"code","93039508":"code","69bd0c5e":"code","04d78cbb":"code","8ddd3bbc":"markdown","b1a98c0f":"markdown","dc7e423b":"markdown","d77ca847":"markdown","b6bb0672":"markdown","95d48dfa":"markdown","27d85d52":"markdown","6409cfda":"markdown","d66d805d":"markdown","84d741cb":"markdown","6ef73ab9":"markdown","d9697b9f":"markdown","d22e0c08":"markdown","4069e4a0":"markdown","f60957e5":"markdown","5f8b56f3":"markdown"},"source":{"ba1cc55a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import to_categorical\n\ntf.random.set_seed(42)","8e763cd8":"# Derived from BRU ERU by Bhumbra (2018)\ndef make_bru_eru(r=1.0):\n  def bru_eru(z): # n.b. tf.where()s for z per the warning below\n    return tf.where(tf.greater_equal(z, 0.0),\n                    tf.subtract(tf.pow(tf.add(\n                        tf.multiply(r**2,\n                                    tf.where(z < 0.0, tf.constant(0.0), z)\n                                    ), 1.0), 1.0\/r), 1.0\/r),\n                    tf.subtract(tf.exp(\n                        tf.multiply(r, \n                                    tf.where(z < 0.0, z, tf.constant(-1.0))\n                                    )), 1.0\/r))\n  return bru_eru","2cd275d8":"# Derived from BRU ORU by Bhumbra (2018)\ndef make_bru_oru(r=1.0):\n\n  # sgn function appears to be perturbing auto-differentiator \n  # at z=0 so providing custom gradient function \n  # https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/custom_gradient\n\n  @tf.custom_gradient\n  def bru_oru(z):\n    def grad(dy):\n      return tf.multiply(dy, tf.multiply(r, tf.pow(tf.add(\n          tf.multiply(r**2, tf.abs(z)), 1.0), (1-r)\/r)))\n    return tf.multiply(tf.sign(z), tf.subtract(tf.pow(tf.add(\n        tf.multiply(r**2, tf.abs(z)), 1.0), 1.0\/r), 1.0)), grad\n  return bru_oru","08c9f6b0":"# Warning: hazardous issue with tf.where() and gradients described below. It is\n# necessary that all branches of the tf.where() be safe from NaNs to prevent\n# overall NaN result. \n#\n#     https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/where\n#\n# \"Note that if the gradient of either branch of the tf.where generates a NaN, \n# then the gradient of the entire tf.where will be NaN. A workaround is to use\n# an inner tf.where to ensure the function has no asymptote, and to avoid \n# computing a value whose gradient is NaN by replacing dangerous inputs with \n# safe inputs.\"","8a5a9046":"r = 2.0\nz = tf.Variable([-1.0, 0.0, 1.0])\n\nassert np.allclose(make_bru_eru(r)(z).numpy(), [-0.364665, 0.5, 1.73607])\n\nassert np.allclose(make_bru_oru(r)(z).numpy(), [-1.23607, 0.0, 1.23607])","95251f13":"r = 3.0\nz = tf.Variable([-3.0, 0.0, 3.0])\n\nwith tf.GradientTape() as tape:\n  y = make_bru_eru(r)(z)\nassert np.allclose(tape.gradient(y, z).numpy(), [0.000370229, 3.0, 0.325349])\n\nwith tf.GradientTape() as tape:\n  y = make_bru_oru(r)(z)\nassert np.allclose(tape.gradient(y, z).numpy(), [0.325349, 3.0, 0.325349])","d09ab4c2":"x = np.linspace(-5, 5, 100)\nplt.figure(figsize=(6, 4))\nplt.plot(x, make_bru_eru(1.0)(x), label='E1RU', color='r')\nplt.plot(x, make_bru_eru(2.0)(x), label='E2RU', color='b')\nplt.plot(x, make_bru_eru(3.0)(x), label='E3RU', color='k')\nplt.legend(loc=\"lower right\");","ce400f51":"x = np.linspace(-5, 5, 100)\nplt.figure(figsize=(6, 4))\nplt.plot(x, make_bru_oru(1.0)(x), label='O1RU', color='r')\nplt.plot(x, make_bru_oru(2.0)(x), label='O2RU', color='b')\nplt.plot(x, make_bru_oru(3.0)(x), label='O3RU', color='k')\nplt.legend(loc=\"lower right\");","d87c6760":"npl = 128 # nodes per layer\n\n# sigmoid\nsigmoid_model = models.Sequential()\nsigmoid_model.add(layers.Dense(npl, activation='sigmoid', \n                               input_shape=(28**2,)))\nsigmoid_model.add(layers.Dense(npl, activation='sigmoid'))\nsigmoid_model.add(layers.Dense(npl, activation='sigmoid'))\nsigmoid_model.add(layers.Dense(npl, activation='sigmoid'))\nsigmoid_model.add(layers.Dense(10, activation='softmax'))\n\n# tanh\ntanh_model = models.Sequential()\ntanh_model.add(layers.Dense(npl, activation='tanh', \n                            input_shape=(28**2,)))\ntanh_model.add(layers.Dense(npl, activation='tanh'))\ntanh_model.add(layers.Dense(npl, activation='tanh'))\ntanh_model.add(layers.Dense(npl, activation='tanh'))\ntanh_model.add(layers.Dense(10, activation='softmax'))\n\n# relu\nrelu_model = models.Sequential()\nrelu_model.add(layers.Dense(npl, activation='relu', \n                            input_shape=(28**2,)))\nrelu_model.add(layers.Dense(npl, activation='relu'))\nrelu_model.add(layers.Dense(npl, activation='relu'))\nrelu_model.add(layers.Dense(npl, activation='relu'))\nrelu_model.add(layers.Dense(10, activation='softmax'))\n\n# bru - configuration inspired by Bhumbra's article\nbru_model = models.Sequential()\nbru_model.add(layers.Dense(npl, activation=make_bru_oru(3.0), \n                           input_shape=(28**2,)))\nbru_model.add(layers.Dense(npl, activation=make_bru_oru(2.0)))\nbru_model.add(layers.Dense(npl, activation=make_bru_oru(2.0)))\nbru_model.add(layers.Dense(npl, activation=make_bru_eru(2.0)))\nbru_model.add(layers.Dense(10, activation='softmax'))","4a76bc55":"compile_params = dict(optimizer='sgd',\n                      loss='categorical_crossentropy',\n                      metrics=['accuracy'])\n\nsigmoid_model.compile(**compile_params)\ntanh_model.compile(**compile_params)\nrelu_model.compile(**compile_params)\nbru_model.compile(**compile_params)","b4497a27":"(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n\ntrain_images =  train_images.reshape((60000, 28**2))\ntrain_images = train_images \/ 255.0\ntest_images =  test_images.reshape((10000, 28**2))\ntest_images = test_images \/ 255.0\n\ntrain_labels = to_categorical(train_labels)\ntest_labels = to_categorical(test_labels)","dcc25478":"fit_params = dict(x=train_images,\n                  y=train_labels, \n                  epochs=5, batch_size=64,\n                  verbose=0)\n\n#print('--sigmoid')\nsigmoid_model_hist = sigmoid_model.fit(**fit_params)\n#print('--tanh')\ntanh_model_hist = tanh_model.fit(**fit_params)\n#print('--relu')\nrelu_model_hist = relu_model.fit(**fit_params)\n#print('--bru')\nbru_model_hist = bru_model.fit(**fit_params);","1614a294":"loss_df = pd.DataFrame(index=range(1, fit_params['epochs']+1))\naccuracy_df = pd.DataFrame(index=range(1, fit_params['epochs']+1))\n\nloss_df['sigmoid'] = sigmoid_model_hist.history['loss']\naccuracy_df['sigmoid'] = sigmoid_model_hist.history['accuracy']\n\nloss_df['tanh'] = tanh_model_hist.history['loss']\naccuracy_df['tanh'] = tanh_model_hist.history['accuracy']\n\nloss_df['relu'] = relu_model_hist.history['loss']\naccuracy_df['relu'] = relu_model_hist.history['accuracy']\n\nloss_df['bru'] = bru_model_hist.history['loss']\naccuracy_df['bru'] = bru_model_hist.history['accuracy']","1e36e062":"loss_df.plot(title='Training Loss', figsize=(6, 4));","16cd795d":"accuracy_df.plot(title='Training Accuracy', figsize=(6, 4));","8e7e948b":"evaluate_params = dict(x=test_images,\n                       y=test_labels,\n                       verbose=0)\n\neval_df = pd.DataFrame(columns=['Activation', 'Loss', 'Accuracy'])\neval_df.set_index('Activation', inplace=True)\neval_df.loc['sigmoid', :] = sigmoid_model.evaluate(**evaluate_params)\neval_df.loc['tanh', :] = tanh_model.evaluate(**evaluate_params)\neval_df.loc['relu', :] = relu_model.evaluate(**evaluate_params)\neval_df.loc['bru', :] = bru_model.evaluate(**evaluate_params)\n\neval_df","93039508":"eval_df.plot(title='Test Data Evaluations', figsize=(6, 4));","69bd0c5e":"axes = eval_df.loc[['tanh', 'relu', 'bru'], 'Loss'].plot.line(\n    title='Selected Test Loss Evaluations', figsize=(6, 4));","04d78cbb":"eval_df.loc[['tanh', 'relu', 'bru'], 'Accuracy'].plot.line(\n    title='Selected Test Accuracy Evaluations', figsize=(6, 4));","8ddd3bbc":"## Test differentiation with hand-calculated values ","b1a98c0f":"# Optimize With SGD Instead Of ReLU","dc7e423b":"## ORU activations","d77ca847":"## ERU activations","b6bb0672":"# Visualize Training Results","95d48dfa":"# Load And Normalize MNIST Image Set","27d85d52":"# Imports","6409cfda":"# Build Comparison Models","d66d805d":"# Unit Tests","84d741cb":"# Collect Histories","6ef73ab9":"# Custom Activation Function Builders","d9697b9f":"## Test functions with hand-calculated values","d22e0c08":"TensorFlow custom activation function experiment based upon:\n\n* [Deep learning improved by biological activation functions (arXiv.org)](https:\/\/arxiv.org\/abs\/1804.11237v2) from Gardave S. Bhumbra\n* [Custom Activation and Loss Functions in Keras and TensorFlow with Automatic Differentiation (youtube.com)](https:\/\/www.youtube.com\/watch?v=4eIlvlP-8wk) from Jeff Heaton\n","4069e4a0":"# Evaluate Test Data","f60957e5":"# Fit Comparison Models On MNIST Image Set","5f8b56f3":"# Function Visualizations"}}