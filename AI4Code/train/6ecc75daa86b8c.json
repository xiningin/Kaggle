{"cell_type":{"6aee84cc":"code","569b8071":"code","a61a1731":"code","c7b03db3":"code","d73855ec":"code","c79233fc":"code","0dcd69b3":"code","5b3710e5":"code","894f53ce":"code","1b28e1f8":"code","acc1cdc0":"code","67391195":"code","83fee9c0":"code","b8ceb0e3":"code","26f20124":"code","040023b5":"code","841c245f":"code","fc4361e0":"code","28c3624b":"code","ee9acb5b":"code","446924cb":"code","193976f3":"code","d80cfebd":"code","08f1ee56":"code","8f99b943":"code","842f471a":"code","d1a382e1":"code","5e8d9a16":"code","700e1d4e":"code","abbefa10":"code","0c8f75a1":"code","008e7a32":"code","480a2b21":"code","39700bd8":"code","147438d5":"code","710e81c8":"code","f0fdb3a7":"markdown","28c1c103":"markdown","7a8cc478":"markdown","85b6a216":"markdown","72529262":"markdown","ecedb114":"markdown","3a566102":"markdown","998f1223":"markdown","6ceda7c6":"markdown","0eeacd7b":"markdown","182426b1":"markdown","7907eeeb":"markdown","6860120e":"markdown","be9d26d4":"markdown","a7270623":"markdown","f4af27a0":"markdown","768d3e99":"markdown","b68714bf":"markdown","85831ac7":"markdown","60984b4c":"markdown"},"source":{"6aee84cc":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression","569b8071":"house_prices= pd.read_csv(\"\/kaggle\/input\/housing-prices\/housing_prices.csv\")","a61a1731":"house_prices","c7b03db3":"house_prices.info()","d73855ec":"plt.figure(figsize=(10,8))\nsns.heatmap(house_prices.corr(), cmap=\"RdBu\")\nplt.title(\"Correlations Between Features\", size=15)\nplt.show()","c79233fc":"X = house_prices.drop(columns=['SalePrice'])\ny = house_prices['SalePrice']","0dcd69b3":"plt.figure(figsize=(15, 8))\nplt.subplot(2, 2, 1)\nsns.scatterplot(data=house_prices, x='GarageCars', y=y)\nplt.subplot(2, 2, 2)\nsns.scatterplot(data=house_prices, x='TotalBsmtSF', y=y)\nplt.subplot(2, 2, 3)\nsns.scatterplot(data=house_prices, x='OverallCond', y=y)\nplt.subplot(2, 2, 4)\nsns.scatterplot(data=house_prices, x='GrLivArea', y=y)\n","5b3710e5":"plt.figure(figsize=(15, 8))\nplt.subplot(2, 2, 1)\nsns.scatterplot(data=house_prices, x='1stFlrSF', y=y)\nplt.subplot(2, 2, 2)\nsns.scatterplot(data=house_prices, x='FullBath', y=y)\nplt.subplot(2, 2, 3)\nsns.scatterplot(data=house_prices, x='TotRmsAbvGrd', y=y)\n","894f53ce":"\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8, random_state=8)","1b28e1f8":"lm = LinearRegression()\nlm.fit(np.array(X_train[['GarageCars','GrLivArea','TotalBsmtSF','OverallCond','1stFlrSF','FullBath','TotRmsAbvGrd']]), y_train)\n","acc1cdc0":"def plot_predictions(y_true, y_pred): \n    max_preds = min([max(y_pred.tolist()), max(y_true.tolist())])\n    min_preds = min([min(y_pred.tolist()), min(y_true.tolist())])\n    plt.figure(figsize=(8,8))\n    sns.scatterplot(x=y_pred, y=y_true)\n    sns.lineplot(x=[min_preds,max_preds], y=[min_preds, max_preds], color='red')\n    plt.ylabel('Reference')\n    plt.xlabel('Predictions')\n    plt.show()","67391195":"plot_predictions(y_train, lm.predict(np.array(X_train[['GarageCars','GrLivArea','TotalBsmtSF','OverallCond','1stFlrSF','FullBath','TotRmsAbvGrd']])))","83fee9c0":"results = pd.DataFrame({\n    'predictions':lm.predict(np.array(X_test[['GarageCars','GrLivArea','TotalBsmtSF','OverallCond','1stFlrSF','FullBath','TotRmsAbvGrd']])), \n    'true_values':y_test\n})\nresults.head()","b8ceb0e3":"plot_predictions(results.true_values, results.predictions)","26f20124":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error","040023b5":"mean_squared_error(results.true_values, results.predictions)","841c245f":"mean_squared_error(results.true_values, results.predictions)**0.5","fc4361e0":"mean_absolute_error(results.true_values, results.predictions)","28c3624b":"def plot_predictions(y_true, y_pred): \n    print(\n        f\"\"\"\n        MSE: {mean_squared_error(y_true, y_pred)}\n        RMSE: {mean_squared_error(y_true, y_pred)**0.5}\n        MAE: {mean_absolute_error(y_true, y_pred)}\n        \"\"\"\n    )\n    max_preds = min([max(y_pred.tolist()), max(y_true.tolist())])\n    min_preds = max([min(y_pred.tolist()), min(y_true.tolist())])\n    print(max_preds, min_preds)\n    plt.figure(figsize=(8,8))\n    sns.scatterplot(x=y_pred, y=y_true)\n    sns.lineplot(x=[min_preds,max_preds], y=[min_preds, max_preds], color='red')\n    plt.ylabel('Reference')\n    plt.xlabel('Predictions')\n    plt.show()\n\nplot_predictions(y_test, results['predictions'])","ee9acb5b":"error_distribution =results.assign(error = lambda x: x['predictions'] - x['true_values'])\nerror_distribution","446924cb":"plt.subplots(figsize=(12, 8))\nsns.histplot(error_distribution['error'])\nplt.vlines(x=0, ymin=0, ymax=60, color='red')\nplt.xlim(-200000, 150000)\nplt.show()","193976f3":"err_distr = error_distribution.sort_values(by='true_values')\n\nplt.subplots(figsize=(12, 8))\nsns.scatterplot(data=err_distr, x='true_values', y='error')\nplt.hlines(y=0, xmin=0, xmax=max(error_distribution['true_values']), color='red')\nplt.show()","d80cfebd":"num_col = ['GarageCars','GrLivArea','TotalBsmtSF','OverallCond','1stFlrSF','FullBath','TotRmsAbvGrd']","08f1ee56":"numeric_pipeline = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='median')),\n    ('scale', MinMaxScaler())])\nfull_processor = ColumnTransformer(transformers=[\n    ('number', numeric_pipeline, num_col), \n])\n\nlm_pipeline = Pipeline(steps=[\n    ('processor', full_processor), \n    ('model', LinearRegression())\n])\n\n_ = lm_pipeline.fit(X_train, y_train)\nlm_pipeline","8f99b943":"def plot_predictions(y_true, y_pred): \n    print(\n        f\"\"\"\n        MSE: {mean_squared_error(y_true, y_pred)}\n        RMSE: {mean_squared_error(y_true, y_pred)**0.5}\n        MAE: {mean_absolute_error(y_true, y_pred)}\n        \"\"\"\n    )\n    max_preds = min([max(y_pred.tolist()), max(y_true.tolist())])\n    min_preds = max([min(y_pred.tolist()), min(y_true.tolist())])\n    print(max_preds, min_preds)\n    plt.figure(figsize=(8,8))\n    sns.scatterplot(x=y_pred, y=y_true)\n    sns.lineplot(x=[min_preds,max_preds], y=[min_preds, max_preds], color='red')\n    plt.ylabel('Reference')\n    plt.xlabel('Predictions')\n    plt.show()\n    \n    errors = y_pred - y_true\n    plt.subplots(figsize=(12, 8))\n    sns.histplot(errors)\n    plt.vlines(x=0, ymin=0, ymax=600, color='red')\n    plt.show()\n\n    p_df = (\n        pd.DataFrame({'y_true':y_true, 'y_pred':y_pred})\n        .assign(error = lambda x: x['y_pred'] - x['y_true'])\n        .sort_values(by='y_true')\n        )\n\n    plt.subplots(figsize=(12, 8))\n    sns.scatterplot(data=p_df, x='y_true', y='error')\n    plt.hlines(y=0, xmin=0, xmax=max(p_df['y_true']), color='red')\n    plt.show()\n","842f471a":"plot_predictions(y_train, lm_pipeline.predict(X_train))","d1a382e1":"train = pd.concat([X_train, y_train], axis=1)\nplt.subplots(figsize=(12, 9))\nsns.heatmap(train.corr(), vmin=-1, vmax=1, cmap='RdGy')\nplt.show()","5e8d9a16":"g = sns.PairGrid(train[['GarageCars','GrLivArea','TotalBsmtSF','OverallCond','1stFlrSF','FullBath','TotRmsAbvGrd','SalePrice']])\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\ng.add_legend()","700e1d4e":"num_col= list(X_train.select_dtypes(include=['int64','float64']))\ncat_col=list(X_train.select_dtypes(include=['object']))","abbefa10":"numeric_pipeline = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='median')),\n    ('scale', MinMaxScaler())])\n\ncategorical_pipeline = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('ordinal',OneHotEncoder())\n])\nfull_processor = ColumnTransformer(transformers=[\n    ('number', numeric_pipeline, num_col), \n    ('category', categorical_pipeline, cat_col)\n])\n\nlm_pipeline_all_features = Pipeline(steps=[\n    ('processor', full_processor), \n    ('model', LinearRegression())\n])\n\n_ = lm_pipeline_all_features.fit(X_train, y_train)\nlm_pipeline_all_features ","0c8f75a1":"def plot_predictions(y_true, y_pred): \n    print(\n        f\"\"\"\n        MSE: {mean_squared_error(y_true, y_pred)}\n        RMSE: {mean_squared_error(y_true, y_pred)**0.5}\n        MAE: {mean_absolute_error(y_true, y_pred)}\n        \"\"\"\n    )\n    max_preds = min([max(y_pred.tolist()), max(y_true.tolist())])\n    min_preds = max([min(y_pred.tolist()), min(y_true.tolist())])\n    print(max_preds, min_preds)\n    plt.figure(figsize=(8,8))\n    sns.scatterplot(x=y_pred, y=y_true)\n    sns.lineplot(x=[min_preds,max_preds], y=[min_preds, max_preds], color='red')\n    plt.ylabel('Reference')\n    plt.xlabel('Predictions')\n    plt.show()\n    \n    errors = y_pred - y_true\n    plt.subplots(figsize=(12, 8))\n    sns.histplot(errors)\n    plt.vlines(x=0, ymin=0, ymax=600, color='red')\n    plt.show()\n\n    p_df = (\n        pd.DataFrame({'y_true':y_true, 'y_pred':y_pred})\n        .assign(error = lambda x: x['y_pred'] - x['y_true'])\n        .sort_values(by='y_true')\n        )\n\n    plt.subplots(figsize=(12, 8))\n    sns.scatterplot(data=p_df, x='y_true', y='error')\n    plt.hlines(y=0, xmin=0, xmax=max(p_df['y_true']), color='red')\n    plt.show()\n\n    \n    \n   \n    \nplot_predictions(y_train, lm_pipeline_all_features .predict(X_train))","008e7a32":"results_all_features = pd.DataFrame({\n    'predictions':lm_pipeline_all_features.predict(X_test),\n    'true_values':y_test\n})\nresults_all_features.head()","480a2b21":"from sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline\n \n","39700bd8":"pca = PCA(n_components=80)\nnum_col= list(X_train.select_dtypes(include=['int64','float64']))\ncat_col=list(X_train.select_dtypes(include=['object']))","147438d5":"\nnumeric_pipeline = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='median')),\n    ('scale', MinMaxScaler())])\n\ncategorical_pipeline = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('ordinal',OneHotEncoder())\n])\nfull_processor = ColumnTransformer(transformers=[\n    ('number', numeric_pipeline, num_col), \n    ('category', categorical_pipeline, cat_col)\n])\n\n\npipe = make_pipeline(\n    ('processor', full_processor), \n    ('model', LinearRegression())\n   \n)\n \n\n","710e81c8":"pipe.fit(X_train, y_train)","f0fdb3a7":"Visualize the results in a dataframe for the test","28c1c103":"We have around 80 features, let's explore the correlation between them.","7a8cc478":"Split the dataset ","85b6a216":"Applying the PCA","72529262":"adjust the function prediction","ecedb114":"Import the libraries we need","3a566102":"![Screenshot 2022-01-05 at 14.23.51.png](attachment:2d9621b7-f5ed-4ef5-b4bd-4ab85fff34b5.png)\n","998f1223":"Define a function to plot the predictions","6ceda7c6":"analyzing the correlation metric in the train dataset","0eeacd7b":"Create the linear model","182426b1":"Import the dataset","7907eeeb":"if we compare the two predictions are quite similar","6860120e":"About the dataset","be9d26d4":"analyze the errors:\n* mean squared error\n* Root mean squared error\n* mean absolute error","a7270623":"**House prices prediction with Linear Regression**","f4af27a0":"But what happens if we keep all the features?","768d3e99":"create a pipeline to automate the process","b68714bf":"Analyzing in the raw dataset,the last row 'SalePrice' we can see we have some features that have collinearity in order to predict the house price like **OverallQual,GrLivArea,Garagecars** etc but for now we will keep all the features.","85831ac7":"analyze the error distribution","60984b4c":"complete function"}}