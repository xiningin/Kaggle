{"cell_type":{"cc5b2712":"code","973fee48":"code","b22da353":"code","f3cf55f4":"code","4c95fd7e":"code","267495c3":"code","0ed6862d":"code","349825c0":"code","6528a8d1":"code","44e82c88":"code","16c8b980":"code","9366a086":"code","9c846929":"code","242947d0":"code","304ff09c":"code","15f75e19":"code","a6355e5b":"code","ba5fcb6a":"code","b6943197":"code","2265c9e6":"code","1573824b":"code","001151e1":"code","67270346":"code","9c119d19":"code","ed63b576":"code","aa9d0d78":"code","369faf5e":"code","a3042913":"markdown","cbbacc1d":"markdown","b8b9ff8a":"markdown","21a95989":"markdown","192f960b":"markdown","e475dcf2":"markdown"},"source":{"cc5b2712":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","973fee48":"\ndf = pd.read_csv('..\/input\/behavioral-risk-factor-surveillance-system\/2015.csv').sample(10000, random_state = 50)\ndf.head()","b22da353":"df['_RFHLTH'].value_counts()","f3cf55f4":"df['_RFHLTH'] = df['_RFHLTH'].replace({2: 0})","4c95fd7e":"df['_RFHLTH'].value_counts()","267495c3":"df = df.loc[df['_RFHLTH'].isin([0, 1])].copy()","0ed6862d":"df['_RFHLTH'].value_counts()","349825c0":"df = df.rename(columns = {'_RFHLTH': 'Label'})","6528a8d1":"df.shape","44e82c88":"percentOfData = df.count()*100\/9980","16c8b980":"percentOfData.where(percentOfData<50).dropna()","9366a086":"badFeatures = percentOfData.where(percentOfData<50).dropna()","9c846929":"# Remove columns with missing values\ndf = df.drop(columns = badFeatures.index.to_list())","242947d0":"# Remove all non float data\ndf = df.select_dtypes(include=['float64'])","304ff09c":"#Removing few more columns\ndf = df.drop(columns=['SEX','_STATE','FMONTH','SEQNO','DISPCODE','MARITAL','EDUCA','POORHLTH', 'PHYSHLTH', 'GENHLTH', 'HLTHPLN1', 'MENTHLTH'])","15f75e19":"from IPython.display import HTML\nHTML(pd.DataFrame(df.dtypes).to_html())","a6355e5b":"df.head()","ba5fcb6a":"from sklearn.model_selection import train_test_split\n\n# Extract the labels\n#labels = np.array(df.pop('Label'))\n\n# 30% examples in test data\ntrain, test, train_labels, test_labels = train_test_split(df, df['Label'], test_size = 0.3, \n                                                          random_state = 50)","b6943197":"# Imputation of missing values\ntrain = train.fillna(train.mean())\ntest = test.fillna(test.mean())","2265c9e6":"train.columns","1573824b":"sns.distplot(train['Label'], kde=False)","001151e1":"train.shape","67270346":"test.shape","9c119d19":"# Train tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(random_state=50, max_depth=60)\ntree.fit(train, train_labels)\nprint(f'Decision tree has {tree.tree_.node_count} nodes with maximum depth {tree.tree_.max_depth}.')","ed63b576":"# Make probability predictions\ntrain_probs = tree.predict_proba(train)[:, 1]\nprobs = tree.predict_proba(test)[:, 1]\n\ntrain_predictions = tree.predict(train)\npredictions = tree.predict(test)","aa9d0d78":"from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve\n\nprint(f'Train ROC AUC Score: {roc_auc_score(train_labels, train_probs)}')\nprint(f'Test ROC AUC  Score: {roc_auc_score(test_labels, probs)}')","369faf5e":"print(f'Baseline ROC AUC: {roc_auc_score(test_labels, [1 for _ in range(len(test_labels))])}')","a3042913":"We won't do any data exploration in this notebook, but in general, exploring the data is a best practice. This can help you for feature engineering (which we also won't do here) or by identifying and correcting anomalies \/ mistakes in the data.","cbbacc1d":"TODO: construct ROC curve and confusion matrix","b8b9ff8a":"### Assess Decision Tree Performance\n\nGiven the number of nodes in our decision tree and the maximum depth, we expect it has overfit to the training data. This means it will do much better on the training data than on the testing data.","21a95989":"The problem we\u2019ll solve is a binary classification task with the goal of predicting an individual\u2019s health. The features are socioeconomic and lifestyle characteristics of individuals and the label is 0 for poor health and 1 for good health. This dataset was collected by the Centers for Disease Control and Prevention ","192f960b":"Some tutorials on Random Forests:\nhttps:\/\/www.youtube.com\/watch?v=J4Wdy0Wc_xQ\nhttps:\/\/towardsdatascience.com\/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76","e475dcf2":"Our model does outperform a baseline guess, but we can see it has severely overfit to the training data, acheiving perfect ROC AUC."}}