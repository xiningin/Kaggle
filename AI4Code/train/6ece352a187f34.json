{"cell_type":{"8e66b881":"code","0af50808":"code","0d322db6":"code","8cc67e58":"code","4b646182":"code","59336d7d":"code","f666d23a":"code","88dd3f75":"code","992a121c":"code","987888c8":"code","0ae5a4fc":"code","9484f2ad":"code","bb1e2b06":"code","6c49c259":"code","6273d869":"code","66d15de7":"code","672aad2f":"code","02d3a504":"code","2e9beccf":"code","74932ead":"code","42e5c368":"code","65339e78":"code","8fd92254":"code","68423520":"code","c51f01d1":"code","2016b3c0":"code","2373ceec":"code","58c71fe6":"markdown","49f0e570":"markdown","9bb6ef0a":"markdown","c52fe421":"markdown","c8b76107":"markdown","784a3ab1":"markdown","d30acc88":"markdown","6e165cf9":"markdown","d69b50b5":"markdown","70779b12":"markdown","c2f5d8ce":"markdown","97ace831":"markdown","134820df":"markdown","33943e4b":"markdown","a901803b":"markdown","b29d44fc":"markdown","5ebec392":"markdown","218e005a":"markdown","c3a4d884":"markdown","ba841574":"markdown","06131f4e":"markdown","350bd3c2":"markdown","5a0b7f72":"markdown","96706479":"markdown","acbd080d":"markdown","4d71833b":"markdown"},"source":{"8e66b881":"!pip uninstall --y typing","0af50808":"!pip install nvtx dask_cuda","0d322db6":"!pip install git+https:\/\/github.com\/NVIDIA\/NVTabular.git@4c92dffac4354d816178264bcfcdec722db2ec1c","8cc67e58":"import os\nimport rmm\nimport cudf\nfrom cudf.io.parquet import ParquetWriter\nfrom fsspec.core import get_fs_token_paths\nimport numpy as np\nimport pyarrow.parquet as pq\n\nfrom dask.dataframe.io.parquet.utils import _analyze_paths\nfrom dask.base import tokenize\nfrom dask.utils import natural_sort_key\nfrom dask.highlevelgraph import HighLevelGraph\nfrom dask.delayed import Delayed\nfrom dask.distributed import Client\nfrom dask_cuda import LocalCUDACluster\n\nimport nvtabular as nvt\nfrom nvtabular.utils import device_mem_size, get_rmm_size","4b646182":"INPUT_PATH = os.environ.get('INPUT_PATH', '\/kaggle\/input\/criteo-dataset\/dac')\nOUTPUT_PATH = os.environ.get('OUTPUT_PATH', '\/raid\/criteo\/tests\/demo_out')\nCUDA_VISIBLE_DEVICES = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"0\")\nn_workers = len(CUDA_VISIBLE_DEVICES.split(\",\"))\nfrac_size = 0.15\nuse_rmm_pool = False\nmax_day = None  # (Optional) -- Limit the dataset to day 0-max_day for debugging","59336d7d":"def _pool(frac=0.8):\n    rmm.reinitialize(\n        pool_allocator=True,\n        initial_pool_size=get_rmm_size(frac * device_mem_size()),\n    )\n    \ndef _convert_file(path, name, out_dir, frac_size, fs, cols, dtypes):\n    fn = f\"{name}.parquet\"\n    out_path = fs.sep.join([out_dir, f\"{name}.parquet\"])\n    writer = ParquetWriter(out_path, compression=None)\n    for gdf in nvt.Dataset(\n        path,\n        engine=\"csv\",\n        names=cols,\n        part_memory_fraction=frac_size,\n        sep='\\t',\n        dtypes=dtypes,\n    ).to_iter():\n        writer.write_table(gdf)\n        del gdf\n    md = writer.close(metadata_file_path=fn)\n    return md\n\ndef _write_metadata(md_list, fs, path):\n    rg_sizes = []\n    if md_list:\n        metadata_path = fs.sep.join([path, \"_metadata\"])\n        _meta = (\n            cudf.io.merge_parquet_filemetadata(md_list)\n            if len(md_list) > 1\n            else md_list[0]\n        )\n        with fs.open(metadata_path, \"wb\") as fil:\n            _meta.tofile(fil)\n    return True","f666d23a":"fs = get_fs_token_paths(INPUT_PATH, mode=\"rb\")[0]\nfile_list = [\n    x for x in fs.glob(fs.sep.join([INPUT_PATH, \"*\"]))\n    if not x.endswith(\"parquet\") and not x.endswith('readme.txt') and not x.endswith('test.txt')\n]\nfile_list = sorted(file_list, key=natural_sort_key)\nfile_list = file_list[:max_day] if max_day else file_list\nname_list = _analyze_paths(file_list, fs)[1]\n\ncont_names = [\"I\" + str(x) for x in range(1, 14)]\ncat_names = [\"C\" + str(x) for x in range(1, 27)]\ncols = [\"label\"] + cont_names + cat_names\n\ndtypes = {}\ndtypes[\"label\"] = np.int32\nfor x in cont_names:\n    dtypes[x] = \"hex\"\nfor x in cat_names:\n    dtypes[x] = \"hex\"\n\ndsk = {}\ntoken = tokenize(file_list, name_list, OUTPUT_PATH, frac_size, fs, cols, dtypes)\nconvert_file_name = \"convert_file-\" + token\nfor i, (path, name) in enumerate(zip(file_list, name_list)):\n    key = (convert_file_name, i)\n    dsk[key] = (_convert_file, path, name, OUTPUT_PATH, frac_size, fs, cols, dtypes)\n\nwrite_meta_name = \"write-metadata-\" + token\ndsk[write_meta_name] = (\n    _write_metadata,\n    [(convert_file_name, i) for i in range(len(file_list))],\n    fs,\n    OUTPUT_PATH,\n)\ngraph = HighLevelGraph.from_collections(write_meta_name, dsk, dependencies=[])\nconversion_delayed = Delayed(write_meta_name, graph)","88dd3f75":"!mkdir -p $OUTPUT_PATH","992a121c":"%%time\n\nconversion_delayed.compute(scheduler=\"synchronous\")","987888c8":"import dask_cudf\nimport dask_ml","0ae5a4fc":"df = dask_cudf.read_parquet(OUTPUT_PATH + '\/train.txt.parquet')\ntrain, valid = dask_ml.model_selection.train_test_split(df, test_size=0.1)\n!mkdir \/kaggle\/working\/criteo-parquet\ntrain.to_parquet('\/kaggle\/working\/criteo-parquet\/train_train.parquet')\nvalid.to_parquet('\/kaggle\/working\/criteo-parquet\/train_valid.parquet')","9484f2ad":"del train, valid, df","bb1e2b06":"df = dask_cudf.read_parquet(OUTPUT_PATH + '\/train.txt.parquet')\n_, df = dask_ml.model_selection.train_test_split(df, test_size=0.2) # Only 20% of the original subset is used\ntrain, valid = dask_ml.model_selection.train_test_split(df, test_size=0.1)\ntrain.to_parquet(OUTPUT_PATH + '\/train_train.parquet')\nvalid.to_parquet(OUTPUT_PATH + '\/train_valid.parquet')\n\ndel train, valid, df","6c49c259":"import os\nfrom time import time\nimport re\nimport glob\nimport warnings\nimport json\n\n# tools for data preproc\/loading\nimport torch\nimport rmm\nimport nvtabular as nvt\nfrom nvtabular.ops import Normalize,  Categorify,  LogOp, FillMissing, Clip, get_embedding_sizes\nfrom nvtabular.loader.torch import TorchAsyncItr, DLDataLoader\nfrom nvtabular.utils import device_mem_size, get_rmm_size\n\n# tools for training\nfrom fastai.basics import Learner\nfrom fastai.tabular.model import TabularModel\nfrom fastai.tabular.data import TabularDataLoaders\nfrom fastai.metrics import accuracy\nfrom fastai.callback.progress import ProgressCallback","6273d869":"# define some information about where to get our data\nINPUT_DATA_DIR = '\/raid\/criteo\/'\nOUTPUT_DATA_DIR = '\/kaggle\/working\/criteo-parquet-subset-preprocessed' # where we'll save our processed data to\n\n# define our dataset schema\nCONTINUOUS_COLUMNS = ['I' + str(x) for x in range(1,14)]\nCATEGORICAL_COLUMNS =  ['C' + str(x) for x in range(1,27)]\nLABEL_COLUMNS = ['label']\nCOLUMNS = CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS + LABEL_COLUMNS\n\ntrain_paths = [OUTPUT_PATH + '\/train_train.parquet']\nvalid_paths = [OUTPUT_PATH + '\/train_valid.parquet']","66d15de7":"cat_features = CATEGORICAL_COLUMNS >> Categorify(freq_threshold=30)\ncont_features = CONTINUOUS_COLUMNS >> FillMissing() >> Clip(min_value=0) >> LogOp() >> Normalize()\nfeatures = cat_features+cont_features+LABEL_COLUMNS\nfeatures.graph","672aad2f":"workflow = nvt.Workflow(features)","02d3a504":"train_dataset = nvt.Dataset(train_paths, engine='parquet', part_mem_fraction=0.15)\nvalid_dataset = nvt.Dataset(valid_paths, engine='parquet', part_mem_fraction=0.15)","2e9beccf":"%%time\nworkflow.fit(train_dataset)","74932ead":"output_train_dir = os.path.join(OUTPUT_DATA_DIR, 'train\/')\noutput_valid_dir = os.path.join(OUTPUT_DATA_DIR, 'valid\/')\n! mkdir -p $output_train_dir\n! mkdir -p $output_valid_dir","42e5c368":"%%time\n\nworkflow.transform(train_dataset).to_parquet(output_path=output_train_dir,\n                                             shuffle=nvt.io.Shuffle.PER_PARTITION, \n                                             out_files_per_proc=5)","65339e78":"%%time\n\nworkflow.transform(valid_dataset).to_parquet(output_path=output_valid_dir, out_files_per_proc=5)","8fd92254":"!ls $output_train_dir","68423520":"!ls $output_valid_dir","c51f01d1":"EMBEDDING_TABLE_SHAPES = nvt.ops.get_embedding_sizes(workflow)\nEMBEDDING_TABLE_SHAPES","2016b3c0":"json.dump({\n    'EMBEDDING_TABLE_SHAPES': EMBEDDING_TABLE_SHAPES\n}, open(OUTPUT_DATA_DIR + \"\/stats.json\", \"w\"))","2373ceec":"!ls $OUTPUT_DATA_DIR","58c71fe6":"We define the data paths and data schema where we specify  column groups for continuous, categorical or the label features.","49f0e570":"\n# Preprocess Criteo to Parquet\n\nThis notebook preprocesses the [Criteo subset dataset](https:\/\/www.kaggle.com\/mrkmakr\/criteo-dataset) from the [Criteo Display Advertising Challenge](https:\/\/www.kaggle.com\/c\/criteo-display-ad-challenge) from `.txt` to `.parquet`. Parquet is a column-oriented, compressed dataformat, which require less data. To simplify it, it is faster to read data from a parquet file.<br><br>\nWe preprocess the dataset for two example notebooks:\n1. [Faster ETL for Tabular Data](https:\/\/www.kaggle.com\/benediktschifferer\/faster-etl-for-tabular-data) is a short introduction to NVTabular and how to use NVTabular to accelerate ETLs on GPU with larger than (GPU) memory dataset.\n2. [Faster FastAI Tabular Deep Learning](https:\/\/www.kaggle.com\/benediktschifferer\/faster-fastai-tabular-deep-learning) shows how to speed up FastAI Tabular Deep Learning Model and use dataset which are larger than memory. FastAI is an amazing library, providing best practices for deep learning. However, we identified optimization opportunities in training tabular data models, which we want to share.\n\nWe preprocess the dataset to provide data to both notebooks to focus on our content in the notebook.","9bb6ef0a":"Now, we are ready to define our feature engineering and preprocessing pipeline.<br><br>\nWe use NVTabular to define the data pipeline:\n1. Categorical features: Use `Categorify` to convert categorical values from String to continuous integer 0, ..., C with C the cardinality of the features. We apply a frequency treshhold of 30 to group low frequent categories together.\n2. Numeric features: `NaN` values are filled with 0, clipped to a minimum of 0, applied with a logarithm function and normalized to mean=0 and std=1.\n\nFirst, we import some libraries.","c52fe421":"We initialize a NVTabular Dataset, which iterates over subsets\/chunks of dataset to avoid `OutOfMemory` errors. `train_paths` and `valid_paths` are lists of filenames.","c8b76107":"We define the NVTabular pipeline. For more information, take a look at the [Faster ETL for Tabular Data](https:\/\/www.kaggle.com\/benediktschifferer\/faster-etl-for-tabular-data).","784a3ab1":"Next, we apply the transformation to the train and valid dataset and persist it to disk.","d30acc88":"Execute conversion","6e165cf9":"We need to define an output path for training and validation dataset.","d69b50b5":"## Splitting train into train and validation","70779b12":"If we want train a deep learning model with either TensorFlow, PyTorch or FastAI, our training pipeline will require information about the data schema to define the neural network architecture. In addition, we store the embedding tables structure.","c2f5d8ce":"## Preprocessing Criteo Dataset\n\nThe raw format is in text form, which is inefficient to store and read. We convert the dataset into a parquet format based on [optimize-criteo.ipynb](https:\/\/github.com\/NVIDIA\/NVTabular\/blob\/main\/examples\/optimize_criteo.ipynb). Parquet is a column oriented and compressed file format, which is more efficient to store the dataset.\n\nWe import the required libraries.","97ace831":"Specify the input and output paths, unless the `INPUT_PATH` and `OUTPUT_PATH` environment variables are already set. This environment provides only a Single-GPU. For a multi-GPU system example, check out [optimize-criteo.ipynb](https:\/\/github.com\/NVIDIA\/NVTabular\/blob\/main\/examples\/optimize_criteo.ipynb).","134820df":"Let's take a look.","33943e4b":"The test.txt file does not contain a target column. Therefore, we preprocessed only the train.txt file and, to create a validation set with a target column, we need to split our training dataset into train and validation datasets. We use dask_cudf for GPU-accelerated processing.","a901803b":"### Note: This is a copy from [Faster ETL for Tabular Data](https:\/\/www.kaggle.com\/benediktschifferer\/faster-etl-for-tabular-data). We just run this pipeline to produce the input files for the other notebook [Faster FastAI Tabular Deep Learning](https:\/\/www.kaggle.com\/benediktschifferer\/faster-fastai-tabular-deep-learning)","b29d44fc":"### Note: We will use the new and more flexible NVTabular API, which will be released with v0.4 in February 2021.","5ebec392":"## Feature Engineering and Preprocessing with NVTabular","218e005a":"Now, we collect statistics on the train set by calling the `fit` function on the `train_dataset`.","c3a4d884":"We apply the workflow to the validation dataset. Since we split the validation set from the training set, it is reasonable to assume the validation set follows the same data distribution as the train set. Here, we use the collected statistics from the training dataset to transform the validation dataset.","ba841574":"The FastAI data loader in [Faster FastAI Tabular Deep Learning](https:\/\/www.kaggle.com\/benediktschifferer\/faster-fastai-tabular-deep-learning) loads the full dataset into memory and the Kaggle.com environment with 13GB host memory cannot fit the full dataset. Thus, we need to create a 20% sampled version.","06131f4e":"We initialize a NVTabular `Workflow` with our pipeline.","350bd3c2":"Main conversion script (build Dask task graph)","5a0b7f72":"Install the library nvtx and dask_cudf.","96706479":"## Installing NVTabular on Kaggle\n\nKaggle added cuDF=0.16 support, which enables to use NVTabular in Kaggle kernels. First, be sure that GPU acceleration and Internet are activated.\n\n<img src=\"https:\/\/bsopenbucket.s3-eu-west-1.amazonaws.com\/kaggle\/GPUInternet.png\" width=\"200px\">\n\nNext uninstall typing, as typing creates a conflict to install NVTabular.","acbd080d":"We define helper\/task functions","4d71833b":"Install NVTabular. As mentioned, we will use the new NVTabular API. We will install NVTabular from GitHub using a specific commit. Later, we will change this to the v0.4 release."}}