{"cell_type":{"d68db2c5":"code","bb2e2bab":"code","8f15367e":"code","2283b3bd":"code","b8a2b67a":"code","84936f75":"code","bb9e295e":"code","4bbbe598":"code","ad7f74ed":"code","690ff151":"code","18bc149d":"code","e638fc75":"code","31bec48e":"code","2808f3e4":"code","ba9ee4a5":"code","ca82c232":"code","62b967e2":"code","288eeb87":"markdown","0bc04d65":"markdown","08d663e2":"markdown","3d003cf8":"markdown","482e3cc1":"markdown","08ee0fd3":"markdown","95d49f92":"markdown","5a5e5764":"markdown","1970f6d1":"markdown","e41678f2":"markdown","fe5e07d3":"markdown","b20b7b6a":"markdown","cf4650bb":"markdown","7ec13490":"markdown","a3433fb9":"markdown","cb6b9ffa":"markdown","114033f4":"markdown","9db49c55":"markdown","d006b4c2":"markdown","91f65e5f":"markdown","84ca8ac0":"markdown"},"source":{"d68db2c5":"import pandas as pd","bb2e2bab":"pd.DataFrame({'Yes': [50, 21], 'No': [131, 2]})","8f15367e":"pd.DataFrame({'Bob': ['I liked it.', 'It was awful.'], 'Sue': ['Pretty good.', 'Bland.']})","2283b3bd":"pd.DataFrame({'Bob': ['I liked it.', 'It was awful.'], \n              'Sue': ['Pretty good.', 'Bland.']},\n             index=['Product A', 'Product B'])","b8a2b67a":"pd.Series([1, 2, 3, 4, 5])","84936f75":"pd.Series([30, 35, 40], index=['2015 Sales', '2016 Sales', '2017 Sales'], name='Product A')","bb9e295e":"wine_reviews = pd.read_csv(\"..\/input\/wine-reviews\/winemag-data-130k-v2.csv\")","4bbbe598":"wine_reviews.shape","ad7f74ed":"wine_reviews.head()","690ff151":"wine_reviews = pd.read_csv(\"..\/input\/wine-reviews\/winemag-data-130k-v2.csv\", index_col=0)\nwine_reviews.head()","18bc149d":"wic = pd.read_excel(\"..\/input\/publicassistance\/xls_files_all\/WICAgencies2013ytd.xls\", \n                    sheet_name='Total Women')\nwic.head()","e638fc75":"import sqlite3\nconn = sqlite3.connect(\"..\/input\/188-million-us-wildfires\/FPA_FOD_20170508.sqlite\")","31bec48e":"fires = pd.read_sql_query(\"SELECT * FROM fires\", conn)","2808f3e4":"fires.head()","ba9ee4a5":"wine_reviews.head().to_csv(\"wine_reviews.csv\")","ca82c232":"wic.to_excel('wic.xlsx', sheet_name='Total Women')","62b967e2":"conn = sqlite3.connect(\"fires.sqlite\")\nfires.head(10).to_sql(\"fires\", conn)","288eeb87":"## Reading common file formats\n\nBeing able to create a `DataFrame` and `Series` by hand is handy. But, most of the time, we won't actually be creating our own data by hand, we'll be working with data that already exists.\n\nData can be stored in any of a number of different forms and formats. By far the most basic of these is the humble CSV file. When you open a CSV file you get something that looks like this:\n\n```csv\nProduct A,Product B,Product C,\n30,21,9,\n35,34,1,\n41,11,11\n```\n\nSo a CSV file is a table of values separated by commas. Hence the name: \"comma-seperated values\", or CSV.\n\nLet's now set aside our toy datasets and see what a real dataset looks like when we read it into a `DataFrame`. We'll use the `read_csv` function to read the data into a `DataFrame`. This goes thusly:","0bc04d65":"To write an Excel file back you need `to_excel` and the `sheet_name` again:","08d663e2":"And finally, to output to a SQL database, supply the name of the table in the database we want to throw the data into, and a connector:","3d003cf8":"As you can see in this example, Excel files are often not formatted as well as CSV files are. Spreadsheets allow (and encourage) creating notes and fields which are human-readable, but not machine-readable.\n\nSo before we can use this particular dataset, we will need to clean it up a bit. We will see how to do so in the next section.\n\nFor now, let's move on to another common data format: SQL files.\n\nSQL databases are where most of the data on the web ultimately gets stored. They can be used to store data on things as simple as recipes to things as complicated as \"almost everything on the Kaggle website\".\n\nConnecting to a SQL database requires a lot more thought than reading from an Excel file. For one, you need to create a **connector**, something that will handle siphoning data from the database.\n\n`pandas` won't do this for you automatically because there are many, many different types of SQL databases out there, each with its own connector. So for a SQLite database (the only kind supported on Kaggle), you would need to first do the following (using the `sqlite3` library that comes with Python):","482e3cc1":"A `Series` is, in essence, a single column of a `DataFrame`. So you can assign column values to the `Series` the same way as before, using an `index` parameter. However, a `Series` do not have a column name, it only has one overall `name`:","08ee0fd3":"A Series, by contrast, is a sequence of data values. If a `DataFrame` is a table, a `Series` is a list. And in fact you can create one with nothing more than a list:","95d49f92":"Let's look at a few more datatypes you're likely to encounter.\n\nFirst up, the venerable Excel spreadsheet. An Excel file (`XLS` or `XLST`) organizes itself as a sequence of named sheets. Each sheet is basically a table. So to load the data into `pandas` we need one additional parameter: the name of the sheet of interest.\n\nSo this:\n\n![](https:\/\/s3.amazonaws.com\/nonwebstorage\/excel.png)\n\nBecomes this:\n\n<!-- First up, the venerable SQL database. You can read a single table in a SQL database directly into a `pandas` `DataFrame` using the `read_sql` method. The only thing you need is -->","5a5e5764":"Painless!","1970f6d1":"Writing data to a file is usually easier than reading it out of one, because `pandas` handles the nuisance of conversions for you. \n\nWe'll start with CSV files again. The opposite of `read_csv`, which reads our data, is `to_csv`, which writes it. With CSV files it's dead simple:","e41678f2":"The other thing you need to do is write a SQL statement. Internally, SQL databases all operate very differently. Externally, however, they all provide the same API, the \"Structured Query Language\" (or...SQL...for short).\n\nWe (very briefly) need to use SQL to load data into \n\nFor the purposes of analysis however we can usually just think of a SQL database as a set of tables with names, and SQL as a minor inconvenience in getting that data out of said tables.\n\nSo, without further ado, here is all the SQL you have to know to get the data out of `SQLite` and into `pandas`:","fe5e07d3":"Every SQL statement begins with `SELECT`. The asterisk (`*`) is a wildcard character, meaning \"everything\", and `FROM fires` tells the database we want only the data from the `fires` table specifically.\n\nAnd, out the other end, data:","b20b7b6a":"In this example, the \"0, No\" entry has the value of 131. The \"0, Yes\" entry has a value of 50, and so on.\n\n`DataFrame` entries are not limited to integers. For instance, here's a `DataFrame` whose values are `str` strings:","cf4650bb":"We are using the `pd.DataFrame` constructor to generate these `DataFrame` objects. The syntax for declaring a new one is a dictionary whose keys are the column names (`Bob` and `Sue` in this example), and whose values are a list of entries. This is the standard way of constructing a new `DataFrame`, and the one you are likliest to encounter.","7ec13490":"`Series` and the `DataFrame` are intimately related. It's helpful to think of a `DataFrame` as actually being just a bunch of `Series` \"glue together\". We'll see more of this in the next section of this tutorial.","a3433fb9":"## Writing common file formats","cb6b9ffa":"# Creating, reading, and writing reference\n\nThis is the reference component to the \"Creating, reading, and writing\" section of the tutorial. \n\nThe very first step in any data analytics project will probably reading the data out of a file somewhere, so it makes sense that that's the first thing we'd need to cover. In this section, we'll look at exercises on creating `pandas` `Series` and `DataFrame` objects, both by hand and by reading data from disc.\n\nThe [IO Tools](http:\/\/pandas.pydata.org\/pandas-docs\/stable\/io.html) section of the official `pandas` docs provides a comprehensive overview on this subject.","114033f4":"The `pandas` `read_csv` function is well-endowed, with over 30 optional parameters you can specify. For example, you can see in this dataset that the `csv` file has an in-built index, which `pandas` did not pick up on automatically. To make `pandas` use that column for the index (instead of creating a new one from scratch), we may specify and use an `index_col`.","9db49c55":"The dictionary-list constructor assigns values to the *column labels*, but just uses an ascending count from 0 (0, 1, 2, 3, ...) for the *row labels*. Sometimes this is OK, but oftentimes we will want to assign these labels ourselves.\n\nThe list of row labels used in a `DataFrame` is known as an **Index**. We can assign values to it by using an `index` parameter in our constructor:","d006b4c2":"We can use the `shape` attribute to check how large the resulting `DataFrame` is:","91f65e5f":"So our new `DataFrame` has 130,000 records split across 14 different columns. That's almost 2 million entries!\n\nWe can examine the contents of the resultant `DataFrame` using the `head` command, which grabs the first five rows:","84ca8ac0":"## Creating data\n\nThere are two core objects in `pandas`: the **DataFrame** and the **Series**.\n\nA DataFrame is a table. It contains an array of individual *entries*, each of which has a certain *value*. Each entry corresponds with a row (or *record*) and a *column*.\n\nFor example, consider the following simple `DataFrame`:"}}