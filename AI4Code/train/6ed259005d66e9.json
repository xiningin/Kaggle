{"cell_type":{"e86ad9fc":"code","09dc1156":"code","1dab568d":"code","4c632035":"code","7de7199f":"code","224ea0f5":"code","80f6e367":"code","a41510fd":"code","c9a8d861":"code","2df288fc":"code","44fa3e5e":"code","f20f85ca":"code","93bcd142":"code","3e3151b6":"code","fd674115":"code","41593b9e":"code","7f667048":"code","14456668":"code","a477eff9":"code","7d3d955d":"markdown","6152a23d":"markdown","b4a54e94":"markdown","85db6e83":"markdown","52b66112":"markdown","2601c2f1":"markdown","be1b08e4":"markdown","592472a1":"markdown","323dd189":"markdown","c4febaa2":"markdown","92add134":"markdown","7d8e22be":"markdown","8821672d":"markdown","e6988594":"markdown","b77edaaa":"markdown","bca38ba8":"markdown","cbb80937":"markdown","bd45ee57":"markdown","7690de3c":"markdown","03cb8662":"markdown"},"source":{"e86ad9fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","09dc1156":"df = pd.read_csv(\"..\/input\/diabetes.csv\")\ndf.head()","1dab568d":"import seaborn as sns\nsns.countplot(df.Outcome)","4c632035":"df.describe()","7de7199f":"df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']] = df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']].replace(0,np.NaN)\ndf.head() #Replaced all 0 values by NaN so it is easy to clean the data","224ea0f5":"df.fillna(df.mean(), inplace = True) #Filled Mising values with Mean\ndf.isnull().sum()","80f6e367":"df.describe()","a41510fd":"import matplotlib.pyplot as plt\nsns.heatmap(df.corr(),annot=True)\nfig = plt.gcf()\nfig.set_size_inches(8,8)","c9a8d861":"#feature selection\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier()\nx=df[df.columns[:8]]\ny=df.Outcome\nclf.fit(x,y)\nfeature_imp = pd.DataFrame(clf.feature_importances_,index=x.columns)\nfeature_imp.sort_values(by = 0 , ascending = False)","2df288fc":"from sklearn.cross_validation import train_test_split\n\nfeatures = df[[\"Glucose\",'BMI','Age','DiabetesPedigreeFunction']]\nlabels = df.Outcome\nfeatures.head()","44fa3e5e":"features_train,features_test,labels_train,labels_test = train_test_split(features,labels,stratify=df.Outcome,test_size=0.4)","f20f85ca":"#DTClassifier\nfrom sklearn.tree import DecisionTreeClassifier \ndtclf = DecisionTreeClassifier()\ndtclf.fit(features_train,labels_train)\ndtclf.score(features_test,labels_test)","93bcd142":"#SVM\nfrom sklearn import svm\nclf = svm.SVC(kernel=\"linear\")\nclf.fit(features_train,labels_train)\nclf.score(features_test,labels_test)","3e3151b6":"#Naive Bayes Classifier\nfrom sklearn import naive_bayes\nnbclf = naive_bayes.GaussianNB()\nnbclf.fit(features_train,labels_train)\nnbclf.score(features_test,labels_test)","fd674115":"#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknnclf = KNeighborsClassifier(n_neighbors=2)\nknnclf.fit(features_train,labels_train)\nprint(knnclf.score(features_test,labels_test))\n    \n ","41593b9e":"#Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nclf1 = LogisticRegression()\nclf1.fit(features_train,labels_train)\nclf1.score(features_test,labels_test)","7f667048":"algos = [\"Support Vector Machine\",\"Decision Tree\",\"Logistic Regression\",\"K Nearest Neighbor\",\"Naive Bayes\"]\nclfs = [svm.SVC(kernel=\"linear\"),DecisionTreeClassifier(),LogisticRegression(),KNeighborsClassifier(n_neighbors=2),naive_bayes.GaussianNB()]\nresult = []\n\nfor clff in clfs:\n    clff.fit(features_train,labels_train)\n    acc = clff.score(features_test,labels_test)\n    result.append(acc)\nresult_df = pd.DataFrame(result,index=algos)\nresult_df.columns=[\"Accuracy\"]\nresult_df.sort_values(by=\"Accuracy\",ascending=False)","14456668":"#Cross Validation\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nkfold =KFold(n_splits=10)\n","a477eff9":"algos = [\"Support Vector Machine\",\"Decision Tree\",\"Logistic Regression\",\"K Nearest Neighbor\",\"Naive Bayes\"]\nclfs = [svm.SVC(kernel=\"linear\"),DecisionTreeClassifier(),LogisticRegression(),KNeighborsClassifier(n_neighbors=2),naive_bayes.GaussianNB()]\ncv_results=[]\nfor classifiers in clfs:\n    cv_score = cross_val_score(classifiers,features,labels,cv=kfold,scoring=\"accuracy\")\n    cv_results.append(cv_score.mean())\ncv_mean = pd.DataFrame(cv_results,index=algos)\ncv_mean.columns=[\"Accuracy\"]\ncv_mean.sort_values(by=\"Accuracy\",ascending=False)","7d3d955d":"Umm.... I'm not getting the heatmap into my head...Lets try another way....Use random forest to get the importances of feature...","6152a23d":"So as we can see... The first 4 features displayed maybe important for us...We might neglect the rest...\n\nNow get your tools ready to sculpt diffrent models... ","b4a54e94":"Hmm..Looks like it is a supervised learning problem as we have predictor variables and one dependent variable(Outcome).","85db6e83":"Good...Better than Decision Tree...**K Neighbor?**","52b66112":"Getting to know your data well be of great help...Lets dive into the dataset and see what are the columns we will be dealing with.","2601c2f1":"Yeah...We have 2 classes(0,1) in which our data is classified..\n\nNow lets seeHow is the distribution of my data along diffrent rows?","be1b08e4":"Great...Looks neat..Also I can see that which models are looking well in terms of accuracy...But did u guys noticed?..We have been working on same training and testing set from a long time..We need to ry diffrent combinations of training and testing sets...Lets bring Cross Validation into picture to help me out! ","592472a1":"Looks like the data is clean...But..Wait a minute..\nIs the data really clean?\n\nWhat about the values that are \"0\"?...Insulin of a person cant be 0 right...\nObviously!\n\nLets clean it up...To do it in a easy way, replace all zeros from all predictor variables(except Pregnancies) by NaN...This represents that we have got a missing value...Fix it!!","323dd189":"To get the total number of classes we have in the outcome column and count of each class, countplot might help us...","c4febaa2":"**Feature Selection**\n\nTo increase the efficiency of the model, we can eliminate some features. This is done by knowing the importance if a particular feature...\n\nLets try to find correlation between the features of our dataset..More the features are correlated, we can eliminate one of them...Heatmap folks!!","92add134":"These are five models we will be seeing...Split the work and get your hands dirty to code the hell up!!\n\nNote that we have used stratification while splitting so that our data gets splitted in proportion with respect to Outcome column.","7d8e22be":"By using cross validation, we will be splitting our dataset into 10 equal parts...We keep one part for testing our algorithm and we train models on the rest...Now these parts that we divided the dataset into, keeps interchanging to form diffrent combinations of training and testing data...We get difffrent accuracy score for each combination...This is done by cross_val_score()..It gives us the list of diffrent accuracies...Now by taking the mean of this score, we can find the general accuracy of our model...\nThis gives a generalised output..","8821672d":" **Hey there!**\n\nThis notebook will be dealing with comparing some basic Machine Learning classification algorithms based on their accuracy. So, lets get started! ","e6988594":"Will Gaussian **Naive Bayes** do well?","b77edaaa":"We can replace missing values by using various methods like mean,median,mode or the number of your choice...We'll do that with mean...You can rather try diffrent methods...And let me know if something works better...(Signs that I'm Lazy:-|)","bca38ba8":"You know what?...I will like to see all these algo's accuracy at one place...Also, lets code everything up in single cell itself..","cbb80937":"Looks fine...Lets see what Support Vector Machine shows\n**SVM**\nTry to change kernel parameter to other than \"linear\"..","bd45ee57":"Cool...Now the last one remaining...Lets do **Logistic Regression**","7690de3c":"And now....We can see the accuracy changed a bit this time...It is because we have done cross validation and trained and tested the algorithms on diffrent combinations of data....\n\nFrom the above output, it is clear that for this dataset, SVM, Logistic Regression and Naive Bayes works better....\n\nSo to revise what we did above:\n\n1.Cleaned our data by replacing missing values(for this data, we considered 0 as a missing value).\n2.Feature Selection using Random Forest Classifier.\n3.Split the data into training and testing sets using train_test_split.\n4.Trained five diffrent classification algorithms and found their accuracies.\n5.Cross validation by splitting data into 10 splits to get the generalised accuracy for each algorithm.","03cb8662":"I'll like to start with **Decision Trees**"}}