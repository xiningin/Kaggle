{"cell_type":{"fc106af2":"code","290c777b":"code","9addee90":"code","d824b446":"code","4046526d":"code","6b5b9e73":"code","f2895b56":"code","81af9f55":"code","a18c704a":"code","5627b4f6":"code","2d204d39":"code","08b42888":"code","b7875dcd":"code","ad7481d6":"code","8891ffd0":"code","c91c0bbe":"code","74f8f6a7":"code","7d36bb85":"code","e28a2eb5":"code","1f310ceb":"code","30dc04bf":"code","65d9e4c0":"code","e3a7b0e3":"code","3334e9cf":"code","c9946665":"code","05a0da34":"code","432878a9":"code","f2c3548d":"code","911e891e":"code","6c286052":"code","bd742fe0":"code","cb71497a":"code","406c0081":"code","755596d0":"code","7e22ce76":"code","9b421da5":"code","db94debc":"code","3dc41695":"code","7513abbe":"code","f9acdc1f":"code","42d93a70":"code","4ab3e381":"code","a3a6cfc9":"code","c2142842":"code","ec1dd325":"code","69f7cc76":"code","ee67a3f8":"code","64c9b06b":"code","d55803c6":"code","251a3778":"code","312be1a9":"code","e6f2fe46":"code","bf2b5db9":"code","00677347":"code","717cac8f":"code","c75d8bb3":"code","26a3469c":"code","c674fac1":"code","7f6cba41":"code","000dd2f7":"code","98ee9da1":"code","5f67ca72":"code","5fb67078":"code","1b1fc897":"code","a006e2b9":"code","2ffce02d":"code","af404a2b":"code","3e78969e":"code","4606bf74":"code","7e6fcc07":"code","dfc90ce9":"code","88d62364":"code","648a7b03":"markdown","81432cb0":"markdown","de0b7b44":"markdown","d7ba6031":"markdown","7af063f7":"markdown","1019c770":"markdown","5e540c54":"markdown","aba5daca":"markdown","532919f4":"markdown","0ddd606e":"markdown","84f74db5":"markdown","648ef94c":"markdown","fa708cc6":"markdown","72580c7b":"markdown","27d1babf":"markdown","cf573e98":"markdown","051f9471":"markdown","e42eb8ce":"markdown","88526f12":"markdown","7b021d60":"markdown","d168d671":"markdown"},"source":{"fc106af2":"import pickle\nimport numpy as np","290c777b":"with open(\"..\/input\/chat-bot-data\/train_qa.txt\", \"rb\") as fp:   # Unpickling\n    train_data =  pickle.load(fp)","9addee90":"with open(\"..\/input\/chat-bot-data\/test_qa.txt\", \"rb\") as fp:   # Unpickling\n    test_data =  pickle.load(fp)","d824b446":"type(test_data)","4046526d":"type(train_data)","6b5b9e73":"len(test_data)","f2895b56":"len(train_data)","81af9f55":"train_data[0]","a18c704a":"' '.join(train_data[0][0])","5627b4f6":"' '.join(train_data[0][1])","2d204d39":"train_data[0][2]","08b42888":"# Create a set that holds the vocab words\nvocab = set()","b7875dcd":"all_data = test_data + train_data","ad7481d6":"for story, question , answer in all_data:\n    # In case we don't know what a union of sets is:\n    # https:\/\/www.programiz.com\/python-programming\/methods\/set\/union\n    vocab = vocab.union(set(story))\n    vocab = vocab.union(set(question))","8891ffd0":"vocab.add('no')\nvocab.add('yes')","c91c0bbe":"vocab","74f8f6a7":"vocab_len = len(vocab) + 1 #we add an extra space to hold a 0 for Keras's pad_sequences","7d36bb85":"max_story_len = max([len(data[0]) for data in all_data])","e28a2eb5":"max_story_len","1f310ceb":"max_question_len = max([len(data[1]) for data in all_data])","30dc04bf":"max_question_len","65d9e4c0":"vocab","e3a7b0e3":"# Reserve 0 for pad_sequences\nvocab_size = len(vocab) + 1","3334e9cf":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer","c9946665":"# integer encode sequences of words\ntokenizer = Tokenizer(filters=[])\ntokenizer.fit_on_texts(vocab)","05a0da34":"tokenizer.word_index","432878a9":"train_story_text = []\ntrain_question_text = []\ntrain_answers = []\n\nfor story,question,answer in train_data:\n    train_story_text.append(story)\n    train_question_text.append(question)","f2c3548d":"train_story_seq = tokenizer.texts_to_sequences(train_story_text)","911e891e":"len(train_story_text)","6c286052":"len(train_story_seq)","bd742fe0":"def vectorize_stories(data, word_index=tokenizer.word_index, max_story_len=max_story_len,max_question_len=max_question_len):\n    '''\n    INPUT: \n    \n    data: consisting of Stories,Queries,and Answers\n    word_index: word index dictionary from tokenizer\n    max_story_len: the length of the longest story (used for pad_sequences function)\n    max_question_len: length of the longest question (used for pad_sequences function)\n\n\n    OUTPUT:\n    \n    Vectorizes the stories,questions, and answers into padded sequences. We first loop for every story, query , and\n    answer in the data. Then we convert the raw words to an word index value. Then we append each set to their appropriate\n    output list. Then once we have converted the words to numbers, we pad the sequences so they are all of equal length.\n    \n    Returns this in the form of a tuple (X,Xq,Y) (padded based on max lengths)\n    '''\n    \n    \n    # X = STORIES\n    X = []\n    # Xq = QUERY\/QUESTION\n    Xq = []\n    # Y = CORRECT ANSWER\n    Y = []\n    \n    \n    for story, query, answer in data:\n        \n        # Grab the word index for every word in story\n        x = [word_index[word.lower()] for word in story]\n        # Grab the word index for every word in query\n        xq = [word_index[word.lower()] for word in query]\n        \n        # Grab the Answers (either Yes\/No so we don't need to use list comprehension here)\n        # Index 0 is reserved so we're going to use + 1\n        y = np.zeros(len(word_index) + 1)\n        \n        # Now that y is all zeros and we know its just Yes\/No , we can use numpy logic to create this assignment\n        #\n        y[word_index[answer]] = 1\n        \n        # Append each set of story,query, and answer to their respective holding lists\n        X.append(x)\n        Xq.append(xq)\n        Y.append(y)\n        \n    # Finally, pad the sequences based on their max length so the RNN can be trained on uniformly long sequences.\n        \n    # RETURN TUPLE FOR UNPACKING\n    return (pad_sequences(X, maxlen=max_story_len),pad_sequences(Xq, maxlen=max_question_len), np.array(Y))","cb71497a":"inputs_train, queries_train, answers_train = vectorize_stories(train_data)","406c0081":"inputs_test, queries_test, answers_test = vectorize_stories(test_data)","755596d0":"inputs_test","7e22ce76":"queries_test","9b421da5":"answers_test","db94debc":"sum(answers_test)","3dc41695":"tokenizer.word_index['yes']","7513abbe":"tokenizer.word_index['no']","f9acdc1f":"from keras.models import Sequential, Model\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Input, Activation, Dense, Permute, Dropout\nfrom keras.layers import add, dot, concatenate\nfrom keras.layers import LSTM","42d93a70":"input_sequence = Input((max_story_len,))\nquestion = Input((max_question_len,))","4ab3e381":"# Input gets embedded to a sequence of vectors\ninput_encoder_m = Sequential()\ninput_encoder_m.add(Embedding(input_dim=vocab_size,output_dim=64))\ninput_encoder_m.add(Dropout(0.3))\n\n# This encoder will output:\n# (samples, story_maxlen, embedding_dim)","a3a6cfc9":"# embed the input into a sequence of vectors of size query_maxlen\ninput_encoder_c = Sequential()\ninput_encoder_c.add(Embedding(input_dim=vocab_size,output_dim=max_question_len))\ninput_encoder_c.add(Dropout(0.3))\n# output: (samples, story_maxlen, query_maxlen)","c2142842":"# embed the question into a sequence of vectors\nquestion_encoder = Sequential()\nquestion_encoder.add(Embedding(input_dim=vocab_size,\n                               output_dim=64,\n                               input_length=max_question_len))\nquestion_encoder.add(Dropout(0.3))\n# output: (samples, query_maxlen, embedding_dim)","ec1dd325":"# encode input sequence and questions (which are indices)\n# to sequences of dense vectors\ninput_encoded_m = input_encoder_m(input_sequence)\ninput_encoded_c = input_encoder_c(input_sequence)\nquestion_encoded = question_encoder(question)","69f7cc76":"# shape: `(samples, story_maxlen, query_maxlen)`\nmatch = dot([input_encoded_m, question_encoded], axes=(2, 2))\nmatch = Activation('softmax')(match)","ee67a3f8":"# add the match matrix with the second input vector sequence\nresponse = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\nresponse = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)","64c9b06b":"# concatenate the match matrix with the question vector sequence\nanswer = concatenate([response, question_encoded])","d55803c6":"answer","251a3778":"# Reduce with RNN (LSTM)\nanswer = LSTM(32)(answer)  # (samples, 32)","312be1a9":"# Regularization with Dropout\nanswer = Dropout(0.5)(answer)\nanswer = Dense(vocab_size)(answer)  # (samples, vocab_size)","e6f2fe46":"# we output a probability distribution over the vocabulary\nanswer = Activation('softmax')(answer)\n\n# build the final model\nmodel = Model([input_sequence, question], answer)\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n              metrics=['accuracy'])","bf2b5db9":"model.summary()","00677347":"# train\nhistory = model.fit([inputs_train, queries_train], answers_train,batch_size=16,\n                    epochs=64,validation_data=([inputs_test, queries_test], answers_test))","717cac8f":"filename = 'chatbot_120_epochs.h5'\nmodel.save(filename)","c75d8bb3":"import matplotlib.pyplot as plt\n%matplotlib inline\nprint(history.history.keys())\n","26a3469c":"# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.grid()\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","c674fac1":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.grid()\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","7f6cba41":"model.load_weights(filename)\npred_results = model.predict(([inputs_test, queries_test]))","000dd2f7":"test_data[0][0]","98ee9da1":"story =' '.join(word for word in test_data[0][0])\nprint(story)","5f67ca72":"query = ' '.join(word for word in test_data[0][1])\nprint(query)","5fb67078":"print(\"True Test Answer from Data is:\",test_data[0][2])","1b1fc897":"#Generate prediction from model\nval_max = np.argmax(pred_results[0])\n\nfor key, val in tokenizer.word_index.items():\n    if val == val_max:\n        k = key\n\nprint(\"Predicted answer is: \", k)\nprint(\"Probability of certainty was: \", pred_results[0][val_max])","a006e2b9":"vocab","2ffce02d":"# Note the whitespace of the periods\nmy_story = \"John left the kitchen . Sandra dropped the football in the garden .\"\nmy_story.split()","af404a2b":"my_question = \"Is the football in the garden ?\"","3e78969e":"my_question.split()","4606bf74":"mydata = [(my_story.split(),my_question.split(),'yes')]","7e6fcc07":"my_story,my_ques,my_ans = vectorize_stories(mydata)","dfc90ce9":"pred_results = model.predict(([ my_story, my_ques]))","88d62364":"#Generate prediction from model\nval_max = np.argmax(pred_results[0])\n\nfor key, val in tokenizer.word_index.items():\n    if val == val_max:\n        k = key\n\nprint(\"Predicted answer is: \", k)\nprint(\"Probability of certainty was: \", pred_results[0][val_max])","648a7b03":"-----------","81432cb0":"### Input Encoder c","de0b7b44":"## Writing our Own Stories and Questions\n\nRemember we can only use words from the existing vocab","d7ba6031":"##### Use dot product to compute the match between first input vector seq and the query","7af063f7":"#### Concatenate","1019c770":"#### Add this match matrix with the second input vector sequence","5e540c54":"# Question and Answer Chat Bots","aba5daca":"### Encode the Sequences","532919f4":"### Evaluating on Given Test Set","0ddd606e":"-----\n\n## Setting up Vocabulary of All Words","84f74db5":"## Creating the Model","648ef94c":"## Vectorizing the Data","fa708cc6":"### Building the Networks\n\nTo understand why we chose this setup, make sure to read the paper we are using:\n\n* Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus,\n  \"End-To-End Memory Networks\",\n  http:\/\/arxiv.org\/abs\/1503.08895","72580c7b":"### Saving the Model","27d1babf":"## Exploring the Format of the Data","cf573e98":"### Functionalize Vectorization","051f9471":"## Encoders\n\n### Input Encoder m","e42eb8ce":"## Evaluating the Model\n\n### Plotting Out Training History","88526f12":"## Loading the Data\n\nWe will be working with the Babi Data Set from Facebook Research.\n\nFull Details: https:\/\/research.fb.com\/downloads\/babi\/\n\n- Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n  \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\",\n  http:\/\/arxiv.org\/abs\/1502.05698\n","7b021d60":"### Question Encoder","d168d671":"### Placeholders for Inputs\n\nRecall we technically have two inputs, stories and questions. So we need to use placeholders. `Input()` is used to instantiate a Keras tensor.\n"}}