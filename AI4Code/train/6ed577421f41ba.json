{"cell_type":{"32c591fe":"code","4654bf62":"code","8f0cfd22":"code","d520dd47":"code","d07ea340":"code","a7aafde5":"code","d23e709c":"markdown"},"source":{"32c591fe":"import torch\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer","4654bf62":"class Dataset:\n    def __init__(self, text, tokenizer, max_len):\n        self.text = text\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        text = str(self.text[item])\n        inputs = self.tokenizer(\n            text, \n            max_length=self.max_len, \n            padding=\"max_length\", \n            truncation=True\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n        }","8f0cfd22":"def generate_predictions(model_path, max_len):\n    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    model.to(\"cuda\")\n    model.eval()\n    \n    df = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\n    \n    dataset = Dataset(text=df.text.values, tokenizer=tokenizer, max_len=max_len)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=32, num_workers=4, pin_memory=True, shuffle=False\n    )\n\n    final_output = []\n\n    for b_idx, data in enumerate(data_loader):\n        with torch.no_grad():\n            for key, value in data.items():\n                data[key] = value.to(\"cuda\")\n            output = model(**data)\n            output = output.logits.detach().cpu().numpy()[:, 1].ravel().tolist()\n            final_output.extend(output)\n    \n    torch.cuda.empty_cache()\n    return np.array(final_output)","d520dd47":"base_model = '..\/input\/autonlp-toxic-1'\npreds = generate_predictions(base_model, max_len=128)   #192","d07ea340":"sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\nsub[\"score\"] = preds\nsub = sub[[\"comment_id\", \"score\"]]\nsub.to_csv(\"submission.csv\", index=False)","a7aafde5":"sub.head()","d23e709c":"### Based on https:\/\/www.kaggle.com\/abhishek\/autonlp-for-toxic-ratings\n\n### This kernel is a naive comparison of the different HF models posted in https:\/\/www.kaggle.com\/c\/jigsaw-toxic-severity-rating\/discussion\/286602\n\nBaseline: https:\/\/www.kaggle.com\/abhishek\/autonlp-for-toxic-ratings [LB 0.762 with max_length 192]\n\nhttps:\/\/www.kaggle.com\/abhishek\/autonlp-for-toxic-ratings [LB 0.762 with max_length 128]\n\nhttps:\/\/www.kaggle.com\/jonathanchan\/roberta-toxicity-classifier [LB 0.560]\n\nhttps:\/\/www.kaggle.com\/jonathanchan\/roberta-toxicity-classifier-v1 [LB 0.269]\n\nhttps:\/\/www.kaggle.com\/jonathanchan\/toxdect-roberta-large [LB 0.489]\n\nhttps:\/\/www.kaggle.com\/jonathanchan\/unbiased-toxic-roberta [LB 0.707]"}}