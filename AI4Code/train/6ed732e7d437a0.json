{"cell_type":{"5def1c4d":"code","09c4ece9":"code","639bade2":"code","a54f07e4":"code","8716179d":"code","9d27a726":"code","4bfe41c6":"code","a4720f23":"code","cc850b70":"code","cdfe2f22":"code","7c180639":"code","ba43da77":"code","6b1c004d":"code","6081c1f6":"code","67a6a9ee":"code","5fb98755":"code","e29db7d2":"code","c488bd8c":"code","3271699a":"code","4fc88a5f":"code","45808f0d":"code","fa2d4bf1":"code","acb93ab3":"code","0029b0ff":"code","6b432920":"markdown","ee5e92ed":"markdown","f3a95770":"markdown","33c2b387":"markdown","db666b7a":"markdown","cefae750":"markdown","f21566eb":"markdown","ad8184bc":"markdown","e389ccb3":"markdown"},"source":{"5def1c4d":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler\nfrom seaborn import countplot\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n#Models to be used for experimentation\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\n\n#Put seed for same experiment outcomes\nfrom numpy.random import seed\nseed(1)","09c4ece9":"raw_df = pd.read_csv('..\/input\/video-games-rating-by-esrb\/Video_games_esrb_rating.csv',delimiter=',')\ncolumns = raw_df.columns.values\nprint('Column names: ',columns)\nprint('-----\\n')\n\nesrb_rating_labels = raw_df.esrb_rating.unique()\nprint('Rating Labels: ',esrb_rating_labels)\nprint('-----\\n')\n\ndescriptors = columns[1:-1]\nprint('Independent Variables: ',descriptors) ","639bade2":"X = raw_df[descriptors].to_numpy()\n\nle = LabelEncoder()\ny = le.fit_transform(raw_df['esrb_rating'].to_numpy())\ny","a54f07e4":"f = plt.figure(figsize=(10,10))\nplt.matshow(raw_df[descriptors].corr(),fignum=f.number)\ncb = plt.colorbar()\nplt.title('Correlation Matrix')","8716179d":"print('We don\\'t see much correlation between the variables other than themselves.' )\nraw_df[descriptors].corr().style.background_gradient(cmap='coolwarm')","9d27a726":"print('It seems that the classes are almost balanced hence we won\\'t try to up or down sample the classes')\ncountplot(raw_df['esrb_rating'])","4bfe41c6":"pca = PCA(n_components=2)\npca_X = pca.fit_transform(X)","a4720f23":"plt.scatter(pca_X[:,0],pca_X[:,1],c=y)\nplt.title('PCA with n_components=2')\nplt.xlabel('PCA-1')\nplt.ylabel('PCA-2')\nprint('NOTE:- We don\\'t see much separation between the classes of ratings when we try to use least number of components. Hence PCA is not suitable')","cc850b70":"exp_var_ratio = []\ncomponents_list = []\nfor k in range(2,32):\n    pca_k = PCA(n_components=k)\n    pca_k.fit(X)\n    exp_var_ratio.append(pca_k.explained_variance_ratio_)\n    components_list.append(pca_k.components_)\n    \nexp_var_per = []\nfor v in exp_var_ratio:\n    exp_var_per.append(np.sum(v))   \n    \n\nplt.plot(exp_var_per)\nplt.title('Data distribution across number of components')\nplt.xlabel('n_components')\nplt.ylabel('Data ratio')\nprint('This suggest us that we have to use almost all the variables instead of few. About 90% of data is in 23-25 components')","cdfe2f22":"\"\"\"\n#Splitting the data into train and test sets\nx_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n\n#Standardize the data first\nsc = StandardScaler()\nstd_x_train = sc.fit_transform(x_train)\nstd_x_test = sc.transform(x_test)\n\"\"\"","7c180639":"models = {'LR': LogisticRegression(),\n          'SVM': SVC(),\n          'RF': RandomForestClassifier(),\n          'ADA': AdaBoostClassifier(),\n          'GBM': GradientBoostingClassifier(),\n          'DT': DecisionTreeClassifier(),\n          'KNN':KNeighborsClassifier(),\n          'QDA':QuadraticDiscriminantAnalysis(),\n          'NB': GaussianNB(),\n          'GDA':LinearDiscriminantAnalysis()}","ba43da77":"def run_experiment(model_list,X,y):\n    \n    folds = KFold(n_splits=10, random_state=1,shuffle=True)\n    print(folds)\n    #Standardize the data first\n    sc = StandardScaler()\n    X = sc.fit_transform(X)\n    \n    model_names = []\n    cv_results = []\n    mean_score_list = []\n    for model in models.keys():\n        scores = cross_val_score(models[model],X,y,scoring='accuracy',cv=folds,n_jobs=-1)\n        mean_score_list.append((model,'mean acc: '+str(np.mean(scores)),'std acc: '+str(np.std(scores))))\n        print('model name: %s -- mean accuracy: %0.3f || std accuracy: %.3f '%(model,np.mean(scores),np.std(scores)))\n        cv_results.append(scores)\n        model_names.append(model)\n    return model_names,mean_score_list,cv_results","6b1c004d":"model_names, mean_score_list, cv_results = run_experiment(models,X,y)","6081c1f6":"def boxplot_comparison(model_names,cv_results):\n    \n    fig = plt.figure()\n    fig.suptitle('BoxPlot Model Comparison')\n    ax = fig.add_subplot(111)\n    plt.boxplot(cv_results)\n    ax.set_xticklabels(model_names)\n    plt.show()\n    \nboxplot_comparison(model_names,cv_results)\nprint('BEST MODELS:\\n-----\\n\\tLR, SVM, RF, GBM, DT\\n\\nWORST MODELS:\\n-----\\n\\tADA, KNN, QDA, GaussianNB, GDA')","67a6a9ee":"models = ['LR','SVM','RF','GBM','DT']\n\nclfs = [\n        LogisticRegression(),\n        SVC(),\n        RandomForestClassifier(),\n        GradientBoostingClassifier(),\n        DecisionTreeClassifier()\n       ]\n\nparam_space = {\n                models[0]:{'penalty':['l1','l2'], 'C':[0.001,0.01], 'solver':['lbfgs','saga'], 'max_iter':[100,200]},\n                models[1]:{'C':[0.001,0.003,1,10], 'kernel':['rbf','linear']},\n                models[2]:{'n_estimators':[100,150], 'criterion':['gini','entropy']},\n                models[3]:{'learning_rate':[0.1,0.01],'n_estimators':[100,150]},\n                models[4]:{'criterion':['gini','entropy']}\n              }","5fb98755":"def grid_optimization(models,clfs,param_space,X,y):\n    \n    #Standardize the data first\n    sc = StandardScaler()\n    X = sc.fit_transform(X)\n    \n    for name,estimator in zip(models,clfs):\n        print(name)\n        clf = GridSearchCV(estimator,param_space[name],scoring='accuracy',refit='True',cv=5)\n        clf.fit(X,y)\n        \n        #print('best estimator: ',clf.best_estimator_)\n        print('best params: ',clf.best_params_)\n        print('best scores: %.3f'%clf.best_score_)\n        print('-----\\n')","e29db7d2":"grid_optimization(models,clfs,param_space,X,y)\nprint('Best models after optimisation are:\\n-----\\n\\tSVM, GBM, RF')","c488bd8c":"test_raw_df = pd.read_csv('..\/input\/video-games-rating-by-esrb\/test_esrb.csv',delimiter=',')\ncolumns = test_raw_df.columns.values\nprint('Column names: ',columns)\nprint('-----\\n')\n\nesrb_rating_labels = test_raw_df.esrb_rating.unique()\nprint('Rating Labels: ',esrb_rating_labels)\nprint('-----\\n')\n\ndescriptors = columns[1:-1]\nprint('Independent Variables: ',descriptors) ","3271699a":"#Assign Train and Test\nX_test = test_raw_df[descriptors].to_numpy()\ny_test = test_raw_df['esrb_rating']\ny_test_en = le.transform(y_test)\nprint('Test X shape: ',X_test.shape)\nprint('Test y shape: ',y_test_en.shape)","4fc88a5f":"#Declare Models and scaler\nsc = StandardScaler()\nSVM_opt = SVC(C=1,kernel='rbf')\nGBM_opt = GradientBoostingClassifier(learning_rate=0.1, n_estimators=150) \nRF_opt = RandomForestClassifier(criterion='entropy', n_estimators=100)","45808f0d":"#Scaling\nX_train = sc.fit_transform(X)\ny_train = y\n\nX_test = sc.transform(X_test)\ny_test = y_test_en","fa2d4bf1":"#Fit and Predict\nSVM_model = SVM_opt.fit(X_train,y_train)\nSVM_preds =SVM_model.predict(X_test)\n\nGBM_model = GBM_opt.fit(X_train,y_train)\nGBM_preds = GBM_model.predict(X_test)\n\nRF_model = RF_opt.fit(X_train,y_train)\nRF_preds = RF_model.predict(X_test)","acb93ab3":"print('SVM accuracy score: ',accuracy_score(SVM_preds,y_test))\nprint('GBM accuracy score: ',accuracy_score(GBM_preds,y_test))\nprint('RF accuracy score: ',accuracy_score(RF_preds,y_test))\nprint('-----')\nprint('SVM precision score: ',precision_score(SVM_preds,y_test,average='macro'))\nprint('GBM precision score: ',precision_score(GBM_preds,y_test,average='macro'))\nprint('RF precision score: ',precision_score(RF_preds,y_test,average='macro'))\nprint('-----')\nprint('SVM recall score: ',recall_score(SVM_preds,y_test,average='macro'))\nprint('GBM recall score: ',recall_score(GBM_preds,y_test,average='macro'))\nprint('RF recall score: ',recall_score(RF_preds,y_test,average='macro'))\nprint('-----')\nprint('SVM f1 score: ',f1_score(SVM_preds,y_test,average='macro'))\nprint('GBM f1 score: ',f1_score(GBM_preds,y_test,average='macro'))\nprint('RF f1 score: ',f1_score(RF_preds,y_test,average='macro'))","0029b0ff":"import seaborn as sns\n\n#Finally let's see where model is getting confused through a ConfusionMatrix\nrf_cm = confusion_matrix(y_test,RF_preds)\n\nrf_cm = rf_cm.astype('float') \/ rf_cm.sum(axis=1)[:,np.newaxis]\nrf_cm = rf_cm * 100\n\n#esrb_rating_labels\nlabels = le.classes_\nsns.heatmap(rf_cm,annot=True)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Random Forest Classifier')\nplt.show()","6b432920":"# 2. Let's look at the correlations between the vairables","ee5e92ed":"# 1. Load the data file","f3a95770":"# 3. Now we try to see if we can find any separation in the clusters for rating types when least components are used in PCA","33c2b387":"# Conclusion: Random Forest is the best model","db666b7a":"# 4. Before we did PCA with n_components=2 and couldn't find clusterings. Let's see why it happened","cefae750":"# 7. Final evaluation on Test Data","f21566eb":"# 5. Let's experiment with several classifiers and see which one might do best","ad8184bc":"# 2. Let's look at the rating class distribution\n","e389ccb3":"# 6. Optimize best algorithms for better evaluation"}}