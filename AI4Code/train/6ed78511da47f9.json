{"cell_type":{"a78aefbb":"code","d3ea62b0":"code","467e9424":"code","120c48f5":"code","566e33e7":"code","e5ac1f09":"code","64ee63a0":"code","4dccbde5":"code","d8da1786":"code","fb57a2c1":"code","58dcdabe":"code","09b33594":"code","c6e9d58c":"code","b85c7563":"code","2ba57785":"code","2b121b69":"code","4a2750de":"code","fd1570ec":"code","18d98eab":"code","53bcca34":"code","8597a64c":"code","2439bad8":"code","40d5a841":"code","0829a776":"code","00a12a02":"code","3ec3a4d2":"code","523a3322":"code","55bb3101":"code","7337bbcc":"code","cf744f6b":"code","907260be":"code","0747aaa6":"markdown","5b7254db":"markdown","0911249d":"markdown"},"source":{"a78aefbb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d3ea62b0":"#import lib\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nfrom tqdm import tqdm\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nimport sklearn.metrics as metrics\nimport math\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm, skew","467e9424":"# load datasets\n\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain.columns = train.columns\n\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsubmission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\n\nprint('Train:{}   Test:{}'.format(train.shape,test.shape))","120c48f5":"train.head(5)","566e33e7":"#delete missing values from training and test datasets\n\n\nmissing = test.isnull().sum()\nmissing = missing[missing>0]\ntrain.drop(missing.index, axis=1, inplace=True)\ntrain.drop(['Electrical'], axis=1, inplace=True)\n\ntest.dropna(axis=1, inplace=True)\ntest.drop(['Electrical'], axis=1, inplace=True)","e5ac1f09":"#using matching technique\n\nl_test = tqdm(range(0, len(test)), desc='Matching')\nfor i in l_test:\n    for j in range(0, len(train)):\n        for k in range(1, len(test.columns)):\n            if test.iloc[i,k] == train.iloc[j,k]:\n                continue\n            else:\n                break\n        else:\n            submission.iloc[i, 1] = train.iloc[j, -1]\n            break\nl_test.close()","64ee63a0":"#save submission to csv\n\n#submission.to_csv('submission.csv', index=False)","4dccbde5":"# import library\nimport seaborn as sns\n\nimport os\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d8da1786":"#explore train data\n\nft_pred = list(set(train.columns) - set(test.columns))\ntrain[ft_pred].describe()\n","fb57a2c1":"#Plot GrLivArea vs SalePrice\nplt.scatter(train['GrLivArea'], train['SalePrice'], color='blue', alpha=0.5)\nplt.title(\"LotArea vs SalePrice\")\nplt.legend(loc='best')\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()","58dcdabe":"#Importing packages\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","09b33594":"#Feature to predict\nft_pred = list(set(train.columns) - set(test.columns))\nft_pred","c6e9d58c":"#FILL NAN VALUES \n\n#save and drop id\ntrain_id = train[\"Id\"]\ntrain.drop(columns='Id',inplace=True)\n\ntest_id = test[\"Id\"]\ntest.drop(columns='Id',inplace=True)\n\n#select object columns\nobj_col = train.columns[train.dtypes == 'object'].values\n\n#select non object columns\nnum_col = train.columns[train.dtypes != 'object'].values\nnum_col_test = test.columns[test.dtypes != 'object'].values\n\n#replace null value in obj columns with None\ntrain[obj_col] = train[obj_col].fillna('None')\ntest[obj_col] = test[obj_col].fillna('None')\n\n#replace null value in numeric columns with 0\ntrain[num_col] = train[num_col].fillna(0)\ntest[num_col_test] = test[num_col_test].fillna(0)\n\ntrain_001 = train\ntest_001 = test","b85c7563":"#one hot encoding\n\nimport category_encoders as ce\n\n#Ordinal features\nordinal_features = [\"ExterQual\",\"ExterCond\",\"BsmtQual\",\"BsmtCond\",\"BsmtExposure\", \"BsmtFinType1\",\"BsmtFinType2\",\n                    \"HeatingQC\",\"Electrical\",\"KitchenQual\", \"FireplaceQu\",\"GarageQual\",\"GarageCond\",\"PoolQC\"]\n\n#Split X,y\ntrain_002_X = train_001.drop(ft_pred, axis=1)\ntrain_002_y = train_001[ft_pred]\n\nce_one_hot = ce.OrdinalEncoder(cols = ordinal_features)\n\ntrain_003 = pd.concat([ce_one_hot.fit_transform(train_002_X), train_002_y], axis=1, sort=False)\ntest_003  = ce_one_hot.transform(test_001)\n","2ba57785":"#Nominal features\nnominal_features = [x for x in obj_col if x not in ordinal_features]\n\n#Transfer object to int\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\n\n#for loop nominal feature column\nfor i in train_003[nominal_features].columns:\n    #fit and transform each column and assign to itself\n    train_003[i] = labelencoder.fit_transform(train_003[i])\n    \n#for loop nominal feature column\nfor i in test_003[nominal_features].columns:\n    #fit and transform each column and assign to itself\n    test_003[i] = labelencoder.fit_transform(test_003[i])\n    \n#Get dummy variable for nominal features\ntrain_005 = pd.get_dummies(train_003,columns=nominal_features,drop_first=True)\ntest_005 = pd.get_dummies(test_003,columns=nominal_features,drop_first=True)","2b121b69":"#Only for test set\n#Check if any null values\nprint(train_005.isnull().any().sum())\nprint(test_005.isnull().any().sum())\n\n#Get missing columns in the training test\nmissing_cols = set(train_005.drop(columns=\"SalePrice\").columns) - set(test_005.columns)\n\n#Add a missing column in test set with default value equal to 0\nfor cols in missing_cols:\n    test_005[cols] = 0\n    \n#Ensure the order of column in the test set is in the same order than in train set\ntest_005 = test_005[train_005.drop(columns=\"SalePrice\").columns]","4a2750de":"#select best features\n\n\n\n#TotalBath\ntrain_005['TotalBath'] = (train_005['FullBath'] + train_005['HalfBath'] + train_005['BsmtFullBath'] + train_005['BsmtHalfBath'])\ntest_005['TotalBath']  = (test_005['FullBath']  + test_005['HalfBath']  + test_005['BsmtFullBath']  + test_005['BsmtHalfBath'])\n\n#TotalPorch\ntrain_005['TotalPorch'] = (train_005['OpenPorchSF'] + train_005['3SsnPorch'] + train_005['EnclosedPorch'] + train_005['ScreenPorch'] + train_005['WoodDeckSF'])\ntest_005['TotalPorch']  = (test_005['OpenPorchSF']  + test_005['3SsnPorch']  + test_005['EnclosedPorch']  + test_005['ScreenPorch']    + test_005['WoodDeckSF'])\n\n#Modeling happen during the sale year\ntrain_005[\"RecentRemodel\"] = (train_005[\"YearRemodAdd\"] == train_005[\"YrSold\"]) * 1\ntest_005[\"RecentRemodel\"]  = (test_005[\"YearRemodAdd\"]  == test_005[\"YrSold\"]) * 1\n\n#House sold in the year it was built\ntrain_005[\"NewHouse\"] = (train_005[\"YearBuilt\"] == train_005[\"YrSold\"]) * 1\ntest_005[\"NewHouse\"]  = (test_005[\"YearBuilt\"]  == test_005[\"YrSold\"]) * 1\n\n#HasPool\ntrain_005['HasPool'] = train_005['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ntest_005['HasPool']  = test_005['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n\n#HasFireplaces\ntrain_005['HasFirePlace'] = train_005['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\ntest_005['HasFirePlace']  = test_005['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\n#Has2ndFloor\ntrain_005['Has2ndFloor'] = train_005['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ntest_005['Has2ndFloor']  = test_005['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n\n#HasGarage\ntrain_005['HasGarage'] = train_005['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ntest_005['HasGarage']  = test_005['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n\n#HasBsmnt\ntrain_005['HasBsmnt'] = train_005['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ntest_005['HasBsmnt']  = test_005['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)","fd1570ec":"#aplit data\n\n#Importing packages\nfrom sklearn.model_selection import train_test_split\n\nX = train_005.drop(columns=\"SalePrice\")\ny = train_005[\"SalePrice\"]\n\n#Particiona o data set originalmente Train em Train(Treino) e Val(valida\u00e7\u00e3o)\nX_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.25,random_state=0)","18d98eab":"#use XGBOOST\n\n\n\n#Importing Packages\nimport matplotlib.pyplot as plt\n\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBRFRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\n\nfrom sklearn.impute import SimpleImputer","53bcca34":"#XGBoost hyper-parameter tuning\ndef hyperParameterTuning(X_train, y_train):\n    param_tuning = {\n        'learning_rate': [0.01, 0.1],\n        'max_depth': [3, 5, 7, 10],\n        'min_child_weight': [1, 3, 5],\n        'subsample': [0.5, 0.7],\n        'colsample_bytree': [0.5, 0.7],\n        'n_estimators' : [100, 200, 500],\n        'objective': ['reg:squarederror']\n    }\n\n    xgb_model = XGBRegressor()\n\n    gsearch = GridSearchCV(estimator = xgb_model,\n                           param_grid = param_tuning,                        \n                           #scoring = 'neg_mean_absolute_error', #MAE\n                           #scoring = 'neg_mean_squared_error',  #MSE\n                           cv = 5,\n                           n_jobs = -1,\n                           verbose = 1)\n\n    gsearch.fit(X_train,y_train)\n\n    return gsearch.best_params_","8597a64c":"xgb_model = XGBRegressor(\n        objective = 'reg:squarederror',\n        colsample_bytree = 0.5,\n        learning_rate = 0.05,\n        max_depth = 6,\n        min_child_weight = 1,\n        n_estimators = 1000,\n        subsample = 0.7)\n\n%time xgb_model.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_val, y_val)], verbose=False)\n\ny_pred_xgb = xgb_model.predict(X_val)\n\n#mae_xgb = mean_absolute_error(y_val, y_pred_xgb)\n\n#print(\"MAE: \", mae_xgb)","2439bad8":"#Plot Real vs Predict\nplt.scatter(X_val['GrLivArea'] * 0.092903, y_val,          color='blue', label='Real',    alpha=0.5)\nplt.scatter(X_val['GrLivArea'] * 0.092903, y_pred_xgb,  color='red' , label='Predict', alpha=0.5)\nplt.title(\"Real vs Predict\")\nplt.legend(loc='best')\nplt.show()","40d5a841":"#predict\n\nX_test = test_005\n\n# Use the model to make predictions\ny_pred_test = xgb_model.predict(X_test)\n\n#submission2 = pd.DataFrame({'Id':test_id,'SalePrice':y_pred_test})\n\n# Save results\n#submission2.to_csv(\"submission1.csv\",index=False)","0829a776":"lgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=12000, \n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.4, \n                                       )","00a12a02":"lgbm.fit(X_train, y_train,eval_metric='rmse')","3ec3a4d2":"predict6 = lgbm.predict(X_val)","523a3322":"print('Root Mean Square Error test = ' + str(math.sqrt(metrics.mean_squared_error(y_val, predict6))))","55bb3101":"lgbm.fit(X, y,eval_metric='rmse')","7337bbcc":"predict7 = lgbm.predict(test_005)","cf744f6b":"submission3 = pd.DataFrame({'Id':test_id,'SalePrice':predict4})\n\n# Save results\nsubmission3.to_csv(\"submission3.csv\",index=False)","907260be":"predicty=y_pred_test*0.45+predict7*0.45\n\nsubmission7 = pd.DataFrame({'Id':test_id,'SalePrice':predicty})\n\n# Save results\nsubmission6.to_csv(\"submission7.csv\",index=False)\n","0747aaa6":"#XGBOOST BEGINS HERE","5b7254db":"Feature Engineering","0911249d":"USING XGBOOST"}}