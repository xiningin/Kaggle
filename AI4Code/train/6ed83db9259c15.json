{"cell_type":{"7b4d3a95":"code","932ca6f9":"code","fd662cf6":"code","46724ea2":"code","c8bec93e":"code","f24f811d":"code","7ba862c1":"code","079839ca":"code","7ce051bc":"code","4fb04ff3":"code","5118d6dd":"code","b8de8b87":"code","c18f9717":"code","e43c6e61":"code","c8d10fe0":"code","4ddd943c":"code","08ece70c":"code","6ee01afa":"code","68dca335":"code","21ea1818":"code","a0e954cb":"code","1d317404":"code","fad4b358":"code","f97748fd":"code","988e70e8":"code","cb5a54f7":"code","a0aba9d9":"code","8b3b61e9":"code","b601adcc":"markdown"},"source":{"7b4d3a95":"!wget \"http:\/\/nlp.stanford.edu\/data\/glove.twitter.27B.zip\"","932ca6f9":"import zipfile\nimport os\nwith zipfile.ZipFile(\".\/glove.twitter.27B.zip\",\"r\") as zip_ref:\n    zip_ref.extract(\"glove.twitter.27B.200d.txt\")\n    print(zip_ref.filelist)\nii = ['glove.twitter.27B.zip']\nfor i in ii:\n    os.remove(i)\nprint(os.listdir(\".\/\"))\ndel zip_ref","fd662cf6":"import pandas as pd\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","46724ea2":"X_train_comments = train_df['comment_text'].values","c8bec93e":"y_train_ori = train_df['target'].values","f24f811d":"y_train_ori2 = []\nfor i in y_train_ori:\n    if (i>=0.5):\n        y_train_ori2.append(1)\n    else:\n        y_train_ori2.append(0)","7ba862c1":"from collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.under_sampling import RandomUnderSampler \nprint('Original dataset shape %s' % Counter(y_train_ori2))\n","079839ca":"rus = RandomUnderSampler(random_state=42)\nX_res, y_res = rus.fit_resample(X_train_comments.reshape(-1,1), y_train_ori2)\nprint('Resampled dataset shape %s' % Counter(y_res))","7ce051bc":"x_test_comments= test_df[\"comment_text\"].values","4fb04ff3":"import re\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom collections import OrderedDict\nimport string\n\nfrom bs4 import BeautifulSoup\nfrom nltk import WordPunctTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\ndef clean(text):\n    tok = WordPunctTokenizer()\n    pat1 = '@[\\w\\-]+'  # for @\n    pat2 = ('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n            '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')  # for url\n    pat3 = '#[\\w\\-]+'  # for hashtag\n    pat4 = '\u00ef\u00bb\u00bf'\n    pat5 = '[' + string.punctuation + ']'  # for punctuation\n    pat6 = '[^\\x00-\\x7f]'\n    soup = BeautifulSoup(text, 'html.parser')  # html decoding (\"@amp\")\n    souped = soup.get_text()\n    souped = re.sub(pat1, '', souped)  # remove @\n    souped = re.sub(pat2, '', souped)  # remove url\n    souped = re.sub(pat4, '', souped)  # remove strange symbols\n    souped = re.sub(pat5, '', souped)  # remove punctuation\n    souped = re.sub(pat3, '', souped)  # remove \"#\" symbol and keeps the words\n    clean = re.sub(pat6, '', souped)  # remove non-ascii characters\n    lower_case = clean.lower()  # convert to lowercase\n    words = tok.tokenize(lower_case)\n    return (\" \".join(words)).strip()\ndef my_clean(text,stops = False,stemming = False,minLength = 2):\n    text = str(text)\n    text = text.lower().split()\n    text = [w for w in text if len(w) >= minLength]\n\n    text = \" \".join(text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"don't\", \"do not \", text)\n    text = re.sub(r\"aren't\", \"are not \", text)\n    text = re.sub(r\"isn't\", \"is not \", text)\n    text = re.sub(r\"%\", \" percent \", text)\n    text = re.sub(r\"that's\", \"that is \", text)\n    text = re.sub(r\"doesn't\", \"dos not \", text)\n    text = re.sub(r\"he's\", \"he is \", text)\n    text = re.sub(r\"she's\", \"she is \", text)\n    text = re.sub(r\"it's\", \"it is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\\/'+-=]\", \" \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    text = text.lower().split()\n    text = [w for w in text if len(w) >= minLength]\n    if stemming and stops:\n        text = [word for word in text if word not in stopwords.words('english')]\n        wordnet_lemmatizer = WordNetLemmatizer()\n        englishStemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n        text = [englishStemmer.stem(word) for word in text]\n        text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n        # text = [lancaster.stem(word) for word in text]\n        text = [word for word in text if word not in stopwords.words('english')]\n    elif stops:\n        text = [word for word in text if word not in stopwords.words('english')]\n    elif stemming:\n        wordnet_lemmatizer = WordNetLemmatizer()\n        englishStemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n        text = [englishStemmer.stem(word) for word in text]\n        text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n    text = \" \".join(text)\n    return text","5118d6dd":"X_train_comments_pre = []\ncount = 0\nmax_length = -5\nimport time\nstart = time.time()\nfor t in X_res:\n    te = my_clean(t,False,True,2)\n    X_train_comments_pre.append(te)#You can add one more clean()\n    length = len(te.split(' '))\n    if length > max_length:\n        max_length = length\n    \n    if count % 10000 == 0:\n        print(count)\n        final = time.time()\n        total = final - start\n        print(total)\n    \n    count = count + 1","b8de8b87":"print(X_res[1])\nprint(X_train_comments_pre[1])\nprint(max_length)","c18f9717":"from sklearn.model_selection import train_test_split\nX_train, X_test ,y_train ,y_test = train_test_split(X_train_comments_pre,y_res, random_state=826, test_size=0.33)","e43c6e61":"from keras.preprocessing.text import Tokenizer\nprint(\"Opening Glove\")\nembeddings_index = dict()\nf = open('.\/glove.twitter.27B.200d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nvocabulary_size = 50000\ntokenizer = Tokenizer(num_words=vocabulary_size)\ntokenizer.fit_on_texts(X_train)\nembedding_matrix = np.zeros((50000, 200))\nfor word, index in tokenizer.word_index.items():\n    if index > 50000 - 1:\n        break\n    else:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[index] = embedding_vector","c8d10fe0":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom keras.preprocessing.sequence import pad_sequences\n\nclass MyPadder(BaseEstimator,TransformerMixin):\n    def __init__(self,maxlen=5000):\n        self.maxlen = maxlen\n        self.max_index = None\n\n    def fit(self,X,y=None):\n        self.max_index = pad_sequences(X,maxlen=self.maxlen).max()\n        return self\n\n    def transform(self,X,y=None):\n        X = pad_sequences(X,maxlen=self.maxlen)\n        X[X>self.max_index] = 0\n        return X\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport numpy as np\nclass MyTextsToSequences(Tokenizer, BaseEstimator, TransformerMixin):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def fit(self,texts,y=None):\n        self.fit_on_texts(texts)\n        return self\n\n    def transform(self,texts,y=None):\n        return np.array(self.texts_to_sequences(texts))\n","4ddd943c":"max_length=250\nsequencer = MyTextsToSequences(num_words=50000)\npadder = MyPadder(max_length)","08ece70c":"sequencer.fit(X_train)\nX_train_copy = sequencer.transform(X_train)\nX_test_copy = sequencer.transform(X_test)\npadder.fit(X_train_copy)\nX_train_copy = padder.transform(X_train_copy)\nX_test_copy = padder.transform(X_test_copy)","6ee01afa":"from keras import Input, Model\nfrom keras.optimizers import Adam\nfrom keras.utils import plot_model\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom lime.lime_text import LimeTextExplainer\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Dropout, concatenate\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.pipeline import make_pipeline\nimport numpy as np\nfrom collections import OrderedDict\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.preprocessing import LabelEncoder","68dca335":"from sklearn.feature_extraction.text import TfidfVectorizer\nvec = TfidfVectorizer(max_features=500)\nvec.fit(X_train)\nX_train_copy2 = vec.transform(X_train)\nX_test_copy2 = vec.transform(X_test)","21ea1818":"main_input = Input(shape=(max_length,), dtype='int32', name='main_input')\nglove_Embed = (Embedding(50000, 200, input_length=max_length, weights=[embedding_matrix], trainable=False))(main_input)\n\nx = Conv1D(64, 5, activation='relu')(glove_Embed)\nx = Conv1D(32, 5, activation='relu')(x)\nx = Dropout(rate=0.05)(x)\nx = MaxPooling1D(pool_size=4)(x)\nx = Dropout(rate=0.35)(x)\nx = LSTM(50)(x)\n\ny = Dense(300,activation='relu')(glove_Embed)\ny = Dropout(rate=0.05)(y)\ny = LSTM(300)(y)\ny = Dropout(rate=0.35)(y)\ny = Dense(100,activation='relu')(y)\ny = Dense(50,activation='relu')(y)\n\nmain_input2 = Input(shape=(len(vec.get_feature_names()),), dtype='float32', name='main_input2')\ne = Dense(300,activation='relu')(main_input2)\ne = Dense(1000,activation='relu')(e)\ne = Dropout(rate=0.35)(e)\ne = Dense(200,activation='relu')(e)\ne = Dropout(rate=0.05)(e)\ne = Dense(50,activation='relu')(e)\n\nz = concatenate([x, y, e])\n\nz = Dense(128,activation='relu')(z)\nz = Dropout(0.05)(z)\nz = Dense(64,activation='relu')(z)\nz = Dropout(0.1)(z)\nz = Dense(32,activation='relu')(z)\noutput_lay = Dense(1, activation='sigmoid')(z)\nmodel = Model(inputs=[main_input,main_input2], outputs=[output_lay])\nmodel.compile(optimizer=Adam(),loss='binary_crossentropy',metrics=['accuracy'])\nprint(model.summary())\nmodel.fit([X_train_copy,X_train_copy2], [y_train],validation_data=([X_test_copy,X_test_copy2],y_test),epochs=4, batch_size=128)  # starts training\ny_predicted = model.predict([X_test_copy,X_test_copy2])\n\ny_pred = []\nfor i in y_predicted:\n    if (i>=0.5):\n        y_pred.append(1)\n    else:\n        y_pred.append(0)\nmodel_name = \"dn\"\n# We want both weighted and macro, because the dataset is imbalanced!\nprint(model_name, 'f1 weighted', metrics.f1_score(y_pred, y_test, average=\"weighted\"))\nprint(model_name, 'f1 macro', metrics.f1_score(y_pred, y_test, average=\"macro\"))\nprint(model_name, 'precision weighted', metrics.precision_score(y_pred, y_test, average=\"weighted\"))\nprint(model_name, 'precision macro', metrics.precision_score(y_pred, y_test, average=\"macro\"))\nprint(model_name, 'recall weighted', metrics.recall_score(y_pred, y_test, average=\"weighted\"))\nprint(model_name, 'recall macro', metrics.recall_score(y_pred, y_test, average=\"macro\"))\nprint(model_name, 'acc', metrics.accuracy_score(y_pred, y_test))\nprint()","a0e954cb":"del X_train_copy2,X_test_copy2, train_df, X_train_copy, X_test_copy, embeddings_index,embedding_matrix, f, X_train_comments_pre, X_train, X_test ,y_train ,y_test, X_res,y_res, y_train_ori2, X_train_comments, y_train_ori","1d317404":"print(x_test_comments[0]) ##Do preproccessing! \nprint(len(x_test_comments))\n#test_df.head","fad4b358":"x_test_comments_pre = []\ncount = 0\nmax_length = -5\nimport time\nstart = time.time()\nfor t in x_test_comments:\n    te = my_clean(t,False,True,2)\n    x_test_comments_pre.append(te)#You can add one more clean()\n    length = len(te.split(' '))\n    if length > max_length:\n        max_length = length\n    \n    if count % 10000 == 0:\n        print(count)\n        final = time.time()\n        total = final - start\n        print(total)\n    \n    count = count + 1","f97748fd":"x_test_comments_pre_copy = sequencer.transform(x_test_comments_pre)\nx_test_comments_pre_copy = padder.transform(x_test_comments_pre_copy)","988e70e8":"x_test_comments_pre_copy2 = vec.transform(x_test_comments_pre)","cb5a54f7":"new_y_preds = model.predict([x_test_comments_pre_copy,x_test_comments_pre_copy2])","a0aba9d9":"submission = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': np.mean(new_y_preds, axis=1)\n})\nsubmission.to_csv('submission.csv', index=False)","8b3b61e9":"submission","b601adcc":"**Aristotaliens: Jigsaw Toxicity Competition**"}}