{"cell_type":{"3492093d":"code","4804ba41":"code","5ff0cc8e":"code","40be302c":"code","1f891c5e":"code","46301d4f":"code","6d3b80ff":"code","449e5756":"code","3c9eb1ab":"code","8ece8804":"code","547eadde":"code","bfb1f159":"code","2309b665":"code","eb7007f2":"code","077b8cac":"code","e270de7d":"code","07e6292e":"code","5ab6ec2d":"code","ce1e3f8d":"code","6f76af33":"code","73eb1082":"code","d8a2f0c9":"code","95f4c9f3":"code","f94eefe2":"code","1c514442":"code","85e1175b":"code","5b7b6839":"code","a672f1b4":"code","8bbf95d0":"code","91d01f98":"code","fc818ba7":"code","3c17d3ae":"code","882a3d8f":"code","db9bbf2d":"code","fddecbef":"code","1316718b":"code","a76266b9":"code","bd11db53":"code","1d94b743":"code","f13a9476":"code","031b6412":"code","c8f5a13a":"code","935c33ad":"code","2806ef5a":"code","87f75405":"code","4f77fbca":"code","4d3f1fe0":"code","f7f85a5a":"code","028c70ed":"code","a69d56cf":"markdown","0eef2194":"markdown","6ef9126b":"markdown","66d7cf22":"markdown","a3c40413":"markdown","723c9141":"markdown","bb9bb72f":"markdown","a5817820":"markdown","7dce2dae":"markdown","8d17026b":"markdown","73ba1dfd":"markdown","ece9dbc5":"markdown","11d55f8d":"markdown","2293e3f8":"markdown","accf76c4":"markdown","44ef287a":"markdown","588c8f44":"markdown","84d0ffe6":"markdown","1aace3dc":"markdown","fdd33a4b":"markdown","36eca6d1":"markdown","865637ce":"markdown","72033442":"markdown","f8bcd5ef":"markdown","b8480ba3":"markdown","f8ab10d1":"markdown","93b12881":"markdown","791f46d1":"markdown","1bdb4a69":"markdown","79d83d83":"markdown","3d9d5478":"markdown","e04ded1c":"markdown","968dd268":"markdown","6ae10cdb":"markdown","c5b412e9":"markdown","93baa282":"markdown","7541710b":"markdown","201baa87":"markdown","aef4e176":"markdown","aebb663f":"markdown","c18311a7":"markdown","b80921a7":"markdown"},"source":{"3492093d":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import ensemble\nfrom sklearn.tree import DecisionTreeClassifier\nimport gc\nfrom imblearn.under_sampling import TomekLinks\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import resample\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\n# import pickle\nimport matplotlib.cm as cm\nimport seaborn as sn\nfrom collections import Counter\nimport lightgbm as lgb\n# from kmodes.kprototypes import KPrototypes\nimport gc\n# %reload_ext autotime","4804ba41":"dataset = pd.read_csv('..\/input\/car-purchase-prediction\/Social_Network_Ads.csv')\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values","5ff0cc8e":"dataset.head()","40be302c":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","1f891c5e":"print(X_train)","46301d4f":"print(X_test)","6d3b80ff":"print(y_train)","449e5756":"print(y_test)","3c9eb1ab":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","8ece8804":"print(X_train)","547eadde":"print(X_test)","bfb1f159":"print(y_train)","2309b665":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state=0)\nclassifier.fit(X_train,y_train)","eb7007f2":"print(classifier.predict(sc.transform([[30,87000]])))","077b8cac":"y_pred = classifier.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","e270de7d":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","07e6292e":"print(classification_report(y_test, y_pred))\ncnf_matrix = confusion_matrix(y_test, y_pred)\n#print(cnf_matrix)\naccuracy_score(y_test, y_pred)","5ab6ec2d":"def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=plt.cm.summer):\n    plt.clf\n    plt.imshow(cm, interpolation='nearest')\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(target_names))\n    plt.xticks(tick_marks, target_names, rotation=45)\n    plt.yticks(tick_marks, target_names)\n    plt.tight_layout()\n \n    width, height = cm.shape\n \n    for x in range(width):\n        for y in range(height):\n            plt.annotate(str(cm[x][y]), xy=(y, x), \n                        horizontalalignment='center',\n                        verticalalignment='center',color='black',fontsize=22)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","ce1e3f8d":"plot_confusion_matrix(cnf_matrix, np.unique(y_pred))","6f76af33":"from matplotlib.colors import ListedColormap\nX_set, y_set = sc.inverse_transform(X_train), y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Logistic Regression (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","73eb1082":"from matplotlib.colors import ListedColormap\nX_set, y_set = sc.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Logistic Regression (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","d8a2f0c9":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, y_train)","95f4c9f3":"print(classifier.predict(sc.transform([[30,87000]])))","f94eefe2":"y_pred = classifier.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","1c514442":"print(classification_report(y_test, y_pred))\ncnf_matrix = confusion_matrix(y_test, y_pred)\n#print(cnf_matrix)\naccuracy_score(y_test, y_pred)","85e1175b":"plot_confusion_matrix(cnf_matrix, np.unique(y_pred))","5b7b6839":"from matplotlib.colors import ListedColormap\nX_set, y_set = sc.inverse_transform(X_train), y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 1),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 1))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('K-NN (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","a672f1b4":"from matplotlib.colors import ListedColormap\nX_set, y_set = sc.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 1),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 1))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('K-NN (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","8bbf95d0":"from sklearn.svm import SVC\nclassifier = SVC(kernel = 'linear', random_state = 0)\nclassifier.fit(X_train, y_train)","91d01f98":"print(classifier.predict(sc.transform([[30,87000]])))","fc818ba7":"y_pred = classifier.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","3c17d3ae":"print(classification_report(y_test, y_pred))\ncnf_matrix = confusion_matrix(y_test, y_pred)\n#print(cnf_matrix)\naccuracy_score(y_test, y_pred)","882a3d8f":"plot_confusion_matrix(cnf_matrix, np.unique(y_pred))","db9bbf2d":"from matplotlib.colors import ListedColormap\nX_set, y_set = sc.inverse_transform(X_train), y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('SVM (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","fddecbef":"from matplotlib.colors import ListedColormap\nX_set, y_set = sc.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('SVM (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","1316718b":"from sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)","a76266b9":"print(classifier.predict(sc.transform([[30,87000]])))","bd11db53":"y_pred = classifier.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","1d94b743":"print(classification_report(y_test, y_pred))\ncnf_matrix = confusion_matrix(y_test, y_pred)\n#print(cnf_matrix)\naccuracy_score(y_test, y_pred)","f13a9476":"plot_confusion_matrix(cnf_matrix, np.unique(y_pred))","031b6412":"from matplotlib.colors import ListedColormap\nX_set, y_set = sc.inverse_transform(X_train), y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Decision Tree Classification (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","c8f5a13a":"from matplotlib.colors import ListedColormap\nX_set, y_set = sc.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Decision Tree Classification (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","935c33ad":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)","2806ef5a":"print(classifier.predict(sc.transform([[30,87000]])))","87f75405":"y_pred = classifier.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","4f77fbca":"print(classification_report(y_test, y_pred))\ncnf_matrix = confusion_matrix(y_test, y_pred)\n#print(cnf_matrix)\naccuracy_score(y_test, y_pred)","4d3f1fe0":"plot_confusion_matrix(cnf_matrix, np.unique(y_pred))","f7f85a5a":"from matplotlib.colors import ListedColormap\nX_set, y_set = sc.inverse_transform(X_train), y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Random Forest Classification (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","028c70ed":"from matplotlib.colors import ListedColormap\nX_set, y_set = sc.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Random Forest Classification (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","a69d56cf":"# Predicting a new result","0eef2194":"# Visualising the Training Set Results for SVM","6ef9126b":"So now we're gonna move on to the next step which is predicting the test results and displaying the vector of the predictions next to the vector of the real results mean the real purchase decisions.","66d7cf22":"# Training the SVM model on the Training set","a3c40413":"Now we have trained our Logistic Regression Model. Now test the cutomer with Age=30 and Salary=87000 but before puting this values in classifier we need to scale these values as well, as we have scaled our orignal values so that's why it should have exact same scaler that we have applied on Training Data.","723c9141":"# Importing the Dataset","bb9bb72f":"# Training the Logistic Regression Model on Training Set","a5817820":"# Predicting a new result","7dce2dae":"# Visualising the Test Set Results for Random Forest","8d17026b":"# Predicting the Test set results","73ba1dfd":"# Visualising the Training Set Results for KNN","ece9dbc5":"# Visualising the Training Set results for Logistic Regression","11d55f8d":"# Classification Report & Confusion Matrix for SVM","2293e3f8":"# Splitting the dataset into Training Set and Test Set","accf76c4":"# Predicting the Test set results","44ef287a":"# Predicting a new result","588c8f44":"# Predicting the Test set results","84d0ffe6":"# Importing the Libraries","1aace3dc":"# Visualising the Test Set Results for SVM","fdd33a4b":"# Now we are going to implement Decision Trees","36eca6d1":"As you can see above that we have trained data on multiple classification models. Lets evaluate the accuracy of all models.\nAs you can see that we have used Multiple Algos and by comparing all models we have following accuracies:\n\n* Logistic Regression - 89%\n* KNN - 93%\n* SVM - 90%\n* Decision Trees - 91%\n* Random Forest - 91%\n\nSo, as per accuracies KNN has highest accuracy as compared to other models. So, for our problem, we are going to use KNN.\n\nThat concludes the basics of Classification Model Selection. Please add if i have missed something and correct me if there is any mistake.","865637ce":"# Visualising the Test Set Results for Logistic Regression","72033442":"# Visualising the Training Set Results for Random Forest","f8bcd5ef":"Now we are going to scale 2 given features named as Age and Salary. No need to scale Target variable as it is in already binary form.","b8480ba3":"According to the Test Data we have this customer who does not buy SUV and we can see that predicted answer is also the same, so the model did well here on this single observation of customer.","f8ab10d1":"# Now we are going to implement K-Nearest Neighbors (K-NN) ","93b12881":"# Training the K-NN model on the Training set","791f46d1":"# Classification Report & Confusion Matrix for Logistic Regression","1bdb4a69":"# Now we are going to implement Random Forest Classification","79d83d83":"# Training the Decision Tree Model on the Training set","3d9d5478":"# Classification Report & Confusion Matrix for Logistic Regression","e04ded1c":"# Classification Report & Confusion Matrix for Decission Tree","968dd268":"# Training the Random Forest Model on the Training set","6ae10cdb":"# Predicting a new result","c5b412e9":"The dataset we have here belongs to a Car company and we have to predict that which of our previous customers will buy brand new SUV as company has launched new SUV. And to predict this we need data on which we train classification model to predict that which customers will buy new SUV. The dataset contains 3 columns.\n\n* Age\n* Estimated Salary\n* Purchased - Target Variable\n\nIn Purchased Column we have 2 values, 0 & 1. 0 means that customer didnot buy any SUV and 1 means that customer have bought SUV. The customers here we are talking about are basically previous customers.\n\nWe will train out data on Multiple Classification Algos and will use the best model with highest Accuracy.\n","93baa282":"# Predicting the Test set results","7541710b":"# Visualising the Training Set Results for Decision Tree","201baa87":"# Classification Report & Confusion Matrix for Logistic Regression","aef4e176":"# Visualising the Test Set Results for Decision Tree","aebb663f":"# Feature Scaling","c18311a7":"# Visualising the Test Set Results for KNN","b80921a7":"# Now we are going to implement Support Vector Machine (SVM)"}}