{"cell_type":{"a5478253":"code","38c14041":"code","da2f4726":"code","d45e2175":"code","38f7a754":"code","3488ff42":"code","b7898817":"code","b765949b":"code","286f1b0b":"code","657c8a2e":"code","6d98108a":"code","4141833a":"code","5385b285":"code","9c5f4aed":"code","c1dbe015":"code","c89f3e20":"code","614824bb":"code","ff25e113":"code","4f72314d":"code","0d80e71c":"code","27c4ec8d":"code","01502b78":"code","b850b9ad":"markdown","4d99a24e":"markdown","d3a825d6":"markdown","9500be8b":"markdown","b8d0262d":"markdown","553955bc":"markdown","4c201b60":"markdown","612b2779":"markdown","49c34f1e":"markdown","2f3af266":"markdown"},"source":{"a5478253":"# import the necessary packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nplt.style.use(\"seaborn-whitegrid\")\nwarnings.filterwarnings(\"ignore\")","38c14041":"# load data\ndata = \"..\/input\/insurance\/insurance.csv\"\ndf = pd.read_csv(data)\n\n# show data (6 row)\ndf.head(6)","da2f4726":"df.describe().T","d45e2175":"df.info()","38f7a754":"df.columns[df.isnull().any()]","3488ff42":"df.isnull().sum()","b7898817":"data = df.copy()\ndata = data.select_dtypes(include=[\"float64\",\"int64\"])\ndata.head()","b765949b":"column_list = ['age', 'bmi', 'children', 'charges']\nfor col in column_list:\n    sns.boxplot(x = data[col])\n    plt.xlabel(col)\n    plt.show()","286f1b0b":"f= plt.figure(figsize=(16,5))\n\nax=f.add_subplot(121)\nsns.distplot(df['charges'],bins=50,color='r',ax=ax)\nax.set_title('Distribution of insurance charges')\n\nax=f.add_subplot(122)\nsns.distplot(np.log10(df['charges']),bins=40,color='b',ax=ax)\n\nplt.show()","657c8a2e":"f = plt.figure(figsize=(14,6))\nax = f.add_subplot(121)\nsns.violinplot(x='sex', y='charges',data=df,palette='Wistia',ax=ax)\nax.set_title('Violin plot of Charges vs sex')\n\nax = f.add_subplot(122)\nsns.violinplot(x='smoker', y='charges',data=df,palette='magma',ax=ax)\n\nplt.show()","6d98108a":"sns.jointplot(x=\"bmi\",y=\"charges\",data=df,kind=\"reg\")\nplt.show()","4141833a":"sns.jointplot(x=\"age\",y=\"charges\",data=df,kind=\"reg\")\nplt.show()","5385b285":"sns.jointplot(x=\"children\",y=\"charges\",data=df,kind=\"reg\")\nplt.show()","9c5f4aed":"# import the necessary packages\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import boxcox\nfrom sklearn import metrics\n\ndf_encode = df.copy()","c1dbe015":"df_encode = pd.get_dummies(data = df_encode, columns = ['sex','smoker','region'])\ndf_encode.head()","c89f3e20":"# normalization\ny_bc,lam, ci= boxcox(df_encode['charges'],alpha=0.05)\ndf_encode['charges'] = np.log(df_encode['charges'])\n\ndf_encode.head()","614824bb":"X = df_encode.drop('charges',axis=1) \ny = df_encode['charges']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=23)","ff25e113":"X = df_encode['bmi'].values.reshape(-1,1)  # Independet variable\ny = df_encode['charges'] # dependent variable\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)\nlin_reg = LinearRegression()\nmodel = lin_reg.fit(X_train,y_train)\npredictions = lin_reg.predict(X_test)\n\nprint(\"intercept: \", model.intercept_)\nprint(\"coef: \", model.coef_)\nprint(\"RScore. \", model.score(X_test,y_test))","4f72314d":"plt.figure(figsize=(12,6))\nplt.scatter(y_test,predictions)\nplt.show()","0d80e71c":"print('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))","27c4ec8d":"plt.figure(figsize=(12,6))\ng = sns.regplot(x=df_encode['bmi'],y=df_encode[\"charges\"],ci=None,scatter_kws = {'color':'r','s':9})\ng.set_title(\"Model Equation\")\ng.set_ylabel(\"charges\")\ng.set_xlabel('bmi')\nplt.show()","01502b78":"plt.figure(figsize=(12,6))\ng = sns.regplot(x=df_encode['age'],y=df_encode[\"charges\"],ci=None,scatter_kws = {'color':'r','s':9})\ng.set_title(\"Model Equation\")\ng.set_ylabel(\"charges\")\ng.set_xlabel('age')\nplt.show()","b850b9ad":"<img src=\"https:\/\/cdn.lynda.com\/course\/721905\/721905-637286247907951322-16x9.jpg\" \/>\n\n**Why is normalization done?** <br>\nNormalization has two main purposes. Eliminating data duplication in the database and increasing data consistency (accuracy).\n\nNormalization is applied to databases with levels (normal forms). In order to be able to say that a database is suitable for any of these normal forms, it must fulfill all the criteria of the normal form in question.\n\nWhen successfully implemented, the normalization process greatly increases the speed of the database.","4d99a24e":"- We see that the average age of the people included in the data is 39. The standard deviation of the age value is 14. That is, the age distribution is generally between 25 and 64. When we examine the Mix and Max points, we see that there are outliers in the data.\n- According to the BMI value, the number of obese and overweight people is high in this data set.\n- In general, the majority of people in the dataset have at least 1 child.","d3a825d6":"Our above-seen dataset is excellent. There is no missing data in our data set.","9500be8b":"# Linear Regressions\n\n<img src=\"https:\/\/cdn.lynda.com\/course\/645049\/645049-637286229812196095-16x9.jpg\" \/>\n\nIn statistics, linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression.\n\ny = b0 + b1*x\nb0 = constant\nb1 = coeff\nx = value\n\n### Advantages\n- Linear Regression is simple to implement and easier to interpret the output coefficients.\n- When you know the relationship between the independent and dependent variable have a linear relationship, this algorithm is the best to use because of it\u2019s less complexity to compared to other algorithms.\n- Linear Regression is susceptible to over-fitting but it can be avoided using some dimensionality reduction techniques, regularization (L1 and L2) techniques and cross-validation.\n\n### Disadvantages\n- On the other hand in linear regression technique outliers can have huge effects on the regression and boundaries are linear in this technique.\n- Diversely, linear regression assumes a linear relationship between dependent and independent variables. That means it assumes that there is a straight-line relationship between them. It assumes independence between attributes.\n- But then linear regression also looks at a relationship between the mean of the dependent variables and the independent variables. Just as the mean is not a complete description of a single variable, linear regression is not a complete description of relationships among variables.\n\n\n### Linear Regression Usage Areas\nLinear Regression is a rather ubiquitous curve fitting and machine learning technique that\u2019s used everywhere from scientific research teams to stock markets. Some uses:\n- Studying engine performance from test data in automobiles\n- Least squares regression is used to model causal relationships between parameters in biological systems\n- OLS regression can be used in weather data analysis\n- Linear regression can be used in market research studies and customer survey results analysis\n- Linear regression is used in observational astronomy commonly enough. A number of statistical tools and methods are used in astronomical data analysis, and there are entire libraries in languages like Python meant to do data analysis in astrophysics.","b8d0262d":"# Data Preparation \n## Inconsistent Observation\n95% of a machine learning model is said to be preprocessing and 5% is model selection. For this we need to teach the data to the model correctly. In order to prepare the available data for machine learning, we must apply certain pre-processing methods. One of these methods is the analysis of outliers. The outlier is any data point that is substantially different from the rest of the observations in a data set. In other words, it is the observation that goes far beyond the general trend.\n<img src=\"https:\/\/miro.medium.com\/max\/854\/1*RW-vfIbKZh-UGsLfTAWpyw.png\" \/>\n\nOutlier values behave differently from other data models and they increase the error with overfitting, so the outlier model must be detected and some operations must be performed on it.\n\n\nWe can see contradictory observations with many visualization techniques. One of them is the box chart. If there is an outlier, this is drawn as the point, but the other population is grouped together and displayed in boxes.","553955bc":"## Missing Value\n\nWe may get wrong results when implementing machine learning due to missing data in the dataset. Therefore, we need to detect and adjust the missing data in the data set.","4c201b60":"# Load and Check Data \ud83e\uddd4 <a href=\"1\"><\/a>\n<img src=\"https:\/\/i1.wp.com\/edulastic.com\/wp-content\/uploads\/sites\/2\/2017\/04\/ebookdribbble7.gif?fit=470%2C353&ssl=1\" width=\"100%\"\/>","612b2779":"## Variable Description\n\n**Columns:** <br>\n- **age:** age of primary beneficiary\n- **sex:** insurance contractor gender, female, male\n- **bmi:** Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg \/ m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9\n- **children:** Number of children covered by health insurance \/ Number of dependents\n- **smoker:** Smoking\n- **region:** the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n- **charges:** Individual medical costs billed by health insurance","49c34f1e":"When the charts above are examined, it is seen that there are outliers in bmi and charges values. However, these outliers do not harm our data set. On the contrary, these data allow us to comment on the data more easily. Therefore, we do not apply any process to this data.","2f3af266":"The Dummy variable trap is a scenario in which the independent variable are multicollinear, a scenario in which two or more variables are highly correlated in simple term one variable can be predicted from the others.\n\nBy using pandas get_dummies function we can do all above three step in line of code. We will this fuction to get dummy variable for sex, children,smoker,region features. By setting drop_first =True function will remove dummy variable trap by droping one variable and original variable.The pandas makes our life easy."}}