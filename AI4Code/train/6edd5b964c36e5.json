{"cell_type":{"a6589c18":"code","21aa089a":"code","58e221ec":"code","09a2d4e7":"code","01f5791b":"code","1f436569":"code","cef49534":"code","a757b3ea":"code","ebff97d4":"code","ad2b5b70":"code","5bfb69dd":"code","33055034":"code","cb2a0c83":"code","2a6bf8da":"code","5dd4a80d":"markdown","a7031cc5":"markdown","a5cdb677":"markdown","c6a4069f":"markdown","f06c0c0e":"markdown","ae0174b7":"markdown","3e048685":"markdown","d9ac71e0":"markdown","7f8d98bf":"markdown","1b1faf14":"markdown","1ba2b185":"markdown"},"source":{"a6589c18":"import kaggle\nimport os\nimport pandas as pd, numpy as np\nfrom datetime import datetime\nimport time\n\ndf = pd.read_csv('..\/input\/ATP.csv', low_memory=False)","21aa089a":"# what does it look like?\nprint(df.shape)\ndf.head()","58e221ec":"df.info()","09a2d4e7":"# these variables do not seem relevant to me. might be assessed in a further work\ndf = df.drop(columns=['tourney_id','tourney_name','tourney_date','match_num','winner_entry','loser_entry','winner_id','winner_name','score','loser_id','loser_name'])\n\n# convert numeric varibales to the correct type (csv_read fct does not make auto convert)\ncol_names_to_convert = ['winner_seed','draw_size','winner_ht','winner_age','winner_rank','winner_rank_points',\n                       'loser_seed','loser_ht','loser_age','loser_rank','loser_rank_points','best_of','minutes',\n                       'w_ace','w_df','w_svpt','w_1stIn','w_1stWon','w_2ndWon','w_SvGms','w_bpSaved','w_bpFaced',\n                       'l_ace','l_df','l_svpt','l_1stIn','l_1stWon','l_2ndWon','l_SvGms','l_bpSaved','l_bpFaced'\n                       ]\nfor col_name in col_names_to_convert:\n    df[col_name] = pd.to_numeric(df[col_name], errors='coerce')","01f5791b":"df.describe().transpose()","1f436569":"# append a new target variable with the code assigned to winner player (0 when P1 | 1 when P2)\n# For this set of data, the winner is always P1, so append 0s to the target variable\ndf['target'] = np.zeros(df.shape[0], dtype = int)","cef49534":"# Now we'll generate the second batch of data, ie, by switching P1 and P2. The winner this time will be P2, and the target variable =1\n# generate data by switching among P1 and P2 (target will be P2)\ndf2 = df.copy()\n# switch between variables from P1 and those from P2\ndf2[['winner_seed','winner_hand','winner_ht','winner_ioc','winner_age','winner_rank','winner_rank_points']] = df[['loser_seed','loser_hand','loser_ht','loser_ioc','loser_age','loser_rank','loser_rank_points']]\ndf2[['loser_seed','loser_hand','loser_ht','loser_ioc','loser_age','loser_rank','loser_rank_points']] = df[['winner_seed','winner_hand','winner_ht','winner_ioc','winner_age','winner_rank','winner_rank_points']]\ndf2[['w_ace','w_df','w_svpt','w_1stIn','w_1stWon','w_2ndWon','w_SvGms','w_bpSaved','w_bpFaced']] = df[['l_ace','l_df','l_svpt','l_1stIn','l_1stWon','l_2ndWon','l_SvGms','l_bpSaved','l_bpFaced']]\ndf2[['l_ace','l_df','l_svpt','l_1stIn','l_1stWon','l_2ndWon','l_SvGms','l_bpSaved','l_bpFaced']] = df[['w_ace','w_df','w_svpt','w_1stIn','w_1stWon','w_2ndWon','w_SvGms','w_bpSaved','w_bpFaced']]\ndf2['target'] = np.ones(df2.shape[0], dtype = int)\n\ndf = df.append(df2)","a757b3ea":"df.head(2).append(df.tail(2))","ebff97d4":"from sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\ndf['surface'] = lb.fit_transform(df['surface'].astype(str))\ndf['tourney_level'] = lb.fit_transform(df['tourney_level'].astype(str))\ndf['winner_hand'] = lb.fit_transform(df['winner_hand'].astype(str))\ndf['loser_hand'] = lb.fit_transform(df['loser_hand'].astype(str))\ndf['round'] = lb.fit_transform(df['round'].astype(str))\ndf['winner_ioc'] = lb.fit_transform(df['winner_ioc'].astype(str))\ndf['loser_ioc'] = lb.fit_transform(df['loser_ioc'].astype(str))","ad2b5b70":"# replace nan with 0 and infinity with large values\ndf = df.fillna(df.median())","5bfb69dd":"# subsample for test purpose : TODO: REMOVE FOR FINAL RUN\ndf = df.sample(100000)\n\n# split train\/test subsets (80% train, 20% test)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df.target, test_size=.2, random_state=0)","33055034":"# import classifiers from sklearn\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.linear_model import PassiveAggressiveClassifier\n\n# set names and prepare the benchmark list\nnames = [\"K Near. Neighb.\", \"Decision Tree\", \"Random Forest\", \"Naive Bayes\", \"Quad. Dis. Analys\", \"AdaBoost\", \n         \"Neural Net\" #, \"RBF SVM\", \"Linear SVM\", \"Ridge Classifier\"\n        ]\n\nclassifiers = [\n    KNeighborsClassifier(10),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis(),\n    AdaBoostClassifier(),\n    MLPClassifier(alpha=1, max_iter=1000)\n    # too long run for the test\n    #SVC(gamma=2, C=1),\n    #SVC(kernel=\"linear\", C=.025),\n    #RidgeClassifier(tol=.01, solver=\"lsqr\")\n]","cb2a0c83":"# init time \ntim = time.time()\nprint('Learn. model\\t\\t score\\t\\t\\ttime')\nscores = []\n\nfor name, clf in zip(names, classifiers):\n        print(name, end='')\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n        print('\\t\\t', round(score, 3), '%', '\\t\\t', round(time.time() - tim, 3))\n        scores.append(score)\n        tim = time.time()\n","2a6bf8da":"# plot results\nimport matplotlib.pyplot as plt\n\nplt.rcdefaults()\n\ny_pos = np.arange(len(names))\n\nplt.bar(y_pos, scores, align='center', alpha=0.5)\nplt.xticks(y_pos, names, rotation='vertical')\nplt.ylabel('Accuracy')\nplt.title('Model comparison for ATP prediction')\n\nplt.show()","5dd4a80d":"Seems we are dealing with a fair quantity of data, with a significant number of variables. But what about the quality? let's get a little closer to the matrix to get the number of NaN occurrences, types of variables..","a7031cc5":"The purpose of this contest is to maximize the likelihood to bid on the winner, in tennis tourneys. Let's get data and see what we can do toward this goal..\n\n# STEP1: Explore\nLoad data and print general overview:","a5cdb677":"Here is what data look like after transformation","c6a4069f":"Let's see what we got","f06c0c0e":"We can now launch the learning step with the selected classifiers, then the test step on the heldout data. Accuracy score is returned for each","ae0174b7":"Start by splitting train\/test subsets","3e048685":"To meet algorithm's expectation, we need to encode the categorical variables, like this:","d9ac71e0":"The prupose is to maximize likelihood to bet on the winner. For a better understanding and modeling of the \"winning pattern\", I would make a deep transformation to the data: an observation, for me, should be a match configuration with Plyer1's variables (whether winner or loser), Player2's variables, along with the variables that are not dependent on either players (eg, surface). The predicted target variable will refer to the winner (Player1 or Player2). Then, a classifier can be easily learnt from these.\nFor convenience, the \"winner\" will be denoted P1 (target=0), and the \"loser\" P2 (target=1).","7f8d98bf":"# STEP2: Predict\n","1b1faf14":"# STEP3: Go Further..\nDespite being learnt on a small subset of data, the tested models achived fairly good results and are way superior to the baseline classifier. For example, a simple perceptron achives 76% accuracy, which means, it only misses 1 out of 4 bids!\n\n\nHowever, this remains a humble first dive into the data. Number of things have been bypassed due to time circumstances. Here are some further investigations one could perform for better and hopefully more accurate predictors:\n- Investigate whether a bias exist ude to the large time period (1968--2017). A straightforward technique to lower the effect of the time bias consists of weighting observations wrt their time\/date of occurrence (most recent ones are more important).\n- Include the discaded variables, and use variable scaler (to corect the differences in scale) along with more adapted techniques for impacting the missing values.\n- Evaluate and select the most relevant features (feature selection) and explore correlation among them.\n- Fine-tune algorithm's parameters, for example with the grid search interface available from scikit-learn.","1ba2b185":"Last, we should not forget about the NaN. As a first approach, let's just fill them with the median value of each column"}}