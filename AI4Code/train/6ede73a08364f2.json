{"cell_type":{"704f82e6":"code","8e67eb50":"code","85248739":"code","ffaf06d7":"code","624c5d28":"code","6e5f3ca6":"code","4c98b878":"code","5fd44574":"code","6fd57acc":"code","3d224f97":"code","3e4f405b":"code","82050e0c":"code","e9073314":"code","2ac879bc":"code","8b10c577":"code","fa85a17a":"code","3b917335":"code","4ba70e61":"code","e40f7b12":"code","1530013b":"code","9ab85d34":"code","7df5827c":"code","bb840ad6":"code","92e7107c":"code","be8971eb":"code","3ccfdecc":"code","03045c95":"code","5062d524":"code","03fb0ea6":"code","a5d036fe":"code","db58bbed":"code","87faa193":"code","97b53dec":"code","0216f4f5":"code","4dd5c58f":"code","5d080090":"code","33adeadd":"code","665b882c":"code","eaf6fb27":"code","36818014":"code","6151679f":"code","8d048312":"code","32b04441":"code","78812ce6":"code","ef06a8ee":"code","33263521":"code","9b837a37":"code","bad0e2c6":"code","86add7dc":"code","1298024a":"code","bbe74b84":"code","40aad1b1":"code","38b72b2e":"code","e1332dab":"code","0ed59263":"code","e9f0ba06":"code","139da19d":"code","49222157":"code","4cc9050e":"code","58d39259":"code","f2739e71":"code","9d9cc257":"code","c3e3d152":"code","84711f72":"code","87c62cf8":"code","c9ce9db7":"code","ce088452":"code","19475f64":"code","0fdb6139":"code","9cc6eff2":"code","885c9ba4":"code","ecbb979b":"code","e31ed82b":"code","668f9121":"code","1ddd915b":"code","8c3a60a5":"code","fd0d4b23":"code","f71741b5":"code","87bd669a":"code","195705e4":"code","310e62de":"code","09348993":"code","1d8eeba1":"code","e75850e4":"code","3e6bd8c1":"code","d92c3321":"code","810422c1":"code","b3fa01bf":"code","59d0fa9c":"code","67b195c7":"code","be7e1001":"code","2f939123":"code","bc2980fe":"code","fd24a73f":"code","1fd46e88":"code","b6e1d123":"code","18f5bf3e":"code","f4e454c6":"code","ae6d1a9f":"code","e1ad9354":"code","58705f76":"code","205eb1ad":"code","251e90f3":"code","cc172d93":"code","2621ecbe":"code","b6b95f16":"code","4dc2fb0f":"code","6de881af":"code","2930c785":"code","5b66b48d":"code","601271c1":"code","c9938a25":"code","ff6d9071":"code","ceb9cb92":"code","0884a91a":"code","715b732f":"code","64c9e441":"code","19356d84":"code","e87b1032":"code","96541604":"code","818ec78c":"code","add333b1":"code","720325de":"code","78a4ed44":"code","3dec7708":"code","df4f4414":"code","c28cbbf8":"code","2defd39e":"code","7fcecf2c":"code","e3b5af08":"code","c0a5f80e":"code","a2af17da":"code","d1dd8827":"code","c5b3d6a5":"code","d97e2393":"code","a93fe492":"code","b8217e5a":"code","c4f8aace":"code","45ce6c85":"code","5a797d1a":"code","7d4f7d5e":"code","bda520e4":"code","e58a7f8a":"code","4b50c0b1":"code","c10bd864":"code","555a2263":"code","ad1890b3":"code","5dbaee42":"code","2f40ee78":"code","17c1acd7":"code","a3260e8f":"code","10e63f84":"code","f047d021":"code","e9a0c361":"code","b9ea5482":"code","e8a8cec9":"code","dda5bc3e":"code","f88010bc":"code","146a4d07":"code","a5fac0dd":"code","c4f478f3":"code","26bd711e":"code","2be5177f":"code","168a566f":"code","6752a06f":"code","87103614":"markdown","c6244e4e":"markdown","ea3bc40e":"markdown","89751755":"markdown","00375369":"markdown","d454c439":"markdown","5ee7f3c2":"markdown","1223e085":"markdown","59c2f41b":"markdown","0312ad84":"markdown","1af82a19":"markdown","364f65f7":"markdown","97231560":"markdown","90b4f285":"markdown","bc966591":"markdown","28abf75e":"markdown","48a81470":"markdown","5bc84eae":"markdown","ef0f8919":"markdown","9fdf8332":"markdown","9a6e1510":"markdown","6e3572bd":"markdown","908aeb44":"markdown","3454c977":"markdown","a1ce4661":"markdown","b6100186":"markdown","44cf392d":"markdown","96f27187":"markdown","73a63c24":"markdown","c6a3d14e":"markdown","518cce29":"markdown","f5b39c80":"markdown","5c71f209":"markdown","9e829075":"markdown","56b980eb":"markdown","8ce265bb":"markdown","7c8e3606":"markdown","0df64959":"markdown","09e32dca":"markdown","13d6d352":"markdown","afeeaccd":"markdown","1b694067":"markdown","b61913ae":"markdown","8e14e64c":"markdown","4ee6c811":"markdown","77de3a35":"markdown","bac5e58f":"markdown","769fbbfe":"markdown","373751c7":"markdown","e3853171":"markdown","f3afb709":"markdown","a0a1068f":"markdown","bbe2c6d2":"markdown","ab8ae271":"markdown","a0d0ba66":"markdown","26610c7d":"markdown","52af5c35":"markdown","0abec5fb":"markdown","63915b2d":"markdown","3cdba162":"markdown","1c8b932e":"markdown","b9a7e7a9":"markdown","7fd3d048":"markdown","ac246d38":"markdown","edf63792":"markdown","36d0b0ae":"markdown","2a876f24":"markdown","ff85ce52":"markdown","a9f4eb6e":"markdown","8f2f280a":"markdown","2eca9287":"markdown","82e28f77":"markdown","48359c9e":"markdown","fb41d592":"markdown","26d1ff67":"markdown","59321405":"markdown","8bc16478":"markdown","9561bbea":"markdown","02aa1630":"markdown","c2d17e70":"markdown","0aa4e582":"markdown","43a86309":"markdown","35cbada6":"markdown","8b1828c5":"markdown","373e1235":"markdown","5070555c":"markdown","743494af":"markdown","b8efea71":"markdown","18bfd9c0":"markdown","6fe707b6":"markdown","4df7cf00":"markdown","44363d0d":"markdown","9dceaa12":"markdown"},"source":{"704f82e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8e67eb50":"# Filtering out the warnings\n\nimport warnings\n\nwarnings.filterwarnings('ignore')","85248739":"# Importing the required libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport itertools\nimport matplotlib.style as style\nfrom mpl_toolkits.mplot3d import Axes3D\nimport os # accessing directory structure\nimport plotly\nimport plotly.express as px\n# graphs to be inline\n%matplotlib inline\n# setting up plot style \nstyle.use('seaborn-poster')\nstyle.use('Solarize_Light2')","ffaf06d7":"#setting display format\n\npd.options.display.float_format='{:.2f}'.format\npd.set_option('display.max_rows',50)\npd.set_option('display.max_columns',122)","624c5d28":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6e5f3ca6":"# reading csv file and read first five rows\napplication_data=pd.read_csv('\/kaggle\/input\/loan-defaulter\/application_data.csv',na_values='XNA')\nprint(application_data.head())\n","4c98b878":"# reading csv file and read first five rows\nprevious_data=pd.read_csv('\/kaggle\/input\/loan-defaulter\/previous_application.csv',na_values='XNA')\nprint(previous_data.head())\n","5fd44574":"# Database dimension\nprint(\"Dimensions of application_data     :\",application_data.shape)\nprint(\"Dimensions of previous_data        :\",previous_data.shape)\n\n#Database size\nprint(\"Size of application_data          :\",application_data.size)\nprint(\"Size of previous_data             :\",previous_data.size)","6fd57acc":"# Understanding the datatypes and info\nprint(application_data.info(verbose=True))","3d224f97":"# Understanding the datatypes and info of previous data\nprevious_data.info(verbose=True)","3e4f405b":"# Checking the columns with float datatype\napplication_data.select_dtypes('float').info()","82050e0c":"# Checking the columns with int datatype\napplication_data.select_dtypes(np.number,exclude='float').info()","e9073314":"# Checking the columns with object datatype\napplication_data.select_dtypes(exclude=np.number).info()","2ac879bc":"# checking datatypes for the columns in previous_data\nprevious_data.info()","8b10c577":"#checking for duplicate rows using SK_ID_CURR which is the unique key in application data\napplication_data.SK_ID_CURR.duplicated().sum()","fa85a17a":"#checking for duplicate rows using SK_ID_PREV which is the unique key in previous data\nprevious_data.SK_ID_PREV.duplicated().sum()","3b917335":"#Describing application data\napplication_data.describe()","4ba70e61":"#Describing previous data\nprevious_data.describe()","e40f7b12":"# Percentage null value calculation for each column\nround(application_data.isnull().sum() \/ application_data.shape[0] * 100.00,2)","1530013b":"# finding  null values\napplication_data.isnull().sum()","9ab85d34":"# nullvalue calcultion for integer datatype columns  \napplication_data.select_dtypes(np.number,exclude='float').isnull().sum()","7df5827c":"application_data.select_dtypes(np.number,exclude='int64').isnull().sum()","bb840ad6":"# nullvalue calcultion for float datatype columns  \nround(application_data.select_dtypes(np.number,exclude='int64').isnull().sum()\/ application_data.shape[0] * 100.00,2)","92e7107c":"# plotting pointplot for Percentage of Missing values with float datatype in application data\nnull_application_data = pd.DataFrame((application_data.select_dtypes(np.number,exclude='int64').isnull().sum())*100\/application_data.shape[0]).reset_index()\nnull_application_data.columns = ['Column Name', 'Null Values Percentage']\nfig = plt.figure(figsize=(18,6))\nax = sns.pointplot(x=\"Column Name\",y=\"Null Values Percentage\",data=null_application_data,orient='v',color='green')\nplt.xticks(rotation =90,fontsize =7)\nax.axhline(50, ls='--',color='red')\nplt.title(\"Percentage of Missing values with float datatype in application data\")\nplt.ylabel(\"Null Values PERCENTAGE\")\nplt.xlabel(\"COLUMNS\")\nplt.show()","be8971eb":"# more than or equal to 50% null value columns with float datatype\nnullcol_application_data = null_application_data[null_application_data[\"Null Values Percentage\"]>=50]\nnullcol_application_data.reset_index()","3ccfdecc":"# percentage null value calculation for each categorical column in application data\nround(application_data.select_dtypes(exclude=np.number).isnull().sum()\/ application_data.shape[0] * 100.00,2)","03045c95":"# plotting pointplot for Percentage of Missing values with object datatype in application data\nnull_application_data = pd.DataFrame((application_data.select_dtypes(exclude=np.number).isnull().sum())*100\/application_data.shape[0]).reset_index()\nnull_application_data.columns = ['Column Name', 'Null Values Percentage']\nfig = plt.figure(figsize=(18,6))\nax = sns.pointplot(x=\"Column Name\",y=\"Null Values Percentage\",data=null_application_data,orient='v',color='green')\nplt.xticks(rotation =90,fontsize =7)\nax.axhline(50, ls='--',color='red')\nax.axhline(15, ls='--',color='red')\nplt.title(\"Percentage of Missing values with object datatype in application data\")\nplt.ylabel(\"Null Values PERCENTAGE\")\nplt.xlabel(\"COLUMNS\")\nplt.show()","5062d524":"# percentage null value calculation for each categorical column in application data\nround(application_data.select_dtypes(exclude=np.number).isnull().sum()\/ application_data.shape[0] * 100.00,2)","03fb0ea6":"# checking null values for categorical variables in application data\nprint(application_data.select_dtypes(exclude=np.number).isnull().sum())","a5d036fe":"# Percentage null value calculation for each column in previous data\nround(previous_data.isnull().sum() \/ previous_data.shape[0] * 100.00,2)","db58bbed":"null_previous_data =pd.DataFrame((previous_data.isnull().sum())*100\/previous_data.shape[0]).reset_index()\nnull_previous_data.columns = ['Column Name', 'Null Values Percentage']\nfig = plt.figure(figsize=(18,6))\nax = sns.pointplot(x=\"Column Name\",y=\"Null Values Percentage\",data=null_previous_data,orient='v',color='green')\nplt.xticks(rotation =90,fontsize =7)\nax.axhline(50, ls='--',color='red')\nax.axhline(15, ls='--',color='red')\nplt.title(\"Percentage of Missing values previous data\")\nplt.ylabel(\"Null Values PERCENTAGE\")\nplt.xlabel(\"COLUMNS\")\nplt.show()","87faa193":"# more than or equal to 50% empty columns\nnullcol_previous = null_previous_data[null_previous_data[\"Null Values Percentage\"]>=50]\nprint(nullcol_previous.reset_index())\nprint('length :',len(nullcol_previous))","97b53dec":"len(nullcol_application_data)","0216f4f5":"#creating list of unnecessary columns in application data\nunwanted_application = nullcol_application_data[\"Column Name\"].tolist()\nunwanted_application\n#len(unwanted_application)","4dd5c58f":"#updating unwanted application list with the object type variables possessing 40% and more missing values\nunwanted_application= unwanted_application+ ['FONDKAPREMONT_MODE','HOUSETYPE_MODE','WALLSMATERIAL_MODE']","5d080090":"\ncol_doc = [ 'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3','FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6','FLAG_DOCUMENT_7', \n           'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9','FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12','FLAG_DOCUMENT_13',\n           'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15','FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18',\n           'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21']\ndf_flag = application_data[col_doc+[\"TARGET\"]]\n\nlength = len(col_doc)\n\ndf_flag[\"TARGET\"] = df_flag[\"TARGET\"].replace({1:\"Defaulter\",0:\"Repayer\"})\n\nfig = plt.figure(figsize=(21,24))\n\nfor i,j in itertools.zip_longest(col_doc,range(length)):\n    plt.subplot(5,4,j+1)\n    ax = sns.countplot(df_flag[i],hue=df_flag[\"TARGET\"],palette=[\"y\",\"b\"])\n    plt.yticks(fontsize=8)\n    plt.xlabel(\"\")\n    plt.ylabel(\"\")\n    plt.title(i)","33adeadd":"#updating unnecessary columns in application data\ncol_doc.remove('FLAG_DOCUMENT_3')\nunwanted_application= unwanted_application+ col_doc\nlen(unwanted_application)","665b882c":"\n# checking is there is any correlation between mobile phone, work phone etc, email, Family members and Region rating\ncontact_col = ['FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE',\n       'FLAG_PHONE', 'FLAG_EMAIL','TARGET']\nContact_corr = application_data[contact_col].corr()\nfig = plt.figure(figsize=(8,8))\nax = sns.heatmap(Contact_corr,\n            xticklabels=Contact_corr.columns,\n            yticklabels=Contact_corr.columns,\n            annot = True,\n            cmap =\"RdYlGn\",\n            linewidth=1)","eaf6fb27":"#updating unwanted application list with six contact variables\ncontact_col.remove('TARGET') \nunwanted_application = unwanted_application + contact_col\nlen(unwanted_application)","36818014":"# Dropping the irrelevant columns from applicationDF\napplication_data.drop(labels=unwanted_application,axis=1,inplace=True)","6151679f":"# Inspecting the dataframe after removal of unnecessary columns\napplication_data.shape","8d048312":"# Getting the 13 columns which has more than 40% unknown\nUnwanted_previous = nullcol_previous[\"Column Name\"].tolist()\nUnwanted_previous","32b04441":"# Listing down columns which are not needed\nUnnecessary_previous = ['WEEKDAY_APPR_PROCESS_START','HOUR_APPR_PROCESS_START',\n                        'FLAG_LAST_APPL_PER_CONTRACT','NFLAG_LAST_APPL_IN_DAY','DAYS_FIRST_DRAWING',\n                        'DAYS_FIRST_DUE','DAYS_LAST_DUE_1ST_VERSION','DAYS_LAST_DUE','DAYS_TERMINATION','NFLAG_INSURED_ON_APPROVAL']","78812ce6":"Unwanted_previous = Unwanted_previous + Unnecessary_previous\nlen(Unwanted_previous)","ef06a8ee":"# Dropping the Irrelevant columns from previous data\nprevious_data.drop(labels=Unwanted_previous,axis=1,inplace=True)\n# Inspecting the dataframe after removal of Irrelevant columns\nprevious_data.shape","33263521":"# Converting Negative days to positive days\ndate_col = ['DAYS_BIRTH','DAYS_EMPLOYED','DAYS_REGISTRATION','DAYS_ID_PUBLISH']\n\nfor col in date_col:\n    application_data[col] = abs(application_data[col])","9b837a37":"# Binning Numerical Columns to create a categorical column\n\n# Creating bins for income amount\napplication_data['AMT_INCOME_TOTAL']=application_data['AMT_INCOME_TOTAL']\/100000\n\nbins = [0,1,2,3,4,5,6,7,8,9,10,11]\nslot = ['0-100K','100K-200K', '200k-300k','300k-400k','400k-500k','500k-600k','600k-700k','700k-800k','800k-900k','900k-1M', '1M Above']\n\napplication_data['AMT_INCOME_RANGE']=pd.cut(application_data['AMT_INCOME_TOTAL'],bins,labels=slot)","bad0e2c6":"application_data['AMT_INCOME_RANGE'].value_counts(normalize=True)*100","86add7dc":"# Creating bins for Credit amount\napplication_data['AMT_CREDIT']=application_data['AMT_CREDIT']\/100000\n\nbins = [0,1,2,3,4,5,6,7,8,9,10,100]\nslots = ['0-100K','100K-200K', '200k-300k','300k-400k','400k-500k','500k-600k','600k-700k','700k-800k',\n       '800k-900k','900k-1M', '1M Above']\n\napplication_data['AMT_CREDIT_RANGE']=pd.cut(application_data['AMT_CREDIT'],bins=bins,labels=slots)","1298024a":"#checking the binning of data and % of data in each category\napplication_data['AMT_CREDIT_RANGE'].value_counts(normalize=True)*100","bbe74b84":"# Creating bins for Age\napplication_data['AGE'] = application_data['DAYS_BIRTH'] \/\/ 365\nbins = [0,20,30,40,50,100]\nslots = ['0-20','20-30','30-40','40-50','50 above']\n\napplication_data['AGE_GROUP']=pd.cut(application_data['AGE'],bins=bins,labels=slots)","40aad1b1":"#checking the binning of data and % of data in each category\napplication_data['AGE_GROUP'].value_counts(normalize=True)*100","38b72b2e":"# Creating bins for Employement Time\napplication_data['YEARS_EMPLOYED'] = application_data['DAYS_EMPLOYED'] \/\/ 365\nbins = [0,5,10,20,30,40,50,60,150]\nslots = ['0-5','5-10','10-20','20-30','30-40','40-50','50-60','60 above']\n\napplication_data['EMPLOYMENT_YEAR']=pd.cut(application_data['YEARS_EMPLOYED'],bins=bins,labels=slots,include_lowest=True)","e1332dab":"application_data[application_data.EMPLOYMENT_YEAR.isnull()].head()","0ed59263":"#checking the binning of data and % of data in each category\napplication_data['EMPLOYMENT_YEAR'].value_counts(normalize=True)*100","e9f0ba06":"#Checking the number of unique values each column possess to identify categorical columns in application Data\napplication_data.nunique().sort_values()","139da19d":"#Checking the number of unique values each column possess to identify categorical columns\nprevious_data.nunique().sort_values() ","49222157":"#previous_data.CNT_PAYMENT=previous_data.CNT_PAYMENT.astype(int).value_counts()\nprevious_data.CNT_PAYMENT.unique()","4cc9050e":"#Converting negative days to positive days \nprevious_data['DAYS_DECISION'] = abs(previous_data['DAYS_DECISION'])","58d39259":"#age group calculation e.g. 388 will be grouped as 300-400\nprevious_data['DAYS_DECISION_GROUP'] = (previous_data['DAYS_DECISION']-(previous_data['DAYS_DECISION'] % 400)).astype(str)+'-'+ ((previous_data['DAYS_DECISION'] - (previous_data['DAYS_DECISION'] % 400)) + (previous_data['DAYS_DECISION'] % 400) + (400 - (previous_data['DAYS_DECISION'] % 400))).astype(str)","f2739e71":"previous_data['DAYS_DECISION_GROUP'].value_counts(normalize=True)*100","9d9cc257":"previous_data.info()","c3e3d152":"#Conversion of Object and Numerical columns to Categorical Columns\ncategorical_columns = ['NAME_CONTRACT_TYPE','CODE_GENDER','NAME_TYPE_SUITE','NAME_INCOME_TYPE','NAME_EDUCATION_TYPE',\n                       'NAME_FAMILY_STATUS','NAME_HOUSING_TYPE','OCCUPATION_TYPE','WEEKDAY_APPR_PROCESS_START',\n                       'ORGANIZATION_TYPE','FLAG_OWN_CAR','FLAG_OWN_REALTY','LIVE_CITY_NOT_WORK_CITY',\n                       'REG_CITY_NOT_LIVE_CITY','REG_CITY_NOT_WORK_CITY','REG_REGION_NOT_WORK_REGION',\n                       'LIVE_REGION_NOT_WORK_REGION','REGION_RATING_CLIENT','WEEKDAY_APPR_PROCESS_START',\n                       'REGION_RATING_CLIENT_W_CITY'\n                      ]\nfor col in categorical_columns:\n    application_data[col] =pd.Categorical(application_data[col])","84711f72":"# inspecting the column types if the above conversion is reflected\napplication_data.info()","87c62cf8":"#Converting Categorical columns from Object to categorical \nCatgorical_col_p = ['NAME_CONTRACT_STATUS','NAME_PAYMENT_TYPE',\n                    'CODE_REJECT_REASON','NAME_CLIENT_TYPE','NAME_PORTFOLIO'\n                ,'CHANNEL_TYPE','NAME_YIELD_GROUP','PRODUCT_COMBINATION',\n                    'NAME_CONTRACT_TYPE','DAYS_DECISION_GROUP']\n\nfor col in Catgorical_col_p:\n    previous_data[col] =pd.Categorical(previous_data[col])","c9ce9db7":"from plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nplt.figure(figsize=(22,10))\n\nfig = make_subplots(\n    rows=3, cols=3,\n    subplot_titles=(\"Plot 1\", \"Plot 2\", \"Plot 3\", \"Plot 4\", \"Plot 5\", \"Plot 6\",'', \"Plot 7\"))                    \n\nfig.add_trace(go.Box(y=application_data.AMT_ANNUITY,name='AMT_ANNUITY', boxmean=True),\n              row=1, col=1)\n\nfig.add_trace(go.Box(y=application_data.AMT_INCOME_TOTAL,name='AMT_INCOME_TOTAL',boxmean=True),\n              row=1, col=2)\n\nfig.add_trace(go.Box(y=application_data.AMT_CREDIT,name='AMT_CREDIT',boxmean=True),\n              row=1, col=3)\n\nfig.add_trace(go.Box(y=application_data.AMT_GOODS_PRICE,name='AMT_GOODS_PRICE',boxmean=True),\n              row=2, col=1)\n\nfig.add_trace(go.Box(y=application_data.DAYS_EMPLOYED,name='DAYS_EMPLOYED',boxmean=True),\n              row=2, col=2)\n\nfig.add_trace(go.Box(y=application_data.CNT_CHILDREN,name='CNT_CHILDREN',boxmean=True),\n              row=2, col=3)\n\nfig.add_trace(go.Box(y=application_data.DAYS_BIRTH,name='DAYS_BIRTH',boxmean=True),\n              row=3, col=2)\n\nfig.update_layout(height=700, width=900,\n                  title_text=\"Outlier Detection\",showlegend=False)\n\nfig.show()","ce088452":"# checking the null value % of each column in applicationDF dataframe\nround(application_data.isnull().sum() \/ application_data.shape[0] * 100.00,2)","19475f64":"application_data['NAME_TYPE_SUITE'].describe()","0fdb6139":"application_data['NAME_TYPE_SUITE'].fillna((application_data['NAME_TYPE_SUITE'].mode()[0]),inplace = True)","9cc6eff2":"application_data['OCCUPATION_TYPE'] = application_data['OCCUPATION_TYPE'].cat.add_categories('Unknown')\napplication_data['OCCUPATION_TYPE'].fillna('Unknown', inplace =True) ","885c9ba4":"application_data[['AMT_REQ_CREDIT_BUREAU_HOUR','AMT_REQ_CREDIT_BUREAU_DAY',\n               'AMT_REQ_CREDIT_BUREAU_WEEK','AMT_REQ_CREDIT_BUREAU_MON',\n               'AMT_REQ_CREDIT_BUREAU_QRT','AMT_REQ_CREDIT_BUREAU_YEAR']].describe()","ecbb979b":"# imputing with median value\namount = ['AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY','AMT_REQ_CREDIT_BUREAU_WEEK','AMT_REQ_CREDIT_BUREAU_MON',\n         'AMT_REQ_CREDIT_BUREAU_QRT','AMT_REQ_CREDIT_BUREAU_YEAR']\n\nfor col in amount:\n    application_data[col].fillna(application_data[col].median(),inplace = True)","e31ed82b":"# imputing EXT_SOURCE_2 with mean (0.50)\napplication_data.EXT_SOURCE_2.fillna(application_data.EXT_SOURCE_2.mean(),inplace=True)","668f9121":"application_data.EXT_SOURCE_2.isnull().sum()","1ddd915b":"# imputing EXT_SOURCE_3 with mean (0.50)\napplication_data.EXT_SOURCE_3.fillna(application_data.EXT_SOURCE_3.mean(),inplace=True)","8c3a60a5":"application_data.EXT_SOURCE_3.isnull().sum()","fd0d4b23":"application_data.EXT_SOURCE_3.head()","f71741b5":"application_data['CREDIT_SCORE']=application_data.apply(lambda x: (x.EXT_SOURCE_2 + x.EXT_SOURCE_3)\/3,axis=1)","87bd669a":"application_data['CREDIT_SCORE'].head(10)","195705e4":"# checking the null value % of each column in application data dataframe\nround(application_data.isnull().sum() \/ application_data.shape[0] * 100.00,2)","310e62de":"application_data.dropna(subset=['EMPLOYMENT_YEAR'],inplace=True)","09348993":"# checking the null value % of each column in application data dataframe\nround(application_data.isnull().sum() \/ application_data.shape[0] * 100.00,2)","1d8eeba1":"# checking the null value % of each column in previousDF dataframe\nround(previous_data.isnull().sum() \/ previous_data.shape[0] * 100.00,2)","e75850e4":"plt.figure(figsize=(6,6))\nsns.kdeplot(previous_data['AMT_ANNUITY'])\nplt.show()","3e6bd8c1":"previous_data['AMT_ANNUITY'].fillna(previous_data['AMT_ANNUITY'].median(),inplace = True)","d92c3321":"plt.figure(figsize=(6,6))\nsns.kdeplot(previous_data['AMT_GOODS_PRICE'][pd.notnull(previous_data['AMT_GOODS_PRICE'])])\nplt.show()","810422c1":"#plot the correlation matrix of AMT_CREDIT,RATE_DOWN_PAYMENT in previous_data dataframe.\nprevious_data[['AMT_CREDIT','AMT_GOODS_PRICE']].corr()","b3fa01bf":"sns.heatmap(previous_data[['AMT_CREDIT','AMT_GOODS_PRICE']].corr(),annot=True,cmap='Greens')","59d0fa9c":"sns.scatterplot(previous_data.AMT_CREDIT,previous_data.AMT_GOODS_PRICE)","67b195c7":"# imputing AMT_GOODS_PRICE with AMT_CREDIT\nprevious_data['AMT_GOODS_PRICE'].fillna(previous_data['AMT_CREDIT'],inplace = True)","be7e1001":"previous_data.loc[previous_data['CNT_PAYMENT'].isnull(),'NAME_CONTRACT_STATUS'].value_counts()","2f939123":"previous_data['CNT_PAYMENT'].fillna(0,inplace = True)","bc2980fe":"previous_data.NAME_PAYMENT_TYPE.value_counts()","fd24a73f":"#imputing with the mode\nprevious_data['NAME_PAYMENT_TYPE'].fillna((previous_data['NAME_PAYMENT_TYPE'].mode()[0]),inplace = True)","1fd46e88":"previous_data.NAME_PORTFOLIO.value_counts()","b6e1d123":"#imputing with the mode\nprevious_data['NAME_PORTFOLIO'].fillna((previous_data['NAME_PORTFOLIO'].mode()[0]),inplace = True)","18f5bf3e":"previous_data.NAME_YIELD_GROUP.value_counts()","f4e454c6":"#imputing with the mode\nprevious_data['NAME_YIELD_GROUP'].fillna((previous_data['NAME_YIELD_GROUP'].mode()[0]),inplace = True)","ae6d1a9f":"previous_data.NAME_CLIENT_TYPE.value_counts()","e1ad9354":"#imputing with the mode\nprevious_data['NAME_CLIENT_TYPE'].fillna((previous_data['NAME_CLIENT_TYPE'].mode()[0]),inplace = True)","58705f76":"previous_data.CODE_REJECT_REASON.value_counts()","205eb1ad":"#imputing with the mode\nprevious_data['CODE_REJECT_REASON'].fillna((previous_data['CODE_REJECT_REASON'].mode()[0]),inplace = True)","251e90f3":"#imputing with the mode\nprevious_data['NAME_CASH_LOAN_PURPOSE'].fillna((previous_data['NAME_CASH_LOAN_PURPOSE'].mode()[0]),inplace = True)","cc172d93":"# checking the null value % of each column in previousDF dataframe\nround(previous_data.isnull().sum() \/ previous_data.shape[0] * 100.00,2)","2621ecbe":"Imbalance = application_data[\"TARGET\"].value_counts().reset_index()\n\nplt.figure(figsize=(10,4))\nx= ['Repayer','Defaulter']\nsns.barplot(x,\"TARGET\",data = Imbalance,palette= ['g','r'])\nplt.xlabel(\"Loan Repayment Status\")\nplt.ylabel(\"Count of Repayers & Defaulters\")\nplt.title(\"Imbalance Plotting\")\nplt.show()","b6b95f16":"#function for plotting number of defaulters and repayers categorically(univariate) \nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\ndef univariate_hist(dataframe,feature,vertical=True):\n    \n    defaulters=dataframe[feature].where(dataframe.TARGET==1)\n    repayers=dataframe[feature].where(dataframe.TARGET==0)\n    \n    fig = go.Figure()\n    if(vertical):\n        \n        fig.add_trace(go.Histogram(x=defaulters,histnorm='',name='Defaulters', marker_color='crimson', opacity=0.75))\n        fig.add_trace(go.Histogram(x=repayers,histnorm='',name='Repayers',marker_color='green', opacity=0.75))\n        fig.update_layout(title_text='Univariate categoriacal Analysis: '+feature, # title of plot\n                  xaxis_title_text=feature, # xaxis label\n                  yaxis_title_text='COUNT', # yaxis label\n                  bargap=0.2, # gap between bars of adjacent location coordinates\n                  bargroupgap=0.1 # gap between bars of the same location coordinates\n                 )\n    else :\n        \n        fig.add_trace(go.Histogram(y=defaulters,histnorm='',name='Defaulters', marker_color='crimson', opacity=0.75))\n        fig.add_trace(go.Histogram(y=repayers,histnorm='',name='Repayers',marker_color='green', opacity=0.75))\n# The two histograms are drawn on top of another\n        fig.update_layout(title_text='Univariate categoriacal Analysis : '+feature, # title of plot\n                  xaxis_title_text='COUNT', # xaxis label\n                  yaxis_title_text= feature, # yaxis label\n                  bargap=0.2, # gap between bars of adjacent location coordinates\n                  bargroupgap=0.1 # gap between bars of the same location coordinates\n                 )\n    \n\n    fig.show()","4dc2fb0f":"# function for Percentage of Defaulters for categorical variables\ndef perc_defaulter(dataframe,feature,vertical=True):\n    \n    cat_perc = dataframe[[feature, 'TARGET']].groupby([feature],as_index=False).mean()\n    cat_perc[\"TARGET\"] = cat_perc[\"TARGET\"]*100\n    cat_perc.sort_values(by='TARGET', ascending=False, inplace=True)\n    if (vertical): \n        \n        fig = px.bar(cat_perc, x=feature ,y=\"TARGET\",height=400,text=\"TARGET\")\n        fig.update_traces(marker_color='rgb(158,202,225)', marker_line_color='rgb(8,48,107)',\n                  marker_line_width=1.5, opacity=0.6,texttemplate='%{text:.2s}'+'%', textposition='outside')\n        fig.update_layout(title_text='Percentage of Defaulters')\n    else :\n        \n        fig = px.bar(cat_perc, x=\"TARGET\" ,y=feature,height=400,text=\"TARGET\")\n        fig.update_traces(marker_color='rgb(150,202,225)', marker_line_color='rgb(8,48,107)',\n                  marker_line_width=1.5, opacity=0.6,texttemplate='%{text:.2s}'+'%', textposition='outside')\n        fig.update_layout(title_text='Percentage of Defaulters')\n    fig.show()","6de881af":"\ndef bivariate_bar(x,y,df,hue,figsize):\n    \n    plt.figure(figsize=figsize)\n    sns.barplot(x=x,\n                  y=y,\n                  data=df, \n                  hue=hue, \n                  palette =['g','r'])     \n        \n    # Defining aesthetics of Labels and Title of the plot using style dictionaries\n    plt.xlabel(x,fontdict={'fontsize' : 10, 'fontweight' : 3, 'color' : 'Blue'})    \n    plt.ylabel(y,fontdict={'fontsize' : 10, 'fontweight' : 3, 'color' : 'Blue'})    \n    plt.title(col, fontdict={'fontsize' : 15, 'fontweight' : 5, 'color' : 'Blue'}) \n    plt.xticks(rotation=90, ha='right')\n    plt.legend(labels = ['Repayer','Defaulter'])\n    plt.show()","2930c785":"# function for plotting repetitive rel plots in bivariate numerical analysis on application_data\n\ndef bivariate_rel(x,y,data, hue, kind, palette, legend,figsize):\n    \n    plt.figure(figsize=figsize)\n    sns.relplot(x=x, \n                y=y, \n                data=application_data, \n                hue=\"TARGET\",\n                kind=kind,\n                palette = ['g','r'],\n                legend = False)\n    plt.legend(['Repayer','Defaulter'])\n    plt.xticks(rotation=90, ha='right')\n    plt.show()","5b66b48d":"\n\ndef univariate_merged(col,df,hue,palette,ylog,figsize):\n    plt.figure(figsize=figsize)\n    ax=sns.countplot(x=col, \n                  data=df,\n                  hue= hue,\n                  palette= palette,\n                  order=df[col].value_counts().index)\n    \n\n    if ylog:\n        plt.yscale('log')\n        plt.ylabel(\"Count (log)\",fontdict={'fontsize' : 10, 'fontweight' : 3, 'color' : 'Blue'})     \n    else:\n        plt.ylabel(\"Count\",fontdict={'fontsize' : 10, 'fontweight' : 3, 'color' : 'Blue'})       \n\n    plt.title(col , fontdict={'fontsize' : 15, 'fontweight' : 5, 'color' : 'Blue'}) \n    plt.legend(loc = \"upper right\")\n    plt.xticks(rotation=90, ha='right')\n    \n    plt.show()","601271c1":"fig = px.pie(application_data, values=application_data.NAME_CONTRACT_TYPE.value_counts(),names=['CASH LOANS','REVOLVING LOANS'], title='percentage of types of loans')\nfig.show()","c9938a25":"fig=px.pie(application_data,values=application_data.TARGET.value_counts(),names=['REPAYERS','DEFAULTERS'],title='Percentage of client with payment difficulties')\nfig.show()","ff6d9071":"# Checking the contract type based on loan repayment status\nunivariate_hist(application_data,'NAME_CONTRACT_TYPE')","ceb9cb92":"# Percentage of defaulters in Cash and revolving loans\nperc_defaulter(application_data,'NAME_CONTRACT_TYPE')","0884a91a":"# Checking the gender based count on loan repayment status\nunivariate_hist(application_data,'CODE_GENDER')","715b732f":"# Percentage of defaulters in Cash and revolving loans\nperc_defaulter(application_data,'CODE_GENDER')","64c9e441":"# Checking the gender based count on loan repayment status\nunivariate_hist(application_data,'FLAG_OWN_CAR')","19356d84":"# Percentage of defaulters based on car owned by applicants\nperc_defaulter(application_data,'FLAG_OWN_CAR')","e87b1032":"#plotting sunburst for defaulters'count in Occupation type\nfig = px.sunburst(application_data, path=['OCCUPATION_TYPE','NAME_CONTRACT_TYPE'], values=application_data.TARGET==1)\nfig.show()","96541604":"# Percentage of defaulters based on car owned by applicants\nperc_defaulter(application_data,'OCCUPATION_TYPE',False)","818ec78c":"# Checking the HOUSING based count on loan repayment status\nunivariate_hist(application_data,'NAME_HOUSING_TYPE',False)","add333b1":"# Percentage of defaulters based on housing of applicants\nperc_defaulter(application_data,'NAME_HOUSING_TYPE',False)","720325de":"# Checking the REGION based count on loan repayment status\nunivariate_hist(application_data,'REGION_RATING_CLIENT')","78a4ed44":"# Percentage of defaulters based on REGION of applicants\nperc_defaulter(application_data,'REGION_RATING_CLIENT')","3dec7708":"# Checking the DOCUMENT SUBMISSION based count on loan repayment status\nunivariate_hist(application_data,'FLAG_DOCUMENT_3')","df4f4414":"# Percentage of defaulters based on DOCUMENT SUBMISSION of applicants\nperc_defaulter(application_data,'FLAG_DOCUMENT_3')","c28cbbf8":"# Checking the FAMILY STATUS based count on loan repayment status\nunivariate_hist(application_data,'NAME_FAMILY_STATUS',False)","2defd39e":"# Percentage of defaulters based on FAMILY STATUS of applicants\nperc_defaulter(application_data,'NAME_FAMILY_STATUS',False)","7fcecf2c":"# Checking the INCOME TYPE based count on loan repayment status\nunivariate_hist(application_data,'NAME_INCOME_TYPE',False)","e3b5af08":"# Percentage of defaulters based on INCOME TYPE of applicants\nperc_defaulter(application_data,'NAME_INCOME_TYPE',False)","c0a5f80e":"# Checking the EDUCATION TYPE based count on loan repayment status\nunivariate_hist(application_data,'NAME_EDUCATION_TYPE',False)","a2af17da":"# Percentage of defaulters based on EDUCATION TYPE of applicants\nperc_defaulter(application_data,'NAME_EDUCATION_TYPE',False)","d1dd8827":"# Checking the ORGANIZATION_TYPE based count on loan repayment status\nunivariate_hist(application_data,'ORGANIZATION_TYPE',False)","c5b3d6a5":"# Percentage of defaulters based on ORGANIZATION_TYPE of applicants\nperc_defaulter(application_data,'ORGANIZATION_TYPE',False)","d97e2393":"# Checking AGE_GROUP based count on loan repayment status\nunivariate_hist(application_data,'AGE_GROUP',False)","a93fe492":"# Percentage of defaulters based on  AGE GROUP of applicants\nperc_defaulter(application_data,'AGE_GROUP',False)","b8217e5a":"# Checking employed years count on loan repayment status\nunivariate_hist(application_data,'EMPLOYMENT_YEAR')","c4f8aace":"# Percentage of defaulters based on  year employed of applicants\nperc_defaulter(application_data,'EMPLOYMENT_YEAR')","45ce6c85":"# Checking amount credited count on loan repayment status\nunivariate_hist(application_data,'AMT_CREDIT_RANGE',False)","5a797d1a":"# Percentage of defaulters based on  amount credited of applicants\nperc_defaulter(application_data,'AMT_CREDIT_RANGE',False)","7d4f7d5e":"# Checking income range count on loan repayment status\nunivariate_hist(application_data,'AMT_INCOME_RANGE',False)","bda520e4":"# Percentage of defaulters based on  income range of applicants\nperc_defaulter(application_data,'AMT_INCOME_RANGE',False)","e58a7f8a":"# Checking  childrens' count on loan repayment status\nunivariate_hist(application_data,'CNT_CHILDREN')","4b50c0b1":"# Percentage of defaulters based on chidrens' count of applicants\nperc_defaulter(application_data,'CNT_CHILDREN')","c10bd864":"# Checking  family members' count on loan repayment status\nunivariate_hist(application_data,'CNT_FAM_MEMBERS')","555a2263":"# Percentage of defaulters based on family members' count of applicants\nperc_defaulter(application_data,'CNT_FAM_MEMBERS')","ad1890b3":"# Income type vs Income TOTAL\nbivariate_bar(\"NAME_INCOME_TYPE\",\"AMT_INCOME_TOTAL\",application_data,\"TARGET\",(18,10))\n","5dbaee42":"# Bifurcating the application_data dataframe based on Target value 0 and 1 for correlation and other analysis\ncols_for_correlation = ['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY','CREDIT_SCORE',\n                        'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', \n                        'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS',\n                        'NAME_HOUSING_TYPE', 'REGION_POPULATION_RELATIVE', 'DAYS_BIRTH', 'DAYS_EMPLOYED', \n                        'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH', 'OCCUPATION_TYPE', 'CNT_FAM_MEMBERS', 'REGION_RATING_CLIENT',\n                        'REGION_RATING_CLIENT_W_CITY', 'WEEKDAY_APPR_PROCESS_START', 'HOUR_APPR_PROCESS_START',\n                        'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION', \n                        'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY', 'ORGANIZATION_TYPE',\n                        'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE', 'DAYS_LAST_PHONE_CHANGE', 'FLAG_DOCUMENT_3', \n                        'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK',\n                        'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR']\n\n\nRepayer_df = application_data.loc[application_data['TARGET']==0, cols_for_correlation] # Repayers\nDefaulter_df = application_data.loc[application_data['TARGET']==1, cols_for_correlation] # Defaulters","2f40ee78":"fig = plt.figure(figsize=(12,12))\nax = sns.heatmap(Repayer_df.corr(), cmap=\"RdYlGn\",annot=False,linewidth =1)\n","17c1acd7":"fig = plt.figure(figsize=(12,12))\nax = sns.heatmap(Defaulter_df.corr(), cmap=\"RdYlGn\",annot=False,linewidth =1)\n","a3260e8f":"! pip install tabulate","10e63f84":"# Getting the top 10 correlation for the Repayers data\n\ncorr_repayer = Repayer_df.corr()\ncorr_repayer = corr_repayer.where(np.triu(np.ones(corr_repayer.shape),k=1).astype(np.bool))\ncorr_df_repayer = corr_repayer.unstack().reset_index()\ncorr_df_repayer.columns =['VAR1','VAR2','Correlation']\ncorr_df_repayer.dropna(subset = [\"Correlation\"], inplace = True)\ncorr_df_repayer[\"Correlation\"]=corr_df_repayer[\"Correlation\"].abs() \ncorr_df_repayer.sort_values(by='Correlation', ascending=False, inplace=True) \nprint(corr_df_repayer.head(10).to_markdown())","f047d021":"# Getting the top 10 correlation for the Defaulter data\n\ncorr_Defaulter = Defaulter_df.corr()\ncorr_Defaulter = corr_Defaulter.where(np.triu(np.ones(corr_Defaulter.shape),k=1).astype(np.bool))\ncorr_df_Defaulter = corr_Defaulter.unstack().reset_index()\ncorr_df_Defaulter.columns =['VAR1','VAR2','Correlation']\ncorr_df_Defaulter.dropna(subset = [\"Correlation\"], inplace = True)\ncorr_df_Defaulter[\"Correlation\"]=corr_df_Defaulter[\"Correlation\"].abs()\ncorr_df_Defaulter.sort_values(by='Correlation', ascending=False, inplace=True)\nprint(corr_df_Defaulter.head(10).to_markdown())","e9a0c361":"plt.figure(figsize=[200,100])\nfig = px.scatter(application_data, x=\"CREDIT_SCORE\", y=\"AMT_CREDIT\",color='TARGET',opacity=0.5 ,marginal_y=\"violin\",\n           marginal_x=\"histogram\")\nfig.show()","b9ea5482":"fig = px.scatter_matrix(application_data, dimensions=['AMT_INCOME_TOTAL','AMT_CREDIT',\n                         'AMT_ANNUITY', 'AMT_GOODS_PRICE'], color=\"TARGET\",opacity=0.5)\nfig.show()","e8a8cec9":"#merge both the dataframe on SK_ID_CURR with Inner Joins\nloan_process_df = pd.merge(application_data, previous_data, how='inner', on='SK_ID_CURR')\nloan_process_df.head()","dda5bc3e":"print('Shape of merged dataframe : ',loan_process_df.shape)\nprint('Size of merged dataframe : ',loan_process_df.size)","f88010bc":"loan_process_df.info()","146a4d07":"loan_process_df.describe()","a5fac0dd":"\n\nL0 = loan_process_df[loan_process_df['TARGET']==0] # Repayers\nL1 = loan_process_df[loan_process_df['TARGET']==1] # Defaulters","c4f478f3":"univariate_merged(\"NAME_CASH_LOAN_PURPOSE\",L0,\"NAME_CONTRACT_STATUS\",[\"#548235\",\"#FF0000\",\"#0070C0\",\"#FFFF00\"],True,(18,7))\n\nunivariate_merged(\"NAME_CASH_LOAN_PURPOSE\",L1,\"NAME_CONTRACT_STATUS\",[\"#548235\",\"#FF0000\",\"#0070C0\",\"#FFFF00\"],True,(18,7))","26bd711e":"# Percentage of defaulters based on previous contract status of merged dataframe\nperc_defaulter(loan_process_df,'NAME_CONTRACT_STATUS',False)","2be5177f":"# Percentage of defaulters based on previous client type of merged dataframe\nperc_defaulter(loan_process_df,'NAME_CLIENT_TYPE',False)","168a566f":"#plotting sunburst for defaulters'count in type of client with type of loans and its status \nfig = px.sunburst(loan_process_df, path=['NAME_CLIENT_TYPE','NAME_CONTRACT_TYPE_x','NAME_CONTRACT_STATUS'], values=loan_process_df.TARGET==1)\nfig.show()","6752a06f":"# Checking the Contract Status based on loan repayment status and whether there is any business loss or financial loss\nunivariate_merged(\"NAME_CONTRACT_STATUS\",loan_process_df,\"TARGET\",['g','r'],False,(12,8))\ng = loan_process_df.groupby(\"NAME_CONTRACT_STATUS\")[\"TARGET\"]\ndf1 = pd.concat([g.value_counts(),round(g.value_counts(normalize=True).mul(100),2)],axis=1, keys=('Counts','Percentage'))\ndf1['Percentage'] = df1['Percentage'].astype(str) +\"%\" # adding percentage symbol in the results for understanding\nprint(df1.to_markdown())","87103614":"#### <font color= blue> 3.b. Inspecting dataframes <\/font> <a id ='insdf'><\/a>","c6244e4e":"- **4.a.i. application_data dataframe**","ea3bc40e":"### <font color= green> 4. Data Cleaning & Imputation <a id='dci'><\/a>","89751755":"### 6.<font color=green> Merged Dataframes Analysis<\/font> <a id = 'mda'><\/a>","00375369":"**Inferences:**\n1. More than 80% of the loan provided are for amount less than 900,000\n2. People who get loan for 300-600k tend to default more than others.","d454c439":"**There are 38 columns with float datatype with 50% and more missing values.**\n**These are mostly related to the locality of the applicant.so these columns can be dropped.**<br>","5ee7f3c2":"**Inferences:**\n1. Family member follows the same trend as children where having more family members increases the risk of defaulting","1223e085":"**attributes with 50% and above missing values with object datatype can be dropped.so these variables will be added to the list of unwanted_application**\n- FONDKAPREMONT_MODE -----          68.39 % <br>\n- HOUSETYPE_MODE          -----     50.18 % <br>\n- WALLSMATERIAL_MODE           -----50.84 % <br>\n","59c2f41b":"**Impute categorical variable 'OCCUPATION_TYPE' which has higher null percentage(31.35%) with a new category as assigning to any existing category might influence the analysis:**","0312ad84":"#### <font color= blue> 3.a. Importing datasets <\/font> <a id ='imds'><\/a>","1af82a19":"- **There are no null values in the integer datatype columns those are 41 columns** ","364f65f7":"**Variables with 50% and above missing values with float datatype can be dropped.**\n- FONDKAPREMONT_MODE -----          68.39 % <br>\n- HOUSETYPE_MODE          -----     50.18 % <br>\n- WALLSMATERIAL_MODE           -----50.84 % \n","97231560":"**Number of null values below 50% with object datatype.These values can be imputed**\n1. For the columns with Null values below 15% can be imputed with its mode \n- CODE_GENDER  ----           4\n- NAME_TYPE_SUITE  ----                1292<br>\n2. For the columns with Null values greater than equal to 15% can be imputed with new category\n- OCCUPATION_TYPE  ----               96391\n- ORGANIZATION_TYPE    ----           55374\n- EMERGENCYSTATE_MODE  -----         145755","90b4f285":"**Inferences**:\n1. Most of the applicants are living in Region_Rating 2 place.\n2. Region Rating 3 has the highest default rate (12%)\n3. Applicant living in Region_Rating 1 has the lowest probability of defaulting, thus safer for approving loans","bc966591":"#### <font color = blue> 4.a. Null value Calculation<\/font> <a id= 'nvc'><\/a>","28abf75e":"\n**Strategy for application data:**<br>\nConvert DAYS_DECISION,DAYS_EMPLOYED, DAYS_REGISTRATION,DAYS_ID_PUBLISH from negative to positive as days cannot be negative.<br>\nConvert DAYS_BIRTH from negative to positive values and calculate age and create categorical bins columns<br>\nCategorize the amount variables into bins<br>\nConvert region rating column and few other columns to categorical","48a81470":"### <font color=blue>5.d Numerical Variables Analysis <\/font> <a id = 'nva'><\/a>","5bc84eae":"**Inferences:**\n1. Credit amount is highly correlated with amount of goods price.\n2. But the loan annuity correlation with credit amount has slightly reduced in defaulters(0.75) when compared to repayers(0.77)\n3. We can also see that repayers have high correlation in number of days employed(0.62) when compared to defaulters(0.58).\n4. There is a severe drop in the correlation between total income of the client and the credit amount amongst defaulters whereas it is 0.33 among repayers.\n5. Days_birth and credit score correlation has reduced to 0.26 in defaulters when compared to 0.30 in repayers.\n6. There is a decrease in correlation between age and  days registration : defaulters(0.24),repayers(0.30) ","ef0f8919":"**Strategy for application_data:**\n1. To impute null values in categorical variables which has lower null percentage, mode() is used to impute the most frequent items.\n2. To impute null values in categorical variables which has higher null percentage, a new category is created.\n3. To impute null values in numerical variables which has lower null percentage, median() is used as\n4. There are no outliers in the columns\n- Mean returned decimal values and median returned whole numbers and the columns were number of requests","9fdf8332":"- Records with null values of EMPLOYMENT_YEAR are to be dropped as for those applicant years employed is 1000 ","9a6e1510":"**Impute numerical variables with the median as there are no outliers that can be seen from results of describe() and mean() returns decimal values and these columns represent number of enquiries made which cannot be decimal:**","6e3572bd":"\n### 1.2 Business Objective\n\nThe case study aims to identify patterns which indicate if a client has difficulty paying their installments which may be used for taking actions such as denying the loan, reducing the amount of loan, lending (to risky applicants) at a higher interest rate, etc. This will ensure that the consumers capable of repaying the loan are not rejected. Identification of such applicants using EDA is the aim of this case study.\n\nIn other words, the company wants to understand the driving factors (or driver variables) behind loan default, i.e. the variables which are strong indicators of default. The company can utilise this knowledge for its portfolio and risk assessment.","908aeb44":"## 2. Import required libraries <a id= 'iml'><\/a>","3454c977":"- Inferences:\n1. Contract type: Revolving loans are just a small fraction (10.3%) from the total number of loans; in the same time,5% defaulters are for Revolving loans, comparing with their frequency.\n2. Cash loans have 9% defaulting rate which is more than for revolving loans comparetively","a1ce4661":"**Inferences:**\n1. 90% of the previously cancelled client have actually repayed the loan. Revisiting the interest rates would increase business opoortunity for these clients\n2. 88% of the clients who have been previously refused a loan has payed back the loan in current case.\n3. Refusal reason should be recorded for further analysis as these clients would turn into potential repaying customer.","b6100186":"**Inferences:**\n1. Loan purpose has high number of unknown values (XAP)\n2. Loan taken for the purpose of Repairs seems to have highest default rate\n3. A very high number application have been rejected by bank or refused by client which has purpose as repair or other. This shows that purpose repair is taken as high risk by bank and either they are rejected or bank offers very high loan interest rate which is not feasible by the clients, thus they refuse the loan.","44cf392d":" ###### Shape and size of  previous data\n -There are total of 37 variables out of which \n- 15 columns are of datatype float\n- 06 columns are of datatype int \n- 16 columns are of datatype object","96f27187":"#### <font color=blue>4.f. Outlier Detection<\/font><a id ='od'><\/a>","73a63c24":"#### <font color=blue>5.c.Categorical Variables Analysis <\/font> <a id = 'cva'><\/a>","c6a3d14e":"**Inferences:**\n1. Majority of the applicants have been employeed in between 0-5 years. The defaulting rating of this group is also the highest which is 10%\n2. With increase of employment year, defaulting rate is gradually decreasing with people having 40+ year experience having less than 1% default rate","518cce29":" ###### Shape and size of application data\n -There are total of 122 attributes out of which \n- 65 columns are of datatype float\n- 41 columns are of datatype int \n- 16 columns are of datatype object","f5b39c80":"**Inferences:**\n1. Most of the people who have taken loan are married, followed by Single\/not married and civil marriage\n2. In terms of percentage of defaulters, Civil marriage and single\/not married has the highest percent of non repayment (10%), with Widow the lowest (exception being Unknown).","5c71f209":"**Strategy for Previous _data:**\n1. To impute null values and negative values in AMT_DOWN_PAYMENT \n2. To impute null values in numerical column, we analysed the loan status and assigned values.\n3. To impute null values in continuous variables, we plotted the distribution of the columns and used\n- median if the distribution is skewed\n- mode if the distribution pattern is preserved.`\n4. To impute null values in categorical columns mode is used","9e829075":"#### <font color= blue>4.b. Analyzing and Droping Irrelevant Variables in application data<\/font><a id='adivad'><\/a>","56b980eb":"#### <font color=blue>4.e.Data Type Conversion <\/font> <a id = 'dtc'><\/a>","8ce265bb":"**Strategy for previous_data:**\n- Convert DAYS_DECISION from negative to positive values and create categorical bins columns.\n- Convert TERM_PAYMENT column as integer as it is in number of months.But it has 22.29% missing values.Need to impute first and then convert datatype\n- Convert loan purpose and few other columns to categorical.\n","7c8e3606":"**Inferences:**\n1. There is no significant correlation between repayers and defaulters in terms of submitting document 3 as we see even if applicants have submitted the document, they have defaulted a slightly more (9.3%) than who have not submitted the document (6.6%)","0df64959":"After primarily inspecting application_data without examining null values \n> 1. DAYS_BIRTH, DAYS_EMPLOYED, DAYS_REGISTRATION, DAYS_ID_PUBLISH columns(attributes) are with negative sign\n> 2. Maximum value for DAYS_EMPLOYED is 365243 which when converted to years gives 1000 .It is an outlier as no person would serve so long.","09e32dca":"\nFrom the plot we can see the columns in which percentage of null values more than 50% are marked above the second red line and the columns which have less than 50 % null values but greater then 15% are marked with the first red line above X-axis.<br>\n**There are 7 columns in previous_data dataframe where missing value is more than 50%.**\n- Number of null values below 50% can be imputed :\n1. For the columns with Null values below 15% can be imputed with its mode<br>\nNAME_CONTRACT_TYPE-----0.02<br>\nCODE_REJECT_REASON-----0.31<br>\nNAME_CLIENT_TYPE-------0.12<br>\nPRODUCT_COMBINATION----0.02\n2. For the columns with Null values greater than equal to 15% can be imputed with new category\nNAME_CASH_LOAN_PURPOSE-40.59<br>\nNAME_PAYMENT_TYPE------37.56<br>\nNAME_TYPE_SUITE--------49.12<br>\nNAME_PORTFOLIO---------22.29<br>\nNAME_YIELD_GROUP-------30.97\n\n","13d6d352":"- After analysing the datasets, there are few attributes of a client with which the bank would be able to identify if they will repay the loan or not. The analysis is consised as below with the contributing factors and categorization:","afeeaccd":"**31% loan applicants have age above 50 years. More than 70% of loan applicants have age over 40 years.**","1b694067":"### 5.<font color=green> Data Analysis<\/font> <a id = 'da'><\/a>","b61913ae":"#### <font color=blue>5.b.Plotting Functions<\/font> <a id = 'pf'><\/a>","8e14e64c":"- Numerical univariate Analysis","4ee6c811":"**The above graph shows that in most of the loan application cases, clients who applied for loans has not submitted FLAG_DOCUMENT_X except FLAG_DOCUMENT_3. Thus, Except for FLAG_DOCUMENT_3, we can delete rest of the columns. Data shows if borrower has submitted FLAG_DOCUMENT_3 then there is a less chance of defaulting the loan**","77de3a35":"**Checkpoint :**\n - 37 total columns - 17 irrelevant columns= 20 columns ","bac5e58f":"##### Imputations \n- EXT_SOURCE_2 and EXT_SOURCE_3 null values can be imputed with the mean of the respective columns, as the normalised credit scores lie between 0 and 1","769fbbfe":"**Decisive Factor whether an applicant will be Repayer:**\n1. NAME_EDUCATION_TYPE: Academic degree has less defaults.\n2. NAME_INCOME_TYPE: Student and Businessmen have no defaults.\n3. REGION_RATING_CLIENT: RATING 1 is safer.\n4. ORGANIZATION_TYPE: Clients with Trade Type 4 and 5 and Industry type 8 have defaulted less than 3%\n5. DAYS_BIRTH: People above age of 50 have low probability of defaulting\n6. DAYS_EMPLOYED: Clients with 40+ year experience having less than 1% default rate\n7. AMT_INCOME_TOTAL:Applicant with Income more than 700,000 are less likely to default\n8. CNT_CHILDREN: People with zero to two children tend to repay the loans.\n9. CREDIT_SCORE : People with average to high credit score are less likely to default ","373751c7":"- **There are no duplicate records in application_data**\n- **For each loan application new ID is generated.**","e3853171":"**Inferences:**\n1. Most of the applicants do not have children\n2. Very few clients have more than 3 children.\n3. Client who have more than 4 children has a very high default rate with child count 9 and 11 showing 100% default rate","f3afb709":"**More Than 16% loan applicants have taken loan which amounts to more than 1M.**","a0a1068f":"**Inferences:**\n- It can be seen that business man's income is the highest and the estimated range with default 95% confidence level seem to indicate that the income of a business man could be in the range of slightly close to 4 lakhs and slightly above 10 lakhs**","bbe2c6d2":"**Inferences**\n1. Repeaters with large quantity of cash loans given and approved are defaulters.At the same time there is nearly equal amount   of repeaters are refused and canceled\n2. Large number of new applicants approved with revolving loans are defaulters\n3. 13% clients were previously refused are defaulters","ab8ae271":"**contact Parameters**","a0d0ba66":"- Clients who own a car are half in number of the clients who dont own a car. But based on the percentage of deault, there is no correlation between owning a car and loan repayment as in both cases the default percentage has just a difference of 2%.","26610c7d":"**Decisive Factor whether an applicant will be Defaulter:**\n1. CODE_GENDER: Men are at relatively higher default rate\n2. NAME_FAMILY_STATUS : People who have civil marriage or who are single default a lot.\n3. NAME_EDUCATION_TYPE: People with Lower Secondary & Secondary education\n4. NAME_INCOME_TYPE: Clients who are either at Maternity leave OR Unemployed default a lot.\n5. REGION_RATING_CLIENT: People who live in Rating 3 has highest defaults.\n6. OCCUPATION_TYPE: Avoid Low-skill Laborers, Drivers and Waiters\/barmen staff, Security staff, Laborers and Cooking staff as the default rate is huge.\n7. ORGANIZATION_TYPE:Organizations with highest percent of loans not repaid are Transport: type 3 (15.75%), Industry: type 13 (13.5%), Industry: type 8 (12.5%) and Restaurant (less than 12%). Self employed people have relative high defaulting rate (10.17%), and thus should be avoided to be approved for loan or provide loan with higher interest rate to mitigate the risk of defaulting.\n8. DAYS_BIRTH: Avoid young people who are in age group of 20-40 as they have higher probability of defaulting\n9. DAYS_EMPLOYED: People who have less than 5 years of employment have high default rate.\n10. CNT_CHILDREN & CNT_FAM_MEMBERS: Client who have children equal to or more than 9 default 100% and hence their applications are to be rejected.\n11. AMT_GOODS_PRICE: When the credit amount goes beyond 3M, there is an increase in defaulters.","52af5c35":"- There is no linear correlation between flags of mobile phone, email etc. with loan repayment; thus these columns can be dropped","0abec5fb":"### 7.<font color=green> Conclusions<\/font> <a id = 'con'><\/a>","63915b2d":"#### <font color=blue>4.g. Null Value Data Imputation<\/font><a id ='nvdi'><\/a>","3cdba162":"- **4.a.ii. previous_data dataframe**","1c8b932e":"#### <font color=blue>4.c. Analyzing and Dropping Irrelevant variables in previous_data <\/font> <a id = 'adivpd'><\/a>","b9a7e7a9":"**Inferences:**\n1. Organizations with highest percent of loans not repaid are Transport: type 3 (15.75%), Industry: type 13 (13.5%), Industry: type 8 (12.5%) and Restaurant (less than 12%). Self employed people have relative high defaulting rate(10.17%), and thus should be avoided to be approved for loan or provide loan with higher interest rate to mitigate the risk of defaulting.\n2. Most of the people application for loan are from Business Entity Type 3\n3. Though business entity type 2 has more loans disbursed, they have comparatively lees default rate(8.5%) \n4. It can be seen that following category of organization type has lesser defaulters thus safer for providing loans:\n- Trade Type 4 \n- Securities Ministries","7fd3d048":"**Inferences:**\n1. Majority of people live in House\/apartment\n2. People living in office apartments have lowest default rate\n3. People living with parents (12%) and living in rented apartments(13%) have higher probability of defaulting","ac246d38":"**Inferences:**\n1. 90% of the applications have Income total less than 300,000\n2. Application with Income less than 300,000 has high probability of defaulting\n3. Applicant with Income more than 700,000 are less likely to default","edf63792":"- Following are the common functions customized to perform uniform anaysis that is called for all plots","36d0b0ae":"### 1.3 Data Understanding\n\n1. 'application_data.csv'\n   It contains all the information of the client at the time of application. The data is about whether a client has payment difficulties.\n   \n2. 'previous_application.csv'\nIt contains information about the client\u2019s previous loan data. It contains the data whether the previous application had been Approved, Cancelled, Refused or Unused offer.\n\n3. 'columns_description.csv'\nIt is data dictionary which describes the meaning of the variables.","2a876f24":"**Impute CNT_PAYMENT with 0 as the NAME_CONTRACT_STATUS for these indicate that most of these loans were not started**","ff85ce52":"#### <font color=blue>4.d. Standardize Values <\/font> <a id = 'sv'><\/a>","a9f4eb6e":"- **There are no duplicate records in application and previous data.**","8f2f280a":"**Strategy:**\n- The data analysis flow has been planned in following way :\n\n1. Imbalance in Data\n2. Categorical Data Analysis\n>Categorical segmented Univariate Analysis<br>\n>Categorical Bi\/Multivariate analysis\n3. Numeric Data Analysis\n>Bi-furcation of databased based on TARGET data<br>\n>Correlation Matrix<br>\n>Numerical segmented Univariate Analysis<br>\n>Numerical Bi\/Multivariate analysis","2eca9287":"###### - Inferences:\n- The number of female clients is almost double the number of male clients. Based on the percentage of defaulted credits, males have a higher chance of not returning their loans(10%), comparing with women (7.6%)","82e28f77":"**Almost 37% loan applicants have applied for a new loan within 0-400 days of previous loan decision**","48359c9e":"## <font color= green> 3. Reading and Understanding the dataset <\/font> <a id ='rud'><\/a>","fb41d592":"**Inferences**<br> \n- Credit score doesn't seems to have impact on defaulters,\n- But density of repayers with more than 0.5 normalised credit score is high irrespective of loan amount","26d1ff67":"- inferences:\n1. Most of the loans are taken by Laborers, followed by Sales staff. IT staff take the lowest amount of loans.\n2. The category with highest percent of not repaid loans are Low-skill Laborers (above 17%), followed by Drivers and Waiters\/barmen staff, Security staff, Laborers and Cooking staff.","59321405":"##### Flag Document","8bc16478":"#### <font color=blue>5.a.Imbalance Analysis<\/font> <a id = 'ia'><\/a>","9561bbea":"- <font color ='purple'>**Checkpoint** <br>\n    38 float datatype with 50% and above missing values columns+ <br>\n    19 FLAG_DOCUMENT_X columns + <br>\n    3 columns with the object type variables possessing 50% and more missing values+<br>\n    6 contact columns =<br>    66 unwanted_application columns<\/font> ","02aa1630":"**Inferences:**\n1. When amt_annuity >15000 amt_goods_price> 3M, there is a lesser chance of defaulters\n2. AMT_CREDIT and AMT_GOODS_PRICE are highly correlated as based on the scatterplot where most of the data are consolidated in form of a line\n3. There are very less defaulters for AMT_CREDIT >3M","c2d17e70":"- **CNT_PAYMENT describes the term of payment in months. Hence, should be in integer datatype**","0aa4e582":"**Impute AMT_ANNUITY with median as the distribution is greatly skewed**","43a86309":"### 1.1 Business UnderStanding\n- The loan providing companies find it hard to give loans to the people due to their insufficient or non-existent credit history. Because of that, some consumers use it as their advantage by becoming a defaulter. Suppose you work for a consumer finance company which specialises in lending various types of loans to urban customers. You have to use EDA to analyse the patterns present in the data. This will ensure that the applicants are capable of repaying the loan are not rejected.\n\nWhen the company receives a loan application, the company has to decide for loan approval based on the applicant\u2019s profile. Two types of risks are associated with the bank\u2019s decision:\n\n- If the applicant is likely to repay the loan, then not approving the loan results in a loss of business to the company\n\n- If the applicant is not likely to repay the loan, i.e. he\/she is likely to default, then approving the loan may lead to a financial loss for the company.\n\nThe data given below contains the information about the loan application at the time of applying for the loan. It contains two types of scenarios:\n\n- The client with payment difficulties: he\/she had late payment more than X days on at least one of the first Y instalments of the loan in our sample,\n\n- All other cases: All other cases when the payment is paid on time.\n\nWhen a client applies for a loan, there are four types of decisions that could be taken by the client\/company):\n\n#### Approved:\nThe Company has approved loan Application\n\n#### Cancelled:\nThe client cancelled the application sometime during approval. Either the client changed her\/his mind about the loan or in some cases due to a higher risk of the client he received worse pricing which he did not want.\n\n#### Refused:\nThe company had rejected the loan (because the client does not meet their requirements etc.).\n\n#### Unused offer:\nLoan has been cancelled by the client but on different stages of the process.\n","35cbada6":"**More than 50% loan applicants have income amount in the range of 100K-200K. Almost 92% loan applicants have income less than 300K**","8b1828c5":"# <font color = brown> Bank Loan Services Assignment <\/font>","373e1235":"**55% of the loan applicants have work experience within 0-5 years and almost 80% of them have less than 10 years of work experience**","5070555c":"**Impute categorical variable 'NAME_TYPE_SUITE' which has lower null percentage(0.42%) with the most frequent category using mode()[0]:**","743494af":"**Inferences:**\n1. Most of applicants for loans have income type as Working, followed by Commercial associate, Pensioner and State servant.\n2. The applicants with the type of income Maternity leave have almost 40% ratio of not returning loans, followed by working (9.6%).\n3. Student and Businessmen,Pensioners though less in numbers do not have any default record. Thus these two category are safest for providing loan.","b8efea71":"**Inferences:**\n1. People in the age group range 20-40 have higher probability of defaulting\n2. People above age of 50 have low probability of defaulting\n3. loan given to above age 40 is safer as defaulting rate is comparatively less.","18bfd9c0":"- AMT_GOODS_PRICE has a positive linear correlation with AMT_CREDIT so impute with the corresponding values","6fe707b6":"**38 columns with 50%  and more  null values with float datatype**","4df7cf00":"### 1. INTRODUCTION <a id='intro'><\/a>\n   - This case study aims to give you an idea of applying EDA in a real business scenario. In this case study, apart from applying the techniques that you have learnt in the EDA module, you will also develop a basic understanding of risk analytics in banking and financial services and understand how data is used to minimise the risk of losing money while lending to customers.","44363d0d":"**Inferences:**\n1. Majority of the clients have Secondary \/ secondary special education, followed by clients with Higher education. Only a very small number having an academic degree\n2. The Lower secondary category, although rare, have the largest rate of not repaying the loan (14%). The people with Academic degree have 2.2% defaulting rate.","9dceaa12":"#### Table of contents\n> 1. [Introduction](#intro)\n> 2. [Import Required Libraries](#iml)\n> 3. [Reading and understanding the dataset](#rud)\n>> a. [Importing datasets](#imds)<br>\n>> b. [Inspecting Dataframes](#insdf)<br>\n> 4. [Data Cleaning and Imputation](#dci)\n>> a. [Null value Calculation](#nvc)<br>\n>> b. [Analyzing and Dropping Irrelevant Variables in application data](#adivad)<br>\n>> c. [Analyze and Dropping Irrelevant Variables in previous_data](#adivpd)<br>\n>> d. [Standardize Values](#sv)<br>\n>> e. [Data Type Conversion](#dtc)<br>\n>> f. [Outlier Detection](#od)<br>\n>> g. [Null Value Data Imputation](#nvdi)<br>\n> 5. [Data Analysis](#da)\n>> a. [Imbalance Analysis](#ia)<br>\n>> b. [Plotting Functions](#pf)<br>\n>> c. [Categorical Variables Analysis](#cva)<br>\n>> d. [Numerical Variables Analysis](#nva)<br>\n> 6. [Merged Dataframes Analysis](#mda)<br>\n> 7. [Conclusions](#con)"}}