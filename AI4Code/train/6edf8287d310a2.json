{"cell_type":{"f737198a":"code","fb2e17f7":"code","fce898f6":"code","42219595":"code","49ff1034":"code","57f5e403":"code","e1868390":"code","e23637df":"code","ce1f8d8d":"code","1c5d0872":"code","3cb23cfa":"code","34b65531":"code","f3ad708f":"code","af6eab8b":"code","ec443d58":"code","5e9579df":"code","bc1576e4":"code","347ea439":"code","d50aa673":"code","7547e06c":"code","ee78bad5":"code","bedff9c4":"code","9d335c73":"code","6cb3bc22":"code","f44e72ac":"code","f024df30":"code","db1d742d":"code","28da65e1":"code","73819cc2":"code","14daf5ba":"code","dc5086ec":"code","f2fa09cc":"code","f74305c4":"code","5ec710b2":"code","6e40732d":"code","4f13d377":"code","f442014c":"code","c957cfbd":"code","1c58f522":"code","966b33a0":"markdown","dbe87a49":"markdown","739be35a":"markdown","0875a131":"markdown","8fe9ad37":"markdown","a95d6e6c":"markdown","9b2e0976":"markdown","98a65f52":"markdown","d0f1267b":"markdown","2f504c1d":"markdown","383936de":"markdown","cedac73e":"markdown","aeb6be25":"markdown","9ab17957":"markdown","064c415e":"markdown","c903729f":"markdown","8e073372":"markdown","4982e560":"markdown","1f9dac70":"markdown","9e0b179c":"markdown","5b349f36":"markdown","85fb71c4":"markdown","0b63a4d4":"markdown","056d7067":"markdown","7f1d9bbc":"markdown","559a9a4e":"markdown","94da7c80":"markdown","14410763":"markdown","f3410264":"markdown","f990f834":"markdown","4def3a6c":"markdown","136ca446":"markdown"},"source":{"f737198a":"\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/libjpeg-turbo-2.1.0-h7f98852_0.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/libgcc-ng-9.3.0-h2828fa1_19.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/gdcm-2.8.9-py37h500ead1_1.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/conda-4.10.1-py37h89c1867_0.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/certifi-2020.12.5-py37h89c1867_1.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/openssl-1.1.1k-h7f98852_0.tar.bz2' -c conda-forge -y\n","fb2e17f7":"import tensorflow as tf\nprint(tf.__version__)\nimport torch\nprint(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")\n\nimport os\nimport gc\nimport cv2\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom shutil import copyfile\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut","fce898f6":"gpus = tf.config.list_physical_devices('GPU')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)","42219595":"# Read the submisison file\nsub_df = pd.read_csv('\/kaggle\/input\/siim-covid19-detection\/sample_submission.csv')\nprint(len(sub_df))\nsub_df.head()","49ff1034":"study_df = sub_df.loc[sub_df.id.str.contains('_study')]\nlen(study_df)","57f5e403":"study_df","e1868390":"image_df = sub_df.loc[sub_df.id.str.contains('_image')]\nlen(image_df)","e23637df":"image_df","ce1f8d8d":"\ndef read_xray(path, voi_lut = True, fix_monochrome = True):\n    # Original from: https:\/\/www.kaggle.com\/raddar\/convert-dicom-to-np-array-the-correct-way\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \n    # \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n        \n    return data\n","1c5d0872":"\ndef resize_xray(array, size, keep_ratio=False, resample=Image.LANCZOS):\n    # Original from: https:\/\/www.kaggle.com\/xhlulu\/vinbigdata-process-and-resize-to-image\n    im = Image.fromarray(array)\n    \n    if keep_ratio:\n        im.thumbnail((size, size), resample)\n    else:\n        im = im.resize((size, size), resample)\n    \n    return im\n","3cb23cfa":"\nTEST_PATH = f'\/kaggle\/tmp\/test\/'\nIMG_SIZE = 512\n\ndef prepare_test_images():\n    image_id = []\n    dim0 = []\n    dim1 = []\n\n    os.makedirs(TEST_PATH, exist_ok=True)\n\n    for dirname, _, filenames in tqdm(os.walk(f'..\/input\/siim-covid19-detection\/test')):\n        for file in filenames:\n            # set keep_ratio=True to have original aspect ratio\n            xray = read_xray(os.path.join(dirname, file))\n            im = resize_xray(xray, size=IMG_SIZE)  \n            im.save(os.path.join(TEST_PATH, file.replace('dcm', 'png')))\n\n            image_id.append(file.replace('.dcm', ''))\n            dim0.append(xray.shape[0])\n            dim1.append(xray.shape[1])\n            \n    return image_id, dim0, dim1\n","34b65531":"\nimage_ids, dim0, dim1 = prepare_test_images()\nprint(f'Number of test images: {len(os.listdir(TEST_PATH))}')\n","f3ad708f":"'''\nimport tarfile\nwith tarfile.open('SIIM-test-size512.tar.gz', 'w:gz') as t:\n     for i in list:\n        t.add(i)\n'''","af6eab8b":"'''\n!cp SIIM-test-size512.tar.gz \/kaggle\/working\n'''","ec443d58":"'''\ntest_list = pd.DataFrame(index=range(len(list)), data={\"image_id\":image_ids})\ntest_list['dim0'] = dim0\ntest_list['dim1'] = dim1\ntest_list.to_csv('\/kaggle\/working\/test_list.csv')\n'''","5e9579df":"'''\nmeta_df = pd.read_csv('..\/input\/siimresize512\/test_list .csv')\nmeta_df = meta_df.rename(columns={'id':'image_id'})\nmeta_df\n'''","bc1576e4":"!ls ..\/input\/siimresize512\/","347ea439":"meta_df = pd.DataFrame.from_dict({'image_id': image_ids, 'dim0': dim0, 'dim1': dim1})\n\n# Associate image-level id with study-level ids.\n# Note that a study-level might have more than one image-level ids.\nfor study_dir in os.listdir('..\/input\/siim-covid19-detection\/test'):\n    for series in os.listdir(f'..\/input\/siim-covid19-detection\/test\/{study_dir}'):\n        for image in os.listdir(f'..\/input\/siim-covid19-detection\/test\/{study_dir}\/{series}\/'):\n            image_id = image[:-4]\n            meta_df.loc[meta_df['image_id'] == image_id, 'study_id'] = study_dir\n        \nmeta_df.head()","d50aa673":"!cp ..\/input\/siimresize512\/yolov5x.pt \/kaggle\/working\n!cp ..\/input\/siimresize512\/requirements.txt \/kaggle\/working","7547e06c":"#TEST_PATH = f'..\/input\/siimresize512\/SIIM-test-size512\/kaggle\/tmp\/test\/'\nIMG_SIZE = 512","ee78bad5":"YOLO_MODEL_PATH = '..\/input\/siimyolo5\/best.pt'\n\n!python ..\/input\/kaggle-yolov5\/detect.py --weights {YOLO_MODEL_PATH} \\\n                                      --source {TEST_PATH} \\\n                                      --img {IMG_SIZE} \\\n                                      --conf 0.22 \\\n                                      --iou-thres 0.5 \\\n                                      --max-det 10 \\\n                                      --save-txt \\\n                                      --save-conf","bedff9c4":"PRED_PATH = 'runs\/detect\/exp\/labels'\nprediction_files = os.listdir(PRED_PATH)\nprint(f'Number of opacity predicted by YOLOv5: {len(prediction_files)}')","9d335c73":"AUTOTUNE = tf.data.AUTOTUNE\n\nCONFIG = dict (\n    seed = 42,\n    num_labels = 4,\n    num_folds = 5,\n    img_width = 512,\n    img_height = 512,\n    batch_size = 32,\n    _wandb_kernel = 'ayut',\n    architecture = \"CNN\",\n    infra = \"GCP\",\n)","6cb3bc22":"image_df['path'] = image_df.apply(lambda row: TEST_PATH+row.id.split('_')[0]+'.png', axis=1)\nimage_df = image_df.reset_index(drop=True)\nimage_df['path'][0] ","f44e72ac":"@tf.function\ndef decode_image(image):\n    # convert the compressed string to a 3D uint8 tensor\n    image = tf.image.decode_png(image, channels=3)\n    # Normalize image\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image\n\n@tf.function\ndef load_image(df_dict):\n    # Load image\n    image = tf.io.read_file(df_dict['path'])\n    image = decode_image(image)\n    \n    # \u3053\u3053\u3067512\u306b\u30ea\u30b5\u30a4\u30ba\u3057\u3066\u3044\u308b\u3068\u3044\u3046\u3053\u3068\u306f\u3001\u5165\u529b\u30d5\u30a1\u30a4\u30eb\u306f\u30ea\u30b5\u30a4\u30ba\u6e08\u307f\u306e\u3082\u306e\u3067\u306f\u306a\u3044\u65b9\u304c\u3044\u3044\u306e\u304b\u306a\u3002yolo\u306f\u30ea\u30b5\u30a4\u30ba\u6e08\u306e\u3082\u306e\u3092\u60f3\u5b9a\u3057\u3066\u3044\u308b\u3002\n    image = tf.image.resize(image, (CONFIG['img_height'], CONFIG['img_width']))\n    \n    return image\n\ntestloader = tf.data.Dataset.from_tensor_slices(dict(image_df))\n\ntestloader = (\n    testloader\n    .shuffle(1024)\n    .map(load_image, num_parallel_calls=AUTOTUNE)\n    .batch(CONFIG['batch_size'])\n    .prefetch(AUTOTUNE)\n)","f024df30":"# Load Model\nSTUDY_MODEL_PATHS = '..\/input\/siim-study-level-models\/effnet-mixup\/effnetb0_mixup\/'\nstudy_models = os.listdir(STUDY_MODEL_PATHS)\nstudy_models","db1d742d":"predictions = []\nfor model in study_models:\n    # Load model\n    tf.keras.backend.clear_session()\n    model = tf.keras.models.load_model(STUDY_MODEL_PATHS+model)\n    # Prediction\n    tmp = []\n    for img_batch in tqdm(testloader):\n        preds = model.predict(img_batch)\n        tmp.extend(preds)\n        \n    predictions.append(tmp)\n    \n    del model\n    _ = gc.collect()\n    \npredictions = np.mean(predictions, axis=0)","28da65e1":"class_labels = ['0', '1', '2', '3']\nimage_df.loc[:, class_labels] = predictions\nimage_df.head()","73819cc2":"class_to_id = { \n    'negative': 0,\n    'typical': 1,\n    'indeterminate': 2,\n    'atypical': 3}\nid_to_class  = {v:k for k, v in class_to_id.items()}\n\ndef get_study_prediction_string(preds, threshold=0):\n    string = ''\n    for idx in range(4):\n        conf =  preds[idx]\n        if conf>threshold:\n            string+=f'{id_to_class[idx]} {conf:0.2f} 0 0 1 1 '\n    string = string.strip()\n    return string","14daf5ba":"study_ids = []\npred_strings = []\n\nfor study_id, df in meta_df.groupby('study_id'):\n    # accumulate preds for diff images belonging to same study_id\n    tmp_pred = []\n    \n    df = df.reset_index(drop=True)\n    for image_id in df.image_id.values:\n        preds = image_df.loc[image_df.id == image_id+'_image'].values[0]\n        tmp_pred.append(preds[3:])\n    \n    preds = np.mean(tmp_pred, axis=0)\n    pred_string = get_study_prediction_string(preds)\n    pred_strings.append(pred_string)\n    \n    study_ids.append(f'{study_id}_study')\n    \nstudy_df = pd.DataFrame.from_dict({'id': study_ids, 'PredictionString': pred_strings})\nstudy_df.head()","dc5086ec":"# The submisison requires xmin, ymin, xmax, ymax format. \n# YOLOv5 returns x_center, y_center, width, height\ndef correct_bbox_format(bboxes):\n    correct_bboxes = []\n    for b in bboxes:\n        xc, yc = int(np.round(b[0]*IMG_SIZE)), int(np.round(b[1]*IMG_SIZE))\n        w, h = int(np.round(b[2]*IMG_SIZE)), int(np.round(b[3]*IMG_SIZE))\n\n        xmin = xc - int(np.round(w\/2))\n        ymin = yc - int(np.round(h\/2))\n        xmax = xc + int(np.round(w\/2))\n        ymax = yc + int(np.round(h\/2))\n        \n        correct_bboxes.append([xmin, ymin, xmax, ymax])\n        \n    return correct_bboxes\n\ndef scale_bboxes_to_original(row, bboxes):\n    # Get scaling factor\n    scale_x = IMG_SIZE\/row.dim1\n    scale_y = IMG_SIZE\/row.dim0\n    \n    scaled_bboxes = []\n    for bbox in bboxes:\n        xmin, ymin, xmax, ymax = bbox\n        \n        xmin = int(np.round(xmin\/scale_x))\n        ymin = int(np.round(ymin\/scale_y))\n        xmax = int(np.round(xmax\/scale_x))\n        ymax = int(np.round(ymax\/scale_y))\n        \n        scaled_bboxes.append([xmin, ymin, xmax, ymax])\n        \n    return scaled_bboxes\n\n# Read the txt file generated by YOLOv5 during inference and extract \n# confidence and bounding box coordinates.\ndef get_conf_bboxes(file_path):\n    confidence = []\n    bboxes = []\n    with open(file_path, 'r') as file:\n        for line in file:\n            preds = line.strip('\\n').split(' ')\n            preds = list(map(float, preds))\n            confidence.append(preds[-1])\n            bboxes.append(preds[1:-1])\n    return confidence, bboxes","f2fa09cc":"image_pred_strings = []\nfor i in tqdm(range(len(image_df))):\n    row = meta_df.loc[i]\n    id_name = row.image_id\n    \n    if f'{id_name}.txt' in prediction_files:\n        # opacity label\n        confidence, bboxes = get_conf_bboxes(f'{PRED_PATH}\/{id_name}.txt')\n        bboxes = correct_bbox_format(bboxes)\n        ori_bboxes = scale_bboxes_to_original(row, bboxes)\n        \n        pred_string = ''\n        for j, conf in enumerate(confidence):\n            pred_string += f'opacity {conf} ' + ' '.join(map(str, ori_bboxes[j])) + ' '\n        image_pred_strings.append(pred_string[:-1]) \n    else:\n        image_pred_strings.append(\"none 1 0 0 1 1\")","f74305c4":"meta_df['PredictionString'] = image_pred_strings\nimage_df = meta_df[['image_id', 'PredictionString']]\nimage_df.insert(0, 'id', image_df.apply(lambda row: row.image_id+'_image', axis=1))\nimage_df = image_df.drop('image_id', axis=1)\nimage_df.head()","5ec710b2":"sub_df","6e40732d":"'''\nimage = sub_df.copy()\nimage =image.drop(image.index[:1214])\nimage =image.reset_index()\nimage = image.drop(['index'],axis=1)\nimage\n'''","4f13d377":"'''\nfor r in tqdm(range(len(image))):\n   for i in range(len(image)):\n       if image.iloc[r].id == image_df.iloc[i].id:\n         image.iloc[r].PredictionString = image_df.iloc[i].PredictionString\n         break\n'''        ","f442014c":"!rm -rf runs","c957cfbd":"sub_df = pd.concat([study_df, image_df])\nsub_df.to_csv('submission.csv', index=False)\nsub_df","1c58f522":"image","966b33a0":"### GPU\u6642\u9593\u304c\u8db3\u3089\u306a\u3044\u305f\u3081\u3001\u3053\u308c\u4ee5\u964d\u306e\u8abf\u67fb\u306f\u6765\u9031\u3002\u3002\u3002","dbe87a49":"### \u30c6\u30b9\u30c8\u753b\u50cf\u3092\u6e96\u5099\u3059\u308b\uff08\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u5316\u3059\u308b\uff09\u2192\u5931\u6557\u3057\u307e\u3057\u305f\u3002\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u5316\u3059\u308b\u3068\u89e3\u6c7a\u3067\u304d\u306a\u3044\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u307e\u3059\u3002","739be35a":"# \u3053\u3053\u3067yolov5\u767b\u5834","0875a131":"## \u63a8\u6e2c","8fe9ad37":"![image.png](attachment:06d3ffa2-444b-4172-b4af-83d851a470bd.png)","a95d6e6c":"\u3053\u306e\u30e2\u30c7\u30eb\u306fEfficentNet\u3068YoloV5\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u3044\u307e\u3059\u3002\u4ed5\u7d44\u307f\u306f\u7406\u89e3\u3067\u304d\u3066\u3044\u307e\u305b\u3093\u3002<br>\nhttps:\/\/towardsdatascience.com\/fusing-efficientnet-yolov5-advanced-object-detection-2-stage-pipeline-tutorial-da3a77b118d1","9b2e0976":"\u3053\u3053\u3067\u3001study\u3068image\u3092\u7d71\u5408\u3057\u3066\u3044\u307e\u3059\u306d\u3002","98a65f52":"### Load Submission ","d0f1267b":"\u30fbappend()\u30e1\u30bd\u30c3\u30c9\u3068\u306f,\u5f15\u6570\u306b\u6e21\u3055\u308c\u305f\u5024\u3092\u30ea\u30b9\u30c8\u306e\u672b\u5c3e\u306b\u6e21\u3057\u307e\u3059\u3002<br>\n\u30fbextend()\u30e1\u30bd\u30c3\u30c9\u3068\u306f,\u5f15\u6570\u306b\u6e21\u3055\u308c\u305f\u5225\u306e\u30ea\u30b9\u30c8\u3092\u8ffd\u52a0\u3057\u3066\u30ea\u30b9\u30c8\u3092\u62e1\u5f35\u3057\u307e\u3059\u3002<br>\n\n\u6587\u5b57\u5217\u306e\u5834\u5408\u3001append\u3067\u8ffd\u52a0\u3059\u308b\u304c\u3001extend\u3060\u3068\u4e00\u6587\u5b57\u305a\u3064\u8ffd\u52a0\u3055\u308c\u308b\u3002<br>\n\u3000\u3000 my_list2 = ['dog', 'cat']<br>\n\u3000\u3000 my_list2.extend('bat')<br>\n\u3000\u3000 my_list2<br>\n\u3000\u3000['dog', 'cat', 'b', 'a', 't']<br>\nextend\u3067\u8981\u7d20\u3092\u6e21\u3057\u3066\u3001\u30a8\u30e9\u30fc\u3092\u8d77\u3053\u3059\u3002","2f504c1d":"thanks https:\/\/www.kaggle.com\/ayuraj\/submission-covid19 I upvoked.\n\u3000\u3000\u3000\u3000https:\/\/www.kaggle.com\/ayuraj\/train-covid-19-detection-using-yolov5 also,upvoked.","383936de":"### image\u306e\u9806\u756a\u304csample\u3068\u7570\u306a\u308b\u3068\u30a8\u30e9\u30fc\u306b\u306a\u308b\u305f\u3081\u88dc\u6b63\u3059\u308b\u3002\u2192\u6c17\u306e\u305b\u3044\u3060\u3063\u305f\u3002\u306a\u3093\u3060\u308d\u3046\u304b\u3002","cedac73e":"![image.png](attachment:3d95043e-92f2-41be-a948-8c7a3196f043.png)","aeb6be25":"### 512*512\u30d0\u30a4\u30c8\u306b\u30ea\u30b5\u30a4\u30ba\u3057\u3066\u3044\u308b\u3002","9ab17957":"# Merge Results","064c415e":"\u3053\u3061\u3089\u306f\u30a4\u30e1\u30fc\u30b8\u30d5\u30a1\u30a4\u30eb\u306a\u306e\u3067\u308f\u304b\u308a\u3084\u3059\u3044\u3002\u3053\u3061\u3089\u306f\u3001none\u306b\u306a\u3063\u3066\u3044\u308b\u3002\u3069\u3046\u3044\u3046\u610f\u5473\uff1f","c903729f":"### Install ","8e073372":"### \u753b\u50cf\u3092\u62bd\u51fa\u3057\u3066\u30b5\u30a4\u30ba\u3092\u5909\u66f4\u3059\u308b\u305f\u3081\u306e\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3","4982e560":"![image.png](attachment:8d95a590-4c8e-4718-b196-9e7113248c0f.png)","1f9dac70":"### Imports","9e0b179c":"study\u3068image\u3092\u51fa\u529b\u3057\u306a\u3044\u3068\u3044\u3051\u306a\u3044\u3002study\u306fnegative\u304c\u591a\u3044\u3067\u3059\u306d\u3002","5b349f36":"# YOLOv5\u63a8\u8ad6","85fb71c4":"## testloader\u7528\u610f","0b63a4d4":"\u3053\u308c\u3092\u3069\u3046\u6539\u5584\u3057\u305f\u3089\u3044\u3044\u3093\u3060\u308d\u3046\u3002\u3084\u3063\u3071\u308a\u5b66\u7fd2\u30e2\u30c7\u30eb\u304b\u3089\u304b\u306a\u3042\u3002\u3068\u308a\u3042\u3048\u305a\u521d\u56de\u306f\u30010.244\u3067\u3057\u305f\u3002<br>\n\u5b66\u7fd2\u30e2\u30c7\u30eb\u3092\u6539\u5584\u3057\u305f\u30890.316\u306b\u306a\u308a\u307e\u3057\u305f\u3002\u5148\u306f\u9060\u3044\u4eca\u73fe\u5728\u30c8\u30c3\u30d7\u306e\u4eba\u306f0.651\u306a\u3093\u3067\u3059\u306d\u3002<br>\nyolo\u52c9\u5f37\u3057\u3066\u307e\u3059\u3002\u5b9f\u88c5\u65b9\u6cd5\u306f\u308f\u304b\u308a\u307e\u3057\u305f\u304c\u3001\u4ed5\u7d44\u307f\u304c\u308f\u304b\u3089\u306a\u3044\u3067\u3059\u3002<br>\n\u3088\u3046\u3084\u304f\u30aa\u30ea\u30b8\u30ca\u30eb\u306e\u30b9\u30b3\u30a2\u3092\u8d85\u3048\u307e\u3057\u305f\u3002\u3057\u304b\u3057\u3001\u3053\u3053\u304b\u3089\u3069\u3046\u3084\u3063\u3066\u30b9\u30b3\u30a2\u3092\u3042\u3052\u3066\u3044\u3051\u3070\u3044\u3044\u306e\u304b\u3002","056d7067":"# August 9, 2021 - Final Submission Deadline.","7f1d9bbc":"read_xray\u306f\u3061\u3083\u3093\u3068\u5165\u529b\u30d5\u30a1\u30a4\u30eb\u304b\u3089dicom\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u3093\u3067\u3044\u308b\u3002","559a9a4e":"\u3053\u306e\u30b3\u30fc\u30c9\u3092\u6539\u826f\u3057\u3066\u3044\u304f\u3053\u3068\u306b\u3057\u307e\u3057\u305f\u3002yolo5\u3092\u3064\u304b\u3063\u3066\u3044\u308b\u3057\u3001\u5b66\u7fd2\u304b\u3089\u63a8\u6e2c\u307e\u3067\u304d\u308c\u3044\u306b\u3084\u3063\u3066\u304f\u308c\u3066\u3044\u308b<br>\n\u5b66\u7fd2\u30d5\u30a7\u30fc\u30ba\u306f\u9055\u3044notebook\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002<br>\nyolo5\u3092\u77e5\u3089\u306a\u3044\u79c1\u306b\u3068\u3063\u3066\u306f\u3001\u3068\u3066\u3082\u3044\u3044\u304b\u3068\u304a\u3082\u3063\u3066\u307e\u3059\u3002<br>\n\u672c\u3067\u52c9\u5f37\u3057\u305fSSD\u306f\u3082\u3046\u53e4\u3044\u3063\u3066\u3053\u3068\u3067\u3059\u304b\u306d\u3002\u52c9\u5f37\u3057\u305f\u3070\u3063\u304b\u308a\u306a\u3093\u3067\u3059\u3051\u3069\u3002\u6700\u65b0\u6280\u8853\u306f\u672c\u306b\u8f09\u3063\u3066\u306a\u3044\u3002\u3002\u3002\u3002","94da7c80":"![image.png](attachment:afd8858b-b129-46e2-9348-2ca2a1b1b6b4.png)","14410763":"> \u3000v5\u306fUltralytics LLC\u3000\u30b9\u30da\u30a4\u30f3\u306e\u4f01\u696d\u304c\u767a\u8868\u3057\u305f\u3082\u306e\u3067\u3059\u304c\u3001\u8ad6\u6587\u3084\u8a73\u7d30\u60c5\u5831\u304c\u306a\u304f\u3001Redmon\u6c0f\u3068\u306e\u304b\u304b\u308f\u308a\u3082\u4e0d\u900f\u660e\u3067\u305d\u306e\u6b63\u5f53\u6027\u306b\u95a2\u3057\u3066\u554f\u984c\u8996\u3059\u308b\u610f\u898b\u304c\u30a6\u30a7\u30d6\u4e0a\u3067\u5b58\u5728\u3057\u3066\u3044\u307e\u3059\u3002\u307e\u305f\u6027\u80fd\u306b\u95a2\u3057\u3066\u3082v4\u306e\u65b9\u304c\u3088\u3044\u3068\u3044\u3046\u6bd4\u8f03\u7d50\u679c\u3092\u8f09\u305b\u3066\u3044\u308b\u30a6\u30a7\u30d6\u30b5\u30a4\u30c8\u3082\u307f\u3089\u308c\u3001v5\u306f\u305d\u306e\u7acb\u3061\u4f4d\u7f6e\u306a\u3069\u3092\u542b\u3081\u53d6\u6271\u3044\u65b9\u304c\u96e3\u3057\u3044\u3068\u3044\u3048\u307e\u3059\u3002\u30e2\u30c7\u30eb\u306f\u30a6\u30a7\u30d6\u4e0a\u306b\u30aa\u30fc\u30d7\u30f3\u306b\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u5b9f\u969b\u306b\u81ea\u8eab\u3067\u3054\u5229\u7528\u3057\u3066\u5224\u65ad\u3055\u308c\u308b\u306e\u304c\u4e00\u756a\u304b\u3068\u304a\u3082\u3044\u307e\u3059\u3002\u3046\u30fc\u3093\u3002<br>\n> yolov4\u306e\u8ad6\u6587\u306f\u3053\u308c\u3067\u3059<br>\n\n> https:\/\/arxiv.org\/pdf\/2004.10934.pdf","f3410264":"### \u5b66\u7fd2\u6e08\u306eEfficientNET\u3092\u5229\u7528\u3002512*512\u30d9\u30fc\u30b9\u3002\u3053\u306e\u30d5\u30a1\u30a4\u30eb\u306e\u4f5c\u308a\u65b9\u306f\u308f\u304b\u3089\u306a\u304b\u3063\u305f\u3002","f990f834":"![image.png](attachment:755e40c7-e29b-4922-878b-a7611fe3a9af.png)","4def3a6c":"### \u5b66\u7fd2\u6e08\u30e2\u30c7\u30eb\u3092\u4f5c\u3063\u3066datasets\u306b\u3057\u3066\u304a\u304f\u3002\u308f\u304b\u308a\u3084\u3059\u3044\u306e\u3067\u3001\u307f\u3093\u306a\u3067\u304d\u308b\u3068\u601d\u3046\u3002google drive\u30678\u6642\u9593\u304b\u3051\u3066\u5b66\u7fd2\u3055\u305b\u307e\u3057\u305f\u3002<br>\n### \u3042\u3068\u3001YOLOv5x\u306b\u3057\u3066\u3001size\u3092512\u306b\u5909\u3048\u3066\u3044\u307e\u3059\u30025x\u306e\u65b9\u304c\u30b9\u30b3\u30a2\u304c\u9ad8\u3044\u3051\u3069\u6642\u9593\u306f\u3059\u3054\u304f\u304b\u304b\u308a\u307e\u3059\u3002\n\n## \u3082\u306e\u30b9\u30b4\u30fc\u304f\u6642\u9593\u304b\u3051\u3066\u3084\u3063\u305f\u5272\u308a\u306b\u306f\u3001\u30b9\u30b3\u30a2\u304c\u4e0a\u304c\u3089\u305a\u3001\u5fc3\u304c\u6298\u308c\u307e\u3057\u305f\u3002\nhttps:\/\/www.kaggle.com\/ayuraj\/train-covid-19-detection-using-yolov5","136ca446":"## Hyperparameters"}}