{"cell_type":{"785cff77":"code","7422cc2a":"code","b70d31f0":"code","eab1c531":"code","5fda6c86":"code","d34e8389":"code","efe58268":"code","25e12f66":"code","829994c3":"code","da8213d2":"code","b9c6d1a5":"code","e0bcac0f":"code","640be639":"code","9b6e4c82":"code","ee17f492":"code","6ba45620":"code","48c486cb":"code","ee2600e7":"markdown"},"source":{"785cff77":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport random\nimport re\nfrom typing import Union\nfrom math import ceil, sqrt\nfrom os import mkdir, listdir\nfrom string import punctuation","7422cc2a":"EOS = '' # End of sentence\n\ndef process_sentences(sentences: list) -> list:\n    \n    def separate_punct(sentence: str) -> str:\n        s = ''\n        for ch in sentence:\n            if ch in punctuation:\n                s += f' {ch} '\n            else:\n                s += ch\n        return s\n    \n    sentences = list(map(separate_punct, sentences))\n    sentences = list(map(lambda x: x.lower().split(), sentences))\n    \n    return sentences\n\ndef build_vocabulary(sentences: list) -> list:\n    \n    v = set()\n    for sentence in sentences:\n        for word in sentence:\n            v.add(word)\n    \n    v.add(EOS)\n    \n    return list(v)\n\ndef word2index(vocabulary: list, word: str) -> int:\n    # returns the index of 'word' in the vocabulary\n    return vocabulary.index(word)\n\ndef index2onehot(idx: int, vocab_size: int) -> tf.Tensor:\n    a = np.zeros((1, vocab_size), dtype=np.double)\n    a[0, idx] = 1\n    return tf.constant(a)\n\ndef words2onehot(vocabulary: list, words: list) -> np.ndarray:\n    # transforms the list of words given as argument into\n    # a one-hot matrix representation using the index in the vocabulary\n    n_words = len(words)\n    n_voc = len(vocabulary)\n    indices = np.array([word2index(vocabulary, word) for word in words])\n    a = np.zeros((n_words, n_voc))\n    a[np.arange(n_words), indices] = 1\n    return a\n\ndef sentences2onehot(vocab: list, sentences: list) -> list:\n    time_steps = max([len(s) for s in sentences])\n    n_sent = len(sentences)\n\n    lst = []\n    for t in range(time_steps+1):\n        words = []\n        for i in range(n_sent):\n            if t < len(sentences[i]):\n                words.append(sentences[i][t])\n            else:\n                words.append(EOS)\n        lst.append(words2onehot(vocab, words))\n    \n    return lst","b70d31f0":"class LSTM:\n    def __init__(self, input_size: int = 0, units: int = 0, lstm = None):\n        \n        if lstm is not None:\n            self.__init_from_lstm(lstm)\n            return\n        \n        self.input_size = input_size\n        self.units = units\n        \n        combined_size = self.input_size + self.units\n        \n        weights_std_dev = sqrt(2.0\/(combined_size+self.units))\n        weights_shape = (combined_size, self.units)\n        \n        biases_std_dev = sqrt(2.0\/(1+self.units))\n        biases_shape = (1, self.units)\n        \n        self.weights = []\n        \n        for s in ['wu', 'wf', 'wo', 'wc']:\n            w = tf.Variable(tf.random.normal(\n                                    stddev=weights_std_dev,\n                                    shape=weights_shape,\n                                    dtype=tf.double))\n            setattr(self, s, w)\n            self.weights.append(w)\n        \n        for s in ['bu', 'bf', 'bo', 'bc']:\n            b = tf.Variable(tf.random.normal(\n                                    stddev=biases_std_dev,\n                                    shape=biases_shape,\n                                    dtype=tf.double))\n            setattr(self, s, b)\n            self.weights.append(b)\n    \n    def __init_from_lstm(self, lstm: 'LSTM'):\n        \n        self.input_size = lstm.input_size\n        self.units = lstm.units\n        \n        self.weights = []\n        \n        for s in ['wu', 'wf', 'wo', 'wc', 'bu', 'bf', 'bo', 'bc']:\n            w = tf.Variable(getattr(lstm, s))\n            setattr(self, s, w)\n            self.weights.append(w)\n        \n        self.a = lstm.a\n        self.c = lstm.c\n\n    def reset_state(self, num_samples: int) -> None:\n        self.a = tf.zeros((num_samples, self.units), dtype=tf.double)\n        self.c = tf.zeros((num_samples, self.units), dtype=tf.double)\n    \n    def __call__(self, x: Union[np.ndarray, tf.Tensor]) -> tf.Tensor:\n        \n        concat_matrix = tf.concat([self.a, x], axis=1)\n        \n        update_gate = tf.math.sigmoid(\n                            tf.linalg.matmul(concat_matrix, self.wu)\n                            + self.bu)\n        forget_gate = tf.math.sigmoid(\n                            tf.linalg.matmul(concat_matrix, self.wf)\n                            + self.bf)\n        output_gate = tf.math.sigmoid(\n                            tf.linalg.matmul(concat_matrix, self.wo)\n                            + self.bo)\n        \n        c_candidate = tf.math.tanh(\n                            tf.linalg.matmul(concat_matrix, self.wc)\n                            + self.bc)\n        \n        self.c = (tf.math.multiply(update_gate, c_candidate) +\n                         tf.math.multiply(forget_gate, self.c))\n        \n        self.a = tf.math.multiply(output_gate, tf.math.tanh(self.c))\n        \n        return self.a\n\n    def save(self, path: str) -> None:\n        \n        mkdir(f'{path}')\n        mkdir(f'{path}\/weights')\n        \n        with open(f'{path}\/units.txt', 'w') as f:\n            f.write(str(self.units))\n        with open(f'{path}\/input_size.txt', 'w') as f:\n            f.write(str(self.input_size))\n        \n        for s in ['wu', 'bu', 'wf', 'bf', 'wo', 'bo', 'wc', 'bc']:\n            np.save(f'{path}\/weights\/{s}.npy',\n                    getattr(self, s).numpy())\n    \n    def load(self, path: str) -> None:\n        \n        with open(f'{path}\/units.txt', 'r') as f:\n            self.units = int(f.read())\n        with open(f'{path}\/input_size.txt', 'r') as f:\n            self.input_size = int(f.read())\n        \n        self.weights = []\n        filenames = listdir(f'{path}\/weights')\n        \n        for filename in filenames:\n            attr_name = filename.replace('.npy', '')\n            w = tf.Variable(np.load(f'{path}\/weights\/{filename}'))\n            setattr(self, attr_name, w)\n            self.weights.append(w)","eab1c531":"class Encoder:\n    def __init__(self, vocab: list, sizes: list):\n        # sizes: [input, embedding, LSTM layer 1, ..., LSTM layer n]\n\n        self.vocab = vocab\n        self.sizes = sizes\n\n        self.we = tf.Variable(tf.random.normal(\n                                stddev=sqrt(2.0\/(sizes[0]+sizes[1])),\n                                shape=(sizes[0], sizes[1]),\n                                dtype=tf.double))\n        self.be = tf.Variable(tf.random.normal(\n                                stddev=sqrt(2.0\/(1+sizes[1])),\n                                shape=(1, sizes[1]),\n                                dtype=tf.double))\n\n        self.lstm_layers_forward = [LSTM(sizes[i], sizes[i+1]) for i in range(1, len(sizes)-1)]\n        \n        self.lstm_layers_backward = [LSTM(sizes[i], sizes[i+1]) for i in range(1, len(sizes)-1)]\n\n        self.weights = [self.we, self.be]\n        \n        for lstm_layer in self.lstm_layers_forward:\n            self.weights.extend(lstm_layer.weights)\n        \n        for lstm_layer in self.lstm_layers_backward:\n            self.weights.extend(lstm_layer.weights)\n    \n    def reset_state(self, num_samples: int) -> None:\n                              \n        for lstm_layer in self.lstm_layers_forward:\n            lstm_layer.reset_state(num_samples)\n        \n        for lstm_layer in self.lstm_layers_backward:\n            lstm_layer.reset_state(num_samples)\n    \n    def get_output_size(self) -> int:\n        return 2*self.sizes[-1]\n    \n    def __call__(self, sentences: list) -> list:\n        \n        x_list = sentences2onehot(self.vocab, sentences)\n        \n        forward_outputs = []\n        for x in x_list:\n            x = tf.linalg.matmul(x, self.we) + self.be\n            for lstm_layer in self.lstm_layers_forward:\n                x = lstm_layer(x)\n            forward_outputs.append(x)\n        \n        backward_outputs = []\n        for x in x_list[::-1]:\n            x = tf.linalg.matmul(x, self.we) + self.be\n            for lstm_layer in self.lstm_layers_backward:\n                x = lstm_layer(x)\n            backward_outputs.append(x)\n        backward_outputs = backward_outputs[::-1]\n        \n        output = list(map(lambda a, b: tf.concat([a, b], axis=1),\n                        forward_outputs, backward_outputs))\n        \n        return tf.transpose(tf.stack(output, axis=0), perm=[1, 0, 2])\n\n    def save(self, path: str) -> None:\n        mkdir(f'{path}')\n        mkdir(f'{path}\/weights')\n        \n        with open(f'{path}\/sizes.txt', 'w') as f:\n            f.write(','.join(map(lambda e: str(e), self.sizes)))\n        with open(f'{path}\/vocabulary.txt', 'w') as f:\n            f.write('[separator]'.join(self.vocab))\n        \n        for s in ['we', 'be']:\n            np.save(f'{path}\/weights\/{s}.npy',\n                    getattr(self, s).numpy())\n        \n        for i, lstm_layer in enumerate(self.lstm_layers_forward):\n            lstm_layer.save(f'{path}\/lstmlayerforward_{i}')\n        \n        for i, lstm_layer in enumerate(self.lstm_layers_backward):\n            lstm_layer.save(f'{path}\/lstmlayerbackward_{i}')\n    \n    def load(self, path: str) -> None:\n        \n        with open(f'{path}\/sizes.txt', 'r') as f:\n            self.sizes = list(map(lambda x: int(x), f.read().split(',')))\n        with open(f'{path}\/vocabulary.txt', 'r') as f:\n            self.vocab = f.read().split('[separator]')\n\n        self.lstm_layers_forward = [LSTM(self.sizes[i], self.sizes[i+1]) for i in range(1, len(self.sizes)-1)]\n        \n        self.lstm_layers_backward = [LSTM(self.sizes[i], self.sizes[i+1]) for i in range(1, len(self.sizes)-1)]\n\n        self.weights = []\n        filenames = listdir(f'{path}\/weights')\n        \n        for filename in filenames:\n            attr_name = filename.replace('.npy', '')\n            w = tf.Variable(np.load(f'{path}\/weights\/{filename}'))\n            setattr(self, attr_name, w)\n            self.weights.append(w)\n        \n        lstm_layers_forward = [f for f in listdir(f'{path}') if f.startswith('lstmlayerforward')]\n        for lstm_layer in lstm_layers_forward:\n            _, layer_index = lstm_layer.split('_')\n            layer_index = int(layer_index)\n            self.lstm_layers_forward[layer_index].load(f'{path}\/{lstm_layer}')\n        \n        lstm_layers_backward = [f for f in listdir(f'{path}') if f.startswith('lstmlayerbackward')]\n        for lstm_layer in lstm_layers_backward:\n            _, layer_index = lstm_layer.split('_')\n            layer_index = int(layer_index)\n            self.lstm_layers_backward[layer_index].load(f'{path}\/{lstm_layer}')\n        \n        for lstm_layer in self.lstm_layers_forward:\n            self.weights.extend(lstm_layer.weights)\n        \n        for lstm_layer in self.lstm_layers_backward:\n            self.weights.extend(lstm_layer.weights)","5fda6c86":"class Decoder:\n    def __init__(self, vocab: list = [], sizes: list = [], decoder = None):\n        # sizes: [input, embedding, LSTM layer 1, ..., LSTM layer n, output]\n\n        if decoder is not None:\n            self.__init_from_decoder(decoder)\n            return\n        \n        self.vocab = vocab\n        self.sizes = sizes\n\n        self.we = tf.Variable(tf.random.normal(\n                                stddev=sqrt(2.0\/(sizes[0]+sizes[1])),\n                                shape=(sizes[0], sizes[1]),\n                                dtype=tf.double))\n        self.be = tf.Variable(tf.random.normal(\n                                stddev=sqrt(2.0\/(1+sizes[1])),\n                                shape=(1, sizes[1]),\n                                dtype=tf.double))\n\n        self.lstm_layers = [LSTM(sizes[i], sizes[i+1]) for i in range(1, len(sizes)-2)]\n\n        self.wy = tf.Variable(tf.random.normal(\n                                stddev=sqrt(2.0\/(sizes[-2]+sizes[-1])),\n                                shape=(sizes[-2], sizes[-1]),\n                                dtype=tf.double))\n        self.by = tf.Variable(tf.random.normal(\n                                stddev=sqrt(2.0\/(1+sizes[-1])),\n                                shape=(1, sizes[-1]),\n                                dtype=tf.double))\n\n        self.weights = [self.we, self.be, self.wy, self.by]\n        for lstm_layer in self.lstm_layers:\n            self.weights.extend(lstm_layer.weights)\n\n    def __init_from_decoder(self, decoder: 'Decoder'):\n\n        self.vocab = decoder.vocab\n        self.sizes = decoder.sizes\n\n        self.we = tf.Variable(decoder.we)\n        self.be = tf.Variable(decoder.be)\n\n        self.lstm_layers = [LSTM(lstm=lstm) for lstm in decoder.lstm_layers]\n\n        self.wy = tf.Variable(decoder.wy)\n        self.by = tf.Variable(decoder.by)\n\n        self.weights = [self.we, self.be, self.wy, self.by]\n        for lstm_layer in self.lstm_layers:\n            self.weights.extend(lstm_layer.weights)\n    \n    def reset_state(self, num_samples: int) -> None:\n        for lstm_layer in self.lstm_layers:\n            lstm_layer.reset_state(num_samples)\n    \n    def get_current_state(self) -> tf.Tensor:\n        return self.lstm_layers[0].a\n    \n    def get_state_size(self) -> int:\n        return self.sizes[2]\n    \n    def __call__(self,\n                 x: Union[np.ndarray, tf.Tensor],\n                 y: Union[np.ndarray, tf.Tensor, None] = None) -> tf.Tensor:\n                              \n        x = tf.linalg.matmul(x, self.we) + self.be\n        \n        for lstm_layer in self.lstm_layers:\n            x = lstm_layer(x)\n        \n        y_logits = tf.linalg.matmul(x, self.wy)+self.by\n        if y is None:\n            # during prediction return softmax probabilities\n            return tf.nn.softmax(y_logits)\n        else:\n            # during training return loss\n            return tf.math.reduce_mean(\n                        tf.nn.softmax_cross_entropy_with_logits(y, y_logits))\n\n    def save(self, path: str) -> None:\n        mkdir(f'{path}')\n        mkdir(f'{path}\/weights')\n        \n        with open(f'{path}\/sizes.txt', 'w') as f:\n            f.write(','.join(map(lambda e: str(e), self.sizes)))\n        with open(f'{path}\/vocabulary.txt', 'w') as f:\n            f.write('[separator]'.join(self.vocab))\n        \n        for s in ['we', 'be', 'wy', 'by']:\n            np.save(f'{path}\/weights\/{s}.npy',\n                    getattr(self, s).numpy())\n        \n        for i, lstm_layer in enumerate(self.lstm_layers):\n            lstm_layer.save(f'{path}\/lstmlayer_{i}')\n    \n    def load(self, path: str) -> None:\n        \n        with open(f'{path}\/sizes.txt', 'r') as f:\n            self.sizes = list(map(lambda x: int(x), f.read().split(',')))\n        with open(f'{path}\/vocabulary.txt', 'r') as f:\n            self.vocab = f.read().split('[separator]')\n\n        self.lstm_layers = [LSTM(self.sizes[i], self.sizes[i+1]) for i in range(1, len(self.sizes)-2)]\n\n        self.weights = []\n        filenames = listdir(f'{path}\/weights')\n        \n        for filename in filenames:\n            attr_name = filename.replace('.npy', '')\n            w = tf.Variable(np.load(f'{path}\/weights\/{filename}'))\n            setattr(self, attr_name, w)\n            self.weights.append(w)\n        \n        lstm_layers = [f for f in listdir(f'{path}') if f.startswith('lstmlayer')]\n        for lstm_layer in lstm_layers:\n            _, layer_index = lstm_layer.split('_')\n            layer_index = int(layer_index)\n            self.lstm_layers[layer_index].load(f'{path}\/{lstm_layer}')\n        \n        for lstm_layer in self.lstm_layers:\n            self.weights.extend(lstm_layer.weights)","d34e8389":"class AttentionWeights:\n    def __init__(self, input_size: int):\n        \n        self.input_size = input_size\n\n        self.w = tf.Variable(tf.random.normal(\n                                stddev=sqrt(2.0\/(input_size+1)),\n                                shape=(input_size, 1),\n                                dtype=tf.double))\n        self.b = tf.Variable(tf.random.normal(\n                                stddev=1.0,\n                                shape=(1, 1),\n                                dtype=tf.double))\n\n        self.weights = [self.w, self.b]\n    \n    def __call__(self, x: Union[np.ndarray, tf.Tensor]) -> tf.Tensor:\n        \n        x = tf.linalg.matmul(x, self.w) + self.b\n        x = tf.nn.softmax(x, axis=1)\n        \n        return x\n\n    def save(self, path: str) -> None:\n        mkdir(f'{path}')\n        mkdir(f'{path}\/weights')\n        \n        with open(f'{path}\/input_size.txt', 'w') as f:\n            f.write(str(self.input_size))\n        \n        np.save(f'{path}\/weights\/w.npy', self.w.numpy())\n        \n        np.save(f'{path}\/weights\/b.npy', self.b.numpy())\n    \n    def load(self, path: str) -> None:\n        \n        with open(f'{path}\/input_size.txt', 'r') as f:\n            self.input_size = int(f.read())\n        \n        self.w = tf.Variable(np.load(f'{path}\/weights\/w.npy'))\n        \n        self.b = tf.Variable(np.load(f'{path}\/weights\/b.npy'))\n\n        self.weights = [self.w, self.b]","efe58268":"class AttentionModel:\n    def __init__(self, encoder: Encoder, decoder: Decoder):\n        self.encoder = encoder\n        self.decoder = decoder\n        self.attention_weights = AttentionWeights(\n                                    encoder.get_output_size()+\n                                    decoder.get_state_size())\n        self.weights = []\n        self.weights.extend(self.encoder.weights)\n        self.weights.extend(self.decoder.weights)\n        self.weights.extend(self.attention_weights.weights)\n        \n        self.optimizer = tf.keras.optimizers.Adam()\n    \n    def translate(self, sentence: str, beam_size: int = 3) -> str:\n        \n        self.encoder.reset_state(1)\n        self.decoder.reset_state(1)\n        sentences = process_sentences([sentence])\n        sentences = [list(filter(lambda e: e in self.encoder.vocab, sentences[0]))]\n        e_out = self.encoder(sentences)\n        out_vocab_size = len(self.decoder.vocab)\n        \n        decoders = [Decoder(decoder=self.decoder) for i in range(beam_size)]\n        avg_log_probs = [0 for i in range(beam_size)]\n        xs = [tf.zeros((1, self.decoder.sizes[-1]), dtype=tf.double)\n              for i in range(beam_size)]\n        translations = ['' for i in range(beam_size)]\n        \n        t = 1\n        while True:\n            prob_choices = []\n            for i in range(beam_size):\n                d_state = decoders[i].get_current_state()\n                d_state = tf.tile(tf.expand_dims(d_state, 1), [1, e_out.shape[1], 1])\n                attention_state = tf.concat([e_out, d_state], axis=2)\n                a_weights = self.attention_weights(attention_state)\n                context = tf.linalg.matmul(tf.transpose(a_weights, [0, 2, 1]), e_out)\n                context = tf.squeeze(context, [1])\n                prob = decoders[i](tf.concat([xs[i], context], axis=1))\n                prob = ((t-1)*avg_log_probs[i]+tf.math.log(prob))\/t\n                prob_choices.extend([(i, j, prob[0, j]) for j in range(prob.shape[1])])\n            \n            prob_choices.sort(reverse=True, key=lambda e: e[2])\n            prob_choices = prob_choices[0:beam_size]\n            \n            decoders =      [Decoder(decoder=decoders[beam_index])\n                             for beam_index, word_index, prob in prob_choices]\n            avg_log_probs = [prob for beam_index, word_index, prob in prob_choices]\n            xs =            [index2onehot(word_index, out_vocab_size)\n                             for beam_index, word_index, prob in prob_choices]\n            translations =  [translations[beam_index]+f' {self.decoder.vocab[word_index]}'\n                             for beam_index, word_index, prob in prob_choices]\n            t += 1\n            \n            num_eos = sum([int(self.decoder.vocab[word_index] == EOS)\n                           for beam_index, word_index, prob in prob_choices])\n            if num_eos == beam_size:\n                break\n        \n        return translations[0].strip()\n    \n    def fit(self,\n            sentences_pairs: list,\n            batch_size: int = 128,\n            epochs: int = 10) -> None:\n        \n        n_sent = len(sentences_pairs)\n        num_batches = ceil(n_sent \/ batch_size)\n        \n        for epoch in range(epochs):\n            \n            random.shuffle(sentences_pairs)\n            start = 0\n            batch_idx = 0\n            \n            while start < n_sent:\n                \n                print('Training model: %05.2f%%' %\n                      (100*(epoch*num_batches+batch_idx+1)\/(epochs*num_batches),),\n                      end='\\r')\n                \n                batch_idx += 1\n                end = min(start+batch_size, n_sent)\n                batch_sent = sentences_pairs[start:end]\n                start = end\n                \n                sentences_in = [pair[0] for pair in batch_sent]\n                sentences_out = [pair[1] for pair in batch_sent]\n                \n                with tf.GradientTape() as tape:\n                \n                    loss = self.process_batch(sentences_in, sentences_out)\n                \n                grads = tape.gradient(loss, self.weights)\n                self.optimizer.apply_gradients(zip(grads, self.weights))\n        \n    \n    def process_batch(self, sentences_in: list, sentences_out: list) -> tf.Tensor:\n        \n        n_sent = len(sentences_in)\n        self.encoder.reset_state(n_sent)\n        self.decoder.reset_state(n_sent)\n        e_out = self.encoder(sentences_in)\n        \n        x = tf.zeros((n_sent, self.decoder.sizes[-1]), dtype=tf.double)\n        y_list = sentences2onehot(self.decoder.vocab, sentences_out)\n        losses = []\n        \n        for y in y_list:\n            d_state = self.decoder.get_current_state()\n            d_state = tf.tile(tf.expand_dims(d_state, 1), [1, e_out.shape[1], 1])\n            attention_state = tf.concat([e_out, d_state], axis=2)\n            a_weights = self.attention_weights(attention_state)\n            context = tf.linalg.matmul(tf.transpose(a_weights, [0, 2, 1]), e_out)\n            context = tf.squeeze(context, [1])\n            l = self.decoder(tf.concat([x, context], axis=1), y)\n            losses.append(l)\n            x = y\n        \n        return tf.math.reduce_mean(losses)\n    \n    def save(self, path: str) -> None:\n        mkdir(f'{path}')\n        \n        self.encoder.save(f'{path}\/encoder')\n        self.decoder.save(f'{path}\/decoder')\n        self.attention_weights.save(f'{path}\/attention_weights')\n    \n    def load(self, path: str) -> None:\n        self.encoder.load(f'{path}\/encoder')\n        self.decoder.load(f'{path}\/decoder')\n        self.attention_weights.load(f'{path}\/attention_weights')\n        \n        self.weights = []\n        self.weights.extend(self.encoder.weights)\n        self.weights.extend(self.decoder.weights)\n        self.weights.extend(self.attention_weights.weights)\n        \n        self.optimizer = tf.keras.optimizers.Adam()","25e12f66":"df = pd.read_csv('..\/input\/language-translation-englishfrench\/eng_-french.csv')\ndf","829994c3":"sentences_en = process_sentences(list(df.values[:, 0]))\nsentences_fr = process_sentences(list(df.values[:, 1]))","da8213d2":"vocabulary_en = build_vocabulary(sentences_en)\nvocabulary_fr = build_vocabulary(sentences_fr)\n\nn_vocab_en = len(vocabulary_en)\nn_vocab_fr = len(vocabulary_fr)","b9c6d1a5":"e_out_size = 256\nencoder = Encoder(vocabulary_en, [n_vocab_en, 128, e_out_size])\ndecoder = Decoder(vocabulary_fr, [n_vocab_fr+2*e_out_size, 128, 256, n_vocab_fr])\nattention_model = AttentionModel(encoder, decoder)\nattention_model.load('..\/input\/attentionmodelenfr\/english_french_translation')","e0bcac0f":"sentences_pairs = [[sentences_en[i], sentences_fr[i]]\n                   for i in range(df.values.shape[0])]","640be639":"attention_model.fit(sentences_pairs, batch_size=64, epochs=2)","9b6e4c82":"attention_model.save('english_french_translation')\n# attention_model.load('english_french_translation')","ee17f492":"attention_model.translate('What is your favourite movie?', 5)","6ba45620":"attention_model.translate('I have homework for tomorrow.', 5)","48c486cb":"attention_model.translate('What do you do for a living?', 5)","ee2600e7":"# Attention Model for English-French Translation"}}