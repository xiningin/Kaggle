{"cell_type":{"57a526cf":"code","72a57076":"code","3a215986":"code","fdcce261":"code","2460660d":"code","af2c36d6":"code","06063397":"code","c69fd1e3":"code","52f3a75d":"code","909834ba":"code","280b97d4":"code","b330cf32":"code","c291e5ed":"code","f5921951":"markdown","a13bcc8a":"markdown","0fbae421":"markdown","79243dad":"markdown","785fdde1":"markdown","458f0401":"markdown","ba9e43d6":"markdown","ecc7d4d7":"markdown","a3e05336":"markdown","dd8c7e28":"markdown","38d10f95":"markdown","2f733118":"markdown","226fc4bb":"markdown","cd3e8672":"markdown","a33d3706":"markdown","d5e76109":"markdown","732a18dd":"markdown","05a007f9":"markdown","c83d57a6":"markdown","d1b91fea":"markdown","d02922e5":"markdown","42929e0a":"markdown","31c530de":"markdown","def01f0d":"markdown"},"source":{"57a526cf":"import pandas as pd\n\ndata=pd.read_csv(r\"..\/input\/bbc-text.csv\")","72a57076":"data.head()","3a215986":"from sklearn.preprocessing import LabelEncoder\ndf2 = pd.DataFrame()\ndf2[\"text\"] = data[\"text\"]\ndf2[\"label\"] = LabelEncoder().fit_transform(data[\"category\"])","fdcce261":"import nltk\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\ndf2['text'] = df2['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\ndf2['text'].head()","2460660d":"df2.head()","af2c36d6":"freq = pd.Series(' '.join(df2['text']).split()).value_counts()[-10:]\ndf2['text'] = df2['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ndf2['text'].head()","06063397":"import pandas as pd\nimport numpy as np\nimport spacy\nfrom tqdm import tqdm\nimport re\nimport time\nimport pickle\npd.set_option('display.max_colwidth', 200)","c69fd1e3":"import tensorflow_hub as hub\nimport tensorflow as tf\n\nembed = hub.Module(\"https:\/\/tfhub.dev\/google\/elmo\/2\", trainable=True)","52f3a75d":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport pandas as pd\nfrom sklearn import preprocessing\nimport keras\nimport numpy as np\n\n\ny = list(df2['label'])\nx = list(df2['text'])\n\nle = preprocessing.LabelEncoder()\nle.fit(y)\n\ndef encode(le, labels):\n    enc = le.transform(labels)\n    return keras.utils.to_categorical(enc)\n\ndef decode(le, one_hot):\n    dec = np.argmax(one_hot, axis=1)\n    return le.inverse_transform(dec)\n\n\nx_enc = x\ny_enc = encode(le, y)","909834ba":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(np.asarray(x_enc), np.asarray(y_enc), test_size=0.2, random_state=42)","280b97d4":"x_train.shape","b330cf32":"from keras.layers import Input, Lambda, Dense\nfrom keras.models import Model\nimport keras.backend as K\n\ndef ELMoEmbedding(x):\n    return embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]\n\ninput_text = Input(shape=(1,), dtype=tf.string)\nembedding = Lambda(ELMoEmbedding, output_shape=(1024, ))(input_text)\ndense = Dense(256, activation='relu')(embedding)\npred = Dense(5, activation='softmax')(dense)\nmodel = Model(inputs=[input_text], outputs=pred)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nwith tf.Session() as session:\n    K.set_session(session)\n    session.run(tf.global_variables_initializer())  \n    session.run(tf.tables_initializer())\n    history = model.fit(x_train, y_train, epochs=1, batch_size=16)\n    model.save_weights('.\/elmo-model.h5')\n\nwith tf.Session() as session:\n    K.set_session(session)\n    session.run(tf.global_variables_initializer())\n    session.run(tf.tables_initializer())\n    model.load_weights('.\/elmo-model.h5')  \n    predicts = model.predict(x_test, batch_size=16)\n\ny_test = decode(le, y_test)\ny_preds = decode(le, predicts)\n\n","c291e5ed":"from sklearn import metrics\n\nprint(metrics.confusion_matrix(y_test, y_preds))\n\nprint(metrics.classification_report(y_test, y_preds))\n\nfrom sklearn.metrics import accuracy_score\n\nprint(\"Accuracy of ELMO is:\",accuracy_score(y_test,y_preds))","f5921951":"### Step 2.4 Convert Sentence to Elmo Vectors","a13bcc8a":"## 1.1 Defination :","0fbae421":"**Accuracy** - Classification accuracy is the number of correct predictions made as a\nratio of all predictions made\n\n**Precision** - precision (also called positive predictive value) is the fraction of\nrelevant instances among the retrieved instances\n\n**F1_score** - considers both the precision and the recall of the test to compute the\nscore\n\n**Recall** \u2013 recall (also known as sensitivity) is the fraction of relevant instances that\nhave been retrieved over the total amount of relevant instances\n\n**Why these metrics?** - We took Accuracy, Precision, F1 Score and Recall as metrics\nfor evaluating our model because accuracy would give an estimate of correct prediction. Precision would give us an estimate about the positive category predicted value i.e. how much our model is giving relevant result. F1 Score gives a clubbed estimate of precision and recall.Recall would provide us the relevant positive category prediction to the false negative and true positive category recognition results.","79243dad":"### Step 2.5 Train Keras neural model with ELMO Embeddings","785fdde1":"* Explore preprocessing steps on data.\n* Explore other models as baseline.\n* Make this notebook more informative and illustrative.\n* Explaination on ELMO Embeddings Model.\n* More time on data exploration\nand many more...","458f0401":"## 1.2 Problem Statement","ba9e43d6":"We will be using **ELMO embeddings with KERAS** for this use case. \n\nELMO and KERAS is not in the scope of this kernal. Kindly refer other external sources.","ecc7d4d7":"In today world** Text Classification\/Segmentation\/Categorization** (for example ticket categorization in a call centre, email classification, logs category detection etc.) is a common task. With humongous data out there, its nearly impossible to do this manually. Let's try to solve this problem automatically using machine learning and natural language processing tools.","a3e05336":">** Past Work mentioned on this dataset at max achieved 95.22 accuracies. keras with ELMO embeddings achieved 95.5. But still BERT base model without any preprocessing and achieved 97.75 accuracies.\n\n[bert model]https:\/\/www.kaggle.com\/sarthak221995\/textclassification-97-77-accuracy-bert\n**","dd8c7e28":"# 2. Data Exploration","38d10f95":"# 3. Implementation","2f733118":"### Step 2.2 Map Textual labels to numeric using Label Encoder","226fc4bb":"BBC articles dataset(2126 records) consist of two features text and the assiciated categories namely \n1. Sport \n2. Business \n3. Politics \n4. Tech \n5. Others\n\n**Our task is to train a multiclass classification model on the mentioned dataset.**","cd3e8672":"> This kernel is based on the work of http:\/\/hunterheidenreich.com\/blog\/elmo-word-vectors-in-keras\/","a33d3706":"# 4. Results","d5e76109":"> This kernel is based on the work of http:\/\/hunterheidenreich.com\/blog\/elmo-word-vectors-in-keras\/","732a18dd":"### Step 2.1 Load Dataset","05a007f9":"### Step 2.5 Divide dataset to test and train dataset","c83d57a6":"### Step 2.3 Import the Libraries","d1b91fea":"# 5. Future Improvements on this kernel:","d02922e5":"1. # 1. Kernel Overview","42929e0a":"## 1.4 Machine Learning Model Considered:","31c530de":"# 6. References","def01f0d":"## 1.3 Metrics"}}