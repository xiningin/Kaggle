{"cell_type":{"2252628a":"code","47688acd":"code","4b5e6b99":"code","a8340deb":"code","6a3b69d7":"code","4dadc057":"code","cf875bde":"code","290fd097":"code","8254f1ec":"code","24541fd8":"code","f3aa8ed9":"code","e859aea1":"code","b1b00434":"code","15faeba5":"code","8a3de2ec":"code","fc86839b":"code","ced23347":"code","4ecc9578":"code","23f86107":"code","8857a988":"code","3b3e2631":"code","a0441764":"code","335082ee":"code","3a4813d1":"code","c05b02a8":"code","3536caca":"code","998142c4":"code","944bf4de":"markdown","a21676d4":"markdown","5abdc9b7":"markdown","480d834e":"markdown","a91843aa":"markdown","908ed38f":"markdown"},"source":{"2252628a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","47688acd":"df0 = pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')\ndf0","4b5e6b99":"df= df0.copy()","a8340deb":"df.isnull().sum() ","6a3b69d7":" # delet some unusefull Columns \ndel df['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2']\ndel df['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1']\ndel df['CLIENTNUM']","4dadc057":"df.describe()","cf875bde":"df.info()","290fd097":"import matplotlib.pyplot as plt\ncount=pd.value_counts(df['Attrition_Flag']).tolist()\nplt.figure(figsize=(5,5))\nplt.title(\"Percentage of Attrited Customer and Existing Customer\")\nplt.pie(x=count,labels=[\"Attrited Customer\",\"Existing Customers\"],autopct='%.2f%%')","8254f1ec":"import seaborn as sns\nplt.figure(figsize=(28,11))\nplt.title(\"Distribution of Age with respect to Churned or not\")\nsns.countplot(data=df,x=df[\"Customer_Age\"],hue=\"Attrition_Flag\")","24541fd8":"\ncats = ['Gender','Education_Level','Marital_Status', 'Income_Category', 'Card_Category','Attrition_Flag']\n\nfig, axis = plt.subplots(len(cats) \/\/ 3,3, figsize=(20,12))\nfig.suptitle('The distibution of the Churners')\n\nindex = 0\nfor i in range(len(cats) \/\/ 3):\n      for j in range(3):\n            \n            ax = sns.countplot(ax=axis[i][j] ,x=df[cats[index]], hue=df['Attrition_Flag'],palette=\"Set1\")\n            ax.legend(title='Customer',loc='upper right',labels=['Existing', 'attired'])\n            for p in ax.patches:\n                height = p.get_height()\n                ax.text(p.get_x()+p.get_width()\/2.,\n                        height + 3,\n                        '{:1.2f}%'.format(height\/len(df)*100),\n                        ha=\"center\") ","f3aa8ed9":"#Cr\u00e9ation de sous-ensembles positifs et n\u00e9gatifs\n\nnumr = ['Months_Inactive_12_mon','Contacts_Count_12_mon','Credit_Limit',\n                  'Total_Amt_Chng_Q4_Q1','Total_Trans_Ct','Total_Ct_Chng_Q4_Q1']\n\nexisting_df = df[df['Attrition_Flag'] == 'Existing Customer']\nattired_df = df[df['Attrition_Flag'] == 'Attrited Customer']\n\n\nfor col in numr:\n    plt.figure()\n    sns.distplot(existing_df[col], label='existing')\n    sns.distplot(attired_df[col], label=' attired')\n    plt.legend()","e859aea1":"import seaborn as sns\n\n\nfig, axis = plt.subplots(len(cats) \/\/ 3,3, figsize=(20,12))\nfig.suptitle('The distibution of the numr')\nsns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n\n\nindex = 0\nfor i in range(len(cats) \/\/ 3):\n      for j in range(3):\n            \n            ax = sns.boxplot(ax=axis[i][j] ,x=df[numr[index]])\n            index += 1\n            #sns.set_theme(style=\"dark\")","b1b00434":"#outlier cleanup\nfrom scipy import stats\nimport numpy as np\ncolumns = [\"Customer_Age\", 'Dependent_count', 'Months_on_book',\n       'Total_Relationship_Count', 'Months_Inactive_12_mon',\n       'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal',\n       'Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt',\n       'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']\nprint(df.shape)\nfor column in columns : \n    z = np.abs(stats.zscore(df[column]))\n    df=df[(z < 3)]\nprint(df.shape)","15faeba5":"df_cat=df.select_dtypes(include=[object])\ndf_numr=df.select_dtypes(exclude=[object])","8a3de2ec":"# one hot encoding of object category data\nobj_data=pd.get_dummies(df_cat.drop(columns=['Attrition_Flag']),drop_first=True)\nobj_data\n","fc86839b":"# joining our numr dataframe and object dataframe\ndf=pd.concat([obj_data,df_numr],axis=1)\ndf.shape","ced23347":"# splitting dependent and independent feature \n\nY=pd.get_dummies(df_cat['Attrition_Flag'],drop_first=True)\nX=df\n\nprint(X.shape)\nprint(Y.shape)","4ecc9578":"# dividing dataset into training and testing set \nfrom sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\nprint(X_train.shape,y_train.shape,X_test.shape,y_test.shape)","23f86107":"from imblearn.combine import SMOTETomek\nfrom collections import Counter \n\n\nsmk=SMOTETomek(random_state=42)\nxres,yres=smk.fit_sample(X_train,y_train.values.ravel())\n\nprint(yres.shape, xres.shape)\nprint('Old Shape {}'.format(Counter(Y)))\nprint('Resampled Shape {}'.format(Counter(yres)))","8857a988":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier \nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\nfrom sklearn.ensemble.forest import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler ","3b3e2631":"preprocessor =  make_pipeline(SelectKBest(f_classif, k=10) ,PolynomialFeatures(2))\n\n\nAdaBoost = make_pipeline(preprocessor,AdaBoostClassifier(random_state=0))\nSVM= make_pipeline(preprocessor, StandardScaler(), SVC(random_state=0))\nGBoost = make_pipeline(preprocessor, StandardScaler(), GradientBoostingClassifier())\nRandomForest = make_pipeline( preprocessor, RandomForestClassifier())\nXGB = make_pipeline( preprocessor, XGBClassifier())\nExtree = make_pipeline( preprocessor, ExtraTreesClassifier())\n\ndict_of_models = {'AdaBoost':AdaBoost,\n                  'SVM':SVM,\n                  'GBoost':GBoost,\n                  'RandomForest':RandomForest,\n                  'XGB':XGB,\n                  'Extree':Extree}","a0441764":"from sklearn.metrics import f1_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import GridSearchCV","335082ee":"def evaluation(model_1):\n    \n    #np.seterr(divide='ignore', invalid='ignore')\n    model_1.fit(xres, yres)\n    ypred = model_1.predict(X_test)\n    cm = confusion_matrix(y_test, ypred)\n    #sns.heatmap(cm,annot=True, annot_kws={\"size\": 16})\n    N, train_score, val_score = learning_curve(model_1, xres, yres,\n                                              cv=4, scoring='f1',\n                                               train_sizes=np.linspace(0.1, 1, 10))\n    \n    \n    plt.figure(figsize=(12, 8))\n    plt.plot(N, train_score.mean(axis=1), label='train score')\n    plt.plot(N, val_score.mean(axis=1), label='validation score')\n    plt.legend()\n    \n    \n    print(confusion_matrix(y_test, ypred))\n    print(classification_report(y_test, ypred))","3a4813d1":"\nfor name, model in dict_of_models.items():\n    print(name)\n    evaluation(model)\n","c05b02a8":"\nparams = {\"max_depth\" : [6,7,8],\n          \"min_child_weight\" : [1,2,3],\n          'gamma ':[0],\n          'subsample':[0.6,0.7,0.8],\n          #'colsample_bytree':\n         }\n    \n         \n\nclf = XGBClassifier()\ngrid= GridSearchCV (clf, params,scoring=\"recall\", cv = 3)\n\ngrid_result = grid.fit(xres, yres)\n\n\n\n\n#Summary \n#print(\"classifcation report: \",classification_report(y_test, y_pred))\nprint(\"Best: %f using %s \" % (grid_result.best_score_, grid_result.best_params_) )\nmeans = grid_result.cv_results_[\"mean_test_score\"]\nstds = grid_result.cv_results_[\"std_test_score\"]\nparams = grid_result.cv_results_[\"params\"]\n\nfor mean, stdev, param in zip (means, stds, params):\n    print(\"%f (%f) with : %r\" % (mean, stdev, param))","3536caca":"from sklearn.metrics import*\nfrom sklearn.model_selection import* \nfrom sklearn.preprocessing import*\n\n\nxgb = XGBClassifier(gamma = 0, max_depth=7, min_child_weight= 1, subsample= 0.8) \nxgb.fit(xres, yres)\ny_pred = xgb.predict(X_test)\n\nacc = accuracy_score(y_test, y_pred)\nmse = mean_squared_error(y_test.values, y_pred)\n\nprint(\"Acc  : \", acc)","998142c4":"pd.DataFrame(xgb.feature_importances_, index=X_train.columns).plot.bar(figsize=(12, 8))","944bf4de":"# Choose the Best MODEL and Tune Hyperparamater","a21676d4":"# Preprocessing & Feature engineering\n","5abdc9b7":"# Start Exploratory analysis of the Data : \n\nin this Step we need to collect some information from our Data. Graph Visualisation(Heatmap, box, bar,Line...) will help us a lot in this Step. \nTake notes of :\n\n   * Data Type  (Numerical\/Categorical) \n   \nand check : \n\n   * Missing Data \n   * Distribution \n   * Relation between Features (Correlations)             \n   * Outleirs \n   * Balance \/ Imbalance Data  \n   \n","480d834e":"# A Basic Approach of a beginner getting <span style=\"color:red\">97,5% <\/span>.","a91843aa":"# BUILDING MODELS & EVALUATION\n","908ed38f":"load the Data "}}