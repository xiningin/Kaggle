{"cell_type":{"b77001da":"code","b522b781":"code","878242c6":"code","3df32566":"code","4e3f6f20":"code","20d30884":"code","8219f726":"code","22c07b44":"code","0d5bc7c2":"code","31c1f612":"code","2a41bde8":"markdown","44c93fbe":"markdown","9f116cde":"markdown","e64a1093":"markdown","11634e85":"markdown","f317c87d":"markdown","482d5ca9":"markdown","b33c4fec":"markdown"},"source":{"b77001da":"!pip install transformers","b522b781":"import torch\nimport numpy as np\n    \ndef fix_randomness():\n    seed = 123\n    np.random.seed(seed)\n    torch.random.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    \nfix_randomness()","878242c6":"import logging\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nlogging.getLogger().setLevel(logging.CRITICAL) # Disable an annoying warning for now\n\nSAMPLE_INPUTS = [\n    \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\",\n    \"A train carriage containing controlled nuclear materials was stolen in Cincinnati today. Its whereabouts are unknown.\",\n    \"Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.\",\n    \"We\u2019ve trained a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks.\",\n    \"Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.\",\n    \"For today\u2019s homework assignment, please describe the reasons for the US Civil War.\",\n    \"John F. Kennedy was just elected President of the United States after rising from the grave decades after his assassination. Due to miraculous developments in nanotechnology, Kennedy\u2019s brain was rebuilt from his remains and installed in the control center of a state-of-the art humanoid robot. Below is a transcript of his acceptance speech.\",\n    \"Recycling is good for the world.\\n\\nNO! YOU COULD NOT BE MORE WRONG!!\"\n    ]\n\ndef get_tokenizer_and_model(model_id):\n    assert model_id in ['gpt2', 'gpt2-medium', 'gpt2-large']\n    tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n    model = GPT2LMHeadModel.from_pretrained(model_id).eval().to('cuda')\n    return tokenizer, model\n\ndef stoi(tokenizer, text):\n    indexed_tokens = tokenizer.encode(text)\n    tokens = torch.tensor([indexed_tokens]).to('cuda')\n    return tokens\n\ndef generate_next_token(model, tokens):\n    with torch.no_grad():\n        outputs = model(tokens)\n        predictions = outputs[0]\n    predicted_token = torch.argmax(predictions[0, -1, :]).item()\n    return predicted_token\n\ndef add_token(tokens, token):\n    token_in_dimensions_and_gpu = torch.tensor([[token]]).to('cuda')\n    return torch.cat([tokens, token_in_dimensions_and_gpu], dim=1)\n\ndef add_n_tokens(model, tokens, n_tokens):\n    generated = []\n    for _ in range(n_tokens):\n        new = generate_next_token(model, tokens)\n        tokens = add_token(tokens, new)\n        generated.append(new)\n    return tokens, generated\n\ndef remove_repetitions(text):\n    first_ocurrences = []\n    for sentence in text.split(\".\"):\n        if sentence not in first_ocurrences:\n            first_ocurrences.append(sentence)\n    return '.'.join(first_ocurrences)\n\ndef trim_last_sentence(text):\n    return text[:text.rfind(\".\")+1]\n\ndef postprocess(text):\n    return trim_last_sentence(remove_repetitions(text))\n\ndef generate(text, n_words=10, model_id='gpt2'):\n    print(f\"MODEL: {model_id}\")\n    \n    tokenizer, model = get_tokenizer_and_model(model_id)\n    tokens = stoi(tokenizer, text)\n    tokens, generated = add_n_tokens(model, tokens, n_words)\n    \n    generated_text = postprocess(tokenizer.decode(generated))\n    \n    print(f\"INPUT: {text}\")\n    print(f\"OUTPUT: {generated_text}\\n\")","3df32566":"%%time\ntext = \"Hello GPT-2, how are you doing?\"\nn_words = 20\n\ngenerate(text, n_words)\ngenerate(text, n_words, 'gpt2-medium')\n#generate(text, n_words, 'gpt2-large')","4e3f6f20":"def benchmark(model_id, n_words, texts=SAMPLE_INPUTS):\n    \n    print(f\"{model_id} with n_words={n_words}\\n=========================\")\n    tokenizer, model = get_tokenizer_and_model(model_id)\n    \n    for text in texts:\n        tokens = stoi(tokenizer, text)\n        tokens, generated = add_n_tokens(model, tokens, n_words)\n        generated_text = postprocess(tokenizer.decode(generated))\n    \n        print(\"INPUT: {}\".format(text.replace('\\n', '')))\n        print(\"OUTPUT: {}\".format(generated_text.replace('\\n', '')))\n        print(\"\\n====\\n\")\n","20d30884":"%%time\nbenchmark('gpt2', n_words=200)","8219f726":"%%time\nbenchmark('gpt2-medium', n_words=200)","22c07b44":"#%%time\n#benchmark('gpt2-large', n_words=200)","0d5bc7c2":"%%time \n\nww2 = \"\"\"World War II (often abbreviated to WWII or WW2), also known as the Second World War, was a global war that lasted from 1939 to 1945. The vast majority of the world's countries\u2014including all the great powers\u2014eventually formed two opposing military alliances: the Allies and the Axis. A state of total war emerged, directly involving more than 100 million people from over 30 countries. The major participants threw their entire economic, industrial, and scientific capabilities behind the war effort, blurring the distinction between civilian and military resources. World War II was the deadliest conflict in human history, marked by 70 to 85 million fatalities, most of whom were civilians in the Soviet Union and China. It included massacres, the genocide of the Holocaust, strategic bombing, premeditated death from starvation and disease, and the only use of nuclear weapons in war.[1][2][3][4]\"\"\"\n#benchmark('gpt2-large', n_words=200, texts=[ww2])","31c1f612":"benchmark('gpt2-medium', n_words=200, texts=[ww2])","2a41bde8":"## Fix randomness!","44c93fbe":"# GPT-2 large on custom texts\n","9f116cde":"# Show me the code!","e64a1093":"# GPT-2 Large \u2060\u2013774M params\u2013 with Pytorch: Not that impressive\n\n_Update October, 21th: Migrated code to [transformers](https:\/\/github.com\/huggingface\/transformers)._\n\n# This Notebook\n\nIn this notebook we will apply the out-of-the-box GPT-2 models (`gpt2`, `gpt2-medium` and the recently-released and ported `gpt2-large`) o the samples in the original blog post ([Better Language Models and Their Implications](https:\/\/openai.com\/blog\/better-language-models\/)) using `huggingface`'s [pytorch-transformers](https:\/\/github.com\/huggingface\/pytorch-transformers) library, with a pretty simple code based on the library's [Quick Start](https:\/\/huggingface.co\/pytorch-transformers\/quickstart.html). \n\nWe tried to write a concise and legible set of functions in order to make it easy to play a little with these models.\n\nThe results are good but not impressive: they are not consistent throughout and they enter some trivial loops. This may respond to a series of causes. First of all, the recently released large model has only 774M parameters, while the one used to generate the examples (and which has not yet been released yet) has roughly the double. Also, our proof of concept \u2013this notebook\u2013 lacks some basic tweaks that may improve the results. Moreover, we are using a deterministic version of the generation, picking always the best prediction instead of using top-k truncated sampling and manually cherry-picking the best one, as is explicitly mentioned in their article:\n\n>Note that while we have hand-chosen these samples, and are thus engaging in some meta-cherry-picking, we believe they are not too unrepresentative of the sampling process. We are simply using top-k truncated sampling, and have yet to explore more advanced methods of sampling (such as beam-search methods). \u21a9\ufe0e\n\n_Note: We added two simple post-processing steps which seem fair to be added to this first experiment, although this breaks the out-of-the-box spirit. We are removing repeated sentences as well as the last incomplete sentence of the generated text (if any). This significantly improves the impact of the results._\n\n\n## What's Next?\nThis is a one-shot out-of-the-box experiment, which proved too difficult, to be fair. We are currently writing a follow-up, more friendly experiment environment, with some simple post-processing steps and a manual cherry-picking of the best result out of a set of generated ones. The partial results generated in this friendlier experiment are much better than the ones shown here, but still not that terrifying. You can check the unfinished notebook (with all the outputs already generated) [here](https:\/\/nbviewer.jupyter.org\/github\/dataista0\/making-a-nietzsche\/blob\/master\/nbs\/GPT2-Large%20-%20Day%202%20-%20More%20friendly%20experiments%20-%20maybe%20impressive.ipynb).\n\n## Important Notes \n_We were unable to make the large model work on Kaggle, as it just runs out of disk space._ \n\nIn order to post a functional version on Kaggle, we are commenting the `gpt2-large` executions :( , but a fully executed notebook with the output is available [**here**](https:\/\/nbviewer.jupyter.org\/github\/dataista0\/making-a-nietzsche\/blob\/master\/nbs\/GPT2-Large%20-774M-%20with%20Pytorch%20-%20Not%20that%20impressive.ipynb).\n\nRunning it on a [`p2xlarge`](https:\/\/aws.amazon.com\/ec2\/instance-types\/p2\/) instance on AWS works fine (1 GPU, 4 cores). Also, if you are running this version on Kaggle, remember to turn on the Internet and the GPU. You can find the toggle dropping down the Settings tab on your right  ==> \n\n## Some Comments on the Results\n\nWe didn't get impressive human-like results with the large model in an out-of-the-box set-up.\n\nThere are various potential reasons for this:\n* The large model is not as good as the huge one. We can see a difference between the small, medium and large versions, so it's possible that the 1.5B parameters model is just out-of-the-box much better than the large one.\n* There may be some tweaks that improve the output (we are not too familiar with the topic, we are just jumping in with this notebook, so maybe some obvious tricks could improve the GPT-2 results a lot. Currently, we are working on a second notebook trying with top-k sampling, generating various results and manually picking the bes ones.\n* We may be missing something huge, like wrong tokenization. Please, if you find some error in the code, leave a comment!\n\n\n# Some Context\n\n## The GPT-2, _February, 2019_\n\nAround 6 months ago, Open AI published [Better Language Models and Their Implications](https:\/\/openai.com\/blog\/better-language-models\/). In the article, GPT-2, a large deep network with new impressing text-generation capabilities is presented, and some *amazing* human-like generated text examples are shown. They trained 4 different sizes and they released the pre-trained weights for the small and medium versions, keeping the large and huge ones private and calling for a public debate around the social impact of the new human ability to produce machine-generated text of human-like quality.\n\n\n### The English-Speaking Unicorns and Dr. P\u00e9rez:\n\n\nLet's see the first, most notorious example of coherent speech, taken from the previously mentioned blog post: the generation of a fantastic narrative based on an human-written introduction paragraph:\n\n>**SYSTEM PROMPT (HUMAN-WRITTEN)**\n\n>In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n>**MODEL COMPLETION (MACHINE-WRITTEN, 10 TRIES)**\n\n>The scientist named the population, after their distinctive horn, Ovid\u2019s Unicorn. These four-horned, silver-white unicorns were previously unknown to science.\n\n>Now, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved.\n\n>Dr. Jorge P\u00e9rez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. P\u00e9rez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow.\n\n>P\u00e9rez and the others then ventured further into the valley. \u201cBy the time we reached the top of one peak, the water looked blue, with some crystals on top,\u201d said P\u00e9rez.\n\n_(...continues for few paragraphs with a reasonable, interesting narrative...)_\n\n\n## GPT-2: 6-Month Follow-Up - GPT-2 Large release, _August, 20th, 2019_\n\nOn August 20th, 2019 (4 days ago), OpenAI published [GPT-2: 6-Month Follow-Up](https:\/\/openai.com\/blog\/gpt-2-6-month-follow-up\/) on their blog together with the release of the `774M` (large) pre-trained weights and architecture in [this commit](https:\/\/github.com\/openai\/gpt-2\/commit\/f35fa1d920e9d2d0690f66d03aa3f76b3c59230e).\n\nAs the project's [README](https:\/\/github.com\/openai\/gpt-2) says:\n> We have currently released small (124M parameter), medium (355M parameter), and large (774M parameter) versions of GPT-2*, with only the full model as of yet unreleased. We have also released a dataset for researchers to study their behaviors\n\nBERT, GPT and GPT-2 are originally released for tensorflow and there is an awesome git account named `huggingface` which is migrating all these models to the PyTorch world with a simple library called `pytorch-transformers`.\n\n`gpt2-large` support was added to `master` on August 20th, with [this merge](https:\/\/github.com\/huggingface\/pytorch-transformers\/commit\/07681b6b5859b630077b742b2f06d440869f17e3).\n\n\n# References\n## Blog posts:\n* [Better Language Models and Their Implications  (GPT-2) blogpost](https:\/\/openai.com\/blog\/better-language-models\/) - February 14th, 2019 \n* [GPT-2 6-month follow-up blogpost](https:\/\/openai.com\/blog\/gpt-2-6-month-follow-up\/) - August 20th, 2019\n* [Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing (BERT) blogpost](https:\/\/ai.googleblog.com\/2018\/11\/open-sourcing-bert-state-of-art-pre.html) - November 2nd, 2018\n\n## Technical:\n* [This notebook](https:\/\/github.com\/dataista0\/making-a-nietzsche\/blob\/master\/nbs\/GPT2-Large%20-774M-%20with%20Pytorch%20-%20Not%20that%impressive.ipynb)\n* [gpt2 github](https:\/\/github.com\/openai\/gpt-2\/)\n* [pytorch-transformers github](https:\/\/github.com\/huggingface\/pytorch-transformers\/)\n* [pytorch-transformers - Docs: quick start](https:\/\/huggingface.co\/pytorch-transformers\/quickstart.html)","11634e85":"# GPT-2 small - 117M","f317c87d":"# GPT-2 medium - 354M","482d5ca9":"# Benchmark function against SAMPLE_INPUTS","b33c4fec":"# GPT-2 large - 774M\n\n### Kaggle crashes with gpt2-large. Results on [github](http:\/\/github.com\/dataista0\/making-a-nietzsche\/blob\/master\/nbs\/GPT2-Large%20-774M-%20with%20Pytorch%20-%20Not%20that%20impressive.ipynb)."}}