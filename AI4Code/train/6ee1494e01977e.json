{"cell_type":{"7854f4b3":"code","3fe08f60":"code","a83b3b0d":"code","76110cc0":"code","e60fed94":"code","045cbb67":"code","395ab3f2":"code","f0890ea0":"code","39d973ab":"code","d12a1d0e":"code","c9ebbbb7":"code","9e046f1c":"code","454f8340":"code","50368460":"code","8cc866f7":"code","57c8eae2":"code","7193b91b":"code","355e1b7a":"code","930c206b":"code","9acc4d41":"code","402f1b62":"code","737f558a":"code","c2eea3aa":"code","42e72e4b":"code","a16fe13e":"code","cafff9e5":"code","0f99376e":"code","f48267b7":"code","823eaa76":"code","818e7329":"code","6825b275":"code","7f7f145d":"code","f5af781e":"code","9200fc42":"code","7cd23847":"code","8a7d4cf8":"code","aca4f1a7":"code","4c97324d":"code","c36398f9":"code","3dc20716":"code","e2c4a703":"code","67624915":"code","40bf2b0d":"code","3d9ac283":"code","9a84792a":"code","274cd0f2":"code","1be13d0a":"code","1415ad89":"code","0c539613":"code","a9fb2c57":"code","a151e3bc":"code","312400b5":"code","8d3618fc":"code","d9fc8bd8":"code","55430359":"code","a7a8f0d2":"code","1feeed27":"code","210d8507":"code","b7e72e2e":"code","36b08a4f":"code","59f24f3d":"code","afb02075":"code","e927868a":"code","e9f26db6":"code","f1c984a5":"code","45bc14bb":"code","e5d50e64":"code","cf0ed3b0":"code","d55116ec":"code","453dac5e":"code","930a097b":"markdown","ff15ab16":"markdown","a6a20ff9":"markdown","69254e39":"markdown","a1091f7d":"markdown","e3be289a":"markdown","3d463438":"markdown","1a17eaf9":"markdown","b66d71c4":"markdown","a69b5ba2":"markdown","17a0f817":"markdown","fb6c75be":"markdown","c1dcdfc7":"markdown","f8fa186f":"markdown","18e0c1b8":"markdown","8920bc49":"markdown","ba589821":"markdown","d4a24598":"markdown","12214169":"markdown","05749f0f":"markdown","d7dd8465":"markdown","e1e8692d":"markdown","d168620e":"markdown","f49a9a90":"markdown","77fe7908":"markdown","76fa5ce4":"markdown","c6cc31d7":"markdown","9f7e1195":"markdown","28aca31d":"markdown","dc522e10":"markdown","402b7e79":"markdown","581c4427":"markdown","103f0bb7":"markdown","393df2b2":"markdown"},"source":{"7854f4b3":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","3fe08f60":"# reading the dataset train_csv and storing it in a dataframe named \"dataset\"\ndataset = pd.read_csv('..\/input\/titanic-dataset-by-logistic-regression\/train_data.csv')","a83b3b0d":"# Looking at the shape of the data\ndataset.shape","76110cc0":"# Looking at the first five rows using \".head()\"\ndataset.head()","e60fed94":"# Looking at the last five rows \ndataset.tail()","045cbb67":"# Looking at the data types\ndataset.info()","395ab3f2":"# we will now use describe() function to check the summary of the numerical data\ndataset.describe()","f0890ea0":"#  summarizing the categorical data \ndataset.describe(include='object')","39d973ab":"sns.countplot(x='Pclass',hue='Survived',data=dataset)\n\n# The Passengers of Pclass 1 survived more than other classes","d12a1d0e":"sns.countplot(x='SibSp',hue='Survived',data=dataset)\n\n# Passengers with 0 or 1 Sibling\/Spouse had greater chances of Survival compared to others","c9ebbbb7":"sns.countplot(x='Parch',hue='Survived',data=dataset)\n\n# Passengers with 0 or 1 parent\/children had greater chances of Survival compared to others","9e046f1c":"sns.catplot(x='Survived',y='Fare',data=dataset,kind='swarm')\n\n# People who paid more Fare had likely greater chances of Survival","454f8340":"sns.countplot(x='Sex',hue='Survived',data=dataset)\n\n# It is clearly visible that the survival rate of female passengers is more than that of the male passengers.","50368460":"sns.countplot(x='Embarked',hue='Survived',data=dataset)\n\n# here we see that the persons who Embarked from S and C had more survical chances compared to the ones who boarded from Q","8cc866f7":"sns.scatterplot(x='Pclass', y='Fare', data=dataset)\n\n# Passengers in the Pclass 1 had to pay more Fare","57c8eae2":"# checking the null values present in the columns of the dataframe \"dataset\"\ndataset.isnull().sum()","7193b91b":"# creating a copy of the dataset so that the original dataframe remains intact and to avoid permanent mistake.\ndataset1=dataset.copy()","355e1b7a":"# checking the missing values in the dataset1\ndataset1.isnull().sum()","930c206b":"#total missing values in each column\ntotal_missing = dataset1.isnull().sum().sort_values(ascending=False)\n#percentage of missing values in each column\npercent_missing = ((dataset1.isnull().sum()\/dataset1.isnull().count())*100).sort_values(ascending = False)\n#Combining total percentage and total missing values\ntotal_missing_data = pd.concat([total_missing,percent_missing],axis=1,keys=['Total Missing','Percent Missing'])\ntotal_missing_data","9acc4d41":"#plotting the scatterplot to find if there exist any relation between the column Age with the missing values of other predictor variables.\nsns.pairplot(x_vars=['Pclass','Sex','SibSp','Parch','Fare','Embarked'],y_vars='Age',data=dataset1)","402f1b62":"# since it can been seen that there is a relationship between \"Sipsp\" and \"Age\" so we describe the dataset for \"age\" groupby \"Sibsp\" \ndataset1.groupby('SibSp')['Age'].describe()","737f558a":"#We see the distribution of null values in Age accordingly with the number of SibSp.\nsns.countplot(x='SibSp',data=dataset1[dataset1['Age'].isnull()])","c2eea3aa":"# since Age is not normally distributed we will replace it's missing values with median\n\n\ndataset1['Age'].median()","42e72e4b":"# we can see from above that all the value of Age whose SibSp is missing so we can replace the Age with the median of overall \n# Age data where no. of SibSp is 8.\n\n\ndataset1['Age']=np.where((dataset1['SibSp']==8) & (dataset1['Age'].isnull()),28.0,dataset1['Age'])","a16fe13e":"# visualising after replacing the null values of Age where SibSp is 8\n\n\nsns.countplot(x='SibSp',data=dataset1[dataset1['Age'].isnull()])","cafff9e5":"# from the above data where we have described Age grouped by SibSp, we have different median values of Age for different no. of\n# SibSp, thus we replace the null values accordingly with the median values.\n\ndataset1['Age']=np.where((dataset1['SibSp']==0) & (dataset1['Age'].isnull()),29.0,dataset1['Age'])\n\ndataset1['Age']=np.where((dataset1['SibSp']==1) & (dataset1['Age'].isnull()),30.0,dataset1['Age'])\n\ndataset1['Age']=np.where((dataset1['SibSp']==2) & (dataset1['Age'].isnull()),23.0,dataset1['Age'])\n\ndataset1['Age']=np.where((dataset1['SibSp']==3) & (dataset1['Age'].isnull()),10.0,dataset1['Age'])","0f99376e":"# we can see there are no more null values in the column \"Age\"\ndataset1.isnull().sum()","f48267b7":"#  We will drop the column Cabin as it has 77 percent of missing values along with the columns which are not so relevant.\n\n\ndataset1.drop(columns=['Name','Ticket','PassengerId','Cabin'],inplace=True)","823eaa76":"# we can see that there are only 2 missing values in the Embarked column\n\n\ndataset1.isnull().sum()","818e7329":"# Since Embarked is a categorical variable we can find its mode and replace the null values with the mode.\n\n\ndataset1['Embarked'].value_counts()","6825b275":"dataset1['Embarked'].fillna('S',inplace=True)","7f7f145d":"# double checking we now see that we don't have any missing values in our dataframe\n\n\ndataset1.isnull().sum()","f5af781e":"# The boxplot shows the data points which are above the Upper Limit and below the Lower Limit.\n\n\nsns.boxplot(dataset1['Age'])\nplt.xlabel('Age')\nplt.title('Boxplot for Age ')","9200fc42":"# using Scatter Plot it is quite evident that the row with Age 80 is an extreme outlier which we can drop\n\nsns.set_style(\"whitegrid\")\nsns.scatterplot(x = 'Age', y = 'Survived', data=dataset1)\nplt.title('Scatterplot between Age and Survived')\nplt.xlabel('Age')\nplt.ylabel('Survived')","7cd23847":"# sorting the values of Age in descending order and showcasing only the first 10 rows\n\n\ndataset1.sort_values(by='Age',ascending=False)[:10]","8a7d4cf8":"# dropping the index where Age = 80 \n\ndataset1.drop(dataset1[dataset1['Age']>=80].index, inplace=True)","aca4f1a7":"# plotting the boxplot from seaborn\n\nsns.boxplot(dataset1['Age'])\nplt.xlabel('Age')\nplt.title('Boxplot for Age ')","4c97324d":"# outlier visualization of Fare\n\nsns.set_style(\"whitegrid\")\nsns.boxplot('Fare',data=dataset1)\nplt.xlabel('Fare')\nplt.title('Boxplot for Fare ')","c36398f9":"# From the scatterplot it is quite evident that we have outliers in fare where fare is greater than 500\n\nsns.set_style(\"whitegrid\")\nsns.scatterplot('Fare','Survived',data=dataset1)\nplt.title('Scatterplot between Fare and Survived')\nplt.xlabel('Fare')\nplt.ylabel('Survived')","3dc20716":"# sorting the values of Fare to find out the index of the outlier value\ndataset1.sort_values(by='Fare',ascending=False)[:10]","e2c4a703":"#dropping the index with values greater than 500\n\ndataset1.drop(dataset1[dataset1['Fare']>500].index, inplace=True)","67624915":"sns.boxplot(dataset1['Fare'])","40bf2b0d":"# selecting the numerical columns\n\nnumerical_cols = dataset1.select_dtypes(include=[np.number]).columns","3d9ac283":"numerical_cols","9a84792a":"# storing the numerical columns in a new dataset named titanic_number\n\ntitanic_number = dataset1[numerical_cols]","274cd0f2":"titanic_number","1be13d0a":"# getting the categorical data  from the dataset1\n\ncharacter_cols =dataset1.select_dtypes(include='object').columns","1415ad89":"character_cols","0c539613":"titanic_dummies = pd.get_dummies(dataset1[character_cols])","a9fb2c57":"titanic_dummies","a151e3bc":"# combining Numerical and Dummy Variables\n\ntitanic_combined = pd.concat([titanic_number,titanic_dummies],axis=1)","312400b5":"#Drop the extra dummy columns to avoid correlation between the features\n\ntitanic_combined.drop(columns=['Sex_male','Embarked_C'],inplace=True)","8d3618fc":"titanic_combined.shape","d9fc8bd8":"titanic_combined.info()","55430359":"# Dividing the dataset in X (features) and Y (response) variable.\n\nX = titanic_combined.drop(columns=['Survived'])\nY = titanic_combined['Survived']","a7a8f0d2":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.3,random_state=10)","1feeed27":"X_train.shape","210d8507":"X_test.shape","b7e72e2e":"from sklearn.linear_model import LogisticRegression\n# creating linear regression object and naming it classifier\nclassifier = LogisticRegression()\n#fitting the data in the model\nclassifier.fit(X_train,Y_train)","36b08a4f":"# Making Predictions using the model\nY_pred = classifier.predict(X_test)","59f24f3d":"Y_pred","afb02075":"# For Classification techniques we use Confusion matrix to check the accuracy. \nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, Y_pred)\nprint(cm)","e927868a":"# Accuracy score of Logistic Regression Model\n\nfrom sklearn.metrics import accuracy_score,precision_score\naccuracy_LR = accuracy_score(Y_test, Y_pred)\nprint(\"Accuracy of the model: \",accuracy_LR)","e9f26db6":"# Precision of our Model\n# given by the formula:\n# Precision = TP\/(TP+FP)\n# where TP -> True Positives \n# and FP -> False Positives\n\nfrom sklearn.metrics import precision_score\n\nprecision = precision_score(Y_test,Y_pred)\n\nprint('Precision is  :',precision)","f1c984a5":"# Recall of our Model\n# given by the formula:\n# Recall= TP\/(TP+FN)\n# where TP -> True Positives \n# and FN -> False Negatives\n\nfrom sklearn.metrics import recall_score\n\nrecall = recall_score(Y_test,Y_pred)\n\nprint('Recall is  :',recall)","45bc14bb":"# F1 Score is given by the formula:\n# F1 Score = 2*Precision*Recall\/(Precision+Recall)\n\n\nfrom sklearn.metrics import f1_score\n\nf1 = f1_score(Y_test,Y_pred)\n\nprint('F1 score is :' ,f1)","e5d50e64":"from sklearn.metrics import roc_auc_score\narea_under_roc = roc_auc_score(Y_test, Y_pred)\nprint(\"AUC: \",area_under_roc)","cf0ed3b0":"from sklearn.metrics import roc_curve\n\nFPR,TPR,Thresholds = roc_curve(Y_test,classifier.predict_proba(X_test)[:,1])","d55116ec":"# For Different Thresholds we have the FPR and TPR values\n\n\ntype(pd.Series(Thresholds))\n\nfpr_series = pd.Series(FPR)\ntpr_series = pd.Series(TPR)\nthresholds_series = pd.Series(Thresholds)\n\ndf = pd.concat([fpr_series,tpr_series,thresholds_series],axis=1,keys=['FPR','TPR','THRESHOLD'])\ndf.sort_values(by='TPR',ascending=False)","453dac5e":"# Plotting the ROC Curve\n\nsns.set_style(\"darkgrid\")\nplt.plot(FPR,TPR)\nplt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\nplt.ylabel('True Positive Rate (recall)', fontsize=14)\nplt.title('Receiver operating characteristic (ROC) curve')","930a097b":"# Step 5: Outlier Treatment","ff15ab16":"### Plotting ROC Curve","a6a20ff9":"#### creating dummy variables ","69254e39":"#### We can find out the percentage of the missing values in each column. Let's see the percentage now.","a1091f7d":"#### For Numerical Columns","e3be289a":"### Precision Score","3d463438":"#### Treating the missing values of \"Embarked\"","1a17eaf9":"#### Treating the missing values of \"Cabin\"","b66d71c4":"### Summarizing the data","a69b5ba2":"# Step 1:Importing the libraries:","17a0f817":"# Step 10: Model Performance metrics","fb6c75be":"### Confusion Matrix","c1dcdfc7":"# Step 3: Treatment of the Missing Values","f8fa186f":"We have 891 Passenger's data with 12 columns to train and test the model. ","18e0c1b8":"### F1 Score","8920bc49":"# Step 8: Performing Logistic Regression\n#### We are going to perform Logistic Regression technique as the variable which we are predicting is categorical\n#### Based on our Visualisation of which type of Passengers are more likely to Survive we will build our model\n#### Let's see How our Model performs.","ba589821":"# Insights:","d4a24598":"# Step 6: Seperating the numerical and categorical data for modelling","12214169":" # PROBLEM STATEMENT\n#### The sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing numerous passengers and crew. This sensational tragedy  shocked the international community and led to better safety regulations for ships.\n#### One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely  to survive than others, such as women, children, and the upper-class.\n\n\n ## Task and Approach:\n #### In this, we are asked to complete the analysis of what sorts of people were likely to survive and also to apply the tools of machine learning to predict which passengers survived the tragedy.\n #### Here we are provided with the data of about 891 passengers with details like Name, Age, Sex, PassengerId, Ticket etc. as well as more detailed information about their PassengerClass, No. of siblings\/spouses, No. of parents\/children onboard, Fare, Cabin number and Port of Embarkation.\n #### We need to predict which Passengers were more likely to Survive. \n.\n #### Let's start by first understanding the data and cleaning the same. Then we shall move on to Modelling.","05749f0f":"### Accuracy","d7dd8465":"#### Outlier treatment of \"Age\"","e1e8692d":"# Step 7: Splitting the data in training and test data","d168620e":"### Using sklearn we split 70% of our data into training set and rest in test set.\n\n### Setting random_state will give the same training and test set everytime on running the code.","f49a9a90":"#### outlier treatment of \"Fare\"","77fe7908":"# Step 2: Visualisation","76fa5ce4":"#### Treating the missing values of \"AGE\"","c6cc31d7":"### Thus, Our Logistic Regression model is able to predict the passengers who are likely to Survive with an accuracy of approx 82% and AUC approx 81% which is a good fit.\n","9f7e1195":"### Recall Score","28aca31d":"#### For Character Columns","dc522e10":"Numpy, pandas , seaborn and matplotlib.pyplot are imported with aliases np, pd, sns and plt respectively","402b7e79":"#### Using Logistic Regression for Predicting the Survival of the Passengers we have:\n#### The Accuracy and Precision and AUC of the model are:\n#### Accuracy:0.8239700374531835\n#### Precision: 0.7722772277227723\n#### F1 Score: 0.7684729064039408\n#### Recall Score: 0.7647058823529411\n#### Area Under The Curve(AUC): 0.8126559714795009","581c4427":"# Step 9: Predictions","103f0bb7":"## An Overview:\n##### Here, I have used Logistic Regression to build the model for predicting the survival of passengers.\n##### Before Data Cleaning:\n##### Dataframe size:(891,12)\n##### The file size was 83.7+ KB\n##### After EDA:\n##### DataFrame shape: (887,9)\n##### Dataframe size : 51.1 KB","393df2b2":"### Area Under The Curve"}}