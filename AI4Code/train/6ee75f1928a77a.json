{"cell_type":{"3422c34d":"code","60764089":"code","11bebc55":"code","86503ea4":"code","f64dcd01":"code","f109e501":"code","29a6992c":"code","c363c64b":"code","748484a4":"code","74a1a87e":"code","dae39068":"markdown","940a773a":"markdown","3b9d94bd":"markdown","55cae283":"markdown","73b4a708":"markdown","41630b27":"markdown","ba385133":"markdown","8ef3ed9b":"markdown","4a7d3bec":"markdown","082f06f0":"markdown","83a06eee":"markdown","728fc8ce":"markdown","ac6690c3":"markdown","61948315":"markdown"},"source":{"3422c34d":"import pandas as pd\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n\nmain_file_path = '..\/input\/house-prices-advanced-regression-techniques\/train.csv' # Path to Iowa house sale data\ndata = pd.read_csv(main_file_path)","60764089":"##### Select single column\n# Grab column from series using name of column via dot-notation\niowa_price_data = data.SalePrice\n\n# Peep first 5 rows of price\nprint(iowa_price_data.head())\n\n##### Select multiple columns\n# declare list, specifying column name(s)\ngiven_columns = ['YrSold', 'LotArea']\n\n# subset DF with said list\ntwo_selected_columns = data[given_columns] \n\n# verify that we have the selected columns\nprint(two_selected_columns.describe())","11bebc55":"# Set prediction target\ny = data.SalePrice\n\n# Set predictors\niowa_sale_predictors = [\"LotArea\",\n                        \"YearBuilt\",\n                        \"1stFlrSF\",\n                        \"2ndFlrSF\",\n                        \"FullBath\",\n                        \"BedroomAbvGr\",\n                        \"TotRmsAbvGrd\"]\n\n# Subset dataset by predictors\nX = data[iowa_sale_predictors]\n\nprint(X.head())","86503ea4":"# from sklearn.tree import DecisionTreeRegressor # as imported in setup\n\n# Define model\niowa_model = DecisionTreeRegressor()\n\n# Fit model with predictors and prediction\niowa_model.fit(X, y)","f64dcd01":"# Use model to predict\nprediction = iowa_model.predict(X)\n\nprint(\"Predictions are for the following 5 houses:\")\nprint(X.head())\n\nprint(\"Predictions are:\")\nprint(prediction[:5])\n\nprint(\"Original values are:\")\nprint(y.head())\n","f109e501":"# from sklearn.metrics import mean_aboslute_error # as imported in setup\n\n# Calculate MAE\n_iowa_MAE = mean_absolute_error(prediction, y)\nprint(\"The MAE of inital prediction is %d\"%(_iowa_MAE))","29a6992c":"# from sklearn.metrics import mean_absolute_error # as imported in setup\n\n# data is split into training and validation sets, for predictors & target\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=0) #random_state=0 so the split will be the same every time\n\n# Define Model\niowa_model = DecisionTreeRegressor()\n# Fit Model on training data\niowa_model.fit(train_X, train_y)\n\n# Predict on validation data\nval_predicts = iowa_model.predict(val_X)\niowaV2_MAE = mean_absolute_error(val_y, val_predicts)\n\nprint(\"The MAE of model on training split: \"+ str(iowaV2_MAE))","c363c64b":"# from sklearn.metrics import mean_absolute_error\n# from sklearn.tree import DecisionTreeRegressor\n\n# function to build & train model, returns MAE \ndef get_mae(max_leaf_nodes, predictors_train, predictors_val, targ_train, targ_val):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(predictors_train, targ_train)\n    preds_val = model.predict(predictors_val)\n    mae = mean_absolute_error(targ_val, preds_val)\n    return(mae)\n\n# Loop over range of values to find sweet spot\nfor max_leaf_nodes in [5, 50, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d \\t\\t Mean Absolute Error: %d\" %(max_leaf_nodes, my_mae))","748484a4":"# from sklearn.ensemble import RandomForestRegressor # as imported in setup\n# from sklearn.metrics import mean_absolute_error\n\n# Define model\niowa_forest_model = RandomForestRegressor()\n# Fit model\niowa_forest_model.fit(train_X, train_y)\n# Predict\niowa_predicts = iowa_forest_model.predict(val_X)\n\n#calculate MAE from new model\nifm_mae = mean_absolute_error(val_y, iowa_predicts)\nold_mae = get_mae(80, train_X, val_X, train_y, val_y)\nprint(\"Random Forest Regressor MAE is %d\" % (ifm_mae))\nprint(\"Best run of Decision Tree Regressor MAE is %d\" % (old_mae))","74a1a87e":"# Get test data\npath = '..\/input\/house-prices-advanced-regression-techniques\/%s.csv'\n\ntrain = pd.read_csv(path % ('train'))\ntest = pd.read_csv(path % ('test'))\n# Handle test data in the same manner as training data, select same columns\npredictor_cols = [\"LotArea\",\n                    \"YearBuilt\",\n                    \"1stFlrSF\",\n                    \"2ndFlrSF\",\n                    \"FullBath\",\n                    \"BedroomAbvGr\",\n                    \"TotRmsAbvGrd\",\n                    \"OverallQual\"]\ntrain_y = train.SalePrice\ntrain_X = train[predictor_cols]\ntest_X = test[predictor_cols]\n\nrfr_model = RandomForestRegressor()\nrfr_model.fit(train_X, train_y)\n# Predict\npredicted_prices = rfr_model.predict(test_X)\n\n# Peep for sanity \nprint(predicted_prices)\n\n# Prepare submission file\nrfr_submission = pd.DataFrame({'ID': test.Id, 'SalePrice': predicted_prices})\nrfr_submission.to_csv('submission.csv', index=False)","dae39068":"# Evaluating the Accuracy of Your Model.\nA first look at using Mean Absolute Error (MAE) as a metric for summarizing model quality.","940a773a":"# Implementing a Complex Model\n To avoid the pitfalls of using a single decision tree, we can use the power of many trees - a forest - using the **random forest** modeling technique, to produce a more accurate prediction.","3b9d94bd":"# Selecting Columns.\nDemonstration of subsetting via dot-notation and index.","55cae283":"# Hello MLWorld, an Introduction to Machine Learning.\n**A workspace for the [Machine Learning course](https:\/\/www.kaggle.com\/learn\/machine-learning).**","73b4a708":"# Using the Model to Predict.\nThe argument passed to the 'predict' method below informs what the model is predicting for.","41630b27":"# Submission.","ba385133":"\n**If you have any questions or hit any problems, come to the [Learn Discussion](https:\/\/www.kaggle.com\/learn-forum) for help. **\n\n**Return to [ML Course Index](https:\/\/www.kaggle.com\/learn\/machine-learning)**","8ef3ed9b":"# Setting your Prediction Target and Predictors.\nChoose which column you would like to predict. \nBy convention, the _prediction target_ is called **y** and _predictors_ **X**.","4a7d3bec":"# Gaining information from your data.\nCheatsheet of **functions, methods and attributes from this notebook** with their types\/return types to extract information and verify data (sanity check) among other things.","082f06f0":"# Building Your Model.\nImporting function from **tree** module of **scikit-learn** to _Define_ and  _Fit_ the model.","83a06eee":"# Overfitting and Underfitting; _The Sweet Spot_\nOverfitting a model occurs when the noise of the data is captured, and the model  fits the dataset _too well_, therefore does not provide the most accurate predictions on unseen data - low bias, high variance.  \nUnderfitting causes the underlying trend in the data to be missed, usually caused by an excessively simple model - high bias, low variance.  \n![http:\/\/i.imgur.com\/2q85n9s.png](http:\/\/i.imgur.com\/2q85n9s.png)\nValidation and Cross-Validation can be used to pit models against one another to find the _sweet spot_, represented in the figure as the low point of the red line.","728fc8ce":"# pandas (pandas)\n#### Reads a CSV file, creates DataFrame (DF) with data from read.\nread_csv = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv') \nprint (read_csv.head())\n\n#### get description of DF (rt: DataFrame)\ndescribe = data.describe() \nprint(describe)\n\n#### get all columns (rt: list)\ncolumns = data.columns\nprint(columns)\n\n#### get first 5 rows\nhead = data.head()\nprint(head)\n\n#### get last 5 rows\ntail = data.tail()\nprint(tail)\n\n# scikit-learn (sklearn)\n## tree (sklearn.tree)\n#### Creates model implementing decision tree regression\narbitrary_model = DecisionTreeRegressor()\n\n## ensemble (sklearn.ensemble)\narbitrary_model2 = RandomForestRegressor()\n\n#### Train model (uses libsvm under the hood)\narbitrary_model.fit(X, y) # X = predictor, y = prediction target\n\n#### Use model to predict Y for values passed by argument\narb_prediction = arbitrary_model.predict(X.head())\n    \n## metrics (sklearn.metrics)\nmean_absolute_error(y, arb_prediction)\n\n#### model_selection (sklearn.model_selection)\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=0)","ac6690c3":"# Okay, but what about the feasibility of the Data?\nAt this point, we are calculating MAE as an _in-sample_ score - Building the model and calculating the MAE on the same dataset -  **very bad**.   \nThis a major issue, as the model has been trained to find patterns in the dataset, so all of the model's predictions derived from those patterns will _appear_ accurate against the training data.\n\n# Try Again; Split your Data.\nA way to smash the pattern bias to split the data into _training_ and  _validation_ datasets, testing the model's prediction accuracy against data it hasn't been exposed to.","61948315":"# Setup.\nPerforming necessary imports for use in notebook."}}