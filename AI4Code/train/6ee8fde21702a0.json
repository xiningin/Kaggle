{"cell_type":{"cde610bd":"code","97028fbd":"code","0d94ad35":"code","9685e470":"code","edcedab2":"code","d884b103":"code","97dd2960":"code","7c071e11":"code","31eb4237":"code","9c9d7eb4":"code","cd15927d":"code","5387a354":"code","839a17be":"code","a320d9c4":"code","35357edd":"code","20dfdbc6":"markdown","b208b76f":"markdown","d43238ee":"markdown","c60868da":"markdown","68c1b22e":"markdown","ce9a184f":"markdown","b641943d":"markdown","8118b635":"markdown","f70764b4":"markdown","de5379cc":"markdown","a5e0d2c2":"markdown","982117bd":"markdown","d868cffb":"markdown","0fb933e3":"markdown","e741a73b":"markdown","215c8f0d":"markdown","a2f45f81":"markdown","06ac36b4":"markdown","2c8034a1":"markdown","23dbed0d":"markdown","fd102cf4":"markdown","5500a1d8":"markdown","63a8cedf":"markdown","f08eda07":"markdown","8208f50e":"markdown","89de9c4f":"markdown","6a8f30e5":"markdown","6c182171":"markdown","a48e9109":"markdown","fbc1b49b":"markdown","35ab0ff8":"markdown","9c0ee29e":"markdown","6f01a4eb":"markdown","393614f6":"markdown","c66d10bd":"markdown","28887195":"markdown","20868ce6":"markdown","a6bef2d9":"markdown","a9bd49bc":"markdown","30293bd6":"markdown","78be062b":"markdown","49646202":"markdown","0b6b693e":"markdown","516dddd6":"markdown","97841cfb":"markdown","b5315f85":"markdown","c3a74f80":"markdown"},"source":{"cde610bd":"#Kernel NB reference: https:\/\/www.kaggle.com\/jiazhuang\/demonstrate-naive-bayes\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#fit arbitary pdf by scipy.stats.kde.gaussian_kde\nfrom scipy.stats.kde import gaussian_kde\n\nfrom sklearn.metrics import roc_auc_score as AUC\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","97028fbd":"train = pd.read_csv('..\/input\/train.csv', index_col=0)\ntest = pd.read_csv('..\/input\/test.csv', index_col=0)\ntarget = train.target.values\ntrain.drop('target', axis=1, inplace=True)\ntrain.shape, target.shape, test.shape","0d94ad35":"pos_idx=(target==1)\nneg_idx=(target==0)\nprior_pos=pos_idx.sum()\/len(target)\nprior_neg=1-prior_pos\n\npos_kdes,neg_kdes=[],[]#kde\u51fd\u6570\u7684\u5217\u8868\nfor col in train.columns:\n    pos_kde=gaussian_kde(train.loc[pos_idx,col])\n    neg_kde=gaussian_kde(train.loc[neg_idx,col])\n    pos_kdes.append(pos_kde)\n    neg_kdes.append(neg_kde)\n\ndef cal_prob_KDE_col_i(df,i,num_of_bins=100):\n    bins=pd.cut(df.iloc[:,i],bins=num_of_bins)\n    uniq=bins.unique()\n    uniq_mid=uniq.map(lambda x:(x.left+x.right)\/2)\n    #\u628a\u6bcf\u4e00\u683cuniq_mid\u6620\u5c04\u5230kde\u503c\n    mapping=pd.DataFrame({\n        'pos':pos_kdes[i](uniq_mid),\n        'neg':neg_kdes[i](uniq_mid)\n    },index=uniq)\n    return bins.map(mapping['pos'])\/bins.map(mapping['neg'])\n\nls=[[prior_pos\/prior_neg]*len(train)]\nfor i in range(200):\n    ls.append(cal_prob_KDE_col_i(train,i))\ntrain_KernelNB=pd.DataFrame(np.array(ls).T,columns=['prior']+['var_'+str(i) for i in range(200)])\n","9685e470":"pred=train_KernelNB.apply(lambda x:np.prod(x),axis=1)\nAUC(target,pred)","edcedab2":"log_train_KernelNB=train_KernelNB.apply(np.log,axis=1)#the log transform  is input for our nerual net.","d884b103":"# it is same as temp=train_KernelNB.apply(lambda x:np.prod(x),axis=1)\ntemp=log_train_KernelNB.apply(lambda x:np.exp(np.sum(x)),axis=1)\nAUC(target,temp)","97dd2960":"#NB:score=exp( log(p(y=1)\/p(y=0))+\u2211log(p(xi|y=1)\/p(xi|y=0)) )\n#weighted NB:score=exp( w0*log(p(y=1)\/p(y=0))+ wi*\u2211log(p(xi|y=1)\/p(xi|y=0)) ) \nimport tensorflow as tf\n\nn=201 #201=1+200: 1 for prior prob's logit, 200 for likelyhood's logit \ny=tf.placeholder(tf.float32,[None,1])\nx=tf.placeholder(tf.float32,[None,n])\nw=tf.Variable(tf.ones([n]))#here, we initlize weighted NB making it start as a normal NB.\nw=tf.nn.relu(w)#ReLU applied on w the make sure weights are positive or sparse\ntf.multiply(w,x).shape\n\nlinear_term=tf.reduce_sum(tf.multiply(w,x),axis=1,keepdims=True)#(None,1)\nlinear_term=tf.math.exp(linear_term)#do the exp transform to reverse log\n\n#define lambda coef for L1-norm, a key parameter to tune.\nlambda_w=tf.constant(2*1e-5,name='lambda_w')\nl1_norm=tf.multiply(lambda_w,tf.reduce_sum(tf.abs(w),keepdims=True))\n\nerror = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=linear_term, labels=y))\nloss = error+l1_norm\n","7c071e11":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(log_train_KernelNB.values, target, test_size=0.2, random_state=42)\nprint(X_test[:5,:])\ndef batcher(X_, y_=None, batch_size=-1):\n    n_samples = X_.shape[0]\n    if batch_size == -1:\n        batch_size = n_samples\n    if batch_size < 1:\n       raise ValueError('Parameter batch_size={} is unsupported'.format(batch_size))\n    for i in range(0, n_samples, batch_size):\n        upper_bound = min(i + batch_size, n_samples)\n        ret_x = X_[i:upper_bound]\n        ret_y = None\n        if y_ is not None:\n            ret_y = y_[i:i + batch_size]\n            yield (ret_x, ret_y)\n\noptimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\nN_EPOCHS=100\nbatch_size=5000\ninit=tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n    for epoch in range(N_EPOCHS):\n        if epoch==0:\n            for bX, bY in batcher(X_test, y_test):#all sample\n                prediction_list=sess.run(linear_term, feed_dict={x: bX.reshape(-1, n)})\n                print('Init w:',sess.run(w, feed_dict={x: bX.reshape(-1, n)}))\n                print('Init pred:',prediction_list)\n                print('Init Test AUC:',AUC(y_test,prediction_list))\n               \n        perm = np.random.permutation(X_train.shape[0])\n        # iterate over batches\n        for bX, bY in batcher(X_train[perm], y_train[perm], batch_size):\n            sess.run(optimizer, feed_dict={x: bX.reshape(-1, n), y: bY.reshape(-1, 1)})\n            \n        if (epoch+1)%20==0:\n            print(\"Epoch:\",epoch)\n            for bX, bY in batcher(X_test, y_test):#all sample\n                print('Test CrossEntropy (logloss):',sess.run(error, feed_dict={x: bX.reshape(-1, n), y: bY.reshape(-1, 1)}))\n                prediction_list=sess.run(linear_term, feed_dict={x: bX.reshape(-1, n)})\n                weights=sess.run(w, feed_dict={x: bX.reshape(-1, n)})\n                print('w:',weights)\n                print('Test pred:',prediction_list)\n                print('Test AUC:',AUC(y_test,prediction_list))\n            print('=======')","31eb4237":"sparse_weights=np.where(np.abs(weights)<1e-2,0,weights)\nprint ('Number of eliminated feature',(sparse_weights<=0).sum())","9c9d7eb4":"ls=[[prior_pos\/prior_neg]*len(test)]\nfor i in range(200):\n    ls.append(cal_prob_KDE_col_i(test,i))\ntest_KernelNB=pd.DataFrame(np.array(ls).T,columns=['prior']+['var_'+str(i) for i in range(200)])","cd15927d":"pred_test=test_KernelNB.loc[:,sparse_weights>0].apply(lambda x:np.prod(x),axis=1)\npd.DataFrame({\n    'ID_code': test.index,\n    'target': pred_test\n}).to_csv('sub01_KernelNB_L1_2E-5_VAL_0.904.csv', index=False)","5387a354":"sparse_weights=pd.Series(sparse_weights,index=['prior']+['var_'+str(i) for i in range(200)])\nsns.distplot(sparse_weights,bins=50,kde=False)","839a17be":"print (sparse_weights.sort_values(ascending=False))","a320d9c4":"sparse_weights.plot()","35357edd":"sparse_weights[['var_'+str(i) for i in [108,184,9,80,76,13,166,94,170,154,133,169,174,123,6] ]]","20dfdbc6":"<img src=\"https:\/\/latex.codecogs.com\/gif.latex?logit\\_prob=exp(ReLU(w_{0})\\cdot&space;log\\frac{p(y=1)}{p(y=0)}&plus;\\sum_{i=1}^{200}ReLU(w_{i})\\cdot&space;log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)})\" title=\"logit\\_prob=exp(ReLU(w_{0})\\cdot log\\frac{p(y=1)}{p(y=0)}+\\sum_{i=1}^{200}ReLU(w_{i})\\cdot log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)})\" \/>","b208b76f":"Futher more, weighted on every column a constant might not be as good as** weighting every columns differently by the attribute of this column**.  That model is called ** Attribute weighted Naive Bayesian**, and there have been many research done in that field for recent years. If you are interested you could check [this paper](https:\/\/ieeexplore.ieee.org\/document\/5593445\/) as start.","d43238ee":"Just curious, let's check if the \"unimportant\" feature have any pattern\/order:  ","c60868da":"## Further  Discussion  \nIs weighted NB model perfect? I don't think so, **althought it is good that our model eliminated useless features,but it still need to be digging why weighted features are not gaining AUC improvment. In theory weighted naive bayes should learn more than naive bayes. Maybe it is an overfitting problem or data noise problem.    **   ","68c1b22e":"**Above are explanation & code for Kernel Naive Bayes. **  \n","ce9a184f":"# Weighted Kernel Naive Bayes with Lasso Feature Elimination by TF - Santander  \n> *More things should not be used than are necessary. - Occam's razor*\n\nIs **EVERY** variable IMPORTANT for prediction? ** The answer is not even close to yes by our finding.**\n\nIn this work, we will explore the santander using two upgraded naive bayes inference -- *Kernel NB and Weighted Kernel NB.*  \n\nFirstly, we are going to introduce an updated version of Gaussian Naive Bayes method called **Kernel Naive Bayes** which release the assumption of every features follow normal distribution.   \n\nSecondly, we are going to use back-prop & gradient decent to learn an updated model called **Weighted Kernel Naive Bayes** with lasso feature elimination. This is implemented by log transform the naive bayes formular from product terms into additive terms, and use TensorFlow to learn the loss function with L1-norm and ReLU constriants on weights.   \n\nOur final experiment show that we can achieve **same level of AUC** using **only 75% or even 50% of features**,meaning that nearly half of the features are not informative for prediction.\n","b641943d":"## So the main formula are below:","8118b635":"-------------------------","f70764b4":"The implement is as follows: We choose **TensorFlow** to do the gradient decent&back-prop to learn the weights wi. Also, in order to elimintate the useless features, we use **Lasso shrikage** method -- also known as L1 normalization. In order to make sure the all weights are greater or equal to zero, **a Rectified Linear Unit(ReLU) are applied to the weights** before its dot product with log of logit.","de5379cc":"There are **two** basic way of calculating this.   \nFirst way to do is assume that ith feature (**xi**) follows a gaussian distribution a.k.a. normal distribution, and calculate <img src=\"https:\/\/latex.codecogs.com\/gif.latex?{p(x_{i}|y=1)}\" title=\"{p(x_{i}|y=1)}\" \/> by the probability density function (PDF) of normal distribution.  \n\nHowever,from other kernel we can see clearly that NOT all of these features follow gaussian distribution, for e.g.https:\/\/www.kaggle.com\/cdeotte\/modified-naive-bayes-santander-0-899\n\nSo a Gaussian assumution might **not be the best choice for estimating p(xi|y)**.  \nFor the second way we introduce** Kernel Density Estimation** to calculate the pdf of an arbitary distribution of features.","a5e0d2c2":"No pattern for the elimintaed features.. So the columns **must have been shuffle**.  \n\nMoreover, we can **compare** some important features in LightGBM (https:\/\/www.kaggle.com\/jiweiliu\/lgb-2-leaves-augment) and see if they have high score in Lasso Weighted Naive Bayes:","982117bd":"<img src=\"https:\/\/latex.codecogs.com\/gif.latex?L1\\_norm=\\lambda&space;\\cdot&space;\\sum_{i=0}^{200}\\left&space;|&space;w_{i}&space;\\right&space;|\" title=\"L1\\_norm=\\lambda \\cdot \\sum_{i=0}^{200}\\left | w_{i} \\right |\" \/>","d868cffb":"**We choose cross_entropy as target function, and add l1 norm to prevent overfitting as well as doing lasso shrinkage.**","0fb933e3":"We are pretty **close to the final prediction**. If we now apply Naive Bayes, we can get the final prediction by multiplying these 201 terms as following:","e741a73b":"As we saw above, prior is still the most importance term because it have largest weights. Over 25% of features are eliminated, we can check their logit's distribution in this kernel(https:\/\/www.kaggle.com\/cdeotte\/modified-naive-bayes-santander-0-899).   \n  \nThese \"unimportant\" features in our model, such as **var_153, really doesn't have any instructive information for our prediction**,its logit vary from maxium of 0.115 to the minumn of 0.975.It is a very small shake.    \n**However, the \"good feature\" in our model -- such as var_105, its logit vary from max of 0.19 to min of 0.08. That's a lot of info lying here.**\n","215c8f0d":"**And,there are more interesting things to try using naive bayes model, such as using LGB to replace kernel density estimation to estimate p(xi|y) in this kernel: https:\/\/www.kaggle.com\/b5strbal\/lightgbm-naive-bayes-santander-0-900 . **","a2f45f81":"As we can see, after training, 53 of 201 features are eliminated. That is a total of 25% of the original data!   \n**And If we increase the lambda\\_w of LASSO from 2e-5 to 3e-5~4e-5, we can best eliminate 50% of the features without lossing too many socres!!!!**","06ac36b4":" as you see, the **test AUC are really hard to decrease** . However it doesn't matter, because we  want our final model to remain to be simple naive bayes with weights fix to 1.0, and the learned coef is not the key. **We just want to eliminate those useless features** using lasso shrinkage. So we mask those features that are sparse or \"almost sparse(<0.01)\". And use the mask to reduce the size of columns.","2c8034a1":"Now,Let's see the weights distribution and rank the most important features by their feature weights.","23dbed0d":"**You might wonder why L1-norm produce sparsity? ** you cuold check [this kaggle discussion](https:\/\/www.kaggle.com\/residentmario\/l1-norms-versus-l2-norms) for further machine learning knowledge.","fd102cf4":"## more\n\n","5500a1d8":"Just a notice, we can get the same AUC score of 0.908 by **doing sum** on the logified data before doing exp on it. We should have a clear understanding of what we have done: first do a log transformation, and **a sum in log transform means product in original form**.   \n**And the exp of sum actually reverse the transform, making it equal to product of origanl data**. That is to say:","63a8cedf":"The weights are **basically consistent** with LGB's importance score. Most LGB's important feature are also important features for Lasso Weighted Naive Bayes.   \n\nBut they are not exactly the same(e.g. var_9), so **it might still be helpful to combine these two algorithms together** because they share some diversity.","f08eda07":"Recall naive bayes follows the **feature independence law**, and use logit probability as prediction score. Conclusively, it should have the following fomular:  \n<img src=\"https:\/\/latex.codecogs.com\/gif.latex?logit\\_prob=\\frac{p(y=1|X)}{p(y=0|X)}=\\frac{p(y=1)}{p(y=0)}*\\frac{p(X|y=1)}{p(X|y=0)}=\\frac{p(y=1)}{p(y=0)}*\\prod_{i=1}^{200}\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}\" title=\"logit_p=\\frac{p(y=1|X)}{p(y=0|X)}=\\frac{p(y=1)}{p(y=0)}*\\frac{p(X|y=1)}{p(X|y=0)}=\\frac{p(y=1)}{p(y=0)}*\\prod_{i=1}^{200}\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}\" \/>\n\nwhere logit_prob is the final prediction. \n","8208f50e":"## Second, we try to **weighted every term above** -- that is why it is called weighted naive bayes.\n<img src=\"https:\/\/latex.codecogs.com\/gif.latex?log\\_logit\\_prob=w_{0}\\cdot&space;log\\frac{p(y=1)}{p(y=0)}&plus;\\sum_{i=1}^{200}w_{i}\\cdot&space;log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}\" title=\"w_{0}\\cdot log\\frac{p(y=1)}{p(y=0)}+\\sum_{i=1}^{200}w_{i}\\cdot log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}\" \/>","89de9c4f":"------------------------\n# Now introduce Weighted Kernel Naive Bayes with Lasso Feature Elimination by gradient decent.","6a8f30e5":"<img src=\"https:\/\/latex.codecogs.com\/gif.latex?w_{0}\\cdot&space;log\\frac{p(y=1)}{p(y=0)}&plus;\\sum_{i=1}^{200}w_{i}\\cdot&space;log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}&space;=log[\\frac{p(y=1)}{p(y=0)}]^{w0}&plus;\\sum_{i=1}^{200}log[\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}]^{w_{i}}\" title=\"w_{0}\\cdot log\\frac{p(y=1)}{p(y=0)}+\\sum_{i=1}^{200}w_{i}\\cdot log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)} =log[\\frac{p(y=1)}{p(y=0)}]^{w0}+\\sum_{i=1}^{200}log[\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}]^{w_{i}}\" \/>","6c182171":"<img src=\"https:\/\/latex.codecogs.com\/gif.latex?loss=cross\\_entropy(y\\_true,logit\\_prob)&space;&plus;L1\\_norm\" title=\"loss=cross\\_entropy(y\\_true,logit\\_prob) +L1\\_norm\" \/>","a48e9109":"As we see, more than 50 features are eliminated when lambda\\_w=2e-5. That's why it called feature elimination using LASSO.","fbc1b49b":"And the constrant is that **every weights wi (201>=i>=0) should be greater or equal to 0.**   \n","35ab0ff8":"Here we have already get the Kernel Naive Bayes transformation of original data. It have 201 columns. Be aware of what we have done, these 201 features are not orignal features, but 201 terms in naive bayes formular:\n<img src=\"https:\/\/latex.codecogs.com\/gif.latex?logit\\_prob=\\frac{p(y=1)}{p(y=0)}*\\prod_{i=1}^{200}\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}\" title=\"\\frac{p(y=1)}{p(y=0)}*\\prod_{i=1}^{200}\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}\" \/>\nwhere the first term is logit of prior probability, and last 200 terms are logit of likelyhood.","9c0ee29e":"Actually, the \"naive bayes\" is just a special case of \"weighted naive bayes\" **when every weights wi are initlized to 1**. --That insire me to initlize every weights wi to 1 (see the code tf.ones), and let the model update the weights. And experiments show that **initilizations are so vital** that it stablize the model training -- the model would become very hard to learn if we init the weights in random way.","6f01a4eb":"## For accelation, instead of calculating **p(xi|y)**, we now calculate **p(bin\\_of\\_xi|y)** for every bins. To achieve that, we cut every continues value in xi into bins, and map continues xi to its bins' probability:** p(bin\\_of\\_xi|y)** . This is bining Kernel Naive Bayes. ","393614f6":"The problem lies on how to calculate the 200 other terms, that is to say, how to calculate the following:\n<img src=\"https:\/\/latex.codecogs.com\/gif.latex?{p(x_{i}|y=1)}\" title=\"{p(x_{i}|y=1)}\" \/> as well as <img src=\"https:\/\/latex.codecogs.com\/gif.latex?{p(x_{i}|y=0)}\" title=\"{p(x_{i}|y=0)}\" \/>","c66d10bd":"## Finally, after we get the sum of weighted log logit probability, we now exp the sum and revocer the raw score!<img src=\"https:\/\/latex.codecogs.com\/gif.latex?logit\\_prob=exp(log\\_logit\\_prob)=exp(w_{0}\\cdot&space;log\\frac{p(y=1)}{p(y=0)}&plus;\\sum_{i=1}^{200}w_{0}\\cdot&space;log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)})\" title=\"logit\\_prob=exp(w_{0}\\cdot log\\frac{p(y=1)}{p(y=0)}+\\sum_{i=1}^{200}w_{0}\\cdot log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)})\" \/>","28887195":"*I would be happy if my post help you understand NB or the data better.*   \nGive me a **thumb up or follow** if this kernel is helpful to you. Appreciate that!","20868ce6":"What's more we use gaussian kernel KDE using scipy.stats.kde.gaussian_kde, and binize the features to reduce the complexity from O(#data) to O(#bins).  ","a6bef2d9":"You can see the **weights w are becoming sparser and sparser** as train epoch goes on, from all ones to many zeros. So we are making progress in eliminting uesless features.","a9bd49bc":"We can rethink our weighted naive bayes model and interprete it in this way:\n","30293bd6":"# Conclusion:\nIn this Kernel, we first explain Kernel Naive Bayes using Kernel Density Estimation(KDE), and illustrate the merits of Kernel NB over Gaussian NB.  \nSecondly, we use log transformation that transform Naive Bayes's product into a sum of 201 logit terms, then **we strengthen the \"simple sum of logit term\" by giving every term a weight wi(w>=0),making it a standard weighted linear regression problem**. So we use gradient decent to learn these weights. For the purpose of eliminating useless feautures, **Lasso(L1-norm)** are used in the linear model. Also, **ReLU** are applied on the weights to make sure that weights either positive(>0) or sparse(=0).   \n\nOur Lasso Weighted Naive Bayes model use back-prop&gradient decent to shrikage the weights. And the experiment shows that our model is **expert** in eliminating these useless features by itself.  By tuning the lambda of L1-norm from 2e-5 to 4e-5, at last we are able to gain almost **same level of AUC using only 50% of the features, which actually indicate that around half of the features are not informative (I prefer saying \"almost useless\") for our prediction.  ** Further analysis show that the feature selection process could be possibly applied to  LGB model.\n\n-------\n","78be062b":"**The AUC is over 0.908.** That is a very impressive score in training set, althought may be a little overfit on the training data.   \nAccording to the original kernel https:\/\/www.kaggle.com\/jiazhuang\/demonstrate-naive-bayes\/notebook, it reported **AUC 0.894** in public leaderboard.","49646202":"Let's start, Frist we log the train_KernelNB as input data.","0b6b693e":"## First, in order to simplfy our naive bayes problem, we log the \"logit\\_prob\" term in order to transform series of product into series of sum.\n<img src=\"https:\/\/latex.codecogs.com\/gif.latex?log[\\frac{p(y=1)}{p(y=0)}*\\prod_{i=1}^{200}\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}]=log\\frac{p(y=1)}{p(y=0)}&plus;\\sum_{i=1}^{200}log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}\" title=\"log[\\frac{p(y=1)}{p(y=0)}*\\prod_{i=1}^{200}\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}]=w_{0}\\cdot log\\frac{p(y=1)}{p(y=0)}+\\sum_{i=1}^{200}w_{i}\\cdot log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}\" \/>","516dddd6":"  \nAs we know already, in the first term <img src=\"https:\/\/latex.codecogs.com\/gif.latex?{p(y=1)}\" title=\"{p(y=1)}\" \/> is prior probability of positive class. And P(y=0) can also be calculated easily by 1-P(y=1).  ","97841cfb":"That provide us **a different view about how the weights works** -- if a sparse weight wi=0, then wi=0 make the logit^wi=logit^0 =1, and log(1)=0, making this logit terms equal to zero. So the our final result(as a linear combination) would not be influence by this features any more since adding zero term means adding nothing.","b5315f85":"## **Let's implement what we have introduce above, and start eliminting the features!**","c3a74f80":"Let's submit the result with only 148 features and see what score we got.  \nThe **AUC on public LB is 0.893** when using only 148 variables, a** very slight drop** from 0.894 using 201 features."}}