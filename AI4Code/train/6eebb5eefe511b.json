{"cell_type":{"79c55229":"code","162d8b78":"code","6af0c019":"code","fad98af3":"code","a4b94e64":"code","8af0de03":"code","a021d1a4":"code","3fe4eac2":"code","7f426968":"code","bdf8feca":"code","b2124e47":"code","e837064a":"code","b1059dba":"code","4b3f8cea":"code","91078053":"code","2ff82e0b":"code","3b22ac16":"code","4beece61":"code","c3635a3b":"code","50a50cdd":"code","8929c71f":"code","8ad722ff":"code","70e4944a":"code","5ae2ec86":"code","ce1d36e4":"markdown","cb1c0cd9":"markdown","f0391eea":"markdown","8356161c":"markdown","fe65c114":"markdown","da5f6866":"markdown","e062a789":"markdown","c9935388":"markdown","3e68414e":"markdown","322e9a83":"markdown","6e376b6a":"markdown","eab422b1":"markdown","41292abb":"markdown","8e17e0a3":"markdown","5507c672":"markdown","4a43f407":"markdown","216e2067":"markdown","03155051":"markdown"},"source":{"79c55229":"!pip install catboost==0.15.2","162d8b78":"import numpy as np\nimport pandas as pd\nimport os\nimport gc\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import GroupKFold\nimport xgboost as xgb\nimport catboost as cb","6af0c019":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","fad98af3":"# COLUMNS WITH STRINGS\nstr_type = ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain','M1', 'M2', 'M3', 'M4','M5',\n            'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29', 'id_30', \n            'id_31', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']\n\n# FIRST 53 COLUMNS\ncols = ['TransactionID', 'TransactionDT', 'TransactionAmt',\n       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n       'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain',\n       'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11',\n       'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8',\n       'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4',\n       'M5', 'M6', 'M7', 'M8', 'M9']\n\n# V COLUMNS TO LOAD DECIDED BY CORRELATION EDA\n# https:\/\/www.kaggle.com\/cdeotte\/eda-for-columns-v-and-id\nv =  [1, 3, 4, 6, 8, 11]\nv += [13, 14, 17, 20, 23, 26, 27, 30]\nv += [36, 37, 40, 41, 44, 47, 48]\nv += [54, 56, 59, 62, 65, 67, 68, 70]\nv += [76, 78, 80, 82, 86, 88, 89, 91]\n\n#v += [96, 98, 99, 104] #relates to groups, no NAN \nv += [107, 108, 111, 115, 117, 120, 121, 123] # maybe group, no NAN\nv += [124, 127, 129, 130, 136] # relates to groups, no NAN\n\n# LOTS OF NAN BELOW\nv += [138, 139, 142, 147, 156, 162] #b1\nv += [165, 160, 166] #b1\nv += [178, 176, 173, 182] #b2\nv += [187, 203, 205, 207, 215] #b2\nv += [169, 171, 175, 180, 185, 188, 198, 210, 209] #b2\nv += [218, 223, 224, 226, 228, 229, 235] #b3\nv += [240, 258, 257, 253, 252, 260, 261] #b3\nv += [264, 266, 267, 274, 277] #b3\nv += [220, 221, 234, 238, 250, 271] #b3\n\nv += [294, 284, 285, 286, 291, 297] # relates to grous, no NAN\nv += [303, 305, 307, 309, 310, 320] # relates to groups, no NAN\nv += [281, 283, 289, 296, 301, 314] # relates to groups, no NAN\n#v += [332, 325, 335, 338] # b4 lots NAN\n\ncols += ['V'+str(x) for x in v]\ndtypes = {}\nfor c in cols+['id_0'+str(x) for x in range(1,10)]+['id_'+str(x) for x in range(10,34)]: \n    dtypes[c] = 'float32'\nfor c in str_type: dtypes[c] = 'object'","a4b94e64":"%%time\n\nfolder_path = '..\/input\/ieee-fraud-detection\/'\nprint('Loading data...')\n\ntrain_identity = pd.read_csv(f'{folder_path}train_identity.csv', index_col='TransactionID',  dtype=dtypes)\nprint('\\tSuccessfully loaded train_identity!')\n\ntrain_transaction = pd.read_csv(f'{folder_path}train_transaction.csv', index_col='TransactionID', dtype=dtypes, usecols=cols+['isFraud'])\nprint('\\tSuccessfully loaded train_transaction!')\n\ntest_identity = pd.read_csv(f'{folder_path}test_identity.csv', index_col='TransactionID', dtype=dtypes)\nprint('\\tSuccessfully loaded test_identity!')\n\ntest_transaction = pd.read_csv(f'{folder_path}test_transaction.csv', index_col='TransactionID', dtype=dtypes, usecols=cols)\nprint('\\tSuccessfully loaded test_transaction!')\n\nsub = pd.read_csv(f'{folder_path}sample_submission.csv')\nprint('\\tSuccessfully loaded sample_submission!')\n\nprint('Data was successfully loaded!\\n')","8af0de03":"print('Merging data...')\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint('Data was successfully merged!\\n')\n\ndel train_identity, train_transaction, test_identity, test_transaction\n\nprint(f'Train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\nprint(f'Test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')\n\ngc.collect()","a021d1a4":"train.head()","3fe4eac2":"test.head()","7f426968":"for i in range(1,16):\n    if i in [1,2,3,5,9]: continue\n    train['D'+str(i)] =  train['D'+str(i)] - train.TransactionDT\/np.float32(24*60*60)\n    test['D'+str(i)] = test['D'+str(i)] - test.TransactionDT\/np.float32(24*60*60)","bdf8feca":"# Demonstrando o que value_counts retorna\npd.concat([train['card1'], test['card1']], ignore_index=True).value_counts(dropna=False)","b2124e47":"# Realizando encoding\nfor feature in ['card1', 'card2', 'card3', 'P_emaildomain']:\n    train[feature + '_count'] = train[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n    test[feature + '_count'] = test[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))","e837064a":"# Apenas para demonstrar o que o groupby faz, n\u00e3o vamos usar como feature agora.\ntrain.groupby('card1')['TransactionAmt'].agg(['mean'])","b1059dba":"def make_day_feature(df, offset=0, tname='TransactionDT'):\n    days = df[tname] \/ (3600*24)        \n    encoded_days = np.floor(days-1+offset) % 7\n    return encoded_days\n\ndef make_hour_feature(df, tname='TransactionDT'):\n    hours = df[tname] \/ (3600)        \n    encoded_hours = np.floor(hours) % 24\n    return encoded_hours","4b3f8cea":"train['weekday'] = make_day_feature(train, offset=0.58)\ntest['weekday'] = make_day_feature(test, offset=0.58)\n\ntrain['hours'] = make_hour_feature(train)\ntest['hours'] = make_hour_feature(test)","91078053":"%%time\n\ncat_features = []\n\nfor col in train.columns:\n    if train[col].dtype == 'object':\n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values))\n        cat_features.append(col)","2ff82e0b":"import datetime\nSTART_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n\ntrain['DT_M'] = train['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\ntrain['DT_M'] = (train['DT_M'].dt.year-2017)*12 + train['DT_M'].dt.month \n\ntest['DT_M'] = test['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\ntest['DT_M'] = (test['DT_M'].dt.year-2017)*12 + test['DT_M'].dt.month","3b22ac16":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","4beece61":"X = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT'], axis=1)\ny = train.sort_values('TransactionDT')['isFraud']\n\nX_test = test.drop(['TransactionDT'], axis=1)\n\ndel train, test\ngc.collect()","c3635a3b":"X = X.fillna(-1)\nX_test = X_test.fillna(-1)\n\ncols = X.columns","50a50cdd":"xgb_params = {'n_estimators':5000,\n              'max_depth':12,\n              'learning_rate':0.02,\n              'subsample':0.8,\n              'colsample_bytree':0.4,\n              'missing':-1,\n              'eval_metric':'auc',\n              'tree_method':'gpu_hist'\n             }","8929c71f":"%%time\n\noof = np.zeros(len(X))\npreds = np.zeros(len(X_test))\n\nskf = GroupKFold(n_splits=6)\nfor i, (idxT, idxV) in enumerate(skf.split(X, y, groups=X['DT_M'])):\n    month = X.iloc[idxV]['DT_M'].iloc[0]\n    print('Fold',i,'withholding month',month)\n    print('rows of train =',len(idxT),'rows of holdout =',len(idxV))\n    \n    clf = xgb.XGBClassifier(**xgb_params)\n\n    h = clf.fit(X[cols].iloc[idxT], y.iloc[idxT], \n            eval_set=[(X[cols].iloc[idxV],y.iloc[idxV])],\n            verbose=100, early_stopping_rounds=200)\n\n    oof[idxV] += clf.predict_proba(X[cols].iloc[idxV])[:,1]\n    preds += clf.predict_proba(X_test[cols])[:,1]\/skf.n_splits\n    \n    del h, clf\n    x = gc.collect()\n\nprint('#'*20)\nprint ('XGB OOF CV=',roc_auc_score(y,oof))","8ad722ff":"sub['isFraud'] = preds\nsub.to_csv(\"submission_xgb.csv\", index=False)","70e4944a":"%%time\n\noof = np.zeros(len(X))\npreds = np.zeros(len(X_test))\n\nskf = GroupKFold(n_splits=6)\nfor i, (idxT, idxV) in enumerate(skf.split(X, y, groups=X['DT_M'])):\n    month = X.iloc[idxV]['DT_M'].iloc[0]\n    print('Fold',i,'withholding month',month)\n    print('rows of train =',len(idxT),'rows of holdout =',len(idxV))\n    \n    clf = cb.CatBoostClassifier(iterations=500, learning_rate = 0.05, max_depth=12, class_weights=[1,2.5], \n                                cat_features=cat_features, objective='Logloss', eval_metric = 'AUC', task_type = 'GPU')\n\n    h = clf.fit(X[cols].iloc[idxT], y.iloc[idxT], \n            eval_set=[(X[cols].iloc[idxV],y.iloc[idxV])],\n            verbose=100, early_stopping_rounds=200)\n\n    oof[idxV] += clf.predict_proba(X[cols].iloc[idxV])[:,1]\n    preds += clf.predict_proba(X_test[cols])[:,1]\/skf.n_splits\n    \n    del h, clf\n    x = gc.collect()\n\nprint('#'*20)\nprint ('XGB95 OOF CV=',roc_auc_score(y,oof))","5ae2ec86":"sub['isFraud'] = preds\nsub.to_csv(\"submission_cat.csv\", index=False)","ce1d36e4":"## DATA 1.2 | The power of Feature Engineering\n\nThis notebook was made for a lecture on feature engineering at the University of S\u00e3o Paulo for the members of the Data Science and Machine Learning group. It is written in Portuguese.","cb1c0cd9":"## Fun\u00e7\u00f5es \u00fateis","f0391eea":"### Normalizando vari\u00e1veis dependentes de tempo","8356161c":"### Aggregations","fe65c114":"## CatBoost","da5f6866":"Will blend with a submission file later","e062a789":"## XGBoost","c9935388":"## Carregando os dados","3e68414e":"## Feature Engineering","322e9a83":"## Encoding","6e376b6a":"## Feature selection (tema n\u00e3o abordado na aula)","eab422b1":"## LGBM","41292abb":"### Frequency encoding","8e17e0a3":"## Juntando os dados","5507c672":"Pode causar data leakage, vale tentar implementar de v\u00e1rias formas.","4a43f407":"## Importando bibliotecas","216e2067":"### Criando dia e hora da transa\u00e7\u00e3o","03155051":"## Preparando dados para o modelo"}}