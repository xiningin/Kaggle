{"cell_type":{"173f1048":"code","6186e048":"code","fd063b12":"code","7f2d25aa":"code","924ef224":"code","c94b484d":"code","1b942726":"code","dd246863":"code","53322cef":"code","e4b41641":"code","0c0874c8":"code","5ee8ad4c":"code","daa3a520":"code","06ae24fc":"code","160d4c88":"code","99da82de":"code","9046daa7":"code","46943661":"code","aaa9cd8a":"code","a070e5ec":"code","770af085":"code","fc1c12ce":"code","fcbc1716":"code","b5cad590":"code","868a2924":"code","71d2d92e":"code","533816a6":"code","15b56584":"code","86341ad2":"markdown","ad6ce47a":"markdown","3ccac47b":"markdown"},"source":{"173f1048":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly\nimport os\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6186e048":"df = pd.read_csv(r'\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv', \n                               error_bad_lines=False)\n","fd063b12":"df.shape","7f2d25aa":"df.dtypes","924ef224":"df['DEATH_EVENT'].unique()","c94b484d":"df.describe()","1b942726":"# Check the missing values in the column\nmissing_data = df.isna().sum().sort_values(ascending=False)\nmissing_data = missing_data.reset_index(drop=False)\nmissing_data = missing_data.rename(columns={\"index\": \"Columns\", 0: \"Value\"})\nmissing_data['Proportion'] = (missing_data['Value']\/len(df))*100\nmissing_data","dd246863":"fig = px.pie(df, names='DEATH_EVENT',\n             color_discrete_sequence=px.colors.sequential.Viridis_r,\n             title='Proportion of data for Class column')\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.update_layout(paper_bgcolor='rgba(0,0,0,0)',\n                  plot_bgcolor='rgba(0,0,0,0)',\n                  font=dict(family='Cambria, monospace', size=12, color='#000000'))\nfig.show()","53322cef":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(12,12))\n\n# Compute the correlation matrix\ncorr = df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=\"GnBu\", vmax=.3, center=0,\n            square=True, linewidths=.7, cbar_kws={\"shrink\": .7})","e4b41641":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef calc_vif(X):\n\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n    return(vif)","0c0874c8":"X = df.iloc[:,:-1]\nnew_X = calc_vif(X)","5ee8ad4c":"new_X","daa3a520":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(14,6))\nsns.set(style=\"darkgrid\")\nsns.countplot(x='age',data = df, hue = 'DEATH_EVENT',palette='RdBu')\nplt.title(\"Count Plot of DEATH EVENT per age\\n\", fontsize=16)\nsns.set_context(\"paper\", font_scale=1.4)\nplt.show()","06ae24fc":"plt.figure(figsize=(14,6))\nsns.set(style=\"darkgrid\")\nsns.countplot(x='sex',data = df, hue = 'DEATH_EVENT',palette='RdBu')\nplt.title(\"Count Plot of DEATH EVENT per sex\\n\", fontsize=16)\nsns.set_context(\"paper\", font_scale=1.4)\nplt.show()","160d4c88":"from sklearn.model_selection import cross_val_score\n# Using 5 folds cross-validation\ndef CrossVal(trainX, trainY, model):\n    f1=cross_val_score(model,trainX , trainY, cv=5, scoring='f1')\n    return(f1)","99da82de":"from xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom imblearn.ensemble import BalancedRandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix,f1_score,recall_score,precision_recall_curve,average_precision_score\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import StackingClassifier","9046daa7":"# define meta learner model\nlevel1 = SVC()\n# define the stacking ensemble\nestimates = list()\nestimates.append(('rf', RandomForestClassifier(n_estimators=500,\n                                          max_depth = 8,\n                                          min_samples_leaf=10,\n                                          # max_features='sqrt',\n                                          max_features = 0.7,\n                                          class_weight={0:1, 1:5},\n                                          n_jobs=4)))\nestimates.append(('brf', BalancedRandomForestClassifier(n_estimators=500, \n                                                   max_depth = 8,\n                                                   random_state=330, \n                                                   # max_features='sqrt', \n                                                   max_features = 0.7,\n                                                   class_weight={0:1, 1:5},\n                                                   n_jobs=4)))\nestimates.append(('xgb', XGBClassifier(max_depth=8,\n                                  learning_rate=0.7,\n                                  n_estimators=500,\n                                  # max_features='sqrt',\n                                  max_features = 0.7,\n                                  min_samples_leaf=10,\n                                  eval_metric=\"logloss\",\n                                  n_jobs=4)))\nestimates.append(('lgbm', LGBMClassifier(boosting_type='gbdt',\n                                    num_leaves=10,\n                                    max_depth=5,\n                                    learning_rate=0.7,\n                                    n_estimators=500,\n                                    # max_features='sqrt',\n                                    max_features = 0.7,\n                                    eval_metric=\"logloss\",\n                                    class_weight={0:1, 1:5},\n                                    n_jobs=4)))\nestimates.append(('bbc', BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n                                              n_estimators=500,\n                                              sampling_strategy='auto',\n                                              max_features = 0.5,\n                                              replacement=False,\n                                              random_state=330)))\n\n# Stacking Classifier\nstack = StackingClassifier(estimators = estimates, final_estimator=level1)\n# Voting Classifier with hard voting\nvot_hard = VotingClassifier(estimators = estimates, voting ='hard')","46943661":"def get_models():\n    models = {}\n    models['rf'] = RandomForestClassifier(n_estimators=500,\n                                          max_depth = 8,\n                                          min_samples_leaf=10,\n                                          # max_features='sqrt',\n                                          max_features = 0.7,\n                                          class_weight={0:1, 1:5},\n                                          n_jobs=4)\n    models['brf'] = BalancedRandomForestClassifier(n_estimators=500, \n                                                   max_depth = 8,\n                                                   random_state=330, \n                                                   # max_features='sqrt', \n                                                   max_features = 0.7,\n                                                   class_weight={0:1, 1:5},\n                                                   n_jobs=4)\n    models['xgb'] = XGBClassifier(max_depth=8,\n                                  learning_rate=0.7,\n                                  n_estimators=500,\n                                  # max_features='sqrt',\n                                  max_features = 0.7,\n                                  min_samples_leaf=10,\n                                  n_jobs=4)\n    models['lgbm'] = LGBMClassifier(boosting_type='gbdt',\n                                    num_leaves=10,\n                                    max_depth=5,\n                                    learning_rate=0.7,\n                                    n_estimators=500,\n                                    # max_features='sqrt',\n                                    max_features = 0.7,\n                                    class_weight={0:1, 1:5},\n                                    n_jobs=4)\n    models['bbc'] = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n                                              n_estimators=500,\n                                              sampling_strategy='auto',\n                                              max_features = 0.5,\n                                              replacement=False,\n                                              random_state=330)\n    models['stack'] = stack\n    models['vot_hard'] = vot_hard\n    return models","aaa9cd8a":"from sklearn.utils import resample\n# Separate input features and target\nY = df['DEATH_EVENT']\nX = df.drop('DEATH_EVENT', axis=1)\n\n# setting up testing and training sets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=2727)\n\n# concatenate our training data back together\nX = pd.concat([X_train, Y_train], axis=1)\n\nnormal = X[X['DEATH_EVENT']==0]\nanamoly = X[X['DEATH_EVENT']!=0]\n\n# upsample minority\nanamoly_upsampled = resample(anamoly,\n                          replace=True, # sample with replacement\n                          n_samples=len(normal), # match number in majority class\n                          random_state=2727) # reproducible results\n\n# combine majority and oversampled minority\noversampled = pd.concat([normal, anamoly_upsampled])\n\n# check new class counts\noversampled['DEATH_EVENT'].value_counts()","a070e5ec":"# Proportion before Oversampling\nprint(Y_train.value_counts())","770af085":"# Unseen data proportion of class\nprint(Y_test.value_counts())","fc1c12ce":"models = get_models()","fcbc1716":"from sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()\n\n# trying xgboost again with the balanced dataset\ny_train = oversampled['DEATH_EVENT']\nX_train = oversampled.drop('DEATH_EVENT', axis=1)\nprint(\"Class Proportion after oversampling\",y_train.value_counts())\n\nprint(\"Train Size\", X_train.shape)\nprint(\"Test Size\", X_test.shape)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_train_scaled = pd.DataFrame(X_train_scaled)\n\nX_test_scaled = scaler.transform(X_test)\nX_test_scaled = pd.DataFrame(X_test_scaled)\n\nprediction_smote = {}\nfor model in models.keys():\n    print(\"Model {0}\".format(model))\n    smote = models[model]\n    f1 = CrossVal(X_train_scaled, y_train, smote)\n    print(\"Cross-Validation F1 Score is {:.2f}%\".format(f1.mean()))\n    smote.fit(X_train_scaled, y_train)\n    # Predict on test\n    smote_pred = smote.predict(X_test_scaled)\n    # predict probabilities\n    #probs = smote.predict_proba(X_test_scaled)\n    # keep probabilities for the positive outcome only\n    #probs = probs[:, 1]\n    prediction_smote[model] = smote_pred","b5cad590":"prediction_smote","868a2924":"import shap\nexplainer = shap.TreeExplainer(models['xgb'])\nshap_values = explainer.shap_values(X_test_scaled)\nshap.summary_plot(shap_values, X_test_scaled, plot_type=\"bar\")","71d2d92e":"# Checking Balanced accuracy\nfrom sklearn.metrics import balanced_accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef evaluate(Y_test, smote_pred):\n    b_a = balanced_accuracy_score(Y_test, smote_pred)\n    print(\"Balanced Test Accuracy is {:.2f}%\".format(b_a * 100.0))\n    f1_over = f1_score(Y_test, smote_pred)\n    print(\"F1 Score is {:.2f}%\".format(f1_over))\n    # assign cnf_matrix with result of confusion_matrix array\n    cnf_matrix = confusion_matrix(Y_test,smote_pred)\n    #create a heat map\n    sns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Blues', fmt = 'd')\n    plt.xlabel('Predicted')\n    plt.ylabel('Expected')\n    plt.show()\n    recall = np.diag(cnf_matrix) \/ np.sum(cnf_matrix, axis = 1)\n    precision = np.diag(cnf_matrix) \/ np.sum(cnf_matrix, axis = 0)\n    print(\"Mean Recall\", np.mean(recall))\n    print(\"Mean Precision\", np.mean(precision))\n    return b_a, f1_over, recall, precision\n","533816a6":"b_accuracy = []\nf1_scores = []\nrecalls = []\nprecisions = []\nfor model in prediction_smote.keys():\n    print(\"\\nModel is {0}\".format(model))\n    a,b,c,d = evaluate(np.array(Y_test.astype(int)), prediction_smote[model])\n    b_accuracy.append(a)\n    f1_scores.append(b)\n    recalls.append(c)\n    precisions.append(d)","15b56584":"# Name List of ML Models used\nmodels = ['Random Forest', 'Balanced Random Forest', 'XGB', 'LGBM', 'Balanced Bagging', 'Stacking', 'Voting']\ny_pos = np.arange(len(models)) #Position = 0,1,2,3,4,5,6\n# Plot Cross Validation Accuracy\nplt.figure(figsize=(12, 6))  \nplt.bar(y_pos, b_accuracy, align='center', alpha=0.8, color=sns.color_palette(\"PuRd\"))\nplt.xticks(y_pos, models)\nplt.ylabel('Balanced Accuracy')\nplt.title('Performance based on Balanced Accuracy')\n\n# Plot F1 Score\nplt.figure(figsize=(12, 6))  \nplt.bar(y_pos, f1_scores, align='center', alpha=0.8, color=sns.color_palette(\"RdPu\"))\nplt.xticks(y_pos, models)\nplt.ylabel('F1 Score')\nplt.title('Performance based on F1 Score')","86341ad2":"<div style=\"text-align:center\"><img src=\"https:\/\/www.verywellhealth.com\/thmb\/PjK68gYYU57mdcDj0XdIcTe7GHQ=\/1001x1001\/smart\/filters:no_upscale()\/heart-failure-causes-40-5ae0bcdec673350037cb2ddd.png\", width=\"500\" height=\"500\"><\/div>","ad6ce47a":"## Index\n\n1. <a href='#1'> Data Ingestion and Exploration <\/a>\n    - <a href='#1.1'>1.1 Data Preparation <\/a>\n    - <a href='#1.2'>1.2 Visualize the dataset <\/a>\n    - <a href='#1.3'>1.3 Feature Engineering <\/a>\n2. <a href='#2'> Integrating Classification Labels <\/a>\n3. <a href='#3'> Resampling Techniques <\/a>\n4. <a href='#4'> Prediction <\/a>\n5. <a href='#5'> Evaluation <\/a>","3ccac47b":"The proportion of Class 0 is double than of Class 1. Thus the dataset is imbalanced. We need to use resampling techniques and different performance metrics to evaluate the results."}}