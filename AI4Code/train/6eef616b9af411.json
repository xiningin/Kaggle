{"cell_type":{"fba15975":"code","a7d2e75d":"code","43b734e9":"code","b3421997":"code","2f67086a":"code","c1bf6bef":"code","8106f1ed":"code","c90d7ef6":"code","43aea493":"code","83cc606f":"code","642074e6":"code","c4801c17":"code","5392b65b":"code","a296f6d4":"code","76391d71":"code","1a19455f":"code","9e7fb7ff":"code","d2471275":"code","9defdb25":"code","90c58c9a":"code","c3c133cb":"markdown","112b156b":"markdown","66a6759b":"markdown","121d6ae9":"markdown","92716e6a":"markdown","80e60130":"markdown","d950dda0":"markdown","6a4678e8":"markdown","4fec3095":"markdown","f3082531":"markdown","5119929f":"markdown","d5a61fa8":"markdown","9fc6770b":"markdown","8d13e793":"markdown","3929fc13":"markdown","6c61a3af":"markdown","2ac1a3a2":"markdown","163a7d62":"markdown","cc059d2b":"markdown","9441f4f4":"markdown","65e0dad3":"markdown","511b588f":"markdown"},"source":{"fba15975":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a7d2e75d":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","43b734e9":"df = pd.read_csv('..\/input\/iris\/Iris.csv')\ndf.sample()","b3421997":"df = df.drop('Id',axis=1)\ndf.head(2)","2f67086a":"df['Species'].value_counts()","c1bf6bef":"df['Species'].replace(to_replace=['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], value=[1, 2, 3], inplace=True)\n\ndf.head(2)","8106f1ed":"X=df.iloc[:,[0,1,2,3]].values\ny=df.iloc[:,4].values","c90d7ef6":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state= 10)","43aea493":"pipeline_lr=Pipeline([('scalar1',StandardScaler()),\n                     ('pca1',PCA(n_components=2)),\n                     ('lr_classifier',LogisticRegression(random_state=0))])","83cc606f":"pipeline_dt=Pipeline([('scalar2',StandardScaler()),\n                     ('pca2',PCA(n_components=2)),\n                     ('dt_classifier',DecisionTreeClassifier())])","642074e6":"pipeline_randomforest=Pipeline([('scalar3',StandardScaler()),\n                     ('pca3',PCA(n_components=2)),\n                     ('rf_classifier',RandomForestClassifier())])","c4801c17":"## Lets make the list of pipelines\npipelines = [pipeline_lr, pipeline_dt, pipeline_randomforest]","5392b65b":"best_accuracy=0.0\nbest_classifier=0\nbest_pipeline=\"\"","a296f6d4":"# Dictionary of pipelines and classifier types for ease of reference\npipe_dict = {0: 'Logistic Regression', 1: 'Decision Tree', 2: 'RandomForest'}\n\n# Fit the pipelines\nfor pipe in pipelines:\n\tpipe.fit(X_train, y_train)","76391d71":"for i,model in enumerate(pipelines):\n    print(\"{} Test Accuracy: {}\".format(pipe_dict[i],model.score(X_test,y_test)))","1a19455f":"for i,model in enumerate(pipelines):\n    if model.score(X_test,y_test)>best_accuracy:\n        best_accuracy=model.score(X_test,y_test)\n        best_pipeline=model\n        best_classifier=i\nprint('Classifier with best accuracy:{}'.format(pipe_dict[best_classifier]))","9e7fb7ff":"from sklearn.model_selection import GridSearchCV\n\n# Create a pipeline\npipe = Pipeline([(\"classifier\", RandomForestClassifier())])\n# Create dictionary with candidate learning algorithms and their hyperparameters\ngrid_param = [\n                {\"classifier\": [LogisticRegression()],\n                 \"classifier__penalty\": ['l2','l1'],\n                 \"classifier__C\": np.logspace(0, 4, 10)\n                 },\n                {\"classifier\": [LogisticRegression()],\n                 \"classifier__penalty\": ['l2'],\n                 \"classifier__C\": np.logspace(0, 4, 10),\n                 \"classifier__solver\":['newton-cg','saga','sag','liblinear'] ##This solvers don't allow L1 penalty\n                 },\n                {\"classifier\": [RandomForestClassifier()],\n                 \"classifier__n_estimators\": [10, 100, 1000],\n                 \"classifier__max_depth\":[5,8,15,25,30,None],\n                 \"classifier__min_samples_leaf\":[1,2,5,10,15,100],\n                 \"classifier__max_leaf_nodes\": [2, 5,10]}]\n# create a gridsearch of the pipeline, the fit the best model\ngridsearch = GridSearchCV(pipe, grid_param, cv=5, verbose=0,n_jobs=-1) # Fit grid search\nbest_model = gridsearch.fit(X_train,y_train)","d2471275":"print(best_model.best_estimator_)\nprint(\"The mean accuracy of the model is:\",best_model.score(X_test,y_test))","9defdb25":"\nfrom sklearn.pipeline import make_pipeline\n# Create a pipeline\npipe = make_pipeline((RandomForestClassifier()))\n# Create dictionary with candidate learning algorithms and their hyperparameters\ngrid_param = [\n                {\"randomforestclassifier\": [RandomForestClassifier()],\n                 \"randomforestclassifier__n_estimators\": [10, 100, 1000],\n                 \"randomforestclassifier__max_depth\":[5,8,15,25,30,None],\n                 \"randomforestclassifier__min_samples_leaf\":[1,2,5,10,15,100],\n                 \"randomforestclassifier__max_leaf_nodes\": [2, 5,10]}]\n# create a gridsearch of the pipeline, the fit the best model\ngridsearch = GridSearchCV(pipe, grid_param, cv=5, verbose=0,n_jobs=-1) # Fit grid search\nbest_model = gridsearch.fit(X_train,y_train)","90c58c9a":"print(best_model.best_estimator_)\nprint(\"The mean accuracy of the model is:\",best_model.score(X_test,y_test))","c3c133cb":"### Test Train Split","112b156b":"In this notebook we will see how to use Pipeline with Scikit Learn Package.This is very useful while doing projects were we use lot of data Preprocessing and Feature Engineering.When we do lot of things together our code looks messy and is difficult for other people to read and understand.We can use Pipeline to optimize our code.In this Note book we will covere following things \n\n1.Data Import and Preprocessing \n\n2.Feature Engineering \n\n3.Pipeline Creating \n\n4.Model Evaluation \n\n5.Pipeline for Hyper parameter tuning \n\n6.Conclusion ","66a6759b":"# 5.Pipeline for Hyper Parameter Tuning ","121d6ae9":"### Dropping Unwanted Columns ","92716e6a":"### Value Counts","80e60130":"# 1.Data Import and Preprocessing ","d950dda0":"### 1.Logistiv Regression ","6a4678e8":"### Matrix of Features ","4fec3095":"### Encoding The Target Species ","f3082531":"Our dataset has features of Iris Flower.We will be using the flower features to predict the species of the Iris flower.","5119929f":"### Recently I published a self help book titled Inspiration: Thoughts on Spirituality, Technology, Wealth, Leadership and Motivation. The preview of the book can be read from the Amazon link https:\/\/lnkd.in\/gj7bMQA\n\n### You can refer to my other notebooks from https:\/\/www.kaggle.com\/binuthomasphilip\/code","d5a61fa8":"Logistic Regression Has least accuracy and Decision Tree and Random Forest have same accurracy.","9fc6770b":"### Hyper Parameter tuning for Random Forest ","8d13e793":"### 2.Desision Tree ","3929fc13":"### Importing Data ","6c61a3af":"# 6.Conclusion \n\n1.We have built pipeline using Sklearn module.\n\n2.We have clubbed together scaling,dimensionality reduction and classification model using Sklearn Pipeline.\n\n3.We could easily compare the performance of three model logistic regression,Decision Tree and Random Forest.\n\n4.We have also seen how we can so hyper parameter tuning using Sklearn Pipeline.\n\n5.Overall making use of Sklearn Pipeline optimizes our codes and makes it more readable.","2ac1a3a2":"# 3.Pipeline Creation\n\nWe will be building the pipe line in three steps \n\n1.Scaling the data \n\n2.PCA for Dimensionality Reduction \n\n3.Classification Model ","163a7d62":"### Importing Python Modules","cc059d2b":"# 2.Feature Engineering ","9441f4f4":"### 3.Random Forest ","65e0dad3":"### Best Classifier ","511b588f":"# 4.Model Evaluation "}}