{"cell_type":{"5d14c59d":"code","c7745ccd":"code","1546c427":"code","9a663f8b":"code","7a9c650a":"code","3ca8396f":"code","f8b3ce3e":"code","4801c97c":"code","3d901f41":"code","00e54a67":"code","27622797":"code","567b11ae":"code","8bae3c33":"code","50e8044f":"code","7ab0c185":"code","8f4f133e":"code","ebb2809a":"code","e04a03ba":"code","4b6827bd":"code","96a7d3fa":"code","215f59de":"code","0a7ddf40":"code","ad842e83":"code","ff26d471":"code","d6b3a20c":"code","1007ecb0":"code","cd61e02e":"code","02364800":"code","b7e6da12":"code","b2959815":"code","43239bf5":"code","eaf2e6ce":"code","47387b3b":"code","f390c6f0":"code","a38b4465":"code","2b8d7d21":"code","0a1095c6":"code","c9b37fc7":"code","0eea3f91":"code","f5dc54e1":"code","39c962c6":"code","5dc67ed3":"code","4dcd9f26":"code","2ba31775":"code","8badbf6a":"code","8cb94c8e":"code","3f45c7eb":"code","5fe28c20":"code","2a8b28b6":"code","93fd7d42":"code","d1bd515c":"code","ec2e61c0":"code","d5811924":"code","46a914df":"code","056b9e0b":"code","baa920ba":"code","3dcebc58":"code","307522b5":"code","42385c26":"code","6bd1f7e9":"code","94322ab4":"code","a1d649a3":"code","21e66a4e":"code","414ad123":"code","570f2e17":"code","17d4d612":"code","2d6836fc":"code","e1081e27":"code","1febcb88":"code","59534630":"code","86f13ebd":"code","53ea115a":"code","fc7e6b9e":"code","5d958f24":"code","d9d72daf":"code","bbe71b56":"code","0c979511":"code","f5b84a6d":"code","79570a1b":"code","03acf083":"code","c93efc8f":"code","810f1d52":"code","4a704856":"code","dade84bf":"code","f284089e":"code","90d2f9e1":"code","86ea2e28":"code","032c03f7":"code","ef93083a":"code","2254544b":"code","e30b0de9":"code","24d7207b":"code","03aeaa86":"code","f0496657":"code","821718b1":"code","301f9c96":"code","8f3dfe83":"code","6b2c979a":"code","f1ccf667":"code","7255381d":"code","290adc12":"code","a14df616":"markdown","4df80745":"markdown","868e9c92":"markdown","3f7e4ab9":"markdown","8281627f":"markdown","0e61d8b1":"markdown","ed6cdb6f":"markdown","610f8a47":"markdown","5e2b825c":"markdown","b9823fd9":"markdown","56c16676":"markdown","75024b33":"markdown","9237594f":"markdown","174a2625":"markdown","f0afa24e":"markdown","fdeecfde":"markdown","b7432928":"markdown","4526d4b1":"markdown","0f2f75dc":"markdown","887d06ec":"markdown","0bd0cf2c":"markdown","f3ffbdb9":"markdown","387ae41f":"markdown","49948f9f":"markdown","6beda2da":"markdown","1caae5fa":"markdown","902ed232":"markdown","7c39220a":"markdown","0fd9b973":"markdown","fb95d33e":"markdown","550bfc0b":"markdown","9bf07752":"markdown","c9c90a64":"markdown","1632e69d":"markdown","725f1570":"markdown","7ba13cd8":"markdown"},"source":{"5d14c59d":"from __future__ import print_function  # Compatability with Python 3\nprint( 'Print function ready to serve.' )","c7745ccd":"# NumPy for numerical computing\nimport numpy as np\n\n# Pandas for DataFrames\nimport pandas as pd\npd.set_option('display.max_columns', 100)\n\n# Matplotlib for visualization\nfrom matplotlib import pyplot as plt\n# display plots in the notebook\n%matplotlib inline \n\n# Seaborn for easier visualization\nimport seaborn as sns\n\nfrom scipy import stats\nsns.set()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n#warnings.filterwarnings(\"ignore\")\n\nfrom subprocess import check_output\n#print(check_output([\"ls\", \"\/\"]).decode(\"utf8\"))","1546c427":"# Load real estate data from CSV\ndf_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")","9a663f8b":"# setting the number of cross validations used in the Model part \nnr_cv = 5\n\n# switch for using log values for SalePrice and features     \nuse_logvals = 1    \n# target used for correlation \ntarget = 'SalePrice_Log'\n    \n# only columns with correlation above this threshold value  \n# are used for the ML Regressors in Part 3\nmin_val_corr = 0.4    \n    \n# switch for dropping columns that are similar to others already used and show a high correlation to these     \ndrop_similar = 1","7a9c650a":"def get_best_score(grid):\n    \n    best_score = np.sqrt(-grid.best_score_)\n    print(best_score)    \n    print(grid.best_params_)\n    print(grid.best_estimator_)\n    \n    return best_score","3ca8396f":"def print_cols_large_corr(df, nr_c, targ) :\n    corr = df.corr()\n    corr_abs = corr.abs()\n    print (corr_abs.nlargest(nr_c, targ)[targ])","f8b3ce3e":"def plot_corr_matrix(df, nr_c, targ) :\n    \n    corr = df.corr()\n    corr_abs = corr.abs()\n    cols = corr_abs.nlargest(nr_c, targ)[targ].index\n    cm = np.corrcoef(df[cols].values.T)\n\n    plt.figure(figsize=(nr_c\/1.5, nr_c\/1.5))\n    sns.set(font_scale=1.25)\n    sns.heatmap(cm, linewidths=1.5, annot=True, square=True, \n                fmt='.2f', annot_kws={'size': 10}, \n                yticklabels=cols.values, xticklabels=cols.values\n               )\n    plt.show()","4801c97c":"# Dataframe dimensions\nprint(df_train.shape)\nprint(\"*\"*50)\nprint(df_test.shape)","3d901f41":"# Column datatypes\nprint(df_train.dtypes)\nprint(\"*\"*50)\nprint(df_test.dtypes)","00e54a67":"# Type of df.types\ntype(df_train.dtypes)","27622797":"# Display first 5 rows of df_train\ndf_train.head()","567b11ae":"# Display first 5 rows of df_test\ndf_test.head()","8bae3c33":"# Filter and display only df.dtypes that are 'object'\ndf_train.dtypes[df_train.dtypes == 'object']","50e8044f":"# Loop through categorical feature names and print each one\nfor feature in df_train.dtypes[df_train.dtypes == 'object'].index:\n    print(feature)","7ab0c185":"# Display the first 10 rows of data\ndf_train.head(10)","8f4f133e":"# Display last 5 rows of data\ndf_train.tail()","ebb2809a":"# Plot histogram grid\ndf_train.hist(figsize=(20,20), xrot=-45)\n\n# Clear the text \"residue\"\nplt.show()","e04a03ba":"# Summarize numerical features\ndf_train.describe()","4b6827bd":"df_test.describe()","96a7d3fa":"sns.distplot(df_train['SalePrice']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())","215f59de":"df_train['SalePrice_Log'] = np.log1p(df_train['SalePrice'])\n\nsns.distplot(df_train['SalePrice_Log']);\n# skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice_Log'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice_Log'].kurt())","0a7ddf40":"# Summarize categorical features\ndf_train.describe(include=['object'])","ad842e83":"numerical_feats = df_train.dtypes[df_train.dtypes != \"object\"].index\nprint(\"Number of Numerical features: \", len(numerical_feats))\n\ncategorical_feats = df_train.dtypes[df_train.dtypes == \"object\"].index\nprint(\"Number of Categorical features: \", len(categorical_feats))","ff26d471":"print(df_train[numerical_feats].columns)\nprint(\"*\"*100)\nprint(df_train[categorical_feats].columns)","d6b3a20c":"# Plot bar plot for each categorical feature\n\nfor feature in df_train.dtypes[df_train.dtypes == 'object'].index:\n    sns.countplot(y=feature, data=df_train)\n    plt.show()","1007ecb0":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","cd61e02e":"# columns where NaN values have meaning e.g. no pool etc.\ncols_fillna = ['PoolQC','MiscFeature','Alley','Fence','MasVnrType','FireplaceQu',\n               'GarageQual','GarageCond','GarageFinish','GarageType', 'Electrical',\n               'KitchenQual', 'SaleType', 'Functional', 'Exterior2nd', 'Exterior1st',\n               'BsmtExposure','BsmtCond','BsmtQual','BsmtFinType1','BsmtFinType2',\n               'MSZoning', 'Utilities']\n\n# replace 'NaN' with 'None' in these columns\nfor col in cols_fillna:\n    df_train[col].fillna('None',inplace=True)\n    df_test[col].fillna('None',inplace=True)","02364800":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(5)","b7e6da12":"# fillna with mean or mode for the remaining values\ndf_train.fillna(df_train.mean(), inplace=True)\ndf_test.fillna(df_test.mean(), inplace=True)\ndf_train.fillna(df_train.mode(), inplace=True)\ndf_test.fillna(df_test.mode(), inplace=True)","b2959815":"df_train.isnull().sum().sum()","43239bf5":"df_test.isnull().sum().sum()","eaf2e6ce":"for col in numerical_feats:\n    print(col)\n    print(\"Skewness: %f\" % df_train[col].skew())\n    print(\"Kurtosis: %f\" % df_train[col].kurt())\n    print(\"*\"*50)","47387b3b":"sns.distplot(df_train['GrLivArea']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['GrLivArea'].skew())\nprint(\"Kurtosis: %f\" % df_train['GrLivArea'].kurt())","f390c6f0":"sns.distplot(df_train['LotArea']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['LotArea'].skew())\nprint(\"Kurtosis: %f\" % df_train['LotArea'].kurt())","a38b4465":"for df in [df_train, df_test]:\n    df['GrLivArea_Log'] = np.log(df['GrLivArea'])\n    df.drop('GrLivArea', inplace= True, axis = 1)\n    df['LotArea_Log'] = np.log(df['LotArea'])\n    df.drop('LotArea', inplace= True, axis = 1)\n    \n    \n    \nnumerical_feats = df_train.dtypes[df_train.dtypes != \"object\"].index","2b8d7d21":"sns.distplot(df_train['GrLivArea_Log']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['GrLivArea_Log'].skew())\nprint(\"Kurtosis: %f\" % df_train['GrLivArea_Log'].kurt())","0a1095c6":"sns.distplot(df_train['LotArea_Log']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['LotArea_Log'].skew())\nprint(\"Kurtosis: %f\" % df_train['LotArea_Log'].kurt())","c9b37fc7":"nr_rows = 12\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n\nli_num_feats = list(numerical_feats)\nli_not_plot = ['Id', 'SalePrice', 'SalePrice_Log']\nli_plot_num_feats = [c for c in list(numerical_feats) if c not in li_not_plot]\n\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(li_plot_num_feats):\n            sns.regplot(df_train[li_plot_num_feats[i]], df_train[target], ax = axs[r][c])\n            stp = stats.pearsonr(df_train[li_plot_num_feats[i]], df_train[target])\n            #axs[r][c].text(0.4,0.9,\"title\",fontsize=7)\n            str_title = \"r = \" + \"{0:.2f}\".format(stp[0]) + \"      \" \"p = \" + \"{0:.2f}\".format(stp[1])\n            axs[r][c].set_title(str_title,fontsize=11)\n            \nplt.tight_layout()    \nplt.show()","0eea3f91":"corr = df_train.corr()\ncorr_abs = corr.abs()\n\nnr_num_cols = len(numerical_feats)\nser_corr = corr_abs.nlargest(nr_num_cols, target)[target]\n\ncols_abv_corr_limit = list(ser_corr[ser_corr.values > min_val_corr].index)\ncols_bel_corr_limit = list(ser_corr[ser_corr.values <= min_val_corr].index)","f5dc54e1":"print(ser_corr)\nprint(\"*\"*30)\nprint(\"List of numerical features with r above min_val_corr :\")\nprint(cols_abv_corr_limit)\nprint(\"*\"*30)\nprint(\"List of numerical features with r below min_val_corr :\")\nprint(cols_bel_corr_limit)","39c962c6":"for catg in list(categorical_feats) :\n    print(df_train[catg].value_counts())\n    print('#'*50)","5dc67ed3":"li_cat_feats = list(categorical_feats)\nnr_rows = 15\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*4,nr_rows*3))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(li_cat_feats):\n            sns.boxplot(x=li_cat_feats[i], y=target, data=df_train, ax = axs[r][c])\n    \nplt.tight_layout()    \nplt.show()   ","4dcd9f26":"catg_strong_corr = [ 'MSZoning', 'Neighborhood', 'Condition2', 'MasVnrType', 'ExterQual', \n                     'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType']\n\ncatg_weak_corr = ['Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', \n                  'LandSlope', 'Condition1',  'BldgType', 'HouseStyle', 'RoofStyle', \n                  'RoofMatl', 'Exterior1st', 'Exterior2nd', 'ExterCond', 'Foundation', \n                  'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', \n                  'HeatingQC', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', \n                  'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', \n                  'SaleCondition' ]\n      ","2ba31775":"nr_feats = len(cols_abv_corr_limit)","8badbf6a":"plot_corr_matrix(df_train, nr_feats, target)","8cb94c8e":"id_test = df_test['Id']\n\nto_drop_num  = cols_bel_corr_limit\nto_drop_catg = catg_weak_corr\n\ncols_to_drop = ['Id'] + to_drop_num + to_drop_catg \n\nfor df in [df_train, df_test]:\n    df.drop(cols_to_drop, inplace= True, axis = 1)","3f45c7eb":"catg_list = catg_strong_corr.copy()\ncatg_list.remove('Neighborhood')\n\nfor catg in catg_list :\n    #sns.catplot(x=catg, y=target, data=df_train, kind='boxen')\n    sns.violinplot(x=catg, y=target, data=df_train)\n    plt.show()\n    #sns.boxenplot(x=catg, y=target, data=df_train)\n    #bp = df_train.boxplot(column=[target], by=catg)","5fe28c20":"fig, ax = plt.subplots()\nfig.set_size_inches(16, 5)\nsns.violinplot(x='Neighborhood', y=target, data=df_train, ax=ax)\nplt.xticks(rotation=45)\nplt.show()","2a8b28b6":"for catg in catg_list :\n    g = df_train.groupby(catg)[target].mean()\n    print(g)","93fd7d42":"# 'MSZoning'\nmsz_catg2 = ['RM', 'RH']\nmsz_catg3 = ['RL', 'FV'] \n\n\n# Neighborhood\nnbhd_catg2 = ['Blmngtn', 'ClearCr', 'CollgCr', 'Crawfor', 'Gilbert', 'NWAmes', 'Somerst', 'Timber', 'Veenker']\nnbhd_catg3 = ['NoRidge', 'NridgHt', 'StoneBr']\n\n# Condition2\ncond2_catg2 = ['Norm', 'RRAe']\ncond2_catg3 = ['PosA', 'PosN'] \n\n# SaleType\nSlTy_catg1 = ['Oth']\nSlTy_catg3 = ['CWD']\nSlTy_catg4 = ['New', 'Con']\n\n#[]","d1bd515c":"for df in [df_train, df_test]:\n    \n    df['MSZ_num'] = 1  \n    df.loc[(df['MSZoning'].isin(msz_catg2) ), 'MSZ_num'] = 2    \n    df.loc[(df['MSZoning'].isin(msz_catg3) ), 'MSZ_num'] = 3        \n    \n    df['NbHd_num'] = 1       \n    df.loc[(df['Neighborhood'].isin(nbhd_catg2) ), 'NbHd_num'] = 2    \n    df.loc[(df['Neighborhood'].isin(nbhd_catg3) ), 'NbHd_num'] = 3    \n\n    df['Cond2_num'] = 1       \n    df.loc[(df['Condition2'].isin(cond2_catg2) ), 'Cond2_num'] = 2    \n    df.loc[(df['Condition2'].isin(cond2_catg3) ), 'Cond2_num'] = 3    \n    \n    df['Mas_num'] = 1       \n    df.loc[(df['MasVnrType'] == 'Stone' ), 'Mas_num'] = 2 \n    \n    df['ExtQ_num'] = 1       \n    df.loc[(df['ExterQual'] == 'TA' ), 'ExtQ_num'] = 2     \n    df.loc[(df['ExterQual'] == 'Gd' ), 'ExtQ_num'] = 3     \n    df.loc[(df['ExterQual'] == 'Ex' ), 'ExtQ_num'] = 4     \n   \n    df['BsQ_num'] = 1          \n    df.loc[(df['BsmtQual'] == 'Gd' ), 'BsQ_num'] = 2     \n    df.loc[(df['BsmtQual'] == 'Ex' ), 'BsQ_num'] = 3     \n \n    df['CA_num'] = 0          \n    df.loc[(df['CentralAir'] == 'Y' ), 'CA_num'] = 1    \n\n    df['Elc_num'] = 1       \n    df.loc[(df['Electrical'] == 'SBrkr' ), 'Elc_num'] = 2 \n\n\n    df['KiQ_num'] = 1       \n    df.loc[(df['KitchenQual'] == 'TA' ), 'KiQ_num'] = 2     \n    df.loc[(df['KitchenQual'] == 'Gd' ), 'KiQ_num'] = 3     \n    df.loc[(df['KitchenQual'] == 'Ex' ), 'KiQ_num'] = 4      \n    \n    df['SlTy_num'] = 2       \n    df.loc[(df['SaleType'].isin(SlTy_catg1) ), 'SlTy_num'] = 1  \n    df.loc[(df['SaleType'].isin(SlTy_catg3) ), 'SlTy_num'] = 3  \n    df.loc[(df['SaleType'].isin(SlTy_catg4) ), 'SlTy_num'] = 4  ","ec2e61c0":"new_col_num = ['MSZ_num', 'NbHd_num', 'Cond2_num', 'Mas_num', 'ExtQ_num', 'BsQ_num', 'CA_num', 'Elc_num', 'KiQ_num', 'SlTy_num']\n\nnr_rows = 4\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(new_col_num):\n            sns.regplot(df_train[new_col_num[i]], df_train[target], ax = axs[r][c])\n            stp = stats.pearsonr(df_train[new_col_num[i]], df_train[target])\n            str_title = \"r = \" + \"{0:.2f}\".format(stp[0]) + \"      \" \"p = \" + \"{0:.2f}\".format(stp[1])\n            axs[r][c].set_title(str_title,fontsize=11)\n            \nplt.tight_layout()    \nplt.show()   ","d5811924":"catg_cols_to_drop = ['Neighborhood' , 'Condition2', 'MasVnrType', 'ExterQual', 'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType']\n\ncorr1 = df_train.corr()\ncorr_abs_1 = corr1.abs()\n\nnr_all_cols = len(df_train)\nser_corr_1 = corr_abs_1.nlargest(nr_all_cols, target)[target]\n\nprint(ser_corr_1)\ncols_bel_corr_limit_1 = list(ser_corr_1[ser_corr_1.values <= min_val_corr].index)\n\n\nfor df in [df_train, df_test] :\n    df.drop(catg_cols_to_drop, inplace= True, axis = 1)\n    df.drop(cols_bel_corr_limit_1, inplace= True, axis = 1)    ","46a914df":"corr2 = df_train.corr()\ncorr_abs_2 = corr2.abs()\n\nnr_all_cols = len(df_train)\nser_corr_2 = corr_abs_2.nlargest(nr_all_cols, target)[target]\n\nprint(ser_corr_2)","056b9e0b":"df_train.head()","baa920ba":"df_test.head()","3dcebc58":"corr = df_train.corr()\ncorr_abs = corr.abs()\n\nnr_all_cols = len(df_train)\nprint (corr_abs.nlargest(nr_all_cols, target)[target])","307522b5":"nr_feats=len(df_train.columns)\nplot_corr_matrix(df_train, nr_feats, target)","42385c26":"cols = corr_abs.nlargest(nr_all_cols, target)[target].index\ncols = list(cols)\n\nif drop_similar == 1 :\n    for col in ['GarageArea','1stFlrSF','TotRmsAbvGrd','GarageYrBlt'] :\n        if col in cols: \n            cols.remove(col)","6bd1f7e9":"cols = list(cols)\nprint(cols)","94322ab4":"feats = cols.copy()\nfeats.remove('SalePrice_Log')\nfeats.remove('SalePrice')\n\nprint(feats)","a1d649a3":"df_test.head()","21e66a4e":"df_train_ml = df_train[feats].copy()\ndf_test_ml  = df_test[feats].copy()\n\ny = df_train[target]\n\nprint(target)","414ad123":"# NumPy for numerical computing\nimport numpy as np\n\n# Pandas for DataFrames\nimport pandas as pd\npd.set_option('display.max_columns', 100)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n# Matplotlib for visualization\nfrom matplotlib import pyplot as plt\n# display plots in the notebook\n%matplotlib inline \n\n# Seaborn for easier visualization\nimport seaborn as sns\n\n# Scikit-Learn for Modeling\nimport sklearn\n\n# Import Elastic Net, Ridge Regression, and Lasso Regression\nfrom sklearn.linear_model import ElasticNet, Ridge, Lasso\n\n# Import Random Forest and Gradient Boosted Trees\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor","570f2e17":"X = df_train_ml.copy()\ny = df_train[target]\nX_test = df_test_ml.copy()\n#y_test = df_train[target]\n\nX.info()\nX_test.info()","17d4d612":"# Split X and y into train and test sets\nX_train = df_train[feats].copy()\nX_test = df_test[feats].copy()\ny_train = df_train[target]\n#y_test = df_test[target]","2d6836fc":"X.head()","e1081e27":"X_test.head()","1febcb88":"print( len(X_train), len(X_test), len(y_train) )","59534630":"# Summary statistics of X_train\nX_train.describe()","86f13ebd":"# Standardize X_train\nX_train_new = (X_train - X_train.mean()) \/ X_train.std()","53ea115a":"# Summary statistics of X_train_new\nX_train_new.describe()","fc7e6b9e":"# Function for creating model pipelines\nfrom sklearn.pipeline import make_pipeline","5d958f24":"# For standardization\nfrom sklearn.preprocessing import StandardScaler","d9d72daf":"make_pipeline(StandardScaler(), Lasso(random_state=123))","bbe71b56":"# Create pipelines dictionary\npipelines = {\n    'lasso' : make_pipeline(StandardScaler(), Lasso(random_state=123)),\n    'ridge' : make_pipeline(StandardScaler(), Ridge(random_state=123)),\n    'enet'  : make_pipeline(StandardScaler(), ElasticNet(random_state=123))\n}","0c979511":"# Add a pipeline for 'rf'\npipelines['rf'] = make_pipeline(StandardScaler(), RandomForestRegressor(random_state=123))\n\n# Add a pipeline for 'gb'\npipelines['gb'] = make_pipeline(StandardScaler(), GradientBoostingRegressor(random_state=123))","f5b84a6d":"# Check that we have all 5 algorithms, and that they are all pipelines\nfor key, value in pipelines.items():\n    print( key, type(value) )","79570a1b":"# List tuneable hyperparameters of our Lasso pipeline\npipelines['lasso'].get_params()","03acf083":"# Lasso hyperparameters\nlasso_hyperparameters = { \n    'lasso__alpha' : [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10] \n}\n\n# Ridge hyperparameters\nridge_hyperparameters = { \n    'ridge__alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]  \n}","c93efc8f":"# Elastic Net hyperparameters\nenet_hyperparameters = { \n    'elasticnet__alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10],                        \n    'elasticnet__l1_ratio' : [0.1, 0.3, 0.5, 0.7, 0.9]  \n}","810f1d52":"# Random forest hyperparameters\nrf_hyperparameters = { \n    'randomforestregressor__n_estimators' : [100, 200],\n    'randomforestregressor__max_features': ['auto', 'sqrt', 0.33],\n}","4a704856":"# Boosted tree hyperparameters\ngb_hyperparameters = { \n    'gradientboostingregressor__n_estimators': [100, 200],\n    'gradientboostingregressor__learning_rate' : [0.05, 0.1, 0.2],\n    'gradientboostingregressor__max_depth': [1, 3, 5]\n}","dade84bf":"# Create hyperparameters dictionary\nhyperparameters = {\n    'rf' : rf_hyperparameters,\n    'gb' : gb_hyperparameters,\n    'lasso' : lasso_hyperparameters,\n    'ridge' : ridge_hyperparameters,\n    'enet' : enet_hyperparameters\n}","f284089e":"for key in ['enet', 'gb', 'ridge', 'rf', 'lasso']:\n    if key in hyperparameters:\n        if type(hyperparameters[key]) is dict:\n            print( key, 'was found in hyperparameters, and it is a grid.' )\n        else:\n            print( key, 'was found in hyperparameters, but it is not a grid.' )\n    else:\n        print( key, 'was not found in hyperparameters')","90d2f9e1":"# Helper for cross-validation\nfrom sklearn.model_selection import GridSearchCV","86ea2e28":"# Create cross-validation object from Lasso pipeline and Lasso hyperparameters\nmodel = GridSearchCV(pipelines['lasso'], hyperparameters['lasso'], cv=10, n_jobs=-1)","032c03f7":"type(model)","ef93083a":"# Fit and tune model\nmodel.fit(X_train, y_train)","2254544b":"# Create empty dictionary called fitted_models\nfitted_models = {}\n\n# Loop through model pipelines, tuning each one and saving it to fitted_models\nfor name, pipeline in pipelines.items():\n    # Create cross-validation object from pipeline and hyperparameters\n    model = GridSearchCV(pipeline, hyperparameters[name], cv=10, n_jobs=-1)\n    \n    # Fit model on X_train, y_train\n    model.fit(X_train, y_train)\n    \n    # Store model in fitted_models[name] \n    fitted_models[name] = model\n    \n    # Print '{name} has been fitted'\n    print(name, 'has been fitted.')","e30b0de9":"# Check that we have 5 cross-validation objects\nfor key, value in fitted_models.items():\n    print( key, type(value) )","24d7207b":"from sklearn.exceptions import NotFittedError\n\nfor name, model in fitted_models.items():\n    try:\n        pred = model.predict(X_test)\n        print(name, 'has been fitted.')\n    except NotFittedError as e:\n        print(repr(e))","03aeaa86":"# Display best_score_ for each fitted model\nfor name, model in fitted_models.items():\n    print( name, model.best_score_ )","f0496657":"# Import r2_score and mean_absolute_error functions\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error","821718b1":"# Display fitted random forest object\nfitted_models['rf']","301f9c96":"# Predict test set using fitted random forest\npred = fitted_models['rf'].predict(X_test)","8f3dfe83":"# Calculate and print R^2 and MAE, for training data set\npred_train = fitted_models['rf'].predict(X_train)\nprint( 'R^2:', r2_score(y_train, pred_train ))\nprint( 'MAE:', mean_absolute_error(y_train, pred_train))","6b2c979a":"print(pred)","f1ccf667":"df_test.head()","7255381d":"# Create final table\n\n\npred_pd = pd.DataFrame({'Id': id_test, 'SalePrice': pred})\n\npred_pd.head()","290adc12":"pred_pd.to_csv('submission_mrig_v1.csv', index=False)","a14df616":"## Distributions of numeric features","4df80745":"columns and correlation after dropping","868e9c92":"### Plots of relation to target for all numerical features","3f7e4ab9":"### List of all features with strong correlation to SalePrice_Log\n<br> after dropping all coumns with weak correlation","8281627f":"#### List of features used for the Regressors in Part 3","0e61d8b1":"List of numerical features and their correlation coefficient to target","ed6cdb6f":"### import the libraries we'll need","610f8a47":"the target variable SalePrice is not normally distributed.\nThis can reduce the performance of the ML models because they assume normal distribution, see sklearn info on preprocessing\n\nTherfore we make a log transformation, the resulting distribution looks much better.","5e2b825c":"Creating Datasets for ML algorithms","b9823fd9":"#### Correlation Matrix 2 : All features with strong correlation to SalePrice","56c16676":"Some useful functions","75024b33":"Relation to SalePrice for all categorical features","9237594f":"List of features with missing values","174a2625":"Check for Multicollinearity\n\n<br> Strong correlation of these features to other, similar features:\n\n<br> 'GrLivArea_Log' and 'TotRmsAbvGrd'\n\n<br> 'GarageCars' and 'GarageArea'\n\n<br> 'TotalBsmtSF' and '1stFlrSF'\n\n<br> 'YearBuilt' and 'GarageYrBlt'\n\n<br> Of those features we drop the one that has smaller correlation coeffiecient to Target.","f0afa24e":"Correlation matrix 1\n<br> Features with largest correlation to SalePrice_Log\n<br> all numerical features with correlation coefficient above threshold","fdeecfde":"new dataframes","b7432928":"Convert categorical columns to numerical\n<br> For those categorcial features where the EDA with boxplots seem to show a strong dependence of the SalePrice on the \n<br> category, we transform the columns to numerical. To investigate the relation of the categories to SalePrice in more detail, we \n<br> make violinplots for these features Also, we look at the mean of SalePrice as function of category.","4526d4b1":"Columns of Numerical and Categorical features","0f2f75dc":"## Part 2: Data wrangling\n<br> Drop all columns with only small correlation to SalePrice\n<br> Transform Categorical to numerical \n<br> Handling columns with missing data\n<br> Log values\n<br> Drop all columns with strong correlation to similar features\n\n<br> Numerical columns : drop similar and low correlation\n\n<br> Categorical columns : Transform to numerical\n\n<br> Dropping all columns with weak correlation to SalePrice","887d06ec":"##### import the libraries we'll need","0bd0cf2c":"Checking correlation to SalePrice for the new numerical columns","f3ffbdb9":"List of categorical features and their unique values","387ae41f":"Conclusion from EDA on categorical columns:\n\nFor many of the categorical there is no strong relation to the target.\nHowever, for some fetaures it is easy to find a strong relation.\nFrom the figures above these are : 'MSZoning', 'Neighborhood', 'Condition2', 'MasVnrType', 'ExterQual', 'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType' Also for the categorical features, I use only those that show a strong relation to SalePrice. So the other columns are dropped when creating the ML dataframes in Part 2 :\n'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'ExterCond', 'Foundation', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleCondition'","49948f9f":"Filling missing values\nFor a few columns there is lots of NaN entries.\nHowever, reading the data description we find this is not missing data:\nFor PoolQC, NaN is not missing data but means no pool, likewise for Fence, FireplaceQu etc.","6beda2da":"There are few columns with quite large correlation to SalePrice (NbHd_num, ExtQ_num, BsQ_num, KiQ_num).\n<br> These will probably be useful for optimal performance of the Regressors in part 3.\n\n<br> Dropping the converted categorical columns and the new numerical columns with weak correlation\n\n<br> columns and correlation before dropping\n\n","1caae5fa":"Display summary statistics for categorical features.","902ed232":"# Relation of features to target (SalePrice_log)","7c39220a":"### shape, info, head and describe","0fd9b973":"### The target variable : Distribution of SalePrice","fb95d33e":"Outliers\n\nFind columns with strong correlation to target\nOnly those with r > min_val_corr are used in the ML Regressors in Part 3\nThe value for min_val_corr can be chosen in global settings","550bfc0b":"log transform\nLike the target variable, also some of the feature values are not normally distributed and it is therefore better to use log values in df_train and df_test. Checking for skewness and kurtosis:","9bf07752":"Number of Numerical and Categorical features","c9c90a64":"Of those features with the largest correlation to SalePrice, some also are correlated strongly to each other.\n\n<br> To avoid failures of the ML regression models due to multicollinearity, these are dropped in part 2.\n\n<br> This is optional and controlled by the switch drop_similar (global settings)","1632e69d":"Conclusion from EDA on numerical columns:\n\nWe see that for some features like 'OverallQual' there is a strong linear correlation (0.79) to the target.\nFor other features like 'MSSubClass' the correlation is very weak.\nFor this kernel I decided to use only those features for prediction that have a correlation larger than a threshold value to SalePrice.\nThis threshold value can be choosen in the global settings : min_val_corr\n\nWith the default threshold for min_val_corr = 0.4, these features are dropped in Part 2, Data Wrangling:\n'Id', 'MSSubClass', 'LotArea', 'OverallCond', 'BsmtFinSF2', 'BsmtUnfSF', 'LowQualFinSF', 'BsmtFullBath', 'BsmtHalfBath', 'HalfBath',\n'BedroomAbvGr', 'KitchenAbvGr', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold'\n\nWe also see that the entries for some of the numerical columns are in fact categorical values.\nFor example, the numbers for 'OverallQual' and 'MSSubClass' represent a certain group for that feature ( see data description txt)","725f1570":"# Exploratory Analysis","7ba13cd8":"## Basic information"}}