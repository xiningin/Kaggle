{"cell_type":{"3ba42791":"code","8dcceb5f":"code","16fe5e73":"code","3cb1fb8c":"code","8f4b02d0":"code","8e7f2a28":"code","228e4afd":"code","3a382dca":"code","f71267c0":"code","46bf8362":"code","0716cd92":"code","32a6170e":"code","d726d798":"code","7fcf109a":"code","73ae2a5b":"code","72cd390a":"code","3bd9da5d":"code","92aeb172":"code","f9a70a2f":"code","7626cb13":"code","073c4540":"code","cc32b3a7":"code","df604b25":"code","78f007b8":"code","80b29e7b":"code","c09f9e83":"code","eb921659":"code","517c430b":"code","6a6c2fc7":"code","d13cb972":"code","5daa18bd":"code","63b3f63f":"code","44b31d28":"code","35660b01":"code","133b8fca":"code","71a9c8fa":"code","d7d4cf57":"code","867dad0f":"code","d42501cd":"code","10d1cb03":"code","a6dd2dbd":"code","80ef3af8":"code","bc0e9b10":"code","6ebda80c":"markdown","f56c9f7a":"markdown","ca2cc748":"markdown","8fb6f4e5":"markdown","352c9d30":"markdown","1b42fdda":"markdown","44501135":"markdown","96563065":"markdown","3ab17012":"markdown","c6a349b6":"markdown"},"source":{"3ba42791":"# import libraries\nimport numpy as np\nimport pandas as pd\nimport datetime as dt\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom pandas.plotting import autocorrelation_plot","8dcceb5f":"# read the dataset into a dataframe\ndf = pd.read_csv('\/kaggle\/input\/bitcoin-historical-data\/bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv')\ndf.head()","16fe5e73":"# Here I start cleaning the data. Firstly, converting Timestamp to datetime64\ndf.Timestamp = pd.to_datetime(df.Timestamp, unit='s')\n# sets the index as the date\ndf.index = df.Timestamp\n# Resamples the data to the average daily value of each column. Removes excessive frequency\ndf = df.resample('D').mean()\n# drops any missing values that are present\ndf = df.dropna()\ndf.head()","3cb1fb8c":"print(df.shape)\n# Now we have 3,376 rows instead of 4 million - with no missing values and accurate datetime information","8f4b02d0":"# graph bitcoin price over the years\ndf.Weighted_Price.plot(title = \"Bitcoin Price\", figsize=(14,6))\nplt.tight_layout()\nplt.xlabel('Years')\nplt.ylabel('US Dollars')\nplt.show()\n# As the graph shows 2017-2021 price behavior looks signficantly different than 2012-2017","8e7f2a28":"# Plot the autocorrelation to see if there are a large number of lags in the time series.\nautocorrelation_plot(df)\nplt.show()","228e4afd":"# let's look at the past 200 days to possibly adjust our data to this period\ndf.Weighted_Price.iloc[-200:].plot(title = \"Bitcoin Price\", figsize=(14,6))\nplt.tight_layout()\nplt.xlabel('Years')\nplt.ylabel('US Dollars')\nplt.show()","3a382dca":"# Since the first couple years of bitcoin don't properly represent the movement and volatility of the price -\n# I decide to simply focus on the previous 4 years of data from March 31st, 2021.\ndf2 = pd.read_csv('\/kaggle\/input\/bitcoin-historical-data\/bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv')\ndf2.Timestamp = pd.to_datetime(df2.Timestamp, unit='s')\ndf2.index = df2.Timestamp\ndf2 = df2.resample('D').mean()\ndf2 = df2.dropna()\n# changes data to strictly to the previous 4 years, which is March 2017 to March 2021\ndf2 = df2.iloc[(-365*4):]\nprint(df2.shape)","f71267c0":"df2.Weighted_Price.plot(title = \"Bitcoin Price\", figsize=(14,6))\nplt.tight_layout()\nplt.xlabel('Dates')\nplt.ylabel('$ Price')\nplt.show()\n# This data looks much more relevant for training a model. \n# However the recent spike will be involved in the testing data split - a tough prediction.","46bf8362":"from statsmodels.tsa.arima_model import ARIMA\n# We're going to create a dataframe for just the price (the index is still the date)\nprice = df2.Weighted_Price\n# Next we're going to assign 70% percent of the data to training and 30% for testing\nX = price.values\nsize = int(len(X) * 0.7)\ntrain, test = X[0:size], X[size:len(X)]\nhistory = [x for x in train]\npredictions = list()","0716cd92":"# walk-forward validation\nfor t in range(len(test)):\n\tmodel = ARIMA(history, order=(5,1,0))\n\tmodel_fit = model.fit()\n\toutput = model_fit.forecast()\n\tyhat = output[0]\n\tpredictions.append(yhat)\n\tobs = test[t]\n\thistory.append(obs)\n\tprint('predicted=%f, expected=%f' % (yhat, obs))\n# evaluate forecasts\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)\n# plot forecasts against actual outcomes\nplt.plot(test)\nplt.plot(predictions, color='red')\nplt.show()","32a6170e":"# if we look at this model, the predicted is indistinuishable from the actual price\n# this is simply because it's predicting day by day.\nplt.figure(figsize=(15,8))\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)\n# plot forecasts against actual outcomes\nplt.plot(test)\nplt.plot(predictions, color='red')\nplt.xlabel('Days')\nplt.ylabel('$ Price')\nplt.title('Predicted vs. Actual BTC Price')\nplt.show()","d726d798":"# I plot 50 days to more accurately see how the models works with its lag\nplt.figure(figsize=(15,8))\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)\n# plot forecasts against actual outcomes\nplt.plot(test[-50:])\nplt.plot(predictions[-50:], color='red')\nplt.xlabel('Days')\nplt.ylabel('$ Price')\nplt.title('Predicted vs. Expected BTC Price Forecast')\nplt.show()","7fcf109a":"# importing Prophet model\n# importing prophet library\nimport fbprophet\nfrom fbprophet import Prophet\n# print version number\nprint('Prophet %s' % fbprophet.__version__)","73ae2a5b":"# For prophet we have to fit the dataframe to two columns: ds, y\ndf3 = df2.copy()\ndf3.reset_index(inplace=True)\ndf3 = df3[['Timestamp', 'Weighted_Price']]\ndf3.columns = ['ds', 'y']\n# Next the data is split as usual\nX = df3\nsize = int(len(X) * 0.7)\ntrain, test = X[0:size], X[size:len(X)]","72cd390a":"model = Prophet()\n# fit the model\nmodel.fit(train)","3bd9da5d":"# frequency is a day and periods are length of test data\nfuture = model.make_future_dataframe(periods=len(test), freq='D')\nforecasting = model.predict(future)\n# prediction is stored and RMSE is returned\ny_true = test['y'].values\ny_pred = forecasting.yhat.values[-len(test):]\nrmse = np.sqrt(mean_squared_error(y_true, y_pred))\nprint('Test RMSE: %.3f' % rmse)\n","92aeb172":"forecasting.head()","f9a70a2f":"# Plotting forecast\nmodel.plot(forecasting)\nplt.title('Prophet Forecast for BTC Price')\nplt.ylabel('$ Price')\nplt.xlabel('Date')\nplt.show()","7626cb13":"# Plotting forecast with test data\nax = forecasting.plot(x='ds', y='yhat', legend=True, label=\"Predicted\", figsize=(15,8), color='red')\nforecasting.plot(x='ds', y='yhat_upper', legend=True, label='Upper Prediction', ax=ax, color='yellow')\nforecasting.plot(x='ds', y='yhat_lower', legend=True, label='Lower Prediction', ax=ax, color='purple')\ntest.plot(x='ds', y='y', legend=True, label='Actual', ax=ax)\nplt.ylabel('$ Price')\nplt.xlabel('Date')\nplt.title('Predicted and Expected BTC Price (Prophet)')\nplt.legend(loc='best')\nplt.show()","073c4540":"from statsmodels.tsa.arima_model import ARIMA\n# method to be used later\ndef difference(dataset, interval=1):\n    diff = list()\n    for i in range(interval, len(dataset)):\n        value = dataset[i] - dataset[i-interval]\n        diff.append(value)\n    return np.array(diff)\n\ndef inverse_difference(history, yhat, interval=1):\n    return yhat + history[-interval]","cc32b3a7":"print(len(test))","df604b25":"# Split the data as usual 70, 30\nprice = df2.Weighted_Price\nX = price.values\ndatesX = price.index\nsize = int(len(X) * 0.70)\ntrain, test = X[0:size], X[size:len(X)]\ndays_in_year = 365\nplotDates = datesX[size:len(X)]\n\n# Next we will forecast with ARIMA using 5,1,0\ndifferenced = difference(train, days_in_year)\nmodel = ARIMA(differenced, order=(5, 1, 0))\nmodel_fit = model.fit()\nstart_index = len(differenced)\nend_index = start_index + 438\nforecast = model_fit.predict(start=start_index, end=end_index)\n\nhistory = [x for x in train]\nday = 1\npredicted_results = list()\n\n# store predicted results \nfor yhat in forecast:\n    inverted = inverse_difference(history, yhat, days_in_year)\n    print(\"Predicted Day %d: %f\" % (day, inverted))\n    history.append(inverted)\n    predicted_results.append(inverted)\n    day += 1\n","78f007b8":"rmse = np.sqrt(mean_squared_error(test, predicted_results))\nprint('Test RMSE: %.3f' % rmse)","80b29e7b":"print(model_fit.summary())\n# line plot of residuals\nresiduals = pd.DataFrame(model_fit.resid)\nresiduals.plot()\nplt.show()\n# density plot of residuals\nresiduals.plot(kind='kde')\nplt.show()\n# summary stats of residuals\nprint(residuals.describe())","c09f9e83":"plt.figure(figsize=(13,10))\nplt.plot(plotDates, test, label='Expected')\nplt.plot(plotDates, predicted_results, label='Predicted')\nplt.ylabel('$ Price')\nplt.xlabel('Date')\nplt.legend(loc='best')\nplt.title('Predicted and Expected BTC Price (ARIMA)')\nplt.show()","eb921659":"from sklearn.preprocessing import MinMaxScaler\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout,Flatten\nfrom tensorflow.keras import Sequential\nfrom statsmodels.graphics.tsaplots import plot_acf","517c430b":"price = df2.Weighted_Price\n# As usual split data, 70, 30\nX = price\nsize = int(len(X) * 0.7)\ntrain_df, test_df = X[0:size], X[size:len(X)]\ntraining_values = train_df.values\ntraining_values = np.reshape(training_values, (len(training_values), 1))\n\n# Scale the data using minMaxScaler\nscaler = MinMaxScaler()\ntraining_values = scaler.fit_transform(training_values)\n# assign training values\nx_train = training_values[0: len(training_values)-1]\ny_train = training_values[1: len(training_values)]\nx_train = np.reshape(x_train, (len(x_train), 1, 1))","6a6c2fc7":"# creates model\nmodel = Sequential()\nmodel.add(LSTM(10,input_shape = (None,1), activation=\"relu\", return_sequences=True))\nmodel.add(Dense(1))","d13cb972":"model.compile(loss=\"mean_squared_error\",optimizer=\"adam\")","5daa18bd":"# fit the model to the training data\nmodel.fit(x_train,y_train,epochs=50,batch_size=32)","63b3f63f":"# assign test and predicted values + reshaping + converting back from scaler\ntest_values = test_df.values\ntest_values = np.reshape(test_values, (len(test_values), 1))\ntest_values = scaler.transform(test_values)\ntest_values = np.reshape(test_values, (len(test_values), 1, 1))\npredicted_price = model.predict(test_values)\npredicted_price = np.reshape(predicted_price, (len(predicted_price), 1))\npredicted_price = scaler.inverse_transform(predicted_price)","44b31d28":"# plotting the results\nplt.figure(figsize=(15, 6))\nax = plt.gca()  \nplt.plot(test_df.values, color = 'red', label = 'Real Price')\nplt.plot(predicted_price, color = 'green', label = 'Predicted Price')\nplt.title('BTC Price Prediction (LSTM)')\ntest_df = test_df.reset_index()\nx= test_df.index\nlabels = test_df['Timestamp']\nplt.xticks(x, labels, rotation = 'vertical')\nplt.xlabel('Time')\nplt.ylabel('$ Price')\nplt.legend(loc=4, prop={'size': 14})\nplt.show()","35660b01":"rmse = np.sqrt(mean_squared_error(test_df.Weighted_Price.values, predicted_price))\nprint('Test RMSE: %.3f' % rmse)","133b8fca":"# splitting data again, 70, 30\ndf5 = df2.copy()\nX = df5\nsize = int(len(X) * 0.7)\ndata_train, data_test = X[0:size], X[size:len(X)]","71a9c8fa":"# a method to create a variety of features from a time series df\ndef create_features(df, label=None):\n    df['date'] = df.index\n    df['hour'] = df['date'].dt.hour\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['quarter'] = df['date'].dt.quarter\n    df['month'] = df['date'].dt.month\n    df['year'] = df['date'].dt.year\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['dayofmonth'] = df['date'].dt.day\n    df['weekofyear'] = df['date'].dt.weekofyear\n    X = df[['hour','dayofweek','quarter','month','year',\n           'dayofyear','dayofmonth','weekofyear']]\n    if label:\n        y = df[label]\n        return X, y\n    return X","d7d4cf57":"# assigning training and testing, features and labels (price)\nX_train, y_train = create_features(data_train, label='Weighted_Price')\nX_test, y_test = create_features(data_test, label='Weighted_Price')","867dad0f":"# import XGBoost, then create model, and fit it to the training data\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nmodel =  xgb.XGBRegressor(objective ='reg:linear',min_child_weight=10, booster='gbtree', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 100)\nmodel.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], early_stopping_rounds=50, verbose=False)","d42501cd":"# assign predictions to data_test and then data_all\ndata_test['Weighted_Price_Prediction'] = model.predict(X_test)\ndata_all = pd.concat([data_test, data_train], sort=False)","10d1cb03":"data_all[['Weighted_Price','Weighted_Price_Prediction']].plot(figsize=(15, 5))","a6dd2dbd":"data_test.head()","80ef3af8":"rmse = np.sqrt(mean_squared_error(data_test['Weighted_Price'], data_test['Weighted_Price_Prediction']))\nprint('Test RMSE: %.3f' % rmse)","bc0e9b10":"# Final graph for the RMSE's of each model\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nmodelz = ['Prophet', 'ARIMA', 'LSTM', 'XGBoost']\nnums = [24810,19633,8950, 18483]\nax.bar(modelz,nums)\nplt.xlabel('Models')\nplt.ylabel('RMSE')\nplt.title('RMSE of the Models')\nplt.show()\nplt.show()","6ebda80c":"Thank you so much for reading! Let me know your thoughts and comments on the notebook!","f56c9f7a":"## **6. Conclusion**\nThe final results of the experiment are plotted below. The Long Short-Term Memory model proves to be the most effective when handling volatile and hard-to-predict data like Bitcoin prices. This Bitcoin dataset consisted of extremely volatile and abnormal time series data. Therefore, I was unsurprised to find that the models had a difficult time predicting the most recent 439 days of prices. In addition, I acknowledge that I could\u2019ve picked better and more informed parameters for some of these models and libraries in order to make the results more fair. However, I think the LSTM model\u2019s low RMSE proves how powerful neural networks can be in Machine Learning. I\u2019d love to explore RNN\u2019s in financial time series data more in the future!\n","ca2cc748":"## **1. Exploratory Data Analysis**\n","8fb6f4e5":"## **5. XGBOOST MODEL**\nXGBoost is one of the most popular machine learning algorithms these days. Regardless of the type of prediction task at hand; regression, or classification. XGBoost is well known to provide better solutions than other machine learning algorithms. In fact, since its inception, it has become the \u201cstate-of-the-art\u201d machine learning algorithm to deal with structured data. However, in this situation we will be using it on time series data. Therefore, the model needed to create time series features from the datetime index - to be used alongside its target price labels when predicting. Unfortunately, in the end the model did not fair well.","352c9d30":"## **3. ARIMA MODEL**\nNext is ARIMA, an acronym that stands for AutoRegressive Integrated Moving Average. ARIMA is a widely used statistical method for analyzing and forecasting time series data. It consists of a suite of standard structures in time series data and provides a simple yet powerful method for making skillful time series forecasts. ARIMA models have 3 parameters (p, d, q), which indicate the specific ARIMA model being used. I simply applied parameters that I\u2019ve used on different time series data in the past, which could\u2019ve possibly led to an unfavorable model.","1b42fdda":"## **2. PROPHET**\nThe Prophet library is an open-source library developed by Facebook and designed for automatic forecasting of time series data. This model mainly focuses on predicting trends and seasonality. However due to its easy use I decided to implement this model first with default settings. As plotted below the prophet model predicts the price of bitcoin will fall. In addition, the model provides upper and lower bounds estimates as well (edges of shaded region). While the upper bounds estimate has an accurate slope direction, this model didn\u2019t perform well when compared to the test data.","44501135":"## **4. LSTM**\nLong Short-Term Memory (LSTM) models are a type of recurrent neural network capable of learning sequences of observations. This makes them a deep learning network well suited for time series forecasting. However, usually a LTSM would struggle with a time series dataset as volatile and unpredictable as Bitcoin. After a grueling process of trying to apply my data, I finally trained the model. In my final fit I used 50 epochs and the \u201cadam\u201d optimizer. (This model is inspired by another user's kaggle notebook).","96563065":"## **1A. ROLLING FORECAST ARIMA**\nI first created a rolling forecast ARIMA model in order to get a baseline for what a great RMSE is. However, since this model predicts in rolling day by day increments, it isn\u2019t fair to compare to the other models. As plotted below, the predictions are in red and the actual prices are in blue. However, we have to zoom in more to distinguish the two.\n","3ab17012":"### **Table of Contents**\n> \nThis notebook will be broken down into 6 sections:\n1. EDA\n2. PROPHET\n3. ARIMA\n4. LSTM\n5. XGBOOST\n5. CONCLUSION (Comparing each model's RMSE)\n\n**Standarized Procedure for Testing Models**\n- Each model will be trained on the first 70% of the data and tested on the last 30%. The bitcoin data will be resampled to days and restricted to the past 4 years only (from March 31st, 2021).\n- The Root Mean Square Error (RMSE) of each model will determine which is the best. The RMSE is the standard deviation of the residuals - or more simply how spread out the residuals are.\n","c6a349b6":"# **Forecasting Bitcoin Prices via ARIMA, XGBOOST, PROPHET, and LSTM**\n\n- Accompanying Medium Blog: https:\/\/floreani.medium.com\/how-well-can-machine-learning-models-predict-the-price-of-bitcoin-f036fdecdc03\n- Kaggle Dataset: https:\/\/www.kaggle.com\/mczielinski\/bitcoin-historical-data\n> \n<img src=\"https:\/\/media.giphy.com\/media\/8yQady2pFVfGJSnde7\/giphy-downsized.gif\">\n\n\n### **Introduction**\n> \nThe cryptocurrency Bitcoin continues to make world headlines and rise in popularity as more and more people\/organizations begin adopting it. In this notebook, I will test the Bitcoin forecasting abilities of 4 different Machine Learning models in Python: ARIMA, Prophet, XGBoost, and LSTM. By splitting the data into a testing and training set, I will compare each model\u2019s performance with one another and conclude which performed best. \n> \nLet me know your thoughts, comments, or suggestions below! I'd appreciate it!"}}