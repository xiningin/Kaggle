{"cell_type":{"fa7ac477":"code","55a374e9":"code","260ae20d":"code","33936055":"code","7416bae3":"code","2f996274":"code","cf0d42a9":"code","572bcd42":"code","df6edfe1":"code","9224e340":"code","89f227a6":"code","eabf35d2":"code","6b543c26":"code","ee3714ee":"code","37be2522":"code","5cc3175f":"code","ff8f87d6":"code","a43557fe":"code","810da7bc":"code","abddbfd9":"code","613251b3":"code","fb141a8b":"code","c51e554b":"code","59a47e3b":"code","1a1448ed":"code","d7dcec6f":"code","d73ae421":"code","5e34e2b0":"code","43f133e8":"code","74b0bec6":"code","31442ab8":"code","0138e7d8":"code","3d7fccbd":"code","89b5594b":"markdown"},"source":{"fa7ac477":"!pip install ..\/input\/sacremoses\/sacremoses-master\/ > \/dev\/null","55a374e9":"!pip install \"..\/input\/kerasswa\/keras-swa-0.1.2\"  > \/dev\/null","260ae20d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport gc\nimport glob\nimport os\nimport sys\nimport string\nimport random\nfrom tqdm import tqdm_notebook\nfrom urllib.parse import urlparse\nfrom sklearn.preprocessing import OneHotEncoder, minmax_scale, MultiLabelBinarizer\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import MultiTaskElasticNet\nfrom scipy.stats import spearmanr, rankdata\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\n#os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # -1 = CPU only\n\nimport nltk\nfrom nltk.tag import pos_tag\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nfrom gensim.models.doc2vec import TaggedDocument\nfrom gensim import utils\nimport torch\n\nsys.path.insert(0, \"..\/input\/transformers\/transformers-master\/\")\nimport transformers\n\n\nimport tensorflow as tf\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        # Currently, memory growth needs to be the same across GPUs\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        # Memory growth must be set before GPUs have been initialized\n        print(e)\n\nimport tensorflow_hub as hub\n\nimport keras\nimport keras.backend as K\nfrom keras.layers import *\nfrom keras.callbacks import *\nfrom keras.optimizers import *\nfrom keras import Model\nfrom swa.keras import SWA\n\nimport lightgbm as lgb\nimport catboost as cb\nimport xgboost as xgb\n\nimport pickle    \n\ndef save_obj(obj, name ):\n    with open(name + '.pkl', 'wb') as f:\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\ndef load_obj(name ):\n    with open(name + '.pkl', 'rb') as f:\n        return pickle.load(f)\n                \n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","33936055":"nltk.data.path.append(\"..\/input\/nltk-data\/nltk_data\")\n# thse are in nltk_data dataset \n# nltk.download('wordnet')\n# nltk.download('averaged_perceptron_tagger')\n# nltk.download('punkt')\n# nltk.download('stopwords')\n","7416bae3":"#INPUT_PATH=\"\/kaggle\/input\/\"\nINPUT_PATH=\"..\/input\/\"\ntrain = pd.read_csv(INPUT_PATH+'google-quest-challenge\/train.csv')\ntest = pd.read_csv(INPUT_PATH+'google-quest-challenge\/test.csv')\nsubmission = pd.read_csv(INPUT_PATH+'google-quest-challenge\/sample_submission.csv')","2f996274":"targets = [\n        'question_asker_intent_understanding',\n        'question_body_critical',\n        'question_conversational',\n        'question_expect_short_answer',\n        'question_fact_seeking',\n        'question_has_commonly_accepted_answer',\n        'question_interestingness_others',\n        'question_interestingness_self',\n        'question_multi_intent',\n        'question_not_really_a_question',\n        'question_opinion_seeking',\n        'question_type_choice',\n        'question_type_compare',\n        'question_type_consequence',\n        'question_type_definition',\n        'question_type_entity',\n        'question_type_instructions',\n        'question_type_procedure',\n        'question_type_reason_explanation',\n        'question_type_spelling',\n        'question_well_written',\n        'answer_helpful',\n        'answer_level_of_information',\n        'answer_plausible',\n        'answer_relevance',\n        'answer_satisfaction',\n        'answer_type_instructions',\n        'answer_type_procedure',\n        'answer_type_reason_explanation',\n        'answer_well_written'    \n    ]\n\ninput_columns = ['question_title','question_body','answer']\n\n","cf0d42a9":"#clean data\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3',\n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', '\\xa0', '\\t',\n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', '\\u3000', '\\u202f',\n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', '\u00ab',\n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\n\ndef clean_text(x):\n    x = str(x).replace(\"\\n\",\"\")\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\n\ndef replace_typical_misspell(text):\n    mispellings, mispellings_re = _get_mispell(mispell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\n\ndef clean_data(df, columns: list):\n    for col in columns:\n#         df[col] = df[col].apply(lambda x: clean_numbers(x))\n        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n\n    return df\n","572bcd42":"train = clean_data(train, input_columns)\ntest = clean_data(test, input_columns)","df6edfe1":"train['question_body'][0]","9224e340":"def constructLabeledSentences(data):\n    sentences=[]\n    for index, row in data.iteritems():\n        sentences.append(TaggedDocument(utils.to_unicode(row).split(), ['Text' + '_%s' % str(index)]))\n    return sentences\n\ndef textClean(text):\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\\/'+-=]\", \" \", text)\n    text = text.lower().split()\n    stops = set(stopwords.words(\"english\"))\n    text = [w for w in text if not w in stops]    \n    text = \" \".join(text)\n    return(text)\n    \ndef cleanup(text):\n    text = textClean(text)\n    text= text.translate(str.maketrans(\"\",\"\", string.punctuation))\n    return text\n\ntrain_question_body_sentences = constructLabeledSentences(train['question_body'])\ntrain_question_title_sentences = constructLabeledSentences(train['question_title'])\ntrain_answer_sentences = constructLabeledSentences(train['answer'])\n\ntest_question_body_sentences = constructLabeledSentences(test['question_body'])\ntest_question_title_sentences = constructLabeledSentences(test['question_title'])\ntest_answer_sentences = constructLabeledSentences(test['answer'])","89f227a6":"from gensim.models import Doc2Vec\n\nall_sentences = train_question_body_sentences + \\\n                train_answer_sentences + \\\n                test_question_body_sentences + \\\n                test_answer_sentences\n\nText_INPUT_DIM=128\ntext_model = Doc2Vec(min_count=1, window=5, vector_size=Text_INPUT_DIM, sample=1e-4, negative=5, workers=4, epochs=5,seed=1)\ntext_model.build_vocab(all_sentences)\ntext_model.train(all_sentences, total_examples=text_model.corpus_count, epochs=text_model.iter)","eabf35d2":"from nltk.stem.wordnet import WordNetLemmatizer\n\ndef normalize_sentence(tokens):\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_sentence = []\n    for word, tag in pos_tag(tokens):\n        if tag.startswith('NN') or tag.startswith('PRP'):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            continue\n            pos = 'a'\n        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos).lower())\n    return lemmatized_sentence\n\n#print(set(normalize_sentence(word_tokenize(train['question_body'][0]))))","6b543c26":"def normalize_vectorize(df, columns: list):\n    for col in columns:\n        print(col)\n        df[col+'_norm'] = df[col].apply(lambda x: ' '.join(set(normalize_sentence(word_tokenize(x)))))\n        df[col+'_vec'] = df[col].apply(lambda x: text_model.infer_vector([x]))\n\n    return df\n\ntrain = normalize_vectorize(train, input_columns)\ntest = normalize_vectorize(test, input_columns)\n","ee3714ee":"#bert embedings\ntry:\n    pbe = load_obj(\"..\/input\/questembeddings\/precomputed_bert_embeddings\")\n    train_question_body_dense = pbe['train_question_body_dense']\n    train_answer_dense = pbe['train_answer_dense']\n    train_question_title_dense = pbe['train_question_title_dense']\n    test_question_body_dense = pbe['test_question_body_dense']\n    test_answer_dense = pbe['test_answer_dense']\n    test_question_title_dense = pbe['test_question_title_dense']\nexcept:\n    print(\"Load failed, build embedding\")\n    def sigmoid(x):\n        return 1 \/ (1 + math.exp(-x))\n\n    def chunks(l, n):\n        \"\"\"Yield successive n-sized chunks from l.\"\"\"\n        for i in range(0, len(l), n):\n            yield l[i:i + n]\n\n    def fetch_vectors(string_list, batch_size=64):\n        # inspired by https:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/\n        DEVICE = torch.device(\"cuda\")\n        tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"..\/input\/distilbertbaseuncased\/\")\n        model = transformers.DistilBertModel.from_pretrained(\"..\/input\/distilbertbaseuncased\/\")\n        model.to(DEVICE)\n\n        fin_features = []\n        for data in tqdm_notebook(chunks(string_list, batch_size)):\n            tokenized = []\n            for x in data:\n                x = \" \".join(x.strip().split()[:300])\n                tok = tokenizer.encode(x, add_special_tokens=True)\n                tokenized.append(tok[:512])\n\n            max_len = 512\n            padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized], dtype='int64')\n            attention_mask = np.where(padded != 0, 1, 0)\n            input_ids = torch.tensor(padded).to(DEVICE)\n            attention_mask = torch.tensor(attention_mask).to(DEVICE)\n\n            with torch.no_grad():\n                last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n            features = last_hidden_states[0][:, 0, :].cpu().numpy()\n            fin_features.append(features)\n\n        fin_features = np.vstack(fin_features)\n        return fin_features\n\n    train_question_body_dense = fetch_vectors(train.question_body.values)\n    train_answer_dense = fetch_vectors(train.answer.values)\n    train_question_title_dense = fetch_vectors(train.question_title.values)\n\n\n    test_question_body_dense = fetch_vectors(test.question_body.values)\n    test_answer_dense = fetch_vectors(test.answer.values)\n    test_question_title_dense = fetch_vectors(test.question_title.values)\n\n    precomputed_bert_embeddings = {\n        'train_question_body_dense': train_question_body_dense,\n        'train_answer_dense': train_answer_dense,\n        'train_question_title_dense': train_question_title_dense,\n        'test_question_body_dense': test_question_body_dense,\n        'test_answer_dense': test_answer_dense,\n        'test_question_title_dense': test_question_title_dense,\n\n    }\n\n    #save_obj(precomputed_bert_embeddings,\"..\/input\/questembeddings\/precomputed_bert_embeddings\")","37be2522":"tfidf = TfidfVectorizer(ngram_range=(1, 3))\ntsvd = TruncatedSVD(n_components = 128, n_iter=5)\ntfquestion_title = tfidf.fit_transform(train[\"question_title\"].values)\ntfquestion_title_test = tfidf.transform(test[\"question_title\"].values)\ntfquestion_title = tsvd.fit_transform(tfquestion_title)\ntfquestion_title_test = tsvd.transform(tfquestion_title_test)\n\ntfquestion_body = tfidf.fit_transform(train[\"question_body\"].values)\ntfquestion_body_test = tfidf.transform(test[\"question_body\"].values)\ntfquestion_body = tsvd.fit_transform(tfquestion_body)\ntfquestion_body_test = tsvd.transform(tfquestion_body_test)\n\ntfanswer = tfidf.fit_transform(train[\"answer\"].values)\ntfanswer_test = tfidf.transform(test[\"answer\"].values)\ntfanswer = tsvd.fit_transform(tfanswer)\ntfanswer_test = tsvd.transform(tfanswer_test)","5cc3175f":"torch.cuda.empty_cache() # release all gpu memory from pytorch","ff8f87d6":"# universal sentence encoder\n\ntry:\n    embeddings_train = load_obj(\"..\/input\/questembeddings\/use_embeddings_train\")\n    embeddings_test = load_obj(\"..\/input\/questembeddings\/use_embeddings_test\")\nexcept:\n    print(\"Load failed, build embedding\")\n    try:\n        module_url = INPUT_PATH+'universalsentenceencoderlarge4\/'\n        embed = hub.load(module_url)\n        def UniversalEmbedding(x):\n            results = embed(tf.squeeze(tf.cast(x, tf.string)))[\"outputs\"]\n            return keras.backend.concatenate([results])\n    except:\n        module_url = INPUT_PATH+'universalsentenceencoderlarge3\/'\n        embed = hub.Module(module_url)\n        def UniversalEmbedding(x):\n            results = embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]\n            return keras.backend.concatenate([results])\n\n    embeddings_train = {}\n    embeddings_test = {}\n    for text in input_columns:\n        print(text)\n        train_text = train[text].str.replace('?', '.').str.replace('!', '.').tolist()\n        test_text = test[text].str.replace('?', '.').str.replace('!', '.').tolist()\n\n        curr_train_emb = []\n        curr_test_emb = []\n        batch_size = 4\n        ind = 0\n        while ind*batch_size < len(train_text):\n            curr_train_emb.append(embed(train_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n            ind += 1\n\n        ind = 0\n        while ind*batch_size < len(test_text):\n            curr_test_emb.append(embed(test_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n            ind += 1    \n\n        embeddings_train[text + '_embedding'] = np.vstack(curr_train_emb)\n        embeddings_test[text + '_embedding'] = np.vstack(curr_test_emb)\n\n    del embed\n    K.clear_session()\n    gc.collect()\n    \n    #save_obj(embeddings_train,\"..\/input\/questembeddings\/use_embeddings_train\")\n    #save_obj(embeddings_test,\"..\/input\/questembeddings\/use_embeddings_test\")","a43557fe":"find = re.compile(r\"^[^.]*\")\ntrain['netloc'] = train['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc'] = test['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n\nfeatures_lrg = ['category', 'netloc', 'question_user_name','answer_user_name','host']\nfeatures_sml = ['category', 'netloc', 'host']\n    \nfeatures_train = []\nfeatures_test = []\nfor feature in features_sml:\n    merged = pd.concat([train[feature], test[feature]]).to_numpy().reshape(-1, 1).squeeze()\n    encoded_size = len(max(merged, key=len))\n    print(feature,encoded_size)\n    ctr = 0\n    column = []\n    for val in train[feature].values:\n        ctr +=1\n        output = np.zeros(encoded_size)\n        output[:len(val)] = [ord(i)\/128. for i in val]\n        column.append(output)\n    features_train.append(column)\n        \n    column = []\n    for val in test[feature].values:\n        output = np.zeros(encoded_size)\n        output[:len(val)] = [ord(i)\/128. for i in val]\n        column.append(output)\n    features_test.append(column)\n\nfeatures_train_lrg = []\nfeatures_test_lrg = []\nfor feature in features_lrg:\n    merged = pd.concat([train[feature], test[feature]]).to_numpy().reshape(-1, 1).squeeze()\n    encoded_size = len(max(merged, key=len))\n    print(feature,encoded_size)\n    ctr = 0\n    column = []\n    for val in train[feature].values:\n        ctr +=1\n        output = np.zeros(encoded_size)\n        output[:len(val)] = [ord(i)\/128. for i in val]\n        column.append(output)\n    features_train.append(column)\n        \n    column = []\n    for val in test[feature].values:\n        output = np.zeros(encoded_size)\n        output[:len(val)] = [ord(i)\/128. for i in val]\n        column.append(output)\n    features_test.append(column)\n    \n    \nl2_dist = lambda x, y: np.power(x - y, 2).sum(axis=1)\ncos_dist = lambda x, y: (x*y).sum(axis=1)\nabs_dist = lambda x, y: np.abs(x-y).sum(axis=1)\nsum_dist = lambda x, y: (x+y).sum(axis=1)\n\ndist_features_train = np.array([\n    l2_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n    l2_dist(np.array([x for x in train.question_body_vec.values]), np.array([x for x in train.answer_vec.values])),\n    l2_dist(np.array([x for x in train.question_title_vec.values]), np.array([x for x in train.answer_vec.values])),\n    l2_dist(np.array([x for x in train.question_body_vec.values]), np.array([x for x in train.question_title_vec.values])),\n    cos_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n    cos_dist(np.array([x for x in train.question_body_vec.values]), np.array([x for x in train.answer_vec.values])),\n    cos_dist(np.array([x for x in train.question_title_vec.values]), np.array([x for x in train.answer_vec.values])),\n    cos_dist(np.array([x for x in train.question_body_vec.values]), np.array([x for x in train.question_title_vec.values])),\n    abs_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    abs_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    abs_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n    abs_dist(np.array([x for x in train.question_body_vec.values]), np.array([x for x in train.answer_vec.values])),\n    abs_dist(np.array([x for x in train.question_title_vec.values]), np.array([x for x in train.answer_vec.values])),\n    abs_dist(np.array([x for x in train.question_body_vec.values]), np.array([x for x in train.question_title_vec.values])),\n    sum_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    sum_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    sum_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n    sum_dist(np.array([x for x in train.question_body_vec.values]), np.array([x for x in train.answer_vec.values])),\n    sum_dist(np.array([x for x in train.question_title_vec.values]), np.array([x for x in train.answer_vec.values])),\n    sum_dist(np.array([x for x in train.question_body_vec.values]), np.array([x for x in train.question_title_vec.values])),\n    l2_dist(train_question_body_dense, train_answer_dense),\n    cos_dist(train_question_body_dense, train_answer_dense),\n    abs_dist(train_question_body_dense, train_answer_dense),\n    sum_dist(train_question_body_dense, train_answer_dense),\n    l2_dist(train_question_body_dense, train_question_title_dense),\n    cos_dist(train_question_body_dense, train_question_title_dense),\n    abs_dist(train_question_body_dense, train_question_title_dense),\n    sum_dist(train_question_body_dense, train_question_title_dense),\n    l2_dist(train_answer_dense, train_question_title_dense),\n    cos_dist(train_answer_dense, train_question_title_dense),\n    abs_dist(train_answer_dense, train_question_title_dense),\n    sum_dist(train_answer_dense, train_question_title_dense),\n]).T\n\n\ndist_features_test = np.array([\n    l2_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n    l2_dist(np.array([x for x in test.question_body_vec.values]), np.array([x for x in test.answer_vec.values])),\n    l2_dist(np.array([x for x in test.question_title_vec.values]), np.array([x for x in test.answer_vec.values])),\n    l2_dist(np.array([x for x in test.question_body_vec.values]), np.array([x for x in test.question_title_vec.values])),\n    cos_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n    cos_dist(np.array([x for x in test.question_body_vec.values]), np.array([x for x in test.answer_vec.values])),\n    cos_dist(np.array([x for x in test.question_title_vec.values]), np.array([x for x in test.answer_vec.values])),\n    cos_dist(np.array([x for x in test.question_body_vec.values]), np.array([x for x in test.question_title_vec.values])),\n    abs_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    abs_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    abs_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n    abs_dist(np.array([x for x in test.question_body_vec.values]), np.array([x for x in test.answer_vec.values])),\n    abs_dist(np.array([x for x in test.question_title_vec.values]), np.array([x for x in test.answer_vec.values])),\n    abs_dist(np.array([x for x in test.question_body_vec.values]), np.array([x for x in test.question_title_vec.values])),\n    sum_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    sum_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    sum_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n    sum_dist(np.array([x for x in test.question_body_vec.values]), np.array([x for x in test.answer_vec.values])),\n    sum_dist(np.array([x for x in test.question_title_vec.values]), np.array([x for x in test.answer_vec.values])),\n    sum_dist(np.array([x for x in test.question_body_vec.values]), np.array([x for x in test.question_title_vec.values])),\n    l2_dist(test_question_body_dense, test_answer_dense),\n    cos_dist(test_question_body_dense, test_answer_dense),\n    abs_dist(test_question_body_dense, test_answer_dense),\n    sum_dist(test_question_body_dense, test_answer_dense),\n    l2_dist(test_question_body_dense, test_question_title_dense),\n    cos_dist(test_question_body_dense, test_question_title_dense),\n    abs_dist(test_question_body_dense, test_question_title_dense),\n    sum_dist(test_question_body_dense, test_question_title_dense),\n    l2_dist(test_answer_dense, test_question_title_dense),\n    cos_dist(test_answer_dense, test_question_title_dense),\n    abs_dist(test_answer_dense, test_question_title_dense),\n    sum_dist(test_answer_dense, test_question_title_dense),\n]).T\n\n\n\n","810da7bc":"possible_features_train = [\n    [item for k, item in embeddings_train.items()],\n    features_train,\n    features_train_lrg,\n    [ dist_features_train ],\n    [ [x for x in train.question_body_vec.values] ],\n    [ [x for x in train.question_title_vec.values] ],\n    [ [x for x in train.answer_vec.values] ],\n    [ train_question_body_dense ],\n    [ train_answer_dense ],\n    [ tfquestion_title ],\n    [ tfquestion_body ],\n    [ tfanswer ]\n    \n]\npossible_features_test = [\n    [item for k, item in embeddings_test.items()],\n    features_test,\n    features_test_lrg,\n    [ dist_features_test ],\n    [ [x for x in test.question_body_vec.values] ],\n    [ [x for x in test.question_title_vec.values] ],\n    [ [x for x in test.answer_vec.values] ],\n    [ test_question_body_dense ],\n    [ test_answer_dense ],\n    [ tfquestion_title_test ],\n    [ tfquestion_body_test ],\n    [ tfanswer_test ]\n]\n\ndef get_train_test(split=0.8):\n    total_len = len(possible_features_train)\n    r_idx = random.sample(range(total_len), int(total_len * split))\n    \n    train = [ train_question_title_dense ]\n\n    test =  [ test_question_title_dense ]\n\n    for i in r_idx:\n        train += possible_features_train[i]\n        test += possible_features_test[i]\n        \n    return np.hstack(train),np.hstack(test)\n\nX_train,X_test = get_train_test()\ny_train = train[targets].values","abddbfd9":"from keras.losses import *\ndef bce(t,p):\n    return binary_crossentropy(t,p)\n\ndef custom_loss(true,pred):\n    bce = binary_crossentropy(true,pred)\n    return bce + logcosh(true,pred)\n\ndef swish(x):\n    return K.sigmoid(x) * x\n\ndef relu1(x):\n    return keras.activations.relu(x, alpha=0.0, max_value=1., threshold=0.0)\n\ndef create_model1(X_train):\n    input1 = Input(shape=(X_train.shape[1],))\n    x = Dense(64, activation='elu',kernel_initializer='lecun_normal')(input1)\n    x = Dense(128, activation='elu',kernel_initializer='lecun_normal')(x)\n    x = Dropout(0.4)(x)\n    output = Dense(len(targets),activation='sigmoid',name='output')(x)\n    model = Model(inputs=input1, outputs=output)\n    return model\n\n\ndef create_model2(X_train):\n    input1 = Input(shape=(X_train.shape[1],))\n    x = Dense(512, activation='elu',kernel_initializer='lecun_normal')(input1)\n    x = Dense(256, activation='elu',\n              kernel_initializer='lecun_normal', \n              #kernel_regularizer=keras.regularizers.l2(0.01)\n             )(x)\n    output = Dense(len(targets),activation='sigmoid',name='output')(x)\n    model = Model(inputs=input1, outputs=output)\n    return model\n# def create_model6(X_train):\n#     input1 = Input(shape=(X_train.shape[1],))\n#     x = Dense(512, activation='elu',kernel_initializer='lecun_normal')(input1)\n#     x = Dense(256, activation='elu',\n#               kernel_initializer='lecun_normal', \n#               #kernel_regularizer=keras.regularizers.l2(0.01)\n#              )(x)\n#     x = Dropout(0.4)(x)\n#     output = Dense(len(targets),activation='sigmoid',name='output')(x)\n#     model = Model(inputs=input1, outputs=output)\n#     return model\n\n\ndef create_model3(X_train):\n    input1 = Input(shape=(X_train.shape[1],))\n    x = Dense(200, activation='selu',kernel_initializer='lecun_normal')(input1)\n    x = Dense(512, activation='selu',\n              kernel_initializer='lecun_normal', \n              )(x)\n    x = Dropout(0.4)(x)\n    output = Dense(len(targets),activation='sigmoid',name='output')(x)\n    model = Model(inputs=input1, outputs=output)\n    return model\n\ndef create_model4(X_train):\n    input1 = Input(shape=(X_train.shape[1],))\n    x = Dense(512, activation='elu',\n              kernel_initializer='lecun_normal', \n              )(input1)\n    x = Dropout(0.2)(x)\n    output = Dense(len(targets),activation='sigmoid',name='output')(x)\n    model = Model(inputs=input1, outputs=output)\n    return model\n\ndef create_model5(X_train):\n    input1 = Input(shape=(X_train.shape[1],))\n    x = Dense(4096, activation='selu',kernel_initializer='lecun_normal')(input1)\n    x = Dense(512, activation='selu',\n              kernel_initializer='lecun_normal', \n              kernel_regularizer=keras.regularizers.l2(0.01)\n             )(x)\n    output = Dense(len(targets),activation='sigmoid',name='output')(x)\n    model = Model(inputs=input1, outputs=output)\n    return model\ndef create_model7(X_train):\n    input1 = Input(shape=(X_train.shape[1],))\n    x = Dense(4096, activation='selu',kernel_initializer='lecun_normal')(input1)\n    x = Dense(512, activation='selu',\n              kernel_initializer='lecun_normal', \n              kernel_regularizer=keras.regularizers.l2(0.01)\n             )(x)\n    x = Dropout(0.2)(x)\n    output = Dense(len(targets),activation='sigmoid',name='output')(x)\n    model = Model(inputs=input1, outputs=output)\n    return model\n\ndef create_model(X_train):\n    model = random.choice([create_model1, create_model2,create_model3, create_model4,create_model7])(X_train)\n    model.summary()\n    return model\n","613251b3":"import gc\nprint(gc.collect())","fb141a8b":"def pearson_metric(y_true, y_pred):\n    y_true = K.clip(y_true, K.epsilon(), 1)\n    y_pred = K.clip(y_pred, K.epsilon(), 1)\n    # reshape stage\n#     y_true = K.reshape(y_true, shape=(-1, 224 * 224 * 3))\n#     y_pred = K.reshape(y_pred, shape=(-1, 224 * 224 * 3))\n    # normalizing stage - setting a 0 mean.\n    y_true -= K.mean(y_true)\n    y_pred -= K.mean(y_pred)\n    # normalizing stage - setting a 1 variance\n    y_true = K.l2_normalize(y_true, axis=-1)\n    y_pred = K.l2_normalize(y_pred, axis=-1)\n    # final result\n    pearson_correlation = K.sum(y_true * y_pred, axis=-1)\n    return 1-pearson_correlation\n\n    ","c51e554b":"# Compatible with tensorflow backend\nerror_pred_y = None\nerror_y = None\nclass SpearmanRhoCallback(Callback):\n    def __init__(self, training_data, validation_data, patience, model_name, reload=False):\n        global noise\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n        \n        self.patience = patience\n        self.value = -1\n        self.bad_epochs = 0\n        self.model_name = model_name\n        self.reload = reload\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        y_pred_val = self.model.predict(self.x_val)\n        noise = np.random.normal(0, 1e-7, y_pred_val.shape[0])\n        rho_val = np.mean([spearmanr(self.y_val[:, ind] + noise, y_pred_val[:, ind] + noise).correlation for ind in range(y_pred_val.shape[1])])\n        print('\\r  val_spearman-rho: %s' % (str(round(rho_val, 4))), end=100*' '+'\\n')\n\n        if rho_val >= self.value:\n            self.model.save_weights(self.model_name)\n            self.value = rho_val\n        else:\n            self.model.load_weights(self.model_name)\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        global error_pred_y, error_y        \n        y_pred_val = self.model.predict(self.x_val)\n        noise = np.random.normal(0, 1e-7, y_pred_val.shape[0])\n        rho_val = np.mean([spearmanr(self.y_val[:, ind] + noise, y_pred_val[:, ind] + noise).correlation for ind in range(y_pred_val.shape[1])])\n        if np.isnan(rho_val):\n            print(\"Error\")\n            error_pred_y = y_pred_val\n            error_y = self.y_val\n            print(y_pred_val)\n            print(self.x_val)\n            print(self.y_val)\n            raise \"bogus error\"\n#         y_pred = self.model.predict(self.x)\n#         rho = np.mean([spearmanr(self.y[:, ind], y_pred[:, ind] + np.random.normal(0, 1e-7, y_pred.shape[0])).correlation for ind in range(y_pred.shape[1])])\n        if rho_val >= self.value:\n            self.model.save_weights(self.model_name)\n            self.value = rho_val\n        else:\n            self.bad_epochs += 1\n#         if self.bad_epochs >= self.patience:\n#             print(\"Epoch %05d: early stopping Threshold\" % epoch)\n#             self.model.stop_training = True\n        print('\\r  val_spearman-rho: %s' % (str(round(rho_val, 4))), end=100*' '+'\\n')\n#         print('\\rtrain_spearman-rho: %s' % (str(round(rho, 4))), end=100*' '+'\\n')\n        if self.reload:\n            print('  reload best: %s' % (str(round(self.value, 4))))\n            self.model.load_weights(self.model_name)\n        return rho_val\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return","59a47e3b":"all_predictions = []\nmodel_idx =0 \ndef run_model():\n    global y_train,all_predictions, model_idx\n    X_train,X_test = get_train_test()\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                                  patience=7, min_lr=1e-6, verbose=1)\n\n    early_stop = EarlyStopping(monitor='val_loss',\n                                  min_delta=0,\n                                  patience=15,\n                                  mode='auto')\n\n    kf = KFold(n_splits=5, random_state=random.randint(0,1000), shuffle=True)\n\n    for ind, (tr, val) in enumerate(kf.split(X_train)):\n        X_tr = X_train[tr]\n        y_tr = y_train[tr]\n        X_vl = X_train[val]\n        y_vl = y_train[val]\n        model_idx+=1\n        model = create_model(X_train)\n        optimizer = Adam(lr=5e-4,clipnorm=1.4)\n        model.compile(optimizer=optimizer, loss=custom_loss, metrics=[bce,logcosh])\n        model.fit(\n            X_tr, y_tr, \n            epochs=100, batch_size=64, \n            validation_data=(X_vl, y_vl), \n            verbose=True, \n            callbacks=[SpearmanRhoCallback(training_data=(X_tr, y_tr), \n                                           validation_data=(X_vl, y_vl),\n                                           patience=10, \n                                           model_name=f'best_model_batch{model_idx}.h5',\n                                           reload=True),\n                       reduce_lr,\n                       early_stop\n                      ]\n        )\n        optimizer = SGD(lr=0.01,clipnorm=1.1)\n        model.compile(optimizer=optimizer, loss=custom_loss, metrics=[bce,logcosh])\n        history = model.fit(\n            X_tr, y_tr, \n            epochs=50, batch_size=64, \n            validation_data=(X_vl, y_vl), \n            verbose=True, \n            callbacks=[SpearmanRhoCallback(training_data=(X_tr, y_tr), \n                                           validation_data=(X_vl, y_vl),\n                                           patience=10, \n                                           model_name=f'best_model_batch_sgd{model_idx}.h5'),\n                       reduce_lr,\n                       early_stop   \n                      ]\n        )\n        if min(history.history['val_bce']) < 0.375:\n            all_predictions.append(model.predict(X_test))\n\n    optimizer = Adam(lr=2e-4,clipnorm=1.9)\n    model.compile(optimizer=optimizer, loss=bce)\n    model.fit(\n        X_train, y_train, \n        epochs=30, batch_size=128, \n        validation_data=(X_train, y_train), \n        verbose=True, \n        callbacks=[SpearmanRhoCallback(training_data=(X_train, y_train), \n                                       validation_data=(X_train, y_train),\n                                       patience=10, \n                                       model_name=f'best_model_batchoverfit{model_idx}.h5',\n                                       reload=False),\n                   reduce_lr,\n                   early_stop\n                  ]\n    )\n    all_predictions.append(model.predict(X_test))\n","1a1448ed":"all_predictions = []\nwhile len(all_predictions) < 20:\n    run_model()\n# run_model()","d7dcec6f":"len(all_predictions)","d73ae421":"# This is slow...\n# from sklearn.linear_model import MultiTaskElasticNet\n# \n# kf = KFold(n_splits=5, random_state=2019, shuffle=True)\n# for ind, (tr, val) in enumerate(kf.split(X_train)):\n#     X_tr = X_train[tr]\n#     y_tr = y_train[tr]\n#     X_vl = X_train[val]\n#     y_vl = y_train[val]\n    \n#     model = MultiTaskElasticNet(alpha=0.001, random_state=42, l1_ratio=0.5)\n#     model.fit(X_tr, y_tr)\n#     all_predictions.append(model.predict(X_test))\n    \n# model = MultiTaskElasticNet(alpha=0.001, random_state=42, l1_ratio=0.5)\n# model.fit(X_train, y_train)\n# all_predictions.append(model.predict(X_test))","5e34e2b0":"K.clear_session()\ngc.collect()","43f133e8":"# Catboost didn't work well\n# # clear session and clear cuda memory for catboost\n# K.clear_session()\n# gc.collect()\n\n# from numba import cuda;\n# cuda.select_device(0);\n# cuda.close()\n","74b0bec6":"# kf = KFold(n_splits=3, random_state=42, shuffle=True)\n# from xgboost import XGBClassifier\n# from sklearn.multiclass import OneVsRestClassifier\n# from sklearn.multioutput import MultiOutputRegressor\n# # all_predictions = []\n# for ind, (tr, val) in enumerate(kf.split(X_train)):\n#     X_tr = X_train[tr]\n#     y_tr = y_train[tr]\n#     X_vl = X_train[val]\n#     y_vl = y_train[val]\n\n#     print(X_tr.shape)\n#     print(y_tr.shape)\n\n\n    \n#     gs1 = MultiOutputRegressor(\n#         cb.CatBoostRegressor(iterations=400,\n#                             learning_rate=0.7,\n#                             depth=9,\n#                             verbose = 20,\n#                             task_type = 'GPU',\n#                             loss_function = 'MAE')\n        \n#     )\n#     gs1.fit(X_tr, y_tr);    \n\n#     all_predictions.append(gs1.predict(X_test))\n    \n","31442ab8":"test_preds = np.array([np.array([rankdata(c) for c in p.T]).T for p in all_predictions]).mean(axis=0)\nmax_val = test_preds.max() + 1\ntest_preds = test_preds\/max_val + 1e-12","0138e7d8":"submission = pd.read_csv(INPUT_PATH+'google-quest-challenge\/sample_submission.csv')\nsubmission[targets] = test_preds\nsubmission.head(20)","3d7fccbd":"submission.to_csv(\"submission.csv\", index = False)\n","89b5594b":"This is fork of https:\/\/www.kaggle.com\/ldm314\/quest-encoding-ensemble. Please upvote the original author and Kernel."}}