{"cell_type":{"04b586d7":"code","4d539a1e":"code","af5c0c4c":"code","913dd0d9":"code","76e680db":"code","10fd5eac":"code","6a611e5f":"code","0b47dc8c":"code","aa2eafbb":"code","43a51b77":"code","f1545999":"code","4a9d246d":"code","1b754d32":"code","6ec6f46b":"code","67160683":"code","88bda2c1":"code","dc52dc38":"code","3079ff50":"code","70112503":"code","69b14d4c":"code","e52261c7":"code","f9fd083f":"code","89817cb8":"code","63490e77":"code","f09a449f":"code","a8565086":"code","28f3dac2":"code","cd4450f2":"code","5d810257":"code","479eaa7a":"code","e019c213":"code","98418731":"code","b2fdc293":"code","fe7cf14d":"code","cbe1e5a3":"code","f9e94692":"code","d406e398":"markdown","a708d316":"markdown","1cd9e663":"markdown","9f096f99":"markdown","9217fed3":"markdown","decee699":"markdown","0b25b3d5":"markdown"},"source":{"04b586d7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4d539a1e":"HOUSING_PATH = \"\/kaggle\/input\/california-housing-prices\/\"\ndef load_housing_data(housing_path=HOUSING_PATH):\n    csv_path = os.path.join(housing_path, \"housing.csv\")\n    return pd.read_csv(csv_path)\nhousing_data = load_housing_data()","af5c0c4c":"housing_data.head()","913dd0d9":"housing_data.info()","76e680db":"housing_data[\"ocean_proximity\"].value_counts()","10fd5eac":"housing_data.describe()","6a611e5f":"import matplotlib.pyplot as plt\nhousing_data.hist(bins=50, figsize=(20,15))\nplt.show()","0b47dc8c":"def split_train_test(data, test_ratio):\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]\ntrain_set, test_set = split_train_test(housing_data, 0.2)","aa2eafbb":"train_set.info()","43a51b77":"from zlib import crc32\n\ndef test_set_check(identifier, test_ratio):\n    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n\ndef split_train_test_by_id(data, test_ratio, id_column):\n    ids = data[id_column]\n    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n    return data.loc[~in_test_set], data.loc[in_test_set]\n\n#using row id as identifier\nhousing_with_id = housing_data.reset_index()   # adds an `index` column\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")\n\n#creating own identifier with stable& unique values\nhousing_with_id[\"id\"] = housing_data[\"longitude\"] * 1000 + housing_data[\"latitude\"]\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")","f1545999":"from sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(housing_data, test_size=0.2, random_state=42)","4a9d246d":"housing_data[\"income_cat\"] = pd.cut(housing_data[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])\nhousing_data[\"income_cat\"].hist()","1b754d32":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing_data, housing_data[\"income_cat\"]):\n    strat_train_set = housing_data.loc[train_index]\n    strat_test_set = housing_data.loc[test_index]","6ec6f46b":"strat_train_set","67160683":"for set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)","88bda2c1":"housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\nhousing_labels = strat_train_set[\"median_house_value\"].copy()","dc52dc38":"\nhousing = strat_train_set.copy()\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\n\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n    s=housing[\"population\"]\/100, label=\"population\", figsize=(10,7),\n    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n)\nplt.legend()","3079ff50":"The correlation coefficient only measures linear correlations (\u201cif x goes up, then y generally goes up\/down\u201d). It may completely miss out on nonlinear relationships (e.g., \u201cif x is close to 0, then y generally goes up\u201d). Note how all the plots of the bottom row have a correlation coefficient equal to 0, despite the fact that their axes are clearly not independent: these are examples of nonlinear relationships. Also, the second row shows examples where the correlation coefficient is equal to 1 or \u20131; notice that this has nothing to do with the slope. For example, your height in inches has a correlation coefficient of 1 with your height in feet or in nanometers","70112503":"housing[\"rooms_per_household\"] = housing[\"total_rooms\"]\/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]\/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]\/housing[\"households\"]\ncorr_matrix = housing.corr()\ncorr_matrix\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(10,10)) \ndataplot = sb.heatmap(corr_matrix, cmap=\"YlGnBu\", annot=True,)\nplt.show()","69b14d4c":"from pandas.plotting import scatter_matrix\n\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n              \"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize=(12, 8))","e52261c7":"housing.plot(kind=\"scatter\", x=\"median_income\", y=\"housing_median_age\",\n             alpha=0.1)","f9fd083f":"from sklearn.impute import SimpleImputer\nhousing.isna().total_bedrooms\n\nimputer = SimpleImputer(strategy=\"median\")\nhousing_num = housing.drop(\"ocean_proximity\", axis=1)\nimputer.fit(housing_num)\nX = imputer.transform(housing_num)\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns,\n                          index=housing_num.index)","89817cb8":"from sklearn.preprocessing import OneHotEncoder\ncat_encoder = OneHotEncoder()\nhousing_cat = housing[[\"ocean_proximity\"]]\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)","63490e77":"from sklearn.base import BaseEstimator, TransformerMixin\n\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X):\n        rooms_per_household = X[:, rooms_ix] \/ X[:, households_ix]\n        population_per_household = X[:, population_ix] \/ X[:, households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] \/ X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,\n                         bedrooms_per_room]\n\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)","f09a449f":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])\n\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)\n\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs),\n    ])\n\nhousing_prepared = full_pipeline.fit_transform(housing)\n","a8565086":"housing.info()","28f3dac2":"housing_data = pd.DataFrame(housing_prepared)\nhousing_data.info()\nhousing_data.describe()","cd4450f2":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)\n","5d810257":"import joblib\n\njoblib.dump(lin_reg, \"my_model.pkl\")\n# and later...\nmy_model_loaded = joblib.load(\"my_model.pkl\")","479eaa7a":"my_model_loaded","e019c213":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)\n\ntree_rmse_scores","98418731":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nparam_grid = [\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\nforest_reg = RandomForestRegressor()\n\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\n\ngrid_search.fit(housing_prepared, housing_labels)\nprint(grid_search.best_params_)\nprint(grid_search.best_estimator_)","b2fdc293":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)\n","fe7cf14d":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances\nextra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\ncat_encoder = full_pipeline.named_transformers_[\"cat\"]\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)","cbe1e5a3":"final_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\n\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)   # => evaluates to 47,730.2","f9e94692":"final_rmse","d406e398":"Since the dataset is not too large, you can easily compute the standard correlation coefficient (also called Pearson\u2019s r) between every pair of attributes using the corr() method:\n\n","a708d316":"**Grid Search**","1cd9e663":"****Exploratin of data****","9f096f99":"**Feature Scaling**  \nThere are two common ways to get all attributes to have the same scale: min-max scaling and standardization.  \nstandardization is much less affected by outliers. For example, suppose a district had a median income equal to 100 (by mistake).  \nMin-max scaling would then crush all the other values from 0\u201315 down to 0\u20130.15, whereas standardization would not be much affected. Scikit-Learn provides a transformer called StandardScaler for standardization.","9217fed3":"**Custom Transformers**","decee699":"**to save model**","0b25b3d5":"But both these solutions will break the next time you fetch an updated dataset. To have a stable train\/test split even after updating the dataset, a common solution is to use each instance\u2019s identifier to decide whether or not it should go in the test set (assuming instances have a unique and immutable identifier). For example, you could compute a hash of each instance\u2019s identifier and put that instance in the test set if the hash is lower than or equal to 20% of the maximum hash value. This ensures that the test set will remain consistent across multiple runs, even if you refresh the dataset. The new test set will contain 20% of the new instances, but it will not contain any instance that was previously in the training set."}}