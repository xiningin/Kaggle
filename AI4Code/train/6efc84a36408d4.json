{"cell_type":{"74728546":"code","fc2f7de4":"code","c4e36072":"code","b8d54516":"code","41f61409":"code","37ee72d5":"code","73b66492":"code","13ad2b94":"code","6d2fd78f":"code","a1881672":"code","ba5bd5e3":"code","c01e8840":"code","caf73c66":"code","16efe72c":"code","39be944e":"code","00f2c65c":"markdown","52adcbbd":"markdown","eda3b2c1":"markdown","4301cf79":"markdown","47da0261":"markdown","aafa7e1c":"markdown","fc5063bb":"markdown","263f7823":"markdown"},"source":{"74728546":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import f1_score","fc2f7de4":"train=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","c4e36072":"train.head()","b8d54516":"test.head()","41f61409":"print(train.shape)\nprint(test.shape)","37ee72d5":"print(train.info())\nprint(test.info())","73b66492":"train.target.value_counts()","13ad2b94":"import nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nimport re\n!pip install contractions\nimport contractions\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\n!pip install pyspellchecker\nfrom spellchecker import SpellChecker","6d2fd78f":"stop_words=nltk.corpus.stopwords.words('english')\ni=0\n#sc=SpellChecker()\n#data=pd.concat([train,test])\nwnl=WordNetLemmatizer()\nstemmer=SnowballStemmer('english')\nfor doc in train.text:\n    doc=re.sub(r'https?:\/\/\\S+|www\\.\\S+','',doc)\n    doc=re.sub(r'<.*?>','',doc)\n    doc=re.sub(r'[^a-zA-Z\\s]','',doc,re.I|re.A)\n    #doc=' '.join([stemmer.stem(i) for i in doc.lower().split()])\n    doc=' '.join([wnl.lemmatize(i) for i in doc.lower().split()])\n    #doc=' '.join([sc.correction(i) for i in doc.split()])\n    doc=contractions.fix(doc)\n    tokens=nltk.word_tokenize(doc)\n    filtered=[token for token in tokens if token not in stop_words]\n    doc=' '.join(filtered)\n    train.text[i]=doc\n    i+=1\ni=0\nfor doc in test.text:\n    doc=re.sub(r'https?:\/\/\\S+|www\\.\\S+','',doc)\n    doc=re.sub(r'<.*?>','',doc)\n    doc=re.sub(r'[^a-zA-Z\\s]','',doc,re.I|re.A)\n    #doc=' '.join([stemmer.stem(i) for i in doc.lower().split()])\n    doc=' '.join([wnl.lemmatize(i) for i in doc.lower().split()])\n    #doc=' '.join([sc.correction(i) for i in doc.split()])\n    doc=contractions.fix(doc)\n    tokens=nltk.word_tokenize(doc)\n    filtered=[token for token in tokens if token not in stop_words]\n    doc=' '.join(filtered)\n    test.text[i]=doc\n    i+=1","a1881672":"train.head()","ba5bd5e3":"test.head()","c01e8840":"from sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer(ngram_range=(1,1)) \n\n#    ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, \n#    and (2, 2) means only bigrams.\n\ncv_matrix=cv.fit_transform(train.text).toarray()\ntrain_df=pd.DataFrame(cv_matrix,columns=cv.get_feature_names())\ntest_df=pd.DataFrame(cv.transform(test.text).toarray(),columns=cv.get_feature_names())\ntrain_df.head()","caf73c66":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf=TfidfVectorizer(ngram_range=(1,1),use_idf=True)\nmat=tfidf.fit_transform(train.text).toarray()\ntrain_df=pd.DataFrame(mat,columns=tfidf.get_feature_names())\ntest_df=pd.DataFrame(tfidf.transform(test.text).toarray(),columns=tfidf.get_feature_names())\ntrain_df.head()","16efe72c":"from sklearn.linear_model import LogisticRegression\nmodel=LogisticRegression()\nmodel.fit(train_df,train.target)\nprint(f1_score(model.predict(train_df),train.target))\npred=model.predict(test_df)","39be944e":"pd.DataFrame({\n    'id':test.id,\n    'target':pred\n}).to_csv('submission.csv',index=False)","00f2c65c":"> ***In this notebook we will see how to build Bag of words\/N-grams model using CountVectorizer and Tf-idf model using TfidfVectorizer from scikit-learn library.***","52adcbbd":"> ***There are null values in the keyword and location columns but as you will see I haven't used those columns for model building as they don't seem to be necessary.***","eda3b2c1":"**We can try out one of BOW,bag of n-grams or tfidf models and use those in model building.**","4301cf79":">  * **Removing URL tags such as www. and https.**\n>  * **Removing HTML tags**\n>  * **Removing all other noise except alphabets such as emojis etc**\n>  * **Lemmatizing each word (Can also use stemming and spell checking if required)**\n>  * **Removing stop words if there are any.**","47da0261":"# Bag of words\/N-grams and Tf-idf models with simple logistic regression can get you to a score of 0.80","aafa7e1c":"**I haven't divided the dataset to train and test sets to validate and directly trained the model on the whole dataset.**","fc5063bb":"> ***Loading necessary modules for cleaning text***","263f7823":"**do upvote if you find this helpful and comment if there are any doubts.**\n\nHere is my next notebook: [NLP disaster tweets-Glove,Word2Vec & BiLSTM](https:\/\/www.kaggle.com\/urstrulysai\/nlp-disaster-tweets-glove-word2vec-bilstm)"}}