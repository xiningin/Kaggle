{"cell_type":{"e48a960c":"code","74a67f2e":"code","50de1ca1":"code","c7a0b651":"code","164569c5":"code","e6481fdd":"code","76b737b5":"code","e30a617c":"code","7f5aa798":"code","e4f6c334":"code","927c77db":"code","22e8d316":"code","11c47921":"code","d7de8e6d":"code","fe8ce030":"code","96ad255f":"code","a6c355b2":"code","4c3a70da":"code","2905c11c":"code","43add6b7":"code","45744928":"code","48be4d23":"code","6220d3e6":"code","aed29b84":"code","592adaf7":"code","05ddad91":"code","a0ac846e":"code","8437d966":"code","eb13fe3d":"code","1bcf8962":"code","9954bac8":"code","61c35306":"code","4550a7ec":"code","b02c23dc":"code","514c633a":"code","803f73b5":"markdown","579a7117":"markdown","facbcbec":"markdown","6eb2928e":"markdown","2db05a90":"markdown","c55fce18":"markdown","870ac4de":"markdown","4505f10d":"markdown","994391b1":"markdown","ac06dd5a":"markdown","982f2caf":"markdown","66d045fb":"markdown","1549d2af":"markdown","6877adbd":"markdown","214289fc":"markdown","e7027291":"markdown"},"source":{"e48a960c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","74a67f2e":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsubmission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\ntrain\nprint('Train:{}   Test:{}'.format(train.shape,test.shape))","50de1ca1":"train.describe()","c7a0b651":"train['SalePrice'].describe()","164569c5":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[cols], height = 2)\nplt.show();","e6481fdd":"# To further know the highly correlated values checking.","76b737b5":"corrmat = train.corr()\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.0f', annot_kws={'size': 20}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","e30a617c":"sns.distplot(train['SalePrice']);","7f5aa798":"print(\"Skewness: %f\" % train['SalePrice'].skew())","e4f6c334":"train['OverallQual'].describe()","927c77db":"overallquality = pd.concat([train['SalePrice'], train['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x='OverallQual', y=\"SalePrice\", data=overallquality)\nfig.axis(ymin=0, ymax=800000);","22e8d316":"plt.scatter(train['OverallQual'], train['SalePrice']);","11c47921":"YearBuilt = pd.concat([train['SalePrice'], train['YearBuilt']], axis=1)\nf, ax = plt.subplots(figsize=(30, 8))\nfig = sns.boxplot(x='YearBuilt', y=\"SalePrice\", data=YearBuilt)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","d7de8e6d":"total = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)[:25]\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","fe8ce030":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=percent.index, y=percent)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","96ad255f":"# my_df.fillna(0) # This is a alternative to fill null values with zeros\n\ndf_dropped = train.drop(['Alley','PoolQC','Fence','MiscFeature'],axis=1)","a6c355b2":"total2 = df_dropped.isnull().sum().sort_values(ascending=False)\npercent2 = (df_dropped.isnull().sum()\/df_dropped.isnull().count()).sort_values(ascending=False)[:25]\nf, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=percent2.index, y=percent2)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","4c3a70da":"object_columns_df = train.select_dtypes(include=['object'])\nnumerical_columns_df =train.select_dtypes(exclude=['object'])","2905c11c":"object_columns_df.dtypes","43add6b7":"numerical_columns_df.dtypes","45744928":"# drop missing values\nmissing = test.isnull().sum()\nmissing = missing[missing>0]\ntrain.drop(missing.index, axis=1, inplace=True)\n\ntest.dropna(axis=1, inplace=True)","48be4d23":"def exam_data_load(df, target, id_name=\"\", null_name=\"\"):\n    if id_name == \"\":\n        df = df.reset_index().rename(columns={\"index\": \"id\"})\n        id_name = 'id'\n    else:\n        id_name = id_name\n    \n    if null_name != \"\":\n        df[df == null_name] = np.nan\n    \n    X_train, X_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=2021)\n    y_train = X_train[[id_name, target]]\n    X_train = X_train.drop(columns=[id_name, target])\n    y_test = X_test[[id_name, target]]\n    X_test = X_test.drop(columns=[id_name, target])\n    return X_train, X_test, y_train, y_test ","6220d3e6":"X_train, X_test, y_train, y_test = exam_data_load(train, target='SalePrice', id_name='Id')\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","aed29b84":"X_train.corr() ","592adaf7":"X_test.corr()","05ddad91":"num_cols = X_train.select_dtypes(exclude='object').columns\nnum_cols\nX_train[num_cols] = X_train[num_cols].fillna(X_train[num_cols].mean)\nX_test[num_cols] = X_test[num_cols].fillna(X_test[num_cols].mean)\nX_test.isna().sum()\ndel_cols =  X_train.select_dtypes(include='object').columns\nX_train = X_train.drop(columns=del_cols)\nX_test = X_test.drop(columns=del_cols)\nX_train.shape, X_test.shape","a0ac846e":"y_train['SalePrice'].sort_values(ascending=True).value_counts()\ny_train['SalePrice'].hist()\ny_train['SalePrice'] = np.log1p(y_train['SalePrice'])\ny_train['SalePrice'].sort_values(ascending=True).value_counts()\ny_train['SalePrice'].hist()","8437d966":"y_train = y_train['SalePrice']\nX_test","eb13fe3d":"scaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled =  scaler.transform(X_test)","1bcf8962":"X_tr, X_val, y_tr, y_val = train_test_split(X_train_scaled, y_train, test_size=0.15, shuffle=True, random_state=2021)\nX_tr.shape, X_val.shape, y_tr.shape, y_val.shape","9954bac8":"model_rf = RandomForestRegressor(n_estimators=100, max_depth=100)\nmodel_rf.fit(X_tr, y_tr)\npred_rf = model_rf.predict(X_val)\nprint('model_rf : ', np.sqrt(mean_squared_error(pred_rf, y_val)))","61c35306":"pred_result = model_rf.predict(X_test_scaled)","4550a7ec":"pred_result","b02c23dc":"final = pd.DataFrame({'Id': y_test['Id'], 'SalePrice': pred_result})\nfinal","514c633a":"final.to_csv('submission.csv', index=False)","803f73b5":"# Rechecking the null values","579a7117":"# How the prices react based on the quality of the property","facbcbec":"# Segregating Categorical and Numerical data","6eb2928e":"# Checking for any null or zero values on Sales Price","2db05a90":"#### Removing the top 4 columns as they dont add any significance to the plot","c55fce18":"# Data preprocessing : Checking which column has highest missing values","870ac4de":"Most of the data points are lying near the 200k mark","4505f10d":"# Reference \nFrom Matplotlib, Seaborn documentations <br>\nStack Overflow for quick queries<br>\nYoutube for better understanding concepts<br>","994391b1":"# Importing the data from CSV file","ac06dd5a":"# Categorical Features","982f2caf":"# How the prices react based on the year built of the property","66d045fb":"# Pair Plot to better understand different attributes and their relation\nPlotting this at the beginning gives the initial glimpse of the data","1549d2af":"# Here we are able to see the positive skewness\n      Mode < Median < Mean","6877adbd":"# Checking the data distribution based on the Sales Price","214289fc":"# Numerical Features","e7027291":"# Plot based on count of null values"}}