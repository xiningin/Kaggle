{"cell_type":{"d73452ec":"code","89c7133e":"code","bb38e43e":"code","5ffedc72":"code","d44b8b30":"code","9eaa7dcb":"code","fd8d2b27":"code","2109bdaf":"code","2382eb26":"code","c948c8e5":"code","c249c449":"code","33c1d05f":"code","479f55bd":"code","1471ef2e":"code","362ab465":"code","69e2ba27":"code","20041412":"code","8e1fa8d1":"code","fd76d76e":"code","eb934269":"code","a616ba89":"code","ac1fcc02":"code","02ec63b2":"code","c2b63483":"code","3311d361":"code","f2765358":"code","c39a7ffc":"code","b8e8d630":"code","0eeea0e1":"code","b193beef":"code","ba4b5497":"markdown","c51d2666":"markdown","b565dcb0":"markdown","2b620ce6":"markdown","b295200d":"markdown","330e027b":"markdown","64484788":"markdown","73def2a9":"markdown","a5c26069":"markdown","45ee53ff":"markdown","61b19359":"markdown","b6a88479":"markdown","55182fd1":"markdown","df1fdb23":"markdown","b2e249ed":"markdown","8c795eb8":"markdown","2064697f":"markdown","8e663dd4":"markdown","c21df266":"markdown","3a1610c0":"markdown","d824344d":"markdown","931fecca":"markdown","e7fb0494":"markdown","e916abf6":"markdown","ecf66ef2":"markdown"},"source":{"d73452ec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\nimport tensorflow as tf\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nimport sklearn.model_selection as sk\n\nimport plotly.express as px\n\nimport re\n\n# Input data files are available in the \"..\/input\/\" directory.\nimport os\nprint(\"Input files:\")\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n# For neural nets with my GPU, RNN doesn't work without this in TF 2.0\nfrom tensorflow.compat.v1 import ConfigProto\nfrom tensorflow.compat.v1 import InteractiveSession\nconfig = ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = InteractiveSession(config=config)\n\nprint()\nprint(\"TF Version: \", tf.__version__)\nprint(\"Eager mode: \", tf.executing_eagerly())\nprint(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")\n\nif tf.test.gpu_device_name():\n    print('GPU found')\nelse:\n    print(\"No GPU found\")","89c7133e":"path_to_file = '\/kaggle\/input\/wine-reviews\/winemag-data-130k-v2.csv'\n\ndfWine = pd.read_csv(path_to_file, index_col=0)","bb38e43e":"dfWine.info()\nprint()\nprint(dfWine.shape)\nprint(dfWine.columns)","5ffedc72":"dfWine.head(5)","d44b8b30":"dfWine.describe()","9eaa7dcb":"# # Removes the Twitter handles, that doesn't matter here\ndfWine = dfWine.drop(['taster_twitter_handle'], axis=1)\ndfWine.head(5)","fd8d2b27":"# Read title and find vintage\nyearSearch = []    \nfor value in dfWine['title']:\n    regexresult = re.search(r'19\\d{2}|20\\d{2}', value)\n    if regexresult:\n        yearSearch.append(regexresult.group())\n    else: yearSearch.append(None)\n\ndfWine['year'] = yearSearch\n\n#Tell me which ones don't have a year listed\nprint(\"We extracted %d years from the wine titles and %d did not have a year.\" %(len(dfWine[dfWine['year'].notna()]), len(dfWine[dfWine['year'].isna()].index)))\ndfWine['year'].describe()","2109bdaf":"#If we're missing year values, remove the row\ndfWine_goodyears=dfWine\ndfWine_goodyears=dfWine_goodyears.dropna(subset=['year'])\nprint('Removed ' + str(dfWine.shape[0]-dfWine_goodyears.shape[0]) + ' rows with empty year values.' + \"\\n\")\n\ndfWine_goodyears['year']=dfWine_goodyears['year'].astype(int)\n# dfWine_goodyears['year']=pd.to_numeric(dfWine_goodyears['year'], downcast='integer', errors='coerce')\n\nprint(dfWine_goodyears['year'].describe())\n\ndfWineYear = dfWine_goodyears.groupby(['year']).mean()\ndfWineYear = pd.DataFrame(data=dfWineYear).reset_index()","2382eb26":"dfWine = dfWine.replace({'country': r'USA?'}, {'country': 'United States of America'}, regex=True)\n\n# #For ISO-3 codes of countries, for mapping\ndfcountry = pd.read_csv('\/kaggle\/input\/country-names-mapping-to-iso3\/countryMap.txt',sep='\\t')\ndfWine = dfWine.merge(dfcountry, on='country')\n\n# #For ISO-3 codes of states, if needed\nstate_codes = {\n    'District of Columbia' : 'DC','Mississippi': 'MS', 'Oklahoma': 'OK', \n    'Delaware': 'DE', 'Minnesota': 'MN', 'Illinois': 'IL', 'Arkansas': 'AR', \n    'New Mexico': 'NM', 'Indiana': 'IN', 'Maryland': 'MD', 'Louisiana': 'LA', \n    'Idaho': 'ID', 'Wyoming': 'WY', 'Tennessee': 'TN', 'Arizona': 'AZ', \n    'Iowa': 'IA', 'Michigan': 'MI', 'Kansas': 'KS', 'Utah': 'UT', \n    'Virginia': 'VA', 'Oregon': 'OR', 'Connecticut': 'CT', 'Montana': 'MT', \n    'California': 'CA', 'Massachusetts': 'MA', 'West Virginia': 'WV', \n    'South Carolina': 'SC', 'New Hampshire': 'NH', 'Wisconsin': 'WI',\n    'Vermont': 'VT', 'Georgia': 'GA', 'North Dakota': 'ND', \n    'Pennsylvania': 'PA', 'Florida': 'FL', 'Alaska': 'AK', 'Kentucky': 'KY', \n    'Hawaii': 'HI', 'Nebraska': 'NE', 'Missouri': 'MO', 'Ohio': 'OH', \n    'Alabama': 'AL', 'Rhode Island': 'RI', 'South Dakota': 'SD', \n    'Colorado': 'CO', 'New Jersey': 'NJ', 'Washington': 'WA', \n    'North Carolina': 'NC', 'New York': 'NY', 'Texas': 'TX', \n    'Nevada': 'NV', 'Maine': 'ME'}\n\ndfWine['state_code'] = dfWine['province'].apply(lambda x : state_codes[x] if x in state_codes.keys() else None)","c948c8e5":"# dfWine['country-3let'].value_counts()\n# dfWine[dfWine.country==\"United States of America\"][dfWine['state_code'].isna()]\n\nprint(\"%d did not get a country code and %d US wines did not get a state code.\" \n      %((len(dfWine[dfWine.country==\"\"])), \n        len(dfWine[dfWine.country==\"United States of America\"][dfWine['state_code'].isna()])))","c249c449":"# Get label frequencies in descending order\nlabel_freq = dfWine['variety'].apply(lambda s: str(s)).explode().value_counts().sort_values(ascending=False)\n\n# Bar plot\nstyle.use(\"fivethirtyeight\")\nplt.figure(figsize=(12,10))\nsns.barplot(y=label_freq.index.values, x=label_freq, order=label_freq.iloc[:15].index)\nplt.title(\"Grape frequency\", fontsize=14)\nplt.xlabel(\"\")\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()","33c1d05f":"dfWineClassifier = dfWine[[ 'description', 'year', 'variety', 'country-3let', 'province' ]]\n\n# Tell us where we have missing or NaN values (isnull or isna):\nprint(dfWineClassifier.isnull().sum())\nprint()\n\n#Tell me which ones don't have a variety listed\nprint(\"Missing entries: %d\" %(dfWineClassifier[dfWineClassifier['variety'].isna()].index[0]))\nprint(dfWineClassifier[dfWineClassifier['variety'].isna()].head(10))\nprint()\n\n# pd.DataFrame(dfWineClassifier.variety.unique()).values\n\n#If we're missing important values, remove the row\ndfWineClassifier=dfWineClassifier.dropna(subset=['description', 'variety'])\nprint('Removed ' + str(dfWine.shape[0]-dfWineClassifier.shape[0]) + ' rows with empty values.' + \"\\n\")","479f55bd":"RARE_CUTOFF = 700 # It must have this many examples of the grape variety, otherwise it's \"other.\"\n\n# Create a list of rare labels\nrare = list(label_freq[label_freq<RARE_CUTOFF].index)\n# print(\"We will be ignoring these rare labels: \\n\", rare)\n\n\n# Transform the rare ones to just \"Other\"\ndfWineClassifier['variety'] = dfWineClassifier['variety'].apply(lambda s: str(s) if s not in rare else 'Other')\n\nlabel_words = list(label_freq[label_freq>=RARE_CUTOFF].index)\nlabel_words.append('Other')\nprint(label_words)\n\nnum_labels = len(label_words)\nprint(\"\\n\"  + str(num_labels) + \" different categories.\")\n\n# pd.DataFrame(dfWineClassifier.variety.unique()).values","1471ef2e":"for i in range(1,5):\n    print(dfWineClassifier['variety'].iloc[i])\n    print(dfWineClassifier['description'].iloc[i])\n    print()","362ab465":"# length of dictionary\nNUM_WORDS = 4000\n\n# Length of each review\nSEQ_LEN = 256\n\n#create tokenizer for our data\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=NUM_WORDS, oov_token='<UNK>')\ntokenizer.fit_on_texts(dfWineClassifier['description'])\n\n#convert text data to numerical indexes\nwine_seqs=tokenizer.texts_to_sequences(dfWineClassifier['description'])\n\n#pad data up to SEQ_LEN (note that we truncate if there are more than SEQ_LEN tokens)\nwine_seqs=tf.keras.preprocessing.sequence.pad_sequences(wine_seqs, maxlen=SEQ_LEN, padding=\"post\")\n\nprint(wine_seqs)","69e2ba27":"# tokenizer.word_index","20041412":"# Creating a mapping from unique words to indices\n# char2idx = {u:i for i, u in enumerate(label_words)}\n# print(char2idx)\n# idx2char = np.array(label_words)\n# print(idx2char)\n\n# print(str(len(idx2char)) + ' wine styles.')\n\nwine_labels=pd.DataFrame({'variety': dfWineClassifier['variety']})\n# wine_labels=wine_labels.replace({'variety' : char2idx})\nwine_labels=wine_labels.replace(' ', '_', regex=True)\n\nwine_labels_list = []\nfor item in wine_labels['variety']:\n    wine_labels_list.append(str(item))\n\nlabel_tokenizer = tf.keras.preprocessing.text.Tokenizer(split=' ', filters='!\"#$%&()*+,.\/:;<=>?@[\\\\]^`{|}~\\t\\n')\nlabel_tokenizer.fit_on_texts(wine_labels_list)\n\nprint(len(label_words))\nprint(label_tokenizer.word_index)\n\nwine_label_seq = np.array(label_tokenizer.texts_to_sequences(wine_labels_list))\nwine_label_seq.shape","8e1fa8d1":"reverse_word_index = dict([(value, key) for (key, value) in tokenizer.word_index.items()])\n\ndef decode_article(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n\n\nreverse_label_index = dict([(value, key) for (key, value) in label_tokenizer.word_index.items()])\n\ndef decode_label(text):\n    return ' '.join([reverse_label_index.get(i, '?') for i in text])\n","fd76d76e":"# Demonstrate what the input looks like, how it gets encoded.\n\ntest_entry=3\n\nprint(decode_article(wine_seqs[test_entry]))\nprint('---')\nprint(wine_seqs[test_entry])\n\nprint(decode_label(wine_label_seq[test_entry]))\nprint('---')\nprint(wine_label_seq[test_entry])\n","eb934269":"# Divide into two\nX_train, X_test, y_train, y_test = sk.train_test_split(wine_seqs,\n                                                    wine_label_seq,\n                                                    test_size=0.20,\n                                                    random_state=42)\n\nprint('Test: ' + str(len(X_test)) + ' Train: ' + str(len(X_train)))\n\nX_train = X_train.astype(\"float32\")\nX_test = X_test.astype(\"float32\")\ny_train = y_train.astype(\"float32\")\ny_test = y_test.astype(\"float32\")\n\nprint(type(X_train), X_train.shape)\n\n# X_train = X_train \/ 1024.0\n# X_test = X_test \/ 1024.0\n# y_train = y_train \/ 1024.0\n# y_test = y_test \/ 1024.0\n\nprint(X_train.shape)\nprint(y_train.shape)\n","a616ba89":"#https:\/\/towardsdatascience.com\/multi-label-image-classification-in-tensorflow-2-0-7d4cf8a4bc72","ac1fcc02":"EMBEDDING_SIZE = 256\nEMBEDDING_SIZE_2 = 64\nEMBEDDING_SIZE_3 = (num_labels+1)\nBATCH_SIZE = 512  # This can really speed things up\nEPOCHS = 10\nLR = 1e-5  # Keep it small when transfer learning\n\n# model = tf.keras.Sequential([\n#     tf.keras.layers.Embedding(NUM_WORDS, EMBEDDING_SIZE),\n#     tf.keras.layers.GlobalAveragePooling1D(),\n#     tf.keras.layers.Dense(1, activation='relu', name='output')])\n# #    tf.keras.layers.Dense(1, activation='sigmoid')])\n# #    tf.keras.layers.Dense(len(idx2char), activation='relu', name='hidden_layer')])\n\n# https:\/\/towardsdatascience.com\/multi-class-text-classification-with-lstm-using-tensorflow-2-0-d88627c10a35\nmodel = tf.keras.Sequential([\n    \n    # Add an Embedding layer expecting input vocab of a given size, and output embedding dimension of fized size we set at the top\n    tf.keras.layers.Embedding(NUM_WORDS, EMBEDDING_SIZE),\n#     tf.keras.layers.Embedding(input_dim=NUM_WORDS, \n#                            output_dim=EMBEDDING_SIZE, \n#                            input_length=SEQ_LEN), \n    \n#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(EMBEDDING_SIZE)),\n    tf.keras.layers.Conv1D(128, 5, activation='relu'), \n    tf.keras.layers.GlobalMaxPooling1D(), \n    \n    # use ReLU in place of tanh function since they are very good alternatives of each other.\n    tf.keras.layers.Dense(EMBEDDING_SIZE_2, activation='relu'),\n    \n    # Add a Dense layer with additional units and softmax activation.\n    # When we have multiple outputs, softmax convert outputs layers into a probability distribution.\n    tf.keras.layers.Dense(EMBEDDING_SIZE_3, activation='softmax')\n])\n\nmodel.summary()\n\nmodel.compile(optimizer='adam',\n#                optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n#               loss='binary_crossentropy',\n              loss='sparse_categorical_crossentropy',\n#               loss=tf.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n","02ec63b2":"# Directory where the checkpoints will be saved\ncheckpoint_dir = '.\/checkpoints\/classifer_training_checkpoints'\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    monitor='accuracy',\n    save_best_only=True,\n    mode='auto',\n    save_weights_only=True)\n\nhistory = model.fit(X_train, y_train,\n                    batch_size=BATCH_SIZE,\n                    epochs=EPOCHS,\n                    validation_data=(X_test, y_test),\n                    validation_steps=30,\n                   callbacks=[checkpoint_callback])\n\n# es = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max')\n# callbacks=[es]\n# history = model.fit(X_train, y_train,\n#                     batch_size=BATCH_SIZE,\n#                     epochs=EPOCHS,\n#                     validation_data=(X_test, y_test),\n#                     callbacks=callbacks)\n\nloss, accuracy = model.evaluate(X_test, y_test)\n\nprint(\"Loss: \", loss)\nprint(\"Accuracy: \", accuracy)","c2b63483":"tf.train.latest_checkpoint(checkpoint_dir)\n\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n\nmodel.build(tf.TensorShape([1, None]))\n\nmodel.summary()","3311d361":"history_dict = history.history\nhistory_dict.keys()\n\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","f2765358":"plt.clf()   # clear figure\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\n\nplt.show()","c39a7ffc":"def sample_predict(sample_pred_text, pad):\n  encoded_sample_pred_text = tokenizer.texts_to_sequences(sample_pred_text)\n  print(encoded_sample_pred_text)\n  print(type(encoded_sample_pred_text))\n\n  if pad:\n    encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text, SEQ_LEN)\n    \n  encoded_sample_pred_text = np.array(encoded_sample_pred_text)\n  encoded_sample_pred_text = encoded_sample_pred_text.astype(\"float32\")\n  predictions = model.predict(encoded_sample_pred_text)\n\n","b8e8d630":"new_review = ['Crisp grapefruit and grassy lemon.']\nencoded_sample_pred_text = tokenizer.texts_to_sequences(new_review)\n# Some models need padding, some don't - depends on the embedding layer.\nencoded_sample_pred_text = tf.keras.preprocessing.sequence.pad_sequences(encoded_sample_pred_text, maxlen=SEQ_LEN, padding=\"post\")\npredictions = model.predict(encoded_sample_pred_text)\n\nfor n in reversed((np.argsort(predictions))[0]):\n    predicted_id = [n]\n    print(\"Guess: %s \\n Probability: %f\" %(decode_label(predicted_id).replace('_', ' '), 100*predictions[0][predicted_id][0]) + '%')","0eeea0e1":"new_review = ['Tart cherry and light, with velvety mushroom with lingering tannins.']\nencoded_sample_pred_text = tokenizer.texts_to_sequences(new_review)\n# Some models need padding, some don't - depends on the embedding layer.\nencoded_sample_pred_text = tf.keras.preprocessing.sequence.pad_sequences(encoded_sample_pred_text, maxlen=SEQ_LEN, padding=\"post\")\npredictions = model.predict(encoded_sample_pred_text)\n\nfor n in reversed((np.argsort(predictions))[0]):\n    predicted_id = [n]\n    print(\"Guess: %s \\n Probability: %f\" %(decode_label(predicted_id).replace('_', ' '), 100*predictions[0][predicted_id][0]) + '%')","b193beef":"new_review = ['Light and fruity with cookie, lemon, and strawberry.']\nencoded_sample_pred_text = tokenizer.texts_to_sequences(new_review)\n# Some models need padding, some don't - depends on the embedding layer.\n# encoded_sample_pred_text = tf.keras.preprocessing.sequence.pad_sequences(encoded_sample_pred_text, maxlen=SEQ_LEN, padding=\"post\")\npredictions = model.predict(encoded_sample_pred_text)\n\nfor n in reversed((np.argsort(predictions))[0]):\n    predicted_id = [n]\n    print(\"Guess: %s \\n Probability: %f\" %(decode_label(predicted_id).replace('_', ' '), 100*predictions[0][predicted_id][0]) + '%')","ba4b5497":"### Clean into a new simpler dataframe for this task","c51d2666":"## Background Analysis","b565dcb0":"### What do we have?\n#### Exploratory analysis and descriptors of the raw dataset","2b620ce6":"# Data Visualization","b295200d":"# Classifier\n\n## Using RNNs - Use the description to predict the grape \/ variety","330e027b":"### Load the Data","64484788":"## Run the model","73def2a9":"# Imports and Setup","a5c26069":"# Guess the grape or wine style from a review\n### Using the most excellent \"wine reviews\" dataset\n\nMy goal is to train a model that can read a wine review and guess what grape varietal is being described.","45ee53ff":"### Encode","61b19359":"# Evaluate with fake reviews","b6a88479":"### Group weird less common grapes into an \"other\" category","55182fd1":"### Create a tf.data.Dataset","df1fdb23":"## Load the result and see how we did","b2e249ed":"## Wines","8c795eb8":"Tokenizer lookup","2064697f":"## Geography\n\n### Country Data\n\nLet's deal with country data now\n\nhttps:\/\/github.com\/gsnaveen\/plotly-worldmap-mapping-2letter-CountryCode-to-3letter-country-code","8e663dd4":"### Drop missing years, convert year to a number type","c21df266":"dataset = np.array([[X_train], y_train]])\n\nfor train_example, train_label in dataset.take(1):\n  print('Encoded text:', train_example[:10].numpy())\n  print('Label:', train_label.numpy())","3a1610c0":"Convert text into numeric values, using words as a vocabulary via the tesnorflow tokenizer.","d824344d":"## Data Cleaning","931fecca":"## Feature Extraction and Engineering","e7fb0494":"### Looks at the title of the wine and extracts the vintages out of there.\n#### Creates a new column for the Vintages","e916abf6":"See if we missed any or had missing data.","ecf66ef2":"## Create the model"}}