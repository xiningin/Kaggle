{"cell_type":{"ad1eb55d":"code","d7b62f33":"code","b118d29b":"code","c9329b24":"code","ab97d5ac":"code","27dc89ca":"code","d2b59b74":"code","aff08a42":"code","cbd5a1fb":"code","de044d8b":"code","ba311392":"code","d0aaa506":"code","6dd656f7":"code","a2388f9f":"code","907aaf3a":"code","4a533c9f":"code","459862e7":"code","64d88dd2":"code","2c722110":"code","98673a12":"code","6c26971b":"code","dbc38d7f":"code","b17a0524":"code","3f83a16c":"code","b67e0c37":"code","3e842fdf":"code","dcbbe9f3":"code","acad7648":"code","caed94a2":"markdown","50f3b63d":"markdown","c74f9941":"markdown","a4833abb":"markdown","5f0796e4":"markdown","5742db52":"markdown","caf928c2":"markdown","e6b825a3":"markdown","dacb8a2b":"markdown"},"source":{"ad1eb55d":"%matplotlib inline","d7b62f33":"import numpy as np # linear algebra\nimport seaborn as sns\nsns.set(style='whitegrid')\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow as tf","b118d29b":"from subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","c9329b24":"iris = pd.read_csv('..\/input\/Iris.csv')","ab97d5ac":"iris.shape","27dc89ca":"iris.head()","d2b59b74":"iris = iris[:100]","aff08a42":"iris.shape","cbd5a1fb":"iris.Species = iris.Species.replace(to_replace=['Iris-setosa', 'Iris-versicolor'], value=[0, 1])","de044d8b":"plt.scatter(iris[:50].SepalLengthCm, iris[:50].SepalWidthCm, label='Iris-setosa')\nplt.scatter(iris[51:].SepalLengthCm, iris[51:].SepalWidthCm, label='Iris-versicolo')\nplt.xlabel('SepalLength')\nplt.ylabel('SepalWidth')\nplt.legend(loc='best')","ba311392":"X = iris.drop(labels=['Id', 'Species'], axis=1).values\ny = iris.Species.values","d0aaa506":"# set seed for numpy and tensorflow\n# set for reproducible results\nseed = 5\nnp.random.seed(seed)\ntf.set_random_seed(seed)","6dd656f7":"# set replace=False, Avoid double sampling\ntrain_index = np.random.choice(len(X), round(len(X) * 0.8), replace=False)","a2388f9f":"# diff set\ntest_index = np.array(list(set(range(len(X))) - set(train_index)))\ntrain_X = X[train_index]\ntrain_y = y[train_index]\ntest_X = X[test_index]\ntest_y = y[test_index]","907aaf3a":"# Define the normalized function\ndef min_max_normalized(data):\n    col_max = np.max(data, axis=0)\n    col_min = np.min(data, axis=0)\n    return np.divide(data - col_min, col_max - col_min)","4a533c9f":"# Normalized processing, must be placed after the data set segmentation, \n# otherwise the test set will be affected by the training set\ntrain_X = min_max_normalized(train_X)\ntest_X = min_max_normalized(test_X)","459862e7":"# Begin building the model framework\n# Declare the variables that need to be learned and initialization\n# There are 4 features here, A's dimension is (4, 1)\nA = tf.Variable(tf.random_normal(shape=[4, 1]))\nb = tf.Variable(tf.random_normal(shape=[1, 1]))\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)","64d88dd2":"# Define placeholders\ndata = tf.placeholder(dtype=tf.float32, shape=[None, 4])\ntarget = tf.placeholder(dtype=tf.float32, shape=[None, 1])","2c722110":"# Declare the model you need to learn\nmod = tf.matmul(data, A) + b","98673a12":"# Declare loss function\n# Use the sigmoid cross-entropy loss function,\n# first doing a sigmoid on the model result and then using the cross-entropy loss function\nloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=mod, labels=target))","6c26971b":"# Define the learning rate\uff0c batch_size etc.\nlearning_rate = 0.003\nbatch_size = 30\niter_num = 1500","dbc38d7f":"# Define the optimizer\nopt = tf.train.GradientDescentOptimizer(learning_rate)","b17a0524":"# Define the goal\ngoal = opt.minimize(loss)","3f83a16c":"# Define the accuracy\n# The default threshold is 0.5, rounded off directly\nprediction = tf.round(tf.sigmoid(mod))\n# Bool into float32 type\ncorrect = tf.cast(tf.equal(prediction, target), dtype=tf.float32)\n# Average\naccuracy = tf.reduce_mean(correct)\n# End of the definition of the model framework","b67e0c37":"# Start training model\n# Define the variable that stores the result\nloss_trace = []\ntrain_acc = []\ntest_acc = []","3e842fdf":"# training model\nfor epoch in range(iter_num):\n    # Generate random batch index\n    batch_index = np.random.choice(len(train_X), size=batch_size)\n    batch_train_X = train_X[batch_index]\n    batch_train_y = np.matrix(train_y[batch_index]).T\n    sess.run(goal, feed_dict={data: batch_train_X, target: batch_train_y})\n    temp_loss = sess.run(loss, feed_dict={data: batch_train_X, target: batch_train_y})\n    # convert into a matrix, and the shape of the placeholder to correspond\n    temp_train_acc = sess.run(accuracy, feed_dict={data: train_X, target: np.matrix(train_y).T})\n    temp_test_acc = sess.run(accuracy, feed_dict={data: test_X, target: np.matrix(test_y).T})\n    # recode the result\n    loss_trace.append(temp_loss)\n    train_acc.append(temp_train_acc)\n    test_acc.append(temp_test_acc)\n    # output\n    if (epoch + 1) % 300 == 0:\n        print('epoch: {:4d} loss: {:5f} train_acc: {:5f} test_acc: {:5f}'.format(epoch + 1, temp_loss,\n                                                                          temp_train_acc, temp_test_acc))","dcbbe9f3":"# Visualization of the results\n# loss function\nplt.plot(loss_trace)\nplt.title('Cross Entropy Loss')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()","acad7648":"# accuracy\nplt.plot(train_acc, 'b-', label='train accuracy')\nplt.plot(test_acc, 'k-', label='test accuracy')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.title('Train and Test Accuracy')\nplt.legend(loc='best')\nplt.show()","caed94a2":"**Step 5: Build the model framework**","50f3b63d":"I want to do a binary classification, so keep the first 100 rows of data\n\nIris-setosa species is linearly separable from the other two, but the other two are not linearly separable from each other.To keep the species blance\nIris-setosa and Iris-versicolor are choosen","c74f9941":"**Step 2:  Numerical processing**\n\n* replace 'Iris-setosa' as 0\n* replace 'Iris-versicolor' as 1","a4833abb":"**Step 4: Normalized processing**","5f0796e4":"**Step 3: Split data** \n\n* trainset: 80%\n* testset: 20%","5742db52":"## Using Tensorflow to implement Logistic Regression model","caf928c2":" **Step 1: Read the data**","e6b825a3":"**Step 7: Visualization**","dacb8a2b":"**Step 6: Model training**"}}