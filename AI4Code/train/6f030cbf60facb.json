{"cell_type":{"1264c1ec":"code","e3a254a0":"code","d0e4866b":"code","605922cf":"code","5c40cea9":"code","dc9198f2":"code","1ae9332b":"code","a354e0d3":"code","cd8e0ec5":"code","112d8889":"code","0ef136e9":"code","07baf2db":"code","5e9f110f":"code","23ae0438":"code","ab7bf404":"code","8862cafc":"code","cfb6844e":"code","7dcb1194":"code","b2d16291":"code","717c5545":"markdown","5b913444":"markdown","4d99b9a0":"markdown","d40a9f43":"markdown","8113556e":"markdown","f2692e72":"markdown"},"source":{"1264c1ec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e3a254a0":"# importing modules and data\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 70)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom xgboost import XGBClassifier\n#from sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import preprocessing\n#from sklearn.preprocessing import StandardScaler\n#from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\nimport warnings\n\nwarnings.simplefilter(action='ignore')\n\ntrain = pd.read_csv('..\/input\/learn-together\/train.csv')\ntest = pd.read_csv('..\/input\/learn-together\/test.csv')\n\n","d0e4866b":"cover_type = {1:'Spruce\/Fir', 2:'Lodgepole Pine',3:'Ponderosa Pine',4:'Cottonwood\/Willow',5:'Aspen',6:'Douglas-fir',7:'Krummholz'}\ntrain['Cover_type_description'] = train['Cover_Type'].map(cover_type)\n\n# I put together train and test data to work on both dataset at the same time\n\ncombined_data = [train, test]\n\ndef distance(a,b):\n    return np.sqrt(np.power(a,2)+np.power(b,2))\n\n# now I can classify different soil type according to their stoniness\n\nextremely_stony = [1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1]\nstony = [0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\nrubbly = [0,0,1,1,1,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\nother = [0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,1,1,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0]\n   \n# grouping for Aspect, Elevation and Slope \n\nfor data in combined_data:\n    \n    # grouping of Aspect feature\n    data['Azimut'] = 0\n    data['Azimut'].loc[(data['Aspect'] == 0)] = 'north'\n    data['Azimut'].loc[(data['Aspect']>0 ) & (data['Aspect']< 90)] = 'north_east'\n    data['Azimut'].loc[data['Aspect']==90] = 'east'\n    data['Azimut'].loc[(data['Aspect'] >90) & (data['Aspect'] <180)] = 'south_east'\n    data['Azimut'].loc[data['Aspect']==180] = 'south'\n    data['Azimut'].loc[(data['Aspect']>180) & (data['Aspect']<270)] = 'south_west'\n    data['Azimut'].loc[data['Aspect']== 270] = 'west'\n    data['Azimut'].loc[(data['Aspect']> 270) & (data['Aspect']< 360)] = 'noth_west'\n    data['Azimut'].loc[data['Aspect']== 360] = 'north'\n    \n    #grouping of Elevation feature\n    \n    data['Elevation_bins'] = 0\n    data['Elevation_bins'].loc[data['Elevation']<= 2000] = 'less than 2000'\n    data['Elevation_bins'].loc[(data['Elevation'] > 2000) & (data['Elevation']< 2500)] = 'between 2000 and 2500'\n    data['Elevation_bins'].loc[(data['Elevation'] > 2500) & (data['Elevation'] <= 3000)] = 'between 2000 and 3000'\n    data['Elevation_bins'].loc[(data['Elevation'] > 3000) & (data['Elevation'] <= 3500)] = 'between 3000 and 3500'\n    data['Elevation_bins'].loc[data['Elevation'] > 3500] = 'greater than 3500'\n    \n    # grouping for slope\n    data['Slope_category'] = 0\n    data['Slope_category'].loc[(data['Slope'] <= 10)] = 'slope less than 10'\n    data['Slope_category'].loc[(data['Slope'] > 10) & (data['Slope'] <= 20)] = 'slope between 10 and 20'\n    data['Slope_category'].loc[(data['Slope'] > 20) & (data['Slope'] <= 30)] = 'slope between 20 and 30'\n    data['Slope_category'].loc[(data['Slope'] > 30)] = 'slope greater than 30'\n\n    # for hillshade I take the mean and for I create a new variable Distance to hydrology\n\n    data['mean_Hillshade'] = (data['Hillshade_9am']+ data['Hillshade_Noon']+data['Hillshade_3pm'])\/3\n    data['Distance_to_hidrology'] = distance(data['Horizontal_Distance_To_Hydrology'],data['Vertical_Distance_To_Hydrology'])\n\n    data['extremely_stony_level'] = data[[col for col in data.columns if col.startswith(\"Soil\")]]@extremely_stony\n    data['stony'] = data[[col for col in data.columns if col.startswith(\"Soil\")]]@stony\n    data['rubbly'] = data[[col for col in data.columns if col.startswith(\"Soil\")]]@rubbly\n    data['other'] = data[[col for col in data.columns if col.startswith(\"Soil\")]]@other\n\n    data['Hillshade_noon_3pm'] = data['Hillshade_Noon']- data['Hillshade_3pm']\n    data['Hillshade_3pm_9am'] = data['Hillshade_3pm']- data['Hillshade_9am']\n    data['Hillshade_9am_noon'] = data['Hillshade_9am']- data['Hillshade_Noon']\n\n    # as I discovered in my previous kernel distance (from firepoints, roads and from hidrology) is an important variable so I will create new features based on that ones\n\n    data['Up_the_water'] = data['Vertical_Distance_To_Hydrology'] > 0\n    data['Total_horizontal_distance'] = data['Horizontal_Distance_To_Hydrology']+ data['Horizontal_Distance_To_Roadways']+ data['Horizontal_Distance_To_Fire_Points']\n    data['Elevation_of_hydrology'] = data['Elevation']+ data['Vertical_Distance_To_Hydrology']\n    data['Distance_to_firepoints plus Distance_to_roads'] = data['Horizontal_Distance_To_Fire_Points']+ data['Horizontal_Distance_To_Roadways']\n    data['Distance_to_roads plus distance_to_hydrology'] = data['Horizontal_Distance_To_Roadways'] + data['Horizontal_Distance_To_Hydrology']\n    data['Distance_to_firepoints minus Distance_to_roads'] = data['Horizontal_Distance_To_Fire_Points']- data['Horizontal_Distance_To_Roadways']\n    data['Distance_to_roads minus distance_to_hydrology'] = data['Horizontal_Distance_To_Roadways'] - data['Horizontal_Distance_To_Hydrology']\n\n ","605922cf":"soil_columns = [col for col in train.columns if col.startswith('Soil')]\n\n\n# we can drop soil columns because we have group each soil according to their stoniness\n\nfor data in combined_data:\n    data.drop(columns = soil_columns, inplace=True)","5c40cea9":"# for categorical values we can encode them into dummy values \n    \ncolumns_to_encode = ['Azimut','Elevation_bins','Slope_category','Up_the_water']\n    \ntrain = pd.get_dummies(train,columns = columns_to_encode)\ntest = pd.get_dummies(test,columns = columns_to_encode)\n\n# once we have encoded the columns we can drop them from both training and testing dataset\n\nfor data in combined_data:\n    data.drop(columns= columns_to_encode, inplace=True)","dc9198f2":"# scaling variables\n\n\ncolumns_to_scale = ['Elevation', 'Aspect', 'Slope',\n       'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n       'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points',\n       'mean_Hillshade', 'Distance_to_hidrology','Hillshade_noon_3pm', 'Hillshade_3pm_9am',\n       'Hillshade_9am_noon', 'Total_horizontal_distance',\n       'Elevation_of_hydrology',\n       'Distance_to_firepoints plus Distance_to_roads',\n       'Distance_to_roads plus distance_to_hydrology',\n       'Distance_to_firepoints minus Distance_to_roads',\n       'Distance_to_roads minus distance_to_hydrology']\n    \n\ns_scaler = preprocessing.StandardScaler()\ntrain_columns_to_scale = train[columns_to_scale]\ntest_columns_to_scale = test[columns_to_scale]\n\ntrain_scaled = s_scaler.fit_transform(train_columns_to_scale)\ntest_scaled = s_scaler.fit_transform(test_columns_to_scale)\n\ntrain_scaled_df = pd.DataFrame(data = train_scaled, columns = columns_to_scale)\ntest_scaled_df = pd.DataFrame(data = test_scaled, columns = columns_to_scale)\n\n# dropping columns scaled from training and testing dataset\ntrain.drop(columns = columns_to_scale, inplace = True)\ntest.drop(columns = columns_to_scale, inplace = True)\n\n#now we can concatenate scaled columns to both training and testing dataset\n    \ntrain_final = pd.concat([train , train_scaled_df],axis = 1)\n\ntest_final = pd.concat([test  ,test_scaled_df],axis = 1)\n\n\n#defining targer variable and features\ntarget = 'Cover_Type'\nfeatures = [ col for col in train_final.columns if col not in ['Id','Cover_Type','Cover_type_description']]\n\n\nX = train_final[features]\ny = train_final[target]\n\n","1ae9332b":"X_train, X_test,y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier,BaggingClassifier,GradientBoostingClassifier,ExtraTreesClassifier\nfrom sklearn import tree, neighbors, svm\nfrom sklearn.naive_bayes import GaussianNB\n\nML_algo = [\n        #Ensemble methods\n        AdaBoostClassifier(),\n        RandomForestClassifier(),\n        BaggingClassifier(),\n        GradientBoostingClassifier(),\n        ExtraTreesClassifier(),\n        #XGBoost\n        XGBClassifier(),\n        #Tree\n        tree.DecisionTreeClassifier(),\n        #Nearest Neighbour\n        neighbors.KNeighborsClassifier(),\n        # support vector machine\n        svm.SVC(),\n        #Gaussian Naive Bayes\n        GaussianNB()\n                \n        ]\n\n\nKfold = KFold(n_splits= 10, random_state= 1)\n\n# create a dataframe to compare ML_algo based on neagative mean absolute error\n\nML_algo_columns = ['ML_name', 'ML_parameters', 'ML_negative_mean_square_error','ML_standard_deviation']\n\nML_compare = pd.DataFrame(columns=ML_algo_columns)\n\n\nrow = 0\n\nfor algo in ML_algo:\n    name = algo.__class__.__name__\n    ML_compare.loc[row, 'ML_name'] = str(name)\n    ML_compare.loc[row, 'ML_parameters'] = str(algo.get_params())\n    # we use cross validation with KFold to score models\n    cv_results = cross_val_score(estimator =algo, X= X_train,scoring='neg_mean_squared_error', y=y_train,cv=Kfold)\n    ML_compare.loc[row, 'ML_negative_mean_square_error'] = str(cv_results.mean())\n    ML_compare.loc[row,'ML_standard_deviation'] = str(cv_results.std())\n    row = row +1\n\n   \nprint(ML_compare)\n","a354e0d3":"def plot_learning_curve(estimator, title, X,y, ylim =None, cv = None, n_jobs = None, train_sizes = np.linspace(.1,1.0,5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training samples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(estimator, X,y,cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis = 1)\n    train_scores_std = np.std(train_scores, axis = 1)\n    test_scores_mean = np.mean(test_scores, axis = 1)\n    test_scores_std = np.mean(test_scores, axis =1)\n    plt.grid()\n    plt.fill_between(train_sizes,train_scores_mean-train_scores_std,train_scores_mean+train_scores_std,alpha = 0.1, color =\"r\")\n    plt.fill_between(train_sizes,test_scores_mean-test_scores_std,test_scores_mean+test_scores_std,alpha = 0.1, color =\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",label=\"Training score\")\n    plt.plot(train_sizes,test_scores_mean,'o-',color = 'g', label =\"Cross_validation score\")\n    plt.legend(loc =\"best\")\n    return plt","cd8e0ec5":"# Params decision for RandomForest\n\nrandom_forest  = RandomForestClassifier()\nparams_decision_random_forest = {'criterion':['gini','entropy'],'max_features':['auto','sqrt','log2']}\ngrid_search_random_forest = GridSearchCV(random_forest, param_grid =params_decision_random_forest, cv=Kfold,scoring= 'neg_mean_squared_error', n_jobs=-1)\ngrid_search_random_forest.fit(X_train, y_train)\nprint('Best parameters for random forest classifier:', grid_search_random_forest.best_params_)\n\n\ndecision_random_forest = RandomForestClassifier(criterion = grid_search_random_forest.best_params_['criterion'], max_features = grid_search_random_forest.best_params_['max_features']) \ndecision_random_forest.fit(X_train,y_train)\n\n\nif False:\n    title = \"Random Forest learning curve\"\n    cv_shuffle = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n    plot_learning_curve(decision_random_forest, title, X, y, cv=cv_shuffle, n_jobs=-1)\n    plt.show()","112d8889":"#Best parameters for bagging classifier\n\nbagging_classifier = BaggingClassifier()\nparams_decision_for_bagging = {'n_estimators':[10,50,75],'max_samples' : [0.05, 0.1, 0.2, 0.5]}\ngrid_search_bagging= GridSearchCV(bagging_classifier, param_grid =params_decision_for_bagging, cv=Kfold,scoring= 'neg_mean_squared_error', n_jobs=-1)\ngrid_search_bagging.fit(X_train, y_train)\nprint('Best parameters for bagging classifier:', grid_search_bagging.best_params_)\n\n\ndecision_bagging_classifier = BaggingClassifier(n_estimators = grid_search_bagging.best_params_['n_estimators'], max_samples = grid_search_bagging.best_params_['max_samples']) \ndecision_bagging_classifier.fit(X_train,y_train)\n\nif False:\n    title = \"Bagging classifier learning curve\"\n    cv_shuffle = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n    plot_learning_curve(decision_bagging_classifier, title, X, y, cv=cv_shuffle, n_jobs=-1)\n    plt.show()\n","0ef136e9":"#Best parameters for ExtraTreeClassifier\n \nextra_tree = ExtraTreesClassifier()\nparams_decision_for_extra_tree= {'n_estimators':[10,50,75],'criterion' : ['gini','entropy']}\ngrid_search_extra_tree= GridSearchCV(extra_tree, param_grid =params_decision_for_extra_tree, cv=Kfold,scoring= 'neg_mean_squared_error', n_jobs=-1)\ngrid_search_extra_tree.fit(X_train, y_train)\nprint('Best parameters for extra tree classifier:', grid_search_extra_tree.best_params_)\n\n\n\ndecision_extra_tree_classifier = ExtraTreesClassifier(n_estimators = grid_search_extra_tree.best_params_['n_estimators'], criterion = grid_search_extra_tree.best_params_['criterion']) \ndecision_extra_tree_classifier.fit(X_train,y_train)\n\nif False:\n    title = \"Extra tree classifier learning curve\"\n    cv_shuffle = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n    plot_learning_curve(decision_extra_tree_classifier, title, X, y, cv=cv_shuffle, n_jobs=-1)\n    plt.show()","07baf2db":"#Best parameters for GradientBoostingClassifier\n\ngradient_boosting = GradientBoostingClassifier()\nparams_decision_gradient_boosting = { 'learning_rate':[0.03,0.05], 'n_estimators':[10,50,75]}\ngrid_search_gradient_boosting = GridSearchCV(gradient_boosting, param_grid =params_decision_gradient_boosting , cv=Kfold,scoring= 'neg_mean_squared_error', n_jobs=-1)\ngrid_search_gradient_boosting.fit(X_train, y_train)\nprint('Best parameters for decision gradient boosting:', grid_search_gradient_boosting.best_params_)\n\ndecision_gradient_boosting = GradientBoostingClassifier(n_estimators= grid_search_gradient_boosting.best_params_['n_estimators'], learning_rate= grid_search_gradient_boosting.best_params_['learning_rate']) \ndecision_gradient_boosting.fit(X_train,y_train)\n\nif False:\n    title = \"Gradient Boosting learning curve\"\n    cv_shuffle = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n    plot_learning_curve(decision_gradient_boosting, title, X, y, cv=cv_shuffle, n_jobs=-1)\n    plt.show()","5e9f110f":"#Best parameters for Decision tree classifier\ndecision_tree = tree.DecisionTreeClassifier()\nparams_decision_tree= {'criterion':['gini','entropy'], 'max_features':['auto','sqrt','log2']}\ngrid_search_decision_tree = GridSearchCV(decision_tree, param_grid =params_decision_tree , cv=Kfold,scoring= 'neg_mean_squared_error', n_jobs=-1)\ngrid_search_decision_tree.fit(X_train, y_train)\nprint('Best parameters for decision tree classifier:', grid_search_decision_tree.best_params_)\n\n\ndecision_tree_model = tree.DecisionTreeClassifier(criterion = grid_search_decision_tree.best_params_['criterion'], max_features = grid_search_decision_tree.best_params_['max_features']) \ndecision_tree_model.fit(X_train,y_train)\n\nif False:\n    title = \"Decision tree classifier\"\n    cv_shuffle = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n    plot_learning_curve(decision_tree_model, title, X, y, cv=cv_shuffle, n_jobs=-1)\n    plt.show()","23ae0438":"#Best parameters for XGBClassifier\nxgbclassifier = XGBClassifier()  \nparams_xgbclassifier = {'n_estimators': [50,100,150], 'learning_rate':[0.01,0.03,0.05]}\ngrid_search_xgboost = GridSearchCV(xgbclassifier, param_grid =params_xgbclassifier,cv=Kfold,scoring= 'neg_mean_squared_error', n_jobs=-1 )\ngrid_search_xgboost.fit(X_train, y_train)\nprint('Best parameters for xgboost classifier :', grid_search_xgboost.best_params_)\n\n\n#creare modello con i parametri ottimali\nxgboost_model = XGBClassifier(n_estimators = grid_search_xgboost.best_params_['n_estimators'],learning_rate = grid_search_xgboost.best_params_['learning_rate'])\nxgboost_model.fit(X_train,y_train)\n\n\nif False:\n    title = \"XGboost classifier\"\n    cv_shuffle = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n    plot_learning_curve(xgboost_model, title, X, y, cv=cv_shuffle, n_jobs=-1)\n    plt.show()","ab7bf404":"#Best parameters for SCV\n\nSVC = svm.SVC()\nparams_decision_SVC = {'C':[0.5,1.0]}\ngrid_search_SVC = GridSearchCV(SVC, param_grid =params_decision_SVC , cv=Kfold,scoring= 'neg_mean_squared_error', n_jobs=-1)\ngrid_search_SVC.fit(X_train, y_train)\nprint('best parameters for SVC:', grid_search_SVC.best_params_)\n\ndecision_SVC = svm.SVC(C = grid_search_SVC.best_params_['C']) \ndecision_SVC.fit(X_train,y_train) \n\n\nif False:\n    title = \"SVC classifier\"\n    cv_shuffle = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n    plot_learning_curve(decision_SVC, title, X, y, cv=cv_shuffle, n_jobs=-1)\n    plt.show()","8862cafc":"#Best parameters for KNeighborsClassifier\n\nKN_classifier = neighbors.KNeighborsClassifier()\nparams_KN = {'weights':['uniform','distance']}\ngrid_search_KN = GridSearchCV(KN_classifier, param_grid = params_KN, cv=Kfold,scoring = 'neg_mean_squared_error' ,n_jobs=-1)\ngrid_search_KN.fit(X_train, y_train)\nprint('Best parameters for KN classifier :', grid_search_KN.best_params_)\n\n\nKN_model = neighbors.KNeighborsClassifier(weights= grid_search_KN.best_params_['weights'])\nKN_model.fit(X_train,y_train)\n\n\nif False:\n    title = \"KN classifier\"\n    cv_shuffle = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n    plot_learning_curve(KN_model, title, X, y, cv=cv_shuffle, n_jobs=-1)\n    plt.show()","cfb6844e":"from sklearn.ensemble import VotingClassifier\n\n#list of models\nestimators = [('Random forest',decision_random_forest),('Bagging',decision_bagging_classifier),('Extra tree classifier', decision_extra_tree_classifier),('Gradient boosting',decision_gradient_boosting),('Decision tree',decision_tree_model),('Xgboost',xgboost_model),('SCV',decision_SVC),('KNeighbors',KN_model)]\nensemble = VotingClassifier(estimators, voting = 'hard')\n\nensemble.fit(X_train, y_train)\n\ntitle = \"Ensemble classifier learning curve\"\ncv_shuffle = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nplot_learning_curve(ensemble, title, X, y, cv=cv_shuffle, n_jobs=-1)\nplt.show()\n\n","7dcb1194":"X = test_final[features]\ny_pred = ensemble.predict(X)","b2d16291":"#submitting data\nsub = pd.DataFrame({'ID': test_final.Id, 'Cover_Type': y_pred})\nsub.to_csv('submission_csv')","717c5545":"# Data preprocessing (encoding of categorical data and standard scaling)","5b913444":"# ENSEMBLING MODELS FOR FOREST COMPETITION\n\nIn my previous kernel (XGBoost with ML explainability and Gridsearch) I perfomed and EDA analysis on Roosevelt Forest data in order to have an understanding of what are the most important features that you have to take into account in classifing the different types of forest. I've also built a xgboost model with ML explainability in order to see the features importance for the model. \n\nIn this kernel I will go further in feature engineering based on the results of previous kernel and I will create a voting classifier based on majority vote in order to put togheter different models.  \n\nEnsembling models together can improve the performance of single algorithms. \n\nHere is a good article to explain this:\n\nhttps:\/\/towardsdatascience.com\/two-is-better-than-one-ensembling-models-611ee4fa9bd8\n\nKernel is divided in this way:\n\n1. Feature engineering \n2. Data preprocessing (encoding of categorical data and standard scaling)\n3. Models building with GridSearch to find the best parameters for each model\n4. Voing classifier to put togheter the models and produce a single result (based on a majority vote)\n\n","4d99b9a0":"# Models building with GridSearch to find the best parameters for each model","d40a9f43":"BEST MODELS ARE (the higher the score, the better the algorithm):\n\n  1 - Random Forest Classifier  \n  2 - BaggingClassifier    \n  3 - ExtraTreesClassifier  \n  4 - GradientBoostingClassifier  \n  5 - DecisionTreeClassifier  \n  6 - XGBClassifier   \n  7 - SVC\n  8 - KNeighborsClassifier \n\nAdaBoostClassifier and GaussianNB have by far a worse score compared to others algorithms so we will exclude them from Voting Classifier\n\nFor each of the previous algorithms I perform GridSearch to find the best parameters for each one and you can  also plot their learning curve. I'm doing that only for ensemble model because doing so for every model is computationally expensive. Anyway I've left the learning curve function for every model: you can comment it with \"if False\" and the code is not runned.\n\nLEARNING CURVE FUNCTION FROM sklearn.model_selection.learning_curve\n","8113556e":"# Voting classifier to put togheter the models and produce a single result (based on a majority vote)","f2692e72":"## Feature engineering "}}