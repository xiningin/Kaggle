{"cell_type":{"a3a5a3b8":"code","f028385c":"code","0ad726e0":"code","fd2ef9b0":"code","c035a697":"code","4cd9c202":"code","212014b8":"code","dcf27ea9":"code","a7e235e0":"code","98a9ed22":"code","0b4c17b7":"code","ececae61":"code","4fdc0908":"code","6f331fbe":"code","04fc9c22":"code","dfd263e5":"code","395267ef":"code","8da48158":"code","addb7ecb":"code","cf1d6ddf":"code","e14c77fe":"code","fc5853b3":"code","524d1db8":"code","29bec9f3":"markdown","45f5e5ed":"markdown"},"source":{"a3a5a3b8":"import pandas as pd\nimport numpy as np\nimport json\nimport seaborn as sns\nimport datetime\nimport matplotlib.pylab as plt\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom scipy.ndimage.filters import gaussian_filter\nimport warnings\nimport random\nimport plotly.express as px\nwarnings.filterwarnings(\"ignore\")\nsns.set_style(\"whitegrid\")\nmy_pal = sns.color_palette(n_colors=10)","f028385c":"n = 11341042 #number of records in file\ns = 2000000 #desired sample size\nfilename = '..\/input\/data-science-bowl-2019\/train.csv'\nskip = sorted(random.sample(range(n),n-s))\ntrain = pd.read_csv(filename, skiprows=skip)\ntrain.columns = ['event_id','game_session','timestamp','event_data',\n            'installation_id','event_count','event_code','game_time','title','type','world']","0ad726e0":"# train = pd.read_csv('..\/input\/data-science-bowl-2019\/train.csv')\ntest = pd.read_csv('..\/input\/data-science-bowl-2019\/test.csv')\nlabels = pd.read_csv('..\/input\/data-science-bowl-2019\/train_labels.csv')","fd2ef9b0":"from more_itertools import sliced\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, SpatialDropout1D\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks.callbacks import EarlyStopping","c035a697":"full = train.merge(labels, how='inner', on=['installation_id','game_session'])\ntrain_ls = full[['installation_id','game_session','event_id']]\n# convert to str\ntrain_ls['event_id'] = train_ls['event_id'].apply(lambda x: str(x))","4cd9c202":"del train","212014b8":"def events_all(aa):\n    xx = ''\n    for i in aa: \n        xx += i + ' '\n    xx = xx.rstrip()\n    return xx","dcf27ea9":"result = train_ls.groupby(['installation_id','game_session']).sum().reset_index()\nresult['event_id'] = result['event_id'].apply(lambda x: list(sliced(x, 8)))\nresult['new_event'] = result['event_id'].apply(events_all)\nresult = result.merge(labels, how='inner', on=['installation_id','game_session'])[['new_event','accuracy_group']]","a7e235e0":"result.head()\n# plt.scatter(result['accuracy_group'], result['event_code'].apply(lambda x: len(x)));","98a9ed22":"# The maximum number of words to be used. (most frequent)\nMAX_NB_WORDS = 100\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 500\n# This is fixed.\nEMBEDDING_DIM = 100\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-.\/:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(result['new_event'].values)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","0b4c17b7":"X = tokenizer.texts_to_sequences(result['new_event'].values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)","ececae61":"Y = pd.get_dummies(result['accuracy_group']).values\nprint('Shape of label tensor:', Y.shape)","4fdc0908":"X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","6f331fbe":"model = Sequential()\nmodel.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\nmodel.add(LSTM(50, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(4, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n# choose epochs and batch_size\nepochs = 5\nbatch_size = 64\nhistory = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.001)])","04fc9c22":"accr = model.evaluate(X_test,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","dfd263e5":"plt.title('Loss')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show();","395267ef":"last_test = test[['installation_id','game_session',\n                  'timestamp']].groupby(['installation_id']).tail(1)[['installation_id','game_session']]\ntest_ = test.merge(last_test,how='inner', on=['installation_id','game_session'])","8da48158":"test_ls = test_[['installation_id','game_session','event_id']]\n# test_ls = test[['installation_id','game_session','event_id']]\ntest_ls['event_id'] = test_ls['event_id'].apply(lambda x: str(x))\nres_test = test_ls.groupby(['installation_id','game_session']).sum().reset_index()\nres_test['event_id'] = res_test['event_id'].apply(lambda x: list(sliced(x, 8)))\nres_test['new_event'] = res_test['event_id'].apply(events_all)","addb7ecb":"X_ts = tokenizer.texts_to_sequences(res_test['new_event'].values)\nX_ts = pad_sequences(X_ts, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X_ts.shape)","cf1d6ddf":"test_pred = model.predict(X_ts)","e14c77fe":"submission = pd.concat([res_test['installation_id'],\n                                     pd.DataFrame(test_pred).idxmax(1)], axis=1)\nsubmission.columns = ['installation_id','accuracy_group']","fc5853b3":"# submission.to_csv('submission.csv')\nsubmission.to_csv('submission.csv', index=None)\nsubmission.head()","524d1db8":"submission['accuracy_group'].hist();","29bec9f3":"### Test prediction:","45f5e5ed":"### LSTM Experiments"}}