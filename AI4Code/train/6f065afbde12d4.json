{"cell_type":{"9cd62b36":"code","3f17fb96":"code","e5a2cb5a":"code","e6ca167f":"code","1ebc44d7":"code","b88db4a0":"markdown","24bc2009":"markdown","25a3ade5":"markdown","7ac0d36c":"markdown","c76dc517":"markdown"},"source":{"9cd62b36":"import pandas as pd\nimport json\nimport urllib3\nfrom time import sleep\nimport warnings\nwarnings.filterwarnings(\"ignore\")","3f17fb96":"http = urllib3.PoolManager() \nmat_data = pd.DataFrame()\nperiods = [\"1\",\"2\"] # For the two innings in the match\npages = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"] # This refers to the no of requests for the inifinte scrolling\nleagueId=\"8048\" # League ID for IPL2019 in espncricinfo\neventId=\"\" #match ID - will be populated from the URLs later\ndat_url = pd.read_csv(\"..\/input\/ipl-2019-match-data\/match_details.csv\") # match_details extracted using scrapy\n#All IPL 2019 URLs and match data in this file. Python script to extract this in my Github. Refer introduction\neventId_gp= [str(url).split(\"\/\")[6] for url in dat_url[\"url\"]] # Extracting indiviual match ids from the URL\nlen_url= len(eventId_gp)","e5a2cb5a":"for count in range(len_url):\n    eventId = eventId_gp[count]\n    for period in periods:\n        for page in pages:\n            sleep(15) # Espncricinfo recommends a scraping delay of 15 seconds. This increases the run time considerably.\n            col_data = pd.DataFrame()\n            match_dat= http.request('GET', 'https:\/\/hsapi.espncricinfo.com\/v1\/pages\/match\/comments?lang=en&leagueId='+leagueId+'&eventId='+eventId+'&period=' +period+ '&page='+page+'&filter=full&liveTest=false')\n            if(len(match_dat.data)<100):\n                break\n            data = json.loads(match_dat.data)\n            df = pd.json_normalize(data['comments'])\n            bowler=[]\n            batsman=[]\n\n            for bat,bowl in zip(df[\"currentBatsmen\"],df[\"currentBowlers\"]):\n                batsman.append(bat[0][\"name\"])\n                bowler.append(bowl[0][\"name\"])\n\n            df[\"bowler\"]= bowler\n            df[\"batsman\"] = batsman\n            col_data = df.copy()    \n\n            if(period==\"1\"):               \n                df[\"innings\"]=1\n            else:\n                df[\"innings\"]=2\n\n            if(\"matchWicket.text\" in col_data.columns):\n                col_data[\"matchWicket.text\"].fillna(\"NA\",inplace=True)\n                col_data[\"run_out\"]= [\"Yes\" if \"run out\" in wicket_text else \"No\" for wicket_text in col_data[\"matchWicket.text\"]]\n            else:\n                col_data[\"matchWicket.text\"]=\"NA\"\n                col_data[\"run_out\"]=\"No\"\n                   \n         \n            col_data[\"match_id\"] = eventId        \n            mat_data = pd.concat([mat_data,col_data])   ","e6ca167f":"# we are dropping the extra columns below. you can remove columns from the below list which you think are useful\nmat_data.drop([\"id\",\"shortText\",\"text\",\"preText\",\"postText\",\"currentBatsmen\",\"currentBowlers\",\"currentInning.balls\",\"currentInning.runs\",\"currentInning.wickets\",\"matchOver.maiden\",\"matchOver.runs\",\"matchOver.wickets\",\"matchOver.totalRuns\",\"matchOver.totalWicket\",\"matchOver.runRate\",\"matchOver.requiredRunRate\",\"matchOver.batsmen\",\"matchOver.bowlers\",\"matchOver.teamShortName\",\"matchOver.remainingOvers\",\"matchOver.remainingBalls\",\"matchOver.remainingRuns\",\"matchWicket.id\",\"matchWicket.batsmanRuns\",\"matchWicket.batsmanBalls\",\"matchWicket.text\"],axis=1,inplace=True)\nmat_data.to_csv(\"score.csv\")\n","1ebc44d7":"mat_data.head()","b88db4a0":"Cleaning up the dataset and exporting it to a CSV file which can be used for EDA and other kinds of analysis.","24bc2009":"# Indian Premier League is back !!!\n\n\nWith the onset of IPL 2020, I am sure that we are going to have a lot of EDA and predictions coming our way for this season. Here I have created a small script that helps getting ball by ball details of an IPL match from espncricinfo.com.\n\n### This script can be used to create a IPL 2020 dataset all by yourself when the tournament starts !!\n\nThere are actually three steps to it as below but we are only going to work on the third step as it is the actual script which gives us ball by ball details.\n\n![](https:\/\/i.ibb.co\/25w9LDT\/Web-SCraping.jpg)\n\n\nThe first two parts of the script which uses [scrapy](https:\/\/scrapy.org) to scrape the match data for a particular series. Based on the data we get from that script here we are going to navigate and scrape the data.\n\nThe scrapy script can be accessed on my Github -> [Click here ](https:\/\/github.com\/ankursalunke\/Web-Scraper-Cricket#readme)\n\nFor detailed flow for the entire IPL data scraping program please visit my blog. [Click Here](https:\/\/medium.com\/@ankur.salunke\/web-scraping-with-python-cricket-scores-74c90bbad293)\n\nThis is meant to be an introduction to web scraping. Here we have only used urllib3 and json since the data is available in json format for these espncricinfo web pages.\n\nIn the below image, if we inspect element in the commentary page we can see in the XML Httprequest we can get the ball by ball data in json format. Since there is infinite scrolling in this page, we will loop overall the pages and extract the data.\n\n![](https:\/\/i.ibb.co\/cT2vbqw\/scrn.png)\n\nIn case of any suggestions please share them in the comments. I hope that this would help kagglers create their datasets for analysis when IPL 2020 kicks off.","25a3ade5":"Initiating all the necessary objects.\nThe match_details.csv file is the output of the scrapy script to get the match data. Please visit my github\/blog mentioned in the introduction for the code\/descriptiom for the scrapy script.","7ac0d36c":"We loop over the matches, innings and pages to extract the data and add it to a dataset. We process the data also to create new columns as required.","c76dc517":"Importing the necessary libraries"}}