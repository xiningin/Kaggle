{"cell_type":{"262e6c44":"code","4d6b2588":"code","9983e8cc":"code","3cd0d200":"code","48fcba4e":"code","713274fb":"code","e12fe9b9":"code","cb561e03":"code","997e261a":"code","cf9c0f8d":"code","5ff97146":"code","c703b728":"code","f8d7729d":"code","c00ef715":"code","edc9c414":"code","c379298f":"code","1e9b82e3":"code","0f0e473c":"code","8a51e840":"code","12490bb1":"code","85a9a03f":"code","dff7d6a5":"code","ba7ac57c":"code","10864170":"code","c2d624fe":"code","d52d2424":"code","c3262f8d":"code","838eeb53":"code","a4879ac8":"code","244e046e":"code","306da763":"code","051f42f9":"code","b320f492":"code","8a33d892":"code","00988883":"markdown","663adfc4":"markdown","322d4261":"markdown","a918ae53":"markdown","82910efe":"markdown","65812398":"markdown","6e643356":"markdown","7ade5e0a":"markdown","db3ffa8e":"markdown","f39e88a8":"markdown","980aec94":"markdown","054136d6":"markdown","cdc9e227":"markdown","6d3c9b85":"markdown","a58c3ec7":"markdown","21f46d60":"markdown","db62ec17":"markdown"},"source":{"262e6c44":"import sys\nimport os\nimport json\nimport time\n\nimport pandas as pd\nimport boto3\nimport dateutil.parser\nimport matplotlib.pyplot as plt\n\n# importing forecast notebook utility\nsys.path.insert( 0, os.path.abspath(\"..\/..\/common\") )\nimport util","4d6b2588":"# Assign the S3 bucket name and region name - must create S3 bucket first\n#text_widget_bucket = util.create_text_widget( \"bucket_name\", \"retailsalesprediction\" )\n#text_widget_region = util.create_text_widget( \"region\", \"ap-southeast-2\", default_value=\"us-west-2\" )\n\n#bucket_name = text_widget_bucket.value\n#assert bucket_name, \"bucket_name not set.\"\n\nregion = \"ap-southeast-2\"\nassert region, \"region not set.\"","9983e8cc":"#validate that your account can communicate with Amazon Forecast\nsession = boto3.Session(region_name=region) \nforecast = session.client(service_name='forecast') \nforecastquery = session.client(service_name='forecastquery')","3cd0d200":"# Read in the data\ndf = pd.read_csv(\"..\/input\/target-time-series\/target_time_series (1).csv\", dtype = object, names=['timestamp','store','item_id','demand'])","48fcba4e":"# Seperate training and test dataframes\n\n# Select training set\nfeb10_to_feb12 = df[(df['timestamp'] >= '2010-02-05') & (df['timestamp'] <= '2012-01-31')]\n\n# Select test set\nremaining_df = df[(df['timestamp'] >= '2012-02-01') & (df['timestamp'] <= '2012-10-26')]","713274fb":"# Export them to the 'data' folder\nfeb10_to_feb12.to_csv(\"data\/item-demand-time-train.csv\", header=False, index=False)\nremaining_df.to_csv(\"data\/item-demand-time-validation.csv\", header=False, index=False)","e12fe9b9":"# Upload the main dataset to S3\n# I already uploaded it manually in this case\n\n#key=\"elec_data\/item-demand-time-train.csv\"\n\n#boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(\"data\/item-demand-time-train.csv\")","cb561e03":"# Set the configuration of the dataset\nDATASET_FREQUENCY = \"W\" \nTIMESTAMP_FORMAT = \"yyyy-MM-dd\"\nproject = 'retail_sales_forecasting'\ndatasetName= project+'_ds'\ndatasetGroupName= project +'_dsg'\ns3DataPath = \"s3:\/\/kaggle-sales-forecasting\/target_time_series.csv\"\n\n# Save it\n%store project","997e261a":"# Create the Dataset Group\ncreate_dataset_group_response = forecast.create_dataset_group(DatasetGroupName=datasetGroupName,\n                                                              Domain=\"RETAIL\",\n                                                             )\ndatasetGroupArn = create_dataset_group_response['DatasetGroupArn']\n\nforecast.describe_dataset_group(DatasetGroupArn=datasetGroupArn)","cf9c0f8d":"# Specify the schema, order must match the data. data must not have headers\nschema ={\n   \"Attributes\":[\n      {\n         \"AttributeName\":\"timestamp\",\n         \"AttributeType\":\"timestamp\"\n      },\n      {\n         \"AttributeName\":\"item_id\",\n         \"AttributeType\":\"string\"\n      },\n       {\n         \"AttributeName\":\"target_value\",\n         \"AttributeType\":\"float\"\n      }\n      \n   ]\n}","5ff97146":"# Create the Dataset\nresponse=forecast.create_dataset(\n                    Domain=\"RETAIL\",\n                    DatasetType='TARGET_TIME_SERIES',\n                    DatasetName=datasetName,\n                    DataFrequency=DATASET_FREQUENCY, \n                    Schema = schema\n)\n\ndatasetArn = response['DatasetArn']\nforecast.describe_dataset(DatasetArn=datasetArn)","c703b728":"# Add Dataset to Dataset Group\nforecast.update_dataset_group(DatasetGroupArn=datasetGroupArn, DatasetArns=[datasetArn])","f8d7729d":"# Create the role to provide to Amazon Forecast.\nrole_name = \"retail_sales_forecast\"\nrole_arn = util.get_or_create_iam_role( role_name = role_name )","c00ef715":"# Import the data from S3 into Amazon Forecast\ndatasetImportJobName = 'EP_DSIMPORT_JOB_TARGET'\nds_import_job_response=forecast.create_dataset_import_job(DatasetImportJobName=datasetImportJobName,\n                                                          DatasetArn=datasetArn,\n                                                          DataSource= {\n                                                              \"S3Config\" : {\n                                                                 \"Path\":s3DataPath,\n                                                                 \"RoleArn\": role_arn\n                                                              } \n                                                          },\n                                                          TimestampFormat=TIMESTAMP_FORMAT\n                                                         )\n\nds_import_job_arn=ds_import_job_response['DatasetImportJobArn']\nprint(ds_import_job_arn)\n\nstatus_indicator = util.StatusIndicator()\n\nwhile True:\n    status = forecast.describe_dataset_import_job(DatasetImportJobArn=ds_import_job_arn)['Status']\n    status_indicator.update(status)\n    if status in ('ACTIVE', 'CREATE_FAILED'): break\n    time.sleep(10)\n\nstatus_indicator.end()","edc9c414":"# Describe the imported dataset\nforecast.describe_dataset_import_job(DatasetImportJobArn=ds_import_job_arn)","c379298f":"# Configure the predictor\npredictorName= project+'_deeparp_algo' # Set a Predictor Name\nforecastHorizon = 26 # Half a year\nalgorithmArn = 'arn:aws:forecast:::algorithm\/Deep_AR_Plus' # Choose an algorithm to use - Deep AR in this case. Could try ARIMA and CNN later","1e9b82e3":"# Create the predictor\ncreate_predictor_response=forecast.create_predictor(PredictorName=predictorName, \n                                                  AlgorithmArn=algorithmArn,\n                                                  ForecastHorizon=forecastHorizon,\n                                                  PerformAutoML= False,\n                                                  PerformHPO=False,\n                                                  EvaluationParameters= {\"NumberOfBacktestWindows\": 1, \n                                                                         \"BackTestWindowOffset\": 24}, \n                                                      InputDataConfig= {\"DatasetGroupArn\": datasetGroupArn},\n                                                          FeaturizationConfig= {\"ForecastFrequency\": \"W\", \n                                                                                    \"Featurizations\": \n                                                                                    [\n                                                                                      {\"AttributeName\": \"target_value\", \n                                                                                       \"FeaturizationPipeline\": \n                                                                                        [\n                                                                                          {\"FeaturizationMethodName\": \"filling\", \n                                                                                           \"FeaturizationMethodParameters\": \n                                                                                            {\"frontfill\": \"none\", \n                                                                                             \"middlefill\": \"zero\", \n                                                                                             \"backfill\": \"zero\"}\n                                                                                          }\n                                                                                        ]\n                                                                                      }\n                                                                                    ]\n                                                                                   }\n                                                 )","0f0e473c":"# Create the resource\npredictor_arn=create_predictor_response['PredictorArn']\n\n# Track the progress of the predictor\nstatus_indicator = util.StatusIndicator()\n\nwhile True:\n    status = forecast.describe_predictor(PredictorArn=predictor_arn)['Status']\n    status_indicator.update(status)\n    if status in ('ACTIVE', 'CREATE_FAILED'): break\n    time.sleep(10)\n\nstatus_indicator.end()","8a51e840":"forecast.get_accuracy_metrics(PredictorArn=predictor_arn)","12490bb1":"# Create the forecast using the data + forecast ARN\nforecastName= project+'_deeparp_algo_forecast'\ncreate_forecast_response=forecast.create_forecast(ForecastName=forecastName,\n                                                  PredictorArn=predictor_arn)\nforecast_arn = create_forecast_response['ForecastArn']\n\n# Track the progress\nstatus_indicator = util.StatusIndicator()\n\nwhile True:\n    status = forecast.describe_forecast(ForecastArn=forecast_arn)['Status']\n    status_indicator.update(status)\n    if status in ('ACTIVE', 'CREATE_FAILED'): break\n    time.sleep(10)\n\nstatus_indicator.end()","85a9a03f":"# Retrieve the forecast\nprint(forecast_arn)\nprint()\nforecastResponse = forecastquery.query_forecast(ForecastArn=forecast_arn)\nprint(forecastResponse)","dff7d6a5":"# Query the predictor\nforecastResponse = forecastquery.query_forecast(ForecastArn=forecast_arn)","ba7ac57c":"# Retrieve the Actuals data\nactual_df = pd.read_csv(\"data\/item-demand-time-validation.csv\", names=['timestamp','store','item_id','demand'])","10864170":"# Generate DF for each confidence level\nprediction_df_p10 = pd.DataFrame.from_dict(forecastResponse['Forecast']['Predictions']['p10'])\nprediction_df_p50 = pd.DataFrame.from_dict(forecastResponse['Forecast']['Predictions']['p50'])\nprediction_df_p90 = pd.DataFrame.from_dict(forecastResponse['Forecast']['Predictions']['p90'])","c2d624fe":"# Create an empty df to add the actual & predicted values into\nresults_df = pd.DataFrame(columns=['timestamp', 'value', 'source'])\n\n# Append the actual values into the df\nfor index, row in actual_df.iterrows():\n    clean_timestamp = dateutil.parser.parse(row['timestamp'])\n    results_df = results_df.append({'timestamp' : clean_timestamp , 'value' : row['value'], 'source': 'actual'} , ignore_index=True)\n    \n# Now add the P10, P50, and P90 Values\nfor index, row in prediction_df_p10.iterrows():\n    clean_timestamp = dateutil.parser.parse(row['Timestamp'])\n    results_df = results_df.append({'timestamp' : clean_timestamp , 'value' : row['Value'], 'source': 'p10'} , ignore_index=True)\nfor index, row in prediction_df_p50.iterrows():\n    clean_timestamp = dateutil.parser.parse(row['Timestamp'])\n    results_df = results_df.append({'timestamp' : clean_timestamp , 'value' : row['Value'], 'source': 'p50'} , ignore_index=True)\nfor index, row in prediction_df_p90.iterrows():\n    clean_timestamp = dateutil.parser.parse(row['Timestamp'])\n    results_df = results_df.append({'timestamp' : clean_timestamp , 'value' : row['Value'], 'source': 'p90'} , ignore_index=True)","d52d2424":"# Pivot the dataframe into a better format for plotting\npivot_df = results_df.pivot(columns='source', values='value', index=\"timestamp\")","c3262f8d":"# Convert the timestamp value to a datetime64 type\nactual_df['timestamp'] = actual_df['timestamp'].astype('datetime64')","838eeb53":"# Plot the Actual Values, the Predicted Values, and the p90 to p10 error range\nplt.figure(figsize=(15,7))\nplt.plot(pivot_df.index, pivot_df['actual'], label=\"Actual\")\nplt.plot(pivot_df.index, pivot_df['p50'], label=\"p50\")\nplt.legend()\nplt.fill_between(pivot_df.index, pivot_df['p90'], pivot_df['p10'],\n                 color='gray', alpha=0.2)\npivot_df.plot()","a4879ac8":"# Delete the Forecast:\nutil.wait_till_delete(lambda: forecast.delete_forecast(ForecastArn=forecast_arn))","244e046e":"# Delete the Predictor:\nutil.wait_till_delete(lambda: forecast.delete_predictor(PredictorArn=predictor_arn))","306da763":"# Delete the Import:\nutil.wait_till_delete(lambda: forecast.delete_dataset_import_job(DatasetImportJobArn=ds_import_job_arn))","051f42f9":"# Delete the Dataset:\nutil.wait_till_delete(lambda: forecast.delete_dataset(DatasetArn=datasetArn))","b320f492":"# Delete the DatasetGroup:\nutil.wait_till_delete(lambda: forecast.delete_dataset_group(DatasetGroupArn=datasetGroupArn))","8a33d892":"# Delete the IAM role\nutil.delete_iam_role( role_name )","00988883":"## Prepare the Actual Results","663adfc4":"## Comparing the Prediction to Actual Result","322d4261":"## Predictor is now built","a918ae53":"## Plot the Prediction","82910efe":"## Get Error Metrics","65812398":"## Create a Forecast","6e643356":"## Create Data Import Job","7ade5e0a":"## Creating the Dataset Group and Dataset","db3ffa8e":"## Obtaining a Prediction","f39e88a8":"## Setup","980aec94":"## Data Preparation","054136d6":"## Create IAM Role for Forecast","cdc9e227":"## Cleanup","6d3c9b85":"# Using Amazon Forecast to predict future retail sales","a58c3ec7":"## Data is now ready","21f46d60":"*This notebook currently not working, needs AWS credentials updated*","db62ec17":"## Create a Predictor"}}