{"cell_type":{"cedbd593":"code","ec7bcf2e":"code","3493ff21":"code","6c884920":"code","78dbf90a":"code","8bacb48a":"code","4ed8bae9":"code","da205585":"code","854525e2":"code","e84aa452":"code","f3b67148":"code","63e90a9f":"code","2195b13c":"code","5333d714":"code","cd9895e9":"code","42c57e79":"code","a5aac0e3":"code","7956bf26":"code","68a3d83c":"code","8f370a2e":"code","81c6fda9":"code","a4681099":"code","8e2d0a92":"code","1df4e9d9":"code","231dd85e":"code","f4ccade7":"code","973dc368":"code","a93ba1de":"code","efa39460":"markdown","088cee89":"markdown"},"source":{"cedbd593":"#imports\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nimport string\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ec7bcf2e":"#import data\n\ntrain_tweets = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_tweets = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","3493ff21":"#define functions to transform text and to generate features\n\ndef remove_punct(text):\n    '''Return text excluding the following punctuation: '!\"#$%&\\'()*+,-.\/:;<=>?@[\\\\]^_`{|}~' '''\n    text_nopunct = ''.join([char for char in text if char not in string.punctuation])\n    return(text_nopunct)\n\ndef count_hashtags(text):\n    '''Count number of hashtags in text'''\n    hashtag_counter = 0\n    for char in text:\n        if char == '#':\n            hashtag_counter += 1\n    return(hashtag_counter)\n\ndef remove_hashtags(text):\n    '''Return text excluding the following punctuation: '!\"#$%&\\'()*+,-.\/:;<=>?@[\\\\]^_`{|}~' '''\n    text_nohashtags = ''.join([char for char in text if char != '#'])\n    return(text_nohashtags)\n\nporter = PorterStemmer()\ndef stemming(tokenized_text):\n    '''Return stemmed text'''\n    text = [porter.stem(word) for word in tokenized_text]\n    return(text)\n\nwordnet_lemmatizer = WordNetLemmatizer()\ndef lemmatizing(tokenized_text):\n    '''Return lemmatized text'''\n    text = [wordnet_lemmatizer.lemmatize(word) for word in tokenized_text]\n    return(text)\n\ndef count_numerics(text):\n    '''Count the number of numeric characters in text'''\n    numeric_counter = 0\n    for char in text:\n        if char.isnumeric():\n            numeric_counter +=1\n    return(numeric_counter)\n\ndef alpha_tokens(tokenized_text):\n    text = [t for t in tokenized_text if t.isalpha()]\n    return(text)\n            \ndef clean_text(tokenized_text):\n    text = \" \".join([word.lower() for word in tokenized_text])\n    return(text)","6c884920":"#Use defined functions to transform text and generate features\n\ntrain_tweets['sentence_tokens'] = train_tweets['text'].apply(lambda x: sent_tokenize(x))\ntrain_tweets['word_tokens'] = train_tweets['text'].apply(lambda x: word_tokenize(x))\ntrain_tweets['num_chars'] = train_tweets['text'].apply(lambda x: len(x))\ntrain_tweets['num_sentences'] = train_tweets['sentence_tokens'].apply(lambda x: len(x))\ntrain_tweets['num_words'] = train_tweets['word_tokens'].apply(lambda x: len(x))\ntrain_tweets['num_hashtags'] = train_tweets['text'].apply(lambda x: count_hashtags(x))\ntrain_tweets['num_numeric'] = train_tweets['text'].apply(lambda x: count_numerics(x))\ntrain_tweets['no_punct'] = train_tweets['text'].apply(lambda x: remove_punct(x))\ntrain_tweets['no_hashtags'] = train_tweets['no_punct'].apply(lambda x: remove_hashtags(x))\ntrain_tweets['word_tokens_nopunct'] = train_tweets['no_hashtags'].apply(lambda x: word_tokenize(x))\ntrain_tweets['word_tokens_alpha'] = train_tweets['word_tokens'].apply(lambda x: alpha_tokens(x))\ntrain_tweets['stemmed'] = train_tweets['word_tokens_alpha'].apply(lambda x: stemming(x))\ntrain_tweets['lemmatized'] = train_tweets['word_tokens_alpha'].apply(lambda x: lemmatizing(x))\ntrain_tweets['clean_text'] = train_tweets['word_tokens_alpha'].apply(lambda x: clean_text(x))\ntrain_tweets['clean_text_stemmed'] = train_tweets['stemmed'].apply(lambda x: clean_text(x))\ntrain_tweets['clean_text_lemmatized'] = train_tweets['lemmatized'].apply(lambda x: clean_text(x))\n","78dbf90a":"test_tweets['sentence_tokens'] = test_tweets['text'].apply(lambda x: sent_tokenize(x))\ntest_tweets['word_tokens'] = test_tweets['text'].apply(lambda x: word_tokenize(x))\ntest_tweets['num_chars'] = test_tweets['text'].apply(lambda x: len(x))\ntest_tweets['num_sentences'] = test_tweets['sentence_tokens'].apply(lambda x: len(x))\ntest_tweets['num_words'] = test_tweets['word_tokens'].apply(lambda x: len(x))\ntest_tweets['num_hashtags'] = test_tweets['text'].apply(lambda x: count_hashtags(x))\ntest_tweets['num_numeric'] = test_tweets['text'].apply(lambda x: count_numerics(x))\ntest_tweets['no_punct'] = test_tweets['text'].apply(lambda x: remove_punct(x))\ntest_tweets['no_hashtags'] = test_tweets['no_punct'].apply(lambda x: remove_hashtags(x))\ntest_tweets['word_tokens_nopunct'] = test_tweets['no_hashtags'].apply(lambda x: word_tokenize(x))\ntest_tweets['word_tokens_alpha'] = test_tweets['word_tokens'].apply(lambda x: alpha_tokens(x))\ntest_tweets['stemmed'] = test_tweets['word_tokens_alpha'].apply(lambda x: stemming(x))\ntest_tweets['lemmatized'] = test_tweets['word_tokens_alpha'].apply(lambda x: lemmatizing(x))\ntest_tweets['clean_text'] = test_tweets['word_tokens_alpha'].apply(lambda x: clean_text(x))\ntest_tweets['clean_text_stemmed'] = test_tweets['stemmed'].apply(lambda x: clean_text(x))\ntest_tweets['clean_text_lemmatized'] = test_tweets['lemmatized'].apply(lambda x: clean_text(x))\n","8bacb48a":"train_tweets.head(2)","4ed8bae9":"#Use this to look at locations of tweets and potentially plot on a map \n\n# train_tweets[train_tweets.location.isnull() == False]","da205585":"#Look at the differences in distribution for various measures, between the real and non-real disaster tweets. Significant differences may indicate a potential feature\n#num_chars = number of characters in tweet, num_words = number of words, num_sentences = number of sentences\n\nsns.set_palette(sns.color_palette(['#0000FF', '#00FF00']))\nsns.set_style(\"darkgrid\")\n\nfig, ax = plt.subplots(1,3)\nfig.set_size_inches(24, 12)\nsns.histplot(data = train_tweets, x='num_chars', hue='target', alpha=0.2, ax=ax[0])\nsns.histplot(data = train_tweets, x='num_words', hue='target', alpha=0.2, ax=ax[1])\nsns.histplot(data = train_tweets, x='num_sentences', hue='target', alpha=0.2, ax=ax[2], shrink=5)\nfig.show()","854525e2":"#num_hashtags = number of hashtags\n\nfig, ax = plt.subplots(figsize=(16,9))\nsns.histplot(data = train_tweets, x='num_hashtags', hue='target', alpha=0.2)\nfig.show()","e84aa452":"#num_numeric = number of numeric characters used \n\nfig, ax = plt.subplots(figsize=(16,9))\nsns.histplot(data = train_tweets, x='num_numeric', hue='target', alpha=0.2, shrink=2)\nfig.show()","f3b67148":"train_tweets.head(2)","63e90a9f":"#create train_test_splits for the three columns of interest. We will repeat and compare results for each of the transformations.\n\n# X_train_text, X_test_text, y_train_text, y_test_text = train_test_split(train_tweets['clean_text'], train_tweets['target'], test_size=0.2, random_state=10)\n# X_train_stem, X_test_stem, y_train_stem, y_test_stem = train_test_split(train_tweets['clean_text_stemmed'], train_tweets['target'], test_size=0.2, random_state=10)\n# X_train_lemma, X_test_lemma, y_train_lemma, y_test_lemma = train_test_split(train_tweets['clean_text_lemmatized'], train_tweets['target'], test_size=0.2, random_state=10)","2195b13c":"#Firstly, create BOW\/TFIDF using different n_grams for count_vectorizer_text, then repeat for stem and lemma (maybe with a loop?)\n\n# count_vectorizer_text = CountVectorizer(stop_words='english')\n#count_vectorizer_stem = CountVectorizer(stop_words='english')\n#count_vectorizer_lemma = CountVectorizer(stop_words='english')\n# count_train_text = count_vectorizer_text.fit_transform(X_train_text)\n#count_train_stem = count_vectorizer_stem.fit_transform(X_train_stem)\n#count_train_lemma = count_vectorizer_lemma.fit_transform(X_train_lemma)\n","5333d714":"#Create functions that return text vectorizers, and train_test split data that has been transformed into sparse matrix DFs\n\ndef create_BOW(X, y):\n    '''Create models for different ngrams specifications, and return a list of DataFrames.'''\n    ngrams = [(1,1), (1,2), (2,2), (1,3), (2,3), (3,3)]\n    text_vectors = []\n    train_X_counts = []\n    test_X_counts = []\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n    \n    for i in range(len(ngrams)):\n        text_vectors.append(CountVectorizer(ngram_range=ngrams[i], stop_words='english').fit(X_train))\n        train_X_counts.append(text_vectors[i].transform(X_train))\n        test_X_counts.append(text_vectors[i].transform(X_test))\n    \n    train_X_counts_transformed = [pd.DataFrame(train_X_counts[i].toarray(), columns=text_vectors[i].get_feature_names()) for i in range(len(ngrams))]\n    test_X_counts_transformed = [pd.DataFrame(test_X_counts[i].toarray(), columns=text_vectors[i].get_feature_names()) for i in range(len(ngrams))]\n    \n    return(train_X_counts_transformed, test_X_counts_transformed, y_train, y_test, text_vectors)\n\ndef create_tfidf(X, y):\n    '''Create models for different ngrams specifications, and return a list of DataFrames.'''\n    ngrams = [(1,1), (1,2), (2,2), (1,3), (2,3), (3,3)]\n    text_vectors = []\n    train_X_tfidf = []\n    test_X_tfidf = []\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n    \n    for i in range(len(ngrams)):\n        text_vectors.append(TfidfVectorizer(ngram_range=ngrams[i],).fit(X_train))\n        train_X_tfidf.append(text_vectors[i].transform(X_train))\n        test_X_tfidf.append(text_vectors[i].transform(X_test))\n    \n    train_X_tfidf_transformed = [pd.DataFrame(train_X_tfidf[i].toarray(), columns=text_vectors[i].get_feature_names()) for i in range(len(ngrams))]\n    test_X_tfidf_transformed = [pd.DataFrame(test_X_tfidf[i].toarray(), columns=text_vectors[i].get_feature_names()) for i in range(len(ngrams))]\n    \n    return(train_X_tfidf_transformed, test_X_tfidf_transformed, y_train, y_test, text_vectors)","cd9895e9":"#Create X_train_DFs, X_test_DFs, y_train, y_test, and vectorizers for each of the processed columns (cleaned, stemmed, lemmatized)\n\nclean_train_BOWS, clean_test_BOWS, clean_y_train, clean_y_test, clean_vectors_BOW  = create_BOW(train_tweets['clean_text'], train_tweets['target'])\nstem_train_BOWS, stem_test_BOWS, stem_y_train, stem_y_test, stem_vectors_BOW = create_BOW(train_tweets['clean_text_stemmed'], train_tweets['target'])\nlemma_train_BOWS, lemma_test_BOWS, lemma_y_train, lemma_y_test, lemma_vectors_BOW = create_BOW(train_tweets['clean_text_lemmatized'], train_tweets['target'])\n\nclean_train_tfidf, clean_test_tfidf, clean_y_train_tfidf, clean_y_test_tfidf, clean_vectors_tfidf = create_tfidf(train_tweets['clean_text'], train_tweets['target'])\nstem_train_tfidf, stem_test_tfidf, stem_y_train_tfidf, stem_y_test_tfidf, stem_vectors_tfidf = create_tfidf(train_tweets['clean_text_stemmed'], train_tweets['target'])\nlemma_train_tfidf, lemma_test_tfidf, lemma_y_train_tfidf, lemma_y_test_tfidf, lemma_vectors_tfidf = create_tfidf(train_tweets['clean_text_lemmatized'], train_tweets['target'])","42c57e79":"#Create function to create logistic regression models and return list of prediction arrays and the models used\n\ndef log_reg_predictions(X_train_DFs, X_test_DFs, y_train):\n    '''Generate a list of arrays that contain y predictions.'''\n    log_reg_models = []\n    y_predicted_list = []\n    \n    for i in range(len(X_train_DFs)):\n        log_reg_models.append(LogisticRegression().fit(X_train_DFs[i], y_train))\n        y_predicted_list.append(log_reg_models[i].predict(X_test_DFs[i]))\n    \n    return(y_predicted_list, log_reg_models)","a5aac0e3":"def log_reg_predictions(X_train_DFs, X_test_DFs, y_train):\n    '''Generate a list of arrays that contain y predictions.'''\n    log_reg_models = []\n    y_predicted_list = []\n    y_predicted_lists_proba = []\n    \n    for i in range(len(X_train_DFs)):\n        log_reg_models.append(LogisticRegression().fit(X_train_DFs[i], y_train))\n        y_predicted_list.append(log_reg_models[i].predict(X_test_DFs[i]))\n        y_predicted_lists_proba.append(log_reg_models[i].predict_proba(X_test_DFs[i]))\n    \n    return(log_reg_models, y_predicted_list, y_predicted_lists_proba)","7956bf26":"#Create models and predictions\n\n#BOW\nclean_BOW_models, clean_BOW_predictions, clean_BOW_probas  = log_reg_predictions(clean_train_BOWS, clean_test_BOWS, clean_y_train)\nstem_BOW_models, stem_BOW_predictions, stem_BOW_probas = log_reg_predictions(stem_train_BOWS, stem_test_BOWS, stem_y_train)\nlemma_BOW_models, lemma_BOW_predictions, lemma_BOW_probas = log_reg_predictions(lemma_train_BOWS, lemma_test_BOWS, lemma_y_train)\n\n#TFIDF\nclean_tfidf_models, clean_tfidf_predictions, clean_tfidf_probas = log_reg_predictions(clean_train_tfidf, clean_test_tfidf, clean_y_train_tfidf)\nstem_tfidf_models, stem_tfidf_predictions, stem_tfidf_probas = log_reg_predictions(stem_train_tfidf, stem_test_tfidf, stem_y_train_tfidf)\nlemma_tfidf_models, lemma_tfidf_predictions, lemma_tfidf_probas = log_reg_predictions(lemma_train_tfidf, lemma_test_tfidf, lemma_y_train_tfidf)","68a3d83c":"#Create function to calculate f1 scores for different thresholds, then loop through the different ngram models that have been created\n\nthresholds = np.round(np.arange(0.25,0.75,0.05), 2)\n\ndef f1_scorer(threshold, prob_preds, y_test):\n    scores = [1 if i[1] > threshold else 0 for i in prob_preds]\n    f1score = f1_score(y_test, scores)\n    return f1score\n\n#f1scores = [threshold_metrics(i) for i in thresholds]","8f370a2e":"#clean_BOW_scores = []\n#for i in thresholds:\n#    for j in clean_BOW_probas:\n#        clean_BOW_scores.append(f1_scorer(i, j, clean_y_test))","81c6fda9":"#clean_BOW_scores.index(max(clean_BOW_scores)) \/\/ len(thresholds)\n#clean_BOW_scores.index(max(clean_BOW_scores)) % len(clean_BOW_probas)","a4681099":"def max_model_score(probas, y_test):\n    scores = []\n    for i in thresholds:\n        for j in probas:\n            scores.append(f1_scorer(i, j, y_test))\n    return(F'Maximum f1score is {max(scores)}, when the threshold is {thresholds[scores.index(max(scores)) \/\/ len(thresholds)]} and the model index is {scores.index(max(scores)) % len(probas)}.')","8e2d0a92":"max_scores = []\n\nmax_scores.append(max_model_score(clean_BOW_probas, clean_y_test))\nmax_scores.append(max_model_score(stem_BOW_probas, stem_y_test))\nmax_scores.append(max_model_score(lemma_BOW_probas, lemma_y_test))\nmax_scores.append(max_model_score(clean_tfidf_probas, clean_y_test_tfidf))\nmax_scores.append(max_model_score(stem_tfidf_probas, stem_y_test_tfidf))\nmax_scores.append(max_model_score(lemma_tfidf_probas, lemma_y_test_tfidf))\n\nfor i in max_scores:\n    print(i)","1df4e9d9":"#Highest score is 0.7624\n#Use the BOW model for lemmatized text, that uses ngram (1,1)\n\nsubmission_X = lemma_vectors_BOW[0].transform(test_tweets['clean_text_lemmatized'])\n\n#preds = lemma_BOW_models[0].predict(submission_X)\nprob_preds = lemma_BOW_models[0].predict_proba(submission_X)","231dd85e":"preds = [1 if i[1] > 0.5 else 0 for i in prob_preds]","f4ccade7":"#Submit to comptetition\n\noutput = pd.DataFrame({'id': test_tweets.id,\n                       'target': preds})\noutput.to_csv('submission.csv', index=False)","973dc368":"#from xgboost import XGBClassifier\n\n\n#model = XGBClassifier(n_estimators=1000, learning_rate=0.07)\n#model.fit(lemma_train_BOWS[0], lemma_y_train, \n#             early_stopping_rounds=10, \n#             eval_set=[(lemma_test_BOWS[0], lemma_y_test)],\n#              verbose=False)\n#preds = model.predict(lemma_test_BOWS[0])\n#f1score = f1_score(lemma_y_test, preds)\n\n#f1score\n                       ","a93ba1de":"#Next tasks\n\n#use threshold testing\n#repeat for different models (RF, SVM, XGBoost)\n#include engineered features\n#consider capping the number of features, i.e. top 1000 tokens","efa39460":"Potential feature? >1 hashtag may indicate that it is a true disaster","088cee89":"Potential feature? 0 numeric chars may indicate that it is a false disater"}}