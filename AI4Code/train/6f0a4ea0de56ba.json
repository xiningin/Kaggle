{"cell_type":{"ee208e79":"code","ffced2a6":"code","f0733a11":"code","382cb9e9":"code","974a2af5":"code","170e48fa":"code","b6f3963e":"code","8cc3f75e":"code","a4902e09":"code","1a0f8ae8":"code","800391bc":"code","1e546bf4":"code","408f5c41":"code","c8f266ff":"code","0504a8c4":"code","2862ef4c":"code","8677ac0e":"code","50bcf624":"code","f23ebd22":"code","c38dd2c4":"code","5337e14f":"code","6d83873e":"code","63ea15e2":"code","e6543b6c":"code","ca7e0690":"code","a76530e5":"code","8ea1aed2":"code","6b27dd33":"code","1ec57562":"code","e96803b6":"code","7f5adda3":"code","61de7e9e":"code","f7802ffc":"code","07744509":"code","e9f9903d":"code","e6444118":"code","cf29e095":"code","ba02430a":"code","50670d3d":"code","162e35b4":"code","1e5ed18e":"code","2a025bcd":"code","66bf3e67":"code","f73c59e1":"code","f188e451":"code","325735a0":"code","b482d97e":"code","69f66e4c":"code","7b13dcf9":"code","8d3749b7":"code","305ed5bb":"code","5a68a9a6":"code","ea048926":"code","e6dd0176":"code","60cc3af4":"code","88042973":"code","5a20a54c":"code","c403a090":"code","1091c5db":"code","42aaf196":"code","a5d46736":"code","d88cd7a1":"code","57c90ef3":"code","b41342a0":"code","83677673":"code","d9bff3ce":"code","8c73848c":"markdown","4ba0b1a6":"markdown","9329fb78":"markdown","0cbaf08d":"markdown","55bc6c46":"markdown","43e739e4":"markdown","c92c7250":"markdown","e510d7bb":"markdown","840aa891":"markdown","066a2498":"markdown","e8ce6889":"markdown","de57cde0":"markdown","8ce6c6f7":"markdown","db8352c4":"markdown","7fff2a35":"markdown","7131899c":"markdown","f803a04d":"markdown","54ce04d3":"markdown","3787c6ec":"markdown","3fde04c2":"markdown","57f1f31e":"markdown","35ec9363":"markdown","7d181484":"markdown","34cb5de7":"markdown","91e9b577":"markdown","f346eda7":"markdown","86aa23d9":"markdown","7b2983be":"markdown","11e37d43":"markdown","15ae0d56":"markdown","5cfa5a98":"markdown","b03bcfa0":"markdown","14bf730f":"markdown","fc2d12fc":"markdown","735d67d8":"markdown","9493c757":"markdown","9cf89141":"markdown","9fd9d8f3":"markdown","79443dc2":"markdown","e4ffd9c8":"markdown","f6534b91":"markdown","f133b47c":"markdown","5ba679c6":"markdown","2c40bb96":"markdown","800bf446":"markdown","77893891":"markdown","557e1873":"markdown","f89a9ebc":"markdown","989fa824":"markdown","891e9cd7":"markdown","af77457d":"markdown","3b856b24":"markdown","dda5bc07":"markdown","8c157ef0":"markdown","93e025b7":"markdown","c31f0c6e":"markdown","d86b30b8":"markdown","a8f8a2b5":"markdown","4527f8b0":"markdown","ebeca267":"markdown","b5c70e00":"markdown","4d774e88":"markdown","d2ba8697":"markdown","86961eb9":"markdown","ad601760":"markdown","32b813c0":"markdown","eded18d7":"markdown","a8fa8bbf":"markdown","9c0b6789":"markdown","864f8b41":"markdown","196ee137":"markdown","399c908c":"markdown","37cde298":"markdown","e1f7f343":"markdown","ba336f1d":"markdown","3f6c66bb":"markdown","9a2fcc04":"markdown","8144a873":"markdown","149f3701":"markdown","4b88e7bc":"markdown","22ec76e1":"markdown","c6f20fc8":"markdown","6e94220b":"markdown","f3699bd2":"markdown"},"source":{"ee208e79":"# comandos m\u00e1gicos que n\u00e3o se comunicam com a linguagem Python e sim diretamente com o kernel do Jupyter\n# come\u00e7am com %\n\n%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","ffced2a6":"# importando os principais m\u00f3dulos que usaremos ao longo da aula\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport sklearn.datasets\nimport sklearn.model_selection\nimport sklearn.metrics\nimport sklearn.ensemble\n\n# voc\u00ea tamb\u00e9m pode importar apenas uma parte de cada m\u00f3dulo, por exemplo:\n# from sklearn.ensemble import RandomForestRegressor()","f0733a11":"boston = sklearn.datasets.load_boston()\nboston","382cb9e9":"print(boston.DESCR)","974a2af5":"X, y = boston.data, boston.target","170e48fa":"X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, \n                                                                            test_size = 0.1, \n                                                                            random_state = 0)","b6f3963e":"m = sklearn.ensemble.RandomForestRegressor()","8cc3f75e":"m.fit(X_train, y_train)","a4902e09":"y_test_pred = m.predict(X_test)","1a0f8ae8":"# plotando valores verdadeiros contra predi\u00e7\u00f5es\nplt.plot(y_test, y_test_pred,'.')\n\n# plotando a reta x=y\nplt.plot(plt.gca().get_ylim(), plt.gca().get_ylim())\n\n# legenda dos eixos\nplt.xlabel('y_test')\nplt.ylabel('y_test_pred');","800391bc":"mae = sklearn.metrics.mean_absolute_error(y_test, y_test_pred)\nr2 = sklearn.metrics.r2_score(y_test, y_test_pred)\n\nprint(f'MAE: {mae}')\nprint(f'R2: {r2}')","1e546bf4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport sklearn.datasets\nimport sklearn.model_selection\nimport sklearn.metrics\nimport sklearn.ensemble\nimport sklearn.neural_network\n\nimport pandas as pd\n\nboston = sklearn.datasets.load_boston()\nX, y = boston.data, boston.target\n\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, \n                                                                            test_size = 0.1, \n                                                                            random_state = 0)\n\n#X_train = X_train[:,(5,12)]\n#X_test = X_test[:,(5,12)]\n\n# Modelo Random Forest\nm = sklearn.ensemble.RandomForestRegressor(random_state=0)\nm.fit(X_train, y_train)\nimportance = m.feature_importances_\n\ny_test_pred = m.predict(X_test)\n\n# Modelo NN MLP\nm2 = sklearn.neural_network.MLPRegressor(random_state=0, max_iter=10000, hidden_layer_sizes=(20,20,20,20,20))\nm2.fit(X_train, y_train)\ny_test_pred2 = m2.predict(X_test)\n\n# plotando valores verdadeiros contra predi\u00e7\u00f5es\nplt.figure()\nplt.plot(y_test, y_test_pred,'.', label='RF')\nplt.plot(y_test, y_test_pred2,'.', label='NLP')\nplt.plot(plt.gca().get_ylim(), plt.gca().get_ylim(), label='y=x')\nplt.xlabel('y_test')\nplt.ylabel('y_test_pred');\nplt.legend();\n\n# m\u00e9tricas de avalia\u00e7\u00e3o\nmae = sklearn.metrics.mean_absolute_error(y_test, y_test_pred)\nmse = sklearn.metrics.mean_squared_error(y_test, y_test_pred)\nmaxerror = sklearn.metrics.max_error(y_test, y_test_pred)\nr2 = sklearn.metrics.r2_score(y_test, y_test_pred)\n\nmae2 = sklearn.metrics.mean_absolute_error(y_test, y_test_pred2)\nmse2 = sklearn.metrics.mean_squared_error(y_test, y_test_pred2)\nmaxerror2 = sklearn.metrics.max_error(y_test, y_test_pred2)\nr22 = sklearn.metrics.r2_score(y_test, y_test_pred2)\n\nnomes =['MAE','MSE','Erro m\u00e1ximo','R^2']\n\ndados = {'Random Forest': [mae,mse,maxerror,r2],\n         'NLP': [mae2,mse2,maxerror2,r22]}\n\ndf1 = pd.DataFrame(dados, index = nomes)\ndisplay(df1)\n\nplt.figure()\nplt.bar(boston.feature_names, importance)\nplt.xticks(rotation=45)\nplt.show()\n\n\n","408f5c41":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport sklearn.datasets\nimport sklearn.model_selection\nimport sklearn.metrics\nimport sklearn.ensemble\nimport sklearn.neural_network\n\nimport pandas as pd\n\ndiabetes = sklearn.datasets.load_diabetes()\nX, y = diabetes.data, diabetes.target\n\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, \n                                                                            test_size = 0.1, \n                                                                            random_state = 0)\n\n# Modelo Random Forest\nm = sklearn.ensemble.RandomForestRegressor(random_state=0)\nm.fit(X_train, y_train)\nimportance = m.feature_importances_\n\ny_test_pred = m.predict(X_test)\n\n# Modelo NN MLP\nm2 = sklearn.neural_network.MLPRegressor(random_state=0, max_iter=10000, hidden_layer_sizes=(20,20,20,20))\nm2.fit(X_train, y_train)\ny_test_pred2 = m2.predict(X_test)\n\n# plotando valores verdadeiros contra predi\u00e7\u00f5es\nplt.figure()\nplt.plot(y_test, y_test_pred,'.', label='RF')\nplt.plot(y_test, y_test_pred2,'.', label='NLP')\nplt.plot(plt.gca().get_ylim(), plt.gca().get_ylim(), label='y=x')\nplt.xlabel('y_test')\nplt.ylabel('y_test_pred');\nplt.legend();\n\n# m\u00e9tricas de avalia\u00e7\u00e3o\nmae = sklearn.metrics.mean_absolute_error(y_test, y_test_pred)\nmse = sklearn.metrics.mean_squared_error(y_test, y_test_pred)\nmaxerror = sklearn.metrics.max_error(y_test, y_test_pred)\nr2 = sklearn.metrics.r2_score(y_test, y_test_pred)\n\nmae2 = sklearn.metrics.mean_absolute_error(y_test, y_test_pred2)\nmse2 = sklearn.metrics.mean_squared_error(y_test, y_test_pred2)\nmaxerror2 = sklearn.metrics.max_error(y_test, y_test_pred2)\nr22 = sklearn.metrics.r2_score(y_test, y_test_pred2)\n\nnomes =['MAE','MSE','Erro m\u00e1ximo','R^2']\n\ndados = {'Random Forest': [mae,mse,maxerror,r2],\n         'NLP': [mae2,mse2,maxerror2,r22]}\n\ndf1 = pd.DataFrame(dados, index = nomes)\ndisplay(df1)\n\nplt.figure()\nplt.bar(diabetes.feature_names, importance)\nplt.xticks(rotation=45)\nplt.show()\n","c8f266ff":"PATH = \"..\/input\/bluebook-for-bulldozers\/\"\n\ndf_raw = pd.read_csv(f'{PATH}Train.zip',\n                     compression='zip', \n                     low_memory=False, \n                     parse_dates=[\"saledate\"])","0504a8c4":"df_raw.shape","2862ef4c":"with pd.option_context(\"display.max_columns\", 100): \n    display(df_raw)\n    display(df_raw.describe(include='all'))","8677ac0e":"df_raw.dtypes","50bcf624":"for n, c in df_raw.items():\n    if not pd.api.types.is_numeric_dtype(c) and not pd.api.types.is_datetime64_any_dtype(c):\n        print(f'{n} ({len(c.unique())}): {c.unique()}')","f23ebd22":"# para poder importar m\u00f3dulos que n\u00e3o estejam nos kernels do kaggle, \n# devemos instal\u00e1-los com o pip\n\n!pip install missingno","c38dd2c4":"import missingno\n\nmissingno.bar(df_raw)\nmissingno.matrix(df_raw);","5337e14f":"df_raw.SalePrice = np.log(df_raw.SalePrice)","6d83873e":"def pre_process (df):\n    \n    new_df = pd.DataFrame()\n    \n    for n,c in df.items():\n                \n        if pd.api.types.is_numeric_dtype(c):\n            # substituindo NaN numericos pelas medianas de cada coluna\n            new_df[n] = c.fillna(value=c.median())\n        else:\n            # interpretando o que nao for numerico como variaveis categoricas \n            # e transformando cada categoria em um numero\n            new_df[n] = pd.Categorical(c.astype('category').cat.as_ordered()).codes+1\n    \n    return new_df     ","63ea15e2":"df_proc = pre_process(df_raw)","e6543b6c":"X, y = df_proc.drop('SalePrice', axis=1), df_proc['SalePrice']","ca7e0690":"n_valid = 12000\nn_trn = len(df_proc)-n_valid\n\nX_treino, X_validacao = X[:n_trn].copy(), X[n_trn:].copy()\ny_treino, y_validacao = y[:n_trn].copy(), y[n_trn:].copy()\n\ny_treino.shape, y_validacao.shape","a76530e5":"def rmse(x,y): \n    \n    return np.sqrt(sklearn.metrics.mean_squared_error(x,y))\n\ndef display_score(m):\n    \n    res = [[rmse(m.predict(X_treino), y_treino), rmse(m.predict(X_validacao), y_validacao)],\n          [m.score(X_treino, y_treino), m.score(X_validacao, y_validacao)]]\n    \n    score = pd.DataFrame(res, index=['RMSE','R2'], columns = ['Treino','Valida\u00e7\u00e3o'])\n    \n    if hasattr(m, 'oob_score_'): \n        score.loc['OOB R2'] = [m.oob_score_,'-']\n        \n    display(score)","8ea1aed2":"m_base = sklearn.ensemble.RandomForestRegressor(n_jobs=-1, oob_score = True, random_state = 0)\n%time m_base.fit(X_treino, y_treino)\ndisplay_score(m_base)","6b27dd33":"m = sklearn.ensemble.RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)","1ec57562":"def draw_tree(t, df, size=10, ratio=1, precision=0):\n   \n    import re\n    import graphviz\n    import sklearn.tree\n    import IPython.display\n    \n    s=sklearn.tree.export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,\n                                   special_characters=True, rotate=True, precision=precision)\n    IPython.display.display(graphviz.Source(re.sub('Tree {',\n       f'Tree {{ size={size}; ratio={ratio}', s)))","e96803b6":"draw_tree(m.estimators_[0], X_treino, precision=3)","7f5adda3":"m = sklearn.ensemble.RandomForestRegressor(n_estimators=1, bootstrap=False, n_jobs=-1, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)","61de7e9e":"display_score(m_base)","f7802ffc":"preds = np.stack([t.predict(X_validacao) for t in m_base.estimators_]).T\npreds_df = pd.DataFrame(preds)\n\npreds_df['medias'] = preds_df.mean(axis=1)\npreds_df['stds'] = preds_df.std(axis=1)\npreds_df['valor real'] = y_validacao.values\npreds_df","07744509":"plt.plot(y_validacao.values, preds_df.mean(axis=1), '.')\n\nplt.plot(plt.gca().get_ylim(), plt.gca().get_ylim());\n\nplt.xlabel('y_valid')\nplt.ylabel('y_valid_pred');","e9f9903d":"plt.plot([sklearn.metrics.r2_score(y_validacao, np.mean(preds[:,:i+1], axis=1)) for i in range(100)]);","e6444118":"m = sklearn.ensemble.RandomForestRegressor(n_estimators = 10, n_jobs=-1, oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)","cf29e095":"m = sklearn.ensemble.RandomForestRegressor(max_samples = 40000, n_jobs=-1, oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)","ba02430a":"m = sklearn.ensemble.RandomForestRegressor(min_samples_leaf = 3, max_features = 0.5, \n                                           n_jobs=-1, oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)","50670d3d":"m = sklearn.ensemble.RandomForestRegressor(n_estimators = 50, min_samples_leaf = 3, \n                                           max_features = 0.5, n_jobs=-1, \n                                           oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)","162e35b4":"#n_estimators: diminuir n\u00e3o prejudica significativamente nas m\u00e9tricas de treino e valida\u00e7\u00e3o e melhora muito o custo computacional (mantive 50 para compara\u00e7\u00e3o)\n#min_samples_leaf: identifiquei que 4 fica melhor que 3 ou 5, talvez seja um valor \u00f3timo para esse conjunto de dados e essa configura\u00e7\u00e3o\n#max_features: mexer no max_features n\u00e3o melhorou o treinamento\n#max_leaf_nodes: n\u00e3o surtiu efeito positivo nas m\u00e9tricas de treino e valida\u00e7\u00e3o, na realidade esse hiperpar\u00e2metro compete com o min_samples_leaf, que apresentou melhora significativa\n#min_impurity_decrease: o aumento desse hiperpar\u00e2metro piorou muito as m\u00e9tricas de treino e valida\u00e7\u00e3o\n#bootstrap: n\u00e3o consegui rodar com a op\u00e7\u00e3o false (aparece um erro)\n#verbose: somente mostra um log mais detalhado do processo de treinamento\n#warm_start: partir da solu\u00e7\u00e3o anterior n\u00e3o apresentou benef\u00edcio\n#ccp_alpha: o aumento desse hiperpar\u00e2metro piorou muito as m\u00e9tricas de treino e valida\u00e7\u00e3o\n#max_samples: limitar o n\u00famero de amostras n\u00e3o melhorou o processo de treinamento\n\nm = sklearn.ensemble.RandomForestRegressor(n_estimators = 50, min_samples_leaf = 4, \n                                           max_features = 0.5, n_jobs=-1,\n                                           oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)","1e5ed18e":"def plotar_importancias(modelo, tags, n=10):\n    \n    fig, ax = plt.subplots(1,2, figsize = (20,4))\n\n    coefs = []\n    abs_coefs = []\n\n    if hasattr(modelo,'coef_'):\n        imp = modelo.coef_\n    elif hasattr(modelo,'feature_importances_'):\n        imp = modelo.feature_importances_\n    else:\n        print('sorry, nao vai rolar!')\n        return\n\n    coefs = (pd.Series(imp, index = tags))\n    coefs.plot(use_index=False, ax=ax[0]);\n    abs_coefs = (abs(coefs)\/(abs(coefs).sum()))\n    abs_coefs.sort_values(ascending=False).plot(use_index=False, ax=ax[1],marker='.')\n\n    ax[0].set_title('Import\u00e2ncias relativas das vari\u00e1veis')\n    ax[1].set_title('Import\u00e2ncias relativas das vari\u00e1veis - ordem decrescente')\n\n    abs_coefs_df = pd.DataFrame(np.array(abs_coefs).T,\n                                columns = ['Importancias'],\n                                index = tags)\n\n    df = abs_coefs_df['Importancias'].sort_values(ascending=False)\n    \n    print(df.iloc[0:n])\n    plt.figure()\n    df.iloc[0:n].plot(kind='barh', figsize=(15,0.25*n), legend=False)\n    \n    return df","2a025bcd":"imp = plotar_importancias(m, X_validacao.columns,30)","66bf3e67":"to_keep = imp[imp>0.005].index\nto_keep.shape","f73c59e1":"X_treino = X_treino[to_keep]\nX_validacao = X_validacao[to_keep]","f188e451":"def dendogram_spearmanr(df, tags):\n\n    import scipy.cluster.hierarchy\n    import scipy.stats\n    \n    corr = np.round(scipy.stats.spearmanr(df).correlation, 4)\n    corr_condensed = scipy.cluster.hierarchy.distance.squareform(1-corr)\n    z = scipy.cluster.hierarchy.linkage(corr_condensed, method='average')\n    fig = plt.figure(figsize=(18,8))\n    dendrogram = scipy.cluster.hierarchy.dendrogram(z, labels=tags, orientation='left', leaf_font_size=16)\n    plt.show()","325735a0":"dendogram_spearmanr(X_treino, X_treino.columns)","b482d97e":"def get_oob(X):\n    m = sklearn.ensemble.RandomForestRegressor(n_estimators=30, min_samples_leaf=5, \n                                               max_features=0.6, n_jobs=-1, max_samples = 100000,\n                                               oob_score=True, random_state = 0)\n    m.fit(X, y_treino)\n    return m.oob_score_","69f66e4c":"get_oob(X_treino)","7b13dcf9":"for c in ('Grouser_Tracks', 'Hydraulics_Flow', 'Coupler_System',\n          'fiModelDesc', 'fiBaseModel','ProductGroupDesc', 'ProductGroup'):\n    print(c, get_oob(X_treino.drop(c, axis=1)))","8d3749b7":"to_drop = ['ProductGroupDesc', 'fiModelDesc', 'Grouser_Tracks', 'Hydraulics_Flow']\nget_oob(X_treino.drop(to_drop, axis=1))","305ed5bb":"X_treino = X_treino.drop(to_drop, axis=1)\nX_treino.shape","5a68a9a6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport sklearn.datasets\nimport sklearn.model_selection\nimport sklearn.metrics\nimport sklearn.ensemble\nimport sklearn.neural_network\n\nimport pandas as pd\n\nboston = sklearn.datasets.load_boston()\nX, y = boston.data, boston.target\n\nX_treino, X_validacao, y_treino, y_validacao = sklearn.model_selection.train_test_split(X, y, \n                                                                            test_size = 0.2, \n                                                                            random_state = 0)\n\n# Modelo Random Forest - Caso Base\nm = sklearn.ensemble.RandomForestRegressor(n_estimators = 130, min_samples_leaf = 1, \n                                           max_features = 0.99, n_jobs=-1,\n                                           oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)\n\n# An\u00e1lise de import\u00e2ncia das vari\u00e1veis\nimp = plotar_importancias(m, boston.feature_names,30)\ndisplay(boston.feature_names)\n#removendo vari\u00e1veis com imp<0.01\nX_treino = X_treino[:,[0,4,5,6,7,9,10,12]]\nX_validacao = X_validacao[:,[0,4,5,6,7,9,10,12]]\n\n# Modelo Random Forest - Removendo Vari\u00e1veis Irrelevantes\nm = sklearn.ensemble.RandomForestRegressor(n_estimators = 130, min_samples_leaf = 1, \n                                           max_features = 0.99, n_jobs=-1,\n                                           oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)\n\n# An\u00e1lise da correla\u00e7\u00e3o entre as vari\u00e1veis\n\ndendogram_spearmanr(X_treino, boston.feature_names)\n# removendo a vari\u00e1vel RM, pois ela tem correla\u00e7\u00e3o com AGE\nX_treino, X_validacao, y_treino, y_validacao = sklearn.model_selection.train_test_split(X, y, \n                                                                            test_size = 0.2, \n                                                                            random_state = 0)\n\nX_treino = X_treino[:,[0,4,6,7,9,10,12]]\nX_validacao = X_validacao[:,[0,4,6,7,9,10,12]]\n\nm = sklearn.ensemble.RandomForestRegressor(n_estimators = 130, min_samples_leaf = 1, \n                                           max_features = 0.99, n_jobs=-1,\n                                           oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)\n\n# Neste modelo n\u00e3o vale a pena remover vari\u00e1veis, nem pelo grau de import\u00e2ncia, nem pela correla\u00e7\u00e3o. \n# A remo\u00e7\u00e3o das vari\u00e1veis com importancia menor do que 0.01 n\u00e3o impactou de forma sens\u00edvel no R^2 da\n# valida\u00e7\u00e3o, por\u00e9m tamb\u00e9m n\u00e3o se refletiu em um ganho de tempo computacional.\n# A remo\u00e7\u00e3o da vari\u00e1vel RM, por ter correla\u00e7\u00e3o com AGE acarretou em uma queda forte do R^2, n\u00e3o se\n# refletindo em qualquer benef\u00edcio para o modelo.","ea048926":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport sklearn.datasets\nimport sklearn.model_selection\nimport sklearn.metrics\nimport sklearn.ensemble\nimport sklearn.neural_network\n\nimport pandas as pd\n\ndiabetes = sklearn.datasets.load_diabetes()\nX, y = diabetes.data, diabetes.target\n\nX_treino, X_validacao, y_treino, y_validacao = sklearn.model_selection.train_test_split(X, y, \n                                                                            test_size = 0.1, \n                                                                            random_state = 0)\n\ndisplay(X_treino.shape)\n\n# Modelo Random Forest - Caso Base\nm = sklearn.ensemble.RandomForestRegressor(n_estimators = 100, min_samples_leaf = 1,\n                                           n_jobs=-1, oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)\n\n#preds = np.stack([t.predict(X_validacao) for t in m.estimators_]).T\n#preds_df = pd.DataFrame(preds)\n\n#preds_df['medias'] = preds_df.mean(axis=1)\n#preds_df['stds'] = preds_df.std(axis=1)\n#preds_df['valor real'] = y_validacao\n#plt.plot([sklearn.metrics.r2_score(y_validacao, np.mean(preds[:,:i+1], axis=1)) for i in range(100)]);\n\n\n# An\u00e1lise de import\u00e2ncia das vari\u00e1veis\nimp = plotar_importancias(m, diabetes.feature_names,30)\ndisplay(diabetes.feature_names)\n#removendo vari\u00e1veis com imp<0.05\nX_treino = X_treino[:,[0,2,3,5,6,8,9]]\nX_validacao = X_validacao[:,[0,2,3,5,6,8,9]]\n\n# Modelo Random Forest - Removendo Vari\u00e1veis Irrelevantes\nm = sklearn.ensemble.RandomForestRegressor(n_estimators = 100, min_samples_leaf = 1,\n                                           n_jobs=-1, oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)\n\n# An\u00e1lise da correla\u00e7\u00e3o entre as vari\u00e1veis\ndendogram_spearmanr(X_treino, diabetes.feature_names)\n\n# Neste modelo n\u00e3o vale a pena remover vari\u00e1veis, nem pelo grau de import\u00e2ncia, nem pela correla\u00e7\u00e3o. \n# A remo\u00e7\u00e3o das vari\u00e1veis com importancia menor do que 0.05 impactou de forma sens\u00edvel no R^2 da\n# valida\u00e7\u00e3o, que j\u00e1 se mostrou muito baixo.\n# N\u00e3o h\u00e1 vari\u00e1veis fortemente correlacionadas.","e6dd0176":"def pre_process_OHE (df, max_cats = 10):\n    \n    new_df = pd.DataFrame()\n    \n    for n,c in df.items():\n                \n        if pd.api.types.is_numeric_dtype(c):\n            # substituindo NaN numericos pelas medianas de cada coluna\n            new_df[n] = c.fillna(value=c.median())\n        else:\n            # interpretando o que nao for numerico como variaveis categoricas \n            new_df[n] = pd.Categorical(c.astype('category').cat.as_ordered())\n            # transformando cada categoria em um numero, caso nao va fazer one hot encoding com ela\n            if len(c.astype('category').cat.categories) > max_cats:\n                new_df[n] = pd.Categorical(new_df[n]).codes+1\n    \n    # a fun\u00e7\u00e3o pd.get_dummies faz o one-hot encoding\n    return pd.get_dummies(new_df)","60cc3af4":"df_raw","88042973":"df_proc_ohe = pre_process_OHE(df_raw)\ndf_proc_ohe","5a20a54c":"X, y = df_proc_ohe.drop('SalePrice', axis=1), df_proc_ohe['SalePrice']\n\nn_valid = 12000\nn_trn = len(df_proc)-n_valid\n\nX_treino, X_validacao = X[:n_trn].copy(), X[n_trn:].copy()\ny_treino, y_validacao = y[:n_trn].copy(), y[n_trn:].copy()","c403a090":"m = sklearn.ensemble.RandomForestRegressor(n_estimators = 50, min_samples_leaf = 3, \n                                           max_features = 0.5, n_jobs=-1, \n                                           oob_score = True, random_state = 0)\n%time m.fit(X_treino, y_treino)\ndisplay_score(m)","1091c5db":"plotar_importancias(m, X_validacao.columns,30)","42aaf196":"!pip install treeinterpreter","a5d46736":"from treeinterpreter import treeinterpreter as ti","d88cd7a1":"row = X_validacao.values[np.newaxis,0]\n\nprediction, bias, contributions = ti.predict(m, row)\n\nidxs = np.argsort(contributions[0])\n[o for o in zip(X_validacao.columns[idxs], X_validacao.iloc[0][idxs], contributions[0][idxs])]","57c90ef3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport sklearn.datasets\nimport sklearn.model_selection\nimport sklearn.metrics\nimport sklearn.ensemble\nimport sklearn.neural_network\n\nimport pandas as pd\n\nboston = sklearn.datasets.load_boston()\nX, y = boston.data, boston.target\n\nX_treino, X_validacao, y_treino, y_validacao = sklearn.model_selection.train_test_split(X, y, \n                                                                            test_size = 0.2, \n                                                                            random_state = 0)\n\n# Modelo Random Forest - Caso Base\nm = sklearn.ensemble.RandomForestRegressor(n_estimators = 130, min_samples_leaf = 1, \n                                           max_features = 0.99, n_jobs=-1,\n                                           oob_score = True, random_state = 0)\nm.fit(X_treino, y_treino)\n\nrow = X_validacao[np.newaxis,0]\nprediction, bias, contributions = ti.predict(m, row)\n\nidxs = np.argsort(contributions[0])\n\nrotulos=boston.feature_names[idxs]\n\nfor o in range(12):\n    print(rotulos[o], row[0,o], contributions[0,o])","b41342a0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport sklearn.datasets\nimport sklearn.model_selection\nimport sklearn.metrics\nimport sklearn.ensemble\nimport sklearn.neural_network\n\nimport pandas as pd\n\ndiabetes = sklearn.datasets.load_diabetes()\nX, y = diabetes.data, diabetes.target\n\nX_treino, X_validacao, y_treino, y_validacao = sklearn.model_selection.train_test_split(X, y, \n                                                                            test_size = 0.1, \n                                                                            random_state = 0)\n\n# Modelo Random Forest - Caso Base\nm = sklearn.ensemble.RandomForestRegressor(n_estimators = 100, min_samples_leaf = 1,\n                                           n_jobs=-1, oob_score = True, random_state = 0)\nm.fit(X_treino, y_treino)\n\nrow = X_validacao[np.newaxis,0]\nprediction, bias, contributions = ti.predict(m, row)\n\nidxs = np.argsort(contributions[0])\n\nrotulos = np.array(diabetes.feature_names)\nrotulos = rotulos[idxs]\n\nfor o in range(8):\n    print(rotulos[o], row[0,o], contributions[0,o])","83677673":"# criando exemplo simples com tend\u00eancia linear\nN = 30\nx = np.arange(N)\ny = 2*x\n\n# adicionando ru\u00eddo\ny = y+ 3*np.random.randn(N)\n\n# separando em treino e teste\n\nn = int(N\/2)\n\n# se eu nao criar esse novo eixo em x a seguir, o sklearn reclama, \n# pq pra ele a array de dados preditores tem q ter 2 dimensoes:\nx = x[:,np.newaxis]   \n\nx_treino, y_treino = x[:n], y[:n]\nx_treino, y_treino = x[:n], y[:n]\n\nx_teste, y_teste = x[n:], y[n:]\nx_teste, y_teste = x[n:], y[n:]\n\n# especificando modelos\n\nimport sklearn.linear_model\nimport sklearn.neural_network\nimport sklearn.svm\nimport sklearn.neighbors\n\nmodelos = [sklearn.linear_model.LinearRegression(),\n           sklearn.neural_network.MLPRegressor(),\n           sklearn.ensemble.RandomForestRegressor(),\n           sklearn.neighbors.KNeighborsRegressor(),\n           sklearn.svm.SVR()]\n\n# preparando janela do gr\u00e1fico\nfig, ax = plt.subplots(1,5,figsize=(20,3))\n\n# calculando e plotando\nfor i in range(len(modelos)):\n    modelos[i].fit(x_treino, y_treino)\n    ax[i].plot(x, y)\n    ax[i].plot(x, modelos[i].predict(x),'.')\n    ax[i].set_title(modelos[i].__class__.__name__)\n    ax[i].axvline(n,ls='--',c='k')","d9bff3ce":"df_final = pre_process(df_raw)\n\nX, y = df_final.drop('SalePrice', axis=1)[to_keep].drop(to_drop, axis=1), df_final['SalePrice']\n\nm = sklearn.ensemble.RandomForestRegressor(min_samples_leaf = 3, \n                                           max_features = 0.5, n_jobs=-1, \n                                           oob_score = True, random_state = 0)\n%time m.fit(X, y)\nm.oob_score_","8c73848c":"Em suma, ajustando os hiperpar\u00e2metros da floresta, conseguimos, em rela\u00e7\u00e3o ao modelo base:\n\n- melhorar a m\u00e9trica de desempenho na terceira casa decimal. Na maioria das situa\u00e7\u00f5es pr\u00e1ticas isso n\u00e3o seria importante, mas pode valer milhares de d\u00f3lares em uma competi\u00e7\u00e3o Kaggle;\n- reduzir o esfor\u00e7o computacional para aproximadamente 20% do original.\n\n\u00c9 poss\u00edvel automatizar a busca pelos hiperpar\u00e2metros utilizando t\u00e9cnicas num\u00e9ricas de otimiza\u00e7\u00e3o, mas este tema ser\u00e1 tratado em uma pr\u00f3xima aula.","4ba0b1a6":"Calculando algumas m\u00e9tricas de desempenho:","9329fb78":"## Escola Piloto Virtual - PEQ\/COPPE\/UFRJ\n\n## Data Science e Machine Learning na Pr\u00e1tica - Introdu\u00e7\u00e3o e Aplica\u00e7\u00f5es na Ind\u00fastria de Processos\n\nEste notebook \u00e9 referente \u00e0 Aula 1 do curso, que trata de t\u00e9cnicas de regress\u00e3o utilizando modelos de florestas aleat\u00f3rias.","0cbaf08d":"[treeinterpreter](https:\/\/pypi.org\/project\/treeinterpreter\/) decomp\u00f5e uma predi\u00e7\u00e3o na soma *bias+contribui\u00e7\u00f5es*. O bias \u00e9 a m\u00e9dia da vari\u00e1vel predita. As contribui\u00e7\u00f5es refletem o quanto cada vari\u00e1vel contribui para afastar uma predi\u00e7\u00e3o em espec\u00edfico dessa m\u00e9dia.\n\nAs contribui\u00e7\u00f5es de cada vari\u00e1vel s\u00e3o calculadas por meio dos efeitos na predi\u00e7\u00e3o dos v\u00e1rios splits que a envolvem. Esse [artigo](http:\/\/blog.datadive.net\/interpreting-random-forests\/) detalha bem a ideia.\n\nVamos dar uma olhada nas contribui\u00e7\u00f5es da primeira linha do conjunto de valida\u00e7\u00e3o:","55bc6c46":"## Aspectos temporais\n\nO modelo de florestas aleat\u00f3rias \u00e9 \u00f3timo para capturar n\u00e3o-linearidades e representar dados sem estrutura matem\u00e1tica definida. No entanto, ele n\u00e3o consegue efetuar extrapola\u00e7\u00f5es e modelar tend\u00eancias temporais. Um exemplo simples ajuda a ilustrar:","43e739e4":"## An\u00e1lise de correla\u00e7\u00f5es\n\nUma an\u00e1lise de correla\u00e7\u00f5es \u00e9 \u00fatil para entender as rela\u00e7\u00f5es entre as vari\u00e1veis.\n\nA correla\u00e7\u00e3o mais utilizada para isso \u00e9 a [correla\u00e7\u00e3o de Pearson](https:\/\/pt.wikipedia.org\/wiki\/Coeficiente_de_correla%C3%A7%C3%A3o_de_Pearson), que mede o grau de *associa\u00e7\u00e3o linear* entre as vari\u00e1veis. Duas vari\u00e1veis s\u00e3o linearmente associadas se mudan\u00e7as em uma vari\u00e1vel implicam em mudan\u00e7as diretamente proporcionais na outra vari\u00e1vel.\n\nAqui usaremos a [correla\u00e7\u00e3o de Spearman](https:\/\/pt.wikipedia.org\/wiki\/Coeficiente_de_correla%C3%A7%C3%A3o_de_postos_de_Spearman), que mede o grau de *associa\u00e7\u00e3o monot\u00f4nica* entre as vari\u00e1veis. Duas vari\u00e1veis s\u00e3o monotonicamente associadas se mudan\u00e7as em uma vari\u00e1vel implicam em mudan\u00e7as no mesmo sentido (crescente ou decrescente) na outra vari\u00e1vel. \u00c9 uma concep\u00e7\u00e3o de associa\u00e7\u00e3o mais gen\u00e9rica do que a de Pearson.\n\nA fun\u00e7\u00e3o abaixo aceita um [DataFrame](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.html) e plota um gr\u00e1fico do tipo [dendograma](https:\/\/en.wikipedia.org\/wiki\/Dendrogram) mostrando as correla\u00e7\u00f5es de Spearman entre as vari\u00e1veis:","c92c7250":"A abordagem aqui apresentada \u00e9 baseada na solu\u00e7\u00e3o proposta por Jeremy Howard em seu excelente curso [Introduction to Machine Learning for Coders](http:\/\/course18.fast.ai\/ml).","e510d7bb":"***M\u00e3o na massa 2!***\n\n* Treine o modelo mais algumas vezes, variando os valores dos hiperpar\u00e2metros apresentados. Analise os efeitos nos resultados. Leia a [refer\u00eancia do modelo](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html) e fa\u00e7a o mesmo com outros hiperpar\u00e2metros que n\u00e3o discutimos. Tente superar o desempenho acima :)","840aa891":"O gr\u00e1fico foi gerado por meio de t\u00e9cnicas de [clusteriza\u00e7\u00e3o hier\u00e1rquica](https:\/\/en.wikipedia.org\/wiki\/Hierarchical_clustering), que separaram as vari\u00e1veis em grupos de acordo com as correla\u00e7\u00f5es entre elas. \u00c9 evidente que algumas vari\u00e1veis possuem alt\u00edssima correla\u00e7\u00e3o, como **GrouserTracks**, **Hydraulics_Flow** e **Coupler_System**, por exemplo. Isso significa que elas possuem a mesma informa\u00e7\u00e3o e s\u00e3o potencialmente redundantes.\n\nNas pr\u00f3ximas c\u00e9lulas, removeremos algumas vari\u00e1veis que o gr\u00e1fico indica como redundantes e verificaremos o efeito no OOB. Caso o efeito seja pequeno, podemos descartar as vari\u00e1veis.","066a2498":"Utilizando-a para visualizar a \u00e1rvore rec\u00e9m-treinada:","e8ce6889":"Utilizando a fun\u00e7\u00e3o para analisar as import\u00e2ncias do nosso \u00faltimo modelo:","de57cde0":"# Treinando a primeira floresta\n\nPara poupar tempo, vamos definir uma fun\u00e7\u00e3o, chamada **display_score**, que aceita um modelo treinado e imprime na tela as m\u00e9tricas $R^2$ e [RMSE](https:\/\/en.wikipedia.org\/wiki\/Root-mean-square_deviation) relativas ao treino e \u00e0 valida\u00e7\u00e3o:","8ce6c6f7":"Eliminamos por volta de 30 vari\u00e1veis irrelevantes.","db8352c4":"H\u00e1 um plat\u00f4 a partir do qual adicionar mais \u00e1rvores n\u00e3o faz muita diferen\u00e7a. Esse \u00e9 o comportamento esperado: a partir de um certo n\u00famero de \u00e1rvores, os ganhos de desempenho passam a ser muito pequenos.\n\nO default do [scikit-learn](https:\/\/scikit-learn.org\/) \u00e9 usar 100 \u00e1rvores. Mas da figura acima nota-se que muito antes disso o modelo atinge o plat\u00f4 de desempenho. Isso sugere a diminui\u00e7\u00e3o do n\u00famero de \u00e1rvores, de modo a economizar custo computacional.","7fff2a35":"Os dois primeiros modelos, [regress\u00e3o linear](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html) e [rede neural MLP](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPRegressor.html), capturam a tend\u00eancia linear com bastante clareza. As redes neurais, que ser\u00e3o tema da pr\u00f3xima aula, s\u00e3o bem-sucedidas na extrapola\u00e7\u00e3o porque possuem estrutura matem\u00e1tica bem-definida. Essa estrutura possibilita a captura da tend\u00eancia linear.\n\nO desempenho do modelo de florestas aleat\u00f3rias \u00e9 triste. Ocorre sobreajuste no treino e a baixa capacidade de extrapola\u00e7\u00e3o \u00e9 evidente no teste. O motivo \u00e9 o fato de o modelo n\u00e3o possuir estrutura matem\u00e1tica definida, j\u00e1 que se baseia puramente em parti\u00e7\u00f5es no conjunto de dados. Isso proporciona flexibilidade para a modelagem, mas o pre\u00e7o a ser pago \u00e9 justamente essa incapacidade de extrapola\u00e7\u00e3o.\n\nO quarto modelo, chamado de [k-vizinhos mais pr\u00f3ximos (kNN, k-nearest neighbors)](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsRegressor.html), tamb\u00e9m n\u00e3o tem estrutura matem\u00e1tica definida, pois [se baseia puramente nas dist\u00e2ncias entre os pontos](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm).\n\nO quinto modelo, [SVR](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVR.html), baseado em [m\u00e1quinas de vetores-suporte](https:\/\/en.wikipedia.org\/wiki\/Support_vector_machine), tem estrutura matem\u00e1tica bem-definida, mas \u00e9 inerentemente n\u00e3o-linear e por isso fracassa miseravelmente.","7131899c":"## n_estimators\n\n**n_estimators** \u00e9 o n\u00famero de \u00e1rvores na floresta. A regra para escolher esse valor \u00e9 simples: quanto mais \u00e1rvores, melhor a capacidade preditiva do modelo, mas maior o custo computacional.\n\nVamos dar uma olhada na rela\u00e7\u00e3o entre a m\u00e9trica $R^2$ e a quantidade de \u00e1rvores em nossa floresta:","f803a04d":"H\u00e1 vari\u00e1veis de alta cardinalidade: por exemplo, **fiModelDesc** tem 4999 valores poss\u00edveis!","54ce04d3":"Estabelecendo nossa refer\u00eancia:","3787c6ec":"## Out-of-bag score\n\nAcabamos de aprender que, devido \u00e0 amostragem com reposi\u00e7\u00e3o, cada \u00e1rvore ignora uma parcela de observa\u00e7\u00f5es.\n\nA m\u00e9trica [OOB (out-of-bag)](https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_ensemble_oob.html) se vale desse fato para medir a capacidade preditiva do modelo *sem a necessidade de um conjunto de teste em separado*. Para efetuar as predi\u00e7\u00f5es e calcular o OOB, cada \u00e1rvore utiliza os dados de treino que foram por ela ignorados. Como a \u00e1rvore n\u00e3o treinou o modelo com esses dados, eles efetivamente funcionam como um bom conjunto de teste!\n\nNo [scikit-learn](https:\/\/scikit-learn.org\/), \u00e9 preciso fornecer o par\u00e2metro **oob_score = True** para que o OOB seja calculado durante o treino.","3fde04c2":"## Removendo vari\u00e1veis pouco importantes\n\nVari\u00e1veis sem import\u00e2ncia podem ser descartadas, o que talvez melhore a acur\u00e1cia do modelo e certamente melhorar\u00e1 o desempenho computacional.\n\nSelecionando, por exemplo, apenas as que apresentam mais que 0,5% import\u00e2ncia:","57f1f31e":"## An\u00e1lise de contribui\u00e7\u00f5es\n\n\u00c1rvores e florestas podem ser interpretadas! Em particular, \u00e9 poss\u00edvel entender o *porqu\u00ea* de uma predi\u00e7\u00e3o em espec\u00edfico, analisando as decis\u00f5es que as \u00e1rvores tomam para chegar a essa predi\u00e7\u00e3o.\n\nO m\u00f3dulo [treeinterpreter](https:\/\/pypi.org\/project\/treeinterpreter\/) serve justamente para isso:","35ec9363":"# Pr\u00e9-processamento dos dados\n\nA primeira etapa de pr\u00e9-processamento ser\u00e1 aplicar a fun\u00e7\u00e3o logaritmo \u00e0 vari\u00e1vel predita:","7d181484":"As vari\u00e1veis preditoras e predita est\u00e3o armazenadas no objeto **boston** em seus elementos **data** e **target**, respectivamente. Vamos dar a elas nomes mais simp\u00e1ticos:","34cb5de7":"Separando o conjunto em X (vari\u00e1veis preditoras) e y (vari\u00e1vel predita):","91e9b577":"## Import\u00e2ncias das vari\u00e1veis\n\nO modelo de florestas aleat\u00f3rias calcula internamente um ranking de import\u00e2ncia das vari\u00e1veis. Para uma dada vari\u00e1vel, quanto maior a diminui\u00e7\u00e3o do erro em splits de decis\u00f5es tomadas com base nessa vari\u00e1vel, mais importante ela ser\u00e1. Esse ranking fica armazenado no atributo **feature_importances_** do modelo. \n\nNa pr\u00f3xima c\u00e9lula, definimos uma fun\u00e7\u00e3o que aceita um modelo e uma lista com os nomes das vari\u00e1veis, imprime na tela informa\u00e7\u00f5es relativas ao ranking de import\u00e2ncia e retorna um [DataFrame](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.html) com o ranking em si.","f346eda7":"Separando o conjunto em duas partes, o treino e a valida\u00e7\u00e3o:","86aa23d9":"Vamos encapsular o restante de nosso procedimento de pr\u00e9-processamento em uma fun\u00e7\u00e3o:","7b2983be":"A lista acima est\u00e1 organizada em ordem crescente de contribui\u00e7\u00f5es.\n\nDela podemos inferir que o que mais contribui para diminuir o pre\u00e7o do trator em quest\u00e3o \u00e9 seu tamanho pequeno (**ProductSize_Mini**) e o que mais contribui para aumentar o pre\u00e7o \u00e9 o ano de fabrica\u00e7\u00e3o (1999). O que parece fazer todo sentido.\n\nSe voc\u00ea acreditava na fal\u00e1cia de que os modelos de aprendizado de m\u00e1quina n\u00e3o possibilitavam interpreta\u00e7\u00e3o de seus resultados, espero que sua opini\u00e3o tenha mudado agora!!!","11e37d43":"# Tabela FIPE para Tratores de Esteira :D","15ae0d56":"Com esse procedimento, conseguimos diminuir ainda mais o n\u00famero de vari\u00e1veis.","5cfa5a98":"O objeto **boston** possui v\u00e1rios elementos: \n\n* data;\n* target;\n* feature_names;\n* DESCR;\n* filename.\n\nImprimindo a descri\u00e7\u00e3o (DESCR):","b03bcfa0":"Aten\u00e7\u00e3o: a reta da figura acima *n\u00e3o* \u00e9 o modelo! \u00c9 apenas a reta $x=y$. N\u00e3o poder\u00edamos visualizar o modelo no plano cartesiano, j\u00e1 que ele \u00e9 multidimensional. Al\u00e9m do mais, como veremos a seguir, o modelo de florestas aleat\u00f3rias \u00e9 n\u00e3o-linear, portanto n\u00e3o assumiria a forma de uma reta.","14bf730f":"As v\u00e1riaveis n\u00e3o-num\u00e9ricas s\u00e3o interpretadas pelo [pandas](https:\/\/pandas.pydata.org\/) como sendo do tipo gen\u00e9rico **object**. Nota-se acima que s\u00e3o a grande maioria!\n\nVamos analisar quantas e quais s\u00e3o as categorias em cada vari\u00e1vel categ\u00f3rica. No output abaixo, cada linha cont\u00e9m o nome de uma vari\u00e1vel, o n\u00famero de categorias entre par\u00eanteses e as categorias em si entre colchetes.","fc2d12fc":"# Modelo final\n\nChegou a hora de gerarmos nosso modelo final! Utilizaremos todos os dados para treinar. A m\u00e9trica de avalia\u00e7\u00e3o ser\u00e1 o OOB.","735d67d8":"***M\u00e3o na massa 3!***\n\n* Repita os procedimentos de an\u00e1lise de import\u00e2ncia e an\u00e1lise de correla\u00e7\u00e3o com os conjuntos de dados [Boston Housing](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.load_boston.html) e [Diabetes](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.load_diabetes.html) usados no come\u00e7o do notebook.","9493c757":"## Removendo vari\u00e1veis redundantes\n\nA fun\u00e7\u00e3o abaixo \u00e9 definida para agilizar as an\u00e1lises: ela aceita um conjunto X, efetua um treino e retorna o score OOB.","9cf89141":"### Dicas sobre aspectos temporais\n\nFlorestas aleat\u00f3rias n\u00e3o s\u00e3o designadas para modelar tend\u00eancias temporais, mas isso n\u00e3o significa que tais aplica\u00e7\u00f5es sejam imposs\u00edveis. No entanto, procedimentos adicionais s\u00e3o necess\u00e1rios para criar boas solu\u00e7\u00f5es. Algumas dicas de engenharia de caracter\u00edsticas a respeito:\n\n* adicionar vari\u00e1veis atrasadas no tempo para modelar autocorrela\u00e7\u00e3o (como [aqui](https:\/\/machinelearningmastery.com\/feature-selection-time-series-forecasting-python\/));\n* adicionar vari\u00e1veis que representem derivadas (*delta features*) para modelar tend\u00eancias (como [aqui](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S002002551931076X));\n* criar vari\u00e1veis que detalham datas em distintas granularidades (a fun\u00e7\u00e3o [add_datepart](https:\/\/docs.fast.ai\/tabular.core#add_datepart), da biblioteca [fastai](https:\/\/docs.fast.ai\/), \u00e9 \u00f3tima para isso).\n","9fd9d8f3":"## Treinando e visualizando uma \u00e1rvore de decis\u00e3o\n\nNa nomenclatura do [scikit-learn](https:\/\/scikit-learn.org\/stable\/), cada \u00e1rvore \u00e9 chamada de *estimador*. Para treinar apenas 1 \u00e1rvore, portanto, podemos fornecer o hiperpar\u00e2metro **n_estimators=1** para o modelo:","79443dc2":"## max_samples\n\nO hiperpar\u00e2metro **max_samples** restringe o n\u00famero de observa\u00e7\u00f5es que ser\u00e3o amostradas por cada \u00e1rvore durante o treino. \u00c9 um bom truque para quando os conjuntos de dados s\u00e3o muito grandes: todos os dados ficam dispon\u00edveis para o treino do modelo, mas a amostragem de cada \u00e1rvore se d\u00e1 apenas em subconjuntos de tamanho **max_samples**. Isso reduz o custo computacional e pode ajudar a atenuar problemas de sobreajuste.","e4ffd9c8":"# Interpreta\u00e7\u00e3o do modelo e engenharia de caracter\u00edsticas\n\nMuito se diz por a\u00ed que n\u00e3o \u00e9 poss\u00edvel interpretar as predi\u00e7\u00f5es feitas por modelos de aprendizado de m\u00e1quina, por eles serem complicados e totalmente emp\u00edricos. Nossa miss\u00e3o nesta se\u00e7\u00e3o \u00e9 mostrar que essa afirma\u00e7\u00e3o \u00e9 falsa.\n\nEm particular, o modelo de florestas aleat\u00f3rias pode fornecer muitas informa\u00e7\u00f5es sobre a natureza das predi\u00e7\u00f5es e as influ\u00eancias exercidas por cada vari\u00e1vel nos resultados. Essas informa\u00e7\u00f5es podem ser valiosas na importante atividade de [engenharia de caracter\u00edsticas](https:\/\/en.wikipedia.org\/wiki\/Feature_engineering) (mais conhecida pela express\u00e3o em ingl\u00eas *feature engineering*), que consiste na manipula\u00e7\u00e3o das vari\u00e1veis (colunas) do conjunto de dados com o objetivo de melhorar o desempenho dos modelos.","f6534b91":"### Treinando o modelo\n\nNosso modelo de regress\u00e3o ser\u00e1 o [modelo de florestas aleat\u00f3rias](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html). Vamos import\u00e1-lo do [scikit-learn](https:\/\/scikit-learn.org\/), armazenando-o em um objeto chamado **m** (poderia ser qualquer outro nome):","f133b47c":"A primeira m\u00e9trica \u00e9 o [erro absoluto m\u00e9dio](https:\/\/en.wikipedia.org\/wiki\/Mean_absolute_error) e corresponde \u00e0 m\u00e9dia dos m\u00f3dulos das dist\u00e2ncias entre os valores verdadeiros e os valores preditos. Quanto menor, melhor.\n\nA segunda m\u00e9trica \u00e9 o [coeficiente de determina\u00e7\u00e3o](https:\/\/en.wikipedia.org\/wiki\/Coefficient_of_determination) $R^2$, uma medida da vari\u00e2ncia dos dados explicada pelo modelo. Pode variar de $-\\infty$ a 1; quanto mais perto de 1, melhor ($R^2 = 0$ implica que o desempenho do modelo \u00e9 equivalente a usar a m\u00e9dia dos dados; valores negativos implicam que o modelo \u00e9 pior que isso).\n\nEssas s\u00e3o apenas algumas das m\u00e9tricas; existem v\u00e1rias [outras](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#model-evaluation); a escolha de qual usar deve ser ditada pelo problema que se est\u00e1 resolvendo.","5ba679c6":"Uma \u00e1rvore \u00e9 uma sequ\u00eancia de decis\u00f5es bin\u00e1rias. Cada quadradinho acima \u00e9 um *n\u00f3* e representa uma por\u00e7\u00e3o dos dados. O primeiro n\u00f3 \u00e9 chamado de *raiz* e cont\u00e9m a totalidade dos dados. Os \u00faltimos n\u00f3s s\u00e3o chamados de *folhas*. Cada n\u00f3 \u00e9 gerado a partir de um n\u00f3 da camada anterior por meio de uma decis\u00e3o correspondente a alguma vari\u00e1vel. Os dois n\u00f3s da segunda camada, por exemplo, s\u00e3o gerados a partir do primeiro n\u00f3 por meio de uma decis\u00e3o relativa \u00e0 vari\u00e1vel **Coupler_System**.\n\nEm cada n\u00f3 s\u00e3o exibidos:\n\n* uma m\u00e9trica de predi\u00e7\u00e3o (no caso, **mse**);\n* o n\u00famero de amostras (**samples**);\n* a predi\u00e7\u00e3o em si (**value**), que corresponde simplesmente \u00e0 m\u00e9dia da vari\u00e1vel predita no n\u00f3.\n\nO *split*, ou seja, a decis\u00e3o a ser tomada em cada n\u00f3, \u00e9 especificada de modo a minimizar os erros dos dois n\u00f3s resultantes. H\u00e1 v\u00e1rios algoritmos capazes de efetuar essa minimiza\u00e7\u00e3o, como o [CART](https:\/\/medium.com\/@arifromadhan19\/regrssion-in-decision-tree-a-step-by-step-cart-classification-and-regression-tree-196c6ac9711e), por exemplo.\n\nA \u00e1rvore acima foi treinada com profundidade 3. O que acontece se treinarmos uma \u00e1rvore maior? Na c\u00e9lula abaixo, usamos o [default do modelo](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html), que cresce a \u00e1rvore at\u00e9 que todas as folhas estejam *puras*. Uma folha pura cont\u00e9m apenas 1 valor da vari\u00e1vel predita. Em outras palavras, cada valor assumido pela vari\u00e1vel predita corresponde a 1 folha na \u00e1rvore. Como nosso conjunto tem 389125 linhas, a \u00e1rvore gerada ser\u00e1 bem grande!!","2c40bb96":"# Importando dados\n\nO primeiro passo \u00e9 importar os dados de treino. Como os arquivos est\u00e3o armazenados no formato [CSV (comma-separated values)](https:\/\/pt.wikipedia.org\/wiki\/Comma-separated_values), utilizamos a fun\u00e7\u00e3o [read_csv](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.read_csv.html), da biblioteca [pandas](https:\/\/pandas.pydata.org\/), para efetuar a leitura:","800bf446":"Pr\u00e9-processando o [DataFrame](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.html) com *one-hot encoding*:","77893891":"Dando uma conferida no [DataFrame](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.html) original, para fins de compara\u00e7\u00e3o:","557e1873":"Nenhuma dessas vari\u00e1veis parece fazer falta, j\u00e1 que o OOB n\u00e3o diminui significativamente!\n\nEfetuando de fato as remo\u00e7\u00f5es:","f89a9ebc":"Nada mal.","989fa824":"Na c\u00e9lula abaixo, criamos e exibimos um [DataFrame](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.html) em que cada linha corresponde a uma observa\u00e7\u00e3o do conjunto de valida\u00e7\u00e3o e cada coluna corresponde \u00e0 predi\u00e7\u00e3o de uma das \u00e1rvores da floresta. No final do [DataFrame](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.html) adicionamos tr\u00eas colunas, respectivamente, com: \n\n* as m\u00e9dias das predi\u00e7\u00f5es de todas as \u00e1rvores;\n* os desvios-padr\u00e3o das predi\u00e7\u00f5es de todas as \u00e1rvores;\n* o valor verdadeiro da vari\u00e1vel predita.","891e9cd7":"Os resultados s\u00e3o bem piores do que antes! Usar a floresta ao inv\u00e9s de 1 s\u00f3 \u00e1rvore parece fazer toda a diferen\u00e7a. Outro detalhe: o treinamento ocorre de forma bem mais r\u00e1pida.\n\nA seguir, define-se uma fun\u00e7\u00e3o para visualizar uma \u00e1rvore:","af77457d":"Melhoramos! Diminuindo agora o n\u00famero de \u00e1rvores para obter um modelo mais eficiente, que usaremos nas an\u00e1lises que seguir\u00e3o:","3b856b24":"### Separando conjuntos de treino e de teste\n\nNo Aprendizado de M\u00e1quina, n\u00e3o \u00e9 boa pr\u00e1tica utilizar na etapa de treino todos os dados dispon\u00edveis. Sempre deve-se reservar uma parcela dos dados para efetuar um teste, de modo a verificar-se a capacidade preditiva do modelo.\n\nPara efetuar a separa\u00e7\u00e3o dos dados Boston em *dados de treino* e *dados de teste*, usaremos a fun\u00e7\u00e3o [train_test_split](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html):","dda5bc07":"***M\u00e3o na massa 1!***\n\n* Na primeira c\u00e9lula:\n    * Procure na [API](https:\/\/scikit-learn.org\/stable\/modules\/classes.html) ou no [guia do usu\u00e1rio](https:\/\/scikit-learn.org\/stable\/user_guide.html#) do [scikit-learn](https:\/\/scikit-learn.org\/) outro modelo e outras m\u00e9tricas para avalia\u00e7\u00e3o de resultados de regress\u00e3o e as utilize para reproduzir os passos acima. Dependendo da sua escolha, talvez seja necess\u00e1rio importar novos m\u00f3dulos do [scikit-learn](https:\/\/scikit-learn.org\/), como fiz no come\u00e7o do notebook.\n* Na segunda c\u00e9lula:\n    * Repita todo o procedimento para o conjunto de dados *diabetes*, tamb\u00e9m dispon\u00edvel no [scikit-learn](https:\/\/scikit-learn.org\/) (procure-o [aqui](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.datasets)).","8c157ef0":"H\u00e1 uma grande propor\u00e7\u00e3o de valores faltantes, a maioria deles nas vari\u00e1veis categ\u00f3ricas (todas al\u00e9m das oito primeiras).\n\nO fato de haver vari\u00e1veis categ\u00f3ricas e valores faltantes \u00e9 cr\u00edtico, j\u00e1 que algoritmos de regress\u00e3o s\u00e3o projetados para lidar com vari\u00e1veis num\u00e9ricas. Portanto, \u00e9 preciso pr\u00e9-processar os dados e transform\u00e1-los em uma matriz de n\u00fameros antes de efetuar treinos de modelos de aprendizado.","93e025b7":"Quase triplicamos a quantidade de vari\u00e1veis!\n\nAgora vamos repetir o processo de treinamento e an\u00e1lise de vari\u00e1veis com o novo conjunto de dados.","c31f0c6e":"# Sintonizando hiperpar\u00e2metros\n\nNesta se\u00e7\u00e3o daremos uma olhada em como podemos mexer em alguns hiperpar\u00e2metros do modelo de modo a melhorar os desempenhos preditivo e computacional.","d86b30b8":"Passamos de 3-4min para 20-30s, sem prejudicar muito a acur\u00e1cia!","a8f8a2b5":"O pr\u00f3ximo passo natural seria aplicar o modelo final a um conjunto de teste em separado. Nosso exemplo, no entanto, veio de uma competi\u00e7\u00e3o  j\u00e1 encerrada do Kaggle, ent\u00e3o n\u00e3o \u00e9 mais poss\u00edvel submeter o modelo e avaliar o desempenho no conjunto de teste da competi\u00e7\u00e3o.\n\nMas lembre-se: em problemas da vida real, separe sempre um conjunto de teste para ser usado apenas na avalia\u00e7\u00e3o do modelo final!\n\n\u00c9 isso! At\u00e9 a pr\u00f3xima aula, galera!!","4527f8b0":"Na fun\u00e7\u00e3o acima, iteramos ao longo de todas as colunas do conjunto de dados. Se a coluna em quest\u00e3o for num\u00e9rica, os valores faltantes s\u00e3o substitu\u00eddos pela mediana. Se n\u00e3o for num\u00e9rica, s\u00e3o transformadas em categorias e cada categoria, por sua vez, \u00e9 associada a um n\u00famero (ao final, o valor 1 \u00e9 adicionado para que a contagem comece do 1 e n\u00e3o do 0, o que \u00e9 conveniente para os algoritmos).\n\nValores faltantes nas v\u00e1riaveis categ\u00f3ricas n\u00e3o s\u00e3o t\u00e3o graves porque o pr\u00f3prio fato de um valor estar faltando pode ser interpretado como uma categoria. J\u00e1 nas vari\u00e1veis num\u00e9ricas, eles s\u00e3o cr\u00edticos, por isso foi preciso substitui-los por algum n\u00famero.\n\nAplicando a fun\u00e7\u00e3o acima e gerando o [DataFrame](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.html) processado:","ebeca267":"## Juntando v\u00e1rias \u00e1rvores\n\nPara entendermos como as v\u00e1rias \u00e1rvores formam a floresta, voltemos ao nosso modelo base:","b5c70e00":"Para visualizar o resultado, plotaremos um gr\u00e1fico com os valores verdadeiros no eixo $x$ e as predi\u00e7\u00f5es no eixo $y$. Como o objetivo \u00e9 que as predi\u00e7\u00f5es estejam o mais perto poss\u00edvel dos valores verdadeiros, quanto mais esses pontos se aproximarem da reta $x=y$, melhores as predi\u00e7\u00f5es!","4d774e88":"Parece que h\u00e1 [vari\u00e1veis categ\u00f3ricas](https:\/\/en.wikipedia.org\/wiki\/Categorical_variable) e [valores faltantes](https:\/\/en.wikipedia.org\/wiki\/Missing_data) no conjunto de dados.\n\nPara analisar quais vari\u00e1veis s\u00e3o num\u00e9ricas e quais s\u00e3o categ\u00f3ricas, daremos uma olhada em seus tipos:","d2ba8697":"Usando a biblioteca [missingno](https:\/\/github.com\/ResidentMario\/missingno) para dar uma olhada nos valores faltantes:","86961eb9":"Olha que interessante: a categoria **w AC** de Enclosure_EROPS \u00e9 a mais relevante para a determina\u00e7\u00e3o do pre\u00e7o (o que faz sentido, j\u00e1 que **w AC** significa \"com ar-condicionado\").","ad601760":"Pronto! Nosso modelo est\u00e1 treinado! F\u00e1cil, n\u00e9?\n\nVamos calcular as predi\u00e7\u00f5es do modelo para o conjunto de teste. Para isso, usa-se o m\u00e9todo **predict**:","32b813c0":"Efetuando an\u00e1lises com remo\u00e7\u00f5es de uma vari\u00e1vel potencialmente redundante por vez:","eded18d7":"S\u00e3o 53 vari\u00e1veis e quase meio milh\u00e3o de observa\u00e7\u00f5es!\n\nVisualizando algumas linhas do conjunto e utilizando o m\u00e9todo [describe](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.describe.html) para calcular algumas medidas estat\u00edsticas:","a8fa8bbf":"N\u00e3o se preocupe, voc\u00ea j\u00e1 vai entender como esse modelo funciona! Antes disso, vamos trein\u00e1-lo. \n\nDe modo a treinar um modelo no [scikit-learn](https:\/\/scikit-learn.org\/), devemos fornecer os valores de X e y ao m\u00e9todo **fit**, contido no modelo:","9c0b6789":"Olha que interessante: o erro de predi\u00e7\u00e3o de cada \u00e1rvore individual \u00e9 alto (j\u00e1 que as \u00e1rvores s\u00e3o sobreajustadas); mas quando *tiramos a m\u00e9dia de todas as 100 \u00e1rvores* (que formam a floresta!), o erro \u00e9 baixo. Parece m\u00e1gica! Tiramos a m\u00e9dia de v\u00e1rias predi\u00e7\u00f5es meia-boca e o resultado... \u00e9 uma excelente predi\u00e7\u00e3o!!! O que est\u00e1 acontecendo?\n\nO truque \u00e9 *fazer com que as \u00e1rvores apresentem o m\u00ednimo poss\u00edvel de correla\u00e7\u00e3o entre elas*. Sendo assim, cada \u00e1rvore aprende de maneira sobreajustada uma por\u00e7\u00e3o isolada dos padr\u00f5es que queremos capturar. Ao tirarmos a m\u00e9dia, juntamos todos os pedacinhos que cada \u00e1rvore aprendeu individualmente... e criamos um modelo completo e robusto!\n\nA principal estrat\u00e9gia para plantar uma floresta de \u00e1rvores descorrelacionadas \u00e9 fazer com que cada \u00e1rvore utilize uma parcela aleat\u00f3ria dos dados. Dessa maneira, cada \u00e1rvore sobreajusta de diferentes maneiras em diferentes fen\u00f4menos; ou seja, todas elas t\u00eam grandes erros, mas os erros s\u00e3o aleat\u00f3rios. \n\n*E, de acordo com a Estat\u00edstica, qual \u00e9 a m\u00e9dia de um monte de erros aleat\u00f3rios?* \n\nZero!!\n\nNo algoritmo de florestas aleat\u00f3rias, cada \u00e1rvore efetua amostras *com reposi\u00e7\u00e3o* (esse procedimento \u00e9 conhecido como [bagging](https:\/\/en.wikipedia.org\/wiki\/Bootstrap_aggregating)). Dessa maneira, nem todo o conjunto de dados \u00e9 utilizado por cada \u00e1rvore, j\u00e1 que no procedimento de amostragem v\u00e1rias observa\u00e7\u00f5es podem se repetir. Em m\u00e9dia, aproximadamente apenas 63,2% dos dados s\u00e3o utilizados por cada \u00e1rvore. Isso ajuda bastante a diminuir a correla\u00e7\u00e3o entre elas.\n\nAbaixo, para termos uma no\u00e7\u00e3o visual do resultado, plotamos as predi\u00e7\u00f5es contra os valores verdadeiros e comparamos com a reta $x=y$:","864f8b41":"Explicando as tr\u00eas linhas acima:\n\n* na primeira linha, definimos o modelo. O hiperpar\u00e2metro **n_jobs = -1** especifica que, caso haja m\u00faltiplos processadores no computador, todos devem ser usados em paralelo. O hiperpar\u00e2metro **oob_score** ser\u00e1 explicado mais adiante.\n\n* na segunda linha, treinamos o modelo. Repare no uso do comando m\u00e1gico %time, que mede o tempo necess\u00e1rio para essa tarefa.\n\n* na terceira linha, usamos a fun\u00e7\u00e3o **display_score**, definida anteriormente, para imprimir as m\u00e9tricas.\n\nOs resultados s\u00e3o bem satisfat\u00f3rios! O $R^2$ \u00e9 pr\u00f3ximo de 1 e, analisando o [leaderboard da competi\u00e7\u00e3o](https:\/\/www.kaggle.com\/c\/bluebook-for-bulldozers\/leaderboard), nosso RMSE est\u00e1 no mesmo patamar do top 20! De fato, o modelo de florestas aleat\u00f3rias \u00e9 excelente para resolver esse tipo de problema: com alto grau de n\u00e3o-linearidade, dados sem estrutura clara e grande n\u00famero de vari\u00e1veis categ\u00f3ricas.\n\nA partir de agora, concentraremos nossos esfor\u00e7os em:\n\n* entender como funciona o modelo;\n* utilizar algumas t\u00e9cnicas para melhorar os resultados.","196ee137":"A fun\u00e7\u00e3o [train_test_split](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html) separa os dados em *treino* e *teste* de maneira aleat\u00f3ria. No caso acima, uma fra\u00e7\u00e3o de 10% dos dados \u00e9 reservada para teste.\n\nA estrat\u00e9gia de separa\u00e7\u00e3o aleat\u00f3ria \u00e9 adequada quando os dados s\u00e3o independentes e identicamente distribu\u00eddos; em situa\u00e7\u00f5es em que isso n\u00e3o vale (por exemplo, quando h\u00e1 depend\u00eancias temporais), devemos usar outras estrat\u00e9gias.\n\nObs: o argumento **random_state** especifica a semente de gera\u00e7\u00e3o de pseudo-aleatoriedade do algoritmo; isso faz com que em todas as execu\u00e7\u00f5es os resultados sejam sempre os mesmos.","399c908c":"### Importando dados\n\n\nO conjunto de dados em quest\u00e3o \u00e9 disponibilizado no pr\u00f3prio [scikit-learn](https:\/\/scikit-learn.org\/stable\/) em seu m\u00f3dulo [sklearn.datasets](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes) e pode ser importado com o seguinte comando:","37cde298":"***M\u00e3o na massa 4!***\n\n* Repita o procedimento de an\u00e1lise de contribui\u00e7\u00f5es com os conjuntos de dados [Boston Housing](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.load_boston.html) e [Diabetes](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.load_diabetes.html) usados no come\u00e7o do notebook.","e1f7f343":"# Um exemplinho introdut\u00f3rio...\n\nO objeto de estudo deste notebook \u00e9 o conjunto de dados da competi\u00e7\u00e3o [Bluebook for Bulldozers](https:\/\/www.kaggle.com\/c\/bluebook-for-bulldozers), que pode ser traduzida livremente como \"*Tabela FIPE para Tratores de Esteira*\". Antes de come\u00e7armos a analisar esses dados, no entanto, vamos usar um exemplo mais simples, o conjunto [Boston Housing](https:\/\/www.cs.toronto.edu\/~delve\/data\/boston\/bostonDetail.html), para nos familiarizarmos com a funcionalidade b\u00e1sica da regress\u00e3o com o [scikit-learn](https:\/\/scikit-learn.org\/stable\/).","ba336f1d":"Alguns par\u00e2metros foram fornecidos \u00e0 fun\u00e7\u00e3o: \n\n* **compression** especificou que os arquivos est\u00e3o comprimidos no formato *zip*;\n* **low_memory** especificou que a fun\u00e7\u00e3o deve ler o arquivo como um todo e n\u00e3o em pequenos peda\u00e7os (isso \u00e9 recomend\u00e1vel quando n\u00e3o sabemos bem que tipos de vari\u00e1veis h\u00e1 no arquivo, por conta de uma quest\u00e3o t\u00e9cnica relacionada \u00e0 forma como o [pandas](https:\/\/pandas.pydata.org\/) infere os tipos de cada vari\u00e1vel); \n* **parse_dates** especifica qual vari\u00e1vel deve ser processada como data (no caso, no formato ano-m\u00eas-dia).\n\nConferindo o tamanho do conjunto:","3f6c66bb":"# O que \u00e9 uma floresta aleat\u00f3ria, afinal?\n\nFlorestas, obviamente, s\u00e3o feitas de \u00e1rvores!\n\nEm particular, os modelos de florestas aleat\u00f3rias s\u00e3o compostos por v\u00e1rios modelos mais simples conhecidos como *\u00e1rvores de decis\u00e3o*.\n\nPortanto, antes de entender a floresta, \u00e9 preciso entender a \u00e1rvore.","9a2fcc04":"A c\u00e9lula abaixo treina um modelo de florestas aleat\u00f3rias que especificaremos como o *modelo base*:","8144a873":"O conjunto de dados analisado nesta aula, retirado da competi\u00e7\u00e3o  [Bluebook for Bulldozers](https:\/\/www.kaggle.com\/c\/bluebook-for-bulldozers), consiste de dados reais relativos a informa\u00e7\u00f5es variadas acerca de tratores de esteira industriais. Nosso objetivo \u00e9 criar um modelo que, dadas informa\u00e7\u00f5es relativas a um trator em particular, seja capaz de predizer seu *pre\u00e7o de venda em leil\u00f5es*.\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/1\/1e\/CAT-D10N-pic001.jpg\" width=\"600\" height=\"600\"\/>","149f3701":"O conjunto de valida\u00e7\u00e3o \u00e9 um conjunto usado para testes intermedi\u00e1rios durante o processo de modelagem. Al\u00e9m da valida\u00e7\u00e3o, \u00e9 recomend\u00e1vel que haja um conjunto de teste em separado para ser usado *ap\u00f3s* a finaliza\u00e7\u00e3o da modelagem. Todas as competi\u00e7\u00f5es do Kaggle possuem esse conjunto, que \u00e9 usado para criar as pontua\u00e7\u00f5es e leaderboards.\n\nPerceba que, como h\u00e1 *evolu\u00e7\u00e3o temporal*, n\u00e3o separamos o treino e o teste de maneira aleat\u00f3ria, como fizemos no exemplo introdut\u00f3rio. Agora retiramos as 12000 \u00faltimas observa\u00e7\u00f5es para teste, o que faz sentido, pois na vida real um modelo \u00e9 aplicado para prever valores em instantes de tempo posteriores aos usados no treino.","4b88e7bc":"Os resultados melhoram, mas o sobreajuste \u00e9 enorme! Esse \u00e9 um problema dos modelos de (uma) \u00e1rvore: como eles s\u00e3o muito flex\u00edveis e conseguem modelar todo o espa\u00e7o dos dados por meio de v\u00e1rias parti\u00e7\u00f5es, grandes s\u00e3o as chances de se aprender um n\u00famero excessivo de comportamentos, n\u00e3o correspondentes a padr\u00f5es generaliz\u00e1veis.\n\nPara melhorar a generaliza\u00e7\u00e3o, n\u00e3o \u00e9 suficiente usar \u00e1rvores maiores. \u00c9 preciso combinar os resultados de m\u00faltiplas \u00e1rvores.","22ec76e1":"## Vari\u00e1veis *dummy* ou *one-hot encoding*\n\nO uso de [vari\u00e1veis *dummy*](https:\/\/en.wikipedia.org\/wiki\/Dummy_variable_(statistics)) ou *one-hot encoding* \u00e9 uma estrat\u00e9gia diferente para organizar vari\u00e1veis categ\u00f3ricas. Nessa metodologia, as vari\u00e1veis s\u00e3o desmembradas em vari\u00e1veis bin\u00e1rias correspondentes a cada uma de suas categorias. A figura a seguir ilustra bem a situa\u00e7\u00e3o:\n\n<img src=\"https:\/\/i1.wp.com\/thierrymoudiki.github.io\/images\/2020-02-28\/2020-02-28-image1.png?w=578&ssl=1\" width=\"500\" height=\"500\"\/>\n\nA vantagem dessa representa\u00e7\u00e3o \u00e9 que a influ\u00eancia de categorias espec\u00edficas pode tornar-se mais clara. Em alguns casos, a acur\u00e1cia do modelo pode aumentar. A desvantagem \u00e9 que a dimensionalidade dos dados (n\u00famero de colunas) aumenta, diminuindo o desempenho computacional.\n\nA seguir definimos uma nova fun\u00e7\u00e3o para pr\u00e9-processamento, que utiliza por sua vez a fun\u00e7\u00e3o [get_dummies](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.get_dummies.html) do [pandas](https:\/\/pandas.pydata.org\/) para implementar o *one-hot encoding*. H\u00e1 a especifica\u00e7\u00e3o de um n\u00famero m\u00e1ximo de categorias, expressa pelo argumento **max_cats**, que evita o aumento excessivo da dimensionalidade no caso de vari\u00e1veis de alta cardinalidade. Caso uma vari\u00e1vel tenha mais do que **max_cats** categorias, n\u00e3o s\u00e3o geradas vari\u00e1veis *dummy* a partir dela.","c6f20fc8":"Nosso exemplo foi simples, mas o suficiente para motivar v\u00e1rias perguntas:\n\n* Como entender melhor a natureza dos dados?\n* O que se deve fazer quando h\u00e1 problemas nos dados (buracos, outliers, etc)?\n* Mesmo que os dados n\u00e3o tenham problemas, \u00e9 poss\u00edvel mexer neles antes de entreg\u00e1-los ao modelo de florestas aleat\u00f3rias, de modo a melhorar os resultados? Como?\n* O que s\u00e3o as tais florestas que comp\u00f5em o modelo?\n* \u00c9 poss\u00edvel mexer nas florestas de modo a melhorar os resultados? Como?\n* Se a estrat\u00e9gia de separa\u00e7\u00e3o treino\/teste usada acima n\u00e3o for adequada, que outras estrat\u00e9gias podemos adotar?\n* O que fazer se o conjunto de dados for muito grande, tornando o treino exageradamente demorado?\n* \u00c9 poss\u00edvel interpretar os resultados?\n\n\nVamos responder a todas elas ao longo desta aula!!","6e94220b":"## min_samples_leaf e max_features\n\n* O hiperpar\u00e2metro **min_samples_leaf** especifica o n\u00famero m\u00ednimo de amostras contidas em cada folha. Em outras palavras, determina o n\u00famero necess\u00e1rio de amostras em um n\u00f3 para interromper o crescimento de seu ramo. Aumentar **min_samples_leaf** faz com que as \u00e1rvores sejam menos profundas, o que diminui a acur\u00e1cia de cada \u00e1rvore individual, mas tamb\u00e9m potencialmente diminui a correla\u00e7\u00e3o entre elas, melhorando a generaliza\u00e7\u00e3o.\n\n* O hiperpar\u00e2metro **max_features** especifica um n\u00famero m\u00e1ximo de vari\u00e1veis a ser considerado para decidir o split de cada n\u00f3. Diminuir esse n\u00famero m\u00e1ximo diminui o efeito de vari\u00e1veis muito influentes, diminuindo assim a correla\u00e7\u00e3o entre as \u00e1rvores. A redu\u00e7\u00e3o das vari\u00e1veis dispon\u00edveis por split foi proposta no [paper original do modelo de florestas aleat\u00f3rias](https:\/\/www.stat.berkeley.edu\/~breiman\/randomforest2001.pdf) e muitos consideram que um modelo s\u00f3 poder ser considerado floresta aleat\u00f3ria se efetuar esse procedimento. O [scikit-learn](https:\/\/scikit-learn.org\/), no entanto, por default n\u00e3o o efetua.\n\nVamos mexer nesses par\u00e2metros e tentar obter um resultado melhor:","f3699bd2":"Esse procedimento n\u00e3o \u00e9 necess\u00e1rio, apenas conveniente. Ele \u00e9 muito comum na predi\u00e7\u00e3o de pre\u00e7os, j\u00e1 que nesses casos nos importamos mais com propor\u00e7\u00f5es do que com diferen\u00e7as absolutas. Por exemplo, muitas vezes \u00e9 mais significativo predizer um aumento ou redu\u00e7\u00e3o de 10% (propor\u00e7\u00e3o) do que de 10 reais (diferen\u00e7a absoluta). Ao aplicar o logaritmo, os dados s\u00e3o transpostos para a escala logar\u00edtmica, em que as propor\u00e7\u00f5es se transformam em diferen\u00e7as absolutas."}}