{"cell_type":{"1dc4eb14":"code","1c518184":"code","ba74e67b":"code","efa9e5be":"code","f99505fd":"code","eed3c6e8":"code","88ac8609":"code","d002a005":"code","74e5eaa1":"code","2b393f65":"code","183f5b4b":"code","5513e2f4":"code","f1e73824":"code","87de48b8":"code","4165a453":"code","4540025d":"code","4df254d1":"code","9b9359c3":"code","b8d3313f":"markdown","7144b4d1":"markdown","adb36ced":"markdown","bbdcf9de":"markdown"},"source":{"1dc4eb14":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport time\nimport matplotlib.pyplot as plt\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom bs4 import BeautifulSoup\nimport string\n\nfrom sklearn.model_selection import (StratifiedKFold, train_test_split, \n                                     GridSearchCV, cross_val_score)\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, \n                             f1_score, classification_report)","1c518184":"d_train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\nd_train.drop(d_train[['keyword','location']], axis=1, inplace=True)\n\nd_train.tail()","ba74e67b":"d_train.info()","efa9e5be":"d_train.isnull().sum()","f99505fd":"sns.countplot(d_train.target)\nplt.show()","eed3c6e8":"# Remove special characters and others\nimport re\n\n## remove URL\ndef remove_URL(text):\n    new_text = \" \".join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\",\" \",text).split())\n    return new_text\n\n## Making text to lower case\ndef lower_case(text):\n    return text.lower()\n\ndef butiful(text):\n    text = BeautifulSoup(text).get_text()\n    return text\n\n## Removing Numbers\ndef remove_num(text):\n    new_text = re.sub(r'\\d+',\"\",text)\n    return new_text\n\n## Removing Punctuation\ndef remove_punc(text):\n    trans = str.maketrans(\"\",\"\",string.punctuation)\n    return text.translate(trans)","88ac8609":"# Tokenizer\ndef tokenizer(text):\n    text = word_tokenize(text)\n    return text\n\n# Stopwords\ndef remove_stopword(text):\n    text = [i for i in text if not i in stopwords.words('english')]\n    return text\n\n# Lemmatize\nlemmatize = WordNetLemmatizer()\ndef Lemmatize(text):\n    text = [lemmatize.lemmatize(token) for token in text]\n    return text","d002a005":"def preprocess(text):\n    text = remove_URL(text)\n    text = butiful(text)\n    text = lower_case(text)\n    text = remove_num(text)\n    text = remove_punc(text)\n    text = tokenizer(text)\n    text = remove_stopword(text)\n    text = Lemmatize(text)\n    text = \" \".join(text)\n    return text\n\n# data training\ntraining_set = []\nfor i in d_train.text:\n    text_data = preprocess(i)\n    training_set.append(text_data)\n    \nd_train['preprocessing_results'] = training_set","74e5eaa1":"d_test = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\nd_test.drop(d_test[['keyword','location']], axis=1, inplace=True)\n\n# data testing\ntesting_set = []\nfor i in d_test.text:\n    text_data = preprocess(i)\n    testing_set.append(text_data)\n    \nd_test['preprocessing_results'] = testing_set","2b393f65":"d_train.head()","183f5b4b":"d_test.head()","5513e2f4":"count_vect = CountVectorizer()\ntrain_vec = count_vect.fit_transform(d_train['preprocessing_results'])\ntest_vec = count_vect.transform(d_test['preprocessing_results'])\n\nprint(train_vec.shape)\nprint(test_vec.shape)","f1e73824":"x_train, x_test, y_train, y_test = train_test_split(train_vec, d_train.target,\n                                                    test_size=0.2,random_state=0)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","87de48b8":"'''\n# Hyperparameter tunning and Cross-Validation\ndef hyperparameter(x_train, y_train):\n    params_grid = dict(n_estimators = list(range(100,600,100)),\n                       max_depth = list(range(100,1000,100)))\n\n    model = RandomForestClassifier()\n    cv = StratifiedKFold(n_splits=10)\n    grid_search = GridSearchCV(model, param_grid=params_grid,\n                                   cv=cv, n_jobs=-1, verbose=5)\n\n    grid_search.fit(x_train, y_train)\n    #result = pd.DataFrame(grid_search_SVC.cv_results_)\n    print('Best score: {:.2f}'.format(grid_search.best_score_))\n    print('Best parameters: {}'.format(grid_search.best_params_))\n\nstart = time.perf_counter()\nhyperparameter(x_train, y_train)\nelapsed = time.perf_counter() - start\nprint('Elapsed %.3f seconds.' % elapsed)\n'''","4165a453":"start = time.perf_counter()\n\n# Model\nrf = RandomForestClassifier(max_depth = 400, n_estimators = 300,random_state=0)\nrf.fit(x_train, y_train) # fit in model\ny_pred = rf.predict(x_test) # predict data \n\nelapsed = time.perf_counter() - start\nprint('Elapsed %.3f seconds.' % elapsed)","4540025d":"print('Accuracy_test : {:.4f}'.format(accuracy_score(y_test, y_pred)), \n      'Precision_test : {:.4f}'.format(precision_score(y_test, y_pred, average='macro')), \n      'Recall_test : {:.4f}'.format(recall_score(y_test, y_pred, average='macro')), \n      'F1-Score : {:.4f}'.format(f1_score(y_test, y_pred, average='macro')))\n\n# Classification report\nprint('\\nclassification report testing : \\n', classification_report(y_test, y_pred))\nprint('Elapsed %.3f seconds.' % elapsed)","4df254d1":"# Plot confusion matrix\nfrom sklearn.metrics import plot_confusion_matrix\ndisp = plot_confusion_matrix(rf, x_test, y_test, \n                             values_format='.1f')\n\ndisp.ax_.set_title(\"Confusion matrix\")\nplt.show()","9b9359c3":"# Predict\nprediction = rf.predict(test_vec)\n\n# Save\nsubmission = pd.DataFrame({\n    'id':d_test['id'], \n    'target':prediction\n})\n\nsubmission.to_csv('Submission.csv',index = False)","b8d3313f":"### Vectorize the text data","7144b4d1":"### Model","adb36ced":"Tokenize (tokenizing means spliting string into words)\n\nLemmatize (Lemmatizeing given bu eg: if we have a word (learn,learning) learning can be lemmatized to learn.)\n\nRemoving stop words (stop wordsa are joining words used to join the text like(is,was,and,or etc.))","bbdcf9de":"### Preprocessing data (NLP)"}}