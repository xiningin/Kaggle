{"cell_type":{"bb24f06d":"code","94d144f9":"code","982ff065":"code","6a7e6bcb":"code","7c05b1fa":"code","11b0a400":"code","1a1b170f":"code","564f6e38":"code","052dc214":"markdown","7ab9042d":"markdown","51cbdc51":"markdown","7a5b6813":"markdown","c7babf57":"markdown","24a5d48b":"markdown","22d58ee4":"markdown","a264945d":"markdown","3bec2b6e":"markdown","1fd237a6":"markdown","697df9e8":"markdown"},"source":{"bb24f06d":"import numpy as np\n\nfrom copy import copy\n\n\ndef policy_eval(env, values, policies, upper_bound):\n    print('\\n===== Policy Evalution =====')\n    delta = upper_bound\n    iteration = 0\n\n    while delta >= upper_bound:\n        delta = 0.\n\n        for s in env.states:\n            v = values.get(s)\n            env.set_state(s)\n\n            action_index = policies.sample(s)\n            action = env.actions[action_index]\n            _, _, rewards, next_states = env.step(action)\n\n            next_values = values.get(list(next_states))\n            td_values = list(map(lambda x, y: x + env.gamma * y, rewards, next_values))\n\n            exp_value = np.mean(td_values)\n            values.update(s, exp_value)\n\n            # update delta\n            delta = max(delta, abs(v - exp_value))\n            \n        iteration += 1\n        print('\\r> iteration: {} delta: {}'.format(iteration, delta), flush=True, end=\"\")","94d144f9":"def policy_improve(env, values, policies):\n    print('\\n===== Policy Improve =====')\n    policy_stable = True\n    \n    for state in env.states:\n        old_act = policies.sample(state)\n\n        # calculate new policy execution\n        actions = env.actions\n        value = [0] * len(env.actions)\n        \n        for i, action in enumerate(actions):\n            env.set_state(state)\n            _, _, rewards, next_states = env.step(action)\n            next_values = values.get(list(next_states))\n            td_values = list(map(lambda x, y: x + env.gamma * y, rewards, next_values))\n            prob = [1 \/ len(next_states)] * len(next_states)\n\n            value[i] = sum(map(lambda x, y: x * y, prob, td_values))\n\n        # action selection\n        new_act = actions[np.argmax(value)]\n\n        # greedy update policy\n        new_policy = [0.] * env.action_space\n        new_policy[new_act] = 1.\n        policies.update(state, new_policy)\n\n        if old_act != new_act:\n            policy_stable = False\n\n    return policy_stable","982ff065":"def value_iter(env, values, upper_bound):\n    print('===== Value Iteration =====')\n    delta = upper_bound + 1.\n    states = copy(env.states)\n    \n    iteration = 0\n\n    while delta >= upper_bound:\n        delta = 0\n\n        for s in states:\n            v = values.get(s)\n\n            # get new value\n            actions = env.actions\n            vs = [0] * len(actions)\n\n            for i, action in enumerate(actions):\n                env.set_state(s)\n                _, _, rewards, next_states = env.step(action)\n                td_values = list(map(lambda x, y: x + env.gamma * y, rewards, values.get(next_states)))\n\n                vs[i] = np.mean(td_values)\n\n            values.update(s, max(vs))\n            delta = max(delta, abs(v - values.get(s)))\n        \n        iteration += 1\n        print('\\r> iteration: {} delta: {}'.format(iteration, delta), end=\"\", flush=True)\n        \n    return","6a7e6bcb":"class Env:\n    def __init__(self):\n        self._states = set()\n        self._state = None\n        self._actions = []\n        self._gamma = None\n        \n    @property\n    def states(self):\n        return self._states\n    \n    @property\n    def state_space(self):\n        return self._state_shape\n    \n    @property\n    def actions(self):\n        return self._actions\n    \n    @property\n    def action_space(self):\n        return len(self._actions)\n    \n    @property\n    def gamma(self):\n        return self._gamma\n    \n    def _world_init(self):\n        raise NotImplementedError\n        \n    def reset(self):\n        raise NotImplementedError\n    \n    def step(self, state, action):\n        \"\"\"Return distribution and next states\"\"\"\n        raise NotImplementedError\n        \n    def set_state(self, state):\n        self._state = state\n\n\nclass MatrixEnv(Env):\n    def __init__(self, height=4, width=4):\n        super().__init__()\n        \n        self._action_space = 4\n        self._actions = list(range(4))\n        \n        self._state_shape = (2,)\n        self._state_shape = (height, width)\n        self._states = [(i, j) for i in range(height) for j in range(width)]\n        \n        self._gamma = 0.9\n        self._height = height\n        self._width = width\n\n        self._world_init()\n        \n    @property\n    def state(self):\n        return self._state\n    \n    @property\n    def gamma(self):\n        return self._gamma\n    \n    def set_gamma(self, value):\n        self._gamma = value\n        \n    def reset(self):\n        self._state = self._start_point\n        \n    def _world_init(self):\n        # start_point\n        self._start_point = (0, 0)\n        self._end_point = (self._height - 1, self._width - 1)\n        \n    def _state_switch(self, act):\n        # 0: h - 1, 1: w + 1, 2: h + 1, 3: w - 1\n        if act == 0:  # up\n            self._state = (max(0, self._state[0] - 1), self._state[1])\n        elif act == 1:  # right\n            self._state = (self._state[0], min(self._width - 1, self._state[1] + 1))\n        elif act == 2:  # down\n            self._state = (min(self._height - 1, self._state[0] + 1), self._state[1])\n        elif act == 3:  # left\n            self._state = (self._state[0], max(0, self._state[1] - 1))\n\n    def step(self, act):\n        assert 0 <= act <= 3\n        \n        done = False\n        reward = 0.\n\n        self._state_switch(act)\n        \n        if self._state == self._end_point:\n            reward = 1.\n            done = True\n\n        return None, done, [reward], [self._state]","7c05b1fa":"class ValueTable:\n    def __init__(self, env):\n        self._values = np.zeros(env.state_space)\n        \n    def update(self, s, value):\n        self._values[s] = value\n        \n    def get(self, state):\n        if type(state) == list:\n            # loop get\n            res = [self._values[s] for s in state]\n            return res\n        elif type(state) == tuple:\n            # return directly\n            return self._values[state]","11b0a400":"from collections import namedtuple\n\n\nPi = namedtuple('Pi', 'act, prob')\n\n\nclass Policies:\n    def __init__(self, env: Env):\n        self._actions = env.actions\n        self._default_policy = [1 \/ env.action_space] * env.action_space\n        self._policies = dict.fromkeys(env.states, Pi(self._actions, self._default_policy))\n    \n    def sample(self, state):\n        if self._policies.get(state, None) is None:\n            self._policies[state] = Pi(self._actions, self._default_policy)\n\n        policy = self._policies[state]\n        return np.random.choice(policy.act, p=policy.prob)\n    \n    def retrieve(self, state):\n        return self._policies[state].prob\n    \n    def update(self, state, policy):\n        self._policies[state] = self._policies[state]._replace(prob=policy)","1a1b170f":"import time\n\nenv = MatrixEnv(width=8, height=8)  # TODO(ming): try different word size\npolicies = Policies(env)\nvalues = ValueTable(env)\nupper_bound = 1e-4\n\nstable = False\n\nstart = time.time()\nwhile not stable:\n    policy_eval(env, values, policies, upper_bound)\n    stable = policy_improve(env, values, policies)\nend = time.time()\n\nprint('\\n[time consumpution]: {} s'.format(end - start))\n\ndone = False\nrewards = 0\nenv.reset()\nstep = 0\n\nwhile not done:\n    act_index = policies.sample(env.state)\n    _, done, reward, next_state = env.step(env.actions[act_index])\n    rewards += sum(reward)\n    step += 1\n\nprint('Evaluation: [reward] {} [step] {}'.format(rewards, step))","564f6e38":"env = MatrixEnv(width=8, height=8)  # try different word size\npolicies = Policies(env)\nvalues = ValueTable(env)\nupper_bound = 1e-4\n\nstart = time.time()\nvalue_iter(env, values, upper_bound)\n_ = policy_improve(env, values, policies)\nend = time.time()\n\nprint('\\n[time consumption] {}s'.format(end - start))\n# print(\"===== Render =====\")\nenv.reset()\ndone = False\nrewards = 0\nstep = 0\nwhile not done:\n    act_index = policies.sample(env.state)\n    _, done, reward, next_state = env.step(env.actions[act_index])\n    rewards += sum(reward)\n    step += 1\n\nprint('Evaluation: [reward] {} [step] {}'.format(rewards, step))","052dc214":"## Value Iteration\n\nOne drawback to policy iteration is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set. Must we wait for exact convergence, or can we stop short of that?\n\n### Pesudo of Value Iteration Learning\n\nAlgorithm parameter: a small threshold $\\theta > 0$ determining accuracy of estimation\n\nInitialize $V(s)$, for all $s \\in \\mathcal{S}^+$, arbitrarily except that $V(\\textit{terminal})=0$\n\nLoop:\n\n>  $\\Delta \\leftarrow 0$  \n>  Loop for each $s \\in \\mathcal{S}$:  \n>> $v \\leftarrow V(s)$  \n>> $V(s) \\leftarrow \\max_{a}\\sum_{s',r}p(s', r | s, a)\\Big[ r + \\gamma V(s') \\Big]$  \n>> $\\Delta \\leftarrow \\max(\\Delta, |v-V(s)|)$  \n\n> until $\\Delta < \\theta$  \n\nOutput a deterministic policy, $\\pi \\approx \\pi_{\\star}$, such that $\\pi(s) = \\arg \\max_{a}\\sum_{s',r}p(s', r | s, a)\\Big[ r + \\gamma V(s') \\Big]$","7ab9042d":"## Reinforcement Learning\n\nIn reinforcement learning, the agent aims to maixmize its cumulative reward:\n\n<center>$\\max \\sum_{t=1}^T \\mathbb{E}_{a_t \\sim \\pi(s_t), s_{t+1} \\sim p(s_{t+1} \\mid s_t, a_t), s_t \\sim p(s)}[ \\gamma^{t-1} r(s_t,a_t)]$<\/center>.\n\nFrom the perspective of Bellman equation, the caculation of cumulative reward can be also formulated as:\n\n<center>$V(s_t) = \\mathbb{E}_{a \\sim \\pi(s_t)}[r(s_t,a_t) + \\gamma V(s_{t+1})]$<\/center>\n\n## Policy Iteration Learning\n\nOnce a policy, $\\pi$, has been improved using $v_{\\pi}$ to yield a better policy, $\\pi'$, we can then compute $v_{\\pi'}$ and improve it again to yield and even better $\\pi''$. We can thus obtain a sequence of monotonically improving policies and value functions:\n\n<center>$\\pi_0 \\stackrel{\\text{E}}{\\longrightarrow} v_{\\pi_0} \\stackrel{\\text{I}}{\\longrightarrow} \\pi_1 \\stackrel{\\text{E}}{\\longrightarrow} \\dots \\stackrel{\\text{I}}{\\longrightarrow}\\pi_{\\star}\\stackrel{\\text{E}}{\\longrightarrow}v_{\\star}$<\/center>\n\nwhere $\\stackrel{\\text{E}}{\\longrightarrow}$ denotes a policy *evaluation*, and $\\stackrel{\\text{I}}{\\longrightarrow}$ denotes a policy improvement. This way of finding an optimal policy is called *policy iteration*.\n\n### Pesudo of Policy Iteration Learning\n\n1. **Initialization**\n\n    $V(s) \\in \\mathbb{R}$ and $\\pi(s) \\in \\mathcal{A}(s)$ arbitrarily for all $s \\in \\mathcal{S}$\n\n2. **Policy Evaluation**\n\n    Loop:\n\n    > $\\Delta \\leftarrow 0$  \n    > Loop for each $s \\in \\mathcal{S}$:  \n    >> $v \\leftarrow V(S)$  \n    >> $V(s) \\leftarrow \\sum_{s',r}p(s', r | s, \\pi(s))[r + \\gamma V(s')]$  \n    >> $\\Delta \\leftarrow \\max(\\Delta, |v-V(s)|)$\n    \n    until $\\Delta < \\theta$ (a small positive number determining the accuracy of estimation)\n\n3. **Policy Improvement**  \n\n    policy-stable $\\leftarrow$ true  \n    For each $s \\in \\mathcal{S}$:\n    \n    > *old-action* $\\leftarrow \\pi(s)$  \n    > $\\pi(s) \\leftarrow \\arg \\max_a\\sum_{s', r}p(s', r | s, a)[r + \\gamma V(s')]$  \n    > If *old-action* $\\ne \\pi(s)$, then *policy-stable* $\\leftarrow$ *false*  \n    \n    If *policy-stable*, then stop and return $V \\sim v_{\\star}$ and $\\pi \\sim \\pi_{\\star}$; else go to 2","51cbdc51":"**\u2728 Value Iteration Implementation**","7a5b6813":"**\u2728 Policy Improvement Implementation**","c7babf57":"## A Simple Environment\n\n### MatrixEnv\n\nA simple maze game, the agent needs to learn walk from the start point to the destination (goal)\n\n![](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/374051\/727215\/matrix_game.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1570783618&Signature=UY0hcC3RxPdolg7dUhDRvzmnNzeCGW06hdvjpNsrHVyCOKl5QNq70Ghjd9XnXG6rHN3bvRZhu9izcylyVbJG7csxaH6a2ceu6jF3vJOfFPEIqbH2U%2B9Mrje6iNegYy1yibyTMzLjv0GRbHGVq606DarENop7AxZMiH%2Bt0AGi3jvqobH%2BR%2BZ7gLC486UjldGNd8bPHdHctovhPFJ4Zxx1KlZ5qcDOmRVygAMJ9Xq9Oh82AxMDcgoIYLxx5zBCGkCz1bL4kjS9EOH7RZ%2FfHzAkkvXnIkhrTj0Z%2Fqr3Cx5f0QvvtWTviG3G9W3a0Q0fajTFGtozSHRXAMiH0LDEl8AEfQ%3D%3D)\n\n```python\nenv = MatrixEnv()\n\n# ...\n\n# before starting a new episode, be sure to call the `env.reset()`\nenv.reset()\n\n# act_index: get from policy sample\n# retrieve action from action space with `act_index`\nact = env.actions[act_index]\n\n# method step accepts action, then update the inner state, return: {None, done, rewards: list, next_states: list} \n_, done, rewards, next_states = env.step(act)\n\n# ...\n```","24a5d48b":"## Solving Matrix Game via Value Iteration Learning","22d58ee4":"## Solving Matrix Game via Policy Iteration Learning","a264945d":"**\u2728 Policy Evaluation Implementation**","3bec2b6e":"## Basic Data Structure for Learning\n\n### ValueTable\n\nClass `ValueTable` maintains the state value function which map state space $\\mathcal{S} \\in \\mathbb{R}^2$ to real number space $\\mathbb{R}$.\n\n**Methods**:\n\n- `update(state, value)`: update state value with given value\n- `get(state)`: return state value with given state or states\n","1fd237a6":"# Dynamic Programming Based Reinforcement Learning Methods","697df9e8":"### Policies\n\nClass `Policies` maintains the polcies of each states\n\n**Methods**:\n\n- `sample(state)`: sample action, return action index\n- `retrieve(state)`: retrieve policy of given state\n- `update(state, policy)`: update policy of state with given policy"}}