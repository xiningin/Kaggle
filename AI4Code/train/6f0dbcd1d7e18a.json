{"cell_type":{"eb55b44a":"code","a3bef942":"code","010d8e7f":"code","61ee2bd1":"code","f38549e3":"code","740d4eea":"code","6487058b":"code","7237ddfc":"code","690f9c4f":"code","7818dfe4":"code","0fa3b8ac":"code","ef6fc3ac":"code","f8fbf502":"code","5836167b":"code","99fb06e7":"code","f4f7405b":"code","5b95f35a":"code","4c05284c":"code","5c3628b6":"code","a29ddcdb":"code","2b7f6fc6":"code","13664d3a":"code","c0f0af56":"code","b71c79c6":"code","70c4a0e9":"code","c2ee2ab9":"code","cc8b5073":"code","9d2661e3":"code","9f4489d9":"code","cb8db859":"code","6ec9e153":"code","2fccc9df":"code","5f515caa":"code","cf28a02a":"code","bf661525":"code","7421d86c":"code","67a4d44d":"code","8d5f2dea":"code","044f933d":"code","145631d7":"code","20256ede":"code","76d3210c":"code","031df2f5":"code","0cd2abb8":"code","4214dc14":"code","039e7af9":"code","7130d39b":"code","6cc79bdc":"code","442f9232":"code","3d691ca7":"code","30bc072e":"code","33a57a1c":"code","c08e9052":"code","db682e8d":"code","962060fe":"code","3f92e522":"code","893c6b13":"markdown","2f0ed5d3":"markdown","7c304e97":"markdown","260ec2d6":"markdown","703216f5":"markdown","7a398a0f":"markdown","c5bf679c":"markdown","9cda1f3d":"markdown","851ff127":"markdown","db5b5fb1":"markdown","db756fce":"markdown","45f5365b":"markdown","1a2c094c":"markdown","edf7ffad":"markdown","263168a4":"markdown","3f0f313e":"markdown","f1497dcf":"markdown","26e2e03a":"markdown","dd588d75":"markdown","15374f51":"markdown","987be41b":"markdown","9492b2d1":"markdown","fdef353e":"markdown","8e3d4d14":"markdown","3e41cad5":"markdown","ccc77273":"markdown","be32faa2":"markdown","f5d5c467":"markdown","463cdc7c":"markdown","7b563b7e":"markdown","a1fe01f5":"markdown","fcd9e3d4":"markdown","5c3d2929":"markdown","de57d6ba":"markdown","da248172":"markdown","7a133315":"markdown","f413c138":"markdown","9af0b17b":"markdown","f12bb295":"markdown","54b517cc":"markdown","32988be4":"markdown","3c3496a6":"markdown","0cf42bda":"markdown"},"source":{"eb55b44a":"import numpy as np\nimport torch\ntorch.set_printoptions(edgeitems=2, linewidth=75)","a3bef942":"\nt_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\nt_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\nt_c = torch.tensor(t_c)\nt_u = torch.tensor(t_u)","010d8e7f":"import matplotlib.pyplot as plt\n\nplt.plot(t_c, t_u, 'ro')\nplt.show()","61ee2bd1":"def model(t_u, w, b):\n    return w * t_u + b","f38549e3":"def loss_fn(t_p, t_c):\n    squared_diffs = (t_p - t_c)**2\n    return squared_diffs.mean()","740d4eea":"w = torch.ones(())\nb = torch.zeros(())\n\nt_p = model(t_u, w, b)\nt_p","6487058b":"loss = loss_fn(t_p, t_c)\nloss","7237ddfc":"x = torch.ones(())\ny = torch.ones(3,1)\nz = torch.ones(1,3)\na = torch.ones(2,1,1)\n\nprint(\"x : \", x.shape,\"y : \", y.shape)\nprint(\"z : \", z.shape,\"a : \", a.shape)\n\nprint(\"x * y : \", (x * y).shape)\nprint(\"y * z : \",(y * z).shape)\nprint(\"y * z * a : \", (y * z * a).shape)","690f9c4f":"delta = 0.1\n\nloss_rate_of_change_w = \\\n    (loss_fn(model(t_u, w + delta, b), t_c) - \n    loss_fn(model(t_u,w-delta, b), t_c)) \/ (2.0 * delta)","7818dfe4":"learning_rate = 1e-2\n\nw = w - learning_rate * loss_rate_of_change_w","0fa3b8ac":"loss_rate_of_change_b = \\\n    (loss_fn(model(t_u, w, b + delta), t_c)-\n     loss_fn(model(t_u, w, b - delta), t_c)) \/ (2.0 * delta)\n\nb = b- learning_rate * loss_rate_of_change_b","ef6fc3ac":"# derivative of loss \ndef dloss_fn(t_p, t_c):\n    # same as\n    # sqared_diffs = (t_p - t_c)**2\n    # return squred_diffs.mean()\n    dsq_diffs = 2 * (t_p - t_c) \/ t_p.size(0)\n    return dsq_diffs","f8fbf502":"# derivative of model w.r.t w\ndef dmodel_dw(t_u, w, b):\n    return t_u","5836167b":"# derivativ of model w.r.t b\ndef dmodel_db(t_u, w, b):\n    return 1.0","99fb06e7":"# chain rule in action\ndef grad_dn(t_u, t_c, t_p, w, b):\n    dloss_dtp = dloss_fn(t_p, t_c)\n    dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)\n    dloss_db = dloss_dtp * dmodel_db(t_u, w, b)\n    return torch.stack([dloss_dw.sum(), dloss_db.sum()])","f4f7405b":"# how stack works\na = torch.tensor([1,2,3])\nb = torch.tensor([4,5,6])\ntorch.stack([a,b])","5b95f35a":"def training_loop(n_epochs, learning_rate,params, t_u, t_c):\n    for epoch in range(1, n_epochs+1):\n        w, b = params\n        \n        t_p = model(t_u, w,b)\n        loss = loss_fn(t_p, t_c)\n        grad = grad_dn(t_u, t_c, t_p, w, b)\n        \n        params = params - learning_rate * grad\n        \n        print(\"Epoch : \", epoch, \" loss : \", loss)\n    return params","4c05284c":"training_loop(n_epochs = 100, learning_rate=1e-2, params= torch.tensor([1.0, 0.0]), t_u=t_u, t_c=t_c)","5c3628b6":"def training_loop(n_epochs, learning_rate, params, t_u, t_c, print_params=True):\n    for epoch in range(1, n_epochs + 1):\n        w, b = params\n        \n        t_p = model(t_u, w, b)\n        loss = loss_fn(t_p, t_c)\n        grad = grad_dn(t_u, t_c, t_p, w, b)\n        \n        params = params - learning_rate * grad\n        \n        if epoch in {1,2,3,10,11,99,100,4000,5000}:\n            print('Epoch loss :', epoch, \" loss \", float(loss))\n            if print_params:\n                print('  params: ', params)\n                print('  grad: ', grad)\n        \n        if epoch in {4, 12, 101}:\n            print('...')\n        \n        if not torch.isfinite(loss).all():\n            break\n    return params","a29ddcdb":"training_loop(n_epochs = 100, learning_rate=1e-2, params= torch.tensor([1.0, 0.0]), t_u=t_u, t_c=t_c)","2b7f6fc6":"training_loop(n_epochs = 100, learning_rate=1e-4, params= torch.tensor([1.0, 0.0]), t_u=t_u, t_c=t_c)","13664d3a":"t_un = 0.1 * t_u","c0f0af56":"training_loop(\n    n_epochs=100, \n    learning_rate=1e-2, \n    params= torch.tensor([1.0, 0.0]),\n    t_u = t_un,\n    t_c = t_c)","b71c79c6":"new_params = training_loop(\n    n_epochs=5000, \n    learning_rate=1e-2, \n    params= torch.tensor([1.0, 0.0]),\n    t_u = t_un,\n    t_c = t_c)","70c4a0e9":"%matplotlib inline\nfrom matplotlib import pyplot as plt\n\nt_p = model(t_un, new_params[0], new_params[1])\n\nfig = plt.figure(dpi=600)\nplt.xlabel(\"Temprature (Fahrenheit)\")\nplt.ylabel(\"Temprature (Celsius)\")\nplt.plot(t_u.numpy(), t_p.detach().numpy())\nplt.plot(t_u.numpy(),t_c.numpy(), 'o')","c2ee2ab9":"# we have our model and loss function above\n# lets initialise the params\n\nparams = torch.tensor([1.0,0.0], requires_grad= True)","cc8b5073":"params.grad is None","9d2661e3":"loss = loss_fn(model(t_u, *params), t_c) \n# equivalent to loss = loss_fun(model(t_u, params[0], params[1]), t_c)\n\nloss.backward() ","9f4489d9":"# params got updated!\nparams.grad","cb8db859":"# setting params to zero\nif params.grad is not None:\n    params.grad.zero_()","6ec9e153":"def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n    for epoch in range(1, n_epochs +1):\n        \n        # this should be done prior to calling backward\n        if params.grad is not None:\n            params.grad.zero_()\n        \n        t_p = model(t_u, *params)\n        loss = loss_fn(t_p, t_c)\n        loss.backward()\n        \n        with torch.no_grad():\n            params -= learning_rate * params.grad\n        \n        if epoch % 500 == 0:\n            print('Epoch ', epoch, ' loss ', float(loss))\n            \n    return params","2fccc9df":"training_loop(\n    n_epochs =5000,\n    learning_rate = 1e-2,\n    params = torch.tensor([1.0, 0.0], requires_grad=True),\n    t_u = t_un,\n    t_c = t_c\n)","5f515caa":"import torch.optim as optim\n\ndir(optim)","cf28a02a":"params = torch.tensor([1.0, 0.0], requires_grad=True)\nlearning_rate = 1e-5\noptimizer = optim.SGD([params],lr=learning_rate)","bf661525":"t_p = model(t_u, *params)\nloss = loss_fn(t_p, t_c)\nloss.backward()\n\noptimizer.step()","7421d86c":"params","67a4d44d":"params =torch.tensor([1.0, 0.0], requires_grad=True)\nlearning_rate = 1e-2\noptimizer = optim.SGD([params], lr=learning_rate)","8d5f2dea":"t_p = model(t_un, *params)\nloss = loss_fn(t_p, t_c)","044f933d":"optimizer.zero_grad()\nloss.backward()\noptimizer.step()","145631d7":"params","20256ede":"def training_loop(n_epochs, optimizer, params, t_u, t_c):\n    for epoch in range(1,n_epochs+1):\n        t_p = model(t_u, *params)\n        loss = loss_fn(t_p, t_c)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if epoch%500 == 0:\n            print(epoch, float(loss))\n    \n    return params","76d3210c":"params = torch.tensor([1.0,0.0], requires_grad=True)\nlearning_rate = 1e-2\noptimizer = optim.SGD([params], lr=learning_rate)\n\ntraining_loop(\n    n_epochs=5000,\n    optimizer=optimizer,\n    params=params,\n    t_u=t_un,\n    t_c=t_c\n)","031df2f5":"params = torch.tensor([1.0, 0.0], requires_grad=True)\nlearning_rate = 1e-1\noptimizer=optim.Adam([params], lr=learning_rate)","0cd2abb8":"training_loop(\n    n_epochs=2000,\n    optimizer=optimizer,\n    params=params,\n    t_u=t_u,\n    t_c=t_c\n)","4214dc14":"n_samples = t_u.shape[0]\nn_val = int(0.2 * n_samples)","039e7af9":"shuffled_indices = torch.randperm(n_samples)","7130d39b":"train_indices = shuffled_indices[:-n_val]\nval_indices = shuffled_indices[-n_val:]","6cc79bdc":"train_indices, val_indices","442f9232":"train_t_u = t_u[train_indices]\ntrain_t_c = t_c[train_indices]","3d691ca7":"val_t_u = t_u[val_indices]\nval_t_c = t_c[val_indices]","30bc072e":"train_t_un = 0.1 * train_t_u\nval_t_un = 0.1 * val_t_u","33a57a1c":"def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):\n    for epoch in range(1, n_epochs + 1):\n        train_t_p = model(train_t_u, *params)\n        train_loss = loss_fn(train_t_p, train_t_c)\n        \n        val_t_p = model(val_t_u, *params)\n        val_loss = loss_fn(val_t_p, val_t_c)\n        \n        optimizer.zero_grad()\n        train_loss.backward()\n        optimizer.step()\n        \n        if epoch <=3 or epoch %500 == 0:\n            print(epoch, train_loss.item(), val_loss.item())\n    return params\n        ","c08e9052":"params = torch.tensor([1.0, 0.0], requires_grad=True)\nlearning_rate = 1e-2\noptimizer= optim.SGD([params], lr=learning_rate)","db682e8d":"training_loop(\nn_epochs = 3000,\noptimizer = optimizer,\nparams = params,\ntrain_t_u = train_t_un,\nval_t_u = val_t_un,\ntrain_t_c = train_t_c,\nval_t_c = val_t_c)","962060fe":"def training_loop(n_epocs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):\n    for epoch in range(1, n_epochs+1):\n        train_t_p = model(train_t_u, *params)\n        train_loss = loss_fn(train_t_p, train_t_c)\n        \n        with torch.no_grad():\n            val_t_p = model(val_t_u, *params)\n            val_loss = loss_fn(val_t_p, val_t_c)\n            assert val_loss.requires_grad == False \n            # checks that requires grad are forced false inside this block\n        \n        optimizer.zero_grad()\n        train_loss.backward()\n        optimizer.step()","3f92e522":"def calc_forward(t_u, t_c, is_train):\n    with torch.set_grad_enabled(is_train):\n        t_p = model(t_u, *params)\n        loss =  loss_fn(t_p,t_c)\n    return loss","893c6b13":"### Normalising inputs\n\nWe can see that the first-epoch gradient for the weight is about 50 times larger than\nthe gradient for the bias. This means the weight and bias live in differently scaled\nspaces. If this is the case, a learning rate that\u2019s large enough to meaningfully update\none will be so large as to be unstable for the other; and a rate that\u2019s appropriate for\nthe other won\u2019t be large enough to meaningfully change the first. That means we\u2019re\nnot going to be able to update our parameters unless we change something about our\nformulation of the problem\n\n There\u2019s a simpler way to keep things in check: changing the inputs so that the gradients aren\u2019t quite so different. We can make sure the range of the input doesn\u2019t get\ntoo far from the range of \u20131.0 to 1.0, roughly speaking. In our case, we can achieve\nsomething close enough to that by simply multiplying t_u by 0.1:\n","2f0ed5d3":"our loss decreases while we change parameters along the direction of gradient\ndescent. It doesn\u2019t go exactly to zero; this could mean there aren\u2019t enough iterations to\nconverge to zero, or that the data points don\u2019t sit exactly on a line.\n\nmeasurements were not perfectly accurate, or there was noise involved in the reading.\n But look: the values for w and b look an awful lot like the numbers we need to use\nto convert Celsius to Fahrenheit (after accounting for our earlier normalization when\nwe multiplied our inputs by 0.1). The exact values would be w=5.5556 and b=-\n17.7778. Our fancy thermometer was showing temperatures in Fahrenheit the whole time","7c304e97":"The only tensors these two graphs have in common are the parameters. When we call\nbackward on train_loss, we run backward on the first graph. In other words, we\naccumulate the derivatives of train_loss with respect to the parameters based on the\ncomputation generated from train_t_u.\n If we (incorrectly) called backward on val_loss as well, we would accumulate the\nderivatives of val_loss with respect to the parameters on the same leaf nodes.","260ec2d6":"### simplest model\n\nWe have a model with some unknown parameters, and we\nneed to estimate those parameters so that the error between predicted outputs and\nmeasured values is as low as possible. We notice that we still need to exactly define a\nmeasure of the error. Such a measure, which we refer to as the loss function, should be\nhigh if the error is high and should ideally be as low as possible for a perfect match.\nOur optimization process should therefore aim at finding w and b so that the loss\nfunction is at a minimum. \n\nWe\u2019ve already created our data tensors, so now let\u2019s write out the model as a\nPython function:","703216f5":"### visualizing\n\nLet\u2019s revisit something we did right at the start: plotting our data. Seriously, this is the\nfirst thing anyone doing data science should do. Always plot the heck out of the data:","7a398a0f":"so its not really working. Lets improve our training loop.","c5bf679c":"We could have any number of tensors with requires_grad set to True and any composition of functions. In this case, PyTorch would compute the derivatives of the loss\nthroughout the chain of functions (the computation graph) and accumulate their values in the grad attribute of those tensors (the leaf nodes of the graph).\n\n Alert! Big gotcha ahead. This is something PyTorch newcomers\u2014and a lot of more\nexperienced folks, too\u2014trip up on regularly. We just wrote accumulate, not store.\nWARNING Calling backward will lead derivatives to accumulate at leaf nodes.\nWe need to zero the gradient explicitly after using it for parameter updates.","9cda1f3d":"Note that our code updating params is not quite as straightforward as we might have\nexpected. There are two particularities. \n\nFirst, we are encapsulating the update in a\nno_grad context using the Python with statement. This means within the with block,\nthe PyTorch autograd mechanism should look away:\n11 that is, not add edges to the forward graph. In fact, when we are executing this bit of code, the forward graph that\nPyTorch records is consumed when we call backward, leaving us with the params leaf\nnode. But now we want to change this leaf node before we start building a fresh forward graph on top of it\n\n Second, we update params in place. This means we keep the same params tensor\naround but subtract our update from it. When using autograd, we usually avoid inplace updates because PyTorch\u2019s autograd engine might need the values we would be\nmodifying for the backward pass. Here, however, we are operating without autograd,\nand it is beneficial to keep the params tensor","851ff127":"All we have to do to populate it is to start with a tensor with requires_grad set to\nTrue, then call the model and compute the loss, and then call backward on the loss\ntensor:","db5b5fb1":"### creating functions out of this","db756fce":"We started this chapter with a big question: how is it that a machine can learn from\nexamples? We spent the rest of the chapter describing the mechanism with which a\nmodel can be optimized to fit data. We chose to stick with a simple model in order to\nsee all the moving parts without unneeded complications.","45f5365b":"Every optimizer constructor takes a list of parameters (aka PyTorch tensors, typically\nwith requires_grad set to True) as the first input. All parameters passed to the optimizer are retained inside the optimizer object so the optimizer can update their values and access their grad attribute","1a2c094c":"Nice\u2014the behavior is now stable. But there\u2019s another problem: the updates to parameters are very small, so the loss decreases very slowly and eventually stalls. We could\nobviate this issue by making learning_rate adaptive: that is, change according to the\nmagnitude of updates.","edf7ffad":"The optimizer is not the only flexible part of our training loop. Let\u2019s turn our attention to the model. In order to train a neural network on the same data and the same\nloss, all we would need to change is the model function. It wouldn\u2019t make particular\nsense in this case, since we know that converting Celsius to Fahrenheit amounts to a\nlinear transformation, but we\u2019ll do it anyway in chapter 6. We\u2019ll see quite soon that\nneural networks allow us to remove our arbitrary assumptions about the shape of the\nfunction we should be approximating. Even so, we\u2019ll see how neural networks manage\nto be trained even when the underlying processes are highly nonlinear","263168a4":"Here, we denote the normalized version of t_u by appending an n to the variable\nname. At this point, we can run the training loop on our normalized input:","3f0f313e":"The value of params is updated upon calling step without us having to touch it ourselves! What happens is that the optimizer looks into params.grad and updates\nparams, subtracting learning_rate times grad from it, exactly as in our former handrolled code.\n Ready to stick this code in a training loop? Nope! The big gotcha almost got us\u2014\nwe forgot to zero out the gradients. Had we called the previous code in a loop, gradients would have accumulated in the leaves at every call to backward, and our gradient\ndescent would have been all over the place! Here\u2019s the loop-ready code, with the extra\nzero_grad at the correct spot (right before the call to backward):","f1497dcf":"Here SGD stands for stochastic gradient descent. Actually, the optimizer itself is exactly a\nvanilla gradient descent (as long as the momentum argument is set to 0.0, which is the\ndefault). The term stochastic comes from the fact that the gradient is typically obtained\nby averaging over a random subset of all input samples, called a minibatch. However, the\noptimizer does not know if the loss was evaluated on all the samples (vanilla) or a random subset of them (stochastic), so the algorithm is literally the same in the two cases.","26e2e03a":"Each optimizer exposes two methods: zero_grad and step. zero_grad zeroes the\ngrad attribute of all the parameters passed to the optimizer upon construction. step\nupdates the value of those parameters according to the optimization strategy implemented by the specific optimizer","dd588d75":"### Defining the gradient function\n\nPutting all of this together, the function returning the gradient of the loss with respect\nto w and b is","15374f51":" In order to address this, PyTorch allows us to switch off autograd when we don\u2019t\nneed it, using the torch.no_grad context manager.12 We won\u2019t see any meaningful\nadvantage in terms of speed or memory consumption on our small problem. However, for larger models, the differences can add up. We can make sure this works by\nchecking the value of the requires_grad attribute on the val_loss tensor:","987be41b":"This represents the basic parameter-update step for gradient descent. By reiterating\nthese evaluations (and provided we choose a small enough learning rate), we will\nconverge to an optimal value of the parameters for which the loss computed on the\ngiven data is minimal","9492b2d1":"Notice the requires_grad=True argument to the tensor constructor? That argument\nis telling PyTorch to track the entire family tree of tensors resulting from operations\non params. In other words, any tensor that will have params as an ancestor will have\naccess to the chain of functions that were called to get from params to that tensor. In\ncase these functions are differentiable (and most PyTorch tensor operations will be),\nthe value of the derivative will be automatically populated as a grad attribute of the\nparams tensor.\n In general, all PyTorch tensors have an attribute named grad. Normally, it\u2019s None:","fdef353e":"Yay ! result as the previous one. Lets look at optimisers next.","8e3d4d14":"### Autograd enabled training loop","3e41cad5":"Even though we set our learning rate back to 1e-2, parameters don\u2019t blow up during\niterative updates. Let\u2019s take a look at the gradients: they\u2019re of similar magnitude, so\nusing a single learning_rate for both parameters works just fine. We could probably\ndo a better job of normalization than a simple rescaling by a factor of 10, but since\ndoing so is good enough for our needs, we\u2019re going to stick with that for now.\n\nrunning for 5000 epochs:","ccc77273":"### trying other optimizers","be32faa2":"Here we are not being entirely fair to our model. The validation set is really small, so\nthe validation loss will only be meaningful up to a point. In any case, we note that the\nvalidation loss is higher than our training loss, although not by an order of magnitude","f5d5c467":"This is the explanation of machine learning basics from from ch5 of deep learning with pytorch book.\n","463cdc7c":"### Broadcasting in pytorch\n\nresult of using vector multiplication","7b563b7e":"At this point, the grad attribute of params contains the derivatives of the loss with\nrespect to each element of params.\n When we compute our loss while the parameters w and b require gradients, in\naddition to performing the actual computation, PyTorch creates the autograd graph\nwith the operations (in black circles) as nodes, as shown in the top row of figure 5.10. When we call loss.backward(), PyTorch traverses this graph in the reverse\ndirection to compute the gradients, ","a1fe01f5":"### Using the autograd in pytorch\n\nLets try wonderful feature called autograd in pytorch!\n\nPyTorch tensors can remember where they come from, in terms of the operations and\nparent tensors that originated them, and they can automatically provide the chain of\nderivatives of such operations with respect to their inputs. This means we won\u2019t need\nto derive our model by hand;10 given a forward expression, no matter how nested,\nPyTorch will automatically provide the gradient of that expression with respect to its\ninput parameters.","fcd9e3d4":"From the previous training loop, we can appreciate that we only ever call backward on\ntrain_loss. Therefore, errors will only ever backpropagate based on the training\nset\u2014the validation set is used to provide an independent evaluation of the accuracy of\nthe model\u2019s output on data that wasn\u2019t used for training.","5c3d2929":"### optimisers \n\nIn the example code, we used vanilla gradient descent for optimization, which worked\nfine for our simple case. Needless to say, there are several optimization strategies and\ntricks that can assist convergence, especially when models get complicated.\n\nPyTorch abstracts the optimization strategy away from user code:\nthat is, the training loop we\u2019ve examined. This saves us from the boilerplate busywork\nof having to update each and every parameter to our model ourselves. The torch\nmodule has an optim submodule where we can find classes implementing different\noptimization algorithms","de57d6ba":"### Overtraining\n\nWait, what happened? Our training process literally blew up, leading to losses becoming inf. This is a clear sign that params is receiving updates that are too large, and\ntheir values start oscillating back and forth as each update overshoots and the next\novercorrects even more. The optimization process is unstable: it diverges instead of\nconverging to a minimum.\n\n\nHow can we limit the magnitude of learning_rate * grad? Well, that looks easy. We\ncould simply choose a smaller learning_rate, and indeed, the learning rate is one of\nthe things we typically change when training does not go as well as we would like.8\n We\nusually change learning rates by orders of magnitude, so we might try with 1e-3 or\n1e-4, which would decrease the magnitude of the updates by orders of magnitude.\nLet\u2019s go with 1e-4 and see how it works out:","da248172":"### splitting a dataset","7a133315":"### Using gradient descent optimiser","f413c138":"### Training loop\n\nWe now have everything in place to optimize our parameters. Starting from a tentative\nvalue for a parameter, we can iteratively apply updates to it for a fixed number of iterations, or until w and b stop changing. There are several stopping criteria; for now,\nwe\u2019ll stick to a fixed number of iterations.\n\nSince we\u2019re at it, let\u2019s introduce another piece of terminology. We call a training iteration during which we update the parameters for all of our training samples an epoch","9af0b17b":"### model in practice\n\n We can now initialize the parameters, invoke the model,","f12bb295":"### loss function\n\nA loss function (or cost function) is a function that computes a single numerical value\nthat the learning process will attempt to minimize. The calculation of loss typically\ninvolves taking the difference between the desired outputs for some training samples\nand the outputs actually produced by the model when fed those samples. In our case,\nthat would be the difference between the predicted temperatures t_p output by our\nmodel and the actual measurements: t_p \u2013 t_c.\n We need to make sure the loss function makes the loss positive both when t_p is\ngreater than and when it is less than the true t_c, since the goal is for t_p to match t_c.\nWe have a few choices, the most straightforward being |t_p \u2013 t_c| and (t_p \u2013 t_c)^2.\nBased on the mathematical expression we choose, we can emphasize or discount certain\nerrors. Conceptually, a loss function is a way of prioritizing which errors to fix from our\ntraining samples, so that our parameter updates result in adjustments to the outputs for\nthe highly weighted samples instead of changes to some other samples\u2019 output that had\na smaller loss.\n\nNote that we are building a tensor of differences, taking their square element-wise,\nand finally producing a scalar loss function by averaging all of the elements in the\nresulting tensor. It is a mean square loss.","54b517cc":"### updating values based on loss\n\nGradient descent is not that different from the scenario we just described. The idea is\nto compute the rate of change of the loss with respect to each parameter, and modify\neach parameter in the direction of decreasing loss. Just like when we were fiddling\nwith the knobs, we can estimate the rate of change by adding a small number to w and\nb and seeing how much the loss changes in that neighborhood:","32988be4":"Using the related set_grad_enabled context, we can also condition the code to run\nwith autograd enabled or disabled, according to a Boolean expression\u2014typically indicating whether we are running in training or inference mode. We could, for instance,\ndefine a calc_forward function that takes data as input and runs model and loss_fn\nwith or without autograd according to a Boolean train_is argument:","3c3496a6":"### data\n\nHere, the t_c values are temperatures in Celsius, and the t_u values are our unknown\nunits. We can expect noise in both measurements, coming from the devices themselves and from our approximate readings. ","0cf42bda":"This is saying that in the neighborhood of the current values of w and b, a unit\nincrease in w leads to some change in the loss. If the change is negative, then we need\nto increase w to minimize the loss, whereas if the change is positive, we need to\ndecrease w. By how much? Applying a change to w that is proportional to the rate of\nchange of the loss is a good idea, especially when the loss has several parameters: we\napply a change to those that exert a significant change on the loss. It is also wise to\nchange the parameters slowly in general, because the rate of change could be dramatically different at a distance from the neighborhood of the current w value. Therefore,\nwe typically should scale the rate of change by a small factor. This scaling factor has\nmany names; the one we use in machine learning is learning_rate:\n"}}