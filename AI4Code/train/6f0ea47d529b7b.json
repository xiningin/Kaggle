{"cell_type":{"e4c38e4a":"code","468457e4":"code","c538613d":"code","de45dfa1":"code","d52e856b":"code","6bce5f1a":"code","1b9ebd5e":"code","21a32cbd":"code","3694ff32":"code","cf7982e3":"code","26d16da6":"code","4ddb6403":"code","b70991c1":"code","e987c8c4":"code","62dcd26d":"code","8cfaf718":"code","6bc09b58":"code","1fc57028":"code","b9d08706":"code","76034985":"code","20b68ec4":"code","0a07cf8f":"code","510bbd11":"code","12ecb222":"code","0fcee7f6":"code","2371dc1f":"code","93d553d6":"code","95ed9f2f":"code","3a79b262":"code","c5d17ae9":"code","3b53dd5f":"code","f45bb3e1":"code","c385915a":"code","a91908ee":"code","260f7200":"code","7ad6e347":"code","47cf3d0f":"code","829de0a1":"code","cf0451cf":"code","9d420e94":"code","44c4e2a6":"code","a5e7f8cc":"code","d4faebb3":"code","3ac1dda3":"code","8cfdcc9d":"code","06a39ad0":"code","a50e8a95":"code","68e39fbe":"code","4ffdec6f":"code","834c781e":"code","fbd2c1d6":"code","3117ce49":"code","e4e737bb":"code","44ddc074":"code","e761b676":"code","9bf63946":"code","61a529a1":"code","bb1176cf":"code","ef6c1483":"code","057d29e6":"code","048d0add":"code","f5cd6a36":"code","68b88a90":"code","8340734a":"code","d3590520":"code","32d8333b":"code","5f0cb24d":"code","14e5b41a":"code","5491ea5c":"code","33521605":"code","0e3d6f90":"code","a1cc3efd":"code","cf27dcf1":"code","ca610a94":"code","17d60437":"code","8cae5476":"code","37ad4796":"code","0c5ef543":"code","c79ad172":"code","5e3079aa":"code","756dbe05":"code","d356add3":"code","5bcd3378":"code","fd941143":"code","615cc864":"code","8c2f5b00":"code","6dfba80a":"code","6a00f1b1":"code","38d2c705":"code","2b855f7a":"code","bfb30326":"code","6d57d67b":"code","76a2ced2":"markdown","3d631bc7":"markdown","aab43c5d":"markdown","8770df85":"markdown","1228e52c":"markdown","973632f2":"markdown","261b0742":"markdown"},"source":{"e4c38e4a":"import pandas as pd\nimport numpy as np\nimport re\nfrom keras.preprocessing import image\nimport matplotlib.pyplot as plt\nfrom keras.applications.resnet50 import ResNet50,preprocess_input\nfrom keras.models import Model\nfrom keras.layers import *\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical","468457e4":"def Read_text_file(path):\n    with open(path) as f:\n        caption=f.read()\n        f.close()\n    return caption","c538613d":"import os","de45dfa1":"os.listdir('..\/input\/flickr8k\/Flickr_Data\/Flickr_Data\/')","d52e856b":"captions=Read_text_file('..\/input\/flickr8k\/Flickr_Data\/Flickr_Data\/Flickr_TextData\/Flickr8k.token.txt')","6bce5f1a":"# captions","1b9ebd5e":"len(captions.split('\\n'))","21a32cbd":"captions=captions.split('\\n')[:-1]","3694ff32":"len(captions)","cf7982e3":"a=captions[1000]","26d16da6":"a","4ddb6403":"a=a.split('\\t')[0][:-6]","b70991c1":"a","e987c8c4":"cap_dict={}","62dcd26d":"for i in captions:\n    a,b=i.split('\\t')\n    a=a[:-6]\n    if cap_dict.get(a) is None:\n        cap_dict[a]=[]\n    cap_dict[a].append(b)","8cfaf718":"len(cap_dict)","6bc09b58":"cap_dict[a]","1fc57028":"# np.save('description_file.npy',cap_dict)","b9d08706":"def sentence_cleaning(sentence):\n    sentence=sentence.lower()\n    sentence=re.sub('[^a-z]+',' ',sentence)\n    sentence=sentence.split()\n    sentence=[s for s in sentence if len(s)>1]\n    sentence=' '.join(sentence)\n    return(sentence)\n    ","76034985":"z='A black and brown dog is laying on a @white shaggy carpet having number # 79 .'","20b68ec4":"sentence_cleaning(z)","0a07cf8f":"for key in cap_dict:\n    for j in range(len(cap_dict[key])):\n        cap_dict[key][j]=sentence_cleaning(cap_dict[key][j])","510bbd11":"cap_dict[a]","12ecb222":"# np.save('description_file_2.npy',cap_dict)","0fcee7f6":"word_dic={}","2371dc1f":"for i in cap_dict:\n    for j in cap_dict[i]:\n        l=j.split()\n        for k in l:\n            if k not in word_dic:\n                word_dic[k]=1\n            else:\n                word_dic[k]+=1\n    ","93d553d6":"len(word_dic)","95ed9f2f":"len(word_dic)","3a79b262":"final_words=[x for x in word_dic if word_dic[x]>10]","c5d17ae9":"len(final_words)","3b53dd5f":" final_words","f45bb3e1":"for i in cap_dict:\n    for j in range(len(cap_dict[i])):\n        cap_dict[i][j]='startseq '+cap_dict[i][j]+' endseq'","c385915a":"cap_dict[a]","a91908ee":"with open('..\/input\/flickr8k\/Flickr_Data\/Flickr_Data\/Flickr_TextData\/Flickr_8k.trainImages.txt') as f:\n    train_img=f.read()","260f7200":"train_img_list=[x.split('.')[0] for x in train_img.split('\\n')[:-1]]","7ad6e347":"train_img_list","47cf3d0f":"train_description={}","829de0a1":"for i in train_img_list:\n    train_description[i]=cap_dict[i]","cf0451cf":"a","9d420e94":"img=image.load_img('..\/input\/flickr8k\/Flickr_Data\/Flickr_Data\/Images\/106490881_5a2dd9b7bd.jpg')","44c4e2a6":"img=image.img_to_array(img)","a5e7f8cc":"img.shape","d4faebb3":"plt.imshow((img\/255)*1)","3ac1dda3":"model=ResNet50(weights='imagenet',input_shape=(224,224,3))","8cfdcc9d":"model.summary()","06a39ad0":"new_model=Model(model.input,model.layers[-2].output)","a50e8a95":"def preprocess_img(path):\n    img=image.load_img(path,target_size=(224,224,3))\n    img=image.img_to_array(img)\n    img=img.reshape(1,224,224,3)\n    img=preprocess_input(img)#mormalizing the img\n    return img","68e39fbe":"img=preprocess_img('..\/input\/flickr8k\/Flickr_Data\/Flickr_Data\/Images\/106490881_5a2dd9b7bd.jpg')","4ffdec6f":"plt.imshow(img[0])","834c781e":"def encode_img(path):\n    img=preprocess_img(path)\n    feature_vector=new_model.predict(img)\n    feature_vector=feature_vector.reshape((-1,))\n    return feature_vector\n    ","fbd2c1d6":"train_description['1244140539_da4804d828']","3117ce49":"encoded_img_dic={}\ns=0\nfor i in train_description:\n    path='..\/input\/flickr8k\/Flickr_Data\/\/Flickr_Data\/Images\/'+i+'.jpg'\n    encoded_img_dic[i]=encode_img(path)\n    s+=1\n    if s%100==0:\n        print(s)","e4e737bb":"np.save('encoded_img_dic.npy',encoded_img_dic)","44ddc074":"!ls -l --b=M  .\/encoded_img_dic.npy | cut -d \" \" -f5","e761b676":"s=1\nword_to_idx={}\nidx_to_word={}\nfor i in final_words:\n    word_to_idx[i]=s\n    idx_to_word[s]=i\n    s+=1","9bf63946":"len(word_to_idx)","61a529a1":"idx_to_word[1845]","bb1176cf":"### Two Special words\nword_to_idx['startseq']=1846\nword_to_idx['endseq']=1847\nidx_to_word[1846]='startseq'\nidx_to_word[1847]='endseq'","ef6c1483":"len(word_to_idx)","057d29e6":"vocab_size=len(word_to_idx)+1 # adding one for 0 because that will also in our vector","048d0add":"vocab_size","f5cd6a36":"sen_len=[]\nfor i in train_description:\n    for sen in train_description[i]:\n        sen_len.append(len(sen.split()))","68b88a90":"len(sen_len)","8340734a":"plt.hist(sen_len,bins=20)","d3590520":"sen_len[0]","32d8333b":"max_len=20","5f0cb24d":"def data_generator(train_description,vocab_size,word_to_idx,encoded_img_dic,max_len,batch_size):\n    X1,X2,y=[],[],[]\n    n=0\n    while True:\n        for key,desc_list in train_description.items():\n            n+=1\n            encoding_of_photo=encoded_img_dic[key]\n            for desc in desc_list:\n                seq=[word_to_idx[i] for i in desc.split() if i in word_to_idx]\n                for i in range(1,len(seq)):\n                    xi=seq[0:i]\n                    yi=seq[i]\n                    \n                    xi=pad_sequences([xi],maxlen=max_len,value=0,padding='post')[0]\n                    yi=to_categorical([yi],num_classes=vocab_size)[0]\n                    \n                    \n                    X1.append(encoding_of_photo)\n                    X2.append(xi)\n                    y.append(yi)\n                if n==batch_size:\n                    yield [np.array(X1),np.array(X2)],np.array(y)\n                    X1,X2,y=[],[],[]\n                    n=0\n                    \n                ","14e5b41a":"# with open('glove.6B.50d.txt',encoding='utf8') as f:\n#     glove_data=f.read()\n\n# len(glove_data)\n\n# type(glove_data)\n\n# glove_data=glove_data.split('\\n')\n\n# type(glove_data)\n\n# len(glove_data)\n\n# glove_data=glove_data[:-1]\n\n# len(glove_data)\n\n# glove_data[0].split()[0]\n\n# embedding_index={}\n# for line in glove_data:\n#     line=line.split()\n#     word=line[0]\n#     embeding=np.array(line[1:],dtype='float')\n#     embedding_index[word]=embeding\n    \n\n# len(embedding_index)\n\n# embedding_index['the'].shape\n\n# def get_embedding_matrix():\n#     dim=50\n#     matrix=np.zeros((vocab_size,dim))\n#     for word,number in word_to_idx.items():\n#         embedding_vector=embedding_index.get(word)\n#         if embedding_vector is not None:\n#             matrix[word_to_idx[word]]=embedding_vector\n#     return matrix\n    \n\n# embedding_matrix=get_embedding_matrix()\n\n# len(embedding_matrix)\n\n# embedding_matrix[word_to_idx['the']]\n\n","5491ea5c":"# np.save('word_to_idx.npy',word_to_idx)\n\n# z=np.load('word_to_idx.npy',allow_pickle=True).item()\n\n# type(z)\n\n# np.save('embedding_matrix.npy',embedding_matrix)","33521605":"embedding_matrix=np.load('..\/input\/embdding-data\/embedding_matrix.npy')","0e3d6f90":"embedding_matrix[3]","a1cc3efd":"encoded_img_dic['1244140539_da4804d828'].shape","cf27dcf1":"## For images\ninput_img_features=Input(shape=(2048,))\ninp_img1=Dropout(0.3)(input_img_features)\ninp_img2=Dense(256,activation='relu')(inp_img1)\n","ca610a94":"vocab_size","17d60437":"input_captions=Input(shape=(max_len,))\ninp_cap1=Embedding(input_dim=vocab_size,output_dim=50,mask_zero=True)(input_captions)\ninp_cap2=Dropout(0.3)(inp_cap1)\ninp_cap3=LSTM(256)(inp_cap2)","8cae5476":"decoder1=add([inp_img2,inp_cap3])\ndecoder2=Dense(256,activation='relu')(decoder1)\noutputs=Dense(vocab_size,activation='softmax')(decoder2)","37ad4796":"actual_model=Model(inputs=[input_img_features,input_captions],outputs=outputs)\n","0c5ef543":"actual_model.summary()","c79ad172":"actual_model.layers[2].output","5e3079aa":"actual_model.layers[2].set_weights([embedding_matrix])\nactual_model.layers[2].trainable=False","756dbe05":"actual_model.compile(loss='categorical_crossentropy',optimizer='adam')","d356add3":"epochs=20\nbatch_size=3\nsteps=len(train_description)\/\/batch_size","5bcd3378":"def train():\n    for i in range(epochs):\n        generator=data_generator(train_description,vocab_size,word_to_idx,encoded_img_dic,max_len,batch_size)\n        actual_model.fit_generator(generator,epochs=1,steps_per_epoch=steps,verbose=1)\n        actual_model.save('.\/Models\/model'+str(i)+'.h5')\n        ","fd941143":"os.mkdir('Models')","615cc864":"train()","8c2f5b00":"max_len","6dfba80a":"def predict_caption(img):\n    img=img.reshape(1,224,224,3)\n    img=preprocess_input(img)\n    feature_vector=new_model.predict(img)\n    feature_vector=feature_vector.reshape((1,2048,1))\n    in_text='startseq'\n    for i in range(max_len):\n        seq=[word_to_idx[w] for w in in_text.split() if w in word_to_idx]\n        seq=pad_sequences([seq],maxlen=max_len,padding='post')\n        y_pred=actual_model.predict([feature_vector,seq])\n        y_pred=y_pred.argmax()\n        word=idx_to_word[y_pred]\n        in_text+=' '+word\n        \n        if word=='endseq':\n            break\n    final_caption=in_text.split()[1:-1]\n    final_caption=' '.join(final_caption)\n    return final_caption","6a00f1b1":"img=image.load_img('..\/input\/flickr8k\/Flickr_Data\/Flickr_Data\/Images\/1007320043_627395c3d8.jpg',target_size=(224,224,3))\nimg=image.img_to_array(img)","38d2c705":"plt.imshow(img\/255)","2b855f7a":"predict_caption(img)","bfb30326":"x=0\nfor i in os.listdir('..\/input\/flickr8k\/Flickr_Data\/Flickr_Data\/Images\/'):\n    i='..\/input\/flickr8k\/Flickr_Data\/Flickr_Data\/Images\/'+i\n    img=image.load_img(i,target_size=(224,224,3))\n    img=image.img_to_array(img)\n    plt.imshow(img\/255)\n    plt.show()\n    print(predict_caption(img))\n    x+=1\n    if x>20:\n        break\n    ","6d57d67b":"ls -l --b=M  .\/Models\/model15.h5 | cut -d \" \" -f5","76a2ced2":"### Cleaning every sentence of dictionary","3d631bc7":"## Creating model ","aab43c5d":"## Custom Data Generator","8770df85":"## Training the Model","1228e52c":"## Model Architecture","973632f2":"## Saving and loading a dictionary using numpy","261b0742":"## data preprocessing for captions"}}