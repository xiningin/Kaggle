{"cell_type":{"9af9becf":"code","836a0a49":"code","2dafef22":"code","31fb29c1":"code","38081d24":"code","6db7884f":"code","a775a441":"code","97bd1953":"code","c4160c29":"code","3428a7ad":"code","633143d7":"code","c0a99008":"code","d16faf8e":"code","2603ee36":"markdown","79549544":"markdown","851186f7":"markdown","04fe473f":"markdown","85619e2b":"markdown","cca63f87":"markdown","709bbd2b":"markdown"},"source":{"9af9becf":"import os\nimport re \nimport math\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.utils import to_categorical, Sequence, plot_model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.models import Sequential, Model\nfrom keras.callbacks import EarlyStopping, Callback, ModelCheckpoint\nfrom keras.layers import Embedding, Dense, Dropout, LSTM, Input, BatchNormalization, concatenate\n\nfrom tensorflow import set_random_seed\nfrom numpy.random import seed\nset_random_seed(2)\nseed(40)","836a0a49":"data = pd.read_csv('..\/input\/facebook-antivaccination-dataset\/posts_full.csv', \n                   index_col=0).dropna(subset=['text'])\ndata = data[['text', 'anti_vax']]\ndata.head()","2dafef22":"discord = pd.read_csv('..\/input\/game-of-cones\/Game of Cones - battle-for-the-cone 231969664027066368.csv', \n                      sep=';').drop('Unnamed: 4', axis=1).dropna(subset=['Content'])\ndiscord['anti_vax'] = False\ndiscord = discord[['Content', 'anti_vax']].rename({'Content': 'text'}, axis=1)\ndiscord.head()","31fb29c1":"data = pd.concat([data, discord]).reset_index(drop=True)\ndata.shape","38081d24":"#Remove unwanted punctuation\nFILTER_STRING = '\"$%&()*+,.!?-\/:;<=>[\\\\]@#^_`{|}~\\t\\n'\nUNWANTED = {x for x in FILTER_STRING}\ndef filter_unwanted(x):\n    x = \"\".join([c if c not in UNWANTED else \" \" for c in x]).lower()\n    return x.encode(\"utf8\").decode(\"ascii\",'ignore')","6db7884f":"data['text'] = [sentence for sentence in data.text.apply(filter_unwanted)]\ndata.text.tail()","a775a441":"#Add n-gram input sequences\nNUM_WORDS = 50_000\nMAX_SEQUENCE_LENGTH = 200\n\ntokenizer = Tokenizer(num_words=NUM_WORDS, filters=FILTER_STRING, \n                      lower=True)\ntokenizer.fit_on_texts(data.text)\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)","97bd1953":"X = tokenizer.texts_to_sequences(data.text)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nX[0][-40:], data.text.head(1)","c4160c29":"X_train, X_eval, y_train, y_eval = train_test_split(X, data.anti_vax.values, \n                                                    test_size=0.2, \n                                                    random_state=3000)","3428a7ad":"model = Sequential()\nmodel.add(Embedding(NUM_WORDS, 10, input_length=(X.shape[1])))\nmodel.add(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dropout(rate=0.3))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nplot_model(model, to_file='model.png', show_shapes=True, show_layer_names=False)","633143d7":"checkpoint = ModelCheckpoint(\"model-{epoch:02d}-{val_loss:.2f}.hdf5\", \n                             monitor='val_loss', verbose=0, \n                             save_best_only=True, period=1)\nstopping = EarlyStopping(monitor='val_loss', patience=2)\nhistory = model.fit(X_train, y_train, epochs=20, \n                    verbose=2, batch_size=32, validation_data=(X_eval, y_eval), \n                    callbacks=[checkpoint, stopping])","c0a99008":"def plot_epochs(results, col, **kwargs):\n    def plot_epoch_helper(hist_df, col, ax):\n        ax.plot(hist_df[col], **kwargs)\n        ax.set_title(col + ' per epoch')\n        ax.set_ylabel(col)\n        ax.set_xlabel('epoch')\n        for sp in ax.spines:\n            ax.spines[sp].set_visible(False)\n        ax.yaxis.grid(True, alpha=0.3)\n        ax.legend(labels=[n[0] for n in results])\n        ax.set_ylim(0, 1)\n    fig, ax = plt.subplots(figsize=(21, 10))\n    for name, hist in results:\n        plot_epoch_helper(hist, col, ax)\nplot_epochs([('Model', pd.DataFrame(history.history))], 'val_Main_Output_loss')","d16faf8e":"plot_epochs([('Model', pd.DataFrame(history.history))], 'val_Aux_Output_loss')","2603ee36":"<h1 style=\"text-align:center\">Final Model<\/h1>\n<img src=\"model.png\" width=\"200\">","79549544":"## Build Training Sets","851186f7":"## Comparing Loss per Epoch","04fe473f":"## Build Tokenizer","85619e2b":"## Prepare Data","cca63f87":"## Train","709bbd2b":"## Build Model"}}