{"cell_type":{"48169331":"code","0813ad1c":"code","e72a5731":"code","61625ca7":"code","980eb5c5":"code","7b3a159c":"code","45329a61":"code","a248fada":"code","2680923c":"code","2751d293":"code","fca25f79":"code","40e00b05":"code","b80df8ad":"code","e4d6dc48":"code","4b28e9a3":"code","275fe317":"code","e02c4b3f":"code","83553897":"code","5976431a":"code","3e61065e":"code","fd354383":"code","8954aec6":"code","a54ca983":"code","52a518a4":"code","082c1c3b":"code","42a24ab5":"code","5685218f":"code","bf2fd92c":"code","992f5fbf":"code","b75ee88b":"markdown","14393e6a":"markdown","b616ef4b":"markdown","60349cae":"markdown","219f8647":"markdown","2c85273a":"markdown","0a2c12f7":"markdown","8269a8e3":"markdown","89a8e7f5":"markdown","936e81e2":"markdown","f289b9c4":"markdown","e997e919":"markdown","bbc9e875":"markdown","daf07303":"markdown","5fada875":"markdown","f752253f":"markdown","590758a3":"markdown","9bb9035b":"markdown","bf97ce58":"markdown","fbccdc22":"markdown","46efed8f":"markdown","dc26c4c1":"markdown"},"source":{"48169331":"# Installing nlp library for loading external dataset.\n!pip install -q nlp\n!pip install -q wordcloud","0813ad1c":"# Importing Libs\nimport nlp\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport random, os, math, cv2\nimport matplotlib.pyplot as plt\nimport tensorflow.keras.backend as K\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import KFold, train_test_split\nfrom transformers import BertTokenizer, TFBertModel, AutoTokenizer, TFAutoModel\n\n# Text visualization\nimport nltk\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS","e72a5731":"#PREPAIRING TPU\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n\nprint('Number of replicas:', strategy.num_replicas_in_sync)","61625ca7":"# Loading original data\ntrain_csv = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/train.csv\")\ntest_csv  = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/test.csv\")","980eb5c5":"# CONFIGURATION\n\nAUTO = tf.data.experimental.AUTOTUNE\nMODEL_NAME = \"jplu\/tf-xlm-roberta-base\"\nREPLICAS  = strategy.num_replicas_in_sync\nTOKENIZER  = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n# HYPER-PARAMS\nBATCH_SIZE = 16 * REPLICAS\nMAX_LEN = 192\nEPOCHS = 8 # Due to running time for notebook. Please try to train model on atleast 5-10 epochs.\nSEED = 48\nFONT_DIR = \"..\/input\/font-dataset\/FontScripts\/\"\n\nnp.random.seed(SEED)\nrandom.seed(SEED)","7b3a159c":"def prepare_input_v2(sentences):\n    \"\"\" Converts the premise and hypothesis to the input format required by model\"\"\"\n    sen_enc = TOKENIZER.batch_encode_plus(sentences,\n                                          pad_to_max_length=True,\n                                          return_attention_mask=False,\n                                          return_token_type_ids=False,\n                                          max_length=MAX_LEN)\n    return np.array(sen_enc[\"input_ids\"])\n\ndef get_dataset(features, labels=None, labelled=True, batch_size=8, repeat=True, shuffle=True):\n    \"\"\"Generates a tf.data pipeline from the encoded sentences.\"\"\"\n    if labelled:\n        ds = tf.data.Dataset.from_tensor_slices((features, labels))\n    else:\n        ds = tf.data.Dataset.from_tensor_slices(features)\n\n    if repeat:\n        ds = ds.repeat()\n        \n    if shuffle:\n        ds = ds.shuffle(2048)\n        \n    ds = ds.batch(batch_size*REPLICAS)\n    ds.prefetch(AUTO)\n    return ds\n\n\ndef build_model():\n    \"\"\"Prepare the model for fine-tuning.\"\"\"\n    encoder = TFAutoModel.from_pretrained(MODEL_NAME)\n    input_word_ids = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n    \n    # Passing input to pretrained model.\n    embeddings = encoder(input_word_ids)[0]\n    x = embeddings[:, 0, :]\n    \n    output = tf.keras.layers.Dense(3, activation=\"softmax\")(x)\n    \n    model = tf.keras.models.Model(inputs=input_word_ids, outputs=output)\n    \n    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n                  optimizer=tf.keras.optimizers.Adam(lr=1e-5),\n                  metrics=[\"accuracy\"])\n    return model\n\ndef ratio_languages(df):\n    \"\"\"Prints out the ratio of all the languages in the dataset\"\"\"\n    languages = np.unique(df.language)\n    total = df.language.value_counts().sum() \n    ratios = {}\n    for e in languages:\n        ratios[e] = round((df.language.value_counts().loc[e] \/ total), 2)*100\n    \n    ratios = sorted(ratios.items(), key=lambda x: (x[1],x[0]), reverse=True)\n    \n    languages = []\n    values = []\n    for e in ratios:\n        languages.append(e[0])\n        values.append(e[1])\n    _, texts, _ = plt.pie(values, explode=[0.2]*(len(values)), labels=languages, autopct=\"%.2i%%\", radius=2, \n                             rotatelabels=True)\n    for e in texts:\n        e.set_fontsize(15)\n        e.set_fontfamily('fantasy')\n    plt.show()\n\ndef get_lr_callback(batch_size):\n    lr_start = 0.000001\n    lr_max   = 0.00000125 * batch_size\n    lr_min   = 0.00000001\n    lr_sus_epoch = 0\n    lr_decay = 0.80\n    lr_ramp_ep = 5\n    lr = lr_start\n    \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max- lr_start)\/lr_ramp_ep * epoch + lr_start\n        elif epoch < (lr_ramp_ep + lr_sus_epoch):\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min)*lr_decay**(epoch - lr_ramp_ep - lr_sus_epoch)+ lr_min\n        return lr\n    \n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n    return lr_callback\n\ndef plot_wordcloud(df, col):\n    \"\"\"Function to plot word cloud for multiple languages\"\"\"\n    words = \" \"\n    font_path = None\n\n    fig, ax = plt.subplots(nrows=2, ncols=2)\n    fig.set_size_inches(12, 12)\n\n    res = []\n    for i in range(2):\n      for j in range(2):\n        res.append([i,j])\n\n    for i,lang in enumerate([\"English\", \n                             \"Hindi\", \n                             \"Urdu\",\n                             \"German\" ,        \n                            ]):\n      \n          for line in df[df.language==lang][col].values:\n                tokens = line.split()\n\n                tokens = [word.lower() for word in tokens]\n                words += \" \".join(tokens)+\" \"\n        \n          fig.add_subplot(ax[res[i][0]][res[i][1]])\n\n          if lang==\"Hindi\":\n            font_path = FONT_DIR + \"Hindi.ttf\"\n\n          if lang==\"French\":\n            font_path =  FONT_DIR + \"French.ttf\"\n\n          if lang==\"Russian\":\n            font_path= FONT_DIR + \"Russian.ttf\"\n\n          if lang==\"Arabic\":\n            font_path = FONT_DIR + \"Arabic.ttf\"\n\n          if lang==\"Chinese\":\n            font_path = FONT_DIR + \"Chinese.otf\"\n\n          if lang==\"Swahili\":\n            font_path = FONT_DIR + \"Swahili.ttf\"\n\n          if lang==\"Urdu\":\n            font_path = FONT_DIR + \"Urdu.ttf\"\n\n          if lang==\"Vietnamese\":\n            font_path = FONT_DIR + \"Vietnamese.ttf\"\n\n          if lang==\"Greek\":\n            font_path = FONT_DIR + \"Greek.ttf\"\n\n          if lang==\"Thai\":\n            font_path = FONT_DIR + \"Thai.ttf\"\n\n          if lang==\"Spanish\":\n            font_path = FONT_DIR + \"Spanish.ttf\"\n\n          if lang==\"German\":\n            font_path = FONT_DIR + \"German.ttf\"\n\n          if lang==\"Turkish\":\n            font_path = FONT_DIR + \"Turkish.ttf\"\n\n          if lang==\"Bulgarian\":\n            font_path = FONT_DIR + \"Bulgarian.ttf\"\n\n          s_words = STOPWORDS\n\n          wordcloud = WordCloud(font_path=font_path, width=800, height=800, \n                                background_color=\"black\",\n                                min_font_size=10,\n                                stopwords=s_words).generate(words)\n\n          ax[res[i][0]][res[i][1]].imshow(wordcloud)\n          ax[res[i][0]][res[i][1]].axis(\"off\")\n          ax[res[i][0]][res[i][1]].set_title(f\"Language: {lang}\",  fontsize=14)  ","45329a61":"# Value counts of samples for each language.\nprint(train_csv[\"language\"].value_counts())\n\n# Printing ratio of different language in the dataset\nprint()\nratio_languages(train_csv)","a248fada":"# Load xnli dataset\nxnli = nlp.load_dataset(path=\"xnli\")\n\n# As this dataset does not contain direct \n# column name (premise, hypothesis) to sentence pair \n# and so we need to extract it out.\nbuff = {}\nbuff[\"premise\"] = []\nbuff[\"hypothesis\"] = []\nbuff[\"label\"] = []\nbuff[\"language\"] = []\n\n# Making a set to map our dataset language abbreviations to \n# their complete names.\nuniq_lang = set()\nfor e in xnli[\"test\"]:\n  for i in e[\"hypothesis\"][\"language\"]:\n    uniq_lang.add(i)\n\n# Creating a dict that maps abv to their complete names. \nlanguage_map = {}\n\n# Taken test_csv just to use lang_abv column and nothing else.\nfor e in uniq_lang:\n  language_map[e] = test_csv.loc[test_csv.lang_abv==e, \"language\"].iloc[0]\n\n# Prepairing the dataset with the required columns.\nfor x in xnli['test']:\n    label = x['label']\n    for idx, lang in enumerate(x['hypothesis']['language']):\n        \n        # Skipping english samples as we don't want to upsample the samples\n        # corresponding to english language.\n        if lang==\"en\":\n            continue\n            \n        hypothesis = x['hypothesis']['translation'][idx]\n        premise = x['premise'][lang]\n        buff['premise'].append(premise)\n        buff['hypothesis'].append(hypothesis)\n        buff['label'].append(label)\n        buff['language'].append(language_map[lang])\n\n# A pandas DataFrame for the prepared dataset.\nxnli_df = pd.DataFrame(buff)","2680923c":"xnli_df[\"language\"].value_counts()","2751d293":"# Extract columns which required from the dataset.\n# Note: The columns in train_df and xnli_df must be same as we would be merging \n# them for upsampling.\ntrain_df = train_csv[[\"premise\", \"hypothesis\", \"label\", \"language\"]]\ntrain_df.head()","fca25f79":"# Concatenate the complete dataset\nnew_df = pd.concat([train_df, xnli_df], axis=0)\nnew_df.sample(5)","40e00b05":"pd.merge(new_df, test_csv, how=\"inner\")","b80df8ad":"new_df = new_df.merge(pd.merge(new_df, test_csv, how=\"inner\"), how=\"left\", indicator=True)\nnew_df = new_df[new_df._merge==\"left_only\"]\nnew_df = new_df.drop([\"id\", \"lang_abv\", \"_merge\"], axis=1)\nnew_df.info()\n# No null instances in the dataset.","e4d6dc48":"pd.merge(new_df, test_csv, how=\"inner\")","4b28e9a3":"ratio_languages(new_df)","275fe317":"new_df.language.value_counts()","e02c4b3f":"# LOAD EXTRA DATA END","83553897":"plot_wordcloud(new_df, \"premise\")","5976431a":"plot_wordcloud(new_df, \"hypothesis\")","3e61065e":"X, y = new_df[[\"premise\", \"hypothesis\"]], new_df.label","fd354383":"X[\"language_label\"] = new_df.language.astype(str) + \"_\" + new_df.label.astype(str)","8954aec6":"print(\"Splitting Data...\")\n\n# Using train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, y, stratify=X.language_label, test_size=0.2, random_state=SEED)\n\ny_train = tf.one_hot(y_train, depth=3)\ny_test  = tf.one_hot(y_test, depth=3)\n\nprint(\"Prepairing Input...\")\ntrain_input = prepare_input_v2(x_train[[\"premise\", \"hypothesis\"]].values.tolist())\nvalid_input = prepare_input_v2(x_test[[\"premise\", \"hypothesis\"]].values.tolist())\n\nprint(\"Preparing Dataset...\")\ntrain_dataset = get_dataset(train_input, y_train, labelled=True, batch_size=BATCH_SIZE, repeat=True, \n                            shuffle=True)\nvalid_dataset   = get_dataset(valid_input, y_test, labelled=True, batch_size=BATCH_SIZE\/\/REPLICAS, repeat=False,\n                            shuffle=False)\n\nprint(\"Downloading and Building Model...\")\nwith strategy.scope():\n    model  = build_model()\n\n# Callbacks\n#reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", mode=\"min\", factor=0.4, patience=3,\n#                                                 verbose=1)\n\nlr_callback = get_lr_callback(BATCH_SIZE)\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\"XLM-R-base.h5\", save_weights_only=True,\n                                                save_best_only=True, save_freq=\"epoch\", monitor=\"val_loss\",\n                                                mode=\"min\")\n\nprint(\"Training...\")\nmodel.fit(train_dataset, \n         steps_per_epoch= x_train.shape[0]\/BATCH_SIZE,\n         validation_data=valid_dataset,\n         epochs=EPOCHS,\n         callbacks=[lr_callback, checkpoint])","a54ca983":"test_input = prepare_input_v2(test_csv[[\"premise\", \"hypothesis\"]].values.tolist())\ntest_dataset = get_dataset(test_input, None, labelled=False, batch_size=BATCH_SIZE, repeat=False, shuffle=False) ","52a518a4":"preds = model.predict(test_dataset)","082c1c3b":"preds = preds.argmax(axis=1)","42a24ab5":"submission = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/sample_submission.csv\")\nsubmission.head()","5685218f":"submission[\"prediction\"] = preds","bf2fd92c":"submission.sample(10)","992f5fbf":"submission.to_csv(\"submission.csv\", header=True, index=False)","b75ee88b":"Now let's check if we accidently added some test samples in our dataset as a result of upsampling.","14393e6a":"Although we have got a good validation accuracy, the model is slightly overfitting and we need to introduce more regularization in it. This regularization could be done by adding BatchNormalization or Dropout layer in the model architecture. This will be the work for later versions. In the meantime if anyone get's success in reducing overfitting for this model, please share with the community.","b616ef4b":"Let's just see what is the number of samples for languages in this dataset.","60349cae":"As we know that the dataset for the competition is multilingual and it is very necessary for the model to get equal number of samples for all languages present in the dataset in order to generalize well i.e the model should not align to only a single language or we would get wrong estimations for the model's performance.\nThe provided dataset contains comparatively higher amount of English samples (Watson's monolingual love) which we would be upsampling in the following notebook using the [XNLI corpus](https:\/\/cims.nyu.edu\/~sbowman\/xnli\/).","219f8647":"### Scope for tweaks:\n\nHowever there could be many things you could try out but here are some of the few ones that I suppose one should try out. If you've already tried these approaches please consider suggesting some new ones in the comments.\n\n1. <b>LR Scheduling:<\/b>  Introducing LRScheduling improves the model performance.\n2. <b>Model architecture:<\/b> Although the model architecture (XLM-R) used in this notebook is itself quite good at performance, you could also try replacing it with one of the architectures given [here](https:\/\/huggingface.co\/transformers\/pretrained_models.html). (As we used TFAutoModel, you only need to change the MODEL_NAME parameter to you model's name.)\n3. <b>One Hot labels<\/b>: To use categorical_crossentropy loss we need to have our labels one hot encoded.\n4. <b>TTA<\/b>: We can use test time augmentations to improve the predictions by our model. (See reference)\n5. <b>StratifiedKFold<\/b>: Using cross validation techniques highly improves the performance of the model. However I am currently unable to do it due to resource limitations.\n6. <b>More training data<\/b>: Adding more data could improve the performance of the model but keep in mind the ratio of languages in the dataset.\n7. <b>Stop Words<\/b>: Stop words for some languages are not available in nltk library and hence we can either create them using word frequency or some better approach. ","2c85273a":"# Plotting Wordcloud","0a2c12f7":"# References:\n\n- https:\/\/www.kaggle.com\/yihdarshieh\/more-nli-datasets-hugging-face-nlp-library\n- https:\/\/www.kaggle.com\/tuckerarrants\/xlm-r-back-translation-tta\n- https:\/\/www.kaggle.com\/anasofiauzsoy\/tutorial-notebook\n- https:\/\/huggingface.co\/transformers\/pretrained_models.html\n- https:\/\/medium.com\/inside-machine-learning\/what-is-a-transformer-d07dd1fbec04\n- https:\/\/ai.facebook.com\/blog\/-xlm-r-state-of-the-art-cross-lingual-understanding-through-self-supervision\/","8269a8e3":"# Preprocessing Data\nNow let us analyze the training dataset to see how many samples are there for each language in the dataset.","89a8e7f5":"There was a problem in creation of word cloud for other languages. That will be the work for further version. ","936e81e2":"Since the intersection of train and test dataset is NULL, we can say that we have separated the samples and now we can train on this dataset.\n\nBefore training let us have a look at the ratio of languages after upsampling. ","f289b9c4":"Hope you enjoyed the Notebook. Please UPVOTE if so. THANKS!","e997e919":"However the gap between validation accuracy and training accuracy is not that much but still the model needs to be more regularized to reduce overfitting.","bbc9e875":"As we can see we accidently added some samples from the test dataset (present in xnli_df) into our training dataset.\nIf we train the model on this dataset, even if get a good accuracy, that would not work as we are providing labels for the test dataset to the model.\n\nSo we need to eradicate these samples from the dataset which we would do in the following cell-","daf07303":"In the following, we would load Cross lingual NLI corpus [XNLI](https:\/\/cims.nyu.edu\/~sbowman\/xnli\/). It contains data in different languages and same labels (entailment, contradiction and neutral) as our dataset.\n\nReference [[1]](https:\/\/www.kaggle.com\/yihdarshieh\/more-nli-datasets-hugging-face-nlp-library#The-Cross-Lingual-NLI-Corpus-(XNLI)).","5fada875":"**Updates:**\n1. Added Wordcloud plot for multiple languages \n2. Used one hot encoded labels","f752253f":"Que. Why not BERT but ROBERTa and sucessors?\n\nAns. Basically BERT is a great success for multilingual embeddings and it works perfectly well for datasets with a) large amount of samples b) multilingual embeddings but when we have either small amount of data or [crosslingual embeddings](https:\/\/www.reddit.com\/r\/LanguageTechnology\/comments\/f3epzu\/what_is_the_difference_between_multilingual_and\/), ROBERTa and its successors perform pretty much well as they have comparatively large model architecture and trained on crosslingual embeddings which takes into consideration all the languages regarding a particular word vector.","590758a3":"Since dataset has multiple languages and we want model to be trained one each language equally. Hence we need to divide the train and validation samples such that both get equal ratio of all languages. But equal splitting of labels is also necessary for good learning of model. Both of these can be achieved by making an additional column which includes info for both labels and languages and we split our dataset on this column.   ","9bb9035b":"- Let's plot wordclouds for some languages in our **new_df** dataframe.","bf97ce58":"# Watson's monolingual love","fbccdc22":"As can be seen that the samples with English language are quite higher (56%) and samples for other languages are less than 5%. Hence we need to upsample data with respect to other languages to reduce this imbalance in the data.\n\nThis can be done in two ways I suppose:\n\n1. Create Synthetic samples for low resource languages from the original data.\n2. Import external data for low resource languages.\n\nBoth the approaches work good in their place. In this notebook we would use the 2nd approach i.e loading external data.","46efed8f":"# Making Submission","dc26c4c1":"# Training the Model"}}