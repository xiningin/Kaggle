{"cell_type":{"3ffeb9b1":"code","708a74d1":"code","e593a1da":"code","248c7b5f":"code","1431fcaa":"code","930c71fa":"code","3ce7c625":"code","71ae7386":"code","15a67a5f":"code","e3adfb71":"code","9fe911b5":"code","2744be67":"code","241f6797":"code","d21724a9":"code","4b6a3a52":"code","41695bd7":"code","905c8bf2":"code","6b5fc0bd":"code","9179954e":"code","d96a1baf":"code","15e0ed7d":"code","dd5fed2d":"markdown","f5aed067":"markdown","c6862f47":"markdown","dec67dac":"markdown","2da8fe59":"markdown","9b9804d8":"markdown","371c6733":"markdown","e50c5a3e":"markdown","8181a9c9":"markdown","29393aac":"markdown","c6be26cb":"markdown","7bec6cf8":"markdown","4a9bf5a9":"markdown","b6ca2c87":"markdown","11cb683f":"markdown","0debf5fe":"markdown","a413d669":"markdown"},"source":{"3ffeb9b1":"import numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport os\nimport cv2\nfrom tqdm import tqdm\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","708a74d1":"IMAGE_HEIGHT = 256\nIMAGE_WIDTH = 256\nBATCH_SIZE = 16\nNUM_CLASSES = 3\nIMG_PATH = '.\/images\/'\nMASK_PATH = '.\/masks\/'\nLABEL_PATH = '..\/input\/bacteria-detection-with-darkfield-microscopy\/masks\/'\nIMG_SUB_PATH = '.\/images\/images\/'\nMASK_SUB_PATH = '.\/masks\/masks\/'","e593a1da":"os.system('mkdir .\/images\/')\nos.system('cp -r ..\/input\/bacteria-detection-with-darkfield-microscopy\/images\/ .\/images\/')\nos.system('mkdir .\/masks\/')\nos.system('cp -r ..\/input\/bacteria-detection-with-darkfield-microscopy\/masks\/ .\/masks\/')","248c7b5f":"mask_files = os.listdir(MASK_SUB_PATH)\nfor mf in tqdm (mask_files):\n    mask_img = cv2.imread(os.path.join(MASK_SUB_PATH, mf), cv2.IMREAD_GRAYSCALE)\n    mask_img = np.around(tf.keras.utils.to_categorical(mask_img, NUM_CLASSES))\n    cv2.imwrite(os.path.join(MASK_SUB_PATH, mf), mask_img)","1431fcaa":"def show_img(img, title=''):\n    # given a numpy array, plot it\n    vis = plt.imshow(img)\n    plt.title(title)\n    plt.show()\n    \ndef show_imgs(imgs, titles=None):\n    # show two images side by side, useful for segmentation projects\n    fig = plt.figure(figsize=(15, 15))\n\n    for i in range(len(imgs)):\n        plt.subplot(1, len(imgs), i+1)\n        if titles:\n            plt.title(titles[i])\n        plt.imshow(imgs[i])\n    \n    plt.show()\n\ndef label_to_image(m):\n    # given a mask, turn it into an image, use for binary segmentations\n    return tf.keras.preprocessing.image.array_to_img(m.reshape((m.shape[0], m.shape[1], 1)))\n\ndef output_to_image(o):\n    # given model output o that is one hot encoded, turn it into an image, use for multi class segmentations\n    mask_im = tf.argmax(o, axis=-1)\n    mask_im = tf.keras.preprocessing.image.array_to_img(mask_im[..., tf.newaxis])\n    \n    return mask_im","930c71fa":"def visualise_source(title): \n    # given an image title, visualise the source data\n    test_img = cv2.imread(os.path.join(IMG_SUB_PATH, title + '.png'))\n    mask_img = cv2.imread(os.path.join(LABEL_PATH, title + '.png'), cv2.IMREAD_GRAYSCALE)\n    show_imgs([test_img, mask_img])","3ce7c625":"visualise_source('003')","71ae7386":"# # detect and init the TPU\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n# tf.config.experimental_connect_to_cluster(tpu)\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n\n# # instantiate a distribution strategy\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))","15a67a5f":"# https:\/\/github.com\/keras-team\/keras\/issues\/3059#issuecomment-364787723\ntraining_generation_args = dict(\n#     width_shift_range=0.3,\n#     height_shift_range=0.3,\n    horizontal_flip=True,\n    vertical_flip=True,\n    zoom_range=0.2,\n    validation_split=0.1\n)\ntrain_image_datagen = ImageDataGenerator(**training_generation_args)\ntrain_label_datagen = ImageDataGenerator(**training_generation_args)\n\n# data load\ntraining_image_generator = train_image_datagen.flow_from_directory(\n    IMG_PATH,\n    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n    class_mode=None,\n    subset='training',\n    batch_size=BATCH_SIZE,\n    seed=1\n)\ntraining_label_generator = train_label_datagen.flow_from_directory(\n    MASK_PATH,\n    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n    class_mode=None,\n    subset='training',\n    batch_size=BATCH_SIZE,\n    # color_mode='grayscale',\n    seed=1\n)\n\n\n# validation data load\nvalidation_image_generator = train_image_datagen.flow_from_directory(\n    IMG_PATH,\n    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n    class_mode=None,\n    subset='validation',\n    batch_size=BATCH_SIZE,\n    seed=1\n)\nvalidation_label_generator = train_label_datagen.flow_from_directory(\n    MASK_PATH,\n    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n    class_mode=None,\n    subset='validation',\n    batch_size=BATCH_SIZE,\n    # color_mode='grayscale',\n    seed=1\n)\n\ntrain_generator = zip(training_image_generator, training_label_generator)\nvalidation_generator = zip(validation_image_generator, validation_label_generator)","e3adfb71":"test_imgs, labels = train_generator.__next__()\nshow_imgs([test_imgs[0] \/ 255., labels[0]])","9fe911b5":"loss_weights = {\n    0: 0,\n    1: 0,\n    2:0\n}\nmask_files = os.listdir(MASK_SUB_PATH)\nfor mf in tqdm(mask_files):\n    mask_img = cv2.imread(os.path.join(MASK_SUB_PATH, mf))\n    classes = tf.argmax(mask_img, axis=-1).numpy()\n    class_counts = np.unique(classes, return_counts=True)\n    \n    for c in range(len(class_counts[0])):\n        loss_weights[class_counts[0][c]] += class_counts[1][c]\n\nprint(loss_weights)","2744be67":"total = sum(loss_weights.values())\nfor cl, v in loss_weights.items():\n    # do inverse\n    loss_weights[cl] = total \/ v\n    \nloss_weights","241f6797":"\nw = [[loss_weights[0], loss_weights[1], loss_weights[2]]] * IMAGE_WIDTH\nh = [w] * IMAGE_HEIGHT\nloss_mod = np.array(h)","d21724a9":"inputs = tf.keras.layers.Input(shape=[IMAGE_HEIGHT, IMAGE_WIDTH, 3])\nx = inputs\n# downstack\nx = tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu')(x)\nconv1 = tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu')(x)\nx = tf.keras.layers.MaxPooling2D((2, 2))(conv1)\nx = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)\nconv2 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)\nx = tf.keras.layers.MaxPooling2D((2, 2))(conv2)\nx = tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)\nconv3 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)\nx = tf.keras.layers.MaxPooling2D((2, 2))(conv3)\nx = tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)\nconv4 = tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)\nx = tf.keras.layers.MaxPooling2D((2, 2))(conv4)\n\nx = tf.keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu')(x)\nx = tf.keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu')(x)\n\n# upstack\nx = tf.keras.layers.UpSampling2D((2, 2))(x)\nx = tf.keras.layers.Concatenate()([conv4, x])\nx = tf.keras.layers.Conv2DTranspose(256, (3, 3), padding='same', activation='relu')(x)\nx = tf.keras.layers.Conv2DTranspose(256, (3, 3), padding='same', activation='relu')(x)\nx = tf.keras.layers.UpSampling2D((2, 2))(x)\nx = tf.keras.layers.Concatenate()([conv3, x])\nx = tf.keras.layers.Conv2DTranspose(128, (3, 3), padding='same', activation='relu')(x)\nx = tf.keras.layers.Conv2DTranspose(128, (3, 3), padding='same', activation='relu')(x)\nx = tf.keras.layers.UpSampling2D((2, 2))(x)\nx = tf.keras.layers.Concatenate()([conv2, x])\nx = tf.keras.layers.Conv2DTranspose(64, (3, 3), padding='same', activation='relu')(x)\nx = tf.keras.layers.Conv2DTranspose(64, (3, 3), padding='same', activation='relu')(x)\nx = tf.keras.layers.UpSampling2D((2, 2))(x)\nx = tf.keras.layers.Concatenate()([conv1, x])\nx = tf.keras.layers.Conv2DTranspose(32, (3, 3), padding='same', activation='relu')(x)\nx = tf.keras.layers.Conv2DTranspose(32, (3, 3), padding='same', activation='relu')(x)\nx = tf.keras.layers.Conv2DTranspose(3, (1, 1), activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs, outputs=x)","4b6a3a52":"model.summary()","41695bd7":"model.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'],\n              loss_weights=loss_mod)","905c8bf2":"tf.keras.utils.plot_model(model, show_shapes=True)","6b5fc0bd":"model_history = model.fit(train_generator,\n                          epochs=20,\n                          steps_per_epoch=100,\n                          validation_data=validation_generator,\n                          validation_steps=9)","9179954e":"test_imgs, labels = validation_generator.__next__()\npredictions = model.predict(test_imgs, use_multiprocessing=False)\n\nfor i in range(min(len(predictions), 5)):\n    show_imgs(\n        [test_imgs[i] \/ 255., labels[i], predictions[i]],\n        ['Source Image', 'True Mask', 'Prediction']\n    )","d96a1baf":"plt.plot(model_history.history['accuracy'])\nplt.plot(model_history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Training set', 'Validation set'], loc='upper left')\nplt.show()\n\n# Loss \n\nplt.plot(model_history.history['val_loss'])\nplt.plot(model_history.history['loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training set', 'Validation set'], loc='upper left')\nplt.show()","15e0ed7d":"os.system('rm -r ' + IMG_PATH)\nos.system('rm -r ' + MASK_PATH)","dd5fed2d":"# Have a look at some of the outputs","f5aed067":"Calculate the weights based on counts","c6862f47":"# Using darkfield microscope images, highlight regions with bacteria\n\nThis kernel uses a lot of learnings from Long Nguyen's [kernel](https:\/\/www.kaggle.com\/longnguyen2306\/unet-plus-plus-for-image-segmentation) (class imbalance, model architecture) to facilitate my learning with these differences.","dec67dac":"For the loss function, use `categorical crossentropy` since the label is one hot encoded. Also pass the loss weights computed before into the model","2da8fe59":"## Load data\nLoad data into Keras generators and also apply some augmentation to it. The image and masks are loaded separately with the same seed and some augmentations are applied","9b9804d8":"## Plot loss and accuracy over time","371c6733":"## Have a look at an image","e50c5a3e":"Have a look at the dataset and make sure there's no issues","8181a9c9":"## Constants","29393aac":"## Class imblanace\nFrom experimentation, and visual inspection classes in each image are highly imbalance (eg: too much background). This resulted in models outputing \"blank\" all background images and consistently yield high accuracy and low loss. Need to calculate `loss_weights` and pass it to the model when compiling to compensate for this\n\n\nNetwork predict blank results: [https:\/\/stackoverflow.com\/questions\/52123670](https:\/\/stackoverflow.com\/questions\/52123670\/neural-network-converges-too-fast-and-predicts-blank-results) \nKeras model loss_weight: [Keras - compile method](https:\/\/keras.io\/api\/models\/model_training_apis\/#compile-method)\n","c6be26cb":"Because this is a multi class segmentation task, each mask needs to be one hot encoded","7bec6cf8":"## Model\nDefine the model. Similar architecture to [Unet](https:\/\/arxiv.org\/abs\/1505.04597) though smaller and more straight forward with no dropout and other regularisation","4a9bf5a9":"## Copy data over to output file system\nKeras data generator expects the images to be in a subfolder","b6ca2c87":"## Helper functions\nUseful functions for any project","11cb683f":"Create a modifier that is the same shape as output","0debf5fe":"## Remove the training data from output","a413d669":"## Use TPU\/GPU"}}