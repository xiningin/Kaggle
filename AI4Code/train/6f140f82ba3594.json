{"cell_type":{"508ea882":"code","891aa2bd":"code","37bb4549":"code","d010d2da":"code","2999b03f":"code","b3d94d93":"code","6302013e":"code","578d3ef0":"code","1c166613":"code","d8150b5a":"code","294d9441":"code","5818c8a3":"code","b5ff810d":"code","f630b4cf":"code","98ec8dbc":"code","8cd2a906":"code","17dd747b":"code","2967bfa1":"code","ccab130e":"code","6d24cf0e":"code","d347da35":"code","6dff7eff":"code","4af64234":"code","ad7e0cda":"code","43852843":"code","f9629b8f":"markdown"},"source":{"508ea882":"!pip install transformers[ja]","891aa2bd":"import transformers\nimport os\nimport gc\nos.makedirs(\".\/models\", exist_ok=True)\nDEVICE = \"cuda\"\nMAX_LEN = 128\nTRAIN_BATCH_SIZE = 8\nVALID_BATCH_SIZE = 64\nEPOCHS = 1\nN_EARLY_STOPPING = 1\nN_SPLITS = 5\nTRAINING_FILE = \"\/kaggle\/input\/rkcup-1\/train.csv\"","37bb4549":"class BERTDataset:\n    def __init__(self, review, target):\n        self.review = review\n        self.target = target\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n\n    def __len__(self):\n        return len(self.review)\n\n    def __getitem__(self, item):\n        review = self.review[item]\n        inputs = self.tokenizer.encode_plus(\n            review,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_attention_mask=True,\n            truncation=True,\n            padding='max_length'\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n\n        return {\n            \"ids\": torch.tensor(ids, dtype=torch.long),\n            \"mask\": torch.tensor(mask, dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n            \"targets\": torch.tensor(self.target[item], dtype=torch.float),\n        }","d010d2da":"import transformers\nimport torch.nn as nn\n\n\nclass BERTBaseUncased(nn.Module):\n    def __init__(self):\n        super(BERTBaseUncased, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n        self.bert_drop = nn.Dropout(0.3)\n        if BERT_PATH == 'cl-tohoku\/bert-large-japanese':\n            self.out = nn.Linear(1024, 1)\n        else:\n            self.out = nn.Linear(768, 1)\n\n    def forward(self, ids, mask, token_type_ids):\n        o1 = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n        o2 = o1['pooler_output']\n        bo = self.bert_drop(o2)\n        output = self.out(bo)\n        return output","2999b03f":"import torch\nimport torch.nn as nn\nfrom tqdm import tqdm\n\n\ndef loss_fn(outputs, targets):\n    return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n\n\ndef train_fn(data_loader, model, optimizer, device, scheduler):\n    model.train()\n\n    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        targets = d[\"targets\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n\n        optimizer.zero_grad()\n        outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n\ndef eval_fn(data_loader, model, device):\n    model.eval()\n    fin_targets = []\n    fin_outputs = []\n    with torch.no_grad():\n        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            targets = d[\"targets\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n\n            outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n    return fin_outputs, fin_targets","b3d94d93":"import torch\nimport pandas as pd\nimport torch.nn as nn\nimport numpy as np\nimport random\n\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\ndef run():\n    seed_everything(111)\n    dfx = pd.read_csv(TRAINING_FILE)\n    dfx[\"sentiment\"] = dfx[\"target\"].copy()\n    \n    kf = StratifiedKFold(n_splits=N_SPLITS, random_state=10, shuffle=True)\n    fold_counter = 0\n    oof = np.zeros(len(dfx))\n    for train_index, valid_index in kf.split(dfx, dfx['target']):\n        print(f'START {fold_counter}')\n        df_train = dfx.loc[train_index].reset_index(drop=True)\n        df_valid = dfx.loc[valid_index].reset_index(drop=True)\n\n        train_dataset = BERTDataset(\n            review=df_train.text.values, target=df_train.target.values\n        )\n\n        train_data_loader = torch.utils.data.DataLoader(\n            train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=2\n        )\n\n        valid_dataset = BERTDataset(\n            review=df_valid.text.values, target=df_valid.target.values\n        )\n\n        valid_data_loader = torch.utils.data.DataLoader(\n            valid_dataset, batch_size=VALID_BATCH_SIZE, num_workers=1\n        )\n\n        device = torch.device(DEVICE)\n        model = BERTBaseUncased()\n        model.to(device)\n\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.001,\n            },\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.0,\n            },\n        ]\n\n        num_train_steps = int(len(df_train) \/ TRAIN_BATCH_SIZE * EPOCHS)\n        optimizer = AdamW(optimizer_parameters, lr=3e-5)\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n        )\n\n        best_score = 0\n        best_epoch = 0\n        early_stoppoing_counter = 0\n        for epoch in range(EPOCHS):\n            train_fn(train_data_loader, model, optimizer, device, scheduler)\n            outputs, targets = eval_fn(valid_data_loader, model, device)\n            auc_score = metrics.roc_auc_score(targets, outputs)\n            print(f'auc {auc_score}')\n            if auc_score > best_score:\n                torch.save(model.state_dict(), model_path(fold_counter))\n                best_score = auc_score\n                best_oof = outputs\n                best_epoch = epoch\n            else:\n                early_stoppoing_counter += 1\n            if early_stoppoing_counter == N_EARLY_STOPPING:\n                print(f'Early stopping on {epoch}th epoch. best auc : {auc_score:.5f} best epoch : {best_epoch}')\n                break\n        oof[valid_index] = [x[0] for x in best_oof]\n        fold_counter += 1\n    oof_auc_score = metrics.roc_auc_score(dfx[\"sentiment\"], oof)\n    print(f'OOF AUC : {oof_auc_score:.5f}')\n    return oof","6302013e":"def sentence_prediction():\n    df_test = pd.read_csv('\/kaggle\/input\/rkcup-1\/test.csv')\n    df_test['target'] = 0\n    outputs_li = []\n    for fold in range(N_SPLITS):\n        model = BERTBaseUncased()\n        model.load_state_dict(torch.load(model_path(fold)))\n        model.to(DEVICE)\n        model.eval()\n        test_dataset = BERTDataset(\n                review=df_test.text.values, target=df_test.target.values)\n        test_data_loader = torch.utils.data.DataLoader(\n                test_dataset, batch_size=VALID_BATCH_SIZE, num_workers=2)\n        outputs, targets = eval_fn(test_data_loader, model, DEVICE)\n        outputs_li.append([x for x in outputs])\n        del model\n        gc.collect()\n    outputs = np.mean(outputs_li, axis=0).reshape(-1)\n    return outputs","578d3ef0":"stage2_oofs = []\nstage2_preds = []","1c166613":"BERT_PATH = 'cl-tohoku\/bert-large-japanese'\ndef model_path(i):\n    return f\".\/models\/bert-large-japanese_{i}.bin\"\nTOKENIZER = transformers.BertJapaneseTokenizer.from_pretrained(BERT_PATH)\noof = run()\noutputs = sentence_prediction()\nstage2_oofs.append(oof)\nstage2_preds.append(outputs)\ngc.collect()","d8150b5a":"BERT_PATH = 'cl-tohoku\/bert-base-japanese-whole-word-masking'\ndef model_path(i):\n    return f\".\/models\/model_bert-base-japanese-whole-word-masking_{i}.bin\"\nTOKENIZER = transformers.BertJapaneseTokenizer.from_pretrained(BERT_PATH)\noof = run()\noutputs = sentence_prediction()\nstage2_oofs.append(oof)\nstage2_preds.append(outputs)\ngc.collect()","294d9441":"\"\"\"\nBERT_PATH = 'cl-tohoku\/bert-base-japanese-char-whole-word-masking'\ndef model_path(i):\n    return f\".\/models\/bert-base-japanese-char-whole-word-masking_{i}.bin\"\nTOKENIZER = transformers.BertJapaneseTokenizer.from_pretrained(BERT_PATH)\noof = run()\noutputs = sentence_prediction()\nstage2_oofs.append(oof)\nstage2_preds.append(outputs)\ngc.collect()\n\"\"\"","5818c8a3":"\"\"\"\nBERT_PATH = 'cl-tohoku\/bert-base-japanese'\ndef model_path(i):\n    return f\".\/models\/bert-base-japanese_{i}.bin\"\nTOKENIZER = transformers.BertJapaneseTokenizer.from_pretrained(BERT_PATH)\noof = run()\noutputs = sentence_prediction()\nstage2_oofs.append(oof)\nstage2_preds.append(outputs)\ngc.collect()\n\"\"\"","b5ff810d":"\"\"\"\nBERT_PATH = 'cl-tohoku\/bert-base-japanese-v2'\ndef model_path(i):\n    return f\".\/models\/bert-base-japanese-v2_{i}.bin\"\nTOKENIZER = transformers.BertJapaneseTokenizer.from_pretrained(BERT_PATH)\noof = run()\noutputs = sentence_prediction()\nstage2_oofs.append(oof)\nstage2_preds.append(outputs)\ngc.collect()\n\"\"\"","f630b4cf":"\"\"\"\nBERT_PATH = 'cl-tohoku\/bert-base-japanese-char-v2'\ndef model_path(i):\n    return f\".\/models\/bert-base-japanese-char-v2_{i}.bin\"\nTOKENIZER = transformers.BertJapaneseTokenizer.from_pretrained(BERT_PATH)\noof = run()\noutputs = sentence_prediction()\nstage2_oofs.append(oof)\nstage2_preds.append(outputs)\ngc.collect()\n\"\"\"","98ec8dbc":"\"\"\"\nBERT_PATH = 'cl-tohoku\/bert-base-japanese-char'\ndef model_path(i):\n    return f\".\/models\/bert-base-japanese-char_{i}.bin\"\nTOKENIZER = transformers.BertJapaneseTokenizer.from_pretrained(BERT_PATH)\noof = run()\noutputs = sentence_prediction()\nstage2_oofs.append(oof)\nstage2_preds.append(outputs)\ngc.collect()\n\"\"\"","8cd2a906":"from sklearn.model_selection import KFold\nfrom scipy.optimize import minimize\n\ndef get_score(weights, train_idx, oofs, labels):\n    blend = np.zeros_like(oofs[0][train_idx])\n\n    for oof, weight in zip(oofs[:-1], weights):\n        blend += weight * oof[train_idx]\n\n    blend += (1 - np.sum(weights)) * oofs[-1][train_idx]\n    return -metrics.roc_auc_score(labels[train_idx], blend)\n\ndef get_best_weights(oofs, labels):\n    weight_list = []\n    weights = np.array([1 \/ len(oofs) for x in range(len(oofs) - 1)])\n\n    for n_splits in tqdm([5, 6, 7]):\n        for i in range(3):\n            kf = KFold(n_splits=n_splits, random_state=i, shuffle=True)\n            for fold, (train_idx, valid_idx) in enumerate(kf.split(X=oofs[0])):\n                res = minimize(get_score, weights, args=(train_idx, oofs, labels), method=\"Nelder-Mead\", tol=1e-6)\n                print(f\"i: {i} fold: {fold} res.x: {res.x}\")\n                weight_list.append(res.x)\n\n    mean_weight = np.mean(weight_list, axis=0)\n    print(f\"optimized weight: {mean_weight}\")\n    return mean_weight","17dd747b":"train_df = pd.read_csv(TRAINING_FILE)\ngc.collect()","2967bfa1":"best_weights = get_best_weights(stage2_oofs, train_df[\"target\"].values)\ngc.collect()\nbest_weights = np.insert(best_weights, len(best_weights), 1 - np.sum(best_weights))\nprint(\"post processed optimized weight\", best_weights)","ccab130e":"oof_preds = np.stack(stage2_oofs).transpose(1, 0).dot(best_weights)\nblend_preds = np.stack(stage2_preds).transpose(1, 0).dot(best_weights)\nprint(\"final oof score\", metrics.roc_auc_score(train_df[\"target\"].values, oof_preds))\ndel train_df\ngc.collect()","6d24cf0e":"stage2_preds[0].shape","d347da35":"blend_preds.shape","6dff7eff":"df_test = pd.read_csv('\/kaggle\/input\/rkcup-1\/test.csv')\ndf_test['pred'] = blend_preds","4af64234":"df_test.head()","ad7e0cda":"sub = pd.read_csv('\/kaggle\/input\/rkcup-1\/sample_submission.csv')\nsub['target'] = blend_preds\nsub.to_csv('submission.csv', index=False)","43852843":"sub.head()","f9629b8f":"## BERT\u3067finetune\u3057\u3066\u307f\u308b\n* \u5b66\u7fd2\u90e8\u5206\u306e\u5927\u90e8\u5206\u3092\u5f15\u7528\u3057\u305f\u30ea\u30dd\u30b8\u30c8\u30ea\n  * https:\/\/github.com\/abhishekkrthakur\/bert-sentiment"}}