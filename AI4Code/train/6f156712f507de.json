{"cell_type":{"5d95df04":"code","dc2bc1bc":"code","1ee25b20":"code","795152d4":"code","81f68668":"code","dd9becd4":"code","a636986b":"code","d453d93e":"code","4c11fc65":"code","9edc5ee9":"code","650d1e24":"code","19344f71":"code","e2eda29d":"code","66f5bb47":"code","c1f2f2a5":"code","a6fe3616":"code","e19985ea":"code","ba765367":"code","bdbe81dd":"code","31c2d7ff":"code","36fb9773":"code","d1d738e8":"code","af296b64":"code","1a076ccf":"code","8846c10c":"code","05dc47f6":"code","020eb1c5":"code","0f018106":"code","c80b88eb":"code","73bd2139":"code","38b8f547":"code","69b2b549":"code","21511725":"code","5a25c7c1":"code","adfa5e54":"code","69d568b2":"code","33024108":"code","5e4f87c1":"code","941f0e9d":"code","fcd50144":"code","e3a605d7":"code","a5c7e324":"code","9a35dfc9":"code","07b67097":"code","24e84f3a":"code","e080b1c1":"code","ce0482d7":"code","dee9c6ca":"code","4fe17caf":"code","e1222814":"code","0f7b7555":"code","4da46aa8":"code","d655173b":"code","0f514118":"code","3450a930":"code","6dd9916d":"code","8c2d5183":"code","a55b6e32":"code","b7cc8456":"code","29a7790e":"code","a81cda82":"markdown","e078c427":"markdown","d04b6715":"markdown","2ac88392":"markdown","311cbc31":"markdown","81eabf13":"markdown","e30bb45b":"markdown","e585550d":"markdown","405d7ea1":"markdown","49998b4a":"markdown","6911f1e9":"markdown","7b33ea18":"markdown","0dad3193":"markdown","552b9cfe":"markdown"},"source":{"5d95df04":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport pandas_profiling\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","dc2bc1bc":"df_train = pd.read_csv(\"..\/input\/Train.csv\")\ndf_train.head()","1ee25b20":"df_train.profile_report(style={'full_width':True},title='Training dataset Profiling Report')","795152d4":"#Checking the columns in Training dataset......\nprint(\"Columns in training dataset based on datatypes {}\".format(df_train.columns.to_series().groupby(df_train.dtypes).groups))","81f68668":"#Checking the dimensions\ndf_train.shape","dd9becd4":"#Checking missing values..............\ndf_train.isnull().sum()","a636986b":"df_train['Item_Weight']=df_train['Item_Weight'].fillna(df_train['Item_Weight'].mean())","d453d93e":"df_train['Outlet_Size']=df_train['Outlet_Size'].fillna(df_train['Outlet_Size'].mode()[0])","4c11fc65":"#Lets check whether we still have missing values in our dataset!!\nimport seaborn as sns\nsns.heatmap(df_train.isnull(),yticklabels=False,cbar=False,cmap='coolwarm')","9edc5ee9":"df_train.describe(include = 'all')","650d1e24":"df_train.info()","19344f71":"#Visualizing the \"Outlet_Identifier\"\ndf_train['Outlet_Identifier'].value_counts().plot(kind='bar',color = 'Black')","e2eda29d":"df_train = df_train.drop(['Item_Identifier','Outlet_Identifier'],axis=1)\ndf_train.head()","66f5bb47":"#Visualizing the \"Item_Fat_Content\"\ndf_train['Item_Fat_Content'].value_counts().plot(kind='bar',color = 'black')","c1f2f2a5":"df_train =  df_train.replace(to_replace =\"low fat\",  value =\"Low Fat\") \ndf_train =  df_train.replace(to_replace =\"LF\",  value =\"Low Fat\") \ndf_train =  df_train.replace(to_replace =\"reg\",  value =\"Regular\") ","a6fe3616":"#Visualizing the \"Item_Fat_Content\"\ndf_train['Item_Fat_Content'].value_counts().plot(kind='bar',color = 'Green')","e19985ea":"#Visualizing the \"Item_Type\"\ndf_train['Item_Type'].value_counts().plot(kind='bar',color = 'Green')","ba765367":"#Visualizing the \"Outlet_Size\"\ndf_train['Outlet_Size'].value_counts().plot(kind='bar',color = 'green')","bdbe81dd":"#Visualizing the \"Outlet_Location_Type\"\ndf_train['Outlet_Location_Type'].value_counts().plot(kind='bar',color = 'Green')","31c2d7ff":"#Visualizing the \"Outlet_Type\"\ndf_train['Outlet_Type'].value_counts().plot(kind='bar',color = 'green')","36fb9773":"y = df_train['Item_Weight']\nplt.figure(1); \nsns.distplot(y, kde=True,color = 'red')","d1d738e8":"y = df_train['Item_Visibility']\nplt.figure(1); \nsns.distplot(y, kde=True,color = 'red')","af296b64":"y = df_train['Item_MRP']\nplt.figure(1);\nsns.distplot(y, kde=True,color = 'red')","1a076ccf":"y = df_train['Outlet_Establishment_Year']\nplt.figure(1); \nsns.distplot(y, kde=True,color = 'red')","8846c10c":"y = df_train['Item_Outlet_Sales']\nplt.figure(1);\nsns.distplot(y, kde=True,color = 'red')","05dc47f6":"df_train[\"Qty_Sold\"] = (df_train[\"Item_Outlet_Sales\"]\/df_train[\"Item_MRP\"])\ndf_train.head()","020eb1c5":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,7))\nsns.heatmap(df_train.corr(),annot=True,cmap=\"YlGnBu\")","0f018106":"categorical_columns=[x for x in df_train.dtypes.index if df_train.dtypes[x]=='object']\ncategorical_columns","c80b88eb":"df_train.pivot_table(index='Outlet_Type',values='Item_Outlet_Sales')","73bd2139":"#print frequencies of these categories\nfor col in categorical_columns:\n    print('Frequency of categories for variable')\n    print(df_train[col].value_counts())\n    print(\"\\n\")","38b8f547":"#Encoding Categorical Variables\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder=LabelEncoder()\nfor col in df_train.columns:\n    df_train[col] = labelencoder.fit_transform(df_train[col])\n","69b2b549":"\n\n#Now one hot encoding\ndf_train=pd.get_dummies(df_train, columns=['Item_Fat_Content',\n 'Item_Type',\n 'Outlet_Size',\n 'Outlet_Location_Type',\n 'Outlet_Type'],drop_first=False)\n\nprint(df_train.shape)","21511725":"df_train.columns","5a25c7c1":"#Rearrangement of the columns......\n\ndf = df_train[['Item_Weight', 'Item_Visibility', 'Item_MRP',\n       'Outlet_Establishment_Year',\n       'Item_Fat_Content_0', 'Item_Fat_Content_1', 'Item_Type_0',\n       'Item_Type_1', 'Item_Type_2', 'Item_Type_3', 'Item_Type_4',\n       'Item_Type_5', 'Item_Type_6', 'Item_Type_7', 'Item_Type_8',\n       'Item_Type_9', 'Item_Type_10', 'Item_Type_11', 'Item_Type_12',\n       'Item_Type_13', 'Item_Type_14', 'Item_Type_15', 'Outlet_Size_0',\n       'Outlet_Size_1', 'Outlet_Size_2', 'Outlet_Location_Type_0',\n       'Outlet_Location_Type_1', 'Outlet_Location_Type_2', 'Outlet_Type_0',\n       'Outlet_Type_1', 'Outlet_Type_2', 'Outlet_Type_3', 'Item_Outlet_Sales', 'Qty_Sold']]\ndf.head()","adfa5e54":"df.shape","69d568b2":"# iterating the columns \nfor col in df.columns: \n    print(col)","33024108":"#Separating features and label\nX = df.iloc[:,0:33].values\ny = df.iloc[:,-1].values","5e4f87c1":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=4)","941f0e9d":"# Applying PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = None)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\nexplained_variance = pca.explained_variance_ratio_\n\nprint(explained_variance)","fcd50144":"len(explained_variance)","e3a605d7":"print(\"Sorted List returned :\")\nprint(sorted(explained_variance,reverse = True))","a5c7e324":"with plt.style.context('dark_background'):\n    plt.figure(figsize=(16, 8))\n    \n    plt.bar(range(33), explained_variance, alpha=0.5, align='center',label='individual explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()\n    ","9a35dfc9":"# Applying PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 3)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\nexplained_variance = pca.explained_variance_ratio_\nprint(explained_variance)","07b67097":"#Model comparison\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.ensemble import AdaBoostRegressor,BaggingRegressor,ExtraTreesRegressor,GradientBoostingRegressor","24e84f3a":"\n#Fit Decision_tree\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)","e080b1c1":"#Fit Decision_tree\ntree = DecisionTreeRegressor()\ntree.fit(X_train, y_train)","ce0482d7":"#Fit Random_forest\nforest = RandomForestRegressor(n_jobs=-1)\nforest.fit(X_train, y_train)","dee9c6ca":"#Fit Ada_Boost_Regressor..........\nAda_boost = AdaBoostRegressor()\nAda_boost.fit(X_train, y_train)","4fe17caf":"#Fit Bagging_Regressor..........\nBagging = BaggingRegressor()\nBagging.fit(X_train, y_train)","e1222814":"#Fit Extra_tree_regressor........\nExtra_trees = ExtraTreesRegressor()\nExtra_trees.fit(X_train, y_train)","0f7b7555":"#Fit Gradient_Boosting_Regressor........\nGradient_boosting = GradientBoostingRegressor()\nGradient_boosting.fit(X_train, y_train)","4da46aa8":"models= [('lin_reg', lin_reg), ('forest', forest), ('dt', tree),('Ada_boost',Ada_boost),('Bagging',Bagging),('Extra_trees',Extra_trees),('Gradient_boosting',Gradient_boosting)]\nscoring = ['neg_mean_squared_error', 'neg_mean_absolute_error', 'r2']\n\n\n#for each model I want to test three different scoring metrics. Therefore, results[0] will be lin_reg x MSE, \n# results[1] lin_reg x MSE and so on until results [8], where we stored dt x r2\n\nresults= []\nmetric= []\nfor name, model in models:\n    for i in scoring:\n        scores = cross_validate(model, X_train, y_train, scoring=i, cv=10, return_train_score=True)\n        results.append(scores)\n\nprint(results[20])","d655173b":"###############################################################################\n\n#if you change signa and square the Mean Square Error you get the RMSE, which is the most common metric to accuracy\nLR_RMSE_mean = np.sqrt(-results[0]['test_score'].mean())\nLR_RMSE_std= results[0]['test_score'].std()\n# note that also here I changed the sign, as the result is originally a negative number for ease of computation\nLR_MAE_mean = -results[1]['test_score'].mean()\nLR_MAE_std= results[1]['test_score'].std()\nLR_r2_mean = results[2]['test_score'].mean()\nLR_r2_std = results[2]['test_score'].std()\n\n#THIS IS FOR RF\nRF_RMSE_mean = np.sqrt(-results[3]['test_score'].mean())\nRF_RMSE_std= results[3]['test_score'].std()\nRF_MAE_mean = -results[4]['test_score'].mean()\nRF_MAE_std= results[4]['test_score'].std()\nRF_r2_mean = results[5]['test_score'].mean()\nRF_r2_std = results[5]['test_score'].std()\n\n#THIS IS FOR DT\nDT_RMSE_mean = np.sqrt(-results[6]['test_score'].mean())\nDT_RMSE_std= results[6]['test_score'].std()\nDT_MAE_mean = -results[7]['test_score'].mean()\nDT_MAE_std= results[7]['test_score'].std()\nDT_r2_mean = results[8]['test_score'].mean()\nDT_r2_std = results[8]['test_score'].std()\n\n\n\n\n#if you change signa and square the Mean Square Error you get the RMSE, which is the most common metric to accuracy\nADA_RMSE_mean = np.sqrt(-results[9]['test_score'].mean())\nADA_RMSE_std= results[9]['test_score'].std()\n# note that also here I changed the sign, as the result is originally a negative number for ease of computation\nADA_MAE_mean = -results[10]['test_score'].mean()\nADA_MAE_std= results[10]['test_score'].std()\nADA_r2_mean = results[11]['test_score'].mean()\nADA_r2_std = results[11]['test_score'].std()\n\n\n\n#if you change signa and square the Mean Square Error you get the RMSE, which is the most common metric to accuracy\nBAGGING_RMSE_mean = np.sqrt(-results[12]['test_score'].mean())\nBAGGING_RMSE_std= results[12]['test_score'].std()\n# note that also here I changed the sign, as the result is originally a negative number for ease of computation\nBAGGING_MAE_mean = -results[13]['test_score'].mean()\nBAGGING_MAE_std= results[13]['test_score'].std()\nBAGGING_r2_mean = results[14]['test_score'].mean()\nBAGGING_r2_std = results[14]['test_score'].std()\n\n\n#if you change signa and square the Mean Square Error you get the RMSE, which is the most common metric to accuracy\nET_RMSE_mean = np.sqrt(-results[15]['test_score'].mean())\nET_RMSE_std= results[15]['test_score'].std()\n# note that also here I changed the sign, as the result is originally a negative number for ease of computation\nET_MAE_mean = -results[16]['test_score'].mean()\nET_MAE_std= results[16]['test_score'].std()\nET_r2_mean = results[17]['test_score'].mean()\nET_r2_std = results[17]['test_score'].std()\n\n\n#if you change signa and square the Mean Square Error you get the RMSE, which is the most common metric to accuracy\nGB_RMSE_mean = np.sqrt(-results[18]['test_score'].mean())\nGB_RMSE_std= results[18]['test_score'].std()\n# note that also here I changed the sign, as the result is originally a negative number for ease of computation\nGB_MAE_mean = -results[19]['test_score'].mean()\nGB_MAE_std= results[19]['test_score'].std()\nGB_r2_mean = results[20]['test_score'].mean()\nGB_r2_std = results[20]['test_score'].std()","0f514118":"modelDF = pd.DataFrame({\n    'Model'       : ['Linear Regression', 'Random Forest', 'Decision Trees','Ada Boosting','Bagging','Extra trees','Gradient Boosting'],\n    'RMSE_mean'    : [LR_RMSE_mean, RF_RMSE_mean, DT_RMSE_mean,ADA_RMSE_mean,BAGGING_RMSE_mean,ET_RMSE_mean,GB_RMSE_mean],\n    'RMSE_std'    : [LR_RMSE_std, RF_RMSE_std, DT_RMSE_std,ADA_RMSE_std,BAGGING_RMSE_std,ET_RMSE_std,GB_RMSE_std],\n    'MAE_mean'   : [LR_MAE_mean, RF_MAE_mean, DT_MAE_mean,ADA_MAE_mean,BAGGING_MAE_mean,ET_MAE_mean,GB_MAE_mean],\n    'MAE_std'   : [LR_MAE_std, RF_MAE_std, DT_MAE_std, ADA_MAE_std, BAGGING_MAE_std, ET_MAE_std, GB_MAE_std],\n    'r2_mean'      : [LR_r2_mean, RF_r2_mean, DT_r2_mean, ADA_r2_mean,BAGGING_r2_mean, ET_r2_mean, GB_r2_mean],\n    'r2_std'      : [LR_r2_std, RF_r2_std, DT_r2_std, ADA_r2_std,BAGGING_r2_std, ET_r2_std, GB_r2_std],\n    }, columns = ['Model', 'RMSE_mean', 'RMSE_std', 'MAE_mean', 'MAE_std', 'r2_mean', 'r2_std'])\n\n    \nmodelDF.sort_values(by='r2_mean', ascending=False)","3450a930":"import seaborn as sns\n\nsns.factorplot(x= 'Model', y= 'RMSE_mean', data= modelDF, kind='bar',size=6, aspect=4)","6dd9916d":"from sklearn.model_selection import GridSearchCV,StratifiedKFold\n\nETC = ExtraTreesRegressor()\ngb_param_grid = {'n_estimators' : [100,200,300,400,500],\n              'max_depth': [4, 8,12,16],\n              'min_samples_leaf' : [100,150,200,250],\n              'max_features' : [0.3, 0.1] \n              }\n\ngsETC = GridSearchCV(ETC,param_grid = gb_param_grid, cv=10, n_jobs= -1, verbose = 0)\n\ngsETC.fit(X_train,y_train)\n\n","8c2d5183":"ETC_best = gsETC.best_estimator_","a55b6e32":"# Best score\ngsETC.best_score_,gsETC.best_params_","b7cc8456":"# =============================================================================\n# Model creation\n# =============================================================================\n\n\nETC = ExtraTreesRegressor(max_depth= 8,max_features = 0.3,min_samples_leaf =  100,n_estimators= 500)\nETC.fit(X_train, y_train)\n\n\n#predicting the test set\ny_pred = ETC.predict(X_test)\n","29a7790e":"from sklearn import metrics\nprint(\"MAE:\", metrics.mean_absolute_error(y_test, y_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","a81cda82":"# Lets check the individual Object Variables......","e078c427":"Looks like there is **no more missing** value exist!","d04b6715":"# Guys i have just tried to show the trick of working on it and as of now i m only considering the model not accuracy.....and if want to edit anything just click fork(top right button).....\n\n# Kindly vote up if u like this.....\n\n\n![](http:\/\/cdn.lowgif.com\/full\/92ab94a9bcf19559-animated-emoji-on-behance.gif)","2ac88392":"# Lets check the individual Int\/float Variables......","311cbc31":"**3.Outlet_Size**","81eabf13":"# ||Problem Statement||\n\n\nThe data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined.\n**The aim is to build a predictive model and find out the sales of each product at a particular store.**\n\n\n\n**Using this model, BigMart will try to understand the properties of products and stores which play a key role in increasing sales.**","e30bb45b":"**1.Item_Fat_Content**","e585550d":"**5.Outlet_Type**","405d7ea1":"# Data\n\nWe have train (8523) and test (5681) data set, train data set has both input and output variable(s). You need to predict the sales for test data set.\n\n\n\n**Variable - Description**\n\nItem_Identifier- Unique product ID\n\nItem_Weight- Weight of product\n\nItem_Fat_Content - Whether the product is low fat or not\n\nItem_Visibility - The % of total display area of all products in a store allocated to the particular product\n\nItem_Type - The category to which the product belongs\n\nItem_MRP - Maximum Retail Price (list price) of the product\n\nOutlet_Identifier - Unique store ID\n\nOutlet_Establishment_Year- The year in which store was established\n\nOutlet_Size - The size of the store in terms of ground area covered\n\nOutlet_Location_Type- The type of city in which the store is located\n\nOutlet_Type- Whether the outlet is just a grocery store or some sort of supermarket\n\nItem_Outlet_Sales - Sales of the product in the particulat store. This is the outcome variable to be predicted.","49998b4a":"Does it make sense? No right?\nLets rename it to\n\n**LF,low fat = Low Fat**\n\n**reg = Regular**","6911f1e9":"# 1. Lets First work on training dataset.....\n","7b33ea18":"Looks like missing values are in **Item_Weight(17.2%)** and **Outlet_Size(28.3%)**.","0dad3193":"**4.Outlet_Location_Type**","552b9cfe":"**2.Item_Type**"}}