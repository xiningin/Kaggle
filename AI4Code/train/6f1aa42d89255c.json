{"cell_type":{"99a77f94":"code","6c069b9d":"code","eaf72906":"code","dc62d333":"code","0d1667c3":"code","f4bd97be":"code","04957bc5":"code","957ad685":"code","644945be":"code","e1768054":"code","320f86d9":"code","ff2d1da1":"code","d733c32e":"code","2a97e8dd":"code","695c2844":"code","d723bbfd":"code","d554a97c":"code","7915b17b":"code","e7d8af73":"code","2ec2811a":"code","93644316":"code","23c5dcbc":"code","1b6d9c01":"code","bc31a353":"code","366813c4":"code","924f98fc":"code","b87ebacf":"code","e26050de":"code","9fa0ec4f":"code","c99a9361":"code","48798321":"code","40b8e365":"code","3e910485":"code","9851c814":"code","315fd1a8":"code","8d70464a":"code","84e0b7b6":"code","c7dde6a0":"code","32650b90":"code","c99cf7c1":"code","4df7c48a":"code","0c2d74ab":"code","c6503ed5":"code","f9e361c6":"code","c3da1512":"code","c6e7b184":"code","99f3c013":"code","5e8df8af":"markdown","ce8a427a":"markdown","a3c48047":"markdown","1357a4d6":"markdown","6745cc35":"markdown","3e513d85":"markdown","68890cd7":"markdown","57d8a792":"markdown","c47253ea":"markdown","2194da68":"markdown","5c03e04e":"markdown","d2662e19":"markdown","7b06d81d":"markdown","9449123d":"markdown","4185aaa1":"markdown","54039f0f":"markdown","0eef65c6":"markdown","c336bfc1":"markdown","929e0316":"markdown"},"source":{"99a77f94":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6c069b9d":"import seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom matplotlib import rcParams\nimport plotly.express as px\nimport plotly.io as pio\nfrom ipywidgets import Dropdown, Button, VBox, HBox, Output\nfrom IPython.display import clear_output, display \nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport cufflinks as cf\nimport plotly.offline","eaf72906":"params = {'legend.fontsize': 10,\n         'axes.labelsize': 16,\n         'axes.titlesize':16,\n         'xtick.labelsize':12,\n         'ytick.labelsize':12}\nrcParams.update(params)\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\n\npio.templates.default = \"plotly_white\"\npx.defaults.color_continuous_scale = px.colors.sequential.Blackbody\n\n\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)","dc62d333":"base_address = '..\/input\/learnplatform-covid19-impact-on-digital-learning'","0d1667c3":"parse_range_feature = lambda x: round(0.5*(float(x.split(',')[0][1:]) + float(x.split(',')[1][:-1])),1) if not pd.isnull(x) else x\n\ndf_district = pd.read_csv(os.path.join(base_address, 'districts_info.csv'))\nprint('Total number of rows', df_district.shape[0])\n\ndf_district['state'] = df_district['state'].astype('category')\ndf_district['locale'] = df_district['locale'].astype('category')\n\n# parse range features to get mean stat\nfor col in ['pct_black\/hispanic', 'pct_free\/reduced', 'county_connections_ratio', 'pp_total_raw']:\n    df_district[col] = df_district[col].apply(parse_range_feature)\n\n\nfor col in ['district_id', 'state', 'locale']:\n    print(f'Number of unique {col}', df_district[col].nunique())\n\nprint()\nprint('Null values column wise distribution')\nprint(pd.isnull(df_district).sum().to_dict())\nprint('Number of non nan, rows', df_district.dropna(how='any', axis=0).shape[0])\n\nprint()\nprint('State wise non nan rows', df_district.dropna(how='any', axis=0).groupby('state')['district_id'].count().to_dict())\n\nprint()\nprint('Locale wise non nan rows', df_district.dropna(how='any', axis=0).groupby('locale')['district_id'].count().to_dict())\n","f4bd97be":"# distribution if each of the range features, statewise and local wise distribution as box plots or histogram with iplot\n# scatter plot for combination of range feature\n# heat map with all range feature\n# county connection is a useless field provides no information","04957bc5":"figure = make_subplots(rows=1, cols=3) \nfigure.add_trace(\n    go.Scatter( x=df_district['pct_black\/hispanic'], y=df_district['pct_free\/reduced'], mode='markers', \n               name='minority vs free'), row=1, col=1)\nfigure.add_trace(\n    go.Scatter( x=df_district['pct_black\/hispanic'], y=df_district['pp_total_raw'], mode='markers', \n               name='minority vs investment'), row=1, col=2)\nfigure.add_trace(\n    go.Scatter( x=df_district['pct_free\/reduced'], y=df_district['pp_total_raw'], mode='markers',\n              name='investment vs free'), row=1, col=3)\n\n\n","957ad685":"px.parallel_coordinates(df_district,dimensions=['pct_black\/hispanic','pct_free\/reduced', 'pp_total_raw'])","644945be":"import itertools\ndistrict_columns = ['pct_black\/hispanic', 'pct_free\/reduced', 'pp_total_raw']","e1768054":"df_district[district_columns].corr()","320f86d9":"fig = make_subplots(rows=1, cols=3) \n\nfor i, (col1, col2) in enumerate(itertools.combinations(district_columns, 2)):\n    for data in px.scatter(df_district[(~pd.isnull(df_district[col1])) & (~pd.isnull(df_district[col2]))], \n               x=col1, y=col2, color='locale')['data']:\n        fig.add_trace(data, row=1, col=i+1)\n        fig.update_xaxes(title_text=col1, row=1, col=i+1)\n        fig.update_yaxes(title_text=col2, row=1, col=i+1)\nfig","ff2d1da1":"print(dict( enumerate(df_district['locale'].cat.categories ) ))\npx.parallel_coordinates(df_district,dimensions=['pct_black\/hispanic','pct_free\/reduced', 'pp_total_raw'], \n                        color=df_district['locale'].cat.codes)","d733c32e":"df_district.groupby('state')['district_id'].count().sort_values()[::-1].iplot(kind='bar', orientation='v',\n                                                                             yTitle='District Count')\ndf_district.dropna(how='any', axis=0).groupby('state')['district_id'].count().sort_values()[::-1].iplot(kind='bar', orientation='v',\n                                                                             yTitle='District Count with all data')","2a97e8dd":"df_district.groupby('state')[district_columns].mean().iplot(kind='bar', secondary_y='pp_total_raw')","695c2844":"df_product = pd.read_csv(os.path.join(base_address, 'products_info.csv'))\ndf_product.rename(columns={'LP ID': 'lp_id'}, inplace=True)\ndf_product['function'] = df_product['Primary Essential Function'].apply(\n    lambda x: x.split('-')[0] if not pd.isnull(x) else x)\nprint('shape', df_product.shape)\n\nfor col in df_product.columns:\n    print(f'Number of unique {col}', df_product[col].nunique(), '  ;',\n          'Number of null values', pd.isnull(df_product[col]).sum())","d723bbfd":"df_product.groupby('function')['lp_id'].count().iplot(kind='bar', theme='white')","d554a97c":"print('Top 10 service providers')\ndf_product.groupby('Provider\/Company Name')['lp_id'].count().sort_values()[::-1].head(10)","7915b17b":"from glob import glob\n\nlist_df = []\nfor x in glob(os.path.join(base_address, 'engagement_data\/*.csv' ))[:1]:\n    df_engage = pd.read_csv(x)\n    df_engage['district_id'] = os.path.splitext(os.path.basename(x))[0]\n    list_df.append(df_engage)\ndf_engage = pd.concat(list_df)\ndf_engage = df_engage.set_index('lp_id', drop=True).join(\n    df_product.set_index('lp_id', drop=True), how='inner').reset_index()\ndf_engage.set_index('time', inplace=True, drop=True)\ndf_engage.index = pd.to_datetime(df_engage.index)\n\nprint('shape', df_engage.shape)\nprint('time frame', df_engage.index.min(), df_engage.index.max())\nprint(df_engage[['engagement_index', 'pct_access']].describe())","e7d8af73":"#df_engage.iplot(kind='line', y='engagement_index', colors='lp_id')\n# is the uniformly sampled on a daily basis\n#cf.help('line')","2ec2811a":"df_engage_ = df_engage.reset_index().groupby(['time', 'Sector(s)']).mean()[['engagement_index', 'pct_access']].unstack(\n    level=1)\ndf_engage_.columns = ['_'.join(x) for x in df_engage_.columns]\nsecondary_y = [x for x in df_engage_.columns if 'pct_access' in x]\ndf_engage_.iplot(kind='line', secondary_y=secondary_y, theme='white', yTitle='Average Engagement Index',\n                secondary_y_title ='Access %', legend='bottom', title='Sectorwise Average Engagement')","93644316":"\n# number of lpids for each district_id\n#df_engage.groupby(['district_id'])['lp_id'].nunique().describe\ndf_engage.groupby(level=0).mean()[['engagement_index', 'pct_access']].iplot(kind='line', secondary_y='pct_access',\n                                                                           theme='white', yTitle='Average Engagement Index',\n                                                                            secondary_y_title ='Access %', \n                                                                            legend='top', \n                                                                            title='Average Engagement')","23c5dcbc":"df_engage.groupby(pd.Grouper(freq='M')).mean()[['engagement_index', 'pct_access']].iplot(\n    kind='line', secondary_y='pct_access',theme='white',yTitle='Average Engagement Index',\n                secondary_y_title ='Access %', legend='bottom', title='Monthly Average Engagement')\n\ndf_engage.groupby(pd.Grouper(freq='M')).mean()[['engagement_index', 'pct_access']].sort_index().diff().iplot(\n    kind='line', secondary_y='pct_access',theme='white',yTitle='Average Engagement Index',\n                secondary_y_title ='Access %', legend='bottom', title='Monthly Average Engagement Diff')","1b6d9c01":"df_engage.groupby(pd.Grouper(freq='M')).sum()[['engagement_index', 'pct_access']].iplot(\n    kind='line', secondary_y='pct_access',theme='white', yTitle='Total Engagement Index',\n                secondary_y_title ='Access %', legend='bottom', title='Monthly Total Engagement')","bc31a353":"px.scatter(df_engage, x='pct_access', y='engagement_index', log_y=True)","366813c4":"\"\"\"col = 'engagement_index'\ndf_engage['q95'] = df_engage.groupby('lp_id')[col].transform(lambda x: x.quantile(.95))    \ndf_engage['q05'] = df_engage.groupby('lp_id')[col].transform(lambda x: x.quantile(.05))\ndf_engage = df_engage[(df_engage[col] > df_engage['q05']) & (df_engage[col] < df_engage['q95'])]\ndel df_engage['q95']\ndel df_engage['q05']\"\"\"","924f98fc":"from tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')","b87ebacf":"def engage_feature_extraction(address):\n    df_engage = pd.read_csv(address)\n    df_engage = df_engage.groupby(['lp_id', 'time']).first().reset_index()\n\n    df_engage = df_engage.groupby(['lp_id'])[['engagement_index', 'pct_access']].agg([np.nansum, np.nanmean, np.nanmedian, \n                                                                                      np.std, np.nanmax])\n    df_engage = df_engage.dropna(how='any', axis=0)\n    if len(df_engage) == 0:\n        return pd.DataFrame()\n    df_engage.columns = ['_'.join(x) for x in df_engage.columns]\n   \n    df_engage = df_engage.join(\n        df_product.set_index('lp_id', drop=True), how='inner').reset_index()\n    \n    df_engage['district_id'] = int(os.path.splitext(os.path.basename(address))[0])\n    \n    df_engage = df_engage.set_index('district_id', drop=True).join(\n    df_district.set_index('district_id', drop=True), how='inner').reset_index()\n    \n    df_engage['district_id'] = df_engage['district_id'].astype('category')\n    df_engage['lp_id'] = df_engage['lp_id'].astype('category')\n    return df_engage","e26050de":"df_engage_features = pd.concat([engage_feature_extraction(x) \n    for x in tqdm(glob(os.path.join(base_address, 'engagement_data\/*.csv' )))]).reset_index(drop=True)\n\nprint('shape', df_engage_features.shape)\ndf_engage_features['n_products_per_district'] = df_engage_features.groupby('district_id')['lp_id'].transform('nunique')","9fa0ec4f":"# utilization of products in districts\ndf_product_utilization = df_engage_features.groupby(['lp_id'])['district_id'].nunique()\ndf_product_utilization.sort_values()[::-1].head(10)\ndf_product_utilization.iplot(kind='hist', bins=100, title='In how many districts a product is being used?',\n                            )","c99a9361":"df_product_utilization = df_engage_features.groupby(['district_id'])['lp_id'].nunique()\n\ndf_product_utilization.iplot(kind='hist', bins=100, title='Number of products being used in a district',)\n#df_product_utilization.sort_values()[::-1].head(10)","48798321":"df_engage_features.describe()","40b8e365":"df_district_engage = df_engage_features.groupby(['district_id']).agg(\n    {'lp_id':'nunique', 'engagement_index_nanmedian':'mean', 'pct_access_nanmedian': 'mean', 'pct_black\/hispanic':'mean',\n    'pct_free\/reduced':'mean', 'pp_total_raw':'mean', 'locale': 'first'})\nfor col in district_columns:\n    df_district_engage[col] = round(df_district_engage[col], 1)\n\ndf_district_engage = df_district_engage[\n    df_district_engage['engagement_index_nanmedian'] <df_district_engage['engagement_index_nanmedian'].quantile(.95)]\ndf_district_engage = df_district_engage[\n    df_district_engage['engagement_index_nanmedian'] >df_district_engage['engagement_index_nanmedian'].quantile(.02)]\ndf_district_engage.corr()","3e910485":"df_district_engage.groupby('pct_black\/hispanic').mean()","9851c814":"df_district_engage.groupby('pct_free\/reduced').mean()","315fd1a8":"fig = make_subplots(rows=1, cols=3) \n\nfor i, (col1, col2) in enumerate([('engagement_index_nanmedian', x) for x in district_columns]):\n    for data in px.scatter(\n        df_district_engage[(~pd.isnull(df_district_engage[col1])) & (~pd.isnull(df_district_engage[col2]))], \n         x=col1, y=col2, color='locale')['data']:\n        fig.add_trace(data, row=1, col=i+1)\n        fig.update_xaxes(title_text=col1, row=1, col=i+1)\n        fig.update_yaxes(title_text=col2, row=1, col=i+1)\nfig","8d70464a":"fig = make_subplots(rows=1, cols=3) \n\nfor i, (col1, col2) in enumerate([('engagement_index_nanmedian', x) for x in district_columns]):\n    \n    fig = px.parallel_coordinates(\n        df_district_engage[(~pd.isnull(df_district_engage[col1])) & (~pd.isnull(df_district_engage[col2]))], \n                                 dimensions=[col1, col2])\n\n    fig.show()","84e0b7b6":"px.parallel_coordinates(df_district_engage, \n                        #dimensions = ['engagement_index_nanmedian', 'pct_black\/hispanic', 'pct_access_nanmedian']\n                       )","c7dde6a0":"def get_ranking(data):\n    return data.sort_values('engagement_index_nanmedian')[::-1].iloc[:10]\n\nfor i, group in df_engage_features.groupby(['district_id', 'lp_id']):\n    break\n\n#df_district_produt_ranking = pd.concat([get_ranking(group) \n                                        #])\n    ","32650b90":"def add_meta_data_to_rank(data):\n    data = df_topk_products.join(df_engage_features.groupby('lp_id')['engagement_index_nanmedian'].mean(), \n                                         how='left')\n    data = df_topk_products.join(df_product.set_index('lp_id', drop=True), how='left')","c99cf7c1":"df_engage_features['used_in_district'] = df_engage_features.groupby('lp_id')['district_id'].transform('nunique')\nlist_df = []\nfor i, group in df_engage_features.groupby('district_id'):\n    group = group.sort_values('engagement_index_nanmedian', ascending=False)\n    group['rank_engagement'] =range(1, len(group)+1)\n    group\n    list_df.append(group)\ndf_product_ranking = pd.concat(list_df).reset_index(drop=True).set_index('lp_id', drop=True)\n\n#df_product_ranking = df_product_ranking.join(df_product.set_index('lp_id', drop=True), how='left').reset_index()","4df7c48a":"df_product_ranking.groupby('Product Name')[['rank_engagement', 'engagement_index_nanmedian']].mean().sort_values(\n    'rank_engagement').head(20)\n","0c2d74ab":"df_district_product_ranking=df_engage_features.groupby(['district_id']).apply(\n    lambda x: x.sort_values('engagement_index_nanmedian', ascending=False).iloc[:10])\n\n\ndf_topk_products = df_district_product_ranking.groupby('lp_id')[['district_id']].count().sort_values(by='district_id')[::-1]\ndf_topk_products = df_topk_products.join(df_engage_features.groupby('lp_id')['engagement_index_nanmedian'].mean(), \n                                         how='left')\ndf_topk_products = df_topk_products.join(df_product.set_index('lp_id', drop=True), how='left')\ndf_topk_products['rank'] = range(1, len(df_topk_products)+1)\n\n","c6503ed5":"from IPython.core.display import display, HTML\nfrom IPython.display import display_html \n\ndef display_side_by_side(dfs:list, captions:list):\n    \"\"\"Display tables side by side to save vertical space\n    Input:\n        dfs: list of pandas.DataFrame\n        captions: list of table captions\n    \"\"\"\n    output = \"\"\n    combined = dict(zip(captions, dfs))\n    for caption, df in combined.items():\n        output += df.style.set_table_attributes(\"style='display:inline'\").set_caption(caption)._repr_html_()\n        #output += \"\\xa0\\xa0\\xa0\"\n    display_html(output,raw=True)","f9e361c6":"list_df = []\ncaptions = []\nfor col in ['Provider\/Company Name', 'Product Name', 'Sector(s)', 'Primary Essential Function', 'function']:\n    list_df.append(df_topk_products.groupby(col)[['district_id']].count().sort_values('district_id')[::-1].head(10).join(\n        df_product.groupby(col)['lp_id'].count(), how='left'))\n    captions =[f'topk={df_topk_products.shape[0]}']\ndisplay_side_by_side(list_df, captions)","c3da1512":"\nfor col in ['Provider\/Company Name', 'Product Name', 'Sector(s)', 'Primary Essential Function', 'function']:\n    print('\\n',f'Top 10 {col}', \n      round(df_topk_products.groupby(col)['district_id'].count().sort_values()[::-1].head(10)\/118, 2).to_dict())\n\n","c6e7b184":"df_topk_products","99f3c013":"df_engage_features","5e8df8af":"- There is some correlation between minority population and free lunches\n- regions with free luncbes seems to have the lowest investment\n- regions with higher minority population has lower investment","ce8a427a":"### Product Ranking","a3c48047":"## Feature Extraction for a district and product","1357a4d6":"- district_id: unique identifier for a district in a state in US\n- state: state to which a district belongs to\n- locale: kind of location to which the district would belong to\n    - suburb, town, rural, city\n- pct_black\/hispanic: percentage of students in a districts who are black or hispanic\n- pct_free\/reduced: percentage of students elligible for free or reduced lunch\n- countyconnectionsratio: ratio of the county residents with high speed internet connection(>=200kbps)\n- pptotalraw: perpupill total expenditure (local + federal)\n    ","6745cc35":"- Rural and suburbs show positive relationship with engagement and investment, too little data for city and town to make a decision\n- Scatter plots are a bit inconclusive for minority relation, but tree plots confirm that minority population and engagement are inversly related, which is also confirmed with correlation value","3e513d85":"- Logarithmic relationship between engagement index and the access percentage specially at the lower regions","68890cd7":"- Rural and suburbs show a clear positive correlation between free lunches and minority population , Also negative relationship between minority population and investment\n- too little data to make any conculsion for rural and town areas","57d8a792":"- Topk learning tools for each district and how many of them are shared based on voting\n- topk among minority dominated regions and their average engagement index and compare them with non minority regions\n- relationship between minority population and engagement index and the spending","c47253ea":"## Sample Location","2194da68":"## District Level Relationships","5c03e04e":"# District Info\n","d2662e19":"- Few outlier products an be seen from the above distribution, what kind of products are present are these?","7b06d81d":"# Product","9449123d":"## State","4185aaa1":"- average engagement index and pct_access show direct relationship\n- second half of 2020 has higher engagement \n    - covid influencedue to strict lockdown and schools being shutdown\n- daily pattern can be observed for engagement_index and pct_access\n- holidays have an impact on the metrics\n*****\n- do exam periods have an impact on the metric?","54039f0f":"# Engagement","0eef65c6":"- Majority of the products of LC products","c336bfc1":"- engagement is negatively correlated with minority and reduced prices and positively correlated with expenditure","929e0316":"### Locale"}}