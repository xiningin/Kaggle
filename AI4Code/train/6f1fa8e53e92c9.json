{"cell_type":{"f9d1a0c0":"code","4ea5c7cb":"code","4cc6adc9":"code","b6932a21":"code","b2d693f8":"code","2d4df08c":"code","14ef6916":"code","fe107f67":"code","a8cbc717":"code","2180947f":"code","86a00b43":"code","5f3d5c81":"code","360740a9":"code","9251da37":"code","a003ec2a":"code","a005bbd2":"code","adfe94f4":"code","dde2c9eb":"code","84e79a66":"code","a4bc23c3":"code","248f84c0":"code","46fe09fb":"code","295082b7":"code","59d8ee0d":"code","88229836":"code","7369c5b5":"code","7fe8abd2":"code","446ea9b5":"code","c578937f":"code","36ea6d62":"code","0b624165":"code","92896ddf":"code","87895f38":"code","fa4b41f4":"code","ea0ba95f":"code","4170a021":"code","494a68e0":"code","9a7a7fcb":"code","50d445c9":"code","9d35bbac":"code","4855b2eb":"code","c55ed441":"code","140d4571":"code","963276f9":"code","fc377b6e":"code","bf257701":"code","94fb1db0":"code","3ccea80c":"code","01ad0931":"markdown","beb3f388":"markdown","739b29f9":"markdown","ac5f5b67":"markdown","bdbf1951":"markdown","1ae9f4a0":"markdown","af454c7b":"markdown","e3ba1b8f":"markdown","1fa821d6":"markdown","3876420f":"markdown","f3bb445e":"markdown","a56eadeb":"markdown","d9923cea":"markdown","7a6da340":"markdown","114eb95e":"markdown","2e709711":"markdown","374e55fb":"markdown","4032d812":"markdown","0f31c9da":"markdown","873b7f7c":"markdown","4fdf5a84":"markdown","6d06d792":"markdown","b82dc55c":"markdown","e7babaf9":"markdown"},"source":{"f9d1a0c0":"import numpy as np \nimport pandas as pd \n\nPath = '\/kaggle\/input\/titanic'\n\ndf_train = pd.read_csv(Path+'\/train.csv').set_index('PassengerId')\ndf_test = pd.read_csv(Path+'\/test.csv').set_index('PassengerId')","4ea5c7cb":"df_train","4cc6adc9":" df_train.isna().sum()","b6932a21":" df_test.isna().sum()","b2d693f8":"import string\n\ndef substrings_in_string(big_string, substrings):\n    for substring in substrings:\n        if str(big_string).find(substring) != -1:\n            return substring\n    return np.nan\n\ndef replace_titles(x):\n    title=x['Title']\n    if title in ['Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col']:\n        return 'Mr'\n    elif title in ['Countess', 'Mme']:\n        return 'Mrs'\n    elif title in ['Mlle', 'Ms']:\n        return 'Miss'\n    elif title =='Dr':\n        if x['Sex']=='Male':\n            return 'Mr'\n        else:\n            return 'Mrs'\n    else:\n        return title\n\ndef create_title(df):     \n    title_list=['Mrs', 'Mr', 'Master', 'Miss', 'Major', 'Rev',\n                        'Dr', 'Ms', 'Mlle','Col', 'Capt', 'Mme', 'Countess',\n                        'Don', 'Jonkheer']\n    df['Title']=df['Name'].map(lambda x: substrings_in_string(x, title_list))\n\n    df['Title']=df.apply(replace_titles, axis=1)\n    \ncreate_title(df_train)\ncreate_title(df_test)","2d4df08c":"def create_adult(df):\n    df['Adult'] = np.where((df['Title'] == 'Mr') | (df['Title'] =='Mrs'), 1, 0)\n\ncreate_adult(df_train)\ncreate_adult(df_test)","14ef6916":"cabin_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'T']\n\ndef create_cabin(df):\n    df['Deck'] = df['Cabin'].map(lambda x: substrings_in_string(x, cabin_list))\n                                \ncreate_cabin(df_train)\ncreate_cabin(df_test)     ","fe107f67":"def create_family(df):\n    df['Family'] = df['SibSp'] + df['Parch'] + 1\n    df.drop(columns=[ \"SibSp\",\"Parch\"], inplace=True)\n    \ncreate_family(df_train)\ncreate_family(df_test) ","a8cbc717":"def create_surname(df):    \n    data = df['Name']\n    families = []\n    \n    for i in range(len(data)):        \n        name = data.iloc[i]\n    \n        if '(' in name:\n            name_no_bracket = name.split('(')[0] \n        else:\n            name_no_bracket = name\n            \n        family = name_no_bracket.split(',')[0]\n        title = name_no_bracket.split(',')[1].strip().split(' ')[0]\n        \n        for c in string.punctuation:\n            family = family.replace(c, '').strip()\n            \n        families.append(family)\n    \n    df['Surname'] = families\n    \ncreate_surname(df_train)\ncreate_surname(df_test)","2180947f":"def create_frequencies_and_sr(df_train, df_test):\n    \n    # Create frequencies of same value for features Surname, Ticket and Cabin \n    df_train_surname_fr = df_train.groupby('Surname')['Surname'].count()\n    df_train_ticket_fr = df_train.groupby('Ticket')['Ticket'].count()\n    df_train_cabin_fr = df_train.groupby('Cabin')['Cabin'].count()\n    \n    # Join in Train and in Test the frequencies of the Train\n    df_train = df_train.join(df_train_surname_fr, on=\"Surname\", rsuffix=\"Frequency\")\n    df_train = df_train.join(df_train_ticket_fr, on=\"Ticket\", rsuffix=\"Frequency\")\n    df_train = df_train.join(df_train_cabin_fr, on=\"Cabin\", rsuffix=\"Frequency\")\n    df_test = df_test.join(df_train_surname_fr, on=\"Surname\", rsuffix=\"Frequency\")\n    df_test = df_test.join(df_train_ticket_fr, on=\"Ticket\", rsuffix=\"Frequency\")\n    df_test = df_test.join(df_train_cabin_fr, on=\"Cabin\", rsuffix=\"Frequency\")\n    \n    #\n    # Create SurvivalRateWeight, more is the value more is the weight. \n    #\n    \n    # For the Train, when frequency is 1 we give 1 point to the weight, since the rate will be not usable, same where Cabin value is NaN.  \n    df_train[\"SurvivalRateWeight\"] = 3 - (df_train[[\"CabinFrequency\"]].isna().sum(axis=1) + \\\n    (df_train[[ \"SurnameFrequency\",\"TicketFrequency\",\"CabinFrequency\"]] == 1).sum(axis=1))\n    \n    # For the Test, when frequency is NaN we give 1 point to the weight, since the rate will be not usable.\n    df_test[\"SurvivalRateWeight\"] =  3 - df_test[[ \"SurnameFrequency\",\"TicketFrequency\",\"CabinFrequency\"]].isna().sum(axis=1)  \n\n    # Fill NaN frequencies with 1 for Train and Test\n    df_train['CabinFrequency'].fillna(value=1, inplace=True)\n    df_test['SurnameFrequency'].fillna(value=1, inplace=True)\n    df_test['TicketFrequency'].fillna(value=1, inplace=True)\n    df_test['CabinFrequency'].fillna(value=1, inplace=True)\n\n    # Create SurvivalRate for the three features\n    df_train_surname_sr = df_train.groupby('Surname')[['Survived', 'Surname']].mean().add_suffix(\"SurnameRate\")\n    df_train_ticket_sr = df_train.groupby('Ticket')[['Survived', 'Ticket',]].mean().add_suffix(\"TicketRate\")\n    df_train_cabin_sr = df_train.groupby('Cabin')[['Survived', 'Cabin']].mean().add_suffix(\"CabinRate\")\n\n    # Join in Train and in Test the rates of the Train\n    df_train = df_train.join(df_train_surname_sr, on=\"Surname\")\n    df_train = df_train.join(df_train_ticket_sr, on=\"Ticket\")\n    df_train = df_train.join(df_train_cabin_sr, on=\"Cabin\")\n    df_test = df_test.join(df_train_surname_sr, on=\"Surname\")\n    df_test = df_test.join(df_train_ticket_sr, on=\"Ticket\")\n    df_test = df_test.join(df_train_cabin_sr, on=\"Cabin\")\n    \n    # Correct rates of the Train with central value 0.5 for every passenger who have unique value (or empty) in the three features \n    # (Otherwise we would have for this passengers a value equal to the target value) \n    df_train['SurvivedSurnameRate'] = [0.5 if freq in [1] else (rate*freq-val)\/(freq-1) for freq,val,rate in zip(df_train[\"SurnameFrequency\"],df_train[\"Survived\"],df_train['SurvivedSurnameRate'])]\n    df_train['SurvivedTicketRate'] = [0.5 if freq in [1] else (rate*freq-val)\/(freq-1) for freq,val,rate in zip(df_train[\"TicketFrequency\"],df_train[\"Survived\"],df_train['SurvivedTicketRate'])]\n    df_train['SurvivedCabinRate'] = [0.5 if freq in [1] else (rate*freq-val)\/(freq-1) for freq,val,rate in zip(df_train[\"CabinFrequency\"],df_train[\"Survived\"],df_train['SurvivedCabinRate'])]\n\n    # Create final SurvivalRate and fill NaN values in the train with central value 0.5\n    df_train['SurvivalRate'] = df_train[[ \"SurvivedTicketRate\",\"SurvivedCabinRate\",\"SurvivedSurnameRate\"]].mean(axis=1)\n    df_test['SurvivalRate'] = df_test[[\"SurvivedTicketRate\",\"SurvivedCabinRate\",\"SurvivedSurnameRate\"]].mean(axis=1)\n    df_test['SurvivalRate'].fillna(value=0.5, inplace=True)\n    \n    \n    drop_cols = [\"SurvivedTicketRate\",\"SurvivedCabinRate\",\"SurvivedSurnameRate\"]    \n    \n    df_train.drop(columns=drop_cols, inplace=True)\n    df_test.drop(columns=drop_cols, inplace=True)\n    \n    return (df_train, df_test)\n\ndf_train, df_test = create_frequencies_and_sr(df_train, df_test)\n\ndf_train = df_train.drop(columns=['Surname'])\ndf_test = df_test.drop(columns=['Surname'])","86a00b43":"df_train","5f3d5c81":"from matplotlib import pyplot as plt\nimport seaborn as sns\n\na4_dims = (16,9)\nfig, ax = plt.subplots(figsize=a4_dims)\n\nsns.heatmap(df_train.corr(), annot = True, vmin=-1, vmax=1, center= 0,cmap='BrBG', square=True)","360740a9":"g = sns.pairplot(data=df_train, hue='Survived', palette = 'seismic',\n                 height=1.2, diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10))\ng.set(xticklabels=[])\ng.fig.set_size_inches(15,15)","9251da37":"from sklearn.preprocessing import KBinsDiscretizer\n\ndef corr_bin(val, feat):\n    test_bin = pd.read_csv(Path+'\/train.csv')\n    disc = KBinsDiscretizer(n_bins=val, encode='ordinal', strategy='kmeans')\n    test_bin[feat] = disc.fit_transform(df_train[[feat]])\n    print(f'\"{feat}\" correlation with {val} bins: {test_bin.corr()[\"Survived\"][feat]}')","a003ec2a":"for i in range(3,10):\n    corr_bin(i, \"Fare\")    ","a005bbd2":"df_test[[\"Fare\"]] = df_test[[\"Fare\"]].fillna(value=df_train[\"Fare\"].median())","adfe94f4":"disc = KBinsDiscretizer(n_bins=7, encode='ordinal', strategy='kmeans')\ndisc.fit(df_train[[\"Fare\"]])\ndf_train[\"Fare\"] = disc.transform(df_train[[\"Fare\"]])\ndf_test[\"Fare\"] = disc.transform(df_test[[\"Fare\"]])","dde2c9eb":"import matplotlib.pyplot as plt \nimport seaborn as sns\n\nfeat_hist = 'Fare'\nfig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x=feat_hist, hue='Survived', data=df_train, palette=sns.color_palette(\"muted\", 2))\n\nplt.xlabel(feat_hist, size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format(feat_hist), size=15, y=1.05)\n\nplt.show()","84e79a66":"sns.histplot(data=df_train[df_train['Age'].isna()], x=\"Title\")","a4bc23c3":"df_train.groupby([\"Title\"]).mean()['Age']","248f84c0":"age_medians = df_train.groupby([\"Title\"]).mean()['Age']\n\ndf_test[[\"Age\"]] = [age if age == age else age_medians[title] for title,age in zip(df_test['Title'], df_test['Age']) ]\ndf_train[[\"Age\"]] = [age if age == age else age_medians[title] for title,age in zip(df_train['Title'], df_train['Age']) ]","46fe09fb":"for i in range(3,12):\n    corr_bin(i, \"Age\")","295082b7":"from sklearn.preprocessing import KBinsDiscretizer\ndisc = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='kmeans')\ndisc.fit(df_train[[\"Age\"]])\ndf_train[\"Age\"] = disc.transform(df_train[[\"Age\"]])\ndf_test[\"Age\"] = disc.transform(df_test[[\"Age\"]])","59d8ee0d":"import matplotlib.pyplot as plt \nimport seaborn as sns\n\nfeat_hist = 'Age'\nfig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x=feat_hist, hue='Survived', data=df_train, palette=sns.color_palette(\"muted\", 2))\n\nplt.xlabel(feat_hist, size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format(feat_hist), size=15, y=1.05)\n\nplt.show()","88229836":"deck_normalized = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'T':8}\n\n\ndef encode_cabin(df):\n    df.replace({\"Deck\": deck_normalized}, inplace=True)\n    df['Deck'].fillna(value=9, inplace=True)        \n\nencode_cabin(df_train)\nencode_cabin(df_test) ","7369c5b5":"df_train['IsMale'] = np.where(df_train['Sex'] == 'male', 1, 0)\ndf_test['IsMale'] = np.where(df_test['Sex'] == 'male', 1, 0)\ndf_train = df_train.drop(columns=['Sex'])\ndf_test = df_test.drop(columns=['Sex'])","7fe8abd2":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\ndef encode_onehot(df_train, df_test, cols):\n\n    df_train = df_train.reset_index()\n    df_test = df_test.reset_index()\n    \n    \n    to_be_encoded = [c for c in df_train.columns if c in cols]\n    \n    if len(to_be_encoded) > 0:\n        enc = OneHotEncoder(handle_unknown='ignore')\n\n        enc_df_train = pd.DataFrame(enc.fit_transform(df_train[to_be_encoded]).toarray())\n        enc_df_train.columns = enc.get_feature_names(to_be_encoded)\n        \n        df_train = df_train.join(enc_df_train)\n        df_train = df_train.drop(columns=[c for c in df_train.columns if c in to_be_encoded])\n        \n\n        enc_df_test = pd.DataFrame(enc.transform(df_test[to_be_encoded]).toarray())\n        enc_df_test.columns = enc_df_train.columns\n        df_test = df_test.join(enc_df_test)\n        df_test = df_test.drop(columns=[c for c in df_test.columns if c in to_be_encoded])\n       \n    return (df_train.set_index(\"PassengerId\"), df_test.set_index(\"PassengerId\"))\n\n\ndf_train['Embarked'].fillna(value='others', inplace=True)\ndf_test['Embarked'].fillna(value='others', inplace=True)\ndf_train, df_test = encode_onehot(df_train, df_test, ['Embarked', 'Title'])\n\ndf_train = df_train.drop(columns=['Embarked_others'])\ndf_test = df_test.drop(columns=['Embarked_others'])","446ea9b5":"to_drop = ['Name', 'Ticket','Cabin']\n\ndf_train = df_train.drop(columns=to_drop)\ndf_test = df_test.drop(columns=to_drop)","c578937f":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.compose import ColumnTransformer\n\ndef scale(df_train, df_test):\n    \n    train_copy = df_train.drop(columns=['Survived'])\n    col_names = [c for c in train_copy.columns if c not in  ['Survived']]\n\n    ct = ColumnTransformer([('rsc', RobustScaler(), col_names)], remainder='passthrough')\n\n    scal_train = ct.fit_transform(train_copy)\n    scal_test = ct.transform(df_test)  \n    \n    return (pd.DataFrame(scal_train, index=df_train.index, columns=train_copy.columns).join(df_train['Survived']),\n            pd.DataFrame(scal_test, index=df_test.index, columns=df_test.columns))\n\ndf_train, df_test = scale(df_train, df_test)","36ea6d62":"def df_split(df, col):\n    y = df[col]\n    X = df.loc[:, df.columns != col]\n    return (X,y)\n\nX_train, y_train = df_split(df_train, 'Survived')","0b624165":"from sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict\n\nMETRIC = 'f1'\nFOLDS = 5\n\ndef modelfit(X, y, alg, cv_folds=FOLDS, plot=False):\n       \n    #Fit the algorithm on the data\n    alg.fit(X, y)\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(X)\n    dtrain_predprob = alg.predict_proba(X)[:,1]\n    \n    #Perform cross-validation:\n    cv_score = cross_val_score(alg, X, y, cv=cv_folds, scoring=METRIC, n_jobs = -1)\n    cv_pred = cross_val_predict(alg, X, y, cv=cv_folds, n_jobs = -1)\n\n\n    if(plot):\n                \n        #Print model report:\n        desc = \"Train Accuracy: %.4g | \" % metrics.accuracy_score(y.values, dtrain_predictions)\n        desc = desc + \" Train AUC Score: %f\\n\" % metrics.roc_auc_score(y, dtrain_predprob)\n\n        desc = desc + \"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % \\\n                (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score))\n\n        #Print Summary:\n        a4_dims = (16, 5)\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=a4_dims)\n        fig.suptitle(f'{type(alg).__name__} Model Report')\n        fig.subplots_adjust(bottom=0.2)\n        fig.text(0.5, 0, desc, ha='center', va='center')\n    \n        ax1.title.set_text(\"Confusion matrix\")\n        cf_matrix = metrics.confusion_matrix(y.values, cv_pred)\n        group_names = ['TN','FP','FN','TP']\n        group_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\n        group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()\/np.sum(cf_matrix)]\n        labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n        labels = np.asarray(labels).reshape(2,2)\n        sns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap='Blues', xticklabels=False, yticklabels=False, ax=ax1)\n\n        fpr, tpr, threshold = metrics.roc_curve(y.values, cv_pred)\n        roc_auc = metrics.auc(fpr, tpr)\n\n        ax2.title.set_text(\"Receiver Operating Characteristic\")\n        ax2.set_ylabel('True Positive Rate')\n        ax2.set_xlabel('False Positive Rate')\n        sns.lineplot(x=fpr, y = tpr, ax=ax2)\n        sns.lineplot(x=[0, 1], y = [0, 1], ax=ax2)\n        ax2.lines[1].set_linestyle(\"--\")\n        ax2.legend(['AUC: %0.2f' % roc_auc], loc=\"lower right\")\n    \n    return cv_score","92896ddf":"from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\n\nSEED = 11\n\nmodels = [\n    ('kNN',KNeighborsClassifier()),\n    ('SVC',SVC(random_state=SEED, probability=True)),\n    ('NuSVC',NuSVC(random_state=SEED, probability=True)),\n    ('DT',DecisionTreeClassifier(random_state=SEED)),\n    ('RF',RandomForestClassifier(random_state=SEED)),\n    ('AdaB',AdaBoostClassifier(random_state=SEED)),\n    ('GB',GradientBoostingClassifier(random_state=SEED)),\n    ('GauNB',GaussianNB()),\n    ('LR',LogisticRegression(solver='liblinear'))\n]","87895f38":"results = []\nagg = []\nnames = []\nfor name, model in models:\n    res = modelfit(X_train, y_train, model)\n    agg.append([res.mean(), res.std()])\n    results.append(res)\n    names.append(name)\n\nscores = pd.DataFrame(agg, index=names, columns=[\"Mean\",\"Std\"])\nscores = scores.sort_values(by='Mean',ascending=False)\n\nscores","fa4b41f4":"# boxplot algorithm comparison\nfig = plt.figure(figsize=(12,7))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nsns.boxplot(data=results)\nax.set_xticklabels(names)\nplt.show()","ea0ba95f":"modelfit(X_train, y_train, GradientBoostingClassifier(random_state=SEED), plot=True)","4170a021":"param_test1 = {'n_estimators':range(10,70,1), 'learning_rate':np.linspace(0.05,0.15,10)}\ngsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(max_depth=8, min_samples_split=10, min_samples_leaf=5, max_features='sqrt', subsample=0.8, random_state=SEED), \n    param_grid = param_test1, scoring=METRIC, n_jobs=-1,cv=FOLDS)\ngsearch1.fit(X_train, y_train)\n\nprint(f'{gsearch1.best_score_, gsearch1.best_params_}')","494a68e0":"param_test2 = {'max_depth':range(1,40,1), 'min_samples_split':range(2,70,1)}\ngsearch2 = GridSearchCV(estimator = GradientBoostingClassifier(min_samples_leaf=5, max_features='sqrt', subsample=0.8, random_state=SEED), \n    param_grid = {**dict(map(lambda x: (x[0], [x[1]]), gsearch1.best_params_.items())), **param_test2}, scoring=METRIC, n_jobs=-1, cv=FOLDS)\ngsearch2.fit(X_train, y_train) \n\nprint(f'{ gsearch2.best_score_, gsearch2.best_params_}')","9a7a7fcb":"param_test3 = {'min_samples_split':range(2,100,1), 'min_samples_leaf':range(1,40,1)}\ngsearch3 = GridSearchCV(estimator = GradientBoostingClassifier(max_features='sqrt', subsample=0.8, random_state=SEED), \n    param_grid = {**dict(map(lambda x: (x[0], [x[1]]), gsearch2.best_params_.items())), **param_test3}, scoring=METRIC, n_jobs=-1, cv=FOLDS)\ngsearch3.fit(X_train, y_train)\n\nprint(f'{gsearch3.best_score_, gsearch3.best_params_}')","50d445c9":"modelfit(X_train, y_train, gsearch3.best_estimator_, plot=True)","9d35bbac":"param_test4 = {'max_features':range(1,50,1)}\ngsearch4 = GridSearchCV(estimator = GradientBoostingClassifier(subsample=0.8, random_state=SEED), \n    param_grid = {**dict(map(lambda x: (x[0], [x[1]]), gsearch3.best_params_.items())), **param_test4}, scoring=METRIC, n_jobs=-1, cv=FOLDS)\ngsearch4.fit(X_train, y_train)\n\nprint(f'{ gsearch4.best_score_, gsearch4.best_params_}')","4855b2eb":"param_test5 = {'subsample':np.linspace(0.1,2,58)}\ngsearch5 = GridSearchCV(estimator = GradientBoostingClassifier(random_state=SEED), \n    param_grid = {**dict(map(lambda x: (x[0], [x[1]]), gsearch4.best_params_.items())), **param_test5}, scoring=METRIC, n_jobs=-1, cv=FOLDS)\ngsearch5.fit(X_train, y_train)\n\nprint(f'{ gsearch5.best_score_, gsearch5.best_params_}')","c55ed441":"gbm_tuned_1 =  GradientBoostingClassifier(learning_rate= 0.10555555555555556, max_depth= 6, \n                                          max_features=4, min_samples_leaf=23, min_samples_split=47, n_estimators= 38, \n                                          subsample=  0.7999999999999999,random_state=SEED)\nmodelfit(X_train, y_train, gbm_tuned_1, plot=True)","140d4571":"gbm_tuned_2 =  GradientBoostingClassifier(learning_rate= 0.010555555555555556,  n_estimators= 380, max_depth= 6, \n                                          max_features=4, min_samples_leaf=23, min_samples_split=47,\n                                          subsample=  0.7999999999999999,random_state=SEED)\nmodelfit(X_train, y_train, gbm_tuned_2, plot=True)","963276f9":"best_model = gbm_tuned_2\nbest_model.fit(X_train,y_train)","fc377b6e":"from sklearn.model_selection import StratifiedKFold\n\nprobs = pd.DataFrame(np.zeros((len(df_test), FOLDS * 2)), columns=['Fold_{}_Prob_{}'.format(i, j) for i in range(1, FOLDS + 1) for j in range(2)])\nimportances = pd.DataFrame(np.zeros((X_train.shape[1], FOLDS)), columns=['Fold_{}'.format(i) for i in range(1, FOLDS + 1)], index=df_test.columns)\n\nskf = StratifiedKFold(n_splits=FOLDS, random_state=SEED, shuffle=True)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n    # Fitting the model\n    best_model.fit(X_train.iloc[trn_idx, :], y_train.iloc[trn_idx])\n        \n    # X_test probabilities\n    probs.loc[:, 'Fold_{}_Prob_0'.format(fold)] = best_model.predict_proba(df_test)[:, 0]\n    probs.loc[:, 'Fold_{}_Prob_1'.format(fold)] = best_model.predict_proba(df_test)[:, 1]\n    importances.iloc[:, fold - 1] = best_model.feature_importances_","bf257701":"importances['Mean_Importance'] = importances.mean(axis=1)\nimportances.sort_values(by='Mean_Importance', inplace=True, ascending=False)\n\nplt.figure(figsize=(15, 20))\nsns.barplot(x='Mean_Importance', y=importances.index, data=importances, palette=\"viridis\")\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\nplt.title('Mean Feature Importance Between Folds', size=15)\n\nplt.show()","94fb1db0":"class_survived = [col for col in probs.columns if col.endswith('Prob_1')]\nprobs['1'] = probs[class_survived].sum(axis=1) \/ FOLDS\nprobs['0'] = probs.drop(columns=class_survived).sum(axis=1) \/ FOLDS\nprobs['pred'] = 0\npos = probs[probs['1'] >= 0.5].index\nprobs.loc[pos, 'pred'] = 1\n\ndf_test.reset_index(inplace=True)\ndf_test['Survived']  = probs['pred'].astype(int)","3ccea80c":"_, sub = df_split(df_test.set_index(\"PassengerId\"), 'Survived')\nsub.to_csv('submission.csv', header=True, index=True)","01ad0931":"## Family\nSibSp and Parch features represent respectively the sum of Siblings-Spouses and Parents-Childrens. We merge this two information in a unique family feature.","beb3f388":"## Scaling dataset\nWe scale the dataset with robust scaler to improve performances of sensible models","739b29f9":"### **Public submission score: 0.80861**","ac5f5b67":"## SurvivalRate and SurvivalRateWeight\nEqual values in the Surname, Cabin and Ticket features indicate that there is a correlation between passengers. For example, passengers who have the same ticket number have probably a parental or emotional connection.\nWe therefore want to create a SurvivalRate feature that represents the average survival value of related passengers from the same Surname, Cabin or Ticket.\n\nWe will also create an additional feature that represents the weight of this feature, called SurvivalRateWeight. Infact, if SurvivalRate comes from three different values or one, it will probably have a different importance.\n\nIn addition we also mantains the frequencies features. ","bdbf1951":"# Features Correlation","1ae9f4a0":" We fix smartly the hyperparameters and perform multiple grid search to fit parameters in order of importance.\n","af454c7b":"## Fare and Age binning\nAge and Fare are features containing too many detail. For example, difference in survival might be high from younger to mid age pasengers and probably irrelevant from similar age passenger (e.g. 18 to 19 years old). In these cases it is useful to bring out the information by dividing the data into bins. We set bin using kmeans algorithm, and we choose quantity of bins basing on highest Survival correlation. \n","e3ba1b8f":"## Comparing baselines\nWe want to understand which algorithm performs best on our dataset. We perform a cross validation with f1 metric and evaluate the results.\n\nThe different classifiers are:\n - K-Nearest Neighbors\n - Support Vector\n - Nu-Support Vector \n - Decision Tree \n - Random Forests\n - AdaBoost\n - Gradient Boosting\n - Gaussian Naive Bayes\n - Logistic Regression\n ","1fa821d6":"# Model Selection","3876420f":"# Feature Engineering","f3bb445e":"## Passenger Title\nPassenger names are saved with a well-defined pattern. Therefore we can extract the person's title from the name, and then normalize it.\n","a56eadeb":"Now we modify learning rate and estimators respectively by dividing and multiplying by the same amount to better generalize on the test set.","d9923cea":"## Deck label encoding\nWe use simple label encoding for deck since decks are ordered by height in the ship.\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/0\/0d\/Olympic_%26_Titanic_cutaway_diagram.png\/402px-Olympic_%26_Titanic_cutaway_diagram.png)","7a6da340":"### Feature Adult\nWe merge Mr and Mrs in Adult feature","114eb95e":"### **Public submission score: 0.80622**","2e709711":"## IsMale binary encoding \nWe transform Sex feature in IsMale binary feature","374e55fb":"# Introduction\nThis is a notebook of the famous Titanic problem.\n\nIn this notebook we opted to not use data or information from test set during the training phase. In this way we do not add information for the training phase that should not be known and could cause an unconventional increase in model performances.","4032d812":"## Deck\nThe cabin code is divided into a first letter which represents the deck and a number. We can extract deck information and thus create a new feature.","0f31c9da":"## Embarked and Title one hot encoding\nFor this features there is no smart ordering therefore we choose to encode one-hotly","873b7f7c":"We remove unused features","4fdf5a84":"\n## Fit Model\n\nFinally, we fit the model and stratify the target variable to generalize better.","6d06d792":"Parts of this notebook was inspired by [Titanic - Advanced Feature Engineering Tutorial](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial), thanks to the author [Gunes Evitan](https:\/\/www.kaggle.com\/gunesevitan)","b82dc55c":" # Gradient Boost Classifier\n \n The Gradient Boost Classification algorithm seems to perform better then other models, therefore we get into fine tuning hyperparameters.","e7babaf9":"# Features Encoding"}}