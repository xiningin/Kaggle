{"cell_type":{"814174f6":"code","483c69ab":"code","f8a7e5fe":"code","c6475835":"code","53553f5f":"code","67bf8db5":"code","4cfd6bf3":"code","1af59a26":"code","1655b9a0":"code","af404e7f":"code","a5da5c0c":"code","247cd77a":"code","5472ef2f":"code","e5948cc0":"code","399459ee":"code","39a0584b":"code","41a3491e":"code","e16673d6":"code","fb5f5a0b":"code","8ed3421b":"code","8ba46125":"code","8b75cded":"code","e9bd4144":"code","3b675476":"code","96bd9934":"code","67d2968d":"code","277f4003":"code","331bb073":"code","201b882d":"code","bcc1620f":"markdown","7be7abc7":"markdown","14fde93e":"markdown","04127c2b":"markdown","fc901bfc":"markdown","132b8e40":"markdown","06e4817e":"markdown","cda5bf30":"markdown","931c8e1e":"markdown","391122c0":"markdown"},"source":{"814174f6":"#-------For DataFrame and Series data manipulation\nimport pandas as pd\nimport numpy as np\n#-------Data visualisation imports\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#-------Interactive data visualisation imports\nfrom plotly import __version__\n#print(__version__)\nimport cufflinks as cf\nfrom plotly.offline import download_plotlyjs,init_notebook_mode,plot,iplot\ninit_notebook_mode(connected=True)\ncf.go_offline()\n#-------To make data visualisations display in Jupyter Notebooks\n%matplotlib inline\n#-------To split data into Training and Test Data\nfrom sklearn.model_selection import train_test_split\n#-------To make pipelines\nfrom sklearn.pipeline import Pipeline\n#-------For Natural Language Processing data cleaning\nfrom sklearn.feature_extraction.text import TfidfTransformer\n#CountVectorizer converts collection of text docs to a matrix of token counts.\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n#-------For model scoring\nfrom sklearn.metrics import classification_report","483c69ab":"trainmessages = pd.read_csv('..\/input\/train.tsv', sep='\\t')","f8a7e5fe":"trainmessages.info()","c6475835":"trainmessages['Phrase'].describe()","53553f5f":"trainmessages.head()","67bf8db5":"#we have to import the test data to fit to our model\ntestmessages = pd.read_csv('..\/input\/test.tsv', sep='\\t')\ntestmessages.head()","4cfd6bf3":"sns.countplot(data=trainmessages,x='Sentiment')","1af59a26":"#to get the numerical values of the above countplot\ntrainmessages['Sentiment'].iplot(kind='hist')","1655b9a0":"trainmessages.isnull().sum()","af404e7f":"trainmessages.isna().sum()","a5da5c0c":"trainmessages['Length'] = trainmessages['Phrase'].apply(lambda x: len(str(x).split(' ')))","247cd77a":"trainmessages['Length'].unique()","5472ef2f":"data = [dict(\n  type = 'box',\n  x = trainmessages['Sentiment'],\n  y = trainmessages['Length'],\n  transforms = [dict(\n    type = 'groupby',\n    groups = trainmessages['Sentiment'],\n  )]\n)]\niplot({'data': data}, validate=False)","e5948cc0":"sns.pairplot(trainmessages,hue='Sentiment',vars=['PhraseId','SentenceId','Length'])","399459ee":"#double-check for any empty Phrases\ntrainmessages = trainmessages[trainmessages['Phrase'].str.len() >0]","39a0584b":"#are there any empty Phrases? if so let's remove them\ntrainmessages[trainmessages['Phrase'].str.len() == 0].head()","41a3491e":"trainmessages = trainmessages[trainmessages['Phrase'].str.len() != 0]","e16673d6":"trainmessages[trainmessages['Phrase'].str.len() == 0].head()","fb5f5a0b":"#create function to clean data\ndef text_process(mess):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    1. Remove all capitalized words\n    2. Remove all punctuation\n    3. Remove all stopwords\n    4. Returns a list of the cleaned text\n    \"\"\"\n    #Remove capitalized words (movie names, actor names, etc.)\n    nocaps = [name for name in mess if name.islower()]\n    \n    #Join the characters again to form the string.\n    nocaps = ' '.join(nocaps)\n    \n    # Check characters to see if they are in punctuation\n    nopunc = [char for char in nocaps if char not in string.punctuation]\n\n    # Join the characters again to form the string.\n    nopunc = ''.join(nopunc)\n    \n    # Now just remove any stopwords\n    nostopwords = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n    \n    # Join the characters again to form the string.\n    nostopwords = ' '.join(nostopwords)\n    \n    return nostopwords","8ed3421b":"#because of our imbalanced classes (categorical labels) we can try over-sampling (making copies of the under-represented classes)\nsent_2 = trainmessages[trainmessages['Sentiment']==2]\n#we will copy class 0 11 times\nsent_0 = trainmessages[trainmessages['Sentiment']==0]\n#we will copy class 1 2 times\nsent_1 = trainmessages[trainmessages['Sentiment']==1]\n#we will copy class 3 2 times\nsent_3 = trainmessages[trainmessages['Sentiment']==3]\n#we will copy class 4 8 times\nsent_4 = trainmessages[trainmessages['Sentiment']==4]\n\n#-----------------------------------------------------\ntrainmessages = trainmessages.append([sent_0,sent_0,sent_0,sent_0,sent_0,sent_0,sent_0,sent_0,sent_0,sent_0])\ntrainmessages = trainmessages.append([sent_1,sent_1])\ntrainmessages = trainmessages.append([sent_3])\ntrainmessages = trainmessages.append([sent_4,sent_4,sent_4,sent_4,sent_4,sent_4,sent_4])","8ba46125":"#to check the amounts of each class\nsns.countplot(data=trainmessages,x='Sentiment')","8b75cded":"#we split our train.tsv into training and test data to test model performance\nX = trainmessages['Phrase']\ny = trainmessages['Sentiment']\nmsg_train,msg_test,label_train,label_test = train_test_split(X,y)","e9bd4144":"#let's try using the RandomForestClassifier model to predict\nfrom sklearn.ensemble import RandomForestClassifier\npipelineRFC = Pipeline([\n    ('bow',CountVectorizer(analyzer=text_process)),\n    ('tfidf',TfidfTransformer()),\n    ('classifier',RandomForestClassifier())\n])","3b675476":"pipelineRFC.fit(msg_train,label_train)\npreds = pipelineRFC.predict(msg_test)\nprint(classification_report(label_test,preds))","96bd9934":"#we choose the pipeline with the BEST most ACCURATE model and store the predictions in a variable\npreds = pipelineRFC.predict(testmessages['Phrase'])","67d2968d":"sub = pd.DataFrame(columns=['PhraseId','Sentiment'])\nsub['PhraseId'] = testmessages['PhraseId']\nsub['Sentiment'] = pd.Series(preds)","277f4003":"sub.head()","331bb073":"sns.countplot(data=sub,x='Sentiment')","201b882d":"#Convert DataFrame to a csv file that can be uploaded\nsubfile = 'RT Movie Review Predictions.csv'\nsub.to_csv(subfile,index=False)\nprint('Saved file: ' + subfile)","bcc1620f":"<h1>Predictions Submission<\/h1>\nLet's prepare the file we want to submit","7be7abc7":"****","14fde93e":"We will try to get the lengths of each Phrase to see if it has an effect on Sentiment rating.","04127c2b":"<center><h1>Sentiment Analysis<\/h1><\/center>\nThe dataset is comprised of tab-separated files with phrases from the Rotten Tomatoes dataset. The train\/test split has been preserved for the purposes of benchmarking, but the sentences have been shuffled from their original order. Each Sentence has been parsed into many phrases by the Stanford parser. Each phrase has a PhraseId. Each sentence has a SentenceId. Phrases that are repeated (such as short\/common words) are only included once in the data.<br><br>\n\ntrain.tsv contains the phrases and their associated sentiment labels. We have additionally provided a SentenceId so that you can track which phrases belong to a single sentence.<br>\ntest.tsv contains just phrases. You must assign a sentiment label to each phrase.<br>\nThe sentiment labels are:<br>\n\n0 - negative<br>\n1 - somewhat negative<br>\n2 - neutral<br>\n3 - somewhat positive<br>\n4 - positive","fc901bfc":"There might be a correlation between Phrase and Sentence ID and\/or Phrase Length since they want us to output PhraseID and Sentiment only...\nMaybe if we do a pairplot we might spot some patters....","132b8e40":"Let's see if we have any null values in our training data.","06e4817e":"<h1>Exploring the data<\/h1>\nLet's see what kind of information we can get from the data.","cda5bf30":"<h1>Importing Data<\/h1>\nLet's import the training data first to see what we are working with","931c8e1e":"<h1>Fitting and Predicting<\/h1>\n\nSo I have decided to focus on two simple methods:<br>\n\n1. We can simply tokenize the words without merging SentenceId's.\n2. We can merge words\/Phrases from a similar SentenceId or simply keep the longest phrase. \n\nI will only show the model\/method I used to get the highest accuracy.<br>\n\n<h2>Splitting Data into Training and Test Data<\/h2>\nTo know the true predictive power of our model, we have to split the training .tsv file into training and test data.","391122c0":"<h1>Predicting Sentiment based on Words<\/h1>\nWe can try to predict now by ***keeping the most words and their respective Sentiment scores***"}}