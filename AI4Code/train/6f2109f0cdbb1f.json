{"cell_type":{"f797d48e":"code","fd0ca996":"code","6f7b247f":"code","5b706386":"code","950c3a73":"code","7e9220d4":"code","4f18a40b":"markdown"},"source":{"f797d48e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#load data\ndftrain=pd.read_csv('\/kaggle\/input\/learn-together\/train.csv')\ndftest=pd.read_csv('\/kaggle\/input\/learn-together\/test.csv')\n\n####### DATA PREPARATION #####\n#split train data in features and labels\ny = dftrain.Cover_Type\nx = dftrain.drop(['Id','Cover_Type'], axis=1)\n\n# split test data in features and Ids\nIds = dftest.Id\nx_predict = dftest.drop('Id', axis=1)\n\n# one data set with all features\nX = pd.concat([x,x_predict],keys=[0,1])","fd0ca996":"###### FEATURE ENGINEERING #####\n#https:\/\/www.kaggle.com\/mancy7\/simple-eda\n#Soil_Type7, Soil_Type15 are non-existent in the training set, nothing to learn\n#I have problems with np.where if I do this, postponed\n#X.drop([\"Soil_Type7\", \"Soil_Type15\"], axis = 1, inplace=True)\n\n#https:\/\/www.kaggle.com\/evimarp\/top-6-roosevelt-national-forest-competition\nfrom itertools import combinations\nfrom bisect import bisect\nX['Euclidean_distance_to_hydro'] = (X.Vertical_Distance_To_Hydrology**2 \n                                         + X.Horizontal_Distance_To_Hydrology**2)**.5\n\ncols = [\n        'Horizontal_Distance_To_Roadways',\n        'Horizontal_Distance_To_Fire_Points',\n        'Horizontal_Distance_To_Hydrology',\n]\nX['distance_mean'] = X[cols].mean(axis=1)\nX['distance_sum'] = X[cols].sum(axis=1)\nX['distance_road_fire'] = X[cols[:2]].mean(axis=1)\nX['distance_hydro_fire'] = X[cols[1:]].mean(axis=1)\nX['distance_road_hydro'] = X[[cols[0], cols[2]]].mean(axis=1)\n    \nX['distance_sum_road_fire'] = X[cols[:2]].sum(axis=1)\nX['distance_sum_hydro_fire'] = X[cols[1:]].sum(axis=1)\nX['distance_sum_road_hydro'] = X[[cols[0], cols[2]]].sum(axis=1)\n    \nX['distance_dif_road_fire'] = X[cols[0]] - X[cols[1]]\nX['distance_dif_hydro_road'] = X[cols[2]] - X[cols[0]]\nX['distance_dif_hydro_fire'] = X[cols[2]] - X[cols[1]]\n    \n# Vertical distances measures\ncolv = ['Elevation', 'Vertical_Distance_To_Hydrology']\nX['Vertical_dif'] = X[colv[0]] - X[colv[1]]\nX['Vertical_sum'] = X[colv].sum(axis=1)\n    \nSHADES = ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\n    \nX['shade_noon_diff'] = X['Hillshade_9am'] - X['Hillshade_Noon']\nX['shade_3pm_diff'] = X['Hillshade_Noon'] - X['Hillshade_3pm']\nX['shade_all_diff'] = X['Hillshade_9am'] - X['Hillshade_3pm']\nX['shade_sum'] = X[SHADES].sum(axis=1)\nX['shade_mean'] = X[SHADES].mean(axis=1)\n  \nX['ElevationHydro'] = X['Elevation'] - 0.25 * X['Euclidean_distance_to_hydro']\nX['ElevationV'] = X['Elevation'] - X['Vertical_Distance_To_Hydrology']\nX['ElevationH'] = X['Elevation'] - 0.19 * X['Horizontal_Distance_To_Hydrology']\n\nX['Elevation2'] = X['Elevation']**2\nX['ElevationLog'] = np.log1p(X['Elevation'])\n\nX['Aspect_cos'] = np.cos(np.radians(X.Aspect))\nX['Aspect_sin'] = np.sin(np.radians(X.Aspect))\n#df['Slope_sin'] = np.sin(np.radians(df.Slope))\nX['Aspectcos_Slope'] = X.Slope * X.Aspect_cos\n#df['Aspectsin_Slope'] = df.Slope * df.Aspect_sin\n    \ncardinals = [i for i in range(45, 361, 90)]\npoints = ['N', 'E', 'S', 'W']\nX['Cardinal'] = X.Aspect.apply(lambda x: points[bisect(cardinals, x) % 4])\nd = {'N': 0, 'E': 1, 'S': 0, 'W':-1}\nX['Cardinal'] = X.Cardinal.apply(lambda x: d[x])\n\n#https:\/\/www.kaggle.com\/jakelj\/basic-ensemble-model\nX['Avg_shade'] = ((X['Hillshade_9am'] + X['Hillshade_Noon'] + X['Hillshade_3pm']) \/ 3)\nX['Morn_noon_int'] = ((X['Hillshade_9am'] + X['Hillshade_Noon']) \/ 2)\nX['noon_eve_int'] = ((X['Hillshade_3pm'] + X['Hillshade_Noon']) \/ 2)\n\n#adding features based on https:\/\/douglas-fraser.com\/forest_cover_management.pdf pages 21,22\n#note: not all climatic and geologic codes have a soil type\ncolumns=['Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6']\nX['Climatic2'] = np.select([X[columns].sum(1).gt(0)], [1])\ncolumns=['Soil_Type7', 'Soil_Type8']\nX['Climatic3'] = np.select([X[columns].sum(1).gt(0)], [1])\ncolumns=['Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13']\nX['Climatic4'] = np.select([X[columns].sum(1).gt(0)], [1])\ncolumns=['Soil_Type14', 'Soil_Type15']\nX['Climatic5'] = np.select([X[columns].sum(1).gt(0)], [1])\ncolumns=['Soil_Type16', 'Soil_Type17', 'Soil_Type18']\nX['Climatic6'] = np.select([X[columns].sum(1).gt(0)], [1])\ncolumns=['Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n    'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30',\n    'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34']\nX['Climatic7'] = np.select([X[columns].sum(1).gt(0)], [1])\ncolumns=['Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\nX['Climatic8'] = np.select([X[columns].sum(1).gt(0)], [1])\n\ncolumns=['Soil_Type14', 'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type19', 'Soil_Type20',\n    'Soil_Type21']\nX['Geologic1'] = np.select([X[columns].sum(1).gt(0)], [1])\ncolumns=['Soil_Type9', 'Soil_Type22', 'Soil_Type23']\nX['Geologic2'] = np.select([X[columns].sum(1).gt(0)], [1])\ncolumns=['Soil_Type7', 'Soil_Type8']\nX['Geologic5'] = np.select([X[columns].sum(1).gt(0)], [1])\ncolumns=['Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6',\n    'Soil_Type10', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type18', 'Soil_Type24',\n    'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30',\n    'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', \n    'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\nX['Geologic7'] = np.select([X[columns].sum(1).gt(0)], [1])\n\n#Reversing One-Hot-Encoding to Categorical attributes, several articles recommend it for decision tree algorithms\n#Doing it for Soil_Type, Wilderness_Area, Geologic and Climatic\nX['Soil_Type']=np.where(X.loc[:, 'Soil_Type1':'Soil_Type40'])[1] +1\nX.drop(X.loc[:,'Soil_Type1':'Soil_Type40'].columns, axis=1, inplace=True)\n\nX['Wilderness_Area']=np.where(X.loc[:, 'Wilderness_Area1':'Wilderness_Area4'])[1] +1\nX.drop(X.loc[:,'Wilderness_Area1':'Wilderness_Area4'].columns, axis=1, inplace=True)\n\nX['Climatic']=np.where(X.loc[:, 'Climatic2':'Climatic8'])[1] +1\nX.drop(X.loc[:,'Climatic2':'Climatic8'].columns, axis=1, inplace=True)\n\nX['Geologic']=np.where(X.loc[:, 'Geologic1':'Geologic7'])[1] +1\nX.drop(X.loc[:,'Geologic1':'Geologic7'].columns, axis=1, inplace=True)\n\nfrom sklearn.preprocessing import StandardScaler\nStandardScaler(copy=False).fit_transform(X)\n\n# Adding Gaussian Mixture features to perform some unsupervised learning hints from the full data\n#https:\/\/www.kaggle.com\/arateris\/2-layer-k-fold-learning-forest-cover \n#https:\/\/www.kaggle.com\/stevegreenau\/stacking-multiple-classifiers-clustering\nfrom sklearn.mixture import GaussianMixture\nX['GM'] = GaussianMixture(n_components=15).fit_predict(X)\n\n#https:\/\/www.kaggle.com\/arateris\/2-layer-k-fold-learning-forest-cover \n# Add PCA features\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=0.99).fit(X)\ntrans = pca.transform(X)\n\nfor i in range(trans.shape[1]):\n    col_name= 'pca'+str(i+1)\n    X[col_name] = trans[:,i]\n\n#https:\/\/www.kaggle.com\/kwabenantim\/forest-cover-stacking-multiple-classifiers\n# Scale and bin features\nfrom sklearn.preprocessing import MinMaxScaler\nMinMaxScaler((0, 100),copy=False).fit_transform(X)\n#X = np.floor(X).astype('int8')\n\nprint(\"Completed feature engineering!\")","6f7b247f":"#break it down again in train and test\nx,x_predict = X.xs(0),X.xs(1)","5b706386":"###### THIS IS THE ENSEMBLE MODEL SECTION ######\n#https:\/\/www.kaggle.com\/kwabenantim\/forest-cover-stacking-multiple-classifiers\nimport random\nrandomstate = 1\nrandom.seed(randomstate)\nnp.random.seed(randomstate)\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nab_clf = AdaBoostClassifier(n_estimators=200,\n                            base_estimator=DecisionTreeClassifier(\n                                min_samples_leaf=2,\n                                random_state=randomstate),\n                            random_state=randomstate)\n\n#max_features = min(30, x.columns.size)\nmax_features = 30\nfrom sklearn.ensemble import ExtraTreesClassifier\net_clf = ExtraTreesClassifier(n_estimators=300,\n                              min_samples_leaf=2,\n                              min_samples_split=2,\n                              max_depth=50,\n                              max_features=max_features,\n                              random_state=randomstate,\n                              n_jobs=1)\n\nfrom lightgbm import LGBMClassifier\nlg_clf = LGBMClassifier(n_estimators=300,\n                        num_leaves=128,\n                        verbose=-1,\n                        random_state=randomstate,\n                        n_jobs=1)\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf_clf = RandomForestClassifier(n_estimators=300,\n                                random_state=randomstate,\n                                n_jobs=1)\n\n#Added a KNN classifier to the ensemble\n#https:\/\/www.kaggle.com\/edumunozsala\/feature-eng-and-a-simple-stacked-model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_clf = KNeighborsClassifier(n_neighbors=y.nunique(), n_jobs=1)\n\n#added several more classifiers at once\n#https:\/\/www.kaggle.com\/edumunozsala\/feature-eng-and-a-simple-stacked-model\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nbag_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(criterion = 'entropy', max_depth=None, \n                                                    min_samples_split=2, min_samples_leaf=1,max_leaf_nodes=None,\n                                                    max_features='auto',\n                                                    random_state = randomstate),\n                    n_estimators=500,max_features=0.75, max_samples=1.0, random_state=randomstate,n_jobs=1,verbose=0)\n\nfrom sklearn.linear_model import LogisticRegression\nlr_clf = LogisticRegression(max_iter=1000,\n                       n_jobs=1,\n                       solver= 'lbfgs',\n                       multi_class = 'multinomial',\n                       random_state=randomstate,\n                       verbose=0)\n\n#https:\/\/www.kaggle.com\/bustam\/6-models-for-forest-classification\nfrom catboost import CatBoostClassifier\ncat_clf = CatBoostClassifier(n_estimators =300, \n                        eval_metric='Accuracy',\n                        metric_period=200,\n                        max_depth = None, \n                        random_state=randomstate,\n                        verbose=0)\n\n#https:\/\/www.kaggle.com\/jakelj\/basic-ensemble-model\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nhbc_clf = HistGradientBoostingClassifier(max_iter = 500, max_depth =25, random_state = randomstate)\n\nensemble = [('AdaBoostClassifier', ab_clf),\n            ('ExtraTreesClassifier', et_clf),\n            ('LGBMClassifier', lg_clf),\n            #('KNNClassifier', knn_clf),\n            ('BaggingClassifier', bag_clf),\n            #('LogRegressionClassifier', lr_clf),\n            #('CatBoostClassifier', cat_clf),\n            #('HBCClassifier', hbc_clf),\n            ('RandomForestClassifier', rf_clf)\n]\n\n#Cross-validating classifiers\nfrom sklearn.model_selection import cross_val_score\nfor label, clf in ensemble:\n    score = cross_val_score(clf, x, y,\n                            cv=10,\n                            scoring='accuracy',\n                            verbose=0,\n                            n_jobs=-1)\n    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" \n        % (score.mean(), score.std(), label))\n\n# Fitting stack\nfrom mlxtend.classifier import StackingCVClassifier\nstack = StackingCVClassifier(classifiers=[ab_clf, et_clf, lg_clf, \n                                          bag_clf,\n                                          rf_clf],\n                             meta_classifier=rf_clf,\n                             cv=10,\n                             stratify=True,\n                             shuffle=True,\n                             use_probas=True,\n                             use_features_in_secondary=True,\n                             verbose=0,\n                             random_state=randomstate)\nstack = stack.fit(x, y)\n\nprint(\"Completed modeling!\")","950c3a73":"#make predictions\ny_predict = stack.predict(x_predict)\ny_predict = pd.Series(y_predict, index=x_predict.index, dtype=y.dtype)\n\nprint(\"Completed predictions!\")","7e9220d4":"# Save predictions to a file for submission\noutput = pd.DataFrame({'Id': Ids,\n                       'Cover_Type': y_predict})\noutput.to_csv('submission.csv', index=False)\n\n#create a link to download the file    \nfrom IPython.display import FileLink\nFileLink(r'submission.csv')","4f18a40b":"I started this competition investigating neural networks with this kernel https:\/\/www.kaggle.com\/mulargui\/keras-nn\nNow switching to using ensembles in this new kernel. As of today V6 is the most performant version.\nYou can find all my notes and versions at https:\/\/github.com\/mulargui\/kaggle-Classify-forest-types"}}