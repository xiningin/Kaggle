{"cell_type":{"fb825b74":"code","bca3c872":"code","75c3de1c":"code","d583cb0d":"code","af7d45e3":"code","1a89fc61":"code","2a551ed3":"code","15c6ab51":"code","1fea35c1":"code","db826bd0":"code","fffb24b3":"code","0497026c":"code","e192926e":"code","50d5a671":"code","a2985c51":"code","3ba020af":"code","0d12da8e":"code","ac75e067":"code","c68b4f8b":"code","632bc186":"code","61057f3e":"code","fa9342bb":"code","77db4c76":"code","6afb6401":"code","97d0808a":"code","c6a1191f":"code","130aad7b":"code","11c8c399":"code","eb6e9c0b":"markdown","1548bd7e":"markdown","b4de1624":"markdown","74b75894":"markdown","fc160ec9":"markdown","cbe4a47e":"markdown","97f97426":"markdown","f404235d":"markdown","5a3770c1":"markdown","a0f207b7":"markdown","46694d38":"markdown","59b774e5":"markdown","2163ae1e":"markdown","75186a2a":"markdown","11182b6a":"markdown","5db1c5de":"markdown","54d09a19":"markdown","00e73042":"markdown","0e84d3bc":"markdown","f1242958":"markdown","1646de7e":"markdown","21e2b747":"markdown","b7d0d7fa":"markdown","fefa3338":"markdown","27db006b":"markdown","23893035":"markdown","043815fb":"markdown","f3cc4636":"markdown","51e4f47e":"markdown","ac03ee91":"markdown","a18c9d76":"markdown","65d7a61e":"markdown","91b36caa":"markdown","48389f64":"markdown"},"source":{"fb825b74":"import pandas as pd\nimport numpy as np\n\nnp.random.seed(2021030601)\n\ndef get_combined_data():\n    stellar_data = pd.read_csv('\/kaggle\/input\/253k-apass-dr9-stars-w-gaia-dr2-photometry\/apassdr9-gaiadr2-photometry-500pc.csv', dtype={'source_id': str})\n    dasch_data = pd.read_csv('\/kaggle\/input\/brightness-trends-from-73k-dasch-stars\/summarized-dasch-trends-wor.csv')\n    joined_data = pd.merge(stellar_data, dasch_data, how='inner', on='apassdr9_id')\n    return joined_data\n\ndata = get_combined_data()\nlen(data)","bca3c872":"data['slope'].describe()","75c3de1c":"np.sqrt(np.mean(data['slope_stderr'] ** 2))","d583cb0d":"def giant_separation_y(x):\n    return x * 40.0 - 25\n\n\ndef transform_rm_giants(data_frame):\n    new_frame = data_frame.copy()\n    distance = 1000.0 \/ new_frame['parallax']\n    new_frame['color_index'] = new_frame['phot_bp_mean_mag'] - new_frame['phot_rp_mean_mag']\n    new_frame['abs_mag_ne'] = new_frame['phot_g_mean_mag'] - 5 * (np.log10(distance) - 1)\n    new_frame = new_frame[new_frame['abs_mag_ne'] ** 2 >= giant_separation_y(new_frame['color_index'])]\n    new_frame.reset_index(inplace=True, drop=True)\n    return new_frame\n\n\ndata = transform_rm_giants(data)","af7d45e3":"data['slope'].describe()","1a89fc61":"%%html\n<style>\n.output_wrapper .output {\n  overflow-y: visible;\n  height: fit-content;\n}\n<\/style>","2a551ed3":"import numpy as np\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport warnings\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\npy.init_notebook_mode(connected=False)\n\ndef line_plot(bins_x, bins_y, lower_y=None, upper_y=None, x_range=None, x_label='', y_range=None, y_label='', title=''):\n    trace1 = go.Scatter(\n        name='bin mean',\n        x=bins_x,\n        y=bins_y,\n        mode='lines',\n        line=dict(width=4),\n        marker=dict(\n            size=3,\n        ),\n        showlegend=False\n    )\n    scatter_data = [trace1]\n    if lower_y is not None and upper_y is not None:\n        scatter_data.append(go.Scatter(\n            name='upper',\n            x=bins_x,\n            y=upper_y,\n            marker=dict(color=\"#444\"),\n            line=dict(width=0),\n            mode='lines',\n            showlegend=False\n        ))\n        scatter_data.append(go.Scatter(\n            name='lower',\n            x=bins_x,\n            y=lower_y,\n            marker=dict(color=\"#444\"),\n            line=dict(width=0),\n            mode='lines',\n            fillcolor='rgba(68, 68, 68, 0.1)',\n            fill='tonexty',\n            showlegend=False\n        ))\n        \n    \n    layout = go.Layout(\n        title=title,\n        xaxis=dict(\n            title=x_label,\n            range=x_range,\n        ),\n        yaxis=dict(\n            title=y_label,\n            range=y_range,\n        ),\n    )\n    fig = go.Figure(data=scatter_data, layout=layout)\n    py.iplot(fig)\n    \n\ndef plot_binned(data_frame, x_column, y_column, step=0.1, x_range=None, x_label='', y_range=None, y_label='', title=''):\n    x = data_frame[x_column].astype(float)\n    factor = 1.0 \/ step\n    bin_x = np.round(x * factor) \/ factor\n    work_frame = data_frame.assign(__bin_x = bin_x)\n    groups = work_frame.groupby('__bin_x')\n    bins_x = []\n    bins_y = []\n    lower_y = []\n    upper_y = []\n    for group in groups:\n        key = group[0]\n        group_frame = group[1]\n        len_gf = len(group_frame)\n        if len_gf >= 3:\n            values = group_frame[y_column].values\n            mean_value = np.mean(values)\n            std_value = np.std(values) \/ np.sqrt(len_gf)\n            upper_bound = mean_value + std_value * 1.96\n            lower_bound = mean_value - std_value * 1.96\n            bins_x.append(key)\n            bins_y.append(mean_value)\n            lower_y.append(lower_bound)\n            upper_y.append(upper_bound)\n    line_plot(bins_x, bins_y, upper_y=upper_y, lower_y=lower_y, x_range=x_range, \n              x_label=x_label, y_range=y_range, y_label=y_label, title=title)\n    \n    \ndata['pm'] = np.sqrt(data['pmra'] ** 2 + data['pmdec'] ** 2)\nplot_binned(data, 'pm', 'slope', \n            x_range=None, x_label='Proper motion (mas \/ year)', \n            y_range=None, y_label='Nominal slope (mags \/ century)',\n            title='Proper motion (binned) vs. nominal DASCH slope',\n            step=10)","15c6ab51":"data['abs_slope'] = np.abs(data['slope'])\nplot_binned(data, 'pm', 'abs_slope', \n            x_range=None, x_label='Proper motion (mas \/ year)', \n            y_range=None, y_label='Absolute slope (mags \/ century)',\n            title='Proper motion (binned) vs. absolute DASCH slope',\n            step=20)","1fea35c1":"data = data[data['pm'] < 240]\nlen(data)","db826bd0":"data['distance'] = 1000.0 \/ data['parallax']\nplot_binned(data, 'distance', 'slope', \n            x_range=None, x_label='Distance (parsecs)', \n            y_range=None, y_label='Nominal slope (mags \/ century)',\n            title='Distance (binned) vs. nominal DASCH slope', \n            step=10)","fffb24b3":"plot_binned(data, 'mean_mag', 'slope', \n            x_range=None, x_label='Mean magnitude', \n            y_range=[-0.2,0.2], y_label='Nominal DASCH slope (mags \/ century)',\n            title='Magnitude (binned) vs. nominal DASCH slope', \n            step=0.1)","0497026c":"plot_binned(data, 'color_index', 'slope', \n            x_range=None, x_label='BP-RP color index', \n            y_range=[-0.2, 0.2], y_label='Nominal slope (mags \/ century)',\n            title='BP-RP color index (binned) vs. nominal DASCH slope', \n            step=0.1)","e192926e":"import types\nimport scipy.stats as stats\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler \n\nimport inspect\n\npd_concat_argspec = inspect.getfullargspec(pd.concat)\npd_concat_has_sort = 'sort' in pd_concat_argspec.args\n\ndef pd_concat(frames):\n    # Due to Pandas versioning issue\n    new_frame = pd.concat(frames, sort=False) if pd_concat_has_sort else pd.concat(frames)\n    new_frame.reset_index(inplace=True, drop=True)\n    return new_frame\n    \n    \ndef plt_hist(x, bins=30):\n    # plt.hist() can be very slow.\n    histo, edges = np.histogram(x, bins=bins)\n    plt.bar(0.5 * edges[1:] + 0.5 * edges[:-1], histo, width=(edges[-1] - edges[0])\/(len(edges) + 1))\n    \n    \ndef train_fold(data_frame: pd.DataFrame, train_idx, test_idx, label_extractor, var_extractor, \n               trainer_factory, max_n_training: int, trim_fraction: float, scale: bool, id_column: str):\n    train_frame = data_frame.iloc[train_idx]\n    if trim_fraction is not None:\n        helper_labels = label_extractor(train_frame) if isinstance(label_extractor, types.FunctionType) else train_frame[label_extractor] \n        train_label_ordering = np.argsort(helper_labels)\n        orig_train_len = len(train_label_ordering)\n        head_tail_len_to_trim = int(round(orig_train_len * trim_fraction * 0.5))\n        assert head_tail_len_to_trim > 0\n        trimmed_ordering = train_label_ordering[head_tail_len_to_trim:-head_tail_len_to_trim]\n        train_frame = train_frame.iloc[trimmed_ordering]\n    if max_n_training is not None:\n        train_frame = train_frame.sample(max_n_training)\n    train_labels = label_extractor(train_frame) if isinstance(label_extractor, types.FunctionType) else train_frame[label_extractor]\n    test_frame = data_frame.iloc[test_idx]\n    train_vars = var_extractor(train_frame)\n    test_vars = var_extractor(test_frame)\n    scaler = None\n    if scale:\n        scaler = StandardScaler()  \n        scaler.fit(train_vars)\n        train_vars = scaler.transform(train_vars)  \n        test_vars = scaler.transform(test_vars) \n    trainer = trainer_factory()\n    fold_model = trainer.fit(train_vars, train_labels)\n    test_responses = fold_model.predict(test_vars)\n    test_id = test_frame[id_column]\n    assert len(test_id) == len(test_responses)\n    return scaler, fold_model, pd.DataFrame({id_column: test_id, 'response': test_responses}),\n\n\ndef get_trained_transform(response_map: dict, id_column: str, response_column: str, default_model_list: list):\n    def _transform(_frame):\n        _response_id_set = set(response_map)\n        _in_trained_set = _frame[id_column].astype(str).isin(_response_id_set)\n        _trained_frame = _frame[_in_trained_set].copy()\n        _trained_frame.reset_index(inplace=True, drop=True)\n        if len(_trained_frame) > 0:\n            _trained_id = _trained_frame[id_column]\n            _tn = len(_trained_id)\n            _response = pd.Series([None] * _tn).astype(float)\n            for i in range(_tn):\n                _response[i] = response_map[str(_trained_id[i])]\n            _trained_frame[response_column] = _response\n        _remain_frame = _frame[~_in_trained_set].copy()\n        _remain_frame.reset_index(inplace=True, drop=True)\n        if len(_remain_frame) > 0:\n            _unscaled_vars = var_extractor(_remain_frame)\n            _response_sum = pd.Series([0] * len(_remain_frame)).astype(float)\n            for _model_tuple in default_model_list:\n                _scaler, _model = _model_tuple\n                _vars = _unscaled_vars if _scaler is None else _scaler.transform(_unscaled_vars)\n                _response = _model.predict(_vars)\n                _response_sum += _response\n            _remain_frame[response_column] = _response_sum \/ len(default_model_list)\n        _frames_list = [_trained_frame, _remain_frame]\n        return pd_concat(_frames_list)\n\n    return _transform    \n\n    \ndef get_cv_model_transform(data_frame, label_extractor, var_extractor, trainer_factory, \n                           response_column='response', id_column='source_id', n_runs=2, \n                           n_splits=2, max_n_training=None, scale=False, trim_fraction=None):\n    '''\n    Creates a transform function that results from averaging out multiple runs of k-fold cross-validation.\n    Training is done with an arbitrary machine learning algorithm. The transform function takes a data \n    frame and adds a response column to it. Responses are out-of-sample.\n    '''\n    default_model_list = []\n    sum_series = pd.Series([0] * len(data_frame)).astype(float)\n    for r in range(n_runs):\n        shuffled_frame = data_frame.sample(frac=1)\n        shuffled_frame.reset_index(inplace=True, drop=True)\n        response_frame = pd.DataFrame(columns=[id_column, 'response'])\n        kf = KFold(n_splits=n_splits)\n        first_fold = True\n        for train_idx, test_idx in kf.split(shuffled_frame):\n            scaler, fold_model, fold_frame = train_fold(shuffled_frame, train_idx, test_idx, \n                                                        label_extractor, var_extractor,\n                                                        trainer_factory,\n                                                        max_n_training=max_n_training,\n                                                        trim_fraction=trim_fraction,\n                                                        scale=scale, id_column=id_column)\n            response_frame = pd_concat([response_frame, fold_frame])\n            if first_fold:\n                first_fold = False\n                default_model_list.append((scaler, fold_model,))\n        # Sort results by id so we can average multiple runs\n        response_frame.sort_values(id_column, inplace=True)\n        response_frame.reset_index(inplace=True, drop=True)\n        assert len(response_frame) == len(data_frame), 'len(response_frame)=%d' % len(response_frame)\n        sum_series += response_frame['response']\n    cv_response = sum_series \/ n_runs\n    assert len(cv_response) == len(data_frame)\n    assert len(default_model_list) == n_runs\n    response_map = dict()\n    sorted_id = np.sort(data_frame[id_column].values) \n    for i in range(len(cv_response)):\n        response_map[str(sorted_id[i])] = cv_response[i]\n    return get_trained_transform(response_map, id_column, response_column, default_model_list)\n\n\ndef print_evaluation(data_frame, label_column, response_column):\n    '''\n    This function evaluates the results of a regression model. \n    It shows the RMSE and the Pearson's correlation.\n    '''\n    response = response_column(data_frame) if isinstance(response_column, types.FunctionType) else data_frame[response_column]\n    label = label_column(data_frame) if isinstance(label_column, types.FunctionType) else data_frame[label_column]\n    residual = label - response\n    rmse = np.sqrt(sum(residual ** 2) \/ len(data_frame))\n    correl = stats.pearsonr(response, label)[0]\n    print('RMSE: %.4f | Correlation: %.4f' % (rmse, correl,), flush=True)\n\n    \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\n\ndef extract_vars_10(data_frame):\n    bp_mag = data_frame['phot_bp_mean_mag']\n    rp_mag = data_frame['phot_rp_mean_mag']\n    color = data_frame['color_index']\n    pm = data_frame['pm']\n    # This is the variable space of the correction model\n    return np.transpose([\n        pm,\n        bp_mag,\n        bp_mag ** 2,\n        rp_mag,\n        rp_mag ** 2,\n        color,\n        color ** 2,\n        color ** 3,\n    ])\n\n\ndef get_trainer_10():\n    # Linear regression with regularization\n    return Ridge(alpha=0.01)\n\n\ntransform_10 = get_cv_model_transform(data, 'slope', extract_vars_10, get_trainer_10, \n                                      n_runs=2, n_splits=5, response_column='slope_model_10', \n                                      scale=False)\ndata = transform_10(data)\nprint_evaluation(data, 'slope', 'slope_model_10')\ndata['corrected_slope_10'] = data['slope'] - data['slope_model_10']","50d5a671":"plot_binned(data, 'pm', 'corrected_slope_10', \n            x_range=None, x_label='Proper motion (mas \/ year)', \n            y_range=None, y_label='Photometry-corrected slope (mags \/ century)',\n            title='Proper motion (binned) vs. corrected slope', step=10)","a2985c51":"plot_binned(data, 'mean_mag', 'corrected_slope_10', \n            x_range=None, x_label='Mean DASCH magnitude', \n            y_range=None, y_label='Photometry-corrected slope (mags \/ century)',\n            title='Mean magnitude (binned) vs. corrected slope', step=0.1)","3ba020af":"plot_binned(data, 'distance', 'corrected_slope_10', \n            x_range=None, x_label='Distance (parsecs)', \n            y_range=None, y_label='Photometry-corrected slope (mags \/ century)',\n            title=None, step=10)","0d12da8e":"plot_binned(data, 'color_index', 'corrected_slope_10', \n            x_range=None, x_label='BP-RP color index', \n            y_range=None, y_label='Photometry-corrected slope (mags \/ century)',\n            title='Color index (binned) vs. corrected slope', step=0.1)","ac75e067":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\ndef extract_pos_vars(data_frame):\n    return data_frame[['ra', 'dec']]\n\n\ndef get_pos_trainer():\n    return RandomForestRegressor(n_estimators=100, max_depth=15, min_samples_split=40, random_state=np.random.randint(1,10000))\n\n\ntransform_pos = get_cv_model_transform(data, 'corrected_slope_10', extract_pos_vars, get_pos_trainer, \n        n_runs=2, n_splits=3, response_column='slope_model_pos', scale=False)\ndata = transform_pos(data)\ntransform_pos = None # Discard\nprint_evaluation(data, 'corrected_slope_10', 'slope_model_pos')","c68b4f8b":"def scatter_plot(bins_x, bins_y, colors, x_range=None, x_label='', y_range=None, y_label='', title='', colorbar_title=''):\n    trace1 = go.Scatter(\n        x=bins_x,\n        y=bins_y,\n        mode='markers',\n        marker=dict(\n            size=5,\n            color=colors,\n            colorbar=dict(\n                title=colorbar_title\n            ),\n        ),\n        showlegend=False\n    )\n    scatter_data = [trace1]\n    layout = go.Layout(\n        title=title,\n        xaxis=dict(\n            title=x_label,\n            range=x_range,\n        ),\n        yaxis=dict(\n            title=y_label,\n            range=y_range,\n        ),\n    )\n    fig = go.Figure(data=scatter_data, layout=layout)\n    py.iplot(fig)\n\n    \nsample_data = data.sample(n=10000)\nscatter_plot(sample_data['ra'], sample_data['dec'], sample_data['slope_model_pos'],\n             x_label='Right ascension (degrees)', y_label='Declination (degrees)', colorbar_title='Modeled slope',\n             title='Responses of slope model<br>by position in the sky')","632bc186":"from sklearn.ensemble import RandomForestRegressor\n\ndef extract_error_vars(data_frame):\n    # The variable space of the error model\n    return data_frame[['slope_stderr', 'slope_stderr_pre_1960', 'l', 'b', 'ra', 'dec', 'mean_limiting_mag']]\n\n\ndef get_error_trainer():\n    return RandomForestRegressor(n_estimators=100, max_depth=12, min_samples_split=10, random_state=np.random.randint(1,10000))\n\n\ntransform_em_bias = get_cv_model_transform(data, 'corrected_slope_10', extract_error_vars, get_error_trainer, \n        n_runs=2, n_splits=4, response_column='expected_em_mc', scale=False,\n        trim_fraction=None)\ndata = transform_em_bias(data)\ntransform_em_bias = None # Discard\nprint_evaluation(data, 'corrected_slope_10', 'expected_em_mc')","61057f3e":"def get_squared_mag_change(data_frame):\n    return (data_frame['corrected_slope_10'] - data_frame['expected_em_mc']) ** 2\n\n\ntransform_expected_mc_sq = get_cv_model_transform(data, get_squared_mag_change, extract_error_vars, get_error_trainer, \n        n_runs=2, n_splits=4, response_column='expected_mc_sq', scale=False,\n        trim_fraction=None)\ndata = transform_expected_mc_sq(data)\ntransform_expected_mc_sq = None # Discard\nprint_evaluation(data, get_squared_mag_change, 'expected_mc_sq')","fa9342bb":"data = data.assign(corrected_slope_stderr = np.sqrt(data['expected_mc_sq'].astype(float)))\ndata = data.assign(corrected_slope = data['corrected_slope_10'] - data['expected_em_mc'])\ndata = data.assign(slope_anomaly = data['corrected_slope'] \/ data['corrected_slope_stderr'])","77db4c76":"np.std(data['slope_anomaly'])","6afb6401":"def multi_scatter_plot(x_matrix, y_matrix, colors, names, texts, x_label='', y_label='', \n                       title='', x_range=None, y_range=None, showlegend=False,\n                       y_autorange=None):\n    scatter_data = []\n    for i in range(len(x_matrix)):\n        x = x_matrix[i]\n        y = y_matrix[i]\n        color = colors[i]\n        name = names[i]\n        text = texts[i]\n        trace = go.Scatter(\n            name=name,\n            x=x,\n            y=y,\n            text=text,\n            mode='markers',\n            marker=dict(\n                size=4,\n                color=color,\n            ),\n            showlegend=showlegend\n        )\n        scatter_data.append(trace)\n    layout = go.Layout(\n        title=title,\n        xaxis=dict(\n            title=x_label,\n            range=x_range,\n        ),\n        yaxis=dict(\n            title=y_label,\n            range=y_range,\n            autorange=y_autorange,\n        ),\n    )\n    fig = go.Figure(data=scatter_data, layout=layout)\n    py.iplot(fig)\n    \n\ndata['abs_mag_ne'] = data['phot_g_mean_mag'] - 5 * (np.log10(data['distance']) - 1)\nrandom_stars = data[data['slope_anomaly'].between(-1,1)].sample(n=2000)\nmulti_scatter_plot([random_stars['color_index']],\n                   [random_stars['abs_mag_ne']],\n                   ['#B0B0B0'],\n                   ['random'],\n                   [random_stars['apassdr9_id']],\n                   x_range=[0, 2], y_range=[9, 0],\n                   x_label='BP-RP color index', y_label='Absolute magnitude',\n                   title='H-R Diagram | Ordinary stars')","97d0808a":"high_outliers = data.sort_values('slope_anomaly', ascending=False).head(1000)\nlow_outliers = data.sort_values('slope_anomaly', ascending=True).head(1000)\nmulti_scatter_plot([high_outliers['color_index'], low_outliers['color_index']],\n                   [high_outliers['abs_mag_ne'], low_outliers['abs_mag_ne']],\n                   ['blue', 'red'],\n                   ['dimming', 'brightening'],\n                   [high_outliers['apassdr9_id'], low_outliers['apassdr9_id']],\n                   x_range=[0,2], y_range=[9, 0],\n                   x_label='BP-RP color index', y_label='Absolute magnitude',\n                   showlegend=True,\n                   title='H-R Diagram | Anomalous DASCH stars')","c6a1191f":"def show_slope(apassdr9_id, corrected=False):\n    row = data[data['apassdr9_id'] == apassdr9_id]\n    slope = row['corrected_slope' if corrected else 'slope']\n    stderr = row['corrected_slope_stderr' if corrected else 'slope_stderr']\n    confint = stderr * 1.96\n    print('Slope \u2248 %.3f \u00b1 %.3f magnitudes \/ century' % (slope, confint,))  \n\n\nKIC_APASS_ID = 14246098\nshow_slope(KIC_APASS_ID)","130aad7b":"show_slope(KIC_APASS_ID, corrected=True)","11c8c399":"out_frame = pd.DataFrame({\n    'apassdr9_id': data['apassdr9_id'],\n    'nominal_slope': data['slope'],\n    'nominal_slope_stderr': data['slope_stderr'],\n    'corrected_slope': data['corrected_slope'],\n    'corrected_slope_stderr': data['corrected_slope_stderr'],\n    'slope_anomaly': data['slope_anomaly'],\n})\nout_frame.round(4).to_csv('corrected-dasch-slopes.csv', index=False)","eb6e9c0b":"## Output file\nWe will write a file called *corrected-dasch-slopes.csv* containing the nominal DASCH slopes as well as the corrected ones. The file will also contain the *slope_anomaly* metric, with standard deviation of ~1.0.","1548bd7e":"This is now the effect of proper motion on the corrected slope:","b4de1624":"The proper motion bias is likely linear, and there are magnitude and color biases that are non-linear. There's no reason to think distance would play a direct role that is not accounted for by color and magnitude biases.\n\nAfter some testing, a model which depends linearly on proper motion, quadratically on magnitude and cubically on color, works well enough. We're using the BP and RP magnitudes from Gaia DR2 as model variables. They seem to produce a better correction than the mean DASCH magnitude. \n\nThe model is trained and evaluated below:","74b75894":"It seems there's a problem after about 300 mas\/year, and arguably even after 240 mas\/year. Conservatively, let's remove stars with a proper motion of 240 mas\/year or more. We're left with these many stars:","fc160ec9":"Now we'll get 1000 stars from each end of the *slope_anomaly* distribution and plot an H-R diagram with them.","cbe4a47e":"Next, let's look at the effect of distance:","97f97426":"We will add some columns to the dataset:\n* *corrected_slope_std_error*: The square root of the modeled square error (similar to an RMSE or standard deviation.)\n* *corrected_slope*: The new corrected slope that accounts for the bias in the error variable space.\n* *slope_anomaly*: The corrected slope divided by the modeled standard error.","f404235d":"The standard deviation of slopes is over 5 times greater than expected if ground-truth slopes were always zero.","5a3770c1":"This bias model improves our overall precision.\n\nNow we calculate a new model residual, and then we model its square.","a0f207b7":"## Acknowledgments\nThis work has made use of data from the European Space Agency (ESA) mission Gaia (https:\/\/www.cosmos.esa.int\/gaia), processed by the Gaia Data Processing and Analysis Consortium (DPAC, https:\/\/www.cosmos.esa.int\/web\/gaia\/dpac\/consortium). Funding for the DPAC has been provided by national institutions, in particular the institutions participating in the Gaia Multilateral Agreement.\n\nThis work has made use of data from the DASCH project (https:\/\/projects.iq.harvard.edu\/dasch). DASCH receives partial support from NSF grants AST-0407380, AST-0909073, and AST-1313370.\n\nThis research was made possible through the use of the AAVSO Photometric All-Sky Survey (APASS), funded by the Robert Martin Ayers Sciences Fund and NSF AST-1412587.","46694d38":"## H-R Diagrams\nLet's take a look at how a random set of 2000 ordinary stars distribute in an H-R diagram. Recall that we've removed giants.","59b774e5":"## Proper motion, color, magnitude and distance\nAs we will see, DASCH slopes have systematic dependencies on proper motion, color, magnitude and distance. The dependencies confound one another.\n\nThis is how the nominal slope changes with proper motion:","2163ae1e":"This color map shows the out-of-sample responses of the Random Forest by position in the sky:","75186a2a":"## KIC 8462852\nControversially, Boyajian's star has been claimed to have a secular dimming trend in DASCH (Schaefer 2016). Nominally, that is true:","11182b6a":"Hippke et al. 2016. _A statistical analysis of the accuracy of the digitized magnitudes of photometric plates on the time scale of decades with an application to the century-long light curve of KIC 8462852_. arXiv:1601.07314\n\nLund et al. 2016. _The Stability of F-star Brightness on Century Timescales_. arXiv:1605.02760v1\n\nSchaefer, 2016. _KIC 8462852 Faded at an Average Rate of 0.165+-0.013 Magnitudes Per Century From 1890 To 1989_. arXiv:1601.03256","5db1c5de":"However, as noted, DASCH slopes have systematics and broad variance. Once we expand the error estimates to consider how slopes vary in practice, we fail to reject the null hypothesis with the data at hand:","54d09a19":"These seem to be systematic distortions requiring correction.","00e73042":"In general, they seem to be ordinary stars, though a handful stand out. For reference, we'll note 4 faders that are dim:\n\n* TYC 3087-2217-1 (Gaia DR2 1343800434040522368)\n* TYC 4199-888-1 (Gaia DR2 1436023144347420032)\n* TYC 4425-58-1 (Gaia DR2 1702711678233282816)\n* TYC 4188-91-1 (Gaia DR2 1644428460926230784)\n\nThe last 3 appear to be fairly close neighbors in space.\n\nWe'll also note 4 stars that exhibit fast brightening while being dimmer than most stars of their type:\n\n* TYC 4366-182-1 (Gaia DR2 1112877875238798592)\n* TYC 4554-12-1\t(Gaia DR2 1716394898578796928)\n* TYC 4418-1408-1 (Gaia DR2 1696660893947163904)\n* TYC 2036-557-1 (Gaia DR2 1223207789709256064)","0e84d3bc":"The effect of magnitude:","f1242958":"## Summary stats\nWe are using slopes estimated after removal of magnitudes that are 3-sigma outliers. The standard deviation of slopes is lower than those estimated without outlier removal. \n\nThis is how nominal DASCH slopes from the dataset distribute:","1646de7e":"## Squared slope (i.e. error) modeling\nWe can deal with the position-based distortions at the same time that we produce new standard error estimates.\n\nThe point of the standard error is to determine if a measurement is anomalous. DASCH slopes (corrected or not) distribute much more broadly than the nominal slope standard errors suggest. This could be because stars do vary in reality or because of systematics we haven't accounted for.\n\nWe can model the actual error as a function of the nominal standard error and position in the sky. The dependent variable of the model will be the squared corrected slope. If we use a Random Forest as the machine learning algorithm, it will calculate local averages of the squared error in the variable space.\n\nThe variable space will have its own biases, so we should first model the corrected slope with the same variables and the same algorithm (same hyperparameters) that we will use to model the error.","21e2b747":"This is the root of the mean standard squared error:","b7d0d7fa":"## Analysis of position-based bias\nPhotometric databases in general seem to have position-based distortions. DASCH is no exception. We can demonstrate this by training a Random Forest on celestial coordinates.","fefa3338":"## Overview\nThe [DASCH Project](http:\/\/dasch.rc.fas.harvard.edu\/project.php) provides data on stellar brightness spanning a century or so. Trend estimates obtained from DASCH have been found to have systematic issues and broad variance (Lund et al. 2016; Hippke et al. 2016). In this notebook we attempt to correct DASCH brightness trends by adjusting for the effects of magnitude, proper motion, color and position in the sky. We also provide new standard error estimates based on observed data variance. A handful of noteworthy stars are listed.","27db006b":"Color also has an effect:","23893035":"The distribution of slopes is now the following:","043815fb":"## Datasets\nWe will use dataset _[Brightness Trends from 73K DASCH stars](https:\/\/www.kaggle.com\/solorzano\/brightness-trends-from-73k-dasch-stars)_, joined with _[253K APASS DR9 Stars W\/ Gaia DR2 Photometry](https:\/\/www.kaggle.com\/solorzano\/253k-apass-dr9-stars-w-gaia-dr2-photometry)_. The number of records in the joined data frame is:","f3cc4636":"The reason why nearby stars tend to have negative slopes is likely because of systematic effects of magnitude and color. Magnitude affects DASCH slopes as follows:","51e4f47e":"And color:","ac03ee91":"The standard deviation of the *slope_anomaly* metric should theoretically be 1.0, provided there are no extreme outliers.","a18c9d76":"## Removal of giants\nThis correction is intended for main-sequence stars. Also, removing giants (and variable stars) reduces the standard deviation of slopes, and improves the precision of the correction model. We will use the same removal methodology that we used in notebook _[New Stellar Magnitude Model](https:\/\/www.kaggle.com\/solorzano\/new-stellar-magnitude-model-dysonian-seti)_.","65d7a61e":"## References","91b36caa":"The effect of proper motion appears to be mostly linear. Presumably, it has to do with how a star is centered in a plate. It's not clear, however, if there's too much error when stars have very high proper motions. It comes to reason that the chance of a bad cross-match would increase with high proper motion, and that displacement would be substantial over the span of a century. Let's look at how proper motion affects the absolute slope (i.e. the mean deviation):","48389f64":"Distance:"}}