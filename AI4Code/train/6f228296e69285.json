{"cell_type":{"34a76639":"code","713e90f0":"code","1290f294":"code","1e042b0d":"code","e66bc1da":"code","655b022b":"code","91c7968c":"code","8ceee305":"code","af93143e":"code","af44d53e":"code","47fe219a":"code","6772599b":"code","7a37d808":"code","1c2b4556":"code","df42b4b0":"code","51d27f86":"code","754cf284":"code","e8536e5f":"code","54409e41":"code","06644ef7":"code","5cbb3230":"code","27a925b7":"code","ae8d6a3d":"code","73e65281":"code","b5d53f82":"code","17aeadcc":"code","e62a45a0":"code","35db42da":"code","88f8a6ee":"code","b142b322":"code","d5155c59":"code","5538251f":"code","f6682a25":"code","e1235979":"code","a1f7769f":"code","a8e6e320":"code","19b04dec":"code","fbb94b1e":"code","6b2e51c8":"code","dab313d2":"code","1938e74d":"code","ffc088ab":"code","330c50dc":"code","c4363233":"code","84afee70":"markdown","787b9ffe":"markdown","e3df336a":"markdown","23404249":"markdown","66f60964":"markdown","1b140e28":"markdown","9d7876fc":"markdown","df540cc6":"markdown","00e48896":"markdown","a1795d8f":"markdown","de3f9361":"markdown","28cc6db7":"markdown","91929bdf":"markdown","e2ebcbf3":"markdown","0b93ea1f":"markdown","5433cb1f":"markdown","f0668113":"markdown","ca34b7d7":"markdown","cd201db0":"markdown","9f69ca9d":"markdown","6b2d4fb9":"markdown","600bad94":"markdown","bc5b2147":"markdown","7a24d8b6":"markdown","6aed32f1":"markdown","93a6f0c6":"markdown","11e68665":"markdown","827b1a03":"markdown","c8bc99b4":"markdown","f0ecd64f":"markdown","5224bedb":"markdown","ac40a2c8":"markdown","e7fb13cf":"markdown","94546118":"markdown","6e4668ae":"markdown","82f7bfeb":"markdown","c1fe8edc":"markdown","a1e79d9f":"markdown","80496f2b":"markdown","d30e5098":"markdown","7d345d4c":"markdown","de3cb910":"markdown","70869be8":"markdown","d9384126":"markdown","e801a451":"markdown","d5498758":"markdown","bbdb2955":"markdown","17667171":"markdown","a0ea92c9":"markdown","2f4221b7":"markdown","6d57f4e9":"markdown"},"source":{"34a76639":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import roc_curve, auc  #Metrics\n\n#ML Libraries\nimport lightgbm as lgb\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport xgboost\n\n#eli5 \nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom eli5 import show_prediction\n\n#partial dependencies\nfrom pdpbox import pdp, get_dataset, info_plots\n\n#shap analysis\nimport shap\n\n#LabelEncoder\nfrom sklearn.preprocessing import LabelEncoder","713e90f0":"fillColor = \"#FFA07A\"\nfillColor2 = \"#F1C40F\"\nloans = pd.read_csv('..\/input\/ny-home-mortgage\/ny_hmda_2015.csv')","1290f294":"loans.head()","1e042b0d":"cols = [f_ for f_ in loans.columns if loans[f_].dtype != 'object']\nfeatures = cols\n\nlist_to_remove = ['action_taken','purchaser_type',\n                  'denial_reason_1','denial_reason_2','denial_reason_3','sequence_number']\n\nfeatures= list(set(cols).difference(set(list_to_remove)))\n\nX = loans[features]\ny = loans['action_taken']","e66bc1da":"def change_action_taken(y):\n    if ( y == 1):\n        return 1\n    else:\n        return 0","655b022b":"y = loans['action_taken'].apply(change_action_taken)\n\nX = X.fillna(0)\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)","91c7968c":"from lightgbm import LGBMClassifier","8ceee305":"train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nfirst_model = LGBMClassifier(random_state=1).fit(train_X, train_y)","af93143e":"predictions =  first_model.predict_proba(val_X)","af44d53e":"fpr, tpr, thresholds = roc_curve(val_y, predictions[:,1])\n\nfig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","47fe219a":"auc(fpr, tpr)","6772599b":"perm = PermutationImportance(first_model, random_state=1).fit(val_X, val_y)\n\neli5.show_weights(perm, feature_names = val_X.columns.tolist())","7a37d808":"eli5.explain_weights_df(perm, feature_names=features).head(10)","1c2b4556":"show_prediction(first_model, val_X.iloc[0,:],show_feature_values=True)","df42b4b0":"feat_name = 'loan_purpose'\npdp_dist = pdp.pdp_isolate(model=first_model, dataset=val_X, model_features=features, feature=feat_name)","51d27f86":"fig, axes = pdp.pdp_plot(pdp_isolate_out=pdp_dist,\n                         feature_name=feat_name)","754cf284":"loans.groupby(['loan_purpose','loan_purpose_name']).loan_type.count()","e8536e5f":"feat_name = 'applicant_income_000s'\npdp_dist = pdp.pdp_isolate(model=first_model, dataset=val_X, model_features=features, feature=feat_name)","54409e41":"fig, axes = pdp.pdp_plot(pdp_isolate_out=pdp_dist,\n                         feature_name=feat_name)","06644ef7":"loans[feat_name].describe()","5cbb3230":"val_X_modified =  val_X[val_X['applicant_income_000s'] <  500]\nfeat_name = 'applicant_income_000s'\npdp_dist = pdp.pdp_isolate(model=first_model, dataset=val_X_modified, model_features=features, feature=feat_name)","27a925b7":"fig, axes = pdp.pdp_plot(pdp_isolate_out=pdp_dist,\n                         feature_name=feat_name)","ae8d6a3d":"val_X_small = val_X[0:1000]\n\nexplainer = shap.TreeExplainer(first_model)\ndata_for_prediction_array = val_X_small.iloc[0,:].values.reshape(1, -1)\n\nfirst_model.predict_proba(data_for_prediction_array)","73e65281":"%time shap_values = explainer.shap_values(val_X_small)\n\n# visualize the first prediction's explanation\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values[0,:], val_X_small.iloc[0,:],link = 'logit')","b5d53f82":"# calculate shap values. This is what we will plot.\nshap_values = explainer.shap_values(val_X_small)\n\nshap.summary_plot(shap_values, val_X_small)","17aeadcc":"shap.dependence_plot(\"lien_status\", shap_values, val_X_small,interaction_index=\"applicant_income_000s\")","e62a45a0":"loans.groupby(['lien_status','lien_status_name']).applicant_income_000s.count()","35db42da":"shap.dependence_plot(\"loan_purpose\", shap_values, val_X_small,interaction_index=\"applicant_income_000s\")","88f8a6ee":"loans.groupby(['loan_purpose','loan_purpose_name']).applicant_income_000s.count()","b142b322":"houses = pd.read_csv('..\/input\/melbourne-housing-market\/Melbourne_housing_FULL.csv')\nhouses_2 = houses.copy()\nhouses.head() ","d5155c59":"def extract_features(houses):\n    cols_to_remove = ['Address','Method']\n    cols = [f_ for f_ in houses.columns if houses[f_].dtype == 'object']\n    features= list(set(cols).difference(set(cols_to_remove)))\n\n    cols = [f_ for f_ in houses.columns if houses[f_].dtype != 'object']\n    features_all = features + cols\n    return features,features_all\n\nfeatures,features_all = extract_features(houses)","5538251f":"def label_encode_dataset(features,houses):\n    for c in features:\n        le = LabelEncoder()\n        le.fit(houses[c].astype(str))\n        houses[c] = le.transform(houses[c].astype(str))\n    return houses\n\nhouses = label_encode_dataset(features,houses)","f6682a25":"houses_all = houses[features_all]\nhouses_all = houses_all.fillna(0)","e1235979":"import xgboost\nfrom xgboost import XGBRegressor\ncols_to_remove = ['Price','Date']\nfeatures2= list(set(features_all).difference(set(cols_to_remove)))","a1f7769f":"def create_model(features2,houses_all):\n    \n    X  = houses_all[features2]\n    y = houses_all['Price']\n\n    params = {}\n    params[\"objective\"] = \"reg:linear\"\n    params[\"eta\"] = 0.01\n    params[\"min_child_weight\"] = 10\n    params[\"subsample\"] = 0.8\n    params[\"colsample_bytree\"] = 0.8\n    params[\"scale_pos_weight\"] = 1.0\n    params[\"silent\"] = 1\n    params[\"max_depth\"] = 7\n    params[\"nthread\"] = 4\n\n    plst = list(params.items())\n    num_rounds=20000 \n\n    train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n    xgtrain = xgboost.DMatrix(train_X, label = train_y,feature_names = features2)\n    xgval = xgboost.DMatrix(val_X,label = val_y,feature_names = features2)\n\n    # define a watch list to observe the change in error of training and holdout data\n    watchlist  = [ (xgtrain,'train'),(xgval,'eval')]\n    model = xgboost.train(plst, \n                      xgtrain, \n                      num_rounds,\n                      watchlist,\n                      early_stopping_rounds=50,verbose_eval=False)  \n    return (model,train_X, val_X, train_y, val_y)\n\nmodel,train_X, val_X, train_y, val_y = create_model(features2,houses_all)","a8e6e320":"# load JS visualization code to notebook\nshap.initjs()\n\n# visualize the first prediction's explanation\nexplainer = shap.TreeExplainer(model)\n\nshap_values2 = explainer.shap_values(val_X)\nshap.force_plot(explainer.expected_value, shap_values2[0,:], val_X.iloc[0,:])","19b04dec":"# summarize the effects of all the features\nshap.summary_plot(shap_values2, val_X)","fbb94b1e":"# make plot.\nshap.dependence_plot('Distance', shap_values2, val_X, interaction_index=\"Postcode\")","6b2e51c8":"# make plot.\nshap.dependence_plot('Postcode', shap_values2, val_X, interaction_index=\"Distance\")","dab313d2":"# make plot.\nshap.dependence_plot('Rooms', shap_values2, val_X, interaction_index=\"Distance\")","1938e74d":"houses_Malvern = houses_2[houses_2.Suburb == 'Malvern']","ffc088ab":"houses_Malvern.head()","330c50dc":"features,features_all = extract_features(houses_Malvern)\n\nhouses_Malvern = label_encode_dataset(features,houses_Malvern)\n\nhouses_Malvern = houses_Malvern[features_all]\nhouses_Malvern = houses_Malvern.fillna(0)\nhouses_Malvern2  = houses_Malvern.copy()\ncols_to_remove = ['Price','Date']\nfeatures2= list(set(features_all).difference(set(cols_to_remove)))\nhouses_Malvern = houses_Malvern[features2]","c4363233":"# load JS visualization code to notebook\nshap.initjs()\n\n# visualize the first prediction's explanation\nexplainer = shap.TreeExplainer(model)\n\nshap_values2 = explainer.shap_values(houses_Malvern)\nshap.force_plot(explainer.expected_value, shap_values2[0,:], val_X.iloc[0,:])","84afee70":"As the number of rooms increases, the price of the house increases","787b9ffe":"# Read the data","e3df336a":"Feature values causing increased predictions are in pink, and their visual size shows the magnitude of the feature's effect. Feature values decreasing the prediction are in blue. The biggest impact comes from `Lien Status =  1` followed by `Loan Purpose = 3`","23404249":"<hr\/>\n# Chapter 1 : New York Home Mortgage Analysis\n<hr\/>","66f60964":"## SHAP Summary Plot","1b140e28":"We see that \n* Too low applicant income decreases the probablity of loan origination         \n* Too high applicant income decreases the probablity of loan origination          \n\nWe are unable to get better analysis from the above graph. Therefore we plot another graph where the applicant income is reduced","9d7876fc":"# SHAP Dependence Contribution Plots","df540cc6":"I stayed in the suburb of Malvern in Melbourne for quite some time. Would be interested to know how the house prices varied","00e48896":"We show the  contribution of each feature in the prediction","a1795d8f":"One of the most basic questions we might ask of a model is `What features have the biggest impact on predictions?`\nThis concept is called **feature importance**. Permutation Importance is one of the techniques to measure feature importance.   \nThe process of Permutation importance is provided below           \n\n1. Get a trained model                        \n2. Shuffle the values in a single column, make predictions using the resulting dataset. Use these predictions and the true target values to calculate how much the loss function suffered from shuffling. That **performance deterioration measures the importance of the variable you just shuffled.**                       \n3. Return the data to the original order (undoing the shuffle from step 2.) Now repeat step 2 with the next column in the dataset, until you have calculated the importance of each column.         ","de3f9361":"Remove the features `Address` and `Method`","28cc6db7":"The Home Mortgage Disclosure Act (HMDA) requires many financial institutions to maintain, report, and publicly disclose information about mortgages.This dataset covers all mortgage decisions made in 2015 for the state of New York.\n\nBefore we dive into solve the problem, let us first understand the business related to this dataset. \n\nEach year thousands of banks and other financial institutions report data about mortgages to the public, thanks to the Home Mortgage Disclosure Act, or **\u201cHMDA\u201d** for short. These public data are important because:\n\n* Help show whether lenders are serving the housing needs of their communities;           \n* Give public officials information that helps them make decisions and policies; and          \n* Shed light on lending patterns that could be discriminatory            ","91929bdf":"For the modelling we include all the features which are numeric. The categorical features in the model have their corresponding numeric values in the numerical features","e2ebcbf3":"## Applicant Income Feature Analysis","0b93ea1f":"# Permutation Importance","5433cb1f":"# SHAP Analysis","f0668113":"# Introduction","ca34b7d7":"## Loan Origination Journey","cd201db0":"Modelling using XGBoost","9f69ca9d":"**Interpretation of Partial Dependence Plots**","6b2d4fb9":"# SHAP Tree explainer","600bad94":"The loan is 42% likely to be originated","bc5b2147":"## SHAP Tree Explainer","7a24d8b6":"From the plot, it is evident that very high applicant income is associated with loan purpose of Home Purchase. The loan purpose of Home improvement and Refinancing is not associated with high applicant income","6aed32f1":"**Explanation**\n\nThe first number in each row shows how much model performance decreased with a random shuffling (in this case, using \"accuracy\" as the performance metric).\n\nLike most things in data science, there is some randomness to the exact performance change from a shuffling a column. We measure the amount of randomness in our permutation importance calculation by repeating the process with multiple shuffles. The number after the \u00b1 measures how performance varied from one-reshuffling to the next.\n\nWe will  occasionally see negative values for permutation importances. In those cases, the predictions on the shuffled (or noisy) data happened to be more accurate than the real data. This happens when the feature didn't matter (should have had an importance close to 0), but random chance caused the predictions on shuffled data to be more accurate. This is more common with small datasets, like the one in this example, because there is more room for luck\/chance.\n\nIn our example, the most important feature was **Lien Status** followed by **Loan Purpose** and **Applicant Income**","93a6f0c6":"# SHAP Dependence Contribution Plots","11e68665":"Feature values causing increased predictions are in pink, and their visual size shows the magnitude of the feature's effect. Feature values decreasing the prediction are in blue. The biggest impact comes from `Rooms = 4 , Postcode and Suburb`. This is understandable since the price in Melbourne is dependent on the number of rooms as well as the suburb. Suburbs like South Yarra , Prahran have houses with high prices","827b1a03":"# Analysis of Malvern Houses","c8bc99b4":"# SHAP analysis","f0ecd64f":"Meet Emily. She wants to buy a home but doesn\u2019t have the money to pay for it in cash, so she applies for a loan at her bank. She tells the bank about her finances, the house she wants to buy, and other information the bank needs to make a decision about whether or not to lend to her, and the terms of the loan. The bank reviews Emily\u2019s application, decides that she meets their criteria, and she gets approved. Once all the papers are signed, Emily closes the loan\u2026 or in mortgage-speak, the loan is **\u201coriginated.\u201d.**\n\nTherefore the last stage of the loan is **Loan Origination.**\n\nThe data provided can be grouped into the following subjects\n\n* **Location**  describes the State, metro area and census tract of the property         \n\n* **Property Type**  describes the Property Type and Occupancy of the property.Property type values include One-to-four family dwelling,Manufactured housing and Multifamily dwelling. This also answers the question \u201cWill the owner use the property as their primary residence ?\u201d . The values include Owner occupied as principal dwelling , Not owner occupied as principal dwelling and Not Applicable.\n\n* **Loan**  describes the action taken on the Loan, purpose of the Loan , Type of the loan ,Loan\u2019s lien status.\n\n* **Lender**  describes the lender associated with the loan and the Federal agency associated with the loan.\n\n*  **Applicant**  describes the demographic information for the applicants and the co-applicants.This has the applicant sex , co- applicant sex , applicant race and ethnicity, co- applicant race and ethnicity.","5224bedb":"From the graph above, it is evident that the loan origination increases till the applicant income is around 100K","ac40a2c8":"# Modelling","e7fb13cf":"In this chapter, we predict the House prices in Melbourne. In this chapter we focus on the **Shapley Analysis**","94546118":"# Load Libraries","6e4668ae":"The figure provides the following information\n\n*  Higher the Lien Status , lower  is the probablity of Loan Origination              \n\n* Higher the Rate Spread, higher is the probablity of Loan Origination","82f7bfeb":"In the same postcode around 3000 to 3200, the price of the house changes. There are other factors which are affecting the price of the house","c1fe8edc":"While feature importance shows what variables most affect predictions, partial dependence plots show how a feature affects predictions.\n\nLike permutation importance, partial dependence plots are calculated after a model has been fit. The model is fit on real data that has not been artificially manipulated in any way.             \n\nWe will use the fitted model to predict our outcome (probability the **loan has been originated**. But we repeatedly alter the **value for one variable to make a series of predictions.**","a1e79d9f":"We define a function in which we mark the **Loans which are originated** as 1 and the **Loans which are NOT originated** as 0","80496f2b":"Do you wish to use complex machine learning models and explain it to business ? Machine learning explainability comes to your rescue. The kernel uses 2 different case studies \n* New York Home Mortgage data          \n* Housing prices in Melbourne \n\nThe kernel makes use generously of the wonderful Kaggle course of `Machine Learning explainibility course in Kaggle Learn by Dan Becker` .     \n\nThe techniques used are **Permutation Importance, Partial Dependence Plots and SHAP analysis**. Please read further more details.","d30e5098":"The dominant features affecting  the model are **Distance,Type,PostCode,Rooms and Landsize**         \n\nThis plot is made of many dots. Each dot has three characteristics:\n\n*  Vertical location shows what feature it is depicting\n* Color shows whether that feature was high or low for that row of the dataset\n* Horizontal location shows whether the effect of that value caused a higher or lower prediction.      \n\nFrom the plot , it is evident \n* High Values of Room , Landsize , BuildingArea  increase the price of the House             \n*  High Values of Distance reduce the price of the House     ","7d345d4c":"<hr\/>\n\n# Chapter 2 : Melbourne Housing Prices\n\n<hr\/>","de3cb910":"# Modelling","70869be8":"We will start by focusing on the shape, and we'll come back to color in a minute. Each dot represents a row of the data. The horizontal location is the actual value from the dataset, and the vertical location shows what having that value did to the prediction. The fact this slopes **downward**  says that the more  the Distance, the higher the model's prediction is for **lower price of the house**.       \n\nThe color is associated with the Postcode. The lower priced houses are in the range 3175 to 3200 postcode","d9384126":"A few items are worth pointing out as you interpret this plot\n\nThe y axis is interpreted as change in the prediction from what it would be predicted at the baseline or leftmost value.\nA blue shaded area indicates level of confidence           \n\nFrom this particular graph, we see that Loan Purpose of **Home purchase**  has the highest probablity of Loan Orgination\n\n","e801a451":"From the plot, it is evident that the lien status of **Not applicable** is associated with low applicant income","d5498758":"Feature values causing increased predictions are in pink, and their visual size shows the magnitude of the feature's effect. Feature values decreasing the prediction are in blue. The biggest impact comes from `Distance, Type and Suburb`. ","bbdb2955":"# SHAP Summary Plot","17667171":"## Loan Purpose Feature Analysis","a0ea92c9":"## SHAP analysis","2f4221b7":"Convert the categorical features into numeric values","6d57f4e9":"# Partial Plots"}}