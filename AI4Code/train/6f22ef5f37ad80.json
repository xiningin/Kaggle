{"cell_type":{"7334086f":"code","66f52841":"code","029831bc":"code","a895f330":"code","d94bd571":"code","1bf587cb":"code","dd65eec1":"code","8eaeac4f":"code","a22e8ad7":"code","5106f2a3":"code","27bab98d":"code","4593270f":"code","3826f270":"code","8d9c522b":"code","8f90ecad":"code","027cd7a8":"code","a7444313":"code","e202cb0f":"code","d7d3260e":"code","584df7b0":"code","6a55b939":"code","98b06891":"code","21d8f199":"code","9cf17c74":"code","71173664":"code","651b7e61":"code","7ad676b4":"code","b1c3bbcd":"code","5825b5a0":"code","a0935d83":"code","28114b5a":"code","0d0a9c01":"code","4a75c829":"code","33684a43":"code","2c4680ab":"code","4786a26a":"code","4510933d":"code","938115d5":"markdown","1a33ed26":"markdown","32be5071":"markdown","68878ca1":"markdown","48f66297":"markdown","de399540":"markdown","fc83e321":"markdown","d9603e37":"markdown","4f7e43d0":"markdown","5130f46e":"markdown","1dd81387":"markdown","2e5275e0":"markdown","3fc070fc":"markdown","87a10dff":"markdown","bf68a17b":"markdown","8d9a3c32":"markdown","e6ca0e2a":"markdown","2eb05c8b":"markdown","bfdc465f":"markdown"},"source":{"7334086f":"import os\nimport torch\nimport torchvision\nimport tarfile\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as tt\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import Dataset, random_split, DataLoader\nfrom PIL import Image\nimport torchvision.models as models\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\n%matplotlib inline","66f52841":"DATA_DIR = '..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/'\n\nTRAIN_DIR = DATA_DIR + 'train\/'                           # Contains training images\nVAL_DIR = DATA_DIR + \"val\/\"                               # Contains tes images\nprint(os.listdir(DATA_DIR))\nclasses = os.listdir(TRAIN_DIR)\nprint(classes)","029831bc":"# Data transforms (normalization & data augmentation)\nstats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\ntrain_tfms = tt.Compose([ tt.RandomHorizontalFlip(), tt.RandomRotation(10),\n                         tt.ToTensor(), tt.Normalize(*stats,inplace=True)])\nvalid_tfms = tt.Compose([tt.ToTensor(), tt.Normalize(*stats)])","a895f330":"# PyTorch datasets\n\ntrain_ds = ImageFolder(TRAIN_DIR, train_tfms)\nvalid_ds = ImageFolder(VAL_DIR, valid_tfms)\nlen(train_ds), len(valid_ds)\n","d94bd571":"img, label = train_ds[0]\nimg.shape","1bf587cb":"batch_size = 64","dd65eec1":"# PyTorch data loaders\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)\nvalid_dl = DataLoader(valid_ds, batch_size*2, num_workers=3, pin_memory=True)","8eaeac4f":"img, label = train_ds[0]\nimg_shape = img.shape\nimg_shape","a22e8ad7":"def show_batch(dl):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(12, 12))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))\n        break","5106f2a3":"img, label = train_ds[5945]\nplt.imshow(img.permute((1, 2, 0)))\nprint('Label (numeric):', label)","27bab98d":"def show_batch(dl, invert=True):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(16, 8))\n        ax.set_xticks([]); ax.set_yticks([])\n        data = 1-images if invert else images\n        ax.imshow(make_grid(data, nrow=16).permute(1, 2, 0))\n        break","4593270f":"show_batch(train_dl)","3826f270":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","8d9c522b":"device = get_default_device()\ndevice","8f90ecad":"train_dl = DeviceDataLoader(train_dl, device)\nvalid_dl = DeviceDataLoader(valid_dl, device)","027cd7a8":"class SimpleResidualBlock(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n        self.relu1 = nn.ReLU()\n        self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n        self.relu2 = nn.ReLU()\n        \n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.relu1(out)\n        out = self.conv2(out)\n        return self.relu2(out) + x # ReLU can be applied before or after adding the input","a7444313":"simple_resnet = to_device(SimpleResidualBlock(), device)\n\nfor images, labels in train_dl:\n    out = simple_resnet(images)\n    print(out.shape)\n    break\n    \ndel simple_resnet, images, labels\ntorch.cuda.empty_cache()","e202cb0f":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))","d7d3260e":"def conv_block(in_channels, out_channels, pool=False):\n    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n              nn.BatchNorm2d(out_channels), \n              nn.ReLU(inplace=True)]\n    if pool: layers.append(nn.MaxPool2d(2))\n    return nn.Sequential(*layers)\n\nclass ResNet9(ImageClassificationBase):\n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        \n        self.conv1 = conv_block(in_channels, 64)\n        self.conv2 = conv_block(64, 128, pool=True)\n        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n        \n        self.conv3 = conv_block(128, 256, pool=True)\n        self.conv4 = conv_block(256, 512, pool=True)\n        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n        \n        self.classifier = nn.Sequential(nn.MaxPool2d(4), \n                                        nn.Flatten(), \n                                        nn.Linear(512, num_classes))\n        \n    def forward(self, xb):\n        out = self.conv1(xb)\n        out = self.conv2(out)\n        out = self.res1(out) + out\n        out = self.conv3(out)\n        out = self.conv4(out)\n        out = self.res2(out) + out\n        out = self.classifier(out)\n        return out","584df7b0":"model = to_device(ResNet9(3, 10), device)\nmodel","6a55b939":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n    \n    # Set up cutom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n                                                steps_per_epoch=len(train_loader))\n    \n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        lrs = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            \n            # Gradient clipping\n            if grad_clip: \n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            sched.step()\n        \n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","98b06891":"history = [evaluate(model, valid_dl)]\nhistory","21d8f199":"epochs = 4\nmax_lr = 0.005\ngrad_clip = 0.1\nweight_decay = 0.000009\nopt_func = torch.optim.Adam","9cf17c74":"%%time\nhistory += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, \n                             grad_clip=grad_clip, \n                             weight_decay=weight_decay, \n                             opt_func=opt_func)","71173664":"train_time='4:07'","651b7e61":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');","7ad676b4":"plot_accuracies(history)","b1c3bbcd":"def plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');","5825b5a0":"def predict_single(image):\n    xb = image.unsqueeze(0)\n    xb = to_device(xb, device)\n    preds = model(xb)\n    prediction = preds[0]\n    print(\"Prediction: \", prediction)\n    show_batch(image)","a0935d83":"plot_losses(history)","28114b5a":"def plot_lrs(history):\n    lrs = np.concatenate([x.get('lrs', []) for x in history])\n    plt.plot(lrs)\n    plt.xlabel('Batch no.')\n    plt.ylabel('Learning rate')\n    plt.title('Learning Rate vs. Batch no.');","0d0a9c01":"plot_lrs(history)","4a75c829":"torch.save(model.state_dict(), 'project.pth')","33684a43":"!pip install jovian --upgrade --quiet","2c4680ab":"import jovian","4786a26a":"jovian.log_metrics(val_loss=history[-1]['val_loss'], \n                   val_acc=history[-1]['val_acc'],\n                   train_loss=history[-1]['train_loss'],\n                   time=train_time)","4510933d":"jovian.commit(project='my-project', environment=None)","938115d5":"This seeming small change produces a drastic improvement in the performance of the model. Also, after each convolutional layer, we'll add a batch normalization layer, which normalizes the outputs of the previous layer.\n\nGo through the following blog posts to learn more:\n\nWhy and how residual blocks work: https:\/\/towardsdatascience.com\/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec\nBatch normalization and dropout explained: https:\/\/towardsdatascience.com\/batch-normalization-and-dropout-in-neural-networks-explained-with-pytorch-47d7a8459bcd\nWe will use the ResNet9 architecture, as described in this blog series :\n![image.png](attachment:image.png)","1a33ed26":"Let's check how many samples the dataset contains","32be5071":"Transforms can be chained using transforms.Compose. See the full list of transforms here: https:\/\/pytorch.org\/docs\/master\/torchvision\/transforms.html","68878ca1":"Let's take a look at some sample images from the training dataloader.","48f66297":"Run the code below on Kaggle, remember select \"GPU\" as the accelerator and turn on internet from the sidebar within the Kaggle notebook.\n\nWe begin by importing the required modules & libraries.","de399540":"Next, we can create data loaders for retrieving images in batches. We'll use a relatively batch size of 64 to utlize a larger portion of the GPU RAM. You can try reducing the batch size & restarting the kernel if you face an \"out of memory\" error.","fc83e321":"Let's plot the valdation set accuracies to study how the model improves over time.","d9603e37":"We can also plot the training and validation losses to study the trend.","4f7e43d0":"# **Model with Residual Blocks and Batch Normalization**\nOne of the key changes to our CNN model this time is the addition of the resudial block, which adds the original input back to the output feature map obtained by passing the input through one or more convolutional layers.\n![image.png](attachment:image.png)\nHere is a very simply Residual block:","5130f46e":"# **Save and Commit**\nLet's save the weights of the model, record the hyperparameters, and commit our experiment to Jovian. As you try different ideas, make sure to record every experiment so you can look back and analyze the results.","1dd81387":"We're now ready to train our model. Instead of SGD (stochastic gradient descent), we'll use the Adam optimizer which uses techniques like momentum and adaptive learning rates for faster training. You can learn more about optimizers here: https:\/\/ruder.io\/optimizing-gradient-descent\/index.html","2e5275e0":"# **Training the model**\nBefore we train the model, we're going to make a bunch of small but important improvements to our fit function:\n\n**Learning rate scheduling:** Instead of using a fixed learning rate, we will use a learning rate scheduler, which will change the learning rate after every batch of training. There are many strategies for varying the learning rate during training, and the one we'll use is called the \"One Cycle Learning Rate Policy\", which involves starting with a low learning rate, gradually increasing it batch-by-batch to a high learning rate for about 30% of epochs, then gradually decreasing it to a very low value for the remaining epochs. Learn more: https:\/\/sgugger.github.io\/the-1cycle-policy.html\n\n**Weight decay:** We also use weight decay, which is yet another regularization technique which prevents the weights from becoming too large by adding an additional term to the loss function.Learn more: https:\/\/towardsdatascience.com\/this-thing-called-weight-decay-a7cd4bcfccab\n\n**Gradient clipping:** Apart from the layer weights and outputs, it also helpful to limit the values of gradients to a small range to prevent undesirable changes in parameters due to large gradient values. This simple yet effective technique is called gradient clipping. Learn more: https:\/\/towardsdatascience.com\/what-is-gradient-clipping-b8e815cdfb48\n\nLet's define a fit_one_cycle function to incorporate these changes. We'll also record the learning rate used for each batch.","3fc070fc":"# **Using a GPU**\nTo seamlessly use a GPU, if one is available, we define a couple of helper functions (get_default_device & to_device) and a helper class DeviceDataLoader to move our model & data to the GPU as required. ","87a10dff":"# **Classifying FACIAL EXPRESSION images using a ResNet and Regularization techniques in PyTorch**\nData is divided into training(80%), testing(10%), and validation(10%) sets in ImageFolder format. The Labels are as follows:\n\n![image.png](attachment:image.png)\n\nFor some models trained on this data gives 68% of accuracy. Following are the techniques that are used:\n* Data normalization\n* Data augmentation\n* Residual connections\n* Batch normalization\n* Learning rate scheduling\n* Weight Decay\n* Gradient clipping\n* Adam optimizer\n","bf68a17b":"The colors seem out of place because of the normalization. Note that normalization is also applied during inference. Horizontal flip is a bit difficult to detect from visual inspection.","8d9a3c32":"It's clear from the trend that our model isn't overfitting to the training data just yet. Finally, let's visualize how the learning rate changed over time, batch-by-batch over all the epochs.","e6ca0e2a":"Based on where you're running this notebook, your default device could be a CPU (torch.device('cpu')) or a GPU (torch.device('cuda'))","2eb05c8b":"We can now wrap our training and validation data loaders using DeviceDataLoader for automatically transferring batches of data to the GPU (if available).","bfdc465f":"# **Exploring the Data**\nWhen you create a notebook with the \"Notebooks\" tab of a Kaggle, the data is automatically included in the ..\/input folder. You can explore the files in the sidebar. Let us create some constants acess the data directories."}}