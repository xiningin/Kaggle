{"cell_type":{"f77f46c9":"code","abe26433":"code","debfd424":"code","fb85914a":"code","b91f0103":"code","8b4d3231":"code","81c98f36":"code","e26e2bc1":"code","22fe8772":"code","7bc90590":"code","1e301219":"code","34397399":"code","c6244a0b":"code","dd37f466":"code","30ff71ab":"code","66d27033":"code","075a6ce8":"code","4727845e":"code","be56a140":"code","3548091a":"code","8c5a5a9f":"code","debddeb7":"code","c989858a":"code","1c0b64d8":"markdown","36217fb0":"markdown","c94ca09d":"markdown","7cc923ae":"markdown","4b6cee2d":"markdown","b36faea6":"markdown","3d898bed":"markdown","c1817e9b":"markdown"},"source":{"f77f46c9":"import os\nimport sys\nimport copy\nimport random\n\nfrom time import time\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer\n\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport warnings\nwarnings.filterwarnings('ignore')","abe26433":"print(\"torch version:\", torch.__version__)","debfd424":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","fb85914a":"ROOT = \"..\/input\/lish-moa\"","b91f0103":"df_train_features          = pd.read_csv(f'{ROOT}\/train_features.csv')\ndf_train_targets_scored    = pd.read_csv(f'{ROOT}\/train_targets_scored.csv')\ndf_train_targets_nonscored = pd.read_csv(f'{ROOT}\/train_targets_nonscored.csv')\ndf_train_drug              = pd.read_csv(f'{ROOT}\/train_drug.csv')\ndf_test_features           = pd.read_csv(f'{ROOT}\/test_features.csv')\ndf_sample_submission       = pd.read_csv(f'{ROOT}\/sample_submission.csv')","8b4d3231":"print(\"Train shapes:\", df_train_features.shape, df_train_targets_scored.shape, df_train_drug.shape, df_train_targets_nonscored.shape)\nprint(\"Test shapes: \", df_test_features.shape, df_sample_submission.shape)","81c98f36":"col_genes   = [col for col in df_train_features.columns.to_list() if col[:2] == \"g-\"]\ncol_cells   = [col for col in df_train_features.columns.to_list() if col[:2] == \"c-\"]\ncol_cats    = [\"cp_time\", \"cp_dose\"]\n\ncol_targets = df_train_targets_scored.columns.to_list()[1:]\n\nprint(\"-\"*25)\nprint(\"Number of Gene columns:       \", len(col_genes))\nprint(\"Number of Cell columns:       \", len(col_cells))\nprint(\"Number of Categorical columns:\", len(col_cats))\nprint(\"-\"*25)\nprint(\"Number of Target columns:     \", len(col_targets))\nprint(\"-\"*25)","e26e2bc1":"print(\"Data explore:\")\nprint(\"-\"*25)\nprint(\"Number of unique in sig_id:  %5d\" % df_train_features.sig_id.unique().shape[0], \"| Examples:\",  df_train_features.sig_id.unique()[:3])\nprint(\"Number of unique in cp_type: %5d\" % df_train_features.cp_type.unique().shape[0], \"| Examples:\",  df_train_features.cp_type.unique()[:3])\nprint(\"Number of unique in cp_time: %5d\" % df_train_features.cp_time.unique().shape[0], \"| Examples:\",  df_train_features.cp_time.unique()[:3])\nprint(\"Number of unique in cp_dose: %5d\" % df_train_features.cp_dose.unique().shape[0], \"| Examples:\",  df_train_features.cp_dose.unique()[:3])\n\nprint(\"-\"*25)\nprint(\"Most frequent targets:\")\nprint(\"-\"*25)\ndf_train_targets_scored.sum()[1:].sort_values(ascending=False)","22fe8772":"df_train = df_train_features.merge(df_train_targets_scored, how=\"left\", on=\"sig_id\")\ndf_train = df_train.merge(df_train_drug, on=\"sig_id\", how=\"left\")\ndf_train = df_train.dropna()\n\ndf_test = df_test_features.copy()\n\nprint(df_train.shape)\nprint(df_test.shape)\ndf_train.head(2)","7bc90590":"### FOLD \nNFOLDS = 5\nSEED = 42\n\n# DELETE FOLD COLUMN\nif \"fold\" in df_train.columns: del df_train['fold']\n\nvc = df_train.drug_id.value_counts()\nvc1 = vc.loc[vc<=18].index.sort_values()\nvc2 = vc.loc[vc>18].index.sort_values()\n\n# STRATIFY DRUGS 18X OR LESS\ndct1 = {}; dct2 = {}\nskf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\ntmp = df_train.groupby('drug_id')[col_targets].mean().loc[vc1]\nfor fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[col_targets])):\n    dd = {k:fold for k in tmp.index[idxV].values}\n    dct1.update(dd)\n\n# STRATIFY DRUGS MORE THAN 18X\nskf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\ntmp = df_train.loc[df_train.drug_id.isin(vc2)].reset_index(drop=True)\nfor fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[col_targets])):\n    dd = {k:fold for k in tmp.sig_id[idxV].values}\n    dct2.update(dd)\n\n# ASSIGN FOLDS\ndf_train['fold'] = df_train.drug_id.map(dct1)\ndf_train.loc[df_train.fold.isna(),'fold'] =\\\n    df_train.loc[df_train.fold.isna(),'sig_id'].map(dct2)\ndf_train.fold = df_train.fold.astype('int8')","1e301219":"print(\"Sum of `Ctl_vehicle` is %d. Meaning targets are zeroes. So that's why we can ignore these data.\" % df_train[df_train.cp_type==\"ctl_vehicle\"][col_targets].sum().sum())\nprint(\"Sum of `Trt_cp` is %d. So we are using these columns.\" % df_train[df_train.cp_type==\"trt_cp\"][col_targets].sum().sum())\n\nprint(\"Before excluding `ctl_vehicle`: \", df_train.shape)\ndf_train = df_train.loc[df_train.cp_type==\"trt_cp\"]\ndf_train = df_train.reset_index(drop=True)\nprint(\"After excluding `ctl_vehicle`: \", df_train.shape)","34397399":"sns.countplot(df_train[\"fold\"])\nplt.title(\"Fold Count Plot\")\nplt.show()","c6244a0b":"# convert categorical data\ndf_train.loc[:, 'cp_dose'] = df_train.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\ndf_test .loc[:, 'cp_dose'] = df_test .loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n\n# getting all input columns\ncol_input = col_cats + col_cells + col_genes","dd37f466":"print(\"####### FINAL FEATURES ###########\")\nprint(\"Number of Total Input features: \", len(col_input))\nprint(\"Number of Categorical features: \", len(col_cats))\nprint(\"Number of Genes features:       \", len(col_cells))\nprint(\"Number of Cells features:       \", len(col_genes))\nprint(\"Number of Target features:      \", len(col_targets))\n\nprint(\"Train data shape:\", df_train[col_input].shape)\nprint(\"Test  data shape: \", df_test [col_input].shape)","30ff71ab":"class MoADataset:\n    def __init__(self, features_cat, features_gene, features_cell, targets):\n        self.features_cat  = features_cat\n        self.features_gene = features_gene\n        self.features_cell = features_cell\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features_cat.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x1' : torch.tensor(self.features_cat[idx, :], dtype=torch.float),\n            'x2' : torch.tensor(self.features_gene[idx, :], dtype=torch.float),\n            'x3' : torch.tensor(self.features_cell[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features_cat, features_gene, features_cell):\n        self.features_cat  = features_cat\n        self.features_gene = features_gene\n        self.features_cell = features_cell\n        \n    def __len__(self):\n        return (self.features_cat.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x1' : torch.tensor(self.features_cat[idx, :], dtype=torch.float),\n            'x2' : torch.tensor(self.features_gene[idx, :], dtype=torch.float),\n            'x3' : torch.tensor(self.features_cell[idx, :], dtype=torch.float),\n        }\n        return dct","66d27033":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        input1, input2, input3, targets = data['x1'].to(device), data['x2'].to(device), data['x3'].to(device), data['y'].to(device)\n\n        outputs = model(input1, input2, input3)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        if scheduler: scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        input1, input2, input3, targets = data['x1'].to(device), data['x2'].to(device), data['x3'].to(device), data['y'].to(device)\n        outputs = model(input1, input2, input3)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        input1, input2, input3 = data['x1'].to(device), data['x2'].to(device), data['x3'].to(device)\n\n        with torch.no_grad():\n            outputs = model(input1, input2, input3)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds","075a6ce8":"# competition metric\ndef logloss(y_true, y_pred, eps=1e-15):\n    y_pred = np.clip(y_pred, eps, 1 - eps)\n    return -(y_true * np.log(y_pred) + (1-y_true) * np.log(1-y_pred)).mean()","4727845e":"class Model(nn.Module):\n    def __init__(self, num_features_cat, num_features_gene, num_features_cell, num_targets, hidden_size, dropout_rate=0.5, activation=F.relu):\n        super(Model, self).__init__()\n        \n        def get_sequential_layers(num_features, n_layers, hidden_size, dropout_rate):\n            layer_list = []\n            in_ = num_features\n\n            for i in range(n_layers):\n                layer_list.append(\n                    torch.nn.Sequential\n                    (\n                        torch.nn.BatchNorm1d(in_),\n                        torch.nn.Dropout(dropout_rate),\n                        torch.nn.Linear(in_, hidden_size),\n                    )\n                )\n                in_ = hidden_size\n\n            return layer_list\n\n        self.layer_list_cat  = torch.nn.ModuleList(get_sequential_layers(num_features_cat , 2, hidden_size, dropout_rate))\n        self.layer_list_gene = torch.nn.ModuleList(get_sequential_layers(num_features_gene, 4, hidden_size, dropout_rate))\n        self.layer_list_cell = torch.nn.ModuleList(get_sequential_layers(num_features_cell, 3, hidden_size, dropout_rate))\n\n        self.activation = activation\n        \n        self.batch_norm_out = nn.BatchNorm1d(hidden_size*3)\n        self.dropout_out = nn.Dropout(dropout_rate)\n        self.dense_out = nn.Linear(hidden_size*3, num_targets)\n    \n    def forward(self, x1, x2, x3):\n\n        for batchnorm, dropout, linear in self.layer_list_cat:\n            x1 = batchnorm(x1)\n            x1 = dropout(x1)\n            x1 = linear(x1)\n            x1 = self.activation(x1)\n\n        for batchnorm, dropout, linear in self.layer_list_gene:\n            x2 = batchnorm(x2)\n            x2 = dropout(x2)\n            x2 = linear(x2)\n            x2 = self.activation(x2)\n\n        for batchnorm, dropout, linear in self.layer_list_cell:\n            x3 = batchnorm(x3)\n            x3 = dropout(x3)\n            x3 = linear(x3)\n            x3 = self.activation(x3)\n        \n        # print(x1.size(), x2.size(), x3.size())\n\n        x = torch.cat([x1, x2, x3], dim=-1)\n        # print(x.size())\n\n        x = self.batch_norm_out(x)\n        x = self.dropout_out(x)\n        x = self.dense_out(x)\n        \n        return x","be56a140":"def run_training(fold, seed, PARAMS, df_train, DEVICE):\n    \n    seed_everything(seed)\n\n    train_ = df_train.copy()\n    \n    trn_idx = train_[train_['fold'] != fold].index\n    val_idx = train_[train_['fold'] == fold].index\n    \n    train_df = train_[train_['fold'] != fold].reset_index(drop=True)\n    valid_df = train_[train_['fold'] == fold].reset_index(drop=True)\n    \n    x_train_cat, x_train_gene, x_train_cell, y_train  = train_df[col_cats].values, train_df[col_genes].values, train_df[col_cells].values, train_df[col_targets].values\n    x_valid_cat, x_valid_gene, x_valid_cell, y_valid  = valid_df[col_cats].values, valid_df[col_genes].values, valid_df[col_cells].values, valid_df[col_targets].values\n    \n    train_dataset = MoADataset(x_train_cat, x_train_gene, x_train_cell, y_train)\n    valid_dataset = MoADataset(x_valid_cat, x_valid_gene, x_valid_cell, y_valid)\n    \n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=PARAMS[\"BATCH_SIZE\"], shuffle=True, drop_last=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=PARAMS[\"BATCH_SIZE\"], shuffle=False)\n    \n    model = Model(\n        num_features_cat=len(col_cats),\n        num_features_gene=len(col_genes),\n        num_features_cell=len(col_cells),\n        num_targets=len(col_targets),\n        hidden_size=PARAMS[\"hidden_size\"],\n        dropout_rate=PARAMS[\"dropout_rate\"],\n        activation=PARAMS[\"activation\"],\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=PARAMS[\"LEARNING_RATE\"], weight_decay=PARAMS[\"WEIGHT_DECAY\"])\n    scheduler = None\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    best_loss = np.inf\n    \n    #--------------------- TRAIN ---------------------\n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_fn, trainloader, DEVICE)\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        \n        if VERBOSE and epoch % VERBOSE == 0:\n            print(f\"FOLD: %d, EPOCH: %3d | train_loss: %.5f | valid_loss: %.5f\"%(fold,epoch,train_loss,valid_loss))\n        \n        if valid_loss < best_loss:\n            \n            if epoch > 1:\n                os.rename(f\"SEED_{seed}_FOLD{fold}_0.pth\", f\"SEED_{seed}_FOLD{fold}_1.pth\")\n            best_loss = valid_loss\n            torch.save(model.state_dict(), f\"SEED_{seed}_FOLD{fold}_0.pth\")\n            early_step = 0\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= EARLY_STOPPING_STEPS):\n                break\n    \n            \n    #--------------------- PREDICTION---------------------\n    x_test_cat, x_test_gene, x_test_cell = df_test[col_cats].values, df_test[col_genes].values, df_test[col_cells].values\n    testdataset = TestDataset(x_test_cat, x_test_gene, x_test_cell)\n    \n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=PARAMS[\"BATCH_SIZE\"], shuffle=False)\n    \n    oof         = np.zeros((df_train.shape[0], len(col_targets)))\n    predictions = np.zeros((df_test.shape[0], len(col_targets)))\n\n    for nb in range(NBEST):\n        model = Model(\n            num_features_cat=len(col_cats),\n            num_features_gene=len(col_genes),\n            num_features_cell=len(col_cells),\n            num_targets=len(col_targets),\n            hidden_size=PARAMS[\"hidden_size\"],\n            dropout_rate=PARAMS[\"dropout_rate\"],\n            activation=PARAMS[\"activation\"],\n        )\n\n        model_path = f\"SEED_{seed}_FOLD{fold}_{nb}.pth\"\n\n        if not os.path.isfile(model_path):\n            continue\n\n        model.load_state_dict(torch.load(model_path))\n        model.to(DEVICE)\n\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        \n        oof[val_idx] += valid_preds \/ NBEST\n        predictions += inference_fn(model, testloader, DEVICE) \/ NBEST\n\n    score = logloss(y_valid, oof[val_idx])\n    print(\"FOLD: %d, Log Loss: %.5f\"%(fold, score))\n    print(\"-\"*15)\n    return oof, predictions, score\n","3548091a":"def run_k_fold(NFOLDS, seed, PARAMS, df_train, DEVICE):\n\n    oof         = np.zeros((df_train.shape[0], len(col_targets)))\n    predictions = np.zeros((df_test.shape[0], len(col_targets)))\n    scores      = []\n    \n    for fold in range(NFOLDS):\n        torch.cuda.empty_cache()\n        oof_, pred_, score_ = run_training(fold, seed, PARAMS, df_train, DEVICE)\n        \n        predictions += pred_ \/ NFOLDS\n        oof += oof_\n        \n        scores.append(score_)\n        \n    return oof, predictions, scores\n","8c5a5a9f":"# Selected parameter for training\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n\nEPOCHS               = 101\nEARLY_STOPPING_STEPS = 30\nEARLY_STOP           = True\n\nNFOLDS               = 5\nVERBOSE              = 10\nNBEST                = 2\nSEED                 = 42\n\nPARAMS = {\n    'BATCH_SIZE':    256, \n    'LEARNING_RATE': 1e-2,\n    'WEIGHT_DECAY':  1e-6, \n    'hidden_size':   512, \n    'dropout_rate':  0.4, \n    'activation':    F.relu,\n}","debddeb7":"oof         = np.zeros((df_train.shape[0], len(col_targets)))\npredictions = np.zeros((df_test.shape[0], len(col_targets)))\n\nst = time()\noof, predictions, scores_ = run_k_fold(NFOLDS, SEED, PARAMS, df_train, DEVICE)\nscore_oof = logloss(df_train[col_targets].values, oof)\nprint(\"-\"*15)\nprint(\"STD: %.6f | OOF: %6f | CV: %6f | done in %.1f min\"%(np.std(scores_), np.mean(scores_), score_oof, (time() - st)\/60))\n\ndf_test[col_targets] = predictions","c989858a":"df_test.loc[df_test.cp_type == \"ctl_vehicle\", col_targets] = 0\ndf_test[[\"sig_id\"] + col_targets].to_csv(\"submission.csv\", index=False)","1c0b64d8":"## Train Utils\n","36217fb0":"## Data Access + Preprocessing ","c94ca09d":"## Chris's KFold + Different Inputs Ensemble\n\n#### Ideas implemented here:\n* [X] 5-fold train using [Chris Deotte's CV Strategy](https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/195195)\n* [X] Blend best 2 models per fold\n* [X] Train separate Neural Networks for inputs into `Categorical`, `Genes`, and `Cells` respectively\n* [X] Train these networks End-to-End\n\n#### Architecture:\n![image.png](attachment:image.png)\n#### References:\n* [namanj27's popular PyTorch Notebook](https:\/\/www.kaggle.com\/namanj27\/new-baseline-pytorch-moa)\n* [Chris Deotte's CV Strategy](https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/195195)\n\n### If the notebook helps you, **Please UPVOTE!**","7cc923ae":"## Submit","4b6cee2d":"## Train","b36faea6":"## Split Folds","3d898bed":"## Train utils","c1817e9b":"## Modeling"}}