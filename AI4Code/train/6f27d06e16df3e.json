{"cell_type":{"0469d7e2":"code","b5352da1":"code","bab4ce54":"code","67e53368":"code","884b184d":"code","132a56e0":"code","88e651fb":"code","ae0b4434":"code","f15af144":"code","c46d957c":"code","d0c32a35":"code","5dde2fb0":"code","1654f9e1":"code","ed4ce3cb":"code","06130e3b":"code","097cef4d":"code","8df8d657":"code","28364d11":"code","5e1406fb":"code","9c052766":"code","54525ece":"code","bef8343d":"code","c83f75a8":"code","9fac60ea":"code","17fa6990":"code","23845c96":"code","6b8e3cda":"code","8e56fcc7":"code","99d2a03b":"code","fb5edd75":"code","6a5dd031":"code","d175fe95":"markdown","d9fe0eaf":"markdown","cad20691":"markdown","3e966f99":"markdown","e924f212":"markdown","e09dff26":"markdown","066366e7":"markdown","c2562b35":"markdown"},"source":{"0469d7e2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b5352da1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n#import seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom wordcloud import WordCloud\nimport seaborn as sns\nimport re","bab4ce54":"df =pd.read_csv('\/kaggle\/input\/fake-news\/train.csv' ,  encoding='ISO-8859-1')\n\ndf.head()","67e53368":"df.isna().sum()","884b184d":"df_news = df[~df['text'].isna()][['text','label']]","132a56e0":"df_news.info()","88e651fb":"df_news.drop_duplicates(subset=['text'],keep='first',inplace=True)\ndf_news.info()","ae0b4434":"fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\nlength=df_news[df_news[\"label\"]==1]['text'].str.len()\nax1.hist(length,bins = 20,color='skyblue')\nax1.set_title('Fake News')\nlength=df_news[df_news[\"label\"]==0]['text'].str.len()\nax2.hist(length, bins = 20)\nax2.set_title('Real News')\nfig.suptitle('Characters in text')\nplt.show()","f15af144":"text = \" \".join([x for x in df_news.text])\n\nwordcloud = WordCloud(background_color='white').generate(text)\n\nplt.figure(figsize=(8,6))\nplt.imshow(wordcloud,interpolation='bilinear')\nplt.axis('off')\nplt.show()","c46d957c":"## for fake\n\ntext = \" \".join([x for x in df_news.text[df_news.label==1]])\n\nwordcloud = WordCloud(background_color='white').generate(text)\n\nplt.figure(figsize=(8,6))\nplt.imshow(wordcloud,interpolation='bilinear')\nplt.axis('off')\nplt.show()","d0c32a35":"## for real\n\ntext = \" \".join([x for x in df_news.text[df_news.label==0]])\n\nwordcloud = WordCloud(background_color='white').generate(text)\n\nplt.figure(figsize=(8,6))\nplt.imshow(wordcloud,interpolation='bilinear')\nplt.axis('off')\nplt.show()","5dde2fb0":"print('Number of 0 (Not Fake) : ', df_news[\"label\"].value_counts()[0])\nprint('Number of 1 (Fake) : ', df_news[\"label\"].value_counts()[1])","1654f9e1":"label = df_news[\"label\"].value_counts()\nsns.barplot(label.index, label)\nplt.title('Target Count', fontsize=14)","ed4ce3cb":"# Dataset Preprocessing\ndef text_cleaning(text):\n    text = re.sub(\"[^a-zA-Z]\", \" \", text) # removing punctuation\n    return text\n\ndf_news['text'] = df_news['text'].apply(text_cleaning)","06130e3b":"train_df,eval_df = train_test_split(df_news,test_size = 0.05)","097cef4d":"!pip install simpletransformers==0.32.3","8df8d657":"from simpletransformers.classification import ClassificationModel\n\n\n# Create a TransformerModel\nmodel = ClassificationModel('bert', 'bert-base-cased', num_labels=2, \n                            args={'reprocess_input_data': True, 'overwrite_output_dir': True},use_cuda=False)\n","28364d11":"model.train_model(train_df)","5e1406fb":"result, model_outputs, wrong_predictions = model.eval_model(eval_df)","9c052766":"print(result)\nprint(model_outputs)","54525ece":"lst = []\nfor arr in model_outputs:\n    lst.append(np.argmax(arr))","bef8343d":"true = eval_df['label'].tolist()\npredicted = lst","c83f75a8":"import sklearn\nmat = sklearn.metrics.confusion_matrix(true , predicted)\nmat","9fac60ea":"print(sklearn.metrics.classification_report(true,predicted,target_names=['real','fake']))\n","17fa6990":"test_df =pd.read_csv('\/kaggle\/input\/fake-news\/test.csv' ,  encoding='ISO-8859-1')\n\ntest_df.head()","23845c96":"test_df.isna().sum()","6b8e3cda":"test_df.fillna('' , inplace=True)","8e56fcc7":"test_df['text'] = test_df['text'].apply(text_cleaning)","99d2a03b":"final_prediction = model.predict(list(test_df.text))","fb5edd75":"final_prediction","6a5dd031":"print('Loading in Submission File...')\n\nsubmit_df = pd.read_csv(\"\/kaggle\/input\/fake-news\/submit.csv\")\nprint(submit_df.columns)\nsubmit_df['label'] = final_prediction[0]\n\nsubmit_df.to_csv('bert_submit.csv', index=False)","d175fe95":"## Lets remove where we dont have text","d9fe0eaf":"## Train Test Split","cad20691":"## Number of words\n\nLet's compare the number of words in the fake news and real news and try to distinguish pattern in the fake and real news based on number of words used","3e966f99":"## Text Cleaning","e924f212":"Fake News\n\nWhy Fake News is a Problem ?\n\nFake news refers to misinformation, disinformation or mal-information which is spread through word of mouth and traditional media and more recently through digital forms of communication such as edited videos, memes, unverified advertisements and social media propagated rumours.Fake news spread through social media has become a serious problem, with the potential of it resulting in mob violence, suicides etc as a result of misinformation circulated on social media.\n\nDescription of Dataset\nThis dataset consists of about 20000 articles consisting of fake as well as real news. Our aim is train our model so that it can correctly predict whether a given piece of news is real or fake.\n\nid: unique id for a news article\ntitle: the title of a news article\nauthor: author of the news article\ntext: the text of the article; could be incomplete\nlabel: a label that marks the article as potentially unreliable\n1: unreliable\n0: reliable\n\nImport Libraries","e09dff26":"## Lets remove duplicates","066366e7":"## install simpletransformers","c2562b35":"## General Word Cloud "}}