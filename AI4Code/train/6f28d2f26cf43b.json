{"cell_type":{"424490d6":"code","461f3edc":"code","eb1cf288":"code","a77515ac":"code","cac9ca43":"code","18b513b9":"code","beebd916":"code","194f2939":"code","f300a36e":"code","f1158c97":"code","0b93969c":"code","303bd7f5":"code","076a3978":"code","6e495c36":"code","ed6e9fe1":"code","a3dbfee0":"markdown","6c0f73f9":"markdown","ce74314c":"markdown","ac5fe9f6":"markdown","9c78186f":"markdown","be06a1b6":"markdown","bd613282":"markdown","6c0c16b4":"markdown","37475a8c":"markdown","68e97d8d":"markdown","de3728ef":"markdown","4bd9e355":"markdown","4878ff0f":"markdown","c950cdb5":"markdown","5df66a4b":"markdown"},"source":{"424490d6":"#Importando as bibliotecas\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold","461f3edc":"dfTitanic = pd.read_csv('..\/input\/lab5_train_no_nulls_no_outliers_ohe.csv')\ndfTitanic.head(3)","eb1cf288":"# aqui montamos a matriz de atributos X e o vetor coluna de respostas Y.\n# Note que n\u00e3o selecionamos algumas colnas, como Nome e Ticket\ny = dfTitanic['Survived'].values\nX = dfTitanic[['Age', 'SibSp', 'Parch', 'Fare', 'C', 'Q', 'S', '1', '2', '3', 'female', 'male']].values","a77515ac":"# Dividindo os dados em 5 folds.\nkf = KFold(n_splits=5, shuffle=True, random_state=5)","cac9ca43":"#Fun\u00e7\u00e3o id\u00eantica \u00e0 usada nos modelos de regress\u00e3o.\ndef avalia_classificador(clf, kf, X, y, f_metrica):\n    metrica_val = []\n    metrica_train = []\n    for train, valid in kf.split(X,y):\n        x_train = X[train]\n        y_train = y[train]\n        x_valid = X[valid]\n        y_valid = y[valid]\n        clf.fit(x_train, y_train)\n        y_pred_val = clf.predict(x_valid)\n        y_pred_train = clf.predict(x_train)\n        metrica_val.append(f_metrica(y_valid, y_pred_val))\n        metrica_train.append(f_metrica(y_train, y_pred_train))\n    return np.array(metrica_val).mean(), np.array(metrica_train).mean()","18b513b9":"def apresenta_metrica(nome_metrica, metrica_val, metrica_train, percentual = False):\n    c = 100.0 if percentual else 1.0\n    print('{} (valida\u00e7\u00e3o): {}{}'.format(nome_metrica, metrica_val * c, '%' if percentual else ''))\n    print('{} (treino): {}{}'.format(nome_metrica, metrica_train * c, '%' if percentual else ''))","beebd916":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nlr = LogisticRegression(solver='liblinear')","194f2939":"media_acuracia_val, media_acuracia_train = avalia_classificador(lr, kf, X, y, accuracy_score) \napresenta_metrica('Acur\u00e1cia', media_acuracia_val, media_acuracia_train, percentual=True)\n\nmedia_auc_val, media_auc_train = avalia_classificador(lr, kf, X, y, roc_auc_score) \napresenta_metrica('AUC', media_auc_val, media_auc_train, percentual=True)","f300a36e":"from sklearn import svm\nsvc = svm.SVC(gamma='auto')","f1158c97":"media_acuracia_val, media_acuracia_train = avalia_classificador(svc, kf, X, y, accuracy_score) \napresenta_metrica('Acur\u00e1cia', media_acuracia_val, media_acuracia_train, percentual=True)\n\nmedia_auc_val, media_auc_train = avalia_classificador(svc, kf, X, y, roc_auc_score) \napresenta_metrica('AUC', media_auc_val, media_auc_train, percentual=True)","0b93969c":"from sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=3)","303bd7f5":"media_acuracia_val, media_acuracia_train = avalia_classificador(neigh, kf, X, y, accuracy_score) \napresenta_metrica('Acur\u00e1cia', media_acuracia_val, media_acuracia_train, percentual=True)\n\nmedia_auc_val, media_auc_train = avalia_classificador(neigh, kf, X, y, roc_auc_score) \napresenta_metrica('AUC', media_auc_val, media_auc_train, percentual=True)","076a3978":"from sklearn import tree\ndt = tree.DecisionTreeClassifier(max_depth=3)","6e495c36":"media_acuracia_val, media_acuracia_train = avalia_classificador(dt, kf, X, y, accuracy_score) \napresenta_metrica('Acur\u00e1cia', media_acuracia_val, media_acuracia_train, percentual=True)\n\nmedia_auc_val, media_auc_train = avalia_classificador(dt, kf, X, y, roc_auc_score) \napresenta_metrica('AUC', media_auc_val, media_auc_train, percentual=True)","ed6e9fe1":"from sklearn import tree\nimport graphviz \ndot_data = tree.export_graphviz(dt, out_file=None, \n                                feature_names=['Age', 'SibSp', 'Parch', 'Fare', 'C', 'Q', 'S', '1', '2', '3', 'female', 'male'],  \n                                filled=True, rounded=True,  \n                                special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph ","a3dbfee0":"# Laborat\u00f3rio 5 - Modelos de classifica\u00e7\u00e3o com Scikit-learn","6c0f73f9":"Aqui usamos a classe [DecisionTreeClassifier](http:\/\/scikit-learn.org\/0.17\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html). Experimente variar os par\u00e2metros do classificador. ","ce74314c":"### Regress\u00e3o log\u00edstica","ac5fe9f6":"Aqui geramos os K-folds para nossa valida\u00e7\u00e3o cruzada dos algoritmos.","9c78186f":"Aqui usamos a classe [KNeighborsClassifier](http:\/\/scikit-learn.org\/0.17\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html). Experimente variar os par\u00e2metros do classificador. ","be06a1b6":"Aqui usamos a classe [LogisticRegression](http:\/\/scikit-learn.org\/0.17\/modules\/generated\/sklearn.linear_model.LogisticRegression.html). Experimente variar os par\u00e2metros do classificador. ","bd613282":"Aqui usamos a classe [SVC](http:\/\/scikit-learn.org\/0.17\/modules\/generated\/sklearn.svm.SVC.html). Experimente variar os par\u00e2metros do classificador. ","6c0c16b4":"Aqui vamos usar o arquivo de features onde usamos OHE (one hot encoding). Fique livre para escolher outro conjunto de *features* que geramos anteriormente ou mesmo gerar suas pr\u00f3prias *features*.","37475a8c":"Para simplificar a apresenta\u00e7\u00e3o dos resultados e evitar repeti\u00e7\u00e3o de c\u00f3digo criamos a seguinte fun\u00e7\u00e3o auxuliar para imprimir os resultados.","68e97d8d":"### \u00c1rvore de decis\u00e3o","de3728ef":"### SVM","4bd9e355":"### KNN","4878ff0f":"### Carregando os dados que trabalhamos anteriormente","c950cdb5":"Assim como no laborat\u00f3rio anterior, aqui temos uma fun\u00e7\u00e3o para avaliar o desempenho dos algoritmos sem termos de ficar repetindo c\u00f3digo. Ele retorna a m\u00e9dia das m\u00e9tricas, acur\u00e1cia no caso.","5df66a4b":"Nesse laborat\u00f3rio vamos exercitar o uso do scikit-learn para algoritmos de classifica\u00e7\u00e3o para os dados do Titanic que trabalhamos anteriormente, no Laborat\u00f3rio 2"}}