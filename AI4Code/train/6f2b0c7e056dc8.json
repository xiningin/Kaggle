{"cell_type":{"2ccdb5eb":"code","67d2b5c5":"code","03a7aca0":"code","2a5234e4":"code","dfd701aa":"code","ca4d19f4":"code","777781d2":"code","007db93a":"code","09a053a8":"code","035dfe89":"code","639baa5e":"code","e0052fac":"code","470b04f9":"code","dd5b8042":"code","1d97d7bb":"code","92f85360":"code","f614ecc7":"code","077a7750":"code","d5daba60":"code","737df8c4":"code","e09497fd":"code","92ef9194":"code","93c418bd":"code","46a871dc":"code","0d81f26e":"code","1771be2d":"code","48a027ef":"code","8c240f36":"code","1452d4e5":"code","52aeabb7":"code","25aa9f52":"code","bf7c3362":"code","68277d76":"code","b1cc17a4":"code","b1ee3512":"code","bfe41d5f":"code","3395eaea":"code","80a175ab":"code","0408b912":"code","3cdd2997":"code","f3982bfc":"code","e085681c":"code","471d501a":"code","4abe33d9":"code","8e731d89":"markdown","b97b6b51":"markdown","8ce28a1f":"markdown","fa138d15":"markdown","fef5fb28":"markdown","8027495a":"markdown","0c0c4a01":"markdown","3ab6606d":"markdown","b0591433":"markdown"},"source":{"2ccdb5eb":"import matplotlib\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm\n\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.datasets import make_blobs\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom ipywidgets import interactive\n\nfrom collections import defaultdict\n\nimport hdbscan\nimport folium\nimport re\n\n\ncols = ['#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4',\n        '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', '#008080', '#e6beff', \n        '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', \n        '#000075', '#808080']*10","67d2b5c5":"import pandas as pd\ndf = pd.read_csv('..\/input\/geographical-loc\/taxi_data.csv')","03a7aca0":"df.head()","2a5234e4":"df.duplicated(subset=['LON', 'LAT']).values.any()","dfd701aa":"df.isna().values.any()","ca4d19f4":"print(f'Before dropping NaNs and dupes\\t:\\tdf.shape = {df.shape}')\ndf.dropna(inplace=True)\ndf.drop_duplicates(subset=['LON', 'LAT'], keep='first', inplace=True)\nprint(f'After dropping NaNs and dupes\\t:\\tdf.shape = {df.shape}')","777781d2":"df.head()","007db93a":"X = np.array(df[['LON', 'LAT']], dtype='float64')","09a053a8":"plt.scatter(X[:,0], X[:,1], alpha=0.2, s=50)","035dfe89":"m = folium.Map(location=[df.LAT.mean(), df.LON.mean()], zoom_start=9, \n               tiles='Stamen Toner')\n\nfor _, row in df.iterrows():\n    folium.CircleMarker(\n        location=[row.LAT, row.LON],\n        radius=5,\n        popup=re.sub(r'[^a-zA-Z ]+', '', row.NAME),\n        color='#1787FE',\n        fill=True,\n        fill_colour='#1787FE'\n    ).add_to(m)","639baa5e":"m","e0052fac":"X_blobs, _ = make_blobs(n_samples=1000, centers=10, n_features=2, \n                        cluster_std=0.5, random_state=4)","470b04f9":"plt.scatter(X_blobs[:,0], X_blobs[:,1], alpha=0.2)","dd5b8042":"import numpy as np\nclass_predictions = np.load('..\/input\/geographical-loc1\/sample_clusters.npy')","1d97d7bb":"unique_clusters = np.unique(class_predictions)\nfor unique_cluster in unique_clusters:\n    X = X_blobs[class_predictions==unique_cluster]\n    plt.scatter(X[:,0], X[:,1], alpha=0.2, c=cols[unique_cluster])","92f85360":"silhouette_score(X_blobs, class_predictions)","f614ecc7":"class_predictions = np.load('..\/input\/geographicalloc\/sample_clusters_improved.npy')\nunique_clusters = np.unique(class_predictions)\nfor unique_cluster in unique_clusters:\n    X = X_blobs[class_predictions==unique_cluster]\n    plt.scatter(X[:,0], X[:,1], alpha=0.2, c=cols[unique_cluster])","077a7750":"silhouette_score(X_blobs, class_predictions)","d5daba60":"X_blobs, _ = make_blobs(n_samples=1000, centers=50, \n                        n_features=2, cluster_std=1, random_state=4)","737df8c4":"data = defaultdict(dict)\nfor x in range(1,21):\n    model = KMeans(n_clusters=3, random_state=17, \n                   max_iter=x, n_init=1).fit(X_blobs)\n    \n    data[x]['class_predictions'] = model.predict(X_blobs)\n    data[x]['centroids'] = model.cluster_centers_\n    data[x]['unique_classes'] = np.unique(class_predictions)","e09497fd":"def f(x):\n    class_predictions = data[x]['class_predictions']\n    centroids = data[x]['centroids']\n    unique_classes = data[x]['unique_classes']\n\n    for unique_class in unique_classes:\n            plt.scatter(X_blobs[class_predictions==unique_class][:,0], \n                        X_blobs[class_predictions==unique_class][:,1], \n                        alpha=0.3, c=cols[unique_class])\n    plt.scatter(centroids[:,0], centroids[:,1], s=200, c='#000000', marker='v')\n    plt.ylim([-15,15]); plt.xlim([-15,15])\n    plt.title('How K-Means Clusters')\n\ninteractive_plot = interactive(f, x=(1, 20))\noutput = interactive_plot.children[-1]\noutput.layout.height = '350px'\ninteractive_plot","92ef9194":"X = np.array(df[['LON', 'LAT']], dtype='float64')\nk = 70\nmodel = KMeans(n_clusters=k, random_state=17).fit(X)\nclass_predictions = model.predict(X)\ndf[f'CLUSTER_kmeans{k}'] = class_predictions","93c418bd":"df.head()","46a871dc":"def create_map(df, cluster_column):\n    m = folium.Map(location=[df.LAT.mean(), df.LON.mean()], zoom_start=9, tiles='Stamen Toner')\n\n    for _, row in df.iterrows():\n\n        if row[cluster_column] == -1:\n            cluster_colour = '#000000'\n        else:\n            cluster_colour = cols[row[cluster_column]]\n\n        folium.CircleMarker(\n            location= [row['LAT'], row['LON']],\n            radius=5,\n            popup= row[cluster_column],\n            color=cluster_colour,\n            fill=True,\n            fill_color=cluster_colour\n        ).add_to(m)\n        \n    return m\n\nm = create_map(df, 'CLUSTER_kmeans70')\nprint(f'K={k}')\nprint(f'Silhouette Score: {silhouette_score(X, class_predictions)}')\n\nm.save('kmeans_70.html')","0d81f26e":"m","1771be2d":"best_silhouette, best_k = -1, 0\n\nfor k in tqdm(range(2, 100)):\n    model = KMeans(n_clusters=k, random_state=1).fit(X)\n    class_predictions = model.predict(X)\n    \n    curr_silhouette = silhouette_score(X, class_predictions)\n    if curr_silhouette > best_silhouette:\n        best_k = k\n        best_silhouette = curr_silhouette\n        \nprint(f'K={best_k}')\nprint(f'Silhouette Score: {best_silhouette}') ","48a027ef":"# code for indexing out certain values\ndummy = np.array([-1, -1, -1, 2, 3, 4, 5, -1])\n\nnew = np.array([(counter+2)*x if x==-1 else x for counter, x in enumerate(dummy)])","8c240f36":"model = DBSCAN(eps=0.01, min_samples=5).fit(X)\nclass_predictions = model.labels_\n\ndf['CLUSTERS_DBSCAN'] = class_predictions","1452d4e5":"m = create_map(df, 'CLUSTERS_DBSCAN')\n\n    \nprint(f'Number of clusters found: {len(np.unique(class_predictions))}')\nprint(f'Number of outliers found: {len(class_predictions[class_predictions==-1])}')\n\nprint(f'Silhouette ignoring outliers: {silhouette_score(X[class_predictions!=-1], class_predictions[class_predictions!=-1])}')\n\nno_outliers = 0\nno_outliers = np.array([(counter+2)*x if x==-1 else x for counter, x in enumerate(class_predictions)])\nprint(f'Silhouette outliers as singletons: {silhouette_score(X, no_outliers)}')","52aeabb7":"m","25aa9f52":"model = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=2, \n                        cluster_selection_epsilon=0.01)\n#min_cluster_size\n#min_samples\n#cluster_slection_epsilon\n\nclass_predictions = model.fit_predict(X)\ndf['CLUSTER_HDBSCAN'] = class_predictions","bf7c3362":"m = create_map(df, 'CLUSTER_HDBSCAN')\n\nprint(f'Number of clusters found: {len(np.unique(class_predictions))-1}')\nprint(f'Number of outliers found: {len(class_predictions[class_predictions==-1])}')\n\nprint(f'Silhouette ignoring outliers: {silhouette_score(X[class_predictions!=-1], class_predictions[class_predictions!=-1])}')\n\nno_outliers = np.array([(counter+2)*x if x==-1 else x for counter, x in enumerate(class_predictions)])\nprint(f'Silhouette outliers as singletons: {silhouette_score(X, no_outliers)}')\n\nm","68277d76":"hdbscan.HDBSCAN?","b1cc17a4":"classifier = KNeighborsClassifier(n_neighbors=1)","b1ee3512":"df_train = df[df.CLUSTER_HDBSCAN!=-1]\ndf_predict = df[df.CLUSTER_HDBSCAN==-1]","bfe41d5f":"X_train = np.array(df_train[['LON', 'LAT']], dtype='float64')\ny_train = np.array(df_train['CLUSTER_HDBSCAN'])\n\nX_predict = np.array(df_predict[['LON', 'LAT']], dtype='float64')","3395eaea":"classifier.fit(X_train, y_train)","80a175ab":"predictions = classifier.predict(X_predict)","0408b912":"df['CLUSTER_hybrid'] = df['CLUSTER_HDBSCAN']","3cdd2997":"df.loc[df.CLUSTER_HDBSCAN==-1, 'CLUSTER_hybrid'] = predictions","f3982bfc":"m = create_map(df, 'CLUSTER_hybrid')","e085681c":"m","471d501a":"class_predictions = df.CLUSTER_hybrid\nprint(f'Number of clusters found: {len(np.unique(class_predictions))}')\nprint(f'Silhouette: {silhouette_score(X, class_predictions)}')\n\nm.save('hybrid.html')","4abe33d9":"df['CLUSTER_hybrid'].value_counts().plot.hist(bins=70, alpha=0.4, \n                                              label='Hybrid')\ndf['CLUSTER_kmeans70'].value_counts().plot.hist(bins=70, alpha=0.4,\n                                               label='K-Means (70)')\nplt.legend()\nplt.title('Comparing Hybrid and K-Means Approaches')\nplt.xlabel('Cluster Sizes')","8e731d89":"<a id='task6'><\/a>\n# Task 6: HDBSCAN\nHierarchical DBSCAN","b97b6b51":"<a id='task3'><\/a>\n# Task 3: Clustering Strength \/ Performance Metric","8ce28a1f":"# Clustering Geolocation Data Intelligently in Python\nWe have taxi rank locations, and want to define key clusters of these taxis where we can build service stations for all taxis operating in that region.\n\n## Prerequisites\n- Basic Matplotlib skills for plotting 2-D data clearly.\n- Basic understanding of Pandas and how to use it for data manipulation.\n- The concepts behind clustering algorithms, although we will go through this throughout the project.\n\n## Project Outline\n\n[**Task 1**](#task1): Exploratory Data Analysis\n\n[**Task 2**](#task2): Visualizing Geographical Data\n\n[**Task 3**](#task3): Clustering Strength \/ Performance Metric\n\n[**Task 4**](#task4): K-Means Clustering\n\n[**Task 5**](#task5): DBSCAN\n\n[**Task 6**](#task6): HDBSCAN\n\n[**Task 7**](#task7): Addressing Outliers\n\n[**Further Reading**](#further)","fa138d15":"<a id='further'><\/a>\n# Further Reading\n\nFor some additional reading, feel free to check out [K-Means](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.KMeans.html), [DBSCAN](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.DBSCAN.html), and [HDBSCAN](https:\/\/hdbscan.readthedocs.io\/en\/latest\/) clustering respectively.\n\nIt may be of use to also check out [other forms of clustering](https:\/\/scikit-learn.org\/stable\/modules\/clustering.html) that are commonly used and available in the scikit-learn library. HDBSCAN documentation also includes [a good methodology](https:\/\/hdbscan.readthedocs.io\/en\/latest\/comparing_clustering_algorithms.html) for choosing your clustering algorithm based on your dataset and other limiting factors.","fef5fb28":"<a id='task7'><\/a>\n# Task 7: Addressing Outliers\n","8027495a":"<a id='task5'><\/a>\n# Task 5: DBSCAN \nDensity-Based Spatial Clustering of Applications with Noise","0c0c4a01":"<a id='task2'><\/a>\n# Task 2: Visualizing Geographical Data\n","3ab6606d":"<a id='task1'><\/a>\n# Task 1: Exploratory Data Analysis","b0591433":"<a id='task4'><\/a>\n# Task 4: K-Means Clustering"}}