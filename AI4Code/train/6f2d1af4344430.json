{"cell_type":{"0dd6885d":"code","33b362a1":"code","11242f82":"code","c9f6ef70":"code","86937edc":"code","49c62867":"code","a2c6853a":"code","85e0a89c":"code","f1d4d06c":"code","cabf7475":"code","da5ac55a":"code","18d7494b":"code","530835a4":"code","283e20ba":"code","29fdb6b3":"code","6d4b3e55":"code","becf2f8a":"code","573a1aa6":"code","6f18adbb":"code","183b8f88":"code","6bfead6c":"code","1f9fd313":"code","72139ae8":"code","1803d676":"code","d5447540":"code","a615a3f6":"code","3f2b5b60":"code","d7359856":"code","84b00b91":"code","d4232963":"code","a62e949b":"markdown","603ca642":"markdown","39dc61a6":"markdown","3675225e":"markdown","ecc58040":"markdown","b29086da":"markdown","b68c217a":"markdown","878777df":"markdown","75450421":"markdown","ee32dd5a":"markdown","e6f070a9":"markdown","70e2951c":"markdown","084a29f4":"markdown","43e08af1":"markdown","1a9b1291":"markdown","55675688":"markdown","13f6ddbb":"markdown","3b9725be":"markdown","68517a47":"markdown","18919fa9":"markdown"},"source":{"0dd6885d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","33b362a1":"#loading the training data\nhousing_filepath_train = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv'\ntrain_data = pd.read_csv(housing_filepath_train, index_col = 'Id')\n\n#load the test data\nhousing_filepath_test = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv'\ntest_data = pd.read_csv(housing_filepath_test, index_col = 'Id')","11242f82":"#Tabulate the info related to train and test data\nprint(\"The detail information viz. dtype, variables and count for training data:\\n\")\nprint(train_data.info())\n\n#print(\"The detail information viz. dtype, variables and count for test data:\\n\")\n#print(test_data.info())","c9f6ef70":"#List our first few lines of the training data\ntrain_data.head()","86937edc":"#Lets visulaize the variation of target variable \n\nimport matplotlib.pyplot as plt\nplt.hist(train_data[\"SalePrice\"], 20, color = \"blue\", lw = 0.89, ec = 'black')\nplt.xlabel('Sale Price')\nplt.ylabel('frequency')\nplt.title('Distribution of Sale Price of House')","49c62867":"#The details for the target varaiable\nmax_sp = train_data['SalePrice'].max()\nprint(\"The largest Sale Price:\", max_sp,'USD')\nprint(\"The # of houses, having largest SalePrice:\", sum(train_data['SalePrice']==max_sp))","a2c6853a":"#For now lets deal with the numeric data\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#plt.figure(figsize= (14,7))\n\nfeautre = [\"LotArea\", \"YearBuilt\",\"YrSold\", \"GrLivArea\", \"MSSubClass\"]\nX_train = train_data[feautre]\ny_train = train_data[\"SalePrice\"]\n#sns.heatmap(data=X_train, annot = True)","85e0a89c":"#plot the barchart\nplt.figure(figsize=(20,6))\nplt.xticks(rotation=90)\nsns.barplot(x = X_train[\"YearBuilt\"], y = y_train)","f1d4d06c":"#Plot showing the variation of Saleprice with respect to Living Area\nsns.scatterplot(x = X_train['GrLivArea'], y = y_train)","cabf7475":"# Testing the data one of the given model\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(random_state=0)\n\n# Train the model (will take about 10 minutes to run)\nmodel.fit(X_train, y_train)","da5ac55a":"from sklearn.metrics import mean_absolute_error\npredicted_Sale_Price = model.predict(X_train)\nmean_absolute_error(y_train, predicted_Sale_Price)","18d7494b":"print(\"predicted_home_price\")\nprint(y_train.head())","530835a4":"from sklearn.model_selection import train_test_split\nX_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, random_state = 0)\n\n#Fit the data with a model\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(random_state=1)\n\nmodel.fit(X_train1, y_train1)\npredict_val = model.predict(X_val)\nprint(mean_absolute_error(y_val, predict_val))","283e20ba":"#print(predict_val)\ny_train.head()","29fdb6b3":"from sklearn.model_selection import train_test_split\nX_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, train_size= 0.8, test_size = 0.2, random_state = 0)\n\n#Fit the data with a model\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nmodel1 = RandomForestRegressor(n_estimators = 50, random_state=0)\nmodel2 = RandomForestRegressor(n_estimators=100, random_state=0)\nmodel3 = RandomForestRegressor(n_estimators=200, random_state=0)\nmodel4 = RandomForestRegressor(n_estimators=200, max_depth=5, random_state=0)\nmodel5 = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=0)\nmodel6 = RandomForestRegressor(n_estimators=200, max_depth=7, random_state=0)\n\nmodels = [model1, model2, model3, model4, model5, model6]\n      \ndef model_fit(model, Xt = X_train1, Xv = X_val, yt= y_train1, yv = y_val):\n    model.fit(Xt, yt)\n    predict_val = model.predict(Xv)\n    return(mean_absolute_error(yv, predict_val))\n\nfor i in range(0, len(models)):\n    mae = model_fit(models[i])\n    print(\"Model %d MAE: %d\" % (i+1, mae))","6d4b3e55":"import numpy as np\nMAE = [23062, 22670, 22392, 23300, 23537, 22423]\nmin_MAE = min(MAE)\nk = MAE.index(min_MAE)\nbest_model = models[k]\nprint(\"Best MAE:\", best_model)","becf2f8a":"X_test = test_data[feautre]\nX_test.head()","573a1aa6":"Final_Prediction = model.predict(X_val)","6f18adbb":"## Check the missing values\ntest_data.columns[test_data.isnull().any()]","183b8f88":"#missing value counts in each of these columns\nmiss = test_data.isnull().sum()\/len(test_data)\nmiss = miss[miss>0]\nmiss.sort_values(inplace = True)\nmiss","6bfead6c":"#Visualizing the missing values\nmiss = miss.to_frame()\nmiss.columns = ['count']\nmiss.index.names = ['Name']\nmiss['Name'] = miss.index\n#Plot the missing value count\nsns.set(style = 'whitegrid', color_codes = True)\nsns.barplot(x = 'Name', y = 'count', data = miss)\nplt.xticks(rotation = 90)\nplt.show()","1f9fd313":"#Lets separate the data into new data types\nnumeric_data = test_data.select_dtypes(include = [np.number])\ncat_data = test_data.select_dtypes(exclude = [np.number])\nprint(\"There are {} numeric and {} categorical columns in train data\".format(numeric_data.shape[1], cat_data.shape[1]))","72139ae8":"corr = numeric_data.corr()\nsns.heatmap(corr)","1803d676":"X_test.columns","d5447540":"X_test.columns[X_test.isnull().any()]","a615a3f6":"X_test.info","3f2b5b60":"#X_test.drop(X_test[X_test['GrLivArea']>4000].index, inplace=True)\n#X_test.shape","d7359856":"#The variable with no values are completely deleted\n#del X_test['TotalBsmtSF']","84b00b91":"X_test.columns","d4232963":"predicted_val = best_model.predict(X_test)\nprint(predicted_val)\nplt.hist(predict_val, 100, color = 'red')\nplt.xlabel('SalePrice')\nplt.ylabel('frequency')","a62e949b":"#### Check the missing datas in the Xtest","603ca642":"## Model Validation\n\nThe purpose of this stage is to test the suitability of a model for the fitting the data. The metric named \"Mean Absolute Error\" is commonly used to predict the suitability of a model, often called MAE.\n\nErrror = Absolute Value - Predicted Value. \n\nThe average of \"Error\" gives the value of mean absolute error.","39dc61a6":"The variable PoolQC has 99.76% of missing values, followed by MiscFeautre, Alley, Fence and FireplaceQu and so on","3675225e":"The prediction is probably a good prediction, because the predicted value is close to the given value. Note that this was completely done on the training dataset. In order to check the accuracy of a model we must try to check this with with some other datasests. \n\nThe common practice to check is to split the training data into training data and validataion data. Then test the data with the model. The function train_test_split from the scikit-learn library is used to split the data usually it is in ratio 80% and 20%.","ecc58040":"#### Data pre-processing","b29086da":"The prediction is close to the actual value,so our model is Ok to proceed. However, it is worth mentioning, the prediction can be improved by - \n\n(i) either fine tuning the parameter of a model,\n\n(ii) changing the model itself or \n\n(iii) changing the right feautres for fitting. ","b68c217a":"Now we are interested to learn about the correlation behaviour of numeric values. Some of them might be correlated. We can remove the correlated variable as they do not provide any useful information.","878777df":"#### Next stage is to clean data and remove the inputs with NaN or empty cells\n\n## 3. Cleaning Data","75450421":"### 1. Loading Data\nFirst I upload the data into the notebook.","ee32dd5a":"#### Playing with Random Forest Regressor model","e6f070a9":"It is appareent that the saleprice varies non-monoticall with the year built, few spikes in saleprice can be seen. But this could be due the bigger size houses, so we do need to consider other feautres too to tell something about the data. Howwever, the prices for the houses built after 1990 almost increases consisitenly, irrespective of other feautre.","70e2951c":"#### Prediction of saleprice with test sample","084a29f4":"## Brief discussion on problem statement: \nWe are given with a datset sharing the relevant details of the residential homes in Ames, Iowa. This datesets comprises of total 79 variables relevant to the houses in that locality. The list of variables are as follows:\n\n##### 'MSSubClass','MSZoning', 'LotFrontage', 'LotArea', 'Street', 'Alley','LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope','Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle','OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle','RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea','ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2','BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC','CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF','GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath','BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd','Functional', 'Fireplaces', 'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond','PavedDrive', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch','ScreenPorch', 'PoolArea', 'PoolQC', 'Fence', 'MiscFeature', 'MiscVal','MoSold', 'YrSold', 'SaleType', 'SaleCondition', 'SalePrice'\n\nNote that we are already given with train and test dataset with the target variable being SalePrice.\n\n## Goal:\nOur job is to predict the house price based from the test data. ","43e08af1":"#### Lets select the model with least mae","1a9b1291":"## 2. Data Visualization","55675688":"#### List of Tasks\n1. Loading Data\n2. Data Visualization \n3. Data Cleaning, Encoding and Handling Missing values\n4. Feautre Engineering\n5. Omptimizing and Solving the problem","13f6ddbb":"#### Separating the numeric and the categorical data","3b9725be":"From the above plot it is apparent that the houses with either very low or very small \"GrLivArea\" has lower Saleprices. While at an intermediate range the price is extremely high.","68517a47":"Remove outliers ","18919fa9":"In the test dataset out of 79 datasets 32 of them has the missing values"}}