{"cell_type":{"e85b7c46":"code","a748023e":"code","7ae8345e":"code","a94064bf":"code","608aa04d":"code","4d9494ac":"code","7498daf2":"code","c6b0953e":"code","73a298fc":"code","8cab9d65":"code","771e9bbb":"code","9f5781f3":"code","d8d7fc18":"code","3a33e9c9":"code","2d1563d1":"code","9a15f975":"code","ed24ad58":"code","12ac0560":"code","da573927":"code","c07fa41b":"code","62e61824":"code","995dc5b7":"code","be31dca1":"code","19dc0090":"code","cfcdf684":"code","6fcba81b":"code","ff8ca26d":"code","f29d2e42":"code","71f7ba00":"code","e14fd514":"code","05d17d34":"code","f9482a3d":"code","8979616c":"code","b9b7fa2f":"code","aff2e5c1":"code","dfe6e2b6":"code","1ba22f87":"code","a0a3423a":"code","40190b60":"code","24c25a47":"code","49bb9225":"code","26931fca":"code","16d7d5b8":"code","8f5da7bc":"code","ee459ff0":"code","1e39e7ef":"code","ee829690":"code","3cb5fe61":"code","b4df2f99":"code","ed9378e4":"code","8a11b374":"code","bdbe922b":"code","6e4eab44":"code","af5e067e":"code","24a7599c":"code","0a466c01":"code","74f5dd99":"code","57ae9a3d":"code","88164da1":"code","be4aa320":"code","9d3091b7":"code","6d1f1d51":"code","3cbcec24":"code","cf7f4be3":"code","e39db5b3":"code","75c6ae26":"code","66967ca2":"markdown","bcbfe26c":"markdown","78471ae0":"markdown","ddee2afb":"markdown","e115a4a9":"markdown","f07c9852":"markdown","a03177c6":"markdown","805becfd":"markdown","b247d96a":"markdown","e53edc3e":"markdown","ed5ad863":"markdown","241468c7":"markdown","b678755b":"markdown","4416b451":"markdown","5bf97cce":"markdown","e9c77ce1":"markdown","eadb6f99":"markdown","53ef2db4":"markdown","0975752e":"markdown","41b488a0":"markdown","8a36c3f2":"markdown","9620763e":"markdown","0bb2dae5":"markdown","73965ce7":"markdown","7a80e6c6":"markdown","b7308958":"markdown","0ccdfb37":"markdown","ae23602c":"markdown","1ad5d80d":"markdown","1cfb18a4":"markdown","93a10966":"markdown","c1a9c6c3":"markdown","042e9b53":"markdown","14fc9340":"markdown","101b463b":"markdown","f5d71ba3":"markdown","60452076":"markdown","d079ca89":"markdown","338e52e8":"markdown","0cd12544":"markdown","e483bcff":"markdown","698463cf":"markdown","9c5c7413":"markdown","ec84ce77":"markdown","bc5414b9":"markdown","3a8300bd":"markdown","1d961be1":"markdown","71d7c86f":"markdown","891ac6fa":"markdown","3bb52930":"markdown","48e4bf5e":"markdown","db174aef":"markdown","dc3cf6f3":"markdown","31734ae6":"markdown","7f40b311":"markdown","5d1b6105":"markdown","e57629c0":"markdown","c50921d8":"markdown","e9eb906c":"markdown","7470aa26":"markdown","f54994ed":"markdown","0f66b49d":"markdown","c735bef0":"markdown","7d4a0f54":"markdown","905abdcc":"markdown","26478927":"markdown","a4fe6336":"markdown","f2845713":"markdown","46edfcd4":"markdown"},"source":{"e85b7c46":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport category_encoders as ce\nimport time\nfrom sklearn import svm\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_curve\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.naive_bayes import CategoricalNB, BernoulliNB, MultinomialNB, GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.preprocessing import label_binarize\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\n","a748023e":"df = pd.read_csv(\"..\/input\/us-accidents\/US_Accidents_Dec20.csv\")\ndf.head() ","7ae8345e":"print(list(df.columns.values)) ","a94064bf":"severity_counts = df[\"Severity\"].value_counts()\n\nplt.figure(figsize=(10, 8))\nplt.title(\"Histogram for the severity\")\nsns.barplot(x = severity_counts.index,y = severity_counts.values)\nsns.set(style=\"darkgrid\")\nsns.set_context(\"talk\")\nplt.xlabel(\"Severity\")\nplt.ylabel(\"Number of Accidents\")\nplt.show()\nprint(severity_counts)","608aa04d":"state_counts = df[\"State\"].value_counts()[:20]\nplt.figure(figsize=(20, 10))\nplt.title(\"State wise distribution of accidents (Top 20 from the list)\")\n\n# Create a pieplot\nplt.pie(state_counts.values, labels=state_counts.index, autopct='%1.1f%%', shadow=True)\n\n# add a circle at the center to transform it in a donut chart\nmy_circle=plt.Circle((0,0), 0.6 , color='white')\np=plt.gcf()\np.gca().add_artist(my_circle)\n\nplt.show()\n","4d9494ac":"road_features = [\"Amenity\", \"Bump\", \"Crossing\", \"Give_Way\", \"Junction\", \"No_Exit\", \"Railway\", \"Roundabout\", \"Station\", \"Stop\", \"Traffic_Calming\", \"Traffic_Signal\", \"Turning_Loop\"]\ndata = df[road_features].sum().sort_values(ascending = False)\n\nplt.figure(figsize=(20, 10))\nplt.title(\"relation between road features and accidents\")\nsns.set(style=\"darkgrid\")\nsns.set_context(\"talk\")\nsns.barplot(x = data.values,y = data.index, orient=\"h\")\nplt.xlabel(\"Number of Accidents\")\nplt.ylabel(\"Road features\")\nplt.show()","7498daf2":"stop = stopwords.words(\"english\") + [\"-\"]\n\ndf_s4_desc = df[df[\"Severity\"] == 4][\"Description\"]\n# Split the description\ndf_words = df_s4_desc.str.lower().str.split(expand=True).stack()\n\n# If the word is not in the stopwords list\ncounts = df_words[~df_words.isin(stop)].value_counts()[:10]\n\nplt.figure(figsize=(20, 10))\nplt.title(\"Top 10 words used to describe an accident with severity 4\")\nsns.set(style=\"darkgrid\")\nsns.set_context(\"talk\")\nsns.barplot(x = counts.values,y = counts.index, orient=\"h\")\nplt.xlabel(\"Number of Accidents\")\nplt.ylabel(\"Word\")\nplt.show()\n","c6b0953e":"counts = pd.to_datetime(df['Start_Time']).dt.day_name().value_counts(ascending = True)\nweekdays = ['Monday', \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n\nplt.figure(figsize = (20,10))\nplt.title(\"Day wise distribution of Accidents\")\nplt.pie(counts.values, labels=counts.index, autopct='%1.1f%%', shadow=True)\nplt.axis('equal')\nplt.show()\ncounts = 0","73a298fc":"severity_distance = df.groupby(\"Severity\").mean()[\"Distance(mi)\"]\nplt.figure(figsize=(18, 8))\nplt.title(\"Medium distance by severity\")\nsns.set(style=\"darkgrid\")\nsns.barplot(x =severity_distance.values,y = severity_distance.index,orient=\"h\", order=severity_distance.index)\nplt.xlabel(\"Distance (mi)\")\nplt.ylabel(\"Severity\")\nplt.show()\nseverity_distance = 0","8cab9d65":"counts = df[\"Weather_Condition\"].value_counts()[:10]\nplt.figure(figsize=(20, 10))\nplt.title(\"Histogram distribution of the top 10 weather conditions\")\nsns.set(style=\"darkgrid\")\nsns.barplot(x = counts.index,y = counts.values)\nplt.xlabel(\"Weather Condition\")\nplt.ylabel(\"Number of Accidents\")\nplt.show()\n\ncounts = 0","771e9bbb":"corr_matrix = df.corr()\n\nplt.figure(figsize=(12, 12))\nsns.heatmap(corr_matrix, vmin=-1, vmax=1, cmap=\"seismic\")\nplt.gca().patch.set(hatch=\"X\", edgecolor=\"#0080ff\")\nplt.show()\ncorr_matrix = 0","9f5781f3":"# Cast Start_Time to datetime\ndf[\"Start_Time\"] = pd.to_datetime(df[\"Start_Time\"])\n\n# Extract year, month, weekday, day, hour and minute\ndf[\"Year\"] = df[\"Start_Time\"].dt.year\ndf[\"Month\"] = df[\"Start_Time\"].dt.month\ndf[\"Weekday\"] = df[\"Start_Time\"].dt.weekday\ndf[\"Day\"] = df[\"Start_Time\"].dt.day\ndf[\"Hour\"] = df[\"Start_Time\"].dt.hour\ndf[\"Minute\"] = df[\"Start_Time\"].dt.minute\n\ndf.head()","d8d7fc18":"df.drop([\"ID\", \"Source\", \"TMC\", \"Country\", \"Description\",\"Start_Lat\", \"Start_Lng\", \"End_Lat\", \"End_Lng\", \"Wind_Chill(F)\", \"Traffic_Signal\", \"Traffic_Calming\", \"Turning_Loop\", \"Start_Time\", \"End_Time\", \"Street\", \"County\", \"State\", \"Zipcode\", \"Weather_Timestamp\"], axis = 1,inplace=True)\ndf.head()","3a33e9c9":"print(\"Number of rows:\", len(df.index))\ndf.drop_duplicates(inplace=True)\nprint(\"Number of rows after drop of duplicates:\", len(df.index))","2d1563d1":"df['Side'].value_counts()","9a15f975":"df = df[df['Side'] != \" \"]\ndf[\"Side\"].value_counts()","ed24ad58":"df[[\"Pressure(in)\", \"Visibility(mi)\"]].describe().round(3)\n","12ac0560":"df = df[df[\"Pressure(in)\"] != 0]\ndf = df[df[\"Visibility(mi)\"] != 0]\ndf[[\"Pressure(in)\", \"Visibility(mi)\"]].describe().round(3)","da573927":"unique_weather = df[\"Weather_Condition\"].unique()\nprint(len(unique_weather))\n\ndf.loc[df[\"Weather_Condition\"].str.contains(\"Thunder|T-Storm\", na=False), \"Weather_Condition\"] = \"Thunderstorm\"\ndf.loc[df[\"Weather_Condition\"].str.contains(\"Snow|Sleet|Wintry\", na=False), \"Weather_Condition\"] = \"Snow\"\ndf.loc[df[\"Weather_Condition\"].str.contains(\"Rain|Drizzle|Shower\", na=False), \"Weather_Condition\"] = \"Rain\"\ndf.loc[df[\"Weather_Condition\"].str.contains(\"Wind|Squalls\", na=False), \"Weather_Condition\"] = \"Windy\"\ndf.loc[df[\"Weather_Condition\"].str.contains(\"Hail|Pellets\", na=False), \"Weather_Condition\"] = \"Hail\"\ndf.loc[df[\"Weather_Condition\"].str.contains(\"Fair\", na=False), \"Weather_Condition\"] = \"Clear\"\ndf.loc[df[\"Weather_Condition\"].str.contains(\"Cloud|Overcast\", na=False), \"Weather_Condition\"] = \"Cloudy\"\ndf.loc[df[\"Weather_Condition\"].str.contains(\"Mist|Haze|Fog\", na=False), \"Weather_Condition\"] = \"Fog\"\ndf.loc[df[\"Weather_Condition\"].str.contains(\"Sand|Dust\", na=False), \"Weather_Condition\"] = \"Sand\"\ndf.loc[df[\"Weather_Condition\"].str.contains(\"Smoke|Volcanic Ash\", na=False), \"Weather_Condition\"] = \"Smoke\"\ndf.loc[df[\"Weather_Condition\"].str.contains(\"N\/A Precipitation\", na=False), \"Weather_Condition\"] = np.nan\n\nprint(df[\"Weather_Condition\"].unique())","c07fa41b":"df[\"Wind_Direction\"].unique()\nprint(len(df[\"Wind_Direction\"].unique()))","62e61824":"df.loc[df[\"Wind_Direction\"] == \"CALM\", \"Wind_Direction\"] = \"Calm\"\ndf.loc[df[\"Wind_Direction\"] == \"VAR\", \"Wind_Direction\"] = \"Variable\"\ndf.loc[df[\"Wind_Direction\"] == \"East\", \"Wind_Direction\"] = \"E\"\ndf.loc[df[\"Wind_Direction\"] == \"North\", \"Wind_Direction\"] = \"N\"\ndf.loc[df[\"Wind_Direction\"] == \"South\", \"Wind_Direction\"] = \"S\"\ndf.loc[df[\"Wind_Direction\"] == \"West\", \"Wind_Direction\"] = \"W\"\n\ndf[\"Wind_Direction\"] = df[\"Wind_Direction\"].map(lambda x : x if len(x) != 3 else x[1:], na_action=\"ignore\")\n\ndf[\"Wind_Direction\"].unique()","995dc5b7":"df.info()","be31dca1":"df.isna().sum()","19dc0090":"df.drop([\"Precipitation(in)\", \"Number\"], axis = 1, inplace = True)\nfeatures_to_fill = [\"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\", \"Wind_Speed(mph)\"]\ndf[features_to_fill] = df[features_to_fill].fillna(df[features_to_fill].mean())\ndf.dropna(inplace=True)\n\ndf.isna().sum()","cfcdf684":"df.describe().round(3)","6fcba81b":"size = len(df[df[\"Severity\"]==4].index)\nX = pd.DataFrame()\nfor i in range(1,5):\n    S = df[df[\"Severity\"]==i]\n    if i == 1:\n        new_size = len(df[df[\"Severity\"]==i].index)\n        X = X.append(S.sample(new_size, random_state=21))\n    else:\n        X = X.append(S.sample(size, random_state=21))\ndf = X\nseverity_counts = df[\"Severity\"].value_counts()\nplt.figure(figsize=(10, 8))\nplt.title(\"Histogram for the severity\")\nsns.barplot(x = severity_counts.index,y =  severity_counts.values)\nplt.xlabel(\"Severity\")\nplt.ylabel(\"Number of Accidents\")\nplt.show()\ndel X\ndel S\n\nseverity_counts = df[\"Severity\"].value_counts()\nprint(\"\\n\",severity_counts)","ff8ca26d":"scaler = MinMaxScaler()\nfeatures = ['Temperature(F)','Distance(mi)','Humidity(%)','Pressure(in)','Visibility(mi)','Wind_Speed(mph)','Year', 'Month','Weekday','Day','Hour','Minute']\ndf[features] = scaler.fit_transform(df[features])\ndf.describe().round(3)","f29d2e42":"categorical_features = set([\"Side\",\"City\", \"Timezone\", \"Airport_Code\",\"Weather_Condition\",\"Wind_Direction\",\"Sunrise_Sunset\",\"Civil_Twilight\",\"Nautical_Twilight\",\"Astronomical_Twilight\"])\n\nfor cat in categorical_features:\n    df[cat] = df[cat].astype(\"category\")\n\ndf.info()\n\nprint(\"\\nUnique classes for each categorical feature:\")\nfor cat in categorical_features:\n    print(\"{:15s}\".format(cat), \"\\t\", len(df[cat].unique()))","71f7ba00":"# Remove city because it will be encoded later\nonehot_cols = categorical_features - set([\"City\", \"Airport_Code\"])\ndf = pd.get_dummies(df, columns=onehot_cols, drop_first=True)\ndf.head()","e14fd514":"binary_encoder = ce.binary.BinaryEncoder()\n\ncity_binary_enc = binary_encoder.fit_transform(df[\"City\"])\nprint(city_binary_enc)\n","05d17d34":"binary_encoder = ce.binary.BinaryEncoder()\n\nairport_binary_enc = binary_encoder.fit_transform(df[\"Airport_Code\"])\nprint(airport_binary_enc)","f9482a3d":"df = pd.concat([df, city_binary_enc, airport_binary_enc], axis=1).drop([\"City\", \"Airport_Code\"], axis=1)\n# df = pd.concat([df, airport_binary_enc], axis=1).drop(\"Airport_Code\", axis=1)\ncity_binary_enc = 0\nairport_binary_enc = 0\ndf.head()","8979616c":"df.replace([True, False], [1, 0], inplace = True)\n\ndf.head()","b9b7fa2f":"# Metrics dictionary\naccuracy = dict()\nf1 = dict()","aff2e5c1":"X, X_test = train_test_split(df, test_size=.1, random_state=21)\ny_test = X_test[\"Severity\"]\nX_test.drop([\"Severity\"], axis = 1, inplace = True)\nprint(X.shape, X_test.shape, y_test.shape)\ndel df\n# X_test.to_csv('.\/X_test.csv',index=False)\n# y_test.to_csv('.\/y_test.csv',index=False)\n# import os\n# os.chdir(r'..\/working')\n# from IPython.display import FileLink\n# FileLink(r'X_test.csv')","dfe6e2b6":"# y = X[\"Severity\"]\n# X.drop([\"Severity\"], axis = 1, inplace = True)\n\n# #Starting the timer\n# start_time = time.perf_counter()\n\n# #Initial distribution\n# counter = Counter(y)\n# print(counter)\n\n# #Final Distribution\n# oversample = SMOTE()\n# X, y = oversample.fit_resample(X, y)\n\n# counter = Counter(y)\n# print(\"\\n\", counter)\n\n# for k,v in counter.items():\n#     per = v \/ len(y) * 100\n#     print('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n    \n# #Ending the timer\n# end_time = time.perf_counter()\n# total_time = end_time-start_time\n\n# print(\"It took {} secs for completing SMOTE on the training data set\".format(total_time))","1ba22f87":"# X.to_csv('.\/X.csv',index=False)\n# y.to_csv('.\/y.csv',index=False)","a0a3423a":"y = pd.read_csv(\"..\/input\/accidentseveritypredictionpreprocesseddataset\/y.csv\")\ny.head()\ny = y.values.ravel()","40190b60":"X = pd.read_csv(\"..\/input\/accidentseveritypredictionpreprocesseddataset\/X.csv\")\nX.head()","24c25a47":"# X_train, X_validate, y_train, y_validate = train_test_split(X, y, random_state=21)\n# print(X_train.shape, y_train.shape)\n# print(X_validate.shape, y_validate.shape)","49bb9225":"#Starting the timer\nstart_time = time.perf_counter()\n\nlr = LogisticRegression(n_jobs=-1)\nparams = {\"solver\": [\"saga\"], \"penalty\" : [\"l1\"]} #Params to check with\ngrid_search = GridSearchCV(lr, params, n_jobs=-1, verbose=5, cv=5, return_train_score = True)\ngrid_search.fit(X, y)\n\nprint(\"Best parameters scores:\")\nprint(grid_search.best_params_)\nprint(\"Mean Train Score: \", grid_search.cv_results_['mean_train_score'][0])\nprint(\"Mean Validation Score:\", grid_search.best_score_) #Mean cross-validation score of best estimator Estimated: 0.6304686540384669\nprint(\"Test Score: \", grid_search.best_estimator_.score(X_test,y_test)) # Estimated: 0.4016917344357712\n\n#Ending the timer\nend_time = time.perf_counter()\ntotal_time = end_time-start_time\n\nprint(\"It took {} secs for completing Logistic Regression training.\".format(total_time))","26931fca":"y_pred = grid_search.predict(X_test)\naccuracy[\"Logistic Regression\"] = accuracy_score(y_test, y_pred)\nf1[\"Logistic Regression\"] = f1_score(y_test, y_pred, average=\"macro\")\n\nprint(classification_report(y_test, y_pred))","16d7d5b8":"confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Logistic Regression\")\nplt.show()","8f5da7bc":"#Starting the timer\nstart_time = time.perf_counter()\n\ndf = pd.DataFrame(y, columns = ['Severity'])\nX_svm = pd.concat([X, df], axis = 1)\nsample = X_svm.sample(50_000, random_state=21)\ny_sample = sample[\"Severity\"]\nX_sample = sample.drop(\"Severity\", axis=1)\n\nsvc = svm.SVC(verbose=5, gamma='auto')\nparams = {'C': [1.0], 'degree':[2], 'kernel': ['rbf']} #Best params\ngrid_search = GridSearchCV(svc, params, n_jobs=-1, verbose=5, cv=5, return_train_score = True)\ngrid_search.fit(X_sample, y_sample)\n\nprint(\"Best parameters scores:\")\nprint(grid_search.best_params_)\nprint(\"Mean Train Score: \", grid_search.cv_results_['mean_train_score'][0])\nprint(\"Mean Validation score:\", grid_search.best_score_) #Mean cross-validation score of best estimator Expected: 0.6083000000000001\nprint(\"Test score: \", grid_search.best_estimator_.score(X_test,y_test))#Mean test score of best estimator Expected: 0.4185546821878315\n\n#Ending the timer\nend_time = time.perf_counter()\ntotal_time = end_time-start_time\n\nprint(\"It took {} secs for completing SVM training.\".format(total_time))","ee459ff0":"y_pred = grid_search.predict(X_test)\naccuracy[\"SVM\"] = accuracy_score(y_test, y_pred)\nf1[\"SVM\"] = f1_score(y_test, y_pred, average=\"macro\")\n\nprint(classification_report(y_test, y_pred))","1e39e7ef":"confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - SVM\")\nplt.show()","ee829690":"#Starting the timer\nstart_time = time.perf_counter()\n\ndtc = DecisionTreeClassifier()\nparams = [{\"criterion\": [\"gini\"], \"max_depth\": [25], \"splitter\" : [\"best\"]}]\ngrid_search = GridSearchCV(dtc, params, n_jobs=-1, verbose=5, cv=2, return_train_score = True)\ngrid_search.fit(X, y)\n\nprint(\"Best parameters scores:\")\nprint(grid_search.best_params_)\nprint(\"Mean Train Score:\", grid_search.cv_results_['mean_train_score'][0])\nprint(\"Mean Validation score:\", grid_search.best_score_) #Mean cross-validation score of best estimator Expected: 0.7351175989564066\nprint(\"Test score: \", grid_search.best_estimator_.score(X_test,y_test)) #Expected: 0.572551472788098\n\n#Ending the timer\nend_time = time.perf_counter()\ntotal_time = end_time-start_time\n\nprint(\"It took {} secs for completing Decision Tree training.\".format(total_time))","3cb5fe61":"y_pred = grid_search.predict(X_test)\naccuracy[\"Decision Tree\"] = accuracy_score(y_test, y_pred)\nf1[\"Decision Tree\"] = f1_score(y_test, y_pred, average=\"macro\")\n\nprint(classification_report(y_test, y_pred))","b4df2f99":"confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Decision Tree\")\nplt.show()","ed9378e4":"#Starting the timer\nstart_time = time.perf_counter()\n\nrfc = RandomForestClassifier(n_jobs=-1)\nparams = [{\"n_estimators\": [500], \"max_depth\": [35], \"criterion\" : [\"gini\"]}]\ngrid_search = GridSearchCV(rfc, params, n_jobs=-1, verbose=5, cv=2, return_train_score = True)\ngrid_search.fit(X, y)\n\nprint(\"Best parameters scores:\")\nprint(grid_search.best_params_)\nprint(\"Mean Train Score:\", grid_search.cv_results_['mean_train_score'][0])\nprint(\"Mean Validation score:\", grid_search.best_score_) #Mean cross-validation score of best estimator Expected: 0.7847685987422364\nprint(\"Test score: \", grid_search.best_estimator_.score(X_test,y_test))#Expected: 0.6068485326515626\n\n#Ending the timer\nend_time = time.perf_counter()\ntotal_time = end_time-start_time\n\nprint(\"It took {} secs for completing Decision Tree training.\".format(total_time))","8a11b374":"y_pred = grid_search.predict(X_test)\naccuracy[\"Random Forest\"] = accuracy_score(y_test, y_pred)\nf1[\"Random Forest\"] = f1_score(y_test, y_pred, average=\"macro\")\n\nprint(classification_report(y_test, y_pred))","bdbe922b":"confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Random Forest\")\nplt.show()","6e4eab44":"#Starting the timer\nstart_time = time.perf_counter()\n\nX_train, X_validate, y_train, y_validate = train_test_split(X, y, random_state=21)\n\nprint(X_train.shape, y_train.shape)\nprint(X_validate.shape, y_validate.shape)\n\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\n\nprint(\"Train score:\", gnb.score(X_train, y_train))\nprint(\"Validation score:\", gnb.score(X_validate, y_validate))\nprint(\"Test score:\", gnb.score(X_test, y_test))\n\n#Ending the timer\nend_time = time.perf_counter()\ntotal_time = end_time-start_time\n\nprint(\"It took {} secs for completing Decision Tree training.\".format(total_time))","af5e067e":"y_pred = gnb.predict(X_test)\naccuracy[\"Naive Bayes\"] = accuracy_score(y_test, y_pred)\nf1[\"Naive Bayes\"] = f1_score(y_test, y_pred, average=\"macro\")\n\nprint(classification_report(y_test, y_pred))","24a7599c":"confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Naive Bayes\")\nplt.show()","0a466c01":"mnb = MultinomialNB()\nmnb.fit(X_train, y_train)\n\nprint(\"Train score:\", mnb.score(X_train, y_train))\nprint(\"Validation score:\", mnb.score(X_validate, y_validate))\nprint(\"Test score:\", mnb.score(X_test, y_test))\n","74f5dd99":"y_pred = mnb.predict(X_test)\naccuracy[\"Multinomial Naive Bayes\"] = accuracy_score(y_test, y_pred)\nf1[\"Multinomial Naive Bayes\"] = f1_score(y_test, y_pred, average=\"macro\")\n\nprint(classification_report(y_test, y_pred))\n","57ae9a3d":"confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Multinomial Naive Bayes\")\nplt.show()","88164da1":"bnb = BernoulliNB()\nbnb.fit(X_train, y_train)\n\nprint(\"Train score:\", bnb.score(X_train, y_train))\nprint(\"Validation score:\", bnb.score(X_validate, y_validate))\nprint(\"Test score:\", bnb.score(X_test, y_test))","be4aa320":"y_pred = bnb.predict(X_test)\naccuracy[\"Bernoulli Naive Bayes\"] = accuracy_score(y_test, y_pred)\nf1[\"Bernoulli Naive Bayes\"] = f1_score(y_test, y_pred, average=\"macro\")\n\nprint(classification_report(y_test, y_pred))","9d3091b7":"confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Bernoulli Naive Bayes\")\nplt.show()","6d1f1d51":"#Starting the timer\nstart_time = time.perf_counter()\n\nmlp = MLPClassifier(random_state=42, verbose=False)\nparams = [{\"hidden_layer_sizes\": [(32, 64, 32)], \"max_iter\": [350], \"solver\": [\"adam\"], \"activation\":[\"relu\"]}]\ngrid_search = GridSearchCV(mlp, params, n_jobs=-1, verbose=5, cv=2, return_train_score = True)\ngrid_search.fit(X, y)\n\nprint(\"Best parameters scores:\")\nprint(grid_search.best_params_)\nprint(\"Mean Train Score:\", grid_search.cv_results_['mean_train_score'][0])\nprint(\"Mean Validation score:\", grid_search.best_score_) #Mean cross-validation score of best estimator Expected: 0.7486176281614454\nprint(\"Test score: \", grid_search.best_estimator_.score(X_test,y_test)) #Expected: 0.7084822722442277\n\n#Ending the timer\nend_time = time.perf_counter()\ntotal_time = end_time-start_time\n\nprint(\"It took {} secs for completing Multilayer Perceptron Training.\".format(total_time))","3cbcec24":"y_pred = grid_search.predict(X_test)\naccuracy[\"Multilayer Perceptron\"] = accuracy_score(y_test, y_pred)\nf1[\"Multilayer Perceptron\"] = f1_score(y_test, y_pred, average=\"macro\")\n\nprint(classification_report(y_test, y_pred))","cf7f4be3":"confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Multilayer Perceptron\")\nplt.show()","e39db5b3":"print(accuracy)\nprint(f1)","75c6ae26":"x = []\ny = []\nfor key, value in accuracy.items():\n    x.append(key)\n    y.append(value)\n\nplt.figure(figsize=(20, 10))\nplt.title(\"Bar Graph of accuracy of various models\")\nsns.set(style=\"darkgrid\")\nsns.barplot(x = x,y = y)\nplt.xlabel(\"Models\")\nplt.ylabel(\"Accuracy\")\nplt.show()","66967ca2":"# Checking the Day wise distribution of Accidents","bcbfe26c":"***Note: We are prviding random state inorder to have same shuffling or split of data whereever required. This keeps the accuracy same in all the runs.***","78471ae0":"***As the minimum value is 0.00. There must be some missing data which is replaced with zeros.***","ddee2afb":"We will try out the following models and compare their results:\n\n* Logistic Regression;\n* Support Vector Machine;\n* Decision Tree\n* Random Forest\n* Naive Bayes\n* Multi Layer Perceptron\n\nWe are going to seach the best hyperparameters to produce the best results for each model. In the end, we will show the performance of each model using different metrics: precision, recall, accuracy.","e115a4a9":"# Checking feature types\n**Checking the types of all the variables so that we can normalize the continuous data and encode th categorical data. Normalization will reduce the computation time of the model and also increase its accuracy. Feature encoding of categorical data is necessary as algorithms cannot work with Booleans and Strings.**","f07c9852":"**Dropping data with zeros in Pressure and Visibility**","a03177c6":"**We will split the data set into test and validation set using train_test_split method from model_selection method of sklearn library.**","805becfd":"# Using SMOTE\n**Oversamplying all the class except the major class to make them equal to the major class. We will use SMOTE only on training and validation data set**","b247d96a":"***As we can see most of the accidents are recorded during Traffic Signals***","e53edc3e":"**Carrying out Naive Bayes using BernoulliNB method of Naive Bayes**","ed5ad863":"# Distance (Length of road affected by accident) vs Severity\nLogically there should be a direct relation between the length of road affected by accident and the severity of accident. Lets check if that is case.","241468c7":"From the above matrix we can see the following: \n\n* (Start_Lat,End_Lat) and (Start_Long, End_Long) (GPS coordinates) of the accidents are highly correlated.\n* Wind_Chill and Temprature are highly negatively corelated\n* Traffic signal is slightly correlated to the severity of an accident but it is in itself is slightly correlated with many features.\n* TMC is slightly related to severity.\n* We couldn't compute the covariance with Turning_Loop because it's always False.\n* The GPS coordinates are also correlated with Temperature , Wind_Chill and slightly with Pressure, Humidity and Visibility.\n* Bump is highly correlated with Traffic_Calming. ","b678755b":"**Next we will use LogisticRegression method from linear_model method of sklearn library**","4416b451":"# Support Vector Machine\n","5bf97cce":"***The data is now clear from missing values***","e9c77ce1":"# Checking and Plotting the accuracy of all the models to compare them.","eadb6f99":"***Reducing the number of unique values from Wind_Direction***","53ef2db4":"# Feature Encoding\n**Encoding categorical features.**\n","0975752e":"# **Checking The Balance of the Data**","41b488a0":"***Binary encoding Airport_Code and City (to reduce memory usage) and One-Hot encoding the rest***","8a36c3f2":"# <u> ***Data Preprocessing*** <u>","9620763e":"**Analyzing missing values**","0bb2dae5":"# Checking relation between road features and accidents","73965ce7":"# **Number of Accidents per State**\nWith the following code we are going to create a map of each state of the US with a color based on the number of accidents present in the dataset for that state.","7a80e6c6":"# Dropping few columns :\n* ID : It is just used to give a uniquesness for every entry.\n* Source: It just contains information about API which reported the accident and not the actual accident.\n* TMC: because it could already contains information about the accident severity \n* Description: As it is just a natural language description of the accident.Elimated it maintain simplicity.\n* Country: As it has a constant value of US.\n* Start_Lat, Start_Lng, End_Lat, End_Lng : They are highly correlated with many other features. Hence dropping them.\n* Wind_Chill: Temperature and Wind_Chill are highly positively correlated. Hence dropping Wind_Chill.\n* Traffic_Signal: It is slightly correlated with many other features and highly correlated with Crossing.\n* Traffic_Calming: It is higly correlated with Bump.\n* Turning_Loop: It is False throughout the data.\n* Start_Time: Because it has been divided into Year, Month, Day ,etc.\n* End_Time: It does not makes sense to keep it as we never know in advance when the traffic flow condition is going to become regular again.\n* Street, County, State, Zipcode: because we just focus on the City where the accident happened\n* Weather_Timestamp: Because it does not provide any useful information.\n\n**We are dropping a feature from two highly correlated features as keeping both of them won't contribute significantly to the model but will only add to the noise** ","b7308958":"***As we can see the data is very unbalanced. We will see into it later.***","0ccdfb37":"# Checking feature variance\n**Checking the feature variance and removing the features with low variance and very high variance inorder to avoid overfitting or underfitting the data points considering the bias-variance tradeoff.**","ae23602c":"**Finally, converting boolean values to numerical**","1ad5d80d":"# Train\/Validation - Test split","1cfb18a4":"# <u>***Building Models***<u>","93a10966":"***There is one record without Side.***\n***Dropping that record***","c1a9c6c3":"# Feature scaling\n**Lets normalize the continuous features**","042e9b53":"**Creating a metrics dictionary**","14fc9340":"**Starting with Side column**","101b463b":"**In order to, reduce the usage of memory and the number of features we will use the BinaryEncoder included in the library category_encoders to encode City and Airport_Code.**","f5d71ba3":"***From the above plot we can see that Wednesdays and Fridays have recorded most number of accident cases***","60452076":"**Using GaussianNB function of sklearn library**","d079ca89":"# <u> ***Importing all the required packages for the Project*** <u>","338e52e8":"# ***From the above results we can see that 191,560 duplicate entries have been dropped.***","0cd12544":"***We find that the variance of pressure is very less, but in nature pressure never varies much. Hence, we will not drop the feature.***","e483bcff":"**Checking Wind_Direction**","698463cf":"**Checking Pressure and Visibiliy**","9c5c7413":"# Handling Missing values.\nMissing values and erroneous data should be handled properly before we feed the data to the model.","ec84ce77":"# Feature Addition\nLets break up Start_Time feature in year, month, day, weekday, hour and minute. We can use these features in our model","bc5414b9":"**On analyzing Weather conditions we find that there are many unique values. Therefore, reducing the number of unique values**","3a8300bd":"***SMOTE takes almost 40 minutes to complete for our data set. Hence I have run SMOTE on the data set and stored the features and labels on a local drive. In the next cell we are preparing two csv files consisting of features and their labels respectively.***","1d961be1":"# Correlation between features\n**Before we move ahead with data preprocessing lets check the correlation between the features using a correlation matrix**","71d7c86f":"# Number of accidents based on Weather conditions\n**Selecting the top 10 weather conditions**","891ac6fa":"**Now lets carry out Naive Bayes using MultinomialNB method of sklearn library**","3bb52930":"***We can see from the above bar graph that the accidents in areas which involve \"closed\" word in their description are generally most severe.***","48e4bf5e":"# Getting a list of all the columns","db174aef":"# Decision Tree\n**We will us DecisionTreeClassifier method from sklearn library for making te Decision Tree Classifier**","dc3cf6f3":"# Lets import the Data set and take a look into it. ","31734ae6":"***As per our prediction the length of road affected by accident is increasing with increasing severity***","7f40b311":"# Train - Validation split","5d1b6105":"# Multilayer Perceptron","e57629c0":"***As shown in the above graph, large number of accidents have occured in CA (California), TX (Texas) and FL (Florida)***","c50921d8":"# Dropping duplicates\nHere we will check if there are any duplicates in the dataset","e9eb906c":"**Here we will split the data set into training and validation set and also seperate them with respect to features and labels. For this, we will again use the train_test_split method from sklearn library**","7470aa26":"# A look into description of most severe cases (severity = 4)","f54994ed":"# Random Forest\n**We will use RandomForestClassifier method from sklearn library for making a Random Forest Classifier**","0f66b49d":"**First of all we will check how many number of unique classes does each categorical feature consists of**","c735bef0":"# Logistic Regresion","7d4a0f54":"**Merging the above two df with original dataset**","905abdcc":"# Handling Unbalanced Data\n**As the data with severity 1 is very less as compared to data with severity 2. We are going to undersample the data of severity 2,3 equal to that of 4(the second lowest) and then use SWOTE to oversample the data with severity 1.**","26478927":"# Importing Preprocessed Data\n**Now we will import the files from our local storage. Please change the link as per your need**","a4fe6336":"# Naive Bayes","f2845713":"# <u> ***Performing EDA*** <u>","46edfcd4":"***Number and Precipitaton(in) have many missing entries. Hence dropping the two features.***\n***For features with continuous values, we are going to fill the missing values with mean of the feature and for the feature with categorical data, we are going to drop the records with missing values.***"}}