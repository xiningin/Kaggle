{"cell_type":{"1f75c60a":"code","0c420bce":"code","1b3cbded":"code","f6e0b220":"code","b3673947":"code","0d763962":"code","ae6a6e46":"code","8ed9e952":"code","3e569112":"code","ce0a1256":"code","f3ebb520":"code","f03cb2f4":"code","6ef610ed":"markdown","7fc3c044":"markdown","6ae21f8c":"markdown","d8291736":"markdown","5a4e800f":"markdown","3b61c6d5":"markdown","f614e372":"markdown"},"source":{"1f75c60a":"from collections import Counter\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport os\nimport math","0c420bce":"b_size = 16\ns_size = 32\ntraining_file = \"\/kaggle\/input\/game-of-thrones-srt\/season1.json\"","1b3cbded":"def get_data_from_training_file(filename, batch_size, sequence_size):\n    lines_array = []\n    words_array = []\n    df = pd.read_json(filename)\n    df = df.dropna()\n    episodes = list(df)\n\n    for episode in episodes:\n        for line in range(len(df[episode].values) - 1):\n            lines_array.append(df[episode].values[line])\n\n    for i in range(len(lines_array)):\n        line = lines_array[i]\n        words = line.split()\n        for word in words:\n            words_array.append(word)\n    \n    word_count = Counter(words_array)\n    sorted_vocab = sorted(word_count, key=word_count.get, reverse=True)\n    int_to_vocab = {k:w for k, w in enumerate(sorted_vocab)}\n    vocab_to_int = {w:k for k, w in int_to_vocab.items()}\n    n_vocab = len(int_to_vocab)\n    \n    int_text = [vocab_to_int[w] for w in words_array]\n    num_batches = int(len(int_text) \/ (sequence_size * batch_size))\n    input_text = int_text[:num_batches * batch_size * sequence_size]\n    \n    output_text = np.zeros_like(input_text)\n    output_text[:-1] = input_text[1:]\n    output_text[-1] = input_text[0]\n    input_text = np.reshape(input_text, (batch_size, -1))\n    output_text = np.reshape(output_text, (batch_size, -1))\n    \n    return int_to_vocab, vocab_to_int, n_vocab, input_text, output_text\n    ","f6e0b220":"def get_batches(input_text, output_text, batch_size, sequence_size):\n    num_batches = np.prod(input_text.shape) \/\/ (sequence_size * batch_size)\n    for i in range(0, num_batches * sequence_size, sequence_size):\n        yield input_text[:, i:i+sequence_size], output_text[:, i:i+sequence_size]","b3673947":"embedding_size = 128\nlstm_size = 128\ndropout_keep_prob = 0.7","0d763962":"def network(batch_size, sequence_size, embedding_size, lstm_size, keep_prob, n_vocab, reuse=False):\n    with tf.compat.v1.variable_scope('LSTM', reuse=reuse):\n        in_op = tf.compat.v1.placeholder(tf.int32, [None, sequence_size])\n        out_op = tf.compat.v1.placeholder(tf.int32, [None, sequence_size])\n        embedding = tf.compat.v1.get_variable('embedding_weights', [n_vocab, embedding_size])\n        embed = tf.compat.v1.nn.embedding_lookup(embedding, in_op)\n        lstm = tf.compat.v1.nn.rnn_cell.LSTMCell(lstm_size)\n        initial_state = lstm.zero_state(batch_size, dtype=tf.float32)\n        output, state = tf.compat.v1.nn.dynamic_rnn(lstm, embed, initial_state=initial_state, dtype=tf.float32)\n        logits = tf.compat.v1.layers.dense(output, n_vocab, reuse=reuse)\n        preds = tf.compat.v1.nn.softmax(logits)\n        \n        return in_op, out_op, lstm, initial_state, state, preds, logits","ae6a6e46":"gradients_norm = 5","8ed9e952":"def get_loss_and_training_optimizer(out_op, logits, gradients_norm):\n    loss_op = tf.compat.v1.reduce_mean(tf.compat.v1.nn.sparse_softmax_cross_entropy_with_logits(labels=out_op, logits=logits))\n    trainable_vars = tf.compat.v1.trainable_variables()\n    grads, _ = tf.compat.v1.clip_by_global_norm(tf.compat.v1.gradients(loss_op, trainable_vars), gradients_norm)\n    opt = tf.compat.v1.train.AdamOptimizer()\n    train_op = opt.apply_gradients(zip(grads, trainable_vars))\n    \n    return loss_op, train_op","3e569112":"num_epochs = 200\ninitial_words = ['I', 'will']\npredict_top_k = 5","ce0a1256":"def main(loadCheckpoint):\n    int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_training_file(training_file, b_size, s_size)\n    in_op, out_op, lstm, initial_state, state, preds, logits = network(b_size, s_size, embedding_size, lstm_size, dropout_keep_prob, n_vocab)\n    val_in_op, _, _, val_initial_state, val_state, val_preds, _ = network(1, 1, embedding_size, lstm_size, dropout_keep_prob, n_vocab, reuse=True)\n    loss_op, train_op = get_loss_and_training_optimizer(out_op, logits, gradients_norm)\n    \n    sess = tf.compat.v1.Session()\n    saver = tf.compat.v1.train.Saver()\n    \n    if not os.path.exists('training_checkpoints\/'):\n        os.mkdir('training_checkpoints\/')\n    \n    if loadCheckpoint:\n        checkpoint = tf.train.Checkpoint(optimizer = train_op, model = lstm)\n        status = checkpoint.restore(tf.train.latest_checkpoint('training_checkpoints\/'))\n        status.initialize_or_restore(sess)\n    sess.run(tf.compat.v1.global_variables_initializer())\n    iteration = 0\n    \n    for e in range(num_epochs):\n        batches = get_batches(in_text, out_text, b_size, s_size)\n        new_state = sess.run(initial_state)\n        for x, y in batches:\n            iteration += 1\n            loss, new_state, _ = sess.run(\n            [loss_op, state, train_op],\n            feed_dict={in_op: x, out_op: y, initial_state: new_state})\n            if iteration % 100 == 0:\n                print('Epoch: {}\/{}'.format(e, num_epochs),\n                     'Iteration: {}'.format(iteration),\n                     'Loss: {:.4f}'.format(loss))\n            if iteration % 1000 == 0:\n                predict(initial_words, predict_top_k, sess, val_in_op, val_initial_state, val_preds, val_state, n_vocab, vocab_to_int, int_to_vocab)\n                saver.save(sess, os.path.join('training_checkpoints\/', 'model-{}.ckpt'.format(iteration)))","f3ebb520":"def predict(initial_words, predict_top_k, sess, in_op,\n            initial_state, preds, state, n_vocab, vocab_to_int, int_to_vocab):\n  new_state = sess.run(initial_state)\n  words = initial_words\n  samples = [w for w in words]\n  for word in words:\n    x = np.zeros((1, 1))\n    x[0, 0] = vocab_to_int[word]\n    pred, new_state = sess.run([preds, state], feed_dict={in_op: x, initial_state: new_state})\n\n  def get_word(pred):\n    p = np.squeeze(pred)\n    p[p.argsort()][:-predict_top_k] = 0\n    p = p \/ np.sum(p)\n    word = np.random.choice(n_vocab, 1, p=p)[0]\n    return word\n\n  word = get_word(pred)\n\n  n_samples = 200\n  samples.append(int_to_vocab[word])\n  for _ in range(n_samples):\n    x[0, 0] = word\n    pred, new_state = sess.run([preds, state], feed_dict={in_op: x, initial_state: new_state})\n    word = get_word(pred)\n    samples.append(int_to_vocab[word])\n\n  print(' '.join(samples))","f03cb2f4":"tf.compat.v1.disable_eager_execution()\ntf.compat.v1.reset_default_graph()\nmain(False)","6ef610ed":"**Define training**","7fc3c044":"**Define loss**","6ae21f8c":"**Define the model**","d8291736":"**Import required packages**","5a4e800f":"**Define data collection and cleanup**","3b61c6d5":"**Set variables**","f614e372":"**Define the batch sorter**"}}