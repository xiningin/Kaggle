{"cell_type":{"63434935":"code","bd725b54":"code","47a9a82b":"code","7f15f330":"code","99e21e81":"code","7208703f":"code","59924915":"code","f1485b75":"code","857eaef2":"code","bf46bb98":"code","761dc975":"code","6d3cced1":"code","24a1e505":"code","c403ead5":"code","1fe8d48b":"code","a3b1097e":"code","70adc31c":"code","2f646882":"code","d4d02fec":"code","9e09bc0d":"code","5f9e136c":"code","26479934":"code","569cb9d4":"code","1798d3d6":"code","c680d7a8":"code","d391196b":"code","f7edad11":"code","f23aefd3":"code","582b7168":"code","545b926e":"code","0e8a5f80":"code","d1733a99":"code","c4fd37ac":"code","13c857ee":"code","154a8a65":"code","808c5446":"code","61e5c951":"code","1ba64a75":"code","7c693b39":"code","a7f1ee5c":"code","d3ff9721":"code","c9f919b5":"code","a09c60b5":"code","d2d6d01b":"code","94eff5d4":"code","eff29199":"code","bd3cf676":"code","bbfe8c33":"code","18dc4560":"code","42e3e6af":"code","c99011b0":"code","e04c095c":"code","309568be":"code","1b526141":"code","b462624b":"code","2f8a3add":"code","0c3afbad":"code","334ff0b5":"code","42e4f2ac":"code","1920f52c":"code","492069db":"code","131ea93f":"code","eed29d77":"code","e04980e5":"code","9a726b74":"code","236cbab3":"code","3bae3f89":"code","4a1691e4":"code","d595094e":"code","ccb55ca7":"code","bf565b8b":"markdown","557785d8":"markdown","c37ea428":"markdown","5a5cff1e":"markdown","c42ae456":"markdown","0d050517":"markdown","28425466":"markdown","cb56e17f":"markdown","b5e74629":"markdown","38b9e069":"markdown","58f35e9a":"markdown","ed2bb77c":"markdown","f7c97f76":"markdown"},"source":{"63434935":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bd725b54":"import matplotlib.pyplot as plt\nimport seaborn as sns","47a9a82b":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","7f15f330":"test = test.drop(columns='Id')\ntest.head()","99e21e81":"test_num = test.columns[test.dtypes !='object']\ntest_numeric = test.loc[:,test_num] \ntest_object = test.drop(train.loc[:,test_num],axis = 1) ","7208703f":"test_numeric = test_numeric.fillna(0)","59924915":"train.head()","f1485b75":"train = train.drop(columns=['Id'])","857eaef2":"train.info()","bf46bb98":"train['MSSubClass'] = train['MSSubClass'].astype(str)\ntrain['OverallCond'] = train['OverallCond'].astype(str)\ntrain['OverallQual'] = train['OverallQual'].astype(str)","761dc975":"train_num = train.columns[train.dtypes !='object']\ndf_numeric = train.loc[:,train_num]     \ndf_numeric.head()","6d3cced1":"df_numeric.info()","24a1e505":"df_numeric = df_numeric.fillna(df_numeric.mean())\ndf_numeric.info(0)","c403ead5":"df_object = train.drop(train.loc[:,train_num],axis = 1) \ndf_object.head()","1fe8d48b":"min = np.int(np.size(train['SalePrice'])*(1-95\/100))\nremove = df_object.count() <= min\nremove = remove.index[remove.values == True]\nfor i in range(len(remove)):\n    df_object = df_object.drop(columns=[remove[i]])\n\ndf_object.head()","a3b1097e":"df_object.info()","70adc31c":"observe_ = df_object.count() <= 1000\nobserve = observe_.index[observe_.values == True]\n\nimport collections\no1 = collections.Counter(df_object[observe[0]])\no2 = collections.Counter(df_object[observe[1]])\no3 = collections.Counter(df_object[observe[2]])\n\n# plt.subplot(\u5217\u6578, \u884c\u6578, \u5716\u5f62\u7de8\u865f)\nplt.figure(figsize=(15,3))\n\nplt.subplot(1,3,1)\nplt.bar(np.arange(len(o1.keys())), o1.values(), alpha = 0.5)\nplt.xticks(np.arange(len(o1.keys())),o1.keys())\nplt.title(observe[0])\n\nplt.subplot(1,3,2)\nplt.bar(np.arange(len(o2.keys())), o2.values(), alpha = 0.5)\nplt.xticks(np.arange(len(o2.keys())),o2.keys())\nplt.title(observe[1])\n\nplt.subplot(1,3,3)\nplt.bar(np.arange(len(o3.keys())), o3.values(), alpha = 0.5)\nplt.xticks(np.arange(len(o3.keys())),o3.keys())\nplt.title(observe[2])\n\nplt.tight_layout\n\n","2f646882":"from sklearn.preprocessing import LabelEncoder\n\nfor i in df_object.columns :\n    df_object[i] = LabelEncoder().fit_transform(df_object[i])\ndf_object.head()","d4d02fec":"'''column_name = observe_.index[:]\ndf = []\nfor i in range(len(column_name)):\n    df_ = pd.get_dummies(df_object[column_name[i]],prefix = column_name[i])\n    df.append(df_)\n\ndf_object = df\ndf_object[0].head()'''","9e09bc0d":"sns.histplot(df_numeric['SalePrice'])","5f9e136c":"sns.boxplot(df_numeric['SalePrice'])","26479934":"df_numeric.plot.scatter(x='YearBuilt',y='SalePrice')","569cb9d4":"plt.subplots(figsize=(20, 10))\nsns.heatmap(df_numeric.corr())","1798d3d6":"X = df_numeric.drop(columns='SalePrice')\nX = pd.concat((X,df_object), axis=1)\nY = np.log(df_numeric.SalePrice)\n\nX_train = train.drop(columns='SalePrice')\n\nplt.figure(figsize=(15,3))\n\nplt.subplot(1,2,1)\nsns.histplot(df_numeric['SalePrice'])    # right skewed distribution\n\nplt.subplot(1,2,2)\nsns.histplot(Y)    # normalrize\n","c680d7a8":"x_train = X[:-100]\ny_train = Y[:-100]\nx_test = X[-100:]\ny_test = Y[-100:]","d391196b":"from sklearn.linear_model import LinearRegression","f7edad11":"lm = LinearRegression()\nlm.fit(x_train, y_train)\n\n# \u5370\u51fa\u622a\u8ddd\nprint(lm.intercept_ )\n\n# \u5370\u51fa\u4fc2\u6578\nprint(lm.coef_)","f23aefd3":"predicted_sales = lm.predict(x_test)\n\nprint(predicted_sales)","582b7168":"mse = np.mean((lm.predict(x_test) - y_test) ** 2)\nrmse = mse**0.5\nr_squared = lm.score(x_train, y_train)\nadj_r_squared = r_squared - (1 - r_squared) * (x_train.shape[1] \/ (x_train.shape[0] - x_train.shape[1] - 1))\n\nprint('MSE =',mse)\nprint('RMSE =',rmse)\nprint('r_squared =',r_squared)\nprint('adj_r_squared =',adj_r_squared)","545b926e":"# https:\/\/www.itread01.com\/content\/1544625546.html","0e8a5f80":"# \u61f2\u7f70\u9805\n# \u900f\u904e\u5728\u7dda\u6027\u8ff4\u6b78\u4e2d\u52a0\u5165L1\u61f2\u7f70\u51fd\u6578\uff0c\u76ee\u7684\u5728\u65bc\u8b93\u6a21\u578b\u4e2d\u4e0d\u8981\u5b58\u5728\u904e\u591a\u7684\u53c3\u6578\uff0c\u7576\u6a21\u578b\u53c3\u6578\u8d8a\u591a\u6642\u61f2\u7f70\u51fd\u6578\u7684\u503c\u6703\u8d8a\u5927\u3002\ndef L_theta_new(intercept, coef, X, Y, lamb):\n    \"\"\"\n    lamb: lambda, the parameter of regularization\n    theta: (n+1)\u00b71 matrix, contains the parameter of x0=1\n    X_x0: m\u00b7(n+1) matrix, plus x0\n    \"\"\"\n    h = np.dot(X, coef) + intercept  \n    L_theta = 0.5 * mean_squared_error(h, Y) + 0.5 * lamb * np.sum(np.square(coef))\n    return L_theta","d1733a99":"from sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error","c4fd37ac":"lamb = 10\nridge = Ridge(alpha=lamb, solver=\"cholesky\")\nridge.fit(x_train, y_train)\nprint('Ridge model : ')\nprint(ridge.intercept_)\nprint(ridge.coef_)\nprint(L_theta_new(intercept=ridge.intercept_, coef=ridge.coef_.T, X=x_train, Y=y_train, lamb=lamb))\n","13c857ee":"predicted_sales = ridge.predict(x_test)\n\nprint(predicted_sales)","154a8a65":"mse = np.mean((ridge.predict(x_test) - y_test) ** 2)\nrmse = mse**0.5\nr_squared = ridge.score(x_train, y_train)\nadj_r_squared = r_squared - (1 - r_squared) * (x_train.shape[1] \/ (x_train.shape[0] - x_train.shape[1] - 1))\n\nprint('MSE =',mse)\nprint('RMSE =',rmse)\nprint('r_squared =',r_squared)\nprint('adj_r_squared =',adj_r_squared)","808c5446":"from sklearn.linear_model import Lasso","61e5c951":"lamb = 0.025\nlasso = Lasso(alpha=lamb)\nlasso.fit(x_train, y_train)\nprint('Lasso model : ')\nprint(lasso.intercept_)\nprint(lasso.coef_)\nprint('\u61f2\u7f70\u9805 = ',L_theta_new(intercept=lasso.intercept_, coef=lasso.coef_.T, X=x_train, Y=y_train, lamb=lamb))\n","1ba64a75":"mse = np.mean((lasso.predict(x_test) - y_test) ** 2)\nrmse = mse**0.5\nr_squared = lasso.score(x_train, y_train)\nadj_r_squared = r_squared - (1 - r_squared) * (x_train.shape[1] \/ (x_train.shape[0] - x_train.shape[1] - 1))\n\nprint('MSE =',mse)\nprint('RMSE =',rmse)\nprint('r_squared =',r_squared)\nprint('adj_r_squared =',adj_r_squared)","7c693b39":"predicted_sales = lasso.predict(x_test)\n\nprint(predicted_sales)","a7f1ee5c":"from numpy import arange\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import LassoCV","d3ff9721":"# define model evaluation method\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# define model\nmodel = RidgeCV(alphas=arange(0, 1, 0.01), cv=cv)\n# fit model\nmodel.fit(x_train, y_train)\n# summarize chosen configuration\nprint('alpha: %f' % model.alpha_)\nprint('MSE =',mse)","c9f919b5":"ridge = Ridge(alpha=model.alpha_)\nridge.fit(x_train, y_train)\nprint('BEST Ridge model : ')\nprint(ridge.intercept_)\nprint(ridge.coef_)\nprint('BEST Penalty = ',L_theta_new(intercept=ridge.intercept_, coef=ridge.coef_.T, X=x_train, Y=y_train, lamb=lamb))","a09c60b5":"mse = np.mean((ridge.predict(x_test) - y_test) ** 2)\nrmse = mse**0.5\nr_squared = ridge.score(x_train, y_train)\nadj_r_squared = r_squared - (1 - r_squared) * (x_train.shape[1] \/ (x_train.shape[0] - x_train.shape[1] - 1))\n\nprint('MSE =',mse)\nprint('RMSE =',rmse)\nprint('r_squared =',r_squared)\nprint('adj_r_squared =',adj_r_squared)","d2d6d01b":"model = LassoCV(alphas=arange(0, 1, 0.01), cv=cv, n_jobs=-1) # n_jobs, Number of CPUs to use during the cross validation. -1 means using all processors.\n# fit model\nmodel.fit(x_train, y_train)\n# summarize chosen configuration\nprint('alpha: %f' % model.alpha_)","94eff5d4":"lasso = Lasso(alpha=model.alpha_)\nlasso.fit(x_train, y_train)\nprint('BEST Lasso model : ')\nprint(lasso.intercept_)\nprint(lasso.coef_)\nprint('BEST Penalty = ',L_theta_new(intercept=lasso.intercept_, coef=lasso.coef_.T, X=x_train, Y=y_train, lamb=lamb))","eff29199":"mse = np.mean((lasso.predict(x_test) - y_test) ** 2)\nrmse = mse**0.5\nr_squared = lasso.score(x_train, y_train)\nadj_r_squared = r_squared - (1 - r_squared) * (x_train.shape[1] \/ (x_train.shape[0] - x_train.shape[1] - 1))\n\nprint('MSE =',mse)\nprint('RMSE =',rmse)\nprint('r_squared =',r_squared)\nprint('adj_r_squared =',adj_r_squared)","bd3cf676":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score","bbfe8c33":"df = pd.concat((X,Y), axis=1)","18dc4560":"clusters = 10  \nk_range = range(2, clusters + 1)  # K \u503c\u7684\u7bc4\u570d (2~10)\n\ndistortions = []\nscores = []\nfor i in k_range:\n    kmeans = KMeans(n_clusters=i).fit(df)\n    distortions.append(kmeans.inertia_) # SSE\n    scores.append(silhouette_score(df, kmeans.predict(df))) # \u5074\u5f71\u4fc2\u6578:\u6108\u63a5\u8fd1 1 \u8868\u793a\u7e3e\u6548\u6108\u597d\uff0c\u53cd\u4e4b\u6108\u63a5\u8fd1 -1 \u8868\u793a\u7e3e\u6548\u6108\u5dee\u3002\n    \nselected_K = scores.index(max(scores)) + 2\nselected_K","42e3e6af":"plt.bar(k_range, scores)","c99011b0":"kmeans = KMeans(n_clusters=selected_K).fit(df)\ny_kmeans = kmeans.predict(df)","e04c095c":"lb = pd.DataFrame(y_kmeans, columns=['labels'])\nX_ = pd.concat((lb, X), axis=1)\n\n# \u5370\u51fa\u6a19\u7c64\u548c\u623f\u50f9\u3001\u4ee5\u53ca\u5176\u7d71\u8a08\u7d50\u679c\nprint('\u539f\u59cb\u8cc7\u6599\\n', Y.describe(), '\\n')\n\n# \u62bd\u51fa\u4e0d\u540c\u7d44\u7684\u623f\u50f9\u4e26\u5370\u51fa\u7d71\u8a08\u7d50\u679c\ndf_group = []\nfor i in range(selected_K):\n    Y_new = Y[X_['labels']==i]\n    print(f'\u5206\u985e {i + 1}\\n', Y_new.describe(), '\\n')\n    df_group.append(Y_new)\n# \u7528 seaborn \u756b\u51fa\u6240\u6709\u7d44\u5225\u623f\u50f9\u7684\u7bb1\u578b\u5716\nsns.boxplot(data=df_group)\nplt.show()\n","309568be":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsRegressor","1b526141":"# KNeighborsClassifier \u5206\u985e\u5668 ,y:\u985e\u5225\u8b8a\u9805\nknn = KNeighborsRegressor().fit(x_train,y_train)     # y: continous","b462624b":"y_predict = knn.predict(x_test)","2f8a3add":"mse = np.mean((y_predict - y_test) ** 2)\nrmse = mse**0.5\nr_squared = knn.score(x_train, y_train)\nadj_r_squared = r_squared - (1 - r_squared) * (x_train.shape[1] \/ (x_train.shape[0] - x_train.shape[1] - 1))\n\nprint('MSE =',mse)\nprint('RMSE =',rmse)\nprint('r_squared =',r_squared)\nprint('adj_r_squared =',adj_r_squared)","0c3afbad":"from sklearn.ensemble import RandomForestRegressor","334ff0b5":"rf = RandomForestRegressor()\nrf.fit(x_train, y_train)\n\n# \u9810\u6e2c\ny_predict = rf.predict(x_test)\n\nmse = np.mean((y_predict - y_test) ** 2)\nrmse = mse**0.5\nr_squared = rf.score(x_train, y_train)\nadj_r_squared = r_squared - (1 - r_squared) * (x_train.shape[1] \/ (x_train.shape[0] - x_train.shape[1] - 1))\n\nprint('MSE =',mse)\nprint('RMSE =',rmse)\nprint('r_squared =',r_squared)\nprint('adj_r_squared =',adj_r_squared)","42e4f2ac":"from  sklearn.model_selection import GridSearchCV     # \u7db2\u683c\u641c\u7d22,https:\/\/blog.csdn.net\/u012969412\/article\/details\/72973055","1920f52c":"rf0 = RandomForestRegressor(oob_score=True, random_state=10)\nrf0.fit(x_train,y_train)\nprint(rf0.oob_score_)","492069db":"param_test1 = {'n_estimators':range(30,201,10)}\ngsearch1 = GridSearchCV(estimator = RandomForestRegressor(oob_score=True, random_state=10), \n                       param_grid = param_test1, scoring='r2',cv=5)          \ngsearch1.fit(x_train,y_train)\ngsearch1.cv_results_\nbest_n_estimators = gsearch1.best_params_['n_estimators'] \ngsearch1.best_score_\n\nplt.plot(range(30,201,10),gsearch1.cv_results_['mean_test_score'])\nplt.ylabel(\"mean_test_score\") \nplt.xlabel(\"n_estimators\") ","131ea93f":"rf1 = RandomForestRegressor(n_estimators=best_n_estimators, oob_score=True, random_state=10)\nrf1.fit(x_train,y_train)\nprint(rf1.oob_score_)","eed29d77":"from mpl_toolkits.mplot3d import Axes3D\n\n# max_depth,\nparam_test2 = {'max_depth':range(10,101,10)}                  # max_depth:10-100, min_samples_split:\u6a23\u672c\u91cf\u6578\u5927\uff0c\u63a8\u85a6\u589e\u5927\ngsearch2 = GridSearchCV(estimator = RandomForestRegressor(n_estimators=best_n_estimators , \n                                   oob_score=True, random_state=10),\n                        param_grid = param_test2, scoring='r2', cv=5)\ngsearch2.fit(x_train,y_train)\ngsearch2.cv_results_ \nbest_max_depth = gsearch2.best_params_['max_depth']\ngsearch2.best_score_\n\nplt.plot(range(10,101,10),gsearch2.cv_results_['mean_test_score'])\nplt.ylabel(\"mean_test_score\") \nplt.xlabel(\"max_depth\") ","e04980e5":"rf2 = RandomForestRegressor(n_estimators= best_n_estimators, max_depth=best_max_depth, \n                                  oob_score=True, random_state=10)\nrf2.fit(x_train,y_train)\nprint(rf2.oob_score_)","9a726b74":"param_test3 = {'min_samples_split':range(2,11,1)}\ngsearch3 = GridSearchCV(estimator = RandomForestRegressor(n_estimators=best_n_estimators, max_depth=best_max_depth,\n                                  oob_score=True, random_state=10),\n                        param_grid = param_test3, scoring='r2', cv=5)\ngsearch3.fit(x_train,y_train)\ngsearch3.cv_results_\nbest_min_samples_split = gsearch3.best_params_['min_samples_split']\ngsearch3.best_score_\n\nplt.plot(range(2,11,1),gsearch3.cv_results_['mean_test_score'])\nplt.ylabel(\"mean_test_score\") \nplt.xlabel(\"min_samples_split\") ","236cbab3":"rf3 = RandomForestRegressor(n_estimators= best_n_estimators, max_depth=best_max_depth, min_samples_split = best_min_samples_split,\n                             oob_score=True, random_state=10)\nrf3.fit(x_train,y_train)\nprint(rf3.oob_score_)","3bae3f89":"param_test4 = {'min_samples_leaf':range(1,10,1)}\ngsearch4 = GridSearchCV(estimator = RandomForestRegressor(n_estimators= best_n_estimators, max_depth=best_max_depth, \n                                                          min_samples_split = best_min_samples_split,oob_score=True, random_state=10),\n                        param_grid = param_test4, scoring='r2', cv=5)\ngsearch4.fit(x_train,y_train)\ngsearch4.cv_results_\nbest_min_samples_leaf = gsearch4.best_params_['min_samples_leaf']\ngsearch4.best_score_\n\nplt.plot(range(1,10,1),gsearch4.cv_results_['mean_test_score'])\nplt.ylabel(\"mean_test_score\") \nplt.xlabel(\"min_samples_leaf\") ","4a1691e4":"rf4 = RandomForestRegressor(n_estimators=best_n_estimators, max_depth=best_max_depth, min_samples_split=best_min_samples_split,\n                                  min_samples_leaf=best_min_samples_leaf,oob_score=True, random_state=10)\nrf4.fit(x_train,y_train)\nprint(rf4.oob_score_)","d595094e":"param_test5 = {'max_features':range(10,51,1)}\ngsearch5 = GridSearchCV(estimator = RandomForestRegressor(n_estimators=best_n_estimators, max_depth=best_max_depth, min_samples_split=best_min_samples_split,\n                                  min_samples_leaf=best_min_samples_leaf ,oob_score=True, random_state=10),\n   param_grid = param_test5, scoring='r2', cv=5)\ngsearch5.fit(x_train,y_train)\ngsearch5.cv_results_\nbest_max_features = gsearch5.best_params_['max_features']\ngsearch4.best_score_\n\nplt.plot(range(10,51,1),gsearch5.cv_results_['mean_test_score'])\nplt.ylabel(\"mean_test_score\") \nplt.xlabel(\"max_features\") ","ccb55ca7":"# Final model\nrf5 = RandomForestRegressor(n_estimators=best_n_estimators, max_depth=best_max_depth, min_samples_split=best_min_samples_split,\n                                  min_samples_leaf=best_min_samples_leaf,max_features=best_max_features ,oob_score=True, random_state=10)\nrf5.fit(x_train,y_train)\nprint(rf5.oob_score_)","bf565b8b":"* choosen a best lambda for Ridge and Lasso","557785d8":"* XGBoost","c37ea428":"* GBM","5a5cff1e":"# 1.Data","c42ae456":"* \u8abf\u53c3 \n* https:\/\/www.itread01.com\/content\/1549571767.html   \n* https:\/\/blog.csdn.net\/u012969412\/article\/details\/72973055","0d050517":"* K-means","28425466":"* Ridge regression","cb56e17f":"# 2.Model","b5e74629":"* Lasso regression","38b9e069":"* Randon Forest","58f35e9a":"* KNN","ed2bb77c":"* sklearn.ensemble.RandomForestRegressor\n* default : \n* n_estimatorsint = 100\n* max_depth = None(int)\n* min_samples_split = 2(int or float)\n* min_samples_leaf = 1(int or float)\n* max_features{\u201cauto\u201d, \u201csqrt\u201d, \u201clog2\u201d} = \u201dauto\u201d(int or float)","f7c97f76":"* Linear regression"}}