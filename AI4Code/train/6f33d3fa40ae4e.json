{"cell_type":{"3555db78":"code","a7bd84ba":"code","b4a47b79":"code","7448b9a3":"code","1ea9285c":"code","d8a8c1c6":"code","1c4aca73":"code","e69c4099":"code","821ad2db":"code","146e629a":"code","a1636662":"code","e7a13da6":"code","580d195e":"code","29cb9eb5":"code","c0e0e011":"code","187044c7":"code","f388a246":"code","55f3e87e":"code","5cea43f1":"code","8eaad3a6":"code","910e6727":"code","74392238":"code","56121df4":"code","2dd6e982":"code","34dd9f47":"code","d19a4ebd":"code","6740d78b":"code","307377e9":"code","de247a03":"code","d3401c20":"code","64a7e1cc":"code","7a89ec49":"code","3db6e3b1":"code","82c14a64":"code","92dd526e":"code","c76d0a63":"code","3b48d4ca":"code","ee472b8a":"code","701d21d9":"code","242df0f4":"code","9e680092":"code","98b8282a":"code","17fcbd39":"code","715268ac":"code","2cc14fb3":"code","effe511d":"code","2cf184c5":"code","95c5ff31":"code","33c1307e":"code","31490457":"code","ec38e123":"code","a8508f6a":"code","3fb05214":"code","4a8a1a50":"code","825b751e":"code","c4dd5c02":"code","02bac7d5":"code","49320f7d":"code","9f24d3b2":"code","79dabc72":"code","14fba14c":"code","deb2491b":"code","21250ac7":"code","07af4a1b":"code","ca5d346c":"code","a4727281":"code","3ef07161":"code","83bbdb8b":"code","bd19ae34":"code","5e1a520e":"code","61595a2b":"code","d9759ce0":"code","d912a063":"code","6b44994d":"code","0f523651":"code","7c15a3bb":"code","795f70af":"code","d1daabd8":"code","820895fa":"code","158d5130":"code","03afda56":"code","22909834":"code","05da7dd3":"code","1404b046":"code","9bfbed34":"code","d7cdd4c4":"code","8a850cfe":"code","f3bf8a25":"code","0df4298e":"code","ff764e77":"code","f25fa84f":"code","d4527e95":"code","3d3f498b":"code","5d6b0f6b":"code","448fa542":"code","e88de7ce":"markdown","7bbcbb4d":"markdown","31e4ece1":"markdown","223a675d":"markdown","832df00f":"markdown","0758c610":"markdown","b18205c1":"markdown","d4176683":"markdown","6ddf2a4c":"markdown","6ab2dac5":"markdown","6b52fa56":"markdown","eea75042":"markdown","77833f8c":"markdown","f775ad0a":"markdown","3e03616f":"markdown","338f19fd":"markdown","6d4d93f2":"markdown","ecd3ea08":"markdown","9a8c9b2f":"markdown","29f2fb10":"markdown","e2db479a":"markdown","bc94f308":"markdown","dbbfbc7c":"markdown","16715a79":"markdown","5519df4c":"markdown","29ef2907":"markdown","0c18e09b":"markdown","491e6845":"markdown","9d54cf74":"markdown","df800be3":"markdown","c53f99f3":"markdown","05744d23":"markdown","4a722eea":"markdown","463ba218":"markdown","fd5ac18e":"markdown","cea834b1":"markdown","edcd06fe":"markdown","ca555359":"markdown","c15e49f5":"markdown","f57de619":"markdown","15b742e4":"markdown","24fd8181":"markdown","8eb040b5":"markdown","300aab2d":"markdown","1e80a7d6":"markdown","00f1e789":"markdown","3f0465e6":"markdown","326cc4c1":"markdown","5b46ec6b":"markdown","b4a9b2fc":"markdown","e8cf6025":"markdown","beb843f9":"markdown","707e804f":"markdown","9bf51364":"markdown","875fa814":"markdown","900b27de":"markdown","b9608f84":"markdown","a29147b3":"markdown","50f09cd0":"markdown","cd401b38":"markdown"},"source":{"3555db78":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.preprocessing import StandardScaler,RobustScaler\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\nfrom sklearn.linear_model import Ridge, Lasso, LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn import preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.datasets import make_classification\n\n# To ignore unwanted warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n# To ignore unwanted warnings\nimport warnings\nwarnings.filterwarnings('ignore')","a7bd84ba":"train = pd.read_csv('..\/input\/house-prices-data\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-data\/test.csv')","b4a47b79":"# df = pd.concat([train, test])\n# df.head(2)","7448b9a3":"df = train.merge(test , how='outer')\ndf.SalePrice = np.log(df.SalePrice)","1ea9285c":"df.shape","d8a8c1c6":"df.isnull().sum()","1c4aca73":"quan = list( test.loc[:,test.dtypes != 'object'].drop('Id',axis=1).columns.values )\nqual = list( test.loc[:,test.dtypes == 'object'].columns.values )","e69c4099":"# Find out how many missing values there are for the quantitative and categorical features\nprint('Quantitative\\n')\nhasNAN = df[quan].isnull().sum()\nhasNAN = hasNAN[hasNAN > 0]\nhasNAN = hasNAN.sort_values(ascending=False)\nprint(hasNAN)\nprint ('-' * 100)\nprint('Qualitative\\n')\nhasNAN = df[qual].isnull().sum()\nhasNAN = hasNAN[hasNAN > 0]\nhasNAN = hasNAN.sort_values(ascending=False)\nprint(hasNAN)","821ad2db":"print(df[['LotShape', 'Neighborhood','LotFrontage']].loc[df['LotFrontage'].isnull()])\n","146e629a":"# Filling missing values for numerical features. Most of the NAN should mean that \n# the corresponding facillity\/structure doesn't exist, so we use zero for most cases\n\ndf.LotFrontage.fillna(df.LotFrontage.median(), inplace=True)\ndf[\"LotFrontage\"] = df[\"LotFrontage\"].fillna(df.groupby([\"Neighborhood\", \"LotShape\"])[\"LotFrontage\"].transform(\"median\"))\n\n# NAN should mean no garage. we temporarily use yr = 0 here. Will come back to this later. \ndf.GarageYrBlt.fillna(0, inplace=True)\n\n# Use zero\ndf.MasVnrArea.fillna(0, inplace=True)    \ndf.BsmtHalfBath.fillna(0, inplace=True)\ndf.BsmtFullBath.fillna(0, inplace=True)\ndf.GarageArea.fillna(0, inplace=True)\ndf.GarageCars.fillna(0, inplace=True)    \ndf.TotalBsmtSF.fillna(0, inplace=True)   \ndf.BsmtUnfSF.fillna(0, inplace=True)     \ndf.BsmtFinSF2.fillna(0, inplace=True)    \ndf.BsmtFinSF1.fillna(0, inplace=True)\ndf.Electrical.fillna(df.Electrical.mode()[0], inplace=True) ","a1636662":"# Filling missing values for categorical features with not available (NA)\ndf.PoolQC.fillna('NA', inplace=True)\ndf.MiscFeature.fillna('NA', inplace=True)    \ndf.Alley.fillna('NA', inplace=True)          \ndf.Fence.fillna('NA', inplace=True)         \ndf.FireplaceQu.fillna('NA', inplace=True)    \ndf.GarageCond.fillna('NA', inplace=True)    \ndf.GarageQual.fillna('NA', inplace=True)     \ndf.GarageFinish.fillna('NA', inplace=True)   \ndf.GarageType.fillna('NA', inplace=True)     \ndf.BsmtExposure.fillna('NA', inplace=True)     \ndf.BsmtCond.fillna('NA', inplace=True)        \ndf.BsmtQual.fillna('NA', inplace=True)        \ndf.BsmtFinType2.fillna('NA', inplace=True)     \ndf.BsmtFinType1.fillna('NA', inplace=True)     \ndf.MasVnrType.fillna('None', inplace=True)   \ndf.Exterior2nd.fillna('None', inplace=True) \n\ndf.KitchenQual.fillna('NA', inplace=True)        \ndf.Exterior1st.fillna('NA', inplace=True)     \ndf.SaleType.fillna('NA', inplace=True)\ndf.Functional.fillna('NA', inplace=True)\ndf.Utilities.fillna('NA', inplace=True)","e7a13da6":"# MSZoning should highly correlate with the location, so I use the mode values of individual \n# Neighborhoods\nfor i in df.Neighborhood.unique():\n    if df.MSZoning[df.Neighborhood == i].isnull().sum() > 0:\n        df.loc[df.Neighborhood == i,'MSZoning'] = \\\n        df.loc[df.Neighborhood == i,'MSZoning'].fillna(df.loc[df.Neighborhood == i,'MSZoning'].mode()[0])","580d195e":"df.describe()","29cb9eb5":"# Qualitative\n\nfig = plt.subplots(nrows = 1, ncols = 1, figsize = (18, 6))\nsns.heatmap(df[qual].isnull(), yticklabels=False, cbar=False, cmap='viridis')","c0e0e011":"plt.figure(figsize=(40,30))\ncorrMatrix = df[quan].corr()\nsns.heatmap(corrMatrix, annot=True, cmap='coolwarm')","187044c7":"df[quan].isnull().sum()","f388a246":"#  Eliminating the duplicates.\ndf.duplicated(subset=None, keep='first').sum()","55f3e87e":"df.Heating.value_counts()","5cea43f1":"df[\"Heating\"] = df[\"Heating\"].apply(lambda x: \"Other\" if x != \"GasA\" else x)\n","8eaad3a6":"df.MasVnrType.value_counts()","910e6727":"df[\"MasVnrType\"] = df[\"MasVnrType\"].apply(lambda x: \"None\" if x in [\"None\", \"BrkCmn\"] else x)","74392238":"df.RoofMatl.value_counts()","56121df4":"df[\"RoofMatl\"] = df[\"RoofMatl\"].apply(lambda x: \"Other\" if x != \"CompShg\" else x)","2dd6e982":"df.RoofStyle.value_counts()","34dd9f47":"df[\"RoofStyle\"] = df[\"RoofStyle\"].apply(lambda x: \"Other\" if x not in [\"Gable\",'Hip'] else x)","d19a4ebd":"df.SaleType.value_counts()","6740d78b":"df[\"SaleType\"] = df[\"SaleType\"].apply(lambda x: \"Other\" if x not in [\"WD\",'New'] else x)#edited","307377e9":"df.SaleCondition.value_counts()","de247a03":"df[\"SaleCondition\"] = df[\"SaleCondition\"].apply(lambda x: \"Other\" if x not in [\"Normal\",'Partial'] else x)","d3401c20":"df.GarageType.value_counts()","64a7e1cc":"df[\"GarageType\"] = df[\"GarageType\"].apply(lambda x: \"Other\" if x not in [\"Attchd\",'Detchd','BuiltIn'] else x)","7a89ec49":"df.BldgType.value_counts()","3db6e3b1":"df[\"BldgType\"] = df[\"BldgType\"].apply(lambda x: \"2Fam\" if x in [\"2fmCon\", \"Duplex\",'Twnhs','TwnhsE'] else x)","82c14a64":"df.PoolQC.value_counts()","92dd526e":"df[\"PoolQC\"] = df[\"PoolQC\"].apply(lambda x: \"NA\" if x in [\"Fa\", \"Gd\"] else x)## i think this column","c76d0a63":"df.FireplaceQu.value_counts()","3b48d4ca":"df[\"FireplaceQu\"] = df[\"FireplaceQu\"].apply(lambda x: \"other\" if x in [\"Fa\", \"Po\",'Ex'] else x)## i think this column","ee472b8a":"df.Functional.value_counts()","701d21d9":"df[\"Functional\"] = df[\"Functional\"].apply(lambda x: 'Typ' if x == 'Typ' else 'Min1')","242df0f4":"# Converting categorical variables into labeles \ndf.Alley = df.Alley.map({'NA':0, 'Grvl':1, 'Pave':2})\ndf.BsmtCond =  df.BsmtCond.map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.BsmtExposure = df.BsmtExposure.map({'NA':0, 'No':1, 'Mn':2, 'Av':3, 'Gd':4})\ndf['BsmtFinType1'] = df['BsmtFinType1'].map({'NA':0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6})\ndf['BsmtFinType2'] = df['BsmtFinType2'].map({'NA':0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6})\ndf.BsmtQual = df.BsmtQual.map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.ExterCond = df.ExterCond.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.ExterQual = df.ExterQual.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.FireplaceQu = df.FireplaceQu.map({'NA':0, 'TA':1, 'Gd':2, 'other':3})\ndf.Functional = df.Functional.map({ 'Min1':2, 'Typ':1})\ndf.GarageCond = df.GarageCond.map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.GarageQual = df.GarageQual.map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.HeatingQC = df.HeatingQC.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.KitchenQual = df.KitchenQual.map({'NA':0,'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.LandSlope = df.LandSlope.map({'Sev':1, 'Mod':2, 'Gtl':3}) \ndf.PavedDrive = df.PavedDrive.map({'N':1, 'P':2, 'Y':3})\ndf.PoolQC = df.PoolQC.map({'NA':0,'Ex':1})\ndf.Street = df.Street.map({'Grvl':1, 'Pave':2})\ndf.Utilities = df.Utilities.map({'NA':0,'ELO':1, 'NoSeWa':2, 'NoSewr':3, 'AllPub':4})","9e680092":"# Update my lists of numerical and categorical features\nnewquan = ['Alley','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','BsmtQual',\n           'ExterCond','ExterQual','FireplaceQu','Functional','GarageCond',\n           'GarageQual','HeatingQC','KitchenQual','LandSlope','PavedDrive','PoolQC',\n           'Street','Utilities']\nquan = quan + newquan \nfor i in newquan: qual.remove(i)","98b8282a":"# This is actually a categorical feature, MSSubClass doesn't mean the meaning of the value of number 20,\n#and so on with other values\ndf.MSSubClass = df.MSSubClass.map({20:'class1', 30:'class2', 40:'class3', 45:'class4',\n                                   50:'class5', 60:'class6', 70:'class7', 75:'class8',\n                                   80:'class9', 85:'class10', 90:'class11', 120:'class12',\n                                   150:'class13', 160:'class14', 180:'class15', 190:'class16'})\n","17fcbd39":"# Keeping \"YrSold\" is enough, the month MoSold: Month Sold (MM)\n# and YrSold: Year Sold (YYYY) is like a redundant feature in some how\ndf=df.drop('MoSold',axis=1)\n\n# Update my lists of numerical and categorical features\nquan.remove('MoSold')\nquan.remove('MSSubClass')\nqual.append('MSSubClass')","715268ac":"df['Age'] = df.YrSold - df.YearBuilt\ndf['AgeRemod'] = df.YrSold - df.YearRemodAdd\ndf['AgeGarage'] = df.YrSold - df.GarageYrBlt\n\n# Here I replace their AgeGarage with the maximum value among the houses with Garages\nmax_AgeGarage = np.max(df.AgeGarage[df.AgeGarage < 1000])\ndf['AgeGarage'] = df['AgeGarage'].map(lambda x: max_AgeGarage if x > 1000 else x)\n\n# Some of the values are negative because the work was done after the house was sold. In these cases, I change them to zero to avoid negative ages.\ndf.Age = df.Age.map(lambda x: 0 if x < 0 else x)\ndf.AgeRemod = df.AgeRemod.map(lambda x: 0 if x < 0 else x)\ndf.AgeGarage = df.AgeGarage.map(lambda x: 0 if x < 0 else x)\n\n# drop the original features \ndf=df.drop(['YrSold','YearBuilt','YearRemodAdd','GarageYrBlt'],axis=1)\n\n# update my list of numerical feature\nfor i in ['YrSold','YearBuilt','YearRemodAdd','GarageYrBlt']: quan.remove(i)\nquan = quan + ['Age','AgeRemod','AgeGarage']","2cc14fb3":"df.shape","effe511d":"df.head()","2cf184c5":"# These are somewhat arbitrary \nindex_drop = df.LotFrontage[df.LotFrontage > 300].index\nindex_drop = np.append(index_drop, df.LotArea[df.LotArea > 100000].index)\nindex_drop = np.append(index_drop, df.BsmtFinSF1[df.BsmtFinSF1 > 4000].index)\nindex_drop = np.append(index_drop, df.TotalBsmtSF[df.TotalBsmtSF > 6000].index)\nindex_drop = np.append(index_drop, df['1stFlrSF'][df['1stFlrSF'] > 4000].index)\nindex_drop = np.append(index_drop, df.GrLivArea[(df.GrLivArea > 4000) & (df.SalePrice < 13)].index)\nindex_drop = np.unique(index_drop)\n\n# make sure we only remove data from the training set\nindex_drop = index_drop[index_drop < 1460] \n\ndf = df.drop(index_drop).reset_index(drop=True)\nprint(\"{} examples in the training set are dropped.\".format(len(index_drop)))","95c5ff31":"df.shape","33c1307e":"from scipy.stats import skew\nfor i in quan:\n    print(i+': {}'.format(round(skew(df[i]),2)))","31490457":"# transform those with skewness > 0.5\nskewed_features = np.array(quan)[np.abs(skew(df[quan])) > 0.5]\ndf[skewed_features] = np.log1p(df[skewed_features])","ec38e123":"df.shape","a8508f6a":"# create of list of dummy variables that I will drop, which will be the last\n# column generated from each categorical feature\ndummy_drop = []\nfor i in qual:\n    dummy_drop += [ i+'_'+str(df[i].unique()[-1]) ]\n\n# create dummy variables\ndf = pd.get_dummies(df,columns=qual) \n# drop the last column generated from each categorical feature\ndf = df.drop(dummy_drop,axis=1)","3fb05214":"df.shape","4a8a1a50":"fig, ax = plt.subplots( figsize = (18, 6))\n\n# train quan data \nsns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis')","825b751e":"X_train  = df[:-1459].drop(['SalePrice','Id'], axis=1)\ny_train  = df[:-1459]['SalePrice']\nX_test  = df[-1459:].drop(['SalePrice','Id'], axis=1)\n","c4dd5c02":"Xtrain  = df[:-1459].drop(['SalePrice','Id'], axis=1)\nytrain  = df[:-1459]['SalePrice']\nXtest  = df[-1459:].drop(['SalePrice','Id'], axis=1)\n","02bac7d5":"# fit the training set only, then transform both the training and test sets\nscaler = RobustScaler()\nXtrain[quan]= scaler.fit_transform(Xtrain[quan])\nXtest[quan]= scaler.transform(Xtest[quan])\n\n#X_train.shape, X_test.shape # now we have 193 features!","49320f7d":"!pip install xgboost\nfrom xgboost import XGBRegressor\nxgb = XGBRegressor()\nxgb.fit(Xtrain, ytrain)\nimp = pd.DataFrame(xgb.feature_importances_ ,columns = ['Importance'],index = Xtrain.columns)\nimp = imp.sort_values(['Importance'], ascending = False)\n\nprint(imp)\n","9f24d3b2":"imp.head(40)","79dabc72":"len (xgb.feature_importances_ )","14fba14c":"from sklearn.metrics import make_scorer \nfrom sklearn.feature_selection import RFECV\n\n# Define a function to calculate RMSE\ndef rmse(y_true, y_pred):\n    return np.sqrt(np.mean((y_true-y_pred)**2))\n\n# Define a function to calculate negative RMSE (as a score)\ndef nrmse(y_true, y_pred):\n    return -1.0*rmse(y_true, y_pred)\n\nneg_rmse = make_scorer(nrmse)\n\nestimator = XGBRegressor()\nselector = RFECV(estimator, cv = 3, n_jobs = -1, scoring = neg_rmse)\nselector = selector.fit(Xtrain, ytrain)\n\nprint(\"The number of selected features is: {}\".format(selector.n_features_))\n\nfeatures_kept = Xtrain.columns.values[selector.support_] \nXtrain = Xtrain[features_kept]\nXtest = Xtest[features_kept]","deb2491b":"X_train,X_test,y_train,y_test =  train_test_split(Xtrain , ytrain, test_size=0.3 , random_state = 101)","21250ac7":"lr = LinearRegression()\nlr.fit(X_train, y_train)\nlr.score(X_train, y_train)","07af4a1b":"lr.score(X_test, y_test)","ca5d346c":"cv_score = cross_val_score(lr,X_train,y_train,cv= 10)\nprint(\"Cross-validated scores:\", cv_score)\nprint(\"Mean cross-validated scores:\", cv_score.mean())","a4727281":"predection=lr.predict(X_train)\nlr_mse = mean_squared_error(y_train, predection)\nlr_rmse = np.sqrt(lr_mse)\nlr_rmse","3ef07161":"# Build Lasso regression model \nlassocv = LassoCV()\nlassocv.fit(X_train, y_train)\nlasso_score=lassocv.score(X_train, y_train)\nlasso_score","83bbdb8b":"lassocv.score(X_test, y_test)","bd19ae34":"# collect the model coefficients in a dataframe\ndf_coef_la = pd.DataFrame(lassocv.coef_, index=X_test.columns,##plz check this\n                       columns=['coefficients'])\n\ndf_coef_la[df_coef_la['coefficients'] > 0] # display the coeficents which stay in lasso\n","5e1a520e":"predection=lassocv.predict(X_train)\nlassocv_mse = mean_squared_error(y_train, predection)\nlassocv_rmse = np.sqrt(lassocv_mse)\nlassocv_rmse","61595a2b":"ridgecv = RidgeCV(alphas=np.logspace(.1, 10, 30))    \nridgecv.fit(X_train, y_train)\nridgecv_score=ridgecv.score(X_train, y_train)\nridgecv_score","d9759ce0":"ridgecv.score(X_test, y_test)","d912a063":"ridgecv.alpha_","6b44994d":"#ridgecv.get_params()","0f523651":"ridgecv = RidgeCV(alphas=np.logspace(.1, 10, 30))    # RidgeCV has the default alphas=[0.1, 1.0, 10.0]\nridgecv.fit(X_train, y_train)\n#ridgecv.score(X_test, y_test)","7c15a3bb":"\npredection=ridgecv.predict(X_train)\nridge_mse = mean_squared_error(y_train, predection)\nridge_rmse = np.sqrt(ridge_mse)\nridge_rmse","795f70af":"lab_enc = preprocessing.LabelEncoder()\ntraining_y = lab_enc.fit_transform(y_train)\ntesting_y = lab_enc.fit_transform(y_test)\nlogreg = LogisticRegression()\nlogreg.fit(X_train, training_y)\n","d1daabd8":"logreg_score=logreg.score(X_train,training_y)\nprint('Train score {}'.format(logreg_score))","820895fa":"logreg.score(X_test, testing_y)\n","158d5130":"print(\"RMSE of the rmse: {}\".format(rmse(y_train, logreg.predict(X_train))))","03afda56":"predection=logreg.predict(X_train)\nlogreg_mse = mean_squared_error(y_train, predection)\nlogreg_rmse = np.sqrt(logreg_mse)\nlogreg_rmse","22909834":"# using grid search on GradientBoostingRegressor model to get the best hyperparameters\nnum_estimators = [2500,3000]\nlearn_rates = [0.044,0.022]\nmax_depths = [2]\nmin_samples_leaf = [11,8]\nmin_samples_split = [15, 9]\nmax_features=['sqrt']\n\n\n\n\nparam_grid = {'n_estimators': num_estimators,\n              'learning_rate': learn_rates,\n              'max_depth': max_depths,\n              'min_samples_leaf': min_samples_leaf,\n              'min_samples_split': min_samples_split , \n             'max_features' : max_features }\n\ngrad = GridSearchCV(GradientBoostingRegressor(loss='huber'),\n                           param_grid, cv=3, verbose= 1 , n_jobs=-1)\ngrad.fit(X_train , y_train)\ngrad_score=grad.score(X_train , y_train)\ngrad_score\n#grad.score(X_test, y_test)","05da7dd3":"grad.best_params_","1404b046":"print(\"RMSE of the rmse: {}\".format(rmse(y_train, grad.predict(X_train))))","9bfbed34":"predection=grad.predict(X_train)\ngrad_mse = mean_squared_error(y_train, predection)\ngrad_rmse = np.sqrt(grad_mse)\ngrad_rmse","d7cdd4c4":"grad.score(X_test , y_test)","8a850cfe":"par = {'bootstrap': [True, False],\n 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n 'max_features': ['auto', 'sqrt'],\n 'min_samples_leaf': [1, 2, 4],\n 'min_samples_split': [2, 5, 10],\n 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n\n# par = {'n_estimators':[ 1500,2000,3000],\n#  'min_samples_split': [5,10],\n#  'min_samples_leaf': [2,5],\n#  'max_features': ['auto', 'sqrt'],\n#  'max_depth': [30, 40, 50],\n#  'bootstrap': [True, False]}\n\nra = RandomizedSearchCV(RandomForestRegressor(),\n                   par , cv = 5 , verbose= 1  , n_jobs= -1)\nra.fit(X_train , y_train)\n","f3bf8a25":"rf_score=ra.score(X_train , y_train)\nrf_score\n","0df4298e":"print(\"RMSE of the rmse: {}\".format(rmse(y_train, ra.predict(X_train))))","ff764e77":"predection=ra.predict(X_train)\nrf_mse = mean_squared_error(y_train, predection)\nrf_rmse = np.sqrt(rf_mse)\nrf_rmse","f25fa84f":"ra.score(X_test, y_test)","d4527e95":"lab_enc = preprocessing.LabelEncoder()\ntraining_y = lab_enc.fit_transform(y_train)\n#testing_y = lab_enc.fit_transform(y_test)\n\nknn = KNeighborsRegressor(n_neighbors=19)\nknn.fit(X_train, training_y)\ntrain_cv_score = np.mean(cross_val_score(knn, X_train, training_y, cv=4))\nknn_score = knn.score(X_train, training_y)\n#test_score = knn.score(X_test, training_y)\n\nprint(train_cv_score )\nprint(knn_score )\n#print(test_score)\nprint(\"RMSE of the knn: {}\".format(rmse(y_train, knn.predict(X_train))))","3d3f498b":"predection=knn.predict(X_train)\nknn_mse = mean_squared_error(y_train, predection)\nknn_rmse = np.sqrt(knn_mse)\nknn_rmse","5d6b0f6b":"knn.score(X_test, y_test)","448fa542":"models = pd.DataFrame({\n    'Model': ['Linear Model', 'Linear with Lasso Model','RidgeCV Model', 'Logistic Regression Model', 'GradientBoostingRegressor with Grid Search'\n             , 'Random Forest Regressor Model', 'KNN'],\n    'Score': [cv_score.mean(),lasso_score,ridgecv_score,logreg_score,grad_score,rf_score,knn_score],\n'RMSE':[lr_rmse,lassocv_rmse,ridge_rmse,logreg_rmse,grad_rmse,rf_rmse,knn_rmse]})\nmodels.sort_values(by='Score', ascending=False)","e88de7ce":" - XGBoost is a scalable and accurate implementation of gradient boosting machines and it has proven to push the limits of computing power for boosted trees algorithms as it was built and developed for the sole purpose of model performance and computational speed. [(3)](https:\/\/www.kdnuggets.com\/2017\/10\/xgboost-top-machine-learning-method-kaggle-explained.html#:~:text=XGBoost%20is%20a%20scalable%20and,model%20performance%20and%20computational%20speed.)","7bbcbb4d":"## Split the Data","31e4ece1":"Feature ranking with recursive feature elimination and cross-validated selection of the best number of features.","223a675d":"---","832df00f":"---","0758c610":" **If we use one-hot encoding we will get huge number of features, to avoid that,  we decide to combine values that have similar data meaning**\n \n Just look deeply to the variuos unique values to some features, it is a good practice to  combine some values which ot repeated often and have similar meaning\n","b18205c1":"## Conclusion","d4176683":"## Data Wrangling","6ddf2a4c":"### RidgeCV Model","6ab2dac5":"## Data Dictionary","6b52fa56":"### GradientBoostingRegressor with Grid Search","eea75042":"## Scaling","77833f8c":" - Feature importance is defined only for tree boosters","f775ad0a":"### Logistic Regression Model","3e03616f":"[1]: https:\/\/medium.com\/@ODSC\/transforming-skewed-data-for-machine-learning-90e6cc364b0 [Accessed 16 January 2021]\n\n[2]: Brownlee, J., 2021. A Gentle Introduction To Xgboost For Applied Machine Learning. [online] Machine Learning Mastery. Available at: <https:\/\/machinelearningmastery.com\/gentle-introduction-xgboost-applied-machine-learning\/> [Accessed 16 January 2021].\n\n[3]: https:\/\/www.kdnuggets.com\/2017\/10\/xgboost-top-machine-learning-method-kaggle-explained.html#:~:text=XGBoost%20is%20a%20scalable%20and,model%20performance%20and%20computational%20speed.","338f19fd":"### Divide the data into numerical (\"quan\") and categorical (\"qual\") features","6d4d93f2":"Main features in the dataset before cleaning steps:","ecd3ea08":"### KNN","9a8c9b2f":"---","29f2fb10":"## Detect the Outliers","e2db479a":"It is necessary to transform the numerical features that are skewed. This is because lots of regression models building assume that the features are distributed normally and have a symmetrical shape.\nFor most of machine learning algorithms, especially linear models, normally distributed features gives us better results. [1](https:\/\/medium.com\/@ODSC\/transforming-skewed-data-for-machine-learning-90e6cc364b0)","bc94f308":"## References","dbbfbc7c":"---","16715a79":"### Handling Missing Values","5519df4c":"### Linear Model ","29ef2907":"**Rather than the statistical computing methods to fill the missing values like mean, median, or mode, we prefer to do it manually, the benefit of this procedure is to get deep insight and understanding about the data, adding that make enhancements to some features in the feature engineering stage.**","0c18e09b":"**Let consider LotFrontage feature:**\n - LotFrontage: Linear feet of street connected to property\n - LotShape: General shape of property\n - Neighborhood: Physical locations within Ames city limits\n\nWe can assign the missin values of LotFrontage feature by group LotShape and Neighborhood and take the insight from them","491e6845":"---","9d54cf74":"## Models","df800be3":"### Random Forest Regressor Model","c53f99f3":"### Dummy variables for the categorical features","05744d23":" - By Jason Brownlee on August 17, 2016 in XGBoost. Last Updated on April 22, 2020. XGBoost is an algorithm that has recently been dominating applied machine learning and Kaggle competitions for structured or tabular data. XGBoost is an implementation of gradient boosted decision trees designed for speed and performance. [(2)](https:\/\/machinelearningmastery.com\/gentle-introduction-xgboost-applied-machine-learning\/)","4a722eea":"## Kaggle submission scores","463ba218":"bad score","fd5ac18e":"---","cea834b1":"In this project Notebook we will go through the whole process of working on a dataset which consists information about the location of the house, price and other aspects such as square feet etc. When we work on these sorts of data, we need to modify and aggregate some features to expand its importance and see which column is important for us and which is not. Our main goal in this notebook is to make a model which can give us a good prediction on the price of the house based on the other variables.\n\n\nIn addition, we join the [House Prices - Advanced Regression Techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) challenge on `Kaggle` according to our work. ","edcd06fe":"At the end, of course there is still more processes for improvement, like doing a more extensive feature engineering, by dive more into the features against each other by comparing and plotting to identifying and removing the outliers and noisy features. Moreover, the kaggle score could be improved by applying extensive hyperparameter tuning on the machine learning models","ca555359":"## Data Cleaning","c15e49f5":"---","f57de619":"![houses.png](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQrXiH9SoatIcCEjqf_F7cvES1AtdrfDgk8gQ&usqp=CAU)","15b742e4":"# House Prices - Advanced Regression Techniques\nMonirah Bin Taleb - Aisha Hakami - Lama Alharbi","24fd8181":"From PCA lecture, \"if ere have a lot of columns relative to our number of rows! (If you have $n$ rows, it's often advised to keep your number of columns below $\\sqrt{n}$.)\"\n - the root of 3000 is  54.79.\n  - The number of selected features is: 42, which is near to 54.79","8eb040b5":"---","300aab2d":"## Contents:\n- [Libraries](#Import-Libraries)\n- [Data Wrangling](#Data-Wrangling)\n- [Data Dictionary](#Data-Dictionary)\n- [Data Cleaning](#Data-Cleaning)\n- [Feature Engineering](#Feature-Engineering)\n- [Detect the Outliers](#Detect-the-Outliers)\n- [Split the Data](#Split-the-Data)\n- [Scaling](#Scaling)\n- [Models](#Models)\n- [Scores](#Scores)\n- [Kaggle submission scores](#Kaggle-submission-scores)\n- [Conclusion](#Conclusion)","1e80a7d6":"## Import Libraries","00f1e789":"### Observeations:\n- Half of the `SalePrice` is null which is the test set before merging datasets.","3f0465e6":"|Features|Discreption|\n|--------|-----------|\n|SalePrice| the property's sale price in dollars. This is the target variable that you're trying to predict.|\n|MSSubClass| The building class|\n|MSZoning| The general zoning classification|\n|LotFrontage| Linear feet of street connected to property|\n|LotArea| Lot size in square feet|\n|Street| Type of road access|\n|Alley| Type of alley access|\n|LotShape| General shape of property|\n|LandContour| Flatness of the property|\n|Utilities| Type of utilities available|\n|LotConfig| Lot configuration|\n|LandSlope| Slope of property|\n|Neighborhood| Physical locations within Ames city limits|\n|Condition1| Proximity to main road or railroad|\n|Condition2| Proximity to main road or railroad (if a second is present)|\n|BldgType| Type of dwelling|\n|HouseStyle| Style of dwelling|\n|OverallQual| Overall material and finish quality|\n|OverallCond| Overall condition rating|\n|YearBuilt| Original construction date|\n|YearRemodAdd| Remodel date|\n|RoofStyle| Type of roof|\n|RoofMatl| Roof material|\n|Exterior1st| Exterior covering on house|\n|Exterior2nd| Exterior covering on house (if more than one material)|\n|MasVnrType| Masonry veneer type|\n|MasVnrArea| Masonry veneer area in square feet|\n|ExterQual| Exterior material quality|\n|ExterCond| Present condition of the material on the exterior|\n|Foundation| Type of foundation|\n|BsmtQual| Height of the basement|\n|BsmtCond| General condition of the basement|\n|BsmtExposure| Walkout or garden level basement walls|\n|BsmtFinType1| Quality of basement finished area|\n|BsmtFinSF1| Type 1 finished square feet|\n|BsmtFinType2| Quality of second finished area (if present)|\n|BsmtFinSF2| Type 2 finished square feet|\n|BsmtUnfSF| Unfinished square feet of basement area|\n|TotalBsmtSF| Total square feet of basement area|\n|Heating| Type of heating|\n|HeatingQC| Heating quality and condition|\n|CentralAir| Central air conditioning|\n|Electrical| Electrical system|\n|1stFlrSF| First Floor square feet|\n|2ndFlrSF| Second floor square feet|\n|LowQualFinSF| Low quality finished square feet (all floors)|\n|GrLivArea| Above grade (ground) living area square feet|\n|BsmtFullBath| Basement full bathrooms|\n|BsmtHalfBath| Basement half bathrooms|\n|FullBath| Full bathrooms above grade|\n|HalfBath| Half baths above grade|\n|Bedroom| Number of bedrooms above basement level|\n|Kitchen| Number of kitchens|\n|KitchenQual| Kitchen quality|\n|TotRmsAbvGrd| Total rooms above grade (does not include bathrooms)|\n|Functional| Home functionality rating|\n|Fireplaces| Number of fireplaces|\n|FireplaceQu| Fireplace quality|\n|GarageType| Garage location|\n|GarageYrBlt| Year garage was built|\n|GarageFinish| Interior finish of the garage|\n|GarageCars| Size of garage in car capacity|\n|GarageAreav Size of garage in square feet|\n|GarageQual| Garage quality|\n|GarageCond| Garage condition|\n|PavedDrive| Paved driveway|\n|WoodDeckSF| Wood deck area in square feet|\n|OpenPorchSF| Open porch area in square feet|\n|EnclosedPorch| Enclosed porch area in square feet|\n|3SsnPorch| Three season porch area in square feet|\n|ScreenPorch| Screen porch area in square feet|\n|PoolArea| Pool area in square feet|\n|PoolQC| Pool quality|\n|Fence| Fence quality|\n|MiscFeature| Miscellaneous feature not covered in other categories|\n|MiscVal| $Value of miscellaneous feature|\n|MoSold| Month Sold|\n|YrSold| Year Sold|\n|SaleType| Type of sale|\n|SaleCondition| Condition of sale|","326cc4c1":"### Linear with Lasso Model","5b46ec6b":"### Transform the numerical features that are skewed.","b4a9b2fc":"![kaggle4.png](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/1101356\/1851571\/kaggle4.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20210116%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210116T213447Z&X-Goog-Expires=172799&X-Goog-SignedHeaders=host&X-Goog-Signature=436e93b723164ffe92a47246278fe6f9c15abcaa73d5a989ba2fbd597d90a1bd5f738d0c307a37f5749e29a53186b3ffc156d8080f858eabda33e801d4c567e30041317afbb5176f6ed283f5a1b5d0f0daf125d2c2e07a27bf37d8af72a62fff095e6c121969da780d2fe74fcd7d724534f3958f9500345cd97087d459bd9713c15feead1803960c36b5fe3fe725cd84ea061171894f5337864d8bf4988a850b73d6dd9cb176bb6276037bc5d10195bc12aecba70b610d5bea679ffcb257b29e4b5438d7419b6e795e9bf3638f212e946681eac83e3520348306183fd26a0e4519e9b852af07fd6c18a335f875651e6f39492063fd0636e92ae19ac7989d6f43)\n![kaggle1.png](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/1101356\/1851571\/kaggle1.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20210116%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210116T213349Z&X-Goog-Expires=172799&X-Goog-SignedHeaders=host&X-Goog-Signature=50bd25ab42fcefc021ad382c9865cbcc663b01f2d8f61b29cecda3f8c2c7104092c0670ac9ef197a35efe5fee1e53fdbf681ddee8ece67350291a7794a8fe0f972a2813ba68262f4f4361efd6e079cc2e446cc566f92ab10bcce871098e37c3aa1ba4123a43da334b7886f3a3ccbc0596a0a5d3e6064927f68108da4e4ce5dec8b49e26414b24ac0f5ce98e2dac44bae22cf3bf1886a18e4b127da7409ae213c4048cd1ebbe5e02e9abcc2eb66d2c76377784ac5a46048119f4f64309791314be248a4930a7226fb22b89bffa849e644517f55738be889e2b4c72bed378f09d513ff86912e1eaa3e78049cad31a1e4e79e2da0ad965ffae78b70d5ef554802f5)\n![kaggle2.png](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/1101356\/1851571\/kaggle2.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20210116%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210116T220516Z&X-Goog-Expires=172799&X-Goog-SignedHeaders=host&X-Goog-Signature=3cee4961fc319d91d1b8448ab6c938c1ce92bd7e8752453eb31aac6d69cb71b365c95c3f4757e079bb2315acf86fa5484d737e99c9297c173fb15cbb4b7b71180b8c31081c2598456a49c371b4fe05b323a8c441863fcfcaa5da8f2bd266ddd60f6fe922f5200c450f42af1d05ecb87619a19d66265726961f5152f07cc48c5a9c9c5002d56cd4ac9714aa7ffe533a607a7d852c979798bcf08c0b81f2397cd52273c83a6a45c18f50afc7156d2dbc60ddd11fce2b5618d4a7533512ba05fab283f7491642396c32a047ec03b29d72b5280b42341af0f71a16cc55064b3bb35f4f3c3eb9a6f89c48b12f3c4c7f59d1241c860c22fbb520b30e3281bc34007541)\n![kaggle3.png](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/1101356\/1851571\/kaggle3.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20210116%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210116T220523Z&X-Goog-Expires=172799&X-Goog-SignedHeaders=host&X-Goog-Signature=2f862554cf035c6cb0d8448d3a060e4d3e3a5f4af1c94e647c534f870e879200515345660a8164810b0daebee1d7777de873b951613b74ee60d8c10934b3a03d56088d8fe0c0360a7a8693d59f6dd2c0c77a85a476de6bcd214883bd1ab559bf634b321b0afc947405925520ea4e36c855ef98d6683b9886aaceb8a9b2eed73e965ddc098abbd35d16dc0060da43961d7747d69df553f9eeaf52cf0af79de6c5ee57c34ade687bd3835b3d679f5a37ceee068e5a640d15ff2a15ccf20f304a84cce69b211050e54ebaa7ba74ebc436dc4c058e2c50dd7a7c267a9cd4313d36ebb7a1ae3c01e2b6d3dfddd5dec67f5f2f08657f478e247509192575d79463efd8)\n","e8cf6025":"---","beb843f9":"---","707e804f":"---","9bf51364":"---","875fa814":"---","900b27de":"###  Check duplicate","b9608f84":"## Scores ","a29147b3":"---","50f09cd0":"---","cd401b38":"## Feature Engineering"}}