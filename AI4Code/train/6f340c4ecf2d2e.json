{"cell_type":{"458b4f1e":"code","5f4f0f61":"code","a4a0f79f":"code","dc41e1ea":"code","1459cf15":"code","335b5c7c":"code","4e920f0d":"code","784d6237":"code","1b0cb011":"code","7fb2497a":"code","0096c231":"code","01fe0589":"code","de00acad":"code","89497603":"code","7c8978eb":"code","bca99702":"code","e3c0a865":"code","6b199ff6":"code","e435e6dc":"code","b2d15c6c":"code","3b60f114":"code","08b16a14":"code","2ef5a480":"code","28bf3c5f":"code","66cb0c97":"code","4684aaf4":"code","3e1c40ba":"code","040f6912":"code","decaef0b":"code","6ae9e9ab":"code","5f754325":"code","36fb0062":"code","e816e9dd":"code","49985479":"code","29a98f55":"code","d01ab0ba":"code","5ac9446c":"code","b4fe99c4":"code","97939e98":"code","632874b7":"code","82cc9e82":"code","b7d7ab5e":"code","c302eb0c":"code","a21f7c20":"markdown","77b9d710":"markdown","4aa97af0":"markdown","9a38d868":"markdown","61a064d0":"markdown","68c0909d":"markdown","55f625fc":"markdown"},"source":{"458b4f1e":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","5f4f0f61":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\nfrom sklearn.ensemble import RandomForestRegressor, forest\nimport re\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nprint(os.listdir('..\/input'))","a4a0f79f":"# Learned these techniques from fast.ai ml course and implemented these functions myself with some changes.\n\nclass LabelEncoder(BaseEstimator, TransformerMixin):\n    def fit(self, y):\n        self.classes_ = ['NaN'] + list(set(y) - set(['NaN']))\n        self.class_maps_ = {label: i for i, label in enumerate(self.classes_)}\n        self.inverse_maps_ = {i: label for label, i in self.class_maps_.items()}\n\n        return self\n\n    def transform(self, y):\n        y = np.array(y)\n        new_labels = list(set(y) - set(self.classes_))\n        y[np.isin(y, new_labels)] = 'NaN'\n\n        return np.array([self.class_maps_[v] for v in y]).astype(np.int32)\n\n    def fit_transform(self, y):\n        self.fit(y)\n        return self.transform(y)\n\n    def inverse_transform(self, y):\n        y = np.array(y)\n        return np.array([self.inverse_maps_[v] for v in y])\n\n    def add_label(self, label, ix):\n        self.classes_ = self.classes_[:ix] + [label] + self.classes_[ix:]\n        self.class_maps_ = {label: i for i, label in enumerate(self.classes_)}\n        self.inverse_maps_ = {i: label for label, i in self.class_maps_.items()}\n\n\ndef replace_nan(df, nan_cols):\n    df.replace(nan_cols, np.nan, inplace=True)\n\n\ndef conv_contncat(df, cont_cols=None, cat_cols=None):\n    if cont_cols is not None:\n        for n in cont_cols:\n            df[n] = pd.to_numeric(df[n], errors='coerce').astype(np.float64)\n\n    if cat_cols is not None:\n        for n in cat_cols:\n            df[n] = df[n].astype(str)\n\n    df[df == 'nan'] = np.nan\n\n\ndef add_datecols(df, col, time=True, drop=True):\n    fld_dtype = df[col].dtype\n\n    if not np.issubdtype(fld_dtype, np.datetime64):\n        df[col] = pd.to_datetime(df[col], infer_datetime_format=True)\n\n    target_pre = re.sub('[Dd]ate$', '', col)\n    attr = ['year', 'month', 'week', 'dayofweek', 'is_month_start', 'is_month_end', 'is_quarter_start', 'is_quarter_end', 'is_year_start', 'is_year_end', ]\n\n    if time:\n        attr += ['hour', 'minute', 'second']\n\n    for at in attr:\n        df[target_pre + at + '_cat'] = getattr(df[col].dt, at)\n\n    df[target_pre + 'elapsed'] = np.int64(df[col])\n\n    if drop:\n        df.drop(col, axis=1, inplace=True)\n\n\ndef get_nn_mappers(df, cat_cols, cont_cols):\n    cat_maps = [(o, LabelEncoder()) for o in cat_cols]\n    cont_maps = [([o], StandardScaler()) for o in cont_cols]\n\n    conv_mapper = DataFrameMapper(cont_maps).fit(df)\n    cat_mapper = DataFrameMapper(cat_maps).fit(df)\n    return cat_mapper, conv_mapper\n\n\ndef fix_missing(df, na_dict=None, cont_cols=None, cat_cols=None):\n    if na_dict is not None:\n        for n in na_dict.keys():\n            col_null = df[n].isnull()\n            df[n + '_na'] = col_null\n            df.loc[col_null, n] = na_dict[n]\n\n        if cont_cols is not None:\n            for n in cont_cols:\n                col_null = df[n].isnull()\n                if col_null.sum():\n                    df.loc[col_null, n] = df[n].median()\n\n        if cat_cols is not None:\n            for n in cat_cols:\n                col_null = df[n].isnull()\n                if col_null.sum():\n                    df.loc[col_null, n] = 'NaN'\n\n    else:\n        na_dict = {}\n        if cont_cols is not None:\n            for n in cont_cols:\n                col_null = df[n].isnull()\n                if col_null.sum():\n                    df[n + '_na'] = col_null\n                    na_dict[n] = df[n].median()\n                    df.loc[col_null, n] = na_dict[n]\n\n        if cat_cols is not None:\n            for n in cat_cols:\n                col_null = df[n].isnull()\n                if col_null.sum():\n                    df[n + '_na'] = col_null\n                    na_dict[n] = 'NaN'\n                    df.loc[col_null, n] = na_dict[n]\n\n    return na_dict\n\n\ndef get_one_hot(df, max_n_cat, drop_first, cat_mapper):\n    one_hot_cols = []\n\n    for n, c in df.items():\n        if n.endswith('_cat') and len(set(df[n])) <= max_n_cat:\n            one_hot_cols.append(n)\n\n    for n, encoder, _ in cat_mapper.built_features:\n        if len(encoder.classes_) <= max_n_cat:\n            one_hot_cols.append(n)\n\n    df = pd.get_dummies(df, columns=one_hot_cols, drop_first=drop_first)\n    return df\n\n\ndef proc_df(df, y_col, subset=None, drop_cols=None, do_scale=False, cat_mapper=None, cont_mapper=None, max_n_cat=None, drop_first=False):\n    if subset is not None:\n        df = df[-subset:]\n\n    if drop_cols is not None:\n        df.drop(drop_cols, axis=1, inplace=True)\n\n    if do_scale:\n        df[cont_mapper.transformed_names_] = cont_mapper.transform(df)\n\n    df[cat_mapper.transformed_names_] = cat_mapper.transform(df)\n\n    if max_n_cat is not None:\n        df = get_one_hot(df, max_n_cat, drop_first, cat_mapper)\n\n    X = df.drop(y_col, axis=1)\n    y = df[y_col]\n\n    return X, y\n\n\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols': df.columns, 'imp': m.feature_importances_}).sort_values('imp', ascending=False)\n\n\ndef set_rf_samples(n):\n    \"\"\" Changes Scikit learn's random forests to give each tree a random sample of\n    n random rows.\n    \"\"\"\n    forest._generate_sample_indices = (lambda rs, n_samples: forest.check_random_state(rs).randint(0, n_samples, n))\n\n\ndef reset_rf_samples():\n    \"\"\" Undoes the changes produced by set_rf_samples.\n    \"\"\"\n    forest._generate_sample_indices = (lambda rs, n_samples: forest.check_random_state(rs).randint(0, n_samples, n_samples))","dc41e1ea":"os.makedirs('tmp', exist_ok=True)\n\nTRAIN_PATH = '.\/data\/Train.csv'\nVALID_PATH = '.\/data\/Valid.csv'\nVALID_SOL = '.\/data\/ValidSolution.csv'\nTEST_PATH = '.\/data\/Test.csv'","1459cf15":"def display_all(df):\n    with pd.option_context('display.max_rows', 100, 'display.max_columns', 1000):\n        display(df)","335b5c7c":"# train_df = pd.read_csv(TRAIN_PATH, low_memory=False, parse_dates=['saledate'])\n# valid_df = pd.read_csv(VALID_PATH, low_memory=False, parse_dates=['saledate'])\n# valid_sol = pd.read_csv(VALID_SOL, low_memory=False)\n# test_sol = pd.read_csv(TEST_PATH, low_memory=False, parse_dates=['saledate'])","4e920f0d":"# train_df.to_feather('tmp\/train_raw')\n# valid_df.to_feather('tmp\/valid_raw')\n# valid_sol.to_feather('tmp\/valid_sol_raw')\n# test_sol.to_feather('tmp\/test_raw')","784d6237":"# valid_df = pd.read_feather('tmp\/valid_raw')\n# valid_sol = pd.read_feather('tmp\/valid_sol_raw')\n\n# valid_df = pd.merge(left=valid_df, right=valid_sol, how='left', left_on='SalesID', right_on='SalesID')\n# valid_df.drop('Usage', axis=1, inplace=True)\n\n# valid_df.to_feather('tmp\/valid_raw')","1b0cb011":"train_df = pd.read_feather('..\/input\/bulldozerstmp\/train_raw')\ntrain_temp_df = train_df.copy()\nvalid_df = pd.read_feather('..\/input\/bulldozerstmp\/valid_raw')\nvalid_temp_df = valid_df.copy()\n\nprint(len(train_df))\nprint(len(valid_df))","7fb2497a":"%time add_datecols(train_df, col='saledate')\n%time add_datecols(valid_df, col='saledate')","0096c231":"def get_n_unique(df):\n    for col in df.columns:\n        print(f'{col}: {len(df[col].unique())}')\n\n# get_n_unique(train_df)","01fe0589":"# Replace None and other null values with np.nan\nnan_cols = [None, 'None or Unspecified', 'None', 'NaN', 'nan']\nreplace_nan(train_df, nan_cols)\nreplace_nan(valid_df, nan_cols)","de00acad":"drop_cols = None\ncat_cols = ['Tire_Size', 'UsageBand', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc', 'fiModelSeries', 'fiModelDescriptor', 'ProductSize', 'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc', 'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control', 'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension', 'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics', 'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow', 'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb', 'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type', 'Travel_Controls', 'Differential_Type', 'Steering_Controls']\ncont_cols = ['SalesID', 'MachineHoursCurrentMeter', 'auctioneerID', 'MachineID', 'ModelID', 'datasource', 'YearMade']","89497603":"# Convert y to log values\ntrain_df['SalePrice'] = np.log(train_df['SalePrice'])\nvalid_df['SalePrice'] = np.log(valid_df['SalePrice'])","7c8978eb":"# Convert categorical data to categoriy dtype and continous data to float32 dtype\nconv_contncat(train_df, cont_cols, cat_cols)\nconv_contncat(valid_df, cont_cols, cat_cols)","bca99702":"# Fill missing values\nna_dict = fix_missing(train_df, None, cont_cols, cat_cols)\nna_dict = fix_missing(valid_df, na_dict, cont_cols, cat_cols)\n# cat_cols.extend([col for col in train_df.columns if col.endswith('_na')])","e3c0a865":"# Get mappers\ncat_mapper, cont_mapper = get_nn_mappers(train_df, cat_cols, cont_cols)","6b199ff6":"# Proc_df\n%time X_train, y_train = proc_df(train_df, 'SalePrice', drop_cols=drop_cols, cat_mapper=cat_mapper, cont_mapper=cont_mapper)","e435e6dc":"%time X_valid, y_valid = proc_df(valid_df, 'SalePrice', drop_cols=drop_cols, cat_mapper=cat_mapper, cont_mapper=cont_mapper)","b2d15c6c":"X_train = X_train.astype(np.float32)\nX_valid = X_valid.astype(np.float32)","3b60f114":"def rmse(y_pred, y_true):\n    return np.mean((y_pred - y_true)**2) ** 0.5\n\ndef get_scores(m, X_train, y_train, X_valid, y_valid):\n    scores = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid), m.score(X_train, y_train), m.score(X_valid, y_valid)]\n\n    if m.oob_score is True:\n        scores.append(m.oob_score_)\n\n    return scores","08b16a14":"m2 = RandomForestRegressor(n_estimators = 40, n_jobs=-1, oob_score=True)\n%time m2.fit(X_train, y_train)\n%time get_scores(m2, X_train, y_train, X_valid, y_valid)","2ef5a480":"m = RandomForestRegressor(n_estimators=80, n_jobs=-1, oob_score=True)\n%time m.fit(X_train, y_train)\n%time get_scores(m, X_train, y_train, X_valid, y_valid)","28bf3c5f":"m3 = RandomForestRegressor(n_estimators=50, min_samples_leaf=3, max_features=0.6, n_jobs=-1, oob_score=True)\n%time m3.fit(X_train, y_train)\n%time get_scores(m3, X_train, y_train, X_valid, y_valid)","66cb0c97":"fi = rf_feat_importance(m3, X_train)","4684aaf4":"fi[:30].plot('cols', 'imp', 'barh', figsize=(12, 8), legend=False)","3e1c40ba":"cols_to_keep = fi[fi['imp'] > 0.005]['cols']\nlen(cols_to_keep)","040f6912":"X_train_keep = X_train[cols_to_keep].copy()\nX_valid_keep = X_valid[cols_to_keep].copy()","decaef0b":"m4 = RandomForestRegressor(n_estimators=50, min_samples_leaf=3, max_features=0.6, n_jobs=-1, oob_score=True)\n%time m4.fit(X_train_keep, y_train)\n%time get_scores(m4, X_train_keep, y_train, X_valid_keep, y_valid)","6ae9e9ab":"import scipy\nfrom scipy.cluster import hierarchy as hc","5f754325":"corr = np.round(scipy.stats.spearmanr(X_train_keep).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16,10))\ndendrogram = hc.dendrogram(z, labels=X_train_keep.columns, orientation='left', leaf_font_size=16)\nplt.show()","36fb0062":"dn_cols = ['saleyear_cat', 'saleelapsed']\n\nfor col in dn_cols:\n    X_train_temp = X_train_keep.drop(col, axis=1).copy()\n    X_valid_temp = X_valid_keep.drop(col, axis=1).copy()\n    m = RandomForestRegressor(n_estimators=50, min_samples_leaf=3, max_features=0.6, oob_score=True)\n    m.fit(X_train_temp, y_train)\n    print(col)\n    print(get_scores(m, X_train_temp, y_train, X_valid_temp, y_valid))","e816e9dd":"X_train_ext = X_train_keep.copy()\nX_valid_ext = X_valid_keep.copy()\nX_train_ext['is_valid'] = 0\nX_valid_ext['is_valid'] = 1\nX_ext = pd.concat([X_train_ext, X_valid_ext], ignore_index=True)\nX_ext = X_ext.sample(frac=1).reset_index(drop=True)\n\ny_ext = X_ext['is_valid']\nX_ext.drop('is_valid', axis=1, inplace=True)\n\nX_ext.head()","49985479":"m6 = RandomForestRegressor(n_estimators=50, min_samples_leaf=3, max_features=0.6, oob_score=True)\n%time m6.fit(X_ext, y_ext)\nprint(m6.oob_score_)","29a98f55":"ft_ext = rf_feat_importance(m6, X_ext)\nft_ext[:10]","d01ab0ba":"t_feats = ['saleyear_cat', 'saleelapsed', 'SalesID', 'saleweek_cat', 'MachineID']","5ac9446c":"X_ext.drop(t_feats, axis=1, inplace=True)","b4fe99c4":"m6 = RandomForestRegressor(n_estimators=50, min_samples_leaf=3, max_features=0.6, oob_score=True)\n%time m6.fit(X_ext, y_ext)\nprint(m6.oob_score_)","97939e98":"m7 = RandomForestRegressor(n_estimators=50, min_samples_leaf=3, max_features=0.6, oob_score=True)\n%time m7.fit(X_train_keep, y_train)\nget_scores(m7, X_train_keep, y_train, X_valid_keep, y_valid)","632874b7":"for col in t_feats:\n    X_train_temp = X_train_keep.drop(col, axis=1)\n    X_valid_temp = X_valid_keep.drop(col, axis=1)\n    \n    m0 = RandomForestRegressor(n_estimators=50, min_samples_leaf=3, max_features=0.6, oob_score=True)\n    m0.fit(X_train_temp, y_train)\n    print(col)\n    print(get_scores(m0, X_train_temp, y_train, X_valid_temp, y_valid))","82cc9e82":"X_train_temp = X_train_keep.drop(['SalesID', 'MachineID'], axis=1)\nX_valid_temp = X_valid_keep.drop(['SalesID', 'MachineID'], axis=1)\n\nm9 = RandomForestRegressor(n_estimators=50, min_samples_leaf=3, max_features=0.6, oob_score=True)\n%time m9.fit(X_train_temp, y_train)\n%time get_scores(m9, X_train_temp, y_train, X_valid_temp, y_valid)","b7d7ab5e":"X_train_temp.columns","c302eb0c":"X_train_final = X_train_keep.drop(['MachineID', 'SalesID'], axis=1)\nX_valid_final = X_valid_keep.drop(['MachineID', 'SalesID'], axis=1)\n\nm9 = RandomForestRegressor(n_estimators=160, min_samples_leaf = 3, max_features=0.5, oob_score=True)\n%time m9.fit(X_train_final, y_train)\n%time get_scores(m9, X_train_final, y_train, X_valid_final, y_valid)","a21f7c20":"# Load dataset","77b9d710":"## Utils","4aa97af0":"### Merge valid data","9a38d868":"## Extrapolation","61a064d0":"## Removing redundant features","68c0909d":"## Feature importances","55f625fc":"## Final random-forest regressor"}}