{"cell_type":{"82fc41b9":"code","918fb5f7":"code","c868a381":"code","252837b1":"code","f65ad2ee":"code","bc57dfe3":"code","890d1f7c":"code","012c4de5":"code","373d7d42":"code","8dd5792b":"code","baa7e429":"code","e18a7e61":"code","7624030e":"code","d7e599f7":"code","e8b01e3c":"code","3535ccde":"code","4119fd58":"code","ed0e78f3":"code","309507ea":"code","9fbae745":"code","69b53d7a":"code","2e508c12":"code","9c76e4f8":"code","e378204c":"code","1c873b25":"code","323d55bc":"code","d0472167":"code","00c01315":"code","146717fa":"code","633153f0":"code","4c67e6f8":"code","2f6c5a6d":"code","498fd5c3":"code","c380faf8":"code","0945bf63":"code","5b93fd27":"code","ae997e40":"code","1312ddd4":"code","5e70ecca":"code","8a206255":"code","d5e54c82":"code","364724d5":"code","0f604ef6":"code","b14f0594":"code","fb68d156":"code","49911fc6":"code","d10ab789":"code","0517eb04":"code","81720641":"code","bcb26dff":"code","0cbe5ac2":"code","a3146ec2":"code","85b36cd6":"code","90f6a977":"code","200d8565":"code","1b565a91":"code","d0d400b4":"code","5c49a91e":"code","163a8acb":"code","b27d3ed4":"code","40da0717":"code","b00c52e6":"markdown","e1a33803":"markdown","bde895aa":"markdown","2a1c17cd":"markdown","d7ebc0a2":"markdown","f1a12063":"markdown","2fc93ed1":"markdown","01953495":"markdown","0e85b456":"markdown","7140dbdb":"markdown","a8b4551a":"markdown","7247c8a5":"markdown","7398eab8":"markdown","d05abcef":"markdown","e575209b":"markdown","b2cd6c6d":"markdown","112d4b09":"markdown","90d1f040":"markdown","60a4cddd":"markdown","b47c9f05":"markdown","f0beb931":"markdown","e463c9a0":"markdown","bc9073c3":"markdown","7797605d":"markdown","a56320c6":"markdown","93f8c42d":"markdown","d52c8f63":"markdown","bcc4e1b2":"markdown"},"source":{"82fc41b9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","918fb5f7":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","c868a381":"train.info()","252837b1":"train.head()","f65ad2ee":"test.head()","bc57dfe3":"train.shape,test.shape","890d1f7c":"categorical_cols_train = train.select_dtypes(include=['object'])\nnumeric_cols_train = train.select_dtypes(exclude=['object'])\nprint(f'The dataset contains {len(categorical_cols_train.columns.tolist())} categorical columns')\nprint(f'The dataset contains {len(numeric_cols_train.columns.tolist())} numeric columns')","012c4de5":"fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,3))\nsns.distplot(train['SalePrice'], ax=ax1).set(title='Distribution Plot of SalePrice')\nsns.distplot(np.log(train['SalePrice']),ax=ax2).set(title='Distribution Plot of Log SalePrice')\nplt.xlabel(\"Log SalePrice\")\nplt.show()","373d7d42":"train.drop(['Id'],axis=1,inplace=True)\ntest.drop(['Id'],axis=1,inplace=True)","8dd5792b":"features=[features for features in train.columns if train[features].isnull().sum()>1]\nfor feature in features:\n    print(feature, np.round(train[feature].isnull().mean()*100, 2),  '% missing values.\\n')","baa7e429":"plt.figure(figsize = (12, 10))\n(train.loc[:, train.isnull().any()].isna().sum() \/ train.shape[0]).sort_values().plot(kind = 'barh', label = '% of missing values')\nplt.axvline(x = 0.2, color = 'r', linestyle = '--', label = 'Reference line')\nplt.legend()\nplt.title('Percentage of missing values:')\nplt.xlabel('% of missing data')\nplt.show()","e18a7e61":"train=train.drop(['MiscFeature','Fence','PoolQC','FireplaceQu','Alley'],1)\ntest=test.drop(['MiscFeature','Fence','PoolQC','FireplaceQu','Alley'],1)","7624030e":"categorical_cols_train= train.select_dtypes(include=['object'])\nnumeric_cols_train = train.select_dtypes(exclude=['object'])\nprint(f'The dataset contains {len(categorical_cols_train.columns.tolist())} categorical columns')\nprint(f'The dataset contains {len(numeric_cols_train.columns.tolist())} numeric columns')","d7e599f7":"categorical_cols_train.info()","e8b01e3c":"categorical_cols_train_missing = categorical_cols_train.columns[categorical_cols_train.isnull().any()]\ncategorical_cols_train_missing","3535ccde":"for feature in categorical_cols_train_missing:\n  train[feature].fillna(train[feature].mode()[0],inplace=True)","4119fd58":"categorical_cols_test= test.select_dtypes(include=['object'])\ncategorical_cols_test_missing = categorical_cols_test.columns[categorical_cols_test.isnull().any()]\ncategorical_cols_test_missing","ed0e78f3":"for feature in categorical_cols_test_missing:\n  test[feature].fillna(train[feature].mode()[0],inplace=True)","309507ea":"plt.figure(figsize=(16, 10))\nsns.heatmap(train.isnull(),yticklabels=False,cbar=False)","9fbae745":"train.columns[train.isnull().any()]","69b53d7a":"train['LotFrontage'].hist(bins = 50)","2e508c12":"train['LotFrontage'].describe()","9c76e4f8":"train['LotFrontage'].fillna(np.random.randint(59,80), inplace = True)\ntest['LotFrontage'].fillna(np.random.randint(59,80), inplace = True)","e378204c":"train['MasVnrArea'].hist(bins = 50)","1c873b25":"train['MasVnrArea'].describe()","323d55bc":"train['MasVnrArea'].fillna(0, inplace = True)\ntest['MasVnrArea'].fillna(0, inplace = True)","d0472167":"train=train.drop(['GarageYrBlt'],1)\ntest=test.drop(['GarageYrBlt'],1)","00c01315":"features=[features for features in train.columns if train[features].isnull().sum()>1]\nif len(features)==0:\n    print(\"No missing values in training set\")","146717fa":"numeric_cols_test = test.select_dtypes(exclude=['object'])\nnumeric_cols_test_missing = numeric_cols_test.columns[numeric_cols_test.isnull().any()]\nnumeric_cols_test_missing","633153f0":"for feature in numeric_cols_test_missing:\n  test[feature].fillna(train[feature].mean(),inplace=True)","4c67e6f8":"features=[features for features in test.columns if test[features].isnull().sum()>1]\nif len(features)==0:\n    print(\"No missing values in test set\")","2f6c5a6d":"numeric_cols_train = train.select_dtypes(exclude=['object'])\ncorr = numeric_cols_train.corr()\nplt.figure(figsize=(14,8))\nplt.title('Numeric Feature Correlation of Sale Price', fontsize=18)\nsns.heatmap(corr,annot=False,cmap='RdYlBu',linewidths=0.2,annot_kws={'size':20})\nplt.show()","498fd5c3":"plt.figure(figsize=(6, 15))\nsns.barplot(data=corr.sort_values([\"SalePrice\"],axis=1)[-1:], orient='h')\nplt.axvline(x = 0.2, color = 'r', linestyle = '--', label = 'Reference line')\nplt.xlim(-0.2,1)\nplt.title(\"Numeric Features' Correlation for Sale Price\")\nplt.xlabel('Corrlelation')\nplt.ylabel('Numeric Features')","c380faf8":"print(\"Find most important features relative to target\")\ncorr = train.corr()\ncorr.sort_values([\"SalePrice\"], ascending = False, inplace = True)\nprint(corr.SalePrice)","0945bf63":"corrmat = train.corr()\ntop_numeric_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.2]\nplt.figure(figsize=(15,15))\ng = sns.heatmap(train[top_numeric_features].corr(),annot=True,cmap=\"RdYlGn\")","5b93fd27":"sns.boxplot(train.OverallQual,train.SalePrice)","ae997e40":"categorical_cols_train= train.select_dtypes(include=['object'])\ntrain_encoded = train.copy()\ndef encode(frame, feature):\n    ordering = pd.DataFrame()\n    ordering['val'] = frame[feature].unique()\n    ordering.index = ordering.val\n    ordering['SPmean'] = frame[[feature, 'SalePrice']].groupby(feature).mean()['SalePrice']\n    ordering = ordering.sort_values('SPmean')\n    ordering['ordering'] = range(1, ordering.shape[0]+1)\n    ordering = ordering['ordering'].to_dict()\n    \n    for cat, rank in ordering.items():\n        frame.loc[frame[feature] == cat, feature] = rank\n    \ncategorical_encoded = []\nfor cat in categorical_cols_train:  \n    encode(train_encoded, cat)\n    categorical_encoded.append(cat)","1312ddd4":"spr = pd.DataFrame()\nspr['Categorical Features'] = categorical_encoded\nspr['Spearman Correlation'] = [train_encoded[f].corr(train_encoded['SalePrice'],'spearman') for f in categorical_encoded]\nspr = spr.sort_values('Spearman Correlation')\nplt.figure(figsize=(6, 0.25*len(categorical_encoded)))\nsns.barplot(data=spr, y='Categorical Features', x='Spearman Correlation', orient='h')\nplt.axvline(x = 0.2, color = 'r', linestyle = '--', label = 'Reference line')\nplt.title(\"Categorical Features' Spearman Correlation for Sale Price\")","5e70ecca":"top_categorical_features = spr[spr['Spearman Correlation']>0.2]['Categorical Features']\ntrain_encoded[top_categorical_features]","8a206255":"selected_features = top_numeric_features.union(train_encoded[top_categorical_features].columns)\nselected_features","d5e54c82":"Plot = pd.concat([train[top_numeric_features],train_encoded[top_categorical_features]],1)\nfor i in range(0, len(selected_features), 6):\n    sns.pairplot(data = Plot,\n                x_vars = Plot.columns[i:i+6],\n                y_vars = ['SalePrice'])","364724d5":"df = pd.concat([train,test],axis=0)\ndf = df[selected_features]\ndf.shape","0f604ef6":"df = pd.get_dummies(df)\ndf.shape","b14f0594":"df_train=df.iloc[:1460,:]\ndf_test=df.iloc[1460:,:]","fb68d156":"X=df_train.drop(['SalePrice'],axis=1)\ny=df_train['SalePrice']","49911fc6":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","d10ab789":"from sklearn import metrics\n\ndef model_scores(true, predicted):  \n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    print('MAE:', mae)\n    print('MSE:', mse)\n    print('RMSE:', rmse)\n    print('R2 Square', r2_square)\n    print('__________________________________')\n    return [mae,mse,rmse,r2_square]","0517eb04":"from sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor","81720641":"lr= LinearRegression()\nlr.fit(x_train, y_train)\nprint(\"Training set score: {:.2f}\".format(lr.score(x_train, y_train))) \nprint(\"Test set score: {:.2f}\".format(lr.score(x_test, y_test)))\nprint(\"\\n_____________________________________\")\ntest_pred = lr.predict(x_test)\ntrain_pred = lr.predict(x_train)\n\nprint('Train set evaluation:\\n_____________________________________')\nmodel_scores(y_train, train_pred)\nprint('Test set evaluation:\\n_____________________________________')\nlr_score = model_scores(y_test, test_pred)","bcb26dff":"from sklearn import linear_model\nlasso = linear_model.Lasso(alpha=0.1, tol=1e-2)\nlasso.fit(x_train, y_train)\nprint(\"Training set score: {:.2f}\".format(lasso.score(x_train, y_train))) \nprint(\"Test set score: {:.2f}\".format(lasso.score(x_test, y_test)))\nprint(\"\\n_____________________________________\")\ntest_pred = lasso.predict(x_test)\ntrain_pred = lasso.predict(x_train)\n\nprint('Train set evaluation:\\n_____________________________________')\nmodel_scores(y_train, train_pred)\nprint('Test set evaluation:\\n_____________________________________')\nlasso_score = model_scores(y_test, test_pred)","0cbe5ac2":"from sklearn import linear_model\nridge = linear_model.Ridge(alpha=0.1)\nridge.fit(x_train, y_train)\nprint(\"Training set score: {:.2f}\".format(ridge.score(x_train, y_train))) \nprint(\"Test set score: {:.2f}\".format(ridge.score(x_test, y_test)))\nprint(\"\\n_____________________________________\")\ntest_pred = ridge.predict(x_test)\ntrain_pred = ridge.predict(x_train)\n\nprint('Train set evaluation:\\n_____________________________________')\nmodel_scores(y_train, train_pred)\nprint('Test set evaluation:\\n_____________________________________')\nridge_score = model_scores(y_test, test_pred)","a3146ec2":"from sklearn.linear_model import ElasticNet\nenet = ElasticNet()\nenet.fit(x_train, y_train)\nprint(\"Training set score: {:.2f}\".format(enet.score(x_train, y_train))) \nprint(\"Test set score: {:.2f}\".format(enet.score(x_test, y_test)))\nprint(\"\\n_____________________________________\")\ntest_pred = enet.predict(x_test)\ntrain_pred = enet.predict(x_train)\n\nprint('Train set evaluation:\\n_____________________________________')\nmodel_scores(y_train, train_pred)\nprint('Test set evaluation:\\n_____________________________________')\nenet_score = model_scores(y_test, test_pred)","85b36cd6":"dt= DecisionTreeRegressor()\ndt.fit(x_train, y_train)\nprint(\"Training set score: {:.2f}\".format(dt.score(x_train, y_train))) \nprint(\"Test set score: {:.2f}\".format(dt.score(x_test, y_test)))\nprint(\"\\n_____________________________________\")\ntest_pred = dt.predict(x_test)\ntrain_pred = dt.predict(x_train)\n\nprint('Train set evaluation:\\n_____________________________________')\nmodel_scores(y_train, train_pred)\nprint('Test set evaluation:\\n_____________________________________')\ndt_score = model_scores(y_test, test_pred)","90f6a977":"bg= BaggingRegressor()\nbg.fit(x_train, y_train)\nprint(\"Training set score: {:.2f}\".format(bg.score(x_train, y_train))) \nprint(\"Test set score: {:.2f}\".format(bg.score(x_test, y_test)))\nprint(\"\\n_____________________________________\")\ntest_pred = bg.predict(x_test)\ntrain_pred = bg.predict(x_train)\n\nprint('Train set evaluation:\\n_____________________________________')\nmodel_scores(y_train, train_pred)\nprint('Test set evaluation:\\n_____________________________________')\nbg_score = model_scores(y_test, test_pred)","200d8565":"gbm= GradientBoostingRegressor()\ngbm.fit(x_train, y_train)\nprint(\"Training set score: {:.2f}\".format(gbm.score(x_train, y_train))) \nprint(\"Test set score: {:.2f}\".format(gbm.score(x_test, y_test)))\nprint(\"\\n_____________________________________\")\ntest_pred = gbm.predict(x_test)\ntrain_pred =gbm.predict(x_train)\n\nprint('Train set evaluation:\\n_____________________________________')\nmodel_scores(y_train, train_pred)\nprint('Test set evaluation:\\n_____________________________________')\ngbm_score = model_scores(y_test, test_pred)","1b565a91":"rf= RandomForestRegressor()\nrf.fit(x_train, y_train)\nprint(\"Training set score: {:.2f}\".format(rf.score(x_train, y_train))) \nprint(\"Test set score: {:.2f}\".format(rf.score(x_test, y_test)))\nprint(\"\\n_____________________________________\")\ntest_pred = rf.predict(x_test)\ntrain_pred =rf.predict(x_train)\n\nprint('Train set evaluation:\\n_____________________________________')\nmodel_scores(y_train, train_pred)\nprint('Test set evaluation:\\n_____________________________________')\nrf_score = model_scores(y_test, test_pred)","d0d400b4":"from sklearn.ensemble import AdaBoostRegressor\nada= AdaBoostRegressor()\nada.fit(x_train, y_train)\nprint(\"Training set score: {:.2f}\".format(ada.score(x_train, y_train))) \nprint(\"Test set score: {:.2f}\".format(ada.score(x_test, y_test)))\nprint(\"\\n_____________________________________\")\ntest_pred = ada.predict(x_test)\ntrain_pred =ada.predict(x_train)\n\nprint('Train set evaluation:\\n_____________________________________')\nmodel_scores(y_train, train_pred)\nprint('Test set evaluation:\\n_____________________________________')\nada_score = model_scores(y_test, test_pred)","5c49a91e":"model_score = pd.DataFrame({'Linear Regression':lr_score, 'Lasso Regression':lasso_score, 'Ridge Regression':ridge_score,\n                    'Elastic Net Regression':enet_score, 'Decision Tree':dt_score, 'Bagging':bg_score,\n                    'Gradient Boosting':gbm_score, 'Random Forest':rf_score, 'Ada Boost':ada_score},\n                    index=['MAE','MSE','RMSE','R2 Square'])\npd.options.display.float_format = '{:.2e}'.format\nmodel_score","163a8acb":"model_score.sort_values(by='MAE',axis=1)","b27d3ed4":"model_score.sort_values(by='R2 Square',axis=1,ascending=False)","40da0717":"preds = gbm.predict(df_test.drop(['SalePrice'],axis=1))\nsubmission = pd.DataFrame({'ID': range(1461,2920),'SalePrice': preds})\nsubmission.to_csv('submission.csv', index=False)","b00c52e6":"<font size='4'>Split the train dataset in a ratio 7:3 for modeling.<\/font>","e1a33803":"<font size='4'>Find out the catergorical columns with missing value in test dataset and fill in the missing value by the most frequent value (mode) in train dataset.<\/font>","bde895aa":"<font size='4'>Create dummy features for categorical values via one-hot encoding.<\/font>","2a1c17cd":"<font size='4'>Using heatmap to show the numeric data consisting missing value in train dataset. The white bars are representing missing value.<\/font>","d7ebc0a2":"## Modeling","f1a12063":"<font size='4'>Show the most correlated feature by box plot.<\/font>","2fc93ed1":"<font size='4'>By observing the statistic of 'MasVnrArea',we can find that the features has >50% are 0, so we use 0 to fill in missing value.<\/font>","01953495":"<font size='4'>Find out the catergorical columns with missing value in train dataset and fill in the missing value by the most frequent value (mode) in train dataset.<\/font>","0e85b456":"<font size='4'>After dropping the features with more than 20 percent missing values, the number of numeric and categorical date changed and the number shown below.<\/font>","7140dbdb":"<font size='4'>Check again to ensure train dataset does not have missing value.<\/font>","a8b4551a":"## Result","7247c8a5":"<font size='4'>We using different model to train the data: <\/font>\n1. <font size='4'>Linear Regression<\/font>\n1. <font size='4'>Lasso Regression<\/font>\n1. <font size='4'>Ridge Regression<\/font>\n1. <font size='4'>Elastic Net Regression<\/font>\n1. <font size='4'>Decision Tree Regressor<\/font>\n1. <font size='4'>Bagging Regressor<\/font>\n1. <font size='4'>Gradient Boosting Regressor<\/font>\n1. <font size='4'>Random Forest Regressor<\/font>\n1. <font size='4'>AdaBoost Regressor<\/font>","7398eab8":"<font size='5'>Introduction<\/font>\n\n<font size='4'>House price prediction is a significant financial decision for individuals working in the housing market as well as for potential buyers. Also, house prices can be predicted by the given explanatory variables that cover many aspects of residential houses.<\/font>\n    \n<font size='4'>Our project goal:<\/font>\n* <font size='4'>To create a suitable model<\/font>\n* <font size='4'>Able to estimate the price of the house given the features.<\/font>","d05abcef":"<font size='4'>Evaluate the models by MAE , MSE , RMSE , R2_square. Select the best model by using MAE and R2_square.<\/font>","e575209b":"## Feature Selection","b2cd6c6d":"<font size='4'>By observing the statistic of 'LotFrontage' ,we use random integer between first quantile 25% and third quantile 75% to fill in missing value.<\/font>","112d4b09":"<font size='4'>Find out the percentage of missing value in each features for train dataset.<\/font>","90d1f040":"## Data Description","60a4cddd":"## Data Preprocessing","b47c9f05":"<font size='4'>Depending on Test set score (highest) and MAE (lowest), Gradient Boosting Regressor is the best model to predict the SalePrice.<\/font>","f0beb931":"<font size='4'>Rank catergorical feature by mean of SalePrice and select features with spearman correlation >0.2 for modeling.<\/font>","e463c9a0":"<font size='4'>Find out the columns with missing value in test dataset and fill in the missing value by the mean value in train dataset.<\/font>","bc9073c3":"<font size='4'>Since 'GarageYrBlt' is measuring date and it is hard to impute missing value of date value correctly, so we decide to drop the features for both train and test dataset.<\/font>","7797605d":"<font size='4'>Find out correlation between numeric features and SalePrice and select features with correlation >0.2 for modeling.<\/font>","a56320c6":"<font size='4'>Check again to ensure test dataset does not have missing value.<\/font>","93f8c42d":"<font size='4'>Can see that features 'LotFrontage', 'MasVnrArea', 'GarageYrBlt' are having missing value.<\/font>","d52c8f63":"<font size='4'>Create pair plot for selected features.<\/font>","bcc4e1b2":"<font size='4'>Remove features with more than 20 percent missing values.<\/font>"}}