{"cell_type":{"e0b9a61f":"code","93a9999d":"code","fcb49e4c":"code","4f00d94d":"code","084b1ab6":"code","d21609cd":"code","64d8cc6e":"code","37651628":"code","62cc58d9":"code","0fea722c":"code","55a7b400":"code","31c10524":"code","624693e1":"code","d7dd37cb":"code","fc5f8fed":"code","e78356c6":"code","d7168ce0":"code","166f9985":"code","360acb1e":"code","b611913c":"code","9a0f6aa2":"code","133e42d8":"code","a0ae8b3e":"code","8278d44f":"code","bff30d0c":"code","27786ccd":"code","3e39fa83":"code","c9e8f79b":"code","7298b819":"code","bf1559b5":"code","2598b772":"code","44605930":"code","2bf67f57":"code","e6d03494":"code","9473cc91":"code","c45b3e88":"code","b98a5ada":"code","db913077":"code","2fbfffa4":"code","4e2c5d50":"markdown","3577e885":"markdown"},"source":{"e0b9a61f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","93a9999d":"# cross entropy loss : it is used in NLMS as well as other classification tasks \n# what is a loss function \n# intuitively : telss us how bad a model is doing at predicting the training data \n# in NLM how bad is the model at predicting the next word \n# assume we have a training ex \n# \"Stuedents opened their\"=> \"books\"\n# p(\"books\"| \"students open their\") we want to maximise this probability in our training data \n# we want to minimize the negative log probabilities \n# L = -log(p(books | students opened their))\n\n# why this called cross entroy loss ? \n# cross entory of two distributoins let say p & q \n# quantifies distance between the distributions \n# H(p, q) = -(summation(p(w)logq(w))) this is we can treat as ground truth distribiton \n# p(w) is 1 for books and 0 for every other type \n# q(w) is predicted conditiional probs \n#\n\n","fcb49e4c":"sentences = ['mohit likes icecream', 'Starks were cool', 'life is fool', 'batman is ironman']\n","4f00d94d":"# we will try to predict next word using NLM \n\n# step one : Tokenization \n\nvocab = {} # map from word type to index \ninputs = [] # stores an indexified version of each sentences \n\n\nfor sent in sentences:\n    sent_idxes = []\n    sent = sent.split() #tokenize w\/ whitespace \n    \n    for w in sent: \n        if w not in vocab: \n            vocab[w] = len(vocab) # add new type to the vocab \n            \n        sent_idxes.append(vocab[w])\n    \n    inputs.append(sent_idxes)","084b1ab6":"print(vocab)","d21609cd":"print(inputs)","64d8cc6e":"import torch \n\n# 1 convert to long tensors \n# 2 define inpts and outpts\nprefixes = torch.LongTensor([sent[:-1] for sent in inputs])\n#print(prefixes)\nlabels = torch.LongTensor([sent[-1] for sent in inputs])\nprint('Prefix',prefixes,'Labels', labels)","37651628":"# onto defining the network \nimport torch.nn as nn\n\nclass NLM(nn.Module):\n    # important things \n    # 1. write init functions (initializes all teh paarameters of the network )\n    # 2. forward functions (defines the forward propagation computations)\n    \n    \n    def __init__(self, d_embedding, d_hidden, window_size, len_vocab):\n        super(NLM, self).__init__() # init the base module class \n        self.d_emb = d_embedding\n        self.embeddings = nn.Embedding(len_vocab, d_embedding)\n        \n        # concatenated embeddings > hidden\n        self.W_hid = nn.Linear(d_embedding * window_size, d_hidden)\n        \n        # hidden > output probability distribution over vocab \n        self.W_out = nn.Linear(d_hidden, len_vocab)\n        \n        \n    def forward(self, input):\n        batch_size, window_size = input.size()\n        embs = self.embeddings(input)\n        #print('Embedding size',embs.size())\n        \n        # next we want to concatenate the prefix emveddings together\n        concat_embs = embs.view(batch_size, window_size * self.d_emb)\n        #print('concat_embs', concat_embs.size())\n        \n        #print(embs[0])\n        #print(concat_embs[0])\n\n        # now we project thsi to the hidden space \n        hiddens = self.W_hid(concat_embs)\n        \n#        print('hidden size:',hiddens.size())\n        \n        \n        # finally project hiddens to vocabulary space \n        out = self.W_out(hiddens)\n #       print(out.size())\n        \n        return out # return unnormalized probability also known as **logits** \n        \n        #probs = nn.functional.softmax(out, dim=1)\n        #print(probs)\nnetwork = NLM(d_embedding=5, d_hidden=12, window_size=2, len_vocab=len(vocab))\n    ","62cc58d9":"print(network)","0fea722c":"logits = network(prefixes)\nprint(logits)","55a7b400":"# number of epochs : how many times we are going trhough our network \nnum_epochs = 100 \nlearning_rate = 0.1 \nloss_fn = nn.CrossEntropyLoss()# it applies log softmax function and then negative log likelihood \n\n# we will use vanilla gradient descent  you can expreiment with others like adam \noptimizer = torch.optim.SGD(params = network.parameters(), lr=learning_rate)\n\n# training loop \nfor i in range(num_epochs):\n    logits = network(prefixes)\n    loss = loss_fn(logits, labels)\n    \n    # now let;s update our params to make the loss smaaller \n   # print(loss)\n\n    #Step1 compute gradient \n    loss.backward()\n    # step 2 update params using gradient descent \n    optimizer.step()\n    \n    # zero the gradients for next epoch\n    optimizer.zero_grad()\n    print(f'Epoch {i}, loss {loss}')\n","31c10524":"# is it working\n# reverse vocabulary mapping (idx> word type)\n\nrev_vocab =dict((idx, word) for (word, idx) in vocab.items()) ","624693e1":"rev_vocab","d7dd37cb":"mohitlikes = prefixes[0].unsqueeze(0)\nlogits = network(mohitlikes)\n#print(logits)\nprob = nn.functional.softmax(logits, dim=1).squeeze()\n\nprint(prob)","fc5f8fed":"argmax_idx = torch.argmax(prob).item()\nprint('given mohit likes model predicts \"%s\" as the next word with %0.4f probability '%(rev_vocab[argmax_idx], prob[argmax_idx]))","e78356c6":"lifeis = prefixes[2].unsqueeze(0)\nlogits = network(lifeis)\n#print(logits)\nprob = nn.functional.softmax(logits, dim=1).squeeze()\n\nprint(prob)\nargmax_idx = torch.argmax(prob).item()\nprint('given \"life is\" model predicts \"%s\" as the next word with %0.4f probability '%(rev_vocab[argmax_idx], prob[argmax_idx]))","d7168ce0":"# let's no use a dataset to Generate new text with help of GRU\n","166f9985":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport nltk \nimport string \nimport unidecode \nimport random \nimport torch \n","360acb1e":"# to check the GPU is working or not \ndevice = torch.cuda.is_available()\nif device : \n    print('GPU is available')\nelse : \n    print(\"we are working on CPU ;P\")","b611913c":"# let's load spooky data set\n\ntrain = pd.read_csv('..\/input\/spooky\/train.csv')\ntrain.head()","9a0f6aa2":"train.author.value_counts()","133e42d8":"# we will use MWS only for this task \ntext = list(train[train['author']=='MWS']['text'][:2000])","a0ae8b3e":"text[:10]","8278d44f":"def joinString(text):\n    return ' '.join(s for s in text)\ntext = joinString(text)\nlen(text.split())","bff30d0c":"text = text.lower().split()","27786ccd":"trigrams = [([text[i], text[i+1]], text[i+2]) for i in range(len(text) - 2)]","3e39fa83":"trigrams[:2]","c9e8f79b":"vocab = set(text)\nprint(len(vocab))","7298b819":"voc_len = len(vocab)\nword_to_idx = { word: i for i, word in enumerate(vocab)}","bf1559b5":"inp = []\ntar = []\nfor context, target in trigrams : \n    context_idxs = torch.tensor([word_to_idx[w] for w in context], dtype=torch.long)\n    inp.append(context_idxs)\n    targ = torch.tensor([word_to_idx[target]], dtype=torch.long)\n    tar.append(targ)","2598b772":"# Let's build our model now \nimport torch\nimport torch.nn as nn \nfrom torch.autograd import Variable \n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n        super(RNN, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size \n        self.n_layers = n_layers\n        \n        self.encoder = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size*2, hidden_size, n_layers, batch_first=True, bidirectional=False)\n        self.decoder = nn.Linear(hidden_size, output_size)\n        \n        \n    def forward(self, input, hidden):\n        input = self.encoder(input.view(1, -1))\n        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n        output = self.decoder(output.view(1, -1))\n        return output, hidden\n    \n    def init_hidden(self):\n        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))","44605930":"rnn = RNN(2, 10, 1)","2bf67f57":"print(rnn)","e6d03494":"# training \nchunklen = len(trigrams)\ndef train(inp , target):\n    hidden = decoder.init_hidden().cuda()\n    decoder.zero_grad()\n    loss = 0\n    \n    for c in range(chunklen):\n        output, hidden = decoder(inp[c].cuda(), hidden)\n        loss += criterion(output, target[c].cuda())\n        \n    loss.backward()\n\n    decoder_optimizer.step()\n    \n    return loss.data.item()\/chunklen\n        \n        ","9473cc91":"import time, math \ndef time_since(since):\n    s = time.time() - since\n    m = math.floor(s\/60)\n    s -= m * 60\n    \n    return '%dm %ds' %(m, s)","c45b3e88":"n_epochs =40\nprint_every = 5\nplot_every = 1\nhidden_size = 100\nn_layers = 1 \nlr = 0.015 \n\ndecoder = RNN(voc_len, hidden_size, voc_len, n_layers)\ndecoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()\nstart = time.time()\nall_losses = []\nloss_avg = 0\nif(device):\n    decoder.cuda()\nfor epoch in range(1, n_epochs+1): \n    loss = train(inp, tar)\n    loss_avg  +=loss\n    \n    if epoch % print_every ==0:\n        print(\"[%s (%d %d%%) %.4f]\" %(time_since(start), epoch, epoch\/n_epochs * 50, loss))\n    \n    if epoch % plot_every == 0: \n        all_losses.append(loss_avg\/plot_every)\n        loss_avg = 0","b98a5ada":"def inference(prime_str='ok so', predict_len=100, temp = 0.8):\n    hidden = decoder.init_hidden().cuda()\n    \n    for p in range(predict_len):\n        prime_input = torch.tensor([word_to_idx[w] for w in prime_str.split()], dtype=torch.long).cuda()\n        inp = prime_input[-2:]\n        output, hidden = decoder(inp, hidden)\n        \n        output_dist = output.data.view(-1).div(temp).exp()\n        top_i = torch.multinomial(output_dist, 1)[0]\n        \n        # add predicted word to string and use as next input\n        predicted_word = list(word_to_idx.keys())[list(word_to_idx.values()).index(top_i)]\n        prime_str +=\" \"+predicted_word\n        \n    return prime_str","db913077":"print(inference(\"i am\", 40, temp=1))","2fbfffa4":"print(inference(\"proud of\", 40))","4e2c5d50":"# TASK 2\n    let's no use a dataset to Generate new text with help of GRU\n","3577e885":"# Simple Neural Langauge Model \nIn this notebook I learned how to create language model from scratch. I followed UMASS Lec of Mohit Iyyer 365. [Lecture](https:\/\/www.youtube.com\/watch?v=pZt2TLSl7L4&list=PLWnsVgP6CzadmQX6qevbar3_vDBioWHJL&index=6)\nIn this note book I followed below steps:\n\n1. Created a sentences \n2. Created vocabulary from it with idx \n3. Build a simple neural Network with help of pytorch \n4. Did predictions like predicting next word (3rd since I am using 3 words sentence) \n"}}