{"cell_type":{"dd05929b":"code","55c73e98":"code","9ea9eda0":"code","764466a8":"code","419c40c0":"code","f9181372":"code","3d05a247":"code","fc9f54a9":"code","7072b161":"code","71e72521":"code","f89d0841":"code","61c918b2":"code","5b9feb66":"code","8a67a0b9":"code","daa1aa9d":"code","26259531":"code","89b51c8b":"code","bb1d40a9":"code","c959fb01":"code","042a29bc":"code","36ab71c8":"code","e06ff27c":"code","a2d5c809":"markdown","98aea457":"markdown","f237e4a7":"markdown","29ec5911":"markdown","ea6b7fe2":"markdown","2eb8c667":"markdown","3df7744c":"markdown","ab4fc859":"markdown","e8769886":"markdown","ba93fa27":"markdown","7f7d781c":"markdown","4cae9633":"markdown","224db458":"markdown","840b29d2":"markdown","d13247d0":"markdown","b12ad7ce":"markdown","e3850132":"markdown","205ec1d3":"markdown"},"source":{"dd05929b":"#!pip -q install ..\/input\/tabnet-install\/pytorch_tabnet-3.1.1-py3-none-any.whl","55c73e98":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\nimport gresearch_crypto\nimport datetime\nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_absolute_error as MSE\nimport warnings\nwarnings.filterwarnings('ignore')\n\nTRAIN_CSV = '\/kaggle\/input\/g-research-crypto-forecasting\/train.csv'\nASSET_DETAILS_CSV = '\/kaggle\/input\/g-research-crypto-forecasting\/asset_details.csv'\n\nSEED = 20\n\nREMOVE_LB_TEST_OVERLAPPING_DATA = True","9ea9eda0":"\n!cp ..\/input\/talibinstall\/ta-lib-0.4.0-src.tar.gzh  .\/ta-lib-0.4.0-src.tar.gz\n!tar -xzvf ta-lib-0.4.0-src.tar.gz > null\n!cd ta-lib && .\/configure --prefix=\/usr > null && make  > null && make install > null\n!cp ..\/input\/talibinstall\/TA-Lib-0.4.21.tar.gzh TA-Lib-0.4.21.tar.gz\n!pip install TA-Lib-0.4.21.tar.gz > null\n!pip install ..\/input\/talibinstall\/numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl >null\nimport talib as ta\n","764466a8":"def fix_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nfix_all_seeds(SEED)","419c40c0":"df_train = pd.read_csv(TRAIN_CSV)\ndf_train","f9181372":"df_test = pd.read_csv('..\/input\/g-research-crypto-forecasting\/example_test.csv')\ndf_test.head()","3d05a247":"%%capture\n'''\nl =len(df_train)\nl = l\/\/2\ndf_train = df_train[l:].reset_index(drop=True)\ndf_train\n'''","fc9f54a9":"# Remove the future\nif REMOVE_LB_TEST_OVERLAPPING_DATA:\n    df_train['datetime'] = pd.to_datetime(df_train['timestamp'], unit='s')\n    df_valid = df_train[(df_train['datetime'] > '2021-06-13 00:00:00')].reset_index(drop=True)\n    df_train = df_train[(df_train['datetime'] < '2021-06-13 00:00:00')].reset_index(drop=True)\n    #df_valid = df_train[(df_train['datetime'] > '2021-06-13 00:00:00') | (df_train['datetime'] < '2019-01-01 00:00:00')]\n    #df_train = df_train[(df_train['datetime'] < '2021-06-13 00:00:00') | (df_train['datetime'] > '2019-01-01 00:00:00')]\n    #print('delete data  ','train=',len(df_train),'  valid=',len(df_valid))\n    df_train  = df_train.drop(['datetime'],axis=1)\n    df_valid  = df_valid.drop(['datetime'],axis=1)\nelse:\n    df_train['datetime'] = pd.to_datetime(df_train['timestamp'], unit='s')\n    df_valid = df_train[(df_train['datetime'] > '2021-06-13 00:00:00')].reset_index(drop=True)\n    df_train  = df_train.drop(['datetime'],axis=1)\n    df_valid  = df_valid.drop(['datetime'],axis=1)\n","7072b161":"df_valid = df_valid.dropna(subset=['Target']).reset_index(drop=True)\n#bkup =df_valid.copy()","71e72521":"df_asset_details = pd.read_csv(ASSET_DETAILS_CSV).sort_values(\"Asset_ID\")\ndf_asset_details","f89d0841":"from sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score\n#!pip install optuna\nimport optuna \nimport optuna.integration.lightgbm as lgbo","61c918b2":"def log_return(series, periods=5):\n    return np.log(series).diff(periods=periods)\n\n# Two features from the competition tutorial\ndef upper_shadow(df): return df['High'] - np.maximum(df['Close'], df['Open'])\ndef lower_shadow(df): return np.minimum(df['Close'], df['Open']) - df['Low']","5b9feb66":"def add_features(df):\n    \n    df[\"high_div_low\"] = df[\"High\"] \/ df[\"Low\"]\n    df[\"open_sub_close\"] = df[\"Open\"] - df[\"Close\"]\n    #df[\"open_sub_close2\"] = df[\"Open\"] \/ df[\"Close\"] #\n    df[\"open_sub_close_1\"] = df[\"open_sub_close\"].shift(-4)\n    \n    df['Open_shift-1'] = df['Open'].shift(-1)\n    df['Open_shift-4'] = df['Open'].shift(-4)\n    df['Open_shift-7'] = df['Open'].shift(-7)\n    \n    df['Open_shift1'] = df['Open'].shift(1)\n    \n    #df['Close_shift-1'] = df['Close'].shift(-1)\n    \n    df['close_log1'] = log_return(df['Close'],periods=1)\n    df['close_log4'] = log_return(df['Close'],periods=4)\n    #df['close_log7'] = log_return(df['Close'],periods=7)#\n    \n    #df['open_log1'] = log_return(df['Open'],periods=1)#\n    \n    #df['close_log4-1'] =df['close_log4'].shift(-1) #\n    #df['avg5'] =  ta.SMA(df['Open'], timeperiod=5) #\u79fb\u52d5\u5e73\u5747\n    #df['avg25'] = ta.SMA(df['Open'], timeperiod=25) #df['Open'].rolling(25).mean()\n    df['avg75'] = ta.SMA(df['Open'], timeperiod=75 )#df['Open'].rolling(75).mean()\n    df['ema']   =ta.EMA(df['Open'], timeperiod=20) \n    df['willr'] = ta.WILLR(df['High'], df['Low'],np.array(df.loc[:, 'Close']), timeperiod=14)\n    times = pd.to_datetime(df[\"timestamp\"],unit=\"s\",infer_datetime_format=True)\n    df[\"hour\"] = times.dt.hour  \n    df[\"dayofweek\"] = times.dt.dayofweek \n    df[\"day\"] = times.dt.day \n    \n    #df['RSI-9'] = ta.RSI(df['Close'], timeperiod=9)#\n    df['RSI'] = ta.RSI(df['Close'], timeperiod=14)\n    df['RSI1'] = df['RSI'].shift(-1)\n    df['RSI4'] = df['RSI'].shift(-4)\n    df['RSI7'] = df['RSI'].shift(-7)\n    df['RSI10'] = df['RSI'].shift(-10)\n    df['RSI13'] = df['RSI'].shift(-13) \n    df['RSI16'] = df['RSI'].shift(-16)  \n    df['RSI~4'] = df['RSI'].shift(4)  #\n    #df['RSI~7'] = df['RSI'].shift(7)  #\n    #df['RSI+1'] = df['RSI'].shift(1)\n    \n    df['MACD'], df['macdsignal'], df['MACD_HIST'] = ta.MACD(df['Close'], fastperiod=12, slowperiod=26, signalperiod=9)\n    #df['MACD1'] = df['MACD'].shift(-1)\n    #df['MACD4'] = df['MACD'].shift(-4)\n    #df['MACD7'] = df['MACD'].shift(-7)\n    #df['MACD_HIST-4'] =df['MACD_HIST'].shift(4)# \n    df['MACD_HIST1'] =df['MACD_HIST'].shift(-1) \n    df['MACD_HIST4'] =df['MACD_HIST'].shift(-4) \n    df['MACD_HIST7'] =df['MACD_HIST'].shift(-7) \n    #df['MACD_HIST10'] =df['MACD_HIST'].shift(-10)#\n    #df['u_bandx'], df['m_bandx'], df['l_bandx'] = ta.BBANDS(df['Close'], timeperiod=7, nbdevup=2, nbdevdn=2, matype=0)#\n    \n    df['u_band'], df['m_band'], df['l_band'] = ta.BBANDS(df['Close'], timeperiod=5, nbdevup=2, nbdevdn=2, matype=0) \n    df['adx'] = ta.ADX(df['High'], df['Low'],np.array(df.loc[:, 'Close']),timeperiod=14)\n    df['adx1'] = df['adx'].shift(-1)\n    df['adx4'] = df['adx'].shift(-4)\n    df['adx+1'] = df['adx'].shift(1)\n    #df['adx+2'] = df['adx'].shift(7)\n    df['adx7'] = df['adx'].shift(-7)\n    #df['adx10'] = df['adx'].shift(-10)#\n    #df['adx+4'] = df['adx'].shift(4)\n    #df['adx+1'] = df['adx'].shift(1)\n    #df['adx7-12'] = ta.ADX(df['High'], df['Low'],np.array(df.loc[:, 'Close']),timeperiod=4) #\n    \n    df['macdsignal1'] = df['macdsignal'].shift(-1)\n    df['macdsignal4'] = df['macdsignal'].shift(-4)\n    df['macdsignal7'] = df['macdsignal'].shift(-7)\n    #df['macdsignal10'] = df['macdsignal'].shift(-10) #\n    \n    df['DI_plus'] = ta.PLUS_DI(df['High'], df['Low'],np.array(df.loc[:, 'Close']), timeperiod=14)\n    df['DI_minus'] = ta.MINUS_DI(df['High'], df['Low'],np.array(df.loc[:, 'Close']), timeperiod=14)\n    \n    #df = df.drop(['Close'],axis=1)\n    #df['DI_plus1+1'] = df['DI_plus'].shift(1) #\n    df['DI_plus1'] = df['DI_plus'].shift(-1)\n    df['DI_plus4'] = df['DI_plus'].shift(-4)\n    df['DI_plus7'] = df['DI_plus'].shift(-7) #\u3044\u308b\n    df['DI_plus10'] = df['DI_plus'].shift(-10)\n    \n    df['DI_minus1'] = df['DI_minus'].shift(-1)\n    df['DI_minus4'] = df['DI_minus'].shift(-4)\n    df['DI_minus7'] = df['DI_minus'].shift(-7)\n    #df['DI_minus10'] = df['DI_minus'].shift(-10) #\n    \n    df['ROCP'] =ta.ROCP(df['Open'])\n    df['momentam'] =ta.MOM(df['Open'])\n    \n    df['APO'] =ta.APO(df['Open'])\n    df['APO1'] = df['APO'].shift(-1)\n    df['APO4'] = df['APO'].shift(-4)\n    df['APO7'] = df['APO'].shift(-7) \n    \n    df['PPO'] =ta.PPO(df['Open'])\n    #df['PPO1'] = df['PPO'].shift(-1)\n    #df['PPO4'] = df['PPO'].shift(-4)\n    #df['PPO7'] = df['PPO'].shift(-7)\n    #df['vwap-c'] =(1-np.exp(df['VWAP']))\n    df = df.drop(['VWAP'],axis=1)\n\n    df['CMO'] =ta.CMO(df['Open'])\n    df['MIDPOINT'] =ta.MIDPOINT(df['Open'])\n    #df['MIDPOINT1'] =df['MIDPOINT'].shift(-1)\n    df['TRENDLINE'] =ta.HT_TRENDLINE(df['Open'])\n    #df['TRENDLINE1']= df['TRENDLINE'].shift(-1)\n    #df['timestamp1'] = df['timestamp'].shift(-1)\n    return df ","8a67a0b9":"df_train","daa1aa9d":"   %%capture\n    '''\n    df = df_train #[df_train[\"Asset_ID\"] == asset_id]\n    df = df.dropna(subset=['Target'])\n    \n    y = df['Target'] \n    df      = df.drop(['Target','Asset_ID'],axis=1)\n    #df_proc = get_features(df)\n    df_proc = add_features(df)\n    df_proc = df_proc.fillna(-1)\n    X= df_proc #.drop(\"y\", axis=1)\n    \nnunique = X.nunique()\ntypes = X.dtypes\n\ncategorical_columns = []\ncategorical_dims =  {}\n\nfor col in df_test.columns:\n    if  col == 'Asset_ID' :\n        l_enc = LabelEncoder()\n        X[col] = l_enc.fit_transform(X[col].values)\n        X_test[col] = l_enc.transform(df_test[col].values)\n        categorical_columns.append(col)\n        categorical_dims[col] = len(l_enc.classes_)\n    else:\n      if col != 'Target':\n        scaler = StandardScaler()\n        X[col] = scaler.fit_transform(X[col].values.reshape(-1, 1))\n        df_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))\n        \n\n\ncat_idxs = [ i for i, f in enumerate(X.columns.tolist()) if f in categorical_columns]\n\ncat_dims = [ categorical_dims[f] for i, f in enumerate(X.columns.tolist()) if f in categorical_columns]\n\ntabnet_params1 = dict(\n    cat_idxs=cat_idxs,\n    cat_dims=cat_dims,\n    cat_emb_dim=1,\n    n_d = 8, #16 8*3\n    n_a = 8, #16\n    n_steps = 2,\n    gamma = 2,\n    n_independent = 2,\n    n_shared = 2,\n    lambda_sparse = 0,#\n    optimizer_fn = Adam,\n    optimizer_params = dict(lr = (2e-2)),\n    mask_type = \"Target\",\n    scheduler_params = dict(T_0=220, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n    scheduler_fn = CosineAnnealingWarmRestarts,\n    seed = 77,\n    verbose = 10\n    \n)\n\nX_train,X_val,y_train,y_val = train_test_split(X.values, y.values.reshape(-1,1), test_size=0.2,random_state = 77)\n\n   clf =  TabNetRegressor(**tabnet_params1)\n    clf.fit(\n      X_train, y_train,\n      eval_set=[(X_val, y_val)],\n      max_epochs = 100,\n      patience = 5,#50\n      batch_size = 1024*20, \n      virtual_batch_size = 128*20,\n      num_workers = 4,\n      drop_last = False,\n      eval_metric=[RMSPE],\n      loss_fn=RMSPELoss\n      )\n'''","26259531":"# Two new features from the competition tutorial\ndef upper_shadow(df):\n    return df['High'] - np.maximum(df['Close'], df['Open'])\n\ndef lower_shadow(df):\n    return np.minimum(df['Close'], df['Open']) - df['Low']\n\n# A utility function to build features from the original df\n# It works for rows to, so we can reutilize it.\ndef get_features(df_feat):\n    #df_feat = df[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']].copy()\n    df_feat['Upper_Shadow'] = upper_shadow(df_feat)\n    df_feat['Lower_Shadow'] = lower_shadow(df_feat)\n    return df_feat\n\ndef get_Xy_and_model_for_asset(df_train,asset_id):\n    df = df_train[df_train[\"Asset_ID\"] == asset_id]\n    df = df.dropna(subset=['Target'])\n    \n    y = df['Target'] \n    df      = df.drop(['Target','Asset_ID'],axis=1)\n    df_proc = get_features(df)\n    df_proc = add_features(df_proc)\n    df_proc = df_proc.fillna(-1)\n    X= df_proc #.drop(\"y\", axis=1)\n    \n    \n    best_lgb_params ={'objective': 'regression',\n    'metric': 'rmse',\n    'feature_pre_filter': False,\n    'lambda_l1': 0.010565309968664168,\n    'lambda_l2': 0.3120057367604998,\n    'num_leaves': 255,\n    'feature_fraction': 1.0,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 0,\n    'min_child_samples': 5}\n\n    #best_lgb_params =model.params\n    best_lgb_params[\"learning_rate\"] = 0.01\n\n    best_lgb_params[\"bagging_freq\"] = 0\n    best_lgb_params[\"device\"] = 'gpu'\n    best_lgb_params[\"early_stopping_round\"] = 100\n    best_lgb_params[\"num_iterations\"] = 2000\n    \n    x_train,x_test,y_train,y_test = train_test_split(X, y, test_size=0.2,random_state = 77)\n    lgb_train = lgb.Dataset(x_train, y_train)\n    lgb_valid = lgb.Dataset(x_test, y_test)\n     \n    model = lgb.train(best_lgb_params,lgb_train, valid_sets=[lgb_valid], verbose_eval=100)\n    \n    #model = LGBMRegressor(n_estimators=1500,num_leaves=700,learning_rate=0.1,silent=True)\n    #model.fit(X, y)\n    \n    #print( 'step3',datetime.datetime.now().strftime('%Y\u5e74%m\u6708%d\u65e5 %H:%M:%S'))\n    #fi =model.feature_importances_\n    #fi_df = pd.DataFrame({'feature': list(X.columns),\n    #     'feature importance': fi[:]}).sort_values('feature importance', ascending = False)\n    #if asset_id ==0:\n    #    display(fi_df)\n    return model","89b51c8b":"Xs = {}\nys = {}\nmodels = {}\n\nfor asset_id, asset_name in zip(df_asset_details['Asset_ID'], df_asset_details['Asset_Name']):\n    print(f\"Training model for {asset_name:<16} (ID={asset_id:<2})   \",datetime.datetime.now().strftime('%Y\/%m\/%d %H:%M:%S'))\n    \n    model = get_Xy_and_model_for_asset(df_train, asset_id)    \n    models[asset_id] =  model\n    \n    #validation\n    x_pred = pd.DataFrame()\n    x = asset_id\n    record = df_valid[df_valid.Asset_ID == x]   \n    target = record.Target \n    record = record.drop(['Target','Asset_ID'],axis=1)\n    model = models[x]\n    x_test = get_features(record)\n    x_test = add_features(x_test) \n    #x_test = x_test.drop(['Asset_ID','Target'],axis=1)\n        \n    x_pred['x'] = model.predict(x_test)\n    print('Test score for LR baseline: ', f\"{np.corrcoef(x_pred.x, target)[0,1]:.5f}\")\n    #print('MSE=',MSE(x_pred.x, target))\n    #print(x_pred.x[:10])\n    #print(target[:10])\n    del record\n    del x_pred\n    del x_test\n    \n    #break","bb1d40a9":"import pickle\nwith open('models','wb') as web:\n    pickle.dump(models,web)","c959fb01":"    x_pred = pd.DataFrame()\n    for x in range(len(df_valid.Asset_ID.unique())):\n        print( 'Asset_ID=',x,' ',datetime.datetime.now().strftime('%Y\u5e74%m\/%d\/ %H:%M:%S'))\n        record = df_valid[df_valid.Asset_ID == x]     \n        record = record.drop(['Target','Asset_ID'],axis=1)\n        model = models[x]\n        x_test = get_features(record)\n        x_test = add_features(pd.DataFrame(x_test)) #.values\n        #x_test = x_test.drop(['Asset_ID','Target'],axis=1)\n\n        x_test['y_pred'] = model.predict(x_test)\n        x_pred = pd.concat([x_test,x_pred])    \n    x_pred = x_pred.sort_index()    \n    print(datetime.datetime.now().strftime('%Y\/%m\/%d %H:%M:%S'),'Test score for LR baseline: ', f\"{np.corrcoef(x_pred.y_pred,df_valid.Target)[0,1]:.5f}\")","042a29bc":"env = gresearch_crypto.make_env()\niter_test = env.iter_test()\n\nfor i, (df_test, df_pred) in enumerate(iter_test):\n    #test = pd.DataFrame(index=(df_test['timestamp'].unique()))\n    #df_test = get_features(df_test)\n    \n    #df_valid.set_index(\"timestamp\",inplace=True) \n    #df_proc = df_proc.fillna(-1)\n    #x_pred = pd.DataFrame()\n    #timestamp= pd.DataFrame()\n    for x in df_test.Asset_ID.unique():\n        #print('Asset_Id=',x)\n        model = models[x]\n        x_test = df_test[df_test.Asset_ID == x]\n        x_test = get_features(x_test)\n        x_test = add_features(x_test)\n\n        row_id = x_test.row_id\n        x_test = x_test.drop(['row_id','Asset_ID'],axis=1)        \n        #timestamp = data['timestamp']\n        #row_id = data['row_id'].values\n        \n        #x_test = df_proc[df_proc.index.isin(data.timestamp.values)]\n        \n        #print('Asset_Id=',x,'  predict',len(x_test.columns),' ',x_test.columns)\n        x_test['pred'] = model.predict(x_test)\n        \n        x_test['row_id'] =row_id\n        #x_pred = pd.concat([x_pred,x_test])\n      \n        for j,row in x_test.iterrows():\n            df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = row.pred\n        \n    #if x == 0:\n    #    display(df_pred)\n        \n    env.predict(df_pred)","36ab71c8":"df_test","e06ff27c":"df_pred","a2d5c809":"## Feature Function \u3092\u4f5c\u308b\n\n## Make Feature Function","98aea457":"## \u4eee\u60f3\u901a\u8ca8\u306e\u904e\u53bb\u306e\u5927\u91cf\u306e\u53d6\u5f15\u30c7\u30fc\u30bf\u304b\u3089\u3001Target\u3092\u4e88\u6e2c\u3059\u308b\u30b3\u30f3\u30da\u3067\u3059\u3002<br>\nEDA\u306f\u3053\u3061\u3089\u3002https:\/\/www.kaggle.com\/tensorchoko\/g-research-crypto-forecasting-eda-jp-en\n\n## It is a competition to predict Target from a large amount of past transaction data of virtual currency. <br>\nClick here for EDA. https:\/\/www.kaggle.com\/tensorchoko\/g-research-crypto-forecasting-eda-jp-en","f237e4a7":"# TabNet under development","29ec5911":"### <u> \u3044\u308d\u3044\u308d\u3053\u306e\u5909\u6570\u3092\u5909\u3048\u3066\u3044\u307e\u3059\u3002\u3069\u3046\u3059\u308c\u3070\u6700\u9069\u304b\u3002\u3059\u3054\u304f\u3044\u3044\u306e\u304c\u3067\u304d\u305f\u3089\u3001\u3053\u306enotebook\u306f\u975e\u516c\u958b\u306b\u3059\u308b\u4e88\u5b9a\u3067\u3059\u3002\u6226\u95d8\u30e2\u30fc\u30c9\u3078\u7a81\u5165<\/u>\n\n### <u> I am changing this variable in various ways. What should I do best? I plan to keep this notebook private when I can do something really good.Enter battle mode <\/u>","ea6b7fe2":"![image.png](attachment:11d7f4d8-3b08-4b4c-a077-6661dcad5152.png)","2eb8c667":"### \u30b3\u30a4\u30f3\u6bce\u3067\u306f\u306a\u304f\u3001\u30c8\u30fc\u30bf\u30eb\u306eValidation\u30b9\u30b3\u30a2\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002\n\n### Calculate the total Validation score, not per coin.","3df7744c":"# model save","ab4fc859":"# Predict & submit","e8769886":"## \u30b3\u30a4\u30f3\u4e8b\u306b\u4e88\u6e2c\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u679c\u305f\u3057\u3066\u305d\u308c\u304c\u6b63\u3057\u3044\u306e\u304b\u7591\u554f\u306b\u601d\u3063\u3066\u3044\u307e\u3059\u3002\n\n## I'm predicting coin, but I'm wondering if that's right.","ba93fa27":"### submit\u5f8c\u306b\u3001\u3068\u306b\u304b\u304f\u6642\u9593\u304c\u304b\u304b\u308b\u3002\n### \u9ad8\u901f\u5316\u306f\u521d\u5fc3\u8005\u306a\u304c\u3089\u3084\u3063\u305f\u3064\u3082\u308a\u3002\n\n### After submitting, it takes time anyway.\n### I think I did speeding up even though I was a beginner.","7f7d781c":"## Leak\u306b\u306a\u308b\u30c7\u30fc\u30bf\u3092\u5916\u3059\u3002valid\u30c7\u30fc\u30bf\u3092\u4f5c\u308b\u3002\n\n## Remove the data that becomes Leak. Create valid data.","4cae9633":"## Training & Validation","224db458":"# In Progress","840b29d2":"![image.png](attachment:c4570d0a-1090-4c73-ac8a-217930015158.png)","d13247d0":"# Validation","b12ad7ce":" \u63d0\u51fa\u7248\u306f\u3001REMOVE_LB_TEST_OVERLAPPING_DATA = False\u306b\u3059\u308b\n\nFor the submitted version, set REMOVE_LB_TEST_OVERLAPPING_DATA = False","e3850132":"\ud83d\ude3a\ud83d\ude05\u3299\ud83d\udd30\ud83d\uddd1\u2b1b\ud83d\udfe5\ud83d\udfe8\ud83d\udfe9","205ec1d3":"### \u30c7\u30fc\u30bf\u3092\u76f4\u8fd1\u306e\u3082\u306e\u3060\u3051\u306b\u3059\u308b\u3000\u4eca\u306f\u7121\u52b9\u5316\n\n### Make the data only the latest one. Now disabled"}}