{"cell_type":{"fe3d961d":"code","2285d893":"code","4bc467c9":"code","73b06403":"code","1bfd67d8":"code","d65b8ddb":"code","0d26830d":"code","5965cd74":"code","d9903952":"code","78ec2426":"code","5ccdb985":"code","9c7437c6":"code","4a0338da":"code","dcdcb86c":"code","fc362543":"code","cd40aa30":"code","b6a4d852":"code","94ab5cb8":"code","35731126":"code","d5593a43":"code","46eb92c4":"code","18c98d5b":"code","5b9cfccf":"code","a53636ee":"code","61dd6117":"code","c57defa3":"code","6905d95e":"code","aedb1b8f":"code","66d8f824":"code","45da0ba0":"code","96e58d23":"code","2cee94e3":"code","057fdb83":"code","c079e684":"code","8ef2ce9e":"code","a1d4151f":"code","eaa6abd0":"code","66ecae9d":"code","b1d54193":"code","52413377":"code","851fd3af":"code","4a5cfa1d":"code","894a3438":"code","cdc3ebae":"code","0b5e6ed2":"code","5144eac2":"code","32179693":"code","e302c860":"code","c00196a9":"code","1c9a872f":"code","bd1b3b50":"code","a44d55dd":"code","61bb6550":"code","1ff6448a":"markdown","3997293a":"markdown","32a606d3":"markdown","9f37cbe9":"markdown","ffbcf96f":"markdown","9864bfbf":"markdown","096542d9":"markdown","ee176a1b":"markdown","046505d4":"markdown","15c999a6":"markdown","cda101d9":"markdown","81ccb6dd":"markdown","9e73156d":"markdown","9529110b":"markdown","64068368":"markdown","93779610":"markdown","2a7896a5":"markdown","08fdf8fd":"markdown","8529968b":"markdown","c2bee724":"markdown","b1cae486":"markdown","a1984634":"markdown","dfa53fcf":"markdown","47aa34e1":"markdown","d4ca207c":"markdown","76ea1e33":"markdown","26ad7e9d":"markdown"},"source":{"fe3d961d":"import time\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfrom IPython.display import clear_output","2285d893":"optimal_policy = pd.read_csv('..\/input\/OptimalPolicy_angletol45.csv')\noptimal_policy.head()","4bc467c9":"# Create Quiver plot showing current optimal policy in one cell\noptimal_action_list = optimal_policy.copy()\n\nx = optimal_action_list['state_x']\ny = optimal_action_list['state_y']\nu = optimal_action_list['u'].values\nv = optimal_action_list['v'].values\nplt.figure(figsize=(10, 10))\nsns.scatterplot( x=\"state_x\", y=\"state_y\", data=optimal_action_list,  hue='Action', alpha = 0.3)\nplt.quiver(x,y,u,v,scale=0.5,scale_units='inches')\nplt.title(\"Optimal Policy for Given Probabilities\")\nplt.show()","73b06403":"# Probability Function\ndef probability(bin_x, bin_y, state_x, state_y, throw_deg):\n\n\n    #First throw exception rule if person is directly on top of bin:\n    if((state_x==bin_x) & (state_y==bin_y)):\n        probability = 1\n    else:\n        \n        \n        # To accomodate for going over the 0 degree line\n        if((throw_deg>270) & (state_x<=bin_x) & (state_y<=bin_y)):\n            throw_deg = throw_deg - 360\n        elif((throw_deg<90) & (state_x>bin_x) & (state_y<bin_y)):\n            throw_deg = 360 + throw_deg\n        else:\n            throw_deg = throw_deg\n            \n        # Calculate Euclidean distance\n        distance = ((bin_x - state_x)**2 + (bin_y - state_y)**2)**0.5\n\n        # max distance for bin will always be on of the 4 corner points:\n        corner_x = [-10,-10,10,10]\n        corner_y = [-10,10,-10,10]\n        dist_table = pd.DataFrame()\n        for corner in range(0,4):\n            dist = pd.DataFrame({'distance':((bin_x - corner_x[corner])**2 + (bin_y - corner_y[corner])**2)**0.5}, index = [corner])\n            dist_table = dist_table.append(dist)\n        dist_table = dist_table.reset_index()\n        dist_table = dist_table.sort_values('distance', ascending = False)\n        max_dist = dist_table['distance'][0]\n        \n        distance_score = 1 - (distance\/max_dist)\n\n\n        # First if person is directly horizontal or vertical of bin:\n        if((state_x==bin_x) & (state_y>bin_y)):\n            direction = 180\n        elif((state_x==bin_x) & (state_y<bin_y)):\n             direction = 0\n        \n        elif((state_x>bin_x) & (state_y==bin_y)):\n             direction = 270\n        elif((state_x<bin_x) & (state_y==bin_y)):\n             direction = 90\n              \n        # If person is north-east of bin:\n        elif((state_x>bin_x) & (state_y>bin_y)):\n            opp = abs(bin_x - state_x)\n            adj = abs(bin_y - state_y)\n            direction = 180 +  np.degrees(np.arctan(opp\/adj))\n\n        # If person is south-east of bin:\n        elif((state_x>bin_x) & (state_y<bin_y)):\n            opp = abs(bin_y - state_y)\n            adj = abs(bin_x - state_x)\n            direction = 270 +  np.degrees(np.arctan(opp\/adj))\n\n        # If person is south-west of bin:\n        elif((state_x<bin_x) & (state_y<bin_y)):\n            opp = abs(bin_x - state_x)\n            adj = abs(bin_y - state_y)\n            direction =  np.degrees(np.arctan(opp\/adj))\n\n        # If person is north-west of bin:\n        elif((state_x<bin_x) & (state_y>bin_y)):\n            opp = abs(bin_y - state_y)\n            adj = abs(bin_x - state_x)\n            direction = 90 +  np.degrees(np.arctan(opp\/adj))\n\n        direction_score = (45-abs(direction - throw_deg))\/45\n      \n        probability = distance_score*direction_score\n        if(probability>0):\n            probability = probability\n        else:\n            probability = 0\n        \n    return(probability)\n    ","1bfd67d8":"#Define Q(s,a) table by all possible states and THROW actions initialised to 0\nQ_table = pd.DataFrame()\nfor z in range(0,360):\n    throw_direction = int(z)\n    for i in range(0,21):\n        state_x = int(-10 + i)\n        for j in range(0,21):\n            state_y = int(-10 + j)\n            reward = 0\n            Q = pd.DataFrame({'throw_dir':throw_direction,'move_dir':\"none\",'state_x':state_x,'state_y':state_y,'Q':0, 'reward': reward}, index = [0])\n            Q_table = Q_table.append(Q)\nQ_table = Q_table.reset_index(drop=True)\nprint(\"Q table 1 initialised\")\n\n#Define Q(s,a) table by all possible states and MOVE actions initialised to 0\n\nfor x in range(0,21):\n    state_x = int(-10 + x)\n    for y in range(0,21):\n        state_y = int(-10 + y)\n        for m in range(0,8):\n            move_dir = int(m)\n            \n            # skip impossible moves starting with 4 corners then edges\n            if((state_x==10)&(state_y==10)&(move_dir==0)):\n                continue\n            elif((state_x==10)&(state_y==10)&(move_dir==2)):\n                continue\n                \n            elif((state_x==10)&(state_y==-10)&(move_dir==2)):\n                continue\n            elif((state_x==10)&(state_y==-10)&(move_dir==4)):\n                continue\n                \n            elif((state_x==-10)&(state_y==-10)&(move_dir==4)):\n                continue\n            elif((state_x==-10)&(state_y==-10)&(move_dir==6)):\n                continue\n                \n            elif((state_x==-10)&(state_y==10)&(move_dir==6)):\n                continue\n            elif((state_x==-10)&(state_y==10)&(move_dir==0)):\n                continue\n                \n            elif((state_x==10) & (move_dir == 1)):\n                continue\n            elif((state_x==10) & (move_dir == 2)):\n                continue\n            elif((state_x==10) & (move_dir == 3)):\n                continue\n                 \n            elif((state_x==-10) & (move_dir == 5)):\n                continue\n            elif((state_x==-10) & (move_dir == 6)):\n                continue\n            elif((state_x==-10) & (move_dir == 7)):\n                continue\n                 \n            elif((state_y==10) & (move_dir == 1)):\n                continue\n            elif((state_y==10) & (move_dir == 0)):\n                continue\n            elif((state_y==10) & (move_dir == 7)):\n                continue\n                 \n            elif((state_y==-10) & (move_dir == 3)):\n                continue\n            elif((state_y==-10) & (move_dir == 4)):\n                continue\n            elif((state_y==-10) & (move_dir == 5)):\n                continue\n                 \n            else:\n                reward = 0\n                Q = pd.DataFrame({'throw_dir':\"none\",'move_dir':move_dir,'state_x':state_x,'state_y':state_y,'Q':0, 'reward': reward}, index = [0])\n                Q_table = Q_table.append(Q)\nQ_table = Q_table.reset_index(drop=True)\nprint(\"Q table 2 initialised\")\nQ_table.tail()","d65b8ddb":"# Initialise V values for all state-action pairs\nQ_table['V'] = 0","0d26830d":"# Calculate Probability of each State-Action pair, 1 for movement else use probability function\nbin_x = 0\nbin_y = 0\n\nprob_list = pd.DataFrame()\nfor n,action in enumerate(Q_table['throw_dir']):\n    # Guarantee 100% probability if movement\n    if(action == \"none\"):\n        prob = 1\n    # Calculate if thrown\n    else:\n        prob = probability(bin_x, bin_y, Q_table['state_x'][n], Q_table['state_y'][n], action)\n    prob_list = prob_list.append(pd.DataFrame({'prob':prob}, index = [n] ))\nprob_list = prob_list.reset_index(drop=True)\nQ_table['prob'] = prob_list['prob']","5965cd74":"Q_table.head()","d9903952":"# Define start position\nstart_x = -5\nstart_y = -5","78ec2426":"# Subset the Q table for just this start state and randomly select an action\nQ_table[ (Q_table['state_x']==start_x) &  (Q_table['state_y']==start_y) & (Q_table['move_dir']!=\"none\") ].sample()","5ccdb985":"a_1 = Q_table[ (Q_table['state_x']==start_x) &  (Q_table['state_y']==start_y) & (Q_table['move_dir']!=\"none\") ].sample()\n\n\nmove_direction = a_1['move_dir'].iloc[0]\n#Map this to actual direction and find V(s) for next state\nif(move_direction == 0):\n    move_x = 0\n    move_y = 1\nelif(move_direction == 1):\n    move_x = 1\n    move_y = 1\nelif(move_direction == 2):\n    move_x = 1\n    move_y = 0\nelif(move_direction == 3):\n    move_x = 1\n    move_y = -1\nelif(move_direction == 4):\n    move_x = 0\n    move_y = -1\nelif(move_direction == 5):\n    move_x = -1\n    move_y = -1\nelif(move_direction == 6):\n    move_x = -1\n    move_y = 0\nelif(move_direction == 7):\n    move_x = -1\n    move_y = 1\n\nnew_x = a_1['state_x'].iloc[0]+move_x\nnew_y = a_1['state_y'].iloc[0]+move_y\n    \na_2 = Q_table[ (Q_table['state_x']==new_x) &  (Q_table['state_y']==new_y) & (Q_table['move_dir']!=\"none\") ].sample()\na_2","9c7437c6":"# Define start position\nstart_x = -5\nstart_y = -5\naction_cap = 100\n\naction_table = pd.DataFrame()\nfor a in range(0,action_cap):\n    \n    # Introduce 50\/50 chance for move or throw action\n    rng = np.random.rand()\n    if rng<=0.5:\n        action_class = \"throw\"\n    else:\n        action_class = \"move\"\n    \n    # THROW ACTION\n    if action_class == \"throw\":\n        # If first action, use start state\n        if a==0:\n            action = Q_table[(Q_table['state_x']==start_x) &  (Q_table['state_y']==start_y) & (Q_table['throw_dir']!=\"none\")].sample()\n        # Else new x and y are from previous itneration's output\n        else:\n            new_x = action['state_x'].iloc[0]\n            new_y = action['state_y'].iloc[0]\n            action = Q_table[(Q_table['state_x']==new_x) &  (Q_table['state_y']==new_y) & (Q_table['throw_dir']!=\"none\")].sample()\n    \n    # ELSE MOVE ACTION\n    else:\n        # If first action, use start state\n        if a==0:\n            action = Q_table[(Q_table['state_x']==start_x) &  (Q_table['state_y']==start_y) & (Q_table['throw_dir']==\"none\")].sample()\n        # Else new x and y are from previous itneration's output\n        else:\n            move_direction = action['move_dir'].iloc[0]\n            #Map this to actual direction and find V(s) for next state\n            if(move_direction == 0):\n                move_x = 0\n                move_y = 1\n            elif(move_direction == 1):\n                move_x = 1\n                move_y = 1\n            elif(move_direction == 2):\n                move_x = 1\n                move_y = 0\n            elif(move_direction == 3):\n                move_x = 1\n                move_y = -1\n            elif(move_direction == 4):\n                move_x = 0\n                move_y = -1\n            elif(move_direction == 5):\n                move_x = -1\n                move_y = -1\n            elif(move_direction == 6):\n                move_x = -1\n                move_y = 0\n            elif(move_direction == 7):\n                move_x = -1\n                move_y = 1\n\n            new_x = action['state_x'].iloc[0]+move_x\n            new_y = action['state_y'].iloc[0]+move_y\n            action = Q_table[(Q_table['state_x']==new_x) &  (Q_table['state_y']==new_y) & (Q_table['throw_dir']==\"none\")].sample()\n            \n    action_table = action_table.append(action)\n   \n    # Break loop if action is a throw\n    if action['throw_dir'].iloc[0]!=\"none\":\n        break\n    else:\n        continue\naction_table = action_table.reset_index(drop=True)     \naction_table.head()\n    \n    ","4a0338da":"# Define start position\nstart_x = -5\nstart_y = -5\naction_cap = 100\n\nepsilon = 0.1\n\naction_table = pd.DataFrame()\nfor a in range(0,action_cap):\n    \n    \n    rng_epsilon = np.random.rand()\n\n    # If our rng is less than or equal to the epsilon parameter, we randomly select\n    if rng_epsilon<=epsilon:\n        # Introduce 50\/50 chance for move or throw action\n        rng = np.random.rand()\n        if rng<=0.5:\n            action_class = \"throw\"\n        else:\n            action_class = \"move\"\n\n        # THROW ACTION\n        if action_class == \"throw\":\n            # If first action, use start state\n            if a==0:\n                action = Q_table[(Q_table['state_x']==start_x) &  (Q_table['state_y']==start_y) & (Q_table['throw_dir']!=\"none\")].sample()\n            # Else new x and y are from previous itneration's output\n            else:\n                new_x = action['state_x'].iloc[0]\n                new_y = action['state_y'].iloc[0]\n                action = Q_table[(Q_table['state_x']==new_x) &  (Q_table['state_y']==new_y) & (Q_table['throw_dir']!=\"none\")].sample()\n\n        # ELSE MOVE ACTION\n        else:\n            # If first action, use start state\n            if a==0:\n                action = Q_table[(Q_table['state_x']==start_x) &  (Q_table['state_y']==start_y) & (Q_table['throw_dir']==\"none\")].sample()\n            # Else new x and y are from previous itneration's output\n            else:\n                move_direction = action['move_dir'].iloc[0]\n                #Map this to actual direction and find V(s) for next state\n                if(move_direction == 0):\n                    move_x = 0\n                    move_y = 1\n                elif(move_direction == 1):\n                    move_x = 1\n                    move_y = 1\n                elif(move_direction == 2):\n                    move_x = 1\n                    move_y = 0\n                elif(move_direction == 3):\n                    move_x = 1\n                    move_y = -1\n                elif(move_direction == 4):\n                    move_x = 0\n                    move_y = -1\n                elif(move_direction == 5):\n                    move_x = -1\n                    move_y = -1\n                elif(move_direction == 6):\n                    move_x = -1\n                    move_y = 0\n                elif(move_direction == 7):\n                    move_x = -1\n                    move_y = 1\n\n                new_x = action['state_x'].iloc[0]+move_x\n                new_y = action['state_y'].iloc[0]+move_y\n                action = Q_table[(Q_table['state_x']==new_x) &  (Q_table['state_y']==new_y) & (Q_table['throw_dir']==\"none\")].sample()\n\n    #  If our rng is more than the epsilon parameter, we select the best action (\"greedily\")\n    else:\n        # Sort by V, use previous action if not first in episode\n        if a==0:\n            sorted_actions = Q_table[(Q_table['state_x']==start_x) &  (Q_table['state_y']==start_y)].sort_values('V', ascending = False)\n        else:\n            move_direction = action['move_dir'].iloc[0]\n            #Map this to actual direction and find V(s) for next state\n            if(move_direction == 0):\n                move_x = 0\n                move_y = 1\n            elif(move_direction == 1):\n                move_x = 1\n                move_y = 1\n            elif(move_direction == 2):\n                move_x = 1\n                move_y = 0\n            elif(move_direction == 3):\n                move_x = 1\n                move_y = -1\n            elif(move_direction == 4):\n                move_x = 0\n                move_y = -1\n            elif(move_direction == 5):\n                move_x = -1\n                move_y = -1\n            elif(move_direction == 6):\n                move_x = -1\n                move_y = 0\n            elif(move_direction == 7):\n                move_x = -1\n                move_y = 1\n\n            new_x = action['state_x'].iloc[0]+move_x\n            new_y = action['state_y'].iloc[0]+move_y\n            sorted_actions = Q_table[(Q_table['state_x']==new_x) &  (Q_table['state_y']==new_y)].sort_values('V', ascending = False)\n            \n        best_action = sorted_actions[sorted_actions['V'] == sorted_actions['V'].iloc[0]]\n\n        # If we only have one best action, simply pick this\n        if len(best_action)==1:\n            action = sorted_actions.iloc[0]\n            \n        # Otherwise, if we have multiple \"best\" actions, we randomly select from these \"best\" actions\n        else:\n            rng = np.random.rand()\n            if rng<=0.5:\n                action_class = \"throw\"\n            else:\n                action_class = \"move\"\n            # THROW ACTION\n            if action_class == \"throw\":\n                action = best_action[(best_action['throw_dir']!=\"none\")].sample()\n            # ELSE MOVE ACTION\n            else:\n                action = best_action[(best_action['throw_dir']==\"none\")].sample()\n    \n    action_table = action_table.append(action)\n   \n    # Break loop if action is a throw\n    if action['throw_dir'].iloc[0]!=\"none\":\n        break\n    else:\n        continue\naction_table = action_table.reset_index(drop=True)     \naction_table.head()\n    \n    ","dcdcb86c":"def eps_greedy_V(Q_table, epsilon, start_x, start_y, action_num, action):\n    a = action_num\n    rng_epsilon = np.random.rand()\n\n    # If our rng is less than or equal to the epsilon parameter, we randomly select\n    if rng_epsilon<=epsilon:\n        # Introduce 50\/50 chance for move or throw action\n        rng = np.random.rand()\n        if rng<=0.5:\n            action_class = \"throw\"\n        else:\n            action_class = \"move\"\n\n        # THROW ACTION\n        if action_class == \"throw\":\n            # If first action, use start state\n            if a==0:\n                action = Q_table[(Q_table['state_x']==start_x) &  (Q_table['state_y']==start_y) & (Q_table['throw_dir']!=\"none\")].sample()\n            # Else new x and y are from previous itneration's output\n            else:\n                new_x = action['state_x'].iloc[0]\n                new_y = action['state_y'].iloc[0]\n                action = Q_table[(Q_table['state_x']==new_x) &  (Q_table['state_y']==new_y) & (Q_table['throw_dir']!=\"none\")].sample()\n\n        # ELSE MOVE ACTION\n        else:\n            # If first action, use start state\n            if a==0:\n                action = Q_table[(Q_table['state_x']==start_x) &  (Q_table['state_y']==start_y) & (Q_table['throw_dir']==\"none\")].sample()\n            # Else new x and y are from previous itneration's output\n            else:\n                move_direction = action['move_dir'].iloc[0]\n                #Map this to actual direction and find V(s) for next state\n                if(move_direction == 0):\n                    move_x = 0\n                    move_y = 1\n                elif(move_direction == 1):\n                    move_x = 1\n                    move_y = 1\n                elif(move_direction == 2):\n                    move_x = 1\n                    move_y = 0\n                elif(move_direction == 3):\n                    move_x = 1\n                    move_y = -1\n                elif(move_direction == 4):\n                    move_x = 0\n                    move_y = -1\n                elif(move_direction == 5):\n                    move_x = -1\n                    move_y = -1\n                elif(move_direction == 6):\n                    move_x = -1\n                    move_y = 0\n                elif(move_direction == 7):\n                    move_x = -1\n                    move_y = 1\n\n                new_x = action['state_x'].iloc[0]+move_x\n                new_y = action['state_y'].iloc[0]+move_y\n                action = Q_table[(Q_table['state_x']==new_x) &  (Q_table['state_y']==new_y) & (Q_table['throw_dir']==\"none\")].sample()\n\n    #  If our rng is more than the epsilon parameter, we select the best action (\"greedily\")\n    else:\n        # Sort by V, use previous action if not first in episode\n        if a==0:\n            sorted_actions = Q_table[(Q_table['state_x']==start_x) &  (Q_table['state_y']==start_y)].sort_values('V', ascending = False)\n        else:\n            move_direction = action['move_dir'].iloc[0]\n            #Map this to actual direction and find V(s) for next state\n            if(move_direction == 0):\n                move_x = 0\n                move_y = 1\n            elif(move_direction == 1):\n                move_x = 1\n                move_y = 1\n            elif(move_direction == 2):\n                move_x = 1\n                move_y = 0\n            elif(move_direction == 3):\n                move_x = 1\n                move_y = -1\n            elif(move_direction == 4):\n                move_x = 0\n                move_y = -1\n            elif(move_direction == 5):\n                move_x = -1\n                move_y = -1\n            elif(move_direction == 6):\n                move_x = -1\n                move_y = 0\n            elif(move_direction == 7):\n                move_x = -1\n                move_y = 1\n\n            new_x = action['state_x'].iloc[0]+move_x\n            new_y = action['state_y'].iloc[0]+move_y\n            sorted_actions = Q_table[(Q_table['state_x']==new_x) &  (Q_table['state_y']==new_y)].sort_values('V', ascending = False)\n            \n        best_action = sorted_actions[sorted_actions['V'] == sorted_actions['V'].iloc[0]]\n\n        # If we only have one best action, simply pick this\n        if len(best_action)==1:\n            action = best_action\n            \n        # Otherwise, if we have multiple \"best\" actions, we randomly select from these \"best\" actions\n        else:\n            rng = np.random.rand()\n            if rng<=0.5:\n                action_class = \"throw\"\n            else:\n                action_class = \"move\"\n            # THROW ACTION\n            if action_class == \"throw\":\n                #Add excemption if no throw directions in \"best\" actions\n                if len(best_action[(best_action['throw_dir']!=\"none\")])>0:\n                    action = best_action[(best_action['throw_dir']!=\"none\")].sample()\n                else:\n                    action = best_action[(best_action['throw_dir']==\"none\")].sample()\n            # ELSE MOVE ACTION\n            else:\n                action = best_action[(best_action['throw_dir']==\"none\")].sample()\n\n    return(action)\n\n","fc362543":"def eps_greedy_Q(Q_table, epsilon, start_x, start_y, action_num, action):\n    a = action_num\n    rng_epsilon = np.random.rand()\n\n    # If our rng is less than or equal to the epsilon parameter, we randomly select\n    if rng_epsilon<=epsilon:\n        # Introduce 50\/50 chance for move or throw action\n        rng = np.random.rand()\n        if rng<=0.5:\n            action_class = \"throw\"\n        else:\n            action_class = \"move\"\n\n        # THROW ACTION\n        if action_class == \"throw\":\n            # If first action, use start state\n            if a==0:\n                action = Q_table[(Q_table['state_x']==start_x) &  (Q_table['state_y']==start_y) & (Q_table['throw_dir']!=\"none\")].sample()\n            # Else new x and y are from previous itneration's output\n            else:\n                new_x = action['state_x'].iloc[0]\n                new_y = action['state_y'].iloc[0]\n                action = Q_table[(Q_table['state_x']==new_x) &  (Q_table['state_y']==new_y) & (Q_table['throw_dir']!=\"none\")].sample()\n\n        # ELSE MOVE ACTION\n        else:\n            # If first action, use start state\n            if a==0:\n                action = Q_table[(Q_table['state_x']==start_x) &  (Q_table['state_y']==start_y) & (Q_table['throw_dir']==\"none\")].sample()\n            # Else new x and y are from previous itneration's output\n            else:\n                move_direction = action['move_dir'].iloc[0]\n                #Map this to actual direction and find V(s) for next state\n                if(move_direction == 0):\n                    move_x = 0\n                    move_y = 1\n                elif(move_direction == 1):\n                    move_x = 1\n                    move_y = 1\n                elif(move_direction == 2):\n                    move_x = 1\n                    move_y = 0\n                elif(move_direction == 3):\n                    move_x = 1\n                    move_y = -1\n                elif(move_direction == 4):\n                    move_x = 0\n                    move_y = -1\n                elif(move_direction == 5):\n                    move_x = -1\n                    move_y = -1\n                elif(move_direction == 6):\n                    move_x = -1\n                    move_y = 0\n                elif(move_direction == 7):\n                    move_x = -1\n                    move_y = 1\n\n                new_x = action['state_x'].iloc[0]+move_x\n                new_y = action['state_y'].iloc[0]+move_y\n                action = Q_table[(Q_table['state_x']==new_x) &  (Q_table['state_y']==new_y) & (Q_table['throw_dir']==\"none\")].sample()\n\n    #  If our rng is more than the epsilon parameter, we select the best action (\"greedily\")\n    else:\n        # Sort by V, use previous action if not first in episode\n        if a==0:\n            sorted_actions = Q_table[(Q_table['state_x']==start_x) &  (Q_table['state_y']==start_y)].sort_values('Q', ascending = False)\n        else:\n            move_direction = action['move_dir'].iloc[0]\n            #Map this to actual direction and find V(s) for next state\n            if(move_direction == 0):\n                move_x = 0\n                move_y = 1\n            elif(move_direction == 1):\n                move_x = 1\n                move_y = 1\n            elif(move_direction == 2):\n                move_x = 1\n                move_y = 0\n            elif(move_direction == 3):\n                move_x = 1\n                move_y = -1\n            elif(move_direction == 4):\n                move_x = 0\n                move_y = -1\n            elif(move_direction == 5):\n                move_x = -1\n                move_y = -1\n            elif(move_direction == 6):\n                move_x = -1\n                move_y = 0\n            elif(move_direction == 7):\n                move_x = -1\n                move_y = 1\n\n            new_x = action['state_x'].iloc[0]+move_x\n            new_y = action['state_y'].iloc[0]+move_y\n            sorted_actions = Q_table[(Q_table['state_x']==new_x) &  (Q_table['state_y']==new_y)].sort_values('Q', ascending = False)\n            \n        best_action = sorted_actions[sorted_actions['Q'] == sorted_actions['Q'].iloc[0]]\n\n        # If we only have one best action, simply pick this\n        if len(best_action)==1:\n            action = pd.DataFrame(best_action)\n            \n        # Otherwise, if we have multiple \"best\" actions, we randomly select from these \"best\" actions\n        else:\n            rng = np.random.rand()\n            if rng<=0.5:\n                action_class = \"throw\"\n            else:\n                action_class = \"move\"\n            # THROW ACTION\n            if action_class == \"throw\":\n                #Add excemption if no throw directions in \"best\" actions\n                if len(best_action[(best_action['throw_dir']!=\"none\")])>0:\n                    action = best_action[(best_action['throw_dir']!=\"none\")].sample()\n                else:\n                    action = best_action[(best_action['throw_dir']==\"none\")].sample()\n            # ELSE MOVE ACTION\n            else:\n                action = best_action[(best_action['throw_dir']==\"none\")].sample()\n    \n\n    return(action)\n\n","cd40aa30":"# Define start position\nstart_x = -5\nstart_y = -5\naction_cap = 100\n\nepsilon = 0.1\n\naction_table = pd.DataFrame()\naction = None\nfor a in range(0,action_cap):\n    \n    action = eps_greedy_V(Q_table, epsilon, start_x, start_y, a, action)\n    \n    action_table = action_table.append(action)\n   \n    # Break loop if action is a throw\n    if action['throw_dir'].iloc[0]!=\"none\":\n        break\n    else:\n        continue\naction_table = action_table.reset_index(drop=True)     \naction_table.head()\n    \n    \n    \n    ","b6a4d852":"# Define start position\nstart_x = -5\nstart_y = -5\naction_cap = 100\n\nepsilon = 0.1\nalpha = 0.5\ngamma = 0.5\nV_bin = 0\n\n\n# Make a copy of the initalised Q table so we don't override this\nQ_table_TD = Q_table.copy()\n\naction_table = pd.DataFrame()\naction = None\nfor a in range(0,action_cap):\n    \n    action = eps_greedy_V(Q_table_TD, epsilon, start_x, start_y, a, action)\n    # If action is to throw, use probability to find whether this was successful or not and update accordingly\n    if action['throw_dir'].iloc[0]!=\"none\":\n        rng_throw = np.random.rand()\n        \n        if rng_throw <= action['prob'].iloc[0]:\n            reward = 1\n        else:\n            reward = -1\n        New_V = action['V'].iloc[0] + alpha*(reward + (gamma* V_bin) - action['V'].iloc[0])\n    # If move action, we have guaranteed probability and currently no reward for this\n    else:\n        New_V = action['V'].iloc[0]\n    #Update V value for state based on outcome\n    Q_table_TD['V'] = np.where( ((Q_table_TD['state_x'] == action['state_x'].iloc[0]) & \n                                (Q_table_TD['state_y'] == action['state_y'].iloc[0])),New_V, Q_table_TD['V'])\n    \n    \n    action_table = action_table.append(action)\n   \n    # Break loop if action is a throw\n    if action['throw_dir'].iloc[0]!=\"none\":\n        break\n    else:\n        continue\naction_table = action_table.reset_index(drop=True)     \naction_table.head()\n    \n    \n    \n    ","94ab5cb8":"Q_table_TD.sort_values('V',ascending=False).drop_duplicates(['state_x', 'state_y', 'Q', 'reward','V']).head()","35731126":"# Define start position\nstart_x = -5\nstart_y = -5\naction_cap = 10000\n\nepsilon = 0.1\nalpha = 0.5\ngamma = 0.5\nV_bin = 0\n\nnum_episodes = 100\n\n# Make a copy of the initalised Q table so we don't override this\nQ_table_TD = Q_table.copy()\n\naction_table = pd.DataFrame()\nbest_states_table = pd.DataFrame()\nfor e in range(0,num_episodes):\n    action = None\n    for a in range(0,action_cap):\n\n        action = eps_greedy_V(Q_table_TD, epsilon, start_x, start_y, a, action)\n        # If action is to throw, use probability to find whether this was successful or not and update accordingly\n        if action['throw_dir'].iloc[0]!=\"none\":\n            rng_throw = np.random.rand()\n\n            if rng_throw <= action['prob'].iloc[0]:\n                reward = 1\n            else:\n                reward = -1\n            New_V = action['V'].iloc[0] + alpha*(reward + (gamma* V_bin) - action['V'].iloc[0])\n        # If move action, we have guaranteed probability and currently no reward for this\n        else:\n            New_V = action['V'].iloc[0]\n        #Update V value for state based on outcome\n        Q_table_TD['V'] = np.where( ((Q_table_TD['state_x'] == action['state_x'].iloc[0]) & \n                                    (Q_table_TD['state_y'] == action['state_y'].iloc[0])),New_V, Q_table_TD['V'])\n    \n        #Add column to denote which episode this is for\n        action['episode'] = e\n        action_table = action_table.append(action)\n\n        # Break loop if action is a throw\n        if action['throw_dir'].iloc[0]!=\"none\":\n            break\n        else:\n            continue\n    action_table = action_table.reset_index(drop=True)  \n    \n    #Find best states\n    best_states = Q_table_TD[Q_table_TD['V']!=0].sort_values('V', ascending=False).drop_duplicates(['state_x', 'state_y', 'Q', 'reward','V'])\n    best_states['episode'] = e\n    \n    best_states_table = best_states_table.append(best_states)\nbest_states_table = best_states_table.reset_index(drop=True)\n#Produce Summary output for each episode so we can observe convergence\nstart_state_value = best_states_table[(best_states_table['state_x']==start_x) & (best_states_table['state_y']==start_y)][['episode','V']].sort_values('episode')\n                                         \nstate_values = Q_table_TD.drop_duplicates(['state_x','state_y'])\n    \n    ","d5593a43":"best_states_table.head(10)","46eb92c4":"best_states_table.tail(10)","18c98d5b":"start_state_value.head()","5b9cfccf":"plt.plot(start_state_value['episode'], start_state_value['V'])\nplt.title(\"Value of Start State by Episode\")\nplt.show()","a53636ee":"state_values.head()","61dd6117":"state_values[[\"state_y\", \"state_x\", \"V\"]].pivot(\"state_y\", \"state_x\", \"V\")","c57defa3":"sns.set(rc={'figure.figsize':(11.7,8.27)})\npivot = state_values[[\"state_y\", \"state_x\", \"V\"]].pivot(\"state_y\", \"state_x\", \"V\")\n\nax = sns.heatmap(pivot)\nax.hlines(range(-10,21), *ax.get_xlim())\nax.vlines(range(-10,21), *ax.get_ylim())\n\nplt.title(\"State Values V(s)\")\nax.invert_yaxis()","6905d95e":"sns.set(rc={'figure.figsize':(15,10)})\npivot = state_values[[\"state_y\", \"state_x\", \"V\"]].pivot(\"state_y\", \"state_x\", \"V\")\n\nax = plt.subplot(221)\nax = sns.heatmap(pivot)\nax.hlines(range(-10,21), *ax.get_xlim())\nax.vlines(range(-10,21), *ax.get_ylim())\nax.set_title(\"State Values V(s)\")\nax.invert_yaxis()\n\nax2 = plt.subplot(222)\nax2.plot(start_state_value['episode'], start_state_value['V'])\nax2.set_title(\"Value of Start State by Episode\")\n\nplt.show()\n","aedb1b8f":"# Define start position\nstart_x = -5\nstart_y = -5\naction_cap = 10000\n\nepsilon = 0.1\nalpha = 0.5\ngamma = 0.5\nV_bin = 0\n\nnum_episodes = 1000\n\n# Make a copy of the initalised Q table so we don't override this\nQ_table_TD = Q_table.copy()\n\naction_table = pd.DataFrame()\nbest_states_table = pd.DataFrame()\nfor e in range(0,num_episodes):\n    clear_output(wait=True)\n    print(\"Current Episode: \",  np.round(e\/num_episodes,4) *100,\"%\")\n    action = None\n    for a in range(0,action_cap):\n\n        action = eps_greedy_V(Q_table_TD, epsilon, start_x, start_y, a, action)\n        # If action is to throw, use probability to find whether this was successful or not and update accordingly\n        if action['throw_dir'].iloc[0]!=\"none\":\n            rng_throw = np.random.rand()\n\n            if rng_throw <= action['prob'].iloc[0]:\n                reward = 1\n            else:\n                reward = -1\n            New_V = action['V'].iloc[0] + alpha*(reward + (gamma* V_bin) - action['V'].iloc[0])\n        # If move action, we have guaranteed probability and currently no reward for this\n        else:\n            New_V = action['V'].iloc[0]\n        #Update V value for state based on outcome\n        Q_table_TD['V'] = np.where( ((Q_table_TD['state_x'] == action['state_x'].iloc[0]) & \n                                    (Q_table_TD['state_y'] == action['state_y'].iloc[0])),New_V, Q_table_TD['V'])\n    \n        #Add column to denote which episode this is for\n        action['episode'] = e\n        action_table = action_table.append(action)\n\n        # Break loop if action is a throw\n        if action['throw_dir'].iloc[0]!=\"none\":\n            break\n        else:\n            continue\n    action_table = action_table.reset_index(drop=True)  \n    \n    #Find best states\n    best_states = Q_table_TD[Q_table_TD['V']!=0].sort_values('V', ascending=False).drop_duplicates(['state_x', 'state_y', 'Q', 'reward','V'])\n    best_states['episode'] = e\n    \n    best_states_table = best_states_table.append(best_states)\nbest_states_table = best_states_table.reset_index(drop=True)\n#Produce Summary output for each episode so we can observe convergence\nstart_state_value = best_states_table[(best_states_table['state_x']==start_x) & (best_states_table['state_y']==start_y)][['episode','V']].sort_values('episode')\n                                         \nstate_values = Q_table_TD.drop_duplicates(['state_x','state_y'])\n    \n    \nsns.set(rc={'figure.figsize':(15,10)})\npivot = state_values[[\"state_y\", \"state_x\", \"V\"]].pivot(\"state_y\", \"state_x\", \"V\")\n\nax = plt.subplot(221)\nax = sns.heatmap(pivot)\nax.hlines(range(-10,21), *ax.get_xlim())\nax.vlines(range(-10,21), *ax.get_ylim())\nax.set_title(\"State Values V(s)\")\nax.invert_yaxis()\n\nax2 = plt.subplot(222)\nax2.plot(start_state_value['episode'], start_state_value['V'])\nax2.set_title(\"Value of Start State by Episode\")\n\nplt.show()\n\n\n    ","66d8f824":"# Define start position\nstart_x = -5\nstart_y = -5\naction_cap = 10000\n\nepsilon = 0.1\nalpha = 0.5\ngamma = 0.5\nV_bin = 0\n\nnum_episodes = 1000\n\n# Make a copy of the initalised Q table so we don't override this\nQ_table_TD = Q_table.copy()\n\naction_table = pd.DataFrame()\nbest_states_table = pd.DataFrame()\nfor e in range(0,num_episodes):\n    clear_output(wait=True)\n    print(\"Current Episode: \",  np.round(e\/num_episodes,4) *100,\"%\")\n    action = None\n    for a in range(0,action_cap):\n\n        action = eps_greedy_V(Q_table_TD, epsilon, start_x, start_y, a, action)\n        # If action is to throw, use probability to find whether this was successful or not and update accordingly\n        if action['throw_dir'].iloc[0]!=\"none\":\n            rng_throw = np.random.rand()\n\n            if rng_throw <= action['prob'].iloc[0]:\n                reward = 1\n            else:\n                reward = -1\n            New_V = action['V'].iloc[0] + alpha*(reward + (gamma* V_bin) - action['V'].iloc[0])\n        # If move action, we have guaranteed probability and no introduce a small positive reward\n        else:\n            reward = 0.1\n            New_V = action['V'].iloc[0] + alpha*(reward + (gamma* V_bin) - action['V'].iloc[0])\n        #Update V value for state based on outcome\n        Q_table_TD['V'] = np.where( ((Q_table_TD['state_x'] == action['state_x'].iloc[0]) & \n                                    (Q_table_TD['state_y'] == action['state_y'].iloc[0])),New_V, Q_table_TD['V'])\n    \n        #Add column to denote which episode this is for\n        action['episode'] = e\n        action_table = action_table.append(action)\n\n        # Break loop if action is a throw\n        if action['throw_dir'].iloc[0]!=\"none\":\n            break\n        else:\n            continue\n    action_table = action_table.reset_index(drop=True)  \n    \n    #Find best states\n    best_states = Q_table_TD[Q_table_TD['V']!=0].sort_values('V', ascending=False).drop_duplicates(['state_x', 'state_y', 'Q', 'reward','V'])\n    best_states['episode'] = e\n    \n    best_states_table = best_states_table.append(best_states)\nbest_states_table = best_states_table.reset_index(drop=True)\n#Produce Summary output for each episode so we can observe convergence\nstart_state_value = best_states_table[(best_states_table['state_x']==start_x) & (best_states_table['state_y']==start_y)][['episode','V']].sort_values('episode')\n                                         \nstate_values = Q_table_TD.drop_duplicates(['state_x','state_y'])\n    \n    \nsns.set(rc={'figure.figsize':(15,10)})\npivot = state_values[[\"state_y\", \"state_x\", \"V\"]].pivot(\"state_y\", \"state_x\", \"V\")\n\nax = plt.subplot(221)\nax = sns.heatmap(pivot)\nax.hlines(range(-10,21), *ax.get_xlim())\nax.vlines(range(-10,21), *ax.get_ylim())\nax.set_title(\"State Values V(s)\")\nax.invert_yaxis()\n\nax2 = plt.subplot(222)\nax2.plot(start_state_value['episode'], start_state_value['V'])\nax2.set_title(\"Value of Start State by Episode\")\n\nplt.show()\n\n\n    ","45da0ba0":"# Define start position\nstart_x = -5\nstart_y = -5\nbin_x = 0\nbin_y = 0\naction_cap = 10000\n\nepsilon = 0.1\nalpha = 0.5\ngamma = 0.5\nV_bin = 0\n\nnum_episodes = 1000\n\n# Make a copy of the initalised Q table so we don't override this\nQ_table_Q = Q_table.copy()\n\naction_table = pd.DataFrame()\nbest_actions_table = pd.DataFrame()\nfor e in range(0,num_episodes):\n    clear_output(wait=True)\n    print(\"Current Episode: \",  np.round(e\/num_episodes,4) *100,\"%\")\n    action = None\n    for a in range(0,action_cap):\n\n        action = eps_greedy_Q(Q_table_Q, epsilon, start_x, start_y, a, action)\n        # If action is to throw, use probability to find whether this was successful or not and update accordingly\n        if action['throw_dir'].iloc[0]!=\"none\":\n            rng_throw = np.random.rand()\n\n            if rng_throw <= action['prob'].iloc[0]:\n                reward = 1\n            else:\n                reward = -1\n            bin_Q = Q_table_Q[(Q_table_Q['state_x']==bin_x) & (Q_table_Q['state_y']==bin_y)]\n            bin_max_Q = bin_Q.sort_values('Q', ascending=False).iloc[0]['Q']\n            New_Q = ((1-alpha)*action['Q'].iloc[0]) + alpha*(reward + (gamma*bin_max_Q)) \n        # If move action, we have guaranteed probability and no introduce a small positive reward\n        else:\n            reward = 0.1\n            move_direction = action['move_dir'].iloc[0]\n            #Map this to actual direction and find V(s) for next state\n            if(move_direction == 0):\n                move_x = 0\n                move_y = 1\n            elif(move_direction == 1):\n                move_x = 1\n                move_y = 1\n            elif(move_direction == 2):\n                move_x = 1\n                move_y = 0\n            elif(move_direction == 3):\n                move_x = 1\n                move_y = -1\n            elif(move_direction == 4):\n                move_x = 0\n                move_y = -1\n            elif(move_direction == 5):\n                move_x = -1\n                move_y = -1\n            elif(move_direction == 6):\n                move_x = -1\n                move_y = 0\n            elif(move_direction == 7):\n                move_x = -1\n                move_y = 1\n\n            new_x = action['state_x'].iloc[0]+move_x\n            new_y = action['state_y'].iloc[0]+move_y\n            next_action_Q = Q_table_Q[(Q_table_Q['state_x']==new_x) &  (Q_table_Q['state_y']==new_y)]\n            next_action_max_Q = next_action_Q.sort_values('Q', ascending=False).iloc[0]['Q']\n            \n\n            New_Q = ((1-alpha)*action['Q'].iloc[0]) + alpha*(reward + (gamma*next_action_max_Q)) \n\n        #Update Q(s,a) value for state based on outcome\n        Q_table_Q['Q'] = np.where( ((Q_table_Q['state_x'] == action['state_x'].iloc[0]) & \n                                    (Q_table_Q['state_y'] == action['state_y'].iloc[0]) &\n                                    (Q_table_Q['throw_dir'] == action['throw_dir'].iloc[0]) &\n                                    (Q_table_Q['move_dir'] == action['move_dir'].iloc[0])\n                                    ),New_Q, Q_table_Q['Q'])\n    \n        #Add column to denote which episode this is for\n        action['episode'] = e\n        action_table = action_table.append(action)\n\n        # Break loop if action is a throw\n        if action['throw_dir'].iloc[0]!=\"none\":\n            break\n        else:\n            continue\n    action_table = action_table.reset_index(drop=True)  \n    \n    #Find best states\n    best_actions = Q_table_Q[Q_table_Q['Q']!=0].sort_values('Q', ascending=False).drop_duplicates(['state_x', 'state_y'])\n    best_actions['episode'] = e\n    \n    best_actions_table = best_actions_table.append(best_actions)\nbest_actions_table = best_actions_table.reset_index(drop=True)\n#Produce Summary output for each episode so we can observe convergence\nstart_state_action_values = best_actions_table[(best_actions_table['state_x']==start_x) & (best_actions_table['state_y']==start_y)][['episode','Q']].sort_values('episode')\n                                         \nstate_action_values = Q_table_Q.sort_values('Q',ascending=False).drop_duplicates(['state_x','state_y'])\n    ","96e58d23":"sns.set(rc={'figure.figsize':(15,10)})\npivot = state_action_values[[\"state_y\", \"state_x\", \"Q\"]].pivot(\"state_y\", \"state_x\", \"Q\")\n\nax = plt.subplot(221)\nax = sns.heatmap(pivot)\nax.hlines(range(-10,21), *ax.get_xlim())\nax.vlines(range(-10,21), *ax.get_ylim())\nax.set_title(\"Best State-Action Values for each State\")\nax.invert_yaxis()\n\nax2 = plt.subplot(222)\nax2.plot(start_state_action_values['episode'], start_state_action_values['Q'], label = \"Start State\")\nax2.plot(best_actions_table[['episode','Q']].groupby('episode').mean().reset_index()['episode'],\n         best_actions_table[['episode','Q']].groupby('episode').mean().reset_index()['Q'],label='All States')\nax2.legend()\nax2.set_title(\"Value of State's Best Action by Episode\")\n\nplt.show()\n","2cee94e3":"optimal_action_start_state = best_actions_table[(best_actions_table['state_x']==start_x)&(best_actions_table['state_y']==start_y)].sort_values('episode',ascending=False)\nif (optimal_action_start_state['throw_dir'].iloc[0]==\"none\"):\n    print(\"The optimal action from the start state is to MOVE in direction: \", optimal_action_start_state['move_dir'].iloc[0])\nelse:\n    print(\"The optimal action from the start state is to THROW in direction: \", optimal_action_start_state['throw_dir'].iloc[0])\n","057fdb83":"# Create Quiver plot showing current optimal policy in one cell\narrow_scale = 0.1\n\noptimal_action_list = state_action_values\n\noptimal_action_list['Action'] = np.where( optimal_action_list['move_dir'] == 'none', 'THROW', 'MOVE'  )\n\n\noptimal_action_list['move_x'] = np.where(optimal_action_list['move_dir'] == 0, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 1, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 2, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 3, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 4, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 5, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 6, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 7, int(-1),\n                                         int(-1000)\n                                        ))))))))\noptimal_action_list['move_y'] = np.where(optimal_action_list['move_dir'] == 0, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 1, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 2, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 3, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 4, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 5, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 6, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 7, int(1),\n                                         int(-1000)\n                                        ))))))))\noptimal_action_list['throw_dir_2'] = np.where(optimal_action_list['throw_dir']==\"none\",int(-1000), optimal_action_list['throw_dir'])\n\n# Define horizontal arrow component as 0.1*move direction or 0.1\/-0.1 depending on throw direction\noptimal_action_list['u'] = np.where(optimal_action_list['Action']==\"MOVE\", optimal_action_list['move_x']*arrow_scale,\n                                    np.where(optimal_action_list['throw_dir_2']==0, 0,np.where(optimal_action_list['throw_dir_2']==180, 0,\n                                    np.where(optimal_action_list['throw_dir_2']==90, arrow_scale ,np.where(optimal_action_list['throw_dir_2']==270, -arrow_scale,\n                                    np.where(optimal_action_list['throw_dir_2']<180, arrow_scale,-arrow_scale))))))\n                                             \n# Define vertical arrow component based 0.1*move direciton or +\/- u*tan(throw_dir) accordingly\noptimal_action_list['v'] = np.where(optimal_action_list['Action']==\"MOVE\", optimal_action_list['move_y']*arrow_scale, \n                                    np.where(optimal_action_list['throw_dir_2']==0, arrow_scale,np.where(optimal_action_list['throw_dir_2']==180, -arrow_scale,\n                                    np.where(optimal_action_list['throw_dir_2']==90, 0,np.where(optimal_action_list['throw_dir_2']==270, 0,\n                                    optimal_action_list['u']\/np.tan(np.deg2rad(optimal_action_list['throw_dir_2'].astype(np.float64))))))))\n                                             \nx = optimal_action_list['state_x']\ny = optimal_action_list['state_y']\nu = optimal_action_list['u'].values\nv = optimal_action_list['v'].values\n\nplt.figure(figsize=(10, 10))\nplt.quiver(x,y,u,v,scale=0.5,scale_units='inches')\nsns.scatterplot( x=\"state_x\", y=\"state_y\", data=optimal_action_list,  hue='Action')\nplt.title(\"Optimal Policy for Given Probabilities\")\nplt.show()","c079e684":"#Change variable name to base for output graphs\nbase_x = -5\nbase_y = -5\n\nbin_x = 0\nbin_y = 0\naction_cap = 10000\n\nepsilon = 0.1\nalpha = 0.5\ngamma = 0.5\nV_bin = 0\n\nnum_episodes = 1000\n\n# Make a copy of the initalised Q table so we don't override this\nQ_table_Q = Q_table.copy()\n\naction_table = pd.DataFrame()\nbest_actions_table = pd.DataFrame()\nfor e in range(0,num_episodes):\n    clear_output(wait=True)\n    print(\"Current Episode: \",  np.round(e\/num_episodes,4) *100,\"%\")\n    \n    # Randomly select start position between: -4,-5,-6 (note the upper bound is soft, i.e. <-3 = <=-4)\n    start_x = np.random.randint(-10,11)\n    start_y = np.random.randint(-10,11)\n    \n    \n    action = None\n    for a in range(0,action_cap):\n\n        action = eps_greedy_Q(Q_table_Q, epsilon, start_x, start_y, a, action)\n        # If action is to throw, use probability to find whether this was successful or not and update accordingly\n        if action['throw_dir'].iloc[0]!=\"none\":\n            rng_throw = np.random.rand()\n\n            if rng_throw <= action['prob'].iloc[0]:\n                reward = 1\n            else:\n                reward = -1\n            bin_Q = Q_table_Q[(Q_table_Q['state_x']==bin_x) & (Q_table_Q['state_y']==bin_y)]\n            bin_max_Q = bin_Q.sort_values('Q', ascending=False).iloc[0]['Q']\n            New_Q = ((1-alpha)*action['Q'].iloc[0]) + alpha*(reward + (gamma*bin_max_Q)) \n        # If move action, we have guaranteed probability and no introduce a small positive reward\n        else:\n            reward = 0.1\n            move_direction = action['move_dir'].iloc[0]\n            #Map this to actual direction and find V(s) for next state\n            if(move_direction == 0):\n                move_x = 0\n                move_y = 1\n            elif(move_direction == 1):\n                move_x = 1\n                move_y = 1\n            elif(move_direction == 2):\n                move_x = 1\n                move_y = 0\n            elif(move_direction == 3):\n                move_x = 1\n                move_y = -1\n            elif(move_direction == 4):\n                move_x = 0\n                move_y = -1\n            elif(move_direction == 5):\n                move_x = -1\n                move_y = -1\n            elif(move_direction == 6):\n                move_x = -1\n                move_y = 0\n            elif(move_direction == 7):\n                move_x = -1\n                move_y = 1\n\n            new_x = action['state_x'].iloc[0]+move_x\n            new_y = action['state_y'].iloc[0]+move_y\n            next_action_Q = Q_table_Q[(Q_table_Q['state_x']==new_x) &  (Q_table_Q['state_y']==new_y)]\n            next_action_max_Q = next_action_Q.sort_values('Q', ascending=False).iloc[0]['Q']\n            \n\n            New_Q = ((1-alpha)*action['Q'].iloc[0]) + alpha*(reward + (gamma*next_action_max_Q)) \n\n        #Update Q(s,a) value for state based on outcome\n        Q_table_Q['Q'] = np.where( ((Q_table_Q['state_x'] == action['state_x'].iloc[0]) & \n                                    (Q_table_Q['state_y'] == action['state_y'].iloc[0]) &\n                                    (Q_table_Q['throw_dir'] == action['throw_dir'].iloc[0]) &\n                                    (Q_table_Q['move_dir'] == action['move_dir'].iloc[0])\n                                    ),New_Q, Q_table_Q['Q'])\n    \n        #Add column to denote which episode this is for\n        action['episode'] = e\n        action_table = action_table.append(action)\n\n        # Break loop if action is a throw\n        if action['throw_dir'].iloc[0]!=\"none\":\n            break\n        else:\n            continue\n    action_table = action_table.reset_index(drop=True)  \n    \n    #Find best states\n    best_actions = Q_table_Q[Q_table_Q['Q']!=0].sort_values('Q', ascending=False).drop_duplicates(['state_x', 'state_y'])\n    best_actions['episode'] = e\n    \n    best_actions_table = best_actions_table.append(best_actions)\nbest_actions_table = best_actions_table.reset_index(drop=True)\n#Produce Summary output for each episode so we can observe convergence\nstart_state_action_values = best_actions_table[(best_actions_table['state_x']==base_x) & (best_actions_table['state_y']==base_y)][['episode','Q']].sort_values('episode')\n                                         \nstate_action_values = Q_table_Q.sort_values('Q',ascending=False).drop_duplicates(['state_x','state_y'])\n    \n    \nsns.set(rc={'figure.figsize':(15,10)})\npivot = state_action_values[[\"state_y\", \"state_x\", \"Q\"]].pivot(\"state_y\", \"state_x\", \"Q\")\n\nax = plt.subplot(221)\nax = sns.heatmap(pivot)\nax.hlines(range(-10,21), *ax.get_xlim())\nax.vlines(range(-10,21), *ax.get_ylim())\nax.set_title(\"Best State-Action Values for each State\")\nax.invert_yaxis()\n\nax2 = plt.subplot(222)\nax2.plot(start_state_action_values['episode'], start_state_action_values['Q'], label = \"Base State\")\nax2.plot(best_actions_table[['episode','Q']].groupby('episode').mean().reset_index()['episode'],\n         best_actions_table[['episode','Q']].groupby('episode').mean().reset_index()['Q'],label='All States')\nax2.legend()\nax2.set_title(\"Value of State's Best Action by Episode\")\n\n\nplt.show()","8ef2ce9e":"# Create Quiver plot showing current optimal policy in one cell\narrow_scale = 0.1\n\noptimal_action_list = state_action_values\n\noptimal_action_list['Action'] = np.where( optimal_action_list['move_dir'] == 'none', 'THROW', 'MOVE'  )\n\n\noptimal_action_list['move_x'] = np.where(optimal_action_list['move_dir'] == 0, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 1, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 2, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 3, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 4, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 5, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 6, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 7, int(-1),\n                                         int(-1000)\n                                        ))))))))\noptimal_action_list['move_y'] = np.where(optimal_action_list['move_dir'] == 0, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 1, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 2, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 3, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 4, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 5, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 6, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 7, int(1),\n                                         int(-1000)\n                                        ))))))))\noptimal_action_list['throw_dir_2'] = np.where(optimal_action_list['throw_dir']==\"none\",int(-1000), optimal_action_list['throw_dir'])\n\n# Define horizontal arrow component as 0.1*move direction or 0.1\/-0.1 depending on throw direction\noptimal_action_list['u'] = np.where(optimal_action_list['Action']==\"MOVE\", optimal_action_list['move_x']*arrow_scale,\n                                    np.where(optimal_action_list['throw_dir_2']==0, 0,np.where(optimal_action_list['throw_dir_2']==180, 0,\n                                    np.where(optimal_action_list['throw_dir_2']==90, arrow_scale ,np.where(optimal_action_list['throw_dir_2']==270, -arrow_scale,\n                                    np.where(optimal_action_list['throw_dir_2']<180, arrow_scale,-arrow_scale))))))\n                                             \n# Define vertical arrow component based 0.1*move direciton or +\/- u*tan(throw_dir) accordingly\noptimal_action_list['v'] = np.where(optimal_action_list['Action']==\"MOVE\", optimal_action_list['move_y']*arrow_scale, \n                                    np.where(optimal_action_list['throw_dir_2']==0, arrow_scale,np.where(optimal_action_list['throw_dir_2']==180, -arrow_scale,\n                                    np.where(optimal_action_list['throw_dir_2']==90, 0,np.where(optimal_action_list['throw_dir_2']==270, 0,\n                                    optimal_action_list['u']\/np.tan(np.deg2rad(optimal_action_list['throw_dir_2'].astype(np.float64))))))))\n                                             \nx = optimal_action_list['state_x']\ny = optimal_action_list['state_y']\nu = optimal_action_list['u'].values\nv = optimal_action_list['v'].values\n\nplt.figure(figsize=(10, 10))\nplt.quiver(x,y,u,v,scale=0.5,scale_units='inches')\nsns.scatterplot( x=\"state_x\", y=\"state_y\", data=optimal_action_list,  hue='Action')\nplt.title(\"Optimal Policy for Given Probabilities\")\nplt.show()","a1d4151f":"#Change variable name to base for output graphs\nbase_x = -5\nbase_y = -5\n\nbin_x = 0\nbin_y = 0\naction_cap = 10000\n\nepsilon = 0.1\nalpha = 0.9\ngamma = 0.5\nV_bin = 0\n\nnum_episodes = 100\n\n# Make a copy of the initalised Q table so we don't override this\nQ_table_Q = Q_table.copy()\n\naction_table = pd.DataFrame()\nbest_actions_table = pd.DataFrame()\nfor e in range(0,num_episodes):\n    clear_output(wait=True)\n    print(\"Current Episode: \",  np.round(e\/num_episodes,4) *100,\"%\")\n    \n    # Randomly select start position between: -4,-5,-6 (note the upper bound is soft, i.e. <-3 = <=-4)\n    start_x = -5\n    start_y = -5\n    \n    \n    action = None\n    for a in range(0,action_cap):\n\n        action = eps_greedy_Q(Q_table_Q, epsilon, start_x, start_y, a, action)\n        # If action is to throw, use probability to find whether this was successful or not and update accordingly\n        if action['throw_dir'].iloc[0]!=\"none\":\n            rng_throw = np.random.rand()\n\n            if rng_throw <= action['prob'].iloc[0]:\n                reward = 1\n            else:\n                reward = -1\n            bin_Q = Q_table_Q[(Q_table_Q['state_x']==bin_x) & (Q_table_Q['state_y']==bin_y)]\n            bin_max_Q = bin_Q.sort_values('Q', ascending=False).iloc[0]['Q']\n            New_Q = ((1-alpha)*action['Q'].iloc[0]) + alpha*(reward + (gamma*bin_max_Q)) \n        # If move action, we have guaranteed probability and no introduce a small positive reward\n        else:\n            reward = 0.1\n            move_direction = action['move_dir'].iloc[0]\n            #Map this to actual direction and find V(s) for next state\n            if(move_direction == 0):\n                move_x = 0\n                move_y = 1\n            elif(move_direction == 1):\n                move_x = 1\n                move_y = 1\n            elif(move_direction == 2):\n                move_x = 1\n                move_y = 0\n            elif(move_direction == 3):\n                move_x = 1\n                move_y = -1\n            elif(move_direction == 4):\n                move_x = 0\n                move_y = -1\n            elif(move_direction == 5):\n                move_x = -1\n                move_y = -1\n            elif(move_direction == 6):\n                move_x = -1\n                move_y = 0\n            elif(move_direction == 7):\n                move_x = -1\n                move_y = 1\n\n            new_x = action['state_x'].iloc[0]+move_x\n            new_y = action['state_y'].iloc[0]+move_y\n            next_action_Q = Q_table_Q[(Q_table_Q['state_x']==new_x) &  (Q_table_Q['state_y']==new_y)]\n            next_action_max_Q = next_action_Q.sort_values('Q', ascending=False).iloc[0]['Q']\n            \n\n            New_Q = ((1-alpha)*action['Q'].iloc[0]) + alpha*(reward + (gamma*next_action_max_Q)) \n\n        #Update Q(s,a) value for state based on outcome\n        Q_table_Q['Q'] = np.where( ((Q_table_Q['state_x'] == action['state_x'].iloc[0]) & \n                                    (Q_table_Q['state_y'] == action['state_y'].iloc[0]) &\n                                    (Q_table_Q['throw_dir'] == action['throw_dir'].iloc[0]) &\n                                    (Q_table_Q['move_dir'] == action['move_dir'].iloc[0])\n                                    ),New_Q, Q_table_Q['Q'])\n    \n        #Add column to denote which episode this is for\n        action['episode'] = e\n        action_table = action_table.append(action)\n\n        # Break loop if action is a throw\n        if action['throw_dir'].iloc[0]!=\"none\":\n            break\n        else:\n            continue\n    action_table = action_table.reset_index(drop=True)  \n    \n    #Find best states\n    best_actions = Q_table_Q[Q_table_Q['Q']!=0].sort_values('Q', ascending=False).drop_duplicates(['state_x', 'state_y'])\n    best_actions['episode'] = e\n    \n    best_actions_table = best_actions_table.append(best_actions)\nbest_actions_table = best_actions_table.reset_index(drop=True)\n#Produce Summary output for each episode so we can observe convergence\nstart_state_action_values = best_actions_table[(best_actions_table['state_x']==base_x) & (best_actions_table['state_y']==base_y)][['episode','Q']].sort_values('episode')\n                                         \nstate_action_values = Q_table_Q.sort_values('Q',ascending=False).drop_duplicates(['state_x','state_y'])\n    \n    \nsns.set(rc={'figure.figsize':(15,10)})\npivot = state_action_values[[\"state_y\", \"state_x\", \"Q\"]].pivot(\"state_y\", \"state_x\", \"Q\")\n\nax = plt.subplot(221)\nax = sns.heatmap(pivot)\nax.hlines(range(-10,21), *ax.get_xlim())\nax.vlines(range(-10,21), *ax.get_ylim())\nax.set_title(\"Best State-Action Values for each State\")\nax.invert_yaxis()\n\nax2 = plt.subplot(222)\nax2.plot(start_state_action_values['episode'], start_state_action_values['Q'], label = \"Base State\")\nax2.plot(best_actions_table[best_actions_table['Q']!=0][['episode','Q']].groupby('episode').mean().reset_index()['episode'],\n         best_actions_table[best_actions_table['Q']!=0][['episode','Q']].groupby('episode').mean().reset_index()['Q'],label='All States')\nax2.legend()\nax2.set_title(\"Value of State's Best Action by Episode, alpha = 0.9\")\n\nplt.show()","eaa6abd0":"#Change variable name to base for output graphs\nbase_x = -5\nbase_y = -5\n\nbin_x = 0\nbin_y = 0\naction_cap = 10000\n\nepsilon = 0.1\nalpha = 0.1\ngamma = 0.5\nV_bin = 0\n\nnum_episodes = 100\n\n# Make a copy of the initalised Q table so we don't override this\nQ_table_Q = Q_table.copy()\n\naction_table = pd.DataFrame()\nbest_actions_table = pd.DataFrame()\nfor e in range(0,num_episodes):\n    clear_output(wait=True)\n    print(\"Current Episode: \",  np.round(e\/num_episodes,4) *100,\"%\")\n    \n    # Randomly select start position between: -4,-5,-6 (note the upper bound is soft, i.e. <-3 = <=-4)\n    start_x = -5\n    start_y = -5\n    \n    \n    action = None\n    for a in range(0,action_cap):\n\n        action = eps_greedy_Q(Q_table_Q, epsilon, start_x, start_y, a, action)\n        # If action is to throw, use probability to find whether this was successful or not and update accordingly\n        if action['throw_dir'].iloc[0]!=\"none\":\n            rng_throw = np.random.rand()\n\n            if rng_throw <= action['prob'].iloc[0]:\n                reward = 1\n            else:\n                reward = -1\n            bin_Q = Q_table_Q[(Q_table_Q['state_x']==bin_x) & (Q_table_Q['state_y']==bin_y)]\n            bin_max_Q = bin_Q.sort_values('Q', ascending=False).iloc[0]['Q']\n            New_Q = ((1-alpha)*action['Q'].iloc[0]) + alpha*(reward + (gamma*bin_max_Q)) \n        # If move action, we have guaranteed probability and no introduce a small positive reward\n        else:\n            reward = 0.1\n            move_direction = action['move_dir'].iloc[0]\n            #Map this to actual direction and find V(s) for next state\n            if(move_direction == 0):\n                move_x = 0\n                move_y = 1\n            elif(move_direction == 1):\n                move_x = 1\n                move_y = 1\n            elif(move_direction == 2):\n                move_x = 1\n                move_y = 0\n            elif(move_direction == 3):\n                move_x = 1\n                move_y = -1\n            elif(move_direction == 4):\n                move_x = 0\n                move_y = -1\n            elif(move_direction == 5):\n                move_x = -1\n                move_y = -1\n            elif(move_direction == 6):\n                move_x = -1\n                move_y = 0\n            elif(move_direction == 7):\n                move_x = -1\n                move_y = 1\n\n            new_x = action['state_x'].iloc[0]+move_x\n            new_y = action['state_y'].iloc[0]+move_y\n            next_action_Q = Q_table_Q[(Q_table_Q['state_x']==new_x) &  (Q_table_Q['state_y']==new_y)]\n            next_action_max_Q = next_action_Q.sort_values('Q', ascending=False).iloc[0]['Q']\n            \n\n            New_Q = ((1-alpha)*action['Q'].iloc[0]) + alpha*(reward + (gamma*next_action_max_Q)) \n\n        #Update Q(s,a) value for state based on outcome\n        Q_table_Q['Q'] = np.where( ((Q_table_Q['state_x'] == action['state_x'].iloc[0]) & \n                                    (Q_table_Q['state_y'] == action['state_y'].iloc[0]) &\n                                    (Q_table_Q['throw_dir'] == action['throw_dir'].iloc[0]) &\n                                    (Q_table_Q['move_dir'] == action['move_dir'].iloc[0])\n                                    ),New_Q, Q_table_Q['Q'])\n    \n        #Add column to denote which episode this is for\n        action['episode'] = e\n        action_table = action_table.append(action)\n\n        # Break loop if action is a throw\n        if action['throw_dir'].iloc[0]!=\"none\":\n            break\n        else:\n            continue\n    action_table = action_table.reset_index(drop=True)  \n    \n    #Find best states\n    best_actions = Q_table_Q[Q_table_Q['Q']!=0].sort_values('Q', ascending=False).drop_duplicates(['state_x', 'state_y'])\n    best_actions['episode'] = e\n    \n    best_actions_table = best_actions_table.append(best_actions)\nbest_actions_table = best_actions_table.reset_index(drop=True)\n#Produce Summary output for each episode so we can observe convergence\nstart_state_action_values = best_actions_table[(best_actions_table['state_x']==base_x) & (best_actions_table['state_y']==base_y)][['episode','Q']].sort_values('episode')\n                                         \nstate_action_values = Q_table_Q.sort_values('Q',ascending=False).drop_duplicates(['state_x','state_y'])\n    \n    \nsns.set(rc={'figure.figsize':(15,10)})\npivot = state_action_values[[\"state_y\", \"state_x\", \"Q\"]].pivot(\"state_y\", \"state_x\", \"Q\")\n\nax = plt.subplot(221)\nax = sns.heatmap(pivot)\nax.hlines(range(-10,21), *ax.get_xlim())\nax.vlines(range(-10,21), *ax.get_ylim())\nax.set_title(\"Best State-Action Values for each State\")\nax.invert_yaxis()\n\nax2 = plt.subplot(222)\nax2.plot(start_state_action_values['episode'], start_state_action_values['Q'], label = \"Base State\")\nax2.plot(best_actions_table[best_actions_table['Q']!=0][['episode','Q']].groupby('episode').mean().reset_index()['episode'],\n         best_actions_table[best_actions_table['Q']!=0][['episode','Q']].groupby('episode').mean().reset_index()['Q'],label='All States')\nax2.legend()\nax2.set_title(\"Value of State's Best Action by Episode, alpha = 0.1\")\n\nplt.show()","66ecae9d":"#Change variable name to base for output graphs\nbase_x = -5\nbase_y = -5\n\nbin_x = 0\nbin_y = 0\naction_cap = 10000\n\nepsilon = 0.1\ngamma = 0.5\nV_bin = 0\n\nnum_episodes = 100\n\n# Make a copy of the initalised Q table so we don't override this\nQ_table_Q = Q_table.copy()\n\n\nbest_actions_table = pd.DataFrame()\nfor alp in range(1,11):\n    alpha = alp\/10\n\n    action_table = pd.DataFrame()\n    for e in range(0,num_episodes):\n        clear_output(wait=True)\n        print(\"Current alpha: \", alpha)\n        print(\"Current Episode: \",  np.round(e\/num_episodes,4) *100,\"%\")\n\n        # Randomly select start position between: -4,-5,-6 (note the upper bound is soft, i.e. <-3 = <=-4)\n        start_x = -5\n        start_y = -5\n\n\n        action = None\n        for a in range(0,action_cap):\n\n            action = eps_greedy_Q(Q_table_Q, epsilon, start_x, start_y, a, action)\n            # If action is to throw, use probability to find whether this was successful or not and update accordingly\n            if action['throw_dir'].iloc[0]!=\"none\":\n                rng_throw = np.random.rand()\n\n                if rng_throw <= action['prob'].iloc[0]:\n                    reward = 1\n                else:\n                    reward = -1\n                bin_Q = Q_table_Q[(Q_table_Q['state_x']==bin_x) & (Q_table_Q['state_y']==bin_y)]\n                bin_max_Q = bin_Q.sort_values('Q', ascending=False).iloc[0]['Q']\n                New_Q = ((1-alpha)*action['Q'].iloc[0]) + alpha*(reward + (gamma*bin_max_Q)) \n            # If move action, we have guaranteed probability and no introduce a small positive reward\n            else:\n                reward = 0.1\n                move_direction = action['move_dir'].iloc[0]\n                #Map this to actual direction and find V(s) for next state\n                if(move_direction == 0):\n                    move_x = 0\n                    move_y = 1\n                elif(move_direction == 1):\n                    move_x = 1\n                    move_y = 1\n                elif(move_direction == 2):\n                    move_x = 1\n                    move_y = 0\n                elif(move_direction == 3):\n                    move_x = 1\n                    move_y = -1\n                elif(move_direction == 4):\n                    move_x = 0\n                    move_y = -1\n                elif(move_direction == 5):\n                    move_x = -1\n                    move_y = -1\n                elif(move_direction == 6):\n                    move_x = -1\n                    move_y = 0\n                elif(move_direction == 7):\n                    move_x = -1\n                    move_y = 1\n\n                new_x = action['state_x'].iloc[0]+move_x\n                new_y = action['state_y'].iloc[0]+move_y\n                next_action_Q = Q_table_Q[(Q_table_Q['state_x']==new_x) &  (Q_table_Q['state_y']==new_y)]\n                next_action_max_Q = next_action_Q.sort_values('Q', ascending=False).iloc[0]['Q']\n\n\n                New_Q = ((1-alpha)*action['Q'].iloc[0]) + alpha*(reward + (gamma*next_action_max_Q)) \n\n            #Update Q(s,a) value for state based on outcome\n            Q_table_Q['Q'] = np.where( ((Q_table_Q['state_x'] == action['state_x'].iloc[0]) & \n                                        (Q_table_Q['state_y'] == action['state_y'].iloc[0]) &\n                                        (Q_table_Q['throw_dir'] == action['throw_dir'].iloc[0]) &\n                                        (Q_table_Q['move_dir'] == action['move_dir'].iloc[0])\n                                        ),New_Q, Q_table_Q['Q'])\n\n            #Add column to denote which episode this is for\n            action['episode'] = e\n            action_table = action_table.append(action)\n\n            # Break loop if action is a throw\n            if action['throw_dir'].iloc[0]!=\"none\":\n                break\n            else:\n                continue\n        action_table = action_table.reset_index(drop=True)  \n\n        #Find best states\n        best_actions = Q_table_Q[Q_table_Q['Q']!=0].sort_values('Q', ascending=False).drop_duplicates(['state_x', 'state_y'])\n        best_actions['episode'] = e\n        best_actions['alpha'] = alpha\n\n        best_actions_table = best_actions_table.append(best_actions)\n    best_actions_table = best_actions_table.reset_index(drop=True)\n    #Produce Summary output for each episode so we can observe convergence\n    start_state_action_values = best_actions_table[(best_actions_table['state_x']==base_x) & (best_actions_table['state_y']==base_y)][['episode','Q']].sort_values('episode')\n\n    state_action_values = Q_table_Q.sort_values('Q',ascending=False).drop_duplicates(['state_x','state_y'])\n\n","b1d54193":"best_actions_table.head(10)","52413377":"actions_plot_data = best_actions_table[best_actions_table['Q']!=0][['episode','alpha','Q']].groupby(['alpha','episode']).mean().reset_index()\nactions_plot_data.head(10)","851fd3af":"import plotly\nfrom plotly.offline import download_plotlyjs, init_notebook_mode,plot,iplot\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\n\n\ndef InterAnim(Param_col, x_col, y_col, marker_size=None, plot_title=None, title_size=None, x_label=None,\n              y_label=None, param_label=None, plot_type=None, marker_col=None, marker_alpha=None, fig_size_auto=None, fig_width=None, fig_height=None, vert_grid=None, horiz_grid=None ):\n\n    # Need format: param(slider)\/repeats(x)\/output(y)\n    ourData = pd.DataFrame()\n    if (Param_col is None)|(x_col is None)|(y_col is None):\n        print(\"Please provide data inputs: Parameter column, x column and y column.\")\n    else:\n        ourData['year'] = Param_col\n        ourData['lifeExp'] = x_col\n        ourData['gdpPercap'] = y_col\n    \n    ourData['continent'] = ''\n    ourData['country'] = ''\n    \n    \n    # SET DEFAULT PARAMETERS\n    if (marker_size is None):\n        marker_size = 1\n    else:\n        marker_size = marker_size\n    ourData['pop'] = 50000*marker_size\n\n    # Find parameter intervals\n    alpha = list(set(ourData['year']))\n    alpha = np.round(alpha,1)\n    alpha = np.sort(alpha)[::-1]\n    years = np.round([(alpha) for alpha in alpha],1)\n\n    \n    if (plot_title is None):\n        plot_title = \"\"\n    else:\n        plot_title = plot_title\n    \n    if (title_size is None):\n        title_size = 24\n    else:\n        title_size = title_size\n        \n    if (x_label is None):\n        x_label = \"\"\n    else:\n        x_label = x_label\n        \n    if (y_label is None):\n        y_label = \"\"\n    else:\n        y_label = y_label\n\n    if (param_label is None):\n        param_label = \"\"\n    else:\n        param_label = param_label\n\n    if (plot_type is None):\n        plot_type = \"markers\"\n    else:\n        plot_type = plot_type    \n        \n    if (marker_col is None):\n        marker_col = \"rgb(66, 134, 244)\"\n    else:\n        marker_col = marker_col    \n    \n    if (marker_alpha is None):\n        marker_alpha = 0.8\n    else:\n        marker_alpha = marker_alpha \n        \n    if (fig_size_auto is None):\n        fig_size_auto = True\n    else:\n        fig_size_auto = fig_size_auto\n        \n    if (fig_size_auto  is  False) & (fig_width is None):\n        fig_width = 1500\n    else:\n        fig_width = fig_width\n        \n    if (fig_size_auto  is  False) & (fig_height is None):\n        fig_height = 1500\n    else:\n        fig_height = fig_height\n        \n    if (vert_grid is None):\n        vert_grid = True\n    else:\n        vert_grid = vert_grid\n    \n    if (horiz_grid is None):\n        horiz_grid = True\n    else:\n        horiz_grid = horiz_grid\n        \n        \n    ## Apply Method for creating animation\n    dataset = ourData\n    continents = []\n    for continent in dataset['continent']:\n        if continent not in continents:\n            continents.append(continent)\n    # make figure\n    figure = {\n        'data': [],\n        'layout': {},\n        'frames': []\n    }\n    # fill in most of layout\n    figure['layout']['title'] = {'text': plot_title, 'font':{'size':title_size}}\n    figure['layout']['xaxis'] = {'range': [ min(dataset['lifeExp']) - (min(dataset['lifeExp'])\/10),\n                                            max(dataset['lifeExp']) + (max(dataset['lifeExp'])\/10)  ], 'title': x_label, 'showgrid':vert_grid }\n    figure['layout']['yaxis'] = {'range': [ min(dataset['gdpPercap']) - (min(dataset['gdpPercap'])\/10),\n                                            max(dataset['gdpPercap']) + (max(dataset['gdpPercap'])\/10) ],'title': y_label, 'type': 'linear', 'showgrid':horiz_grid}\n    figure['layout']['hovermode'] = 'closest'\n    \n    figure['layout']['autosize'] = fig_size_auto\n    figure['layout']['width'] = fig_width\n    figure['layout']['height'] = fig_height\n    \n    figure['layout']['sliders'] = {\n        'args': [\n            'transition', {\n                'duration': 900,\n                'easing': 'cubic-in-out'\n            }\n        ],\n        'initialValue': '1952',\n        'plotlycommand': 'animate',\n        'values': years,\n        'visible': True\n    }\n    figure['layout']['updatemenus'] = [\n        {\n            'buttons': [\n                {\n                    'args': [None, {'frame': {'duration': 500, 'redraw': False},\n                             'fromcurrent': True, 'transition': {'duration': 300, 'easing': 'quadratic-in-out'}}],\n                    'label': 'Play',\n                    'method': 'animate'\n                },\n                {\n                    'args': [[None], {'frame': {'duration': 0, 'redraw': False}, 'mode': 'immediate',\n                    'transition': {'duration': 0}}],\n                    'label': 'Pause',\n                    'method': 'animate'\n                }\n            ],\n            'direction': 'left',\n            'pad': {'r': 10, 't': 87},\n            'showactive': False,\n            'type': 'buttons',\n            'x': 0.1,\n            'xanchor': 'right',\n            'y': 0,\n            'yanchor': 'top'\n        }\n    ]\n\n\n    sliders_dict = {\n        'active': 0,\n        'yanchor': 'top',\n        'xanchor': 'left',\n        'currentvalue': {\n            'font': {'size': 20},\n            'prefix': param_label,\n            'visible': True,\n            'xanchor': 'right'\n        },\n        'transition': {'duration': 300, 'easing': 'cubic-in-out'},\n        'pad': {'b': 10, 't': 50},\n        'len': 0.9,\n        'x': 0.1,\n        'y': 0,\n        'steps': []\n    }\n    # make data\n    year = years[0]\n    for continent in continents:\n        dataset_by_year = dataset[np.round(dataset['year'],1) == np.round(year,1)]\n        dataset_by_year_and_cont = dataset_by_year[dataset_by_year['continent'] == continent]\n        data_dict = {\n            'x': list(dataset_by_year_and_cont['lifeExp']),\n            'y': list(dataset_by_year_and_cont['gdpPercap']),\n            'mode': plot_type,\n            'text': list(dataset_by_year_and_cont['country']),\n            'marker': {\n                'sizemode': 'area',\n                'sizeref': 100,\n                'size': list(dataset_by_year_and_cont['pop'])\n            },\n            'name': continent\n        }\n        figure['data'].append(data_dict)\n    # make frames\n    for year in years:\n        frame = {'data': [], 'name': str(year)}\n        for continent in continents:\n            dataset_by_year = dataset[np.round(dataset['year'],1) == np.round(year,1)]\n            dataset_by_year_and_cont = dataset_by_year[dataset_by_year['continent'] == continent]\n            data_dict = {\n                'x': list(dataset_by_year_and_cont['lifeExp']),\n                'y': list(dataset_by_year_and_cont['gdpPercap']),\n                'mode': plot_type,\n                'text': list(dataset_by_year_and_cont['country']),\n                'opacity':marker_alpha,\n                'marker': {\n                    'sizemode': 'area',\n                    'sizeref': 100,\n                    'size': list(dataset_by_year_and_cont['pop']),\n                    'color':marker_col,\n                },\n                'name': continent\n            }\n            frame['data'].append(data_dict)\n        figure['frames'].append(frame)\n        slider_step = {'args': [\n            [year],\n            {'frame': {'duration': 700, 'redraw': False},\n             'mode': 'immediate',\n           'transition': {'duration': 700}}\n         ],\n         'label': year,\n         'method': 'animate'}\n        sliders_dict['steps'].append(slider_step)\n\n    figure['layout']['sliders'] = [sliders_dict]\n    \n    return(figure)\n","4a5cfa1d":"Param_col = actions_plot_data['alpha']\nx_col = actions_plot_data['episode']\ny_col = actions_plot_data['Q']\n\nmarker_size = 0.7\n\nplot_title = \"Interactive-Animation Parameter Optimisation of RL Convergence - Varying Alpha\"\ntitle_size = 28\nx_label = \"Episode\"\ny_label = \"Mean Q for all non-zero states\"\nparam_label = \"Alpha = \"\n\n#plot_type = 'markers', 'lines+markers' or 'lines'\nplot_type = 'markers'\n\n# color could also be hex code\nmarker_col = 'rgb(17, 157, 255)'\nmarker_alpha = 0.8\n\nfig_size_auto = False\nfig_width = 1500\nfig_height = 700\n\n# Gridlines\nvert_grid = False\nhoriz_grid = True\n","894a3438":"animation_figure = InterAnim(\n                            # REQUIRED\n                            Param_col=Param_col, x_col=x_col, y_col=y_col,\n\n                            # OPTIONAL AESTHETICS\n                             marker_size=marker_size, plot_title=plot_title, title_size=title_size,\n                             x_label=x_label, y_label=y_label, param_label=param_label, plot_type=plot_type,\n                             marker_col=marker_col, marker_alpha=marker_alpha, fig_size_auto=fig_size_auto, \n                             fig_width=fig_width, fig_height=fig_height, vert_grid=vert_grid, horiz_grid=horiz_grid  )\niplot(animation_figure)\n","cdc3ebae":"#Change variable name to base for output graphs\nbase_x = -5\nbase_y = -5\n\nbin_x = 0\nbin_y = 0\naction_cap = 10000\n\nepsilon = 0.1\nalpha = 0.1\ngamma = 0.9\nV_bin = 0\n\nnum_episodes = 100\n\n# Make a copy of the initalised Q table so we don't override this\nQ_table_Q = Q_table.copy()\n\naction_table = pd.DataFrame()\nbest_actions_table = pd.DataFrame()\nfor e in range(0,num_episodes):\n    clear_output(wait=True)\n    print(\"Current Episode: \",  np.round(e\/num_episodes,4) *100,\"%\")\n    \n    # Randomly select start position between: -4,-5,-6 (note the upper bound is soft, i.e. <-3 = <=-4)\n    start_x = -5\n    start_y = -5\n    \n    \n    action = None\n    for a in range(0,action_cap):\n\n        action = eps_greedy_Q(Q_table_Q, epsilon, start_x, start_y, a, action)\n        # If action is to throw, use probability to find whether this was successful or not and update accordingly\n        if action['throw_dir'].iloc[0]!=\"none\":\n            rng_throw = np.random.rand()\n\n            if rng_throw <= action['prob'].iloc[0]:\n                reward = 1\n            else:\n                reward = -1\n            bin_Q = Q_table_Q[(Q_table_Q['state_x']==bin_x) & (Q_table_Q['state_y']==bin_y)]\n            bin_max_Q = bin_Q.sort_values('Q', ascending=False).iloc[0]['Q']\n            New_Q = ((1-alpha)*action['Q'].iloc[0]) + alpha*(reward + (gamma*bin_max_Q)) \n        # If move action, we have guaranteed probability and no introduce a small positive reward\n        else:\n            reward = 0.1\n            move_direction = action['move_dir'].iloc[0]\n            #Map this to actual direction and find V(s) for next state\n            if(move_direction == 0):\n                move_x = 0\n                move_y = 1\n            elif(move_direction == 1):\n                move_x = 1\n                move_y = 1\n            elif(move_direction == 2):\n                move_x = 1\n                move_y = 0\n            elif(move_direction == 3):\n                move_x = 1\n                move_y = -1\n            elif(move_direction == 4):\n                move_x = 0\n                move_y = -1\n            elif(move_direction == 5):\n                move_x = -1\n                move_y = -1\n            elif(move_direction == 6):\n                move_x = -1\n                move_y = 0\n            elif(move_direction == 7):\n                move_x = -1\n                move_y = 1\n\n            new_x = action['state_x'].iloc[0]+move_x\n            new_y = action['state_y'].iloc[0]+move_y\n            next_action_Q = Q_table_Q[(Q_table_Q['state_x']==new_x) &  (Q_table_Q['state_y']==new_y)]\n            next_action_max_Q = next_action_Q.sort_values('Q', ascending=False).iloc[0]['Q']\n            \n\n            New_Q = ((1-alpha)*action['Q'].iloc[0]) + alpha*(reward + (gamma*next_action_max_Q)) \n\n        #Update Q(s,a) value for state based on outcome\n        Q_table_Q['Q'] = np.where( ((Q_table_Q['state_x'] == action['state_x'].iloc[0]) & \n                                    (Q_table_Q['state_y'] == action['state_y'].iloc[0]) &\n                                    (Q_table_Q['throw_dir'] == action['throw_dir'].iloc[0]) &\n                                    (Q_table_Q['move_dir'] == action['move_dir'].iloc[0])\n                                    ),New_Q, Q_table_Q['Q'])\n    \n        #Add column to denote which episode this is for\n        action['episode'] = e\n        action_table = action_table.append(action)\n\n        # Break loop if action is a throw\n        if action['throw_dir'].iloc[0]!=\"none\":\n            break\n        else:\n            continue\n    action_table = action_table.reset_index(drop=True)  \n    \n    #Find best states\n    best_actions = Q_table_Q[Q_table_Q['Q']!=0].sort_values('Q', ascending=False).drop_duplicates(['state_x', 'state_y'])\n    best_actions['episode'] = e\n    \n    best_actions_table = best_actions_table.append(best_actions)\nbest_actions_table = best_actions_table.reset_index(drop=True)\n#Produce Summary output for each episode so we can observe convergence\nstart_state_action_values = best_actions_table[(best_actions_table['state_x']==base_x) & (best_actions_table['state_y']==base_y)][['episode','Q']].sort_values('episode')\n                                         \nstate_action_values = Q_table_Q.sort_values('Q',ascending=False).drop_duplicates(['state_x','state_y'])\n    \n    \nsns.set(rc={'figure.figsize':(15,10)})\npivot = state_action_values[[\"state_y\", \"state_x\", \"Q\"]].pivot(\"state_y\", \"state_x\", \"Q\")\n\nax = plt.subplot(221)\nax = sns.heatmap(pivot)\nax.hlines(range(-10,21), *ax.get_xlim())\nax.vlines(range(-10,21), *ax.get_ylim())\nax.set_title(\"Best State-Action Values for each State\")\nax.invert_yaxis()\n\nax2 = plt.subplot(222)\nax2.plot(start_state_action_values['episode'], start_state_action_values['Q'], label = \"Base State\")\nax2.plot(best_actions_table[best_actions_table['Q']!=0][['episode','Q']].groupby('episode').mean().reset_index()['episode'],\n         best_actions_table[best_actions_table['Q']!=0][['episode','Q']].groupby('episode').mean().reset_index()['Q'],label='All States')\nax2.legend()\nax2.set_title(\"Value of State's Best Action by Episode, alpha = 0.1, gamma = 0.9\")\n\nplt.show()","0b5e6ed2":"#Change variable name to base for output graphs\nbase_x = -5\nbase_y = -5\n\nbin_x = 0\nbin_y = 0\naction_cap = 10000\n\nepsilon = 0.1\nalpha = 0.1\ngamma = 0.1\nV_bin = 0\n\nnum_episodes = 100\n\n# Make a copy of the initalised Q table so we don't override this\nQ_table_Q = Q_table.copy()\n\naction_table = pd.DataFrame()\nbest_actions_table = pd.DataFrame()\nfor e in range(0,num_episodes):\n    clear_output(wait=True)\n    print(\"Current Episode: \",  np.round(e\/num_episodes,4) *100,\"%\")\n    \n    # Randomly select start position between: -4,-5,-6 (note the upper bound is soft, i.e. <-3 = <=-4)\n    start_x = -5\n    start_y = -5\n    \n    \n    action = None\n    for a in range(0,action_cap):\n\n        action = eps_greedy_Q(Q_table_Q, epsilon, start_x, start_y, a, action)\n        # If action is to throw, use probability to find whether this was successful or not and update accordingly\n        if action['throw_dir'].iloc[0]!=\"none\":\n            rng_throw = np.random.rand()\n\n            if rng_throw <= action['prob'].iloc[0]:\n                reward = 1\n            else:\n                reward = -1\n            bin_Q = Q_table_Q[(Q_table_Q['state_x']==bin_x) & (Q_table_Q['state_y']==bin_y)]\n            bin_max_Q = bin_Q.sort_values('Q', ascending=False).iloc[0]['Q']\n            New_Q = ((1-alpha)*action['Q'].iloc[0]) + alpha*(reward + (gamma*bin_max_Q)) \n        # If move action, we have guaranteed probability and no introduce a small positive reward\n        else:\n            reward = 0.1\n            move_direction = action['move_dir'].iloc[0]\n            #Map this to actual direction and find V(s) for next state\n            if(move_direction == 0):\n                move_x = 0\n                move_y = 1\n            elif(move_direction == 1):\n                move_x = 1\n                move_y = 1\n            elif(move_direction == 2):\n                move_x = 1\n                move_y = 0\n            elif(move_direction == 3):\n                move_x = 1\n                move_y = -1\n            elif(move_direction == 4):\n                move_x = 0\n                move_y = -1\n            elif(move_direction == 5):\n                move_x = -1\n                move_y = -1\n            elif(move_direction == 6):\n                move_x = -1\n                move_y = 0\n            elif(move_direction == 7):\n                move_x = -1\n                move_y = 1\n\n            new_x = action['state_x'].iloc[0]+move_x\n            new_y = action['state_y'].iloc[0]+move_y\n            next_action_Q = Q_table_Q[(Q_table_Q['state_x']==new_x) &  (Q_table_Q['state_y']==new_y)]\n            next_action_max_Q = next_action_Q.sort_values('Q', ascending=False).iloc[0]['Q']\n            \n\n            New_Q = ((1-alpha)*action['Q'].iloc[0]) + alpha*(reward + (gamma*next_action_max_Q)) \n\n        #Update Q(s,a) value for state based on outcome\n        Q_table_Q['Q'] = np.where( ((Q_table_Q['state_x'] == action['state_x'].iloc[0]) & \n                                    (Q_table_Q['state_y'] == action['state_y'].iloc[0]) &\n                                    (Q_table_Q['throw_dir'] == action['throw_dir'].iloc[0]) &\n                                    (Q_table_Q['move_dir'] == action['move_dir'].iloc[0])\n                                    ),New_Q, Q_table_Q['Q'])\n    \n        #Add column to denote which episode this is for\n        action['episode'] = e\n        action_table = action_table.append(action)\n\n        # Break loop if action is a throw\n        if action['throw_dir'].iloc[0]!=\"none\":\n            break\n        else:\n            continue\n    action_table = action_table.reset_index(drop=True)  \n    \n    #Find best states\n    best_actions = Q_table_Q[Q_table_Q['Q']!=0].sort_values('Q', ascending=False).drop_duplicates(['state_x', 'state_y'])\n    best_actions['episode'] = e\n    \n    best_actions_table = best_actions_table.append(best_actions)\nbest_actions_table = best_actions_table.reset_index(drop=True)\n#Produce Summary output for each episode so we can observe convergence\nstart_state_action_values = best_actions_table[(best_actions_table['state_x']==base_x) & (best_actions_table['state_y']==base_y)][['episode','Q']].sort_values('episode')\n                                         \nstate_action_values = Q_table_Q.sort_values('Q',ascending=False).drop_duplicates(['state_x','state_y'])\n    \n    \nsns.set(rc={'figure.figsize':(15,10)})\npivot = state_action_values[[\"state_y\", \"state_x\", \"Q\"]].pivot(\"state_y\", \"state_x\", \"Q\")\n\nax = plt.subplot(221)\nax = sns.heatmap(pivot)\nax.hlines(range(-10,21), *ax.get_xlim())\nax.vlines(range(-10,21), *ax.get_ylim())\nax.set_title(\"Best State-Action Values for each State\")\nax.invert_yaxis()\n\nax2 = plt.subplot(222)\nax2.plot(start_state_action_values['episode'], start_state_action_values['Q'], label = \"Base State\")\nax2.plot(best_actions_table[best_actions_table['Q']!=0][['episode','Q']].groupby('episode').mean().reset_index()['episode'],\n         best_actions_table[best_actions_table['Q']!=0][['episode','Q']].groupby('episode').mean().reset_index()['Q'],label='All States')\nax2.legend()\nax2.set_title(\"Value of State's Best Action by Episode, alpha = 0.1, gamma = 0.1\")\n\nplt.show()","5144eac2":"#Change variable name to base for output graphs\nbase_x = -5\nbase_y = -5\n\nbin_x = 0\nbin_y = 0\naction_cap = 10000\n\nepsilon = 0.1\nalpha = 0.1\nV_bin = 0\n\nnum_episodes = 100\n\n# Make a copy of the initalised Q table so we don't override this\nQ_table_Q = Q_table.copy()\n\n\nbest_actions_table = pd.DataFrame()\nfor gam in range(1,11):\n    gamma = gam\/10\n\n    action_table = pd.DataFrame()\n    for e in range(0,num_episodes):\n        clear_output(wait=True)\n        print(\"Current alpha: \", alpha)\n        print(\"Current Episode: \",  np.round(e\/num_episodes,4) *100,\"%\")\n\n        # Randomly select start position between: -4,-5,-6 (note the upper bound is soft, i.e. <-3 = <=-4)\n        start_x = -5\n        start_y = -5\n\n\n        action = None\n        for a in range(0,action_cap):\n\n            action = eps_greedy_Q(Q_table_Q, epsilon, start_x, start_y, a, action)\n            # If action is to throw, use probability to find whether this was successful or not and update accordingly\n            if action['throw_dir'].iloc[0]!=\"none\":\n                rng_throw = np.random.rand()\n\n                if rng_throw <= action['prob'].iloc[0]:\n                    reward = 1\n                else:\n                    reward = -1\n                bin_Q = Q_table_Q[(Q_table_Q['state_x']==bin_x) & (Q_table_Q['state_y']==bin_y)]\n                bin_max_Q = bin_Q.sort_values('Q', ascending=False).iloc[0]['Q']\n                New_Q = ((1-alpha)*action['Q'].iloc[0]) + alpha*(reward + (gamma*bin_max_Q)) \n            # If move action, we have guaranteed probability and no introduce a small positive reward\n            else:\n                reward = 0.1\n                move_direction = action['move_dir'].iloc[0]\n                #Map this to actual direction and find V(s) for next state\n                if(move_direction == 0):\n                    move_x = 0\n                    move_y = 1\n                elif(move_direction == 1):\n                    move_x = 1\n                    move_y = 1\n                elif(move_direction == 2):\n                    move_x = 1\n                    move_y = 0\n                elif(move_direction == 3):\n                    move_x = 1\n                    move_y = -1\n                elif(move_direction == 4):\n                    move_x = 0\n                    move_y = -1\n                elif(move_direction == 5):\n                    move_x = -1\n                    move_y = -1\n                elif(move_direction == 6):\n                    move_x = -1\n                    move_y = 0\n                elif(move_direction == 7):\n                    move_x = -1\n                    move_y = 1\n\n                new_x = action['state_x'].iloc[0]+move_x\n                new_y = action['state_y'].iloc[0]+move_y\n                next_action_Q = Q_table_Q[(Q_table_Q['state_x']==new_x) &  (Q_table_Q['state_y']==new_y)]\n                next_action_max_Q = next_action_Q.sort_values('Q', ascending=False).iloc[0]['Q']\n\n\n                New_Q = ((1-alpha)*action['Q'].iloc[0]) + alpha*(reward + (gamma*next_action_max_Q)) \n\n            #Update Q(s,a) value for state based on outcome\n            Q_table_Q['Q'] = np.where( ((Q_table_Q['state_x'] == action['state_x'].iloc[0]) & \n                                        (Q_table_Q['state_y'] == action['state_y'].iloc[0]) &\n                                        (Q_table_Q['throw_dir'] == action['throw_dir'].iloc[0]) &\n                                        (Q_table_Q['move_dir'] == action['move_dir'].iloc[0])\n                                        ),New_Q, Q_table_Q['Q'])\n\n            #Add column to denote which episode this is for\n            action['episode'] = e\n            action_table = action_table.append(action)\n\n            # Break loop if action is a throw\n            if action['throw_dir'].iloc[0]!=\"none\":\n                break\n            else:\n                continue\n        action_table = action_table.reset_index(drop=True)  \n\n        #Find best states\n        best_actions = Q_table_Q[Q_table_Q['Q']!=0].sort_values('Q', ascending=False).drop_duplicates(['state_x', 'state_y'])\n        best_actions['episode'] = e\n        best_actions['gamma'] = gamma\n\n        best_actions_table = best_actions_table.append(best_actions)\n    best_actions_table = best_actions_table.reset_index(drop=True)\n    #Produce Summary output for each episode so we can observe convergence\n    start_state_action_values = best_actions_table[(best_actions_table['state_x']==base_x) & (best_actions_table['state_y']==base_y)][['episode','Q']].sort_values('episode')\n\n    state_action_values = Q_table_Q.sort_values('Q',ascending=False).drop_duplicates(['state_x','state_y'])\n\n","32179693":"actions_plot_data = best_actions_table[best_actions_table['Q']!=0][['episode','gamma','Q']].groupby(['gamma','episode']).mean().reset_index()\nactions_plot_data.head(10)","e302c860":"Param_col = actions_plot_data['gamma']\nx_col = actions_plot_data['episode']\ny_col = actions_plot_data['Q']\n\nmarker_size = 0.7\n\nplot_title = \"Interactive-Animation Parameter Optimisation of RL Convergence - Varying Gamma\"\ntitle_size = 28\nx_label = \"Episode\"\ny_label = \"Mean Q for all non-zero states\"\nparam_label = \"Gamma = \"\n\n#plot_type = 'markers', 'lines+markers' or 'lines'\nplot_type = 'markers'\n\n# color could also be hex code\nmarker_col = 'rgb(204, 51, 255)'\nmarker_alpha = 0.8\n\nfig_size_auto = False\nfig_width = 1500\nfig_height = 700\n\n# Gridlines\nvert_grid = False\nhoriz_grid = True\n","c00196a9":"animation_figure = InterAnim(\n                            # REQUIRED\n                            Param_col=Param_col, x_col=x_col, y_col=y_col,\n\n                            # OPTIONAL AESTHETICS\n                             marker_size=marker_size, plot_title=plot_title, title_size=title_size,\n                             x_label=x_label, y_label=y_label, param_label=param_label, plot_type=plot_type,\n                             marker_col=marker_col, marker_alpha=marker_alpha, fig_size_auto=fig_size_auto, \n                             fig_width=fig_width, fig_height=fig_height, vert_grid=vert_grid, horiz_grid=horiz_grid  )\niplot(animation_figure)\n","1c9a872f":"# Define start position\nstart_x = -5\nstart_y = -5\nbin_x = 0\nbin_y = 0\naction_cap = 10000\n\nepsilon = 0.1\nalpha = 0.1\ngamma = 0.9\nV_bin = 0\n\nnum_episodes = 10000\n\n# Make a copy of the initalised Q table so we don't override this\nQ_table_Q = Q_table.copy()\n\naction_table = pd.DataFrame()\nbest_actions_table = pd.DataFrame()\nfor e in range(0,num_episodes):\n    clear_output(wait=True)\n    print(\"Current Episode: \",  np.round(e\/num_episodes,4) *100,\"%\")\n    action = None\n    for a in range(0,action_cap):\n\n        action = eps_greedy_Q(Q_table_Q, epsilon, start_x, start_y, a, action)\n        # If action is to throw, use probability to find whether this was successful or not and update accordingly\n        if action['throw_dir'].iloc[0]!=\"none\":\n            rng_throw = np.random.rand()\n\n            if rng_throw <= action['prob'].iloc[0]:\n                reward = 1\n            else:\n                reward = -1\n            bin_Q = Q_table_Q[(Q_table_Q['state_x']==bin_x) & (Q_table_Q['state_y']==bin_y)]\n            bin_max_Q = bin_Q.sort_values('Q', ascending=False).iloc[0]['Q']\n            New_Q = ((1-alpha)*action['Q'].iloc[0]) + alpha*(reward + (gamma*bin_max_Q)) \n        # If move action, we have guaranteed probability and no introduce a small positive reward\n        else:\n            reward = 0.1\n            move_direction = action['move_dir'].iloc[0]\n            #Map this to actual direction and find V(s) for next state\n            if(move_direction == 0):\n                move_x = 0\n                move_y = 1\n            elif(move_direction == 1):\n                move_x = 1\n                move_y = 1\n            elif(move_direction == 2):\n                move_x = 1\n                move_y = 0\n            elif(move_direction == 3):\n                move_x = 1\n                move_y = -1\n            elif(move_direction == 4):\n                move_x = 0\n                move_y = -1\n            elif(move_direction == 5):\n                move_x = -1\n                move_y = -1\n            elif(move_direction == 6):\n                move_x = -1\n                move_y = 0\n            elif(move_direction == 7):\n                move_x = -1\n                move_y = 1\n\n            new_x = action['state_x'].iloc[0]+move_x\n            new_y = action['state_y'].iloc[0]+move_y\n            next_action_Q = Q_table_Q[(Q_table_Q['state_x']==new_x) &  (Q_table_Q['state_y']==new_y)]\n            next_action_max_Q = next_action_Q.sort_values('Q', ascending=False).iloc[0]['Q']\n            \n\n            New_Q = ((1-alpha)*action['Q'].iloc[0]) + alpha*(reward + (gamma*next_action_max_Q)) \n\n        #Update Q(s,a) value for state based on outcome\n        Q_table_Q['Q'] = np.where( ((Q_table_Q['state_x'] == action['state_x'].iloc[0]) & \n                                    (Q_table_Q['state_y'] == action['state_y'].iloc[0]) &\n                                    (Q_table_Q['throw_dir'] == action['throw_dir'].iloc[0]) &\n                                    (Q_table_Q['move_dir'] == action['move_dir'].iloc[0])\n                                    ),New_Q, Q_table_Q['Q'])\n    \n        #Add column to denote which episode this is for\n        action['episode'] = e\n        action_table = action_table.append(action)\n\n        # Break loop if action is a throw\n        if action['throw_dir'].iloc[0]!=\"none\":\n            break\n        else:\n            continue\n    action_table = action_table.reset_index(drop=True)  \n    \n    #Find best states\n    best_actions = Q_table_Q[Q_table_Q['Q']!=0].sort_values('Q', ascending=False).drop_duplicates(['state_x', 'state_y'])\n    best_actions['episode'] = e\n    \n    best_actions_table = best_actions_table.append(best_actions)\nbest_actions_table = best_actions_table.reset_index(drop=True)\n#Produce Summary output for each episode so we can observe convergence\nstart_state_action_values = best_actions_table[(best_actions_table['state_x']==start_x) & (best_actions_table['state_y']==start_y)][['episode','Q']].sort_values('episode')\n                                         \nstate_action_values = Q_table_Q.sort_values('Q',ascending=False).drop_duplicates(['state_x','state_y'])\n    \n    \nsns.set(rc={'figure.figsize':(15,10)})\npivot = state_action_values[[\"state_y\", \"state_x\", \"Q\"]].pivot(\"state_y\", \"state_x\", \"Q\")\n\nax = plt.subplot(221)\nax = sns.heatmap(pivot)\nax.hlines(range(-10,21), *ax.get_xlim())\nax.vlines(range(-10,21), *ax.get_ylim())\nax.set_title(\"Best State-Action Values for each State\")\nax.invert_yaxis()\n\nax2 = plt.subplot(222)\nax2.plot(start_state_action_values['episode'], start_state_action_values['Q'], label = \"Start State\")\nax2.plot(best_actions_table[['episode','Q']].groupby('episode').mean().reset_index()['episode'],\n         best_actions_table[['episode','Q']].groupby('episode').mean().reset_index()['Q'],label='All States')\nax2.legend()\nax2.set_title(\"Value of State's Best Action by Episode\")\n\nplt.show()\n\n","bd1b3b50":"\noptimal_action_start_state = best_actions_table[(best_actions_table['state_x']==start_x)&(best_actions_table['state_y']==start_y)].sort_values('episode',ascending=False)\nif (optimal_action_start_state['throw_dir'].iloc[0]==\"none\"):\n    print(\"The optimal action from the start state is to MOVE in direction: \", optimal_action_start_state['move_dir'].iloc[0])\nelse:\n    print(\"The optimal action from the start state is to THROW in direction: \", optimal_action_start_state['throw_dir'].iloc[0])\n\n","a44d55dd":"optimal_action_start_state.head()","61bb6550":"    \n# Create Quiver plot showing current optimal policy in one cell\narrow_scale = 0.1\n\noptimal_action_list = state_action_values\n\noptimal_action_list['Action'] = np.where( optimal_action_list['move_dir'] == 'none', 'THROW', 'MOVE'  )\n\n\noptimal_action_list['move_x'] = np.where(optimal_action_list['move_dir'] == 0, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 1, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 2, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 3, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 4, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 5, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 6, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 7, int(-1),\n                                         int(-1000)\n                                        ))))))))\noptimal_action_list['move_y'] = np.where(optimal_action_list['move_dir'] == 0, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 1, int(1),\n                                         np.where(optimal_action_list['move_dir'] == 2, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 3, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 4, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 5, int(-1),\n                                         np.where(optimal_action_list['move_dir'] == 6, int(0),\n                                         np.where(optimal_action_list['move_dir'] == 7, int(1),\n                                         int(-1000)\n                                        ))))))))\noptimal_action_list['throw_dir_2'] = np.where(optimal_action_list['throw_dir']==\"none\",int(-1000), optimal_action_list['throw_dir'])\n\n# Define horizontal arrow component as 0.1*move direction or 0.1\/-0.1 depending on throw direction\noptimal_action_list['u'] = np.where(optimal_action_list['Action']==\"MOVE\", optimal_action_list['move_x']*arrow_scale,\n                                    np.where(optimal_action_list['throw_dir_2']==0, 0,np.where(optimal_action_list['throw_dir_2']==180, 0,\n                                    np.where(optimal_action_list['throw_dir_2']==90, arrow_scale ,np.where(optimal_action_list['throw_dir_2']==270, -arrow_scale,\n                                    np.where(optimal_action_list['throw_dir_2']<180, arrow_scale,-arrow_scale))))))\n                                             \n# Define vertical arrow component based 0.1*move direciton or +\/- u*tan(throw_dir) accordingly\noptimal_action_list['v'] = np.where(optimal_action_list['Action']==\"MOVE\", optimal_action_list['move_y']*arrow_scale, \n                                    np.where(optimal_action_list['throw_dir_2']==0, arrow_scale,np.where(optimal_action_list['throw_dir_2']==180, -arrow_scale,\n                                    np.where(optimal_action_list['throw_dir_2']==90, 0,np.where(optimal_action_list['throw_dir_2']==270, 0,\n                                    optimal_action_list['u']\/np.tan(np.deg2rad(optimal_action_list['throw_dir_2'].astype(np.float64))))))))\n                                             \nx = optimal_action_list['state_x']\ny = optimal_action_list['state_y']\nu = optimal_action_list['u'].values\nv = optimal_action_list['v'].values\n\nplt.figure(figsize=(10, 10))\nplt.quiver(x,y,u,v,scale=0.5,scale_units='inches')\nsns.scatterplot( x=\"state_x\", y=\"state_y\", data=optimal_action_list,  hue='Action')\nplt.title(\"Optimal Policy for Given Probabilities\")\nplt.show()","1ff6448a":"**We improve this with a plot that shows the optimal action for each state as demonstrated in Part 1.**\n","3997293a":"#### Interactive Animation for Gamma","32a606d3":"**We therefore increase the number of episodes greatly from 100 to 1,000**\n\nAs we are starting to find that this takes longer and longer, a good idea is to introduce a method to keep track of the progress of the loop. [To do this, I will apply a method introduced in this post](https:\/\/towardsdatascience.com\/3-tips-to-improving-your-data-science-workflow-71a6fb8e6f19).","9f37cbe9":"And we can continue this for multiple actions until the paper is thrown","ffbcf96f":"**Introduce Update Policy for TD(0) to Actions**","9864bfbf":"### Gamma Analysis","096542d9":"--- \n\n# Varying Parameters\n\nWe have three main parameters to vary, the learning rate $\\alpha$, the discount factor $\\gamma$ and our value for the $\\epsilon-greedy$ action selection. \n\nThe following explinations for each are taken directly from [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Q-learning) and have already introduced the $\\epsilon$ parameter in some detail.\n\n\n**Explore vs exploit**\n\n*The learning rate or step size determines to what extent newly acquired information overrides old information. A factor of 0 makes the agent learn nothing (exclusively exploiting prior knowledge), while a factor of 1 makes the agent consider only the most recent information (ignoring prior knowledge to explore possibilities). In fully deterministic environments, a learning rate of $\\alpha _{t}=1$ is optimal. When the problem is stochastic, the algorithm converges under some technical conditions on the learning rate that require it to decrease to zero. In practice, often a constant learning rate is used, such as $ \\alpha _{t}=0.1$ for all $t$.[3]*\n\n**Discount factor**\n\n*The discount factor $\\gamma$  determines the importance of future rewards. A factor of 0 will make the agent \"myopic\" (or short-sighted) by only considering current rewards, i.e. $r_{t}$ (in the update rule above), while a factor approaching 1 will make it strive for a long-term high reward. If the discount factor meets or exceeds 1, the action values may diverge. For $\\gamma =1$, without a terminal state, or if the agent never reaches one, all environment histories become infinitely long, and utilities with additive, undiscounted rewards generally become infinite.[4] Even with a discount factor only slightly lower than 1, Q-function learning leads to propagation of errors and instabilities when the value function is approximated with an artificial neural network.[5] In that case, starting with a lower discount factor and increasing it towards its final value accelerates learning.[6]*\n\n\n**So what do these mean, and, more importantly, what are we trying to achieve with our parameter selection?**\n\nThe overall aim is that we are trying to find the optimal action for any given state whilst achieving this in a reasonable number of effort (measured by the number of episodes, computation needed or time). A good explination for the **learning rate** is that a high value means we value the information gained in each action more and so learn faster but may find it hard to full converge whilst a small value will take longer but will steadily converge. \n\nA good analogy for this is to think of it like we are playing Golf with just one club; a high alpha corresponds to using a big-hitting club whilst a small alpha value is akin to using a small-hitting club. The big-hitting club will initially get us closer to the green but once we get close it is hard to accurately hit the hole. However, a small-hitting club will take more attempts to reach the green but once it does we have more control and can reach the hole easier.\n\n![Alpha Analogy](https:\/\/i.imgur.com\/ExGpRwl.png)\n\n![Interactive Animation Example](https:\/\/i.imgur.com\/qrpwvCi.gif)\n\nWe have already observed the effect of a large alpha parameter in our earlier applications where the values oscillated between each episode. Therefore, we need to use a small value but this introduces the challenge regarding the number of episodes required to converge. We already need thousands of episodes to converge for a fixed start state and we have 100 to consider for the full environment. \n\n\n**This is the trade-off we have to consider and the best decision may be to only learn for one fixed start state at a time when needed rather than trying to find the optimal policy for all states.**\n\nWhen we observe the trends for varying alpha in the animation below, we see that, if we fix the start state, we are able to use a small alpha value without needing an impossibly large number of episodes. If we were to consider all states, we would likely need to use a slightly larger alpha value to reach a suitable result in good time.\n\n**We therefore go back to considering a single fixed start state of [-5,-5] and choose an alpha value of $\\alpha =  0.1$. **\n\n**With these set, we then evaluate the choice of $\\gamma$.** From the Wikipedia explination, we understand that the value corresponds to whether we consider furture rewards important or not. It helps when we consider this to remind ourselves of the update rule for Q-learning:\n\n\\begin{equation}\n Q^{new}(s_{t},a_{t})\\leftarrow (1-\\alpha )\\cdot \\underbrace {Q(s_{t},a_{t})} _{\\text{old value}}+\\underbrace {\\alpha } _{\\text{learning rate}}\\cdot \\overbrace {{\\bigg (}\\underbrace {r_{t}} _{\\text{reward}}+\\underbrace {\\gamma } _{\\text{discount factor}}\\cdot \\underbrace {\\max _{a}Q(s_{t+1},a)} _{\\text{estimate of optimal future value}}{\\bigg )}} ^{\\text{learned value}}\n \\end{equation}\n \n\nWith this, we see that $\\gamma$ scales the Q value taken for the best action from the next state. This is in relation to the reward of the action itself as part of the same bracket and so if we reduce this by having a small gamma value then the reward has more weight. Conversely, if we take a high gamma value, we consider the imformation obtained from this next state to be more important.\n\n**Therefore, we would ideally choose a value that adds value to future rewards so that our decisions lead to optimally to the bin and select a value of $\\gamma = 0.9$.**\n\n\n\n\n\n","ee176a1b":"## Introducing RL Algorithms\n\nSo we have introduced episodes and how to choose actions but we have yet to demonstrate how and algorithm uses this to learn the best actions. Therefore, we will formally define our first RL algorithm, **Temporal Difference 0*.\n\n#### Temporal Difference 0\n\nTemporal Difference $\\lambda$ are a family of algorithms depending on the choice of $\\lambda$. The simplest is to set this to zero at which point have the following update rule:\n\n**Definition: TD(0) Update Rule:** [Wiki](https:\/\/en.wikipedia.org\/wiki\/Temporal_difference_learning)\n\\begin{equation}\nV(s)\\leftarrow V(s)+\\alpha (\\overbrace {r+\\gamma V(s')} ^{\\text{The TD target}}-V(s))\n\\end{equation}\n\nwhere:\n\n- $V(s)$ is the value of state $s$, \n- $\\alpha$ is the **learning rate parameter**, \n- $r$ is the reward,\n- $\\gamma$ is the **discount factor parameter** and, \n- $V(s')$ is the vale of the next state.\n\nSo what does this equation mean? In short, we update our knowledge of the quality of the current state, denoted $V(s)$, based on a combination of what its value is and the result of taking the action to the next state defined in the episode. \n\nFor example, say we start the learning processing and our first action is to throw from state [-5,-5] and it successfully hits the bin, then we have a positive reward of $+1$ for reaching to goal. Therefore, we have the following update:\n\n\\begin{equation}\nV_1([-5,-5])\\leftarrow V_0([-5,-5])+\\alpha (\\overbrace { +1 +\\gamma V_0([bin])} ^{\\text{The TD target}}-V_0([-5,-5]))\n\\end{equation}\n\nand say, just for now, we set $\\alpha = 0.5 $ and $\\gamma = 0.5$ and $V([bin]) = 0$ by definition, then we have:\n\n\\begin{equation}\nV_1([-5,-5])\\leftarrow 0+0.5 ( +1 + 0.5*0 - 0) = 0.5\n\\end{equation}\n\nThis may seem like a trivial calulation but it is important to remember that success is not guaranteed. Therefore, if we think about all possible actions, the result of this first throw means we believe that this throw action is current the best choice. This throw action has a value of 0.5 compared to 0 for all other actions that haven't been tested yet.\n\nTherefore, under the $\\epsilon-greedy$ selection process, we would try this again. However this time, the paper does not go in the bin but misses and we therefore have a negative terminal reward of $-1$:\n\n\\begin{equation}\nV_2([-5,-5])\\leftarrow V_1([-5,-5])+\\alpha (\\overbrace { -1 +\\gamma V_1([bin])} ^{\\text{The TD target}}-V_1([-5,-5]))\n\\end{equation}\n\nand with the values:\n\n\\begin{equation}\nV_2([-5,-5])\\leftarrow 0.5+0.5 ( +1 + 0.5*0 - 0.5) = 0.5-0.25 = 0.25\n\\end{equation}\n\nSo we see that our value of this state has now reduced slightly to account for the second throw. \n\n**The core concept in Reinforcement Learning is that we are testing actions by repeated sampling; we need to repeat the number of samples until the results converge to an estimate of the true probabilistic outcomes.** \n\nFor example, if we consider tossing a coin 2 times, we are fairly like to have both outcomes being heads but if we throw it 100 times then we would likely see a 50\/50 split between heads and tails. In our example, if throwing from state [-5,-5] is a good action, then repeatedly trying this should, overall, lead to a positive result. This can be quite difficult to comprehend at first but in simple terms, we are testing the action by trial and error and making our algorithm do all the work so we don't have to.\n\n**Note: For now, we will fix the parameters to be $\\epsilon = 0.1$,  $\\alpha = 0.5 $ and $\\gamma = 0.5$ until we demonstrate parameter changes later.**\n","046505d4":"**Varying Rewards**\n\nWe note that the results of this show that the value of the states are very negative and that they are diverging (i.e. not stable). \n\nThere are a few steps we can take to improve this, first we will introduce rewards for other actions. Currently the only rewards we have are for when the algorithm throws and recieves a $+1$ for the positive goal or a $-1$ for the negative goal.\n\nThis is part of the process in Reinforcement Learning that gives us control as to what the algorithm optimises for. For example, say we want to discourage the algorithm from throwing, we could introduce a small positive reward (say 0.1) for each move action as shown below.\n\n","15c999a6":"# Reinforcement Learning from Scratch Part 2: Applying Model-free Methods and Evaluating Parameters in Detail\n\n\n\n## Introduction\n\nThe aim is to find the best action between throwing or moving to a better position in order to get paper into a bin (trash can). In this problem, we may throw from any position in the room but the probability of it is relative to the current distance from the bin and the direction in which the paper is thrown. Therefore the actions available are to throw the paper in any 360 degree direction or move to a new position to try and increase the probability that a throw made will go into the bin.\n\n[Previously, in part 1, we introduced the problem where the bin's location is known and can be solved directly with Value-Itearation methods.](https:\/\/www.kaggle.com\/osbornep\/rl-from-scratch-part-1-defining-the-environment)\n\n**In part 2, we now show how RL can be used similarly to find the optimal policy if the probabilities are hidden using model-free methods (e.g. Q-learning, Monte Carlo, etc).**\n\n**Furthermore, a key aim of this project is to demsontrate, in detail, the effect that varying parameters within the RL model has on the output.**\n\nI will also be publishing an accompanying article explaining the process in a simpler format that can be found on my Medium page:\n\nhttps:\/\/medium.com\/@sterlingosborne\n\n## Motivation\n\nThis project was created as a means to learn Reinforcement Learning independently in a Python notebook. In other words, without the need of a complex, virtual environment to interact with. By fully defining the probabilistic environment, we are able to simplify the learning process and clearly demonstrate the effect changing parameters has on the results. This is a valuable in any Machine Learning task but particuarly in Reinforcement Learning where it can be hard to understand the impact varying parameters without a clear and well-defined example.\n\n**Furthermore, we use this as an opportunity to introduce a novel visualisation method for examining parameters.** When we vary parameters in Reinforcemnet Learning, we observe the results over a number of attempts (or episodes) where we hope to show stability and convergence. This is a two dimensional comparison (x = episode and y = output) and when we want to observe the results if we vary a parameter this becomes three dimentional (x = episode, y = output and z = parameter). The simplest and most commonly used solution is to produce multiple plots for each parameter choice. An alternative, but more complex visual choice, is to plot in three dimensions.\n\n**Instead, we introduce our novel method of an interactive animation whereby the parameter change is shown over time.**\n\nThe aim of this visulisation is to improve the way in which you compare parameter choices. However, because parameter selection is often a process that needs to be performed quickly, we acknowledge the requirement that this must be simple to achieve. The final parameter choices may then be formatted appropiately but the aesthetics of the visuals in the decision process may be less strict. \n\nTherefore, although this is a method that we have [introduced previously](https:\/\/towardsdatascience.com\/creating-interactive-animation-for-parameter-optimisation-using-plot-ly-8136b2997db), we have since taken this further and formally defined as a package that can be easily downloaded and used without extensive knowledge of Plot.ly's library. \n\n**-----------------GITHUB RELEASE IN PROGRESS---------------------**\n\nWe will demosntrate how to use the graphing function in this notebook. A quick example of the interactive animation can be used is shown below where we can see the dimensions discussed previously.\n\n![Interactive Animation Example](https:\/\/i.imgur.com\/qrpwvCi.gif)\n\n\n**Lastly, state of the art research in model explainability emphasis the requirement to digrammatically outline methods clearly and consistently.** \n\n[Researchers from Google](https:\/\/arxiv.org\/abs\/1810.03993) have introduced **Model Cards** as a means to provide transparency in training algorithms. \n\n*Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. *\n\n![Model Cards](https:\/\/i.imgur.com\/2no3rHb.png)\n\n\nFurthermore, a colleague of mine is introducing a framework for formalising diagrammatic methods for skilled researchers to share their work; known as [DIAL](https:\/\/arxiv.org\/abs\/1812.11142). This research attempts to indentify recurring primitives and building blocks of AI systems, and proposes a schematic notation to try and facilitate improved communication about AI Systems research.\n\n*Currently, there is no consistent model for visually or formally representing the architecture of AI systems. This lack of representation brings interpretability, correctness and completeness challenges in the description of existing models and systems. DIAL (The Diagrammatic AI Language) has been created with the aspiration of being an \"engineering schematic\" for AI Systems. It is presented here as a starting point for a community dialogue towards a common diagrammatic language for AI Systems.*\n\n\n\n**Therefore, we have created a visual that shows and summarises the entire workflow to produce the final output:**\n\n\n![RL Complete Process](https:\/\/i.imgur.com\/2mHMuQg.png)","cda101d9":"**Completely Random Action Selection**","81ccb6dd":"**We note that although we have a good result for the fixed start state, those around it that are also passed in episodes are less than optimal.**\n\nAs our goal is to learn for any given start state, we increase the range of start states to be considered for all between the [-10,10] range and pick randomly.","9e73156d":"---\n\n# Defining Model-free Reinforcement Learning Methods\n\nWe introduce three model-free methods that are considered to be the simplest to apply and compare their strengths and weaknesses. However, before we do this we consider the difference between model-free methods and our previously used value-iteration model-based method. In short, model-based methods use the knowledge of the probabilistic environment as a guide and plans the best actions accordingly. In model-free methods, the algorithm has no knowledge of this probabilities it just tries actions and observes the results.\n\nIn this example, we have calculated the proabiblities and will use these to find the outcome of the actions but they are not used within the algorithm's learning directly. \n\nFurthmore, in the model-based methods, we update all actions in large \"sweeps\" where the value of all states are updated in one pass. In model-free methods we use eipsodes where only states that are visited are updated. An **episode** is a path from a start state to the terminal state; in our example, the terminal state is when the algorithm throws the papers and the outcome may be successful or a miss. \n\n![Model Update Differences](https:\/\/i.imgur.com\/vgE21Wg.png)\n\n\n","9529110b":"### Initialise State-Action Pairs\nBefore applying the algorithm, we intialise each state-action value into a table. First we formthis for all throwing actions then all moving actions.\n\nWe can throw in any direction and therefore there are 360 actions for each degree starting from north as 0 clockwise to 359 degrees.\n\nAlthough movement may seem simpler in that there are 8 possible actions (north, north east, east, etc) there are complications in that unlike being able to throw in any direction from any position, there are some movements that aren't possible. For example, if we are at the edge of the room, we cannot move beyong the boundary and this needs to be accounted for. Although this could be coded nicer, I have done this manually with the if\/elif statements shown that skips the row if the position and movement is not possible.","64068368":"## Q-Learning\n\nMuch like TD(0), Q-learning learns as we take each action but instead searches through the possible subsequent actions to learn faster. \n\n\n\n**Definition: Q-Learning Update Rule:** [Wiki](https:\/\/en.wikipedia.org\/wiki\/Q-learning)\n\\begin{equation}\n Q^{new}(s_{t},a_{t})\\leftarrow (1-\\alpha )\\cdot \\underbrace {Q(s_{t},a_{t})} _{\\text{old value}}+\\underbrace {\\alpha } _{\\text{learning rate}}\\cdot \\overbrace {{\\bigg (}\\underbrace {r_{t}} _{\\text{reward}}+\\underbrace {\\gamma } _{\\text{discount factor}}\\cdot \\underbrace {\\max _{a}Q(s_{t+1},a)} _{\\text{estimate of optimal future value}}{\\bigg )}} ^{\\text{learned value}}\n \\end{equation}\n\nwhere:\n\n- $Q(s_t,a_t)$ is the value of state-action pair $s$, \n- $\\alpha$ is the **learning rate parameter**, \n- $r$ is the reward,\n- $\\gamma$ is the **discount factor parameter** and, \n- $Q(s_{t+1}, a )$ is the value of action-pairs in the next state.\n\nAs before, we will fix the parameters to be $\\epsilon = 0.1$,  $\\alpha = 0.5 $ and $\\gamma = 0.5$ ","93779610":"**$\\epsilon-greedy$ Action Selection**","2a7896a5":"### Alpha Analysis","08fdf8fd":"**Although this appears worse at first, the value of the state oscillating shows that there is a value its trying to find but our choice of parameters is causing it to diverge. But, we at least can see that it is getting closer to converging.**\n\nWe could start varying parameters but part of the issue is that we are summarising the value of the state for a large number of actions (360 throw directions and 8 move directions). Therefore, instead of summarising this into one value, it would be better to consider the quality of each state-action pair individually.\n\nTo do this, we can introduce our second model-free method: **Q-learning**.\n","8529968b":"**After 100 episodes, we see that the states around our fixed start point have updated but if we compare the following heatmap side by side with the previous line plot, we see that this has not fully converged after 100 episodes and is still updating.**","c2bee724":"**$\\epsilon-greedy$ as a Function **\n\nTo make things easier later, we define this now as a function so that our code later on is simpler and we can focus on the algorithm. ","b1cae486":"# Conclusion\n\nWe see that the final output for start state is to move south east. As mentioned, we may want to consider changing the rewards so that throwing is more encouraged than moving. The results for all states covered by the episodes converge within the 10,000 episodes though it appears that many have yet to be fully explored and are not optimal. But if we only concered with the start state then these do not matter significantly.\n\nI hope this notebook is useful for demonstrating the impact each parameter has on learning and the overall process of RL in a self contained example.\n\nThanks\n\n","a1984634":"### Probability Function\nThis function defines the probability of a sucessful throw from any given state and is calculated by the following:\n\nFirst, if the position is the same as the bin (i.e. the person is directly inside the bin already) then the probability is fixed to 100%.\n\nNext, we have to re-define the throwing direction in two cases to accomodate for the fact that 360 degrees is the same as 0 degrees. For example, if we are south-west of the bin and throw 350 degrees, this would be the same as -10 degrees and would then relate to a bearing from the person to the bin less than 90 correctly.\n\nThen the euclidean distance is calculated followed by the max distance a person could be from the bin.\n\nWe then calculate the bearing from the person to the bin following the previous figure and calcualte the score bounded within a +\/- 45 degree window. Throws that are closest to the true bearing score higher whilst those further away score less, anything more than 45 degrees (or less than -45 degrees) are negative and then set to a zero probability.\n\nLastly, the overall probability is related to both the distance and direction given the current position.","dfa53fcf":"### Action Selection\nWe could continue this selection process but this is a highly inefficient method of choosing which action to take. When we implement our learning process we will begin to learn which actions lead towards the positive goal so if we keep randomly selecting we are wasting all that effort.\n\nTherefore, we instead introduce a method that takes this into account, known as **epsilon-greedy**.\n\n*The best lever is selected for a proportion $1-\\epsilon$ of the trials, and a lever is selected at random (with uniform probability) for a proportion $\\epsilon$ . A typical parameter value might be $epsilon =0.1$, but this can vary widely depending on circumstances and predilections.* ([wiki](https:\/\/en.wikipedia.org\/wiki\/Multi-armed_bandit))\n\nIn other words, we pick an action randomly with probability $\\epsilon$ and will otherwise pick the best action. If we have mutliple \"best actions\" we then randomly select from this list.\n\nSo why do we not simply keep picking the best action each time? Well this can cause a problem if we have an action that works but is not neccessarily the best. This is often considered in other Machine Learning problems as the local minimum\/maxmum. If we keep using an action that seems to work we may miss the opportunity to try a better action because we never tried it and this can cause instability in the results. \n\nThe animation below demonstrates the outcome as we reduce epsilon. With high epsilon values, we are randomly selecting actions so will likely pick bad ones. As we reduce epsilon we select actions more and more greedily improving the results whilst still ensuring we can explore new actions to minimise the risk we are in a local max ratherh than global max.\n\n**We therefore select a small epislon value $\\epsilon = 0.1$**\n\n![epsilon varied](https:\/\/i.imgur.com\/Rx7HVAE.gif)\n\nThere are other methods that can be considered, such as the [Softmax Function](https:\/\/en.wikipedia.org\/wiki\/Softmax_function).\n\n#### Action Cap\nWe also introduce an **action cap** which stops the episode occuring endlessly. If we reach this cap then we can consider the episode goal as negative but set it very large so is unlikely to reach it.\n\n#### Action Selection between throw and move\nBecause we have many more throwing actions than moving action (360 vs 8), as before, we introduce the notion that have a 50\/50 chance of being a throw or move action first and then subsequently select from these sub-choices. \n\n#### Code Format\nJust to make things easier to track, we introduce a method that creates a data table with the output actions and will continue to use this for all methods. [Information on this can be found in this post](https:\/\/towardsdatascience.com\/3-tips-to-improving-your-data-science-workflow-71a6fb8e6f19).","47aa34e1":"## Forming Episodes and Defining the Action Selection Process\n\nIf we define the starting position, an episode is the actions taken from that position until the paper is thrown. If it reaches the bin, then we have a positive goal reward of +1. However, if we miss the bin, then we have a negative goal reward of -1.\n\nWe can form the episodes randomly:","d4ca207c":"## Final Parameter Output\n\n    - alpha = 0.1\n    - gamma = 0.9\n    - epsilon = 0.1\n    - 10,000 episodes\n    - Fixed start state: [-5,-5]","76ea1e33":"## Pre-processing: Introducing the Probabilistic Environment \n\nFollowing the previously defined environment (see [META notebook](https:\/\/www.kaggle.com\/osbornep\/meta-environment-defined-only-for-forking)) we have also found the optimal policy calculated from value iteration.\n\nThe optimal policy can be imported from the data file and is fixed given the bin is at (0,0) and the probabilities are calculated as shown in the function below.\n\n### RL Environment Definitions\n\nA **policy** is the currently recommended actions for all given states. The **states ($s$ and $s'$)** are the position in the room and the **actions ($a$)** are either moving in one of 8 directions (north, north east, east, ... north west) or throwing in any 360 degree direction (0,1,2,3,4,...,359,360 degrees) from due north (see [Navigation Bearings](https:\/\/en.wikipedia.org\/wiki\/Bearing_(navigation)). \n\nIn our probabilistic environment, we have defined that **moving has a guaranteed outcome** of being followed correctly (i.e. you dont miss-step) but **throwing in a direction is not guaranteed to get into the bin**. The probability of the throw successfully going into the bin is relative to the distance of the current position to the bin and the direction thrown from the true direction. \n\nThis defines our probabilistic environment and transition function:\n\n$$ P_{a}(s,s')=Pr(s_{t+1}=s'|s_{t}=s,a_{t}=a) $$\n\nis the probability of transition from state $s$ to state $s'$ under action $a$ (https:\/\/en.wikipedia.org\/wiki\/Reinforcement_learning).\n\n(Note: bin is an English term for trash can\/ dustbin\/ garbage pail\/ litter basket\/ trash barrel\/ trash bin\/ trash can\/ wastebasket\/ wastepaper basket)","26ad7e9d":"**Repeat for a number of episodes and collect results**"}}