{"cell_type":{"98314308":"code","23b207f2":"code","60e8122e":"code","20bbb306":"code","493f98c5":"code","38658e09":"code","c7a18e1c":"code","192083ce":"code","db0f844f":"code","4cd39a16":"code","188d4bbb":"code","c80c3615":"code","cb23c7cb":"code","1c6d5966":"code","9c8d8dbd":"code","4c852cf8":"code","75bb4851":"code","fad2a93e":"code","200cc234":"code","520883a6":"code","6c2ea03b":"code","5b4c36c4":"code","e1417695":"code","23a8606c":"code","4492894c":"code","65f274e7":"code","e5b11a96":"code","2097f662":"code","9c857e32":"code","613a3ae8":"code","c9a60e17":"code","23c756be":"code","a638b376":"markdown","2ba93a26":"markdown","b539a9ad":"markdown","39759a49":"markdown","a8bdcd32":"markdown","ea99d455":"markdown","dd1d57ca":"markdown","bb4f0cda":"markdown","24f30ea9":"markdown","22274fa6":"markdown","f20ab339":"markdown","caddec69":"markdown"},"source":{"98314308":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","23b207f2":"df=pd.read_csv('\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\ndf.head()","60e8122e":"#Details about the columns of the Dataframe\ndf.info()","20bbb306":"#Finding out total number of null values in either columns\ndf.isnull().sum()","493f98c5":"#Composition of Reviews\nimport seaborn as sns\nsns.countplot(x=\"sentiment\",data=df)","38658e09":"!pip install contractions","c7a18e1c":"import contractions\n#This package is used to replace the contractions in English language with their actual forms\nfrom tqdm import tqdm\n#tqdm is used to display the percentage of work done by a for loop.\nimport nltk\n#Contains various language specific datasets and tools for analysis\nimport re\n#used to work with regular expression and helps in finding the matches for a given regex.\nimport time\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n#donwloadin the stopwords of english language\nstopwords=stopwords.words('english')\n#Removing stopwords 'no','nor' and 'not'\nstopwords.remove('no')\nstopwords.remove('nor')\nstopwords.remove('not')\n\n#converting the sentiment column which of type object to integer to perform machine Learning algorithms\ndf['sentiment']=df['sentiment'].apply(lambda x: 1 if x=='positive' else 0)\ndf.head(10)","192083ce":"processed_reviews=[]\nfor i in tqdm(df['review']):\n    #Regular expression that removes all the html tags pressent in the reviews\n    i=re.sub('(<[\\w\\s]*\/?>)',\"\",i)\n    #Expanding all the contractions present in the review to is respective actual form\n    i=contractions.fix(i)\n    #Removing all the special charactesrs from the review text\n    i=re.sub('[^a-zA-Z0-9\\s]+',\"\",i)\n    #Removing all the digits present in the review text\n    \n    i=re.sub('\\d+',\"\",i)\n    #Making all the review text to be of lower case as well as removing the stopwords and words of length less than 3\n    #processed_reviews.append(\" \".join([lemmatizer.lemmatize(j.lower()) for j in i.split() if j not in stopwords and len(j)>=3]))\n    processed_reviews.append(\" \".join([j.lower() for j in i.split() if j not in stopwords and len(j)>=3]))","db0f844f":"#Creating a new datafram using the Processed Reviews\nprocessed_df=pd.DataFrame({'review':processed_reviews,'sentiment':list(df['sentiment'])})","4cd39a16":"processed_df.head()","188d4bbb":"#Splitting the data into dependent and independent variables i.e, features and the target columns\nX=processed_df['review']\nY=processed_df['sentiment']\n#Splitting the data such that 33% will be used for testing and the remaining 67% will be used for training. \nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(X,Y,stratify=Y,test_size=0.33)\n#when stratify is provided the splitting of data into train and test datasets agree with the composition of actual possitive and negative reviews present in the dataset","c80c3615":"from gensim.models import Word2Vec","cb23c7cb":"#Preparing data for training the Word2Vec model. It requies each review to be as a list of words.\nwords_in_sentences=[]\nfor i in tqdm(x_train):\n    words_in_sentences.append(i.split())","1c6d5966":"print(\"Model Training Started...\")\nmodel = Word2Vec(sentences=words_in_sentences, vector_size=200,workers=-1)\nprint(\"Model Training Completed...\")","9c8d8dbd":"#Word2Vec model returns the similar words for a given word\nmodel.wv.most_similar('interesting', topn=10)","4c852cf8":"model.wv.similar_by_word('interesting')","75bb4851":"#Word emebedding for a given word.\nmodel.wv.get_vector('interesting')","fad2a93e":"model.wv.n_similarity(['king','male'],['queen','female'])","200cc234":"model.wv.distance('king','queen')","520883a6":"model.wv.doesnt_match([\"king\", \"george\",\"stephen\",\"truck\"])","6c2ea03b":"vocab=list(model.wv.key_to_index.keys())\nprint(len(vocab))","5b4c36c4":"def avg_w2vec(sentences):\n    \"\"\"\n    This Function is using Average Word2Vec approach for creating a numerical vector for a given review from the word embeddings of each words of the review.\n    \"\"\"\n    transformed=[]\n    for sentence in tqdm(sentences):\n        count=0\n        vector=np.zeros(200)\n        for word in sentence.split():\n            if word in vocab:\n                vector+=model.wv.get_vector(word)\n                count+=1\n        if count!=0:\n            vector\/=count\n            transformed.append(vector)\n    return np.array(transformed)","e1417695":"x_train_transformed=avg_w2vec(x_train)\nx_test_transformed=avg_w2vec(x_test)","23a8606c":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\ngrid_params = { 'n_neighbors' : [10,20,30,40,50,60],\n               'metric' : ['manhattan']}\nknn=KNeighborsClassifier()\nclf = RandomizedSearchCV(knn, grid_params, random_state=0,n_jobs=-1,verbose=1)\nclf.fit(x_train_transformed,y_train)","4492894c":"clf.best_params_","65f274e7":"clf.best_score_","e5b11a96":"clf.cv_results_","2097f662":"from sklearn.metrics import roc_curve, auc,classification_report,confusion_matrix\ntrain_fpr,train_tpr,thresholds=roc_curve(y_train,clf.predict_proba(x_train_transformed)[:,1])\ntest_fpr,test_tpr,thresholds=roc_curve(y_test,clf.predict_proba(x_test_transformed)[:,1])\nimport matplotlib.pyplot as plt\nplt.plot(train_fpr,train_tpr,label=\"Training Accuracy=\"+str(round(auc(train_fpr, train_tpr),2)))\nplt.plot(test_fpr,test_tpr,label=\"Testing Accuracy =\"+str(round(auc(test_fpr, test_tpr),2)))\nplt.scatter(train_fpr,train_tpr,label=\"Training Accuracy=\"+str(round(auc(train_fpr, train_tpr),2)))\nplt.scatter(test_fpr,test_tpr,label=\"Testing Accuracy =\"+str(round(auc(test_fpr, test_tpr),2)))\nplt.legend()\nplt.xlabel(\"Thresholds\")\nplt.ylabel(\"ACCURACY\")\nplt.title(\"Training and Testing ROC Curves\")\nplt.show()","9c857e32":"sns.heatmap(confusion_matrix(y_train,clf.predict(x_train_transformed)),annot=True)\nplt.show()","613a3ae8":"sns.heatmap(confusion_matrix(y_test,clf.predict(x_test_transformed)),annot=True)\nplt.show()","c9a60e17":"print(classification_report(y_train,clf.predict(x_train_transformed)))","23c756be":"print(classification_report(y_test,clf.predict(x_test_transformed)))","a638b376":"\n<center> <img src=\"https:\/\/devopedia.org\/images\/article\/221\/4080.1570464995.png\"> <\/center>","2ba93a26":"## Confusion Matrix and Classification Report for both Training and Testing Samples","b539a9ad":"# <center style=\"color:red\"> \ud83c\udf89\ud83c\udf8a Upvote the notebook if it is useful and Informative. \ud83c\udf89 \ud83c\udf8a <\/center>","39759a49":"# <center> Model Training and Hyper-Parameter Tuning using GridSearchCV<\/center>","a8bdcd32":"## Average Word2Vec approach to create embedding for a sentence","ea99d455":"# <center> Data Cleaning <\/center>","dd1d57ca":"# <center> Word2Vec <\/center>\n","bb4f0cda":"# <center> Model Evaluation <\/center>","24f30ea9":"### The Dataset is balanced. The compositon of both positive and Negative reviews is same. In other words, total numner of positive and negative reviews are equal in number","22274fa6":"# <center> Data Preparaton<\/center>","f20ab339":"## <center> Data Analysis <\/center>","caddec69":"## ROC curve for Training and Testing Samples"}}