{"cell_type":{"cf381436":"code","ca81f158":"code","f85fa9a9":"code","57e988c6":"code","15d42b4e":"code","a1f7506c":"code","9c6c7a25":"code","f22d85fe":"markdown","bf8492c7":"markdown","d29ff82e":"markdown","c9689fa9":"markdown","7ae935d5":"markdown","6a1330ca":"markdown","81091cbf":"markdown","883603e4":"markdown"},"source":{"cf381436":"# to use the csvvalidator package, you'll need to \n# install it. Turn on the internet (in the right-hand\n# panel; you'll need to have phone validated your account)\n\nimport sys\n!{sys.executable} -m pip install csvvalidator","ca81f158":"# import everything from the csvvalidator package\nfrom csvvalidator import *\n\n# Specify which fields (columns) your .csv needs to have\n# You should include all fields you use in your dashboard\nfield_names = ('key',\n               'date',\n               'unit_name',\n               'units_sold',\n               'store'\n               )\n\n# create a validator object using these fields\nvalidator = CSVValidator(field_names)\n\n# write some checks to make sure specific fields \n# are the way we expect them to be\nvalidator.add_value_check('key', # the name of the field\n                          int, # a function that accepts a single argument and \n                               # raises a `ValueError` if the value is not valid.\n                               # Here checks that \"key\" is an integer.\n                          'EX1', # code for exception\n                          'key must be an integer'# message to report if error thrown\n                         )\nvalidator.add_value_check('date', \n                          # check for a date with the sepcified format\n                          datetime_string('%Y-%m-%d'), \n                          'EX2',\n                          'invalid date'\n                         )\nvalidator.add_value_check('units_sold',\n                          # makes sure the number of units sold is an integer\n                          int,\n                          'EX3',\n                          'number of units sold not an integer'\n                         )\nvalidator.add_value_check('store', \n                          # check that the \"store\" field only has\n                          # \"store1\" and \"store2\" in it\n                          enumeration('store1', 'store2'),\n                          'EX4', \n                          'store name not recognized')","f85fa9a9":"import csv\nfrom io import StringIO\n\n# this is example of validating a good dataset that's set\n# up the way we want it to be\n\n# sample csv\ngood_data = StringIO(\"\"\"key,date,unit_name,units_sold,store,notes\n1,2018-12-01,product1,5,store1,\"\"\n2,2018-12-01,product2,200,store2,\"Big day!\"\n3,2018-12-04,product1,2,store1,\"\"\n\"\"\")\n\n# read text in as a csv\ntest_csv = csv.reader(good_data)\n\n# validate our good csv\nvalidator.validate(test_csv)","57e988c6":"# and this one is an example of a dataset with a lot\n# of problems, which throws a lot of helpful exceptions\n\n# sample csv\nbad_data = StringIO(\"\"\"key,date,unit_name,units,store\n1,2018-12-01,product1,5,store1\n2,2018-12-01,product2,two hundred,store2\n2.5,12-04-2018,product1,2,store3\n\"\"\")\n\n# read text in as a csv\ntest_csv = csv.reader(bad_data)\n\n# validate our bad csv (generates a lot of errors)\nvalidator.validate(test_csv)","15d42b4e":"# we'll be using the testing functions build into pandas\nimport pandas as pd\n\n# function to do some data cleaning \n# (adopted from Dinara Sultangulova's dashboad:\n# https:\/\/www.kaggle.com\/oftomorrow\/dashboarding-with-notebooks-ny-collisions)\ndef process_data(raw_data):\n    processed_data = raw_data[raw_data.BOROUGH.notnull()]\\\n    .filter(items=['BOROUGH','NUMBER OF PERSONS INJURED','NUMBER OF PERSONS KILLED'])\\\n    .groupby(['BOROUGH'], as_index=False)\\\n    .sum()\n    \n    return(processed_data)","a1f7506c":"# testing data to pass into our function (based on \n# slice from data function was originally written\n# for)\ntest_data = StringIO(\"\"\"BOROUGH,ZIP CODE,LATITUDE,LONGITUDE,LOCATION,NUMBER OF PERSONS KILLED,NUMBER OF PERSONS INJURED\n    QUEENS,11362,40.76272,-73.72816999999999,\"{\\'longitude\\': \\'-73.72817\\', \\'latitude\\': \\'40.76272\\', \\'needs_recoding\\': False}\",0,0\n    BROOKLYN,11211,40.710196999999994,-73.95843,\"{\\'longitude\\': \\'-73.95843\\', \\'latitude\\': \\'40.710197\\', \\'needs_recoding\\': False}\",1,3\n    BRONX,10454,40.803554999999996,-73.91184,\"{\\'longitude\\': \\'-73.91184\\', \\'latitude\\': \\'40.803555\\', \\'needs_recoding\\': False}\",0,1\n    BROOKLYN,11221,40.694922999999996,-73.915565,\"{\\'longitude\\': \\'-73.915565\\', \\'latitude\\': \\'40.694923\\', \\'needs_recoding\\': False}\",0,2\"\"\")\ntest_csv = pd.read_csv(test_data)\n\n\n# hand built example with what we expect\n# our function to output given the test data\noutput_data_correct = StringIO(\"\"\"BOROUGH,NUMBER OF PERSONS INJURED,NUMBER OF PERSONS KILLED\n    BRONX,1,0\n    BROOKLYN,5,1\n    QUEENS,0,0\"\"\")\noutput_csv_correct = pd.read_csv(output_data_correct)\n\n# check to see if the test dataframe & the\n# dataframe our function prouduces are the same\n# (this will produce no output)\npd.testing.assert_frame_equal(output_csv_correct, process_data(test_csv))","9c6c7a25":"# now a dataframe that has an error in it\noutput_data_errors = StringIO(\"\"\"BOROUGH,NUMBER OF PERSONS INJURED,NUMBER OF PERSONS KILLED\n    BRONX,0,0\n    BROOKLYN,5,1\n    QUEENS,0,b\"\"\")\noutput_csv_errors = pd.read_csv(output_data_errors)\n\n# check to see if the test dataframe & the\n# dataframe our function produces are the same\n# (this will raise an error!)\npd.testing.assert_frame_equal(output_csv_errors, process_data(test_csv))","f22d85fe":"Great! But we should probably check that our validator actually works. Here I'm going to create a little test dataset for myself. \n\nNotice that the test dataset has an extra column in it called \"notes\". Because I haven't told my validator to look for it or that it should have any specific qualities, it's just ignored. \n\n> **You should only validate data you actually use in your analysis.** If you validate data you don't actually need to rely on, you'll introduce more places for your validation to break without actually making your pipeline more robust.","bf8492c7":"Just an example, let's also look at a bad dataset that will throw up some exceptions. ","d29ff82e":"____\n\n* **Day 1**: Determining what information should be monitored with a dashboard. [Notebook](https:\/\/www.kaggle.com\/rtatman\/dashboarding-with-notebooks-day-1), [Livestream Recording](https:\/\/www.youtube.com\/watch?v=QO2ihJS2QLM)\n* **Day 2**: How to create effective dashboards in notebooks, [Python Notebook](https:\/\/www.kaggle.com\/rtatman\/dashboarding-with-notebooks-day-2-python), [R Notebook](https:\/\/www.kaggle.com\/rtatman\/dashboarding-with-notebooks-day-2-r), [Livestream](https:\/\/www.youtube.com\/watch?v=rhi_nexCUMI)\n* **Day 3**: Running notebooks with the Kaggle API, [Notebook](https:\/\/www.kaggle.com\/rtatman\/dashboarding-with-notebooks-day-3), [Livestream](https:\/\/youtu.be\/cdEUEe2scNo)\n* **Day 4**: Scheduling notebook runs using cloud services, [Notebook](https:\/\/www.kaggle.com\/rtatman\/dashboarding-with-notebooks-day-4), [Livestream](https:\/\/youtu.be\/Oujj6nT7etY)\n* **Day 5**: Testing and validation, [Python Notebook](), [R Notebook](), [Livestream](https:\/\/www.youtube.com\/watch?v=H6DcpIykT8E)\n\n____\n\n\nWelcome to the fifth and final day of Dashboarding with scheduled notebooks. Today we\u2019re doing two things:\n\n* Validating that our data has the same format we expect it to have.\n* Writing tests to make sure that our data processing pipeline is doing what we think it is\n\nToday's timeline:\n\n* **5 minutes**: Read notebook\n* **10 minutes**: Set up data validation for your notebook\n* **5 minutes**: Set up tests for your notebook\n___\n\n# Why use testing and validation?\n\nTesting and validation do take some time to set up and it doesn\u2019t always make sense to go to all the trouble of getting them set up. I personally use the following list of criteria to decide whether I think it\u2019s a good idea to spend time setting up testing and validation. \n\n* Will this code be run repeatedly?\n* Will this code be run without supervision (i.e. scheduled)?\n* Would bad things happen if there were mistakes in this code output that no one noticed?\n\nIf the answer to these questions are \u201cyes\u201d, then it\u2019s probably a good idea to test your code and validate your data. I know a lot of data scientists (my included!) don\u2019t have a lot of software engineering background, so testing can be a little intimidating. That\u2019s why I\u2019ve included this nice, gentle walkthrough in this event. \n\nIn this notebook I\u2019m going to walk through testing and validation using Python code. If you\u2019re an R person, head over to the R notebook using this link: \n\n## [I\u2019m an R user, show me the R code, please!](https:\/\/www.kaggle.com\/rtatman\/dashboarding-with-notebooks-day-5-r)\n\n___\n\n# Validating data\n\nValidating data means checking to make sure that the assumptions you\u2019ve made about the format and contents of your data are correct. This is especially important if you\u2019re relying on data someone else is updating; you want to make sure the updates they\u2019ve made aren\u2019t going to break your processing pipeline!\n\nTo validate .csv files in Python, you can use the [csvvalidator package](https:\/\/pypi.org\/project\/csvvalidator\/). There are a lot more options for validating .json files, which I won't walk through in this notebook. If you're interested, though, you can check out a [tutorial on how to use the Cerberus library here](http:\/\/docs.python-cerberus.org\/en\/stable\/usage.html). \n\n> **Hold on, why aren't you talking about validating .json files?** It's more common for software engineers to work with .json than .csv files, so there are a lot more resources on how to validate .json data than .csv data out there. I'm only talking about .csv's here because I don't want this activity to take too long and I think it\u2019s more valuable to talk about .csv's than write a slightly different copy of a tutorial that a lot of people have already done.\n\nSince the csvvalidator package isn't included in our default Docker image, the first thing we'll need to do is install it.","c9689fa9":"\nAs you can see, the error messages we wrote earlier really help us understand what's wrong here.\n\nYou can write much more complex data validation if you need it, but this example should be enough to get you started on validating .csv data. :)\n\n___\n\n# Testing data processing\n\nNow that you've validated that your data is in the format that you assume it is, it's time to test your data processing pipeline. For this, I'm going to be using black box testing with a small sample dataset. \n\n> **Black box testing** means that you test the output of your code given a specific input. This means that you're not validating *how* your data processing is making the changes it is, just that they're the right changes. I find this useful for data processing pipelines because it means I don't have to rewrite tests if I refactor my processing code for some reason. \n\n\nFirst, however, we'll need something to test! I've written a function for doing some data pre-processing. (Borrowed heavily from [Dinara Sultangulova's dashboard looking at NYPD Motor Vehicle Collisions](https:\/\/www.kaggle.com\/oftomorrow\/dashboarding-with-notebooks-ny-collisions)!)","7ae935d5":"As you can see, the function I wrote doesn't produce output that's the same as the `output_data_errors` dataframe, so I get a lot of errors. \n\n____\n\n# Your turn!\n\nNow that you've seen some examples of data validation and testing, it's time for you to try your own hand at it. \n\n* Validate the fields and the contents of at least one of the columns in a .csv that's being used as input to a dashboard. If you didn't use a .csv you can fork a dashboard that did, like [this one by Dan Cripe](https:\/\/www.kaggle.com\/ecodan\/dashboard-course-seattle-incidents\/data).\n* If you haven't already, put at least part of your data processing into a function. Then test that function using small example input and output .csv's. \n\nFeel free to share links to your notebooks (I love seeing y'all's work!) or share your favorite testing and validation tips in the comments.","6a1330ca":"Next, I create two custom dataframes based on the NYPD Motor Vehicle Collisions dataset. The first is a slightly-modified slice from the dataset, and the second is the output I expect from my function given that sample dataset as input.(If you're interested in trying out [test driven developement](https:\/\/en.wikipedia.org\/wiki\/Test-driven_development), you can even make these little sample dataframes before writing your function!)","81091cbf":"Now that it's installed, we can get to work setting up our validation. Here, I'm assuming that you're working with a .csv of sales data that has fields called \"key\", \"date\", \"unit_name\", \"units_sold\" and \"store\". I'm going to check that my data has: \n\n* An integer for the key. (If I wanted to write fancier validation, I could also check that the key values were unique.)\n* Dates of the format Year-Month-Day in the date column.\n* An integer for number of units sold\n* Stores in the \"store\" column that are part of my list of known stores (here called \"store1\" and \"store2\"). \n\nLet's look at the code for doing that:","883603e4":"Great! I didn't get any errors, which means my function is doing what I expect it to be doing. Just as an example, however, I'm also going to create a dataframe that should result in errors as well:"}}