{"cell_type":{"9ded4f63":"code","e5d466bf":"code","f0bc17ab":"code","6378b2d5":"code","4312d238":"code","d4123a5a":"code","d48ccb1a":"code","7f476ceb":"code","fe25412e":"code","bba08d28":"code","e58554c8":"code","1b3b31c2":"code","fde518bb":"code","48b4bf8e":"code","f765400e":"code","acd6a7ce":"code","b94602d8":"code","14f6ef68":"code","8fedc810":"code","026e6c44":"markdown","d880d416":"markdown","71698f14":"markdown","4fc9ea83":"markdown","28ab0f6f":"markdown","d087d2a4":"markdown","0a9d7b6a":"markdown","68152206":"markdown"},"source":{"9ded4f63":"import sys\nsys.path.append('..\/input\/timm-pytorch-image-models\/pytorch-image-models-master')","e5d466bf":"# Import libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport path\nimport random\nimport cv2\nimport timm\nimport gc\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n# Import PyTorch Libraries\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\n\n# Deciding the device used for calculation. CUDA = GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","f0bc17ab":"csv_dir = '..\/input\/petfinder-pawpularity-score'\ntest_dir = '..\/input\/petfinder-pawpularity-score\/test'\n\ntest_file_path = os.path.join(csv_dir, 'test.csv')\nsample_sub_file_path = os.path.join(csv_dir, 'sample_submission.csv')\nprint(f'Test file: {test_file_path}')","6378b2d5":"test_df = pd.read_csv(test_file_path)\nsample_df = pd.read_csv(sample_sub_file_path)","4312d238":"def return_filpath(name, folder):\n    path = os.path.join(folder, f'{name}.jpg')\n    return path","d4123a5a":"test_df['image_path'] = test_df['Id'].apply(lambda x: return_filpath(x, folder=test_dir))","d48ccb1a":"test_df.head()","7f476ceb":"target = ['Pawpularity']\nnot_features = ['Id', 'kfold', 'image_path', 'Pawpularity']\ncols = list(test_df.columns)\nfeatures = [feat for feat in cols if feat not in not_features]\nprint(features)","fe25412e":"params = {\n    'model': 'swin_large_patch4_window12_384',\n    'features': features,\n    'pretrained': False,\n    'inp_channels': 3,\n    'im_size': 384,\n    'device': device,\n    'batch_size': 8,\n    'num_workers' : 2,\n    'out_features': 1,\n    'debug': False\n}","bba08d28":"if params['debug']:\n    test_df = test_df.sample(frac=0.1)","e58554c8":"def get_test_transforms(DIM = params['im_size']):\n    return albumentations.Compose(\n        [\n          albumentations.Resize(DIM,DIM),\n          albumentations.Normalize(\n              mean=[0.485, 0.456, 0.406],\n              std=[0.229, 0.224, 0.225],\n          ),\n          ToTensorV2(p=1.0)\n        ]\n    )","1b3b31c2":"class CuteDataset(Dataset):\n    def __init__(self, images_filepaths, dense_features, targets, transform=None):\n        self.images_filepaths = images_filepaths\n        self.dense_features = dense_features\n        self.targets = targets\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images_filepaths)\n\n    def __getitem__(self, idx):\n        image_filepath = self.images_filepaths[idx]\n        image = cv2.imread(image_filepath)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.transform is not None:\n            image = self.transform(image=image)['image']\n        \n        dense = self.dense_features[idx, :]\n        label = torch.tensor(self.targets[idx]).float()\n        return image, dense, label","fde518bb":"class PetNet(nn.Module):\n    def __init__(self, model_name=params['model'], pretrained=params['pretrained'], features=len(params['features']) ):\n        super().__init__()\n        self.model = timm.create_model(model_name=model_name, pretrained=pretrained, in_chans=3)\n        # Replaced the final head layers in model with our own Linear layer\n        num_features = self.model.head.in_features\n        self.model.head = nn.Linear(num_features, 128)\n        self.fully_connect = nn.Sequential(nn.Linear(128 + features, 64),\n                                           nn.ReLU(),\n                                           nn.Linear(64, 1)\n                                          )\n        self.dropout = nn.Dropout(p=0.5)\n    \n    def forward(self, image, features):\n        x = self.model(image)\n        # Using dropout functions to randomly shutdown some of the nodes in hidden layers to prevent overfitting.\n        x = self.dropout(x)\n        # Concatenate the metadata into the results.\n        x = torch.cat([x, features], dim=1)\n        output = self.fully_connect(x)\n        return output","48b4bf8e":"# class PetNet2(nn.Module):\n#     def __init__(self, model_name=\"tf_efficientnet_b0_ns\", pretrained=False, features=len(params['features']) ):\n#         super().__init__()\n#         self.model = timm.create_model(model_name=model_name, pretrained=pretrained, in_chans=3)\n#          # Replace the classifier layers in model with our own Linear layer\n#         num_features = self.model.classifier.in_features\n#         self.model.classifier = nn.Linear(num_features, 128)\n#         self.fully_connect = nn.Sequential(nn.Linear(128 + features, 64),\n#                                            nn.ReLU(),\n#                                            nn.Linear(64, 1)\n#                                           )\n#         self.dropout = nn.Dropout(p=0.4)\n    \n#     def forward(self, image, features):\n#         x = self.model(image)\n#         x = self.dropout(x)\n#         x = torch.cat([x, features], dim=1)\n#         output = self.fully_connect(x)\n#         return output\n","f765400e":"predictions_nn = None\nfor model_name in range(4):\n    model = PetNet()\n    model.load_state_dict(torch.load(f\"..\/input\/swin-transformer-3rd-model\/swin_large_patch4_window12_384_epoch_f{model_name}.pth\"))\n    model.eval()\n    model.to(device)\n\n    test_dataset = CuteDataset(\n        images_filepaths = test_df['image_path'].values,\n        dense_features = test_df[params['features']].values,\n        targets = sample_df['Pawpularity'].values,\n        transform = get_test_transforms()\n    )\n    test_loader = DataLoader(\n        test_dataset, batch_size=params['batch_size'],\n        shuffle=False, num_workers=params['num_workers'],\n        pin_memory=True\n    )\n\n    temp_preds = None\n    with torch.no_grad():\n        for (images, dense, target) in tqdm(test_loader, desc=f'Predicting. '):\n            images = images.to(device)\n            dense = dense.to(device)\n            predictions = torch.sigmoid(model(images, dense)).to('cpu').numpy()*100\n            \n            if temp_preds is None:\n                temp_preds = predictions\n            else:\n                temp_preds = np.vstack((temp_preds, predictions))\n            print(temp_preds)\n\n    if predictions_nn is None:\n        predictions_nn = temp_preds\n    else:\n        predictions_nn += temp_preds\n        \npredictions_nn \/= 4\nprint(predictions_nn)","acd6a7ce":"predictions_nn_2 = None\nfor model_name in range(8):\n    model = PetNet()\n    model.load_state_dict(torch.load(f\"..\/input\/swin-transform-2nd-model\/swin_large_patch4_window12_384_epoch_f{model_name}.pth\"))\n    model.eval()\n    model.to(device)\n\n    test_dataset = CuteDataset(\n        images_filepaths = test_df['image_path'].values,\n        dense_features = test_df[params['features']].values,\n        targets = sample_df['Pawpularity'].values,\n        transform = get_test_transforms()\n    )\n    test_loader = DataLoader(\n        test_dataset, batch_size=params['batch_size'],\n        shuffle=False, num_workers=params['num_workers'],\n        pin_memory=True\n    )\n\n    temp_preds = None\n    with torch.no_grad():\n        for (images, dense, target) in tqdm(test_loader, desc=f'Predicting. '):\n            images = images.to(device)\n            dense = dense.to(device)\n            predictions = torch.sigmoid(model(images, dense)).to('cpu').numpy()*100\n            \n            if temp_preds is None:\n                temp_preds = predictions\n            else:\n                temp_preds = np.vstack((temp_preds, predictions))\n            print(temp_preds)\n\n    if predictions_nn_2 is None:\n        predictions_nn_2 = temp_preds\n    else:\n        predictions_nn_2 += temp_preds\n        \npredictions_nn_2 \/= 8\nprint(predictions_nn_2)","b94602d8":"sub_df = pd.DataFrame()\nsub_df['Id'] = test_df['Id']\nsub_df['Pawpularity'] = (predictions_nn + predictions_nn_2) \/ 2\nprint(sub_df['Pawpularity'])","14f6ef68":"sub_df.head()","8fedc810":"sub_df.to_csv('submission.csv', index=False)","026e6c44":"# CNN Model","d880d416":"# Prediction","71698f14":"# Submission","4fc9ea83":"# Pretrained SWIN Transformer (Inference)\n\n**Description:** Millions of stray animal suffer on the streets or euthanized in shelters every day around the world. A good picture of homeless animal might increase their chance of getting adopted. But what makes a good picture? Our mission is to build a ML model which is able to accurately determine a pet photo's appeal and even suggest improvements to give these rescue animals a higher chance of loving homes.\n\nThis competition is organized by PetFinder.my. They are Malaysia's leading animal welfare platform, featuring 180,000 animals with 54,000 happily adopted. If we can developed a model that able to provide accurate recommendations, our model will be adapted into AI tools that will guide shelters and rescuers around the world to improve the photo quality of their shelter pet. Which in the end will increase the chances of stray animals getting adopted.\n\n**Data:** 9912 images of pet animals labeled with \"Pawpularity\". Photo Metadata = (Focus, Eyes, Face, Near, Action, Accessory, Group, Collage, Human, Occlusion, Info, Blur)\n\n![](https:\/\/pbs.twimg.com\/media\/CvhLlXxXgAA5TDJ.jpg)\n","28ab0f6f":"# Augmentations","d087d2a4":"# Params","0a9d7b6a":"# Introduction\n\nThis notebook is to run model that I have trained in other notebook. Please refer the the baseline notebook link for details.\n\nBaseline notebook: \n\nRefered notebook: https:\/\/www.kaggle.com\/manabendrarout\/transformers-classifier-method-starter-infer","68152206":"# Dataset"}}