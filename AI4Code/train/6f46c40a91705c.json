{"cell_type":{"ac5e072a":"code","11612bec":"code","e3d40907":"code","e92c4108":"code","b29b745d":"code","baded515":"code","4829557d":"code","a8abc3ce":"code","e50a4923":"code","738b3bce":"code","bbfa8d13":"code","6058d6c0":"code","936899d0":"code","9a0a9c5d":"code","e7eb2318":"code","7d1add7b":"markdown","3b2f55c2":"markdown","d76a1ae6":"markdown","83249096":"markdown","339d02ed":"markdown"},"source":{"ac5e072a":"from transformers import AutoModelForQuestionAnswering, AutoModel, AutoConfig, get_linear_schedule_with_warmup\nfrom transformers.optimization import AdamW\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport os\nfrom itertools import compress","11612bec":"from torch.utils.data import DataLoader\nfrom functools import partial\nfrom tokenizers import BertWordPieceTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom fastai.core import *\nfrom fastai.text import *","e3d40907":"file_dir, electra_dir = [Path(f'\/kaggle\/input\/{i}') for i in ['tweet-sentiment-extraction', 'electrabase']]\n\ntrain_df = pd.read_csv(file_dir\/'train.csv')\ntrain_df['text'] = train_df['text'].apply(lambda x: str(x))\ntrain_df['sentiment'] = train_df['sentiment'].apply(lambda x: str(x))\ntrain_df['selected_text'] = train_df['selected_text'].apply(lambda x: str(x))\n\ntest_df = pd.read_csv(file_dir\/'test.csv')\ntest_df['text'] = test_df['text'].apply(lambda x: str(x))\ntest_df['sentiment'] = test_df['sentiment'].apply(lambda x: str(x))","e92c4108":"max_len = 128\nbs = 64\ntokenizer = BertWordPieceTokenizer(str(electra_dir\/'vocab.txt'), lowercase=True)","b29b745d":"def preprocess(sentiment, tweet, selected, tokenizer, max_len):\n    _input = tokenizer.encode(sentiment, tweet)\n    _span = tokenizer.encode(selected, add_special_tokens=False)\n    \n    len_span = len(_span.ids)\n    start_idx = None\n    end_idx = None\n    \n    for ind in (i for i, e in enumerate(_input.ids) if e == _span.ids[0]):\n        if _input.ids[ind: ind + len_span] == _span.ids:\n            start_idx = ind\n            end_idx = ind + len_span - 1\n            break\n    \n    # Handles cases where Wordpiece tokenizing input & span separately produces different outputs\n    if not start_idx:\n        idx0 = tweet.find(selected)\n        idx1 = idx0 + len(selected)\n        \n        char_targets = [0] * len(tweet)\n        if idx0 != None and idx1 != None:\n            for ct in range(idx0, idx1):\n                char_targets[ct] = 1\n                \n        tweet_offsets = list(compress(_input.offsets, _input.type_ids))[0:-1]\n        \n        target_idx = []\n        for j, (offset1, offset2) in enumerate(tweet_offsets):\n            if sum(char_targets[offset1: offset2]) > 0:\n                target_idx.append(j)\n                \n        start_idx, end_idx = target_idx[0] +3 , target_idx[-1] + 3\n        \n    _input.start_target = start_idx\n    _input.end_target = end_idx\n    _input.tweet = tweet\n    _input.sentiment = sentiment\n    _input.selected = selected\n    \n    _input.pad(max_len)\n    \n    return _input","baded515":"def reduce_loss(loss, reduction='mean'):\n    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss    \n\nclass LabelSmoothingCrossEntropy(nn.Module):\n    def __init__(self, \u03b5:float=0.1, reduction='mean'):\n        super().__init__()\n        self.\u03b5,self.reduction = \u03b5,reduction\n    \n    def forward(self, output, target):\n        c = output.size()[-1]\n        log_preds = F.log_softmax(output, dim=-1)\n        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n        return torch.lerp(nll, loss\/c, self.\u03b5) ","4829557d":"class TweetDataset(Dataset):\n    def __init__(self, dataset, test = None):\n        self.df = dataset\n        self.test = test\n        \n    def __getitem__(self, idx):\n        if not self.test:\n            sentiment, tweet, selected = (self.df[col][idx] for col in ['sentiment', 'text', 'selected_text'])\n            _input = preprocess(sentiment, tweet, selected, tokenizer, max_len)\n            \n            yb = [torch.tensor(_input.start_target), torch.tensor(_input.end_target)]\n            \n        else:\n            _input = tokenizer.encode(self.df.sentiment[idx], self.df.text[idx])\n            _input.pad(max_len)\n            \n            yb = 0\n\n        xb = [torch.LongTensor(_input.ids),\n              torch.LongTensor(_input.attention_mask),\n              torch.LongTensor(_input.type_ids),\n              np.array(_input.offsets)]\n\n        return xb, yb     \n\n    def __len__(self):\n        return len(self.df)","a8abc3ce":"pt_model = AutoModel.from_pretrained(electra_dir)","e50a4923":"class SpanModel(nn.Module):\n    def __init__(self,pt_model):\n        super().__init__()\n        self.model = pt_model\n        self.drop_out = nn.Dropout(0.5)\n        self.qa_outputs1c = torch.nn.Conv1d(768*2, 128, 2)\n        self.qa_outputs2c = torch.nn.Conv1d(768*2, 128, 2)\n        self.qa_outputs1 = nn.Linear(128, 1)\n        self.qa_outputs2 = nn.Linear(128, 1)\n        \n#         self.qa_outputs = nn.Linear(768 * 2, 2) # update hidden size\n\n    # could pass offsets here and not use - can grab in last_input\n    def forward(self, input_ids, attention_mask, token_type_ids, offsets = None):\n        \n        _, hidden_states = self.model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n\n        out = torch.cat((hidden_states[-1], hidden_states[-2]), dim=-1)\n        out = self.drop_out(out)\n        out = torch.nn.functional.pad(out.transpose(1,2), (1, 0))\n        \n        out1 = self.qa_outputs1c(out).transpose(1,2)\n        out2 = self.qa_outputs2c(out).transpose(1,2)\n\n        start_logits = self.qa_outputs1(self.drop_out(out1)).squeeze(-1)\n        end_logits = self.qa_outputs2(self.drop_out(out2)).squeeze(-1)\n        return start_logits, end_logits","738b3bce":"class CELoss(Module):\n    def __init__(self, loss_fn = nn.CrossEntropyLoss()): \n        self.loss_fn = loss_fn\n        \n    def forward(self, inputs, start_targets, end_targets):\n        start_logits, end_logits = inputs # assumes tuple input\n        \n        logits = torch.cat([start_logits, end_logits]).contiguous()\n        \n        targets = torch.cat([start_targets, end_targets]).contiguous()\n        \n        return self.loss_fn(logits, targets)","bbfa8d13":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","6058d6c0":"# Note that validation ds is by default not shuffled in fastai - so indexing like this will work for Callback\n# https:\/\/forums.fast.ai\/t\/how-to-set-shuffle-false-of-train-and-val\/33730\n\nclass JaccardScore(Callback):\n    \"Stores predictions and targets to perform calculations on epoch end.\"\n    def __init__(self, valid_ds): \n        self.valid_ds = valid_ds\n        self.context_text = valid_ds.df.text\n        self.answer_text = valid_ds.df.selected_text\n        \n    def on_epoch_begin(self, **kwargs):\n        self.jaccard_scores = []  \n        self.valid_ds_idx = 0\n        \n        \n    def on_batch_end(self, last_input:Tensor, last_output:Tensor, last_target:Tensor, **kwargs):\n              \n        input_ids = last_input[0]\n        attention_masks = last_input[1].bool()\n        token_type_ids = last_input[2].bool()\n        offsets = last_input[3]\n\n        start_logits, end_logits = last_output\n        \n        # for id in batch of ids\n        for i in range(len(input_ids)):\n            \n            _offsets = offsets[i]\n            start_idx, end_idx = torch.argmax(start_logits[i]), torch.argmax(end_logits[i])\n            _answer_text = self.answer_text[self.valid_ds_idx]\n            original_start, original_end = _offsets[start_idx][0], _offsets[end_idx][1]\n            pred_span = self.context_text[self.valid_ds_idx][original_start : original_end]\n                \n            score = jaccard(pred_span, _answer_text)\n            self.jaccard_scores.append(score)\n\n            self.valid_ds_idx += 1\n            \n    def on_epoch_end(self, last_metrics, **kwargs):        \n        res = np.mean(self.jaccard_scores)\n        return add_metrics(last_metrics, res)","936899d0":"model = SpanModel(pt_model)\ntr_df, val_df = train_test_split(train_df, test_size = 0.2, random_state = 42)\ntr_df, val_df  = [df.reset_index(drop=True) for df in [tr_df, val_df]]\ntrain_ds, valid_ds = [TweetDataset(i) for i in [tr_df, val_df]]\n\ntest_ds = TweetDataset(test_df, test = True)\nloss_fn = partial(CELoss, LabelSmoothingCrossEntropy())\n\ndata = DataBunch.create(train_ds, valid_ds, test_ds, path=\".\", bs = bs)\nlearner = Learner(data, model, loss_func = loss_fn(), path = electra_dir\/'electra-conv', model_dir=f\".\")","9a0a9c5d":"preds = []\ntest_df_idx = 0\n\nwith torch.no_grad():\n    \n    for xb,yb in tqdm(learner.data.test_dl):\n        model0 = learner.load(f'electra_conv_0').model.eval()\n        start_logits0, end_logits0 = to_cpu(model0(*xb))\n        start_logits0, end_logits0 = start_logits0.float(), end_logits0.float()\n        \n        model1 = learner.load(f'electra_conv_1').model.eval()\n        start_logits1, end_logits1 = to_cpu(model1(*xb))\n        start_logits1, end_logits1 = start_logits1.float(), end_logits1.float()\n        \n        model2 = learner.load(f'electra_conv_2').model.eval()\n        start_logits2, end_logits2 = to_cpu(model2(*xb))\n        start_logits2, end_logits2 = start_logits2.float(), end_logits2.float()\n        \n        model3 = learner.load(f'electra_conv_3').model.eval()\n        start_logits3, end_logits3 = to_cpu(model3(*xb))\n        start_logits3, end_logits3 = start_logits3.float(), end_logits3.float()\n        \n        model4 = learner.load(f'electra_conv_4').model.eval()\n        start_logits4, end_logits4 = to_cpu(model4(*xb))\n        start_logits4, end_logits4 = start_logits4.float(), end_logits4.float()\n        \n        input_ids = to_cpu(xb[0])\n        attention_masks = to_cpu(xb[1].bool())\n        token_type_ids = to_cpu(xb[2].bool())\n        offsets = to_cpu(xb[3])\n        \n        start_logits = (start_logits0 + start_logits1 + start_logits2 + start_logits3 + start_logits4) \/ 5\n        end_logits = (end_logits0 + end_logits1 + end_logits2 + end_logits3 + end_logits4) \/ 5\n        \n        for i in range(len(input_ids)):\n            \n            _offsets = offsets[i]\n#             start_idx, end_idx = torch.argmax(start_logits[i]), torch.argmax(end_logits[i])\n\n            start_idx, end_idx = get_best_start_end_idxs(start_logits[i], end_logits[i])\n            original_start, original_end = _offsets[start_idx][0], _offsets[end_idx][1]\n            pred_span = test_ds.df.text[test_df_idx][original_start : original_end]\n            preds.append(pred_span)\n            test_df_idx += 1","e7eb2318":"test_df['selected_text'] = preds\ntest_df['selected_text'] = test_df.apply(lambda o: o['text'] if len(o['text']) < 3 else o['selected_text'], 1)\nsubdf = test_df[['textID', 'selected_text']]\nsubdf.to_csv(\"submission.csv\", index=False)","7d1add7b":"## Overview\n\nSharing a notebook on pre-processing Electra with Fastai.  \nI was really excited about fine-tuning Electra for this competition & was my first approach.  \nIt didn't really outperform Roberta for me in the end.\n\nI took a lot inspiration from these posts:\n\nhttps:\/\/www.kaggle.com\/keremt\/tse-transformers-q-a-with-fastai-training  \nhttps:\/\/www.kaggle.com\/abhishek\/roberta-inference-5-folds  \nhttps:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/151878\n\nSo, if this notebook is interesting - please look at those too  ","3b2f55c2":"I had fun trying to create my 'own' preprocessing pipeline  \n\nI thought this was an 'efficient' way to tokenize - but maybe not in the end  \nWordPiece tokenizing cases like `gonna` vs `onna` will give input ids, which is why there are two start\/end idx loops  \n\nElectra uses the same vocab & tokenizer as Bert","d76a1ae6":"## Electra\n\n![](https:\/\/1.bp.blogspot.com\/-CYdmy-CavFQ\/XmfA7gPlDxI\/AAAAAAAAFaw\/QV_m8U6m82wKIJOV6XckhBRrET_DOTqawCEwYBhgLKskDAMBZVoDkAXgBeYLK0UlPs8KTLC000nVXlb_Vf0ViyBhl7daowMj46skeP5iBS15xmM9P2Y7l6GgUXKwATCbwSw5j3fIuStwsfGXY4eDyE_xP-dk4vYEkjn-6EoBR8bj-yTxWRmVLl3QKpt9xQyMVEv4Fq4Lq5xNshTm2UuGOfrntf_83f7CGsEaAQYtY33St2jCXTL-MMUZohpk9AZaIvZsHtvYElDJ8k0JmVJYGukD73BFnibrv-dw4dBolN8aOtcrIBgn7D-IkUW1bul0_Fzzmhf6L410ngtGsA3HR8PGFGUL8Xv0Unl2lW58Sv2bNeJSFLnyyTsp5d63vorjL9NnMPJgFtIRor1t0-_1XJIk9e1NRUDkn63CPlFjkl1LGYjbc_cEgyMhX431SM8RBE-AqTbR1I7tpu-IkbIzpmfVCxqQSYW1rPhDzn9T6dXT3GxI7mHy7AKfuRDj24nT5cj3Fx8opS5ygDDa582vdxmxIR8pz1Ly3lzKKPGdVVqZhhIlXKGJXpOnsEeELFEyPoq95SwmHG-BBK07_tyR0qySY3wI3Ks-r5v4YoYVw88GZ0JW1V5_jgcBQV5TOTZLGR7aZK4NV9TwcZTC2hJ_zBQ\/s1600\/image4.gif)\n\nElectra's a recent pre-training objective for Transformer models. \n\nIt's a self-supervised pre-training task, which involves training both a generator and discrimator transformer.   \n\nThe discriminator performs **replaced token detection** and is the model you generally fine-tune on downstream tasks.  \n\nThere's an excellent post on Electra and Transformer pre-training [here](https:\/\/ai.googleblog.com\/2020\/03\/more-efficient-nlp-model-pre-training.html)\n","83249096":"Note that in the Model's forward I pass an optional `offsets` parameter.  \n\nI did this to experiment with callbacks, but it's not standard\/necessary.  \nValidation DataLoaders aren't shuffled by default in Fastai, so you can grab the offsets differently.","339d02ed":"LabelSmoothingCrossEntropy by fastai - changed slightly to use `torch.lerp`"}}