{"cell_type":{"a4f40caa":"code","6bb54f38":"code","1800cac4":"code","ce2fb864":"code","dff7eced":"code","26d22cb0":"code","2e97b01e":"code","8ec8a383":"code","8a2a40a0":"code","e609cb46":"code","c416226b":"code","375f1262":"code","d1313c1a":"code","0f9e9567":"code","c2ab7148":"code","5d2e6851":"code","8f28a575":"code","07102caa":"code","eb5bf3eb":"code","f05c89c4":"markdown","1dd40a19":"markdown","53f6fc38":"markdown","9f12be33":"markdown","f06fc630":"markdown","5c3cef25":"markdown","dfbe5011":"markdown","9199049d":"markdown","7fba4964":"markdown","86e8fe94":"markdown","0b2d7b4d":"markdown","f2ac7852":"markdown","f93cce8e":"markdown","2ac46475":"markdown","6d3182bd":"markdown","1ed05d8e":"markdown","58047c1a":"markdown","500e835c":"markdown","494141b3":"markdown"},"source":{"a4f40caa":"import torch\n\n# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n    !nvidia-smi\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","6bb54f38":"!pip install pyarabic\n!pip install emoji\n!pip install pystemmer\n!pip install optuna==2.3.0\n!pip install transformers==4.2.1","1800cac4":"import numpy as np\nimport pandas as pd\nimport pyarabic.araby as ar\n\nimport re , emoji, Stemmer, functools, operator, string\nimport torch , optuna, gc, random, os\n\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\nfrom transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\nfrom transformers.data.processors import SingleSentenceClassificationProcessor\nfrom transformers import Trainer , TrainingArguments\nfrom transformers.trainer_utils import EvaluationStrategy\nfrom transformers.data.processors.utils import InputFeatures\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom sklearn.utils import resample\n\nimport logging\n\nlogging.basicConfig(level=logging.WARNING)\nlogger = logging.getLogger(__name__)","ce2fb864":"st =  Stemmer.Stemmer('arabic')\ndef data_cleaning (text):\n  text = re.sub(r'^https?:\\\/\\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n  text = re.sub(r'^http?:\\\/\\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n  text = re.sub(r\"http\\S+\", \"\", text)\n  text = re.sub(r\"https\\S+\", \"\", text)\n  text = re.sub(r'\\s+', ' ', text)\n  text = re.sub(\"(\\s\\d+)\",\"\",text) \n  text = re.sub(r\"$\\d+\\W+|\\b\\d+\\b|\\W+\\d+$\", \"\", text)\n  text = re.sub(\"\\d+\", \" \", text)\n  text = ar.strip_tashkeel(text)\n  text = ar.strip_tatweel(text)\n  text = text.replace(\"#\", \" \");\n  text = text.replace(\"@\", \" \");\n  text = text.replace(\"_\", \" \");\n  translator = str.maketrans('', '', string.punctuation)\n  text = text.translate(translator)\n  em = text\n  em_split_emoji = emoji.get_emoji_regexp().split(em)\n  em_split_whitespace = [substr.split() for substr in em_split_emoji]\n  em_split = functools.reduce(operator.concat, em_split_whitespace)\n  text = \" \".join(em_split)\n  text = re.sub(r'(.)\\1+', r'\\1', text)\n  text_stem = \" \".join([st.stemWord(i) for i in text.split()])\n  text = text +\" \"+ text_stem\n  text = text.replace(\"\u0622\", \"\u0627\")\n  text = text.replace(\"\u0625\", \"\u0627\")\n  text = text.replace(\"\u0623\", \"\u0627\")\n  text = text.replace(\"\u0624\", \"\u0648\")\n  text = text.replace(\"\u0626\", \"\u064a\")\n   \n  return text","dff7eced":"# Define the variables needed:\n\nUse_Train_Extended_Data = True #set False if using orginal training set with not feedback from test1\n\nTweets_Ids_Col_Train =\"Tweet_id\"\nTweets_Text_Col_Train = \"Text\"\nTweets_Sentiment_Col_Train = \"sentiment\"\nTrain_Data_File = \"..\/input\/train-files\/train_all.csv\"\nTrain_Data_Extended_File = \"..\/input\/train-files\/train_all_ext.csv\"\n\ntrain_data = pd.DataFrame()\n\nif Use_Train_Extended_Data :\n  train_data = pd.read_csv(Train_Data_Extended_File, sep=\",\")\nelse :\n  train_data = pd.read_csv(Train_Data_File, sep=\",\")\n\nprint(train_data[Tweets_Sentiment_Col_Train].value_counts())\nprint(train_data.value_counts())","26d22cb0":"# Cleaning Training Data \ntrain_data[Tweets_Text_Col_Train] = train_data[Tweets_Text_Col_Train].apply(lambda x:   data_cleaning(x))\n\n# Removing un-needed feilds\nif Tweets_Ids_Col_Train in train_data.columns:\n  del train_data[Tweets_Ids_Col_Train]\ntrain_data.columns = [Tweets_Sentiment_Col_Train,Tweets_Text_Col_Train]\n\ntrain_data[Tweets_Text_Col_Train].head(50)","2e97b01e":"# First setting the max_len , will be useful later for BERT Model\nExtra_Len = 6 # an extra padding in length , found to be useful for increasing F-score\nMax_Len = train_data[Tweets_Text_Col_Train].str.split().str.len().max() + Extra_Len\nprint(Max_Len)\n\n#Spliting the Training data\nTest_Size = 0\nif Use_Train_Extended_Data :\n  Test_Size = 0.001  # low percentage to keep the training data as large as possible,\n                     # the value 0.001 found to be best for F-Score with extended data\nelse :\n  Test_Size = 0.0005 # low percentage to keep the training data as large as possible,\n                     # the value 0.0005 found to be best for F-Score with original data\nRand_Seed = 42 \n\ntrain_set, evaluation_set = train_test_split( train_data, test_size= Test_Size, random_state= Rand_Seed)\n\nprint(\"Train set: \")\nprint(train_set[Tweets_Sentiment_Col_Train].value_counts())\nprint(\"---------------------------\")\nprint (\"Evaluation set: \")\nprint (evaluation_set[Tweets_Sentiment_Col_Train].value_counts())\n\n","8ec8a383":"# preparing test_data (which will be sumbitted to kaggle)\n\nTweets_Ids_Col_Test = \"Tweet_id\"\nTweets_Text_Col_Test = \"Text\"\nTest_Data_File = \"..\/input\/test-files\/test1_with_text.csv\"\n\ntest_data = pd.read_csv(Test_Data_File, sep=\",\")\ntest_data.columns = [Tweets_Ids_Col_Test,Tweets_Text_Col_Test]\n\ntest_data[Tweets_Text_Col_Test] = test_data[Tweets_Text_Col_Test].apply(lambda x:   data_cleaning(x))\ntest_data[Tweets_Text_Col_Test].head(50)","8a2a40a0":"Model_Used = \"UBC-NLP\/MARBERT\"\nTask_Name = \"classification\"\n\nclass Dataset:\n    def __init__(\n        self,\n        name,\n        train,\n        test,\n        label_list,\n    ):\n        self.name = name\n        self.train = train\n        self.test = test\n        self.label_list = label_list\n        \nclass BERTModelDataset(Dataset):\n    def __init__(self, text, target, model_name, max_len, label_map):\n      super(BERTModelDataset).__init__()\n      self.text = text\n      self.target = target\n      self.tokenizer_name = model_name\n      self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n      self.max_len = max_len\n      self.label_map = label_map\n  \n    def __len__(self):\n      return len(self.text)\n\n    def __getitem__(self,item):\n      text = str(self.text[item])\n      text = \" \".join(text.split())\n    \n      encoded_review = self.tokenizer.encode_plus(\n      text,\n      max_length= self.max_len,\n      add_special_tokens= True,\n      return_token_type_ids=False,\n      pad_to_max_length=True,\n      truncation='longest_first',\n      return_attention_mask=True,\n      return_tensors='pt'\n    )\n      input_ids = encoded_review['input_ids'].to(device)\n      attention_mask = encoded_review['attention_mask'].to(device)\n\n      return InputFeatures(input_ids=input_ids.flatten(), attention_mask=attention_mask.flatten(), label=self.label_map[self.target[item]])","e609cb46":"def model_init():\n  return AutoModelForSequenceClassification.from_pretrained(Model_Used, return_dict=True, num_labels=len(label_map))\n\ndef compute_metrics(p): #p should be of type EvalPrediction\n  preds = np.argmax(p.predictions, axis=1)\n  assert len(preds) == len(p.label_ids)\n  print(classification_report(p.label_ids,preds))\n  #print(confusion_matrix(p.label_ids,preds))\n\n  macro_f1_pos_neg = f1_score(p.label_ids,preds,average='macro',labels=[1,2])\n  macro_f1 = f1_score(p.label_ids,preds,average='macro')\n  macro_precision = precision_score(p.label_ids,preds,average='macro')\n  macro_recall = recall_score(p.label_ids,preds,average='macro')\n  acc = accuracy_score(p.label_ids,preds)\n  return {\n      'macro_f1' : macro_f1,\n      'macro_f1_pos_neg' : macro_f1_pos_neg,  \n      'macro_precision': macro_precision,\n      'macro_recall': macro_recall,\n      'accuracy': acc\n  }\n\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n","c416226b":"\n\nlabel_list = list(train_set[Tweets_Sentiment_Col_Train].unique())\n\nprint(label_list)\nprint(train_set[Tweets_Sentiment_Col_Train].value_counts())\n\ndata_set = Dataset( \"KAUST\", train_set, evaluation_set, label_list )\n\nlabel_map = { v:index for index, v in enumerate(label_list) }\nprint(label_map)\n\ntrain_dataset = BERTModelDataset(train_set[Tweets_Text_Col_Train].to_list(),\n                                 train_set[Tweets_Sentiment_Col_Train].to_list(),Model_Used,Max_Len,label_map)\n\nevaluation_dataset = BERTModelDataset(evaluation_set[Tweets_Text_Col_Train].to_list(),\n                                      evaluation_set[Tweets_Sentiment_Col_Train].to_list(),Model_Used,Max_Len,label_map)\n\n","375f1262":"#define training arguments\ntraining_args = TrainingArguments(\".\/train\")\ntraining_args.lr_scheduler_type = 'cosine'\ntraining_args.evaluate_during_training = True\ntraining_args.adam_epsilon =1e-8 \nif Use_Train_Extended_Data :\n    training_args.learning_rate = 1.215e-05 # use this with extended data\nelse:\n    training_args.learning_rate = 1.78255000000000001e-05 # use this with org data  \ntraining_args.fp16 = True\ntraining_args.per_device_train_batch_size = 16 #64 \ntraining_args.per_device_eval_batch_size = 16 # 64 \ntraining_args.gradient_accumulation_steps = 2\ntraining_args.num_train_epochs= 2\ntraining_args.warmup_steps = 0 \ntraining_args.evaluation_strategy = EvaluationStrategy.EPOCH\ntraining_args.logging_steps = 200\ntraining_args.save_steps = 100000 \ntraining_args.seed = 42 \ntraining_args.disable_tqdm = False","d1313c1a":"training_args.dataloader_pin_memory = False\ngc.collect()\ntorch.cuda.empty_cache()\nset_seed(Rand_Seed) \n\ntrainer = Trainer(\n    model = model_init(),\n    args = training_args,\n    train_dataset = train_dataset,\n    eval_dataset= evaluation_dataset,\n    compute_metrics=compute_metrics\n)\n\nprint(training_args.seed)","0f9e9567":"print(Max_Len)\nprint(training_args.learning_rate)\nprint(training_args.adam_epsilon)\nprint(training_args.warmup_steps)\n#wandbkey if needed (depend on the transformers package version) = 0a58b374c46a154de1ba77c8634c6be279a9dcdb\ntrainer.train()","c2ab7148":"# first define the predection method\ndef predict(text, tokenizer):\n \n  encoded_review = tokenizer.encode_plus(\n    text,\n    max_length=Max_Len,\n    add_special_tokens=True,\n    return_token_type_ids=False,\n    pad_to_max_length=True, #True,\n    truncation='longest_first',\n    return_attention_mask=True,\n    return_tensors='pt'\n  )\n\n  input_ids = encoded_review['input_ids'].to(device) #(input_ids + ([tokenizer.pad_token_id] * padding_length)).to(device)  \n  attention_mask = encoded_review['attention_mask'].to(device)\n    \n\n  output = trainer.model(input_ids, attention_mask)\n  _, prediction = torch.max(output[0], dim=1)\n  return prediction[0]\n\n#then lets play !\n\ntokenizer = AutoTokenizer.from_pretrained(Model_Used)\n\nprediction_list = []\ni = 0\nfor tweet in test_data[Tweets_Text_Col_Test]:\n    id = test_data[Tweets_Ids_Col_Test][i]\n  \n    pre = predict(tweet,tokenizer)\n    pre_txt = label_list[pre]\n   \n    if pre_txt == 'positive': pre_txt = 1\n    if pre_txt == 'negative': pre_txt = -1\n    if pre_txt == 'neutral': pre_txt = 0\n    prediction_list.append(pre_txt)\n    \n    i = i + 1","5d2e6851":"\n#print(prediction_list)\nresults = pd.DataFrame({'Tweet_id' : test_data[Tweets_Ids_Col_Test].astype(str), 'sentiment' : prediction_list},\n                       columns = ['Tweet_id', 'sentiment'])\nprint(results)\n\n\nos.chdir(r'\/kaggle\/working')\nresult_file = \"sub_test3.csv\"\nresults.to_csv(result_file, sep= \",\", index = False)\n","8f28a575":"from IPython.display import FileLink\nFileLink(r'sub_test3.csv')","07102caa":"# Do not run, unless you need to generate the train_all_ext.csv again.\n\nimport pandas as pd\nimport numpy as np\nres = pd.read_csv('..\/input\/train-extend-by-test\/'+'sub_1.csv', sep =\",\", dtype={'Tweet_id': np.int64})\nfiles_to_train = ['sub_2.csv', 'sub_3.csv', 'sub_4.csv', 'sub_5.csv', 'sub_6.csv',\n                  'sub_7.csv', 'sub_8.csv', 'sub_9.csv', 'sub_10.csv', 'sub_11.csv',\n                  'sub_12.csv', 'sub_13.csv', 'sub_14.csv', 'sub_15.csv', 'sub_16.csv',\n                  'sub_17.csv', 'sub_18.csv', 'sub_19.csv', 'sub_20.csv','sub_21.csv',\n                  'sub_22.csv', 'sub_23.csv','sub_24.csv']\nfor i in files_to_train:\n  t = pd.read_csv('..\/input\/train-extend-by-test\/'+i, sep =\",\", dtype={'Tweet_id': np.int64})\n  res = pd.merge(res, t, on='Tweet_id', how='inner')\n\n\nres['Tweet_id'] = res['Tweet_id'] .astype(str)\nres['total'] = res.sum(numeric_only=True , axis=1)\nres.head()\nzero = 0\npositive_limit = len(files_to_train) + 1\nnegative_limit =  -1 * positive_limit\n\nprint(positive_limit)\nprint(negative_limit)\n\nres = res.loc[(res['total'] == positive_limit) | \n            (res['total'] == negative_limit) | \n            (res['total'] == zero)]\n\ntest1_file = pd.read_csv('..\/input\/test-files\/test1_with_text.csv', sep =\",\", dtype={'Tweet_id': str})\n\nres = pd.merge(res , test1_file, on='Tweet_id', how='inner')\n\n\nres = res [['Tweet_id','total','Text']]\nres['total'] = res['total'].map({positive_limit:'positive',\n                              negative_limit: 'negative',\n                              zero: 'neutral'})\nres = res.rename(columns={'total': 'sentiment'})\n\ntrain_org = pd.read_csv('..\/input\/train-files\/train_all.csv', sep =\",\", dtype={'Tweet_id': str})\n\nprint(len(res))\ntrain_ext = train_org.append(res, ignore_index = True)\ntrain_ext['sentiment'].value_counts()\n\n# result_file = \"train_all_ext.csv\"\n# res.to_csv(result_file, sep= \",\", index = False)\n\nos.chdir(r'\/kaggle\/working')\nresult_file = \"train_all_ext_demo.csv\"\nresults.to_csv(result_file, sep= \",\", index = False)\n\nprint(results.count())","eb5bf3eb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"..\/input\/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f05c89c4":"**Part 11:** Build Train and Evaluation Data Sets","1dd40a19":"**Arabic Sentiment Analysis 2021 @ KAUST**\n\n**Author of this file:** Ali Salhi ( [http:\/\/asalhi.info\/](http:\/\/) )\n\nMain Idea: **Using MARBERT with Test Set Feedback & improved data cleaning process (appending stems as extra text)**\n\n**Best Scores (Macro F-Score)**\n* On Orginal training set : **0.79220** (Using **train_all.csv**)\n* On Extended training set (by using test1 set) : **0.79624**  (best score achivied during competition: **0.79349**) , file: **train_all_ext.csv**\n  \n![](https:\/\/i.ibb.co\/r3PKXsS\/Screen-Shot-2021-06-08-at-3-38-10.png)  \n  \n  Note1: Extending mechanisim is explained later within this notebook (Check Appindex 1 at the end)\n  \n  Note2: \n  The code runs best with best predictions on this colab url:\n  https:\/\/colab.research.google.com\/drive\/1o2ozOTrQayqpykkkC4it-X9RH7DZEJPD?usp=sharing\n  \n  Kaggle gives slightly less F-Score than colab, proppley related to packages versions.\n  \n**BERT Model used** :\n\n[https:\/\/huggingface.co\/UBC-NLP\/MARBERT](http:\/\/)\n \n  \n**Main Refrences:** \n- Abdul-Mageed, M., Elmadany, A., & Nagoudi, E. M. B. (2020). ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic. arXiv preprint arXiv:2101.01785.\n\n- Antoun, W., Baly, F., & Hajj, H. (2020). Arabert: Transformer-based model for arabic language understanding. arXiv preprint arXiv:2003.00104.\n\n**Acknowledgement**\n\nThank you Wissam Antoun, Mohamed Al Salti & Fady Baly for the great code examples at\nhttps:\/\/github.com\/aub-mind\/arabert\n\n\n\n\n     ","53f6fc38":"**Part 12:** Definge Training Arguments","9f12be33":"**Part 16:** Save the prediction ","f06fc630":"**Part 4:** Define cleaning method","5c3cef25":"**Part 7:** Spliting Training Data (Train , Evaluation) ","dfbe5011":"**Part One:**\nInstalling needed packages","9199049d":"Part 13: Build Trainer","7fba4964":"**Part 5:** Prepare Training Set","86e8fe94":"**Part 10:** Defining Needed Methods for training and evaluation","0b2d7b4d":"**Part 6:** Cleaning Traing Data","f2ac7852":"**Part 9:** Preparing BERTModel Classes","f93cce8e":"**Appindex 1 : Creating the Extended Training dataset from test1**\n\nThe extended data set which pushes the score from 0.79220 to 0.79624 was generated as follow:\n\n1. I selected X learning rates (X = 24 currently).\n2. I fixed the batch size to 32 and test size to 0.1 on the orginal training data \n3. Seed was 42 with warmup_steps to zero. \n4. For X (X = 24) times I ran the above code and generated a submission file (sub_<num>.csv).\n5. The files can be found on \"train-extend-by-test\" folder.\n6. The following table shows the files and thier local results (not tested on test1) only evaluation.\n\n![](https:\/\/i.ibb.co\/F6G8HGS\/Screen-Shot-2021-06-08-at-2-35-11.png)    \n   \n7. The following chart shows a range of the learing rates and thier results on test1\n![](https:\/\/i.ibb.co\/8YpP8vj\/Screen-Shot-2021-06-08-at-3-22-53.png)   \n \n8. The 24 files then were merged to filter the tweets with same result in all of them (for example if negative then the tweet will score -24 (-1 * 24 times) , if positive the tweet will score 24 (1 * 24) , if neutral the tweet will score 0 (0 * 24) , any other value means that there was a change in the result on one or more learning rates, and that tweet will be droped.\n    \n9. A total of 18614 tweets out of 20000 in test1 had either a score of 24 or -24 or 0, those tweets were appended to the orginal training set to generate the train_all_ext set. \n    \n10. Below is the code that was used to do so.\n\n\n \n    ","2ac46475":"**Part Zero:**\nGPU Or CPU ?","6d3182bd":"**Part Two:** Import needed packages","1ed05d8e":"**Part 15:** Lets Predict on test1 data ! ","58047c1a":"**Part 8:** Preparing Test Data","500e835c":"Colab version scores: 0.79624\n\nKaggle version scors : 0.79581 (sub_test3.csv) ","494141b3":"**Part 14:** Train ! "}}