{"cell_type":{"7d085118":"code","ccb5c108":"code","a6b0851a":"code","159504fb":"code","0e5a065d":"code","553a2a59":"code","9b5db605":"code","12acb9f0":"code","a744addc":"code","b1bde332":"code","0098d1a8":"code","3a1d25f3":"code","2b8521fc":"code","79944cc4":"code","bef93bd7":"code","a9c78129":"code","c71dd255":"code","226ee6ff":"code","591cfa71":"code","fd586b7f":"code","ec1aba15":"code","92268911":"code","961cb278":"code","1d982c25":"code","b9d137f8":"code","837c51f6":"code","371f235f":"code","bc6b763a":"code","eb0a63ed":"code","40ff08e8":"code","baf67c6a":"code","6e8e30c6":"code","06b66c0b":"code","d1f8e238":"markdown","7f7754f4":"markdown","c9136138":"markdown","bab4f6ec":"markdown","b0c4e094":"markdown","f75d10b2":"markdown","938af9c6":"markdown","ee8b3eb8":"markdown","33d03ff4":"markdown","e3cd136d":"markdown","dd682130":"markdown","69b04cce":"markdown","e3994a1a":"markdown","62595344":"markdown","c06e4724":"markdown","080f33cc":"markdown","adaefbf8":"markdown","e9aa3b0a":"markdown","881aae30":"markdown","b247783c":"markdown","51d5b48f":"markdown"},"source":{"7d085118":"## Necessary Libraries\nimport pandas as pd\nimport numpy as np","ccb5c108":"## --- FUNCTIONS --\n## a function to display cross validation scores \ndef display_scores(scores):\n    print(f\"\"\"Scores: {scores},\nMean: {scores.mean()},\nStandard Deviation: {scores.std()}\n    \"\"\"  \n    )\n    \n## a function to compute age by PClass\ndef get_age_by_class(pclass):\n    if pclass == 1: return 38\n    elif pclass == 2:return 30\n    elif pclass == 3:return 25","a6b0851a":"## loading the training data\ntrain_df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntrain_df.head()","159504fb":"train_df.info()","0e5a065d":"train_df.isnull().sum()","553a2a59":"train_df[\"Embarked\"].value_counts()","9b5db605":"train_df[\"Embarked\"].fillna(\"S\", inplace=True)","12acb9f0":"train_df[\"Age\"] = train_df.apply(lambda row: get_age_by_class(row[\"Pclass\"]) if np.isnan(row[\"Age\"]) else row[\"Age\"], axis=1)","a744addc":"req_cols = [1,2,4,5, 6, 7, 9, 11]\n\ntrain_df = train_df.iloc[:,req_cols]\n\ntrain_df.head()","b1bde332":"train_df.isnull().sum()","0098d1a8":"from sklearn.preprocessing import OneHotEncoder\n\ncat_encoder = OneHotEncoder()","3a1d25f3":"cat_attribs = [\"Sex\", \"Embarked\"]\n\ncat_enc = cat_encoder.fit_transform(train_df[cat_attribs])\n\ncat_df = pd.DataFrame(cat_enc.toarray(), columns=list(cat_encoder.categories_[0])+list(cat_encoder.categories_[1]))\n\ncat_df.head()","2b8521fc":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()","79944cc4":"num_attribs = [\"Age\", \"Fare\"]\n\nnum_enc = scaler.fit_transform(train_df[num_attribs])\n\nnum_df = pd.DataFrame(num_enc, columns=[\"Age_std\", \"Fare_std\"])\n\nnum_df.head()","bef93bd7":"dfs = [train_df, cat_df, num_df]\n\ntrain_df = dfs[0].join(dfs[1:]).iloc[:,[0,1,4,5,8,10,11,13,14]]\n\ntrain_df.head()","a9c78129":"## splitting the data\nX_train = train_df.drop(\"Survived\", axis=1)\n\ny_train = train_df[\"Survived\"]","c71dd255":"## loading required models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier","226ee6ff":"## creating a dictionary of model and\n## their respective parameter grid\nmodel_params = {\n    \"Logistic Regressor\":{\n        \n        \"model\": LogisticRegression(),\n        \"params\":{\"C\":[0.001, 0.01, 0.1, 1, 10, 100, 1000], \"penalty\":[\"l2\"]}\n        \n    },\n    \"Decision Tree Classfier\":{\n        \n        \"model\": DecisionTreeClassifier(),\n        \"params\":{'criterion':['gini','entropy'],\n                  'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}\n    },\n    \"Random Forest Classifier\":{\n        \"model\": RandomForestClassifier(),\n        \"params\": { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\n    },\n    \"K Neighbors Classifier\":{\n        \"model\": KNeighborsClassifier(),\n        \"params\":{\"n_neighbors\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, \n                                  16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]}\n    }\n}","591cfa71":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\nfor model_name, mp in model_params.items():\n    cur_model = mp[\"model\"]\n       \n    cur_model.fit(X_train, y_train)\n    \n    y_pred = cur_model.predict(X_train)\n    \n    cur_mse = mean_squared_error(y_train, y_pred)\n    cur_rmse = np.sqrt(cur_mse)\n    \n    print(model_name, cur_mse, \"\\n\")\n    \n    scores = cross_val_score(cur_model, X_train, y_train, cv=5,\n                            scoring=\"neg_mean_squared_error\")\n    \n    cur_rmse_scores = np.sqrt(-scores)\n    \n    display_scores(cur_rmse_scores)\n    \n    print(\"----------------------------------------------\")","fd586b7f":"## finding best hyperparameter\nfrom sklearn.model_selection import GridSearchCV\n\nscores = {}\nfor model_name, mp in  model_params.items():\n    cur_grid_search = GridSearchCV(mp[\"model\"], mp[\"params\"], \n                                cv=5, return_train_score=True,\n                              scoring=\"neg_mean_squared_error\")\n                          \n    cur_grid_search.fit(X_train, y_train)\n    \n    scores[model_name] = {\"best_param\":cur_grid_search.best_params_,\n                          \"best_score\":cur_grid_search.best_score_}\n    print(f\"{model_name} ----------------------------------------------------------\")","ec1aba15":"pd.DataFrame.from_dict(scores, orient=\"index\").sort_values(by=\"best_score\", ascending=False)","92268911":"final_model = RandomForestClassifier(criterion = 'gini', max_depth= 8,\n                                    max_features = 'log2', n_estimators = 200)\n\nfinal_model.fit(X_train, y_train)","961cb278":"### testing final model\ny_pred = final_model.predict(X_train)\n    \nfinal_mse = mean_squared_error(y_train, y_pred)\nfinal_rmse = np.sqrt(final_mse)\n\nprint(f\"Random Forest Classifier:{final_mse}\\n\")\n\nscores = cross_val_score(final_model, X_train, y_train, cv=5,\n                        scoring=\"neg_mean_squared_error\")\n\nfinal_rmse_scores = np.sqrt(-scores)\n\ndisplay_scores(final_rmse_scores)","1d982c25":"test_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\ntest_df.head()","b9d137f8":"test_df.info()","837c51f6":"test_df.isnull().sum()","371f235f":"import matplotlib.pyplot as plt\n\ntest_df[\"Fare\"].hist()\n\nplt.xlabel(\"Fare\")\nplt.ylabel(\"count\")\nplt.title(\"Fare Distribution\")\n\nplt.show()","bc6b763a":"## imputing missing data\ntest_df[\"Fare\"].fillna(test_df[\"Fare\"].median(), inplace=True)\ntest_df[\"Age\"] = test_df.apply(lambda row: get_age_by_class(row[\"Pclass\"]) if np.isnan(row[\"Age\"]) else row[\"Age\"], axis=1)","eb0a63ed":"cat_enc = cat_encoder.fit_transform(test_df[cat_attribs])\ncat_df = pd.DataFrame(cat_enc.toarray(), columns=list(cat_encoder.categories_[0])+list(cat_encoder.categories_[1]))\n\nnum_enc = scaler.fit_transform(test_df[num_attribs])\nnum_df = pd.DataFrame(num_enc, columns=[\"Age_std\", \"Fare_std\"])\n\ndfs = [test_df, cat_df, num_df]\ntest_df = dfs[0].join(dfs[1:])\n\ntest_df.head()","40ff08e8":"X_test = test_df[train_df.columns.tolist()[1:]]","baf67c6a":"final_pred = final_model.predict(X_test)\n\nfinal_pred[:10]","6e8e30c6":"final_df = pd.concat([test_df[\"PassengerId\"], pd.Series(final_pred)], axis=1, keys=[\"PassengerId\",\"Survived\"])\n\nfinal_df.head()","06b66c0b":"## saving the data\nfinal_df.to_csv(\".\/kaggle\/working\/\", index=False)","d1f8e238":"### Standardizing numerical data\nAccording to [humans-of-data.atlan.com](https:\/\/humansofdata.atlan.com\/2018\/12\/data-standardization\/#:~:text=Data%20standardization%20is%20about%20making,friend%20went%20to%20different%20universities.),  \n&nbsp;&nbsp;&nbsp;&nbsp;Data standardization is about making sure that data is internally consistent; that is, each data type has the same content and format.\n\n#### Standard Scaler:  \n&nbsp;&nbsp;&nbsp;&nbsp;StandardScaler removes the mean and scales each feature\/variable to unit variance. This operation is performed feature-wise in an independent way\n","7f7754f4":"## Model Building and Performance","c9136138":"## Data Preprocesssing","bab4f6ec":"Now that we have handled the missing data, let's remove unwanted columns.","b0c4e094":"### Handling missing data\nLet's see percentage of null values in each column","f75d10b2":"Now let's combine all of this and create our final training data set.  \n\nNote that we will be considering only required columns.","938af9c6":"### Final model","ee8b3eb8":"We are going to replace the nan values in *Age* by using the Pclass(as we have seen in the exploration phase).","33d03ff4":"To find the answer to our question let's load the test data.","e3cd136d":"Let's impute the feature *Embarked* as it has a very low amount of null values.  \n\nAs it is categorical, we will be using the most frequent catgeory. ","dd682130":"Here our encoded categories much better to train a machine learning model.  ","69b04cce":"> ### Insights\n- As we can see that it is largely right skewed and hence we are replace it with meadian.","e3994a1a":"> ### Insights\n- There are very low non null values in column *cabin*.\n- whereas there are very few null values in the feature *Age*.   ","62595344":"Seems like Random Forest Classifier is the best so far. Let's go with it ","c06e4724":"&nbsp;&nbsp;&nbsp;&nbsp;In the previous notebook [*Exploring the Training Data*](https:\/\/www.kaggle.com\/suhruthyambakam\/exploring-the-training-data\/), I have explored the [titanic train data](https:\/\/www.kaggle.com\/c\/titanic\/data?select=train.csv) in R programming language. We figured out percentage of missing values, plotted basic visualizations, imputed the missing data (removed some rows), multivariate plots,...  \n\n&nbsp;&nbsp;&nbsp;&nbsp;In this notebook we are going to prepare, train , validate and test the titanic data and predict the people who survived.","080f33cc":"## Finally, Who survived?","adaefbf8":"Let's see the Fare distribution","e9aa3b0a":"As you can see there are no na values anymore.","881aae30":"> ### Insights\n&nbsp;&nbsp;&nbsp;&nbsp;The *Decision Tree Classifier* and  the *Random Tree Classifier* has great scores. ","b247783c":"> ### Assumptions\n- As *cabin* has way more na values, we are not going to use this variable in the model building.\n- We are assuming that the variables *Name* and *Ticket* are also not of use.","51d5b48f":"### Encoding categorical data\nAcoording to [Analytics Vidhya](https:\/\/www.analyticsvidhya.com\/blog\/2020\/08\/types-of-categorical-data-encoding\/),  \n&nbsp;&nbsp;&nbsp;&nbsp;Since most machine learning models only accept numerical variables, preprocessing the categorical variables becomes a necessary step. We need to convert these categorical variables to numbers such that the model is able to understand and extract valuable information.\n\n#### One Hot Encoding:  \n&nbsp;&nbsp;&nbsp;&nbsp;In one hot encoding, for each level of a categorical feature, each category is mapped with a binary variable containing either 0 or 1. Here, 0 represents the absence, and 1 represents the presence of that category.\n\nThese newly created binary features are known as **Dummy variables**. The number of dummy variables depends on the levels present in the categorical variable. "}}