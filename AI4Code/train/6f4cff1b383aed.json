{"cell_type":{"14ccfdb4":"code","4db6109f":"code","c9bc1326":"code","9d13bc65":"code","451ec48c":"code","42547346":"code","7f0c0c4e":"code","5b623e13":"code","e4a4d37b":"code","b7c56284":"code","0c48de0b":"code","58eb343d":"code","038e8a21":"code","840c02f5":"code","b2719974":"code","e5fb006d":"code","f106be9e":"code","e9f9378e":"code","72df061e":"markdown","0fb54608":"markdown","32200ade":"markdown","7cc231f4":"markdown"},"source":{"14ccfdb4":"# https:\/\/www.kaggle.com\/aerdem4\/optiver-pytorch-gpu-no-feature-eng\n\ndef nn_notebook():\n    import numpy as np\n    import pandas as pd\n    import glob\n    from tqdm import tqdm\n    import sys, os\n\n\n    def load_data(mode, path=\"\/kaggle\/input\/optiver-realized-volatility-prediction\"):\n        # mode = \"train\"\/\"test\"\n        file_name = f'{path}\/{mode}.csv'\n        return pd.read_csv(file_name)\n\n    df = load_data(\"test\")\n    \n    SCALE = 100\n    PATH = \"\/kaggle\/input\/optiver-realized-volatility-prediction\"\n\n    order_book_paths = glob.glob(f'{PATH}\/book_test.parquet\/*\/*')\n    trade_paths = glob.glob(f'{PATH}\/trade_test.parquet\/*\/*')\n\n    \n    order_books = dict()\n    for path in tqdm(order_book_paths):\n        stock_id = int(path.split(\"=\")[1].split(\"\/\")[0])\n        book_df = pd.read_parquet(path)\n        books_by_time = dict()\n\n        for time_id in book_df.time_id.unique():\n            books_by_time[time_id] = book_df[book_df[\"time_id\"] == time_id].reset_index(drop=True)\n\n        order_books[stock_id] = books_by_time\n    \n    trades = dict()\n    for path in tqdm(trade_paths):\n        stock_id = int(path.split(\"=\")[1].split(\"\/\")[0])\n        trade_df = pd.read_parquet(path)\n        trade_by_time = dict()\n\n        for time_id in trade_df.time_id.unique():\n            trade_by_time[time_id] = trade_df[trade_df[\"time_id\"] == time_id].reset_index(drop=True)\n\n        trades[stock_id] = trade_by_time\n        \n    import torch\n    import torch.nn as nn\n    from torch.utils.data import DataLoader, Dataset\n\n\n    means_order = torch.FloatTensor([  0.9997,   1.0003, 769.9902, 766.7346,   0.9995,   1.0005, 959.3417,\n            928.2203, 300])\n    stds_order = torch.FloatTensor([3.6881e-03, 3.6871e-03, 5.3541e+03, 4.9549e+03, 3.7009e-03, 3.6991e-03,\n            6.6838e+03, 5.7353e+03, 300])\n\n    means_trade = torch.FloatTensor([300, 1.0, 100, 3.0])\n    stds_trade = torch.FloatTensor([300, 0.004, 153, 3.5])\n\n\n\n    class OptiverDataset(Dataset):\n\n        def __init__(self, df, aug=False):\n            super().__init__()\n            self.df = df.reset_index(drop=True)\n            self.aug = aug\n            self.seq_len = 600\n            self.order_features = ['bid_price1', 'ask_price1', 'bid_size1', 'ask_size1','bid_price2', \n                             'ask_price2', 'bid_size2', 'ask_size2', \"seconds_in_bucket\"]\n            self.trade_features = [\"seconds_in_bucket\", \"price\", \"size\", \"order_count\"]\n\n\n        def extract_features(self, data_dict, stock_id, time_id, features, means, stds):\n            X = -torch.ones((self.seq_len, len(features)))\n            try:\n                df = data_dict[stock_id][time_id]\n                feature_array = df[features].values\n                X[-feature_array.shape[0]:] = (torch.FloatTensor(feature_array) - means)\/stds\n            except:\n                pass\n            return X\n\n\n        def __getitem__(self, index):\n            row = self.df.iloc[index]\n\n            X1 = self.extract_features(order_books, row.stock_id, row.time_id, self.order_features,\n                                      means_order, stds_order)\n            try:\n                X2 = self.extract_features(trades, row.stock_id, row.time_id, self.trade_features,\n                                          means_trade, stds_trade) \n            except:\n                X2 = -torch.ones((self.seq_len, len(self.trade_features)))\n            target = torch.FloatTensor([0.0])\n            stock = torch.LongTensor([row.stock_id])\n            return X1, X2, stock, target\n\n        def __len__(self):\n            return self.df.shape[0]\n\n    ds = OptiverDataset(df)\n    \n    class ConvBlock(nn.Module):\n        def __init__(self, in_dim, out_dim, kernel_size, stride=1):\n            super().__init__()\n            self.lin = nn.Conv1d(in_dim, out_dim, kernel_size, stride=stride)\n            self.bn = nn.BatchNorm1d(out_dim)\n            self.activation = nn.ReLU()\n\n        def forward(self, x):\n            x = self.lin(x)\n            x = self.bn(x)\n            return self.activation(x)\n\n\n    class SubModel(nn.Module):\n        def __init__(self, in_dim):\n            super().__init__()\n            self.convs1 = nn.Sequential(ConvBlock(in_dim, 16, 3),\n                                       ConvBlock(16, 32, 3))\n            self.stock_conv = ConvBlock(36, 64, 4, stride=4)\n            self.avg_pool = nn.AdaptiveAvgPool1d(8)\n            self.max_pool = nn.AdaptiveMaxPool1d(8)\n            self.convs2 = nn.Sequential(ConvBlock(128, 128, 2, stride=2),\n                                        ConvBlock(128, 32, 2, stride=2),\n                                        ConvBlock(32, 8, 2, stride=2))\n\n        def forward(self, x, s):\n            x = self.convs1(x.transpose(2, 1))\n            x = self.stock_conv(torch.cat([x, s.repeat(1, 1, x.shape[2])], axis=1))\n            x = torch.cat([self.avg_pool(x), self.max_pool(x)], axis=1)\n            x = self.convs2(x).squeeze(-1)\n            return x\n\n\n    class Model(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.order_model = SubModel(in_dim=9)\n            self.trade_model = SubModel(in_dim=4)\n            self.top = nn.Linear(16, 1)\n            self.stock_emb = nn.Embedding(127, 4)\n\n        def forward(self, inputs):\n            x1, x2, s = inputs\n            s = self.stock_emb(s).transpose(2, 1)\n\n            x1 = self.order_model(x1, s)\n            x2 = self.trade_model(x2, s)\n            x = self.top(torch.cat([x1, x2], axis=1))\n            return x\n        \n    def read_data(data):\n        return tuple(d.cuda() for d in data[:-1]), data[-1].cuda()\n\n    def inference(model, loader, num_folds=5):\n        model.eval()\n\n        tbar = tqdm(loader, file=sys.stdout)\n\n        preds = []\n\n        model_weights = {i: torch.load(f\"\/kaggle\/input\/optiver-nn\/optiver_nn_v01_{i}.pth\") for i in range(num_folds)}\n\n        with torch.no_grad():\n            for idx, data in enumerate(tbar):\n                inputs, target = read_data(data)\n\n                model.load_state_dict(model_weights[0])\n                pred = model(inputs)\/num_folds\n                for i in range(1, num_folds):\n                    model.load_state_dict(model_weights[i])\n                    pred += model(inputs)\/num_folds\n\n\n                preds.append(pred.detach().cpu().numpy().ravel())\n\n        return np.concatenate(preds)\n\n    NW = 4\n    BS = 256\n    loader = DataLoader(ds, batch_size=BS, shuffle=False, num_workers=NW, pin_memory=False, drop_last=False)\n\n\n    model = Model()\n    model = model.cuda()\n\n    y = inference(model, loader)\n    \n    df[\"nn_pred\"] = np.clip(y, 0.0, None)\/SCALE\n\n    df.to_csv(\"nn_preds.csv\", index=False, columns=[\"stock_id\", \"time_id\", \"nn_pred\"])\nnn_notebook()","4db6109f":"import cupy as cp\nimport cudf\nimport cuml\nimport glob\nfrom tqdm import tqdm\n\ncudf.__version__","c9bc1326":"PATH = \"\/kaggle\/input\/optiver-realized-volatility-prediction\"\n\n\ndef load_data(mode, path=\"\/kaggle\/input\/optiver-realized-volatility-prediction\"):\n    # mode = \"train\"\/\"test\"\n    file_name = f'{path}\/{mode}.csv'\n    return cudf.read_csv(file_name)\n\ndev_df = load_data(\"train\", path=PATH)\ndev_df.head()","9d13bc65":"SCALE = 100\ndev_df[\"target\"] *= SCALE\n\nstock_ids = dev_df[\"stock_id\"].unique()\nlen(stock_ids)","451ec48c":"order_book_training = glob.glob(f'{PATH}\/book_train.parquet\/*\/*')\norder_book_test = glob.glob(f'{PATH}\/book_test.parquet\/*\/*')\n\nlen(order_book_training), len(order_book_test)","42547346":"trades_training = glob.glob(f'{PATH}\/trade_train.parquet\/*\/*')\ntrades_test = glob.glob(f'{PATH}\/trade_test.parquet\/*\/*')\n\nlen(trades_training), len(trades_test)","7f0c0c4e":"%cd \/kaggle\/input\/rapids-kaggle-utils\/","5b623e13":"import cu_utils.transform as cutran\n\n\n\ndef log_diff(df, in_col, null_val):\n    df[\"logx\"] = df[in_col].log()\n    df[\"logx_shifted\"] = (df[[\"time_id\", \"logx\"]].groupby(\"time_id\")\n                             .apply_grouped(cutran.get_cu_shift_transform(shift_by=1, null_val=null_val),\n                                            incols={\"logx\": 'x'},\n                                            outcols=dict(y_out=cp.float32),\n                                            tpb=32)[\"y_out\"])\n    df[\"keep_row\"] = df[f\"logx_shifted\"] != null_val\n    return df[\"logx\"] - df[\"logx_shifted\"]\n\n\n\ndef extract_raw_book_features(df, null_val=-9999):\n    for n in range(1, 3):\n        p1 = df[f\"bid_price{n}\"]\n        p2 = df[f\"ask_price{n}\"]\n        s1 = df[f\"bid_size{n}\"]\n        s2 = df[f\"ask_size{n}\"]\n        df[f\"wap{n}\"] = (p1*s2 + p2*s1) \/ (s1 + s2)\n        df[f\"log_return{n}\"] = log_diff(df, in_col=f\"wap{n}\", null_val=null_val)\n        df[f\"realized_vol{n}\"] = df[f\"log_return{n}\"]**2\n        \n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    df[\"c\"] = 1\n    \n    df = df[df[\"keep_row\"]]\n    return df\n\n\ndef extract_raw_trade_features(df, null_val=-9999):\n    df[\"realized_vol_trade\"] = log_diff(df, in_col=f\"price\", null_val=null_val)**2\n    df = df[df[\"keep_row\"]]\n    return df\n\n\ndef agg(df, feature_dict):\n    agg_df = df.groupby(\"time_id\").agg(feature_dict).reset_index()\n    def f(x):\n        if x[1] == \"\":\n            return x[0]\n        return x[0] + \"_\" + x[1]\n    \n    agg_df.columns = [f(x) for x in agg_df.columns]\n    return agg_df    \n\n\ndef extract_book_stats(df):\n    default_stats = [\"sum\", \"mean\", \"std\"]\n    feature_dict = {\n        'wap1': default_stats,\n        'wap2': default_stats,\n        'log_return1': default_stats,\n        'log_return2': default_stats,\n        'wap_balance': default_stats,\n        'price_spread': default_stats,\n        'bid_spread': default_stats,\n        'ask_spread': default_stats,\n        'total_volume': default_stats,\n        'volume_imbalance': default_stats,\n        'c': [\"sum\"],\n        'realized_vol1': [\"sum\"],\n        'realized_vol2': [\"sum\"],\n    }\n    \n    return agg(df, feature_dict)\n    \n\n    \n    \ndef extract_trade_stats(df):\n    feature_dict = {\n        'realized_vol_trade': [\"sum\"],\n        'seconds_in_bucket':[\"count\"],\n        'size': [\"sum\"],\n        'order_count': [\"mean\"],\n    }\n    \n    return agg(df, feature_dict)\n\n\ndef time_constraint_fe(df, stats_df, last_sec, fe_function, cols):\n    sub_df = df[df[\"seconds_in_bucket\"] >= (600 - last_sec)].reset_index(drop=True)\n    if sub_df.shape[0] > 0:\n        sub_stats = fe_function(sub_df)\n    else:\n        sub_stats = cudf.DataFrame(columns=cols)\n    return stats_df.merge(sub_stats, on=\"time_id\", how=\"left\", suffixes=('', f'_{last_sec}'))    \n    \n\ndef feature_engineering(book_path, trade_path):\n    book_df = cudf.read_parquet(book_path)\n    book_df = extract_raw_book_features(book_df)\n    book_stats = extract_book_stats(book_df)\n    book_cols = book_stats.columns\n    \n    trade_df = cudf.read_parquet(trade_path)\n    trade_df = extract_raw_trade_features(trade_df)\n    trade_stats = extract_trade_stats(trade_df)\n    trade_cols = trade_stats.columns\n    \n    for last_sec in [150, 300, 450]:\n        book_stats = time_constraint_fe(book_df, book_stats, last_sec, extract_book_stats, book_cols) \n        trade_stats = time_constraint_fe(trade_df, trade_stats, last_sec, extract_trade_stats, trade_cols) \n\n    return book_stats.merge(trade_stats, on=\"time_id\", how=\"left\")\n\n\ndef process_data(order_book_paths, trade_paths, stock_ids):\n    stock_dfs = []\n    for book_path, trade_path in tqdm(list(zip(order_book_paths, trade_paths))):\n        stock_id = int(book_path.split(\"=\")[1].split(\"\/\")[0])\n\n        df = feature_engineering(book_path, trade_path)\n        df[\"stock_id\"] = stock_id\n        stock_dfs.append(df)\n    return cudf.concat(stock_dfs)","e4a4d37b":"past_volatility = process_data(order_book_training, trades_training, stock_ids)\npast_test_volatility = process_data(order_book_test, trades_test, stock_ids)\n\npast_volatility.shape, past_test_volatility.shape","b7c56284":"past_volatility = past_volatility.merge(cudf.read_csv(\"\/kaggle\/input\/optiver-nn\/optiver_nn_v01_oof.csv\"), on=[\"stock_id\", \"time_id\"], how=\"left\")\npast_test_volatility = past_test_volatility.merge(cudf.read_csv(\"\/kaggle\/working\/nn_preds.csv\"), on=[\"stock_id\", \"time_id\"], how=\"left\")\n\n\npast_volatility[\"nn_pred\"] = past_volatility[\"nn_pred\"].clip(0.0, None)*SCALE\npast_test_volatility[\"nn_pred\"] = past_test_volatility[\"nn_pred\"].clip(0.0, None)*SCALE","0c48de0b":"def stock_time_fe(df):\n    cols = ['realized_vol1_sum', 'realized_vol2_sum', 'realized_vol_trade_sum',\n            'realized_vol1_sum_150', 'realized_vol2_sum_150', 'realized_vol_trade_sum_150',\n            'realized_vol1_sum_300', 'realized_vol2_sum_300', 'realized_vol_trade_sum_300',\n            'realized_vol1_sum_450', 'realized_vol2_sum_450', 'realized_vol_trade_sum_450',\n            'nn_pred']\n    \n    for agg_col in [\"stock_id\", \"time_id\"]:\n        for agg_func in [\"mean\", \"max\", \"std\", \"min\"]:\n            agg_df = df.groupby(agg_col)[cols].agg(agg_func)\n            agg_df.columns = [f\"{agg_col}_{agg_func}_{col}\" for col in agg_df.columns]\n            df = df.merge(agg_df.reset_index(), on=agg_col, how=\"left\")\n    \n    return df\n\npast_volatility[\"is_test\"] = False\npast_test_volatility[\"is_test\"] = True\nall_df = past_volatility.append(past_test_volatility).reset_index(drop=True)\n\nall_df = stock_time_fe(all_df)\n\npast_volatility = all_df[~all_df[\"is_test\"]]\npast_test_volatility = all_df[all_df[\"is_test\"]]","58eb343d":"dev_df = dev_df.merge(past_volatility, on=[\"stock_id\", \"time_id\"], how=\"left\")\n\nfeatures = [col for col in list(dev_df.columns)\n            if col not in {\"stock_id\", \"time_id\", \"target\", \"is_test\"}]\nlen(features)","038e8a21":"import xgboost as xgb\n\ndef rmspe(y_true, y_pred):\n    return (cp.sqrt(cp.mean(cp.square((y_true - y_pred) \/ y_true))))\n\n\ndef rmspe_xgb(pred, dtrain):\n    y = dtrain.get_label()\n    return 'rmspe', rmspe(cp.array(y), cp.array(pred))\n\n\nNUM_FOLDS = 5\nparam = {'objective': 'reg:squarederror',\n         'learning_rate': 0.1,\n         'max_depth': 3,\n         \"min_child_weight\": 200,\n         \"reg_alpha\": 10.0,\n         \"tree_method\": 'gpu_hist', \"gpu_id\": 0,\n         'disable_default_eval_metric': 1\n    }\n\ntarget = \"target\"\n\noof_preds = cp.zeros(dev_df.shape[0])\ntest_preds = cp.zeros(past_test_volatility.shape[0])\n\nfor fold in range(NUM_FOLDS):\n    print(\"Fold\", fold)\n    train_ind = cp.where(dev_df[\"time_id\"].values % NUM_FOLDS != fold)[0]\n    val_ind = cp.where(dev_df[\"time_id\"].values % NUM_FOLDS == fold)[0]\n        \n    train_df, val_df = dev_df.iloc[train_ind], dev_df.iloc[val_ind]\n\n    d_train = xgb.DMatrix(train_df[features], train_df[target], weight=1\/cp.square(train_df[target]))\n    d_val = xgb.DMatrix(val_df[features], val_df[target], weight=1\/cp.square(val_df[target]))\n\n    model = xgb.train(param, d_train, evals=[(d_train, \"train\"), (d_val, \"val\")], \n                      num_boost_round=5000, verbose_eval=50, feval=rmspe_xgb,\n                      early_stopping_rounds=200)\n    \n    oof_preds[val_ind] = model.predict(d_val)\n    test_preds += cp.array(model.predict(xgb.DMatrix(past_test_volatility[features].astype(\"float\")))\/NUM_FOLDS)","840c02f5":"dev_df[\"pred\"] = oof_preds\nprint(f'The RMSPE score of XGB is {rmspe(dev_df[\"target\"], dev_df[\"pred\"])}')","b2719974":"past_test_volatility[\"row_id\"] = past_test_volatility[\"stock_id\"].astype(str) + \"-\" + past_test_volatility[\"time_id\"].astype(str) \npast_test_volatility[\"target\"] = test_preds.clip(0.0, 100.0)\/SCALE","e5fb006d":"%cd \/kaggle\/working","f106be9e":"sub_df = load_data(\"test\", path=PATH).merge(past_test_volatility[[\"row_id\", \"target\"]], \n                                            on=\"row_id\", how=\"left\").fillna(0.0)\n\nsub_df.to_csv(\"submission.csv\", index=False, columns=[\"row_id\", \"target\"])","e9f9378e":"cudf.read_csv(\"submission.csv\")","72df061e":"## Train XGBoost model on GPU","0fb54608":"## Using rapids-kaggle-utils for missing cuDF aggregation functions","32200ade":"# Accelerating Trading on GPU via RAPIDS\n## Best scoring CPU kernel is accelerated on GPU. 3.5x Speedup!!!\n\n![](https:\/\/i.imgur.com\/lkjVW2f.png)","7cc231f4":"### Get NN with no FE features from https:\/\/www.kaggle.com\/aerdem4\/optiver-pytorch-gpu-no-feature-eng"}}