{"cell_type":{"b7cf2022":"code","ffb6a412":"code","30e53007":"code","d2080891":"code","1285916a":"code","578e32b4":"code","08125e24":"code","28394b8b":"code","5ed54185":"code","c8c81bdc":"code","a635e78c":"code","29b37b4d":"code","5da1da2c":"code","68369de9":"code","f27594c5":"code","a9699f2a":"code","043b9abf":"code","dc7f7398":"code","b49c31c1":"code","4af50f81":"code","6907ef7f":"code","ff99e217":"code","a12918a5":"code","aa9dffc5":"code","9f3334f1":"code","672809e0":"code","d44baea6":"code","31ef14db":"code","7c46e694":"code","227502fa":"code","e0b46ea2":"code","3701bc61":"code","5e128b05":"code","b4091e2d":"code","69742ad1":"code","19240ddf":"code","ee7c2c0d":"code","0d7c4dce":"code","c372e9d0":"code","3332e4c3":"code","8157a34c":"code","32506472":"code","11f3ad9e":"code","2ebb468f":"code","330966dd":"code","6b73e072":"code","fa80231e":"code","c7254413":"code","8f82bd30":"code","12d6033a":"code","4f4fea2c":"code","cf818c6b":"code","a73f8b52":"code","179f7d25":"code","5537230f":"code","0e28b5c7":"code","9f910a58":"code","08581c7e":"code","d619c7d3":"code","ed3b1e4e":"code","5fb24089":"code","e8221f93":"code","58f5ee59":"code","a2e49eea":"code","7ddf7ee3":"code","cf21f27b":"code","66436525":"code","ada50f60":"code","2ebca45a":"code","58b83131":"code","78eee71c":"code","6abee619":"code","a3fe312a":"code","c226d911":"code","7af02a95":"code","7b7e36b2":"code","773b5401":"code","8c96d0ae":"code","6c356322":"code","f33a3838":"code","7668b55f":"code","12b1bb1b":"code","23ba4c5e":"code","f12f661f":"code","128c2bcd":"code","cab7d1c1":"code","cb88310a":"code","e24916f0":"code","8bf9eda3":"markdown","7d6f7d3b":"markdown","ff0582ec":"markdown","d3aaf909":"markdown","6fcffae6":"markdown","0b1b81d9":"markdown","8deb072d":"markdown","269829fb":"markdown","4a57f58b":"markdown","1127397e":"markdown","876bdef4":"markdown","79170ae1":"markdown","4507c803":"markdown","ebf06161":"markdown","a216a488":"markdown","511847c5":"markdown","e0c344a2":"markdown","e3a5afd1":"markdown","dba4ef56":"markdown","a39eec7e":"markdown","32bb8400":"markdown","39049924":"markdown","ce2cbfc4":"markdown","71831220":"markdown","cc7144c4":"markdown"},"source":{"b7cf2022":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ffb6a412":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt","30e53007":"train = pd.read_csv(\"\/kaggle\/input\/song-popularity-prediction\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/song-popularity-prediction\/test.csv\")","d2080891":"print(\"The training set's shape is :\", train.shape)\nprint(\"The test set's shape is :\", test.shape)","1285916a":"train.info()","578e32b4":"train.describe()","08125e24":"sns.countplot(data=train,x=\"song_popularity\")","28394b8b":"train.nunique()","5ed54185":"discrete = ['key', 'audio_mode', 'time_signature']\ncontinuous = train.columns.difference(discrete)\ncontinuous = continuous.difference(['song_popularity','id'])","c8c81bdc":"(train.isnull().sum(axis=0)[train.isnull().sum(axis=0)>0]*100\/train.shape[0]).sort_values(ascending = False)","a635e78c":"(test.isnull().sum(axis=0)[test.isnull().sum(axis=0)>0]*100\/test.shape[0]).sort_values(ascending = False)","29b37b4d":"missing = (test.isnull().sum(axis=0)[test.isnull().sum(axis=0)>0]).index\nmissing_discrete = missing.intersection(discrete)\nmissing_continuous = missing.intersection(continuous)","5da1da2c":"print(\"Discrete variables that contain missing values : \", list(missing_discrete))\nprint(\"Continuous variables that contain missing values : \", list(missing_continuous))","68369de9":"missingness_in_rows = (train.loc[:,(missing)].isnull().sum(axis=1)*100\/len(missing))","f27594c5":"sns.countplot(x=missingness_in_rows)","a9699f2a":"(missingness_in_rows>50).sum()","043b9abf":"((test.loc[:,(missing)].isnull().sum(axis=1)*100\/len(missing))>50).sum()","dc7f7398":"row_with_missing_value = (train.loc[:,(missing)].isnull().sum(axis=1)*100\/len(missing))>0","b49c31c1":"sns.countplot(x=row_with_missing_value, hue = train['song_popularity'])","4af50f81":"df = pd.DataFrame({\"missingness_in_row\":row_with_missing_value,\"song_popularity\":train[\"song_popularity\"]})\ndf.head()","6907ef7f":"x = \"missingness_in_row\"\ny = \"song_popularity\"\n(df\n.groupby(x)[y]\n.value_counts(normalize=True)\n.mul(100)\n.rename('percent')\n.reset_index()\n.pipe((sns.catplot,'data'), x=x,y='percent',hue=y,kind='bar'))","ff99e217":"#This cell took a lot of time running the first time I ran it and it didn't offer me that much that much of intel\n#viewing the result, you can uncomment and try to run it by yourself if you are brave enough :) .\n#sns.pairplot(train.loc[:,train.columns.difference(['id'])],kind = 'kde', hue = 'song_popularity')","a12918a5":"fig = plt.figure(figsize=(10,10))\nsns.heatmap(train.corr(), annot=True)","aa9dffc5":"train.corr()[abs(train.corr())>0.6]","9f3334f1":"sns.scatterplot(data=train,x=\"loudness\",y=\"energy\",hue=\"song_popularity\")","672809e0":"discrete","d44baea6":"sns.countplot(data=train,x=\"key\",hue=\"song_popularity\")","31ef14db":"y = \"song_popularity\"\nfor i, x in enumerate(discrete):\n    (train\n    .groupby(x)[y]\n    .value_counts(normalize=True)\n    .mul(100)\n    .rename('percent')\n    .reset_index()\n    .pipe((sns.catplot, 'data'), x=x,y='percent',hue=y,kind='bar'))\nplt.show()","7c46e694":"fig, axs = plt.subplots(2,5,figsize = (24,12))\naxs = axs.flatten()\ny = \"song_popularity\"\nfor i, x in enumerate(continuous):\n    sns.histplot(data = train, x = x, hue = y, stat = \"percent\", common_norm = False, ax = axs[i])\nplt.show()","227502fa":"fig, axs = plt.subplots(2,5,figsize = (24,12))\naxs = axs.flatten()\ny = \"song_popularity\"\nfor i, x in enumerate(continuous):\n    if (x!=\"loudness\"):\n        sns.histplot(data = train, x = np.log1p(train[x]), hue = y, stat = \"percent\", common_norm = False, ax = axs[i])\n    else:\n        sns.histplot(data = train, x = np.log1p(-train[x]), hue = y, stat = \"percent\", common_norm = False, ax = axs[i])\nplt.show()","e0b46ea2":"fig = plt.figure(figsize=(10,10))\nsns.histplot(data = train[train[\"instrumentalness\"]<0.01], x = \"instrumentalness\", hue = y, stat = \"percent\", common_norm = False)","3701bc61":"train[train[\"instrumentalness\"]>=0.01][\"instrumentalness\"].nunique()\/train[train[\"instrumentalness\"]>=0.01].shape[0]","5e128b05":"fig = plt.figure(figsize=(10,10))\nsns.histplot(data = train[(train[\"instrumentalness\"]>=0.01) & (train[\"instrumentalness\"]<0.5)], x = \"instrumentalness\", hue = y, stat = \"percent\", common_norm = False)","b4091e2d":"fig = plt.figure(figsize=(10,10))\nsns.histplot(data = train[(train[\"instrumentalness\"]>=0.055) & (train[\"instrumentalness\"]<0.4)], x = \"instrumentalness\", hue = y, stat = \"percent\", common_norm = False)","69742ad1":"fig = plt.figure(figsize=(10,10))\nsns.histplot(data = train[(train[\"instrumentalness\"]>=0.195) & (train[\"instrumentalness\"]<0.5)],\n             x = \"instrumentalness\", hue = y, hue_norm = (0,1))","19240ddf":"fig = plt.figure(figsize=(10,10))\nsns.histplot(data = train[train[\"instrumentalness\"]>=0.5], x = \"instrumentalness\", hue = y, stat = \"percent\", common_norm = False)","ee7c2c0d":"df = pd.DataFrame()\neps = 1e-6\nranges = [(train.instrumentalness.min(),0.01),(0.01,0.055),(0.055,0.195),\n          (0.195,0.5),(0.5,0.82),(0.82,train.instrumentalness.max())]\nfor i, (low,up) in enumerate(ranges):\n    df[\"instru\"+str(i)] = train[\"instrumentalness\"].apply(lambda x: x if (low <= x <= up) else np.NaN)\n    \ndf.describe()","0d7c4dce":"fig, axs = plt.subplots(1,6,figsize=(24,6))\naxs.flatten()\nfor i,x in enumerate(df.columns):\n    sns.histplot(data = df, x = df[x], hue = train[\"song_popularity\"], stat = \"percent\", common_norm = False, ax = axs[i])\nplt.show()","c372e9d0":"df[\"song_popularity\"]=train[\"song_popularity\"]","3332e4c3":"df.isna().sum()\/df.shape[0]","8157a34c":"train[\"instrumentalness\"] = df[\"instru0\"]\ntrain[\"instrumentalness\"].describe()","32506472":"test[\"instrumentalness\"] = test[\"instrumentalness\"].apply(lambda x: x if (ranges[0][0] <= x <= ranges[0][1]) else np.NaN)","11f3ad9e":"train.drop(index=train[(train.loc[:,(missing)].isnull().sum(axis=1)*100\/len(missing))>50].index,inplace=True)","2ebb468f":"#train['row_with_missing_value'] = row_with_missing_value","330966dd":"#test['row_with_missing_value'] = (test.loc[:,(missing)].isnull().sum(axis=1)*100\/len(missing))>0","6b73e072":"train_orig = train.copy()\ntest_orig = test.copy()","fa80231e":"from sklearn.impute import SimpleImputer","c7254413":"imp_median = SimpleImputer(strategy='median')","8f82bd30":"for i in continuous:\n    imp_median = imp_median.fit(train[[i]])\n    train[i] = imp_median.transform(train[[i]]).ravel()\n    test[i] = imp_median.transform(test[[i]]).ravel()","12d6033a":"imp_freq = SimpleImputer(strategy='most_frequent')","4f4fea2c":"for i in discrete:\n    imp_median = imp_median.fit(train[[i]])\n    train[i] = imp_median.transform(train[[i]]).ravel()\n    test[i] = imp_median.transform(test[[i]]).ravel()","cf818c6b":"from sklearn.model_selection import train_test_split","a73f8b52":"X_train, X_test, y_train, y_test = train_test_split(train[train.columns.difference(['song_popularity','id'])], train['song_popularity'], test_size=0.3, random_state=42)","179f7d25":"from sklearn.linear_model import LogisticRegression","5537230f":"model = LogisticRegression()","0e28b5c7":"model.fit(X_train,y_train)","9f910a58":"from sklearn.metrics import roc_auc_score","08581c7e":"roc_auc_score(y_train, model.predict_proba(X_train)[:,1])","d619c7d3":"roc_auc_score(y_test, model.predict_proba(X_test)[:,1])","ed3b1e4e":"from sklearn.metrics import classification_report","5fb24089":"print(classification_report(y_test, model.predict(X_test)))","e8221f93":"from sklearn.preprocessing import StandardScaler","58f5ee59":"ss = StandardScaler()","a2e49eea":"for i in continuous:\n    ss = ss.fit(train[[i]])\n    train[i] = ss.transform(train[[i]]).ravel()\n    test[i] = ss.transform(test[[i]]).ravel()","7ddf7ee3":"X_train, X_test, y_train, y_test = train_test_split(train[train.columns.difference(['song_popularity','id'])],\n                                                    train['song_popularity'], test_size=0.3, random_state=42)","cf21f27b":"from sklearn.linear_model import LogisticRegressionCV","66436525":"model_1 = LogisticRegressionCV(Cs = np.logspace(-6,6,13), scoring = \"roc_auc\", n_jobs = -1, random_state=8).fit(X_train, y_train)","ada50f60":"model_1.scores_","2ebca45a":"roc_auc_score(y_train, model_1.predict_proba(X_train)[:,1])","58b83131":"roc_auc_score(y_test, model_1.predict_proba(X_test)[:,1])","78eee71c":"sample_submission = pd.read_csv(\"..\/input\/song-popularity-prediction\/sample_submission.csv\")\nsample_submission.head()","6abee619":"sample_submission[\"song_popularity\"] = model_1.predict_proba(test[test.columns.difference([\"id\"])])[:,1]","a3fe312a":"sample_submission.to_csv(\"first.csv\",index=False)","c226d911":"train_light = train_orig.copy()\ntest_light = test_orig.copy()","7af02a95":"# !rm -r kuma_utils\n!git clone https:\/\/github.com\/analokmaus\/kuma_utils.git","7b7e36b2":"import sys\nsys.path.append(\"kuma_utils\/\")\nfrom kuma_utils.preprocessing.imputer import LGBMImputer","773b5401":"%%time\nlgbm_imtr = LGBMImputer(cat_features = discrete, n_iter=100, verbose=True)\n\ntrain_lgbmimp = pd.DataFrame(lgbm_imtr.fit_transform(train_light[train.columns.difference([\"id\", \"song_popularity\"])]))\ntest_lgbmimp = pd.DataFrame(lgbm_imtr.transform(test_light[test.columns.difference([\"id\"])]))","8c96d0ae":"train_lgbmimp.info()","6c356322":"for i in discrete:\n    train_lgbmimp[i] = train_lgbmimp[i].astype(\"int64\")","f33a3838":"for i in discrete:\n    test_lgbmimp[i] = test_lgbmimp[i].astype(\"int64\")","7668b55f":"from lightgbm import LGBMClassifier","12b1bb1b":"train_lgbmimp.columns","23ba4c5e":"X_train, X_test, y_train, y_test = train_test_split(train_lgbmimp,\n                                                    train['song_popularity'], test_size=0.3, random_state=42)","f12f661f":"model_2 = LGBMClassifier(num_leaves= 33, \n                        max_depth= 3,\n                        learning_rate=0.005,\n                        n_estimators= 500,\n                        objective='binary',\n                        min_split_gain=0.05,\n                        min_child_weight=1,\n                        min_child_samples=15,\n                        subsample=0.5,\n                        colsample_bytree=0.8,\n                        reg_alpha=0.1,\n                        reg_lambda=0.1,\n                        n_jobs= -1,\n                        verbose= -1)","128c2bcd":"model_2 = model_2.fit(X_train, y_train, eval_set = [(X_train, y_train), (X_test, y_test)],\n                      eval_metric = \"auc\", categorical_feature=discrete,\n                      verbose=500, early_stopping_rounds=30)","cab7d1c1":"model_2.best_score_","cb88310a":"sample_submission[\"song_popularity\"] = model_2.predict_proba(test[test.columns.difference([\"id\"])])[:,1]","e24916f0":"sample_submission.to_csv(\"second.csv\",index=False)","8bf9eda3":"Let's check the proportion of missing data (for both the training and the testing sets):","7d6f7d3b":"Then, let's remove the non really informative rows :","ff0582ec":"Let's start by having an overview on our data","d3aaf909":"Even though the additional features that we've created, other than the first one, seem like they have a good distribution, it would be better to not consider them due to the high number of their missing values.","6fcffae6":"First, let's change the instrumentalness variable to its first distribution.","0b1b81d9":"The percentage of missing data isn't that high for any of the variables, so we needn't consider deleting any of them. Each kind of variable needs a different treatement to cover this issue, we will deal with it later on in the notebook.","8deb072d":"Now let's start using lightgbm.","269829fb":"We may consider to delete rows having more than half of its data missing as they may not be very informative (and also they are not that many in both the sets so we wouldn't worry about it)","4a57f58b":"Uncomment the following cells if you think adding a column indicating missing values is necessary.","1127397e":"# Exploraty Data Analysis","876bdef4":"Given the fact that those are actually categorical variables I think it can be a good idea to find another way of encoding them (to avoid any bias due to the ordering of the current encoding).","79170ae1":"We can already observe that almost half the variables contain missing data. Also we can see that all of the features are of non-object type, so there is no need for encoding methods in this contest.","4507c803":"# Data Cleaning","ebf06161":"It may be a good idea to divide the instrumentalness feature into 6 distributions.","a216a488":"Now et's check the missing values by datapoints :","511847c5":"It seems that target is a bit unbalanced, of course not as bad as other problems such as fraud or cancer detection, bur we need to keep this information in mind.","e0c344a2":"So 3 of the features (key, audio_mode and time_signature) are discrete meanwhile the rest are continuous.\nLet's distinguish them for later use:","e3a5afd1":"As most of the continuous variables are skewed we can impute them using their medians.","dba4ef56":"Let's see what a simple logistic regression yields to us.","a39eec7e":"And of course we need to apply the same thing to this feature in the testdata.","32bb8400":"We understand that a song is more likely to be unpopular when it has a missing value.","39049924":"Before having this plot, we were blinded by the inbalance of the target when we made our latest judgement. We can conclude that adding a feature depicting the missingness of values in each row doesn't help us in any way.","ce2cbfc4":"# Modeling","71831220":"Now this is better.","cc7144c4":"There is certainly a room for improvement."}}