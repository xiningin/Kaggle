{"cell_type":{"90b49a7a":"code","3be1cd0a":"code","63832523":"code","ce1aa190":"code","48e0a650":"code","3dead9d4":"code","8bda6180":"code","82f05ee4":"code","c72664be":"code","000cf460":"code","d4adeacc":"code","1b4db667":"code","7c0f125a":"code","9a713d2f":"code","b0796c6a":"code","760f69b3":"code","afbeed22":"code","b84bb9ed":"code","c4b8745d":"code","85a87e13":"code","af94419c":"code","3fb650e1":"code","b7a7e461":"code","0257927c":"code","b7052fd3":"code","6cf28954":"code","4b0cf6d0":"code","9cc282d1":"code","de1e8f9d":"code","c5323586":"code","565000dd":"code","adc80846":"code","c507f789":"code","b0616865":"code","28e6ff22":"code","02a86a9d":"code","a48096d3":"code","0344b459":"code","c05d7f38":"code","0f572046":"code","097e6163":"code","3a5ca7cf":"code","7fa535ab":"code","12757dfe":"code","f9156612":"code","dc9f1e35":"code","1bffbd8e":"code","d35030a1":"code","0880937b":"code","82bd3dee":"code","42f0fbce":"code","49c509ac":"code","29fe299e":"code","14b5e357":"code","4246403c":"code","a33843b8":"code","c30f9a4c":"code","51fc724c":"code","4cbdd528":"code","80053887":"code","caa035bc":"code","49ebf71e":"code","dcae0ea1":"code","eed3889b":"code","d5ea18a1":"code","0d51487d":"code","3ca8e327":"code","8371bbb5":"code","156b49d2":"code","3f47f390":"code","b1e3c739":"code","792a9b72":"code","aa3137cd":"code","8815a9d6":"code","eaf4d585":"code","5116cdc0":"code","13442d3a":"code","bf3002d6":"code","0839c7ca":"code","dd513712":"code","4076dcff":"code","f91fdfb5":"code","b11d5555":"code","92dde9f7":"code","935f3453":"code","e5101b15":"code","4b3d2564":"code","3754d46e":"code","96f754e3":"code","5de7f53e":"markdown","10cd294a":"markdown","71ef8164":"markdown","c94002a9":"markdown","02e8ae4b":"markdown","d3e5dc91":"markdown","68932253":"markdown","f078a092":"markdown","76279745":"markdown","ba3bff4b":"markdown","d45a2199":"markdown","6c94ac2e":"markdown","965ca508":"markdown","32f8f868":"markdown","f5bdb05a":"markdown","15c7d03e":"markdown","4eb1cb05":"markdown","62efdde3":"markdown","d6dce13f":"markdown","6c0c2162":"markdown","118403b6":"markdown","841832e0":"markdown","52d36852":"markdown"},"source":{"90b49a7a":"!pip install bubbly\n!pip install joypy","3be1cd0a":"!pip install plotly_express","63832523":"import warnings\nwarnings.filterwarnings('ignore')\n\n# for some basic operations\nimport numpy as np \nimport pandas as pd \nimport joypy\n\n# for visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas import plotting\nfrom pandas.plotting import parallel_coordinates\n\n# for interactive visualizations\nimport plotly\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode(connected = True)\nimport plotly.figure_factory as ff\n\n# for animated visualizations\nfrom bubbly.bubbly import bubbleplot\nimport plotly_express as px\n\n# for providing path\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# for modelling\nimport sklearn\nimport imblearn\n\n# for model explanation\nimport shap \nimport eli5","ce1aa190":"# let's import the data\ndata = pd.read_csv('..\/input\/insurance_claims.csv')\n\n# let's take a look at the data\npd.set_option('display.max_columns', None)\ndata.head()","48e0a650":"# let's check the shape of the dataset\n\ndata.shape","3dead9d4":"# let's get the information about the dataset\n\ndata.info()","8bda6180":"# let's describe the data\n# It will demonstrate the count, mean, std dev, min, max, etc values for the Numerical features present in the data.\n\ndata.describe()","82f05ee4":"# lets check the correlation\ndata.corr()","c72664be":"# lets check the covriance\ndata.cov()","000cf460":"# let's check whether the data has any null values or not.\n\n# but there are '?' in the datset which we have to remove by NaN Values\ndata = data.replace('?',np.NaN)\n\ndata.isnull().any()","d4adeacc":"# missing value treatment using fillna\n\n# we will replace the '?' by the most common collision type as we are unaware of the type.\ndata['collision_type'].fillna(data['collision_type'].mode()[0], inplace = True)\n\n# It may be the case that there are no responses for property damage then we might take it as No property damage.\ndata['property_damage'].fillna('NO', inplace = True)\n\n# again, if there are no responses fpr police report available then we might take it as No report available\ndata['police_report_available'].fillna('NO', inplace = True)\n\ndata.isnull().any().any()","1b4db667":"# plotting a scatter plot\n\nfig = px.scatter(data, x = 'total_claim_amount', y = 'policy_annual_premium', color = 'insured_sex',\n                marginal_x = 'rug', marginal_y = 'histogram')\nfig.show()","7c0f125a":"fig = px.scatter_matrix(data, dimensions=[\"injury_claim\", \"property_claim\", \"vehicle_claim\"],\n                        color = \"insured_sex\")\nfig.show()","9a713d2f":"fraud = data['fraud_reported'].value_counts()\n\nlabel_fraud = fraud.index\nsize_fraud = fraud.values\n\ncolors = ['silver', 'gold']\ntrace = go.Pie(\n         labels = label_fraud, values = size_fraud, marker = dict(colors = colors), name = 'Frauds', hole = 0.3)\n\n\ndf = [trace]\n\nlayout = go.Layout(\n           title = 'Distribution of Frauds')\n\nfig = go.Figure(data = df, layout = layout)\n\npy.iplot(fig)","b0796c6a":"plt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = (15, 8)\n\nsns.stripplot(data['property_damage'], data['property_claim'], palette = 'bone')\nplt.title('Incident Type vs Vehicle Claim', fontsize = 20)\nplt.show()","760f69b3":"# let's check the incident types\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = (15, 8)\n\nsns.countplot(data['incident_type'], palette = 'spring')\nplt.title('Different Types of Incidents', fontsize = 20)\nplt.show()\n","afbeed22":"# swarm plot\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = (15, 8)\n\nsns.swarmplot(data['policy_state'], data['total_claim_amount'], palette = 'copper')\nplt.title('Policy State vs Total Claim Amount', fontsize = 20)\nplt.show()","b84bb9ed":"\ntrace = go.Histogram(\n          x = data['insured_education_level'],\n          name = 'Marvel',\n          opacity = 0.75,\n          marker = dict(\n                 color = 'Blue'\n          )\n)\ndf = [trace]\n\nlayout = go.Layout(\n    title = 'Education Level of the Customers')\n\nfig = go.Figure(data = df, layout = layout)\npy.iplot(fig)","c4b8745d":"\ntrace = go.Histogram(\n          x = data['insured_occupation'],\n          name = 'Marvel',\n          opacity = 0.75,\n          marker = dict(\n                 color = 'Red'\n          )\n)\ndf = [trace]\n\nlayout = go.Layout(\n    title = 'Occupation of the Customers')\n\nfig = go.Figure(data = df, layout = layout)\npy.iplot(fig)","85a87e13":"sex = data['insured_sex'].value_counts()\nrel = data['insured_relationship'].value_counts()\n\nlabel_sex = sex.index\nsize_sex = sex.values\n\nlabel_rel = rel.index\nsize_rel = rel.values\n\ncolors = ['aqua', 'gold']\ntrace = go.Pie(\n         labels = label_sex, values = size_sex, marker = dict(colors = colors), name = 'Gender', hole = 0.3)\n\ncolors2 = ['pink', 'lightblue','lightgreen','grey','red']\ntrace2 = go.Pie(labels = label_rel, values = size_rel, marker = dict(colors = colors2), name = 'Relationship',\n                hole = 0.3)\n\ndf = [trace]\ndf2 = [trace2]\n\nlayout1 = go.Layout(\n           title = 'Gender of the Customers')\nlayout2 = go.Layout(\n           title = 'Relationship')\n\nfig = go.Figure(data = df, layout = layout1)\nfig2 = go.Figure(data = df2, layout = layout2)\npy.iplot(fig)\npy.iplot(fig2)","af94419c":"trace = go.Box(\n          x = data['auto_make'],\n          y = data['vehicle_claim'],\n          opacity = 0.7,\n          marker = dict(\n                 color = 'rgb(215, 195, 5, 0.5)'\n          )\n)\ndf = [trace]\n\nlayout = go.Layout(\n    title = 'Automobile Company vs Vehicle Claim')\n\nfig = go.Figure(data = df, layout = layout)\npy.iplot(fig)","3fb650e1":"trace = go.Histogram(\n          x = data['policy_annual_premium'],\n          \n          #fill = 'tozeroy',\n          marker = dict(\n                 color = 'rgb(100, 75, 25, 0.5)'\n          )\n)\ndf = [trace]\n\nlayout = go.Layout(\n    title = 'Distribution of Annual Policy among the Customers',\n    scene = dict(\n            xaxis = dict(title  = 'Age'),\n            yaxis = dict(title  = 'Count')\n        ))\n\nfig = go.Figure(data = df, layout = layout)\npy.iplot(fig)","b7a7e461":"trace = go.Histogram(\n          x = data['age'],\n          \n          marker = dict(\n                 color = 'Red'\n          )\n)\ndf = [trace]\n\nlayout = go.Layout(\n    title = 'Distribution of Age among the Customers',\n    scene = dict(\n            xaxis = dict(title  = 'Age'),\n            yaxis = dict(title  = 'Count')\n        ))\n\nfig = go.Figure(data = df, layout = layout)\npy.iplot(fig)","0257927c":"# let's extrat days, month and year from policy bind date\n\ndata['policy_bind_date'] = pd.to_datetime(data['policy_bind_date'], errors = 'coerce')\n","b7052fd3":"# let's encode the fraud report to numerical values\n\ndata['fraud_reported'] = data['fraud_reported'].replace(('Y','N'),(0,1))\n\n# checking the values of fraud reported\n# data['fraud_reported'].value_counts()","6cf28954":"# let's check the correlation of authorities_contacted with the target\n\ndata[['auto_model','fraud_reported']].groupby(['auto_model'], \n                as_index = False).mean().sort_values(by = 'fraud_reported', ascending = False)","4b0cf6d0":"# let's perform target encoding for auto make\n\ndata['auto_make'] = data['auto_make'].replace(('3 Series','RSX','Malibu','Wrangler','Pathfinder','Ultima','Camry',\n                'Corolla','CRV','Legacy','Neon','95','TL','93','MDX','Accord','Grand Cherokee','Escape','E4000',\n            'A3','Highlander','Passat','92x','Jetta','Fusion','Forrestor','Maxima','Impreza','X5','RAM','M5','A5',\n                'Civic','F150','Tahaoe','C300','ML350','Silverado','X6'),\n                (0.95,0.91, 0.90,0.88,0.87,0.86,0.855,0.85,0.85,0.84,0.83,0.81,0.80,0.80,0.78,0.77,0.76,0.75,0.74,\n                 0.73,0.72,0.72,0.71,0.71,0.71,0.71,0.70,0.70,0.69,0.67,0.66,0.65,0.64,0.63,0.62,0.61,0.60,0.59,0.56))\n\n# let's check the values\n# data['auto_make'].value_counts()","9cc282d1":"# let's check the correlation auto make with the target\n\ndata[['auto_make','fraud_reported']].groupby(['auto_make'], \n                as_index = False).mean().sort_values(by = 'fraud_reported', ascending = False)","de1e8f9d":"# let's perform target encoding for auto make\n\ndata['auto_make'] = data['auto_make'].replace(('Jeep','Nissan','Toyota','Accura','Saab','Suburu',\n                                'Dodge','Honda','Chevrolet','BMW','Volkswagen','Audi','Ford','Mercedes'),\n                                              (0.84,0.82,0.81,0.80,0.77,0.76,0.75,0.74,0.73,0.72,0.71,0.69,0.69,0.66))\n\n# let's check the values\n# data['auto_make'].value_counts()","c5323586":"# let's check the correlation of authorities_contacted with the target\n\ndata[['police_report_available','fraud_reported']].groupby(['police_report_available'], \n                as_index = False).mean().sort_values(by = 'fraud_reported', ascending = False)","565000dd":"# let's perform target encoding for property damage\n\ndata['police_report_available'] = data['police_report_available'].replace(('NO','YES'),(0.77,0.74))\n\n# let's check the values\n# data['police_report_available'].value_counts()","adc80846":"# let's check the correlation of authorities_contacted with the target\n\ndata[['property_damage','fraud_reported']].groupby(['property_damage'], \n                as_index = False).mean().sort_values(by = 'fraud_reported', ascending = False)","c507f789":"# let's perform target encoding for property damage\n\ndata['property_damage'] = data['property_damage'].replace(('NO','YES'),(0.76,0.74))\n\n# let's check the values\n# data['property_damage'].value_counts()","b0616865":"# let's check the correlation of authorities_contacted with the target\n\ndata[['incident_city','fraud_reported']].groupby(['incident_city'], \n                as_index = False).mean().sort_values(by = 'fraud_reported', ascending = False)","28e6ff22":"# let's check the correlation of authorities_contacted with the target\n\ndata[['incident_state','fraud_reported']].groupby(['incident_state'], \n                as_index = False).mean().sort_values(by = 'fraud_reported', ascending = False)","02a86a9d":"# let's perform target encoding for incident state\n\ndata['incident_state'] = data['incident_state'].replace(('WV','NY','VA','PA','SC','NC','OH'),\n                                                        (0.82,0.77,0.76,0.73,0.70,0.69,0.56))\n\n# checking the values\n# data['incident_state'].value_counts()","a48096d3":"# let's check the correlation of authorities_contacted with the target\n\ndata[['authorities_contacted','fraud_reported']].groupby(['authorities_contacted'], \n                as_index = False).mean().sort_values(by = 'fraud_reported', ascending = False)","0344b459":"# let's perform target encoding for authorities contacted\n\ndata['authorities_contacted'] = data['authorities_contacted'].replace(('None','Police','Fire','Ambulance','Other'),\n                                                                      (0.94,0.79,0.73,0.70,0.68))\n\n# let's check the values\n#data['authorities'].value_counts()","c05d7f38":"# let's check the correlation of incident_severity with the target\n\ndata[['incident_severity','fraud_reported']].groupby(['incident_severity'], \n                as_index = False).mean().sort_values(by = 'fraud_reported', ascending = False)","0f572046":"# let's perform target encoding for incident severity\n\ndata['incident_severity'] = data['incident_severity'].replace(('Trivial Damage','Minor Damage','Total Loss',\n                                                              'Major Damage'),(0.94,0.89,0.87,0.39))\n\n# let's check the values\n# data['incident_severity'].value_counts()","097e6163":"# let's check the correlation of collision_type with the target\n\ndata[['collision_type','fraud_reported']].groupby(['collision_type'], \n                as_index = False).mean().sort_values(by = 'fraud_reported', ascending = False)","3a5ca7cf":"# let's perform target encoding for collision type\n\ndata['collision_type'] = data['collision_type'].replace(('Rear Collision', 'Side Collision', 'Front Collision'),\n                                                        (0.78,0.74,0.72))\n\n# let's check the values of collision type\n# data['collision_type'].value_counts()","7fa535ab":"# let's check the correlation of incident_type with the target\n\ndata[['incident_type','fraud_reported']].groupby(['incident_type'], \n                as_index = False).mean().sort_values(by = 'fraud_reported', ascending = False)","12757dfe":"# let's perform target encoing for incident type\n\ndata['incident_type'] = data['incident_type'].replace(('Vehicle Theft','Parked Car','Multi-vehicle Collision',\n                                'Single Vehicle Collision'),(0.91, 0.90, 0.72,0.70))\n\n# let's check the values\n#data['incident_type'].value_counts()","f9156612":"data['incident_date'] = pd.to_datetime(data['incident_date'], errors = 'coerce')\n\n# extracting days and month from date\ndata['incident_month'] = data['incident_date'].dt.month\ndata['incident_day'] = data['incident_date'].dt.day\n","dc9f1e35":"# let's know the relation between insured_relationship and fraud reported\n\ndata[['insured_relationship','fraud_reported']].groupby(['insured_relationship'], \n                as_index = False).mean().sort_values(by = 'fraud_reported', ascending = False)","1bffbd8e":"# let's do target encoding for insured relationship\n\ndata['insured_relationship'] = data['insured_relationship'].replace(('husband','own-child','unmarried',\n                                        'not-in-family','wife','other-relative'),(0.79,0.78,0.75,0.74,0.72,0.70))\n\n#data['insured-relationship'].value_counts()","d35030a1":"# let's know the relation between insured_hobbies and fraud reported\n\ndata[['insured_hobbies','fraud_reported']].groupby(['insured_hobbies'], \n                as_index = False).mean().sort_values(by = 'fraud_reported', ascending = False)","0880937b":"# let's perform target encoding for insured_hobbies\n\ndata['insured_hobbies'] = data['insured_hobbies'].replace(('camping', 'kayaking', 'golf','dancing',\n        'bungie-jumping','movies', 'basketball','exercise','sleeping','video-games','skydiving','paintball',\n            'hiking','base-jumping','reading','polo','board-games','yachting', 'cross-fit','chess'),(0.91, 0.90,\n                0.89, 0.88,0.84,0.83,0.82,0.81,0.805,0.80,0.78,0.77,0.76,0.73,0.73,0.72,0.70,0.69,0.25,0.17))\n\n#data['insured_hobbies'].value_counts()","82bd3dee":"# let's know the relation between insured_occupation and fraud reported\n\ndata[['insured_occupation','fraud_reported']].groupby(['insured_occupation'], \n                as_index = False).mean().sort_values(by = 'fraud_reported', ascending = False)","42f0fbce":"# let's perform target encoding for insured_occupation\n\ndata['insured_occupation'] = data['insured_occupation'].replace(('other-service','priv-house-serv',\n                        'adm-clerical','handlers-cleaners','prof-specialty','protective-serv',\n                'machine-op-inspct','armed-forces','sales','tech-support','transport-moving','craft-repair',\n                    'farming-fishing','exec-managerial'),(0.84, 0.84,0.83, 0.79,0.78,0.77,0.76,0.75,0.72,0.71,\n                                                          0.705,0.70,0.69,0.63))\n# data['insured_occupation'].value_counts()","49c509ac":"# let's know the relation of insured_education_level with faud_reported\n\ndata[['insured_education_level','fraud_reported']].groupby(['insured_education_level'], \n                as_index = False).mean().sort_values(by = 'fraud_reported', ascending = False)","29fe299e":"# let's perform target encoding\n\ndata['insured_education_level'] = data['insured_education_level'].replace(('Masters', 'High School','Associate',\n                                        'JD','College', 'MD','PhD'),(0.78,0.77,0.76,0.74,0.73,0.72,0.71))\n#data['insured_education_level'].value_counts()","14b5e357":"# lets know the relation of insured sex and fraud reported\n\ndata[['insured_sex','fraud_reported']].groupby(['insured_sex'], as_index = False).mean().sort_values(\n    by = 'fraud_reported', ascending = False)","4246403c":"# target encoding for sex\n\ndata['insured_sex'] = data['insured_sex'].replace(('FEMALE','MALE'),(0.76,0.73))\n#data['insured_sex'].value_counts()","a33843b8":"# csl - combined single limit\n\n'''CSL is a single number that describes the predetermined limit for the combined total of the Bodily Injury \nLiability coverage and Property Damage Liability coverage per occurrence or accident.'''\n\n# lets know the relation of policy state and fraud reported\n\ndata[['policy_csl','fraud_reported']].groupby(['policy_csl'], as_index = False).mean().sort_values(\n    by = 'fraud_reported', ascending = False)","c30f9a4c":"# target encoding for policy_csl\n\ndata['policy_csl'] = data['policy_csl'].replace(('500\/1000','100\/300','250\/500'),(0.78,0.74,0.73))\n\n# check the values\n# data['policy_csl'].value_counts()","51fc724c":"\n# lets know the relation of policy state and fraud reported\n\ndata[['policy_state','fraud_reported']].groupby(['policy_state'], as_index = False).mean().sort_values(\n    by = 'fraud_reported', ascending = False)\n","4cbdd528":"# target encoding for policy_csl\n\ndata['policy_state'] = data['policy_state'].replace(('IL','IN','OH'),(0.77,0.745,0.74))\n\n# check the values\n# data['policy_state'].value_counts()","80053887":"# let's delete unnecassary columns\n\ndata = data.drop(['policy_number','policy_bind_date', 'incident_date','incident_location','auto_model'], axis = 1)\n\n# let's check the columns after deleting the columns\ndata.columns","caa035bc":"# let's split the data into dependent and independent sets\n\nx = data.drop(['fraud_reported'], axis = 1)\ny = data['fraud_reported']\n\nprint(\"Shape of x :\", x.shape)\nprint(\"Shape of y :\", y.shape)","49ebf71e":"# let's split the dataset into train and test sets\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n\nprint(\"Shape of x_train :\", x_train.shape)\nprint(\"Shape of x_test :\", x_test.shape)\nprint(\"Shape of y_train :\", y_train.shape)\nprint(\"Shape of y_test :\", y_test.shape)","dcae0ea1":"plt.rcParams['figure.figsize'] = (15, 10)\nsns.heatmap(x_train.corr(), cmap = 'copper')\nplt.title('Heat Map for Correlations', fontsize = 20)\nplt.show()","eed3889b":"# Random Forest Classifier\n\nfrom imblearn.ensemble import BalancedRandomForestClassifier \nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n                 \n\nmodel = BalancedRandomForestClassifier(n_estimators = 100, random_state = 0)\n\nmodel.fit(x_train, y_train)\ny_pred_rf = model.predict(x_test)\n\nprint(\"Training Accuracy: \", model.score(x_train, y_train))\nprint('Testing Accuarcy: ', model.score(x_test, y_test))\n\n# making a classification report\ncr = classification_report(y_test,  y_pred_rf)\nprint(cr)\n\n# making a confusion matrix\ncm = confusion_matrix(y_test, y_pred_rf)\nprint(cm)","d5ea18a1":"# Easy Ensemble Classifier\n\nfrom imblearn.ensemble import EasyEnsembleClassifier \nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n                 \n\nmodel1 = EasyEnsembleClassifier(n_estimators = 100, random_state = 0)\n\nmodel1.fit(x_train, y_train)\ny_pred_ef = model1.predict(x_test)\n\nprint(\"Training Accuracy: \", model1.score(x_train, y_train))\nprint('Testing Accuarcy: ', model1.score(x_test, y_test))\n\n# making a classification report\ncr = classification_report(y_test,  y_pred_ef)\nprint(cr)\n\n# making a confusion matrix\ncm = confusion_matrix(y_test, y_pred_ef)\nprint(cm)","0d51487d":"# Random Forest with Bagging Classifier\n\nfrom imblearn.ensemble import BalancedBaggingClassifier \nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\n                 \n\nmodel2 = BalancedBaggingClassifier(base_estimator = RandomForestClassifier(),\n                                 sampling_strategy = 'auto',\n                                 replacement = False,\n                                 random_state = 0)\n\nmodel2.fit(x_train, y_train)\ny_pred_bc = model2.predict(x_test)\n\nprint(\"Training Accuracy: \", model2.score(x_train, y_train))\nprint('Testing Accuarcy: ', model2.score(x_test, y_test))\n\n# making a classification report\ncr = classification_report(y_test,  y_pred_bc)\nprint(cr)\n\n# making a confusion matrix\ncm = confusion_matrix(y_test, y_pred_bc)\nprint(cm)","3ca8e327":"# boosting\n\ny_pred = y_pred_rf*0.5 + y_pred_ef*0.2 + y_pred_bc*0.3\n\ny_pred[y_pred > 0.5] = 1\ny_pred[y_pred <= 0.5] = 0\n\n# making a classification report\ncr = classification_report(y_test,  y_pred)\nprint(cr)\n\n# making a confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","8371bbb5":"y.value_counts()","156b49d2":"frauds = np.array(data[data['fraud_reported'] == 0].index)\nno_frauds = len(frauds)\n\nprint(no_frauds)","3f47f390":"normal_indices = data[data['fraud_reported'] == 1]\nno_normal_indices = len(normal_indices)\n\nprint(no_normal_indices)","b1e3c739":"\nrandom_normal_indices = np.random.choice(no_normal_indices, size = no_frauds, replace = True)\nrandom_normal_indices = np.array(random_normal_indices)\n\nprint(len(random_normal_indices))","792a9b72":"under_sample = np.concatenate([frauds, random_normal_indices])\nprint(len(under_sample))","aa3137cd":"\n# creating the undersample data\n\nundersample_data = data.iloc[under_sample, :]","8815a9d6":"# splitting the undersample dataset into x and y sets\n\nx_u = undersample_data.iloc[:, undersample_data.columns != 'fraud_reported'] \ny_u = undersample_data.iloc[:, undersample_data.columns == 'fraud_reported']\n\nprint(x_u.shape)\nprint(y_u.shape)","eaf4d585":"from sklearn.model_selection import train_test_split\n\nx_train1, x_test1, y_train1, y_test1 = train_test_split(x_u, y_u, test_size = 0.2, random_state = 0)\n\nprint(x_train1.shape)\nprint(y_train1.shape)\nprint(x_test1.shape)","5116cdc0":"# standardization\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nx_train1 = sc.fit_transform(x_train1)\nx_test1 = sc.transform(x_test1)\n","13442d3a":"from sklearn.ensemble import RandomForestClassifier\n\nmodel_u = RandomForestClassifier()\nmodel_u.fit(x_train1, y_train1)\n\ny_pred = model_u.predict(x_test1)\n\nprint(\"Training Accuracy: \", model_u.score(x_train1, y_train1))\nprint('Testing Accuarcy: ', model_u.score(x_test1, y_test1))\n\n# confusion matrix\ncm = confusion_matrix(y_test1, y_pred)\nplt.rcParams['figure.figsize'] = (5, 5)\nsns.heatmap(cm, annot = True, cmap = 'winter')\nplt.show()\n\n# classification report\ncr = classification_report(y_test1, y_pred)\nprint(cr)","bf3002d6":"from imblearn.over_sampling import SMOTE\n\nx_resample, y_resample  = SMOTE().fit_sample(x, y.values.ravel())\n\nprint(x_resample.shape)\nprint(y_resample.shape)","0839c7ca":"from sklearn.model_selection import train_test_split\n\nx_train2, x_test2, y_train2, y_test2 = train_test_split(x_resample, y_resample, test_size = 0.2, random_state = 0)\n\nprint(x_train2.shape)\nprint(y_train2.shape)\nprint(x_test2.shape)\nprint(y_test2.shape)","dd513712":"# standardization\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nx_train2 = sc.fit_transform(x_train2)\nx_test2 = sc.transform(x_test2)","4076dcff":"# Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel_o = RandomForestClassifier()\nmodel_o.fit(x_train2, y_train2)\n\ny_pred = model_o.predict(x_test2)\n\nprint(\"Training Accuracy: \", model_o.score(x_train2, y_train2))\nprint('Testing Accuarcy: ', model_o.score(x_test2, y_test2))\n\n# confusion matrix\ncm = confusion_matrix(y_test2, y_pred)\nplt.rcParams['figure.figsize'] = (5, 5)\nsns.heatmap(cm, annot = True, cmap = 'winter')\nplt.show()\n\n# classification report\ncr = classification_report(y_test2, y_pred)\nprint(cr)","f91fdfb5":"# let's check the importance of each attributes\n\nfrom eli5.sklearn import PermutationImportance\n\n\nperm = PermutationImportance(model, random_state = 0).fit(x_test, y_test)\neli5.show_weights(perm, feature_names = x_test.columns.tolist())","b11d5555":"from pdpbox import pdp, info_plots #for partial plots\n\nbase_features = x_train.columns.values.tolist()\n\nfeat_name = 'incident_severity'\npdp_dist = pdp.pdp_isolate(model=model, dataset=x_test, model_features = base_features, feature = feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","92dde9f7":"from pdpbox import pdp, info_plots #for partial plots\n\nbase_features = x_train.columns.values.tolist()\n\nfeat_name = 'collision_type'\npdp_dist = pdp.pdp_isolate(model=model, dataset=x_test, model_features = base_features, feature = feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","935f3453":"from pdpbox import pdp, info_plots #for partial plots\n\nbase_features = x_train.columns.values.tolist()\n\nfeat_name = 'incident_severity'\npdp_dist = pdp.pdp_isolate(model=model, dataset=x_test, model_features = base_features, feature = feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","e5101b15":"from pdpbox import pdp, info_plots #for partial plots\n\nbase_features = x_train.columns.values.tolist()\n\nfeat_name = 'insured_zip'\npdp_dist = pdp.pdp_isolate(model=model, dataset=x_test, model_features = base_features, feature = feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","4b3d2564":"from pdpbox import pdp, info_plots #for partial plots\n\nbase_features = x_train.columns.values.tolist()\n\nfeat_name = 'age'\npdp_dist = pdp.pdp_isolate(model=model, dataset=x_test, model_features = base_features, feature = feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","3754d46e":"# let's see the shap values\n\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(x_test)\n\nshap.summary_plot(shap_values[1], x_test, plot_type=\"bar\")","96f754e3":"shap.summary_plot(shap_values[1], x_test)","5de7f53e":"## Reading the Dataset","10cd294a":"**Under Sampling**","71ef8164":"## Fraud Detection","c94002a9":"## Modelling with Ensemble of Samplers","02e8ae4b":"**Bagging Classifier**","d3e5dc91":"**Boosting the Predictions of above Models**","68932253":"# ","f078a092":"## Data Processing","76279745":"**Scatter Plot between Policy annual premium vs total claim amount**","ba3bff4b":"<img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcS1B1mo09UtC5o_GcLgxRAg4Q6rd3ehoR9r0Z-9lI55nyGp4F-GRA\" width=\"400px\">","d45a2199":"**Easy Ensemble Classifier**","6c94ac2e":"## Data Visualizations","965ca508":"## Importing the Libraries","32f8f868":"## Data Cleaning","f5bdb05a":"**Random Forest Classifier**","15c7d03e":"__Most of the Customers Age lies betwenn 25-50 years__","4eb1cb05":"## Model Explanation for Random Forest Classifier","62efdde3":"__In above visualzation the data follow normal distribution__\n","d6dce13f":"**Over Sampling with SMOTE** ","6c0c2162":"__Highest vehicle damage claim by Audi__","118403b6":"## Applying Sampling Techniques","841832e0":"## Descriptive Statistics","52d36852":"__ONly 24.7% fraud reported__"}}