{"cell_type":{"77295315":"code","26a69dea":"code","f87f14ba":"code","86e90e47":"code","b14d71d8":"code","02c7fe8d":"code","ee42b4e4":"code","9f9409ee":"code","e1c55d1c":"code","042ebbb5":"code","60cc8b64":"code","e9ff9245":"code","1caece0f":"code","c8398543":"code","153a595f":"code","06cf9685":"markdown"},"source":{"77295315":"# import all packages\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nimport zipfile\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","26a69dea":"with zipfile.ZipFile(\"\/kaggle\/input\/sberbank-russian-housing-market\/train.csv.zip\", 'r') as zip_ref:\n    zip_ref.extractall(\"\/data\")","f87f14ba":"os.listdir(\"\/data\")","86e90e47":"# read the data\ndf = pd.read_csv('\/data\/train.csv')\n\ndf.head()","b14d71d8":"# import packages \nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.preprocessing import MinMaxScaler\n\n# simple data cleaning\nX = df.select_dtypes('number').drop(['id', 'price_doc'], axis=1)\ny = np.log1p(df['price_doc'])\n\n# impute the missing values and create the missing value indicator variables for each numeric column.\nnumeric_cols = X.columns.values\n\nfor col in numeric_cols:\n    missing = X[col].isnull()\n    num_missing = np.sum(missing)\n\n    if num_missing > 0:  \n        print('Imputing missing values for: {}'.format(col))\n        df['{}_ismissing'.format(col)] = missing\n        med = X[col].median()\n        X[col] = X[col].fillna(med)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n\n\n# Need to scale the features for neural networks, otherwise the training doesn't converge.\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\n\nX_test_scaled = scaler.transform(X_test)","02c7fe8d":"# This returns a multi-layer-perceptron model in Keras.\ndef get_keras_model(num_hidden_layers, \n                    num_neurons_per_layer, \n                    dropout_rate, \n                    activation):\n    # create the MLP model.\n    \n    # define the layers.\n    inputs = tf.keras.Input(shape=(X_train_scaled.shape[1],))  # input layer.\n    x = layers.Dropout(dropout_rate)(inputs) # dropout on the weights.\n    \n    # Add the hidden layers.\n    for i in range(num_hidden_layers):\n        x = layers.Dense(num_neurons_per_layer, \n                         activation=activation)(x)\n        x = layers.Dropout(dropout_rate)(x)\n    \n    # output layer.\n    outputs = layers.Dense(1, activation='linear')(x)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    \n    return model\n    \n\n# This function takes in the hyperparameters and returns a score (Cross validation).\ndef keras_mlp_cv_score(parameterization, weight=None):\n    \n    model = get_keras_model(parameterization.get('num_hidden_layers'),\n                            parameterization.get('neurons_per_layer'),\n                            parameterization.get('dropout_rate'),\n                            parameterization.get('activation'))\n    \n    opt = parameterization.get('optimizer')\n    opt = opt.lower()\n    \n    learning_rate = parameterization.get('learning_rate')\n    \n    if opt == 'adam':\n        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    elif opt == 'rms':\n        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n    else:\n        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n    \n    NUM_EPOCHS = 50\n    \n    # Specify the training configuration.\n    model.compile(optimizer=optimizer,\n                  loss=tf.keras.losses.MeanSquaredError(),\n                  metrics=['mse'])\n\n    data = X_train_scaled\n    labels = y_train.values\n    \n    # fit the model using a 20% validation set.\n    res = model.fit(data, labels, epochs=NUM_EPOCHS, batch_size=parameterization.get('batch_size'),\n                    validation_split=0.2)\n    \n    # look at the last 10 epochs. Get the mean and standard deviation of the validation score.\n    last10_scores = np.array(res.history['val_loss'][-10:])\n    mean = last10_scores.mean()\n    sem = last10_scores.std()\n    \n    # If the model didn't converge then set a high loss.\n    if np.isnan(mean):\n        return 9999.0, 0.0\n    \n    return mean, sem","ee42b4e4":"# Define the search space.\nparameters=[\n    {\n        \"name\": \"learning_rate\",\n        \"type\": \"range\",\n        \"bounds\": [0.0001, 0.5],\n        \"log_scale\": True,\n    },\n    {\n        \"name\": \"dropout_rate\",\n        \"type\": \"range\",\n        \"bounds\": [0.01, 0.5],\n        \"log_scale\": True,\n    },\n    {\n        \"name\": \"num_hidden_layers\",\n        \"type\": \"range\",\n        \"bounds\": [1, 10],\n        \"value_type\": \"int\"\n    },\n    {\n        \"name\": \"neurons_per_layer\",\n        \"type\": \"range\",\n        \"bounds\": [1, 300],\n        \"value_type\": \"int\"\n    },\n    {\n        \"name\": \"batch_size\",\n        \"type\": \"choice\",\n        \"values\": [8, 16, 32, 64, 128, 256],\n    },\n    \n    {\n        \"name\": \"activation\",\n        \"type\": \"choice\",\n        \"values\": ['tanh', 'sigmoid', 'relu'],\n    },\n    {\n        \"name\": \"optimizer\",\n        \"type\": \"choice\",\n        \"values\": ['adam', 'rms', 'sgd'],\n    },\n]","9f9409ee":"!pip install ax-platform","e1c55d1c":"# import more packages\nfrom ax.service.ax_client import AxClient\nfrom ax.utils.notebook.plotting import render, init_notebook_plotting\n\ninit_notebook_plotting()\n\nax_client = AxClient()\n\n# create the experiment.\nax_client.create_experiment(\n    name=\"keras_experiment\",\n    parameters=parameters,\n    objective_name='keras_cv',\n    minimize=True)\n\ndef evaluate(parameters):\n    return {\"keras_cv\": keras_mlp_cv_score(parameters)}","042ebbb5":"for i in range(10):\n    parameters, trial_index = ax_client.get_next_trial()\n    ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate(parameters))","60cc8b64":"# look at all the trials.\nax_client.get_trials_data_frame().sort_values('trial_index')","e9ff9245":"best_parameters, values = ax_client.get_best_parameters()\n\n# the best set of parameters.\nfor k in best_parameters.items():\n    print(k)\n\nprint()\n\n# the best score achieved.\nmeans, covariances = values\nprint(means)","1caece0f":"render(ax_client.get_optimization_trace()) # Objective_optimum is optional.\n\n# Cannot do contour plot because it doesn't use a GP model.","c8398543":"# train the model on the full training set and test.\nkeras_model = get_keras_model(best_parameters['num_hidden_layers'], \n                              best_parameters['neurons_per_layer'], \n                              best_parameters['dropout_rate'],\n                              best_parameters['activation'])\n\nopt = best_parameters['optimizer']\nopt = opt.lower()\n\nlearning_rate = best_parameters['learning_rate']\n\nif opt == 'adam':\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\nelif opt == 'rms':\n    optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\nelse:\n    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n\nNUM_EPOCHS = 50\n\n# Specify the training configuration.\nkeras_model.compile(optimizer=optimizer,\n              loss=tf.keras.losses.MeanSquaredError(),\n              metrics=['mse'])\n\ndata = X_train_scaled\nlabels = y_train.values\nres = keras_model.fit(data, labels, epochs=NUM_EPOCHS, batch_size=best_parameters['batch_size'])\n","153a595f":"# Use the model to predict the test values.\ntest_pred = keras_model.predict(X_test_scaled)\nprint(\"MSE with best params: {:.2f}\".format(mean_squared_error(y_test.values, test_pred)))","06cf9685":"### Parameters:\n* num_hidden_layers \n* neurons_per_layer\n* dropout_rate\n* activation\n* optimizer\n* learning_rate\n* batch_size"}}