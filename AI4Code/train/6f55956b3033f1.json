{"cell_type":{"7a046be6":"code","767feac5":"code","669f1fdf":"code","89d893d6":"code","ad1cbb02":"code","1e25da2c":"code","ba7a9016":"code","6d256cce":"code","b6e339b9":"code","fcd6d855":"code","cdfe0a7e":"code","c5153c60":"code","b1d018d6":"code","73013f97":"code","de0f3403":"code","fcffbac3":"code","834bd514":"code","862ae360":"code","1aea645a":"code","439a0851":"code","72eb4dca":"code","1dfb1484":"code","3802a77b":"code","53c7d19d":"code","9ee5a27f":"code","29f959df":"code","ec8e018e":"code","798f13ac":"code","e361c853":"code","516b1c10":"code","0b7fde56":"code","c9b8f6df":"code","b9c0a8dd":"code","5715a8ae":"code","680e28dc":"code","13ccdce6":"code","9445f8b4":"code","0934803a":"code","edaeee41":"code","556e0c60":"code","a9f8448b":"code","8ba8751e":"code","afde5ce9":"code","2a8550e5":"code","11a1b932":"code","efc9147f":"code","33840726":"code","ae343dc0":"code","dcb57a61":"code","8f8b1d76":"code","112538f8":"code","1e8c3dcc":"code","4160b52c":"code","a5aaa107":"code","b4064abe":"code","129602b0":"code","062b96a5":"code","33d58381":"code","1e7027d5":"code","6b3fdfc5":"code","0c4cf63c":"code","0ec207c7":"code","95fa7a2d":"code","0a6bc5d7":"code","e98a1ad9":"code","f359c31e":"code","3e571a26":"code","6f627fae":"code","c137a905":"code","411382e7":"code","b61942c8":"code","fd726732":"code","918ae454":"code","e412ea62":"code","d605c59e":"code","0fd3cf73":"code","ddd30d2d":"code","35c90726":"code","7fa3d17a":"code","d1552f36":"code","93d15b30":"code","fd003eeb":"code","1e3dad3e":"code","2d1534c4":"code","071880c1":"code","686b2de7":"code","d87f8085":"code","3d27ee2b":"code","050bbb95":"code","d65bd5d1":"markdown","e6fcfa27":"markdown","4f882e02":"markdown","b407918d":"markdown","7e6450f1":"markdown","e2c0d447":"markdown","64107241":"markdown","9948fb07":"markdown","10fa6c7f":"markdown","8e956b51":"markdown","a57fc878":"markdown","6e5a6a63":"markdown","5f03b3ee":"markdown","5f239304":"markdown"},"source":{"7a046be6":"# Import general libraries such as pandas, numpy, matplotlib.pyplot and seaborn, that will be used in the program.\nimport pandas as pd\nimport numpy as  np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport ast","767feac5":"# Read the data set from local into a dataframe called df.\ndf=pd.read_csv('..\/input\/data.csv')","669f1fdf":"# Check the dataframe\ndf.head(5)","89d893d6":"#Checkout the dataframe columns\ndf.columns","ad1cbb02":"# I'm going to list all of JSON columns\n# CustomDimensions device geoNetwork hits trafficSource","1e25da2c":"#I'm going to check out each JSON column\ndf['customDimensions'][0]","ba7a9016":"df['device'][0]","6d256cce":"df['trafficSource'][0]","b6e339b9":"df['geoNetwork'][0]","fcd6d855":"df['hits'][0]","cdfe0a7e":"# As shown above in the dataset, because we have many variables consist of dictionaries or lists.\n# We need to convert the columns and split into a normal column version.\n#import json and to convert the columns\nimport json\nfrom pandas.io.json import json_normalize","c5153c60":"# Read all data of list of dict in the column customDimensions and resign it to a new customDimensions column\ndf['customDimensions']=df['customDimensions'].apply(ast.literal_eval)\n# convert the list of dict into dict and resign\ndf['customDimensions']=df['customDimensions'].str[0]\n# format null-value data\ndf['customDimensions']=df['customDimensions'].apply(lambda x: {'index':np.NaN,'value':np.NaN} if pd.isnull(x) else x)","b1d018d6":"# Using json to normalize the dict colomn into a dataframe\ndf_customDimensions = json_normalize(df['customDimensions'])","73013f97":"# define the name of columns\ndf_customDimensions.columns = [f\"customDimension_{subcolumn}\" for subcolumn in df_customDimensions.columns]","de0f3403":"#check the shape of the dataframe\ndf_customDimensions.shape","fcffbac3":"# Read all data of list of dict in the column customDimensions and resign it to a new hits column\ndf['hits']=df['hits'].apply(ast.literal_eval)\n# convert the list of dict into dict and resign\ndf['hits']=df['hits'].str[0]\n# format null-value data\ndf['hits']=df['hits'].apply(lambda x: {'index':np.NaN,'value':np.NaN} if pd.isnull(x) else x)","834bd514":"# Using json to normalize the dict colomn into a dataframe\ndf_hits = json_normalize(df['hits'])","862ae360":"# define the name of columns\ndf_hits.columns = [f\"hits_{subcolumn}\" for subcolumn in df_hits.columns]","1aea645a":"#check the shape of the dataframe\ndf_hits.shape","439a0851":"# Using json to normalize the dict colomn into a dataframe\n#check the shape of the dataframe\ndf_device=json_normalize(df['device'].apply(eval))\ndf_device.shape","72eb4dca":"# Using json to normalize the dict colomn into a dataframe\n#check the shape of the dataframe\ndf_trafficSource=json_normalize(df['trafficSource'].apply(eval))\ndf_trafficSource.shape","1dfb1484":"# Using json to normalize the dict colomn into a dataframe\n#check the shape of the dataframe\ndf_geoNetwork=json_normalize(df['geoNetwork'].apply(eval))\ndf_geoNetwork.shape","3802a77b":"# define a new dataframe which is a concat of all five json dataframe then delete original redundent columns \ndf=pd.concat([df,df_device,df_customDimensions,df_geoNetwork,df_hits,df_trafficSource],axis=1,sort=True)\ndf.drop(columns=['device','customDimensions','geoNetwork','trafficSource','hits'],inplace=True)","53c7d19d":"df.head()","9ee5a27f":"# define a methond a find all missing values in the dataframe\ndef find_missing(df):\n    count_missing=df.isnull().sum().values\n    total=df.shape[0]\n    ratio_missing=count_missing\/total\n    return pd.DataFrame({'missing':count_missing,'missing_ratio':ratio_missing},index=df.columns)","29f959df":"# define a dataframe called df_missing and use the function defined in the last step with the df dataframe\ndf_missing=find_missing(df)","ec8e018e":"# Check the dataframe df_missing\ndf_missing[df_missing['missing_ratio']>0].sort_values('missing_ratio',ascending=True).head(30)","798f13ac":"#Then I'm going to visualize the missing data\nmissing_values = df.isnull().sum(axis=0).reset_index()\nmissing_values.columns = ['column_name', 'missing_count']\nmissing_values = missing_values.loc[missing_values['missing_count']>0]\nmissing_values = missing_values.sort_values(by='missing_count')\nind = np.arange(missing_values.shape[0])\nwidth = 0.1\nfig, ax = plt.subplots(figsize=(12,3))\nrects = ax.barh(ind, missing_values.missing_count.values, color='b')\nax.set_yticks(ind)\nax.set_yticklabels(missing_values.column_name.values, rotation='horizontal')\nax.set_xlabel(\"Missing Observations Count\")\nax.set_title(\"Missing Categorical Observations in Train Dataset\")\nplt.show()","e361c853":"#Then I'm going to delete columns containing more than 90 percent.\ndf_missing[df_missing['missing_ratio']>0].sort_values('missing_ratio',ascending=False).head(20)","516b1c10":"#Create a list of columns those are going to be deleted except target variable.\nmissing_drop=['hits_index','hits_value','hits_promotionActionInfo.promoIsClick','hits_page.searchCategory',\n             'hits_page.searchKeyword','hits_eventInfo.eventLabel','hits_eventInfo.eventCategory',\n             'hits_eventInfo.eventAction','hits_contentGroup.contentGroupUniqueViews1','totals_totalTransactionRevenue',\n             'totals_transactions','adContent','hits_contentGroup.contentGroupUniqueViews3','adwordsClickInfo.adNetworkType',\n             'adwordsClickInfo.isVideoAd','adwordsClickInfo.page','adwordsClickInfo.slot','adwordsClickInfo.gclId']","0b7fde56":"#Then define a new dataframe called df_missing_drop and drop all columns defined at the last step.\ndf_missing_drop=df.drop(columns=missing_drop)\ndf_missing_drop.head()","c9b8f6df":"# Then I'm going to fill all missing values with 0 and create a new dataframe called df_missing_replace\ndf_missing_replace=df_missing_drop.fillna(0)\ndf_missing_replace.head()","b9c0a8dd":"# As shown in the dataframe, we can find the data consists of both numeric and catagorical variables. So I'm going to \n# preprocess the data within two steps interm of numeric and catagorical variables.\n# Check the numeric data in the dataset.\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ndf_numerics_row = df_missing_replace.select_dtypes(include=numerics)\ndf_numerics_row.head()","5715a8ae":"# Then I'm going to delete some incorrelate columns within the dataframe and then define a new dataframe to store the data\nnumerics_columns_drop=['date','visitId','visitStartTime']\ndf_numerics_drop=df_missing_replace.drop(columns=numerics_columns_drop)\ndf_numerics_drop.head()","680e28dc":"# Then I'm going to deal with the catagorical variables\n# look at the whole dataset to find which columns contain list cells.\nfor col in df_numerics_drop.columns:\n    try:\n        print(col, ':', df_numerics_drop[col].nunique(dropna=False))\n    except TypeError:\n        a=df_numerics_drop[col].astype('str')\n        #print(a)\n        print( col, ':', a.nunique(dropna=False), ' >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> LIST')","13ccdce6":"# As we can see in the output, there are seven columns containing list data. So I'm going to delete these columns firstly.\nlist_drop=['hits_customDimensions','hits_customMetrics','hits_customVariables','hits_experiment',\n          'hits_product','hits_promotion','hits_publisher_infos']\ndf_list_drop=df_numerics_drop.drop(columns=list_drop)\ndf_list_drop.head()","9445f8b4":"# Then I'm going to drop more incorrelate catagorical variables in the dataframe.\n# look through the feature and define lists of columns which are needed to be deleted for orginal variables.\nsocialEngagementType=['socialEngagementType']\ncustomDimensions=['customDimension_index']\nfullVistorId=['fullVisitorId']\ndevice=['browserSize','browserVersion','flashVersion','language','mobileDeviceBranding','mobileDeviceInfo',\n        'mobileDeviceMarketingName','mobileDeviceModel','mobileInputSelector','operatingSystemVersion',\n        'screenColors','screenResolution']\ngeoNetwork=['cityId','metro','region','networkDomain','networkLocation','longitude','latitude']\ndf_socialEngagementType_drop=df_list_drop.drop(columns=socialEngagementType)\ndf_customDimensions_drop=df_socialEngagementType_drop.drop(columns=customDimensions)\ndf_fullVistorID_drop=df_customDimensions_drop.drop(columns=fullVistorId)\ndf_device_drop=df_fullVistorID_drop.drop(columns=device)\ndf_geoNetwork_drop=df_device_drop.drop(columns=geoNetwork)\ndf_geoNetwork_drop.head()","0934803a":"# Then I'm going to look through other features to figure out if there is any other features to delete\nfor col in df_geoNetwork_drop.columns:\n    print(col,\" : \",df_geoNetwork_drop[col].unique())\n    print(\"=================================\")","edaeee41":"# Then I'm going to create a list of columns  which are going to be deleted in the next step\n# Those columns are going to be which contains large variance and those just have \"not available\" cells.\nother_drop=['operatingSystem','hits_appInfo.exitScreenName',\n           'hits_appInfo.landingScreenName','hits_page.pagePath','hits_page.pagePathLevel1','hits_page.pagePathLevel2',\n           'hits_page.pagePathLevel3','hits_page.pagePathLevel4','hits_page.pageTitle','hits_referer',\n            'hits_social.socialInteractionNetworkAction','hits_transaction.currencyCode','campaign','keyword',\n            'referralPath','source','hits_appInfo.screenDepth','hits_contentGroup.contentGroup4','hits_contentGroup.contentGroup5',\n           'adwordsClickInfo.criteriaParameters','hits_item.currencyCode','hits_contentGroup.contentGroup2',\n            'hits_contentGroup.contentGroup4','hits_contentGroup.contentGroup5','hits_contentGroup.previousContentGroup1',\n           'hits_contentGroup.previousContentGroup2','hits_contentGroup.previousContentGroup3',\n           'hits_contentGroup.previousContentGroup4','hits_contentGroup.previousContentGroup5','hits_hitNumber',\n           'hits_hour','hits_eCommerceAction.action_type','hits_dataSource','hits_minute','hits_page.hostname','hits_isEntrance',\n           'hits_isExit','hits_isInteraction','hits_social.socialNetwork','hits_time','continent','hits_promotionActionInfo.promoIsView',\n           'isTrueDirect','medium','hits_social.hasSocialSourceReferral','hits_appInfo.screenName','hits_contentGroup.contentGroupUniqueViews2',\n            'hits_eCommerceAction.step','hits_contentGroup.contentGroup3','hits_contentGroup.contentGroup1']\ndf_all_drop=df_geoNetwork_drop.drop(columns=other_drop)\ndf_all_drop_total=df_all_drop[df_all_drop['totals_transactionRevenue']>0]\ndf_all_drop_total.head()","556e0c60":"# After data cleansing, the next step I'm going to do is to scale the data with a proper scaling model.\n# I'm going to use Dicvectorizer as my scaling model because in this dataset it has both numeric and catagorical \n# variables\n# Then I'm going to assign this dataframe from the scaling process (with transaction_revenue>0) to the final \n# dataframe as my main dataframe to continue on work.\n# Split features and target into different dataframes and also give data scaling\nfrom sklearn.feature_extraction import DictVectorizer\nvec1 = DictVectorizer(sparse=False, dtype=int)\nvec2 = DictVectorizer(sparse=False, dtype=int)\nvec3 = DictVectorizer(sparse=False, dtype=int)\nscaled_df=df_all_drop_total.drop(columns='totals_transactionRevenue')\nscaled_target=df_all_drop_total[['totals_transactionRevenue']]\nscaled_data_all=vec1.fit_transform(df_all_drop_total.to_dict('records'))\nscaled_data1=vec2.fit_transform(scaled_df.to_dict('records'))\nscaled_data_target=vec3.fit_transform(scaled_target.to_dict('records'))                                \ndf_final=pd.DataFrame(scaled_data_all,columns=vec1.get_feature_names())\ndf_scaled=pd.DataFrame(scaled_data1,columns=vec2.get_feature_names())\ndf_target=pd.DataFrame(scaled_target,columns=vec3.get_feature_names())\ndf_final.head()","a9f8448b":"# Have a look on correlations between varied features\nfx=plt.figure(figsize=(12,6))\nsns.heatmap(df_final.corr(),cmap='coolwarm')","8ba8751e":"# Import train_test_split \n# Then assign all features except the target feature totals_transactionRevenue as X\n# Assign totals_transactionRevenue as y\n# Split the dataframe into X_train,X_test,y_train,y_test and give a test size as 0.25\nfrom sklearn.model_selection import train_test_split\nX=df_final.drop(columns='totals_transactionRevenue')\ny=df_final['totals_transactionRevenue']\nX_train, X_test, y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=101)","afde5ce9":"# First, I'm tring to find the best parameters for the linear regression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nparam_grid = {'fit_intercept':[True,False],\n              'normalize':[True,False], \n              'copy_X':[True, False]}\ngrid = GridSearchCV(LinearRegression(), param_grid, cv=7)\ngrid.fit(X,y)\ngrid.best_params_","2a8550e5":"# Then I'm going to run a supervised model which is linear regression\nmodel=LinearRegression(copy_X=True,fit_intercept=False,normalize=True)\nmodel.fit(X_train,y_train)","11a1b932":"# I'm going to use regression evaluation metrics to evaluate the model which are Mean Absolute Error (MAE),\n# Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)\nfrom sklearn import metrics","efc9147f":"# Check out the coefficient of the model\nmodel.coef_","33840726":"# Check out the intercept of the model\nmodel.intercept_","ae343dc0":"# Predict the totals_transactionRevenue and plot the prediction\npredictions=model.predict(X_test)\nplt.scatter(y_test,predictions)\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Revenue')","dcb57a61":"# Print three metrics\n# We can see both of the three metrics are large so the regression model is not good\nprint('MAE:',metrics.mean_absolute_error(y_test,predictions))\nprint('MSE:',metrics.mean_squared_error(y_test,predictions))\nprint(\"RMSE:\",np.sqrt(metrics.mean_squared_error(y_test,predictions)))","8f8b1d76":"#Check the r square which shows the model is overfit\nfrom sklearn.metrics import r2_score\nr2_score(y_test, predictions)","112538f8":"# print out the residual histogram to see the difference between predcitons and actual dataset.\nprint(predictions)\nsns.distplot(tuple(y_test-predictions),bins=50)","1e8c3dcc":"# Define a dataframe to see top 10 features having large coeffecient.\ncoeffecients=pd.DataFrame(model.coef_,X.columns)\ncoeffecients.columns=['Coeffecient']\ncoeffecients.sort_values('Coeffecient',ascending=False).head(10)","4160b52c":"# First, I'm tring to find the best parameters for the KNN\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\ngridlog = GridSearchCV(LogisticRegression(), param_grid,cv=None)\ngridlog.fit(X,y)\ngridlog.best_params_","a5aaa107":"# Then I'm going to create a model called logmodel with the best parameters.\nlogmodel = LogisticRegression(C=0.001)\nlogmodel.fit(X_train,y_train)","b4064abe":"# make predictions with the logistic regression model\npredictions = logmodel.predict(X_test)\npredictions","129602b0":"# I'm going to use classsification report to evaluate the prediction model.\n# As we can see in the report, because the target feature is not binary, and also because the amount of predicting variables\n# is huge. So the logistic model does not make any sence about the prediction.\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,predictions))\nsns.distplot(tuple(y_test-predictions),bins=50)","062b96a5":"#Check the r square which shows the model is overfit\nfrom sklearn.metrics import r2_score\nr2_score(y_test, predictions)","33d58381":"# Then I'm going to use an unsupervised learning model called pca to reduce dimensionality\n# I'm going to try to see how many components should be assigned\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nscaler.fit(scaled_data1)\nscaled_data=scaler.transform(scaled_data1)\npca = PCA().fit(scaled_data)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');","1e7027d5":"# As we can see above, around 8 components can expalain most variance,but at this step, I'm going to assign components \n# as 20.\npca=PCA(n_components=20)\npca.fit(scaled_data)\nx_pca=pca.transform(scaled_data)\nplt.figure(figsize=(8,6))\nplt.scatter(x_pca[:,0],x_pca[:,1],c=df_final['totals_transactionRevenue'],cmap='plasma')\nplt.xlabel('First Principle Component')\nplt.ylabel('Second Principle Component')","6b3fdfc5":"# Check out the components\n#In this numpy matrix array, each row represents a principal component, and each column relates back to the \n#original features\npca.components_","0c4cf63c":"# Check out the components dataframe\ndf_comp=pd.DataFrame(pca.components_,columns=df_scaled.columns)\ndf_comp.head()","0ec207c7":"# Using heatmap to represent the correlation between varied features and then principle component itself.\nplt.figure(figsize=(15,6))\nsns.heatmap(df_comp,cmap='plasma')","95fa7a2d":"# Then I'm going to use K-Means clustering unserpervied learning model.\n# Create a K-means model with 4 clusters\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=4)\nkmeans.fit(df_final.drop(columns='totals_transactionRevenue',axis=1))","0a6bc5d7":"# Check out the cluster center\nkmeans.cluster_centers_","e98a1ad9":"# Have a look on scatter plot to see the difference from dataset before and after clustering.\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True,figsize=(10,6))\nax1.set_title('K Means')\nax1.scatter(df_final.iloc[:,-4],df_final.iloc[:,-3],c=kmeans.labels_,cmap='rainbow')\nax2.set_title(\"Original\")\nax2.scatter(df_final.iloc[:,-4],df_final.iloc[:,-3],c=df_final['totals_transactionRevenue'],cmap='rainbow')","f359c31e":"# Define a method to convert the clustering model.\ndef converter(revenue):\n    if 1000000<revenue<10000000:\n        return 0\n    elif 10000000<=revenue<1000000000:\n        return 1\n    elif 100000000<=revenue<1000000000:\n        return 2\n    else:\n        return 3","3e571a26":"# Add a new column cluster\ndf_final_cluster=df_final\ndf_final_cluster['Cluster'] = df_final_cluster['totals_transactionRevenue'].apply(converter)","6f627fae":"# Check the dataframe\ndf_final_cluster.head()","c137a905":"# Using confusion matrix and classification report to evaluate the model.\nfrom sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(df_final['Cluster'],kmeans.labels_))\nprint(classification_report(df_final['Cluster'],kmeans.labels_))","411382e7":"# In this step, I'm going to run another pca model which contains 10 principle components\nfrom sklearn.decomposition import PCA  \nmodel = PCA(n_components=10)           \nmodel.fit(scaled_data)                      \nX_2D = model.transform(scaled_data) \ndf_final_pca=df_final","b61942c8":"# Assign two columns PCA1 PCA2 into the dataframe and visualize the PCA\ndf_final_pca['PCA1'] = X_2D[:, 0]\ndf_final_pca['PCA2'] = X_2D[:, 1]\nsns.lmplot(\"PCA1\", \"PCA2\", hue='totals_transactionRevenue', data=df_final,legend=False,fit_reg=False)","fd726732":"# I'm going to create a clustering model which contains 4 clusters\nfrom sklearn.mixture import GaussianMixture                     \nmodel = GaussianMixture(n_components=4, covariance_type='full')  \nmodel.fit(scaled_data1)                                               \ny_gmm = model.predict(scaled_data1)                                    ","918ae454":"# Assign a new column cluster in the dataframe and visualize the clustering\ndf_cluster=df_final\ndf_cluster['cluster'] = y_gmm\nsns.lmplot(\"PCA1\", \"PCA2\", data=df_cluster, hue='totals_transactionRevenue', \n           col='cluster', fit_reg=False, palette = 'tab10',legend=False);","e412ea62":"# Now Im going to use a new pca to repredict with linear regression and logistic regression \n# Reassign the df_final dataframe\ndf_final=df_final.drop(columns=['Cluster','cluster','PCA1','PCA2'])\ndf_final.head()","d605c59e":"# I'm going to use the pca created in the previous step to predict the data but make make n_components to be 40.\nfrom sklearn.decomposition import PCA\npca1=PCA(n_components=40)\npca1.fit(scaled_data)\nx_pca1=pca1.transform(scaled_data)\nplt.figure(figsize=(8,6))\nplt.scatter(x_pca1[:,0],x_pca1[:,1],c=df_final['totals_transactionRevenue'],cmap='plasma')\nplt.xlabel('First Principle Component')\nplt.ylabel('Second Principle Component')","0fd3cf73":"# Check out the components\npca1.components_","ddd30d2d":"#Assign a new dataframe and store all components then print it out.\ndf_comp1=pd.DataFrame(pca1.components_,columns=df_scaled.columns)\ndf_comp1.head()","35c90726":"# Using heatmap to represent the correlation between varied features and then principle component itself.\nplt.figure(figsize=(15,6))\nsns.heatmap(df_comp1,cmap='viridis')","7fa3d17a":"# Rerun the linear regression with the same parameters as preivious one and use the result of pca instead of original data\nfrom sklearn.model_selection import train_test_split\nX_scaled=x_pca1\ny_scaled=scaled_data_target\nXtrain, Xtest, ytrain,ytest=train_test_split(X_scaled,y_scaled,test_size=0.2,random_state=101)","d1552f36":"from sklearn.linear_model import LinearRegression\nmodel1=LinearRegression(copy_X=True,fit_intercept=False,normalize=True)\nmodel1.fit(Xtrain,ytrain)","93d15b30":"# Check the coefficient\nmodel1.coef_","fd003eeb":"model1.intercept_","1e3dad3e":"# Plot the linear regression model\npredictions1=model1.predict(Xtest)\nplt.scatter(ytest,predictions1)\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Revenue')","2d1534c4":"# Evaluate the result\nprint('MAE:',metrics.mean_absolute_error(ytest,predictions1))\nprint('MSE:',metrics.mean_squared_error(ytest,predictions1))\nprint(\"RMSE:\",np.sqrt(metrics.mean_squared_error(ytest,predictions1)))","071880c1":"#Check the r square which shows the model is overfit\nfrom sklearn.metrics import r2_score\nr2_score(ytest, predictions1)","686b2de7":"sns.distplot(tuple(ytest-predictions1),bins=10)","d87f8085":"# Rerun the logistic regression with the same parameters as privious one and use the result from pca\nfrom sklearn.model_selection import GridSearchCV\nlogmodel1 = LogisticRegression(C=0.001)\nlogmodel1.fit(X_scaled,y_scaled)","3d27ee2b":"# Check the predictions\npredictions_log = logmodel1.predict(Xtest)\npredictions_log","050bbb95":"# Evaluate the result\n# We can see the difference from this and previous one although the model is not good enough as well.\nfrom sklearn.metrics import classification_report\nprint(classification_report(ytest,predictions_log))","d65bd5d1":"***Dimensionality reduction***","e6fcfa27":"### Supervised learning Model","4f882e02":"### Running Supervised Models","b407918d":"***PCA***","7e6450f1":"### Dimensionality Reduction","e2c0d447":"***Logistic Regression***","64107241":"### Split dataset into train and test dataset","9948fb07":"### Unsupervised Learning","10fa6c7f":"***Linear Regression***","8e956b51":"***Unsupervised Learning Dimensionality***","a57fc878":"***K-Means clusering***","6e5a6a63":"***Linear Regression***","5f03b3ee":"***Logistic Regression***","5f239304":"***Unsupervised Learning Clustering***"}}