{"cell_type":{"40192e2e":"code","45b13005":"code","20ae8d88":"code","6e019de1":"code","d638459f":"code","a72704f2":"code","e7fb1b7b":"code","ba2b48a0":"code","ce818c5a":"code","d65eb578":"code","0ddbb138":"code","bd981d0b":"code","0f8af65d":"code","c1b4dafc":"code","6965ebc4":"markdown","8377df3e":"markdown"},"source":{"40192e2e":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nimport lightgbm as lgb\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\nimport pickle\nimport os\nimport gc\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\ngc.enable()","45b13005":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","20ae8d88":"def pre_pro(df):\n    df = df.astype('float32')\n    col = df.columns\n    for i in range(len(col)):\n        m = df.loc[df[col[i]] != -np.inf, col[i]].min()\n        df[col[i]].replace(-np.inf,m,inplace=True)\n        M = df.loc[df[col[i]] != np.inf, col[i]].max()\n        df[col[i]].replace(np.inf,M,inplace=True)\n    \n    df.fillna(0, inplace = True)\n    return df ","6e019de1":"def feat_eng(df):\n    df.replace(0, 0.001)\n    \n    df['follower_diff'] = (df['A_follower_count'] > df['B_follower_count'])\n    df['following_diff'] = (df['A_following_count'] > df['B_following_count'])\n    df['listed_diff'] = (df['A_listed_count'] > df['B_listed_count'])\n    df['ment_rec_diff'] = (df['A_mentions_received'] > df['B_mentions_received'])\n    df['rt_rec_diff'] = (df['A_retweets_received'] > df['B_retweets_received'])\n    df['ment_sent_diff'] = (df['A_mentions_sent'] > df['B_mentions_sent'])\n    df['rt_sent_diff'] = (df['A_retweets_sent'] > df['B_retweets_sent'])\n    df['posts_diff'] = (df['A_posts'] > df['B_posts'])\n\n    df['A_pop_ratio'] = df['A_mentions_sent']\/df['A_listed_count']\n    df['A_foll_ratio'] = df['A_follower_count']\/df['A_following_count']\n    df['A_ment_ratio'] = df['A_mentions_sent']\/df['A_mentions_received']\n    df['A_rt_ratio'] = df['A_retweets_sent']\/df['A_retweets_received']\n    \n    df['B_pop_ratio'] = df['B_mentions_sent']\/df['B_listed_count']\n    df['B_foll_ratio'] = df['B_follower_count']\/df['B_following_count']\n    df['B_ment_ratio'] = df['B_mentions_sent']\/df['B_mentions_received']\n    df['B_rt_ratio'] = df['B_retweets_sent']\/df['B_retweets_received']\n    \n    df['A\/B_foll_ratio'] = (df['A_foll_ratio'] > df['B_foll_ratio'])\n    df['A\/B_ment_ratio'] = (df['A_ment_ratio'] > df['B_ment_ratio'])\n    df['A\/B_rt_ratio'] = (df['A_rt_ratio'] > df['B_rt_ratio'])\n\n    df['nf1_diff'] = (df['A_network_feature_1'] > df['B_network_feature_1'])\n    df['nf2_diff'] = (df['A_network_feature_2'] > df['B_network_feature_2'])\n    df['nf3_diff'] = (df['A_network_feature_3'] > df['B_network_feature_3'])\n    \n    df['nf3_ratio'] = df['A_network_feature_3'] \/ df['B_network_feature_3']\n    df['nf2_ratio'] = df['A_network_feature_2'] \/ df['B_network_feature_2']\n    df['nf1_ratio'] = df['A_network_feature_1'] \/ df['B_network_feature_1']\n    \n    return(pre_pro(df))\n# # # # # # # # # # # # # # # # # # # # # #","d638459f":"fe_train = feat_eng(train.copy())\nfe_test = feat_eng(test.copy())","a72704f2":"train_df = fe_train\ntest_df = fe_test\ny_train = np.array(train_df['Choice'])","e7fb1b7b":"target = 'Choice'\npredictors = train_df.columns.values.tolist()[1:]","ba2b48a0":"# bayesian_tr_index, bayesian_val_index  = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=1).split(train_df, train_df.Choice.values))[0]\n\n# def LGB_bayesian(\n#     num_leaves,  # int\n#     min_data_in_leaf,  # int\n#     learning_rate,\n#     min_sum_hessian_in_leaf,    # int  \n#     feature_fraction,\n#     lambda_l1,\n#     lambda_l2,\n#     min_gain_to_split,\n#     max_depth):\n    \n#     # LightGBM expects next three parameters need to be integer. So we make them integer\n#     num_leaves = int(round(num_leaves))\n#     min_data_in_leaf = int(round(min_data_in_leaf))\n#     max_depth = int(round(max_depth))\n\n#     assert type(num_leaves) == int\n#     assert type(min_data_in_leaf) == int\n#     assert type(max_depth) == int\n\n#     param = {\n#         'num_leaves': num_leaves,\n#         'max_bin': 63,\n#         'min_data_in_leaf': min_data_in_leaf,\n#         'learning_rate': learning_rate,\n#         'min_sum_hessian_in_leaf': min_sum_hessian_in_leaf,\n#         'bagging_fraction': 1.0,\n#         'bagging_freq': 5,\n#         'feature_fraction': feature_fraction,\n#         'lambda_l1': lambda_l1,\n#         'lambda_l2': lambda_l2,\n#         'min_gain_to_split': min_gain_to_split,\n#         'max_depth': max_depth,\n#         'save_binary': True, \n#         'seed': 1337,\n#         'feature_fraction_seed': 1337,\n#         'bagging_seed': 1337,\n#         'drop_seed': 1337,\n#         'data_random_seed': 1337,\n#         'objective': 'binary',\n#         'boosting_type': 'gbdt',\n#         'verbose': 1,\n#         'metric': 'auc',\n#         'is_unbalance': True,\n#         'boost_from_average': False,   \n\n#     }    \n    \n    \n#     xg_train = lgb.Dataset(train_df.iloc[bayesian_tr_index][predictors].values,\n#                            label=train_df.iloc[bayesian_tr_index][target].values,\n#                            feature_name=predictors,\n#                            free_raw_data = False\n#                            )\n#     xg_valid = lgb.Dataset(train_df.iloc[bayesian_val_index][predictors].values,\n#                            label=train_df.iloc[bayesian_val_index][target].values,\n#                            feature_name=predictors,\n#                            free_raw_data = False\n#                            )   \n\n#     num_round = 5000\n#     clf = lgb.train(param, xg_train, num_round, valid_sets = [xg_valid], verbose_eval=250, early_stopping_rounds = 50)\n    \n#     predictions = clf.predict(train_df.iloc[bayesian_val_index][predictors].values, num_iteration=clf.best_iteration)   \n    \n#     score = metrics.roc_auc_score(train_df.iloc[bayesian_val_index][target].values, predictions)\n    \n#     return score","ce818c5a":"# # Bounded region of parameter space\n# bounds_LGB = {\n#     'num_leaves': (2, 5), \n#     'min_data_in_leaf': (1, 10),  \n#     'learning_rate': (0.03, 0.07),\n#     'min_sum_hessian_in_leaf': (0.1, 0.5),    \n#     'feature_fraction': (0.2, 0.5),\n#     'lambda_l1': (0, 1), \n#     'lambda_l2': (0, 1), \n#     'min_gain_to_split': (0.1, 1.0),\n#     'max_depth':(2,10),\n# }\n\n\n# from bayes_opt import BayesianOptimization\n\n# LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=13)\n\n# print(LGB_BO.space.keys)\n\n# init_points = 10\n# n_iter = 10\n\n# target = 'Choice'\n# predictors = train_df.columns.values.tolist()[1:]\n\n# print('-' * 130)\n\n# with warnings.catch_warnings():\n#     warnings.filterwarnings('ignore')\n#     LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)","d65eb578":"# LGB_BO.max\n## used to updat first 9 parameters following","0ddbb138":"param_lgb = {\n        'feature_fraction': 0.4647875434283183,\n        'lambda_l1': 0.14487098904632512,\n        'lambda_l2': 0.9546002933329684,\n        'learning_rate': 0.050592093295320606,\n        'max_depth': int(round(7.696194993998026)),\n        'min_data_in_leaf': int(round(9.879507661608065)),\n        'min_gain_to_split': 0.7998292013880356,\n        'min_sum_hessian_in_leaf': 0.24962103361366683,\n        'num_leaves': int(round(2.854239951949671)),\n        'max_bin': 63,\n        'bagging_fraction': 1.0, \n        'bagging_freq': 5, \n        'save_binary': True,\n        'seed': 1965,\n        'feature_fraction_seed': 1965,\n        'bagging_seed': 1965,\n        'drop_seed': 1965,\n        'data_random_seed': 1965,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': False}","bd981d0b":"nfold = 20\n\nskf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=2019)\n\noof = np.zeros(len(fe_train))\npredictions = np.zeros((len(fe_test),nfold))\n\ni = 1\nfor train_index, valid_index in skf.split(fe_train, fe_train.Choice.values):\n    print(\"\\nfold {}\".format(i))\n\n    xg_train = lgb.Dataset(fe_train.iloc[train_index][predictors].values,\n                           label=fe_train.iloc[train_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )\n    xg_valid = lgb.Dataset(fe_train.iloc[valid_index][predictors].values,\n                           label=fe_train.iloc[valid_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )   \n\n    \n    clf = lgb.train(param_lgb, xg_train, 10000000, valid_sets = [xg_valid], verbose_eval=250, early_stopping_rounds = 100)\n    oof[valid_index] = clf.predict(fe_train.iloc[valid_index][predictors].values, num_iteration=clf.best_iteration) \n    \n    predictions[:,i-1] += clf.predict(fe_test[predictors], num_iteration=clf.best_iteration)\n    i = i + 1\n\nprint(\"\\n\\nCV AUC: {:<0.8f}\".format(metrics.roc_auc_score(fe_train.Choice.values, oof)))","0f8af65d":"lgb_bay = []\n\nfor i in range(len(predictions)):\n    lgb_bay.append(predictions[i][-1])","c1b4dafc":"submission = pd.read_csv('..\/input\/sample_predictions.csv')\nsubmission['Choice'] = lgb_bay\nsubmission.to_csv('sub.csv', index = False, header = True)","6965ebc4":"Parameters came from a Bayesian Optimized Parameter Search","8377df3e":"The following single model scores 0.87169 on the private leaderboard, between 12th & 13th (private)."}}