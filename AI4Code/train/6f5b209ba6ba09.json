{"cell_type":{"4d73bed7":"code","a0f072aa":"code","d2f871ca":"code","92730100":"code","437a41f2":"code","c85bdd1f":"code","710dd6da":"code","b15ee3fc":"code","070895c1":"code","13545c53":"code","c9d37317":"code","a93c5fcb":"code","9e646356":"code","5ea9a64d":"code","4ba4584b":"code","f4142514":"code","c6823703":"code","977da728":"code","4b446538":"code","c9ab1a37":"code","a14b53d2":"code","5b7c85d8":"code","eaa27f78":"code","2bb3273a":"code","6b731b50":"code","06511b5f":"code","1572089a":"code","977e344d":"code","11ac0be7":"code","cd349483":"code","9d0788a0":"code","d711c6a8":"code","d085d609":"markdown","279ce73a":"markdown","ea477be0":"markdown","b53b72b2":"markdown","734d20e5":"markdown"},"source":{"4d73bed7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a0f072aa":"df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")","d2f871ca":"print(\"Size of the DataFrame \", df.shape)\ndf.isnull().sum()[df.isnull().sum() > 0] ","92730100":"df.head()","437a41f2":"df.head().PoolQC","c85bdd1f":"#B == Boolean Variable\n#I did this cos there is a possibility that values may be missing cos the houses don't have them\ndropCols = [\"PoolQC\",\"Fence\", \"MiscFeature\", \"Alley\"]\ndf[\"PoolQCB\"] = (df[\"PoolQC\"].isnull()).astype(int) \ndf[\"FenceB\"] = (df[\"Fence\"].isnull()).astype(int) \ndf[\"MiscFeatureB\"] = (df[\"MiscFeature\"].isnull()).astype(int) \ndf[\"AlleyB\"] = (df[\"Alley\"].isnull()).astype(int) \ndf.head().PoolQCB","710dd6da":"testdf = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","b15ee3fc":"testdf[\"PoolQCB\"] = (testdf[\"PoolQC\"].isnull()).astype(int) \ntestdf[\"FenceB\"] = (testdf[\"Fence\"].isnull()).astype(int) \ntestdf[\"MiscFeatureB\"] = (testdf[\"MiscFeature\"].isnull()).astype(int) \ntestdf[\"AlleyB\"] = (testdf[\"Alley\"].isnull()).astype(int) ","070895c1":"catCols = [c for c in df.columns if\n                    df[c].nunique() < 10 and \n                    df[c].dtype == \"object\"]\n\nnumCols = [c for c in df.columns if \n                df[c].dtype in ['int64', 'float64']]","13545c53":"df.drop(dropCols, axis = 1, inplace = True)\ntestdf.drop(dropCols, axis = 1,inplace = True)","c9d37317":"print(numCols)","a93c5fcb":"df['OverallQual'].nunique()","9e646356":"plt.figure(figsize = (15,15))\n\n#Baths\nplt.subplot(331)\ndfCorr = df[['BsmtHalfBath']+ ['FullBath']+['HalfBath']+['SalePrice']]\nsns.heatmap(dfCorr.corr(), annot = True)\n\n#Lot, Overall Conditions and Qualities\ndfCorr = df[['LotFrontage']+ ['LotArea']+['OverallQual']+['SalePrice']]\nplt.subplot(332)\nsns.heatmap(dfCorr.corr(), annot = True)\n\n#Basements\ndfCorr = df[['BsmtFinSF1'] +['BsmtUnfSF']+['TotalBsmtSF']+['SalePrice']] #['BsmtUnfSF'] ['BsmtFinSF1'] + \nplt.subplot(333)\nsns.heatmap(dfCorr.corr(), annot = True)\n\n#BoolVariables\ndfCorr = df[['PoolQCB'] +['FenceB']+['MiscFeatureB']+['AlleyB']+['SalePrice']]\nplt.subplot(334)\nsns.heatmap(dfCorr.corr(), annot = True)\n\n#Years\nplt.subplot(335)\ndfCorr = df[['YearBuilt']+ ['YearRemodAdd']+['SalePrice']]\nsns.heatmap(dfCorr.corr(), annot = True)","5ea9a64d":"#Numerical Columns to be dropped\n#['BsmtUnfSF'] ['BsmtFinSF1'] 'BsmtHalfBath','OverallCond', 'BsmtFinSF2', \ndropCols2 = ['PoolQCB', 'FenceB', 'MiscFeatureB', 'AlleyB'] ","4ba4584b":"sns.set()\nplt.figure(figsize = (20,20))\nplt.subplot(331)\nsns.scatterplot(x = 'TotalBsmtSF', y = 'SalePrice', data = df, hue = 'OverallQual', alpha = 0.85)\n\nplt.subplot(332)\nsns.scatterplot(x = 'PoolQCB', y = 'SalePrice', data = df, hue = 'OverallQual', alpha = 0.85)","f4142514":"target = df[\"SalePrice\"]","c6823703":"df.drop([\"SalePrice\"], axis = 1, inplace = True)\ndf.drop(dropCols2, axis = 1, inplace = True)\ntestdf.drop(dropCols2, axis = 1, inplace = True)","977da728":"numCols = [c for c in df.columns if \n                df[c].dtype in ['int64', 'float64']]\n\ncatCols = [c for c in df.columns if\n                    df[c].nunique() < 10 and \n                    df[c].dtype == \"object\"]\n","4b446538":"print(numCols)","c9ab1a37":"from sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error","a14b53d2":"train, valid, yTrain, yValid = train_test_split(df, target, train_size=0.85, random_state=0)","5b7c85d8":"cols = catCols + numCols  \ntrain = train[cols].copy()\nvalid = valid[cols].copy()\ntest = testdf[cols].copy()","eaa27f78":"valid.head()","2bb3273a":"train.shape","6b731b50":"# Preprocessing for numerical data\nnumTrans = SimpleImputer(strategy='mean')\n\n# Preprocessing for categorical data\ncatTrans = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])","06511b5f":"# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numTrans, numCols),    \n        ('cat', catTrans, catCols)\n    ])\n#,        ('cat', catTrans, catCols)\n\n\n# Define model\nmodel = RandomForestRegressor(n_estimators=200, random_state=0)\n\n# Bundle preprocessing and modeling code in a pipeline\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ])","1572089a":"from xgboost import XGBRegressor","977e344d":"\"\"\"\n# Preprocessing of training data, fit model \nclf.fit(train, yTrain)\n\n# Preprocessing of validation data, get predictions\npreds = clf.predict(valid)\n\nprint('MAE:', mean_absolute_error(yValid, preds))\n\"\"\"","11ac0be7":"model1 = XGBRegressor(n_estimators=700, learning_rate=0.0255, early_stopping_rounds = 5, random_state = 0)\n\n# Bundle preprocessing and modeling code in a pipeline\nclf1 = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model1)\n                     ])","cd349483":"clf1.fit(train, yTrain)\n\n# Preprocessing of validation data, get predictions\npreds1 = clf1.predict(valid)\n\nprint('MAE:', mean_absolute_error(yValid, preds1))\n","9d0788a0":"predsTest = clf1.predict(test)","d711c6a8":"output = pd.DataFrame({'Id': test.Id,\n                       'SalePrice': predsTest})\noutput.to_csv('submission.csv', index=False)\n","d085d609":"## Improvising the model \nNote : This cell is written after applying XGBRegressor()<br>\nNow I'll try to improve my model by trying to do basic EDA and indentifying important features. ","279ce73a":"## Using XGBoost\nNow I'll be using Gradient Descent to make the algorithm better. Also, I did try a couple of algorithms but Random Forest seems to be doing the best. The above code got me a score of 0.1515 (66th percentile at the time)<br>\n<ul>\n<li>Update1 : Running the code with XGBRegressor for the first time caused the error to decrease by 1000(approx)<\/li>\n<li>Update 2: Using XGBoost increased my score( or reduced my error) to 0.1351 (50th Percentile)<\/li>\n    <\/ul>\n","ea477be0":"Now let me split the data into training and validation set and just randomly plug in all the variables after using a simple imputer function.","b53b72b2":"This is just a very vague attempt at trying to predict the prices using whatever I learnt on kaggle's Intermediate ML Course. Upvote if you like it :)","734d20e5":"I'm gonna be dropping all columns which have missing values > 1000 and replacing them by a feature which indicates if they're present."}}