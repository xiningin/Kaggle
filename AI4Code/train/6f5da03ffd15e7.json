{"cell_type":{"089a12e4":"code","33986c7e":"code","fa08ad48":"code","d9e0f4b7":"code","da9f3fef":"code","cd08b462":"code","875dbddf":"code","cbfe4b6a":"code","10449a0c":"code","a18cf29d":"code","a1239f8a":"code","0eedd46c":"code","642ee12e":"code","b861be69":"code","d7c04975":"code","61b7f851":"code","ad8c65be":"code","41141dac":"code","e36b222b":"code","647ffb86":"code","5d4b5188":"code","aeec2c7c":"code","56db2e33":"code","f0def5bd":"code","e6b8a432":"code","7c969c1b":"code","ccd1c59a":"code","d04c522a":"code","3d7890f8":"code","753d7443":"code","25598762":"code","9b49b25e":"code","fcaa4463":"code","0a5d1d2c":"code","715a67c9":"code","06412fe9":"code","22a5d47d":"code","09a68fe4":"code","02577c11":"code","42ed83be":"code","1cee4266":"code","c8a202a7":"code","7176874a":"code","82503358":"code","b2ab61bd":"code","be58f3f1":"code","74dee854":"code","f48eed27":"code","d3266658":"code","67abcd35":"code","d2b1bed5":"code","aa581d6b":"code","bc91c54e":"code","958fd121":"code","3c6b3dba":"markdown","655473cd":"markdown","4d80d4e7":"markdown","c4420b07":"markdown","736a3db7":"markdown","5697f70b":"markdown","fff7eafb":"markdown","9e999eda":"markdown","4163138b":"markdown","ec26e718":"markdown","4c16cadd":"markdown","a4b147fa":"markdown","0a6ed828":"markdown","4733ffe9":"markdown","310be236":"markdown","962c0f59":"markdown","34bfd94f":"markdown","84dffeaf":"markdown","d5b5d87b":"markdown","189c435e":"markdown","a0f37e3f":"markdown","f3e4d29b":"markdown","83c315b7":"markdown","dd916d74":"markdown","12382f04":"markdown","b45a829a":"markdown","59e341ca":"markdown","257278c7":"markdown","a8957a4c":"markdown"},"source":{"089a12e4":"# Work with Data - the main Python libraries\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\n\n# Data download \nimport os\nimport requests\n\n# Date and time\nfrom datetime import datetime, timedelta\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# Preprocessing\nfrom datetime import date, timedelta, datetime\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, ShuffleSplit, GridSearchCV\n\n# Modeling\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\n\n# Metrics\nfrom sklearn.metrics import accuracy_score\n\n# Ignore all warnings\nimport warnings\nwarnings.simplefilter('ignore')","33986c7e":"pd.set_option('max_columns', 150)","fa08ad48":"# Duration of the forecast, in weeks\nN = 2\n\n# The duration of the continuous decline of the new_confirmed, in weeks\nM = 3","d9e0f4b7":"name_country = 'Ukraine'\nmain_country = 'UA' # code of Ukraine","da9f3fef":"# Selection the list of code of countries \ncountries_codes = ['UA', 'RO']","cd08b462":"# Thanks https:\/\/github.com\/CSSEGISandData\/COVID-19\ndf = pd.DataFrame()\nfor code in countries_codes:\n    if len(df) == 0:\n        df = pd.read_csv(f\"https:\/\/storage.googleapis.com\/covid19-open-data\/v2\/{code}\/main.csv\")\n    else: \n        confirmed_global_df = pd.read_csv(f\"https:\/\/storage.googleapis.com\/covid19-open-data\/v2\/{code}\/main.csv\")\n        df = pd.concat([df, confirmed_global_df])","875dbddf":"df","cbfe4b6a":"# All features\nprint(np.array(df.columns))\nprint(f\"Number of features = {len(df.columns)}\")","10449a0c":"# Data statistics\ndf.describe()","a18cf29d":"# Remove bad features (with the small number of data)\ndf = df.drop(columns=['key', 'place_id', 'wikidata', 'datacommons', 'country_name', \n                                                            'subregion1_code', 'subregion1_name', 'subregion2_code', \n                                                            'subregion2_name', 'locality_code', 'locality_name', \n                                                            '3166-1-alpha-2', '3166-1-alpha-3', 'aggregation_level',\n                                                            'new_hospitalized', 'total_hospitalized', 'current_hospitalized',\n                                                            'new_intensive_care', 'total_intensive_care', 'new_ventilator',\n                                                            'total_ventilator', 'current_ventilator', 'noaa_station', 'noaa_distance',\n                                                            'elevation', 'hospital_beds'])","a1239f8a":"# Data statistics without removed features\ndf.describe()","0eedd46c":"# All features\ndf['ds'] = pd.to_datetime(df['date'], format = \"%Y-%m-%d\")\nprint(np.array(df.columns))\nprint(f\"Number of features without removed features = {len(df.columns)}\")\ndf","642ee12e":"df.info()","b861be69":"df[df['country_code']=='UA'].tail(12)[:-2].info()","d7c04975":"df[df['country_code']=='UA'].tail(12)[:-2]","61b7f851":"# Removed uninformative on Ukraine features \ndf = df.drop(columns = ['date', 'current_intensive_care', 'adult_male_mortality_rate', 'adult_female_mortality_rate', 'nurses', \n 'physicians', 'fiscal_measures', 'international_support', 'emergency_investment_in_healthcare', 'latitude', 'longitude'])\n\n# Fill 'snowfall' NA to zero\ndf['snowfall'] = df['snowfall'].fillna(0)\n\ndf[df['country_code']=='UA'].tail(12)[:-2]","ad8c65be":"# Thanks to https:\/\/www.kaggle.com\/vbmokin\/50-advanced-tips-data-science-for-tabular-data\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","41141dac":"def plot_with_anomalies(df, cols_y_list, cols_y_list_name, dates_x, anomalous_dates, log_y=False):\n    # Thanks to https:\/\/www.kaggle.com\/vbmokin\/covid-in-ua-prophet-with-4-nd-seasonality\n    # Draws a plot with title - the features cols_y_list (y) and dates_x (x) from the dataframe df\n    # and with vertical lines in the dates from the list anomalous_dates\n    # with the length between the minimum and maximum of feature cols_y_list[0]\n    # with log_y = False or True\n    # cols_y_list - dictionary of the names of cols from cols_y_list (keys - name of feature, value - it's name for the plot legend), \n    # name of cols_y_list[0] is the title of the all plot\n    \n    fig = px.line(df, x=dates_x, y=cols_y_list[0], title=cols_y_list_name[cols_y_list[0]], log_y=log_y, template='gridon',width=800, height=600)\n    y_max = df[cols_y_list[0]].max()\n    for i in range(len(cols_y_list)-1):\n        fig.add_trace(go.Scatter(x=df[dates_x], y=df[cols_y_list[i+1]], mode='lines', name=cols_y_list_name[cols_y_list[i+1]]))\n        max_i = df[cols_y_list[i+1]].max()\n        y_max = max_i if max_i > y_max else y_max\n    \n    y_min = min(df[cols_y_list[0]].min(),0)\n    for i in range(len(anomalous_dates)):\n        anomal_date = anomalous_dates[i]\n        #print(anomal_date, y_min, y_max)\n        fig.add_shape(dict(type=\"line\", x0=anomal_date, y0=y_min, x1=anomal_date, y1=y_max, line=dict(color=\"red\", width=1)))\n    fig.show()","e36b222b":"def get_new_data_for_country(df_i, country_code, country_id):\n    # Get new (with aggregation and cleaning) data from dataframe df for given country_code \n    # and add feature 'country_id'\n    \n    print(f\"\\nCountry code - {country_code}\")\n    # Data preparing for week aggregation of data\n    # Transfering date with country_code from 'ds' to index\n    df_i.index = df_i['ds']\n    df_i = df_i.drop(columns=['ds', 'country_code'])\n    df_i = df_i.fillna(0)\n    \n    # Week aggregation data with mean values\n    df_i = df_i.resample('1W', label='right').mean().dropna()\n    \n    # Delete weeks with incomplete data\n    if datetime.today().weekday() > 4:  # Today is Friday or later in the week?\n        # Remove this week only - there is already enough data for the previous week\n        df_i = df_i[:-1]\n    else:\n        # Remove this and previous weeks - there is not enough data for the previous week\n        df_i = df_i[:-2]\n    \n    # Target generating\n    # Calculation features with difference of values\n    df_i = df_i.diff(1)\n    df_i = df_i[1:]\n    \n    # Determination the dates with descent of the new_confirmed - 1, others (zero or increase) - 0\n    df_i['new_confirmed_descent'] = (df_i['new_confirmed'] < 0).astype('int')\n    \n    # Shifted to N weeks the dates with descent\/increase of the new_confirmed (1\/0)\n    df_i['new_confirmed_descent_shift'] = df_i['new_confirmed_descent'].shift(N)\n    df_i = df_i[N:]\n    df_i['target'] = df_i['new_confirmed_descent_shift'].astype('int')  # 1 (descent) or 0 (other value)\n\n    # Add feature 'country_id'\n    df_i['country_id'] = country_id\n    \n    # Generate features by date\n    df_i = df_i.reset_index(drop=False)\n    df_i['year'] = df_i['ds'].dt.year\n    df_i['month'] = df_i['ds'].dt.month\n    df_i['week'] = df_i['ds'].dt.week\n\n    # Remove leak feature\n    df_i = df_i.drop(columns = ['new_confirmed_descent_shift'])\n    \n    # Display of result \n    display(df_i.tail(2))\n    \n    return df_i","647ffb86":"# Creation the new dataset\ndf2 = pd.DataFrame(columns=df.columns.tolist())\nfor i in range(len(countries_codes)):\n    if countries_codes[i]==main_country:\n        main_country_id = i\n    df_i = get_new_data_for_country(df[df['country_code']==countries_codes[i]], countries_codes[i], i)\n    if len(df2) == 0:\n        df2 = df_i\n    else:\n        df2 = pd.concat([df2, df_i])\n\n# Output result table\nprint(\"\\nResult table:\")\ndf2","5d4b5188":"# Target and new_confirmed visualization\n#plot_with_anomalies(df2, [\"new_confirmed\"], {\"new_confirmed\" : f\"Target with new_confirmed data\"}, 'ds', anomalous_dates, False)","aeec2c7c":"# Remove feature 'ds'\ndf2 = df2.drop(columns=['ds'])\ndf2 = df2.reset_index(drop=True)","56db2e33":"# Selecting a feature 'target' and removing it from the dataset\ntarget_all = df2.pop('target')","f0def5bd":"# Reduce df3 in memory\ndf2 = reduce_mem_usage(df2)\ndf2","e6b8a432":"# Standartization data\nscaler = StandardScaler()\ndf2 = pd.DataFrame(scaler.fit_transform(df2), columns = df2.columns)\n\n# Display training data\ndf2","7c969c1b":"# Display the statistics for training data\ndf2.describe()","ccd1c59a":"test_size = 0.2 # from all data\nvalid_size = 0.2 # from training data\n\n# Data splitting to training and test datasets\ntrain_all, test, target_train_all, target_test = train_test_split(df2, target_all, test_size=test_size, random_state=0)\n\n# All training data splitting to training (part of the all training) and valid datasets\ntrain, valid, target_train, target_valid = train_test_split(train_all, target_train_all, test_size=valid_size, random_state=0)","d04c522a":"# Display information about new training data\ntrain.info()","3d7890f8":"# Display information about valid data\nvalid.info()","753d7443":"# Display information about test data\ntest.info()","25598762":"# Cross-validation of training data with shuffle\ncv_train = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)","9b49b25e":"# Creation the dataframe with the resulting score of all models\nresult = pd.DataFrame({'model' : ['Decision Tree Classifier', 'Random Forest Classifier'], \n                       'train_score': 0, 'valid_score': 0})\nresult","fcaa4463":"# Decision Tree Classifier\ndecision_tree = DecisionTreeClassifier()\nparam_grid = {'min_samples_leaf': [3], \n              'max_depth': [i for i in range(3,5)]}\n\n# Training model\ndecision_tree_CV = GridSearchCV(decision_tree, param_grid=param_grid, cv=cv_train, verbose=False)\ndecision_tree_CV.fit(train, target_train)\nprint(decision_tree_CV.best_params_)\n\n# Prediction for training data\ny_train_decision_tree = decision_tree_CV.predict(train)\n\n# Accuracy of model\nacc = round(accuracy_score(target_train, y_train_decision_tree)*100, 1)\nprint(f'Accuracy of DecisionTreeClassifier model training is {acc}')\n\n# Save to result dataframe\nresult.loc[result['model'] == 'Decision Tree Classifier', 'train_score'] = acc","0a5d1d2c":"# Print rounded accuracy to 2 decimal values after the text\ny_val_decision_tree = decision_tree_CV.predict(valid)\nacc_valid = round(accuracy_score(target_valid, y_val_decision_tree)*100,1)\nresult.loc[result['model'] == 'Decision Tree Classifier', 'valid_score'] = acc_valid\nprint(f'Accuracy of DecisionTreeClassifier model prediction for valid dataset is {acc_valid}')","715a67c9":"# Random Forest Classifier\nrf = RandomForestClassifier()\nparam_grid = {'n_estimators': [10], \n              'min_samples_split': [3], \n              'min_samples_leaf': [2], \n              'max_features': ['auto'], \n              'max_depth': [i for i in range(3,6)], \n              'criterion': ['gini'], \n              'bootstrap': [False]}\n\n# Training model\nrf_CV = GridSearchCV(rf, param_grid=param_grid, cv=cv_train, verbose=False)\nrf_CV.fit(train, target_train)\nprint(rf_CV.best_params_)\n\n# Prediction for training data\ny_train_rf = rf_CV.predict(train)\n\n# Accuracy of model\nacc = round(accuracy_score(target_train, y_train_rf)*100,1)\nprint(f'Accuracy of RandomForestClassifier model training is {acc}')\n\n# Save to result dataframe\nresult.loc[result['model'] == 'Random Forest Classifier', 'train_score'] = acc","06412fe9":"# Print rounded accuracy to 2 decimal values after the text\ny_val_rf = rf_CV.predict(valid)\nacc_valid = round(accuracy_score(target_valid, y_val_rf)*100,1)\nresult.loc[result['model'] == 'Random Forest Classifier', 'valid_score'] = acc_valid\nprint(f'Accuracy of RandomForestClassifier model prediction for valid dataset is {acc_valid}')","22a5d47d":"# Prediction of target for test data for all models\ny_test_decision_tree = decision_tree_CV.predict(test)\ny_test_rf = rf_CV.predict(test)","09a68fe4":"# Building plot for prediction for the training data \nx = np.arange(len(train))\nplt.figure(figsize=(16,10))\nplt.scatter(x, target_train, label = \"Target data\", color = 'g')\nplt.scatter(x, y_train_decision_tree, label = \"Decision Tree prediction\", color = 'b')\nplt.scatter(x, y_train_rf, label = \"Random Forest prediction\", color = 'y')\nplt.title('Prediction for the training data')\nplt.legend(loc='best')\nplt.grid(True)","02577c11":"# Building plot for prediction for the valid data \nx = np.arange(len(valid))\nplt.figure(figsize=(16,10))\nplt.scatter(x, target_valid, label = \"Target data\", color = 'g')\nplt.scatter(x, y_val_decision_tree, label = \"Decision Tree prediction\", color = 'b')\nplt.scatter(x, y_val_rf, label = \"Random Forest prediction\", color = 'y')\nplt.title('Prediction for the valid data')\nplt.legend(loc='best')\nplt.grid(True)","42ed83be":"# Building plot for prediction for the test data \nx = np.arange(len(test))\nplt.figure(figsize=(16,10))\nplt.scatter(x, y_test_decision_tree, label = \"Decision Tree prediction\", color = 'b')\nplt.scatter(x, y_test_rf, label = \"Random Forest prediction\", color = 'y')\nplt.title('Prediction for the test data')\nplt.legend(loc='best')\nplt.grid(True)","1cee4266":"# Display results of modeling\nresult.sort_values(by=['valid_score', 'train_score'], ascending=False)","c8a202a7":"# Select models with minimal overfitting\nresult_best = result[(result['train_score'] - result['valid_score']).abs() < 20]\nresult_best.sort_values(by=['valid_score', 'train_score'], ascending=False)","7176874a":"# Select the best model\nresult_best.nlargest(1, 'valid_score')","82503358":"# Find a name of the best model (with maximal valid score)\nbest_model_name = result_best.nlargest(1, 'valid_score')['model'].tolist()[0]","b2ab61bd":"print(f'The best model is \"{best_model_name}\"')","be58f3f1":"# Tuning the best model for the all data\ndef model_training(best_model_name):\n    \n    def model_prediction(df, target, model, type_data):\n        # Prediction by the model\n        y_pred = model.predict(df)\n        acc = round(accuracy_score(target, y_pred)*100, 1)\n        print(f'Accuracy of model for {type_data} data is {acc}')\n        return y_pred, acc\n    \n    \n    if best_model_name == 'Decision Tree Classifier':\n        # Decision Tree Classifier\n        decision_tree = DecisionTreeClassifier()\n        param_grid = {'min_samples_leaf': [3], \n                      'max_depth': [i for i in range(3,5)]}\n\n        # Set model\n        model = GridSearchCV(decision_tree, param_grid=param_grid, cv=cv_train, verbose=False)\n    \n    elif best_model_name == 'Random Forest Classifier':\n        # Random Forest Classifier\n        rf = RandomForestClassifier()\n        param_grid = {'n_estimators': [10], \n                      'min_samples_split': [3], \n                      'min_samples_leaf': [2], \n                      'max_features': ['auto'], \n                      'max_depth': [i for i in range(3,6)], \n                      'criterion': ['gini'], \n                      'bootstrap': [False]}\n\n        # Set model\n        model = GridSearchCV(rf, param_grid=param_grid, cv=cv_train, verbose=False)\n\n    # Training model\n    model.fit(train_all, target_train_all)\n    print(model.best_params_)\n\n    # Prediction for all data\n    y_pred_train, acc_train = model_prediction(train_all, target_train_all, model, 'all training')\n\n    # Training model for all data\n    model.fit(df2, target_all)\n    print(model.best_params_)\n\n    # Draw plot with feature importances of model\n    importances = model.best_estimator_.feature_importances_\n    model_importances = pd.Series(importances, index=df2.columns.tolist()).sort_values(ascending=False)\n    fig = plt.figure(figsize=(16,10))\n    model_importances.plot.bar()\n    plt.title(\"Feature importances\")\n    plt.ylabel(\"Mean decrease in impurity\")\n    fig.tight_layout()\n    \n    # Prediction for all data\n    y_pred_all_data, acc_all_data = model_prediction(df2, target_all, model, 'all')\n\n\n    return y_pred_train, acc_train, y_pred_all_data, acc_all_data","74dee854":"y_pred_train, acc_train, y_pred_all_data, acc_all_data = model_training(best_model_name)","f48eed27":"# Invers standartization\ndf3 = pd.DataFrame(scaler.inverse_transform(df2), columns = df2.columns)\ndf3","d3266658":"# Reduce df3 in memory\ndf3 = reduce_mem_usage(df3)\ndf3","67abcd35":"%%time\n# Change the type of real numbers with a zero fractional part in the whole column to integers\nfor col in df3.columns.tolist():\n    if (df3[col].sum() % 1 == 0):\n        df3[col] = df3[col].astype('int')\n# Additional changing\ndf3['year'] = df3['year'].astype('int')\ndf3","d2b1bed5":"#df3 = df3.replace([np.inf, -np.inf], np.nan).fillna(-1)","aa581d6b":"# Restore date by year and week number\ndf3['year_week'] = df3['year'].astype('str')+\"-\"+df3['week'].astype('str')\ndf3['date'] = pd.to_datetime(df3['year_week']+\"-6\", format='%Y-%W-%w') + timedelta(days = 1)","bc91c54e":"# Add prediction data\ndf3['pred'] = y_pred_all_data\ndf3['target'] = target_all\nprint('The complete coincidence of forecast and real data in the target:')\ndf3[['date', 'target', 'pred', 'country_id']][(df3['target']==1) & (df3['pred']==1)]","958fd121":"# Last 14 days for the name_country\nprint(f'Forecast for {name_country} that in {N} weeks the average number of new confirmed per week will begin to decline:')\ndf3[['date', 'target', 'pred', 'country_id']][df3['country_id']==main_country_id].tail(14)[['date', 'target', 'pred']]","3c6b3dba":"**ADDITIONAL TASKS:** \n1. Add to dataframe result also calculated array: y_test.\n2. Add the line with XGBRegressor model prediction (train, valid, test take from the dataframe result).\n3. Creation the function with all commands and output information for all models (for type_plot = 'training', 'valid' or 'test'):\n\n        plot_prediction(result, type_plot='training')","655473cd":"## 2. Download data<a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","4d80d4e7":"**ADDITIONAL TASK:** \n1. Set number of splitting = 5, 7, 10 and to compare of results.\n2. Try use another method for cross-validation of training data (without shuffle):\n\n        KFold(n_splits=5, shuffle=False, random_state=0)","c4420b07":"### 3.5. Data splitting to training, valid and test datasets<a class=\"anchor\" id=\"3.5\"><\/a>\n\n[Back to Table of Contents](#0.1)","736a3db7":"### 3.1. Statistics and remove features with a small number of data<a class=\"anchor\" id=\"3.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","5697f70b":"### 4.1. Decision Tree Classifier<a class=\"anchor\" id=\"4.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","fff7eafb":"I hope you find this notebook useful and enjoyable.\n\nYour comments and feedback are most welcome.\n\n[Go to Top](#0)","9e999eda":"### 4.2. Random Forest Classifier<a class=\"anchor\" id=\"4.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","4163138b":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download data](#2)\n1. [EDA & FE](#3)\n    - [Statistics and remove features with a small number of data](#3.1)    \n    - [Imputing and cleaning data](#3.2)\n    - [FE](#3.3)\n    - [Data standartization](#3.4)\n    - [Data splitting to training, valid and test datasets](#3.5)\n    - [Cross-validation of training data](#3.6)\n1. [Modeling](#4)\n    - [Decision Tree Classifier](#4.1)\n    - [Random Forest Classifier](#4.2)\n1. [Test prediction](#5)\n1. [Results visualization](#6)\n1. [Select the best model](#7)\n1. [Forecasting the week of wave decline for the name_country](#8)","ec26e718":"## 8. Forecasting the week of wave decline for the name_country <a class=\"anchor\" id=\"8\"><\/a>\n\n[Back to Table of Contents](#0.1)","4c16cadd":"**TASK:** Building plot for prediction for the valid data.","a4b147fa":"## 5. Test prediction<a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","0a6ed828":"## 1. Import libraries<a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","4733ffe9":"## 6. Visualization<a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","310be236":"### 3.4. Data standartization<a class=\"anchor\" id=\"3.4\"><\/a>\n\n[Back to Table of Contents](#0.1)","962c0f59":"The data is taken from [COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University](https:\/\/github.com\/CSSEGISandData\/COVID-19) (usually this dataset are updated there daily and are available as of yesterday), so the next 3 days are counted from the date of the last committee of this notebook. ","34bfd94f":"**ADDITIONAL TASKS:** \n1. Add to dataframe result also calculated array: y_train, y_val.\n2. Creation the function with all commands and output information (in each section of this chapter 4) for all models:\n\n        result = get_model(train, valid, target_train, target_valid, model_name, param_grid, cv_train, result)","84dffeaf":"### 3.3. FE<a class=\"anchor\" id=\"3.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","d5b5d87b":"## Acknowledgements\n* [AI-ML-DS Training. L2T: NH4 - Tree Regress models](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l2t-nh4-tree-regress-models)\n- dataset [COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University](https:\/\/github.com\/CSSEGISandData\/COVID-19)\n- my dataset with holidays data [COVID-19: Holidays of countries](https:\/\/www.kaggle.com\/vbmokin\/covid19-holidays-of-countries) - it is recommended to follow the updates\n- notebook with the code to read the data [COVID-19: current situation on August](https:\/\/www.kaggle.com\/corochann\/covid-19-current-situation-on-august)\n- notebook [COVID-19 Novel Coronavirus EDA & Forecasting Cases](https:\/\/www.kaggle.com\/khoongweihao\/covid-19-novel-coronavirus-eda-forecasting-cases) from [@Wei Hao Khoong](https:\/\/www.kaggle.com\/khoongweihao)\n- notebook [Heart Disease - Automatic AdvEDA & FE & 20 models](https:\/\/www.kaggle.com\/vbmokin\/heart-disease-automatic-adveda-fe-20-models)\n- notebook [Air Quality Station - Daily Forecasting - Prophet](https:\/\/www.kaggle.com\/vbmokin\/air-quality-station-daily-forecasting-prophet)\n\nIt is recommended to study:\n* [AI-ML-DS Training. L4AT: Heart Disease prediction](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l4at-heart-disease-prediction)\n* [Data Science for tabular data: Advanced Techniques](https:\/\/www.kaggle.com\/vbmokin\/data-science-for-tabular-data-advanced-techniques)\n* [EDA for tabular data: Advanced Techniques](https:\/\/www.kaggle.com\/vbmokin\/eda-for-tabular-data-advanced-techniques)","189c435e":"## 3. EDA & FE<a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","a0f37e3f":"## 7. Select the best model <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","f3e4d29b":"**ADDITIONAL TASK:** Try use other values in the parameter test_size or valid_size above: 0.1, 0.15, 0.3, 0.5 and to analyze what is the difference for accuracy of models will be below.","83c315b7":"Unfortunately, the forecast is not very accurate. \n\nI will continue to improve it.\n\nI invite everyone to improve.","dd916d74":"## 4. Modeling<a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","12382f04":"<a class=\"anchor\" id=\"0\"><\/a>\n# COVID-19 waves forecasting (baseline notebook)","b45a829a":"### 3.2. Imputing and cleaning data<a class=\"anchor\" id=\"3.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","59e341ca":"### 3.6. Cross-validation of training data<a class=\"anchor\" id=\"3.6\"><\/a>\n\n[Back to Table of Contents](#0.1)","257278c7":"## Forecasting the date of a cob of constantly decline\/growth of the number of confirmed sickness to coronavirus in world countries (date of the start of a new wave or date of start of the stage of finish of given wave).","a8957a4c":"**TASK:** Building plot for prediction for the test data."}}