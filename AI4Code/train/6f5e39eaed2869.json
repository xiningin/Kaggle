{"cell_type":{"977f68ae":"code","4bdeb2f9":"code","05593ef4":"code","ce976eb0":"code","7211f83f":"code","ac489b62":"code","e8fe7717":"code","d4e21ad0":"code","30a3e4fc":"code","d563700c":"code","53901cc4":"code","422e2eb3":"code","111382ee":"code","fd29a9a5":"code","248b79aa":"code","2871577d":"code","301acc0b":"code","6eac9064":"code","4e617242":"code","bb449a0a":"code","2d1c78f0":"code","8e28f5ab":"markdown","23714b16":"markdown","91f44e99":"markdown","3e3e109e":"markdown","b7714e9b":"markdown","979689c9":"markdown","f8272c33":"markdown","43ecb03d":"markdown","86738460":"markdown","78994666":"markdown","2edbbf12":"markdown","188e86b6":"markdown","b6ee4fb0":"markdown","f4788e3c":"markdown","231824cf":"markdown","10f0e31e":"markdown","e6275fe9":"markdown","11ccff57":"markdown","8d441b7d":"markdown","42d36f1c":"markdown","0efcdb4c":"markdown","38ebd593":"markdown","f7026ebd":"markdown","05de2c77":"markdown","f0b8142e":"markdown","bfc112be":"markdown","14cb9744":"markdown","e123547b":"markdown","ff99c3b2":"markdown","14b0e6fb":"markdown","dd3f70f7":"markdown","2ca3a792":"markdown","cf8eef77":"markdown","0e72f23b":"markdown","d07ff07d":"markdown","4745d8f1":"markdown","063a5c0f":"markdown","3f5937b7":"markdown"},"source":{"977f68ae":"import pandas as pd\nimport pylab as pl\nimport numpy as np\nimport scipy.optimize as opt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline \nimport matplotlib.pyplot as plt","4bdeb2f9":"#Click here and press Shift+Enter\n!wget -O cell_samples.csv https:\/\/s3-api.us-geo.objectstorage.softlayer.net\/cf-courses-data\/CognitiveClass\/ML0101ENv3\/labs\/cell_samples.csv","05593ef4":"cell_df = pd.read_csv(\"cell_samples.csv\")\ncell_df.head()","ce976eb0":"#refer the DataSlicing and Mathplotlib Notebooks for details about ploting and slicing the data\nax = cell_df[cell_df['Class'] == 4][0:50].plot(kind='scatter', x='Clump', y='UnifSize', color='DarkBlue', label='malignant');\ncell_df[cell_df['Class'] == 2][0:50].plot(kind='scatter', x='Clump', y='UnifSize', color='Yellow', label='benign', ax=ax);\nplt.show()","7211f83f":"cell_df.dtypes","ac489b62":"cell_df = cell_df[pd.to_numeric(cell_df['BareNuc'], errors='coerce').notnull()]\ncell_df['BareNuc'] = cell_df['BareNuc'].astype('int')\ncell_df.dtypes","e8fe7717":"feature_df = cell_df[['Clump', 'UnifSize', 'UnifShape', 'MargAdh', 'SingEpiSize', 'BareNuc', 'BlandChrom', 'NormNucl', 'Mit']]\nX = np.asarray(feature_df)\nX[0:5]","d4e21ad0":"cell_df['Class'] = cell_df['Class'].astype('int')\ny = np.asarray(cell_df['Class'])\ny [0:5]","30a3e4fc":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","d563700c":"from sklearn import svm\nclf = svm.SVC(kernel='rbf')\nclf.fit(X_train, y_train) ","53901cc4":"yhat = clf.predict(X_test)\nyhat [0:5]","422e2eb3":"from sklearn.metrics import classification_report, confusion_matrix\nimport itertools","111382ee":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","fd29a9a5":"# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, yhat, labels=[2,4])\nnp.set_printoptions(precision=2)\n\nprint (classification_report(y_test, yhat))\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['Benign(2)','Malignant(4)'],normalize= False,  title='Confusion matrix')","248b79aa":"from sklearn.metrics import f1_score\nf1_score(y_test, yhat, average='weighted') ","2871577d":"from sklearn.metrics import jaccard_similarity_score\njaccard_similarity_score(y_test, yhat)","301acc0b":"from sklearn import svm\nclf2 = svm.SVC(kernel='linear')\nclf2.fit(X_train, y_train) ","6eac9064":"yhat1 = clf.predict(X_test)\nyhat1 [0:5]","4e617242":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","bb449a0a":"# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, yhat1, labels=[2,4])\nnp.set_printoptions(precision=2)\n\nprint (classification_report(y_test, yhat1))\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['Benign(2)','Malignant(4)'],normalize= False,  title='Confusion matrix')","2d1c78f0":"from sklearn.metrics import f1_score\nf1_score(y_test, yhat1, average='weighted') ","8e28f5ab":"Okay, we split our dataset into train and test set:","23714b16":"The SVM algorithm offers a choice of kernel functions for performing its processing. Basically, mapping data into a higher dimensional space is called kernelling. The mathematical function used for the transformation is known as the\u00a0kernel\u00a0function, and can be of different types, such as:\n\n    1.Linear\n    2.Polynomial\n    3.Radial basis function (RBF)\n    4.Sigmoid\nEach of these functions has its characteristics, its pros and cons, and its equation, but as there's no easy way of knowing which function performs best with any given dataset, we usually choose different functions in turn and compare the results. Let's just use the default, RBF (Radial Basis Function) for this lab.","91f44e99":"<h2 id=\"practice\">Practice<\/h2>\nCan you rebuild the model, but this time with a __linear__ kernel? You can use __kernel='linear'__ option, when you define the svm. How the accuracy changes with the new kernel function?","3e3e109e":"Lets first look at columns data types:","b7714e9b":"The ID field contains the patient identifiers. The characteristics of the cell samples from each patient are contained in fields Clump to Mit. The values are graded from 1 to 10, with 1 being the closest to benign.\n\nThe Class field contains the diagnosis, as confirmed by separate medical procedures, as to whether the samples are benign (value = 2) or malignant (value = 4).\n\nLets look at the distribution of the classes based on Clump thickness and Uniformity of cell size:","979689c9":"*Converting the datatype of BareNuc from object to int using pd.to_numeric function of pandas*","f8272c33":"\n<h1 align=center><font size=\"5\"> Machine Learning (IT):  Sem V <\/font><\/h1>\n<h1 align=center><font size=\"5\"> SVM (Support Vector Machines)<\/font><\/h1>\n","43ecb03d":"*Printed the confusion matrix as array and as a plot*","86738460":"*We plotted a scatter graph between unifsize and clump here where the value of benign and maligent is 2 and 4 respectively blue color dots are maligent and yellow color are benign*","78994666":"*Reading the data set into data frame cell_df and printing the data frame*","2edbbf12":"## Train\/Test dataset","188e86b6":"We want the model to predict the value of Class (that is, benign (=2) or malignant (=4)). As this field can have one of only two possible values, we need to change its measurement level to reflect this.","b6ee4fb0":"You can also easily use the __f1_score__ from sklearn library:","f4788e3c":"<h2 id=\"evaluation\">Evaluation<\/h2>","231824cf":"Lets try jaccard index for accuracy:","10f0e31e":"*Here using .dtypes function we identified the data types of all the columns*","e6275fe9":"<h2>Thanks for completing this lesson!<\/h2>\n<h4>Author:  Santosh Bothe <\/a><\/h4>\n<hr>\n\n<p>Copyright &copy; 2020 ML Class IEDC Shirur.","11ccff57":"*Defining the nupy array y consisting the data of Class*","8d441b7d":"## Data pre-processing and selection","42d36f1c":"*Importing the various libraries we will require to build our model like pandas, pylab, numpy,matplot etc.*","0efcdb4c":"<h2 id=\"modeling\">Modeling (SVM with Scikit-learn)<\/h2>","38ebd593":"<h1>Table of contents<\/h1>\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n    <ol>\n        <li><a href=\"#load_dataset\">Load the Cancer data<\/a><\/li>\n        <li><a href=\"#modeling\">Modeling<\/a><\/li>\n        <li><a href=\"#evaluation\">Evaluation<\/a><\/li>\n        <li><a href=\"#practice\">Practice<\/a><\/li>\n    <\/ol>\n<\/div>\n<br>\n<hr>","f7026ebd":"*importing classification report and confusion matrix from sklearn.metrics*","05de2c77":"You can refer <a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.to_numeric.html\">Pandas Documentation <\/a> of to_numeric for more details.  \n\n* pandas.to_numeric(arg, errors='raise', downcast=None)\n\n* Convert argument to a numeric type.\n\nThe default return dtype is float64 or int64 depending on the data supplied. Use the downcast parameter to obtain other dtypes.\n\n   Parameters\n\n1. args : Scalar, list, tuple, 1-d array, or Series\n2. errors : {\u2018ignore\u2019, \u2018raise\u2019, \u2018coerce\u2019}, default \u2018raise\u2019\n            If \u2018raise\u2019, then invalid parsing will raise an exception.\n            If \u2018coerce\u2019, then invalid parsing will be set as NaN.\n            If \u2018ignore\u2019, then invalid parsing will return the input.\n        downcast{\u2018integer\u2019, \u2018signed\u2019, \u2018unsigned\u2019, \u2018float\u2019}, default None\n\nIf not None, and if the data has been successfully cast to a numerical dtype (or if the data was numeric to begin with), downcast that resulting data to the smallest numerical dtype possible according to the following rules:\n\n  \u2018integer\u2019 or \u2018signed\u2019: smallest signed int dtype (min.: np.int8)\n\n  \u2018unsigned\u2019: smallest unsigned int dtype (min.: np.uint8)\n\n  \u2018float\u2019: smallest float dtype (min.: np.float32)\n","f0b8142e":"*importing svm from sklearn we are fitting the trained data using kernel as rbf*","bfc112be":"### Load Data From CSV File  ","14cb9744":"*Defining the training and testing data set as 80:20 ratio and getting the shape of data set as x_train,x_test,y_train,y_test*","e123547b":"Saransh Goyal\\\nA214\\\nB.Tech IT\\\n70011118018","ff99c3b2":"After being fitted, the model can then be used to predict new values:","14b0e6fb":"It looks like the __BareNuc__ column includes some values that are not numerical. We can drop those rows: ","dd3f70f7":"*printing the f1 score*","2ca3a792":"*Defining the features which will be used for svm in another data frame and also defining an numpy array of the features*","cf8eef77":"<h2 id=\"load_dataset\">Load the Cancer data<\/h2>\nThe example is based on a dataset that is publicly available from the UCI Machine Learning Repository (Asuncion and Newman, 2007)[http:\/\/mlearn.ics.uci.edu\/MLRepository.html]. The dataset consists of several hundred human cell sample records, each of which contains the values of a set of cell characteristics. The fields in each record are:\n\n|Field name|Description|\n|--- |--- |\n|ID|Clump ID|\n|Clump|Clump thickness|\n|UnifSize|Uniformity of cell size|\n|UnifShape|Uniformity of cell shape|\n|MargAdh|Marginal adhesion|\n|SingEpiSize|Single epithelial cell size|\n|BareNuc|Bare nuclei|\n|BlandChrom|Bland chromatin|\n|NormNucl|Normal nucleoli|\n|Mit|Mitoses|\n|Class|Benign or malignant|\n\n<br>\n<br>\n\nFor the purposes of this example, we're using a dataset that has a relatively small number of predictors in each record. To download the data, we will use `!wget` (We studied it in last Lab) to download it from IBM Object Storage.  \n","0e72f23b":"In this notebook, you will use SVM (Support Vector Machines) to build and train a model using human cell records, and classify cells to whether the samples are benign or malignant.\n\nAs we discussed in the class SVM is to map data to a high-dimensional feature space so that data points can be categorized, even when the data are not otherwise linearly separable. A separator between the categories is found, then the data is transformed in such a way that the separator could be drawn as a hyperplane. Following this, characteristics of new data can be used to predict the group to which a new record should belong.","d07ff07d":"*predicting the values using the fitted model using the X_test*","4745d8f1":"*Using the wget! function to retrive data set from object storage*","063a5c0f":"Double-click __here__ for the solution.\n\n<!-- Your answer is below:\n    \nclf2 = svm.SVC(kernel='linear')\nclf2.fit(X_train, y_train) \nyhat2 = clf2.predict(X_test)\nprint(\"Avg F1-score: %.4f\" % f1_score(y_test, yhat2, average='weighted'))\nprint(\"Jaccard score: %.4f\" % jaccard_similarity_score(y_test, yhat2))\n\n-->","3f5937b7":"*using jaccard_similarity score for y_test,yhta* "}}