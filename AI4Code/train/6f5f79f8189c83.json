{"cell_type":{"d56d0d13":"code","8e4ef27f":"code","6a801aaa":"code","b54d1097":"code","0bf29387":"code","4b90b914":"code","69c6aff8":"code","c34a6ca8":"code","99c1f11d":"code","ee0431f3":"markdown"},"source":{"d56d0d13":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\nfrom torch import nn, optim\nfrom torchvision import transforms, models, datasets\nimport json\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\ntrain_data_path = \"..\/input\/flower_data\/flower_data\/train\"\nvalidation_data_path = \"..\/input\/flower_data\/flower_data\/valid\"\ntest_data_path = \"..\/input\/test set\/test set\"\n\n# Any results you write to the current directory are saved as output.\ncat_dic = json.load(open(\"..\/input\/cat_to_name.json\"))\nprint(cat_dic[\"21\"])","8e4ef27f":"#load the data using a generator\ntrain_transforms = transforms.Compose([transforms.RandomRotation(30),\n                                       transforms.RandomResizedCrop(224),\n                                       transforms.RandomHorizontalFlip(),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize([0.485, 0.456, 0.406],\n                                                            [0.229, 0.224, 0.225])])\n\nvalidation_transforms = transforms.Compose([transforms.Resize(255),\n                                      transforms.CenterCrop(224),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize([0.485, 0.456, 0.406],\n                                                           [0.229, 0.224, 0.225])])\n\n# Pass transforms in here, then run the next cell to see how the transforms look\ntrain_data = datasets.ImageFolder(train_data_path , transform=train_transforms)\nvalidation_data = datasets.ImageFolder(validation_data_path, transform=validation_transforms)\n#test_data = datasets.ImageFolder(test_data_path, transforms = validation_transforms)\n\ntrainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\nvalidationloader = torch.utils.data.DataLoader(validation_data, batch_size=64)\n#testlodaerr = torch.utils.data.DataLoader(test_data, batch_size=64)","6a801aaa":"# checking the distribution of data\ndef count_samples_in_class(images, classes):\n    samples = [0] * len(classes)\n    for item in images:\n        samples[item[1]] +=1\n    return samples\n    \nsamples = count_samples_in_class(train_data.imgs, train_data.classes)\nsamples = torch.FloatTensor(samples)\n\ncategories = np.array(list(cat_dic.values()), dtype=str)\nd = pd.DataFrame(categories, columns=['Name'])\nd['Frequency'] = samples\nprint(f'Total Images:{len(train_data.imgs)}')\nprint(f'mean of dataset is:{torch.mean(samples)}')\nprint(f'STd of dataset is: {torch.std(samples)}')\nprint(f'Variance of dataset is: {torch.var(samples)}')\nplt.boxplot(samples)\nplt.show()\n#plt.plot(samples)\nplt.plot(samples.numpy())\n#plt.plot(samples.numpy())\nd.T\n","b54d1097":"# util for sampling balanced data - and checking the class distribution\nsampler = torch.utils.data.sampler.RandomSampler(train_data, replacement=True, num_samples=64)                     \n                                                                                \ntrainloader = torch.utils.data.DataLoader(train_data, batch_size=64,                               \n                                                             sampler = sampler)  \n#trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n#print(weights)\n","0bf29387":"#download the model\nmodel = models.resnext101_32x8d(pretrained=True,progress=True)\n","4b90b914":"#remove the final layer from model\norig_model = model\n\n# freezing with no backprop\nfor param in model.parameters():\n    param.requires_grad = False\n\nfrom collections import OrderedDict\nclassifier = nn.Sequential(OrderedDict([\n                          ('fc1', nn.Linear(2048, 1000)),\n                          ('relu', nn.ReLU()),\n                          ('fc2', nn.Linear(1000, 102)),\n                          ('output', nn.LogSoftmax(dim=1))\n                          ]))\n    \nmodel.fc = classifier\ncriterion = nn.NLLLoss()\n# Only train the classifier parameters, feature parameters are frozen\noptimizer = optim.SGD(model.fc.parameters(), lr=0.1)\n\n#print(model)","69c6aff8":"import time\n# Use GPU if it's available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# shift to device the mdoel\nmodel.to(device);\nprint(device)\n\nepochs = 45\nsteps = 0\nrunning_loss = 0\nprint_every = 5\nfor epoch in range(epochs):\n    \n    # for the logs in kaggle kernel.\n    print_string = f\"echo epoch is: {epoch}\"\n    os.system(print_string)\n    \n    # after 35th epoch slow down the learning rate as per rexnet paper\n    if epoch == 30:\n        for g in optimizer.param_groups:\n            g['lr'] = 0.001\n    \n    for inputs, labels in trainloader:\n        steps += 1\n        # Move input and label tensors to the default device\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        logps = model.forward(inputs)\n        loss = criterion(logps, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        \n        if steps % print_every == 0:\n            test_loss = 0\n            accuracy = 0\n            model.eval()\n            with torch.no_grad():\n                for inputs, labels in validationloader:\n                    inputs, labels = inputs.to(device), labels.to(device)\n                    logps = model.forward(inputs)\n                    batch_loss = criterion(logps, labels)\n                    \n                    test_loss += batch_loss.item()\n                    \n                    # Calculate accuracy\n                    ps = torch.exp(logps)\n                    top_p, top_class = ps.topk(1, dim=1)\n                    equals = top_class == labels.view(*top_class.shape)\n                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n                    \n            print(f\"Epoch {epoch+1}\/{epochs}.. \"\n                  f\"Step: {steps}...\"\n                  f\"Train loss: {running_loss\/print_every:.3f}.. \"\n                  f\"Test loss: {test_loss\/len(validationloader):.3f}.. \"\n                  f\"Test accuracy: {accuracy\/len(validationloader):.3f}\")\n            running_loss = 0\n            model.train()","c34a6ca8":"# Process our image\ndef process_image(image_path):\n    # Load Image\n    img = Image.open(image_path)\n    \n    # Get the dimensions of the image\n    width, height = img.size\n    \n    # Resize by keeping the aspect ratio, but changing the dimension\n    # so the shortest size is 255px\n    img = img.resize((255, int(255*(height\/width))) if width < height else (int(255*(width\/height)), 255))\n    \n    # Get the dimensions of the new image size\n    width, height = img.size\n    \n    # Set the coordinates to do a center crop of 224 x 224\n    left = (width - 224)\/2\n    top = (height - 224)\/2\n    right = (width + 224)\/2\n    bottom = (height + 224)\/2\n    img = img.crop((left, top, right, bottom))\n    \n    # Turn image into numpy array\n    img = np.array(img)\n    \n    # Make the color channel dimension first instead of last\n    img = img.transpose((2, 0, 1))\n    \n    # Make all values between 0 and 1\n    img = img\/255\n    \n    # Normalize based on the preset mean and standard deviation\n    img[0] = (img[0] - 0.485)\/0.229\n    img[1] = (img[1] - 0.456)\/0.224\n    img[2] = (img[2] - 0.406)\/0.225\n    \n    # Add a fourth dimension to the beginning to indicate batch size\n    img = img[np.newaxis,:]\n    \n    # Turn into a torch tensor\n    image = torch.from_numpy(img)\n    image = image.float()\n    return image","99c1f11d":"#Kindly run this for predictions. There might be some seems to be some issue with some of the images with respect to dimensions. So running a  :)\nfrom PIL import *\nfiles = os.listdir(\"..\/input\/test set\/test set\")\nindex = 0\nprint(len(files))\nfor file in files:\n    image = process_image(test_data_path+\"\/\"+file)\n    image = image.to(device)\n    try:\n        output = model.forward(image)\n        # Reverse the log function in our output\n        output = torch.exp(output)\n\n        # Get the top predicted class, and the output percentage for that class\n        probs, classes = output.topk(1, dim=1)\n        class_name = cat_dic[str(classes.item())]\n        print (f\"{index}: \"\n                f\"file name: {file},  \"\n                f\"Predicted Class: {class_name},  \"\n                f\"Class Number: {classes.item()}\")\n        index+=1\n    except:\n        print(\"An exception occurred\")\n        continue\n        index+=1\n    ","ee0431f3":"**As seen in the plot above the distribution is not very uniform. And hence we will end up with a model which is squed towards the majority classes.** In fact if you see commit 11, you will see how the biace is very clear in the final models testing.\n\nbased on the above analysis, we are going to apply a random sampler which will sample the average from each class. \nThe number of samples drawn per class: 64\nNumber of classes = 102\nRepetition = True\nSampler = RrandomSampler\nTotal Samples Which Will be Taken = 102*64 = 6528\nTotal Images = "}}