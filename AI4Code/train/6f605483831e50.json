{"cell_type":{"1b2d6b24":"code","33203f24":"code","48cde314":"code","6f9a8dc8":"code","1cb2d231":"code","867d9bb7":"code","823863bc":"code","df18b073":"code","c3891497":"code","2a62cd3e":"code","ae4856af":"code","05eb703a":"code","b9f9d64e":"code","b70782bb":"markdown","e7fc8441":"markdown","ad6fbf70":"markdown","76f9de62":"markdown","b0463dbb":"markdown","a54bebde":"markdown","751a5403":"markdown","373daa94":"markdown","bbaca4df":"markdown","1bc04502":"markdown","9ae35360":"markdown","a41a30e4":"markdown"},"source":{"1b2d6b24":"!pip install -q git+https:\/\/github.com\/keras-team\/keras-tuner.git\n!pip install -q autokeras","33203f24":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nimport autokeras as ak","48cde314":"AUTOTUNE = tf.data.AUTOTUNE\n\nCONFIGURATION = dict (\n    seed = 0,\n    nbr_folds = 5,\n    img_width = 250,\n    img_height = 250,\n    batch_size = 16,\n    epochs = 1000\n)","6f9a8dc8":"df = pd.read_csv('..\/input\/seti-breakthrough-listen\/train_labels.csv')\n\ndf['img_path'] = df['id'].apply(lambda x: f'..\/input\/seti-breakthrough-listen\/train\/{x[0]}\/{x}.npy')\n\nprint(df.shape)\n\ndf.head()","1cb2d231":"neg = df.target.value_counts()[0]\npos = df.target.value_counts()[1]\n\ntotal = neg + pos\n\n# Weigts to correct imbalance:\n\n# Scaling by total\/2 helps keep the loss to a similar magnitude.\n# The sum of the weights of all examples stays the same.\nweight_for_0 = (1 \/ neg) * (total \/ 2.0)\nweight_for_1 = (1 \/ pos) * (total \/ 2.0)\n\nclass_weights = {0: weight_for_0, 1: weight_for_1}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))","867d9bb7":"kfold = StratifiedKFold(\n    n_splits=CONFIGURATION[\"nbr_folds\"],\n    shuffle=True,\n    random_state=CONFIGURATION[\"seed\"]\n)\n\nfor n, (train_index, val_index) in enumerate(kfold.split(df, df['target'])):\n    df.loc[val_index, 'fold'] = int(n)\n\ndf['fold'] = df['fold'].astype(int)\n\ndf.groupby(['fold', 'target']).size()","823863bc":"def load_npy(path):\n    \n    # load npy data\n    data = np.load(path.numpy()).astype(np.float32)\n    \n    # stack -> we only keep the on-target observations\n    data = np.dstack((data[0], data[2], data[4]))\n    \n    return data\n\n# decoration\n\n@tf.function\ndef load_resize_data_augmentation(df_dict):\n    # Load image\n    [image,] = tf.py_function(load_npy, [df_dict['img_path']], [tf.float32])\n    image.set_shape((273, 256, 3))\n    \n    # Resize image\n    image = tf.image.resize(image, (CONFIGURATION['img_height'], CONFIGURATION['img_width']))\n    \n    \n    # Data augmentation\n    image = tf.image.random_brightness(image, 0.2, seed=CONFIGURATION[\"seed\"])\n    image = tf.image.random_hue(image, 0.2, seed=CONFIGURATION[\"seed\"])\n    image = tf.image.random_flip_up_down(image, seed=CONFIGURATION[\"seed\"])\n    \n    # Parse label\n    label = df_dict['target']\n    label = tf.cast(label, tf.float32)\n    \n    return image, label\n\n@tf.function\ndef load_resize_spec(df_dict):\n    # Load image\n    [image,] = tf.py_function(load_npy, [df_dict['img_path']], [tf.float32])\n    image.set_shape((273, 256, 3))\n    \n    # Resize image\n    image = tf.image.resize(image, (CONFIGURATION['img_height'], CONFIGURATION['img_width']))\n    \n    # Parse label\n    label = df_dict['target']\n    label = tf.cast(label, tf.float32)\n    \n    return image, label","df18b073":"def get_dataloaders(train_df, valid_df):\n    trainloader = tf.data.Dataset.from_tensor_slices(dict(train_df))\n    validloader = tf.data.Dataset.from_tensor_slices(dict(valid_df))\n\n    trainloader = (\n        trainloader\n        .shuffle(1024)\n        .map(load_resize_data_augmentation, num_parallel_calls=AUTOTUNE)\n        .batch(CONFIGURATION['batch_size'])\n        .prefetch(AUTOTUNE)\n    )\n\n    validloader = (\n        validloader\n        .map(load_resize_spec, num_parallel_calls=AUTOTUNE)\n        .batch(CONFIGURATION['batch_size'])\n        .prefetch(AUTOTUNE)\n    )\n    \n    return trainloader, validloader","c3891497":"rdm_state = CONFIGURATION[\"seed\"]\n\n# Preparing the train and validation df\n# We are only using one fold for this example\n# You can create a loop in order to train models with different folds\ntrain_df = df.loc[df.fold != 0].reset_index(drop=True)\nvalid_df = df.loc[df.fold == 0].reset_index(drop=True)\n\n# We will use a subset in order to accelerate the training time\n# size of the subset -> 10 % of the whole dataset\n# Delete these lines of code, from here...\ntrain_subset, _ = train_test_split(train_df, stratify=train_df.target, random_state=rdm_state, train_size=0.1)\nvalid_subset, _ = train_test_split(valid_df, stratify=valid_df.target, random_state=rdm_state, train_size=0.1)\ntrain_df = train_subset.reset_index(drop=True)\nvalid_df = valid_subset.reset_index(drop=True)\n# ... to here, in order to use the whole dataset\n\n# Preparing the dataloaders\ntrainloader, validloader = get_dataloaders(train_df, valid_df)","2a62cd3e":"# Parameters are explained here: https:\/\/autokeras.com\/image_classifier\/\n\n# The maximum number of different Keras Models to try (for example: 10)\nmax_number = 10\n\nclf = ak.ImageClassifier(\n    max_trials=max_number,\n    metrics=[tf.keras.metrics.AUC(curve='ROC')],\n    loss=tfa.losses.SigmoidFocalCrossEntropy(),\n    overwrite=True,\n    tuner=\"bayesian\",\n    seed=CONFIGURATION[\"seed\"]\n)","ae4856af":"# Feed the tensorflow Dataset to the classifier.\n\nclf.fit(\n    trainloader,\n    epochs=CONFIGURATION[\"epochs\"],\n    validation_data=validloader,\n    class_weight=class_weights,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            patience=3,\n            restore_best_weights=True)\n    ]\n)","05eb703a":"# Export as a Keras Model.\nmodel = clf.export_model()\n\nmodel.evaluate(validloader)","b9f9d64e":"model.summary()","b70782bb":"# Load the *train_label.csv* file with pandas","e7fc8441":"# Set the configuration we will need during this demonstration","ad6fbf70":"# Create folds","76f9de62":"# Import libraries","b0463dbb":"# Estimate class weights based on the imbalance of the data set","a54bebde":"# Download AutoKeras","751a5403":"# Initialize the image classifier","373daa94":"# Training of the image classifier","bbaca4df":"This kernel will show you **how to use AutoML with the SETI dataset**.\n\nI used AutoKeras to perform AutoML. You can find information and examples on how to use AutoKeras on its [website](https:\/\/autokeras.com\/).\n\nAlso, in order to use the SETI dataset correctly and to create dataloaders, I used this [kernel](https:\/\/www.kaggle.com\/ayuraj\/train-tensorflow-efficientnet-kfold-w-b) from Ayush Thakur.\n\nFor this demonstration, I will only use 10% of the dataset in order to get results faster.\n\nNote: this code is not intended to win this competition, only to present a tutorial on how to use AutoML with the SETI dataset.\n\n------------------\n\n*AutoKeras:*\n\n*Haifeng Jin, Qingquan Song, and Xia Hu. \"Auto-keras: An efficient neural architecture search system.\" Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2019*","1bc04502":"# Dataloaders with 10% of the dataset","9ae35360":"The number of epochs is very high due to the use of an [**early stopping callback**](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/EarlyStopping).\nThe model will stop learning if it is overfitting (patience parameter).\nSo the number of epochs is not important.\n\nIf you don't use an early stopping callback, you should optimise the number of epochs to avoid overfitting and underfitting.","a41a30e4":"# Data loading and pre-processing functions"}}