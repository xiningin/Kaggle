{"cell_type":{"d55defb0":"code","efb7b37f":"code","8e972483":"code","7a53c57b":"code","5f4d62b7":"code","4707e0ab":"code","da828fa0":"code","c55cb435":"markdown","3fd6f634":"markdown","7bf8b5ba":"markdown","c018eea6":"markdown","833fd405":"markdown","95d59585":"markdown","c4493ba3":"markdown"},"source":{"d55defb0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","efb7b37f":"# matplotlib configure\nplt.rcParams['image.cmap'] = 'gray'\n# Color from R ggplot colormap\ncolor = ['#6388b4', '#ffae34', '#ef6f6a', '#8cc2ca', '#55ad89', '#c3bc3f', '#bb7693', '#baa094', '#a9b5ae', '#767676']","8e972483":"mnist = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\nmnist.head()","7a53c57b":"label = mnist['label']\nmnist.drop(['label'], inplace=True, axis=1)","5f4d62b7":"def arr2img(arr, img_size=(28, 28)):\n    return arr.reshape(img_size)\n\nfig, axes = plt.subplots(2, 5, figsize=(10, 2))\n\nfor idx, ax in enumerate(axes.flat):\n    ax.imshow(arr2img(mnist[idx:idx+1].values))\n    ax.set_title(label[idx], fontweight='bold', fontsize=8)\n    ax.axis('off')\n\nplt.subplots_adjust(bottom=0.1, right=0.5, top=0.9)\nplt.show()\n","4707e0ab":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\npca.fit(mnist)\nmnist_pca = pca.transform(mnist)","da828fa0":"import plotly.graph_objects as go\n\nfig = go.Figure()\n\nfor idx in range(10):\n    fig.add_trace(go.Scatter(\n        x = mnist_pca[:,0][label==idx],\n        y = mnist_pca[:,1][label==idx],\n        name=str(idx),\n        opacity=0.6,\n        mode='markers',\n        marker=dict(color=color[idx])\n        \n    ))\n\nfig.update_layout(\n    width = 800,\n    height = 800,\n    title = \"PCA result\",\n    yaxis = dict(\n      scaleanchor = \"x\",\n      scaleratio = 1\n    ),\n    legend=dict(\n        orientation=\"h\",\n        yanchor=\"bottom\",\n        y=1.02,\n        xanchor=\"right\",\n        x=1\n    )\n)\n\n\nfig.show()","c55cb435":"## Import Library & Default Setting","3fd6f634":"## Dimension Reduction [1] : PCA\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/2\/27\/MnistExamples.png)\n\nI am going to make a series with dimension reduction.\n\n- [\ud83c\udf9b Dimension Reduction [2] : LDA](https:\/\/www.kaggle.com\/subinium\/dimension-reduction-2-lda)\n- [\ud83c\udf9b Dimension Reduction [3] : T-SNE](https:\/\/www.kaggle.com\/subinium\/dimension-reduction-3-t-sne)\n- [\ud83c\udf9b Dimension Reduction [4] : UMAP](https:\/\/www.kaggle.com\/subinium\/dimension-reduction-4-umap)","7bf8b5ba":"As it is an interactive visualization, you can check if they are clustered by clicking the above category(legend).","c018eea6":"After plotting with plotly, it's actually not really necessary, so let's always make the settings for custom","833fd405":"## Check Dataset\n\nNo one knows the mnist, but let's see what kind of data it is","95d59585":"## PCA & Result\n\n**PCA** stands for Principal Component Analysis. \n\nThe goal is to reduce the dimensions in multi-dimensional data, and to do this we use the technique of linear algebra.\n\nIn short, it is a method to find the axis with the highest variance in the distribution and eliminate the vertical axis.\n\nIn this way, it is assumed that only the axes with the highest information remain, so that the meaning is maintained to some extent when the dimension is reduced.\n\nMuch of this dimensionality reduction is provided by scikit-learn. There are various types of PCA, but this time, only the most basic is applied.","c4493ba3":"We can recognize the 784's binary information as a 28x28 sized image, but the computer is not. \n\nSo the computer simply understands it as a 784-dimensional vector."}}