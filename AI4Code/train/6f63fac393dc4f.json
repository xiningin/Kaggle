{"cell_type":{"192e821d":"code","7d384cf5":"code","6a9345b1":"code","8377c36b":"code","3c7e3df9":"code","c93b1610":"code","82cbba29":"code","bcfe455b":"code","3e963e74":"code","516c39e2":"code","a288480f":"code","a59858b3":"code","d74ae1c4":"code","e0c8be0e":"code","63bcf5a4":"code","3121697f":"code","57d7322e":"code","39b68fda":"code","75f8ffed":"code","14595b20":"code","de3b1dcf":"code","92d43f4d":"code","5c5b4801":"code","e09bd80d":"code","d567b8fc":"code","3d777e89":"code","43abf1dc":"code","e3981417":"code","f613a067":"code","8c5e8f59":"code","3cb7320f":"code","683ea05c":"code","f49111bb":"code","df73c114":"code","058a16e0":"code","896cdb95":"code","c013fb27":"code","af6aab7b":"markdown","24f24727":"markdown","427e0060":"markdown","2ae94d25":"markdown","0b9aab8e":"markdown","e30bb6a2":"markdown","82d4e486":"markdown","1ee63743":"markdown","ef84331d":"markdown","7694bd5b":"markdown","6a3782ed":"markdown","5f05b7ee":"markdown","d8433b02":"markdown"},"source":{"192e821d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7d384cf5":"# Import standard libraries\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')","6a9345b1":"# Create a preproprocessig function to preprocess the dataFrames\ndef preprocessor (df):\n    \n    # A function to assign values for marital_status based on the number of SibSp and Parch\n    def marital_status_conditions(df):\n        # if passenger has no SibSp and no Parch\n        if df.SibSp == 0 and df.Parch == 0:\n            return 1\n        # if passenger has at least a SibSp and no Parch\n        elif df.SibSp != 0 and df.Parch == 0:\n            return 2\n        # if passenger has no SibSp and at least a Parch\n        elif df.SibSp == 0 and df.Parch != 0:\n            return 3\n        # if passenger has both SibSp and  Parch\n        else:\n            return 4    \n    \n    # A loop to convert non_numeric dtypes to categories and then to numbers\n    for label, content in df.items():\n        if not pd.api.types.is_numeric_dtype(content):\n            # change non-numeric columns to categories\n            df[label] = content.astype('category').cat.as_ordered()\n            # Convert the categories into codes\n            df[label] = pd.Categorical(content).codes + 1\n     \n    # Create the marital status column from the marital_status_conditions \n    df['marital_status'] = df.apply(marital_status_conditions, axis = 1)\n    \n    # drop irrelevant columns\n    df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis = 1, inplace=True)\n    \n    return df","8377c36b":"# Import and preprocess the training data using the preprocessor function\ntraining_df = preprocessor(pd.read_csv('\/kaggle\/input\/titanic\/train.csv'))\ntraining_df.head()","3c7e3df9":"# Split the training data into X an y, then training and validation set\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(94)\n\n# Splitting into features and labels\nX = training_df.drop('Survived', axis = 1)\ny = training_df['Survived']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.30)","c93b1610":"# Fill the missing age values with the 'median' age\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\nage_col = ['Age']\nage_imputer = SimpleImputer(strategy = 'median')\n    \nimputer = ColumnTransformer([('age_imputer', age_imputer, age_col)],\n                                remainder = 'passthrough')\n\nfilled_X_train = imputer.fit_transform(X_train)\nfilled_X_val = imputer.transform(X_val)","82cbba29":"# View the shape of the filled X data\nfilled_X_train.shape","bcfe455b":"# A function to convert the ages into categories. \n# 1 (child) if passenger is under 21 and 2 (adult) if passenger is at least 21 yrs\ndef age_category(dataset):\n    age_cat = np.array([1 if age >= 21 else 2 for age in dataset[: , 0]])[:, np.newaxis]\n    dataset = np.concatenate((dataset, age_cat), axis = 1)\n    \n    return dataset","3e963e74":"# Categorise the ages for the training and validation sets\nfilled_X_train = age_category(filled_X_train)\nfilled_X_val = age_category(filled_X_val)","516c39e2":"# Import different machine learning models\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\n# A dictionary to store the models {model_name, model()}\nmodels = {'RandomForest': RandomForestClassifier(),\n          'LogisticRegression' : LogisticRegression(),\n          'KNN' : KNeighborsClassifier(),\n          'SVC': SVC()}","a288480f":"# A function to do cross_validation scores of the various classification metrics\n\nfrom sklearn.model_selection import cross_val_score\n\nnp.random.seed(94)\n\ndef evaluate_preds(model, X_train, y_train):\n    '''\n    Performs evaluation comparison on y_true labels vrs y_preds labels on a classification model\n    '''\n    accuracy = np.mean(cross_val_score(model, X_train, y_train, scoring='accuracy', cv = 5))\n    precision = np.mean(cross_val_score(model, X_train, y_train, scoring='precision', cv = 5))\n    recall = np.mean(cross_val_score(model, X_train, y_train, scoring='recall',cv = 5))\n    f1 = np.mean(cross_val_score(model, X_train, y_train, scoring='f1', cv= 5))\n    \n    metric_dict = {'accuracy' : round(accuracy, 2),\n                   'precision' : round(precision, 2),\n                   'recall' : round(recall, 2),\n                   'f1' : round(f1, 2)}\n    \n    return metric_dict ","a59858b3":"# An empty dataframe intended to store the evaluation metrics for each model\nmetrics_df = pd.DataFrame()\n\nfor name, model in models.items():\n    # Return the metrics for each model\n    metrics = evaluate_preds(model, filled_X_train, y_train)    \n    # Create a new column to store the metrics of each model\n    metrics_df[name] = metrics.values()\n    \n# Set the indices of the dataFrame with the keys of the metrics dictionary\nmetrics_df.index = metrics.keys()\n\n# Show the DataFrame\nmetrics_df","d74ae1c4":"# Visualise the performance of the models\nmetrics_df.plot.bar(rot = 0);","e0c8be0e":"# %%time\n# from sklearn.model_selection import RandomizedSearchCV\n# np.random.seed(94)\n# grid = {'n_estimators' : np.arange(10, 1000, 5),\n#         'max_depth' : [None, 3, 5, 10],\n#         'min_samples_split' : np.arange(2, 20, 2),\n#         'min_samples_leaf' : np.arange(1, 20, 2)}\n\n# rs_model = RandomizedSearchCV(RandomForestClassifier(),\n#                               param_distributions = grid,\n#                               cv = 5,\n#                               n_iter = 40,\n#                               n_jobs = -1,\n#                               verbose = 2)\n\n# rs_model.fit(filled_X_train, y_train)","63bcf5a4":"# # Score the rs_model on the validation set\n# rs_score= rs_model.score(filled_X_val, y_val)\n# rs_score","3121697f":"# Get the best parameters for rs_model\n# rs_model.best_params_","57d7322e":"# %%time\n# from sklearn.model_selection import GridSearchCV\n\n# # Create a param grid\n# gs_grid = {'n_estimators': [250, 260, 270, 280],\n#            'min_samples_split': [2, 4, 6],\n#            'min_samples_leaf': [3, 5],\n#            'max_depth': [None, 10]}\n\n# #Instantiate the grid_serach model\n# gs_model = GridSearchCV(RandomForestClassifier(),\n#                         param_grid = gs_grid,\n#                         verbose = 2,\n#                         cv = 5,\n#                         n_jobs = -1)\n# # Fit the model\n# gs_model.fit(filled_X_train, y_train)","39b68fda":"# Score the grid_search model on the validation set\n# gs_model.score(filled_X_val, y_val)","75f8ffed":"# Get the best parameters\n# gs_model.best_params_","14595b20":"# randomizedSearch metrics using the best parameters\nrf_rs_metrics = evaluate_preds(RandomForestClassifier(n_estimators= 250, \n                                                      min_samples_split= 4,\n                                                      min_samples_leaf= 3,\n                                                      max_depth= 10), \n                               filled_X_train, y_train)\n\n# randomizedSearch metrics using the best parameters\nrf_gs_metrics = evaluate_preds(RandomForestClassifier(max_depth= None,\n                                                      min_samples_leaf= 3,\n                                                      min_samples_split= 6,\n                                                      n_estimators= 260), \n                               filled_X_train, y_train)\n\n# A default randomForest metrics\nrf_clf = RandomForestClassifier()\nrf_metrics = evaluate_preds(rf_clf, filled_X_train, y_train)","de3b1dcf":"# Create a dataframe for the RF models\nrf_models_metrics = pd.DataFrame({'Default RF' : rf_metrics,\n              'RandomisedSearch RF' : rf_rs_metrics,\n              'GridSearch RF' : rf_gs_metrics})\n# View the DataFrame\nrf_models_metrics","92d43f4d":"# Visulise the RF models to compare performance\nrf_models_metrics.plot.bar(rot = 0, title = 'Comparing RandomForest models');","5c5b4801":"# Import and preprocess the test_df using the preprocessor function\ntest_df = preprocessor(pd.read_csv('\/kaggle\/input\/titanic\/test.csv'))\ntest_df.head()","e09bd80d":"# view the lenght of the test data\nlen(test_df)","d567b8fc":"# Check for missing values\ntest_df.isna().sum()","3d777e89":"# Fill missing fare value\ntest_df.Fare.fillna(test_df.Fare.median(), inplace=True)","43abf1dc":"# Use the imputer to fill the missing age values for the test data\nfilled_test_df = imputer.fit_transform(test_df)\n\n# Categorise the ages of the passenger using the age_category function\nfilled_test_df = age_category(filled_test_df)","e3981417":"## Make predictions on the test data using the best of the RF models (gs_model)\nnp.random.seed(94)\n\n# Instantiate the model\nclf = RandomForestClassifier(max_depth= None,\n                             min_samples_leaf= 3,\n                             min_samples_split= 2,\n                             n_estimators= 460)\n# Fit the model\nclf.fit(filled_X_train, y_train)\n\n# Make predictions from the test df\ntest_predictions = clf.predict(filled_test_df)","f613a067":"# Create and view a DataFrame for the predicted values for submission\noutput = pd.DataFrame({'PassengerID' : np.arange(892, 892 + len(test_df)),\n                        'Survived' : test_predictions})\noutput.to_csv('Titanic-submission.csv', index=False)","8c5e8f59":"## View the feature importances pf the RF model\nclf.feature_importances_","3cb7320f":"# Save the prediction file to csv\n# results.to_csv('..\/Data\/Titanic Data\/results.csv',\n#               index= False)","683ea05c":"# Instantiate the Logistic regression model\nlr_clf = LogisticRegression()\n\n# Fit it to the training data\nlr_clf.fit(filled_X_train, y_train)\n\n# Make predictions on the test_data\npredictions = lr_clf.predict(filled_test_df)","f49111bb":"# Create and view a DataFrame for the predicted values for submission\noutput = pd.DataFrame({'PassengerID' : np.arange(892, 892 + len(test_df)),\n                        'Survived' : predictions})\noutput.to_csv('lr_model.csv', index = False)","df73c114":"## View the feature importances pf the LR model\n# lr_clf.feature_importances_","058a16e0":"# results.to_csv('..\/Data\/Titanic Data\/lr_model_results.csv',\n#               index= False)","896cdb95":"from  xgboost import XGBClassifier\n\nxg_clf = XGBClassifier(n_estimators = 500, random_state = 94, learning_rate = 0.05, use_label_encoder=False)\n\nxg_clf.fit(filled_X_train, y_train,\n           eval_set = [(filled_X_val, y_val)],\n           eval_metric = 'error',\n           early_stopping_rounds = 20)","c013fb27":"xg_preds = xg_clf.predict(filled_test_df)\n\n# Create and view a DataFrame for the predicted values for submission\noutput = pd.DataFrame({'PassengerID' : np.arange(892, 892 + len(test_df)),\n                        'Survived' : xg_preds})\noutput.to_csv('xg_model.csv', index = False)","af6aab7b":"## Import Machine Learning Classification models","24f24727":"Create metrics for the various RandomForest models for comparison","427e0060":"### Make predictions on the test data with a default logistic regression model","2ae94d25":"........... Coming in the my next version","0b9aab8e":"# Hyperparameter tuning of the RandomForestClassifier()","e30bb6a2":"## Preparing the data for machine learning","82d4e486":"# Import and preprocessing the training data","1ee63743":"## Exploratory analysis on the training data","ef84331d":"## Compare the evaluation metrics of various RandomForest models","7694bd5b":"## ...UsingGridSearchCV","6a3782ed":"# Import and process the test_data","5f05b7ee":"## Evaluation metrics function","d8433b02":"## ...UsingRandomised SearchCV"}}