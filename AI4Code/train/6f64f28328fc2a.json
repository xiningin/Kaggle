{"cell_type":{"c46c6f70":"code","36c25b9d":"code","98d5d615":"code","b29944c9":"code","d7c5e4b9":"code","538908cc":"markdown","0f115464":"markdown","fbaa4c3c":"markdown","6eb8e0de":"markdown"},"source":{"c46c6f70":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np, pandas as pd, os\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.metrics import roc_auc_score as auc\nfrom functools import partial\n\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\nMagicFeat = 'wheezy-copper-turtle-magic'\ncols = [c for c in train_df.columns if c not in ['id', 'target', MagicFeat]]\n","36c25b9d":"def trainer(Model, train, test):\n    oof = np.zeros(len(train))\n    preds = np.zeros(len(test))\n    # BUILD 512 SEPARATE MODELS\n    for i in range(512):\n        # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n        train2 = train[train[MagicFeat]==i]\n        test2 = test[test[MagicFeat]==i]\n        idx1 = train2.index; idx2 = test2.index\n        train2.reset_index(drop=True,inplace=True)\n\n        # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n        sel = VarianceThreshold(threshold=1.5).fit(train2[cols])\n        train3 = sel.transform(train2[cols])\n        test3 = sel.transform(test2[cols])\n        \n        # STRATIFIED K-FOLD\n        skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n        for train_index, test_index in skf.split(train3, train2['target']):\n            # Train MODEL AND PREDICT\n            clf = Model()\n            clf.fit(train3[train_index,:], train2.loc[train_index]['target'])\n            oof[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n            preds[idx2] += clf.predict_proba(test3)[:,1] \/ skf.n_splits\n    return oof, preds","98d5d615":"from sklearn.neighbors import NearestNeighbors\nfrom lightgbm import LGBMClassifier\nfrom scipy.stats import skew, kurtosis, hmean, gmean\n\ndef getFeatures(dists):\n    '''\n    Generates features from the distance matrix of shape (n_samples by n_neighbors)\n    '''\n    Features = []\n    Funcs = [np.amin, np.amax, np.mean, np.std, np.median, hmean, gmean, kurtosis, skew]+\\\n                [partial(np.percentile, q=p) for p in [1,5,10,90,95,99]]\n    for func in Funcs:\n        Features.append(func(dists, axis=1).reshape(-1,1))\n    return np.concatenate(Features, axis=1)\n\n\nclass KNN_CLF():\n    '''\n    Binary KNN Classifier:\n    It takes the distances to the k nearest neighbors of a node from\n    samples from both 0, and 1 classes, extracts some features from\n    the distance matrices, and trains a classifier over it.\n    By default the classifier is LightGBM.\n    '''\n    __slots__ = ['k', 'knn0', 'knn1', 'clf']\n    def __init__(self, knn_params={}, k=5, CLF=LGBMClassifier, clf_params={}):\n        self.k = k\n        self.knn0 = NearestNeighbors(**knn_params)\n        self.knn1 = NearestNeighbors(**knn_params)\n        self.clf = CLF(**clf_params)\n        \n    def fit(self, X, y):\n        self.knn0.fit(X[y==0, :])\n        self.knn1.fit(X[y==1, :])\n        # during training the first neighbor is the sample itself, \n        # so we take k+1 neighbors and take the first out\n        F0 = getFeatures(\n            self.knn0.kneighbors(X,\n                                 n_neighbors=min(self.k+1, self.knn0._fit_X.shape[0]),\n                                 return_distance=True)[0][:,1:])\n        F1 = getFeatures(\n            self.knn1.kneighbors(X,\n                                 n_neighbors=min(self.k+1, self.knn1._fit_X.shape[0]),\n                                 return_distance=True)[0][:,1:])\n        XX = np.concatenate((F0, F1, F1-F0), axis=1)\n        self.clf.fit(XX, y, verbose=0)\n\n    def predict_proba(self, X):\n        F0 = getFeatures(\n            self.knn0.kneighbors(X,\n                                 n_neighbors=min(self.k, self.knn0._fit_X.shape[0]),\n                                 return_distance=True)[0])\n        F1 = getFeatures(\n            self.knn1.kneighbors(X,\n                                 n_neighbors=min(self.k, self.knn1._fit_X.shape[0]),\n                                 return_distance=True)[0])\n        XX = np.concatenate((F0, F1, F1-F0), axis=1)\n        return self.clf.predict_proba(XX)\n        ","b29944c9":"Model = partial(KNN_CLF, k=150)\ntrOut, tsOut = trainer(Model, train_df, test_df)\nprint(auc(train_df.target, trOut))\n\nimport matplotlib.pyplot as plt\nplt.hist(trOut,bins=100)\nplt.title('OOF predictions')\nplt.show()\n\nplt.figure()\nplt.hist(tsOut,bins=100)\nplt.title('Test.csv predictions')\nplt.show()\n","d7c5e4b9":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['target'] = tsOut\nsub.to_csv('submission.csv',index=False)\n\n","538908cc":"The wrapper class of the KNNs and the classifier:","0f115464":"training ...","fbaa4c3c":"This part is mainly taken from Chris's kernel:\nhttps:\/\/www.kaggle.com\/cdeotte\/support-vector-machine-0-925\n\nThanks [@cdeotte](https:\/\/www.kaggle.com\/cdeotte)","6eb8e0de":"In this kernel, I'm presenting another approach to the problem, that I didn't see in the public kernels. (Maybe I have not noticed, sorry in that case).\n\nThe approach is to take the closest samples from 0 and 1 classes, for each querry sample, then extract some features from them, and train a classifier on top of those features. Here I have used LightGBM, and my features are normal statistical features calculated from the distance matrices."}}