{"cell_type":{"d72bbfb4":"code","997f6784":"code","f21d5439":"code","4cc71d5b":"code","bb8cd4bb":"code","fa5199b7":"code","30266cbf":"code","a220429b":"markdown","ccfb27aa":"markdown","d2ddc40a":"markdown"},"source":{"d72bbfb4":"from IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport statistics\nimport gc\nimport scipy.stats as scipy\n\nfrom statsmodels.tsa.arima_model import ARIMA\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nimport lightgbm as lgb\n\nfrom sklearn.metrics import r2_score\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\npath_root = '..\/input\/optiver-realized-volatility-prediction'\npath_data = '..\/input\/optiver-realized-volatility-prediction'\npath_submissions = '\/'\n\ntarget_name = 'target'\nscores_folds = {}","997f6784":"#added feature engineering functions\n#simple average\ndef simple_averaging(series):\n    return np.mean(series)\n\ndef variance(series):\n    return np.var(series)\n\n#standard distribution\ndef std(series):\n    return np.std(series)\n\ndef skew(series):\n    return scipy.skew(series)\n\ndef kurtosis(series):\n    return scipy.kurtosis(series)\n\n#0.25 quantile\ndef quantile_1(series):\n    return np.quantile(series,0.25)\n\n#0.5 quantile\ndef quantile_2(series):\n    return np.quantile(series,0.5)\n\n#0.75 quantile\ndef quantile_3(series):\n    return np.quantile(series,0.75)\n\n#ARIMA Model\ndef arima(series):\n    model = ARIMA(series, order=(1,0,0))\n    results = model.fit()\n    return results.fittedvalues\n\n# imported from https:\/\/www.kaggle.com\/tommy1028\/lightgbm-starter-with-feature-engineering-idea\ndef count_unique(series):\n    return len(np.unique(series))\n\n#original ones\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() # the log return of two prices\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2)) # realized volatility given series log returns\n\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))) # rmsp error used for grading\n\ndef book_preprocessor(df_book, stock_id):    \n    #additional feature code, added params\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df_book[df_book['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg([simple_averaging,variance,std,skew,kurtosis,quantile_1,quantile_2,quantile_3, count_unique]).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n    \n    # Create row_id so we can merge\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n\ndef get_stock_stat(stock_id : int, dataType = 'train'):\n    key = ['stock_id', 'time_id', 'seconds_in_bucket']\n    \n    #Book features\n    df_book = pd.read_parquet(os.path.join(path_data, 'book_{}.parquet\/stock_id={}\/'.format(dataType, stock_id))) # get data for one particular stockid\n    df_book['stock_id'] = stock_id\n    cols = key + [col for col in df_book.columns if col not in key] # add some columns to the key\n    df_book = df_book[cols] # filter out things not in columns\n    \n    df_book['price_spread'] = (df_book['ask_price1'] - df_book['bid_price1']) \/ ((df_book['ask_price1'] + df_book['bid_price1'])\/2)\n    df_book['bid_spread'] = df_book['bid_price1'] - df_book['bid_price2']\n    df_book['ask_spread'] = df_book['ask_price1'] - df_book['ask_price2']\n    df_book['total_volume'] = (df_book['ask_size1'] + df_book['ask_size2']) + (df_book['bid_size1'] + df_book['bid_size2'])\n    df_book['volume_imbalance'] = abs((df_book['ask_size1'] + df_book['ask_size2']) - (df_book['bid_size1'] + df_book['bid_size2']))\n    \n    df_book['wap1'] = (df_book['bid_price1'] * df_book['ask_size1'] +\n                                    df_book['ask_price1'] * df_book['bid_size1']) \/ (df_book['bid_size1'] + df_book['ask_size1']) #wap for best offer\n    df_book['wap2'] = (df_book['bid_price2'] * df_book['ask_size2'] +\n                                    df_book['ask_price2'] * df_book['bid_size2']) \/ (df_book['bid_size2'] + df_book['ask_size2']) #wap for second best\n    df_book['wap_balance'] = abs(df_book['wap1'] - df_book['wap2'])\n    \n    df_book['log_return1'] = df_book.groupby(by = ['time_id'])['wap1'].apply(log_return).fillna(0)\n    df_book['log_return2'] = df_book.groupby(by = ['time_id'])['wap2'].apply(log_return).fillna(0)# log return calculation\n    \n    features_to_apply_realized_volatility = ['log_return'+str(i+1) for i in range(2)] #['logreturn1','logreturn2']\n    stock_stat = df_book.groupby(by = ['stock_id', 'time_id'])[features_to_apply_realized_volatility]\\\n                        .agg(realized_volatility).reset_index() #doing some regrouping\n    \n    #other features to look at \n    features_to_apply_common_stats=['price_spread','bid_spread','ask_spread','total_volume','volume_imbalance','wap1','wap2','wap_balance']\n    more_stock_stat=df_book.groupby(by = ['stock_id', 'time_id'])[features_to_apply_common_stats]\\\n    .agg([simple_averaging,variance,std,skew,kurtosis,quantile_1,quantile_2,quantile_3, count_unique]).reset_index()\n    helper_stock_stat=book_preprocessor(df_book,stock_id)\n    # merging the new features with the old\n    stock_stat = stock_stat.merge(more_stock_stat, # more_stock_stat is the new features engineered\n                                  on=['stock_id', 'time_id'], \n                                  how='left').fillna(-999) #merge two datasets\n    stock_stat = stock_stat.merge(helper_stock_stat, # more_stock_stat is the new features engineered\n                                  on=['stock_id', 'time_id'], \n                                  how='left').fillna(-999) #merge two datasets\n    \n    #Trade features\n    trade_stat =  pd.read_parquet(os.path.join(path_data,'trade_{}.parquet\/stock_id={}'.format(dataType, stock_id)))\n    trade_stat = trade_stat.sort_values(by=['time_id', 'seconds_in_bucket']).reset_index(drop=True)\n    trade_stat['stock_id'] = stock_id\n    cols = key + [col for col in trade_stat.columns if col not in key]\n    trade_stat = trade_stat[cols]\n    trade_stat['trade_log_return1'] = trade_stat.groupby(by = ['time_id'])['price'].apply(log_return).fillna(0) #some feature engineering\n    trade_stat = trade_stat.groupby(by = ['stock_id', 'time_id'])[['trade_log_return1']]\\\n                           .agg(realized_volatility).reset_index()\n    #Joining book and trade features\n    stock_stat = stock_stat.merge(trade_stat, on=['stock_id', 'time_id'], how='left').fillna(-999) #merge two datasets\n    \n    return stock_stat\n\ndef get_dataSet(stock_ids : list, dataType = 'train'):\n\n    stock_stat = Parallel(n_jobs=-1)(\n        delayed(get_stock_stat)(stock_id, dataType) \n        for stock_id in stock_ids\n    ) #some parallel loading?\n    \n    stock_stat_df = pd.concat(stock_stat, ignore_index = True)\n\n    return stock_stat_df\n\ndef feval_RMSPE(preds, train_data):\n    labels = train_data.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False #getting results\n\nparams_lgbm = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'learning_rate': 0.01,\n        'objective': 'regression',\n        'metric': 'None',\n        'max_depth': -1,\n        'n_jobs': -1,\n        'feature_fraction': 0.7,\n        'bagging_fraction': 0.7,\n        'lambda_l2': 1,\n        'verbose': -1\n        #'bagging_freq': 5\n}","f21d5439":"train = pd.read_csv(os.path.join(path_data, 'train.csv')) #reading and filtering  some data\n\n\nDEBUG=True\nif DEBUG:\n    stock_ids = train['stock_id'].unique()[:5]\nelse:\n    stock_ids = train['stock_id'].unique()","4cc71d5b":"# train = pd.read_csv(os.path.join(path_data, 'train.csv')) #reading and filtering  some data\ntrain_stock_stat_df = get_dataSet(stock_ids = train['stock_id'].unique(), dataType = 'train')\n","bb8cd4bb":"train = pd.merge(train, train_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\nprint('Train shape: {}'.format(train.shape))\ndisplay(train.head(2))\n\ntest = pd.read_csv(os.path.join(path_data, 'test.csv'))\ntest_stock_stat_df = get_dataSet(stock_ids = test['stock_id'].unique(), dataType = 'test')\ntest = pd.merge(test, test_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left').fillna(0)\nprint('Test shape: {}'.format(test.shape))\ndisplay(test.head(2))","fa5199b7":"all_columns = train.columns.tolist()\nall_columns_test = test.columns.tolist()\n\nfor ix, col in enumerate(all_columns):\n    if isinstance(col, tuple):\n        all_columns[ix] = \"_\".join(col)\n\nfor ix, col in enumerate(all_columns_test):\n    if isinstance(col, tuple):\n        all_columns_test[ix] = \"_\".join(col)\n        \ntrain.columns = all_columns\ntest.columns = all_columns_test","30266cbf":"cats = ['stock_id']\nmodel_name = 'lgb1'\npred_name = 'pred_{}'.format(model_name)\nfeatures_to_consider = ['stock_id', 'log_return1', 'log_return2', 'trade_log_return1', \n                        'wap1_simple_averaging','wap2_simple_averaging',\n                        'wap_balance_simple_averaging','price_spread_simple_averaging',\n                        'bid_spread_simple_averaging','ask_spread_simple_averaging',\n                        'total_volume_simple_averaging','volume_imbalance_simple_averaging',\n                        'wap1_variance','wap2_variance','wap_balance_variance','price_spread_variance',\n                        'bid_spread_variance','ask_spread_variance','total_volume_variance','volume_imbalance_variance',\n                        'wap1_std','wap2_std','wap_balance_std','price_spread_std',\n                        'bid_spread_std','ask_spread_std','total_volume_std','volume_imbalance_std',\n                        'wap1_skew','wap2_skew','wap_balance_skew','price_spread_skew',\n                        'bid_spread_skew','ask_spread_skew','total_volume_skew','volume_imbalance_skew',\n                        'wap1_kurtosis','wap2_kurtosis','wap_balance_kurtosis','price_spread_kurtosis',\n                        'bid_spread_kurtosis','ask_spread_kurtosis','total_volume_kurtosis','volume_imbalance_kurtosis',\n                        'wap1_quantile_1','wap2_quantile_1','wap_balance_quantile_1','price_spread_quantile_1',\n                        'bid_spread_quantile_1','ask_spread_quantile_1','total_volume_quantile_1','volume_imbalance_quantile_1',\n                        'wap1_quantile_2','wap2_quantile_2','wap_balance_quantile_2','price_spread_quantile_2',\n                        'bid_spread_quantile_2','ask_spread_quantile_2','total_volume_quantile_2','volume_imbalance_quantile_2',\n                        'wap1_quantile_3','wap2_quantile_3','wap_balance_quantile_3','price_spread_quantile_3',\n                        'bid_spread_quantile_3','ask_spread_quantile_3','total_volume_quantile_3','volume_imbalance_quantile_3',\n                        'wap1_count_unique','wap2_count_unique','wap_balance_count_unique','price_spread_count_unique',\n                        'bid_spread_count_unique','ask_spread_count_unique','total_volume_count_unique','volume_imbalance_count_unique',\n                        \n                       ]# some parameters in this part\nprint('We consider {} features'.format(len(features_to_consider)))\n\ntrain[pred_name] = 0\ntest['target'] = 0\n\nn_folds = 4\nn_rounds = 5000\nkf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2016) #not sure what is this part\nscores_folds[model_name] = []\ncounter = 1\nfor dev_index, val_index in kf.split(range(len(train))): \n    print('CV {}\/{}'.format(counter, n_folds))\n    X_train = train.loc[dev_index, features_to_consider]\n    y_train = train.loc[dev_index, target_name].values\n    X_val = train.loc[val_index, features_to_consider]\n    y_val = train.loc[val_index, target_name].values\n    \n    #############################################################################################\n    #LGB\n    #############################################################################################\n    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cats, weight=1\/np.power(y_train,2))\n    val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cats, weight=1\/np.power(y_val,2))\n    \n    model = lgb.train(params_lgbm, \n                      train_data, \n                      n_rounds, \n                      valid_sets=val_data, \n                      feval=feval_RMSPE,\n                      verbose_eval= 250,\n                      early_stopping_rounds=500\n                     )\n    preds = model.predict(train.loc[val_index, features_to_consider])\n    train.loc[val_index, pred_name] = preds\n    score = round(rmspe(y_true = y_val, y_pred = preds),5)\n    print('Fold {} {}: {}'.format(counter, model_name, score))\n    scores_folds[model_name].append(score)\n    counter += 1\n    test[target_name] += model.predict(test[features_to_consider]).clip(0,1e10)\ndel train_data, val_data\ntest[target_name] = test[target_name]\/n_folds\n\nscore = round(rmspe(y_true = train[target_name].values, y_pred = train[pred_name].values),5)\nprint('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name])) #getting the scores and stuff\n\ndisplay(test[['row_id', target_name]].head(2))\ntest[['row_id', target_name]].to_csv('submission.csv',index = False)\n\nimportances = pd.DataFrame({'Feature': model.feature_name(), \n                            'Importance': model.feature_importance(importance_type='gain')})\nimportances.sort_values(by = 'Importance', inplace=True)\nimportances2 = importances.nlargest(50,'Importance', keep='first').sort_values(by='Importance', ascending=True)\nimportances2[['Importance', 'Feature']].plot(kind = 'barh', x = 'Feature', figsize = (8,6), color = 'blue', fontsize=11);plt.ylabel('Feature', fontsize=12)","a220429b":"## Training model and making predictions","ccfb27aa":"## Train and test datasets","d2ddc40a":"## LGB starter\n\nIn this notebook:\n* I build simple features from book and trade datasets;\n* I train a lightgbm model **with weights** with a custom metric (RMSPE) and obtain a CV score;\n\nCredits to:\n* https:\/\/www.kaggle.com\/jiashenliu\/introduction-to-financial-concepts-and-data\n* https:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/discussion\/250324\n* https:\/\/www.kaggle.com\/swimmy\/optiver-lgb-with-optimized-params\n\n"}}