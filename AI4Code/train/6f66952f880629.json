{"cell_type":{"95a603c5":"code","2b94f8c5":"code","5e01523f":"code","a80b7a95":"code","3ec2faa7":"code","c4ef2cc8":"code","7446a054":"code","152d8817":"code","7b744d55":"code","a72e80f4":"code","916907d2":"code","faadd27a":"code","92ddbf32":"code","cecb760b":"code","33863e3e":"code","65260dfa":"code","663a1ebf":"code","bac03db1":"markdown","e305dcec":"markdown","62e6707e":"markdown","a16a2f5b":"markdown","2db83ccc":"markdown","7a9d64ed":"markdown"},"source":{"95a603c5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2b94f8c5":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.callbacks import EarlyStopping\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix,classification_report\n\nimport nltk\nimport nltk as nlp\nimport string\nimport re\n","5e01523f":"train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","a80b7a95":"news = pd.concat([train, test]).reset_index(drop = True)\nnews.head()","3ec2faa7":"news.dropna(inplace=True)\n","c4ef2cc8":"print(news.size)\nmissing_val_count_by_column = (news.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","7446a054":"x_train,x_test,y_train,y_test = train_test_split(news['text'], news.target, test_size=0.2, random_state=2020)\n\npipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', LogisticRegression())])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))","152d8817":"print(confusion_matrix(y_test, prediction))","7b744d55":"print(classification_report(y_test, prediction))","a72e80f4":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', DecisionTreeClassifier(criterion= 'entropy',\n                                           max_depth = 10, \n                                           splitter='best', \n                                           random_state=2020))])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))","916907d2":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', RandomForestClassifier())])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))","faadd27a":"X = news.text\nY = news.target\nle = LabelEncoder()\nY = le.fit_transform(Y)\nY = Y.reshape(-1,1)","92ddbf32":"LabelEncoder()\nY = le.fit_transform(Y)\nY = Y.reshape(-1,1)\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15)\nmax_words = 500\nmax_len = 75\ntok = Tokenizer(num_words=max_words)\ntok.fit_on_texts(X_train)\nsequences = tok.texts_to_sequences(X_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\ndef RNN():\n    inputs = Input(name='inputs',shape=[max_len])\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    layer = LSTM(64)(layer)\n    layer = Dense(256,name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1,name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model\nmodel = RNN()","cecb760b":"from tensorflow.keras.utils import plot_model \nplot_model(model, to_file='model1.png')\nmodel.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])","33863e3e":"from tensorflow.keras.callbacks import EarlyStopping","65260dfa":"model.fit(sequences_matrix,Y_train,batch_size=256,epochs=1,\n          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","663a1ebf":"test_sequences = tok.texts_to_sequences(X_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\naccr = model.evaluate(test_sequences_matrix,Y_test)\nprint('Accuracy: {:0.2f}'.format(accr[1]))","bac03db1":"# Decision Tree","e305dcec":"# Logistic Regression Classifier","62e6707e":"# Logistic Regression Classifier","a16a2f5b":"Hi all, I tried cleaning the dataset. But I wasn't able to do it, hence applied classification algorithms without cleaning and processing the data. \nIf one of you could clean it, it would be great. ","2db83ccc":"# Random Forest Classifier","7a9d64ed":"# LSTM"}}