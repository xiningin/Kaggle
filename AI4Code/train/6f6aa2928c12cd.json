{"cell_type":{"f37418c2":"code","105e4c95":"code","526da5c0":"code","5dec118f":"code","f0114632":"code","42158155":"code","1c13c51c":"code","15f370db":"code","82543491":"code","c95fbfb7":"code","ed4d2a63":"code","847d0930":"code","c34153f5":"code","cdac41f2":"code","b39bcb1d":"code","08de8b95":"code","8b8867ec":"code","80bc16dd":"code","01f787f8":"code","f7f855df":"code","a184ccfe":"code","354139bf":"code","ba682ba2":"code","f6efba46":"code","29f88bf0":"code","c77c1b73":"code","8ae20cde":"code","43f37f1b":"code","f2a879e9":"code","31799f28":"code","785db4d8":"code","17ef3cd9":"code","57871c87":"code","2a17b2f5":"code","1f46ecb9":"code","7b16b378":"code","de7981f9":"code","4c8cb711":"code","354c9446":"code","7214f7f2":"code","a5f74319":"code","8e2625d9":"code","5f6dc05d":"code","c1753844":"code","2fe72745":"code","34ca1038":"code","a081a001":"code","8062c2d5":"code","2d5cbcac":"code","fa2f550e":"code","e3e79869":"code","3ec09898":"code","0954e29d":"code","22017aa8":"code","43cb543f":"code","87f6f39b":"code","db1d9f14":"code","7e53830c":"code","d9091b46":"code","a94f1a2b":"code","57c2eead":"code","9ec375a0":"code","795c5a66":"code","9b938b66":"code","4ec18593":"code","528b5f3a":"code","62e13cbe":"code","42a0c8ea":"code","95008b0a":"code","0268e8e2":"code","530aad7e":"code","9ee692c6":"code","0ccdc16d":"code","948f9f13":"code","b9d6bac6":"code","d295a7a4":"code","a989eea4":"code","9aa34122":"code","235fa5b3":"code","2c434666":"code","47c65fc3":"code","51ab9372":"code","6066ce42":"code","0b320b28":"code","d36d0e07":"code","411bd1ed":"code","2e9cfe2d":"code","3e42751d":"code","9a19d48b":"code","d6366b2f":"code","dc43c752":"code","4454cf48":"code","9e15bd92":"code","1020e199":"code","5010f5fb":"code","372ca748":"code","26b2538d":"code","1fffbb8a":"code","09fb697d":"code","1d1423ae":"code","f79fd294":"code","079e31db":"code","903d6d63":"code","9e1b9607":"code","db76793b":"code","96e7dc6b":"code","ac6e17a2":"markdown","c5634479":"markdown","fc70f6d9":"markdown","e03f360d":"markdown","e07abd27":"markdown","16d7758b":"markdown","1a7f04a4":"markdown","782bca78":"markdown","cfc721fe":"markdown","b24b9ad4":"markdown","4f252f11":"markdown","f45ab89c":"markdown","4aa7eb34":"markdown","b1344633":"markdown","a11ffedf":"markdown","42fea8ec":"markdown","fb590916":"markdown","af99d333":"markdown","a3fd67bd":"markdown","c60b9aae":"markdown","f176a930":"markdown","8b31d7f3":"markdown","2c7714a2":"markdown","c90b5077":"markdown","71ecb02b":"markdown","cfaa2ee1":"markdown","bc0edd07":"markdown","3368f5df":"markdown","fddfd3e7":"markdown","7fb1c72a":"markdown","b439a9c9":"markdown","eb2e315e":"markdown","6796801f":"markdown","3cd6bb94":"markdown","dfa54fbc":"markdown","528d7661":"markdown","f74a7392":"markdown","0bf45ce7":"markdown","b4f7a693":"markdown","4b2674fe":"markdown","bdd6387b":"markdown","1870f36a":"markdown","ad0baa19":"markdown","f3ff13e0":"markdown","5401e9e2":"markdown","5b68be5e":"markdown","6094e90f":"markdown","5cb1ee90":"markdown","537c364f":"markdown","4f89a10c":"markdown","d22006d5":"markdown","84324ba9":"markdown","f513aa17":"markdown","b38d08d8":"markdown","02d7e2bd":"markdown","b7b865e1":"markdown","a012b9d6":"markdown"},"source":{"f37418c2":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\npd.set_option(\"display.max_columns\", None)\n# pd.set_option('display.max_rows', None)\npd.set_option(\"display.max_rows\", 200)\n\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score,roc_auc_score, roc_curve, confusion_matrix, precision_recall_curve\n# For pandas profiling\nfrom pandas_profiling import ProfileReport","105e4c95":"path = '\/kaggle\/input\/tourism\/Tourism.csv'\ndata = pd.read_csv(path) #load the data","526da5c0":"df= data.copy() #making a copy to avoid changes to data\nprint(f\"There are {df.shape[0]} rows and {df.shape[1]} columns.\")\n#checking the shape of the dataset\nnp.random.seed(15) \ndf.sample(10) #loading random 10 rows","5dec118f":"df.info() # looking at the structure of the data","f0114632":"df.ProdTaken.unique()","42158155":"df.drop(['CustomerID'],axis=1,inplace=True) ","1c13c51c":"cat_cols = ['CityTier','ProdTaken','NumberOfPersonVisiting','NumberOfChildrenVisiting','PreferredPropertyStar','Passport','PitchSatisfactionScore','OwnCar']\ndf[cat_cols] = df[cat_cols].astype('category')\n\ncols = data.select_dtypes(['object']) #selecting all object datatypes and converting to category\nfor i in cols.columns:\n    df[i] = df[i].astype('category')\n\ndf.info() #rechecking the dataset    ","15f370db":"df.isna().sum()","82543491":"missing_numerical = df.select_dtypes(include=np.number).columns.tolist()\nmissing_numerical.remove('Age')\nmissing_numerical.remove('MonthlyIncome')\nmissing_numerical","c95fbfb7":"medianFiller = lambda x: x.fillna(x.median()) #replacing with the Median value of the attributes\ndf[missing_numerical] = df[missing_numerical].apply(medianFiller,axis=0)","ed4d2a63":"#we will replace the missing values with median income w.r.t the customer's designation\ndf[\"MonthlyIncome\"] = df.groupby(['Designation'])['MonthlyIncome'].transform(lambda x: x.fillna(x.median()))\ndf[\"Age\"] = df.groupby(['Designation'])['Age'].transform(lambda x: x.fillna(x.median()))","847d0930":"df.describe().T","c34153f5":"cat_cols =  df.select_dtypes(['category'])\nfor i in cat_cols.columns:\n    print(cat_cols[i].value_counts())\n    print('-'*50)\n    print('\\n')","cdac41f2":"#treating missing values in categorical variables\ndf['TypeofContact'] = df['TypeofContact'].fillna('Self Enquiry')\ndf['NumberOfChildrenVisiting'] = df['NumberOfChildrenVisiting'].fillna(1.0)\ndf['PreferredPropertyStar'] = df['PreferredPropertyStar'].fillna(3.0)\n\ndf.Gender = df.Gender.replace('Fe Male','Female') #treating error","b39bcb1d":"df.isnull().sum()","08de8b95":"df.describe(include=\"category\").T","8b8867ec":"#Performing Univariate Analysis to study the central tendency and dispersion\n#Plotting histogram to study distribution\nUni_num = df.select_dtypes(include=np.number).columns.tolist()\nplt.figure(figsize=(17,75))\nfor i in range(len(Uni_num)):     #creating a loop that will show the plots for the columns in one plot\n    plt.subplot(18,3,i+1)\n    sns.histplot(df[Uni_num[i]],kde=False)\n    plt.tight_layout()\n    plt.title(Uni_num[i],fontsize=25)\n\nplt.show()","80bc16dd":"plt.figure(figsize=(15,35))\nfor i in range(len(Uni_num)):\n    plt.subplot(10,3,i+1)\n    sns.boxplot(df[Uni_num[i]],showmeans=True, color='yellow')\n    plt.tight_layout()\n    plt.title(Uni_num[i],fontsize=25)\n\nplt.show()","01f787f8":"categorical_val = df.select_dtypes(exclude=np.number).columns.tolist()","f7f855df":"plt.figure(figsize=(15,75))\nfor i in range(len(categorical_val)):     #creating a loop that will show the plots for the columns in one plot\n    plt.subplot(18,3,i+1)\n    ax=sns.countplot(df[categorical_val[i]],palette='Spectral')\n    plt.tight_layout()\n    plt.title(categorical_val[i],fontsize=25)\n    total = len (df[categorical_val[i]])\n    for p in ax.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_height()\/total) # percentage of each class of the category\n        x = p.get_x() + (p.get_width() \/ 2)-0.1  # width of the plot\n        y = p.get_y() + p.get_height()           # hieght of the plot\n        ax.annotate(percentage, (x, y), size = 12.5,color='black') # To annonate\nplt.xticks(rotation=90)\nplt.show()","a184ccfe":"corr= df.corr()\nplt.figure(figsize=(10,7))\nsns.heatmap(corr,annot= True,vmin=0,vmax=0.7, cmap='RdYlGn_r',linewidths=0.75)\nplt.show()","354139bf":"# For all numerical variables with Personal_Loan\nplt.figure(figsize=(15,10))\nfor i, variable in enumerate(Uni_num):\n                     plt.subplot(3,2,i+1)\n                     sns.boxplot(df['ProdTaken'],df[variable],palette=\"Set1\")\n                     plt.tight_layout()\n                     plt.title(variable)\nplt.show()","ba682ba2":"#Stacked plot of categorical variables with Personal Loans\ndef stacked_plot(x):\n    sns.set(palette='Dark2')\n    tab1 = pd.crosstab(x,df['ProdTaken'],margins=True)\n    print(tab1)\n    print('-'*120)\n    tab = pd.crosstab(x,df['ProdTaken'],normalize='index')\n    tab.plot(kind='bar',stacked=True,figsize=(10,5))\n    plt.legend(loc='lower left', frameon=True)\n    plt.legend(loc=\"upper left\", bbox_to_anchor=(1,1))\n    plt.ylabel('Percentage')\n    plt.show()","f6efba46":"stacked_plot(df.TypeofContact)","29f88bf0":"stacked_plot(df.CityTier)","c77c1b73":"stacked_plot(df.Occupation)","8ae20cde":"stacked_plot(df.Gender)","43f37f1b":"stacked_plot(df.NumberOfPersonVisiting)","f2a879e9":"stacked_plot(df.ProductPitched)","31799f28":"stacked_plot(df.PreferredPropertyStar)","785db4d8":"stacked_plot(df.MaritalStatus)","17ef3cd9":"stacked_plot(df.Passport)","57871c87":"stacked_plot(df.PitchSatisfactionScore)","2a17b2f5":"stacked_plot(df.OwnCar)","1f46ecb9":"stacked_plot(df.NumberOfChildrenVisiting)","7b16b378":"stacked_plot(df.Designation)","de7981f9":"#Let's find the percentage of outliers using IQR","4c8cb711":"Q1 = data.quantile(0.25)             #To find the 25th percentile and 75th percentile.\nQ3 = data.quantile(0.75)\n\nIQR = Q3 - Q1                           #Inter Quantile Range (75th perentile - 25th percentile)\n\nlower=Q1-1.5*IQR                        #Finding lower and upper bounds for all values. All values outside these bounds are outliers\nupper=Q3+1.5*IQR","354c9446":"outlier_num = df.select_dtypes(include=np.number)","7214f7f2":"((outlier_num<lower)|(outlier_num>upper)).sum()\/len(df)*100","a5f74319":"from sklearn.metrics import classification_report,confusion_matrix\n\ndef make_confusion_matrix(y_actual,y_predict):\n    '''\n    y_predict: prediction of class\n    y_actual : ground truth  \n    '''\n    sns.set(font_scale=2.0) # to set font size for the matrix\n    cm=confusion_matrix(y_actual,y_predict)\n    group_names = ['True -ve','False +ve','False -ve','True +ve']\n    group_counts = [\"{0:0.0f}\".format(value) for value in\n                cm.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in\n                         cm.flatten()\/np.sum(cm)]\n    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2,v3 in\n              zip(group_names,group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    plt.figure(figsize = (10,7))\n    sns.heatmap(cm, annot=labels,fmt='',cmap='Blues')\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","8e2625d9":"#Defining a function to calculate all the metric scores for the model\ndef scores(model,flag=True):\n    \"\"\" model : classifier to predict X values \"\"\"\n    score_list=[] # creating an empty list to store the accuracy and f1(metric of interst)\n    \n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    \n    train_acc = metrics.accuracy_score(y_train,y_pred_train)\n    test_acc = metrics.accuracy_score(y_test,y_pred_test)\n    \n    train_f1 = metrics.f1_score(y_train,y_pred_train)\n    test_f1 = metrics.f1_score(y_test,y_pred_test)\n    score_list.extend((train_acc,test_acc,train_f1,test_f1))\n    \n    if flag== True:\n        print(\"Accuracy on training set : \",metrics.accuracy_score(y_train,y_pred_train))\n        print(\"Accuracy on test set : \",metrics.accuracy_score(y_test,y_pred_test))\n\n        print(\"\\nRecall on training set : \",metrics.recall_score(y_train,y_pred_train))\n        print(\"Recall on test set : \",metrics.recall_score(y_test,y_pred_test))\n    \n        print(\"\\nPrecision on training set : \",metrics.precision_score(y_train,y_pred_train))\n        print(\"Precision on test set : \",metrics.precision_score(y_test,y_pred_test))\n    \n        print(\"\\nF1 on training set : \",metrics.f1_score(y_train,y_pred_train))\n        print(\"F1 on test set : \",metrics.f1_score(y_test,y_pred_test))\n    elif flag == False:\n        return score_list #return this when flag is False\n    ","5f6dc05d":"X= df.drop(['ProdTaken','PitchSatisfactionScore','ProductPitched','NumberOfFollowups','DurationOfPitch'],axis=1)\ny= df['ProdTaken']","c1753844":"X = pd.get_dummies(X, drop_first=True)\n# Splitting data into training and test set:\nX_train,X_test, y_train, y_test =train_test_split(X,y, test_size=0.3, random_state=25,stratify=y)\nprint(X_train.shape,X_test.shape)","2fe72745":"y.value_counts(1)","34ca1038":"y_test.value_counts(1)","a081a001":"from sklearn import tree\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeClassifier\nDt = DecisionTreeClassifier(criterion='gini',class_weight={0:0.15,1:0.85},random_state=25)","8062c2d5":"Dt.fit(X_train, y_train) #fit the train set to Decision tree\ny_predict = Dt.predict(X_test) \nmake_confusion_matrix(y_test,y_predict) #calculate confusion matrix","2d5cbcac":"scores(Dt) #calculate the metric scores","fa2f550e":"bagging = BaggingClassifier(random_state=25) #defining the classifier with randomstate 25\nbagging.fit(X_train,y_train)","e3e79869":"y_predict = bagging.predict(X_test)\nmake_confusion_matrix(y_test,y_predict)","3ec09898":"scores(bagging)","0954e29d":"bagging_wt = BaggingClassifier(base_estimator=DecisionTreeClassifier(criterion='gini',class_weight={0:0.15,1:0.85},random_state=25),random_state=25)\nbagging_wt.fit(X_train,y_train)","22017aa8":"y_predict = bagging_wt.predict(X_test)\nmake_confusion_matrix(y_test,y_predict)","43cb543f":"scores(bagging_wt)","87f6f39b":"random_forest = RandomForestClassifier(random_state=25)\nrandom_forest.fit(X_train,y_train)","db1d9f14":"y_predict = random_forest.predict(X_test)\nmake_confusion_matrix(y_test,y_predict)","7e53830c":"scores(random_forest)","d9091b46":"random_forest_wt = RandomForestClassifier(class_weight={0:0.15,1:0.85},random_state=25)\nrandom_forest_wt.fit(X_train,y_train)","a94f1a2b":"y_predict = random_forest_wt.predict(X_test)\nmake_confusion_matrix(y_test,y_predict)","57c2eead":"scores(random_forest_wt)","9ec375a0":"# Choose the type of classifier. \nDTclassifier = DecisionTreeClassifier(random_state=25,class_weight = {0:.15,1:.85}) #adding classweights \n\n#Defining the Hyperparameters\n\nparameters = {'max_depth': np.arange(10,60,10), \n            'criterion': ['gini','entropy'],\n            'min_samples_leaf': [ 2, 5, 7, 10],\n            'max_leaf_nodes' : [3, 5, 10,15],}\n\n# Type of scoring used to compare parameter combinations\nF1_scorer = metrics.make_scorer(metrics.f1_score)\n\n# Run the grid search with the above parameters\ngrid_obj = GridSearchCV(DTclassifier, parameters, scoring=F1_scorer,cv=5)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set to the best combination of parameters\nclassifier = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nclassifier.fit(X_train, y_train)","795c5a66":"y_predict = classifier.predict(X_test) \nmake_confusion_matrix(y_test,y_predict) ","9b938b66":"scores(classifier)","4ec18593":"# grid search for bagging classifier\nparameters = {\n              'n_estimators':np.arange(10,60,10),\n              'max_features': [0.7,0.8,0.9],\n              'max_samples': [0.7,0.8,0.9], \n             }\n\n#Assigning Bootstrap = True to select features with Replacement\nbagging_tuned = BaggingClassifier(random_state=25,bootstrap=True)\n\n# Type of scoring used to compare parameter combinations\nF1_scorer = metrics.make_scorer(metrics.f1_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(bagging_tuned, parameters, scoring=F1_scorer,cv=5)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nbagging_estimator_tuned = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data.\nbagging_estimator_tuned.fit(X_train, y_train)","528b5f3a":"y_predict = bagging_estimator_tuned.predict(X_test)\nmake_confusion_matrix(y_test,y_predict)","62e13cbe":"scores(bagging_estimator_tuned)","42a0c8ea":"# Grid of parameters to choose from\nrandomforest_tuned = RandomForestClassifier(class_weight={0:0.15,1:0.85},random_state=29)\n\nparameters = {\"n_estimators\": np.arange(10,60,5),\n              'criterion':['gini','entropy'],\n            \"min_samples_leaf\": np.arange(5,11,1),\n            \"max_features\":['sqrt','log2'],\n            \"max_samples\": np.arange(0.5, 1, 0.1),\n             }\n\n# Type of scoring used to compare parameter combinations\nF1_scorer = metrics.make_scorer(metrics.f1_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(randomforest_tuned, parameters, scoring=F1_scorer,cv=5)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nrf_estimator_tuned = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data.\nrf_estimator_tuned.fit(X_train, y_train)","95008b0a":"y_predict = rf_estimator_tuned.predict(X_test)\nmake_confusion_matrix(y_test,y_predict)","0268e8e2":"scores(rf_estimator_tuned)","530aad7e":"Ada_boost = AdaBoostClassifier(random_state=25)\nAda_boost.fit(X_train,y_train)","9ee692c6":"y_pred = Ada_boost.predict(X_test)\nmake_confusion_matrix(y_test,y_pred)","0ccdc16d":"scores(Ada_boost)","948f9f13":"Grad_boost = GradientBoostingClassifier(random_state=25)\nGrad_boost.fit(X_train,y_train)","b9d6bac6":"y_predict = Grad_boost.predict(X_test)\nmake_confusion_matrix(y_test,y_predict)","d295a7a4":"scores(Grad_boost)","a989eea4":"Grad_boost_ada = GradientBoostingClassifier(init=AdaBoostClassifier(random_state=25),random_state=25)\nGrad_boost_ada.fit(X_train,y_train)","9aa34122":"y_predict = Grad_boost_ada.predict(X_test)\nmake_confusion_matrix(y_test,y_predict)","235fa5b3":"scores(Grad_boost_ada)","2c434666":"XG_boost = XGBClassifier(random_state=25, eval_metric='logloss')\nXG_boost.fit(X_train,y_train)","47c65fc3":"y_predict = XG_boost.predict(X_test)\nmake_confusion_matrix(y_test,y_predict)","51ab9372":"scores(XG_boost)","6066ce42":"# Choose the type of classifier. \nAda_tuned = AdaBoostClassifier(random_state=25)\n\n# Grid of parameters to choose from\nparameters = {\n    #Let's try different max_depth for base_estimator\n    \"base_estimator\":[DecisionTreeClassifier(max_depth=1),DecisionTreeClassifier(max_depth=2),\n                      DecisionTreeClassifier(max_depth=3)],\n    \"n_estimators\": np.arange(10,60,10),\n    'learning_rate': [0.05,0.15,0.45,0.75]\n}\n\n# Type of scoring used to compare parameter  combinations\nscorer = metrics.make_scorer(metrics.f1_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(Ada_tuned, parameters, scoring=scorer,cv=5)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nAda_boost_tuned = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data.\nAda_boost_tuned.fit(X_train, y_train)","0b320b28":"y_pred = Ada_boost_tuned.predict(X_test)\nmake_confusion_matrix(y_test,y_pred)","d36d0e07":"scores(Ada_boost_tuned)","411bd1ed":"# Choose the type of classifier. \nGrad_boost_ada_tuned = GradientBoostingClassifier(init=AdaBoostClassifier(random_state=25),random_state=25)\n\n# Grid of parameters to choose from\nparameters = {\n    \"n_estimators\": np.arange(10,60,5),\n    \"subsample\":[0.6,0.7,0.8],\n    \"max_features\":[0.6,0.7,0.8],\n    'learning_rate': [0.05,0.15,0.5]\n}\n\n# Type of scoring used to compare parameter combinations\nscorer = metrics.make_scorer(metrics.f1_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(Grad_boost_ada_tuned, parameters, scoring=scorer,cv=5)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nGrad_boost_tuned = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data.\nGrad_boost_tuned.fit(X_train, y_train)","2e9cfe2d":"y_predict = Grad_boost_tuned.predict(X_test)\nmake_confusion_matrix(y_test,y_predict)","3e42751d":"scores(Grad_boost_tuned)","9a19d48b":"xgb_tuned = XGBClassifier(random_state=25, eval_metric='logloss')\n\n# Grid of parameters to choose from\nparameters = {\n    \"n_estimators\": np.arange(10,60,10),\n     \"subsample\":[0.6,0.7,0.8],\n    \"learning_rate\":[0.1,0.3,0.55],\n    \"colsample_bytree\":[0.5,0.7,0.9],\n    \"colsample_bylevel\":[0.5,0.7,0.9]\n}\n\n# Type of scoring used to compare parameter combinations\nscorer = metrics.make_scorer(metrics.f1_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(xgb_tuned, parameters,scoring=scorer,cv=5)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nxgb_tuned = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data.\nxgb_tuned.fit(X_train, y_train)","d6366b2f":"y_predict = xgb_tuned.predict(X_test)\nmake_confusion_matrix(y_test,y_predict)","dc43c752":"scores(xgb_tuned)","4454cf48":"from sklearn.ensemble import StackingClassifier","9e15bd92":"estimators = [('Random Forest',rf_estimator_tuned),('ADA Boosting',Ada_boost_tuned),('Decision Tree',classifier)]\n\nfinal_estimator = xgb_tuned\n\nstacking_classifier= StackingClassifier(estimators=estimators,final_estimator=final_estimator)\n\nstacking_classifier.fit(X_train,y_train)","1020e199":"y_predict = xgb_tuned.predict(X_test)\nmake_confusion_matrix(y_test,y_predict)","5010f5fb":"scores(stacking_classifier)","372ca748":"# defining list of models\nall_models = [Dt, classifier,bagging,bagging_estimator_tuned,random_forest, rf_estimator_tuned,\n          Ada_boost, Ada_boost_tuned, Grad_boost, Grad_boost_tuned, XG_boost,xgb_tuned, stacking_classifier]\n\n# defining empty lists to add train and test results\nacc_train = []\nacc_test = []\nf1_train = [] \nf1_test = []\n\n# looping through all the models to get the metrics score - Accuracy and F1 Score\nfor model in all_models:\n    j = scores(model,False)\n    acc_train.append(j[0])\n    acc_test.append(j[1])\n    f1_train.append(j[2])\n    f1_test.append(j[3])","26b2538d":"comparison_frame = pd.DataFrame({'Model':['Decision Tree','Tuned Decision Tree','Bagging Classifier','Tuned Bagging Classifier ',\n                                          'Random Forest','Tuned Random Forest','AdaBoost Classifier','Tuned AdaBoost Classifier',\n                                          'Gradient Boosting Classifier', 'Tuned Gradient Boosting Classifier',\n                                          'XGBoost Classifier',  'Tuned XGBoost Classifier', 'Stacking Classifier'], \n                                          'Train_Accuracy': acc_train,'Test_Accuracy': acc_test,\n                                          'Train_F1-Score':f1_train, 'Test_F1-Score':f1_test}) \n\n#Sorting models in decreasing order of test recall\ncomparison_frame.sort_values(by='Test_F1-Score',ascending=False)","1fffbb8a":"feature_names = X_train.columns\nimportances = rf_estimator_tuned.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='green', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","09fb697d":"scores(rf_estimator_tuned) #peformance scores for Random Forest-Tuned","1d1423ae":"df2=df.copy() # making a new copy from the dataset\ndf2 = df2.drop(['PitchSatisfactionScore','ProductPitched','NumberOfFollowups','DurationOfPitch'],axis=1) \nA= df2.drop(['ProdTaken'],axis=1)\nB= df2['ProdTaken']\n\nA = pd.get_dummies(A, drop_first=True)\n\n# Splitting data into training and test set:\nA_train,A_test,B_train,B_test =train_test_split(A,B, test_size=0.3, random_state=25,stratify=y)\nprint(A_train.shape,A_test.shape)","f79fd294":"A_test.head()","079e31db":"#running the chosen model on test set\nfinal_pred_test = rf_estimator_tuned.predict(A_test)","903d6d63":"data = df2.loc[A_test.index] #selecting rows with same index as test set\ndata['Predicted'] = final_pred_test\ndata.head()","9e1b9607":"comparison_column = np.where(data[\"Predicted\"] == data[\"ProdTaken\"], True, False) #identifying the misclassification\ndata['Misclassification'] = comparison_column\ndata['Misclassification'].value_counts()","db76793b":"incorrect =data[data['Misclassification']== False] # Grouping only the misidentified rows \nincorrect","96e7dc6b":"#Crearting a Pandas Profile report to identify pattern\nprofile  = ProfileReport(incorrect,title = 'Misclassification Pattern Profile',minimal=True) \nprofile.to_widgets()","ac6e17a2":"* The metrics for ADA boost model is close and comparable for train and test set; but the F1 score is too low\n\n**Gradient Boost Classifier**","c5634479":"* Among Customers who plan to take between 2-4 persons with them during travel, close to 20%  have bought a travel package product.\n* Interestingly, we see that all Customers with one companion and five comapanions, did not purchase any product. \n* This suugests that the products dont seem either appealing or beneficial to the customers of the above two categories. This area needs further investigation","fc70f6d9":"* There isnt any improvement in the metrics with the weighted Bagging classifier\n\n**Random Forest Classifier**","e03f360d":"**Observations:**\n\n* Age variable is almost normally distributed with no outliers. we see that most customers are in the age brackets 30- 45 yrs.\n* DurationofPitch is slightly right-skewed. We see that most customer's pitch duration was under 20 mins. We also see few outliers at 40 mins and at 120+ mins. \n* The highest number of followups is 4.0 followed by 3.0. \n* NumberofTrips is right-skwed a little and majority of the customers seem to take atleast 3 trips per year. We also see very few outliers in the higher end\n* MonthlyIncome is also right-skewd. However, we see that the majority of customers are between income bracket 20K dollars and 30K dollars. We also see two outliers in the low end and on the highest end. There are several outliers after the approx 35K dollars income level.\n\n\n### Univariate Analysis - Categorical Columns:","e07abd27":"**Decision Tree **\n\n* Due to class imbalance in the dependent variable, we will add class_weight hyperparameter to give more importance to class 1\n* We will keep the same randomstate = 25 for all the models so that the same random values are chosen","16d7758b":"* The Decision Tree model seems to be overfitting in the train set. \n* The F1Score for test set is 0.60\n\n**Bagging Classifier:**\n","1a7f04a4":"* The Basic Package is the most preffered, with Standard and Deluxe following up.\n* Comparitively very few customers purchased Super Deluxe products","782bca78":"* Though customers who are  Freelancers by Occupation have bought travel packages, the sample size is only two. \n* Of the 434 Large Business owning customers, almost 30% bought travel packages. \n* Among Salaried and Small Business owning customers,close to 20% have bought travel packages","cfc721fe":"**We want to predict the customers who will purchase the newly introduced travel package. Hence the Customer Interaction Data for the previous existing travel packages will not add any information to the models. So we will be dropping them for further model building and analysis process.**\n\n### Splitting Data into Train and Test set:","b24b9ad4":"**Misclassification Patterns on Customers**\n* About 16% of data from test set has been misclassified.\n* The pattern seems to be distributed across all the variables but is significant in some\n* Customers between ages 30-40 yrs with MonthlyIncome of 20-35K dollars are highly misidentified.\n* Salaried and Small Business Occupation plus Executives,Manager and Sr.Manager by Designation are also significant. \n* Married Customers and those planing to bring atleast one Child are also high \n* Even though, customer with Passport were considered an important feature, the misidentification is equal in the Passport variable. ","4f252f11":"* Again the metrics are comparable and close for both train and test set and the F1Score metric has increased.\n\n**Gradient Boost with ADABoost classifier**","f45ab89c":"* Despite Male customers being significantly higher than Female customer, the percentage of those who bought travel packages is almost the same(or with minimum difference).  ","4aa7eb34":"### UT-Austin DSBA\n\n# Context:\n\n* **Visit With Us** tourism company wants to establish a viable business model to expand its customer base\n\n* Currently there are 5 types of packages the company is offering - Basic, Standard, Deluxe, Super Deluxe, King. Over 18% of customers purchased a product from the available packages according to the data from the last year\n\n* However, the marketing cost was quite high because customers were contacted at random without looking at the available information.\n\n* The company is now planning to launch a new product **Wellness Tourism Package**. Wellness Tourism is defined as Travel that allows the traveler to maintain, enhance or kick-start a healthy lifestyle, and support or increase one's sense of well-being.\n\n# Objective:\n* To harness the available data of existing and potential customers and improve marketing expenditure, thus avoiding high costs\n\n* To Predict customers more likely to purchase the newly introduced package\n\n# Key Questions:\n1. What key variables are important in identifying potential purchasing customers?\n2. What are the different characteristics of the Customers who bought the packages ?\n3. What's the important performance metric for the model and how can it be improved?\n\n# Dataset Attributes:\n\n## Customer details:\n\n1. CustomerID: Unique customer ID\n2. ProdTaken: Whether the customer has purchased a package or not (0: No, 1: Yes)\n3. Age: Age of customer\n4. TypeofContact: How customer was contacted (Company Invited or Self Inquiry)\n5. CityTier: City tier depends on the development of a city, population, facilities, and living standards. The categories are ordered i.e. Tier 1 > Tier 2 > Tier 3\n6. Occupation: Occupation of customer\n7. Gender: Gender of customer\n8. NumberOfPersonVisiting: Total number of persons planning to take the trip with the customer\n9. PreferredPropertyStar: Preferred hotel property rating by customer\n10. MaritalStatus: Marital status of customer\n11. NumberOfTrips: Average number of trips in a year by customer\n12. Passport: The customer has a passport or not (0: No, 1: Yes)\n13. OwnCar: Whether the customers own a car or not (0: No, 1: Yes)\n14. NumberOfChildrenVisiting: Total number of children with age less than 5 planning to take the trip with the customer\n15. Designation: Designation of the customer in the current organization\n16. MonthlyIncome: Gross monthly income of the customer\n\n## Customer interaction data: \n\n1. PitchSatisfactionScore: Sales pitch satisfaction score\n2. ProductPitched: Product pitched by the salesperson\n3. NumberOfFollowups: Total number of follow-ups has been done by the salesperson after the sales pitch\n4. DurationOfPitch: Duration of the pitch by a salesperson to the customer","b1344633":"**Observations:**\n* ProdTaken is the dependent variable. We that only 18.8% of the total customers purchased any of the travel package.The plot shows heavy imbalance in the dataset\n\n* Self-Enquiry is the most preffered contact method by the customers at 71%\n\n* 65.3% of customers are from Tier 1 cities and Tier3 cities comes second at 30.7%. \n\n* 48.4% of customers are Salaried, i.e work for an organization and customers with Small Business are the next highest in Occupation at 42.6%. \n\n* Male customers(59.7%) are higher than Female customers (40.3%)\n\n* 49.1% of customers plan to take atleast 3 persons with them during trip. Around 29% customers want to take 2 people and 21% customers want to take 4 additional persons with them during their travel\n\n* Basic(37.7%) and Deluxe(35.4%) are the most popular travel packages. The next slightly popular one is the Standard Travel package at 15.2%\n\n* 61.8% customers prefer a three star hotel rating compared to four (18.7%) and five (19.6%) star rating hotels\n\n* Married customers form the bulk of the data at 47.9% with Divorced (19.4%) and Single (18.7%) coming in close second and Unmarried(with partners) customers form 14% of the data\n\n* Only 29.1% of customers have a passport and almost 62% of customers own a car\n\n* Only 30.2% of customers rated the Sales Pitch with a score of 3. Even though 18.7% customers rated at 4 and 19.8% rated a pitch score of 5, we also see that 19.3% rated the Sales pitch score at 1. This shows a need for improvement in this area\n\n* Around 43.9% of customers have atleast one child under age Five, planning to accompany them in the travels\n\n* Executive (37.7%) and Manager(35.4%) are the highest Designations of the customers in the dataset","a11ffedf":"## Correlation Matrix","42fea8ec":"### Boosting Models:\n    - We will now build Models using Boosting Ensemble Techniques; ADA, Gradient and XGBoost\n**ADA Boost Classifier**","fb590916":"* We see that the Train and Test Accuracy and F1Score Performance has increased after tuning compared to the previous models\n* The Model is over-fitting as the difference between  Train and Test scores are very high.\n* The Model seems to identify all non-buyers better as the False Positve value is low.\n\n**Hyperparameters for Random Forest Classifier**","af99d333":"## Outliers Detection and Treatment : ","a3fd67bd":"**The Stratify arguments maintains the original distribution of classes in the target variable while splitting the data into train and test sets.**","c60b9aae":"## Load and Explore the Data","f176a930":"* Random Forest classifier is also overfitting for the training set and the F1 score metric has also reduced.\n\n**Random Forest Classifier with weights**","8b31d7f3":"### Building the Model:\n* We will start with a Decision Tree model\n* Next, we will build two ensemble models - Bagging Classifier and Random Forest Classifier\n* Then, we will build three Boosting ensemble models - ADABoost, GradientBoost and XG Boost\n* We will first build them with their default parameters and later hypertune them for optimization.\n* We will also calculate all four metrics; Accuracy, Recall, Precision and F1Score which is the metric of interest\n* F1Score is the weighted average of Precision and Recall. Its takes both False positives and False negatives into account.","2c7714a2":"* The F1Score has decreased to 0.51 for Train set and 0.49 for test set. However, the performance values for F1Score are close and comparable.\n\n\n**Hyperparameters for Bagging Classifier**","c90b5077":"* There are 237 misclassification on the test set\n* This shows that the model indeed has an 83% accuracy rate for test set.","71ecb02b":"**Observations:**\n\n* In the Gender column, we have an error value Fe Male. We will treat this as an data entry issue and replace it to Female.\n* Self Inquiry is the most preffered in TypeofContact feature.\n* 3.0 is the highest property rating\n* And 1.0 is the highest value for the NumberOfChildrenVisiting column.\n* Hence we will replace the missing values in the above columns accordingly\n","cfaa2ee1":"* The Bagging classifier has a better accuracy metric and the F1 score is also higher.\n* But this model only predicts 10.43% of the total 13% of True positives; ie customers who have purchased a product.\n\n**Bagging Classifier with weighted decision tree**","bc0edd07":"## Recommendations:\n\n* Product Vs Designation Correlation:\n    * Based on the above plot, the marketing team can curate the individual packages to the specific business designation\n    * The Agency can also provide incentives w.r.t each packages to ensure more sales\n    * The Business needs to investigate further on this correlation.\n    \n* Age and Income have a correlation and we see that higher age groups and higher Monthly Income groups lean towards the expensive packages.\n* The DurationofPitch needs to be more effective and concise.\n    * The Average PitchSatisfactionScore is 3.0 across all packages\n    * Longer pitch duration doesnt effectively lead to Product putchase\n    * The Marketing team needs to improve on the overall package presentation to ensure that the important aspects are covered quickly and effectively.\n\n* We do not have a any Product Review from the customers.\n* A Key missing variable is if the Product pitched was the same product that was bought. \n\n* Basic and Deluxe are the most popular packages.\n* Possible suggestion is to analyse if the  **WELLNESS TOURISM PACKAGE** has any similar features w.r.t available Products. This may help build a customer profile, thereby ensuring specific targeted base.\n\n\n* There was imbalance in data, ie only 18% of customers bought any product. This must be fixed for future analysis.\n\n* NumberofChilden and NumberofPeoplevisiting doesnt seem to have great impact on the prediction\n* The business can provide incentives or coupons for couples and those with family to encourage more sales \n\n* The Business can target customers with Passport with special international packages, as such customers have higher buying rate.\n* With the above mentioned recommendations in place, the company can run the model to achieve desired performance levels for new dat. This can help reduce the cost of Marketing and improve prediction. \n* After identifying a potential customer, the company should pitch packages per the customer's monthly incomes.\n* Young and single people are more likely to buy the offered packages. The company can offer discounts or customize the package to attract more couples, families, and customers above 30 years of age.\n\n\n\n\n### Misclassification analysis\n* To check if there is any pattern on the incorrectly  classified samples by our chosen model.","3368f5df":"* We see that the percentage of customes who purchased a product is fairly same across all categories of variable NumberOfChildrenVisiting.","fddfd3e7":"### Summary of Numerical Columns","7fb1c72a":"* Around  30% Customers with Executive Designation have purchased a product \n* Sr. Manager(16%) and Manager(~11%) Designation customers have purchased a product. \n* Very few customers of VP and AVP Designation have purchased a product. ","b439a9c9":"**Observations:**\n* The correlation values are quite low between all the variables.\n* Only Age and DurationofPitch have a very low negative correlation.\n* MonthlyIncome and Age have the highest positive correlation at 0.47; i.e as Age increases, so does MontlyIncome\n* NumberofFollowups and NumberofTrips have a  moderate positive correlation between them and also individually with Monthly Income.\n\n## Bivariate Analysis:\n* Let's analyse the dependent variable with all the numerical and categorical features and investigate possible relationships","eb2e315e":"**Observations:**\n* The mean Age for customers who purchased any Product is slightly less than those who didnt. We also see that Age variable doesnt have any outliers. \n* The mean DurationofPitch for both classed of ProdTaken is almost equal. We see there are many outliers in Class '0' of ProdTaken, suggesting that longer pitch durations doesnt lead to product purchase.\n* Interestingly, Customers who purchased the packages had an average of atleast four followups, compared to customers who didnt.\n* The Averages for  NumberofTrips and MonthlyIncome;for both Classes of ProdTaken is almost equal. MonthlyIncome variable has several outliers in the higher end for both ProdTaken classes and very few in low end of Class '0'.\n","6796801f":"* More Customers from Tier2 and Tier3 cities have purchased Travel Packages","3cd6bb94":"## Exploratory Data Analysis:\n\n### Univariate Analysis - Numerical Columns:","dfa54fbc":"* More customers with passport tend to purchase products than those who dont.","528d7661":"* The F1Score has dropped to 0.506\n\n**XGBoost Classifier**","f74a7392":"* XGBoost is overfitting the model but the F1Score of test set has increased\n\n#### GridSearch for Hyperparameter Tuning of the Boosting models:\n\n**Hyperparameters for ADABoost**","0bf45ce7":"* There is no improvement in the metrics for the weighted Random Forest classifier.\n\n\n#### GridSearch for Hyperparameter Tuning of the models:\n\n* Hyperparameters are variables that control the network structure of the Decision tree.\n* As there is no direct way to calculate the effects of value change in hyperparamter has on the model, we will use a GridSearch\n* This is a tuning technique that will compute the optimum values of specific hyperparamters of the model\n* The parameters are optimized using a cross-validated GridSearch over a parameter grid\n\n**Hyperparameters for Decision Tree Model**","b4f7a693":"* All missing values are treated\n\n### Summary of Categorical Variables","4b2674fe":"# Model Building:\n\n## Model Evaluation Criterion\n\n### Model can make two kinds of wrong predictions:\n\n1. Predicting that the customer will purchase a Travel Package when they dont. - False Positive\n2. Predicting that the customer will not purchase a Travel Package when they do. - False Negative\n\n* The Travel company's objectives are :\n    - Make Marketing Expenditure more efficient;i.e target potential customers who have higher chances of buying a product.\n    - Predict and Identify all potential customers who will purchase the newly introduced travel package.\n\n### Metric for Optimization:\n* For the above objective, its important that both False positive and False negative values are low. Hence we would want the F1-Score to be maximized.\n* The greater the F1-Score, greater the chances of predicting both classes correctly.\n\n### Creating a Confusion Matrix","bdd6387b":"## Importing Necessary Libraries","1870f36a":"* From the above table we see that almost all the models are overfitting w.r.t F1 Score.\n* Though XGBoost classifer has the highest F1 Score, its overfitting on the training data\n* Despite having lower F1 Score to XGBoost, Tuned Random Forest has more generalized metric scores and doesnt seem to be over-fitting the data\n\n**Feature importance of Tuned Random Forest Classifier**","ad0baa19":"* MonthlyIncome and NumberofFollowups have high outliers compared to the other features.\n* However, we will not be treating outliers, as we will be building Decision Tree based models and Decision Tree models are not influenced by Outliers.\n* Furthermore, in real case scenario, we will encounter similar outliers and that would require the model to investigate if there is any pattern among the customers","f3ff13e0":"* Majority of customers have given a score of 3.0 to the Sale pitch for the products.\n* But we observe that the number of customers who purchased any product is almost equal across all pitch scores. \n* This suggests that a high product pitch score doesnt guarantee purchase ","5401e9e2":"## Conclusion: \n* Hence Tuned Random Forest Classifier Model gives an overall generalised metric performace w.r.t F1 score and doesnt seem to be over-fitting. \n* The most importance features for this model are:\n     - MonthlyIncome\n     - Age\n     - Passport_1(Customers with Passport)    \n* This model has an 83% accuracy rate, which is quite good despite the imbalance in data.","5b68be5e":"* The parameters have handled the over-fit issue.\n* The models Recall and F1score is has dropped considerably.\n\n\n**Hyperparameters for XGBoost**","6094e90f":"* The accuracy and F1score has increased, despite slight overfitting. \n\n\n### Stacking Classifier :\n* Of all the above models, we will take Random Forest classifier, Gradient boost classifer and Decision Tree; we will use the prediction as new features to train the XGboost classifer. \n* The reason for choosing the above three model, is due to the fact that they have the least comparable overfitting issue and good perfomance metrics","5cb1ee90":"* Around 30% of all Single customers have bought a product and about 25% of Unmarried customers have also purchased a product\n* Almost 50% of the total customers belong to the married category, but we see that only approx 15% of them have actually purchased any product. ","537c364f":"* The overall model performance metric has dropped after Hypertuning, but it looks like there is not much over-fit issues.\n* Despite a lower F1score of 0.61 for the test set, the difference between the Train and Test set is comparitively better.\n* The model seems to be identifying all True Positives, ie customers who bought a product better than rest. ","4f89a10c":"* Hypertuning the model has only decreased the overall scores\n\n**Hyperparameters for GradientBoost**","d22006d5":"**Observations:**\n* Self Inquiry is the most preffered Type of Contact\n* ProdTaken : There is heavy imbalance in this column where atleast 80% customers did not purchase any product\n* CityTier : Most customers are from Tier 1\n* Occupation : Most customers earn a salary\n* Gender : Male customers are slightly higher than Female Customers\n* NoOfPersonsVisting: Most customers plan to take atleast 3 additional persons with them in the trip\n* ProductPitched : Basic is the popular product\n* MaritalStatus : Most customers are married\n* Passport : Most customers dont have a passport\n* PitchSatisfactionScore : Most customers have rated 3.0 \n* OwnCar: Most customers own a car\n* NumberofChildrenVisting : Most customers plan to take atleast 1 child under five with them for the trip.\n* Designation : Most customers belong to Executive designation","84324ba9":"* The number of customers who bought a product is fairly equal across both classes of OwnCar","f513aa17":"* Though majority of customers prefer a 3.0 star rated Property, the percentage of customers purchasing the products is comparitively less than customers who prefer a 4.0 and 5.0 star rated property. \n* The higher the proprety star rating, higher the number of customers who purchased a product","b38d08d8":"## Data Pre-Processing:\n\n\n## Fixing Datatypes","02d7e2bd":"* Despite a reduce F1 score of 0.54, the Stacking classifer model is not over-fitting.\n* We also see a good accuracy score of 0.85\n\n## Comparing all Models:\n","b7b865e1":"* The datatypes have been fixed and the memory reduced.\n\n## Missing Value Treatment:","a012b9d6":"* More Customers with CompanyInvited contact have bought Travel Packages"}}