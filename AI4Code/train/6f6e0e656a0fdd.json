{"cell_type":{"57d3befb":"code","eb70fe98":"code","29171d8e":"code","2cf3e085":"code","d5d21922":"code","3c1a013a":"code","c794b82e":"code","82a7881d":"code","c2736105":"code","f903e720":"code","71c71b14":"code","0b675361":"code","cd0288d9":"code","a2ae75f3":"code","fff2f548":"code","3c4e4a16":"code","f617187f":"code","44197f89":"code","b656de7d":"code","08088d23":"code","cbce88d7":"code","1a7b4e21":"code","0b28639e":"code","87967e76":"code","829c5c4f":"markdown","2a522f4a":"markdown","0e1bdf96":"markdown","af657782":"markdown","0092c56e":"markdown","c93bdba6":"markdown","73d7813a":"markdown"},"source":{"57d3befb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eb70fe98":"sms=pd.read_csv(\"\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv\", encoding='latin1')\nsms = sms.iloc[:,[0,1]]\n\nsms.head()","29171d8e":"sms.columns = [\"label\", \"message\"]\ndf=sms","2cf3e085":"sms.describe()","d5d21922":"df.isnull().sum()","3c1a013a":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.countplot(x=\"label\", data=sms);\nplt.show()","c794b82e":"from wordcloud import WordCloud\n\nplt.figure(figsize = (15,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(sms[sms.label == \"spam\"].message))\nplt.imshow(wc , interpolation = 'bilinear')","82a7881d":"plt.figure(figsize = (15,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(sms[sms.label == \"ham\"].message))\nplt.imshow(wc , interpolation = 'bilinear')","c2736105":"import nltk\nimport string\nfrom nltk.corpus import stopwords\nimport re","f903e720":"def rem_punctuation(text):\n  return text.translate(str.maketrans('','',string.punctuation))\n\ndef rem_numbers(text):\n  return re.sub('[0-9]+','',text)\n\n\ndef rem_urls(text):\n  return re.sub('https?:\\S+','',text)\n\n\ndef rem_tags(text):\n  return re.sub('<.*?>',\" \",text)\n\n\n\ndf['message'].apply(rem_urls)\ndf['message'].apply(rem_punctuation)\ndf['message'].apply(rem_tags)\ndf['message'].apply(rem_numbers)","71c71b14":"stop = set(stopwords.words('english'))\n\ndef rem_stopwords(df_news):\n    \n    words = [ch for ch in df_news if ch not in stop]\n    words= \"\".join(words).split()\n    words= [words.lower() for words in df_news.split()]\n    \n    return words    \n\ndf['message'].apply(rem_stopwords)","0b675361":"from nltk.stem import WordNetLemmatizer\n#nltk.download('wordnet')\nlemmatizer = WordNetLemmatizer()\n\ndef lemmatize_words(text):\n  lemmas = []\n  for word in text.split():\n    lemmas.append(lemmatizer.lemmatize(word))\n  return \" \".join(lemmas)\n\n\ndf['message'].apply(lemmatize_words)","cd0288d9":"df.dtypes","a2ae75f3":"encode = ({'ham': 0, 'spam': 1} )\n#new dataset with replaced values\ndf = df.replace(encode)","fff2f548":"import keras\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow import keras \nfrom keras import backend as K\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.layers import LSTM,Dense,Bidirectional,Input\nfrom keras.models import Model\nfrom sklearn.model_selection import train_test_split\nimport transformers","3c4e4a16":"x_train,x_test,y_train,y_test = train_test_split(df.message,df.label,random_state = 0,stratify = df.label)","f617187f":"from tokenizers import BertWordPieceTokenizer\n# First load the real tokenizer\ntokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased' , lower = True)\n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=True)\nfast_tokenizer","44197f89":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=400):\n\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in range(0, len(texts), chunk_size):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","b656de7d":"x_train = fast_encode(x_train.values, fast_tokenizer, maxlen=400)\nx_test = fast_encode(x_test.values, fast_tokenizer, maxlen=400)","08088d23":"def build_model(transformer, max_len=400):\n    \n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","cbce88d7":"bert_model = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')","1a7b4e21":"model = build_model(bert_model, max_len=400)\nmodel.summary()","0b28639e":"history = model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs = 4)","87967e76":"print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")","829c5c4f":"## Data visualization","2a522f4a":"## About the dataset\nDataset consist of 747 spam messages are found out of 5572 messages\n\n","0e1bdf96":"## Data Preprocessing\nWe have to convert the raw messages (sequence of characters) into vectors (sequences of numbers).before that we need to do the following:\n\n* Remove punctuation\n* Remove numbers\n* Remove tags\n* Remove urls\n* Remove stepwords\n* Change the news to lower case\n* Lemmatisation","af657782":"# Introduction to NLP using BERT\n\n## About BERT \nBidirectional Encoder Representations from Transformers (BERT) is a technique for NLP (Natural Language Processing) pre-training developed by Google. BERT was created and published in 2018 by Jacob Devlin and his colleagues from Google. **Google is leveraging BERT to better understand user searches.**\n![](https:\/\/miro.medium.com\/max\/2960\/0*63_xsVQp0Wezk9ua.jpg)\n ","0092c56e":"## What is BERT \n\n**BERT**:Bidirectional Encoder Representations from Transformers is a pretrained  NLP algorithm devolped by google AI wing.BERT \nis a bidirectionally trained so we can   have a deeper sense of language context and flow compared to the single-direction language models.Instead of predicting the next word in a sequence, BERT makes use of a novel technique called Masked LM (MLM): it randomly masks words in the sentence and then it tries to predict them. Unlike the previous language models, it takes both the previous and next tokens into account at the same time. The existing combined left-to-right and right-to-left LSTM based models were missing this \u201csame-time part.\n\nFor more informatiom refer:https:\/\/towardsml.com\/2019\/09\/17\/bert-explained-a-complete-guide-with-theory-and-tutorial\/\n","c93bdba6":"<h5>this is a cleaned dataset and no null values are present<\/h5>","73d7813a":"## How it works \nBERT relies on a Transformer (the attention mechanism that learns contextual relationships between words in a text). A basic Transformer consists of an encoder to read the text input and a decoder to produce a prediction for the task. Since BERT\u2019s goal is to generate a language representation model, it only needs the encoder part. The input to the encoder for BERT is a sequence of tokens, which are first converted into vectors and then processed in the neural network. But before processing can start, BERT needs the input to be massaged and decorated with some extra metadata:\n\n Token embeddings: A [CLS] token is added to the input word tokens at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.\nSegment embeddings: A marker indicating Sentence A or Sentence B is added to each token. This allows the encoder to distinguish between sentences.\nPositional embeddings: A positional embedding is added to each token to indicate its position in the sentence."}}