{"cell_type":{"3cf678c1":"code","3b5b3f57":"code","b66e0729":"code","f2b513bc":"code","75cb7e3b":"code","e8a4bcb3":"code","4fe7ccf2":"code","41318f79":"code","5a6fa984":"code","476c6494":"code","93711c7f":"code","ab319e83":"code","7a2b9af6":"code","86aa8a2d":"code","62432e53":"code","1b57da8b":"code","65d1de70":"code","d4990dab":"code","26816140":"code","b010c8c1":"code","b73b6023":"code","1260941c":"code","ccaf8a29":"code","7017020e":"code","56720f84":"code","4fd9a434":"code","628abc17":"code","6c088473":"code","90fef269":"code","7a648ed5":"code","4d575297":"code","92a4a822":"code","a75844d1":"code","839ba3dd":"code","6223ad93":"code","0b2334fb":"code","05a50fab":"code","31881ed9":"code","b2548a6b":"code","12d9e1b4":"code","ffe5c882":"code","d5439cac":"code","e75f5014":"code","9149ed18":"code","a7b36589":"code","49d57ec4":"code","c4e0f2b0":"code","f1ba96f4":"code","9d842265":"code","17ddb8bf":"code","0474f660":"code","800c7a9b":"code","73ec5ba6":"code","6c6411cd":"code","29ada88e":"code","29a7196d":"code","7c99c05a":"code","8f7bedcb":"code","056dfbc8":"code","ac2ffa52":"code","06182483":"code","2d14ee16":"code","01167e3c":"code","604c4eb5":"code","780622d8":"code","ce65bafa":"markdown","a5ffd436":"markdown","897f7918":"markdown","49afe4a4":"markdown","c493fc9e":"markdown","899d3c13":"markdown","7b21b0c4":"markdown","6ec064ad":"markdown","eaa30b93":"markdown","af33e606":"markdown","07434e9c":"markdown","5474d4d3":"markdown","ca5ababd":"markdown","8fdbdb5c":"markdown","1ef43b5d":"markdown","985c26ac":"markdown","7a9644fc":"markdown","41a3d654":"markdown","f845e3b1":"markdown","5b9bc2ac":"markdown","6c409284":"markdown","a6a07224":"markdown","9bfa1adb":"markdown","a6083ec3":"markdown","661ccb82":"markdown","7177a7e9":"markdown","e394d956":"markdown","dd726c30":"markdown","4fb837b5":"markdown","5beac5dc":"markdown","c89caa01":"markdown","55351b54":"markdown","1f03f577":"markdown","7d8ea83e":"markdown","a21c7852":"markdown"},"source":{"3cf678c1":"import pandas as pd\n\n%matplotlib inline\nimport seaborn as sns\nprint(\"Setup Complete\")\n\nfrom sklearn.preprocessing import Imputer\nfrom sklearn_pandas import CategoricalImputer\n#pip install sklearn-pandas\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\nfrom sklearn import ensemble\n\nfrom sklearn.model_selection import KFold,cross_val_score\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom xgboost import XGBRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')","3b5b3f57":"#read data\ndf_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv',na_values='nan')\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv',na_values='nan')\nprint(\"Train size:\",df_train.shape)\nprint(\"Test size:\",df_test.shape)\n\n#Save the 'Id' column\ntrain_ID =df_train['Id']\ntest_ID =df_test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ndf_train.drop(\"Id\", axis = 1, inplace = True)\ndf_test.drop(\"Id\", axis = 1, inplace = True)\nprint (\"Size of train data after dropping Id: {}\" .format(df_train.shape))\nprint (\"Size of test data after dropping Id: {}\" .format(df_test.shape))\n\n","b66e0729":"df_train.columns","f2b513bc":"df_train_num = df_train.select_dtypes(exclude=['object'])\ndf_train_num.columns","75cb7e3b":"#Deleting outliers\ntrain = df_train.drop(df_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<300000)].index)","e8a4bcb3":"\nfrom scipy.stats import shapiro\nstat, p = shapiro(df_train['SalePrice'])\nprint('Value for SalePrice column statistics=%.3f, p=%.3f' % (stat, p))","4fe7ccf2":"# Check normality for features\nfor col in df_train_num.columns:\n    fig, ax = plt.subplots()\n    stat, p = shapiro(train[col])\n    ax.scatter(x =train[col], y = train['SalePrice'])\n    plt.ylabel('SalePrice', fontsize=13)\n    plt.xlabel((p,col), fontsize=13)\n    plt.show()","41318f79":"p_df=[]\nfor col in df_train_num.columns:\n    stat, p = shapiro(df_train[col])\n    p_df.append(p)\n    \n#>0.05 means are normally distributed\n# Only 3 columns have normal distribution\np_df_=pd.DataFrame(p_df,df_train_num.columns).sort_values(by=0,ascending=True)\np_df_=p_df_.loc[p_df_[0]>0.05]\np_df_","5a6fa984":"# most correlated features to SalePrice\ncorrmat = train.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.5]\nplt.figure(figsize=(10,10))\ng = sns.heatmap(train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","476c6494":"\ntrain[top_corr_features].columns","93711c7f":"#Let's combine together train\/test data for faster data processing\n\nntrain = train.shape[0]\nntest = df_test.shape[0]\n#y_train = train.SalePrice.values\nall_data = pd.concat((train,df_test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))\n","ab319e83":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})","7a2b9af6":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","86aa8a2d":"all_data_rep=all_data.copy()\nall_data_rep[\"PoolQC\"] = all_data_rep[\"PoolQC\"].fillna(0)\nall_data_rep[\"MiscFeature\"] = all_data_rep[\"MiscFeature\"].fillna(0)\nall_data_rep[\"Fence\"] = all_data_rep[\"Fence\"].fillna(0)\n\nall_data_rep[\"FireplaceQu\"] = all_data_rep[\"FireplaceQu\"].fillna(0)\n\nall_data_rep[\"LotFrontage\"] = all_data_rep.groupby('Neighborhood')[\"LotFrontage\"].transform(\n    lambda x:x.fillna(x.median()))\n\ngrd_col=[\"GarageQual\",\"GarageCond\",\"GarageType\",\"GarageFinish\",\"GarageArea\",\"GarageCars\",\"GarageYrBlt\"]\nfor col in grd_col:\n    all_data_rep[col] = all_data_rep[col].fillna(0)\n    \ngrd_col=[\"BsmtExposure\",\"BsmtCond\",\"BsmtQual\",\"BsmtFinType1\",\"BsmtFinType2\",\"BsmtUnfSF\",\"BsmtFinSF1\",\"BsmtFinSF2\"]\nfor col in grd_col:\n    all_data_rep[col] = all_data_rep[col].fillna(0)\n    \nall_data_rep[\"TotalBsmtSF\"] = all_data_rep[\"TotalBsmtSF\"].fillna(0)\n\n\nall_data_rep[\"MasVnrArea\"] = all_data_rep[\"MasVnrArea\"].fillna(0)\nall_data_rep[\"MasVnrType\"] = all_data_rep[\"MasVnrType\"].fillna(0)\n\nall_data_rep['MSZoning']=all_data_rep['MSZoning'].fillna(all_data_rep['MSZoning'].mode()[0])\n\nall_data_rep[\"BsmtFullBath\"] = all_data_rep[\"BsmtFullBath\"].fillna(0)\nall_data_rep[\"BsmtHalfBath\"] = all_data_rep[\"BsmtHalfBath\"].fillna(0)\n\n\n\nall_data_rep[\"Functional\"] =all_data_rep[\"Functional\"].fillna(\"Typ\")\n\nall_data_rep[\"Alley\"] = all_data_rep[\"Alley\"].fillna(0)\n\nmode_col = ['Electrical','KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType']\nfor col in mode_col:\n    all_data_rep[col] = all_data_rep[col].fillna(all_data_rep[col].mode()[0])\n\n    \nall_data_rep[\"MoSold\"] = all_data_rep[\"MoSold\"].fillna(0)","62432e53":"#Utilities still has null value, however we will drop this column later (it has only 1 value for majority-no result affect)\nall_data_rep.isnull().sum().sort_values(ascending=False)[:3]","1b57da8b":"all_data_rep['TotalSF'] = all_data_rep['TotalBsmtSF'] + all_data_rep['1stFlrSF'] + all_data_rep['2ndFlrSF']","65d1de70":"all_data_rep_enc_1=all_data_rep.copy()","d4990dab":"#MSSubClass will encode base on the SalePrice mean \nmean_sale_subclass=train.groupby(['MSSubClass']).mean()\ndf_mean_sale_subclass=pd.DataFrame(mean_sale_subclass['SalePrice'].sort_values(ascending=True)).reset_index()\ndf_mean_sale_subclass[:2]","26816140":"subclass_order=list(df_mean_sale_subclass['MSSubClass'])\nsubclass_order","b010c8c1":"qual_map = {0: 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n\nqual_col=['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond','ExterQual',\n          'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual']\nfor col in qual_col:\n    all_data_rep_enc_1[col] = all_data_rep_enc_1[col].map(qual_map)\n#--------------------------------------------------------------\nfurnit_map = {0: 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ':6 }\n\nfurn_col=['BsmtFinType1', 'BsmtFinType2']\nfor col in furn_col:\n    all_data_rep_enc_1[col] = all_data_rep_enc_1[col].map(furnit_map)\n\nfunctional_map={0: 0, 'Sal': 1, 'Sev': 2, 'Maj2': 3, 'Maj1': 4, 'Mod': 5, 'Min2':6,'Min1': 7, 'Typ':8}\nfunc_col=['Functional']\nfor col in func_col:\n    all_data_rep_enc_1[col] = all_data_rep_enc_1[col].map(functional_map)\n\nfence_map={0: 0, 'MnWw': 1, 'GdWo': 2, 'GdWo': 3, 'MnPrv': 4, 'GdPrv': 5}\nfence_col=['Fence']\nfor col in fence_col:\n    all_data_rep_enc_1[col] = all_data_rep_enc_1[col].map(fence_map)\n    \nbsm_exp_map={0: 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4}\nall_data_rep_enc_1['BsmtExposure'] = all_data_rep_enc_1['BsmtExposure'].map(bsm_exp_map)\n\ngarag_fin_map={0: 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}\nall_data_rep_enc_1['GarageFinish'] = all_data_rep_enc_1['GarageFinish'].map(garag_fin_map)\n\nslope_map={'Sev': 1, 'Mod': 2, 'Gtl': 3}\nall_data_rep_enc_1['LandSlope'] = all_data_rep_enc_1['LandSlope'].map(slope_map)\n\nshape_map={'IR3': 1, 'IR2': 2, 'IR1': 3, 'Reg': 4}\nall_data_rep_enc_1['LotShape'] = all_data_rep_enc_1['LotShape'].map(shape_map)\n\npave_map={'N': 1, 'P': 2, 'Y': 3}\nall_data_rep_enc_1['PavedDrive'] = all_data_rep_enc_1['PavedDrive'].map(pave_map)\n\nstreet_map={0:0,'Pave': 1, 'Grvl': 2}\nall_data_rep_enc_1['Street'] = all_data_rep_enc_1['Street'].map(street_map)\n\nalley_map={0: 0, 'Pave': 1, 'Grvl': 2}\nall_data_rep_enc_1['Alley'] = all_data_rep_enc_1['Alley'].map(alley_map)\n\nair_map={ 'N': 1, 'Y': 2}\nall_data_rep_enc_1['CentralAir'] = all_data_rep_enc_1['CentralAir'].map(air_map)        \n\n#1 value of MSSubClass(150) was not encoded,there is no SalePrice price for it, which we can use for analysis\n#Let's take the same as 120\n\nclass_map={0:0,30:1, 180:2, 45:3, 190:4, 90:5, 160:6, 50:7, 85:8, 40:9, 70:10, 80:11, 20:12, 75:13, 120:14, 150:14, 60:15}\nall_data_rep_enc_1['MSSubClass'] = all_data_rep_enc_1['MSSubClass'].map(class_map)\n\nall_data_rep_enc_1['YrSold'].astype('int')\n\n#all_data_rep_enc_1['MoSold'].astype('int')\n\nmo_map={0:0, 4:1, 5:2, 6:7, 7:7.5, 8:9, 9:11, 10:2.5, 11:8.5, 2:8.5, 12:9.5, 1:1.5, 3:7.5}\nall_data_rep_enc_1['MoSold'] = all_data_rep_enc_1['MoSold'].map(mo_map)","b73b6023":"# Utilities we will drop later because it useless\n\nall_data_rep_enc_1.isnull().sum().sort_values(ascending=False)[:3]","1260941c":"def check_skewness(col):\n    sns.distplot(train[col] , fit=norm);\n    fig = plt.figure()\n    res = stats.probplot(train[col], plot=plt)\n    # Get the fitted parameters used by the function\n    (mu, sigma) = norm.fit(train[col])\n    print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n    \ncheck_skewness('SalePrice')","ccaf8a29":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\ncheck_skewness('SalePrice')","7017020e":"y_train=train[\"SalePrice\"]","56720f84":"all_data_rep_enc_1.dtypes[all_data_rep_enc_1.dtypes=='object'].index","4fd9a434":"numeric_feats = all_data_rep_enc_1.dtypes[all_data_rep_enc_1.dtypes!= \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data_rep_enc_1[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(3)","628abc17":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.3\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data_rep_enc_1[feat] = boxcox1p(all_data_rep_enc_1[feat], lam)","6c088473":"all_data_rep_enc_1.dtypes[all_data_rep_enc_1.dtypes== \"object\"].index\n","90fef269":"col_ob=list(all_data_rep_enc_1.dtypes[all_data_rep_enc_1.dtypes== \"object\"].index)\nun=[]\nfor col in col_ob:\n    n=all_data_rep_enc_1[col].nunique()\n    un.append(n)","7a648ed5":"all_data_rep_enc_1_=all_data_rep_enc_1.copy()","4d575297":"all_data_rep_enc_1=all_data_rep_enc_1_.drop(['Utilities'],axis=1)","92a4a822":"all_data_dum = pd.get_dummies(all_data_rep_enc_1,drop_first=True)\nall_data_dum.shape\n","a75844d1":"X_train = all_data_dum[:ntrain]\nx_test = all_data_dum[ntrain:]\nX_train.shape","839ba3dd":"cv=KFold(n_splits=5,shuffle=True,random_state=241)","6223ad93":"# Function to check score\ndef mse_cv(model):\n    mse_my=np.sqrt(-cross_val_score(model,X=X_train,y=y_train,scoring=\"neg_mean_squared_error\", cv = cv))\n    return(mse_my.mean())","0b2334fb":"model=XGBRegressor(objective='reg:squarederror',n_estimators=1000)\nsimp_model=mse_cv(model)\nprint('Score_simple_model',simp_model)","05a50fab":"lean_r=[0.0005,0.005,0.05,0.1,0.2]\nfinal_score=[]\n\nfor lr in lean_r:\n    model=XGBRegressor(objective='reg:squarederror',n_estimators=500, learning_rate=lr, early_stop_round=5)\n    impr_score_ln_rate=mse_cv(model)\n    final_score.append(impr_score_ln_rate)","31881ed9":"# Best ln_rate - 0.05\ncv_boost_ln_rate = pd.Series(final_score[:4], index = lean_r[:4])\ncv_boost_ln_rate.plot(title = \"CV_cross_val_ln_rate\")\nplt.xlabel(\"ln_rate\")\nplt.ylabel(\"mse\")","b2548a6b":"cv_boost =cv_boost_ln_rate\n\nmin_err=min(cv_boost.values)\nimpr_lr_rate=min_err\n\n\n\ndef get_key(cv_boost,min_err):\n    for k,v in cv_boost.items():\n        if v==min_err:\n            return k,v\nbest_ln_rate=get_key(cv_boost,min_err)[0]\nprint('Impored_score_ln_rate',impr_lr_rate)\nprint('Best_ln_rate',best_ln_rate)","12d9e1b4":"n_est=[500,600,700,750,1000]\nlearning_rate=best_ln_rate\nfinal_score_n_est=[]\n\nfor n in n_est:\n    model=XGBRegressor(objective='reg:squarederror',n_estimators=n, early_stop_round=5,learning_rate=best_ln_rate)\n    impr_score=mse_cv(model)\n    final_score_n_est.append(impr_score)","ffe5c882":"cv_boost_n_est = pd.Series(final_score_n_est, index = n_est)\nimpr_score_n_est=min(cv_boost_n_est.values)\ncv_boost_n_est.plot(title = \"CV_cross_val_score_n-est\")\nplt.xlabel(\"n_est\")\nplt.ylabel(\"mse\")\nprint('Impored_score_n_est',impr_score_n_est)\n","d5439cac":"max_d=[2,3,4,5,6]\nfinal_score_max_d=[]\n\nfor m in max_d:\n    model=XGBRegressor(objective='reg:squarederror',n_estimators=600, learning_rate=0.05, max_depth=m)\n    impr_score=mse_cv(model)\n    final_score_max_d.append(impr_score)","e75f5014":"# best max_d=5\ncv_boost_max_d = pd.Series(final_score_max_d, index = max_d)\ncv_boost_max_d.plot(title = \"CV_cross_val_max_d\")\nplt.xlabel(\"max_d\")\nplt.ylabel(\"mse\")","9149ed18":"model=XGBRegressor(objective='reg:squarederror',n_estimators=600, learning_rate=0.05,max_depth=5)\nimpr_score_3_par=mse_cv(model)","a7b36589":"min_child=[1,2,3,4,5]\nfinal_score_min_child=[]\n\nfor m in min_child:\n    model=XGBRegressor(objective='reg:squarederror',n_estimators=600, learning_rate=0.05, max_depth=5, min_child_weight=m)\n    impr_score=mse_cv(model)\n    final_score_min_child.append(impr_score)","49d57ec4":"cv_boost=pd.DataFrame((zip(min_child,final_score_min_child)), columns=['Child','Score']).sort_values(by='Score',ascending=True)\n#cv_boost[:1]\nimpr_score_4_par=cv_boost[:1]['Score']\nimpr_min_child_weight=cv_boost[:1]['Child']\n\nprint('Score XGBoost improved ln_rate,n_est,max_depth,min_child',round(impr_score_4_par,5))\nprint('Best_min_child',round(impr_min_child_weight))","c4e0f2b0":"model=XGBRegressor(objective='reg:squarederror',n_estimators=600, learning_rate=0.05,max_depth=5, min_child_weight=3)\nimpr_score_4_par=mse_cv(model)","f1ba96f4":"subsample=[1,0.8,0.7,0.6,0.5]\nfinal_score_subsample=[]\n\nfor m in subsample:\n    model=XGBRegressor(objective='reg:squarederror',n_estimators=600, learning_rate=0.05, max_depth=5, min_child_weight=3,subsample=m)\n    impr_score=mse_cv(model)\n    final_score_subsample.append(impr_score)","9d842265":"cv_boost=pd.DataFrame((zip(subsample,final_score_subsample)), columns=['SubSam','Score']).sort_values(by='Score',ascending=True)\n#cv_boost[:1]\nimpr_score_5_par=cv_boost[:1]['Score']\nimpr_min_subsample=cv_boost[:1]['SubSam']\n\nprint('Score XGBoost improved ln_rate,n_est,max_depth,min_child,subsample',round(impr_score_5_par,5))\nprint('Best_subsample',impr_min_subsample)","17ddb8bf":"model=XGBRegressor(objective='reg:squarederror',n_estimators=600, learning_rate=0.05,max_depth=5, min_child_weight=3,subsample=0.5)\nimpr_score_5_par=mse_cv(model)","0474f660":"print('Score XGBoost improved ln_rate,n_est,max_depth,min_child,subs',round(impr_score_5_par,5))\nprint('Score XGBoost impln_rate,n_est,max_dep,min_chi',round(impr_score_4_par,5))\nprint('Score XGBoost improved ln_rate,n_est,max_depth',round(impr_score_3_par,5))\nprint('Score XGBoost improved ln_rate,n_est          ',round(impr_score_n_est,5))\nprint('Score XGBoost improved ln_rate                ',round(impr_lr_rate,5))\nprint('Score XGBoost simple_model                    ',round(simp_model,5))","800c7a9b":"model_param_grid= XGBRegressor(objective='reg:squarederror',subsample=0.6,n_estimators=1400,learning_rate=0.03,max_depth=3,min_child_weight=3)","73ec5ba6":"impr_score_grid=mse_cv(model_param_grid)\nprint('Score XGBoost improved GridSearchCV',round(impr_score_grid,5))","6c6411cd":"model = XGBRegressor(objective='reg:squarederror',learning_rate=0.03,n_estimators=1400,max_depth=3,min_child_weight=3,subsample=0.6)\nmodel.fit(X_train, y_train)\n","29ada88e":"importance = model.feature_importances_\nfeat_impot=pd.DataFrame((zip(X_train.columns, importance)), columns=['Feat','Importance']).sort_values(by='Importance',ascending=False)\nfeat_impot","29a7196d":"def mse_cv_1(model,X):\n    mse_my=np.sqrt(-cross_val_score(model,X=X,y=y_train,scoring=\"neg_mean_squared_error\", cv = cv))\n    return(mse_my.mean())","7c99c05a":"feat=[120,130,140,201]\nscore_feat=[]\nfor f in feat:\n    feat_best=feat_impot['Feat'][:f]\n    feat_best_list=list(feat_best)\n    X_feat=X_train[feat_best_list]\n    score_feat.append(mse_cv_1(model,X_feat))","8f7bedcb":"score_feat","056dfbc8":"feat_best=feat_impot['Feat'][:130]\nfeat_best_list=list(feat_best)\nX_feat=X_train[feat_best_list]","ac2ffa52":"model_feat_sel=XGBRegressor(objective='reg:squarederror',learning_rate=0.03,n_estimators=1400,max_depth=3,min_child_weight=3,subsample=0.6)\nimpr_score_feat_sel=mse_cv_1(model,X_feat)","06182483":"print('Score XGBoost improved GridSearchCV feat selection_130 ',round(impr_score_feat_sel,5))","2d14ee16":"from sklearn.ensemble import GradientBoostingRegressor\nmodel_boost=GradientBoostingRegressor()\nscore_boost=mse_cv(model_boost)\nprint('Score_GradBoost',score_boost)","01167e3c":"print('Score XGBoost improved GridSearchCV feat selection_130       ',round(impr_score_feat_sel,5))\nprint('Score XGBoost improved GridSearchCV                          ',round(impr_score_grid,5))\nprint('Score XGBoost improved ln_rate,n_est,max_depth,min_child,subs',round(impr_score_5_par,5))\nprint('Score XGBoost impln_rate,n_est,max_dep,min_chi               ',round(impr_score_4_par,5))\nprint('Score XGBoost improved ln_rate,n_est,max_depth',round(impr_score_3_par,5))\nprint('Score XGBoost improved ln_rate,n_est          ',round(impr_score_n_est,5))\nprint('Score XGBoost improved ln_rate                ',round(impr_lr_rate,5))\nprint('Score XGBoost improved simple_model           ',round(simp_model,5))\nprint('Score GradientBoost                           ',round(score_boost,5))","604c4eb5":"\nmodel=XGBRegressor(objective='reg:squarederror',learning_rate=0.03,n_estimators=1400,\n                   max_depth=3,min_child_weight=3,subsample=0.6)\nX_train_feat=X_train[feat_best_list]\nx_test_feat=x_test[feat_best_list]\n\nXGB=model.fit(X_train_feat,y_train)\n\ntest=x_test_feat.copy()\n\nfinalMd = np.expm1(XGB.predict(test))\n\nfinalMd\n","780622d8":"\nsample_submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsample_submission.iloc[:,1] = finalMd\nsample_submission.to_csv(\"sample_submission.csv\", index=False)","ce65bafa":"## 2.2 Check Correlation\nMain idea of correlation - quantify the strengh of relation between values. Max correlation val = abs(1) only for numeric columns","a5ffd436":"# 2. Data Processing","897f7918":"# 2.4 Missing_Data","49afe4a4":"## Check skew for numerical value","c493fc9e":"# 7.8 Feature Selection score tune\nNot all features equally important. We already saw it correlation chapter, that some influence more then another. Here we will check feature importance and will improve score by refusing from useless features.","899d3c13":"Import to check data dictribution in the columns. It is good to have *normal distribution.***** The normal (or Gaussian) distribution is one particular kind of a bell shaped curve. It is unimodal (there is one peak\"), symmetric (that is you can flip it around its mid point) and its mean, median and mode are all equal. ML algothitms are working better on the data with normal distribution, in this case we can more accurately predict. The lesser its value differs from the mean value of the random variable, the higher is its probability.\n\nThere are different tools to check data normality (quantile check,Shapiro-Wilk\u2019s W test, Kolmogorov-Smirnov test). Let's use shapiro test and check p-value. If defined p-val >0.05, then data has Normal Distribution.\n\n","7b21b0c4":"## 7.7 GridSearch Parameters tune\nAs well we can use GridSearchCV for parameters tuning. We have different scores and different best parameters in cross-validation and GridSearchCV.\nWhy is it so? The difference may be due to that the cross-validation\nsplits used in cross_validate and GridSearchCV are different, due to the randomness. The effect of this randomness becomes even larger as your dataset is very small and the number of folds\nis large. A possible fix is to feed into cv a fix train\/test splits, \nand reduce the number of folds to reduce the variance.\n\n","6ec064ad":"# 7.5 Min_child_weight tune\nThis parameter represent minimum amount of observation in the leaf. Help to prevent overfitting","eaa30b93":"# 6.Dummy encode for categorical values","af33e606":"### Improved Score XGBoost","07434e9c":"# 4. X_log_transform","5474d4d3":"# 2.6 New feature creation","ca5ababd":"# 2.1 Outliars drop and Normality check","8fdbdb5c":"p-val for SalePrice is less than 0.05, distrubution not Normal.Some Tranfrormation is required","1ef43b5d":"Parameter n_estimators shows us - how many itterations we will go till the stop (stop to improve our score, stop to decrease our error). Too low n_estimators leads to model underfit,  too high n_estimators leads to model overfit. The best situation - a bit underfit in training dataset and minimum error in validation dataset.","985c26ac":"# 1. XGBoost in theory\nBeing a newcomer in ML,my goal in this Kernel to get familiar with XGboost. Theoretical description below is my interpretation of different public material from internet. Hope I understand it right) The best way to keet in memory new material is to try explain it to others, consolidate learned with discussion and to improve to results with advices.\n\nXGBoost is one of the most powerful ML algorithm from tree ensemble group. XGboost based on the decision tree answer search and it`s using GradientBoosting framework.\nTo have better understanding of XGBoost, let's remember basics: \n\n*There are 2 main types of decision tree ensemble:*\n\n1. Bagging\n2. Boosting\n\nIn Bagging we combine predictors from multiple-decision trees through a majority voting mechanism.\n\nIn Boosting we build models sequentially by minimizing the error from previous models while increasing (or boosting) influence of high performance models. \n\nIf compare new employee hiring process by few recruitment specialists in a company (sequential interviews) with Boosting, here each next recruitment specialist is based his decision on the assesment of the candidate by the previous manager.This speed up hiring process, as unsuitable candidates are immediately eliminated.\n\n*Now the question is - How Boosting minimize  the error?*\n\nA spesial case of Boosting in which the error is minimized is Gradient Descent Algorithm (GDA-optimization algorithm)-in case of recruitment process-\nthe least qualified candidates are eliminated as early as possible.\nTaking about Gradient Descent Algorithm different articles are using picture of mountain, from which you have to find a way from the top to the bottom (where lake is located) with zero-visibility.\nThe best way is to check the ground near you and observe where the land tends to descend. \nThis will give an idea in what direction you should take your first step. If you follow the descending path, it is very likely you would reach the lake.\n\nThe same with minimizing errors from different trees.\nIf you are able to minimize errors (here with GDA) - this tree-candidate is promissing and worth to continue work with it.\n\nXGBoost is optimized GradientBoosting algorithm through parallel processing, tree-pruning,handling missing values and regularization to avoid overfitting\/bias.\n\nSo, we more or less get familiar with basis of XGBoost and have some understanding of its principles.\n![image.png](attachment:image.png)\n### XGBoost requirements and properties:\n 1. XGBoost is working with numerical vectors, so all object data should be transformed (as option- One-Hot_Encoder:output- new columns with feature name and binary 0\/1 value - availabiity of given feature)\n 2. XGBoost doesn`t suffer from colinear features, however it is a good practice to drop them.","7a9644fc":"Box Cox Transformation of (highly) skewed features.\n\nWhen you are dealing with real-world data, you are going to deal with features that are heavily skewed. Transformation technique is useful to stabilize variance, make the data more normal distribution-like, improve the validity of measures of association.\n\nThe problem with the Box-Cox Transformation is estimating lambda. This value will depend on the existing data, and should be considered when performing cross validation on out of sample datasets.","41a3d654":"# 7.6 Subsample parameter tune\nThis parameter defines the share of data to be taken for each tree building. Usually takes value 0.5-1","f845e3b1":"Lets check score with parameters that we defined with GridSearchCV (in parallel kernel, to save our time)","5b9bc2ac":"## 7.2 Learning Rate tuning\nLearning_rate is an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. From each new tree we will multiply output by small number(=learning_rate), before adding them in. This will help to reduce overfit. So we can use high value of n_estimator without overfit. \nEarly_stop_round - causes the model to stop iteration when the validation score stops to improve. Smart to set a high value for n_estimator, and then use early_stop_round to find optimum time to stop. In our exapmle after 5 rounds of score decreasing or standstill, itterations will be stopped.","6c409284":"# Content\n    1. XGBoost in theory\n    2. Data Processing\n        2.1 Outliars drop and normality check\n        2.2 Check Correlation\n        2.3 Together train\/test\n        2.4 Missing Data\n        2.5 Replace_Missing Data\n        2.6 New feature creation\n        2.7 Categorical to Ordinal\n    3. Y_log_transform\n    4. X_log_transform\n    5.Drop before prediction\n    6.Dummy encode for categorical values\n    7.XGboost Algo\n        7.1 Basic model\n        7.2 Learning Rate tune\n        7.3 n_estimators tune\n        7.4 Max_depth tune\n        7.5 Min_child_weight tune\n        7.6 SubSample tune\n        7.7 GridSearch Parameters tune\n        7.8 Feature Selection score tune\n    8.GradientBoosting model\n    9.Conclusion","a6a07224":"# 7. XGboost Algo","9bfa1adb":"Here we see some colinear features, like 'YearBuilt'\/'YearRemodAdd', or 'GarageCars'\/ 'GarageArea',","a6083ec3":"# 2.3 Together Train\/Test","661ccb82":"#### Score with Feature Selection","7177a7e9":" # 8.GradientBoosting model\n\nTo compare XGBoost scores we will GradientBoostingRegressor\n","e394d956":"# 2.5 Replace_Missing Data","dd726c30":"# 9.Conclusion\nIn this kernel we familiar XGBoostRegression, defined main hyperparametes of this model and imroved a bit our model by parameters` tuning. Final result is far from the top, however this work was created for XGBoost basic understanding.","4fb837b5":"# 2.7 Categorical to Ordinal data\nThis step will help us to find some dependencies with categorical data, or some numerical will be encode in another more logical way","5beac5dc":"## 7.3 n_estimators tune\n","c89caa01":"# 3.Y-normality -log TRansform\nAccording to Shapiro test, SalePrice is not normally distributed\nThe best way to fix it is to perform a log transform of the same data, with the intent to reduce the skewness.","55351b54":"## 7.1 Basic model","1f03f577":"# House price prediction with XGBoost","7d8ea83e":"# 5.Drop_befor_prediction","a21c7852":"## 7.4 Max_depth tune\nIt represents the depth of each tree, which is the maximum number of different features used in each tree. I recommend going from a low max_depth (3 for instance) and then increasing it incrementally by 1, and stopping when there\u2019s no performance gain of increasing it. This will help simplify your model and avoid overfitting"}}