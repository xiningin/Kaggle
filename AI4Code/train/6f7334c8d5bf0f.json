{"cell_type":{"e9c1dbdd":"code","046ea993":"code","ca50a3d1":"code","7969781f":"code","cc968a91":"code","3044c3c3":"code","321c7cf5":"code","6554f320":"code","2c9ed1da":"code","fc1f17f9":"code","3e67271c":"code","f05ecc63":"code","56929842":"code","43ce14bb":"code","fa709df1":"markdown"},"source":{"e9c1dbdd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","046ea993":"#installing pyspark\n!pip install pyspark","ca50a3d1":"#Importing required libraries \nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom pyspark.sql import SparkSession\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer\nfrom pyspark.ml.regression import LinearRegression","7969781f":"#creating a spark session\nspark = SparkSession.builder.master(\"house_price\").getOrCreate()","cc968a91":"#reading csv files\ntrain=spark.read.csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\", inferSchema=True, header=True)\ntest=spark.read.csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\", inferSchema=True, header=True)\ntrain.printSchema()","3044c3c3":"#viewing first 3 lines using head() and looping to make it readable\nfor house in train.head(3):\n    print(house)\n    print(\"\\n\")","321c7cf5":"#String Indexer are used tranform strings into categorical data. We are doing it for only one column here but we can doit for all string data\nindexer= StringIndexer(inputCol=\"LotShape\", outputCol=\"LotShape2\")\nindexed= indexer.fit(train).transform(train)\nindexed.head(1)","6554f320":"#Assembler combines all integer and create a vector which is used as input to predict. Here we have only selected columns with data type as integer\nassembler= VectorAssembler(inputCols=[\"MSSubClass\",\"LotArea\",\"OverallQual\",\"OverallCond\",\"BsmtFinSF1\",\n                                      \"BsmtFinSF2\",\"BsmtUnfSF\",\"TotalBsmtSF\",\"1stFlrSF\",\"2ndFlrSF\",\"LowQualFinSF\",\"GrLivArea\",\"BsmtFullBath\",\"BsmtHalfBath\",\n                                     \"FullBath\",\"HalfBath\",\"BedroomAbvGr\",\"KitchenAbvGr\",\"TotRmsAbvGrd\",\"Fireplaces\",\"YearBuilt\",\n                                     \"YearRemodAdd\",\"GarageCars\",\"GarageArea\",\"WoodDeckSF\",\"OpenPorchSF\",\"EnclosedPorch\",\"3SsnPorch\",\"ScreenPorch\",\"PoolArea\",\n                                     \"MiscVal\",\"MoSold\",\"YrSold\",\"LotShape2\"],outputCol=\"features\")","2c9ed1da":"#transforming assembler\noutput= assembler.transform(indexed)\noutput.select(\"features\",\"SalePrice\")","fc1f17f9":"#We can see column features is dense vector\nfinal=output.select(\"features\",\"SalePrice\")\nfinal.head(3)","3e67271c":"#We will split data into train and validate\ntrain_df,valid_df= final.randomSplit([0.7,0.3])\ntrain_df.describe().show()","f05ecc63":"#initializing and fitting model\nlr= LinearRegression(labelCol=\"SalePrice\")\nmodel= lr.fit(train_df)","56929842":"#fitting model of validation set\nvalidate=model.evaluate(valid_df)","43ce14bb":"#let's check how model performed\nprint(validate.rootMeanSquaredError)\nprint(validate.r2)","fa709df1":"# **Conclusion and Recommendation**\n\nModel currently covers only 75% variance. We can improve model by \n1. Imputing null values\n2. encoding more categorical features\n3. Hypertuning the model\n\nThis notebook only provides a basic gist of pyspark."}}