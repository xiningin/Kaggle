{"cell_type":{"caeb02eb":"code","fd3ed55c":"code","e6842e7e":"code","054f0fc4":"code","fc1a0eb6":"code","f1bb51b2":"code","168ea737":"code","31faccc9":"code","4a570dd9":"code","27923f9a":"code","28a45d0b":"code","598c323e":"code","e6fb91f8":"code","32345c55":"code","7a95de5f":"code","9f3e7d25":"markdown","ed6f7447":"markdown","e42c273d":"markdown","2483c84f":"markdown","34d4cac1":"markdown","4e1cf2f7":"markdown"},"source":{"caeb02eb":"import pandas as pd\nprint(f\"Last updated on {pd.to_datetime('today').strftime('%m\/%d\/%Y')}\")","fd3ed55c":"import glob\nimport json\nimport os\nimport numpy as np\nimport pandas as pd\nfrom pprint import pprint\n\ninput_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\n\n# See JSON schema here: https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge#json_schema.txt\ndef read_text(file, key):\n    entries = file[key]\n    if isinstance(entries, dict):\n        entries = entries.values()\n    return '\\n'.join([x['text'] for x in entries])\n\n# Each paragraph is a separate entry in the body_text data.\ndef read_body_text(file):\n    prev_section = None\n    text = []\n    for x in file['body_text']:\n        section = x['section']\n        if section != prev_section:\n            text.append('<SECTION>')\n            text.append(section)\n            prev_section = section\n        text.append(x['text'])\n    return '\\n'.join(text)\n    \ndef read_file(filename):\n    file = json.load(open(filename, 'rb'))\n    data = {\n        'paper_id': file['paper_id'],\n        'title': file['metadata']['title'],\n        'abstract': read_text(file, 'abstract'),\n        'body_text': read_body_text(file),\n        'ref_entries': read_text(file, 'ref_entries'),\n    }\n    return data","e6842e7e":"json_files = sorted(glob.glob(f'{input_path}\/**\/pdf_json\/*.json', recursive=True))\nprint('Total pdf files: ', len(json_files))\n\nfile_contents = []\nfor i, file in enumerate(json_files):\n    if i % 2000 == 0:\n        print(f'Processing {i} of {len(json_files)}')\n    file_contents.append(read_file(file))\ncontent_df = pd.DataFrame(file_contents)","054f0fc4":"metadata_path = f'{input_path}\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'sha': str,\n    'doi': str,\n    'pmcid': str,\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str,\n    'WHO #Covidence': str,\n})\npd.options.display.max_colwidth = 100\nmeta_df.info()","fc1a0eb6":"print('PDF-only: ', meta_df[meta_df.has_pdf_parse & ~meta_df.has_pmc_xml_parse].cord_uid.count())\nprint('PMC-only: ', meta_df[~meta_df.has_pdf_parse & meta_df.has_pmc_xml_parse].cord_uid.count())","f1bb51b2":"full_df = content_df.merge(meta_df, how='inner', left_on='paper_id', right_on='sha')\nfull_df = full_df.replace(r'^\\s*$', np.nan, regex=True)\nfull_df.info()","168ea737":"full_df['title'] = full_df.title_y.fillna(full_df.title_x)\nfull_df['abstract'] = full_df.abstract_y.fillna(full_df.abstract_x)\nfull_df.drop(columns=['sha', 'title_x', 'title_y', 'abstract_x', 'abstract_y'], inplace=True)\nfull_df.info()","31faccc9":"stats = {}\nstats['total'] = meta_df.shape[0]\nstats['full text'] = full_df.shape[0]\ndf = full_df\n\n# Drop duplicates by title (an article may come from multiple sources resulting in dups).\ndf = df.drop_duplicates(['title'])\nstats['uniques'] = df.shape[0]\ndf.to_csv('cord19.csv', index=False)\n\n# Filter by publish time.\ndf = df[pd.to_datetime(df.publish_time, errors='coerce') >= pd.to_datetime('2019-12-01')]\nstats['after 12\/01\/2019'] = df.shape[0]\n\n# Filter by covid-19 keywords.\nkeywords = ['novel coronavirus', 'COVID-19', 'COVID19', 'SARS-CoV-2', '2019-nCov']\nkeywords_regex = '|'.join(keywords)\ndf_covid19 = df[df.title.str.contains(keywords_regex, na=False, case=False, regex=True) | df.abstract.str.contains(keywords_regex, na=False, case=False, regex=True)]\nstats['covid-19'] = df_covid19.shape[0] \ndf_covid19.to_csv('covid19.csv', index=False)","4a570dd9":"# Write full metadata as well:\nmeta_df.to_csv('cord19_meta.csv', index=False)","27923f9a":"# Also write covid19_metadata for all articles, including those without full text.\ndf = meta_df\ndf = df.drop_duplicates(['title'])\ndf = df[pd.to_datetime(df.publish_time, errors='coerce') >= pd.to_datetime('2019-12-01')]\ndf_covid19_meta = df[df.title.str.contains(keywords_regex, na=False, case=False, regex=True) | df.abstract.str.contains(keywords_regex, na=False, case=False, regex=True)]\nstats['covid-19-meta'] = df_covid19_meta.shape[0] \ndf_covid19_meta.to_csv('covid19_meta.csv', index=False)\npd.DataFrame([stats]).transpose()","28a45d0b":"df_covid19_meta.head(1).transpose()","598c323e":"df_covid19.head(1).transpose()","e6fb91f8":"print(df_covid19.iloc[0].abstract)","32345c55":"print(df_covid19.iloc[0].body_text.split('<SECTION>')[1])","7a95de5f":"meta_df[meta_df.title.str.contains('Coronaviruses: a paradigm of new emerging zoonotic diseases', case=False, na=False)]","9f3e7d25":"# Dedup, filter, and write to csv","ed6f7447":"# Covid-19 subset of articles","e42c273d":"# Load metadata and join with contents","2483c84f":"# Load all pdf json article contents","34d4cac1":"# Sample data","4e1cf2f7":"[CORD-19](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge) data set of scholarly articles about COVID-19 and the coronavirus group. This notebook uses the title and publication dates of articles to create a subset that are specifically about Covid-19 or SARS-CoV-2.\n\nDataset | # Articles | Output Files\n---|---|---\nCORD-19 | 52398 | cord19_meta.csv\nFull text | 36977 |\nUniques | 36610 | cord19.csv\nAfter 12\/01\/2019 | 4714\nCovid-19 | 3106 | covid19.csv\nCovid-19-meta | 4561 | covid19_meta.csv"}}