{"cell_type":{"e74dcec5":"code","f466c683":"code","eed9d1b0":"code","c69f309e":"code","e9513e7a":"code","9d97fc95":"code","27eb1fc1":"code","c1af2d96":"code","c9448dac":"code","092b8b9b":"code","ac17599a":"code","14e2f903":"code","a755472d":"code","542691e4":"code","3d0934ae":"code","4c654c9f":"code","db4ccbd4":"code","74912871":"code","58d8a5ba":"code","2699b10f":"code","4e9c49da":"code","21bf753e":"code","8cac31bc":"code","bcbe4de8":"code","e838d65a":"code","d8e3f281":"markdown","00964d53":"markdown","53b6e3a7":"markdown","4a99dcf3":"markdown","4cfa8002":"markdown","d48d4965":"markdown","788a5849":"markdown","ee7464ae":"markdown","edc576b1":"markdown","bcded482":"markdown","015c84aa":"markdown","247f5298":"markdown","cc22b9f1":"markdown"},"source":{"e74dcec5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f466c683":"import matplotlib.pyplot as plt\nimport seaborn as sns","eed9d1b0":"wine_df = pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","c69f309e":"wine_df.head()","e9513e7a":"wine_df['quality'].value_counts().plot(kind='pie', figsize = (10,10))","9d97fc95":"sns.countplot(data = wine_df, x='quality')","27eb1fc1":"for j in wine_df.drop('quality', axis = 1):\n    sns.distplot(wine_df[j], kde = False)\n    plt.legend()\n    plt.show()","c1af2d96":"from sklearn.preprocessing import StandardScaler","c9448dac":"scaler = StandardScaler()","092b8b9b":"scaler.fit(wine_df.drop('quality', axis = 1))","ac17599a":"scaler_feat = scaler.transform(wine_df.drop('quality', axis = 1))","14e2f903":"scaler_feat","a755472d":"df_feat = pd.DataFrame(scaler_feat, columns=wine_df.columns[:-1])","542691e4":"df_feat.head()","3d0934ae":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\nX = df_feat\ny = wine_df['quality']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)","4c654c9f":"error_rate = []\n\nfor i in range(1,100):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i!=y_test))","db4ccbd4":"plt.figure(figsize = (15,10))\nplt.plot(range(1, 100), error_rate, color = 'green', linestyle = 'dashed', \n         marker = 'o', markerfacecolor = 'red', markersize = 10)\nplt.title('Error Rate vs K values')\nplt.xlabel('K values')\nplt.ylabel('Error Rate')\nfor i_x, i_y in zip(range(1,100), error_rate):\n    plt.text(i_x, i_y, '({},{})'.format(round(i_x,2), round(i_y,2)))","74912871":"knn_1 = KNeighborsClassifier(n_neighbors= 1)\nknn_1.fit(X_train, y_train)\npred = knn.predict(X_test)","58d8a5ba":"from sklearn.metrics import classification_report, r2_score, confusion_matrix\n\nprint(classification_report(y_test, pred))\nprint('\\n')\nprint(r2_score(y_test, pred))\nprint('\\n')\nprint(confusion_matrix(y_test, pred))","2699b10f":"X_1 = wine_df.drop('quality', axis=1)\ny_1 = wine_df['quality']\n\nX_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, test_size = 0.3, random_state = 101)\n\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\npara_grid = { 'C':[0.1, 1, 10, 100, 1000, 10000, 100000], 'gamma':[1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]}\ngrid = GridSearchCV(SVC(), para_grid, verbose = 3)\ngrid.fit(X_train_1, y_train_1)","4e9c49da":"grid.best_params_","21bf753e":"grid_predictions = grid.predict(X_test_1)","8cac31bc":"print(classification_report(y_test_1, grid_predictions))\nprint('\\n')\nprint(r2_score(y_test_1, grid_predictions))\nprint('\\n')\nprint(confusion_matrix(y_test_1, grid_predictions))","bcbe4de8":"X_2 = wine_df.drop('quality', axis= 1)\ny_2 = wine_df['quality']\n\nX_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, test_size = 0.3, random_state = 101)\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators= 120)\nrfc.fit(X_train_2, y_train_2)\nrfc_pred = rfc.predict(X_test_2)","e838d65a":"print(classification_report(y_test_2, rfc_pred))\nprint('\\n')\nprint(r2_score(y_test_2, rfc_pred))\n","d8e3f281":"# Data Visualisation","00964d53":"Importing required libraries","53b6e3a7":"# KNN Classifier","4a99dcf3":"**Hope this has helped you! :)\nAnd please do upvote the notebook.**","4cfa8002":"# Support Vector Classifier","d48d4965":"# Accuracy of Random Forest seems to be better compared to both KNN and SVC","788a5849":"Accuracy seems to be 57% using KNN Classifier","ee7464ae":"Finding best possible fit for K value","edc576b1":"Distribution plot of all the columns of the dataset","bcded482":"# Random Forest Classifier","015c84aa":"Defining new dataframe for scaled features","247f5298":"Standardising the values for easy evaluation by model","cc22b9f1":"Accuracy seems to be comparitively better than KNN Classifier"}}