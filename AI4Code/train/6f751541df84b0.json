{"cell_type":{"f5c1d130":"code","6caa8606":"code","c129ed17":"code","c770aacf":"code","4b9f6e41":"code","3f321f0d":"code","5800437f":"code","4c354982":"code","f660b0e2":"code","a205d0dc":"code","06479c21":"code","1454585c":"code","f0606487":"code","fec0ff9f":"code","3a840827":"code","bbb89cf3":"code","bb12929a":"code","6d3954ad":"code","8d14d509":"code","02c9ac3d":"code","62c22cff":"code","c966c01a":"code","4a3c4964":"code","61041385":"code","8a8ee648":"code","b9e223f1":"code","c24cf41f":"code","c3f7e12f":"code","4b75f4af":"code","8995ced1":"code","6873db29":"code","633923e7":"code","cfb4eb09":"code","fd2ed309":"code","96c75e5d":"code","38e435f8":"code","93898d87":"code","06aa11d9":"code","1e867f54":"code","d0bf1313":"code","c04e6bbe":"code","440c7a2c":"code","6d9fc908":"code","0065e14b":"code","2babfcbb":"code","7cc28dbb":"code","2442f672":"code","e65f49f0":"code","5c6b9cbd":"code","f48a5e55":"code","0c6dab97":"code","7e96bbef":"code","7daaf9fb":"code","24db501b":"code","3b011628":"code","c74fd437":"code","4976da88":"code","e73de7b7":"code","6810a744":"code","eb3a7a6b":"code","9f9d66ce":"code","a528b9cb":"code","b2ff6d76":"code","b75b010e":"code","a55bb13e":"code","36381925":"code","f0747fba":"code","796d3fc1":"code","56084560":"markdown","eec8ef8b":"markdown","7389c56a":"markdown","4d01ae38":"markdown","42898162":"markdown","628fa8d9":"markdown","09fda3b3":"markdown","331c7c91":"markdown","c6f7741c":"markdown","3323e21d":"markdown","0bd67067":"markdown","ae8fb313":"markdown","0717356d":"markdown","e66a69cd":"markdown","3d9294a3":"markdown"},"source":{"f5c1d130":"# This notebook demonstrates the attempt to predict whether the passenger survived or not in \n# the Titanic event using Kaggle Titanic dataset\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","6caa8606":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","c129ed17":"# Check the shape of train and test files\ntrain.shape, test.shape","c770aacf":"train.head()","4b9f6e41":"test.head()","3f321f0d":"# Let us combine both train and test for data cleaning & preparation for modeling\ndata = pd.concat([train,test],axis=0,ignore_index=True,sort=False)\ndata.shape","5800437f":"# Check for proper concatenation is done\ndata.shape[0] == train.shape[0]+ test.shape[0]","4c354982":"data['dtype'] = np.where((data['Survived']== 0.0) | (data['Survived']== 1.0) ,'train','test')\ndata.tail()","f660b0e2":"data['dtype'].value_counts()","a205d0dc":"# Adding the column names to the list\ncolumns = data.columns.to_list()\n# Checking the number of cells without values and datatype of each column\ndata.info()","06479c21":"# Create Correlation matrix for all variables\ncorr = data.corr()\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nsns.heatmap(corr,annot=True,cmap='RdYlGn',linewidth=0.1)\nfig=plt.gcf()\nfig.set_size_inches(12,10)\nplt.show()","1454585c":"# Check for total no. of cells with missing values\ndata.isnull().sum()","f0606487":"#Filling the missing value for Fare & Embarked\n\nsns.countplot(x='Embarked',data=data)\nplt.show()","fec0ff9f":"table = pd.crosstab(data['Survived'].dropna(),data['Embarked'])\ntable","3a840827":"import statsmodels.api as sm\nimport scipy as sp\n\nvalue, p ,dof ,expected = sp.stats.chi2_contingency(table)\n\nif p <= 0.05:\n    print('Reject H0: Dependant')\nelse:\n    print('Fail to reject H0: Independant')","bbb89cf3":"data[data['Embarked'].isna()]","bb12929a":"data['Embarked'].fillna('Q',inplace=True)","6d3954ad":"# Fare values\nsns.distplot(data['Fare'].dropna())\nplt.show()","8d14d509":"# Filling NaN with median values\ndata['Fare'].fillna(np.median(data['Fare'].dropna()),inplace=True)","02c9ac3d":"# Filling zero values with median values as fare price cannot zero\ndata['Fare'].replace(0,np.median(data['Fare']),inplace=True)","62c22cff":"import re\ndata['Name'] = data['Name'].apply(lambda x: re.sub('[^A-Za-z. ]+', '',x))\ndata['Name'] = data['Name'].apply(lambda x: x.lower())","c966c01a":"words = pd.DataFrame(data['Name'].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis=0))\nwords = words[0].sort_values(ascending=False)\nwords.head(5)","4a3c4964":"# Encoding the gender sex variable\ndata['Sex'] = np.where(data['Sex']=='male',1,0)","61041385":"# Get dummy codes for Embarked variables\ndummy = pd.get_dummies(data['Embarked'])\ndummy.head()","8a8ee648":"dummy.drop(columns='Q',inplace=True)\ndummy.rename(columns={'C':'Embarked_C','S':'Embarked_S'},inplace=True)\ndummy.head()","b9e223f1":"# Adding the dummy variables and dropping the original column from data\n\ndata = pd.merge(data,dummy,left_index=True,right_index=True)\ndata.drop(columns='Embarked',inplace=True)\ndata.head()","c24cf41f":"# Impute Age group for missing values using Iterative Imputer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimp = IterativeImputer(max_iter=10,random_state=0,verbose=1)\nimp.fit(data.drop(columns=['PassengerId','Survived','Name','Parch','Ticket','Cabin','dtype']))","c3f7e12f":"drop_col = ['PassengerId','Survived','Name','Parch','Ticket','Cabin','dtype']\ndata_imp = pd.DataFrame(imp.transform(data.drop(columns=drop_col)))\ndata_imp","4b75f4af":"data['Age_imp'] = abs(data_imp[2])\nage_imp = np.median(data.loc[(data['Age'].dropna()>18.0)&(data['Sex']==1),'Age'])","8995ced1":"# Creating user defined function to impute age value wherever it is missing\ndef a(x):\n    if (np.isnan(x['Age'])) & (x['Age_imp'] < 18.0):\n        return age_imp\n    elif (np.isnan(x['Age'])) & (x['Age_imp'] >= 18.0):\n        return x['Age_imp']\n    else:\n        return x['Age']\n    \ndata['Age_x'] = data.apply(a,axis=1)\ndata.drop(columns=['Age','Age_imp'],inplace=True)\ndata.rename(columns={'Age_x':'Age'},inplace= True)\ndata.head()","6873db29":"# Create variable IsAlone\ndata['IsAlone'] = np.where(data['SibSp']+data['Parch'] == 0,1,0)\ndata['Fam_size'] = np.where(data['IsAlone'] == 0,data['SibSp']+data['Parch']+1,1)","633923e7":"data['Fam_size'].value_counts()","cfb4eb09":"# Creating user defined function to assign Gender based on the age group\ndef f(x):\n    if x['Age'] <= 15.0:\n        return 'Children'\n    elif (x['Age'] > 15.0) & (x['Sex'] == 1):\n        return 'Adult-Male'\n    else:\n        return 'Adult-Female'\n\ndata['Gender'] = data.apply(f,axis=1)\n\n# Lookup the frequency distribution table between gender and passenger class \npd.crosstab(data['Gender'],data['Pclass'])","fd2ed309":"# 2 more user defined functions to assign each passenger with time of boarding\n# based on which the passenger class got filled\ndef g(x):\n    if (x['Embarked_S'] == 1) & (x['Embarked_C'] == 0):\n        return 'first'\n    elif (x['Embarked_S'] == 0) & (x['Embarked_C'] == 1):\n        return 'second'\n    else:\n        return 'third'\n\ndef h(x):\n    if x['Pclass'] == 1:\n        return 'upper'\n    elif x['Pclass'] == 2:\n        return 'middle'\n    else:\n        return 'lower'\n    \ndata['Boarding'] = data.apply(g,axis=1) +'-' + data.apply(h,axis=1)","96c75e5d":"levels = pd.DataFrame(data['Boarding'].value_counts())\nlevels","38e435f8":"level_list = levels.index.to_list()\nlevel_dict = {'lower':['Deck_G','Deck_F','Deck_E'],\n              'middle':['Deck_E','Deck_D'],\n              'upper':['Deck_C','Deck_B','Deck_A']}\nlevel_list","93898d87":"level_dict.get(level_list[0].split('-')[1])\n\nFare1 = pd.DataFrame(columns=None)\n\nfor level in level_list:\n    Fare = pd.DataFrame(data[data['Boarding'] == level]['Fare'])\n    deck = level_dict.get(level.split('-')[1])\n    Fare['Deck'] = pd.cut(np.array(Fare['Fare']),len(deck), labels=deck)\n    Fare1 = pd.concat([Fare1,Fare],axis=0,sort=False)","06aa11d9":"Fare1['Deck'].value_counts()","1e867f54":"# Deck class is assigned for each passenger based on the fair price,embarked station,Pclass\nFare1.head()","d0bf1313":"# Merging this data with original dataframe\ndata = pd.merge(data,Fare1['Deck'],left_index=True,right_index=True)\ndata.drop(columns=['Name','Ticket','Cabin','Boarding'],inplace=True) # remove unwanted columns\ndata.head()","c04e6bbe":"# Create dummy variables for categorical values\ndummy1 = pd.get_dummies(data['Gender'])\ndummy1.drop(columns='Children',inplace=True)\n\ndummy2 = pd.get_dummies(data['Deck'])\ndummy2.drop(columns='Deck_A',inplace=True)\n\ndummies = pd.merge(dummy1,dummy2,left_index=True,right_index=True)\ndata = pd.merge(data,dummies,left_index=True,right_index=True)\n\ndata.drop(columns=['Gender','Deck'],inplace=True)\ndata.head()","440c7a2c":"# Box Cox transformation for Fare value, since it is highly skewed\nfrom sklearn.preprocessing import power_transform\n\ndata['Fare'] = power_transform(np.array(data['Fare']).reshape(-1,1),method='box-cox')\ndata.head()","6d9fc908":"# Split the train and test to build ML models\ntrain_new = data[data['dtype']=='train'].drop(columns='dtype')\ntest_new = data[data['dtype']=='test'].drop(columns=['dtype','Survived'])\ntest_new.head()","0065e14b":"corr_n = data.corr()\n\nsns.heatmap(corr_n,annot=True,cmap='RdYlGn',linewidth=0.1)\nfig=plt.gcf()\nfig.set_size_inches(20,12)\nplt.show()","2babfcbb":"# Split the dataset\n\nX = train_new.loc[:,train_new.columns != 'Survived'].copy()\ny = train_new.loc[:,train_new.columns == 'Survived'].copy()\n\nX.drop(columns='PassengerId',inplace=True)","7cc28dbb":"# Run the logistic regression model\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport statsmodels.api as sm\nfrom sklearn.metrics import roc_auc_score,plot_roc_curve\n\nlogit_model = sm.Logit(y,X)\nresult = logit_model.fit()\nprint(result.summary())","2442f672":"from sklearn.model_selection import GridSearchCV\n\npenalty = ['l1','l2']\nC = np.logspace(0,4,10)\nhyperparameters = dict(C= C,penalty = penalty)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3,random_state=0)\n\nlr = LogisticRegression(max_iter=200,solver='liblinear')\nclf = GridSearchCV(lr, hyperparameters, cv=5, verbose=0,scoring='recall')\n\nmodel_lr = clf.fit(X_train,y_train.values.ravel())\nprint('Best penalty:',model_lr.best_estimator_.get_params()['penalty'])\nprint('Best C:', round(model_lr.best_estimator_.get_params()['C'],2))","e65f49f0":"y_pred_lr = model_lr.predict(X_test)\n\nconfusion1 = pd.DataFrame(pd.crosstab(y_test['Survived'],y_pred_lr))\nconfusion1","5c6b9cbd":"plot_roc_curve(model_lr,X_test,y_test)\nplt.show()","f48a5e55":"# Model Evaluation metrics \nfrom sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\nprint('Accuracy Score : {}'.format(round(accuracy_score(y_test,y_pred_lr),2)))\nprint('Precision Score : {}'.format(round(precision_score(y_test,y_pred_lr),2)))\nprint('Recall Score : {}'.format(round(recall_score(y_test,y_pred_lr),2)))\nprint('F1 Score : {}'.format(round(f1_score(y_test,y_pred_lr),2)))","0c6dab97":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","7e96bbef":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3,random_state=0)\n\nrf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train.values.ravel())","7daaf9fb":"# Getting the best parameters from the randomised search\nrf_random.best_params_","24db501b":"# Build Random forest model\nbest_rf = rf_random.best_estimator_\n\ny_pred_rfs = best_rf.predict(X_test)\n\nconfusion2 = pd.DataFrame(pd.crosstab(y_test['Survived'],y_pred_rfs))\nconfusion2","3b011628":"print('Accuracy Score : {}'.format(round(accuracy_score(y_test,y_pred_rfs),2)))\nprint('Precision Score : {}'.format(round(precision_score(y_test,y_pred_rfs),2)))\nprint('Recall Score : {}'.format(round(recall_score(y_test,y_pred_rfs),2)))\nprint('F1 Score : {}'.format(round(f1_score(y_test,y_pred_rfs),2)))","c74fd437":"plot_roc_curve(best_rf,X_test,y_test)\nplt.show()","4976da88":"# Variable importance table to look for most important variables\n\nvar_imp1 = pd.DataFrame({'Variable': X.columns,\n                        'Importance':best_rf.feature_importances_}).sort_values('Importance', ascending=False)\nvar_imp1.head(10)","e73de7b7":"from sklearn.ensemble import RandomForestClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3,random_state=0)\n\n# Create the model with 100 trees\nmodel_rf = RandomForestClassifier(n_estimators=100, \n                               random_state= 0, \n                               max_features = 'sqrt',\n                               n_jobs=-1, verbose = 1)\n\n# Fit on training data\nmodel_rf.fit(X_train, y_train.values.ravel())\n\nn_nodes = []\nmax_depths = []\n\nfor ind_tree in model_rf.estimators_:\n    n_nodes.append(ind_tree.tree_.node_count)\n    max_depths.append(ind_tree.tree_.max_depth)\n    \nprint(f'Average number of nodes {int(np.mean(n_nodes))}')\nprint(f'Average maximum depth {int(np.mean(max_depths))}')","6810a744":"ytrain_rf_pred = model_rf.predict(X_train)\nytrain_rf_prob = model_rf.predict_proba(X_train)[:, 1]\n\nytest_rf_pred = model_rf.predict(X_test)\nytest_rf_prob = model_rf.predict_proba(X_test)[:, 1]","eb3a7a6b":"confusion3 = pd.crosstab(y_test['Survived'],ytest_rf_pred)\nconfusion3","9f9d66ce":"print('Accuracy Score : {}'.format(round(accuracy_score(y_test,ytest_rf_pred),2)))\nprint('Precision Score : {}'.format(round(precision_score(y_test,ytest_rf_pred),2)))\nprint('Recall Score : {}'.format(round(recall_score(y_test,ytest_rf_pred),2)))\nprint('F1 Score : {}'.format(round(f1_score(y_test,ytest_rf_pred),2)))","a528b9cb":"plot_roc_curve(model_rf,X_test,y_test)\nplt.show()","b2ff6d76":"model_prob1 = model_lr.predict_proba(test_new.drop(columns='PassengerId'))[:,1]\nmodel_prob2 = best_rf.predict_proba(test_new.drop(columns='PassengerId'))[:,1]\nmodel_prob3 = model_rf.predict_proba(test_new.drop(columns=['PassengerId']))[:,1]","b75b010e":"ensemble_prob = pd.DataFrame({'PassengerId': test['PassengerId'],\n                              'LogisticR': model_prob1,\n                              'RF_search': model_prob2,\n                              'RandomF': model_prob3})\nensemble_prob.head()","a55bb13e":"ensemble_prob['Prob'] = ensemble_prob.apply(lambda x: (x['LogisticR'] + x['RF_search'] + x['RandomF'] )\/3,axis=1)\nensemble_prob['Survived'] = np.where(ensemble_prob['Prob']>= 0.5,1,0)\nensemble_prob.head()","36381925":"# Preparing the submission file\nsubmission = pd.DataFrame({'PassengerId': ensemble_prob['PassengerId'],\n                           'Survived': ensemble_prob['Survived']})","f0747fba":"submission['Survived'].value_counts()","796d3fc1":"submission","56084560":"1. Survived column shows 418 missing values, which are from test data\n2. Age has 263 missing values\n3. Cabin has 1014 missing values\n4. Fare has 1 value missing and Embarked has 2 value missing","eec8ef8b":"# **Data preparation**\n\nFollowing are the steps to follow for data preparation:\n\n1. Find the missing values in the dataset and do imputation\/drop those missing values\n2. Check for outliers in the dataset and do required transformation on dataset\n3. Encode the categorical variables into dummy variables for model building\n4. Do univariate & bivariate analysis with dependant variables","7389c56a":"Above missing values show that both the persons has survived, and from the expected table, if the passenger is survived, the expected value doesnt meet for level C, as others are already achieved. Thus, filling with level C is statiscally correct.","4d01ae38":"# Predictive modeling","42898162":"# Random Forest","628fa8d9":"Thank you for viewing this notebook. Please do upvote if you like it. Comment if you have doubts or concerns.","09fda3b3":"Before impute the missing values for Age, let us encode the catergorical variables into dummy variables.","331c7c91":"From the above Chi-Square test, it is evident that Embarked and Survivors are statistically dependent to each other. And the expected values for each combination is given by chiSquare test. Let us impute the missing values based on suitable combination, where still the expected values are not met.","c6f7741c":"# Feature Engineering\n\n1. Let us first create the passenger travelled along with family or alone\n2. Calculate the no.of family members travelled in the ship along with the passenger, if they travelled with family\n3. Change of Sex, based on the name that contains Mr. , Mrs. , Miss.","3323e21d":"Ensembling the result of all 3 models by taking probability from each model and estimate the average of the probabilities. Then, the threshold value is applied by assigning survived if probability is greater than 0.5 and not survived for prob less than 0.5","0bd67067":"As it is seen, the fare values are highly skewed to the right. We can impute median value for the missing one. We need to do transformation on Fare price to make it normally distributed for model, which will be done later.","ae8fb313":"# Logistic regression with GridSearch","0717356d":"# Ensembling","e66a69cd":"# Random forest with Randomised search","3d9294a3":"In order to use these dummy code variables, we need to drop one column as we already have the information of third column in combination of first 2 columns, thus column C is dropped for ease of model building."}}