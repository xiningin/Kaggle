{"cell_type":{"fcd4bf9c":"code","3dbd1716":"code","f901410d":"code","0a9e097d":"code","ad01cb94":"code","07c82279":"code","9f908451":"code","7df9f023":"code","bd92478c":"code","61d93a17":"code","9259fc9e":"code","fbd9c067":"code","4c1fd125":"code","2841e90a":"code","5e5a727e":"code","cc0ae0a4":"code","b811f361":"code","c888629a":"code","a249b074":"code","caaec33b":"code","f9147ae0":"code","c468d0bc":"code","30b4b4e4":"code","a1be0029":"code","7f9523f5":"code","0825681a":"code","fc6b9306":"code","28899785":"code","def1d0b1":"code","2014b9a0":"code","e9009ecc":"code","f3918dac":"code","22788ebe":"markdown","a214342b":"markdown","2b78844e":"markdown","d42f3765":"markdown","736452f7":"markdown","8542bc86":"markdown","dfc63272":"markdown","8dca8c4b":"markdown","1bca5b80":"markdown","044f8af5":"markdown","9bae9f4a":"markdown","9e31b011":"markdown","70e2411f":"markdown"},"source":{"fcd4bf9c":"import nltk\nnltk.download('wordnet')","3dbd1716":"#import necessary libraries\nimport numpy as np\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.stem import PorterStemmer\nword_stemmer = PorterStemmer() \nlemmatizer = WordNetLemmatizer() ","f901410d":"#initialize necessary variables\npath = \"..\/input\/songgeneration\/dataset\/lyrics-PostMalone(5).txt\"                 #change the path accordingly \nreader = open(path,\"r\").read()\nsong_names=[]\nnum_songs = 0\nlines_in_song= []\nnum_words_in_line=[]\nnum_words_in_song = []\nwords_in_song=[]\nnum_lines=0\ntotal_words = []","0a9e097d":"#preprocess the lyrics.txt file\nfor line in reader.split(\"\\n\"):\n  if line.startswith(\"-\"):\n    song_names.append(line.split(\"-\")[1])\n    num_words_in_line.append([])\n    words_in_song.append([])\n    num_songs+=1\n    lines_in_song.append(num_lines-2)\n    num_lines=0\n    continue\n  num_lines+=1\n  for word in line.split(\" \"):\n    if word.isdigit():\n      continue\n    word = word.lower()\n    word = word_stemmer.stem(word)\n    words_in_song[num_songs-1].append(word)\n  if line.split(\" \") != \"\":\n    num_words_in_line[num_songs-1].append(len(line.split(\" \")))\n\nlines_in_song.append(num_lines)\nlines_in_song.remove(lines_in_song[0])\nnum_words_in_line = [i[0:len(i)-2] for i in num_words_in_line]\nwords_in_song = [i[0:len(i)-2] for i in words_in_song]\nnum_words_in_song = [len(x) for x in words_in_song]\n\n\nlines_count = np.mean(lines_in_song).round().astype(\"int32\")\nwords_count= np.mean(num_words_in_song).round().astype(\"int32\")","ad01cb94":"#remove punctuations from words obtained after above preprocessing\nno_punct = \"\"\npunctuations = '''!()-[]{};:'\"\\,<>.\/?@#$%^&*_~'''\nfor i,song in enumerate(words_in_song):\n  for j,word in enumerate(song):\n    for ch in word:\n      if ch not in punctuations:\n        no_punct = no_punct + ch\n    words_in_song[i][j] = no_punct\n    total_words.append(no_punct)\n    no_punct=\"\"\n\ndict_words = {}\nfor word in total_words:\n  if dict_words.get(word,0) ==0:\n    dict_words[word] =1\n  else:\n    dict_words[word] +=1\n\ndict_words_order = dict( sorted(dict_words.items(),\n                           key=lambda x: x[1]))\nvocab = set(total_words)\nvocab_len = len(vocab)\ntext = \"\"\nfor word in total_words:\n  text = text+ \" \"+word","07c82279":"#print the obtained results from above cell\nprint(\"Number of songs:\",num_songs)\nprint(\"Song names:\",song_names)\nprint(\"Lines in each song:\",lines_in_song)\n#print(\"Words in each line:\",num_words_in_line)\n#print(\"Words in each song:\",words_in_song)\n#print(\"Total words:\",total_words)\nprint(\"Total number of words:\",len(total_words))\n#print(\"Vocabulary:\",vocab)\nprint(\"vocabulary size:\",vocab_len)\nprint(\"Total number of lines:\",np.sum(lines_in_song))\nprint(\"Average number of lines in song:\",lines_count)\nprint(\"Average number of words in song:\",words_count)","9f908451":"import tensorflow as tf\nimport time\nimport os\n\nchar2idx = {u:i for i,u in enumerate(vocab)}\nidx2char = np.array(list(vocab))\ntext_as_int = np.array([char2idx[c] for c in text.split(\" \")[1:]])\nprint(idx2char)","7df9f023":"#prepare the dataset that is to be given to model\nseq_len = words_count\nexamples_per_epoch = len(total_words)\/\/(seq_len+1)\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\nsequences = char_dataset.batch(seq_len+1,drop_remainder =True)\n'''for item in sequences.take(5):\n  print(repr(' '.join(idx2char[item.numpy()])))'''\ndef split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\ndataset = sequences.map(split_input_target)\n#print(examples_per_epoch)\n#print(sequences)\n#print(dataset)","bd92478c":"#print a data sample\n'''for input_example, target_example in  dataset.take(1):\n    print('Input data: ', repr(' '.join(idx2char[input_example.numpy()])))\n    print('Target data:', repr(' '.join(idx2char[target_example.numpy()])))\n    for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n    print(\"Step {:4d}\".format(i))\n    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))'''","61d93a17":"BATCH_SIZE=8\nBUFFER_SIZE=500\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE,drop_remainder=True)\ndataset","9259fc9e":"#model\nembedding_dim = 256\nrnn_units = 1024\ndef create_model(vocab_size,embedding_dim,rnn_units,batch_size):\n  model = tf.keras.Sequential([\n          tf.keras.layers.Embedding(vocab_size,embedding_dim,\n                                     batch_input_shape = [batch_size,None]),\n          tf.keras.layers.LSTM(rnn_units,return_sequences= True,stateful =True,recurrent_initializer = \"glorot_uniform\"),\n          tf.keras.layers.Dense(vocab_size)\n  ])\n  return model\nprint(vocab_len)","fbd9c067":"model = create_model(\n    vocab_size=vocab_len,\n    embedding_dim=embedding_dim,\n    rnn_units=rnn_units,\n    batch_size=BATCH_SIZE)","4c1fd125":"for input, target in dataset.take(1):\n  example = model(input)\n  sample = tf.random.categorical(example[0], num_samples=1)\n  sample = tf.squeeze(sample,axis=-1).numpy()\n#print(sample)","2841e90a":"# summary of the model layers and weights\nmodel.summary()","5e5a727e":"#loss function\ndef loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\nexample_loss = loss(target, example)\nprint(\"Prediction shape: \", example.shape, \" # (batch_size, sequence_length, vocab_size)\")\nprint(\"scalar_loss:      \", example_loss.numpy().mean())","cc0ae0a4":"model.compile(optimizer='adam', loss=loss)\ncheckpoint_dir = '.\/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)\nEPOCHS = 25\nhistory = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])","b811f361":"tf.train.latest_checkpoint(checkpoint_dir)","c888629a":"model = create_model(vocab_len, embedding_dim, rnn_units, batch_size=1)\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\nmodel.build(tf.TensorShape([1, None]))\nmodel.save(\".\/generate_text.h5\")","a249b074":"def generate_text(model, start_string,words_count):\n    num_generate = words_count\n    input_eval = char2idx[start_string.lower()]\n    input_eval = tf.expand_dims(input_eval, 0)\n    text_generated = []\n    temperature = 1.0\n    model.reset_states()\n    for i in range(num_generate):\n        predictions = model.predict(input_eval)\n        predictions = tf.squeeze(predictions, 0)\n        predictions = predictions \/ temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n        input_eval = tf.expand_dims([predicted_id], 0)\n        text_generated.append(idx2char[predicted_id])\n    return (start_string +\" \"+ ' '.join(text_generated))","caaec33b":"song_raw=generate_text(model,\"ayy\",words_count) #start_string can be only from the vocab we obtained\nprint(song_raw)","f9147ae0":"words_per_line =[]\nfor i in num_words_in_line:\n  for j in i:\n    words_per_line.append(j)\nseq_len = lines_count\nexamples_per_epoch = len(words_per_line)\/\/(seq_len+1)\n\nprint(examples_per_epoch)\nprint(seq_len)\n\nwords_per_line_str = [str(i) for i in words_per_line]\nfor i in range(max(words_per_line)):\n  if i+1 not in words_per_line:\n    words_per_line_str.append(str(i+1))\nvocab = sorted(set(words_per_line_str))\n\nchar2idx = {u:i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)\ntext_as_int = np.array([char2idx[c] for c in words_per_line_str])","c468d0bc":"char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\nsequences = char_dataset.batch(seq_len+1, drop_remainder=True)","30b4b4e4":"def split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\ndataset = sequences.map(split_input_target)\nBATCH_SIZE = 2\nBUFFER_SIZE = 50\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\nvocab_size = max(words_per_line)","a1be0029":"embedding_dim = 64\nrnn_units = 512\ndef build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                                  batch_input_shape=[batch_size, None]),\n        tf.keras.layers.LSTM(rnn_units,\n                            return_sequences=True,\n                            stateful=True,\n                            recurrent_initializer='glorot_uniform'),\n        tf.keras.layers.Dense(vocab_size)\n    ])\n    return model\nmodel_lines = build_model(\n    vocab_size=len(vocab),\n    embedding_dim=embedding_dim,\n    rnn_units=rnn_units,\n    batch_size=BATCH_SIZE)\n\ndef loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)","7f9523f5":"model_lines.compile(optimizer='adam', loss=loss)\n\ncheckpoint_dir = '.\/training_checkpoints_lines'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)\nEPOCHS = 15\nhistory = model_lines.fit(dataset, epochs=EPOCHS,steps_per_epoch=1, callbacks=[checkpoint_callback])","0825681a":"print(vocab)","fc6b9306":"model_lines.summary()","28899785":"tf.train.latest_checkpoint(checkpoint_dir)\nmodel_lines = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\nmodel_lines.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\nmodel_lines.build(tf.TensorShape([1, None]))\nmodel_lines.save(\".\/generate_lines.h5\")","def1d0b1":"def generate_text(model, start_string,lines_count):\n    num_generate = lines_count\n    input_eval = [char2idx[s] for s in start_string]\n    input_eval = tf.expand_dims(input_eval, 0)\n    lines_generated = []\n    lines_generated.append(start_string)\n    temperature = 1.0\n    model.reset_states()\n    for i in range(num_generate):\n        predictions = model.predict(input_eval)\n        predictions = tf.squeeze(predictions, 0)\n        predictions = predictions \/ temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n        input_eval = tf.expand_dims([predicted_id], 0)\n        lines_generated.append(idx2char[np.array(input_eval)[0][0]])\n    return lines_generated","2014b9a0":"words_per_line=generate_text(model_lines,\"5\",lines_count)\nprint(words_per_line)","e9009ecc":"lines = [int(i) for i in words_per_line]\nsong_words = song_raw.split(\" \")\ncount=0\nfinal_song = \"\"\nfor i in lines:\n  for j in range(i):\n    count+=1\n    if count> len(song_words):\n      break\n    final_song = final_song+ \" \"\n    final_song= final_song+ song_words[count-1]\n  final_song+=\"\\n\"","f3918dac":"print(final_song)","22788ebe":"# Thanks for going through the notebook\n\nIf you found the notebook useful, show some \u2764, I will be happy if someone sings the song that was generated\ud83e\udd2d\ud83d\ude09. \n### Happy coding\ud83e\udd70","a214342b":"# Dataset preparation","2b78844e":"# Model and Training","d42f3765":"**FINAL SONG** (use the above two models to get the final song!:))","736452f7":"**Above model just creates words in song, but we need it line wise, below model will fulfill tis need.**","8542bc86":"# Preprocessing of lyrics file","dfc63272":"**Character to id conversion and viceversa**","8dca8c4b":"# Introduction","1bca5b80":"# Removing Punctuations","044f8af5":"# Generating text","9bae9f4a":"**Loading latest checkpoint we created**","9e31b011":"# **GENERATE NUM WORDS PER LINE** ","70e2411f":"This notebook will walk you through the steps needed to generate a song using simple RNN model. There are mainly 2 models that I used to generate final song'\n1. Model to generate words needed for song\n2. Model that will output number of words per line\n**Finally I integrated both to generate final song. Let's dive in**\n\n## First try to go through the [Song Generation](https:\/\/www.kaggle.com\/shanmukh05\/songgeneration) dataset I prepared for this task."}}