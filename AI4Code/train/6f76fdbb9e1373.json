{"cell_type":{"7889dd4a":"code","22ddc3cd":"code","90e8c0f0":"code","d26a7f22":"code","8407ba75":"code","db950281":"code","22a81f18":"code","ac9ee0ae":"code","34c25821":"code","99359124":"code","09b195ec":"code","92c26cac":"code","02670aee":"code","44403fe9":"code","ac17c8f4":"code","53aff387":"code","d23dbc76":"code","2516b489":"code","935902c7":"code","d770a5c8":"code","f491e729":"code","fe542801":"code","1420fbf1":"code","9d6ea4de":"code","a8d182be":"code","e39d3629":"code","05815abd":"code","6cd0791f":"code","08902623":"code","164a3c6d":"code","751b0a8b":"code","8e4f85ee":"code","48d0dfb4":"code","04222f13":"code","ba35e4b4":"code","68445946":"code","d2ebe0b4":"code","702d2132":"code","f7eea869":"markdown","318c7209":"markdown","9e655151":"markdown","7736d60d":"markdown","1d1e7795":"markdown","14016939":"markdown","7e57183d":"markdown","7050ade8":"markdown"},"source":{"7889dd4a":"from sklearn import *\nimport numpy as np\nimport pandas as pd\nimport glob\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\ntrain = pd.read_csv('..\/input\/jigsaw-train-translated-yandex-api\/train_yandex.csv')\nval = pd.read_csv('..\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv', usecols=['comment_text', 'toxic', 'lang'])","22ddc3cd":"test = pd.read_csv('..\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv')\ntest['comment_text'] = test['content']\ntest['toxic'] = 0.5","90e8c0f0":"val_languages = val.lang.unique().tolist()\nval_languages","d26a7f22":"non_val_languages = [l for l in test.lang.unique() if l not in val_languages]\nnon_val_languages","8407ba75":"train_non_val_lang = train[train.lang.isin(non_val_languages)].sample(frac = 1).reset_index()","db950281":"train_non_val_lang.head(10)","22a81f18":"toxic_count = train.toxic.sum()\nprint(toxic_count)\ntrain = pd.concat([train[train.toxic == 1], train[train.toxic == 0].sample(toxic_count + 5000)]).sample(frac = 1)","ac9ee0ae":"[train_v, val] = train_test_split(val, test_size = 0.05, random_state = 411)","34c25821":"train.shape, train_v.shape","99359124":"import re, string\n\nre_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\ndef tokenize(s): \n    return re_tok.sub(r' \\1 ', s).split()","09b195ec":"# from stop_words import get_stop_words\n# stop_words = get_stop_words('spanish') + get_stop_words('turkish')+ get_stop_words('italian')\n# stop_words = set(stop_words)","92c26cac":"COMMENT = 'comment_text'\nLABEL = 'toxic'","02670aee":"binary = False","44403fe9":"vec_train = TfidfVectorizer(ngram_range=(1,1), tokenizer=tokenize,\n               min_df=2, max_df=0.9, strip_accents='unicode', use_idf=1, binary=binary,\n               smooth_idf=1, sublinear_tf=1 )\nvec_train.fit(train[COMMENT])\nval_on_train = vec_train.transform(val[COMMENT])\ntrn_on_train = vec_train.transform(train[COMMENT])","ac17c8f4":"vec_val = TfidfVectorizer(ngram_range=(1,1), tokenizer=tokenize,\n               min_df=2, max_df=0.9, strip_accents='unicode', use_idf=1, binary=binary,\n               smooth_idf=1, sublinear_tf=1 )\n\nvec_val.fit(pd.concat([train_v[COMMENT], val[COMMENT]]))\ntrn_on_val = vec_val.transform(train_v[COMMENT])\nval_on_val = vec_val.transform(val[COMMENT])","53aff387":"val_on_train, trn_on_train, trn_on_val, val_on_val","d23dbc76":"x_on_train = trn_on_train\nval_x_on_train = val_on_train\n\nx_on_val = trn_on_val\nval_x_on_val = val_on_val","2516b489":"y_val = val[LABEL].values","935902c7":"def pr_train(y_i, y):\n    p = x_on_train[y==y_i].sum(0)\n    return (p+3) \/ ((y==y_i).sum()+3)\n\ndef pr_val(y_i, y):\n    p = x_on_val[y==y_i].sum(0)\n    return (p+0.2) \/ ((y==y_i).sum()+0.2)\n\ny_train = train[LABEL].values\ny_train_v = train_v[LABEL].values\n\nr_train = np.log(pr_train(1,y_train) \/ pr_train(0,y_train))\nr_val = np.log(pr_val(1,y_train_v) \/ pr_val(0,y_train_v))","d770a5c8":"x_nb_on_train = x_on_train.multiply(r_train)\n\nC_PARAMETERS = [1.5, 2, 4]\nmodels_train = [LogisticRegression(C=c, dual=True, solver='liblinear') for c in C_PARAMETERS]\npreds_val_on_train = []\n\nfor model in models_train:\n    model.fit(x_nb_on_train, y_train)\n    p = model.predict_proba(val_x_on_train.multiply(r_train))[:,1]\n    print(roc_auc_score(y_val, p))\n    preds_val_on_train.append(p)\n    \npreds_ensemble_val_on_train = 2**((np.log2(preds_val_on_train[1]) + np.log2(preds_val_on_train[2]))\/2)\n\nprint(roc_auc_score(y_val, preds_ensemble_val_on_train))","f491e729":"x_nb_on_val = x_on_val.multiply(r_val)\n\nmodels_train_v = [LogisticRegression(C=c, dual=True, solver='liblinear') for c in C_PARAMETERS]\npreds_val_on_val = []\n\nfor model in models_train_v:\n    model.fit(x_nb_on_val, y_train_v)\n    p = model.predict_proba(val_x_on_val.multiply(r_val))[:,1]\n    print(roc_auc_score(y_val, p))\n    preds_val_on_val.append(p)\n    \npreds_ensemble_val_on_val = 2**((np.log2(preds_val_on_val[1]) + np.log2(preds_val_on_val[2]))\/2)\n\nprint(roc_auc_score(y_val, preds_ensemble_val_on_val))","fe542801":"preds_val = [\n    models_train[1].predict_proba(val_x_on_train.multiply(r_train))[:,1],\n    models_train[2].predict_proba(val_x_on_train.multiply(r_train))[:,1],\n    models_train_v[1].predict_proba(val_x_on_val.multiply(r_val))[:,1],\n    models_train_v[2].predict_proba(val_x_on_val.multiply(r_val))[:,1],\n]","1420fbf1":"preds_val_ens = 2**np.mean([np.log2(p) for p in preds_val], axis = 0)\nprint(roc_auc_score(y_val, preds_val_ens))","9d6ea4de":"is_val_lang = test.lang.isin(['tr', 'es', 'it'])","a8d182be":"np.sum(is_val_lang.values)","e39d3629":"test_val_lang = test.loc[is_val_lang, COMMENT]\ntest_non_val_lang = test.loc[~is_val_lang, COMMENT]\n","05815abd":"test_val_lang_on_train = vec_train.transform(test_val_lang)\ntest_nonval_lang_on_train = vec_train.transform(test_non_val_lang)\ntest_val_lang_on_val = vec_val.transform(test_val_lang)","6cd0791f":"test_val_lang_on_train, test_nonval_lang_on_train, test_val_lang_on_val","08902623":"preds1_val_lang_on_train = models_train[0].predict_proba(test_val_lang_on_train.multiply(r_train))[:,1]\npreds2_val_lang_on_train = models_train[1].predict_proba(test_val_lang_on_train.multiply(r_train))[:,1]\npreds4_val_lang_on_train = models_train[2].predict_proba(test_val_lang_on_train.multiply(r_train))[:,1]\n\npreds1_nonval_lang_on_train = models_train[0].predict_proba(test_nonval_lang_on_train.multiply(r_train))[:,1]\npreds2_nonval_lang_on_train = models_train[1].predict_proba(test_nonval_lang_on_train.multiply(r_train))[:,1]\npreds4_nonval_lang_on_train = models_train[2].predict_proba(test_nonval_lang_on_train.multiply(r_train))[:,1]","164a3c6d":"preds1_val_lang_on_val = models_train_v[0].predict_proba(test_val_lang_on_val.multiply(r_val))[:,1]\npreds2_val_lang_on_val = models_train_v[1].predict_proba(test_val_lang_on_val.multiply(r_val))[:,1]\npreds4_val_lang_on_val = models_train_v[2].predict_proba(test_val_lang_on_val.multiply(r_val))[:,1]","751b0a8b":"\npreds_val = 2**((np.log2(preds2_val_lang_on_train) + np.log2(preds4_val_lang_on_train) +np.log2(preds2_val_lang_on_val) + np.log2(preds4_val_lang_on_val)) \/ 4)\npreds_nonval = 2**((np.log2(preds2_nonval_lang_on_train) + np.log2(preds4_nonval_lang_on_train)) \/ 2)","8e4f85ee":"test.loc[is_val_lang, 'toxic'] = preds_val\ntest.loc[~is_val_lang, 'toxic'] = preds_nonval","48d0dfb4":"test.iloc[28].toxic, test.iloc[28].content","04222f13":"submission1 = test[['id', 'toxic', 'lang']]\nsubmission1.to_csv('submission1.csv', index=False)","ba35e4b4":"submission2 = pd.read_csv('..\/input\/tpu-inference-super-fast-xlmroberta\/submission.csv') # Ver 4","68445946":"submission1['toxic'] = submission1['toxic'] * 0.04 + submission2['toxic'] * 0.96","d2ebe0b4":"submission1.loc[submission1[\"lang\"] == \"es\", \"toxic\"] *= 1.06\nsubmission1.loc[submission1[\"lang\"] == \"fr\", \"toxic\"] *= 1.04\nsubmission1.loc[submission1[\"lang\"] == \"it\", \"toxic\"] *= 0.97\nsubmission1.loc[submission1[\"lang\"] == \"pt\", \"toxic\"] *= 0.96\nsubmission1.loc[submission1[\"lang\"] == \"tr\", \"toxic\"] *= 0.98","702d2132":"submission1[['id', 'toxic']].to_csv('submission.csv', index=False)","f7eea869":"I do not like this method, but it works (+0.0008 9461 -> 9469). \nIt would be better to use mean language-dependent AUC, not AUC for 6 lang. In that case language weights probing wouldn't work\n\nhttps:\/\/www.kaggle.com\/c\/jigsaw-multilingual-toxic-comment-classification\/discussion\/160980","318c7209":"We can try using a binary version of tf-idf because toxic words influence their presence","9e655151":"#### Stop words doesn't increase score","7736d60d":"Validation set can be splitted into an additional dataset and a small validation dataset.","1d1e7795":"Train dataset can be resampled with equal numbers of toxic and non-toxic comments","14016939":"## Multilingual NB-SVM with separate validation set presented \/ non-presented languages processing\n### log ensembling is using.\n#### Upvote please","7e57183d":"We have two different datasets. The first is a train which contains 6 laguages (after translation). Many toxic words are lost while translation. \nThe second is a validation dataset which contains 3 languages","7050ade8":"Ensembling with a non-ensamble kernel:\n    https:\/\/www.kaggle.com\/shonenkov\/tpu-inference-super-fast-xlmroberta"}}