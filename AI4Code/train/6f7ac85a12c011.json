{"cell_type":{"ee69ad8f":"code","749e3deb":"code","7acef3ec":"code","1b60b090":"code","6f9fd760":"code","d758a668":"code","32897947":"code","e582e886":"code","fcd91e4e":"code","18a256e2":"code","f50fad68":"code","832aec12":"code","187a1264":"code","aa5bd1ce":"code","fed49f21":"code","63ff5ed7":"code","fdad41d9":"code","b27a203d":"code","b50c27b7":"code","18c893d3":"code","b20fafd8":"code","46a9e509":"code","ab8a433f":"code","38c61661":"code","f7c4d0d7":"code","f37e19c5":"code","ed22eda0":"code","0cfcc2c5":"code","982bb14a":"code","ca67679a":"code","27a1ee5b":"code","0d83abfb":"code","bc38dd5a":"code","ba0d1471":"code","14b569d5":"code","9f5f81f6":"code","daa62118":"code","f58c5131":"code","5029ee17":"code","30e42f1e":"code","4a4674da":"code","901310aa":"code","a7afa181":"code","166ab3b4":"code","4906242e":"code","04540096":"code","7e33ce30":"code","3625e7b3":"code","1a09b12c":"code","5516fe0b":"code","a1713eb0":"code","cc79ac28":"code","ff599bcc":"code","8391d31a":"code","961b46bc":"code","73c37fa0":"code","4206cd2c":"code","4e7f2d6c":"code","e2f47fd9":"code","2d9486a2":"code","a50df04f":"code","36770fd5":"code","48307825":"code","89eb75aa":"code","095efa7e":"code","abf2c8cf":"code","dd9db6ef":"code","438f1abb":"markdown","7aa007c9":"markdown","9a7e03ca":"markdown","da468201":"markdown","01d02e01":"markdown","7e73f3b3":"markdown","c52c76af":"markdown","7357f927":"markdown","17c20dde":"markdown","2974c3ed":"markdown","7b8fcc49":"markdown","e1376ee8":"markdown","17dc3156":"markdown","7a3c9bfd":"markdown","0a00ce6f":"markdown","4bf66610":"markdown","161d9216":"markdown"},"source":{"ee69ad8f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.linear_model import LogisticRegression  # for Logistic Regression algorithm\nfrom sklearn.model_selection import train_test_split #to split the dataset for training and testing\nfrom sklearn.neighbors import KNeighborsClassifier  # for K nearest neighbours\nfrom sklearn import svm  #for Support Vector Machine (SVM) Algorithm\nfrom sklearn import metrics #for checking the model accuracy\nfrom sklearn.tree import DecisionTreeClassifier #for using Decision Tree Algoithm\nfrom sklearn import tree\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import export_graphviz #plot tree\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC  \nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nfrom numpy.random import seed\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom subprocess import call\nfrom IPython.display import Image\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nimport time\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import FastICA\nfrom sklearn.random_projection import SparseRandomProjection, GaussianRandomProjection\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.datasets import load_digits\nfrom scipy.sparse import csr_matrix\nimport sklearn\nfrom numpy.testing import assert_array_almost_equal\nfrom scipy.stats import  kurtosis\nnp.random.seed(42)\nfrom sklearn.decomposition import DictionaryLearning\n\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 19})\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","749e3deb":"def get_accuracy(X_train_NN,y_train_NN):\n    with torch.no_grad():\n        no_samples=0\n        no_preds=0\n        tp=0.01\n        tn=0.01\n        fp=0.01\n        fn=0.01\n        for i in range(0,list(y_train_NN.size())[0]):\n            prediction = net(X_train_NN[i])\n            predicted_class = np.argmax(prediction)\n            no_samples=no_samples+1\n            if(predicted_class==y_train_NN[i]):\n                no_preds=no_preds+1\n                if(predicted_class==1):\n                    tp=tp+1\n                else:\n                    tn=tn+1\n            else:\n                if(predicted_class==1):\n                    fp=fp+1\n                else:\n                    fn=fn+1\n        accuracy=no_preds\/no_samples\n        precision=tp\/(tp+fp)\n        recall=tp\/(tp+fn)\n        f1=2*(precision*recall)\/(precision+recall)        \n        return {'accuracy':accuracy,'precision':precision,'recall':recall,'f1':f1}","7acef3ec":"bank=pd.read_csv('\/kaggle\/input\/bank-marketing-dataset\/bank.csv')","1b60b090":"bank.deposit.value_counts()","6f9fd760":"bank.describe(include='all')","d758a668":"bank['deposit'][bank['deposit'] == 'yes'] =1\nbank['deposit'][bank['deposit'] == 'no'] = 0\nbank['deposit']=bank['deposit'].astype(int)\nbank2 = pd.get_dummies(bank, drop_first=True)\nbank2_norm = (bank2.drop('deposit',1) - np.min(bank2.drop('deposit',1))) \/ (np.max(bank2.drop('deposit',1)) - np.min(bank2.drop('deposit',1))).values\nbank2_norm['deposit']=bank2.deposit","32897947":"# bank['deposit'][bank['deposit'] == 'yes'] =1\n# bank['deposit'][bank['deposit'] == 'no'] = 0\n# bank['deposit']=bank['deposit'].astype(int)\n# bank2 = pd.get_dummies(bank, drop_first=True)\n# bank2_norm = (bank2.drop('deposit',1) - np.min(bank2.drop('deposit',1))) \/ (np.max(bank2.drop('deposit',1)) - np.min(bank2.drop('deposit',1))).values\n# bank2_norm['deposit']=bank2.deposit\n# train_norm, test_norm = train_test_split(bank2_norm, test_size = 0.3,random_state=0)# in this our main data is split into train and test\n# # the attribute test_size=0.3 splits the data into 70% and 30% ratio. train=70% and test=30%\n# print(train_norm.shape)\n# print(test_norm.shape)\n# train_X_norm = train_norm.drop('deposit',1)# taking the training data features\n# train_y_norm=train_norm.deposit # output of our training data\n# test_X_norm= test_norm.drop('deposit',1) # taking test data features\n# test_y_norm =test_norm.deposit   #output value of test data","e582e886":"bank2X=bank2_norm.drop('deposit',1)","fcd91e4e":"kmeans = KMeans(n_clusters=2, random_state=0).fit(bank2X)","18a256e2":"print(kmeans.labels_)\nprint(kmeans.cluster_centers_.shape)","f50fad68":"bank2X","832aec12":"# from yellowbrick.cluster import SilhouetteVisualizer\n\n# fig, ax = plt.subplots(2, 2, figsize=(15,8))\n# for i in [2, 3, 4, 5]:\n#     '''\n#     Create KMeans instance for different number of clusters\n#     '''\n#     km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)\n#     q, mod = divmod(i, 2)\n#     '''\n#     Create SilhouetteVisualizer instance with KMeans instance\n#     Fit the visualizer\n#     '''\n#     visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])\n#     visualizer.fit(bank2X)","187a1264":"SC=[]\nCHS=[]\nDBS=[]\nhom=[]\ncom=[]\nvm=[]\nfor i in range(2,50):\n    km = KMeans(n_clusters=i, init='k-means++', n_init=10, random_state=42).fit(bank2X)\n    SC.append(sklearn.metrics.silhouette_score(bank2X, km.labels_, metric='euclidean', sample_size=None, random_state=42))\n    CHS.append(sklearn.metrics.calinski_harabasz_score(bank2X, km.labels_))\n    DBS.append(sklearn.metrics.davies_bouldin_score(bank2X, km.predict(bank2X)))\n    hom.append(metrics.homogeneity_score(bank2['deposit'], km.labels_))\n    com.append(metrics.completeness_score(bank2['deposit'], km.labels_))\n    vm.append(metrics.v_measure_score(bank2['deposit'], km.labels_))\n    \n","aa5bd1ce":"fig, axs = plt.subplots(3,2)\naxs[0,0].plot(range(2,50),SC,marker='o')\naxs[0,0].set( ylabel='Silhouette Score')\n\naxs[1,0].plot(range(2,50),CHS,marker='o')\naxs[1,0].set( ylabel='Calinski-Harabasz Index')\n\naxs[2,0].plot(range(2,50),DBS,marker='o')\naxs[2,0].set(xlabel='Number of Clusters', ylabel='Davies-Bouldin Index')\n\naxs[0,1].plot(range(2,50),hom,marker='o')\naxs[0,1].set( ylabel='Homogeneity')\n\naxs[1,1].plot(range(2,50),com,marker='o')\naxs[1,1].set( ylabel='Completeness')\n\naxs[2,1].plot(range(2,50),vm,marker='o')\naxs[2,1].set(xlabel='Number of Clusters', ylabel='V-Measure')\n\n\nfig=plt.gcf()\nfig.set_size_inches(15,15)","fed49f21":"gmm = GaussianMixture(n_components=2).fit(bank2X)\nlabels = gmm.predict(bank2X)\nprobs = gmm.predict_proba(bank2X)","63ff5ed7":"SC=[]\nCHS=[]\nDBS=[]\nhom=[]\ncom=[]\nvm=[]\nfor i in range(2,50):\n    km = GaussianMixture(n_components=i).fit(bank2X)\n    SC.append(sklearn.metrics.silhouette_score(bank2X, km.predict(bank2X), metric='euclidean', sample_size=None, random_state=42))\n    CHS.append(sklearn.metrics.calinski_harabasz_score(bank2X, km.predict(bank2X)))\n    DBS.append(sklearn.metrics.davies_bouldin_score(bank2X, km.predict(bank2X)))\n    hom.append(metrics.homogeneity_score(bank2['deposit'], km.predict(bank2X)))\n    com.append(metrics.completeness_score(bank2['deposit'], km.predict(bank2X)))\n    vm.append(metrics.v_measure_score(bank2['deposit'], km.predict(bank2X)))\n\n","fdad41d9":"fig, axs = plt.subplots(3,2)\naxs[0,0].plot(range(2,50),SC,marker='o')\naxs[0,0].set( ylabel='Silhouette Score')\n\naxs[1,0].plot(range(2,50),CHS,marker='o')\naxs[1,0].set( ylabel='Calinski-Harabasz Index')\n\naxs[2,0].plot(range(2,50),DBS,marker='o')\naxs[2,0].set(xlabel='Number of Clusters', ylabel='Davies-Bouldin Index')\n\naxs[0,1].plot(range(2,50),hom,marker='o')\naxs[0,1].set( ylabel='Homogeneity')\n\naxs[1,1].plot(range(2,50),com,marker='o')\naxs[1,1].set( ylabel='Completeness')\n\naxs[2,1].plot(range(2,50),vm,marker='o')\naxs[2,1].set(xlabel='Number of Clusters', ylabel='V-Measure')\n\n\nfig=plt.gcf()\nfig.set_size_inches(15,15)","b27a203d":"pca = PCA(n_components=41)\npca.fit(bank2X)\n\nlosses=[]\n\nfor i in range(1,42):\n    pca = PCA(n_components=i)\n    pca.fit(bank2X)\n    X_train_pca = pca.transform(bank2X)\n    X_projected = pca.inverse_transform(X_train_pca)\n    loss = np.sum((bank2X - X_projected) ** 2, axis=1).mean()\n    losses.append(loss)\n","b50c27b7":"fig, axs = plt.subplots(1,2)\n  \n\naxs[0].plot(range(1,42),pca.explained_variance_,marker='o')\naxs[0].set(xlabel='Component Number', ylabel='Explained Variance')\n\naxs[1].plot(range(1,42),losses,marker='o')\naxs[1].set(xlabel='# Components', ylabel='Re-construction error')\n\n\nfig=plt.gcf()\nfig.set_size_inches(16,7)\n","18c893d3":"losses=[]\nkur=[]\n\nfor i in range(1,42):\n    transformer = FastICA(n_components=i,random_state=0)\n    X_transformed = transformer.fit_transform(bank2X)\n    kur.append(np.mean(np.abs(kurtosis(X_transformed))))\n    X_projected = transformer.inverse_transform(X_transformed)\n    loss = np.sum((bank2X - X_projected) ** 2, axis=1).mean()\n    losses.append(loss)\n    ","b20fafd8":"fig, axs = plt.subplots(1,2)\n  \n\naxs[0].plot(range(1,42),losses,marker='o')\naxs[0].set(xlabel='# Components', ylabel='Re-construction error')\n\naxs[1].plot(range(1,42),kur,marker='o')\naxs[1].set(xlabel='# Components', ylabel='Average Kurtosis')\n\n\nfig=plt.gcf()\nfig.set_size_inches(16,7)","46a9e509":"losses=[]\n\nfor i in range(1,42):\n    random_projection = SparseRandomProjection(n_components=i)\n    random_projection.fit(bank2X)\n    components =  random_projection.components_.toarray() # shape=(5, 11) \n    p_inverse = np.linalg.pinv(components.T) # shape=(5, 11) \n    #now get the transformed data using the projection components\n    reduced_data = random_projection.transform(bank2X) #shape=(4898, 5) \n    reconstructed= reduced_data.dot(p_inverse)  #shape=(4898, 11) \n    error = metrics.mean_squared_error(bank2X, reconstructed)\n    losses.append(error)\n    ","ab8a433f":"plt.plot(range(1,42),losses,marker='o')\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.xlabel(\"# Components\", fontsize=18)\nplt.ylabel(\"Re-construction error\", fontsize=18)","38c61661":"losses1=[]\nlosses2=[]\nlosses3=[]\nlosses4=[]\nfor i in range(1,42):\n    random_projection = SparseRandomProjection(n_components=i,random_state=np.random)\n    random_projection.fit(bank2X)\n    components =  random_projection.components_.toarray() # shape=(5, 11) \n    p_inverse = np.linalg.pinv(components.T) # shape=(5, 11) \n    #now get the transformed data using the projection components\n    reduced_data = random_projection.transform(bank2X) #shape=(4898, 5) \n    reconstructed= reduced_data.dot(p_inverse)  #shape=(4898, 11) \n    error = metrics.mean_squared_error(bank2X, reconstructed)\n    losses1.append(error)\nfor i in range(1,42):\n    random_projection = SparseRandomProjection(n_components=i,random_state=np.random)\n    random_projection.fit(bank2X)\n    components =  random_projection.components_.toarray() # shape=(5, 11) \n    p_inverse = np.linalg.pinv(components.T) # shape=(5, 11) \n    #now get the transformed data using the projection components\n    reduced_data = random_projection.transform(bank2X) #shape=(4898, 5) \n    reconstructed= reduced_data.dot(p_inverse)  #shape=(4898, 11) \n    error = metrics.mean_squared_error(bank2X, reconstructed)\n    losses2.append(error)\nfor i in range(1,42):\n    random_projection = SparseRandomProjection(n_components=i,random_state=np.random)\n    random_projection.fit(bank2X)\n    components =  random_projection.components_.toarray() # shape=(5, 11) \n    p_inverse = np.linalg.pinv(components.T) # shape=(5, 11) \n    #now get the transformed data using the projection components\n    reduced_data = random_projection.transform(bank2X) #shape=(4898, 5) \n    reconstructed= reduced_data.dot(p_inverse)  #shape=(4898, 11) \n    error = metrics.mean_squared_error(bank2X, reconstructed)\n    losses3.append(error)\nfor i in range(1,42):\n    random_projection = SparseRandomProjection(n_components=i,random_state=np.random)\n    random_projection.fit(bank2X)\n    components =  random_projection.components_.toarray() # shape=(5, 11) \n    p_inverse = np.linalg.pinv(components.T) # shape=(5, 11) \n    #now get the transformed data using the projection components\n    reduced_data = random_projection.transform(bank2X) #shape=(4898, 5) \n    reconstructed= reduced_data.dot(p_inverse)  #shape=(4898, 11) \n    error = metrics.mean_squared_error(bank2X, reconstructed)\n    losses4.append(error)\n    \nplt.plot(range(1,42),losses,marker='o')\nplt.plot(range(1,42),losses1,marker='o')\nplt.plot(range(1,42),losses2,marker='o')\nplt.plot(range(1,42),losses3,marker='o')\nplt.plot(range(1,42),losses4,marker='o')\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.xlabel(\"# Components\", fontsize=18)\nplt.ylabel(\"Re-construction error\", fontsize=18)","f7c4d0d7":"from sklearn.decomposition import DictionaryLearning\n","f37e19c5":"losses=[]\nspars=[]\nfor i in range(1,42):\n    dict_learner = DictionaryLearning(n_components=i, transform_algorithm='lasso_cd',fit_algorithm='cd',max_iter=10, random_state=42,transform_max_iter=10)\n    X_train_pca = dict_learner.fit_transform(bank2X)\n    X_hat = X_train_pca @ dict_learner.components_\n    loss = np.mean(np.sum((X_hat - bank2X) ** 2, axis=1) \/ np.sum(bank2X ** 2, axis=1))\n    losses.append(loss)\n    spars.append(np.mean(X_train_pca == 0))","ed22eda0":"fig, axs = plt.subplots(1,2)\n  \n\naxs[0].plot(range(1,42),losses,marker='o')\naxs[0].set(xlabel='# Components', ylabel='Re-construction error')\n\naxs[1].plot(range(1,42),spars,marker='o')\naxs[1].set(xlabel='# Components', ylabel='Level of sparsity')\n\n\nfig=plt.gcf()\nfig.set_size_inches(16,7)","0cfcc2c5":"anu=pd.read_csv('\/kaggle\/input\/anuran-calls-mfccs-data-set\/Frogs_MFCCs.csv')","982bb14a":"anu.describe(include='all')","ca67679a":"pd.DataFrame(anu[['Family']].value_counts(),columns=['# Species']).reset_index().plot.bar(x='Family',y='# Species',rot=0)","27a1ee5b":"pd.DataFrame(anu[['Genus']].value_counts(),columns=['# Species']).reset_index().plot.barh(x='Genus',y='# Species',rot=0)","0d83abfb":"pd.DataFrame(anu[['Species']].value_counts(),columns=['# Species']).reset_index().plot.barh(x='Species',y='# Species',rot=0)","bc38dd5a":"anu_features=anu.drop(['Family','Genus','Species','RecordID'],1)","ba0d1471":"anu_features","14b569d5":"anu_features_norm = (anu_features - np.min(anu_features)) \/ (np.max(anu_features) - np.min(anu_features)).values\n","9f5f81f6":"anu_features_norm","daa62118":"# from yellowbrick.cluster import SilhouetteVisualizer\n\n# fig, ax = plt.subplots(2, 2, figsize=(15,8))\n# for i in [2, 3, 4, 5]:\n#     '''\n#     Create KMeans instance for different number of clusters\n#     '''\n#     km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)\n#     q, mod = divmod(i, 2)\n#     '''\n#     Create SilhouetteVisualizer instance with KMeans instance\n#     Fit the visualizer\n#     '''\n#     visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])\n#     visualizer.fit(bank2X)","f58c5131":"SC=[]\nCHS=[]\nDBS=[]\nhom=[]\ncom=[]\nvm=[]\n\n\nfor i in range(2,50):\n    km = KMeans(n_clusters=i, init='k-means++', n_init=10, random_state=42).fit(anu_features_norm)\n    SC.append(sklearn.metrics.silhouette_score(anu_features_norm, km.labels_, metric='euclidean', sample_size=None, random_state=42))\n    CHS.append(sklearn.metrics.calinski_harabasz_score(anu_features_norm, km.labels_))\n    DBS.append(sklearn.metrics.davies_bouldin_score(anu_features_norm, km.predict(anu_features_norm)))\n    hom.append([metrics.homogeneity_score(anu['Family'], km.labels_),metrics.homogeneity_score(anu['Genus'], km.labels_),metrics.homogeneity_score(anu['Species'], km.labels_)])\n    com.append([metrics.completeness_score(anu['Family'], km.labels_),metrics.completeness_score(anu['Genus'], km.labels_),metrics.completeness_score(anu['Species'], km.labels_)])\n    vm.append([metrics.v_measure_score(anu['Family'], km.labels_),metrics.v_measure_score(anu['Genus'], km.labels_),metrics.v_measure_score(anu['Species'], km.labels_)])\n    \n","5029ee17":"fig, axs = plt.subplots(3,2)\naxs[0,0].plot(range(2,50),SC,marker='o')\naxs[0,0].set( ylabel='Silhouette Score')\n\naxs[1,0].plot(range(2,50),CHS,marker='o')\naxs[1,0].set( ylabel='Calinski-Harabasz Index')\n\naxs[2,0].plot(range(2,50),DBS,marker='o')\naxs[2,0].set(xlabel='Number of Clusters', ylabel='Davies-Bouldin Index')\n\naxs[0,1].plot(range(2,50),hom,marker='o')\naxs[0,1].set( ylabel='Homogeneity')\naxs[0,1].legend(['Family','Genus','Species'])\n\naxs[1,1].plot(range(2,50),com,marker='o')\naxs[1,1].set( ylabel='Completeness')\naxs[1,1].legend(['Family','Genus','Species'])\n\naxs[2,1].plot(range(2,50),vm,marker='o')\naxs[2,1].set(xlabel='Number of Clusters', ylabel='V-Measure')\naxs[2,1].legend(['Family','Genus','Species'])\n\n\nfig=plt.gcf()\nfig.set_size_inches(15,15)","30e42f1e":"SC=[]\nCHS=[]\nDBS=[]\nhom=[]\ncom=[]\nvm=[]\n\nfor i in range(2,50):\n    km = GaussianMixture(n_components=i).fit(anu_features_norm)\n    SC.append(sklearn.metrics.silhouette_score(anu_features_norm, km.predict(anu_features_norm), metric='euclidean', sample_size=None, random_state=42))\n    CHS.append(sklearn.metrics.calinski_harabasz_score(anu_features_norm, km.predict(anu_features_norm)))\n    DBS.append(sklearn.metrics.davies_bouldin_score(anu_features_norm, km.predict(anu_features_norm)))\n    hom.append([metrics.homogeneity_score(anu['Family'], km.predict(anu_features_norm)),metrics.homogeneity_score(anu['Genus'], km.predict(anu_features_norm)),metrics.homogeneity_score(anu['Species'], km.predict(anu_features_norm))])\n    com.append([metrics.completeness_score(anu['Family'], km.predict(anu_features_norm)),metrics.completeness_score(anu['Genus'], km.predict(anu_features_norm)),metrics.completeness_score(anu['Species'], km.predict(anu_features_norm))])\n    vm.append([metrics.v_measure_score(anu['Family'], km.predict(anu_features_norm)),metrics.v_measure_score(anu['Genus'], km.predict(anu_features_norm)),metrics.v_measure_score(anu['Species'], km.predict(anu_features_norm))])\n\n","4a4674da":"fig, axs = plt.subplots(3,2)\naxs[0,0].plot(range(2,50),SC,marker='o')\naxs[0,0].set( ylabel='Silhouette Score')\n\naxs[1,0].plot(range(2,50),CHS,marker='o')\naxs[1,0].set( ylabel='Calinski-Harabasz Index')\n\naxs[2,0].plot(range(2,50),DBS,marker='o')\naxs[2,0].set(xlabel='Number of Clusters', ylabel='Davies-Bouldin Index')\n\naxs[0,1].plot(range(2,50),hom,marker='o')\naxs[0,1].set( ylabel='Homogeneity')\naxs[0,1].legend(['Family','Genus','Species'])\n\naxs[1,1].plot(range(2,50),com,marker='o')\naxs[1,1].set( ylabel='Completeness')\naxs[1,1].legend(['Family','Genus','Species'])\n\naxs[2,1].plot(range(2,50),vm,marker='o')\naxs[2,1].set(xlabel='Number of Clusters', ylabel='V-Measure')\naxs[2,1].legend(['Family','Genus','Species'])\n\n\nfig=plt.gcf()\nfig.set_size_inches(15,15)","901310aa":"pca = PCA(n_components=22)\npca.fit(anu_features_norm)\n\nlosses=[]\n\nfor i in range(1,23):\n    pca = PCA(n_components=i)\n    pca.fit(anu_features_norm)\n    X_train_pca = pca.transform(anu_features_norm)\n    X_projected = pca.inverse_transform(X_train_pca)\n    loss = np.sum((anu_features_norm - X_projected) ** 2, axis=1).mean()\n    losses.append(loss)\n    ","a7afa181":"fig, axs = plt.subplots(1,2)\n  \n\naxs[0].plot(range(1,23),pca.explained_variance_,marker='o')\naxs[0].set(xlabel='Component Number', ylabel='Explained Variance')\n\naxs[1].plot(range(1,23),losses,marker='o')\naxs[1].set(xlabel='# Components', ylabel='Re-construction error')\n\n\nfig=plt.gcf()\nfig.set_size_inches(16,7)\n","166ab3b4":"losses=[]\nkur=[]\n\nfor i in range(1,23):\n    transformer = FastICA(n_components=i,random_state=0)\n    X_transformed = transformer.fit_transform(anu_features_norm)\n    kur.append(np.mean(np.abs(kurtosis(X_transformed))))\n    X_projected = transformer.inverse_transform(X_transformed)\n    loss = np.sum((anu_features_norm - X_projected) ** 2, axis=1).mean()\n    losses.append(loss)\n    ","4906242e":"fig, axs = plt.subplots(1,2)\n  \n\naxs[0].plot(range(1,23),losses,marker='o')\naxs[0].set(xlabel='# Components', ylabel='Re-construction error')\n\naxs[1].plot(range(1,23),kur,marker='o')\naxs[1].set(xlabel='# Components', ylabel='Average Kurtosis')\n\n\nfig=plt.gcf()\nfig.set_size_inches(16,7)","04540096":"losses=[]\n\nfor i in range(1,23):\n    random_projection = SparseRandomProjection(n_components=i,random_state=0)\n    random_projection.fit(anu_features_norm)\n    components =  random_projection.components_.toarray() # shape=(5, 11) \n    p_inverse = np.linalg.pinv(components.T) # shape=(5, 11) \n    #now get the transformed data using the projection components\n    reduced_data = random_projection.transform(anu_features_norm) #shape=(4898, 5) \n    reconstructed= reduced_data.dot(p_inverse)  #shape=(4898, 11) \n    error = metrics.mean_squared_error(anu_features_norm, reconstructed)\n    losses.append(error)\nplt.plot(range(1,23),losses,marker='o')\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.xlabel(\"# Components\", fontsize=18)\nplt.ylabel(\"Re-construction error\", fontsize=18)","7e33ce30":"losses1=[]\nlosses2=[]\nlosses3=[]\nlosses4=[]\nfor i in range(1,23):\n    random_projection = SparseRandomProjection(n_components=i,random_state=np.random)\n    random_projection.fit(anu_features_norm)\n    components =  random_projection.components_.toarray() # shape=(5, 11) \n    p_inverse = np.linalg.pinv(components.T) # shape=(5, 11) \n    #now get the transformed data using the projection components\n    reduced_data = random_projection.transform(anu_features_norm) #shape=(4898, 5) \n    reconstructed= reduced_data.dot(p_inverse)  #shape=(4898, 11) \n    error = metrics.mean_squared_error(anu_features_norm, reconstructed)\n    losses1.append(error)\nfor i in range(1,23):\n    random_projection = SparseRandomProjection(n_components=i,random_state=np.random)\n    random_projection.fit(anu_features_norm)\n    components =  random_projection.components_.toarray() # shape=(5, 11) \n    p_inverse = np.linalg.pinv(components.T) # shape=(5, 11) \n    #now get the transformed data using the projection components\n    reduced_data = random_projection.transform(anu_features_norm) #shape=(4898, 5) \n    reconstructed= reduced_data.dot(p_inverse)  #shape=(4898, 11) \n    error = metrics.mean_squared_error(anu_features_norm, reconstructed)\n    losses2.append(error)\nfor i in range(1,23):\n    random_projection = SparseRandomProjection(n_components=i,random_state=np.random)\n    random_projection.fit(anu_features_norm)\n    components =  random_projection.components_.toarray() # shape=(5, 11) \n    p_inverse = np.linalg.pinv(components.T) # shape=(5, 11) \n    #now get the transformed data using the projection components\n    reduced_data = random_projection.transform(anu_features_norm) #shape=(4898, 5) \n    reconstructed= reduced_data.dot(p_inverse)  #shape=(4898, 11) \n    error = metrics.mean_squared_error(anu_features_norm, reconstructed)\n    losses3.append(error)\nfor i in range(1,23):\n    random_projection = SparseRandomProjection(n_components=i,random_state=np.random)\n    random_projection.fit(anu_features_norm)\n    components =  random_projection.components_.toarray() # shape=(5, 11) \n    p_inverse = np.linalg.pinv(components.T) # shape=(5, 11) \n    #now get the transformed data using the projection components\n    reduced_data = random_projection.transform(anu_features_norm) #shape=(4898, 5) \n    reconstructed= reduced_data.dot(p_inverse)  #shape=(4898, 11) \n    error = metrics.mean_squared_error(anu_features_norm, reconstructed)\n    losses4.append(error)\n    \nplt.plot(range(1,23),losses,marker='o')\nplt.plot(range(1,23),losses1,marker='o')\nplt.plot(range(1,23),losses2,marker='o')\nplt.plot(range(1,23),losses3,marker='o')\nplt.plot(range(1,23),losses4,marker='o')\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.xlabel(\"# Components\", fontsize=18)\nplt.ylabel(\"Re-construction error\", fontsize=18)","3625e7b3":"losses=[]\nspars=[]\nfor i in range(1,23):\n    dict_learner = DictionaryLearning(n_components=i, transform_algorithm='lasso_cd',fit_algorithm='cd',max_iter=10, random_state=42,transform_max_iter=10)\n    X_train_pca = dict_learner.fit_transform(anu_features_norm)\n    X_hat = X_train_pca @ dict_learner.components_\n    loss = np.mean(np.sum((X_hat - anu_features_norm) ** 2, axis=1) \/ np.sum(anu_features_norm ** 2, axis=1))\n    losses.append(loss)\n    spars.append(np.mean(X_train_pca == 0))","1a09b12c":"fig, axs = plt.subplots(1,2)\n  \n\naxs[0].plot(range(1,23),losses,marker='o')\naxs[0].set(xlabel='# Components', ylabel='Re-construction error')\n\naxs[1].plot(range(1,23),spars,marker='o')\naxs[1].set(xlabel='# Components', ylabel='Level of sparsity')\n\n\nfig=plt.gcf()\nfig.set_size_inches(16,7)","5516fe0b":"#Bank\n\n# PCA : 8\n# ICA : 15\n# RCA: 16\n# Dictionarylearning :10\n    \n#Bank\n\n# PCA : 4\n# ICA : 8\n# RCA: 10\n# Dictionarylearning :6","a1713eb0":"pca = PCA(n_components=8)\npca.fit(bank2X)\nbank2x_pca = pd.DataFrame(pca.transform(bank2X))\n\ntransformer = FastICA(n_components=15,random_state=0)\nbank2x_ica = pd.DataFrame(transformer.fit_transform(bank2X))\n\nrandom_projection = SparseRandomProjection(n_components=16)\nrandom_projection.fit(bank2X)\nbank2x_rca = pd.DataFrame(random_projection.fit_transform(bank2X))\n\ndict_learner = DictionaryLearning(n_components=10, transform_algorithm='lasso_cd',fit_algorithm='cd',max_iter=10, random_state=42,transform_max_iter=10)\nbank2x_dict= pd.DataFrame(dict_learner.fit_transform(bank2X))\n\nbank2X_dr=[bank2x_pca,bank2x_ica,bank2x_rca,bank2x_dict]\n","cc79ac28":"DRs=['PCA','ICA','RP','SDL'] \nSC=[];CHS=[];DBS=[];hom=[];com=[];vm=[]\n\nfor j in range(0,4):\n    print(DRs[j])\n    bank2X=bank2X_dr[j]\n    SC=[];CHS=[];DBS=[];hom=[];com=[];vm=[]\n\n    for i in range(2,50):\n        km = KMeans(n_clusters=i, init='k-means++', n_init=10, random_state=42).fit(bank2X)\n        SC.append(sklearn.metrics.silhouette_score(bank2X, km.predict(bank2X), metric='euclidean', sample_size=None, random_state=42))\n        CHS.append(sklearn.metrics.calinski_harabasz_score(bank2X, km.predict(bank2X)))\n        DBS.append(sklearn.metrics.davies_bouldin_score(bank2X, km.predict(bank2X)))\n        hom.append(metrics.homogeneity_score(bank2['deposit'], km.predict(bank2X)))\n        com.append(metrics.completeness_score(bank2['deposit'], km.predict(bank2X)))\n        vm.append(metrics.v_measure_score(bank2['deposit'], km.predict(bank2X)))\n\n    fig, axs = plt.subplots(3,2)\n    axs[0,0].plot(range(2,50),SC,marker='o')\n    axs[0,0].set( ylabel='Silhouette Score')\n    axs[1,0].plot(range(2,50),CHS,marker='o')\n    axs[1,0].set( ylabel='Calinski-Harabasz Index')\n    axs[2,0].plot(range(2,50),DBS,marker='o')\n    axs[2,0].set(xlabel='Number of Clusters', ylabel='Davies-Bouldin Index')\n    axs[0,1].plot(range(2,50),hom,marker='o')\n    axs[0,1].set( ylabel='Homogeneity')\n    axs[1,1].plot(range(2,50),com,marker='o')\n    axs[1,1].set( ylabel='Completeness')\n    axs[2,1].plot(range(2,50),vm,marker='o')\n    axs[2,1].set(xlabel='Number of Clusters', ylabel='V-Measure')\n    fig=plt.gcf()\n    fig.set_size_inches(15,15)\n    fig.suptitle('Clustering using K-Means after dimensionality reduction using '+ DRs[j])","ff599bcc":"DRs=['PCA','ICA','RP','SDL'] \nSC=[];CHS=[];DBS=[];hom=[];com=[];vm=[]\n\nfor j in range(0,4):\n    print(DRs[j])\n    bank2X=bank2X_dr[j]\n    SC=[];CHS=[];DBS=[];hom=[];com=[];vm=[]\n\n    for i in range(2,50):\n        km = GaussianMixture(n_components=i).fit(bank2X)\n        SC.append(sklearn.metrics.silhouette_score(bank2X, km.predict(bank2X), metric='euclidean', sample_size=None, random_state=42))\n        CHS.append(sklearn.metrics.calinski_harabasz_score(bank2X, km.predict(bank2X)))\n        DBS.append(sklearn.metrics.davies_bouldin_score(bank2X, km.predict(bank2X)))\n        hom.append(metrics.homogeneity_score(bank2['deposit'], km.predict(bank2X)))\n        com.append(metrics.completeness_score(bank2['deposit'], km.predict(bank2X)))\n        vm.append(metrics.v_measure_score(bank2['deposit'], km.predict(bank2X)))\n\n    fig, axs = plt.subplots(3,2)\n    axs[0,0].plot(range(2,50),SC,marker='o')\n    axs[0,0].set( ylabel='Silhouette Score')\n    axs[1,0].plot(range(2,50),CHS,marker='o')\n    axs[1,0].set( ylabel='Calinski-Harabasz Index')\n    axs[2,0].plot(range(2,50),DBS,marker='o')\n    axs[2,0].set(xlabel='Number of Clusters', ylabel='Davies-Bouldin Index')\n    axs[0,1].plot(range(2,50),hom,marker='o')\n    axs[0,1].set( ylabel='Homogeneity')\n    axs[1,1].plot(range(2,50),com,marker='o')\n    axs[1,1].set( ylabel='Completeness')\n    axs[2,1].plot(range(2,50),vm,marker='o')\n    axs[2,1].set(xlabel='Number of Clusters', ylabel='V-Measure')\n    fig=plt.gcf()\n    fig.set_size_inches(15,15)\n    fig.suptitle('Clustering using GMM after dimensionality reduction using '+ DRs[j])","8391d31a":"pca = PCA(n_components=4)\npca.fit(anu_features_norm)\nanu_features_norm_pca = pd.DataFrame(pca.transform(anu_features_norm))\n\ntransformer = FastICA(n_components=8,random_state=0)\nanu_features_norm_ica = pd.DataFrame(transformer.fit_transform(anu_features_norm))\n\nrandom_projection = SparseRandomProjection(n_components=10)\nrandom_projection.fit(anu_features_norm)\nanu_features_norm_rca = pd.DataFrame(random_projection.fit_transform(anu_features_norm))\n\ndict_learner = DictionaryLearning(n_components=6, transform_algorithm='lasso_cd',fit_algorithm='cd',max_iter=10, random_state=42,transform_max_iter=10)\nanu_features_norm_dict= pd.DataFrame(dict_learner.fit_transform(anu_features_norm))\n\nanu_features_dr =[anu_features_norm_pca,anu_features_norm_ica,anu_features_norm_ica,anu_features_norm_dict]","961b46bc":"DRs=['PCA','ICA','RP','SDL'] \nfor j in range(0,4):\n    print(DRs[j])\n    anu_features_norm=anu_features_dr[j]\n    SC=[];CHS=[];DBS=[];hom=[];com=[];vm=[]\n\n    for i in range(2,50):\n        km = GaussianMixture(n_components=i).fit(anu_features_norm)\n        SC.append(sklearn.metrics.silhouette_score(anu_features_norm, km.predict(anu_features_norm), metric='euclidean', sample_size=None, random_state=42))\n        CHS.append(sklearn.metrics.calinski_harabasz_score(anu_features_norm, km.predict(anu_features_norm)))\n        DBS.append(sklearn.metrics.davies_bouldin_score(anu_features_norm, km.predict(anu_features_norm)))\n        hom.append([metrics.homogeneity_score(anu['Family'], km.predict(anu_features_norm)),metrics.homogeneity_score(anu['Genus'], km.predict(anu_features_norm)),metrics.homogeneity_score(anu['Species'], km.predict(anu_features_norm))])\n        com.append([metrics.completeness_score(anu['Family'], km.predict(anu_features_norm)),metrics.completeness_score(anu['Genus'], km.predict(anu_features_norm)),metrics.completeness_score(anu['Species'], km.predict(anu_features_norm))])\n        vm.append([metrics.v_measure_score(anu['Family'], km.predict(anu_features_norm)),metrics.v_measure_score(anu['Genus'], km.predict(anu_features_norm)),metrics.v_measure_score(anu['Species'], km.predict(anu_features_norm))])\n\n    fig, axs = plt.subplots(3,2)\n    axs[0,0].plot(range(2,50),SC,marker='o')\n    axs[0,0].set( ylabel='Silhouette Score')\n    axs[1,0].plot(range(2,50),CHS,marker='o')\n    axs[1,0].set( ylabel='Calinski-Harabasz Index')\n    axs[2,0].plot(range(2,50),DBS,marker='o')\n    axs[2,0].set(xlabel='Number of Clusters', ylabel='Davies-Bouldin Index')\n    axs[0,1].plot(range(2,50),hom,marker='o')\n    axs[0,1].set( ylabel='Homogeneity')\n    axs[0,1].legend(['Family','Genus','Species'])\n    axs[1,1].plot(range(2,50),com,marker='o')\n    axs[1,1].set( ylabel='Completeness')\n    axs[1,1].legend(['Family','Genus','Species'])\n    axs[2,1].plot(range(2,50),vm,marker='o')\n    axs[2,1].set(xlabel='Number of Clusters', ylabel='V-Measure')\n    axs[2,1].legend(['Family','Genus','Species'])\n    fig=plt.gcf()\n    fig.set_size_inches(15,15)\n    fig.suptitle('Clustering using GMM after dimensionality reduction using '+ DRs[j])","73c37fa0":"DRs=['PCA','ICA','RP','SDL'] \nfor j in range(0,4):\n    print(DRs[j])\n    anu_features_norm=anu_features_dr[j]\n    SC=[];CHS=[];DBS=[];hom=[];com=[];vm=[]\n\n    for i in range(2,50):\n        km = KMeans(n_clusters=i, init='k-means++', n_init=10, random_state=42).fit(anu_features_norm)\n        SC.append(sklearn.metrics.silhouette_score(anu_features_norm, km.predict(anu_features_norm), metric='euclidean', sample_size=None, random_state=42))\n        CHS.append(sklearn.metrics.calinski_harabasz_score(anu_features_norm, km.predict(anu_features_norm)))\n        DBS.append(sklearn.metrics.davies_bouldin_score(anu_features_norm, km.predict(anu_features_norm)))\n        hom.append([metrics.homogeneity_score(anu['Family'], km.predict(anu_features_norm)),metrics.homogeneity_score(anu['Genus'], km.predict(anu_features_norm)),metrics.homogeneity_score(anu['Species'], km.predict(anu_features_norm))])\n        com.append([metrics.completeness_score(anu['Family'], km.predict(anu_features_norm)),metrics.completeness_score(anu['Genus'], km.predict(anu_features_norm)),metrics.completeness_score(anu['Species'], km.predict(anu_features_norm))])\n        vm.append([metrics.v_measure_score(anu['Family'], km.predict(anu_features_norm)),metrics.v_measure_score(anu['Genus'], km.predict(anu_features_norm)),metrics.v_measure_score(anu['Species'], km.predict(anu_features_norm))])\n\n    fig, axs = plt.subplots(3,2)\n    axs[0,0].plot(range(2,50),SC,marker='o')\n    axs[0,0].set( ylabel='Silhouette Score')\n    axs[1,0].plot(range(2,50),CHS,marker='o')\n    axs[1,0].set( ylabel='Calinski-Harabasz Index')\n    axs[2,0].plot(range(2,50),DBS,marker='o')\n    axs[2,0].set(xlabel='Number of Clusters', ylabel='Davies-Bouldin Index')\n    axs[0,1].plot(range(2,50),hom,marker='o')\n    axs[0,1].set( ylabel='Homogeneity')\n    axs[0,1].legend(['Family','Genus','Species'])\n    axs[1,1].plot(range(2,50),com,marker='o')\n    axs[1,1].set( ylabel='Completeness')\n    axs[1,1].legend(['Family','Genus','Species'])\n    axs[2,1].plot(range(2,50),vm,marker='o')\n    axs[2,1].set(xlabel='Number of Clusters', ylabel='V-Measure')\n    axs[2,1].legend(['Family','Genus','Species'])\n    fig=plt.gcf()\n    fig.set_size_inches(15,15)\n    fig.suptitle('Clustering using K-means after dimensionality reduction using '+ DRs[j])","4206cd2c":"train_norm, test_norm = train_test_split(bank2_norm, test_size = 0.3,random_state=0)\nX_train_NN = torch.tensor(train_norm.drop('deposit',axis=1).values).float()\ny_train_NN = torch.tensor(train_norm['deposit'].values).long()\nX_test_NN = torch.tensor(test_norm.drop('deposit',axis=1).values).float()\ny_test_NN = torch.tensor(test_norm['deposit'].values).long()\nclass bankNN(nn.Module):\n    def __init__(self):\n        super(bankNN, self).__init__()\n        self.fc1 = nn.Linear(42, 500)\n        self.fc2 = nn.Linear(500, 350)\n        self.fc3 = nn.Linear(350, 200)\n        self.fc5 = nn.Linear(200, 100)\n        self.fc6 = nn.Linear(100, 20)\n        self.fc4 = nn.Linear(20, 2)\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.relu(self.fc5(x))\n        x = F.relu(self.fc6(x))\n        return self.fc4(x)\nnet = bankNN()\noptimizer = optim.SGD(net.parameters(),lr=0.3,momentum=0.1)\ncriterion = nn.CrossEntropyLoss()\nlosses = []\nstart_time=time.time()\nfor epoch in range(1, 500):\n    optimizer.zero_grad()\n    outputs = net(X_train_NN)\n    loss = criterion(outputs, y_train_NN)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\nprint('Run Time:', time.time()-start_time)\nplt.plot(losses)\nplt.legend()\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.xlabel(\"Epoch\", fontsize=18)\nplt.ylabel(\"Loss\", fontsize=18)\nprint(\"Training Accuracy:\",get_accuracy(X_train_NN,y_train_NN))\nprint(\"Testing Accuracy:\",get_accuracy(X_test_NN,y_test_NN))","4e7f2d6c":"bank2X_dr=[bank2x_pca,bank2x_ica,bank2x_rca,bank2x_dict]\nbank2_norm_dr=bank2X_dr[0].copy()\nbank2_norm_dr['deposit']=bank2_norm['deposit']\nclass bankNN(nn.Module):\n    def __init__(self):\n        super(bankNN, self).__init__()\n        self.fc1 = nn.Linear(bank2_norm_dr.shape[1]-1, 500)\n        self.fc2 = nn.Linear(500, 350)\n        self.fc3 = nn.Linear(350, 200)\n        self.fc5 = nn.Linear(200, 100)\n        self.fc6 = nn.Linear(100, 20)\n        self.fc4 = nn.Linear(20, 2)\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.relu(self.fc5(x))\n        x = F.relu(self.fc6(x))\n        return self.fc4(x)\ntrain_norm, test_norm = train_test_split(bank2_norm_dr, test_size = 0.3,random_state=0)\nX_train_NN = torch.tensor(train_norm.drop('deposit',axis=1).values).float()\ny_train_NN = torch.tensor(train_norm['deposit'].values).long()\nX_test_NN = torch.tensor(test_norm.drop('deposit',axis=1).values).float()\ny_test_NN = torch.tensor(test_norm['deposit'].values).long()\nnet = bankNN()\noptimizer = optim.SGD(net.parameters(),lr=0.3,momentum=0.1)\ncriterion = nn.CrossEntropyLoss()\nlosses = []\nstart_time=time.time()\nfor epoch in range(1, 500):\n    optimizer.zero_grad()\n    outputs = net(X_train_NN)\n    loss = criterion(outputs, y_train_NN)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\nprint('Run Time:', time.time()-start_time)\nplt.plot(losses)\nplt.legend()\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.xlabel(\"Epoch\", fontsize=18)\nplt.ylabel(\"Loss\", fontsize=18)\nprint(\"Training Accuracy:\",get_accuracy(X_train_NN,y_train_NN))\nprint(\"Testing Accuracy:\",get_accuracy(X_test_NN,y_test_NN))","e2f47fd9":"bank2X_dr=[bank2x_pca,bank2x_ica,bank2x_rca,bank2x_dict]\nbank2_norm_dr=bank2X_dr[1].copy()\nbank2_norm_dr['deposit']=bank2_norm['deposit']\nclass bankNN(nn.Module):\n    def __init__(self):\n        super(bankNN, self).__init__()\n        self.fc1 = nn.Linear(bank2_norm_dr.shape[1]-1, 500)\n        self.fc2 = nn.Linear(500, 350)\n        self.fc3 = nn.Linear(350, 200)\n        self.fc5 = nn.Linear(200, 100)\n        self.fc6 = nn.Linear(100, 20)\n        self.fc4 = nn.Linear(20, 2)\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.relu(self.fc5(x))\n        x = F.relu(self.fc6(x))\n        return self.fc4(x)\ntrain_norm, test_norm = train_test_split(bank2_norm_dr, test_size = 0.3,random_state=0)\nX_train_NN = torch.tensor(train_norm.drop('deposit',axis=1).values).float()\ny_train_NN = torch.tensor(train_norm['deposit'].values).long()\nX_test_NN = torch.tensor(test_norm.drop('deposit',axis=1).values).float()\ny_test_NN = torch.tensor(test_norm['deposit'].values).long()\nnet = bankNN()\noptimizer = optim.SGD(net.parameters(),lr=0.3,momentum=0.1)\ncriterion = nn.CrossEntropyLoss()\nlosses = []\nstart_time=time.time()\nfor epoch in range(1, 500):\n    optimizer.zero_grad()\n    outputs = net(X_train_NN)\n    loss = criterion(outputs, y_train_NN)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\nprint('Run Time:', time.time()-start_time)\nplt.plot(losses)\nplt.legend()\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.xlabel(\"Epoch\", fontsize=18)\nplt.ylabel(\"Loss\", fontsize=18)\nprint(\"Training Accuracy:\",get_accuracy(X_train_NN,y_train_NN))\nprint(\"Testing Accuracy:\",get_accuracy(X_test_NN,y_test_NN))","2d9486a2":"bank2X_dr=[bank2x_pca,bank2x_ica,bank2x_rca,bank2x_dict]\nbank2_norm_dr=bank2X_dr[2].copy()\nbank2_norm_dr['deposit']=bank2_norm['deposit']\nclass bankNN(nn.Module):\n    def __init__(self):\n        super(bankNN, self).__init__()\n        self.fc1 = nn.Linear(bank2_norm_dr.shape[1]-1, 500)\n        self.fc2 = nn.Linear(500, 350)\n        self.fc3 = nn.Linear(350, 200)\n        self.fc5 = nn.Linear(200, 100)\n        self.fc6 = nn.Linear(100, 20)\n        self.fc4 = nn.Linear(20, 2)\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.relu(self.fc5(x))\n        x = F.relu(self.fc6(x))\n        return self.fc4(x)\ntrain_norm, test_norm = train_test_split(bank2_norm_dr, test_size = 0.3,random_state=0)\nX_train_NN = torch.tensor(train_norm.drop('deposit',axis=1).values).float()\ny_train_NN = torch.tensor(train_norm['deposit'].values).long()\nX_test_NN = torch.tensor(test_norm.drop('deposit',axis=1).values).float()\ny_test_NN = torch.tensor(test_norm['deposit'].values).long()\nnet = bankNN()\noptimizer = optim.SGD(net.parameters(),lr=0.3,momentum=0.1)\ncriterion = nn.CrossEntropyLoss()\nlosses = []\nstart_time=time.time()\nfor epoch in range(1, 500):\n    optimizer.zero_grad()\n    outputs = net(X_train_NN)\n    loss = criterion(outputs, y_train_NN)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\nprint('Run Time:', time.time()-start_time)\nplt.plot(losses)\nplt.legend()\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.xlabel(\"Epoch\", fontsize=18)\nplt.ylabel(\"Loss\", fontsize=18)\nprint(\"Training Accuracy:\",get_accuracy(X_train_NN,y_train_NN))\nprint(\"Testing Accuracy:\",get_accuracy(X_test_NN,y_test_NN))","a50df04f":"bank2X_dr=[bank2x_pca,bank2x_ica,bank2x_rca,bank2x_dict]\nbank2_norm_dr=bank2X_dr[3].copy()\nbank2_norm_dr['deposit']=bank2_norm['deposit']\nclass bankNN(nn.Module):\n    def __init__(self):\n        super(bankNN, self).__init__()\n        self.fc1 = nn.Linear(bank2_norm_dr.shape[1]-1, 500)\n        self.fc2 = nn.Linear(500, 350)\n        self.fc3 = nn.Linear(350, 200)\n        self.fc5 = nn.Linear(200, 100)\n        self.fc6 = nn.Linear(100, 20)\n        self.fc4 = nn.Linear(20, 2)\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.relu(self.fc5(x))\n        x = F.relu(self.fc6(x))\n        return self.fc4(x)\ntrain_norm, test_norm = train_test_split(bank2_norm_dr, test_size = 0.3,random_state=0)\nX_train_NN = torch.tensor(train_norm.drop('deposit',axis=1).values).float()\ny_train_NN = torch.tensor(train_norm['deposit'].values).long()\nX_test_NN = torch.tensor(test_norm.drop('deposit',axis=1).values).float()\ny_test_NN = torch.tensor(test_norm['deposit'].values).long()\nnet = bankNN()\noptimizer = optim.SGD(net.parameters(),lr=0.3,momentum=0.1)\ncriterion = nn.CrossEntropyLoss()\nlosses = []\nstart_time=time.time()\nfor epoch in range(1, 500):\n    optimizer.zero_grad()\n    outputs = net(X_train_NN)\n    loss = criterion(outputs, y_train_NN)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\nprint('Run Time:', time.time()-start_time)\nplt.plot(losses)\nplt.legend()\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.xlabel(\"Epoch\", fontsize=18)\nplt.ylabel(\"Loss\", fontsize=18)\nprint(\"Training Accuracy:\",get_accuracy(X_train_NN,y_train_NN))\nprint(\"Testing Accuracy:\",get_accuracy(X_test_NN,y_test_NN))","36770fd5":"train_norm, test_norm = train_test_split(bank2_norm, test_size = 0.3,random_state=0)\nX_train_NN = torch.tensor(train_norm.drop('deposit',axis=1).values).float()\ny_train_NN = torch.tensor(train_norm['deposit'].values).long()\nX_test_NN = torch.tensor(test_norm.drop('deposit',axis=1).values).float()\ny_test_NN = torch.tensor(test_norm['deposit'].values).long()\nclass bankNN(nn.Module):\n    def __init__(self):\n        super(bankNN, self).__init__()\n        self.fc1 = nn.Linear(42, 500)\n        self.fc2 = nn.Linear(500, 350)\n        self.fc3 = nn.Linear(350, 200)\n        self.fc5 = nn.Linear(200, 100)\n        self.fc6 = nn.Linear(100, 20)\n        self.fc4 = nn.Linear(20, 2)\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.relu(self.fc5(x))\n        x = F.relu(self.fc6(x))\n        return self.fc4(x)\nnet = bankNN()\noptimizer = optim.SGD(net.parameters(),lr=0.3,momentum=0.1)\ncriterion = nn.CrossEntropyLoss()\nlosses = []\nstart_time=time.time()\nfor epoch in range(1, 500):\n    optimizer.zero_grad()\n    outputs = net(X_train_NN)\n    loss = criterion(outputs, y_train_NN)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\nprint('Run Time:', time.time()-start_time)\nplt.plot(losses)\nplt.legend()\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.xlabel(\"Epoch\", fontsize=18)\nplt.ylabel(\"Loss\", fontsize=18)\nprint(\"Training Accuracy:\",get_accuracy(X_train_NN,y_train_NN))\nprint(\"Testing Accuracy:\",get_accuracy(X_test_NN,y_test_NN))","48307825":"bank2x_pca_c=bank2x_pca.copy()\nbank2x_pca_c['cluster']= KMeans(n_clusters=15, init='k-means++', n_init=10, random_state=42).fit(bank2x_pca).labels_\nbank2x_ica_c=bank2x_ica.copy()\nbank2x_ica_c['cluster']= KMeans(n_clusters=12, init='k-means++', n_init=10, random_state=42).fit(bank2x_ica).labels_\nbank2x_rca_c=bank2x_rca.copy()\nbank2x_rca_c['cluster']= KMeans(n_clusters=4, init='k-means++', n_init=10, random_state=42).fit(bank2x_rca).labels_\nbank2x_dict_c=bank2x_dict.copy()\nbank2x_dict_c['cluster']= KMeans(n_clusters=10, init='k-means++', n_init=10, random_state=42).fit(bank2x_dict).labels_\n    \n    \nbank2X_dr=[bank2x_pca_c,bank2x_ica_c,bank2x_rca_c,bank2x_dict_c]\n","89eb75aa":"bank2_norm_dr=bank2X_dr[0].copy()\nbank2_norm_dr['deposit']=bank2_norm['deposit']\nclass bankNN(nn.Module):\n    def __init__(self):\n        super(bankNN, self).__init__()\n        self.fc1 = nn.Linear(bank2_norm_dr.shape[1]-1, 500)\n        self.fc2 = nn.Linear(500, 350)\n        self.fc3 = nn.Linear(350, 200)\n        self.fc5 = nn.Linear(200, 100)\n        self.fc6 = nn.Linear(100, 20)\n        self.fc4 = nn.Linear(20, 2)\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.relu(self.fc5(x))\n        x = F.relu(self.fc6(x))\n        return self.fc4(x)\ntrain_norm, test_norm = train_test_split(bank2_norm_dr, test_size = 0.3,random_state=0)\nX_train_NN = torch.tensor(train_norm.drop('deposit',axis=1).values).float()\ny_train_NN = torch.tensor(train_norm['deposit'].values).long()\nX_test_NN = torch.tensor(test_norm.drop('deposit',axis=1).values).float()\ny_test_NN = torch.tensor(test_norm['deposit'].values).long()\nnet = bankNN()\noptimizer = optim.SGD(net.parameters(),lr=0.3,momentum=0.1)\ncriterion = nn.CrossEntropyLoss()\nlosses = []\nstart_time=time.time()\nfor epoch in range(1, 500):\n    optimizer.zero_grad()\n    outputs = net(X_train_NN)\n    loss = criterion(outputs, y_train_NN)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\nprint('Run Time:', time.time()-start_time)\nplt.plot(losses)\nplt.legend()\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.xlabel(\"Epoch\", fontsize=18)\nplt.ylabel(\"Loss\", fontsize=18)\nprint(\"Training Accuracy:\",get_accuracy(X_train_NN,y_train_NN))\nprint(\"Testing Accuracy:\",get_accuracy(X_test_NN,y_test_NN))","095efa7e":"bank2_norm_dr=bank2X_dr[1].copy()\nbank2_norm_dr['deposit']=bank2_norm['deposit']\nclass bankNN(nn.Module):\n    def __init__(self):\n        super(bankNN, self).__init__()\n        self.fc1 = nn.Linear(bank2_norm_dr.shape[1]-1, 500)\n        self.fc2 = nn.Linear(500, 350)\n        self.fc3 = nn.Linear(350, 200)\n        self.fc5 = nn.Linear(200, 100)\n        self.fc6 = nn.Linear(100, 20)\n        self.fc4 = nn.Linear(20, 2)\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.relu(self.fc5(x))\n        x = F.relu(self.fc6(x))\n        return self.fc4(x)\ntrain_norm, test_norm = train_test_split(bank2_norm_dr, test_size = 0.3,random_state=0)\nX_train_NN = torch.tensor(train_norm.drop('deposit',axis=1).values).float()\ny_train_NN = torch.tensor(train_norm['deposit'].values).long()\nX_test_NN = torch.tensor(test_norm.drop('deposit',axis=1).values).float()\ny_test_NN = torch.tensor(test_norm['deposit'].values).long()\nnet = bankNN()\noptimizer = optim.SGD(net.parameters(),lr=0.3,momentum=0.1)\ncriterion = nn.CrossEntropyLoss()\nlosses = []\nstart_time=time.time()\nfor epoch in range(1, 500):\n    optimizer.zero_grad()\n    outputs = net(X_train_NN)\n    loss = criterion(outputs, y_train_NN)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\nprint('Run Time:', time.time()-start_time)\nplt.plot(losses)\nplt.legend()\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.xlabel(\"Epoch\", fontsize=18)\nplt.ylabel(\"Loss\", fontsize=18)\nprint(\"Training Accuracy:\",get_accuracy(X_train_NN,y_train_NN))\nprint(\"Testing Accuracy:\",get_accuracy(X_test_NN,y_test_NN))","abf2c8cf":"bank2_norm_dr=bank2X_dr[2].copy()\nbank2_norm_dr['deposit']=bank2_norm['deposit']\nclass bankNN(nn.Module):\n    def __init__(self):\n        super(bankNN, self).__init__()\n        self.fc1 = nn.Linear(bank2_norm_dr.shape[1]-1, 500)\n        self.fc2 = nn.Linear(500, 350)\n        self.fc3 = nn.Linear(350, 200)\n        self.fc5 = nn.Linear(200, 100)\n        self.fc6 = nn.Linear(100, 20)\n        self.fc4 = nn.Linear(20, 2)\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.relu(self.fc5(x))\n        x = F.relu(self.fc6(x))\n        return self.fc4(x)\ntrain_norm, test_norm = train_test_split(bank2_norm_dr, test_size = 0.3,random_state=0)\nX_train_NN = torch.tensor(train_norm.drop('deposit',axis=1).values).float()\ny_train_NN = torch.tensor(train_norm['deposit'].values).long()\nX_test_NN = torch.tensor(test_norm.drop('deposit',axis=1).values).float()\ny_test_NN = torch.tensor(test_norm['deposit'].values).long()\nnet = bankNN()\noptimizer = optim.SGD(net.parameters(),lr=0.3,momentum=0.1)\ncriterion = nn.CrossEntropyLoss()\nlosses = []\nstart_time=time.time()\nfor epoch in range(1, 500):\n    optimizer.zero_grad()\n    outputs = net(X_train_NN)\n    loss = criterion(outputs, y_train_NN)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\nprint('Run Time:', time.time()-start_time)\nplt.plot(losses)\nplt.legend()\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.xlabel(\"Epoch\", fontsize=18)\nplt.ylabel(\"Loss\", fontsize=18)\nprint(\"Training Accuracy:\",get_accuracy(X_train_NN,y_train_NN))\nprint(\"Testing Accuracy:\",get_accuracy(X_test_NN,y_test_NN))","dd9db6ef":"bank2_norm_dr=bank2X_dr[3].copy()\nbank2_norm_dr['deposit']=bank2_norm['deposit']\nclass bankNN(nn.Module):\n    def __init__(self):\n        super(bankNN, self).__init__()\n        self.fc1 = nn.Linear(bank2_norm_dr.shape[1]-1, 500)\n        self.fc2 = nn.Linear(500, 350)\n        self.fc3 = nn.Linear(350, 200)\n        self.fc5 = nn.Linear(200, 100)\n        self.fc6 = nn.Linear(100, 20)\n        self.fc4 = nn.Linear(20, 2)\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.relu(self.fc5(x))\n        x = F.relu(self.fc6(x))\n        return self.fc4(x)\ntrain_norm, test_norm = train_test_split(bank2_norm_dr, test_size = 0.3,random_state=0)\nX_train_NN = torch.tensor(train_norm.drop('deposit',axis=1).values).float()\ny_train_NN = torch.tensor(train_norm['deposit'].values).long()\nX_test_NN = torch.tensor(test_norm.drop('deposit',axis=1).values).float()\ny_test_NN = torch.tensor(test_norm['deposit'].values).long()\nnet = bankNN()\noptimizer = optim.SGD(net.parameters(),lr=0.3,momentum=0.1)\ncriterion = nn.CrossEntropyLoss()\nlosses = []\nstart_time=time.time()\nfor epoch in range(1, 500):\n    optimizer.zero_grad()\n    outputs = net(X_train_NN)\n    loss = criterion(outputs, y_train_NN)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\nprint('Run Time:', time.time()-start_time)\nplt.plot(losses)\nplt.legend()\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.xlabel(\"Epoch\", fontsize=18)\nplt.ylabel(\"Loss\", fontsize=18)\nprint(\"Training Accuracy:\",get_accuracy(X_train_NN,y_train_NN))\nprint(\"Testing Accuracy:\",get_accuracy(X_test_NN,y_test_NN))","438f1abb":"## Randomized Projections ","7aa007c9":"## PCA ","9a7e03ca":"## ICA ","da468201":"## GMM ","01d02e01":"## Dimension Reduction on Bank Marketing dataset and then NN","7e73f3b3":"## GMM ","c52c76af":"## Randomized Projections ","7357f927":"## KNN ","17c20dde":"## Clustering after dimentionality reduction","2974c3ed":"## Dimension Reduction + Cluster information on Bank Marketing dataset and then NN","7b8fcc49":"## Anuran Cells","e1376ee8":"## KMeans","17dc3156":"## KMeans","7a3c9bfd":"## PCA ","0a00ce6f":"## ICA ","4bf66610":"## DictionaryLearning","161d9216":"## DictionaryLearning"}}