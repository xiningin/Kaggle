{"cell_type":{"cbc4d79b":"code","fbc6070c":"code","6099d5ad":"code","fe5511ed":"code","36dc44c2":"code","3c17b4d5":"code","137db18d":"code","d4c9c684":"code","11aac55f":"code","6b4f9308":"code","d5e69869":"code","34e65fcb":"code","b08303fe":"code","5e23d954":"code","2d5ea900":"code","e9a37f80":"code","d0f4dfd5":"code","693bec2a":"code","8d5786c7":"code","c307091f":"code","987d5a10":"code","04790caf":"code","a3995284":"markdown","3ce031a0":"markdown","0815edbd":"markdown","4b58f641":"markdown","2f98af2f":"markdown"},"source":{"cbc4d79b":"# set the matplotlib backend so figures can be saved in the background\n!pip install --upgrade imutils\n\nimport matplotlib\nmatplotlib.use(\"Agg\")\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Conv2D,MaxPooling2D\nfrom tensorflow.keras.layers import Conv2DTranspose\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import load_model\n#from google.colab.patches import cv2_imshow\nfrom imutils import build_montages\n#from google.colab import files\nimport numpy as np\nimport argparse\nimport pickle\nimport shutil\nimport os\nimport cv2","fbc6070c":"class ConvAutoencoder:\n\t@staticmethod\n\tdef build(width, height, depth, filters=(32, 64,128,256), latentDim=512):\n\t\t# initialize the input shape to be \"channels last\" along with\n\t\t# the channels dimension itself\n\t\t# channels dimension itself\n\t\tinputShape = (height, width, depth)\n\t\tchanDim = -1\n\n\t\t# define the input to the encoder\n\t\tinputs = Input(shape=inputShape)\n\t\tx = inputs\n\t\t# loop over the number of filters\n\t\tfor f in filters:\n\t\t\tx = Conv2D(f, (3, 3), strides=2, padding=\"same\")(x)\n\t\t\tx = LeakyReLU(alpha=0.2)(x)\n\t\t\t#x = MaxPooling2D(pool_size=(2, 2))\n\t\t\tx = BatchNormalization(axis=chanDim)(x)\n\n\t\t# flatten the network and then construct our latent vector\n\t\tvolumeSize = K.int_shape(x)\n\t\tx = Flatten()(x)\n\t\tlatent = Dense(latentDim, name=\"encoded\")(x)\n\n\t\t# start building the decoder model which will accept the\n\t\t# output of the encoder as its inputs\n\t\tx = Dense(np.prod(volumeSize[1:]))(latent)\n\t\tx = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n\n\t\t# loop over our number of filters again, but this time in\n\t\t# reverse order\n\t\tfor f in filters[::-1]:\n\t\t\tx = Conv2DTranspose(f, (3, 3), strides=2,padding=\"same\")(x)\n\t\t\tx = LeakyReLU(alpha=0.2)(x)\n\t\t\tx = BatchNormalization(axis=chanDim)(x)\n\t\t\t# apply a CONV_TRANSPOSE => RELU => BN operation\n\t\t\t\n\t\t# apply a single CONV_TRANSPOSE layer used to recover the\n\t\t# original depth of the image\n\t\tx = Conv2DTranspose(depth, (3, 3), padding=\"same\")(x)\n\t\toutputs = Activation(\"sigmoid\", name=\"decoded\")(x)\n\n\t\t# construct our autoencoder model\n\t\tautoencoder = Model(inputs, outputs, name=\"autoencoder\")\n\n\t\t# return the autoencoder model\n\t\treturn autoencoder","6099d5ad":"def visualize_predictions(decoded, gt, samples=4):\n\t# initialize our list of output images\n\toutputs = None\n\n\t# loop over our number of output samples\n\tfor i in range(0, samples):\n\t\t# grab the original image and reconstructed image\n\t\toriginal = (gt[i] * 255).astype(\"uint8\")\n\t\trecon = (decoded[i] * 255).astype(\"uint8\")\n\n\t\t# stack the original and reconstructed image side-by-side\n\t\toutput = np.hstack([original, recon])\n\n\t\t# if the outputs array is empty, initialize it as the current\n\t\t# side-by-side image display\n\t\tif outputs is None:\n\t\t\toutputs = output\n\n\t\t# otherwise, vertically stack the outputs\n\t\telse:\n\t\t\toutputs = np.vstack([outputs, output])\n\n\t# return the output images\n\treturn outputs\n","fe5511ed":"# construct the argument\nargs=[\"autoencoder.h5\",\"recon_vis.png\",\"plot.png\"]","36dc44c2":"img_height=512\nimg_width=512\n# load the image\nfolder_path='..\/input\/unsupervised-animal-dataset\/dataset\/'\nimages = []\nfiles =os.listdir(folder_path)","3c17b4d5":"i=0\nfor img in files[:1000]:\n    if img.find('jpg')!=-1:\n        if i % 250 == 0:\n            print(\"%d images to array\" % i)\n        i+=1\n        img = cv2.imread(folder_path+img)\n        img= cv2.cvtColor(np.float32(img), cv2.COLOR_BGR2GRAY)\n        #print(img.shape)\n        img = img_to_array(img)\n        img = np.expand_dims(img, axis=0)\n        images.append(img)\nimages = np.vstack(images)\ntrainX=images\nimages=[]","137db18d":"i=0\nimages=[]\nfor img in files[:100]:\n  if img.find('jpg')!=-1:\n    if i % 250 == 0:\n        print(\"%d images to array\" % i)\n    i+=1\n    img = cv2.imread(folder_path+img)\n    query = img_to_array(img)\n    img= cv2.cvtColor(np.float32(img), cv2.COLOR_RGB2GRAY)\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    images.append(img)\nimages = np.vstack(images)\ntestX=images\ntestX.shape\nimages=[]","d4c9c684":"trainX = trainX.astype(\"float32\") \/ 255.0\ntestX = testX.astype(\"float32\") \/ 255.0","11aac55f":"EPOCHS = 18\nINIT_LR = 1e-5\nBS = 15\n# construct our convolutional autoencoder\nprint(\"[INFO] building autoencoder...\")\nautoencoder = ConvAutoencoder.build(512,512,1)\nopt = Adam(lr=INIT_LR, decay=INIT_LR \/ EPOCHS)\nautoencoder.compile(loss=\"mse\", optimizer=opt)","6b4f9308":"# train the convolutional autoencoder\nH = autoencoder.fit(\n\ttrainX, trainX,\n\tvalidation_data=(testX, testX),\n\tepochs=EPOCHS,\n\tbatch_size=BS)\n\n# testing images, construct the visualization, and then save it\n# to disk","d5e69869":"print(\"[INFO] making predictions...\")\ndecoded = autoencoder.predict(testX)\nvis = visualize_predictions(decoded, testX)\ncv2.imwrite(args[1], vis)\n\n# construct a plot that plots and saves the training history\nN = np.arange(0, EPOCHS)\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(N, H.history[\"loss\"], label=\"train_loss\")\nplt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\nplt.title(\"Training Loss and Accuracy\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\/Accuracy\")\nplt.legend(loc=\"lower left\")\nplt.savefig(args[2])\n\n","34e65fcb":"# serialize the autoencoder model to disk\nprint(\"[INFO] saving autoencoder...\")\nautoencoder.save(args[0], save_format=\"h5\")","b08303fe":"from IPython.display import Image\nImage('.\/recon_vis.png')","5e23d954":"args=[\"\/content\/drive\/My Drive\/weights\/autoencoder.h5\",\"\/content\/index.pickle\"]","2d5ea900":"img_height=512\nimg_width=512\n# load the image\nfolder_path='\/content\/drive\/My Drive\/dataset\/'\nimages = []\ntrainX=[]\nfiles =os.listdir(folder_path)\ni=0\nfor img in files:\n    if img.find('jpg')!=-1:\n      if i % 250 == 0:\n        print(\"%d images to array\" % i)\n      i+=1\n      img = cv2.imread(folder_path+img)\n      img= cv2.cvtColor(np.float32(img), cv2.COLOR_BGR2GRAY)\n      #print(img.shape)\n      img = img_to_array(img)\n      img = np.expand_dims(img, axis=0)\n      images.append(img)\n      \nimages = np.vstack(images)\ntrainX=images\ntrainX = trainX.astype(\"float32\") \/ 255.0\nimages=[]","e9a37f80":"print(\"[INFO] loading autoencoder model...\")\nautoencoder = load_model(args[0])\n# create the encoder model which consists of *just* the encoder\n# portion of the autoencoder\nencoder = Model(inputs=autoencoder.input,\n\toutputs=autoencoder.get_layer(\"encoded\").output)\n# quantify the contents of our input images using the encoder\nprint(\"[INFO] encoding images...\")\nfeatures = encoder.predict(trainX)","d0f4dfd5":"indexes = list(range(0, trainX.shape[0]))\ndata = {\"indexes\": indexes, \"features\": features}\n# write the data dictionary to disk\nprint(\"[INFO] saving index...\")\nf = open(args[1], \"wb\")\nf.write(pickle.dumps(data))\nf.close()","693bec2a":"uploaded =files.upload()\nfor fn in uploaded.keys():\n  print(fn)\n  testX=cv2.imread(fn)\n  testX = cv2.cvtColor(np.float32(testX), cv2.COLOR_RGB2GRAY)\n  cv2_imshow(testX)\n  testX = testX.astype(\"float32\") \/ 255.0\n  testX = np.expand_dims(testX, axis=0)\n","8d5786c7":"testX.shape","c307091f":"def euclidean(a, b):\n\t# compute and return the euclidean distance between two vectors\n\treturn np.linalg.norm(a - b)\n\ndef perform_search(queryFeatures, index, maxResults=64):\n\t# initialize our list of results\n\tresults = []\n\n\t# loop over our index\n\tfor i in range(0, len(index[\"features\"])):\n\t\t# compute the euclidean distance between our query features\n\t\t# and the features for the current image in our index, then\n\t\t# update our results list with a 2-tuple consisting of the\n\t\t# computed distance and the index of the image\n\t\td = euclidean(queryFeatures, index[\"features\"][i])\n\t\tresults.append((d, i))\n\n\t# sort the results and grab the top ones\n\tresults = sorted(results)[:maxResults]\n\n\t# return the list of results\n\treturn results\nargs=[\"\/content\/drive\/My Drive\/weights\/autoencoder.h5\",\"\/content\/index.pickle\",1]\n# load the autoencoder model and index from disk\nprint(\"[INFO] loading autoencoder and index...\")\nautoencoder = load_model(args[0])\nindex = pickle.loads(open(args[1], \"rb\").read())\n\n# create the encoder model which consists of *just* the encoder\n# portion of the autoencoder\nencoder = Model(inputs=autoencoder.input,\n\toutputs=autoencoder.get_layer(\"encoded\").output)\n\n\n","987d5a10":"# quantify the contents of our input testing images using the encoder\nprint(\"[INFO] encoding testing images...\")\nfeatures = encoder.predict(testX)\n\n# randomly sample a set of testing query image indexes\nqueryIdxs = list(range(0, testX.shape[0]))\nqueryIdxs = np.random.choice(queryIdxs, size=args[2],\n\treplace=False)\n\nqueryIdxs","04790caf":"# loop over the testing indexes\nfor i in queryIdxs:\n\t# take the features for the current image, find all similar\n\t# images in our dataset, and then initialize our list of result\n\t# images\n\tqueryFeatures = features[i]\n\tresults = perform_search(queryFeatures, index, maxResults=20)\n\tprint(len(results))\n\timages = []\n\tfor (d, j) in results:\n\t\t# grab the result image, convert it back to the range\n\t\t# [0, 255], and then update the images list\n\t\timage = (trainX[j] * 255).astype(\"uint8\")\n\t\timage= cv2.cvtColor(np.float32(image), cv2.COLOR_GRAY2RGB)\n\t\t#image = np.dstack([image] * 3)\n\t\timages.append(image)\n\t# display the query image\n\tquery = (testX[i] * 255).astype(\"uint8\")\n\tcv2_imshow(query)\n\n\t# build a montage from the results and display it\n\tmontage = build_montages(images, (512, 512), (5,4))[0]\n\tcv2_imshow( montage)\n\tcv2.waitKey(0)\n","a3995284":"# Visualization of recontructing image","3ce031a0":"## Testing the Model","0815edbd":"## Importing libraries","4b58f641":"**This notebook is an implementation of Content-Based Image Retrieval (CBIR).**\n\n**Content-Based Image Retrieval (CBIR) consists of retrieving the most visually similar images to a given query image from a database of images.**","2f98af2f":"## Training"}}