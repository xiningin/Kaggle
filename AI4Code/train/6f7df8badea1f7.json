{"cell_type":{"b7561f00":"code","e162009d":"code","298ea87f":"code","ec0e89c8":"code","c34af381":"code","20c5612b":"code","33dcb640":"code","4c52469d":"code","7440dd4c":"code","640eb6bd":"code","80ec6353":"code","b0e90189":"code","ae01e442":"code","0d71634a":"code","b9d95011":"code","f095b2d0":"code","4672a125":"code","7526341b":"code","e63f4704":"code","65812406":"code","bc4159eb":"code","374173ae":"code","3232e156":"code","3f47a353":"code","50f5f3e4":"code","8e794564":"code","48d0ac0f":"code","cb00d46f":"code","ce862c15":"code","6d3b28de":"code","56b4849a":"code","522909d7":"code","47cc84d7":"code","b1e70c42":"code","e185d930":"code","bc31244e":"code","13eeea5b":"code","d4670931":"markdown","f042f6cc":"markdown","0d470139":"markdown","78e83d92":"markdown","f46c3270":"markdown","0953b29d":"markdown","c5107135":"markdown","7cd31984":"markdown","5e314371":"markdown","a9dbad67":"markdown","d41a544b":"markdown","2fc330e5":"markdown","9ddbab1e":"markdown","8e487a6b":"markdown","e44462f8":"markdown","47227c4f":"markdown","7f4e4a7e":"markdown","618c0805":"markdown","61f49c9b":"markdown","89470362":"markdown","46c0cb19":"markdown","d6869ec7":"markdown","38ce6b81":"markdown","1770912c":"markdown","b36b7781":"markdown","2ba29a5c":"markdown","40441a7f":"markdown","54623636":"markdown","d4d295a7":"markdown","29187c86":"markdown","7644a68f":"markdown","f71b2410":"markdown","8bbf525c":"markdown","f798ca95":"markdown","02aab579":"markdown","1b3f708e":"markdown","eb74ace4":"markdown","0aab9998":"markdown","3969c932":"markdown"},"source":{"b7561f00":"!pip install ftfy\n\n#Para o uso geral\nimport random\nimport numpy as np\nimport pandas as pd\nimport copy\nimport time\nfrom tqdm import tqdm as tqdm\nfrom scipy.stats import uniform\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport requests\nimport io\n\n#Para o processamento de textos\nfrom ftfy import fix_text\nimport string\nimport re\nfrom gensim.test.utils import common_texts\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\n\n#Para Machine Learning e NLP\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import plot_roc_curve\n\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential, regularizers\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.python.framework import ops\nfrom tensorflow.keras.callbacks import EarlyStopping","e162009d":"data = pd.read_csv(\"..\/input\/sentiment-analysis-pmr3508\/data_train.csv\")\ndata.head()","298ea87f":"data.loc[data.duplicated(subset='review', keep=False)==True].sort_values(by='review').head(10)","ec0e89c8":"data=data.drop_duplicates(subset='review', keep='first')\ndata.shape","c34af381":"X_train = data.loc[:,'review'].tolist()\ny_train = np.array(data.loc[:,'positive'].tolist())","20c5612b":"X_train[69]","33dcb640":"def clean(text):\n    txt=text.replace(\"<br \/>\",\" \") #retirando tags\n    txt=fix_text(txt) #consertando Mojibakes (Ver https:\/\/pypi.org\/project\/ftfy\/)\n    txt=txt.lower() #passando tudo para min\u00fasculo\n    txt=txt.translate(str.maketrans('', '', string.punctuation)) #retirando toda pontua\u00e7\u00e3o\n    txt=txt.replace(\" \u2014 \", \" \") #retirando h\u00edfens\n    txt=re.sub(\"\\d+\", ' <number> ', txt) #colocando um token especial para os n\u00fameros\n    txt=re.sub(' +', ' ', txt) #deletando espa\u00e7os extras\n    return txt","4c52469d":"X_train = [clean(x) for x in tqdm(X_train)]","7440dd4c":"X_train = [x.split() for x in X_train]","640eb6bd":"d2v = Doc2Vec.load('..\/input\/sentiment-analysis-pmr3508\/doc2vec')","80ec6353":"def emb(txt, model, normalize=False): \n    model.random.seed(42)\n    x=model.infer_vector(txt, steps=20)\n    \n    if normalize: return(x\/np.sqrt(x@x))\n    else: return(x)","b0e90189":"X_train = [emb(x, d2v) for x in tqdm(X_train)] \nX_train = np.array(X_train)","ae01e442":"X_train.shape, y_train.shape","0d71634a":"X_train[69]","b9d95011":"%%time\n\nmlp = MLPClassifier(early_stopping=True)\nparameters= {'hidden_layer_sizes': [x for x in range(10, 101, 10)], 'alpha':[0.001,0.01,0.1,1]}\n\nclf01 = GridSearchCV(mlp, parameters, n_jobs=4, verbose=1, scoring=\"roc_auc\")\nclf01.fit(X_train, y_train)","f095b2d0":"clf01.best_params_, clf01.best_score_","4672a125":"best_alpha = clf01.best_params_[\"alpha\"]\nbest_layers = clf01.best_params_[\"hidden_layer_sizes\"]\n\nmlp1 = MLPClassifier(early_stopping=True, alpha=0.01, hidden_layer_sizes=90)\nmlp1.fit(X_train, y_train)","7526341b":"%%time\n\nsizes = [(x,y) for x in range(40, 101, 10) for y in range(20, x, 10)]\n\nmlp = MLPClassifier(early_stopping=True)\nparameters= {'hidden_layer_sizes': sizes, 'alpha':[0.001,0.01,0.1]}\n\nclf02 = GridSearchCV(mlp, parameters, n_jobs=4, verbose=1, scoring=\"roc_auc\")\nclf02.fit(X_train, y_train)","e63f4704":"clf02.best_params_, clf02.best_score_","65812406":"best_alpha = clf02.best_params_[\"alpha\"]\nbest_layers = clf02.best_params_[\"hidden_layer_sizes\"]\n\nmlp2 = MLPClassifier(early_stopping=True, alpha=best_alpha, hidden_layer_sizes=best_layers)\nmlp2.fit(X_train, y_train)","bc4159eb":"n_features = X_train.shape[1]","374173ae":"n_iter=200\n\nneurons=[]\nreg=[]\n\n#Sorteando valores\nfor i in range(n_iter):\n    h1=random.randrange(25, 100, 5)\n    h2=random.randrange(20, h1, 5)\n    neurons.append((h1, h2))\n    \n    l1=random.choice([0, 1e-15, 1e-10, 1e-5, 1e-3, 1e-2, 1e-1])\n    l2=random.choice([0, 1e-15, 1e-10, 1e-5, 1e-3, 1e-2, 1e-1])\n    reg.append((l1, l2))\n    \n#DataFrame\nhyper = {'neurons': neurons, 'reg': reg, 'epochs': n_iter*[None], 'auc': n_iter*[None]}\nhyper = pd.DataFrame(hyper)\nhyper = hyper[['neurons', 'reg', 'epochs', 'auc']]","3232e156":"def create_model(neurons=(10,10), reg=(.001, .001)):\n    \n    ops.reset_default_graph() #\u00e9 importante resetar os grafos das redes neurais j\u00e1 criadas para n\u00e3o tornam o processo muito lento \n    \n    #Criando modelo\n    model = Sequential()\n    model.add(Dense(neurons[0], input_shape=(n_features,), activation='relu', \n                    kernel_regularizer=regularizers.l1_l2(l1=reg[0], l2=reg[1]), \n                    bias_regularizer=regularizers.l1_l2(l1=reg[0], l2=reg[1])))\n    model.add(Dense(neurons[1], activation='relu', \n                    kernel_regularizer=regularizers.l1_l2(l1=reg[0], l2=reg[1]), \n                    bias_regularizer=regularizers.l1_l2(l1=reg[0], l2=reg[1])))\n    model.add(Dense(1, activation='sigmoid', \n                    kernel_regularizer=regularizers.l1_l2(l1=reg[0], l2=reg[1]), \n                    bias_regularizer=regularizers.l1_l2(l1=reg[0], l2=reg[1])))\n    \n    #Compilando\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n    return model","3f47a353":"X_train_t, X_train_v, y_train_t, y_train_v = train_test_split(X_train,\n                                                            y_train,\n                                                            test_size=0.25,\n                                                            random_state=42)","50f5f3e4":"%%time\nfor j in tqdm(range(n_iter)):\n    model = create_model(hyper.loc[j,'neurons'], hyper.loc[j,'reg'])\n    \n    es = EarlyStopping(monitor='val_auc', patience=10)\n    \n    history = model.fit(X_train_t, y_train_t,\n                        epochs=50,\n                        validation_data=(X_train_v,y_train_v), \n                        batch_size=122, \n                        shuffle=True, \n                        verbose=False,\n                        callbacks=[es]) \n    \n    hyper.loc[j,'epochs'] = len(history.history['val_auc'])\n    hyper.loc[j,'auc'] = history.history['val_auc'][-1]\n","8e794564":"hyper = hyper.iloc[np.argsort(hyper.loc[:,'auc']),:]\nhyper.tail(10)","48d0ac0f":"#Melhores valores\nneurons = hyper.iloc[-1,0]\nreg = hyper.iloc[-1,1]\nepochs = hyper.iloc[-1,2]\n\n#Criando modelo\nmlp3 = create_model(neurons, reg)\n\n#Treinando modelo\nmlp3.fit(X_train, y_train, \n         validation_split=0,\n         epochs=epochs, \n         batch_size=122, \n         shuffle=True, \n         verbose=False) ","cb00d46f":"data = pd.read_csv(\"..\/input\/sentiment-analysis-pmr3508\/data_test1.csv\")\ndata.head()","ce862c15":"# Eliminando duplicatas\ndata=data.drop_duplicates(subset='review', keep='first')\n\n# Separando em listas\nX_val = data.loc[:,'review'].tolist()\ny_val = np.array(data.loc[:,'positive'].tolist())\n\n# Limpeza dos dados\nX_val = [clean(x) for x in tqdm(X_val)]\nX_val = [x.split() for x in X_val]","6d3b28de":"# Doc2Vec\nX_val = [emb(x, d2v) for x in tqdm(X_val)] \nX_val = np.array(X_val)","56b4849a":"aucs = []\naucs.append(roc_auc_score(y_val, mlp1.predict_proba(X_val)[:,1]))\naucs.append(roc_auc_score(y_val, mlp2.predict_proba(X_val)[:,1]))\naucs.append(roc_auc_score(y_val, mlp3.predict(X_val).squeeze()))\n\nprint('AUC --- SciKit 1 : {:.4f}'.format(aucs[0]))\nprint('AUC --- SciKit 2 : {:.4f}'.format(aucs[1]))\nprint('AUC --- TF\/Keras : {:.4f}'.format(aucs[2]))","522909d7":"plt.figure(figsize=(10,10))\n\nprobs1 = mlp1.predict_proba(X_val)[:,1]\nprobs2 = mlp2.predict_proba(X_val)[:,1]\nprobs3 = mlp3.predict(X_val).squeeze()\n\nfpr1 , tpr1, th1 = roc_curve (y_val , probs1)\nfpr2 , tpr2, th2 = roc_curve (y_val , probs2)\nfpr3 , tpr3, th3 = roc_curve (y_val , probs3)\n\nplt.plot(fpr1,tpr1, label = \"SciKit 1\")\nplt.plot(fpr2,tpr2, label = \"SciKit 2\") \nplt.plot(fpr3,tpr3, label = \"TF\/Keras\") \nplt.xlabel('False Positive Rate') \nplt.ylabel('True Positive Rate') \n\nplt.axis([0,1,0,1]) \nplt.legend()\nplt.show()","47cc84d7":"data = pd.read_csv(\"..\/input\/sentiment-analysis-pmr3508\/data_test2_X.csv\")\ndata.head()","b1e70c42":"# Separando em listas\nX_test = data.loc[:,'review'].tolist()\n\n# Limpeza dos dados\nX_test = [clean(x) for x in tqdm(X_test)]\nX_test = [x.split() for x in X_test]","e185d930":"# Doc2Vec\nX_test = [emb(x, d2v) for x in tqdm(X_test)] \nX_test = np.array(X_test)","bc31244e":"best = np.argmax(aucs)\npredictions = []\n\n# Uso o modelo que obteve maior AUC\nif best == 0:\n    predictions = mlp1.predict_proba(X_val)[:,1]\nelif best == 1:\n    predicitons = mlp2.predict_proba(X_val)[:,1]\nelse:\n    predictions = mlp3.predict(X_test).squeeze()\n\noutput = pd.DataFrame({'positive': predictions})\noutput.head()","13eeea5b":"output.to_csv(\"submission.csv\", index = True, index_label = 'Id')","d4670931":"### Criando Arquivo de Submiss\u00e3o","f042f6cc":"### Comparando as AUCs","0d470139":"E finalmente criamos as listas $X$ e $y$:","78e83d92":"Para esse modelo, utilizaremos a t\u00e9cnica de Random Search (j\u00e1 que temos uma dimens\u00e3o a mais no nosso problema, e o espa\u00e7o de busca seria muito maior do que no anterior).","f46c3270":"E verificamos se h\u00e1 textos duplicados:","0953b29d":"### Importando e Tratando a Base de Valida\u00e7\u00e3o","c5107135":"Primeiro, precisamos otimizar os hiperpar\u00e2metros **n\u00famero de neur\u00f4nios** e **L2** da Rede Neural utilizando Cross Validation de 5 folds em conjunto com a t\u00e9cnica de Grid Search.\n\n**Nota**: Foi utilizada a op\u00e7\u00e3o Early Stopping nas rede neurais,  que faz com que, durante o seu treinamento, 10% de sua base de treino seja separada para valida\u00e7\u00e3o. Caso n\u00e3o ocorra melhoria nela em 10 itera\u00e7\u00f5es, o algoritmo para.","7cd31984":"Vamos agora utilizar o TensorFlow para treinar mais uma Rede Neural, com 2 camadas ocultas, seguindo as orienta\u00e7\u00f5es do notebook *tensorflow_kears.ipynb*.","5e314371":"Rodamos o Random Search para otimizar os hiperpar\u00e2metros:","a9dbad67":"Fun\u00e7\u00e3o que retorna as representa\u00e7\u00f5es vetoriais dos textos:","d41a544b":"Separamos a base de treino em treino e valida\u00e7\u00e3o \"manualmente\" pois, utilizando a op\u00e7\u00e3o \"validation_split\" do m\u00e9todo fit, todas as AUCs de valida\u00e7\u00e3o eram calculadas como zeros:","2fc330e5":"Novamente observamos resultados bem similares. Irei escolher o modelo que obteve o maior AUC, mas note que, acada vez que rodo o caderno, os resultados s\u00e3o ligeiramente diferentes.","9ddbab1e":"### Importando e Tratando Base de Teste","8e487a6b":"# 1. Criando *Embeddings* para o Conjunto de Treino","e44462f8":"### Limpeza e Manipula\u00e7\u00e3o dos Dados","47227c4f":"E finalmente treinamos o modelo com os par\u00e2metros otimizados:","7f4e4a7e":"# An\u00e1lise de Sentimentos \n\nFelipe Kenzo Kusakawa Mashuda - 10274415\n\n### Bibliotecas\n\nVou importar as mesmas bibliotecas que as utilizadas nos notebooks *analise-de-sentimentos.ipynb* e *tensorflow_keras.ipynb*.","618c0805":"# 3. Rede Neural com TensorFlow (op\u00e7\u00e3o B)","61f49c9b":"Nesta primeira se\u00e7\u00e3o, apenas sigo as instru\u00e7\u00f5es disponibilizadas no notebook *analise-de-sentimentos.ipynb*.","89470362":"Carregando o modelo:","46c0cb19":"### Utilizando o Doc2Vec","d6869ec7":"Definimos a mesma fun\u00e7\u00e3o de limpeza utilizada no treinamento do modelo Doc2Vec:","38ce6b81":"Vamos treinar duas redes neurais (Multilayer Perceptrons) - uma com apenas uma camada oculta e outra com duas.","1770912c":"Aplicando na base de treino:","b36b7781":"E, com eles, treinamos o nosso terceiro modelo:","2ba29a5c":"Em seguida, retiramos as duplicatas:","40441a7f":"Come\u00e7amos importando a base de teste 2 e tratando-a da mesma forma que as anteriores (exceto remo\u00e7\u00e3o de duplicatas):","54623636":"Assim como no caderno, utilizamos a metodologia de Random Search:","d4d295a7":"### Modelo com Uma Camada Oculta","29187c86":"Podemos tamb\u00e9m observar um dos textos:","7644a68f":"E aplicamos \u00e0 nossa base:","f71b2410":"Por fim, tokenizamos os textos:","8bbf525c":"### Modelo com Duas Camadas Ocultas","f798ca95":"Agora vamos utilizar a base de teste para comparar a performance entre o s tr\u00eas modelos que treinamos. Primeiro, precisamos import\u00e1-la e trat\u00e1-la do mesmo modo que tratamos a base de treino.","02aab579":"# 2. Redes Neurais com SciKit","1b3f708e":"\nOs tr\u00eas produziram valores bem pr\u00f3ximos. Vamos olhar para as curvas ROC de cada um:","eb74ace4":"# 4. Comparando os Classificadores","0aab9998":"Vamos olhar primeiro para o valor da AUC para cada um dos modelos:","3969c932":"# 5. Submiss\u00e3o"}}