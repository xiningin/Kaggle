{"cell_type":{"f277e2d2":"code","df4344f3":"code","a62cc391":"code","eeba24bf":"code","84a49dbd":"code","4e40a097":"code","9b9adb1b":"code","64cadfe3":"code","2d531955":"code","9c6cf0b7":"code","12a89b68":"code","25acb5c0":"code","52cb7acf":"code","3d8a6693":"code","92c69a6d":"code","b05dc83c":"code","8fcb8e61":"code","b647bc50":"code","bf3b3eb1":"code","1efa6750":"code","2648a60d":"code","8a5f0794":"code","cfb687f5":"code","5687b94e":"code","49900a02":"code","ddce3f48":"code","339c0b55":"code","93816b37":"code","356f0a73":"code","78075147":"code","8f9362a8":"code","4d913f1e":"code","af1b2e6c":"code","979c4614":"code","e055a261":"code","5158e15d":"code","6918a9c3":"code","f7772c33":"code","58ba1794":"code","4dd09786":"code","3baf6cf7":"code","cac1d766":"code","da103208":"code","034c1f19":"code","5517a843":"code","30e4335d":"code","386ae23b":"code","2e630c68":"code","b09fc2cb":"code","24ef1fc2":"code","1ed70bbd":"code","f3eae3aa":"code","4454a3e4":"code","7e6cddc9":"code","6a223a38":"code","09d5f710":"code","d52dcb83":"code","7cf275bf":"code","dccbb356":"code","22a1cb3a":"code","b809fb4c":"code","0478cf7c":"code","5da2f788":"code","34dd77a8":"markdown","c096374f":"markdown","3956a4f0":"markdown","b66245bf":"markdown","3b72413b":"markdown","2bf89851":"markdown","35042df0":"markdown","c7122bdf":"markdown","be0ce532":"markdown","2dcbe4b0":"markdown","502330cb":"markdown","0128c7fc":"markdown","41ff0f42":"markdown","b6fbb8df":"markdown","ef74413a":"markdown","d1671f8c":"markdown","9d9428be":"markdown","8dd397be":"markdown","e7379138":"markdown","4f1638b8":"markdown","bdd433b5":"markdown","78d62ff9":"markdown","e9d9a849":"markdown","76f1b9c3":"markdown","ba77b0b1":"markdown","e7f8f2bd":"markdown","f7dc45d8":"markdown","ceafc2fc":"markdown","0e53719d":"markdown","8e9675bf":"markdown","26656484":"markdown","709fa3dd":"markdown","94c1d148":"markdown","14780788":"markdown","ce823c53":"markdown","65b0acbf":"markdown","1955eae1":"markdown","8b16721f":"markdown","04ecd66a":"markdown","a9797ff1":"markdown","4090d935":"markdown","4f19eae3":"markdown","2db80b69":"markdown","36635ca7":"markdown","a16b0cfb":"markdown","186aa25c":"markdown","c41ad331":"markdown","da00a90f":"markdown","c60f420d":"markdown","55c814f1":"markdown","53ab1f03":"markdown","fbc25ccf":"markdown","ecd4c79e":"markdown","b053bcf4":"markdown","af828a09":"markdown","54be910a":"markdown"},"source":{"f277e2d2":"from IPython.display import HTML\nfrom IPython.display import Image\nImage(url= \"http:\/\/www.techhuman.com\/wp-content\/uploads\/2018\/10\/FIFA-19-Wallpaper-4-1080x675.jpg\")","df4344f3":"from IPython.core.display import HTML\nHTML('''<script>\ncode_show=true; \nfunction code_toggle() {\n if (code_show){\n $('div.input').hide();\n } else {\n $('div.input').show();\n }\n code_show = !code_show\n} \n$( document ).ready(code_toggle);\n<\/script>\nThe raw code for this IPython notebook is by default hidden for easier reading.\nTo toggle on\/off the raw code, click <a href=\"javascript:code_toggle()\">here<\/a>.''')","a62cc391":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np # linear algebra\n\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n#from pandas.plotting import scatter_matrix as sm\n\nimport os\n\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm as cm\n\nimport scipy.stats  as stats\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.feature_selection import RFE\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn import metrics\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nimport warnings; warnings.simplefilter('ignore')\n\nfifa_dataset = pd.read_csv('..\/input\/FullData.csv',encoding=\"ISO-8859-1\")","eeba24bf":"fifa_dataset.head()","84a49dbd":"fifa_dataset.drop(['Index','ID','Photo','Flag','Club Logo','Real Face','Jersey Number','Joined','Loaned From','Contract Valid Until','LS','ST','RS','LW','LF','CF','RF','RW','LAM','CAM','RAM','LM','LCM','CM','RCM','RM','LWB','LDM','CDM','RDM','RWB','LB','LCB','CB','RCB','RB'],axis=1,inplace=True)\n#'Crossing','Finishing','HeadingAccuracy','ShortPassing','Volleys','Dribbling','Curve','FKAccuracy','LongPassing','BallControl','Acceleration','SprintSpeed','Agility','Reactions','Balance','ShotPower','Jumping','Stamina','Strength','LongShots','Aggression','Interceptions','Positioning','Vision','Penalties','Composure','Marking','StandingTackle','SlidingTackle','GKDiving','GKHandling','GKKicking','GKPositioning','GKReflexes'\nfifa_dataset.head()","4e40a097":"fifa_dataset.describe()","9b9adb1b":"fifa_dataset.info()","64cadfe3":"print(\"Are there Null Values in the dataset? \")\nfifa_dataset.isnull().values.any()","2d531955":"total = fifa_dataset.isnull().sum()[fifa_dataset.isnull().sum() != 0].sort_values(ascending = False)\npercent = pd.Series(round(total\/len(fifa_dataset)*100,2))\npd.concat([total, percent], axis=1, keys=['total_missing', 'percent'])","9c6cf0b7":"def value_to_float(x):\n    if type(x) == float or type(x) == int:\n        return x\n    if 'K' in x:\n        if len(x) > 1:\n            return float(x.replace('K', '')) * 1000\n        return 1000.0\n    if 'M' in x:\n        if len(x) > 1:\n            return float(x.replace('M', '')) * 1000000\n        return 1000000.0\n    return 0.0\n","12a89b68":"fifa_dataset['Release Clause'] = fifa_dataset['Release Clause'].str.replace('\u00e2\\x82\u00ac', '')  \nfifa_dataset['Release Clause'] = fifa_dataset['Release Clause'] .apply(value_to_float)","25acb5c0":"\nfifa_dataset['Wage'] = fifa_dataset['Wage'].str.replace('\u00e2\\x82\u00ac', '')      \nfifa_dataset['Wage'] = fifa_dataset['Wage'] .apply(value_to_float)","52cb7acf":"fifa_dataset['Value'] = fifa_dataset['Value'].str.replace('\u00e2\\x82\u00ac', '')\nfifa_dataset['Value'] = fifa_dataset['Value'] .apply(value_to_float)","3d8a6693":"x=np.array(fifa_dataset['Release Clause'])\nRelease_Clause_no_nan= fifa_dataset['Release Clause'][~np.isnan(fifa_dataset['Release Clause'])]\ny=np.array(Release_Clause_no_nan)\n\nz=fifa_dataset[['Value','Release Clause','Wage']]\nprint (z)","92c69a6d":"#Filling Missing Values\nfor col in fifa_dataset.columns.values:\n    if fifa_dataset[col].isnull().sum()==0:\n        continue\n    if col == 'Release Clause':\n        guess_values = fifa_dataset['Release Clause'].fillna(value=fifa_dataset['Value'])\n    else:\n        guess_values = fifa_dataset['Club'].apply(lambda x: 'No Club')\n        \n    fifa_dataset[col].loc[(fifa_dataset[col].isnull())] = guess_values\n    \n   \n     #if fifa_dataset['Release Clause'].isna().any():\n    #fifa_dataset['Release Clause']. = fifa_dataset['Release Clause'].apply(lambda x: y+1)\n\n    \n ","b05dc83c":"fifa_dataset.isnull().values.any()","8fcb8e61":"#correlation\nfifa_dataset.corr()\nplt.figure(figsize=(40,30))\nax=plt.axes()\n#sns.heatmap(data=fifa_dataset.iloc[:,2:].corr(),annot=True,fmt='.2f',cmap='coolwarm',ax=ax)\nmask = np.zeros_like(fifa_dataset.iloc[:,:].corr())\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    ax = sns.heatmap(data=fifa_dataset.iloc[:,:].corr(), mask=mask, vmax=.3, annot=True,fmt='.2f', square=True, cmap='coolwarm')\n    \nax.set_title('Heatmap showing correlated values for the Dataset')\nplt.show()\n","b647bc50":"fifa_data_p = fifa_dataset.drop(['Name', 'Nationality', 'Club','Position','Work Rate', 'Preferred Foot' ,'Body Type', 'Height', 'Weight'],axis=1)\n\ncorr = fifa_data_p.corr()\n\ncolumns = np.full((corr.shape[0],), True, dtype=bool)\n\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >= 0.9:\n            if columns[j]:\n                columns[j] = False\nselected_column=fifa_data_p.columns[columns]                \nfifa_data_p=fifa_data_p[selected_column]","bf3b3eb1":"def general_position(x):\n    if type(x) == str:\n        \n        if x==\"LS\" or x==\"RS\" or x==\"ST\":\n            return \"Striker\"\n        elif x==\"LF\" or x==\"CF\" or x==\"RF\":\n            return \"Forward\"\n        elif x==\"LW\" or x==\"RW\":\n            return \"Winger\"\n        elif x==\"LAM\" or x==\"CAM\" or x==\"RAM\":\n            return \"Attacking Midfielder\"\n        elif x==\"LM\" or x==\"CM\" or x==\"RM\" or x==\"LCM\" or x==\"RCM\":\n            return \"Central Midfielder\"\n        elif x==\"LDM\" or x==\"RDM\" or x==\"CDM\":\n            return \"Holding Midfielder\"\n        elif x==\"LWB\" or x==\"RWB\":\n            return \"WingBack\"\n        elif x==\"LB\" or x==\"RB\":\n            return \"Fullback\"\n        elif x==\"RCB\" or x==\"LCB\" or x==\"CB\":\n            return \"Defender\"\n        else:\n            return \"GoalKeeper\"\n            \n        \nfifa_dataset['GeneralPosition'] = fifa_dataset['Position'] .apply(general_position)\n\n\n","1efa6750":"#at least one independent variable needs to be a multi-class categorical variable. and its conversion to numeric data\nfifa_dataset['GP_Label'] = fifa_dataset['GeneralPosition'].astype('category')\ncat_columns = fifa_dataset.select_dtypes(['category']).columns\nfifa_dataset[cat_columns] = fifa_dataset[cat_columns].apply(lambda x: x.cat.codes)\n","2648a60d":"fifa_dataset[\"PF_label\"]=fifa_dataset[\"Preferred Foot\"].astype(str)\nfifa_dataset[\"PF_label\"] = np.where(fifa_dataset[\"Preferred Foot\"].str.contains('Left'), 1, 0)\nfifa_dataset.head()","8a5f0794":"fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(34,20))\nplt.subplots_adjust(hspace=0.4)\n\nfifa_new_3= fifa_dataset.drop(['Age', 'Special', 'Name', 'Club', 'Body Type', 'Position', 'Work Rate' ,'GeneralPosition', 'Height', 'Weight', 'Nationality','Preferred Foot', 'Weak Foot', 'Skill Moves', 'Crossing', 'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling', 'Curve', 'FKAccuracy', 'LongPassing', 'BallControl', 'Acceleration', 'SprintSpeed', 'Agility', 'Balance', 'ShotPower', 'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression', 'Interceptions', 'Positioning', 'Vision', 'Penalties', 'Composure', 'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling', 'GKKicking', 'GKPositioning', 'GKReflexes', 'PF_label','GP_Label'], axis = 1) \n\nz = pd.Series()\nfor col in fifa_new_3.columns.values[:]:\n    if(col!='Overall'):\n        colums=np.array(fifa_new_3[col])\n        z[col]=colums\n#p=z.loc[z.index]\n\nfor i in range(2):\n    for j in range(3):\n        \n        #x=z.index.values[i*3+j]\n        #sns.barplot(z.index[i*3+j],z.values[i*3+j])\n        #x=z.index.values[i*3+j]\n        \n        y_label=z.index[i*3+j]\n        x_label=z[i*3+j]\n        \n        sns.regplot(data=fifa_new_3, x=z.index[i*3+j], y='Overall',ax=axes[i,j])\n\n\nfig.suptitle('Univariate Distribution of Positively Correlated Factors', fontsize='25')\nplt.show()","cfb687f5":"x_=fifa_data_p.drop(['Overall'],axis=1)\nx_.columns","5687b94e":"#factors on the basis of p-value\nselected_columns_1 = selected_column[0:1].values\nselected_columns_2 = selected_column[2:].values\nselected_columns = np.concatenate((selected_columns_1,selected_columns_2),axis=0)\n\n\ndef backwardelimination(x, Y, sl, columns):\n    numVars = len(x[0])\n    \n    for i in range(0, numVars):\n        regressor_OLS = smf.OLS(Y, x).fit()\n        maxVar = max(regressor_OLS.pvalues).astype(float)\n        \n        if maxVar > sl:\n            \n            for j in range(0, numVars - i):\n                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n                    x = np.delete(x, j,1)\n                    columns = np.delete(columns, j)\n                    \n\n    regressor_OLS.summary()\n    return x, columns\nSL = 0.05\n\nY=fifa_dataset['Overall'].values\ndata_modeled, selected_columns = backwardelimination(x_.values,fifa_data_p['Overall'].values, SL,selected_columns )\n","49900a02":"def linear_regression(X,y):\n#SPLIT TEST AND TRAIN\n    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n\n#One Hot Encoding\n    X_train = pd.get_dummies(X_train)\n    X_test = pd.get_dummies(X_test)\n\n#Linear Regression\n    LR = LinearRegression()\n    LR.fit(X_train, y_train)\n    predictions = LR.predict(X_test)\n\n    print(X_test.shape,X_train.shape,y_test.shape,y_train.shape)\n    print('r2_Square:%.2f '% r2_score(y_test, predictions))\n    print('MSE:%.2f '% np.sqrt(mean_squared_error(y_test, predictions)))\n\n    regressor_OLS = smf.OLS(y_train, X_train).fit()\n    \n    plt.figure(figsize=(18,10))\n    plt.scatter(predictions,y_test,alpha = 0.3)\n    plt.xlabel('Predictions')\n    plt.ylabel('Overall')\n    plt.title(\"Linear Prediction \")\n    plt.show()\n#cross validation    \n    Kfold = KFold(len(X), shuffle=True)\n    #X_train = sc.fit_transform(X_train)\n    #X_test = sc.transform(X_test)\n    print(\"KfoldCrossVal mean score using Linear regression is %s\" %cross_val_score(LR,X_train,y_train,cv=10).mean())\n    z=print(regressor_OLS.summary())\n    return z\n\n   ","ddce3f48":"####function to calculate cross validation score only\ndef cross_val(X,y):\n    #SPLIT TEST AND TRAIN\n    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n\n#One Hot Encoding\n    X_train = pd.get_dummies(X_train)\n    X_test = pd.get_dummies(X_test)\n\n#Linear Regression\n    LR = LinearRegression()\n    LR.fit(X_train, y_train)\n    predictions = LR.predict(X_test)\n    Kfold = KFold(len(X), shuffle=True)\n    print(\"KfoldCrossVal mean score using Linear regression is %s\" %cross_val_score(LR,X_train,y_train,cv=10).mean())","339c0b55":"#Linear Regression 1\n\nX_1= fifa_dataset.drop(['Overall','Preferred Foot','Name','Nationality','Club','Work Rate','GeneralPosition','Body Type','Weight','Height','Wage','Position','Value'], axis = 1)\ny_1= fifa_dataset['Overall']\nlinear_regression(X_1,y_1)\n#SPLIT TEST AND TRAIN\n#X_train, X_test, y_train, y_test = train_test_split(fifa_new_LR_1, target, test_size=0.2)\n\n#One Hot Encoding\n#X_train = pd.get_dummies(X_train)\n#X_test = pd.get_dummies(X_test)\n#print(X_test.shape,X_train.shape,y_test.shape,y_train.shape)","93816b37":"y_2 = pd.DataFrame()\ny_2['Overall'] = fifa_dataset.iloc[:,3]\ny_2.head()\nX_2 = pd.DataFrame(data= data_modeled, columns = selected_columns)\nX_2.head()\nlinear_regression(X_2,y_2)","356f0a73":"#Linear Regression with min p-value\nX_3= fifa_dataset.drop(['Overall','Age', 'Special', 'Name', 'Club', 'Body Type', 'Position', 'Work Rate' , 'Height', 'Weight', 'Nationality','Preferred Foot', 'Skill Moves', 'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Curve', 'FKAccuracy', 'LongPassing', 'BallControl',  'SprintSpeed', 'Jumping',  'Aggression', 'Interceptions', 'Positioning', 'Vision',  'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling', 'GKKicking', 'GKPositioning', 'GKReflexes', 'Release Clause', 'GeneralPosition', 'GP_Label', 'PF_label'], axis = 1) \n#'Potential','Value','Wage','International Reputation','Reaction','Release Clause'\ny_3 = fifa_dataset['Overall']\nlinear_regression(X_3,y_3)","78075147":"#Linear Regression 4\n\nX_4= fifa_dataset.drop(['Overall','Age', 'Name', 'Club', 'Body Type', 'Position', 'Work Rate' ,'GeneralPosition', 'Height', 'Weight', 'Nationality','Preferred Foot', 'Weak Foot', 'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys',  'Curve', 'FKAccuracy', 'LongPassing', 'BallControl', 'Acceleration', 'SprintSpeed', 'Agility', 'Balance',  'Jumping', 'Stamina', 'Strength', 'Aggression', 'Interceptions', 'Positioning',  'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling', 'GKKicking', 'GKPositioning', 'GKReflexes'], axis = 1) \n#'Reactions','Vision','Composure','Crossing','Dribbling','ShotPower','LongShots','Penalties','Value','Release Clause','Wage','Potential'\n#'Potential','Value','Wage','International Reputation','Reaction','Release Clause'\ny_4 = fifa_dataset['Overall']\nlinear_regression(X_4,y_4)\n#SPLIT TEST AND TRAIN\n#X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(fifa_new_LR_3, target, test_size=0.2)\n\n#One Hot Encoding\n#X_train = pd.get_dummies(X_train)\n#X_test = pd.get_dummies(X_test)\n#print(X_test_3.shape,X_train_3.shape,y_test_3.shape,y_train_3.shape)","8f9362a8":"fifa_dataset_box=fifa_dataset.drop(['Name', 'Nationality', 'Club',  'Preferred Foot',  'Work Rate', 'Body Type', 'Position',\n     'GeneralPosition', 'GP_Label', 'PF_label','Special','Value','International Reputation','Height','Weight',\n       'Weak Foot', 'Skill Moves',\n       'Wage', 'Release Clause'],axis=1)\nf, ax = plt.subplots(figsize=(20, 30))\n\nax.set_facecolor('#FFFFFF')\nplt.title(\"Box Plot AQI Dataset Scaled\")\nax.set(xlim=(0, 100))\nax = sns.boxplot(data = fifa_dataset_box, \n  orient = 'h', \n  palette = 'Set3')","4d913f1e":"predictor_names=fifa_dataset_box.columns.get_values()\npredictor_names=predictor_names.tolist()\npredictor_names","af1b2e6c":"def rank_predictors(dat,l,f='PF_label'):\n    rank={}\n    max_vals=dat.max()\n    mean_vals=dat.groupby(f).mean()  # We are using the mean \n    for p in l:\n        score=np.abs((mean_vals[p][1]-mean_vals[p][0])\/max_vals[p])\n        rank[p]=score\n    return rank\ncat_rank=rank_predictors(fifa_dataset,predictor_names) \ncat_rank","979c4614":"# Take the top predictors based on median difference\ncat_rank=sorted(cat_rank.items(), key=lambda x: x[1],reverse= True)\n\nranked_predictors=[]\nfor f in cat_rank:\n    ranked_predictors.append(f[0])\nranked_predictors\n\n","e055a261":"fifa_new_LG_3= fifa_dataset.drop(['Overall','Age', 'Name', 'Club', 'Body Type', 'Position', 'Work Rate' , 'Height', 'Weight', 'Nationality','Preferred Foot', 'Weak Foot', 'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys',  'Curve', 'FKAccuracy', 'LongPassing', 'BallControl', 'Acceleration', 'SprintSpeed', 'Agility', 'Balance',  'Jumping', 'Stamina', 'Strength', 'Aggression', 'Interceptions', 'Positioning',  'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling', 'GKKicking', 'GKPositioning', 'GKReflexes'], axis = 1) \n\n","5158e15d":"def attacking_position(x):\n    if type(x) == str:\n        \n        if x==\"GK\" or x==\"RCB\" or x==\"LCB\" or x==\"CB\" or x==\"RCB\" or x==\"LCB\" or x==\"CB\" or x==\"LB\" or x==\"RB\" or x==\"LDM\" or x==\"RDM\" or x==\"CDM\":\n            return 0\n        else:\n            return 1     \n        \nfifa_dataset['AttackingPosition'] = fifa_dataset['Position'] .apply(attacking_position)\nfifa_dataset.head()","6918a9c3":"def logistic_regression(x,y):\n    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=0)\n    sc = StandardScaler()\n    \n    # Feature scaling\n    x_train = sc.fit_transform(x_train)\n    x_test = sc.fit_transform(x_test)\n    \n    #Fitting logistic regression to the training set\n    classifier = LogisticRegression(random_state = 0)\n    classifier.fit(x_train,y_train)\n    \n    \n    # Logistic regression cross validation\n    #Kfold = KFold(len(ranked_predictors), shuffle=False)\n    \n    k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n    cvs=cross_val_score(classifier, x_train, y_train, cv=k_fold).mean()\n    print(\"KfoldCrossVal mean score using Logistic regression is %s \\n\"%cvs)\n    \n    \n    \n    print(\"Logistic Analysis Report\")\n    y_pred = classifier.predict(x_test)\n    print(classification_report(y_test,y_pred))\n    print(y_pred)\n    print (\"Accuracy Score:%.2f\" % metrics.accuracy_score(y_test,classifier.predict(x_test)))\n    \n    \n    y_pred_proba = classifier.predict_proba(x_test)[::,1]\n    print('Probabilty of dependent variable')\n    print(y_pred_proba.mean())\n    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n    plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.legend(loc=4)\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.show()\n    \n# Predicting the Test set results\n#Precision \u2013 Accuracy of positive predictions.\n#Precision = TP\/(TP + FP)\n\n#FN \u2013 False Negatives\n#Recall (aka sensitivity or true positive rate): Fraction of positives That were correctly identified.\n#Recall = TP\/(TP+FN)\n\n#F1 Score (aka F-Score or F-Measure) \u2013 A helpful metric for comparing two classifiers. F1 Score takes into account precision and the recall. It is created by finding the the harmonic mean of precision and recall.\n#F1 = 2 x (precision x recall)\/(precision + recall)\n","f7772c33":"def logit_summary(y,X):\n    logit_model=sm.Logit(y,X)\n    result=logit_model.fit()\n    print(\"Model Summary\")\n    print(result.summary2())","58ba1794":"fifa_log = fifa_dataset.drop(['Club','Preferred Foot','GeneralPosition','Work Rate','Body Type','Position','Height', 'Weight'],axis=1)\nfifa_log.head()","4dd09786":"x=fifa_log.iloc[:,7:46]\n#w=fifa_log.iloc[:,47]\n#z.insert(loc=39, column='PF_label', value=w)\n\ny=fifa_log.iloc[:,47]\nlogistic_regression(x,y)\nlogit_summary(y,x)","3baf6cf7":"x=fifa_log.iloc[:,7:46]\n#w=fifa_log.iloc[:,47]\n#x.insert(loc=39, column='PF_label', value=w)\ny=fifa_log.iloc[:,48]\nlogistic_regression(x,y)\nlogit_summary(y,x)","cac1d766":"x=fifa_log[predictor_names]\ny=fifa_log.iloc[:,47]\nlogistic_regression(x,y)\nlogit_summary(y,x)","da103208":"## high variance inflation factor because they \"explain\" the same variance within this dataset. We would need to discard one of these variables before \n#moving on to model building or risk building a model with high multicolinearity.\ndef variance_IF(X):\n    vif=vif = pd.DataFrame()\n    vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif[\"features\"] = X.columns\n    return vif\n","034c1f19":"variance_IF(X_1)","5517a843":"variance_IF(X_2)","30e4335d":"variance_IF(X_3)","386ae23b":"variance_IF(X_4)","2e630c68":"#Linear Model 1:-\n##X_M1=X_1[[]]\nprint('Linear Model 1')\ncross_val(X_1,y_1)","b09fc2cb":"#Linear Model 2:-\nprint('Linear Model 2')\ncross_val(X_2,y_2)","24ef1fc2":"#Linear Model 3:-\nprint('Linear Model 3')\ncross_val(X_3,y_3)","1ed70bbd":"#Linear Model 4:-\n##X_M1=X_1[[]]\nprint('Linear Model 1')\ncross_val(X_4,y_4)","f3eae3aa":"#https:\/\/datascience.stackexchange.com\/questions\/937\/does-scikit-learn-have-forward-selection-stepwise-regression-algorithm\n\n#X=fifa_dataset.drop(['date','state', 'location', 'type','AQI_Range'],axis=1)\nX=X_1.astype(float)\ny=fifa_dataset['Overall']\n\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    \n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.argmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n\nresult = stepwise_selection(X, y)\n\nprint('resulting features:',result)\n\n","4454a3e4":"StepwiseCrossValidate=fifa_dataset[['Reactions', 'Release Clause', 'International Reputation', 'Potential', 'Age', 'Stamina', 'GKDiving', 'Special', 'HeadingAccuracy', 'Balance', 'SprintSpeed', 'ShortPassing', 'Positioning', 'Composure', 'GKHandling', 'ShotPower', 'GP_Label', 'BallControl', 'Vision', 'GKPositioning', 'Skill Moves', 'LongShots']]\ntarget=fifa_dataset['Overall']\nlinear_regression(StepwiseCrossValidate,target)","7e6cddc9":"def evaluateModel (model):\n    print(\"RSS = \", ((fifa_dataset.Overall - model.predict())**2).sum())\n    print(\"R2 = \", model.rsquared)\n    \nfifa_dataset['InternationalReputation']=fifa_dataset['International Reputation']\nmodelAll = smf.ols('Overall ~ Age + InternationalReputation + Potential', fifa_dataset).fit()\nprint(modelAll.summary().tables[1])\nevaluateModel (modelAll)","6a223a38":"modelIntR = smf.ols('Overall ~ InternationalReputation', fifa_dataset).fit()\nprint(modelIntR.summary().tables[1])\nevaluateModel (modelIntR)","09d5f710":"modelAge = smf.ols('Overall ~ Age', fifa_dataset).fit()\nprint(modelAge.summary().tables[1])\nevaluateModel (modelAge)","d52dcb83":"modelPot = smf.ols('Overall ~ Potential', fifa_dataset).fit()\nprint(modelPot.summary().tables[1])\nevaluateModel (modelPot)","7cf275bf":"modelIntRAge = smf.ols('Overall ~ InternationalReputation + Age', fifa_dataset).fit()\nprint(modelIntRAge.summary().tables[1])\nevaluateModel (modelIntRAge)","dccbb356":"modelIntRPot = smf.ols('Overall ~ InternationalReputation + Potential', fifa_dataset).fit()\nprint(modelIntRPot.summary().tables[1])\nevaluateModel (modelIntRPot)","22a1cb3a":"modelAgePot = smf.ols('Overall ~ Age + Potential', fifa_dataset).fit()\nprint(modelAgePot.summary().tables[1])\nevaluateModel (modelAgePot)","b809fb4c":"modelAgePot = smf.ols('Overall ~ Age + Potential + Age*Potential', fifa_dataset).fit()\nprint(modelAgePot.summary().tables[1])\nevaluateModel (modelAgePot)","0478cf7c":"#Ridge\n\nX_train, X_test, y_train, y_test = train_test_split(X_1,y_1, test_size=0.2)\nX_train = pd.get_dummies(X_train)\nX_test = pd.get_dummies(X_test)\n\nridgeReg = Ridge(alpha=0.05, normalize=True)\nridgeReg.fit(X_train,y_train)\npred = ridgeReg.predict(X_test)\n\nprint(X_test.shape,X_train.shape,y_test.shape,y_train.shape)\nprint('r2_Square:%.2f '% r2_score(y_test, pred))\nprint('MSE:%.2f '% np.sqrt(mean_squared_error(y_test, pred)))\n\nregressor_OLS = smf.OLS(y_train, X_train).fit()\n\nplt.figure(figsize=(18,10))\nplt.scatter(pred,y_test,alpha = 0.3)\nplt.xlabel('Predictions')\nplt.ylabel('AQI')\nplt.title(\"Linear Prediction \")\nplt.show()\n#cross validation    \nKfold = KFold(len(X_1), shuffle=True)\n    #X_train = sc.fit_transform(X_train)\n    #X_test = sc.transform(X_test)\nprint(\"KfoldCrossVal mean score using Linear regression is %s\" %cross_val_score(ridgeReg,X_train,y_train,cv=10).mean())\n\n\nregressor_OLS.summary()\n\n","5da2f788":"#Lasso\n#X_train, X_test, y_train, y_test = train_test_split(X_3,y_3, test_size=0.2)\n#X_train = pd.get_dummies(X_train)\n#X_test = pd.get_dummies(X_test)\n\n#lassoReg = Lasso(alpha=0.3, normalize=True)\n#lassoReg.fit(X_train,y_train)\n\n#print(X_test.shape,X_train.shape,y_test.shape,y_train.shape)\n#print('r2_Square:%.2f '% r2_score(y_test, pred))\n#print('MSE:%.2f '% np.sqrt(mean_squared_error(y_test, pred)))\n\n#regressor_OLS = smf.OLS(y_train, X_train).fit()\n\n#plt.figure(figsize=(18,10))\n#plt.scatter(pred,y_test,alpha = 0.3)\n#plt.xlabel('Predictions')\n#plt.ylabel('AQI')\n#plt.title(\"Linear Prediction \")\n#plt.show()\n#cross validation    \n#Kfold = KFold(len(X_1), shuffle=True)\n    #X_train = sc.fit_transform(X_train)\n    #X_test = sc.transform(X_test)\n#print(\"KfoldCrossVal mean score using Linear regression is %s\" %cross_val_score(lassoReg,X_train,y_train,cv=10).mean())\n\n\n#regressor_OLS.summary()","34dd77a8":"# Age and Potential <a id='Age&Potential'><\/a>\n\n[Back to top](#InteractionEffect)","c096374f":"#### Significant Questions on the basis of the all 3 Linear Models\n##### * Is there any multi-colinearity in the model?   \n\nYes there is very high multicollinearity in all models  .(based on VIF values calculated)\n\n##### * In the multiple regression models are predictor variables independent of all the other predictor variables?  \nFor Model 1:- Most independant variables have very high VIF, thus they are not independent of all the other predictor variables, since the calculated VIF value is greater than 5.\n\nFor Model 2:- All the independant variables have very high VIF although there are no infinite values in this\n\nFor Model 3:- The model is somewhat good, although the VIF values are still very high\n\nFore Model 4:- This model has a couple of values with low VIF, but still, most values are very very high.\n\n##### * In multiple regression models rank the most significant predictor variables and exclude insignificant ones from the model. \n\nThe regular\u00a0regression coefficients\u00a0that we see in statistical output describe the relationship between the independent variables and the dependent variable. The\u00a0coefficient\u00a0value\u00a0represents the\u00a0mean\u00a0change of the dependent variable given a one-unit shift in an independent variable Consequently, we might feel that we can use the absolute sizes of the\u00a0coefficients\u00a0to identify the most important variable. After all, a larger coefficient signifies a greater change in the mean of the independent variable. However, the independent variables can have dramatically different types of units, which make comparing the coefficients meaningless.\n\nCalculations for p-values include various properties of the variable, but importance is not one of them. A very small p-value does not indicate that the variable is important in a practical sense. \n\nWe have already excluded the variables in the previous answer and about ranking the significant variables. Output for stepwise regression gives us the important variables. [Stepwise Regression](#StepwiseRegression)\n\n##### * Cross-validate the models. How well did they do?   ","3956a4f0":"#   1.5  Exploratory Data Analysis (EDA)  <a id='eda'><\/a>\nExploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations.\n\nIt is a good practice to understand the data first and try to gather as many insights from it. EDA is all about making sense of data in hand,before getting them dirty with it.\n\n[Back to top](#Introduction)","b66245bf":"# International Reputation <a id='InternationalReputation'><\/a>\n\n[Back to top](#InteractionEffect)","3b72413b":"#  <p style=\"text-align: center;\"> Multicollinearity <\/p> <a id='Multicollinearity'><\/a>\nIn statistics, multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.\n\nWhy Multicollinearity is a problem?\n>Multicollinearity occurs when independent variables in a regression model are correlated. This correlation is a problem because independent variables should be independent. If the degree of correlation between variables is high enough, it can cause problems when you fit the model and interpret the results.\n\nVIF assesses whether factors are correlated to each other (multicollinearity)\nHigh variance inflation factor means that they \"have\" the same variance within this dataset. We would need to discard one of these variables before moving on to model building or risk building a model with high multicolinearity.\nIf the VIF is equal to 1 there is no multicollinearity among factors, but if the VIF is greater than 1, the predictors may be moderately correlated. If the VIF for a factor is near or above 5 we may have to remove highly correlated factors.\n\n[Back to top](#Introduction)","2bf89851":"Since we are to form 3 different linear models for our given project, it would be better if form a function for linear regression that could be utilised everytime we want to perform linear regression . Also, it makes the code less complex and easier to read.\n\nWe will also be validating our model. \nCross Validation:- Cross Validation is a technique which involves reserving a particular sample of a dataset on which we do not train the model. Later, we will test our model on this sample before finalizing it.\n\nWe will be performing K-Fold Cross Validation.Below are the steps for it:\n\n1. Randomly split your entire dataset into k\u201dfolds\u201d\n2. For each k-fold in your dataset, build your model on k \u2013 1 folds of the dataset. Then, test the model to check the effectiveness for kth fold\n3. Record the error you see on each of the predictions\n4. Repeat this until each of the k-folds has served as the test set\n5. The average of your k recorded errors is called the cross-validation error and will serve as your performance metric for the model","35042df0":"### Taking the top predictors based on median difference","c7122bdf":"### Replacing missing values in the dataset\nThe two columns that have missing values are Release Clause and CLub. Club gives us information about at which club is the player playing. A null value in this column means that the player is currently not employed by any club, and in this case, all Null values should be replaced by the string 'No Club', as the player has no club in which the player is playing. Release Clause in Football is a Clause signed by the player in the contract of the player. \n\nRelease Clause sets the selling value of a player to a certain amount. If any club wishes to buy the said player, then they must pay the Release Clause amount to the club first. After paying the release clause, the club is free to negotiate with the player. When the Release Clause of a player is not mentioned, the club will have to pay an amount close to the value of the player to enter negotiation with the said player, Thus we will replace Null Values in the Release Clause Column by the value of that player.","be0ce532":"# <p style=\"text-align: center;\">Contribution<p><a id='Contribution'><\/a>\nAs this was a learning assignment, the majority of the code has been taken from the GitHub account of the professor Nik Brown.\n    \n- Code by self : 65%\n- Code from external Sources : 35%\n\n[Back to top](#Introduction)","2dcbe4b0":"#### Table Overview:- Top 5 rows of the dataset consisting of independent variables that have p-value<0.05\nAnd we will be using the following Independent Variables present in the given dataset for modeling our data using linear regression.","502330cb":"# <p style=\"text-align: center;\"> Introduction <\/p> <a id='Introduction'><\/a>","0128c7fc":"# International Reputation and Potential <a id='InternationalReputation&Potential'><\/a>\n\n[Back to top](#InteractionEffect)","41ff0f42":"# Logistic Model 1 <a id='LogisticModel1'><\/a>\n\nThe Following plot gives us our first Logistic Model along with the summary for the same.\n\n>Training Features :- Special, International Reputation, Weak Foot, Skill Moves, Crossing, Finishing, HeadingAccuracy, ShortPassing, Volleys, Dribbling, Curve, FKAccuracy, LongPassing, BallControl, Acceleration, SprintSpeed, Agility, Reactions, Balance, ShotPower, Jumping, Stamina, Strength, LongShots, Aggression, Interceptions, Positioning, Vision, Penalties, Composure, Marking, StandingTackle, SlidingTackle, GKDiving, GKHandling, GKKicking, GKPositioning, GKReflexes, Release Clause\n\n>Target Feature:- Preferred Foot\n\n[Back to top](#LogisticRegression)","b6fbb8df":"### Box Plot of Fifa19 Dataset (Scaled)","ef74413a":"### Predictor Names","d1671f8c":"### VIF of Linear Model 3 Independant variables","9d9428be":"##   1.5.2  Regplot  <a id='regplot'><\/a>\nRegplot plots a regular linear regression of a single independant variable. The central thick line represents the line of regression, where as the translucent band lines, however, describe a bootstrap confidence interval generated for the estimate.\n\n[Back to top](#eda)","8dd397be":"# Linear Model 1 <a id='LinearModel1'><\/a>\nThe Following plot gives us our first Linear Model along with the summary for the same.\n\n>Training Features :- Age, Potential, Special, International Reputation, Weak Foot, Skill Moves, Crossing, Finishing, HeadingAccuracy, ShortPassing, Volleys, Dribbling, Curve, FKAccuracy, LongPassing, BallControl, Acceleration, SprintSpeed, Agility, Reactions, Balance, ShotPower, Jumping, Stamina, Strength, LongShots, Aggression, Interceptions, Positioning,  Vision, Penalties, Composure, Marking, StandingTackle, SlidingTackle, GKDiving, GKHandling, GKKicking, GKPositioning, GKReflexes, Release Clause, GP_Label, PF_label\n\n>Target Feature:- Overall\n\n[Back to top](#LinearRegression)","e7379138":"#  <p style=\"text-align: center;\"> Stepwise Regression <\/p> <a id='StepwiseRegression'><\/a>\nStepwise regression is a method of fitting regression models in which the choice of predictive variables is carried out by an automatic procedure. (p-value in our case). \n\n>BACKWARD STEPWISE REGRESSION is a stepwise regression approach, that begins with a full model and at each step gradually eliminates variables from the regression model to find a reduced model that best explains the data. Also known as Backward Elimination regression.\n\n>FORWARD STEPWISE REGRESSION is a type of stepwise regression which begins with an empty model and adds in variables one by one. ... It is one of two commonly used methods of stepwise regression; the other is backward elimination, and is almost opposite.Se\n\n[Back to top](#Introduction)","4f1638b8":"# All Columns <a id='AllColumns'><\/a>\n\n[Back to top](#InteractionEffect)","bdd433b5":"#   1.3 Dataset Summary  <a id='dataset_summary'><\/a>\nHere we are looking at the top 5 rows of the dataset to view, what type of dataset it is. We also look at the columns which show the various attributes in the dataset.\n[Back to top](#Introduction)","78d62ff9":"#  <p style=\"text-align: center;\"> Logistic Regression <\/p> <a id='LogisticRegression'><\/a>\nLogistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.\n\nDifference between logistic and linear regression:-\nIn linear regression, the outcome (dependent variable) is continuous. It can have any one of an infinite number of possible values. In logistic regression, the outcome (dependent variable) has only a limited number of possible values(discrete). Also , we use logistic for classification purpose i.e for categorical variables. and linear regression for prediction (by line of regression)\n\n\n[Back to top](#Introduction)","e9d9a849":"#   1.2 Importing Libraries  <a id='importing_libraries'><\/a>\nIn this step we import a few libraries that are required in our program. Some major libraries that are used are Numpy, Pandas, MatplotLib, Seaborn and sklearn.","76f1b9c3":"# <p style=\"text-align: center;\">License<p><a id='License'><\/a>\nCopyright (c) 2019 Rushabh Nisher\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n[Back to top](#Introduction)","ba77b0b1":"# Age and Potential Interaction <a id='Age&PotentialInteraction'><\/a>\n\n[Back to top](#InteractionEffect)","e7f8f2bd":"### VIF of Linear Model 2 Independant variables","f7dc45d8":"#### Significant Questions on the basis of the all 4 Linear Models","ceafc2fc":"## Reviewing the dataset for Logistic Regression","0e53719d":"#   1.1 Abstract  <a id='abstract'><\/a>\nFIFA 19 is a dataset by Karan Gadiya, which contains list of attributes of players like Name, Age, Nationality, Overall, Wage etc. The main focus of this project is learning about modeling of data by supervised algorithms i.e (Linear Regression (regression) and Logistic Regression (classification)). The main focus of this particular kernel is FIFA 19, and factors that affects Overall. Also in the following project there is a brief explanation of how combination of the independent variables (Interaction effect) has what impact on dependent variable and how is the accuracy of the model has been changed because of the same and how interdependence\/ correlation (Multicollinearity) between various independent variable has adverse effect on the dependent varaiable and given data model. The solution to the problems of multicollinearity is also been discussed in the following kernel i.e Regularization and Stepwise Regression. Both of which gives us an enhanced model , with better predictors and estimators in alignment with dependent variable. Furthermore, following kernel contains some EDA(Explorartory Data Analysis) which is usually the first step in your data analysis process. We take a broader look at patterns, trends, outliers, unexpected results and so on in the dataset, using visual and quantitative methods to get a sense of the story it tells. \n","8e9675bf":"# <p style=\"text-align: center;\"> FIFA 19 <\/p>","26656484":"## Generalizing the Value\nThe Position column tells us at which position does the player plays. But this column can be further generalized to 'General Position' as it'll help us convert this column into a categorical value.\n\nWe also convert the General Position into GP_Label and Preferred_Foot PF_Label to convert the categorical value into numerical categorical, so that we can apply regression on it.","709fa3dd":"#  <p style=\"text-align: center;\"> Linear Regression <\/p> <a id='LinearRegression'><\/a>\nLinear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable (Factors), and the other is considered to be a dependent variable (Overall). Linear regression looks at various data points and plots a trend line. Linear regression can create a predictive model on apparently random data, showing trends in data. Linear regression looks at various data points and plots a trend line. Linear regression can create a predictive model on apparently random data, showing trends in data. \n\n### Multiple Linear Regression\n\n#### The case of multiple explanatory variable (independent variable) is called multiple linear regression.\nTo build a well-performing machine learning (ML) model, it is important to seperate data into training and testing dataset . Basically we are training the model on and testing it against the data that comes from the same set of target distribution. \n\n### Simple Linear Regression\nThe case of single explanatory variable (independent variable) is called single linear regression.\u00b6  \n\n#### We applied linear regression model on our dataset and calculated the value for Root Mean Squared Error ,Mean Squared Error(log), AIC and BIC\n\nRoot Mean Squared Error:-\nRoot Mean Square Error (RMSE) mathematically is the standard deviation of the residuals. Residuals is the measure od how far the data points are spreaded across the line of regression which we get by our training data set. RMSE is the measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit.\n\n##### Mean Squared Error:-\nThe Mean Squared Error (MSE) is a measure of how close a fitted line is to data points. It is the sum, over all the data points, of the square of the difference between the predicted and actual target variables, divided by the number of data points. RMSE is the square root of MSE.\n\n##### Akaike information criterion (AIC):-\nThe Akaike information criterion (AIC) is an estimator of the relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. Thus, AIC provides a means for model selection. When a statistical model is used to represent the process that generated the data, the representation will almost never be exact; so some information will be lost by using the model to represent the process. AIC estimates the relative amount of information lost by a given model: the less information a model loses, the higher the quality of that model.\n\nIn estimating the amount of information lost by a model, AIC deals with the trade-off between the goodness of fit of the model and the simplicity of the model. In other words, AIC deals with both the risk of overfitting and the risk of underfitting.\n\n\\begin{equation*}\nAIC = 2k + n\\times log(\\frac {RSS}{N})       \n\\end{equation*}\n\n<p style=\"text-align: center;\">where k=number of parameters, RSS=Residual Sum of Squares and n= number of rows<\/p>\n\n##### Bayesian information criterion (BIC) :-\nIt is a criterion for model selection among a finite set of models; the model with the lowest BIC is preferred. It is based, in part, on the likelihood function and it is closely related to the Akaike information criterion (AIC).\n\nWhen fitting models, it is possible to increase the likelihood by adding parameters, but doing so may result in overfitting. Both BIC and AIC attempt to resolve this problem by introducing a penalty term for the number of parameters in the model; the penalty term is larger in BIC than in AIC.\n\n\\begin{equation*}\nBIC = {n}\\cdot \\ln(RSS\/n) + {k}\\cdot\\ln(n)      \n\\end{equation*}\n\n<p style=\"text-align: center;\">where k=number of parameters, RSS=Residual Sum of Squares and n= number of rows<\/p>\n\nLower the value of AIC and BIC means better model since both measure the loss of data while modeling data, and low value denotes less data is lost overall.\n\n##### P-value:-\nIn statistics, the p-value is a function of the observed sample results (a statistic) that is used for testing a statistical hypothesis. Before the test is performed, a threshold value is chosen, called the significance level of the test, traditionally 5% or 1% and denoted as $\\alpha$.\n\nIf the p-value is equal to or smaller than the significance level ($\\alpha$), it suggests that the observed data are inconsistent with the assumption that the null hypothesis is true and thus that hypothesis must be rejected (but this does not automatically mean the alternative hypothesis can be accepted as true). When the p-value is calculated correctly, such a test is guaranteed to control the Type I error rate to be no greater than $\\alpha$.\n\n[Back to top](#Introduction)","94c1d148":"##### * Is the relationship significant?   \n\nBy the results of all three models we can say , that the relationship between the dependent variable (Overall) and significant independent variables(Potential, Value, Wage, Special, International Reputation, etc..) are significant. \n\n##### * Are any model assumptions violated?    \n\nAssumptions for Linear Regression are as follow:-\n1. Linear relationship.\n2. Multivariate normality.(correlated variables do clustering around mean value)\n3. No or little multicollinearity.\n5. No auto-correlation.\n6. Homoscedasticity.(all random variables in the sequence have the same finite variance)\n\nby the results we can say that they have linear relationship and no auto correlation for multicollinearity and multivariate normality please refer to section [Multicollinearity](#Multicollinearity)\n\n##### * Does the model make sense? Interpret the meaning of each independent variable.   \n\nAll four model makes sense.And we can say as the value of each independent variable we have used increase Overall will increase.\n\n##### * Cross-validate the model. How well did it do? \n\nFor all four models cross validation gives good result, such that it is has high accuracy score(>85%).\n\n##### * Compare the AIC, BIC and adjusted R^2.  Do they agree in their ranking of the models?  \nAIC, BIC and adjusted R^2  for Linear Model 2 is the lowest, lowest and highest , hence it is the the one we should choose.","14780788":"# <p style=\"text-align: center;\">Citation<p><a id='Citation'><\/a>\n1. https:\/\/github.com\/nikbearbrown\/INFO_6105\/tree\/master\/Week_2 - GitHub Account of Professor \n   <a id=\"Reference Link 1\"><\/a>\n2. https:\/\/www.kaggle.com\/karangadiya\/fifa19 - Kaggle Kernel on the same dataset\n   <a id=\"Reference Link 2\"><\/a>\n3. https:\/\/www.youtube.com\/watch?v=E5RjzSK0fvY&t=394s - Youtube video of Linear Regression\n   <a id=\"Reference Link 3\"><\/a>\n4. https:\/\/www.youtube.com\/watch?v=E5RjzSK0fvY&t=396s - Youtube video of Logistic Regression\n   <a id=\"Reference Link 4\"><\/a>\n5. http:\/\/joshlawman.com\/metrics-classification-report-breakdown-precision-recall-f1\/ - F-Score, Precision\n   <a id=\"Reference Link 5\"><\/a>\n6. https:\/\/datascience.stackexchange.com\/questions\/937\/does-scikit-learn-have-forward-selection-stepwise-regression-algorithm - Stepwise Regression\n   <a id=\"Reference Link 6\"><\/a>\n7. https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/a-comprehensive-guide-for-linear-ridge-and-lasso-regression\/ - Ridge and Lasso Regression\n   <a id=\"Reference Link 7\"><\/a>\n8. https:\/\/www.kaggle.com\/marcogdepinto\/feature-engineering-eda-data-cleaning-tutorial - Interaction Effect\n   <a id=\"Reference Link 8\"><\/a>\n\n[Back to top](#Introduction)","ce823c53":"# Potential <a id='Potential'><\/a>\n\n[Back to top](#InteractionEffect)","65b0acbf":"# Logistic Model 2 <a id='LogisticModel2'><\/a>\n\nThe Following plot gives us our second Logistic Model along with the summary for the same.\n\n>Training Features :- Special, International Reputation, Weak Foot, Skill Moves, Crossing, Finishing, HeadingAccuracy, ShortPassing, Volleys, Dribbling, Curve, FKAccuracy, LongPassing, BallControl, Acceleration, SprintSpeed, Agility, Reactions, Balance, ShotPower, Jumping, Stamina, Strength, LongShots, Aggression, Interceptions, Positioning, Vision, Penalties, Composure, Marking, StandingTackle, SlidingTackle, GKDiving, GKHandling, GKKicking, GKPositioning, GKReflexes, Release Clause\n\n>Target Feature:- Attacking Position\n\n[Back to top](#LogisticRegression)","1955eae1":"##   1.5.1  Heatmap  <a id='heatmap'><\/a>\nIn the first step of our EDA we are finding out the correlation among the various attributes of the dataset. Correlation value gives us the measure of linear relationship amongst two numerical quantities. The range of correlation is between -1 and 1.\n\nWhen two variables have a positive correlation, it means the variables move in the same direction. This means that as one variable increases, so does the other one. In a negative correlation, the variables move in inverse, or opposite, directions. In other words, as one variable increases, the other variable decreases. When the correlation value is 0, no correlation exists between the attributes.\n\nHeatmap is a visual representation of the Correlation. In our heatmap, we show positively correlated values with a shade of red. The shade of red keeps on getting darker as the correlation increases. While the negatively correlated values are shown with a shade of blue, which gets darker as the inverse correlation increases.\n\nHere, we will take a look at a few factors which have high correlation with our dependant variable i.e. Overall so that we can perform various tests on it.\n\n[Back to top](#eda)","8b16721f":"### Statistical Analysis\nHere we are running basic Statistical analysis on the given data to find any abnormal values in the dataset","04ecd66a":"### Removing Unnecessary Columns\nNow, here we see that there are a few columns which may not be relevant to our analysis, so we'll drop those columns before we proceed.","a9797ff1":"# Linear Model 4 <a id='LinearModel4'><\/a>\n\nThe Following plot gives us our first Linear Model along with the summary for the same.\n\n>Training Features :- Potential, Value, Wage, Special, International Reputation, Skill Moves, Crossing, Dribbling, Reactions, ShotPower, LongShots, Vision, Penalties, Composure, Release Clause, GP_Label, PF_label, InternationalReputation\n\n>Target Feature:- Overall\n\n[Back to top](#LinearRegression)","4090d935":"### VIF of Linear Model 4 Independant variables","4f19eae3":"### Rank Predictors","2db80b69":"#  <p style=\"text-align: center;\"> Regularization <\/p> <a id='Regularization'><\/a>\nRegularization is a very important technique in machine learning to prevent overfitting. Mathematically speaking, it adds a regularization term in order to prevent the coefficients to fit so perfectly to overfit. The difference between the L1 and L2 is just that L2 is the sum of the square of the weights, while L1 is just the sum of the weights\n\nIs there collinearity among some features? L2 regularization can improve prediction quality in this case, as implied by its alternative name, \"ridge regression.\" However, it is true in general that either form of regularization will improve out-of-sample prediction, whether or not there is multicollinearity and whether or not there are irrelevant features, simply because of the shrinkage properties of the regularized estimators. L1 regularization can't help with multicollinearity it will just pick the feature with the largest correlation to the outcome  (which isn't useful if you have an interest in estimating coefficients for all features which are strongly correlated with your target). Ridge regression can obtain coefficient estimates even when you have more features than examples... but the probability that any will be estimated precisely at 0 is 0.\n\n[Back to top](#Introduction)","36635ca7":"# Logistic Model 3 <a id='LogisticModel3'><\/a>\nThe Following plot gives us our third Logistic Model along with the summary for the same.\n\n>Training Features :- Special, International Reputation, Weak Foot, Skill Moves, Crossing, Finishing, HeadingAccuracy, ShortPassing, Volleys, Dribbling, Curve, FKAccuracy, LongPassing, BallControl, Acceleration, SprintSpeed, Agility, Reactions, Balance, ShotPower, Jumping, Stamina, Strength, LongShots, Aggression, Interceptions, Positioning, Vision, Penalties, Composure, Marking, StandingTackle, SlidingTackle, GKDiving, GKHandling, GKKicking, GKPositioning, GKReflexes, Release Clause\n\n>Target Feature:- Attacking Position\n\n[Back to top](#LogisticRegression)","a16b0cfb":"Since we are to form 3 different logistic models for our given project, it would be better if form a function for logistic regression that could be utilised everytime we want to perform logistic regression . Also, it makes the code less complex and easier to read.\n\nWe will also be validating our model. Cross Validation:- Cross Validation is a technique which involves reserving a particular sample of a dataset on which we do not train the model. Later, we will test our model on this sample before finalizing it.\n\nWe will be performing K-Fold Cross Validation.Below are the steps for it:\n\n1. Randomly split your entire dataset into k\u201dfolds\u201d\n2. For each k-fold in your dataset, build your model on k \u2013 1 folds of the dataset. Then, test the model to check the effectiveness for kth fold\n3. Record the error you see on each of the predictions\n4. Repeat this until each of the k-folds has served as the test set\n5. The average of your k recorded errors is called the cross-validation error and will serve as your performance metric for the model.\n\n    \n    \n##### Predicting the Test set results(Classification Report - Interpreting meaning of values we get in it)\nPrecision \u2013 Accuracy of positive predictions.\n\n\\begin{equation*}\nPrecision = \\frac {TP}{(TP + FP)}\n\\end{equation*}\n\nFN \u2013 False Negatives\n\nRecall (aka sensitivity or true positive rate): Fraction of positives That were correctly identified.\n\n\\begin{equation*}\nRecall = \\frac {TP}{(TP+FN)}\n\\end{equation*}\n\n\nF1 Score (aka F-Score or F-Measure) \u2013 A helpful metric for comparing two classifiers. F1 Score takes into account precision and the recall. It is created by finding the the harmonic mean of precision and recall.\n\n\n\\begin{equation*}\nF1 = \\frac {2 \\times(precision \\times recall)}{(precision + recall)}\n\\end{equation*}","186aa25c":"# Linear Model 2 <a id='LinearModel2'><\/a>\n\nThe Following plot gives us our first Linear Model along with the summary for the same.\n\n>Training Features :- Age, Potential, Value, Special, International Reputation, Skill Moves, Crossing, Finishing, HeadingAccuracy, Volleys, Curve, LongPassing, Acceleration, Reactions, Balance, ShotPower, Jumping, Stamina, Strength, LongShots, Aggression, Interceptions, Vision, Penalties, Composure, Marking, GKDiving\n\n>Target Feature:- Overall\n\n[Back to top](#LinearRegression)","c41ad331":"# <p style=\"text-align: center;\"> Table of Contents <\/p>\n- ## 1. [Introduction](#Introduction)\n   - ### 1.1 [Abstract](#abstract)\n   - ### 1.2 [Importing Libraries](#importing_libraries)\n   - ### 1.3 [Dataset Summary](#dataset_summary)\n   - ### 1.4 [Dataset Cleaning](#dataset_cleaning)\n   - ### 1.5 [Exploratory Data Analysis (EDA)](#eda)\n       - ### 1.5.1 [Heatmap](#heatmap)\n       - ### 1.5.2 [Regplot](#regplot)\n- ## 2. [Linear Regression](#LinearRegression)\n    - ### 2.1 [Linear Model 1](#LinearModel1)\n    - ### 2.2 [Linear Model 2](#LinearModel2)\n    - ### 2.3 [Linear Model 3](#LinearModel3)\n    - ### 2.4 [Linear Model 3](#LinearModel3)\n- ## 3. [Logistic Regression](#LogisticRegression)\n    - ### 3.1 [Logistic Model 1](#LogisticModel1)\n    - ### 3.2 [Logistic Model 2](#LogisticModel2)\n    - ### 3.3 [Logistic Model 3](#LogisticModel3)\n- ## 4. [Multicollinearity](#Multicollinearity)\n- ## 5. [Stepwise Regression](#StepwiseRegression)\n- ## 6. [Interaction Effect](#InteractionEffect)\n    - ### 6.1 [All Columns](#AllColumns)\n    - ### 6.2 [International Reputation](#InternationalReputation)\n    - ### 6.3 [Age](#Age)\n    - ### 6.4 [Potential](#Potential)\n    - ### 6.5 [International Reputation & Age](#InternationalReputation&Age)\n    - ### 6.6 [International Reputation & Potential](#InternationalReputation&Potential)\n    - ### 6.7 [Age & Potential](#Age&Potential)\n    - ### 6.8 [Age & Potential Interaction](#Age&PotentialInteraction)\n- ## 7. [Regularization](#Regularization)\n- ## 8. [Conclusion](#Conclusion)\n- ## 9. [Contribution](#Contribution)\n- ## 10. [Citation](#Citation)\n- ## 11. [License](#License)","da00a90f":"# <p style=\"text-align: center;\">Conclusion<p><a id='Conclusion'><\/a>\n    \n1. On the basis of AIC, BIC and R^2 value for linear regression, we can say that Model 2 is the best, since it has the best values.\n2. Overall is highly correlated with most of the independent variables(since overall is an average of all the fields)\n3. Generally players with high Wage, Value, Release Clause and International Reputation have high overall.\n4. Other factors that affect the Overall are dependant on the position.(i.e. A goalkeeper will have a high overall if their goalkeeping stats are high and so on..)\n5. In logistic regression, we have tried predicting the PF_Label(Preferred Foot) in model 1 & 3, and Attacking Position in model 2. We can say that predicting whether the player is pkaying in an attacking position or not, based on the factors is easier rather than predicting which footed the player is.\n6. The dataset has very high multicollinearity and thus high VIF, that makes sense because the independent variables are somehat reliant on one other.\n7. Selecting features just on the basis of and regular regression coefficients and p-value doesn't give very good models.\n8. Our dataset also contains multicollinearity, all the independent variables are somewhat related to each other as we can see in our results.\n9. After concluding that Multicollinearity does exist in our dataset, and when we try to remove highly multicollinear variables the value of R^2 drops , thus making us conclude that our dataset is not fit for both linear regression and logistic regression (because assumptions for same are violated).\n10. After stepwise regression, we conclude that the most significant variables that should be used for regression with our dependent variable are 'Potential', 'Age', 'Release Clause', 'Reactions', 'International Reputation', 'Stamina', 'GKDiving', 'Special', 'HeadingAccuracy', 'Balance', 'SprintSpeed', 'ShortPassing', 'Positioning', 'Composure', 'GKHandling', 'ShotPower', 'GP_Label', 'BallControl', 'Vision', 'GKPositioning', 'Skill Moves', 'LongShots' (with p-value < 0.05).\n11. Regularization as such as no effect on the model, though there is a slight increase in the accuracy and cross_val_score but it is not that big that we should do it.\n12. Interaction effect for (spm*rspm) is maximum and that we have utilised and and has gotten good R^2 value\n\n[Back to top](#Introduction)","c60f420d":"### Standardizing the Financial Values\nBefore we fill in the missing values in our dataset, we first need to standardize our financial values, i.e. since the values are in thousands(K) and millions(M), we need to stardadize them to units place so that analysis can be run on it.","55c814f1":"# Linear Model 3 <a id='LinearModel3'><\/a>\nThe Following plot gives us our first Linear Model along with the summary for the same.\n\n>Training Features :- Potential, Value, Wage, International Reputation, Weak Foot, Crossing, Dribbling, Acceleration, Agility, Reactions, Balance, ShotPower, Stamina, Strength, LongShots, Penalties, Composure\n\n>Target Feature:- Overall\n\n[Back to top](#LinearRegression)","53ab1f03":"### VIF of Linear Model 1 Independant variables","fbc25ccf":"## International Reputation and Age <a id='InternationalReputation&Age'><\/a>\n\n[Back to top](#InteractionEffect)","ecd4c79e":"#   1.4 Dataset Cleaning  <a id='dataset_cleaning'><\/a>\nNext we will check if there are any missing or Null values in the dataset.\n\n[Back to top](#Introduction)","b053bcf4":"# Age <a id='Age'><\/a>\n\n[Back to top](#InteractionEffect)","af828a09":"### Finding the location of Null values\nNow that we know that there are missing(null) values in the dataset, we need to find the columns which have missing values and then find the percentage of how much data is missing in those columns to get a better picture.","54be910a":"#  <p style=\"text-align: center;\"> Interaction Effect <\/p> <a id='InteractionEffect'><\/a>\nInteraction effects occur when the effect of one variable depends on the value of another variable. Interaction effects are common in regression analysis. In any study, many variables can affect the outcome. Changing these variables can affect the outcome directly. In more complex study areas, the independent variables might interact with each other. Interaction effects indicate that a third variable influences the relationship between an independent and dependent variable. This type of effect makes the model more complex, but if the real world behaves this way, it is critical to incorporate it in your model. \n\nHere we are taking 3 independant variables and seeing their individual standard error, t score and P values, and these values in presence of each other. And finally we see interaction of age and potential. The 3 independent variables are Age, International Reputation and Potential\n\n[Back to top](#Introduction)"}}