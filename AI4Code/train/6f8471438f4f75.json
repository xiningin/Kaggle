{"cell_type":{"dcbd4c61":"code","95e33a2c":"code","784bb756":"code","6f6e26c8":"code","5a45e95a":"code","a285574a":"code","1e063ca3":"code","3f25f86f":"code","7a62221c":"code","cd91ebd3":"code","bb9420ca":"code","c305afe9":"code","8e125f56":"markdown","65e80e85":"markdown","1ef958b6":"markdown","4ca70e8a":"markdown","aeb404d9":"markdown","b15dc06a":"markdown","d21bc0df":"markdown"},"source":{"dcbd4c61":"!pip install ..\/input\/pytorchlightning\/tensorboard-2.2.0-py3-none-any.whl\n!pip install ..\/input\/pytorchlightning\/pytorch_lightning-0.9.0-py3-none-any.whl\nimport sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')","95e33a2c":"import os\nimport gc\nimport glob\nimport random\nimport itertools\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.decomposition import PCA\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import RandomSampler, SequentialSampler\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\nimport warnings\nwarnings.filterwarnings('ignore')","784bb756":"pl.__version__","6f6e26c8":"class cfg:\n    seed = 42\n    g_comp = 29\n    c_comp = 4\n    fold = 0\n    emb_dims = [(2, 15), (3, 20), (2, 15)]\n    \n    dropout_rate = 0.4\n    hidden_size = 2048\n    \n    batch_size = 128\n    lr = 0.001\n    epoch = 15","5a45e95a":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","a285574a":"seed_everything(cfg.seed)","1e063ca3":"class MoADataset(Dataset):\n    def __init__(self, df, feature_cols, target_cols, phase='train'):\n        self.df = df\n        self.cat_cols = ['cp_type', 'cp_time', 'cp_dose']\n        self.cont_cols = [c for c in feature_cols if c not in self.cat_cols]\n        self.target_cols = target_cols\n        self.phase = phase\n        \n        self.cont_features = self.df[self.cont_cols].values\n        self.cat_features = self.df[self.cat_cols].values\n        self.targets = self.df[self.target_cols].values\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        cont_f = torch.tensor(self.cont_features[idx, :], dtype=torch.float)\n        cat_f = torch.tensor(self.cat_features[idx, :], dtype=torch.long)\n\n        if self.phase != 'test':\n            target = torch.tensor(self.targets[idx, :], dtype=torch.float)\n\n            return cont_f, cat_f, target\n\n        else:\n            sig_id = self.df['sig_id'].iloc[idx]\n            return cont_f, cat_f, sig_id","3f25f86f":"class DataModule(pl.LightningDataModule):\n    def __init__(self, data_dir, cfg, cv):\n        super(DataModule, self).__init__()\n        self.cfg = cfg\n        self.data_dir = data_dir\n        self.cv = cv\n\n    def prepare_data(self):\n        # Prepare Data\n        train_target = pd.read_csv(os.path.join(self.data_dir, 'train_targets_scored.csv'))\n        train_feature = pd.read_csv(os.path.join(self.data_dir, 'train_features.csv'))\n        test = pd.read_csv(os.path.join(self.data_dir, 'test_features.csv'))\n\n        train = pd.merge(train_target, train_feature, on='sig_id')\n        self.target_cols = [c for c in train_target.columns if c != 'sig_id']\n\n        test['is_train'] = 0\n        train['is_train'] = 1\n        self.df = pd.concat([train, test], axis=0, ignore_index=True)\n        self.test_id = test['sig_id'].values\n        \n        # Label Encoding\n        self.df = self.Encode(self.df)\n        # add PCA Features\n        self.df = self.add_PCA(self.df, g_comp=self.cfg.g_comp, c_comp=self.cfg.c_comp)\n        self.feature_cols = [c for c in self.df.columns if c not in self.target_cols + ['sig_id', 'is_train', 'fold']]\n\n        del train, train_target, train_feature, test\n        gc.collect()\n\n    def setup(self, stage=None):\n        # Split Train, Test\n        df = self.df[self.df['is_train'] == 1].reset_index(drop=True)\n        test = self.df[self.df['is_train'] == 0].reset_index(drop=True)\n        self.test_id = test['sig_id'].values\n\n        # Split Train, Validation\n        df['fold'] = -1\n        for i, (trn_idx, val_idx) in enumerate(self.cv.split(df, df[self.target_cols])):\n            df.loc[val_idx, 'fold'] = i\n        fold = self.cfg.fold\n        train = df[df['fold'] != fold].reset_index(drop=True)\n        val = df[df['fold'] == fold].reset_index(drop=True)\n\n        self.train_dataset = MoADataset(train, self.feature_cols, self.target_cols, phase='train')\n        self.val_dataset = MoADataset(val, self.feature_cols, self.target_cols, phase='train')\n        self.test_dataset = MoADataset(test, self.feature_cols, self.target_cols, phase='test')\n\n        del df, test, train, val\n        gc.collect()\n\n    def train_dataloader(self):\n        return DataLoader(self.train_dataset,\n                          batch_size=self.cfg.batch_size,\n                          pin_memory=True,\n                          sampler=RandomSampler(self.train_dataset), drop_last=False)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset,\n                          batch_size=self.cfg.batch_size,\n                          pin_memory=True,\n                          sampler=SequentialSampler(self.val_dataset), drop_last=False)\n\n    def test_dataloader(self):\n        return DataLoader(self.test_dataset,\n                          batch_size=self.cfg.batch_size,\n                          pin_memory=False,\n                          shuffle=False, drop_last=False)\n    \n    def Encode(self, df):\n        cp_type_encoder = {\n            'trt_cp': 0,\n            'ctl_vehicle': 1\n        }\n\n        cp_time_encoder = {\n            24: 0,\n            48: 1,\n            72: 2,\n        }\n\n        cp_dose_encoder = {\n            'D1': 0,\n            'D2': 1\n        }\n\n        df['cp_type'] = df['cp_type'].map(cp_type_encoder)\n        df['cp_time'] = df['cp_time'].map(cp_time_encoder)\n        df['cp_dose'] = df['cp_dose'].map(cp_dose_encoder)\n\n        for c in ['cp_type', 'cp_time', 'cp_dose']:\n            df[c] = df[c].astype(int)\n\n        return df\n    \n    \n    def add_PCA(self, df, g_comp=29, c_comp=4):\n        # g-features\n        g_cols = [c for c in df.columns if 'g-' in c]\n        temp = PCA(n_components=g_comp, random_state=self.cfg.seed).fit_transform(df[g_cols])\n        temp = pd.DataFrame(temp, columns=[f'g-pca_{i}' for i in range(g_comp)])\n        df = pd.concat([df, temp], axis=1)\n\n        # c-features\n        c_cols = [c for c in df.columns if 'c-' in c]\n        temp = PCA(n_components=c_comp, random_state=self.cfg.seed).fit_transform(df[c_cols])\n        temp = pd.DataFrame(temp, columns=[f'c-pca_{i}' for i in range(c_comp)])\n        df = pd.concat([df, temp], axis=1)\n\n        del temp\n\n        return df","7a62221c":"class LightningSystem(pl.LightningModule):\n    def __init__(self, net, cfg, target_cols):\n        super(LightningSystem, self).__init__()\n        self.net = net\n        self.cfg = cfg\n        self.target_cols = target_cols\n        self.criterion = nn.BCEWithLogitsLoss()\n        self.best_loss = 1e+9\n\n    def configure_optimizers(self):\n        self.optimizer = optim.AdamW(self.parameters(), lr=self.cfg.lr, weight_decay=2e-5)\n        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=self.cfg.epoch, eta_min=0)\n\n        return [self.optimizer], [self.scheduler]\n\n    def forward(self, cont_f, cat_f):\n        return self.net(cont_f, cat_f)\n\n    def step(self, batch):\n        cont_f, cat_f, label = batch\n        out = self.forward(cont_f, cat_f)\n        loss = self.criterion(out, label)\n\n        return loss, label\n\n    def training_step(self, batch, batch_idx):\n        loss, label = self.step(batch)\n        logs = {'train\/loss': loss.item()}\n\n        return {'loss': loss, 'labels': label}\n\n    def validation_step(self, batch, batch_idx):\n        loss, label = self.step(batch)\n        val_logs = {'val\/loss': loss.item()}\n\n        return {'val_loss': loss, 'labels': label.detach()}\n\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        logs = {'val\/epoch_loss': avg_loss.item()}\n\n        return {'avg_val_loss': avg_loss}\n\n\n    def test_step(self, batch, batch_idx):\n        cont_f, cat_f, ids = batch\n        out = self.forward(cont_f, cat_f)\n        logits = torch.sigmoid(out)\n\n        return {'pred': logits, 'id': ids}\n\n\n    def test_epoch_end(self, outputs):\n        preds = torch.cat([x['pred'] for x in outputs]).detach().cpu().numpy()\n        res = pd.DataFrame(preds, columns=self.target_cols)\n\n        ids = [x['id'] for x in outputs]\n        ids = [list(x) for x in ids]\n        ids = list(itertools.chain.from_iterable(ids))\n\n        res.insert(0, 'sig_id', ids)\n\n        res.to_csv('submission.csv', index=False)\n        \n        return {}","cd91ebd3":"class LinearReluBnDropout(nn.Module):\n    def __init__(self, in_features, out_features, dropout_rate):\n        super(LinearReluBnDropout, self).__init__()\n\n        self.block = nn.Sequential(\n            nn.utils.weight_norm(nn.Linear(in_features, out_features)),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm1d(out_features),\n            nn.Dropout(dropout_rate)\n        )\n\n    def forward(self, x):\n        x = self.block(x)\n\n        return x\n\n\nclass TablarNet(nn.Module):\n    def __init__(self, emb_dims, cfg, in_cont_features=875, out_features=206):\n        super(TablarNet, self).__init__()\n\n        self.embedding_layer = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims])\n        self.dropout = nn.Dropout(cfg.dropout_rate, inplace=True)\n\n        self.first_bn_layer = nn.Sequential(\n            nn.BatchNorm1d(in_cont_features),\n            nn.Dropout(cfg.dropout_rate)\n        )\n\n        first_in_feature = in_cont_features + sum([y for x, y in emb_dims])\n\n        self.block = nn.Sequential(\n            LinearReluBnDropout(in_features=first_in_feature,\n                                out_features=cfg.hidden_size,\n                                dropout_rate=cfg.dropout_rate),\n            LinearReluBnDropout(in_features=cfg.hidden_size,\n                                out_features=cfg.hidden_size,\n                                dropout_rate=cfg.dropout_rate)\n        )\n\n        self.last = nn.Linear(cfg.hidden_size, out_features)\n\n    def forward(self, cont_f, cat_f):\n\n        cat_x = [layer(cat_f[:, i]) for i, layer in enumerate(self.embedding_layer)]\n        cat_x = torch.cat(cat_x, 1)\n        cat_x = self.dropout(cat_x)\n\n        cont_x = self.first_bn_layer(cont_f)\n\n        x = torch.cat([cont_x, cat_x], 1)\n\n        x = self.block(x)\n        x = self.last(x)\n\n        return x","bb9420ca":"def main():\n    # Set data dir\n    data_dir = '..\/input\/lish-moa'\n    # CV\n    cv = MultilabelStratifiedKFold(n_splits=4)\n    # Random Seed\n    seed_everything(cfg.seed)\n\n    # Lightning Data Module  ####################################################\n    datamodule = DataModule(data_dir, cfg, cv)\n    datamodule.prepare_data()\n    target_cols = datamodule.target_cols\n    feature_cols = datamodule.feature_cols\n\n    # Model  ####################################################################\n    # Adjust input dim (original + composition dim - category features)\n    in_features = len(feature_cols) - 3\n    net = TablarNet(cfg.emb_dims, cfg, in_cont_features=in_features)\n\n    # Lightning Module  #########################################################\n    model = LightningSystem(net, cfg, target_cols)\n\n    checkpoint_callback = ModelCheckpoint(\n        save_top_k=1,\n        verbose=False,\n        monitor='avg_val_loss',\n        mode='min'\n    )\n    \n    early_stop_callback = EarlyStopping(\n        monitor='avg_val_loss',\n        min_delta=0.00,\n        patience=3,\n        verbose=False,\n        mode='min'\n    )\n\n    trainer = Trainer(\n        logger=False,\n        max_epochs=cfg.epoch,\n        checkpoint_callback=checkpoint_callback,\n        early_stop_callback=early_stop_callback,\n#         gpus=1\n            )\n\n    # Train & Test  ############################################################\n    # Train\n    trainer.fit(model, datamodule=datamodule)\n\n    # Test\n    trainer.test(model, datamodule=datamodule)","c305afe9":"main()","8e125f56":"# MoA - Pytorch Lighting","65e80e85":"---\n## Definition of Pytorch Dataset","1ef958b6":"---\n## Lightning Module","4ca70e8a":"---\n## Model Definition","aeb404d9":"---\n## Config","b15dc06a":"---\n## Lightning Data Module","d21bc0df":"---\n## Library Install"}}