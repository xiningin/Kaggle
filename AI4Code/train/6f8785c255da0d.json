{"cell_type":{"74c798ea":"code","f793ae2e":"code","5c30932e":"code","23a1ae88":"code","ebb2fa75":"code","139d812a":"code","6d748fcc":"code","27d1184b":"code","f0df9cc7":"code","b94f2f36":"code","0fc5b67a":"code","c0e29e97":"markdown","c26834c6":"markdown","3805027a":"markdown","bc333eb3":"markdown","c5ba9872":"markdown","092a9215":"markdown","a5a61195":"markdown","0b0f017f":"markdown","e3098d21":"markdown","1e6e05d7":"markdown"},"source":{"74c798ea":"from hyperopt import hp, fmin, tpe, rand, STATUS_OK, Trials\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport time\n\nseed = 42 # Set seed for reproducibility purposes\nmetric = 'accuracy' # See other options https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html\nkFoldSplits = 5\n\nnp.random.seed(seed) # Set numpy seed for reproducibility\n\n# Create a toy-dataset using make_classification function from scikit-learn\nX,Y=make_classification(n_samples=1000,\n                        n_features=25,\n                        n_informative=2,\n                        n_redundant=10,\n                        n_classes=2,\n                        random_state=seed)\n\n# Split in train-test-validation datasets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)\nX_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=0.25, random_state=seed) # 0.25 x 0.8 = 0.2\n\n# Check on created data\nprint(\"Training features size:   %s x %s\\nTesting features size:    %s x %s\\nValidation features size: %s x %s\\n\" % (X_train.shape[0],X_train.shape[1], \n                                                                                                                     X_test.shape[0],X_test.shape[1], \n                                                                                                                     X_validation.shape[0],X_validation.shape[1]))\n\n# Create a function to print variable name\ndef namestr(obj, namespace = globals()):\n    return [name for name in namespace if namespace[name] is obj]\n\n# Check on class distribution\nfor x in [Y_train, Y_test, Y_validation]:\n    print(namestr(x)[0])\n    counter = Counter(x)\n    for k,v in counter.items():\n        pct = v \/ len(x) * 100\n        print(\"Class: %1.0f, Count: %3.0f, Percentage: %.1f%%\" % (k,v,pct))\n    print(\"\")","f793ae2e":"space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n       'gamma': hp.uniform ('gamma', 1, 9),\n       'reg_alpha' : hp.quniform('reg_alpha', 40, 180, 1),\n       'reg_lambda' : hp.uniform('reg_lambda', 0, 1),\n       'colsample_bytree' : hp.uniform('colsample_bytree', 0.5, 1),\n       'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n       'n_estimators': hp.quniform('n_estimators', 50, 250, 1)}","5c30932e":"# If regression, then: \ndef hyperparameter_tuning(space):\n    global best_score\n    \n    reg=xgb.XGBRegressor(n_estimators = int(space['n_estimators']), max_depth = int(space['max_depth']), gamma = space['gamma'],\n                         reg_alpha = int(space['reg_alpha']),min_child_weight=space['min_child_weight'],\n                         colsample_bytree=space['colsample_bytree'])\n    \n    evaluation = [(X_train, Y_train), (X_test, Y_test)]\n    \n    reg.fit(X_train, y_train,\n            eval_set = evaluation, eval_metric = \"rmse\",\n            early_stopping_rounds = 10,verbose = False)\n\n    pred = reg.predict(X_test)\n    mse = mean_squared_error(Y_test, pred)\n    \n    if (mse < best_score):\n        best_score=mse\n        \n    # Change the metric according to the needs\n    return {'loss':mse, 'status': STATUS_OK}\n    \n# If classifier (our case), then:\ndef hyperparameter_tuning(space):\n    global best_score\n    \n    clf = XGBClassifier(n_estimators = int(space['n_estimators']), max_depth = int(space['max_depth']), gamma = space['gamma'],\n                        reg_alpha = int(space['reg_alpha']),min_child_weight=space['min_child_weight'],\n                        colsample_bytree=space['colsample_bytree'])\n    \n    evaluation = [(X_train, Y_train), (X_test, Y_test)]\n    \n    clf.fit(X_train, Y_train,\n            eval_set = evaluation, eval_metric = 'logloss',\n            early_stopping_rounds = 10, verbose = False)\n\n    pred = clf.predict(X_test)\n    accuracy = 1-accuracy_score(Y_test, pred>0.5)\n    \n    if (accuracy < best_score):\n        best_score = accuracy\n    \n    # Change the metric according to the needs\n    return {'loss': accuracy, 'status': STATUS_OK }","23a1ae88":"trials = Trials()\nstart = time.time()\nneval = 100\nbest_score = 1.0\n\nbest = fmin(fn = hyperparameter_tuning,\n            space = space,\n            algo = tpe.suggest,\n            max_evals = neval,\n            trials = trials,\n            rstate = np.random.RandomState(seed))\n\nelapsed_time = time.time() - start","ebb2fa75":"print(\"Parameters optimization took %.0f seconds for %d candidates. Accuracy reached: %.3f\\nOptimal parameters found:\\n%s\" % (elapsed_time, neval, (1-best_score), best))","139d812a":"space={'max_depth':hp.quniform('max_depth', 1, 50, 1),\n       'eta':hp.uniform ('eta', 0, 0.5),\n       'subsample':hp.uniform ('subsample', 0, 1),\n       'colsample_bylevel':hp.uniform ('colsample_bylevel', 0, 1),\n       'colsample_bytree':hp.uniform ('colsample_bytree', 0, 1),\n       'n_estimators':hp.quniform('n_estimators', 25, 500, 5),\n       'gamma': hp.uniform ('gamma', 1, 25),\n       'reg_alpha' : hp.quniform('reg_alpha', 25, 500, 1),\n       'reg_lambda' : hp.uniform('reg_lambda', 0, 1),\n       'min_child_weight' : hp.quniform('min_child_weight', 0, 50, 1)}","6d748fcc":"# Considering only our classification tuning:\ndef xgboost_tuning(space, kFoldSplits = 5, seed = 42, metric = 'accuracy'):\n    \n    global best_score, score_history, best_score_history\n    \n    clf = XGBClassifier(eta = space['eta'],\n                        subsample = space['subsample'],\n                        n_estimators = int(space['n_estimators']), \n                        max_depth = int(space['max_depth']), \n                        gamma = space['gamma'],\n                        reg_alpha = int(space['reg_alpha']),\n                        reg_lambda = space['reg_lambda'],\n                        min_child_weight=space['min_child_weight'],\n                        colsample_bytree=space['colsample_bytree'],\n                        colsample_bylevel=space['colsample_bylevel'], n_jobs=-1)\n    \n    kfold = StratifiedKFold(n_splits=kFoldSplits, random_state=seed, shuffle=True)\n    accuracy = 1-cross_val_score(clf, X_train, Y_train, cv=kfold, scoring=metric, verbose=False).mean() \n    \n    if (accuracy < best_score):\n        best_score = accuracy\n    \n    best_score_history.append(1-best_score)\n    score_history.append(1-accuracy)\n    \n    # Change the metric according to the needs\n    return {'loss': accuracy, 'status': STATUS_OK}","27d1184b":"trials = Trials()\nstart = time.time()\nneval = 500\nbest_score = 1\nscore_history = []\nbest_score_history = []\n\nbest = fmin(fn = xgboost_tuning,\n            space = space,\n            algo = tpe.suggest,\n            max_evals = neval,\n            trials = trials,\n            rstate = np.random.RandomState(seed))\n\nelapsed_time = time.time() - start","f0df9cc7":"print(\"Parameters optimization took %.0f seconds for %d candidates. Accuracy reached: %.3f\\n\\nOptimal parameters found:\\n%s\" % (elapsed_time, neval, (1-best_score), best))","b94f2f36":"model = XGBClassifier(eta = best['eta'],\n                      subsample = best['subsample'],\n                      n_estimators = int(best['n_estimators']), \n                      max_depth = int(best['max_depth']), \n                      gamma = best['gamma'],\n                      reg_alpha = best['reg_alpha'],\n                      reg_lambda = best['reg_lambda'],\n                      min_child_weight=best['min_child_weight'],\n                      colsample_bytree=best['colsample_bytree'],\n                      colsample_bylevel=best['colsample_bylevel'],\n                      n_jobs=-1)\n\nevaluation = [(X_train, Y_train), (X_test, Y_test)]\n\nmodel.fit(X_train, Y_train,\n          eval_set = evaluation, eval_metric = 'logloss',\n          early_stopping_rounds = 10, verbose = False)\n\npred = model.predict(X_test)\naccuracy = 1-accuracy_score(Y_test, pred>0.5)","0fc5b67a":"print(\"Accuracy reached with best params on test set: %.3f\" % (1-accuracy))","c0e29e97":"### Step 2: Define objective function\n### Standard approach","c26834c6":"### Step 2: Define objective function\n### k-Fold Cross Validation approach","3805027a":"### Step 3: Run Hyperopt function\n### Standard approach","bc333eb3":"Example from https:\/\/medium.com\/analytics-vidhya\/hyperparameter-tuning-hyperopt-bayesian-optimization-for-xgboost-and-neural-network-8aedf278a1c9","c5ba9872":"# Hyperparameter tuning on XGBoost","092a9215":"### Step 4: Fit model using best parameters\n### k-Fold Cross Validation approach","a5a61195":"## XGBoost\n### Step 1: Initialize space or a required range of values","0b0f017f":"**Other available hyperopt optimization algorithms are can be found [here](https:\/\/github.com\/hyperopt\/hyperopt\/wiki\/FMin).**\n* hp.choice(label, options) \u2014 Returns one of the options, which should be a list or tuple.<br\/>\n* hp.randint(label, upper) \u2014 Returns a random integer between the range (0, upper), 0 included.<br\/>\n* hp.uniform(label, low, high) \u2014 Returns a value uniformly between low and high.<br\/>\n* hp.quniform(label, low, high, q) \u2014 Returns a value round(uniform(low, high) \/ q) * q, i.e it rounds the decimal values and returns an integer.<br\/>\n* hp.normal(label, mean, std) \u2014 Returns a real value that\u2019s normally-distributed with mean and standard deviation sigma.","e3098d21":"### Step 3: Run Hyperopt function\n### k-Fold Cross Validation approach","1e6e05d7":"### Step 0: Load required packages and create a toy-dataset"}}