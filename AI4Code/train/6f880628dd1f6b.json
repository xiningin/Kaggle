{"cell_type":{"cae5235b":"code","688296f3":"code","45a10e22":"code","9fdb37c9":"code","98dde7bf":"code","2e13fd46":"code","940cc063":"code","686419f4":"code","12a7db93":"code","c2cdee16":"code","333ae637":"code","78bfd251":"code","c3e598f8":"code","689aa23d":"code","804db7cd":"code","58c7fa50":"code","68f76780":"code","b400fdb9":"code","c27d51d0":"code","3ce50c9b":"code","9186dc93":"code","bb29f466":"code","f0a611fb":"code","73b283e0":"markdown","8b38a0f0":"markdown","20f51e27":"markdown","daaa4625":"markdown","18572ff7":"markdown","32abaea2":"markdown","bcf5c243":"markdown","fb60acc4":"markdown"},"source":{"cae5235b":"#Importing necessary modules\nimport numpy as np \nimport pandas as pd \nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport string\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Conv1D\nfrom keras.optimizers import Adam\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\nfrom nltk.corpus import stopwords","688296f3":"#Load kaggle dataset into dataframe\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndf = pd.read_csv('\/kaggle\/input\/trip-advisor-hotel-reviews\/tripadvisor_hotel_reviews.csv')","45a10e22":"#Examine first few rows of dataframe\ndf.head()","9fdb37c9":"#Label count for each label\ndf['Rating'].value_counts()","98dde7bf":"#Visualzing the label count\nsns.countplot(df.Rating)","2e13fd46":"#converting review text to lower case\n\ndataset = df['Review'].str.lower()","940cc063":"#removing the punctuation marks\n\nPUNCTUATION = string.punctuation\ndef remove_punctuation(text):\n    return text.translate(str.maketrans('', '', PUNCTUATION))\n\ndf[\"Review\"] = df[\"Review\"].apply(lambda sentence: remove_punctuation(sentence))\ndf.head()","686419f4":"#removing the words whose length is less than 2 and also numbers if present\ndef remove_stopwords(text):\n    return \" \".join([word for word in str(text).split() if ((len(word)>2) and (word.isalpha()))])\n\ndf[\"Review\"] = df[\"Review\"].apply(lambda sentence: remove_stopwords(sentence))\ndf.head()","12a7db93":"#stop words list\nstop_words_list = stopwords.words('english')","c2cdee16":"#removing the stop words from the review text\ndef remove_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in stop_words_list])\n\ndf[\"Review\"] = df[\"Review\"].apply(lambda sentence: remove_stopwords(sentence))\ndf.head()","333ae637":"#removing blank characters from the review text\ndef remove_blankCharacters(text):\n    return \" \".join([word for word in str(text).split() if word not in ['',' ']])\n\ndf[\"Review\"] = df[\"Review\"].apply(lambda sentence: remove_blankCharacters(sentence))\ndf.head()","78bfd251":"#Calculating average length of the sentences\ndef calculate_length(text):\n    words = [word for word in str(text).split()]\n    return len(words)\n\ndf[\"length\"] = df[\"Review\"].apply(lambda sentence: calculate_length(sentence))\navg_len = df[\"length\"].mean()\ndf.head()\nprint(\"Average length of the sentences is {}\".format(avg_len))","c3e598f8":"#Calculating maximum and minimum length of review sentences\nmaximum = df['length'].max()\nminimum = df['length'].min()\n\nprint('maximum length is {}, minimum length is {}'.format(maximum,minimum))\ndf.head()","689aa23d":"#Preparing the labels and the train data\nle = LabelEncoder()\nY = le.fit_transform(df.Rating)\nY = Y.reshape(-1,1)\nX = df['Review'].tolist()","804db7cd":"#splitting the data into train and test\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=0, stratify=Y)","58c7fa50":"print('No. of train samples: {}'.format(len(X_train)))\nprint('No. of train labels: {}\\n'.format(len(Y_train)))\nprint('No. of test samples: {}'.format(len(X_test)))\nprint('No. of test labels: {}'.format(len(Y_test)))","68f76780":"#convert train labels into one hot vectors\nY_train_one_hot_labels = to_categorical(Y_train)","b400fdb9":"#Tokenizing, converting to sequences and padding the train data \nmax_words = 3000\nmax_len = 200\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(X_train)\nsequences = tokenizer.texts_to_sequences(X_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","c27d51d0":"#model\ninputs = Input(name='inputs',shape=[max_len])\nlayer = Embedding(max_words,100,embeddings_initializer=\"uniform\",input_length=max_len)(inputs)\nlayer = Conv1D(128, 5, activation='relu')(layer)\nlayer = LSTM(128)(layer)\nlayer = Dropout(0.2)(layer)\nlayer = Dense(512, name='FC1', activation='relu')(layer)\nlayer = Dropout(0.2)(layer)\nlayer = Dense(128, name='FC2')(layer)\nlayer = Dropout(0.2)(layer)\nlayer = Dense(64, name='FC3')(layer)\nlayer = Dense(5, name='out_layer')(layer)\nlayer = Activation('softmax')(layer)\nmodel = Model(inputs=inputs,outputs=layer)\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])","3ce50c9b":"#Training\n\nmodel.fit(sequences_matrix,Y_train_one_hot_labels,batch_size=128,epochs=5,\n          validation_split=0.1)","9186dc93":"#convert test data into sequences and padding them\ntest_sequences = tokenizer.texts_to_sequences(X_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)","bb29f466":"#Calculating the accuracy of the model\nY_test_one_hot_labels = to_categorical(Y_test)\naccr = model.evaluate(test_sequences_matrix,Y_test_one_hot_labels)","f0a611fb":"print('Loss: {:0.3f}  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","73b283e0":"### Splitting the dataset into train and test\n\n#### I am training on 80% data and testing on rest 20% data","8b38a0f0":"#### Testing on test data","20f51e27":"### Note: You can also perform other preprocessing techniques like Stemming, Lemmatization, removing urls, spelling correction, etc ","daaa4625":"Tokenizing - Before training the model, you need to first tokenize the text into tokens, then convert them into sequences and pad them according to the maximum length.\nmax length is taken into account so that no sentence has length greater than or less than that.\n\nPadding is done to sentences with length less than max length assumed. Basically to ensure that all the sequences which are given as input have the same length.","18572ff7":"### Loading the Dataset into dataframe","32abaea2":"### Preprocessing\n1. First converting the review column data into lower case.\n2. Removing the punctuation marks.\n3. Removing words whose length to less than 2 and also those words which have characters other than alphabets\n4. Removing stop words from the review text.\n5. Removing blank characters if any.","bcf5c243":"#### Training","fb60acc4":"#### Below is a simple CNN+LSTM model"}}