{"cell_type":{"aeba0e33":"code","804d35b1":"code","d5281e5c":"code","dcc50856":"code","8aa912a9":"code","d73ed0d2":"code","e9476e59":"code","69f52714":"code","71b91207":"code","d3dd225b":"code","7ab6f650":"code","55133393":"code","eceb2a5d":"code","4d65983a":"code","6b8d075a":"code","846be1b7":"code","80abd9c1":"code","768e7d04":"markdown","f76f35bc":"markdown","62fbdb31":"markdown","a951b75c":"markdown","8f17872c":"markdown","342d3335":"markdown","cad4bc7a":"markdown","19958b8e":"markdown","5156b232":"markdown","8f24f453":"markdown","1d7f5b4a":"markdown","ac1d1256":"markdown","a4564a74":"markdown","84832cc1":"markdown","93307c9a":"markdown","b6965e93":"markdown"},"source":{"aeba0e33":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport os\n\nimport seaborn as sns\nfrom scipy.stats import pearsonr\nfrom lightgbm import LGBMRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport gc \nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport plotly.express as px","804d35b1":"DATA_PATH = Path('\/kaggle\/input\/ubiquant-market-prediction')\n!ls -ilsh $DATA_PATH","d5281e5c":"pd.read_csv(DATA_PATH\/\"example_sample_submission.csv\")","dcc50856":"pd.read_csv(DATA_PATH\/\"example_test.csv\")","8aa912a9":"%%time\ntrain = pd.read_parquet('..\/input\/ubiquant-parquet\/train_low_mem.parquet')\ntrain.sample(5)","d73ed0d2":"# copy from https:\/\/www.kaggle.com\/edwardcrookenden\/eda-and-lgbm-baseline\nnum_data_points = len(train)\nprint(f'We have {num_data_points} data points')\n\n# copy from https:\/\/www.kaggle.com\/edwardcrookenden\/eda-and-lgbm-baseline\nnum_investments = train['investment_id'].nunique()\nmissing_investments_ids = set(range(3773)) - set(train['investment_id'].unique())\nprint(f\"We have {num_investments} unique investments, \", \\\n          f\"min investment_id: {min(train['investment_id'].unique())}, \" \\\n          f\"max investment_id: {max(train['investment_id'].unique())} \")\n\n# copy from https:\/\/www.kaggle.com\/edwardcrookenden\/eda-and-lgbm-baseline\nnum_time_intervals = train['time_id'].nunique()\nmissing_investments_ids = set(range(1219)) - set(train['time_id'].unique())\nprint(f\"We have {num_time_intervals} unique time_id, \", \\\n          f\"min time_id: {min(train['time_id'].unique())}, \" \\\n          f\"max time_id: {max(train['time_id'].unique())} \")\n\nprint(f\"Number of features with null values: {train.isna().sum().sum()}\")","e9476e59":"df_ov = train[['row_id','time_id','investment_id','target']]\ndf_ov.sample(10)","69f52714":"# it validated our idea \n(df_ov['row_id'] != df_ov['time_id'].astype(\"str\") + \"_\" + df_ov['investment_id'].astype(str)).sum()","71b91207":"f, ax = plt.subplots(3,1,figsize=(18,6))\nsns.distplot(df_ov['target'], ax=ax[0])\nax[0].set_title('The distribution of target')\n\nsns.distplot(df_ov['time_id'], ax=ax[1])\nax[1].set_title('The distribution of time_id')\n\nsns.distplot(df_ov['investment_id'], ax=ax[2])\nax[2].set_title('The distribution of investment_id')\n\nplt.tight_layout()","d3dd225b":"df_ov.groupby(['investment_id','time_id']).agg(\"count\").value_counts()","7ab6f650":"f, ax = plt.subplots(2,1,figsize=(20,6))\nax[0].plot(df_ov.groupby(['investment_id'])['time_id'].agg('count'))\nax[0].set_xlabel(\"investment_id\")\nax[0].set_ylabel(\"samples num\")\nax[0].set_title('number of samples per invesment')\n\ntmp = df_ov.groupby(['investment_id'])['time_id'].agg('count')\nx = tmp.index.values\ny = tmp.cumsum().values\nax[1].plot(x, y)\nax[1].set_xlabel(\"investment_id\")\nax[1].set_ylabel(\"cumulative samples num\")\nax[1].set_title(\"cumulative samples num ordered by investment_id\")\n\n\nplt.tight_layout()\nplt.show()\n\nimport numpy as np\nfrom scipy.stats import linregress\nfrom sklearn.metrics import mean_squared_error\n# scipy linear regression\nslope, intercept, r_value, p_value, std_err = linregress(x, y)\ny_pred = intercept + slope * x\n\nmse = mean_squared_error(y_true=y, y_pred=y_pred, squared=True)\nrmse = mean_squared_error(y_true=y, y_pred=y_pred, squared=False)\n\nprint('scipy r_value: {:.20f}'.format(r_value))\nprint('scipy p_value: {:.20f}'.format(p_value))\nprint('scipy intercept: {:.6f}'.format(intercept))\nprint('scipy slope: {:.6f}'.format(slope))\nprint('scipy MSE: {:.6f}'.format(mse))\nprint('scipy RMSE: {:.6f}'.format(rmse))","55133393":"f, ax = plt.subplots(2,1,figsize=(20,6))\nax[0].plot(df_ov.groupby(['time_id'])['investment_id'].agg('count'))\nax[0].set_xlabel(\"time_id\")\nax[0].set_ylabel(\"investment_id count\")\nax[0].set_title('groupby time_id and count by investment_id')\n\ntmp = df_ov.groupby(['time_id'])['investment_id'].agg('count')\nx = tmp.index.values\ny = tmp.cumsum().values\nax[1].plot(x, y)\nax[1].set_xlabel(\"time_id\")\nax[1].set_ylabel(\"cumulative samples num\")\nax[1].set_title(\"cumulative samples num ordered by investment_id\")\n\n\nplt.tight_layout()\nplt.show()\n\nimport numpy as np\nfrom scipy.stats import linregress\nfrom sklearn.metrics import mean_squared_error\n# scipy linear regression\nslope, intercept, r_value, p_value, std_err = linregress(x, y)\ny_pred = intercept + slope * x\n\nmse = mean_squared_error(y_true=y, y_pred=y_pred, squared=True)\nrmse = mean_squared_error(y_true=y, y_pred=y_pred, squared=False)\n\nprint('scipy r_value: {:.20f}'.format(r_value))\nprint('scipy p_value: {:.20f}'.format(p_value))\nprint('scipy intercept: {:.6f}'.format(intercept))\nprint('scipy slope: {:.6f}'.format(slope))\nprint('scipy MSE: {:.6f}'.format(mse))\nprint('scipy RMSE: {:.6f}'.format(rmse))","eceb2a5d":"f, ax = plt.subplots(1,1,figsize=(22,4))\nax.plot(df_ov.groupby(['time_id'])['target'].mean(), label='target mean')\nax.plot(df_ov.groupby(['time_id'])['investment_id'].agg('count')\/3500, label='groupby time_id and count by investment_id',linestyle='--')\nax.set_xlabel(\"time_id\")\nax.set_ylabel(\"mean target\")\nax.set_xlim(-10, 1250)\nax.legend()\nplt.show()","4d65983a":"f, ax = plt.subplots(1,1,figsize=(22,4))\nax.plot(df_ov.groupby(['investment_id'])['target'].mean()[:1000], label='target mean')\nax.plot((df_ov.groupby(['investment_id'])['time_id'].agg('count')\/1000+0.5)[:1000], label='investment_id count')\n\nax.set_xlabel(\"investment_id\")\nax.set_ylabel(\"target\")\nax.set_xlim(-10, 1000)\nax.legend()\nplt.show()","6b8d075a":"FEATURES = [f\"f_{x}\" for x in range(300)]\ndata = train[['target'] + FEATURES].to_numpy()\ndel train;gc.collect()\ncorrelation = np.corrcoef(data.T)","846be1b7":"f, ax = plt.subplots(1,1,figsize=(24,3))\nsns.distplot(correlation[:, -1], ax=ax)\nplt.title(\"target correlation\")\nplt.show()","80abd9c1":"fig = px.imshow(correlation)\nfig.update_layout(\n    title=\"Correlation matrix\",\n    width = 800, height = 800,\n    autosize = False)\n# sns.heatmap(correlation)","768e7d04":"1. check whether we have duplicate dataset","f76f35bc":"### groupby investment_id and count by time_id\nConcluson:\n\n* disorder by investment_id\n\n* maybe the investment_id is random?\n\n* However, the second figure is the cumulative sample numbers ordered by investment_id. Based on p_value and r_value, is it just a coincidence\uff1f","62fbdb31":"## dataset info\n```row_id``` - A unique identifier for the row.\n\n```time_id``` - The ID code for the time the data was gathered. The time IDs are in order, but the real time between the time IDs is not constant and will likely be shorter for the final private test set than in the training set.\n\n```investment_id``` - The ID code for an investment. Not all investment have data in all time IDs.\n\n```target``` - The target.\n\n```[f_0:f_299]``` - Anonymized features generated from market data.","a951b75c":"## overview to some columns except features columns\nIf you don't familiar with `dask`, we could transform `dask.dataframe` to `pandas.dataframe`.","8f17872c":"3. Is the `target` influenced by `time_id`, `investment_id`?\n\nwe could see that at some time the `target` could have sharp fluctuations. These fluctuations is seems to correspond\nto the the last image \ud83d\udc46. So we also draw it.\n\n\nIt did! Maybe we should have a much more detailed analysis to this fluctuations ","342d3335":"# Overview to the example_sample_submission.csv\n\nAn example submission file provided so the publicly accessible copy of the API provides the correct data shape and format.","cad4bc7a":"Since train.csv is a 18G dataset! Many techniques could be used to load it fast.\nHere are some:\n\n## 1. Dask packages:\n   Details could be seen here: https:\/\/www.kaggle.com\/edwardcrookenden\/eda-and-lgbm-baseline\n    \n   More information about `dask` could be seen here: https:\/\/www.kaggle.com\/yuliagm\/how-to-work-with-big-datasets-on-16g-ram-dask#TIP-#7-Using-Dask\n   \n## 2. Parquet dataset:\n   Details could be seen here: https:\/\/www.kaggle.com\/robikscube\/fast-data-loading-and-low-mem-with-parquet-files?kernelSessionId=85628453\n    \n   Here we just use the processed dataset which is provied by [robikscube](https:\/\/www.kaggle.com\/robikscube), Thank you!\n   \n   You could load this dataset directly in you notebook\n   \n   Dataset Link here: https:\/\/www.kaggle.com\/robikscube\/ubiquant-parquet","19958b8e":"2. Does one investment_id to multiple time_id?","5156b232":"### visualize the distribution of `target`, `time_id` and `investment_id`","8f24f453":"# features","1d7f5b4a":"# Overview to the example_test.csv\n\nRandom data provided to demonstrate what shape and format of data the API will deliver to your notebook when you submit.","ac1d1256":"## all correlations","a4564a74":"# Overview to the train.csv","84832cc1":"Can't see any difference.","93307c9a":"it seems that the `row_id` is the combinaton of `time_id` and `incestment_id`, now we could check it.","b6965e93":"## target correlation"}}