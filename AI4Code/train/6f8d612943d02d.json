{"cell_type":{"aa2f6726":"code","6d2d658e":"code","d8effd86":"code","718c78db":"code","0a9e8833":"code","c7694dfd":"code","7eff7366":"code","998c2b7b":"code","b1dd6821":"code","d725be36":"code","6af3cc38":"code","b362c0e4":"code","75b98471":"code","de9fca71":"code","ce542605":"code","98d19a3b":"code","f16fbaee":"code","b8f2210e":"code","1deefe99":"code","e5fb437a":"code","bb111c62":"code","00b3dcc1":"code","23adec82":"code","077f95bf":"code","bf22e141":"code","28eecb2c":"code","38d1f076":"code","321d83b5":"code","98b09f25":"code","7636e72f":"code","b3298cf1":"markdown","910e28aa":"markdown","1a42eb80":"markdown","0616a269":"markdown","49b3ffdc":"markdown"},"source":{"aa2f6726":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6d2d658e":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Bidirectional, Dense,Dropout,LSTM,Activation, RepeatVector, SimpleRNN\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\nfrom datetime import datetime\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt","d8effd86":"def read_data(data, index_col = 0):\n    data = pd.read_csv(filepath)\n    return data\n\nfilepath = \"..\/input\/hourly-energy-consumption\/AEP_hourly.csv\"\ndf = read_data(filepath, index_col=0)\ndf.head()","718c78db":"# convert dates into datetime type\ndf['Datetime'] = pd.to_datetime(df['Datetime'])\ndf.head()\n\n# df['hour'] = df['Datetime'].dt.year","0a9e8833":"# extract date related features\n\ndef data_conversion(data, index='none'):\n    data = df.copy()\n    \n    data['year'] = data['Datetime'].dt.year\n    data['month'] = data['Datetime'].dt.month\n    data['day'] = data['Datetime'].dt.day\n    data['week'] = data['Datetime'].dt.dayofweek\n    data['yearday'] = data['Datetime'].dt.dayofyear\n    return data\n","c7694dfd":"data = data_conversion(df)\ndata.info()","7eff7366":"data.loc[data['year'] == 2018]\ndf[:107401]","998c2b7b":"# check for null values\n\ndf.isnull().sum()","b1dd6821":"# perform mean min-max normalization \n\ndataset = df.AEP_MW.values.reshape(-1,1)\n\nsclar = MinMaxScaler(feature_range=(0,1))\ndataset = sclar.fit_transform(dataset)\ndataset.shape","d725be36":"def load_data(dataset, seq_len):\n    X_train = []\n    y_train = []\n    for i in range(seq_len, len(dataset)):\n        X_train.append(dataset[i - seq_len: i, 0])\n        y_train.append(dataset[i, 0])\n\n    # last year i.e. 2018 data is used for testing\n    X_test = X_train[107401:]\n    y_test = y_train[107401:]\n\n    # remaining data is used for training.\n    X_train = X_train[:107401]\n    y_train = y_train[:107401]\n\n    # convert to numpy array\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n\n    X_test = np.array(X_test)\n    y_test = np.array(y_test)\n\n    return [X_train, y_train, X_test, y_test]","6af3cc38":"seq_len = 20 #choose sequence length\n\nX_train, y_train, X_test, y_test = load_data(dataset, seq_len)\n\nX_train = X_train.reshape(X_train.shape[0], seq_len, 1)\nX_test = X_test.reshape(X_test.shape[0], seq_len, 1)\n\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","b362c0e4":"model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Bidirectional(LSTM(40, return_sequences=True, input_shape=(X_train.shape[1],1))))\nmodel.add(Bidirectional(LSTM(20,return_sequences= False, activation= 'linear')))\nmodel.add(tf.keras.layers.Dropout(0.5))\n\nmodel.add(Dense(1))\n# model.add(layers.LSTM(256))\n\nmodel.compile(loss='MSE', optimizer = 'adam')\nhist = model.fit(X_train, y_train, epochs=10, validation_split=0.3, batch_size=1000)\nmodel.summary()","75b98471":"import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(14,6))\nax.plot(hist.history['loss'], 'b' ,label = 'train loss', linewidth=2)\nax.plot(hist.history['val_loss'], 'r', label ='Validation loss', linewidth=2)\nax.set_title('model loss')\nax.set_ylabel('mse')\nax.set_xlabel('epoch')\nax.legend()\nplt.show()","de9fca71":"\npred = model.predict(X_test)\n\nmse = mean_squared_error(y_test, pred)\nprint(\"mean sequare error = \",mse)","ce542605":"def plot_predictions(test, predicted, title):\n    plt.figure(figsize=(16, 4))\n    plt.plot(test, color='blue', label='Actual power consumption data')\n    plt.plot(predicted, alpha=0.7, color='red', label='Predicted power consumption data')\n    plt.title(title)\n    plt.xlabel('Time')\n    plt.ylabel('Normalized power consumption scale')\n    plt.legend()\n    plt.show()\n\n\nplot_predictions(y_test, pred, \"Predictions made by model\")","98d19a3b":"seasons = np.array(['Winter', 'Spring', 'Summer', 'Fall', 'Winter'])\nf = np.searchsorted([80, 172, 264, 355], data.yearday)\ndata['season'] = seasons[f]\n\nfig, ax = plt.subplots(figsize=(10,6))\ndata.groupby('season')['AEP_MW'].mean().plot.bar()","f16fbaee":"res = dict(tuple(data.groupby('season')))\nres, res.keys()","b8f2210e":"fall_data = res['Fall']\nspring_data = res['Spring']\nsummer_data = res['Summer']\nwinter_data = res['Winter']","1deefe99":"fig, ax = plt.subplots(figsize=(10,6))\n\nwinter_data.groupby('year')['AEP_MW'].mean().plot.bar()","e5fb437a":"concat_data = [fall_data, spring_data, summer_data]\nfor_train = pd.concat(concat_data)","bb111c62":"for_train = for_train.drop('season',1)\nfor_train = for_train.reset_index()\nfor_train = for_train.drop('index',1)\nfor_train","00b3dcc1":"for_test = winter_data.drop('season', 1)\nfor_test","23adec82":"dataset1 = for_train.AEP_MW.values.reshape(-1,1)\n\nsclar = MinMaxScaler(feature_range=(0,1))\ndataset1 = sclar.fit_transform(dataset1)\n\ndataset11 = for_test.AEP_MW.values.reshape(-1,1)\n\nsclar = MinMaxScaler(feature_range=(0,1))\ndataset11 = sclar.fit_transform(dataset11)\n\ndataset1.shape, dataset11.shape","077f95bf":"# convert an array of values into a dataset matrix\ndef create_dataset(dataset, seq_len):\n    dataX, dataY = [], []\n    for i in range(seq_len, len(dataset)):\n        appnd = dataset[i - seq_len: i, 0]\n        dataX.append(appnd)\n        dataY.append(dataset[i,0])\n    return np.array(dataX), np.array(dataY)","bf22e141":"seq_len = 20 #choose sequence length\n\ntrainX, trainY = create_dataset(dataset1, seq_len)\ntestX, testY = create_dataset(dataset11, seq_len)","28eecb2c":"trainX = trainX.reshape(trainX.shape[0], trainX.shape[1],1)\ntestX = testX.reshape(testX.shape[0], testX.shape[1], 1)\n\ntrainX.shape, testX.shape","38d1f076":"model_s = tf.keras.Sequential()\nmodel_s.add(tf.keras.layers.Bidirectional(LSTM(60, return_sequences=True, input_shape=(trainX.shape[1],1))))\nmodel_s.add(Bidirectional(LSTM(20, return_sequences= False, activation= 'linear')))\nmodel_s.add(tf.keras.layers.Dropout(0.5))\nmodel_s.add(Dense(1))\n# model.add(layers.LSTM(256))\n\nmodel_s.compile(loss='MSE', optimizer = 'adam')\nhist_s = model_s.fit(trainX, trainY, epochs=10, validation_split=0.3, batch_size=1000)\nmodel_s.summary()","321d83b5":"import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(14,6))\nax.plot(hist_s.history['loss'], 'b' ,label = 'train loss', linewidth=2)\nax.plot(hist_s.history['val_loss'], 'r', label ='Validation loss', linewidth=2)\nax.set_title('model loss')\nax.set_ylabel('mse')\nax.set_xlabel('epoch')\nax.legend()\nplt.show()","98b09f25":"# compute mean sequare error\npredictions = model_s.predict(testX)\n\nmse = mean_squared_error(testY, predictions)\nprint(\"mean square error = \", mse)","7636e72f":"def plot_predictions(test, predicted, title):\n    plt.figure(figsize=(16, 4))\n    plt.plot(test, color='blue', label='Actual power consumption data')\n    plt.plot(predicted, alpha=0.7, color='red', label='Predicted power consumption data')\n    plt.title(title)\n    plt.xlabel('Time')\n    plt.ylabel('Normalized power consumption scale')\n    plt.legend()\n    plt.show()\n\n\nplot_predictions(testY, predictions, \"Predictions made by model\")\n","b3298cf1":"# **Bidirectional LSTM**","910e28aa":"# **Data Preparation**\n\n2. In second method we will categorize data based on seasons and predict energy consumption for a season which has height enegry consumption.","1a42eb80":"**Based on above chart in winter, the energy consumption is highest**\n\nHence, for testing we will use winter data and for training we will use fall, spring and summer data.","0616a269":"# **Data preparation**\n\n1.  For data preparation we have used AEP_hourly.csv and for testing we will use last year data and remainging data for training.\n ","49b3ffdc":"**Bidirectional LSTM model**\n\nBelow we have constructed a simple bidirectional LSTM model"}}