{"cell_type":{"5b149333":"code","dbbf34b6":"code","ede71a91":"code","ff9779b5":"code","6b80ed2d":"code","f5a3a93b":"code","8d62fab1":"code","5ae0e9fd":"code","a21e0c07":"code","8be7ded5":"code","876c390d":"code","dcaf770b":"code","7b6cc124":"code","b8ade930":"code","b7b50e7a":"code","fad39503":"code","f5e90c27":"code","0a500d46":"code","9f0cdd26":"code","a8a4f3bb":"code","8963eaf4":"code","09becfcf":"code","dc54ba17":"code","88de0b56":"code","f65655d8":"code","49b8b19f":"code","3ec5c371":"code","d9dc0cf2":"code","8a32313e":"code","aba541c9":"code","e00499ca":"code","16653b5b":"code","a320e54a":"code","b924dc31":"code","b6b25756":"code","ca9305bf":"code","7898af56":"code","86fbeb65":"code","febb115c":"code","dcc134d5":"code","fc48e37f":"code","54b37b86":"markdown","17efc26e":"markdown","3fdbe478":"markdown","53478d35":"markdown","08dc2f07":"markdown","fe23d9e8":"markdown","289b9945":"markdown","5f12eca7":"markdown","8639379f":"markdown","1c5cb14a":"markdown","24b96bb6":"markdown","6e6103a9":"markdown","27200351":"markdown","3c4f0cb3":"markdown","a856eeb8":"markdown","22a04ee3":"markdown","faf0055f":"markdown","c68181c7":"markdown","2a6f2ce1":"markdown","e5b974fc":"markdown","6a759a48":"markdown","df3babc0":"markdown","48385e47":"markdown","bde26486":"markdown","588c5448":"markdown","da5dd28e":"markdown","6c11e0a2":"markdown","ad39dcd3":"markdown","3f9ae6cd":"markdown","03c1af0a":"markdown","9c65d39c":"markdown","3df96b51":"markdown","3aa67061":"markdown","1cdce09d":"markdown","24cdb524":"markdown","361a4671":"markdown","1a985260":"markdown","287c7311":"markdown","c97004c4":"markdown","bbe2e0ab":"markdown","1ac79e14":"markdown","1ebfb45c":"markdown","1e2e6f27":"markdown","71669266":"markdown"},"source":{"5b149333":"import os \nos.chdir(\"..\/input\/parkinsons-disease-speech-signal-features\/\")\n!ls","dbbf34b6":"# !wget https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/00470\/pd_speech_features.rar\n","ede71a91":"# !unrar x pd_speech_features.rar","ff9779b5":"import pandas as pd\ndf = pd.read_csv(\"pd_speech_features.csv\") # import dataset \n\ndf","6b80ed2d":"# df.columns = df.iloc[0]\n# df = df.iloc[1:,].reindex()\n# df","f5a3a93b":"df.columns","8d62fab1":"df.info()","5ae0e9fd":"X = df.iloc[:, 0:754].values  # select the independent variables\ny = df.iloc[:, 754].values    # select the dependent variable and target column","a21e0c07":"from sklearn.preprocessing import StandardScaler\nX = StandardScaler().fit_transform(X)","8be7ded5":"import numpy as np\n\nX_mean = np.mean(X, axis=0)\n# cov_mat = np.cov(X) # another method \ncov_mat = (X - X_mean).T.dot((X - X_mean)) \/ (X.shape[0]-1)\nprint('Covariance matrix \\n%s' %cov_mat)","876c390d":"print('NumPy covariance matrix: \\n%s' %np.cov(X.T))","dcaf770b":"eigenvalues, eigenvectors = np.linalg.eig(cov_mat)\n\nprint('Eigenvectors \\n%s' %eigenvectors[:5])\nprint('\\nEigenvalues \\n%s' %eigenvalues[:5])","7b6cc124":"len(eigenvalues)","b8ade930":"total_of_eigenvalues = sum(eigenvalues)\nvarariance = [(i \/ total_of_eigenvalues)*100 for i in sorted(eigenvalues, reverse=True)]\n\nvarariance[:50]","b7b50e7a":"\nimport matplotlib.pyplot as plt\n\nwith plt.style.context('dark_background'):\n    plt.figure(figsize=(15, 10))\n\n    plt.bar(range(len(eigenvalues)), varariance, alpha=0.8, align='center',\n            label='individual explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()","fad39503":"varariance[0]","f5e90c27":"eigenpairs = [(np.abs(eigenvalues[i]), eigenvectors[:,i]) for i in range(len(eigenvalues))]\n\n# Sorting eigenvalues and eigenvectors from higher values to lower values\neigenpairs.sort(key=lambda x: x[0], reverse=True)\n\neigenpairs[0][0]","0a500d46":"eigenpairs[5][1].shape","9f0cdd26":"# only for 6 features \nmatrix_weighing = np.hstack((eigenpairs[0][1].reshape(754,1),\n                      eigenpairs[1][1].reshape(754,1),\n                      eigenpairs[2][1].reshape(754,1),\n                      eigenpairs[3][1].reshape(754,1),\n                      eigenpairs[4][1].reshape(754,1),\n                      eigenpairs[5][1].reshape(754,1)))\nmatrix_weighing","a8a4f3bb":"Y = X.dot(matrix_weighing)\nY.shape","8963eaf4":"df[\"class\"].unique()","09becfcf":"import matplotlib.pyplot as plt\n\nwith plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n    for lab, col in zip(('0', '1'), ('red', 'green')):\n        plt.scatter(Y[y==lab, 0], Y[y==lab, 1], label=lab, c=col)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.legend(loc='lower center')\n    plt.tight_layout();\n    plt.show();","dc54ba17":"from sklearn.decomposition import PCA\npca = PCA().fit(X)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlim(0,754,1)\nplt.grid()\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')","88de0b56":"# e\u011fitim ve test k\u00fcmelerinin b\u00f6l\u00fcnmesi\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","f65655d8":"# Standard scaler haline getirme verileri\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nprint(\"X_train shape = \",X_train.shape)\nprint(\"X_test shape = \",X_test.shape)","49b8b19f":"# PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 6)\n\nX_train2 = pca.fit_transform(X_train) # sadece bir tane PCA ile \u00e7al\u0131\u015f\u0131yor ayn\u0131 uzayda olmas\u0131 i\u00e7in\nX_test2 = pca.transform(X_test)       # test verisini e\u011fitmiyoruz sadece transform uyguluyoruz\n\nprint(\"X_train2 shape = \",X_train2.shape)\nprint(\"X_test2 shape = \",X_test2.shape)\n","3ec5c371":"#pca d\u00f6n\u00fc\u015f\u00fcm\u00fcnden \u00f6nce gelen Logistic regression\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state=0)\nclassifier.fit(X_train,y_train)","d9dc0cf2":"\n#pca d\u00f6n\u00fc\u015f\u00fcm\u00fcnden sonra gelen LR\nclassifier2 = LogisticRegression(random_state=0)\nclassifier2.fit(X_train2,y_train)","8a32313e":"#Predictions : tahminler\ny_pred = classifier.predict(X_test)    # without PCA\ny_pred2 = classifier2.predict(X_test2) # after PCA","aba541c9":"from sklearn.metrics import confusion_matrix\nfrom sklearn import neighbors, datasets, preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\n#actual \/ PCA olmadan \u00e7\u0131kan sonu\u00e7\nprint(\"Comparison between real and before PCA\")\n\nprint('Accuracy Score:', accuracy_score(y_test, y_pred))\nprint('Confusion matrix \\n',  confusion_matrix(y_test, y_pred))\nprint('Classification \\n', classification_report(y_test, y_pred))","e00499ca":"#actual \/ PCA sonras\u0131 \u00e7\u0131kan sonu\u00e7\nprint(\"Comparison between real and after PCA \")\n\nprint('Accuracy Score:', accuracy_score(y_test, y_pred2))\nprint('Confusion matrix \\n',  confusion_matrix(y_test, y_pred2))\nprint('Classification \\n', classification_report(y_test, y_pred2))\n\n","16653b5b":"\n#Support Vector Machine\nfrom sklearn.svm import SVC\n \n\nclassifier = SVC()\nclassifier.fit(X_train,y_train)\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\naccuracy = accuracy_score(y_test,y_pred)\nprint(\"Support Vector Machine:\")\n\nprint(\"Comparison between real and before PCA\")\n\nprint('Accuracy Score:', accuracy_score(y_test, y_pred))\nprint('Confusion matrix \\n',  confusion_matrix(y_test, y_pred))\nprint('Classification \\n', classification_report(y_test, y_pred))","a320e54a":"\n#Support Vector Machine\nfrom sklearn.svm import SVC\n \n\nclassifier = SVC()\nclassifier.fit(X_train2,y_train)\ny_pred2 = classifier.predict(X_test2)\ncm = confusion_matrix(y_test,y_pred2)\naccuracy = accuracy_score(y_test,y_pred2)\nprint(\"Support Vector Machine:\")\n\nprint(\"Comparison between real and after PCA\")\n\nprint('Accuracy Score:', accuracy_score(y_test, y_pred2))\nprint('Confusion matrix \\n',  confusion_matrix(y_test, y_pred2))\nprint('Classification \\n', classification_report(y_test, y_pred2))","b924dc31":"\nfrom sklearn.model_selection import train_test_split\n\n\nfrom sklearn.tree import DecisionTreeClassifier as DT\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\nclassifier = DT(criterion='entropy', random_state=0)\nclassifier.fit(X_train,y_train)\ny_pred = classifier.predict(X_test)\n\nprint(\"Decision Tree Classifier :\")\n\nprint(\"Comparison between real and before PCA\")\n\nprint('Accuracy Score:', accuracy_score(y_test, y_pred))\nprint('Confusion matrix \\n',  confusion_matrix(y_test, y_pred))\nprint('Classification \\n', classification_report(y_test, y_pred))","b6b25756":"\nfrom sklearn.model_selection import train_test_split\n\n\nfrom sklearn.tree import DecisionTreeClassifier as DT\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\nclassifier = DT(criterion='entropy', random_state=0)\nclassifier.fit(X_train2,y_train)\ny_pred2 = classifier.predict(X_test2)\n\n\nprint(\"Comparison between real and after PCA\")\n\nprint('Accuracy Score:', accuracy_score(y_test, y_pred2))\nprint('Confusion matrix \\n',  confusion_matrix(y_test, y_pred2))\nprint('Classification \\n', classification_report(y_test, y_pred2))","ca9305bf":"\nfrom sklearn import neighbors, datasets, preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nknn = neighbors.KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nprint(\"K-Neighbors Classifier :\")\n\nprint(\"Comparison between real and before PCA\")\n\nprint('Accuracy Score:', accuracy_score(y_test, y_pred))\nprint('Confusion matrix \\n',  confusion_matrix(y_test, y_pred))\nprint('Classification \\n', classification_report(y_test, y_pred))","7898af56":"knn = neighbors.KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train2, y_train)\ny_pred2 = knn.predict(X_test2)\n\nprint(\"Comparison between real and after PCA\")\nprint('Accuracy Score:', accuracy_score(y_test, y_pred2))\nprint('Confusion matrix \\n',  confusion_matrix(y_test, y_pred2))\nprint('Classification \\n', classification_report(y_test, y_pred2))","86fbeb65":"\nfrom sklearn.ensemble import RandomForestClassifier as RF\n\nclassifier = RF(n_estimators=10, criterion='entropy', random_state=0)\nclassifier.fit(X_train,y_train)\ny_pred = classifier.predict(X_test)\n\nprint(\"Random Forest Classifier :\")\n\nprint(\"Comparison between real and before PCA\")\nprint('Accuracy Score:', accuracy_score(y_test, y_pred))\nprint('Confusion matrix \\n',  confusion_matrix(y_test, y_pred))\nprint('Classification \\n', classification_report(y_test, y_pred))","febb115c":"\nfrom sklearn.ensemble import RandomForestClassifier as RF\n\nclassifier = RF(n_estimators=10, criterion='entropy', random_state=0)\nclassifier.fit(X_train2,y_train)\ny_pred2 = classifier.predict(X_test2)\n\nprint(\"Comparison between real and after PCA\")\nprint('Accuracy Score:', accuracy_score(y_test, y_pred2))\nprint('Confusion matrix \\n',  confusion_matrix(y_test, y_pred2))\nprint('Classification \\n', classification_report(y_test, y_pred2))","dcc134d5":"\n#Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train,y_train)\ny_pred = classifier.predict(X_test)\n\nprint(\"Gaussian Naive Bayes :\")\n\nprint(\"Comparison between real and before PCA\")\nprint('Accuracy Score:', accuracy_score(y_test, y_pred))\nprint('Confusion matrix \\n',  confusion_matrix(y_test, y_pred))\nprint('Classification \\n', classification_report(y_test, y_pred))","fc48e37f":"\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train2,y_train)\ny_pred2 = classifier.predict(X_test2)\n\nprint(\"Comparison between real and after PCA\")\nprint('Accuracy Score:', accuracy_score(y_test, y_pred2))\nprint('Confusion matrix \\n',  confusion_matrix(y_test, y_pred2))\nprint('Classification \\n', classification_report(y_test, y_pred2))","54b37b86":"# Step 2: Eigendecomposition - Eigenvalues, Eigenvectors and Eigenspace \nThe eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the \"core\" of a PCA: The eigenvectors (elementary components) determine the directions of the new feature space, and the eigenvalues determine their size. In other words, eigenvalues describe the variance of the data along the new feature axes. Covariance Matrix The classical approach to PCA is to perform eigende composition on the covariance matrix, which is a matrix in which each element represents the covariance between two features. The covariance between two properties is calculated as follows:\n\nCov(X,Y)=\u2211(xi\u2212x\u00af)(yi\u2212y\u00af)N\u22121\n","17efc26e":"**Comparison between real and before PCA**","3fdbe478":"## **2. Model : Support Vector Machines (SVM)**","53478d35":"**After PCA**","08dc2f07":"**The second method for compute the covariance matrix**","fe23d9e8":"## **3. Model : Decision Tree Classifier**","289b9945":"**Preparation of new data set to be used in training models. Principal Component Analysis(PCA) implementation. Feature extraction of the data set. And the size reduction has been done.**","5f12eca7":"**Comparison between real and after PCA**","8639379f":"# **Step 6: Comparison Accurancy 6 Machine Learning Models : before-after PCA**","1c5cb14a":"## **5. Model : Random Forest Classifier**","24b96bb6":"# **Feature Extraction and Dimensionality Reduction with Principal Component Analysis (PCA) and Comparison Accuracy 6 Machine Learning Classification Models: before-after PCA.**\n\nStep 1:Collect Data: UCI Parkinson's Disease Classification Data Set\n\nStep 2: Eigendecomposition - Eigenvalues, Eigenvectors and Eigenspace\n\nStep 3: Primary Component Selection\n\nStep 4: Projection New Feature Space\n\nStep 5: Principal Component Analysis (PCA)\n\nStep 6:  Comparison Accurancy 6 Machine Learning Models : before-after PCA\n\n1. Model : Logistic Regression\n2. Model : Support Vector Machines (SVM)\n3. Model : Decision Tree Classifier\n4. Model : KNN(k-nearest neighbors algorithm)\n5. Model : Random Forest Classifier\n6. Model: Gaussian Naive Bayes\n","6e6103a9":"# **Step 4: Projection in a New Feature Space**","27200351":"**Comparison between real and before PCA**","3c4f0cb3":"## ** 6. Model:  Gaussian Naive Bayes**","a856eeb8":"**Compute the covariance matrix**","22a04ee3":"# **Step 1:Collect Data: UCI Parkinson's Disease Classification Data Set**\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Parkinson%27s+Disease+Classification","faf0055f":"**It is observed that reducing the size with PCA, that is, reducing the number of variables, has a positive effect on the success score of some machine learning classification models. It is possible to produce more effective and faster solutions by taking a small amount of data loss. Reducing dimensions with PCA will provide us with great convenience, especially in studies related to Big Data.**","c68181c7":"**Determining dependent and independent variables of the dataset**","2a6f2ce1":"**Before PCA**","e5b974fc":"**Division of training and test data**","6a759a48":"**Comparison between real and after PCA**","df3babc0":"## **4. Model : KNN(k-nearest neighbors algorithm)**","48385e47":"Projection into the New Feature Space In this last step, we will use the 754 \u00d7 6 dimensional projection matrix W to transform our samples into the new hexahedron through the equation Y = X \u00d7 W.","bde26486":"## **1. Model : Logistic Regression**","588c5448":"**Comparison between real and after PCA**","da5dd28e":"**As seen in the figure, the properties after 350 affect the target column by 0. These do not have any effect on the functioning of the model.**","6c11e0a2":"#**Note: ** I chose the (n_components)top 6 components with the highest variance. anyone can give a different number. It is an optional choice. Decide to process only 6 of the 754 features with the highest variance. It reduces the size very much and enables fast processing and only the most effective features will be processed. \n\n**PCA enabled only 6 variables to be processed instead of 754 variables.**","ad39dcd3":"**Comparison between real and before PCA**","3f9ae6cd":" Data Cleaning and Data Manipulation**","03c1af0a":"**Comparison between real and before PCA**","9c65d39c":"**Compute the Eigenvalues and Eigenvectors**\nWe make an identification on the covariance matrix: All three approaches yield the same eigenvectors and eigenvalue pairs: Identification of the covariance matrix after standardizing the data. Essence composition of the correlation matrix.","3df96b51":"**Data Standardization**\n","3aa67061":"**Implementing Standard scaling data**","1cdce09d":"# **Step 3: Primary Component Selection**\nSorting Eigenpairs (Sorting of self-pairs)\n  The purpose of PCA is to reduce the dimensionality of the original feature space by projecting it into a smaller subspace where the eigenvectors will form the axes. However, the eigenvectors only describe the directions of the new axis, because they all have the same unit length 1.To decide which eigenvector (s) can be omitted without losing too much information, we need to examine the corresponding eigenvalues: Eigenvectors with the lowest eigenvalues carries little information; these can fall. The common approach is to order the eigenvalues from highest to lowest.","24cdb524":"**Comparison between real and before PCA**","361a4671":"# **Step 5: Principal Component Analysis (PCA)**","1a985260":"**Data Set Information:**\n\nThe data used in this study were gathered from 188 patients with PD (107 men and 81 women) with ages ranging from 33 to 87 (65.1\u00c2\u00b110.9) at the Department of Neurology in Cerrahpa\u00c5\u0178a Faculty of Medicine, Istanbul University. The control group consists of 64 healthy individuals (23 men and 41 women) with ages varying between 41 and 82 (61.1\u00c2\u00b18.9). During the data collection process, the microphone is set to 44.1 KHz and following the physician\u00e2\u20ac\u2122s examination, the sustained phonation of the vowel \/a\/ was collected from each subject with three repetitions.\n\n\n**Attribute Information:**\n\nVarious speech signal processing algorithms including Time Frequency Features, Mel Frequency Cepstral Coefficients (MFCCs), Wavelet Transform based Features, Vocal Fold Features and TWQT features have been applied to the speech recordings of Parkinson's Disease (PD) patients to extract clinically useful information for PD assessment.\n\n\n**Relevant Papers:**\n\nProvide references to papers that have cited this data set in the past (if any).","287c7311":"**Success comparison of PCA and non-PCA models**","c97004c4":"**Comparison between real and after PCA**","bbe2e0ab":"**Compute the variance of eigen values**\nWe select only first 6 features for this project","1ac79e14":"**Comparison between real and after PCA**","1ebfb45c":"**Comparison between real and before PCA**","1e2e6f27":"**Projection Matrix** \nThe projection matrix is used to transform the Input data (X) into the new property subspace. The Projection Matrix is a matrix of combined upper k eigenvectors. Here, we reduce the 4-dimensional feature space to a 2-dimensional feature subspace by selecting the \"first 2\" eigenvectors with the highest eigenvalues to construct our 2 dimensional eigenvector matrix.","71669266":"**Comparison between real and before PCA**"}}