{"cell_type":{"c22bc47b":"code","01ea37bf":"code","ab6f15e9":"code","f1af356d":"code","17dee998":"code","8221ee1c":"code","a67901b3":"code","16a2c5f1":"code","6f89b8fe":"code","8d8fea8d":"code","096035c9":"code","dbb99193":"code","f9096a36":"code","29c0ca7d":"code","6de38ad2":"code","4a222e57":"markdown","efc71b22":"markdown","722e19d0":"markdown","aa7a5098":"markdown","ad059573":"markdown","7a91ffc5":"markdown","9478434f":"markdown","f7cc348e":"markdown","3804c826":"markdown","eae3dab5":"markdown","b67f9842":"markdown","988dd985":"markdown","2d9a53bc":"markdown"},"source":{"c22bc47b":"# Standard python libraries\nimport os\nimport time\nimport re\n\n# Installed libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n\n# Imports from our package\n\nfrom xgboost import XGBClassifier, plot_importance\nfrom lightgbm import LGBMClassifier, plot_importance\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold\nimport datetime\nimport pandas as pd\nimport numpy as np\nfrom time import time\n\n\n\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom xgboost import XGBClassifier\n\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import IsolationForest\n\nimport seaborn as sns\n","01ea37bf":"%%time\n\ntrain_data = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntrain_data['target'] = train_data['target'].str.slice(start=6).astype(int) - 1\ntrain_data.head()","ab6f15e9":"test_data = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')\ntest_data.head()","f1af356d":"def create_gr_feats(data):\n    pass\n\nclass bcolors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n    \n\nall_df = pd.concat([train_data, test_data]).reset_index(drop = True)\ncreate_gr_feats(all_df)\ntrain_data, test_data = all_df[:len(train_data)], all_df[len(train_data):]\nprint(train_data.shape, test_data.shape)","17dee998":"le = LabelEncoder()\ntrain_data['target'] = le.fit_transform(train_data['target'])\ntrain_data\n\ny = train_data['target']\nX = train_data.drop(['id', 'target'], axis=1)","8221ee1c":"X.isnull().sum()","a67901b3":"X = (X**(1\/3))","16a2c5f1":"sns.distplot(X['feature_10'])\nplt.title(\"Distribution plot after Cube transformation\")\nsns.despine()\nplt.show()","6f89b8fe":"sns.boxplot(X['feature_30'])\nplt.title(\"Distribution plot after Cube transformation\")\nsns.despine()\nplt.show()","8d8fea8d":"for col in X.columns:\n    X[col] = X[col].fillna(X[col].median())","096035c9":"class ModelStager:\n\n    def __init__(self, penalty, n_folds,\n                 verbose=1, shuffle=True, random_state=1):\n        self._penalty = penalty\n        self._n_folds = n_folds\n        self._verbose = verbose\n        self._random_state = random_state\n        self._shuffle = shuffle\n\n    def _print(self, input_str):\n        time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n        print(bcolors.HEADER + \"[ModelStager | \" + time + \"] \" + bcolors.ENDC + str(input_str)) \n\n    def fit(self, X, y, model, model_type=None):\n        kfold = KFold(n_splits=self._n_folds, shuffle=self._shuffle,\n                  random_state=self._random_state)\n\n        cv_scores = []\n        oof_predictions = pd.DataFrame(index=X.index, columns=range(y.nunique()))\n\n        fold_idx = 0\n\n        for tr_idx, val_idx in kfold.split(X):\n\n            X_tr = X.iloc[tr_idx]\n            X_val = X.iloc[val_idx]\n\n            y_tr = y.iloc[tr_idx]\n            y_val = y.iloc[val_idx]\n\n            if self._verbose:\n                self._print(\"Data_tr shape : \" + str(X_tr.shape))\n\n            fold_idx = fold_idx + 1\n            t = time()\n            \n            if model_type == 'LightAutoML':\n                model.fit_predict(pd.concat([X_tr, y_tr], axis=1), roles = roles)\n            else:\n                model.fit(X_tr, y_tr)\n            \n            if model_type == 'LightAutoML':\n                validation_prediction = model.predict(X_val)\n            else:\n                validation_prediction = model.predict_proba(X_val)\n\n            oof_predictions.iloc[val_idx] = validation_prediction\n\n            cv_score_model = self._penalty(y_val, validation_prediction)\n            cv_scores.append(cv_score_model)\n\n            if self._verbose:\n                self._print(\"Fold %.0f : TEST %.5f | TIME %.2fm (1-fold)\" %\n                            (fold_idx, cv_score_model, (time() - t) \/ 60))\n\n        self._print(\"TEST AVERAGE : %.5f\" % (np.mean(cv_scores)))\n\n        return oof_predictions","dbb99193":"cat_best_params_cb = {'learning_rate': 0.04421824097495285, 'reg_lambda': 23.319572135686258, 'subsample': 0.22509693846883988, 'random_strength': 0.13972768817453876, 'depth': 11, 'min_data_in_leaf': 9, 'num_leaves': 20, 'leaf_estimation_iterations': 2}\ncat_best_params_cb['loss_function'] = 'MultiClass'\ncat_best_params_cb['eval_metric'] = 'MultiClass'\ncat_best_params_cb['verbose'] = False\ncat_best_params_cb['bootstrap_type']= 'Bernoulli'\ncat_best_params_cb['leaf_estimation_method'] = 'Newton'\ncat_best_params_cb['random_state'] = 42\ncat_best_params_cb['n_estimators'] = 1000\ncat_best_params_cb['task_type'] = 'CPU'\ncat_best_params_cb['grow_policy'] = 'Lossguide'","f9096a36":"best_params_lgbm = {'reg_alpha': 1.968952683454436e-05, 'reg_lambda': 23.573499535643215, 'colsample_bytree': 0.6, 'subsample': 0.06343012933691167, 'learning_rate': 0.08143516756001878, 'max_depth': 1, 'num_leaves': 884, 'min_child_samples': 185, 'min_child_weight': 3.692224868094191e-05, 'cat_smooth': 17, 'cat_l2': 14}\nbest_params_lgbm['objective'] = 'multiclass'\nbest_params_lgbm['random_state'] = 42\nbest_params_lgbm['n_estimators'] = 1000\nbest_params_lgbm['metric'] = 'multi_logloss'\nbest_params_lgbm['device_type'] : 'cpu'","29c0ca7d":"best_params_xgb = {'learning_rate': 0.4456929987528251, 'gamma': 1.6443478072941096, 'max_depth': 29, 'min_child_weight': 51.45867185135785, 'max_delta_step': 3.54917148452682, 'subsample': 0.7132967600600638, 'colsample_bytree': 0.3802004057000849, 'lambda': 15.516716769784777, 'alpha': 16.618133595583096, 'max_leaves': 31}\nbest_params_xgb['objective'] = 'multi:softprob'\nbest_params_xgb['random_state'] = 13\nbest_params_xgb['eval_metric'] = 'mlogloss'\nbest_params_xgb['grow_policy'] = 'lossguide'\nbest_params_xgb['tree_method'] ='hist'\nbest_params_xgb['n_estimators'] = 1000\nbest_params_xgb['predictor'] ='cpu_predictor'","6de38ad2":"stager = ModelStager(log_loss, 5)\n\nprint(\"CatBoost model\")\ncatboost_model = CatBoostClassifier(**cat_best_params_cb)\nstage1_cat = stager.fit(X, y, catboost_model)\n\nprint(\"LightGBM model\")\nlightgbm_model = LGBMClassifier(**best_params_lgbm)\nstage1_gbm = stager.fit(X, y, lightgbm_model)\n\nprint(\"XGB model\")\nmodel_xgb = XGBClassifier(**best_params_xgb)\nstage_1_xgb = stager.fit(X, y, model_xgb)\n\n\nprint(\"Stage 1 : (RF, ET) -> logistic model\")\nstage1_rf_et = pd.concat([stage1_cat, stage1_gbm, stage_1_xgb], axis=1)\nstager.fit(stage1_rf_et, y, LogisticRegression())","4a222e57":"Ensemble learning techniques has several approaches\n - Bagging\n - Boosting\n - Stacking \n - Blending\n \nThis notebook will focus on Stacking","efc71b22":"[More info on Staking](https:\/\/towardsdatascience.com\/ensemble-learning-stacking-blending-voting-b37737c4f483)","722e19d0":"## CATBOOST","aa7a5098":"## Training","ad059573":"![](https:\/\/miro.medium.com\/max\/7932\/1*CoauXirckomVXxw2Id2w_Q.jpeg)","7a91ffc5":"### Resources","9478434f":"### Missing Values","f7cc348e":"## Stacking","3804c826":"#### Cube Root Transform","eae3dab5":"## XGBClassifier","b67f9842":"The way to stack any number of models and train 2 stage model LogisticClassification \non meta features. The goal is to create several week models to make predictions and after train second level model to make predictions based on first models level. ","988dd985":"## Feature Engineering","2d9a53bc":"## LightGBM"}}