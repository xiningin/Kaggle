{"cell_type":{"b6c9865d":"code","0606e57e":"code","6380d9db":"code","0a96cd79":"code","7dc2fe04":"code","535cf1ad":"code","8c6cf9a6":"code","07366553":"code","69f2e786":"code","01521d48":"code","4ec66950":"code","d2bb10d5":"code","0dd6fab2":"code","503dc20b":"code","0affe7ff":"code","2affcbb8":"code","750b23b5":"code","4bd5b3fa":"code","799ccebe":"code","e3316e51":"code","1b9dc39a":"code","26702340":"code","f57f8104":"code","a4f2a2c6":"code","669f036b":"code","9dff8195":"code","44e656c4":"code","1f45e82c":"code","24013c6a":"code","860f3c24":"code","f2ac942c":"code","2dff0cfd":"code","533ec7eb":"code","d7bdc7d5":"code","e037af00":"code","e1702709":"code","8efdf4e1":"code","ba822490":"code","3e5f6649":"code","2c07c989":"code","d35da3ba":"code","9d42d826":"code","3e6c3b23":"code","8aebdbec":"code","3a46585e":"code","31c5cd19":"code","df0c1099":"code","d66e583b":"code","ef95d0cc":"code","b4cf72ed":"code","9064ce15":"code","9cce922a":"code","948976de":"code","37be976f":"code","0a35a715":"code","21e57630":"code","7bafe326":"code","d56ed457":"code","80de2012":"code","0a4898e8":"code","99207bc2":"code","dc9eb5dc":"code","8cb61b70":"code","132f6f36":"code","7cf4fddc":"code","9ccb799e":"code","733ca8f4":"code","971efe43":"code","f3938704":"code","f2dd8ca2":"code","93e92cea":"code","dec36bbb":"code","b9264551":"code","79c9b2b0":"code","42560b14":"code","7275d813":"code","50839dc5":"code","27826911":"code","dd079295":"code","18f50d77":"code","f9c10b7a":"code","b129096e":"code","f4667ba3":"code","cd8ba05c":"code","79d82e71":"code","36eed675":"code","a91961af":"code","597b6ce1":"code","95f7e7f9":"code","06462744":"code","096d3522":"code","bd9c85c6":"code","29b0b5c4":"code","e0cdc59b":"code","3d70203e":"code","be4cc413":"code","e6ab1a4e":"code","457cf994":"code","c42c12b4":"code","59048879":"code","4cbc825d":"code","d9e2ce50":"code","4d145a47":"code","aaa68882":"code","331b1da5":"code","49fec0d6":"code","a92b7c7c":"code","8c13b5f7":"code","3d4d5c2b":"code","2e6fc768":"code","47c4d52d":"code","7c301b65":"code","d6523ead":"code","9491d237":"code","eaaf305e":"code","00742d2b":"code","ab3c360c":"code","dbc8afc3":"code","7acdcdc0":"code","99abccff":"code","00d5d72a":"code","293d0ccc":"code","98aff9c6":"code","37aabe31":"code","b4aa35da":"code","8bdbae2e":"code","0b5977e5":"code","97ca5885":"code","ce473e55":"code","5d76259f":"code","75d382bb":"code","5ff1fdf8":"code","94044455":"code","aa7c6156":"code","57f6bb98":"code","59412af1":"code","a8c45b20":"code","e1dd237c":"code","40764ad4":"code","efeeb743":"code","6f44a0bb":"code","0ffc4cd6":"code","8f2d212f":"code","b9e97ce2":"code","b6ffb5c1":"code","4ba6a52d":"code","1e474592":"code","92ad90a5":"code","d0038a65":"code","f072b751":"code","db2d4b05":"code","bd61e031":"code","a564a880":"code","aa6de4a5":"code","c53696fa":"code","2294d465":"code","960beb4c":"code","6f0fdbc2":"code","8e0a3c33":"code","54779d43":"code","7abb6e89":"code","8d9ad8e2":"code","ecac4904":"code","fe925056":"code","6ee9130d":"code","81f00986":"code","e0167d95":"code","845badff":"code","e58398b9":"code","033fd6af":"code","5ae79853":"code","9c17923f":"code","1b142ec8":"code","d3069a02":"code","61d71678":"code","a2674aa5":"code","cc489c87":"code","7881bab1":"code","6c1aff23":"code","7e2581d0":"code","c1f496f7":"code","87d4c447":"code","3db7ed2a":"code","5afed9e4":"code","573b3484":"code","bf31f065":"code","0bc401e7":"code","66ce8b86":"code","5cc0f36e":"code","3125ef01":"code","9f11645e":"code","bc6395d4":"code","0ce3c306":"code","abb4e3df":"code","d01b0d86":"code","d15625b2":"code","68a086f9":"code","ec98dadb":"code","b8a4c149":"code","fce36b64":"code","76e20bba":"code","d4a64f6d":"code","e37f80ed":"code","30c762ee":"code","2a761d65":"code","a172f2da":"code","f6240a88":"code","b00eb8b3":"code","b14bea6a":"code","1bb17422":"code","79e56571":"code","e7bf768e":"code","9ad9d7b3":"code","3ad13bf1":"code","2197eb0b":"code","abf2340b":"code","4f7ce1d3":"code","62b6e33a":"code","1c6131cc":"code","1e2651a9":"code","56455774":"code","3e5fb598":"code","f685b7d6":"code","bfe16e6f":"code","7a602588":"code","5d2e8136":"code","7e4bab22":"code","a0014961":"code","2d841181":"code","81d8bdb2":"code","75a45188":"code","1bc4917f":"code","e3efd66d":"code","98af2385":"code","227ea7c9":"code","5913f94e":"code","1cd6b6ee":"code","05c54a42":"code","169eb3b9":"code","c1d7a51b":"code","28d47d65":"code","5b213c97":"code","c6b3a6ce":"code","df2b74ea":"code","f3023db3":"code","3825ec24":"code","9ab81be1":"code","b83bdc1d":"code","d9a0a449":"code","68dd41c0":"code","3c544f02":"markdown","3ad2fe47":"markdown","1a3d1c77":"markdown","5d60fdd2":"markdown","b5387578":"markdown","9e34eed4":"markdown","060e7f67":"markdown","c7991038":"markdown","062a2685":"markdown","f08bfded":"markdown","2b229a44":"markdown","3f1a3d9b":"markdown","169f5f42":"markdown","f59f7c9e":"markdown","d306db71":"markdown","089c5f31":"markdown","f266bc9f":"markdown","3f11af70":"markdown","c2068a6a":"markdown","912e692b":"markdown","5378fbce":"markdown","f0c0657c":"markdown","ce0d6d0c":"markdown","a4fabab4":"markdown","521bdba6":"markdown","850d774f":"markdown","577d0872":"markdown","df0d1640":"markdown","31b9149b":"markdown","e67784d8":"markdown","0fb64712":"markdown","10eae380":"markdown","8fd10dfb":"markdown","b41a3ae6":"markdown","0d33299d":"markdown","cd67e637":"markdown","43e0eeb6":"markdown","441f3840":"markdown","b469eea3":"markdown","51b422d5":"markdown","55120972":"markdown","d4b7c5d5":"markdown","ebddfa45":"markdown","870cfcb7":"markdown","bbe36d57":"markdown","6da17a58":"markdown","6b024b18":"markdown","17698037":"markdown","94f0c116":"markdown","eea0735c":"markdown","7ed9b753":"markdown","28a88762":"markdown","0af65b53":"markdown","7eb0ccf8":"markdown","99853657":"markdown","481cb8fa":"markdown","046ca5b3":"markdown","336981ec":"markdown","43c80e5c":"markdown","b1d40564":"markdown","c45fa4e7":"markdown","9f0e4037":"markdown","d00245f4":"markdown","ffdddc09":"markdown","4adc035c":"markdown","e7f505c1":"markdown","65459129":"markdown","d19a2190":"markdown","c347f0b9":"markdown","8d1f4e37":"markdown","55acb983":"markdown","2ee1a305":"markdown","f8fb019d":"markdown","3546c742":"markdown","0389a466":"markdown","8935faab":"markdown","ca5d55e0":"markdown","5cd3f6aa":"markdown","d314e43e":"markdown","2940fc3d":"markdown","cd45f7c4":"markdown","6c054e55":"markdown","25605d0c":"markdown","0bd65d7b":"markdown","0ff7ada3":"markdown","48a24f20":"markdown","5a90bd5d":"markdown","aae7cca3":"markdown","0c888760":"markdown","122e359b":"markdown","a0b7df0f":"markdown","e3848a11":"markdown","f24998fd":"markdown","7f2c6331":"markdown","ac8e7617":"markdown","a6d9521c":"markdown","19cbedde":"markdown","9f545ec9":"markdown","1d8f825b":"markdown","731e643d":"markdown","9cba7ad7":"markdown","7d0feb2c":"markdown","343da864":"markdown","8999c747":"markdown","56d8d68f":"markdown","d7379ebd":"markdown","dde895be":"markdown","c364bfff":"markdown","e8600c11":"markdown","94c2478c":"markdown","a277c7f8":"markdown","aefea220":"markdown","ebd61ffe":"markdown","658a3a9e":"markdown","6f33363e":"markdown","7b1f97f7":"markdown","4f548f44":"markdown","2475e422":"markdown","4d21ad39":"markdown","680b8fc5":"markdown","edf54e7f":"markdown","64f47bce":"markdown","37566384":"markdown","af002b1a":"markdown","b31e08a8":"markdown","14ad2e89":"markdown","caf34dd2":"markdown","b8b39535":"markdown","325d0869":"markdown","f0804651":"markdown","0a741e17":"markdown","a63411ae":"markdown","b84c4aa9":"markdown","f0e31f06":"markdown","8ada96fb":"markdown","7b191fb9":"markdown","08684b34":"markdown","7850fdc2":"markdown","22a1ec8e":"markdown","a1b816c7":"markdown","218324ea":"markdown","e84c918b":"markdown","0492e791":"markdown","ce8bb069":"markdown","d45fc94c":"markdown","07385b20":"markdown","de4b368d":"markdown","5b4672f2":"markdown","0d406339":"markdown","c6b1e300":"markdown","49b4f1e9":"markdown","a0757085":"markdown","c31ea3be":"markdown","baa04b25":"markdown","050fac00":"markdown","fefb6e18":"markdown","7152e65b":"markdown","40edd910":"markdown","555fd12e":"markdown","45dc280e":"markdown","c54d440e":"markdown","6a102e50":"markdown","48633263":"markdown","771430be":"markdown","8d2a0e79":"markdown","b0b287bd":"markdown","034f09c3":"markdown","4dc68b6b":"markdown","59468be3":"markdown","82f12a46":"markdown","a6c924cf":"markdown","6be3c7b5":"markdown","9ea58584":"markdown","79286455":"markdown","752666a0":"markdown","896560cc":"markdown","0e33fff1":"markdown","86325d8c":"markdown","60791774":"markdown","8109a621":"markdown","3e84ffce":"markdown","1c7d0b19":"markdown","dad0f8ca":"markdown","03ccda01":"markdown","2bd3b9a3":"markdown","c98a949d":"markdown","c8b7ada7":"markdown","5e0149c1":"markdown","622964b6":"markdown","54ef5210":"markdown","f936b595":"markdown","1535506d":"markdown","9c572bd5":"markdown","8cc9f91f":"markdown","6f2ce721":"markdown","c7d50d45":"markdown","e1c3dbe4":"markdown","b3041734":"markdown","65806438":"markdown","92271af5":"markdown","a132885d":"markdown","37c75420":"markdown","5b094371":"markdown","1cb2cab7":"markdown","4c8c4854":"markdown","933764e3":"markdown","493d5260":"markdown","29b3998b":"markdown","f73536dc":"markdown","96185c13":"markdown","4be8f142":"markdown","7f50d2ae":"markdown","83927c12":"markdown","af2ae984":"markdown","c44a2aa3":"markdown","eee4ac47":"markdown","d0cb6a00":"markdown","9784e22e":"markdown","9d53a7c0":"markdown","307e89d6":"markdown","e7dee4c2":"markdown","35495fa0":"markdown","ce38929a":"markdown","eafb208c":"markdown","0df5e5d3":"markdown","6e10d44b":"markdown","243f03a6":"markdown","44ed3518":"markdown","701e8c48":"markdown","9355e00a":"markdown","3034af33":"markdown","593a6e2a":"markdown","2b17183b":"markdown","993bab98":"markdown","96af8925":"markdown","2af27e14":"markdown","eb6d4fa8":"markdown","d84aec95":"markdown","7fcb8edb":"markdown","972fdeec":"markdown","e9f57aaa":"markdown","642597ff":"markdown","5bc5830f":"markdown","135ead52":"markdown","409d58c4":"markdown","619c0ead":"markdown","58e67589":"markdown","96d9f176":"markdown","887f8b23":"markdown","3194a519":"markdown","e8bc8008":"markdown","33754040":"markdown","2234e268":"markdown","522bd57f":"markdown","6c69cfc9":"markdown","833493ac":"markdown","20c96c8b":"markdown","6d539236":"markdown","2d2889b7":"markdown","24a509a3":"markdown","98dca1b1":"markdown","792648af":"markdown","581d65ad":"markdown","146dc856":"markdown","91099ad9":"markdown","e1fcd81f":"markdown","3ee5df37":"markdown","310a5289":"markdown","42ec9446":"markdown","ce8af497":"markdown","979744a7":"markdown","10b04eb5":"markdown","d5d861a6":"markdown","8aeb7171":"markdown","7029a8cd":"markdown","65adb19c":"markdown","76df63d0":"markdown","b26f19af":"markdown","ba62aea9":"markdown","5d9d34d0":"markdown","5037441c":"markdown","fe771e38":"markdown","f89d0dd0":"markdown"},"source":{"b6c9865d":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","0606e57e":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","6380d9db":"train.head()","0a96cd79":"# Concatenating train and test dataframe\ndef concat_df(train_df, test_df):\n    return pd.concat([train_df, test_df], sort = False).reset_index(drop = True)\n\n# Breaking combined dataframe into train and test\ndef divide_df(combined_df):\n    return combined_df.loc[:890], combined_df.loc[891:].drop(['Survived'], axis=1)","7dc2fe04":"combined = concat_df(train, test)","535cf1ad":"# yticklabels -> To get the value of each row's y-label\n# cbar -> For the scale\n\n# One can change the plot size by altering the figure size from below\nplt.figure(figsize = (10, 5))\nsns.heatmap(combined.isnull(), yticklabels=False, cbar=False, cmap='viridis')","8c6cf9a6":"# Counting missing values for each feature\ncombined.isnull().sum()","07366553":"# Looping over features to count missing values\nfor feature in combined.columns:\n    print('Missing values in feature ' + str(feature) + ' : ' + str(len(combined[combined[feature].isnull() == True])))","69f2e786":"# Percentage of missing values\nfor feature in combined.columns:\n    print('Missing value percent in '+ feature +': {:.2f}%'.format((combined[combined[feature].isnull() == True].shape[0])\/(combined.shape[0]) * 100))","01521d48":"# Defining plot sizes\nplt.figure(figsize = (10, 8))\n\n# Creating correlation matrix\ncorr_mat = combined.corr()\n\n# Plotting the matrix\nsns.heatmap(corr_mat, xticklabels = corr_mat.columns, yticklabels = corr_mat.columns, annot=True)","4ec66950":"plt.figure(figsize = (10, 8))\n\n# Barplot with 'x' as 'Pclass', 'y' as 'Age'\n# 'hue' acts as a third dimension to visualize data\nsns.barplot(x = 'Pclass', y = 'Age', hue = 'Sex', data = combined)","d2bb10d5":"# Grouping data on the basis of 'Sex' and 'Pclass'\nage_by_pclass_sex = combined.groupby(['Sex', 'Pclass']).median()['Age']\n\nfor pclass in range(1, 4):\n    for sex in ['female', 'male']:\n        print('Median age of Pclass {} {}s: {}'.format(pclass, sex, age_by_pclass_sex[sex][pclass]))\n\nprint('--------')\nprint('Median age of all passengers: {}'.format(combined['Age'].median()))","0dd6fab2":"plt.figure(figsize = (10, 5))\nsns.barplot(x = 'SibSp', y = 'Age', hue = 'Sex', data = combined)","503dc20b":"sib_arr = combined['SibSp'].unique()\nsib_arr.sort()","0affe7ff":"age_by_sibsp_sex = combined.groupby(['Sex', 'SibSp']).median()['Age']\n\nfor sibsp in sib_arr:\n    for sex in ['female', 'male']:\n        print('Median age of SibSp {} {}s: {}'.format(sibsp, sex, age_by_sibsp_sex[sex][sibsp]))\n\nprint('--------')\nprint('Median age of all passengers: {}'.format(combined['Age'].median()))","2affcbb8":"# Imputing missing values through lambda function\ncombined['Age'] = combined.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))","750b23b5":"# Setting up condition to check specific values in dataframe\ncombined[combined['Embarked'].isnull() == True]","4bd5b3fa":"sns.countplot(combined[combined['Pclass'] == 1]['Embarked'])","799ccebe":"# Applying multiple conditions to the dataframe\nsns.countplot(combined[(combined['SibSp'] == 0) & (combined['Parch'] == 0)]['Embarked'])","e3316e51":"sns.countplot(combined[combined['Embarked'] == 'S']['SibSp'])","1b9dc39a":"# Imputing missing values using 'fillna' method\ncombined['Embarked'] = combined['Embarked'].fillna(combined['Embarked'].mode()[0])","26702340":"combined[combined['Fare'].isnull() == True]","f57f8104":"combined[(combined['Pclass'] == 3) & (combined['SibSp'] == 0) & (combined['Parch'] == 0)]","a4f2a2c6":"combined[(combined['Pclass'] == 3) & (combined['SibSp'] == 0) & (combined['Parch'] == 0) & (combined['Sex'] == 'male')]['Fare'].median()","669f036b":"combined[(combined['Pclass'] == 3) & (combined['SibSp'] == 0) & (combined['Parch'] == 0) & (combined['Sex'] == 'female')]['Fare'].median()","9dff8195":"combined[(combined['Pclass'] == 3) & (combined['SibSp'] == 0) & (combined['Parch'] == 0)]['Fare'].median()","44e656c4":"combined['Fare'] = combined['Fare'].fillna(combined[(combined['Pclass'] == 3) & (combined['SibSp'] == 0) & (combined['Parch'] == 0)]['Fare'].median())","1f45e82c":"combined['Cabin'].unique()","24013c6a":"def deck_extractor(combined):\n    \n    # Function to separate deck name from 'Cabin' feature.\n    deck_list = []\n    for deck in combined['Cabin']:\n        if type(deck) == str:\n            deck_list.append(deck[0])\n        else:\n            # 'M' stands for missing cabin values\n            deck_list.append('M')\n            \n    # Creating a new coloumn in the dataframe \n    combined['Deck'] = deck_list\n    \n    return combined","860f3c24":"combined = deck_extractor(combined)","f2ac942c":"plt.figure(figsize = (10, 5))\nsns.countplot(x = combined[combined['Deck'] != 'M']['Deck'], hue = combined['Pclass'],\n             order = combined[combined['Deck'] != 'M']['Deck'].value_counts().index)","2dff0cfd":"# Identifying unique values for a feature\ndeck_list = combined['Deck'].unique()\n\n# Sorting the array\ndeck_list.sort()\n\nfor deck in deck_list:\n    # Ignoring the missing values of decks\n    if deck == 'M':\n        continue\n    print('For passengers from deck ' + str(deck))\n    print('Number of first class passengers are: ', len(combined[(combined['Deck'] == deck) & (combined['Pclass'] == 1)]))\n    print('Number of second class passengers are: ', len(combined[(combined['Deck'] == deck) & (combined['Pclass'] == 2)]))\n    print('Number of third class passengers are: ', len(combined[(combined['Deck'] == deck) & (combined['Pclass'] == 3)]))\n    print('----------------')","533ec7eb":"plt.figure(figsize = (10, 5))\nsns.countplot(x = combined[combined['Deck'] != 'M']['Deck'], hue = combined['Survived'],\n             order = combined[combined['Deck'] != 'M']['Deck'].value_counts().index)","d7bdc7d5":"for deck in deck_list:\n    if deck == 'M':\n        continue\n    print('For passengers from deck ', str(deck))\n    print('Percentage of survival: ' + '{:.2f}%'.format((len(combined[(combined['Deck'] == deck) & (combined['Survived'] == 1)])) \/ (len(combined[(combined['Deck'] == deck) & (combined['Survived'].isnull() == False)])) * 100))\n    print('Percentage of death: ' + '{:.2f}%'.format((len(combined[(combined['Deck'] == deck) & (combined['Survived'] == 0)])) \/ (len(combined[(combined['Deck'] == deck) & (combined['Survived'].isnull() == False)])) * 100))\n    print('--------------')","e037af00":"plt.figure(figsize = (10, 8))\nsns.countplot(x = combined['Pclass'], hue = combined['Survived'])","e1702709":"print('Total number of first class passengers with survival data known: ', len(combined[(combined['Pclass'] == 1) & (combined['Survived'].isnull() == False)]))\nprint('Total number of first class passengers survived: ', len(combined[(combined['Pclass'] == 1) & (combined['Survived'] == 1)]))\nprint('Percentage of first class passengers survived: ' + '{:.2f}%'.format((len(combined[(combined['Pclass'] == 1) & (combined['Survived'] == 1)])) \/ (len(combined[(combined['Pclass'] == 1) & (combined['Survived'].isnull() == False)])) * 100))","8efdf4e1":"print('Total number of second class passengers with survival data known: ', len(combined[(combined['Pclass'] == 2) & (combined['Survived'].isnull() == False)]))\nprint('Total number of second class passengers survived: ', len(combined[(combined['Pclass'] == 2) & (combined['Survived'] == 1)]))\nprint('Percentage of second class passengers survived: ' + '{:.2f}%'.format((len(combined[(combined['Pclass'] == 2) & (combined['Survived'] == 1)])) \/ (len(combined[(combined['Pclass'] == 2) & (combined['Survived'].isnull() == False)])) * 100))","ba822490":"print('Total number of third class passengers with survival data known: ', len(combined[(combined['Pclass'] == 3) & (combined['Survived'].isnull() == False)]))\nprint('Total number of third class passengers survived: ', len(combined[(combined['Pclass'] == 3) & (combined['Survived'] == 1)]))\nprint('Percentage of third class passengers survived: ' + '{:.2f}%'.format((len(combined[(combined['Pclass'] == 3) & (combined['Survived'] == 1)])) \/ (len(combined[(combined['Pclass'] == 3) & (combined['Survived'].isnull() == False)])) * 100))","3e5f6649":"combined[combined['Deck'] == 'T']","2c07c989":"# Replacing the deck 'T' with 'A'\ncombined[combined['Deck'] == 'T'] = combined[combined['Deck'] == 'T'].replace('T', 'A') ","d35da3ba":"# Replacing multiple values in a dataframe\ncombined['Deck'] = combined['Deck'].replace(['A', 'B', 'C'], 'ABC')\ncombined['Deck'] = combined['Deck'].replace(['D', 'E'], 'DE')\ncombined['Deck'] = combined['Deck'].replace(['F', 'G'], 'FG')","9d42d826":"# Dropping features after specifying axis along which it is to be dropped\ncombined = combined.drop(['Cabin'], axis = 1)","3e6c3b23":"combined.isnull().sum()","8aebdbec":"plt.figure(figsize = (10, 8))\nsns.countplot(combined['Survived'])","3a46585e":"print('Total number of people who survived: ', len(combined[(combined['Survived'].isnull() == False) & (combined['Survived'] == 1)]))\nprint('Percent of people who survived: ' + '{:.2f}%'.format(len(combined[(combined['Survived'].isnull() == False) & (combined['Survived'] == 1)]) \/ (len(combined[(combined['Survived'].isnull() == False)])) * 100))","31c5cd19":"print('Total number of people who did not survive: ', len(combined[(combined['Survived'].isnull() == False) & (combined['Survived'] == 0)]))\nprint('Percent of people who did not survive: ' + '{:.2f}%'.format(len(combined[(combined['Survived'].isnull() == False) & (combined['Survived'] == 0)]) \/ (len(combined[(combined['Survived'].isnull() == False)])) * 100))","df0c1099":"# Executing user defined function to break the dataframe\ntrain, test = divide_df(combined)","d66e583b":"plt.figure(figsize = (10, 8))\ncorr_mat = train.corr()\nsns.heatmap(corr_mat, xticklabels = corr_mat.columns, yticklabels = corr_mat.columns, annot=True)","ef95d0cc":"plt.figure(figsize = (10, 8))\ncorr_mat = test.corr()\nsns.heatmap(corr_mat, xticklabels = corr_mat.columns, yticklabels = corr_mat.columns, annot=True)","b4cf72ed":"# Creating sub-plots to visualize multiple graphs\nfig,ax = plt.subplots(nrows=2, ncols=2, figsize = (20, 10))\n\n# Defining individual graphs with coordinates specified by parameter 'ax'\nsns.kdeplot(data = train[train['Survived'] == 1]['Age'], label = 'Survived', ax = ax[0, 0])\nsns.kdeplot(data = train[train['Survived'] == 0]['Age'], label = 'Not Survived', ax = ax[0, 0])\nsns.kdeplot(data = train['Age'], label = 'Train Set', ax = ax[1, 0])\nsns.kdeplot(data = test['Age'], label = 'Test Set', ax = ax[1, 0])\nsns.kdeplot(data = train['Fare'], label = 'Train Set', ax = ax[1, 1])\nsns.kdeplot(data = test['Fare'], label = 'Test Set', ax = ax[1, 1])\nsns.kdeplot(data = train[train['Survived'] == 1]['Fare'], label = 'Survived', ax = ax[0, 1])\nsns.kdeplot(data = train[train['Survived'] == 0]['Fare'], label = 'Not Survived', ax = ax[0, 1])\n\n# Setting up labels for the sub-plots\nax[0, 0].set(xlabel=\"Age\")\nax[0, 1].set(xlabel=\"Fare\")\nax[1, 0].set(xlabel=\"Age\")\nax[1, 1].set(xlabel=\"Fare\")\n\nplt.show()","9064ce15":"plt.figure(figsize = (10, 5))\n\n# Plotting fare w.r.t. survival\n# Labelling each plot using the parameter 'label'\nsns.distplot(train[train['Survived'] == 1]['Fare'], label = 'Survived')\nsns.distplot(train[train['Survived'] == 0]['Fare'], label = 'Not Survived')\nplt.legend()\nplt.plot()","9cce922a":"train[train['Survived'] == 1]['Fare'].max()","948976de":"train[train['Survived'] == 0]['Fare'].max()","37be976f":"train[train['Fare'] == 263.0]","0a35a715":"sns.countplot(train[train['Fare'] > 263.0]['Survived'])","21e57630":"train[train['Survived'] == 1]['Fare'].min()","7bafe326":"sns.countplot(train[train['Fare'] == 0]['Survived'])","d56ed457":"train[train['Fare'] == 0]","80de2012":"plt.figure(figsize = (10, 5))\nsns.countplot(train[train['Fare'] == 0]['Pclass'])","0a4898e8":"train[(train['Fare'] == 0) & (train['Survived'] == 1)]","99207bc2":"plt.figure(figsize = (10, 5))\n\n# Creating grids in the plot\nax = plt.gca()\nax.xaxis.grid(True)\n\n# Plotting the graph\nsns.distplot(train[train['Survived'] == 1]['Age'], label = 'Survived')\nsns.distplot(train[train['Survived'] == 0]['Age'], label = 'Not Survived')\nplt.xticks(np.arange(0, train['Age'].max(), step = 5))\n\n# Creating a legend for the graph\nplt.legend()\nplt.plot()","dc9eb5dc":"plt.figure(figsize = (10, 8))\nsns.countplot(x = train['Pclass'], hue = train['Survived'])","8cb61b70":"plt.figure(figsize = (10, 5))\nsns.countplot(x = train['Sex'], hue = train['Survived'])","132f6f36":"print('Percent of males who survived:' + ' {:.2f}%'.format((len(train[(train['Sex'] == 'male') & (train['Survived'] == 1)])) \/ (len(train)) * 100))\nprint('Percent of males who did not survive:' + ' {:.2f}%'.format((len(train[(train['Sex'] == 'male') & (train['Survived'] == 0)])) \/ (len(train)) * 100))\nprint('------------')\nprint('Percent of females who survived:' + ' {:.2f}%'.format((len(train[(train['Sex'] == 'female') & (train['Survived'] == 1)])) \/ (len(train)) * 100))\nprint('Percent of females who did not survive:' + ' {:.2f}%'.format((len(train[(train['Sex'] == 'female') & (train['Survived'] == 0)])) \/ (len(train)) * 100))","7cf4fddc":"plt.figure(figsize = (10, 5))\nsns.countplot(x = train['SibSp'], hue = train['Survived'])","9ccb799e":"sib_arr = train['SibSp'].unique()\nsib_arr.sort()","733ca8f4":"for sib in sib_arr:\n    print('Percent of passengers with ' + str(sib) + ' siblings\/spouce who survived: {:.2f}%'.format((len(train[(train['SibSp'] == sib) & (train['Survived'] == 1)])) \/ (len(train[(train['SibSp'] == sib)])) * 100))\n    print('Percent of passengers with ' + str(sib) + ' siblings\/spouce who did not survive: {:.2f}%'.format((len(train[(train['SibSp'] == sib) & (train['Survived'] == 0)])) \/ (len(train[(train['SibSp'] == sib)])) * 100))\n    print('------------')","971efe43":"plt.figure(figsize = (10, 5))\nsns.countplot(x = train['Parch'], hue = train['Survived'])","f3938704":"parc_arr = train['Parch'].unique()\nparc_arr.sort()","f2dd8ca2":"for parc in parc_arr:\n    print('Percent of passengers with ' + str(parc) + ' parents\/children who survived: {:.2f}%'.format((len(train[(train['Parch'] == parc) & (train['Survived'] == 1)])) \/ (len(train[(train['Parch'] == parc)])) * 100))\n    print('Percent of passengers with ' + str(parc) + ' parents\/children who did not survive: {:.2f}%'.format((len(train[(train['Parch'] == parc) & (train['Survived'] == 0)])) \/ (len(train[(train['Parch'] == parc)])) * 100))\n    print('------------')","93e92cea":"plt.figure(figsize = (10, 5))\nsns.countplot(x = train['Embarked'], hue = train['Survived'])","dec36bbb":"for port in train['Embarked'].unique():\n    print('Percent of passengers embarked on ' + str(port) + ' who survived: {:.2f}%'.format((len(train[(train['Embarked'] == port) & (train['Survived'] == 1)])) \/ (len(train[(train['Embarked'] == port)])) * 100))\n    print('Percent of passengers embarked on ' + str(port) + ' who did not survive: {:.2f}%'.format((len(train[(train['Embarked'] == port) & (train['Survived'] == 0)])) \/ (len(train[(train['Embarked'] == port)])) * 100))\n    print('------------')","b9264551":"plt.figure(figsize = (10, 5))\nsns.countplot(x = train[train['Deck'] != 'M']['Deck'], hue = train['Survived'])","79c9b2b0":"for deck in train['Deck'].unique():\n    if deck == 'M':\n        continue\n    print('Percent of passengers on deck ' + str(deck) + ' who survived: {:.2f}%'.format((len(train[(train['Deck'] == deck) & (train['Survived'] == 1)])) \/ (len(train[(train['Deck'] == deck)])) * 100))\n    print('Percent of passengers on deck ' + str(deck) + ' who did not survive: {:.2f}%'.format((len(train[(train['Deck'] == deck) & (train['Survived'] == 0)])) \/ (len(train[(train['Deck'] == deck)])) * 100))\n    print('------------')","42560b14":"plt.figure(figsize = (10, 5))\nax = plt.gca()\nax.yaxis.grid(True)\nsns.barplot(x = train['Pclass'], y = train['Age'])","7275d813":"plt.figure(figsize = (10, 5))\nsns.barplot(x = train['Pclass'], y = train['Fare'], hue = train['Survived'])","50839dc5":"fig, ax = plt.subplots(ncols = 2, figsize = (20, 6))\nplt.figure(figsize = (10, 5))\nsns.barplot(x = train['SibSp'], y = train['Age'], hue = train['Survived'], ax = ax[0])\nsns.barplot(x = train['Parch'], y = train['Age'], hue = train['Survived'], ax = ax[1])\nplt.show()","27826911":"plt.figure(figsize = (15, 8))\nsns.scatterplot(x = train['Age'], y = train['Fare'], hue = train['Survived'])\nplt.xlabel('Age')\nplt.ylabel('Fare')","dd079295":"fig, ax = plt.subplots(ncols = 2, figsize = (20, 6))\nsns.distplot(train[(train['Age'] <= 10) & (train['Survived'] == True)]['Fare'], label = 'Survived', ax = ax[0])\nsns.distplot(train[(train['Age'] <= 10) & (train['Survived'] == False)]['Fare'], label = 'Not Survived', ax = ax[0])\nsns.distplot(train[(train['Age'] > 10) & (train['Survived'] == True) & (train['Fare'] <= 200)]['Fare'], label = 'Survived', ax = ax[1])\nsns.distplot(train[(train['Age'] > 10) & (train['Survived'] == False) & (train['Fare'] <= 200)]['Fare'], label = 'Survived', ax = ax[1])\nax[0].title.set_text('Passengers less than 10 years of age')\nax[1].title.set_text('Passengers more than 10 years of age')\nax[0].legend()\nax[1].legend()\nplt.xlabel('Fare')","18f50d77":"# Specifying the color scheme for the datapoints\ncolor_dict = dict({'male': 'blue', \n                   'female': 'red'})\n\nfig, ax = plt.subplots(ncols = 2, figsize = (15,5))\nsns.scatterplot(x = train['Age'], y = train['Fare'], hue = train[train['Survived'] == 1]['Sex'], palette = color_dict, ax = ax[0])\nsns.scatterplot(x = train['Age'], y = train['Fare'], hue = train[train['Survived'] == 0]['Sex'], palette = color_dict, ax = ax[1])\nplt.xlabel('Age')\nplt.ylabel('Fare')\nax[0].title.set_text('Survived')\nax[1].title.set_text('Not Survived')","f9c10b7a":"fig, ax = plt.subplots(ncols = 2, figsize = (20, 6))\nsns.countplot(train[(train['Age'] <= 10) & (train['Sex'] == 'male')]['Survived'], ax = ax[0])\nsns.countplot(train[(train['Age'] <= 10) & (train['Sex'] == 'female')]['Survived'], ax = ax[1])\nax[0].title.set_text('Male children less than 10 years of age')\nax[1].title.set_text('Female children less than 10 years of age')","b129096e":"print('Percentage of male children who survived: {:.2f}%'.format(((len(train[(train['Age'] <= 10) & (train['Sex'] == 'male') & (train['Survived'] == True)])) \/ (len(train[(train['Age'] <= 10) & (train['Sex'] == 'male')])) * 100)))\nprint('Percentage of male children who did not survive: {:.2f}%'.format(((len(train[(train['Age'] <= 10) & (train['Sex'] == 'male') & (train['Survived'] == False)])) \/ (len(train[(train['Age'] <= 10) & (train['Sex'] == 'male')])) * 100)))\nprint('---------------')\nprint('Percentage of female children who survived: {:.2f}%'.format(((len(train[(train['Age'] <= 10) & (train['Sex'] == 'female') & (train['Survived'] == True)])) \/ (len(train[(train['Age'] <= 10) & (train['Sex'] == 'female')])) * 100)))\nprint('Percentage of female children who did not survive: {:.2f}%'.format(((len(train[(train['Age'] <= 10) & (train['Sex'] == 'female') & (train['Survived'] == False)])) \/ (len(train[(train['Age'] <= 10) & (train['Sex'] == 'female')])) * 100)))","f4667ba3":"plt.figure(figsize = (10, 5))\nsns.countplot(train['SibSp'], hue = train['Parch'])\nplt.legend(loc = 'upper right', title = 'Parch')","cd8ba05c":"plt.figure(figsize = (10, 5))\nsns.barplot(x = train['SibSp'], y = train['Fare'])","79d82e71":"print('Minimum fare for SibSp = 3 is: ', train[train['SibSp'] == 3]['Fare'].min())\nprint('Minimum fare for SibSp = 8 is: ', train[train['SibSp'] == 8]['Fare'].min())\nprint('----------------')\nprint('Maximum fare for SibSp = 3 is: ', train[train['SibSp'] == 3]['Fare'].max())\nprint('Maximum fare for SibSp = 8 is: ', train[train['SibSp'] == 8]['Fare'].max())","36eed675":"train[train['SibSp'] == 8]","a91961af":"plt.figure(figsize = (10, 5))\nsns.barplot(x = train['SibSp'], y = train['Fare'], hue = train['Survived'])","597b6ce1":"plt.figure(figsize = (10, 5))\nsns.barplot(x = train['Parch'], y = train['Fare'])","95f7e7f9":"plt.figure(figsize = (10, 5))\nsns.barplot(x = train['Parch'], y = train['Fare'], hue = train['Survived'])","06462744":"train[train['Parch'] == 4]","096d3522":"plt.figure(figsize = (10, 5))\nsns.distplot(combined['Age'])","bd9c85c6":"plt.figure(figsize = (10, 5))\nsns.boxplot(combined['Age'])","29b0b5c4":"age_quartile_1, age_quartile_3 = np.percentile(combined['Age'], [25, 75])\n\nprint(age_quartile_1, age_quartile_3)","e0cdc59b":"IQR_age = age_quartile_3 - age_quartile_1\n\nprint(IQR_age)","3d70203e":"lower_bound_age = age_quartile_1 - (1.5 * IQR_age)\nupper_bound_age = age_quartile_3 + (1.5 * IQR_age)\n\nprint('Age Lower Bound (IQR): ', lower_bound_age)\nprint('Age Upper Bound (IQR): ', upper_bound_age)","be4cc413":"print('Percentage of non-outliers according to IQR: {:.2f}%'.format(len(combined[(combined['Age'] >= lower_bound_age) & (combined['Age'] <= upper_bound_age)]) \/ (len(combined)) * 100))","e6ab1a4e":"from scipy import stats","457cf994":"unique, counts = np.unique((stats.zscore(combined['Age']) > -3) & (stats.zscore(combined['Age']) < 3), return_counts = True)\ncount_dict = dict(zip(unique, counts))\nprint('Percentage of non-outliers according to Z-Scores: {:.2f}%'.format((count_dict[True]) \/ (count_dict[True] + count_dict[False]) * 100))","c42c12b4":"combined[(stats.zscore(combined['Age']) >= -3) & (stats.zscore(combined['Age']) <= 3)]","59048879":"print('Age Lower Bound (Z-Score): ', combined[(stats.zscore(combined['Age']) >= -3) & (stats.zscore(combined['Age']) <=3)]['Age'].min())\nprint('Age Upper Bound (Z-Score): ', combined[(stats.zscore(combined['Age']) >= -3) & (stats.zscore(combined['Age']) <= 3)]['Age'].max())","4cbc825d":"plt.figure(figsize = (10, 5))\nsns.distplot(combined['Fare'])","d9e2ce50":"plt.figure(figsize = (10, 5))\nsns.boxplot(combined['Fare'])","4d145a47":"fare_quartile_1, fare_quartile_3 = np.percentile(combined['Fare'], [25, 75])\n\nprint(fare_quartile_1, fare_quartile_3)","aaa68882":"IQR_fare = fare_quartile_3 - fare_quartile_1\n\nprint(IQR_fare)","331b1da5":"lower_bound_fare = fare_quartile_1 - (1.5 * IQR_fare)\nupper_bound_fare = fare_quartile_3 + (1.5 * IQR_fare)\n\nprint('Fare Lower Bound (IQR): ', lower_bound_fare)\nprint('Fare Upper Bound (IQR): ', upper_bound_fare)","49fec0d6":"print('Percentage of non-outliers according to IQR: {:.2f}%'.format(len(combined[(combined['Fare'] >= lower_bound_fare) & (combined['Fare'] <= upper_bound_fare)]) \/ (len(combined)) * 100))","a92b7c7c":"unique_fare, counts_fare = np.unique((stats.zscore(combined['Fare']) > -3) & (stats.zscore(combined['Fare']) < 3), return_counts = True)\nfare_count_dict = dict(zip(unique_fare, counts_fare))\nprint('Percentage of non-outliers according to Z-Scores: {:.2f}%'.format((fare_count_dict[True]) \/ (fare_count_dict[True] + fare_count_dict[False]) * 100))","8c13b5f7":"print('Fare Lower Bound (Z-Score): ', combined[(stats.zscore(combined['Fare']) >= -3) & (stats.zscore(combined['Fare']) <= 3)]['Fare'].min())\nprint('Fare Upper Bound (Z-Score): ', combined[(stats.zscore(combined['Fare']) >= -3) & (stats.zscore(combined['Fare']) <= 3)]['Fare'].max())","3d4d5c2b":"combined_dummy = combined.copy()","2e6fc768":"combined[stats.zscore(combined['Age']) > 3]","47c4d52d":"combined_dummy = combined_dummy.replace(dict.fromkeys(combined_dummy[stats.zscore(combined_dummy['Age']) > 3]['Age'], 67.0))","7c301b65":"combined_dummy = combined_dummy.replace(dict.fromkeys(combined_dummy[stats.zscore(combined_dummy['Fare']) > 3]['Fare'], 164.8667))","d6523ead":"fig, ax = plt.subplots(ncols = 2, figsize = (20, 6))\nsns.kdeplot(combined['Age'], ax = ax[0], label = 'Before Filter', color = 'red')\nsns.kdeplot(combined_dummy['Age'], ax = ax[0], label = 'After Filter', color = 'blue')\nsns.kdeplot(combined['Fare'], ax = ax[1], label = 'Before Filter', color = 'red')\nsns.kdeplot(combined_dummy['Fare'], ax = ax[1], label = 'After Filter', color = 'blue')\nax[0].title.set_text('Distribution of Age')\nax[1].title.set_text('Distribution of Fare')\nax[0].legend(loc = 'upper right')\nax[1].legend(loc = 'upper right')","9491d237":"plt.figure(figsize = (10, 5))\nsns.kdeplot(combined['Fare'], label = 'Before Filtering', color = 'red')\nsns.kdeplot(combined_dummy['Fare'], label = 'After Filtering', color = 'blue')\nplt.axvline(x = 164.8667, color = 'yellow')","eaaf305e":"# Creating bins for continous variables\n# Number of bins can be specified. For this case number of bins are 13\ncombined_dummy['Fare Bins'] = pd.qcut(combined_dummy['Fare'], 13)\ncombined['Fare Bins'] = pd.qcut(combined['Fare'], 13)\n\nfig, axs = plt.subplots(nrows = 2, figsize=(20, 10))\nplt.subplots_adjust(bottom = 0.1)\nsns.countplot(x='Fare Bins', hue='Survived', data=combined_dummy, ax = axs[0])\nsns.countplot(x='Fare Bins', hue='Survived', data=combined, ax = axs[1])\n\naxs[0].title.set_text('After Filtering')\naxs[1].title.set_text('Before Filtering')","00742d2b":"combined_dummy['Age Bins'] = pd.qcut(combined_dummy['Age'], 9)\ncombined['Age Bins'] = pd.qcut(combined['Age'], 9)\n\nfig, axs = plt.subplots(nrows = 2, figsize=(20, 10))\nplt.subplots_adjust(bottom = 0.1)\nplt.figure(figsize = (20, 6))\nsns.countplot(x='Age Bins', hue='Survived', data=combined_dummy, ax = axs[0])\nsns.countplot(x='Age Bins', hue='Survived', data=combined, ax = axs[1])\n\naxs[0].title.set_text('After Filtering')\naxs[1].title.set_text('Before Filtering')","ab3c360c":"name_title = []\nfor name in combined_dummy['Name']:\n    for i in name.split():\n        if '.' in i:\n            name_title.append(i)\n\nunique_name_title = list(set(name_title))         ","dbc8afc3":"unique_name_title","7acdcdc0":"strange_title = ['Mlle.', 'Mme.', 'Jonkheer.', 'L.', 'Rev.']","99abccff":"for name in combined_dummy['Name']:\n    for title in strange_title:    \n        if title in name.split():\n            print(name)","00d5d72a":"unique_name_title.remove('L.')\nname_title.remove('L.')\n\n#printing the name list\nunique_name_title","293d0ccc":"occurence_list = []\nfor title in unique_name_title:\n    print('{} has {} occurences.'.format(title, name_title.count(title)))\n    occurence_list.append(name_title.count(title))","98aff9c6":"title_dict = dict(zip(unique_name_title, occurence_list))\ntitle_df = pd.DataFrame(title_dict.items(), columns = ['Title', 'Occurences'])","37aabe31":"title_df.head()","b4aa35da":"plt.figure(figsize = (15, 8))\nsns.barplot(x = 'Title', y = 'Occurences', data = title_df)\nplt.title('Title count')\nplt.show()","8bdbae2e":"combined_dummy['Title'] = name_title","0b5977e5":"plt.figure(figsize = (10, 5))\nsns.countplot(x = 'Title', data = combined_dummy[combined_dummy['Survived'] == True])","97ca5885":"def generic_title(combined_dummy):\n    # Function to generalize 'Title' feature\n    title_arr = combined_dummy['Title']\n    generic_title_arr = []\n    \n    for title in combined_dummy['Title']:\n        if title == 'Mr.':\n            generic_title_arr.append('Mr')\n        elif title == 'Mrs.' or title == 'Miss.' or title == 'Countess.' or title == 'Ms.' or title == 'Mme.' or title == 'Lady.' or title == 'Mlle.' or title == 'Dona.':\n            generic_title_arr.append('Mrs\/Miss\/Ms')\n        elif title == 'Major.' or title == 'Col.' or title == 'Dr.' or title == 'Sir.' or title == 'Jonkheer.' or title == 'Rev.' or title == 'Capt.' or title == 'Don.':\n            generic_title_arr.append('Dr\/Military\/Noble\/Clergy')\n        elif title == 'Master.':\n            generic_title_arr.append('Master')\n    \n    if len(combined_dummy) != len(generic_title_arr):\n        print('Check the titles again!')\n        return False\n    \n    combined_dummy['Generic Title'] = generic_title_arr\n    \n    return combined_dummy","ce473e55":"combined_dummy = generic_title(combined_dummy)","5d76259f":"combined_dummy.head()","75d382bb":"plt.figure(figsize = (10, 5))\nsns.countplot(x = 'Generic Title', hue = combined_dummy['Survived'], data = combined_dummy,\n              order = combined_dummy['Generic Title'].value_counts().index)","5ff1fdf8":"generic_title_arr = combined_dummy['Generic Title'].unique()\ngeneric_title_arr.sort()","94044455":"for title in generic_title_arr:\n    print('Percent of passengers with a generic title of ' + str(title) + ' who survived: {:.2f}%'.format((len(combined_dummy[(combined_dummy['Generic Title'] == title) & (combined_dummy['Survived'] == 1)])) \/ (len(combined_dummy[(combined_dummy['Generic Title'] == title) & (combined_dummy['Survived'].isnull() == False)])) * 100))\n    print('Percent of passengers with a generic title of ' + str(title) + ' who did not survive: {:.2f}%'.format((len(combined_dummy[(combined_dummy['Generic Title'] == title) & (combined_dummy['Survived'] == 0)])) \/ (len(combined_dummy[(combined_dummy['Generic Title'] == title) & (combined_dummy['Survived'].isnull() == False)])) * 100))\n    print('------------')","aa7c6156":"def family_size(train_dummy):\n    sibsp_size = train_dummy['SibSp']\n    parch_size = train_dummy['Parch']\n    \n    sibsp_size = list(sibsp_size)\n    parch_size = list(parch_size)\n    \n    family_size = []\n    \n    for i in range(len(train_dummy)):\n        family_size.append(sibsp_size[i] + parch_size[i] + 1)\n        \n    train_dummy['Family Size'] = family_size\n    \n    return train_dummy","57f6bb98":"combined_dummy = family_size(combined_dummy)","59412af1":"plt.figure(figsize = (10, 5))\nsns.countplot(x = combined_dummy['Family Size'], hue = combined_dummy['Survived'])","a8c45b20":"family_arr = combined_dummy['Family Size'].unique()\nfamily_arr.sort()","e1dd237c":"for family in family_arr:\n    print('Percent of passengers with a family size of ' + str(family) + ' who survived: {:.2f}%'.format((len(combined_dummy[(combined_dummy['Family Size'] == family) & (combined_dummy['Survived'] == 1)])) \/ (len(combined_dummy[(combined_dummy['Family Size'] == family) & (combined_dummy['Survived'].isnull() == False)])) * 100))\n    print('Percent of passengers with a family size of ' + str(family) + ' who did not survive: {:.2f}%'.format((len(combined_dummy[(combined_dummy['Family Size'] == family) & (combined_dummy['Survived'] == 0)])) \/ (len(combined_dummy[(combined_dummy['Family Size'] == family) & (combined_dummy['Survived'].isnull() == False)])) * 100))\n    print('------------')","40764ad4":"def family_category(train_dummy):\n    family_size = train_dummy['Family Size']\n    family_size = list(family_size)\n    \n    family_category = []\n    \n    for i in range(len(family_size)):\n        if family_size[i] == 1:\n            family_category.append('Alone')\n        elif family_size[i] == 2 or family_size[i] == 3 or family_size[i] == 4:\n            family_category.append('Small')\n        elif family_size[i] == 5 or family_size[i] == 6:\n            family_category.append('Medium')\n        elif family_size[i] == 7 or family_size[i] == 8 or family_size[i] == 11:\n            family_category.append('Large')\n        else:\n            print('Invalid case for family size: ', family_size[i])\n            \n    train_dummy['Family Category'] = family_category\n    \n    return train_dummy","efeeb743":"combined_dummy = family_category(combined_dummy)","6f44a0bb":"combined_dummy.head()","0ffc4cd6":"plt.figure(figsize = (10, 5))\nsns.countplot(x = combined_dummy['Family Category'], hue = combined_dummy['Survived'],\n              order = combined_dummy['Family Category'].value_counts().index)","8f2d212f":"category_arr = combined_dummy['Family Category'].unique()\ncategory_arr.sort()","b9e97ce2":"for category in category_arr:\n    print('Percent of passengers categorized as ' + str(category) + ' who survived: {:.2f}%'.format((len(combined_dummy[(combined_dummy['Family Category'] == category) & (combined_dummy['Survived'] == 1)])) \/ (len(combined_dummy[(combined_dummy['Family Category'] == category) & (combined_dummy['Survived'].isnull() == False)])) * 100))\n    print('Percent of passengers categorized as ' + str(category) + ' who did not survive: {:.2f}%'.format((len(combined_dummy[(combined_dummy['Family Category'] == category) & (combined_dummy['Survived'] == 0)])) \/ (len(combined_dummy[(combined_dummy['Family Category'] == category) & (combined_dummy['Survived'].isnull() == False)])) * 100))\n    print('------------')","b6ffb5c1":"combined_dummy.head()","4ba6a52d":"combined_dummy[combined_dummy['Ticket'] == '113572']","1e474592":"combined_dummy['Ticket Frequency'] = combined_dummy.groupby('Ticket')['Ticket'].transform('count')","92ad90a5":"plt.figure(figsize = (10, 5))\nsns.countplot(x = combined_dummy['Ticket Frequency'], hue = combined_dummy['Survived'],\n              order = combined_dummy['Ticket Frequency'].value_counts().index)","d0038a65":"ticket_freq_arr = combined_dummy['Ticket Frequency'].unique()\nticket_freq_arr.sort()","f072b751":"for ticket_freq in ticket_freq_arr:\n    print('Percent of passengers travelling in a group of ' + str(ticket_freq) + ' who survived: {:.2f}%'.format((len(combined_dummy[(combined_dummy['Ticket Frequency'] == ticket_freq) & (combined_dummy['Survived'] == 1)])) \/ (len(combined_dummy[(combined_dummy['Ticket Frequency'] == ticket_freq) & (combined_dummy['Survived'].isnull() == False)])) * 100))\n    print('Percent of passengers travelling in a group of ' + str(ticket_freq) + ' who did not survive: {:.2f}%'.format((len(combined_dummy[(combined_dummy['Ticket Frequency'] == ticket_freq) & (combined_dummy['Survived'] == 0)])) \/ (len(combined_dummy[(combined_dummy['Ticket Frequency'] == ticket_freq) & (combined_dummy['Survived'].isnull() == False)])) * 100))\n    print('------------')","db2d4b05":"combined_dummy.head()","bd61e031":"print('Percentage of women who survived in 1st class: {:.2f}%'.format((len(combined_dummy[(combined_dummy['Survived'].isnull() == False) & (combined_dummy['Sex'] == 'female') & (combined_dummy['Pclass'] == 1) & (combined_dummy['Survived'] == 1)])) \/ (len(combined_dummy[(combined_dummy['Survived'].isnull() == False) & (combined_dummy['Sex'] == 'female') & (combined_dummy['Pclass'] == 1)]['Survived'])) * 100))\nprint('Percentage of women who survived in 2nd class: {:.2f}%'.format((len(combined_dummy[(combined_dummy['Survived'].isnull() == False) & (combined_dummy['Sex'] == 'female') & (combined_dummy['Pclass'] == 2) & (combined_dummy['Survived'] == 1)])) \/ (len(combined_dummy[(combined_dummy['Survived'].isnull() == False) & (combined_dummy['Sex'] == 'female') & (combined_dummy['Pclass'] == 2)]['Survived'])) * 100))\nprint('Percentage of women who survived in 3rd class: {:.2f}%'.format((len(combined_dummy[(combined_dummy['Survived'].isnull() == False) & (combined_dummy['Sex'] == 'female') & (combined_dummy['Pclass'] == 3) & (combined_dummy['Survived'] == 1)])) \/ (len(combined_dummy[(combined_dummy['Survived'].isnull() == False) & (combined_dummy['Sex'] == 'female') & (combined_dummy['Pclass'] == 3)]['Survived'])) * 100))","a564a880":"percent_survival_list = [96.81, 92.11, 50.00]\npclass_list = [1, 2, 3]\n\nplt.figure(figsize = (10, 5))\nsns.lineplot(x = pclass_list, y = percent_survival_list)","aa6de4a5":"from sklearn.preprocessing import LabelEncoder","c53696fa":"label_encode_features = ['Age Bins', 'Fare Bins']\n\nfor feature in label_encode_features:\n    combined_dummy[str(feature) + ' Encoded'] = LabelEncoder().fit_transform(combined_dummy[feature])","2294d465":"from sklearn.preprocessing import OneHotEncoder","960beb4c":"one_hot_encoded_features = ['Sex', 'Deck', 'Embarked', 'Generic Title', 'Family Category']\n\nencoded_features = []\n\nfor feature in one_hot_encoded_features:\n    encoded_feature_array =  OneHotEncoder().fit_transform(combined_dummy[feature].values.reshape(-1, 1)).toarray()\n    column_names = [str(feature) + '_{}'.format(n) for n in range((combined_dummy[feature].nunique()))]\n    one_hot_df = pd.DataFrame(encoded_feature_array, columns = column_names)\n    one_hot_df.index = combined_dummy.index\n    encoded_features.append(one_hot_df)\n    \ncombined_dummy = pd.concat([combined_dummy, *encoded_features], axis=1)","6f0fdbc2":"combined_dummy.head()","8e0a3c33":"drop_columns = ['PassengerId', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Embarked', 'Deck', 'Fare Bins', 'Age Bins', 'Title', 'Generic Title', 'Family Category', 'Family Size']\n\ncombined_dummy.drop(columns = drop_columns, axis = 1, inplace = True)","54779d43":"combined_dummy.head()","7abb6e89":"train_df = combined_dummy[combined_dummy['Survived'].isnull() == False]\ntest_df = combined_dummy[combined_dummy['Survived'].isnull() == True]","8d9ad8e2":"# Creating training dataframe\nX_train = train_df.drop('Survived', axis = 1, inplace = False)\ny_train = train_df['Survived'].values\n\n# Creating the test dataframe\nX_test = test_df.drop('Survived', axis = 1, inplace = False)","ecac4904":"from sklearn.preprocessing import StandardScaler","fe925056":"X_train = StandardScaler().fit_transform(X_train)\nX_test = StandardScaler().fit_transform(X_test)","6ee9130d":"from sklearn.linear_model import LogisticRegression","81f00986":"# Fitting the model on training data\nlog_reg_model = LogisticRegression()\nlog_reg_model.fit(X_train, y_train)","e0167d95":"from sklearn.model_selection import cross_val_score","845badff":"log_reg_accuracy = cross_val_score(log_reg_model, X_train, y_train).mean() * 100","e58398b9":"print('Accuracy: {:.2f}%'.format(log_reg_accuracy))","033fd6af":"from sklearn.metrics import classification_report, confusion_matrix","5ae79853":"print(confusion_matrix(y_train,  log_reg_model.predict(X_train)))","9c17923f":"print(classification_report(y_train, log_reg_model.predict(X_train)))","1b142ec8":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV","d3069a02":"param_grid = [    \n    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n    'C' : np.logspace(-4, 4, 20),\n    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n    'max_iter' : [100, 1000,2500, 5000]\n    }\n]","61d71678":"log_reg_random = RandomizedSearchCV(log_reg_model, param_distributions = param_grid, cv = 5, verbose=True, n_jobs=-1)","a2674aa5":"best_log_reg_random = log_reg_random.fit(X_train,y_train)","cc489c87":"best_log_reg_random.best_estimator_","7881bab1":"def random_search_cv(model, param_grid):\n    # Creating a Randomized Search Model\n    model_random = RandomizedSearchCV(model, param_distributions = param_grid, cv = 3, verbose=True, n_jobs=-1)\n    \n    # Fitting the model to the data\n    best_model_random = model_random.fit(X_train,y_train)\n    print('Best Estimator: \\n', best_model_random.best_estimator_)\n    \n    return best_model_random.best_estimator_","6c1aff23":"def model_evaluation(model):\n    # Calculating accuracy\n    model_accuracy = cross_val_score(model, X_train, y_train).mean() * 100\n    print('Accuracy: {:.2f}%'.format(model_accuracy))\n    \n    # Printing the confusion matrix\n    print('\\nConfusion Matrix: ')\n    print(confusion_matrix(y_train,  model.predict(X_train)))\n    \n    # Printing the classification report\n    print('\\nClassification Report: ')\n    print(classification_report(y_train, model.predict(X_train)))","7e2581d0":"model_evaluation(best_log_reg_random.best_estimator_)","c1f496f7":"param_grid = [    \n    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n    'C' : np.logspace(1, 2, 20),\n    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n    'max_iter' : [800, 900, 1000, 1500, 1800]\n    }\n]","87d4c447":"def grid_search_cv(model, param_grid):\n    # Creating a Grid Search Model\n    model_grid = GridSearchCV(model, param_grid= param_grid, cv = 5, verbose=True, n_jobs=-1)\n    \n    # Fitting the model to the data\n    best_model_grid = model_grid.fit(X_train,y_train)\n    print('Best Estimator: \\n', best_model_grid.best_estimator_)\n    \n    return best_model_grid.best_estimator_","3db7ed2a":"best_log_reg_estimator = grid_search_cv(log_reg_model, param_grid)","5afed9e4":"model_evaluation(best_log_reg_estimator)","573b3484":"from sklearn.svm import SVC","bf31f065":"# Fitting the model on training data\nsvc_model = SVC()\nsvc_model.fit(X_train, y_train)","0bc401e7":"print('Accuracy: {:.2f}%'.format(cross_val_score(svc_model, X_train, y_train).mean() * 100))","66ce8b86":"print(confusion_matrix(y_train,  svc_model.predict(X_train)))","5cc0f36e":"print(classification_report(y_train, svc_model.predict(X_train)))","3125ef01":"param_grid = [{\n    'C': [0.1, 1, 10, 100, 1000],\n    'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n    'kernel': ['rbf', 'poly', 'linear', 'sigmoid'],\n    'degree': [2, 3, 5, 7, 10]\n}]","9f11645e":"random_svc_best_model = random_search_cv(svc_model, param_grid)","bc6395d4":"model_evaluation(random_svc_best_model)","0ce3c306":"param_grid = [{\n    'C': [80, 85, 90],\n    'gamma': [0.0001, 0.0025, 0.003],\n    'kernel': ['rbf', 'linear', 'sigmoid'],\n}]","abb4e3df":"grid_svc_best_model = grid_search_cv(svc_model, param_grid)","d01b0d86":"model_evaluation(grid_svc_best_model)","d15625b2":"from sklearn.tree import DecisionTreeClassifier","68a086f9":"# Fitting the model on training data\ndt_model = DecisionTreeClassifier()\ndt_model.fit(X_train, y_train)","ec98dadb":"print('Accuracy: {:.2f}%'.format(cross_val_score(dt_model, X_train, y_train).mean() * 100))","b8a4c149":"print(confusion_matrix(y_train,  dt_model.predict(X_train)))","fce36b64":"print(classification_report(y_train, dt_model.predict(X_train)))","76e20bba":"param_grid = [    \n    {'splitter' : ['best', 'random'],\n     'max_depth' : np.linspace(1, 32, 32, endpoint=True),\n     'min_samples_split' : [i for i in range(1, 11)],\n     'min_samples_leaf' : np.linspace(0.1, 0.5, 10, endpoint=True),\n     'max_features' : list(range(1,X_train.shape[1])),\n    }\n]","d4a64f6d":"random_dt_best_model = random_search_cv(dt_model, param_grid)","e37f80ed":"model_evaluation(random_dt_best_model)","30c762ee":"dt_model = DecisionTreeClassifier(max_features = 8, splitter = 'best', min_samples_split = 10)","2a761d65":"param_grid = [    \n    {'max_depth' : np.linspace(1, 5, 5, endpoint=True),\n     'min_samples_leaf' : np.linspace(0.01, 0.1, 10, endpoint=True),\n    }\n]","a172f2da":"grid_dt_best_model = grid_search_cv(dt_model, param_grid)","f6240a88":"model_evaluation(grid_dt_best_model)","b00eb8b3":"from sklearn.ensemble import RandomForestClassifier","b14bea6a":"# Fitting the model on training data\nrf_model = RandomForestClassifier(n_estimators=1000)\nrf_model.fit(X_train, y_train)","1bb17422":"print('Accuracy: {:.2f}%'.format(cross_val_score(rf_model, X_train, y_train).mean() * 100))","79e56571":"print(confusion_matrix(y_train,  rf_model.predict(X_train)))","e7bf768e":"print(classification_report(y_train, rf_model.predict(X_train)))","9ad9d7b3":"param_grid = [\n  {'n_estimators': [int(x) for x in np.linspace(start = 1500, stop = 2500, num = 10)],\n    'max_features': ['auto', 'sqrt'],\n    'max_depth': [int(x) for x in np.linspace(5, 50, num = 11)],\n    'min_samples_split': [2, 3, 5, 7, 10],\n    'min_samples_leaf': [3, 4, 6, 8],\n    'random_state' : [42, 43],\n    'bootstrap': [True, False]\n  }\n]","3ad13bf1":"random_rf_best_model = random_search_cv(rf_model, param_grid)","2197eb0b":"model_evaluation(random_rf_best_model)","abf2340b":"rf_model = RandomForestClassifier(max_features='sqrt', random_state=43, max_depth=17, min_samples_leaf=3, min_samples_split=7)","4f7ce1d3":"param_grid = [\n  { 'n_estimators' : [int(x) for x in np.linspace(start = 23, stop = 30, num = 7)]  }\n]","62b6e33a":"grid_rf_best_model = grid_search_cv(rf_model, param_grid)","1c6131cc":"model_evaluation(grid_rf_best_model)","1e2651a9":"from sklearn.neighbors import KNeighborsClassifier","56455774":"# Fitting the model on training data\nknn_model = KNeighborsClassifier(n_neighbors = 5)\nknn_model.fit(X_train, y_train)","3e5fb598":"print('Accuracy: {:.2f}%'.format(cross_val_score(knn_model, X_train, y_train).mean() * 100))","f685b7d6":"print(confusion_matrix(y_train,  knn_model.predict(X_train)))","bfe16e6f":"print(classification_report(y_train, knn_model.predict(X_train)))","7a602588":"param_grid = [\n  {'n_neighbors' : [3, 5, 7, 9],\n   'algorithm' : ['auto', 'ball_tree', 'kd_tree'], \n   'leaf_size': [10, 20, 30, 40],\n  }\n]","5d2e8136":"random_knn_best_model = random_search_cv(knn_model, param_grid)","7e4bab22":"model_evaluation(random_knn_best_model)","a0014961":"n_neighbours = [3, 5, 7, 9, 11, 13, 15, 17]\naccuracy_list = []\n\nfor neighbour in n_neighbours:\n    knn_model = KNeighborsClassifier(algorithm='ball_tree', leaf_size=20, metric='minkowski',\n                     metric_params=None, n_jobs=None, n_neighbors=neighbour, p=2,\n                     weights='uniform')\n    accuracy_list.append(cross_val_score(knn_model, X_train, y_train).mean() * 100)\n    print('For neighbour: ' + str(neighbour) + ' accuracy is {:.2f}%'.format(cross_val_score(knn_model, X_train, y_train, cv = 5).mean() * 100))\n    \nplt.figure(figsize = (10, 5))\nplt.xticks(np.arange(3, 18, 2))\nplt.plot(n_neighbours, accuracy_list)","2d841181":"knn_model = KNeighborsClassifier(n_neighbors=5)","81d8bdb2":"param_grid = [\n  {\n      'algorithm' : ['auto', 'ball_tree', 'kd_tree'], \n      'leaf_size': [5, 6, 7, 9, 11],\n      'p': [1, 2]\n  }\n]","75a45188":"grid_knn_best_model = grid_search_cv(knn_model, param_grid)","1bc4917f":"model_evaluation(grid_knn_best_model)","e3efd66d":"from sklearn.linear_model import SGDClassifier","98af2385":"# Fitting the model on training data\nsgd_model = SGDClassifier()\nsgd_model.fit(X_train, y_train)","227ea7c9":"print('Accuracy: {:.2f}%'.format(cross_val_score(sgd_model, X_train, y_train).mean() * 100))","5913f94e":"print(confusion_matrix(y_train,  sgd_model.predict(X_train)))","1cd6b6ee":"print(classification_report(y_train, sgd_model.predict(X_train)))","05c54a42":"param_grid = [\n  {\n      'loss' : ['hinge', 'log', 'squared_hinge', 'modified_huber'],\n      'alpha' : [0.0001, 0.001, 0.1, 0.5, 1],\n      'penalty' : ['l2', 'l1'],\n      'max_iter' : [4000, 5000, 6000, 7000]\n  }\n]","169eb3b9":"random_sgd_best_model = random_search_cv(sgd_model, param_grid)","c1d7a51b":"model_evaluation(random_sgd_best_model)","28d47d65":"sgd_model = SGDClassifier(penalty = 'l2', max_iter = 5000)","5b213c97":"param_grid = [\n  {\n      'loss' : ['hinge', 'log', 'squared_hinge', 'modified_huber'],\n      'alpha' : [0.4, 0.5]\n  }\n]","c6b3a6ce":"grid_sgd_best_model = grid_search_cv(sgd_model, param_grid)","df2b74ea":"model_evaluation(grid_sgd_best_model)","f3023db3":"tuned_model_summary = pd.DataFrame({'Model' : ['Logistic Regession', 'Support Vector Classifier', 'Decision Tree', 'Random Forest', 'K-Nearest Neighbours', 'SGDClassifier'],\n                       'Accuracy' : [82.72, 82.83, 79.80, 84.51, 83.28, 80.13],\n                       'False Negatives' : [71, 62, 85, 35, 46, 59]})\n\ntuned_model_summary","3825ec24":"from sklearn.metrics import roc_curve","9ab81be1":"model_list = [best_log_reg_estimator, grid_svc_best_model, grid_dt_best_model, grid_rf_best_model, grid_knn_best_model, grid_sgd_best_model]\nx = np.linspace(0, 1, 11)\ny = np.linspace(0, 1, 11)\nplt.figure(figsize = (10, 8))\n\nfor model in model_list:\n    fpr, tpr, thresholds = roc_curve(y_true = y_train, y_score = model.predict(X_train), pos_label=0)\n    if model == model_list[0]:\n        model_name = 'Logistic Regression'\n    elif model == model_list[1]:\n        model_name = 'Support Vector Classifier'\n    elif model == model_list[2]:\n        model_name = 'Decision Tree'\n    elif model == model_list[3]:\n        model_name = 'Random Forest'\n    elif model == model_list[4]:\n        model_name = 'K-Nearest Neighbours'\n    elif model == model_list[5]:\n        model_name = 'SGD Classifier'\n    plt.plot(tpr, fpr, label = model_name)\n    \nplt.plot(x, y, color = 'black')\nplt.legend(loc = 'lower right')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()","b83bdc1d":"y_pred = grid_rf_best_model.predict(X_test).astype(int)","d9a0a449":"submission_df = pd.DataFrame({'PassengerId' : test['PassengerId'],\n                              'Survived' : y_pred.tolist()})","68dd41c0":"submission_df.to_csv('submission.csv', header = True, index = False)","3c544f02":"1. <b>We can therefore conclude that people travelling alone had a poor chance of survival.<\/b>\n2. <b>A family smaller family size had a better chance of survival.<\/b> (`SibSp` - 1, 2; `Parch` - 1, 2, 3)\n3. <b>A larger family had again a poor chance of survival<\/b> (`SibSp` - 3, 4, 5, 8; `Parch` - 4, 5, 6)","3ad2fe47":"Since now we are using the feature `Deck` instead of `Cabin`, we can drop `Cabin` feature.","1a3d1c77":"Mostly `Age` and `Cabin` is missing in the entire dataset.","5d60fdd2":"Since this is a first class passenger, we can club him with the **ABC** category.","b5387578":"Definitely we can see that at some point above the age of **15** and below the age of **20**, we had a high survival rate.\n\nBased on the graph, an estimate can be **18**.\n\nAlso there is a spike for the age group with an age less than **10**.\n\nSo we can come to a conclusion that younger people had a high chance of survival. This is quite natural as younger children are given priority during an evacuation.\n\nAlso do notice that somewhere around the age of **30** to **40**, the high rate of survival is more pronounced.\n\nThis rate of survival gradually diminishes as the value of `Age` increases. Quite logical as it would have been difficult for elder people to move around and so there was a high rate of deaths among that age group.","9e34eed4":"Let's begin with **IQR**.","060e7f67":"Alltogether, we can say that passengers with high ticket fare had a high chance of survival. However from the graph we can see that it's positively skewed and therefore we have a high chance of outliers here.","c7991038":"This model gives us an accuracy of **83.61%**.\n\nNumber of **False Negative** cases are **48**.","062a2685":"This model gives us an accuracy of **80.80%**.\n\nNumber of **False Negative** cases are **14**.","f08bfded":"One thing is quite clear from the above plot. Those passengers who had a high fare value had a greater chance of survival. The above positively skewed graph makes this quite clear.\n\nThe **Blue** line which denotes the passengers who survived extends all the way to some 500+ value.\n\nThis should definitely correspond with passengers from the first class.","2b229a44":"Not much difference can be observed for the feature `Age` but we can see quite a difference for `Fare`.\n\nLet's plot this separately to check this difference.","3f1a3d9b":"Clearly, there are more number of females who survived the accident.\n\nThis is expected also as during an accident, children and women are rescued first. Also, if one remembers the movie 'Titanic', the same practice was being followed there, women and children were being evacuated first.","169f5f42":"We can clearly see that passengers with a family size of **2**, **3** and **4** had a higher chance of survival.\n\nTo understand this further, we will find the percentages of survival for each family size.","f59f7c9e":"Based on the above scores, it's quite obvious that we are going to select the **Random Forest** model.","d306db71":"The most logical solution to this problem would be that these passengers were staff members who worked on board the ship and therefore had no ticket fare.","089c5f31":"As we saw for `SibSp` feature, an almost similar trend is observed here.\n\nMore than half of the passengers travelling alone did not survive.\n\nPassengers with **1** ot **2** parents\/children had a better chance of survival, although the threshold is just a bit higher for passengers having **1** parent\/children and is almost 50% for passengers having **2** parent\/children.\n\nAnother thing to observe os that almost 60% passengers having **3** parents\/children survived the accident.\n\nThe trend then again begins to match that of `SibSp` where larger family i.e. passengers with **4**, **5** and **6** parents\/children had a poor chance of survival.","f266bc9f":"Based on the above data, we can see among the decks housing first class passengers, cabin **B** had the highest survival rate, followed by **C** and **A**.\n\nDecks **D** and **E** has a similar passenger class data distribution. Each decks also has a similar survival rate.\n\nDecks **F** and **G** doesn't house any first class passenger and each had an average survival rate.","3f11af70":"**Filtering `Fare` Feature**","c2068a6a":"We shall now calculate the **Lower Bound** and **Upper Bound**.","912e692b":"### 7. Exploring relationships between features","5378fbce":"#### 1. Using graphical approach through <b>Heatmap<\/b>.","f0c0657c":"Based on what we can gather from the data above is that there were different compartments or decks with it's labels as - **A, B, C, D, E, F, G**.\n\nWe shall create another feature `Deck` which will take this **compartments** into account. ","ce0d6d0c":"### 2. Missing values","a4fabab4":"Let's see where most of the people with no siblings or spouce or parents or children boarded from.\n\nWe are trying to investigate this because we notice that both our passengers had the attributes `SibSp` and `Parch` value as **0**.","521bdba6":"Next we'll analyse this dataframe for the survival chances of individual family sizes.","850d774f":"#### 6.1 Exploring the relation between `Pclass` and `Survived`","577d0872":"#### 11.2. Support Vector Classifier","df0d1640":"We shall also scale our features as most machine learning algorithms assume that data is normally distributed.","31b9149b":"Let's calculate our **training accuracy**.","e67784d8":"Clearly, a linear realtionship doesn't define the above data. \n\nHowever, there is no clearly defined rule as such. It's all upto experimentation and investigating which model performs better on what kind of data.\n\nFor the purpose of this notebook, we shall resort to use the pre label-encoded `Pclass` feature.","0fb64712":"Based on this we can see that only a single passenger with such a case survived.\n\nAlso do note that there were passengers from second and first class as well who had a fare of **0.00**.","10eae380":"Let's just check this distribution.","8fd10dfb":"#### 11.2.1. Hyperparameter Tuning","b41a3ae6":"Clearly there's a lot of variance for the fare rate for those passengers having three siblings\/spouce.\n\nIn contrast the fare rate for passengers having eight siblings\/spouce is consistent with it being around **69.55**.\n\nIs there only a single family with this consistent result?","0d33299d":"We still can see a trend that the survival chances for the passengers with a higher fare seems to be high.\n\nSimilarly, we shall do binning of `Age` feature and attempt to capture any trend in these groups.","cd67e637":"We shall try to implement the most basic model and check it's performance.","43e0eeb6":"#### 9.3. Investigating Ticket Feature","441f3840":"We can observe that the title **'L.'** was incorrectly taken.\n\n<b>So for our understanding:-<\/b>\n\n1. 'Mlle.' is french for 'Ms.'.\n\n\n2. 'Mme.' is french for 'Madame'.\n\n\n3. 'Jonkheer' is an honorific title given to a person holding the lowest rank within the nobility.\n\n\n4. 'Rev.' is a title of a priest.\n\n\nWe shall remove this entry of **'L.'** from the list.","b469eea3":"#### 7.7. Exploring relationship between `Parch` and `Fare`.","51b422d5":"#### 11.2.1. Hyperparameter Tuning","55120972":"Let's first begin by checking this instance.","d4b7c5d5":"#### 7.5. Exploring relationship between `SibSp` and `Parch`.","ebddfa45":"#### 8.2. Fare","870cfcb7":"We shall plot the counts of titles for those passengers who survived the accident.","bbe36d57":"Although the evaluation metrics for the competition is **Accuracy**, it's a good practice to check other evaluation parameters for the model.","6da17a58":"Based on this, we get the value for **Lower Bound** and **Upper Bound** beyond which datapoints will be considered outliers.\n\nLet's see what percent of data are now being categorized as outliers based on the above criteria.","6b024b18":"Clearly, we can see tha all missing values have been taken care of. The missing values for `Survived` corresponds to the data from the **test** dataset.","17698037":"Let's compare the data plots before and after implementing the filters for each feature.","94f0c116":"Was there any significant difference between fares of different genders with similar properties?\n\nIt might be possible that **females** might have received some concession.","eea0735c":"We can see clearly that passengers with title of **Mrs\/Miss\/Ms** had a high chance of survival followed by **Master**.","7ed9b753":"* `PassengerId` is a unique ID assigned to each passenger.\n* `Survived` is the variable we are trying to predict (**0**[Not Survived] or **1**[Survived]):\n* `Pclass` (Passenger Class) is the ticket class of the passenger. This is a categorical feature with values (**1**,  **2** or **3**)\n* `Name`, `Sex` and `Age` are the name, gender and age for each passenger.\n* `SibSp` is the number the passengers' siblings and spouse.\n* `Parch` is the number of the passengers' parents and children.\n* `Ticket` is the passenger's ticket number.\n* `Fare` is the passenger's ticket fare.\n* `Cabin` is the cabin number of the passenger.\n* `Embarked` is port of at which the passenger embarked from. This is a categorical feature and has **3** unique values (**C**, **Q** or **S**)","28a88762":"One thing which is very much pronounced is that the frequency of **Red** dots signifying *females* are extremely high in the **Survived** graph(left side) compared and the number of **Blue** dots signifying *males* are extremely high in the **Not Survived** graph(right side).\n\nThis clearly tells us that there were more number of females who survived the accident.","0af65b53":"One example of such occurrence is the following.","7eb0ccf8":"`Age` seems to be highly correlated `Pclass` and `SibSp`.\n\n`Sex` seems also to play an important role.","99853657":"We will ignore the decks labeled **M** as we have no info on these passengers. We shall focus only on decks **ABC**, **DE** and **FG**.","481cb8fa":"We can see that there's almost a similar **60:40** ratio of survival rate between both the genders which means there wasn't any bias between male and female children.","046ca5b3":"Let's take a look at our newly created dataframe.","336981ec":"Before we go ahead with this, it would be better to do sich changes to a dummy train dataframe.\n\nThis will act as a checkpoint if something goes wrong.","43c80e5c":"#### Age and Fare distribution","b1d40564":"Let's check the minimum amount of ticket fare for those who survived the accident.","c45fa4e7":"We shall now begin the replacement.","9f0e4037":"### 11. Submission","d00245f4":"#### 11.2.1. Hyperparameter Tuning","ffdddc09":"Remember to change the output type to **integer**.","4adc035c":"We shall explore different methods to check missing values.","e7f505c1":"The feature `Fare` has a positive skewness, this is quite evident from the graph above.\n\nLet's plot a **Boxplot** for the above and check these outliers.","65459129":"Let's begin with analysing continous variables.\n\nOur continous features include:-\n\n1. <b>Age<\/b>\n2. <b>Fare<\/b>","d19a2190":"Now that we have fixed the missing values in our dataset, let's split it back into it's **train** and **test** splits.\n\nThis is important as we need to check if data in **train** split follows same pattern in **test** split.","c347f0b9":"#### 11.3. Decision Tree","8d1f4e37":"We save this dataframe to be submitted.","55acb983":"We create a dataframe for submission.","2ee1a305":"#### 6.5 Exploring the relationship between `Deck` and `Survived`","f8fb019d":"For this particular problem, it would be more important to correctly classify passengers who didn't survive the accident.\n\nBased on our confusion matrix, it seems that **71** passengers were incorrectly classified as those who would survive the accident. These **71** passengers are our **False Negative** cases.\n\nIn this case **False Negatives** are those passengers who were predicted to have survived but actually didn't survive.\n\nIt would be imperative to reduce this number in a real world scenario.","3546c742":"We notice that most of the first class passengers belong to the deck **A, B** and **C**.\n\nDeck **D** and **E** also have a similar type of passenger distribution.\n\nSimilarly, decks **F** and **G** have a similar distribution.\n\nDeck **T** has only one passenger. This one passenger needs to be explored further.","0389a466":"### 4. Let's check the correlations in our data","8935faab":"#### 2. Using a simple count of missing values.","ca5d55e0":"Let's break this down into percentages as well.","5cd3f6aa":"Again, after filtering, there isn't much difference in the trend.\n\nYounger age group has the highest chance of survival.\n\nThe age group between **24** and **25** seem to have the lowest survival rate.\n\nThe age group between **32** and **39** seem to have a balanced survival rate.","d314e43e":"Thus we can say that there is a higer rate of survival among first class passengers.","2940fc3d":"Again, we can see that most of the passengers belonged to the second class.","cd45f7c4":"We should expect a similar result as the explanation for the above holds true for this too.","6c054e55":"### 1. Understanding the data","25605d0c":"Based on the analysis above, we can say that more than half of the passengers travelling alone did not survive the accident.\n\n**What can be the reason for this?**\n\nMore than half of the passengers having **1** sibling\/spouce survived the accident.\n\nAn almost similar trend can be observed with passengers having **2** siblings\/spouce. Although less than half of the passengers survived the accident, there is a very small margin between those who survived and those who didn't. It's better to club these people with the group having 1 sibling\/spouce as both show a similar trend.\n\nAs we go on further with passengers having **3**, **4**, **5** and **8** siblings, the survival rate decreses. One reason could be that having such a large family would have made it difficult to move around and thus ended up mostly getting stranded or stuck.","0bd65d7b":"We perform this parameter search operation from the search space.","0ff7ada3":"We create a new feature based on such ticket frequency.","48a24f20":"We can see that females had a higher chance of survival based on the plot above.\n\nWe shall also calculate the percentage of males and females who survived.","5a90bd5d":"#### 10.1. Label Encoding Features","aae7cca3":"### References:-\n\nhttps:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial","0c888760":"### 10. Feature Encoding","122e359b":"Let's try to get an **ROC Curve** for this model.\n\nThe more the area an **ROC Curve** covers, the better.","a0b7df0f":"#### 6.5 Exploring the relationship between `Embarked` and `Survived`","e3848a11":"Let's also check the frequency of these titles.","f24998fd":"**Filtering `Age` Feature**","7f2c6331":"We can clearly see through the plots and percentage data that first class passengers had a higher chance of survival than any other classes.\n\nThough the second class passengers through the plot seems to have less number of survivers when compared to the third class passengers, on taking out percentage values, we can clearly see that second class passengers has a higher survival rate than third class passengers.","ac8e7617":"**Upper Bound** and **Lower Bound** according to **Z-Score** are:-","a6d9521c":"#### 5.2. Exploring `Age` Distribution","19cbedde":"Now we will explore the median value for `Fare` for **females**.","9f545ec9":"The `Cabin` feature is a bit different from all other features. This has a lot of missing values with **1014** values missing which accounts for  **77.46%** of the total values.\n\nFor such cases it's better to drop the entire feature. However, there is a high chance that `Cabin` played a role in determining the survivability of a passenger.","1d8f825b":"Quite evident to our assumption. The maximum value of fare for a passenger who survived is **512.32** as expected.","731e643d":"#### I. Randomized Search","9cba7ad7":"From the case above, we can clearly see that this was a third class passenger with no siblings or spouce or parents or children.\n\nTherefore, we can impute this case with other data instances with similar properties.\n\nLet's explore these instances.","7d0feb2c":"First let's merge the **train** and **test** datasets to have a **combined** dataset.","343da864":"Was there any bias among **male** and **female** children less than 10 years of age during evacuation?","8999c747":"Clearly we can see that there is a taller **Blue** spike signifying **Survivors** for children less than 10 years of age compared to the **Blue** spike for passengers greater than 10 years of age. \n\nLet's also check the role of gender during evacuation.","56d8d68f":"Going with our percentages for granular level analysis.","d7379ebd":"#### 7.2. Exploring relationship between `Pclass` and `Fare`.","dde895be":"#### 7.1. Exploring relationship between `Pclass` and `Age`.","c364bfff":"#### 7.3. Exploring relationship between `Age`, `SibSp` and `Parch`.","e8600c11":"#### I. Randomized Search","94c2478c":"#### I. Randomized Search","a277c7f8":"#### 3. A more detailed explanation can be given by writing the code manually","aefea220":"Let's explore the instances where there are missing values for the `Embarked` feature.","ebd61ffe":"#### Exploring and imputing Age","658a3a9e":"Let's begin with analysing the relation of `Pclass` and `Survived`. We had done this analysis before but we shall delve into it again.\n\nFrom our previous analysis, we observed that passengers from the first class had a higher survival rate and while calculating the percentage as well, we found that **62.96%** first class passengers survived.","6f33363e":"#### 10.2. One-Hot Encoding Features","7b1f97f7":"We can see a paradox appearing here. Even though the average fare value for passengers with four parents\/children is high compared to the average fare of other passengers, almost all of them didn't survive the accident.\n\nLet's go deeper into this.","4f548f44":"We will print the names with these titles to check the validity of such titles.","2475e422":"Again we shall focus on the **Upper Bound** for the feature `Fare`.\n\n`Fare` **Upper Bound** is **164.8667**","4d21ad39":"##### Conclusion ","680b8fc5":"We get the value of quartiles:-\n1. **Quartile 1** = 22.0\n2. **Quartile 3** = 36.0\n\nNext we shall calculate the **IQR (Inter Quartile Range)** which is the difference between **Quartile 3** and **Quartile 1**.","edf54e7f":"There are a lot of important things we can gauge from here. \n\n1. The first things is both were first class passengers. \n2. They both had the same ticket number.\n3. They both were in the same cabin.\n4. They both had the same fare.\n\nFrom the above information, it seems that these two passengers knew each other. If this assumption is correct, then we may also say that they weren't family members as both have `SibSp` and `Parch` values as **0**.\n\nTherefore, it can be said that both embarked from the same port.","64f47bce":"#### II. Grid Search","37566384":"##### Conclusion","af002b1a":"#### Exploring and Imputing Fare","b31e08a8":"We get the value of quartiles:-\n1. **Quartile 1** = 7.9\n2. **Quartile 3** = 31.28\n\nNow we shall calculate **IQR**.","14ad2e89":"Let's also calculate percentages for each category.","caf34dd2":"Using this method, we lost only about **3%** of our data.\n\nTherefore, it seems this is the best option as overall loss of data is less.","b8b39535":"Let's now try to implement **Z-Scores** method.","325d0869":"This model gives us an accuracy of **78.00%**.\n\nNumber of **False Negative** cases are **91**.","f0804651":"A parameter search space needs to be defined for the optimal parameters to be obtained from this search space.","0a741e17":"We will be replacing the missing value with data instances having `Pclass` of **3**, `SibSp` of **0** and `Parch` of **0**.","a63411ae":"### 3. Survival Distribution","b84c4aa9":"### 6. Categorical Feature Analysis","f0e31f06":"We shall go ahead with imputing **S (Southampton)** for the missing values as per our analysis.","8ada96fb":"Now the picture seems to be clear. This paradox appeared due to the presence of a passenger name *Mr. Mark Fortune*. We can see that his ticket fare is extremely high when compared to other passengers having four parents\/children.\n\nWe also do observe a family couple in this dataframe. *Mr.* and *Mrs. Skoog*. We know this due to several reasons:-\n\n1. They had the same value for `Parch`.\n2. They had the same `Fare`.\n3. They both embarked from **S (Southampton)**.\n4. They both seem to have the same surnames.\n5. The biggest clue -> The both had the same `Ticket` number.","7b191fb9":"#### 11.2.1. Hyperparameter Tuning","08684b34":"The same can be represented in percentages","7850fdc2":"#### II. Grid Search","22a1ec8e":"Let's evaluate our model based on these features.\n\nSince we are going to do this every time for each model, we shall create a function.","a1b816c7":"#### 9.1. Investigating Passenger Title ","218324ea":"There's another method through which we can calculate the outliers.\n\nThis is through a method called **Z-Scores**.\n\nLet's try to implement it for the data above.","e84c918b":"Let's begin by exploring this feature.","0492e791":"#### 9.2. Investigating Family Size","ce8bb069":"Better to create a plot too so that it's easy to visualize.\n\nWe create a temporary dataframe to help us visualize the titles.","d45fc94c":"#### 11.6. SGDClassifier","07385b20":"As mentioned above, a small family with **2**, **3** and **4** members had a high chance of survival.\n\nThose who were travelling alone and those who had a large family size had a smaller chance of survival.\n\nInstead of having family sizes, let's group them into different categories:-\n\n1. A passenger with a `Family Size` of **1** can be labeled **Alone**.\n\n2. A passenger with a `Family Size` of **2**, **3** and **4** can be labeled **Small**.\n\n3. A passenger with a `Family Size` of **5** and **6** can be labeled **Medium**.\n\n4. A passenger with a `Family Size` of **7**, **8** and **11** can be labeled **Large**.","de4b368d":"Let's now try to understand how `Age` is associated with other features. This will help us to impute it's value.","5b4672f2":"The graph appears to be normally distributed and younger passengers had a high chance of survival. Elder passengers on the other hand had a low survival rate.","0d406339":"We have only those people who survived above this value of fare. \n\nThis definitely means that as the price of ticket rose, the chance of survival increased as even though passengers with a ticket fare of **263.00** belonged to the first class category, there was a chance that they wouldn't survive.","c6b1e300":"#### I. Randomized Search","49b4f1e9":"Hyperparameter tuning needs to be done to tune the nitty gritty details of a model. This is like tuning a radio once a frequency is obtained to imporve the quality of the sound being produced.\n\nWe will be employing two methods namely **Randomized Search CV** and **Grid Search CV**.\n\n**Randomized Search CV** randomly searches the search space to pick random set of hyperparameters. Since this process is random, it has the added advantage of being fast in searching the parameters. However, the parameter set it returns may not be optimal.\n\n**Grid Search CV** meticulously searches the search space, even checking the corner cases to search for optimal parameter set. Since this searches every corner cases as well, it's time and resource consuming process.","a0757085":"Let's fix certain hyperparameters.","c31ea3be":"Strange, the minimum amount at which a passenger survived is **0.00**.\n\nLet's check the frequency of such cases.","baa04b25":"One thing to notice here is that an increase in the number of neighbours is causing the **accuracy** to fall. However, the number of **False Negatives** in the case of higher value of neighbours is less compared to that for low value of neighbours.\n\nIn summary:-\n\n**N-Neighbours: 3** \n\n-> Accuracy: 83.61% \n\n-> False Negatives: 47\n\n\n**N-Neighbours: 7**\n\n-> Accuracy: 82.60%\n\n-> False Negatives: 41\n\nTherefore, we have a tradeoff that needs to be considered.\n\nSince this problem is evaluated on the basis of **accuracy**, we need to maximize this value. Therefore, it is reasonable to accept the value of **3**.","050fac00":"Similarly, we shall try to filter `Fare` from the *train_dummy* dataframe.","fefb6e18":"## We have made it to the end of the problem Titanic: Machine Learning from Disaster!!!","7152e65b":"We don't have any outliers according to **Z-Score** for the lower limit.\n\nTherefore, we will have to focus on the larger values of `Age`.","40edd910":"We shall be replacing `Age` feature for the above data instances with the **Upper Bound** according to **Z-Score** which is **67.0**.","555fd12e":"One thing we can clearly see is that those passenger who have no siblings or spouce or parents or children are clearly in the majority. These passengers were travelling alone.\n\nThe next highest frequency is observed among those who had a single sibling or spouce but didn't have any parents or children. This can mostly be couples travelling together wihout any parent or child.\n\nThis is followed by those passengers who had a single sibling or spouce along with one parent or child. These passengers can be classified as a small family with the passenger travelling with his\/her spouce along with a child.\n\nSimilarly we can see those passengers who had no siblings or spouce but had a parent or child. This may be mostly children travelling with a single parent or a parent travelling with a single child.\n\nThe next are those passengers travelling with no siblings or spouce but having two parents or children. Again, this can mostly be children travelling with both of their parents or a single parent travelling with two children.","45dc280e":"The best estimator is identified based on **Randomized Search** operation.","c54d440e":"Applying the function.","6a102e50":"We had done a previous analysis on decks and it's passengers. We had also done an analysis of survival chance of passengers based on their ticket class.\n\nWe came to a conclusion that passengers with first class tickets had a higher chance of survival. Also that decks **ABC** tended to house more number of first class passengers, **DE** although housing first class passengers also contained second and third class passengers, while most of **FG** deck had second and third class passengers.","48633263":"We lost around **4.65%** of our data which is not much and we can go ahead with this.","771430be":"We shall impute the missing values in the **combined** dataframe using the median.","8d2a0e79":"Let's also calculate the percentage for the above to get a detailed understanding.","b0b287bd":"We can see that passengers with the title **Mr.** is high in frequency, followed by **Miss.** and **Mrs.**\n\nWe can create a new feature in our dataframe with a name of **Title**.","034f09c3":"This does confirm the fact that most of the passengers from **S (Southampton)** were travelling alone.","4dc68b6b":"Another look at the chances of survival for different passengers based on the number of siblings\/spouce and fare tells us that mostly families with three siblings\/spouce who survived had on an average, high fare of ticket. However, this again has a lot of variability.","59468be3":"During our initial investigation of `SibSp` and `Parch`, we had come up with the following conclusion:-\n\n1. <b>We can therefore conclude that people travelling alone had a poor chance of survival.<\/b>\n2. <b>A family smaller family size had a better chance of survival.<\/b> (`SibSp` - 1, 2; `Parch` - 1, 2, 3)\n3. <b>A larger family had again a poor chance of survival<\/b> (`SibSp` - 3, 4, 5, 8; `Parch` - 4, 5, 6)\n\nAs we saw that a small family size had a better chance of survival, let's make a new feature entirely based on `SibSp` and `Parch` to have a better idea for the same.\n\nTo determine the family size, we shall add `SibSp`, `Parch` and **1** to signify the passenger himself\/herself and name this feature - `Family Size`.","82f12a46":"We can view the first 5 columns by using the 'head()' function to get get a glimpse of the data.\n\nWe can even view more rows by specifying the value as parameters in 'head()'.","a6c924cf":"We can simplify the above tasks through a function.","6be3c7b5":"Here, you have two options.\n\n1. Since these missing values constitute only **0.15%** of the dataset, we can drop them.\n2. These values can be imputed with **S (Southampton)**.","9ea58584":"Based on the graph above, it's quite clear that passengers of the **1** class were older than other classes.","79286455":"These passengers all belong to the first class. Obviously there were cases where first class passengers did not survive.","752666a0":"In order to prevent loss of data, let's replace left outliers with **Lower Bound** and right outliers with **Upper Bound**","896560cc":"We do notice that certain passengers had similar ticket numbers.\n\nThis can mostly be because these passengers with similar ticket numbers were family members and so would have the same ticket number. However, based on the data, we do notice that certain passengers even though not being a family member had the same ticket number.\n\nThis can be possible when there were passengers travelling with their valet, domestic help or nanny.\n\nTherefore even though they weren't a direct family member, there survival chances are high.","0e33fff1":"Many values from the feature `Survived` seems to be missing at the lower end. This is because these data instances are from the **test** set and therefore lack the `Survived` coloumn.","86325d8c":"We shall load the datasets into the dataframes 'train' and 'test'","60791774":"It's clear from the percentages and graphs that those passengers who embarked on **C (Cherbourg)** had more number of survivers.\n\n**Not clear if there is a logical explanation for this to have any impact on survival chances?**","8109a621":"Let's see this distribution with respect to passenger class.\n\nWe shall ignore the **M** category as these are missing values and we can't gain much info from this.","3e84ffce":"### 8. Outlier Detection and Processing","1c7d0b19":"Let's implement the function above to create a new `Family Category` feature.","dad0f8ca":"### 11. Model Building","03ccda01":"#### 11.5. K-Nearest Neighbors","2bd3b9a3":"We only have one missing value for `Fare`.\n\nAgain, we have two choices:-\n\n1. We can drop this missing instance.\n2. We can try to impute it with instances having similar properties.","c98a949d":"We can also use the **Elbow Method** or draw a plot of number of neighbours with the metrics we are interested  in to determine the optimal number of neighbours.","c8b7ada7":"There are only a limited number of cases who had a fare of **0.00** but had managed to survive.\n\nAgain, let's explore these particular cases.","5e0149c1":"We may also manually do hyperparamter tuning and print the accuracy for each changes.\n\nFor now it seems it's the best option to take the **N-Neighbours** value as **5**.","622964b6":"#### 6.4 Exploring the relationship between `Parch` and `Survived`","54ef5210":"Next, we shall explore categorical variables.","f936b595":"#### 7.4. Exploring relationship between `Age` and `Fare`.","1535506d":"As mentioned above, we have a slight positive skewness and this is evident with the dots present around and after the age value of 60.\n\nA few outliers can also be seen at the left end.","9c572bd5":"Similar to the analysis above, we observe that passengers with four parents\/children seem to have a high fare of ticket.","8cc9f91f":"Now that the function is applied, we shall attempt to print the dataframe.","6f2ce721":"In this case we have the following categorical features:-\n\n1. <b>Pclass<\/b>\n2. <b>Sex<\/b>\n3. <b>SibSp<\/b>\n4. <b>Parch<\/b>\n5. <b>Embarked<\/b>\n6. <b>Deck<\/b>","c7d50d45":"#### 8.1. Age","e1c3dbe4":"## Beginning Exploratory Data Analysis","b3041734":"For this model, we got an accuracy of **82.71%**.\n\nNumber of **False Negative** cases are **65**.","65806438":"Based on all this data, we can come up with a strategy. We can club passengers from decks **A**, **B** and **C** as they all house first class passengers and therefore should have a higher rate of survival.\n\nDecks **D** and **E** can be combined as they have similar passenger data distribution and survival rate.\n\nDeck **F** and **G** can be taken together as neither houses any first class passengers and also has a similar survival rate.\n\nFinally we have only a single passenger in deck **T**.","92271af5":"Not a significant difference, as expected. \n\nTherefore, we can go ahead with replacing the missing value with the medain of all such instances not taking into account the gender.","a132885d":"Based on the correlation heatmap, certain interesting correlations are:-\n\n1. <b>`Pclass` and `Survived`<\/b>\n2. <b>`Pclass` and `Age`<\/b>\n3. <b>`Pclass` and `Fare`<\/b>\n4. <b>`Survived` and `Fare`<\/b>\n5. <b>`Age` and `SibSp`<\/b>\n6. <b>`Age` and `Parch`<\/b>\n7. <b>`Age` and `Fare`<\/b>\n8. <b>`SibSp` and `Parch`<\/b>\n9. <b>`SibSp` and `Fare`<\/b>\n10. <b>`Parch` and `Fare`<\/b>\n\nMost of the above correlations would be due to transitive property and therefore each must be analyzed.","37c75420":"Based on the graph above, it's quite evident that passengers with `Pclass` of **1** are on an average elder to passengers from other classes. This average being somwhere around **38**.\n\nPassengers in second class again seem to have a higher average age than those of third class passengers.\n\nAverage age of second class passengers being somewhere around **30** and that of third class passengers being **25**.","5b094371":"#### 11.4. Random Forest","1cb2cab7":"We can see clearly that a small family size had a high chance of survival.\n\nLet's also print the percentages of survival for each to get a granular analysis.","4c8c4854":"### 5. Continous Feature Analysis","933764e3":"We shall try to identify outliers present in our continous features.\n\nLet's plot these features again starting with `Age`.\n\nWe shall resort to use the **combined** dataframe to check the outliers and perform the processing steps.","493d5260":"The passenger who survived:-","29b3998b":"Now that we have filtered the data, let's try to break the data into train and test splits.","f73536dc":"Let's check the survival chances based on this new feature.","96185c13":"There are a few titles that are quite unique in nature. Let's investigate these.","4be8f142":"There are certain interesting relationships that are worth exploring.\n\n1. <b>`Pclass` and `Age`<\/b>\n2. <b>`Pclass` and `Fare`<\/b>\n3. <b>`Age` and `SibSp`<\/b>\n4. <b>`Age` and `Parch`<\/b>\n5. <b>`Age` and `Fare`<\/b>\n6. <b>`SibSp` and `Parch`<\/b>\n7. <b>`SibSp` and `Fare`<\/b>\n8. <b>`Parch` and `Fare`<\/b>","7f50d2ae":"In this case we lost only around **0.61%** of data.\n\nWe can go ahead with the **Z-Scores** method as we tend to loose very few data points.\n\nLet's give the filtered dataframe a look.","83927c12":"#### I. Randomized Search","af2ae984":"We get the value for **IQR** = 14.0\n\nFollowed by this, we shall calculate the **Lower Bound** and **Upper Bound**.\n\nThe significance for this is that any datapoint outside these bounds will be our outliers.","c44a2aa3":"##### Conclusion","eee4ac47":"We can see that the `Age` distribution is fairly normal with a slight positive skewness.\n\nA very common and easy procedure to detect the presence of outliers is to plot a **Boxplot**.","d0cb6a00":"Again, as usual we can take out the percentages for each category of `Ticket Frequency`.","9784e22e":"Correlation for **train** split.","9d53a7c0":"For now, let's see the survival rate for each deck.","307e89d6":"We shall first identify the features to be **One-Hot Encoded** and create a list of these features.","e7dee4c2":"We have the data ready and are now going to fit a model with this data.","35495fa0":"**Imputing missing Age values**","ce38929a":"#### 7.6. Exploring relationship between `SibSp` and `Fare`.","eafb208c":"With this, we have dealt with the missing values in our dataset.\n\nTo ensure this, we shall print the number of missing values in each field.","0df5e5d3":"Now that we have broken the data into it's original splits, let's separate the coloumn `Survived` and the features from **train_df** and **test_df**.","6e10d44b":"#### Exploring and Imputing Embarked","243f03a6":"#### 11.1. Logistic Regression","44ed3518":"We shall plot the `Fare` feature after breaking it into bins for both before and after filtering to check if it has affected the distribution.","701e8c48":"#### Imputing missing Fare values","9355e00a":"Let's calculate the IQR for the above.\n\nWe shall begin with calculating the first and third quartile range.","3034af33":"We shall compile a dataframe of all the **Accuracies** and **False Negatives** for the best hypertuned model for through **Grid Search CV**.","593a6e2a":"#### I. Randomized Search","2b17183b":"#### 6.2 Exploring the relation between `Sex` and `Survived`","993bab98":"Let's find this our for each gender.\n\nWe shall begin with the median value for `Fare` for **males**.","96af8925":"Correlation for **test** split","2af27e14":"Based on the features we have, we can label encode `Age Bins` and `Fare Bins` as these are **ordinal** variables.\n\nShould `Pclass` be considered an **ordinal** variable?\n\nOne thing to know is that when using **ordinal** varaibles, a linear model like an **SVM** or **Logistic Regression** would assume that such **ordinal** variables have some linear relation with our target variable.\n\nLet's take an example of the number of women who survived the accident in each class.","eb6d4fa8":"This model gives us an accuracy of **77.89%**.\n\nNumber of **False Negative** cases are **9**.","d84aec95":"Now that we have encoded all are required features, it's time to drop the columns that are not required.\n\nThese columns would be the following:-\n\n1. PassengerId\n2. Name\n3. Sex\n4. Age\n5. SibSp\n6. Parch\n7. Ticket\n8. Fare\n9. Embarked\n10. Deck\n11. Fare Bins\n12. Age Bins\n13. Title\n14. Generic Title\n15. Family Category\n16. Family Size","7fcb8edb":"From the plot above and percentage, we can see that most of the passengers didn't survive.","972fdeec":"#### 6.3 Exploring the relationship between `SibSp` and `Survived`","e9f57aaa":"Now that we have a `Deck` feature in our dataset, let's try to analyze it.","642597ff":"The maximum value for the ticket of a passenger who didn't survive is **263.0**. Let's check the details of such passengers.","5bc5830f":"It's clearly evident from the above data that passengers who were travelling in a group of **2**, **3** and **4** had a higher chance of survival.\n\nNotice, that this number matches with the feature `Family Size` with respect to the chance of survival.\n\nTherefore, we can say that a higher chance of survival is not limited to `Family Size` but this correlates more with the number of passengers in a group travelling together.","135ead52":"Clearly this is a single family travelling together.","409d58c4":"Let's see if the feature `Embarked` is related with the `Pclass` feature. \n\nSince both of these passengers belonged to the first class, let's see where most of the first class passengers boarded from.","619c0ead":"Let's try to investigate the titles of passengers and check if it has any relation to survival.","58e67589":"#### 11.2.1. Hyperparameter Tuning","96d9f176":"#### II. Grid Search","887f8b23":"Clearly there are a lot of outliers.\n\nAgain we shall go ahead with both the methods, namely - **IQR** and **Z-Scores**.","3194a519":"Again, as it was exprected first class passengers have a high ticket fare and among those the average ticket fare for those who surived is even greater.\n\nSame trend can be observed for second class passenger. Those second class passengers who had a high survival rate had on an average higher fare.","e8bc8008":"Seems like elder people tended to have lesser siblings than younger passengers. Moreover the female `Age` for `SibSp` of **8** seems to be missing.\n\nLet's go in a bit deeper by printing individual **Median** values for each `SibSp` grouped by `Age`.","33754040":"#### II. Grid Search","2234e268":"#### II. Grid Search","522bd57f":"We can clearly see that passengers having three siblings\/spouce had on an average the highest fare among all, however there is a lot of variance in this too.","6c69cfc9":"##### Conclusion","833493ac":"#### 5.1. Exploring `Fare` Distribution","20c96c8b":"Since `Embarked` is categorical feature, we can impute such instances with **S (Southampton)** which we came up with from our analysis.","6d539236":"Does this mean that all passengers with a fare higher than **263.00** survived the accident?","2d2889b7":"**Imputing missing Embarked values**","24a509a3":"#### 11.2.1. Hyperparameter Tuning","98dca1b1":"We gathered all the titles in a list and now we shall take a look at these.","792648af":"### 9. Feature Engineering","581d65ad":"Based on the analysis above, we know that family size played a crucial role in determining if a passenger survived or not.\n\nIn feature engineering section, we shall create a new feature based on family size and determine the survival chances for the families.","146dc856":"From the plot above, **S (Southampton)** seems to be port from where most of the first class passengers boarded from.","91099ad9":"##### Conclusion","e1fcd81f":"Again the plot shows us that most of the passengers with `SibSp` and `Parch` value as **0** boarded from here. Just to quench our thirst of curiosity, lets check the demographics of **S (Southampton)**.","3ee5df37":"We can see that all the data beyond the **yellow** line can be classified as outliers and have been filtered out.\n\nThe spike at the **yellow** point is because all the data beyond that limit has been accumulated there.\n\nWe can even break the `Fare` feature into bins and check it with frequency of survival.","310a5289":"We did see previously that females and children were given more preference during evacuation and this can also be seen in the above plot.\n\nClearly from the previous plot of title frequency, we could see that passengers with the title of **Mr.** were more in number but from the above plot we can see that passengers with titles **Mrs.** and **Miss.** had a higher chance of survival.\n\nThere are certain titles that do not appear in the above plot as none of the passengers with that title survived the accident.\n\nWe do notice that there are certain exotic titles that represent certain type of people but are very few in number. It would be better to club these people together.","42ec9446":"#### II. Grid Search","ce8af497":"Let's calculate how much data we lost in this process.","979744a7":"We shall pass our dataframe through the function above which shall return us a dataframe with `Family Size` feature.","10b04eb5":"Based on this, we can say that the results are quite expected as deck **ABC** seems to have a high survival rate as does deck **DE** which may be because of the more number of first class passengers. Deck **FG** has the least survival rate among all.","d5d861a6":"Based on the **Random Search** operation, we shall fix the values of certain hyperparameter.","8aeb7171":"One thing to pay attention here is the high number of survivors at the left extreme of the graph.\n\nThis clearly shows that children were given priority while evacuating.\n\nThe low fare for these children can be explained by the fact that this may not have been a governing criteria while evacuating.","7029a8cd":"Let's begin with replacing left outliers, however from our analysis, it was revealed that according to **Z-Score** there aren't any outliers in the lower limit.\n\nTherefore we shall focus on replacing the higher values of `Age` outliers.","65adb19c":"We lost alomst **13%** of our data.","76df63d0":"One more thing to notice is that the survival rate at the other extreme also seems to be high.","b26f19af":"We shall also plot the survival chances of each passenger class.","ba62aea9":"Let's check the individual passenger distribution.","5d9d34d0":"##### Conclusion \n\nWe shall go ahead with `Pclass` for imputing missing values for `Age` as:-\n\n1. `Pclass` seems to be a lot more organized with a clear trend of reducing ages as classes increase.\n2. `SibSp` seems to have some missing values which may hinder imputation.","5037441c":"We don't have any outliers in the data for the lower limit.\n\nTherefore we shall focus on the larger values for the `Fare` feature.","fe771e38":"#### Exploring and Imputing Cabin","f89d0dd0":"We can look into our dataframe to check the new feature added."}}