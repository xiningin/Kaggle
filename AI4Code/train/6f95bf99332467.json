{"cell_type":{"18be7a88":"code","2e4a278f":"code","1644095a":"code","d3603c91":"code","6ddab8ae":"code","cf68a27a":"code","215cfa28":"code","51d7d55d":"code","c1084bf5":"code","1122b3d7":"code","4c812e65":"code","a05a9532":"code","39d0ab35":"code","4a3edf0a":"code","a17eec96":"code","0b824dd5":"code","a3541ccf":"code","edf28713":"code","e106e106":"code","aa09e6e8":"code","c197f91f":"code","6d7531e4":"code","427e5cef":"code","b1f13f85":"code","0f106706":"code","0b24d29b":"code","dfe24382":"code","4cc2574f":"code","e4327da8":"code","29aab738":"code","7d068e2c":"code","3ebf053e":"code","9f3515f0":"code","e67be145":"code","7a76bff6":"code","115cf7c7":"code","3621497a":"code","ce6a624d":"code","6b0c379d":"code","86da4d3f":"code","0939e77a":"code","93b3ca33":"code","07d11f22":"markdown","dd951b89":"markdown","7abd36ea":"markdown","c5d276ca":"markdown","dc8762b3":"markdown","1b8194bf":"markdown","6b82e507":"markdown","9cb0e06c":"markdown","48e2852c":"markdown","b47d651e":"markdown","07c1048c":"markdown"},"source":{"18be7a88":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2e4a278f":"from glob import glob\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport shutil\nfrom torchvision import transforms\nfrom torchvision import models\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torch import optim\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import Dataset,DataLoader\nimport time\n%matplotlib inline\n\nfrom torch.nn import Sequential","1644095a":"# Model to device\ndevice = torch.device('cuda:0' \n                      if torch.cuda.is_available() \n                      else 'cpu')\nis_cuda = False\nif torch.cuda.is_available():\n    is_cuda = True\n","d3603c91":"transform = transforms.Compose([transforms.Resize((224,224)),\n                                transforms.RandomHorizontalFlip(),\n                                transforms.RandomRotation(0.1),\n                                transforms.ToTensor(),\n                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n                                      ])\ndataset = ImageFolder('..\/input\/flowers-recognition\/flowers\/flowers', transform = transform)","6ddab8ae":"idx2class = {v: k for k, v in dataset.class_to_idx.items()}\ndef get_class_distribution(dataset_obj):\n    count_dict = {k:0 for k,v in dataset_obj.class_to_idx.items()}\n    \n    for element in dataset_obj:\n        y_lbl = element[1]\n        y_lbl = idx2class[y_lbl]\n        count_dict[y_lbl] += 1\n            \n    return count_dict\nprint(\"Distribution of classes: \\n\", get_class_distribution(dataset))","cf68a27a":"train, val = torch.utils.data.random_split(dataset, [3000, 1323])\n\ntrain_dataloader = torch.utils.data.DataLoader(dataset = train,\n                                           batch_size = 64, \n                                           shuffle = True,\n                                              num_workers = 4)\n\nval_dataloader = torch.utils.data.DataLoader(dataset = val,\n                                         batch_size = 64, \n                                          num_workers = 4)","215cfa28":"#how many classes are in my dataset\nprint(\"Number of classes : {}\\n\".format(len(dataset.classes)))\nprint(\"And the classes are : \\n{}\\n\".format(dataset.classes))\nprint('shape of a sample in format (C,H,W): {}'.format(dataset[0][0].shape))","51d7d55d":"def imshow(inp,cmap=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))#to display,we have to convert from tensor(C,H,W) to array(H,W,C)\n    mean = np.array([0.485, 0.456, 0.406])#we are aplying normalization whenever we read image into dataset\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    #inp = np.clip(inp, 0, 1)\n    plt.imshow(inp,cmap)\n","c1084bf5":"is_cuda = False\nif torch.cuda.is_available():\n    is_cuda = True","1122b3d7":"data_iter = iter(train_dataloader)\nimages, labels = next(data_iter)\n\nimport matplotlib.pyplot as plt\nimport helper\n\nfig = plt.figure(figsize = (12,8))\nk = 12\nfor i in range(2):\n    for j in range(5):\n        fig.add_subplot(1+i,5,j+1)\n        plt.tight_layout()\n        imshow(images[k])\n        k += 1","4c812e65":"class CNN_Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.layer1 = Sequential(nn.Conv2d(3,32,kernel_size = (5,5),padding = 2,stride =1),\n                                 nn.BatchNorm2d(32, affine = True),\n                                 nn.ReLU(),\n                                 nn.MaxPool2d(kernel_size = 2, stride = 2))\n                                 \n        self.layer2 = Sequential(nn.Conv2d(32,64, kernel_size =(5,5),padding =2, stride = 1),\n                                nn.BatchNorm2d(64, affine = True),\n                                nn.ReLU(),\n                                nn.MaxPool2d(kernel_size =2,stride = 2))\n                                 \n        self.layer3 = Sequential(nn.Conv2d(64,128, kernel_size = (3,3), padding = 1,stride =1),\n                                nn.BatchNorm2d(128, affine = True),\n                                nn.ReLU(),\n                                nn.MaxPool2d(kernel_size = 2, stride = 2))\n                                 \n        self.layer4 = Sequential(nn.Conv2d(128,256, kernel_size = (3,3), padding = 1,stride =1),\n                                nn.BatchNorm2d(256, affine = True),\n                                nn.ReLU(),\n                                nn.MaxPool2d(kernel_size = 2, stride = 2))\n                                 \n        self.layer5 = Sequential(nn.Conv2d(256,512, kernel_size = (3,3), padding = 1,stride =1),\n                                nn.BatchNorm2d(512, affine = True),\n                                nn.ReLU(),\n                                nn.MaxPool2d(kernel_size = 2, stride = 2))\n         \n        self.fc1 = nn.Linear(7*7*512 ,256 )\n        self.fc2 = nn.Linear(256,128)\n        self.fc3 = nn.Linear(128,5)   \n    \n    def forward(self, inp):\n        out = self.layer1(inp)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.layer5(out)\n        out = out.reshape(out.shape[0] , -1)\n        out = self.fc1(out)\n        out = F.dropout(out, training = self.training)\n        out = self.fc2(out)\n        out = F.dropout(out, training = self.training)\n        out = self.fc3(out)\n        \n        return F.log_softmax(out,dim=1)\n    \n            ","a05a9532":"model = CNN_Model()#object of network class\nif is_cuda:#send to apt. device\n    model.cuda()\n","39d0ab35":"optimizer = optim.Adam(model.parameters(), lr=0.001)","4a3edf0a":"def fit(epoch,model,data_loader,phase='training',volatile=False):\n    if phase == 'training':\n        model.train()\n    if phase == 'validation':\n        model.eval()\n        volatile=True\n    running_loss = 0.0\n    running_correct = 0\n    for batch_idx , (data,target) in enumerate(data_loader):\n        if is_cuda:\n            data,target = data.cuda(),target.cuda()\n        data , target = Variable(data,volatile),Variable(target)\n        if phase == 'training':\n            optimizer.zero_grad()#dont want to accumulate the gradients in every step, memory usage will be exploded\n        output = model(data)\n        loss = F.nll_loss(output,target)#negative log likelihood loss , target = true label\n        \n        running_loss += F.nll_loss(output,target,size_average=False).item()\n        preds = output.data.max(dim=1,keepdim=True)[1]\n        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n        if phase == 'training':\n            loss.backward() #calculates the gradient\n            optimizer.step() #updates the parameters\n    \n    loss = running_loss\/len(data_loader.dataset)\n    accuracy = 100. * running_correct\/len(data_loader.dataset)\n    \n    print(f'{phase} loss is ==> {loss:{5}.{2}} and {phase} accuracy is ==> {running_correct}\/{len(data_loader.dataset)}{accuracy:{10}.{4}}')\n    return loss,accuracy","a17eec96":"val_losses , val_accuracy,train_losses , train_accuracy = [],[] ,[],[]\nfor epoch in range(1,20):\n    epoch_loss, epoch_accuracy = fit(epoch,model,train_dataloader,phase='training')\n    val_epoch_loss , val_epoch_accuracy = fit(epoch,model,val_dataloader,phase='validation')\n    train_losses.append(epoch_loss)\n    train_accuracy.append(epoch_accuracy)\n    val_losses.append(val_epoch_loss)\n    val_accuracy.append(val_epoch_accuracy)","0b824dd5":"plt.figure(figsize = (12,6))\n\nplt.subplot(121)\nplt.plot(range(1,len(train_losses)+1),train_losses,'bo',label = 'training loss')\nplt.plot(range(1,len(val_losses)+1),val_losses,'r',label = 'validation loss')\nplt.legend()\nplt.title(\"Learning Curve for Loss\",fontsize = 14)\n\n\nplt.subplot(122)\nplt.plot(range(1,len(train_accuracy)+1),train_accuracy,'bo',label = 'train accuracy')\nplt.plot(range(1,len(val_accuracy)+1),val_accuracy,'r',label = 'val accuracy')\nplt.legend()\nplt.title(\"Learnuing curve for Accuracy\", fontsize = 14)\nplt.show()","a3541ccf":"transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n                                transforms.RandomRotation(0.2),\n                                transforms.ToTensor(),\n                                transforms.Resize((224,224))\n                               ])\n\n\ndataset = ImageFolder('..\/input\/flowers-recognition\/flowers\/flowers', transform = transform)\n\ntrain, val = torch.utils.data.random_split(dataset, [3000, 1323])\n\ntrain_dataloader = torch.utils.data.DataLoader(dataset = train,\n                                           batch_size = 64, \n                                           shuffle = True,\n                                              num_workers = 3)\n\nval_dataloader = torch.utils.data.DataLoader(dataset = val,\n                                         batch_size = 64, \n                                          num_workers = 3)","edf28713":"resnet18 = models.resnet18(pretrained = True)\n","e106e106":"resnet18","aa09e6e8":"num_ftrs = resnet18.fc.in_features\nresnet18.fc = nn.Linear(num_ftrs, 5)\nresnet18 = resnet18.to(device)","c197f91f":"optimizer = optim.Adam(resnet18.parameters(),lr=0.0001)","6d7531e4":"def fit(epoch,model,data_loader,phase='training',volatile=False):\n    if phase == 'training':\n        model.train()\n    if phase == 'validation':\n        model.eval()\n        volatile=True\n    running_loss = 0.0\n    running_correct = 0\n    \n    for batch_idx , (data,target) in enumerate(data_loader):\n        if is_cuda:\n            data,target = data.cuda(),target.cuda()\n        data , target = Variable(data,volatile),Variable(target)\n        if phase == 'training':\n            optimizer.zero_grad()\n        output = model(data)\n        loss = F.cross_entropy(output,target)#applies softmax function and compare it with the target\n        \n        \n        running_loss += F.cross_entropy(output,target,size_average=False).item()\n        preds = output.data.max(dim=1,keepdim=True)[1]\n        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n        if phase == 'training':\n            loss.backward()\n            optimizer.step()\n    \n    loss = running_loss\/len(data_loader.dataset)\n    accuracy = 100. * running_correct\/len(data_loader.dataset)\n    #print(\"EPOCH ===>> \", epoch)\n    print(f'{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}\/{len(data_loader.dataset)}{accuracy:{10}.{4}}')\n    return loss,accuracy","427e5cef":"train_losses , train_accuracy = [],[]\nval_losses , val_accuracy = [],[]\nfor epoch in range(1,10):\n    \n    epoch_loss, epoch_accuracy = fit(epoch,resnet18,train_dataloader,phase='training')\n    val_epoch_loss , val_epoch_accuracy = fit(epoch,resnet18,val_dataloader,phase='validation')\n    \n    train_losses.append(epoch_loss)\n    train_accuracy.append(epoch_accuracy)\n    val_losses.append(val_epoch_loss)\n    val_accuracy.append(val_epoch_accuracy)","b1f13f85":"plt.figure(figsize = (12,6))\n\nplt.subplot(121)\nplt.plot(range(1,len(train_losses)+1),train_losses,'bo',label = 'training loss')\nplt.plot(range(1,len(val_losses)+1),val_losses,'r',label = 'validation loss')\nplt.legend()\nplt.title(\"Learning Curve for Loss\",fontsize = 14)\n\n\nplt.subplot(122)\nplt.plot(range(1,len(train_accuracy)+1),train_accuracy,'bo',label = 'train accuracy')\nplt.plot(range(1,len(val_accuracy)+1),val_accuracy,'r',label = 'val accuracy')\nplt.legend()\nplt.title(\"Learnuing curve for Accuracy\", fontsize = 14)\nplt.show()","0f106706":"resnet34 = models.resnet34(pretrained = True)\nresnet34","0b24d29b":"num_ftrs = resnet34.fc.in_features\nresnet34.fc = nn.Linear(num_ftrs, 5)\nresnet34 = resnet34.to(device)\nresnet34","dfe24382":"optimizer = optim.Adam(resnet34.parameters(),lr=0.0001)","4cc2574f":"def fit(epoch,model,data_loader,phase='training',volatile=False):\n    if phase == 'training':\n        model.train()\n    if phase == 'validation':\n        model.eval()\n        volatile=True\n    running_loss = 0.0\n    running_correct = 0\n    \n    for batch_idx , (data,target) in enumerate(data_loader):\n        if is_cuda:\n            data,target = data.cuda(),target.cuda()\n        data , target = Variable(data,volatile),Variable(target)\n        if phase == 'training':\n            optimizer.zero_grad()\n        output = model(data)\n        loss = F.cross_entropy(output,target)#applies softmax function and compare it with the target\n        \n        \n        running_loss += F.cross_entropy(output,target,size_average=False).item()\n        preds = output.data.max(dim=1,keepdim=True)[1]\n        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n        if phase == 'training':\n            loss.backward()\n            optimizer.step()\n    \n    loss = running_loss\/len(data_loader.dataset)\n    accuracy = 100. * running_correct\/len(data_loader.dataset)\n    #print(\"EPOCH ===>> \", epoch)\n    print(f'{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}\/{len(data_loader.dataset)}{accuracy:{10}.{4}}')\n    return loss,accuracy","e4327da8":"train_losses , train_accuracy = [],[]\nval_losses , val_accuracy = [],[]\nfor epoch in range(1,10):\n    \n    epoch_loss, epoch_accuracy = fit(epoch,resnet34,train_dataloader,phase='training')\n    val_epoch_loss , val_epoch_accuracy = fit(epoch,resnet34,val_dataloader,phase='validation')\n    \n    train_losses.append(epoch_loss)\n    train_accuracy.append(epoch_accuracy)\n    val_losses.append(val_epoch_loss)\n    val_accuracy.append(val_epoch_accuracy)","29aab738":"plt.figure(figsize = (12,6))\n\nplt.subplot(121)\nplt.plot(range(1,len(train_losses)+1),train_losses,'bo',label = 'training loss')\nplt.plot(range(1,len(val_losses)+1),val_losses,'r',label = 'validation loss')\nplt.legend()\nplt.title(\"Learning Curve for Loss\",fontsize = 14)\n\n\nplt.subplot(122)\nplt.plot(range(1,len(train_accuracy)+1),train_accuracy,'bo',label = 'train accuracy')\nplt.plot(range(1,len(val_accuracy)+1),val_accuracy,'r',label = 'val accuracy')\nplt.legend()\nplt.title(\"Learnuing curve for Accuracy\", fontsize = 14)\nplt.show()","7d068e2c":"alexnet = models.alexnet(pretrained = True)\nalexnet","3ebf053e":"alexnet.classifier[6].out_features = 5\nalexnet = alexnet.to(device)\nalexnet","9f3515f0":"optimizer = optim.Adam(alexnet.parameters(),lr=0.0001)","e67be145":"def fit(epoch,model,data_loader,phase='training',volatile=False):\n    if phase == 'training':\n        model.train()\n    if phase == 'validation':\n        model.eval()\n        volatile=True\n    running_loss = 0.0\n    running_correct = 0\n    \n    for batch_idx , (data,target) in enumerate(data_loader):\n        if is_cuda:\n            data,target = data.cuda(),target.cuda()\n        data , target = Variable(data,volatile),Variable(target)\n        if phase == 'training':\n            optimizer.zero_grad()\n        output = model(data)\n        loss = F.cross_entropy(output,target)#applies softmax function and compare it with the target\n        \n        \n        running_loss += F.cross_entropy(output,target,size_average=False).item()\n        preds = output.data.max(dim=1,keepdim=True)[1]\n        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n        if phase == 'training':\n            loss.backward()\n            optimizer.step()\n    \n    loss = running_loss\/len(data_loader.dataset)\n    accuracy = 100. * running_correct\/len(data_loader.dataset)\n    #print(\"EPOCH ===>> \", epoch)\n    print(f'{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}\/{len(data_loader.dataset)}{accuracy:{10}.{4}}')\n    return loss,accuracy","7a76bff6":"train_losses , train_accuracy = [],[]\nval_losses , val_accuracy = [],[]\nfor epoch in range(1,10):\n    \n    epoch_loss, epoch_accuracy = fit(epoch,alexnet,train_dataloader,phase='training')\n    val_epoch_loss , val_epoch_accuracy = fit(epoch,alexnet,val_dataloader,phase='validation')\n    \n    train_losses.append(epoch_loss)\n    train_accuracy.append(epoch_accuracy)\n    val_losses.append(val_epoch_loss)\n    val_accuracy.append(val_epoch_accuracy)","115cf7c7":"plt.figure(figsize = (12,6))\n\nplt.subplot(121)\nplt.plot(range(1,len(train_losses)+1),train_losses,'bo',label = 'training loss')\nplt.plot(range(1,len(val_losses)+1),val_losses,'r',label = 'validation loss')\nplt.legend()\nplt.title(\"Learning Curve for Loss\",fontsize = 14)\n\n\nplt.subplot(122)\nplt.plot(range(1,len(train_accuracy)+1),train_accuracy,'bo',label = 'train accuracy')\nplt.plot(range(1,len(val_accuracy)+1),val_accuracy,'r',label = 'val accuracy')\nplt.legend()\nplt.title(\"Learnuing curve for Accuracy\", fontsize = 14)\nplt.show()","3621497a":"vgg = models.vgg16() #goint to use the weights of teh pretrained and send to the device\nvgg","ce6a624d":"vgg.classifier[6].out_features = 5\nvgg = vgg.to(device)\nvgg","6b0c379d":"optimizer = optim.Adam(vgg.parameters(),lr=0.0001)","86da4d3f":"def fit(epoch,model,data_loader,phase='training',volatile=False):\n    if phase == 'training':\n        model.train()\n    if phase == 'validation':\n        model.eval()\n        volatile=True\n    running_loss = 0.0\n    running_correct = 0\n    \n    for batch_idx , (data,target) in enumerate(data_loader):\n        if is_cuda:\n            data,target = data.cuda(),target.cuda()\n        data , target = Variable(data,volatile),Variable(target)\n        if phase == 'training':\n            optimizer.zero_grad()\n        output = model(data)\n        loss = F.cross_entropy(output,target)#applies softmax function and compare it with the target\n        \n        \n        running_loss += F.cross_entropy(output,target,size_average=False).item()\n        preds = output.data.max(dim=1,keepdim=True)[1]\n        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n        if phase == 'training':\n            loss.backward()\n            optimizer.step()\n    \n    loss = running_loss\/len(data_loader.dataset)\n    accuracy = 100. * running_correct\/len(data_loader.dataset)\n    #print(\"EPOCH ===>> \", epoch)\n    print(f'{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}\/{len(data_loader.dataset)}{accuracy:{10}.{4}}')\n    return loss,accuracy","0939e77a":"val_losses , val_accuracy,train_losses , train_accuracy = [],[] ,[],[]\nfor epoch in range(1,12):\n    epoch_loss, epoch_accuracy = fit(epoch,vgg,train_dataloader,phase='training')\n    val_epoch_loss , val_epoch_accuracy = fit(epoch,vgg,val_dataloader,phase='validation')\n    train_losses.append(epoch_loss)\n    train_accuracy.append(epoch_accuracy)\n    val_losses.append(val_epoch_loss)\n    val_accuracy.append(val_epoch_accuracy)","93b3ca33":"plt.figure(figsize = (12,6))\n\nplt.subplot(121)\nplt.plot(range(1,len(train_losses)+1),train_losses,'bo',label = 'training loss')\nplt.plot(range(1,len(val_losses)+1),val_losses,'r',label = 'validation loss')\nplt.legend()\nplt.title(\"Learning Curve for Loss\",fontsize = 14)\n\n\nplt.subplot(122)\nplt.plot(range(1,len(train_accuracy)+1),train_accuracy,'bo',label = 'train accuracy')\nplt.plot(range(1,len(val_accuracy)+1),val_accuracy,'r',label = 'val accuracy')\nplt.legend()\nplt.title(\"Learnuing curve for Accuracy\", fontsize = 14)\nplt.show()","07d11f22":"## Resnet34","dd951b89":"# Transfer Learning","7abd36ea":"# Loading data","c5d276ca":"# importing Library ","dc8762b3":"## Loading data","1b8194bf":"# Visualisation","6b82e507":"# Custom Model","9cb0e06c":"## VGG16","48e2852c":"## Resnet18","b47d651e":"## Alexnet","07c1048c":"# splitting data"}}