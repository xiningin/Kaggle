{"cell_type":{"1ecf353a":"code","f374bffb":"code","32a49aa0":"code","af3e8d9f":"code","dfbf9290":"code","812373f0":"code","8c4cf3c1":"code","1de66085":"code","8f2085d5":"code","03c4880d":"code","c10212ce":"code","123af689":"code","b6784b89":"code","13f4cf1e":"code","4d877f41":"code","8e94d0a2":"code","e0a0500b":"code","bc35f599":"code","52bbbeca":"code","f7b345ca":"code","c0087fe6":"code","e62e0a20":"code","d3688309":"code","bb7e4647":"code","18f42595":"code","cde38da2":"code","e18cf04c":"code","f4749e8c":"code","49da71dc":"markdown","0038c0f0":"markdown","49970ec9":"markdown","7e322bf8":"markdown","db11e6b6":"markdown","0384a7d5":"markdown","a7ed98a2":"markdown","47725df4":"markdown","72e9530e":"markdown","20898621":"markdown","55d60840":"markdown","1b58eae4":"markdown"},"source":{"1ecf353a":"#We used h5py library to read our data because it in H5 file \"data file saved in the Hierarchical Data Format (HDF)\" \nimport numpy as np\nimport h5py\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\n\n%matplotlib inline","f374bffb":"path_to_train = \"..\/input\/happy-house-dataset\/train_happy.h5\"\npath_to_test = \"..\/input\/happy-house-dataset\/test_happy.h5\"\n\ntrain_dataset = h5py.File(path_to_train)\nx_train = np.array(train_dataset['train_set_x'])\ny_train = np.array(train_dataset['train_set_y'], ndmin=2)\n\ntest_dataset = h5py.File(path_to_test)\nx_test = np.array(test_dataset['test_set_x'])\ny_test = np.array(test_dataset['test_set_y'], ndmin=2)\n\nprint (\"number of training examples = \" + str(x_train.shape[0]))\nprint (\"number of test examples = \" + str(x_test.shape[0]))\nprint (\"X_train shape: \" + str(x_train.shape))\nprint (\"Y_train shape: \" + str(y_train.shape))\nprint (\"X_test shape: \" + str(x_test.shape))\nprint (\"Y_test shape: \" + str(y_test.shape))","32a49aa0":"# Normalizing the pixel values between 0 and 1 \n# where pixel values original range were between 0 and 255 so we divided by 255\nx_train = x_train\/255\nx_test = x_test\/255","af3e8d9f":"# Reshape\ny_train = y_train.T #(600,1)\ny_test = y_test.T #(150,1)","dfbf9290":"# Sample from dataset\nprint(\"Image shape :\",x_train[5].shape)\nprint(\"The person is happy '1' or not happy '0' :\", y_train[5])\nimshow(x_train[5])","812373f0":"from tensorflow import keras\nfrom keras.models import Sequential\nfrom keras import regularizers,optimizers\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense,Dropout","8c4cf3c1":"model = keras.Sequential()\n\nmodel.add(Conv2D(32, kernel_size=(7,7), input_shape=(64,64,3),padding ='same',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(32,(3,3), padding ='same',activation='relu'))\nmodel.add(MaxPooling2D(pool_size = (2, 2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(120,kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),activation = 'relu'))\nmodel.add(Dense(1,kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),activation='sigmoid'))","1de66085":"model.summary()","8f2085d5":"initial_model = model\ninitial_model.compile(loss ='binary_crossentropy',metrics =['accuracy'])","03c4880d":"model_history= initial_model.fit(x_train, y_train, epochs=8, batch_size=50, validation_split=0.1)","c10212ce":"fig = plt.figure(figsize=(20, 4))\nfig.subplots_adjust(hspace=0.4, wspace=0.2)\n\nplt.subplot(1, 2, 1)\nplt.plot(model_history.history['accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend([\"Train\"], loc='upper left')\n\nplt.subplot(1, 2, 2)\nplt.plot(model_history.history['loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend([\"Train\"], loc='upper left')\nplt.show()","123af689":"evaluation = initial_model.evaluate(x_test, y_test)\nprint (\"\\nTest Accuracy = \" + str(evaluation[1]))\nprint (\"Loss = \" + str(evaluation[0]))","b6784b89":"SGD_model = model\nSGD_model.compile(loss ='binary_crossentropy',metrics =['accuracy'],optimizer='SGD')","13f4cf1e":"model_history = SGD_model.fit(x_train, y_train, epochs=8, batch_size=50, validation_split=0.1)","4d877f41":"fig = plt.figure(figsize=(20, 4))\nfig.subplots_adjust(hspace=0.4, wspace=0.2)\n\nplt.subplot(1, 2, 1)\nplt.plot(model_history.history['accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend([\"Train\"], loc='upper left')\n\nplt.subplot(1, 2, 2)\nplt.plot(model_history.history['loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend([\"Train\"], loc='upper left')\nplt.show()","8e94d0a2":"evaluation = SGD_model.evaluate(x_test, y_test)\nprint (\"\\nTest Accuracy = \" + str(evaluation[1]))\nprint (\"Loss = \" + str(evaluation[0]))","e0a0500b":"RMSProp_model = model\nRMSProp_model.compile(loss ='binary_crossentropy',metrics =['accuracy'],optimizer='RMSProp')","bc35f599":"model_history = RMSProp_model.fit(x_train, y_train, epochs=8, batch_size=50, validation_split=0.1)","52bbbeca":"fig = plt.figure(figsize=(20, 4))\nfig.subplots_adjust(hspace=0.4, wspace=0.2)\n\nplt.subplot(1, 2, 1)\nplt.plot(model_history.history['accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend([\"Train\"], loc='upper left')\n\nplt.subplot(1, 2, 2)\nplt.plot(model_history.history['loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend([\"Train\"], loc='upper left')\nplt.show()","f7b345ca":"evaluation = RMSProp_model.evaluate(x_test, y_test)\nprint (\"\\nTest Accuracy = \" + str(evaluation[1]))\nprint (\"Loss = \" + str(evaluation[0]))","c0087fe6":"Adam_model = model\nAdam_model.compile(loss ='binary_crossentropy',metrics =['accuracy'],optimizer='Adam')","e62e0a20":"model_history = Adam_model.fit(x_train, y_train, epochs=8, batch_size=50, validation_split=0.1)","d3688309":"fig = plt.figure(figsize=(20, 4))\nfig.subplots_adjust(hspace=0.4, wspace=0.2)\n\nplt.subplot(1, 2, 1)\nplt.plot(model_history.history['accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend([\"Train\"], loc='upper left')\n\nplt.subplot(1, 2, 2)\nplt.plot(model_history.history['loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend([\"Train\"], loc='upper left')\nplt.show()","bb7e4647":"evaluation = Adam_model.evaluate(x_test, y_test)\nprint (\"\\nTest Accuracy = \" + str(evaluation[1]))\nprint (\"Loss = \" + str(evaluation[0]))","18f42595":"Final = model\nFinal.compile(loss ='binary_crossentropy',metrics =['accuracy'],optimizer='Adam')","cde38da2":"model_history = Final.fit(x_train, y_train, epochs=60, batch_size=150, validation_split=0.1)","e18cf04c":"fig = plt.figure(figsize=(20, 4))\nfig.subplots_adjust(hspace=0.4, wspace=0.2)\n\nplt.subplot(1, 2, 1)\nplt.plot(model_history.history['accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend([\"Train\"], loc='upper left')\n\nplt.subplot(1, 2, 2)\nplt.plot(model_history.history['loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend([\"Train\"], loc='upper left')\nplt.show()","f4749e8c":"evaluation = Final.evaluate(x_test, y_test)\nprint (\"\\nTest Accuracy = \" + str(evaluation[1]))\nprint (\"Loss = \" + str(evaluation[0]))","49da71dc":"* In the comparison of SGD, RMSprop and Adam Optimizers, Adam got highest accuracy.\n* We got the best accuracy with Epoch 60 and Batch size 150. \n\n\n\n","0038c0f0":"> In Conclusion : \nAdam got the highest accuracy. So, it is the best optimizer in our case.","49970ec9":"<a id='uploading'><\/a>\n### 1. Uploading the data\n\n","7e322bf8":"# **Happy House Problem**\n\nSimply the problem is about detection, which we should detect if the person is smiling or not. As we only allow a person to enter the house if he is smiling!\n\n#### **Table of Contents :**\n- [Uploading data](#uploading)\n- [Cleaning data](#clean)\n- [Building a model](#model)\n- [Optimizers](#optimizer)\n- [Conclusion](#conc)\n","db11e6b6":"### 5. Conclusion\n <a id='conc'><\/a>","0384a7d5":"Details of the dataset :\n*   Images are of shape (64,64,3)\n*   Training: 600 pictures\n*   Test: 150 pictures!\n\n\n\\begin{equation}\n  y =\\begin{cases}\n    0, & \\text{In case if (not happy)}.\\\\\n    1, & \\text{if (happy)}.\n  \\end{cases}\n\\end{equation}\n\n","a7ed98a2":"### 4. Optimizers\n <a id='optimizer'><\/a>","47725df4":"### 2. Cleaning\n<a id='clean'><\/a>\n","72e9530e":"#### 4.2. Root Mean Square Propagation (RMSProp)","20898621":" ### 3. Building a model\n <a id='model'><\/a>\n","55d60840":"#### 4.3. Adaptive Moment Estimation (ADAM) ","1b58eae4":"#### 4.1.  Gradient Descent with Momentum (SGD)"}}