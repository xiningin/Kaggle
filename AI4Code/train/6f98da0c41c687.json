{"cell_type":{"5e6d3708":"code","466c38e5":"code","87917e7e":"code","d0caa317":"code","93e84c23":"code","e0803064":"code","6189b940":"code","d8c46837":"code","f1f74814":"code","5b989881":"code","03f317c8":"code","57980821":"code","6625e0ce":"code","66f58ab5":"code","c07f896c":"code","866a5f7e":"code","0670b37f":"code","425c8e54":"code","4a8a2c4e":"code","b5c63850":"code","ea398cc2":"code","69fd9f56":"code","d7c31300":"code","e720b7f7":"code","c4034ef8":"code","0bd806f1":"code","b53fb541":"code","a0b5db01":"code","922b59f4":"code","3d1d23a6":"code","845b631a":"markdown","3ac7838a":"markdown","43f37a0f":"markdown","3816dca1":"markdown","71d9a482":"markdown","e9233c23":"markdown","cfe7e64e":"markdown","ff3dddc9":"markdown","be4354b7":"markdown","3864677b":"markdown","8852f1b0":"markdown","b1823784":"markdown","5b16f111":"markdown"},"source":{"5e6d3708":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","466c38e5":"import matplotlib.pyplot as plt\nimport catboost as cb\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\nimport csv\nfrom sklearn.impute import SimpleImputer ","87917e7e":"item_categories = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nitems = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\nsales_train = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv',parse_dates = ['date'])\nsample_submission = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\nshops = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\ndata_test = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')\n\nprint(f'item_categories.csv : {item_categories.shape}')\nitem_categories.isnull().sum()","d0caa317":"item_categories.head(3)","93e84c23":"print(f'items.csv : {items.shape}')\nitems.isnull().sum()","e0803064":"items.head(3)","6189b940":"print(f'sales_train.csv : {sales_train.shape}')\nprint(sales_train.dtypes)\nsales_train.isnull().sum()","d8c46837":"sales_train.head(3)","f1f74814":"print(f'sample_submission.csv : {sample_submission.shape}')\nsample_submission.isnull().sum()","5b989881":"sample_submission.head(3)","03f317c8":"print(f'shops.csv : {shops.shape}')\nshops.isnull().sum()","57980821":"shops.head(3)","6625e0ce":"print(f'test.csv : {data_test.shape}')\ndata_test.isnull().sum()","66f58ab5":"data_test.head(3)","c07f896c":"plt.figure(figsize=(8,8))\nplt.scatter(sales_train.item_cnt_day,sales_train.item_price)\nplt.show()","866a5f7e":"sales_train = sales_train[sales_train.item_price<45000]\nsales_train = sales_train[sales_train.item_cnt_day<600]\ncolumns = ['date', 'date_block_num', 'shop_id', 'item_id','item_price','item_cnt_day']\nsales_train.drop_duplicates(columns,keep='first', inplace=True) ","0670b37f":"plt.figure(figsize=(8,8))\nplt.scatter(sales_train.item_cnt_day,sales_train.item_price)\nplt.show()","425c8e54":"monthly = sales_train.groupby([\"date_block_num\",\"shop_id\",\"item_id\"])[\"item_cnt_day\"].agg('sum').reset_index()\nmonthly.columns = ['date_block_num','shop_id','item_id','item_cnt_month']\n\nprice_monthly = sales_train.groupby([\"date_block_num\",\"shop_id\",\"item_id\"])[\"item_price\"].agg('mean').reset_index()\n\ncombine = pd.merge(monthly, price_monthly)\ncombine.head(5)","4a8a2c4e":"sales_train['year'] = sales_train['date'].dt.year\nsales_train['day_of_year'] = sales_train['date'].dt.dayofyear\nsales_train['weekday'] = sales_train['date'].dt.weekday\nsales_train['week_of_year'] = sales_train['date'].dt.week\nsales_train['day_of_month'] = sales_train['date'].dt.day\nsales_train['quarter'] = sales_train['date'].dt.quarter\nsales_train['month'] = sales_train['date'].dt.month\nsales_train.drop('date', axis=1, inplace=True)\nsales_train.head(5)","b5c63850":"datatest_monthly = price_monthly.groupby([\"shop_id\",\"item_id\"])[\"item_price\"].agg('mean').reset_index()\ndata_test['date_block_num'] = 34\ndata_test_end = pd.merge(data_test, datatest_monthly,how = 'left')\ndata_test_end = data_test_end.drop(['ID'], axis = 1)\n\nnumeric = SimpleImputer(missing_values=np.nan, strategy='mean')\nnumeric = numeric.fit(data_test_end)\ndata_test_end = numeric.transform(data_test_end)\ndata_test_end = pd.DataFrame(data_test_end,columns=[\"shop_id\",\"item_id\",\"date_block_num\",\"item_price\"])\ndata_test_end.head(5)","ea398cc2":"X = combine.drop('item_cnt_month', axis=1)\nY = combine.item_cnt_month\ntrain, test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=10)","69fd9f56":"model = xgb.XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n\nmodel.fit(\n    train, \n    y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(train, y_train), (test, y_test)], \n    verbose=True, \n    early_stopping_rounds = 10)\n\ny_pred = model.predict(test)\ny_pred = y_pred.tolist()\n\nprint('R2 XGBoost: ',r2_score(y_test,y_pred))","d7c31300":"fig, ax = plt.subplots(figsize=(10,10))\nxgb.plot_importance(model, importance_type='gain',ax=ax)\nplt.show()","e720b7f7":"model =  cb.CatBoostRegressor(iterations=1000,\n                             learning_rate=0.01,\n                             depth=16,\n                             eval_metric='RMSE',\n                             random_seed = 42,\n                             bagging_temperature = 0.2,\n                             od_type='Iter',\n                             metric_period = 75,\n                             od_wait=100)\n\nmodel.fit(train, y_train,eval_set=(test, y_test),plot=True)\n\ny_pred = model.predict(test)\ny_pred = y_pred.tolist()\n\nprint('R2 CatBoost : ',r2_score(y_test,y_pred))","c4034ef8":"fea_imp = pd.DataFrame({'imp': model.feature_importances_, 'col': combine.drop('item_cnt_month', axis=1).columns})\nfea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc[-30:]\nfea_imp.plot(kind='barh', x='col', y='imp', figsize=(10, 7), legend=None)\nplt.title('CatBoost - Feature Importance')\nplt.ylabel('Features')\nplt.xlabel('Importance');","0bd806f1":"hyper_params = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n#    'metric': ['l2', 'auc'],\n    'learning_rate': 0.005,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.7,\n    'bagging_freq': 10,\n    'verbose': 0,\n    \"max_depth\": 8,\n    \"num_leaves\": 128,  \n    \"max_bin\": 512,\n    \"num_iterations\": 1000,\n    \"n_estimators\": 1000\n}\n\nmodel = lgb.LGBMRegressor(**hyper_params)\n\nmodel.fit(train, y_train,\n        eval_set=[(test, y_test)],\n        eval_metric='l1',\n        early_stopping_rounds=1000)\n\ny_pred = model.predict(test)\n\nprint('R2 LightGBM: ',r2_score(y_test,y_pred))\n#print('RMSE : 'metrics.mean_squared_error(y_test, y_pred, squared=False))","b53fb541":"import matplotlib.pyplot as plt\nlgb.plot_importance(model, importance_type='gain', max_num_features=20,figsize=(10,10))\nplt.show()","a0b5db01":"from sklearn.ensemble import RandomForestRegressor\nrf_reg = RandomForestRegressor(n_estimators=100,random_state=0)\nrf_reg.fit(train,y_train)\n\ny_pred = rf_reg.predict(test)\nprint('R2 RandomForest: ',r2_score(y_test,y_pred))\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, y_pred)))","922b59f4":"y_pred = rf_reg.predict(data_test_end)\ny_pred = pd.DataFrame(y_pred,columns=[\"item_cnt_month\"])\ny_pred = y_pred.clip(0,20)\n# print('R2 : ',r2_score(y_test,y_pred))\n# print('Validation rmse:', np.sqrt(mean_squared_error(y_test, y_pred)))\n# print(accuracy_score(y_pred, y_test)*100)\n\nsample_submission = sample_submission.drop(['item_cnt_month'], axis = 1)\nsample_submission=pd.concat([sample_submission,y_pred],axis=1)\nsample_submission.to_csv('submission.csv', index=False)","3d1d23a6":"sample_submission.head(5)","845b631a":"K\u00fct\u00fcphanelerin Eklenmesi","3ac7838a":"LightGBM Uygulanmas\u0131","43f37a0f":"Outlier verilerin silinmesi","3816dca1":"Test verisinin haz\u0131rlanmas\u0131 date_block_num ve ayl\u0131k price verisinin eklenmesi","71d9a482":"# \u00c7\u0131kt\u0131\nAlgoritmalardan al\u0131nan \u00e7\u0131kt\u0131lar \u00fczerine test verisinin tahmini i\u00e7in RandomForestRegressor algoritmas\u0131 se\u00e7ilmi\u015ftir. \nSe\u00e7ilen algoritma ile as\u0131l test verisi \u00fczerinde tahmin uygulan\u0131p sonu\u00e7lar\u0131 submission.csv'ye kaydedilmi\u015ftir. ","e9233c23":"Verilerin okunup incelenmesi","cfe7e64e":"# Model Se\u00e7imi\n\nXGBoost Uygulanmas\u0131","ff3dddc9":"RandomForestRegressor Uygulanmas\u0131","be4354b7":"Train verisinin haz\u0131rlanmas\u0131 ayl\u0131k olarak gruplama","3864677b":"CatBoost Uygulanmas\u0131","8852f1b0":"Outlier veri kontrol\u00fc","b1823784":"# B\u00fcy\u00fck Veri Final\nOnur Kaplan - 160202061","5b16f111":"Verilerin train test olarak b\u00f6l\u00fcnmesi"}}