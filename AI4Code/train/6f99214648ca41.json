{"cell_type":{"ab475b11":"code","c6ea0eaf":"code","94e87dd9":"code","e70f729f":"code","e107faa7":"code","e4b2e810":"code","6d566ac1":"code","acc79cf7":"code","807d9347":"code","e8517e1a":"code","041b4c17":"code","e1393353":"code","60e82439":"code","3ee33492":"code","e71bffc6":"code","b5a081f0":"code","60cad4fc":"code","d2441800":"code","10b5cfc9":"code","940daea1":"code","8dc4ddde":"code","46ee4aeb":"code","322674f9":"code","381314b4":"code","a1adbc34":"code","febd4e82":"code","5f9c59ef":"code","b333275b":"code","952dc1e6":"code","6cea2356":"code","9c4bfd73":"code","d07814e3":"code","17bb87d4":"code","3bb6c60a":"code","412a1d13":"code","467ee754":"code","aa7c3dcf":"code","79a3ab63":"code","3e99822a":"code","c9e69db8":"code","5ecfe21a":"code","152a8b9e":"code","4a635e9f":"code","68e90c03":"code","94b0ea73":"code","53531910":"code","4a2e43e8":"code","7a5a3abc":"code","eded1c45":"code","aed506d7":"code","5d9e20f9":"code","8a8e8f19":"code","54dd93d1":"code","436ee12f":"code","acfb5d65":"markdown","0985eadd":"markdown","983e8895":"markdown","9537d867":"markdown","1cd6eee9":"markdown","5b1b9402":"markdown","c60a950e":"markdown","9d4e8b98":"markdown","2bb40bf7":"markdown","f55d9f24":"markdown","00cadb9c":"markdown","b718839d":"markdown","d65a1ec2":"markdown","74ffe57c":"markdown","d63badae":"markdown","435ede0a":"markdown","f94e701e":"markdown","c4b1a6c8":"markdown","2ae942c9":"markdown","4ba0f645":"markdown","e77d1d02":"markdown","4dba169b":"markdown","2c2c2a4f":"markdown","e6355528":"markdown","dd24ef0c":"markdown","430ff050":"markdown","b31d6cbc":"markdown","d7f536ba":"markdown","19de44c6":"markdown","49809f01":"markdown","656b4af8":"markdown","0b9fda68":"markdown","a1582107":"markdown","1ce52797":"markdown","e737f8c2":"markdown","741e6198":"markdown","ade866b0":"markdown","65766d2a":"markdown","09aaa26f":"markdown","6f1ca8f7":"markdown","810a4886":"markdown","ed23a031":"markdown"},"source":{"ab475b11":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%pylab inline\nimport seaborn as sns\nimport xgboost\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c6ea0eaf":"heart_dataset = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")","94e87dd9":"heart_dataset.head()","e70f729f":"sns.countplot(x = \"target\", data = heart_dataset)","e107faa7":"splitBySex = pd.crosstab(heart_dataset[\"target\"], heart_dataset[\"sex\"])\nsplitBySex[1] \/= sum(splitBySex[1])\nsplitBySex[0] \/= sum(splitBySex[0])\nsplitBySex.plot(kind = \"bar\")","e4b2e810":"sexSplitByDisease = pd.crosstab(heart_dataset[\"sex\"], heart_dataset[\"target\"])\nsexSplitByDisease[0] \/= sum(sexSplitByDisease[0])\nsexSplitByDisease[1] \/= sum(sexSplitByDisease[1])","6d566ac1":"sexSplitByDisease.plot(kind = \"bar\")","acc79cf7":"positives = heart_dataset.loc[heart_dataset[\"target\"] == 1]\nnegatives = heart_dataset.loc[heart_dataset[\"target\"] == 0]","807d9347":"sns.distplot(positives[\"age\"],kde = False)","e8517e1a":"sns.distplot(negatives[\"age\"],kde = False)","041b4c17":"pd.crosstab(heart_dataset[\"target\"], heart_dataset[\"cp\"]).plot(kind = \"bar\")","e1393353":"sns.distplot(positives[\"trestbps\"])","60e82439":"sns.distplot(negatives[\"trestbps\"])","3ee33492":"sns.jointplot(x = positives[\"trestbps\"], y = positives[\"age\"],kind = \"hex\")","e71bffc6":"sns.jointplot(x = negatives[\"trestbps\"], y = negatives[\"age\"], kind = \"hex\")","b5a081f0":"(positives[\"ca\"].value_counts()).plot(kind = \"bar\")","60cad4fc":"negatives[\"ca\"].value_counts().plot(kind = \"bar\")","d2441800":"pd.crosstab(heart_dataset[\"exang\"],heart_dataset[\"target\"]).plot(kind = \"bar\")","10b5cfc9":"sns.swarmplot(y = heart_dataset[\"oldpeak\"], x = heart_dataset[\"slope\"], hue = heart_dataset[\"target\"])","940daea1":"sns.distplot(positives[\"chol\"], kde = False)","8dc4ddde":"sns.distplot(negatives[\"chol\"], kde = False)","46ee4aeb":"from  sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder","322674f9":"plt.plot(heart_dataset[\"target\"])","381314b4":"X = heart_dataset.sample(frac=1) #shuffle shuffle shuffle\nX = X.drop([\"thal\"], axis = 1)","a1adbc34":"#Insert the ca boolean variable in place of the ca variable\nX[\"cabool\"] = X[\"ca\"].apply(lambda x : 1 if x > 0 else 0)\nX = X.drop([\"ca\"], axis = 1)\nX[\"ca\"] = X[\"cabool\"]\nX = X.drop([\"cabool\"], axis = 1)","febd4e82":"#copy the targets to y\ny = X[\"target\"].copy()\nX = X.drop([\"target\"], axis = 1)","5f9c59ef":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split","b333275b":"#Using pipelines to do everything together - makes life easy\nonehot_encoder = OneHotEncoder(handle_unknown = \"ignore\", sparse = False)\nnumerical_scaler = StandardScaler()\npreprocessor = ColumnTransformer(transformers = [(\"numerical_scaler\",numerical_scaler,[\"trestbps\",\"chol\",\"thalach\",\"oldpeak\"]),(\"onehot_encoder\",onehot_encoder,[\"cp\",\"restecg\",\"slope\"])], remainder = \"passthrough\")\n","952dc1e6":"X_train, X_valid, y_train, y_valid = train_test_split(X,y,train_size = 0.8)","6cea2356":"from sklearn.linear_model import LogisticRegression\nfit_model = LogisticRegression(max_iter = 1000, C = 0.5, class_weight = \"balanced\")\nlr_pipeline = Pipeline(steps = [(\"preprocessor\",preprocessor),(\"model\",fit_model)])","9c4bfd73":"from sklearn.model_selection import GridSearchCV\nparam_grid = {\"model__C\" : np.logspace(-5,0,10)}\nsearch = GridSearchCV(lr_pipeline, param_grid, n_jobs = -1)\nsearch.fit(X_train,y_train)\n","d07814e3":"search.best_score_","17bb87d4":"search.best_params_","3bb6c60a":"#Redefine the pipeline with the best C\nfit_model = LogisticRegression(max_iter = 1000, C = search.best_params_[\"model__C\"], class_weight = \"balanced\")\nlr_pipeline = Pipeline(steps = [(\"preprocessor\",preprocessor),(\"model\",fit_model)])","412a1d13":"lr_pipeline.fit(X_train,y_train)\nlr_preds = lr_pipeline.predict(X_valid)\nprint(\"Accuracy = \",sum(lr_preds == y_valid)\/len(y_valid))","467ee754":"from sklearn.metrics import confusion_matrix,f1_score\nlr_CM = confusion_matrix(y_valid, lr_preds)\nlr_TP = lr_CM[1,1]\nlr_TN = lr_CM[0,0]\nlr_FP = lr_CM[0,1]\nlr_FN = lr_CM[1,0]\nprint(lr_CM)","aa7c3dcf":"lr_recall = lr_TP\/(lr_TP + lr_FN)\nlr_specificity = lr_TN\/(lr_TN + lr_FP)\nlr_precision = lr_TP\/(lr_TP + lr_FP)\n\nprint(lr_recall,lr_specificity)","79a3ab63":"lr_f1_score = f1_score(y_valid, lr_preds)\nlr_f1_score","3e99822a":"#Prep for ROC score curves\nfrom sklearn.metrics import roc_curve, roc_auc_score\nlr_y_proba = lr_pipeline.predict_proba(X_valid)[:,1]\nlr_fpr,lr_tpr,lr_threshold = roc_curve(y_valid,lr_y_proba)","c9e69db8":"fit_model.coef_, fit_model.intercept_","5ecfe21a":"from sklearn.ensemble import RandomForestClassifier","152a8b9e":"rf_model = RandomForestClassifier(n_estimators = 1000)\nrf_pipeline = Pipeline([(\"preprocessor\",preprocessor),(\"model\",rf_model)])","4a635e9f":"rf_search = GridSearchCV(rf_pipeline,param_grid = {\"model__n_estimators\":np.arange(100,1100,100)},n_jobs = -1)\nrf_search.fit(X_train,y_train)","68e90c03":"rf_search.best_params_","94b0ea73":"rf_search.best_score_","53531910":"rf_model = RandomForestClassifier(n_estimators = rf_search.best_params_[\"model__n_estimators\"])\nrf_pipeline = Pipeline([(\"preprocessor\",preprocessor),(\"model\",rf_model)])\nrf_pipeline.fit(X_train,y_train)","4a2e43e8":"rf_preds = rf_pipeline.predict(X_valid)\nprint(sum(rf_preds == y_valid)\/len(rf_preds))","7a5a3abc":"all(rf_preds == lr_preds) #Confirmation in case the accuracies turn out to be equal","eded1c45":"rf_CM = confusion_matrix(y_valid, rf_preds)\nrf_TP = rf_CM[1,1]\nrf_TN = rf_CM[0,0]\nrf_FP = rf_CM[0,1]\nrf_FN = rf_CM[1,0]\nprint(rf_CM)","aed506d7":"rf_recall = rf_TP\/(rf_TP + rf_FN)\nrf_specificity = rf_TN\/(rf_TN + rf_FP)\nlr_precision = rf_TP\/(rf_TP + rf_FP)\n\nprint(rf_recall,rf_specificity)","5d9e20f9":"print(lr_recall, lr_specificity)","8a8e8f19":"rf_y_proba = rf_pipeline.predict_proba(X_valid)[:,1]\nrf_fpr,rf_tpr,rf_threshold = roc_curve(y_valid,rf_y_proba)","54dd93d1":"plt.plot(lr_fpr,lr_tpr,\"b\", label = \"Logistic Regression\")\nplt.plot(rf_fpr, rf_tpr, \"r\", label = \"Random Forest\")\nplt.plot([0,1],ls = \"--\")\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()","436ee12f":"print(\"logistic regression AUC = \",roc_auc_score(y_valid,lr_preds))\nprint(\"Random forests AUC = \",roc_auc_score(y_valid,rf_preds))","acfb5d65":"Angina - A type of chest pain caused by reduced blood flow to the heart. Let's check the correlation of cp with heart failure rates, given that heart failure criteria is given as narrowing of artery diameter (which could cause angina, hence angina being the predictor)","0985eadd":"Our model seems to have decent recall and specificity","983e8895":"If you have a heart failure, there seems to be a greater chance that you're female, i.e., P(female | diseased person) > P(male | diseased person)","9537d867":"This doesn't look like a very important variable, given that the core regions of positives and negatives overlap","1cd6eee9":"We will do a grid search to find the best logistic regression regularization parameter","5b1b9402":"slope 2 and low oldpeak seems to be clear cut indicators of heart failure. High values of olepeak seems to be correlated with no heart failure. So we expect similar results in the model. We will one-hot encode slope","c60a950e":"### Logistic Regression\n\nLet's start with good ol' logistic regression, because we'll be able to understand the individual coefficients in a linear model[](http:\/\/)\n","9d4e8b98":"#### ca - Number of major vessels coloured by fluoroscopy","2bb40bf7":"If you're female there's a greater chance that you're diseased i.e., P(diseased | female) > P(diseased | male)","f55d9f24":"The other probability also (for completion)","00cadb9c":"#### A look at the coefficients","b718839d":"Yep, we need to shuffle the dataset","d65a1ec2":"### Random forests\n","74ffe57c":"Logistic Regression seems to have a slight edge in the AUC score","d63badae":"Some of our conclusions from the visualization part seem to have come true\n\n* `ca` is a very good predictor of not having a heart disease (in fact the best predictor)\n* `exang` being moderately negatively correlated with heart disease\n* `sex` being negatively correlated (women being more prone than men)\n* `slope` = 2 being a good indicator (moderately high positive weight)\n* `cp` = 2 (non-anginal pain) having the highest positive weight\n\nOverall, this has confirmed some of the trends we saw in the visualization phase","435ede0a":"### Angina - cp","f94e701e":"Confusion matrix order :- \n\nTN | FP  \nFN | TP","c4b1a6c8":"So this means if a fluoroscopy is done, there is a greater chance of not having a heart failure. For the time being, we will include `ca` as a bool - ca > 0 : 1, else 0","2ae942c9":"There seems to exist a slight class imbalance. We might need to accommodate this if required","4ba0f645":"#### chol - Serum cholesterol","e77d1d02":"### Age group","4dba169b":"Recall\n* Value 0: typical angina\n* Value 1: atypical angina\n* Value 2: non-anginal pain\n* Value 3: asymptomatic","2c2c2a4f":"The highest distribution here too seems to be on the lower side (120s) and in the same age group as above (50s)","e6355528":"To summarize, we shall be doing the following for modelling\n\n* Age - as is, no normalization\n* Sex - as is, no modifications\n* cp - one hot encode\n* fbs - as is, no modification\n* restecg - one hot encode\n* trestbps, chol, thalach, oldpeak - Normalize using StandardScaler so that the numbers that enter the model have the same order of magnitude\n* exang - include (expect negative correlation)\n* slope - on ehot encode\n* ca - make it bool ( 1 if at least one vesse coloured by fluoroscopy)\n* thal - drop (because I don't know what this variable is)","dd24ef0c":"The bi-modality in the positives is quite striking, as is the peak in the 60s for the negatives. Looks like old people are healthy after all :) ","430ff050":"What I understand from [here](https:\/\/www.hopkinsmedicine.org\/health\/treatment-tests-and-therapies\/fluoroscopy-procedure) and [here](https:\/\/www.hopkinsmedicine.org\/health\/treatment-tests-and-therapies\/cardiac-catheterization) is that a tube is inserted and the arteries coloured to check for any blockages. So I'm assuming the more vessels that were checked, the stronger our conclusions are, so the `ca` variable could be a very strong predictor of the presence or absence of the disease","b31d6cbc":"#### Description of columns from the text file","d7f536ba":"#### Oldpeak and thal","19de44c6":"Since I'm no doctor, I don't have an intuitive idea of what these are supposed to mean and how the slope affects diagnosis, so I plot these without any intuition","49809f01":"Looks like most people having heart failures seem to be in their 50s and having moderately high (130 mm Hg) blood pressure","656b4af8":"column names in X_train : trestbps, chol, thalach, oldpeak, cp = 0, cp = 1, cp = 2, cp = 3, restecg = 0, restecg = 1, restecg = 2, slope = 0, slope = 1, slope = 2, age, sex, fbs, exang, ca. The fit gives us weights in the same order","0b9fda68":"#### Create X and Y arrays for ML","a1582107":"## Exploring specific columns","1ce52797":"Looks like non-anginal pain seems to be a marker for heart failure, while anginal pain seems to be a marker for no heart failure","e737f8c2":"Exercise induced angina seems to be a negative indicator of heart failure","741e6198":"#### trestbps - Resting blood pressure","ade866b0":"* age - age\n* sex - biological sex (1 = male, 0 = female)\n* cp - chest pain type\n  * Value 1: typical angina\n  * Value 2: atypical angina\n  * Value 3: non-anginal pain\n  * Value 4: asymptomatic\n* trestbps - resting blood pressure on admission to hospital\n* chol - serum cholesterol (mg\/dl)\n* fbs - fasting blood sugar (indicator variable, 1 if > 120 mg\/dl, 0 o.w)\n* restecg - resting electrocardiographic results\n  * Value 0: normal\n  * Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n  * Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n                    \n* thalach - maximum heart rate achieved\n* exang - exercise induced angina (1 = yes; 0 = no)\n* oldpeak - ST depression induced by exercise relative to rest\n* slope: the slope of the peak exercise ST segment\n  * Value 1: upsloping\n  * Value 2: flat\n  * Value 3: downsloping\n* ca - number of major vessels colored by fluroscopy\n* thal - 3 = normal; 6 = fixed defect; 7 = reversable defect\n*  target - diagnosis of heart disease (angiographic disease status)\n   * Value 0: < 50% diameter narrowing\n   * Value 1: > 50% diameter narrowing\n\n\n\n","65766d2a":"Let's check how the data is distributed in the dataset - we don't want all positive and all negative targets together","09aaa26f":"the random forest model seems to give us a better recall sacrificing specifity. We need a very good recall rate so that we have low false negatives in our diagnosis","6f1ca8f7":"## ML Models","810a4886":"## Heart disease dataset - exploration","ed23a031":"Time for some metrics"}}