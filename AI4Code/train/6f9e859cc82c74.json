{"cell_type":{"5dcff9a9":"code","10dfff47":"code","e2198c22":"code","c958a38d":"code","cb7d7c6b":"code","fb4e29c4":"code","f1d4947b":"code","7359f995":"code","c60fb480":"code","c9817d01":"code","36d664f7":"code","aabc7829":"code","f48c1f32":"code","6a9a0195":"code","5a1151f1":"code","186423cc":"code","ea398e63":"code","6230b14a":"code","d80efcca":"code","291ee146":"code","66219353":"code","3303e715":"code","9c88806a":"code","421c4765":"code","01db86f1":"code","d08030de":"code","47329776":"code","af868658":"code","a750f002":"code","8885729d":"code","5357d7e7":"code","fe10cd9a":"code","5ac56a90":"code","d22c6e9d":"code","cca123c5":"code","7da9d671":"code","c61ff73e":"code","b5670c39":"code","e087a978":"markdown","0695c2ab":"markdown","d6d17b9e":"markdown","6f4b50b6":"markdown","3e885175":"markdown","dd65db23":"markdown","0bb3f671":"markdown","491f0cf8":"markdown","509759c5":"markdown","fa2adee1":"markdown","a7997dc8":"markdown","d920cd7e":"markdown","d39d042c":"markdown","363d0cbb":"markdown","d0c10de7":"markdown","dd145b4f":"markdown","14d15210":"markdown","11e04292":"markdown","9c17aa4d":"markdown","493555e4":"markdown","04950702":"markdown","65b637a8":"markdown","5f799a8d":"markdown","01889f1d":"markdown","0054f2e1":"markdown","2f13e949":"markdown","5385c5a0":"markdown","2048a36b":"markdown","9be5e80f":"markdown","95599224":"markdown","03b6aab4":"markdown","33d28f8f":"markdown","6a817452":"markdown"},"source":{"5dcff9a9":"# in google colab uncomment this\n\nimport os\n\nos.system('apt-get update')\nos.system('apt-get install -y xvfb')\nos.system('wget https:\/\/raw.githubusercontent.com\/yandexdataschool\/Practical_DL\/fall18\/xvfb -O ..\/xvfb')\nos.system('apt-get install -y python-opengl ffmpeg')\nos.system('pip install pyglet==1.5.0')\n\nos.system('python -m pip install -U pygame --user')\n\nprefix = 'https:\/\/raw.githubusercontent.com\/yandexdataschool\/Practical_RL\/master\/week04_approx_rl\/'\n\nos.system('wget ' + prefix + 'atari_wrappers.py')\nos.system('wget ' + prefix + 'utils.py')\nos.system('wget ' + prefix + 'replay_buffer.py')\nos.system('wget ' + prefix + 'framebuffer.py')\n\n# print('setup complete')\n\n# XVFB will be launched if you run on a server\nimport os\nif type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n    !bash ..\/xvfb start\n    os.environ['DISPLAY'] = ':1'","10dfff47":"import random\nimport numpy as np\nimport torch\nimport utils","e2198c22":"import gym\nimport numpy as np\nimport matplotlib.pyplot as plt","c958a38d":"ENV_NAME = \"BreakoutNoFrameskip-v4\"","cb7d7c6b":"env = gym.make(ENV_NAME)\nenv.reset()\n\nn_cols = 5\nn_rows = 2\nfig = plt.figure(figsize=(16, 9))\n\nfor row in range(n_rows):\n    for col in range(n_cols):\n        ax = fig.add_subplot(n_rows, n_cols, row * n_cols + col + 1)\n        ax.imshow(env.render('rgb_array'))\n        env.step(env.action_space.sample())\nplt.show()","fb4e29c4":"# # does not work in colab.\n# # make keyboard interrupt to continue\n\nfrom gym.utils.play import play\n\nplay(env=gym.make(ENV_NAME), zoom=5, fps=30)","f1d4947b":"from gym.core import ObservationWrapper\nfrom gym.spaces import Box\nfrom skimage import color, img_as_float\nfrom skimage.transform import rescale, resize\nfrom skimage.color import rgb2gray \n\nclass PreprocessAtariObs(ObservationWrapper):\n    def __init__(self, env):\n        \"\"\"Una gym wrapper que recorta, escala la imagen en las formas deseadas y la escala de grises.\"\"\" \n        ObservationWrapper.__init__(self, env)\n\n        self.img_size = (1, 64, 64)\n        self.observation_space = Box(0.0, 1.0, self.img_size)\n\n    def _to_gray_scale(self, rgb, channel_weights=[0.8, 0.1, 0.1]):\n        return np.dot(rgb[...,:3], channel_weights)\n    \n    def observation(self, img):\n        \"\"\"Qu\u00e9 sucede con cada observaci\u00f3n\"\"\"\n        # Esto es lo que debe hacer:\n         # * recortar imagen, eliminar partes irrelevantes\n         # * cambiar el tama\u00f1o de la imagen a self.img_size\n         # (use imresize de cualquier biblioteca que desee,\n         #      p.ej. opencv, skimage, PIL, keras)\n         # * convertir imagen a escala de grises\n         # * convertir p\u00edxeles de imagen a rango (0,1), tipo float3\n        img = img = img[40:-10,3:-3] #img[50:-15,5:-5]\n        img = self._to_gray_scale(img)\n        img = resize(img, self.img_size[1:3]).reshape(self.img_size) \n        img = np.array(img, dtype=np.uint8)\n        img = img_as_float(img)\n        img = np.float32(img)\n        \n        return img","7359f995":"import gym\n# instancia de juego de generaci\u00f3n para pruebas\nenv = gym.make(ENV_NAME)  # crear env crudo\nenv = PreprocessAtariObs(env)\nobservation_shape = env.observation_space.shape\nn_actions = env.action_space.n\nenv.reset()\nobs, _, _, _ = env.step(env.action_space.sample())\n\n# test observation\nassert obs.ndim == 3, \"observation must be [channel, h, w] even if there's just one channel\"\nassert obs.shape == observation_shape\nassert obs.dtype == 'float32'\nassert len(np.unique(obs)) > 2, \"your image must not be binary\"\nassert 0 <= np.min(obs) and np.max(\n    obs) <= 1, \"convert image pixels to [0,1] range\"\n\nassert np.max(obs) >= 0.5, \"It would be easier to see a brighter observation\"\nassert np.mean(obs) >= 0.1, \"It would be easier to see a brighter observation\"\n\nprint(\"Las pruebas formales parecen estar bien. Aqu\u00ed tienes un ejemplo de lo que obtendr\u00e1s.\")\n\nn_cols = 5\nn_rows = 2\nfig = plt.figure(figsize=(16, 9))\nobs = env.reset()\nfor row in range(n_rows):\n    for col in range(n_cols):\n        ax = fig.add_subplot(n_rows, n_cols, row * n_cols + col + 1)\n        ax.imshow(obs[0, :, :], interpolation='none', cmap='gray')\n        obs, _, _, _ = env.step(env.action_space.sample())\nplt.show()\n","c60fb480":"%load_ext autoreload\n%autoreload 2\nimport atari_wrappers\n\ndef PrimaryAtariWrap(env, clip_rewards=True):\n    assert 'NoFrameskip' in env.spec.id\n\n    # Este contenedor tiene la misma acci\u00f3n para <skip> frames y outputs\n     # el valor m\u00e1ximo de p\u00edxeles de 2 \u00faltimos fotogramas (para manejar el parpadeo\n     # en algunos envs)\n    env = atari_wrappers.MaxAndSkipEnv(env, skip=4)\n\n    # Este contenedor env\u00eda done = True cuando se pierde cada vida\n     # (no todas las 5 vidas que est\u00e1n dadas por las reglas del juego).\n     # Deber\u00eda facilitarle al agente la comprensi\u00f3n de que perder es malo.\n    env = atari_wrappers.EpisodicLifeEnv(env)\n\n    # Este envoltorio lanza la pelota cuando comienza un episodio. \n    # Sin \u00e9l, el agente tambi\u00e9n tiene que aprender esta acci\u00f3n. \n    # De hecho, puede, pero el aprendizaje tomar\u00eda m\u00e1s tiempo.\n    env = atari_wrappers.FireResetEnv(env)\n\n    # Este contenedor transforma las recompensas en {-1, 0, 1} seg\u00fan su signo.\n    if clip_rewards:\n        env = atari_wrappers.ClipRewardEnv(env)\n\n    # This wrapper is yours :)\n    env = PreprocessAtariObs(env)\n    return env","c9817d01":"# # does not work in colab.\n# # make keyboard interrupt to continue\n\nfrom gym.utils.play import play\n\ndef make_play_env():\n    env = gym.make(ENV_NAME)\n    env = PrimaryAtariWrap(env)\n# en torch, las im\u00e1genes tienen forma [c, h, w] en lugar de com\u00fan [h, w, c]\n    env = atari_wrappers.AntiTorchWrapper(env)\n    return env\n\nplay(make_play_env(), zoom=5, fps=3)","36d664f7":"from framebuffer import FrameBuffer\n\ndef make_env(clip_rewards=True, seed=None):\n    env = gym.make(ENV_NAME)  # create raw env\n    if seed is not None:\n        env.seed(seed)\n    env = PrimaryAtariWrap(env, clip_rewards)\n    env = FrameBuffer(env, n_frames=4, dim_order='pytorch')\n    return env\n\nenv = make_env()\nenv.reset()\nn_actions = env.action_space.n\nstate_shape = env.observation_space.shape","aabc7829":"for _ in range(12):\n    obs, _, _, _ = env.step(env.action_space.sample())\n\nplt.figure(figsize=[12,10])\nplt.title(\"Game image\")\nplt.imshow(env.render(\"rgb_array\"))\nplt.show()\n\nplt.figure(figsize=[15,15])\nplt.title(\"Agent observation (4 frames top to bottom)\")\nplt.imshow(utils.img_by_obs(obs, state_shape), cmap='gray')\nplt.show()","f48c1f32":"import torch\nimport torch.nn as nn\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# aquellos que tienen una GPU pero se sienten injustos al usarla pueden descomentar:\n# dispositivo = dispositivo de antorcha ('cpu')\ndevice","6a9a0195":"def conv2d_size_out(size, kernel_size, stride):\n    \"\"\"\n    caso de uso com\u00fan:\n     cur_layer_img_w = conv2d_size_out (cur_layer_img_w, kernel_size, stride)\n     cur_layer_img_h = conv2d_size_out (cur_layer_img_h, kernel_size, stride)\n     para comprender la forma de la entrada de la capa densa\n    \"\"\"\n    return (size - (kernel_size - 1) - 1) \/\/ stride  + 1\n","5a1151f1":"print(conv2d_size_out(64,3,2))\nprint(conv2d_size_out(conv2d_size_out(64,3,2), 3, 2))\nprint(conv2d_size_out(conv2d_size_out(conv2d_size_out(64,3,2), 3, 2), 3, 2))\n\nprint(conv2d_size_out(conv2d_size_out(conv2d_size_out(64,3,2), 3, 2), 3, 2)**2)\nprint(conv2d_size_out(conv2d_size_out(conv2d_size_out(64,3,2), 3, 2), 3, 2)**2 * 64)","186423cc":"class DQNAgent(nn.Module):\n    def __init__(self, state_shape, n_actions, epsilon=0):\n\n        super().__init__()\n        self.epsilon = epsilon\n        self.n_actions = n_actions\n        self.state_shape = state_shape\n\n        # Defina aqu\u00ed el cuerpo de su red. Aseg\u00farese de que el agente est\u00e9 completamente incluido aqu\u00ed\n        # nn.Flatten () puede ser \u00fatil\n        state_dim = state_shape[0]\n        self.network = nn.Sequential() #input(4,1,64,64)\n        self.network.add_module('layer1', nn.Conv2d(in_channels=state_dim, out_channels=16, kernel_size=3, stride=2)) #(4,16,31,31)\n        self.network.add_module('activation1', nn.ReLU())\n        self.network.add_module('layer2', nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2)) #(4,32,15,15)\n        self.network.add_module('activation2', nn.ReLU())\n        self.network.add_module('layer3', nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2)) #(4,64,7,7)\n        self.network.add_module('activation3', nn.ReLU())\n        self.network.add_module('flatten', nn.Flatten())\n        self.network.add_module('layer4', nn.Linear(in_features=conv2d_size_out(conv2d_size_out(conv2d_size_out(64,3,2), 3, 2), 3, 2)**2 * 64, out_features=256))\n        self.network.add_module('activation4', nn.ReLU())\n        self.network.add_module('layer5', nn.Linear(in_features=256, out_features=n_actions))\n        \n\n    def forward(self, state_t):\n        \"\"\"\n        toma la observaci\u00f3n del agente (tensor), devuelve qvalues (tensor)\n         : param state_t: un lote de b\u00faferes de 4 cuadros, forma = [batch_size, 4, h, w]\n        \"\"\"\n        # Use su red para calcular los valores q para un estado dado\n        qvalues = self.network(state_t)\n\n        assert qvalues.requires_grad, \"qvalues must be a torch tensor with grad\"\n        assert len(\n            qvalues.shape) == 2 and qvalues.shape[0] == state_t.shape[0] and qvalues.shape[1] == n_actions\n\n        return qvalues\n\n    def get_qvalues(self, states):\n        \"\"\"\n        como hacia adelante, pero funciona en matrices numerosas, no en tensores\n        \"\"\"\n        model_device = next(self.parameters()).device\n        states = torch.tensor(states, device=model_device, dtype=torch.float)\n        qvalues = self.forward(states)\n        return qvalues.data.cpu().numpy()\n\n    def sample_actions(self, qvalues):\n        \"\"\"elegir acciones dados qvalores. Utiliza una estrategia de exploraci\u00f3n \u00e1vida de \u00e9psilon. \"\"\"\n        epsilon = self.epsilon\n        batch_size, n_actions = qvalues.shape\n\n        random_actions = np.random.choice(n_actions, size=batch_size)\n        best_actions = qvalues.argmax(axis=-1)\n\n        should_explore = np.random.choice(\n            [0, 1], batch_size, p=[1-epsilon, epsilon])\n        return np.where(should_explore, random_actions, best_actions)","ea398e63":"agent = DQNAgent(state_shape, n_actions, epsilon=0.5).to(device)","6230b14a":"print(\"Descripcion del la red neuronal:\")\nagent","d80efcca":"def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n    rewards = []\n    for _ in range(n_games):\n        s = env.reset()\n        reward = 0\n        for _ in range(t_max):\n            qvalues = agent.get_qvalues([s])\n            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n            s, r, done, _ = env.step(action)\n            reward += r\n            if done:\n                break\n\n        rewards.append(reward)\n    return np.mean(rewards)","291ee146":"e=0\nwhile(e==0):\n    e=evaluate(env, agent, n_games=1)\n\nprint(e)","66219353":"from replay_buffer import ReplayBuffer\nexp_replay = ReplayBuffer(10)\n\nfor _ in range(30):\n    exp_replay.add(env.reset(), env.action_space.sample(), 1.0, env.reset(), done=False)\n\nobs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(5)\n\nassert len(exp_replay) == 10, \"el tama\u00f1o de reproducci\u00f3n de la experiencia deber\u00eda ser 10 porque esa es la capacidad m\u00e1xima\"","3303e715":"def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n    \"\"\"\n    Juega exactamente n pasos, graba cada (s, a, r, s ', hecho) para reproducir el b\u00fafer.\n    Siempre que el juego termine, agregue el registro con done = True y reinicie el juego.\n    Se garantiza que env ha hecho = False cuando se pasa a esta funci\u00f3n.\n\n    POR FAVOR NO REINICIE ENV A MENOS QUE EST\u00c9 \"HECHO\"\n\n    : devuelve: devuelve la suma de las recompensas a lo largo del tiempo y el estado en el que permanece el env\n    \"\"\"\n    s = initial_state\n    sum_rewards = 0\n\n    # Juega el juego por n_steps seg\u00fan las instrucciones anteriores\n    for i in range(0, n_steps):\n        qvalues = agent.get_qvalues([s])\n        action = agent.sample_actions(qvalues)[0]\n            \n        next_s, r, done, _ = env.step(action)\n    \n        exp_replay.add(s, action, r, next_s, done)\n        \n        s = next_s\n        sum_rewards += r\n        if done:\n            s = env.reset()\n\n    return sum_rewards, s","9c88806a":"# testing your code.\nexp_replay = ReplayBuffer(2000)\n\nstate = env.reset()\nplay_and_record(state, agent, env, exp_replay, n_steps=1000)\n\n# Si est\u00e1 utilizando su propio b\u00fafer de reproducci\u00f3n de experiencias, es posible que algunas de esas pruebas necesiten correcci\u00f3n.\n# solo aseg\u00farate de saber lo que hace tu c\u00f3digo\nassert len(exp_replay) == 1000, \"play_and_record should have added exactly 1000 steps, \"\\\n                                 \"but instead added %i\" % len(exp_replay)\nis_dones = list(zip(*exp_replay._storage))[-1]\n\nassert 0 < np.mean(is_dones) < 0.1, \"Please make sure you restart the game whenever it is 'done' and record the is_done correctly into the buffer.\"\\\n                                    \"Got %f is_done rate over %i steps. [If you think it's your tough luck, just re-run the test]\" % (\n                                        np.mean(is_dones), len(exp_replay))\n\nfor _ in range(100):\n    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(\n        10)\n    assert obs_batch.shape == next_obs_batch.shape == (10,) + state_shape\n    assert act_batch.shape == (10,), \"actions batch should have shape (10,) but is instead %s\" % str(act_batch.shape)\n    assert reward_batch.shape == (10,), \"rewards batch should have shape (10,) but is instead %s\" % str(reward_batch.shape)\n    assert is_done_batch.shape == (10,), \"is_done batch should have shape (10,) but is instead %s\" % str(is_done_batch.shape)\n    assert [int(i) in (0, 1)for i in is_dones], \"is_done should be strictly True or False\"\n    assert [0 <= a < n_actions for a in act_batch], \"actions should be within [0, n_actions)\"\n\nprint(\"Well done!\")","421c4765":"target_network = DQNAgent(agent.state_shape, agent.n_actions, epsilon=0.5).to(device)\n# As\u00ed es como puede cargar pesos del agente a la red de destino\ntarget_network.load_state_dict(agent.state_dict())","01db86f1":"def compute_td_loss(states, actions, rewards, next_states, is_done,\n                    agent, target_network,\n                    gamma=0.99,\n                    check_shapes=False,\n                    device=device):\n    \"\"\"Calcule la p\u00e9rdida td utilizando \u00fanicamente operaciones de torch. Utilice las f\u00f3rmulas anteriores.\"\"\"\n    states = torch.tensor(states, device=device, dtype=torch.float)    # shape: [batch_size, *state_shape]\n\n    # por alguna raz\u00f3n de la torch  no deber\u00eda convertir las acciones en un tensor\n    actions = torch.tensor(actions, device=device, dtype=torch.long)    # shape: [batch_size]\n    rewards = torch.tensor(rewards, device=device, dtype=torch.float)  # shape: [batch_size]\n    # shape: [batch_size, *state_shape]\n    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n    is_done = torch.tensor(\n        is_done.astype('float32'),\n        device=device,\n        dtype=torch.float\n    )  # shape: [batch_size]\n    is_not_done = 1 - is_done\n\n    # obtener valores q para todas las acciones en los estados actuales\n    predicted_qvalues = agent(states)\n\n    # calcular los valores q para todas las acciones en los siguientes estados\n    predicted_next_qvalues = target_network(next_states)\n    \n    # seleccione los valores q para las acciones elegidas\n    predicted_qvalues_for_actions = predicted_qvalues[range(\n        len(actions)), actions]\n\n    # calcular V * (next_states) usando los siguientes valores q predichos\n    next_state_values = torch.max(predicted_next_qvalues, dim=1).values\n\n    assert next_state_values.dim(\n    ) == 1 and next_state_values.shape[0] == states.shape[0], \"must predict one value per state\"\n\n    # Calcule los \"valores q objetivo\" para la p\u00e9rdida: es lo que est\u00e1 dentro de los par\u00e9ntesis cuadrados en la f\u00f3rmula anterior.\n    # en el \u00faltimo estado use la f\u00f3rmula simplificada: Q (s, a) = r (s, a) ya que s 'no existe\n    # puede multiplicar los valores del siguiente estado por is_not_done para lograr esto.\n    target_qvalues_for_actions = rewards + gamma * is_not_done * next_state_values\n\n    # mean squared error loss to minimize\n    loss = torch.mean((predicted_qvalues_for_actions -\n                       target_qvalues_for_actions.detach()) ** 2)\n\n    if check_shapes:\n        assert predicted_next_qvalues.data.dim(\n        ) == 2, \"make sure you predicted q-values for all actions in next state\"\n        assert next_state_values.data.dim(\n        ) == 1, \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n        assert target_qvalues_for_actions.data.dim(\n        ) == 1, \"there's something wrong with target q-values, they must be a vector\"\n\n    return loss","d08030de":"obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(10)\n\nloss = compute_td_loss(obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch,\n                       agent, target_network,\n                       gamma=0.99, check_shapes=True)\nloss.backward()\n\nassert loss.requires_grad and tuple(loss.data.size()) == (), \"you must return scalar loss - mean over batch\"\nassert np.any(next(agent.parameters()).grad.data.cpu().numpy() != 0), \"loss must be differentiable w.r.t. network weights\"\nassert np.all(next(target_network.parameters()).grad is None), \"target network should not have grads\"","47329776":"from tqdm import trange\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt","af868658":"seed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)","a750f002":"env = make_env(seed)\nstate_shape = env.observation_space.shape\nn_actions = env.action_space.n\nstate = env.reset()\n\nagent = DQNAgent(state_shape, n_actions, epsilon=1).to(device)\ntarget_network = DQNAgent(state_shape, n_actions).to(device)\ntarget_network.load_state_dict(agent.state_dict())","8885729d":"exp_replay = ReplayBuffer(10**4)\nfor i in range(100):\n    if not utils.is_enough_ram(min_available_gb=0.1):\n        print(\"\"\"\n            Menos de 100 Mb de RAM disponibles.\n            Aseg\u00farese de que el tama\u00f1o del b\u00fafer no sea demasiado grande.\n            Tambi\u00e9n verifique, tal vez otros procesos consuman mucho RAM.\n            \"\"\"\n             )\n        break\n    play_and_record(state, agent, env, exp_replay, n_steps=10**2)\n    if len(exp_replay) == 10**4:\n        break\nprint(len(exp_replay))","5357d7e7":"timesteps_per_epoch = 1\nbatch_size = 16\ntotal_steps = 3 * 10**6\ndecay_steps = 10**6\n\nopt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n\ninit_epsilon = 1\nfinal_epsilon = 0.1\n\nloss_freq = 50\nrefresh_target_network_freq = 5000 #10000\neval_freq = 5000\n\nmax_grad_norm = 50\n\nn_lives = 5","fe10cd9a":"mean_rw_history = []\ntd_loss_history = []\ngrad_norm_history = []\ninitial_state_v_history = []\nstep = 0","5ac56a90":"state = env.reset()\nfor step in trange(step, total_steps + 1):\n    if not utils.is_enough_ram():\n        print('less that 100 Mb RAM available, freezing')\n        print('make sure everythin is ok and make KeyboardInterrupt to continue')\n        try:\n            while True:\n                pass\n        except KeyboardInterrupt:\n            pass\n\n    agent.epsilon = utils.linear_decay(init_epsilon, final_epsilon, step, decay_steps)\n\n    # play\n    _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n\n    # train\n    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(batch_size)#< sample batch_size of data from experience replay >\n\n    loss = compute_td_loss(obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch, agent, target_network)  # < compute TD loss >\n\n    loss.backward()\n    grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n    opt.step()\n    opt.zero_grad()\n\n    if step % loss_freq == 0:\n        td_loss_history.append(loss.data.cpu().item())\n        grad_norm_history.append(grad_norm)\n\n    if step % refresh_target_network_freq == 0:\n        # Load agent weights into target_network\n        target_network.load_state_dict(agent.state_dict())   #<YOUR CODE >\n\n    if step % eval_freq == 0:\n        mean_rw_history.append(evaluate(\n            make_env(clip_rewards=True, seed=step), agent, n_games=3 * n_lives, greedy=True)\n        )\n        initial_state_q_values = agent.get_qvalues(\n            [make_env(seed=step).reset()]\n        )\n        initial_state_v_history.append(np.max(initial_state_q_values))\n\n        clear_output(True)\n        print(\"buffer size = %i, epsilon = %.5f\" %\n              (len(exp_replay), agent.epsilon))\n\n        plt.figure(figsize=[16, 9])\n\n        plt.subplot(2, 2, 1)\n        plt.title(\"Mean reward per life\")\n        plt.plot(mean_rw_history)\n        plt.grid()\n\n        assert not np.isnan(td_loss_history[-1])\n        plt.subplot(2, 2, 2)\n        plt.title(\"TD loss history (smoothened)\")\n        plt.plot(utils.smoothen(td_loss_history))\n        plt.grid()\n\n        plt.subplot(2, 2, 3)\n        plt.title(\"Initial state V\")\n        plt.plot(initial_state_v_history)\n        plt.grid()\n\n        plt.subplot(2, 2, 4)\n        plt.title(\"Grad norm history (smoothened)\")\n        plt.plot(utils.smoothen(grad_norm_history))\n        plt.grid()\n\n        plt.show()","d22c6e9d":"final_score = evaluate(\n  make_env(clip_rewards=False, seed=9),\n    agent, n_games=30, greedy=True, t_max=10 * 1000\n) * n_lives\nprint('final score:', final_score)\nassert final_score >= 15, 'not as cool as DQN can'\nprint('Cool!')","cca123c5":"# record sessions\nimport gym.wrappers\nenv_monitor = gym.wrappers.Monitor(make_env(), directory=\"videos\", force=True)\nsessions = [evaluate(env_monitor, agent, n_games=n_lives, greedy=True) for _ in range(10)]\nenv_monitor.close()","7da9d671":"# show video\nfrom IPython.display import HTML\nimport os\n\nvideo_names = list(\n    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\".\/videos\/\")))\n\nHTML(\"\"\"\n<video width=\"640\" height=\"480\" controls>\n  <source src=\"{}\" type=\"video\/mp4\">\n<\/video>\n\"\"\".format(\".\/videos\/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices","c61ff73e":"eval_env = make_env(clip_rewards=False)\nrecord = utils.play_and_log_episode(eval_env, agent)\nprint('total reward for life:', np.sum(record['rewards']))\nfor key in record:\n    print(key)","b5670c39":"fig = plt.figure(figsize=(5, 5))\nax = fig.add_subplot(1, 1, 1)\n\nax.scatter(record['v_mc'], record['v_agent'])\nax.plot(sorted(record['v_mc']), sorted(record['v_mc']),\n       'black', linestyle='--', label='x=y')\n\nax.grid()\nax.legend()\nax.set_title('State Value Estimates')\nax.set_xlabel('Monte-Carlo')\nax.set_ylabel('Agent')\n\nplt.show()","e087a978":"**Dueling network: (+2 pts)**\n$$Q_{\\theta}(s, a) = V_{\\eta}(f_{\\xi}(s)) + A_{\\psi}(f_{\\xi}(s), a) - \\frac{\\sum_{a'}A_{\\psi}(f_{\\xi}(s), a')}{N_{actions}},$$\nwhere $\\xi$, $\\eta$, and $\\psi$ are, respectively, the parameters of the\nshared encoder $f_\u03be$ , of the value stream $V_\\eta$ , and of the advan\ntage stream $A_\\psi$; and $\\theta = \\{\\xi, \\eta, \\psi\\}$ is their concatenation.\n\nFor the architecture on the image $V$ and $A$ heads can follow the dense layer instead of $Q$. Please don't worry that the model becomes a little bigger.","0695c2ab":"![img](https:\/\/github.com\/yandexdataschool\/Practical_RL\/raw\/master\/yet_another_week\/_resource\/dqn_arch.png)","d6d17b9e":"## DQN as it is","6f4b50b6":"## Let's have a closer look at this.\n\nIf average episode score is below 200 using all 5 lives, then probably DQN has not converged fully. But anyway let's make a more complete record of an episode.","3e885175":"**Let's see if the game is still playable after applying the wrappers.**\nAt playing the EpisodicLifeEnv wrapper seems not to work but actually it does (because after when life finishes a new ball is dropped automatically - it means that FireResetEnv wrapper understands that a new episode began).","dd65db23":"Now let's try out our agent to see if it raises any errors.","0bb3f671":"**This notebook is the main notebook.** Another notebook is given for debug. (**homework_pytorch_main**). The tasks are similar and share most of the code. The main difference is in environments. In main notebook it can take some 2 hours for the agent to start improving so it seems reasonable to launch the algorithm on a simpler env first. In debug one it is CartPole and it will train in several minutes.","491f0cf8":"## Preprocessing","509759c5":"**About the game:** You have 5 lives and get points for breaking the wall. Higher bricks cost more than the lower ones. There are 4 actions: start game (should be called at the beginning and after each life is lost), move left, move right and do nothing. There are some common wrappers used for Atari environments.","fa2adee1":"__Frameworks__ - we'll accept this homework in any deep learning framework. This particular notebook was designed for pytoch, but you find it easy to adapt it to almost any python-based deep learning framework.","a7997dc8":"#### The interface is fairly simple:\n* `exp_replay.add(obs, act, rw, next_obs, done)` - saves (s,a,r,s',done) tuple into the buffer\n* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n* `len(exp_replay)` - returns number of elements stored in replay buffer.","d920cd7e":"Agent is evaluated for 1 life, not for a whole episode of 5 lives. Rewards in evaluation are also truncated. Cuz this is what environment the agent is learning in and in this way mean rewards per life can be compared with initial state value\n\n**The goal is to get 15 points in the real env**. So 3 or better 4 points in the preprocessed one will probably be enough. You can interrupt learning then.","d39d042c":"Let's see what observations look like.","363d0cbb":"## Main loop \n\nIt's time to put everything together and see if it learns anything.","d0c10de7":"**Let's play a little.**\n\nPay attention to zoom and fps args of play function. Control: A, D, space.","dd145b4f":"### Frame buffer\n\nOur agent can only process one observation at a time, so we gotta make sure it contains enough information to find optimal actions. For instance, agent has to react to moving objects so he must be able to measure object's velocity.\n\nTo do so, we introduce a buffer that stores 4 last images. This time everything is pre-implemented for you, not really by the staff of the course :)","14d15210":"$\\hat V_{Monte-Carlo}(s_t) = \\sum_{\\tau=0}^{episode~end} \\gamma^{\\tau-t}r_t$","11e04292":"Buffer of size $10^4$ fits into 5 Gb RAM.\n\nLarger sizes ($10^5$ and $10^6$ are common) can be used. It can improve the learning, but $10^4$ is quiet enough. $10^2$ will probably fail learning.","9c17aa4d":"### Let's play some old videogames\n![img](https:\/\/github.com\/yandexdataschool\/Practical_RL\/raw\/master\/yet_another_week\/_resource\/nerd.png)\n\nThis time we're gonna apply approximate q-learning to an atari game called Breakout. It's not the hardest thing out there, but it's definitely way more complex than anything we tried before.\n","493555e4":"Final scoring is done on a whole episode with all 5 lives.","04950702":"Compute Q-learning TD error:\n\n$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n\nWith Q-reference defined as\n\n$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n\nWhere\n* $Q_{target}(s',a')$ denotes q-value of next state and next action predicted by __target_network__\n* $s, a, r, s'$ are current state, action, reward and next state respectively\n* $\\gamma$ is a discount factor defined two cells above.\n\n\n__Note 1:__ there's an example input below. Feel free to experiment with it before you write the function.\n\n__Note 2:__ compute_td_loss is a source of 99% of bugs in this homework. If reward doesn't improve, it often helps to go through it line by line [with a rubber duck](https:\/\/rubberduckdebugging.com\/).\n\n**Double DQN **\n\n$$ Q_{reference}(s,a) = r(s, a) + \\gamma \\cdot\nQ_{target}(s',argmax_{a'}Q_\\theta(s', a')) $$","65b637a8":"### Wrapping.","5f799a8d":"### Building a network\n\nWe now need to build a neural network that can map images to state q-values. This network will be called on every agent's step so it better not be resnet-152 unless you have an array of GPUs. Instead, you can use strided convolutions with a small number of features to save time and memory.\n\nYou can build any architecture you want, but for reference, here's something that will more or less work:","01889f1d":"## How to interpret plots:\n\nThis aint no supervised learning so don't expect anything to improve monotonously. \n* **TD loss** is the MSE between agent's current Q-values and target Q-values. It may slowly increase or decrease, it's ok. The \"not ok\" behavior includes going NaN or stayng at exactly zero before agent has perfect performance.\n* **grad norm** just shows the intensivity of training. Not ok is growing to values of about 100 (or maybe even 50) though it depends on network architecture.\n* **mean reward** is the expected sum of r(s,a) agent gets over the full game session. It will oscillate, but on average it should get higher over time (after a few thousand iterations...). \n * In basic q-learning implementation it takes about 40k steps to \"warm up\" agent before it starts to get better.\n* **Initial state V** is the expected discounted reward for episode in the oppinion of the agent. It should behave more smoothly than **mean reward**. It should get higher over time but sometimes can experience drawdowns because of the agaent's overestimates.\n* **buffer size** - this one is simple. It should go up and cap at max size.\n* **epsilon** - agent's willingness to explore. If you see that agent's already at 0.01 epsilon before it's average reward is above 0 - it means you need to increase epsilon. Set it back to some 0.2 - 0.5 and decrease the pace at which it goes down.\n* Smoothing of plots is done with a gaussian kernel\n\nAt first your agent will lose quickly. Then it will learn to suck less and at least hit the ball a few times before it loses. Finally it will learn to actually score points.\n\n**Training will take time.** A lot of it actually. Probably you will not see any improvment during first **150k** time steps (note that by default in this notebook agent is evaluated every 5000 time steps).\n\nBut hey, long training time isn't _that_ bad:\n![img](https:\/\/github.com\/yandexdataschool\/Practical_RL\/raw\/master\/yet_another_week\/_resource\/training.png)","0054f2e1":"### Learning with... Q-learning\nHere we write a function similar to `agent.update` from tabular q-learning.","2f13e949":"Is there a big bias? It's ok, anyway it works.","5385c5a0":"### Target networks\n\nWe also employ the so called \"target network\" - a copy of neural network weights to be used for reference Q-values:\n\nThe network itself is an exact copy of agent network, but it's parameters are not trained. Instead, they are moved here from agent's actual network every so often.\n\n$$ Q_{reference}(s,a) = r + \\gamma \\cdot \\max _{a'} Q_{target}(s',a') $$\n\n![img](https:\/\/github.com\/yandexdataschool\/Practical_RL\/raw\/master\/yet_another_week\/_resource\/target_net.png)","2048a36b":"### Experience replay\n\n![img](https:\/\/github.com\/yandexdataschool\/Practical_RL\/raw\/master\/yet_another_week\/_resource\/exp_replay.png)","9be5e80f":"# Deep Q-Network implementation.\n\nThis homework shamelessly demands you to implement a DQN - an approximate q-learning algorithm with experience replay and target networks - and see if it works any better this way.\n\nOriginal paper:\nhttps:\/\/arxiv.org\/pdf\/1312.5602.pdf","95599224":"## About hyperparameters:\n\nThe task has something in common with supervised learning: loss is optimized through the buffer (instead of Train dataset). But the distribution of states and actions in the buffer **is not stationary** and depends on the policy that generated it. It can even happen that the mean TD error across the buffer is very low but the performance is extremely poor (imagine the agent collecting data to the buffer always manages to avoid the ball).\n\n* Total timesteps and training time: It seems to be so huge, but actually it is normal for RL.\n\n* $\\epsilon$ decay shedule was taken from the original paper and is like traditional for epsilon-greedy policies. At the beginning of the training the agent's greedy policy is poor so many random actions should be taken.\n\n* Optimizer: In the original paper RMSProp was used (they did not have Adam in 2013) and it can work not worse than Adam. For us Adam was default and it worked.\n\n* lr: $10^{-3}$ would probably be too huge\n\n* batch size: This one can be very important: if it is too small the agent can fail to learn. Huge batch takes more time to process. If batch of size 8 can not be processed on the hardware you use take 2 (or even 4) batches of size 4, divide the loss on them by 2 (or 4) and make optimization step after both backward() calls in torch.\n\n* target network update frequency: has something in common with learning rate. Too frequent updates can lead to divergence. Too rare can lead to slow leraning. For millions of total timesteps thousands of inner steps seem ok. One iteration of target network updating is an iteration of the (this time approximate) $\\gamma$-compression that stands behind Q-learning. The more inner steps it makes the more accurate is the compression.\n* max_grad_norm - just huge enough. In torch clip_grad_norm also evaluates the norm before clipping and it can be convenient for logging.","03b6aab4":"Sanity checks","33d28f8f":"### Processing game image \n\nRaw atari images are large, 210x160x3 by default. However, we don't need that level of detail in order to learn them.\n\nWe can thus save a lot of time by preprocessing game image, including\n* Resizing to a smaller shape, 64 x 64\n* Converting to grayscale\n* Cropping irrelevant image parts (top, bottom and edges)\n\nAlso please keep one dimension for channel so that final shape would be 1 x 64 x 64.\n\nTip: You can implement your own grayscale converter and assign a huge weight to the red channel. This dirty trick is not necessary but it will speed up learning.","6a817452":"### Video"}}