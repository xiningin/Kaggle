{"cell_type":{"364cf77b":"code","ce945639":"code","b102e2e3":"code","c79c2fb9":"code","a08906d1":"code","16abeb9f":"code","f9330f54":"code","60746708":"code","3fa9361e":"code","d11dfe5f":"code","f8b21692":"code","ecc6b8fd":"code","dcbe5906":"code","fa11ae03":"code","676e63a2":"code","80b1ee25":"code","a050d55d":"code","91fb5275":"code","bfa7a58a":"code","c5665a3f":"code","304ca7bf":"code","77d4330a":"code","a17e5f33":"code","1435a503":"code","1a882a81":"code","0f0cbb08":"code","ed88d76a":"code","245cbba1":"code","472f2203":"code","6bdcc349":"code","4c61d6a2":"code","e6b84808":"code","205bb921":"code","a6b9a899":"code","bb4a2937":"code","d1a4d3ed":"code","33438210":"code","a35d7265":"code","8feb80e1":"code","2878477b":"code","26dbb104":"code","f32fe7b7":"code","9ef3a8dd":"code","ea0a4ff1":"code","6556cbf5":"code","3cc5f21e":"code","ae9aeae7":"code","855baf72":"code","ae173a18":"code","48461534":"code","95cb38fe":"code","75d179b8":"code","e7759465":"code","53e3f78c":"code","f65abc43":"code","0eaba117":"code","bdbf44a1":"code","cd134d68":"code","234baad5":"code","f56a6e9b":"code","d4c47196":"code","545e8aa2":"code","4e225156":"code","083efc8b":"code","22b740d4":"markdown","d7917b46":"markdown","792d9a58":"markdown","f073d435":"markdown","b43eda47":"markdown","2d29c167":"markdown","f1adcb8e":"markdown","78bdedd4":"markdown","6f44e82f":"markdown","460c2581":"markdown","29807313":"markdown","09a63f72":"markdown","152ee632":"markdown","79989c4d":"markdown","e3e73b0f":"markdown","633a48cd":"markdown","3b6ab103":"markdown","64d18c2e":"markdown","e4c14534":"markdown","f2c8d486":"markdown","bb258f21":"markdown"},"source":{"364cf77b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split","ce945639":"data = pd.read_csv('..\/input\/framingham-heart-study-dataset\/framingham.csv')","b102e2e3":"data.head()","c79c2fb9":"data.info()","a08906d1":"data.describe()","16abeb9f":"# education feature is not required as its not predicting the Ten Year CHD\n# target is Ten Year CHD (0 or 1)\ndata.drop('education', axis=1, inplace=True)","f9330f54":"# renaming TenYearCHD to CHD\ndata.rename(columns={\"TenYearCHD\": \"CHD\"}, inplace=True)","60746708":"data.head()","3fa9361e":"X_train, X_test, y_train, y_test = train_test_split(data.iloc[:,:-1], data.iloc[:,-1], test_size=0.14, random_state=0)\n\ntrain_data = pd.concat([X_train, y_train], axis=1)\ntest_data = pd.concat([X_test, y_test], axis=1)","d11dfe5f":"# age vs CHD\nplt.figure(figsize=(10,10))\nsns.swarmplot(x='CHD', y='age', data=train_data)","f8b21692":"plt.figure(figsize=(10,10))\nsns.violinplot(x='CHD', y='age', data=train_data)","ecc6b8fd":"# age vs CHD for smokers or non-smoker\nplt.figure(figsize=(10,10))\nsns.swarmplot(x='CHD', y='age', data=train_data, hue='currentSmoker')","dcbe5906":"plt.figure(figsize=(10,10))\nsns.violinplot(x='CHD', y='age', data=train_data, hue='currentSmoker', split=True)","fa11ae03":"# male and female countplot\nsns.countplot(x=train_data['male'])","676e63a2":"# male and female having disease or not\nsns.countplot(x=train_data['male'], hue=train_data['CHD'])","80b1ee25":"train_data.iloc[:,:5]","a050d55d":"# To understand correlation between some features, pairplot is used\nplt.figure(figsize=(20,15))\nsns.pairplot(train_data.loc[:,'totChol': 'glucose'])","91fb5275":"plt.figure(figsize=(15,15))\nsns.heatmap(train_data.corr(), annot=True, linewidths=0.1)","bfa7a58a":"# dropping features which are highly correlated\nfeatures_to_drop = ['currentSmoker', 'diaBP']\n\ntrain_data.drop(features_to_drop, axis=1, inplace=True)","c5665a3f":"train_data.head()","304ca7bf":"missing_values_count = train_data.isnull().sum()\nmissing_values_count = missing_values_count[missing_values_count > 0]\nmissing_values_percent = (missing_values_count * 100) \/ (train_data.shape[0])\n\nprint(max(missing_values_percent))","77d4330a":"print(missing_values_count)","a17e5f33":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='most_frequent')","1435a503":"new_train_data = pd.DataFrame(imputer.fit_transform(train_data))\nnew_train_data.columns = train_data.columns\nnew_train_data.index = train_data.index","1a882a81":"train_data.isnull().sum()","0f0cbb08":"new_train_data.isnull().sum()","ed88d76a":"new_train_data.head()","245cbba1":"train_data = new_train_data.copy()","472f2203":"fig, ax = plt.subplots(figsize=(10,10), nrows=3, ncols=4)\nax = ax.flatten()\n\ni = 0\nfor k,v in train_data.items():\n    sns.boxplot(y=v, ax=ax[i])\n    i+=1\n    if i==12:\n        break\nplt.tight_layout(pad=1.25, h_pad=0.8, w_pad=0.8)","6bdcc349":"# Outliers handling\nprint('Number of training examples to be deleted for outliers removal is ',len(train_data[train_data['sysBP'] > 220]) + len(train_data[train_data['BMI'] > 43]) + len(\n    train_data[train_data['heartRate'] > 125]) + len(train_data[train_data['glucose'] > 200]) + len(\n    train_data[train_data['totChol'] > 450]))","4c61d6a2":"# deleting outliers\n\ntrain_data = train_data[~(train_data['sysBP'] > 220)]\ntrain_data = train_data[~(train_data['BMI'] > 43)]\ntrain_data = train_data[~(train_data['heartRate'] > 125)]\ntrain_data = train_data[~(train_data['glucose'] > 200)]\ntrain_data = train_data[~(train_data['totChol'] > 450)]\nprint(train_data.shape)","e6b84808":"# fig, ax = plt.subplots(figsize=(10,10), nrows=3, ncols=4)\n# ax = ax.flatten()\n\n# i = 0\n# for k,v in train_data.items():\n#     sns.distplot(v, ax=ax[i])\n#     i+=1\n#     if i==12:\n#         break\n# plt.tight_layout(pad=1.25, h_pad=0.8, w_pad=0.8)","205bb921":"# Standardise some features\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncols_to_standardise = ['age','totChol','sysBP','BMI', 'heartRate', 'glucose', 'cigsPerDay']\ntrain_data[cols_to_standardise] = scaler.fit_transform(train_data[cols_to_standardise])","a6b9a899":"train_data.head()","bb4a2937":"# dropping unwanted features as done in train data\ntest_data.drop(features_to_drop, axis=1, inplace=True)\n\n# imputing missing values if any\nimputer = SimpleImputer(strategy='most_frequent')\nnew_test_data = pd.DataFrame(imputer.fit_transform(test_data))\nnew_test_data.columns = test_data.columns\nnew_test_data.index = test_data.index\n\ntest_data = new_test_data.copy()","d1a4d3ed":"# Standardising features\nscaler = StandardScaler()\ntest_data[cols_to_standardise] = scaler.fit_transform(test_data[cols_to_standardise])","33438210":"test_data.head()","a35d7265":"from sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()","8feb80e1":"X_train = train_data.loc[:,train_data.columns != 'CHD']\ny_train = train_data.loc[:,'CHD']\nX_test = test_data.loc[:, test_data.columns !='CHD']\ny_test = test_data.loc[:, 'CHD']","2878477b":"log_reg.fit(X_train, y_train)\ny_pred_log = log_reg.predict(X_test)","26dbb104":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report","f32fe7b7":"log_reg_accuracy = accuracy_score(y_pred_log, y_test) * 100\nprint('Accuracy Score for logistic regression is %f'%log_reg_accuracy)","9ef3a8dd":"log_train_score = log_reg.score(X_train, y_train) * 100\nprint('Train score for Logistic Regression is %f'%log_train_score)","ea0a4ff1":"print('Difference between train and test score for Logistic Regression is %f'%(log_train_score - log_reg_accuracy))","6556cbf5":"confusion_matrix(y_pred_log, y_test)","3cc5f21e":"print(classification_report(y_pred_log, y_test))","ae9aeae7":"from sklearn.tree import DecisionTreeClassifier","855baf72":"dt_clf = DecisionTreeClassifier(min_samples_split=40, random_state=0) \n# that fraction of samples(if float) or that many number(if int) of samples is atleast present in the node \n# before splitting, then only split that node\n\n# for min_samples_split as 180 I got a better accuracy and train score and difference was less\n# but f1 score was very bad for positive class\n# and setting min_samples_split as 40, we got good results for all metrics","ae173a18":"dt_clf.fit(X_train, y_train)\ny_pred_dt = dt_clf.predict(X_test)","48461534":"dt_accuracy = accuracy_score(y_pred_dt, y_test)*100\nprint('Accuracy score for Decision tree is %f'%dt_accuracy)","95cb38fe":"dt_train_score = dt_clf.score(X_train, y_train)*100\nprint('Train score for Decision tree is %f'%dt_train_score)","75d179b8":"print('Difference between train and test scores for Decision tree is : %f'%(dt_train_score - dt_accuracy))","e7759465":"confusion_matrix(y_pred_dt, y_test)","53e3f78c":"print(classification_report(y_pred_dt, y_test))","f65abc43":"# Exporting the tree in text format\nfrom sklearn.tree import export_text\ndt_text_format = export_text(dt_clf, feature_names=list(train_data.columns[:12]))\nprint('Decision tree in text format : \\n%s'%dt_text_format)","0eaba117":"from sklearn.ensemble import RandomForestClassifier","bdbf44a1":"rf_clf = RandomForestClassifier(n_estimators = 150,min_samples_split=10,random_state=0)","cd134d68":"rf_clf.fit(X_train, y_train)\ny_pred_rf = rf_clf.predict(X_test)","234baad5":"rf_accuracy = accuracy_score(y_pred_rf, y_test)*100\nprint('Accuracy score for Random Forest is %f'%rf_accuracy)","f56a6e9b":"rf_train_score = rf_clf.score(X_train, y_train)*100\nprint('Train score for Random Forest is %f'%rf_train_score)","d4c47196":"print('Difference between train and test scores for Random Forest is : %f'%(rf_train_score - rf_accuracy))","545e8aa2":"confusion_matrix(y_pred_rf, y_test)","4e225156":"print(classification_report(y_pred_rf, y_test))","083efc8b":"pd.DataFrame(y_pred_dt).to_csv('testPredictions.csv', index=False)","22b740d4":"Violinplot tells that most patients of age around 40-55 have 0 risk","d7917b46":"# Model Fitting","792d9a58":"Maximum missing percentage is 9% approx so imputation will be done","f073d435":"# Decision Tree Classifier","b43eda47":"Source of Dataset : https:\/\/www.kaggle.com\/amanajmera1\/framingham-heart-study-dataset\n\nAuthor Name : Gourav Khator\n\nGoal : Study Heart Disease Dataset and predict the 10-year CHD\n\nHope you would like my work!!","2d29c167":"# Recommended Model","f1adcb8e":"# EDA","78bdedd4":"# Random Forest Classifier","6f44e82f":"# Normalisation Checking","460c2581":"From pairplot and heatmap we see that sysBP and diaBP are highly correlated\n\nAnd currentSmoker and cigsPerDay are highly correlated","29807313":"# Missing Values Checking\n\nMissing values can be done before EDA or after EDA. But before EDA, it will impute or drop missing values for all features, whether some features are needed or not\n\nAnd after EDA, we choose the features which are needed and those features only get imputed.\n\nAlso, the steps best for model preparation is : EDA -> Preprocessing (Missing values, Outliers, Normalise etc.) -> Model Fitting and Prediction","09a63f72":"Most patients of age around 60-65 have risk of disease (CHD)","152ee632":"# Outliers Checking","79989c4d":"## Test Data Preprocessing Similar to Train Data","e3e73b0f":"From this violinplot, we see that most of smokers having no risk of CHD are in age around 40 years\n\nBut most of non-smokers having risk are in age around 65-70 years\nAlso most smokers having risk are in age around 50 years","633a48cd":"Here from the above countplot, we see that most data are females\n\nThere are more females having no risk than males having no risk\n\nThere are slightly more males having risk than females having risk","3b6ab103":"Conclusion of Boxplot : \n\nOutliers found in features named ['totChol', 'sysBP', 'BMI','heartRate', 'glucose']","64d18c2e":"# Conclusion","e4c14534":"Logistic Regression model has least difference in train and test scores and has accuracy of 85% approx\nSo, it does not overfit the data. The F1-score is not good for positive class\n\nSo, Decision Tree Classifier was taken and we saw for min_samples_split as 40, we got almost good accuracy as logistic regression but better f1-score than logistic regression.\n\nWe also tried Random Forest Classifier with n_estimators as 150 and min_samples_split as 10. We got f1-score as the worst in all three models.\n\nThe hyperparameters were tested and tuned repeatedly to get better accuracy with better f1-score.\n\nThus, Decision Tree model is recommended for our dataset.","f2c8d486":"## Logistic Regression","bb258f21":"## Decision Tree Classifier with hyper-parameters (min_samples_split = 40)"}}