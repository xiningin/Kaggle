{"cell_type":{"185a8572":"code","daba8ad7":"code","d3b471f8":"code","80343f46":"code","4b24e991":"code","4b059fd1":"code","3eae77ef":"code","0e21ac20":"code","e7331526":"code","f4c79d83":"code","29541610":"code","90cee45d":"code","ddda2bd7":"code","a640f098":"code","0b73d2f9":"code","5873f540":"code","8dd7dcc7":"code","188c2e86":"code","1528ef6f":"code","10d0b72a":"code","6f3a9391":"code","250a2e33":"code","9f2bea04":"code","874594c6":"code","f8c17570":"code","663a61c9":"code","60a73e96":"code","c17e8c18":"code","2b0ec1cb":"code","ec6f5d33":"code","5a205803":"code","60d9400f":"code","b0a44cb5":"code","25434187":"code","debb8334":"code","e39cf460":"code","b65319d6":"code","382a438e":"code","ad894ea8":"code","0cbcc3ae":"code","27719a09":"code","7a694291":"code","b7ba1b13":"code","24f7fda9":"code","4c4d6661":"code","9e9bfca4":"markdown","348079f6":"markdown","a8fd09ad":"markdown","b9a150cd":"markdown","56cebf95":"markdown","2e913bb4":"markdown","d5966f1d":"markdown","53f622d3":"markdown","8d2417a9":"markdown","3606ea4c":"markdown","e48af390":"markdown","f5efd034":"markdown","802b2d5a":"markdown","f27f1725":"markdown","c4a40696":"markdown","901294d1":"markdown","2cd16e6e":"markdown","1f7e2e74":"markdown"},"source":{"185a8572":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge,RidgeCV, LinearRegression\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler, MaxAbsScaler , FunctionTransformer\nimport scipy\nimport gc\nimport re\npd.options.display.max_colwidth=300\npd.options.display.min_rows=300\n","daba8ad7":"df = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\nprint(df.shape)\n\n# Give more weight to severe toxic \ndf['severe_toxic'] = df.severe_toxic * 3\ndf['threat'] = df.threat * 2\n\ndf['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) ).astype(int)\ndf = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\ndf.sample(5)","d3b471f8":"df['y'].value_counts()","80343f46":"df = pd.concat([df[df.y>0] , \n                df[df.y==0].sample(int(len(df[df.y>0])*1.5)) ], axis=0).sample(frac=1)\n\nprint(df.shape)","4b24e991":"df['y'].value_counts()","4b059fd1":"def clean(data, col):\n\n    # Clean some punctutations\n    data[col] = data[col].str.replace('\\n', ' ')\n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([\/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3')\n    # Replace repeating characters\n    data[col] = data[col].str.replace(r'(\")\\1+',r'\\1')    \n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1+\\B',r'\\1\\1')    \n    data[col] = data[col].str.replace(r'(\\w)\\1\\1+\\B',r'\\1\\1')    \n    data[col] = data[col].str.replace(r'(\\w)\\1+\\b',r'\\1').str.strip()\n    \n    return data\n","3eae77ef":"#df.text.str.extractall(r'([a-zA-Z]+[\/!?.][a-zA-Z]+)')\n#df.text.str.extractall(r'([^\\w ]{3,})')[[0]].value_counts()#.reset_index()#[:20]","0e21ac20":"print(re.sub(r'(\")\\1+',r'\\1', 'gooo\"\"\"\"\"od \"\"\"\" \"\" \" brooo goodoo'))\n\nprint(re.sub(r'(\\w)\\1+\\b',r'\\1', 'gooood brooo goodoo'))\n\nprint(re.sub(r'([*!?\\'])\\1+\\B',r'\\1\\1', \"g*******d  g**d br!!!!! g''''oodoo\"))\n\nprint(re.sub(r'([a-zA-Z]+)([\/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3', 'gooood\/brooo 4.3 df'))","e7331526":"def get_text_length(x):\n    return np.array([len(t) for t in x]).reshape(-1, 1)\n","f4c79d83":"features = FeatureUnion(\n[\n        (\"vect1\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))),\n        (\"vect2\", TfidfVectorizer(min_df= 2, max_df=0.5, analyzer = 'word', token_pattern=r'(?u)\\b\\w{6,}\\b')),\n        #('count', FunctionTransformer(get_text_length, validate=False)),\n    \n])\n\npipeline = Pipeline(\n    [\n        (\"vect\", features),\n        #('scale', MaxAbsScaler()),\n        #(\"clf\", RandomForestRegressor(n_estimators = 5, min_sample_leaf=3)),\n        (\"clf\", Ridge(alpha=1 )),\n    ]\n)","29541610":"# Train the pipeline\ndf = clean(df, 'text')\n\npipeline.fit(df['text'], df['y'])","90cee45d":"# What are the important features for toxicity\n\nprint('Total number of features:', len(pipeline['vect'].get_feature_names()) )\n\nfeature_wts = sorted(list(zip(pipeline['vect'].get_feature_names(), \n                              np.round(pipeline['clf'].coef_,2) )), \n                     key = lambda x:x[1], \n                     reverse=True)\n\nfeature_wts[:50]","ddda2bd7":"# Extract top features from Ridge model \n\nf1_lst = [x.replace('vect1__','') for x,y in feature_wts[:2000] if (x.startswith('vect1__')) & (len(x.replace('vect1__','').strip())>1)]\nf2_lst = [x.replace('vect2__','') for x,y in feature_wts[:2000] if (x.startswith('vect2__'))]\nprint(len(f1_lst),len(f2_lst))\nprint(f2_lst[:10])","a640f098":"features1b = FeatureUnion(\n[\n        (\"vect1\", TfidfVectorizer(analyzer = 'char_wb', vocabulary = f1_lst, ngram_range = (3,5))),\n        (\"vect2\", TfidfVectorizer(analyzer = 'word', token_pattern=r'(?u)\\b\\w{6,}\\b', vocabulary = f2_lst)),\n        #('count', FunctionTransformer(get_text_length, validate=False)),\n    \n])\n\npipeline1b = Pipeline(\n    [\n        (\"vect\", features1b),\n        #('scale', MaxAbsScaler()),\n        (\"clf\", RandomForestRegressor(n_estimators = 50, min_samples_leaf = 3)),\n        #(\"clf\", RidgeCV(alphas=(2,1), cv = 3 )),\n    ]\n)","0b73d2f9":"pipeline1b.fit(df['text'], df['y'])","5873f540":"print(len(pipeline1b['vect'].get_feature_names()))","8dd7dcc7":"#df_pred=pipeline.predict(df.text)\n# df['pred'] = df_pred\n# df['diff'] = np.abs(df['pred'] - df['y'])\n# df.sort_values('diff',ascending=False).head(30)\n\nfeature_wts1b = sorted(list(zip(pipeline['vect'].get_feature_names(), \n                              np.round(pipeline1b['clf'].feature_importances_,2) )), \n                     key = lambda x:x[1], \n                     reverse=True)\n\nfeature_wts1b[:40]\n","188c2e86":"\ndel df, feature_wts, feature_wts1b\ngc.collect()\n","1528ef6f":"# df2 = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv\")\n# print(df2.shape)\n\n# df2['y'] = df2[[ 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack']].sum(axis=1)\n# df2 = df2[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\n# df2.sample(5)","10d0b72a":"#df2.y.value_counts()","6f3a9391":"# df2 = pd.concat([df2[df2.y>0] , \n#                  df2[df2.y==0].sample(int(len(df2[df2.y>0])*0.5)) ], axis=0)\\\n#     .sample(frac=0.2)\n\n# print(df2.shape)","250a2e33":"# features2 = FeatureUnion(\n# [\n#         (\"vect1\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))),\n#         (\"vect2\", TfidfVectorizer(min_df= 2, max_df=0.5, analyzer = 'word', token_pattern=r'(?u)\\b\\w{6,}\\b')),\n#         #('count', FunctionTransformer(get_text_length, validate=False)),\n     \n# ])\n\n# pipeline2 = Pipeline(\n#     [\n#         (\"vect\", features2),\n#         (\"sca\", MaxAbsScaler()),\n#         #(\"vect\", TfidfVectorizer(min_df= 5, max_df=0.3, analyzer = 'char_wb', ngram_range = (4,6))),\n#         #(\"clf\", RandomForestRegressor(n_estimators = 5, min_sample_leaf=3)),\n#         #(\"clf\", Ridge(alpha=1)),\n#         (\"clf\", RidgeCV(alphas=(1,3), cv = 3 )),\n#         #(\"clf\",LinearRegression())\n#     ]\n# )","9f2bea04":"# # Train the pipeline\n# df2 = clean(df2, 'text')\n\n# pipeline2.fit(df2['text'], df2['y'])","874594c6":"# # What are the important features for toxicity\n\n# feature_wts2 = sorted(list(zip(pipeline2['vect'].get_feature_names(), \n#                                np.round(pipeline2['clf'].coef_,2) )), \n#                       key = lambda x:x[1], \n#                       reverse=True)\n\n# feature_wts2[:50]","f8c17570":"\n# del df2, feature_wts2\n# gc.collect()\n","663a61c9":"df_val = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\nprint(df_val.shape)\n\ndf_val = clean(df_val, 'less_toxic')\ndf_val = clean(df_val, 'more_toxic')\n","60a73e96":"\np1 = pipeline.predict(df_val['less_toxic'])\np2 = pipeline.predict(df_val['more_toxic'])\n\nf'Validation Accuracy from Model 1 is { np.round((p1 < p2).mean() * 100,2)}'","c17e8c18":"p3 = pipeline1b.predict(df_val['less_toxic'])\np4 = pipeline1b.predict(df_val['more_toxic'])\n\nf'Validation Accuracy from Model 2 is { np.round((p3 < p4).mean() * 100,2)}'","2b0ec1cb":"scale1 = StandardScaler()\nscale2 = StandardScaler()\n\nscale1.fit(np.hstack([p1,p2]).reshape(-1,1))\nscale2.fit(np.hstack([p3,p4]).reshape(-1,1))\n","ec6f5d33":"p5 = scale1.transform(p1.reshape(-1,1)) + scale2.transform(p3.reshape(-1,1))\np6 = scale1.transform(p2.reshape(-1,1)) + scale2.transform(p4.reshape(-1,1))\n\nf'Validation Accuracy from Model 2 is { np.round((p5 < p6).mean() * 100,2)}'","5a205803":"df_val['p1'] = p5\ndf_val['p2'] = p6\ndf_val['diff'] = np.abs(p6 - p5)\n\ndf_val['correct'] = (p5 < p6).astype('int')\n","60d9400f":"# Comparing prediction differences between less toxic and more toxic sentences\n\ndf_val = df_val.assign(diff_grp = lambda x: np.ceil(df_val['diff']),\n              s1_grp = lambda x: np.ceil(df_val['p1']),\n             )\n\ndisplay(\n    df_val[df_val.correct == 0]\\\n    .groupby(['s1_grp']).size().reset_index()\\\n    .set_axis(['s1_grp','cnt'],axis='columns')\\\n    .sort_values('cnt', ascending=False)[:15]\n)\ndisplay(\n    df_val[df_val.correct == 0]\\\n    .groupby(['diff_grp']).size().reset_index()\\\n    .set_axis(['diff_grp','cnt'],axis='columns')\\\n    .sort_values('cnt', ascending=False)[:15]\n)\n\ndisplay(\n    df_val[df_val.correct == 0]\\\n    .groupby(['s1_grp','diff_grp']).size().reset_index()\\\n    .set_axis(['s1_grp','diff_grp','cnt'],axis='columns')\\\n    .sort_values('cnt', ascending=False)[:15]\n)","b0a44cb5":"\n### Incorrect predictions with similar scores\n\ndf_val[df_val.correct == 0].sort_values('diff', ascending=True).head(20)","25434187":"### Incorrect predictions with dis-similar scores\n\ndf_val[df_val.correct == 0].sort_values('diff', ascending=False).head(20)","debb8334":"# #df_val[df_val.correct == 0].sort_values('diff', ascending=False).head(20).more_toxic.tolist()\n# comm=df_val.more_toxic[29057]\n# print(comm)\n# print(vect_an(comm))\n# [v for v in vect_an(comm) if (v not in vocab) & (v.strip() not in obj.stop_words_)]\n# vect_an(comm)\n","e39cf460":"# Load TFIDF vectorizer\nobj = pipeline[\"vect\"].transformer_list[0][1]; print(obj)\nvect_an = obj.build_analyzer()\nvocab = obj.vocabulary_\nprint(len(vocab))\n\nobj2 = pipeline[\"vect\"].transformer_list[1][1]; print(obj)\nvect_an2 = obj2.build_analyzer()\nvocab2 = obj2.vocabulary_\n\ntmp=[]\nfor comm in df_val[df_val.correct == 0].sort_values('diff', ascending=False).head(20).more_toxic.tolist():\n    tmp.append((comm, vect_an(comm), vect_an2(comm)))\npd.DataFrame(tmp, columns = [\"comment\", \"tokenized1\", \"tokenized2\"])","b65319d6":"df_sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\n","382a438e":"# Predict using pipeline\ndf_sub = clean(df_sub, 'text')\n\nm1_preds = pipeline.predict(df_sub['text'])\nm2_preds = pipeline1b.predict(df_sub['text'])\n\ndf_sub['score'] = scale1.transform(m1_preds.reshape(-1,1)) + scale2.transform(m2_preds.reshape(-1,1))","ad894ea8":"# Cases with duplicates scores\n\ndf_sub['score'].count() - df_sub['score'].nunique()","0cbcc3ae":"df_sub['score'].value_counts().reset_index()[:10]","27719a09":"# Rank the predictions \n\ndf_sub['score']  = scipy.stats.rankdata(df_sub['score'], method='ordinal')\n\nprint(df_sub['score'].rank().nunique())","7a694291":"df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","b7ba1b13":"test = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')\ntest['score'] = df_sub.score.tolist()","24f7fda9":"test.sort_values('score',ascending=False).head(20)","4c4d6661":"test.sort_values('score',ascending=True).head(20)","9e9bfca4":"#### Some of these just look incorrectly tagged \n","348079f6":"#### Built on top of the amazing notebook here : \nhttps:\/\/www.kaggle.com\/julian3833\/jigsaw-incredibly-simple-naive-bayes-0-768\n","a8fd09ad":"# Imports","b9a150cd":"## Model1 + Model2\n### Start with scaling the predictions first before combining","56cebf95":"# Training data \n\n## Convert the label to SUM of all toxic labels (This might help with maintaining toxicity order of comments)","2e913bb4":"## Reduce the rows with 0 toxicity ","d5966f1d":"## Correct the rank ordering","53f622d3":"## Text cleaning ","8d2417a9":"## Analyze the vocabulary of misclassified sentences ","3606ea4c":"## Analyze bad predictions \n### Incorrect predictions with similar scores\n### Incorrect predictions with different scores","e48af390":"### Where does most misclassification happen\n\n#### Most come from similar predictions from less toxic comments","f5efd034":"# Create Sklearn Pipeline with \n## TFIDF - Take 'char_wb' as analyzer to capture subwords well\n## Ridge - Ridge is a simple regression algorithm that will reduce overfitting ","802b2d5a":"## Create model 2 from Unintended Bias competition ","f27f1725":"# 0.81+ score by simple TF-Idf and Ridge regression\n\n## Built as ensemble of 2 models using data from past 2 Jigsaw competitions \n\n### Analysis of bad predictions for additional insights\n","c4a40696":"### Model 2","901294d1":"### Model 1","2cd16e6e":"# Predict on test data ","1f7e2e74":"# Validate the pipeline "}}