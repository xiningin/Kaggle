{"cell_type":{"71b4ec39":"code","7f207e2d":"code","72ea17fb":"code","e05f135e":"code","164d40d0":"code","7208eae4":"code","d5763841":"code","ef510c02":"code","2f6527c0":"code","740867a8":"code","2587697f":"code","e0252887":"code","58ff3f35":"code","f53633ff":"code","ddf458de":"code","c6a7acb6":"code","c1d04d31":"code","bec8166a":"code","975bc1c0":"code","4300355b":"code","a59c83fd":"code","8f6d2ee5":"code","cc1fcb1f":"code","77a98134":"code","a7351725":"code","cea4f6d2":"code","54a86e60":"code","b1ecb963":"code","0a98a561":"code","41f94122":"code","53d90f88":"markdown","f3a3ba3a":"markdown","6657f479":"markdown","a2f53e96":"markdown","34bb2c78":"markdown","ef70f3d5":"markdown","cd0b254f":"markdown","91dd28a2":"markdown"},"source":{"71b4ec39":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nprint(\"Load Packages\")\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom fastai.tabular import *  #fast.ai tabular models\nimport os, gc, pickle, copy, datetime, warnings\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nprint(\"Print Directories\")\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7f207e2d":"# Any results you write to the current directory are saved as output.\n#read in training data, outcomes and testing  \nprint(\"load train, test and submission\")\ntrain = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-2\/train.csv') \ntest = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-2\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-2\/submission.csv')\nprint(train.shape)\nprint(test.shape)\nprint(submission.shape)","72ea17fb":"#read in population data \npop = pd.read_csv('\/kaggle\/input\/world-population-by-age-group-2020\/WorldPopulationByAge2020.csv')\npop.head()","e05f135e":"#spread the age group column \npop = pop.pivot(index='Location',columns='AgeGrp',values=['PopMale', 'PopFemale','PopTotal'])\npop.head()\n#pop_test.shape","164d40d0":"#flatten the multi - index \npop.columns = pop.columns.get_level_values(0)+pop.columns.get_level_values(1)\n#pop_test.head()\npop['Location_column'] = pop.index\npop.head()\n","7208eae4":"#rename united states or other variables to improve match \npop=pop.replace(\"United States of America\", \"US\")\npop[pop[\"Location_column\"]==\"US\"]","d5763841":"#combine the population file with the train and test files \n#train_ex = train\n#pop_test = pop \n\ntrain = pd.merge(train, pop, how=\"left\",left_on='Country_Region', right_on='Location_column')\ntest = pd.merge(test, pop, how=\"left\",left_on='Country_Region', right_on='Location_column')\ntest.head()","ef510c02":"train = train.rename(columns={'ConfirmedCases': 'ConfirmedCases_old', 'Fatalities': 'Fatalities_old'})\ntrain.head()","2f6527c0":"test.head()","740867a8":"submission.head()","2587697f":"#separate out the first date available to both train and test \n#that is jan 22 2020 for train and march 12 2020 for test \n#first_day_train = train[train.Date == '2020-01-22']\n#first_day_test = train[train.Date == '2020-03-12']\n\n#drop (keep needed) unneeded variables in both datasets \n#first_day_train=pd.DataFrame(first_day_train,columns=[\"Province\/State\",\"Country\/Region\",\"ConfirmedCases\",\"Fatalities\"])\n#first_day_test=pd.DataFrame(first_day_test,columns=[\"Province\/State\",\"Country\/Region\",\"ConfirmedCases\",\"Fatalities\"])\n\n#change names to first day confirmed and first day fatalities \n#first_day_train.rename(columns={'ConfirmedCases': 'FirstDayConfirmed', 'Fatalities': 'FirstDayFatalities'}, inplace=True)\n#first_day_test.rename(columns={'ConfirmedCases': 'FirstDayConfirmed', 'Fatalities': 'FirstDayFatalities'}, inplace=True)\n\n#merge both datasets to add this new variable \n#train = pd.merge(train, first_day_train, on=['Province\/State', 'Country\/Region'])\n#test = pd.merge(test, first_day_test, on=['Province\/State', 'Country\/Region'])\n\n#train.head()\n\n#train[train[\"Location_column\"]==\"US\"]","e0252887":"#investigate missing \ntrain.isnull().sum()","58ff3f35":"#investigate missing \ntest.isnull().sum()","f53633ff":"#Potentially sort the training database and prepare to take a new type of validation data set\nprint(\"sort the train file\")\nmake_date(train, 'Date')\nmake_date(test, 'Date')","ddf458de":"print(\"delete columns that might not be useful\")\n#train=train.drop(['Id', 'Province\/State', 'Country\/Region', 'Lat', 'Long', 'Date','ConfirmedCases', 'Fatalities'],axis=1)\n#test=test.drop(['ForecastId', 'Province\/State', 'Country\/Region', 'Lat', 'Long','Date'],axis=1)\n\n#create date variables in train and test \nprint(\"create time variables in both train and test\")\ntrain_data = add_datepart(train, 'Date',drop=False)\ntest_data = add_datepart(test, 'Date',drop=False)\n\n#add fatalities to test\ntest_data['Fatalities_old'] = 0\ntest_data['ConfirmedCases_old'] = 0\n\n#procedures for cleaning data \nprint(\"set the procedures for cleaning\")\nprocs = [FillMissing, Categorify, Normalize]","c6a7acb6":"#impute new additions with median \nPopMale19_median = train['PopMale0-19'].median()\ntrain['PopMale0-19'].fillna(PopMale19_median,inplace=True)\ntest['PopMale0-19'].fillna(PopMale19_median,inplace=True)\n\nPopMale39_median = train['PopMale20-39'].median()\ntrain['PopMale20-39'].fillna(PopMale39_median,inplace=True)\ntest['PopMale20-39'].fillna(PopMale39_median,inplace=True)\n\nPopMale59_median = train['PopMale40-59'].median()\ntrain['PopMale40-59'].fillna(PopMale59_median,inplace=True)\ntest['PopMale40-59'].fillna(PopMale59_median,inplace=True)\n\nPopMale60_median = train['PopMale60+'].median()\ntrain['PopMale60+'].fillna(PopMale60_median,inplace=True)\ntest['PopMale60+'].fillna(PopMale60_median,inplace=True)\n\nPopFemale19_median = train['PopFemale0-19'].median()\ntrain['PopFemale0-19'].fillna(PopFemale19_median,inplace=True)\ntest['PopFemale0-19'].fillna(PopFemale19_median,inplace=True)\n\nPopFemale39_median = train['PopFemale20-39'].median()\ntrain['PopFemale20-39'].fillna(PopFemale39_median,inplace=True)\ntest['PopFemale20-39'].fillna(PopFemale39_median,inplace=True)\n\nPopFemale59_median = train['PopFemale40-59'].median()\ntrain['PopFemale40-59'].fillna(PopFemale59_median,inplace=True)\ntest['PopFemale40-59'].fillna(PopFemale59_median,inplace=True)\n\nPopFemale60_median = train['PopFemale60+'].median()\ntrain['PopFemale60+'].fillna(PopFemale60_median,inplace=True)\ntest['PopFemale60+'].fillna(PopFemale60_median,inplace=True)\n\nPopTotal19_median = train['PopTotal0-19'].median()\ntrain['PopTotal0-19'].fillna(PopTotal19_median,inplace=True)\ntest['PopTotal0-19'].fillna(PopTotal19_median,inplace=True)\n\nPopTotal39_median = train['PopTotal20-39'].median()\ntrain['PopTotal20-39'].fillna(PopTotal39_median,inplace=True)\ntest['PopTotal20-39'].fillna(PopTotal39_median,inplace=True)\n\nPopTotal59_median = train['PopTotal40-59'].median()\ntrain['PopTotal40-59'].fillna(PopTotal59_median,inplace=True)\ntest['PopTotal40-59'].fillna(PopTotal59_median,inplace=True)\n\nPopTotal60_median = train['PopTotal60+'].median()\ntrain['PopTotal60+'].fillna(PopTotal60_median,inplace=True)\ntest['PopTotal60+'].fillna(PopTotal60_median,inplace=True)\n\n\n\ntrain.isnull().sum()","c1d04d31":"#set missing regions to the countries if necessary to fill blanks \n#train.shape\n#train[train.Province_State.isnull()][\"Province_State\"]=train[\"Country_Region\"]\n#train.head()\n#values = {'Province_State': \"Blank\", 'Location_column': \"Blank\"}\n#train=train.fillna(value=values,inplace=True)\n#test=test.fillna(value=values)\ntrain.loc[train['Province_State'].isnull(), 'Province_State'] = \"WholeCountry\"\ntrain.loc[train['Location_column'].isnull(), 'Location_column'] = \"WholeCountry\"\ntest.loc[test['Province_State'].isnull(), 'Province_State'] = \"WholeCountry\"\ntest.loc[test['Location_column'].isnull(), 'Location_column'] = \"WholeCountry\"\n\ntest.isnull().sum()","bec8166a":"train['place'] = train['Province_State']+'_'+train['Country_Region']\ntest['place'] = test['Province_State']+'_'+test['Country_Region']\ntrain.place","975bc1c0":"#examine data for train\ntrain_data.dtypes\n","4300355b":"#examine data for test \ntest_data.dtypes","a59c83fd":"#sort the data \nprint(\"Sort the training data set for validation\")\ntrain_data = train_data.sort_values(by=['place','Date'], ascending=True)\ntrain_data = train_data.reset_index(drop=True)","8f6d2ee5":"train_data.tail(5)","cc1fcb1f":"##\n#fastLearner\n#takes a train and test dataframe object and outputs the test file with predictions\n#input: train and test pandas dataframe objects, size of validation set as numeric, \n#and the dep variable name \n#output: pandas dataframe object test with predictions \n##\ndef fastLearning(df1,df2,size,dep,databunch=25,initial_cycle=100,next_cycle=100,wd_size1=1e-1,wd_size2=1e-1):\n    #instantiate variables  \n    train_data = df1\n    test_data =df2\n    val_size = size \n    path = ''\n    deep_var=dep\n    db_size=databunch\n    learn_cycle=initial_cycle\n    one_cycle=next_cycle\n    wd_decay1=wd_size1\n    wd_decay2=wd_size2\n    \n    #model parameters \n    dep_var = deep_var\n    \n    cat_names = ['Province_State', 'Country_Region','Is_month_end',\n             'Is_month_start','Is_quarter_end','Is_quarter_start','Is_year_end']\n    \n    cont_names = ['Year', \n              'Month', 'Week', \n              'Day', 'Dayofweek', \n              'Dayofyear', 'Elapsed','PopMale0-19',\n              'PopMale20-39','PopMale40-59','PopMale60+',\n              'PopFemale0-19','PopFemale20-39','PopFemale40-59',\n              'PopFemale60+','PopTotal0-19','PopTotal20-39',\n              'PopTotal40-59','PopTotal60+']\n    \n    #Start index for creating a validation set from train\n    start_indx = len(train_data) - int(len(train_data) * val_size)\n\n    #End index for creating a validation set from train\n    end_indx = len(train_data)\n    \n    #TabularList for Validation\n    #val = (TabularList.from_df(train_data.iloc[start_indx:end_indx].copy(), path=path, cat_names=cat_names, cont_names=cont_names))\n    test = (TabularList.from_df(test_data, path=path, cat_names=cat_names, cont_names=cont_names, procs=procs))\n    data = (TabularList.from_df(train_data, path=path, cat_names=cat_names, cont_names=cont_names, procs=procs)\n                           .split_by_idx(list(range(start_indx,end_indx)))\n                           .label_from_df(cols=dep_var)\n                           .add_test(test)\n                           .databunch(bs=db_size))\n    #create learner \n    #learn = tabular_learner(data, layers=[15,10], wd=wd_decay,ps=[0.001,0.01], \n    #                        emb_drop=0.04,metrics= [rmse])\n    learn = tabular_learner(data, layers=[300,200], wd=wd_decay1,metrics= [rmse])\n    \n    #Exploring the learning rates\n    #learn.lr_find(start_lr = 1e-03,end_lr = 1e+02, num_it = 100)\n    #learn.lr_find()\n    #learn.recorder.plot()\n    \n    #Fitting data and training the network\n    learn.fit_one_cycle(learn_cycle,wd=wd_decay1)\n\n    #save stage 1 learning \n    learn.save('stage-1')\n\n    #unfreeze the learner\n    learn.unfreeze()\n\n    #Fitting data and training the network\n    learn.fit_one_cycle(one_cycle,wd=wd_size2)\n\n    #apply learning model to test \n    #print(\"#apply learning model to test \")\n    test_predictions = learn.get_preds(ds_type=DatasetType.Test)[0]\n\n    #Converting the tensor output to a list of predicted values\n    #print(\"Converting the tensor output to a list of predicted values\")\n    test_predictions = [i[0] for i in test_predictions.tolist()]\n\n    #Converting the prediction to . a dataframe\n    test_predictions = pd.DataFrame(test_predictions, columns = [dep_var+\"_new\"])\n    \n    return test_predictions\n","77a98134":"################# Iterate Tablular Learner ##############\n#make state\/country column in both train and test \n#train_data[\"state_country\"] = train_data[\"Province\/State\"].astype(str) + train_data[\"Country\/Region\"].astype(str)\n#test_data[\"state_country\"] = test_data[\"Province\/State\"].astype(str) + test_data[\"Country\/Region\"].astype(str)\n#test_data.head()\n\n#ensure both have state_country column \n#categories=test_data.groupby('state_country')['state_country'].count() #true \n#categories=pd.DataFrame(categories)\n\n#view the categories\n#print(categories.index)\n\n#subset the file\n#train_data=train_data.head(66+1)\n\n#train_data.head()\n#train_data.shape","a7351725":"################# Iterate Tablular Learner ##############\n#make state\/country column in both train and test \n#train_data[\"state_country\"] = train_data[\"Province\/State\"].astype(str) + train_data[\"Country\/Region\"].astype(str)\n#test_data[\"state_country\"] = test_data[\"Province\/State\"].astype(str) + test_data[\"Country\/Region\"].astype(str)\n#test_data.head()\n\n#ensure both have state_country column \ncategories=test_data.groupby(\"place\")[\"place\"].count() #true \ncategories=pd.DataFrame(categories)\n#print(categories.index)\n\n#create holding dataframe for test\nconfirmed_holding = pd.DataFrame()\n#fatalities_holding = pd.DataFrame()\n#train_data.head(50)","cea4f6d2":"#show categories \ncategories","54a86e60":"#for each state_country run the main program \nfor i in categories.index:\n    #print name for testing \n    #print(i)\n    #subset both train and testing data \n    train_temp=train_data[train_data[\"place\"]==i]\n    test_temp=test_data[test_data[\"place\"]==i]\n    \n    #run main AI function for the portion of data \n    confirmed_file=fastLearning(df1=train_temp,df2=test_temp,\n                                size=.05,dep='ConfirmedCases_old')\n    \n    fatalities_file=fastLearning(df1=train_temp,df2=test_temp,\n                                 size=.05,dep='Fatalities_old')\n    \n    #make test file \n    #test_temp = test_temp.assign(pd.Series(Fatalities_old_new=fatalities_file[\"Fatalities_old_new\"]))\n    fatalities_file = fatalities_file.set_index(test_temp.index)\n    test_temp[\"Fatalities_old_new\"] = fatalities_file\n\n    confirmed_file = confirmed_file.set_index(test_temp.index)\n    test_temp[\"Confirmed_old_new\"] = confirmed_file\n    \n    #append output file to the holding dataframe \n    confirmed_holding=pd.concat([confirmed_holding,test_temp],ignore_index=True)\n    #fatalities_holding=pd.concat([fatalities_holding,fatalities_file],ignore_index=True)\n    \n     \n\n#ensure test and holding dataframe are the same    \n#holding.shape==test_data.shape","b1ecb963":"confirmed_holding","0a98a561":"#make submission file \nfinal=confirmed_holding[[\"ForecastId\",\"Confirmed_old_new\",\"Fatalities_old_new\"]]\nfinal = final.rename(columns={'Confirmed_old_new': 'ConfirmedCases', 'Fatalities_old_new': 'Fatalities'})\nfinal.to_csv('submission.csv',index=False)\nfinal.head()\n","41f94122":"#make test file \nconfirmed_holding = confirmed_holding.rename(columns={'Confirmed_old_new': 'ConfirmedCases', 'Fatalities_old_new': 'Fatalities'})\ndel confirmed_holding['ConfirmedCases_old']\ndel confirmed_holding['Fatalities_old']\nconfirmed_holding.to_csv('complete_test.csv',index=False)","53d90f88":"# Create Submission File ","f3a3ba3a":"# Train AI and Forecast - One region at a time ","6657f479":"# Refactor the Population data\nRead in the population data. Set it so that it can be factorized. Combine to Train and Test","a2f53e96":"Examine the train and test files prior to fixing them","34bb2c78":"# Clean Data:\nConvert date into a date variable and turn them. Fill missing variables, categorify and normalize. Sort data for time series forecasting ","ef70f3d5":"# Build the AI function for Looping through Countries\/States","cd0b254f":"# Read in Data ","91dd28a2":"# Build Testing Files\n"}}