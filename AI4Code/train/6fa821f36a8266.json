{"cell_type":{"137c68b6":"code","90366586":"code","58008286":"code","50f3b615":"code","2ece7a2a":"code","dfeece85":"code","5e8fdcc5":"code","bd43cc7b":"code","2a07b0c4":"code","16451883":"code","e1e3e87f":"code","c2b3ab64":"code","96383d4d":"code","cf1738b9":"code","48ee0012":"code","ae3696e6":"code","a258531c":"code","1b7e9484":"code","d686237e":"code","ad620520":"code","a596adbd":"code","24e9dcae":"code","04caeb10":"code","b7443384":"code","5a9ce101":"code","89ead8e0":"code","b6a0cc7b":"code","f93a63d4":"code","32e507fa":"markdown","83f4eeb1":"markdown","2960794f":"markdown","88c380ac":"markdown","3f979489":"markdown","72a34d26":"markdown","1422ff32":"markdown","1489fc7c":"markdown","5a0c21bd":"markdown","5bffe8a1":"markdown","93d87c6f":"markdown","3481c013":"markdown","dbe06976":"markdown"},"source":{"137c68b6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom datetime import datetime, date\nfrom dateutil.relativedelta import relativedelta\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom math import ceil\n\nfrom keras.callbacks import LambdaCallback\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers import LSTM\nfrom keras.optimizers import RMSprop\n\n%matplotlib inline\n\ntrain = pd.read_csv('..\/input\/sales_train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\nitems = pd.read_csv('..\/input\/items.csv')\nitem_cats = pd.read_csv('..\/input\/item_categories.csv')\nshops = pd.read_csv('..\/input\/shops.csv')","90366586":"test_shops = test.shop_id.unique()\ntrain = train[train.shop_id.isin(test_shops)]\ntest_items = test.item_id.unique()\ntrain = train[train.item_id.isin(test_items)]","58008286":"MAX_BLOCK_NUM = train.date_block_num.max()\nMAX_ITEM = len(test_items)\nMAX_CAT = len(item_cats)\nMAX_YEAR = 3\nMAX_MONTH = 4 # 7 8 9 10\nMAX_SHOP = len(test_shops)","50f3b615":"grouped = pd.DataFrame(train.groupby(['shop_id', 'date_block_num'])['item_cnt_day'].sum().reset_index())\nfig, axes = plt.subplots(nrows=5, ncols=2, sharex=True, sharey=True, figsize=(16,20))\nnum_graph = 10\nid_per_graph = ceil(grouped.shop_id.max() \/ num_graph)\ncount = 0\nfor i in range(5):\n    for j in range(2):\n        sns.pointplot(x='date_block_num', y='item_cnt_day', hue='shop_id', data=grouped[np.logical_and(count*id_per_graph <= grouped['shop_id'], grouped['shop_id'] < (count+1)*id_per_graph)], ax=axes[i][j])\n        count += 1","2ece7a2a":"# add categories\ntrain = train.set_index('item_id').join(items.set_index('item_id')).drop('item_name', axis=1).reset_index()","dfeece85":"train['month'] = train.date.apply(lambda x: datetime.strptime(x, '%d.%m.%Y').strftime('%m'))\ntrain['year'] = train.date.apply(lambda x: datetime.strptime(x, '%d.%m.%Y').strftime('%Y'))","5e8fdcc5":"fig, axes = plt.subplots(nrows=5, ncols=2, sharex=True, sharey=True, figsize=(16,20))\nnum_graph = 10\nid_per_graph = ceil(train.item_category_id.max() \/ num_graph)\ncount = 0\nfor i in range(5):\n    for j in range(2):\n        sns.pointplot(x='month', y='item_cnt_day', hue='item_category_id', \n                      data=train[np.logical_and(count*id_per_graph <= train['item_category_id'], train['item_category_id'] < (count+1)*id_per_graph)], \n                      ax=axes[i][j])\n        count += 1","bd43cc7b":"fig, axes = plt.subplots(nrows=5, ncols=2, sharex=True, sharey=True, figsize=(16,20))\nnum_graph = 10\nid_per_graph = ceil(train.item_category_id.max() \/ num_graph)\ncount = 0\nfor i in range(5):\n    for j in range(2):\n        sns.pointplot(x='date_block_num', y='item_cnt_day', hue='item_category_id', \n                      data=train[np.logical_and(count*id_per_graph <= train['item_category_id'], train['item_category_id'] < (count+1)*id_per_graph)], \n                      ax=axes[i][j])\n        count += 1","2a07b0c4":"train = train.drop('date', axis=1)\ntrain = train.drop('item_category_id', axis=1)\ntrain = train.groupby(['shop_id', 'item_id', 'date_block_num', 'month', 'year']).sum()\ntrain = train.sort_index()","16451883":"scaler = StandardScaler()\ncnt_scaler = StandardScaler()\n\nscaler.fit(train.item_price.as_matrix().reshape(-1, 1))\ncnt_scaler.fit(train.item_cnt_day.as_matrix().reshape(-1, 1))\n\ntrain.item_price = scaler.transform(train.item_price.as_matrix().reshape(-1, 1))\ntrain.item_cnt_day = cnt_scaler.transform(train.item_cnt_day.as_matrix().reshape(-1, 1))","e1e3e87f":"train.reset_index().groupby(['item_id', 'date_block_num', 'shop_id']).mean()","c2b3ab64":"price = train.reset_index().set_index(['item_id', 'shop_id', 'date_block_num'])\nprice = price.sort_index()","96383d4d":"def convert(date_block):\n    date = datetime(2013, 1, 1)\n    date += relativedelta(months = date_block)\n    return (date.month, date.year)\n\ndef closest_date_block(current_day, item_id, shop_id):\n    \"\"\"Find the block_date which is closest to the current_day, given item_id and shop_id. Returns index integer\"\"\"\n    if (item_id, shop_id) in price.index:\n        search_lst = np.array(price.loc[(item_id, shop_id)].index)        \n        return search_lst[np.abs(current_day - search_lst).argmin()]\n    return -1\n                \ndef closest_price(current_day, item_id, shop_id):\n    closest_date = closest_date_block(current_day, item_id, shop_id)\n    if closest_date != -1:\n        return price.loc[( item_id, shop_id, closest_date )]['item_price']\n    return np.nan\n\ndef closest_price_lambda(x):\n    return closest_price(34, x.item_id, x.shop_id)","cf1738b9":"assert closest_date_block(18, 30, 5) == 18","48ee0012":"# Some simple math to know what date_block_num to start learning\nprint(convert(6))\nprint(convert(18))\nprint(convert(30))","ae3696e6":"maxlen = 4 # 4 months\nstep = 1\n# 0: train, 1: val, 2:test\nsentences = [[],[],[]]\nnext_chars = [[], []]\nBLOCKS = [6, 18, 30]\n\nfor s in test_shops:\n    shop_items = list(train.loc[s].index.get_level_values(0).unique())\n    for it in shop_items:        \n        for i_index, i in enumerate(BLOCKS):\n            sentence = []\n            closest_pc = closest_price(i, it, s)            \n            for j in range(maxlen+1):\n                if j < maxlen:\n                    if (s, it, i+j) in train.index:\n                        r = train.loc[(s, it, i + j)].to_dict(orient='list')                    \n                        closest_pc = r['item_price'][0]\n                        item_cnt_day = r['item_cnt_day'][0]\n                        row = {'shop_id': s, 'date_block_num': i+j, 'item_cnt_day': item_cnt_day, \n                               'month': month, 'item_id': it, 'item_price': closest_pc, 'year': year}\n                    else:\n                        month, year = convert(i+j)                    \n                        row = {'shop_id': s, 'date_block_num': i+j, 'item_cnt_day': 0, \n                               'month': month, 'item_id': it, 'item_price': closest_pc, 'year': year}\n                    sentence.append(row)\n                elif i_index < 2:   # not in test set\n                    next_chars[i_index].append(row)\n            sentences[i_index].append(sentence)","a258531c":"x_train_o = np.array(sentences[0])\nx_val_o = np.array(sentences[1])\nx_test_o = np.array(sentences[2])\ny_train = np.array([x['item_cnt_day'] for x in next_chars[0]])\ny_val = np.array([x['item_cnt_day'] for x in next_chars[1]])","1b7e9484":"length = MAX_SHOP + MAX_ITEM + MAX_MONTH + 1 + 1 + 1","d686237e":"from sklearn import preprocessing\n\nshop_le = preprocessing.LabelEncoder()\nshop_le.fit(test_shops)\nshop_dm = dict(zip(test_shops, shop_le.transform(test_shops)))\n\nitem_le = preprocessing.LabelEncoder()\nitem_le.fit(test_items)\nitem_dm = dict(zip(test_items, item_le.transform(test_items)))\n\nmonth_le = preprocessing.LabelEncoder()\nmonth_le.fit(range(7,11))\nmonth_dm = dict(zip(range(7,11), month_le.transform(range(7,11))))\n\n#cat_le = preprocessing.LabelEncoder()\n#cat_le.fit(item_cats.item_category_id)\n#cat_dm = dict(zip(item_cats.item_category_id.unique(), cat_le.transform(item_cats.item_category_id.unique())))","ad620520":"def vectorize(inp):\n    print('Vectorization...')   \n    x = np.zeros((len(inp), maxlen, length), dtype=np.float32)\n    for i, sentence in enumerate(inp):\n        for t, char in enumerate(sentence):            \n            x[i][t][ shop_dm[char['shop_id']] ] = 1        \n            x[i][t][ MAX_SHOP + item_dm[char['item_id']] ] = 1\n            x[i][t][ MAX_SHOP + MAX_ITEM + month_dm[char['month']] ] = 1\n            x[i][t][ MAX_SHOP + MAX_ITEM + MAX_MONTH + 1 ] = char['item_price']\n            x[i][t][ MAX_SHOP + MAX_ITEM + MAX_MONTH + 1 + 1] = char['item_cnt_day']    \n    return x","a596adbd":"x_train = vectorize(x_train_o)\nx_val = vectorize(x_val_o)\nx_test = vectorize(x_test_o)","24e9dcae":"# build the model: a single LSTM\nprint('Build model...')\nmodel = Sequential()\nmodel.add(LSTM(32, input_shape=(maxlen, length)))\nmodel.add(Dense(1, activation='relu'))\n\noptimizer = RMSprop(lr=0.005)\nmodel.compile(loss='mean_squared_error', optimizer=optimizer)\n\nmodel.fit(x_train, y_train, batch_size=128, epochs=13)","04caeb10":"#import math\n#from sklearn.metrics import mean_squared_error\n\n# make predictions\n#predict_train = model.predict(x_train)\n#predict_val = model.predict(x_val)\n# invert predictions\n#predict_train = cnt_scaler.inverse_transform(predict_train)\n#y_train = cnt_scaler.inverse_transform(y_train)\n#predict_val = cnt_scaler.inverse_transform(predict_val)\n#y_val = cnt_scaler.inverse_transform(y_val)\n# calculate root mean squared error\n#trainScore = math.sqrt(mean_squared_error(predict_train, y_train))\n#print('Train Score: %.2f RMSE' % (trainScore))\n#valScore = math.sqrt(mean_squared_error(predict_val, y_val))\n#print('Test Score: %.2f RMSE' % (valScore))\n#For 1 epoch\n#Train Score: 3.85 RMSE\n#Test Score: 4.29 RMSE","b7443384":"model.fit(x_val, y_val, batch_size=128, epochs=13)","5a9ce101":"predict_test = model.predict(x_test)\npredict_test = cnt_scaler.inverse_transform(predict_test)","89ead8e0":"test = test.set_index(['shop_id', 'item_id'])\ntest['item_cnt_month'] = 0","b6a0cc7b":"for index, sentence in enumerate(x_test_o):\n    (shop_id, item_id) = (sentence[0]['shop_id'], sentence[0]['item_id'])\n    test.loc[(shop_id, item_id)]['item_cnt_month'] = predict_test[index]","f93a63d4":"test = test.reset_index().drop(['shop_id', 'item_id'], axis=1)\ntest.to_csv('submission.csv', index=False)","32e507fa":"I would then define some helper functions","83f4eeb1":"Welcome to Predict Future Sales Challenge. In this kernel, I will focus on doing some illustrative data visualizations and then use LSTM to predict November 2015's sale \n\n# Loading Libraries and Data","2960794f":"For easier grouping, I would change the grouping order a little","88c380ac":"It is natural to try to encode the whole training set to feed to the network. However, this approach has two drawbacks:\n- It is unlikely that sale data from Jan 2013 or any time close by has any effect with sale of Nov 2015.\n- The memory requirements exceed the limit that Kaggle provides.\n\nWe would instead learn the sequence of July, August, September, October, November in 2013 and 2014. \n\n## Missing data\nNot every item is saled in the above time period, we will add a record for them with `item_cnt_day` as 0. The `price` is a little bit tricky. As can be seen at the code below, price of an item depends on shop and point of time. We will fill in empty values with the closest past record.","3f979489":"The learning seems to converge. We will then incrementally train on the validation set ([why does it work](https:\/\/github.com\/keras-team\/keras\/issues\/4446#issuecomment-261804574))","72a34d26":"Next, we would transform categorical value to their one-hot encoding version","1422ff32":"# Inference\nWith everything prepared, let's moved to the inference part","1489fc7c":"# Training\nIn gradient based learning method, it is common to normalize the numerical variable to speed up the training","5a0c21bd":"Below is the evaluation code for the model. For the speed of the kernel, I have commented them, but you can still find the result at the end of the cell","5bffe8a1":"We will do some simple math to find out what block should we train on","93d87c6f":"# Data Exploration\nNext, we would like to gain some insights about the data. I will attack from two angles, shop and item categories.","3481c013":"Clearly, there is a peak in shop shop at the end of the year, probably due to the holiday season. Therefore, it would be beneficial to add month and year, so that the network can pickup this pattern. It would be nice to see how each item sale is going. However, given the number of items, it would be more beneficial if we look at how each item categories is doing instead.","dbe06976":"First we will check if all shop and items in the test set is also in the training set."}}