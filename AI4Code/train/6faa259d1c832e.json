{"cell_type":{"7aa41529":"code","4729bb55":"code","cc6764a4":"code","ec4f6d5b":"code","772052dc":"code","689ff3b8":"code","04ffefdd":"code","569049aa":"code","b47141ef":"code","290c50e2":"code","86ecd902":"markdown","3c413df0":"markdown","c683815b":"markdown","59947999":"markdown","037ccc2f":"markdown","1e4fc8b5":"markdown","f9133293":"markdown","efa358c7":"markdown","129f0ece":"markdown"},"source":{"7aa41529":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","4729bb55":"data = pd.read_csv(\"\/kaggle\/input\/lower-back-pain-symptoms-dataset\/Dataset_spine.csv\")\nprint(data.info())\n\n","cc6764a4":"data.dropna(how=\"any\", inplace = True)  # Delete useless raw\ndata.drop([\"Unnamed: 13\"], axis = 1, inplace = True)\ndata.Class_att = [1 if each == \"Normal\" else 0 for each in data.Class_att]\n\n\nresult = data.Class_att.values\nfeatures_data = data.drop([\"Class_att\"], axis = 1)\nfeatures_data","ec4f6d5b":"features = (features_data - np.min(features_data))\/(np.max(features_data)-np.min(features_data)).values\nfeatures","772052dc":"from sklearn.model_selection import train_test_split\nfeatures_train, features_test, result_train, result_test = train_test_split(features, result, test_size = 2, random_state = 42)\n\n\nfeatures_train = features_train.T\nfeatures_test = features_test.T\nresult_train = result_train.T\nresult_test = result_test.T\n\nprint(\"Changed of Features and Values place.\")\n\n\nprint(\"features_train: \", features_train.shape)\nprint(\"features_test \", features_test.shape)\nprint(\"result_train: \", result_train.shape)\nprint(\"result_test: \", result_test.shape)","689ff3b8":"def initialize_weights_bias(dimension): # dimension = 12\n    weights = np.full((dimension,1), 0.01)\n    bias = 0.0\n    return weights, bias\n\ndef sigmoid(z):\n    result_head = 1\/(1+np.exp(-z))\n    return result_head\n\nprint(sigmoid(0)) #test sigmoid(z)","04ffefdd":"def forward_backward_propagation(weights, bias, features_train, result_train):\n    #forward\n    z = np.dot(weights.T,features_train) + bias\n    result_head = sigmoid(z)\n    \n    loss = -result_train*np.log(result_head) - (1-result_train)*np.log(1-result_head)\n    cost = (np.sum(loss))\/features_train.shape[1]\n    \n    #backward\n    derivative_weights = (np.dot(features_train,((result_head-result_train).T)))\/features_train.shape[1]\n    derivative_bias = np.sum(result_head-result_train)\/features_train.shape[1]\n    gradients = {\"derivative_weights\" : derivative_weights, \"derivative_bias\" : derivative_bias}\n    return cost,gradients","569049aa":"def update(weights, bias, features_train, result_train, learning_rate , number_of_iterations):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    for i in range(number_of_iterations):\n        cost,gradients = forward_backward_propagation(weights, bias, features_train, result_train)\n        cost_list.append(cost)\n        \n        weights = weights - learning_rate*gradients[\"derivative_weights\"]\n        bias = bias - learning_rate*gradients[\"derivative_bias\"]\n        \n        if i % 5 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iteration %i : %f\" %(i,cost))\n            \n    parameters = {\"weights\" : weights,\"bias\" : bias}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation=\"vertical\")\n    plt.xlabel(\"Number Of Iterations\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","b47141ef":"def predict(weights,bias,features_test):\n    z = sigmoid(np.dot(weights.T,features_test)+bias)\n    result_prediction = np.zeros((1,features_test.shape[1]))\n    \n    for i in range(z.shape[1]):\n        if z[0,i] <= 0.5:\n            result_prediction[0,i] = 0\n        else:\n            result_prediction[0,i] = 1\n            \n    return result_prediction","290c50e2":"def logistic_regression(features_train, result_train, features_test, result_test, learning_rate, number_of_iterations):\n    \n    dimension = features_train.shape[0]\n    weights, bias = initialize_weights_bias(dimension)\n    \n    parameters, gradients, cost_list = update(weights, bias, features_train, result_train, learning_rate, number_of_iterations) \n    \n    result_prediction_test = predict(parameters[\"weights\"], parameters[\"bias\"], features_test)\n    \n    print(\"Test accuracy: {}%\".format(100-np.mean(np.abs(result_prediction_test - result_test))*100))\n\nlogistic_regression(features_train, result_train, features_test, result_test, learning_rate = 1, number_of_iterations = 100)  ","86ecd902":"# **UPDATE**\nUpdate weights and bias with backward-forward propagation.","3c413df0":"data source : https:\/\/www.kaggle.com\/sammy123\/lower-back-pain-symptoms-dataset\/data#","c683815b":"# **TRAIN-TEST SPLIT**\nTrain Test Split data==> 80% of data set for Train, 20% of data set for Test","59947999":"# **LOGISTIC REGRESSION**\nMain part. Put it all together.","037ccc2f":"As you can see my labels are ordered -normal= 1, abnormal= 0-. ","1e4fc8b5":"# **FORWARD AND BACKWARD PROPAGATION FUNCTION**\nz = bias + px1w1 + px2w2 + ... + pxn*wn loss function = -(1 - y) log(1- y_head) - y log(y_head) cost function = sum(loss value) \/ train dataset sample count","f9133293":"# **PARAMETER INITALIZE AND SIGMOID FUNCTION**\nTime to start defining functions.First of all I need to initialize my weights and bias, then I will need a sigmoid function.\n\nSigmoid Function : f(x) = 1 \/ ( 1 + (e ^ -x) Initialize weight = 0.01 for each data Initialize bias = 0","efa358c7":"# **NORMALIZATION**\nFormula = (x -min(x))\/(max(x)-min(x)","129f0ece":"# **PREDICT**\nPredict function for testing purposes"}}