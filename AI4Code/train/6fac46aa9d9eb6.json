{"cell_type":{"3f17e24e":"code","a2da982e":"code","5da05729":"code","b364e6b0":"code","f29407ea":"code","66737ee8":"code","124890ae":"code","a9a1c5df":"code","f8cdba5d":"code","88f8c19f":"code","c791f6e1":"code","05a4b00b":"code","c4edf2a5":"code","af6a6e2e":"code","2b91c3b8":"code","ae55f117":"code","34cca2d8":"code","8a62ebf1":"code","26c692d2":"code","03a8c417":"code","cfb97ba8":"code","bd321258":"code","c0be710c":"code","d0f723b1":"code","bc393a5c":"code","55e59385":"code","54e6c912":"code","66b45469":"code","9214c60c":"code","0dec8e12":"code","a38cefcc":"code","2ac36020":"code","e9809a80":"code","850b7579":"code","00b19c32":"code","56a4580f":"code","7f5ff47a":"code","0a03257d":"code","c2410bd2":"code","f12c776d":"code","c07d9da4":"code","6bd9c1d7":"code","24b739b4":"code","9bf81d4c":"code","f65a9b3b":"code","4b8b567b":"code","c4615241":"markdown","3b9d0f0e":"markdown"},"source":{"3f17e24e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a2da982e":"import numpy as np#import all necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","5da05729":"train=pd.read_csv(\"..\/input\/titanic\/train.csv\")#import the dataset\ntest=pd.read_csv(\"..\/input\/titanic\/train.csv\")","b364e6b0":"train.head()","f29407ea":"test.head()","66737ee8":"#store the passengerid of test data\npassenger_id=test[\"PassengerId\"]\ntrain.head()","124890ae":"#set the index as passengerid\ntrain.set_index([\"PassengerId\"],inplace=True)\ntest.set_index([\"PassengerId\"],inplace=True)\ntrain.head()","a9a1c5df":"# Checking the missing data:\ntrain.isnull().sum()#show the missing datas","f8cdba5d":"100*(train.isnull().sum()\/len(train))\ndef missing_values_percent(train):#we can use this function in all dataframes.\n    nan_percent=100*(train.isnull().sum()\/len(train))\n    nan_percent=nan_percent[nan_percent>0].sort_values()\n    return(nan_percent)\n\nnan_percent=missing_values_percent(train)\nnan_percent","88f8c19f":"# Imputer age coloumn\nfrom sklearn.impute import SimpleImputer\n#train data             \nImp=SimpleImputer(strategy='median')\nnew_train=Imp.fit_transform(train.Age.values.reshape(-1,1))\ntrain['Age2'] = new_train\n\n#test data\nnew_test=Imp.fit_transform(test.Age.values.reshape(-1,1))\ntest['Age2'] = new_test\n\n\ntrain.drop('Age',axis=1,inplace=True)\ntest.drop('Age',axis=1,inplace=True)\n\n\ntrain.head()\n","c791f6e1":"train.isnull().sum()","05a4b00b":"train[\"Embarked\"].value_counts()","c4edf2a5":"#So we can replace missing datas in Embarked with s\ntrain[\"Embarked\"].fillna(\"s\",inplace=True)","af6a6e2e":"#cabin has 327 missing datas so we can get rid of it by dropping this feature.\ntrain.drop(\"Cabin\",axis=1,inplace=True)","2b91c3b8":"train.isnull().sum()","ae55f117":"train[\"Survived\"].value_counts(normalize=True)#How many passengers survived?\n","34cca2d8":"def bar_chart_stacked(dataset,feature,stacked=True):\n  survived=train[train[\"Survived\"]==1][feature].value_counts()\n  dead=train[train[\"Survived\"]==0][feature].value_counts()   \n  df_survived_dead=pd.DataFrame([survived,dead])  \n  df_survived_dead.index=[\"passengers survived\",\"passengers died\"]   \n  df_survived_dead.plot(kind=\"bar\",stacked=stacked,figsize=(8,5))\n                              ","8a62ebf1":"bar_chart_stacked(train,\"Survived\")","26c692d2":"train[\"Sex\"].value_counts().to_frame()#passengers count on gender","03a8c417":"bar_chart_stacked(train,\"Sex\")#compare the survived  and dead passengers counts on gender","cfb97ba8":"train.groupby([\"Pclass\"])[\"Survived\"].mean().to_frame()","bd321258":"bar_chart_stacked(train,\"Pclass\")","c0be710c":"def bar_chart_compare(dataset,feature1,feature2=None):\n    plt.figure(figsize=(8,5))\n    plt.title(\"survived rate by sex and pclass\")\n    g=sns.barplot(x=feature1,y=\"Survived\",hue=feature2,data=dataset).set ","d0f723b1":"bar_chart_compare(train,\"Pclass\",\"Sex\")","bc393a5c":"sns.pairplot(train)","55e59385":"corr = train.corr()\nplt.subplots(figsize=(12, 8))\nsns.heatmap(corr, annot=True)","54e6c912":"train[\"Name\"]","66b45469":"train['Title'] =train['Name'].apply(lambda x: x.split(', ')[1].split('. ')[0].strip())\ntest['Title'] =train['Name'].apply(lambda x: x.split(', ')[1].split('. ')[0].strip())\ntrain[\"Title\"].unique()\n","9214c60c":"train[\"family_size\"]=train[\"SibSp\"]+train[\"Parch\"]+1","0dec8e12":"def family_group(size):\n    a=\"\"\n    if(size<=1):\n        a=\"alone\"\n    elif(size<=4):\n        a=\"small\"\n    else:\n        a=\"large\"\n    return a","a38cefcc":"train[\"family_group\"]=train[\"family_size\"].map(family_group)\ntrain[\"fare_per_person\"]=train[\"Fare\"]\/train[\"family_size\"]\ntrain.head()","2ac36020":"plt.figure(figsize=(8,5))\nsns.barplot(data=train,x=\"family_group\",y=\"Survived\")","e9809a80":"train.drop([\"Ticket\"],axis=1,inplace=True)#This feature doesnt give us any useful infomation.","850b7579":"train[\"Pclass\"].apply(str)","00b19c32":"#Divide dataframe to 2 parts(num and str)\ntrain_num=train.select_dtypes(exclude=\"object\")\ntrain_obj=train.select_dtypes(include=\"object\")","56a4580f":"train_obj=pd.get_dummies(train_obj,drop_first=True)#use one-hot encoding to transform str to int and float\ntrain_obj.shape","7f5ff47a":"Final_train=pd.concat([train_num,train_obj],axis=1)\nFinal_train.info()","0a03257d":"#Determine the feature and lable\nX=Final_train.drop(\"Survived\",axis=1)\ny=Final_train[\"Survived\"]\n","c2410bd2":"#Split the dataset to train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)","f12c776d":"from sklearn.preprocessing import StandardScaler#scaling the features\nscaler= StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","c07d9da4":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\n\ny_pred = model.predict(X_test)","6bd9c1d7":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, plot_confusion_matrix","24b739b4":"accuracy_score(y_test, y_pred)","9bf81d4c":"confusion_matrix(y_test, y_pred)","f65a9b3b":"print(classification_report(y_test, y_pred))","4b8b567b":"# Preparing data for submission\ncolumn_names = ['PassengerId', 'Survived']\nSubmission = pd.DataFrame(columns = column_names)\n\nSubmission['PassengerId'] = pd.read_csv('..\/input\/titanic\/test.csv')['PassengerId']\nSubmission['Survived'] = pd.DataFrame(gender_submission)\nSubmission.reset_index(drop=True, inplace=True)\n\n# Saving prediction\nSubmission.to_csv(r'.\/submission.csv', index=False)\nSubmission.head()","c4615241":"**Feature Engineering**","3b9d0f0e":"#EDA"}}