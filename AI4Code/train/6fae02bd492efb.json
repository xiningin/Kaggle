{"cell_type":{"05a8c352":"code","8816fd53":"code","5b0df937":"code","ca518e26":"code","59d4152a":"code","13e228f3":"code","f3bcc054":"code","6c7c681d":"code","726e13fc":"code","e375f3b5":"code","efb8249f":"code","4c07e0ec":"code","db5d8112":"code","469cdf86":"code","81890196":"code","e182ba64":"code","e70c6603":"code","f3bf92f8":"code","229cc5f6":"code","2376f02c":"code","f4153b02":"code","def3c3dd":"code","b4becfb6":"code","613b4f08":"code","a38a4c6c":"code","dcc538ad":"code","1082bec6":"code","92ee29ad":"code","a8d4c41b":"code","f3d10665":"markdown","891ed7b4":"markdown","7a19c942":"markdown","06efa547":"markdown","b8849fbd":"markdown","67e47d04":"markdown","02e9f57d":"markdown","24f4c073":"markdown","7471e45f":"markdown","edd89bce":"markdown","d7eb2ed9":"markdown","5f54579b":"markdown","8579c2f6":"markdown","d3cea5db":"markdown","38478dab":"markdown","4d44cfec":"markdown","941b2076":"markdown","cde2bcec":"markdown","d055eac2":"markdown"},"source":{"05a8c352":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt","8816fd53":"from kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\n(market_train_df, news_train_df) = env.get_training_data()\napple = market_train_df[market_train_df.assetCode == 'AAPL.O']","5b0df937":"apple.shape","ca518e26":"score_df = pd.DataFrame(apple.returnsOpenNextMktres10)\nscore_df['label'] = 1\nscore_df.label.mask(score_df.returnsOpenNextMktres10 < 0, -1.0, inplace=True)","59d4152a":"score_df.head()","13e228f3":"def compute_score(_score_df):\n    \"\"\"Args: score_df : pd.DataFrame([returnsOpenNextMktres10, label])\"\"\"\n    _x_t = _score_df.iloc[:,0] * _score_df['label']\n    return np.mean(_x_t) \/ np.std(_x_t, ddof=1)","f3bcc054":"compute_score(score_df)","6c7c681d":"def visualize(score_df):\n    \"\"\"plot distributions given score_df\n    Args: score_df : pd.DataFrame([returnsOpenNextMktres10, label])\n    \"\"\"\n    x_t = score_df.iloc[:,0] * score_df.iloc[:,1]\n    x_t_mean, x_t_std = np.mean(x_t), np.std(x_t, ddof=1)\n    real_mean, real_std = np.mean(score_df.iloc[:,0]), np.std(score_df.iloc[:,0], ddof=1)\n    plt.hist(score_df.iloc[:,0].clip(-0.3,0.3), bins='auto', label=\"returnsOpenNextMktres10\")\n    plt.xlim([-0.3,0.3])\n    plt.plot([real_mean, real_mean],[0,300], label=\"mean\")\n    plt.title(\"real returns distribution\")\n    plt.legend()\n    plt.show()\n    plt.hist(x_t.clip(-0.3,0.3), bins='auto', label='x_t')\n    plt.xlim([-0.3,0.3])\n    plt.plot([x_t_mean, x_t_mean],[0,300], label=\"mean\")\n    plt.legend()\n    plt.title(\"returns distribution of predictions\")\n    plt.show()","726e13fc":"visualize(score_df)","e375f3b5":"confidence = 3\nscore_df = pd.DataFrame(apple.returnsOpenNextMktres10)\nscore_df['label'] = 3\nscore_df.label.mask(score_df.returnsOpenNextMktres10 < 0, -3.0, inplace=True)","efb8249f":"compute_score(score_df)","4c07e0ec":"score_df.head()","db5d8112":"visualize(score_df)","469cdf86":"score_df = pd.DataFrame(apple.returnsOpenNextMktres10)\nscore_df['label'] = 1\nscore_df.label.mask(score_df.returnsOpenNextMktres10 < 0, -1.0, inplace=True)\nx_t = score_df.iloc[:,0] * score_df.iloc[:,1]\nx_t_mean, x_t_std = np.mean(x_t), np.std(x_t, ddof=1)\nscore_df['label'] = x_t_mean \/ score_df.returnsOpenNextMktres10","81890196":"score_df.head()","e182ba64":"compute_score(score_df)","e70c6603":"visualize(score_df)","f3bf92f8":"score_df = pd.DataFrame(apple.returnsOpenNextMktres10)\nmean = 1\nscore_df['label'] = 1 \/ score_df.returnsOpenNextMktres10","229cc5f6":"score_df.head()","2376f02c":"compute_score(score_df)","f4153b02":"score_df = pd.DataFrame(apple.returnsOpenNextMktres10)","def3c3dd":"def compare(score_df):\n    A,B = score_df.copy(),score_df.copy()\n    A['label']=A['predictions']\n    B['label']=1\/(B['predictions'])\n    print(\"[model A] score with label = target_value -> {}\".format(compute_score(A)))\n    print(\"[model B] score with label = 1 \/ target_value -> {}\".format(compute_score(B)))","b4becfb6":"score_df['predictions'] =score_df.iloc[:,0]","613b4f08":"compare(score_df)","a38a4c6c":"score_df['predictions'] = score_df.iloc[:,0] + np.random.normal(0, 0.0001, len(score_df.iloc[:,0]))\ncompare(score_df)","dcc538ad":"score_df['predictions'] = score_df.iloc[:,0] + np.random.normal(0, 0.0005, len(score_df.iloc[:,0]))\ncompare(score_df)","1082bec6":"score_df['predictions'] = score_df.iloc[:,0] + np.random.normal(0, 0.001, len(score_df.iloc[:,0]))\ncompare(score_df)","92ee29ad":"score_df['predictions'] = score_df.iloc[:,0] + np.random.normal(0, 0.01, len(score_df.iloc[:,0]))\ncompare(score_df)","a8d4c41b":"score_df['predictions'] = score_df.iloc[:,0] + np.random.normal(0, 0.1, len(score_df.iloc[:,0]))\ncompare(score_df)","f3d10665":"In the plot above you see how the binary classifier only has positive returns (since is perfect). The mean is of course positive but the variance look pretty big.\nThe orange line representing the mean can be simply interpreted as \"how profitable is this trading strategy\" (what is the mean of the returns).\nIf our goal was only to maximize this mean we would just need to naively **put a higher value in our confidence**. But what would happen to the competition score in this case? Let's see.","891ed7b4":"<h2>I - Perfect binary classifier","7a19c942":"<h2>IV - Variance minimizer<\/h2>","06efa547":"What you will find in this kernel:\n    - an exploration of the metric of the competition\n    - an attempt to answer the question: given that I have perfect predictions, how do I maximize my scoring?\n        \nWe go thourgh some models\n- I - Perfect binary classifier\n- II - (Confident) Perfect binary classifier\n- III - (Artificial) Perfect binary classifier\n- IV - Variance minimizer","b8849fbd":"Here is our perfect binary classifier","67e47d04":"We can start by examining, what if we had a binary classifier that predict only 1 for positive target value and -1 for negative target value? \nWe build the classifier and see what scores does it get.","02e9f57d":"Here we compare **[model A]** that output label = target_value and **[model B] ** that output label = 1 \/ target_value, we add some random noise and we check how the two models compare","24f4c073":"<h2>II - (Confident) Perfect binary classifier<\/h2>\n<p>Since our classifier is perfect, we should be confident right? We try to increase the confidence from 1 to 3. We want to see how an increase in the mean of x_t affect our scoring","7471e45f":"This time the mean of x_t (returns from binary classifier) is bigger, this mean our 'trading strategy' is way more profitable. \nBut sadly our score remains the same. This is of course due to the variance increasing with the mean. So to optimize our score we need to increase the mean and reduce the variance. Let's work on **reducing the variance** now.","edd89bce":"**[Metric Description]**\nIn this competition, you must predict a signed confidence value, y\u0302 ti\u2208[\u22121,1] , which is multiplied by the market-adjusted return of a given assetCode over a ten day window. If you expect a stock to have a large positive return--compared to the broad market--over the next ten days, you might assign it a large, positive confidenceValue (near 1.0). If you expect a stock to have a negative return, you might assign it a large, negative confidenceValue (near -1.0). If unsure, you might assign it a value near zero.\n\nFor each day in the evaluation time period, we calculate:\n$$xt=\u2211iy\u0302 tirtiuti$$\n\nwhere rti is the 10-day market-adjusted leading return for day t for instrument i, and uti is a 0\/1 universe variable (see the data description for details) that controls whether a particular asset is included in scoring on a particular day.\n\nYour submission score is then calculated as the mean divided by the standard deviation of your daily xt values:\nscore=x\u00aft\u03c3(xt).\nIf the standard deviation of predictions is 0, the score is defined as 0.","d7eb2ed9":"We can increase our scoring aribitrarly if we already know the mean of all our predictions and our predictions are all perfect, in this way we would decrease the variance:\n- predict perfect binary labels [-1, 1]\n- compute x_t_mean of predictions\n- label = x_t_mean \/ returnsOpenNextMktres10","5f54579b":"<h2>III - (Artificial) Perfect binary classifier<\/h2>","8579c2f6":"Now, based on your random seed you will get different result, but you can notice how model B with a bit of random noice to our predictions goes down immediately. **So we DID NOT break the competition (as expected)**","d3cea5db":"<h1>Exploratory Metric Analysis (EMA)","38478dab":"As predicted, the score got aribitrarly large. In the plot is not clear but what happens is that all predictions of our classifier are exactly the mean value, so that the variance got minimized. We just theoretically proved this concept. What can we learn from it?\nOne question might be: do we really care about the mean? What if we just minimize the variance using this method and we d**on't care about increasing the mean.**","4d44cfec":"The score is still aribtrarly large, and as you can notice I am using 1 as mean, our label are 1\/target_value.\nWow! Did we just break the competition? **If instead of output target_value we output 1 \/ target_value we get huge improvement in the score?!** Let's take a moment to check this idea","941b2076":"Now we don't care about the mean of x_t, we only want to minimize the variance. We still have a perfect predictor, how should be calibrate our labels?","cde2bcec":"Now let's add some noise to our predictions (in real life they are not perfect)","d055eac2":"A perfect binary classifier will get a score of only 1.0185581036583518, why is this? \n**The mean for x_t is high, but the low score is probably due to a high variance. ** Let's examine better."}}