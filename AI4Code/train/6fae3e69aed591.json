{"cell_type":{"fc543d42":"code","c33421bd":"code","3a16b58f":"code","a0d25873":"code","111b056d":"code","159552de":"code","0b8f4640":"code","ddc515cc":"code","e9ffd7b1":"code","34ae1da0":"code","72ed3f32":"code","6fdb4e7f":"code","d12fc195":"code","0191e1ce":"code","44a30175":"code","a1d2a65c":"code","98243beb":"code","3a76ac1e":"code","94fcb299":"code","f40b3934":"code","de56b976":"code","4bc547b5":"code","0c9f984f":"code","b09859da":"code","39417e3e":"code","740370d4":"code","5ca6cec4":"code","5f1e0dbf":"code","194c26fb":"code","12fef459":"code","f0112928":"code","5785065d":"code","6ca8fd6b":"code","9a84f111":"code","2000975a":"code","9936e6df":"code","acd06ec5":"code","969430ee":"code","ae6f496b":"code","b2df2ea3":"code","78ce3ea0":"code","da63a06b":"code","e6ba5975":"code","7486b825":"code","2186c83e":"code","b258ab21":"code","65fbcb90":"code","1342f7bf":"code","954633d3":"markdown","31d0d19b":"markdown","86fc3ff0":"markdown","f30b042c":"markdown","c466b12d":"markdown","ffc43e7f":"markdown","2826b367":"markdown","4caac09d":"markdown","48d70dc1":"markdown","b21ea200":"markdown","ca041fec":"markdown","c5180064":"markdown","760f55ef":"markdown","e1b934bb":"markdown","4a504d89":"markdown","7a2ae369":"markdown","06044e14":"markdown"},"source":{"fc543d42":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c33421bd":"# Reading a text-based dataset into pandas\ndef readData_rawSMS(filepath):\n    data_rawSMS = pd.read_csv(filepath, header=0, usecols=[0,1], encoding='latin-1')\n    data_rawSMS.columns = ['label', 'content']\n    return data_rawSMS\n\ndata_rawSMS = readData_rawSMS(os.path.join(dirname, filename))\ndata_rawSMS.head()\n\n# \u5783\u573e\u8a0a\u606f(spam) \/ \u6709\u6548\u8a0a\u606f(ham)","3a16b58f":"# Generate descriptive statistics\ndata_rawSMS.describe()","a0d25873":"# Group large amounts of data and compute operations on these groups\ndata_rawSMS.groupby('label').describe()","111b056d":"# Make a new column `length` to detect how long the content are.\ndata_rawSMS['length'] = data_rawSMS['content'].apply(len)\ndata_rawSMS.head()","159552de":"data_rawSMS.groupby('label').describe()","0b8f4640":"data_rawSMS[data_rawSMS.label=='ham']","ddc515cc":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nplt.style.use(\"ggplot\")\n\nplt.figure(figsize=(6, 4))\n\ndata_rawSMS[data_rawSMS.label=='ham'].length.plot(\n    bins=35, kind='hist', color='blue', \n    label='Ham messages', alpha=0.6)\ndata_rawSMS[data_rawSMS.label=='spam'].length.plot(\n    kind='hist', color='red', \n    label='Spam messages', alpha=0.6)\n\nplt.legend()\nplt.xlabel(\"Message Length\")\n#Through just basic EDA we've been able to discover a trend that `spam messages` tend to have `more characters`.","e9ffd7b1":"data_rawSMS.length.describe()","34ae1da0":"data_rawSMS[data_rawSMS.label=='ham'].describe()","72ed3f32":"data_rawSMS[data_rawSMS['length'] == 910]","6fdb4e7f":"data_rawSMS[data_rawSMS['length'] == 910]['content'].iloc[0]\n# data_rawSMS[data_rawSMS.length == 910].content.iloc[0]     #same result","d12fc195":"from nltk.corpus import stopwords\n\nprint(stopwords.words('english'))\n\nimport string\n\nprint(string.punctuation)","0191e1ce":"def text_process(mess):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    1. Remove all punctuation\n    2. Remove all stopwords\n    3. Returns a list of the cleaned text\n    \"\"\"\n    STOPWORDS = stopwords.words('english') + ['u', '\u00fc', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure']\n    \n    # Check characters to see if they are in punctuation\n    # \u975e\u6a19\u9ede\u7b26\u865f\u7684 characters\uff0c\u5c31\u5b58\u9032 list\n    nopunc = [char for char in mess if char not in string.punctuation] \n    \n    # Join the characters again to form the string.\n    nopunc = ''.join(nopunc)\n    \n    # Now just remove any stopwords\n    # \u975e\u505c\u7528\u5b57\u7684 word\uff0c\u8f49\u6210\u5c0f\u5beb\u5b57\u6bcd\u5f8c\u5c31\u5b58\u9032 list\n    return ' '.join([word.lower() for word in nopunc.split() if word.lower() not in STOPWORDS]) \n\ndata_rawSMS['clean_msg'] = data_rawSMS.content.apply(text_process)\ndata_rawSMS.head()","44a30175":"# Unigram Analysis\nfrom collections import Counter\n\ndef get_words(content):\n    words = []\n    for row in content:\n        for j in row.split():\n            words.append(j.strip())\n    return words\n\ncounter = Counter(get_words(data_rawSMS['clean_msg']))\nmost_common = dict(counter.most_common(20))\nprint(most_common)\n","a1d2a65c":"# Unigram Analysis\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.barplot(x=list(most_common.values()), y=list(most_common.keys()))","98243beb":"from collections import Counter\n\nwords = data_rawSMS[data_rawSMS.label=='ham'].clean_msg.apply(lambda x: [word for word in x.split()])\nham_words = Counter()\n\nfor msg in words:\n    ham_words.update(msg)\n    \nprint(ham_words.most_common(50))","3a76ac1e":"from collections import Counter\n\nwords = data_rawSMS[data_rawSMS.label=='spam'].clean_msg.apply(lambda x: [word for word in x.split()])\nspam_words = Counter()\n\nfor msg in words:\n    spam_words.update(msg)\n    \nprint(spam_words.most_common(50))","94fcb299":"# Use `CountVectorizer` to \"convert text into a matrix of token counts\"\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n# The raw data, a sequence of symbols cannot be fed directly to the machine learning algorithms.\n# They require numerical feature vectors with a fixed size.\nsimple_train = ['call you tonight call', 'Call me a cab', 'Please call me... PLEASE!']\n\n# 1. import and instantiate CountVectorizer (with the default parameters)\nvect = CountVectorizer()\n\n# 2. learn the 'vocabulary' of the training data (occurs in-place)\nvect.fit(simple_train)\n\n# 3. examine the fitted vocabulary\nprint(vect.get_feature_names())","f40b3934":"# 4. transform training data into a 'document-term matrix'\nsimple_train_dtm = vect.transform(simple_train)\nprint(simple_train_dtm)\n\"\"\"\n   0       1      2       3        4         5\n['cab', 'call', 'me', 'please', 'tonight', 'you'] = vect.get_feature_names()\n\n-> 'call you tonight'\n->   0    5     4\n\n(docID, wordID)  word count\n  (0, 1)\t1\n  (0, 4)\t1\n  (0, 5)\t1\n  \nindex  0 1 2 3 4 5\n====> [0 1 0 0 1 1]\n\"\"\"\nprint()\nprint(simple_train_dtm.toarray())\n\n# 5. examine the vocabulary and document-term matrix together\npd.DataFrame(simple_train_dtm.toarray(), columns=vect.get_feature_names())","de56b976":"# 6. example text for model testing\nsimple_test = [\"please don't call me\"]\n\n# 7. transform testing data into a document-term matrix (using existing vocabulary)\nsimple_test_dtm = vect.transform(simple_test)\n\"\"\"\n   0       1      2       3        4         5\n['cab', 'call', 'me', 'please', 'tonight', 'you'] = vect.get_feature_names()\n\n-> \"please don't call me\"\n->     3    x     1   2\nvector -> [[0, 1, 1, 1, 0 , 0]]\n\"\"\"\n\npd.DataFrame(simple_test_dtm.toarray(), columns=vect.get_feature_names())","4bc547b5":"# convert label to a numerical variable\ndata_rawSMS['label_num'] = data_rawSMS.label.map({'ham':0, 'spam':1})\ndata_rawSMS.head()","0c9f984f":"X = data_rawSMS.clean_msg\ny = data_rawSMS.label_num\nprint(X.shape)\nprint(y.shape)","b09859da":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=0, train_size=0.8)\n\nprint('\u3010Training set\u3011')\nprint('Row_count: {}\\n\\nData content:\\n{}\\n'.format(X_train.shape, X_train))\nprint('Row_count: {}\\n\\nData label:\\n{}\\n'.format(y_train.shape, y_train))\nprint('------------')\nprint('\u3010Testing set\u3011')\nprint('Row_count: {}\\n\\nData:\\n{}\\n'.format(X_test.shape, X_test))\nprint('Row_count: {}\\n\\nData label:\\n{}\\n'.format(y_test.shape, y_test))","39417e3e":"### Method1 ###\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer()\n\nvect.fit(X_train)\n# print(len(vect.get_feature_names()))\n# print(vect.get_feature_names())\n\n\n# learn training data vocabulary, then use it to create a document-term matrix\nX_train_dtm = vect.transform(X_train)\n\n# word's TF\npd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names())","740370d4":"### Method2 ###\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer()\n\n# equivalently: combine fit and transform into a single step\nX_train_dtm = vect.fit_transform(X_train)","5ca6cec4":"# examine the vocabulary and document-term matrix together\npd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names())","5f1e0dbf":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer()\ntfidf_transformer.fit(X_train_dtm)\ntfidf_transformer.transform(X_train_dtm)","194c26fb":"pd.options.display.float_format = \"{:,.10f}\".format\n\n# word's TF-IDF weight\nTFIDF = tfidf_transformer.transform(X_train_dtm).toarray()\npd.DataFrame(TFIDF, columns=vect.get_feature_names()) ","12fef459":"pd.DataFrame(tfidf_transformer.transform(X_train_dtm).toarray(), columns=vect.get_feature_names()) #.iloc[4170]     #TF-IDF (L2 norm)","f0112928":"df = pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names()) #['\u00ec\u00ef'].iloc[4170]\ndf.loc[df['\u00ec\u00ef'] != 0]","5785065d":"TF = pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names())['\u00ec\u00ef'].iloc[4170]\nTF","6ca8fd6b":"pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names())['\u00ec\u00ef'].sum()","9a84f111":"pd.options.display.float_format = \"{:,.4f}\".format\n\n# word's IDF weight\npd.DataFrame([tfidf_transformer.idf_], columns=vect.get_feature_names()) ","2000975a":"for i in range(len(vect.get_feature_names())):\n    if '\u00ec\u00ef' == vect.get_feature_names()[i]:\n        print(i)\n        # '\u00ec\u00ef' => index 8147","9936e6df":"# examine the vocabulary and document-term matrix together\npd.DataFrame(tfidf_transformer.transform(X_train_dtm).toarray(), columns=vect.get_feature_names()).iloc[4170]\n\n\n\n### TF-IDF (L2 norm) ###\n# \u00ec\u00ef             0.4197\n\n### TF ###\n# \"\u00ec\u00ef\" \u5728\u7b2c 4170 \u7bc7\uff0c\u50c5\u51fa\u73fe\u4e00\u6b21\u3002\n\n### IDF ###\n# import math\n# TF = 1\n# DF = 36\n# total_Doc = 4179\n# math.log((4179 + 1)\/(36 + 1)) + 1 = 5.727148612874577","acd06ec5":"from sklearn.feature_extraction.text import CountVectorizer\n\n# --- start --- Use Method2 --- #\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nvect = CountVectorizer()\nX_train_dtm = vect.fit_transform(X_train)\n\ntfidf_transformer = TfidfTransformer()\ntfidf_transformer.fit(X_train_dtm)\ntfidf_transformer.transform(X_train_dtm)\n# --- end --- Use Method2 --- #\n\n\nX_test_dtm = vect.transform(X_test)\nprint(X_test_dtm.toarray())","969430ee":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=20, random_state=0)\n\n# train the model using X_train_dtm (timing it with an IPython \"magic command\")\n%time clf = rf.fit(X_train_dtm, y_train)\n","ae6f496b":"\nimport matplotlib.pyplot as plt\n\n# Extract single tree\nestimator = rf.estimators_[5]\n\nn_nodes = rf.estimators_[4].tree_.node_count\nprint(estimator, n_nodes)","b2df2ea3":"# from sklearn.ensemble import RandomForestClassifier\n\n# rf = RandomForestClassifier(n_estimators=20, oob_score=True, random_state=0)\n\n# # train the model using X_train_dtm (timing it with an IPython \"magic command\")\n# %time clf = rf.fit(X_train_dtm, y_train)\n\n# ### OOB ###\n# print(rf.oob_score_)","78ce3ea0":"# rf.feature_importances_.shape\n# rf.feature_importances_\n\n\ndf = pd.DataFrame([rf.feature_importances_], columns=vect.get_feature_names())\ndf.nlargest(20, '\u00ec\u00ef')","da63a06b":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# make class predictions for X_test_dtm\ny_predTest_class = rf.predict(X_test_dtm)\n\n# calculate accuracy of class predictions\nprint(\"\u3010 Testing \u3011\")\nprint('Accuracy score: {}'.format(accuracy_score(y_test, y_predTest_class)))\nprint('Precision score: {}'.format(precision_score(y_test, y_predTest_class)))\nprint('Recall score: {}'.format(recall_score(y_test, y_predTest_class)))\nprint('F1 score: {}'.format(f1_score(y_test, y_predTest_class)))","e6ba5975":"### \u88dc\u5145\uff1acalculate AUC (\u6a5f\u5668\u5b78\u7fd2\u7684\u6548\u80fd\u8861\u91cf\u6307\u6a19) ###\nfrom sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_test, clf.predict_proba(X_test_dtm)[:,1])\n\n\n# \u7576AUC = 1\u6642\uff0c\u4ee3\u8868\u5206\u985e\u5668\u975e\u5e38\u5b8c\u7f8e\uff0c\u4f46\u9019\u7562\u7adf\u662f\u7406\u60f3\u72c0\u6cc1\u3002\n# \u7576AUC > 0.5\u6642\uff0c\u4ee3\u8868\u5206\u985e\u5668\u5206\u985e\u6548\u679c\u512a\u65bc\u96a8\u6a5f\u731c\u6e2c\uff0c\u6a21\u578b\u6709\u9810\u6e2c\u50f9\u503c\u3002\n# https:\/\/ithelp.ithome.com.tw\/articles\/10229049\n# https:\/\/medium.com\/marketingdatascience\/%E5%88%86%E9%A1%9E%E5%99%A8%E8%A9%95%E4%BC%B0%E6%96%B9%E6%B3%95-roc%E6%9B%B2%E7%B7%9A-auc-accuracy-pr%E6%9B%B2%E7%B7%9A-d3a39977022c","7486b825":"from sklearn.metrics import classification_report, confusion_matrix\n\nprint(classification_report(y_test, y_predTest_class))\nprint()\nprint(confusion_matrix(y_test, y_predTest_class))","2186c83e":"# print the confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test, y_predTest_class), index=['Ham','Spam'] , columns=['Ham','Spam'])\nplt.figure(figsize=(3,3))\nsns.heatmap(cm, cmap=\"Blues\", linecolor='black', linewidth=1, annot=True, fmt='', xticklabels=['Ham','Spam'], yticklabels=['Ham','Spam'])\n\n# x \u8ef8\uff1a\u9810\u6e2c(Predict)\n# y \u8ef8\uff1a\u5be6\u969b(Actual)\n\n### Type I error  (\u56b4\u91cd) ###    Predict: Ham (0) & Actual: Spam (1)\nprint(len(X_test[y_predTest_class < y_test]), '\u500b\\t=> false negatives (spam incorrectly classifier)\\n') # X_test[y_predTest_class < y_test]\n\n### Type II error  (\u8f15\u5fae) ###   Predict: Spam (1) & Actual: Ham (0)\nprint(len(X_test[y_predTest_class > y_test]), '\u500b\\t=> false positives (ham incorrectly classifier)\\n') # X_test[y_predTest_class > y_test]\nprint(X_test[(y_predTest_class==1) & (y_test==0)]) # same result\nprint()\nprint(X_test.shape)","b258ab21":"from sklearn.metrics import precision_recall_fscore_support as score\n\n# make class predictions for X_test_dtm\ny_predTest_class = rf.predict(X_test_dtm)\n\nprecision, recall, fscore, support = score(y_test, y_predTest_class, pos_label=0, average='binary')\nprint('Precision : {} \/ Recall : {} \/ fscore : {} \/ Accuracy: {}'.format(round(precision,3),round(recall,3),round(fscore,3),round((y_predTest_class==y_test).sum()\/len(y_test),3)))\n\nprecision, recall, fscore, support = score(y_test, y_predTest_class, pos_label=1, average='binary')\nprint('Precision : {} \/ Recall : {} \/ fscore : {} \/ Accuracy: {}'.format(round(precision,3),round(recall,3),round(fscore,3),round((y_predTest_class==y_test).sum()\/len(y_test),3)))","65fbcb90":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.ensemble import RandomForestClassifier\n\n# vect = CountVectorizer()\n# X_train_dtm = vect.fit_transform(X_train)\n\n# tfidf_transformer = TfidfTransformer()\n# tfidf_transformer.fit(X_train_dtm)\n# tfidf_transformer.transform(X_train_dtm)\n    \n# rf = RandomForestClassifier() #n_estimators=20\n# rf.fit(X_train_dtm, y_train)\n\nSMS = 'Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005.'\nclean_text = text_process(SMS)\n# print(clean_text)\nsimple_test_dtm = vect.transform([clean_text])\nprint(simple_test_dtm.toarray(), simple_test_dtm.reshape(1,-1).shape)\n\ny_predSimpleTest_class = rf.predict(simple_test_dtm.reshape(1,-1))\nif int(y_predSimpleTest_class) == 1:\n    print ('SPAM: {}'.format(SMS))\nelse:\n    print ('ham: {}'.format(SMS))    ","1342f7bf":"### Pipeline (\u88dc\u5145) ###\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn import metrics\n\nX = data_rawSMS.clean_msg\ny = data_rawSMS.label_num\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)\n\npipe = Pipeline([('bow', CountVectorizer()),\n                 ('tfid', TfidfTransformer()),  \n                 ('model', RandomForestClassifier(n_estimators=20, bootstrap=True, oob_score=False, random_state=1))])\npipe.fit(X_train, y_train)\npipe.score(X_test, y_test)\n\ny_pred = pipe.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred))\nprint(metrics.confusion_matrix(y_test, y_pred))\n\n\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# make class predictions for X_test_dtm\ny_predTest_class = rf.predict(X_test_dtm)\n\n# calculate accuracy of class predictions\nprint(\"\u3010 Testing \u3011\")\nprint('Accuracy score: {}'.format(metrics.accuracy_score(y_test, y_pred)))\nprint('Precision score: {}'.format(metrics.precision_score(y_test, y_pred)))\nprint('Recall score: {}'.format(metrics.recall_score(y_test, y_pred)))\nprint('F1 score: {}'.format(metrics.f1_score(y_test, y_pred)))\n\nprint(classification_report(y_test, y_pred))","954633d3":"### Testing data\n* TfidfTransformer\n    * transform testing data (using fitted vocabulary) into a document-term matrix","31d0d19b":"# Dataset overview    \n* \u63a2\u7d22\u5f0f\u8cc7\u6599\u5206\u6790 (EDA)\n    * EDA (Exploratory Data Analysis) uses **visualization** and **basic statistics** to get an overview of the data we have, in order to do more complicated and thorough analysis to it.\n    * EDA should let us be able to achieve the following three main things:\n        1. To Know the Data - what information does the data provide, the structure of the data, etc.\n        2. Check the Data - if there\u2019s any outliers or unusual value.\n        3. Correlation between Data - find out important variables.\n    * We can also check if the data meet our assumption of it and figure out latent errors before actually building the model, so as to do adjusts for the further analysis.","86fc3ff0":"* https:\/\/ithelp.ithome.com.tw\/articles\/10194006\n* `pandas.DataFrame.iloc`\n    * Purely integer-location based indexing for selection by position. (\u7528 index \u4f4d\u7f6e\u4f86\u53d6\u6211\u5011\u8981\u7684\u8cc7\u6599)\n* `pandas.DataFrame.loc`\n    * Access a group of rows and columns by label(s) or a boolean array. (\u7528 \u6a19\u7c64 \u4f86\u53d6\u51fa\u8cc7\u6599)","f30b042c":"# Machine Learning workflow with Vectorization\n## 1. Divided into training set and testing set","c466b12d":"# Data Visualization","ffc43e7f":"## 2. Vectorization (+ Feature Engineering)\n### Training data\n* CountVectorizer\n* TfidfTransformer","2826b367":"# [Example] Vectorization\n* We'll do that in 3 steps using the `bag-of-words model`:\n  1. Count how many times does a word occur in each message (**term frequency**)\n  2. Weigh the counts, so that frequent tokens get lower weight (**inverse document frequency**)\n  3. Normalize the vectors to unit length, to abstract from the original text length (**L2 norm**)","4caac09d":"## TF-IDF Explain","48d70dc1":"* `train_test_split()`\n    1. `random_state`\n        * This ensures that if I have to rerun my code, I\u2019ll get the exact same train-test split, so my results won\u2019t change.\n    2. `stratify=y`\n        * This tells train_test_split to make sure that the training and test datasets contain examples of each class **in the same proportions as in the original dataset**. \n        * \u7531\u65bc classes \u7684\u4e0d\u5e73\u8861\u6027(imbalanced)\uff0c\u56e0\u6b64\u7279\u5225\u91cd\u8981\uff01\n        * \u82e5\u5b8c\u5168\u96a8\u6a5f\u5730\u62c6\u6210 Train \u548c Test\uff0c\u5bb9\u6613\u9020\u6210\u67d0\u500b\u5c0f\u985e\u5225\u5728 Test \u6709\u3001\u4f46\u5728 Train \u6c92\u6709\u7684\u60c5\u6cc1\u767c\u751f\uff0c\u4f7f\u5f97 model \u7121\u6cd5\u8fa8\u8b58\u90a3\u500b\u5c0f\u985e\u5225(\u56e0\u70ba Train \u6c92\u6709\uff0c\u6240\u4ee5\u6c92\u8fa6\u6cd5\u5b78\u7fd2)\uff01\n            > A random split could easily end up with all examples of the smallest class in the test set and none in the training set, and then the model would be unable to identify that class.","b21ea200":"## 3. Building and Evaluating a model (\u4f9d\u64da\u7279\u5fb5\u8cc7\u6599\u8a13\u7df4\u5206\u985e\u5668)","ca041fec":"* `data_rawSMS['length'] = data_rawSMS['content'].apply(len)`\n    * \u4f7f\u7528 len \u51fd\u6578 \u8a08\u7b97 `data_rawSMS` DataFrame \u7684 `content`\uff0c\u4e26\u5c07\u7d50\u679c\u7d66\u4e88\u53e6\u589e\u7684\u6b04\u4f4d `length`\u3002\n* `DataFrame.apply(func, axis=0)`\n    * Apply a function along an axis of the DataFrame.\n    * Example: `df = pd.DataFrame([[4, 9]] * 3, columns=['A', 'B'])`\n        ```\n           A  B\n        0  4  9\n        1  4  9\n        2  4  9\n        ```\n        * df.apply(np.sum, axis=0)\n            ```\n            A    12\n            B    27\n            dtype: int64\n            ```\n        * df.apply(np.sum, axis=1)\n            ```\n            0    13\n            1    13\n            2    13\n            dtype: int64\n            ```","c5180064":"## Evaluate (Testing)\n* accuracy_score\n* precision_score\n* recall_score\n* f1_score","760f55ef":"```\nDataFrame.plot(x=None, y=None, kind='line', ax=None, subplots=False, sharex=None, sharey=False, layout=None, figsize=None, \n    use_index=True, title=None, grid=None, legend=True, style=None, logx=False, logy=False, loglog=False, \n    xticks=None, yticks=None, xlim=None, ylim=None, rot=None, fontsize=None, colormap=None, table=False, \n    yerr=None, xerr=None, secondary_y=False, sort_columns=False, **kwds)\n```\n* bins: int or sequence or str, default: rcParams[\"hist.bins\"] (default: 10)\n    * \u628a\u5206\u4f48(\u9577\u689d)\u5207\u6210 N \u7b49\u5206\uff0cN \u9810\u8a2d\u70ba 10\u3002\n        > \u5373\u9577\u689d\u7e3d\u5171\u6709 10 \u500b\u3002\n* legend: Place legend on axis subplots\n    * \u653e\u7f6e\u5716\u4f8b","e1b934bb":"# Reference\n* [Fares Sayah - Natural Language Processing (NLP) \ud83e\uddfe for Beginners](https:\/\/www.kaggle.com\/faressayah\/natural-language-processing-nlp-for-beginners)\n* [Ravi Chaubey - Natural Language Processing with Python](https:\/\/www.kaggle.com\/ravichaubey1506\/natural-language-processing-with-python)\n* [Madz2000 - Simple EDA with Data Cleaning & GloVe(98%Accuracy)](https:\/\/www.kaggle.com\/madz2000\/simple-eda-with-data-cleaning-glove-98-accuracy)\n* [adityapatil - Spam detector using NLP and Random Forest](https:\/\/www.kaggle.com\/adityapatil673\/spam-detector-using-nlp-and-random-forest)\n* [Shekhar - Spam Detection using NLP and Random Forest](https:\/\/www.kaggle.com\/shekhart47\/spam-detection-using-nlp-and-random-forest)","4a504d89":"# Text Pre-processing\n* The simplest is the `\u8a5e\u888b\u6a21\u578b (Bag-of-words Model, BoW)` approach, where each unique word in a text will be represented by one number.\n    ![](https:\/\/miro.medium.com\/max\/875\/1*ujkZ3JrQ6ubSuEpepHE4Aw.png)\n    > [NLP \u5165\u9580 (1) \u2014 Text Classification (Sentiment Analysis) \u2014 \u6975\u7c21\u6613\u60c5\u611f\u5206\u985e\u5668 Bag of words + Naive Bayes](https:\/\/sfhsu29.medium.com\/nlp-%E5%85%A5%E9%96%80-1-text-classification-sentiment-analysis-%E6%A5%B5%E7%B0%A1%E6%98%93%E6%83%85%E6%84%9F%E5%88%86%E9%A1%9E%E5%99%A8-bag-of-words-naive-bayes-e40d61de9a7f)\n    \n    * \u5982\u4f55\u5229\u7528 bag-of-words \u65b9\u6cd5\u5c07\u6587\u5b57\u8f49\u63db\u6210\u6578\u5b57\uff1f\n        > [NLP\u7684\u57fa\u672c\u57f7\u884c\u6b65\u9a5f(II) \u2014 Bag of Words \u8a5e\u888b\u8a9e\u8a00\u6a21\u578b](https:\/\/medium.com\/@derekliao_62575\/nlp%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%9F%B7%E8%A1%8C%E6%AD%A5%E9%A9%9F-ii-bag-of-words-%E8%A9%9E%E8%A2%8B%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B-3b670a0c7009)\n        * Example: \n            * a. \u770b\u5230\u4ed6\u6211\u5c31\u4e0d\u723d\u3002(\u5148\u65b7\u8a5e) => \u770b\u5230\/\u4ed6\/\u6211\/\u5c31\/\u4e0d\u723d\n            * b. \u770b\u5230\u4ed6\u6211\u5c31\u706b\u5927\u3002(\u5148\u65b7\u8a5e) => \u770b\u5230\/\u4ed6\/\u6211\/\u5c31\/\u706b\u5927\n        * \u9019\u500b\u8a9e\u6599\u5eab\u7684\u8a5e\u888b\u9577\u9019\u6a23\uff1a(\u5c31 ,\u4ed6 , \u770b\u5230, \u6211 , \u706b\u5927, \u4e0d\u723d)\n            * Notice! \n                * \u8a5e\u888b\u88e1\u7684\u8a5e\u300c\u6c92\u6709\u7d55\u5c0d\u7684\u9806\u5e8f\u95dc\u4fc2\u300d\uff0c\u6b64\u70ba\u96a8\u6a5f\u6392\u6cd5\u3002\n        * \u63a5\u8457\uff0c\u5982\u679c\u4e00\u500b\u8a5e\u5728\u53e5\u5b50\u4e2d\u6709\u51fa\u73fe\uff0c\u6211\u5011\u5c31\u5e6b\u4ed6\u505a\u4e00\u500b\u8a18\u865f\uff0c\u5982\u4e0b\uff1a\n            * a. \u8868\u793a\u6210\uff1a[1, 1, 1, 1, 0, 1] (a \u53e5\u6c92\u6709\u300c\u706b\u5927\u300d\uff0c\u6a19\u793a\u70ba 0)\n            * b. \u8868\u793a\u6210\uff1a[1, 1, 1, 1, 1, 0] (b \u53e5\u6c92\u6709\u300c\u4e0d\u723d\u300d\uff0c\u6a19\u793a\u70ba 0)\n        * \u50cf\u9019\u6a23\u7528 1 \u8ddf 0 \u4f86\u8868\u793a\u53e5\u5b50\u4e2d\u8a5e\u8a9e\u6709\u6c92\u6709\u51fa\u73fe\u7684\u65b9\u5f0f\uff0c\u5b83\u6709\u500b\u9177\u70ab\u540d\u5b57\uff1a\u300e\u7368\u71b1\u7de8\u78bc (One-hot encoding)\u300f\n            * \u5229\u7528\u7368\u71b1\u7de8\u78bc\u7684\u65b9\u5f0f\uff0c\u4e00\u500b\u53e5\u5b50\u53ef\u4ee5\u88ab\u8f49\u63db\u6210\u4e00\u500b\u5411\u91cf\u7684\u5f62\u5f0f\u8868\u9054(\u5411\u91cf \"vector\" \u5c31\u662f\u4e00\u5217\u6578\u5b57\u800c\u5df2)\uff0c\u5c31\u53ef\u4ee5\u9054\u6210\u7c21\u55ae\u7684\u6587\u5b57\u8f49\u63db\u6210\u6578\u5b57\u3002\n    * BoW \u7684\u884d\u751f\u6a21\u578b\uff1a\n        * TF-IDF\n            * \u7372\u53d6\u4e00\u500b\u80fd\u4ee3\u8868\u4e00\u500b\u8a5e\u5728\u6587\u4ef6\u4e2d\u91cd\u8981\u7a0b\u5ea6\u7684\u6578\u503c\u3002\n        * CBoW (Continuous Bag of Words, \u9023\u7e8c\u8a5e\u888b\u6a21\u578b)\n            * \u9019\u500b\u6a21\u578b\u662f\u4e00\u500b\u6dfa\u5c64\u7684\u985e\u795e\u7d93\u7db2\u8def\u3002\n            * \u76f8\u8f03\u65bc\u50b3\u7d71\u8a5e\u888b\u6a21\u578b\uff0cCBoW \u7684\u8f38\u5165\u4e00\u6a23\u662f\u7368\u71b1\u7684\u5f62\u5f0f\uff0c\u4f46\u4e0d\u540c\u7684\u9ede\u662f\uff0cCBoW \u6a21\u578b\u6703\u5c07\u4e00\u958b\u59cb\u6bcf\u500b\u8a5e\u90fd\u900f\u904e\u4e2d\u9593\u7684\u96b1\u85cf\u5c64\u4f5c\u8f49\u63db\uff0c\u8b93\u6bcf\u500b\u8a5e\u7684\u8a5e\u5411\u91cf\u4e2d\u4e0d\u518d\u53ea\u5305\u542b0\u82071\uff0c\u800c\u662f\u6709\u610f\u7fa9\u7684\u6578\u503c\u3002\n                * \u4e2d\u9593\u7684\u96b1\u85cf\u5c64\u8f49\u63db\u662f\u600e\u9ebc\u9032\u884c\u7684\uff1f\n                    * CBoW \u6703\u540c\u6642\u53c3\u8003**\u4e00\u500b\u8a5e\u524d\u5f8c\u7684\u8a9e\u5883**\u4f86\u6c7a\u5b9a\u90a3\u500b\u8a5e\u6240\u4ee3\u8868\u7684\u8a5e\u5411\u91cf\u662f\u4ec0\u9ebc\u3002\n                        > Ex:\u300c\u4e0d\u723d\u300d\u548c\u300c\u706b\u5927\u300d\u524d\u9762\u6240\u63a5\u7684\u8a5e\u90fd\u662f\u300c\u770b\u5230\u300d\u3001\u300c\u4ed6\u300d\u3001\u300c\u6211\u300d\u3001\u300c\u5c31\u300d\n                        > \u56e0\u6b64\u6a21\u578b\u5c31\u6703\u5224\u65b7\uff1a\u300c\u4e0d\u723d\u300d\u548c\u300c\u706b\u5927\u300d\u53ef\u80fd\u8868\u73fe\u51fa\u76f8\u4f3c\u7684\u8a9e\u610f\u548c\u53e5\u6cd5\u7d50\u69cb\uff0c\u9019\u500b\u5169\u500b\u8a5e\u5c31\u6703\u88ab\u8ce6\u4e88\u975e\u5e38\u63a5\u8fd1\u7684\u8a5e\u5411\u91cf\u3002\n        * Word2vec\n            * \u7531\u300c\u9023\u7e8c\u8a5e\u888b\u6a21\u578b CBoW\u300d\u548c\u300c\u8df3\u5b57\u6a21\u578b skip-gram\u300d\u69cb\u6210 word2vec \u6a21\u578b\u3002\n            * \u7531 Google \u7684 Mikolov \u7b49\u4eba\u5728 2013 \u5e74\u63d0\u51fa\u3002","7a2ae369":"## Input SMS and Predict","06044e14":"* \u5b9a\u7fa9 `readData_rawSMS` function\n    * header\n        * `0`: \u7b2c\u4e00\u5217(\u6a6b)\u70ba\u6b04\u4f4d\u540d\u7a31\n            > \u5373 v1, v2\n        * `1`: \u7b2c\u4e8c\u5217(\u6a6b)\u70ba\u6b04\u4f4d\u540d\u7a31\n            > \u5373 ham, Go until jurong point, crazy.. Available only ...\n        * `None`: \u672c\u8cc7\u6599(spam.csv)\u6c92\u6709\u6b04\u4f4d\u540d\u7a31\n    * usecols\n        * `[0,1]`: \u50c5\u4f7f\u7528\u7b2c\u4e00\u884c(\u76f4)\u548c\u7b2c\u4e8c\u884c(\u76f4)\u7684\u8cc7\u6599\uff0c\u5176\u4ed6\u884c(\u76f4)\u7565\u904e\u4e0d\u8b80\u53d6\u4e14\u4e0d\u4f7f\u7528\u3002\n    * data_rawSMS.columns = ['label', 'content']\n        > \u91cd\u65b0\u547d\u540d\u6b04\u4f4d\u540d\u7a31\uff1a\u7531 `v1, v2` \u6539\u70ba `label, content`"}}