{"cell_type":{"5bb14d90":"code","935358a2":"code","6d955145":"code","e3b2373b":"code","89b3250a":"code","e5c2b320":"code","818cef50":"code","088cd17a":"code","1f2c216e":"code","f49db8bf":"code","cfd5b143":"markdown","cda1f630":"markdown","87b4b836":"markdown","4519c9df":"markdown","cee45ca0":"markdown"},"source":{"5bb14d90":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","935358a2":"data = pd.read_csv(\"..\/input\/tennis.csv\")","6d955145":"data.info()","e3b2373b":"data.columns","89b3250a":"data.head(14)","e5c2b320":"# outlook_count = data.groupby(['outlook', 'play']).size()\n# outlook_total = data.groupby(['outlook']).size()\n# temp_count = data.groupby(['temp', 'play']).size()\n# temp_total = data.groupby(['temp']).size()\n# humidity_count = data.groupby(['humidity', 'play']).size()\n# humidity_total = data.groupby(['outlook']).size()\n# windy_count = data.groupby(['windy', 'play']).size()\n# windy_total = data.groupby(['windy']).size()\n# print(outlook_count)\n# print(windy_total)\n# print(outlook_total)\n# print(temp_count)\n# print(temp_total)\n# print(humidity_count)\n# print(humidity_total)\n# print(windy_count)\n# print(windy_total)\n\n","818cef50":"# p_over_yes = outlook_count['overcast','yes']\n# p_over_no = 0\n# p_rainy_yes = outlook_count['rainy','yes']\n# p_rainy_no = outlook_count['rainy','no']\n# p_rainy_yes = outlook_count['sunny', 'yes']\n","088cd17a":"X_train = pd.get_dummies(data[['outlook', 'temp', 'humidity', 'windy']])\ny_train = pd.DataFrame(data['play'])\n\n#assigning predictor and target variables\n#x= np.array([[-3,7],[1,5], [1,2], [-2,0], [2,3], [-4,0], [-1,1], [1,1], [-2,2], [2,7], [-4,1], [-2,7]])\n#Y = np.array([3, 3, 3, 3, 4, 3, 3, 4, 3, 4, 4, 4])\nprint(X_train.info())\nprint(X_train.head())","1f2c216e":"#Import Library of Gaussian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\nimport numpy as np","f49db8bf":"#Create a Gaussian Classifier\nmodel = GaussianNB()\n\n# Train the model using the training sets \nmodel.fit(X_train, y_train)\n\n#Predict Output \npredicted= model.predict([[False,1,0,0,0,1,0,1,0]])\nprint (predicted)","cfd5b143":"# **Naive Bayes Classifier**\n***","cda1f630":"# In Progress","87b4b836":"**What is Naive Bayes algorithm?**\n\nNaive Bayes is a classification technique based on Bayes\u2019 Theorem(*Probability theory*) with an assumption that all the features that predicts the target value are independent of each other. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature in determining the target value.\n\nThis assumption we just read about is very **Naive** when we are dealing with real world data because most of the times, features do depend on each other in determining the target - this is why the algorithm gets its name **Naive Bayes** (duhh!).\n\n> Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.\n\nBayes theorem provides a way of calculating posterior probability P(c|x) - *(read as Probability of **c** given **x**)*,  from P(c), P(x) and P(x|c). Look at the equation below:\n>\n> $$\\mathbf{P} \\left({x \\mid c} \\right) = \\frac{\\mathbf{P} \\left ({c \\mid x} \\right) \\mathbf{P} \\left({c} \\right)}{\\mathbf{P} \\left( {x} \\right)}$$\n\nIn the *above* equation,\n\n* P(c|x) is the posterior probability of class (c, target) given predictor (x, attributes).\n* P(c) is the prior probability of class **c**.\n* P(x|c) is the likelihood which is the probability of predictor(the query  **x**) given class.\n* P(x) is the prior probability of predictor **x**.\n\n**Note : ** Independence assumption is never correct but often works well in practice.\n\n","4519c9df":"**Why should we use Naive Bayes ?**\n\n* As stated above, It is **_easy_** to build and is particularly useful for **_very large data sets_**.\n* It is **extremely fast** for both training and prediction.\n* It provide straightforward probabilistic prediction.\n* It is often very easily interpretable.\n* It has very few (if any) tunable parameters.\n* It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).","cee45ca0":"### Read the dataset:\n\nNow, we read the dataset in a varible named **data** using pandas library which we imported above as '*pd*'  (data-type: DataFrame).\n\n\n**Note: ** *Here the dataset used is for Tennis games and weather conditions where the target is if a tennis game is played in the given conditions or not, the dataset is very small, just containing 14 rows and 5 columns for the purpose of this tutorial for beginners.*"}}