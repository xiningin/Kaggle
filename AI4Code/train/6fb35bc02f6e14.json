{"cell_type":{"76d123e5":"code","64fddd5f":"code","8f6f05f8":"code","ba92a0be":"code","a293138b":"code","5c63601e":"code","cb6141e3":"code","3a992a59":"code","23c3a0fd":"code","d84b257b":"code","db8e935f":"code","8cb7b95b":"code","d6f94555":"code","f77f4b87":"code","5a5aa7ae":"code","2660d547":"code","b220c029":"code","dc884c87":"code","dbf88bc9":"code","a7e2d0ec":"code","8f1ac1a2":"markdown","d58c328c":"markdown","7bb377ef":"markdown","69db5346":"markdown","d0c250d8":"markdown","dc82efef":"markdown","2c387e34":"markdown","36f9ab13":"markdown","931f3ca8":"markdown","00a2f43e":"markdown","c533cd97":"markdown"},"source":{"76d123e5":"from tqdm import tqdm_notebook as tqdm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\nfrom sklearn import preprocessing\nimport lightgbm as lgb\nimport os\npd.options.display.float_format = '{:,}'.format\nimport gc\n\n# # Load Data\ntrain_df = pd.read_csv(\"..\/input\/champs-scalar-coupling\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/champs-scalar-coupling\/test.csv\")\n\nstructures = pd.read_csv(\"..\/input\/champs-scalar-coupling\/structures.csv\")\nyuk = pd.read_csv(\"..\/input\/submolecularyukawapotential\/structures_yukawa.csv\",\n                 usecols=[\n                     'dist_O_0', 'dist_O_1',\n                     'dist_F_0','dist_F_1',\n                     'dist_C_0','dist_C_1', 'dist_C_2', 'dist_C_3',\n                     'dist_H_0','dist_H_1', 'dist_H_2', 'dist_H_3'\n                 ]\n                 )\nyuk['molecule_name'] = structures.molecule_name\nyuk['atom_index'] = structures.atom_index\nyuk.to_csv(\"yuk.csv\", index=False)\ndel yuk\n\ntarget = train_df['scalar_coupling_constant']\ntrain_size = train_df.shape[0]\n\ntrain_df = train_df\\\n    .merge(structures, left_on=['molecule_name', 'atom_index_0'], right_on=['molecule_name', 'atom_index'], how='left')\\\n    .drop(['atom_index'], axis=1)\\\n    .merge(structures, left_on=['molecule_name', 'atom_index_1'], right_on=['molecule_name', 'atom_index'], how='left',\n           suffixes=('_0', '_1'))\\\n    .drop(['atom_index'], axis=1)\n\ntest_df = test_df\\\n    .merge(structures, left_on=['molecule_name', 'atom_index_0'], right_on=['molecule_name', 'atom_index'], how='left')\\\n    .drop(['atom_index'], axis=1)\\\n    .merge(structures, left_on=['molecule_name', 'atom_index_1'], right_on=['molecule_name', 'atom_index'], how='left',\n           suffixes=('_0', '_1'))\\\n    .drop(['atom_index'], axis=1)\n\ndel structures\ngc.collect()","64fddd5f":"target = train_df.pop('scalar_coupling_constant')\n\nfeatures = [col for col in train_df.columns if col != 'id' and col != 'scalar_coupling_constant']\n\nall_data = pd.concat([train_df[features], test_df[features]], axis=0, sort=False)\nall_data[['molecule_name', 'atom_index_0', 'atom_index_1']].to_csv(\"all_data_index.csv\", index=False)\ntest_df = test_df[['id']]\ndel train_df\ngc.collect()\n\nall_data['distance'] = (\n    (all_data['x_0'] - all_data['x_1']).pow(2) + (all_data['y_0'] - all_data['y_1']).pow(2) + (\n        all_data['z_0'] - all_data['z_1']).pow(2)\n    ).pow(0.5)","8f6f05f8":"structures = pd.read_csv(\"..\/input\/champs-scalar-coupling\/structures.csv\")\nstructures_df = structures.merge(structures, how='left', on= ['molecule_name'], suffixes = ('_0', '_1'))\ndel structures\ngc.collect()\n\nstructures_df['distance'] = (\n    (structures_df['x_0'] - structures_df['x_1']).pow(2) + (structures_df['y_0'] - structures_df['y_1']).pow(2) + (\n        structures_df['z_0'] - structures_df['z_1']).pow(2)\n    ).pow(0.5)\n\nstructures_df = structures_df[structures_df.atom_1.isin(['F', 'O'])]\ngc.collect()","ba92a0be":"for atom in ['O', 'F']:\n    rank_col = '{0}_rank'.format(atom)\n    structures_df.loc[structures_df.atom_1 == atom, rank_col] = structures_df[structures_df.atom_1 == atom].groupby(\n        ['molecule_name', 'atom_index_0'])['distance'].rank(method='first')\n\n    atom_distance_ranks = structures_df[structures_df[rank_col] <= 2][['molecule_name', 'atom_index_0', 'distance', rank_col]]\n    atom_distance_ranks[rank_col] = atom_distance_ranks[rank_col].astype(int).astype(str)\n    atom_distance_ranks = atom_distance_ranks.pivot_table(\n        index=['molecule_name', 'atom_index_0'], columns=[rank_col], values=['distance'])\n\n    atom_distance_ranks.columns = [atom + '_' + '_'.join(col) for col in atom_distance_ranks.columns]\n\n    atom_distance_ranks = atom_distance_ranks.reset_index()\n    \n    all_data = all_data.merge(atom_distance_ranks, on=['molecule_name', 'atom_index_0'], how='left')\n    del atom_distance_ranks\n    gc.collect()\n    \ndel structures_df\ngc.collect()\n\nfor atom in ['H', 'C', 'N']:\n    rank_col = '{0}_rank'.format(atom)\n    all_data.loc[all_data.atom_1 == atom, rank_col] = all_data[all_data.atom_1 == atom].groupby(\n        ['molecule_name', 'atom_index_0'])['distance'].rank(method='first')\n\n    atom_distance_ranks = all_data[all_data[rank_col] <= 4]\n    atom_distance_ranks[rank_col] = atom_distance_ranks[rank_col].astype(int).astype(str)\n    atom_distance_ranks = atom_distance_ranks.pivot_table(\n        index=['molecule_name', 'atom_index_0'], columns=[rank_col], values=['distance'])\n    \n    atom_distance_ranks.columns = [atom + '_' + '_'.join(col) for col in atom_distance_ranks.columns]\n\n    atom_distance_ranks = atom_distance_ranks.reset_index()\n    \n    all_data = all_data.merge(atom_distance_ranks, on=['molecule_name', 'atom_index_0'], how='left')\n    del atom_distance_ranks\n    gc.collect()\n\nall_data.drop(columns=['C_rank', 'H_rank', 'N_rank'], inplace=True)\ngc.collect()\n\nnum_features = 0\nprev_features = 0 \nnum_features = all_data.shape[1]\nall_data.iloc[:5,prev_features:num_features].head()","a293138b":"grps = [['molecule_name', 'atom_index_0'], ['molecule_name', 'atom_index_1']]\ngrp_columns = ['distance']\ngc.collect()\n\nfor grp in grps:\n    for column in grp_columns:\n        stats = all_data.groupby(grp).agg(\n            {column: [\"mean\", \"min\", \"std\", \"max\"]}\n        )\n        stats.columns = ['_'.join(col) for col in stats.columns]\n        stats.columns = ['_'.join(grp) + '_' + col for col in stats.columns]\n\n        all_data = all_data.join(stats, on=grp, how='left')\n\n        del stats\n        gc.collect()\n\n        columns_prefix = '_'.join(grp) + '_' + column\n        all_data[columns_prefix + '_mean_ratio'] = all_data[column] \/ all_data[columns_prefix + '_mean']\n        all_data[columns_prefix + '_mean_diff'] = all_data[column] - all_data[columns_prefix + '_mean']\n        all_data[columns_prefix + '_mean_diff_std_ratio'] = all_data[columns_prefix + '_mean_diff'] \\\n                                                            \/ all_data[columns_prefix + '_std']        \n\nprev_features = num_features\nnum_features = all_data.shape[1]\nall_data.iloc[:5,prev_features:num_features].head()","5c63601e":"bond_data = pd.read_csv(\"..\/input\/submolecular-bond-data\/structures_bond.csv\", usecols = [\n    'molecule_name', 'atom_index', 'n_bonds', 'bond_lengths_mean', 'bond_lengths_std'])\n\nall_data = all_data.merge(bond_data, left_on=['molecule_name', 'atom_index_0'],\n                          right_on=['molecule_name', 'atom_index'], how='left')\\\n                   \nall_data.drop(columns=['atom_index'], inplace=True)\n\nall_data = all_data.merge(bond_data, left_on=['molecule_name', 'atom_index_1'],\n                          right_on=['molecule_name', 'atom_index'], how='left', suffixes=('_0', '_1'))\\\n\ndel bond_data\ngc.collect()\nall_data.drop(columns=['atom_index'], inplace=True)\n\nprev_features = num_features\nnum_features = all_data.shape[1]\nall_data.iloc[:5,prev_features:num_features].head()\n\nstructures = pd.read_csv(\"..\/input\/champs-scalar-coupling\/structures.csv\")\natom_counts = structures.pivot_table(\n    index=['molecule_name'], columns=['atom'], values='atom_index', aggfunc='count')\\\n    .reset_index()\n\natom_counts.columns = [col + '_structure_count' if col != 'molecule_name' else col for col in atom_counts.columns ]\n\ndel structures\ngc.collect()\n\natoms_unique = all_data.pivot_table(\n    index='molecule_name', columns=['atom_1'], values='atom_index_1', aggfunc='nunique')\\\n    .reset_index()\n\natoms_unique.columns = [col + '_atoms_count' if col != 'molecule_name' else col for col in atoms_unique.columns ]\n\nall_data = all_data.merge(atom_counts, on=['molecule_name'], how='left')\nall_data = all_data.merge(atoms_unique, on=['molecule_name'], how='left')\n\ndel atoms_unique, atom_counts","cb6141e3":"atomic_radius = {'H':0.38, 'C':0.77, 'N':0.75, 'O':0.73, 'F':0.71} # Without fudge factor\n\nfudge_factor = 0.05\natomic_radius = {k:v + fudge_factor for k,v in atomic_radius.items()}\nprint(atomic_radius)\n\nelectronegativity = {'H':2.2, 'C':2.55, 'N':3.04, 'O':3.44, 'F':3.98}\n\nfor idx in [0,1]:\n    atom = 'atom_{0}'.format(idx)\n    atoms = all_data[atom].values\n    atoms_EN = [electronegativity[x] for x in atoms]\n    atoms_rad = [atomic_radius[x] for x in atoms]\n\n    all_data['rad_{0}'.format(idx)] = atoms_rad\n    all_data['EN_{0}'.format(idx)] = atoms_EN\n\nall_data['inverse_square'] = 1 \/ all_data['distance'].pow(2)\nall_data['inverse_cube'] =  1 \/ all_data['distance'].pow(3)\n\nall_data['inv_distance_minus_R'] = 1 \/ (\n    all_data['distance'] - all_data['rad_0'] - all_data['rad_1']).pow(2)\nall_data['inv_distance_EN'] = 1 \/ (\n    all_data['distance']*(0.5*all_data['EN_0'] + 0.5*(all_data['EN_1']))).pow(2)\n\nall_data['inv_distance_sum_atom_0'] = all_data.groupby(\n    ['molecule_name', 'atom_index_0'])['inv_distance_minus_R'].transform('sum')\nall_data['inv_distance_sum_atom_1'] = all_data.groupby(\n    ['molecule_name', 'atom_index_1'])['inv_distance_minus_R'].transform('sum')\n\nall_data['inv_distance_EN_atom_0'] = all_data.groupby(\n    ['molecule_name', 'atom_index_0'])['inv_distance_EN'].transform('sum')\nall_data['inv_distance_EN_atom_1'] = all_data.groupby(\n    ['molecule_name', 'atom_index_1'])['inv_distance_EN'].transform('sum')\n\nall_data['inv_distance_harmonic'] = all_data['inv_distance_sum_atom_0']*all_data['inv_distance_sum_atom_1']\\\n    \/ (all_data['inv_distance_sum_atom_0'] + all_data['inv_distance_sum_atom_1'])\n\nall_data['inv_distance_EN_harmonic'] = all_data['inv_distance_EN_atom_0']*all_data['inv_distance_EN_atom_1']\\\n    \/ ( all_data['inv_distance_EN_atom_0'] + all_data['inv_distance_EN_atom_1'])\n\nall_data.drop(columns=['rad_0', 'EN_0', 'rad_1', 'EN_1'\n                      ], inplace=True)\n\ndel atoms_EN, atoms_rad, atoms\ngc.collect()\n\nprev_features = num_features\nnum_features = all_data.shape[1]\nall_data.iloc[:5,prev_features:num_features].head()","3a992a59":"# Cosine Distance\nall_data['dot_product'] = all_data['x_0']*all_data['x_1'] + all_data['y_0']*all_data['y_1'] + all_data['z_0']*all_data['z_1']\nall_data['norm_atom_0'] = all_data['x_0'].pow(2) + all_data['y_0'].pow(2) + all_data['z_0'].pow(2)\nall_data['norm_atom_1'] = all_data['x_1'].pow(2) + all_data['y_1'].pow(2) + all_data['z_1'].pow(2)\n\nall_data['cosine'] = all_data['dot_product'] \/ (all_data['norm_atom_0']*all_data['norm_atom_1'])\n\nall_data.drop(columns=['dot_product', 'norm_atom_0', 'norm_atom_1'], inplace=True)\ngc.collect()","23c3a0fd":"for idx in [0, 1]:\n    atom_index = 'atom_index_{0}'.format(idx)\n    \n    atom_bond_counts = all_data.pivot_table(\n        index=['molecule_name', atom_index], columns=['atom_1'], values='atom_0', aggfunc='count')\\\n    .reset_index()\n\n    atom_bond_counts.columns = [\n        col + '_bond_count_{0}'.format(idx) if col not in ['molecule_name', atom_index]\n        else col for col in atom_bond_counts.columns ]\n\n    atom_bond_distances = all_data.pivot_table(\n        index=['molecule_name', atom_index], columns=['atom_1'], values='distance', aggfunc='sum')\\\n        .reset_index()\n    \n    atom_bond_distances.columns = [\n        col + '_bond_distances_{0}'.format(idx) if col not in ['molecule_name', atom_index]\n        else col for col in atom_bond_distances.columns ]\n    \n    all_data = all_data.merge(\n        atom_bond_counts, on=['molecule_name', atom_index], how='left')\n    all_data = all_data.merge(\n        atom_bond_distances, on=['molecule_name', atom_index], how='left')\n    \n    all_data['C_bond_distances_{0}_mean'.format(idx)] = all_data[\n        'C_bond_distances_{0}'.format(idx)] \/ all_data['C_bond_count_{0}'.format(idx)]\n    \n    all_data['H_bond_distances_{0}_mean'.format(idx)] = all_data[\n        'H_bond_distances_{0}'.format(idx)] \/ all_data['H_bond_count_{0}'.format(idx)]\n    \n    all_data['N_bond_distances_{0}_mean'.format(idx)] = all_data[\n        'N_bond_distances_{0}'.format(idx)] \/ all_data['N_bond_count_{0}'.format(idx)]\n\n    del atom_bond_counts, atom_bond_distances\n    gc.collect()\n\ngc.collect()\n\nprev_features = num_features\nnum_features = all_data.shape[1]\nall_data.iloc[:5,prev_features:num_features].head()","d84b257b":"yuk0 = pd.read_csv(\"yuk.csv\").rename(columns={'atom_index': 'atom_index_0'})\nyuk1 = pd.read_csv(\"yuk.csv\").rename(columns={'atom_index': 'atom_index_1'})\nall_data_index = pd.read_csv(\"all_data_index.csv\")\n    \nall_data_index = all_data_index\\\n    .merge(yuk0, on=['molecule_name', 'atom_index_0'], how='left', copy=False)\\\n    .merge(yuk1, on=['molecule_name', 'atom_index_1'], how='left',\n           suffixes=('_0', '_1'), copy=False)\\\n\ndel yuk0\ndel yuk1\ngc.collect()\nall_data_index.head()","db8e935f":"for col in all_data_index.columns:\n    if col not in all_data.columns:\n        all_data[col] = all_data_index[col]","8cb7b95b":"del  all_data_index\ngc.collect()","d6f94555":"types = all_data.dtypes\ncat_columns = [t[0] for t in types.iteritems() if ((t[1] not in ['int64', 'float64']))]\n\nprint('Label encoding categorical columns:', cat_columns)\nencoders = {}\nfor col in cat_columns:\n    lbl = preprocessing.LabelEncoder()\n    all_data[col] = lbl.fit_transform(all_data[col].astype(str))\n    encoders[col] = lbl","f77f4b87":"# Error function\ndef group_mean_log_mae(y_true, y_pred, groups, floor=1e-9):\n    maes = (y_true-y_pred).abs().groupby(groups).mean()\n    print((y_true-y_pred).abs().groupby(groups))\n    return np.log(maes.map(lambda x: max(x, floor))).mean()","5a5aa7ae":"features = [col for col in all_data.columns if col != 'fc' and col != 'id' and col not in ['atom_index_0', 'atom_index_1', 'scalar_coupling_constant', 'molecule_name']]\n\ndef lgbm_model_oof(train_X, train_y, test_X, n_folds, lgbm_params):\n    folds = KFold(n_splits=n_folds, shuffle=False, random_state=7557)\n    oof = np.zeros(len(train_X))\n    predictions = np.zeros(len(test_X))\n    feature_importance_df = pd.DataFrame()\n    \n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_X, train_y)):\n        print(\"Fold {}\".format(fold_))\n        \n        trn_data = lgb.Dataset(\n            train_X.iloc[trn_idx][features], label=train_y.iloc[trn_idx], free_raw_data=True)\n        val_data = lgb.Dataset(\n            train_X.iloc[val_idx][features], label=train_y.iloc[val_idx], free_raw_data=True)\n\n        num_round = 1000\n        clf = lgb.train(lgbm_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000,\n                        early_stopping_rounds = 200)\n        oof[val_idx] = clf.predict(train_X.iloc[val_idx][features], num_iteration=clf.best_iteration)\n\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"Feature\"] = features\n        fold_importance_df[\"importance\"] = clf.feature_importance()\n        fold_importance_df[\"fold\"] = fold_ + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n        predictions += clf.predict(test_X, num_iteration=clf.best_iteration) \/ folds.n_splits\n        \n        del trn_data, val_data\n        gc.collect()    \n    return predictions, oof, feature_importance_df","2660d547":"params_dict = {}\nparams_dict['1JHC'] = param = {\n    'num_leaves': 150,\n    'objective': 'huber',\n    'colsample_bytree': 0.5,\n    'subsample': 0.9,\n    'eta': 0.05,\n    'n_estimators': 20000,\n    'metric': 'mae'\n}\n\nparams_dict['2JHC'] = param = {\n    'num_leaves': 200,\n    'objective': 'huber',\n    'colsample_bytree': 0.5,\n    'subsample': 0.9,\n    'eta': 0.05,\n    'n_estimators': 10000,\n    'metric': 'mae'\n}\n\nparams_dict['1JHN'] = {\n    'num_leaves': 200,\n    'objective': 'huber',\n    'colsample_bytree': 0.5,\n    'subsample': 0.9,\n    'eta': 0.05,\n    'n_estimators': 10000,\n    'metric': 'mae'\n}","b220c029":"predictions_type = np.zeros(len(test_df))\nfeature_importance_dfs = {}\noof_type = np.zeros(train_size)\n\nfor typ in  all_data.type.unique():\n    gc.collect()\n    print('Predicting type:', typ)\n    print(encoders['type'].classes_[typ])\n    if encoders['type'].classes_[typ] in params_dict.keys():\n        param = params_dict[encoders['type'].classes_[typ]]\n    else:\n        param = {\n    'num_leaves': 200,\n    'min_child_samples': 79,\n    'objective': 'huber',\n    'colsample_bytree': 0.75,\n    'subsample': 0.8,\n    'eta': 0.3,\n    'n_estimators': 10000,\n    'reg_alpha': 0.1,\n    'reg_lambda': 0.3,\n    'metric': 'mae'\n    }\n        \n    print(param)\n    type_df_train = all_data[:train_size][all_data.type == typ]\n    type_df_train_indices = type_df_train[type_df_train.type == typ].index\n    type_target = target.iloc[type_df_train_indices]\n\n    type_df_test = all_data[train_size:].reset_index().drop(columns=['index'])\n    type_df_test = type_df_test[type_df_test.type == typ]\n    type_df_test_indices = type_df_test[type_df_test.type == typ].index\n    \n    predictions, oof, type_feature_importance_df  = lgbm_model_oof(\n        type_df_train[features], type_target, type_df_test[features], 3, param)\n    \n    predictions_type[type_df_test_indices] += predictions\n    oof_type[type_df_train_indices] += oof\n    \n    feature_importance_dfs[typ] = type_feature_importance_df\n    print(\n        type_feature_importance_df.groupby(['Feature'])[['importance']].mean().sort_values(\n        \"importance\", ascending=False).head(10)\n         )\n\n    del type_df_train, type_target, type_df_test\n    gc.collect()","dc884c87":"types = encoders['type'].inverse_transform(all_data[:train_size]['type'])\n\nmaes = (target-oof_type).abs().groupby(types).mean()\nfloor=1e-9\nprint('Log of Mean absolute errors on VAL:')\nprint(np.log(maes.map(lambda x: max(x, floor))))\nprint('')\nprint('Overal Log Mae:')\nprint(np.log(maes.map(lambda x: max(x, floor))).mean())","dbf88bc9":"test_df['scalar_coupling_constant'] = predictions_type\ntest_df[['id', 'scalar_coupling_constant']].to_csv('submission.csv', index=False)","a7e2d0ec":"for key, value in feature_importance_dfs.items():\n    print('Feature Importance for type ', encoders['type'].classes_[key], ' :')\n    print(\n    value.groupby(['Feature'])[['importance']].mean().sort_values(\n        \"importance\", ascending=False).head(20)\n         )","8f1ac1a2":"# Aggregations by molecule and atom position","d58c328c":"# yukawa potential ","7bb377ef":"# Feature Engineering","69db5346":"# Feature Importances by Type","d0c250d8":"## Bonds count Features","dc82efef":"# Data Preparation","2c387e34":"# Model Training by types","36f9ab13":"# Atom - Bond Features","931f3ca8":"# Distances to Nearest neighbors","00a2f43e":"![image.png](attachment:image.png)\n\nFeature Engineering + LGBM Model ( Almost everything is run in kernel Memory )\n\nInspired from this awesome R Kernel from Giba - https:\/\/www.kaggle.com\/titericz\/giba-r-data-table-simple-features-1-17-lb\n\n","c533cd97":"# Different distance metrics"}}