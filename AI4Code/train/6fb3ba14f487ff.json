{"cell_type":{"5cc09718":"code","afef67cd":"code","dbc93484":"code","b89127a0":"code","96726545":"code","d5dccc77":"code","5d207a8a":"code","b2c1eb2a":"code","ffde7ffb":"code","834c469e":"code","51fa9f7c":"code","91edf9e2":"code","bfeb593b":"code","1ae5dba8":"code","aa5f141c":"code","4468002f":"code","deeaf1ec":"code","f1208832":"code","1f3f3a06":"markdown","d3294cbc":"markdown","59065f4f":"markdown","e9154ecc":"markdown","634db2e0":"markdown","ae3b60c9":"markdown","bac6b6e7":"markdown","953bccd5":"markdown","365bcd2a":"markdown","cc7c8eda":"markdown","ab0d18ab":"markdown","427d6d7e":"markdown","cf45f4c3":"markdown","91d62673":"markdown","c1a571cd":"markdown","dc73cabc":"markdown","dab24b14":"markdown","bf602a25":"markdown","ba72273c":"markdown","aec231f0":"markdown","badff256":"markdown","383c9722":"markdown"},"source":{"5cc09718":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","afef67cd":"cik_lookup = {\n    'AMZN': '0001018724',\n    'JNJ': '0000200406',   \n    'MCD': '0000063908',\n    'PEP': '0000077476',\n    'WMT': '0000104169'}","dbc93484":"!pip install ratelimit","b89127a0":"from ratelimit import limits, sleep_and_retry\nimport requests\n\nclass SecAPI(object):\n    SEC_CALL_LIMIT = {'calls': 10, 'seconds': 1}\n\n    @staticmethod\n    @sleep_and_retry\n    @limits(calls=SEC_CALL_LIMIT['calls'] \/ 2, period=SEC_CALL_LIMIT['seconds'])\n    def _call_sec(url):\n        return requests.get(url)\n\n    def get(self, url):\n        return self._call_sec(url).text\n\ndef print_ten_k_data(ten_k_data, fields, field_length_limit=50):\n    indentation = '  '\n\n    print('[')\n    for ten_k in ten_k_data:\n        print_statement = '{}{{'.format(indentation)\n        for field in fields:\n            value = str(ten_k[field])\n\n            if isinstance(value, str):\n                value_str = '\\'{}\\''.format(value.replace('\\n', '\\\\n'))\n            else:\n                value_str = str(value)\n\n            if len(value_str) > field_length_limit:\n                value_str = value_str[:field_length_limit] + '...'\n\n            print_statement += '\\n{}{}: {}'.format(indentation * 2, field, value_str)\n\n        print_statement += '},'\n        print(print_statement)\n    print(']')\n","96726545":"from bs4 import BeautifulSoup\n\nsec_api = SecAPI()\n\ndef get_sec_data(cik, doc_type, start=0, count=60):\n    newest_pricing_date = pd.to_datetime('2018-08-01')\n    rss_url = 'https:\/\/www.sec.gov\/cgi-bin\/browse-edgar?action=getcompany' \\\n        '&CIK={}&type={}&start={}&count={}&owner=exclude&output=atom' \\\n        .format(cik, doc_type, start, count)\n    sec_data = sec_api.get(rss_url)\n    return sec_data","d5dccc77":"def get_sec_data(cik, doc_type, start=0, count=60):\n    newest_pricing_data = pd.to_datetime('2020-01-01')\n    rss_url = 'https:\/\/www.sec.gov\/cgi-bin\/browse-edgar?action=getcompany' \\\n        '&CIK={}&type={}&start={}&count={}&owner=exclude&output=atom' \\\n        .format(cik, doc_type, start, count)\n    sec_data = sec_api.get(rss_url)\n    feed = BeautifulSoup(sec_data.encode('ascii'), 'xml').feed\n    entries = [\n        (\n            entry.content.find('filing-href').getText(),\n            entry.content.find('filing-type').getText(),\n            entry.content.find('filing-date').getText())\n        for entry in feed.find_all('entry', recursive=False)\n        if pd.to_datetime(entry.content.find('filing-date').getText()) <= newest_pricing_data]\n\n    return entries","5d207a8a":"import pprint\n\nexample_ticker = 'AMZN'\nsec_data = {}\n\nfor ticker, cik in cik_lookup.items():\n    sec_data[ticker] = get_sec_data(cik, '10-K')\n\npprint.pprint(sec_data[example_ticker][:5])","b2c1eb2a":"from tqdm import tqdm\n\nraw_fillings_by_ticker = {}\n\nfor ticker, data in sec_data.items():\n    raw_fillings_by_ticker[ticker] = {}\n    for index_url, file_type, file_date in tqdm(data, desc='Downloading {} Fillings'.format(ticker), unit='filling'):\n        if (file_type == '10-K'):\n            file_url = index_url.replace('-index.htm', '.txt').replace('.txtl', '.txt')            \n            \n            raw_fillings_by_ticker[ticker][file_date] = sec_api.get(file_url)\n\n\nprint('Example Document:\\n\\n{}...'.format(next(iter(raw_fillings_by_ticker[example_ticker].values()))[:1000]))","ffde7ffb":"import re\n\n\ndef get_documents(text):\n    \"\"\"\n    Extract the documents from the text\n\n    Parameters\n    ----------\n    text : str\n        The text with the document strings inside\n\n    Returns\n    -------\n    extracted_docs : list of str\n        The document strings found in `text`\n    \"\"\"\n\n    final_docs = []\n    start_regex = re.compile(r'<DOCUMENT>')\n    end_regex = re.compile(r'<\/DOCUMENT>')\n    \n    start_idx = [x.end() for x in re.finditer(start_regex, text)]\n    end_idx = [x.start() for x in re.finditer(end_regex, text)]\n    \n    for start_i, end_i in zip(start_idx, end_idx):\n        final_docs.append(text[start_i:end_i])\n    \n    \n    return final_docs","834c469e":"filling_documents_by_ticker = {}\n\nfor ticker, raw_fillings in raw_fillings_by_ticker.items():\n    filling_documents_by_ticker[ticker] = {}\n    for file_date, filling in tqdm(raw_fillings.items(), desc='Getting Documents from {} Fillings'.format(ticker), unit='filling'):\n        filling_documents_by_ticker[ticker][file_date] = get_documents(filling)\n\n\nprint('\\n\\n'.join([\n    'Document {} Filed on {}:\\n{}...'.format(doc_i, file_date, doc[:200])\n    for file_date, docs in filling_documents_by_ticker[example_ticker].items()\n    for doc_i, doc in enumerate(docs)][:3]))","51fa9f7c":"def get_document_type(doc):\n    \"\"\"\n    Return the document type lowercased\n\n    Parameters\n    ----------\n    doc : str\n        The document string\n\n    Returns\n    -------\n    doc_type : str\n        The document type lowercased\n    \"\"\"\n    \n    # Regex explaination : Here I am tryng to do a positive lookbehind\n    # (?<=a)b (positive lookbehind) matches the b (and only the b) in cab, but does not match bed or debt.\n    # More reference : https:\/\/www.regular-expressions.info\/lookaround.html\n    \n    type_regex = re.compile(r'(?<=<TYPE>)\\w+[^\\n]+') # gives out \\w\n    type_idx = re.search(type_regex, doc).group(0).lower()\n    return type_idx","91edf9e2":"ten_ks_by_ticker = {}\n\nfor ticker, filling_documents in filling_documents_by_ticker.items():\n    ten_ks_by_ticker[ticker] = []\n    for file_date, documents in filling_documents.items():\n        for document in documents:\n            if get_document_type(document) == '10-k':\n                ten_ks_by_ticker[ticker].append({\n                    'cik': cik_lookup[ticker],\n                    'file': document,\n                    'file_date': file_date})\n\nprint_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['cik', 'file', 'file_date'])","bfeb593b":"def remove_html_tags(text):\n    text = BeautifulSoup(text, 'html.parser').get_text()\n    \n    return text\n\n\ndef clean_text(text):\n    text = text.lower()\n    text = remove_html_tags(text)\n    \n    return text","1ae5dba8":"for ticker, ten_ks in ten_ks_by_ticker.items():\n    for ten_k in tqdm(ten_ks, desc='Cleaning {} 10-Ks'.format(ticker), unit='10-K'):\n        ten_k['file_clean'] = clean_text(ten_k['file'])\n\n\nprint_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['file_clean'])","aa5f141c":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\n\ndef lemmatize_words(words):\n    \"\"\"\n    Lemmatize words \n\n    Parameters\n    ----------\n    words : list of str\n        List of words\n\n    Returns\n    -------\n    lemmatized_words : list of str\n        List of lemmatized words\n    \"\"\"\n\n    wnl = WordNetLemmatizer()\n    lemmatized_words = [wnl.lemmatize(word, 'v') for word in words]\n    \n    return lemmatized_words","4468002f":"word_pattern = re.compile('\\w+')\n\nfor ticker, ten_ks in ten_ks_by_ticker.items():\n    for ten_k in tqdm(ten_ks, desc='Lemmatize {} 10-Ks'.format(ticker), unit='10-K'):\n        ten_k['file_lemma'] = lemmatize_words(word_pattern.findall(ten_k['file_clean']))","deeaf1ec":"print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['file_lemma'])","f1208832":"from nltk.corpus import stopwords\n\n\nlemma_english_stopwords = lemmatize_words(stopwords.words('english'))\n\nfor ticker, ten_ks in ten_ks_by_ticker.items():\n    for ten_k in tqdm(ten_ks, desc='Remove Stop Words for {} 10-Ks'.format(ticker), unit='10-K'):\n        ten_k['file_lemma'] = [word for word in ten_k['file_lemma'] if word not in lemma_english_stopwords]\n        \n        \nprint_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['file_lemma'])","1f3f3a06":"<h1 style=\"background-color: #ffd5cd; text-align: center\">Preprocessing 10-K documents<\/h1>","d3294cbc":"<h1 style=\"background-color: #ffd5cd; text-align: center\"> Download 10-K Statements<\/h1>","59065f4f":"With theses fillings downloaded, we want to break them into their associated documents. These documents are sectioned off in the fillings with the tags `<DOCUMENT>` for the start of each document and `<\/DOCUMENT>` for the end of each document. There's no overlap with these documents, so each `<\/DOCUMENT>` tag should come after the `<DOCUMENT>` with no `<DOCUMENT>` tag in between.","e9154ecc":"<h1 style=\"background-color: #ffd5cd; text-align: center\"> SEC EDGAR Database Overview<\/h1>","634db2e0":"<h1 style=\"background-color: #ffd5cd; text-align: center\">Get Document Types<\/h1>","ae3b60c9":"Now that we have all the documents, we want to find the 10-k form in this 10-k filing. The `get_document_type` function returns the type of document given. The document type is located on a line with the `<TYPE>` tag. For example, a form of type \"TEST\" would have the line `<TYPE>TEST`.","bac6b6e7":"<h1 style=\"background-color: #ffd5cd; text-align: center\"> Fetching Financial 10-K Reports via SEC API<\/h1>","953bccd5":"In this notebook, I aim to extract financial 10-K statements from the U.S. *Securities and Exchange Commision (SEC's)*, EDGAR database. In order to give an example, I would be extracting financial 10-K statements of 5 companies that belong to different sectors. These 5 stocks will constitute my diverse portfolio. This notebook is a work-in-progress. I would be leveraging the textual information from financial 10-K statements to perform NLP Analysis on them. This notebook covers extracting financial 10-K statements of companies and their preprocessing steps. The notebook is organized as follows:\n\n1. SEC EDGAR database overview.\n2. Fetching financial 10-K reports via SEC API\n3. Download 10-K statements\n4. Get documents.\n5. Get document types.\n6. Preprocessing 10-K documents\n    * Parsing via BeautifulSoup\n    * Lemmatization\n    * Stop-words removal\n7. Future work\n8. References","365bcd2a":"After fetching the web page as a response, we can perform web scraping with Python by leveraging the *BeautifulSoup* library and access the links that would help us download the 10-K filing reports. These document links will help us download the pure HTML version of the desired 10-K document, which we store in a dictionary against the corresponding stock\u2019s CIK number.","cc7c8eda":"<h2 style=\"background-color: #ffd5cd; text-align: center\">Lemmatization<\/h2>","ab0d18ab":"This notebook is a *work-in-progress*. In future, I intend to work on the following -\n1. Perform sentiment analysis on financial documents.\n2. Create a separate financial dataset out of the preprocessed 10-K documents.\n3. Leverage Loughran-McDonald financial sentiment word-list with this dataset.","427d6d7e":"1. [GitHub - AI for Trading](https:\/\/github.com\/purvasingh96\/AI-for-Trading\/tree\/master\/Term%202\/Projects\/Project%20-%205%20-%20NLP%20on%20Financial%20Statements)\n2. [SEC website](https:\/\/www.sec.gov\/edgar\/searchedgar\/companysearch.html)\n3. [Udacity's Nanodegree materials](https:\/\/www.udacity.com\/course\/ai-for-trading--nd880)","cf45f4c3":"<h2 style=\"background-color: #ffd5cd; text-align: center\">Stop-words Removal<\/h2>","91d62673":"The SEC has created a website called Electronic Data Gathering Analysis Retrieval (EDGAR) that gives us straightforward access to all the available financial statements and is leveraged by the proposed framework to extract 10-K reports of the selected stocks. Table 1 shows the list of companies whose 10-K statements I would be extracting.\n\n\n![image.png](attachment:image.png)\n\n\n\n\nBelow, I have explained the steps to follow in order to leverge SEC API to extract the 10-K filings. ","c1a571cd":"<h1 style=\"background-color: #ffd5cd; text-align: center\">Future Work<\/h1>","dc73cabc":"<h1 style=\"background-color: #ffd5cd; text-align: center\">References<\/h1>","dab24b14":"<h1 style=\"background-color: #ffd5cd; text-align: center\"> Get Documents<\/h1>","bf602a25":"Hitting the EDGAR REST Url that we formed above would redirect us to a web page that contains tabular data related to the company\u2019s type of filing document, document description, filing date, and file number. Figure 1 shows the EDGAR\u2019s search result dashboard after hitting the REST Url formed above.\n\n![image.png](attachment:image.png)<br>\n<p style=\"text-align:center;\"><b>Figure 1: EDGAR search results for Nike\u2019s 10-K documents prior-to 2020-01-01.<\/b>\n<\/p>\n","ba72273c":"<h2 style=\"background-color: #ffd5cd; text-align: center\">Parsing via BeautifulSoup<\/h2>","aec231f0":"<h1 style=\"background-color: #ffd5cd; text-align: center\"> Extracting Financial 10-K Statements from SEC'S EDGAR Database<\/h1>","badff256":"Let's pull the list using the `get_sec_data` function, then display some of the results. For displaying some of the data, we'll use Amazon as an example.","383c9722":"When companies file their 10-K reports to the SEC, it is gathered in the EDGAR database and is publicly available for investors to download or search for company-wise filing reports, we need to submit an HTTPS request to the following REST Url:\n\n<p style=\"text-align:center; color:blue;\">https:\/\/www.sec.gov\/cgi-bin\/browse-edgar<\/p>\n\nTo specify  the details of the report in which we are specifically interested in, we need to pass the following query parameters. To specify the details of the report in which we are specifically interested in, we need to pass the following query parameters:\n\n1. *CIK number (CIK)*: a unique numerical identifier assigned by the EDGAR system.\n2. *Report type (type)*:  type of financial report that we wish to query. Example 10-K, 10-Q, 14-K.\n3. *Prior-to date (dateb)*: EDGAR accepts a prior-to date that identifies the latest date in which we are interested.   \n4. *The number of reports (count)*: this quantity describes the number of filings up to the prior-to date.\n5. *Ownership (owner)*: The SEC requires filings from individuals who own significant amounts of the company\u2019s stock. Setting the owner parameter to exclude, EDGAR won\u2019t provide reports related to its director or officer ownership [1].\n\nAs an example, to download Nike\u2019s annual report before 2020, where Nike\u2019s CIK number is 0000320187, and 10-K denotes the type of annual reports, we would form the EDGAR Url as follows :\n\n<p style=\"text-align:center; color:blue\"> https:\/\/www.sec.gov\/cgi-bin\/browse-edgar?action=getcompany&CIK=0000320187&type=10-K&dateb=20200101&count=60&owner=exclude<\/p>\n\n \n "}}