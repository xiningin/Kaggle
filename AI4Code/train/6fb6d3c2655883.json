{"cell_type":{"f54cf8de":"code","6ff05708":"code","f9b3d184":"code","bfa8c9cc":"code","8f79531a":"code","eb4fc620":"code","795facbb":"code","064fc229":"code","225f7ba5":"code","e862988a":"code","b9593453":"code","fc92dff0":"code","b013b1eb":"code","ab2cab4f":"code","21cbe7c6":"code","13af6e0e":"code","adf6f28a":"code","77268d3a":"code","75083420":"code","60dde456":"code","f67423fb":"code","ef0f929a":"code","ddf5ca0b":"code","19b4c701":"code","7d0203cd":"code","71d6408d":"code","32902817":"code","94ffdab1":"code","2fa217d2":"code","7d5c6a57":"code","107ab8dc":"code","e0a4cd70":"code","56ba9128":"code","01ac4f3c":"code","23f7ce38":"code","bef0621c":"code","f7385f79":"code","04ef76a2":"code","80623a11":"code","aae67341":"code","3e8e0597":"code","14b29f30":"code","e0f314cb":"code","9fc818fc":"code","ab1e60e3":"code","70d00539":"code","458dac05":"code","8d477708":"code","9953921b":"code","a1c00183":"code","6703b767":"code","98133951":"code","e422fce1":"code","f54f6b61":"code","7626df68":"code","5b9e55d1":"markdown","4f043d23":"markdown","a8c5456b":"markdown","4368bd00":"markdown","aa3fa5f7":"markdown","483d5c53":"markdown","aff5f240":"markdown","c297b0a1":"markdown","5e3f18b0":"markdown","0770b54b":"markdown","71985abf":"markdown","db0e32d5":"markdown","721db4a3":"markdown","311914be":"markdown","91848d34":"markdown","7f1acabc":"markdown","b53d2b99":"markdown"},"source":{"f54cf8de":"# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n# data visualisation and manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\nimport missingno as msno\n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\n\n#import the necessary modelling algos.\n\n#classifiaction.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC,SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\n \n\n#regression\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\n\n#model selection\nfrom sklearn.model_selection import train_test_split,cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n#preprocessing\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,Imputer,LabelEncoder\n\n#evaluation metrics\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error # for regression\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  # for classification","6ff05708":"df=pd.read_csv(r'..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","f9b3d184":"df.shape","bfa8c9cc":"df.columns # the quality is the target variable that we have to predict.","8f79531a":"df.info()","eb4fc620":"df.isnull().sum() # no null or Nan values.","795facbb":"msno.matrix(df)  # just to visualize. no missing values.","064fc229":"df.describe(include='all')","225f7ba5":"#fixed acidity.\nsns.factorplot(data=df,kind='box',size=10,aspect=2.5) # the values are distributed over a very small scale.","e862988a":"# using a histogram.\nfig,axes=plt.subplots(5,5)\ncolumns=['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n       'pH', 'sulphates', 'alcohol', 'quality']\nfor i in range (5):\n    for j in range (5):\n        axes[i,j].hist(x=columns[i+j],data=df,edgecolor='#000000',linewidth=2,color='#ff4125')\n        axes[i,j].set_title('Variation of '+columns[i+j])\nfig=plt.gcf()\nfig.set_size_inches(18,18)\nfig.tight_layout()\n","b9593453":"#corelation matrix.\ncor_mat= df.corr()\nmask = np.array(cor_mat)\nmask[np.tril_indices_from(mask)] = False\nfig=plt.gcf()\nfig.set_size_inches(30,12)\nsns.heatmap(data=cor_mat,mask=mask,square=True,annot=True,cbar=True)","fc92dff0":"# can remove some highly corelated features but for now let us keep them.","b013b1eb":"def plot(feature_x,target='quality'):\n    sns.factorplot(x=target,y=feature_x,data=df,kind='bar',size=5,aspect=1)\n    sns.factorplot(x=target,y=feature_x,data=df,kind='violin',size=5,aspect=1)\n    sns.factorplot(x=target,y=feature_x,data=df,kind='swarm',size=5,aspect=1)\n    ","ab2cab4f":"# for fixed acidity.\nplot('fixed acidity','quality')","21cbe7c6":"# for alcohol.\nplot('alcohol','quality')","13af6e0e":"# similarly for other variables.","adf6f28a":"bins = (2, 6.5, 8)\ngroup_names = ['bad', 'good']\ndf['quality'] = pd.cut(df['quality'], bins = bins, labels = group_names)","77268d3a":"label_quality = LabelEncoder()","75083420":"#Bad becomes 0 and good becomes 1 \ndf['quality'] = label_quality.fit_transform(df['quality'])","60dde456":"x_train,x_test,y_train,y_test=train_test_split(df.drop('quality',axis=1),df['quality'],test_size=0.25,random_state=42)","f67423fb":"models=[LogisticRegression(),LinearSVC(),SVC(kernel='rbf'),KNeighborsClassifier(),RandomForestClassifier(),\n        DecisionTreeClassifier(),GradientBoostingClassifier(),GaussianNB()]\nmodel_names=['LogisticRegression','LinearSVM','rbfSVM','KNearestNeighbors','RandomForestClassifier','DecisionTree',\n             'GradientBoostingClassifier','GaussianNB']\n\nacc=[]\nd={}\n\nfor model in range(len(models)):\n    clf=models[model]\n    clf.fit(x_train,y_train)\n    pred=clf.predict(x_test)\n    acc.append(accuracy_score(pred,y_test))\n     \nd={'Modelling Algo':model_names,'Accuracy':acc}\nd","ef0f929a":"acc_frame=pd.DataFrame(d)\nacc_frame","ddf5ca0b":"sns.barplot(y='Modelling Algo',x='Accuracy',data=acc_frame)","19b4c701":"sns.factorplot(x='Modelling Algo',y='Accuracy',data=acc_frame,kind='point',size=4,aspect=3.5)","7d0203cd":"def func(x_train,x_test,y_train,y_test,name_scaler):\n    models=[LogisticRegression(),LinearSVC(),SVC(kernel='rbf'),KNeighborsClassifier(),RandomForestClassifier(),\n        DecisionTreeClassifier(),GradientBoostingClassifier(),GaussianNB()]\n    acc_sc=[]\n    for model in range(len(models)):\n        clf=models[model]\n        clf.fit(x_train,y_train)\n        pred=clf.predict(x_test)\n        acc_sc.append(accuracy_score(pred,y_test))\n     \n    acc_frame[name_scaler]=np.array(acc_sc)\n    ","71d6408d":"scalers=[MinMaxScaler(),StandardScaler()]\nnames=['Acc_Min_Max_Scaler','Acc_Standard_Scaler']\nfor scale in range(len(scalers)):\n    scaler=scalers[scale]\n    scaler.fit(df)\n    scaled_df=scaler.transform(df)\n    X=scaled_df[:,0:11]\n    Y=df['quality'].as_matrix()\n    x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.25,random_state=42)\n    func(x_train,x_test,y_train,y_test,names[scale])\n    \n    ","32902817":"acc_frame","94ffdab1":"# just to visualize the accuracies.\nsns.barplot(y='Modelling Algo',x='Accuracy',data=acc_frame)","2fa217d2":"sns.barplot(y='Modelling Algo',x='Acc_Min_Max_Scaler',data=acc_frame)","7d5c6a57":"sns.barplot(y='Modelling Algo',x='Acc_Standard_Scaler',data=acc_frame)","107ab8dc":"# preparing the features by using a StandardScaler as it gave better resluts.\nscaler=StandardScaler()\nscaled_df=scaler.fit_transform(df)\nX=scaled_df[:,0:11]\nY=df['quality'].as_matrix()\nx_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.25,random_state=42)","e0a4cd70":"params_dict={'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000],'penalty':['l1','l2']}\nclf_lr=GridSearchCV(estimator=LogisticRegression(),param_grid=params_dict,scoring='accuracy',cv=10)\nclf_lr.fit(x_train,y_train)","56ba9128":"clf_lr.best_params_","01ac4f3c":"clf_lr.best_score_ # the best accuracy obtained by Grid search on the train set.","23f7ce38":"pred=clf_lr.predict(x_test)\naccuracy_score(pred,y_test)","bef0621c":"l=[i+1 for i in range(50)]\nparams_dict={'n_neighbors':l,'n_jobs':[-1]}\nclf_knn=GridSearchCV(estimator=KNeighborsClassifier(),param_grid=params_dict,scoring='accuracy',cv=10)\nclf_knn.fit(x_train,y_train)\n","f7385f79":"clf_knn.best_score_","04ef76a2":"clf_knn.best_params_","80623a11":"pred=clf_knn.predict(x_test)\naccuracy_score(pred,y_test)   # actual accuarcy on our test set.","aae67341":"params_dict={'C':[0.001,0.01,0.1,1,10,100],'gamma':[0.001,0.01,0.1,1,10,100],'kernel':['linear','rbf']}\nclf=GridSearchCV(estimator=SVC(),param_grid=params_dict,scoring='accuracy',cv=10)\nclf.fit(x_train,y_train)","3e8e0597":"clf.best_score_","14b29f30":"clf.best_params_","e0f314cb":"# now tuning finally around these values of C and gamma and the kernel for further increasing the accuracy.\nparams_dict={'C':[0.90,0.92,0.96,0.98,1.0,1.2,1.5],'gamma':[0.90,0.92,0.96,0.98,1.0,1.2,1.5],'kernel':['linear','rbf']}\nclf_svm=GridSearchCV(estimator=SVC(),param_grid=params_dict,scoring='accuracy',cv=10)\nclf_svm.fit(x_train,y_train)","9fc818fc":"clf_svm.best_score_","ab1e60e3":"clf_svm.best_params_","70d00539":"pred=clf_svm.predict(x_test)\naccuracy_score(pred,y_test)   # actual accuarcy on our test set.","458dac05":"#### HENCE TILL NOW THE BEST ACCURACY IS GIVEN BY SVM WITH rbf KERNEL WITH  C=1.5 and gamma=0.90 .","8d477708":"params_dict={'n_estimators':[500],'max_features':['auto','sqrt','log2']}\nclf_rf=GridSearchCV(estimator=RandomForestClassifier(n_jobs=-1),param_grid=params_dict,scoring='accuracy',cv=10)\nclf_rf.fit(x_train,y_train)","9953921b":"clf_rf.best_score_","a1c00183":"clf_rf.best_params_","6703b767":"pred=clf_rf.predict(x_test)\naccuracy_score(pred,y_test)   # actual accuarcy on our test set.","98133951":"clf_gb=GridSearchCV(estimator=GradientBoostingClassifier(),cv=10,param_grid=dict({'n_estimators':[500]}))\nclf_gb.fit(x_train,y_train)","e422fce1":"clf_gb.best_score_","f54f6b61":"clf_gb.best_params_","7626df68":"pred=clf_gb.predict(x_test)\naccuracy_score(pred,y_test)","5b9e55d1":"# CHEERS GUYS::)","4f043d23":"**Modelling Algorithms**","a8c5456b":"###### 1.   LOGISTIC REGRESSION.","4368bd00":"# HENCE ON GIVEN PARAMETER TUNING THE SVM WITH rbf KERNEL GIVES THE HIGHEST ACCURACY OF 91.75%","aa3fa5f7":"**NOTE THAT THIS IS WITHOUT FEATURE SCALING. NOW SINCE FEATURES HAVE DIFFERENT SCALES LET US TRY TO DO FEATURE SCALING AND SEE THE IMPACT.**","483d5c53":"######   4. RANDOM FOREST.","aff5f240":"**NOW WE CAN VISUALIZE HOW QUALITY(ie Target) VARIES WITH DIFFERENT NUMERIC FEATURES.**","c297b0a1":"**Now we can move onto Univariate Analysis.**","5e3f18b0":"**NOW THIS CLEARLY SHOWS THE ACCUARCIES OF DIFFERENT MODELLING ALGOS ON USING DIFFERENT SCALERS.**\n\n1. Note that here the accuracies increase marginally on scaling.\n\n2. Also for this data, StandardScaling seems to give slightly better results than the MinMaxScaling.\n\n3. For some modelling algos there is a considerable increase in accuracies upon scaling the features like SVM, KNN wheras for others there isn't a considerable increase in accuracies upon scaling.","0770b54b":"######   2. KNN.","71985abf":"# RED WINE QUALITY. [Accuracy :: 91.75 %]","db0e32d5":"# THE END. [please star\/upvote if u find it helpful.]","721db4a3":"###### 3. SUPPORT VECTOR MACHINE (SVM)","311914be":"**INFERENCES FROM THE ABOVE HEAT MAP--**\n\n1. The quality of wine is highly related to volatile acidity.\n\n2. Also the quality of wine is highly corelated to alcohol.\n\n3. pH and citric acid\/ fixed acidity are highly inversely related as all of us know that acids have smaller pH values.\n\n4. Self Relation ie of a fetaure to itself is 1 as expected.\n\n5.  some other similar inferences can be drawn.","91848d34":"###### 5. GRADIENT BOOSTING.","7f1acabc":"**PARAMETER TUNING USING GridSearchCV.","b53d2b99":"**CORELATION MAP.**"}}