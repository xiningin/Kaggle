{"cell_type":{"305201b0":"code","1bf784bd":"code","22127ec0":"code","170233e3":"code","dfe05a77":"code","caaa154b":"code","5560fb5f":"code","abc82f3b":"code","36fd040b":"code","f31686ff":"code","df7717f5":"code","7d6bbef4":"code","64fd8563":"code","1d562b77":"code","e4dc3540":"code","11ad8f8e":"code","8240b1ab":"code","97e81d9b":"code","b8dc540f":"code","8baeeecf":"code","e28890df":"code","cacd686a":"code","6ae0ca38":"code","20d4f33f":"code","4a4020b3":"code","d134145c":"code","94e39b8b":"code","49a4dc18":"code","c9120e5f":"code","01f32972":"code","07a272b6":"code","4c1dfd14":"code","d1f0b33f":"code","9349b8da":"code","74aabfaf":"code","7b8f4e43":"code","48d3b9fd":"code","9eb80bec":"code","2aeda673":"code","9155a57a":"code","aa190955":"code","880409d2":"code","c064e182":"code","4a9fd6aa":"code","54e65e32":"code","d6997b69":"code","1dafb429":"code","4d5fa33d":"code","9ec11df6":"code","77a559a7":"code","052a23c7":"code","d0295544":"code","838dbd98":"code","97093e41":"code","a81cf97b":"code","69e78213":"code","f0fa8708":"code","8d8384cb":"code","d4d667a2":"code","4cf993fc":"code","218322dd":"code","6bcb0632":"code","5671f103":"code","f44b24e0":"code","fa5fcefd":"code","16580229":"code","50378669":"code","4f3f0d46":"code","1eb8ced6":"code","8eca8960":"code","2d08193d":"code","b3dfd6a0":"code","fdc74d0c":"code","8f4b306a":"code","6bdc3957":"code","a64335a1":"code","617b2f68":"code","33ca3db1":"code","70769a48":"code","2fc43ff3":"code","6c697bcc":"code","68c142a8":"code","dd058720":"code","789e12e7":"code","8c5cc5eb":"code","d264d560":"code","7a2dcadb":"code","7770e243":"code","7f004862":"code","7f0ab79e":"code","a569bd78":"code","bbdfa4ec":"code","d5877b9b":"code","0e4c39db":"code","65e8add9":"code","4241af56":"code","3a290e16":"code","01de5527":"code","216c7870":"code","0fa134fc":"code","d92428e1":"code","39f91d7d":"code","e5e99d3d":"code","0200f570":"code","d23c81a1":"code","8dcbe427":"code","65023049":"code","22842e7e":"code","e9572467":"code","7d52322a":"markdown","6602bb97":"markdown","00f164aa":"markdown","ae703236":"markdown","8b57c2c1":"markdown","347567c3":"markdown","37d23749":"markdown","662f8c48":"markdown","e07ae997":"markdown","46abb46c":"markdown","da447cea":"markdown","458e8df3":"markdown","ad7a045e":"markdown","00d96274":"markdown","8d0ecdbf":"markdown","fc596af8":"markdown","c880fdb4":"markdown"},"source":{"305201b0":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import normalize\nimport keras \nfrom keras.models import Sequential # For defining the type of the Neural Network\nfrom keras.layers import Dense  # For defining the layers of the Neural Network\nfrom keras.layers import Dropout # For Dropout Regularization\nfrom keras.optimizers import RMSprop # For the RMSprop optimizer \nfrom keras.callbacks import ReduceLROnPlateau # For Simulated Annealing\nfrom keras.wrappers.scikit_learn import KerasClassifier","1bf784bd":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ncombined = train.append(test, ignore_index=True, sort=False)","22127ec0":"train.info()","170233e3":"test.info()","dfe05a77":"combined.info()","caaa154b":"#The columns with missing values are as follows:\n# 1.Age\n# 2.Fare\n# 3.Cabin\n# 4.Embarked","5560fb5f":"combined.head()","abc82f3b":"# The following attributes will be dropped based on a cursory look.\n# PassengerId as it is not relevant for the survival of a passenger.\n# Name as it is not relevant for the survival of a passenger.\n# Ticket will be dropped as the passenger class and the Fare attribute makes the Ticket attribute redundant.\n# Cabin attribute will be dropped due to a very high number of missing values (77 percent)\n# We combine the training and test sets for simplicity in processing the data.\ncombined = combined.drop(['PassengerId','Name','Ticket','Cabin'],axis = 1)","36fd040b":"combined.head()","f31686ff":"#Creating the training set\ntrain1 = combined.iloc[:,1:]\ntrain1.head()","df7717f5":"#Now to determine the importance of each feature.\n#For Pclass\n#Checking the count of every value to check for bias.\npd.value_counts(train['Pclass'])","7d6bbef4":"sns.lmplot(x='Pclass', y='Survived', data=train)","64fd8563":"#We can see that Pclass plays an important role as the Pclass increases, the probability of survival decreases.","1d562b77":"#Now, let's check Gender\npd.value_counts(train['Sex'])","e4dc3540":"#Male values are 50 percent more.\n#Let's check the probability of survival of each class.\nsns.catplot(x=\"Sex\",y=\"Survived\",kind='bar',data=train)","11ad8f8e":"#The probability of female survival is more. The sex attribute therefore, will play an important role in this problem.","8240b1ab":"#Now let's look at the age attribute.\ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 0) & (train[\"Age\"].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 1) & (train[\"Age\"].notnull())], ax =g, color=\"Green\", shade= True)\ng.set_xlabel(\"Age\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","97e81d9b":"#We can see that increased number of babies survived as compared to other ages.","b8dc540f":"#Let's take at the number of siblings\npd.value_counts(train['SibSp'])","8baeeecf":"#We can see a bias here. Almost 70 percent of the training data has 0 SibSp value.\n#Let's visualize a bar graph to check the importance of the SibSp attribute.\ng  = sns.factorplot(x=\"SibSp\",y=\"Survived\",data=train,kind=\"bar\")","e28890df":"#So as the number of siblings increases, the probability of survival decreases.\n#However, due to increased bias, the importance of SibSp does not come close to the previous attributes.\n#Now, let's have a look at the number of parent\/child attribute.\npd.value_counts(train['Parch'])","cacd686a":"g  = sns.factorplot(x=\"Parch\",y=\"Survived\",data=train,kind=\"bar\")","6ae0ca38":"#The probability of survival decreases as the family members increases.\n#However, the deviation for the 3 parch value is pretty high.\n#The importance of this attribute is a bit less than the sibling attribute.","20d4f33f":"#Now for fare.\npd.value_counts(train['Fare'])\ng = sns.distplot(train['Fare'])\ntrain['Fare'].skew()","4a4020b3":"# We can see that the data is negatively skewed. Therefore, we will apply the logarithmic transformation.","d134145c":"# Now, let's check for the embarked attribute\npd.value_counts(train['Embarked'])","94e39b8b":"# Pretty high bias for Embarked attribute.","49a4dc18":"g  = sns.factorplot(x=\"Embarked\",y=\"Survived\",data=train,kind=\"bar\")","c9120e5f":"# The passengers from Cherbourg have a higher survival rate.\n# Maybe more number of passengers from there have a  first class ticket.\n# Embarked attribute might be more important than parents and sbilings but less than the first three attributes.","01f32972":"combined.info()","07a272b6":"# For Age\n# We will try to use Regression to predict the missing Age values.\n# To determine the variables to be used in predicting the Age, we will plot a heatmap.\n# We need to encode the Sex attribute before using the heatmap.\nlabelencoder = LabelEncoder()\ncombined.iloc[:, 2] = labelencoder.fit_transform(combined.iloc[:, 2])","4c1dfd14":"combined.head()","d1f0b33f":"g = sns.heatmap(combined[[\"Age\",\"SibSp\",\"Parch\",\"Sex\",\"Pclass\"]].corr(),annot=True, fmt = \".2f\")","9349b8da":"# We can see that SibSp, Pclass have higher magnitudes of co-relation with Age.\n# Let's try to predict the missing Age values by using SibSp and Pclass values","74aabfaf":"combined.head()","7b8f4e43":"Age_pred = combined.iloc[:,1:5]\nAge_pred = Age_pred.drop([\"Sex\"],axis = 1)\nAge_pred.head()","48d3b9fd":"# Our model will train on the non-null rows of Age attribute and predict the null rows of the same.\n# However, for testing the model, we split the training set initially. ","9eb80bec":"# Creating the training set and the test set.\nindex = []\nAge_Test =  []\n#Here we create the test set.\nfor x in range(len(Age_pred)):\n    if math.isnan(Age_pred.iloc[x,1]) == True:\n        index.append(x)\n        Age_Test.append(Age_pred.iloc[x,:])\n","2aeda673":"Age_Test = pd.DataFrame(Age_Test)\nAge_Test.head()\nAge_Test = Age_Test.drop([\"Age\"],axis = 1)","9155a57a":"#Here we create the training set\nfor x in range(len(index)):\n    Age_pred = Age_pred.drop(index[x])","aa190955":"Age_pred.info()","880409d2":"X = Age_pred.drop([\"Age\"],axis = 1)\ny = Age_pred.iloc[:,1]","c064e182":"X.head()","4a9fd6aa":"y.head()","54e65e32":"Age_X_train,Age_X_test,Age_y_train,Age_y_test = train_test_split(X,y, test_size=0.2, random_state = 0)","d6997b69":"# I hvae used the xgboost model. I have optimized the following parameters by using a simple for loop.\n# 1. n_estimators\n# 2. max_depth\n# 3. learning_rate","1dafb429":"regressor = XGBRegressor()\nregressor.fit(Age_X_train,Age_y_train)\ny_pred = regressor.predict(Age_X_test)\nrms = math.sqrt(mean_squared_error(Age_y_test, y_pred))\nrms","4d5fa33d":"rms = []\nfor x in np.arange(0,0.1,0.01):\n    regressor = XGBRegressor(learning_rate = x, max_depth = 1)\n    regressor.fit(Age_X_train,Age_y_train)\n    y_pred = regressor.predict(Age_X_test)\n    rms.append(math.sqrt(mean_squared_error(Age_y_test, y_pred)))\n    ","9ec11df6":"regressor = XGBRegressor(max_depth = 1, learning_rate = 0.03)\nregressor.fit(Age_X_train,Age_y_train)\ny_pred = regressor.predict(Age_X_test)\nrms = math.sqrt(mean_squared_error(Age_y_test, y_pred))\nrms","77a559a7":"# After optimization, the rms value has decreased from 12.5 to 12.1.","052a23c7":"# Now let's try other models.\n#SVR\nfrom sklearn.svm import SVR\nregressor = SVR(kernel = 'rbf')\nregressor.fit(Age_X_train,Age_y_train)\ny_pred = regressor.predict(Age_X_test)\nrms = math.sqrt(mean_squared_error(Age_y_test, y_pred))\nrms","d0295544":"#Random Forest\nfrom sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators = 5000, random_state = 0)\nregressor.fit(Age_X_train,Age_y_train)\ny_pred = regressor.predict(Age_X_test)\nrms = math.sqrt(mean_squared_error(Age_y_test, y_pred))\nrms","838dbd98":"# XGBoost works best.\n# We'll use XGBoost\n# Predicting the missing values\nregressor = XGBRegressor(max_depth = 1, learning_rate = 0.03)\nregressor.fit(X,y)\ny_pred = regressor.predict(Age_Test)","97093e41":"for x in range(len(y_pred)):\n    y_pred[x] = int(y_pred[x])","a81cf97b":"y_pred","69e78213":"# Getting the column index of Age\ncombined.columns.get_loc(\"Age\")","f0fa8708":"count = 0\nfor x in range(len(combined)):\n    if math.isnan(combined.iloc[x,3]):\n        combined.iloc[x,3] = y_pred[count]\n        count = count+1","8d8384cb":"combined.info()","d4d667a2":"# For Parch and Fare, we will just replace the missing values by the most frequent and the average respectively due to the extremely small number of missing values","4cf993fc":"# For Fare\ncombined.columns.get_loc(\"Fare\")\nimputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\nimputer = imputer.fit(combined.iloc[:, 6:7])\ncombined.iloc[:, 6:7] = imputer.transform(combined.iloc[:, 6:7])\ncombined[\"Fare\"].isnull().sum()","218322dd":"# For Embarked\ncombined.columns.get_loc(\"Embarked\")\ntemp = combined[\"Embarked\"].isnull()\nfor x in range(len(combined)):\n    if temp[x] == True:\n        combined.iloc[x,7] = \"S\"\ncombined[\"Embarked\"].isnull().sum()","6bcb0632":"combined.info()","5671f103":"X = combined.drop([\"Survived\"],axis = 1)","f44b24e0":"y = combined.iloc[0:891,0]","fa5fcefd":"len(y)","16580229":"X.info()","50378669":"X.head()","4f3f0d46":"X.iloc[:, 6] = labelencoder.fit_transform(X.iloc[:, 6])","1eb8ced6":"X.head()","8eca8960":"# Now, we OneHotEncode the Sex and the Embarked attribute.","2d08193d":"# For Sex\nonehotencoder = OneHotEncoder(categorical_features = [1])\nX = onehotencoder.fit_transform(X).toarray()","b3dfd6a0":"X = pd.DataFrame(X)\nX.head()","fdc74d0c":"# For embarked\nonehotencoder = OneHotEncoder(categorical_features = [7])\nX = onehotencoder.fit_transform(X).toarray()","8f4b306a":"X = pd.DataFrame(X)","6bdc3957":"X.head()","a64335a1":"X.iloc[:,9].describe()","617b2f68":"X.iloc[:,9] = np.log(X.iloc[:,9] + 1) # + 1 to avoid divide by zero error.","33ca3db1":"g = sns.distplot(X.iloc[:,9])\nX.iloc[:,9].skew()","70769a48":"# We shall now normalize the values to obtain better results.\nX.iloc[:,9] = normalize(X.iloc[:,-1:], axis=0, norm='max')\n","2fc43ff3":"g = sns.distplot(X.iloc[:,9])\nX.iloc[:,9].skew()","6c697bcc":"X.head()","68c142a8":"X.info()","dd058720":"# Now, we separate the combined set back to the training and the test set.","789e12e7":"train_final = X.iloc[0:891,:]\ntrain_final.info()","8c5cc5eb":"test_final = X.iloc[891:,:]\ntest_final.info()\n# Dropping the Survived attribute from the test set\n#test_final = test_final.drop(test_final.columns[0], axis = 1)\n#test_final.info()","d264d560":"y.head()","7a2dcadb":"# The dataset is ready.\n# Since, this is a classification problem, I am going to use a Artificial Neural Network using the Keras Library. ","7770e243":"# Let us split X and y into training set and test set.\nX_train, X_test, y_train, y_test = train_test_split(train_final,y, test_size=0.2, random_state = 0)","7f004862":"#I have used the used following formula for the number of nodes in the hidden layer.\n#Nh=Ns\/(\u03b1\u2217(Ni+No))\n#Ni  = number of input neurons.\n#No = number of output neurons.\n#Ns = number of samples in training data set.\n#\u03b1 = an arbitrary scaling factor usually 2-10.\nNh = int(891\/32)","7f0ab79e":"classifier_arr = []\nfor x in range(10,100,10):\n    # Initialising the ANN\n    classifier = Sequential()\n\n    # Adding the input layer and the first hidden layer\n    classifier.add(Dense(units = Nh, kernel_initializer = 'uniform', activation = 'relu', input_dim = 10))\n    #Adding dropout regularization to prevent overfitting.\n    classifier.add(Dropout(0.01))\n\n    # Adding the second hidden layer\n    classifier.add(Dense(units = Nh, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dropout(0.01))\n\n    # Adding the output layer\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.add(Dropout(0.01))\n\n    # Define the optimizer\n    optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n    # Compile the model\n    classifier.compile(optimizer = optimizer , loss = \"binary_crossentropy\", metrics=[\"accuracy\"])\n    \n    learning_rate_reduction = ReduceLROnPlateau(monitor='acc', \n                                                patience=3, \n                                                verbose=1, \n                                                factor=0.5, \n                                                min_lr=0.00001)\n\n    # Fitting the ANN to the Training set\n    history = classifier.fit(X_train, y_train, batch_size = 25, epochs = 45, callbacks = [learning_rate_reduction])\n    classifier_arr.append(classifier)\n    # I have tried keeping the number of epochs as 1000 ,500 , 100 , 50. \n    # However, after observation I can see that after the learning rate is reduced by the annealer, the accuracy remains constant and then increases with further reduction in learning rate.\n    # So, I have observed the epoch with  the final reduction in learning rate and have kept the total number of epochs uptill that  only.","a569bd78":"# Predicting the results.\ny_pred_arr = []\nfor x in range(len(classifier_arr)):\n    y_pred = classifier_arr[x].predict(X_test)\n    y_pred = (y_pred > 0.5)\n    y_pred = y_pred.astype(int)\n    y_pred_arr.append(y_pred)\n    ","bbdfa4ec":"# Let us see the Confusion Matrix.\n# Creating the Confusion Matrix.\naccuracy_arr = []\nfor x in range(len(classifier_arr)):\n    \n    confusion_mtx = confusion_matrix(y_test, y_pred_arr[x]) \n    accuracy = (confusion_mtx[0][0] + confusion_mtx[1][1]) \/ sum(sum(confusion_mtx))\n    accuracy_arr.append(accuracy)\n#Visualise the Confusion Matrix \n#sns.heatmap(confusion_mtx, annot=True, fmt='d')","d5877b9b":"accuracy_arr.index(max(accuracy_arr))","0e4c39db":"# However, the reduction of epochs gave an accuracy of 0.77511.\n# So, I decided to increase the number of epochs.","65e8add9":"Nh = int(891\/32)\n# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = Nh, kernel_initializer = 'uniform', activation = 'relu', input_dim = 10))\n#Adding dropout regularization to prevent overfitting.\nclassifier.add(Dropout(0.01))\n\n# Adding the second hidden layer\nclassifier.add(Dense(units = Nh, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dropout(0.01))\n\n# Adding the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\nclassifier.add(Dropout(0.01))\n\n# Define the optimizer\noptimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n# Compile the model\nclassifier.compile(optimizer = optimizer , loss = \"binary_crossentropy\", metrics=[\"accuracy\"])\n\n#In order to make the optimizer converge faster and closest to the global minimum of the loss function, I have used an annealing method of the learning rate (LR).\n#The LR is the step by which the optimizer walks through the 'loss landscape'. The higher LR, the bigger are the steps and the quicker is the convergence. However the sampling is very poor with an high LR and the optimizer could probably fall into a local minima.\n#Its better to have a decreasing learning rate during the training to reach efficiently the global minimum of the loss function.\n#To keep the advantage of the fast computation time with a high LR, I have decreased the LR dynamically every X steps (epochs) if necessary (when accuracy is not improved).\n#With the ReduceLROnPlateau function from Keras.callbacks, I have choosen to reduce the LR by half if the accuracy is not improved after 3 epochs.\n# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='acc', \n                                                patience=3, \n                                                verbose=1, \n                                                factor=0.5, \n                                                min_lr=0.00001)\n\n# Fitting the ANN to the Training set\nclassifier.fit(train_final, y, batch_size = 25, epochs = 1610, callbacks = [learning_rate_reduction])","4241af56":"# By a lot of trial and error in the number of epochs, I got an accuracy of 0.79904","3a290e16":"# Making the predictions.\ny_pred = classifier.predict(test_final)\ny_pred = (y_pred > 0.5)\ny_pred = y_pred.astype(int)","01de5527":"# Making the final submission file.\nfinal = pd.DataFrame(y_pred)\nfinal['PassengerId'] = pd.Series(data = np.arange(892,1310), index=final.index)\nfinal.columns = ['Survived','PassengerId']\ncolumnsTitles=[\"PassengerId\",\"Survived\"]\nfinal=final.reindex(columns=columnsTitles)\n\n# Exporting the dataframe\nfinal.to_csv('Predictions_ANN.csv', index = False)","216c7870":"y_pred_arr = []\nfrom xgboost import XGBClassifier\nfor x in np.arange(0,0.1,0.01):\n    classifier = XGBClassifier(n_estimators = 200, max_depth = 8, learning_rate = x)\n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n    y_pred_arr.append(y_pred)\n    ","0fa134fc":"acc = []\nfrom sklearn.metrics import accuracy_score\nfor x in range(len(y_pred_arr)):\n     acc.append(accuracy_score(y_test, y_pred_arr[x]))","d92428e1":"acc.index(max(acc))\nmax(acc)","39f91d7d":"classifier = XGBClassifier(n_estimators = 200, max_depth = 8)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\naccuracy_score(y_test, y_pred)","e5e99d3d":"classifier = XGBClassifier(n_estimators = 200, max_depth = 8)\nclassifier.fit(train_final, y)\ny_pred = classifier.predict(test_final)\ny_pred = y_pred.astype(int)\n# XGBoost Classifier gave an accuracy of 0.76076","0200f570":"y_pred_arr = []\nfrom sklearn.ensemble import RandomForestClassifier\nfor x in np.arange(100,1000,100):\n    classifier = RandomForestClassifier(n_estimators = x, random_state = 0)\n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n    y_pred_arr.append(y_pred)","d23c81a1":"acc = []\nfrom sklearn.metrics import accuracy_score\nfor x in range(len(y_pred_arr)):\n     acc.append(accuracy_score(y_test, y_pred_arr[x]))","8dcbe427":"acc.index(max(acc))\n#max(acc)","65023049":"classifier = RandomForestClassifier(n_estimators = 100, random_state = 0)\nclassifier.fit(train_final, y)\ny_pred = classifier.predict(test_final)\n#accuracy_score(y_test, y_pred)\n","22842e7e":"y_pred = y_pred.astype(int)\n# Random Forest gave an accuracy of 0.75598.","e9572467":"# Let's try Support Vector Classification\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(train_final, y)\ny_pred = classifier.predict(test_final)\ny_pred = y_pred.astype(int)\n# Support Vector Classification gave an accuracy of 0.75598 too.","7d52322a":"## 1. Importing the libraries and the Dataset","6602bb97":"[<h2>5. Comparing various models<\/h2>](#cm)","00f164aa":"## 2. Data Analysis","ae703236":"# Titanic Survival Predictions","8b57c2c1":"## Table of Contents:","347567c3":"[<h2>4. Model Development, Evaluation and Tuning<\/h2>](#md)","37d23749":"[<h2>2. Data Analysis<\/h2>](#da)","662f8c48":"<a id='cm'><\/a>","e07ae997":"<a id='il'><\/a>","46abb46c":"## 4. Model Development, Evaluation and Tuning","da447cea":"<a id='da'><\/a>","458e8df3":"<a id='dt'><\/a>","ad7a045e":"[<h2>3. Data Transformation<\/h2>](#dt)","00d96274":"<a id='md'><\/a>","8d0ecdbf":"[<h2>1. Importing the libraries and the Dataset<\/h2>](#il)","fc596af8":"## 3. Data Tranformation","c880fdb4":"## 5. Comparing various models."}}