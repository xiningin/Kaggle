{"cell_type":{"e905cac0":"code","7e2e876c":"code","44487bc9":"code","ec5fd29a":"code","ca80369e":"code","709c80ab":"code","3b1eea98":"code","66974e70":"code","925de1c8":"code","9f296b1b":"code","5fc12fc3":"code","327f233b":"code","af7373ac":"code","9e34b8f7":"code","deba3e2e":"code","a816ee5d":"code","c75fe4e9":"code","cd040a69":"code","4e7af265":"code","333d08e5":"code","8c6b52e2":"code","ca209344":"code","be08dcac":"code","f4a533c0":"code","6c653d0c":"code","08383cf5":"code","ea99359f":"code","91f5a4ae":"code","3ae27a73":"code","a2d121a1":"code","05c36542":"code","572ee971":"code","437fc123":"code","a72b80d2":"code","791b1032":"code","6e640afe":"code","635135c1":"code","fbc4c876":"code","e84b1f9c":"code","c7a1d3c7":"code","857670c1":"code","90373563":"code","c52c3b89":"code","f278a544":"code","e9e78fec":"markdown","1db3e30d":"markdown","dab0e7d0":"markdown","3360ef56":"markdown","1d751e27":"markdown"},"source":{"e905cac0":"!wget https:\/\/raw.githubusercontent.com\/DeFacto\/SimpleLSTM\/master\/data\/fever\/fever_simple_claim\/fever_simple_claim.json","7e2e876c":"import json\nimport pandas as pd\nimport datetime as dt","44487bc9":"with open('fever_simple_claim.json','r') as my_file:\n    fever_simple_claim = json.load(my_file)\nlen(fever_simple_claim)","ec5fd29a":"for count_l, FSC in enumerate(fever_simple_claim):\n    print(\"%4i\"%count_l,'-'*80)\n    for k, V in FSC.items():\n        print(k)\n        print(len(V), type(V))\n        if type(V)==list:\n            for v in V:\n                print((v,))\n        else:\n            print(V)\n        print()\n    print()\n    if count_l>=0:\n        break","ca80369e":"DATA_list = []\nfor count_l, FSC in enumerate(fever_simple_claim):\n    c = FSC['claim']\n    DATA_list.append(['claim',c])\n    for s in FSC['sentences']:\n        DATA_list.append(['sentences',s])\nlen(DATA_list)","709c80ab":"DATA = pd.DataFrame(DATA_list, columns=['type','sentence'])\nlen(DATA)","3b1eea98":"DATA.groupby('type').count()","66974e70":"!pip install -U sentence_transformers","925de1c8":"from sentence_transformers import SentenceTransformer","9f296b1b":"model = SentenceTransformer('bert-base-nli-mean-tokens')","5fc12fc3":"X, Y = [], []\nfor x, y in DATA[['sentence','type']].values.tolist():\n    X.append(x)\n    Y.append(y)\nlen(X), len(Y)","327f233b":"!pip install -U sklearn","af7373ac":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support, classification_report\nfrom sklearn.svm import SVC","9e34b8f7":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\nlen(X_train), len(X_test), len(Y_train), len(Y_test)","deba3e2e":"from collections import Counter\nprint(len(Y_train), len(Y_test))\nC_train = Counter(Y_train)\nC_test = Counter(Y_test)\nprint(C_train)\nprint(C_test)\nprint(\"%.2f\"%(100*C_train['claim'] \/ sum(list(C_train.values()))), \"%\", \"claims in train set!\")\nprint(\"%.2f\"%(100*C_test['claim'] \/ sum(list(C_test.values()))), \"%\", \"claims in test set!\")","a816ee5d":"X_train_embedds = model.encode(X_train)\nlen(X_train_embedds)","c75fe4e9":"clf = SVC(kernel='rbf', gamma='auto')","cd040a69":"print(dt.datetime.now())\nclf.fit(X_train_embedds, Y_train)\nprint(dt.datetime.now())","4e7af265":"X_test_embedds = model.encode(X_test)\nlen(X_test_embedds)","333d08e5":"Y_pred = clf.predict(X_test_embedds)\nlen(Y_pred)","8c6b52e2":"p,r,f1,s = precision_recall_fscore_support(y_true=Y_test, y_pred=Y_pred, average='macro', warn_for=tuple())\nprint(\"%.3f Precision\\n%.3f Recall\\n%.3f F1\"%(p,r,f1))","ca209344":"print(classification_report(y_true=Y_test, y_pred=Y_pred))","be08dcac":"for i, y_pred in enumerate(Y_pred):\n    if y_pred=='claim':\n        print(X_test[i])\n        print()\n    if i>=10:\n        break","f4a533c0":"!pip install -U stanza","6c653d0c":"import stanza\nimport hashlib","08383cf5":"stanza.download('en')","ea99359f":"nlp = stanza.Pipeline(processors='tokenize', lang='en', use_gpu=True)","91f5a4ae":"!git clone https:\/\/github.com\/COVIEWED\/coviewed_web_scraping","3ae27a73":"!pip install -r coviewed_web_scraping\/requirements.txt","a2d121a1":"#EXAMPLE_URL = 'https:\/\/www.euronews.com\/2020\/04\/01\/the-best-way-prevent-future-pandemics-like-coronavirus-stop-eating-meat-and-go-vegan-view'\nEXAMPLE_URL = 'https:\/\/edition.cnn.com\/2020\/03\/04\/health\/debunking-coronavirus-myths-trnd\/'\nprint(EXAMPLE_URL)","05c36542":"!cd coviewed_web_scraping\/ && python3 src\/scrape.py -u={EXAMPLE_URL}","572ee971":"txt_files = [f for f in os.listdir(data_path) if f.endswith('.txt')]\nlen(txt_files), txt_files","437fc123":"data_path = 'coviewed_web_scraping\/data\/'\nfname = txt_files[0]\nprint(fname)\nwith open(os.path.join(data_path, fname), 'r') as my_file:\n    txt_data = my_file.readlines()\ntxt_data = [line.strip() for line in txt_data if line.strip()]\nlen(txt_data)","a72b80d2":"article_url = txt_data[0]\nprint(article_url)\narticle_published_datetime = txt_data[1]\nprint(article_published_datetime)","791b1032":"article_title = txt_data[2]\nprint(article_title)","6e640afe":"article_text = \"\\n\\n\".join(txt_data[3:])\nprint(article_text)","635135c1":"ALL_SENTENCES = []\ntxt = [p.strip() for p in article_text.split('\\n') if p.strip()]\nfile_id = fname.split('.')[0]\nprint(file_id)\nprint()\nfor i, paragraph in enumerate(txt):\n    doc = nlp(paragraph)\n    for sent in doc.sentences:\n        S = ' '.join([w.text for w in sent.words])\n        sH = hashlib.md5(S.encode('utf-8')).hexdigest()\n        print(sH)\n        print(S)\n        print()\n        ALL_SENTENCES.append([file_id, sH, S])","fbc4c876":"fname = file_id+'_sentences.tsv'\nprint(fname)\nAS = pd.DataFrame(ALL_SENTENCES, columns=['file_id','sentenceHash','sentence'])\nlen(AS)","e84b1f9c":"sentences = [sentence for sentence in AS.sentence.values.tolist()]\nlen(sentences)","c7a1d3c7":"embedded_sentences = model.encode(sentences)\nlen(embedded_sentences)","857670c1":"predictions = clf.predict(embedded_sentences)\nlen(predictions)","90373563":"len(predictions), len(sentences)","c52c3b89":"Counter(predictions)","f278a544":"for i, prediction in enumerate(predictions):\n    if prediction=='claim':\n        print(sentences[i])\n        print()","e9e78fec":"based on http:\/\/jens-lehmann.org\/files\/2019\/epia_simple_lstm.pdf\n    \nand https:\/\/github.com\/DeFacto\/SimpleLSTM\/tree\/master\/data\/fever\/fever_simple_claim","1db3e30d":"---","dab0e7d0":"---","3360ef56":"# Project COVIEWED","1d751e27":"### Apply model on news article"}}