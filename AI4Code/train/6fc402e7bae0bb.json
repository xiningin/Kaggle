{"cell_type":{"f2622830":"code","dcdace43":"code","f91328ec":"code","c9cefa0c":"code","fe725e95":"code","d2a1812b":"code","ed119845":"code","6d2f6d89":"code","2143b562":"code","510784e4":"code","d5a1c5c9":"markdown","1617d36b":"markdown","4c86c809":"markdown","3aa35bb3":"markdown","44df4810":"markdown","8b6b3073":"markdown","f369f43e":"markdown","b21b357c":"markdown","d45a8ba1":"markdown","6138e2e6":"markdown"},"source":{"f2622830":"!rm -rf ..\/output  # Clear previous output","dcdace43":"import tensorflow as tf\nfrom tensorflow.data.experimental import AUTOTUNE\nimport glob\nimport imageio\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport PIL\nfrom tensorflow.keras import layers\nimport time\nfrom IPython import display\n\nBATCH_SIZE = 128\nIMAGE_SIZE = 120  # reduce this to increase performance\nIMAGE_CHANNELS = 3  # can be 3 (RGB) or 1 (Grayscale)\nLATENT_SPACE_DIM = 100  # dimensions of the latent space that is used to generate the images\n\nassert IMAGE_SIZE % 4 == 0\n\n\ndef preprocess(file_path):\n    # load the raw data from the file as a string\n    img = tf.io.read_file(file_path)\n    # load the image as uint8 array and transform to grayscale\n    img = tf.image.decode_jpeg(img, channels=IMAGE_CHANNELS)\n    # resize the image to the desired size\n    img = tf.image.resize(img, [IMAGE_SIZE, IMAGE_SIZE])\n    # transform the color values from [0, 255] to [-1, 1]. The division changes the datatype to float32\n    img = (img - 127.5) \/ 127.5\n    return img\n\n\ndef filter(img):\n    return img[0, 0, 0] == -1  # discard white bg images (estimate by the R channel of the top left pixel)\n\n\ndef configure_for_performance(ds):\n    ds = ds.cache()\n    ds = ds.filter(filter)\n    ds = ds.shuffle(buffer_size=1000)\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n    return ds\n\n\nlist_ds = tf.data.Dataset.list_files(str('..\/input\/pokemon-images-and-types\/images\/*\/*'), shuffle=True)  # Get all images from subfolders\ntrain_dataset = list_ds.take(-1)\n# Set `num_parallel_calls` so multiple images are loaded\/processed in parallel.\ntrain_dataset = train_dataset.map(preprocess, num_parallel_calls=AUTOTUNE)\ntrain_dataset = configure_for_performance(train_dataset)","f91328ec":"def make_generator_model():\n    model = tf.keras.Sequential()\n    \n    n = IMAGE_SIZE \/\/ 4\n    \n    model.add(layers.Dense(n * n * 256, use_bias=False, input_shape=(LATENT_SPACE_DIM,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Reshape((n, n, 256)))\n\n    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(IMAGE_CHANNELS, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n\n    return model","c9cefa0c":"def make_discriminator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=(IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS)))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n\n    return model","fe725e95":"generator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n\ndiscriminator = make_discriminator_model()\ngenerator = make_generator_model()","d2a1812b":"# This method returns a helper function to compute cross entropy loss\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\ndef discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)","ed119845":"# Notice the use of `tf.function`\n# This annotation causes the function to be \"compiled\".\n@tf.function\ndef train_step(images):\n    noise = tf.random.normal([BATCH_SIZE, LATENT_SPACE_DIM])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated_images = generator(noise, training=True)  # training=True is important, sicne Dropout and BatchNorm behave differently during inference\n\n        real_output = discriminator(images, training=True)\n        fake_output = discriminator(generated_images, training=True)\n\n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output, fake_output)\n\n        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))","6d2f6d89":"num_examples_to_generate = 16\n# We will reuse this seed overtime (so it's easier)\n# to visualize progress in the animated GIF)\nseed = tf.random.normal([num_examples_to_generate, LATENT_SPACE_DIM])\n\ndef train(dataset, epochs, save_after):\n    \n    generate_and_save_images(generator,\n                       0,\n                       seed)\n    \n    for epoch in range(epochs):\n        for image_batch in dataset:\n            train_step(image_batch)\n\n        if (epoch + 1) % save_after == 0:\n            # Produce images for the GIF as we go\n            display.clear_output(wait=True)\n            generate_and_save_images(generator,\n                             epoch + 1,\n                             seed)\n\n    # Generate after the final epoch\n    display.clear_output(wait=True)\n    generate_and_save_images(generator,\n                       epochs,\n                       seed)","2143b562":"def generate_and_save_images(model, epoch, test_input):\n    # Notice `training` is set to False.\n    # This is so all layers run in inference mode (batchnorm).\n    predictions = model(test_input, training=False)\n\n    fig = plt.figure(figsize=(10, 10))\n\n    for i in range(predictions.shape[0]):\n        plt.subplot(4, 4, i + 1)\n        if predictions.shape[-1] == 3:\n            plt.imshow(predictions[i] * 0.5 + .5)  # scale image to [0, 1] floats (or you could also scale to [0, 255] ints) \n        else: \n            plt.imshow(predictions[i, :, :, 0] * 0.5 + .5, cmap='gray')  # scale image to [0, 1] floats (or you could also scale to [0, 255] ints) \n        plt.axis('off')\n    plt.suptitle(f'Epoch {epoch}')\n    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n    plt.show()\n","510784e4":"train(train_dataset, epochs=10000, save_after=100)","d5a1c5c9":"# Fun Time\n\n**Warning: training these models takes a long time. Prepare for multiple hours even with GPU acceleration. The model will output and save generations every `save_after` epochs.**\n\nHow to improve performance:\n* Increase `save_after` or completely discard it\n* Decrease the `IMAGE_SIZE` (will generate smaller images)\n* Changes from color to grayscale by setting `IMAGE_CHANNELS = 1`\n* Run on TPU (untested)","1617d36b":"Let's start by loading up the Pok\u00e9mon images. We will use the dataset provided by https:\/\/www.kaggle.com\/vishalsubbiah\/pokemon-images-and-types. \n\nWe will create a `tf.data.Dataset` as this appears to be the fastest solution. I have previously tried `tf.keras.preprocessing.image_dataset_from_directory` and although this makes things simpler and has built in augementation it became a bottleneck for the GAN. It is not necessarly important to understand what is happening here but I will explain anyway. We load up images from the input folder, allow caching (for GPU), prefetching (so we do not have to loadup data just in time), tell the dataset to use batching and apply preprocessing and filtering.\n\n### Preprocessing\nImages are loaded up as RGB (or grayscale if `IMAGE_CHANNELS = 1`), scaled to `[IMAGE_SIZE, IMAGE_SIZE]` and each image channel is transformed from $[0, 255]$ to $[-1, 1]$. The images are now stored as a matrix of shape `[IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS]`.\n\n### Filtering\nWe kick out all images that have a white background (ca. 10%). This decreases our already quite small dataset to 721 images but also prevents color bleeding and improves convergence. We detect images with a white background by simply checking the red channel of the top left pixel - this will be a bad estimate in most cases but works well for this dataset.","4c86c809":"# Generating Pok\u00e9mon with Generative Adversial Networks (GAN)\n\nWe will generate new Pok\u00e9mon based on images of the existing 800+ Pok\u00e9monn with DCGAN.\n\nGANs pair a generator model, which learns to produce the target output, with a discriminator model, which learns to distinguish true data from the output of the generator. The generator tries to fool the discriminator, and the discriminator tries to keep from being fooled. Using this architecture GANs can create images that resemble the training set. (modified from: https:\/\/developers.google.com\/machine-learning\/gan)\n\nThis kernel is based on https:\/\/www.tensorflow.org\/tutorials\/generative\/dcgan\n**Run this kernel with GPU acceleration.**","3aa35bb3":"## Generator model\n\nThe goal of the generator is to create an image from some input tensor. This input tensor is a representation of the image in latent space. Think of this latent space as some kind of super-compressed zip-file, that ones extracted returns an image. This \"extraction\" is done by this model. Note that the model converts the input into a image, that will be upscaled while traversing through the model resulting in quadrupling in size compared to the input after the `Reshape` layer. ","44df4810":"## Discriminator model\n\nThe task of the discriminator is to classify whether a given input image is a real image (i.e. similar to the ones trained on) or fake. It will return a single output: the probability $p$ of the input beeing real ($p\\in[0,1]$ where 1 would mean real).","8b6b3073":"Now, we generate the models and set Adam as optimizer for both models.","f369f43e":"The training itself is pretty straight forward. Given our dataset we call `training_set` for each batch in the dataset. We repeat this for the number of epochs provided. Additionally, we generate and save the current predictions based on a gloabl seed after `save_after` epochs. This helps us to understand the progress of the model. Since this adds a significant computational load it is best to keep `save_after` as high as possible.","b21b357c":"## Loss functions\n\nTo assess the quality of our models we need to provide a loss function for each. We will later use these functions to update our model weights durign training.\n\n\n### Generator Loss\nThe goal of the generator is to create an image that fools the discriminator i.e. an image that is classified as real. \n\nTherefore, the loss is simply the difference between the scores of the discriminator for each fake image that the generator created and a tensor full of 1's (i.e. saying all input images are real). \n\nThe loss would be 0 if all fake images are labeled as real by the discriminator.\n\n### Discriminator Loss\nThe discriminators goal on the other hand is to distinguish real from fake images.\n\nIt computes two sub-losses and adds them. One loss represents how good real images are detected (outputting 1) and the other how good fake data is rejected (\"called out\" by outputting 0). The real loss is the difference between the scores of the discriminator for each training image and a tensor full of 1's. While, the fake loss is the difference between the scores of the discriminator for each fake image that the generator created and a tensor full of 0's. \n\nThe loss would be 0 if images are classified correctly.","d45a8ba1":"Add a little helper function that makes predictions from a given model, plots them in a grid and saves into the output folder.","6138e2e6":"Now we need to define our training steps. In each step we will generate images from random noise using the generator and supply a batch of real training images. We will then compute the loss and the corresponding gradient for both models. Since both models are Keras models we can then simply update the weights by using `Model.apply_gradients()`.\n\nPlease note that we annotate the function as `@tf.function` which significantly increases performance."}}