{"cell_type":{"f63d0968":"code","d7a9beb1":"code","13c028c2":"code","082a204f":"code","f70a4d9c":"code","8c963179":"code","15231cf6":"code","c69996e5":"code","e631104c":"code","65fe458c":"code","2c6c23e6":"code","0f20c248":"code","235df93f":"code","9d32b605":"code","8932389d":"code","6e2f072e":"code","ad01a822":"code","9ad1a4f6":"code","2703d2fc":"code","2d033160":"code","c75db954":"code","abbf8753":"code","d13859cf":"code","c36fd2fa":"code","cccf3a77":"code","920b2ee3":"code","c72fc37d":"code","221d331b":"code","36bd689c":"code","0249fc38":"code","73028637":"code","bc277967":"code","f9e32e36":"code","919bbd25":"code","24a20902":"code","199da5d9":"code","2c5cd64d":"code","7c28af46":"code","c72cc3f9":"code","9e429a0f":"code","8c7d4d31":"code","e999cfe2":"code","b68b5dc9":"code","1e2f56b2":"code","d4f9b3a2":"code","2169c9f7":"markdown","5572bc78":"markdown","8abe8fbc":"markdown","f8b9cc7d":"markdown","12b1fdd8":"markdown","1283c21d":"markdown","f916f3b8":"markdown","d2afdd4b":"markdown","24afbe10":"markdown","42597497":"markdown","0b0d9472":"markdown","71cae4bb":"markdown","7fbccd27":"markdown","9519b2b8":"markdown","df1cd48f":"markdown","1a83ecfe":"markdown","bae7aecf":"markdown","54d6e5a4":"markdown","8b04978a":"markdown","71ab4eea":"markdown","5a7b8eea":"markdown","793d57ff":"markdown","5225ec55":"markdown","a3fe8441":"markdown","3ffb7620":"markdown","78c7a0f3":"markdown","3225a1a4":"markdown","8eb77581":"markdown","8b6baeaf":"markdown","aa86c8f1":"markdown","7c0f5970":"markdown","6176bf52":"markdown","6d87f4b9":"markdown","13567c84":"markdown","959d6be4":"markdown","5cf5d16f":"markdown","c0c58d21":"markdown","39f704b3":"markdown","ee27a7f8":"markdown","69a2451f":"markdown","fdc4f8da":"markdown","ff7ec587":"markdown","4f5d8f67":"markdown","93b2c178":"markdown","84d7abc4":"markdown"},"source":{"f63d0968":"# Install packages\n!pip install -q --upgrade tensorflow==2.0.0\n!pip install -q --upgrade tensorflow-probability==0.8.0\n!pip install -q catboost\n!pip install -q --pre vaex\n# pip install -q google-cloud-bigquery #TODO: have to install if not using kaggle kernels","d7a9beb1":"# Packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport vaex\n\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.metrics import mean_absolute_error, make_scorer\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\n\nimport tensorflow as tf\nimport tensorflow_probability as tfp\ntfd = tfp.distributions\nfrom tensorflow_probability.python.math import random_rademacher\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, BatchNormalization, ReLU, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\nfrom catboost import CatBoostRegressor\n\nfrom google.cloud import bigquery\n\n# Settings\nsns.set()\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n%config InlineBackend.figure_format = 'svg'\nnp.random.seed(12345)\ntf.random.set_seed(12345)","13c028c2":"%%time\n\n# SQL Query to retreive the data\n# There are 131M records in full dataset!\nQUERY = \"\"\"\n    SELECT\n        pickup_datetime,\n        dropoff_datetime,\n        pickup_longitude,\n        pickup_latitude,\n        dropoff_longitude,\n        dropoff_latitude\n    FROM `bigquery-public-data.new_york.tlc_yellow_trips_2016`\n    LIMIT 1000000;\n\"\"\"\n\n# Load a subset of the dataset\ndata = bigquery.Client().query(QUERY).to_dataframe()","082a204f":"# Drop rows with empty values\ndata.dropna(inplace=True)\n\n# Compute trip duration in seconds\ndata['trip_duration'] = (data['dropoff_datetime'] - data['pickup_datetime']).dt.seconds\n\n# Extract useful time information\ndata['min_of_day'] = (60*data['pickup_datetime'].dt.hour + \n                       data['pickup_datetime'].dt.minute)\ndata['day_of_week'] = data['pickup_datetime'].dt.dayofweek\ndata['day_of_year'] = data['pickup_datetime'].dt.dayofyear\n\n# Remove datetime columns\ndata.drop('pickup_datetime', axis=1, inplace=True)\ndata.drop('dropoff_datetime', axis=1, inplace=True)\n\n# Function to remove rows outside range\ndef clip(df, a, b, col):\n    for c in col:\n        df = df[(df[c]>a) & (df[c]<b)]\n    return df\n\n# Remove outliers\ndata = clip(data, 1, 4*3600, ['trip_duration'])\ndata = clip(data,  -75, -72.5,\n             ['pickup_longitude', 'dropoff_longitude'])\ndata = clip(data, 40, 41.5,\n             ['pickup_latitude', 'dropoff_latitude'])\n\n# Transform target column\ndata['trip_duration'] = np.log(data['trip_duration'])\n\n# Normalize data\ndata = (data - data.mean()) \/ data.std()\n\n# Cast down to float32\ndata = data.astype('float32')\n\n# Shuffle\ndata = data.sample(frac=1)\n\n# Separate in- from dependent variables\nx_taxi = data.drop('trip_duration', axis=1)\ny_taxi = data['trip_duration']","f70a4d9c":"x_taxi.head()","8c963179":"# Plot feature distributions\nplt.figure(figsize=(6.4, 8))\nfor i in range(7):\n    plt.subplot(4, 2, i+1)\n    plt.hist(x_taxi.iloc[:, i], 13)\n    plt.title(x_taxi.columns[i])\nplt.tight_layout()\nplt.show()","15231cf6":"# Plot trip duration distribution\nplt.hist(y_taxi, bins=np.linspace(-6, 6, 21))\nplt.xlabel('Normalized Trip Duration')","c69996e5":"# Make Mean Absolute Error scorer\nmae_scorer = make_scorer(mean_absolute_error)\n\n# Function to print cross-validated mean abs deviation\ndef cv_mae(regressor, x, y, cv=3, scorer=mae_scorer):\n    scores = cross_val_score(regressor, \n                             x, y, cv=cv,\n                             scoring=scorer)\n    print('MAE:', scores.mean())","e631104c":"# MAE from predicting just the mean\ncv_mae(DummyRegressor(), x_taxi, y_taxi)","65fe458c":"%%time\n\n# Distance between pickup and dropoff locations\ndist = np.sqrt(\n    np.power(x_taxi['pickup_longitude'] -\n             x_taxi['dropoff_longitude'], 2) + \n    np.power(x_taxi['pickup_latitude'] - \n             x_taxi['dropoff_latitude'], 2))\n\n# MAE from using just distance as predictor\ncv_mae(IsotonicRegression(out_of_bounds='clip'), \n       dist, y_taxi)","2c6c23e6":"%%time\n\n# MAE using CatBoost\ncv_mae(CatBoostRegressor(verbose=False, depth=9), x_taxi, y_taxi)","0f20c248":"# Batch size\nBATCH_SIZE = 1024\n\n# Number of training epochs\nEPOCHS = 100\n\n# Learning rate\nL_RATE = 1e-4\n\n# Proportion of samples to hold out\nVAL_SPLIT = 0.2","235df93f":"# Multilayer dense neural network\nD = x_taxi.shape[1]\nmodel = Sequential([\n    Dense(512, use_bias=False, input_shape=(D,)),\n    BatchNormalization(),\n    ReLU(),\n    Dropout(0.1),\n    Dense(128, use_bias=False),\n    BatchNormalization(),\n    ReLU(),\n    Dropout(0.1),\n    Dense(64, use_bias=False),\n    BatchNormalization(),\n    ReLU(),\n    Dropout(0.1),\n    Dense(32, use_bias=False),\n    BatchNormalization(),\n    ReLU(),\n    Dropout(0.1),\n    Dense(1)\n])","9d32b605":"# Compile the model with MAE loss\nmodel.compile(tf.keras.optimizers.Adam(lr=L_RATE),\n              loss='mean_absolute_error')","8932389d":"%%time\n\n# Fit the model\nhistory = model.fit(x_taxi, y_taxi,\n                    batch_size=BATCH_SIZE,\n                    epochs=EPOCHS,\n                    validation_split=VAL_SPLIT,\n                    verbose=0)","6e2f072e":"plt.plot(history.history['loss'], label='Train')\nplt.plot(history.history['val_loss'], label='Val')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('MAE')\nplt.show()","ad01a822":"# Make predictions (tautological)\npreds = model.predict(x_taxi)\n\n# Plot true vs predicted durations\nplt.figure(figsize=(6.4, 8))\nfor i in range(8):\n    plt.subplot(4,2,i+1)\n    plt.axvline(preds[i], label='Pred')\n    plt.axvline(y_taxi[i], ls=':', color='gray', label='True')\n    plt.xlim([-5, 5])\n    plt.legend()\n    plt.gca().get_yaxis().set_ticklabels([])\n    if i<6:\n        plt.gca().get_xaxis().set_ticklabels([])","9ad1a4f6":"# Split data randomly into training + validation\ntr_ind = np.random.choice([False, True],\n                          size=x_taxi.shape[0],\n                          p=[VAL_SPLIT, 1.0-VAL_SPLIT])\nx_train = x_taxi[tr_ind].values\ny_train = y_taxi[tr_ind].values\nx_val = x_taxi[~tr_ind].values\ny_val = y_taxi[~tr_ind].values\nN_train = x_train.shape[0]\nN_val = x_val.shape[0]\n\n# Make y 2d\ny_train = np.expand_dims(y_train, 1)\ny_val = np.expand_dims(y_val, 1)\n\n# Make a TensorFlow Dataset from training data\ndata_train = tf.data.Dataset.from_tensor_slices(\n    (x_train, y_train)).shuffle(10000).batch(BATCH_SIZE)\n\n# Make a TensorFlow Dataset from validation data\ndata_val = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(N_val)","2703d2fc":"# Xavier initializer\ndef xavier(shape):\n    return tf.random.truncated_normal(\n        shape, \n        mean=0.0,\n        stddev=np.sqrt(2\/sum(shape)))","2d033160":"class BayesianDenseLayer(tf.keras.Model):\n    \"\"\"A fully-connected Bayesian neural network layer\n    \n    Parameters\n    ----------\n    d_in : int\n        Dimensionality of the input (# input features)\n    d_out : int\n        Output dimensionality (# units in the layer)\n    name : str\n        Name for the layer\n        \n    Attributes\n    ----------\n    weight : tensorflow_probability.distributions.Normal\n        Variational distributions for the network weights\n    bias : tensorflow_probability.distributions.Normal\n        Variational distributions for the network biases\n    losses : tensorflow.Tensor\n        Sum of the Kullback\u2013Leibler divergences between\n        the posterior distributions and their priors\n        \n    Methods\n    -------\n    call : tensorflow.Tensor\n        Perform the forward pass of the data through\n        the layer\n    \"\"\"\n\n    def __init__(self, d_in, d_out, name=None):\n        super(BayesianDenseLayer, self).__init__(name=name)\n        self.w_loc = tf.Variable(xavier([d_in, d_out]), name='w_loc')\n        self.w_std = tf.Variable(xavier([d_in, d_out])-6.0, name='w_std')\n        self.b_loc = tf.Variable(xavier([1, d_out]), name='b_loc')\n        self.b_std = tf.Variable(xavier([1, d_out])-6.0, name='b_std')\n        \n    \n    @property\n    def weight(self):\n        return tfd.Normal(self.w_loc, tf.nn.softplus(self.w_std))\n    \n    \n    @property\n    def bias(self):\n        return tfd.Normal(self.b_loc, tf.nn.softplus(self.b_std))\n        \n        \n    def call(self, x, sampling=True):\n        if sampling:\n            return x @ self.weight.sample() + self.bias.sample()        \n        else:\n            return x @ self.w_loc + self.b_loc\n            \n            \n    @property\n    def losses(self):\n        prior = tfd.Normal(0, 1)\n        return (tf.reduce_sum(tfd.kl_divergence(self.weight, prior)) +\n                tf.reduce_sum(tfd.kl_divergence(self.bias, prior)))","c75db954":"class BayesianDenseLayer(tf.keras.Model):\n    \"\"\"A fully-connected Bayesian neural network layer\n    \n    Parameters\n    ----------\n    d_in : int\n        Dimensionality of the input (# input features)\n    d_out : int\n        Output dimensionality (# units in the layer)\n    name : str\n        Name for the layer\n        \n    Attributes\n    ----------\n    losses : tensorflow.Tensor\n        Sum of the Kullback\u2013Leibler divergences between\n        the posterior distributions and their priors\n        \n    Methods\n    -------\n    call : tensorflow.Tensor\n        Perform the forward pass of the data through\n        the layer\n    \"\"\"\n    \n    def __init__(self, d_in, d_out, name=None):\n        \n        super(BayesianDenseLayer, self).__init__(name=name)\n        self.d_in = d_in\n        self.d_out = d_out\n        \n        self.w_loc = tf.Variable(xavier([d_in, d_out]), name='w_loc')\n        self.w_std = tf.Variable(xavier([d_in, d_out])-6.0, name='w_std')\n        self.b_loc = tf.Variable(xavier([1, d_out]), name='b_loc')\n        self.b_std = tf.Variable(xavier([1, d_out])-6.0, name='b_std')\n    \n    \n    def call(self, x, sampling=True):\n        \"\"\"Perform the forward pass\"\"\"\n        \n        if sampling:\n        \n            # Flipout-estimated weight samples\n            s = random_rademacher(tf.shape(x))\n            r = random_rademacher([x.shape[0], self.d_out])\n            w_samples = tf.nn.softplus(self.w_std)*tf.random.normal([self.d_in, self.d_out])\n            w_perturbations = r*tf.matmul(x*s, w_samples)\n            w_outputs = tf.matmul(x, self.w_loc) + w_perturbations\n            \n            # Flipout-estimated bias samples\n            r = random_rademacher([x.shape[0], self.d_out])\n            b_samples = tf.nn.softplus(self.b_std)*tf.random.normal([self.d_out])\n            b_outputs = self.b_loc + r*b_samples\n            \n            return w_outputs + b_outputs\n        \n        else:\n            return x @ self.w_loc + self.b_loc\n    \n    \n    @property\n    def losses(self):\n        \"\"\"Sum of the KL divergences between priors + posteriors\"\"\"\n        weight = tfd.Normal(self.w_loc, tf.nn.softplus(self.w_std))\n        bias = tfd.Normal(self.b_loc, tf.nn.softplus(self.b_std))\n        prior = tfd.Normal(0, 1)\n        return (tf.reduce_sum(tfd.kl_divergence(weight, prior)) +\n                tf.reduce_sum(tfd.kl_divergence(bias, prior)))","abbf8753":"class BayesianDenseNetwork(tf.keras.Model):\n    \"\"\"A multilayer fully-connected Bayesian neural network\n    \n    Parameters\n    ----------\n    dims : List[int]\n        List of units in each layer\n    name : str\n        Name for the network\n        \n    Attributes\n    ----------\n    losses : tensorflow.Tensor\n        Sum of the Kullback\u2013Leibler divergences between\n        the posterior distributions and their priors, \n        over all layers in the network\n        \n    Methods\n    -------\n    call : tensorflow.Tensor\n        Perform the forward pass of the data through\n        the network\n    \"\"\"\n    \n    def __init__(self, dims, name=None):\n        \n        super(BayesianDenseNetwork, self).__init__(name=name)\n        \n        self.steps = []\n        self.acts = []\n        for i in range(len(dims)-1):\n            self.steps += [BayesianDenseLayer(dims[i], dims[i+1])]\n            self.acts += [tf.nn.relu]\n            \n        self.acts[-1] = lambda x: x\n        \n    \n    def call(self, x, sampling=True):\n        \"\"\"Perform the forward pass\"\"\"\n\n        for i in range(len(self.steps)):\n            x = self.steps[i](x, sampling=sampling)\n            x = self.acts[i](x)\n            \n        return x\n    \n    \n    @property\n    def losses(self):\n        \"\"\"Sum of the KL divergences between priors + posteriors\"\"\"\n        return tf.reduce_sum([s.losses for s in self.steps])","d13859cf":"class BayesianDenseRegression(tf.keras.Model):\n    \"\"\"A multilayer fully-connected Bayesian neural network regression\n    \n    Parameters\n    ----------\n    dims : List[int]\n        List of units in each layer\n    name : str\n        Name for the network\n        \n    Attributes\n    ----------\n    losses : tensorflow.Tensor\n        Sum of the Kullback\u2013Leibler divergences between\n        the posterior distributions and their priors, \n        over all layers in the network\n        \n    Methods\n    -------\n    call : tensorflow.Tensor\n        Perform the forward pass of the data through\n        the network, predicting both means and stds\n    log_likelihood : tensorflow.Tensor\n        Compute the log likelihood of y given x\n    samples : tensorflow.Tensor\n        Draw multiple samples from the predictive distribution\n    \"\"\"    \n    \n    \n    def __init__(self, dims, name=None):\n        \n        super(BayesianDenseRegression, self).__init__(name=name)\n        \n        # Multilayer fully-connected neural network to predict mean\n        self.loc_net = BayesianDenseNetwork(dims)\n        \n        # Variational distribution variables for observation error\n        self.std_alpha = tf.Variable([10.0], name='std_alpha')\n        self.std_beta = tf.Variable([10.0], name='std_beta')\n\n    \n    def call(self, x, sampling=True):\n        \"\"\"Perform the forward pass, predicting both means and stds\"\"\"\n        \n        # Predict means\n        loc_preds = self.loc_net(x, sampling=sampling)\n    \n        # Predict std deviation\n        posterior = tfd.Gamma(self.std_alpha, self.std_beta)\n        transform = lambda x: tf.sqrt(tf.math.reciprocal(x))\n        N = x.shape[0]\n        if sampling:\n            std_preds = transform(posterior.sample([N]))\n        else:\n            std_preds = tf.ones([N, 1])*transform(posterior.mean())\n    \n        # Return mean and std predictions\n        return tf.concat([loc_preds, std_preds], 1)\n    \n    \n    def log_likelihood(self, x, y, sampling=True):\n        \"\"\"Compute the log likelihood of y given x\"\"\"\n        \n        # Compute mean and std predictions\n        preds = self.call(x, sampling=sampling)\n        \n        # Return log likelihood of true data given predictions\n        return tfd.Normal(preds[:,0], preds[:,1]).log_prob(y[:,0])\n    \n    \n    @tf.function\n    def sample(self, x):\n        \"\"\"Draw one sample from the predictive distribution\"\"\"\n        preds = self.call(x)\n        return tfd.Normal(preds[:,0], preds[:,1]).sample()\n    \n    \n    def samples(self, x, n_samples=1):\n        \"\"\"Draw multiple samples from the predictive distribution\"\"\"\n        samples = np.zeros((x.shape[0], n_samples))\n        for i in range(n_samples):\n            samples[:,i] = self.sample(x)\n        return samples\n    \n    \n    @property\n    def losses(self):\n        \"\"\"Sum of the KL divergences between priors + posteriors\"\"\"\n                \n        # Loss due to network weights\n        net_loss = self.loc_net.losses\n\n        # Loss due to std deviation parameter\n        posterior = tfd.Gamma(self.std_alpha, self.std_beta)\n        prior = tfd.Gamma(10.0, 10.0)\n        std_loss = tfd.kl_divergence(posterior, prior)\n\n        # Return the sum of both\n        return net_loss + std_loss","c36fd2fa":"model1 = BayesianDenseRegression([7, 256, 128, 64, 32, 1])","cccf3a77":"# Adam optimizer\noptimizer = tf.keras.optimizers.Adam(lr=L_RATE)","920b2ee3":"N = x_train.shape[0]\n\n@tf.function\ndef train_step(x_data, y_data):\n    with tf.GradientTape() as tape:\n        log_likelihoods = model1.log_likelihood(x_data, y_data)\n        kl_loss = model1.losses\n        elbo_loss = kl_loss\/N - tf.reduce_mean(log_likelihoods)\n    gradients = tape.gradient(elbo_loss, model1.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model1.trainable_variables))\n    return elbo_loss","c72fc37d":"%%time\n\n# Fit the model\nelbo1 = np.zeros(EPOCHS)\nmae1 = np.zeros(EPOCHS)\nfor epoch in range(EPOCHS):\n    \n    # Update weights each batch\n    for x_data, y_data in data_train:\n        elbo1[epoch] += train_step(x_data, y_data)\n        \n    # Evaluate performance on validation data\n    for x_data, y_data in data_val:\n        y_pred = model1(x_data, sampling=False)[:, 0]\n        mae1[epoch] = mean_absolute_error(y_pred, y_data)","221d331b":"# Plot the ELBO loss\nplt.plot(elbo1)\nplt.xlabel('Epoch')\nplt.ylabel('ELBO Loss')\nplt.show()","36bd689c":"# Plot validation error over training\nplt.plot(mae1)\nplt.xlabel('Epoch')\nplt.ylabel('Mean Absolute Error')\nplt.show()","0249fc38":"class BayesianDensityNetwork(tf.keras.Model):\n    \"\"\"Multilayer fully-connected Bayesian neural network, with\n    two heads to predict both the mean and the standard deviation.\n    \n    Parameters\n    ----------\n    units : List[int]\n        Number of output dimensions for each layer\n        in the core network.\n    units : List[int]\n        Number of output dimensions for each layer\n        in the head networks.\n    name : None or str\n        Name for the layer\n    \"\"\"\n    \n    \n    def __init__(self, units, head_units, name=None):\n        \n        # Initialize\n        super(BayesianDensityNetwork, self).__init__(name=name)\n        \n        # Create sub-networks\n        self.core_net = BayesianDenseNetwork(units)\n        self.loc_net = BayesianDenseNetwork([units[-1]]+head_units)\n        self.std_net = BayesianDenseNetwork([units[-1]]+head_units)\n\n    \n    def call(self, x, sampling=True):\n        \"\"\"Pass data through the model\n        \n        Parameters\n        ----------\n        x : tf.Tensor\n            Input data\n        sampling : bool\n            Whether to sample parameter values from their variational\n            distributions (if True, the default), or just use the\n            Maximum a Posteriori parameter value estimates (if False).\n            \n        Returns\n        -------\n        preds : tf.Tensor of shape (Nsamples, 2)\n            Output of this model, the predictions.  First column is\n            the mean predictions, and second column is the standard\n            deviation predictions.\n        \"\"\"\n        \n        # Pass data through core network\n        x = self.core_net(x, sampling=sampling)\n        x = tf.nn.relu(x)\n        \n        # Make predictions with each head network\n        loc_preds = self.loc_net(x, sampling=sampling)\n        std_preds = self.std_net(x, sampling=sampling)\n        std_preds = tf.nn.softplus(std_preds)\n        \n        # Return mean and std predictions\n        return tf.concat([loc_preds, std_preds], 1)\n    \n    \n    def log_likelihood(self, x, y, sampling=True):\n        \"\"\"Compute the log likelihood of y given x\"\"\"\n        \n        # Compute mean and std predictions\n        preds = self.call(x, sampling=sampling)\n        \n        # Return log likelihood of true data given predictions\n        return tfd.Normal(preds[:,0], preds[:,1]).log_prob(y[:,0])\n        \n        \n    @tf.function\n    def sample(self, x):\n        \"\"\"Draw one sample from the predictive distribution\"\"\"\n        preds = self.call(x)\n        return tfd.Normal(preds[:,0], preds[:,1]).sample()\n    \n    \n    def samples(self, x, n_samples=1):\n        \"\"\"Draw multiple samples from the predictive distribution\"\"\"\n        samples = np.zeros((x.shape[0], n_samples))\n        for i in range(n_samples):\n            samples[:,i] = self.sample(x)\n        return samples\n    \n    \n    @property\n    def losses(self):\n        \"\"\"Sum of the KL divergences between priors + posteriors\"\"\"\n        return (self.core_net.losses +\n                self.loc_net.losses +\n                self.std_net.losses)","73028637":"# Instantiate the model\nmodel2 = BayesianDensityNetwork([7, 256, 128], [64, 32, 1])","bc277967":"# Use the Adam optimizer\noptimizer = tf.keras.optimizers.Adam(lr=L_RATE)","f9e32e36":"N = x_train.shape[0]\n\n@tf.function\ndef train_step(x_data, y_data):\n    with tf.GradientTape() as tape:\n        log_likelihoods = model2.log_likelihood(x_data, y_data)\n        kl_loss = model2.losses\n        elbo_loss = kl_loss\/N - tf.reduce_mean(log_likelihoods)\n    gradients = tape.gradient(elbo_loss, model2.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model2.trainable_variables))\n    return elbo_loss","919bbd25":"%%time\n\n# Fit the model\nelbo2 = np.zeros(EPOCHS)\nmae2 = np.zeros(EPOCHS)\nfor epoch in range(EPOCHS):\n    \n    # Update weights each batch\n    for x_data, y_data in data_train:\n        elbo2[epoch] += train_step(x_data, y_data)\n        \n    # Evaluate performance on validation data\n    for x_data, y_data in data_val:\n        y_pred = model2(x_data, sampling=False)[:, 0]\n        mae2[epoch] = mean_absolute_error(y_pred, y_data)","24a20902":"# Plot the ELBO Loss over training\nplt.plot(elbo2)\nplt.xlabel('Epoch')\nplt.ylabel('ELBO Loss')\nplt.show()","199da5d9":"# Plot error over training\nplt.plot(mae2)\nplt.xlabel('Epoch')\nplt.ylabel('Mean Absolue Error')\nplt.show()","2c5cd64d":"# Plot error vs epoch curves for all 3 models\nplt.plot(mae1, label='No Error Estimation')\nplt.plot(mae2, label='Density Network')\nplt.plot(history.history['val_loss'], label='Non-Bayesian')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('Mean Absolute Error')\nplt.show()","7c28af46":"# Make predictions on validation data\nfor x_data, y_data in data_val:\n    resids1 = y_data[:, 0] - model1(x_data, sampling=False)[:, 0]\n    resids2 = y_data[:, 0] - model2(x_data, sampling=False)[:, 0]\n    \n# Plot residual distributions\nbins = np.linspace(-2, 2, 100)\nplt.hist(resids1.numpy(), bins, alpha=0.5,\n         label='No Error Estimation')\nplt.hist(resids2.numpy(), bins, alpha=0.5,\n         label='Density Network')\nplt.legend()\nplt.xlabel('Residuals')\nplt.ylabel('Count')","c72cc3f9":"%%time\n\n# Sample from predictive distributions\nfor x_data, y_data in data_val:\n    samples1 = model1.samples(x_data, 1000)\n    samples2 = model2.samples(x_data, 1000)","9e429a0f":"# Plot predictive distributions\nplt.figure(figsize=(6.4, 8))\nfor i in range(8):\n    plt.subplot(4,2,i+1)\n    sns.kdeplot(samples1[i,:], shade=True)\n    sns.kdeplot(samples2[i,:], shade=True)\n    plt.axvline(y_data.numpy()[i], ls=':', color='gray')\n    plt.xlim([-5, 5])\n    plt.ylim([0, 2.2])\n    plt.title(str(i))\n    plt.gca().get_yaxis().set_ticklabels([])\n    if i<6:\n        plt.gca().get_xaxis().set_ticklabels([])","8c7d4d31":"%%time\n\ndef covered(samples, y_true, prc=95.0):\n    \"\"\"Whether each sample was covered by its predictive interval\"\"\"\n    q0 = (100.0-prc)\/2.0 #lower percentile \n    q1 = 100.0-q0        #upper percentile\n    within_conf_int = np.zeros(len(y_true))\n    for i in range(len(y_true)):\n        p0 = np.percentile(samples[i,:], q0)\n        p1 = np.percentile(samples[i,:], q1)\n        if p0<=y_true[i] and p1>y_true[i]:\n            within_conf_int[i] = 1\n    return within_conf_int\n\n# Compute what samples are covered by their 95% predictive intervals\ncovered1 = covered(samples1, y_data)\ncovered2 = covered(samples2, y_data)","e999cfe2":"print('No Unc Estimation: ', 100*np.mean(covered1))\nprint('Density Network: ', 100*np.mean(covered2))","b68b5dc9":"# Compute hour of the day\nhour = x_data[:,4].numpy()\nhour = hour-hour.min()\nhour = hour\/hour.max()\nhour = np.floor(23.99*hour)\n\n# Compute coverage as a fn of time of day\ncovs1 = np.zeros(24)\ncovs2 = np.zeros(24)\nfor iT in range(0,24):\n    ix = hour==iT\n    covs1[iT] = 100.0*np.mean(covered1[ix])\n    covs2[iT] = 100.0*np.mean(covered2[ix])\n    \n# Plot coverage as a fn of time of day\nplt.plot(covs1, label='No Error Estimation')\nplt.plot(covs2, label='Density Network')\nplt.axhline(95.0, label='Ideal', ls=':', color='k')\nplt.xlabel('Hour')\nplt.ylabel('95% Predictive Interval Coverage')\nplt.title('Coverage of 95% Predictive Interval by Hour')\nplt.ylim([90, 100])\nplt.legend()\nplt.show()","1e2f56b2":"# Create vaex df with predictive intervals\ncov_by_loc = pd.DataFrame()\ncov_by_loc['x'] = x_data[:, 0].numpy()\ncov_by_loc['y'] = x_data[:, 1].numpy()\ncov_by_loc['covered'] = covered1\nvdf = vaex.from_pandas(cov_by_loc)\n\n# Compute coverage of the predictive interval\nlims = [[-3, 3.5],[-4, 4]]\ncov = vdf.mean(vdf.covered, limits=lims, shape=250,\n               binby=[vdf.x,\n                      vdf.y])\n\n# Plot coverage of the predictive interval\ncmap = matplotlib.cm.PuOr\ncmap.set_bad('black', 1.)\nplt.imshow(cov.T, origin='lower', aspect='auto',\n           vmin=0.9, vmax=1.0, cmap=cmap,\n           extent=[lims[0][0], lims[0][1], \n                   lims[1][0], lims[1][1]])\nax = plt.gca()\nax.grid(False)\ncbar = plt.colorbar()\ncbar.set_label('Coverage of 95% predictive interval', \n               rotation=270)\nplt.title('No Error Estimation')","d4f9b3a2":"# Create vaex df with predictive intervals\ncov_by_loc['covered'] = covered2\nvdf = vaex.from_pandas(cov_by_loc)\n\n# Compute coverage of the predictive interval\ncov = vdf.mean(vdf.covered, limits=lims, shape=250,\n               binby=[vdf.x,\n                      vdf.y])\n\n# Plot coverage of the predictive interval\nplt.imshow(cov.T, origin='lower', aspect='auto',\n           vmin=0.9, vmax=1.0, cmap=cmap,\n           extent=[lims[0][0], lims[0][1], \n                   lims[1][0], lims[1][1]])\nax = plt.gca()\nax.grid(False)\ncbar = plt.colorbar()\ncbar.set_label('Coverage of 95% predictive interval', \n               rotation=270)\nplt.title('Density Network')","2169c9f7":"And then for the dual-headed density network:","5572bc78":"The Bayesian network does pretty well, about as well as the non-Bayesian network!\n\nHowever, there's one problem with the model: it assumes a constant level of uncertainty.  That is, the standard deviation parameter doesn't change depending on the input data - it's just fit to model the *average* level of uncertainty.  To fix that problem, and create a network which more dynamically updates its uncertainty estimates, we'll build a dual-headed Bayesian network which predicts both the mean *and* the standard deviation of the predictive distribution.","8abe8fbc":"### Coverage of the 95% Confidence Interval\n\nTo measure how accurate the uncertainty predictions are, we can compute the coverage of the 95% predictive interval.  If our uncertainty estimates are perfect, we would expect exactly 95% of samples to fall within the inner 95% of the predictive distribution.  The \"coverage\" of the 95% interval measures what percentage of samples actually do fall within that range, and so the coverage should be around 95%.\n\nLet's make a function to compute the coverage of a given predictive interval, and then measure the coverage of the 95% interval for both the simple Bayesian network, and the dual-headed density network.","f8b9cc7d":"And again we'll define a `tf.function` to perform one training step.","12b1fdd8":"## Dual-headed Bayesian Density Network\n\nTo allow our Bayesian network to predict both the value of the target and the uncertainty as to that value, we'll give the network two \"heads\", or endpoints of the network.  One head will predict the mean of the predictive distribution (the network's best guess), while the other head will predict the standard deviation of the predictive distribution (how much uncertainty is behind that guess).\n\n![Dual headed net](http:\/\/drive.google.com\/uc?export=view&id=1MPspM2UhxEziX924MWX8RmGe6Sk1ovg8)\n\nHaving defined the `BayesianDenseNetwork` class (in the previous section), it's relatively easy to define another class which implements the dual-headed Bayesian density network: we can just define a core network consisting of several layers, and then two sub-networks which receive the output of the core network and output independent predictions.  The output of one network we'll use as the mean predictions, and the output of the other as the standard deviation predictions.","1283c21d":"Neither network seems to be drastically over- or under-estimating uncertainty as the time of day changes, though again the simple network overestimates uncertainty overall.\n\nLet's also take a look at the coverage of the 95% predictive interval as a function of the pickup location.  First, for the simple network:","f916f3b8":"Also, we'll want to create a function which uses [Xavier initialization](http:\/\/proceedings.mlr.press\/v9\/glorot10a.html) to initialize the weights of the network.","d2afdd4b":"Now we can build the network using Keras's [Sequential](https:\/\/www.tensorflow.org\/versions\/r2.0\/api_docs\/python\/tf\/keras\/Sequential) model.  We'll make a network with 4 hidden layers, and which outputs a single prediction as to the trip duration.\n\n![Vanilla neural network](http:\/\/drive.google.com\/uc?export=view&id=1e3uw9SYoOPCeW10_zB86FVEsR4_UsyXt)\n\nIt also uses [batch normalization](http:\/\/arxiv.org\/abs\/1502.03167) and [dropout](http:\/\/jmlr.org\/papers\/v15\/srivastava14a.html) at each of the hidden layers:","24afbe10":"Now let's create a class which represents one fully-connected Bayesian neural network layer, using the [Keras functional API (aka subclassing)](https:\/\/www.tensorflow.org\/beta\/guide\/keras\/functional).  We can instantiate this class to create one layer, and `__call__`ing that object performs the forward pass of the data through the layer.  We'll use TensorFlow Probability distribution objects to represent the prior and posterior distributions, and we'll give the class a `losses` property which returns the sum of the Kullback\u2013Leibler divergence between the posterior distributions and their priors, so that we can fit the weights via stochastic variational inference.\n\nSide note: I was going to use TensorFlow Probability's [DenseFlipout layer](https:\/\/www.tensorflow.org\/probability\/api_docs\/python\/tfp\/layers\/DenseFlipout) here, but unfortunately it seems it doesn't fully support TensorFlow 2.0's eager execution yet!  The forward pass through the layer is fine, but the KL loss doesn't get updated.  So, we'll just build a Dense variational layer here manually, which isn't really all that bad using other aspects of TensorFlow Probability and the functional API.","42597497":"OK, so the MAE of our predictions from any reasonable model should definitely be better than ~0.76.\n\nWhat about if we compute the distance between the pickup and dropoff locations, and just do an [isotonic regression](http:\/\/en.wikipedia.org\/wiki\/Isotonic_regression) using that one variable?","0b0d9472":"## Vanilla Neural Network\n\nBefore we make a Bayesian neural network, let's get a *normal* neural network up and running to predict the taxi trip durations.  We'll use [Keras](http:\/\/www.tensorflow.org\/versions\/r2.0\/api_docs\/python\/tf\/keras) and [TensorFlow 2.0](http:\/\/www.tensorflow.org\/beta\/).  Of course, Keras works pretty much exactly the same way with TF 2.0 as it did with TF 1.0.\n\nLet's set some neural-network-specific settings which we'll use for all the neural networks in this post (including the Bayesian neural nets later one).","71cae4bb":"How do they compare?  Again, we want these values to be as close to 95.0% as possible.","7fbccd27":"### Learning Curves\n\nThe three models do roughly the same in terms of predictive performance.  The dual-headed density network looks to have a slightly lower error, if anything.","9519b2b8":"After compiling the model,","df1cd48f":"Finally, we need to have a full model which we can fit. The network above (`BayesianDenseNetwork`) only gave us predictions of the *mean*, but didn't include an observation distribution or expected error.  Let's create a model which includes a standard deviation parameter (which we'll model with a square-root inverse Gamma distribution).  This model will have not just a `call` method (which returns the mean prediction) but a `log_likelihood` method (which computes the log likelihood of some data given the current parameter values).\n\nIn this model, the `call` method will return a Tensor with two columns: the first is the mean predictions, and the second column will contain the standard deviation predictions.  Though note that these standard deviation \"predictions\" aren't coming from the network (yet!), and are just based off the one standard deviation parameter.\n\nWe'll also add a method which returns samples from the predictive distribution given test samples (`samples`).  Note that this method calls another (`sample`) which computes a single predictive sample.  The `sample` method has a `tf.function` decorator, which causes TensorFlow to optimize the computations within that function.  It works just the same without the `tf.function` decorator, but is quite a bit slower.","1a83ecfe":"How well can we do if we just predict the mean duration for every trip?","bae7aecf":"Next we need to create a function (again decorated with `tf.function` to make TensorFlow optimize the computations within) to perform a single training step.  In that function we'll compute the log likelihood of the target values according to the model given the predictors, and also the Kullback\u2013Leibler divergence between the parameters' variational posteriors and their priors.\n\nTo fit the model via variational inference, we'll minimize the (negative) expected lower bound (ELBO), which is the sum of the expected log likelihood and the KL divergences.  Note that these two terms need to be on the same scale, and so we need to take the average log likelihood (that is, divide the sum by the number of samples in the batch), but divide the sum of the divergences by the total number of samples in the dataset (NOT in the batch, see eq 18 in [this paper](http:\/\/papers.nips.cc\/paper\/4329-practical-variational-inference-for-neural-networks)).\n\nThen, we'll use [TensorFlow's GradientTape](https:\/\/www.tensorflow.org\/versions\/r2.0\/api_docs\/python\/tf\/GradientTape), which allows us to backpropogate the loss gradients to our variables when using eager execution mode (much like PyTorch's autograd).  The function then passes those gradients to the optimizer, which updates the variables controling the network weights' variational posteriors.","54d6e5a4":"We'll use the same optimizer and learning rate as we did for the vanilla neural network:","8b04978a":"That's a lot better than just predicting the mean, and the model uses only one variable.  Note also that it's really fast to train.\n\nHow about if we use a gradient-boosted decision tree ensemble?  Let's use [CatBoost](http:\/\/catboost.ai) to predict the ride durations:","71ab4eea":"Let's take a look at the cleaned data table and the distributions of the independent variables.","5a7b8eea":"After defining a single Bayesian neural network layer, we can easily create another class which represents a multi-layer network:","793d57ff":"## Simple Bayesian Neural Network\n\nBefore creating a Bayesian neural network with two heads, we'll create a network with just one.  This network will generate predictions just like the previous, non-Bayesian net, but will also have a parameter which estimates the overall level of uncertainty (sigma in the figure below).\n\n![Single headed net](http:\/\/drive.google.com\/uc?export=view&id=1oLxktWxQRt6DcJkMSe6CZOh2A-IJ42Jo)\n\nIt's difficult to fit a Bayesian neural network using Keras, because the loss isn't a simple function of the true vs predicted target values: with a Bayesian neural network we'll be using variational inference, which depends on the true target value, the predictive distribution, and the Kullback\u2013Leibler divergences between the parameter's variational posteriors and their priors.\n\nSo, we'll do it in \"raw\" TensorFlow.  The recommended way to feed data into a model in TensorFlow 2.0 is using TF's [dataset API](https:\/\/www.tensorflow.org\/versions\/r2.0\/api_docs\/python\/tf\/data\/Dataset), as opposed to the old `feed_dict` mechanism.  Let's build a TensorFlow Dataset which contains the taxi data:","5225ec55":"## Baseline Models\n\nBefore we start messing with any fancy neural networks, let's evaluate the performance of some simple baseline models.  We want to see how much better than simple baselines the neural network performs (if it's better at all!), and also how much slower training the network is than those baselines.  First, let's make a function to evaluate any model on the data using cross-validation.  We'll use the mean absolute error as our metric, and by default will use 3-fold cross validation to evaluate the MAE.","a3fe8441":"### Residuals\n\nThe distributions of the residuals (the differences between the true values and the predicted values) look about the same between the simple Bayesian network and the dual-headed density network.  This makes sense given that the parts of each network which are predicting the means are actually identical (in terms of network structure, not the weights).","3ffb7620":"Again we'll use the same optimizer and learning rate as before:","78c7a0f3":"### Predictive Distributions\n\nWhat do the two models' predictive distributions look like?  We can draw samples from the predictive distributions using the `samples` method we added to the `BayesianDenseRegression` class:","3225a1a4":"Let's take a look at the loss and mean absolute error over the course of training.  Note that we wouldn't expect the ELBO loss to be on quite the same scale as with the previous network - the networks have slightly different numbers of parameters.","8eb77581":"Now we can instantiate the dual-headed model:","8b6baeaf":"Let's plot the true values against the predictive distributions for both networks.  In the plots below, each panel is a single validation sample (a single taxi ride!), the dotted gray line is the true value, the blue distribution is the predictive distribution of the simple Bayesian net, and the orange distribution is the predictive distribution of the dual-headed density network.","aa86c8f1":"However, the above version of a dense layer only takes one sample from the variational distribution per batch.  This causes the gradient estimate to be very noisy, causing the network to be difficult to fit (or at least to take a long time to fit!).  To speed up training, we can use the [Flipout estimator](https:\/\/arxiv.org\/abs\/1803.04386) for  parameters whose variational distributions are in the location-scale family and are symmetric.  However, we won't use Flipout when we're not sampling from the network.  When the `sampling` argument to the `call` method is False, we'll just use maximum a posteriori estimation (just use the mean of each parameter's variational distribution).","7c0f5970":"Again, much better than the simpler baseline models.  Now the real question is, can a neural network beat CatBoost?","6176bf52":"Looks like the simple network doesn't have quite as ideal a coverage as the dual-headed network.  Its coverage percentage is too high, meaning the simple network is *under*confident in its predictions.  Generally speaking though, I wouldn't expect one or the other network to have a worse coverage *overall*.  The advantage of the dual-headed network is that it can make uncertainty predictions which change dynamically as the input features change.  So, let's take a look at the coverage of the 95% predictive interval as a function of some feature, say, the hour of the day.","6d87f4b9":"The dependent variable (our \"target\") will be the normalized duration of the taxi trip:","13567c84":"Now we can clean and process the data for modeling.  I've done a more comprehensive exploratory data analysis in [a previous post](https:\/\/brendanhasz.github.io\/2018\/12\/15\/quantile-regression.html), so we'll skip the EDA here and just clean the data.","959d6be4":"Finally, we can actually fit the model!  We'll record the ELBO loss and the MAE over the course of training, and also evaluate the model on held-out validation data each epoch.  When evaluating the model, however, we won't sample from the variational posterior distributions, but will just use the maximum a posteriori estimates of the parameter values (the means of the variational distributions).","5cf5d16f":"Whew!  That was a lot of work, but now we can easily instantiate a fully-connected Bayesian neural network with however many layers and units we want:","c0c58d21":"We can fit it to the taxi trip duration data:","39f704b3":"Next we'll make a Bayesian neural network which includes an observation distribution, and so we'll be able to measure the uncertainty of its estimates.","ee27a7f8":"And then we can train the model!","69a2451f":"## Trip Duration Prediction using Bayesian Neural Networks and TensorFlow 2.0\n\nNeural networks are great for generating predictions when you have lots of training data, but by default they don't report the uncertainty of their estimates.  Uncertainty information can be super important for applications where your risk function isn't linear.  For example, suppose you're working for a logistics company, and need to choose a shipping company to ship a part which *needs* to arrive by Friday - but you don't really care how early it arrives, as long as it arrives by the deadline.  A vanilla neural network would only give you the best estimate for each company: for example, the shipment arrival time estimate for company A might be Wednesday, and Thursday for company B.  But choosing company A just because the estimated arrival time is sooner could be a mistake!  Suppose the uncertainty as to the model's estimate is much larger for company A than B: even if the *expected* arrival time is earlier for company A, it could be that company B is 99% likely to get the shipment there by Friday, while company A is only 90% likely to get it there by Friday!  My point is: often uncertainty information can really help with decision-making.\n\n![Shipping Example](http:\/\/drive.google.com\/uc?export=view&id=1dYAfnvB1wwjriK2-aKsLDyN4tk7q9Gx7)\n\nIn this post we'll build a Bayesian neural network which has two \"heads\" - that is, two endpoints of the network.  One head will predict the *value* of the estimate, and the other to predict the *uncertainty* of that estimate.  This dual-headed structure allows the model to dynamically adjust its uncertainty estimates, and because it's a Bayesian network, also captures uncertainty as to what the network parameters should be, leading to more accurate uncertainty estimates.\n\n![Dual headed net](http:\/\/drive.google.com\/uc?export=view&id=1MPspM2UhxEziX924MWX8RmGe6Sk1ovg8)\n\nUsing this network, we'll predict taxi trip durations using taxi trip data from [New York City's Open Data](http:\/\/opendata.cityofnewyork.us\/), which is hosted as a public dataset [on Google BigQuery](http:\/\/console.cloud.google.com\/marketplace\/details\/city-of-new-york\/nyc-tlc-trips).  We'll be using some simple SQL and [google-cloud-bigquery](http:\/\/pypi.org\/project\/google-cloud-bigquery\/) to load the data; [TensorFlow 2.0](http:\/\/www.tensorflow.org\/beta\/), [TensorFlow Probability](http:\/\/www.tensorflow.org\/probability), and [CatBoost](http:\/\/catboost.ai) to do the modeling; [Vaex](http:\/\/github.com\/vaexio\/vaex) for visualization; as well as the numpy\/pandas\/matplotlib\/sklearn\/seaborn stack.\n\n**Outline**\n\n- [Data](#data)\n- [Baseline Models](#baseline-models)\n- [Vanilla Neural Network](#vanilla-neural-network)\n- [Simple Bayesian Neural Network](#simple-bayesian-neural-network)\n- [Dual-headed Bayesian Density Network](#dual-headed-bayesian-density-network)\n- [Model Evaluation](#model-evaluation)\n\nFirst let's install and import the packages we'll use.","fdc4f8da":"That's a bit better than CatBoost!  Also note that the validation error is less than the training error - this is because we used dropout during training (and not during evaluation of the validation data). But again, with this network we don't get any uncertainty information, just point estimates:","ff7ec587":"## Data\n\nWe'll be predicting taxi trip durations from the start and end locations of the ride, as well as the time of day when the trip started.  New York City releases a lot of their data [publicly](http:\/\/opendata.cityofnewyork.us\/), including information about taxi rides, which is hosted as a public dataset [on Google BigQuery](http:\/\/console.cloud.google.com\/marketplace\/details\/city-of-new-york\/nyc-tlc-trips)!  Let's load the first several million rows from the yellow taxi trip dataset using Google BigQuery:","4f5d8f67":"Let's look at the validation error over the course of training.","93b2c178":"Notice how the dual-headed density network (orange) varies its uncertainty estimates, unlike the model which doesn't estimate uncertainty (blue).  For example, in the upper-left panel, the density network is much more certain of its estimate than the other model, and its predictive distribution is much sharper.  Though for other datapoints, like that in panel 5, the density network is *less* certain of its estimate, and the predictive distribution is wider.  And sometimes both models have similar levels of uncertainty, like in the lower-right panel.","84d7abc4":"## Model Evaluation\n\nLet's compare the three models we trained in terms of their predictive performance, and also in terms of the accuracy of their uncertainty estimates."}}