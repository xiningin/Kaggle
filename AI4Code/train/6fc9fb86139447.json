{"cell_type":{"61b550b7":"code","6157a71a":"code","e7940660":"code","b40b4c28":"code","68829ef5":"code","03caa433":"code","05f1eec7":"markdown","b911a922":"markdown","d03d9c4e":"markdown","af6f7b3a":"markdown","7a1d2e18":"markdown","5735dafa":"markdown","df88bf9f":"markdown"},"source":{"61b550b7":"import numpy as np\n#OpenAI gym for environments\nimport gym\nimport random","6157a71a":"#create the environment\nenv = gym.make(\"FrozenLake-v0\");","e7940660":"#get the size of the Q-table i.e. number of states and actions on each state from the env\nn_actions = env.action_space.n;\nn_states = env.observation_space.n;\n\n#create the Q-table with above dimensions and filled with zeros intially as the agent is unaware of the environment\nq_table = np.zeros((n_states,n_actions))\n#check to see 16 states with 4 actions in each state\nprint(q_table.shape)","b40b4c28":"#number of episodes for the agent to learn from\nn_episodes = 20000;\n#learning rate used in Bellman equation\nlearning_rate = 0.9;\n#maximum steps to end an episodes\nmax_steps = 99;\n#discounting rate used in Bellman equation\ngamma = 0.9;\n\n#Exploration parameters\nepsilon = 1.0;\n#exploration probability at start of episode\nmax_epsilon = 1.0;\n#minimum exploration probability\nmin_epsilon = 0.01;\n#decay rate for exploration probability\ndecay = 0.006;","68829ef5":"#store rewards\nrewards = [];\n\n#let the agent play for defined number of episodes\nfor episode in range(n_episodes):\n    #reset the environment for each episode\n    state = env.reset();\n    #define initial parameters\n    step = 0;\n    #to keep track whether the agent dies\n    done = False;\n    #keep track of rewards at each episode\n    total_rewards = 0;\n    \n    #run for each episode\n    for step in range(max_steps):\n        #generate a random number between 0,1 for exploration-eploitation tradeoff \n        #i.e. random number > epsilon -> eploitation else exploration \n        #as the agent does'nt know much because epsilon is being lowered at each step\n        #its only exploration at the start\n        e_e_tradeoff = random.uniform(0,1);\n        \n        #exploitation\n        if(e_e_tradeoff > epsilon):\n            #take the maximum reward paying action on the current state\n            action  = np.argmax(q_table[state,:]);\n            \n        #exploration\n        else:\n            #get a random action\n            action = env.action_space.sample();\n            \n        #take the action and observe the new outcome state and reward\n        new_state, reward, done, info  = env.step(action);\n        \n        #update the state-action reward value in the q-table using the Bellman equation\n        #Q(s,a) = Q(s,a) + learning_rate*[Reward(s,a) + gamma*max Q(snew,anew) - Q(s,a)]\n        q_table[state,action] = q_table[state,action] + learning_rate * (reward + gamma * np.max(q_table[new_state,:]) - q_table[state,action]);\n        \n        #add to total rewards for this episode\n        total_rewards += reward;\n        \n        #define new state\n        state = new_state;\n        \n        #end the episode if agent dies\n        if(done == True):\n            break;\n            \n    #reduce the epsilon after each episode\n    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay*episode);\n    \n    #keep track of total rewards for each episode\n    rewards.append(total_rewards);\n    \nprint(\"Score over time: \" + str(sum(rewards)\/n_episodes));\nprint(\"Q-Table: \");\nprint(q_table);\n        ","03caa433":"#reset the state\nenv.reset()\n\n# lets test for 10 episodes\nfor episode in range(10):\n    state = env.reset();\n    step = 0;\n    done = False;\n    print(\"XXXXXXXXXX\");\n    print(\"Episode: \",episode);\n    \n    for step in range(max_steps):\n        \n        #take the action with maximum expected future reward form the q-table\n        action = np.argmax(q_table[state,:]);\n        \n        new_state, reward, done, info = env.step(action);\n        \n        if done:\n            #print only the last stage to check if the agent reached the goal(G) or fell into a hole(H)\n            env.render();\n            \n            print(\"Number of steps taken: \",step);\n            break;\n            \n        state = new_state;\n\n#close the connection to the environment\nenv.close();\n    ","05f1eec7":"Implement the Q-Learning Algotrithm:","b911a922":"> The following notebook is an implementation of the **Q-Learning algorithm** using the **Q-table** for an agent to play **FrozenLake**(https:\/\/gym.openai.com\/envs\/FrozenLake-v0\/). This is an assigment of the course 2 on Deep Reinforcement Learning by **Thomas Simonini**(https:\/\/simoninithomas.github.io\/Deep_reinforcement_learning_Course\/).\n\n**Note**: Comments are included for easy understanding.","d03d9c4e":"Let our agent use this Q-table as a cheatsheet to play the game FrozenLake i.e. testing our model:","af6f7b3a":"So as we can see the **highlighted(red mark)** shows the agent's position at the end of the episode. We can see that with current parameters the agent reached the Goal(G) **7\/10 times**. That's awesome!!\n\nWe can tune the above parameters to achive better accuracy.","7a1d2e18":"Import necessary packages:","5735dafa":"Define the Hyperparameters for the training:","df88bf9f":"Create the environement and Q-Table:"}}