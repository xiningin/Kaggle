{"cell_type":{"ace41d55":"code","fbf8df64":"code","86ef0c98":"code","31bb599f":"code","ffb6d4db":"code","cd933558":"code","7fdc1056":"code","f791b588":"code","5a191229":"code","44b8d9cc":"code","dbc6c94b":"markdown","9701a181":"markdown","6bd58811":"markdown","798129ea":"markdown"},"source":{"ace41d55":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nfrom keras.models import Sequential\nfrom keras.layers import LSTM,Dense,Dropout\nfrom keras import optimizers\n\n\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\npd.set_option('display.max_rows', 600)\npd.set_option('display.max_columns', 50)\n\nimport lightgbm as lgb\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom tqdm import tqdm_notebook\n\nfrom itertools import product\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","fbf8df64":"input_dir = '..\/input\/'\npd.set_option('display.max_colwidth', 20)\npd.set_option('display.max_columns', 500)\nshops = pd.read_csv(input_dir + 'shops.csv')\nprint(\"Shops:\\n\", shops.head(), '\\n', shops.shape, '\\n')\nitem_cat = pd.read_csv(input_dir + 'item_categories.csv')\nprint(\"item_categories: \\n\", item_cat.head(), '\\n', item_cat.shape, '\\n')\nsales_data = pd.read_csv(input_dir + 'sales_train.csv')\nprint(\"sales_train: \\n\", sales_data.head(), '\\n', sales_data.shape, '\\n')\nitems = pd.read_csv(input_dir + 'items.csv')\nprint(\"items: \\n\", items.head(), '\\n', items.shape, '\\n')\nsample_submission = pd.read_csv(input_dir + 'sample_submission.csv')\nprint(\"sample_submission: \\n\", sample_submission.head(), '\\n', sample_submission.shape, '\\n')\ntest_data = pd.read_csv(input_dir + 'test.csv')\nprint(\"test: \\n\", test_data.head(), '\\n', test_data.shape, '\\n')","86ef0c98":"def simple_eda(df):\n    print(\"----------TOP 5 RECORDS--------\")\n    print(df.head(5))\n    print(\"----------INFO-----------------\")\n    print(df.info())\n    print(\"----------Describe-------------\")\n    print(df.describe())\n    print(\"----------Columns--------------\")\n    print(df.columns)\n    print(\"----------Data Types-----------\")\n    print(df.dtypes)\n    print(\"-------Missing Values----------\")\n    print(df.isnull().sum())\n    print(\"-------NULL values-------------\")\n    print(df.isna().sum())\n    print(\"-----Shape Of Data-------------\")\n    print(df.shape)","31bb599f":"print(\"Sales Data -------------------------> \\n\")\nsimple_eda(sales_data)\nprint(\"Test data -------------------------> \\n\")\nsimple_eda(test_data)\nprint(\"Item Categories -------------------------> \\n\")\nsimple_eda(item_cat)\nprint(\"Items -------------------------> \\n\")\nsimple_eda(items)\nprint(\"Shops -------------------------> \\n\")\nsimple_eda(shops)\nprint(\"Sample Submission -------------------------> \\n\")\nsimple_eda(sample_submission)","ffb6d4db":"sales_data['date'] = pd.to_datetime(sales_data['date'],format = '%d.%m.%Y')\ndataset = sales_data.pivot_table(index = ['shop_id','item_id'],values = ['item_cnt_day'],columns = ['date_block_num'],fill_value = 0)","cd933558":"dataset.reset_index(inplace = True)\ndataset = pd.merge(test_data,dataset,on = ['item_id','shop_id'],how = 'left')\ndataset.fillna(0,inplace = True)\ndataset.drop(['shop_id','item_id','ID'],inplace = True, axis = 1)\nprint(dataset.head())","7fdc1056":"# X we will keep all columns execpt the last one \nX_train = np.expand_dims(dataset.values[:,:-1],axis = 2)\n# the last column is our label\ny_train = dataset.values[:,-1:]\n\n# for test we keep all the columns execpt the first one\nX_test = np.expand_dims(dataset.values[:,1:],axis = 2)\n\n# lets have a look on the shape \nprint(X_train.shape,y_train.shape,X_test.shape)","f791b588":"# our defining our model \nmy_model = Sequential()\nmy_model.add(LSTM(units = 256,input_shape = (33,1)))\nmy_model.add(Dropout(0.5))\nmy_model.add(Dense(1))\n\nsgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmy_model.compile(loss = 'mse',optimizer = sgd, metrics = ['mean_squared_error'])\nmy_model.summary()","5a191229":"my_model.fit(X_train,y_train,batch_size = 1000,epochs = 5)\n","44b8d9cc":"# creating submission file \nsubmission_pfs = my_model.predict(X_test)\n# we will keep every value between 0 and 20\nsubmission_pfs = submission_pfs.clip(0,20)\n# creating dataframe with required columns \nsubmission = pd.DataFrame({'ID':test_data['ID'],'item_cnt_month':submission_pfs.ravel()})\n# creating csv file from dataframe\nsubmission.to_csv('sub_pfs.csv',index = False)","dbc6c94b":"* Step FINAL: submit a solution csv","9701a181":"## Review criteria \nBefore preparing to submit the assignment, pay attention to the following criterions. Try to complete most of them and present results in a form that can be easily assessed.\n\n### Clarity\n- The clear step-by-step instruction on how to produce the final submit file is provided\n- Code has comments where it is needed and meaningful function names\n\n### Feature preprocessing and generation with respect to models\n- Several simple features are generated\n- For non-tree-based models preprocessing is used or the absence of it is explained\n\n### Feature extraction from text and images\n- Features from text are extracted\n- Special preprocessings for text are utilized (TF-IDF, stemming, levenshtening...)\n\n### EDA\n- Several interesting observations about data are discovered and explained\n- Target distribution is visualized, time trend is assessed\n\n### Validation\n- Type of train\/test split is identified and used for validation\n- Type of public\/private split is identified\n\n### Data leakages\n- Data is investigated for data leakages and investigation process is described\n- Found data leakages are utilized\n\n### Metrics optimization\n- Correct metric is optimized\n\n### Advanced Features I: mean encodings\n- Mean-encoding is applied\n- Mean-encoding is set up correctly, i.e. KFold or expanding scheme are utilized correctly\n\n### Advanced Features II\n - At least one feature from this topic is introduced\n - Hyperparameter tuning\n - Parameters of models are roughly optimal\n \n### Ensembles\n - Ensembling is utilized (linear combination counts)\n - Validation with ensembling scheme is set up correctly, i.e. KFold or Holdout is utilized\n - Models from different classes are utilized (at least two from the following: KNN, linear models, RF, GBDT, NN)","6bd58811":"Next Step: Join dataframes and do groupby","798129ea":"Step 0: load and print out the data"}}