{"cell_type":{"71cbe6ed":"code","b22834b5":"code","e842965e":"code","d5c73a50":"code","820b3f35":"code","4c570288":"code","acf4b415":"code","30a09ac8":"code","ed101c0f":"code","5fcef7c7":"code","b63e258a":"code","3cc75288":"code","47f86449":"code","f05d0b1d":"code","6a99c6d3":"code","656a17d8":"code","da09b56f":"code","3407444a":"code","86996a1c":"code","f470a2cc":"code","544aba6d":"code","f70c263f":"code","c0a04fbb":"code","8dae4c33":"code","765158b4":"code","c34b6c8f":"code","9c0faf42":"code","753096d3":"code","b9131992":"code","c5474a6c":"code","48b52125":"code","51f4fc9d":"code","56858da0":"code","51c6367c":"code","35f31499":"code","558d97d3":"code","85f9b7a3":"code","39939b6e":"code","0b2ffaa7":"markdown","7696496f":"markdown","9f7013df":"markdown","ec755ab6":"markdown","5224adda":"markdown","f0cb0fb8":"markdown","9d9199da":"markdown","e1d806e3":"markdown","05530d87":"markdown","d2cb043c":"markdown","739f63d3":"markdown","2c7842b6":"markdown","86db7cb8":"markdown","af71575d":"markdown","88f9729e":"markdown","bf4c0e13":"markdown","5e0c73f5":"markdown","dba44b30":"markdown","cdad28eb":"markdown","a371105b":"markdown","a7de10b3":"markdown","0b5b0119":"markdown","6ce96c7e":"markdown","6892b44a":"markdown","2223a98d":"markdown","54089e14":"markdown","254ea03d":"markdown","77e54ab3":"markdown","9de4d1dc":"markdown","a3c79d30":"markdown","c824c1b9":"markdown","5c38c122":"markdown","bf44eedc":"markdown","0f5d5d14":"markdown","117df6a2":"markdown","41be1910":"markdown","1f04c259":"markdown","9a2e8e88":"markdown","a4e77c55":"markdown","04f80125":"markdown","00fc0ecf":"markdown","f7ade7ee":"markdown","0905fde7":"markdown"},"source":{"71cbe6ed":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b22834b5":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\ncombine = [train_df, test_df]","e842965e":"train_df.head()","d5c73a50":"train_missing_values_count = train_df.isnull().sum()\ntest_missing_values_count = test_df.isnull().sum()\n\nprint(\"Missing values in training data:\")\nfor feature, count in train_missing_values_count.items():\n    if count > 0:\n        print(feature, count)\nprint(\"\\nMissing values in test data:\")\nfor feature, count in test_missing_values_count.items():\n    if count > 0:\n        print(feature, count)","820b3f35":"train_df.describe()","4c570288":"train_df.describe(include=['O'])","acf4b415":"train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","30a09ac8":"train_df[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","ed101c0f":"train_df[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","5fcef7c7":"train_df[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","b63e258a":"g = sns.FacetGrid(train_df, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","3cc75288":"g = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ng.map(plt.hist, 'Age', alpha=.5, bins=20)\ng.add_legend();","47f86449":"train_df = train_df.drop(['PassengerId', 'Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncombine=[train_df, test_df]\ntrain_df.head()","f05d0b1d":"test_df.head()","6a99c6d3":"# empty array to hold Age's median values for Pclass x Sex pairs\nage_guesses = np.zeros((3, 2)) # 3 values for Pclass, 2 values for Sex\n\nclasses = [1, 2, 3]\nsexes = ['female', 'male']\n\nfor dataset in combine:\n    # obtain the median Age values for every Pclass x Sex pair\n    for i in range(3): # Pclass's values: 1, 2, 3\n        for j in range(2): # Sex's values: 0, 1\n            guess_pair = dataset[(dataset['Pclass'] == classes[i]) & (dataset['Sex'] == sexes[j])]['Age'].dropna() # dataframe contains all Age values for the specific Pclass, Sex pair\n            age_guess = guess_pair.median() # the median Age value for the specific pair\n            age_guesses[i, j] = int(age_guess\/0.5 + 0.5) * 0.5 # convert age_guess to the nearest 0.5 and assign it to age_guesses\n    \n    # fill in the missing values in Age with the corresponding median Age value for each pair\n    for i in range(3):\n        for j in range(2):\n            dataset.loc[(dataset['Age'].isnull()) & (dataset['Pclass'] == classes[i]) & (dataset['Sex'] == sexes[j]), 'Age'] = age_guesses[i, j]\n            \nprint(\"Missing values in Age for\\n   training dataset: \", train_df['Age'].isnull().sum())\nprint(\"   test dataset: \", test_df['Age'].isnull().sum())","656a17d8":"# fill in missing values with the most frequently occuring fare price\ntest_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\n\nprint(\"Missing values in Fare for\\n   test dataset : \", test_df['Fare'].isnull().sum())","da09b56f":"# fill in missing values with the most frequently occuring port of embarkation\ntrain_df['Embarked'].fillna(train_df['Embarked'].dropna().mode()[0], inplace=True)\n\ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Embarked', ascending=False)","3407444a":"for dataset in combine:\n    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False) # extract alphabet letters that end with a period\n    \nprint(\"Training data:\")\nprint(train_df['Title'].value_counts())\nprint(\"\\nTest data:\")\nprint(test_df['Title'].value_counts())","86996a1c":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Dr', 'Rev', 'Major', 'Mlle', 'Col', 'Countess', 'Jonkheer', 'Don', 'Lady', 'Mme', 'Capt', 'Sir', 'Ms', 'Dona'], 'Rare')\n\nprint(\"Training data:\")\nprint(train_df['Title'].value_counts())\nprint(\"\\nTest data:\")\nprint(test_df['Title'].value_counts())","f470a2cc":"train_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","544aba6d":"train_df = train_df.drop(['Name'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()","f70c263f":"for dataset in combine:\n    dataset['Family'] = dataset['SibSp'] + dataset['Parch'] + 1 # add 1 to include the sample passenger\n    \ntrain_df[['Family', 'Survived']].groupby(['Family'], as_index=False).mean().sort_values(by='Family', ascending=True)","c0a04fbb":"train_df['Family'].value_counts()","8dae4c33":"for dataset in combine:\n    dataset['Alone'] = 0\n    dataset.loc[dataset['Family'] == 1, 'Alone'] = 1\n    \ntrain_df[['Alone', 'Survived']].groupby(['Alone'], as_index=False).mean()","765158b4":"train_df = train_df.drop(['SibSp', 'Parch', 'Family'], axis=1)\ntest_df = test_df.drop(['SibSp', 'Parch', 'Family'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()","c34b6c8f":"sex_mapping = {'female': 0, 'male': 1}\nembarked_mapping = {'C': 0, 'Q': 1, 'S': 2}\ntitle_mapping = {'Master': 1, 'Miss': 2, 'Mr': 3, 'Mrs': 4, 'Rare': 5}\nfor dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map(sex_mapping)\n    dataset['Sex'] = dataset['Sex'].fillna(0)\n    dataset['Embarked'] = dataset['Embarked'].map(embarked_mapping)\n    dataset['Embarked'] = dataset['Embarked'].fillna(0)\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \ntrain_df.head()","9c0faf42":"# convert Age values into integers to band Age in integer numbers\nfor dataset in combine:\n    dataset['Age'] = dataset['Age'].astype(int)\n\n# split Age into 5 bands. use cut() to obtain equally ranged bands since Age is relatively limited\ntrain_df['AgeBand'] = pd.cut(train_df['Age'], 5) \n\n# determine AgeBand's corerlation with Survived\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","753096d3":"for dataset in combine:\n    dataset.loc[dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[dataset['Age'] > 64, 'Age'] = 4\ntrain_df.head()","b9131992":"train_df = train_df.drop(['AgeBand'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","c5474a6c":"for dataset in combine:\n    dataset['Class*Age'] = (dataset['Pclass'] * dataset['Age']).astype(int)\n\ntrain_df.loc[:, ['Pclass', 'Age', 'Class*Age']].head()","48b52125":"train_df[['Class*Age', 'Survived']].groupby(['Class*Age'], as_index=False).mean()","51f4fc9d":"# split Fare into 4 bands. use qcut() to get equally distributed bins since Fare can range exponentially\ntrain_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\n\n# determine FareBand's correaltion with Survived\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","56858da0":"for dataset in combine:\n    dataset.loc[dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31.0), 'Fare'] = 2\n    dataset.loc[dataset['Fare'] > 31.0, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \ntrain_df.head()","51c6367c":"train_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","35f31499":"from sklearn.model_selection import cross_val_score\n\nX_train = train_df.drop(['Survived'], axis=1)\ny_train = train_df['Survived']\n\ndef get_cvs(model, X_train, y_train, cv = 5, scoring = 'accuracy'):\n    scores = cross_val_score(model, \n                             X_train, \n                             y_train,\n                             cv = cv,\n                             scoring = scoring)\n    return round(scores.mean() * 100, 2)","558d97d3":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\n\nscores = []\n\ndecTree = DecisionTreeClassifier()\ndecTree_score = get_cvs(decTree, X_train, y_train)\nscores.append(('Decision Tree', decTree_score))\n\nknn = KNeighborsClassifier(n_neighbors = 7) # 7 neighbors resulted with the highest accuracy\nknn_score = get_cvs(knn, X_train, y_train)\nscores.append(('k-Nearest Neighbors', knn_score))\n\nlinSvc = LinearSVC(max_iter=5000)\nlinSvc_score = get_cvs(linSvc, X_train, y_train)\nscores.append(('Linear SVC', linSvc_score))\n\nlogReg = LogisticRegression()\nlogReg_score = get_cvs(logReg, X_train, y_train)\nscores.append(('Logistic Regression', logReg_score))\n\ngaussian = GaussianNB()\ngaussian_score = get_cvs(gaussian, X_train, y_train)\nscores.append(('Naive Bayes Classifier', gaussian_score))\n\nperceptron = Perceptron()\nperceptron_score = get_cvs(perceptron, X_train, y_train)\nscores.append(('Perceptron', perceptron_score))\n\nrandForest = RandomForestClassifier(n_estimators=100) # 100 estimators seems to yield the highest score\nrandForest_score = get_cvs(randForest, X_train, y_train)\nscores.append(('Random Forest', randForest_score))\n\nsgd = SGDClassifier()\nsgd_score = get_cvs(sgd, X_train, y_train)\nscores.append(('Stochastic Gradient Descent', sgd_score))\n\nsvc = SVC(max_iter=5000)\nsvc_score = get_cvs(svc, X_train, y_train)\nscores.append(('Support Vector Machines', svc_score))\n\nscores_df = pd.DataFrame(scores, columns=['Model', 'Score'])\nscores_df.sort_values(by='Score', ascending=False)","85f9b7a3":"logReg.fit(X_train, y_train)\n\ncoeff_df = pd.DataFrame(train_df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df['Correlation'] = pd.Series(logReg.coef_[0])\ncoeff_df.sort_values(by='Correlation', ascending=True)","39939b6e":"svc.fit(X_train, y_train)\n\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\ny_pred = svc.predict(X_test)\n\nsubmission = pd.DataFrame({\n    'PassengerId': test_df['PassengerId'],\n    'Survived': y_pred\n})\n\nsubmission.to_csv('submission.csv', index=False)","0b2ffaa7":"### Categorical Feature Distribution","7696496f":"Observations:\n* Passengers who embarked from port C had a relatively higher survival rate than those who embarked from port Q or S.\n\nDecisions:\n* Include Embarked to our model training.","9f7013df":"Observations:\n- Support Vector Machines resulted with the highest cross-validation score of 81.26%.\n\nDecisions:\n- We decide to use **Support Vector Machines** to make our final prediction using the test dataset.\n\n### Validate Assumptions using Logistic Regression\nBefore jumping into our final prediction, it may be a good idea to validate our initial assumptions. One useful aspect of Logistic Regression is that we can inspect the coefficients of each feature to determine its correlation with the target feature (Survived). This way, we can check whether the features we assumed to have the most correlation (contributed most to one's survival) did indeed have the most or not.","ec755ab6":"Observations: \n* Smaller values of Class\\*Age have a higher survival rate than larger values.\n* Few values had zero correlation with survival.\n\nDecisions:\n* Include Class\\*Age to model training, but double-check its contribution to accuracy.","5224adda":"## 3. Create\n### 3.1) Extract Features\n#### *Name -> Title*\nThe Name feature contains many titles such as Mr. and Mrs. which may have some correlation with a passenger's survival. We can extract these titles using RegEx patterns.","f0cb0fb8":"Observations:\n* The survival rate for different family sizes varies.\n* Families from 2 to 4 members had a higher survival rate than those alone.\n* Families of more than 4 members had a lower survival rate than those alone.\n* Families of 8 and 11 had zero survival rates.\n\nDecisions:\n* The zero survival rates of larger families may be due to small corresponding sample sizes.\n* Further examine the number of passengers for each Family.","9d9199da":"Now we are ready to train our model.\n***\n# Training Model\n## Model Selection\nFirst, we identify the type of our problem. We are provided with a training dataset, so we are performing *supervised learning*. Also, we are determining the relationship between a target feature (Survived) and other fetaures (Pclass, Sex, Age, etc.), so this is a *classification* and *regression problem*. Keeping these in mind, we have the following model choices:\n* Artificial Neural Network\n* Decision Tree\n* KNN (k-Nearest Neighbors)\n* Logistic Regression\n* Naive Bayes Classifier\n* Perceptron\n* Random Forest\n* RVM (Relevance Vector Machine)\n* Support Vector Machine","e1d806e3":"Observations:\n- Pclass = 3 had the most passengers, but most of them did not survive.\n- Most passengers in Pclass = 1 survived. (Assumption #3)\n- Most infants in every Pclass had a high survival rate (Assumption #2).\n\nDecisions:\n- Include Pclass for model training.\n***","05530d87":"Observations:\n* More than half of the passengers boarded the Titanic alone. (Total 891 passengers)\n* The larger the family size, the smaller the number of corresponding passengers.\n\nDecisions:\n* Instead of using Family, derive a new feature which indcates whether the passenger was alone or not.","d2cb043c":"### 3.3) Combine Features *(cont.)*\n#### *Pclass, Age -> Class*\\**Age*\nThe survival rate of passengers of the same class may differ across their age. We can combine the two to create a new feature multiplying Pclass and Age.","739f63d3":"* The training dataset's survival rate of 38% is close to the actual survival rate of 32%.\n* The passengers are generally adults, 50% of them ranging from 20-years old to 38-years old, though there are also elderly people.\n* Most of the passengers boarded without sibings, spouses, nor parents.\n* Most of the passengers paid a fare around 32 dollars, though a few paid a very lot reaching up to 512 dollars.","2c7842b6":"* Pclass = 1 has the highest survival rate, supporting our assumption of upper-class passengers. We should include the feature Pclass.\n* Sex = female has the highest survival rate, supporting our assumption of female passengers. We should include the feature Sex.\n* Some values of SibSp have zero correlation with survival. Perhaps deriving a new feature from SibSp may prove to be more useful.\n* Some values of Parch have zero correlation with survival. Perhaps deriving a new feature from Parch may prove to be more useful.","86db7cb8":"Now we analyze the survival rates of each title to determine if there is a correlation between Title and Survived.","af71575d":"## Missing Values\nCheck if there are any features with missing values.","88f9729e":"Observations:\n* The survival rate of each age band differs across age bands.\n* The first band (children) survived the most, whereas the last band (elderly people) survived the least.\n* Supports our assumption of high survival rate for children.\n\nDecisions:\n* Use 5 bands for Age.\n\nWe now convert Age into an ordinal feature of 5 bands.","bf4c0e13":"# Strategy\n1. Drop:\n    1. PassengerId - does not contribute to survival in training dataset.\n    2. Name - every value is unique, so unlikely to be correlated to survival. (But may be useful to extract a new feature.)\n    3. Ticket - many duplicate values (25%), and unlikely to be correlated to survival.\n    4. Cabin - many missing values (77.1%, 78.2%), many duplicate values (28%), and unlikely to be correlated to survival.\n2. Complete:\n    1. Age - considerable amount of missing values, but very likely to be correlated to survival.\n    2. Embarked - only a few missing values, and likely to be correlated to survival.\n    3. Fare - only a few missing values, and likely to be correlated to survival.\n3. Create: \n    1. Title - extract titles from Name (e.g. Mr., Miss.).\n    2. Family - combine SibSp and Parch to get a total family member count.\n4. Convert:\n    1. Sex, Embarked, Title - convert to a numerical nominal feature.\n    2. Age - convert to an ordinal feature using bands.  \n        3-3. Class*Age - multiply Pclass and Age (after it is converted).\n    3. Fare - convert to an ordinal feature using bands.\n    \n## Assumptions\n* Women (Sex = female) had a higher survival rate.\n* Children (Age < ?) had a higher survival rate.\n* Upper-class passengers (Pclass = 1) had a higher survival rate.\n***","5e0c73f5":"## 2. Complete \n### 2.1) Continuous Features\n#### *Age*\nFirst, we complete the missing values in Age. We can do so by taking the median value of Age for passengers with the same values for other features. Pclass and Sex seem to be a reasonable pair. \n\nSince Age consists of float values in steps of 0.5, we need to convert the median age guess to the nearest 0.5 age as well.","dba44b30":"\nNumerical:\n* Discrete: Age, SibSp, Parch\n* Continuous: Fare\n* Timeseries based: None\n\nCategorical:\n* Nominal: PassengerId, Survived, Name, Sex, Embarked\n* Ordinal: Pclass\n\nMixed:\n* Alphanumeric: Ticket, Cabin","cdad28eb":"### Correlating Numerical and Ordinal Features \n#### *Pclass, Age <-> Survived*","a371105b":"## Analyze by Visualizing Data\n### Correlating Numerical Features\n#### *Age <-> Survived*","a7de10b3":"Missing values in training data:\n* Cabin (77.1%), Age (19.9%), Embarked (0.2 %)\n\nMissing values in test data:\n* Cabin (78.2%), Age (20.6%), Fare (0.2%)","0b5b0119":"* The name of each passenger is unique.\n* Sex has two possible values, male or female.\n* Ticket has many duplicate values, around 24% (210 tickets). \n* Cabin has many duplicate values as well, around 28% (57 cabins).\n* Embarked has three possible values, C, Q, or S.\n***","6ce96c7e":"## Model Quality\nWe have two choices to measure our model's quality: train-test split, cross-validation. In this case, we have a relatively small dataset of training data, 891 passengers, so we decide to use cross-validation. \n\n### Cross-Validation","6892b44a":"#### *Fare*\nFare is missing only one value in the test dataset, so we can simply fill it in with the most frequently occurring value. ","2223a98d":"Since Fare is converted, we no longer need FareBand so we drop it from the dataset.","54089e14":"### 3.2) Combine Features\n#### *SibSp, Parch -> Family -> Alone*\nCombining features to create a new feature may further help with training a model. Earlier we have examined that certain values of SibSp and Parch resulted in zero correlation with Survived. Combining these two to get a new Family feature may help to improve our model training.","254ea03d":"Observations:\n* Whereas many passengers have titles such as \"Mr.\" or \"Miss.\", only very few have other titles such as \"Dr.\" or \"Rev.\"\n\nDecisions:\n* Categorize the minority titles as 'Rare' to help model training.","77e54ab3":"Observations:\n* Most fares ranged between 0 dollars to 31 dollars.\n* Fares above 31 dollars went up as high as aroudn 512 dollars. \n* As the fare generally increases, so does the passenger's survival rate.\n\nDecisions:\n* Use 4 bands for Fare.\n\nWe now convert Fare into an ordinal feature using 4 bands.","9de4d1dc":"Observations:\n* Many Title values had a significant survival rate.\n\nDecisions:\n* Include Title to model training.\n\nNow that we have successfully extracted the Title feature from Name, we can now drop Name from the datasets.","a3c79d30":"## 4. Convert \n### 4.1) Categorical -> Numerical\n#### *Sex, Embarked, Title*\nWe convert categorical features of string values to number values. Such features include Sex and Title.","c824c1b9":"### 2.2) Categorical Features\n#### *Embarked*\nEmbarked is missing only two values in the training dataset, so we can simply fill them in with the most frequently occurring value.","5c38c122":"Observations:\n* Passengers who were alone had a higher survival rate thant those who were in families.\n\nDecisions:\n* Include Alone to our model training.\n\nNow that we have derived a new useful feature, we can now drop SibSp, Parch, and FamilySize.","bf44eedc":"# Titanic Survival with ML\n(Based on [Titanic Data Science Solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\/comments) notebook.)\n\n# Data Analysis\nRead in the training and test data.","0f5d5d14":"### 4.3) Continuous -> Ordinal\n#### *Fare*\nSimilar to how we converted Age from a numerical discrete variable to an ordinal variable using bands, we convert Fare from a continuous variable to a ordinal variable as well.","117df6a2":"# Data Wrangling\n## 1. Drop \n#### *PassengerId, Ticket, Cabin, Name*\nFrom our initial data analysis, we decided to drop PassengerId (only from training dataset), Name, Ticket, and Cabin. However, we drop Name *after* we are done extracting name titles to create a new feature.","41be1910":"## Model Predictions\nWe now check the model quality for each model from our model choices from above:","1f04c259":"### 4.2) Discrete -> Ordinal\n#### *Age*\nCreating bands of a numerical continuous feature may help determine its correlation with the target feature. We first start with Age.","9a2e8e88":"## Feature Distribution\n### Numerical Feature Distribution","a4e77c55":"## Feature Datatypes\nExamine the data and determine each feature's datatype.","04f80125":"# Feature Analysis\n## Analyze by Pivoting Features\n#### *Pclass, Sex, SibSp, Parch <-> Survived*\nPivoting features can help us quickly analyze feature correlations early in our analysis. We can only do so with features without empty values, and are categorical or discrete.","00fc0ecf":"Observations:\n- Infants (Age < 4) had a significantly high survival rate. (Assumption #2)\n- Many young people (16 < Age < 28) died than survived.\n- Elderly people (Age > 60) had a significantly low survival rate.\n- Most passengers' ages ranged from around 16~40 years old.\n\nDecisions:\n- Include Age into our model training.\n- Complete the Age feature for missing values.\n- Band the Age feature into age groups. ","f7ade7ee":"Since Age is converted, we no longer need AgeBand so we drop it from the dataset.","0905fde7":"Observations:\n* 'Sex', 'Pclass', and 'Age' contributed most in determining one's survival in the respective order with the highest negative correlations.\n* 'Sex' has the highest negative correlation, indicating that being female (female: 0, male: 1) increased one's survival rate (Survived = 1).\n* 'Pclass' has the second highest negative correlation, indicating that being upper-class (smaller 'Pclass') increased one's survival rate.\n* 'Age' has the third highest negative correlation, indicating that being younger (smaller 'Age') increased one's survival rate.\n\nDecisions:\n* We successfully validate our assumptions on 'Sex', 'Pclass', and 'Age'.\n---\n# Submission\nWe now submit our final prediction."}}