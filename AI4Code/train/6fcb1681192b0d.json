{"cell_type":{"0a584c06":"code","3cfb701b":"code","4956842d":"code","fbb6b59d":"code","34a73b49":"code","50d9d470":"code","fb8eefb2":"code","18fed7eb":"code","6e1ab10b":"code","b3180f04":"code","2c83755a":"code","3e440f58":"code","60204280":"code","de12f75e":"code","392d60d0":"code","92666e31":"code","4e665e1d":"code","f1a33a7e":"code","7a9fa926":"code","941b7b9f":"code","e50844eb":"code","4108f342":"code","165c53e8":"code","69eac545":"code","d72e8cbb":"code","1954a9a9":"code","09cd93a9":"code","83fda6fa":"code","289b53b1":"code","5d3775a4":"code","89fb7bce":"code","2534abf9":"code","4d4352d5":"code","a7da920b":"code","e316e35c":"code","440d0290":"code","309ee713":"code","bf4a32d9":"code","7fe7ead3":"code","1ba25d68":"code","68688d00":"code","40e2b041":"code","d31b2dc1":"code","f7dbe67c":"code","9eb18a11":"code","ad5ee468":"code","61aef271":"code","9071283c":"code","509a8f30":"code","94901a0b":"code","533f889c":"code","198e3515":"code","961dadb9":"code","31cdc305":"code","0e448540":"code","51a8be92":"code","e741cb47":"code","48cb3d0c":"code","451dce7a":"code","522d7295":"code","c2e50b2b":"code","9f0bc015":"code","b60738bd":"code","c7b767ed":"code","c830d402":"code","c9b72471":"code","09052fd3":"code","8ab38357":"code","688c5775":"code","a7fcb713":"code","30be9f38":"code","7a53f776":"code","c5e59f80":"code","5b61a694":"code","ac1ea865":"code","b2af51d0":"code","d72e8d18":"code","05258bc2":"code","bd1ab158":"code","7308184f":"code","e1667fe7":"code","f1f80804":"code","97f00a3a":"code","67a31ea1":"code","654d5238":"code","77cdba2c":"code","b39a547e":"code","41508db7":"code","dea45197":"code","f42a0378":"code","17711667":"code","f29e7f2b":"code","aebd921f":"code","befcd975":"code","6e51aa86":"code","97f6762c":"markdown","dac84dc8":"markdown","aa4b30ec":"markdown","778d36d3":"markdown","48bccd6e":"markdown","d751d5ea":"markdown","6561d959":"markdown","23b4a002":"markdown","e433131f":"markdown","e624f623":"markdown","519a816a":"markdown","14ff49a6":"markdown","006ef874":"markdown","2f76ce90":"markdown","a9a9054c":"markdown","ebce0432":"markdown","b5e280fb":"markdown","d4a32f71":"markdown","ca6a44eb":"markdown","038b3b6b":"markdown","7f381fbb":"markdown","14ed2b6f":"markdown","bf6645b8":"markdown","76f7e73b":"markdown","52f20f9f":"markdown","2ffa925e":"markdown","b6a5b1ae":"markdown","2d938d63":"markdown","1b42d257":"markdown","15d23ba8":"markdown","98fd51b0":"markdown","97e1d91a":"markdown","9f3a6aa7":"markdown","5128e749":"markdown","2b4f3175":"markdown","19328c26":"markdown","67674802":"markdown","430c1d2c":"markdown","a9159819":"markdown","5b9d340d":"markdown","8e849f15":"markdown","90298489":"markdown","e81a710b":"markdown","ec76e4d3":"markdown","9273475f":"markdown","1f4c6514":"markdown","edef486a":"markdown","966e56fd":"markdown","6f41c83b":"markdown","1833fe01":"markdown","e7252d59":"markdown","d1bf84d0":"markdown","038a4894":"markdown","61b846ce":"markdown","d6507aa6":"markdown","884e2a23":"markdown","34ebf0a6":"markdown","9f84d796":"markdown","9b6032a8":"markdown","03da3286":"markdown","5a0f60a6":"markdown","ea304bd6":"markdown","0baab8fd":"markdown","5ee18073":"markdown","0fa1883c":"markdown","78df65b3":"markdown","74f43aa0":"markdown","f6b792a9":"markdown","2a4a2753":"markdown","babf2565":"markdown","5051abf2":"markdown","a9b95938":"markdown","09e1bb31":"markdown","73c506f9":"markdown","8bc01e3f":"markdown"},"source":{"0a584c06":"import re\nfrom pathlib import Path\nimport numpy as np \nimport pandas as pd\nimport pickle\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport matplotlib.pyplot as plt\n\nfrom skimage import io\n\n# tqdm is a library that enables you to visualize the progress of a for loop by displaying a configurable progress bar\nfrom tqdm.notebook import tqdm\ntqdm().pandas();\n\nprint('import complete')","3cfb701b":"def extract_all_image_paths(input_path = Path.cwd()\/'..\/input\/breast-histopathology-images'):\n    image_paths = [image_path for image_path in Path.glob(input_path,pattern = '*\/*\/*.png')]\n    return image_paths","4956842d":"image_paths = extract_all_image_paths()","fbb6b59d":"print(f'We have a total of {len(image_paths)} images.')","34a73b49":"# let's check that we correctly extracted our paths\ndisplay(image_paths[0:3])\ndisplay(image_paths[10000:10003])","50d9d470":"# Extracts a dict of lists containing the informaion of each path\ndef extract_metadata(image_paths) -> dict:\n    path_data = {'path':[],'patient_id':[],'x_coord':[] ,'y_coord':[],'target':[]}\n    pattern = '\\\/(\\d+)_.+_x(\\d+)_y(\\d+)_.+(\\d)'\n    for image_path in tqdm(image_paths,total = 277524):\n        meta_data = re.search(pattern, str(image_path))\n        path_data['path'].append(image_path)\n        path_data['patient_id'].append(meta_data.group(1))\n        path_data['x_coord'].append(meta_data.group(2))\n        path_data['y_coord'].append(meta_data.group(3))\n        path_data['target'].append(meta_data.group(4))\n    return path_data","fb8eefb2":"# retrieve dict of path metadata\npath_data = extract_metadata(image_paths)","18fed7eb":"# extract and display the first 3 rows of each value\nfor value in path_data.values():\n    display(value[0:3]) ","6e1ab10b":"# convert dictionary to pandas dataframe --> convert path_data dict to dataframe\ndf = pd.DataFrame.from_dict(path_data)","b3180f04":"# prints the first five rows of the dataframe\ndf.head()","2c83755a":"# to get a better understanding of our dataframe we use the .info() method\ndf.info()","3e440f58":"df.iloc[:,2:5]","60204280":"# loop through the desired columns and change the types\nfor col in df.iloc[:,2:5]:\n    df[col] = df[col].astype('int')","de12f75e":"df.info()","392d60d0":"# Function below gets the the number of patches for each patient, \n# and gets the ratio of cancerous patches to non-cancerous patches for each patient\n\ndef get_image_count_and_cancer_ratios(path_df):\n    # Return the count of images per patient\n    s1 = path_df.groupby('patient_id')['target'].count()\n    # Return the ratio of cancerous to non-cancerous images per-patient\n    s2 = path_df.groupby('patient_id')['target'].mean()\n\n    # Merge the series into one dataframe that uses the same index\n    df_summary = pd.concat([s1,s2],axis = 1)\n    df_summary.columns = ['n_patches\/patient','cancer_ratio']\n    return df_summary","92666e31":"df_summary = get_image_count_and_cancer_ratios(path_df = df)\ndisplay(df_summary)","4e665e1d":"# Lets get a statistical summary of our dataframe\ndisplay (df_summary.describe())","f1a33a7e":"# set bin values for number of images\nbin_values = np.linspace(0,2400,25)\n# below we will group our data into bins in order to e\ndef get_binned_cancer_ratio_df(path_df,bin_values):\n    df_summary = get_image_count_and_cancer_ratios(path_df)\n    # use cut to determine which values fit into which bin\n    bins = pd.cut(df_summary['n_patches\/patient'], bin_values)\n    # group your dataframe by the bins\n    binned_df = df_summary.groupby(bins).median() \n    binned_df['non-cancer_ratio_median'] = binned_df['cancer_ratio'].apply(lambda x: 1-x)\n    binned_df['n_patients_per_bin'] = df_summary['n_patches\/patient'].groupby(bins).count()\n    # rename cancer ratio to cancer ratio median since we are taking the median now\n    binned_df.rename(columns={'cancer_ratio':'cancer_ratio_median'},inplace=True)\n    return binned_df","7a9fa926":"binned_df = get_binned_cancer_ratio_df(df,bin_values)\nbinned_df # fix the average number n_imgs\/patient","941b7b9f":"def plot_patch_info(binned_df,bin_values):\n    \n    # get the the ranges for the x-axis since this is supposed to emulate a histogram\n    ranges = []\n    for i in range(len(bin_values)):\n        try: r = '('+str(int(bin_values[i]))+'-'+str(int(bin_values[i+1]))+')'\n        except: pass\n        ranges.append(r)\n    \n    # Number of Patients per Bin -> Bar Plot\n    trace_0 = go.Bar(name='Number of Patients Per Bin',x=ranges, y=binned_df['n_patients_per_bin'],marker_color='#ff6efa');\n    # Ratio of Cancerous Images -> Bar Plot\n    trace_1 = go.Bar(name='Cancerous Images', x=ranges, y=binned_df['cancer_ratio_median'],marker_color='#8634eb');\n    # Ratio of Non-Cancerous Images -> Bar Plot\n    trace_2 = go.Bar(name='Non-Cancerous Images', x=ranges, y=binned_df['non-cancer_ratio_median'],marker_color='#ff87c9')\n    # Number of Patches per Patient vs Cancer Ratio of Image -> Scatter Plot\n    trace_3 = go.Scatter(name='# Patches vs Cancer Ratio ',x = df_summary['n_patches\/patient'],y = df_summary['cancer_ratio'],mode='markers',marker=dict(size=16,color=df_summary['cancer_ratio'], colorscale=[[0.0, \"#ff87c9\"],[1.0, \"#8634eb\"]]))\n    \n    fig = make_subplots(rows=3, cols=1, \n                        shared_xaxes=True, \n                        vertical_spacing=0.05,\n                        subplot_titles=(\"Frequency of Number of Patches\",\n                                        \"Ratio of Cancerous to Non Cancerous Images as a Function of Number of Images per Patient\",\n                                        \"Number of Patches per Patient vs Cancer Ratio of Image\")\n                       )\n\n    # Change the bar mode\n    fig.update_layout(\n        barmode='stack',\n        bargap=0\n    )\n\n    fig.update_yaxes(title_text=\"Frequency\", row=1, col=1)\n    fig.update_yaxes(title_text=\"Percentage\", row=2, col=1)\n    fig.update_yaxes(title_text=\"Percentage\", row=3, col=1)\n\n    fig.update_xaxes(title_text=\"Number of Patches\", row=3, col=1)\n    \n    fig.update_layout(height=1800,legend={'traceorder':'normal'})\n    fig.update_xaxes(rangemode=\"tozero\")\n\n    fig.append_trace(trace_0,1,1)\n    fig.append_trace(trace_1,2,1)\n    fig.append_trace(trace_2,2,1)\n    fig.append_trace(trace_3,3,1)\n\n\n    fig.show()","e50844eb":"%%javascript\nIPython.OutputArea.auto_scroll_threshold = 130;","4108f342":"plot_patch_info(binned_df,bin_values) # put image in next cell hidden in case image does not show ","165c53e8":"# plt.figure(figsize = (50,50));\n# io.imshow('..\/input\/plot-images\/patch_info.jpg');","69eac545":"# read image from path\nimg = io.imread('..\/input\/breast-histopathology-images\/10253\/0\/10253_idx5_x1001_y1001_class0.png')\n# show the image after being read\nio.imshow(img);","d72e8cbb":"print(img.shape)","1954a9a9":"img","09cd93a9":"# Accessing the first row\nfirst_row = img[0:1]\nprint('FIRST ROW:')\nprint(f'The shape of the first row: {first_row.shape}')\nprint(f'The first five pixels of the first row: \\n\\n{first_row[0:1,0:5]}\\n')\n# Accessing the first column\nfirst_column = img[:,0:1]\nprint('FIRST COLUMN:')\nprint(f'The shape of the first column: {first_column.shape}')\nprint(f'The first five pixels of the first row: \\n\\n{first_column[0:5,0:1]}\\n')\n# Accessing a specific pixel\npixel = img[19:20,16:17]\nprint('RGB VALUE 20th ROW, 17th COLUMN:')\nprint(f'The shape of the pixel: {pixel.shape}')\nprint(f'The first five pixels of the first row: \\n\\n{pixel}\\n')","83fda6fa":"fig,axs = plt.subplots(1,3,figsize = (30,10));\n# Displaying the first row\naxs[0].imshow(first_row);\n# Displaying the first column\naxs[1].imshow(first_column);\n#Displaying the pixel\naxs[2].imshow(pixel);","289b53b1":"display(img[0:1].shape)\ndisplay(img[0:1].ndim)\ndisplay(img[0].shape)\ndisplay(img[0].ndim)","5d3775a4":"display(img[0:1])\ndisplay(img[0])","89fb7bce":"io.imshow(img[0]);","2534abf9":"fig,axs = plt.subplots(1,3,figsize = (20,50));\naxs[0].imshow(img[:,0]);\naxs[1].imshow(img[:,0:1]);\n# using reshape to achieve the same result\naxs[2].imshow(img[:,0].reshape(50,1,3));","4d4352d5":"path_df = df","a7da920b":"def show_patches(target='Any',nrows = 2,ncols = 5, path_df = path_df):\n    '''\n    Function accepts type of target:\n    target = 0 --> Non-Cancerous Tissue\n    target = 1 --> Cancerous Tissue\n    path_df: dataframe containing paths\n    '''\n    if target == 'Any':\n        # if target is set to any retrieve both cancerous and non-cancerous cells\n        tissue_indices = np.random.choice(path_df.index, size=nrows*ncols, replace=False)\n    else:\n        # replace = False means that no duplication is allowed\n        tissue_indices = np.random.choice(path_df[path_df['target']==target].index, size=nrows*ncols, replace=False)\n    \n    fig,axes = plt.subplots(nrows=nrows,ncols=ncols,figsize = (ncols*6,nrows*6))\n            \n    for i,ax in enumerate(axes.flatten()):\n        idx = tissue_indices[i]\n        img = io.imread(path_df.loc[idx,'path'])\n        ax.imshow(img)","e316e35c":"show_patches(target=0,ncols=10)","440d0290":"show_patches(target=1,ncols=10)","309ee713":"def get_patient_df(p_id,df=df):\n    return df.loc[df['patient_id']== p_id,:] ","bf4a32d9":"from matplotlib.colors import LinearSegmentedColormap\n\ndef scatter_patient_xy(nrows=3,ncols=3):\n    n_imgs = nrows*ncols\n    # get random patient ids to plot\n    p_ids = np.random.choice(df['patient_id'].unique(), size=n_imgs, replace=False)\n    \n    fig, axs = plt.subplots(nrows=nrows,ncols=ncols,figsize=(30,30))\n    \n    colors = ['#ff70db','#9334eb']\n    \n    # setting the point colors for the target values\n    cmap = LinearSegmentedColormap.from_list(name='',colors = colors, N=2)\n    \n    for i,row in enumerate(axs):\n        for j,ax in enumerate(row):\n            p_id = p_ids[i*nrows+j]\n            p_df = get_patient_df(p_id)\n            \n            x_coords = p_df['x_coord'].values\n            y_coords = p_df['y_coord'].values\n            \n            # get the min coordinate of each patient image\n            min_coord = min(x_coords.max(),y_coords.max())\n            # use min_coord to to set the size of the plotted square\n            s = int(4000\/min_coord*15)\n            \n            # used to determine the color of the point\n            targets = p_df['target'].values\n            ax.scatter(x_coords,y_coords,c=targets,cmap=cmap,s=s,marker='s')\n            ax.set_title('Patient: '+p_id)","7fe7ead3":"scatter_patient_xy()","1ba25d68":"def get_tissue_image_array(patient_id,pred = False):\n    # get patient dataframe\n    path_df = get_patient_df(patient_id)\n    # get the max_coordinate to define the numpy array size, numpy array will be containing the rgb values for the image\n    max_coord = np.max((*path_df['x_coord'],*path_df['y_coord']))\n    # add 50 to the max_coord to get the final image dimension,50 here is the patch size\n    image_dimension = max_coord + 50\n    # create a 3 dimensional array with RGB values = 255, we set the type as uint since no RGB value can be > 255, thus increasing efficiency\n    grid = 255*np.ones(shape = (image_dimension,image_dimension,3)).astype('uint8')\n    mask = 255*np.ones(shape = (image_dimension,image_dimension,3)).astype('uint8')\n    for x,y,target,path in path_df[['x_coord','y_coord','target','path']].values:\n        img_array = io.imread(path)\n        # some patches have dimension less than 50 x 50 which would cause the code to break\n        try:\n            # replace values in grid array by the image array values\n            grid[y:y+50,x:x+50] = img_array \n            # if the image is cancerous add\n            if target != 0:\n                mask[y:y+50,x:x+50] = [0,0,255]\n        except: pass\n    # check if prediction is specified\n    if pred == False:\n        img = grid\n    else:\n        alpha = 0.8\n        # This is step is very important, multiplying the 2 values by a float value converts the arrays to float64, which is why convert them back to uint8\n        img  = (mask * (1.0 - alpha) + grid * alpha).astype('uint8')\n        \n    return img\n            ","68688d00":"def plot_tissue_images(nrows=3,ncols=3,pred = False,df=df):\n    n_imgs = nrows*ncols\n    p_ids = np.random.choice(df['patient_id'].unique(), size=n_imgs, replace=False)\n    \n    fig,axs = plt.subplots(nrows=nrows,ncols=ncols,figsize=(30,30))\n    for i,row in enumerate(axs):\n        for j,ax in enumerate(row):\n            p_id = p_ids[i*nrows+j]\n            img = get_tissue_image_array(p_id,pred = pred)\n            ax.set_title(f'Breast Tissue Slice for patient {p_id}')\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.imshow(img)","40e2b041":"plot_tissue_images(pred = True)","d31b2dc1":"# function that accepts number of patients and returns a dataframe containing all the images for each patient. \n# prioritizes patients with higher number of patches.\ndef get_patients(df,n_patients=10):\n    p_ids = df.groupby('patient_id')['patient_id'].count().sort_values(ascending=False).index[0:n_patients]\n    df =  df.loc[df['patient_id'].isin(p_ids)].reset_index(drop=True)\n    return df","f7dbe67c":"sample_df= get_patients(df=df)\nsample_df.info()","9eb18a11":"sample_df.head()","ad5ee468":"def get_img_arrays(df,):\n    # read each image array from corresponding path as grayscale and flatten the image array\n    df['img_array'] = df.progress_apply(lambda x : io.imread(x['path'],as_gray=True).flatten(),axis=1); # make sure to specify axis = 1\n    # get the shape of each image array and store it in the dataframe\n    df['array_shape'] = df.progress_apply(lambda x : x['img_array'].shape[0],axis=1); # make sure to specify axis = 1\n    return df","61aef271":"sample_df = get_img_arrays(df = sample_df)","9071283c":"sample_df.head()","509a8f30":"# get the count of the unique values in the column array_shape\nsample_df['array_shape'].value_counts() ","94901a0b":"# get all images that do not have an array shape of 2500\nweird_imgs = sample_df[sample_df['array_shape'] != 2500] ","533f889c":"# use the show patches function that we created earlier to visualize those images\nshow_patches(nrows = 2,ncols = 5, path_df = weird_imgs)","198e3515":"# get the shape of the dataframe before dropping the artifacts\nsample_df.shape ","961dadb9":"# drop images using indices of the filter\nsample_df.drop(weird_imgs.index,inplace=True) ","31cdc305":"# get the shape of the dataframe before dropping the artifacts\nsample_df.shape ","0e448540":"sample_df.head()","51a8be92":"# get the number of rows in the pandas dataframe in order to determine the number of rows in our numpy array\nnrows=sample_df.shape[0]\n# set the number of columns to 2500, which is the length of our array\nncols=2500\n# initialize the array using the information above\nimg_arrays= np.zeros((nrows,ncols))\nprint(img_arrays.shape)","e741cb47":"# add all the image arrays to the numpy array that we just initialized\nfor i,array in enumerate(sample_df['img_array']):\n    img_arrays[i,:] = array\ndisplay(img_arrays[0:5,:])","48cb3d0c":"from sklearn.decomposition import PCA\nimages_pca = PCA()\n# fit the function to our image arrays\nimages_pca.fit(img_arrays);","451dce7a":"# explained get the cumalitive sum of the explained variance ratio for each principle component\nevr = np.cumsum(images_pca.explained_variance_ratio_)\n# plot the explained variance ratio\nfig = go.Figure(data=go.Scatter(y = evr,line=dict(color='#ff70db')))\nfig.update_layout(title='Explained Variance Ratio After PCA',\n                   xaxis_title='Number of Principle Components',\n                   yaxis_title='Cumalitive Explained Variance Ratio')\nfig.show()","522d7295":"# let's now apply PCA but only retrieve the first 150 components or the components that account for 80 percent of the variance\nimages_pca = PCA(0.8)\n# OR\nimages_pca = PCA(150)\n# fit the function to our image arrays\nimages_pca.fit(img_arrays);","c2e50b2b":"# get the shape of the first image array\nsample_df['img_array'][0].shape","9f0bc015":"# Apply PCA transformation to each row in the img_array column\n# hint: the transform function accepts a list, so if you wanted to feed it one value you would have to place that value in a list\nsample_df['pca_array'] = sample_df['img_array'].progress_apply(lambda x: images_pca.transform([x]).flatten());","b60738bd":"sample_df['pca_array'][0].shape","c7b767ed":"sample_df.head()","c830d402":"# visualizing the first 30 principle components\nfig, axes = plt.subplots(3, 10, figsize=(30, 10))\nfor i, ax in enumerate(axes.flat):\n    # return each of the components, and reshape them to 50x50\n    ax.imshow(images_pca.components_[i].reshape(50, 50), cmap='binary_r')","c9b72471":"components = images_pca.transform(img_arrays)\nprojected = images_pca.inverse_transform(components)\n\n# Plot the results\nfig, ax = plt.subplots(2, 10, figsize=(30, 7))\nfor i in range(10):\n    ax[0, i].imshow(img_arrays[i].reshape(50, 50), cmap='binary_r')\n    ax[1, i].imshow(projected[i].reshape(50, 50), cmap='binary_r')\n    \nax[0, 0].set_ylabel('Before PCA')\nax[1, 0].set_ylabel('150-dim\\nReconstruction');","09052fd3":"# get the count of each of the target values\nsample_df['target'].value_counts() ","8ab38357":"# get the dataframe containing the negative target variables\nnegative = sample_df[sample_df['target'] == 0]\n# get the dataframe containing the positive target variables\npositive = sample_df[sample_df['target'] == 1]\n# get the shapes of each dataframe\ndisplay(negative.shape)\ndisplay(positive.shape)","688c5775":"from sklearn.utils import resample\n# downsample the negative targets\nneg_downsampled = resample(negative,n_samples=positive.shape[0], random_state=42)\n# combine minority and downsampled majority\ndownsampled = pd.concat([positive, neg_downsampled])\n# check new class counts\ndownsampled['target'].value_counts()","a7fcb713":"# show the first 5 values of the dataframe\ndownsampled.head()","30be9f38":"# let's extract our variables of interest and store them in a new dataframe\ndfd = downsampled.loc[:,['img_array','pca_array','target']]","7a53f776":"dfd.head()","c5e59f80":"# get the number of rows in the pandas dataframe in order to determine the number of rows in our numpy array\nnrows=dfd.shape[0]\n# set the number of columns to 150, which is the length of our array\nncols=150\n# initialize the array using the information above\npca_arrays= np.zeros((nrows,ncols))\nprint(pca_arrays.shape)","5b61a694":"pca_arrays = np.vstack(np.array(dfd['pca_array']))\ntargets = np.vstack(np.array(dfd['target']))","ac1ea865":"print(pca_arrays.shape)\nprint(targets.shape)","b2af51d0":"from sklearn.model_selection import train_test_split\n# split our data into training and testing data, and input data and target data\nX_train, X_test, y_train, y_test =  train_test_split(pca_arrays, targets, train_size=0.7, shuffle = True)","d72e8d18":"# compare the shape of the train and test inputs\nprint(f'X_train Shape: {X_train.shape}')\nprint(f'X_test Shape: {X_test.shape}')\nprint(f'y_train Shape: {y_train.shape}')\nprint(f'y_test Shape: {y_test.shape}')","05258bc2":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","bd1ab158":"print(torch.__version__)","7308184f":"print(type(X_train))\nprint(type(y_train))","e1667fe7":"# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nprint()\n\n#Additional Info when using cuda\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)\/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_cached(0)\/1024**3,1), 'GB')","f1f80804":"X_train,y_train,X_test,y_test = map(lambda x: torch.as_tensor(data=x,device=device,dtype=torch.float32),(X_train,y_train,X_test,y_test))","97f00a3a":"print(type(X_train), X_train.device)\nprint(type(y_train), y_train.device)","67a31ea1":"class Model(nn.Module):\n    def __init__(self,device,n_inputs,lr=0.01):\n        super(Model,self).__init__()\n        self.device = device\n        self.n_inputs = n_inputs\n        self.lr = lr\n        self.dense1 = nn.Linear(in_features=n_inputs, out_features=64).cuda(device=self.device)\n        self.dense2 = nn.Linear(in_features=64, out_features=32).cuda(device=self.device)\n        \n        self.out = nn.Linear(in_features=32, out_features=1).cuda(device=self.device)\n        \n    def __call__(t):\n        return self.forward(t)\n        \n    \n    def forward(self,t):\n        # (1) input layer\n        t = t\n        \n        # (2) hidden dense layer\n        t = self.dense1(t)\n        t = F.relu(t)\n        \n        # (3) hidden dense layer\n        t = self.dense2(t)\n        t = F.relu(t)\n        \n        # (4) output_layer\n        t = self.out(t)\n        t = torch.sigmoid(t)\n        \n        return t\n    \n    def get_num_correct(self,y_preds,y):\n        return y_preds.round().eq(y).sum().item()\n    \n    def fit(self,X_train,y_train,epochs=3,batch_size = 32,verbose=True):\n        \n        self.optimizer = optim.Adam(self.parameters(),lr=self.lr)\n        n_samples = X_train.shape[0]\n\n        \n        for epoch in range(epochs):  \n            # set the initial loss for the current epoch\n            total_loss = 0\n            total_correct = 0\n            \n            for i in range((n_samples - 1) \/\/ batch_size + 1):\n                \n                # Update indices for loading batches\n                start_i = i * batch_size\n                end_i = start_i + batch_size\n                \n                # Load the next batch\n                X = X_train[start_i:end_i]\n                y = y_train[start_i:end_i]\n                \n                # Pass in the Batch\n                y_preds = self.forward(X)\n                \n                # Calculate the Loss\n                loss = nn.BCELoss()(y_preds, y)\n\n                if verbose:\n                    total_loss += loss.item()\n                    total_correct += self.get_num_correct(y_preds,y)\n                        \n                \n                # Calculating the gradients\n                loss.backward()\n                \n                # update the weights by paramater -= parameter - grads*lr\n                self.optimizer.step()\n                \n                # reset the gradients\n                self.optimizer.zero_grad()\n\n            print(f'epoch: {epoch} , total_correct: {total_correct} , total_loss: {total_loss}')\n            print('Accuracy: ',total_correct\/n_samples)\n    \n    @torch.no_grad()\n    def predict(self,X,batch_size=1):\n        n_samples = X.shape[0]\n        y = torch.empty(n_samples,1).cuda(device=self.device)\n        \n        for i in range((n_samples - 1) \/\/ batch_size + 1):\n            # Update indices for loading batches\n            start_i = i * batch_size\n            end_i = start_i + batch_size\n            \n            # Load next batch\n            Xb = X[start_i:end_i]\n            \n            y[start_i:end_i] = self.forward(Xb)\n                \n        return y.round()\n    \n    def accuracy(self,X,y,batch_size=1):\n        y_pred = self.predict(X,batch_size=1)\n        return self.get_num_correct(y_pred,y)\/len(y_pred)","654d5238":"n_inputs = X_train.shape[1]","77cdba2c":"model = Model(device=device,n_inputs=n_inputs)","b39a547e":"model.fit(X_train=X_train,y_train=y_train,epochs=3)","41508db7":"output_path = Path(Path.cwd()\/'model_1')","dea45197":"torch.save(model.state_dict(), output_path)","f42a0378":"model.accuracy(X_test,y_test,batch_size=500)","17711667":"X_test.device","f29e7f2b":"import tensorboard","aebd921f":"tensorboard.__version__","befcd975":"torch.__version__","6e51aa86":"from torch.utils.tensorboard import SummaryWriter","97f6762c":"#### After Dropping Artifacts","dac84dc8":"Let's now visualize what some of the reconstructed tissue cells look like by using the function above. \n\nNote: The function can be improved by having edges on the mask, rather than overlaying a color on top. This will help us preserve the true image of the tissue cells.","aa4b30ec":"#### An RGB image is 3 dimensional, while a grayscale image is 2 dimensional","778d36d3":"## Reading and Displaying an Image","48bccd6e":"Let's make sure that our changes where applied by checking the info one more time","d751d5ea":"### Something to be careful of\n\nIs `io.imshow(img[0:1])` ,the same as `io.imshow(img[0])`?\n\nIs `io.imshow(img[:,0:1])` ,the same as `io.imshow(img[:,0])`?\n\nThe answer is a big fat\n# NO!","6561d959":"### Insights\n1. Some patiets have as little as 63 patches\n1. The average cancer ratio for each patient is about 70% non-cancerous and 30% cancerous\n1. Our Cancer ratio varies just as much as our number of patches, is there any relation?","23b4a002":"#### Increase the maximum number of rows that will be displayed in the output before a scroll bar is used\n","e433131f":"#### Shape of Array after applying PCA","e624f623":"### Eigen Patches\nIf you've hearded eigen faces, this right here is the breast cancer version of that","519a816a":"### Drilling deeper into what an image is\nSo far we:\n* Read the image from a path\n* We then stored that image in a variable called img\n* We discovered that img is a python numpy array\n\nNow let's have a look at the raw image array","14ff49a6":"### Extracting Metadata into DataFrame","006ef874":"Now that we have our eigen vectors, let's compare how our images looked like before PCA and after PCA. What we are about to do is multiply each of one of our components by its corresponding eigen vector and sum it, think of it like a weighted sum.","2f76ce90":"Finally we will transfer our values from dictionaries into a pandas dataframe which will make our analysis much easier moving forward.","a9a9054c":"### Comparing Images Before & After PCA","ebce0432":"## Applying PCA on to the Images\nIn the above step we managed to read our image arrays into our numpy arrays, we also managed to reduce the total length of our image arrays from 7500, to 2500, by converting our images into grayscale. Now we would like to further reduce the number of dimensions by applying Principle Component Analysis onto our image arrays. ","b5e280fb":"### Analytics\n* How many patches do we have on average per patient?\n\n* On average how many of those patches are cancerous vs non-cancerous?\n\nLet's try and answer those questions by querying our dataframe.","d4a32f71":"# Training Model Using PyTorch","ca6a44eb":"### Plotting the First Row, Column, and specific Pixel","038b3b6b":"### Setting model parameters","7f381fbb":"#### Initial shape of our image array","14ed2b6f":"As expected most of our image arrays have a length of 2500, this is because our arrays where originally, 50 x 50 x 3 so when we convert them to grayscale we get an array of 50 x 50 x 1 and when we flatten it we get 2500 total values.\n\nNotice how we have some image arrays that don't have a length of 2500. Let's have a look at what those images look like.","bf6645b8":"#### Before Dropping Artifacts","76f7e73b":"### Insights\n1. The number of image patches per patient varies a lot! It seems that not all images have the same resolution, as the resolution of the image and the number of patches is directly proportional.\n2. Some patients appear to have over 70% of the image covered in cancer. Patients with a lower number of images tend to suffer from that the most, indicating that we are only receiving a fraction of the full picture.\n3. The classes of Cancerous vs Non-Cancerous patches are imbalanced. We may consider rebalancing the class at a later stage.","52f20f9f":"### Insights\u00b6\n* Sometimes we can find artifacts or incomplete patches, some images are also less than 50 x 50 pxs. \n* Patches with cancer look more violet and crowded than healthy ones. Is this really typical for cancer or is it more typical for ductal cells and tissue?\n* Though some of the healthy patches are very violet colored too!\n* Would be very interesting to hear what criteria are important for a [pathologist](https:\/\/en.wikipedia.org\/wiki\/Pathology).\n","2ffa925e":"#### Notice how the image arrays now contain values between 0 and 1. This is because we read the image as grayscale image. Normally in Machine Learning if a we scale our data between 0 and 1, but since this step is already done for us we will be skipping it.","b6a5b1ae":"Notice how our x & y coords and our targets are formatted as strings instead of ints","2d938d63":"### Binary target visualisation per tissue slice \nBefore we will take a look at the whole tissue let's keep it a bit simpler by looking at the target structure in the x-y-space for a handful of patients:","1b42d257":"#### Explaining the Variance Ratio","15d23ba8":"The files containing the tissue images of patient use the naming pattern below:\n\nPatient_id\/xcoordinate_of_patch\/ycoordinate_of_patch\/class_of_cancer","98fd51b0":"### Accessing rows & columns of an image\nNow that we know that the image is basically a numpy array, we can access the rows and columns using numpy array slicing.\n\nIn other words,accessing the rows and columns in an image is as simple as accessing them in a numpy array.","97e1d91a":"Our images seem to be of 50 Pixels wide and 50 Pixels tall. The 3 represents the RGB space.","9f3a6aa7":"### Splitting our Data\n\n* ***Training Data*** : Data that we will use to train our model\n* ***Testing Data*** : Data that we will use to test our model\n* ***Out of Sample Data*** : Data that we will use to further validate our testing. Usually this is taken before preprocessing the data.\n\nWe will be using sklearn's train_test_split to split our data. \n\nShuffle is set to False, this is because we already manually shuffled our data in the previous cell. \n\nFor simplicity's sake we will not be considering the Out of Sample data, and we will only be splitting our data to training and testing sets.","5128e749":"BIAS IN THE DATASET: for example if a patient has a cancerous spot but the spot is not there and therofore classified as non-cancerous.","2b4f3175":"In our case we will be undersampling our Non-Cancerous Data, in order to make our target distribution more balanced. Ideally we would artificially oversample our data by rotating our cancerous images.","19328c26":"#### Initialize Image Array","67674802":"#### In case plot doesn't show above, try refreshing or unhide and run the lines of code below..","430c1d2c":"### Let's take a closer look","a9159819":"The function below extracts all the paths containing images by examining the file extension. Note that this will only work if your image is a .png file; which is the case with our images.","5b9d340d":"Let's double check that our function is extracting everything correctly","8e849f15":"In order to randomly resample our data we will be using the sklearns.utils function resample. For a more in depth explanation about the parameters of resample, check out the documentation.","90298489":"### Insights\n* Sometimes we don't have the full tissue information. It seems that tissue patches have been discarded or lost during preparation.\n* Cancerous Tissue tends to appear in clusters rather than, being dispersed all over the place.\n","e81a710b":"## Storing the Images\n\nNow that we know that our images are a bunch of arrays, how do we actually feed them to a machine learning model?","ec76e4d3":"### Repatching the Actual Breast Tissue Image\nNow it's time to go one step deeper with our EDA. Instead of plotting the target values using the x-y coordinates, we now plot the images themselves on their respective x-y coordinates. This will help us visualize how the cancerous tissue looks like from a macro perspective.","9273475f":"### Undersampling Our Data","1f4c6514":"Now that we have set up our data, let's create a visual summary to help us draw some insights from our data.","edef486a":"#### Applying PCA on Image Arrays\nThe PCA function accepts a percentage between 0 and 1, for example n_components = 0.8, would mean return for me the eigen vectors that account for 80% percent of the variation from the in my image arrays.\n\nThe PCA function also accepts any number of absolute components that you specify, for example: we initially have 2500 components, if we want the first 150 components then we set n_components = 150.","966e56fd":"## Visualizing the Breast Tissue\nEarlier we extracted the coordinates of the cropped tissue cells, we can use those coordinates to reconstruct the whole breast tissue of the patient. This way we can explore how the diseased tissue looks when compared to the healthy tissue. \n\nWe can also explore the most common places that the cancer tends to occur in. It would be interesting to plot a heatmap of the  most common areas where the cancer appears. \n\nIf position of the crop has significance then perhaps we can use it as an input feature for our model.","6f41c83b":"From the graph above we can determine that the first 150 components account for over 80% percent of the variance in our data. We managed to reduce the number of dimensions of our image arrays from 2500 to 150, while still being able to maintain over 80% of the information, this a 96% reduction in the number of dimensions that we have!","1833fe01":"## Healthy Tissue Patches Vs Cancerous Tissue Patches\nLet us now explore the visual differences between cancerous tissue cells, and healthy tissue cells. Usually partnering with a specialist is a good idea so that they can point the exact points of interest that differentiate the 2 from each other.","e7252d59":"### What is an image in python?\n\nWhen we do io.imread(path) we are converting this image into it's numerical form. An image in python is represented by a numpy array. (n x m x 3) in the case of an RGB Image. or (n x m x 4) in the case of an RGBA image or CMYK image. Finally it is also possible to have a shape as follows (n x m x 1) which indicates that we are dealing with a grayscale image.\n\nWe will be dealing with the first case.","d1bf84d0":"#### Splitting the Data","038a4894":"We will be using skimage as a library to process the images.","61b846ce":"## Storing the image_path, patient_id, target and x & y coordinates\nWe have about 278,000 images. To feed the algorithm with image patches we will store the path of each image and create a dataframe containing all the paths. **This way we can load batches of images one by one without storing the individual pixel values of all images**. \n\nThe file structure which seems to be as follows:\n\nbreast-histopathology-images:<br>\n    -->patient_id:<br>\n           -->-->cancer_class:<br>\n                     -->-->-->image_files<br>\n                     \nNow we need to find a way to loop through all of our directories to extract the paths and labels. Some of the information that we will be interested in extracting includes:\n* patient_id\n* x_coordinate\n* y_coordinate\n* target\n* path_to_image","d6507aa6":"Now that we have extracted the paths to our images, let's check out how many images we have. The number of images is simply equal to the number of paths that we have.","884e2a23":"Let's start by extracting our image arrays from our dataframe and storing them in a numpy array.","34ebf0a6":"The reason the above happens is because, when you look at the shape of your image you will have 50 rows , and 3 columns, and no value for the color part of the image, which will cause imshow to take it as grayscale by default.","9f84d796":"Great, we managed to extract the patients containing the most images, giving us a total of 22259 images that we will be using to train.","9b6032a8":"Let's take a peak at the paths that we have and verify that everything makes sense.","03da3286":"Let's drop these images as they add noise unnecessary noise to our model.","5a0f60a6":"### What do you think will happen if I try to display an image?\n### Let's take a look...","ea304bd6":"Recall that, each patient directory has 2 sub-folders, labeled 0 and 1 <br\/>\nFolder 0: Non-Invasive Ductal Carcinoma (IDC) <br\/>\nFolder 1 : Invasive Ductal Carcinoma (IDC) <br\/>","0baab8fd":"#### Extracting Data into numpy arrays","5ee18073":"## Taking a sample of our data\nBelow we are going to take a smaller sample of our data. Ideally we would like to use our entire dataset, but for the sake of demonstration purposes and saving time we will be using a smaller sample.\n\nWe will be taking the top 10 patients in terms of number of patches for our sample.","0fa1883c":"### Non-Cancerous Patches","78df65b3":"# Preparation & peek at the data structure\n\n\n## Loading packages","74f43aa0":"## Training & Testing Split\n\nThere are a few things that we must consider before splitting our data. The first being the ratio of cancerous to non cancerous images that we have. We determined earlier that our data is not split uniformly. We have many more, non-cancerous images that non-cancerous images, which may possibly bias our model. \n\nThis may also pose a problem when evaluating our model. Let's take an extreme example where we have a dataset consisting of 100 points, 90 of them are cancerous and 10 of them are non-cancerous, let's also assume that our model classified our entire dataset as cancerous, this would give an accuracy of 90%, however this is not really representitive of what is really happening. In reality it is always misclassifying our non-cancerous data.\n\nSo how do we go about solving this?\n\nWe can use the train-test split function offered in sklearn model_selection, this function has the option to split our data such that we get equally distributed training and testing sets when grouped by the classes.\n\nFor more information, check out this [article](https:\/\/towardsdatascience.com\/what-to-do-when-your-classification-dataset-is-imbalanced-6af031b12a36), that talks about the considerations that need to be taken before splitting your data.","f6b792a9":"uint8 is used unsigned 8 bit integer. And that is the range of pixel. We can't have pixel value more than 2^8 -1. Therefore, for images uint8 type is used. Whereas double is used to handle very big numbers.","2a4a2753":"To simplify things we will create a function that slices our existing dataframe and retrieves the values associated with a patient id.","babf2565":"So how do we extract the data that we want, now that we have all the paths?<br>\n\nThe answer is using **regex**.\n\nI know that regex may be daunting at first, but if this is your first time using regex I highly suggest you check out [regexr.com](https:\/\/regexr.com\/)","5051abf2":"### Convert Numpy arrays to torch tensors","a9b95938":"# Exploratory Data Analysis (EDA)\n\n## Let's create a visual summary of our data","09e1bb31":"# Training - Testing Split\n\nIn order to verify our model's performance we need to split our data to testing and training data.\nThe testing data will verify that our model is able to classify cases that it hasn't seen before. \n\nNormally, this step would be done before performing PCA, as the testing data would influence our PCA fit. But I will split it here for simplicities sake.","73c506f9":"####  Let's see if all of our images have the same shape:","8bc01e3f":"### Cancerous Patches"}}