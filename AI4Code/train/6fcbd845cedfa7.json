{"cell_type":{"b2ed1332":"code","77e35486":"code","3360d737":"code","b29ab105":"code","7ea43c05":"code","abecc4e2":"code","a246a427":"code","b1b46918":"code","49e30b7d":"code","d6b3a649":"code","591cc677":"code","d67ab618":"code","390b4d50":"code","f1d82035":"code","1c925481":"code","08a99c48":"code","0dd94625":"code","5195f504":"code","de50e1ca":"code","c0f774fb":"code","b44b0c19":"code","46a00709":"code","68cd6455":"code","fbbc98c9":"code","c8b93c5b":"code","3a56c393":"code","3df9ddd4":"code","6f2f5d8c":"code","85b077e9":"code","4da88a68":"code","c19dfefb":"code","e0939a06":"code","63988154":"code","6a80125a":"code","dddcd658":"code","1e37fc64":"code","ec63da3a":"code","79b04b7d":"code","2d50ac50":"code","57eec126":"markdown","192a4da6":"markdown","60feb1d4":"markdown","1d736755":"markdown","72213de8":"markdown","6acf5ebb":"markdown","153eeaa5":"markdown"},"source":{"b2ed1332":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","77e35486":"import sklearn\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","3360d737":"# mnist_train_small dataset from google colab sample data\ntrain=pd.read_csv(\"\/kaggle\/input\/mnist-ml-crash-course\/mnist_train_small.csv\")\ntrain","b29ab105":"y=train.iloc[0:, 0:1]\ntrain['target']=y\ntrain=train.drop(y, axis=1)\ntrain","7ea43c05":"# Taking only 2 classes and discarding rest of the data\nnew_train=train[train['target'].isin([8,9])]\nnew_train","abecc4e2":"X=new_train.drop('target', axis=1) \nX","a246a427":"y=new_train['target']\ny","b1b46918":"from sklearn.model_selection import train_test_split\ntrain_X, test_X, train_y, test_y=train_test_split(X, y, test_size=0.5, random_state=2)","49e30b7d":"#Normalize data before feeding it to SVMs\nfrom sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\ntrain_X= scaler.fit_transform(train_X)\ntest_X = scaler.transform(test_X)","d6b3a649":"from sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score\ntest_acc=[]\ntrain_acc=[]\nC_arr=[]","591cc677":"#Train model with different values of C\nfor i in range(1,101):\n    model= LinearSVC(C=i)\n    model.fit(train_X, train_y)\n    testpred=model.predict(test_X)\n    trainpred=model.predict(train_X)\n    testacc=accuracy_score(test_y, testpred)\n    trainacc=accuracy_score(train_y, trainpred)\n    test_acc.append(testacc)\n    train_acc.append(trainacc)\n    C_arr.append(i)","d67ab618":"print(test_acc)\nprint(train_acc)\nprint(C_arr)","390b4d50":"plt.plot(C_arr,train_acc)","f1d82035":"plt.plot(C_arr,test_acc)","1c925481":"from sklearn.svm import SVC\nacc_train=[]\nacc_test=[]\nd=[2,3,4,5]","08a99c48":"for i in d:\n    svclassifier = SVC(kernel='poly', degree=i, C=2)\n    svclassifier.fit(train_X, train_y)\n    trainpreds=svclassifier.predict(train_X)\n    testpreds=svclassifier.predict(test_X)\n    acctrain= accuracy_score(train_y, trainpreds)\n    acctest=accuracy_score(test_y, testpreds)\n    acc_train.append(acctrain)\n    acc_test.append(acctest)","0dd94625":"print(acc_train)\nprint(acc_test)","5195f504":"plt.plot(d,acc_test)","de50e1ca":"plt.plot(d,acc_train)","c0f774fb":"from sklearn.ensemble import RandomForestClassifier\ntrain_errors = list()\ntest_errors = list()","b44b0c19":"n_estimators=[100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]","46a00709":"for i in range(len(n_estimators)):\n    rfclassifier= RandomForestClassifier(n_estimators=n_estimators[i])\n    rfclassifier.fit(train_X, train_y)\n    train_errors.append(rfclassifier.score(train_X, train_y))\n    test_errors.append(rfclassifier.score(test_X, test_y))","68cd6455":"print(train_errors)\nprint(test_errors)","fbbc98c9":"plt.semilogx(n_estimators, train_errors, label='Train')","c8b93c5b":"plt.semilogx(n_estimators, test_errors, label='Test')","3a56c393":"new_X = train.drop('target', axis=1)\nnew_y = train['target']\nnew_X","3df9ddd4":"train_X2, test_X2, train_y2, test_y2=train_test_split(new_X, new_y, test_size=0.5, random_state=2)","6f2f5d8c":"scaler2=StandardScaler()\ntrain_X2= scaler2.fit_transform(train_X2)\ntest_X2 = scaler2.transform(test_X2)","85b077e9":"from sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.combine import SMOTEENN\n\n#To reduce class bias in 1-vs-rest, keeping the ratio of positive to negative data points as 1:3 (down sample the negative examples)\npipe = Pipeline([('sampl', SMOTEENN(sampling_strategy=0.33)), \n                 ('clf', LinearSVC(max_iter= 1000))])\novr = OneVsRestClassifier(pipe)\novr.fit(train_X2, train_y2)","4da88a68":"trainacc=ovr.score(train_X2, train_y2)\ntestacc=ovr.score(test_X2, test_y2)\nprint(\"train_accuracy : \", trainacc)\nprint(\"test_accuracy : \", testacc)","c19dfefb":"y_values=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\npairs=[]\npredictions=[]\ntrain_errors3=[]\ntest_errors3=[]\nfor i in range(len(y_values)):\n    for j in range(i+1,len(y_values)):\n        new_train=train[train['target'].isin([i, i+1])]\n        New_X=new_train.drop('target', axis=1)\n        New_y=new_train['target']\n        Train_X2, Test_X2, Train_y2, Test_y2 = train_test_split(New_X, New_y, test_size=0.5, random_state=2)\n        Train_X2= scaler.fit_transform(Train_X2)\n        Test_X2 = scaler.transform(Test_X2)\n        classifier=LinearSVC()\n        classifier.fit(Train_X2, Train_y2)\n        prediction= classifier.predict(Test_X2)\n        predictions.append(prediction)\n        train_errors3.append(classifier.score(Train_X2, Train_y2))\n        test_errors3.append(classifier.score(Test_X2, Test_y2))\n        pairs.append([y_values[i], y_values[j]])","e0939a06":"C=[]\nC_=[]\nfor i in range(len(pairs)):\n    C.append(pairs[i][0])\n    C_.append(pairs[i][1])","63988154":"df=pd.DataFrame()\ndf['C']=C\ndf[['C_']]=C_\ndf['train_errors']=train_errors3\ndf['test_errors']=test_errors3","6a80125a":"# Using VotingClassifier for MajorityVoting assignment\nfrom sklearn.ensemble import VotingClassifier","dddcd658":"train_errors4=[]\ntest_errors4=[]\nfpredictions=[]\nfor i in range(len(y_values)):\n    for j in range(i+1,len(y_values)):\n        new_train=train[train['target'].isin([i, i+1])]\n        New_X=new_train.drop('target', axis=1)\n        New_y=new_train['target']\n        Train_X2, Test_X2, Train_y2, Test_y2 = train_test_split(New_X, New_y, test_size=0.5, random_state=2)\n        Train_X2= scaler.fit_transform(Train_X2)\n        Test_X2 = scaler.transform(Test_X2)\n        classifier=('lr', LinearSVC())\n        eclf1 = VotingClassifier(estimators=[classifier], voting='hard')\n        eclf1.fit(Train_X2, Train_y2)\n        prediction= eclf1.predict(Test_X2)\n        fpredictions.append(prediction)\n        train_errors4.append(eclf1.score(Train_X2, Train_y2))\n        test_errors4.append(eclf1.score(Test_X2, Test_y2))\n    ","1e37fc64":"print(train_errors4)\nprint(test_errors4)","ec63da3a":"train_accpwc=sum(train_errors4)\/len(train_errors4)\ntest_accpwc=sum(test_errors4)\/len(test_errors4)","79b04b7d":"print(train_accpwc)\nprint(test_accpwc)","2d50ac50":"# Comparing accuracies of OnevsRest and Pair-wise Classifier\nprint('Pair-wise classifier is more accurate by : ',test_accpwc-testacc) \n","57eec126":"Polynomial kernel SVM classifier","192a4da6":"**Pair-wise Classifier**","60feb1d4":"**Scoring Pair-wise Classifier using Voting**","1d736755":"**OneVsRest Classifier**","72213de8":"LinearSVC Classifier","6acf5ebb":"**Random Forest Classifier**","153eeaa5":"**1.\tSVM classifier**"}}