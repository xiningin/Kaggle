{"cell_type":{"a25d6f61":"code","cf60a621":"code","fe97d704":"code","67b792a8":"code","cb93ed02":"code","17aa197f":"code","459f6a72":"code","d563218e":"code","8983d476":"code","c8ea3ef6":"markdown","e68eaff5":"markdown","3560630b":"markdown","9e899605":"markdown","3921502a":"markdown","11162816":"markdown","5795583b":"markdown","eb6ff30d":"markdown","0e8db6fe":"markdown","6583987e":"markdown","7f0c916f":"markdown","d14ca6e2":"markdown","6f8d4e75":"markdown"},"source":{"a25d6f61":"import codecs\nimport numpy as np\nimport tensorflow as tf\nprint(tf.__version__)","cf60a621":"data_fpath = tf.keras.utils.get_file(\n    'shakespeare.txt', \n    'https:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/shakespeare.txt')\n\ntext = codecs.open(data_fpath, 'r', encoding='utf8').read()\nprint(text[:250])","fe97d704":"vocab = sorted(set(text))\nVOCAB_SIZE = len(vocab)\n\nprint('Vocab: {}'.format(vocab))\nprint('{} unique characters'.format(VOCAB_SIZE))\n\nchar2idx = {u:i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)\n\ntext_as_int = np.array([char2idx[c] for c in text])\n\nprint('Example of the encoded text: {}'.format(text_as_int[:13]))","67b792a8":"SEQ_LEN = 1000\nBATCH_SIZE = 64\n\ninput_seqs = []\ntarget_seqs = []\n\nnum_seqs = len(text_as_int) \/\/ (SEQ_LEN+1)\nfor i in range(num_seqs):\n    seq = text_as_int[i:i+SEQ_LEN+1]\n    input_seqs.append(np.array(seq[:-1]))\n    target_seqs.append(np.array(seq[1:]))\n\ninput_seqs = np.array(input_seqs)\ntarget_seqs = np.array(target_seqs)\n\ninput_seqs = input_seqs[:(len(input_seqs)\/\/BATCH_SIZE)*BATCH_SIZE]\ntarget_seqs = target_seqs[:(len(input_seqs)\/\/BATCH_SIZE)*BATCH_SIZE]\n\nprint('Input: {} ...'.format([idx2char[i] for i in input_seqs[0][:15]]))\nprint('Target: {} ...'.format([idx2char[i] for i in target_seqs[0][:15]]))","cb93ed02":"def build_model(batch_size):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(VOCAB_SIZE, 256, batch_input_shape=(batch_size, None)),\n        tf.keras.layers.GRU(256, return_sequences=True, stateful=True),\n        tf.keras.layers.Dense(VOCAB_SIZE),\n    ])\n    model.build()\n    return model\n\nmodel = build_model(BATCH_SIZE)","17aa197f":"EPOCHS = 100\n\nloss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel.compile(optimizer='adam', loss=loss)\n\nhistory = model.fit(input_seqs, \n    target_seqs,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE)","459f6a72":"model_inf = build_model(1)\n\nfor i in range(len(model_inf.layers)):\n    for j in range(len(model_inf.layers[i].weights)):\n        model_inf.layers[i].weights[j].assign(model.layers[i].weights[j])","d563218e":"def generate_text(model, seed, out_len):\n\n    text_generated = []\n\n    model.reset_states()\n    \n    inp = np.array([char2idx[s] for s in seed])\n\n    for i in range(out_len):\n\n        pred = model(inp[None, ...])[0]\n\n        temperature = 1.0\n        pred = pred \/ temperature\n        pred_c = tf.random.categorical(pred, num_samples=1)[-1][0].numpy()\n        \n        text_generated.append(idx2char[pred_c])\n        \n        inp = np.array([pred_c])\n\n    return (seed + ''.join(text_generated))","8983d476":"print(generate_text(model_inf, seed=u\"MONTAGUE:\", out_len=500))","c8ea3ef6":"### Preparing the dataset\n\nDuring inference, our generator will work as follows: first we feed in an input character or an input sequence (grain), get the first output character, and then feed it as an input character, and so on (generate one character at a time).\n\nAnd during training, we will train the model to work with a whole sequence at once. For example, the trained model should output `[i, r, s, t, ...]` by input `[F, i, r, s, ...]`. That is, the same sequence, but shifted by 1 element. In this case, it will be equivalent to Many-to-Many Sync (only during training).\n\nThus, we fix the length of the working chain `SEQ_LEN` (on which we will train), cut the entire text into chains of length` SEQ_LEN + 1` (discard the remainder), and from each such chain of length `SEQ_LEN + 1` we get a pair of chains of length` SEQ_LEN `, shifted by 1 element: the input rung (without the last element) and the target (true) rung (starting at the second element).\n\nIn addition, we fix the size of the batch for training and discard the chains at the end that cannot fill the full batch (so that the number of training chains is a multiple of the size of the batch).\n","e68eaff5":"To improve the result, you need to deepen the model, expand the data and learn more.","3560630b":"When you train the model, the result will be something like this -\n<br><br> MONTAGUE: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\nSecond Citizen:\nWould you proceed especially against Caius Marcius?\n\nAll:\nAgainst him first: he's a very dog to the commonalty.\n\nSecond Citizen:\nConsider you what services he has done for his country?\n","9e899605":"# Generating text\n\nIn this lesson, we will get acquainted with text generation in practice, namely, we will implement the language model (that is, we will train the model to generate text in a given style). Let's take Char-RNN as a model, that is, we will work at the level of individual characters (letters).\n\nLet us take the text of Shakespeare's work as a training sample (corpus).\nAs a result, our trained language model should generate text in the same style.","3921502a":"### Start the text generator\n\nWe start generating the text by passing the desired beginning of the `seed` chain to the input.","11162816":"### Model building function\n\nHere we will create our model with `tf.keras.Sequential`.\n\nThe model will consist of three layers:\n* Embedding layer for displaying letter indices in their vector representation\n* Recurrent GRU layer\n* Fully connected layer that predicts the distribution by different letters (for example, you can then hang Argmax to understand which letter the layer predicts)\n\nAt the output, we need to get a sequence of the same length as the input, that is, so that the GRU layer returns the entire chain of hidden vectors, and not just the last one. To do this, set `return_sequences = True` in the parameters of the GRU layer.\n\nDuring training, we will ask the model to predict the input chain shifted by 1 element. A during inference - we will gradually feed character by character (that is, with each inference, the model will make a one-to-one mapping). Theoretically, it would be possible to do the same during training - to gradually generate an output (character by character), feeding the result of the previous iteration (the previous character) to the input in the current iteration. But the network will learn better if we \"force\" it to use the \"correct\" inputs, rather than what it generated itself.\n\nSo that during inference at various launches of the model it remembers its previous state (otherwise it will not be possible to use recurrence, because we will feed sequentially sequences from one element), we will indicate the flag `stateful = True`\n\nIn addition, if the model has the flag `stateful = True`, it needs to know the size of the batch in advance (set via` batch_input_shape`). And since our batch size will be different (for training> 1, for inference = 1), we need to create the model twice. In order not to duplicate the code for creating a model, we wrap its creation in the `build_model` function, which takes the size of the batch as an argument.","5795583b":"### Converting characters to indices\n\nAs before (as we did in the case of text classification), it will be convenient for us to work with a certain dictionary and word indices in this dictionary. Only now, instead of words, we will use symbols (letters, etc.).\n\nTo get the vocab, it is enough to apply the set operator to our text (that is, convert the sequence of all characters to a set = remove all repetitions). `VOCAB_SIZE` - the number of elements in the dictionary.\n\nThen, as before, we create maps of the character to the index and vice versa (`char2idx`,` idx2char`)\n\nNow we convert our text to a sequence of indices using `char2idx`","eb6ff30d":"### Loading dataset\n\nDownload the text file (`shakespeare.txt`) and load its contents into the` text` variable.\n\nLet's see what the text looks like by printing a fragment of it. It can be seen that the text is characterized by some stylistics of the play.","0e8db6fe":"### Train the model\n\nAt the output of the model, we have a fully connected layer without an activation function, that is, these are just logits. According to them, we essentially need to make a classification (class number = predicted symbol number). For logits and target (true) values represented by indices, use the loss `SparseCategoricalCrossentropy (from_logits = True)`\n\nNext, we train the model in the standard way on the sets `(input_seqs, target_seqs)`","6583987e":"### Text Generation Function\n\nLet's create a function that generates text for a given model (`model`), an input string (seed` seed`) and the desired number of generated characters (`out_len`).\n\nInside the function, we first call `model.reset_states ()` to reset the previous state history.\n\nThen we convert the input text `seed` (characters to indexes) and run it through our model. We are interested in the result that matches only the last output (character).\n\nHow to get the symbol itself from the model prediction? One of the simplest ways is to just take argmax from the predicted logits (take the most likely symbol). However, in this case, the output will often be too predictable and sometimes the chain will get stuck in a loop (repeating).\n\nTo make the prediction less predictable, let's use a real distribution and \"sample\" from it (that is, choose a symbol at random according to its probability). This can be done using the `tf.random.categorical` function. At the entrance to the categorical we supply the entire pred matrix (in which the first dimension is the chain length, and the second is the distribution by classes), and at the output we get a list of samples, one for each element of the sequence). And since we are only interested in the last character, we only need to select it using the index `[-1]`.\n\nIn addition, a parameter (so-called 'temperature') can be entered, which can be used to control the output distribution and thus influence the unpredictability of the result. The higher the temperature, the more random the predicted symbol will be. For example, if we divide the logits by a large number (temperature), then if we applied softmax (to obtain probabilities), then the distribution would tend to be uniform (different classes have equal chances).\n\nEverything we have seen above corresponds to the first iteration of the `for` loop. Then the process is repeated, but now at the input each time there is a chain of one character (generated at the previous iteration). And we do so until we collect the output chain of length `out_len`.","7f0c916f":"### Creating a model for inference\n\nAfter training the model, we need to create the same model, but with batch size = 1, which we will later use for inference (`model_inf`). Let's use the `build_model` function for this, and then copy the trained weights from` model` into the new model `model_inf`.","d14ca6e2":"### Loading Libraries\nTensorFlow must be at least version 2.0","6f8d4e75":"> training..."}}