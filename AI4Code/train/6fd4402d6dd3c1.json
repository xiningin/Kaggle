{"cell_type":{"eff0fa3d":"code","bf3f8aa7":"code","933a70b9":"code","4988cbbc":"code","dde4746b":"code","0b6246e4":"code","e31c8ef9":"code","570f05d9":"code","4bac90b9":"code","adc08e15":"code","884bbde0":"code","1a672775":"code","d72d4199":"code","26f618e0":"code","bdbb1fab":"code","90ec06a6":"code","ca9014a0":"code","0ff5352f":"code","aefebe21":"code","4166197b":"code","1ddb40af":"code","757529fe":"code","ecba8639":"code","8c17a1be":"code","247c9859":"code","9eac3688":"code","62fca432":"code","4fca9ca6":"code","bcf9b884":"code","9801a8ef":"code","a53e3cfd":"code","d9543285":"code","0e1c0e8e":"code","a2df45f5":"code","076fd679":"code","2cf182e7":"code","5bc2e0f9":"code","ba27b629":"code","f97986a3":"code","c3c92285":"code","e4a4756e":"code","5cd3074e":"code","4385c069":"code","95082f91":"code","fc137dde":"code","4546933b":"code","03d1cef9":"code","7d137bf4":"code","32372161":"code","bed225c2":"code","a1cf7728":"code","c2b0e0bd":"code","85264267":"code","77f2d8d3":"code","e787c7ee":"code","df53aef7":"code","cb2f5ed9":"code","57200f10":"code","e14e03ee":"code","1f231d62":"markdown","67be8a0d":"markdown","20540daa":"markdown","75976ba5":"markdown","66901dc7":"markdown","38fefaa3":"markdown","1fea23a3":"markdown","64941cf3":"markdown","7c8aa239":"markdown","1a404b5c":"markdown","127b3eb1":"markdown","7a3da884":"markdown","843ff3af":"markdown","336820f8":"markdown","6e03e576":"markdown","49acaf3e":"markdown","2fea7722":"markdown","59505610":"markdown","12b06a46":"markdown","efc079b2":"markdown","b58cd401":"markdown","89d39f3e":"markdown","ee4f8da7":"markdown","e79cd3d0":"markdown","80fec3ff":"markdown","c78b3a3c":"markdown","d09c8ed7":"markdown","5c0233d9":"markdown","e92ba8de":"markdown","888345c5":"markdown","54fbde42":"markdown","6a59b89a":"markdown"},"source":{"eff0fa3d":"!pip install pygam\n!pip install dmba","bf3f8aa7":"from pathlib import Path\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.linear_model import LinearRegression\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.outliers_influence import OLSInfluence\n\nfrom pygam import LinearGAM, s, l\nfrom pygam.datasets import wage\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')\n\nfrom dmba import stepwise_selection\nfrom dmba import AIC_score","933a70b9":"%matplotlib inline","4988cbbc":"try:\n    import common\n    DATA = common.dataDirectory()\nexcept ImportError:\n    DATA = Path().resolve() \/ 'data'","dde4746b":"LUNG_CSV = DATA \/ 'LungDisease.csv'\nHOUSE_CSV = DATA \/ 'house_sales.csv'","0b6246e4":"lung = pd.read_csv('..\/input\/loan-csv\/data\/LungDisease.csv')\n\nlung.plot.scatter(x='Exposure', y='PEFR')\n\nplt.tight_layout()\nplt.show()","e31c8ef9":"predictors = ['Exposure']\noutcome = 'PEFR'\n\nmodel = LinearRegression()\nmodel.fit(lung[predictors], lung[outcome])\n\nprint(f'Intercept: {model.intercept_:.3f}')\nprint(f'Coefficient Exposure: {model.coef_[0]:.3f}')","570f05d9":"model.predict([[0], [23]])","4bac90b9":"fig, ax = plt.subplots(figsize=(4, 4))\nax.set_xlim(0, 23)\nax.set_ylim(295, 450)\nax.set_xlabel('Exposure')\nax.set_ylabel('PEFR')\nax.plot((0, 23), model.predict([[0], [23]]))\nax.text(0.4, model.intercept_, r'$b_0$', size='larger')\n\nx = [[7.5], [17.5]]\ny = model.predict(x)\nax.plot((7.5, 7.5, 17.5), (y[0], y[1], y[1]), '--')\nax.text(5, np.mean(y), r'$\\Delta Y$', size='larger')\nax.text(12, y[1] - 10, r'$\\Delta X$', size='larger')\nax.text(12, 390, r'$b_1 = \\frac{\\Delta Y}{\\Delta X}$', size='larger')\n\nplt.tight_layout()\nplt.show()","adc08e15":"fitted = model.predict(lung[predictors])\nresiduals = lung[outcome] - fitted","884bbde0":"ax = lung.plot.scatter(x='Exposure', y='PEFR', figsize=(4, 4))\nax.plot(lung.Exposure, fitted)\nfor x, yactual, yfitted in zip(lung.Exposure, lung.PEFR, fitted): \n    ax.plot((x, x), (yactual, yfitted), '--', color='red')\n\nplt.tight_layout()\nplt.show()","1a672775":"lung.Exposure.value_counts()","d72d4199":"subset = ['AdjSalePrice', 'SqFtTotLiving', 'SqFtLot', 'Bathrooms', \n          'Bedrooms', 'BldgGrade']\n\nhouse = pd.read_csv('..\/input\/loan-csv\/data\/house_sales.csv', sep='\\t')\nprint(house[subset].head())","26f618e0":"predictors = ['SqFtTotLiving', 'SqFtLot', 'Bathrooms', \n              'Bedrooms', 'BldgGrade']\noutcome = 'AdjSalePrice'\n\nhouse_lm = LinearRegression()\nhouse_lm.fit(house[predictors], house[outcome])\n\nprint(f'Intercept: {house_lm.intercept_:.3f}')\nprint('Coefficients:')\nfor name, coef in zip(predictors, house_lm.coef_):\n    print(f' {name}: {coef}')","bdbb1fab":"fitted = house_lm.predict(house[predictors])\nRMSE = np.sqrt(mean_squared_error(house[outcome], fitted))\nr2 = r2_score(house[outcome], fitted)\nprint(f'RMSE: {RMSE:.0f}')\nprint(f'r2: {r2:.4f}')","90ec06a6":"model = sm.OLS(house[outcome], house[predictors].assign(const=1))\nresults = model.fit()\nprint(results.summary())","ca9014a0":"predictors = ['SqFtTotLiving', 'SqFtLot', 'Bathrooms', 'Bedrooms',\n              'BldgGrade', 'PropertyType', 'NbrLivingUnits',\n              'SqFtFinBasement', 'YrBuilt', 'YrRenovated', \n              'NewConstruction']\n\nX = pd.get_dummies(house[predictors], drop_first=True)\nX['NewConstruction'] = [1 if nc else 0 for nc in X['NewConstruction']]\n\nhouse_full = sm.OLS(house[outcome], X.assign(const=1))\nresults = house_full.fit()\nprint(results.summary())","0ff5352f":"y = house[outcome]\n\ndef train_model(variables):\n    if len(variables) == 0:\n        return None\n    model = LinearRegression()\n    model.fit(X[variables], y)\n    return model\n\ndef score_model(model, variables):\n    if len(variables) == 0:\n        return AIC_score(y, [y.mean()] * len(y), model, df=1)\n    return AIC_score(y, model.predict(X[variables]), model)\n\nbest_model, best_variables = stepwise_selection(X.columns, train_model, score_model, \n                                                verbose=True)\n\nprint()\nprint(f'Intercept: {best_model.intercept_:.3f}')\nprint('Coefficients:')\nfor name, coef in zip(best_variables, best_model.coef_):\n    print(f' {name}: {coef}')","aefebe21":"house['Year'] = [int(date.split('-')[0]) for date in house.DocumentDate]\nhouse['Year'] = house.DocumentDate.apply(lambda d: int(d.split('-')[0]))\nhouse['Weight'] = house.Year - 2005","4166197b":"predictors = ['SqFtTotLiving', 'SqFtLot', 'Bathrooms', \n              'Bedrooms', 'BldgGrade']\noutcome = 'AdjSalePrice'\n\nhouse_wt = LinearRegression()\nhouse_wt.fit(house[predictors], house[outcome], sample_weight=house.Weight)\npd.DataFrame({\n    'predictor': predictors,\n    'house_lm': house_lm.coef_,\n    'house_wt': house_wt.coef_,\n}).append({\n    'predictor': 'intercept', \n    'house_lm': house_lm.intercept_,\n    'house_wt': house_wt.intercept_,\n}, ignore_index=True)","1ddb40af":"residuals = pd.DataFrame({\n    'abs_residual_lm': np.abs(house_lm.predict(house[predictors]) - house[outcome]),\n    'abs_residual_wt': np.abs(house_wt.predict(house[predictors]) - house[outcome]),\n    'Year': house['Year'],\n})\nprint(residuals.head())\n# axes = residuals.boxplot(['abs_residual_lm', 'abs_residual_wt'], by='Year', figsize=(10, 4))\n# axes[0].set_ylim(0, 300000)\n\npd.DataFrame(([year, np.mean(group['abs_residual_lm']), np.mean(group['abs_residual_wt'])] \n              for year, group in residuals.groupby('Year')),\n             columns=['Year', 'mean abs_residual_lm', 'mean abs_residual_wt'])\n# for year, group in residuals.groupby('Year'):\n#     print(year, np.mean(group['abs_residual_lm']), np.mean(group['abs_residual_wt']))","757529fe":"print(house.PropertyType.head())","ecba8639":"house.PropertyType.unique()","8c17a1be":"print(pd.get_dummies(house['PropertyType']).head(6))","247c9859":"print(pd.get_dummies(house['PropertyType'], drop_first=True).head(6))","9eac3688":"predictors = ['SqFtTotLiving', 'SqFtLot', 'Bathrooms', 'Bedrooms',\n              'BldgGrade', 'PropertyType']\n\nX = pd.get_dummies(house[predictors], drop_first=True)\n\nhouse_lm_factor = LinearRegression()\nhouse_lm_factor.fit(X, house[outcome])\n\nprint(f'Intercept: {house_lm_factor.intercept_:.3f}')\nprint('Coefficients:')\nfor name, coef in zip(X.columns, house_lm_factor.coef_):\n    print(f' {name}: {coef}')","62fca432":"house[outcome]","4fca9ca6":"print(pd.DataFrame(house['ZipCode'].value_counts()).transpose())","bcf9b884":"house = pd.read_csv('..\/input\/loan-csv\/data\/house_sales.csv', sep='\\t')\n\npredictors = ['SqFtTotLiving', 'SqFtLot', 'Bathrooms', \n              'Bedrooms', 'BldgGrade']\noutcome = 'AdjSalePrice'\n\nhouse_lm = LinearRegression()\nhouse_lm.fit(house[predictors], house[outcome])\n\n\nzip_groups = pd.DataFrame([\n    *pd.DataFrame({\n        'ZipCode': house['ZipCode'],\n        'residual' : house[outcome] - house_lm.predict(house[predictors]),\n    })\n    .groupby(['ZipCode'])\n    .apply(lambda x: {\n        'ZipCode': x.iloc[0,0],\n        'count': len(x),\n        'median_residual': x.residual.median()\n    })\n]).sort_values('median_residual')\nzip_groups['cum_count'] = np.cumsum(zip_groups['count'])\nzip_groups['ZipGroup'] = pd.qcut(zip_groups['cum_count'], 5, labels=False, retbins=False)\nzip_groups.head()\nprint(zip_groups.ZipGroup.value_counts().sort_index())","9801a8ef":"to_join = zip_groups[['ZipCode', 'ZipGroup']].set_index('ZipCode')\nhouse = house.join(to_join, on='ZipCode')\nhouse['ZipGroup'] = house['ZipGroup'].astype('category')","a53e3cfd":"print(f'Intercept: {best_model.intercept_:.3f}')\nprint('Coefficients:')\nfor name, coef in zip(best_variables, best_model.coef_):\n    print(f' {name}: {coef}')","d9543285":"predictors = ['Bedrooms', 'BldgGrade', 'PropertyType', 'YrBuilt']\noutcome = 'AdjSalePrice'\n\nX = pd.get_dummies(house[predictors], drop_first=True)\n\nreduced_lm = LinearRegression()\nreduced_lm.fit(X, house[outcome])\n\n\nprint(f'Intercept: {reduced_lm.intercept_:.3f}')\nprint('Coefficients:')\nfor name, coef in zip(X.columns, reduced_lm.coef_):\n    print(f' {name}: {coef}')","0e1c0e8e":"predictors = ['SqFtTotLiving', 'SqFtLot', 'Bathrooms', 'Bedrooms',\n              'BldgGrade', 'PropertyType', 'ZipGroup']\noutcome = 'AdjSalePrice'\n\nX = pd.get_dummies(house[predictors], drop_first=True)\n\nconfounding_lm = LinearRegression()\nconfounding_lm.fit(X, house[outcome])\n\nprint(f'Intercept: {confounding_lm.intercept_:.3f}')\nprint('Coefficients:')\nfor name, coef in zip(X.columns, confounding_lm.coef_):\n    print(f' {name}: {coef}')","a2df45f5":"house.ZipGroup.unique(), house.PropertyType.unique()","076fd679":"model = smf.ols(formula='AdjSalePrice ~  SqFtTotLiving*ZipGroup + SqFtLot + ' +\n     'Bathrooms + Bedrooms + BldgGrade + PropertyType', data=house)\nresults = model.fit()\nprint(results.summary())","2cf182e7":"house_98105 = house.loc[house['ZipCode'] == 98105, ]\n\npredictors = ['SqFtTotLiving', 'SqFtLot', 'Bathrooms', 'Bedrooms',\n              'BldgGrade']\noutcome = 'AdjSalePrice'\n\nhouse_outlier = sm.OLS(house_98105[outcome], house_98105[predictors].assign(const=1))\nresult_98105 = house_outlier.fit()\nprint(result_98105.summary())","5bc2e0f9":"influence = OLSInfluence(result_98105)\nsresiduals = influence.resid_studentized_internal\n\nprint(sresiduals.idxmin(), sresiduals.min())","ba27b629":"print(result_98105.resid.loc[sresiduals.idxmin()])","f97986a3":"outlier = house_98105.loc[sresiduals.idxmin(), :]\nprint('AdjSalePrice', outlier[outcome])\nprint(outlier[predictors])","c3c92285":"%matplotlib inline\nfrom scipy.stats import linregress\n\nnp.random.seed(5)\nx = np.random.normal(size=25)\ny = -x \/ 5 + np.random.normal(size=25)\nx[0] = 8\ny[0] = 8\n\ndef abline(slope, intercept, ax):\n    \"\"\"Calculate coordinates of a line based on slope and intercept\"\"\"\n    x_vals = np.array(ax.get_xlim())\n    return (x_vals, intercept + slope * x_vals)\n\nfig, ax = plt.subplots(figsize=(4, 4))\nax.scatter(x, y)\nslope, intercept, _, _, _ = linregress(x, y)\nax.plot(*abline(slope, intercept, ax))\nslope, intercept, _, _, _ = linregress(x[1:], y[1:])\nax.plot(*abline(slope, intercept, ax), '--')\nax.set_xlim(-2.5, 8.5)\nax.set_ylim(-2.5, 8.5)\n\nplt.tight_layout()\nplt.show()","e4a4756e":"influence = OLSInfluence(result_98105)\nfig, ax = plt.subplots(figsize=(5, 5))\nax.axhline(-2.5, linestyle='--', color='C1')\nax.axhline(2.5, linestyle='--', color='C1')\nax.scatter(influence.hat_matrix_diag, influence.resid_studentized_internal, \n           s=1000 * np.sqrt(influence.cooks_distance[0]),\n           alpha=0.5)\n\nax.set_xlabel('hat values')\nax.set_ylabel('studentized residuals')\n\nplt.tight_layout()\nplt.show()","5cd3074e":"mask = [dist < .08 for dist in influence.cooks_distance[0]]\nhouse_infl = house_98105.loc[mask]\n\nols_infl = sm.OLS(house_infl[outcome], house_infl[predictors].assign(const=1))\nresult_infl = ols_infl.fit()\n\npd.DataFrame({\n    'Original': result_98105.params,\n    'Influential removed': result_infl.params,\n})","4385c069":"fig, ax = plt.subplots(figsize=(5, 5))\nsns.regplot(x=result_98105.fittedvalues, y=np.abs(result_98105.resid), \n            scatter_kws={'alpha': 0.25},\n            line_kws={'color': 'C1'},\n            lowess=True, ax=ax)\nax.set_xlabel('predicted')\nax.set_ylabel('abs(residual)')\n\nplt.tight_layout()\nplt.show()","95082f91":"fig, ax = plt.subplots(figsize=(4, 4))\npd.Series(influence.resid_studentized_internal).hist(ax=ax)\nax.set_xlabel('std. residual')\nax.set_ylabel('Frequency')\n\n\nplt.tight_layout()\nplt.show()","fc137dde":"fig, ax = plt.subplots(figsize=(5, 5))\nfig = sm.graphics.plot_ccpr(result_98105, 'SqFtTotLiving', ax=ax)\n\nplt.tight_layout()\nplt.show()","4546933b":"fig = plt.figure(figsize=(8, 12))\nfig = sm.graphics.plot_ccpr_grid(result_98105, fig=fig)","03d1cef9":"model_poly = smf.ols(formula='AdjSalePrice ~  SqFtTotLiving + np.power(SqFtTotLiving, 2) + ' + \n                'SqFtLot + Bathrooms + Bedrooms + BldgGrade', data=house_98105)\nresult_poly = model_poly.fit()\nprint(result_poly.summary())","7d137bf4":"def partialResidualPlot(model, df, outcome, feature, ax):\n    y_pred = model.predict(df)\n    copy_df = df.copy()\n    for c in copy_df.columns:\n        if c == feature:\n            continue\n        copy_df[c] = 0.0\n    feature_prediction = model.predict(copy_df)\n    results = pd.DataFrame({\n        'feature': df[feature],\n        'residual': df[outcome] - y_pred,\n        'ypartial': feature_prediction - model.params[0],\n    })\n    results = results.sort_values(by=['feature'])\n    smoothed = sm.nonparametric.lowess(results.ypartial, results.feature, frac=1\/3)\n    \n    ax.scatter(results.feature, results.ypartial + results.residual)\n    ax.plot(smoothed[:, 0], smoothed[:, 1], color='gray')\n    ax.plot(results.feature, results.ypartial, color='black')\n    ax.set_xlabel(feature)\n    ax.set_ylabel(f'Residual + {feature} contribution')\n    return ax\n\nfig, ax = plt.subplots(figsize=(5, 5))\npartialResidualPlot(result_poly, house_98105, 'AdjSalePrice', 'SqFtTotLiving', ax)\n\nplt.tight_layout()\nplt.show()\nprint(result_poly.params[2])","32372161":"formula = ('AdjSalePrice ~ bs(SqFtTotLiving, df=6, degree=3) + ' + \n           'SqFtLot + Bathrooms + Bedrooms + BldgGrade')\nmodel_spline = smf.ols(formula=formula, data=house_98105)\nresult_spline = model_spline.fit()\nprint(result_spline.summary())","bed225c2":"fig, ax = plt.subplots(figsize=(5, 5))\npartialResidualPlot(result_spline, house_98105, 'AdjSalePrice', 'SqFtTotLiving', ax)\n\nplt.tight_layout()\nplt.show()","a1cf7728":"predictors = ['SqFtTotLiving', 'SqFtLot', 'Bathrooms', \n              'Bedrooms', 'BldgGrade']\noutcome = 'AdjSalePrice'\nX = house_98105[predictors].values\ny = house_98105[outcome]\n\n## model\ngam = LinearGAM(s(0, n_splines=12) + l(1) + l(2) + l(3) + l(4))\ngam.gridsearch(X, y)\nprint(gam.summary())","c2b0e0bd":"fig, axes = plt.subplots(figsize=(8, 8), ncols=2, nrows=3)\n\ntitles = ['SqFtTotLiving', 'SqFtLot', 'Bathrooms', 'Bedrooms', 'BldgGrade']\nfor i, title in enumerate(titles):\n    ax = axes[i \/\/ 2, i % 2]\n    XX = gam.generate_X_grid(term=i)\n    ax.plot(XX[:, i], gam.partial_dependence(term=i, X=XX))\n    ax.plot(XX[:, i], gam.partial_dependence(term=i, X=XX, width=.95)[1], c='r', ls='--')\n    ax.set_title(titles[i]);\n    \naxes[2][1].set_visible(False)\n\nplt.tight_layout()\nplt.show()","85264267":"from sklearn.linear_model import Lasso, LassoLars, LassoCV, LassoLarsCV\nfrom sklearn.preprocessing import StandardScaler","77f2d8d3":"subset = ['AdjSalePrice', 'SqFtTotLiving', 'SqFtLot', 'Bathrooms', \n          'Bedrooms', 'BldgGrade']\n\nhouse = pd.read_csv('..\/input\/loan-csv\/data\/house_sales.csv', sep='\\t')\nprint(house[subset].head())","e787c7ee":"predictors = ['SqFtTotLiving', 'SqFtLot', 'Bathrooms', 'Bedrooms',\n              'BldgGrade', 'PropertyType', 'NbrLivingUnits',\n              'SqFtFinBasement', 'YrBuilt', 'YrRenovated', \n              'NewConstruction']\noutcome = 'AdjSalePrice'\n\nX = pd.get_dummies(house[predictors], drop_first=True)\nX['NewConstruction'] = [1 if nc else 0 for nc in X['NewConstruction']]\ncolumns = X.columns\n# X = StandardScaler().fit_transform(X * 1.0)\ny = house[outcome]\n\nhouse_lm = LinearRegression()\nprint(house_lm.fit(X, y))","df53aef7":"house_lasso = Lasso(alpha=10)\nprint(house_lasso.fit(X, y))","cb2f5ed9":"Method = LassoLars\nMethodCV = LassoLarsCV\nMethod = Lasso\nMethodCV = LassoCV\n\nalpha_values = []\nresults = []\nfor alpha in [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000]:\n    model = Method(alpha=alpha)\n    model.fit(X, y)\n    alpha_values.append(alpha)\n    results.append(model.coef_)\nmodelCV = MethodCV(cv=5)\nmodelCV.fit(X, y)\nax = pd.DataFrame(results, index=alpha_values, columns=columns).plot(logx=True, legend=False)\nax.axvline(modelCV.alpha_)\nplt.show()","57200f10":"pd.DataFrame({\n    'name': columns,\n    'coef': modelCV.coef_, \n})","e14e03ee":"# Intercept: 6177658.144\n# Coefficients:\n#  SqFtTotLiving: 199.27474217544048\n#  BldgGrade: 137181.13724627026\n#  YrBuilt: -3564.934870415041\n#  Bedrooms: -51974.76845567939\n#  Bathrooms: 42403.059999677665\n#  PropertyType_Townhouse: 84378.9333363999\n#  SqFtFinBasement: 7.032178917565108\n#  PropertyType_Single Family: 22854.87954019308","1f231d62":"While _scikit-learn_ provides a variety of different metrics, _statsmodels_ provides a more in-depth analysis of the linear regression model. This package has two different ways of specifying the model, one that is similar to _scikit-learn_ and one that allows specifying _R_-style formulas. Here we use the first approach. As _statsmodels_ doesn't add an intercept automaticaly, we need to add a constant column with value 1 to the predictors. We can use the _pandas_ method assign for this.","67be8a0d":"Import required Python packages.","20540daa":"> Results differ from R due to different binning. Enforcing the same binning gives identical results","75976ba5":"## Factor Variables with many levels","66901dc7":"## Weighted regression\nWe can calculate the Year from the date column using either a list comprehension or the data frame's `apply` method.","38fefaa3":"The _statsmodels_ package has the most developed support for outlier analysis. ","1fea23a3":"Define paths to data sets. If you don't keep your data in the same directory as the code, adapt the path names.","64941cf3":"## Splines","7c8aa239":"## Assessing the Model\n_Scikit-learn_ provides a number of metrics to determine the quality of a model. Here we use the `r2_score`.","1a404b5c":"# Simple Linear Regression\n## The Regression Equation","127b3eb1":"We can use the `stepwise_selection` method from the _dmba_ package.","7a3da884":"## Partial Residual Plots and Nonlinearity","843ff3af":"## Heteroskedasticity, Non-Normality and Correlated Errors","336820f8":"We can use the `LinearRegression` model from _scikit-learn_.","6e03e576":"## Influential values","49acaf3e":"# Testing the Assumptions: Regression Diagnostics\n## Outliers","2fea7722":"# Additional material - not in book\n# Regularization\n## Lasso","59505610":"# Interpreting the Regression Equation\n## Correlated predictors","12b06a46":"## Interactions and Main Effects","efc079b2":"The `regplot` in _seaborn_ allows adding a lowess smoothing line to the scatterplot.","b58cd401":"## Generalized Additive Models","89d39f3e":"## Confounding variables","ee4f8da7":"# Practical Statistics for Data Scientists (Python)\n# Chapter 4. Regression and Prediction\n> (c) 2019 Peter C. Bruce, Andrew Bruce, Peter Gedeck","e79cd3d0":"The `OLSInfluence` class is initialized with the OLS regression results and gives access to a number of usefule properties. Here we use the studentized residuals. ","80fec3ff":"## Polynomial and Spline Regression","c78b3a3c":"The package _statsmodel_ provides a number of plots to analyze the data point influence","d09c8ed7":"# Factor variables in regression\n## Dummy Variables Representation","5c0233d9":"The results from the stepwise regression are.","e92ba8de":"## Fitted Values and Residuals\nThe method `predict` of a fitted _scikit-learn_ model can be used to predict new data points.","888345c5":"## Model Selection and Stepwise Regression","54fbde42":"The statsmodels implementation of a partial residual plot works only for linear term. Here is an implementation of a partial residual plot that, while inefficient, works for the polynomial regression.","6a59b89a":"# Multiple linear regression"}}