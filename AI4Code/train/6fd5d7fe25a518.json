{"cell_type":{"9d538604":"code","17e8dc7b":"code","ba46e1a4":"code","afc25b00":"code","9ca2c92a":"code","3a414622":"code","69b9b449":"code","61d02590":"code","05513325":"code","730877cf":"code","e150087f":"code","0dd8e646":"code","2e0d7a0a":"code","22ed7401":"code","8f33ee25":"code","2ed7b013":"code","53400230":"code","9231f332":"code","c091ccf6":"code","7249b6e3":"code","85bbe545":"code","86473f55":"code","12e0d972":"code","4a75d1eb":"code","7081c0bb":"code","e827e8f3":"code","744670e8":"code","d72dcc26":"code","81b525aa":"code","bf361545":"code","27524a59":"code","86af0869":"code","ce094d72":"code","6577e651":"code","55300fd1":"code","5ecafaea":"code","57991c76":"code","9ac22f27":"code","db803158":"code","4468f971":"code","1554f263":"code","631e2dad":"code","f787a6f9":"code","1516ac03":"code","27cab134":"code","b690bcf9":"code","fa605098":"code","e23df39d":"code","68fbd272":"code","5ca3cd7a":"code","db39e67f":"code","5b75e6af":"code","eeb98d23":"code","10c4aacf":"code","893b5e5e":"code","47d49ad7":"code","d0c4501a":"code","fffcfd3f":"code","c9bef25f":"code","66512fc4":"code","13f36ebf":"code","01b76584":"code","776974b9":"code","fd807d87":"code","0e8f119b":"code","8e47d8a5":"code","dc60cd0d":"code","929c10f8":"code","948771aa":"code","74fe082f":"code","0014518f":"code","fb2cc898":"code","a4f5ee08":"code","5c968b14":"code","d8d3c4f1":"code","20f3f742":"code","2f738088":"code","1fd942bd":"code","2d26516f":"code","42d49f95":"code","38df78ba":"code","9915ae62":"code","f227b83f":"code","c3caef31":"code","dcc08801":"code","45f415db":"code","0c420cfe":"code","77e0f209":"code","c59fa218":"code","48af323e":"code","a5ddb2cb":"markdown","9ef79b1c":"markdown","833ba2b6":"markdown","6fcc1481":"markdown","05eda749":"markdown","77fa9069":"markdown","7c8f9e16":"markdown","842cf74e":"markdown","38029d3d":"markdown"},"source":{"9d538604":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n%matplotlib inline","17e8dc7b":"hdb_string = \"..\/input\/hdb-prices-with-closest-mrt-distance\/HDBdata with district_coord_dist_full_corrected2.csv\"\ndf = pd.read_csv(hdb_string, na_values=0)","ba46e1a4":"df.head()","afc25b00":"df.describe().T","9ca2c92a":"df[df.mrtdist.isna()]","3a414622":"df_dropped = df.dropna(axis=0)","69b9b449":"df_dropped.describe().T","61d02590":"df_dropped.head()","05513325":"df_dropped.plot(kind='scatter', \n                x = 'Latitude', \n                y= 'Longitude', \n                c ='Price\/sqm', \n                label= 'Price\/sqm', \n                cmap = 'cool',\n                colorbar = True,\n                figsize = (20,10))\nplt.show()","730877cf":"# Total features dropped are town, lease commence date, block, address and price\/sqm\n\ndf_dropped.drop(columns=['town','lease_commence_date','Price\/sqm','District info','District.1'], inplace = True)\ndf_dropped.District = df_dropped.District.astype(str)\ndf_dropped.drop(columns=['street_name','block'], inplace = True)","e150087f":"df_dropped.head()","0dd8e646":"df_dropped.info()","2e0d7a0a":"## PCA here, DBSCAN\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import DBSCAN","22ed7401":"from sklearn.preprocessing import StandardScaler , OneHotEncoder , PolynomialFeatures\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.linear_model import LinearRegression , Lasso, Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, BaggingRegressor, VotingRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nimport pickle\n\nseed_number = 42","8f33ee25":"y = df_dropped.resale_price\nx = df_dropped.loc[:, df_dropped.columns != 'resale_price']","2ed7b013":"num_cols = x.select_dtypes(['float64','int64']).columns\ncat_cols = x.select_dtypes(['object']).columns","53400230":"df_dropped.shape","9231f332":"# Creating training, validation and test sets\n\nX_train, X_temp, y_train, y_temp = train_test_split(x, y ,\n                                                    test_size = 0.2, \n                                                    shuffle = True,\n                                                   random_state = seed_number)","c091ccf6":"X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp ,\n                                                    test_size = 0.5, \n                                                    shuffle = True,\n                                                   random_state = seed_number)","7249b6e3":"print('Shapes')\nprint('X_train.shape \\t' , X_train.shape)\nprint('X_val.shape \\t' , X_val.shape)\nprint('X_test.shape \\t' , X_test.shape)\nprint('y_train.shape \\t' , y_train.shape)\nprint('y_val.shape \\t' , y_val.shape)\nprint('y_test.shape \\t' , y_test.shape)","85bbe545":"X_train.head()","86473f55":"# Pipeline needs to include the elimination of the column and feature transformation \/ engineering","12e0d972":"# Pipeline\n\n# Scale numerical variables\n\nnum_pipeline = Pipeline([\n    ('ss', StandardScaler()),\n    ('poly', PolynomialFeatures())\n])\n\n# One hot encoding categorical variables\n\ncat_pipline = Pipeline([\n    ('ohe', OneHotEncoder(drop = 'first'))\n])\n\npreprocess = ColumnTransformer([\n    ('num', num_pipeline, num_cols),\n    ('cat', cat_pipline, cat_cols)\n])","4a75d1eb":"X_train_preprocessed = preprocess.fit_transform(X_train)","7081c0bb":"X_val_preprocessed = preprocess.transform(X_val)","e827e8f3":"X_train_preprocessed.shape","744670e8":"X_train_preprocessed[:10]","d72dcc26":"X_train_preprocessed[0].shape","81b525aa":"X_val_preprocessed.shape","bf361545":"def plot_results(score, title):\n    \n    train_score = score['train_score']\n    test_score = score['test_score']\n    \n    train_score = (-train_score)**(1\/2) \n    test_score = (-test_score)**(1\/2)\n    \n    cv = len(score['score_time'])\n    \n    fig, ax = plt.subplots(figsize = (5,5))\n    \n    ax.plot(range(cv), train_score, 'r-', label = 'Training RMSE')\n    ax.plot(range(cv), test_score, 'b--', label = 'Validation RMSE')\n    ax.set_title(title)\n    ax.set_xlabel('K-Folds')\n    ax.set_ylabel('Error')\n    ax.legend()\n    plt.show()","27524a59":"model_list = {'Linear Regression':LinearRegression(), \n              'Lasso':Lasso(),\n              'Ridge': Ridge(), \n              'Light Gradient Boosting Machine' : lgb.LGBMRegressor(random_state=seed_number),\n              'Extreme Gradient Boosting Machine' : xgb.XGBRegressor(random_state=seed_number),\n              'Decision Tree' :DecisionTreeRegressor(),\n              'Random Forest' :RandomForestRegressor()\n             }","86af0869":"# Check for overfitting, shortlisting models","ce094d72":"def plot_results_wtrain(score, title):\n    \n    train_score = score['train_score']\n    test_score = score['test_score']\n    \n    train_score = (-train_score)**(1\/2) \n    test_score = (-test_score)**(1\/2)\n    \n    cv = len(score['score_time'])\n    \n    plt.plot(range(cv), train_score, 'r-', label = 'Training RMSE')\n    plt.plot(range(cv), test_score, 'b--', label = 'Validation RMSE')\n    plt.title(title)\n    plt.xlabel('K-Folds')\n    plt.ylabel('Error')\n    plt.legend()\n    plt.show()","6577e651":"from sklearn.model_selection import cross_validate","55300fd1":"cv = 10\ntraining_samples = 5000\n\ntrain_rmse_list = []\nval_rmse_list = []\n\nfor name , model in model_list.items():\n    model_instance = model.fit(X_train_preprocessed[:training_samples], y_train[:training_samples])\n    \n    model_cv_score = cross_validate(model_instance,X_train_preprocessed[:training_samples], y_train[:training_samples], scoring = 'neg_mean_squared_error', cv=cv, n_jobs = -1,return_train_score= True )\n    \n    #model_cv_rmse = (-model_cv_score)**(1\/2)\n    \n    # with open(name+'_pkl.pkl', 'wb') as files:\n    #     pickle.dump(model, files)\n    \n    # with open(name+'_results_pkl', 'wb') as files:\n    #     pickle.dump(model, files)\n    \n    train_rmse_list.append(model_cv_score['train_score'])\n    val_rmse_list.append(model_cv_score['test_score'])\n    \n    print('Model is ', name)\n    print('Mean Train RMSE', ((-model_cv_score['train_score'])**(1\/2)).mean())\n    print('Train RMSE ', list(model_cv_score['train_score']))\n    print('Val RMSE ' , list(model_cv_score['test_score']))\n    print('Mean Val RMSE ' , ((-model_cv_score['test_score'])**(1\/2)).mean())\n\n    plot_results(model_cv_score, name)","5ecafaea":"fig, ax = plt.subplots(figsize = (15,15))\n\nfor index, model_name in enumerate(model_list.keys()):\n    \n    train_rmse_list\n    \n    ax.plot(range(len(val_rmse_list[index])), ((-train_rmse_list[index])**(1\/2)),label = 'Training RMSE of ' + model_name)\n    ax.plot(range(len(val_rmse_list[index])), ((-val_rmse_list[index])**(1\/2)), '--' , label = 'Validation RMSE of ' + model_name)\n\nax.set_title('Comparison of all models')\nax.set_xlabel('K-Folds')\nax.set_ylabel('Error')\nax.legend()\nplt.show()","57991c76":"# Hyperparameter Tunning for selected models","9ac22f27":"params = {'max_bin': [300,350,400],\n          'learning_rate': np.arange(0.1,0.5,0.2),\n          'num_leaves': range(300,360,20),\n         'random_state': [seed_number]}\n\ncv = 3\n\nlgb = lgb.LGBMRegressor(random_state = seed_number)\n\ncv = GridSearchCV(lgb,param_grid = params, scoring = 'neg_mean_squared_error', cv=3)\n\nlgb_cv = cv.fit(X_train_preprocessed, y_train)\n\n\nprint(lgb_cv.best_params_)","db803158":"lgb_score = (-lgb_cv.best_score_)**(1\/2)\nprint(lgb_score)","4468f971":"params = {'eta': [0.1,0.3,0.5,0.7],\n          'max_depth': range(4,12,2),\n          'max_leaves': range(0,4,2),\n          'max_bin' : [0,32,64,128,256],\n         'random_state': [seed_number]}\n\ncv = 3\n\nxgb_model = xgb.XGBRegressor(random_state = seed_number)\n\ncv = GridSearchCV(xgb_model,param_grid = params, scoring = 'neg_mean_squared_error', cv=3)\n\nxgb_model_cv = cv.fit(X_train_preprocessed, y_train)\n\n\nprint(xgb_model_cv.best_params_)","1554f263":"xgb_model_cv_score = (-xgb_model_cv.best_score_)**(1\/2)\nprint(xgb_model_cv_score)","631e2dad":"# Feature importance","f787a6f9":"len(lgb_cv.best_estimator_.feature_importances_)","1516ac03":"print(lgb_cv.best_estimator_.feature_importances_)","27cab134":"preprocess.get_params","b690bcf9":"code_list = ['x0','x1','x2','x3','x4']","fa605098":"map_dict = {}\n\nfor index , item in enumerate(code_list):\n    map_dict[item] = num_cols[index]","e23df39d":"map_dict","68fbd272":"len(preprocess.named_transformers_['num']['poly'].get_feature_names())","5ca3cd7a":"num_transformed_list = preprocess.named_transformers_['num']['poly'].get_feature_names()","db39e67f":"num_transformed_list","5b75e6af":"for index, item in enumerate(num_transformed_list):\n    for key, value in map_dict.items():\n        if key in item: \n            num_transformed_list[index] = num_transformed_list[index].replace(key,value)","eeb98d23":"num_transformed_list","10c4aacf":"cat_cols","893b5e5e":"map_dict = {}\n\nfor index , item in enumerate(code_list):\n    map_dict[item] = cat_cols[index]","47d49ad7":"map_dict","d0c4501a":"preprocess.named_transformers_['cat']['ohe'].get_feature_names()","fffcfd3f":"cat_transformed_list = preprocess.named_transformers_['cat']['ohe'].get_feature_names()","c9bef25f":"cat_transformed_list","66512fc4":"for index, item in enumerate(cat_transformed_list):\n    for key, value in map_dict.items():\n        if key in item: \n            cat_transformed_list[index] = cat_transformed_list[index].replace(key,value)","13f36ebf":"num_transformed_list.extend(cat_transformed_list)","01b76584":"num_transformed_list","776974b9":"sorted(zip(lgb_cv.best_estimator_.feature_importances_,preprocess.named_transformers_['num']['poly'].get_feature_names() + \\\n           preprocess.named_transformers_['cat']['ohe'].get_feature_names().tolist()),reverse = True)","fd807d87":"feature_importance = sorted(zip(lgb_cv.best_estimator_.feature_importances_,num_transformed_list),reverse = True)","0e8f119b":"# top Ten most important features\nfeature_importance[:10]","8e47d8a5":"# bottom ten least important\nfeature_importance[-10:]","dc60cd0d":"y_pred = lgb_cv.predict(preprocess.transform(X_test))\n\nlgb_mse = mean_squared_error(y_test,y_pred)**(1\/2)\nlgb_mae = mean_absolute_error(y_test,y_pred)\nlgb_r2 = r2_score(y_test,y_pred)\n\nprint('RMSE on test set', lgb_mse)\nprint('MAE on test set', lgb_mae)\nprint('R2 on test set', lgb_r2)","929c10f8":"# Asessing Final model on test set","948771aa":"X_test.iloc[0]","74fe082f":"y_test.iloc[0]","0014518f":"X_test_preprocess = preprocess.transform(X_test)","fb2cc898":"lgb_cv.predict(X_test_preprocess[0])","a4f5ee08":"import tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K","5c968b14":"def R2_score(y_true, y_pred):\n    SS_res =  K.sum(K.square(y_true - y_pred)) \n    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n    return ( 1 - SS_res\/(SS_tot + K.epsilon()) )","d8d3c4f1":"dnn_model = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(512,input_dim = X_train_preprocessed.shape[1],activation = 'relu'),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(1024, activation = 'relu'),\n    tf.keras.layers.Dense(1, activation = 'relu')\n])","20f3f742":"dnn_model.summary()","2f738088":"early_callback = tf.keras.callbacks.EarlyStopping(patience=50)","1fd942bd":"dnn_model.compile(loss = 'mse', \n                  metrics =[tf.metrics.RootMeanSquaredError(),tf.metrics.MeanAbsoluteError(),R2_score],#.,tfa.metrics.RSquare()], \n                  optimizer = tf.keras.optimizers.RMSprop())","2d26516f":"# to.array() converts the sparse matrix to numpy array\ndnn_model_history = dnn_model.fit(X_train_preprocessed.toarray(), \n                                  np.array(y_train), \n                                  epochs = 250, \n                                  validation_data = (X_val_preprocessed.toarray(), np.array(y_val)), \n                                  batch_size = 32,\n                                  callbacks=[early_callback])","42d49f95":"dnn_model_history.history.keys()","38df78ba":"def plot_dnn_results(history):\n    \n    fig , axes = plt.subplots(nrows=2,figsize = (10,10))\n    \n    rmse = history.history['root_mean_squared_error']\n    val_rmse = history.history['val_root_mean_squared_error']\n        \n    axes[0].plot(range(len(rmse)),np.round(rmse,2), 'r-', label = 'Training RMSE')\n    axes[0].plot(range(len(rmse)),np.round(val_rmse,2), 'b--', label = 'Validation RMSE')\n    axes[0].set_ylabel('RMSE')\n    axes[0].set_xlabel('Epochs')\n    axes[0].set_title('Training vs Validation RMSE for Full Training Data')\n    axes[0].legend()\n    plt.show\n    \n    rmse = history.history['root_mean_squared_error'][-50:]\n    val_rmse = history.history['val_root_mean_squared_error'][-50:]\n    \n    axes[1].plot(range(len(rmse))[-50:],np.round(rmse,2), 'r-', label = 'Training RMSE')\n    axes[1].plot(range(len(rmse))[-50:],np.round(val_rmse,2), 'b--', label = 'Validation RMSE')\n    axes[1].set_ylabel('RMSE')\n    axes[1].set_xlabel('Last Epochs')\n    axes[1].set_title('Training vs Validation RMSE for last 50 epochs')\n    axes[1].legend()\n    plt.show","9915ae62":"plot_dnn_results(dnn_model_history)","f227b83f":"X_test_preprocess = preprocess.transform(X_test)","c3caef31":"dnn_model_score = dnn_model.evaluate(X_test_preprocess.toarray(), y_test)","dcc08801":"dnn_model_score","45f415db":"print('RMSE of Deep Learning Model :' ,dnn_model_score[1])\nprint('MAE of Deep Learning Model :' ,dnn_model_score[2])\nprint('R2 score of Deep Learning Model :' ,dnn_model_score[3])","0c420cfe":"X_test.iloc[0]","77e0f209":"y_test.iloc[0]","c59fa218":"dnn_model.predict(X_test_preprocess[0])","48af323e":"mse_train = mean_squared_error(lgb_cv.predict(preprocess.transform(X_train)), y_train)**(1\/2)\nmse_test = mean_squared_error(lgb_cv.predict(preprocess.transform(X_test)), y_test)**(1\/2)\n\nprint('mse_train ' , mse_train)\nprint('mse_test ' , mse_test)","a5ddb2cb":"Random Forest and Decision Tree clearly overfits too much. Light GBM is quick , XGB is slower to tune","9ef79b1c":"polynominal features (degree 2) added in for numerical features below in pipeline","833ba2b6":"# EDA","6fcc1481":"# Feature Engineering","05eda749":"Right off the bat we notice some outliers when it comes to location coordinates","77fa9069":"https:\/\/towardsdatascience.com\/how-to-get-feature-importances-from-any-sklearn-pipeline-167a19f1214","7c8f9e16":"# Deep Learning Model","842cf74e":"# Modelling","38029d3d":"We narrow down to lgbm because of its accuracy and quick training speed "}}