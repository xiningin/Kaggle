{"cell_type":{"dbda2623":"code","5782d8e0":"code","37f4c16b":"code","14653c7a":"code","9b0ea1de":"code","1f280fb9":"code","2eadfd8b":"code","e9350044":"code","92ffdf61":"code","673b2b10":"code","69ffcfe1":"code","e97047d4":"code","2cfb1ee6":"code","2fd7c63d":"code","6c7eb522":"code","44c4f628":"code","ba93f597":"code","9ae4e04b":"code","0dc9b9f7":"code","480284e8":"code","a047867b":"code","5ff008b8":"code","c355a93b":"code","047c959b":"code","68bfeb72":"code","44107286":"code","4570b69c":"code","971f9f6a":"code","f0caa289":"code","4adfa561":"code","f632acda":"code","daa85804":"code","ac8ba100":"code","bdb36b92":"code","c4f291e7":"code","b7c7199a":"code","f59af8ab":"markdown","b5403ab7":"markdown","dda704d0":"markdown","775b201b":"markdown","2b9cf420":"markdown","a1c73d8a":"markdown","0d7e83f9":"markdown","74c1de2f":"markdown","339dce8c":"markdown","934e7757":"markdown","25cb3a92":"markdown","a3967c30":"markdown","a842664e":"markdown","91d77d34":"markdown","6b1c3296":"markdown","538357a8":"markdown","0e24a909":"markdown","3ba69b67":"markdown","ee50d512":"markdown","87e9cdc8":"markdown","b4fbef30":"markdown"},"source":{"dbda2623":"from fastai import *\nfrom fastai.vision import *\nfrom fastai.callbacks.hooks import *\nfrom fastai.callbacks import *\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.image as immg\nimport gc\nimport numpy as np\nimport random\nfrom PIL import Image\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5782d8e0":"open_image('..\/input\/semantic-drone-dataset\/semantic_drone_dataset\/original_images\/001.jpg').data.shape","37f4c16b":"path = Path('..\/input\/drone-images-mask-resized\/drone_data_small')  ","14653c7a":"path.ls()","9b0ea1de":"fnames = get_files(path\/'train_small')\nfnames_mask = get_files(path\/'label_small')","1f280fb9":"open_image(fnames[4]).data.shape","2eadfd8b":"from tqdm.notebook import tqdm,tnrange","e9350044":"path_im = path\/'train_small'\npath_lb = path\/'label_small'\nget_y_fns = lambda x: path_lb\/f'{x.stem}.png'       # Function to get masks for a image","92ffdf61":"fnames[30],get_y_fns(fnames[30])","673b2b10":"def get_classes(fnames):\n    class_codes=[]\n    for i in tqdm(range(400)):\n        class_codes += list(np.unique(np.asarray(Image.open(get_y_fns(fnames[i])))))\n    return np.array(list(set(class_codes)))","69ffcfe1":"# Run this once to get total classes if you want, other wise below cell gives total classes\ncodes = get_classes(fnames)  ","e97047d4":"codes = np.array(codes)\ncodes","2cfb1ee6":"sns.set_style('darkgrid')","2fd7c63d":"def drone_mask(f):  # f = file_name\n  img_a = immg.imread(f)\n  img_a_mask = immg.imread(get_y_fns(f))\n  plt.figure(1,figsize=(20,8))\n  plt.subplot(121)\n  plt.imshow(img_a);plt.title('Raw Drone footage ');plt.axis('off')\n  plt.subplot(122)\n  plt.imshow(img_a,alpha=0.8);\n  plt.imshow(img_a_mask,alpha=0.8);plt.title('Drone with  mask');plt.axis('off')\n  plt.show()","6c7eb522":"for i in range(3):\n    img_num = random.randint(10,200)\n    drone_mask(fnames[img_num])","44c4f628":"src=np.array([400,600])\n#src=src\/\/2\nsrc","ba93f597":"data = (SegmentationItemList.from_folder(path=path_im)  # Location from path\n        .split_by_rand_pct(0.2)                          # Split for train and validation set\n        .label_from_func(get_y_fns, classes=codes)      # Label from a above defined function\n        .transform(get_transforms(), size=src\/\/2, tfm_y=True)   # If you want to apply any image Transform\n        .databunch(bs=4)                                   # Batch size  please decrese batch size if cuda out of memory\n        .normalize(imagenet_stats))            # Normalise with imagenet stats","9ae4e04b":"data.show_batch(rows=2,figsize=(20,10));","0dc9b9f7":"len(data.train_ds), len(data.valid_ds), data.c  ","480284e8":"name2id = {v:k for k,v in enumerate(codes)}\nvoid_code = -1\n\ndef drone_accuracy_mask(input, target):\n    target = target.squeeze(1)\n    mask = target != void_code\n    return (input.argmax(dim=1)[mask]==target[mask]).float().mean()","a047867b":"metrics = drone_accuracy_mask\nwd=1e-2    # wd = weight decay","5ff008b8":"arch = models.resnet34\nlearn = unet_learner(data, # DatBunch\n                     arch, # Backbone pretrained arch\n                     metrics = [metrics], # metrics\n                     wd = wd, bottle=True, # weight decay\n                     model_dir = '\/kaggle\/working\/') # model directory to save","c355a93b":"learn.lr_find()\nlearn.recorder.plot()","047c959b":"gc.collect() # to clear the cache","68bfeb72":"callbacks = SaveModelCallback(learn, monitor = 'drone_accuracy_mask', every = 'improvement', mode='max', name = 'best_model' )","44107286":"lr = 1e-3           # Learning Rate","4570b69c":"learn.fit(10, lr,callbacks = [callbacks] )","971f9f6a":"learn.freeze()\nlearn.lr_find()\nlearn.recorder.plot()","f0caa289":"gc.collect()","4adfa561":"learn.load('best_model');\ncallbacks2 = SaveModelCallback(learn, monitor = 'drone_accuracy_mask', every = 'improvement', mode='max', name = 'best_model_ft' )","f632acda":"learn.unfreeze()\nlearn.fit_one_cycle(10,max_lr= slice(1e-5,1e-3\/2),callbacks = [callbacks2] )","daa85804":"learn.show_results(rows = 4, figsize=(16,18))","ac8ba100":"learn.save('stage-1-big')  # saving the model ","bdb36b92":"learn.export('\/kaggle\/working\/drone_mask.pkl')","c4f291e7":"def drone_predict(f):\n    img = open_image(f).resize((3,200,300))\n    mask = learn.predict(img)[0]\n    _,axs = plt.subplots(1,3, figsize=(24,10))\n    img.show(ax=axs[0], title='no mask')\n    img.show(ax=axs[1], y=mask, title='masked')\n    mask.show(ax=axs[2], title='mask only', alpha=1.)","b7c7199a":"for i in range(3):\n    n = random.randint(20,200)\n    drone_predict(fnames[n])","f59af8ab":"## Model Summary","b5403ab7":"\n\n<center><h2> On How to train A neural network for image Segmentation using Fast.ai and Transfer Learning<\/h2><\/center>\n\n\n***\n\n<center><img src=\"https:\/\/github.com\/shadab4150\/Aerial_drone_image_segmentation\/raw\/master\/image_drone\/drone1.png\"><\/center>\n","dda704d0":"## What is semantic segmentation ?\n\n* Source: **https:\/\/divamgupta.com\/image-segmentation\/2019\/06\/06\/deep-learning-semantic-segmentation-keras.html**\n\n* **Semantic image segmentation is the task of classifying each pixel in an image from a predefined set of classes.**\n\n***\n\nIn the following example, different entities are classified.\n\n![kd](https:\/\/divamgupta.com\/assets\/images\/posts\/imgseg\/image15.png?style=centerme)\n\n***\n\n\nIn the above example, the pixels belonging to the bed are classified in the class \u201cbed\u201d, the pixels corresponding to the walls are labeled as \u201cwall\u201d, etc.\n\nIn particular, our goal is to take an image of size W x H x 3 and generate a W x H matrix containing the predicted class ID\u2019s corresponding to all the pixels.\n\n***\n![kd](https:\/\/divamgupta.com\/assets\/images\/posts\/imgseg\/image14.png?style=centerme)\n\n***\n\nUsually, in an image with various entities, we want to know which pixel belongs to which entity, For example in an outdoor image, we can segment the sky, ground, trees, people, etc.","775b201b":"## Export the model","2b9cf420":"### Fastai's unet_learner\n* Source [**Fast.ai**](www.fast.ai)\n\n* This module builds a dynamic U-Net from any backbone **pretrained on ImageNet**, automatically inferring the intermediate sizes.\n\n![kd](https:\/\/www.researchgate.net\/profile\/Alan_Jackson9\/publication\/323597886\/figure\/fig2\/AS:601386504957959@1520393124691\/Convolutional-neural-network-CNN-architecture-based-on-UNET-Ronneberger-et-al.png)\n\n* **This is the original U-Net. The difference here is that the left part is a pretrained model.**\n\n* **This U-Net will sit on top of an encoder ( that can be a pretrained model -- eg. resnet50 ) and with a final output of num_classes.**","a1c73d8a":"## Finding a suitable learning rate for our model\n\n* With help fast.ai **learning rate finder** function","0d7e83f9":"# Model","74c1de2f":"## To check results of our trained model","339dce8c":"## Creating A DatabLock for the model","934e7757":"* **Function to make a prediction and Overlap the Drone Images with Predicted Drone Mask**","25cb3a92":"## Prediction","a3967c30":"## Data PreProcessing","a842664e":"* Since each pixel belongs to a diffrent class below function counts total number of such classes","91d77d34":"* **Metrics for Drone mask**\n","6b1c3296":"## Path to the dataset\n* Images were too big from original dataset So, resized them in 2 size\n* > 1800x1200\n* > 600x400","538357a8":"### Load the model  and predict","0e24a909":"## Importing useful libraries","3ba69b67":"## Function to show Drone with Mask","ee50d512":"## A sample Drone with Mask","87e9cdc8":"## Results \n* Intial dynamic unet on top of an encoder ( resnet34 pretrained = 'imagenet' ), trained for 30 epochs gave an **accuracy** of **80.00%** .","b4fbef30":"<center><h3> Please Upvote if you like it. <\/h3><\/center>"}}