{"cell_type":{"165e2ef5":"code","406adf7d":"code","7844096f":"code","2c332fc3":"code","4f881b3e":"code","bf5029f0":"code","ef479af1":"code","256971dd":"code","b36ccfca":"code","b784790a":"code","0da44a1d":"code","5a38e905":"code","4d552807":"code","fec19676":"code","17a51ee9":"code","c10763a2":"code","f56e85d0":"code","b0c19d10":"code","7bf16bb9":"code","5b803fb3":"code","a775dba3":"code","20237b78":"code","78c16adc":"code","666731e8":"code","e6b1f4fe":"code","66427913":"code","fa0fa3bc":"code","9157e335":"code","aabe42b7":"code","6c85eb65":"code","e9ec30ef":"code","2c28309d":"code","4872f575":"code","1c8c9516":"code","55abfdd3":"code","27cfae74":"code","6d5449d1":"code","668b0d85":"code","78a0fd61":"code","e011d7b4":"code","644fd601":"code","637dd87b":"code","4891ecfa":"code","32d7f293":"code","1270ec52":"code","db397e5f":"code","d1fdb5aa":"code","2f0e1dc8":"code","fdc1829a":"code","58f4aee6":"code","8bac5477":"code","7dcb32f2":"code","b5e0a150":"code","9401a504":"code","6bae6677":"code","733aba2e":"code","d3299612":"code","53189940":"code","9ba61bd4":"code","c0c3252e":"code","44d02f69":"code","ccfb0325":"code","8eb41927":"code","0898017e":"code","5c3d1b20":"code","45acc021":"code","3472a1b1":"code","4155bd9b":"code","01ed6e0d":"code","39ccf711":"code","c4167bcc":"code","adf1ee2b":"code","f3f4e132":"code","bc70d34b":"code","62557fbb":"code","d26ff92b":"code","afb98510":"code","0e4eb9bd":"code","046e4a12":"code","1e41f53a":"code","a90c4097":"code","2ab9262d":"code","36b77388":"code","4a4d5fe6":"code","55b9b119":"code","00256bb1":"code","5bef2886":"code","fe6c1781":"code","461b3f9a":"code","efc44012":"code","a984427d":"code","3ae1780a":"code","7fd91623":"code","bf57dd29":"code","90acb835":"code","882c4374":"code","1e86a4db":"code","60c5ac9e":"code","fd01e9d3":"code","a3f5a999":"code","16fdba6f":"code","3da63df6":"code","05fc1a49":"code","1fb6aec5":"code","590d8524":"markdown","f5c50fe6":"markdown","e4fa001e":"markdown","67f5e046":"markdown","01bef2c6":"markdown","7db6662c":"markdown","871f2666":"markdown","a77cb902":"markdown","5ead9de9":"markdown","e162a8aa":"markdown","0de84c70":"markdown","152205c8":"markdown","529ef183":"markdown","30d7ffdf":"markdown","d79b879e":"markdown","181d5325":"markdown","726f7aee":"markdown","791b33e6":"markdown","a69dd697":"markdown","7813456e":"markdown","7859ba24":"markdown","2076903a":"markdown","d6ad07b4":"markdown","d85393e2":"markdown","d779f973":"markdown","b45c399f":"markdown","ae1fdcbf":"markdown","8902e471":"markdown","d85359e5":"markdown","9ffcc54b":"markdown","0a87c4f0":"markdown","79dc8107":"markdown","5ab750b5":"markdown","296eec94":"markdown","242bb7bd":"markdown","6d43cfd5":"markdown","29ac97c8":"markdown","022663f9":"markdown","12cbd411":"markdown","b96fda36":"markdown","915ba266":"markdown","460d3234":"markdown","2c071c84":"markdown","94813347":"markdown","5f7f9e2a":"markdown","4641acab":"markdown","802e07ad":"markdown","4f6da850":"markdown","0f5ae000":"markdown"},"source":{"165e2ef5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","406adf7d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(color_codes=True,style='darkgrid')","7844096f":"file = \"\/kaggle\/input\/vehicle1\/vehicle-1.csv\"\ndf = pd.read_csv(file)","2c332fc3":"df.head()","4f881b3e":"df.shape","bf5029f0":"df['class'].unique()","ef479af1":"df.info()","256971dd":"df.isna().sum()","b36ccfca":"df.describe(include='all').transpose()","b784790a":"for feature in df.select_dtypes(\"number\").columns:\n    sns.distplot(df[feature])\n    plt.title(f\"Distribution of {feature.capitalize()}\\n\")\n    plt.tight_layout()\n    plt.show()","0da44a1d":"for feature in df.select_dtypes(\"number\").columns:\n    sns.boxplot(df['class'],df[feature])\n    plt.tight_layout()\n    plt.show()","5a38e905":"df1 = df.replace(['van','car','bus'],[1,0,2])","4d552807":"df1.head()","fec19676":"sns.pairplot(df1,diag_kind='kde')","17a51ee9":"df.isna().sum().sum()","c10763a2":"null_data = df[df.isnull().any(axis=1)]\nnull_data","f56e85d0":"from sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=5,weights='distance')\ndf1 = imputer.fit_transform(df1)","b0c19d10":"cols = df.columns\ndf1 = pd.DataFrame(df1,columns=cols)","7bf16bb9":"df1.head()","5b803fb3":"df1.isna().sum()","a775dba3":"df1.skew()","20237b78":"df[df['max.length_aspect_ratio'] >= 20]","78c16adc":"df[df['pr.axis_aspect_ratio'] >= 82]","666731e8":"df1['pr.axis_aspect_ratio'] = np.log(df1['pr.axis_aspect_ratio'])","e6b1f4fe":"df1['pr.axis_aspect_ratio'].skew()","66427913":"df1['max.length_aspect_ratio'] = np.log(df1['max.length_aspect_ratio'])","fa0fa3bc":"df1['max.length_aspect_ratio'].skew()","9157e335":"df1['scaled_radius_of_gyration.1'] = np.log(df1['scaled_radius_of_gyration.1'])","aabe42b7":"df1['scaled_radius_of_gyration.1'].skew()","6c85eb65":"corr = df1.corr()\nplt.figure(figsize=(20,10))\nsns.heatmap(corr,annot=True)","e9ec30ef":"X = df1.drop(['class'],axis=1)\ny = df1['class']","2c28309d":"X.head()","4872f575":"from sklearn.ensemble import ExtraTreesClassifier","1c8c9516":"ensemble = ExtraTreesClassifier(random_state=0)","55abfdd3":"fit = ensemble.fit(X,y)","27cfae74":"imp = fit.feature_importances_","6d5449d1":"score = pd.DataFrame(X.columns,columns=['feature'])","668b0d85":"score['scores'] = imp","78a0fd61":"score.sort_values(by='scores',ascending=False)","e011d7b4":"X = X.drop(['scaled_radius_of_gyration','skewness_about','skewness_about.1'],axis=1)","644fd601":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)","637dd87b":"x_train.shape","4891ecfa":"x_test.shape","32d7f293":"y_test.value_counts()","1270ec52":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()","db397e5f":"xs_train = scaler.fit_transform(x_train)\nxs_test = scaler.fit_transform(x_test)\nXscaled = scaler.fit_transform(X)","d1fdb5aa":"from sklearn.svm import LinearSVC","2f0e1dc8":"lsvc = LinearSVC(C=1,max_iter=5000)","fdc1829a":"model1 = lsvc.fit(xs_train,y_train)","58f4aee6":"y_pred1 = model1.predict(xs_test)","8bac5477":"from sklearn.metrics import accuracy_score,f1_score,confusion_matrix","7dcb32f2":"acc = accuracy_score(y_test,y_pred1)\nacc","b5e0a150":"f1_score(y_test,y_pred1,average='weighted')","9401a504":"confusion_matrix(y_test,y_pred1)","6bae6677":"from sklearn.svm import SVC","733aba2e":"svc = SVC(kernel='rbf',random_state=0)","d3299612":"model2 = svc.fit(xs_train,y_train)","53189940":"y_pred2 = model2.predict(xs_test)","9ba61bd4":"rbf_acc = accuracy_score(y_test,y_pred2)\nrbf_acc","c0c3252e":"f1_score(y_test,y_pred2,average='macro')","44d02f69":"confusion_matrix(y_test,y_pred2)","ccfb0325":"from sklearn.model_selection import cross_val_score,KFold","8eb41927":"kf = KFold(n_splits=10,shuffle=True,random_state=1)","0898017e":"cv = cross_val_score(lsvc,Xscaled,y,cv=kf,scoring='accuracy',n_jobs=-1)","5c3d1b20":"cv","45acc021":"cv_acc = cv.mean()\ncv_acc","3472a1b1":"cv.std()","4155bd9b":"cv_ = cross_val_score(svc,Xscaled,y,cv=kf,scoring='accuracy',n_jobs=-1)","01ed6e0d":"cv_","39ccf711":"rbf_cv_acc = cv_.mean()\nrbf_cv_acc","c4167bcc":"cv_.std()","adf1ee2b":"from sklearn.decomposition import PCA","f3f4e132":"pca = PCA()","bc70d34b":"pca.fit(Xscaled)","62557fbb":"pca.explained_variance_","d26ff92b":"pca.explained_variance_ratio_","afb98510":"plt.figure(figsize=(10,5))\nplt.bar(list(range(1,16)),pca.explained_variance_ratio_)","0e4eb9bd":"plt.figure(figsize=(10,5))\nplt.step(list(range(1,16)),np.cumsum(pca.explained_variance_ratio_))","046e4a12":"reduced_pca = PCA(n_components=0.95)","1e41f53a":"reduced_pca.fit(Xscaled)","a90c4097":"reduced_pca.components_","2ab9262d":"reduced_pca.explained_variance_","36b77388":"reduced_pca.explained_variance_ratio_","4a4d5fe6":"Xpca = reduced_pca.transform(Xscaled)","55b9b119":"xp_train,xp_test,y_train,y_test = train_test_split(Xpca,y,test_size=0.3,random_state=42)","00256bb1":"model3 = LinearSVC(max_iter=5000).fit(xp_train,y_train)","5bef2886":"y_pred3 = model3.predict(xp_test)","fe6c1781":"pca_acc = accuracy_score(y_test,y_pred3)\npca_acc","461b3f9a":"f1_score(y_test,y_pred3,average='weighted')","efc44012":"confusion_matrix(y_test,y_pred3)","a984427d":"model4 = SVC().fit(xp_train,y_train)","3ae1780a":"y_pred4 = model4.predict(xp_test)","7fd91623":"pca_rbf_acc = accuracy_score(y_test,y_pred4)\npca_rbf_acc","bf57dd29":"confusion_matrix(y_test,y_pred4)","90acb835":"p_cv = cross_val_score(lsvc,Xpca,y,cv=kf,scoring='accuracy',n_jobs=-1)","882c4374":"p_cv","1e86a4db":"pca_cv_acc = p_cv.mean()\npca_cv_acc","60c5ac9e":"p_cv.std()","fd01e9d3":"p_cv_ = cross_val_score(svc,Xpca,y,cv=kf,scoring='accuracy',n_jobs=-1)","a3f5a999":"p_cv_","16fdba6f":"pca_rbf_cv_acc = p_cv_.mean()\npca_rbf_cv_acc","3da63df6":"p_cv_.std()","05fc1a49":"print(\"Accracy scores on raw Data\")\nprint(\"\")\nprint(\"Accuracy of linear support vector classifier:\",round(acc*100,2))\nprint(\"Accuracy of rbf kernel support vector calssifier:\",round(rbf_acc*100,2))\nprint(\"Mean Accuraccy of linear support Vector Classifier on Cross Validation:\",round(cv_acc*100,2))\nprint(\"Mean Accuracy of rbf kernel support vector Classifier on Cross Validation:\",round(rbf_cv_acc*100,2))","1fb6aec5":"print(\"Accracy scores on pca reduced Data\")\nprint(\"\")\nprint(\"Accuracy of linear support vector classifier:\",round(pca_acc*100,2))\nprint(\"Accuracy of rbf kernel support vector calssifier:\",round(pca_rbf_acc*100,2))\nprint(\"Mean Accuraccy of linear support Vector Classifier on Cross Validation:\",round(pca_cv_acc*100,2))\nprint(\"Mean Accuracy of rbf kernel support vector Classifier on Cross Validation:\",round(pca_rbf_cv_acc*100,2))","590d8524":"Converting class values into numericals.","f5c50fe6":"From the pairplot we can see that lot of the independent variables are highly correlated. And we see independent variables are not so much correlated with the target class. The variables like compactness,circularity,distance_circularity,radius_ratio,scatter_ratio,elongatedness,pr.axis_rectangularity,max.length_rectangularity,scaled_variance,scaled_variance.1 and scaled_radius_of_gyration have a high correaltion with each other which is not so helpful. Except elongatedness all are positively correlated, Elongatedness is negatively correalated.","e4fa001e":"### feature scaling","67f5e046":"### By observing the accuracy scores there is slight accuracy drop with PCA reduced data as it captured only 95% of varaition in data. In Both raw data and PCA reduced data campared to linear support vector classifier rbf kernel support vector classifier is performing better in both the cases. As rbf kernel trains model by transforming the data into higher dimensions it is performing better than linear model. As we can see from reduction of dimesnions from 15 in raw data to 5 dimensions on pca reduced data there is a drop in accuracy of linear suppot vector classifier model from 96% to 79% which is big jump of around 17% downwards. But with rbf kernel support vector classifier only 2.5% drop in accuracy from dimensionality reduction because rbf kernel trains model by transforming the data into higher dimensions. So we can say that our models are performing better on higher dimesnions. As we know all the features are geometric featues viewed from one of many different angles so with many angles we can from a accurate dimnesional space which helps in better understanding of data and can generalize well.","01bef2c6":"Dataset contains 3 types of class. We will classify them accordingly.","7db6662c":"## Linear Support Vector Classifier","871f2666":"Surprisingly, Our fourth model rbf kernel support vector classifier perform with 94% accuracy. Compared to previous rbf kernel model it has dropped only 1% accuracy. Out of 254 test samples only 14 records were wrongly calssified.","a77cb902":"### Missing values","5ead9de9":"From the maximum values of independent variables we can see that there are some extreme values in the attributes to be looked at.","e162a8aa":"We can observe that these missing values don't have pattern. These missing values are missing at random so we impute the missing values using KNNImputer.","0de84c70":"cross validation with rbf kernel support vector classifier on reduced data.","152205c8":"## Linear Support Vector Classsifier model","529ef183":"Variables like pr.axis_aspect_ratio,max.lenght_aspect_ratio and scaled_radius_of_gyration.1 are heavily skewed.","30d7ffdf":"So we select the principal components that explain only 95% of varaition in the data.","d79b879e":"## Kernel Support Vector Classifier model","181d5325":"We can observe the variance explained by each of the principal component from this plot. It seems only 8 to 10 principal components are explaining the most amount of variation in the data.","726f7aee":"Our first model linear support vector calssifier performs well with 93% accuracy. Out of 254 test samples only 18 samples were wrongly predicted.","791b33e6":"cross validation with linear support vector classifier.","a69dd697":"From the cross validation scores our pca linear support vector classifier performs with an accuracy range of 73% to 85% in production.","7813456e":"### Distribution plots of each attribute","7859ba24":"### Importing necessary libraries","2076903a":"We can see that only few of the independent variables have a good correlation wiht the target class. Many independent variables are highly correlated with each other. Some of them are circularity,max_lenth_rectangularity,scaled_radius_of_gyration,scatter_ratio,pr.axis_rectangularity,scaled_variance,elongatedness,scaled_variance.1 which cause redundacy in our model.","d6ad07b4":"Our second model rbf kernel Support vector classifier is performing greatly on test sample with 96.4% accuracy. Out of test 254 samples only 9 samples were wrongly classified.","d85393e2":"Cross Validation with rbf kernel support vector classifier.","d779f973":"Using Extratrees Classifier select only the important predictors for our model.","b45c399f":"There are some of the missing values inthe dataset. We will deal with them later.","ae1fdcbf":"Loading the file and having a look at it!","8902e471":"## Cross Validation","d85359e5":"We have 41 data points with missing values in them.","9ffcc54b":"We can see the distribution of median and range of classes for each variable. The wide variation of these classes on variable implies better the variable for our classification model. We can observe variables like compactness,distance_circularity,scatter_ratio,elongatedness,pr.axis_rectangularity,scaled_variance,scaled_varaince.1,hollow_ratio have a good variation in class distribution in the feature which helps for our prediction.","0a87c4f0":"From the kernel density estimations we can observe there are around two to three peaks in the distributions which shows the there are ranges of different class in the distribution. We can see few variables are normally distributed, most of them are skewed towards right tail and some of them are skewed towards left tail.\nWe can also observe that some of the variables like max.length_aspect_ratio, pr.axis_aspect_ratio and scaled_radius_of_gyration.1 have some outliers in them.","79dc8107":"### Split the Data","5ab750b5":"Out of 18 variables we drop 3 least important variables from the dataset so we left with 15 independent variables. By dropping these variables we reduce the redundancy and improve the performace of the model.","296eec94":"Splitting the reduced data using same random state so that the samples will not mix up differently. By this we can compare on scores on the same test records.","242bb7bd":"From looking at the extreme values it seems these values are reasonable so we are not dropping them instead we transform these varialbles to come close to normal distribution.","6d43cfd5":"Transforming the data with PCA using only 5 principal components.","29ac97c8":"from the cross validation with Linear support vector classifier we can say that our model will perform with accuracy range of 91% to 99% in production.","022663f9":"Dataset contains 846 records and 19 attributes. One of the Attribute is target valriable (class).","12cbd411":"## Cross Validation on Reduced Data","b96fda36":"From the cross validation scores we can say that our rbf kernel support vector classsifier performs with an accuracy range of 89% to 97% in production with 95% confidence.","915ba266":"Our third model linear support vector classifier performs with 75% accuracy on test data. From pca transformation the accuracy is dropped from 93% to 75%.","460d3234":"From the data info we can see that there are no catergorical variables to deal with except the target variable.","2c071c84":"## Dimensionality Reduction","94813347":"From cross validation with rbf kernel support vector classifier we can say that our model will perform with accuracy range of 93% to 97% in production.","5f7f9e2a":"We have seen that our variables are has some outliers and are skewed. Let's have a look at them.","4641acab":"cross validation with linear support vector classifier on reduced data.","802e07ad":"## Kernel Support Vector Classifier model","4f6da850":"We can see that only 5 principal components are explaining the 95% of variation the data.","0f5ae000":"### Feature Selection"}}