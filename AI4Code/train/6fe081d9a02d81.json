{"cell_type":{"68cd9d87":"code","962c79ec":"code","fea51f41":"code","295f6931":"code","130dc657":"code","087dbfdc":"code","335107af":"code","1fb7276a":"code","6b336e00":"code","d38ec674":"code","05ca0cb0":"code","a6f40582":"code","08ff4da7":"code","035f9550":"code","27d7e208":"code","58e16463":"code","7ab64eb4":"code","ba510795":"code","7df349e0":"code","22e2192a":"code","b8508081":"code","0dc5883c":"code","7fd6c6ca":"code","398da4e5":"code","7079b0b7":"code","68ab7121":"code","ae696fac":"code","a461a691":"code","5914a898":"code","2a0801d5":"code","6a7c7dcb":"code","2e769d1f":"code","881809d8":"code","23379743":"code","37745979":"code","276cbf28":"code","b0dc4b68":"code","f2cb46ad":"code","c2f5c1d5":"code","0b9ccf3c":"code","d1c480be":"code","5972efe9":"code","c96cbf19":"code","f51be159":"code","7ef72b5b":"code","9ca9122e":"code","9fdae8eb":"code","26741b55":"code","9782563c":"code","872c768c":"code","ff1f3efe":"code","9224fb37":"code","8d7f3f36":"code","8d4cee2d":"code","f8a7447c":"code","a9b77ef2":"code","4387d845":"code","aecf8bea":"code","6bcab88f":"code","d0629db5":"code","db7bac99":"code","b33c565a":"code","3144295d":"code","aee2283e":"code","6ff9ac86":"code","280b7c50":"code","cc3a024e":"code","4d020c3e":"code","0f36b193":"code","16505217":"code","5a6b72e5":"code","38af4e03":"code","aa058453":"code","6f71d613":"code","0679f7f6":"code","ea2631e2":"code","6e458c27":"code","62b8ade4":"code","66096be5":"code","d724a1d4":"code","f83efaec":"code","df02f3ba":"code","971933e4":"code","677f84a8":"code","79313752":"code","2ca1173f":"code","23112983":"code","c6acb232":"code","5a296944":"code","e2cd7ee8":"code","90e75023":"code","3746b257":"code","f2c6ad56":"code","d409ab22":"code","97908395":"code","854d2de4":"code","5f664b66":"code","27fa5e3e":"code","c1e1e0e6":"code","0d91f970":"code","5f92501b":"code","60fca193":"code","168033aa":"code","27b7ebe8":"code","1e760e1e":"code","6722127d":"code","a86eb1c5":"code","1292907a":"code","72de5af0":"code","d554af75":"code","d8ffd697":"code","4b04a65d":"code","038b8964":"code","11ca91ca":"code","c69d812d":"code","009d0ff0":"code","3c508769":"code","366ad43a":"code","2daf4ec7":"code","a257d5b2":"code","cab025f5":"code","ec6517a3":"code","d48800a6":"code","b2cbf76c":"code","d308faee":"code","0023ec6f":"code","53f840f8":"code","dcb6cb7b":"code","ea626f14":"code","dfdc2c78":"code","509f972a":"code","0642baa9":"code","99124ee7":"code","d31ec72d":"code","4501bb07":"code","4684521d":"code","270e2f3c":"code","50cac949":"code","2d79d3d5":"code","08721b71":"code","8bfd3a89":"code","caf67e74":"code","141097c5":"code","87516957":"code","16b914f1":"code","b8727277":"code","026bc547":"code","19714af2":"code","1a3ca625":"code","353c5ede":"code","8747ae8a":"code","88593895":"code","27d5992b":"code","4e982e1e":"code","3dede2a6":"code","cb257d12":"code","8d928633":"code","ed02a738":"code","a645f80f":"code","f355c79f":"code","1f3fd373":"code","963903e9":"code","1ce01f31":"code","25b11060":"code","8b5f4a8b":"code","8742b067":"code","a2d82922":"code","6ec4c7ca":"code","a148b2a7":"code","a5ee158a":"code","d261c2fc":"code","e0263668":"code","a97a4e52":"code","51986b8b":"code","5429722c":"code","7c04c69b":"code","53e0fc0c":"code","5e1d0939":"code","4390bb65":"code","83a79d74":"code","ab1d1366":"code","762e0116":"code","0b0697ea":"code","cfb2556d":"code","5bee0e3e":"code","544de5c3":"code","8a013891":"code","1c056804":"code","0c0670e4":"code","e9b43207":"code","202ca614":"code","28b0a76b":"code","73114a15":"code","2de026e9":"code","e7165543":"code","42826b77":"code","5e0de0c5":"code","641db251":"code","0dcf897b":"code","de288a9d":"code","8179794e":"code","3b9e176b":"code","f873410b":"code","74485d34":"code","b64e8c30":"code","9fb9d21d":"code","22287d98":"code","624c8848":"code","0bfb743b":"code","d4f33c46":"code","60f0b3c4":"code","1c4f9d62":"code","2869fe90":"code","70182097":"code","cbded65f":"code","2081830b":"code","3e00a124":"code","7b1bb53a":"code","413de141":"code","371d4e9f":"markdown","c2ee4b8c":"markdown","ac957297":"markdown","7c347f95":"markdown","34b1cc8f":"markdown","04d598bb":"markdown","bb48db08":"markdown","fe7f13d6":"markdown","6a7d7bbc":"markdown","c76ba935":"markdown","d7f236a8":"markdown","16d40ddc":"markdown","7132e735":"markdown","738336ce":"markdown","11f61fd2":"markdown","bef366ec":"markdown","41a55b61":"markdown","a12595ce":"markdown","2d458cdf":"markdown","a938f7e7":"markdown","9d116153":"markdown","5dda72c7":"markdown","9800fdfc":"markdown","0c2d9c8e":"markdown","50aae5d4":"markdown","f5653a70":"markdown","ff898bbc":"markdown","0ef1edf4":"markdown","65a10d75":"markdown","47b4ea6e":"markdown","89809de2":"markdown","f6c2fdd8":"markdown","fb509552":"markdown","e6f69727":"markdown","5d7f1d48":"markdown","0a008427":"markdown","ce700157":"markdown","03b04ab3":"markdown","9d96d388":"markdown","3d64e774":"markdown","1b032c94":"markdown","48090a5f":"markdown","3378dc0b":"markdown","9498d628":"markdown","bcfbbeed":"markdown","0c27f180":"markdown","d7b92c20":"markdown","00a568c6":"markdown","b610535f":"markdown","42551593":"markdown","b6e6418c":"markdown","696bdf14":"markdown","9da3cc10":"markdown","82636e45":"markdown","688f8a11":"markdown","3b9e9e03":"markdown","86c01206":"markdown","8f7aadec":"markdown","66d6f3d5":"markdown","1e5ea5e9":"markdown","4ea9cb27":"markdown","8cb32f0e":"markdown","91c48d49":"markdown","d6492a23":"markdown","cc118f56":"markdown","6ac7f897":"markdown","9a3fbc8c":"markdown","8f664661":"markdown","43958402":"markdown","05dbfa63":"markdown","c71933d6":"markdown","b06d1fd0":"markdown","f83275b8":"markdown","ca4de17a":"markdown","dd87656a":"markdown","77247c12":"markdown","51c37a13":"markdown","ea285072":"markdown","91bb04d3":"markdown","f4de77ae":"markdown","df5fb9ce":"markdown","9148da45":"markdown","1707bd03":"markdown","5ed65981":"markdown","a725810c":"markdown","b2033604":"markdown","4b657fc9":"markdown","9ccb3813":"markdown","f26aba89":"markdown","058a6439":"markdown","2ea20cf6":"markdown","aecf577a":"markdown","407395d0":"markdown","3bdc776b":"markdown","f3b73de1":"markdown","55deef55":"markdown","dab7b3fc":"markdown","f07fcec6":"markdown","d109a712":"markdown","c2195b4b":"markdown","9479b1e4":"markdown","4dc1bc0b":"markdown","b3e93fcf":"markdown","5c5deb2b":"markdown","c610b2e6":"markdown","8f9fadee":"markdown","46383f11":"markdown","baa84c9e":"markdown","ab73fc97":"markdown","0f244729":"markdown","14dba7f6":"markdown","dde5a464":"markdown","9d9f113c":"markdown","f25e249f":"markdown","3381997a":"markdown","367a309f":"markdown","b8937ca8":"markdown","8bfab8d5":"markdown","9afc580b":"markdown","1afa6076":"markdown","12f17407":"markdown","d0f2bb15":"markdown","63795b2b":"markdown","55214cef":"markdown","7471b1be":"markdown","46a025bc":"markdown","9ae3f6e3":"markdown","4ac0db71":"markdown","8ca69c4f":"markdown","86a0bbad":"markdown","c4adb430":"markdown","64440c56":"markdown","310c0c0a":"markdown","86efc016":"markdown","f94df6a1":"markdown","4c14e314":"markdown","d470c55a":"markdown","2d90f180":"markdown","5ee33816":"markdown","ca7dfb3d":"markdown","6f749fd0":"markdown","b6b4ad10":"markdown","fd399a4b":"markdown","55ead2f0":"markdown","674ebac4":"markdown","ed06d779":"markdown","6b38b124":"markdown","3d3f574d":"markdown","5b959a3d":"markdown","7254f1f3":"markdown","da853b1d":"markdown","3d2d6fc3":"markdown","e1dc9917":"markdown","afb3e288":"markdown","a5ea808c":"markdown","7b666a76":"markdown","046a6d08":"markdown","693c7dde":"markdown","d6707beb":"markdown","4333599b":"markdown","40f635f9":"markdown","81964bcd":"markdown","2c8014e3":"markdown","82fab889":"markdown","6f7edf33":"markdown","f0d999c8":"markdown","efa01e15":"markdown","f799e297":"markdown","2afac390":"markdown","37942eb7":"markdown","4bf2015e":"markdown","d479c6b1":"markdown","ba0014fe":"markdown","5657baf8":"markdown","a416a57a":"markdown","83badc70":"markdown","c1393ce1":"markdown","f115763c":"markdown","a6f1b6a0":"markdown","c42f6147":"markdown","8b848043":"markdown","942839a1":"markdown","74920cbd":"markdown","d7e9675d":"markdown","7962cd36":"markdown","4fc336cf":"markdown","603038aa":"markdown","08b035f2":"markdown","3bd69e94":"markdown","a38aebd2":"markdown","9646ea39":"markdown","895d4fb4":"markdown","6c952d74":"markdown","921f89e6":"markdown","c3639eaf":"markdown","b1755528":"markdown","e5b631f3":"markdown","924dc885":"markdown","afa4ab52":"markdown","5b90f87a":"markdown","79688a7f":"markdown","2e39782d":"markdown","20591aa4":"markdown","f73c2692":"markdown","98d78142":"markdown","b64eeefa":"markdown","c5b4fa19":"markdown","37e04683":"markdown","4b69701c":"markdown","0b454726":"markdown","1309466c":"markdown","f74548d6":"markdown","95686df3":"markdown","a800e9d4":"markdown","91471a0c":"markdown","09220f80":"markdown","fa6046b2":"markdown","fcae31b2":"markdown","d8197290":"markdown","9f398276":"markdown","ce1a0fdc":"markdown","aebf5230":"markdown","496286a7":"markdown","41d3f591":"markdown","d96f5e51":"markdown","6c2a61f9":"markdown","fa9c2bed":"markdown","aecfe7bd":"markdown","bfdf5d19":"markdown","9a278f19":"markdown","d273c5d1":"markdown","2f7b1d82":"markdown","f01ade5e":"markdown","c3a0f48d":"markdown","8d33b839":"markdown","7a9ded06":"markdown","7923cbd8":"markdown","2eb6a59a":"markdown","1417fb6d":"markdown","ae8ecb46":"markdown","25b33321":"markdown","94df4eb7":"markdown","214ae8e7":"markdown","b557700f":"markdown","4b8c333b":"markdown","04aee44f":"markdown","4e3c20b8":"markdown","07c00fec":"markdown","c5fe6a4a":"markdown","32bd7fac":"markdown","bf40a371":"markdown","51920753":"markdown","90a985bc":"markdown","6d29968f":"markdown","0fb35dca":"markdown","b3665db1":"markdown","f8db9d77":"markdown","d67689f6":"markdown","a4d88d3b":"markdown","f3e20908":"markdown","4ecdf74e":"markdown","24ada868":"markdown","54e5d44d":"markdown","c0106634":"markdown","72f9956f":"markdown","9872e6fe":"markdown","55b27439":"markdown","86a524e9":"markdown","898a2c03":"markdown","adba4e4b":"markdown","ca927552":"markdown","b4c6caaf":"markdown","65662099":"markdown","25fef126":"markdown","1e671368":"markdown","49cef70a":"markdown","3e6cd44f":"markdown","79fa380d":"markdown","1bf7fe61":"markdown","6eacbb82":"markdown","43677340":"markdown","8b07fed2":"markdown","9998d9bf":"markdown","856d50cc":"markdown","615fa8e1":"markdown","9c4d17e4":"markdown","6d02df63":"markdown","4231900d":"markdown","d94dd08c":"markdown","135b7541":"markdown","dffad861":"markdown","d728c410":"markdown","91b03541":"markdown","e34298e1":"markdown","87c33435":"markdown","f47ed55e":"markdown","7ed851b2":"markdown","f8b57248":"markdown"},"source":{"68cd9d87":"# Library to suppress warnings or deprecation notes \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Libraries to help with reading and manipulating data\nimport numpy as np\nimport pandas as pd\n\n# Libraries to help with data visualization\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n%matplotlib inline\n\nimport seaborn as sns\n\n# Libraries for Imputation, Scaling, OneHotEncoding and Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\n\n# Libraries to split data \nfrom sklearn.model_selection import train_test_split\n\n# Libraries to import decision tree classifier and different ensemble classifiers\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\n# Libraries to tune model, get different metric scores\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\n\n# Pandas dataframe options\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.max_rows', 300)\npd.set_option('display.max_colwidth',400)\n\n\n # set the background for the graphs\nplt.style.use('ggplot')\n\n# For pandas profiling\nfrom pandas_profiling import ProfileReport\n\n# Printing style\n!pip install tabulate\nfrom tabulate import tabulate\n","962c79ec":"tourism = pd.read_csv('..\/input\/tourpackageprediction\/tour_package.csv')","fea51f41":"# copying data to another varaible to avoid any changes to original data\ndata = tourism.copy()","295f6931":"data.head()","130dc657":"data.tail()","087dbfdc":"data.shape","335107af":"data.info()","1fb7276a":"df_null_summary = pd.concat([data.isnull().sum(), data.isnull().sum() * 100 \/data.isnull().count()], axis = 1)\ndf_null_summary.columns = ['Null Record Count', 'Percentage of Null Records']","6b336e00":"df_null_summary.sort_values(by='Percentage of Null Records', ascending=False).style.background_gradient(cmap ='YlOrRd')","d38ec674":"data.describe(include='all').T","05ca0cb0":"for cat_cols in data.select_dtypes(exclude=[np.int64, np.float64]).columns.unique().to_list():\n    print('Unique values and corresponding data counts for feature: '+cat_cols)\n    print('-'*90)\n    df_temp = pd.concat([data[cat_cols].value_counts(), data[cat_cols].value_counts(normalize=True)*100], axis=1)\n    df_temp.columns = ['Count', 'Percentage']\n    print(df_temp)\n    print('-'*90)","a6f40582":"data.drop(columns=['CustomerID'], inplace=True)","08ff4da7":"data.Gender.value_counts()","035f9550":"data['Gender'] = data['Gender'].str.replace(' ', '').str.capitalize()","27d7e208":"data.Gender.value_counts()","58e16463":"category_columns = ['ProdTaken', 'TypeofContact', 'CityTier', 'Occupation', 'Gender', 'ProductPitched' \\\n                   , 'MaritalStatus', 'Passport', 'OwnCar', 'Designation']","7ab64eb4":"data[category_columns] = data[category_columns].astype('category')","ba510795":"data.columns = [i.replace(\" \", \"_\").lower() for i in data.columns]","7df349e0":"data.info()","22e2192a":"data[['age', 'monthlyincome']].describe().T","b8508081":"# Creating categories from Age and Monthly Income to analyze the trend of borrowing Personal Loan\n\ndata['age_bin'] = pd.cut(\n    x=data['age'],\n    bins=[18, 30, 40, 50, 61],\n    labels=[\"18-30\", \"31-40\", \"41-50\", \"50+\"],\n)\n\ndata['income_bin'] = pd.cut(\n    x=data['monthlyincome'],\n    bins=[1000, 10000, 25000, 50000, 80000, 100000],\n    labels=[\"1K - 10K\", \"10K+ - 25K\", \"25K+ - 50K\", \"50K+ - 80K\", \"80K+\"],\n)","0dc5883c":"def summary(x):\n    '''\n    The function prints the 5 point summary and histogram, box plot, \n    violin plot, and cumulative density distribution plots for each \n    feature name passed as the argument.\n    \n    Parameters:\n    ----------\n    \n    x: str, feature name\n    \n    Usage:\n    ------------\n    \n    summary('age')\n    '''\n    x_min = data[x].min()\n    x_max = data[x].max()\n    Q1 = data[x].quantile(0.25)\n    Q2 = data[x].quantile(0.50)\n    Q3 = data[x].quantile(0.75)\n    \n    dict={'Min': x_min, 'Q1': Q1, 'Q2': Q2, 'Q3': Q3, 'Max': x_max}\n    df = pd.DataFrame(data=dict, index=['Value'])\n    print(f'5 Point Summary of {x.capitalize()} Attribute:\\n')\n    print(tabulate(df, headers = 'keys', tablefmt = 'psql'))\n\n    fig = plt.figure(figsize=(16, 8))\n    plt.subplots_adjust(hspace = 0.6)\n    sns.set_palette('Pastel1')\n    \n    plt.subplot(221, frameon=True)\n    ax1 = sns.distplot(data[x], color = 'purple')\n    ax1.axvline(\n        np.mean(data[x]), color=\"purple\", linestyle=\"--\"\n    )  # Add mean to the histogram\n    ax1.axvline(\n        np.median(data[x]), color=\"black\", linestyle=\"-\"\n    )  # Add median to the histogram\n    plt.title(f'{x.capitalize()} Density Distribution')\n    \n    plt.subplot(222, frameon=True)\n    ax2 = sns.violinplot(x = data[x], palette = 'Accent', split = True)\n    plt.title(f'{x.capitalize()} Violinplot')\n    \n    plt.subplot(223, frameon=True, sharex=ax1)\n    ax3 = sns.boxplot(x=data[x], palette = 'cool', width=0.7, linewidth=0.6, showmeans=True)\n    plt.title(f'{x.capitalize()} Boxplot')\n    \n    plt.subplot(224, frameon=True, sharex=ax2)\n    ax4 = sns.kdeplot(data[x], cumulative=True)\n    plt.title(f'{x.capitalize()} Cumulative Density Distribution')\n    \n    plt.show()","7fd6c6ca":"summary('age')","398da4e5":"summary('durationofpitch')","7079b0b7":"summary('numberofpersonvisiting')","68ab7121":"summary('numberoffollowups')","ae696fac":"summary('preferredpropertystar')","a461a691":"summary('numberoftrips')","5914a898":"summary('pitchsatisfactionscore')","2a0801d5":"summary('numberofchildrenvisiting')","6a7c7dcb":"summary('monthlyincome')","2e769d1f":"# Below code plots grouped bar for each categorical feature\n\ndef perc_on_bar(data: pd.DataFrame, cat_columns, target, hue=None, perc=True):\n    '''\n    The function takes a category column as input and plots bar chart with percentages on top of each bar\n    \n    Usage:\n    ------\n    \n    perc_on_bar(df, ['age'], 'prodtaken')\n    '''\n    \n    subplot_cols = 2\n    subplot_rows = int(len(cat_columns)\/2 + 1)\n    plt.figure(figsize=(16,5*subplot_rows))\n    for i, col in enumerate(cat_columns):\n        plt.subplot(subplot_rows,subplot_cols,i+1)\n        order = data[col].value_counts(ascending=False).index  # Data order  \n        ax=sns.countplot(data=data, x=col, palette = 'crest', order=order, hue=hue);\n        for p in ax.patches:\n            percentage = '{:.1f}%\\n({})'.format(100 * p.get_height()\/len(data[target]), p.get_height())\n            # Added percentage and actual value\n            x = p.get_x() + p.get_width() \/ 2\n            y = p.get_y() + p.get_height() + 40\n            if perc:\n                plt.annotate(percentage, (x, y), ha='center', color='black', fontsize='medium'); # Annotation on top of bars\n            plt.xticks(color='black', fontsize='medium', rotation= (-90 if col=='region' else 0));\n            plt.tight_layout()\n            plt.title(col.capitalize() + ' Percentage Bar Charts\\n\\n')\n","881809d8":"category_columns = data.select_dtypes(include='category').columns.tolist()\ntarget_variable = 'prodtaken'\nperc_on_bar(data, category_columns, target_variable)","23379743":"# Below code plots box charts for each numerical feature by each type of Personal Loan (0: Not Borrowed, 1: Borroed)\ndef box_by_target(numeric_columns, target, include_outliers):\n    '''\n    The function takes a category column, target column, and whether to include outliers or not as input \n    and plots bar chart with percentages on top of each bar\n    \n    Usage:\n    ------\n    \n    perc_on_bar(['age'], 'prodtaken', True)\n    '''\n    subplot_cols = 2\n    subplot_rows = int(len(numeric_columns)\/2 + 1)\n    plt.figure(figsize=(16,8*subplot_rows))\n    for i, col in enumerate(numeric_columns):\n        plt.subplot(8,2,i+1)\n        sns.boxplot(data=data, x=target, y=col, orient='vertical', palette=\"Blues\", showfliers=include_outliers)\n        plt.xticks(ticks=[0,1], labels=['Not Opted (0)', 'Opted (1)'])\n        plt.tight_layout()\n        plt.title(str(i+1)+ ': '+ target +' vs. ' + col, color='black')","37745979":"numeric_columns = data.select_dtypes(exclude='category').columns.tolist()\ntarget_variable = 'prodtaken'\nbox_by_target(numeric_columns, target_variable, True)","276cbf28":"box_by_target(numeric_columns, target_variable, False)","b0dc4b68":"# Create a function that returns a Pie chart and a Bar Graph for the categorical variables:\ndef cat_view(x, target):\n    \"\"\"\n    Function to create a Bar chart and a Pie chart for categorical variables.\n    \"\"\"\n    from matplotlib import cm\n    color1 = cm.inferno(np.linspace(.4, .8, 30))\n    color2 = cm.viridis(np.linspace(.4, .8, 30))\n    sns.set_palette('cubehelix')\n    fig, ax = plt.subplots(1, 2, figsize=(16, 4))\n    \n     \n    \"\"\"\n    Draw a Pie Chart on first subplot.\n    \"\"\"    \n    s = data.groupby(x).size()\n\n    mydata_values = s.values.tolist()\n    mydata_index = s.index.tolist()\n\n    def func(pct, allvals):\n        absolute = int(pct\/100.*np.sum(allvals))\n        return \"{:.1f}%\\n({:d})\".format(pct, absolute)\n\n\n    wedges, texts, autotexts = ax[0].pie(mydata_values, autopct=lambda pct: func(pct, mydata_values),\n                                      textprops=dict(color=\"w\"))\n\n    ax[0].legend(wedges, mydata_index,\n              title=x.capitalize(),\n              loc=\"center left\",\n              bbox_to_anchor=(1, 0, 0.5, 1))\n\n    plt.setp(autotexts, size=12)\n\n    ax[0].set_title(f'{x.capitalize()} Pie Chart')\n    \n    \"\"\"\n    Draw a Bar Graph on second subplot.\n    \"\"\"\n    \n    df = pd.pivot_table(data, index = [x], columns = [target], values = ['monthlyincome'], aggfunc = len)\n\n    labels = df.index.tolist()\n    loan_no = df.values[:, 0].tolist()\n    loan_yes = df.values[:, 1].tolist()\n    \n    l = np.arange(len(labels))  # the label locations\n    width = 0.35  # the width of the bars\n\n    rects1 = ax[1].bar(l - width\/2, loan_no, width, label='Not Opted', color = color1)\n    rects2 = ax[1].bar(l + width\/2, loan_yes, width, label='Opted', color = color2)\n\n    # Add some text for labels, title and custom x-axis tick labels, etc.\n    ax[1].set_ylabel('Scores')\n    ax[1].set_title(f'{x.capitalize()} Bar Graph')\n    ax[1].set_xticks(l)\n    ax[1].set_xticklabels(labels)\n    ax[1].legend()\n    \n    def autolabel(rects):\n        \n        \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n        \n        for rect in rects:\n            height = rect.get_height()\n            ax[1].annotate('{}'.format(height),\n                        xy=(rect.get_x() + rect.get_width() \/ 2, height),\n                        xytext=(0, 3),  # 3 points vertical offset\n                        textcoords=\"offset points\",\n                        fontsize = 'medium',   \n                        ha='center', va='bottom')\n\n\n    autolabel(rects1)\n    autolabel(rects2)\n\n    fig.tight_layout()\n    plt.show()\n    \n    \"\"\"\n    Draw a Stacked Bar Graph on bottom.\n    \"\"\"\n    \n    sns.set(palette=\"tab10\")\n    tab = pd.crosstab(data[x], data[target].map({0:'Not Opted', 1:'Opted'}), normalize=\"index\")\n    \n    tab.plot.bar(stacked=True, figsize=(16, 3))\n    plt.title(x.capitalize() + ' Stacked Bar Plot')\n    plt.legend(loc=\"upper right\", bbox_to_anchor=(0,1))\n    plt.show()","f2cb46ad":"cat_view('typeofcontact', 'prodtaken')","c2f5c1d5":"cat_view('citytier', 'prodtaken')","0b9ccf3c":"cat_view('occupation', 'prodtaken')","d1c480be":"cat_view('gender', 'prodtaken')","5972efe9":"cat_view('productpitched', 'prodtaken')","c96cbf19":"cat_view('maritalstatus', 'prodtaken')","f51be159":"cat_view('passport', 'prodtaken')","7ef72b5b":"cat_view('owncar', 'prodtaken')","9ca9122e":"cat_view('designation', 'prodtaken')","9fdae8eb":"cat_view('age_bin', 'prodtaken')","26741b55":"cat_view('income_bin', 'prodtaken')","9782563c":"# Below plot shows correlations between the numerical features in the dataset\n\nplt.figure(figsize=(20,20));\nsns.set(palette=\"nipy_spectral\");\nsns.pairplot(data=data, hue='prodtaken', corner=True);","872c768c":"# Plotting correlation heatmap of the features\n\nsns.set(rc={\"figure.figsize\": (15, 15)})\nsns.heatmap(\n    data.corr(),\n    annot=True,\n    linewidths=0.5,\n    center=0,\n    cbar=False,\n    cmap=\"YlGnBu\",\n    fmt=\"0.2f\",\n)\nplt.show()\n","ff1f3efe":"# Function to plot numerical feature by each category with target hue\n\ndef plot_numeric_by_cat(category_columns: list, numeric_column: str, hue: str):\n    '''\n    The function plots a numerical feature in box plot by every category column specified in the list, \n    with hue of a target category\n    '''\n    num_cols = 2\n    num_rows = int(len(category_columns) \/2 + 1)\n\n    plt.figure(figsize=(15, 8*num_rows))\n    for i, col in enumerate(category_columns):\n        plt.subplot(num_rows, num_cols, i+1)\n        sns.set(palette=\"nipy_spectral\");\n        sns.boxplot(data=data, x=col, y=numeric_column, hue=hue, showfliers=True).set(title = numeric_column + ' vs. ' + col + ' by ' + hue);","9224fb37":"category_columns = data.select_dtypes(include='category').columns.tolist()\ncategory_columns.remove('prodtaken')\ncategory_columns.remove('income_bin')\n\nplot_numeric_by_cat(category_columns, 'monthlyincome', 'prodtaken')","8d7f3f36":"category_columns = data.select_dtypes(include='category').columns.tolist()\ncategory_columns.remove('prodtaken')\ncategory_columns.remove('age_bin')\n\n\nplot_numeric_by_cat(category_columns, 'age', 'prodtaken')","8d4cee2d":"category_columns = data.select_dtypes(include='category').columns.tolist()\ncategory_columns.remove('prodtaken')\n\n\nplot_numeric_by_cat(category_columns, 'numberoffollowups', 'prodtaken')","f8a7447c":"category_columns = data.select_dtypes(include='category').columns.tolist()\ncategory_columns.remove('prodtaken')\n\n\nplot_numeric_by_cat(category_columns, 'preferredpropertystar', 'prodtaken')","a9b77ef2":"# Creating a dataset out of the primary dataset for only the customers who bought travel products\n\ndf_cust_prodtaken = data[data['prodtaken'] == 1]","4387d845":"category_columns = data.select_dtypes(include='category').columns.tolist()\ntarget_variable = 'productpitched'\nperc_on_bar(df_cust_prodtaken, category_columns, 'prodtaken', hue=target_variable, perc=False)","aecf8bea":"plt.figure(figsize=(15,5))\nsns.barplot(y='monthlyincome',x='productpitched',data=df_cust_prodtaken).set_title('Monthly Income vs Product Pitched');","6bcab88f":"plt.figure(figsize=(15,5))\nsns.barplot(y='age',x='productpitched',data=df_cust_prodtaken).set_title('Monthly Income vs Product Pitched');","d0629db5":"df_cust_prodtaken.groupby(['productpitched']).agg({'monthlyincome':{'mean','min','max'},'age':{'mean','min','max'}})","db7bac99":"df_cust_prodtaken[df_cust_prodtaken['productpitched']=='Basic'].describe(include='all').T","b33c565a":"df_cust_prodtaken[df_cust_prodtaken['productpitched']=='Standard'].describe(include='all').T","3144295d":"df_cust_prodtaken[df_cust_prodtaken['productpitched']=='Deluxe'].describe(include='all').T","aee2283e":"df_cust_prodtaken[df_cust_prodtaken['productpitched']=='Super Deluxe'].describe(include='all').T","6ff9ac86":"df_cust_prodtaken[df_cust_prodtaken['productpitched']=='King'].describe(include='all').T","280b7c50":"# First building a copy dataset so that if required we can re-visit and re-execute some parts of the EDA \n# without running the whole notebook \n\ndf = data.copy()\n\n# Dropping unnecessary columns from the df dataframe, which we'll use going forward\n\ndf.drop(columns=['durationofpitch', 'numberoffollowups', 'productpitched', 'pitchsatisfactionscore', 'typeofcontact'\n                , 'income_bin', 'age_bin'], inplace=True)","cc3a024e":"X = df.drop(['prodtaken'], axis=1)\ny = df['prodtaken']\n\n#Splitting data in train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state = 1, stratify=y)\n\n\nprint('Shape of Training set : ',X_train.shape )\nprint('Shape of test set : ',X_test.shape )\nprint('Percentage of classes in training set:\\n',y_train.value_counts(normalize=True)*100)\nprint('Percentage of classes in test set:\\n',y_test.value_counts(normalize=True)*100)","4d020c3e":"numerical_col = X_train.select_dtypes(exclude='category').columns.tolist()\nplt.figure(figsize=(20,5))\n\nfor i, variable in enumerate(numerical_col):\n    plt.subplot(1,6,i+1)\n    sns.boxplot(y=X_train[variable], orient='h')\n    plt.tight_layout()\n    plt.title(variable)\n\nplt.show()","0f36b193":"X_train[(X_train['monthlyincome'] > 40000) | (X_train['monthlyincome'] < 10000)]","16505217":"X_train[(X_train['numberoftrips'] > 10)]","5a6b72e5":"#Dropping observation with monthly income less than 10000 or greater than 40000. There are just 3 such observations\n# idx = X_train[(X_train.monthlyincome>40000) | (X_train.monthlyincome<10000)].index\n# X_train.drop(index=idx,inplace=True)\n# y_train.drop(index=idx, inplace=True)\n# \n# Dropping observations with number of trips greater than 10. There are just 3 such observations\n# idx = X_train[X_train.numberoftrips>10].index\n# X_train.drop(index=idx,inplace=True)\n# y_train.drop(index=idx, inplace=True)","38af4e03":"def treat_outliers(data,col):\n    '''\n    treats outliers in a varaible\n    col: str, name of the numerical varaible\n    data: data frame\n    col: name of the column\n    '''\n    Q1=data[col].quantile(0.25) # 25th quantile\n    Q3=data[col].quantile(0.75)  # 75th quantile\n    IQR=Q3-Q1\n    Lower_Whisker = Q1 - 1.5*IQR \n    Upper_Whisker = Q3 + 1.5*IQR\n    data[col] = np.clip(data[col], Lower_Whisker, Upper_Whisker) \n    # all the values smaller than Lower_Whisker will be assigned value of Lower_whisker                                                         \n    # and all the values above upper_whisker will be assigned value of upper_Whisker \n    return data\n\ndef treat_outliers_all(data, col_list):\n    '''\n    treat outlier in all numerical varaibles\n    col_list: list of numerical varaibles\n    data: data frame\n    '''\n    for c in col_list:\n        data = treat_outliers(data,c)\n        \n    return data","aa058453":"outlier_treatment_col = ['monthlyincome', 'numberoftrips']\n\n\n\n# treating outliers\nX_train = treat_outliers_all(X_train, outlier_treatment_col)","6f71d613":"numerical_col = X_train.select_dtypes(exclude='category').columns.tolist()\nplt.figure(figsize=(20,5))\n\nfor i, variable in enumerate(numerical_col):\n    plt.subplot(1,6,i+1)\n    sns.boxplot(y=X_train[variable], orient='h')\n    plt.tight_layout()\n    plt.title(variable)\n\nplt.show()","0679f7f6":"X_train.isnull().sum()","ea2631e2":"X_test.isnull().sum()","6e458c27":"X_train.groupby(['designation', 'occupation', 'gender', 'maritalstatus'])\\\n[['preferredpropertystar', 'numberoftrips', 'age']].mean()","62b8ade4":"X_test.groupby(['designation', 'occupation', 'gender', 'maritalstatus'])\\\n[['preferredpropertystar', 'numberoftrips', 'age']].mean()","66096be5":"# Using the above mentioned grouping strategies and assumptions to find the medians and fill in the missing values\n# in both training and testing datasets\n\nfor dataset in [X_train, X_test]:\n    dataset['preferredpropertystar']=dataset.groupby(['designation', 'occupation', 'gender', 'maritalstatus'])\\\n    ['preferredpropertystar'].apply(lambda x:x.fillna(x.median()))\n    \n    dataset['monthlyincome']=dataset.groupby(['designation', 'occupation', 'gender', 'citytier'])\\\n    ['monthlyincome'].apply(lambda x:x.fillna(x.median()))\n    \n    dataset['numberoftrips']=dataset.groupby(['designation', 'occupation', 'gender', 'maritalstatus'])\\\n    ['numberoftrips'].apply(lambda x:x.fillna(x.median()))\n    \n    dataset['age']=dataset.groupby(['designation', 'occupation', 'gender', 'maritalstatus'])['age']\\\n    .apply(lambda x:x.fillna(x.median()))\n    \n    dataset.numberofchildrenvisiting.fillna(0, inplace=True)","d724a1d4":"X_train.isnull().sum()","f83efaec":"X_test.isnull().sum()","df02f3ba":"X_test[X_test['numberoftrips'].isnull()]","971933e4":"X_test['numberoftrips']=dataset.groupby(['designation', 'gender', 'maritalstatus'])['numberoftrips']\\\n.apply(lambda x:x.fillna(x.median()))","677f84a8":"X_test.isnull().sum()","79313752":"X_train = pd.get_dummies(X_train, drop_first=True)\nX_test = pd.get_dummies(X_test, drop_first=True)","2ca1173f":"def get_metrics_score(model,train,test,train_y,test_y,threshold=0.5,flag=True,roc=False):\n    '''\n    Function to calculate different metric scores of the model - Accuracy, Recall, Precision, and F1 score\n    model: classifier to predict values of X\n    train, test: Independent features\n    train_y,test_y: Dependent variable\n    threshold: thresold for classifiying the observation as 1\n    flag: If the flag is set to True then only the print statements showing different will be displayed. The default value is set to True.\n    roc: If the roc is set to True then only roc score will be displayed. The default value is set to False.\n    '''\n    # defining an empty list to store train and test results\n    \n    score_list=[] \n    \n    pred_train = (model.predict_proba(train)[:,1]>threshold)\n    pred_test = (model.predict_proba(test)[:,1]>threshold)\n\n    pred_train = np.round(pred_train)\n    pred_test = np.round(pred_test)\n    \n    train_acc = accuracy_score(pred_train,train_y)\n    test_acc = accuracy_score(pred_test,test_y)\n    \n    train_recall = recall_score(train_y,pred_train)\n    test_recall = recall_score(test_y,pred_test)\n    \n    train_precision = precision_score(train_y,pred_train)\n    test_precision = precision_score(test_y,pred_test)\n    \n    train_f1 = f1_score(train_y,pred_train)\n    test_f1 = f1_score(test_y,pred_test)\n    \n    \n    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision,train_f1,test_f1,pred_train,pred_test))\n        \n    \n    if flag == True: \n        print(\"Accuracy on training set : \",accuracy_score(pred_train,train_y))\n        print(\"Accuracy on test set : \",accuracy_score(pred_test,test_y))\n        print(\"Recall on training set : \",recall_score(train_y,pred_train))\n        print(\"Recall on test set : \",recall_score(test_y,pred_test))\n        print(\"Precision on training set : \",precision_score(train_y,pred_train))\n        print(\"Precision on test set : \",precision_score(test_y,pred_test))\n        print(\"F1 on training set : \",f1_score(train_y,pred_train))\n        print(\"F1 on test set : \",f1_score(test_y,pred_test))\n   \n    if roc == True:\n        pred_train_prob = model.predict_proba(train)[:,1]\n        pred_test_prob = model.predict_proba(test)[:,1]\n        print(\"ROC-AUC Score on training set : \",roc_auc_score(train_y,pred_train))\n        print(\"ROC-AUC Score on test set : \",roc_auc_score(test_y,pred_test))\n    \n    return score_list # returning the list with train and test scores","23112983":"def make_confusion_matrix(model,test_X,y_actual,i,seg,labels=[1, 0]):\n    '''\n    model : classifier to predict values of X\n    test_X: test set\n    y_actual : ground truth  \n    \n    '''\n    y_predict = model.predict(test_X)\n    cm=metrics.confusion_matrix( y_actual, y_predict, labels=[1,0])\n    df_cm = pd.DataFrame(cm, index = [i for i in ['Actual - Purchased', 'Actual - Not Purchased']],\n                  columns = [i for i in ['Predicted - Purchased','Predicted - Not Purchased']])\n    group_counts = [\"{0:0.0f}\".format(value) for value in\n                cm.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in\n                         cm.flatten()\/np.sum(cm)]\n    labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n              zip(group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    plt.figure(figsize = (10,7))\n    sns.heatmap(df_cm, annot=labels,fmt='', ax=axes[i], cmap='Blues').set(title='Confusion Matrix of {} Set'.format(seg))","c6acb232":"# # defining empty lists to add train and test results\nacc_train = []\nacc_test = []\nrecall_train = []\nrecall_test = []\nprecision_train = []\nprecision_test = []\nf1_train = []\nf1_test = []\n\ndef add_score_model(score):\n    '''Add scores to list so that we can compare all models score together'''   \n    acc_train.append(score[0])\n    acc_test.append(score[1])\n    recall_train.append(score[2])\n    recall_test.append(score[3])\n    precision_train.append(score[4])\n    precision_test.append(score[5])\n    f1_train.append(score[6])\n    f1_test.append(score[7])","5a296944":"dtree=DecisionTreeClassifier(random_state=1)\ndtree.fit(X_train,y_train)","e2cd7ee8":"dtree_score=get_metrics_score(dtree,X_train,X_test,y_train,y_test)\nadd_score_model(dtree_score)","90e75023":"fig, axes = plt.subplots(1,2,figsize=(16,5));\n\nmake_confusion_matrix(dtree, X_train, y_train, i=0, seg='Training')\nmake_confusion_matrix(dtree, X_test, y_test, i=1, seg='Testing')","3746b257":"feature_names = list(X_train.columns)\nprint(feature_names)","f2c6ad56":"plt.figure(figsize=(20,30))\nout = tree.plot_tree(dtree,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)\nfor o in out:\n     arrow = o.arrow_patch\n     if arrow is not None:\n        arrow.set_edgecolor('black')\n        arrow.set_linewidth(1)\nplt.show()\nplt.show()","d409ab22":"# importance of features in the tree building ( The importance of a feature is computed as the \n# (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n\nprint (pd.DataFrame(dtree.feature_importances_, columns = [\"Imp\"], \\\n                    index = X_train.columns).sort_values(by = 'Imp', ascending = False))","97908395":"importances = dtree.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='green', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","854d2de4":"dtree_weighted=DecisionTreeClassifier(class_weight={0: 0.19, 1: 0.81}, random_state=1)\ndtree_weighted.fit(X_train,y_train)","5f664b66":"dtree_weighted_score=get_metrics_score(dtree_weighted,X_train,X_test,y_train,y_test)\nadd_score_model(dtree_weighted_score)","27fa5e3e":"fig, axes = plt.subplots(1,2,figsize=(16,5));\n\nmake_confusion_matrix(dtree_weighted, X_train, y_train, i=0, seg='Training')\nmake_confusion_matrix(dtree_weighted, X_test, y_test, i=1, seg='Testing')","c1e1e0e6":"plt.figure(figsize=(20,30))\nout = tree.plot_tree(dtree_weighted,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)\nfor o in out:\n     arrow = o.arrow_patch\n     if arrow is not None:\n        arrow.set_edgecolor('black')\n        arrow.set_linewidth(1)\nplt.show()\nplt.show()","0d91f970":"# importance of features in the tree building ( The importance of a feature is computed as the \n# (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n\nprint (pd.DataFrame(dtree_weighted.feature_importances_, columns = [\"Imp\"], \\\n                    index = X_train.columns).sort_values(by = 'Imp', ascending = False))","5f92501b":"importances = dtree_weighted.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='green', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","60fca193":"#Fitting the model\nbagging_classifier = BaggingClassifier(random_state=1, verbose=1)\nbagging_classifier.fit(X_train,y_train)","168033aa":"bagging_score=get_metrics_score(bagging_classifier,X_train,X_test,y_train,y_test)\nadd_score_model(bagging_score)","27b7ebe8":"fig, axes = plt.subplots(1,2,figsize=(16,5));\n\nmake_confusion_matrix(bagging_classifier, X_train, y_train, i=0, seg='Training')\nmake_confusion_matrix(bagging_classifier, X_test, y_test, i=1, seg='Testing')","1e760e1e":"rf_estimator = RandomForestClassifier(random_state=1)\nrf_estimator.fit(X_train,y_train)","6722127d":"score_list_rf=get_metrics_score(rf_estimator,X_train,X_test,y_train,y_test)\nadd_score_model(score_list_rf)","a86eb1c5":"fig, axes = plt.subplots(1,2,figsize=(16,5));\n\nmake_confusion_matrix(rf_estimator, X_train, y_train, i=0, seg='Training')\nmake_confusion_matrix(rf_estimator, X_test, y_test, i=1, seg='Testing')","1292907a":"comparison_frame = pd.DataFrame({'Model':['Decision Tree',\n                                          'Decision Tree Weighted',\n                                          'Bagging Classifier',\n                                          'Random Forest',\n                                          ], \n                                          'Train_Accuracy': acc_train,'Test_Accuracy': acc_test,\n                                          'Train_Recall':recall_train,'Test_Recall':recall_test,\n                                          'Train_Precision':precision_train,'Test_Precision':precision_test,\n                                          'Train_F1':f1_train,\n                                          'Test_F1':f1_test  }) \n\n#Sorting models in decreasing order of test recall\ncomparison_frame.style.highlight_max(color = 'lightgreen', axis = 0).highlight_min(color = 'pink',axis = 0) ","72de5af0":"#Choose the type of classifier. \ndtree_tuned = DecisionTreeClassifier(class_weight={0:0.19,1:0.81},random_state=1)\n\n# Grid of parameters to choose from\nparameters = {'max_depth': list(np.arange(2,20)) + [None], \n              'min_samples_leaf': [1, 3, 5, 7, 10],\n              'max_leaf_nodes' : [2, 3, 5, 10, 15] + [None],\n              'min_impurity_decrease': [0.001, 0.01, 0.1, 0.0]\n             }\n\n# Type of scoring used to compare parameter combinations\nscorer = metrics.make_scorer(metrics.recall_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(dtree_tuned, parameters, scoring=scorer,n_jobs=-1)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\ndtree_tuned = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \ndtree_tuned.fit(X_train, y_train)","d554af75":"score_tune_dt=get_metrics_score(dtree_tuned,X_train,X_test,y_train,y_test)\nadd_score_model(score_tune_dt) # add score to dataframe","d8ffd697":"fig, axes = plt.subplots(1,2,figsize=(16,5));\n\nmake_confusion_matrix(dtree_tuned, X_train, y_train, i=0, seg='Training')\nmake_confusion_matrix(dtree_tuned, X_test, y_test, i=1, seg='Testing')","4b04a65d":"plt.figure(figsize=(15,10))\nfeature_names = X_train.columns\nout = tree.plot_tree(dtree_tuned,feature_names=feature_names,filled=True,fontsize=9,class_names=['1','0'])\nfor o in out:\n    arrow = o.arrow_patch\n    if arrow is not None:\n        arrow.set_edgecolor('black')\n        arrow.set_linewidth(1)\nplt.show()","038b8964":"feature_names = X_train.columns\nimportances = dtree_tuned.feature_importances_\nindices = np.argsort(importances)\nprint (pd.DataFrame(dtree_tuned.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))","11ca91ca":"importances = dtree_tuned.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='green', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","c69d812d":"clf = DecisionTreeClassifier(random_state=1)\npath = clf.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities","009d0ff0":"pd.DataFrame(path).T","3c508769":"fig, ax = plt.subplots(figsize=(10,5))\nax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\nax.set_xlabel(\"effective alpha\")\nax.set_ylabel(\"total impurity of leaves\")\nax.set_title(\"Total Impurity vs effective alpha for training set\")\nplt.show()","366ad43a":"clfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=1, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, y_train)\n    clfs.append(clf)\nprint(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n      clfs[-1].tree_.node_count, ccp_alphas[-1]))","2daf4ec7":"clfs = clfs[:-1]\nccp_alphas = ccp_alphas[:-1]\n\nnode_counts = [clf.tree_.node_count for clf in clfs]\ndepth = [clf.tree_.max_depth for clf in clfs]\nfig, ax = plt.subplots(2, 1,figsize=(16,12))\nax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle=\"steps-post\")\nax[0].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"number of nodes\")\nax[0].set_title(\"Number of nodes vs alpha\")\nax[1].plot(ccp_alphas, depth, marker='o', drawstyle=\"steps-post\")\nax[1].set_xlabel(\"alpha\")\nax[1].set_ylabel(\"depth of tree\")\nax[1].set_title(\"Depth vs alpha\")\nfig.tight_layout()","a257d5b2":"train_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]","cab025f5":"fig, ax = plt.subplots(figsize=(10,5))\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n        drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n        drawstyle=\"steps-post\")\nax.legend()\nplt.show()","ec6517a3":"recall_train_ccp=[]\nfor clf in clfs:\n    pred_train3=clf.predict(X_train)\n    values_train=metrics.recall_score(y_train,pred_train3)\n    recall_train_ccp.append(values_train)\n\nrecall_test_ccp=[]\nfor clf in clfs:\n    pred_test3=clf.predict(X_test)\n    values_test=metrics.recall_score(y_test,pred_test3)\n    recall_test_ccp.append(values_test)","d48800a6":"fig, ax = plt.subplots(figsize=(15,5))\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"Recall\")\nax.set_title(\"Recall vs alpha for training and testing sets\")\nax.plot(ccp_alphas, recall_train_ccp, marker='o', label=\"train\",\n        drawstyle=\"steps-post\")\nax.plot(ccp_alphas, recall_test_ccp, marker='o', label=\"test\",\n        drawstyle=\"steps-post\")\nax.legend()\nplt.show()","b2cbf76c":"# creating the model where we get highest train and test recall\nindex_best_model = np.argmax(recall_test_ccp)\nbest_model = clfs[index_best_model]\nprint(best_model)","d308faee":"# Accuracy on train and test\nprint(\"Accuracy on training set : \",best_model.score(X_train, y_train))\nprint(\"Accuracy on test set : \",best_model.score(X_test, y_test))\n# Recall on train and test\nprint(\"Recall on training set : \",metrics.recall_score(y_train,best_model.predict(X_train)))\nprint(\"Recall on test set : \",metrics.recall_score(y_test,best_model.predict(X_test)))","0023ec6f":"fig, axes = plt.subplots(1,2,figsize=(16,5));\n\nmake_confusion_matrix(best_model, X_train, y_train, i=0, seg='Training')\nmake_confusion_matrix(best_model, X_test, y_test, i=1, seg='Testing')","53f840f8":"plt.figure(figsize=(15,10))\n\nout = tree.plot_tree(best_model,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)\nfor o in out:\n     arrow = o.arrow_patch\n     if arrow is not None:\n        arrow.set_edgecolor('black')\n        arrow.set_linewidth(1)\nplt.show()\nplt.show()","dcb6cb7b":"# Choose the type of classifier. \nrf_tuned = RandomForestClassifier(class_weight={0:0.19,1:0.81},random_state=1)\n\nparameters = {  \n    'max_depth': list(np.arange(5,30,5)) + [None],\n    'max_features': ['sqrt','log2',None],\n    'min_samples_leaf': np.arange(1,15,5),\n    'min_samples_split': np.arange(2, 20, 5),\n    'n_estimators': np.arange(10,110,10)}\n\nscorer = metrics.make_scorer(metrics.recall_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(rf_tuned, parameters, scoring=scorer,cv=5,n_jobs=-1)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nrf_tuned = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nrf_tuned.fit(X_train, y_train)","ea626f14":"score_tune_rf=get_metrics_score(rf_tuned,X_train,X_test,y_train,y_test)\n","dfdc2c78":"add_score_model(score_tune_rf) ","509f972a":"fig, axes = plt.subplots(1,2,figsize=(16,5));\n\nmake_confusion_matrix(rf_tuned, X_train, y_train, i=0, seg='Training')\nmake_confusion_matrix(rf_tuned, X_test, y_test, i=1, seg='Testing')","0642baa9":"feature_names = X_train.columns\nimportances = rf_tuned.feature_importances_\nindices = np.argsort(importances)\nprint (pd.DataFrame(rf_tuned.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))","99124ee7":"plt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='violet', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","d31ec72d":"# Choose the type of classifier. \n\nbagging_estimator_tuned = BaggingClassifier(random_state=1)\n\n# Grid of parameters to choose from\nparameters = {'max_samples': [0.7,0.8,0.9,1], \n              'max_features': [0.7,0.8,0.9,1],\n              'n_estimators' : [10,20,30,40,50]\n             }\n# Type of scoring used to compare parameter combinations\nacc_scorer = metrics.make_scorer(metrics.recall_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(bagging_estimator_tuned, parameters, scoring=acc_scorer,cv=5)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nbagging_estimator_tuned = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data.\nbagging_estimator_tuned.fit(X_train, y_train)","4501bb07":"bagging_tuned=get_metrics_score(bagging_estimator_tuned,X_train,X_test,y_train,y_test)\n","4684521d":"add_score_model(bagging_tuned)","270e2f3c":"fig, axes = plt.subplots(1,2,figsize=(16,5));\n\nmake_confusion_matrix(bagging_estimator_tuned, X_train, y_train, i=0, seg='Training')\nmake_confusion_matrix(bagging_estimator_tuned, X_test, y_test, i=1, seg='Testing')","50cac949":"comparison_frame = pd.DataFrame({'Model':['Decision Tree',\n                                          'Decision Tree Weighted',\n                                          'Bagging Classifier',\n                                          'Random Forest',\n                                          'Tuned Decision Tree',\n                                          'Tuned Random Forest',\n                                          'Tuned Bagging Classifier'], \n                                          'Train_Accuracy': acc_train,'Test_Accuracy': acc_test,\n                                          'Train_Recall':recall_train,'Test_Recall':recall_test,\n                                          'Train_Precision':precision_train,'Test_Precision':precision_test,\n                                          'Train_F1':f1_train,\n                                          'Test_F1':f1_test  }) \n\n#Sorting models in decreasing order of test recall\ncomparison_frame.sort_values(by='Test_Recall',ascending=False).style.highlight_max(color = 'lightgreen', axis = 0).highlight_min(color = 'pink',axis = 0) ","2d79d3d5":"#Fitting the model\nab_classifier = AdaBoostClassifier(random_state=1)\nab_classifier.fit(X_train,y_train)","08721b71":"adaboost_score=get_metrics_score(ab_classifier,X_train,X_test,y_train,y_test)\n","8bfd3a89":"add_score_model(adaboost_score)","caf67e74":"fig, axes = plt.subplots(1,2,figsize=(16,5));\n\nmake_confusion_matrix(ab_classifier, X_train, y_train, i=0, seg='Training')\nmake_confusion_matrix(ab_classifier, X_test, y_test, i=1, seg='Testing')","141097c5":"feature_names = X_train.columns\nimportances = ab_classifier.feature_importances_\nindices = np.argsort(importances)\nprint (pd.DataFrame(ab_classifier.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))","87516957":"plt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='violet', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","16b914f1":"#Fitting the model\ngb_classifier = GradientBoostingClassifier(random_state=1)\ngb_classifier.fit(X_train,y_train)","b8727277":"gb_score=get_metrics_score(gb_classifier,X_train,X_test,y_train,y_test)\nadd_score_model(gb_score)","026bc547":"fig, axes = plt.subplots(1,2,figsize=(16,5));\n\nmake_confusion_matrix(gb_classifier, X_train, y_train, i=0, seg='Training')\nmake_confusion_matrix(gb_classifier, X_test, y_test, i=1, seg='Testing')","19714af2":"feature_names = X_train.columns\nimportances = gb_classifier.feature_importances_\nindices = np.argsort(importances)\nprint (pd.DataFrame(gb_classifier.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))","1a3ca625":"plt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='violet', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","353c5ede":"#Fitting the model\nxgb_classifier = XGBClassifier(random_state=1, eval_metric='logloss')\nxgb_classifier.fit(X_train,y_train)","8747ae8a":"xgb_score=get_metrics_score(xgb_classifier,X_train,X_test,y_train,y_test)\nadd_score_model(xgb_score)","88593895":"fig, axes = plt.subplots(1,2,figsize=(16,5));\n\nmake_confusion_matrix(xgb_classifier, X_train, y_train, i=0, seg='Training')\nmake_confusion_matrix(xgb_classifier, X_test, y_test, i=1, seg='Testing')","27d5992b":"feature_names = X_train.columns\nimportances = xgb_classifier.feature_importances_\nindices = np.argsort(importances)\nprint (pd.DataFrame(xgb_classifier.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))","4e982e1e":"plt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='violet', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","3dede2a6":"comparison_frame = pd.DataFrame({'Model':['Decision Tree',\n                                          'Decision Tree Weighted',\n                                          'Bagging Classifier',\n                                          'Random Forest',\n                                          'Tuned Decision Tree',\n                                          'Tuned Random Forest',\n                                          'Tuned Bagging Classifier',\n                                          'Adaptive Boosting',\n                                          'Gradient Boosting',\n                                          'eXtreme Gradient Boosting'], \n                                          'Train_Accuracy': acc_train,'Test_Accuracy': acc_test,\n                                          'Train_Recall':recall_train,'Test_Recall':recall_test,\n                                          'Train_Precision':precision_train,'Test_Precision':precision_test,\n                                          'Train_F1':f1_train,\n                                          'Test_F1':f1_test  }) \n\n#Sorting models in decreasing order of test recall\ncomparison_frame.sort_values(by='Test_Recall',ascending=False).style.highlight_max(color = 'lightgreen', axis = 0).highlight_min(color = 'pink',axis = 0) ","cb257d12":"# Choose the type of classifier. \nabc_tuned = AdaBoostClassifier(random_state=1)\n\n# Grid of parameters to choose from\nparameters = {\n    #Let's try different max_depth for base_estimator\n    \"base_estimator\":[DecisionTreeClassifier(max_depth=2),\n                      DecisionTreeClassifier(max_depth=3)],\n    \"n_estimators\": np.arange(10,110,10),\n    \"learning_rate\":np.arange(0.1,2,0.1)\n}\n\n# Type of scoring used to compare parameter  combinations\nscorer = metrics.make_scorer(metrics.f1_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(abc_tuned, parameters, scoring=scorer,cv=5)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nabc_tuned = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data.\nabc_tuned.fit(X_train, y_train)","8d928633":"abc_tuned_score=get_metrics_score(abc_tuned,X_train,X_test,y_train,y_test)\n","ed02a738":"add_score_model(abc_tuned_score)","a645f80f":"fig, axes = plt.subplots(1,2,figsize=(16,5));\n\nmake_confusion_matrix(abc_tuned, X_train, y_train, i=0, seg='Training')\nmake_confusion_matrix(abc_tuned, X_test, y_test, i=1, seg='Testing')","f355c79f":"feature_names = X_train.columns\nimportances = abc_tuned.feature_importances_\nindices = np.argsort(importances)\nprint (pd.DataFrame(abc_tuned.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))","1f3fd373":"plt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='violet', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","963903e9":"# Choose the type of classifier. \ngbc_tuned = GradientBoostingClassifier(init=AdaBoostClassifier(random_state=1),random_state=1)\n\n# Grid of parameters to choose from\nparameters = {\n    \"n_estimators\": [100,150,200,250],\n    \"subsample\":[0.8,0.9,1],\n    \"max_features\":[0.7,0.8,0.9,1]\n}\n\n# Type of scoring used to compare parameter combinations\nscorer = metrics.make_scorer(metrics.f1_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(gbc_tuned, parameters, scoring=scorer,cv=5)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\ngbc_tuned = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data.\ngbc_tuned.fit(X_train, y_train)","1ce01f31":"gbc_tuned_score=get_metrics_score(gbc_tuned,X_train,X_test,y_train,y_test)\n","25b11060":"add_score_model(gbc_tuned_score)","8b5f4a8b":"fig, axes = plt.subplots(1,2,figsize=(16,5));\n\nmake_confusion_matrix(gbc_tuned, X_train, y_train, i=0, seg='Training')\nmake_confusion_matrix(gbc_tuned, X_test, y_test, i=1, seg='Testing')","8742b067":"feature_names = X_train.columns\nimportances = gbc_tuned.feature_importances_\nindices = np.argsort(importances)\nprint (pd.DataFrame(gbc_tuned.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))","a2d82922":"plt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='violet', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","6ec4c7ca":"# Choose the type of classifier. \nxgb_tuned = XGBClassifier(random_state=1, eval_metric='logloss')\n\n# Grid of parameters to choose from\nparameters = {\n    \"n_estimators\": [10,30,50],\n    \"scale_pos_weight\":[1,2,5],\n    \"subsample\":[0.7,0.9,1],\n    \"learning_rate\":[0.05, 0.1,0.2],\n    \"colsample_bytree\":[0.7,0.9,1],\n    \"colsample_bylevel\":[0.5,0.7,1]\n}\n\n# Type of scoring used to compare parameter combinations\nscorer = metrics.make_scorer(metrics.f1_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(xgb_tuned, parameters,scoring=scorer,cv=5)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nxgb_tuned = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data.\nxgb_tuned.fit(X_train, y_train)","a148b2a7":"xgb_tuned_score=get_metrics_score(xgb_tuned,X_train,X_test,y_train,y_test)\n","a5ee158a":"add_score_model(xgb_tuned_score)","d261c2fc":"fig, axes = plt.subplots(1,2,figsize=(16,5));\n\nmake_confusion_matrix(xgb_tuned, X_train, y_train, i=0, seg='Training')\nmake_confusion_matrix(xgb_tuned, X_test, y_test, i=1, seg='Testing')","e0263668":"feature_names = X_train.columns\nimportances = xgb_tuned.feature_importances_\nindices = np.argsort(importances)\nprint (pd.DataFrame(xgb_tuned.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))","a97a4e52":"plt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='violet', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","51986b8b":"comparison_frame = pd.DataFrame({'Model':['Decision Tree',\n                                          'Decision Tree Weighted',\n                                          'Bagging Classifier',\n                                          'Random Forest',\n                                          'Tuned Decision Tree',\n                                          'Tuned Random Forest',\n                                          'Tuned Bagging Classifier',\n                                          'Adaptive Boosting',\n                                          'Gradient Boosting',\n                                          'eXtreme Gradient Boosting', \n                                          'Adaptive Boosting Tuned',\n                                          'Gradient Boosting Tuned',\n                                          'eXtreme Gradient Boosting Tuned'],\n                                          'Train_Accuracy': acc_train,'Test_Accuracy': acc_test,\n                                          'Train_Recall':recall_train,'Test_Recall':recall_test,\n                                          'Train_Precision':precision_train,'Test_Precision':precision_test,\n                                          'Train_F1':f1_train,\n                                          'Test_F1':f1_test  }) \n\n#Sorting models in decreasing order of test recall\ncomparison_frame.sort_values(by='Test_Recall',ascending=False).style.highlight_max(color = 'lightgreen', axis = 0).highlight_min(color = 'pink',axis = 0) ","5429722c":"estimators = [('Gb Boosting',gbc_tuned), ('Ada Boosting',abc_tuned), ('Decision Tree',dtree_tuned), ('Random Forest',rf_tuned) ]\n\nfinal_estimator = xgb_tuned\n\nstacking_classifier= StackingClassifier(estimators=estimators,final_estimator=final_estimator)\n\nstacking_classifier.fit(X_train,y_train)","7c04c69b":"stacking_classifier_score=get_metrics_score(stacking_classifier,X_train,X_test,y_train,y_test)\n","53e0fc0c":"add_score_model(stacking_classifier_score)","5e1d0939":"fig, axes = plt.subplots(1,2,figsize=(16,5));\n\nmake_confusion_matrix(stacking_classifier, X_train, y_train, i=0, seg='Training')\nmake_confusion_matrix(stacking_classifier, X_test, y_test, i=1, seg='Testing')","4390bb65":"estimators_2 = [('Gb Boosting',gbc_tuned), ('Ada Boosting',abc_tuned), ('Decision Tree',dtree_tuned) ]\n\nfinal_estimator_2 = rf_tuned\n\nstacking_classifier_2= StackingClassifier(estimators=estimators_2,final_estimator=final_estimator_2)\n\nstacking_classifier_2.fit(X_train,y_train)","83a79d74":"stacking_classifier_2_score=get_metrics_score(stacking_classifier_2,X_train,X_test,y_train,y_test)\n","ab1d1366":"add_score_model(stacking_classifier_2_score)","762e0116":"fig, axes = plt.subplots(1,2,figsize=(16,5));\n\nmake_confusion_matrix(stacking_classifier_2, X_train, y_train, i=0, seg='Training')\nmake_confusion_matrix(stacking_classifier_2, X_test, y_test, i=1, seg='Testing')","0b0697ea":"estimators_3 = [('Gb Boosting',gbc_tuned), ('XGB',xgb_tuned) ]\n\nfinal_estimator_3 = rf_tuned\n\nstacking_classifier_3= StackingClassifier(estimators=estimators_3,final_estimator=final_estimator_3)\n\nstacking_classifier_3.fit(X_train,y_train)","cfb2556d":"stacking_classifier_3_score=get_metrics_score(stacking_classifier_3,X_train,X_test,y_train,y_test)\n","5bee0e3e":"add_score_model(stacking_classifier_3_score)","544de5c3":"fig, axes = plt.subplots(1,2,figsize=(16,5));\n\nmake_confusion_matrix(stacking_classifier_3, X_train, y_train, i=0, seg='Training')\nmake_confusion_matrix(stacking_classifier_3, X_test, y_test, i=1, seg='Testing')","8a013891":"comparison_frame = pd.DataFrame({'Model':['Decision Tree',\n                                          'Decision Tree Weighted',\n                                          'Bagging Classifier',\n                                          'Random Forest',\n                                          'Tuned Decision Tree',\n                                          'Tuned Random Forest',\n                                          'Tuned Bagging Classifier',\n                                          'Adaptive Boosting',\n                                          'Gradient Boosting',\n                                          'eXtreme Gradient Boosting', \n                                          'Adaptive Boosting Tuned',\n                                          'Gradient Boosting Tuned',\n                                          'eXtreme Gradient Boosting Tuned',\n                                          'Stacking Classifier (gbm + ada + rf + dtree -> xgb)',\n                                          'Stacking Classifier (gbm + ada + dtree -> rf)',\n                                          'Stacking Classifier (gbm + xgb -> rf)'],\n                                          'Train_Accuracy': acc_train,'Test_Accuracy': acc_test,\n                                          'Train_Recall':recall_train,'Test_Recall':recall_test,\n                                          'Train_Precision':precision_train,'Test_Precision':precision_test,\n                                          'Train_F1':f1_train,\n                                          'Test_F1':f1_test  }) \n\nfor col in comparison_frame.select_dtypes(include='float').columns.to_list():\n    comparison_frame[col] = (round(comparison_frame[col] * 100, 0)).astype(int)\n    \n#Sorting models in decreasing order of test recall\ncomparison_frame.sort_values(by='Test_Recall',ascending=False).style.highlight_max(color = 'lightgreen', axis = 0).highlight_min(color = 'pink',axis = 0) ","1c056804":"!pip install scikit-plot","0c0670e4":"import scikitplot as skplt\n\ny_pred_prob = stacking_classifier_2.predict_proba(X_test)\n\nskplt.metrics.plot_cumulative_gain(y_test, y_pred_prob, figsize=(15,5))\nplt.show()","e9b43207":"y_pred_prob = stacking_classifier_3.predict_proba(X_test)\n\nskplt.metrics.plot_cumulative_gain(y_test, y_pred_prob, figsize=(15,5))\nplt.show()","202ca614":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score","28b0a76b":"kfold = KFold(n_splits= 10, shuffle=True, random_state=1)","73114a15":"X_full = pd.concat([X_train, X_test])\ny_full = pd.concat([y_train, y_test], ignore_index=True)","2de026e9":"results_accuracy = cross_val_score(estimator= stacking_classifier_2, X=X_full, y=y_full, cv=kfold, scoring='accuracy')\nresults_recall = cross_val_score(estimator= stacking_classifier_2, X=X_full, y=y_full, cv=kfold, scoring='recall')","e7165543":"plt.figure(figsize=(15,3))\nsns.boxplot(x=results_accuracy);","42826b77":"plt.figure(figsize=(15,3))\nsns.boxplot(x=results_recall);","5e0de0c5":"print('Accuracy of the model at 95% confidence: ' \n      + str(round((results_accuracy.mean() - results_accuracy.std() * 1.96) * 100, 1)) \\\n      + '% - ' + str(round((results_accuracy.mean() + results_accuracy.std() * 1.96) * 100, 1)) + '%')\n\nprint('Recall of the model at 95% confidence: ' \n      + str(round((results_recall.mean() - results_recall.std() * 1.96) * 100, 1)) \\\n      + '% - ' + str(round((results_recall.mean() + results_recall.std() * 1.96) * 100, 1)) + '%')","641db251":"results_accuracy = cross_val_score(estimator= stacking_classifier_3, X=X_full, y=y_full, cv=kfold, scoring='accuracy')\nresults_recall = cross_val_score(estimator= stacking_classifier_3, X=X_full, y=y_full, cv=kfold, scoring='recall')","0dcf897b":"plt.figure(figsize=(15,3))\nsns.boxplot(x=results_accuracy);","de288a9d":"plt.figure(figsize=(15,3))\nsns.boxplot(x=results_recall);","8179794e":"print('Accuracy of the model at 95% confidence: ' \n      + str(round((results_accuracy.mean() - results_accuracy.std() * 1.96) * 100, 1)) \\\n      + '% - ' + str(round((results_accuracy.mean() + results_accuracy.std() * 1.96) * 100, 1)) + '%')\n\nprint('Recall of the model at 95% confidence: ' \n      + str(round((results_recall.mean() - results_recall.std() * 1.96) * 100, 1)) \\\n      + '% - ' + str(round((results_recall.mean() + results_recall.std() * 1.96) * 100, 1)) + '%')","3b9e176b":"results_accuracy = cross_val_score(estimator= stacking_classifier, X=X_full, y=y_full, cv=kfold, scoring='accuracy')\nresults_recall = cross_val_score(estimator= stacking_classifier, X=X_full, y=y_full, cv=kfold, scoring='recall')","f873410b":"plt.figure(figsize=(15,3))\nsns.boxplot(x=results_accuracy);","74485d34":"plt.figure(figsize=(15,3))\nsns.boxplot(x=results_recall);","b64e8c30":"print('Accuracy of the model at 95% confidence: ' \n      + str(round((results_accuracy.mean() - results_accuracy.std() * 1.96) * 100, 1)) \\\n      + '% - ' + str(round((results_accuracy.mean() + results_accuracy.std() * 1.96) * 100, 1)) + '%')\n\nprint('Recall of the model at 95% confidence: ' \n      + str(round((results_recall.mean() - results_recall.std() * 1.96) * 100, 1)) \\\n      + '% - ' + str(round((results_recall.mean() + results_recall.std() * 1.96) * 100, 1)) + '%')","9fb9d21d":"!pip install lime\nimport lime\nimport lime.lime_tabular","22287d98":"predict_fn_stacking_2=lambda x: stacking_classifier_2.predict_proba(x).astype(float)\nX=X_train.values\nexplainer=lime.lime_tabular.LimeTabularExplainer(X,feature_names=X_train.columns,class_names=['No','Yes'],kernel_width=5)","624c8848":"X_test.head(3).T","0bfb743b":"y_test.head(3)","d4f33c46":"explanation = explainer.explain_instance(X_test.values[0], predict_fn_stacking_2)\nexplanation.show_in_notebook(show_table=True, show_all=False)","60f0b3c4":"explanation = explainer.explain_instance(X_test.values[1], predict_fn_stacking_2)\nexplanation.show_in_notebook(show_table=True, show_all=False)","1c4f9d62":"explanation = explainer.explain_instance(X_test.values[2], predict_fn_stacking_2)\nexplanation.show_in_notebook(show_table=True, show_all=False)","2869fe90":"test_dataset = X_test.copy()\ntest_dataset['Actual'] = y_test\ntest_dataset['Predicted'] = stacking_classifier_2.predict(X_test)","70182097":"fp = test_dataset[(test_dataset['Actual'] == 0) & (test_dataset['Predicted'] == 1)]","cbded65f":"fp.head(1)","2081830b":"explanation = explainer.explain_instance(fp.iloc[:,:-2].values[0], predict_fn_stacking_2)\nexplanation.show_in_notebook(show_table=True, show_all=False)","3e00a124":"fn = test_dataset[(test_dataset['Actual'] == 1) & (test_dataset['Predicted'] == 0)]","7b1bb53a":"fn.head(1)","413de141":"explanation = explainer.explain_instance(fn.iloc[:,:-2].values[0], predict_fn_stacking_2)\nexplanation.show_in_notebook(show_table=True, show_all=False)","371d4e9f":"Converting the data type of the category variables from object\/float to category","c2ee4b8c":"### Get Score","ac957297":"## Read the dataset","7c347f95":"We are not removing\/treating outliers in the feature `number of persons vising`, since it is in the range of `1 - 5` and `5` is the outlying value.  \n\nWe are going to treat the outliers in `monthly income` and `number of trips` using capping.","34b1cc8f":"# Pre-EDA Data Preprocessing","04d598bb":">**Important**:\n>\n>- The monthly income of a person is assumed here to be determined on the basis of the Occupation, Designation, Gender (unfortunately), and City.\n>\n>- Number of Children visiting is probably null since zero children are visiting","bb48db08":"We are assigning a class weight in the ratio of dependent variable values, but giving more importance to detecting the positive results.","fe7f13d6":">Let's check a random one using LIME","6a7d7bbc":"### Confusion Matrix","c76ba935":"**Important**:\n\nWe usually use median values of a particular group in a dataset to fill in the null values. Now, in ideal scenario, `test` dataset is unknown to us. So, to train the model, only the `training` dataset should be considered for null imputation. However, before checking model performance, we need to impute null values in the `test` dataset. But, the `imputation of null values should be done separately for train and test data so that test data would not create biased medians in train dataset, and vice versa`.","d7f236a8":"# Data Description\n\n**- Customer details:**\n\n1. `CustomerID`: Unique customer ID\n1. `ProdTaken`: Whether the customer has purchased a package or not `(0: No, 1: Yes)`\n1. `Age`: Age of customer\n1. `TypeofContact`: How customer was contacted (Company Invited or Self Inquiry)\n1. `CityTier`: City tier depends on the development of a city, population, facilities, and living standards. The categories are ordered i.e. Tier 1 > Tier 2 > Tier 3\n1. `Occupation`: Occupation of customer\n1. `Gender`: Gender of customer\n1. `NumberOfPersonVisiting`: Total number of persons planning to take the trip with the customer\n1. `PreferredPropertyStar`: Preferred hotel property rating by customer\n1. `MaritalStatus`: Marital status of customer\n1. `NumberOfTrips`: Average number of trips in a year by customer\n1. `Passport`: The customer has a passport or not (0: No, 1: Yes)\n1. `OwnCar`: Whether the customers own a car or not (0: No, 1: Yes)\n1. `NumberOfChildrenVisiting`: Total number of children with age less than 5 planning to take the trip with the customer\n1. `Designation`: Designation of the customer in the current organization\n1. `MonthlyIncome`: Gross monthly income of the customer\n\n&nbsp;\n**- Customer interaction data:**\n\n1. `PitchSatisfactionScore`: Sales pitch satisfaction score\n1. `ProductPitched`: Product pitched by the salesperson\n1. `NumberOfFollowups`: Total number of follow-ups has been done by the salesperson after the sales pitch\n1. `DurationOfPitch`: Duration of the pitch by a salesperson to the customer\n","16d40ddc":"## Dropping ID column","7132e735":"#### Designation","738336ce":"## Gradient Boosting","11f61fd2":">**Observation**:\n>- The `recall` definitely `improved` a lot\n>- However, the `accuracy is lower than the base accuracy`\n>- According to this model, `passport status`, `age`, `executive designation`, and `monthly income` are the top 4 most important features\n>- This model `generalizes quite well`","bef366ec":"## Decision Tree : Cost Complexity Pruning","41a55b61":"### Confusion Matrix","a12595ce":"### Feature Importances","2d458cdf":"> **Observations**:\n>- The 3 most important features are `Monthly Income`, `Age`, `Number of Trips`","a938f7e7":"#### With outliers","9d116153":"> Though the model is `generalizing well`, `recall is very bad`.","5dda72c7":"## Import necessary libraries","9800fdfc":"#### Number of Children Visiting","0c2d9c8e":"# Gain\/Lift chart ","50aae5d4":"# Model Tuning: Decision Tree, Bagging, Random Forest","f5653a70":"> Most import features appear to be `passport status`, `monthly income`, `age`, `executive designation`","ff898bbc":">`Stacking Classifier` with `Tuned Gradient Boosting`, `Tuned eXtreme Gradient Boosting` as base estimators and `Tuned Random Forest` as final estimator","0ef1edf4":">Let's check a random one using LIME","65a10d75":"> According to the cumulative gain chart, both of these models are performing really well.","47b4ea6e":"### Check testing set nulls","89809de2":"## XGB Tuning","f6c2fdd8":"### Build Model","fb509552":"### Confusion Matrix","e6f69727":"> **Observation**:\n>- Class weight did not improve the model performance on the test data.  \n\n> **Points for improvement**:\n>- We need to try hyper parameter tuning","5d7f1d48":"### Overall statistics for `Basic` package","0a008427":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Problem-Statement\" data-toc-modified-id=\"Problem-Statement-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Problem Statement<\/a><\/span><\/li><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Objective<\/a><\/span><\/li><li><span><a href=\"#Data-Description\" data-toc-modified-id=\"Data-Description-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Data Description<\/a><\/span><\/li><li><span><a href=\"#Initial-Set-up\" data-toc-modified-id=\"Initial-Set-up-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Initial Set-up<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Import-necessary-libraries\" data-toc-modified-id=\"Import-necessary-libraries-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Import necessary libraries<\/a><\/span><\/li><li><span><a href=\"#Read-the-dataset\" data-toc-modified-id=\"Read-the-dataset-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Read the dataset<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Understand-the-data\" data-toc-modified-id=\"Understand-the-data-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Understand the data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#View-the-first-and-last-5-rows-of-the-dataset.\" data-toc-modified-id=\"View-the-first-and-last-5-rows-of-the-dataset.-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>View the first and last 5 rows of the dataset.<\/a><\/span><\/li><li><span><a href=\"#Understand-the-shape-of-the-dataset.\" data-toc-modified-id=\"Understand-the-shape-of-the-dataset.-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>Understand the shape of the dataset.<\/a><\/span><\/li><li><span><a href=\"#Check-the-data-types-of-the-columns-for-the-dataset.\" data-toc-modified-id=\"Check-the-data-types-of-the-columns-for-the-dataset.-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;<\/span>Check the data types of the columns for the dataset.<\/a><\/span><\/li><li><span><a href=\"#Summary-of-the-dataset.\" data-toc-modified-id=\"Summary-of-the-dataset.-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;<\/span>Summary of the dataset.<\/a><\/span><\/li><li><span><a href=\"#Categorical-column-statistics\" data-toc-modified-id=\"Categorical-column-statistics-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;<\/span>Categorical column statistics<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Pre-EDA-Data-Preprocessing\" data-toc-modified-id=\"Pre-EDA-Data-Preprocessing-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Pre-EDA Data Preprocessing<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Dropping-ID-column\" data-toc-modified-id=\"Dropping-ID-column-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;<\/span>Dropping ID column<\/a><\/span><\/li><li><span><a href=\"#Fixing-Gender-value\" data-toc-modified-id=\"Fixing-Gender-value-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;<\/span>Fixing Gender value<\/a><\/span><\/li><li><span><a href=\"#Data-type-conversions\" data-toc-modified-id=\"Data-type-conversions-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;<\/span>Data type conversions<\/a><\/span><\/li><li><span><a href=\"#Standardizing-column-names\" data-toc-modified-id=\"Standardizing-column-names-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;<\/span>Standardizing column names<\/a><\/span><\/li><li><span><a href=\"#Create-bins-for-Age-and-Monthly-Income\" data-toc-modified-id=\"Create-bins-for-Age-and-Monthly-Income-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;<\/span>Create bins for Age and Monthly Income<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Exploratory Data Analysis<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Univariate-Analysis\" data-toc-modified-id=\"Univariate-Analysis-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;<\/span>Univariate Analysis<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Numerical-Feature-Summary\" data-toc-modified-id=\"Numerical-Feature-Summary-7.1.1\"><span class=\"toc-item-num\">7.1.1&nbsp;&nbsp;<\/span>Numerical Feature Summary<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Age\" data-toc-modified-id=\"Age-7.1.1.1\"><span class=\"toc-item-num\">7.1.1.1&nbsp;&nbsp;<\/span>Age<\/a><\/span><\/li><li><span><a href=\"#Duration-of-Pitch\" data-toc-modified-id=\"Duration-of-Pitch-7.1.1.2\"><span class=\"toc-item-num\">7.1.1.2&nbsp;&nbsp;<\/span>Duration of Pitch<\/a><\/span><\/li><li><span><a href=\"#Number-of-Person-Visiting\" data-toc-modified-id=\"Number-of-Person-Visiting-7.1.1.3\"><span class=\"toc-item-num\">7.1.1.3&nbsp;&nbsp;<\/span>Number of Person Visiting<\/a><\/span><\/li><li><span><a href=\"#Number-of-Follow-ups\" data-toc-modified-id=\"Number-of-Follow-ups-7.1.1.4\"><span class=\"toc-item-num\">7.1.1.4&nbsp;&nbsp;<\/span>Number of Follow-ups<\/a><\/span><\/li><li><span><a href=\"#Preferred-Property-Stars\" data-toc-modified-id=\"Preferred-Property-Stars-7.1.1.5\"><span class=\"toc-item-num\">7.1.1.5&nbsp;&nbsp;<\/span>Preferred Property Stars<\/a><\/span><\/li><li><span><a href=\"#Number-of-Trips\" data-toc-modified-id=\"Number-of-Trips-7.1.1.6\"><span class=\"toc-item-num\">7.1.1.6&nbsp;&nbsp;<\/span>Number of Trips<\/a><\/span><\/li><li><span><a href=\"#Pitch-Satisfaction-Score\" data-toc-modified-id=\"Pitch-Satisfaction-Score-7.1.1.7\"><span class=\"toc-item-num\">7.1.1.7&nbsp;&nbsp;<\/span>Pitch Satisfaction Score<\/a><\/span><\/li><li><span><a href=\"#Number-of-Children-Visiting\" data-toc-modified-id=\"Number-of-Children-Visiting-7.1.1.8\"><span class=\"toc-item-num\">7.1.1.8&nbsp;&nbsp;<\/span>Number of Children Visiting<\/a><\/span><\/li><li><span><a href=\"#Monthly-Income\" data-toc-modified-id=\"Monthly-Income-7.1.1.9\"><span class=\"toc-item-num\">7.1.1.9&nbsp;&nbsp;<\/span>Monthly Income<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Percentage-on-bar-chart-for-Categorical-Features\" data-toc-modified-id=\"Percentage-on-bar-chart-for-Categorical-Features-7.1.2\"><span class=\"toc-item-num\">7.1.2&nbsp;&nbsp;<\/span>Percentage on bar chart for Categorical Features<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Bi-variate-Analysis\" data-toc-modified-id=\"Bi-variate-Analysis-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;<\/span>Bi-variate Analysis<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Target-vs.-All-numerical-columns\" data-toc-modified-id=\"Target-vs.-All-numerical-columns-7.2.1\"><span class=\"toc-item-num\">7.2.1&nbsp;&nbsp;<\/span>Target vs. All numerical columns<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#With-outliers\" data-toc-modified-id=\"With-outliers-7.2.1.1\"><span class=\"toc-item-num\">7.2.1.1&nbsp;&nbsp;<\/span>With outliers<\/a><\/span><\/li><li><span><a href=\"#Without-outliers\" data-toc-modified-id=\"Without-outliers-7.2.1.2\"><span class=\"toc-item-num\">7.2.1.2&nbsp;&nbsp;<\/span>Without outliers<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Target-vs.-All-Categorical-Columns\" data-toc-modified-id=\"Target-vs.-All-Categorical-Columns-7.2.2\"><span class=\"toc-item-num\">7.2.2&nbsp;&nbsp;<\/span>Target vs. All Categorical Columns<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Type-of-Contact\" data-toc-modified-id=\"Type-of-Contact-7.2.2.1\"><span class=\"toc-item-num\">7.2.2.1&nbsp;&nbsp;<\/span>Type of Contact<\/a><\/span><\/li><li><span><a href=\"#City-Tier\" data-toc-modified-id=\"City-Tier-7.2.2.2\"><span class=\"toc-item-num\">7.2.2.2&nbsp;&nbsp;<\/span>City Tier<\/a><\/span><\/li><li><span><a href=\"#Occupation\" data-toc-modified-id=\"Occupation-7.2.2.3\"><span class=\"toc-item-num\">7.2.2.3&nbsp;&nbsp;<\/span>Occupation<\/a><\/span><\/li><li><span><a href=\"#Gender\" data-toc-modified-id=\"Gender-7.2.2.4\"><span class=\"toc-item-num\">7.2.2.4&nbsp;&nbsp;<\/span>Gender<\/a><\/span><\/li><li><span><a href=\"#Product-Pitched\" data-toc-modified-id=\"Product-Pitched-7.2.2.5\"><span class=\"toc-item-num\">7.2.2.5&nbsp;&nbsp;<\/span>Product Pitched<\/a><\/span><\/li><li><span><a href=\"#Marital-Status\" data-toc-modified-id=\"Marital-Status-7.2.2.6\"><span class=\"toc-item-num\">7.2.2.6&nbsp;&nbsp;<\/span>Marital Status<\/a><\/span><\/li><li><span><a href=\"#Passport\" data-toc-modified-id=\"Passport-7.2.2.7\"><span class=\"toc-item-num\">7.2.2.7&nbsp;&nbsp;<\/span>Passport<\/a><\/span><\/li><li><span><a href=\"#Own-Car\" data-toc-modified-id=\"Own-Car-7.2.2.8\"><span class=\"toc-item-num\">7.2.2.8&nbsp;&nbsp;<\/span>Own Car<\/a><\/span><\/li><li><span><a href=\"#Designation\" data-toc-modified-id=\"Designation-7.2.2.9\"><span class=\"toc-item-num\">7.2.2.9&nbsp;&nbsp;<\/span>Designation<\/a><\/span><\/li><li><span><a href=\"#Age-bin\" data-toc-modified-id=\"Age-bin-7.2.2.10\"><span class=\"toc-item-num\">7.2.2.10&nbsp;&nbsp;<\/span>Age bin<\/a><\/span><\/li><li><span><a href=\"#Income-bin\" data-toc-modified-id=\"Income-bin-7.2.2.11\"><span class=\"toc-item-num\">7.2.2.11&nbsp;&nbsp;<\/span>Income bin<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Important-features\" data-toc-modified-id=\"Important-features-7.2.3\"><span class=\"toc-item-num\">7.2.3&nbsp;&nbsp;<\/span>Important features<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Multi-variate-Plots\" data-toc-modified-id=\"Multi-variate-Plots-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;<\/span>Multi-variate Plots<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Pairplot-of-all-available-numeric-columns,-hued-by-Personal-Loan\" data-toc-modified-id=\"Pairplot-of-all-available-numeric-columns,-hued-by-Personal-Loan-7.3.1\"><span class=\"toc-item-num\">7.3.1&nbsp;&nbsp;<\/span>Pairplot of all available numeric columns, hued by Personal Loan<\/a><\/span><\/li><li><span><a href=\"#Heatmap-to-understand-correlations-between-independent-and-dependent-variables\" data-toc-modified-id=\"Heatmap-to-understand-correlations-between-independent-and-dependent-variables-7.3.2\"><span class=\"toc-item-num\">7.3.2&nbsp;&nbsp;<\/span>Heatmap to understand correlations between independent and dependent variables<\/a><\/span><\/li><li><span><a href=\"#Categorical-variables-vs.-Monthly-Income-by-Product-Taken-(Including-outliers)\" data-toc-modified-id=\"Categorical-variables-vs.-Monthly-Income-by-Product-Taken-(Including-outliers)-7.3.3\"><span class=\"toc-item-num\">7.3.3&nbsp;&nbsp;<\/span>Categorical variables vs. Monthly Income by Product Taken (Including outliers)<\/a><\/span><\/li><li><span><a href=\"#Categorical-variables-vs.-Age-by-Product-Taken\" data-toc-modified-id=\"Categorical-variables-vs.-Age-by-Product-Taken-7.3.4\"><span class=\"toc-item-num\">7.3.4&nbsp;&nbsp;<\/span>Categorical variables vs. Age by Product Taken<\/a><\/span><\/li><li><span><a href=\"#Categorical-variables-vs.-Number-of-Followups-by-Product-Taken\" data-toc-modified-id=\"Categorical-variables-vs.-Number-of-Followups-by-Product-Taken-7.3.5\"><span class=\"toc-item-num\">7.3.5&nbsp;&nbsp;<\/span>Categorical variables vs. Number of Followups by Product Taken<\/a><\/span><\/li><li><span><a href=\"#Categorical-variables-vs.-Preferred-Property-Rating-by-Product-Taken\" data-toc-modified-id=\"Categorical-variables-vs.-Preferred-Property-Rating-by-Product-Taken-7.3.6\"><span class=\"toc-item-num\">7.3.6&nbsp;&nbsp;<\/span>Categorical variables vs. Preferred Property Rating by Product Taken<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Customer-Profiling-for-Purchased-Products\" data-toc-modified-id=\"Customer-Profiling-for-Purchased-Products-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;<\/span>Customer Profiling for Purchased Products<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Count-per-Category-for-each-type-of-product-pitched\" data-toc-modified-id=\"Count-per-Category-for-each-type-of-product-pitched-7.4.1\"><span class=\"toc-item-num\">7.4.1&nbsp;&nbsp;<\/span>Count per Category for each type of product pitched<\/a><\/span><\/li><li><span><a href=\"#Bar-plot-of-Monthly-Income-for-each-type-of-product-pitched\" data-toc-modified-id=\"Bar-plot-of-Monthly-Income-for-each-type-of-product-pitched-7.4.2\"><span class=\"toc-item-num\">7.4.2&nbsp;&nbsp;<\/span>Bar plot of Monthly Income for each type of product pitched<\/a><\/span><\/li><li><span><a href=\"#Bar-plot-of-Age-for-each-type-of-product-pitched\" data-toc-modified-id=\"Bar-plot-of-Age-for-each-type-of-product-pitched-7.4.3\"><span class=\"toc-item-num\">7.4.3&nbsp;&nbsp;<\/span>Bar plot of Age for each type of product pitched<\/a><\/span><\/li><li><span><a href=\"#Monthly-Income-and-Age-statistics-per-product-pitched\" data-toc-modified-id=\"Monthly-Income-and-Age-statistics-per-product-pitched-7.4.4\"><span class=\"toc-item-num\">7.4.4&nbsp;&nbsp;<\/span>Monthly Income and Age statistics per product pitched<\/a><\/span><\/li><li><span><a href=\"#Overall-statistics-for-Basic-package\" data-toc-modified-id=\"Overall-statistics-for-Basic-package-7.4.5\"><span class=\"toc-item-num\">7.4.5&nbsp;&nbsp;<\/span>Overall statistics for <code>Basic<\/code> package<\/a><\/span><\/li><li><span><a href=\"#Overall-statistics-for-Standard-package\" data-toc-modified-id=\"Overall-statistics-for-Standard-package-7.4.6\"><span class=\"toc-item-num\">7.4.6&nbsp;&nbsp;<\/span>Overall statistics for <code>Standard<\/code> package<\/a><\/span><\/li><li><span><a href=\"#Overall-statistics-for-Deluxe-package\" data-toc-modified-id=\"Overall-statistics-for-Deluxe-package-7.4.7\"><span class=\"toc-item-num\">7.4.7&nbsp;&nbsp;<\/span>Overall statistics for <code>Deluxe<\/code> package<\/a><\/span><\/li><li><span><a href=\"#Overall-statistics-for-Super-Deluxe-package\" data-toc-modified-id=\"Overall-statistics-for-Super-Deluxe-package-7.4.8\"><span class=\"toc-item-num\">7.4.8&nbsp;&nbsp;<\/span>Overall statistics for <code>Super Deluxe<\/code> package<\/a><\/span><\/li><li><span><a href=\"#Overall-statistics-for-King-package\" data-toc-modified-id=\"Overall-statistics-for-King-package-7.4.9\"><span class=\"toc-item-num\">7.4.9&nbsp;&nbsp;<\/span>Overall statistics for <code>King<\/code> package<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#insights-based-on-EDA\" data-toc-modified-id=\"insights-based-on-EDA-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span><strong>insights based on EDA<\/strong><\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Customer-Profile\" data-toc-modified-id=\"Customer-Profile-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;<\/span><strong>Customer Profile<\/strong><\/a><\/span><\/li><li><span><a href=\"#Observations-on-Patterns\" data-toc-modified-id=\"Observations-on-Patterns-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;<\/span><strong>Observations on Patterns<\/strong><\/a><\/span><\/li><li><span><a href=\"#Outliers\" data-toc-modified-id=\"Outliers-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;<\/span><strong>Outliers<\/strong><\/a><\/span><\/li><li><span><a href=\"#Columns-to-be-dropped\" data-toc-modified-id=\"Columns-to-be-dropped-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;<\/span><strong>Columns to be dropped<\/strong><\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Data-Pre-processing-for-Model-Building\" data-toc-modified-id=\"Data-Pre-processing-for-Model-Building-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;<\/span>Data Pre-processing for Model Building<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Drop-the-columns-that-are-not-useful-for-the-model\" data-toc-modified-id=\"Drop-the-columns-that-are-not-useful-for-the-model-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;<\/span>Drop the columns that are not useful for the model<\/a><\/span><\/li><li><span><a href=\"#Split-into-training-and-testing-dataset\" data-toc-modified-id=\"Split-into-training-and-testing-dataset-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;<\/span>Split into training and testing dataset<\/a><\/span><\/li><li><span><a href=\"#Outliers-detection-using-Box-plot\" data-toc-modified-id=\"Outliers-detection-using-Box-plot-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;<\/span>Outliers detection using Box plot<\/a><\/span><\/li><li><span><a href=\"#Treating-Outliers\" data-toc-modified-id=\"Treating-Outliers-9.4\"><span class=\"toc-item-num\">9.4&nbsp;&nbsp;<\/span>Treating Outliers<\/a><\/span><\/li><li><span><a href=\"#Null-Handling\" data-toc-modified-id=\"Null-Handling-9.5\"><span class=\"toc-item-num\">9.5&nbsp;&nbsp;<\/span>Null Handling<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Check-training-set-nulls\" data-toc-modified-id=\"Check-training-set-nulls-9.5.1\"><span class=\"toc-item-num\">9.5.1&nbsp;&nbsp;<\/span>Check training set nulls<\/a><\/span><\/li><li><span><a href=\"#Check-testing-set-nulls\" data-toc-modified-id=\"Check-testing-set-nulls-9.5.2\"><span class=\"toc-item-num\">9.5.2&nbsp;&nbsp;<\/span>Check testing set nulls<\/a><\/span><\/li><li><span><a href=\"#Check-training-and-testing-set-median-by-a-group\" data-toc-modified-id=\"Check-training-and-testing-set-median-by-a-group-9.5.3\"><span class=\"toc-item-num\">9.5.3&nbsp;&nbsp;<\/span>Check training and testing set median by a group<\/a><\/span><\/li><li><span><a href=\"#Missing-value-imputation\" data-toc-modified-id=\"Missing-value-imputation-9.5.4\"><span class=\"toc-item-num\">9.5.4&nbsp;&nbsp;<\/span>Missing value imputation<\/a><\/span><\/li><li><span><a href=\"#Final-check-in-Training-and-Testing-dataset\" data-toc-modified-id=\"Final-check-in-Training-and-Testing-dataset-9.5.5\"><span class=\"toc-item-num\">9.5.5&nbsp;&nbsp;<\/span>Final check in Training and Testing dataset<\/a><\/span><\/li><li><span><a href=\"#Encoding-Categorical-Variables\" data-toc-modified-id=\"Encoding-Categorical-Variables-9.5.6\"><span class=\"toc-item-num\">9.5.6&nbsp;&nbsp;<\/span>Encoding Categorical Variables<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Model-Building\" data-toc-modified-id=\"Model-Building-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;<\/span>Model Building<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Model-Evaluation-Criteria\" data-toc-modified-id=\"Model-Evaluation-Criteria-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;<\/span>Model Evaluation Criteria<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Model-can-make-wrong-predictions-as:\" data-toc-modified-id=\"Model-can-make-wrong-predictions-as:-10.1.1\"><span class=\"toc-item-num\">10.1.1&nbsp;&nbsp;<\/span>Model can make wrong predictions as:<\/a><\/span><\/li><li><span><a href=\"#Which-case-is-more-important?\" data-toc-modified-id=\"Which-case-is-more-important?-10.1.2\"><span class=\"toc-item-num\">10.1.2&nbsp;&nbsp;<\/span>Which case is more important?<\/a><\/span><\/li><li><span><a href=\"#How-to-reduce-this-loss-i.e-need-to-reduce-False-Negatives?\" data-toc-modified-id=\"How-to-reduce-this-loss-i.e-need-to-reduce-False-Negatives?-10.1.3\"><span class=\"toc-item-num\">10.1.3&nbsp;&nbsp;<\/span>How to reduce this loss i.e need to reduce False Negatives?<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-Evaluation-Functions---Scoring-&amp;-Confusion-Matrix\" data-toc-modified-id=\"Model-Evaluation-Functions---Scoring-&amp;-Confusion-Matrix-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;<\/span>Model Evaluation Functions - Scoring &amp; Confusion Matrix<\/a><\/span><\/li><li><span><a href=\"#Decision-Tree\" data-toc-modified-id=\"Decision-Tree-10.3\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;<\/span>Decision Tree<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Build-Model\" data-toc-modified-id=\"Build-Model-10.3.1\"><span class=\"toc-item-num\">10.3.1&nbsp;&nbsp;<\/span>Build Model<\/a><\/span><\/li><li><span><a href=\"#Get-Score\" data-toc-modified-id=\"Get-Score-10.3.2\"><span class=\"toc-item-num\">10.3.2&nbsp;&nbsp;<\/span>Get Score<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-10.3.3\"><span class=\"toc-item-num\">10.3.3&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><li><span><a href=\"#Visualize-the-Decision-Tree\" data-toc-modified-id=\"Visualize-the-Decision-Tree-10.3.4\"><span class=\"toc-item-num\">10.3.4&nbsp;&nbsp;<\/span>Visualize the Decision Tree<\/a><\/span><\/li><li><span><a href=\"#Feature-Importances\" data-toc-modified-id=\"Feature-Importances-10.3.5\"><span class=\"toc-item-num\">10.3.5&nbsp;&nbsp;<\/span>Feature Importances<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Decision-Tree-with-Class-Weight\" data-toc-modified-id=\"Decision-Tree-with-Class-Weight-10.4\"><span class=\"toc-item-num\">10.4&nbsp;&nbsp;<\/span>Decision Tree with Class Weight<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Build-Model\" data-toc-modified-id=\"Build-Model-10.4.1\"><span class=\"toc-item-num\">10.4.1&nbsp;&nbsp;<\/span>Build Model<\/a><\/span><\/li><li><span><a href=\"#Get-Score\" data-toc-modified-id=\"Get-Score-10.4.2\"><span class=\"toc-item-num\">10.4.2&nbsp;&nbsp;<\/span>Get Score<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-10.4.3\"><span class=\"toc-item-num\">10.4.3&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><li><span><a href=\"#Visualize-the-Decision-Tree\" data-toc-modified-id=\"Visualize-the-Decision-Tree-10.4.4\"><span class=\"toc-item-num\">10.4.4&nbsp;&nbsp;<\/span>Visualize the Decision Tree<\/a><\/span><\/li><li><span><a href=\"#Feature-Importances\" data-toc-modified-id=\"Feature-Importances-10.4.5\"><span class=\"toc-item-num\">10.4.5&nbsp;&nbsp;<\/span>Feature Importances<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Model-Building:-Bagging\" data-toc-modified-id=\"Model-Building:-Bagging-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;<\/span>Model Building: Bagging<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Bagging-Classifier\" data-toc-modified-id=\"Bagging-Classifier-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;<\/span>Bagging Classifier<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Build-Model\" data-toc-modified-id=\"Build-Model-11.1.1\"><span class=\"toc-item-num\">11.1.1&nbsp;&nbsp;<\/span>Build Model<\/a><\/span><\/li><li><span><a href=\"#Get-Score\" data-toc-modified-id=\"Get-Score-11.1.2\"><span class=\"toc-item-num\">11.1.2&nbsp;&nbsp;<\/span>Get Score<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-11.1.3\"><span class=\"toc-item-num\">11.1.3&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Model-Building:-Random-Forest\" data-toc-modified-id=\"Model-Building:-Random-Forest-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;<\/span>Model Building: Random Forest<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Random-Forest-Classifier\" data-toc-modified-id=\"Random-Forest-Classifier-12.1\"><span class=\"toc-item-num\">12.1&nbsp;&nbsp;<\/span>Random Forest Classifier<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Build-Model\" data-toc-modified-id=\"Build-Model-12.1.1\"><span class=\"toc-item-num\">12.1.1&nbsp;&nbsp;<\/span>Build Model<\/a><\/span><\/li><li><span><a href=\"#Get-Score\" data-toc-modified-id=\"Get-Score-12.1.2\"><span class=\"toc-item-num\">12.1.2&nbsp;&nbsp;<\/span>Get Score<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-12.1.3\"><span class=\"toc-item-num\">12.1.3&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-Score-Comparison\" data-toc-modified-id=\"Model-Score-Comparison-12.2\"><span class=\"toc-item-num\">12.2&nbsp;&nbsp;<\/span>Model Score Comparison<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-Tuning:-Decision-Tree,-Bagging,-Random-Forest\" data-toc-modified-id=\"Model-Tuning:-Decision-Tree,-Bagging,-Random-Forest-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;<\/span>Model Tuning: Decision Tree, Bagging, Random Forest<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Decision-Tree-Model-Tuning\" data-toc-modified-id=\"Decision-Tree-Model-Tuning-13.1\"><span class=\"toc-item-num\">13.1&nbsp;&nbsp;<\/span>Decision Tree Model Tuning<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Tuning-Decision-Tree\" data-toc-modified-id=\"Tuning-Decision-Tree-13.1.1\"><span class=\"toc-item-num\">13.1.1&nbsp;&nbsp;<\/span>Tuning Decision Tree<\/a><\/span><\/li><li><span><a href=\"#Tuned-Decision-Tree-Score\" data-toc-modified-id=\"Tuned-Decision-Tree-Score-13.1.2\"><span class=\"toc-item-num\">13.1.2&nbsp;&nbsp;<\/span>Tuned Decision Tree Score<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-13.1.3\"><span class=\"toc-item-num\">13.1.3&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><li><span><a href=\"#Visualize-the-Tree\" data-toc-modified-id=\"Visualize-the-Tree-13.1.4\"><span class=\"toc-item-num\">13.1.4&nbsp;&nbsp;<\/span>Visualize the Tree<\/a><\/span><\/li><li><span><a href=\"#Feature-Importances\" data-toc-modified-id=\"Feature-Importances-13.1.5\"><span class=\"toc-item-num\">13.1.5&nbsp;&nbsp;<\/span>Feature Importances<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Decision-Tree-:-Cost-Complexity-Pruning\" data-toc-modified-id=\"Decision-Tree-:-Cost-Complexity-Pruning-13.2\"><span class=\"toc-item-num\">13.2&nbsp;&nbsp;<\/span>Decision Tree : Cost Complexity Pruning<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Build-the-model\" data-toc-modified-id=\"Build-the-model-13.2.1\"><span class=\"toc-item-num\">13.2.1&nbsp;&nbsp;<\/span>Build the model<\/a><\/span><\/li><li><span><a href=\"#Check-Model-Score\" data-toc-modified-id=\"Check-Model-Score-13.2.2\"><span class=\"toc-item-num\">13.2.2&nbsp;&nbsp;<\/span>Check Model Score<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-13.2.3\"><span class=\"toc-item-num\">13.2.3&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><li><span><a href=\"#Visualizing-the-Decision-Tree\" data-toc-modified-id=\"Visualizing-the-Decision-Tree-13.2.4\"><span class=\"toc-item-num\">13.2.4&nbsp;&nbsp;<\/span>Visualizing the Decision Tree<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Random-Forest-Model-Tuning\" data-toc-modified-id=\"Random-Forest-Model-Tuning-13.3\"><span class=\"toc-item-num\">13.3&nbsp;&nbsp;<\/span>Random Forest Model Tuning<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Tuning-Random-Forest-Classifier-Model\" data-toc-modified-id=\"Tuning-Random-Forest-Classifier-Model-13.3.1\"><span class=\"toc-item-num\">13.3.1&nbsp;&nbsp;<\/span>Tuning Random Forest Classifier Model<\/a><\/span><\/li><li><span><a href=\"#Tuned-Random-Forest-Scores\" data-toc-modified-id=\"Tuned-Random-Forest-Scores-13.3.2\"><span class=\"toc-item-num\">13.3.2&nbsp;&nbsp;<\/span>Tuned Random Forest Scores<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-13.3.3\"><span class=\"toc-item-num\">13.3.3&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><li><span><a href=\"#Feature-Importances\" data-toc-modified-id=\"Feature-Importances-13.3.4\"><span class=\"toc-item-num\">13.3.4&nbsp;&nbsp;<\/span>Feature Importances<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Bagging-Classifier-Model-Tuning\" data-toc-modified-id=\"Bagging-Classifier-Model-Tuning-13.4\"><span class=\"toc-item-num\">13.4&nbsp;&nbsp;<\/span>Bagging Classifier Model Tuning<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Tuning-Bagging-Classifier-Model\" data-toc-modified-id=\"Tuning-Bagging-Classifier-Model-13.4.1\"><span class=\"toc-item-num\">13.4.1&nbsp;&nbsp;<\/span>Tuning Bagging Classifier Model<\/a><\/span><\/li><li><span><a href=\"#Tuned-Bagging-Classifier-Scores\" data-toc-modified-id=\"Tuned-Bagging-Classifier-Scores-13.4.2\"><span class=\"toc-item-num\">13.4.2&nbsp;&nbsp;<\/span>Tuned Bagging Classifier Scores<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-13.4.3\"><span class=\"toc-item-num\">13.4.3&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-Score-Comparison\" data-toc-modified-id=\"Model-Score-Comparison-13.5\"><span class=\"toc-item-num\">13.5&nbsp;&nbsp;<\/span>Model Score Comparison<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-Building:-Boosting\" data-toc-modified-id=\"Model-Building:-Boosting-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;<\/span>Model Building: Boosting<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Adaptive-Boosting\" data-toc-modified-id=\"Adaptive-Boosting-14.1\"><span class=\"toc-item-num\">14.1&nbsp;&nbsp;<\/span>Adaptive Boosting<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Build-AdaBoost-Model\" data-toc-modified-id=\"Build-AdaBoost-Model-14.1.1\"><span class=\"toc-item-num\">14.1.1&nbsp;&nbsp;<\/span>Build AdaBoost Model<\/a><\/span><\/li><li><span><a href=\"#Get-Score\" data-toc-modified-id=\"Get-Score-14.1.2\"><span class=\"toc-item-num\">14.1.2&nbsp;&nbsp;<\/span>Get Score<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-14.1.3\"><span class=\"toc-item-num\">14.1.3&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><li><span><a href=\"#Feature-Importances\" data-toc-modified-id=\"Feature-Importances-14.1.4\"><span class=\"toc-item-num\">14.1.4&nbsp;&nbsp;<\/span>Feature Importances<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Gradient-Boosting\" data-toc-modified-id=\"Gradient-Boosting-14.2\"><span class=\"toc-item-num\">14.2&nbsp;&nbsp;<\/span>Gradient Boosting<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Build-Gradient-Boost-Model\" data-toc-modified-id=\"Build-Gradient-Boost-Model-14.2.1\"><span class=\"toc-item-num\">14.2.1&nbsp;&nbsp;<\/span>Build Gradient Boost Model<\/a><\/span><\/li><li><span><a href=\"#Get-Score\" data-toc-modified-id=\"Get-Score-14.2.2\"><span class=\"toc-item-num\">14.2.2&nbsp;&nbsp;<\/span>Get Score<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-14.2.3\"><span class=\"toc-item-num\">14.2.3&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><li><span><a href=\"#Feature-Importances\" data-toc-modified-id=\"Feature-Importances-14.2.4\"><span class=\"toc-item-num\">14.2.4&nbsp;&nbsp;<\/span>Feature Importances<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#eXtreme-Gradient-Boosting\" data-toc-modified-id=\"eXtreme-Gradient-Boosting-14.3\"><span class=\"toc-item-num\">14.3&nbsp;&nbsp;<\/span>eXtreme Gradient Boosting<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Build-XGBoost-Model\" data-toc-modified-id=\"Build-XGBoost-Model-14.3.1\"><span class=\"toc-item-num\">14.3.1&nbsp;&nbsp;<\/span>Build XGBoost Model<\/a><\/span><\/li><li><span><a href=\"#Get-Score\" data-toc-modified-id=\"Get-Score-14.3.2\"><span class=\"toc-item-num\">14.3.2&nbsp;&nbsp;<\/span>Get Score<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-14.3.3\"><span class=\"toc-item-num\">14.3.3&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><li><span><a href=\"#Feature-Importances\" data-toc-modified-id=\"Feature-Importances-14.3.4\"><span class=\"toc-item-num\">14.3.4&nbsp;&nbsp;<\/span>Feature Importances<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-Score-Comparison\" data-toc-modified-id=\"Model-Score-Comparison-14.4\"><span class=\"toc-item-num\">14.4&nbsp;&nbsp;<\/span>Model Score Comparison<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-Tuning:-Boosting-Models\" data-toc-modified-id=\"Model-Tuning:-Boosting-Models-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;<\/span>Model Tuning: Boosting Models<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#AdaBoost-Tuning\" data-toc-modified-id=\"AdaBoost-Tuning-15.1\"><span class=\"toc-item-num\">15.1&nbsp;&nbsp;<\/span>AdaBoost Tuning<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Model-Tuning\" data-toc-modified-id=\"Model-Tuning-15.1.1\"><span class=\"toc-item-num\">15.1.1&nbsp;&nbsp;<\/span>Model Tuning<\/a><\/span><\/li><li><span><a href=\"#Get-Score\" data-toc-modified-id=\"Get-Score-15.1.2\"><span class=\"toc-item-num\">15.1.2&nbsp;&nbsp;<\/span>Get Score<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-15.1.3\"><span class=\"toc-item-num\">15.1.3&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><li><span><a href=\"#Feature-Importances\" data-toc-modified-id=\"Feature-Importances-15.1.4\"><span class=\"toc-item-num\">15.1.4&nbsp;&nbsp;<\/span>Feature Importances<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Gradient-Boost-Tuning\" data-toc-modified-id=\"Gradient-Boost-Tuning-15.2\"><span class=\"toc-item-num\">15.2&nbsp;&nbsp;<\/span>Gradient Boost Tuning<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Model-Tuning\" data-toc-modified-id=\"Model-Tuning-15.2.1\"><span class=\"toc-item-num\">15.2.1&nbsp;&nbsp;<\/span>Model Tuning<\/a><\/span><\/li><li><span><a href=\"#Get-Score\" data-toc-modified-id=\"Get-Score-15.2.2\"><span class=\"toc-item-num\">15.2.2&nbsp;&nbsp;<\/span>Get Score<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-15.2.3\"><span class=\"toc-item-num\">15.2.3&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><li><span><a href=\"#Feature-Importances\" data-toc-modified-id=\"Feature-Importances-15.2.4\"><span class=\"toc-item-num\">15.2.4&nbsp;&nbsp;<\/span>Feature Importances<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#XGB-Tuning\" data-toc-modified-id=\"XGB-Tuning-15.3\"><span class=\"toc-item-num\">15.3&nbsp;&nbsp;<\/span>XGB Tuning<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Model-Tuning\" data-toc-modified-id=\"Model-Tuning-15.3.1\"><span class=\"toc-item-num\">15.3.1&nbsp;&nbsp;<\/span>Model Tuning<\/a><\/span><\/li><li><span><a href=\"#Get-Score\" data-toc-modified-id=\"Get-Score-15.3.2\"><span class=\"toc-item-num\">15.3.2&nbsp;&nbsp;<\/span>Get Score<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-15.3.3\"><span class=\"toc-item-num\">15.3.3&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><li><span><a href=\"#Feature-Importances\" data-toc-modified-id=\"Feature-Importances-15.3.4\"><span class=\"toc-item-num\">15.3.4&nbsp;&nbsp;<\/span>Feature Importances<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-Score-Comparison\" data-toc-modified-id=\"Model-Score-Comparison-15.4\"><span class=\"toc-item-num\">15.4&nbsp;&nbsp;<\/span>Model Score Comparison<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-Building:-Stacking\" data-toc-modified-id=\"Model-Building:-Stacking-16\"><span class=\"toc-item-num\">16&nbsp;&nbsp;<\/span>Model Building: Stacking<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Stacking-Classifier\" data-toc-modified-id=\"Stacking-Classifier-16.1\"><span class=\"toc-item-num\">16.1&nbsp;&nbsp;<\/span>Stacking Classifier<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Build-Model-1\" data-toc-modified-id=\"Build-Model-1-16.1.1\"><span class=\"toc-item-num\">16.1.1&nbsp;&nbsp;<\/span>Build Model 1<\/a><\/span><\/li><li><span><a href=\"#Get-Score\" data-toc-modified-id=\"Get-Score-16.1.2\"><span class=\"toc-item-num\">16.1.2&nbsp;&nbsp;<\/span>Get Score<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-16.1.3\"><span class=\"toc-item-num\">16.1.3&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><li><span><a href=\"#Build-Model-2\" data-toc-modified-id=\"Build-Model-2-16.1.4\"><span class=\"toc-item-num\">16.1.4&nbsp;&nbsp;<\/span>Build Model 2<\/a><\/span><\/li><li><span><a href=\"#Get-Score\" data-toc-modified-id=\"Get-Score-16.1.5\"><span class=\"toc-item-num\">16.1.5&nbsp;&nbsp;<\/span>Get Score<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-16.1.6\"><span class=\"toc-item-num\">16.1.6&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><li><span><a href=\"#Build-Model-3\" data-toc-modified-id=\"Build-Model-3-16.1.7\"><span class=\"toc-item-num\">16.1.7&nbsp;&nbsp;<\/span>Build Model 3<\/a><\/span><\/li><li><span><a href=\"#Get-Score\" data-toc-modified-id=\"Get-Score-16.1.8\"><span class=\"toc-item-num\">16.1.8&nbsp;&nbsp;<\/span>Get Score<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-16.1.9\"><span class=\"toc-item-num\">16.1.9&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#All-Model-Score-Comparison\" data-toc-modified-id=\"All-Model-Score-Comparison-17\"><span class=\"toc-item-num\">17&nbsp;&nbsp;<\/span>All Model Score Comparison<\/a><\/span><\/li><li><span><a href=\"#Gain\/Lift-chart\" data-toc-modified-id=\"Gain\/Lift-chart-18\"><span class=\"toc-item-num\">18&nbsp;&nbsp;<\/span>Gain\/Lift chart<\/a><\/span><\/li><li><span><a href=\"#Cross-Validation\" data-toc-modified-id=\"Cross-Validation-19\"><span class=\"toc-item-num\">19&nbsp;&nbsp;<\/span>Cross Validation<\/a><\/span><\/li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-20\"><span class=\"toc-item-num\">20&nbsp;&nbsp;<\/span>Conclusion<\/a><\/span><\/li><li><span><a href=\"#Scopes-of-Model-Improvement\" data-toc-modified-id=\"Scopes-of-Model-Improvement-21\"><span class=\"toc-item-num\">21&nbsp;&nbsp;<\/span>Scopes of Model Improvement<\/a><\/span><\/li><li><span><a href=\"#Prediction-explanation-using-LIME\" data-toc-modified-id=\"Prediction-explanation-using-LIME-22\"><span class=\"toc-item-num\">22&nbsp;&nbsp;<\/span>Prediction explanation using LIME<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Check-a-few-predictions\" data-toc-modified-id=\"Check-a-few-predictions-22.1\"><span class=\"toc-item-num\">22.1&nbsp;&nbsp;<\/span>Check a few predictions<\/a><\/span><\/li><li><span><a href=\"#Check-the-data-where-classification-is-incorrect\" data-toc-modified-id=\"Check-the-data-where-classification-is-incorrect-22.2\"><span class=\"toc-item-num\">22.2&nbsp;&nbsp;<\/span>Check the data where classification is incorrect<\/a><\/span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#False-Positives\" data-toc-modified-id=\"False-Positives-22.2.0.1\"><span class=\"toc-item-num\">22.2.0.1&nbsp;&nbsp;<\/span>False Positives<\/a><\/span><\/li><li><span><a href=\"#False-Negatives\" data-toc-modified-id=\"False-Negatives-22.2.0.2\"><span class=\"toc-item-num\">22.2.0.2&nbsp;&nbsp;<\/span>False Negatives<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Conclusion-on-Feature-Importance\" data-toc-modified-id=\"Conclusion-on-Feature-Importance-23\"><span class=\"toc-item-num\">23&nbsp;&nbsp;<\/span>Conclusion on Feature Importance<\/a><\/span><\/li><li><span><a href=\"#Business-Recommendations-and-Insights\" data-toc-modified-id=\"Business-Recommendations-and-Insights-24\"><span class=\"toc-item-num\">24&nbsp;&nbsp;<\/span>Business Recommendations and Insights<\/a><\/span><\/li><\/ul><\/div>","ce700157":"### Confusion Matrix","03b04ab3":">**Observation**:\n>- Most of the customers are `salaried` or running `Small Business`. Together they contribute to `81%` of our customer base\n>- There are only `2 freelancers` who actually `purchased` a travel package\n>- Other than that, large business running people tend to opt for purchasing a package more than salaried and small business running people","9d96d388":"#### Monthly Income","3d64e774":"# Model Building: Random Forest","1b032c94":"## Standardizing column names","48090a5f":"### Confusion Matrix","3378dc0b":"### Heatmap to understand correlations between independent and dependent variables","9498d628":"### Visualize the Decision Tree","bcfbbeed":"### Confusion Matrix","0c27f180":">Trying to visualize the explanation of just 3 prediction by the Stacking Classifier model using `Tuned Gradient Boosting`, `Tuned Adaptive Boosting` and `Tuned Decision Tree` as base estimators and `Tuned Random Forest` as final estimator","d7b92c20":"## Fixing Gender value","00a568c6":"### Categorical variables vs. Number of Followups by Product Taken","b610535f":"## Treating Outliers","42551593":"### Build AdaBoost Model","b6e6418c":"### Get Score","696bdf14":"#### Pitch Satisfaction Score","9da3cc10":">- It is a full grown, over-fitting decision tree. And, it is super complex.\n>- Model is `over-fitting`\n>- Hyper-parameter tuning is required","82636e45":"### Build Model 2","688f8a11":"### Final check in Training and Testing dataset","3b9e9e03":"## View the first and last 5 rows of the dataset.","86c01206":">**Observation**:\n>- `Younger` people seem to have opted travel package slightly more than the older people\n>- People who had had `more follow-ups`, tend to purchase a package\n>- People who preferred `highly rated properties`, tend to purchase travel package","8f7aadec":">**Observation**:\n>- Most of the customers are from Tier 1 Cities\n>- Tier 2 and 3 residents seem to be slightly more interested in purchasing travel packages","66d6f3d5":">**Observation**:\n>- Most of the customers provide 3 stars as pitch satisfaction score","1e5ea5e9":">**Observation**:\n>- The gender column has discrepancy in value (`Female` and `Fe male`). We'll fix that in pre-EDA data processing phase.\n>- We'll re-visit the percentage distributions during EDA, and jot down the observations after looking at the distribution visuals.","4ea9cb27":"### Get Score","8cb32f0e":">**Observation**:\n>- Overall, the higher the rating of the property, more is the tendency to purchase the product.\n>- Customers in city tier 1 and 2 have the higher tendency to purchase travel package where the properties are rated higher.\n>- Customers who are salaried or running large business have the higher tendency to purchase travel package where the properties are rated higher.\n>- Male Customers have the higher tendency to purchase travel package where the properties are rated higher.\n>- Customers pitched about Standard, Deluxe and Basic packages have the higher tendency to purchase travel package where the properties are rated higher.\n>- Customers without passport have the higher tendency to purchase travel package where the properties are rated higher.\n>- Executive, Manager and Senior Managers have the higher tendency to purchase travel package where the properties are rated higher.\n>- Younger (18-40) Customers have the higher tendency to purchase travel package where the properties are rated higher.\n>- Customers with salary range 10K+ to 50K have the higher tendency to purchase travel package where the properties are rated higher.","91c48d49":"### Encoding Categorical Variables","d6492a23":">**Observation**:\n>There are missing values in the data.","cc118f56":"#### Marital Status","6ac7f897":"## Gradient Boost Tuning","9a3fbc8c":">**Observation**:\n>There are `4888` observations and `20` columns in the dataset","8f664661":"#### Age bin","43958402":"## Data type conversions","05dbfa63":"### Tuned Decision Tree Score","c71933d6":"### Confusion Matrix","b06d1fd0":">**Observation**:\n>- `Basic` and `Deluxe` are the two packages pitched to most of the customers\n>- People tend to purchase travel package more when they were pitched with `Basic` package, and `Standard` package","f83275b8":"Next, we train a decision tree using the effective alphas. The last value in `ccp_alphas` is the alpha value that prunes the whole tree, leaving the tree, clfs[-1], with one node.","ca4de17a":"### Confusion Matrix","dd87656a":"# Understand the data","77247c12":">- `Designation`, `Passport`, `City Tier` , `Martial status`, `Occupation` are most important features. `Income` and `Age` can also be looked into, some of the models have given higher importance to these.  \n>&nbsp;\n>- `Gender`, `number of children visiting`, `owning a car` appears to be insignificant.  \n>&nbsp;\n>- Customers with `Designation` as `Executive` should be the target customers for the company.  \n>&nbsp;\n>- Customers who `have passport` and are from `tier 3 city` and are `single or unmarried`, have `large business`, have higher chances of taking new package.  \n>&nbsp;\n>- Customers monthly income in range of `15000- 25000`, and age range `18-30`, prefer `5 star properties` also have higher chances of taking new package based on EDA.  \n>&nbsp;\n>- Based on EDA, the marketing\/sales people should also consider\n        - Having a higher duration of pitch by salesman to the customer.\n        - Getting a Pitch Satisfaction Score of 3 or 5.\n        - Multiple follow ups with the customers.\n>&nbsp;\n> These factors are expected improve the chance of selling a package.","51c37a13":"#### False Negatives","ea285072":"### Tuned Random Forest Scores","91bb04d3":"### Get Score","f4de77ae":"The `DecisionTreeClassifier` provides parameters such as `min_samples_leaf` and `max_depth` to prevent a tree from overfiting. `Cost complexity pruning` provides another option to control the size of a tree. In `DecisionTreeClassifier`, this pruning technique is parameterized by the `cost complexity parameter, ccp_alpha`. Greater values of `ccp_alpha` increase the number of nodes pruned.","df5fb9ce":"#### Occupation","9148da45":">**Observation**  \n>Trend for most of the customers who purchased `King` package:\n>- `Age Group`: 42 - 56  \n>- `Contacted by`: Self\n>- `Gender`: Female\n>- `City Tier`: 1  \n>- `Occupation`: Small Business  \n>- `Number of Persons Visiting`: 2 - 3  \n>- `Marital Status`: Single  \n>- `Designation`: VP  \n>- `Monthly Income Mean`: ~35K  ","1707bd03":">**Clearly, the misclassification was due to the fact that this person is a `Single` person, working as an `Executive` in a `Large Business`, who `prefers 5 star` properties, and thus, is expected to opt for a travel package**","5ed65981":">**Observation**:\n>- Number of Trips is highly right skewed.\n>- While most of the customers have taken 3 trips, there extreme values like 22 trips.","a725810c":"### Get Score","b2033604":">**Observation**:\n>- Monthly income is right skewed\n>- There are outliers on both extremities","4b657fc9":"### Build Gradient Boost Model","9ccb3813":"#### Passport","f26aba89":"## Check the data where classification is incorrect","058a6439":">**Observation**:\n>- Duration of Pitch is right skewed. \n>- There are some extreme right outliers\n>- The max duration is 127 minutes, which, compared to Q3 (20 minutes) is more than 6 times higher","2ea20cf6":"#### Number of Trips","aecf577a":"# Business Recommendations and Insights","407395d0":"## Decision Tree with Class Weight","3bdc776b":"### Confusion Matrix","f3b73de1":"#### Number of Person Visiting","55deef55":"### Build XGBoost Model","dab7b3fc":"### Categorical variables vs. Preferred Property Rating by Product Taken","f07fcec6":">**Observation**:\n>- Most of the customers `do not have a passport`\n>- People having passport purchased travel package more","d109a712":">**Observation**:\n>- Number of follow-ups are mostly 4\n>- There are outliers, only 1 or 6 follow-ups","c2195b4b":"### Categorical variables vs. Age by Product Taken","9479b1e4":"## Bi-variate Analysis","4dc1bc0b":"#### City Tier","b3e93fcf":"> Again, `well generalization`, but very `poor recall`","5c5deb2b":">**Observations**:\n>- Model is `over-fitting`\n>- `Recall` on test is only `47%`, even worse than the Bagging classifier. \n\n>**Points for improvement**:\n>- Hyper-parameter tuning is required","c610b2e6":"## Adaptive Boosting","8f9fadee":"### Overall statistics for `Standard` package","46383f11":"### Confusion Matrix","baa84c9e":"### Missing value imputation","ab73fc97":">**Observation**:\n>- Overall, the higher the number of follow-ups, more is the tendency to purchase the product.\n>- Customers in city tier 2 and 3 have the higher tendency to purchase travel package where the number of follow-ups are higher.\n>- Customers who are freelancers, salaried or running small business have the higher tendency to purchase travel package where the number of follow-ups are higher.\n>- Customers pitched about Basic, Deluxe and King packages have the higher tendency to purchase travel package where the number of follow-ups are higher.\n>- Executive, Manager and VPs have the higher tendency to purchase travel package where the number of follow-ups are higher.\n>- Customers with salary range 10K+ to 50K have the higher tendency to purchase travel package where the number of follow-ups are higher.","0f244729":"### Feature Importances","14dba7f6":">**Observation**:\n>- Most of the customers did self-enquiry\n>- However, whether customer chose to take a product or not, appears to be not dependent on type of contact","dde5a464":">**Observation**:\n>- Minimum `Age` of customer is 18 and Maximum age is 61 with mean of 37.\n>- Mean `Duration of pitch` is 15 minutes to max of 127 minutes.\n>- Mean `Number of trips` is 3 with maximum of 22. There might be outliers to be handled.\n>- Average `Monthly Income` of customer is 23619 with maximum of 98678. There might be outliers to be handled.\n>- Most of the customers did `self-enquiry`","9d9f113c":"## Check a few predictions","f25e249f":"## Split into training and testing dataset","3381997a":">**Observation**:\n>- Most of the customers had 1 child visitor","367a309f":">**Observation**:\n>- Most of the customers prefer 3 star accommodation.","b8937ca8":">**Observation**:\n>- Most of the customers are `Executives` and `Managers`\n>- `Executives` seems to have higher tendency of buying travel package","8bfab8d5":"## Model Score Comparison","9afc580b":">When ``ccp_alpha`` is set to zero and keeping the other default parameters\n>of `DecisionTreeClassifier`, the tree overfits.\n>As alpha increases, more\n>of the tree is pruned, thus creating a decision tree that generalizes better.","1afa6076":"# Data Pre-processing for Model Building","12f17407":"### Build Model 3","d0f2bb15":"### Confusion Matrix","63795b2b":"### Feature Importances","55214cef":"> The model appears to be `over-fitting`, the recall is bad as well.","7471b1be":">**Observation**:\n>- Most of the customers are `married`\n>- `Single` and `Unmarried` people tend to buy travel package more than other categories","46a025bc":"### Overall statistics for `Deluxe` package","9ae3f6e3":">- `Tuned Random Forest` provides a very generalized model, with descent `test recall` of `68%`  \n&nbsp;\n>- `Tuned Decision Tree` provides `best recall` on test of `79%`, however, the `test accuracy` is only `74%`, which is much lower than the base accuracy  \n&nbsp;\n>- `Stacking Classifier` with `Tuned Gradient Boosting`, `Tuned eXtreme Gradient Boosting` as base estimators and `Tuned Random Forest` as final estimator, provides `best recall` on test (along with `Tuned Decision Tree`), however, it does not generalize well, since there is a 10% difference between Train and Test accuracy  \n&nbsp;\n>- `Stacking Classifier` with `Tuned Gradient Boosting`, `Tuned Adaptive Boosting`, `Tuned Random Forest` and `Tuned Decision Tree` as base estimators and `Tuned eXtreme Gradient Boosting` as final estimator, provides `75% recall` on test and generalize well enough to close the gap between Train and Test accuracy to `7%`  \n&nbsp;\n>- `Stacking Classifier` with `Tuned Gradient Boosting`, `Tuned Adaptive Boosting` and `Tuned Decision Tree` as base estimators and `Tuned Random Forest` as final estimator, provides `77% recall` on test and generalize well enough to close the gap between Train (`92%`) and Test (`84%`) accuracy as well.  \n&nbsp;\n>-`Stacking Classifier` with `Tuned Gradient Boosting`, `Tuned eXtreme Gradient Boosting` as base estimators and `Tuned Random Forest` as final estimator, has a very good `95% confidence accuracy and recall range`.  \n        - Accuracy of the model at 95% confidence: 82.2% - 88.2%\n        - Recall of the model at 95% confidence: 70.6% - 86.0%","4ac0db71":"### Get Score","8ca69c4f":"## **Observations on Patterns**\n\n- Customer from `18 - 30` age purchased the product taken, followed by `31 - 40`.\n- Customers having `Monthly Income` of `15K - 25K` purchased product, followed by customers in income range `25K - 50K`.\n- Mostly customer visiting with `2 - 4 travelers` purchased some product\n- Most of the customers who bought the product were `Executive` and `Senior Manager`\n- Customers who were followed up `6 times` had purchases the product\n- `Company invited` customers mostly purchased packages and preferred `5 star rated` properties and were mostly from `city tier 2,3`.\n- Customers who were pitched `basic package` mostly brought the product, followed by `standard`. We can understand that this is because the cost involved.\n- Mostly Customers who `have passport` bought the product.\n- `FreeLancers` and `Large Business` owners have higher chance of purchasing the travel package. However, since there are only 2 data points for freelancers, this might be causing bias.\n- `Single` and `unmarried` people has higher chance of purchasing the travel package.\n- Customers who took `7 - 8 trips` had higher chances of purchasing a product.\n- `Gender, number of children visiting, having a car seem to be insignificant.`\n- Products were sold when `followed up many times`, with `higher duration of product pitch`, and when `Basic` Package was offered.","86a0bbad":"### Build Model","c4adb430":"Removing the spaces from column names, and standardizing the column names to lower case","64440c56":">We are now ready to start building models","310c0c0a":">**Observations**:\n>- The bagging classifier is `still over-fitting`\n>- `Recall on test is very poor`","86efc016":">**Observation**:\n>- Overall, customers with the income range 15K to 30K has higher tendency to purchase the product.\n>- Customers with higher income range, pitched about King and Super Deluxe packages have the higher tendency to purchase travel package\n>- VPs have higher income range, and the higher tendency to purchase travel package.\n>- Younger (18-40) Customers have the higher tendency to purchase travel package where the properties are rated higher.\n>- Customers with salary range 10K+ to 50K have the higher tendency to purchase travel package where the properties are rated higher.","f94df6a1":"# Model Building: Bagging","4c14e314":"## Null Handling","d470c55a":"#### Age","2d90f180":"### Get Score","5ee33816":"Let's check the outlier cases. \n\n**Important**:\n\nSince the `test` dataset is ideally unknown to us, we should only be able to detect and treat outliers in the `training` dataset.  \n\nWhen it comes to the test dataset, it is generally a good idea to find a model that can incorporate outliers. However depending on the reason for the analysis deleting or better yet transforming the outliers could be a better strategy.\n\nIf the outliers don't reflect reality because they are mistakes then deleting the records is fine. But the real world is filled with outliers, so if we want to model the real world, we can't delete the parts of it that we think don't belong.  \n\nI am making a choice of capping outliers in the training dataset, and not treating outliers in test dataset.","ca7dfb3d":">**Observation**:\n>- Almost `60%` of our customers are `Male`\n>- Tendency of purchasing a travel package doesn't seem to be impacted by gender of customer","6f749fd0":"**Verifying outlier treatment status**","b6b4ad10":"### Monthly Income and Age statistics per product pitched","fd399a4b":">**Comment**: Till now, the best model is `Tuned Random Forest`.   \n\n&nbsp;\n>**Improvement Scopes**:  \n>- We will tune the hyper-parameters in all the boosting models to get the best out of those, so that we can reduce over-fitting and bad recall problems.","55ead2f0":"# **insights based on EDA**","674ebac4":"We'll build below 2 functions to treat outliers by below rule\n\n- all the values smaller than Lower Whisker will be assigned value of Lower_whisker                                  \n- all the values above Upper Whisker will be assigned value of upper_Whisker ","ed06d779":"### Check Model Score","6b38b124":"## Bagging Classifier","3d3f574d":"The point of `stacking` (`Stacked Generalization`) is to explore a space of different models for the same problem. The idea is that you can attack a learning problem with different types of models which are capable to learn some part of the problem, but not the whole space of the problem. So, you can build multiple different learners and you use them to build an intermediate prediction, one prediction for each learned model. Then you add a new model which learns from the intermediate predictions the same target.  \n\nThis final model is said to be stacked on the top of the others, hence the name. Thus, you might improve your overall performance, and often you end up with a model which is better than any individual intermediate model. Notice however, that it does not give you any guarantee, as is often the case with any machine learning technique.","5b959a3d":">**Observation**:\n>- Most of the customers are in the `10K+ to 25K` monthly income range\n>- Customers earning `less than 10K or more than 50K` per month, did not buy travel package","7254f1f3":"### Confusion Matrix","da853b1d":"#### Type of Contact","3d2d6fc3":"### Get Score","e1dc9917":"### Visualizing the Decision Tree","afb3e288":">Since our goal is to increase the recall score, we'll use the above observation to determine optimal `ccp_alpha`","a5ea808c":"### Tuning Decision Tree","7b666a76":"### Tuning Random Forest Classifier Model","046a6d08":">- The tuned Gradient Boosting model is performing better than the base GBM.\n>- The recall is however still not better than the tuned random forest model\n>- This model does `not` appear to be `over-fitting`","693c7dde":"### Model Tuning","d6707beb":"## Summary of the dataset.","4333599b":"#### Number of Follow-ups","40f635f9":"#### Without outliers","81964bcd":"> Most important features appear to be `executive designation`, `passport status`, `city tier 3`","2c8014e3":">**Observation**:\n>- `~38%` customers are `Executive`, followed by `35%` `managers`.\n>- `~18%` customers `accepted product` offered last time.\n>- `~62%` customer `own car`.\n>- `~29%` customers has a `passport`.\n>- `~65%` customers are from `Tier 1` cities.\n>- `~61%` customers prefer `3 star` property.\n>- `~48%` customers are `married`.\n>- `Basic` package was pitched to `~38%` of customers and `35%` were pitched `Deluxe` package.\n>- `60%` customers are `male`.\n>- Occupation of `~49%` customer is `salaried`.\n>- `70.5%` customer `self-enquired` for the packages.\n>- `~67%` of customer monthly income is in `10K + to 25K` range.\n>- `~39%` are in `31-40` Age group. Only `~11%` customers are aged `50+`.","82fab889":">**Observation**  \n>Trend for most of the customers who purchased `Deluxe` package:\n>- `Age Group`: 21 - 44 \n>- `Contacted by`: Self\n>- `Gender`: Male\n>- `City Tier`: 3  \n>- `Occupation`: Small Business  \n>- `Number of Persons Visiting`: 2 - 3  \n>- `Marital Status`: Married  \n>- `Designation`: Manager  \n>- `Monthly Income Mean`: ~23K  ","6f7edf33":"**All of the above models are `over-fitting`. Decision tree gives the best test recall.**","f0d999c8":"### Important features\n\n>According to the bi-variate analysis so far, the important measures that might impact tendency of purchasing a travel package, are:\n>\n>- Age\/Age bin\n>- Number of follow-ups\n>- Preferred property rating\n>- Income\/Income bin\n>- City Tier\n>- Occupation\n>- Product Pitched\n>- Marital Status\n>- Passport\n>- Designation","efa01e15":"## Model Score Comparison","f799e297":"### Target vs. All Categorical Columns","2afac390":"### Count per Category for each type of product pitched","37942eb7":"In machine learning, `boosting` is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones.  \n\nWhile boosting is not algorithmically constrained, most boosting algorithms consist of iteratively learning weak classifiers with respect to a distribution and adding them to a final strong classifier. When they are added, they are weighted in a way that is related to the weak learners' accuracy. After a weak learner is added, the data weights are readjusted, known as `re-weighting`. Misclassified input data gain a higher weight and examples that are classified correctly lose weight. Thus, future weak learners focus more on the examples that previous weak learners misclassified.","4bf2015e":"### Confusion Matrix","d479c6b1":"The first step of univariate analysis is to check the distribution\/spread of the data. This is done using primarily `histograms` and `box plots`. Additionally we'll plot each numerical feature on `violin plot` and ` cumulative density distribution plot`. For these 4 kind of plots, we are building below `summary()` function to plot each of the numerical attributes. Also, we'll display feature-wise `5 point summary`.","ba0014fe":"### Get Score","5657baf8":"### Overall statistics for `King` package","a416a57a":"## Random Forest Classifier","83badc70":"#### Preferred Property Stars","c1393ce1":"### Build Model 1","f115763c":">**Observations**:\n>- The `accuracy on test data` improved, `~82%`\n>- The model is `not over-fitting`\n>- The `recall` is `~68%`\n>- The overall performance seems quite good\n>- Most important features: `passport status`, `age`, `monthly income`, `executive designation`","a6f1b6a0":">**Observation**:\n>- Most of the customers are in `31-40` years age range\n>- `18-30` years aged people have tendency to buy travel package compared to other age groups","c42f6147":"## Model Evaluation Criteria","8b848043":"## **Outliers**\n\n- `Monthly Income` and `Number of trips` have many outliers.\n- `Duration of Product pitch` has very extreme outliers.","942839a1":"## AdaBoost Tuning","74920cbd":">- The `accuracy` and `recall` both has `improved`\n>- This model still seems to `over-fit` though","d7e9675d":"# Scopes of Model Improvement\n\n>- Need to try more combinations of hyper-parameters to check further improvement possibilities  \n&nbsp;\n>- Need to try combinations of weak learners in stacking classifier to check model performance gain\/loss","7962cd36":"Let's check the categorical feature-wise value distributions by count","4fc336cf":"## Bagging Classifier Model Tuning","603038aa":"`Random forests` or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned.","08b035f2":"#### Product Pitched","3bd69e94":"> 4 Most important features appear to be: `passport status`, `executive designation`, `marital status single`, `city tier 3`","a38aebd2":"## Univariate Analysis","9646ea39":"Goal of `Bi-variate` analysis is to find inter-dependencies between features.","895d4fb4":"We are going to try a few more combinations to see if the scores can be improved.","6c952d74":"#### Gender","921f89e6":"For the categorical variables, it is best to analyze them at percentage of total on bar charts\nBelow function takes a category column as input and plots bar chart with percentages on top of each bar","c3639eaf":"### Tuning Bagging Classifier Model","b1755528":"#### Duration of Pitch","e5b631f3":"### Feature Importances","924dc885":">`Stacking Classifier` with `Tuned Gradient Boosting`, `Tuned Adaptive Boosting`, `Tuned Random Forest` and `Tuned Decision Tree` as base estimators and `Tuned eXtreme Gradient Boosting` as final estimator","afa4ab52":"### Overall statistics for `Super Deluxe` package","5b90f87a":"# Model Building: Stacking","79688a7f":"### Visualize the Tree","2e39782d":"### Build the model","20591aa4":"> Most important features: `monthly income`, `age`, `number if trips`","f73c2692":"## Decision Tree Model Tuning","98d78142":">**Observation**  \n>Trend for most of the customers who purchased `Standard` package:\n>- `Age Group`: 33 - 49 \n>- `Contacted by`: Self\n>- `Gender`: Male\n>- `City Tier`: 3  \n>- `Occupation`: Small Business  \n>- `Number of Persons Visiting`: 2 - 3  \n>- `Marital Status`: Married  \n>- `Designation`: Senior Managers  \n>- `Monthly Income Mean`: ~26K   ","b64eeefa":"## Customer Profiling for Purchased Products","c5b4fa19":"We'll first check `very high outliers` (`number of trips > 10` and `monthly income > 40K Or <10K`) ","37e04683":"### Feature Importances","4b69701c":"### Feature Importances","0b454726":"## Model Evaluation Functions - Scoring & Confusion Matrix","1309466c":"### Feature Importances","f74548d6":"# Problem Statement\n\nThe Policy Maker of the `Visit with us` travel company wants to enable and establish a viable business model to expand the customer base.\n\nA viable business model is a central concept that helps you to understand the existing ways of doing the business and how to change the ways for the benefit of the tourism sector.\n\nOne of the ways to expand the customer base is to introduce a new offering of packages.\n\nCurrently, there are 5 types of packages the company is offering - Basic, Standard, Deluxe, Super Deluxe, King. Looking at the data of the last year, we observed that 18% of the customers purchased the packages.\n\nHowever, the marketing cost was quite high because customers were contacted at random without looking at the available information.\n\nThe company is now planning to launch a new product i.e. Wellness Tourism Package. Wellness Tourism is defined as Travel that allows the traveler to maintain, enhance or kick-start a healthy lifestyle, and support or increase one's sense of well-being.\n\nHowever, this time company wants to harness the available data of existing and potential customers to make the marketing expenditure more efficient.\n\n`We need to analyze the customers' data and information to provide recommendations to the Policy Maker and Marketing Team and also build a model to predict the potential customer who is going to purchase the newly introduced travel package.`","95686df3":"### Build Model","a800e9d4":">There is still one record in test data with NAN in number of trips","91471a0c":"### Target vs. All numerical columns","09220f80":"Let's now check how the accuracy of the model changes with increasing `ccp_alpha` values for train and test datasets","fa6046b2":"## Outliers detection using Box plot","fcae31b2":">**Observation**  \n>Trend for most of the customers who purchased `Basic` package:\n>- `Age Group`: 25 - 35 \n>- `Contacted by`: Self\n>- `Gender`: Male\n>- `City Tier`: 1  \n>- `Occupation`: Salaried  \n>- `Number of Persons Visiting`: 2 - 3  \n>- `Marital Status`: Single  \n>- `Designation`: Executives  \n>- `Monthly Income Mean`: ~20K  ","d8197290":"# Prediction explanation using LIME","9f398276":">**Observation**:\n>- Overall, the lower the age, more is the tendency to purchase the product.\n>- Older Customers pitched about King package have the higher tendency to purchase travel package\n>- VPs, Managers and Senior Managers have the higher tendency to purchase travel package when the age is higher. This is because age is a bit correlated with designation.\n>- Older Customers with salary range 25K+ to 50K have the higher tendency to purchase travel package. Again this is because age is slightly correlated with income.","ce1a0fdc":"## Drop the columns that are not useful for the model","aebf5230":"Treating the outliers","496286a7":"### Get Score","41d3f591":"# Exploratory Data Analysis","d96f5e51":"## Stacking Classifier","6c2a61f9":"## Check the data types of the columns for the dataset.","fa9c2bed":">**Observation**  \n>Trend for most of the customers who purchased `Super Deluxe` package:\n>- `Age Group`: 39 - 45  \n>- `Contacted by`: Company\n>- `Gender`: Male\n>- `City Tier`: 3  \n>- `Occupation`: Salaried  \n>- `Number of Persons Visiting`: 2 - 3  \n>- `Marital Status`: Single  \n>- `Designation`: AVP  \n>- `Monthly Income Mean`: ~30K  ","aecfe7bd":">**Observation**:\n>- `Number of Persons Visting` as expected has good positive correlation with `Number of Children Visting`\n>- `Monthly Income` shows moderate positive correlation with `Age`","bfdf5d19":"### Feature Importances","9a278f19":"### Model Tuning","d273c5d1":"### Bar plot of Monthly Income for each type of product pitched","2f7b1d82":"## Decision Tree","f01ade5e":"## Categorical column statistics","c3a0f48d":"### Percentage on bar chart for Categorical Features","8d33b839":"# All Model Score Comparison","7a9ded06":">`Stacking Classifier` with `Tuned Gradient Boosting`, `Tuned Adaptive Boosting` and `Tuned Decision Tree` as base estimators and `Tuned Random Forest` as final estimator","7923cbd8":"# Model Tuning: Boosting Models","2eb6a59a":"# Conclusion","1417fb6d":"### Build Model","ae8ecb46":"## Understand the shape of the dataset.","25b33321":"# Model Building: Boosting","94df4eb7":"### Confusion Matrix","214ae8e7":">**Clearly, the misclassification was due to the fact that this person is a `Single` person, `salaried` worker as a `Manager`, who although `prefers 5 star` properties, has a lower monthly income, and have done only 2 trips per year, `does not` have a `passport`, and thus highly unlikely to purchase a travel package**","b557700f":"## Create bins for Age and Monthly Income","4b8c333b":"### Visualize the Decision Tree","04aee44f":">The group used is consisting of Gender, Marital Status, Occupation and Designation. In ideal situation, these are the driving factors to determine what kind of holiday property a person would be comfortable with, how many trips the person does in a year, and what is the age of the person.","4e3c20b8":">- Excluding the outliers, it appears, people with income range of `18K to 24K` monthly, tend to purchase travel package a lot","07c00fec":"### Model can make wrong predictions as:\n\n1. Predicting a person will buy a package but he\/she actually doesn't - **Loss of Resource**\n2. Predicting a person will not buy a package but he\/she actually does - **Loss of Opportunity**\n\n### Which case is more important? \n\n<span style='background:yellow'\/>The whole purpose of the campaign is to bring in more customers. 2nd case is more important to us. If a potential customer is missed by the sales\/marketing team, it will be loss of opportunity. The evaluation criteria should be minimal `Loss of Opportunity`.\n\n### How to reduce this loss i.e need to reduce False Negatives?\n\n`Recall` should be maximized, the greater the `Recall`, higher the chances of identifying both the classes correctly.","c5fe6a4a":">**Observation**\n>There are overall 6 extreme cases in the training dataset out of 3421 records.\n\n&nbsp;\n\n>We will treat all the outliers using capping, as opposed to deleting the extreme cases.","32bd7fac":"> The tuned AdaBoost model is performing better than the base AdaBoost model. Recall has improved. However, looking at the accuracy scores, it appears it is `still over-fitting` a bit.","bf40a371":">**Comment**:\n> Although the tuned decision tree gives highest test recall, since the accuracy is the lowest of all models, the best model till this point appears to be the `Tuned Random Forest` model.","51920753":">**Observations**:\n>- Model is `over-fitting`\n>- `Recall` on test is only `50%`, worse than the decision tree. \n\n>**Points for improvement**:\n>- Hyper-parameter tuning is required","90a985bc":">**Observation**:\n>- Most of the customers `own car`\n>- Owning a car does not seem to have any impact on purchasing travel package tendency","6d29968f":"# Conclusion on Feature Importance\n\n\n>- Looking at feature importance, `Designation`, `Passport`, `City Tier` , `Martial status`, `Occupation` are most important features. `Income` and `Age` can also be looked into, some of the models have given higher importance to these.  \n&nbsp;\n>- `Gender`, `number of children visiting`, `owning a car` appears to be insignificant.","0fb35dca":"### Feature Importances","b3665db1":"### Bar plot of Age for each type of product pitched","f8db9d77":"For `Customer Profiling` we'll be looking into the `packages\/products offered` for `each of the categories` to find patterns.","d67689f6":"### Feature Importances","a4d88d3b":"### Categorical variables vs. Monthly Income by Product Taken (Including outliers)","f3e20908":"### Check training set nulls","4ecdf74e":"#### Own Car","24ada868":"### Get Score","54e5d44d":"#### Income bin","c0106634":"### Confusion Matrix","72f9956f":"## Random Forest Model Tuning","9872e6fe":"### Get Score","55b27439":"### Model Tuning","86a524e9":"## **Columns to be dropped**\n- Since the new customers will be first analyzed via model and then contacted, the customer interaction data would not be available. It is sensible to drop the features related to customer interactions. Those columns\/features are:\n`durationofpitch`, `numberoffollowups`, `productpitched`, `pitchsatisfactionscore`.\n- `typeofcontact` can also be dropped since the new customers will all be contacted by the company.\n- `income_bin` and `age_bin` to be dropped since these were derived for performing the EDA","898a2c03":"Let's now check how the recall of the model changes with increasing `ccp_alpha` values for train and test datasets","adba4e4b":">**Observation**:  \n>- There are almost `5%` nulls in `DurationofPitch`, `MonthlyIncome`, and `Age` columns  \n>- `NumberOfTrips` has `~3%` null values  \n>- `NumberOfChildrenVisiting` has some null values, but that might mean there were no vising children in the group  \n>- `NumberOfFollowups`, `PreferredProprtyStar` and `TypeOfContact` have less than `1%` null values","ca927552":"### Check training and testing set median by a group","b4c6caaf":"### Get Score","65662099":"# Objective\n\nTo predict which customer is more likely to purchase the newly introduced travel package.","25fef126":"We are creating a few functions to score the models, show the confusion matrix","1e671368":"# Initial Set-up","49cef70a":"### Numerical Feature Summary","3e6cd44f":"### Confusion Matrix","79fa380d":"## **Customer Profile**\n\n**`Basic`**:\n- `Age Group`: 25 - 35 \n- `Contacted by`: Self\n- `Gender`: Male\n- `City Tier`: 1  \n- `Occupation`: Salaried   \n- `Marital Status`: Single  \n- `Designation`: Executives  \n- `Monthly Income`: ~20K (<25K)\n\n&nbsp;\n**`Standard`**:\n- `Age Group`: 33 - 49 \n- `Contacted by`: Self\n- `Gender`: Male\n- `City Tier`: 3  \n- `Occupation`: Small Business   \n- `Marital Status`: Married  \n- `Designation`: Senior Managers  \n- `Monthly Income`: ~26K (<30K)\n\n&nbsp; \n**`Deluxe`**:\n- `Age Group`: 21 - 44 \n- `Contacted by`: Self\n- `Gender`: Male\n- `City Tier`: 3  \n- `Occupation`: Small Business  \n- `Marital Status`: Married  \n- `Designation`: Manager  \n- `Monthly Income`: ~23K (<25K)\n\n&nbsp;\n**`Super Deluxe`**:\n- `Age Group`: 39 - 45  \n- `Contacted by`: Company\n- `Gender`: Male\n- `City Tier`: 3  \n- `Occupation`: Salaried   \n- `Marital Status`: Single  \n- `Designation`: AVP  \n- `Monthly Income`: ~30K (<30K) \n\n&nbsp;\n**`King`**:\n- `Age Group`: 42 - 56  \n- `Contacted by`: Self\n- `Gender`: Female\n- `City Tier`: 1  \n- `Occupation`: Small Business   \n- `Marital Status`: Single  \n- `Designation`: VP  \n- `Monthly Income`: ~35K (<35K) ","1bf7fe61":"> Most important features according to AdaBoost model are, `monthly income` and `age`","6eacbb82":"## Model Score Comparison","43677340":"### Confusion Matrix","8b07fed2":"> Most important features appear to be `monthly income`, `passport status`, and `age`","9998d9bf":"## eXtreme Gradient Boosting","856d50cc":"Since I am interested to check the cross validation scores for a few more models.","615fa8e1":"## Model Score Comparison","9c4d17e4":"# Cross Validation","6d02df63":"> CCP does not appear to be helpful since the best alpha value selected is 0, so we are basically back to the base model.","4231900d":"# Model Building","d94dd08c":"#### False Positives","135b7541":"`Bootstrap aggregating`, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid over-fitting.","dffad861":"### Pairplot of all available numeric columns, hued by Personal Loan","d728c410":"For the remainder, we remove the last element in `clfs` and `ccp_alphas`, because it is the trivial tree with only one node. Here we show that the number of nodes and tree depth decreases as alpha increases.","91b03541":"> We are not checking anything more on this since the performance is not good. We'll move on to tune Random Forest model.","e34298e1":"### Tuned Bagging Classifier Scores","87c33435":">**Observation**:\n>- Age looks normally distributed, with a lot of spikes though\n>- There are no outliers","f47ed55e":"## Multi-variate Plots","7ed851b2":">**Observation**:\n>- Number of visitors is mostly 3 persons\n>- There is an outlier of 5 persons","f8b57248":"Using only Designation, Gender and Marital Status grouping to find the median and impute the remaining missing value"}}