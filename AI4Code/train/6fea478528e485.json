{"cell_type":{"9514ab1e":"code","f8e1716b":"code","d7106a40":"code","4702651b":"code","fe186a33":"code","5f61603f":"code","b7300a08":"code","930d27d8":"code","8d685e8d":"code","3e4ee200":"code","7df5fcf3":"code","85c3b604":"code","57b3b00e":"code","ca060401":"code","b53ac801":"code","d87b34ce":"code","a63ccc59":"markdown","5a40486a":"markdown"},"source":{"9514ab1e":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f8e1716b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt","d7106a40":"df=pd.read_csv(r\"..\/input\/kc-house-data\/kc_house_data.csv\")\ndf.head()","4702651b":"df.shape","fe186a33":"df.isnull().sum()","5f61603f":"dateTime=pd.DatetimeIndex(df.date)\ndf['year']=dateTime.year\ndf['month']=dateTime.month\ndf['day']=dateTime.day\ndf.drop(columns=['date'],inplace=True)\ndf.columns","b7300a08":"df.describe()","930d27d8":"df.drop(columns=['id']).plot(kind='box',figsize=(50,10))","8d685e8d":"df.hist(figsize=(30,20))\nplt.show()","3e4ee200":"plt.figure(figsize=(12,10))\nsns.heatmap(df.corr(),cmap='Reds')","7df5fcf3":"priceTransform=np.log(df.price)\npriceTransform.plot(kind='hist')","85c3b604":"def RegressionAlgorithms(df,outputVariable,transformationY=np.array):\n    class color:\n        PURPLE = '\\033[95m'\n        CYAN = '\\033[96m'\n        DARKCYAN = '\\033[36m'\n        BLUE = '\\033[94m'\n        GREEN = '\\033[92m'\n        YELLOW = '\\033[93m'\n        RED = '\\033[91m'\n        BOLD = '\\033[1m'\n        UNDERLINE = '\\033[4m'\n        END = '\\033[0m'\n        \n    from sklearn.model_selection import train_test_split\n    X_train,X_test,y_train,y_test=train_test_split(df[df.columns.difference([outputVariable])],transformationY(df[outputVariable]),random_state=0)\n    \n    #linear Regression\n    print(color.PURPLE + color.BOLD + color.UNDERLINE + 'LINEAR REGRESSION\\n' + color.END)\n    from sklearn.metrics import mean_absolute_error\n    import statsmodels.api as sm\n    X_train = sm.add_constant(X_train)\n    X_test = sm.add_constant(X_test)\n    model = sm.OLS(y_train,X_train).fit()\n    print(model.summary())\n    pred=model.predict(X_test)\n    print(f\"\\nMean Absolute Error For Test : {mean_absolute_error(y_test,pred)}\\n\")\n    pred=model.predict(X_train)\n    print(f\"\\nMean Absolute Error For Train: {mean_absolute_error(y_train,pred)}\\n\\n\\n\")\n    \n    #Decision Tree Regression\n    print(color.CYAN + color.BOLD + color.UNDERLINE + 'DECISION TREE REGRESSION\\n' + color.END)\n    from sklearn.tree import DecisionTreeRegressor\n    model=DecisionTreeRegressor()\n    model.fit(X_train,y_train)\n    pred=model.predict(X_test)\n    print(f\"\\nMean Absolute Error For Test : {mean_absolute_error(y_test,pred)}\\n\")\n    pred=model.predict(X_train)\n    print(f\"\\nMean Absolute Error For Train : {mean_absolute_error(y_train,pred)}\\n\\n\\n\")\n    \n    #Random Forest Regression\n    print(color.DARKCYAN + color.BOLD + color.UNDERLINE + 'RANDOM FOREST REGRESSION\\n' + color.END)\n    from sklearn.ensemble import RandomForestRegressor\n    model=RandomForestRegressor()\n    model.fit(X_train,y_train)\n    pred=model.predict(X_test)\n    print(f\"\\nMean Absolute Error For Test : {mean_absolute_error(y_test,pred)}\\n\")\n    pred=model.predict(X_train)\n    print(f\"\\nMean Absolute Error For Train : {mean_absolute_error(y_train,pred)}\\n\\n\\n\")\n    \n    #StandardScaler\n    from sklearn.preprocessing import StandardScaler\n    ss=StandardScaler()\n    ssTrain=ss.fit_transform(X_train)\n    ssTest=ss.transform(X_test)\n    \n    #Support Vector Regression\n    print(color.BLUE + color.BOLD + color.UNDERLINE + 'SUPPORT VECTOR REGRESSION\\n' + color.END)\n    from sklearn.svm import SVR\n    model=SVR()\n    model.fit(ssTrain,y_train)\n    pred=model.predict(ssTest)\n    print(f\"\\nMean Absolute Error For Test : {mean_absolute_error(y_test,pred)}\\n\")\n    pred=model.predict(ssTrain)\n    print(f\"\\nMean Absolute Error For Train : {mean_absolute_error(y_train,pred)}\\n\\n\\n\")\n    \n    #KNN\n    print(color.GREEN + color.BOLD + color.UNDERLINE + 'K NEAREST NEIGHBOUR\\n' + color.END)\n    from sklearn.neighbors import KNeighborsRegressor\n    model=KNeighborsRegressor()\n    model.fit(ssTrain,y_train)\n    pred=model.predict(ssTest)\n    print(f\"\\nMean Absolute Error For Test : {mean_absolute_error(y_test,pred)}\\n\")\n    pred=model.predict(ssTrain)\n    print(f\"\\nMean Absolute Error For Train : {mean_absolute_error(y_train,pred)}\\n\\n\\n\")","57b3b00e":"RegressionAlgorithms(df,'price',np.log)","ca060401":"def LinearRegressionFunction(df,y=['price']): \n    from sklearn.model_selection import train_test_split\n    X_train,X_test,y_train,y_test=train_test_split(df[df.columns.difference(y)],priceTransform,random_state=0)\n    from sklearn.metrics import mean_absolute_error\n    import statsmodels.api as sm\n    X_train = sm.add_constant(X_train)\n    X_test = sm.add_constant(X_test)\n    model = sm.OLS(y_train,X_train).fit()\n    print(model.summary())\n    pred=model.predict(X_test)\n    print(f\"\\nMean Absolute Error For Test : {mean_absolute_error(y_test,pred)}\\n\")\n    pred=model.predict(X_train)\n    print(f\"\\nMean Absolute Error For Train: {mean_absolute_error(y_train,pred)}\\n\\n\\n\")\n","b53ac801":"LinearRegressionFunction(df)","d87b34ce":"LinearRegressionFunction(df,['price','id'])","a63ccc59":"> Going with **LINEAR REGRESSION** because it doesnt overfit the data as most of the algorithms are doing. And gives not bad accuracy","5a40486a":">There Is Something Wrong With Kaggle Kernel, Because The Same Thing Run Perfectly On My Jupyter Notebook."}}