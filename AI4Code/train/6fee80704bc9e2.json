{"cell_type":{"c4fd669c":"code","925f181a":"code","232bb7a1":"code","970c346c":"code","afed9e32":"code","17e30c73":"code","40ad2e3f":"code","85c8d8f6":"code","99fc0f94":"code","72f21ae7":"code","ff3cc2c1":"code","8f2f6199":"code","0fc3030e":"code","b1bef12a":"code","8c79c9e5":"code","649484ca":"code","ebe45512":"code","d3cc2cb3":"code","670de042":"code","d72a5520":"markdown","52874387":"markdown","6e6941c0":"markdown","9553828d":"markdown","35a6ae3a":"markdown","0d8cbeaa":"markdown","ac53b79b":"markdown","46b14531":"markdown","79f27677":"markdown","e62ed98b":"markdown","ef469ca4":"markdown","81be75d7":"markdown","9ef7148c":"markdown"},"source":{"c4fd669c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\nfrom glob import glob\nimport sys\nimport random\n\nfrom tqdm import tqdm_notebook\nfrom skimage.io import imread, imshow\nfrom skimage.transform import resize\nfrom sklearn.metrics import jaccard_similarity_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.datasets as dsets\nfrom torch.autograd import Variable","925f181a":"# Set some parameters# Set s \nim_width = 128\nim_height = 128\nim_chan = 1\npath_train = '..\/input\/train'\npath_test = '..\/input\/test'\n\ntrain_path_images = os.path.abspath(path_train + \"\/images\/\")\ntrain_path_masks = os.path.abspath(path_train + \"\/masks\/\")\n\ntest_path_images = os.path.abspath(path_test + \"\/images\/\")\ntest_path_masks = os.path.abspath(path_test + \"\/masks\/\")","232bb7a1":"train_path_images_list = glob(os.path.join(train_path_images, \"*.png\"))\ntrain_path_masks_list = glob(os.path.join(train_path_masks, \"*.png\"))\ntest_path_images_list = glob(os.path.join(test_path_images, \"*.png\"))\ntest_path_masks_list = glob(os.path.join(test_path_masks, \"*.png\"))","970c346c":"ids= ['1f1cc6b3a4','5b7c160d0d','6c40978ddf','7dfdf6eeb8','7e5a6e5013']\nplt.figure(figsize=(20,10))\nfor j, img_name in enumerate(ids):\n    q = j+1\n    img = imread(train_path_images + \"\/\" + img_name + '.png')\n    img_mask = imread(train_path_masks + \"\/\" + img_name + '.png')\n    \n    plt.subplot(1,2*(1+len(ids)),q*2-1)\n    plt.imshow(img)\n    plt.subplot(1,2*(1+len(ids)),q*2)\n    plt.imshow(img_mask)\nplt.show()","afed9e32":"train_ids = next(os.walk(train_path_images))[2]\ntest_ids = next(os.walk(test_path_images))[2]","17e30c73":"# Get and resize train images and masks\nX_train = np.zeros((len(train_ids), im_height, im_width, im_chan), dtype=np.uint8)\nY_train = np.zeros((len(train_ids), im_height, im_width, 1), dtype=np.bool_)\nprint('Getting and resizing train images and masks ... ')\nsys.stdout.flush()\nfor n, id_ in tqdm_notebook(enumerate(train_ids), total=len(train_ids)):\n    img = imread(path_train + '\/images\/' + id_)\n    x = resize(img, (128, 128, 1), mode='constant', preserve_range=True)\n    X_train[n] = x\n    mask = imread(path_train + '\/masks\/' + id_)\n    Y_train[n] = resize(mask, (128, 128, 1), \n                        mode='constant', \n                        preserve_range=True)\n\nprint('Done!')","40ad2e3f":"# Check if training data looks all right\nix = random.randint(0, len(train_ids))\nplt.imshow(np.dstack((X_train[ix],X_train[ix],X_train[ix])))\nplt.show()\ntmp = np.squeeze(Y_train[ix]).astype(np.float32)\nplt.imshow(np.dstack((tmp,tmp,tmp)))\nplt.show()","85c8d8f6":"# https:\/\/stackoverflow.com\/questions\/50052295\/how-do-you-load-images-into-pytorch-dataloader\nclass saltIDDataset(torch.utils.data.Dataset):\n\n    def __init__(self,preprocessed_images,train=True, preprocessed_masks=None):\n        \"\"\"\n        Args:\n            text_file(string): path to text file\n            root_dir(string): directory with all train images\n        \"\"\"\n        self.train = train\n        self.images = preprocessed_images\n        if self.train:\n            self.masks = preprocessed_masks\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]\n        mask = None\n        if self.train:\n            mask = self.masks[idx]\n        return (image, mask)","99fc0f94":"X_train_shaped = X_train.reshape(-1, 1, 128, 128)\/255\nY_train_shaped = Y_train.reshape(-1, 1, 128, 128)","72f21ae7":"X_train_shaped = X_train_shaped.astype(np.float32)\nY_train_shaped = Y_train_shaped.astype(np.float32)","ff3cc2c1":"torch.cuda.manual_seed_all(4200)\nnp.random.seed(133700)","8f2f6199":"indices = list(range(len(X_train_shaped)))\nnp.random.shuffle(indices)\n\nval_size = 1\/10\nsplit = np.int_(np.floor(val_size * len(X_train_shaped)))\n\ntrain_idxs = indices[split:]\nval_idxs = indices[:split]","0fc3030e":"salt_ID_dataset_train = saltIDDataset(X_train_shaped[train_idxs], \n                                      train=True, \n                                      preprocessed_masks=Y_train_shaped[train_idxs])\nsalt_ID_dataset_val = saltIDDataset(X_train_shaped[val_idxs], \n                                      train=True, \n                                      preprocessed_masks=Y_train_shaped[val_idxs])\n\nbatch_size = 16\n\ntrain_loader = torch.utils.data.DataLoader(dataset=salt_ID_dataset_train, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\nval_loader = torch.utils.data.DataLoader(dataset=salt_ID_dataset_val, \n                                           batch_size=batch_size, \n                                           shuffle=False)","b1bef12a":"class double_conv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n        super(double_conv, self).__init__()\n        self.conv = nn.Sequential(\n                    nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n                              stride=stride, padding=padding),\n                    nn.BatchNorm2d(out_channels),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size,\n                              stride=stride, padding=padding),\n                    nn.BatchNorm2d(out_channels),\n                    nn.ReLU(inplace=True))\n        \n    def forward(self, x):\n        x = self.conv(x)\n        return x\n        \nstart_fm = 16\n\nclass Unet(nn.Module):\n    \n    def __init__(self):\n        super(Unet, self).__init__()\n        \n        # Input 128x128x1\n        \n        #Contracting Path\n        \n        #(Double) Convolution 1        \n        self.double_conv1 = double_conv(1, start_fm, 3, 1, 1)\n        #Max Pooling 1\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n        \n        #Convolution 2\n        self.double_conv2 = double_conv(start_fm, start_fm * 2, 3, 1, 1)\n        #Max Pooling 2\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n        \n        #Convolution 3\n        self.double_conv3 = double_conv(start_fm * 2, start_fm * 4, 3, 1, 1)\n        #Max Pooling 3\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n        \n        #Convolution 4\n        self.double_conv4 = double_conv(start_fm * 4, start_fm * 8, 3, 1, 1)\n        #Max Pooling 4\n        self.maxpool4 = nn.MaxPool2d(kernel_size=2)\n        \n        #Convolution 5\n        self.double_conv5 = double_conv(start_fm * 8, start_fm * 16, 3, 1, 1)\n        \n        #Transposed Convolution 4\n        self.t_conv4 = nn.ConvTranspose2d(start_fm * 16, start_fm * 8, 2, 2)\n        # Expanding Path Convolution 4 \n        self.ex_double_conv4 = double_conv(start_fm * 16, start_fm * 8, 3, 1, 1)\n        \n        #Transposed Convolution 3\n        self.t_conv3 = nn.ConvTranspose2d(start_fm * 8, start_fm * 4, 2, 2)\n        #Convolution 3\n        self.ex_double_conv3 = double_conv(start_fm * 8, start_fm * 4, 3, 1, 1)\n        \n        #Transposed Convolution 2\n        self.t_conv2 = nn.ConvTranspose2d(start_fm * 4, start_fm * 2, 2, 2)\n        #Convolution 2\n        self.ex_double_conv2 = double_conv(start_fm * 4, start_fm * 2, 3, 1, 1)\n        \n        #Transposed Convolution 1\n        self.t_conv1 = nn.ConvTranspose2d(start_fm * 2, start_fm, 2, 2)\n        #Convolution 1\n        self.ex_double_conv1 = double_conv(start_fm * 2, start_fm, 3, 1, 1)\n        \n        # One by One Conv\n        self.one_by_one = nn.Conv2d(start_fm, 1, 1, 1, 0)\n        #self.final_act = nn.Sigmoid()\n        \n        \n    def forward(self, inputs):\n        # Contracting Path\n        conv1 = self.double_conv1(inputs)\n        maxpool1 = self.maxpool1(conv1)\n\n        conv2 = self.double_conv2(maxpool1)\n        maxpool2 = self.maxpool2(conv2)\n\n        conv3 = self.double_conv3(maxpool2)\n        maxpool3 = self.maxpool3(conv3)\n\n        conv4 = self.double_conv4(maxpool3)\n        maxpool4 = self.maxpool4(conv4)\n            \n        # Bottom\n        conv5 = self.double_conv5(maxpool4)\n        \n        # Expanding Path\n        t_conv4 = self.t_conv4(conv5)\n        cat4 = torch.cat([conv4 ,t_conv4], 1)\n        ex_conv4 = self.ex_double_conv4(cat4)\n        \n        t_conv3 = self.t_conv3(ex_conv4)\n        cat3 = torch.cat([conv3 ,t_conv3], 1)\n        ex_conv3 = self.ex_double_conv3(cat3)\n\n        t_conv2 = self.t_conv2(ex_conv3)\n        cat2 = torch.cat([conv2 ,t_conv2], 1)\n        ex_conv2 = self.ex_double_conv2(cat2)\n        \n        t_conv1 = self.t_conv1(ex_conv2)\n        cat1 = torch.cat([conv1 ,t_conv1], 1)\n        ex_conv1 = self.ex_double_conv1(cat1)\n        \n        one_by_one = self.one_by_one(ex_conv1)\n        \n        return one_by_one","8c79c9e5":"model = Unet()\nmodel.cuda();\n\ncriterion = nn.BCEWithLogitsLoss()\n\nlearning_rate = 1e-3\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","649484ca":"mean_train_losses = []\nmean_val_losses = []\nfor epoch in range(12):\n    train_losses = []\n    val_losses = []\n    for images, masks in train_loader:        \n        images = Variable(images.cuda())\n        masks = Variable(masks.cuda())\n        \n        outputs = model(images)        \n        \n        loss = criterion(outputs, masks)\n        train_losses.append(loss.data)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    for images, masks in val_loader:\n        images = Variable(images.cuda())\n        masks = Variable(masks.cuda())\n        \n        outputs = model(images)\n        loss = criterion(outputs, masks)\n        val_losses.append(loss.data)\n    \n    mean_train_losses.append(np.mean(train_losses))\n    mean_val_losses.append(np.mean(val_losses))\n    # Print Loss\n    print('Epoch: {}. Train Loss: {}. Val Loss: {}'.format(epoch+1, np.mean(train_losses), np.mean(val_losses)))","ebe45512":"train_loss_series = pd.Series(mean_train_losses)\nval_loss_series = pd.Series(mean_val_losses)\ntrain_loss_series.plot(label=\"train\")\nval_loss_series.plot(label=\"validation\")\nplt.legend()","d3cc2cb3":"y_pred_true_pairs = []\nfor images, masks in val_loader:\n    images = Variable(images.cuda())\n    y_preds = model(images)\n    for i, _ in enumerate(images):\n        y_pred = y_preds[i] \n        y_pred = torch.sigmoid(y_pred)\n        y_pred = y_pred.cpu().data.numpy()\n        y_pred_true_pairs.append((y_pred, masks[i].numpy()))","670de042":"# https:\/\/www.kaggle.com\/leighplt\/goto-pytorch-fix-for-v0-3\nfor threshold in np.linspace(0, 1, 11):\n    \n    ious = []\n    for y_pred, mask in y_pred_true_pairs:\n        prediction = (y_pred > threshold).astype(int)\n        iou = jaccard_similarity_score(mask.flatten(), prediction.flatten())\n        ious.append(iou)\n        \n    accuracies = [np.mean(ious > iou_threshold)\n                 for iou_threshold in np.linspace(0.5, 0.95, 10)]\n    print('Threshold: %.1f, Metric: %.3f' % (threshold, np.mean(accuracies)))","d72a5520":"# Define U-Net Model","52874387":"We use a method to calculate the IOU score as found in this kernel here: https:\/\/www.kaggle.com\/leighplt\/goto-pytorch-fix-for-v0-3.\n","6e6941c0":"You may need to tweak your batch_size based on how much memory you have on your GPU.","9553828d":"This kernel demonstrates a quick and naive implementation of a U-Net in Pytorch, trained on a GPU. Enjoy!","35a6ae3a":"# Image Preparation","0d8cbeaa":"Here's the meat of the kernel, where we define our U-Net architecture. See also this excellent kernel from an older challenge: https:\/\/www.kaggle.com\/mlagunas\/naive-unet-with-pytorch-tensorboard-logging.","ac53b79b":"Finally we compute our IOU score for various thresholds. ","46b14531":"# Prepare Images for Pytorch","79f27677":"The following image preparation was taken almost verbatim from another excellent kernel of the TGS Salt Identification challenge: https:\/\/www.kaggle.com\/jesperdramsch\/intro-to-seismic-salt-and-how-to-geophysics. I take no credit for it.  ","e62ed98b":"The following is what allows us to easily use our with Pytorch. We create a class with the following methods which then allows us to use a DataLoader.","ef469ca4":"We set a random seed for reproducibility.","81be75d7":"We define a BCEWithLogitsLoss since we're comparing pixel by pixel. In addition, we didn't include a final sigmoid activation as this loss function includes a sigmoid for us.","9ef7148c":"We note that around 11-13 epochs is when we start to worry about overfitting to our training data as we see a rise in our validation loss."}}