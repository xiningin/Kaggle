{"cell_type":{"87f8e86b":"code","fca41f1c":"code","54ec19f2":"code","25a4bfd9":"code","ab9a646f":"code","fc021ddd":"code","61e59a51":"code","05018e2c":"code","30a40269":"code","3beea6c2":"code","f3286df0":"code","5dbcfb6c":"code","c025d674":"code","7bf7368a":"code","902afc48":"markdown","fe87d1cf":"markdown"},"source":{"87f8e86b":"!pip install torchsummary","fca41f1c":"\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\n\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision import datasets, models, transforms\n\nfrom torchsummary import summary\n\nimport os, time, sys","54ec19f2":"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Block_LN(nn.Module):\n    '''expand + depthwise + pointwise'''\n    def __init__(self, in_planes, out_planes, expansion, stride):\n        super(Block_LN, self).__init__()\n        self.stride = stride\n\n        planes = expansion * in_planes\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn1 = nn.GroupNorm(1, planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, groups=planes, bias=False)\n        self.bn2 = nn.GroupNorm(1, planes)\n        self.conv3 = nn.Conv2d(planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn3 = nn.GroupNorm(1, out_planes)\n\n        self.shortcut = nn.Sequential()\n        if stride == 1 and in_planes != out_planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.GroupNorm(1, out_planes),\n            )\n\n    def forward(self, x):\n        out = F.relu6(self.bn1(self.conv1(x)))\n        out = F.relu6(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out = out + self.shortcut(x) if self.stride==1 else out\n#         out = out + x if self.stride==1 else out\n        return out\n\n\nclass MobileNetv2LN(nn.Module):\n    # (expansion, out_planes, num_blocks, stride)\n    cfg = [(1,  16, 1, 1),\n           (6,  24, 2, 2), \n           (6,  32, 3, 2),\n           (6,  64, 4, 2),\n           (6,  96, 3, 1),\n           (6, 160, 3, 2),\n           (6, 320, 1, 1)]\n\n    def __init__(self, num_classes=3):\n        super(MobileNetv2LN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.GroupNorm(1, 32)\n        self.layers = self._make_layers(in_planes=32)\n        self.conv2 = nn.Conv2d(320, 160, kernel_size=1, stride=1, padding=0, bias=False)\n        # NOTE: output channel of origin MobileNet v2 : 1280 \/\/ Ours : 160\n        self.bn2 = nn.GroupNorm(1, 160)\n        self.linear = nn.Linear(160, num_classes)\n\n    def _make_layers(self, in_planes):\n        layers = []\n        for expansion, out_planes, num_blocks, stride in self.cfg:\n            strides = [stride] + [1]*(num_blocks-1)\n            for stride in strides:\n                layers.append(Block_LN(in_planes, out_planes, expansion, stride))\n                in_planes = out_planes\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu6(self.bn1(self.conv1(x)))\n        out = self.layers(out)\n        out = F.relu6(self.bn2(self.conv2(out)))\n        out = F.avg_pool2d(out, 7)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n","25a4bfd9":"## util\n\n# _, term_width = os.popen('stty size', 'r').read().split()\nterm_width = 80\n\nTOTAL_BAR_LENGTH = 65.\nlast_time = time.time()\nbegin_time = last_time\ndef progress_bar(current, total, msg=None):\n    global last_time, begin_time\n    if current == 0:\n        begin_time = time.time()  # Reset for new bar.\n\n    cur_len = int(TOTAL_BAR_LENGTH*current\/total)\n    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n\n    sys.stdout.write(' [')\n    for i in range(cur_len):\n        sys.stdout.write('=')\n    sys.stdout.write('>')\n    for i in range(rest_len):\n        sys.stdout.write('.')\n    sys.stdout.write(']')\n\n    cur_time = time.time()\n    step_time = cur_time - last_time\n    last_time = cur_time\n    tot_time = cur_time - begin_time\n\n    L = []\n    L.append('  Step: %s' % format_time(step_time))\n    L.append(' | Tot: %s' % format_time(tot_time))\n    if msg:\n        L.append(' | ' + msg)\n\n    msg = ''.join(L)\n    sys.stdout.write(msg)\n    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n        sys.stdout.write(' ')\n\n    # Go back to the center of the bar.\n    for i in range(term_width-int(TOTAL_BAR_LENGTH\/2)+2):\n        sys.stdout.write('\\b')\n    sys.stdout.write(' %d\/%d ' % (current+1, total))\n\n    if current < total-1:\n        sys.stdout.write('\\r')\n    else:\n        sys.stdout.write('\\n')\n    sys.stdout.flush()\n\ndef format_time(seconds):\n    days = int(seconds \/ 3600\/24)\n    seconds = seconds - days*3600*24\n    hours = int(seconds \/ 3600)\n    seconds = seconds - hours*3600\n    minutes = int(seconds \/ 60)\n    seconds = seconds - minutes*60\n    secondsf = int(seconds)\n    seconds = seconds - secondsf\n    millis = int(seconds*1000)\n\n    f = ''\n    i = 1\n    if days > 0:\n        f += str(days) + 'D'\n        i += 1\n    if hours > 0 and i <= 2:\n        f += str(hours) + 'h'\n        i += 1\n    if minutes > 0 and i <= 2:\n        f += str(minutes) + 'm'\n        i += 1\n    if secondsf > 0 and i <= 2:\n        f += str(secondsf) + 's'\n        i += 1\n    if millis > 0 and i <= 2:\n        f += str(millis) + 'ms'\n        i += 1\n    if f == '':\n        f = '0ms'\n    return f","ab9a646f":"\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nbest_acc = 0  # best test accuracy\nbest_loss = 1000\nbest_acc_idx = 0\nbest_loss_idx = 0\nstart_epoch = 0  # start from epoch 0 or last checkpoint epoch\n\n","fc021ddd":"learning_rate = 0.001\ncosine_tmax=200\nbatch_size=128\nepochs=200\ndropout=0.3\narchitecture='mobilenetv2_ln'","61e59a51":"data_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=(0.2, 2), \n                            contrast=(0.3, 2), \n                            saturation=(0.2, 2), \n                            hue=(-0.3, 0.3)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\n","05018e2c":"path = \"\/kaggle\/input\/large-covid19-ct-slice-dataset\/curated_data\/curated_data\/\"","30a40269":"image_datasets = datasets.ImageFolder(path, data_transforms)\n\nclasses = image_datasets.classes\ntrain_size = int(0.8*len(image_datasets))\ntest_size = len(image_datasets)-train_size\n\ntrain_dataset, test_dataset = torch.utils.data.random_split(\n    image_datasets, [train_size, test_size])\n\ntrainloader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\ntestloader = torch.utils.data.DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n","3beea6c2":"net = MobileNetv2LN()\nnet = net.to(device)\nprint(summary(net,(3,224,224)))","f3286df0":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=learning_rate,\n                      momentum=0.9, weight_decay=5e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cosine_tmax)\n\n# Creates a GradScaler once at the beginning of training.\nscaler = torch.cuda.amp.GradScaler()","5dbcfb6c":"# Training\ndef train(epoch):\n    print('\\nEpoch: %d' % epoch)\n\n    net.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    closs=0\n    for batch_idx, (inputs, targets) in enumerate(trainloader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        \n        with torch.cuda.amp.autocast():\n            outputs = net(inputs)\n            loss = criterion(outputs, targets)\n        \n        scaler.scale(loss).backward()\n#         loss.backward()\n        scaler.step(optimizer)\n#         optimizer.step()\n        # Updates the scale for next iteration.\n        scaler.update()\n        \n        closs=closs+loss.item()\n#         wandb.log({\"batch loss\":loss.item()})\n\n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n\n        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d\/%d)'\n                     % (train_loss \/ (batch_idx + 1), 100. * correct \/ total, correct, total))\n#     wandb.log({\"loss\":closs\/config.batch_size})\n\ndef test(epoch):\n    global best_acc, best_loss, best_acc_idx, best_loss_idx\n    net.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    example_images=[]\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(testloader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = net(inputs)\n            loss = criterion(outputs, targets)\n\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n#             example_images.append(wandb.Image(\n#                 inputs[0], caption=\"Pred: {} Truth: {}\".format(predicted[0].item(), targets[0])))\n            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d\/%d)'\n                         % (test_loss \/ (batch_idx + 1), 100. * correct \/ total, correct, total))\n    print({\n#         \"Examples\": example_images,\n        \"Test Accuracy\": 100. * correct \/ len(testloader.dataset),\n        \"Test Loss\": test_loss\n    })\n\n    # Save checkpoint.\n    acc = 100. * correct \/ total\n    if acc > best_acc:\n        print('Saving best accuracy model..')\n        state = {\n            'net': net.state_dict(),\n            'acc': acc,\n            'epoch': epoch,\n        }\n        if not os.path.isdir('checkpoint'):\n            os.mkdir('checkpoint')\n        torch.save(state, '.\/checkpoint\/ckpt.pth')\n        best_acc = acc\n        best_acc_idx = epoch\n\n    if test_loss < best_loss:\n        print('Saving best loss model..')\n        state = {\n            'net': net.state_dict(),\n            'acc': acc,\n            'loss': test_loss,\n            'epoch': epoch,\n        }\n        if not os.path.isdir('checkpoint'):\n            os.mkdir('checkpoint')\n        torch.save(state, '.\/checkpoint\/ckpt_loss.pth')\n        best_loss = test_loss\n        best_loss_idx = epoch\n        \n    print({\n        \"best_acc\": best_acc,\n        \"best_acc_idx\": best_acc_idx,\n        \"best_loss\": best_loss,\n        \"best_loss_idx\" : best_loss_idx\n    })","c025d674":"for epoch in range(start_epoch, start_epoch + epochs):\n    train(epoch)\n    test(epoch)\n    scheduler.step()","7bf7368a":"test(47)","902afc48":"Because training takes too long, we evaluate the performance of the best model ended at 47 epochs.","fe87d1cf":"## MobileNet v2 with LayerNorm"}}