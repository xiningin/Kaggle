{"cell_type":{"d5fbaf2e":"code","24a96e11":"code","c12f4fe4":"code","4fce789e":"code","3c477f17":"code","65dc9699":"code","3c5ddab8":"code","973b56c2":"code","c8cba61f":"code","5ba92484":"code","82130d99":"code","c5aca5d1":"code","16a468eb":"code","792865f5":"code","067b9f97":"code","347c3e6a":"code","8496f9cf":"code","1e356d97":"code","8d4e99b3":"markdown","5e92a53e":"markdown","7f9db851":"markdown","a13f5352":"markdown","49f6e7e8":"markdown","3c71cfb9":"markdown","8f11cd00":"markdown","d4c44e79":"markdown","e856083f":"markdown"},"source":{"d5fbaf2e":"import numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport spacy\nimport wordcloud\nimport pandas as pd\nimport os\nimport re\nimport tensorflow as tf\nfrom tqdm import tqdm\nimport random\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.losses import *\nfrom tensorflow.keras.metrics import *\nfrom tensorflow.keras.activations import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.metrics import classification_report","24a96e11":"df1 = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/True.csv')\ndf1['Class'] = 1\ndf2 = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv')\ndf2['Class'] = 0\ndf = pd.concat([df1, df2], axis=0, join=\"inner\")\ndf = df.sample(frac=1).reset_index(drop=True)","c12f4fe4":"df","4fce789e":"train_df = df[:35000]\nval_df = df[35000:]","3c477f17":"wordcloud = wordcloud.WordCloud(stopwords=wordcloud.STOPWORDS, max_font_size=80, max_words=5000,\n                      width = 600, height = 400,\n                      background_color='black').generate(' '.join(txt for txt in df[\"title\"]))\nfig, ax = plt.subplots(figsize=(10,7))\nax.imshow(wordcloud, interpolation='bilinear')\nax.set_axis_off()\nplt.imshow(wordcloud);","65dc9699":"df['Class'].hist()","3c5ddab8":"fig = px.bar(x = np.unique(df[\"subject\"]),\ny = [list(df[\"subject\"]).count(i) for i in np.unique(df[\"subject\"])] , \n            color = np.unique(df[\"subject\"]),\n             color_continuous_scale=\"Emrld\") \nfig.update_xaxes(title=\"Subject\")\nfig.update_yaxes(title = \"Number of Rows\")\nfig.update_layout(showlegend = False,\n    title = {\n        'text': 'News Subject Distribution',\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\nfig.show()","973b56c2":"def clean_text(text):\n    chars = ',<.>\/?;:\"[{}]|)(*&^%$#@!=+-_0123456789'\n    text = text.lower()\n    for l in chars:\n        text = text.replace(l, str(' '+l+' '))\n        text = text.replace('  ',' ')\n    return text","c8cba61f":"unique = []\nmaxlen = 0\nfor x in tqdm(range(len(df))):\n    i = df['title'][x]\n    i = i.split('.')\n    for k in i:\n        k = k.split(' ')\n        if len(k)>maxlen:\n            maxlen = len(k)\n        for j in k:\n            if j not in unique:\n                unique.append(j)\nprint('Maxlen:', maxlen)\nprint('Unique:', len(unique))","5ba92484":"def encode_text(text, maxlen=maxlen):\n    new_text = []\n    for x in text:\n        if x in unique:\n            new_text.append(unique.index(x)+1)\n        else:\n            new_text.append(0)\n    new_text = pad_sequences([np.array(new_text)], maxlen=maxlen, dtype='float', padding='post', value=0.0)\n    return new_text[0]","82130d99":"def datagen(dataframe, batch_size=32):\n    for x in range(0, len(dataframe), batch_size):\n        i = dataframe[x:x+batch_size]\n        texts = i['title']\n        t = []\n        for j in texts:\n            a = clean_text(j).split(' ')\n            a = encode_text(a)\n            t.append(a)\n        labels = i['Class']\n        l = []\n        for j in labels:\n            a = float(j)\n            l.append(a)\n        yield np.array(t), np.array(l).reshape(-1,1)\n        ","c5aca5d1":"model = Sequential()\nmodel.add(Input(shape=(maxlen)))\nmodel.add(Embedding(len(unique), 192))\nmodel.add(LSTM(96, activation='relu', return_sequences=True))\nmodel.add(LSTM(192, activation='relu', return_sequences=False))\nmodel.add(Dense(192, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(96, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\n\nmodel.summary()","16a468eb":"model.compile(optimizer=Adam(learning_rate=0.0001),\n             loss='sparse_categorical_crossentropy',\n             metrics=['sparse_categorical_accuracy'])","792865f5":"batch_size=32\nsteps = int(len(train_df)\/batch_size)\nepochs = 3\nfor _ in range(epochs):\n    model.fit(datagen(train_df, batch_size=batch_size), epochs=1, steps_per_epoch=steps)","067b9f97":"model.save('model.h5')","347c3e6a":"decode_label = {0:'Fake', 1:'Real'}","8496f9cf":"batch_size=32\nsteps = int(len(val_df)\/batch_size)\ny_pred = []\ny_true = []\nfor x,y in tqdm(datagen(val_df, batch_size=batch_size), total=steps):\n    pred = model.predict(x)\n    pred = np.argmax(pred, axis=-1)\n    y = [decode_label[int(i)] for i in y]\n    pred = [decode_label[int(i)] for i in pred]\n    for i in pred:\n        y_pred.append(i)\n    for i in y:\n        y_true.append(i)","1e356d97":"print(classification_report(y_pred, y_true))","8d4e99b3":"# Model","5e92a53e":"# Train-Val Split","7f9db851":"# Evaluate Model","a13f5352":"# Imports","49f6e7e8":"# Data Generator","3c71cfb9":"# Data Preprocessing","8f11cd00":"**The Data is reasonably balanced**","d4c44e79":"# Train Model","e856083f":"# Data Analysis"}}