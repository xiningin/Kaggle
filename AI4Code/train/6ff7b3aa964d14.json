{"cell_type":{"76083efb":"code","49d50382":"code","646d6ec6":"code","88ef3179":"code","39358b95":"code","50341a90":"code","ae60308c":"code","fa216365":"code","ff287a33":"code","18b4b940":"code","c514700c":"code","4c93bc1a":"code","4f32c803":"code","8164d934":"code","f2ede039":"code","f64df3c0":"code","a84e9be1":"code","2f2787c6":"code","8ad2a7c7":"code","68c77782":"code","1da8b9d9":"code","cc84bdd2":"code","573fdc04":"code","6a787ed3":"code","6ca4bccd":"code","d9b30f7b":"code","3195ec44":"code","59a95f9b":"code","049ae3f4":"code","828ae9ee":"code","4e20ccf3":"code","3fcc195f":"code","4cc5f574":"markdown","4be53059":"markdown","60158607":"markdown","32e47a22":"markdown","ab2e7088":"markdown","48107f17":"markdown","82b0e85b":"markdown","a15cb125":"markdown","e368eef3":"markdown","e3d37372":"markdown","21d0562c":"markdown","86a12716":"markdown","144a6888":"markdown","dc0d0c44":"markdown","e5d06f3c":"markdown","65edba44":"markdown","b3c5272d":"markdown","c0bbaa52":"markdown","fb929a7d":"markdown","6915456c":"markdown","2e28be8d":"markdown","6409daf2":"markdown","16feb5e5":"markdown","0a65790c":"markdown","960a448e":"markdown"},"source":{"76083efb":"#importing common libraries  \nimport os , random\nimport datetime\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, \nimport matplotlib.pyplot as plt # ploting\nimport seaborn as sns # visualisation \nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nimport catboost as cat\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import GridSearchCV\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set_style(\"ticks\")\n","49d50382":"train_data = pd.read_csv(\"..\/input\/tabular-playground-series-jul-2021\/train.csv\"  ) # reading the train data to a data frame \ntest_data = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/test.csv' ) # reading the test data into a data frame \nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/sample_submission.csv') # reading the test data into a data frame\nprint(\" data imported keep going....\")","646d6ec6":"train_data.head()\ntrain_data.tail()","88ef3179":"train_data.shape #7111,12\ntrain_data.info()","39358b95":"\ntrain_data.isnull().sum() # no null values so we can continue\n","50341a90":"# describing the data beautifully\ntrain_data.describe().T.style.bar().background_gradient().background_gradient()","ae60308c":"corrMatrix =train_data.corr(method='pearson', min_periods=1)\ncorrMatrix ","fa216365":"sns.set(rc={\"figure.figsize\":(10, 8)})\nsns.heatmap(corrMatrix, cmap=\"YlGnBu\",annot=True)\nplt.show()","ff287a33":"#Variation\ntrain_data.var()","18b4b940":"#Standard  deviation  \ntrain_data.std()","c514700c":"train_data.hist(bins=10,color='#A0E8AF',figsize=(16,12))\nplt.show()","4c93bc1a":"sns.set(rc={\"figure.figsize\":(14, 6)})\nplot = train_data.iloc[:,:9]\nsns.boxplot(data=plot)\n","4f32c803":"features = train_data.iloc[:,1:9]\nfeatures ","8164d934":"# we will look into the features distribution now, to get insight into the data\ni = 1\nplt.figure()\nfig, ax = plt.subplots(5, 3,figsize=(14, 24))\nfor feature in features:\n    plt.subplot(3, 3,i)\n    sns.distplot(train_data[feature],color=\"blue\", kde=True,bins=120, label='train')\n    sns.distplot(test_data[feature],color=\"red\", kde=True,bins=120, label='test')\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()\n\n","f2ede039":"targets = [\"target_carbon_monoxide\", \"target_benzene\", \"target_nitrogen_oxides\"]\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(5, 1), facecolor='#f6f6f4')\ngs = fig.add_gridspec(1, 3)\ngs.update(wspace=0.2, hspace=0.5)\n\nbackground_color = \"#f6f5f5\"\n\nrun_no = 0\nfor row in range(0, 1):\n    for col in range(0, 3):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        locals()[\"ax\"+str(run_no)].tick_params(axis='y', which=u'both',length=0)\n        locals()[\"ax\"+str(run_no)].set_yticklabels([])\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nrun_no = 0\nfor col in targets:\n    sns.kdeplot(train_data[col], ax=locals()[\"ax\"+str(run_no)], shade=True, color='darkblue', alpha=0.95, linewidth=0, zorder=2)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=5, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=5, width=0.5, length=1.5)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEF', linewidth=0.7)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEF', linewidth=0.7)\n    run_no += 1\n    \nax0.text(-1.2, 0.44, 'Target Distribution', fontsize=8, fontweight='bold')\nplt.show()","f64df3c0":"test_data.head()\ntest_data.tail()","a84e9be1":"test_data.shape #2247,9\ntest_data.info()","2f2787c6":"test_data.describe().T.style.bar().background_gradient().background_gradient()","8ad2a7c7":"train_data.set_index('date_time')\ntest_data.set_index('date_time' )","68c77782":"#concating the data \nall_df = pd.concat([train_data, test_data]).reset_index(drop = True)\nprint(all_df.shape)","1da8b9d9":"# Some Features Engineering check discussion for more .\nall_df['sensor_7'] = (all_df['sensor_3'] - all_df['sensor_4']) \/ all_df['sensor_4']\nall_df['Dew_Point'] = 243.12*(np.log(all_df['relative_humidity'] * 0.01) + (17.62 * all_df['deg_C'])\/(243.12+all_df['deg_C']))\/(17.62-(np.log(all_df['relative_humidity'] * 0.01)+17.62*all_df['deg_C']\/(243.12+all_df['deg_C'])))\nall_df['SMC'] = (all_df['absolute_humidity'] * 100) \/ all_df['relative_humidity']\nall_df['temperature_lag_3'] = all_df['deg_C'] - all_df['deg_C'].shift(periods=3, fill_value=0)\nall_df['temperature_lag_6'] = all_df['deg_C'] - all_df['deg_C'].shift(periods=6, fill_value=0)\nall_df['Partial_pressure'] = 243.12*(np.log(all_df['absolute_humidity'] * 0.01) + (17.62 * all_df['deg_C'])\/(243.12+all_df['deg_C']))\/(17.62-(np.log(all_df['relative_humidity'] * 0.01)+17.62*all_df['deg_C']\/(243.12+all_df['deg_C'])))\nall_df ['Saturated_wvd'] = (all_df ['absolute_humidity'] * 100) \/ all_df ['relative_humidity']\nall_df['humidity_lag_3'] = all_df['absolute_humidity'] - all_df['absolute_humidity'].shift(periods=3, fill_value=0)\nall_df['humidity_lag_6'] = all_df['absolute_humidity'] - all_df['absolute_humidity'].shift(periods=6, fill_value=0)\n#droping the sensor3 since it has the minimal correlaion with the targets \nall_df.drop(['sensor_3'],axis=1,inplace=True)\n# date_time_features \nall_df['date_time'] = pd.to_datetime(all_df['date_time'])\nall_df['month'] = all_df['date_time'].dt.month\nall_df['week'] = all_df['date_time'].dt.week\nall_df['day'] = all_df['date_time'].dt.day\nall_df['hour'] = all_df['date_time'].dt.hour\nall_df[\"working_hours\"] =  all_df[\"hour\"].isin(np.arange(8, 21, 1)).astype(\"int\")\nall_df[\"quarter\"] = all_df[\"date_time\"].dt.quarter\nall_df[\"is_weekend\"] = (all_df[\"date_time\"].dt.dayofweek >= 5).astype(\"int\")\nall_df.shape","cc84bdd2":"# seperating the train and test data again \ntrain, test = all_df.iloc[:(len(all_df) - len(test_data)), :], all_df.iloc[(len(all_df) - len(test_data)):, :]\nprint(train.shape, test.shape)","573fdc04":"# converting date time to unix time\ntrain['date_time'] = train['date_time'].astype('datetime64[ns]').astype(np.int64)\/10**9\ntest['date_time'] = test['date_time'].astype('datetime64[ns]').astype(np.int64)\/10**9\n# preparing targets \nlabels = ['target_carbon_monoxide','target_benzene','target_nitrogen_oxides'] \ntest = test.drop(columns= labels)\nX_train= train.drop(columns=labels)  \ny_carbon_monoxide = train['target_carbon_monoxide']\ny_benzene = train['target_benzene']\ny_nitrogen_oxides = train['target_nitrogen_oxides']","6a787ed3":"#paramaters found using gridsearch \nxgb_submission = sample_submission.copy()\n#######define fit and predict for carbon_monoxide ######\nxgboost = XGBRegressor( colsample_bytree=0.7, \n                       learning_rate = 0.03 ,\n                       n_estimators=500, \n                       subsample=0.7,\n                       alpha=0.9) # define \nxgboost.fit(X_train, y_carbon_monoxide) #fit\nxgb_submission['target_carbon_monoxide'] = xgboost.predict(test) #predict \n######### fit and predict for benzen ########\nxgboost.fit(X_train, y_benzene) #fit\nxgb_submission['target_benzene'] = xgboost.predict(test) #predict\n######fit and predict for nitrogen_oxide#######\nxgboost.fit(X_train, y_nitrogen_oxides) #fit\nxgb_submission['target_nitrogen_oxides'] = xgboost.predict(test) #predict \nxgb_submission.head()","6ca4bccd":"lgb1 = LGBMRegressor()\nparameters = { \n               'objective' :['regression'], \n                'max_depth' : [5,7,9],\n                'learning_rate': [.001, 0.05, .07], \n                'n_estimators': [500,700,300],\n                'num_leaves':[30,40,50],\n                ' max_bin':[55,35,75],\n                'bagging_seed' :[7,9,5],\n                                    }\n\nlgb_grid = GridSearchCV(lgb1,\n                        parameters,\n                        cv = 2,\n                        n_jobs = 5,\n                        verbose=True)","d9b30f7b":"lgb_grid.fit(X_train,\n         y_nitrogen_oxides)\nprint(lgb_grid.best_score_)\nprint(lgb_grid.best_params_)","3195ec44":"lgb_submission = sample_submission.copy()\nlightgbm =LGBMRegressor(    \n                                       objective='regression', \n                                       max_depth = 9,\n                                       num_leaves=30,\n                                       learning_rate=0.07, \n                                       n_estimators=300,\n                                       max_bin=75, \n                                       bagging_seed=7,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       )\nlightgbm.fit(X_train, y_carbon_monoxide)\nlgb_submission['target_carbon_monoxide'] = lightgbm.predict(test) #predict\n#####\nlightgbm.fit(X_train, y_benzene) #fit\nlgb_submission['target_benzene'] = lightgbm.predict(test) #predict\n####\nlightgbm.fit(X_train, y_nitrogen_oxides) #fit\nlgb_submission['target_nitrogen_oxides'] = lightgbm.predict(test) #predict \nlgb_submission.head()","59a95f9b":"catb1 = CatBoostRegressor()\nparams = {'iterations': [500,300,100],\n          'depth': [4, 5, 6],\n         'learning_rate': [.001, .04, .07], \n          'l2_leaf_reg': np.logspace(-20, -19, 3),\n          'leaf_estimation_iterations': [10],\n          'eval_metric': ['RMSE'],\n          'logging_level':['Silent'],\n          'random_seed': [42]\n         }\n\ncatb_grid = GridSearchCV(catb1,\n                        params,\n                        cv = 2,\n                        n_jobs = 5,\n                        verbose=True)\ncatb_grid.fit(X_train,\n         y_benzene)\n","049ae3f4":"print(catb_grid.best_score_)\nprint(catb_grid.best_params_)","828ae9ee":"cat_submission = sample_submission.copy()\ncat_boost = CatBoostRegressor(\n                 learning_rate = 0.04,\n                  iterations =300,\n                 l2_leaf_reg = 1e-20,\n                 bagging_temperature=4,\n                 leaf_estimation_iterations=10,\n                 random_strength = 1.5,\n                 depth= 4,\n                 random_seed = 42,\n                 eval_metric = 'RMSE',\n                 logging_level='Silent' ,           \n               \n)\n         \ncat_boost.fit(X_train, y_carbon_monoxide)\ncat_submission['target_carbon_monoxide'] = cat_boost.predict(test) #predict\n#####\ncat_boost.fit(X_train, y_benzene) #fit\ncat_submission['target_benzene'] = cat_boost.predict(test) #predict\n####\ncat_boost.fit(X_train, y_nitrogen_oxides) #fit\ncat_submission['target_nitrogen_oxides'] = cat_boost.predict(test) #predict \ncat_submission.head()","4e20ccf3":"ensembe_sub = sample_submission.copy()\nensembe_sub['target_carbon_monoxide'] = 0.4*lgb_submission['target_carbon_monoxide'] + 0.2*cat_submission['target_carbon_monoxide'] + 0.4*xgb_submission['target_carbon_monoxide'] \nensembe_sub['target_benzene'] = 0.4*lgb_submission['target_benzene'] + 0.2*cat_submission['target_benzene'] + 0.4*xgb_submission['target_benzene'] \nensembe_sub['target_nitrogen_oxides'] = 0.4*lgb_submission['target_nitrogen_oxides'] + 0.2*cat_submission['target_nitrogen_oxides'] + 0.4*xgb_submission['target_nitrogen_oxides']\nensembe_sub.head()","3fcc195f":"# saving a submissions\n#xgb_prediction\nxgb_submission.to_csv('xgb_submission.csv', index=False)\n#lgb_prediction\nlgb_submission.to_csv('lgb_submission.csv', index=False)\n#cat_prediction\ncat_submission.to_csv('cat_submission.csv', index=False)\n#essemble prediction\nensembe_sub.to_csv('ensembled_submission.csv', index=False)\n\n","4cc5f574":"**A quick look at the test data**","4be53059":"As we can see sensor ( 1,2,3,4,5) come with much outliers. le's invesitage sensors columns one more time by ploting their distribution.","60158607":"**XGB Model prediction**","32e47a22":"**Loading the Data**","ab2e7088":"**Ensemble predictions**","48107f17":"### Data source ","82b0e85b":"### Notebook setup \nlet's start by loading the diffrent libraries and packages.","a15cb125":"# Problem \nIn this competition we are predicting the values of air pollution measurements over time, based on basic weather information (temperature and humidity) and the input values of 5 sensors. we will first do exploratory analysis and after that we will build a model.","e368eef3":"**Target distrubitions**","e3d37372":"**GridSearch to find the best paramaters**","21d0562c":"**Gridsearch to find the best parameters** ","86a12716":"**Variables's Boxplot**","144a6888":"**lGB model prediction**","dc0d0c44":"### Building The Model\nFor this problem we are going to use ensemnling predictions of three models (  XGB,LGB and CatB ).","e5d06f3c":"\nLooks some features are more related to targets than other features ","65edba44":"#### Data preparation and some Features Engineering","b3c5272d":"<center><h1>Tabular Playground Series - Jul 2021<h1> <center> \n    <center> <h5> I hope you find this helpful \ud83d\ude0a <h5> <center>","c0bbaa52":"**Variables Histogram**","fb929a7d":"**Checking missing or null values**","6915456c":"\n\n\n**Checking Correlation**","2e28be8d":"The data is available at [this link](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jul-2021\/data) and it contains this files.\n*  train.csv - the training data, including the weather data, sensor data, and values for the 3 targets\n*  test.csv - the same format as train.csv, but without the target value; your task is to predict the value for each of these targets.\n* sample_submission.csv - a sample submission file in the correct format.","6409daf2":"### Recommendation\nXGB ,LGB and CatB showed a good performance in predicting the targets but it still need to be improved,for that you may play with Hyperparameters . However I recommend spending most of your time on feature-engineering.And ofcourse you can experiment with other methods \/ autoMl libraries.","16feb5e5":"**CatBoost model**","0a65790c":"<center> <h3> If you find this usefull you can UPvote , Thank you\ud83d\ude0a <h3> <center> \n","960a448e":"**Basic summary statistics**"}}