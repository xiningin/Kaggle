{"cell_type":{"67e6b320":"code","3c0033ed":"code","ef9bc8b9":"code","d28f92eb":"code","ad85aa0f":"code","bbb8e526":"code","406839ba":"code","4e06e12c":"code","ca2da18e":"code","d009b61e":"code","7388dadc":"code","8c2adf3b":"code","61abe3c9":"code","906d3017":"code","17553a47":"code","fa27fedb":"code","552d4773":"code","e5f9d1f7":"code","4ddcda18":"code","4fb320ae":"code","e61e7a61":"code","974d60cb":"code","caa641ce":"code","23e640a5":"code","22a88018":"code","d20deb45":"code","fa4a364b":"markdown","fbb8facb":"markdown","74e7d768":"markdown","a35139c1":"markdown","dfe5f899":"markdown","4ae1fb1f":"markdown","8b0a15a2":"markdown","3dd7c380":"markdown","027d6400":"markdown","5670a09d":"markdown","f69c08d6":"markdown","bfcd29a7":"markdown","7f0cea49":"markdown","42f5521f":"markdown","cb86e2f4":"markdown","f7f72b47":"markdown","75cca5b0":"markdown","29ee84c0":"markdown","39bac8e1":"markdown","6a246f1b":"markdown","d99ae805":"markdown","f79dcb8a":"markdown","dadfe86b":"markdown","addb7624":"markdown","f473e59b":"markdown"},"source":{"67e6b320":"!jupyter nbextension enable --py widgetsnbextension\n\nfrom catboost import CatBoostClassifier, Pool, cv\n#from dataprep.eda import plot\nfrom matplotlib import pyplot as plt\nfrom numpy.random import RandomState\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n\nimport catboost\nimport hyperopt\nimport json\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport shap \n\n\n\nprint(catboost.__version__)\n#print(dataprep.__version__)\nprint(shap.__version__)\n\n!python --version","3c0033ed":"data_path = '..\/input\/covid19\/dataset.xlsx'\nraw_data_einstein = pd.read_excel(data_path)\nprint(raw_data_einstein.shape)\nraw_data_einstein.head()","ef9bc8b9":"\n# Columns to explicitely drop\nvars_to_drop = [\n  'Patient ID',\n  'Relationship (Patient\/Normal)',\n  'International normalized ratio (INR)',\n  'Urine - pH',\n]\n\n\n# Variables related to type of admission that won't be used in a first approach\nadmition_vars = [\n  'Patient addmited to regular ward (1=yes, 0=no)',\n  'Patient addmited to semi-intensive unit (1=yes, 0=no)',\n  'Patient addmited to intensive care unit (1=yes, 0=no)'\n]\n\n# Results of others tests\nother_tests_vars = [\n  # just the 25% of the dataset have values for these\n  'Respiratory Syncytial Virus',\n  'Influenza A',\n  'Influenza B',\n  'Parainfluenza 1',\n  'CoronavirusNL63',\n  'Rhinovirus\/Enterovirus',\n  'Coronavirus HKU1',\n  'Parainfluenza 3',\n  'Chlamydophila pneumoniae',\n  'Adenovirus',\n  'Parainfluenza 4',\n  'Coronavirus229E',\n  'CoronavirusOC43',\n  'Inf A H1N1 2009',\n  'Bordetella pertussis',\n  'Metapneumovirus',\n  'Parainfluenza 2',\n  # just the 15% of the dataset have values for these\n  'Influenza B, rapid test',\n  'Influenza A, rapid test',\n\n]\n\n# name of the column that holds the results of the test\ntarget_variable = 'SARS-Cov-2 exam result'\n\n# list to variables to work with, dropping what we don't want from the total columns of the raw dataset\nvars_to_work = list(set(raw_data_einstein.columns) - set(vars_to_drop) - set(admition_vars))\nworking_data = raw_data_einstein[vars_to_work]\n\nprint(f'New dataset size:{working_data.shape}')\n","d28f92eb":"# get the number of exampeles with null values by column\ndef get_null_counts(df):\n  null_columns=df.columns[df.isnull().any()]\n  nan_count_df = pd.DataFrame(data=df[null_columns].isnull().sum(),columns=['NaN count'])\n  nan_count_df.sort_values('NaN count',ascending=False,inplace=True)\n  return nan_count_df\n\n\nnull_count = get_null_counts(working_data)\n\nprint('\\nAmount of rows with Null values by column')\nprint(null_count.to_string())","ad85aa0f":"nrows_limit = working_data.shape[0]\n\ncols_empty = null_count.loc[null_count['NaN count'] >= nrows_limit].index.values\nprint('More columns to drop :\\n{}'.format(cols_empty))\n\nvars_to_work = list(set(working_data.columns) - set(cols_empty))\n\nworking_data = working_data[vars_to_work]\nprint(working_data.shape)\nprint(working_data.columns)\nworking_data.head(3)","bbb8e526":"working_data['y'] = 0\nworking_data.loc[working_data[target_variable]=='positive',['y']] = 1\n\nprint(working_data.loc[working_data.y == 1][[target_variable,'y']].sample(3))\nprint(working_data.loc[working_data.y == 0][[target_variable,'y']].sample(3))\n","406839ba":"print(working_data['y'].value_counts())\nprint(working_data.shape)","4e06e12c":"#col = target_variable\ncol = 'y'\n\n# this value parametrizes how much % more of negatives we have from the positive count\nsample_weight = 1.30\n\nprint(f'Classes proportions before sub-sample')\nprint(working_data[col].value_counts())\n\n# split the dataset in positive and negative cases\nworking_data_pos = working_data.loc[working_data[col] == 1]\nworking_data_neg = working_data.loc[working_data[col] == 0]\n\n# build a new one with only a sample of the negative cases, considering the weight we set before\nworking_data = pd.concat([\n  working_data_pos,\n  working_data_neg.sample(int(working_data_pos.shape[0] * sample_weight),random_state=123)\n])\n\nprint(f'Umbalance after sub-sample')\nprint(working_data[col].value_counts())\n","ca2da18e":"print(working_data.shape)\nnull_count = get_null_counts(working_data)\nnrows_limit = working_data.shape[0]\n\ncols_empty = null_count.loc[null_count['NaN count'] >= nrows_limit].index.values\nprint('More columns to drop :\\n{}'.format(cols_empty))\n\n# armamos una lista con las columnas que quedaran: el total menos las que tienen todas las filas en nulo\nvars_to_work = list(set(working_data.columns) - set(cols_empty))\n\n# obtenemos solo las columnas de inter\u00e9s\nworking_data = working_data[vars_to_work]\nprint(working_data.shape)\nprint(working_data.columns)\nworking_data.head(3)","d009b61e":"X = working_data.drop(target_variable, axis=1)\nX = X.drop('y', axis=1)\ny = working_data['y']\n#y = working_data[target_variable]\n\nprint(f'Input (X) shape : {X.shape}')\nprint(f'Output (y) shape : {y.shape}')","7388dadc":"X = X.fillna(-999)\nX.sample(3)","8c2adf3b":"categorical_features_indices = np.where(X.dtypes != np.float)[0]\ncategorical_features_indices = list(set(categorical_features_indices))\nprint(f'Categorical columns: \\n{X.columns[categorical_features_indices]}')","61abe3c9":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.1)\nX_train, X_validation, y_train, y_validation = train_test_split(X_train,y_train,test_size = 0.1)\nprint(X_train.shape)\nX_train.head(3)","906d3017":"# which metric do we want to optimize? \n#metric = 'Precision' \nmetric = 'Recall'\n#metric = 'F1'\n#metric = 'BalancedAccuracy'\n\ndef hyperopt_objective(params):\n    \n    model = CatBoostClassifier(\n        l2_leaf_reg=int(params['l2_leaf_reg']),\n        learning_rate=float(params['learning_rate']),\n        max_depth=int(params['max_depth']),\n        subsample=float(params['subsample']),\n        colsample_bylevel=float(params['colsample_bylevel']),\n        bootstrap_type = 'Bernoulli',\n        iterations=500,\n        eval_metric=metric,\n        random_seed=123,\n        verbose=False,\n        loss_function='CrossEntropy',\n        use_best_model = True,\n        #task_type='GPU',\n        task_type='CPU',\n        early_stopping_rounds=100,\n    )\n    \n    cv_data = cv(\n        Pool(X_validation, y_validation, cat_features=categorical_features_indices),\n        model.get_params()\n    )\n    best_metric = np.max(cv_data[f'test-{metric}-mean'])\n    \n    return 1 - best_metric # as hyperopt minimises","17553a47":"\n# the parameters to choose are sampled with uniform destribution between the ranges specified\nparams_space = {\n    'l2_leaf_reg': hyperopt.hp.uniform('l2_leaf_reg',2,5),\n    'learning_rate': hyperopt.hp.uniform('learning_rate',0.001,0.01),\n    'max_depth' : hyperopt.hp.uniform('max_depth',4,8),\n    'subsample' : hyperopt.hp.uniform('subsample',0.8,1.0),\n    'colsample_bylevel' : hyperopt.hp.uniform('colsample_bylevel',0.7,0.9)\n}\n\ntrials = hyperopt.Trials()\n\nbest = hyperopt.fmin(\n    hyperopt_objective,\n    space=params_space,\n    algo=hyperopt.tpe.suggest,\n    max_evals=5,\n    trials=trials,\n    rstate=RandomState(123)\n)\n\nwith open(f'best_{metric}.json', 'w') as fp:\n    json.dump(best, fp)\n    \nprint(best)","fa27fedb":"\nparams = {\n    'iterations': 500,\n    'eval_metric': metric,\n    'random_seed' : 123,\n    'use_best_model': True,\n    'loss_function': 'CrossEntropy',\n    #'task_type':'GPU',\n    'task_type':'CPU',\n    'early_stopping_rounds' : 100,\n    'bootstrap_type' : 'Bernoulli',\n    'verbose' : True,\n    'learning_rate': float(best['learning_rate']),\n    'l2_leaf_reg' : float(best['l2_leaf_reg']),\n    'max_depth' : int(best['max_depth']),\n    'subsample' : float(best['subsample']),\n    'colsample_bylevel' : float(best['colsample_bylevel']),\n    #'class_weights' : train_class_weights\n}\n\nmodel = CatBoostClassifier(**params)\n\nprint(model)\n\nmodel.fit(\n    X_train, y_train,\n    cat_features=categorical_features_indices,\n    eval_set=(X_validation, y_validation),\n    logging_level='Verbose',  # you can uncomment this for text output\n    #verbose=True,\n    plot=True\n)\n\nmodel.save_model(f'catboost_model_{metric}.bin')\nmodel.save_model(f'catboost_model_{metric}.json', format='json')\n","552d4773":"'''\n  Function to plot and print a confusion matrix and common classification metrics\n'''\ndef plot_cm(labels, predictions, p=0.5):\n\n  cm = confusion_matrix(labels, predictions > p)\n  plt.figure(figsize=(5,5))\n  sns.heatmap(cm, annot=True, fmt=\"d\")\n  plt.title('Confusion matrix @{:.2f}'.format(p))\n  plt.ylabel('Actual label')\n  plt.xlabel('Predicted label')\n\n  print('True Negatives: ', cm[0][0])\n  print('False Positives: ', cm[0][1])\n  print('False Negatives: ', cm[1][0])\n  print('True Positives: ', cm[1][1])\n  print('Total : ', np.sum(cm[1]))\n\n","e5f9d1f7":"\nmetric = 'Recall'\nbest_model = CatBoostClassifier()\nbest_model.load_model(f'catboost_model_{metric}.bin')\nprint(f'Using model with parameters : \\n {best_model.get_params()}')\n\ntrain_predictions = best_model.predict(X_train)\n\nprint('Train report')\nprint(classification_report(y_train, train_predictions))\n\nprint('Train Confusion matrix')\nprint()\nplot_cm(y_train, train_predictions)\nplt.show()\n\n\n\ntest_predictions = best_model.predict(X_test)\n\nprint('Test report')\nprint(classification_report(y_test, test_predictions))\n\nprint('Test Confusion matrix')\nprint()\nplot_cm(y_test, test_predictions)\nplt.show()\n","4ddcda18":"feature_importances = model.get_feature_importance(prettified=True)\nimportant_features = feature_importances.loc[feature_importances['Importances'] > 0.0]\nimportant_features","4fb320ae":"\ntest_df = working_data\n\nf,ax = plt.subplots(int((important_features.shape[0]+1)\/2),2,figsize=(20,35))\n\nrow = 0\ncol = 0\n\nfor i in range(important_features.shape[0]):\n    #print(f'{row}-{col}')\n    \n    the_ax = ax[row,col]\n\n    if test_df[important_features['Feature Id'].values[i]].dtypes != np.float :\n        sns.countplot(data=test_df,y=important_features['Feature Id'].values[i],hue='y', ax = the_ax)\n    else:\n        sns.violinplot(data=test_df,x='y',y=important_features['Feature Id'].values[i] , ax = the_ax)\n        \n    if col == 0 :\n        col += 1\n    else :\n        row += 1\n        col = 0\n\nplt.show()\n        \n","e61e7a61":"positive_cases_index = y_test.iloc[np.where(y_test == 1)].index\n\ntrue_datapoints = X_test.loc[positive_cases_index]\nprint(true_datapoints.shape)\n\npostive_datapoints_predictions = best_model.predict(true_datapoints)\n\ntrue_positives_index = np.where(postive_datapoints_predictions == 1)[0]\ntrue_positives_datapoints = true_datapoints.iloc[true_positives_index,:]\nprint(true_positives_datapoints.shape)\n\npool_tp = Pool(data=true_positives_datapoints, label=pd.Series(np.ones(true_positives_datapoints.shape[0])), cat_features=categorical_features_indices)\nshap_values_tp = model.get_feature_importance(pool_tp, type='ShapValues')\nexpected_value = shap_values_tp[0,-1]\nprint(expected_value)\nshap_values_tp = shap_values_tp[:,:-1]\nprint(shap_values_tp.shape)\n\nshap.summary_plot(shap_values_tp, true_positives_datapoints)","974d60cb":"plt.figure(figsize=(10,5))\nsns.distplot(best_model.predict_proba(true_datapoints)[:,1])","caa641ce":"good_true_positives_index = np.where(best_model.predict_proba(true_datapoints)[:,1] >= 0.5)[0]\ngood_true_positives_datapoints = true_datapoints.iloc[good_true_positives_index,:]\nprint(good_true_positives_datapoints.shape)\npool_gtp = Pool(data=good_true_positives_datapoints, label=pd.Series(np.ones(good_true_positives_datapoints.shape[0])), cat_features=categorical_features_indices)\nshap_values_gtp = model.get_feature_importance(pool_gtp, type='ShapValues')\nexpected_value = shap_values_gtp[0,-1]\nprint(expected_value)\nshap_values_gtp = shap_values_gtp[:,:-1]\nprint(shap_values_gtp.shape)\nfor i in range(min([good_true_positives_datapoints.shape[0],5])):\n    shap.force_plot(expected_value, shap_values_gtp[i,:], good_true_positives_datapoints.iloc[i,:],matplotlib=True,text_rotation=45)","23e640a5":"negative_cases_index = y_test.iloc[np.where(y_test == 0)].index\nfalse_datapoints = X_test.loc[negative_cases_index]\nprint(false_datapoints.shape)\n\nnegative_datapoints_predictions = model.predict(false_datapoints)\ntrue_negative_index = np.where(negative_datapoints_predictions == 0)[0]\ntrue_negative_datapoints = false_datapoints.iloc[true_negative_index,:]\nprint(true_negative_datapoints.shape)\n\npool_tn = Pool(data=true_negative_datapoints, label=pd.Series(np.zeros(true_negative_datapoints.shape[0])), cat_features=categorical_features_indices)\nshap_values_tn = model.get_feature_importance(pool_tn, type='ShapValues')\nexpected_value_tn = shap_values_tn[0,-1]\nprint(expected_value_tn)\nshap_values_tn = shap_values_tn[:,:-1]\nprint(shap_values_tn.shape)\n\nshap.summary_plot(shap_values_tn, true_negative_datapoints)","22a88018":"plt.figure(figsize=(10,5))\nsns.distplot(best_model.predict_proba(false_datapoints)[:,0])","d20deb45":"good_true_negatives_index = np.where(best_model.predict_proba(false_datapoints)[:,0] >= 0.505)[0]\ngood_true_negatives_datapoints = false_datapoints.iloc[good_true_negatives_index,:]\nprint(good_true_negatives_datapoints.shape[0])\npool_gtn = Pool(data=good_true_negatives_datapoints, label=pd.Series(np.ones(good_true_negatives_datapoints.shape[0])), cat_features=categorical_features_indices)\nshap_values_gtn = model.get_feature_importance(pool_gtn, type='ShapValues')\nexpected_value = shap_values_gtn[0,-1]\nprint(expected_value)\nshap_values_gtn = shap_values_gtn[:,:-1]\nprint(shap_values_gtn.shape)\nfor i in range(min([good_true_negatives_datapoints.shape[0],5])):\n    shap.force_plot(expected_value, shap_values_gtn[i,:], good_true_negatives_datapoints.iloc[i,:],matplotlib=True,text_rotation=45)","fa4a364b":"In this case we have several predictions with good scores having probabilities over 0.7 so we choose this value as thresshold","fbb8facb":"We can see in detail with Force plots how each variable moves he prediction probability for the examples that predicted positive with higher score.\n\nFirst we see the distribution of the probability scores for the positive class","74e7d768":"The full version of this dataset have ~5600 examples and ~111 variables.\n\nTo move forward properly, we will need to filter-out some input variables like the `Patient ID` and some others that produced errors when processing.","a35139c1":"Next we will train the actual classifier and do some hyper-parameter optimization to try different configurations for the classifier.","dfe5f899":"Using the `shap_values` we get a [`summary_plot`](https:\/\/christophm.github.io\/interpretable-ml-book\/shap.html#shap-summary-plot) where we see at the left the most important variables and each point corresponds to an example. Points located to the righ move the  prediction probability for the postive class to higher values. The color of the point shows the value that each of them had for the current variable.","4ae1fb1f":"Now we repeat the process for negative cases, focusing on the _true negatives_","8b0a15a2":"As expected, the number of examples with  `positive` result is less than the number of `negative` examples. \n\nWe will treat the imbalance to get better results by taking a smaller number of `negative` examples trying to have a almost the same number of examples for this class as for the `positive` one. ","3dd7c380":"Given that we sub-sampled the dataset, we should repeat the process to drop columns with all examples with null values. ","027d6400":"# COVID-19 Tests result prediction with a Catboost classifier\n\nIn this project we apply a [Catboost](https:\/\/catboost.ai\/) classifier to try to predict the results of COVID-19 tests from clinical medical studies data. We will try to understand a bit more about how these clinical variabes influence the probability of having a positive result in a COVID-19 test result.\n","5670a09d":"Next we split the dataset into three disjoint _train_ , _validation_ and _test_ sets:\n* `train-set`: `80%` of the full dataset used to train\n* `validation-set` : `10%` of the total, used to choose the best hyper parameter configuration\n* `test-set` : `10%` from the total dataset, used to evaluate how good the model is to predict brand new cases\n","f69c08d6":"Because this dataset have a lot of missing values, we need to treat them in some way. [It is expected that if we set a value extremely different from the common in each column for the `Null` examples, the classification method used here will consider them as missing and work with them properly](https:\/\/render.githubusercontent.com\/view\/ipynb?commit=25c7f813184604d1b5d1d118fc82e001add6cf30&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f636174626f6f73742f7475746f7269616c732f323563376638313331383436303464316235643164313138666338326530303161646436636633302f707974686f6e5f7475746f7269616c2e6970796e62&nwo=catboost%2Ftutorials&path=python_tutorial.ipynb&repository_id=138584767&repository_type=Repository#1.3-Feature-Preparation). In this case we will use `-999` to fill the `NaN`s","bfcd29a7":"We are going to inspect the class balance by counting the total of examples by unique value of the target column.","7f0cea49":"Depending on what metric we decided to optimize is what we get from the model. In this case we decided to optimize F1 and we have a decent _Test Precision_ (90%) even though we still see a bit of possible overfitting if we look at the difference with _Train Precision_.\n\nAlso, it is possible to see that the recall is really low so if we use this model, we could be missing the 90% of potential cases. \n","42f5521f":"## Understanding the predictions\n\nNow that we have a baseline model we try to see how it could behave in the real-life analizing some predictions over the testset.","cb86e2f4":"Now we evaluate the model over the trainset and test set as well.","f7f72b47":"\nLet's first look at how it behaves with _true positive_ cases to analyze where the model is good.","75cca5b0":"Again we will see the force plots for the \"good\" negative predictions, starting with the distribution of prediction scores for the negative class.","29ee84c0":"## Pre-processing and EDA\n\nWe are going to work over [patient data collected in the Hospital Isrealita Albert Einstein in S\u00e3o Paulo, Brazil](https:\/\/www.kaggle.com\/einsteindata4u\/covid19#dataset.xlsx)\n\nThis dataset contains anonimous data from patients that were tested with _SARS-CoV-2 RT-PCR_ and other aditional laboratory tests, done during their visit to the hospital. ","39bac8e1":"Most of our scores for the postive predictions are near 0.5 so we consider as \"good\" predictions those over ~0.5 ","6a246f1b":"The main benefit of the _Catboost_ technique is that is better than other in dealing with categorical variables. That is why we need to indicate which of the columns are categorical to indicate it to the classifier. ","d99ae805":"## Modeling\nTo start the modeling phase we split the dataset to have the inputs and the ouputs in different variables.","f79dcb8a":"Next we build a numerica variable named `y` that will be `1` for the `postive` exmaples and `0` for the `negative` ones.","dadfe86b":"Next we use the [SHAP](https:\/\/github.com\/slundberg\/shap) package to understand these _true positive_ predictions and see which are the most important variables for the model in this case and how they influence the prediction probability.","addb7624":"We can see which are the most important variables and how they behave by test result","f473e59b":"The first step to get a cleaner dataset is to drop the columns that have `Nulls` in all the examples."}}