{"cell_type":{"34fb1764":"code","995a34e9":"code","8a9a44a7":"code","6ec4be58":"code","87faaaf9":"code","d9afa144":"code","2ca7d00b":"code","5de06541":"code","9c766fe4":"code","b9bd8e92":"code","d280b769":"code","37257952":"code","7583512e":"code","eaa2e8a9":"code","13365d2d":"code","44fa69f4":"code","c8367c28":"code","61f3c549":"code","82dcc66e":"code","077b5901":"code","68c58bb5":"code","5abddf2a":"code","0c60ff14":"code","b8ed18b3":"code","b1f33026":"code","37cc42bc":"code","86f76d29":"code","654bb81e":"code","fbd71f17":"code","3e655219":"code","9c136b65":"markdown","b50728ec":"markdown","bbb3ec26":"markdown","f965a6fa":"markdown","0bd63abc":"markdown","faad6631":"markdown","2e0a0ed3":"markdown","f61b04a1":"markdown","a372bc91":"markdown","7a82c8f5":"markdown","13a1bc6f":"markdown","a72a3b40":"markdown","696aa704":"markdown","3778095c":"markdown","48aa3e1f":"markdown","032de75d":"markdown"},"source":{"34fb1764":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","995a34e9":"# Official BERT tokenizer\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","8a9a44a7":"import tensorflow_hub as hub\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, wordpunct_tokenize, RegexpTokenizer\nfrom nltk.probability import FreqDist\nimport tokenization\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Conv1D, LSTM, GRU, MaxPool1D, GlobalMaxPooling1D, Embedding, Dropout, SpatialDropout1D, Lambda\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\nimport tensorflow as tf\nimport re\n!pip install inflect\nimport inflect\nfrom collections import Counter\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt","6ec4be58":"# Append keyword to sentences\nb_add_keyword = True\n\n# Types of cleaning to apply - name helper functions accordingly\n# cleaning = ['remove_url', 'remove_numbers', 'remove_unicode', 'remove_emoji', 'remove_stopwords']\n# cleaning = ['remove_stopwords']\ncleaning = []\n\nmax_len = 160  # 100","87faaaf9":"# Read data\ntrain_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\nprint('n train\/test: {}\/{}'.format(train_df.shape[0], test_df.shape[0]))","d9afa144":"print('n per class:\\n{}'.format(train_df['target'].value_counts()))","2ca7d00b":"train_df['location'].value_counts()","5de06541":"train_df['keyword'].isna().value_counts()","9c766fe4":"a = train_df['text']","b9bd8e92":"train_df['text'].values","d280b769":"all_words = []\nfor t in train_df['text'].values:\n    all_words.extend(wordpunct_tokenize(t))\n\ncounter = Counter(all_words)\ncounter.most_common(10)","37257952":"def remove_url(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_numbers(text):\n    return re.sub(r'[^a-zA-Z\\']', ' ', text)\n\ndef remove_unicode(text):\n    return re.sub(r'[^\\x00-\\x7F]+', '', text)\n    \n# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nstop_words = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    text = text.lower()\n    \n    text = \" \".join([w for w in wordpunct_tokenize(text) if w not in stop_words])\n    \n    return text\n\n# Convert number into words\np = inflect.engine()\ndef convert_number(text):\n    # split string into list of words \n    temp_str = text.split() \n    # initialise empty list \n    new_string = [] \n\n    for word in temp_str: \n        # if word is a digit, convert the digit \n        # to numbers and append into the new_string list \n        if word.isdigit(): \n            temp = p.number_to_words(word) \n            new_string.append(temp) \n\n        # append the word as it is \n        else: \n            new_string.append(word) \n\n    # join the words of new_string to form a string \n    temp_str = ' '.join(new_string) \n    return temp_str\n\n# longform_dict = {\"don't\": \"do not\", \"\"}\n# def convert_to_longform(text):\n    \n","7583512e":"# Word cloud\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stop_words,\n        max_words=200,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(12, 12))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","eaa2e8a9":"# module_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/2\"\n# module_url = \"https:\/\/tfhub.dev\/google\/mobilebert\/uncased_L-24_H-128_B-512_A-4_F-4_OPT\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","13365d2d":"# Define tokenizer\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","44fa69f4":"def encode_text(texts, tokenizer, max_len):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n    ","c8367c28":"train_text = train_df['text']\ntest_text = test_df['text']\n\n# Add keyword\nif b_add_keyword:\n    for ii in range(train_text.shape[0]):\n        if not pd.isna(train_df.loc[ii, 'keyword']):\n            train_text[ii] = train_text[ii] + ' ' + train_df.loc[ii, 'keyword']\n    \n    for ii in range(test_text.shape[0]):\n        if not pd.isna(test_df.loc[ii, 'keyword']):\n            test_text[ii] = test_text[ii] + ' ' + test_df.loc[ii, 'keyword']\n\n# Clean\nfor ii in cleaning:\n    train_text = train_text.apply(eval(ii))\n    test_text = test_text.apply(eval(ii))\n","61f3c549":"all_words_train = []\nfor t in train_text.values:\n    all_words_train.extend(wordpunct_tokenize(t))\nall_words_test = []\nfor t in test_text.values:\n    all_words_test.extend(wordpunct_tokenize(t))","82dcc66e":"counter = Counter(all_words_train)\nprint('train\\n{}'.format(counter.most_common(10)))\n\ncounter = Counter(all_words_test)\nprint('test\\n{}'.format(counter.most_common(10)))","077b5901":"wordpunct_tokenize(\"this is great don't\"), word_tokenize(\"this is great don't\"), tokenizer.tokenize(\"this is great don't\")","68c58bb5":"print('train 0\\n')\nshow_wordcloud(train_text.loc[train_df['target']==0])\nprint('train 1\\n')\nshow_wordcloud(train_text.loc[train_df['target']==1])","5abddf2a":"train_text_enc = encode_text(train_text.values, tokenizer, max_len)\ntest_text_enc = encode_text(test_text.values, tokenizer, max_len)","0c60ff14":"# This model is the best so far without any cleaning!\ndef build_bert_model(bert_layer, max_len):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    \n    # Using [CLS] token output from sequence output\n    x = sequence_output[:, 0, :]  # use 0th output - belongs to CLS token - means classification\n    \n    out = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    \n    return model\n\n\n\n\"\"\"\ndef build_bert_model(bert_layer, max_len):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    \n    # Using [CLS] token output from sequence output\n    x = sequence_output[:, 0, :]  # use 0th output - belongs to CLS token - means classification\n#     x = Dropout(0.2)(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    out = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    \n    return model\n\"\"\"\n","b8ed18b3":"# Early stopping\nes = EarlyStopping(monitor='val_accuracy', mode='max', patience=3, verbose=1)\n\n# Build model\nm = build_bert_model(bert_layer, max_len)\nm.summary()\ntrain_perf = pd.DataFrame()","b1f33026":"m.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])","37cc42bc":"fit_hist = m.fit(train_text_enc, train_df['target'].values, epochs=10, validation_split=0.2, batch_size=32)\ntrain_perf = pd.concat((train_perf, pd.DataFrame(fit_hist.history)), ignore_index=True)","86f76d29":"# Demo model to test out syntax\n# in_l = Input(shape=(train_text_enc[0].shape[1], ))\n# o = Dense(1, activation='sigmoid')(in_l)\n# m = Model(inputs=in_l, outputs=o)\n\n# # Early stopping\n# es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1)\n\n# m.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n# fit_hist = m.fit(train_text_enc, train_df['target'].values, epochs=20, validation_split=0.2, batch_size=32, callbacks=[es])","654bb81e":"# Plot training performance\nplt.figure(figsize=(12, 8))\nplt.subplot(121)\nplt.plot(train_perf['loss'])\nplt.plot(train_perf['val_loss'])\nplt.legend(['loss', 'val_loss'])\nplt.xlabel('epoch')\nplt.title('loss')\n\nplt.subplot(122)\nplt.plot(train_perf['accuracy'])\nplt.plot(train_perf['val_accuracy'])\nplt.legend(['acc', 'val_acc'])\nplt.xlabel('epoch')\nplt.title('acc')\nplt.show()","fbd71f17":"test_output = m.predict(test_text_enc)\ntest_pred = (test_output >= 0.5).astype(int)","3e655219":"submission = pd.DataFrame(data={'id': test_df['id'].values, 'target': test_pred.reshape(-1, )}) # {'id': test_df['id'].values.reshape(-1, 1), 'target': test_pred})\nsubmission.head()\n\nsubmission.to_csv(r'submission.csv', index=False)\n\nmodel_score = pd.DataFrame(data={'id': test_df['id'].values, 'target': test_pred.reshape(-1, ), 'score': test_output.reshape(-1, )})\nmodel_score.to_csv(r'model_score_bert.csv', index=False)\n\nm.save('m_bert.h5')","9c136b65":"# Import","b50728ec":"#### --- For cleaning up text ---","bbb3ec26":"# Create model","f965a6fa":"#### Most common words","0bd63abc":"# Learnings\n\nSetting bert trainable to False doesn't improve accuracy, need to fine tune it!","faad6631":"#### --- Apply encoding ---","2e0a0ed3":"#### Most common words after cleaning","f61b04a1":"#### --- For encoding ---","a372bc91":"# Init","7a82c8f5":"#### --- Apply cleaning ---","13a1bc6f":"# **Versions**\n[version \/ public lb score \/ descriptions]\n* **ver 7\/7:** 0.78732, bert 1024 uncased pooled output + 1 dense layer, with text cleaning, max_len 100, epoch=1\n* **ver 8\/8:** 0.79856, same as version 7, but 5 epochs\n* **ver 11\/11:** **0.82413**, same model arch as ver 7 and 8, but using [CLS] output from sequence output + 1 dense layer, **NO CLEANING**\n* **ver 13\/13:** 0.81186, exactly same as ver 11, but get a different score\n* **ver 16\/17:** 0.79652, **model changed** bert 1024 uncased [CLS] output + dropout(0.2) + dense(512) + dense(1), 3 epochs\n* **ver 21\/23:** **0.80674**, **model changed** using bert 768 uncased + same as ver 16\/17, early stopping on val_acc, no patience, training stopped at 2nd epoch -> addressed in the next version\n* **ver 23\/25:** **0.81492**, same as version 21\/23, no early stopping, 10 epochs -> val acc is mostly flat thru training\n* **ver 26\/28:** **0.82106**, same bert 768 + dropout + dense(512), 10 epochs, **only stopwords cleaning, max_len 160**\n* **ver 27\/30:** , **0.81288** **model changed** bert 768 + dense(1), **ALL CLEANING**\n* **{BEST}** **ver 28\/31:** , **0.84049** **model changed** bert 768 + dense(1), **NO CLEANING**\n* **ver 29\/32:** , **0.** **model changed** bert 768 + dropout(0.2) + dense(128) + dense(1), **NO CLEANING**, early stopping, patience 3, bert trainable=True","a72a3b40":"# EDA","696aa704":"### Helper functions","3778095c":"#### Word cloud","48aa3e1f":"# Pre process text","032de75d":"### There are a lot of '  -> because wordpunct_tokenize splits ' as as separate token\n### Replace individual words before cleaning: e.g. don't-> do not"}}