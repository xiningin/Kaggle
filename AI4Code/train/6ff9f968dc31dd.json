{"cell_type":{"fdc03fd5":"code","ace422fa":"code","ef48a118":"code","5c5af69c":"code","7ea7ca87":"code","059fdd0a":"code","cc77c810":"code","2672b03d":"code","ad6f1333":"code","315492ae":"code","287bb391":"code","a664a625":"code","3131a7a9":"code","ada799c4":"code","1b072994":"code","db9af4f5":"code","7cdbe48f":"code","41bdcca5":"code","8bbde6ca":"code","ffee504c":"code","31284aa1":"code","2aa47f6d":"code","25ad4535":"code","33fcddd1":"code","315373cd":"code","f09190d5":"code","2e56bc92":"code","d83af8c3":"code","9d26312a":"code","436c06cb":"code","ccece5a5":"code","e41d6aab":"code","79714430":"code","79622079":"code","60061c4b":"code","cc28cae1":"code","a4682ec9":"code","064ffdb8":"code","51946671":"code","1525a636":"code","b3b41bb4":"code","014578d2":"code","28ad5c50":"code","5befc809":"code","e2238a94":"code","7edc46d8":"code","9b47f901":"code","4d37f049":"code","eb81ef11":"code","d38abe6d":"code","d297322e":"code","3a2ef9a0":"code","2880d2dc":"code","f81cf203":"code","9cdda2ae":"code","291b9146":"code","d0aec0ce":"code","6a2f4b64":"code","5b79d094":"code","1868bff4":"code","7d924c9b":"code","2eb951d6":"code","108ead85":"markdown","237152de":"markdown","151e172a":"markdown","0cadadb4":"markdown","f5561d7f":"markdown","2d87b591":"markdown","718f27e4":"markdown","be6ecc8c":"markdown","9144ebdb":"markdown","8188f5fe":"markdown","89c3701f":"markdown","b922d78a":"markdown","f856bace":"markdown","cc1657ed":"markdown","e868758b":"markdown","32e768ae":"markdown","ce7c8d6e":"markdown","89217db6":"markdown","27d74ba6":"markdown","ab44a266":"markdown","e7127cd0":"markdown","86eb33d8":"markdown","1f887682":"markdown","2af1eaec":"markdown","688f0330":"markdown","431e5112":"markdown","0d7954d6":"markdown","e809de16":"markdown","19d55de5":"markdown","987e1d8e":"markdown","893d2276":"markdown"},"source":{"fdc03fd5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import mean_absolute_error","ace422fa":"def less_values(frame, percentile):\n    \"\"\"\n    Checks which columns have more than certain number of missing values and drops them.\n    \"\"\"\n    df = frame\n    for col in df.columns:\n        count = df[col].count()\n        if count < 1460*float(percentile):\n            df.drop(col, axis=1, inplace=True)\n    \n    return df","ef48a118":"pd.set_option('display.max_columns', None)","5c5af69c":"path = '..\/input\/house-prices-advanced-regression-techniques\/train.csv'\ndf = pd.read_csv(path, index_col = 0)","7ea7ca87":"# Blank page","059fdd0a":"import category_encoders as ce\nfrom sklearn.impute import SimpleImputer","cc77c810":"# Some columns have an integer data type, while they are categorical.\n# For future convenience, I'll change the data type to object\n\nfor col in ['MSSubClass','OverallQual', 'OverallCond']:\n    df[col] = df[col].astype('object')","2672b03d":"# As there are some columns with insignificant amount of values\ndf = less_values(frame=df, percentile=0.8)","ad6f1333":"# I want to focus more on continuous and categorcical data, so I'll just drop inforamtion about date\ndate = np.array(['YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'MoSold', 'YrSold'])\ndf = df.drop(date, axis = 1)","315492ae":"# Define numerical data columns\nnum_cols = np.array(df.select_dtypes(include=['int64', 'float64']).columns)\nnum_cols = np.delete(num_cols, len(num_cols) - 1) # Removing SalePrice i.e target\n\n# Define categorical data columns\ncat_cols = np.array(df.select_dtypes(include=['object']).columns)","287bb391":"# Since I want to use imputers and encoders, I need train data to fit them\nX_train, X_test, y_train, y_test = train_test_split(df.iloc[:,:-1], df.iloc[:, -1], test_size=.3, random_state=42)","a664a625":"num_transformer = SimpleImputer(strategy='median', missing_values=np.nan)\nX_train[num_cols] = num_transformer.fit_transform(X_train[num_cols])\nprint('Missing values: {}'.format(X_train[num_cols].isnull().any().any()))","3131a7a9":"X_test[num_cols] = num_transformer.transform(X_test[num_cols])\nprint('Missing values: {}'.format(X_test[num_cols].isnull().any().any()))","ada799c4":"cat_imputer = SimpleImputer(strategy='most_frequent', missing_values=np.nan)\nX_train[cat_cols] = cat_imputer.fit_transform(X_train[cat_cols])\nprint('Missing values: {}'.format(X_train[cat_cols].isnull().any().any()))","1b072994":"X_test[cat_cols] = cat_imputer.transform(X_test[cat_cols])\nprint('Missing values: {}'.format(X_test[cat_cols].isnull().any().any()))","db9af4f5":"t_encoder = ce.TargetEncoder(cols=cat_cols)","7cdbe48f":"X_train_target = X_train.copy()\nX_train_target[cat_cols] = t_encoder.fit_transform(X_train[cat_cols], y_train)\nX_train_target[cat_cols[0:5]].head(1) # Checking the results","41bdcca5":"X_test_target = X_test.copy()\nX_test_target[cat_cols] = t_encoder.transform(X_test[cat_cols])\nX_test_target[cat_cols[0:5]].head(1)","8bbde6ca":"glmm = ce.glmm.GLMMEncoder()","ffee504c":"X_train_glmm = X_train.copy()\nX_train_glmm[cat_cols] = glmm.fit_transform(X_train[cat_cols], y_train)\nX_train_glmm[cat_cols[5:10]].head(1)","31284aa1":"X_test_glmm = X_test.copy()\nX_test_glmm[cat_cols] = glmm.transform(X_test[cat_cols])\nX_test_glmm[cat_cols[5:10]].head(1)","2aa47f6d":"# Blank page","25ad4535":"# Blank page","33fcddd1":"from sklearn.feature_selection import SelectKBest, f_regression","315373cd":"selector = SelectKBest(f_regression, k=45) # I have found out best k simply by trial and error","f09190d5":"X_train_new = selector.fit_transform(X_train_target, y_train)\nX_train_new = pd.DataFrame(selector.inverse_transform(X_train_new),\n                          index=X_train.index,\n                          columns=X_train.columns)\nselected_columns_target = X_train_new.columns[X_train_new.var() != 0]","2e56bc92":"print('First 5 features from target encoded data: {}'.format(list(selected_columns_target[:5])))","d83af8c3":"X_train_new = selector.fit_transform(X_train_glmm, y_train)\nX_train_new = pd.DataFrame(selector.inverse_transform(X_train_new),\n                          index=X_train.index,\n                          columns=X_train.columns)\nselected_columns_glmm = X_train_new.columns[X_train_new.var() != 0]","9d26312a":"print('First 5 features from glmm encoded data: {}'.format(list(selected_columns_glmm[:5])))","436c06cb":"# Blank page","ccece5a5":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_val_score","e41d6aab":"reg = LinearRegression()\nforest = RandomForestRegressor(random_state=42)\nxgb = XGBRegressor(random_state=42)","79714430":"models = {'Linear Regression': reg,\n         'Random Forest': forest,\n         'XGB Regression': xgb}","79622079":"# For future cross validation I'll use all data I have\nall_target = pd.concat((X_train_target, X_test_target))\nall_glmm = pd.concat((X_train_glmm, X_test_glmm))\nall_y = pd.concat((y_train, y_test))","60061c4b":"results = {}\ntarget_means = []\nglmm_means = []\n\nfor i in models:\n    # Calculating cross validation scores and mean for target encoded data\n    models[i].fit(X_train_target[selected_columns_target], y_train)\n    t_score = cross_val_score(models[i], all_target[selected_columns_target], all_y, scoring='neg_mean_absolute_error', cv=3)\n    t_score = -1 * t_score\n    t_score = np.round(t_score, 1)\n    t_mean = np.round(t_score.mean(), 1)\n    t_score = ('targer score', t_score, t_mean)\n    target_means.append(t_mean)\n    \n    # Calculating cross validation scores and mean for glmm encoded data\n    models[i].fit(X_train_glmm[selected_columns_glmm], y_train)\n    glmm_score = cross_val_score(models[i], all_glmm[selected_columns_glmm], all_y, scoring='neg_mean_absolute_error', cv=3)\n    glmm_score = -1 * glmm_score\n    glmm_score = np.round(glmm_score, 1)\n    glmm_mean = np.round(glmm_score.mean(), 1)\n    glmm_score = ('glmm score', glmm_score, glmm_mean)\n    glmm_means.append(glmm_mean)\n    \n    # score\n    score = [t_score, glmm_score]\n    results[i] = score","cc28cae1":"sns.set_style('whitegrid')","a4682ec9":"fig, (ax1, ax2) = plt.subplots(1,2, figsize=(13, 5))\nplt.subplots_adjust(top=0.80)\nfig.suptitle('Cross validation results from train data', fontsize=16)\n\nx_axis = list(models.keys())\nax1 = sns.barplot(x=x_axis, y=target_means, palette='Set2', ax=ax1)\nax1.set_title('Target encoded data', fontsize=14)\nax1.set_ylim((0, 22000))\nax1.set_ylabel('Mean absolute error')\n\nax2 = sns.barplot(x=x_axis, y=glmm_means, palette='Set2', ax=ax2)\nax2.set_title('Glmm encoded data', fontsize=14)\nax2.set_ylim((0, 22000))\nax2.set_ylabel('Mean absolute error')\n\n\nplt.show()","064ffdb8":"# More detailed\nfor key in results:\n    print(key + ':')\n    print(results[key])\n    \n    if key == 'XGB Regression':\n        print('_'*125)\n    else:\n        print('\\n')","51946671":"def train_model(model, X_train, X_test, y_train=y_train, y_test=y_test):\n    \"\"\"\n    Fitting model, and scoring with mean absolute error\n    \"\"\"\n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    score = mean_absolute_error(y_test, pred)\n    return score","1525a636":"test_target = []\ntest_glmm = []\n\nfor i in models:\n    # Calculating mean absolute error for target encoded data\n    score = train_model(models[i], X_train_target[selected_columns_target], X_test_target[selected_columns_target])\n    score = np.round(score, 1)\n    test_target.append(score)\n    \n    # Calculating mean absolute error for glmm encoded data\n    score = train_model(models[i], X_train_glmm[selected_columns_glmm], X_test_glmm[selected_columns_glmm])\n    score = np.round(score, 1)\n    test_glmm.append(score)","b3b41bb4":"fig, (ax1, ax2) = plt.subplots(1,2, figsize=(13, 5))\nplt.subplots_adjust(top=0.8)\nfig.suptitle('Evaluation results from test data', fontsize=16)\n\nx_axis = list(models.keys())\nax1 = sns.barplot(x=x_axis, y=test_target, palette='Set2', ax=ax1)\nax1.set_title('Target encoded data', fontsize=14)\nax1.set_ylim((0, 22000))\nax1.set_ylabel('Mean absolute error')\n\nax2 = sns.barplot(x=x_axis, y=test_glmm, palette='Set2', ax=ax2)\nax2.set_title('Glmm encoded data', fontsize=14)\nax2.set_ylim((0, 22000))\nax2.set_ylabel('Mean absolute error')\n\nplt.show()","014578d2":"# More detailed\nfor i in range(3):\n    print(x_axis[i] + ':')\n    print('  Target encoded data: ' + str(test_target[i]))\n    print('  Glmm encoded data: ' + str(test_glmm[i]))\n    \n    if i == 2:\n        print('_'*125)\n    else:\n        print('\\n')","28ad5c50":"# Blank page","5befc809":"X_train_new = X_train_glmm[selected_columns_glmm]\nX_test_new = X_test_glmm[selected_columns_glmm]","e2238a94":"forest = RandomForestRegressor(n_estimators=1000,\n                              min_samples_leaf=2,\n                              max_features='sqrt',\n                              oob_score=True,\n                              n_jobs=4,\n                              random_state=42)","7edc46d8":"forest.fit(X_train_new, y_train)","9b47f901":"pred = forest.predict(X_test_new)","4d37f049":"from sklearn.metrics import r2_score","eb81ef11":"error = mean_absolute_error(y_test, pred)\nerror = np.round(error, 0)\n\nr2 = r2_score(y_test, pred)\nr2 = np.round(r2, 2)\n\noob = forest.oob_score_\noob = np.round(oob, 2)","d38abe6d":"# More detailed metrics\npd.DataFrame(index=['Mean absolute error', 'R^2', 'Out-of-bag'], columns=['Random Forest'], data=[error, r2, oob])","d297322e":"# Blank page","3a2ef9a0":"x = np.arange(0, 800001, 100000)","2880d2dc":"plt.figure(figsize=(10,6))\n\nsns.lineplot(x=x, y=x, color='orange')\nsns.scatterplot(y=y_test, x=pred, color='deepskyblue', s=50)\n\nplt.xlim(0,800000)\nplt.ylim(0,800000)\nplt.title('Relation between true and predicted values\\n', fontsize=16)\nplt.xlabel('Predicted values', fontsize=14)\nplt.ylabel('True values', fontsize=14)\n\nplt.text(100000, 70000, '  2  ', ha=\"center\", va=\"center\", rotation=0, size=20,\n    bbox=dict(boxstyle=\"circle,pad=0.3\", lw=2, color='darkslategray', fill=False))\nplt.text(480000, 620000, '  1  ', ha=\"center\", va=\"center\", rotation=0, size=60,\n    bbox=dict(boxstyle=\"circle,pad=0.3\", lw=2, color='darkslategray', fill=False))\n\nplt.show()","f81cf203":"pred_table = pd.DataFrame({'True values': y_test,\n             'Predicted values': pred}, \n            index=y_test.index)","9cdda2ae":"pred_table['Absolute'] = pred_table['True values'] - pred_table['Predicted values']\npred_table['Absolute'] = pred_table['Absolute'].apply('abs')\npred_table['Percentage'] = pred_table['Absolute'] \/ pred_table['True values']","291b9146":"def highlight_columns(df, rows=20, color='lightgreen', columns_to_highlight=[], columns_to_show=[]):\n    \"\"\"\n    Highlights selected columns of dataframe\n    \"\"\"\n    highlight = lambda slice_of_df: 'background-color: %s' % color\n    sample_df = df.head(rows)\n    if len(columns_to_show) != 0:\n        sample_df = sample_df[columns_to_show]\n    highlighted_df = sample_df.style.applymap(highlight, subset=pd.IndexSlice[:, columns_to_highlight])\n    return highlighted_df","d0aec0ce":"abs_error = pred_table.sort_values(by='Absolute', ascending=False)\nhighlight_columns(abs_error[:10], columns_to_highlight=['Absolute'])","6a2f4b64":"plt.figure(figsize=(8,5))\n\nsns.distplot(abs_error['True values'], color='deepskyblue')\nplt.axvline(x=400000, color='orange', linewidth = 2)\nplt.title('Distribution of True values', fontsize=16)\n\nplt.show()","5b79d094":"# Blank page","1868bff4":"prc_error = pred_table.sort_values(by='Percentage', ascending=False)\nhighlight_columns(prc_error[:10], columns_to_highlight=['Percentage'])","7d924c9b":"plt.figure(figsize=(8,5))\n\nsns.distplot(abs_error['True values'], color='deepskyblue')\nplt.axvline(x=70000, color='orange', linewidth = 2)\nplt.title('Distribution of True values', fontsize=16)\n\nplt.show()","2eb951d6":"# Blank page","108ead85":"## 1.0 Understanding business problem <br>2.0 Data collection","237152de":"But it is a topic for another talk.<br>\n<i>Thank you for your time.<\/i>","151e172a":"## 5.0 Feature selection","0cadadb4":"### 3.2 Enocoders\nI'll use 2 different encoders. Which one is better I already know, but I want to show the difference.","f5561d7f":"#### With this kind of a plot we expect dots to be closer to the yellow line(45 degree), which will indicate that our predictions match with true values. But, as we can see, there are some problem zones(circles). <i>Circle 1<\/i> includes predictions with high absolute errors(see the table below). In contrast to that, <i>Circle 2<\/i> includes values with high percentage errors(see the table below). ","2d87b591":"# Every data science project's full pipeline on the example of House Prices data.","718f27e4":"## 7.0 Model deployment","be6ecc8c":"##### Target encoder","9144ebdb":"As data is taken from [kaggle](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques), I'll skip this 2 steps.","8188f5fe":"##### We can see from the above, that Random Forest and  XGB Regression algorithms show better results. But to make sure about my choice, I'll evaluate them on test data.","89c3701f":"I considered three models:<br>\n1) Simple Linear regression <br>\n2) Random forest <br>\n3) XGBoost regression <br>","b922d78a":"##### [GLMM](https:\/\/contrib.scikit-learn.org\/category_encoders\/glmm.html) encoder","f856bace":"I'll not show the whole process of tunning hyperparameters with GridSearchCV from sklearn package. But here what I found:<br><br>\n\u2022 With more <i>n_estimators<\/i> the model predicts more accurate, which is obvious, but that improvement  isn't significant (about 100-300\\$ in the case of absolute error). So I decided to use 1000 estimators for more convenience.<br>\n\u2022 <i>max_depth<\/i> and <i>min_samples_leaf<\/i>: As these two parameters are related to each other I changed only one - <i>min_samples_leaf<\/i>. GridSearchCV showed that the optimal value for this parameter is 1, but common sense tells that it will cause overfitting, so I'll use <i>min_samples_leaf=2<\/i>.<br>\n\u2022 <i>max_features<\/i>: I'll use the best result from GridSearchCV - 'sqrt'<br>\n\u2022 Other parameters will be default.\n<br><br>\nThus, final parametrs will be:<br><br>\n<b>n_estimators<\/b> = 1000<br>\n<b>min_samples_leaf<\/b> = 2<br>\n<b>max_features<\/b> = 'sqrt'","cc1657ed":"All credits for this function goes to [this guy](https:\/\/towardsdatascience.com\/highlighting-columns-in-pandas-dataframe-bf2ff77d00bc).","e868758b":"##### Categorical features","32e768ae":"##### It can be noticed from the table above, that most objects have True values more than 400000. It indicates, that algorithm mostly makes mistakes with extremely high True values.","ce7c8d6e":"##### Numerical features","89217db6":"##### Conversely, here we can see, that objects with high percentage errors are those, which have relatively low prices.","27d74ba6":"## 6.0 Model selection","ab44a266":"<i>Enjoy your reading<\/i>","e7127cd0":"##### From the barplots it becomes clear that Random Forest shows more accurate results on average. Further, I'll use glmm data as in my draft notebook it shows better results.","86eb33d8":"## Summary","1f887682":"Apparently, the reason why Random Forest couldn't explain target variance well is its proclivity to overfeeting. If I look at these problem objects, and explore their features more deeply, maybe I'll find the actual reason of their extreme prices and I will be able to fix model to detect those extreme reasons.","2af1eaec":"## 3.0 Data preporation","688f0330":"### 3.1 Imputers: handling missing values","431e5112":"For feature selection I'll use SelectKBest from sklearn package. In my draft notebook I tried other methods(like Linear Regression with looking at coefficients, and SelectFromModel from sklearn package with Lasso, but they showed worse results.","0d7954d6":"#### 10 objects with the highest absolute errors","e809de16":"## 8.0 Results visualization","19d55de5":"I already done data exploration im my previous notebooks. You can find them in my [github](https:\/\/github.com\/GevHovh\/my_resume).","987e1d8e":"## 4.0 Data exploration","893d2276":"#### 10 objects with the highest percentage errors"}}