{"cell_type":{"28029f19":"code","a7ef8a21":"code","4788e601":"code","4188cc75":"code","95ea5fe8":"code","1994b332":"code","5cc893b9":"code","330e1366":"code","70376b64":"code","3926ebb8":"code","b11df5cf":"code","5e58b9a6":"code","6d33a233":"code","e88645d9":"code","4ca07de9":"code","93056439":"code","d8acbc68":"code","bf3c1385":"code","91a4d058":"code","9cb3685a":"code","c9bccb6b":"code","f6cb27a9":"code","e82c07cf":"code","d6bc8c72":"code","188838fe":"code","83897fa3":"code","566b0d5b":"markdown","042f9f87":"markdown","607a407b":"markdown","35f43928":"markdown","fc368604":"markdown","aa61181c":"markdown","31a8f973":"markdown","5f0877e8":"markdown","37386d73":"markdown"},"source":{"28029f19":"import pandas as pd\nimport numpy as np\n\nimport gzip, pickle","a7ef8a21":"import matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline","4788e601":"from sklearn.linear_model import SGDClassifier \n\nfrom sklearn.ensemble import BaggingClassifier \nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc","4188cc75":"def plotConfusionMatrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax","95ea5fe8":"!ls ..\/input\/mnistpklgz\nwith gzip.open(\"..\/input\/mnistpklgz\/mnist.pkl.gz\",\"rb\") as ff :\n    u = pickle._Unpickler( ff )\n    u.encoding = \"latin1\"\n    train, val, test = u.load()","1994b332":"print( train[0].shape, train[1].shape )","5cc893b9":"print( val[0].shape, val[1].shape )","330e1366":"print( test[0].shape, test[1].shape )","70376b64":"some_digit = train[0][0]\nsome_digit_image = some_digit.reshape(28, 28)\nplt.imshow(some_digit_image, cmap = mpl.cm.binary, interpolation=\"lanczos\")\nplt.axis(\"off\")\nplt.show()","3926ebb8":"train[1][0]","b11df5cf":"X_train = train[0]\nX_val = val[0]\nX_test = test[0]","5e58b9a6":"y_train = train[1].astype(np.uint8)\ny_val = val[1].astype(np.uint8)\ny_test = test[1].astype(np.uint8)","6d33a233":"from sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\nimport pandas as pd\n\nclf = DecisionTreeClassifier(random_state=0)\niris = load_iris()\niris_pd = pd.DataFrame(iris.data, columns=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"])\nclf = clf.fit(iris_pd, iris.target)","e88645d9":"print(dict(zip(iris_pd.columns, clf.feature_importances_)))","4ca07de9":"bagClf = BaggingClassifier(\n    DecisionTreeClassifier(), n_estimators=100,\n    max_samples=100, bootstrap=True\n)\nbagClf.fit(X_train, y_train)\ny_pred = bagClf.predict(X_test)","93056439":"set(y_test)","d8acbc68":"unique_labels(y_test, y_pred)","bf3c1385":"classesName = np.array(range(10))","91a4d058":"featureImportances = np.mean([\n    tree.feature_importances_ for tree in bagClf.estimators_\n], axis=0)","9cb3685a":"baggingPixelImportances = featureImportances.reshape(28, 28)\nfig, ax = plt.subplots()\nim = ax.imshow(baggingPixelImportances, interpolation=\"lanczos\", cmap=mpl.cm.afmhot)\nax.figure.colorbar(im, ax=ax)\nplt.show()","c9bccb6b":"## Confusion matrix\nplotConfusionMatrix(y_test, y_pred, classesName)","f6cb27a9":"rndClf = RandomForestClassifier(\n    n_estimators=100, max_leaf_nodes=16, n_jobs=-1\n)\nrndClf.fit(X_train, y_train)\ny_pred_rf = rndClf.predict(X_test)","e82c07cf":"rndClf.feature_importances_","d6bc8c72":"pixelImportance = rndClf.feature_importances_.reshape(28, 28)","188838fe":"fig, ax = plt.subplots()\nim = ax.imshow(pixelImportance, interpolation=\"lanczos\", cmap=mpl.cm.afmhot)\nax.figure.colorbar(im, ax=ax)\nplt.show()","83897fa3":"## Confusion matrix\nplotConfusionMatrix(y_pred_rf, y_pred, classesName)","566b0d5b":"### 1. Loading MNIST data","042f9f87":"### 2. Train a Ensemble of Decision Trees","607a407b":"### 0. Functions","35f43928":"MNIST dataset, a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau","fc368604":"#### 2.2. Example","aa61181c":"### 3. Train a Random Forest with the same Setup","31a8f973":"#### 2.1. Single Tree Feature Importances","5f0877e8":"Load the MNIST data (introduced in Chapter 3), and \nsplit it into a training set, a validation set, and a test set \n(e.g., use 50,000 instances for training, 10,000 for validation, and 10,000 for testing). \nThen train various classifiers, such as a Random Forest classifier, an Extra-Trees classifier, and an SVM. \nNext, try to combine them into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier. \nOnce you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?","37386d73":"[MNIST Dataset](http:\/\/deeplearning.net\/data\/mnist\/mnist.pkl.gz)"}}