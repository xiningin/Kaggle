{"cell_type":{"f994d61b":"code","b21c97e2":"code","cb6e16fc":"code","cfb765af":"code","743a8690":"code","13061918":"code","670b4ba0":"code","f435534c":"code","e4ac6e09":"code","18f4a8d7":"code","46b8c3f2":"code","419c901e":"code","d56fa5b6":"code","80261d3f":"code","384392d7":"code","8c22d9ef":"code","77e88c15":"code","50f07748":"code","f1115fcf":"code","aa746e06":"code","3e6959d1":"code","43f6829e":"code","8b6f0806":"code","b8b5f698":"code","6ee235c4":"code","5bc59d04":"code","4f9443d7":"markdown"},"source":{"f994d61b":"import numpy as np \nimport pandas as pd \n\nimport os\nprint(os.listdir(\"..\/input\"))","b21c97e2":"import os\nprint(os.listdir(\"..\/working\"))\nimport pandas as pd\npd.DataFrame([1,2,3,4]).to_csv(\"..\/working\/test.csv\")\nprint(os.listdir(\"..\/working\"))","cb6e16fc":"import torch","cfb765af":"x = torch.empty(5, 3)\nprint(x)","743a8690":"x = torch.rand(5, 3)\nprint(x)","13061918":"x = torch.zeros(5, 3, dtype=torch.long)\nprint(x)\n","670b4ba0":"x = torch.tensor([5.5, 3])\nprint(x)","f435534c":"x = x.new_ones(5, 3, dtype=torch.double)      \nprint(x)","e4ac6e09":"x = torch.randn_like(x, dtype=torch.float)    \nprint(x) ","18f4a8d7":"print(x.size())","46b8c3f2":"y = torch.rand(5, 3)\nprint(x + y)","419c901e":"y.view(-1,5).mm(x)","d56fa5b6":"result = torch.empty(5, 3)\ntorch.add(x, y, out=result)\nprint(result)","80261d3f":"y.add_(x)\nprint(y)","384392d7":"print(x[:, 1])","8c22d9ef":"x = torch.randn(4, 4)\ny = x.view(16)\nz = x.view(-1, 8)  # the size -1 is inferred from other dimensions\nprint(x.size(), y.size(), z.size())","77e88c15":"x = torch.randn(1)\nprint(x)\nprint(x.item())","50f07748":"a = torch.ones(5)\nprint(a)","f1115fcf":"b = a.numpy()\nprint(b)","aa746e06":"a.add_(1)\nprint(a)\nprint(b)","3e6959d1":"\nimport numpy as np\na = np.ones(5)\nb = torch.from_numpy(a)\nnp.add(a, 1, out=a)\nprint(a)\nprint(b)","43f6829e":"torch.cuda.is_available()","8b6f0806":"device = torch.device(\"cuda\")          \ny = torch.ones_like(x, device=device)  \nx = x.to(device)                       \nz = x + y\nprint(z)\nprint(z.to(\"cpu\", torch.double))","b8b5f698":"import numpy as np\nimport time\n\n# N is batch size; D_in is input dimension;\n# H is hidden dimension; D_out is output dimension.\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# Create random input and output data\nx = np.random.randn(N, D_in)\ny = np.random.randn(N, D_out)\n\n# Randomly initialize weights\nw1 = np.random.randn(D_in, H)\nw2 = np.random.randn(H, D_out)\n\nstart_time=time.time()\nlearning_rate = 1e-6\nfor t in range(1):\n    # Forward pass: compute predicted y\n    h = x.dot(w1)\n    h_relu = np.maximum(h, 0)\n    y_pred = h_relu.dot(w2)\n\n    # Compute and print loss\n    loss = np.square(y_pred - y).sum()\n    print(t, loss)\n\n    # Backprop to compute gradients of w1 and w2 with respect to loss\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_w2 = h_relu.T.dot(grad_y_pred)\n    grad_h_relu = grad_y_pred.dot(w2.T)\n    grad_h = grad_h_relu.copy()\n    grad_h[h < 0] = 0\n    grad_w1 = x.T.dot(grad_h)\n\n    # Update weights\n    w1 -= learning_rate * grad_w1\n    w2 -= learning_rate * grad_w2\n    \nend_time=time.time()\nprint(end_time-start_time)","6ee235c4":"import torch\nimport time\n\ndtype = torch.float\ndevice = torch.device(\"cpu\")\n\n# N is batch size; D_in is input dimension;\n# H is hidden dimension; D_out is output dimension.\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# Create random input and output data\nx = torch.randn(N, D_in, device=device, dtype=dtype)\ny = torch.randn(N, D_out, device=device, dtype=dtype)\n\n# Randomly initialize weights\nw1 = torch.randn(D_in, H, device=device, dtype=dtype)\nw2 = torch.randn(H, D_out, device=device, dtype=dtype)\n\nstart_time=time.time()\nlearning_rate = 1e-6\nfor t in range(1):\n    # Forward pass: compute predicted y\n    h = x.mm(w1)\n    h_relu = h.clamp(min=0)\n    y_pred = h_relu.mm(w2)\n\n    # Compute and print loss\n    loss = (y_pred - y).pow(2).sum().item()\n    print(t, loss)\n\n    # Backprop to compute gradients of w1 and w2 with respect to loss\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_w2 = h_relu.t().mm(grad_y_pred)\n    grad_h_relu = grad_y_pred.mm(w2.t())\n    grad_h = grad_h_relu.clone()\n    grad_h[h < 0] = 0\n    grad_w1 = x.t().mm(grad_h)\n\n    # Update weights using gradient descent\n    w1 -= learning_rate * grad_w1\n    w2 -= learning_rate * grad_w2\n    \nend_time=time.time()\nprint(end_time-start_time)","5bc59d04":"import torch\nimport time\n\ndtype = torch.float\ndevice = torch.device(\"cuda:0\")\n\n# N is batch size; D_in is input dimension;\n# H is hidden dimension; D_out is output dimension.\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# Create random input and output data\nx = torch.randn(N, D_in, device=device, dtype=dtype)\ny = torch.randn(N, D_out, device=device, dtype=dtype)\n\n# Randomly initialize weights\nw1 = torch.randn(D_in, H, device=device, dtype=dtype)\nw2 = torch.randn(H, D_out, device=device, dtype=dtype)\n\nstart_time=time.time()\nlearning_rate = 1e-6\nfor t in range(1):\n    # Forward pass: compute predicted y\n    h = x.mm(w1)\n    h_relu = h.clamp(min=0)\n    y_pred = h_relu.mm(w2)\n\n    # Compute and print loss\n    loss = (y_pred - y).pow(2).sum().item()\n    print(t, loss)\n\n    # Backprop to compute gradients of w1 and w2 with respect to loss\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_w2 = h_relu.t().mm(grad_y_pred)\n    grad_h_relu = grad_y_pred.mm(w2.t())\n    grad_h = grad_h_relu.clone()\n    grad_h[h < 0] = 0\n    grad_w1 = x.t().mm(grad_h)\n\n    # Update weights using gradient descent\n    w1 -= learning_rate * grad_w1\n    w2 -= learning_rate * grad_w2\n    \nend_time=time.time()\nprint(end_time-start_time)","4f9443d7":"**Simple Neural Net using Numpy**"}}