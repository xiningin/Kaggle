{"cell_type":{"82186f92":"code","e15777ce":"code","121fa833":"code","13b4a2da":"code","7c8f650d":"code","23820522":"code","c4f189cc":"code","cea191a8":"code","4b55bd63":"code","af153a5c":"code","0e68328a":"code","ec5835ce":"code","2cd15169":"code","5d293f16":"code","10581b3f":"code","8eac8fe9":"code","71fca7b0":"code","0a7e9059":"code","62e3ad2c":"code","e23d9238":"markdown","fa85c4bb":"markdown","d4ae392b":"markdown","36977bca":"markdown","793f2a61":"markdown","5b81f74a":"markdown","a69a03ff":"markdown","5a3b81da":"markdown"},"source":{"82186f92":"#importing the required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#importing the train data\ntrain_df = pd.read_csv(\"..\/input\/bigmart-sales\/train_data.csv\")\n\n#now checking for NULL values\ntrain_df.isnull().sum()","e15777ce":"#replacing the missing data by mean in Item_Weight column\ntrain_df.Item_Weight.fillna(train_df.Item_Weight.mean(), inplace = True)\n\n#replacing the missing data by mode in Outlet_Size column\ntrain_df.Outlet_Size.fillna(train_df.Outlet_Size.mode()[0], inplace = True)","121fa833":"#checking for NULL values again\ntrain_df.isnull().sum()","13b4a2da":"#displaying the top 10 rows from the dataset\ntrain_df.head(10)","7c8f650d":"#displaying the bottom 10 rows from the dataset\ntrain_df.tail(10)","23820522":"#importing the required library\nimport category_encoders as ce\n\n#creating an object of One Hot Encoder\none = ce.OneHotEncoder(cols = ['Item_Fat_Content',\n                               'Item_Type',\n                               'Outlet_Identifier',\n                               'Outlet_Size',\n                               'Outlet_Location_Type',\n                               'Outlet_Type'], use_cat_names = True)\n\n#encoding the categorical values\ntrain_df = one.fit_transform(train_df)","c4f189cc":"#importing the required library\nfrom sklearn.preprocessing import StandardScaler\n\n#creating an object of StandardScaler\nscaler = StandardScaler()\n\n#fitting with Item_MRP\nscaler.fit(np.array(train_df.Item_MRP).reshape(-1, 1))\n\n#transforming the data\ntrain_df.Item_MRP = scaler.transform(np.array(train_df.Item_MRP).reshape(-1, 1))","cea191a8":"#importing required libraries\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error","4b55bd63":"#seperating the independent and target variables\ntrain_X = train_df.drop(columns = ['Item_Identifier', 'Item_Outlet_Sales'])\ntrain_Y = train_df['Item_Outlet_Sales']\n\n#now split the data randomly\ntrain_x, test_x, train_y, test_y = train_test_split(train_X, train_Y, test_size = 0.25, random_state = 0)\n\n#now display the shape of train and test data\ntrain_x.shape, test_x.shape, train_y.shape, test_y.shape","af153a5c":"#creating an object of linear regression model\nmodel_LR = LinearRegression()\n\n#fitting the model with train data\nmodel_LR.fit(train_x, train_y)\n\n#predicting the trarget on train and test datasets\npredict_train = model_LR.predict(train_x)\npredict_test = model_LR.predict(test_x)\n\n#Root Mean Squared Error on train and test datasets\nprint(\"RMSE on train dataset : \", mean_squared_error(train_y, predict_train)**(0.5))\nprint(\"RMSE on test  dataset : \", mean_squared_error(test_y, predict_test)**(0.5))","0e68328a":"#creating an object for Random Forest Regressor\nmodel_RFR = RandomForestRegressor(max_depth = 10)\n\n#fitting the model with traning data\nmodel_RFR.fit(train_x, train_y)\n\n#predicting the target on train and test data\npredict_train = model_RFR.predict(train_x)\npredict_test = model_RFR.predict(test_x)\n\n#finding the Root Mean Squared Error on train and test datasets\nprint(\"RMSE on train dataset : \", mean_squared_error(train_y, predict_train) ** (0.5))\nprint(\"RMSE on test  dataset : \", mean_squared_error(test_y, predict_test) ** (0.5))","ec5835ce":"#plotting the 7 most important features of the dataset\nplt.figure(figsize = (10, 7))\nfeat_importances = pd.Series(model_RFR.feature_importances_, index = train_x.columns)\nfeat_importances.nlargest(7).plot(kind = 'barh');","2cd15169":"#now we train the RandomForest Model using these features\n#training data with 7 most important features\ntrain_x_if = train_x[['Item_MRP', \n                      'Outlet_Type_Grocery Store', \n                      'Item_Visibility', \n                      'Outlet_Type_Supermarket Type3', \n                      'Outlet_Identifier_OUT027', \n                      'Outlet_Establishment_Year', \n                      'Item_Weight']]\n\n#testing data with 7 most important features\ntest_x_if = test_x[['Item_MRP', \n                    'Outlet_Type_Grocery Store', \n                    'Item_Visibility', \n                    'Outlet_Type_Supermarket Type3', \n                    'Outlet_Identifier_OUT027', \n                    'Outlet_Establishment_Year', \n                    'Item_Weight']]\n\n#creating an object of RandomForestRegressor model\nmodel_RFR_with_if = RandomForestRegressor(max_depth = 10, random_state = 2)\n\n#fitting the model with training data\nmodel_RFR_with_if.fit(train_x_if, train_y)\n\n#predicting the target on train and test data\npredict_train_with_if = model_RFR_with_if.predict(train_x_if)\npredict_test_with_if = model_RFR_with_if.predict(test_x_if)\n\n#Calculating Root Mean Square Error on train and test data\nprint('RMSE on train data: ', mean_squared_error(train_y, predict_train_with_if) ** (0.5))\nprint('RMSE on test data: ', mean_squared_error(test_y, predict_test_with_if) ** (0.5))","5d293f16":"#Importing the required libraries\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\n\n#Reading the dataset again\ndata = pd.read_csv('..\/input\/bigmart-sales\/train_data.csv')\n\n#Displaying the top 5 rows\ndata.head()","10581b3f":"#Seperating the independent and target values\ntrain_x = data.drop(columns = ['Item_Outlet_Sales'])\ntrain_y = data['Item_Outlet_Sales']","8eac8fe9":"#Importing the Base Estimator\nfrom sklearn.base import BaseEstimator\n\n#Defining the class OutletTypeEncoder\n#This is our custom transformer which will create three new binary columns\n#Custom transformers must have methods fit and transform\nclass OutletTypeEncoder(BaseEstimator):\n    def __init__(self):\n        pass\n    def fit(self, documents, y = None):\n        return self\n    def transform(self, x_dataset):\n        x_dataset['outlet_grocery_store'] = (x_dataset['Outlet_Type'] == 'Grocery Store') * 1\n        x_dataset['outlet_supermarket_3'] = (x_dataset['Outlet_Type'] == 'Supermarket Type3') * 1\n        x_dataset['outlet_IDENTIFIER_out027'] = (x_dataset['Outlet_Identifier'] == 'OUT027') * 1\n        \n        return x_dataset","71fca7b0":"#Preprocessing step\n#Dropping the columns\n#Imputing the missing values in columns Item_Weight by mean\n#Scale the data in the column Item_MRP\npre_process = ColumnTransformer(remainder = 'passthrough',\n                              transformers = [('drop_columns', 'drop', ['Item_Identifier',\n                                                                        'Outlet_Identifier',\n                                                                        'Item_Fat_Content',\n                                                                        'Item_Type',\n                                                                        'Outlet_Identifier',\n                                                                        'Outlet_Size',\n                                                                        'Outlet_Location_Type',\n                                                                        'Outlet_Type'\n                                                                       ]), \n                                               ('impute_item_weight', SimpleImputer(strategy = 'mean'), ['Item_Weight']),\n                                               ('scale_data', StandardScaler(), ['Item_MRP'])]) ","0a7e9059":"#Defining the pipeline\n#Step 1: get the outlet binary columns\n#Step 2: pre processing\n#Step 3: training a Random Forest Model\nmodel_pipeline = Pipeline(steps = [('get_outlet_binary_columns', OutletTypeEncoder()),\n                                   ('pre_processing', pre_process),\n                                   ('random_forest', RandomForestRegressor(max_depth = 10, random_state = 2))\n                                  ])\n\n#fitting the pipeline with training data\nmodel_pipeline.fit(train_x, train_y)\n\n#predicting the target values on the training data\nmodel_pipeline.predict(train_x)","62e3ad2c":"#reading the test data\ntest_data = pd.read_csv('..\/input\/test-data\/test_data.csv')\n\n#predicting the target values on the test data\nmodel_pipeline.predict(test_data)","e23d9238":"### Step 6: Building Pipeline","fa85c4bb":"### Step 4: Building the Model","d4ae392b":"### Step 2: Encoding the Categorical Values","36977bca":"### Step 7: Predicting the Target","793f2a61":"### Step 5: Feature Importance","5b81f74a":"### Step 1: Data Importing, Exploration and Preprocessing\n\nHere we import the required libraries and check for NULL values present in the dataset","a69a03ff":"### Step 3: Scaling the Data","5a3b81da":"## Machine Learning Pipeline using Scikit-Learn\n\nIn this project we will explore our BigMart Sales dataset and develop a Machine Learning pipeline to forecast the sales of product in the stores"}}