{"cell_type":{"3d34a403":"code","0c647966":"code","48079a69":"code","6268eaee":"code","c5afc8de":"code","c9a9a5f5":"code","d7697597":"code","8eec35c6":"code","ff812b0d":"code","98801302":"code","7e4f5000":"code","969c424a":"code","3bb7f036":"code","a2db96e5":"code","2e0b9bea":"code","e5ead5bf":"code","4690564e":"code","1bdaba76":"code","7c4fc39c":"code","d85f1a30":"code","e2ebc27c":"code","1f5a3228":"code","cd69bee3":"code","0eb58ad6":"code","1d28f9bc":"code","4751262c":"code","a3533383":"code","a6eb4558":"code","579ca150":"code","108a41af":"code","6f6eb267":"code","a7033643":"code","2a8fc3e0":"code","90517fb2":"code","2e700ff8":"code","37a6fc86":"code","f7b2dc21":"code","e1c08618":"code","4dfe95bc":"code","e80a66f3":"code","2c071c05":"code","886cb65d":"code","d4f6f132":"code","6dbe7dd6":"code","674fec26":"code","4fd64646":"code","cf094698":"code","06e141f1":"code","3492c883":"code","83b84205":"code","2fb36514":"code","c7eb40b4":"code","468b476c":"code","6e5b750c":"code","efb1469c":"code","576718e4":"code","b30155f9":"code","d8dab357":"code","58655786":"code","f2d8f2e2":"code","2167e6c6":"markdown","fedf095d":"markdown","ee0c7eba":"markdown","7a1b7e0e":"markdown","36a04050":"markdown","59dac443":"markdown","51ca9336":"markdown","a0d2fc60":"markdown","fa7b6008":"markdown","3d06746b":"markdown","e07aed10":"markdown","d53eb0ae":"markdown","482ef1bc":"markdown","1025a6b9":"markdown","ad9977d8":"markdown","3fa5d94c":"markdown","8c2c5d21":"markdown","ed0798fd":"markdown","52445b8a":"markdown","4dec4c64":"markdown","55a83caa":"markdown","9fc1ed12":"markdown","d9609675":"markdown","f2e377ab":"markdown","8e7400da":"markdown","4d075e28":"markdown","2b0ee2f2":"markdown","a66aa33a":"markdown","735639a7":"markdown","1c090858":"markdown","f336864f":"markdown","1cabd973":"markdown","1c7a6e8e":"markdown","56bec99d":"markdown"},"source":{"3d34a403":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0c647966":"%precision %.2f","48079a69":"import plotly.graph_objs as go","6268eaee":"df=pd.read_csv('..\/input\/WA_Fn-UseC_-HR-Employee-Attrition.csv')","c5afc8de":"df.sample(4)","c9a9a5f5":"df.info()","d7697597":"df.describe(include='all').fillna(' ')","8eec35c6":"print('Attrition count')\nprint(df.Attrition.value_counts())\nprint('Attrition in percentage')\nprint(df.Attrition.value_counts()*100\/len(df))","ff812b0d":"import plotly.plotly as py\nimport cufflinks as cf\ncf.set_config_file(offline=True, world_readable=True, theme='ggplot')\n","98801302":"df.Attrition.value_counts().iplot(kind='bar',title='Bar graph of attrition count in both category')","7e4f5000":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import style\nstyle.use('ggplot')\nimport warnings\nwarnings.filterwarnings('ignore')","969c424a":"plt.figure(figsize=(20,10))\nplt.subplot(221)\nsns.distplot(df.Age[df.Gender=='Male'],label='Male avg age=36.65',norm_hist=True,color='c',hist=False)\nplt.ylabel('Density')\nplt.axvline(df.Age[df.Gender=='Male'].mean(),linestyle='dashed',color='c',linewidth=1)\nsns.distplot(df.Age[df.Gender=='Female'],label='Female avg age=37.32',norm_hist=True,color='k',hist=False)\nplt.axvline(df.Age[df.Gender=='Female'].mean(),linestyle='dashed',color='k',linewidth=2)\nplt.subplot(222)\nsns.distplot(df.Age,norm_hist=True,label='Age',hist=False,color='b')\nplt.axvline(df.Age.mean())","3bb7f036":"plt.figure(figsize=(20,10))\nplt.subplot(221)\nsns.distplot(df.Age[df.Attrition=='Yes'],label='attrition',norm_hist=True,color='c')\n#plt.ylabel('Density')\nplt.axvline(df.Age[df.Attrition=='Yes'].mean(),linestyle='dashed',color='c',linewidth=1)\nplt.subplot(222)\nsns.distplot(df.Age[df.Attrition=='No'],label='Age dist of non attrition',norm_hist=True,color='k')\nplt.axvline(df.Age[df.Gender=='Female'].mean(),linestyle='dashed',color='k',linewidth=2)\nplt.subplot(223)\nsns.distplot(df.Age,norm_hist=True,label='Age',hist=False,color='b')\nplt.axvline(df.Age.mean())","a2db96e5":"print('Employee dept wise')\nprint(df.Department.value_counts())\nprint('....Attrition departmet wise....')\nprint(df.Department[df.Attrition=='Yes'].value_counts())\nsns.countplot(df.Department[df.Attrition=='Yes'])","2e0b9bea":"print('Attriion rate for the R & D depat =',100*133\/961)\nprint('Attriion rate for the Sales depat =',100*92\/446)\nprint('Attriion rate for the HR depat =',100*12\/63)","e5ead5bf":"#Lets check thes summary of daily rate and monthly rate in attrition categaory\nprint('--------Daily and Monthly rate of the Attrition category--------')\nprint(df[df.Attrition=='Yes'].describe()[['DailyRate','MonthlyRate']])\nprint('--------Daily and Monthly rate of the Non-Attrition category--------')\nprint(df[df.Attrition=='No'].describe()[['DailyRate','MonthlyRate']])\n#Lets check the distribution\nplt.figure(figsize=(20,10))\nplt.subplot(121)\nsns.distplot(df.DailyRate[df.Attrition=='Yes'],label='Attrition',norm_hist=True,color='c',hist=False)\nsns.distplot(df.DailyRate[df.Attrition=='No'],label='No Attrition',norm_hist=True,color='b',hist=False)\nplt.title('Distribution of Daily rate')\nplt.subplot(122)\nsns.distplot(df.MonthlyRate[df.Attrition=='Yes'],label='Attrition',norm_hist=True,color='c',hist=False)\nsns.distplot(df.MonthlyRate[df.Attrition=='No'],label='No Attrition',norm_hist=True,color='b',hist=False)\nplt.title('Distribution of monthly rate')","4690564e":"#Now try to analyze the business travel impact on the attrition rate\nsns.countplot(x='BusinessTravel',hue='Attrition',data=df)","1bdaba76":"print('Percentage of attriton in Travel_Rarely',\n      100*len(df[(df.BusinessTravel=='Travel_Rarely') & (df.Attrition=='Yes')])\n      \/len(df[df.BusinessTravel=='Travel_Rarely']))\n\nprint('Percentage of attriton in Travel_Frequently',\n      100*len(df[(df.BusinessTravel=='Travel_Frequently') & (df.Attrition=='Yes')])\n      \/len(df[df.BusinessTravel=='Travel_Frequently']))\n\nprint('Percentage of attriton in Non_Travel',\n      100*len(df[(df.BusinessTravel=='Non-Travel') & (df.Attrition=='Yes')])\n      \/len(df[df.BusinessTravel=='Non-Travel']))","7c4fc39c":"#Now plot the distribtion of distance from home\n\nprint('--------Sumary of Distance from home in the Attrition category--------')\nprint(df[df.Attrition=='Yes'].describe()[['DistanceFromHome']])\nprint('--------Daily and Monthly rate of the Non-Attrition category--------')\nprint(df[df.Attrition=='No'].describe()[['DistanceFromHome']])\n#Lets check the distribution\nplt.figure(figsize=(7,7))\nsns.distplot(df.DistanceFromHome[df.Attrition=='Yes'],label='Attrition',norm_hist=True,color='c',hist=False)\nsns.distplot(df.DistanceFromHome[df.Attrition=='No'],label='No Attrition',norm_hist=True,color='b',hist=False)\n","d85f1a30":"dict(zip([1,2,3,4,5],['Below_College','College','Bachelor','Master','Doctor']))","e2ebc27c":"def education(value):\n    edu=dict(zip([1,2,3,4,5],['Below_College','College','Bachelor','Master','Doctor']))\n    if value in edu:\n        #print(value,edu[value])\n        return edu[value]","1f5a3228":"#list(map(lambda x:education(x),list(df.Education)))\n#df.Education.apply(lambda x:education(x))\ndf['Education_labels']=list(map(lambda x:education(x),list(df.Education)))","cd69bee3":"pd.concat([df.Education.apply(lambda x:education(x)),df.EducationField,df.Attrition],axis=1,ignore_index=True).head(2)","0eb58ad6":"#Education 1 'Below College' 2 'College' 3 'Bachelor' 4 'Master' 5 'Doctor'\n#Let uss find out the percentage of attrition in each education\n\nfor i in set(list(df.Education_labels)):\n    print('Percentage Attrition in {0} degree = {1} %'.format(i,100*len(df[(df.Education_labels==i) & (df.Attrition=='Yes')])\/\n                                                           len(df[(df.Education_labels==i)])))\n\n#See the distribution of attrition across the education field and education_labels(Master,bachelors\n#degree etc)\ng=sns.catplot(x='EducationField',hue='Attrition',col='Education_labels',data=df,\n            kind='count',height=4,aspect=0.9,col_wrap=3,sharex =True)\ng.set_xticklabels(rotation=45)","1d28f9bc":"for i in set(list(df.EducationField)):\n    print('Percentage Attrition in {0} field = {1} %'.format(i,100*len(df[(df.EducationField==i) & (df.Attrition=='Yes')])\/\n                                                           len(df[(df.EducationField==i)])))\n","4751262c":"def EnvironmentSatisfaction(value):\n    env=dict(zip([1,2,3,4],['Low','Medium','High','Very_High']))\n    if value in env:\n        return env[value]","a3533383":"df['EnvSatisfaction_labels']=list(map(lambda x:EnvironmentSatisfaction(x),list(df.EnvironmentSatisfaction)))","a6eb4558":"for i in set(list(df.EnvSatisfaction_labels)):\n    print('Percentage Attrition in {0} env satisfaction = {1} %'.format(i,100*len(df[(df.EnvSatisfaction_labels==i) & (df.Attrition=='Yes')])\/\n                                                           len(df[(df.EnvSatisfaction_labels==i)])))\nplt.figure(figsize=(10,5))\nplt.subplot(121)\nsns.countplot(x='EnvSatisfaction_labels', hue='Attrition',data=df)\n#plt.subplot(122)\nsns.catplot(x='EnvSatisfaction_labels', hue='Gender',data=df,col='Attrition',kind='count')","579ca150":"plt.figure(figsize=(10,5))\nsns.distplot(df.HourlyRate[df.Attrition=='Yes'],label='Attrion',color='c',norm_hist=True,hist=False)\nsns.distplot(df.HourlyRate[df.Attrition=='No'],label='No Attrition',color='black',hist=False)","108a41af":"#Job role wise \n\njr_A=dict(df.JobRole[df.Attrition=='Yes'].value_counts())\nplt.figure(figsize=(20,10))\nplt.subplot(121)\nplt.pie(jr_A.values(),labels=jr_A.keys(),autopct='%1.1f')\nplt.title('Job role under attrition')\n\nplt.subplot(122)\nms=dict(df.MaritalStatus[df.Attrition=='Yes'].value_counts())\nplt.pie(ms.values(),labels=ms.keys(),autopct='%1.1f')\nplt.title('MaritalStatus under attrition')\n","6f6eb267":"print('Average monthly income of the people who left the company in dollar= ',df.MonthlyIncome[df.Attrition=='Yes'].mean())\nprint('Median income of the people who left the company in dollar= ',df.MonthlyIncome[df.Attrition=='Yes'].median())\nprint('Average monthly income of the people who dont left the company in dollar= ',df.MonthlyIncome[df.Attrition=='No'].mean())\nprint('Median income of the people who don''t left the company in dollar= ',df.MonthlyIncome[df.Attrition=='No'].median())\nprint('Average Income of the people who left the company is {0} percentage below then the who dont left'.\n      format(100*(df.MonthlyIncome[df.Attrition=='No'].mean()-df.MonthlyIncome[df.Attrition=='Yes'].mean())\/df.MonthlyIncome[df.Attrition=='Yes'].mean()))\n#g=sns.FacetGrid(col='Attrition',data=df,height=6,aspect=0.9)\n#g.map(plt.hist,'MonthlyIncome')\nplt.figure(figsize=(20,4))\nplt.subplot(131)\nsns.countplot(df.NumCompaniesWorked)\nplt.title('Employee count with respect to no. of companies worked')\n\nplt.subplot(132)\nsns.countplot(df.NumCompaniesWorked[df.Attrition=='Yes'])\nplt.title('Attrition')\n\nplt.subplot(133)\nsns.countplot(df.NumCompaniesWorked[df.Attrition=='No'])\nplt.title('Non-Attrition')","a7033643":"plt.figure(figsize=(13,5))\nplt.subplot(131)\nsns.countplot(df.OverTime)\nplt.title('Overall')\nplt.subplot(132)\nsns.countplot(df.OverTime[df.Attrition=='Yes'])\nplt.title('Attrition')\nplt.subplot(133)\nsns.countplot(df.OverTime[df.Attrition=='No'])\nplt.title('Non-Attriton')","2a8fc3e0":"\nplt.figure(figsize=(20,5))\nplt.subplot(121)\nsns.violinplot(x='PercentSalaryHike',y='Attrition',data=df,hue='Gender',scale='count',inner='quartile',split=True)\nplt.subplot(122)\nsns.violinplot(x='PercentSalaryHike',y='Attrition',data=df,hue='PerformanceRating',inner='quartile',split='True')","90517fb2":"#g=sns.FacetGrid(col='Department',data=df,height=6,aspect=0.9)\n#g.map(plt.hist,'TotalWorkingYears',normed=1)\nplt.figure(figsize=(15,7))\nsns.violinplot(x='TotalWorkingYears',y='Attrition',hue='Department',data=df,inner='quartile',palette='Set2')","2e700ff8":"#t=dict(df.groupby(['Attrition','TrainingTimesLastYear'])['TrainingTimesLastYear'].count())\n#t.key(('No',0))\nt_a=dict(df.TrainingTimesLastYear[df.Attrition=='Yes'].value_counts())\nt_a.keys()","37a6fc86":"plt.figure(figsize=(15,7))\nplt.subplot(121)\nt_a=dict(df.TrainingTimesLastYear[df.Attrition=='Yes'].value_counts())\nplt.pie(t_a.values(),labels=t_a.keys(),autopct='%1.1f%%',colors = ['silver', 'yellowgreen', 'lightcoral', 'lightskyblue'])\nplt.title('Attrition')\nplt.xlabel('TrainingTimesLastYear')\nplt.subplot(122)\nt_a=dict(df.TrainingTimesLastYear[df.Attrition=='No'].value_counts())\n#sns.countplot(df.TrainingTimesLastYear[df.Attrition=='No'],normed=1)\nplt.pie(t_a.values(),labels=t_a.keys(),autopct='%1.1f%%')\nplt.title('No Attrition')\nplt.xlabel('TrainingTimesLastYear')","f7b2dc21":"dict_WorkLifeBalance={ 1:'Bad', 2:'Good', 3:'Better' ,4:'Best'}\ndict1=dict(df.WorkLifeBalance[df.Attrition=='Yes'].value_counts().sort_index())\ndict3=dict((dict_WorkLifeBalance[key],val*100\/len(df.Attrition[df.Attrition=='Yes'])) for key,val in dict1.items())\nplt.figure(figsize=(15,4))\nplt.subplot(121)\nsns.barplot(x=list(dict3.keys()),y=list(dict3.values()))\nplt.xlabel('WorkLifeBalance')\nplt.ylabel('% of people who attrited')\nplt.subplot(122)\ndict1=dict(df.WorkLifeBalance[df.Attrition=='No'].value_counts().sort_index())\ndict3=dict((dict_WorkLifeBalance[key],val*100\/len(df.Attrition[df.Attrition=='No'])) for key,val in dict1.items())\nsns.barplot(x=list(dict3.keys()),y=list(dict3.values()))\nplt.xlabel('WorkLifeBalance')\nplt.ylabel('% of people who do not attrited')","e1c08618":"#Drop the addition variables created in exploratory analysis\ndf.drop(['Education_labels','EnvSatisfaction_labels'],axis=1,inplace=True)\n#Lets Drop the not useful predictors\ndf.drop(['EmployeeCount','EmployeeNumber','Over18','StandardHours'],axis=1,inplace=True)\nprint('no of unique values in dept  ',np.unique(df.Department))\nprint('no of unique values in EducationField  ',np.unique(df.EducationField))\nprint('no of unique values in Gender  ',np.unique(df.Gender))\nprint('no of unique values in JobRole  ',np.unique(df.JobRole))\nprint('no of unique values in MaritalStatus  ',np.unique(df.MaritalStatus))\nprint('no of unique values in OverTime  ',np.unique(df.OverTime))\n","4dfe95bc":"#Lets use the one hot encoding to transform the categorical fatures\nfrom sklearn.preprocessing import OneHotEncoder\noneh_enc=OneHotEncoder()\noneh_features=oneh_enc.fit_transform(df[['BusinessTravel','EducationField','Gender','JobRole','MaritalStatus','OverTime']])","e80a66f3":"oneh_features=pd.DataFrame(oneh_features.toarray(),columns=oneh_enc.get_feature_names())\noneh_features.sample(3)","2c071c05":"oneh_features.drop(['x5_No','x2_Female','x5_No'],axis=1,inplace=True)","886cb65d":"l_ind=[]\n#Lets perform chi square test of independence\nfrom sklearn.feature_selection import chi2\nchisq,pval=chi2(oneh_features,df[['Attrition']])\nfor i in pval:\n    if i<0.05:\n        l_ind.append('Y')\n    else:\n        l_ind.append('N')\n","d4f6f132":"data={'chisq':chisq,'pval':pval,'ind':l_ind}\ndata=pd.DataFrame(data=data,index=[i for i in list(oneh_enc.get_feature_names()) if i not in ['x5_No','x2_Female','x5_No']])","6dbe7dd6":"data.sort_values(by=['chisq','ind'],ascending=False)","674fec26":"#Create a df having only significance levels\noneh_features=oneh_features[list(data[data.ind=='Y'].index)]","4fd64646":"#Observed frequency table\nObserved=pd.crosstab(df.Attrition,df.BusinessTravel,margins=True)\nObserved.index=['No', 'Yes', 'row_total']\nObserved.columns=['Non-Travel', 'Travel_Frequently', 'Travel_Rarely', 'col_total']\nObserved","cf094698":"#Calculate the expected frquency table\nExpected=pd.DataFrame(np.outer(Observed.iloc[0:2:,3:4],Observed.iloc[2:3:,0:3])\/1470)\nExpected.index=['No', 'Yes']\nExpected.columns=['Non-Travel', 'Travel_Frequently', 'Travel_Rarely']","06e141f1":"Expected","3492c883":"#pd.options.display.float_format = '{:,.2f}'.format\n\nfrom scipy.stats import chi2_contingency\nObserved=pd.crosstab(df.Attrition,df.BusinessTravel)\nchi2_contingency(observed=Observed)# Displays chi2, p, dof, ex","83b84205":"#create an empty dataframe to hold the chi2 and p value of the categorical predictors\n#As you can see, Gender is independent from the attrition hece we will drop it now\nl_chisq=[]\nl_pval=[]\nindependent_status=[]\nfor col in ['BusinessTravel','EducationField','Gender','JobRole','MaritalStatus','OverTime']:\n    \n    Observed=pd.crosstab(df.Attrition,df[col])\n    chi2, p, dof, ex=chi2_contingency(observed=Observed)\n    chi2=round(chi2,4)\n    p=round(p,3)\n    l_chisq.append(chi2)\n    l_pval.append(p)\n    if p<0.05:\n        independent_status.append('Y')\n    else:\n        independent_status.append('N')\nchisq_dict={'chisq':l_chisq,'pval':l_pval,'indicator':independent_status}\npd.DataFrame(data=chisq_dict,index=['BusinessTravel','EducationField','Gender','JobRole','MaritalStatus','OverTime'])","2fb36514":"#Now lets use another feature selection methods for categorical data\n#https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold\n#Lets use variance threshold method\n\nfrom sklearn.feature_selection import VarianceThreshold\nselector=VarianceThreshold(threshold=0.2)\nselector.fit_transform(oneh_features)\n","c7eb40b4":"selector.get_support(indices=True)","468b476c":"#Using mutual information to select the categorical features\nfrom sklearn.feature_selection import mutual_info_classif\nmi=mutual_info_classif(oneh_features,df[['Attrition']])\ndata={'Features':list(oneh_features.columns),'MI val':list(mi)}\nprint('Chi squae columns',oneh_features.columns)\npd.DataFrame(data=data).sort_values(by='MI val',ascending=False)\n","6e5b750c":"#['NumCompaniesWorked','StockOptionLevel',\n#                             'TrainingTimesLastYear','TotalWorkingYears','YearsAtCompany',\n#                             'YearsInCurrentRole','YearsSinceLastPromotion','YearsWithCurrManager']\nsns.distplot(df.Age)","efb1469c":"#from statsmodels.graphics.gofplots import qqplot\n#qqplot(df.Age)\n#df['TotalWorkingYears'].quantile([0, .25, .5, .75, 1.])","576718e4":"#Let us use the Anderson Darlington test\n#H0: the sample has a Gaussian distribution.\n#H1: the sample does not have a Gaussian distribution.\n#Tests whether a data sample has a Gaussian distribution.\n#Assumptions\n#Observations in each sample are independent and identically distributed (iid).\n\nfrom scipy.stats import anderson\n\nfor col in ['Age','DailyRate','DistanceFromHome','HourlyRate','MonthlyIncome','MonthlyRate','PercentSalaryHike']:\n    result=anderson(df[col])\n    print('Statistics for the {0} variable'.format(col),result)\n","b30155f9":"from scipy.stats import shapiro\nfor col in ['Age','DailyRate','DistanceFromHome','HourlyRate','MonthlyIncome','MonthlyRate','PercentSalaryHike']:\n    result=shapiro(df[col])\n    print('Statistics for the {0} variable statistics= {1} and P-val= {2}'.format(col,result[0],result[1]))","d8dab357":"from scipy.stats import kstest\nnp.random.seed(987654321)\nfor col in ['Age','DailyRate','DistanceFromHome','HourlyRate','MonthlyIncome','MonthlyRate','PercentSalaryHike']:\n    result=kstest(np.array(df[col]),cdf='norm')\n    print('Statistics for the {0} variable statistics= {1} and P-val= {2}'.format(col,result[0],result[1]))","58655786":"from scipy.stats import mannwhitneyu\n#Under the null hypothesis H0, the distributions of both populations are equal.[3]\n#The alternative hypothesis H1 is that the distributions are not equal.\n#mannwhitneyu(np.array(pd.get_dummies(df[['Attrition']],drop_first=True)).ravel(),np.array(df[['Age']]).ravel())\nfor col in ['Age','DailyRate','DistanceFromHome','HourlyRate','MonthlyIncome','MonthlyRate','PercentSalaryHike']:\n    \n    stat,p=mannwhitneyu(df[col][df.Attrition=='Yes'],df[col][df.Attrition=='No'],alternative='less')\n    print('----------------------------{0}----------------------------'.format(col))\n    print('Statistics=%.3f,p=%.4f'%(stat,p))\n    alpha=0.05\n    if p>alpha:\n        print('Same dist of feature {0} on both the categories, fail to reject null hypothesis'.format(col))\n    else:\n        print('Accept the alternate hypothesis i.e. values in attrition population of {0} is less than non attrition population of {1}'.format(col,col))","f2d8f2e2":"from scipy.stats import kruskal\nfor col in ['Age','DailyRate','DistanceFromHome','HourlyRate','MonthlyIncome','MonthlyRate','PercentSalaryHike']:\n    \n    stat,p=kruskal(df[col][df.Attrition=='Yes'],df[col][df.Attrition=='No'],alternative='less')\n    print('----------------------------{0}----------------------------'.format(col))\n    print('Statistics=%.3f,p=%.4f'%(stat,p))\n    alpha=0.05\n    if p>alpha:\n        print('Same dist of feature {0} on both the categories, fail to reject null hypothesis'.format(col))\n    else:\n        print('Accept the alternate hypothesis i.e. values in attrition population of {0} is differ than non attrition population of {1}'.format(col,col))","2167e6c6":"So above calculations shows that employee who travel frequently are having highet percentage of attrition","fedf095d":"**Now working on continuous data:-**\nWe have below features in continuous form\nAge,DailyRate,DistanceFromHome,HourlyRate,MonthlyIncome,MonthlyRate,PercentSalaryHike.\n\n**I will consider below variables as Discrete **\nNumCompaniesWorked,StockOptionLevel,TotalWorkingYears,TrainingTimesLastYear,YearsAtCompany,YearsInCurrentRole,YearsSinceLastPromotion,YearsWithCurrManager\n\nLet us check the normality of data points","ee0c7eba":"#Let us use the \n**Shapiro-Wilk Test**\nThe Shapiro-Wilk test tests the null hypothesis that the data was drawn from a normal distribution\n\nAssumptions\n\nObservations in each sample are independent and identically distributed (iid).\nInterpretation\n\nH0: the sample has a Gaussian distribution.\nH1: the sample does not have a Gaussian distribution.","7a1b7e0e":"Now I want to see department attrition overall then with respect to male and female","36a04050":"https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.anderson.html\nIf the returned statistic is larger than these critical values then for the corresponding significance level, the null hypothesis that the data come from the chosen distribution can be rejected. The returned statistic is referred to as \u2018A2\u2019 in the references.\n\nHere statistics is larger than the all critical values so we will reject the null hypothesis and accept the alternate. All the continuous varibles are not normal","59dac443":"We can't conclude much from distribution of daily rate but distribution of monthly rate suggests that distribution of aatrition is slighlty right shifted compare to not attrition category  so it suggest that higher monthly rate leads to attrition","51ca9336":"As it is clear that highest attrition in low enviornment satisfaction\n\n\nLet us analyze the hourly rate with attrtion","a0d2fc60":"I have divided the independent varaibles into three part\n\n1. Not useful-EmployeeCount,EmployeeNumber,Over18,StandardHours(Since these are not having any variability)\n\n2.Continuous variables\nAge,DailyRate,DistanceFromHome,HourlyRate,MonthlyIncome,MonthlyRate,NumCompaniesWorked,PercentSalaryHike,StockOptionLevel\n,TotalWorkingYears,TrainingTimesLastYear,YearsAtCompany,YearsInCurrentRole,YearsSinceLastPromotion,YearsWithCurrManager\n\n3.Nominal variables-\nBusiness Travel,EducationField,Gender,JobRole,MaritalStatus,OverTime,\n\n4.Ordinal variables\nEnvironmentSatisfaction,JobInvolvement,JobLevel,JobSatisfaction,PerformanceRating,RelationshipSatisfaction,WorkLifeBalance,Education\n\nAlso we need to encode the  BusinessTravel BusinessTravel\nBusinessTravel ,Department,EducationField,Gender,JobRole,MaritalStatus,OverTime\n","fa7b6008":"You can see that single left the company more frequently then married and  divorces are less tend to leave the company\n\nLet us plot the histogram of the monthly income in both the categories","3d06746b":"As it is clear from the figure that if training time of last year=0,2 or 4 then chances of attrition is higher. \n\nLet's analyze the data in WorkLifeBalance 1 'Bad' 2 'Good' 3 'Better' 4 'Best'","e07aed10":"AS you can see that at 95% confidence, levels which are having indicator=\"Y\" are related to the response.\n\n**Below is example to showcase that how to calculate chisquare statistics and p val**","d53eb0ae":"In non attriton data, we can see that as no. of companies worked increases employe will not tend to switch \n\nNow let us analyze the Impact of overtime on the aatrition","482ef1bc":"Questions we could Ask Ourselves:\nColumns and Observations: How many columns and observations is there in our dataset?\nMissing data: Are there any missing data in our dataset?\nData Type: The different datatypes we are dealing in this dataset.\nDistribution of our Data: Is it right-skewed, left-skewed or symmetric? This might be useful especially if we are implementing any type of statistical analysis or even for modelling.\nStructure of our Data: Some datasets are a bit complex to work with however, the tidyverse package is really useful to deal with complex datasets.\nMeaning of our Data: What does our data mean? Most features in this dataset are ordinal variables which are similar to categorical variables however, ordering of those variables matter. A lot of the variables in this dataset have a range from 1-4 or 1-5, The lower the ordinal variable, the worse it is in this case. For instance, Job Satisfaction 1 = \"Low\" while 4 = \"Very High\".\nLabel: What is our label in the dataset or in otherwords the output","1025a6b9":"From mannwhitneyu you came to know that Age,DailyRate,Montly Income are the significant variable because their distribution in both the categories are different\n\nWe can Also use another test Kruskal-Wallis H Test\nhttps:\/\/machinelearningmastery.com\/nonparametric-statistical-significance-tests-in-python\/\n\nscipy.stats.kruskal(*args, **kwargs)[source]\u00b6\nCompute the Kruskal-Wallis H-test for independent samples\n\nThe Kruskal-Wallis H-test tests the null hypothesis that the population median of all of the groups are equal. It is a non-parametric version of ANOVA. The test works on 2 or more independent samples, which may have different sizes. Note that rejecting the null hypothesis does not indicate which of the groups differs. Post-hoc comparisons between groups are required to determine which groups are different.\n\nFail to Reject H0: All sample distributions are equal.\nReject H0: One or more sample distributions are not equal.\n\n","ad9977d8":"only job role Healthcare Representative,x3_Human Resources,Laboratory Technician,Manager are having variability >0.2","3fa5d94c":"https:\/\/machinelearningmastery.com\/nonparametric-statistical-significance-tests-in-python\n\nhttps:\/\/statistics.laerd.com\/spss-tutorials\/mann-whitney-u-test-using-spss-statistics.php\n\nhttps:\/\/statistics.laerd.com\/spss-tutorials\/kruskal-wallis-h-test-using-spss-statistics.php","8c2c5d21":"Summary:\nDataset Structure: 1470 observations (rows), 35 features (variables)\nMissing Data: Luckily for us, there is no missing data! this will make it easier to work with the dataset.\nData Type: We only have two datatypes in this dataset: factors and integers\nLabel\" Attrition is the label in our dataset and we would like to find out why employees are leaving the organization!\nImbalanced dataset: 1237 (84% of cases) employees did not leave the organization while 237 (16% of cases) did leave the organization making our dataset to be considered imbalanced since more people stay in the organization than they actually leave.","ed0798fd":"As you can see a very nice pattern in this analysis that  'below_college_degree' person attrion percentage>college_degree>bachelor>master>doctor. So in general we can say that person\nwho is having less qualification is tend to leave the organization.\nAlso you can observe the attrition rate in education field. Highest attrition rate occured in Technocal degre and HR degree","52445b8a":"As it is clear that in HR department person with less experience tends to leave the organization. Also in sales department there are chances that person can leave the company after 10 years of workex as uper quartile value for both HR and R&D is less then 10 while Q3 for sales greater then 10\n\n","4dec4c64":"You can see in first and second  plot that avg age(non attrition)>avg age(attrition)\nMost of the people whot left the company, are  between the age 25 and 35. Once age is getting increased the tail is getting less heavier hence people are not tend to change the company in higher age","55a83caa":"\nLet us analyze the EnvironmentSatisfaction \n1 'Low' 2 'Medium' 3 'High' 4 'Very High'","9fc1ed12":"Lets calculate the attrition rate department wise","d9609675":"In overall dataset number of people are higher who do not do overtime. Also one interesting thing in the Attrition that people left the organization irrespective of the overtime as yoou can see in the midlde graph\n\nLet us check the impact of percentage salary hike","f2e377ab":"Result can be interpreted as (Stas,pval).You can see that all the P values are very small so we can reject the null hypothesis\n\n**Perform the Kolmogorov-Smirnov test for goodness of fit.**\nhttp:\/\/lagrange.univ-lyon1.fr\/docs\/scipy\/0.17.1\/generated\/scipy.stats.kstest.html\n\nThis performs a test of the distribution G(x) of an observed random variable against a given distribution F(x). Under the null hypothesis the two distributions are identical, G(x)=F(x). The alternative hypothesis can be either \u2018two-sided\u2019 (default), \u2018less\u2019 or \u2018greater\u2019. The KS test is only valid for continuous distributions.\n\nIn the one-sided test, the alternative is that the empirical cumulative distribution function of the random variable is \u201cless\u201d or \u201cgreater\u201d than the cumulative distribution function F(x) of the hypothesis, G(x)<=F(x), resp. G(x)>=F(x).","8e7400da":"\/All the P values are very small. It suggests that None of the above variables follow normal distribution","4d075e28":"Uncover the factors that lead to employee attrition and explore important questions such as \u2018show me a breakdown of distance from home by job role and attrition\u2019 or \u2018compare average monthly income by education and attrition\u2019. This is a fictional data set created by IBM data scientists.\n\nEducation 1 'Below College' 2 'College' 3 'Bachelor' 4 'Master' 5 'Doctor'\n\nEnvironmentSatisfaction 1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n\nJobInvolvement \n1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n\nJobSatisfaction 1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n\nPerformanceRating \n1 'Low' 2 'Good' 3 'Excellent' 4 'Outstanding'\n\nRelationshipSatisfaction \n1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n\nWorkLifeBalance 1 'Bad' 2 'Good' 3 'Better' 4 'Best'","2b0ee2f2":"Since data is not balanced. We have 'travel rarely' mostly in the data also attriton is highest in travel rarely","a66aa33a":"We can not conclude much from the hourly rate. As you can see from black curve that even higher hourly rate people don't leave the company","735639a7":"Distribution of the percentage salary hike between male and female ia approximately same. Also maximum attririon occured between 11-15 % bucket.\nAs percentge salary hike increases attrition decreases.\nAs you can see from thesecond plot that in rating 3(orange), average percentage salary hike is less in attrition group compare to non attriton group for the rating 3.\nBut in rating 4 group, one interesting fact that peoplr tend to left even they get high salary hike. You can see that distribution of pecent salary hike in no attriton category is right skewed while in attriton category it is notmal(Blue color)\n\nLet's analyze on the working experience:-\n","1c090858":"You can see that attrition rate is higher in bad work life balance","f336864f":"From above two tests you can understand that Age,DailyRate,Montly Income are strong predictors but Distance from home is coming as strong in kruskal-wallis test but it is not significant in mannwhitney test. You can observe that it is weak\/medium predictor it's p value in mann whitney test is 0.9988 but it is very less compare to other not significant predictors. We are going to include this in our model later on we will drop and will see the model performance\n\n'HourlyRate','MonthlyRate','PercentSalaryHike' are not significant at 95% confidence ","1cabd973":"Now figure out the age distribution in both the attriton category","1c7a6e8e":"As you can seeabove as distance from increases chances of leaving the organization also increases. Attrition supass the non attrition if distance from home >10","56bec99d":"You can see that attrition rate is highest and approx equal in the sales and HR department compare to R and D"}}