{"cell_type":{"478953fb":"code","307c3f4e":"code","c058cfbb":"code","24c8d340":"code","d2ef6e9c":"code","05272ff5":"code","7188b456":"code","29cb55f6":"code","d28ddf1b":"code","410068e3":"code","b8ef4a6b":"code","bd50cfa6":"code","9a9c2209":"code","2c55e468":"code","130d1384":"code","a9b50a1b":"code","882ce5c5":"code","2a20b7d4":"code","94e8f2e3":"code","268cd122":"code","9b76bf9a":"code","bdb458b9":"code","64a933a4":"code","6f2e27fd":"code","049d54b4":"code","92ee73e6":"markdown","c5bbbde1":"markdown","5d4bf4a1":"markdown","86d8ef35":"markdown","803208db":"markdown","1719a9a6":"markdown","e60efb10":"markdown","2103c792":"markdown","7c1ee2fe":"markdown","7df2c8ba":"markdown","9503a07a":"markdown","d6237c08":"markdown","effa8100":"markdown"},"source":{"478953fb":"import string\nimport re\nfrom  numpy import array, argmax, random, take\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributed\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import load_model\nfrom keras import optimizers\nimport matplotlib.pyplot as plt\n%matplotlib inline\npd.set_option('display.max_colwidth', 200)\n","307c3f4e":"# function to read raw text file\ndef read_text(filename):\n  # open file\n  file = open(filename, mode='rt', encoding='utf-8')\n  # read all text\n  text = file.read()\n  file.close()\n  return text","c058cfbb":"# split a text into sentences\ndef to_lines(text):\n  sents = text.strip().split('\\n')\n  sents = [i.split('\\t') for i in sents]\n  return sents","24c8d340":"data = read_text(\"..\/input\/german-to-english\/deu.txt\")\ndeu_eng = to_lines(data)\ndeu_eng = array(deu_eng)","d2ef6e9c":"deu_eng = deu_eng[:50000,:]","05272ff5":"# Let's take a look at our data\ndeu_eng","7188b456":"#empty lists\neng_l = []\ndeu_l = []\n\n# populate the lists with sentence lengths\nfor i in deu_eng[:,0]:\n  eng_l.append(len(i.split()))\n\nfor i in deu_eng[:,1]:\n  deu_l.append(len(i.split()))\n  ","29cb55f6":"length_df = pd.DataFrame({'eng':eng_l, 'deu':deu_l})","d28ddf1b":"length_df.hist(bins = 30)\nplt.show","410068e3":"# function to build a tokenizer\ndef tokenization(lines):\n  tokenizer = Tokenizer()\n  tokenizer.fit_on_texts(lines)\n  return tokenizer","b8ef4a6b":"# prepare english tokenizer\neng_tokenizer = tokenization(deu_eng[:, 0])\neng_vocab_size = len(eng_tokenizer.word_index) + 1\n\neng_length = 8\nprint('English Vocabulary Size: %d' % eng_vocab_size)","bd50cfa6":"# prepare deutch tokenizer\ndeu_tokenizer = tokenization(deu_eng[:, 1])\ndeu_vocab_size = len(deu_tokenizer.word_index) + 1\n\ndeu_length = 11\nprint('Deutch Vocabulary Size: %d' % deu_vocab_size)","9a9c2209":"# encode and pad sequences \ndef encode_sequences(tokenizer, length, lines):\n  # integer encode sequences\n  seq = tokenizer.texts_to_sequences(lines)\n  #pad sequences with 0 values\n  seq = pad_sequences(seq, maxlen=length, padding='post')\n  return seq","2c55e468":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(deu_eng, test_size=0.2, random_state = 12) ","130d1384":"# prepare training data \ntrainX = encode_sequences(deu_tokenizer, deu_length, train[:, 1])\ntrainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])","a9b50a1b":"# prepare validation data \ntestX = encode_sequences(deu_tokenizer, deu_length, test[:, 1])\ntestY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])","882ce5c5":"# build NMT model\ndef build_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n  model = Sequential()\n  model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n  model.add(LSTM(units))\n  model.add(RepeatVector(out_timesteps))\n  model.add(LSTM(units, return_sequences=True))\n  model.add(Dense(out_vocab, activation='softmax'))\n  return model","2a20b7d4":"model = build_model(deu_vocab_size, eng_vocab_size, deu_length, eng_length, 512)\nrms = optimizers.RMSprop(learning_rate=0.001)\nmodel.compile(optimizer=rms, loss='sparse_categorical_crossentropy')","94e8f2e3":"filename = 'model.h1.24_manish'\ncheckpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n\nhistory = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n                    epochs=30, batch_size=512,\n                    validation_split= 0.2,\n                    callbacks=[checkpoint], verbose=1)","268cd122":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['train','validation'])\nplt.show()","9b76bf9a":"import numpy as np\nmodel = load_model('model.h1.24_manish')\npreds = np.argmax(model.predict(testX.reshape((testX.shape[0],testX.shape[1]))), axis=-1)","bdb458b9":"def get_word(n, tokenizer):\n  for word, index in tokenizer.word_index.items():\n    if index == n:\n      return word\n  return None","64a933a4":"# convert predictions into text (English)\npreds_text = []\nfor i in preds:\n  temp = []\n  for j in range(len(i)):\n    t = get_word(i[j], eng_tokenizer)\n    if j>0:\n      if(t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n        temp.append('')\n      else:\n        temp.append(t)\n    else:\n      if(t == None):\n        temp.append('')\n      else:\n        temp.append(t)\n\n  preds_text.append(' '.join(temp))","6f2e27fd":"pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})","049d54b4":"pred_df.head(15)","92ee73e6":"Now comes the exciting part! Let us define our Seq2Seq model architecture. We are using an Embedding layer and an LSTM layer as our encoder and another LSTM layer followed by a Dense layer as the decoder.","c5bbbde1":"Please note that we have used 'sparse_categorical_crossentropy' as the loss function because it allows us to use the target sequence as it is instead of one hot encoded format. One hot encoding the target sequences with such a huge vocabulary might consume our system's entire memory.\n\nIt seems we are all set to start training our model. We will train it for 30 epochs and with a batch size of 512. You may change and play with the hyperparameters. We will also be using ModelCheckpoint() to save the best model with lowest validation loss. I personally prefer this method over early stopping.","5d4bf4a1":"It's time to encode the sentences. We will encode German sentences as the input sequences and English sentences as the target sequences. It will be done for both train and test datasets.","86d8ef35":"Let's compare the training loss and the validation loss.","803208db":"Text to Sequence Conversion:\n\nTo feed our data in a Seq2Seq model, we will have to convert both the input and the output sentences into integer sequences of fixed length. Before that, let's visualize the length of the sentences. We will capture the lengths of all the sentences in two separate lists for English and German, respectively.","1719a9a6":"Now let's define a function to split the text into English-German pairs separated by '\\n' and then split these pairs into English sentences and German sentences.","e60efb10":"Given below is a function to prepare the sequences. It will also perform sequence padding to a maximum sentence length as mentioned above","2103c792":"Model Building\n\nWe will now split the data into train and test set for model training and evaluation, respectively.","7c1ee2fe":"The maximum length of the German sentences is 11 and that of English phrases is 8. Let's vectorize our text data by using Keras's Tokenizer class. It will turn our sentences into sequences of the integers. Then we will pad those sequences with zeros to make all the sequences of same length.","7df2c8ba":"Let's load the saved model to make predictions.","9503a07a":"We are using RMSprop optimizer in the model as it is usually a good choice for recurrent neural networks.","d6237c08":"Our Data is a text file of English-German sentence pairs. First we will read the file using the function defined below.","effa8100":"The actual data contains over 150,000 sentence-pairs. However, we will use the first 50,000 sentence pairs only to reduce the training time of the model. You can change this number as per system computation power."}}