{"cell_type":{"c8eb825a":"code","ecff4e05":"code","9e840ff2":"code","5169f33b":"code","b02757de":"code","a480977a":"code","d293dd62":"code","eb4c7dd1":"code","1ce3da8a":"code","17a2b04c":"code","cbf8abb5":"code","d7cf1526":"code","1bb1df84":"code","7cad86a2":"code","21e7ad09":"code","ffbd4981":"code","10695f8a":"code","01f2b732":"code","d01e8938":"code","32a58339":"code","5ec7559d":"code","77ad7d3e":"code","3f3e958a":"code","55365b47":"code","4938ebe2":"code","35141cff":"code","61b32876":"code","0691be2d":"code","501fc6de":"code","369b6e6e":"code","c0225888":"code","8aaaf54b":"code","4169a9c3":"code","3d6acb8c":"code","a4570c4f":"code","fa11faa3":"code","332c0cfd":"code","7613ce05":"code","b8156f37":"code","5bb5a138":"code","635fc067":"code","5bcd1701":"code","e9a116a0":"code","90424b6e":"code","f20adaef":"code","a45eb778":"code","5762eea3":"markdown","2ec29c53":"markdown","4afea794":"markdown","4fb201f8":"markdown","ad83c695":"markdown"},"source":{"c8eb825a":"import io\nimport math\nfrom time import time\nfrom typing import List, Tuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\nfrom numpy.linalg import norm\n#import torch.nn.functional as F","ecff4e05":"use_cuda = torch.cuda.is_available()","9e840ff2":"use_cuda","5169f33b":"def load_vec(emb_path, nmax=50000):\n    vectors = []\n    word2id = {}\n    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n        next(f)\n        for i, line in enumerate(f):\n            word, vect = line.rstrip().split(' ', 1)\n            vect = np.fromstring(vect, sep=' ')\n            assert word not in word2id, 'word found twice'\n            vectors.append(vect)\n            word2id[word] = len(word2id)\n            if len(word2id) == nmax:\n                break\n    id2word = {v: k for k, v in word2id.items()}\n    embeddings = np.vstack(vectors)\n    return embeddings, id2word, word2id","b02757de":"def align_vectors(dic_path, src_embeddings, tgt_embeddings, src_word2id, tgt_word2id, verbose=0, encoding='utf-8', separator=' '):\n    \"\"\"\n    Returns src_embeddings, tgt_embeddings so that their correspondant lines represent the same word in \n    the two languages\n    \"\"\"\n    src = []\n    tgt = []\n    with io.open(dic_path, 'r', encoding=encoding, newline='\\n', errors='ignore') as f:\n        next(f)\n        for i, line in enumerate(f):\n            word_src, word_tgt = line.rstrip().split(separator, 1)\n            try :\n                word_src_id, word_tgt_id = src_word2id[word_src], tgt_word2id[word_tgt]\n                src.append(src_embeddings[word_src_id])\n                tgt.append(tgt_embeddings[word_tgt_id])\n            except:\n                if verbose == 1:\n                    print(\"Word from dictionnary:\", word_src, \"or\", word_tgt, \"is not found in original embeddings\")\n            \n    return np.array(src), np.array(tgt)","a480977a":"def synchronize_dict_and_embeddings(src_embeddings, src_id2word, src_word2id,\n                                    tgt_embeddings, tgt_id2word, tgt_word2id, dic_path):\n    \"\"\"returns src_embeddings, src_id2word, src_word2id and the same for target and a dictionnary but with only the words that are present in the dictionnary\"\"\"\n    dic = {}\n    src_ids_to_remove = []\n    tgt_ids_to_remove = []\n    new_src_word2id, new_src_id2word = {}, {}\n    new_tgt_word2id, new_tgt_id2word = {}, {}\n\n    with io.open(dic_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n        next(f)\n        for i, line in enumerate(f):\n            word_src, word_tgt = line.rstrip().split(' ', 1)\n            in_src = (word_src in src_word2id.keys())\n            in_tgt = (word_tgt in tgt_word2id.keys())\n\n            if in_src and in_tgt:\n                dic[word_src] = word_tgt\n            else:\n                if in_src == True :\n                    src_ids_to_remove.append(src_word2id[word_src])\n                elif in_tgt == True :\n                    tgt_ids_to_remove.append(tgt_word2id[word_tgt])\n  \n  #dic now contains the words we should be training and testing on\n\n    decalage = 0\n    for i in range(src_embeddings.shape[0]):\n        if (i in src_ids_to_remove) or (src_id2word[i] not in dic.keys()):\n            decalage += 1\n            continue\n        new_src_id2word[i-decalage] = src_id2word[i]\n        new_src_word2id[src_id2word[i]] = i-decalage\n\n    decalage = 0\n    for i in range(tgt_embeddings.shape[0]):\n        if (i in tgt_ids_to_remove) or (tgt_id2word[i] not in dic.values()):\n            decalage += 1\n            continue\n        new_tgt_id2word[i-decalage] = tgt_id2word[i]\n        new_tgt_word2id[src_id2word[i]] = i-decalage\n\n    src_vectors = []\n  \n    src_ids_rmv = set(src_ids_to_remove)\n  # print(src_ids_rmv)\n    for ind in range(src_embeddings.shape[0]):\n        if ind in src_ids_rmv :\n            continue\n        src_vectors.append(src_embeddings[ind])\n    new_src_embeddings = np.vstack(src_vectors)\n\n  # for ind in sorted(tgt_ids_to_remove, reverse=True) :\n  #     np.delete(tgt_embeddings,ind,0)\n    tgt_vectors = []\n  \n    tgt_ids_rmv = set(tgt_ids_to_remove)\n    for ind in range(tgt_embeddings.shape[0]):\n        if ind in tgt_ids_rmv :\n            continue\n        tgt_vectors.append(tgt_embeddings[ind])\n    new_tgt_embeddings = np.vstack(tgt_vectors)\n\n    return new_src_embeddings, new_src_id2word, new_src_word2id, new_tgt_embeddings, new_tgt_id2word, new_tgt_word2id, dic","d293dd62":"tgt_lang = 'fr'\nvectors_folder = '..\/input\/fast-text-translation-data\/fast_text_files\/vectors'\ndictionaries_folder = '..\/input\/fast-text-translation-data\/fast_text_files\/dictionaries'","eb4c7dd1":"src_path = f'{vectors_folder}\/wiki.en.vec'\ntgt_path = f'{vectors_folder}\/wiki.{tgt_lang}.vec'\nnmax = 150000  # maximum number of word embeddings to load\n\nsrc_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\ntgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)","1ce3da8a":"if use_cuda:\n    X_torch = torch.from_numpy(src_embeddings).float().to('cuda:0')\n    Z_torch = torch.from_numpy(tgt_embeddings).float().to('cuda:0')\n    detached_X = torch.clone(X_torch).detach().to('cuda:0')\n    detached_Z = torch.clone(Z_torch).detach().to('cuda:0')\nelse:\n    X_torch = torch.from_numpy(src_embeddings).float()\n    Z_torch = torch.from_numpy(tgt_embeddings).float()\n    detached_X = torch.clone(X_torch).detach()\n    detached_Z = torch.clone(Z_torch).detach()","17a2b04c":"train_size = 0.75\nX_train = X_torch[:int(X_torch.shape[0] * train_size)]\nX_test = X_torch[int(X_torch.shape[0] * train_size):]\nZ_train = Z_torch[:int(Z_torch.shape[0] * train_size)]\nZ_test = Z_torch[int(Z_torch.shape[0] * train_size):]","cbf8abb5":"X_train_norm = X_train.cpu() \/ norm(X_train.cpu(), axis = 1).reshape(X_train.cpu().shape[0],1)\nZ_train_norm = Z_train.cpu() \/ norm(Z_train.cpu(), axis = 1).reshape(Z_train.cpu().shape[0],1)\n\nX_test_norm = X_test.cpu() \/ norm(X_test.cpu(), axis = 1).reshape(X_test.cpu().shape[0],1)\nZ_test_norm = Z_test.cpu() \/ norm(Z_test.cpu(), axis = 1).reshape(Z_test.cpu().shape[0],1)","d7cf1526":"if use_cuda:\n    X_train_norm = X_train_norm.to('cuda:0')\n    Z_train_norm = Z_train_norm.to('cuda:0')\n    X_test_norm = X_test_norm.to('cuda:0')\n    Z_test_norm = Z_test_norm.to('cuda:0')","1bb1df84":"def generate_random(generated_data:torch.Tensor, Z:torch.Tensor,\n                    desired_len:int, ratio:float=0.5) -> Tuple[torch.Tensor, torch.Tensor]:\n    # ADD LABEL SMOOTHING HERE? (returning 1-eps, eps instead of 1. and 0.)\n    _Z = Z.clone()\n    dim0, dim1 = _Z.shape\n    proportion = ratio#torch.rand(1)\n    mask = (torch.rand(desired_len) > proportion)\n    trues = generated_data[mask]\n    falses = get_n_random_from_tensor(_Z, desired_len)[~mask]\n    mixed_elements = torch.cat([trues, falses])\n    mixed_labels = torch.cat([torch.ones(trues.shape[0]), torch.zeros(falses.shape[0])])\n    shuffle = torch.randperm(desired_len)\n    mixed_elements = mixed_elements[shuffle]\n    mixed_labels = mixed_labels[shuffle]\n    return mixed_elements, mixed_labels","7cad86a2":"def get_n_random_from_tensor(X:torch.Tensor, n:int) -> torch.Tensor:\n    indices = torch.randperm(len(X))[:n]\n    return X[indices]","21e7ad09":"INIT_FUNCTIONS = {\n    'xavier_uniform': torch.nn.init.xavier_uniform_,\n    'uniform': torch.nn.init.uniform_,#Gradient problems\n    'normal': torch.nn.init.normal_,#Gradient problems\n    'zeros': torch.nn.init.zeros_,\n    'kaiming': torch.nn.init.kaiming_uniform_,\n    'eye': torch.nn.init.eye_\n}","ffbd4981":"def extract(v):\n    return v.data.storage().tolist()","10695f8a":"def get_nn(word, src_emb, src_id2word, tgt_emb, tgt_id2word,\n           generator, K=5, is_cuda=False, debug=False):\n    print(\"Nearest neighbors of \\\"%s\\\":\" % word)\n    word2id = {v: k for k, v in src_id2word.items()}\n    word_emb = generator(src_emb[word2id[word]].reshape(1,-1).float()).reshape(-1)\n    print(f\"Mean of the generated vector for {word}: {word_emb.mean()}\")\n    if is_cuda:\n        word_emb = word_emb.cpu().detach().numpy()\n        tgt_np = tgt_emb.cpu().detach().numpy()\n    else:\n        word_emb = word_emb.detach().numpy()\n        tgt_np = tgt_emb.detach().numpy()\n    if debug:\n        #print(np.linalg.norm(tgt_np, 2, 1)[:, None])\n        print('\\n')\n        print(np.linalg.norm(word_emb))\n    scores = (tgt_np \/ np.linalg.norm(tgt_np, 2, 1)[:, None]).dot(word_emb \/ np.linalg.norm(word_emb))\n    k_best = scores.argsort()[-K:][::-1]\n    for i, idx in enumerate(k_best):\n        print('    %.4f - %s' % (scores[idx], tgt_id2word[idx]))","01f2b732":"class Generator(nn.Module):\n    def __init__(self, input_length: int, beta:float=0.01, init_function:str='xavier_uniform'):\n        super(Generator, self).__init__()\n        #self.dense_layer = nn.Linear(int(input_length), int(input_length))\n        #self.activation = nn.Sigmoid()\n        self.beta = beta\n        self.dense_layer = nn.Linear(int(input_length), int(input_length))\n        #self.dense_weights = self.dense_layer.weight.data\n        #self.dense_weight.requires_grad_()\n        #torch.nn.init.xavier_uniform(conv1.weight)\n        INIT_FUNCTIONS[init_function](self.dense_layer.weight)\n        #torch.nn.init.zeros_(tensor)\n\n    def forward(self, x):\n        #return torch.matmul(self.W, x)\n        return self.dense_layer(x)\n    \n    def predict(self, x):\n        return self.forward(x)\n    \n    def orthogonalize(self):\n        updated_weights = (1 + self.beta) * self.dense_layer.weight - self.beta * torch.matmul(torch.matmul(self.dense_layer.weight, self.dense_layer.weight.T), self.dense_layer.weight)\n        self.dense_layer.weight = nn.parameter.Parameter(updated_weights, requires_grad=False)","d01e8938":"class AlternativeGenerator(nn.Module):\n    def __init__(self, input_length):\n        super(AlternativeGenerator, self).__init__()\n        self.map1 = nn.Linear(input_length, 2048)\n        self.map2 = nn.Linear(2048, 2048)\n        self.map3 = nn.Linear(2048, input_length)\n        self.f = torch.tanh\n\n    def forward(self, x):\n        x = self.map1(x)\n        x = self.f(x)\n        x = self.map2(x)\n        x = self.f(x)\n        x = self.map3(x)\n        return x","32a58339":"class Discriminator(nn.Module):\n    def __init__(self, input_length: int, init_function:str='xavier_uniform'):\n        super(Discriminator, self).__init__()\n        self.dropout = nn.Dropout(p=0.0)\n        self.dense1 = nn.Linear(int(input_length), 2048)\n        self.activation1 = nn.LeakyReLU()\n        self.dense2 = nn.Linear(2048, 2048)\n        self.activation2 = nn.LeakyReLU()\n        self.dense3 = nn.Linear(2048, 1)\n        self.sigmoid = nn.Sigmoid()\n        initialization_function = INIT_FUNCTIONS[init_function]\n        initialization_function(self.dense1.weight)\n        initialization_function(self.dense2.weight)\n        initialization_function(self.dense3.weight)\n        \n\n    def forward(self, inputs):\n        #return self.activation(self.dense(x))\n        #return (self.activation2(self.dense2(self.activation(self.dense(self.dropou(x))))) > 0.5).float()\n        x = self.dropout(inputs)\n        x = self.dense1(x)\n        x = self.activation1(x) \n        x = self.dense2(x)\n        x = self.activation2(x)\n        x = self.dense3(x)\n        return self.sigmoid(x)# > 0.5).float()\n    \n    def predict(self, x):\n        return self.forward(x)\n    \n    def predict_binary(self, x):\n        return (self.forward(x) >= 0.5).float() ","5ec7559d":"class RepoDiscriminator(nn.Module):\n    def __init__(self):\n        super(RepoDiscriminator, self).__init__()\n        \n        self.emb_dim = 300\n        self.dis_layers = 2\n        self.dis_hid_dim = 2048\n        self.dis_dropout = 0.\n        self.dis_input_dropout = 0.1\n\n        layers = [torch.nn.Dropout(self.dis_input_dropout)]\n        for i in range(self.dis_layers + 1):\n            input_dim = self.emb_dim if i == 0 else self.dis_hid_dim\n            output_dim = 1 if i == self.dis_layers else self.dis_hid_dim\n            layers.append(torch.nn.Linear(input_dim, output_dim))\n            if i < self.dis_layers:\n                layers.append(torch.nn.LeakyReLU(0.2))\n                layers.append(torch.nn.Dropout(self.dis_dropout))\n        layers.append(torch.nn.Sigmoid())\n        self.layers = torch.nn.Sequential(*layers)\n\n    def forward(self, x):\n        assert x.dim() == 2 and x.size(1) == self.emb_dim\n        return self.layers(x).view(-1)","77ad7d3e":"class AlternativeDiscriminator(nn.Module):\n    def __init__(self, input_length):\n        super(AlternativeDiscriminator, self).__init__()\n        self.map1 = nn.Linear(input_length, 2048)\n        self.map2 = nn.Linear(2048, 2048)\n        self.map3 = nn.Linear(2048, 1)\n        self.f = torch.sigmoid\n\n    def forward(self, x):\n        x = self.f(self.map1(x))\n        x = self.f(self.map2(x))\n        return self.f(self.map3(x))","3f3e958a":"#X_copy = X_train_norm[torch.randperm(len(X_train_norm))].clone()\n#Z_copy = Z_train_norm[torch.randperm(len(Z_train_norm))].clone()\nX_copy = X_train_norm.clone()\nZ_copy = Z_train_norm.clone()","55365b47":"X_copy = X_torch.clone().float()\nZ_copy = Z_torch.clone().float()","4938ebe2":"dim = X_torch.shape[1]\ngenerator = Generator(input_length=dim, init_function='eye')\ndiscriminator = Discriminator(input_length=dim, init_function='xavier_uniform')\n#generator = AlternativeGenerator(input_length=dim)\n#discriminator = AlternativeDiscriminator(input_length=dim)\ndiscriminator = RepoDiscriminator()\ngenerator = nn.Linear(dim, dim, bias=False)\nif use_cuda:\n    generator.to('cuda:0')\n    discriminator.to('cuda:0')\ngenerator_optimizer = torch.optim.SGD(generator.parameters(), lr=0.1, weight_decay=0.95)\ndiscriminator_optimizer = torch.optim.SGD(discriminator.parameters(), lr=0.1, weight_decay=0.95)\nepochs = 500000\nepoch_size = 10000\nd_steps = 3# number of trains on each epoch for the Discriminator\ng_steps = 3# number of trains on each epoch for the Generator\nminibatch_size = 32\nsmoothing_coeff = 0.1\nmf = 37500\nloss = nn.BCELoss()\nbeta = 0.01","35141cff":"#New approach\nfor i in range(epochs):\n    for j in range(d_steps):\n        src_ids = torch.LongTensor(minibatch_size).random_(len(X_copy) if mf == 0 else mf)\n        tgt_ids = torch.LongTensor(minibatch_size).random_(len(Z_copy) if mf == 0 else mf)\n        src_elems = generator(X_copy[src_ids].float())\n        tgt_elems = Z_copy[tgt_ids]\n        x = torch.cat([src_elems, tgt_elems], 0)\n        y = torch.FloatTensor(2 * minibatch_size).zero_()\n        y[:minibatch_size] = 1 - smoothing_coeff\n        y[minibatch_size:] = smoothing_coeff\n        preds = discriminator(Variable(x.data))\n        d_loss = F.binary_cross_entropy(preds, y)\n        discriminator_optimizer.zero_grad()\n        d_loss.backward()\n        discriminator_optimizer.step()\n\n    discriminator.eval()\n    src_ids = torch.LongTensor(minibatch_size).random_(len(X_copy) if mf == 0 else mf)\n    tgt_ids = torch.LongTensor(minibatch_size).random_(len(Z_copy) if mf == 0 else mf)\n    src_elems = generator(X_copy[src_ids])\n    tgt_elems = Z_copy[tgt_ids]\n    x = torch.cat([src_elems, tgt_elems], 0)\n    y = torch.FloatTensor(2 * minibatch_size).zero_()\n    y[:minibatch_size] = 1 - smoothing_coeff\n    y[minibatch_size:] = smoothing_coeff\n    preds = discriminator(x)\n    g_loss = F.binary_cross_entropy(preds, 1 - y)\n    generator_optimizer.zero_grad()\n    g_loss.backward()\n    generator_optimizer.step()\n    W = generator.weight.data\n    generator.weight.data = (1 + beta) * W - beta * W.mm(W.transpose(0, 1).mm(W))\n    if i % 100 == 0:\n        print(f'\\nDiscriminator Loss: {d_loss:.5f}\\tGenerator Loss:  {g_loss:.5f}')\n        get_nn(word='against', src_emb=X_torch, src_id2word=src_id2word, tgt_emb=Z_copy,\n               tgt_id2word=tgt_id2word, generator=generator, K=5, is_cuda=use_cuda)\n    if i % 10 == 0:\n        print('.', end='')\n","61b32876":"#acs_on_real_data = list()\n#acs_on_gen_data = list()\n#X_copy = X_torch[torch.randperm(len(X_torch))].clone()\nfor i in range(epochs):\n    #print(f\"Epoch {i}\")\n    #print('.', end='')\n    for j in range(d_steps):\n        #noise = torch.rand((minibatch_size, dim))\n        #indices = torch.randperm(len(X_torch))[:minibatch_size]\n        noise = get_n_random_from_tensor(X_copy[:mf], minibatch_size)\n        #noise = X_copy[:minibatch_size]\n        #X_copy = X_copy[minibatch_size:] if len(X_copy) > 2 * minibatch_size else X_torch[torch.randperm(len(X_torch))].clone()\n        generated_data = generator(noise)\n        \n        #\u00a0Mix elements generated by the Generator with real elements from the target space\n        mixed_elements, mixed_labels = generate_random(\n            generated_data, Z_copy[:mf], desired_len=minibatch_size)\n        \n        #Label smoothing:\n        mixed_labels = torch.where(\n            mixed_labels==torch.tensor(1.),\n            torch.tensor(1-smoothing_coeff),\n            torch.tensor(smoothing_coeff)\n        )\n        if use_cuda:\n                mixed_labels = mixed_labels.to('cuda:0')\n        discriminator_preds_on_mix = discriminator(mixed_elements).reshape(-1)\n        discriminator_loss = loss(\n            input=discriminator_preds_on_mix,\n            target=mixed_labels\n        ).float()\n        discriminator_optimizer.zero_grad()\n        discriminator_loss.backward()\n        discriminator_optimizer.step()\n        #print(f\"Mean discriminator acc on mixed data: {discriminator_preds_on_generated.mean()}\")\n        #print(f\"Discriminator loss: {discriminator_loss.item():.4f}\")\n        #if j % 10 == 0:\n            #print(discriminator_loss)\n    \n    #noise = torch.rand((minibatch_size, dim))\n    noise = get_n_random_from_tensor(X_copy[:mf], minibatch_size)\n    generated_data = generator(noise)\n    discriminator_preds_on_generated = discriminator(generated_data)\n    #print(f'Mean discriminator acc on generated data: {discriminator_preds_on_generated.mean()}')   \n    generator_loss = loss(\n        input=discriminator_preds_on_generated,\n        target=torch.ones_like(discriminator_preds_on_generated) - smoothing_coeff\n    )\n    #print(f\"Discriminator loss: {discriminator_loss.item()}\")\n    #print(f\" Generator loss: {generator_loss.item()}\\n\")\n    generator_optimizer.zero_grad()\n    generator_loss.backward()\n    generator_optimizer.step()\n    #generator.orthogonalize()\n    #ac_on_gen_data = discriminator.predict(generated_data).mean()\n    #ac_on_real_data = discriminator.predict(get_n_random_from_tensor(X_torch, minibatch_size)).mean()\n    #acs_on_real_data.append(ac_on_real_data)\n    #acs_on_gen_data.append(ac_on_gen_data)\n    if i % 1000 == 0:\n        print(f'\\nDiscriminator Loss: { discriminator_loss:.5f}\\tGenerator Loss:  {generator_loss:.5f}')\n        get_nn(word='real', src_emb=X_torch, src_id2word=src_id2word, tgt_emb=Z_copy,\n               tgt_id2word=tgt_id2word, generator=generator, K=5, is_cuda=use_cuda)\n    if i % 100 == 0:\n        print('.', end='')\n    #print(f\"  Predictions on generated data: {ac_on_gen_data:.2f} (out of {len(generated_data)}) (best=0.0)\")\n    #print(f\"  Predictions on real data: {ac_on_real_data:.2f} (out of {minibatch_size}) (best=0.0)\")","0691be2d":"# MIX Generated objects go to 0 :(\nfor i in range(epochs):\n    for j in range(d_steps):\n        discriminator.zero_grad()\n        real_data = get_n_random_from_tensor(Z_copy, minibatch_size)\n        d_real_decision = discriminator(real_data).reshape(-1)\n        d_real_error = loss(\n            input=d_real_decision,\n            target=torch.zeros_like(d_real_decision) + smoothing_coeff\n        ).float()\n        if i % 100 == 0 and j == 0:\n            print('\\nGradient for d_real_error: ', end='')\n            d_real_error.register_hook(lambda grad: print(grad))\n        d_real_error.backward()\n        \n        x_samples = get_n_random_from_tensor(Z_copy, minibatch_size)\n        generated_data = generator(x_samples).detach()\n        d_generated_decission = discriminator(generated_data).reshape(-1)\n        d_fake_error = loss(\n            input=d_generated_decission,\n            target=torch.ones_like(d_generated_decission) - smoothing_coeff\n        ).float()\n        if i % 100 == 0 and j == 0:\n            print('Gradient for d_fake_error: ', end='')\n            d_fake_error.register_hook(lambda grad: print(grad))\n        d_fake_error.backward()\n        discriminator_optimizer.step()\n        discriminator_loss = 0.5 * (d_fake_error + d_real_error).detach()\n        \n    for j in range(g_steps):\n        generator.zero_grad()\n        x_samples = get_n_random_from_tensor(Z_copy, minibatch_size)\n        g_fake_data = generator(x_samples)\n        dg_fake_decission = discriminator(g_fake_data).reshape(-1)\n        generator_loss = loss(\n            input=dg_fake_decission,\n            target=torch.zeros_like(dg_fake_decission) + smoothing_coeff\n        )\n        if i % 100 == 0 and j == 0:\n            print('Gradient for generator_loss: ', end='')\n            generator_loss.register_hook(lambda grad: print(grad))\n        generator_loss.backward()\n        generator_optimizer.step()\n        generator.orthogonalize()\n    if i % 1000 == 0:\n        print(f'\\nDiscriminator Loss: {discriminator_loss:.5f}\\tGenerator Loss:  {generator_loss:.5f}')\n        get_nn(word='real', src_emb=X_torch, src_id2word=src_id2word, tgt_emb=Z_torch,\n               tgt_id2word=tgt_id2word, generator=generator, K=5, is_cuda=use_cuda)\n    if i % 100 == 0:\n        print('.', end='')\n    #print(f\"  Predictions on generated data: {ac_on_gen_data:.2f} (out of {len(generated_data)}) (best=0.0)\")\n    #print(f\"  Predictions on real data: {ac_on_real_data:.2f} (out of {minibatch_size}) (best=0.0)\")","501fc6de":"# FAKE = 0, TRUE = 1\ndebug=False\nfor epoch in range(epochs):\n    for d_index in range(d_steps):\n        # 1. Train D on real+fake\n        discriminator.zero_grad()\n\n        #  1A: Train D on real\n        d_real_data = get_n_random_from_tensor(Z_copy[:mf], minibatch_size)\n        d_real_decision = discriminator(d_real_data.float())\n        d_real_labels = torch.zeros([minibatch_size, 1]) + smoothing_coeff\n        if use_cuda:\n            d_real_labels = d_real_labels.to('cuda:0')\n        d_real_error = loss(\n            d_real_decision,\n            d_real_labels\n        ).float()  # ones = true\n        if debug:\n            with torch.autograd.detect_anomaly():\n                d_real_error.backward() # compute\/store gradients, but don't change params\n        else:\n            d_real_error.backward()\n        #  1B: Train D on fake\n        d_gen_input = get_n_random_from_tensor(X_copy[:mf], minibatch_size)\n        d_fake_data = generator(d_gen_input.float()).detach()\n        \n        #d_gen_input = Variable(generator(minibatch_size, g_input_size))\n        #d_fake_data = G(d_gen_input).detach()  # detach to avoid training G on these labels\n        d_fake_decision = discriminator(d_fake_data)\n        d_fake_labels = Variable(torch.ones([minibatch_size, 1]) - smoothing_coeff)\n        if use_cuda:\n            d_fake_labels = d_fake_labels.to('cuda:0')\n        d_fake_error = loss(\n            d_fake_decision,\n            d_fake_labels\n        ).float()  # zeros = fake\n        if debug:\n            with torch.autograd.detect_anomaly():\n                d_fake_error.backward()\n        else:\n            d_fake_error.backward()\n        discriminator_optimizer.step()# Only optimizes D's parameters; changes based on stored gradients from backward()\n\n        #dre, dfe = extract(d_real_error)[0], extract(d_fake_error)[0]\n\n    for g_index in range(g_steps):\n        # 2. Train G on D's response (but DO NOT train D on these labels)\n        generator.zero_grad()\n\n        gen_input = get_n_random_from_tensor(X_copy[:mf], minibatch_size)\n        g_fake_data = generator(gen_input.float())\n        dg_fake_decision = discriminator(g_fake_data)\n        dg_fake_labels = Variable(torch.zeros([minibatch_size, 1]) + smoothing_coeff)\n        if use_cuda:\n            dg_fake_labels = dg_fake_labels.to('cuda:0')\n        g_error = loss(\n            dg_fake_decision, \n            dg_fake_labels\n        ).float()  # Train G to pretend it's genuine\n\n        if debug:\n            with torch.autograd.detect_anomaly():\n                g_error.backward()\n        else:\n            g_error.backward()\n        generator_optimizer.step()  # Only optimizes G's parameters\n        generator.orthogonalize()\n        #ge = extract(g_error)[0]\n    if epoch % 20 == 0:\n        print('.', end='')\n    if epoch % 100 == 0:\n        print(f'\\nDiscriminator Loss: {d_fake_error:.5f}\\tGenerator Loss:  {g_error:.5f}')\n        get_nn(word='real', src_emb=X_torch, src_id2word=src_id2word, tgt_emb=Z_torch,\n               tgt_id2word=tgt_id2word, generator=generator, K=5, is_cuda=use_cuda, debug=debug)","369b6e6e":"plt.figure(figsize=(10,10))\nplt.plot([100 * a for a in acs_on_gen_data])\nplt.plot([100 * a for a in acs_on_real_data])\nplt.title('Accuracy of the Discriminator')\nplt.ylabel('% Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['% of correctly detected as generated', '% of true data wrongly labeled as generated'])","c0225888":"from keras.models import Model, Sequential\nfrom keras.layers import Dense, Dropout, Input\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.optimizers import SGD, schedules","8aaaf54b":"def get_nn_old(word, src_emb, src_id2word, tgt_emb, tgt_id2word, generator, K=5):\n    \n    word2id = {v: k for k, v in src_id2word.items()}\n    word_emb = generator.predict(src_emb[word2id[word]].reshape(1,-1)).reshape(-1)\n    print(f\"Nearest neighbors of {word} ({word_emb.mean()}):\")\n    scores = (tgt_emb \/ np.linalg.norm(tgt_emb, 2, 1)[:, None]).dot(word_emb \/ np.linalg.norm(word_emb))\n    k_best = scores.argsort()[-K:][::-1]\n    for i, idx in enumerate(k_best):\n        print('    %.4f - %s' % (scores[idx], tgt_id2word[idx]))","4169a9c3":"train_size = 0.75\nX_train, X_test = src_embeddings[:int(src_embeddings.shape[0]*train_size)], src_embeddings[int(src_embeddings.shape[0]*train_size):]\nZ_train, Z_test = tgt_embeddings[:int(tgt_embeddings.shape[0]*train_size)], tgt_embeddings[int(tgt_embeddings.shape[0]*train_size):]\nX_train_norm = X_train \/ norm(X_train, axis = 1).reshape(X_train.shape[0],1)\nZ_train_norm = Z_train \/ norm(Z_train, axis = 1).reshape(Z_train.shape[0],1)\n\nX_test_norm = X_test \/ norm(X_test, axis = 1).reshape(X_test.shape[0],1)\nZ_test_norm = Z_test \/ norm(Z_test, axis = 1).reshape(Z_test.shape[0],1)","3d6acb8c":"X_torch = torch.from_numpy(X_train_norm)\nZ_torch = torch.from_numpy(Z_train_norm)\nif use_cuda:\n    X_torch = X_torch.to('cuda:0')\n    Z_torch = Z_torch.to('cuda:0')","a4570c4f":"class NewDiscriminator(nn.Module):\n    def __init__(self, input_length: int):\n        super(NewDiscriminator, self).__init__()\n        self.dropout = nn.Dropout(p=0.1)\n        self.dense1 = nn.Linear(int(input_length), 2048)\n        self.activation1 = nn.LeakyReLU(0.2)\n        self.dense2 = nn.Linear(2048, 2048)\n        self.activation2 = nn.LeakyReLU(0.2)\n        self.dense3 = nn.Linear(2048, 1)\n        self.sigmoid = nn.Sigmoid()\n        \n\n    def forward(self, inputs):\n        #return self.activation(self.dense(x))\n        #return (self.activation2(self.dense2(self.activation(self.dense(self.dropou(x))))) > 0.5).float()\n        x = self.dropout(inputs)\n        x = self.dense1(x)\n        x = self.activation1(x) \n        x = self.dense2(x)\n        x = self.activation2(x)\n        x = self.dense3(x)\n        return self.sigmoid(x)# > 0.5).float()\n    \n    def predict(self, x):\n        return self.forward(x)\n    \n    def predict_binary(self, x):\n        return (self.forward(x) >= 0.5).float() ","fa11faa3":"def create_discriminator():\n    discriminator = Sequential()\n    \n    #Add dropout to input\n    discriminator.add(Dropout(0.1, input_shape=(word_dimension,)))\n\n    #First hidden layer\n    discriminator.add(Dense(2048))\n    discriminator.add(LeakyReLU(0.2))\n\n    #Second hidden layer\n    discriminator.add(Dense(2048))\n    discriminator.add(LeakyReLU(0.2))\n\n    #Output layer\n    discriminator.add(Dense(1, activation = 'sigmoid'))\n   \n    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n\n    return discriminator","332c0cfd":"class NewGenerator(nn.Module):\n    def __init__(self, input_length: int, beta:float=0.01, init_function:str='xavier_uniform'):\n        super(NewGenerator, self).__init__()\n        self.beta = beta\n        self.dense_layer = nn.Linear(int(input_length), int(input_length), bias=False)\n        INIT_FUNCTIONS[init_function](self.dense_layer.weight)\n\n    def forward(self, x):\n        return self.dense_layer(x)\n\n    def orthogonalize(self):\n        updated_weights = (1 + self.beta) * self.dense_layer.weight - self.beta * torch.matmul(torch.matmul(self.dense_layer.weight, self.dense_layer.weight.T), self.dense_layer.weight)\n        self.dense_layer.weight = nn.parameter.Parameter(updated_weights, requires_grad=False)","7613ce05":"def create_generator():\n    generator = Sequential()\n    \n    generator.add(Dense(300, use_bias = False, input_dim=word_dimension))\n    \n    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n\n    generator.set_weights([np.eye(300)])\n\n    return generator","b8156f37":"class GAN(nn.Module):\n    def __init__(self, generator, discriminator):\n        super(GAN, self).__init__()\n        self.generator = generator\n        self.discriminator = discriminator\n\n    def forward(self, x):\n        return self.discriminator(self.generator(x))","5bb5a138":"def get_nn_temporal(word, src_emb, src_id2word, tgt_emb, tgt_id2word,\n           generator, K=5, is_cuda=False, debug=False):\n    print(\"Nearest neighbors of \\\"%s\\\":\" % word)\n    word2id = {v: k for k, v in src_id2word.items()}\n    word_emb = generator(src_emb[word2id[word]].reshape(1,-1).float()).reshape(-1)\n    #print(word_emb.device)\n    print(f\"Mean of the generated vector for {word}: {word_emb.mean()}\")\n    if is_cuda:\n        word_emb = word_emb.cpu().detach().numpy()\n        tgt_np = tgt_emb.cpu().detach().numpy()\n    else:\n        word_emb = word_emb.detach().numpy()\n        tgt_np = tgt_emb.detach().numpy()\n    if debug:\n        #print(np.linalg.norm(tgt_np, 2, 1)[:, None])\n        print('\\n')\n        print(np.linalg.norm(word_emb))\n    scores = (tgt_np \/ np.linalg.norm(tgt_np, 2, 1)[:, None]).dot(word_emb \/ np.linalg.norm(word_emb))\n    k_best = scores.argsort()[-K:][::-1]\n    for i, idx in enumerate(k_best):\n        print('    %.4f - %s' % (scores[idx], tgt_id2word[idx]))","635fc067":"dim = X_torch.shape[1]\ngenerator = NewGenerator(input_length=dim, init_function='eye')\ndiscriminator = NewDiscriminator(input_length=dim)\ngan = GAN(generator, discriminator)\nif use_cuda:\n    generator.to('cuda:0')\n    discriminator.to('cuda:0')\n    \ngenerator_optimizer = torch.optim.SGD(generator.parameters(), lr=0.1, weight_decay=0.98)\ndiscriminator_optimizer = torch.optim.SGD(discriminator.parameters(), lr=0.1, weight_decay=0.98)\ngan_optimizer = torch.optim.SGD(gan.parameters(), lr=0.1, weight_decay=0.98)\n\ngenerator_scheduler = torch.optim.lr_scheduler.ExponentialLR(\n    generator_optimizer,\n    gamma=0.999998, last_epoch=-1\n)\ndiscriminator_scheduler = torch.optim.lr_scheduler.ExponentialLR(\n    discriminator_optimizer,\n    gamma=0.999998, last_epoch=-1\n)\ngan_scheduler = torch.optim.lr_scheduler.ExponentialLR(\n    gan_optimizer,\n    gamma=0.999998, last_epoch=-1\n)\nloss = nn.BCELoss()","5bcd1701":"epochs = 5\nbatch_size = 32\nepoch_size = 10000\ndis_steps = 3\nbeta = 0.001\nsm = 0.1\nmf = 37500\nsrc_embeddings_torch = torch.from_numpy(src_embeddings)\ntgt_embeddings_torch = torch.from_numpy(tgt_embeddings)\nif use_cuda:\n    src_embeddings_torch = src_embeddings_torch.to('cuda:0')\n    tgt_embeddings_torch = tgt_embeddings_torch.to('cuda:0')","e9a116a0":"for epoch in range(epochs):\n    print(f'Start epoch: {epoch}')\n    for i in range(epoch_size):\n        # prepare batch for generator\n\n        word_batch = X_torch[np.random.randint(0, mf, size=batch_size)].float()\n        word_batch.reshape(batch_size, dim)\n\n        rotated_batch = generator(word_batch)\n\n        # dis_steps training the  discriminator\n        for j in range(dis_steps) :\n            discriminator_optimizer.zero_grad()\n            gan_optimizer.zero_grad()\n            target_batch = Z_torch[np.random.randint(0, mf, size=batch_size)].float()\n            x = torch.cat((rotated_batch,target_batch))\n            disc_y = torch.zeros(2 * batch_size)\n            disc_y[:batch_size] = 1-sm\n            disc_y[batch_size:] = sm\n            discriminator_preds_on_mix = discriminator(x).reshape(-1)\n            if use_cuda:\n                disc_y = disc_y.to('cuda:0')\n            discriminator_loss = loss(\n                input=discriminator_preds_on_mix,\n                target=disc_y\n            ).float()\n            discriminator_loss.backward(retain_graph=True)\n            discriminator_optimizer.step()\n            discriminator_scheduler.step()\n\n            #d_loss = h_discriminator.train_on_batch(x, disc_y)\n        \n        # training the mapping\n        gan_optimizer.zero_grad()\n        y_gen = torch.ones(batch_size) * sm\n        gan_preds = gan(word_batch).reshape(-1)\n        if use_cuda:\n                y_gen = y_gen.to('cuda:0')\n        gan_loss = loss(\n                input=gan_preds,\n                target=y_gen\n            ).float()\n        gan_loss.backward()\n        gan_optimizer.step()\n        gan_scheduler.step()\n        \n        #Orthogonalize\n        generator.orthogonalize()\n\n        if i%1000 == 0 :\n            print(f'**Batch {i} \\t Discriminator Loss: {discriminator_loss:.6f} \\t\\t Generator Loss: {gan_loss:.6f}')\n            with torch.no_grad():\n                get_nn_temporal(\"against\", src_embeddings_torch, src_id2word,\n                   tgt_embeddings_torch, tgt_id2word, generator=generator, K=5, is_cuda=True)\n\n    print(f'End epoch: {epoch} \\t Discriminator Loss: {discriminator_loss:.6f} \\t\\t Generator Loss: {gan_loss:.6f}')\n    # get_nn(\"against\", src_embeddings,src_id2word,tgt_embeddings,tgt_id2word,K=30)","90424b6e":"word_dimension = 300\n\nlr_schedule = schedules.ExponentialDecay(\ninitial_learning_rate=0.1,\ndecay_steps=10000,\ndecay_rate=0.98)\n\noptimizer = SGD(learning_rate=lr_schedule)\n\n\nh_discriminator = create_discriminator()\nh_generator = create_generator()\n\nh_discriminator.trainable = False\n\ngan_input = Input(shape=(word_dimension,))\nrotated_word = h_generator(gan_input)\n\ngan_output = h_discriminator(rotated_word)\n\nh_gan = Model(gan_input, gan_output)\nh_gan.compile(loss='binary_crossentropy', optimizer=optimizer)","f20adaef":"epochs = 5\nbatch_size = 32\nepoch_size = 10000\ndis_steps = 3\nbeta = 0.001\nsm = 0.1\nmf = 37500\n\nfor epoch in range(epochs):\n    print(f'Start epoch: {epoch}')\n    for i in range(epoch_size):\n      # prepare batch for generator\n\n      word_batch = X_train_norm[np.random.randint(0, mf, size=batch_size)]\n      word_batch.reshape(batch_size,word_dimension)\n\n      rotated_batch = h_generator.predict(word_batch)\n\n      # dis_steps training the  discriminator\n      for j in range(dis_steps) :\n        target_batch = Z_train_norm[np.random.randint(0, mf, size=batch_size)]\n        x = np.concatenate((rotated_batch,target_batch))\n        disc_y = np.zeros(2 * batch_size)\n        disc_y[:batch_size] = 1-sm\n        disc_y[batch_size:] = sm\n        d_loss = h_discriminator.train_on_batch(x, disc_y)\n\n      # training the mapping\n      y_gen = np.ones(batch_size)*sm\n      g_loss = h_gan.train_on_batch(word_batch, y_gen)\n\n      #Orthogonalize\n      W = h_generator.get_weights()[0]\n      W_orth =  (1 + beta)*W - beta*(W.dot(W.T)).dot(W)\n      h_generator.set_weights([W_orth])\n\n      if i%1000 == 0 :\n        print(f'**Batch {i} \\t Discriminator Loss: {d_loss:.6f} \\t\\t Generator Loss: {g_loss:.6f}')\n        get_nn_old(\"against\", src_embeddings, src_id2word,\n               tgt_embeddings, tgt_id2word, generator=h_generator, K=5)\n\n    print(f'End epoch: {epoch} \\t Discriminator Loss: {d_loss:.6f} \\t\\t Generator Loss: {g_loss:.6f}')\n    # get_nn(\"against\", src_embeddings,src_id2word,tgt_embeddings,tgt_id2word,K=30)","a45eb778":"##### NUEVA PRUEBA\n#X_copy = X_train_norm[torch.randperm(len(X_train_norm))].clone()\n#for i in range(epochs):\n#    for k in range(epoch_size):\n#        for j in range(training_steps):\n#            noise = get_n_random_from_tensor(X_copy, minibatch_size)\n#            generated_data = generator(noise)\n#            #\u00a0Mix elements generated by the Generator with real elements from the target space\n#            mixed_elements, mixed_labels = generate_random(\n#                generated_data, Z_train_norm, desired_len=minibatch_size\n#            )\n#            #Label smoothing:\n#            mixed_labels = torch.where(\n#                mixed_labels==torch.tensor(1.),\n#                torch.tensor(1-smoothing_coeff),\n#                torch.tensor(smoothing_coeff)\n#            )\n#            if use_cuda:\n#                mixed_labels = mixed_labels.to('cuda:0')\n#            discriminator_preds_on_mix = discriminator(mixed_elements).reshape(-1)\n#            discriminator_loss = loss(\n#                input=discriminator_preds_on_mix,\n#                target=mixed_labels\n#            ).float()\n#            discriminator_optimizer.zero_grad()\n#            discriminator_loss.backward()\n#            discriminator_optimizer.step()\n#\n#\n#        discriminator_preds_on_generated = discriminator.predict(generated_data)\n#\n#        generator_loss = loss(\n#            input=discriminator_preds_on_generated,\n#            target=torch.ones_like(discriminator_preds_on_generated) - torch.tensor(smoothing_coeff)\n#        )\n#\n#        generator_optimizer.zero_grad()\n#        generator_loss.backward()\n#        generator_optimizer.step()\n#        generator.orthogonalize()\n#\n#        if k % 50 == 0:\n#            print('.', end='')\n#        if k % 1000 == 0:\n#            print(f'\\nDiscriminator Loss: { discriminator_loss:.5f}\\tGenerator Loss:  {generator_loss:.5f}')\n#            get_nn(word='against', src_emb=X_torch, src_id2word=src_id2word,\n#                   tgt_emb=Z_torch, tgt_id2word=tgt_id2word, generator=generator, K=6,\n#                   is_cuda=use_cuda\n#                  )\n#            ","5762eea3":"### Old code I don't want to delete just in case","2ec29c53":"### Functions for generating the data","4afea794":"### Hassan version","4fb201f8":"Bad news, we expect this two graphs to converge during the training process (meaning that the Generator is learning to fool the Discriminator by applying the correct transformation to the source embeddings).","ad83c695":"# WORK IN PROGRESS"}}