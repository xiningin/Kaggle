{"cell_type":{"1aae1e21":"code","11a0652c":"code","b868682e":"code","4cb728af":"code","806a58ff":"code","ae489093":"code","d88b22c6":"code","14f2bb32":"code","a3114f9b":"code","1aa383de":"code","516e1773":"code","588087e3":"code","3f57b292":"code","61180037":"code","18b898f4":"code","4d08dc2d":"code","237eef52":"code","77838aad":"code","006f0aa8":"code","c11eaf0f":"code","e3d2cccc":"code","12825c2e":"code","d7eb13e0":"code","5607279d":"code","e7db8ade":"code","6d4232b7":"markdown"},"source":{"1aae1e21":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","11a0652c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, confusion_matrix, classification_report\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import SMOTE \n","b868682e":"df = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndf.head()","4cb728af":"df.info()","806a58ff":"df.describe()","ae489093":"df.isnull().sum()","d88b22c6":"sns.pairplot(df,hue = 'Outcome')","14f2bb32":"df_c = df.iloc[:,:]\ndf_c.iloc[:,1:-1]= df_c.iloc[:,1:-1].replace(0,np.NaN)\ndf_c.isnull().sum()","a3114f9b":"df_c.hist(figsize = (20,20))","1aa383de":"df_c.Glucose.fillna(df_c.Glucose.mean(),inplace = True)\ndf_c.BloodPressure.fillna(df_c.BloodPressure.mean(),inplace = True)","516e1773":"df_c.SkinThickness.fillna(df_c.SkinThickness.median(),inplace = True)\ndf_c.Insulin.fillna(df_c.Insulin.median(),inplace = True)\ndf_c.BMI.fillna(df_c.BMI.median(),inplace = True)","588087e3":"df_c.hist(figsize = (20,20))","3f57b292":"fig, ax = plt.subplots(figsize=(15,8))\n\ndf_copy = df_c.iloc[:,:-1]\ntarget = df_c.iloc[:,-1].to_frame()\n\nsns.boxplot(data = df_copy)\ndf_copy","61180037":"X_train, X_test, y_train, y_test = train_test_split(df_copy, target, \n                                                    test_size = 0.2,\n                                                    shuffle = True, \n                                                    random_state = 0)\nX_train = np.asarray(X_train)\nX_test = np.asarray(X_test)\ny_train = np.asarray(y_train)\ny_test = np.asarray(y_test)\n\nX_train.shape","18b898f4":"sm = SMOTE()\nX_res, y_res = sm.fit_resample(X_train,y_train)","4d08dc2d":"# scaler = Normalizer().fit(X_train) #the scaler is fitted to the training set\n# normalized_X_train = scaler.transform(X_train) #the scaler is applied to the training set\n# normalized_X_test = scaler.transform(X_test) #the scaler is applied to the test set\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_res)                      \nX_test = scaler.transform(X_test)\nprint(\"X train before Normalization\")\nprint(X_train[0:5])\nprint(\"\\nX train after Normalization\")\nprint(X_train[0:5])","237eef52":"df2 = pd.DataFrame(data = np.c_[X_train , y_res] , columns = df.columns )\ndf2","77838aad":"sns.pairplot(df2,hue = 'Outcome')","006f0aa8":"knn = KNeighborsClassifier(20)\nknn.fit(X_train,y_res)\ny_exp = knn.predict(X_test)\naccuracy_score(y_test, y_exp)","c11eaf0f":"param_grid = {\n                'n_neighbors': np.arange(1, 50, 1)\n             }\n\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn,param_grid, cv=5)\nknn_cv.fit(X_train, y_res)\n\nprint(\"Best Score:\" + str(knn_cv.best_score_))\nprint(\"Best Parameters: \" + str(knn_cv.best_params_))","e3d2cccc":"y_pred = knn_cv.best_estimator_.predict(X_test)\nprint(classification_report(y_test, y_pred))\n","12825c2e":"lr = LogisticRegression(random_state=0)\n\nparam_grid = \\\n{\n    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n    'max_iter': list(range(100,800,100)),\n    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n}\n\nlr_cv = GridSearchCV(lr, param_grid=param_grid, cv=5, scoring='roc_auc')\n\nlr_cv.fit(X_train, y_res)\n\nprint('Config: %s' % lr_cv.best_params_)\nprint('Best Score: %s' % lr_cv.best_score_)","d7eb13e0":"y_pred = lr_cv.best_estimator_.predict(X_test)\nprint(classification_report(y_test, y_pred))\n","5607279d":"svc = SVC(kernel='rbf')\nsvc.fit(X_train, y_res)\ny_pred = svc.predict(X_test)","e7db8ade":"y_pred = svc.predict(X_test)\nprint(classification_report(y_test, y_pred))\n","6d4232b7":"### so no nulls in this data"}}