{"cell_type":{"9e31a49f":"code","62ccfeea":"code","60caf1c9":"code","aa780c40":"code","6bbaa3bf":"code","c944ab0a":"code","989c70b5":"code","beb24295":"code","28ecf755":"code","f5b8d818":"code","f1606cf8":"code","97af2e2c":"code","0c541e6c":"code","77c88914":"code","0e6da4cc":"code","95668930":"code","e3d62e08":"code","fd1da206":"code","466f1c75":"code","c9aac009":"code","bf1c5f66":"code","797d00a9":"code","10fee88b":"code","b7de0ebb":"code","237a02c8":"code","45f79e49":"code","948362c6":"code","701d858a":"code","1baef7f0":"code","0e72e437":"code","b0329193":"code","cf763e69":"code","ada88434":"code","31c67e29":"code","7aada1b9":"code","c27cf8bf":"code","6b6a68de":"code","2fca3d48":"code","e6301fb6":"code","12e17e94":"code","ac5ef946":"code","d0ddb6df":"code","a15d5c10":"code","9fef7a2b":"code","099cd8a3":"code","c922ef72":"code","83c81477":"code","f07fa537":"markdown","2d4e627b":"markdown","0835eaec":"markdown","087fdb70":"markdown","c016bf2e":"markdown","0ac97732":"markdown","473d799c":"markdown","4c731bac":"markdown","a2b2e450":"markdown","71f6f7f8":"markdown","ed2b9772":"markdown","4b9a8f08":"markdown","035b80a6":"markdown","fb898cca":"markdown","d8c4c5d1":"markdown","67061d81":"markdown","e9fa2f99":"markdown","bfe05032":"markdown","c07414a3":"markdown","721cb8a1":"markdown","e0e609ad":"markdown","3719add7":"markdown","3bd28449":"markdown","c088dc02":"markdown","d6627aff":"markdown","dde5d087":"markdown"},"source":{"9e31a49f":"\nimport numpy as np # linear algebra\nimport pandas as pd# data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nsns.set_style('whitegrid')\nimport os\n%matplotlib inline \n\n#Importing Deep Learning Modules \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","62ccfeea":"#import the csv file as a pandas dataframe \ndata = pd.read_csv('\/kaggle\/input\/bmw-pricing-challenge\/bmw_pricing_challenge.csv')","60caf1c9":"data.head()","aa780c40":"data.shape\n\n# We have 4843 rows and 18 columns ","6bbaa3bf":"# We have no missing values in any of the columns \ndata.isnull().sum()","c944ab0a":"data.describe()\n\n# We see that the range of price variable is pretty large but most of the cars sold lie between 10,000 to 19000 price range \n#The minimum value  of mileage does not makes sense .Inter quartile for mileage is 100,000 to 175,000\n# In engine we have sold cars having no engine to have 423HP cars ","989c70b5":"\n#Checking the Data types for each column \ndata.dtypes","beb24295":"# All data types correct apart from sold_at and registration_data they need to be datatime \n\ndata['registration_date']=pd.to_datetime(data['registration_date'])\ndata['sold_at']=pd.to_datetime(data['sold_at'])\n","28ecf755":"data[data['mileage']<0]","f5b8d818":"#correcting the mileage value \n\ndata.set_value(2938, 'mileage', 64)\n","f1606cf8":"data[data['mileage']==64]","97af2e2c":"plt.figure(figsize=(8,8))\nbase_color=sns.color_palette()[8]\nsns.countplot(data=data, x='fuel',color=base_color)\nplt.title('Type of Fuel',fontsize=30)\nplt.ylabel('Count',fontsize=30)\n#plt.xlabel(fontsize=40)\n\nlocs, labels = plt.xticks()\n\n\n# loop through each pair of locations and labels\nfor loc, label in zip(locs, labels):\n\n    # get the text property for the label to get the correct count\n    count = data['fuel'].value_counts()[label.get_text()]\n    pct_string = '{:}'.format(count)\n\n    # print the annotation just below the top of the bar\n    plt.text(loc, count+10, pct_string, ha = 'center', color = 'black',fontsize=20)","0c541e6c":"plt.figure(figsize=(10,10))\nsns.boxplot(data=data,x='fuel',y='price',color=base_color,order=data.fuel.value_counts().index);\nplt.title('Fuel',fontsize=30)\nplt.ylabel('Price',fontsize=30)\nplt.xticks(fontsize=20,rotation=90)","77c88914":"plt.figure(figsize=(8,8))\nbase_color=sns.color_palette()[8]\nsns.countplot(data=data, x='paint_color',color=base_color,order=data.paint_color.value_counts().index)\nplt.title('Paint Color',fontsize=30)\nplt.ylabel('Count',fontsize=30)\nplt.xticks(rotation=90,fontsize=20)\n\nlocs, labels = plt.xticks()\n\n\n# loop through each pair of locations and labels\nfor loc, label in zip(locs, labels):\n\n    # get the text property for the label to get the correct count\n    count = data['paint_color'].value_counts()[label.get_text()]\n    pct_string = '{:}'.format(count)\n\n    # print the annotation just below the top of the bar\n    plt.text(loc, count+10, pct_string, ha = 'center', color = 'black',fontsize=20)","0e6da4cc":"#Let see how the color is related to price in the resale market . THe data is has more points for black , grey and blue\n\nplt.figure(figsize=(10,10));\nsns.violinplot(data=data,x='paint_color',y='price',inner='quartile',color=base_color);\nplt.ylabel('Price',fontsize=30);\n#plt.xlabel('Paint Color',fontsize=30);\nplt.title(\"Paint Color\",fontsize=30);\nplt.xticks(rotation=90,fontsize=20);","95668930":"\nplt.figure(figsize=(8,8))\nbase_color=sns.color_palette()[8]\nsns.countplot(data=data, x='car_type',color=base_color,order=data.car_type.value_counts().index)\nplt.title('Car Type',fontsize=30)\nplt.ylabel('Count',fontsize=30)\nplt.xticks(rotation=90,fontsize=20)\n\nlocs, labels = plt.xticks()\n\n\n# loop through each pair of locations and labels\nfor loc, label in zip(locs, labels):\n\n    # get the text property for the label to get the correct count\n    count = data['car_type'].value_counts()[label.get_text()]\n    pct_string = '{:}'.format(count)\n\n    # print the annotation just below the top of the bar\n    plt.text(loc, count+10, pct_string, ha = 'center', color = 'black',fontsize=20)","e3d62e08":"plt.figure(figsize=(10,10))\nsns.boxplot(data=data,x='car_type',y='price',color=base_color,order=data.car_type.value_counts().index);\nplt.title('Car Type',fontsize=30)\nplt.ylabel('Price',fontsize=30)\nplt.xticks(fontsize=20,rotation=90)","fd1da206":"plt.figure(figsize=(10,10))\nsns.scatterplot(data=data,x='car_type',y='fuel',alpha=0.2,x_jitter=0.2);\n","466f1c75":"\n#Creating a colum called as registration year \ndata['registration_year']=data['registration_date'].dt.year","c9aac009":"plt.figure(figsize=(10,10));\nsns.countplot(data=data,x='registration_year',color=base_color);\nplt.xticks(rotation=90);\nplt.xlabel('Registration Year',fontsize=20);\nplt.ylabel('Count',fontsize=20);\n","bf1c5f66":"data['sold_at'].dt.year.value_counts()","797d00a9":"plt.figure(figsize=(10,10))\nsns.scatterplot(data=data.sample(2000),x='registration_year',y='price',hue='car_type');","10fee88b":"data.dtypes","b7de0ebb":"\nplt.figure(figsize=(10,15));\n#sns.regplot(x=\"car_type\",y=\"registration_date\",data=data)\nsns.scatterplot(data=data,x='car_type',y='registration_year',alpha=0.3);\nplt.xticks(rotation=90,fontsize=20);\nplt.xlabel('Car Type',fontsize=20);\nplt.xlabel('Registration Year',fontsize=20);\nplt.title('Cart_type vs Registration Year',fontsize=20);","237a02c8":" \ndata['vechile_days']=data['sold_at']-data['registration_date']\ndata['year_diff']=data['sold_at'].dt.year-data['registration_year']","45f79e49":"plt.figure(figsize=(20,10))\ncolor=sns.color_palette()[0]\nsns.countplot(data=data,x='model_key',color=color,order=data.model_key.value_counts().index);\nplt.xticks(rotation=90);\nplt.title('Count BMW models');\n\nlocs, labels = plt.xticks() # get the current tick locations and labels\n\n# add annotations\n\n\n# loop through each pair of locations and labels\nfor loc, label in zip(locs, labels):\n\n    # get the text property for the label to get the correct count\n    count = data.model_key.value_counts()[label.get_text()]\n    pct_string = '{:}'.format(count)\n\n    # print the annotation just below the top of the bar\n    plt.text(loc, count+10, pct_string, ha = 'center', color = 'black')","948362c6":"plt.figure(figsize=(10,10))\ncolor=sns.color_palette()[0]\nsns.countplot(data=data,x='car_type',color=color,order=data.car_type.value_counts().index);\nplt.xticks(rotation=90,fontsize=20);\nplt.title('Type BMW models',fontsize=20);\nplt.ylabel('Count',fontsize=20);\nplt.xlabel(\"CarType\")","701d858a":"plt.hist(data=data,x='engine_power',bins=5);\nplt.xlabel('Engine Capacity',fontsize=20);\nplt.title('Histogram of Engine Capacity');\nplt.ylabel('Number of Engines')","1baef7f0":"plt.figure(figsize=(10,10))\nbin_edges = 10 ** np.arange(np.log10(data.price.min()), np.log10(data.price.max())+0.1, 0.1);\nplt.hist(data=data,x='price',bins=bin_edges);\nplt.xscale('log');\ntick_locs = [10, 30, 100, 300, 1000, 3000,10000,130000];\nticks=np.arange(100,1300,100);\nplt.xticks(tick_locs, tick_locs);\nplt.yticks(ticks,ticks)\nplt.xlabel('Price cars sold at');\nplt.title('Price Distribution');\nplt.ylabel('Count');","0e72e437":"data.head()","b0329193":"# Here I am only plotting a subsample of the data ( 1500 points ) to prevent over crowding . But we can see a some what negative trend in price as mileage increase \n\nplt.figure(figsize=(10,10));\nsns.scatterplot(data=data.sample(1500),x='mileage',y='price',alpha=0.8,hue='car_type');\nplt.xlim(0,500000)\nplt.ylim(0,100000)\nplt.xlabel('Mileage',fontsize=20);\nplt.ylabel('Price',fontsize=20);\nplt.title('Mileage vs Price vs Car_type',fontsize=20);","cf763e69":"plt.figure(figsize=(10,5))\nplt.scatter(data = data.sample(1500), x = 'mileage', y = 'price', c = 'registration_year',cmap='viridis',alpha=0.5)\nplt.colorbar();\nplt.xlim(0,500000)\nplt.ylim(0,100000)\nplt.xlabel('Mileage',fontsize=20);\nplt.ylabel('Price',fontsize=20);\nplt.title('Mileage vs Price vs Year',fontsize=20);","ada88434":"plt.figure(figsize=(10,10))\nvar=['mileage','engine_power','registration_year','price']\nsns.heatmap(data[var].corr(),annot=True,cmap='viridis');\nplt.xticks(fontsize=20,rotation=90);\nplt.yticks(fontsize=20,rotation=45);\n","31c67e29":"# Fitting a neural network to only 8 boolean features \n\n# Importing the Keras libraries and packages\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import optimizers\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score\nfrom sklearn.metrics import r2_score\n\n\nbaseline=data[['feature_1','feature_2','feature_3','feature_4','feature_5','feature_6','feature_7','feature_8','price']]\n\n\n#Casting True\/Fasle to 1\/0 \nbaseline['feature_1']=baseline['feature_1'].astype('int')\nbaseline['feature_2']=baseline['feature_2'].astype('int')\nbaseline['feature_3']=baseline['feature_3'].astype('int')\nbaseline['feature_4']=baseline['feature_4'].astype('int')\nbaseline['feature_5']=baseline['feature_5'].astype('int')\nbaseline['feature_6']=baseline['feature_6'].astype('int')\nbaseline['feature_7']=baseline['feature_7'].astype('int')\nbaseline['feature_8']=baseline['feature_8'].astype('int')\n\ncolumns_names=baseline.columns\n\n\n#Scaling the dataframe \n\nsc = StandardScaler()\nbaseline = sc.fit_transform(baseline[columns_names])\n\n\n#converting it back to dataframe \nbaseline_scaled=pd.DataFrame(baseline,columns=columns_names)\n\n\n\nX=baseline_scaled.drop(['price'],axis=1).values\ny=baseline_scaled['price'].values.reshape(len(X),1)\n\n\nprint(X.shape,y.shape)\n\n\n\n# Splitting the dataset into the Training set and Test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state =42)","7aada1b9":"\n#Baseline Model1\n\n# Initialising the ANN\nmodel_baseline = Sequential()\n\n# Adding the input layer and the first hidden layer\nmodel_baseline.add(Dense(units = 5, kernel_initializer = 'he_normal', activation = 'relu', input_dim = X.shape[1]))\n\n# Adding the second hidden layer\nmodel_baseline.add(Dense(units = 5, kernel_initializer = 'he_normal', activation = 'relu'))\n\n\n# Adding the second hidden layer\n#model_baseline.add(Dense(units = 8, kernel_initializer = 'he_normal', activation = 'relu'))\n\n\n# Adding the output layer\nmodel_baseline.add(Dense(units = 1, kernel_initializer = 'he_normal', activation = 'linear'))\n\n# Compiling the ANN\n\nsgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\nmodel_baseline.compile(optimizer =sgd, loss = 'mean_squared_error',metrics=['MAE'])\n\n# Fitting the ANN to the Training set\nmodel_baseline.fit(X_train, y_train,validation_data=(X_test,y_test) , batch_size = 128, epochs = 500,verbose=0)\n\n\n\npredictions=model_baseline.predict(X_test)\n\n","c27cf8bf":"model_baseline.summary()","6b6a68de":"print('Explained_varianve_score={}'.format( explained_variance_score(y_test,predictions)))\nprint('R-squared={}'.format( r2_score(y_test,predictions)))","2fca3d48":"losses = pd.DataFrame(model_baseline.history.history)\nlosses[['loss','val_loss']].plot()","e6301fb6":"\n# Model 2\n# I am selecting the features below for fitting the price model .\n# Dropping the fuel type as it is screwed towards the diseal fuel \n# Also dropping maker_key as its all BMW  and model_key as that is captured in car_type \n\ndata=data[data.price<100000] # Removing price outliers \ndata=data[data.mileage<400000] #Removing mileage outliers . In total 8 rows removed \n\nfeatures=data[['mileage','engine_power','paint_color','car_type','feature_1','feature_2','feature_3','feature_4','feature_5','feature_6','feature_7','feature_8','price','vechile_days']]\n\n#Creating Dummy Variables \nfeatures=pd.get_dummies(features,drop_first=True)\n\n\nfeatures['vechile_days']=features['vechile_days'].astype('int')\nfeatures['feature_1']=features['feature_1'].astype('int')\nfeatures['feature_2']=features['feature_2'].astype('int')\nfeatures['feature_3']=features['feature_3'].astype('int')\nfeatures['feature_4']=features['feature_4'].astype('int')\nfeatures['feature_5']=features['feature_5'].astype('int')\nfeatures['feature_6']=features['feature_6'].astype('int')\nfeatures['feature_7']=features['feature_7'].astype('int')\nfeatures['feature_8']=features['feature_8'].astype('int')\n\ncolumns_names=features.columns\n\nsc = StandardScaler()\nfeatures = sc.fit_transform(features[columns_names])\n\n#converting it back to dataframe \nfeatures_scales=pd.DataFrame(features,columns=columns_names)\n\n\nX=features_scales.drop(['price'],axis=1).values\ny=features_scales['price'].values.reshape(len(X),1)\n\n\n\n# Splitting the dataset into the Training set and Test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state =42)\n","12e17e94":"print(X.shape,y.shape)","ac5ef946":"\n\n# Initialising the ANN\nmodel = Sequential()\n\n# Adding the input layer and the first hidden layer\nmodel.add(Dense(units = 12, kernel_initializer = 'he_normal', activation = 'relu', input_dim = X.shape[1]))\n\n# Adding the second hidden layer\nmodel.add(Dense(units = 12, kernel_initializer = 'he_normal', activation = 'relu'))\n\n# Adding the output layer\nmodel.add(Dense(units = 1, kernel_initializer = 'he_normal', activation = 'linear'))\n\n# Compiling the ANN\n\nsgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(optimizer =sgd, loss = 'mean_squared_error',metrics=['MAE'])\n\n# Fitting the ANN to the Training set\nmodel.fit(X_train, y_train,validation_data=(X_test,y_test) , batch_size = 128, epochs = 350,verbose=0)","d0ddb6df":"model.summary()","a15d5c10":"\npredictions=model.predict(X_test)\nprint('Explained_varianve_score={}'.format( explained_variance_score(y_test,predictions)))\nprint('R-squared={}'.format( r2_score(y_test,predictions)))\n","9fef7a2b":"losses = pd.DataFrame(model.history.history)\nlosses[['loss','val_loss']].plot()","099cd8a3":"#Saving weights for this model \n\nweights = model.get_weights()","c922ef72":"errors = y_test - predictions\nsns.distplot(errors)","83c81477":"# Our predictions\nplt.figure(figsize=(8,8));\nplt.scatter(y_test,predictions,alpha=0.3);\n\n# Perfect predictions\nplt.plot(y_test,y_test,'r');","f07fa537":"# We see that most cars sold are estate followed by sedans and suv","2d4e627b":"# We see that SUV and coupe show higher median price over other types ","0835eaec":"Let see how the fuel is distributed with respect to price ","087fdb70":"**Let explonatory analysis on this dataset so as to know the dataset better \n**\n\n","c016bf2e":"# We can also see which car types where brought in which year ","0ac97732":"# We see that most car sold are 3 series like ( 320,318,X3,318) followed by 5 series ","473d799c":"Lets see how each car types stacks up against price","4c731bac":"# We see there is only one year for the sold at columns ","a2b2e450":"# We see mostly a positive corelation between the year and price ","71f6f7f8":"# Model 2 ","ed2b9772":"Not all paint colors are equally distributed in the dataset ","4b9a8f08":"We see that color is not a big factor in determining the final price of the BMW sold . As first six plot on the above graph show almost the same quartile ranges. So while fitting a neural network model I will drop the feature from the dataset ","035b80a6":"# Even though the diesel car are more in number we see that hybrid_Petrol sell at a higher price followed by diesel ","fb898cca":"# Let see how the time frame is distributed ","d8c4c5d1":"# Let see how mileage and price relate \n","67061d81":"# As expected newer cars have low mileage and hence a higher prices . Also By removing the xlim we see that there are very few model which are above 500,000 mileage and 100000 price range so I am thinking of removing them from the prediction model ","e9fa2f99":"plt.figure(figsize=(10,10));\nsns.scatterplot(data=data.sample(2500),x='mileage',y='price',alpha=0.8,hue='registration_year');\nplt.xlim(0,500000)\nplt.ylim(0,100000)\nplt.xlabel('Mileage',fontsize=20);\nplt.ylabel('Price',fontsize=20);\nplt.title('Mileage vs Price vs Year',fontsize=20);\n","bfe05032":"# I am going to add another column to the datafame called vechile_days which is the difference of time in days between registration_date and sold_at columns","c07414a3":"# Most cars are between 100 and 150 HP ","721cb8a1":"# We see that price variable has a decent correlation with the other quantitative variables shown above ","e0e609ad":"# How much impact does each of features have on the estimate value of the car?\n\nIt very difficult to assign value of each individual feature but from the Model 1 ANN which takes into account the 8 boolean feature we where able to explain only 40% of the variance in price.  But the current model (Model 2) can capture 83% of variance in the model .So it would be safe to conclude that the 8 unknown boolean features can explain ~40% variation in price \n\n\n\n# How does the estimated value of a car change over time? Can you detect any patterns? (e.g. the price of a convertible should be higher in summer than in winter)\n\nSome of the patterns we could draw form the dataset are as follows \n\n* Diesel was the most common type of fuel in the cars which where sold \n* Even though the diesel car are more in number we see that hybrid_Petrol sell at a higher price followed by diesel \n* We see that color is not a big factor in determining the final price of the BMW sold .\n* Estate cars are more in numbers in the data set followed by Sedan . Looking the car type I feel this data might be collected in EMEA (Europe) region <br>\n* We see that SUV and coupe show higher median price over other types of cars \n* Most cars where purchased in 2013 and 2014 and sold\/acutioned off in 2018 \n* SUV command a higher price in each year \n* Most cars have engine between 100 and 150 HP.Overall engine range is from 0-450HP \n* For a similar mileage value SUV command a higher price over other models \n* We see that the range of price variable is pretty large but most of the cars sold lie between 10,000 to 19000 price range \n* Inter quartile for mileage is 100,000 to 175,000\n* Sedans where most earlier car registered followed by estats and SUV but of late not many sedans are being registered \n\n\n\n\n# How big is the influence of the factors not represented in the data on the price? Or, in other words, what is the estimated variance included in your statistical model?\n\nThe statistical model created above has a R-square value of 83% that is it explains 83% of the variance in the model . The other features which are not captured in the dataset controls the other 17%. The R-squared value of the model fluctuates depending on where it lands up after 350 iteration but most of the time it fluctuates between 80-83% R squared value . ","3719add7":"# Building a basic ANN model which takes into account only the 8 features into acount and avoiding everthing else .\n\n","3bd28449":"# Conclusion from the dataset ","c088dc02":"Hello All ,\n\nWhen I moved to Canada couple of years back I was looking to buy a pre-owned\/pre-certified car. However I use the feel the prices qouted by the dealers to be very high for a decent car. The prices which used to be qouted always use to beat my prediction by 2-3K . Also since I am mechanical engineer who also happen to work in the automotive domain I always use to wonder what factors influence the price of a pre-certified car. With this dataset I got an chance to predict just that . I am sure many more people face the same question . I hope this data analysis and subsequent deep learning prediction model can help them . \n\n\nThe data set contains the following column\n\nmaker_key-         The brand of the car\t\nmodel_key-         The model of the car\t\nmileage\t  -        Total miles driven \nengine_power -     Engine capacity\t\nregistration_date- Date car was registered \t\nfuel\t-          Type of fuel ( diesel, petrol,..)\npaint_color\t-       The color of the car \ncar_type-           The type of car ( sedan ,SUV,..)\t\nfeature_1\t        Some features which the company wants to explore \nfeature_2\t\nfeature_3\t\nfeature_4\t\nfeature_5\t\nfeature_6\t\nfeature_7\t\nfeature_8\t\nprice\t-          The price at which it was auctioned \nsold_at-           The date at which it was sold at \n\n","d6627aff":"# Let see how the price changes with year ","dde5d087":"# For similar mileage we see that SUV command higher price and hatchbacks among the lowest\n"}}