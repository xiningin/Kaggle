{"cell_type":{"b87dd02c":"code","d69eeba0":"code","dd11218f":"code","effbc77c":"code","e8db27fd":"code","1a3af79a":"code","21d7c6e8":"code","176a45d0":"code","02160ab9":"code","45d5badb":"code","a9cb7698":"code","19fb5e22":"code","40757595":"code","f7189c31":"code","531f3db0":"code","10feb77b":"code","14258b56":"code","082f3fd2":"code","8afb2d7a":"code","ed9b887c":"code","6ed16ed2":"code","fe04d34a":"code","4f5d4510":"code","f53c0a34":"code","9cf2eaeb":"code","2c206253":"code","7af0b5b9":"code","7c40fa52":"code","456e4f52":"code","2f3f4263":"code","19ed1f7a":"code","aafe1613":"code","fffed1f6":"code","522a0f8a":"code","c110b9e9":"code","089a8797":"code","4138ce54":"code","aa8b7a6f":"markdown","da872b4c":"markdown"},"source":{"b87dd02c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d69eeba0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","dd11218f":"telugu_df=pd.read_csv(\"\/kaggle\/input\/telugu-nlp\/telugu_books\/telugu_books.csv\")\ntrain_df=pd.read_csv(\"\/kaggle\/input\/telugu-nlp\/telugu_news\/train_telugu_news.csv\")\ntest_df=pd.read_csv(\"\/kaggle\/input\/telugu-nlp\/telugu_news\/test_telugu_news.csv\")\nprint(telugu_df.info())\nprint(test_df.info(10))\nprint(train_df.info(10))","effbc77c":"train_df[\"text\"]=train_df['heading']+train_df['body']\ntest_df['text']=test_df['heading']+test_df['body']\n\nconcat_df=pd.concat([train_df['text'],test_df['text']])\nconcat_df1=pd.DataFrame(concat_df,columns=['text'])\nresult_df=pd.concat([telugu_df['text'],concat_df1['text']])\n","e8db27fd":"result_df1=pd.DataFrame(result_df,columns=['text'])\nresult_df1.info()","1a3af79a":"import re\nimport re\ndef clean_telugu_text_vocab(str_element):\n    a=str(str_element)\n    \n    a=a.replace(\"\\r\",'')\n    a=a.replace(\"\\n\",'')\n    #a=a.replace(u'xao','')\n    a=a.replace(\"  \",\"\")\n    a=a.replace('\"','')\n    a=a.replace(u'xao',u'')\n  \n    a=a.split()\n    \n    a=' '.join(map(str, a))\n    return a\nresult_df1['text2']=result_df1['text'].apply(clean_telugu_text_vocab)\ndef clean_telugu_text(str_element):\n    a=str(str_element)\n    \n    a=a.replace(\"\\r\",'')\n    a=a.replace(\"\\n\",'')\n    #a=a.replace(u'xao','')\n    a=a.replace(\"  \",\"\")\n    a=a.replace('\"','')\n    a=a.replace(u'xao',u'')\n  \n    a=a.split()\n    \n    a=' '.join(map(str, a))\n    return a.split()\nresult_df1['text1']=result_df1['text'].apply(clean_telugu_text)","21d7c6e8":"result_df1['text1'].head()","176a45d0":"from gensim.test.utils import get_tmpfile\nfrom gensim.models import Word2Vec\ncommon_texts=list(result_df1['text1'])\npath = get_tmpfile(\"word2vec.model\")\nmodel = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)","02160ab9":"print(model.most_similar(\"\u0c35\u0c41\u0c02\u0c21\u0c32\u0c47\u0c30\u0c41\"))\nprint()\nprint(model.most_similar(\"\u0c15\u0c33\u0c4d\u0c33\u0c32\u0c4b\"))","45d5badb":"print(model.most_similar(\"\u0c26\u0c42\u0c30\u0c02\"))\nprint()\nprint(model.most_similar(\"\u0c2d\u0c2f\u0c02\"))\nprint()\nprint(model.most_similar(\"\u0c2d\u0c3e\u0c30\u0c40\"))","a9cb7698":"#model.save(\"\/content\/Gdrive\/My Drive\/Kaggle_prjs\/Telugu_NLP\/W2V_TE\")\nmodel.wv.save_word2vec_format(\"\/kaggle\/working\/Word2Vec_TE1.txt\",binary=False)","19fb5e22":"#train_df=pd.read_csv(\"\/content\/Gdrive\/My Drive\/Kaggle_prjs\/Telugu_NLP\/train_telugu_news.csv\")\n\ntrain_df.head()","40757595":"test_df.head()","f7189c31":"train_df.info()","531f3db0":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ntrain_df['target']=le.fit_transform(train_df['topic'])\nprint(train_df['target'].unique())\nprint(train_df['topic'].unique())","10feb77b":"test_df['target']=le.fit_transform(test_df['topic'])\nprint(test_df['target'].unique())\nprint(test_df['topic'].unique())","14258b56":"print(train_df[\"topic\"].value_counts())\nsns.countplot(x='topic',data=train_df)\n","082f3fd2":"max_len = 500\ntrain_df['text1']=train_df['body'].apply(clean_telugu_text)\ntrain_df['heading1']=train_df['heading'].apply(clean_telugu_text)\ntrain_df[\"concat_text\"]=train_df['heading1']+train_df['text1']\ntrain_df[\"concat_text\"].head()\n","8afb2d7a":"train_df['t2']=train_df['body']+train_df['heading']\ntrain_df[\"text2\"]=train_df[\"t2\"].apply(clean_telugu_text_vocab)\ntrain_df[\"text2\"].head()","ed9b887c":"test_df['t2']=test_df['body']+test_df['heading']\ntest_df[\"text2\"]=test_df[\"t2\"].apply(clean_telugu_text_vocab)\ntest_df[\"text2\"].head()","6ed16ed2":"import os \nembeddings_index={}\n\nf = open(os.path.join(\"\/kaggle\/working\/Word2Vec_TE1.txt\"))\nfor line in f:\n    values=line.split()\n    word=values[0]\n    #print(word)\n    coefs=np.asarray(values[1:])\n    embeddings_index[word]=coefs\nf.close()","fe04d34a":"len(embeddings_index)\n","4f5d4510":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","f53c0a34":"tokenizer=Tokenizer(oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(train_df['text2'])\ntokenizer1=Tokenizer(oov_token=\"<OOV>\")\ntokenizer1.fit_on_texts(test_df['text2'])\nword_index=tokenizer.word_index\n\nword_index","9cf2eaeb":"word_index_test=tokenizer1.word_index\n\nword_index_test","2c206253":"train_df[\"seq_train\"]=tokenizer.texts_to_sequences(train_df['text2'])\npadded_X=pad_sequences(list(train_df[\"seq_train\"]),maxlen=256)\ntest_df[\"seq_train\"]=tokenizer1.texts_to_sequences(test_df['text2'])\npadded_Xtest=pad_sequences(list(test_df[\"seq_train\"]),maxlen=256)","7af0b5b9":"print(len(padded_X[0]))\nprint(len(padded_Xtest[0]))","7c40fa52":"print(len(word_index))\nprint(len(word_index_test))","456e4f52":"from keras.models import Model, Input\nimport keras\nfrom keras.applications.densenet import DenseNet121\nfrom keras.layers import Input\nfrom keras.models import Model,Sequential\nfrom keras.layers import Dense,Conv1D,MaxPool1D,BatchNormalization,MaxPooling1D,SpatialDropout1D\nfrom keras.optimizers import Adam,SGD\nfrom keras.layers import LSTM, Embedding, Dense\nfrom keras.models import Model, Input\nfrom keras.layers.merge import add\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda,Flatten","2f3f4263":"embedding_matrix = np.zeros((len(word_index) + 1, 100))\nfor word, i in word_index.items():\n  \n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n     \n    \nembedding_layer = Embedding(len(word_index) + 1,\n                            100,\n                            weights=[embedding_matrix],\n                            input_length=256,\n                            trainable=False)","19ed1f7a":"max_len=256\n\nsequence_input = Input(shape=(max_len,), )\n\nemb_word = embedding_layer(sequence_input)\n \n# Adding dropout layer\nemb_word = Dropout(0.2)(emb_word)\n\nx = emb_word\nx = Bidirectional(LSTM(units=50, return_sequences=True,\n                       recurrent_dropout=0.2, dropout=0.2))(x)\nx_rnn = Bidirectional(LSTM(units=50, return_sequences=True,\n                           recurrent_dropout=0.2, dropout=0.2))(x)\nx = add([x, x_rnn])  # residual connection to the first biLSTM\n\nx = (Flatten())(x)\nout = (Dense(5, activation=\"softmax\"))(x)\nmodel_word2vec = Model(sequence_input, out)\nmodel_word2vec.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\",metrics=['acc'])","aafe1613":"model_word2vec.summary()","fffed1f6":"padded_X=np.array(padded_X).astype(\"float32\")\nprint(padded_X.shape)\npadded_Xtest=np.array(padded_Xtest).astype(\"float32\")\nprint(padded_Xtest.shape)","522a0f8a":"ycat_tr=np.array(train_df[\"target\"]).reshape(-1,1)\nprint(ycat_tr.shape)\nycat_te=np.array(test_df[\"target\"]).reshape(-1,1)\nprint(ycat_te.shape)","c110b9e9":"history = model_word2vec.fit(padded_X,                    \n                    ycat_tr,\n                    batch_size=32, epochs=3, validation_split=0.15, verbose=1)","089a8797":"import matplotlib.pyplot as plt\nhist = pd.DataFrame(history.history)\nplt.style.use(\"ggplot\")\nplt.figure(figsize=(20,4))\nplt.plot(hist[\"acc\"])\nplt.plot(hist[\"val_acc\"])\nplt.show()\n\nhist = pd.DataFrame(history.history)","4138ce54":"from sklearn.metrics import classification_report,confusion_matrix\ny_preds = model_word2vec.predict(padded_Xtest)\ny_preds_te=np.argmax(y_preds,axis=-1)\ntarget_names=['business','editorial','entertainment','nation','sports']\nprint(classification_report(test_df['target'], y_preds_te, target_names=target_names))\nprint(confusion_matrix(test_df['target'], y_preds_te))","aa8b7a6f":"#Now use the embddings_index dict and the embedding layer from the pytorch to generate the lower dimension embeddings ","da872b4c":"So far trained the corpus on books.csv,now would like to use the words in train csv to generate the sequences"}}