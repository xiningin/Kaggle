{"cell_type":{"cac64016":"code","6d7b7630":"code","c255715c":"code","0578910f":"code","313241fe":"code","bbf910c8":"code","4bb25cb9":"code","80372c30":"code","52dcd7ac":"code","cc8866b4":"code","67377553":"code","3603dac1":"code","b7e9d03d":"code","0354bd3b":"code","6e79f602":"code","0673569d":"code","135ca644":"code","1cb154a1":"code","2229762d":"code","cc600fdc":"code","e35baf53":"code","1b7d9f78":"code","f63ef9b6":"code","67126d52":"code","3122ce3f":"code","b867f946":"code","e7c65d56":"code","5fb6611f":"code","f8fd6263":"code","2d2cd071":"markdown","35aeaeb6":"markdown","9ebf9558":"markdown","6540f698":"markdown","3f3a4928":"markdown","e03ea16a":"markdown","97aee39e":"markdown","dc104df2":"markdown","88057f92":"markdown","4251249a":"markdown","206e552d":"markdown","16ab9e38":"markdown","d698183f":"markdown","f3509e9f":"markdown","cf3c1a5d":"markdown","09ccadbc":"markdown","91d8312f":"markdown","fa30130e":"markdown"},"source":{"cac64016":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6d7b7630":"df_train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv\")\n\ndf_valid_preds_0 = pd.read_csv(\"\/kaggle\/input\/tps-nov-2021-logistic-reg-xgboost-stacking1\/valid_preds_LogisticRegression.csv\")\ndf_valid_preds_1 = pd.read_csv(\"\/kaggle\/input\/tps-nov-2021-logistic-reg-xgboost-stacking1\/valid_preds_XGB.csv\")\n\ndf_test_preds_0 = pd.read_csv(\"\/kaggle\/input\/tps-nov-2021-logistic-reg-xgboost-stacking1\/test_preds_LogisticRegression.csv\")\ndf_test_preds_1 = pd.read_csv(\"\/kaggle\/input\/tps-nov-2021-logistic-reg-xgboost-stacking1\/test_preds_XGB.csv\")\n\nrandom_state = 42","c255715c":"df_train.head()","0578910f":"df_valid_preds_0.head()","313241fe":"df_test_preds_0.head()","bbf910c8":"df_train = df_train.merge(df_valid_preds_0, on=\"id\", how=\"left\")\ndf_train = df_train.merge(df_valid_preds_1, on=\"id\", how=\"left\")\n\ndf_train.head()","4bb25cb9":"df_test = df_test.merge(df_test_preds_0, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test_preds_1, on=\"id\", how=\"left\")\n\ndf_test.head()","80372c30":"useful_features_train = [\"pred_0\", \"pred_1\", \"target\"]\n\ndf_train[useful_features_train].head()","52dcd7ac":"columns = df_test.columns[1:]\n\n# columns = [\"pred_0\", \"pred_1\"]\nprint(columns)","cc8866b4":"# assert False","67377553":"y_train = df_train[\"target\"]\ndf_train = df_train.drop(labels = \"target\", axis=1)\n\ndf_train.head()","3603dac1":"df_test.head()","b7e9d03d":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer","0354bd3b":"std_scaler = StandardScaler()\n\nnum_pipeline = Pipeline([(('std_scaler'), StandardScaler()),])\nfull_pipeline = ColumnTransformer([('num', num_pipeline, columns),])\n\ndf_train[columns] = full_pipeline.fit_transform(df_train)\ndf_test[columns] = full_pipeline.transform(df_test)","6e79f602":"df_train.head()","0673569d":"df_test.head()","135ca644":"from sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score, accuracy_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier","1cb154a1":"def plot_roc_curve(fpr, tpr, label=None, title=None):\n    \"\"\"\n    \"\"\"\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--') \n    plt.legend(loc = 'lower right')\n    plt.title(title)\n    plt.show()\n\n    \ndef roc_auc_score_func(y_true, y_head, plot_roc=False):\n    \"\"\" evaluate roc auc score\n    Args:\n        y_true : a numpy array. True labels.\n        y_head : a numpy array. Predicted labels.\n        plot_roc : if you want to plot roc curve. (Default is False)\n        \n    \"\"\"\n    \n    if plot_roc:\n        fpr, tpr, thresholds = roc_curve(y_true, y_head)\n        plot_roc_curve(fpr, tpr, label=None, title=None)\n        \n        \n    # evaluate roc auc score\n    model_roc_auc_score = roc_auc_score(y_true, y_true)\n    \n    return model_roc_auc_score\n\n\ndef train_and_predict(clf, clf_name, X, Y, x_test, n_splits=5):\n    \"\"\"train ml models with Stratified Kfol return auc score for probability of test data with ml model name\n    \n    Args:\n        clf : model classifier\n        clf_name : classifier name\n        x_train : a numpy.darray training data \n        y_train : a numpy.darray training labels\n        x_test: test data\n        n_splits : StratifiedKFold splits number\n        \n    Returns:\n        roc_auc_score_mean\n        accuracy_mean\n        valid_preds_dict : a dict that stores validation set ids and predictions probabilities from StratifiedKFold\n        test_preds: predictions probabilities of test set\n    \"\"\"\n    \n    valid_preds = []\n    valid_ids = []\n    valid_preds_dict = {}\n    \n    test_preds = []\n    \n    roc_auc_score_list = []  # roc auc score list\n    acc_score_list = [] # auc score list\n    \n\n\n    skf = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n    \n    for i, (train_index, val_index) in enumerate(skf.split(X, Y)):\n        print(\"Fitting fold\", i+1)\n        X_train, X_val = X[train_index], X[val_index]\n        Y_train, Y_val = Y[train_index], Y[val_index]\n        \n        print(\"Model:\", clf_name)\n\n        # TRAINING\n        training_start_time = time.time()\n        local_time = time.ctime(training_start_time)\n        print(\"Local time:\", local_time)\n        \n        model = clf\n        model.fit(X_train, Y_train)  # except id column\n\n        training_end_time = time.time()\n        training_time = training_end_time - training_start_time\n        print(f\"Training elapsed seconds: {round(training_time,3)}\")\n\n\n        # EVALUATING\n        evaluating_start_time = time.time()\n        local_time = time.ctime(evaluating_start_time)\n        print(\"Local time:\", local_time)\n        \n        valid_preds = model.predict_proba(X_val)[:, 1]  # predict probabilty of validation set\n        valid_ids = X[val_index,0].tolist()\n        valid_preds_dict.update(dict(zip(valid_ids, valid_preds)))  \n    \n        \n        roc_auc_score_list.append(roc_auc_score(Y_val, valid_preds))\n        acc_score_list.append(accuracy_score(Y_val, model.predict(X_val)))\n\n        evaluating_end_time = time.time()\n        evaluating_time = evaluating_end_time - evaluating_start_time\n        print(f\"evaluating scores elapsed seconds: {round(evaluating_time,3)}\")\n    \n    \n        # PREDICTION\n        prediction_start_time = time.time()\n        local_time = time.ctime(prediction_start_time)\n        print(\"Local time:\", local_time)\n        \n        test_pred = clf.predict_proba(x_test)[:, 1]\n        test_preds.append(test_pred)\n\n        prediction_end_time = time.time()\n        prediction_time = prediction_end_time - prediction_start_time\n        print(f\"predicting test probability scores elapsed seconds: {round(prediction_time,3)}\")\n    \n    roc_auc_score_mean = np.mean(roc_auc_score_list)\n    accuracy_mean = np.mean(acc_score_list) \n    \n    \n    print(f\"Mean accuracy: {round(accuracy_mean*100,3)}, Mean AUC Score: {round(roc_auc_score_mean*100,3)}\")\n    \n    return roc_auc_score_mean, accuracy_mean, valid_preds_dict, test_preds","2229762d":"# IT CONTROLS THE CODE WORKS BEFORE SUBMIT\n# from sklearn.model_selection import StratifiedShuffleSplit\n\n# split = StratifiedShuffleSplit(n_splits=1, test_size=0.001, random_state=42)\n\n# for train_index, val_index in split.split(df_train, y_train):\n#     df_train, y_train = df_train.loc[val_index], y_train.loc[val_index] \n#     x_val, y_val = df_train.loc[val_index], y_train.loc[val_index]\n\n# # x_train_2 = x_train_2.values\n# # y_train_2 = y_train_2.values\n\n# # # x_train_2[:,0] # id","cc600fdc":"!pip install --upgrade xgboost\n\n# # xgb.__version__","e35baf53":"from xgboost import XGBClassifier","1b7d9f78":"# assert False","f63ef9b6":"df_train.head()","67126d52":"df_train = df_train.values\ny_train = y_train.values","3122ce3f":"df_train.shape","b867f946":"# clf = XGBClassifier(max_depth=8,\n#                     learning_rate=0.01,\n#                     n_estimators=10000,\n#                     verbosity=1,\n#                     silent=None,\n#                     objective='binary:logistic',  \n#                     tree_method = 'gpu_hist',\n#                     booster='gbtree',\n#                     n_jobs=-1,\n#                     nthread=None,\n#                     eval_metric='auc',\n#                     gamma=0,\n#                     min_child_weight=1,\n#                     max_delta_step=0,\n#                     subsample=0.7,\n#                     colsample_bytree=1,\n#                     colsample_bylevel=1,\n#                     colsample_bynode=1,\n#                     reg_alpha=0,\n#                     reg_lambda=1,\n#                     scale_pos_weight=1,\n#                     base_score=0.5,\n#                     random_state=random_state,\n#                     seed=None)\n\n# clf_name = \"XGB\"","e7c65d56":"clf = LogisticRegression(solver='liblinear', random_state = random_state)\nclf_name = \"LogisticRegression\"","5fb6611f":"df_train[:,1:].shape","f8fd6263":"%%time\n\nimport time\n\n\nroc_auc_score_mean, accuracy_mean, valid_preds_dict, test_preds = train_and_predict(clf, \n                                                                                    clf_name, \n                                                                                    df_train[:,1:], \n                                                                                    y_train,  \n                                                                                    df_test[columns],\n                                                                                    n_splits=2)\n    \n    \n    \n\n\n#save predictions of test data to csv\nsub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')\nprint(sub)\ntest_preds = np.mean(np.column_stack(test_preds), axis=1)  # mean of every kfold predictions. Before that it gives a list has two columns\n\nsub['target'] = test_preds\nsub.to_csv('submission.csv', index=False)\nsub","2d2cd071":"One of my references here [abhishek's notebook](https:\/\/www.kaggle.com\/abhishek\/competition-part-6-stacking\/notebookhttps:\/\/www.kaggle.com\/abhishek\/competition-part-6-stacking\/notebook), especially mean of several kfold test predictions. \n\n[back to the top](#0)","35aeaeb6":"[Variable Describtions](#7)\n[back to the top](#0)","9ebf9558":"<a id=\"4\"><\/a> <br>\n# 3. Merge the Data","6540f698":"<a id=\"4\"><\/a> <br>\n# 3. Feature Scaling\nIn order to, ML algorithms perform well, I will scale data with Standardization method.\n\n[Variable Describtions](#7)\n\n[back to the top](#0)","3f3a4928":"<a id=\"2\"><\/a> <br>\n## A. Variable Describtions:\n- **df_train** : Pandas data frame for training data set\n- **df_test** : Pandas data frame for test data set\n- **x_train** : Pandas data frame removed target columns from df_train\n- **y_train** : Pandas data frame from df_train","e03ea16a":"<a id=\"3\"><\/a> <br>\n# 2. Load the Data\n\n[back to the top](#0)","97aee39e":"[back to the top](#0)","dc104df2":"<a id=\"5\"><\/a> <br>\n# 4. Helper Functions\n[back to the top](#0)","88057f92":"<a id=\"7\"><\/a> <br>\n## A. Split Train and Validation Data\n[back to the top](#0)","4251249a":"[back to the top](#0)","206e552d":"[back to the top](#0)","16ab9e38":"<a id=\"1\"><\/a> <br>\n# 1. Introduction to Tabular Playground Series - Nov 2021 \n\nTPS is a monthly competition prepared by Kaggle. The data is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. More information can be found on the [Competition Overview Page](https:\/\/www.kaggle.com\/c\/tabular-playground-series-nov-2021\/overview).\n\n**The goal** is **predicting probability** of the observed target 0 or 1. So it is **supervised learning** and **classification task**. Also **evaluation metric** is selected **area under the ROC curve**.\n\n1. [My first Notebook on this competition: TPS Nov 2021 Starter with XGBoost](https:\/\/www.kaggle.com\/ahmetekiz\/tps-nov-2021-starter-with-xgboost#7.-Selecting-Models)\n1. [My second Notebook on this competition: TPS Nov 2021 Stacking](https:\/\/www.kaggle.com\/ahmetekiz\/tps-nov-2021-stacking)\n\n[back to the top](#0)","d698183f":"[Variable Describtions](#7)\n\n[back to the top](#0)","f3509e9f":"I've chosen three classifiers according to my previous notebook's evaluation results.","cf3c1a5d":"## Create a Pipeline for Preparing Data to Training","09ccadbc":"<a id=\"0\"><\/a> <br>\n# Table of Contents\n\n1. [Introduction to Tabular Playground Series - Nov 2021](#1)\n    1. [Variable Describtions](#2)\n1. [Load and Glance at the Data](#3)   \n1. [Merge the Data](#4)\n1. [Helper Functions](#5) \n1. [XGBoost](#6) ","91d8312f":"[back to the top](#0)","fa30130e":"<a id=\"6\"><\/a> <br>\n# 5. XGBoost\n[back to the top](#0)"}}