{"cell_type":{"f13af4ac":"code","6090ce45":"code","b234d2ef":"code","9b457065":"code","5d02e3c6":"code","52e479db":"code","82f971ed":"code","552fd958":"code","adf36169":"code","e7d9c224":"code","2075e6f4":"code","d7fd7a08":"code","ab4cf202":"code","febbcd2d":"code","22e564af":"code","4006cc7f":"code","a94124b4":"code","413b6027":"code","8e498d03":"code","8c8f18b7":"code","536c6e58":"code","3341dcf7":"code","d75d1c76":"code","09d57dc9":"code","d0817c7c":"code","c4412410":"code","62373d1c":"code","2c2f12c3":"code","2b5514a5":"code","b0de125c":"code","b2d239b4":"code","65287dae":"code","42f3072e":"code","d337755c":"code","daa8505d":"code","53e2382a":"code","48d87eef":"code","13c9208b":"code","1e5d64cc":"code","47277fb3":"code","ef96faaf":"code","8462c81f":"code","a450c16a":"code","39a718c1":"code","72941155":"code","95c6d75e":"code","bffcb1a6":"code","d0337422":"code","8ac6b4a6":"code","6f04d031":"code","81a4c36d":"code","70719f4c":"code","b4c52232":"code","689a8cc5":"code","1c7d989b":"code","1fa7b29b":"code","c34a2ffb":"code","8ae4779e":"code","1e52f9c0":"code","b8180bde":"code","84bc830e":"code","91da71ce":"code","dd949524":"code","51fa7f6d":"code","34a26276":"code","27b3cc62":"code","0cb1ccc2":"code","d4d71076":"code","039e3661":"code","c01e929f":"code","eda7e601":"code","6d3a3706":"code","1739d604":"code","f630f168":"code","65d1777f":"code","46756ece":"code","c9f52add":"code","dff85d11":"code","f94a3de0":"code","36b45c60":"code","3d32d962":"code","b15d668f":"code","4cf342e4":"code","56a5247d":"code","8d53b01f":"code","8bbcce30":"code","bbd9b1e9":"code","1506414f":"code","26a4b497":"code","04cd6df5":"code","eeecd461":"code","bd88fabb":"code","0c3c871e":"code","c3754cc0":"code","011a008a":"markdown","9885b048":"markdown","5d36323e":"markdown","75163b65":"markdown","50132307":"markdown","f247c522":"markdown","ace18b71":"markdown","5389d6d7":"markdown","c38ef02b":"markdown","54d3a69b":"markdown","9abcaabb":"markdown","ad80c51a":"markdown","f6bdfd5a":"markdown","2f07359a":"markdown","1680a4fd":"markdown","a43dd1f3":"markdown","def06e8d":"markdown","d243f379":"markdown","fd9a798d":"markdown","9217c787":"markdown","e9ee7cb8":"markdown","04cfcac8":"markdown","85ede82a":"markdown","bd4e19ac":"markdown","e8b759ef":"markdown","20e17513":"markdown","b93ac455":"markdown","b5fc05cd":"markdown","bb3afdd6":"markdown","c8542b48":"markdown","a62fdb7d":"markdown","883a6804":"markdown","a79f9910":"markdown","f0032434":"markdown","47d911b3":"markdown","d7196b6f":"markdown","a4c0e858":"markdown","a80f1f2b":"markdown","c0a745dd":"markdown"},"source":{"f13af4ac":"!pip install numpy pandas matplotlib seaborn --quiet","6090ce45":"!pip install jovian opendatasets xgboost graphviz lightgbm scikit-learn xgboost lightgbm --upgrade --quiet","b234d2ef":"import os\nimport opendatasets as od\nimport pandas as pd\npd.set_option(\"display.max_columns\", 120)\npd.set_option(\"display.max_rows\", 120)","9b457065":"od.download('https:\/\/www.kaggle.com\/c\/walmart-recruiting-store-sales-forecasting')","5d02e3c6":"os.listdir('walmart-recruiting-store-sales-forecasting')","52e479db":"from zipfile import ZipFile\n\nwith ZipFile('.\/walmart-recruiting-store-sales-forecasting\/features.csv.zip') as f:\n    f.extractall(path='walmart-recruiting-store-sales-forecasting')\n\nwith ZipFile('.\/walmart-recruiting-store-sales-forecasting\/sampleSubmission.csv.zip') as f:\n    f.extractall(path='walmart-recruiting-store-sales-forecasting')\n    \nwith ZipFile('.\/walmart-recruiting-store-sales-forecasting\/test.csv.zip') as f:\n    f.extractall(path='walmart-recruiting-store-sales-forecasting')\n\nwith ZipFile('.\/walmart-recruiting-store-sales-forecasting\/train.csv.zip') as f:\n    f.extractall(path='walmart-recruiting-store-sales-forecasting')\n\nos.listdir('walmart-recruiting-store-sales-forecasting')","82f971ed":"features = pd.read_csv(\".\/walmart-recruiting-store-sales-forecasting\/features.csv\")\nstores = pd.read_csv(\".\/walmart-recruiting-store-sales-forecasting\/stores.csv\")\nwalmart = pd.read_csv(\".\/walmart-recruiting-store-sales-forecasting\/train.csv\")\ntest = pd.read_csv(\".\/walmart-recruiting-store-sales-forecasting\/test.csv\")\nsubmission = pd.read_csv(\".\/walmart-recruiting-store-sales-forecasting\/sampleSubmission.csv\")","552fd958":"print(\"features.shape\", features.shape)\nprint(\"stores.shape\", stores.shape)\nprint(\"walmart.shape\", walmart.shape)\nprint(\"test.shape\", test.shape)\nprint(\"submission.shape\", submission.shape)","adf36169":"features.head(5)","e7d9c224":"stores.head(5)","2075e6f4":"walmart.head(5)","d7fd7a08":"test.head(5)","ab4cf202":"submission.head(5)","febbcd2d":"merged_df = walmart.merge(stores, how='left').merge(features, how='left')\nmerged_test_df = test.merge(stores, how='left').merge(features, how='left')","22e564af":"print(\"merged_df.shape\", merged_df.shape)\nprint(\"merged_test_df.shape\", merged_test_df.shape)","4006cc7f":"import numpy as np\nimport seaborn as sns\nimport os\nimport plotly.express as px\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\n\nstyle.use('seaborn-poster')\nstyle.use(\"fivethirtyeight\")\nplt.rcParams['font.family'] = 'serif'\n\nmatplotlib.rcParams['font.size'] = 14\nmatplotlib.rcParams['figure.figsize'] = (8, 6)\nmatplotlib.rcParams['figure.facecolor'] = '#00000000'\n%matplotlib inline","a94124b4":"def split_date(df):\n    df['Date'] = pd.to_datetime(df['Date'])\n    df['Year'] = df.Date.dt.year\n    df['Month'] = df.Date.dt.month\n    df['Day'] = df.Date.dt.day\n    df['WeekOfYear'] = df.Date.dt.isocalendar().week","413b6027":"split_date(merged_df)\nsplit_date(merged_test_df)","8e498d03":"merged_df.head(5)","8c8f18b7":"merged_df.info()","536c6e58":"merged_df.isna().sum()","3341dcf7":"weekly_sales_2010 = merged_df[merged_df.Year==2010].groupby('WeekOfYear')['Weekly_Sales'].mean()\nweekly_sales_2011 = merged_df[merged_df.Year==2011].groupby('WeekOfYear')['Weekly_Sales'].mean()\nweekly_sales_2012 = merged_df[merged_df.Year==2012].groupby('WeekOfYear')['Weekly_Sales'].mean()\n\nplt.figure(figsize=(22,8))\nplt.plot(weekly_sales_2010.index, weekly_sales_2010.values)\nplt.plot(weekly_sales_2011.index, weekly_sales_2011.values)\nplt.plot(weekly_sales_2012.index, weekly_sales_2012.values)\n\nplt.xticks(np.arange(1, 53, step=1), fontsize=16)\nplt.yticks( fontsize=16)\nplt.xlabel('Week of Year', fontsize=20, labelpad=20)\nplt.ylabel('Sales', fontsize=20, labelpad=20)\n\nplt.title(\"Average Weekly Sales - Per Year\", fontsize=24)\nplt.legend(['2010', '2011', '2012'], fontsize=20);","d75d1c76":"merged_df.head(10)","09d57dc9":"merged_df.columns","d0817c7c":"input_cols = ['Store', 'Dept', 'IsHoliday', 'Type', 'Size',\n       'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3',\n       'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Year', 'Month', 'Day',\n       'WeekOfYear']\ntarget_col = 'Weekly_Sales'","c4412410":"inputs = merged_df[input_cols].copy()\ntargets = merged_df[target_col].copy()","62373d1c":"test_inputs = merged_test_df[input_cols].copy()","2c2f12c3":"numeric_cols = ['Store', 'Dept', 'Size',\n       'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3',\n       'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Year', 'Month', 'Day',\n       'WeekOfYear']\ncategorical_cols = ['IsHoliday', 'Type']","2b5514a5":"#numeric_cols = ['Store', 'Dept', 'Size', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Year', 'Month', 'Day', 'WeekOfYear']\n#categorical_cols = ['IsHoliday', 'Type']","b0de125c":"inputs[numeric_cols].isna().sum()","b2d239b4":"test_inputs[numeric_cols].isna().sum()","65287dae":"zero = 0\nzero","42f3072e":"inputs['MarkDown1'].fillna(zero, inplace=True)\ninputs['MarkDown2'].fillna(zero, inplace=True)\ninputs['MarkDown3'].fillna(zero, inplace=True)\ninputs['MarkDown4'].fillna(zero, inplace=True)\ninputs['MarkDown5'].fillna(zero, inplace=True)\ntest_inputs['MarkDown1'].fillna(zero, inplace=True)\ntest_inputs['MarkDown2'].fillna(zero, inplace=True)\ntest_inputs['MarkDown3'].fillna(zero, inplace=True)\ntest_inputs['MarkDown4'].fillna(zero, inplace=True)\ntest_inputs['MarkDown5'].fillna(zero, inplace=True)","d337755c":"mean_CPI = inputs.CPI.mean()\nmean_Unemployment = inputs.Unemployment.mean()","daa8505d":"inputs['CPI'].fillna(mean_CPI, inplace=True)\ninputs['Unemployment'].fillna(mean_Unemployment, inplace=True)\ntest_inputs['CPI'].fillna(mean_CPI, inplace=True)\ntest_inputs['Unemployment'].fillna(mean_Unemployment, inplace=True)\n","53e2382a":"test_inputs[numeric_cols].isna().sum()","48d87eef":"inputs[numeric_cols].isna().sum()","13c9208b":"from sklearn.preprocessing import MinMaxScaler","1e5d64cc":"scaler = MinMaxScaler().fit(inputs[numeric_cols])","47277fb3":"inputs[numeric_cols] = scaler.transform(inputs[numeric_cols])\ntest_inputs[numeric_cols] = scaler.transform(test_inputs[numeric_cols])","ef96faaf":"from sklearn.preprocessing import OneHotEncoder","8462c81f":"encoder = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(inputs[categorical_cols])\nencoder1 = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(test_inputs[categorical_cols])\nencoded_cols = list(encoder.get_feature_names(categorical_cols))\nencoded_cols1 = list(encoder1.get_feature_names(categorical_cols))","a450c16a":"inputs[encoded_cols] = encoder.transform(inputs[categorical_cols])\ntest_inputs[encoded_cols1] = encoder1.transform(test_inputs[categorical_cols])","39a718c1":"X = inputs[numeric_cols + encoded_cols]\nX_test = test_inputs[numeric_cols + encoded_cols1]","72941155":"from xgboost import XGBRegressor","95c6d75e":"model = XGBRegressor(random_state=42, n_jobs=-1, n_estimators=20, max_depth=4)","bffcb1a6":"%%time\nmodel.fit(X, targets)","d0337422":"preds = model.predict(X)","8ac6b4a6":"preds1 = model.predict(X_test)","6f04d031":"from sklearn.metrics import mean_squared_error\n\ndef rmse(a, b):\n    return mean_squared_error(a, b, squared=False)","81a4c36d":"rmse(preds, targets)","70719f4c":"import matplotlib.pyplot as plt\nfrom xgboost import plot_tree\nfrom matplotlib.pylab import rcParams\n%matplotlib inline\n\nrcParams['figure.figsize'] = 30,30","b4c52232":"plot_tree(model, rankdir='LR');","689a8cc5":"plot_tree(model, rankdir='LR', num_trees=1);","1c7d989b":"trees = model.get_booster().get_dump()","1fa7b29b":"len(trees)","c34a2ffb":"print(trees[0])","8ae4779e":"importance_df = pd.DataFrame({\n    'feature': X.columns,\n    'importance': model.feature_importances_\n}).sort_values('importance', ascending=False)","1e52f9c0":"importance_df.head(10)","b8180bde":"import seaborn as sns\nplt.figure(figsize=(10,6))\nplt.title('Feature Importance')\nsns.barplot(data=importance_df.head(10), x='importance', y='feature');","84bc830e":"from sklearn.model_selection import KFold","91da71ce":"def train_and_evaluate(X_train, train_targets, X_val, val_targets, **params):\n    model = XGBRegressor(random_state=42, n_jobs=-1, **params)\n    model.fit(X_train, train_targets)\n    train_rmse = rmse(model.predict(X_train), train_targets)\n    val_rmse = rmse(model.predict(X_val), val_targets)\n    return model, train_rmse, val_rmse","dd949524":"kfold = KFold(n_splits=5)","51fa7f6d":"models = []\n\nfor train_idxs, val_idxs in kfold.split(X):\n    X_train, train_targets = X.iloc[train_idxs], targets.iloc[train_idxs]\n    X_val, val_targets = X.iloc[val_idxs], targets.iloc[val_idxs]\n    model, train_rmse, val_rmse = train_and_evaluate(X_train, \n                                                     train_targets, \n                                                     X_val, \n                                                     val_targets, \n                                                     max_depth=4, \n                                                     n_estimators=20)\n    models.append(model)\n    print('Train RMSE: {}, Validation RMSE: {}'.format(train_rmse, val_rmse))","34a26276":"import numpy as np\n\ndef predict_avg(models, inputs):\n    return np.mean([model.predict(inputs) for model in models], axis=0)","27b3cc62":"preds = predict_avg(models, X)","0cb1ccc2":"model","d4d71076":"def test_params_kfold(n_splits, **params):\n    train_rmses, val_rmses, models = [], [], []\n    kfold = KFold(n_splits)\n    for train_idxs, val_idxs in kfold.split(X):\n        X_train, train_targets = X.iloc[train_idxs], targets.iloc[train_idxs]\n        X_val, val_targets = X.iloc[val_idxs], targets.iloc[val_idxs]\n        model, train_rmse, val_rmse = train_and_evaluate(X_train, train_targets, X_val, val_targets, **params)\n        models.append(model)\n        train_rmses.append(train_rmse)\n        val_rmses.append(val_rmse)\n    print('Train RMSE: {}, Validation RMSE: {}'.format(np.mean(train_rmses), np.mean(val_rmses)))\n    return models","039e3661":"from sklearn.model_selection import train_test_split","c01e929f":"X_train, X_val, train_targets, val_targets = train_test_split(X, targets, test_size=0.1)","eda7e601":"def test_params(**params):\n    model = XGBRegressor(n_jobs=-1, random_state=42, **params)\n    model.fit(X_train, train_targets)\n    train_rmse = rmse(model.predict(X_train), train_targets)\n    val_rmse = rmse(model.predict(X_val), val_targets)\n    print('Train RMSE: {}, Validation RMSE: {}'.format(train_rmse, val_rmse))","6d3a3706":"test_params(n_estimators=10)","1739d604":"test_params(n_estimators=100)","f630f168":"test_params(n_estimators=500)","65d1777f":"test_params(max_depth=2)","46756ece":"test_params(max_depth=15)","c9f52add":"test_params(n_estimators=50, learning_rate=0.01)","dff85d11":"test_params(n_estimators=500, learning_rate=0.9)","f94a3de0":"test_params(n_estimators=500, learning_rate=0.9, max_depth=15)","36b45c60":"test_params(booster='gblinear')","3d32d962":"model = XGBRegressor(n_jobs=-1, random_state=42, n_estimators=1000, \n                     learning_rate=0.2, max_depth=10, subsample=0.9, \n                     colsample_bytree=0.7)","b15d668f":"model1 = XGBRegressor(n_jobs=-1, random_state=42, n_estimators=1000, \n                     learning_rate=0.9, max_depth=15, subsample=0.9, \n                     colsample_bytree=0.7)","4cf342e4":"%%time\nmodel.fit(X, targets)","56a5247d":"%%time\nmodel1.fit(X, targets)","8d53b01f":"test_preds_sub = model.predict(X_test)","8bbcce30":"test_preds = model.predict(X)","bbd9b1e9":"test_preds1 = model1.predict(X)","1506414f":"test_preds_sub1 = model1.predict(X_test)","26a4b497":"rmse(test_preds, targets)","04cd6df5":"rmse(test_preds1, test_preds)","eeecd461":"rmse(preds, targets)","bd88fabb":"submission","0c3c871e":"test_preds1.shape","c3754cc0":"submission['Weekly_Sales'] = test_preds_sub1\nsubmission.to_csv('submission.csv',index=False)","011a008a":"Let's merge the information from `stores` into `walmart` and `test`.","9885b048":"We tested final regult with XGBRegressor without any useful parameters and with parameters","5d36323e":"#### `max_depth`\n\nAs you increase the max depth of each tree, the capacity of the tree increases and it can capture more information about the training set.","75163b65":"### Prediction\n\nWe can now make predictions and evaluate the model using `model.predict`.","50132307":"#### `n_estimators`\n\nThe number of trees to be created. More trees = greater capacity of the model.\n","f247c522":"### Feature importance\n\nJust like decision trees and random forests, XGBoost also provides a feature importance score for each column in the input.","ace18b71":"Let's define a helper function `train_and_evaluate` which trains a model the given parameters and returns the trained model, training error and validation error.","5389d6d7":"### Exploratory Data Analysis","c38ef02b":"Let's load the data into Pandas dataframes.","54d3a69b":"Here's a helper function to test hyperparameters with K-fold cross validation.","9abcaabb":"Scikit-learn provides utilities for performing K fold cross validation.","ad80c51a":"# Importing Libraries","f6bdfd5a":"Now that the model is trained, we can make predictions on the test set.","2f07359a":"### Visualization\n\nWe can visualize individual trees using `plot_tree` (note: this requires the `graphviz` library to be installed).","1680a4fd":"### Scale Numeric Values\n\nLet's scale numeric values to the 0 to 1 range.","a43dd1f3":"## Preprocessing and Feature Engineering\n\nLet's take a look at the available columns, and figure out if we can create new columns or apply any useful transformations.","def06e8d":"Now, we can use the `KFold` utility to create the different training\/validations splits and train a separate model for each fold.","d243f379":"model1 is by using (n_jobs=-1, random_state=42, n_estimators=1000, \n                     learning_rate=0.9, max_depth=15, subsample=0.9, \n                     colsample_bytree=0.7) these tested parameters","fd9a798d":"## K Fold Cross Validation\n\nNotice that we didn't create a validation set before training our XGBoost model. We'll use a different validation strategy this time, called K-fold cross validation ","9217c787":"Let's also define a function to average predictions from the 5 different models.","e9ee7cb8":"# Walmart Recruiting - Store Sales Forecasting\nUse historical markdown data to predict store sales\n\n\n![](https:\/\/cw39.com\/wp-content\/uploads\/sites\/10\/2021\/06\/AP20323809226583.jpg?w=1752&h=986&crop=1)","04cfcac8":"### Impute missing numerical data","85ede82a":"#### `learning_rate`\n\nThe scaling factor to be applied to the prediction of each tree. A very high learning rate (close to 1) will lead to overfitting, and a low learning rate (close to 0) will lead to underfitting.","bd4e19ac":"Let's train the model using `model.fit`.","e8b759ef":"### Prediction\n\nLet's predict the submission set directly.","20e17513":"Let's also identify numeric and categorical columns. Note that we can treat binary categorical columns (0\/1) as numeric columns.","b93ac455":"Since it may take a long time to perform 5-fold cross validation for each set of parameters we wish to try, we'll just pick a random 10% sample of the dataset as the validation set.","b5fc05cd":"### Training\n\nTo train a GBM, we can use the `XGBRegressor` class from the [`XGBoost`](https:\/\/xgboost.readthedocs.io\/en\/latest\/) library.","bb3afdd6":"## Downloading the Data\n\nWe can download the dataset from Kaggle directly within the Jupyter notebook using the `opendatasets` library.","c8542b48":"\n### Date\n\nFirst, let's convert `Date` to a `datecolumn` and extract different parts of the date.","a62fdb7d":"model is by using (n_jobs=-1, random_state=42, n_estimators=1000, \n                     learning_rate=0.2, max_depth=10, subsample=0.9, \n                     colsample_bytree=0.7)) these parameters","883a6804":"### Encode Categorical Columns\n\n\nLet's one-hot encode categorical columns.","a79f9910":"## Hyperparameter Tuning and Regularization\n\nJust like other machine learning models, there are several hyperparameters we can to adjust the capacity of model and reduce overfitting.\n","f0032434":"#### `booster`\n\nInstead of using Decision Trees, XGBoost can also train a linear model for each iteration. This can be configured using `booster`.","47d911b3":"Let's also identify Input and Target columns.","d7196b6f":"Creating Submitting file","a4c0e858":"### Evaluation\n\nLet's evaluate the predictions using RMSE error.","a80f1f2b":"## Putting it Together and Making Predictions\n\nLet's train a final model on the entire training set with custom hyperparameters. ","c0a745dd":"Finally, let's extract out all the numeric data for training."}}