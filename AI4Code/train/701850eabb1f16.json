{"cell_type":{"8ee940fd":"code","944a6660":"code","d8987bc2":"code","2806a108":"code","afecea04":"code","d5687c4f":"code","38d7a953":"code","de6712f8":"code","89254886":"code","dbb50f95":"code","15d84d8b":"code","9db73451":"code","e90cd0c6":"code","1d097a43":"code","561bae6c":"markdown","33b1f708":"markdown"},"source":{"8ee940fd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\nfrom hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt.pyll.base import scope\nfrom PIL import Image\nfrom tqdm import tqdm\nimport os\nfrom pathlib import Path\nimport lightgbm as lgb","944a6660":"sampel_sub = '\/kaggle\/input\/petfinder-pawpularity-score\/sample_submission.csv'\ntrain_metadata = '\/kaggle\/input\/petfinder-pawpularity-score\/train.csv'\ntest_metadata = '\/kaggle\/input\/petfinder-pawpularity-score\/test.csv'","d8987bc2":"# Credit to: https:\/\/www.kaggle.com\/currypurin\/petfinder-eda-lgb-meta-features-and-img-size\ndef create_shape_feature(df):\n    width_height_list = []\n    file_size_list = []\n    for path_ in tqdm(df['img_path']):\n        width_height_list.append(Image.open(path_).size)\n        file_size_list.append(os.path.getsize(path_))\n    df['width_height'] = width_height_list\n    df['file_size'] = file_size_list\n    df['width'] = df['width_height'].apply(lambda x: x[0])\n    df['height'] = df['width_height'].apply(lambda x: x[1])\n    df['area'] = df['width'] * df['height']\n    df['size_per_pixel'] = df['area'] \/ df['file_size']\n    return df","2806a108":"df_train = pd.read_csv(train_metadata)\ndf_test = pd.read_csv(test_metadata)\n\ndf_train['img_path'] = df_train['Id'].apply(lambda x: f'..\/input\/petfinder-pawpularity-score\/train\/{str(x)}.jpg')\ndf_test['img_path'] = df_test['Id'].apply(lambda x: f'..\/input\/petfinder-pawpularity-score\/test\/{str(x)}.jpg')\n\ndf_train = create_shape_feature(df_train)\ndf_test = create_shape_feature(df_test)\n\nmetadata = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']","afecea04":"df_train.head()","d5687c4f":"fig = plt.figure(figsize = (12,12))\nax = fig.gca()\ndf_train.hist(ax=ax)\nplt.show()","38d7a953":"fig = plt.figure(figsize = (12,12))\nax = fig.gca()\ndf_test.hist(ax=ax)\nplt.show()","de6712f8":"fig, ax = plt.subplots(4, 3,figsize=(15,18))\n\ni = 0\nj = 0\n\nfor x in metadata:\n    sns.boxplot(x=x, y=\"Pawpularity\", data=df_train, ax=ax[i, j])\n    i+=1\n    if i > 3:\n        i = 0\n        j += 1","89254886":"corr = df_train.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nf, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","dbb50f95":"#df_train['Pawpularity_tgt'] = 100 - df_train['Pawpularity']","15d84d8b":"def rmse(y, yhat):\n    return np.sqrt(np.sum(np.power(y - yhat, 2)))","9db73451":"seed = 42\ndef train_and_optimize_lgb(p):\n    print(p)\n    params = {\n        'objective': 'rmse',\n        'boosting_type': 'gbdt',\n        'max_depth': p['max_depth'],\n        'max_bin':p['max_bin'],\n        'min_data_in_leaf': p['min_data_in_leaf'],\n        'learning_rate': p['learning_rate'],\n        'subsample': p['subsample'],\n        'subsample_freq': p['subsample_freq'],\n        'feature_fraction': p['feature_fraction'],\n        'lambda_l1': p['lambda_l1'],\n        'lambda_l2': p['lambda_l2'],\n        'seed':seed,\n        'feature_fraction_seed': seed,\n        'bagging_seed': seed,\n        'drop_seed': seed,\n        'data_random_seed': seed,\n        'n_jobs':-1,\n        'verbose': -1}\n    \n    features = metadata + ['width', 'height','file_size', 'area', 'size_per_pixel']\n    oof_predictions = np.zeros(df_train.shape[0])\n    kfold = KFold(n_splits = 4, random_state = seed, shuffle = True)\n    \n    # Iterate through each fold\n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(df_train)):\n        #print(f'Training fold {fold + 1}')\n\n        x_train, x_val = df_train[features].loc[trn_ind], df_train[features].loc[val_ind]\n        y_train, y_val = df_train['Pawpularity'].loc[trn_ind], df_train['Pawpularity'].loc[val_ind]\n\n        train_dataset = lgb.Dataset(x_train, y_train)\n        val_dataset = lgb.Dataset(x_val, y_val)\n        \n        model = lgb.train(params = params,\n                          num_boost_round=800,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          verbose_eval = -1,\n                          early_stopping_rounds=20)\n        \n        # Add predictions to the out of folds array\n        oof_predictions[val_ind] = model.predict(x_val)\n        print(rmse(df_train['Pawpularity'], oof_predictions))\n    return rmse(df_train['Pawpularity'], oof_predictions)\n\ndef make_predictions(p, kf_size=4):\n    params = {\n        'objective': 'rmse',\n        'boosting_type': 'gbdt',\n        'max_depth': int(p['max_depth']),\n        'max_bin':int(p['max_bin']),\n        'min_data_in_leaf': int(p['min_data_in_leaf']),\n        'learning_rate': p['learning_rate'],\n        'subsample': p['subsample'],\n        'subsample_freq': int(p['subsample_freq']),\n        'feature_fraction': p['feature_fraction'],\n        'lambda_l1': p['lambda_l1'],\n        'lambda_l2': p['lambda_l2'],\n        'seed':seed,\n        'feature_fraction_seed': seed,\n        'bagging_seed': seed,\n        'drop_seed': seed,\n        'data_random_seed': seed,\n        'n_jobs':-1,\n        'verbose': -1}\n    \n    features = metadata + ['width', 'height','file_size', 'area', 'size_per_pixel']\n    kfold = KFold(n_splits = kf_size, random_state = seed, shuffle = True)\n    pawpularity_test = np.zeros(df_test.shape[0])\n    \n    # Iterate through each fold\n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(df_train)):\n        x_train, x_val = df_train[features].loc[trn_ind], df_train[features].loc[val_ind]\n        y_train, y_val = df_train['Pawpularity'].loc[trn_ind], df_train['Pawpularity'].loc[val_ind]\n        \n        train_dataset = lgb.Dataset(x_train, y_train) \n        val_dataset = lgb.Dataset(x_val, y_val)\n        \n        model = lgb.train(params = params,\n                          num_boost_round=800,\n                          train_set = train_dataset,\n                          valid_sets = [train_dataset, val_dataset], \n                          verbose_eval = -1,\n                          early_stopping_rounds=20)\n        \n        pawpularity_test += model.predict(df_test[features])\/kf_size\n        \n    df_test['Pawpularity'] = pawpularity_test\n    df_test[['Id', 'Pawpularity']].to_csv('submission.csv', index=False)","e90cd0c6":"param_space = {\n    'max_depth': scope.int(hp.uniform('max_depth', 2, 8)),\n    'max_bin': scope.int(hp.uniform('max_bin', 2, 100)),\n    'min_data_in_leaf': scope.int(hp.uniform('min_data_in_leaf', 10, 1000)),\n    'learning_rate': hp.uniform('learning_rate',0.001,0.1),\n    'subsample': hp.uniform('subsample', 0.2, 0.9),\n    'subsample_freq': scope.int(hp.uniform('subsample_freq',1,30)),\n    'feature_fraction': hp.uniform('feature_fraction',0.5, 0.9),\n    'lambda_l1': hp.uniform('lambda_l1',0.1,3),\n    'lambda_l2': hp.uniform('lambda_l2',0.1,3)\n}\n\ntrials = Trials()\n\nhopt = fmin(fn = train_and_optimize_lgb, \n            space = param_space, \n            algo = tpe.suggest, \n            max_evals = 500, \n            trials = trials\n           )","1d097a43":"make_predictions(hopt, 4)","561bae6c":"In previous versions I used tweedie regression from LGBM but transforming the target variable so it matches a tweedie distribution. (Credits to the following discussion: https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/discussion\/274073). However it seems that for this particular problem using standard RMSE yield better results","33b1f708":"## \"Baselining\""}}