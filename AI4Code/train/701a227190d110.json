{"cell_type":{"e7879c0c":"code","179bcd01":"code","0d166992":"code","7ef2e5ca":"code","87b9ca61":"code","46cb7bbb":"code","1d383ae1":"code","3c0188b4":"code","7f5a0c22":"code","b9f2051a":"code","fe63e54f":"markdown","a5df1679":"markdown","b1492c1a":"markdown","13f1fe6b":"markdown","eed4f1b4":"markdown","b659706d":"markdown","97cfeb8d":"markdown","b9364447":"markdown","7468ae45":"markdown","f5df0c59":"markdown","4ae5b691":"markdown"},"source":{"e7879c0c":"# the only way to order your imports\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import RandomizedSearchCV, KFold\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.utils.fixes import loguniform\nfrom sklearn.metrics import roc_auc_score\nfrom imblearn.over_sampling import SMOTE\nfrom collections import OrderedDict\nfrom scipy import sparse\nimport pandas as pd\nimport numpy as np\nimport random","179bcd01":"random.seed(42)\nnp.random.seed(42)\ntrain = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ntest = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')\nanswers = np.load('..\/input\/job-change-dataset-answer\/jobchange_test_target_values.npy')","0d166992":"x_train = train.drop(['enrollee_id', 'target'], axis=1)\ny_train = train['target']\nx_test = test.drop(['enrollee_id'], axis=1)\n\ncats = [c for c in x_train.columns if train[c].dtypes =='object']\nnominals = ['city', 'gender', 'relevent_experience', 'enrolled_university', 'major_discipline', 'company_type'] # will be onehot encoded\nordinals = list(set(cats)-set(nominals))\nnums = [c for c in x_train.columns if train[c].dtypes !='object']\nprint(len(nominals),len(ordinals), len(nums))\n# # create mappings for categorical features\ngender = ['Female', 'Male', 'Other']\nrelevent_experience = ['No relevent experience', 'Has relevent experience']\nenrolled_university = ['no_enrollment', 'Full time course', 'Part time course']\neducation_level = ['Primary School', 'High School', 'Graduate', 'Masters', 'Phd']\nmajor_discipline = ['STEM', 'Business Degree', 'Arts', 'Humanities', 'No Major', 'Other']\nexperience = ['<1', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '>20']\ncompany_type = ['Pvt Ltd', 'Funded Startup', 'Early Stage Startup', 'Other', 'Public Sector', 'NGO']\ncompany_size = ['<10', '10\/49', '50-99', '100-500', '500-999', '1000-4999', '5000-9999', '10000+']\nlast_new_job = ['never', '1', '2', '3', '4', '>4']\nmapping = dict()\nfor key in cats:\n    if key == 'city': \n        continue\n    mapping[key] = OrderedDict()\n    for subkey, value in zip(eval(key),range(len(eval(key)))):\n        mapping[key][subkey] = value\n\ncity_label = LabelEncoder().fit(x_train['city'])\nfor df in [x_train, x_test]:\n    df[cats] = df[cats].apply(lambda x: x.map(mapping[x.name]) if x.name != 'city' else city_label.transform(x))\n\nnum_scaler = StandardScaler().fit(x_train[nums])\nordinals_scaler = StandardScaler().fit(x_train[ordinals])\n\n# Important: Imput train and test set separately. Otherwise information of the training set will leak into the test set.\n# ConvergenceWarning of IterativeImputer can be ignored. github.com\/scikit-learn\/scikit-learn\/issues\/14338\nfor df in [x_train, x_test]:\n    df[nums] = num_scaler.transform(df[nums])\n    df.iloc[:, :] = IterativeImputer(estimator=KNeighborsRegressor(),random_state=42).fit_transform(df)\n    df[cats] = df[cats].round(0).astype(int)\n    df[ordinals] = ordinals_scaler.transform(df[ordinals])\n    \n\n# Same OneHotEncoder for both sets, otherwise columns will not match \nenc = OneHotEncoder(sparse=False)\nenc.fit(x_train[nominals])\nx_train = sparse.csr_matrix(np.hstack([x_train[nums + ordinals].to_numpy(), enc.transform(x_train[nominals])]))\nx_test = sparse.csr_matrix(np.hstack([x_test[nums + ordinals].to_numpy(), enc.transform(x_test[nominals])]))","7ef2e5ca":"# Some so unfancy random parameter search \/\/  \nsmote = SMOTE(random_state=42)\nsearchspace = {\n    'C': loguniform(1e-3, 1e2),\n    'l1_ratio': np.random.default_rng().uniform(0,1,1000),\n}\n\nclass SMOTELogisticRegression(LogisticRegression):\n    \n    def __init__(self,C=1.0, l1_ratio=None, penalty='elasticnet', solver='saga', max_iter=1000, **kwargs):\n        super().__init__(C=C, l1_ratio=l1_ratio, penalty=penalty, solver=solver, max_iter=max_iter, **kwargs)\n        \n    def fit(self, X, y, sample_weight=None):\n        smote = SMOTE(random_state=42)\n        x_smote, y_smote = smote.fit_resample(X, y)\n        super().fit(x_smote, y_smote, sample_weight)\n\nrs = RandomizedSearchCV(estimator=SMOTELogisticRegression(),\n                        param_distributions=searchspace,\n                        n_iter=25,\n                        scoring='roc_auc',\n                        n_jobs=-1,\n                        refit=False,\n                        cv=5,\n                        verbose=3,\n                        random_state=42)\nrs.fit(x_train,y_train)\nprint(rs.best_params_)","87b9ca61":"# Refit of best params with correct resampling. Kundos to kaggle.com\/arashnic\/handling-imbalanced-resampling-the-right-way\/notebook\nfolds = KFold(n_splits=5, shuffle=True, random_state=42)\npred_cls = np.zeros(len(answers))\ntrain_auc = []\nval_auc = []\n\nfor train_idx, val_idx in folds.split(x_train,y_train):\n    x_smote, y_smote = SMOTE(random_state=42).fit_resample(x_train[train_idx],y_train[train_idx])\n    x_val, y_val = x_train[val_idx], y_train[val_idx]\n    cls = LogisticRegression(penalty='elasticnet', solver='saga', max_iter=1000, random_state=42, **rs.best_params_)\n    cls.fit(x_smote,y_smote)\n    train_auc.append(roc_auc_score(y_smote, cls.predict_proba(x_smote)[:,1]))\n    val_auc.append(roc_auc_score(y_val, cls.predict_proba(x_val)[:,1]))\n    pred_cls = cls.predict_proba(x_test)[:,1]\/folds.n_splits\n\nprint(f'Train auc: {np.mean(train_auc)}')\nprint(f'Val auc: {np.mean(val_auc)}')\nprint(f'Test auc: {roc_auc_score(answers, pred_cls)}')\nprint(f'Confusion for test data:')\nfor label, value in zip(['TN', 'FP', 'FN', 'TP'], confusion_matrix(answers,np.where(pred_cls > 0.5,1,0)).flatten()):\n    print(label, value)\n\n# submission\npd.DataFrame({'enrollee_id':test['enrollee_id'],'target':pred_cls}).to_csv('submission.csv',index=False)","46cb7bbb":"cls = LogisticRegression(penalty='elasticnet', solver='saga', max_iter=1000,random_state=42, **rs.best_params_)\ncls.fit(*SMOTE(random_state=42).fit_resample(x_train[train_idx],y_train[train_idx]))\nodd_ratios = np.round(np.exp(cls.coef_),3).flatten()","1d383ae1":"def print_changes_in_odds(features, odd_ratios, padding=30):\n    [print(x.ljust(padding),end='') for x in ['feature', 'influence on odds', 'change in %']]\n    print()\n    for k,v in zip(features, odd_ratios):\n        influence = 'negativ' if v < 1 else 'positiv'\n        [print(x.ljust(padding), end='') for x in [k,influence,f'{np.abs(1-v)*100:.2f}']]\n        print()","3c0188b4":"upper=len(nums)\nprint_changes_in_odds(nums, odd_ratios[:upper])","7f5a0c22":"lower = upper\nupper += len(ordinals)\nprint_changes_in_odds(ordinals,odd_ratios[lower:upper])","b9f2051a":"for nominal in nominals:\n    lower = upper\n    upper += city_label.classes_.shape[0] if nominal == 'city' else len(mapping[nominal])\n    print('#'*20, nominal, '#'*20)\n    if nominal == 'city':\n        # only use the highly influential cities\n        highly_influential_idx = np.where(np.abs(np.ones(len(range(lower,upper))) - odd_ratios[lower:upper]) > 1.1)\n        print_changes_in_odds(city_label.classes_[highly_influential_idx],odd_ratios[lower:upper][highly_influential_idx])\n    else:\n        print_changes_in_odds(mapping[nominal].keys(),odd_ratios[lower:upper])","fe63e54f":"# Preprocessing\nOrdinal: Order matters.  \nNominal: Order does not matter.  \n\n- LabelEncode categorical data\n- Scale numerical data\n- Impute missing data\n- Scale ordinal data\n- OneHotEncode nominal data","a5df1679":"## Final Interpretation\nNumerical:\n- Higher `city_development_index` => employee probably not looking for new job\n- Higher `training_hours` => employee probably not looking for new job\n\nOrdinal:\n- `comany_size` has no nearly no influence\n- Higher `experience` => employee probably not looking for new job\n- Higher `education_level` => employee probably looking for new job\n- Higher `last_new_job` (longer in same job) => employee probably looking for new job\n\nNominal (subset of most influential):\n- If employee works in city `100`, `103`, `160` or `21` => employee probably looking for job\n- If employee is `female` => employee probably not looking for job\n- If employee has `no relevent experience` => employee probably looking for job\n- If employee has `relevent experience` => employee probably not looking for job\n- If employee is enrolled in `full time course` at university  => employee probably looking for job\n- If employee majored in `STEM` => employee probably looking for job\n- If employee works for `Funded Startup` => employee probably looking for job\n- If employee works in `Other` => employee probably looking for job\n- If employee works for `NGO` => employee  probably not looking for job\n","b1492c1a":"# Prediction","13f1fe6b":"# Old but Interpretable: The Logit Model\n![](http:\/\/)The motivation for using the logit model \/ logistic regression lies in the interpretability of its coefficients as log odd ratios.\n\n**Recap odds:**  \nThe odds is a relative value between the chances of two mutual exlusive events. In our problem at hand these chances are the expected probabilies generated by the logit model. The odds are greater than one, if the probability of an employe looking for a new job given an expression of features is bigger than the probability of the counter event and vice versa.\n\n**Log odds ratio in the context of a logit model:**  \n$$\n\\exp \\left(\\hat{\\beta}_{i}\\right)=\\frac{\\frac{P\\left(y=1 \\mid x_{1}, \\ldots, x_{i}+1, \\ldots, x_{p}\\right)}{P\\left(y=0 \\mid x_{1}, \\ldots, x_{i}+1, \\ldots, x_{p}\\right)}}{\\frac{P\\left(y=1 \\mid x_{1}, \\ldots, x_{i}, \\ldots, x_{p}\\right)}{P\\left(y=0 \\mid x_{1}, \\ldots, x_{i}, \\ldots, x_{p}\\right)}}\n$$\n\nwith $\\hat{\\beta}_{i}$ beeing the i-th estimated coefficient and ${P\\left(Y \\mid X \\right)}$ beeing the probability output of our logistic model. With $Y$ as a random variable of the target and $X$ as vector of the random variables of the features.\n\n\n**Interpretation:**  \n- $\\exp \\left(\\hat{\\beta}_{i}\\right)$ < 1: Increase of feature $x_{i}$ by one unit decreases the odds \n- $\\exp \\left(\\hat{\\beta}_{i}\\right)$ > 1: Increase of feature $x_{i}$ by one unit increases the odds \n\nWhen $x_{i}$ is increased all other features remain constant.  \nThe increase of the expected probability is only relative and not linear, due to the non-linearity of our model's link function (sigmoid).","eed4f1b4":"# Statistical Inference\n\nOverfitting does not matter in this case. We like to have best possible estimates of the coefficients with respect to the data.\n\nEstimate coefficients with the whole upsampled dataset. No CV.","b659706d":"# Last Words\n\nThank you for the attention. Feel free to point out any errors.\n\nUnsolved Problem:  \nI would like to test of the coefficients of the logit model. Unfortunately the logit model of the statsmodels package could not compute the inverse of the Hessian (I have tried different solvers, increased max_iter and different initializations). In Maximum Likely Hood Estimation the negative inverse of the expectations of the Hessian are an approximation of the covariance matrix of the estimated coefficients. \n\nNo inverse Hessian => no variances of coefficents => no test statistics => no p-values => no test decisions => no tests \n\n:(\n\nI would appreciate your help!  \nYes there is not a single chart in this notebook. :p","97cfeb8d":"# Hyperparam Tuning\nExtend LogisticRegression for resampling inside the cross validation. ","b9364447":"### Nominal Features","7468ae45":"## Change In The Odds\n\nHow to read a negative\/positive percental change in the odds ratio?  \n\n**Structure of odds ratio:**   \nThe Odds with the feature increased by one unit are in the numerator and odds without increase are in the denominator.\n\n**Influence is positiv:**  \nAn increase of the feature by one unit\/level increases the odds by x percent.  \nOdds in the numerator are bigger than odds in the denominator.  \nThe nominator gets bigger if the probability for the event `not looking for job` decreases or if the probability for `looking for a job` increases.\n\n**Influence is negativ:**  \nA increase of the feature by one unit\/level decreases the odds by x percent.  \nOdds in the numerator are smaller than odds in the denominator.  \nThe nominator gets smaller if the probability for the event `not looking for job` increases or if the probability for `looking for a job` decreases.  \n\n\n### Numerical Features","f5df0c59":"### Ordinal Features: ","4ae5b691":"## Prediction Results \nOverfitting. Bad generalization. Lots of Type-II errors.\nWell it is a logistic model. If you have any tips for improvements, I am happy to read them in the comment section."}}