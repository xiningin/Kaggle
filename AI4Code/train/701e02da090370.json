{"cell_type":{"a6cd66c2":"code","1bb3a920":"code","d40c1b96":"code","1613c321":"code","ce4130f3":"code","af268372":"code","d80bda06":"code","cdc51864":"code","2ff2018a":"code","61431916":"code","c1b3ba00":"code","d74170b8":"code","c9cdc5b4":"code","94135f18":"code","427f0177":"code","48239768":"code","eae99918":"code","2622e25d":"code","de5f09bf":"code","33455307":"code","59cd7415":"code","08a034ef":"code","7f78c0b2":"code","94d99891":"code","03c7a46a":"code","381ca84e":"code","cbeff2d5":"code","22e3c4ae":"code","e61fa5dc":"code","138bf4bc":"markdown","6a85feac":"markdown","635549f9":"markdown","6f7c4b64":"markdown","2c097274":"markdown","b5c05266":"markdown","38178a9e":"markdown","8340ed5f":"markdown","47683a22":"markdown","5b07249d":"markdown","a67c67c4":"markdown","a7805e7e":"markdown","446511d2":"markdown","4139178c":"markdown","5f119cc1":"markdown"},"source":{"a6cd66c2":"import pandas as pd\nimport numpy as np\nimport datetime\n%matplotlib notebook\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport seaborn as sns\n%matplotlib inline\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport string\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error","1bb3a920":"df_e = pd.read_csv('..\/input\/hourly-energy-consumption\/PJME_hourly.csv')\ndf_w = pd.read_csv('..\/input\/hourly-energy-consumption\/PJMW_hourly.csv')","d40c1b96":"df_e.shape","1613c321":"df_e['Datetime'] =  pd.to_datetime(df_e['Datetime'])\ndf_w['Datetime'] =  pd.to_datetime(df_w['Datetime'])\n","ce4130f3":"df =  pd.merge(df_e, df_w,  how = 'left', on= 'Datetime') #","af268372":"df.head()","d80bda06":"df['PJMW_MW'] = df['PJMW_MW'].interpolate(method='linear', axis=0).ffill()","cdc51864":"df['Energy'] = df['PJME_MW'] + df['PJMW_MW']\ndf_ = df.copy()\ndf = df.drop(['PJME_MW','PJMW_MW'], axis = 1)","2ff2018a":"df.set_index('Datetime', inplace = True) #make the index is out date time","61431916":"df.head().append(df.tail())","c1b3ba00":"# Plot time series data\ndf.plot(y=[\"Energy\"], figsize=(15,4))","d74170b8":"df[[\"Energy\"]].resample(\"1w\").median().plot(figsize=(15,4))","c9cdc5b4":"df[[\"Energy\"]].resample(\"1m\").median().plot(figsize=(15,4))","94135f18":"def test_stationarity(df, ts):\n    \"\"\"\n    Test stationarity using moving average statistics and Dickey-Fuller test\n    Source: https:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/time-series-forecasting-codes-python\/\n    \"\"\"\n    \n    # Determing rolling statistics\n    rolmean = df[ts].rolling(window = 12, center = False).mean()\n    rolstd = df[ts].rolling(window = 12, center = False).std()\n    \n    # Plot rolling statistics:\n    orig = plt.plot(df[ts], \n                    color = 'blue', \n                    label = 'Original')\n    mean = plt.plot(rolmean, \n                    color = 'red', \n                    label = 'Rolling Mean')\n    std = plt.plot(rolstd, \n                   color = 'black', \n                   label = 'Rolling Std')\n    plt.legend(loc = 'best')\n    plt.title('Rolling Mean & Standard Deviation for %s' %(ts))\n    plt.xticks(rotation = 45)\n    plt.show(block = False)\n    plt.close()\n    \n    # Perform Dickey-Fuller test:\n    # Null Hypothesis (H_0): time series is not stationary\n    # Alternate Hypothesis (H_1): time series is stationary\n    print ('Results of Dickey-Fuller Test:')\n    dftest = adfuller(df[ts], \n                      autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], \n                         index = ['Test Statistic',\n                                  'p-value',\n                                  '# Lags Used',\n                                  'Number of Observations Used'])\n    for key, value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)","427f0177":"test_stationarity(df = df, ts = 'Energy')","48239768":"!pip install pystan\n!pip install fbprophet\nfrom fbprophet import Prophet\nimport datetime\nfrom datetime import datetime","eae99918":"split_date = '2017-01-01'\ndf_train = df.loc[df.index <= split_date].copy()\ndf_test = df.loc[df.index > split_date].copy()","2622e25d":"# Plot train and test so you can see where we have split\ndf_test \\\n    .rename(columns={'Energy': 'TEST SET'}) \\\n    .join(df_train.rename(columns={'Energy': 'TRAINING SET'}),\n          how='outer') \\\n    .plot(figsize=(15,5), title='PJM Total', style='.')\nplt.show()","de5f09bf":"# get relevant data - note: could also try this with ts_log_diff\ndf_prophet = df[['Energy']] # can try with ts_log_diff\n\n# reset index\ndf_prophet = df_prophet.reset_index()\n\n# rename columns\ndf_prophet = df_prophet.rename(columns = {'Datetime': 'ds', 'Energy': 'y'}) # can try with ts_log_diff, this names are set to be named like this\n\n# Change 'ds' type from datetime to date (necessary for FB Prophet)\ndf_prophet['ds'] = pd.to_datetime(df_prophet['ds'])\n\n# Change 'y' type to numeric (necessary for FB Prophet)\ndf_prophet['y'] = pd.to_numeric(df_prophet['y'], errors='ignore')\n","33455307":"model = Prophet(growth = 'linear',daily_seasonality = True)\nmodel.fit(df_prophet)","59cd7415":"# Predict on training set with model\ndf_test_fcst = model.predict(df=df_test.reset_index() \\\n                                   .rename(columns={'Datetime':'ds'}))\n#we could have also used the function:\n#future = m.make_future_dataframe(period = 365), this would have forecasted 1 year into the future","08a034ef":"# Plot the forecast\nf, ax = plt.subplots(1)\nf.set_figheight(5)\nf.set_figwidth(15)\nfig = model.plot(df_test_fcst,\n                 ax=ax)\nplt.show()","7f78c0b2":"# Plot the components of the model\nfig = model.plot_components(df_test_fcst)","94d99891":"df_test_fcst.head().append(df_test.tail())","03c7a46a":"import datetime\n# Plot Yearly forecasts\nf, ax = plt.subplots(1)\nf.set_figheight(5)\nf.set_figwidth(15)\nax.scatter(df_test.index, df_test['Energy'], color='r')\nax.set_xlim([datetime.datetime(2017, 1, 1), datetime.datetime(2018, 1, 1)])\nfig = model.plot(df_test_fcst, ax=ax)\nplot = plt.suptitle('Yeary Prediction')\n\n\n","381ca84e":"# Plot monthly forecasts (last month)\nf, ax = plt.subplots(1)\nf.set_figheight(5)\nf.set_figwidth(15)\nax.scatter(df_test.index, df_test['Energy'], color='r')\nax.set_xlim([datetime.date(2017, 12, 1), datetime.date(2017, 12, 31)])\nfig = model.plot(df_test_fcst, ax=ax)\nplot = plt.suptitle('December Prediction')\n\n","cbeff2d5":"def mean_absolute_percentage_error(y_true, y_pred): \n    \"\"\"Calculates MAPE given y_true and y_pred\"\"\"\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","22e3c4ae":"\nMSE = mean_squared_error(y_true=df_test['Energy'],\n                   y_pred=df_test_fcst['yhat'])\n\nMAE = mean_absolute_error(y_true=df_test['Energy'],\n                   y_pred=df_test_fcst['yhat'])\n\nMAPE = mean_absolute_percentage_error(y_true=df_test['Energy'],\n                   y_pred=df_test_fcst['yhat'])\n\nprint('MSE:'+' '+ str(MSE))\nprint('MAE:' +' '+str(MAE))\nprint('MAPE:' +' '+ str(MAPE) + '%')","e61fa5dc":"df.to_csv('Energy_PJM.csv',index=True)\ndf_test_fcst.to_csv('Prophet_Forecast.csv', index = True)","138bf4bc":"In blue we can see the prophet forecast for the year 2017 going into 2018 (our testing dataset). One of the many advantages of forecasting with prophet is that it can decompose the forecast into:\n\n- trend\n- yearly\n- weekly\n- daily","6a85feac":"Prophet expects a dataframe where the Dates are named 'ds' and the target value (in this case, Energy) is named 'y' so we rename the columns of our dataset.","635549f9":"I first have to create my training set and my testing set. I will train the models on prior data to last year, which will be forecasted","6f7c4b64":"Here we can see some interesting trends when it comes to the data, and the data starts to reveal itself. We can see a peak energy consumption from Tuesday to Friday, decreasing a little bit on Saturday. \n\nThe yearly trend is clear, being end of July and August peak energy consumption by the grid.\n\nDaily data reveals a starting point in energy past 4:00 AM going into a peak energy consumption by 20:00.\n\nNow we are going to calculate the accuracy of the model forecasted.","2c097274":"# Error measuring","b5c05266":"Whats good about forecasting with prophet (if youre a python user familiar with sklearn) is that they followed a similar synthax when it comed to choose the model and fit the data.","38178a9e":"# Modelling with prophet\n\nTimeseries modelling can take two approaches, leveraging the timeserie aspect of the data using statistical model such as ARIMA (where past data affects future data) or take an approach of linear regression where we forget about the time dependency nature and try to fit the best curve to the dataset.\n\nFacebook developed this tool (https:\/\/facebook.github.io\/prophet\/) to make the forecasting problem simpler to the daily analyst without having to do heavy researvh on statistichal modelling techniques such as Autoregressive models or more sophisticated ones such as ARIMA.\n\nThis tool  allows forecatsing using additive or component models relatively easily. It can also include things like:\n\n- Day of week effects\n- Day of year effects\n- Holiday effects\n- Trend trajectory\n- Can do MCMC sampling","8340ed5f":"# TimeSeries preprocessing\n\nWe make sure our data is a univariate time series to prepare it for EDA and future analysis.","47683a22":"# Project Overview\n\n**PJM Hourly Energy Consumption Data**\n\nPJM Interconnection LLC (PJM) is a regional transmission organization (RTO) in the United States. It is part of the Eastern Interconnection grid operating an electric transmission system serving all or parts of Delaware, Illinois, Indiana, Kentucky, Maryland, Michigan, New Jersey, North Carolina, Ohio, Pennsylvania, Tennessee, Virginia, West Virginia, and the District of Columbia.\n\nThe hourly power consumption data comes from PJM's website and are in megawatts (MW).\n\nThe regions have changed over the years so data may only appear for certain dates per region.\n\n**Modelling Techniques:**\n\n**Statistical models:**\n\n- Ignore the time-series aspect completely and model using traditional statistical modeling toolbox. \n\n    *Examples: Regression-based models (prophet from facebook)*\n    \n    \n- Univariate statistical time-series modeling.\n\n    *Examples: Averaging and smoothing models, ARIMA models.*\n\n**Machine learning models:**\n\n- Ignore the time-series aspect completely and model using traditional machine learning modeling toolbox.\n       *Examples. Support Vector Machines (SVMs), Random Forest Regression, Gradient-Boosted Decision Trees (GBDTs).*\n\n- Recurrent neural networks (RNNs)\n\nFor this project we are going to focus on the scope on comparing the forecasting performances of these two types of modelling techniques with further recommendations for the dataset. We are not going to include external data information.","5b07249d":"we make the datetime the index colum to join the dataframes later","a67c67c4":"# Plotting the data\n\nNow that we made sure our timeseries has no NaN values (interpolation with forward fill to avoid lookout) and its in the correct format, we proceed to visualize it.","a7805e7e":"We now save the dataframe into a csv file to work it with another forecasting model","446511d2":"We failed the reject the hypothesis of the Augmented Dickey fuller test so we confirm that the Energy consumption is stationary with more than 99% certainty.","4139178c":"Here we see 3 plots of the same timeserie of the Energy consumption (East and West) resampled in minutes, weeks and months. This was made to see variance and first signs of stationarity.\n\nthe .resample() method has given us a lower cadence dataset without all the noise. We still can see there are high spikes in energy consumption over some weeks.\n\nSimilar to this method is the rolling window calculations, which return the same cadence of data but performed over a rolling window of a given width. We can summon this calculation using the .rolling() method.\n\nWe can see that our data doesnt follow any trend and is hard to appreciate its stationarity.\n\nStationarity is a statistical assumption that a time-series has:\n\n1. Constant mean\n2. Constant variance\n3. Autocovariance does not depend on time\n\nIf we are using past data to predict future data, we should assume that the data will follow the same general trends and patterns as in the past. This general statement holds for most training data and modeling tasks.\n\nWe extracted a script of code from https:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/time-series-forecasting-codes-python\/ to analyze stationarity of our dataset performing an augmented dickey-fuller test. This test asumes the hypothesis that our data is stationary and we have to prove its not.","5f119cc1":"We can appreciate the errors made when we disect and visualize December 2017 energy consumption forecast."}}