{"cell_type":{"f5422919":"code","76583002":"code","177f3a00":"code","33eaae4e":"code","130b00d4":"code","4b2fd561":"code","22ce307e":"code","3007c9ee":"code","7e5e187a":"code","b3ca4dad":"code","a3e60de0":"code","71d5b253":"code","bc2e923c":"code","d14155d1":"code","0f50c371":"code","92cf4cd6":"code","86fae239":"code","21d9c275":"code","edaa4543":"code","c4193684":"markdown","0449b41e":"markdown","f96227da":"markdown","84a44b6d":"markdown","0b080bff":"markdown","896bc9ea":"markdown","91a6aee7":"markdown","e09e3408":"markdown","49ccdd00":"markdown"},"source":{"f5422919":"#importing all the libraries.\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns","76583002":"#importing and reaading the dataset\ndataset = pd.read_csv(\"..\/input\/iris-data\/Iris.csv\")","177f3a00":"#the first five values in the dataset\ndataset.head()","33eaae4e":"#getting all the unique values in SepalLengthCm\ndataset[\"SepalLengthCm\"].unique()","130b00d4":"##getting all the unique values in SepalWidthCm\ndataset[\"SepalWidthCm\"].unique()","4b2fd561":"#number of rows and columns\ndataset.shape","22ce307e":"#Number of species in the dataset\ndataset.groupby([\"Species\"]).count()","3007c9ee":"dataset.describe()","7e5e187a":"sns.countplot(x='Species',data=dataset)\nplt.title('Species',fontsize=20)\nplt.show()","b3ca4dad":"dataset.plot(kind =\"scatter\", \n          x ='SepalLengthCm', \n          y ='PetalLengthCm') \nplt.grid()","a3e60de0":"sns.scatterplot(x=dataset[\"SepalLengthCm\"], y=dataset[\"SepalWidthCm\"], hue=dataset[\"Species\"])\nplt.show()","71d5b253":"sns.scatterplot(x=dataset[\"PetalLengthCm\"], y=dataset[\"PetalWidthCm\"], hue=dataset[\"Species\"])\nplt.show()","bc2e923c":"#Taking values except for \"id\" and \"Species\"\nX = dataset.iloc[:, [1,2,3,4]].values\nprint(X)","d14155d1":"#k-means clustering\nfrom sklearn.cluster import KMeans\nwcss = []\nfor i in range(1, 15):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1, 15), wcss)\nplt.title(' Elbow Method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\nplt.show()","0f50c371":"kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 42)\ny_kmeans = kmeans.fit_predict(X)\nprint(y_kmeans)","92cf4cd6":"plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], \n            s = 100, c = 'magenta', label = 'Iris-setosa')\nplt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], \n            s = 100, c = 'cyan', label = 'Iris-versicolour')\nplt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1],\n            s = 100, c = 'orange', label = 'Iris-virginica')\n\n# Plotting the centroids of the clusters\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], \n            s = 200, c = 'red', label = 'Centroids')\nplt.title('Clusters of Iris Species')\nplt.xlabel('sepal length')\nplt.ylabel('sepal width')\nplt.legend()\nplt.show()","86fae239":"#Hierarchical Clustering\nimport scipy.cluster.hierarchy as sch\ndendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\nplt.title('Dendrogram')\nplt.xlabel('Species')\nplt.ylabel('Euclidean distances')\nplt.show()","21d9c275":"#AgglomerativeClustering\nfrom sklearn.cluster import AgglomerativeClustering\nhc = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'ward')\ny_hc = hc.fit_predict(X)","edaa4543":"plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], \n            s = 100, c = 'red', label = 'Iris-setosa')\nplt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], \n            s = 100, c = 'blue', label = 'Iris-versicolour')\nplt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1],\n            s = 100, c = 'orange', label = 'Iris-virginica')\n\nplt.title('Clusters of Iris Species')\nplt.xlabel('sepal length')\nplt.ylabel('sepal width')\nplt.legend()\nplt.show()","c4193684":"## Conclusion\n### We were able to predict the optimum number of clusters and represent it visually from the given \u2018Iris\u2019 dataset.","0449b41e":"## Using the elbow method to find the optimal number of clusters","f96227da":"## Using the dendrogram to find the optimal number of clusters","84a44b6d":"## Visualising the clusters","0b080bff":"## Visualization","896bc9ea":"## Predict the optimum number of clusters and represent it visually.","91a6aee7":"## Visualising the clusters","e09e3408":"## Training the Hierarchical Clustering model on the dataset","49ccdd00":"## Training the K-Means model on the dataset"}}