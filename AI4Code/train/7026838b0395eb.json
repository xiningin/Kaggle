{"cell_type":{"00afe17b":"code","09d52350":"code","63d9d627":"code","0d4aeb48":"code","e87aeaf6":"code","4cb22d9d":"code","62d25ace":"code","43b15d8b":"code","6bd19d0d":"code","88658cdf":"code","35e3a172":"code","95d3ee66":"code","027ac258":"code","45dc9620":"code","18954a28":"code","a228dd85":"code","7228e5d5":"code","7aaaf6a5":"code","6fb4ee08":"code","4e690b76":"markdown"},"source":{"00afe17b":"%%html\n<marquee style='width: 50%; color: red;'><b>\u0412\u0441\u0435\u043c \u043f\u0440\u0438\u0432\u0435\u0442!!!!! \u0420\u0430\u0434 \u0432\u0430\u0441 \u0442\u0443\u0442 \u0432\u0438\u0434\u0435\u0442\u044c \u041e\u0446\u0435\u043d\u0438\u0442\u0435 \u043d\u043e\u0443\u0442<\/b><\/marquee>","09d52350":"# Author: Timur Abdualimov, SOVIET team\n# Competition: Recommended system, SkillFctory\n# First date code: 17.05.2020\n# Used: Kaggle notebook, GPU!\n\n\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\nimport matplotlib\nfrom matplotlib.pyplot import figure\n\n%matplotlib inline\nmatplotlib.rcParams['figure.figsize'] = (12,8)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, accuracy_score, f1_score, precision_score, recall_score\n\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential, Model\nfrom keras.layers import Embedding, Flatten, Dense, Dropout, concatenate, multiply, Input, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras import metrics\nfrom keras.utils.vis_utils import plot_model\n\nfrom keras import backend as K\ndef root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n\nimport scikitplot as skplt\n\nimport sys\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n    \nRANDOM_SEED = 13\n\nprint('Python       :', sys.version.split('\\n')[0])\nprint('Pandas       :', pd.__version__)\nprint('Numpy        :', np.__version__)\nprint('Keras        :', keras.__version__)","63d9d627":"def open_data():\n    \"\"\" open datasets\"\"\"\n    global train, test, sample_submission # \u043e\u0431\u044a\u044f\u0432\u043b\u044f\u0435\u043c \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u044b\u043c\u0438\n    train = pd.read_csv('\/kaggle\/input\/recommendationsv4\/train.csv', low_memory = False)\n    train = train.drop_duplicates().reset_index(drop = True) # \u0443\u0434\u0430\u043b\u0438\u043c \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b, \u0435\u0441\u043b\u0438 \u0435\u0441\u0442\u044c\n    test = pd.read_csv('\/kaggle\/input\/recommendationsv4\/test_v3.csv', low_memory = False)\n    sample_submission = pd.read_csv('\/kaggle\/input\/recommendationsv4\/sample_submission.csv')\n    \nopen_data() # \u043e\u0442\u043a\u0440\u044b\u0432\u0430\u0435\u043c \u0432\u0441\u0435 \u0438 \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0432 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435\n\ndef param_data(data): # \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0435\n    \"\"\"dataset required parameters \"\"\"\n    param = pd.DataFrame({\n              'dtypes': data.dtypes.values,\n              'nunique': data.nunique().values,\n              'isna': data.isna().sum().values,\n              'loc[0]': data.loc[0].values,\n              }, \n             index = data.loc[0].index)\n    return param\n\npd.concat([param_data(train), param_data(test)], \n          axis=1, \n          keys = [f'\u2193 \u041e\u0411\u0423\u0427\u0410\u042e\u0429\u0410\u042f \u0412\u042b\u0411\u041e\u0420\u041a\u0410 \u2193 {train.shape}', f'\u2193 \u0422\u0415\u0421\u0422\u041e\u0412\u0410\u042f \u0412\u042b\u0411\u041e\u0420\u041a\u0410 \u2193 {test.shape}'],  \n          sort=False)","0d4aeb48":"def viz_na(data):\n    \"\"\"NA visualisation\"\"\"\n    global cols\n    cols = data.columns # \u0437\u0430\u043f\u0438\u0448\u0435\u043c \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u0441\u0442\u0440\u043e\u043a\u0438 \u0441\u0434\u0435\u043b\u0430\u0435\u043c \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0443\u044e \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u043e\u0439\n    # \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u0446\u0432\u0435\u0442\u0430 \n    # \u0436\u0435\u043b\u0442\u044b\u0439 - \u043f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435, \u0441\u0438\u043d\u0438\u0439 - \u043d\u0435 \u043f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u0435\n    colours = ['#000099', '#ffff00'] \n    sns.heatmap(data[cols].isnull(), cmap=sns.color_palette(colours))\n    plt.show()\n\n\nviz_na(train)\nviz_na(test)","e87aeaf6":"def stat_na_per_percent(data):\n    print(f'{data.shape}')\n    for col in data.columns:\n        pct_missing = np.mean(data[col].isnull())\n        print('{} - {}%'.format(col, round(pct_missing*100)))\n    print(\"END\", end = '\\n\\n')\nstat_na_per_percent(train)\nstat_na_per_percent(test)","4cb22d9d":"sns.countplot(train['rating']);","62d25ace":"sns.heatmap(train.corr())\nplt.show()","43b15d8b":"train_data_GB = pd.DataFrame({\n    'userid': train['userid'],\n    'itemid': train['itemid'],\n    'rating': train['rating']\n})\n\ntest_data_GB = pd.DataFrame({\n    'userid': test['userid'],\n    'itemid': test['itemid'],\n})\n\n#train_data_GB = pd.get_dummies(train_data_GB, prefix='', prefix_sep='', columns=['rating'])","6bd19d0d":"#train_data_GB['rating'] = train_data_GB['rating'].astype(int)\n## unisue users, books\nuserid, utemid = len(train_data_GB.userid.unique()), len(train_data_GB.itemid.unique())\n\n\nf'The dataset includes {len(train_data_GB)} ratings by {userid} unique users on {utemid} unique itemid.'","88658cdf":"train_, test_ = train_test_split(train_data_GB, test_size=0.01)\nf\"The training and testing data include {len(train), len(test)} records.\"","35e3a172":"keras.backend.clear_session()\n\nlatent_dim = 60\n\n# Define inputs\nutem_input = Input(shape=[1],name='utem-input')\nuser_input = Input(shape=[1], name='user-input')\n\n# MLP Embeddings\nutem_embedding_mlp = Embedding(utemid + 1, latent_dim, name='utem-embedding-mlp')(utem_input)\nutem_vec_mlp = Flatten(name='flatten-utem-mlp')(utem_embedding_mlp)\n\nuser_embedding_mlp = Embedding(userid + 1, latent_dim, name='user-embedding-mlp')(user_input)\nuser_vec_mlp = Flatten(name='flatten-user-mlp')(user_embedding_mlp)\n\n# MF Embeddings\nutem_embedding_mf = Embedding(utemid + 1, latent_dim, name='utem-embedding-mf')(utem_input)\nutem_vec_mf = Flatten(name='flatten-movie-mf')(utem_embedding_mf)\n\nuser_embedding_mf = Embedding(userid + 1, latent_dim, name='user-embedding-mf')(user_input)\nuser_vec_mf = Flatten(name='flatten-user-mf')(user_embedding_mf)\n\n# MLP layers\nconcat = concatenate([utem_vec_mlp, user_vec_mlp], name='concat')\nconcat_dropout = Dropout(0.2)(concat)\nfc_1 = Dense(100, name='fc-1', activation='relu')(concat_dropout)\nfc_1_bn = BatchNormalization(name='batch-norm-1')(fc_1)\nfc_1_dropout = Dropout(0.2)(fc_1_bn)\nfc_2 = Dense(50, name='fc-2', activation='relu')(fc_1_dropout)\nfc_2_bn = BatchNormalization(name='batch-norm-2')(fc_2)\nfc_2_dropout = Dropout(0.2)(fc_2_bn)\n\n# Prediction from both layers\npred_mlp = Dense(10, name='pred-mlp', activation='relu')(fc_2_dropout)\npred_mf = concatenate([utem_vec_mf, user_vec_mf], name='pred-mf')\ncombine_mlp_mf = concatenate([pred_mf, pred_mlp], name='combine-mlp-mf')\n\n# Final prediction\nresult = Dense(1, name='result1', activation='relu')(combine_mlp_mf)\n\nmodel_mlp = Model([user_input, utem_input], result)\nmodel_mlp.summary()","95d3ee66":"#plot_model(model_mlp, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","027ac258":"## specify learning rate (or use the default by specifying optimizer = 'adam')\nopt_adam = Adam(lr = 0.0005)\n\n## compile model\nmodel_mlp.compile(optimizer= opt_adam, loss= ['mse'], metrics=[keras.metrics.AUC()])\n\n## fit model\nhistory_tabular = model_mlp.fit([train_['userid'], train_['itemid']],\n                                    train_['rating'],\n                                    batch_size = 1000,\n                                    validation_split = 0.1,\n                                    epochs = 4,\n                                    verbose = 1)","45dc9620":"test_pred_GB = model_mlp.predict([test_data_GB['userid'], test_data_GB['itemid']])[:,1]","18954a28":"test_pred = model_mlp.predict([test_['userid'], test_['itemid']])","a228dd85":"skplt.metrics.plot_roc(test_['rating'], test_pred)\nplt.show()","7228e5d5":"fpr, tpr, _ = roc_curve(test_.iloc[:, -1], test_pred[:,1])\nauc = roc_auc_score(test_.iloc[:,  -1], test_pred[:, 1])\naccuracy = accuracy_score(test_.iloc[:, -1], np.argmax(test_pred, axis = -1))\nf1 = f1_score(test_.iloc[:, -1], np.argmax(test_pred, axis = -1))\nprecision =  precision_score(test_.iloc[:, -1], np.argmax(test_pred, axis = -1))\nrecall = recall_score(test_.iloc[:, -1], np.argmax(test_pred, axis = -1))\n\nprint('FP, TP              :', sum(fpr), sum(tpr))\nprint('ROC_AUC_SCORE       :', auc)\nprint('accuracy            :', accuracy)\nprint('precision           :', precision)\nprint('recall              :', recall)\nprint('f1                  :', f1)","7aaaf6a5":"sample_submission['rating'] = test_pred_GB\nsample_submission.to_csv('submission_XXXXXX.csv', index=False)\nsample_submission.head(3)","6fb4ee08":"#\u042d\u043c\u0431\u0435\u0434\u0438\u043d\u0433\u0438\nutem_embedding_weights = model_mlp.layers[2].get_weights()[0]","4e690b76":"### Tabular data method"}}