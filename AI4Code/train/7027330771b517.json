{"cell_type":{"374c695c":"code","8e2a89bc":"code","4c2e503a":"code","e0c53051":"code","c0d7ad20":"code","3c2d2414":"code","66eefada":"code","fbfcd49a":"code","2bed5083":"code","b523899e":"code","4ecb76a7":"code","e69564ed":"code","2eb1e664":"code","01aaaa59":"code","dad05c6b":"code","32cf32a2":"code","4b96d538":"code","f9a4e4f4":"code","09e75216":"code","747e7b34":"code","4354201d":"code","19abd3e7":"code","6fd501e2":"code","600d7348":"code","ce4e1871":"code","d6a81b71":"code","5f5a5b79":"code","66e50ec8":"code","a00fae24":"code","3a5f8637":"code","d5d957b0":"code","11364595":"code","a6f1243a":"code","c74f683b":"code","bc5eabd6":"code","ebc84859":"code","26b579de":"code","2b3700d7":"code","fe498bfe":"code","d0c2a8fb":"code","09e2b3a6":"code","11b5e54b":"code","00e97633":"code","1f886abb":"code","73a034b0":"code","a0c7489d":"code","071b44ba":"code","83be66fc":"code","6b9af9ae":"code","291e9754":"code","9014bdb3":"code","7b8030cc":"code","e9278385":"code","93ed32bb":"code","64f42411":"code","1589f555":"code","8233749a":"code","ed3f1e04":"code","03a4846e":"code","d8f9630d":"code","873db550":"code","a1605023":"code","20014a8e":"code","037670d2":"code","128adb77":"code","1f201401":"code","9fa2e873":"code","770d0def":"code","7bfb8deb":"code","4dcd1cfa":"code","c8c49d46":"code","11a52ca1":"code","8b21ac60":"code","4a16d74f":"code","402c34cc":"code","4b580fdc":"code","e6374acd":"code","6139a8f8":"code","7be4f065":"code","02028e0f":"code","01fb3089":"code","6606f9b4":"code","a33bbc10":"code","ecae2184":"code","82466d1d":"code","5e293b0b":"markdown","86d09835":"markdown","2c6f7da8":"markdown","d754b12c":"markdown","9413101c":"markdown","6d2549cc":"markdown","2b3eba5f":"markdown","0fd674c8":"markdown","ecf453d9":"markdown","a4afe8de":"markdown","5940b95a":"markdown","8ed0464c":"markdown","f0b80c5a":"markdown","ee445d6a":"markdown","221d7297":"markdown","74ddd034":"markdown","e0093f74":"markdown","fccef287":"markdown","3c1296a1":"markdown","f8f1c08b":"markdown","d5436251":"markdown","3374e504":"markdown","89936f45":"markdown","85cc1072":"markdown","30f90ea6":"markdown","36793e30":"markdown"},"source":{"374c695c":"import numpy as np\nimport pandas as pd","8e2a89bc":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4c2e503a":"# visualising Data\ndf = pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv',encoding=\"ISO-8859-1\")\ndf.head()","e0c53051":"# Checking the shape of the data\ndf.shape","c0d7ad20":"# Checking the information of the data\ndf.info()","3c2d2414":"df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)","66eefada":"df.sample(5)","fbfcd49a":"df.rename(columns={'v1':'target','v2':'text'},inplace=True)\ndf.sample(5)","2bed5083":"df['target'].unique()","b523899e":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()","4ecb76a7":"#converting categorical to numerical\ndf['target'] = encoder.fit_transform(df['target'])\ndf.head()","e69564ed":"df.isnull().sum()","2eb1e664":"#Checking info of the data set\ndf.info()","01aaaa59":"# Checking Duplicates in the data set\ndf.duplicated().sum()","dad05c6b":"# Removing duplicated data set\ndf = df.drop_duplicates(keep='first')","32cf32a2":"#Checking duplicate values\ndf.duplicated().sum()","4b96d538":"# Checking the value counts in the data set\ndf['target'].value_counts()","f9a4e4f4":"# Visualising\nimport matplotlib.pyplot as plt\nplt.pie(df['target'].value_counts(), labels=['ham','spam'], autopct=\"%0.2f\")\nplt.show()","09e75216":"import nltk\nnltk.download('punkt')","747e7b34":"# Checking the length of the words and creating a new column\ndf['num_characters'] = df['text'].apply(len)","4354201d":"#Tokenization\ndf['num_words'] = df['text'].apply(lambda x: len(nltk.word_tokenize(x)))","19abd3e7":"# Creating a seprate list for visualising words\ndf['num_sentences'] = df['text'].apply(lambda x: len(nltk.sent_tokenize(x)))","6fd501e2":"df.head()","600d7348":"# Checking the statistics on newly created column\ndf[['num_characters','num_words','num_sentences']].describe()","ce4e1871":"## Statistics on 'ham' and 'spam' ","d6a81b71":"# Checking ham on newly created columns\ndf[df['target'] == 0][['num_characters','num_words','num_sentences']].describe()","5f5a5b79":"# Checking on spam\ndf[df['target'] == 1][['num_characters','num_words','num_sentences']].describe()","66e50ec8":"# Visualising \nimport seaborn as sns\nplt.figure(figsize=(12,8))\nsns.histplot(df[df['target'] == 0]['num_characters'])\nsns.histplot(df[df['target'] == 1]['num_characters'],color = 'red')\nplt.show()","a00fae24":"# Checking no-words\nplt.figure(figsize=(12,8))\nsns.histplot(df[df['target'] == 0]['num_words'])\nsns.histplot(df[df['target'] == 1]['num_words'],color = 'red')\nplt.show()","3a5f8637":"sns.pairplot(df,hue='target')\nplt.show()","d5d957b0":"sns.heatmap(df.corr(),annot=True)\nplt.show()","11364595":"# For stemming\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\nps.stem('loving')","a6f1243a":"# Visualisng stop words\nfrom nltk.corpus import stopwords\nstopwords.words('english')","c74f683b":"# Visualising Punctuvation marks\nimport string\nstring.punctuation","bc5eabd6":"def transform_text(text):\n    text = text.lower()\n    text = nltk.word_tokenize(text)\n    \n    y = []\n    for i in text:\n        if i.isalnum():\n            y.append(i)\n    \n    \n    text = y[:]\n    y.clear()\n    for i in text:\n        if i not in stopwords.words('english') and i  not in string.punctuation:\n            y.append(i)\n            \n    text = y[:]\n    y.clear()\n    \n    for i in text:\n        y.append(ps.stem(i))\n            \n    return \" \".join(y)","ebc84859":"# Testing example\ntransform_text('I Loved the YT lectures on Machine Learning. How about you?')","26b579de":"# Checking the first column\ndf['text'][0]","2b3700d7":"# Appling helper function to transform 'text' column and create new column\ndf['transformed_text'] = df['text'].apply(transform_text)","fe498bfe":"df.head()","d0c2a8fb":"from wordcloud import WordCloud\nwc = WordCloud(width=1500,height=1500,min_font_size=10,background_color='white')","09e2b3a6":"spam_wc = wc.generate(df[df['target'] == 1]['transformed_text'].str.cat(sep=\" \"))","11b5e54b":"plt.figure(figsize=(15,6))\nplt.imshow(spam_wc)","00e97633":"ham_wc = wc.generate(df[df['target'] == 0]['transformed_text'].str.cat(sep=\" \"))","1f886abb":"plt.figure(figsize=(15,6))\nplt.imshow(ham_wc)","73a034b0":"# Checking Spam\nspam_corpus = []\nfor msg in df[df['target'] == 1]['transformed_text'].tolist():\n    for word in msg.split():\n        spam_corpus.append(word)","a0c7489d":"len(spam_corpus)","071b44ba":"# Displays most common words\nfrom collections import Counter\nsns.barplot(pd.DataFrame(Counter(spam_corpus).most_common(30))[0],pd.DataFrame(Counter(spam_corpus).most_common(30))[1])\nplt.xticks(rotation='vertical')\nplt.show()","83be66fc":"#Ham\nham_corpus = []\nfor msg in df[df['target'] == 0]['transformed_text'].tolist():\n    for word in msg.split():\n        ham_corpus.append(word)","6b9af9ae":"len(ham_corpus)","291e9754":"# Displays most common words on \"ham\"\nfrom collections import Counter\nsns.barplot(pd.DataFrame(Counter(ham_corpus).most_common(30))[0],pd.DataFrame(Counter(ham_corpus).most_common(30))[1])\nplt.xticks(rotation='vertical')\nplt.show()","9014bdb3":"from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\ncv = CountVectorizer()\ntfidf = TfidfVectorizer(max_features=3000)","7b8030cc":"x = tfidf.fit_transform(df['transformed_text']).toarray()\nx.shape","e9278385":"y = df['target'].values\ny","93ed32bb":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=2)","64f42411":"from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\nfrom sklearn.metrics import accuracy_score,confusion_matrix,precision_score","1589f555":"gnb = GaussianNB()\nmnb = MultinomialNB()\nbnb = BernoulliNB()","8233749a":"# Gaussian NaiveBayes\ngnb.fit(x_train,y_train)\ny_pred1 = gnb.predict(x_test)\nprint(accuracy_score(y_test,y_pred1))\nprint(confusion_matrix(y_test,y_pred1))\nprint(precision_score(y_test,y_pred1))","ed3f1e04":"#Multinomial Naive Bayes\nmnb.fit(x_train,y_train)\ny_pred2 = mnb.predict(x_test)\nprint(accuracy_score(y_test,y_pred2))\nprint(confusion_matrix(y_test,y_pred2))\nprint(precision_score(y_test,y_pred2))","03a4846e":"#Bernoyli NaiveBayes\nbnb.fit(x_train,y_train)\ny_pred3 = bnb.predict(x_test)\nprint(accuracy_score(y_test,y_pred3))\nprint(confusion_matrix(y_test,y_pred3))\nprint(precision_score(y_test,y_pred3))","d8f9630d":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier","873db550":"svc = SVC(kernel='sigmoid', gamma=1.0)\nknc = KNeighborsClassifier()\nmnb = MultinomialNB()\ndtc = DecisionTreeClassifier(max_depth=5)\nlrc = LogisticRegression(solver='liblinear', penalty = 'l1')\nrfc = RandomForestClassifier(n_estimators=50, random_state=2)\nabc = AdaBoostClassifier(n_estimators=50, random_state=2)\nbc = BaggingClassifier(n_estimators=50, random_state=2)\netc = ExtraTreesClassifier(n_estimators=50, random_state=2)\ngbdt = GradientBoostingClassifier(n_estimators=50,random_state=2)\nxgb = XGBClassifier(n_estimators=50,random_state=2)","a1605023":"clfs = {\n    'SVC' : svc,\n    'KN'  : knc,\n    'NB'  : mnb,\n    'DT'  : dtc,\n    'LR'  : lrc,\n    'RF'  : rfc,\n    'AdaBoost' : abc,\n    'BgC' : bc,\n    'ETC' : etc,\n    'GBDT': gbdt,\n    'xgb' : xgb\n}","20014a8e":"def train_classifier(clf,x_train,y_train,x_test,y_test):\n    clf.fit(x_train,y_train)\n    y_pred = clf.predict(x_test)\n    accuracy = accuracy_score(y_test,y_pred)\n    precision = precision_score(y_test,y_pred)\n    \n    return accuracy,precision","037670d2":"train_classifier(svc,x_train,y_train,x_test,y_test)","128adb77":"accuracy_scores = []\nprecision_scores = []\n\nfor name,clf in clfs.items():\n    \n    current_accuracy,current_precision = train_classifier(clf,x_train,y_train,x_test,y_test)\n    \n    print(\"For \",name)\n    print(\"Accuracy - \",current_accuracy)\n    print(\"Precision - \",current_precision)\n    \n    accuracy_scores.append(current_accuracy)\n    precision_scores.append(current_precision)","1f201401":"performance_df = pd.DataFrame({'Algorithm' : clfs.keys(),\n              'Accuracy':accuracy_scores,'Precision':precision_scores}).sort_values('Precision',ascending=False)","9fa2e873":"performance_df","770d0def":"performance_df1 = pd.melt(performance_df, id_vars = \"Algorithm\")\nperformance_df1","7bfb8deb":"# Visualising\nsns.catplot(x = 'Algorithm', y='value',hue = 'variable',data=performance_df1, kind='bar',height=5)\nplt.ylim(0.5,1.0)\nplt.xticks(rotation='vertical')\nplt.show()","4dcd1cfa":"temp_df = pd.DataFrame({'Algorithm':clfs.keys(),\n                        'Accuracy_max_ft_3000':accuracy_scores,\n                        'Precision_max_ft_3000':precision_scores}).sort_values('Precision_max_ft_3000',ascending=False)","c8c49d46":"temp_df","11a52ca1":"temp_df = pd.DataFrame({'Algorithm':clfs.keys(),\n                        'Accuracy_scaling':accuracy_scores,\n                        'Precision_scaling':precision_scores}).sort_values('Precision_scaling',ascending=False)","8b21ac60":"temp_df","4a16d74f":"new_df = performance_df.merge(temp_df,on='Algorithm')\nnew_df","402c34cc":"new_df_scaled = new_df.merge(temp_df,on='Algorithm')\nnew_df_scaled","4b580fdc":"temp_df = pd.DataFrame({'Algorithm':clfs.keys(),\n                        'Accuracy_num_chars':accuracy_scores,\n                        'Precision_num_chars':precision_scores}).sort_values('Precision_num_chars',ascending=False)","e6374acd":"temp_df","6139a8f8":"new_df_scaled.merge(temp_df,on='Algorithm')","7be4f065":"# To improve model model accuracy by using Voting classifier\nsvc = SVC(kernel='sigmoid', gamma=1.0,probability=True)\nmnb = MultinomialNB()\netc = ExtraTreesClassifier(n_estimators=50, random_state=2)\n\nfrom sklearn.ensemble import VotingClassifier","02028e0f":"voting = VotingClassifier(estimators=[('svm', svc), ('nb', mnb), ('et', etc)],voting='soft')","01fb3089":"voting.fit(x_train,y_train)","6606f9b4":"y_pred = voting.predict(x_test)\nprint(\"Accuracy\",accuracy_score(y_test,y_pred))\nprint(\"Precision\",precision_score(y_test,y_pred))","a33bbc10":"# Applying stacking\nestimators=[('svm', svc), ('nb', mnb), ('et', etc)]\nfinal_estimator=RandomForestClassifier()","ecae2184":"from sklearn.ensemble import StackingClassifier\nclf = StackingClassifier(estimators=estimators, final_estimator=final_estimator)","82466d1d":"clf.fit(x_train,y_train)\ny_pred = clf.predict(x_test)\nprint(\"Accuracy\",accuracy_score(y_test,y_pred))\nprint(\"Precision\",precision_score(y_test,y_pred))","5e293b0b":"# Predecting some other model tfidf --> MNB","86d09835":"# Model Building","2c6f7da8":"# EDA","d754b12c":"* By visualising the co-realton i am taking only on value","9413101c":"## Splitting Data","6d2549cc":"# Renaming columns\n* Creating a dataset to meaningful way","2b3eba5f":"# Loading Data","0fd674c8":"# Checking Top-30 words in ham and spam","ecf453d9":"### Visualising Co-realtion","a4afe8de":"* Checking whether the Multinomial Naivebayes predecting True are not..!","5940b95a":"* After visualising the data set is slightly imbalanced","8ed0464c":"### Checkinh relation b\/w number of words and sentences","f0b80c5a":"### Label Encoder","ee445d6a":"# Improving Model Accuracy\n* Changing the max_feature parameters to TFIDF","221d7297":"# Importing Modules","74ddd034":"# Visualing Word Cloud\n* ham\n* spam","e0093f74":"# Data Preprocessing\n* Lower case\n* Tokenization\n* Removing special characters\n* Removing stop words and punctuations\n* Stemming","fccef287":"# Appling Stacking which is similar to Voting\n* It decided the weightage by using random classifier","3c1296a1":"### Checking unique values","f8f1c08b":"# Suport Vector Meachine Model","d5436251":"### Removing Duplicated values","3374e504":"### Checking Missing values","89936f45":"* Me tried scaling but there is no use","85cc1072":"# Helper tranformation function","30f90ea6":"* Lower case problem and Tokensized solved\n* Removing special proclem solved","36793e30":"## Removing unnecessary columns"}}