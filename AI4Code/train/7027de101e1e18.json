{"cell_type":{"7eaa6c18":"code","96a6d023":"code","03d5bde7":"code","ec3e9558":"code","523018f2":"code","72ebe18a":"code","6765d709":"code","45152fd8":"code","01780169":"code","0c74f0e5":"code","17c07180":"code","ae449f06":"code","0daae33f":"code","dbe98b59":"code","d7c5274f":"code","330396b3":"code","d0de6eae":"code","f5f785ca":"code","a86abf5e":"code","7de2a03e":"code","7e829fc0":"code","b6ef7908":"code","a169e4d6":"code","1f52c3dc":"code","65e12f05":"markdown","8983196f":"markdown","99ad574e":"markdown","f8233be6":"markdown","52068153":"markdown","274411eb":"markdown","61d82766":"markdown","2c742595":"markdown","5763ffce":"markdown","53932ec3":"markdown","14054e54":"markdown","1f681c45":"markdown","b15d92a8":"markdown","21ad00d9":"markdown","b2ecc69a":"markdown","eda4010e":"markdown","c0f82b94":"markdown","677f2175":"markdown","135cc01d":"markdown","63392dbf":"markdown"},"source":{"7eaa6c18":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.models import Sequential\nfrom keras.layers import BatchNormalization, Dropout, Dense, Conv2D, MaxPooling2D, Flatten\nfrom keras.utils import np_utils\n\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","96a6d023":"train_df = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\nprint(train_df.shape)\ntrain_df.head()","03d5bde7":"test_df = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\nprint(test_df.shape)\ntest_df.head()","ec3e9558":"print(pd.value_counts(train_df['label']))\nprint(\" \")\n\nsns.countplot(x='label', data=train_df, palette='GnBu_r')","523018f2":"X_train = train_df.iloc[:,1:]\ny_train = train_df.iloc[:,0]\n\nX_train.shape, y_train.shape","72ebe18a":"X_train = X_train.values.reshape(X_train.shape[0],28,28,1)\n\ntest_df = test_df.values.reshape(test_df.shape[0],28,28,1)\n\nX_train.shape, test_df.shape","6765d709":"X_train[0]","45152fd8":"y_train[0]","01780169":"plt.imshow(np.squeeze(X_train[4]), cmap=plt.get_cmap('gray'))\nprint('Label : ', plt.title(label=y_train[4]))\nplt.show()","0c74f0e5":"mean = X_train.mean()\nstd = X_train.std()\n\nX_train = (X_train - mean)\/std","17c07180":"mean = test_df.mean()\nstd = test_df.std()\n\ntest_df = (test_df - mean)\/std","ae449f06":"y_train = np_utils.to_categorical(y_train, num_classes=10)\n\ny_train[0]","0daae33f":"model = Sequential()","dbe98b59":"model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)))\nmodel.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Dropout(rate=0.2))\n\nmodel.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)))\nmodel.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Dropout(rate=0.2))","d7c5274f":"model.add(Flatten())","330396b3":"model.add(Dense(units=128, activation='relu'))\nmodel.add(Dropout(rate=0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(units=256, activation='relu'))\nmodel.add(Dropout(rate=0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(units=10, activation='softmax'))","d0de6eae":"model.summary()","f5f785ca":"model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])","a86abf5e":"history = model.fit(x=X_train, y=y_train, batch_size=200, epochs=10, validation_split=0.2)\nhistory","7de2a03e":"plt.plot(history.history['loss'], label='train loss')\nplt.plot(history.history['val_loss'], label='val loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","7e829fc0":"plt.plot(history.history['accuracy'], label='train accuracy')\nplt.plot(history.history['val_accuracy'], label='val accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","b6ef7908":"test_df.shape","a169e4d6":"predict = model.predict_classes(test_df, verbose=1)\n\npred_df = pd.DataFrame({\"ImageId\": list(range(1,len(predict)+1)), \"Label\": predict})\n\npred_df.head()","1f52c3dc":"pred_df.to_csv(path_or_buf=\"Kaggle_mnist.csv\", index=False, header=True)","65e12f05":"# **Building Neural Network**","8983196f":"Using Countplot, visulizing different labels count in Train data. As labels vary from integer 0 to 9, it is Multi class classification problem","99ad574e":"Hi All,\n\nThis Notebook is like self assessment for me. Since I started learning Deep Learning concepts recently. As I am enthusiast to learn new things and apply my knowledge, I executed this Deep Learning \"Hello world\" dataset with Keras and Convolution Neural Network.\n\nThanks for visiting this Notebook, Hope it might be useful for someone else here.\n\nI built Neural Network on MNIST hand written images pixel dataset to identify their correct label i.e number in image.\n\nLet's Start....\n","f8233be6":"# **Building model with Sequential method**","52068153":"# **Compilation of Model**\n\nLoss function helps to measure how poorly our model performs on images with known labels. It is the error rate between the observed labels and the predicted ones. We use a specific form for categorical classifications (>2 classes) called the \"categorical_crossentropy\".\n\nthe Optimizer function will iteratively improve parameters (filters kernel values, weights and bias of neurons ...) in order to minimise the loss.\n\nI choosed RMSprop (with default values), it is a very effective optimizer. The RMSProp update adjusts the Adagrad method in a very simple way in an attempt to reduce its aggressive, monotonically decreasing learning rate.\n\nThe Metric function \"accuracy\" is used is to evaluate the performance our model. This metric function is similar to the loss function, except that the results from the metric evaluation are not used when training the model (only for evaluation).","274411eb":"# **Visualization of Loss and Accuracy of model**","61d82766":"# **Reshaping Features of Train and Test dataset**\n\ndataset reshaped to (values count, height, width, channel)\n\nfor gray scale image channel is equal to 1\nfor coloured image channel is equal to 3","2c742595":"# **Importing Libraries**","5763ffce":"# **Summary of Model**","53932ec3":"# **Preprocessing of Data**","14054e54":"# **Flattening of Convolution Output**\n\nit is use to convert the final feature maps into a one single 1D vector","1f681c45":"# **Loading Train and Test data**","b15d92a8":"# **Normalization of Train and Test features**\n\nIt is important preprocessing step. It is used to centre the data around zero mean and unit variance.","21ad00d9":"# **Splitting Train data into Features and Labels**","b2ecc69a":"# **Fitting Model**\n\nsplitting train set into 80% part used for actual training and 20% that is used to check if the model is overfitting","eda4010e":"# **Visualization of one image of Feature data**","c0f82b94":"**you found this notebook helpful or you just liked it , some upvotes would be very much appreciated - That will keep me motivated :)**","677f2175":"# **Predicting model with Test dataset**","135cc01d":"# **Adding Convolution layers with Dropout and Maxpooling layers**\n\nCNN architechture is In -> [[Conv2D -> relu]* 2 -> MaxPool2D -> Dropout] * 2 -> Flatten -> Dense -> Dropout -> Batch Normalization -> Out","63392dbf":"# **Conversion of Labels to categorical**\n\nConverts a class vector (integers) to binary class matrix.\n"}}