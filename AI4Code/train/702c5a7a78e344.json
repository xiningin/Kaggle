{"cell_type":{"ddb8e528":"code","79955286":"code","e1aeb9ed":"code","fe1f8d42":"code","ed7cc8be":"code","bea909eb":"code","a471d98c":"code","ee76d893":"code","8a45278c":"markdown","b2bbe30a":"markdown","38ab02d1":"markdown"},"source":{"ddb8e528":"import pandas as pd\nimport time\nfrom datetime import date\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","79955286":"def myround(x, base=5):\n    'Rounds to the nearest 5'\n    return base * round(x\/base)\n\ndef drop_false(series):\n    '''\n    This function takes a series and drops any indices with False boolean values.\n    '''\n    return series.apply(lambda x: x if x else np.nan).dropna()\n\ndef supermin(iterable):\n    '''\n    A more robust minimum function that returns the minimum of empty iterables as -1.\n    '''\n    if len(iterable) > 0:\n        return min(iterable)\n    else:\n        return -1\n\ndef optimal_drops(state):\n    '''\n    Optimal drops takes a given state of connected points and applies the congruence rule and the \n    proper subset rule once.\n    '''\n    # I find any points that satisfy the proper subset rule.\n    subset_drops = list(state[state.apply(\n        lambda y: sum(state.apply(lambda x: x.issubset(y) and x != y)) > 0)].index)\n\n    # same_tuples returns a series which, for each node, gives which nodes have the same connections. \n    # I return the results in tuples because tuples are hashable, which means I can apply set() to \n    # return unique nodes.\n    same_tuples = state.apply(lambda x: tuple(drop_false(state == x).index))\n\n    # LOL returns a set of tuples of nodes to drop; i.e. all but one node from each group of \n    # equivalent nodes, from those groups of nodes with more than one member.\n    LOL = set(same_tuples[same_tuples.apply(lambda x: len(x)) > 1].apply(lambda x: x[1:]))\n\n    # same_drops then flattens this set of lists into a single list. \n    same_drops = [node for l in LOL for node in l]\n\n    # Aggregating and appending the items to be dropped.\n    drops = list(set(subset_drops + same_drops))\n\n    return drops\n    \ndef optimal_drops_loop(state):\n    '''\n    This function runs optimal_drops in a loop until no more applications of the congruence and\n    proper subset rule can be found, then returns an aggregated list of variables to remove.\n    '''\n    \n    subtree_drops = []\n    updating_state = state\n    # I make sure the while loop goes ahead by not starting 'drops' as an empty list.\n    drops = ['dont ignore the while loop']\n    while len(drops) > 0:\n        \n        # I apply optimal drops.\n        drops = optimal_drops(updating_state)\n        \n        # I update the state of play by dropping the drops identified by optimal_drops and \n        # removing them from the corresponding variable sets.\n        updating_state = updating_state.drop(drops).apply(lambda x: x.difference(set(drops)))\n        \n        # I update the aggregate list of variables to drop with the current set of identified\n        # variables to drop.\n        subtree_drops = subtree_drops + drops\n        \n    return subtree_drops\n\ndef fork(state,opsets,checklist):\n    '''\n    This function takes the current potential optimal sets and bifurcates them each into two\n    sub-branches in such a way as that, if the original was truly optimal, then one of the\n    sub-branches must be also.\n    '''\n    assert len(opsets) == len(checklist), \"checklist and opsets are not the same length\"\n    # next_sets will contain the new bifurcated sub-branches.\n    next_sets = []\n    # I run a for loop on every current branch, and will bifurcate each within this loop.\n    for opset in opsets:\n        # I ignore those that have already solved the problem.\n        if checklist[opsets.index(opset)]:\n            pass\n        else:\n            # Otherwise I calculate the state of play with the current branch.\n            rump = state.drop(opset).apply(lambda x: x.difference(set(opset)))\n\n            # I identify the most connected point and the points that it is connected to, \n            # excluding itself.\n            grinch = rump.apply(lambda x: len(x)).idxmax()\n            elf = rump[grinch].difference(grinch)\n            \n            # I evaluate the state of play by taking the greedy approach and by taking the \n            # opposite approach where I drop every connected point aside from the most connected \n            # one.\n            greed = rump.drop(grinch).apply(lambda x: x.difference({grinch}))\n            altruism = rump.drop(elf).apply(lambda x: x.difference(elf))\n            \n            # I use optimal_drops_loop to identify the optimal points to drop given the two \n            # states identified above. I also add in the points from the mother branch and the \n            # pivotal points I decided to remove (grinch\/elf)\n            greedy_tree = optimal_drops_loop(greed) + opset + [grinch]\n            altruistic_tree = optimal_drops_loop(altruism) + opset + list(elf)\n            \n            # I add these two new sub-branches to next_sets.\n            next_sets = next_sets + [greedy_tree] + [altruistic_tree]\n        \n    # I then add back in the solved branches\n    next_sets = next_sets + [opsets[i] for i in range(len(checklist)) if checklist[i]]\n        \n    return next_sets\n\ndef lazy_solution(state):\n    '''\n    lazy_solution solves the problem by finding optimal points to remove until it can no longer do\n    so, then takes the greedy approach by eliminating the point with the most connections, \n    and then finding more optimal points given this elimination, and then repeating until the \n    problem is solved.\n    '''\n\n    lazy_drops = []\n    # In this loop I continuously evaluate the state of play, so I can just use it as a condition \n    # to regulate the while loop.\n    while state.apply(lambda x: len(x)).max() > 1:\n    \n        # I identify the point with the most connections, re-evaluate the state of play without \n        # that point and add the point to lazy_drops.\n        max_drop = state.apply(lambda x: len(x)).idxmax()\n        state = state.drop(max_drop).apply(lambda x: x.difference({max_drop}))\n        lazy_drops.append(max_drop)\n        \n        # I use optimal_drops_loop to find optimal drops given I have taken a greedy step. I \n        # append these drops to lazy_drops and re-evaluate the state of play.\n        drops = optimal_drops_loop(state)\n        lazy_drops += drops\n        state = state.drop(drops).apply(lambda x: x.difference(set(drops)))\n\n    return lazy_drops\n\ndef isolator(state,give_up=112):\n    '''\n    This function solves a problem as represented by the 'state' input, but gives up and employs\n    the lazy_solution function if it reaches give_up branches. \n    '''\n    # I start the algorithm by finding any optimal drops I can and updating the state of play, \n    # calling the state with these first optimal drops removed 'first_rump'\n    first_drops = optimal_drops_loop(state)\n    first_rump = state.drop(first_drops).apply(lambda x: x.difference(set(first_drops)))\n    \n    # I check whether or not I have already solved the problem. If I have, then the condition \n    # inside the list will be True. I have the boolean inside the list since I will have to check \n    # multiple branches as the algorithm moves on, so I set up the initial infrastructure to be \n    # consistent with this.\n    checklist = [first_rump.apply(lambda x: len(x)).max() == 1]\n    # However, if the condition actually is True then I can just exit now and return first_drops\n    if checklist[0]:\n        return first_drops\n    # I pretend that the initial problem is represetned by first_rump (and add in first_drops \n    # later) and therefore pretend that I have identified an empty optimal set to drop.\n    optimal_sets = [[]]\n    # The length of the optimal set identified so far is therefore zero.\n    opset_length = [0]\n    # I have reached no leaf node since my optimal set doesn't actually solve the problem, so\n    # leaf_lengths is empty\n    leaf_lengths = []\n    \n    # I explain the status reports below.\n    print(\"\\033[1m\" + \"The algorithm will have found an optimal solution once 'Leaf Minimum' is equal to 'Branches Minimum.'\" + \"\\033[0m\")\n    print(\"\\033[1m\" + \"Otherwise, the algorithm will default to a quicker approximation if 'Current Branches' reaches 'Give Up.'\" + \"\\033[0m\")\n    print('--------------------------------------------------------------------')\n    print('--------------------------------------------------------------------')    \n    \n    # The while loop will stop once the length of a solved leaf node (list of variables to drop) \n    # is the minimum of all present branch solutions, or if there are so many branch potential \n    # solutions that the user tells it to give up.\n    while not (min(opset_length) == supermin(leaf_lengths) or len(checklist) >= give_up):\n        # I use the corrent optimal_sets and the fork function to bifurcate each of the current \n        # optimal sets. I then evaluate the length of these new sets.\n        optimal_sets = fork(first_rump,optimal_sets,checklist)\n        opset_length = [len(opset) for opset in optimal_sets]\n        \n        # I check to see if any of the optimal sets solves the problems. I then get the lengths of \n        # only those optimal sets that do solve the problem. Now I have everything needed to check \n        # to see if I can stop the while loop, since I can check if I already have an optimal set \n        # that solves the problem and is shorter than any other.\n        checklist = [first_rump.drop(opset).apply(lambda x: x.difference(set(opset))).apply(\n        lambda x: len(x)).max() == 1 for opset in optimal_sets]\n        leaf_lengths = [opset_length[i] for i in range(len(checklist)) if checklist[i]]\n        # I report on the progress of the algorithm\n        print(f'Current Branches: {len(checklist)} | Leaf Nodes: {sum(checklist)} | Give Up: {give_up}')\n        print('--------------------------------------------------------------------')\n        print(f'Branches Minimum: {min(opset_length)} | Leaf Minimum: {supermin(leaf_lengths)}')\n        print('--------------------------------------------------------------------')\n        print('--------------------------------------------------------------------')\n    # I check to see if the while loop gave up or not. If not, then it contains at least one \n    # optimal set that is a leaf node, solves the problem, and is truly optimal.\n    if len(checklist) < give_up or np.isnan(give_up):\n        # I identify truly optimal sets as the shortest of the optimal sets (which given the \n        # nature of the while loop must solve the problem)\n        truly_optimal_sets = [opset for opset in optimal_sets if len(opset) == supermin(leaf_lengths)]\n        # I then add back in the first_drops to these truly optimal sets to make these truly \n        # optimal sets of the original problem.\n        really_truly_optimal_sets = [first_drops + opset for opset in truly_optimal_sets]\n        \n        return really_truly_optimal_sets\n        \n    else:\n        # Otherwise I complete the leftmost branch \n        second_rump = first_rump.drop(optimal_sets[0]).apply(lambda x: x.difference(set(optimal_sets[0])))\n        # I apply lazy_solution to get an approximate answer.\n        return first_drops + optimal_sets[0] + lazy_solution(second_rump)\n    \ndef correlation_eliminator(dataframe,correlation_cutoff,give_up=112,override_corr=None):\n    '''\n    This function is the full monty when it comes to your high correlation eliminating needs. It \n    takes a raw dataframe and a correlation_cutoff value that specifies an unacceptable level of \n    absolute correlation between any two variables. It specifies the fewest number of variables to\n    remove so as to ensure that no pairwise absolute correlation above the specified level exists. \n    The give_up value specifies how many branch solutions being evaluated at once should trigger \n    giving up on achieving an optimal solution. A pre-calculated correlation matrix can be \n    inputted in the override_corr argument to avoid calculating it from scratch, which takes a lot \n    of time.\n    '''\n    if override_corr is None:\n        # I make sure there are no purely nan columns and then calculate the correlation matrix\n        nancols = drop_false(dataframe.isnull().sum() == len(dataframe)).index\n        dataframe.drop(nancols,index=1)\n        corr_matrix = dataframe.corr()\n    else:\n        corr_matrix = override_corr    \n    \n    # for each variable, I get a series indicating which other variables it is highly absolutely \n    # correlated with above correlation_cutoff.\n    corr_sets = corr_matrix.apply(lambda x: x[abs(x) >= correlation_cutoff].index.tolist(),axis=1)\n    # I retain only those indices which indicate correlation above 'correlation_cutoff' with more\n    # than one variable (i.e. I retain only those indices that are correlated not just with \n    # themselves). I also transform the elements of the series into sets rather than lists.\n    corr_sets = corr_sets[corr_sets[corr_sets.apply(lambda x: len(x) > 1)].index]\n    corr_sets = corr_sets.apply(lambda x: set(x))\n    # Then I run isolator to work out the optimal sets of variables to drop to solve the problem.\n    return isolator(corr_sets,give_up)","e1aeb9ed":"# I load in some messy data.\ndf = pd.read_csv('..\/input\/messy_data.csv',index_col=0,\n                          header=[0,1],infer_datetime_format=True,low_memory=False)","fe1f8d42":"%%time\n# I remove any columns that are purely nans and calculate the correlation matrix of this data.\nnancols = drop_false(df.isnull().sum() == len(df)).index\ncorr_matrix = df.drop(nancols,axis=1).corr()","ed7cc8be":"# Looking at a heatmap of the correlation matrix we can see just how simimlar the variables are.\nplt.figure(figsize=(12,12))\nsns.heatmap(corr_matrix,cmap='coolwarm',xticklabels=False,yticklabels=False)\nplt.xlabel('')\nplt.ylabel('')\nplt.title('Heatmap of Correlation Matrix') ;","bea909eb":"# Amongst those variables that are not highly correlated only with themselves, a histogram of the \n# number of variables with which they are correlated above corr_cutoff is displayed here. \ncorr_cutoff = 0.95\n# I get the correlation matrix into a series\/dictionary representation...\ncorr_pairs = corr_matrix.apply(lambda x: x[abs(x) > corr_cutoff].index.tolist(),axis=1)\n# ...and then drop any variable that is only correlated with itself.\ncorr_pairs = corr_pairs.drop(corr_pairs[corr_pairs.apply(lambda x: len(x) == 1)].index)\n# I use the remainder to make a histogram.\nplt.figure(figsize=(14,10))\nplt.title(f'Histogram of Correlations Amongst Variables absolutely correlated with at least one other variable above {corr_cutoff}')\nplt.xlabel('Number of High Pairwise Correlations (Including Self-Correlation)')\ncorr_len = corr_pairs.apply(lambda x: len(x))\nplt.xticks(np.arange(0,corr_len.max()+3,myround(corr_len.max()\/10)))\ncorr_len.hist(bins = corr_len.max()) ;","a471d98c":"%%time\n# This cell runs the algorithm. It's a good idea to use the override_corr argument and input the \n# pre-calculated correlation matrix.\nsol = correlation_eliminator(df,corr_cutoff,give_up=112,override_corr=corr_matrix)","ee76d893":"# Using the same code from earlier but dropping the variables identified by the solution, then \n# dropping every variable only highly correlated with itself, returns an empty series. This means \n# that the problem has been solved - if we were to display the same histogram as before,\n# no bars would appear.\ncorr_pairs = corr_matrix.drop(sol[0]).drop(sol[0],axis=1).apply(lambda x: x[abs(x) > corr_cutoff].index.tolist(),axis=1)\ncorr_pairs.drop(corr_pairs[corr_pairs.apply(lambda x: len(x) == 1)].index)","8a45278c":"# Defining the Functions","b2bbe30a":"# Application to example data","38ab02d1":"# The Correlation Eliminator Algorithm - A Surprise Solution to The Minimum Vertex Cover Problem\nIn this notebook, I share the correlation_eliminator function and its supporting functions, which identify the smallest number of variables in a dataframe that must be removed in order for there to be no two variables absolutely correlated with each other above some user defined cutoff. My cousin [Gus Kasper](https:\/\/github.com\/guskasper) helpfully pointed out that solving this problem is equivalent to solving the np complete [minimum vertex cover problem](https:\/\/en.wikipedia.org\/wiki\/Vertex_cover), so I created a [github repository](https:\/\/github.com\/VoxMetrica\/Min-Vertex-Cover-Solution) dedicated to this algorithm in which you can find a mini-treatise justifying and explaining the rules used by this algorithm. I highly encourage perusal of the explanatory document in my github repository, since understanding the rules of the algorithm and the nature of the problem can be quite involved.  In brief, however, the correlation_eliminator algorithm works as follows:\n1. The inputted dataframe and correlation cutoff are used to create a representation of the problem.\n2. The representation (which I call 'state' or 'state of play') is fed to the 'isolator' function, which starts by repeatedly removing whole swathes of variables (or 'points' if you think of the graphical vertex cover problem) that are optimal to remove according to the 'congruence' and 'proper subset' rules explained in the github repository.\n3. Once it can no longer find optimal points, it creates two branch solutions, one of which must be optimal, and resumes removing optimal points until it can no longer find any.\n4. This bifurcation and removal continues until a branch solution that solves the problem is the shortest branch under consideration (and is therefore an optimal solution).\n5. If the problem gets out of hand and the number of branches under consideration balloons past a user defined level, the algorithm defaults to a quasi-greedy approach in which the algorithm 'removes' the variable with the greatest number of correlations ('or connections' if you think about the graphical vertex cover problem) instead of bifurcating.\n6. The function returns a list or a list of lists of variable names that must be dropped from the dataframe in order for there to be no correlation above the user defined cutoff. These are called 'optimal sets.'\n\nThe representation or 'state' of the problem is programmed as a pandas series where each variable appears in the index and has as its corresponding entry a list of variables with which it is highly correlated (including itself). The 'congruence' rule mandates the removal of all but one variable within any 'congruence set' of variables whose associated list of correlated variables is the same. The 'proper subset' rule removes any variable for which there exists another variable whose associated list of variables is a proper subset of the first. Programmatically, 'removing' a variable means eliminating the variable from the series representation's index and removing it from any associated list. Bifurcation involves considering the state of play both when the point with most connections is removed and when every point such a point is connected to is removed. The justifcation for all these actions can be found in the github repository.\n\nI personally used this function to remove near duplicate data before carrying out principal component analysis in order to not artificially skew the rankings of the resultant principal components, but removing highly correlated variables that hardly add any new information is a good idea in general, both for the sake of being able to conduct analysis with a more manageable dataset and for the sake of avoiding overfitting. The function is also flexible in the sense that any measure of similarity (such as two variables having some high proportion of values that are identical) can be substituted for correlation so long as it can be put into a series representation, and the function (more specifically, the isolator sub-function) will ensure that the smallest number of variables are removed subject to such a measure ceasing to exist between any two variables, so you can incorporate the functions in this notebook to fit your situation\n\nThe rest of this notbook proceeds by defining the correlation_eliminator and isolator function, loading and visualising some very messy data, then applying the correlation_eliminator function to that data. If you like this notebook, please consider giving it and the associated data an upvote; it helps me out a lot."}}