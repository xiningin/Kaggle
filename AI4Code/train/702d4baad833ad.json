{"cell_type":{"55aa4a01":"code","4d9da8bc":"code","bd8ac824":"code","5ef8d415":"code","53acc687":"code","0ea29239":"code","20ec8328":"code","1c2d8794":"code","42da3568":"code","7ae7ed30":"code","57433c8d":"code","97abd22a":"code","610f5acf":"code","cb7c0e0b":"code","20d315f7":"code","72d9c28a":"code","db1d2004":"code","5f63f8a6":"code","ac2c5e28":"code","63bf4df7":"code","42a41a1d":"code","1b71a04c":"code","f4c760b5":"code","f3ea9491":"code","4a8f3cfb":"code","88c3e747":"code","3fa3c666":"markdown","86085f7f":"markdown","8c69388f":"markdown","a59f0f7a":"markdown","962eed00":"markdown","c1dbacce":"markdown","0fd93851":"markdown","9b4fd0d0":"markdown","3f90c0c5":"markdown","915ace20":"markdown","8da6eab1":"markdown","0af754dc":"markdown","2f5cb082":"markdown","f4fc35aa":"markdown","3b48ca56":"markdown","fd373b91":"markdown","78432b65":"markdown","6fc1b118":"markdown","be038d5f":"markdown","8b1f50dd":"markdown","f9fcb742":"markdown","4aea2cb4":"markdown","506c6cb9":"markdown","1943072a":"markdown","eed3d92d":"markdown","7ecfebdb":"markdown","a0d9f7f8":"markdown","ee0a62e5":"markdown","6ea41b0d":"markdown","11dc7ee2":"markdown","9946b4af":"markdown"},"source":{"55aa4a01":"# !pip install -q -U torch torchvision -f https:\/\/download.pytorch.org\/whl\/torch_stable.html \n# !pip install -q -U 'git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI'\n# !pip install -q detectron2 -f https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cpu\/index.html","4d9da8bc":"# Helper function, used these for debugging purposes\n# detector2 build only succeeds if CUDA version is correct\n\n#!nvidia-smi\n#!nvcc --version\n\n#import torch\n#torch.__version__\n#import torchvision\n#torchvision.__version__\n\n!pip install detectron2 -f https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu102\/torch1.7\/index.html","bd8ac824":"# Base setup:\n# detectron2 logger\nimport detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\n# common libraries\nimport numpy as np\nimport os, json, cv2, random\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# detectron2 utilities\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog\nfrom detectron2.structures import BoxMode","5ef8d415":"!wget http:\/\/images.cocodataset.org\/val2017\/000000282037.jpg -q -O input.jpg\nim = cv2.imread(\".\/input.jpg\")\n\nplt.figure(figsize=(15,7.5))\nplt.imshow(im[..., ::-1]) #bgr to rgb","53acc687":"cfg = get_cfg()\n\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation\/mask_rcnn_R_50_FPN_3x.yaml\"))\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation\/mask_rcnn_R_50_FPN_3x.yaml\")\npredictor = DefaultPredictor(cfg)\noutputs = predictor(im[..., ::-1])","0ea29239":"print(outputs[\"instances\"].pred_classes)\n# print(outputs[\"instances\"].pred_boxes)","20ec8328":"MetadataCatalog.get(cfg.DATASETS.TRAIN[0])","1c2d8794":"import pandas as pd\nmodelclasses = MetadataCatalog.get(cfg.DATASETS.TRAIN[0]).thing_classes\ndf = pd.DataFrame(modelclasses,columns=['Model classes'])\ndf","42da3568":"v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\nout = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\nplt.figure(figsize=(20,10))\nplt.imshow(out.get_image()[..., ::-1][..., ::-1])","7ae7ed30":"!wget https:\/\/github.com\/matterport\/Mask_RCNN\/releases\/download\/v2.1\/balloon_dataset.zip\n!unzip balloon_dataset.zip > \/dev\/null","57433c8d":"# if your dataset is in COCO format, this cell can be replaced by the following three lines:\n# from detectron2.data.datasets import register_coco_instances\n# register_coco_instances(\"my_dataset_train\", {}, \"json_annotation_train.json\", \"path\/to\/image\/dir\")\n# register_coco_instances(\"my_dataset_val\", {}, \"json_annotation_val.json\", \"path\/to\/image\/dir\")\n\nfrom detectron2.structures import BoxMode\n\ndef get_balloon_dicts(img_dir):\n    json_file = os.path.join(img_dir, \"via_region_data.json\")\n    with open(json_file) as f:\n        imgs_anns = json.load(f)\n\n    dataset_dicts = []\n    for idx, v in enumerate(imgs_anns.values()):\n        record = {}\n        \n        filename = os.path.join(img_dir, v[\"filename\"])\n        height, width = cv2.imread(filename).shape[:2]\n        \n        record[\"file_name\"] = filename\n        record[\"image_id\"] = idx\n        record[\"height\"] = height\n        record[\"width\"] = width\n      \n        annos = v[\"regions\"]\n        objs = []\n        for _, anno in annos.items():\n            assert not anno[\"region_attributes\"]\n            anno = anno[\"shape_attributes\"]\n            px = anno[\"all_points_x\"]\n            py = anno[\"all_points_y\"]\n            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n            poly = [p for x in poly for p in x]\n\n            obj = {\n                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n                \"bbox_mode\": BoxMode.XYXY_ABS,\n                \"segmentation\": [poly],\n                \"category_id\": 0,\n            }\n            objs.append(obj)\n        record[\"annotations\"] = objs\n        dataset_dicts.append(record)\n    return dataset_dicts\n\nfor d in [\"train\", \"val\"]:\n    DatasetCatalog.register(\"balloon_\" + d, lambda d=d: get_balloon_dicts(\"balloon\/\" + d))\n    MetadataCatalog.get(\"balloon_\" + d).set(thing_classes=[\"balloon\"])\nballoon_metadata = MetadataCatalog.get(\"balloon_train\")","97abd22a":"dataset_dicts = get_balloon_dicts(\"balloon\/train\")\nfor d in random.sample(dataset_dicts, 3):\n    img = cv2.imread(d[\"file_name\"])\n    visualizer = Visualizer(img[:, :, ::-1], metadata=balloon_metadata, scale=0.5)\n    out = visualizer.draw_dataset_dict(d)\n    plt.figure(figsize=(15,7))\n    plt.imshow(out.get_image()[:, :, ::-1][..., ::-1])","610f5acf":"from detectron2.engine import DefaultTrainer\n\ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation\/mask_rcnn_R_50_FPN_3x.yaml\"))\ncfg.DATASETS.TRAIN = (\"balloon_train\",)\ncfg.DATASETS.TEST = ()\ncfg.DATALOADER.NUM_WORKERS = 2\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation\/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\ncfg.SOLVER.IMS_PER_BATCH = 2\ncfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\ncfg.SOLVER.MAX_ITER = 300    # 300 iterations enough for this dataset; Train longer for a practical dataset\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, enough for this dataset (default: 512)\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # one class (ballon). \n\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\ntrainer = DefaultTrainer(cfg) \ntrainer.resume_or_load(resume=False)\ntrainer.train()","cb7c0e0b":"# Look at training curves in tensorboard:\n%load_ext tensorboard\n%tensorboard --logdir logs","20d315f7":"cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we trained\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a testing threshold\npredictor = DefaultPredictor(cfg)","72d9c28a":"from detectron2.utils.visualizer import ColorMode\ndataset_dicts = get_balloon_dicts(\"balloon\/val\")\nfor d in random.sample(dataset_dicts, 3):    \n    im = cv2.imread(d[\"file_name\"])\n    outputs = predictor(im) \n    v = Visualizer(im[:, :, ::-1],\n                   metadata=balloon_metadata, \n                   scale=0.5, \n                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. Only available for segmentation models\n    )\n    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n    plt.figure(figsize=(15,7))\n    plt.imshow(out.get_image()[:, :, ::-1][..., ::-1])","db1d2004":"from detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.data import build_detection_test_loader\n\nevaluator = COCOEvaluator(\"balloon_val\", (\"bbox\", \"segm\"), False, output_dir=\".\/output\/\")\nval_loader = build_detection_test_loader(cfg, \"balloon_val\")\nprint(inference_on_dataset(trainer.model, val_loader, evaluator))","5f63f8a6":"!wget http:\/\/images.cocodataset.org\/val2017\/000000282037.jpg -q -O input.jpg\nim = cv2.imread(\".\/input.jpg\")","ac2c5e28":"cfg = get_cfg()   # fresh config\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints\/keypoint_rcnn_R_50_FPN_3x.yaml\"))\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints\/keypoint_rcnn_R_50_FPN_3x.yaml\")\npredictor = DefaultPredictor(cfg)\noutputs = predictor(im)\nv = Visualizer(im[:,:,::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\nout = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\nplt.figure(figsize=(15,7))\nplt.imshow(out.get_image()[:, :, ::-1][..., ::-1])","63bf4df7":"!wget http:\/\/images.cocodataset.org\/val2017\/000000282037.jpg -q -O input.jpg\nim = cv2.imread(\".\/input.jpg\")","42a41a1d":"cfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation\/panoptic_fpn_R_101_3x.yaml\"))\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation\/panoptic_fpn_R_101_3x.yaml\")\npredictor = DefaultPredictor(cfg)\npanoptic_seg, segments_info = predictor(im)[\"panoptic_seg\"]\nv = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\nout = v.draw_panoptic_seg_predictions(panoptic_seg.to(\"cpu\"), segments_info)\nplt.figure(figsize=(25,15))\nplt.imshow(out.get_image()[:, :, ::-1][..., ::-1])","1b71a04c":"from IPython.display import YouTubeVideo, display, Video # for viewing the video\n!pip install youtube-dl # for downloading the video","f4c760b5":"#video = YouTubeVideo(\"ll8TgCZ0plk\", width=500)#7HaJArMDKgI\nvideo = YouTubeVideo(\"7HaJArMDKgI\", width=750, height= 450)#\ndisplay(video)","f3ea9491":"!youtube-dl https:\/\/www.youtube.com\/watch?v=7HaJArMDKgI -f 22 -o video.mp4\n!ffmpeg -i video.mp4 -t 00:00:10 -c:v copy video-clip.mp4 ","4a8f3cfb":"!git clone https:\/\/github.com\/facebookresearch\/detectron2\n!python detectron2\/demo\/demo.py --config-file detectron2\/configs\/COCO-PanopticSegmentation\/panoptic_fpn_R_101_3x.yaml --video-input video-clip.mp4 --confidence-threshold 0.6 --output 1video-output.mkv \\\n  --opts MODEL.WEIGHTS detectron2:\/\/COCO-PanopticSegmentation\/panoptic_fpn_R_101_3x\/139514519\/model_final_cafdb1.pkl","88c3e747":"!git clone https:\/\/github.com\/vandeveldemaarten\/tempdetector2video.git\nVideo(\".\/tempdetector2video\/myvideo.mkv\")","3fa3c666":"Downloading the video and cropping 6 seconds for processing\n","86085f7f":"In the output above we see an array which shows us the predictions made by the model. But what number stands for which class?\n\nEvery dataset is associated with [metadata](https:\/\/detectron2.readthedocs.io\/en\/latest\/tutorials\/datasets.html#metadata-for-datasets). It is a key-value mapping that contains information about the dataset. It can be used to further interpret the dataset. This information can later be used for data augmentation, evaluation, visualization, logging, ... .","8c69388f":"<a id=\"pretrainedinference\"><\/a>\n## 3.3. Inference with a pretrained model\n\nIn this first \"coding\" part are two important utils from Detector2. On the one hand we are using **cfg** or better [configs](https:\/\/detectron2.readthedocs.io\/en\/latest\/tutorials\/configs.html) which represents the complete configuration of a object detection model. These configurations are stored within a YAML-file and can be easily received from the modelzoo.\n\nAfter the configuration is complete we'll use the [**DefaultPredictor**](https:\/\/detectron2.readthedocs.io\/en\/latest\/tutorials\/models.html?highlight=DefaultPredictor#use-a-model) class to make predictions.","a59f0f7a":"Since the our new data looks good. Let's now train our model!\n\nAs initial weights we'll use the pretrained weights from a model from the modelzoo. After finishing setting up the config we'll use the [**DefaultTrainer()**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/utils.html#module-detectron2.utils.visualizer) class to train our model!\n\nMore information about updating the config-file can be found [here](https:\/\/detectron2.readthedocs.io\/tutorials\/datasets.html#update-the-config-for-new-datasets).","962eed00":"<a id=import ><\/a>\n# 2. Installing dependencies and libraries\n\nWe can use both CPU and GPU for training and inference of the models.\n\nRunning on CPU:","c1dbacce":"*Note: I suppose TensorBoard doesn't work on Kaggle?*","0fd93851":"<a id=\"thevideo\" ><\/a>\n## 6.2. The video","9b4fd0d0":"<a id=\"loadmodel\" ><\/a>\n# 3. Loading and using an existing model\n\nIn this chapter we'll have a look at a pretrained model and the base data it's trained on. Later on we'll do some inference with it.\n\n<a id=\"basedata\" ><\/a>\n## 3.1. The base data\n\nThe model we'll be using is pretrained on the [COCO](https:\/\/cocodataset.org\/#home) dataset (2017). This dataset contains a lot of labeled data people can use to train there Object detection models on. (Object, Keypoint, Panoptic, Instance and Densepose detectors) \n\nLet's take a look at a sample:","3f90c0c5":"<a id=\"modelzoo\"><\/a>\n## 3.2. The model zoo\nMany pretrained models can be found back within the \"[modelzoo](https:\/\/github.com\/facebookresearch\/detectron2\/blob\/master\/MODEL_ZOO.md)\". This is a collection of models pretrained on a certain dataset that are ready to be used. Mostly people will use the pretrained weights of these model for initalization of there own custom model. This significantly shortens the training time and performance. And that's exactly what we'll be doing!\n\nThe model we'll be using can be found [here](https:\/\/github.com\/facebookresearch\/detectron2\/blob\/master\/configs\/COCO-Detection\/retinanet_R_50_FPN_3x.yaml). \n\nHow does it work?\nAs we can find [here](https:\/\/www.researchgate.net\/figure\/Our-Mask-R-CNN-framework-In-the-first-stage-we-use-Resnet50-Resnet101-and-Resnet_fig1_334011187):\n> Region proposal network (RPN) utilizes feature maps at one of the intermediate layers (usually the last convolutional layer) of the CNN feature extractor networks to generate box proposals (300 boxes in our study). The proposed boxes are a grid of anchors tiled in different aspect ratios and scales. The second stage predicts the confidence value, the offsets for the proposed box and the mask within the box for each anchor.\nSource publication\n\n![Mask R-CNN Resnet](https:\/\/www.researchgate.net\/profile\/Hemin_Ali_Qadir\/publication\/334011187\/figure\/fig1\/AS:774289183735808@1561616335154\/Our-Mask-R-CNN-framework-In-the-first-stage-we-use-Resnet50-Resnet101-and-Resnet.ppm)\n\nAlright, so now that we know how our model works lets test it out!","915ace20":"Tensorboard usable in kaggle?","8da6eab1":"<a id=intro ><\/a>\n# 1. Introduction\n[Detectron2](https:\/\/ai.facebook.com\/blog\/-detectron2-a-pytorch-based-modular-object-detection-library-\/) is Facebook AI Research's next generation software system that implements state-of-the-art object detection algorithms. It is a ground-up rewrite of the previous version, Detectron that started from maskrcnn-benchmark.\n\nThis platform is implemented in PyTorch. Thanks to its modular design its a very flexible and extensible framework providing fast training.\n\nIt includes implementations of state-of-the-art object detection algorithms such as:\n* Box detection\n* Mask detection\n* KeyPoint detection\n* Densepose detection\n* Semantic segmentation\n* Panoptic segmentation\n\nIn this notebook we'll have a look at several of these implementations and show how you can use custom datasets to train your own customer model.\n\n*Note: This notebook is a work in progress. I will keep on extending this notebook to fully explore all the capabilities of the \"Detectron2\"- framework!*\n*Future updates will consist in adding more high-end object detection algorithms and explaining more of the functions within the framework.*","0af754dc":"Notice that by using the [**ColorMode.IMAGE_BW**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/utils.html?highlight=ColorMode#module-detectron2.utils.visualizer) we we're capable of removing the colors from objects which aren't detected!","2f5cb082":"Running on GPU:","f4fc35aa":"<a id=\"videoinference\" ><\/a>\n## 6.3. Inference on the video\nLet's now run an panoptic model over the video above.\n\n*note: For now I'll be using some [demo](https:\/\/github.com\/facebookresearch\/detectron2\/tree\/master\/demo) files, I'll later add the code implementations to this notebook.*","3b48ca56":"<a id=helperfunction > <\/a>\n## 4.2. Helper functions\n\nLet's create some helper functions.\n\nThe **get_balloon_dicts()** will convert our data to the correct format. The [**BoxMode**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/structures.html?highlight=BoxMode#detectron2.structures.BoxMode) can be used to get the structure.\n\nAfterwards we'll have to add or register our new dataset with [**DatasetCatalog.register()**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/data.html?highlight=DatasetCatalog#detectron2.data.DatasetCatalog). Ofcourse don't forget to add your metadata with [**MetadataCatalog.get()**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/data.html?highlight=MetadataCatalog#detectron2.data.MetadataCatalog).","fd373b91":"Above we can see that our models performs pretty well! Let's now evaluate our custom model with [Evaluators](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/engine.html?highlight=DefaultPredictor#detectron2.engine.defaults.DefaultPredictor). Two evaluators can be used:\n* [**COCOEvaluator**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/evaluation.html#detectron2.evaluation.COCOEvaluator) can evaluate AP (Average Precision) for box detection, instance segmentation and keypoint detection.\n* [**SemSegEvaluator**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/evaluation.html#detectron2.evaluation.SemSegEvaluator) can evaluate semantic segmentation metrics.\n\nAfterwards we'll use the [**build_detection_test_loader**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/data.html?highlight=build_detection_test_loader#detectron2.data.build_detection_test_loader) which returns a torch DataLoader, that loads the given detection dataset.\n\nAt last we'll use the model, evaluated and dataloader within the [inference_on_dataset](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/evaluation.html#detectron2.evaluation.inference_on_dataset) function. It runs the model on the dataloader and evaluates the metric with the evaluator.","78432b65":"Now that we know each label through the metadata. Let's visualize the result of the pretrained model from the modelzoo. For this we'll use the [**Visualizer**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/utils.html#module-detectron2.utils.visualizer) class.","6fc1b118":"# That's all for now!\n\nThank you for reading this notebook! If you enjoyed it, please upvote!\n\n*More coming soon!*","be038d5f":"For example you can find which objects it can recognize:","8b1f50dd":"<a id=\"semantic\" ><\/a>\n## 5.3. Semantic, Densepose, ...\n\nWill be added in a future version! Stay tuned!","f9fcb742":"<a id=trainmodel > <\/a>\n# 4. Train on a custom dataset\n\nAlright so it's pretty easy to run an existing model. Let's now train the model with our own data!\n\n<a id=downloaddataset > <\/a>\n## 4.1. Download the dataset\n\nWithout data we are nothing. So let's download our dataset!","4aea2cb4":"<a id=\"othermodels\" ><\/a>\n# 5. Other models\n\nIt's possible to use other high-end object detection models aswell. Let's check it out!\n\n<a id=\"keypoint\" ><\/a>\n## 5.1. Keypoint detection","506c6cb9":"Underneed you'll find the extra libraries we'll use in this notebook. More libraries will be added througout the notebook when needed.","1943072a":"<a id=\"panoptic\" ><\/a>\n## 5.2. Panoptic segmentation","eed3d92d":"The output of the predictions is saved within the outputs variable. See [model output format](https:\/\/detectron2.readthedocs.io\/tutorials\/models.html#model-output-format) for all the available functions. ","7ecfebdb":"Reload the data.","a0d9f7f8":"<a id=traincustom> <\/a>\n## 4.3. Training with a custom dataset\nLet's first check our training data! Ofcourse we'll use the **Visualizer** class again.","ee0a62e5":"# Object Detection with Detectron 2 - PyTorch \ud83d\udd25\ud83d\udd25\n\n![Detectron2](https:\/\/miro.medium.com\/max\/4000\/0*VbMjGBHMC6GnDKUp.png)\n\n**In this notebook we'll be checking out the new object detection framework \"Detectron2\" within PyTorch. It allows us to quickly build object detection models.**\n\n*Note: This notebook is a work in progress. I will keep on extending this notebook to fully explain and explore all the capabilities of the \"Detectron2\"- framework!*\n\n\n\n## Table of contents\n* 1.[Introduction](#intro)\n* 2.[Installing dependencies and libraries](#import)\n* 3.[Loading & using existing model](#loadmodel)\n    * 3.1.[The base data](#basedata)\n    * 3.2.[The model zoo](#modelzoo)\n    * 3.3.[Inference with a pretrained model](#pretrainedinference)\n* 4.[Train with custom data](#trainmodel)\n    * 4.1.[Download the dataset](#downloaddataset)\n    * 4.2.[Helper functions](#helperfunction)\n    * 4.3.[Training](#traincustom)\n    * 4.4.[Model evaluation](#modelevaluation)\n* 5.[Other models](#othermodels)\n    * 5.1.[Keypoint detection](#keypoint)\n    * 5.2.[Panoptic segmentation](#panoptic)\n    * 5.3.[Semantic, Densepose, ...](#semantic)\n* 6.[Video](#video)\n    * 6.1.[Libraries](#videolib)\n    * 6.2.[The video](#thevideo)\n    * 6.3.[Inferencing](#videoinference)","6ea41b0d":"<a id=\"modelevaluation\" ><\/a>\n## 4.4. Model evaluation\nLet's check out the performance of our model!\n\nFirst of all let's make some predictions! We're going to use the [**DefaultPredictor**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/engine.html?highlight=DefaultPredictor#detectron2.engine.defaults.DefaultPredictor) class. Ofcourse we'll use the same cfg that we used during training. We'll change two parameters for our inferencing.","11dc7ee2":"Let's check the result! \n\n*I've ran into some trouble with video encoding opencv and ffmpeg (fix in future version of this notebook).*","9946b4af":"<a id=\"video\" ><\/a>\n# 6. Video\n\nSo up until now we've been working with images only. Can we quickly use the models for videos? The answer is YES!\n\n<a id=\"videolib\" ><\/a>\n## 6.1. Libraries\nAs you can see we actually don't need many other libraries. Lets import a library to handle the video."}}