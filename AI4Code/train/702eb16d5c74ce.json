{"cell_type":{"227f8f5e":"code","2d1ffd19":"code","c18ac5c2":"code","f43b877d":"code","a83a0e09":"code","e12d6443":"code","61255d16":"code","d6b2d5f6":"code","5127d84c":"code","80da6b5d":"code","20d9c273":"code","ed97c635":"code","ab05df68":"code","c6c6a284":"code","ca3b2cbd":"code","b9bca30a":"code","502d752b":"code","f7540891":"code","207c11bd":"markdown","d008b7d2":"markdown","b187c60f":"markdown","c44b1332":"markdown","8f6626fe":"markdown","f0e1c525":"markdown","5728f4ba":"markdown","ac062c40":"markdown","5485de46":"markdown","3c1867c0":"markdown","bc832b08":"markdown","f1f606e0":"markdown","df2105c1":"markdown","13061ea3":"markdown","59a99fb0":"markdown"},"source":{"227f8f5e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfor dirname, _, filenames in os.walk('\/kaggle\/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Any results you write to the current directory are saved as output.","2d1ffd19":"import pandas as pd\ndata = pd.read_csv(\"..\/input\/word2vec-nlp-tutorial\/labeledTrainData.tsv\", header=0, \\\n delimiter=\"\\t\", quoting=3)\ndisplay(data.head())\nprint(data.shape)\nprint(data.columns)","c18ac5c2":"print (data[\"review\"][3])","f43b877d":"from bs4 import BeautifulSoup \nexample1 = BeautifulSoup(data[\"review\"][3]) \nprint (example1.get_text())","a83a0e09":"import re\nletters_only=re.sub(\"[^a-zA-Z]\",\" \",example1.get_text())\nprint (letters_only)","e12d6443":"lower_case = letters_only.lower()\nwords = lower_case.split()\nclean_text=(\" \".join(words))\nprint(clean_text)","61255d16":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nprint (stopwords.words(\"english\") )\n","d6b2d5f6":"words = [w for w in words if not w in (stopwords.words(\"english\"))]\nprint(words)","5127d84c":"def review_to_words( raw_review ):\n    review_text=BeautifulSoup(raw_review).get_text()\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n    words = letters_only.lower().split()\n    stops = set(stopwords.words(\"english\"))                  \n    meaningful_words = [w for w in words if not w in stops] \n    return( \" \".join( meaningful_words )) ","80da6b5d":"num_reviews = data[\"review\"].size\n\nclean_reviews = []\nfor i in range( 0, num_reviews ):\n clean_reviews.append( review_to_words(data[\"review\"][i] ) )\nprint(num_reviews)","20d9c273":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(analyzer = \"word\", \\\ntokenizer = None, \\\n preprocessor = None, \\\n stop_words = None, \\\n max_features = 5000) \ntrain_data_features = vectorizer.fit_transform(clean_reviews)\ntrain_data_features = train_data_features.toarray()\nprint(train_data_features.shape)","ed97c635":"X = train_data_features\ny = data.sentiment \n'''from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler().fit(X)\nX=scaler.transform(X)'''","ab05df68":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=130)","c6c6a284":"'''from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\ngrid={\"C\":[0.1,0.3,1,3,10]}\nLR=LogisticRegression()\nLRgrid=GridSearchCV(LR,grid,cv=10)\nLRgrid.fit(X_train,y_train)\n\nprint(\"\u041b\u0443\u0447\u0448\u0438\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b: ) \",LR.best_params_)\nprint(\"\u041b\u0443\u0447\u0448\u0438\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: :\",LR.best_score_)'''","ca3b2cbd":"from sklearn.linear_model import LogisticRegression\na=[0.1,0.3,1,3,10]\nfor i in range (5):\n    LR = LogisticRegression(C=a[i])\n    LR.fit(X_train, y_train)\n    print(\"\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438: C=\",a[i], LR.score(X_test, y_test))","b9bca30a":"LR = LogisticRegression(C=0.1)\nLR.fit(X_train, y_train)\nprint(\"\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438: C=0.1\", LR.score(X_test, y_test))","502d752b":"from sklearn.naive_bayes import GaussianNB\nNB = GaussianNB()\nNB.fit(X_train,y_train)\nprint(\"\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438: \", NB.score(X_test, y_test))\nNB.fit(X_test, y_test)\nprint(\"\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438(\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0435): \", NB.score(X_test, y_test))","f7540891":"#\u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\ntest = pd.read_csv(\"..\/input\/word2vec-nlp-tutorial\/testData.tsv\", header=0, delimiter=\"\\t\", \\\n quoting=3 )\n\nnum_reviews = len(test[\"review\"])\nclean_test_reviews = [] \nfor i in range(0,num_reviews):\n if( (i+1) % 1000 == 0 ):\n     print(\"Review %d of %d\\n\" % (i+1, num_reviews))\n clean_review = review_to_words( test[\"review\"][i] )\n clean_test_reviews.append( clean_review )\ntest_data_features = vectorizer.transform(clean_test_reviews)\ntest_data_features = test_data_features.toarray()\nresult = LR.predict(test_data_features)\n\noutput = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n# \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c\noutput.to_csv( \"..kaggle\/workong\/Bag_of_Words_model.csv\", index=False, quoting=3 )","207c11bd":"\u0412 \u0442\u0435\u043a\u0441\u0442\u0435 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442 HTML \u0442\u0435\u0433\u0438. \u0418\u0437\u0431\u0430\u0432\u0438\u043c\u0441\u044f \u043e\u0442 \u043d\u0438\u0445","d008b7d2":"\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435","b187c60f":"\u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0441\u043b\u043e\u0432\u0430 \u0432 \u043d\u0438\u0436\u043d\u0438\u0439 \u0440\u0435\u0433\u0438\u0441\u0442\u0440 \u0438 \u0441\u043d\u043e\u0432\u0430 \u0441\u043e\u0435\u0434\u0438\u043d\u044f\u0435\u043c \u0432 \u0441\u0442\u0440\u043e\u043a\u0443","c44b1332":"GridSearch+\u041b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f","8f6626fe":"**Bag of Words Meets Bags of Popcorn**","f0e1c525":"\u0423\u0434\u0430\u043b\u0435\u043d\u0438\u0435 \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432","5728f4ba":"\u0412\u0437\u0433\u043b\u044f\u043d\u0435\u043c \u043d\u0430 \u043a\u0430\u043a\u043e\u0439-\u043d\u0438\u0431\u0443\u0434\u044c \u0438\u0437 \u043e\u0431\u0437\u043e\u0440\u043e\u0432([\u043d\u043e\u043c\u0435\u0440 \u0442\u0435\u043a\u0441\u0442\u0430])","ac062c40":"\u041b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f","5485de46":"\u0422\u0435\u043f\u0435\u0440\u044c \u0443\u0434\u0430\u043b\u0438\u043c \u0437\u043d\u0430\u043a\u0438 \u043f\u0440\u0435\u043f\u0438\u043d\u0430\u043d\u0438\u044f","3c1867c0":"\u041b\u0443\u0447\u0448\u0435 \u0421=0.1","bc832b08":"\u041d\u0430\u0438\u0432\u043d\u044b\u0439 \u0411\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440","f1f606e0":"\u041b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u043b\u0443\u0447\u0448\u0435. \u0411\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0435\u0451 \u0434\u043b\u044f \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445.","df2105c1":"\u041f\u0440\u043e\u0432\u0435\u0434\u0451\u043c \u0447\u0438\u0441\u0442\u043a\u0443 \u0432\u0441\u0435\u0445 \u043e\u0442\u0437\u044b\u0432\u043e\u0432","13061ea3":"\u0420\u0430\u0437\u0434\u0435\u043b\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u0435 \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0435\n","59a99fb0":"\u041e\u0431\u044a\u0435\u0434\u0438\u043d\u0438\u0432 \u0432\u0441\u0451 \u0432 \u043e\u0434\u043d\u0443 \u0444\u0443\u043d\u043a\u0446\u0438\u044e"}}