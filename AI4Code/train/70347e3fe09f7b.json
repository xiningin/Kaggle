{"cell_type":{"36d49db0":"code","d2495df7":"code","b691193b":"code","1176c8e6":"code","87ac432e":"code","b27d7c25":"code","668647d1":"code","80e66c4d":"code","808071fc":"code","4e1dad24":"code","a621ec07":"code","69553b44":"code","5bd14ee3":"code","471ce664":"code","7d9b29cb":"code","d36cef9f":"code","971c6f3d":"code","08129f94":"code","d8f12c2a":"code","2ec8bad2":"code","0b492ab2":"code","a5f22261":"code","7b51c6e0":"code","e257ef3c":"code","de03e8b0":"code","699916ed":"code","cff238bc":"code","3aff9c46":"code","630c9219":"code","76d64446":"code","d799d27b":"code","8827360c":"code","247ee5e6":"code","04473f11":"code","a92f124e":"code","a7f7f086":"code","8e41882a":"markdown","45556b53":"markdown","9921fff7":"markdown","e4b429a2":"markdown","10179593":"markdown","21272460":"markdown","62551832":"markdown","a4feb59f":"markdown","a168c13d":"markdown","827adcd8":"markdown","26ab5e6d":"markdown","13187058":"markdown","8b81a3d2":"markdown","61547fbe":"markdown","0a5adc79":"markdown","d212d0ce":"markdown","1f3080c4":"markdown","ac5c2537":"markdown","67aacdda":"markdown","66200072":"markdown","2dec117e":"markdown","54460dce":"markdown","9ec0a6b4":"markdown","18c1dd3e":"markdown","b13b88f0":"markdown"},"source":{"36d49db0":"import math\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import stats, norm, skew\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, ElasticNet, Lasso\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom keras.models import Sequential\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.utils.np_utils import to_categorical\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom scipy.special import boxcox1p\nimport lightgbm as lgb\nimport xgboost as xgb\n\n%matplotlib inline\nnp.random.seed(2)","d2495df7":"# Load the data\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\nprint(train.shape)\nprint(test.shape)","b691193b":"train = train.drop('Id', axis=1)\ntrain.columns\n","1176c8e6":"cols_with_none_as_nan = [\n    \"PoolQC\", \n    \"MiscFeature\", \n    \"Alley\", \n    \"Fence\", \n    \"FireplaceQu\", \n    'GarageType', \n    'GarageFinish', \n    'GarageQual', \n    'GarageCond', \n    'BsmtQual', \n    'BsmtCond', \n    'BsmtExposure', \n    'Electrical', \n    'BsmtFinType1', \n    'BsmtFinType2', \n    \"MasVnrType\",\n    'MSZoning',\n    'Utilities',\n    'Exterior1st',\n    'Exterior2nd',\n    'KitchenQual',\n    'Functional',\n    'SaleType']\n\n\no = train.dtypes[train.dtypes==object].index\nprint(o)\n\n# fill missing text fields with a default string\nobject_columns = train[o]\ntest_object_columns = test[o]\n\n# for these colunms the string 'None' will be inserted in place of nan\nfor col in cols_with_none_as_nan:\n    object_columns.loc[:, col] = object_columns.loc[:, col].fillna('None')\n    test_object_columns.loc[:, col] = test_object_columns.loc[:, col].fillna('None')\n\nremaining_fix = object_columns.isnull().sum()\nprint('Fixes remaining on train set\\n', remaining_fix[remaining_fix>0])\n\nremaining_fix = test_object_columns.isnull().sum()\nprint('Fixes remaining on test set\\n',remaining_fix[remaining_fix>0])","87ac432e":"# Base scale, taken from 'OverallCond' feature\n#   10   Very Excellent\n#   9    Excellent\n#   8    Very Good\n#   7    Good\n#   6    Above Average   \n#   5    Average\n#   4    Below Average   \n#   3    Fair\n#   2    Poor\n#   1    Very Poor\n\n#   Ex   Excellent (100+ inches) \n#   Gd   Good (90-99 inches)\n#   TA   Typical (80-89 inches)\n#   Fa   Fair (70-79 inches)\n#   Po   Poor (&lt;70 inches\n#   NA   No Basement\n\nquals_mapping = {\n    \"Ex\": 9,\n    \"Gd\": 7,\n    \"Av\": 5,\n    \"TA\": 5,\n    \"Fa\": 3,\n    \"Po\": 2,\n    \"NA\": 0,\n    \"None\": 0\n}\nquals_columns = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC', ]\n\nfor c in object_columns.columns:\n    if c in quals_columns:\n        vals = np.unique(object_columns[c])\n        for v in quals_mapping:\n            object_columns.loc[:,c] = object_columns[c].replace(to_replace=v, value=quals_mapping[v])\n            test_object_columns.loc[:,c] = test_object_columns[c].replace(to_replace=v, value=quals_mapping[v])\n","b27d7c25":"numeric_columns = train.select_dtypes(include=[int, float])\n\nremaining_fix = numeric_columns.isnull().sum()\nprint('Fixes remaining on train set\\n',remaining_fix[remaining_fix>0])\n\ntest_numeric_columns = test.select_dtypes(include=[int, float])\n\nremaining_fix = test_numeric_columns.isnull().sum()\nprint('Fixes remaining on test set\\n',remaining_fix[remaining_fix>0])","668647d1":"cols_with_zero_as_nan = ['MasVnrArea', \n                         'GarageYrBlt',\n                         'BsmtFinSF1',\n                         'BsmtFinSF2',\n                         'BsmtUnfSF',\n                         'TotalBsmtSF',\n                         'BsmtFullBath',\n                         'BsmtHalfBath',\n                         'GarageCars',\n                         'GarageArea',\n                        ]\ncols_with_mean_as_nan = ['LotFrontage']\n\n# for these colunms the mean will be inserted in place of nan\nfor col in cols_with_mean_as_nan:\n    numeric_columns.loc[:, col] = numeric_columns.loc[:, col].fillna(numeric_columns[col].mean())\n    test_numeric_columns.loc[:, col] = test_numeric_columns.loc[:, col].fillna(test_numeric_columns[col].mean())\n\n# for these colunms a zero will be inserted in place of nan\nfor col in cols_with_zero_as_nan:\n    numeric_columns.loc[:, col] = numeric_columns.loc[:, col].fillna(0)\n    test_numeric_columns.loc[:, col] = test_numeric_columns.loc[:, col].fillna(0)\n\n\nremaining_fix = numeric_columns.isnull().sum()\nprint('Fixes remaining on train set\\n',remaining_fix[remaining_fix>0])\n\nremaining_fix = test_numeric_columns.isnull().sum()\nprint('Fixes remaining on test set\\n',remaining_fix[remaining_fix>0])","80e66c4d":"plt.figure(figsize=(10,5))\nsns.scatterplot(x=numeric_columns.loc[:,'OverallQual'], y=np.log1p(train['SalePrice']))\nplt.show()\n","808071fc":"for d in numeric_columns[numeric_columns['OverallQual'] == 4].index:\n    if np.log1p(numeric_columns.loc[d,'SalePrice']) < 10.8:\n        numeric_columns = numeric_columns.drop(d, axis=0)\n        object_columns = object_columns.drop(d, axis=0)\n\nplt.figure(figsize=(10,5))\nsns.scatterplot(x=numeric_columns.loc[:,'OverallQual'], y=np.log1p(train['SalePrice']))\nplt.show()\n","4e1dad24":"plt.figure(figsize=(10,5))\nsns.scatterplot(x=numeric_columns['GrLivArea'], y=np.log1p(train['SalePrice']))\nplt.show()\n","a621ec07":"for d in numeric_columns[numeric_columns['GrLivArea'] > 4000].index:\n    numeric_columns = numeric_columns.drop(d, axis=0)\n    object_columns = object_columns.drop(d, axis=0)\n\nplt.figure(figsize=(10,5))\nsns.scatterplot(x=numeric_columns['GrLivArea'], y=np.log1p(train['SalePrice']))\nplt.show()\n","69553b44":"\nstrings = np.array([])\nfor c in object_columns.columns:\n    if c not in quals_columns:\n        strings = np.append(strings, pd.unique(object_columns[c].values))\n\nfor c in test_object_columns.columns:\n    if c not in quals_columns:\n        strings = np.append(strings, pd.unique(test_object_columns[c].values))\n\nprint(len(strings), 'distinct labels generated')\n\nlabeler = LabelEncoder()\nlabeler.fit(strings)\n\nfor c in object_columns.columns:\n    if c not in quals_columns:\n        object_columns.loc[:,c] = labeler.transform(object_columns.loc[:,c])\n        test_object_columns.loc[:,c] = labeler.transform(test_object_columns.loc[:,c])\n\nobject_columns[0:5]","5bd14ee3":"# final train dataset\ntrain_ds = object_columns.join(numeric_columns)\ntrain_ds[\"SalePrice\"] = np.log1p(train_ds[\"SalePrice\"])\n\n# test dataset\ntest_ds = test_object_columns.join(test_numeric_columns)\n\n","471ce664":"# keeping only the most promising features\ncorrmat = train_ds.corr()['SalePrice']\nbest_columns = corrmat[abs(corrmat) > 0.01].index\n\nprint('Keeping only ', len(best_columns), 'features out of ', len(train_ds.columns))\n\ntrain_ds = train_ds[best_columns]\ntest_ds = test_ds[best_columns.drop('SalePrice')]\n","7d9b29cb":"X_train = (train_ds.values[:,:-1])\ny_train = np.asarray([[t] for t in (train_ds.values[:,-1])])\nX_test = test_ds.values\n\nprint('Training set features shape', X_train.shape)\nprint('Training set labels shape', y_train.shape)\nprint('Test set shape', test_ds.shape)\n","d36cef9f":"print(test_ds.dtypes[test_ds.dtypes==object])\n\nfrom sklearn.preprocessing import RobustScaler\ntransformer = RobustScaler().fit(X_train)\nX_train = pd.DataFrame(transformer.transform(X_train))\nX_test = pd.DataFrame(transformer.transform(X_test))\n","971c6f3d":"print('Training set features shape', X_train.shape)\nprint('Training set labels shape', y_train.shape)\n","08129f94":"def train_model(depth, learning_rate, n_estimators, model_type='xgb'):\n    if model_type == 'xgb':\n        print('XGBRegressor')\n        model = xgb.XGBRegressor(learning_rate=learning_rate, max_depth=depth, \n                                 n_estimators=n_estimators,\n                                 silent=1,\n                                 random_state =7, nthread = -1)\n    if model_type == 'lgb':\n        print('LGBMRegressor')\n        model = lgb.LGBMRegressor(learning_rate=learning_rate, max_depth=depth, \n                                 n_estimators=n_estimators,\n                                 silent=1,\n                                 random_state =7, nthread = -1)\n    if model_type == 'lasso':\n        print('Lasso')\n        model = Lasso(alpha =0.0006)\n        \n    if model_type == 'elastic':\n        print('ElasticNet')\n        model = ElasticNet(alpha=0.0012, l1_ratio=.5)\n        \n    score = rmsle_cv(model)\n    print(model_type, \" score: depth={:d} lr={:.2f} est={:d} -> mean:{:.5f} std:{:.4f}\".format(depth, learning_rate, n_estimators, score.mean(), score.std()))\n    return score\n\n#Validation function\nn_folds = 10\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train)\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_train.flatten(), scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n","d8f12c2a":"depth = 0\nlearning_rate = 0\nn_estimators = 0\n\nscore = train_model(depth, learning_rate\/100, n_estimators, model_type='lasso')\n","2ec8bad2":"# train with the best parameters\nmodel_lasso = Lasso(alpha=0.0006)\n\nscore = rmsle_cv(model_lasso)\nprint(\"model_lr score: depth={:d} lr={:.2f} est={:d} -> mean:{:.5f} std:{:.4f}\".format(depth, learning_rate, n_estimators, score.mean(), score.std()))\nmodel_lasso.fit(X_train, y_train)\ny_pred = model_lasso.predict(X_train)\nprint('RMSLE LR = ', rmsle(y_train, y_pred))\n","0b492ab2":"plt.figure(figsize=(8, 8))\ny_pred = model_lasso.predict(X_train)\nplt.scatter(np.expm1(y_train), np.expm1(y_pred))\nplt.xlim(0, 600_000)\nplt.ylim(0, 600_000)\nplt.xlabel('actual', fontsize=12)\nplt.ylabel('predicted', fontsize=12)\nplt.grid()\nplt.plot([(0, 0), (1_000_000, 1_000_000)], [(0, 0), (1_000_000, 1_000_000)])\nplt.show()\n","a5f22261":"score = train_model(depth, learning_rate\/100, n_estimators, model_type='elastic')\n","7b51c6e0":"# train with the best parameters\nmodel_elastic = ElasticNet(alpha=0.0005, l1_ratio=.5)\n\nscore = rmsle_cv(model_elastic)\nprint(\"model_elastic score: depth={:d} lr={:.2f} est={:d} -> mean:{:.5f} std:{:.4f}\".format(depth, learning_rate, n_estimators, score.mean(), score.std()))\nmodel_elastic.fit(X_train, y_train)\ny_pred = model_elastic.predict(X_train)\nprint('RMSLE model_elastic = ', rmsle(y_train, y_pred))\n","e257ef3c":"plt.figure(figsize=(8, 8))\ny_pred = model_elastic.predict(X_train)\nplt.scatter(np.expm1(y_train), np.expm1(y_pred))\nplt.xlim(0, 600_000)\nplt.ylim(0, 600_000)\nplt.xlabel('actual', fontsize=12)\nplt.ylabel('predicted', fontsize=12)\nplt.grid()\nplt.plot([(0, 0), (1_000_000, 1_000_000)], [(0, 0), (1_000_000, 1_000_000)])\nplt.show()\n","de03e8b0":"\nresult = []\nfor depth in range(3, 5):\n    for learning_rate in range(2, 3, 1):\n        for n_estimators in range(1200, 2000, 200):\n            score = train_model(depth, learning_rate\/100, n_estimators, model_type='xgb')\n            result.append([depth, learning_rate\/100, n_estimators, score.mean(), score.std()])\n","699916ed":"# check results\nresult = pd.DataFrame(result, columns=['depth', 'learning_rate', 'n_estimators', 'score_mean', 'score_std'])\nresult.describe()\n","cff238bc":"best = np.argmin(result['score_mean'].values)\nprint('Best params = \\n', result.iloc[best])\n","3aff9c46":"# train with the best parameters\ndepth = int(result.iloc[best]['depth'])\nlearning_rate = result.iloc[best]['learning_rate']\nn_estimators = int(result.iloc[best]['n_estimators'])\n\nmodel_xgb = xgb.XGBRegressor(learning_rate=learning_rate, max_depth=depth, \n                         n_estimators=n_estimators,\n                         silent=1,\n                         random_state =7, nthread = -1)\n\nscore = rmsle_cv(model_xgb)\nprint(\"Xgboost score: depth={:d} lr={:.2f} est={:d} -> mean:{:.5f} std:{:.4f}\".format(depth, learning_rate, n_estimators, score.mean(), score.std()))\nmodel_xgb.fit(X_train, y_train)\ny_pred = model_xgb.predict(X_train)\nprint('RMSLE XGB = ', rmsle(y_train, y_pred))\n","630c9219":"plt.figure(figsize=(8, 8))\ny_pred = model_xgb.predict(X_train)\nplt.scatter(np.expm1(y_train), np.expm1(y_pred))\nplt.xlim(0, 600_000)\nplt.ylim(0, 600_000)\nplt.xlabel('actual', fontsize=12)\nplt.ylabel('predicted', fontsize=12)\nplt.grid()\nplt.plot([(0, 0), (1_000_000, 1_000_000)], [(0, 0), (1_000_000, 1_000_000)])\nplt.show()\n","76d64446":"result = []\nfor depth in range(3, 5):\n    for learning_rate in range(2, 4, 1):\n        for n_estimators in range(1200, 2000, 200):\n            score = train_model(depth, learning_rate\/100, n_estimators, model_type='lgb')\n            result.append([depth, learning_rate\/100, n_estimators, score.mean(), score.std()])\n","d799d27b":"# check results\nresult = pd.DataFrame(result, columns=['depth', 'learning_rate', 'n_estimators', 'score_mean', 'score_std'])\nresult.describe()\n","8827360c":"best = np.argmin(result['score_mean'].values)\nprint('Best params = \\n', result.iloc[best])\n","247ee5e6":"# train with the best parameters\ndepth = int(result.iloc[best]['depth'])\nlearning_rate = result.iloc[best]['learning_rate']\nn_estimators = int(result.iloc[best]['n_estimators'])\n\nmodel_lgb = lgb.LGBMRegressor(learning_rate=learning_rate, max_depth=depth, \n                         n_estimators=n_estimators,\n                         silent=1,\n                         random_state =7, nthread = -1)\n\n\nscore = rmsle_cv(model_lgb)\nprint(\"lgb score: depth={:d} lr={:.2f} est={:d} -> mean:{:.5f} std:{:.4f}\".format(depth, learning_rate, n_estimators, score.mean(), score.std()))\n\nmodel_lgb.fit(X_train, y_train.flatten())\ny_pred = model_lgb.predict(X_train)\n\nprint('RMSLE LGB = ', rmsle(y_train, y_pred))\n","04473f11":"\nplt.figure(figsize=(8, 8))\nplt.scatter(np.expm1(y_train), np.expm1(y_pred))\nplt.xlim(0, 600_000)\nplt.ylim(0, 600_000)\nplt.xlabel('actual', fontsize=12)\nplt.ylabel('predicted', fontsize=12)\nplt.plot([(0, 0), (1_000_000, 1_000_000)], [(0, 0), (1_000_000, 1_000_000)])\nplt.show()\n","a92f124e":"\navg_predicts = np.column_stack([\n    model_xgb.predict(X_train),\n    model_lgb.predict(X_train),\n    model_lasso.predict(X_train),\n    model_elastic.predict(X_train)\n])\n\navg_predict = [np.mean(p) for p in avg_predicts]\ny_pred = avg_predict\n\nplt.figure(figsize=(8, 8))\nplt.scatter(np.expm1(y_train), np.expm1(y_pred))\nplt.xlim(0, 600_000)\nplt.ylim(0, 600_000)\nplt.xlabel('actual', fontsize=12)\nplt.ylabel('predicted', fontsize=12)\nplt.plot([(0, 0), (1_000_000, 1_000_000)], [(0, 0), (1_000_000, 1_000_000)])\nplt.show()\n\nprint('RMSLE LGB = ', rmsle(y_train, y_pred))\n\n","a7f7f086":"# Submission\n\navg_predicts = np.column_stack([\n    model_xgb.predict(X_test),\n    model_lgb.predict(X_test),\n    model_lasso.predict(X_test),\n    model_elastic.predict(X_test)\n])\n\n\navg_predict = [np.mean(p) for p in avg_predicts]\n\nsubm_predict = np.expm1(avg_predict)\n\ndsubm_predict = pd.DataFrame(subm_predict)\ndsubm_predict['Id'] = test.values[:,0]\ndsubm_predict.columns = ['SalePrice', 'Id']\n\ndsubm_predict.to_csv('submission.csv', index=False)\ndsubm_predict[0:10]\n","8e41882a":"# Averaged model","45556b53":"#### RobustScaler","9921fff7":"# Training: Lasso\n","e4b429a2":"# Training: ElasticNet\n","10179593":"### 2: numerics","21272460":"```\nBest params = \n depth               3.000000\nlearning_rate       0.030000\nn_estimators     1800.000000\nscore_mean          0.112694\nscore_std           0.016205\nName: 7, dtype: float64\n```","62551832":"# Filling NaNs\n\n## TODO: spend some more time on these columns","a4feb59f":"# Best score on kaggle: 0.12174","a168c13d":"* model_elastic score: depth=0 lr=0.00 est=0 -> mean:0.11709 std:0.0111\n* RMSLE model_elastic =  0.10947185086398303","827adcd8":"# Training: lightgbm\n","26ab5e6d":"* Xgboost score: depth=3 lr=0.03 est=1800 -> mean:0.11269 std:0.0162\n* RMSLE XGB =  0.04269559832775177\n","13187058":"```\nBest params = \n depth               3.000000\nlearning_rate       0.030000\nn_estimators     1200.000000\nscore_mean          0.114307\nscore_std           0.015887\nName: 4, dtype: float64\n```","8b81a3d2":"# Averaged XGBoost and LightGBM model\n---","61547fbe":"* model_lr score: depth=0 lr=0.00 est=0 -> mean:0.11706 std:0.0111\n* RMSLE LR =  0.10950991422359595\n","0a5adc79":"# Target: RMSLE LGB =  0.06539346038526068\n","d212d0ce":"## Removing outliers in OverallQual and SalePrice","1f3080c4":"# Assembling the datasets","ac5c2537":"## Training functions","67aacdda":"In order to make the category influence the predicted price with the correct amount, I'm going to exchange each category with the mean saleprice value for all the entries with that category.","66200072":"### Discarding less correlated features","2dec117e":"### 1: strings","54460dce":"# Training: XGBoost\n","9ec0a6b4":"# From string to ordinal\nMany categorical features contain **ordinal** values; the chart that follows shows that in most cases, the category influences the SalePrice label.","18c1dd3e":"# Enconding string columns with LabelEncoder","b13b88f0":"* lgb score: depth=3 lr=0.03 est=1200 -> mean:0.11431 std:0.0159\n* RMSLE LGB =  0.0618131706199214\n"}}