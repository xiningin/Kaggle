{"cell_type":{"a306d516":"code","921e6c38":"code","097c9e61":"code","75c8ec3a":"code","851d39a3":"code","a018f4d3":"code","7ef4d6ca":"code","5de0fdde":"code","5ea74058":"code","78961bf0":"code","bb733f1b":"code","2fe75167":"code","7e53caa7":"code","bce2fc4b":"code","09e2c072":"code","a0299dc5":"code","39169803":"code","1f428300":"code","189b553c":"code","a8bd4e77":"code","c6ebe751":"code","eb4a5778":"code","f4b1b0a5":"code","cea2d829":"code","bb5ef736":"code","215c9ddb":"code","53acebe2":"code","4b7f8caf":"code","3e376333":"code","3dd1a665":"code","e85ae335":"code","3f9acd12":"code","4f27f360":"code","705bd07e":"code","0d057c7a":"code","4b2cec62":"code","385d1070":"code","a6db23bd":"code","eaa16349":"code","253cc5af":"code","ca041079":"code","9d041ac3":"code","92cae109":"code","dda9a25b":"code","f12b5e47":"code","8a5d67f6":"markdown","c196e2e1":"markdown","9e731eee":"markdown","3b2b8c1a":"markdown","9bf79ef4":"markdown","41b61637":"markdown","fe4e0b9f":"markdown","58d6d700":"markdown","99418c49":"markdown","3b864cdc":"markdown","face8b02":"markdown","13621614":"markdown","bcea2d15":"markdown","f970bd9d":"markdown","b85eb721":"markdown","33f33327":"markdown","c2ddda29":"markdown","c3ce117f":"markdown","8f27df12":"markdown","99956372":"markdown","0129c201":"markdown","0b8dc5d6":"markdown","0a89ed5e":"markdown"},"source":{"a306d516":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","921e6c38":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom keras.preprocessing import text, sequence\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nimport keras\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,Dropout\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model","097c9e61":"df = pd.read_json(\"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\ndf.head()","75c8ec3a":"df.isna().sum() # Checking for NaN values","851d39a3":"del df['article_link'] # Deleting this column as it is of no use","a018f4d3":"df.head()","7ef4d6ca":"sns.set_style(\"dark\")\nsns.countplot(df.is_sarcastic)","5de0fdde":"stop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)","5ea74058":"def strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#Removing the square brackets\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n# Removing URL's\ndef remove_between_square_brackets(text):\n    return re.sub(r'http\\S+', '', text)\n#Removing the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\n#Removing the noisy text\ndef denoise_text(text):\n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    text = remove_stopwords(text)\n    return text\n#Apply function on review column\ndf['headline']=df['headline'].apply(denoise_text)","78961bf0":"plt.figure(figsize = (20,20)) # Text that is Not Sarcastic\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.is_sarcastic == 0].headline))\nplt.imshow(wc , interpolation = 'bilinear')","bb733f1b":"plt.figure(figsize = (20,20)) # Text that is Sarcastic\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.is_sarcastic == 1].headline))\nplt.imshow(wc , interpolation = 'bilinear')","2fe75167":"words = []\nfor i in df.headline.values:\n    l = []\n    for j in i.split():\n        l.extend(i.split())\n        break\n    words.append(l)    ","7e53caa7":"import gensim\n#Dimension of vectors we are generating\nEMBEDDING_DIM = 100\n\n#Creating Word Vectors by Word2Vec Method (takes time...)\nw2v_model = gensim.models.Word2Vec(sentences = words , size=EMBEDDING_DIM , window = 5 , min_count = 1)","bce2fc4b":"#vocab size\nlen(w2v_model.wv.vocab)\n#We have now represented each of 38071 words by a 100dim vector.","09e2c072":"# For determining size of input...\n\n# Making histogram for no of words in news shows that all news article are under 20 words.\n# Lets keep each news small and truncate all news to 20 while tokenizing\nplt.hist([len(j) for j in words], bins = 100)\nplt.show()","a0299dc5":"tokenizer = text.Tokenizer(num_words=35000)\ntokenizer.fit_on_texts(words)\ntokenized_train = tokenizer.texts_to_sequences(words)\nx = sequence.pad_sequences(tokenized_train, maxlen = 20)","39169803":"# Adding 1 because of reserved 0 index\n# Embedding Layer creates one more vector for \"UNKNOWN\" words, or padded words (0s). This Vector is filled with zeros.\n# Thus our vocab size inceeases by 1\nvocab_size = len(tokenizer.word_index) + 1","1f428300":"# Function to create weight matrix from word2vec gensim model\ndef get_weight_matrix(model, vocab):\n    # total vocabulary size plus 0 for unknown words\n    vocab_size = len(vocab) + 1\n    # define weight matrix dimensions with all 0\n    weight_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n    # step vocab, store vectors using the Tokenizer's integer mapping\n    for word, i in vocab.items():\n        weight_matrix[i] = model[word]\n    return weight_matrix","189b553c":"#Getting embedding vectors from word2vec and usings it as weights of non-trainable keras embedding layer\nembedding_vectors = get_weight_matrix(w2v_model, tokenizer.word_index)","a8bd4e77":"#Defining Neural Network\nmodel = Sequential()\n#Non-trainable embeddidng layer\nmodel.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=20, trainable=True))\n#LSTM \nmodel.add(LSTM(units=128))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n\ndel embedding_vectors","c6ebe751":"model.summary()","eb4a5778":"x_train, x_test, y_train, y_test = train_test_split(x, df.is_sarcastic , test_size = 0.3 , random_state = 0) ","f4b1b0a5":"history = model.fit(x_train, y_train, batch_size = 128 , validation_data = (x_test,y_test) , epochs = 5)","cea2d829":"print(\"Accuracy of the model on Training Data is - \" , model.evaluate(x_train,y_train)[1]*100)\nprint(\"Accuracy of the model on Testing Data is - \" , model.evaluate(x_test,y_test)[1]*100)","bb5ef736":"epochs = [i for i in range(5)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['acc']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_acc']\nval_loss = history.history['val_loss']\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\nax[0].set_title('Training & Testing Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')\nax[1].set_title('Training & Testing Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Loss\")\nplt.show()","215c9ddb":"pred = model.predict_classes(x_test)\npred[:5]","53acebe2":"cm = confusion_matrix(y_test,pred)\ncm","4b7f8caf":"cm = pd.DataFrame(cm , index = ['Not Sarcastic','Sarcastic'] , columns = ['Not Sarcastic','Sarcastic'])\nplt.figure(figsize = (10,10))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['Not Sarcastic','Sarcastic'] , yticklabels = ['Not Sarcastic','Sarcastic'])","3e376333":"x_train,x_test,y_train,y_test = train_test_split(df.headline,df.is_sarcastic, test_size = 0.3 , random_state = 0)","3dd1a665":"max_features = 10000\nmaxlen = 200","e85ae335":"tokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(x_train)\ntokenized_train = tokenizer.texts_to_sequences(x_train)\nx_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)","3f9acd12":"tokenized_test = tokenizer.texts_to_sequences(x_test)\nX_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)","4f27f360":"EMBEDDING_FILE = '..\/input\/glove-twitter\/glove.twitter.27B.200d.txt'","705bd07e":"def get_coefs(word, *arr): \n    return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))","0d057c7a":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n#change below line if computing normal stats is too slow\nembedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","4b2cec62":"batch_size = 128\nepochs = 5\nembed_size = 200","385d1070":"#Defining Neural Network\nmodel = Sequential()\n#Non-trainable embeddidng layer\nmodel.add(Embedding(max_features, output_dim=embed_size, weights=[embedding_matrix], input_length=maxlen, trainable=True))\n#LSTM \nmodel.add(LSTM(units=128))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","a6db23bd":"model.summary()\ntf.keras.utils.plot_model(model,to_file='mymodel.png')","eaa16349":"history = model.fit(x_train, y_train, batch_size = batch_size , validation_data = (X_test,y_test) , epochs = epochs)","253cc5af":"print(\"Accuracy of the model on Training Data is - \" , model.evaluate(x_train,y_train)[1]*100)\nprint(\"Accuracy of the model on Testing Data is - \" , model.evaluate(X_test,y_test)[1]*100)","ca041079":"epochs = [i for i in range(5)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\nax[0].set_title('Training & Testing Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')\nax[1].set_title('Training & Testing Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Loss\")\nplt.show()","9d041ac3":"pred = model.predict_classes(X_test)\npred[:5]","92cae109":"print(classification_report(y_test, pred, target_names = ['Not Sarcastic','Sarcastic']))","dda9a25b":"cm = confusion_matrix(y_test,pred)\ncm","f12b5e47":"cm = pd.DataFrame(cm , index = ['Not Sarcastic','Sarcastic'] , columns = ['Not Sarcastic','Sarcastic'])\nplt.figure(figsize = (10,10))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['Not Sarcastic','Sarcastic'] , yticklabels = ['Not Sarcastic','Sarcastic'])","8a5d67f6":"**Tokenizing Text -> Repsesenting each word by a number**\n\n**Mapping of orginal word to number is preserved in word_index property of tokenizer**\n\n**Tokenized applies basic processing like changing it to lower case, explicitely setting that as False**\n\n**Lets keep all news to 200, add padding to news with less than 200 words and truncating long ones**","c196e2e1":"**SEEMS LIKE THE MODEL IS OVERFITTING AND NOT PERFORMING WELL ON THE TEST DATA**","9e731eee":"# ANALYSIS AFTER TRAINING OF WORD2VEC MODEL ","3b2b8c1a":"# LOADING THE NECESSARY LIBRARIES","9bf79ef4":"# Introduction to GloVe\n**GloVe method is built on an important idea, You can derive semantic relationships between words from the co-occurrence matrix. Given a corpus having V words, the co-occurrence matrix X will be a V x V matrix, where the i th row and j th column of X, X_ij denotes how many times word i has co-occurred with word j. An example co-occurrence matrix might look as follows.**\n![image.png](attachment:image.png)\n**The co-occurrence matrix for the sentence \u201cthe cat sat on the mat\u201d with a window size of 1. As you probably noticed it is a symmetric matrix. How do we get a metric that measures semantic similarity between words from this? For that, you will need three words at a time. Let me concretely lay down this statement.**","41b61637":"**BASIC MODEL PARAMETERS**","fe4e0b9f":"**SO, WE CAN SEE THAT THE DATASET IS BALANCED**","58d6d700":"**PLS UPVOTE THIS NOTEBOOK IF YOU LIKE IT. THANKS FOR YOUR TIME!**","99418c49":"**THE ACCURACY IMPROVED FROM 79% TO 82%**","3b864cdc":"# DATA VISUALIZATION AND PREPROCESSING","face8b02":"**BASIC DATA CLEANING**","13621614":"![image.png](attachment:image.png)\n**The behavior of P_ik\/P_jk for various words Consider the entity P_ik\/P_jk where P_ik = X_ik\/X_i Here P_ik denotes the probability of seeing word i and k together, which is computed by dividing the number of times i and k appeared together (X_ik) by the total number of times word i appeared in the corpus (X_i). You can see that given two words, i.e. ice and steam, if the third word k (also called the \u201cprobe word\u201d), is very similar to ice but irrelevant to steam (e.g. k=solid), P_ik\/P_jk will be very high (>1), is very similar to steam but irrelevant to ice (e.g. k=gas), P_ik\/P_jk will be very small (<1), is related or unrelated to either words, then P_ik\/P_jk will be close to 1 So, if we can find a way to incorporate P_ik\/P_jk to computing word vectors we will be achieving the goal of using global statistics when learning word vectors.**","bcea2d15":"# TRAINING GLOVE EMBEDDINGS MODEL","f970bd9d":"**WORDCLOUD FOR SARCASTIC TEXT (LABEL - 1)**","b85eb721":"**WHAT ARE STOPWORDS?**\n\n**Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like the, he, have etc. Such words are already captured this in corpus named corpus. We first download it to our python environment.**","33f33327":"# Why do we need them?\n**Consider the following similar sentences: Have a good day and Have a great day. They hardly have different meaning. If we construct an exhaustive vocabulary (let\u2019s call it V), it would have V = {Have, a, good, great, day}.\nNow, let us create a one-hot encoded vector for each of these words in V. Length of our one-hot encoded vector would be equal to the size of V (=5). We would have a vector of zeros except for the element at the index representing the corresponding word in the vocabulary. That particular element would be one. The encodings below would explain this better.\nHave = [1,0,0,0,0] ; a=[1,0,0,0,0] ; good=[0,0,1,0,0] ; great=[0,0,0,1,0] ; day=[0,0,0,0,1] (represents transpose)\nIf we try to visualize these encodings, we can think of a 5 dimensional space, where each word occupies one of the dimensions and has nothing to do with the rest (no projection along the other dimensions). This means \u2018good\u2019 and \u2018great\u2019 are as different as \u2018day\u2019 and \u2018have\u2019, which is not true.**\n**Our objective is to have words with similar context occupy close spatial positions. Mathematically, the cosine of the angle between such vectors should be close to 1, i.e. angle close to 0.**\n![image.png](attachment:image.png)\n**Here comes the idea of generating distributed representations. Intuitively, we introduce some dependence of one word on the other words. The words in context of this word would get a greater share of this dependence. In one hot encoding representations, all the words are independent of each other, as mentioned earlier.**\n\n**Source Credits : https:\/\/towardsdatascience.com\/introduction-to-word-embedding-and-word2vec-652d0c2060fa **","c2ddda29":"**WORDCLOUD FOR TEXT THAT IS NOT SARCASTIC (LABEL - 0)**","c3ce117f":"# TRAINING WORD2VEC MODEL","8f27df12":"**Converting text to format acceptable by gensim**","99956372":"# OVERVIEW OF DATASET\n**Past studies in Sarcasm Detection mostly make use of Twitter datasets collected using hashtag based supervision but such datasets are noisy in terms of labels and language. Furthermore, many tweets are replies to other tweets and detecting sarcasm in these requires the availability of contextual tweets.**\n\n**To overcome the limitations related to noise in Twitter datasets, this News Headlines dataset for Sarcasm Detection is collected from two news website. TheOnion aims at producing sarcastic versions of current events and we collected all the headlines from News in Brief and News in Photos categories (which are sarcastic). We collect real (and non-sarcastic) news headlines from HuffPost.**\n\n**The dataset consists about 28000 text data points where each data category belongs to 2 category - Sarcastic or Not Sarcastic**\n\n**We will use two models for making predictions - Word2Vec and GloVe Embeddings. We will then compare their results and see which performs better**\n![image.png](attachment:image.png)","0129c201":"# LOADING THE DATASET","0b8dc5d6":"# Introduction to Word Embedding and Word2Vec\n**Word embedding is one of the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\nWhat are word embeddings exactly? Loosely speaking, they are vector representations of a particular word. Having said this, what follows is how do we generate them? More importantly, how do they capture the context?\nWord2Vec is one of the most popular technique to learn word embeddings using shallow neural network. It was developed by Tomas Mikolov in 2013 at Google.**","0a89ed5e":"# ANALYSIS AFTER TRAINING OF GLOVE EMBEDDINGS MODEL"}}