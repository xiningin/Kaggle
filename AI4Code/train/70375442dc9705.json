{"cell_type":{"ed6a91f3":"code","fdb93ea6":"code","ffd54f7a":"code","3f353a3a":"code","2e83631d":"code","f24638f9":"code","2901e0d9":"code","85919ed4":"code","ac2b5fd4":"code","b7a63da2":"code","096fb7af":"code","428a74de":"code","63b93a5f":"code","9c109a3c":"code","2e72d69e":"markdown","f8236f2b":"markdown","985e5ac9":"markdown","c0f138f2":"markdown","b31a32d3":"markdown","1d17a599":"markdown","a24f1cb2":"markdown"},"source":{"ed6a91f3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fdb93ea6":"data = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","ffd54f7a":"data.head(10)","3f353a3a":"# check if there is null values in the data\ndata.isna().sum()","2e83631d":"import seaborn as sns\n# check the class distributions\nsns.countplot(\"Class\",data=data)","f24638f9":"print('The normal class percentage is ',round((len(data[data[\"Class\"]==0])\/(len(data[data[\"Class\"]==0])+len(data[data[\"Class\"]==1])))*100,2),'%')\nprint('The fraud class percentage is ',round((len(data[data[\"Class\"]==1])\/(len(data[data[\"Class\"]==0])+len(data[data[\"Class\"]==1])))*100,2),'%')","2901e0d9":"from sklearn.preprocessing import StandardScaler\n# before starting we should standridze our amount column\nprint(\"Before:\")\nprint(data[\"Amount\"].head())\n\ndata[\"Normalized Amount\"] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\ndata.drop([\"Time\",\"Amount\"],axis=1,inplace=True)\n\nprint(\"\\nAfter:\")\nprint(data[\"Normalized Amount\"].head())","85919ed4":"fraud_indices= np.array(data[data.Class==1].index)\nnormal_indices = np.array(data[data.Class==0].index)\n\ndef undersample(normal_indices,fraud_indices,ratio):#ratio is the (normal data \/ fraud data)\n    \n    # select randomly (no repetition) N normal_indices such that N = ratio*len(data[data[\"Class\"]==1])\n    Normal_indices_undersample = np.array(np.random.choice(normal_indices,(ratio*len(data[data[\"Class\"]==1])),replace=False))\n    \n    # the next 2 lines will put the fraud and normal indices then get those rows (that are choosen from the selected indices) and make a new df with them\n    undersample_data= np.concatenate([fraud_indices,Normal_indices_undersample])\n    undersample_data = data.iloc[undersample_data,:]\n    \n    #plot the dataset after the undersampling\n    plt.figure(figsize=(4, 4))\n    sns.countplot('Class', data=undersample_data)\n    plt.title('Balanced Classes')\n    plt.show()\n    \n    return undersample_data\n","ac2b5fd4":"import matplotlib.pyplot as plt\n\ndef undersampleF(ratio):#ratio is the (normal data \/ fraud data)\n    # Shuffle the Dataset.\n    shuffled_data = data.sample(frac=1,random_state=4)\n\n    # Put all the fraud class in a separate dataset.\n    fraud_data = shuffled_data.loc[shuffled_data['Class'] == 1]\n\n    #Randomly select n observations from the non-fraud (majority class)\n    non_fraud_data = shuffled_data.loc[shuffled_data['Class'] == 0].sample(n=ratio*len(fraud_data),random_state=42)\n\n    # Concatenate both dataframes again\n    undersample_data = pd.concat([fraud_data, non_fraud_data])\n    \n    #plot the dataset after the undersampling\n    plt.figure(figsize=(4, 4))\n    sns.countplot('Class', data=undersample_data)\n    plt.title('Balanced Classes')\n    plt.show()\n    return undersample_data\n    ","b7a63da2":"from sklearn.model_selection import train_test_split\n\ndef TrainTestSplit(data,test_size):\n    y= data.iloc[:,data.columns == \"Class\"]\n    X= data.iloc[:,data.columns != \"Class\"]\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n    print(\"shape of training data\")\n    print(X_train.shape)\n    print(\"shape of test data\")\n    print(X_test.shape)\n    return X_train, X_test, y_train, y_test\n","096fb7af":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nfrom sklearn import metrics\n\n\ndef model(model,X_train,y_train,X_test,y_test):\n    clf = model\n    clf.fit(X_train,y_train)\n    \n    pred = clf.predict(X_test)\n    cnf_matrix = confusion_matrix(y_test,pred)\n    \n    print(\"the recall for this model is :\",cnf_matrix[1,1]\/(cnf_matrix[1,1]+cnf_matrix[1,0]))\n    fig= plt.figure(figsize=(4,2))# to plot the graph\n    print(\"TP\",cnf_matrix[1,1,]) # no of fraud transaction which are predicted fraud\n    print(\"TN\",cnf_matrix[0,0]) # no. of normal transaction which are predited normal\n    print(\"FP\",cnf_matrix[0,1]) # no of normal transaction which are predicted fraud\n    print(\"FN\",cnf_matrix[1,0]) # no of fraud Transaction which are predicted normal\n    sns.heatmap(cnf_matrix,cmap = \"coolwarm_r\",annot=True,linewidths=0.5)\n    plt.title(\"Confusion_matrix\")\n    plt.xlabel(\"Predicted_class\")\n    plt.ylabel(\"Real class\")\n    plt.show()\n    print(\"\\n----------Classification Report------------------------------------\")\n    print(classification_report(y_test,pred))","428a74de":"for ratio in range(1,4):\n    \n    undersample_data = undersample(normal_indices,fraud_indices,ratio)\n#     undersample_data = undersampleF(ratio)\n\n    X_train, X_test, y_train, y_test = TrainTestSplit(undersample_data,0.25)\n    clf = xgb.XGBClassifier()\n    model(clf,X_train,y_train,X_test,y_test)\n    print()\n    print(\"-------------------------------------------\")","63b93a5f":"for ratio in range(1,4):\n    \n    undersample_data = undersample(normal_indices,fraud_indices,ratio)\n#     undersample_data = undersampleF(ratio)\n    \n    X_train, X_test, y_train, y_test = TrainTestSplit(undersample_data,0.25)\n    clf = LogisticRegression()\n    model(clf,X_train,y_train,X_test,y_test)\n    print()\n    print(\"-------------------------------------------\")","9c109a3c":"for ratio in range(1,4):\n    \n    undersample_data = undersample(normal_indices,fraud_indices,ratio)\n#     undersample_data = undersampleF(ratio)\n    \n    X_train, X_test, y_train, y_test = TrainTestSplit(undersample_data,0.25)\n    models = RandomForestClassifier(n_estimators=100)\n    model(models , X_train , y_train , X_test,y_test)\n    print()\n    print(\"-------------------------------------------\")","2e72d69e":"\nBefore re sampling lets have look at the different accuracy matrices\n\nAccuracy = TP+TN\/Total\n\nPrecison = TP\/(TP+FP)\n\nRecall = TP\/(TP+FN)\n\nTP = True possitive means no of possitve cases which are predicted possitive\n\nTN = True negative means no of negative cases which are predicted negative\n\nFP = False possitve means no of negative cases which are predicted possitive\n\nFN= False Negative means no of possitive cases which are predicted negative\n\nNow for our case recall will be a better option because in these case no of normal transacations will be very high than the no of fraud cases and sometime a fraud case will be predicted as normal. So, recall will give us a sense of only fraud cases\n\nResampling\n\nin this we will resample our data with different size\n\nthen we will try to use this resampled data to train our model\n\nthen we will use this model to predict for our original data\n","f8236f2b":"Trying to undersample the data","985e5ac9":"Next is a function that takes our data and test size then splits the data into train,test ","c0f138f2":"next is a model function as i will test different models ","b31a32d3":"To conclude i think the best model is RandomForestClassifier with 100 estimators as it get a recall of 92 % and accuracy of 96% with undersampling ratio 1:1","1d17a599":"the next function is another way to undersample but i wont use it as it gets less recall ... idk why for now","a24f1cb2":"as we can see below the data is highly imbalanced\n"}}