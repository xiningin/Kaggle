{"cell_type":{"9331697e":"code","450bd377":"code","afc7ad9f":"code","daac66d3":"code","07a9ffb8":"code","029b1837":"code","99be1a67":"code","6e02b13f":"code","680cdfd6":"code","998438b9":"code","b7409864":"code","db071ae8":"code","a6c52d31":"code","086f95c9":"code","bf0aff73":"code","fef12373":"code","50fd5b88":"code","92b85abb":"code","58d76c98":"code","b01b2a2f":"code","8beef816":"code","945e1b74":"code","12110f35":"code","431c5feb":"code","b48d2129":"code","24d0b5aa":"code","1321f221":"code","c6f3d859":"code","f7a41ec9":"code","73a9084f":"code","4951a6be":"code","4bd71049":"code","2cf42dbe":"markdown","c64e9c93":"markdown","908211bb":"markdown","fb7c723b":"markdown","1c963734":"markdown","0ce9fbd3":"markdown","b243d609":"markdown","c3cbae05":"markdown","5d80d849":"markdown","47ec3a63":"markdown","c6362883":"markdown","beb8405e":"markdown","b1b4ed64":"markdown","12648716":"markdown","914636be":"markdown","545abed7":"markdown","dc2598f5":"markdown","04efbe85":"markdown","2780221a":"markdown","d4c37740":"markdown","05f94c91":"markdown","3ec7385b":"markdown","89521cc8":"markdown","9cacb85b":"markdown","7806deb7":"markdown","984611ba":"markdown","ca45fd38":"markdown","8c3cf6b0":"markdown","3454df7f":"markdown","85d4bf89":"markdown","9564eedc":"markdown","3cf0b56f":"markdown","d53b955d":"markdown","c019711d":"markdown","27435fda":"markdown","f23b5c6d":"markdown","918e4f02":"markdown","f7fcde7e":"markdown","8257a1a0":"markdown"},"source":{"9331697e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","450bd377":"#Loading Data\ndata = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")","afc7ad9f":"#Let's see the columns\ndata.columns","daac66d3":"# then we need to see info\ndata.info()","07a9ffb8":"data.head()","029b1837":"#Let's Visualize the Data \ncolor_list = [\"red\" if i == 1 else \"green\" for i in data.loc[:,\"Outcome\"]]\npd.plotting.scatter_matrix(data.loc[:,data.columns !=\"Outcome\"],\n                          c=color_list,\n                          figsize = [20,20],\n                          diagonal =\"hist\",\n                          alpha = 0.6,\n                          s=200,\n                          marker =\"*\",\n                          edgecolor = \"black\")\nplt.show()","99be1a67":"#To see the distrubution of the outcome we'll use seaborn sns.countplot\nimport seaborn as sns\nsns.countplot(x=\"Outcome\", data = data)\ndata.loc[:,\"Outcome\"].value_counts()\n","6e02b13f":"y = data.Outcome.values\nx_data = data.iloc[:,:-1]\nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))\n\nfrom sklearn.model_selection import train_test_split \nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.30, random_state = 42) #Y\u00fczde 25 i x_test vey_test e atanacak \n\nx_train = x_train.T\ny_train = y_train.T\nx_test = x_test.T\ny_test = y_test.T\n\nprint(\"x_train shape\",x_train.shape)\nprint(\"y_train shape\",y_train.shape)\nprint(\"x_test shape\",x_test.shape)\nprint(\"y_test shape\",y_test.shape)","680cdfd6":"#%% Initializing Parameters and Sigmoid Function \n\ndef initialize_weights_and_bias(dimension): #30 feature var o zaman 30 dimension olmal\u0131\n    \n    w = np.full((dimension,1),0.01) #burada dimension 30 girdi\u011fimiz zaman [0,0.01] lik weightler atayacag\u0131z\n    b = 0.0 #float olsun diye 0.0 yazd\u0131m\n    return w,b\n# w,b = initialize_weight_and_bias(30)\n\ndef sigmoid(z):\n    y_head = 1\/(1+ np.exp(-z)) #form\u00fcl\u00fc budur z nin\n    return y_head\n\n#sigmoid(0) de\u011feri 0.5 vermelidir \n    ","998438b9":"#%% Forward - Backward Propagation\n#Bu k\u0131s\u0131mda w ile train data m\u0131z\u0131 \u00e7arpaca\u011f\u0131z Bias ekleyip sigmoid fonksiyona sokaca\u011f\u0131z \n\ndef forward_backward_propagation(w,b,x_train,y_train):\n    #Forward Prop \n    z = np.dot(w.T,x_train) + b #Transpoz alma sebebimiz matris carp\u0131m\u0131n\u0131 yapabilmek i\u00e7in \n    y_head = sigmoid(z) #Sigmoid fonksiyonuna soktuk\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head) #Loss fonksiyonunu yazd\u0131k \n    cost = (np.sum(loss)) \/ x_train.shape[1] #Losslar toplam\u0131n\u0131 normalize etmek i\u00e7in sample say\u0131s\u0131na b\u00f6ld\u00fck \n    #x_train_shape[1] = 455\n    \n\n    #Backward Prop\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] #Form\u00fcl bu, shape b\u00f6lmek normalize etmek i\u00e7in\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients","b7409864":"#%% Updating Parameters \n\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    #Iteration \n    for i in range(number_of_iterarion):\n        #Doing forward and Backward Propagation \n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost) #G\u00fcncelleme \u00f6ncesi cost list e at\u0131yorum (T\u00fcm cost listleri depolamak)\n        \n        #Updating \n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n    \n        if i %10 == 0:\n            cost_list2.append(cost) #Her 10 ad\u0131mda bir costlar\u0131 depola    \n            index.append(i)\n            print(\"Cost after iteration %i: %f\"%(i,cost))\n            \n    #Number of iteration ka\u00e7 olacag\u0131 karar\u0131n\u0131 deneyerek bulacag\u0131z T\u00fcrevi 0 a yakla\u015f\u0131nca yeterli olacakt\u0131r\n    #We updaate (learn) parameters weights and Bias \n    parameters = {\"weight\":w , \"bias\":b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation ='vertical')\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters,gradients,cost_list","db071ae8":"#%% Prediction \ndef predict(w,b,x_test): #w,b zaten laz\u0131m ama x_test de class \u0131 belli olmayan ve test edece\u011fim (tahmin edece\u011fim) data \n    z = sigmoid(np.dot(w.T,x_test)+b)\n    y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    #Eger z 0.5 den b\u00fcy\u00fck ise y_head = 1 yani k\u00f6t\u00fc huylu\n    #Eger < 0.5 ise y_head = 0 yani iyi huylu \n    \n    for i in range(z.shape[1]):\n        if z[0,i]<=0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n            \n    return y_prediction\n        \n#\u015fimdi y prediction u y test ile kar\u015f\u0131last\u0131r\u0131p e\u011fitimin dogruluguna bak\u0131caz \n    ","a6c52d31":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 30\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 400) ","086f95c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","bf0aff73":"#Loading Data\ndata = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")","fef12373":"data0 = data[data.Outcome == 0] # Healthy group\ndata1 = data[data.Outcome == 1] # Sick group\ndata1.sort_values(by=\"Age\")","50fd5b88":"#We will use BloodPressure and Age parameters in the Sick group\ndata1 = data[data.Outcome == 1]\nxlin=data1.BloodPressure.values.reshape(-1,1)\nylin=data1.Age.values.reshape(-1,1)\n\n#Linear Regression Model\nfrom sklearn.linear_model import LinearRegression\nlinear_reg = LinearRegression()\n#Creating Prediction Space to get more efficient results\npredict_space = np.linspace(min(xlin),max(xlin)).reshape(-1,1)  \n#Fit\nlinear_reg.fit(xlin,ylin)\n#Prediction \npredicted = linear_reg.predict(predict_space)\n#Perfomance Analysis w\/R^2 Score method\nprint(\"R^2 Score is :\",linear_reg.score(xlin,ylin))\n\nplt.plot(predict_space, predicted, color=\"black\",linewidth=2)\nplt.scatter(x=xlin, y=ylin)\nplt.xlabel(\"Blood Pressure\")\nplt.ylabel(\"Age\")\nplt.show()","92b85abb":"#We will use Glucose and Age parameters in the Sick group\ndata1 = data[data.Outcome == 1]\nxpol=data1.Glucose.values.reshape(-1,1) #In Sklearn we need to reshape our data like that\nypol=data1.Age.values.reshape(-1,1)\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\npoly_reg = PolynomialFeatures(degree = 3) #degree = 3 means we have limited the equation with x^3\nx_polynomial = poly_reg.fit_transform(xpol) #We transformed our xpol values to x^3\n#Fit (For fitting we use Linear Regression again..)\nlinear_reg2 = LinearRegression()  \nlinear_reg2.fit(x_polynomial,ypol)\n#Prediction \ny_head = linear_reg2.predict(x_polynomial)\n#Visualisation \nplt.plot(x_polynomial,y_head,color=\"red\")\nplt.show()","58d76c98":"#We will use BloodPressure and Age parameters in the Sick group\ndata1 = data[data.Outcome == 1]\nxdt=data1.BloodPressure.values.reshape(-1,1)\nydt=data1.Age.values.reshape(-1,1)\n\nfrom sklearn.tree import DecisionTreeRegressor\ndtreg = DecisionTreeRegressor()\n#Fit\ndtreg.fit(xdt,ydt) \n#Prediction space\nxdt_ = np.arange(min(xdt),max(xdt),0.01).reshape(-1,1)\ny_headdt = dtreg.predict(xdt_)\n#Visualisation \nplt.scatter(xdt,ydt,color =\"red\",label=\"Values\")\nplt.plot(xdt_,y_headdt,color=\"blue\",label=\"Predicted\")\nplt.show()","b01b2a2f":"#Let's work same features in the Decision Tree Regression model that BloodPressure and Age \ndata1 = data[data.Outcome == 1]\nxrf=data1.BloodPressure.values.reshape(-1,1)\nyrf=data1.Age.values.reshape(-1,1)\n\nfrom sklearn.ensemble import RandomForestRegressor  \nrfreg = RandomForestRegressor(n_estimators = 100, #We work with 100 times decision tree reg\n                             random_state= 42)\n#Fit\nrfreg.fit(xrf,yrf) \n#Prediction space\nxrf_ = np.arange(min(xrf),max(xrf),0.01).reshape(-1,1)\ny_headrf = rfreg.predict(xdt_)\n#Visualisation \nplt.scatter(xrf,yrf,color =\"red\",label=\"Values\")\nplt.plot(xrf_,y_headrf,color=\"blue\",label=\"Predicted\")\nplt.show()","8beef816":"y = data.Outcome.values\nx_data = data.iloc[:,:-1]\nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))\n\nfrom sklearn.model_selection import train_test_split \nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.30, random_state = 42) #Y\u00fczde 25 i x_test vey_test e atanacak \n\n\nprint(\"x_train shape\",x_train.shape)\nprint(\"y_train shape\",y_train.shape)\nprint(\"x_test shape\",x_test.shape)\nprint(\"y_test shape\",y_test.shape)\n\n#When we need to do any process without any mistake, error arrays should be like that \n# 537,8 \n# 537,\n# 231,8\n# 231","945e1b74":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\nprint(\"Test Accuracy : %{}\".format(lr.score(x_test,y_test)*100))\n","12110f35":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 40) #n_neighbors is a hyperparameter that's why we need to try to examine the Optimum value \nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\n# print(\"Prediction:\",prediction) if you want to compare the test data and predictions you can remove # and try \nprint(\"for n={} KNN Score : {}\".format(40,knn.score(x_test,y_test))) \n","431c5feb":"#if we want to see which n number will be optimum we can define a for loop for that \nscore_list = []\nfor each in range(1,50):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train, y_train)\n    score_list.append(knn2.score(x_test, y_test))\n\nplt.figure(figsize=(8,5))\nplt.scatter(range(1,50),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show() \n\n#40 might be the optimum number for N \n#The answer is 0.753","b48d2129":"neig = np.arange(1,50)\ntrain_accuracy = []\ntest_accuracy = []\n#Loop all over in k values\nfor i,k in enumerate(neig):\n    knn3 = KNeighborsClassifier(n_neighbors = k)\n    #Fit process\n    knn3.fit(x_train,y_train)\n    #Train Accuracy\n    train_accuracy.append(knn3.score(x_train,y_train))\n    #Test Accuracy\n    test_accuracy.append(knn3.score(x_test,y_test))\n    \n#Plotting the Values \nplt.figure(figsize =(13,10))\nplt.plot(neig, test_accuracy, label = \"Testing Accuracy\")\nplt.plot(neig, train_accuracy, label = \"Training Accuracy\")\nplt.legend()\nplt.title(\"Values vs Accuracy\")\nplt.xlabel(\"Number of Neighbors\")\nplt.ylabel(\"Accuracy\")\nplt.xticks(neig) #We limit the Max min values in the plot axis according to Number of max neighbor\nplt.savefig(\"graph.png\")\nplt.show()\nprint(\"Best Accuracy : {} with K : {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","24d0b5aa":"from sklearn.svm import SVC \nsvm = SVC(random_state = 1)\nsvm.fit(x_train,y_train)\n\n#Test \nprint(\"Accuracy of the SVM Algorithm : \",svm.score(x_test,y_test))","1321f221":"from sklearn.naive_bayes import GaussianNB \nnb = GaussianNB()\nnb.fit(x_train,y_train)\n\nprint(\"Accuracy of the Naive Bayes Algorithm :\",nb.score(x_test,y_test))","c6f3d859":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train, y_train)\nprint(\"Accuracy of the Decision Tree :\",dt.score(x_test,y_test))","f7a41ec9":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators= 24, random_state=42)\nrf.fit(x_train,y_train)\nprint(\"Accuracy of the Random Forest Classification : \",rf.score(x_test,y_test))\n\n#We need to find optimum value that's why need to decide best number for n_estimators parameter\n#if we want to see which n number will be optimum we can define a for loop for that \nscore_list2 = []\nfor each in range(1,200):\n    rf2 = RandomForestClassifier(n_estimators = each)\n    rf2.fit(x_train, y_train)\n    score_list2.append(rf2.score(x_test, y_test))\n\nplt.figure(figsize=(8,5))\nplt.scatter(range(1,200),score_list2)\nplt.xlabel(\"n values\")\nplt.ylabel(\"accuracy\")\nplt.show() ","73a9084f":"aa = np.max(score_list2) #We can see the max value would be 0.7878 and n = 24 might be the great option\naa","4951a6be":"print(\"Test Accuracy for Logistic Regression: %{}\".format(lr.score(x_test,y_test)*100))\nprint(\"for n={} KNN Score : %{}\".format(40,knn.score(x_test,y_test)*100))\nprint(\"Accuracy of the SVM Algorithm : %{}\".format(svm.score(x_test,y_test)*100))\nprint(\"Accuracy of the Naive Bayes Algorithm : %{}\".format(nb.score(x_test,y_test)*100))\nprint(\"Accuracy of the Decision Tree : %{}\".format(dt.score(x_test,y_test)*100))\nprint(\"Accuracy of the Random Forest Classification : %{}\".format(rf.score(x_test,y_test)*100))","4bd71049":"# I'm gonna show an example on Random Forest C. model \nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators= 24, random_state=42)\nrf.fit(x_train,y_train)\nprint(\"Accuracy of the Random Forest Classification : \",rf.score(x_test,y_test))\n\n#In this method we need to predict x test values \ny_pred = rf.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n#Lets Visualize it \nimport seaborn as sns \nf,ax =plt.subplots(figsize=(6,6))\nsns.heatmap(cm, annot = True, linecolor = \"blue\", ax = ax)\n\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()\n","2cf42dbe":"### Logistic Regression \n","c64e9c93":"### In\u0131tializing Parameters and Sigmoid Function\n\n","908211bb":"### As you see for classification best results given from Random Forest Classification by difference from the nearest oppenent %2","fb7c723b":"<a id ='8'><\/a>\n### 2-) Polynomial Linear Regression\n- y=b0 + b1x1 + b2*x^2 + ... + bn*x^n \n- This method kind a complex of Polynomial and Linear Regression thats why in this method's solution there are 2 steps and libraries","1c963734":"<a id=\"18\"><\/a>\n## Performans Comparison of Classification Methods\n ****Finally Let's check all of the classification methods' accuracy results****","0ce9fbd3":"<a id ='10'><\/a>\n### 4-) Random Forest Regression \n- Random Forest Regression is a complex form of Decision Tree Regression that we work with many of Decision Tree Regression model \n","b243d609":"Outcome is 1, means that patience is sick 0 is healthy\n","c3cbae05":"<a id ='12'><\/a>\n### Logistic Regression w\/Sklearn Library\n","5d80d849":"<font color =\"green\">\n### Content:\n1. [Load and Check Data](#1)\n2. [Basic Data Analysis](#2)\n3. [Visualisation of the Data](#3)\n4. [Machine Learning Algorithms](#4)\n    * [Supervised Learning](#5) \n        * [Regression](#6) \n            1. [Linear Regression](#7)\n            1. [Polynomial Linear Regression](#8) \n            1. [Decision Tree Regression](#9)\n            1. [Random Forest Regression](#10)\n            \n        * [Classification](#11) \n            1. [Logistic Regression Classification](#12)\n            1. [K-Nearest Neighbour (KNN) Classification](#13)\n            1. [Support Vector Machine (SVM) Classification](#14)\n            1. [Naive Bayes Classification](#15)\n            1. [Decision Tree Classification](#16)\n            1. [Random Forest Classification](#17)\n                \n                - [Perfomance Comparison of Classification Methods](#18)\n       \n       * [Evaluation Classification Method](#19) \n            1. [Confusion Matrix](#19)","47ec3a63":"## **By Using SKLEARN Library**\n- In this section we'll use just sklearn library to find exact values for all the process (regression, classification models) ","c6362883":"First of all, we need to split our data for training and testing ","beb8405e":"<a id=\"15\"><\/a>\n### Naive Bayes Classification ","b1b4ed64":"### Updating Parameters","12648716":"<a id ='7'><\/a>\n### 1-) Linear Regression \n- y=b0 + b1x1","914636be":"<a id ='9'><\/a>\n### 3-) Decision Tree Regression","545abed7":"# Machine Learning Tutorial w\/ Pima Indians Diabates Database ","dc2598f5":"<a id=\"14\"><\/a>\n### Support Vector Machine (SVM) Classification \n- This method provides that the optimum line which seperates the \"2\" classes objects ","04efbe85":"As we see there are 500 healthy and 268 sick people","2780221a":"### Prediction ","d4c37740":"<a id=\"19\"><\/a>\n## Evaluation Classification Methods \n  ### Confusion Matrix \n  - In this section we'll learn the accuracy test method alternative to .score(x_test,y_test)) (**Actually better way to examine details**)","05f94c91":"<a id ='2'><\/a>\n## Basic Data Analysis","3ec7385b":"### Forward and Backward Propagation","89521cc8":"To apply manual method we need to get Transpose version of the data","9cacb85b":"<a id ='3'><\/a>\n## Visualisation of the Data\n- pd.plotting.scatter_matrix:\n- \n    - green: healthy\n    - red: sick\n    - c: color\n    - figsize: figure size\n    - diagonal: histohram of each features\n    - alpha: opacity\n    - s: size of marker\n    - marker: marker type","7806deb7":"### We trained the data with Logistic Regression Model and the model will predict the values truely by %74.4 ratio","984611ba":"<a id ='11'><\/a>\n## Classification\n- In this section we'll learn and apply all the Classification Methods such as:\n    + Logistic Regression Classification\n    + K-Nearest Neighbour (KNN) Classification\n    + Support Vector Machine (SVM) Classification\n    + Naive Bayes Classification \n    + Decision Tree Classification \n    + Random Forest Classification\n    EXTRA : Evaluation Classification Methods (Alternative to score(x_test,y_test) method)\n        + Confusion Matrix","ca45fd38":"- *First square says that the model predicts value 0 (healthy) as value 0 for 13*e^02 values (correct)\n- *Second sqare says that the model predict  value 0 (healthy) as value 1 for 24 values (incorrect)\n- *Third square says that the model predicts value 1 (sick) as value 0 for 29 values (incorrect)\n- *Forth square says that the model predicts value 1 (sick) as value 1 for 51 values (correct)\n\n- So the model predicts wrong values for 29 + 24 = 53 times ","8c3cf6b0":"<a id=\"13\"><\/a>\n### K - Nearest Neighbour (KNN) Classification\n","3454df7f":"We use Regression Models to predict the future values by using data\nFor Example, we try to predict a house value,price in the California by using real data California house prices. ","85d4bf89":"<a id=\"16\"><\/a>\n### Decision Tree Classification\n","9564eedc":"### Alternative 1 - Find best n number to get optimum value","3cf0b56f":"<a id ='1'><\/a>\n## Loading and Check Data","d53b955d":"<a id ='4'><\/a>\n## Machine Learning ","c019711d":"<a id=\"17\"><\/a>\n### Random Forest Classification \n- This method is a advanced version of Decision Tree Classification Method \n- In this method we use many of the Decision Tree algorithm but this number is a hyperparameter and we need to find a exact number for optimum solution\n- Obvious that **RF Classification Accuracy must better than Decision Tree C.**","27435fda":"<a id ='5'><\/a>\n### ***Supervised Learning Algorithms***","f23b5c6d":"### Alternative 2 - Find best n number to get optimum value ( More Beneficial )\n","918e4f02":"### Train and Test Data","f7fcde7e":"<a id ='6'><\/a>\n## Regression \n- In this section we'll learn also apply the Regression Methods such as:\n    * Linear Regression (y=b0 + b1x1)\n    * Polynomial Linear Regression (y=b0 + b1x1 + b2*x^2 + ... + bn*x^n)\n    * Decision Tree Regression \n    * Random Forest Regression \n    * EXTRA = Performance Analysis by using R-Square Method","8257a1a0":"- **As you see our R^2 Score is too bad because this features are not able to use Linear Regression efficiently**\n- **But \u0131 wanted to add this table here as an example** \n"}}