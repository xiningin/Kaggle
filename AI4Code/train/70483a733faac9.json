{"cell_type":{"98f2c0fe":"code","ee285efd":"code","fe692eb6":"code","03edf431":"code","28c372fd":"code","de4f87e5":"code","d1cb74db":"code","f95289a7":"code","7ab62500":"code","0c41a75a":"code","3e0a6121":"code","5a84a5d3":"code","22343807":"code","054e9016":"code","ab7ca6e6":"code","fe55c327":"code","eb11960f":"code","eca10e89":"code","fc138aaa":"code","3e694695":"code","49cb6053":"code","5236c329":"code","c452a96d":"code","24e42f17":"code","74a515db":"markdown","97a0a2eb":"markdown","3cb3ca5d":"markdown","5693f499":"markdown","2dfdefcc":"markdown","b62dc5d0":"markdown","13c5d1e1":"markdown","c870428e":"markdown","b78ec37e":"markdown","2b7c8e50":"markdown","d584e0d0":"markdown","eb23ea32":"markdown","70c75c84":"markdown","46b82225":"markdown","fbd2dfcd":"markdown","e8befedf":"markdown","f5da7f6d":"markdown","61dd3649":"markdown","54c4d24b":"markdown","d66ef23e":"markdown","c0305397":"markdown"},"source":{"98f2c0fe":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, sampler\nfrom torchvision import transforms\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\nfrom random import shuffle, randint\nfrom PIL import Image\nimport math","ee285efd":"class DigitDataset(Dataset):\n    \"\"\" Digit Dataset \"\"\"\n\n    def __init__(self, csv_file, root_dir, train=False, transform=None):\n        self.digit_df = pd.read_csv(root_dir + csv_file)\n        self.transform = transform\n        self.train = train\n\n    def __len__(self):\n        return len(self.digit_df)\n\n    def __getitem__(self, item):\n        if self.train:\n            digit = self.digit_df.iloc[item, 1:].values\n            digit = digit.astype('float').reshape((28, 28))\n            label = self.digit_df.iloc[item, 0]\n        else:\n            digit = self.digit_df.iloc[item, :].values\n            digit = digit.astype('float').reshape((28, 28))\n            label = 0\n        sample = [digit, label]\n        if self.transform:\n            sample[0] = self.transform(sample[0])\n        return sample","fe692eb6":"class Regularize(object):\n    \"\"\" Regularize digit pixel value \"\"\"\n\n    def __init__(self, max_pixel=255):\n        self.max_pixel = max_pixel\n\n    def __call__(self, digit):\n        assert isinstance(digit, np.ndarray)\n        digit = digit \/ self.max_pixel\n        return digit\n\n\nclass ToTensor(object):\n    \"\"\" Covert ndarrays to Tensors \"\"\"\n\n    def __call__(self, digit):\n        assert isinstance(digit, np.ndarray)\n        digit = digit.reshape((1, 28, 28))\n        digit = torch.from_numpy(digit)\n        digit = digit.float()\n        return digit","03edf431":"data_np = DigitDataset('train.csv', '..\/input\/', train=True)\nprint(\"Number of Training Images: \", len(data_np))\nplt.imshow(data_np[5][0], cmap='gray')\nplt.show()\nprint(\"Label for the Image: \", data_np[5][1])","28c372fd":"composed_transform = transforms.Compose([Regularize(), ToTensor()])\ndata_torch = DigitDataset('train.csv', '..\/input\/', train=True, transform=composed_transform)\ndataloader = DataLoader(data_torch,\n                       batch_size=4,\n                       shuffle=True,\n                       num_workers=4)\nfor i, data in enumerate(dataloader, 0):\n    digits, labels = data\n    # digits, labels = digits.to(\"cuda\"), labels.to(\"cuda\")\n    print(\"Type of Digits: \", type(digits))\n    print(\"Dimension of the Tensor: \", digits.shape)\n    print(\"Type of Labels: \", type(labels))\n    print(\"Dimension of the Tensor: \", labels.shape)\n    if i == 0:\n        break","de4f87e5":"def digits_per_class(digit_df, indices):\n    assert isinstance(digit_df, pd.DataFrame)\n    assert isinstance(indices, list)\n    digit_num = [0 for num in range(10)]\n    for idx in indices:\n        label = digit_df.iloc[idx, 0]\n        digit_num[label] += 1\n    return digit_num","d1cb74db":"digit_class_num = digits_per_class(data_torch.digit_df,\n                                  [num for num in range(len(data_torch))])\nfor i, num in enumerate(digit_class_num, 0):\n    print(\"Number of Images for Digit \", i, \": \", num)\nprint(\"Overall Images: \", sum(digit_class_num))","f95289a7":"def train_validate_split(digit_df, test_ratio=0.2):\n    assert isinstance(digit_df, pd.DataFrame)\n    digit_num = len(digit_df)\n    overall_indices = [num for num in range(digit_num)]\n    overall_class_num = digits_per_class(digit_df, overall_indices)\n    test_class_num = [int(num*test_ratio) for num in overall_class_num]\n    tmp_test_class_num = [0 for num in range(10)]\n    shuffle(overall_indices)\n    train_indices = []\n    val_indices = []\n    for idx in overall_indices:\n        tmp_label = digit_df.iloc[idx, 0]\n        if tmp_test_class_num[tmp_label] < test_class_num[tmp_label]:\n            val_indices.append(idx)\n            tmp_test_class_num[tmp_label] += 1\n        else:\n            train_indices.append(idx)\n    return train_indices, val_indices","7ab62500":"train_data, val_data = train_validate_split(data_torch.digit_df)\ntrain_class_num = digits_per_class(data_torch.digit_df, train_data)\nval_class_num = digits_per_class(data_torch.digit_df, val_data)\nfor i, num in enumerate(train_class_num, 0):\n    print(\"Number of Images for Digit \", i, \"- Train: \", num, \"Validate: \", val_class_num[i])\nprint(\"Train Images: \", sum(train_class_num), \"Validate Images: \", sum(val_class_num))","0c41a75a":"train_sampler = sampler.SubsetRandomSampler(train_data)\ntrain_dataloader = DataLoader(data_torch,\n                              batch_size=4,\n                              shuffle=False,\n                              sampler=train_sampler,\n                              num_workers=4)","3e0a6121":"class BasicLeNet(nn.Module):\n    \"\"\" Basic LeNet-5 as defined in LeCun's paper\"\"\"\n\n    def __init__(self):\n        super(BasicLeNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 6, 5),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n            nn.Conv2d(6, 16, 5),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(16*4*4, 120),\n            nn.ReLU(inplace=True),\n            nn.Linear(120, 84),\n            nn.ReLU(inplace=True),\n            nn.Linear(84, 10)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        Feedforward function\n\n        :param x: input batch\n        :return: x\n        \"\"\"\n        x = self.features(x)\n        x = x.view(x.size(0), 16*4*4)\n        x = self.classifier(x)\n        return x","5a84a5d3":"def training(network, criterion, optimizer, epoch_num, test=True):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Start Training with\", device, epoch_num, \"overall epoch\")\n    network.to(device)\n    \"\"\"\n    Create Dataloader for training and validating\n    \"\"\"\n    composed_transform = transforms.Compose([Regularize(), ToTensor()])\n    digit_dataset = DigitDataset('train.csv', '..\/input\/', train=True, transform=composed_transform)\n    if test:\n        train_indices, val_indices = train_validate_split(digit_dataset.digit_df)\n        train_sampler = sampler.SubsetRandomSampler(train_indices)\n        val_sampler = sampler.SubsetRandomSampler(val_indices)\n        train_dataloader = DataLoader(\n            digit_dataset,\n            batch_size=32,\n            shuffle=False,\n            sampler=train_sampler,\n            num_workers=4,\n            pin_memory=True\n        )\n        val_dataloader = DataLoader(\n            digit_dataset,\n            batch_size=32,\n            shuffle=False,\n            sampler=val_sampler,\n            num_workers=4,\n            pin_memory=True\n        )\n        print(\"Training with validation, \", \"Overall Data:\", len(train_indices)+len(val_indices))\n        print(\"Training Data:\", len(train_indices), \"Validate Data:\", len(val_indices))\n    else:\n        train_dataloader = DataLoader(\n            digit_dataset,\n            batch_size=32,\n            shuffle=True,\n            num_workers=4,\n            pin_memory=True\n        )\n        val_dataloader = None\n        print(\"Training all data, \", \"Overall Data:\", len(digit_dataset))\n    \"\"\"\n    Start Training\n    \"\"\"\n    batch_num = 0\n    ita = []\n    loss_avg = []\n    val_acc = []\n    for epoch in range(epoch_num):\n        running_loss = 0.0\n        for i, data in enumerate(train_dataloader, 0):\n            digits, labels = data\n            digits, labels = digits.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = network(digits)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            batch_num += 1\n            if test == True and i % 500 == 499:\n                ita.append(batch_num)\n                loss_avg.append(running_loss\/500.)\n                val_acc.append(validating(network, val_dataloader))\n                running_loss = 0.\n    if test:\n        train_accuracy = validating(network, train_dataloader)\n        val_accuracy = validating(network, val_dataloader)\n        print('Training accuracy: %.5f' % (train_accuracy))\n        print('Validation accuracy: %.5f' % (val_accuracy))\n    return network, ita, loss_avg, val_acc","22343807":"def validating(network, loader):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    correct_num = 0\n    total_num = 0\n    for i, data in enumerate(loader, 0):\n        digits, labels = data\n        total_num += labels.size(0)\n        digits, labels = digits.to(device), labels.to(device)\n        outputs = network(digits)\n        _, predicted = torch.max(outputs, 1)\n        correct_num += ((predicted == labels).sum().to(\"cpu\")).item()\n    accuracy = correct_num \/ total_num\n    return accuracy","054e9016":"lenet = BasicLeNet()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(lenet.parameters())\nlenet, batch_ita, loss_list, val_acc_list = training(lenet, criterion, optimizer, 30)","ab7ca6e6":"fig = plt.figure(figsize=(15, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(batch_ita, loss_list)\nplt.title(\"Loss function\")\n\nplt.subplot(1, 2, 2)\nplt.plot(batch_ita, val_acc_list)\nplt.title(\"Validation accuracy\")\n\nplt.show()","fe55c327":"def digit_argument(digit, angle, translate, scale):\n    digit_img = Image.fromarray(digit)\n    t1 = np.array([[1, 0, 14],\n                   [0, 1, 14],\n                   [0, 0, 1]])\n    t2 = np.array([[math.cos(angle), math.sin(angle), 0],\n                   [-math.sin(angle), math.cos(angle), 0],\n                   [0, 0, 1]])\n    t3 = np.array([[scale, 0, 0],\n                   [0, scale, 0],\n                   [0, 0, 1]])\n    t4 = np.array([[1, 0, -14],\n                   [0, 1, -14],\n                   [0, 0, 1]])\n    t5 = np.array([[1, 0, translate[0]],\n                   [0, 1, translate[1]],\n                   [0, 0, 1]])\n    t_inv = np.linalg.inv(t1 @ t2 @ t3 @ t4 @ t5)\n    digit_img = digit_img.transform((28, 28),\n                                    Image.AFFINE,\n                                    data=t_inv.flatten()[:6],\n                                    resample=Image.BILINEAR)\n    digit_arg = np.asarray(digit_img)\n    return digit_arg","eb11960f":"old_digit = data_np[10][0]\nprint(\"Digit Image before argumentation: \")\nplt.imshow(old_digit, cmap=\"gray\")\nplt.show()\nprint(\"Digit Image after Rotation: \")\nrotate_digit = digit_argument(old_digit, 1, [0, 0], 1)\nplt.imshow(rotate_digit, cmap=\"gray\")\nplt.show()\nprint(\"Digit Image after Translation: \")\ntranslate_digit = digit_argument(old_digit, 0, [-2, -2], 1)\nplt.imshow(translate_digit, cmap=\"gray\")\nplt.show()\nprint(\"Digit Image after Scaling: \")\nscale_digit = digit_argument(old_digit, 0, [0, 0], 1.2)\nplt.imshow(scale_digit, cmap=\"gray\")\nplt.show()","eca10e89":"class DigitDataset(Dataset):\n    \"\"\" Digit Dataset \"\"\"\n\n    def __init__(self, csv_file, root_dir, train=False, argument=True, transform=None):\n        self.digit_df = pd.read_csv(root_dir + csv_file)\n        self.transform = transform\n        self.train = train\n        self.argument = argument\n\n    def __len__(self):\n        if self.argument:\n            return 2 * len(self.digit_df)\n        else:\n            return len(self.digit_df)\n\n    def __getitem__(self, item):\n        if item < len(self.digit_df):\n            if self.train:\n                digit = self.digit_df.iloc[item, 1:].values\n                digit = digit.astype('float').reshape((28, 28))\n                label = self.digit_df.iloc[item, 0]\n            else:\n                digit = self.digit_df.iloc[item, :].values\n                digit = digit.astype('float').reshape((28, 28))\n                label = 0\n        else:\n            assert self.argument and self.train\n            digit = self.digit_df.iloc[item % len(self.digit_df), 1:].values\n            digit = digit.astype('float').reshape((28, 28))\n            rand_theta = (randint(-20, 20) \/ 180) * math.pi\n            rand_x = randint(-2, 2)\n            rand_y = randint(-2, 2)\n            rand_scale = randint(9, 11) * 0.1\n            digit = digit_argument(digit, rand_theta, [rand_x, rand_y], rand_scale)\n            label = self.digit_df.iloc[item % len(self.digit_df), 0]\n        sample = [digit, label]\n        if self.transform:\n            sample[0] = self.transform(sample[0])\n        return sample","fc138aaa":"def train_validate_split(digit_df, test_ratio=0.2, argument=True):\n    assert isinstance(digit_df, pd.DataFrame)\n    digit_num = len(digit_df)\n    overall_indices = [num for num in range(digit_num)]\n    overall_class_num = digits_per_class(digit_df, overall_indices)\n    test_class_num = [int(num*test_ratio) for num in overall_class_num]\n    tmp_test_class_num = [0 for num in range(10)]\n    shuffle(overall_indices)\n    train_indices = []\n    val_indices = []\n    for idx in overall_indices:\n        tmp_label = digit_df.iloc[idx, 0]\n        if tmp_test_class_num[tmp_label] < test_class_num[tmp_label]:\n            val_indices.append(idx)\n            tmp_test_class_num[tmp_label] += 1\n        else:\n            train_indices.append(idx)\n            if argument:\n                train_indices.append(idx + digit_num)\n    return train_indices, val_indices","3e694695":"class EnhancedLeNet(nn.Module):\n\n    def __init__(self):\n        super(EnhancedLeNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 64, 5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, 5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, 5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, 5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(128),\n            nn.MaxPool2d(2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(128*7*7, 512),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm1d(512),\n            nn.Linear(512, 10)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), 128*7*7)\n        x = self.classifier(x)\n        return x","49cb6053":"lenet = EnhancedLeNet()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(lenet.parameters())\nlenet, batch_ita, loss_list, val_acc_list = training(lenet, criterion, optimizer, 30)","5236c329":"fig = plt.figure(figsize=(15, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(batch_ita, loss_list)\nplt.title(\"Loss function\")\n\nplt.subplot(1, 2, 2)\nplt.plot(batch_ita, val_acc_list)\nplt.title(\"Validation accuracy\")\n\nplt.show()","c452a96d":"def testing(network):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    composed_transform = transforms.Compose([Regularize(), ToTensor()])\n    digit_dataset = DigitDataset('test.csv', '..\/input\/', train=False, argument=False, transform=composed_transform)\n    test_dataloader = DataLoader(\n        digit_dataset,\n        batch_size=128,\n        shuffle=False,\n        num_workers=4,\n        pin_memory=True\n    )\n    test_results = []\n    for i, data in enumerate(test_dataloader, 0):\n        digits, label = data\n        digits = digits.to(device)\n        outputs = network(digits)\n        _, predicted = torch.max(outputs, 1)\n        test_results += np.int_(predicted.to(\"cpu\").numpy().squeeze()).tolist()\n    \"\"\"\n    Write test results to a csv file\n    \"\"\"\n    test_df = pd.read_csv(\"..\/input\/sample_submission.csv\")\n    assert (len(test_df) == len(test_results))\n    test_df.loc[:, 'Label'] = test_results\n    test_df.to_csv('test_results.csv', index=False)\n    print(\"Test Results for Kaggle Generated ...\")","24e42f17":"lenet = EnhancedLeNet()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(lenet.parameters())\nlenet, batch_ita, loss_list, val_acc_list = training(lenet, criterion, optimizer, 50, test=False)\ntesting(lenet)","74a515db":"I also defined a validating function. The function efficiently generates the prediction accuracy of the network. I used this function to generate the training accuracy and validation accuracy after training.","97a0a2eb":"Here, the function computed the class distribution of the data given by Kaggle.","3cb3ca5d":"# Pytorch from a Beginner's View - Dataloader, Network, and Training\n\nAs a beginner in deep learning and PyTorch, I want to share my understandings and struggles from implementing a recognizer for the MNIST competition on Kaggle. Though the MNIST competition is never a hard problem, I want to use this opportunity as the foundation for other problems in the future. Because of this, I tried to implement the recognizer following a systematic pipeline (\"Systematic\" in my limited knowledge, I can do better with more experiences in deep learning). Here are the components I will cover in this notebook for a complete recognizer pipeline:\n\n1. **Dataset and Dataloader:** Split data into training and validation dataset respectively and randomly construct mini-batch.\n2. **Network:** Convolutional neural network starting with Basic LeNet-5.\n3. **Training (and Validation)**: Train the network with customized Dataloader.\n4. **Data Augmentation:** Add affine image transformations to the digit image. ","5693f499":"# 1. Dataset and Dataloader\n\nAfter some search, I found one of the standard ways to load data into models in PyTorch is using the Dataloader. To use the Dataloader, I have to define a Dataset class for the digit images. Then, the Dataloader splits the data into mini-batches for training.\n\n### 1.1 Basic Dataset Class\n\nFirst, I defined a Dataset class for the digit images. The PyTorch tutorial has a detailed explanation of how to construct a Dataset class ([Data Loading and Processing Tutorial](https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html)). Here, the Dataset class reads data from the CSV file when the getitem function is called.","2dfdefcc":"### 1.3 Dataloader\n\nWith a correctly defined Dataset class, I can now use the Dataloader provided by PyTorch to form mini-batches. The Dataloader prepares mini-batches during iteration. By setting the correct number for num_workers (base on the memory) and make pin_memory to True, the Dataloader can make the training iteration a lot faster on GPU.","b62dc5d0":"Here I also defined a more complicated network than LeNet-5 with more Conv layers and batch normalization.","13c5d1e1":"# 2. Network\n\nAfter some paper reading, I think the most natural network to start with is the LeNet-5 (If you want to read the paper, [Here is LeCun's LeNet-5 paper from 1998](http:\/\/yann.lecun.com\/exdb\/publis\/pdf\/lecun-01a.pdf)). Honestly, I am amazed by how much they already have back in 1998 that built the foundation of deep learning today.\n\nThis network can be tuned later to maximize the performance by,\n\n1. Adding more convolutional filters.\n2. Adding more convolutional layers.\n3. Increasing the dimension of fully connected layers.\n4. Adding batch normalization layers.\n5. and many other techniques.","c870428e":"Let's test the Dataset class by first defining a Dataset object without any transformation. Here, the result shows the digit image and label get directly from the Dataset object.","b78ec37e":"Finally, I can train and test the network. Here I used the Cross-Entropy Loss function and Adam optimizer. After 30 epochs (with 80% of the training data given by Kaggle), the network has a validation accuracy over 98.5% with a converged loss function.","2b7c8e50":"Now let's test the argumentation function.","d584e0d0":"# 3. Training (and Validation)\n\nNow I want to define a training function that trains any network. The function first creates a training Dataloader and a validation Dataloader, and then it optimizes the network using mini-batches from the Dataloader using iterations.\n","eb23ea32":"An easy way to use the generated index list is to use the sampler provided by PyTorch. By using the defined sampler in the Dataloader, the Dataloader will only prepare mini-batches from the data included in the sampler.\n\nNow we have all the necessary things we need on Dataset and Dataloader for training the model.","70c75c84":"Then, I defined a function to split the dataset into training and validation. The function returns two lists, representing the indices of the training set and the validation set, respectively. The code is relatively straightforward. I first get the class distribution of the complete dataset, then use it to compute the class distribution of the validation set based on the ratio provided to the user. Based on the validation class distribution, I put part of the data into the validation set.","46b82225":"Finally, I can train and test the new network with data argumentation. After 30 epochs, the network has a validation accuracy over 99% with a converged loss function.","fbd2dfcd":"Here shows the class distribution of the validation set is proportional to the training set.","e8befedf":"### 1.4 Training and Validation Data Split\n\nSince I have to validate how good is the trained model, I have to split the data from Kaggle into a training set and a validation set. At the same time, I want the validation set has the same class distribution as in the training set. (I tried to find a PyTorch function to do this but failed. Thus, I have to write one by myself. ) To do so, I first defined a function to compute the class distribution of a dataset, which returns how many digits are in the data for each label.","f5da7f6d":"Now I can test how the model performs on the real test data provided by Kaggle. To get a better testing performance, I have to use all the training data (not split into training and validation set) during training. I also defined a testing function to generate the predictions of the trained model.","61dd3649":"# 4. Data Augmentation\n\nThe result using the LeNet-5 from 1998 is very good. To push the result better than 99%, I need to do some additional things. The first extra thing I came to mind immediately is data augmentation. Since deep learning trains a model based on the training data it sees, having more data will benefit and train a better model. The only thing to be careful is not to overfit the model.\n\nThe most common data augmentation technique is the affine transformation on images. [Here is a detailed tutorial on affine transformation in python.](https:\/\/stackabuse.com\/affine-image-transformations-in-python-with-numpy-pillow-and-opencv\/) Here, I created a function for fundamental affine transformation on the digit image from the Kaggle dataset. The function can do three transformations, including rotation, translation, and scaling.","54c4d24b":"I also need to redefine the function of splitting the training data and the validation data. The validation data won't contain any argument image, and the training data won't contain any data argument from the validation data. Here I need to be very careful to get the correct validation data.","d66ef23e":"For each training image, I want to create a new argument image. Thus, the overall training data will be doubled after argumentation. To do so, I have to update the Dataset class to differentiate the original training data and the new argument data.","c0305397":"### 1.2 Basic Transformations\n\nDataset class can perform different transformations on the data. Here, I defined two transformations: Regularize pixel value from [0, 255] to [0, 1] and transform data type of the image from Numpy to PyTorch Tensor."}}