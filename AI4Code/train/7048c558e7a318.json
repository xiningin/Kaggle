{"cell_type":{"26e8a2da":"code","bcf1f00a":"code","8d148e1f":"code","f00e8724":"code","81e9e207":"code","b8f05e71":"code","bd74b0b1":"code","e85ad41f":"code","2a039e25":"code","c8128f25":"code","de001534":"code","10c01e92":"code","419d1678":"code","e177aa37":"code","7df0e3f1":"code","da9c0676":"code","cac8e779":"code","5a21bb51":"code","07b3a091":"code","39ab213f":"code","9075cf86":"code","c6eb9e05":"code","356be286":"code","751bd31b":"code","47e3800e":"code","fc8387d2":"code","3cdb91cd":"code","0a6566f5":"markdown","2374f589":"markdown","1b62db0e":"markdown","76c896b4":"markdown","a109374e":"markdown","1e0d2d24":"markdown","7d7facd9":"markdown","2bcbf4d2":"markdown","73f4000b":"markdown","09f8e86c":"markdown","1591ed8b":"markdown","ec0e6ac9":"markdown","50fefbb3":"markdown","93267b8d":"markdown","3a3d5989":"markdown","4aa93986":"markdown","3436cfe1":"markdown","35147718":"markdown","af8ed643":"markdown"},"source":{"26e8a2da":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","bcf1f00a":"data = pd.read_csv(\"..\/input\/data.csv\")\ndata.head(20)","8d148e1f":"\ndata = data.loc[:,[\"diagnosis\",\"radius_mean\",\n            \"fractal_dimension_mean\",\n           \"texture_mean\",\"perimeter_mean\",\n           \"area_mean\", \"smoothness_mean\",\n           \"compactness_mean\",\"concavity_mean\",\n           \"concave points_mean\",\"symmetry_mean\",\n           \"fractal_dimension_mean\"]]\n\ndata.info()","f00e8724":"data.describe()","81e9e207":"data.columns","b8f05e71":"sns.countplot(data.diagnosis)\n\ndata[\"diagnosis\"].value_counts()","bd74b0b1":"target_M_data = data[data.diagnosis == \"M\"] \ntarget_B_data = data[data.diagnosis == \"B\"]\n\ntrace1 = go.Scatter(\n                    x= target_M_data.radius_mean,\n                    y= target_M_data.area_mean,\n                    mode = \"markers\",\n                    name = \"Malignant\",\n                    marker = dict(color = \"rgba(120,15,150,0.8)\"),\n                    text = target_M_data.diagnosis)\n\ntrace2 = go.Scatter(\n                    x= target_B_data.radius_mean,\n                    y= target_B_data.area_mean,\n                    mode = \"markers\",\n                    name = \"Beningn\",\n                    marker = dict(color = \"rgba(66,222,222,0.8)\"),\n                    text = target_B_data.diagnosis)\n\nscatter_data = [trace1, trace2]\n\nlayout = dict(title = \"M or B according to radius and area\",\n             xaxis = dict(title = \"radius\"),\n             yaxis = dict(title = \"area\")\n             )\n\nfig = dict(layout = layout, data = scatter_data)\n\niplot(fig)\n","e85ad41f":"trace1 = go.Scatter(\n                    x= target_M_data.radius_mean,\n                    y= target_M_data.smoothness_mean,\n                    mode = \"markers\",\n                    name = \"Malignant\",\n                    marker = dict(color = \"rgba(120,15,150,0.8)\"),\n                    text = target_M_data.diagnosis)\n\ntrace2 = go.Scatter(\n                    x= target_B_data.radius_mean,\n                    y= target_B_data.smoothness_mean,\n                    mode = \"markers\",\n                    name = \"Beningn\",\n                    marker = dict(color = \"rgba(66,222,222,0.8)\"),\n                    text = target_B_data.diagnosis)\n\nscatter_data = [trace1, trace2]\n\nlayout = dict(title = \"M or B according to radius and smoothness\",\n             xaxis = dict(title = \"radius\"),\n             yaxis = dict(title = \"smoothness\")\n             )\n\nfig = dict(layout = layout, data = scatter_data)\n\niplot(fig)\n\n","2a039e25":"plt.bar(data.radius_mean, data.smoothness_mean, color=\"blue\")","c8128f25":"\ntrace1 = go.Scatter(\n                    x= target_M_data.concavity_mean,\n                    y= target_M_data.compactness_mean,\n                    mode = \"markers\",\n                    name = \"Malignant\",\n                    marker = dict(color = \"rgba(120,15,150,0.8)\"),\n                    text = target_M_data.diagnosis)\n\ntrace2 = go.Scatter(\n                    x= target_B_data.concavity_mean,\n                    y= target_B_data.compactness_mean,\n                    mode = \"markers\",\n                    name = \"Beningn\",\n                    marker = dict(color = \"rgba(66,222,222,0.8)\"),\n                    text = target_B_data.diagnosis)\n\nscatter_data = [trace1, trace2]\n\nlayout = dict(title = \"M or B according to concavity and compactness\",\n             xaxis = dict(title = \"concavity\"),\n             yaxis = dict(title = \"compactness\")\n             )\n\nfig = dict(layout = layout, data = scatter_data)\n\niplot(fig)","de001534":"\ntrace1 = go.Scatter(\n            x = target_M_data.symmetry_mean,\n            y = target_M_data.texture_mean,\n            mode = \"markers\",\n            name = \"Malignant\",\n            marker = dict(color = \"rgba(200,5,5,0.8)\"),\n            text = target_M_data.diagnosis\n\n)\n\ntrace2 = go.Scatter(\n            x = target_B_data.symmetry_mean,\n            y = target_B_data.texture_mean,\n            mode = \"markers\",\n            name = \"Beningn\",\n            marker = dict(color = \"rgba(5,5,200,0.8)\"),\n            text = target_B_data.diagnosis\n\n)\n\n\ngraph_data = [trace1, trace2]\n\nlayout = dict(title =  \"M or B according to symmetry and texture\",\n             xaxis = dict(title = \"symmetry\"),\n             yaxis = dict(title = \"texture\"))\n\n\nfig = dict(layout = layout,data = graph_data)\n\n\niplot(fig)\n\n","10c01e92":"radius_group = []\n\nfor each in data.radius_mean:\n    if each <= 13:\n        radius_group.append(\"probably good\")\n    elif 13 < each < 17:\n        radius_group.append(\"test is needed\")\n    else:\n        radius_group.append(\"probably bad\")\n        \n        \ndata[\"first_look\"] = radius_group\n\nsns.countplot(data.first_look)\n\ndata[\"first_look\"].value_counts()\n\n\n","419d1678":"\npd.crosstab(data.first_look,data.diagnosis).plot(kind=\"bar\",figsize=(10,6))\n\nplt.title(\"Is first look successfull to diagnose\")\nplt.xlabel(\"First look observations\")\nplt.ylabel(\"Distribution\")\nplt.show()","e177aa37":"y = data.diagnosis\n\nx = data.drop([\"diagnosis\",\"first_look\"],axis=1)\n\nx_norm = (x-x.min())\/(x.max()-x.min())\n\nnew_data = pd.concat([y,x_norm],axis=1)\nnew_data = pd.melt(new_data,\n                  id_vars =\"diagnosis\",\n                  var_name = \"features\",\n                  value_name = \"value\")\n\nplt.figure(figsize = (20,10))\n\nsns.boxplot(x=\"features\", y=\"value\", hue = \"diagnosis\", data=new_data)\nplt.xticks(rotation = 90)\n\n","7df0e3f1":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x_norm,y,test_size =0.3,random_state=42)\n\nprint(\"X train :  {}\".format(x_train.shape))\nprint(\"X test : {}\".format(x_test.shape))\nprint(\"Y train : {}\".format(y_train.shape))\nprint(\"Y test : {}\".format(y_test.shape))","da9c0676":"from sklearn.neighbors import KNeighborsClassifier\n\ntrain_score = []\ntest_score =[]\n\nfor each in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors=each)\n    knn.fit(x_train,y_train)\n    \n    train_score.append(knn.score(x_train,y_train))\n    test_score.append(knn.score(x_test,y_test))\n\n\nplt.figure(figsize = (10,6))\nplt.plot(train_score, label = \"Train accuracy\")\nplt.plot(test_score, label = \"Test accuracy\")\nplt.grid()\nplt.xlabel(\"n neighbor\")\nplt.ylabel(\"score\")\n\n\ncheck = 0\ncount =0\nfor num in test_score:\n    if check < num :\n        check = num\n        count += 1\n    else:\n        pass\n\nprint(\"The most accuracy is {} with the neighbor value of {}\".format(check,count))\n\n","cac8e779":"knn2 = KNeighborsClassifier(n_neighbors = 2)\nknn2.fit(x_train,y_train)\n\n\ny_pred = knn2.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_true,y_pred)\n\nsns.heatmap(cm,annot= True,fmt=\"d\")\n","5a21bb51":"data1 = data[data.diagnosis ==\"M\"]\n\n\nx = np.array(data1.loc[:,\"radius_mean\"]).reshape(-1,1)\n\ny= np.array(data1.loc[:,\"concave points_mean\"]).reshape(-1,1)\n\nplt.figure(figsize = (10,6))\n\nplt.scatter(x=x,y=y)\nplt.title(\"Graph of radius and concave point\")\nplt.xlabel(\"radius\")\nplt.ylabel(\"concave point\")\n\n","07b3a091":"from sklearn.linear_model import LinearRegression\n\nreg = LinearRegression()\npredict_space = np.linspace(min(x), max(x)).reshape(-1,1)\n\n\nreg.fit(x,y)\npredict = reg.predict(predict_space)\n\nprint(\"Score of regression : {}\".format(reg.score(x,y)))\n\nplt.figure(figsize = (10,6))\nplt.plot(predict_space, predict, color='black', linewidth=3)\nplt.scatter(x=x,y=y)\nplt.title(\"Graph of radius and concave point\")\nplt.xlabel(\"radius\")\nplt.ylabel(\"concave point\")\n\n\n\n\n\n\n","39ab213f":"from sklearn.model_selection import cross_val_score\nreg = LinearRegression()\nk = 5\ncv_result = cross_val_score(reg,x,y,cv=k) \nprint('CV Scores: ',cv_result)\nprint('CV scores average: ',np.sum(cv_result)\/k)","9075cf86":"# ROC Curve with logistic regression\nfrom sklearn.metrics import roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\n# abnormal = 1 and normal = 0\ndata['class_binary'] = [1 if i == \"M\" else 0 for i in data.loc[:,\"diagnosis\"]]\nx,y = data.loc[:,(data.columns != \"diagnosis\") & (data.columns != \"class_binary\") &(data.columns != \"first_look\")],data.loc[:,'class_binary']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=42)\nlogreg = LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred_prob = logreg.predict_proba(x_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\nplt.show()","c6eb9e05":"from sklearn.ensemble import RandomForestClassifier\n\nx_train , x_test, y_train, y_test = train_test_split(x_norm,y,test_size = 0.3,random_state = 42)\nrf = RandomForestClassifier()\n\nrf.fit(x_train,y_train)\n\nscore = rf.score(x_test,y_test)\n\nprint(\"Accuracy of the Random Forest Algorithm : {}\".format(score))\n\n","356be286":"from sklearn.metrics import confusion_matrix\n\ny_pred = rf.predict(x_test)\ny_true = y_test\n\ncm = confusion_matrix(y_true,y_pred)\nsns.heatmap(cm, annot = True,fmt=\"d\")","751bd31b":"from sklearn.metrics import classification_report\n\nreport = classification_report(y_true,y_pred)\n\nprint(\"Classification report : {}\".format(report))","47e3800e":"plt.scatter(data[\"radius_mean\"], data[\"compactness_mean\"])\nplt.xlabel(\"radius\")\nplt.ylabel(\"compactness\")\nplt.title(\"Distribution\")","fc8387d2":"new_data = data.loc[:,[\"radius_mean\",\"compactness_mean\"]]\nfrom sklearn.cluster import KMeans\n\nkmean = KMeans(n_clusters = 2)\n\nkmean.fit(new_data)\n\nlabels = kmean.predict(new_data)\n\nplt.scatter(data[\"radius_mean\"], data[\"compactness_mean\"], c = labels)","3cdb91cd":"df = pd.DataFrame({\"label\" : labels , \"class\" : data.diagnosis} )\n\nnew_df = pd.crosstab(df[\"label\"],df[\"class\"])\nnew_df","0a6566f5":"According to the upper graph, we realize that smoothness is not a good classifier for our data. Since we do not observe a clear difference between to cell type.","2374f589":"**REGRESSION**\n\n\n","1b62db0e":"**KNN ALGORITHM**","76c896b4":"Lets create another column and call it first look. In that column we just put datas into the groups considering their radius.","a109374e":"It is clearly seen that dangerous cell (Malignant) has a bigger concavity and compactness rather than Beningn.","1e0d2d24":"**KMEANS**","7d7facd9":"In this kernell, we will try to predict is cancer bad or good? Also we will see the distribution of features and some comparison about features.","2bcbf4d2":"**FIRST LOOK TO DATA AND CLEANING IT**\n\ndiagnosis = Malignant for M, Benign for B\n* Malignant ---->>> dangereous\n* Benign ----->>> harmless\n\n\nTen real-valued features are computed for each cell nucleus: \n\n* a) radius (mean of distances from center to points on the perimeter) \n* b) texture (standard deviation of gray-scale values) \n* c) perimeter \n* d) area \n* e) smoothness (local variation in radius lengths) \n* f) compactness (perimeter^2 \/ area - 1.0) \n* g) concavity (severity of concave portions of the contour) \n* h) concave points (number of concave portions of the contour) \n* i) symmetry \n* j) fractal dimension (\"coastline approximation\" - 1)\n\n\nIn this kernell we will use the upper features of data.\n","73f4000b":"We can check our data with cross validation. In the below I choose k as 5, so it means that data is predicted 5 times.","09f8e86c":"In the upper table represents the how much class \"B\" and class \"M\" in the label 0 and label 1. For example,\n* In label 0 (violet) 351 B and 76 M located.\n* In label 1 (yellow) 6 B and 136 M located.","1591ed8b":"**VISUALIZATION OF DATA**\n\nIn this part, we will make a comparison between each feature with another feature, so we will find the which feature effects how to our data? Is there a positive correlation or negative correlation between each other.","ec0e6ac9":"Clearly see that when symmetry and texture mean increase also chance of being malignant increases.","50fefbb3":"In the upper graph, we can see the effect of the each feature individually.","93267b8d":"The upper graph explains the radius,area relationship and also the positions of Malignant and Bening cells. It can be easily seen that, when the cell radius and area increased, it is closer to the Malignant.","3a3d5989":"* With KNN algorithm we get %95 accuracy with using K=2\n* The other K values also give us different result\n* As can be seen in the confusion matrix,totally we can not predict only 7 values.(6+1)\n","4aa93986":"This graph also proves our statement, because same cituation is here. All the bars have a similar length.","3436cfe1":"Yeah probably directly looking to radius is a good option for the first look but also some exceptions are exist.","35147718":"**FUTURE ENGINEERING**","af8ed643":"**RANDOM FOREST**"}}