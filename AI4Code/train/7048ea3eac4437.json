{"cell_type":{"6a008228":"code","5fe7a895":"code","b416677e":"code","123a914b":"code","a727d358":"code","44814722":"code","78505f76":"code","99696d43":"code","79496877":"code","ef138ac7":"code","15d8d86e":"code","2f883faf":"code","c7813b92":"code","fe957a7c":"code","18202320":"code","46c47020":"code","b6f80b0e":"code","7ea86f97":"code","5947f805":"code","1886de74":"markdown"},"source":{"6a008228":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5fe7a895":"df = pd.read_fwf('\/kaggle\/input\/game-of-thrones-books\/002ssb.txt',sep='\\s+', index_col=False,columns=['Index'])","b416677e":"df.columns","123a914b":"import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk","a727d358":"sent_array=df.loc[0:500].to_numpy()","44814722":"len(sent_array)","78505f76":"df1=pd.DataFrame(data=sent_array.flatten(),columns=['Sentences'])","99696d43":"df1['Sentences'][3]","79496877":"len(df1['Sentences'])","ef138ac7":"from nltk.tokenize import word_tokenize\nimport re\n\ndef clean_text(\n    string: str, \n    punctuations=r'''!()-[]{};:'\"\\,<>.\/?@#$%^&*_~''',\n    stop_words=['the', 'a', 'and', 'is', 'be', 'will']) -> str:\n    \"\"\"\n    A method to clean text \n    \"\"\"\n    # Cleaning the urls\n    string = re.sub(r'https?:\/\/\\S+|www\\.\\S+', '', string)\n\n    # Cleaning the html elements\n    string = re.sub(r'<.*?>', '', string)\n\n    # Removing the punctuations\n    for x in string.lower(): \n        if x in punctuations: \n            string = string.replace(x, \"\") \n\n    # Converting the text to lower\n    string = string.lower()\n\n    # Removing stop words\n    string = ' '.join([word for word in string.split() if word not in stop_words])\n\n    # Cleaning the whitespaces\n    string = word_tokenize(string)\n\n    return string ","15d8d86e":"text_window=2\nall_text=[]\nword_lists=[]\nfor text in df1['Sentences']:\n    text = clean_text(text)\n    \n\n    # Appending to the all text list\n    all_text += text \n\n    # Creating a context dictionary\n    for i, word in enumerate(text):\n        for w in range(text_window):\n            # Getting the context that is ahead by *window* words\n            if i + 1 + w < len(text): \n                word_lists.append([word] + [text[(i + 1 + w)]])\n                \n            # Getting the context that is behind by *window* words    \n            if i - w - 1 >= 0:\n                word_lists.append([word] + [text[(i - w - 1)]])","2f883faf":"def create_dict(text):\n    word_list=list(set(text))\n    word_list.sort()\n    word_freq={};\n    for i,word in enumerate(word_list):\n        word_freq.update({\n            word:i\n        })\n        \n    return word_freq;","c7813b92":"word_freq=create_dict(all_text)\nword_lists","fe957a7c":"from scipy import sparse\nfrom tqdm import tqdm\nX=[]\nY=[]\nnum_words=len(word_freq)\nwords=word_freq.keys()\n\nfor i,context_word in tqdm(enumerate(word_lists)):\n    main_word_index=word_freq.get(context_word[0])\n    context_word_index=word_freq.get(context_word[1])\n    \n    X_row=np.zeros(num_words)\n    Y_row=np.zeros(num_words)\n    \n    X_row[main_word_index]=1\n    Y_row[context_word_index]=1\n    \n    X.append(X_row)\n    Y.append(Y_row)\n    \nX = np.asarray(X)\nY = np.asarray(Y)","18202320":"import tensorflow as tf\nfrom keras.models import Input, Model\nfrom keras.layers import Dense\nimport matplotlib.pyplot as plt","46c47020":"tf.__version__","b6f80b0e":"embed_size = 2\nimport itertools\n# Defining the neural network\ninp = Input(shape=(X.shape[1],))\nx = Dense(units=embed_size, activation='linear')(inp)\nx = Dense(units=Y.shape[1], activation='softmax')(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam')","7ea86f97":"model.summary()","5947f805":"model.fit(\n    x=X, \n    y=Y, \n    batch_size=32,\n    epochs=500\n    )","1886de74":"Since there is no classification , let's have a look at POS Tagging."}}