{"cell_type":{"e56cd486":"code","14759b04":"code","d95a75b6":"code","698e0e7c":"code","e76214e6":"code","7d8dc430":"code","7bbd6850":"code","1ff8023a":"code","87595c21":"code","3868271a":"code","3df657b0":"code","15bde724":"code","a5534525":"code","01d7f62b":"code","12f2c334":"code","be669c9f":"code","9d516303":"code","a6f6a7bf":"code","5b62cdaa":"code","bd96220b":"code","b742795c":"code","c2db35df":"code","2ae4dc45":"code","5b012732":"code","d0b7e9b3":"code","11647567":"markdown","73e6eb6e":"markdown","36fa6488":"markdown","22ce1def":"markdown","9f20ca6d":"markdown","982ce641":"markdown","b7bc8f47":"markdown","f9346d25":"markdown","e61c4eb4":"markdown","08d2a04d":"markdown","8481161a":"markdown","1d2e4450":"markdown","f3e08666":"markdown","0e19ba5b":"markdown","b3a660ad":"markdown","f2ca7e50":"markdown"},"source":{"e56cd486":"import numpy as np \nimport pandas as pd \n\nimport os, random\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nimport random as python_random\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport tensorflow as tf; print(tf.__version__)\nimport tensorflow_hub as hub\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import Input, Model, Sequential, layers\n\n# control gpu ram growth \ntf.config.optimizer.set_jit(True)\nphysical_devices = tf.config.list_physical_devices('GPU')\ntry: tf.config.experimental.set_memory_growth(physical_devices[0], True)\nexcept: pass \n\n# for reproducibiity\ndef seed_all(s):\n    random.seed(s)\n    python_random.seed(s)\n    tf.random.set_seed(s)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n    os.environ['PYTHONHASHSEED'] = str(0)\n    \n# seed all\nSEED  = 1994\nsns.set(style=\"darkgrid\")\nseed_all(SEED)","14759b04":"# Set DEVICE = 'TPU' for training. Please check version 2. \nDEVICE = 'GPU' \n\nif DEVICE == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        DEVICE = \"GPU\"\n\nif DEVICE != \"TPU\":\n    print(\"Using default strategy for CPU and single GPU\")\n    strategy = tf.distribute.get_strategy()\n\nif DEVICE == \"GPU\":\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n    physical_devices = tf.config.list_physical_devices('GPU')\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n    \n    \nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","d95a75b6":"IMG_SIZE   = 896 \nBATCH_SIZE = 10\nEPOCHS     = 5\n\n# Folders\nif DEVICE == \"TPU\":\n    from kaggle_datasets import KaggleDatasets\n    DATA_DIR = '\/kaggle\/input\/petfinder-pawpularity-score\/'\n    GCS_PATH  = KaggleDatasets().get_gcs_path('petfinder-pawpularity-score')\n    TRAIN_DIR = GCS_PATH + '\/train\/'\n    TEST_DIR  = GCS_PATH + '\/test\/'\nelse:\n    DATA_DIR  = '\/kaggle\/input\/petfinder-pawpularity-score\/'\n    TRAIN_DIR = DATA_DIR + 'train\/'\n    TEST_DIR  = DATA_DIR + 'test\/'","698e0e7c":"from sklearn.model_selection import StratifiedKFold\n\nFOLDS = 10\n# Load Train Data\ntrain_df = pd.read_csv(f'{DATA_DIR}train.csv')\ntrain_df['Id'] = train_df['Id'].apply(lambda x: f'{TRAIN_DIR}{x}.jpg')\n# Set a specific label to be able to perform stratification\ntrain_df['stratify_label'] = pd.qcut(train_df['Pawpularity'], q = 30, labels = range(30))\n# Label value to be used for feature model 'classification' training.\ntrain_df['target_value'] = train_df['Pawpularity'] \/ 100.\n# list of feature that will be second input along with image input\ndense_features = [\n    'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n    'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'\n]\nkfold = StratifiedKFold(n_splits = FOLDS,  shuffle = True, random_state = SEED)\ntrain_df['kfold'] = -1\nfor fold, (train_index, val_index) in enumerate(kfold.split(train_df.index,\n                                                            train_df['stratify_label'])):\n    train_df.loc[val_index, 'kfold'] = fold \n    \n# save or not\ntrain_df.to_csv(\"train_df_folds.csv\", index=False)\ndisplay(train_df.head(3))\n\n# test set \ntest_df = pd.read_csv(f'{DATA_DIR}test.csv')\ntest_df['Id'] = test_df['Id'].apply(lambda x: f'{TEST_DIR}{x}.jpg')\ntest_df['Pawpularity'] = 0\ndisplay(test_df.head(3))","e76214e6":"AUTOTUNE = tf.data.experimental.AUTOTUNE  \ndef build_augmenter(is_labelled):\n    def augment(img):\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_flip_up_down(img)\n        img = tf.image.random_saturation(img, 0.95, 1.05)\n        img = tf.image.random_brightness(img, 0.05)\n        img = tf.image.random_contrast(img, 0.95, 1.05)\n        img = tf.image.random_hue(img, 0.05)\n        return img\n    \n    def augment_with_labels(path, label):\n        image, feature = path \n        augment_image = augment(image)\n        return (augment_image, feature), label\n    \n    def augment_without_labels(path, _):\n        image, feature = path \n        augment_image = augment(image)\n        return (augment_image, feature), None\n    \n    return augment_with_labels if is_labelled else augment_without_labels","7d8dc430":"def build_decoder(is_labelled):\n    def decode(path):\n        file_bytes = tf.io.read_file(path)\n        img = tf.image.decode_jpeg(file_bytes, channels = 3)\n        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE)) \n        return tf.divide(img, 255.)\n    \n    def decode_with_labels(path, label):\n        image, feature = path \n        decode_image = decode(image)\n        return (decode_image, feature), label \n    \n    def decode_without_labels(path, feature):\n        decode_image = decode(path)\n        return (decode_image, feature), None\n    \n    return decode_with_labels if is_labelled else decode_without_labels\n\ndef create_dataset(df, \n                   batch_size  = 32, \n                   is_labelled = False, \n                   augment     = False,\n                   repeat      = False, \n                   shuffle     = False):\n    \n    decode_fn    = build_decoder(is_labelled)\n    augmenter_fn = build_augmenter(is_labelled)\n    \n    # Create Dataset\n    if is_labelled:\n        dataset_sample = tf.data.Dataset.from_tensor_slices((df['Id'].values, df[dense_features].values))\n        dataset_labels = tf.data.Dataset.from_tensor_slices(df['target_value'].values)\n        dataset = tf.data.Dataset.zip((dataset_sample, dataset_labels))\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices((df['Id'].values, df[dense_features].values))\n        \n    dataset = dataset.map(decode_fn, num_parallel_calls = AUTOTUNE)\n    dataset = dataset.map(augmenter_fn, num_parallel_calls = AUTOTUNE) if augment else dataset\n    dataset = dataset.repeat() if repeat else dataset\n    dataset = dataset.shuffle(1024 * REPLICAS, reshuffle_each_iteration = True) if shuffle else dataset\n    dataset = dataset.batch(batch_size * REPLICAS, drop_remainder=shuffle)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","7bbd6850":"training_dataset = create_dataset(train_df, \n                                  batch_size  = BATCH_SIZE, \n                                  is_labelled = True, \n                                  augment     = True,\n                                  repeat      = False, \n                                  shuffle     = False)\n(sample_images, sample_feature), sample_labels = next(iter(training_dataset))\nprint(sample_images.shape, sample_feature.shape, sample_labels.shape)\n\nimport matplotlib.pyplot as plt \nplt.figure(figsize=(16, 10))\nfor i, (image, label) in enumerate(zip(sample_images[:8], sample_labels[:8])):\n    ax = plt.subplot(3, 4, i + 1)\n    plt.title(f'{label.numpy()} , Raw: {image.numpy().shape}')\n    plt.imshow(image.numpy().squeeze())\n    plt.axis(\"off\")","1ff8023a":"class PatchEmbed(tf.keras.layers.Layer):\n    def __init__(self, img_size=(224, 224), patch_size=(4, 4),  embed_dim=96):\n        super().__init__(name='patch_embed')\n        patches_resolution = [img_size[0] \/\/ patch_size[0], \n                              img_size[1] \/\/ patch_size[1]]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n        self.embed_dim = embed_dim\n        self.proj = layers.Conv2D(embed_dim, \n                                  kernel_size=patch_size, \n                                  strides=patch_size, name='proj')\n        self.norm = layers.LayerNormalization(epsilon=1e-5, name='norm')\n     \n    def call(self, x):\n        B, H, W, C = x.get_shape().as_list()\n        x = self.proj(x)\n        x = tf.reshape(\n            x, shape=[-1, \n                      (H \/\/ self.patch_size[0]) * (W \/\/ self.patch_size[0]), \n                      self.embed_dim]\n        )\n        x = self.norm(x)\n        return x","87595c21":"class ExternalAttention(layers.Layer):\n    def __init__(self, dim, num_heads, dim_coefficient = 4, \n                 attention_dropout = 0,  projection_dropout = 0, \n                 **kwargs):\n        super(ExternalAttention, self).__init__(name= 'ExternalAttention', **kwargs)\n        self.dim       = dim \n        self.num_heads = num_heads \n        self.dim_coefficient    = dim_coefficient\n        self.attention_dropout  = attention_dropout\n        self.projection_dropout = projection_dropout\n        \n        k = 256 \/\/ dim_coefficient\n        self.trans_dims = layers.Dense(dim * dim_coefficient)\n        self.linear_0 = layers.Dense(k)\n        self.linear_1 = layers.Dense(dim * dim_coefficient \/\/ num_heads)\n        self.proj = layers.Dense(dim)\n    \n        self.attn_drop  = layers.Dropout(attention_dropout)\n        self.proj_drop  = layers.Dropout(projection_dropout)\n        \n    def call(self, inputs, return_attention_scores=False, training=None):\n        num_patch = tf.shape(inputs)[1]\n        channel   = tf.shape(inputs)[2]\n        x = self.trans_dims(inputs)\n        x = tf.reshape(x, shape=(-1, \n                                 num_patch, \n                                 self.num_heads,\n                                 self.dim * self.dim_coefficient \/\/ self.num_heads))\n        x = tf.transpose(x, perm=[0, 2, 1, 3])\n        \n        # a linear layer M_k\n        attn = self.linear_0(x)\n        # normalize attention map\n        attn = layers.Softmax(axis=2)(attn)\n        # dobule-normalization\n        attn = attn \/ (1e-9 + tf.reduce_sum(attn, axis=-1, keepdims=True))\n        attn_drop = self.attn_drop(attn, training=training)\n        \n        # a linear layer M_v\n        attn_dense = self.linear_1(attn_drop)\n        x = tf.transpose(attn_dense, perm=[0, 2, 1, 3])\n        x = tf.reshape(x, [-1, num_patch, self.dim * self.dim_coefficient])\n        # a linear layer to project original dim\n        x = self.proj(x)\n        x = self.proj_drop(x, training=training)\n  \n        if return_attention_scores:\n            return x, attn\n        else:\n            return x \n        \n    def get_config(self):\n        config = {\n            'dim'                : self.dim,\n            'num_heads'          : self.num_heads,\n            'dim_coefficient'    : self.dim_coefficient,\n            'attention_dropout'  : self.attention_dropout,\n            'projection_dropout' : self.projection_dropout\n        }\n        base_config = super(ExternalAttention, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","3868271a":"class MLP(layers.Layer):\n    def __init__(self, mlp_dim, embedding_dim=None, \n                 act_layer=tf.nn.gelu, drop_rate=0.2, **kwargs):\n        super(MLP, self).__init__(name='MLP', **kwargs)\n        self.fc1  = layers.Dense(mlp_dim, activation=act_layer)\n        self.fc2  = layers.Dense(embedding_dim)\n        self.drop = layers.Dropout(drop_rate)\n\n    def call(self, inputs, training=None):\n        x = self.fc1(inputs)\n        x = self.drop(x, training=training)\n        x = self.fc2(x)\n        x = self.drop(x, training=training)\n        return x","3df657b0":"patch_size       = 4\nnum_heads        = 4\nembedding_dim    = 128\nmlp_dim          = 64\ndim_coefficient  = 4\nnum_patches      = (IMG_SIZE \/\/ patch_size) ** 2\nattention_dropout   = 0.2\nprojection_dropout  = 0.2\nnum_ext_transformer_blocks = 10","15bde724":"class AttentionEncoder(layers.Layer):\n    def __init__(self, embedding_dim, \n                 mlp_dim, num_heads, \n                 dim_coefficient,  \n                 attention_dropout,  \n                 projection_dropout, \n                 get_attention_matrix=False,\n                 **kwargs):\n        super(AttentionEncoder, self).__init__(**kwargs)\n        self.embedding_dim = embedding_dim\n        self.mlp_dim   = mlp_dim\n        self.num_heads = num_heads\n        self.dim_coefficient    = dim_coefficient\n        self.attention_dropout  = attention_dropout\n        self.projection_dropout = projection_dropout\n        self.get_attention_matrix = get_attention_matrix\n        self.mlp = MLP(mlp_dim, embedding_dim)\n        \n        self.etn = ExternalAttention(\n            embedding_dim,\n            num_heads,\n            dim_coefficient,\n            attention_dropout,\n            projection_dropout\n        )\n    \n    def call(self, inputs):\n        residual_1 = inputs \n        x, ext_attention_scores = self.etn(inputs, return_attention_scores=True) \n        x = layers.add([x, residual_1])\n        residual_2 = x\n        x = self.mlp(x)\n        x = layers.add([x, residual_2])\n        \n        if self.get_attention_matrix:\n            return x, ext_attention_scores\n        else:\n            return x \n    \n    def get_config(self):\n        config = {\n            'embedding_dim'     : self.embedding_dim,\n            'mlp_dim'           : self.mlp_dim,\n            'num_heads'         : self.num_heads,\n            'dim_coefficient'   : self.dim_coefficient,\n            'attention_dropout' : self.attention_dropout,\n            'projection_dropout': self.projection_dropout,\n            'get_attention_matrix': self.get_attention_matrix\n        }\n        base_config = super(AttentionEncoder, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","a5534525":"def get_model(plot_model, print_summary, with_compile):\n    # multi-input \n    image_inputs  = layers.Input((IMG_SIZE, IMG_SIZE, 3))\n    feature_input =  layers.Input((len(dense_features),))\n\n    # base mdoel: image-net (replace with yours)\n    # multi-output \n    backbone = tf.keras.applications.ResNet50(\n        include_top=False,\n        weights=None,\n        input_tensor=layers.Input((IMG_SIZE, IMG_SIZE, 3)),\n    )\n    multi_op_backbone = Model(\n        backbone.input, \n        [\n            backbone.get_layer('conv2_block3_out').output, # for transformer blocks \n            backbone.output # for cnn blocks \n        ]\n    )\n    mid_y, last_y = multi_op_backbone(image_inputs)\n    \n    # Tranformer Blocks \n    patchedx = PatchEmbed(\n        img_size=(224, 224),\n        patch_size=(patch_size, patch_size),\n        embed_dim=embedding_dim\n    )(mid_y)\n    \n    x = patchedx\n    for _ in range(num_ext_transformer_blocks):\n        x, attn_weight_matrix = AttentionEncoder(\n            embedding_dim,\n            mlp_dim,\n            num_heads,\n            dim_coefficient,\n            attention_dropout,\n            projection_dropout,\n            get_attention_matrix = True\n        )(x)\n\n    # end layers : for transformer head \n    tail_1 = Sequential(\n        [\n            layers.GlobalAveragePooling1D(),\n            layers.Dropout(0.5),\n            layers.BatchNormalization()\n        ], name='tail_1'\n    )\n    \n    # end layers : for cnn head \n    tail_2 = Sequential(\n        [\n            layers.GlobalAveragePooling2D(),\n            layers.Dropout(0.5),\n        ], name='tail_2'\n    )\n    \n    # end layers : head layers for feature input (simple mlp)\n    tail_3 = Sequential(\n        [\n            layers.Dense(32,  activation='selu'),\n            layers.Dense(64,  activation='selu'),\n            layers.Dense(128, activation='selu'),\n            layers.Dropout(0.2),\n        ], name='tail_3'\n    )\n    \n    # bring all together \n    cating = tf.concat(\n        [\n            tail_1(x), \n            tail_2(last_y), \n            tail_3(feature_input)\n        ], \n        axis=-1\n    )\n    classifier = layers.Dense(1, activation = 'sigmoid')(cating)\n    \n    # construct the DAG graph \n    model = Model([image_inputs, feature_input], classifier)\n\n    # plotting \n    if plot_model:\n        display(tf.keras.utils.plot_model(model, \n                                          show_shapes=True, \n                                          show_layer_names=True,  \n                                          expand_nested=False))\n    # overal summary  \n    if print_summary:\n        print(model.summary())\n        \n    # compiling \n    if with_compile:\n        model.compile(\n            optimizer = optimizers.Adam(), \n            loss = losses.BinaryCrossentropy(), \n            metrics = [metrics.RootMeanSquaredError('rmse')])  \n\n    return model ","01d7f62b":"model = get_model(plot_model    = True,  \n                  print_summary = True, \n                  with_compile  = False)","12f2c334":"from tensorflow.keras import losses, optimizers , metrics\nfrom tensorflow.keras import callbacks\n\ndef get_lr_callback(batch_size=8):\n    lr_start   = 0.000005\n    lr_max     = 0.00000125 * batch_size * REPLICAS\n    lr_min     = 0.000001\n    lr_ramp_ep = 5\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start\n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n        return lr\n    return callbacks.LearningRateScheduler(lrfn, verbose=True)\n\n\n# Set Callbacks\ndef model_checkpoint(fold):\n    return callbacks.ModelCheckpoint(f'feature_model_{fold}.h5',\n                                              verbose = 1, \n                                              monitor = 'val_rmse', \n                                              mode  = 'min', \n                                              save_weights_only = True,\n                                              save_best_only    = True)","be669c9f":"training_fold = 0 # 1st fold training: total fold 10\nprint(f'\\nFold {training_fold}\\n')\n\ndf = pd.read_csv(\".\/train_df_folds.csv\")\ndf_train = df[df.kfold != training_fold].reset_index(drop=True)\ndf_valid = df[df.kfold == training_fold].reset_index(drop=True)\n\nwith strategy.scope():\n    # Create Model\n    model = get_model(plot_model=False, print_summary=False, with_compile=True)\n\ntraining_dataset = create_dataset(df_train, \n                                  batch_size  = BATCH_SIZE, \n                                  is_labelled = True, \n                                  repeat      = True, \n                                  shuffle     = True)\nvalidation_dataset = create_dataset(df_valid, \n                                    batch_size  = BATCH_SIZE, \n                                    is_labelled = True,\n                                    repeat      = True, \n                                    shuffle     = False)\n\n# Only True for training. \n# Current Image_Size is too big for other device.\nif DEVICE == \"TPU\":\n    # Fit Model\n    history = model.fit(training_dataset,\n                        epochs = EPOCHS,\n                        steps_per_epoch  = df_train.shape[0] \/ batch_size \/\/ REPLICAS,\n                        validation_steps = df_valid.shape[0] \/ batch_size \/\/ REPLICAS,\n                        callbacks = [model_checkpoint(training_fold),  get_lr_callback(batch_size)],\n                        validation_data = validation_dataset,\n                        verbose = 1)   \n    # Validation Information\n    best_val_rmse = min(history.history['val_rmse'])\n    print(f'\\nValidation RMSE: {best_val_rmse}\\n')\nelse:\n    # load trained weights \n    model.load_weights(f'..\/input\/pet-test-wg\/ext_attn_wg\/feature_model_{training_fold}.h5')","9d516303":"if DEVICE == \"TPU\":\n    plt.figure(figsize=(19,6))\n\n    plt.subplot(131)\n    plt.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n    plt.plot(history.epoch, history.history[\"val_loss\"], label=\"Valid loss\")\n    plt.legend()\n\n    plt.subplot(132)\n    plt.plot(history.epoch, history.history[\"rmse\"], label=\"Train RMSE\")\n    plt.plot(history.epoch, history.history[\"val_rmse\"], label=\"Valid RMSE\")\n    plt.legend()\n\n    plt.subplot(133)\n    rng = [i for i in range(epochs)]\n    lr = history.history[\"lr\"]\n    plt.plot(rng, lr, '-o')\n    plt.xlabel('Epoch',size=14)\n    plt.ylabel('Learning Rate',size=14)\n    plt.legend()\n    plt.show()","a6f6a7bf":"# build model with transformer blocks only \ntransformer_feature_blocks = [layer.output for layer in model.layers if isinstance(layer, AttentionEncoder)]\ntrans_act_model = Model(inputs=model.inputs, outputs=transformer_feature_blocks)\n\n# only transformer blocks\ntf.keras.utils.plot_model(trans_act_model, show_shapes=True, show_layer_names=True)","5b62cdaa":"(sample_img, sample_feat), sample_gt = next(iter(validation_dataset))\nprint(sample_img.shape, sample_feat.shape)\n\nplt.figure(figsize=(10, 10))\nplt.axis('off')\nplt.imshow(sample_img[5])\nplt.show()","bd96220b":"feature_maps = trans_act_model.predict(\n    (\n        tf.expand_dims(sample_img[5],  axis=0),\n        tf.expand_dims(sample_feat[5], axis=0)\n    )\n)\n\nfeats_maps  = np.array(list(zip(*feature_maps))[0]).squeeze(axis=1) ; print(feats_maps.shape)\nattn_weight = np.array(list(zip(*feature_maps))[1]).squeeze(axis=1) ; print(attn_weight.shape)\n\nfor i, feature_map in enumerate(feats_maps):\n    print('ExtTransformerBlock ', i)\n    num_sequence, channel = feature_map.shape\n    height = width = int(np.sqrt(num_sequence))\n    feature_map = tf.reshape(feature_map, shape=(-1, height, width, channel))\n    ix = 1\n    plt.figure(figsize=(25, 25))\n    for _ in range(64):\n        ax = plt.subplot(10, 10, ix)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        plt.imshow(feature_map[0, :, :, ix - 1], cmap=\"viridis\")\n        ix += 1\n    plt.show()","b742795c":"all_weit = '..\/input\/pet-test-wg\/ext_attn_wg'\nall_rmse = []\nall_pred = []\n\nfor fold_ in range(df.kfold.nunique())[:len(os.listdir(all_weit))]:\n    print('FOLD ', fold_)\n    # Get fold-wise samples \n    df_valid = df[df.kfold == fold_].reset_index(drop=True)\n    \n    # load trained model \n    model = get_model(plot_model = False,  print_summary = False,  with_compile  = True)\n    model.load_weights(f'{all_weit}\/feature_model_{fold_}.h5')\n    \n    # get validaiton data set to compute Out-of-fold \n    cb_val_set = create_dataset(df_valid, \n                                 batch_size  = BATCH_SIZE,\n                                 is_labelled = True, \n                                 augment     = False,\n                                 repeat      = False, \n                                 shuffle     = False)\n    loss, rmse = model.evaluate(cb_val_set, verbose=1)\n    all_rmse.append(rmse)\n    \n    # Inference on test set \n    test_dataset = create_dataset(test_df, \n                                 batch_size  = BATCH_SIZE,\n                                 is_labelled = False, \n                                 augment     = False, \n                                 repeat      = False, \n                                 shuffle     = False)\n    fold_wise_pred = model.predict(test_dataset, verbose=1)\n    fold_wise_pred = [x * 100 for x in fold_wise_pred]\n    all_pred.append(fold_wise_pred)\n    del model","c2db35df":"print('Mean of all RMSE ', np.mean(all_rmse))\ntest_df['Pawpularity'] = np.mean(np.column_stack(all_pred), axis=1)\ntest_df = test_df[[\"Id\", \"Pawpularity\"]]\ntest_df['Id'] = test_df.Id.apply(lambda x: x.split('\/')[-1].split('.')[0])\ntest_df.to_csv(\"submission.csv\", index=False)\ntest_df.head(15)","2ae4dc45":"import cuml, pickle\nfrom cuml.svm import SVR\nprint('RAPIDS version',cuml.__version__,'\\n')","5b012732":"LOAD_SVR_FROM_PATH = None\ndf = pd.read_csv('..\/input\/pet-test-wg\/train_df_folds.csv')\nprint('Train shape:', df.shape )\n\ntest_df = pd.read_csv(f'{DATA_DIR}test.csv')\ntest_df['Id'] = test_df['Id'].apply(lambda x: f'{TEST_DIR}{x}.jpg')\ntest_df['Pawpularity'] = 0\nprint('Train shape:', test_df.shape )","d0b7e9b3":"dnn_test_preds = []\nsvr_test_preds = []\n\ndnn_val_preds = []\nsvr_val_preds = []\n\nval_true = []\n\nfor fold_ in range(df.kfold.nunique())[:len(os.listdir(all_weit))]:\n    print('-'*10)\n    print('FOLD',fold_)\n    print('-'*10)\n    \n    # build model \n    model = get_model(plot_model = False,   print_summary = False,  with_compile  = True)\n    model.load_weights(f'..\/input\/pet-test-wg\/ext_attn_wg\/feature_model_{fold_}.h5')\n    \n    # move out the embedding and predictin layer \n    new_model = Model(model.input, [model.layers[-2].output, model.output])\n    \n    # get validation fold on current fold \n    df_valid = df[df.kfold == fold_].reset_index(drop=True)\n\n    name = f\"SVR_fold_{fold_}.pkl\" \n    if LOAD_SVR_FROM_PATH is None:\n        # extract embeddings \n        # get training fold on current fold \n        df_train = df[df.kfold != fold_].reset_index(drop=True)\n        training_dataset = create_dataset(df_train,\n                                          batch_size  = BATCH_SIZE, \n                                          is_labelled = False, \n                                          augment     = False,\n                                          repeat      = False, \n                                          shuffle     = False)\n        print('Extracting train embedding...')\n        embed, _ = new_model.predict(training_dataset, verbose=1)\n        \n        print('Fitting SVR...')\n        clf = SVR(C=20.0)\n        clf.fit(embed.astype('float32'), df_train.Pawpularity.values.astype('int32'))\n        pickle.dump(clf, open(name, \"wb\"))\n    else:\n        # LOAD RAPIDS SVR \n        print('Loading SVR...',LOAD_SVR_FROM_PATH+name)\n        clf = pickle.load(open(LOAD_SVR_FROM_PATH+name, \"rb\"))\n  \n    # test set \n    test_dataset = create_dataset(test_df, \n                                 batch_size  = BATCH_SIZE,\n                                 is_labelled = False, \n                                 augment     = False, \n                                 repeat      = False, \n                                 shuffle     = False)\n\n    print('Predicting test...')\n    # Step 1: Get Classification Prediction and Preceding layer feature \/ Embeddings \n    dnn_embed, dnn_test_pred = new_model.predict(test_dataset, verbose=1)\n    dnn_test_pred = [x * 100 for x in dnn_test_pred]\n    \n    # Step 2: Pass The Embeddings to RAPIDS-SVR Model \n    svr_test_pred = clf.predict(dnn_embed)\n    \n    # Step 3: Save \n    dnn_test_preds.append(dnn_test_pred)\n    svr_test_preds.append(svr_test_pred)\n\n    # OOF \n    valid_dataset = create_dataset(df_valid,\n                                   batch_size  = BATCH_SIZE, \n                                   is_labelled = False, \n                                   augment     = False,\n                                   repeat      = False, \n                                   shuffle     = False)\n    print('Predicting Out-of-Fold...')\n    # Step 1: Get Classification Prediction and Preceding layer feature \/ Embeddings \n    dnn_embed, dnn_val_pred = new_model.predict(valid_dataset, verbose=1)\n    dnn_val_pred = [x * 100 for x in dnn_val_pred]\n    \n    # Step 2: Pass The Embeddings to RAPIDS-SVR Model \n    svr_val_pred = clf.predict(dnn_embed)    \n    \n    # Step 3: Save \n    dnn_val_preds.append(dnn_val_pred)\n    svr_val_preds.append(svr_val_pred)\n    \n    # Step 4: Save GT for computing OOF \n    val_true.append(df_valid['Pawpularity'].values)\n \n    ##################\n    # COMPUTE RSME\n    rsme = np.sqrt( np.mean( (val_true[-1] - np.array(dnn_val_preds[-1]))**2.0 ) )\n    print('NN RSME =',rsme)\n    \n    rsme = np.sqrt( np.mean( (val_true[-1] - np.array(svr_val_preds[-1]))**2.0 ) )\n    print('SVR RSME =',rsme)\n    \n    w = 0.5\n    oof2 = (1-w)*np.array(dnn_val_preds[-1]) + w*np.array(svr_val_preds[-1])\n    rsme = np.sqrt( np.mean( (val_true[-1] - oof2)**2.0 ) )\n    print('Ensemble RSME =',rsme,'\\n')","11647567":"## Implement the MLP block","73e6eb6e":"**Check Dataloader**","36fa6488":"# Activaiton Maps of External MHA Transformer Blocks ","22ce1def":"# `tf.data` API for Multi-Input","9f20ca6d":"# Configured Competition Data","982ce641":"## ImageNet-Model + External Attention Transformer \n\n![up](https:\/\/user-images.githubusercontent.com\/17668390\/141291222-b3184730-11ba-4e12-bd87-30a85d82e854.png)","b7bc8f47":"# [Optional]: RAPIDS SVR \n\n[Reference.](https:\/\/www.kaggle.com\/cdeotte\/rapids-svr-boost-17-8) - [Discussion](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/discussion\/276724). ","f9346d25":"# Training Modules","e61c4eb4":"## Implement the External Attention Block","08d2a04d":"# Plot Learning Curve","8481161a":"## Abstraction\n\n- **[About Competition]**: In the given [cat-dog pet dataset](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/data), both image files and also meta informations are provided which can be used in training. The overall task is to analyze this raw images and metadata to predict the **Pawpularity** of pet photos.\n- **[About Hybrid External Multi-Head Transformer]**: \n    - In this notebook, we will be implementing the **External Self-Attention Transformer, EAT in `TensorFlow.Keras` ([paper](https:\/\/arxiv.org\/pdf\/2105.02358.pdf) 2021)**. Also implementation will be maintained in this repo [External-Attention-TensorFlow](https:\/\/github.com\/innat\/External-Attention-TensorFlow). The official PyTorch implementation is [here](https:\/\/github.com\/MenghaoGuo\/EANet). \n    - Specifically, in this notebook, we will use the **EAT** and build a **hybrid model** with ImageNet mdoels (diagram below, we choose `resnet` here, but you can use any). From the existing `tf.keras.applications`, we will import `resnet` and further we'll build a new feature extraction model with 2 output. One of the layer is `top_activation` and another is `conv2_block3_out` layer from `resnet`. \n    - In the next call, the output of the `conv2_block3_out` layer will be passed to the **EAT** model for further training. At the end, according to the model architecture, we will have 3 output to merge followed by classificaiton layer. \n- **[About Input Format]**: As in this competition, we have both **image data** and **structure data**, we'll be building a multi-input model in order to utilize both information. In the above diagram, the `input 1` is the raw image and `input 2` refer the structure data. And for structure data, a simple mlp model will be used. FYI, for structure data, we can also try [TensorFlow Decision Forests](https:\/\/blog.tensorflow.org\/2021\/05\/introducing-tensorflow-decision-forests.html).\n\n![up](https:\/\/user-images.githubusercontent.com\/17668390\/141291222-b3184730-11ba-4e12-bd87-30a85d82e854.png)\n\n- **[About Training]**: We'll train the model on **TPU** and next we'll inference with **GPU**. For training details with **TPU** settings, check **Version 2**; we've trained already and saved the weight file. And lastly, we'll try to inspect the **activaiton feature maps** from the **External-Transformer** blocks.\n- **[About Inference]**: Next, in the **Inference** section, we'll compute out-of-fold validation and do inference with all trained folds and followed by simple **Ensemble**. \n- **[(Optional): About RAPIDS SVR]**: Added RAPIDS SVR for second stage training approach. Reference [kernel](https:\/\/www.kaggle.com\/cdeotte\/rapids-svr-boost-17-8) - [Discussion](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/discussion\/276724). ","1d2e4450":"## Implement the Transformer block","f3e08666":"## Implement the patch extraction and encoding layer","0e19ba5b":"# Modeling\n\nAs described at the beginning, we'll implement [External Attention](https:\/\/arxiv.org\/pdf\/2105.02358.pdf) in `TensorFlow.Keras` and later we'll integrate it into a ImageNet model (here, we pick `resnet`). Describing the model is beyond the scope of this code example. [Here](https:\/\/github.com\/MenghaoGuo\/EANet) is the official PyTorch implementation. And we'll try to rebuild based on that.  \n\n<img width=\"756\" alt=\"ea\" src=\"https:\/\/user-images.githubusercontent.com\/17668390\/141291708-7c3cd892-d508-4cca-8306-a8b06a38c158.png\">\n","b3a660ad":"# OOF + Ensemble Inference \n\n- **Classification Head Models**\n- **Let's try TTA later.**\n- **We'll try [RAPIDS SVR](https:\/\/www.kaggle.com\/cdeotte\/rapids-svr-boost-17-8) next.**","f2ca7e50":"# Additional Resources\n1. How to use it on my own dataset?\n    - First, understand the competition task and its data format. And try to relate with yours.\n    - Second, run this notebook successfully on the competition data.\n    - Lastly, replace the dataset with yours.\n2. More Code Exampels.\n    - [TF.Keras: EfficientNet Hybrid Swin Transformer TPU](https:\/\/www.kaggle.com\/ipythonx\/tf-keras-efficientnet-hybrid-swin-transformer-tpu) - [Discussion](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/discussion\/280531).\n    - [[TF.Keras]:Learning to Resize Images for ViT Model](https:\/\/www.kaggle.com\/ipythonx\/tf-keras-learning-to-resize-images-for-vit-model) - [Discussion](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/discussion\/280438).\n    - [Discussion: DOLG Models in TensorFlow 2 (Keras) Implementation](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/discussion\/281914) - [TF.Keras Code](https:\/\/github.com\/innat\/DOLG-TensorFlow)"}}