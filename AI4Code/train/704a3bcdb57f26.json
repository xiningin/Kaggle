{"cell_type":{"3bdc433d":"code","414e43a9":"code","74dc1d75":"code","15c62281":"code","c452939e":"code","595efe64":"code","f0d727d1":"code","2c3d5b31":"code","4ea44f3c":"code","95e85d36":"code","124dd174":"code","154a5289":"code","df6fd368":"code","2d303ff1":"code","4194e73e":"code","f6ef7d87":"code","9e5ce724":"code","96e714e0":"code","f194484b":"code","4ff05559":"code","992b77f4":"code","95237080":"code","7a0e87ad":"code","f7546197":"code","672996e5":"code","12a58827":"code","b36f6fb4":"code","e61e5911":"code","0931dd8b":"code","180a84f7":"code","0661c6b8":"code","35d7adf2":"code","265e27cb":"code","795b8e52":"code","ef866481":"code","1c554296":"code","416c55cb":"code","c9010256":"code","20ddf56c":"code","950bd47f":"code","854e329a":"code","2495b8bd":"code","ec533e24":"code","248a4c29":"code","04f5a787":"code","22ec36d2":"code","53928a8a":"code","6ec1c2c2":"code","c3761870":"code","dcb1cd0d":"code","2223deb9":"code","5e4753a9":"code","8c046d9a":"code","3fc8dd21":"code","4e846426":"code","ccd0db4b":"code","cbc0a9a1":"code","91966a6e":"code","bded95f0":"code","6a29dd02":"code","f3d75616":"code","e8b59b68":"code","6adfaed3":"code","b6de593e":"code","11452aa5":"code","2432f5cb":"code","498ac99d":"code","fa09e2f0":"code","822cadf5":"code","d508b73a":"code","116733a5":"code","23840c48":"code","85f7e3d8":"code","15b8fd27":"code","6d9bb255":"code","848577ed":"code","7f700933":"code","227edd50":"markdown","7c80b571":"markdown","e7ce1cd2":"markdown","af3aaa06":"markdown","99146391":"markdown","d2ab0400":"markdown","b70f6792":"markdown","fee7981f":"markdown","66ca166f":"markdown","4ff192f3":"markdown","a781ea95":"markdown","96b872ee":"markdown","0b4231f3":"markdown","e02b14b8":"markdown","4f6cc111":"markdown","7994f386":"markdown","a3a77289":"markdown","c73667f9":"markdown","bf590d15":"markdown","cbaa9c55":"markdown","4951baaa":"markdown","47590e6f":"markdown","6232f7bc":"markdown","15b63001":"markdown"},"source":{"3bdc433d":"#------------Importing EDA Libraries--------------------------------------\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#-------------Importing Machine Learnig Libraries--------------------------\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import r2_score\n\n#---------------Importing RFE and LinearRegression-------------------------\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\n\n#-------------Handling Warnings--------------------------------------------\nimport warnings\nwarnings.filterwarnings('ignore')","414e43a9":"# load the dataset\ndf = pd.read_csv(\"..\/input\/boom-bikes\/day.csv\")","74dc1d75":"# Take a look at the data\ndf.head()","15c62281":"# Shape----\ndf.shape","c452939e":"# Data Discription----\ndf.describe()","595efe64":"# Checking the dtypes and missing values----\ndf.info()","f0d727d1":"sns.pairplot(df[['temp','atemp','hum','windspeed','cnt']])     \nplt.show","2c3d5b31":"# Plotting boxplots to dipict the affect of each categorical variable on the target variable 'cnt'.\nplt.figure(figsize=(20, 12))\nplt.subplot(2,3,1)\nsns.boxplot(x = 'season', y = 'cnt', data = df)\nplt.subplot(2,3,2)\nsns.boxplot(x = 'yr', y = 'cnt', data = df)\nplt.subplot(2,3,3)\nsns.boxplot(x = 'holiday', y = 'cnt', data = df)\nplt.subplot(2,3,4)\nsns.boxplot(x = 'weekday', y = 'cnt', data = df)\nplt.subplot(2,3,5)\nsns.boxplot(x = 'workingday', y = 'cnt', data = df)\nplt.subplot(2,3,6)\nsns.boxplot(x = 'weathersit', y = 'cnt', data = df)\nplt.show()\n# We can see that how the median values varies for each of these categorical variables.","4ea44f3c":"# Boxplot to show how 'cnt' varies on holidays for all the season \nplt.figure(figsize = (10, 5))\nsns.boxplot(x = 'season', y = 'cnt', hue = 'holiday', data = df)\nplt.show()","95e85d36":"# Droping the unnecessary columns\ndrop_cols = ['instant','dteday','casual','registered']\ndf.drop(drop_cols,axis = 1,inplace = True)","124dd174":"# Let's check the correlation coefficients to see which variables are highly correlated\n\nplt.figure(figsize = (16, 10))\nsns.heatmap(df.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","154a5289":"# Season--- \ndef season(value):\n    if value == 1:\n        return 'Spring'\n    elif value == 2:\n        return 'Summer'\n    elif value == 3:\n        return 'Fall'\n    else:\n        return 'Winter'\n\ndf['Season'] = df['season'].apply(season)\n\n\n# Year---\n\ndef year(value):\n    if value == 0:\n        return '2018'\n    else:\n        return '2019'\ndf['Yr'] = df['yr'].apply(year)\n\n# Weekday---\n\ndef weekday(value):\n    if value == 0:\n        return 'sunday'\n    elif value == 1:\n        return 'monday'\n    elif value == 2:\n        return 'tuesday'\n    elif value ==3:\n        return 'wednesday'\n    elif value ==4:\n        return 'thursday'\n    elif value == 5:\n        return 'friday'\n    else:\n        return 'saturday'\n    \ndf['Weekday'] = df['weekday'].apply(weekday) \n\n# Weathersit---\n\ndef weathersit(value):\n    if value == 1:\n        return 'clear'\n    elif value == 2:\n        return 'mist'\n    elif value == 3:\n        return 'snow'\n    else:\n        return 'heavy_rain'\n    \ndf['Weathersit'] = df['weathersit'].apply(weathersit)\n\n\n# Month---\n\ndef month(value):\n    if value == 1:\n        return 'jan'\n    elif value == 2:\n        return 'feb'\n    elif value == 3:\n        return 'mar'\n    elif value == 4:\n        return 'apr'\n    elif value == 5:\n        return 'may'\n    elif value == 6:\n        return 'jun'\n    elif value == 7:\n        return 'jul'\n    elif value == 8:\n        return 'aug'\n    elif value == 9:\n        return 'sep'\n    elif value == 10:\n        return 'oct'\n    elif value == 11:\n        return 'nov'\n    else:\n        return 'dec'\n\ndf['Month'] = df['mnth'].apply(month) \n\n# Holiday\n\ndef holiday(value):\n    if value == 0:\n        return 'No_holiday'\n    else:\n        return 'Yes_holiday'\n    \ndf['Holiday'] = df['holiday'].apply(holiday)\n\ndef workingday(value):\n    if value == 0:\n        return 'No_working'\n    else:\n        return 'Yes_working'\n    \ndf['Workingday'] = df['workingday'].apply(workingday)\n","df6fd368":"# Drop the columns which has been modified\n\ndrop_columns = ['season','yr','weekday','weathersit','mnth','holiday','workingday']\ndf.drop(drop_columns,axis=1,inplace = True)","2d303ff1":"# Lets look at the dataset after conversion into categorical variables\ndf.head()","4194e73e":"# Checking the dtypes of the variables\ndf.info()\n# We can see that all the binary and numerical categorical columns have been converted into categorical columns","f6ef7d87":"# Season variable\nseason = pd.get_dummies(df['Season'],drop_first=True)\nseason.head()","9e5ce724":"# Year variable\nyr = pd.get_dummies(df['Yr'],drop_first=True)\nyr.head()","96e714e0":"# Weekday variable\nweekday = pd.get_dummies(df['Weekday'],drop_first=True)\nweekday.head()","f194484b":"# Weathersit variable\nweathersit = pd.get_dummies(df['Weathersit'],drop_first=True)\nweathersit.head()","4ff05559":"# month variable\nmonth = pd.get_dummies(df['Month'],drop_first=True)\nmonth.head()","992b77f4":"# Holiday variable\nholiday = pd.get_dummies(df['Holiday'],drop_first=True)\nholiday.head()","95237080":"# Working day variable\nworkingday = pd.get_dummies(df['Workingday'],drop_first=True)\nworkingday.head()","7a0e87ad":"# Firsly we will merge them altogether\nframes = [df,season,yr,weekday,weathersit,month,holiday,workingday]\ndf = pd.concat(frames,axis = 1)","f7546197":"# Dataset\ndf.head()","672996e5":"# Shape\ndf.shape","12a58827":"# lets drop the columns we used dummy encoding for--\ndrop_col1 = ['Season','Yr','Weekday','Weathersit','Month','Holiday','Workingday']\ndf.drop(drop_col1,axis = 1, inplace = True)","b36f6fb4":"# Head\ndf.head()","e61e5911":"# Dataset columns\ndf.columns","0931dd8b":"# Shape\ndf.shape","180a84f7":"#------------- Correlation between atemp and other variables\nprint(round(df.atemp.corr(df.cnt),4))\nprint(round(df.atemp.corr(df.hum),4))\nprint(round(df.atemp.corr(df.windspeed),4))\n\n# We can see that actual temp has a positive correlation with the cnt variable ","0661c6b8":"#------------- Correlation between hum and other variables\nprint(round(df.hum.corr(df.cnt),4))\nprint(round(df.hum.corr(df.Summer),4))\nprint(round(df.hum.corr(df.windspeed),4))\n\n# We can see that as the humidity increases the cnt decreases which states that bikers do not like the \n# humid weather","35d7adf2":"#------------- Correlation between windspeed and other variables\nprint(round(df.windspeed.corr(df.cnt),4))\nprint(round(df.windspeed.corr(df.atemp),4))\nprint(round(df.windspeed.corr(df.Winter),4))\n\n# We can see that as the windpeed increases the cnt decreases which states that bikers do not like the \n# windy climate","265e27cb":"# Spliting the data into train and test datasets\nnp.random.seed(0)\ndf_train, df_test = train_test_split(df, train_size = 0.7, test_size = 0.3, random_state = 100)","795b8e52":"#Shape of train and test data\nprint(df_train.shape)\nprint(df_test.shape)","ef866481":"# Scaling the some of the features\nvar = ['temp','atemp','hum','windspeed']","1c554296":"scaler = MinMaxScaler()\ndf_train[var] = scaler.fit_transform(df_train[var])","416c55cb":"# Dividing the training dataset into X and y.\ny_train = df_train.pop('cnt')\nX_train = df_train","c9010256":"# Building a model using statsmodel\n# Add a constant\nX_train_lm = sm.add_constant(X_train)\n# Create a first fitted model\nlr_1 = sm.OLS(y_train, X_train_lm).fit()","20ddf56c":"# First model\nlr_1.summary()","950bd47f":"# Running RFE with the output number of the variable equal to 15\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 15)             # running RFE\nrfe = rfe.fit(X_train, y_train)","854e329a":"# List of 15 columns selected by RFE\nlist(zip(X_train.columns,rfe.support_,rfe.ranking_))","2495b8bd":"# Columns selected by RFE\ncol = X_train.columns[rfe.support_]\ncol","ec533e24":"# Columns rejected by RFE\nX_train.columns[~rfe.support_]","248a4c29":"# Creating X_test dataframe with RFE selected variables\nX_train_rfe = X_train[col]","04f5a787":"# Adding a constant variable  \nX_train_rfe = sm.add_constant(X_train_rfe)","22ec36d2":"# Second model\nlm_2 = sm.OLS(y_train,X_train_rfe).fit()   # Running the linear model","53928a8a":"# Summary statistics\nlm_2.summary()","6ec1c2c2":"# Calculate the VIFs for the new model\nvif = pd.DataFrame()\nX = X_train[col]\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","c3761870":"X_train_new = X_train[col].drop([\"hum\"], axis = 1)","dcb1cd0d":"X_train_lm = sm.add_constant(X_train_new)","2223deb9":"# Third model\nlm_3 = sm.OLS(y_train,X_train_lm).fit()   # Running the linear model","5e4753a9":"lm_3.summary()","8c046d9a":"# Calculate the VIFs for the new model\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif\n# Surely the VIF values have come down","3fc8dd21":"X_train_new = X_train[col].drop(['temp','Yes_working'], axis = 1)","4e846426":"X_train_lm = sm.add_constant(X_train_new)","ccd0db4b":"# Fourth model\nlm_4 = sm.OLS(y_train,X_train_lm).fit()   # Running the linear model","cbc0a9a1":"lm_4.summary()","91966a6e":"X_train_new = X_train[col].drop(['Yes_working','hum'], axis = 1)","bded95f0":"X_train_lm = sm.add_constant(X_train_new)","6a29dd02":"# Fifth model\nlm_5 = sm.OLS(y_train,X_train_lm).fit()   # Running the linear model","f3d75616":"lm_5.summary()","e8b59b68":"# Calculate the VIFs for the new model\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","6adfaed3":"# We will try to find the variables humidity is correlated with and try removing them.\n#------------- Correlation between hum and other variables\nprint(round(df.hum.corr(df.cnt),4))\nprint(round(df.hum.corr(df.Summer),4))\nprint(round(df.hum.corr(df.windspeed),4))\nprint(round(df.hum.corr(df.Spring),4))\nprint(round(df.hum.corr(df.jan),4))\nprint(round(df.hum.corr(df.jul),4))\nprint(round(df.hum.corr(df.sep),4))\nprint(round(df.hum.corr(df.saturday),4))\nprint(round(df.hum.corr(df.Yes_holiday),4))","b6de593e":"X_train_new = X_train[col].drop(['hum','jul','jan','Yes_holiday','saturday'], axis = 1)","11452aa5":"X_train_lm = sm.add_constant(X_train_new)","2432f5cb":"lm_6 = sm.OLS(y_train,X_train_lm).fit()   # Running the linear model","498ac99d":"# Final model\nlm_6.summary()","fa09e2f0":"# Calculate the VIFs for the new model\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","822cadf5":"# Pairplot of final variables\nsns.pairplot(df[['temp','atemp','hum','windspeed','Yes_working','Spring','2019','Summer','Winter','mist','sep','snow','cnt']]) ","d508b73a":"y_train_price = lm_6.predict(X_train_lm)","116733a5":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_price), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)    ","23840c48":"var = ['temp','atemp','hum','windspeed']\n\ndf_test[var] = scaler.transform(df_test[var])","85f7e3d8":"y_test = df_test.pop('cnt')\nX_test = df_test","15b8fd27":"# Now let's use our model to make predictions.\n\n# Creating X_test_new dataframe by dropping variables from X_test\nX_test_new = X_test[X_train_new.columns]\n\n# Adding a constant variable \nX_test_new = sm.add_constant(X_test_new)","6d9bb255":"# Making predictions\ny_pred = lm_6.predict(X_test_new)","848577ed":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_pred)\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)                          # Y-label","7f700933":"# Calculating R2 value for test data\nr2 = r2_score(y_test,y_pred)\nprint(r2)","227edd50":"### Building model using statsmodel, for the detailed statistics","7c80b571":"#### We can see that removing the humidity has decreased the adj R2 value, therfore the predictive power gets decreased","e7ce1cd2":"#### From the above summary statistics we can see that the significance of each variable has increased,however the adj R2 value has dropped a little. Therefore we will check the VIFs of the variables.","af3aaa06":"## Residual Analysis of the train data\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like.","99146391":"#### From the final model we can say that variables ['temp','windspeed','Yes_working','Spring','2019','Summer','Winter','mist','sep','snow'] are the essential features required to predict count of bikes each year. It is pretty obvious that weather conditions and holidays affect the count of the bikes the most.\n","d2ab0400":"#### Lets look at the correlation of different variables","b70f6792":"#### From the above model we can see that there many variable having p value greater than 0.05 making them highly insignificant for the model.","fee7981f":"### Data Preparation","66ca166f":"####  We can how linear the spread is, showing the accuracy of our model.","4ff192f3":"### Some data visualization using seaborn","a781ea95":"#### So we are getting a adj. R2 value of 0.828 using exactly 10 variables","96b872ee":"## Model Evaluation","0b4231f3":"#### Clearly dropping the temp and Yes_working variable has decreased the adj R2 value, therefore we will keep them.","e02b14b8":"### Problem Statement","4f6cc111":"#### Now that we have all the dummy variables lets perform two tasks--\n\n##### Drop the orginal columns \n##### Concatenate these dummy variables to the main dataset","7994f386":"#### Lets covert the binary and numerical categorical variables into categorical variables to perform dummy encoding---","a3a77289":"A bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.\n\nA US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state.\n\nIn such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.\n\nThey have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:\n\nWhich variables are significant in predicting the demand for shared bikes.\nHow well those variables describe the bike demands Based on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors.\nBusiness Goal: You are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market.","c73667f9":"#### Applying the scaling on the test sets","bf590d15":"#### We can see that temp and Yes_working have high VIF values so we will drop them ","cbaa9c55":"## Making Predictions","4951baaa":"#### From the above VIF result we can see that Humidity has high multicolinearity with the other variables.","47590e6f":"#### Dividing into X_test and y_test","6232f7bc":"#### From the above pairplot of numerical variables we can observe that there is a linear corelation of temp and atemp with the dependent variable(cnt).","15b63001":"#### Dummy Encoding --"}}