{"cell_type":{"db9cd73d":"code","0ff96035":"code","0c54a4dd":"code","c6358794":"code","93681914":"code","13d172c7":"code","070ef862":"code","091cd037":"code","1d019d03":"code","5c793c82":"code","607a274c":"code","151ef804":"code","4b459e06":"code","5196b31c":"code","e4e0da2f":"code","5fa4f465":"code","d9db1f9a":"code","1a47e129":"code","a0236c6e":"code","762fd227":"code","116a77c1":"code","86902fea":"code","636f3994":"code","d5c6b0fd":"code","be03f231":"code","3ab354dc":"code","dd5c9df7":"code","c7d7e268":"markdown","31c948ac":"markdown","3f2f93c6":"markdown","f4c64b6a":"markdown","969c5998":"markdown","32e27b74":"markdown","6f3a26e3":"markdown","af34d913":"markdown","b520081d":"markdown","073d72e7":"markdown","2d9a739f":"markdown","45027c66":"markdown","81864402":"markdown"},"source":{"db9cd73d":"import os\nimport random\nimport gc\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n\nimport tensorflow as tf\nfrom keras.utils.vis_utils import plot_model\nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, AveragePooling2D, Dense, Flatten\nfrom keras.callbacks import LearningRateScheduler\n\nBATCH_SIZE = 64","0ff96035":"seed = 666\ntf.random.set_seed(seed)\nnp.random.seed(seed)\nos.environ[\"PYTHONHASHSEED\"] = str(seed)                      \nrandom.seed(666)","0c54a4dd":"train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\n\nprint(f\"Training observations {train.shape[0]}, Test observations {test.shape[0]} \\n\")\n\ntrain.head()","c6358794":"fig, ax = plt.subplots(figsize = (8, 4))\n\nsns.countplot(data = train, x = \"label\", ax = ax, color = \"#101820\")\n\nax.set_title(\"Countplot for Train Labels\")\n\nsns.despine()\nplt.show()","93681914":"train_X = train.drop(\"label\", axis = 1)\ntrain_y = train[\"label\"]\n\ndel train\n_ = gc.collect()","13d172c7":"train_X = train_X \/ 255\nX_test = test \/ 255\n\ndel test\n_ = gc.collect()","070ef862":"print(f\"Training data shape: {train_X.shape} \\nTest data shape: {X_test.shape}\")\n\ntrain_X = train_X.values.reshape(-1, 28, 28, 1)\nX_test = X_test.values.reshape(-1, 28, 28, 1)\n\nprint(f\"Training data shape after rescaling: {train_X.shape} \\nTest data shape after rescaling: {X_test.shape}\")","091cd037":"train_X2 = np.pad(train_X, ((0,0),(2,2),(2,2),(0,0)), \"constant\")\nX_test = np.pad(X_test, ((0,0),(2,2),(2,2),(0,0)), \"constant\")","1d019d03":"train_y = to_categorical(train_y, num_classes = 10)","5c793c82":"fig = plt.figure(1, figsize = (8, 8))\nfig.suptitle(\"Training Set Images (Sample)\")\n\nfor i in range(100):\n    \n    plt.subplot(10, 10, i + 1)\n    plt.imshow(train_X[i], cmap = plt.cm.binary)\n    plt.axis(\"off\")\n    \nplt.tight_layout()\nplt.show()","607a274c":"fig = plt.figure(1, figsize = (8, 8))\nfig.suptitle(\"Test Set Images (Sample)\")\n\nfor i in range(100):\n    \n    plt.subplot(10, 10, i + 1)\n    plt.imshow(X_test[i], cmap = plt.cm.binary)\n    plt.axis(\"off\")\n    \nplt.tight_layout()\nplt.show()","151ef804":"X_train, X_val, y_train, y_val = train_test_split(train_X2, \n                                                  train_y, \n                                                  test_size = 0.1, \n                                                  random_state = 666, \n                                                  stratify = train_y)\n\nprint(f\"Training set shape: {X_train.shape} \\nValidation set shape: {X_val.shape}\")","4b459e06":"input_shape = (32, 32, 1)\n\ndef lenet5():\n    \n    model = Sequential(\n        [\n            Conv2D(filters = 6, kernel_size = (5, 5), strides = (1, 1), activation = \"tanh\", input_shape = input_shape),\n            AveragePooling2D(pool_size = (2, 2)),\n            Conv2D(filters = 16, kernel_size = (5, 5), strides = (1, 1), activation = \"tanh\"),\n            AveragePooling2D(pool_size = (2, 2), strides = 2),\n            \n            Flatten(),\n            Dense(units = 120, activation = \"tanh\"),\n            Dense(units = 84, activation = \"tanh\"),\n            Dense(units = 10, activation = \"softmax\")\n        ]\n    )\n    \n    return model","5196b31c":"model = lenet5()","e4e0da2f":"model.summary()\n\nmodel.compile(loss = \"categorical_crossentropy\", metrics = \"accuracy\", optimizer = \"SGD\")\n\nprint(\"LeNet5 Architecture\\n\")\nplot_model(model, to_file = \"lenet5.png\", show_shapes = True, show_layer_names = True)","5fa4f465":"history = model.fit(\n    X_train, y_train,\n    epochs = 20, batch_size = BATCH_SIZE, validation_data = (X_val, y_val),\n    steps_per_epoch = X_train.shape[0] \/\/ BATCH_SIZE\n)","d9db1f9a":"fig, axes = plt.subplots(1, 2, figsize = (12, 4))\n\nsns.lineplot(x = range(len(history.history[\"loss\"])), \n             y = history.history[\"loss\"], \n             ax = axes[0], label = \"Training Loss\")\n\nsns.lineplot(x = range(len(history.history[\"loss\"])), \n             y = history.history[\"val_loss\"], \n             ax = axes[0], label = \"Validation Loss\")\n\nsns.lineplot(x = range(len(history.history[\"accuracy\"])), \n             y = history.history[\"accuracy\"], \n             ax = axes[1], label = \"Training Accuracy\")\n\nsns.lineplot(x = range(len(history.history[\"accuracy\"])), \n             y = history.history[\"val_accuracy\"], \n             ax = axes[1], label = \"Validation Accuracy\")\n\naxes[0].set_title(\"Loss\")\naxes[1].set_title(\"Accuracy\")\nfig.suptitle(\"LeNet5 \\nSGD Optimizer\")\n\nplt.tight_layout()\n\nsns.despine()\nplt.show()","1a47e129":"tf.keras.backend.clear_session()\n\nmodel = lenet5()\n\nmodel.compile(loss = \"categorical_crossentropy\", metrics = \"accuracy\", optimizer = \"adam\")\n\nhistory = model.fit(\n    X_train, y_train,\n    epochs = 20, batch_size = BATCH_SIZE, validation_data = (X_val, y_val),\n    steps_per_epoch = X_train.shape[0] \/\/ BATCH_SIZE\n)","a0236c6e":"fig, axes = plt.subplots(1, 2, figsize = (12, 4))\n\nsns.lineplot(x = range(len(history.history[\"loss\"])), \n             y = history.history[\"loss\"], \n             ax = axes[0], label = \"Training Loss\")\n\nsns.lineplot(x = range(len(history.history[\"loss\"])), \n             y = history.history[\"val_loss\"], \n             ax = axes[0], label = \"Validation Loss\")\n\nsns.lineplot(x = range(len(history.history[\"accuracy\"])), \n             y = history.history[\"accuracy\"], \n             ax = axes[1], label = \"Training Accuracy\")\n\nsns.lineplot(x = range(len(history.history[\"accuracy\"])), \n             y = history.history[\"val_accuracy\"], \n             ax = axes[1], label = \"Validation Accuracy\")\n\naxes[0].set_title(\"Loss\")\naxes[1].set_title(\"Accuracy\")\nfig.suptitle(\"LeNet5 \\nAdam Optimizer\")\n\nplt.tight_layout()\n\nsns.despine()\nplt.show()","762fd227":"#from paper\ndef lenet_scheduler(epoch):\n    \n    if epoch < 3: \n        return 0.0005\n    \n    elif epoch < 6: \n        return 0.0002\n    \n    elif epoch < 9: \n        return 0.0001\n    \n    elif epoch < 13: \n        return 0.00005\n    \n    else:\n        return 0.00001\n    \nscheduler = LearningRateScheduler(lenet_scheduler, verbose = 1)","116a77c1":"tf.keras.backend.clear_session()\n\nmodel = lenet5()\n\nmodel.compile(loss = \"categorical_crossentropy\", metrics = \"accuracy\", optimizer = \"adam\")\n\nhistory = model.fit(\n    X_train, y_train,\n    epochs = 20, batch_size = BATCH_SIZE, validation_data = (X_val, y_val),\n    steps_per_epoch = X_train.shape[0] \/\/ BATCH_SIZE,\n    callbacks = [scheduler]\n)","86902fea":"fig, axes = plt.subplots(1, 2, figsize = (12, 4))\n\nsns.lineplot(x = range(len(history.history[\"loss\"])), \n             y = history.history[\"loss\"], \n             ax = axes[0], label = \"Training Loss\")\n\nsns.lineplot(x = range(len(history.history[\"loss\"])), \n             y = history.history[\"val_loss\"], \n             ax = axes[0], label = \"Validation Loss\")\n\nsns.lineplot(x = range(len(history.history[\"accuracy\"])), \n             y = history.history[\"accuracy\"], \n             ax = axes[1], label = \"Training Accuracy\")\n\nsns.lineplot(x = range(len(history.history[\"accuracy\"])), \n             y = history.history[\"val_accuracy\"], \n             ax = axes[1], label = \"Validation Accuracy\")\n\naxes[0].set_title(\"Loss\")\naxes[1].set_title(\"Accuracy\")\nfig.suptitle(\"LeNet5 \\nAdam with learning rate scheduler\")\n\nplt.tight_layout()\nsns.despine()\nplt.show()","636f3994":"val_preds = np.argmax(model.predict(X_val), axis = 1)\ntrain_preds = np.argmax(model.predict(X_train), axis = 1)","d5c6b0fd":"fig, axes = plt.subplots(1, 2, figsize = (18, 6))\n\ncm_train = confusion_matrix(np.argmax(y_train, axis = 1), train_preds)\ndisp = ConfusionMatrixDisplay(confusion_matrix = cm_train)\ndisp.plot(cmap = plt.cm.Blues, ax = axes[0])\n\ncm_val = confusion_matrix(np.argmax(y_val, axis = 1), val_preds)\ndisp = ConfusionMatrixDisplay(confusion_matrix = cm_val)\ndisp.plot(cmap = plt.cm.Blues, ax = axes[1])\n\naxes[0].set_title(\"Training Set\"); axes[1].set_title(\"Validation Set\")\n\nplt.show()","be03f231":"errors = (val_preds - np.argmax(y_val, axis = 1) != 0)\n\npred_error = val_preds[errors]\nobserved_error = np.argmax(y_val, axis = 1)[errors]\nimage_error = X_val[errors]\nlen(pred_error)","3ab354dc":"fig = plt.figure(1, figsize=(15, 15))\nfig.suptitle(\"Errors in Validation\")\n\nrows = int(len(pred_error) ** 0.5) - 1\ncols = int(len(pred_error) \/ rows) + 1\n\nfor i in range(len(pred_error)):\n    \n    plt.subplot(rows, cols, i + 1)\n    plt.imshow(image_error[i], cmap = plt.cm.binary)\n    plt.axis(\"off\")\n    plt.title(f\"True Value: {observed_error[i]} \\nPrediction: {pred_error[i]}\")\n    \nplt.tight_layout()\nplt.show()","dd5c9df7":"submission = pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")\n\npreds = np.argmax(model.predict(X_test), axis = 1)\nsubmission[\"Label\"] = preds\nsubmission.to_csv(\"cnn.csv\",index = False)\nsubmission","c7d7e268":"# Conclusion","31c948ac":"# Lenet5 with SGD Optimizer","3f2f93c6":"# Lenet5 with Adam Optimizer","f4c64b6a":"## Architecture\n\n![](https:\/\/www.researchgate.net\/profile\/Sheraz-Khan-14\/publication\/321586653\/figure\/fig4\/AS:568546847014912@1512563539828\/The-LeNet-5-Architecture-a-convolutional-neural-network.png)","969c5998":"I give an importance to implementations or analysis for different architectures. This is one of the first CNN architectures. It is simple with just a few layer. It won't give us best results. But, reading a paper, analysing its structure and coding it probably provides a benefit to you.\n\nI think, researching process and getting other ideas about that topic is more important than creating a model without enough knowledge.","32e27b74":"https:\/\/d2l.ai\/chapter_convolutional-neural-networks\/lenet.html\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2021\/03\/the-architecture-of-lenet-5\/\n\nhttps:\/\/hackmd.io\/@bouteille\/S1WvJyqmI\n\nhttps:\/\/debuggercafe.com\/lenet-5-a-practical-approach\/\n\nhttp:\/\/yann.lecun.com\/exdb\/lenet\/index.html\n\n\nMy similar works with image data;\n\nhttps:\/\/www.kaggle.com\/mustafacicek\/mnist-cnn-data-augmentation\n\nhttps:\/\/www.kaggle.com\/mustafacicek\/dogs-cats-vgg16-implementation-transfer-learning","6f3a26e3":"# Lenet5 with Adam Optimizer & Learning Rate Scheduler","af34d913":"# Readings","b520081d":"# Lenet5 Architecture & Implementation","073d72e7":"## Keras Implementation","2d9a739f":"## Paper\n\n[Gradient-Based Learning Applied to Document. Recognition](https:\/\/www.google.com\/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwiBvuapsqL0AhVI3aQKHRRhAgEQFnoECAMQAQ&url=http%3A%2F%2Fyann.lecun.com%2Fexdb%2Fpublis%2Fpdf%2Flecun-98.pdf&usg=AOvVaw00sBiUROin1_Z1KYLBXPX6)\n\n\nand also, \n\nhttp:\/\/yann.lecun.com\/exdb\/lenet\/index.html","45027c66":"# Introduction\n\nIn this notebook, I will implement LeNet-5 architecture.\n\nShortly, LeNet-5 is one of the first CNN architectures. It is really simple architecture with just two convolutional layers, two average pooling layers and three fully connected layers.\n\nIt is based on MNIST dataset, and for its trained over 20 epoch. For convergence, about 10 epochs is enough.","81864402":"# Predictions & Metrics"}}