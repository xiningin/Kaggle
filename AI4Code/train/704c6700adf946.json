{"cell_type":{"487e9b41":"code","285d19f0":"code","b1d35c88":"code","a1a75369":"code","ec4b28fe":"code","9f6ac461":"code","f2afe57a":"code","95896637":"code","67ae591c":"code","b26b5323":"code","a2ec249b":"code","6dfd8bd1":"code","bf13e6fd":"code","a4b8a010":"code","2e25f643":"code","f7a7878b":"code","bee565b7":"code","dffbf0ed":"code","ad216cc5":"markdown","cf84263f":"markdown","6af661b7":"markdown","ef8ec60c":"markdown","9c14cbe2":"markdown","3b5b70db":"markdown","0efc1cec":"markdown","591f92c0":"markdown","7d00f7d1":"markdown","130b2ce8":"markdown","fdd64433":"markdown","a0197533":"markdown"},"source":{"487e9b41":"!pip install torchvision","285d19f0":"!pip uninstall -y transformers\n!pip install transformers","b1d35c88":"import numpy as np\nimport torch\nfrom transformers import BertTokenizer, BertForMaskedLM","a1a75369":"BERT_MODEL = 'bert-base-multilingual-cased'","ec4b28fe":"tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\nmodel = BertForMaskedLM.from_pretrained(BERT_MODEL)","9f6ac461":"def get_masks(tokens, max_len=128):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens) > max_len:\n        raise IndexError(\"Token length more than max length!\")\n    return [1] * len(tokens) + [0] * (max_len - len(tokens))\n\n\ndef get_segments(tokens, max_len=128):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens) > max_len:\n        raise IndexError(\"Token length more than max length!\")\n    segments = []\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            current_segment_id = 1\n    return segments + [0] * (max_len - len(tokens))\n\n\ndef get_ids(tokens, tokenizer, max_len=128):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [tokenizer.pad_token_id] * (max_len - len(token_ids))\n    return input_ids\n\n\ndef encode_input(sentence, max_len=128):\n    stokens = tokenize(sentence)\n    \n    input_ids = get_ids(stokens, tokenizer, max_len)\n    input_masks = get_masks(stokens, max_len)\n    input_segments = get_segments(stokens, max_len)\n    \n    model_input = {\n        'input_word_ids': np.array([input_ids]),\n        'input_masks': np.array([input_masks]),\n        'input_segments': np.array([input_segments])\n    }\n    \n    mask_pos = np.array(np.array(stokens) == '[MASK]', dtype='int')\n    mask_pos = np.concatenate((mask_pos, np.zeros(max_len - len(mask_pos))))\n    mask_pos = mask_pos.astype('int')\n    \n    return model_input, mask_pos","f2afe57a":"def tokenize(sentence):\n    stokens = tokenizer.tokenize(sentence)\n    i = 0\n    while i < len(stokens) - 2:\n        if stokens[i] == '[' and stokens[i+1] == 'mask' and stokens[i+2] == ']':\n            stokens[i] = '[MASK]'\n            stokens.pop(i+2)\n            stokens.pop(i+1)\n        i = i + 1\n    \n    stokens = ['[CLS]'] + stokens + ['[SEP]']\n    return stokens","95896637":"text = \"Hello, I'm a [MASK] model.\"\ntokenize(text)","67ae591c":"# Example of BERT inputs\ntext = \"Hello, I'm a [MASK] model.\"\nencode_input(text, max_len=15) # max_len=15 for display purpose","b26b5323":"text = \"Hello, I'm a [MASK] model.\"\ntokenizer.tokenize(text)","a2ec249b":"text = \"Hello, I'm a [MASK] model.\"\n\nencoded_input = tokenizer(text, return_tensors='pt')\nencoded_input","6dfd8bd1":"def get_topk_predictions(model, tokenizer, text, topk=5):\n    encoded_input = tokenizer(text, return_tensors='pt')\n    logits = model(encoded_input['input_ids'],\n                   encoded_input['token_type_ids'],\n                   encoded_input['attention_mask'],\n                   masked_lm_labels=None)[0]\n\n    logits = logits.squeeze(0)\n    probs = torch.softmax(logits, dim=-1)\n\n    mask_cnt = 0\n    token_ids = encoded_input['input_ids'][0]\n    \n    top_preds = []\n\n    for idx, _ in enumerate(token_ids):\n        if token_ids[idx] == tokenizer.mask_token_id:\n            mask_cnt += 1\n            \n            topk_prob, topk_indices = torch.topk(probs[idx, :], topk)\n            topk_indices = topk_indices.cpu().numpy()\n            topk_tokens = tokenizer.convert_ids_to_tokens(topk_indices)\n            for prob, tok_str, tok_id in zip(topk_prob, topk_tokens, topk_indices):\n                top_preds.append({'token_str': tok_str,\n                                  'token_id': tok_id,\n                                  'probability': float(prob)})\n    \n    return top_preds","bf13e6fd":"def display_topk_predictions(model, tokenizer, text, pretty_prob=False):\n    top_preds = get_topk_predictions(model, tokenizer, text)\n    \n    print(text)\n    print('=' * 40)\n    for item in top_preds:\n        if not pretty_prob:\n            print('%s %.4f' % (item['token_str'], item['probability']))\n        else:\n            probability = item['probability'] * 100\n            print('%s %.2f%%' % (item['token_str'], probability))","a4b8a010":"text = \"Hello, I'm a [MASK] model.\"\ndisplay_topk_predictions(model, tokenizer, text)","2e25f643":"text = 'The doctor ran to the emergency room to see [MASK] patient.'\ndisplay_topk_predictions(model, tokenizer, text)","f7a7878b":"text = 'Este coche es [MASK].'\ndisplay_topk_predictions(model, tokenizer, text)","bee565b7":"text = 'Este coche es muy [MASK].'\ndisplay_topk_predictions(model, tokenizer, text)","dffbf0ed":"text = '\u042f \u0441\u0447\u0438\u0442\u0430\u044e, \u0447\u0442\u043e \u041d\u0430\u0441\u0442\u044f \u043e\u0447\u0435\u043d\u044c [MASK] \u0447\u0435\u043b\u043e\u0432\u0435\u043a.'\ndisplay_topk_predictions(model, tokenizer, text, pretty_prob=True)","ad216cc5":"# Infer masked token with BERT","cf84263f":"# Infer masked word\n\nNow, we can use the described encoded data as input of our BERT model and, then, we apply softmax to the output. The result are the probabilities that a token fit in a certain position, so we should focus on the position of the masked token and get the top probabilities.","6af661b7":"# Get model\n\nVisit https:\/\/huggingface.co\/transformers\/pretrained_models.html to see the full list of pretrained models.","ef8ec60c":"Here is one more example but, in this case, the sentence is written in Russian: `I think that Nastya is a person very [MASK]`. Spoiler: the output words make sense :)","9c14cbe2":"### Manual encoding\n\nThis section is an example of how to get the encoded input of BERT.\n\nWe use the BERT Tokenizer to split the text into tokens. Then, according to the paper, it is necessary to add two tokens at the beginning and the ending: `[CLS]` and `[SEP]`.","3b5b70db":"## Tokenize & encode\n\nBERT expects 3 different inputs:\n- Token IDs: the tokens transformed into numbers.\n- Mask: sequence of `0` (if there are PAD tokens in that position) and `1` (otherwise).\n- Segments or Type IDs: sequence of `0` and `1` to distinguish between the first and the second sentence in NSP tasks. In this notebook, we do not need this input, so it will be always `0`.\n\nThe only constraint is that the **maximum number of tokens is 512**. Please, note that there are extra tokens which we are going to add (see the next section).\n\nFor example:\n```\nText:       Is this jacksonville?\n---------------------------------------------------------------------------------\nTokens:     [CLS] Is    this  ja    ##cks ##on  ##ville ?   [SEP] [PAD] [PAD] ...\nToken IDs:  101   12034 10531 10201 18676 10263 12043   136 102   100   100   ...\nMask:       1     1     1     1     1     1     1       1   1     0     0     ...\nType IDs:   0     0     0     0     0     0     0       0   0     0     0     ...\n```\n\nNote: Token IDs may be different depending on the tokenizer.\n\nFor further details, see:\n- BERT implementation `convert_single_example()` at https:\/\/github.com\/google-research\/bert\/blob\/eedf5716ce1268e56f0a50264a88cafad334ac61\/run_classifier.py#L377 (permalink).\n- BERT paper: https:\/\/arxiv.org\/pdf\/1810.04805.pdf","0efc1cec":"In this case, the model success to return tokens which make sense.","591f92c0":"Despite I introduced a very simple sentence (`This car is [MASK]`), the outputs do not make sense in a sentence (although they are related to cars). I think that the model is confusing the two Spanish verbs `ser` and `estar` (both mean `to be` in English) or it does not have enough context to output a good result.","7d00f7d1":"Here I added an adverb to the previous sentence (`The car is very [MASK].`), so I expect that it outputs better tokens since the context is better too.","130b2ce8":"## Prepare and import modules\n\nWith your environment configured, you can now prepare and import the BERT modules.","fdd64433":"## Torch encoder\n\nTorch has already implemented similar code as above, thus you should use these functions if you do not need any custom behaviour.\n\nHere you can see the result of `tokenize`. Not that it does not include `[CLS]` and `[SEP]`, but the result of `encoded_input` does include them.","a0197533":"The predictions in English seems to work quite good.\n\nSince I am using a multilingual model, it should have nice predictions in other languages too, e.g. Spanish."}}