{"cell_type":{"49094e59":"code","e0ec832e":"code","ffe4e7ea":"code","fdd7f9f8":"code","c01e4106":"code","8978fce3":"code","b47adaa6":"code","460c778e":"code","16df5404":"code","4a362ecb":"code","d619918c":"code","5ac8309f":"code","57507527":"code","b71238eb":"code","7a9a71b8":"code","ac24ac5d":"code","29db644b":"code","588cf078":"code","5bd4f3f5":"code","b49a8f25":"code","8dd9443b":"code","5b95c4ef":"code","dfc409dd":"code","6e9dbd12":"code","bb57e23c":"code","4069cafb":"code","3caaf04e":"code","6c190c72":"code","ebb352e8":"code","575d64aa":"code","26a761e2":"code","c60369a5":"code","462e108b":"code","ce1a5b41":"code","5ad6472a":"code","63e74d7b":"code","e141afdf":"code","2c108404":"code","77178c77":"code","77b80aeb":"code","be990a49":"code","540b1c73":"code","11e02922":"code","6a3fd628":"code","2571dcc2":"code","275c7cbf":"code","f42a09e4":"code","59433df0":"code","cf983a49":"code","da734377":"code","dcde2f02":"code","2a5d07f6":"code","fafe66ec":"code","d74a32ec":"code","4b424f10":"code","6d9ccde7":"code","19389cae":"code","816a2152":"code","305f3b73":"code","ce763680":"code","821921c1":"code","19fd6588":"code","0edc7083":"markdown","2b4580b0":"markdown","afb52fd7":"markdown"},"source":{"49094e59":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e0ec832e":"!pip install twython","ffe4e7ea":"import nltk\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA","fdd7f9f8":"data=pd.read_csv('..\/input\/all-covid19-vaccines-tweets\/vaccination_all_tweets.csv')\ndata.head()","c01e4106":"import re\ndef clean(text):\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('<.*?>+', '', text)\n    return text","8978fce3":"data['text'] = data['text'].apply(lambda x:clean(x))","b47adaa6":"sia=SIA()\nscores=[]\nfor i in range(len(data['text'])):\n    \n    score = sia.polarity_scores(data['text'][i])\n    score=score['compound']\n    scores.append(score)\nsentiment=[]\nfor i in scores:\n    if i>=0.05:\n        sentiment.append('Positive')\n    elif i<=(-0.05):\n        sentiment.append('Negative')\n    else:\n        sentiment.append('Neutral')\ndata['sentiment']=pd.Series(np.array(sentiment))","460c778e":"import string\n\ndef clean_text(text):\n    \n    text = str(text).lower()\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    \n    return text\ndata['text'] = data['text'].apply(lambda x:clean_text(x))\n\ndata['text']","16df5404":"df=pd.DataFrame()\ndf['text']=data['text']\ndef tokenization(text):\n    text = re.split('\\W+', text)\n    return text\n\ndf['tokenized'] = df['text'].apply(lambda x: tokenization(x.lower()))\nstopword = nltk.corpus.stopwords.words('english')\ndef remove_stopwords(text):\n    text = [word for word in text if word not in stopword]\n    return text\n    \ndf['No_stopwords'] = df['tokenized'].apply(lambda x: remove_stopwords(x))\n\nps = nltk.PorterStemmer()\n\ndef stemming1(text):\n    text = [ps.stem(word) for word in text]\n    return text\n\ndf['stemmed_porter'] = df['No_stopwords'].apply(lambda x: stemming1(x))\n\nfrom nltk.stem.snowball import SnowballStemmer\ns_stemmer = SnowballStemmer(language='english')\ndef stemming2(text):\n    text = [s_stemmer.stem(word) for word in text]\n    return text\ndf['stemmed_snowball'] = df['No_stopwords'].apply(lambda x: stemming2(x))\n\nwn = nltk.WordNetLemmatizer()\n\ndef lemmatizer(text):\n    text = [wn.lemmatize(word) for word in text]\n    return text\n\ndf['lemmatized'] = df['No_stopwords'].apply(lambda x: lemmatizer(x))\n\n","4a362ecb":"df.head(10)","d619918c":"temp = data.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\ntemp.style.background_gradient(cmap='coolwarm_r')","5ac8309f":"from sklearn.feature_extraction.text import CountVectorizer\nn = nltk.WordNetLemmatizer()\n\nfrom nltk.tokenize import RegexpTokenizer\ntoken = RegexpTokenizer(r'[a-zA-Z0-9]+')\ncv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\ntext_counts = cv.fit_transform(df['text'])","57507527":"print (text_counts)","b71238eb":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(text_counts, data['sentiment'], test_size=0.25, random_state=5)","7a9a71b8":"print (X_train)","ac24ac5d":"from sklearn.naive_bayes import MultinomialNB","29db644b":"MNB = MultinomialNB()\nMNB.fit(X_train, Y_train)","588cf078":"from sklearn import metrics\npredicted = MNB.predict(X_test)\naccuracy_score = metrics.accuracy_score(predicted, Y_test)","5bd4f3f5":"print(str('{:04.2f}'.format(accuracy_score*100))+'%')","b49a8f25":"from sklearn.naive_bayes import ComplementNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nCNB = ComplementNB()\nGNB = GaussianNB()\nBNB = BernoulliNB()","8dd9443b":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer()\ntext_count_2 = tfidf.fit_transform(df['text'])\n\n#splitting the data in test and training\n#from sklearn.model_selection() import train_test_split()\nx_train, x_test, y_train, y_test = train_test_split(text_count_2, data['sentiment'],test_size=0.25,random_state=5)\n\n#defining the model\n#compilimg the model -> we are going to use already used models  MNB, CNB, BNB\n#fitting the model\nMNB.fit(x_train, y_train)\naccuracy_score_mnb = metrics.accuracy_score(MNB.predict(x_test), y_test)\nprint('accuracy_score_mnb = '+str('{:4.2f}'.format(accuracy_score_mnb*100))+'%')\n\nBNB.fit(x_train, y_train)\naccuracy_score_bnb = metrics.accuracy_score(BNB.predict(x_test), y_test)\nprint('accuracy_score_bnb = '+str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')\n\nCNB.fit(x_train, y_train)\naccuracy_score_cnb = metrics.accuracy_score(CNB.predict(x_test), y_test)\nprint('accuracy_score_cnb = '+str('{:4.2f}'.format(accuracy_score_cnb*100))+'%')\n\n","5b95c4ef":"print(text_count_2)","dfc409dd":"import matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.colors import n_colors\nfrom plotly.subplots import make_subplots","6e9dbd12":"def ngram_df(corpus,nrange,n=None):\n    vec = CountVectorizer(stop_words = 'english',ngram_range=nrange).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    total_list=words_freq[:n]\n    df=pd.DataFrame(total_list,columns=['text','count'])\n    return df\nunigram_df=ngram_df(df['text'],(1,1),20)\nbigram_df=ngram_df(df['text'],(2,2),20)\ntrigram_df=ngram_df(df['text'],(3,3),20)\nfig = make_subplots(\n    rows=3, cols=1,subplot_titles=(\"Unigram\",\"Bigram\",'Trigram'),\n    specs=[[{\"type\": \"scatter\"}],\n           [{\"type\": \"scatter\"}],\n           [{\"type\": \"scatter\"}]\n          ])\n\nfig.add_trace(go.Bar(\n    y=unigram_df['text'][::-1],\n    x=unigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=unigram_df['count'],\n    textposition = \"outside\",\n    orientation=\"h\",\n    name=\"Months\",\n),row=1,col=1)\n\nfig.add_trace(go.Bar(\n    y=bigram_df['text'][::-1],\n    x=bigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=bigram_df['count'],\n     name=\"Days\",\n    textposition = \"outside\",\n    orientation=\"h\",\n),row=2,col=1)\n\nfig.add_trace(go.Bar(\n    y=trigram_df['text'][::-1],\n    x=trigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=trigram_df['count'],\n     name=\"Days\",\n    orientation=\"h\",\n    textposition = \"outside\",\n),row=3,col=1)\n\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_layout(title_text='Top N Grams',xaxis_title=\" \",yaxis_title=\" \",\n                  showlegend=False,title_x=0.5,height=1200,template=\"plotly_dark\")\nfig.show()","bb57e23c":"Positive_tweet = data[data['sentiment']=='Positive'].reset_index()\nNegative_tweet = data[data['sentiment']=='Negative'].reset_index()\nNeutral_tweet = data[data['sentiment']=='Neutral'].reset_index()","4069cafb":"Positive_tweet.head()","3caaf04e":"import unicodedata","6c190c72":"def basic_clean(text):\n  wnl = nltk.stem.WordNetLemmatizer()\n  stopwords = nltk.corpus.stopwords.words('english') \n  text = (unicodedata.normalize('NFKD', text)\n    .encode('ascii', 'ignore')\n    .decode('utf-8', 'ignore')\n    .lower())\n  words = re.sub(r'[^\\w\\s]', '', text).split()\n  return [wnl.lemmatize(word) for word in words if word not in stopwords]","ebb352e8":"words = basic_clean(''.join(str(Positive_tweet['text'].tolist())))","575d64aa":"print (Positive_tweet['text'].count())","26a761e2":"unigrams_series =(pd.Series(nltk.ngrams(words, 1)).value_counts())[:30]\nunigrams_series.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\nplt.title('30 Most Frequently Occuring unigrams')\nplt.ylabel('Unigram')\nplt.xlabel('# of Occurances')\n\n","c60369a5":"print (unigrams_series)\n","462e108b":"unigrams_series.plot()\nplt.show()","ce1a5b41":"bigrams_series =(pd.Series(nltk.ngrams(words, 2)).value_counts())[:30]\nbigrams_series.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\nplt.title('30 Most Frequently Occuring Bigrams')\nplt.ylabel('Bigram')\nplt.xlabel('# of Occurances')\n","5ad6472a":"trigrams_series =(pd.Series(nltk.ngrams(words, 3)).value_counts())[:30]\ntrigrams_series.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\nplt.title('30 Most Frequently Occuring Trigrams')\nplt.ylabel('trigram')\nplt.xlabel('# of Occurances')","63e74d7b":"nwords = basic_clean(''.join(str(Negative_tweet['text'].tolist())))","e141afdf":"unigrams_nseries =(pd.Series(nltk.ngrams(nwords, 1)).value_counts())[:30]\nunigrams_nseries.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\nplt.title('30 Most Frequently Occuring unigrams')\nplt.ylabel('unigram')\nplt.xlabel('# of Occurances')","2c108404":"bigrams_nseries =(pd.Series(nltk.ngrams(nwords, 2)).value_counts())[:30]\nbigrams_nseries.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\nplt.title('30 Most Frequently Occuring Bigrams')\nplt.ylabel('Bigram')\nplt.xlabel('# of Occurances')\n","77178c77":"trigrams_nseries =(pd.Series(nltk.ngrams(nwords, 3)).value_counts())[:20]\ntrigrams_nseries.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\nplt.title('20 Most Frequently Occuring trigrams')\nplt.ylabel('trigram')\nplt.xlabel('# of Occurances')","77b80aeb":"neuwords = basic_clean(''.join(str(Neutral_tweet['text'].tolist())))","be990a49":"unigrams_neuseries =(pd.Series(nltk.ngrams(neuwords, 1)).value_counts())[:30]\nunigrams_neuseries.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\nplt.title('30 Most Frequently Occuring unigrams')\nplt.ylabel('unigram')\nplt.xlabel('# of Occurances')","540b1c73":"bigrams_neuseries =(pd.Series(nltk.ngrams(neuwords, 2)).value_counts())[:30]\nbigrams_neuseries.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\nplt.title('30 Most Frequently Occuring Bigrams')\nplt.ylabel('Bigram')\nplt.xlabel('# of Occurances')","11e02922":"trigrams_neuseries =(pd.Series(nltk.ngrams(neuwords, 3)).value_counts())[:30]\ntrigrams_neuseries.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\nplt.title('30 Most Frequently Occuring trigrams')\nplt.ylabel('trigram')\nplt.xlabel('# of Occurances')","6a3fd628":"import scattertext as st","2571dcc2":"data.head()","275c7cbf":"data1=pd.DataFrame()\nfrom IPython.display import IFrame\n","f42a09e4":"data1=data.copy()\n\ndata1['binary_sentiment'] = data1['sentiment'].apply(lambda x: x if x ==\"Negative\" else \"non-negative\")\ndata1['date'] = data1['date'].apply(str)\n\ndata = data1.assign(\n    parse=lambda data: data.text.apply(st.whitespace_nlp_with_sentences)\n)\n\ncorpus = st.CorpusFromParsedDocuments(\n    data, category_col='binary_sentiment', parsed_col='parse'\n).build().get_unigram_corpus().compact(st.AssociationCompactor(2000))\n\nhtml = st.produce_scattertext_explorer(\n    corpus,\n    category='Negative', category_name='Negative', not_category_name='Neutral\/Positive',\n    minimum_term_frequency=0, pmi_threshold_coefficient=0,\n    width_in_pixels=1000, metadata=corpus.get_df()['date'],\n    transform=st.Scalers.dense_rank\n    \n)\n\n","59433df0":"open('.\/demo_compact.html', 'w').write(html)\nIFrame(src='.\/demo_compact.html', width=1200, height=700)","cf983a49":"docs=Positive_tweet['text'].tolist()\ncv=CountVectorizer(max_df=0.85,stop_words='english',max_features=20000)\nword_count_vector=cv.fit_transform(docs)","da734377":"word_count_vector","dcde2f02":"list(cv.vocabulary_.keys())[:10]","2a5d07f6":"\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(word_count_vector)","fafe66ec":"\n\ndef sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n\ndef extract_topn_from_vector(feature_names, sorted_items, topn=32):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn]\n\n    score_vals = []\n    feature_vals = []\n    \n    # word index and corresponding tf-idf score\n    for idx, score in sorted_items:\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n\n    #create a tuples of feature,score\n    #results = zip(feature_vals,score_vals)\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    \n    return results\n\n\nfeature_names=cv.get_feature_names()\n\n\n#generate tf-idf for the given document\ntf_idf_vector=tfidf_transformer.transform(cv.transform(docs))\n\n#sort the tf-idf vectors by descending order of scores\nsorted_items=sort_coo(tf_idf_vector.tocoo())\n\n#extract only the top n; n here is 32\nkeywords=extract_topn_from_vector(feature_names,sorted_items,32)\n\n\nprint(\"\\n===Keywords===\")\nfor k in keywords:\n    print(k,keywords[k])\n","d74a32ec":"ndocs=Negative_tweet['text'].tolist()\nneudocs=Neutral_tweet['text'].tolist()","4b424f10":"word_count_vector_neu=cv.fit_transform(neudocs)","6d9ccde7":"\nword_count_vector_neu","19389cae":"list(cv.vocabulary_.keys())[:10]","816a2152":"tfidf_transformer.fit(word_count_vector_neu)","305f3b73":"feature_names=cv.get_feature_names()\n\n\n\n\n#generate tf-idf for the given document\ntf_idf_vector=tfidf_transformer.transform(cv.transform(neudocs))\n\n#sort the tf-idf vectors by descending order of scores\nsorted_items=sort_coo(tf_idf_vector.tocoo())\n\n#extract only the top n; n here is 32\nkeywords=extract_topn_from_vector(feature_names,sorted_items,32)\n\n\nprint(\"\\n===Keywords===\")\nfor k in keywords:\n    print(k,keywords[k])\n\n\ndef sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n\ndef extract_topn_from_vector(feature_names, sorted_items, topn=32):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn]\n\n    score_vals = []\n    feature_vals = []\n    \n    # word index and corresponding tf-idf score\n    for idx, score in sorted_items:\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n\n    #create a tuples of feature,score\n    #results = zip(feature_vals,score_vals)\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    \n    return results","ce763680":"word_count_vector_n=cv.fit_transform(ndocs)\nlist(cv.vocabulary_.keys())[:10]","821921c1":"tfidf_transformer.fit(word_count_vector_n)","19fd6588":"feature_names=cv.get_feature_names()\n\n\n\n\n#generate tf-idf for the given document\ntf_idf_vector=tfidf_transformer.transform(cv.transform(ndocs))\n\n#sort the tf-idf vectors by descending order of scores\nsorted_items=sort_coo(tf_idf_vector.tocoo())\n\n#extract only the top n; n here is 32\nkeywords=extract_topn_from_vector(feature_names,sorted_items,100)\n\n\nprint(\"\\n===Keywords===\")\nfor k in keywords:\n    print(k,keywords[k])\n\n\ndef sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n\ndef extract_topn_from_vector(feature_names, sorted_items, topn=32):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn]\n\n    score_vals = []\n    feature_vals = []\n    \n    # word index and corresponding tf-idf score\n    for idx, score in sorted_items:\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n\n    #create a tuples of feature,score\n    #results = zip(feature_vals,score_vals)\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    \n    return results","0edc7083":"# ****Features of neutral tweets using tf-idf:","2b4580b0":"# ****Features of negative tweets using tf-idf:","afb52fd7":"# ****Features of positive tweets using tf-idf:"}}