{"cell_type":{"7cc4e676":"code","631409a1":"code","3e6cd268":"code","c14e174b":"code","3a7bce7f":"code","ea34da7a":"code","125665ca":"code","14d70218":"code","dc12fc25":"code","a55cdec7":"code","a5646d78":"code","2b8b841b":"code","2baa35c4":"code","c0e03c42":"code","96294017":"code","868b92ad":"code","54efddb3":"code","42e5e35e":"code","d67d64f0":"code","efd5180f":"code","ff984a26":"code","b53dff7b":"code","0aa1e86e":"code","b8a18345":"code","8d6dbf16":"code","01559ce3":"code","0a82bd22":"markdown","b1fc67a5":"markdown","888c897c":"markdown","879e2519":"markdown","a55803d4":"markdown","95d9ad82":"markdown","06532a36":"markdown","4277a2cf":"markdown","e89e326d":"markdown","dbaf6165":"markdown","3fb2e683":"markdown","d55e1da4":"markdown","3844fa83":"markdown","ae319257":"markdown","9660b4ad":"markdown","53d0f341":"markdown","d3cfe321":"markdown","dc85aa13":"markdown","bee72103":"markdown"},"source":{"7cc4e676":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","631409a1":"from scipy.stats import norm","3e6cd268":"#10 columns, 100000 samples\nl = [norm.rvs(size = 10**5, loc = 0, scale = i+1) for i in range(10)]\nl_test = [norm.rvs(size = 10**5, loc = 0, scale = i+1) for i in range(10)]\ndf = pd.DataFrame(l).T\ndf_test = pd.DataFrame(l_test).T","c14e174b":"c = {}\nfor i in range(10):\n    c[i] = 'Variable ' + str(i+1)\n\ndf.rename(columns = c, inplace = True)\ndf_test.rename(columns = c, inplace = True)","3a7bce7f":"noise = norm.rvs(size = 10**5, loc = 0, scale = 0.05)\nnoise_test = norm.rvs(size = 10**5, loc = 0, scale = 0.05)","ea34da7a":"df['y'] = df['Variable 1']\ndf_test['y'] = df_test['Variable 1']\nfor i in range(2,11):\n    df['y'] = df['y'] + df['Variable ' + str(i)]\n    df_test['y'] = df_test['y'] + df_test['Variable ' + str(i)]\n    \ndf['y'] = df['y'] + noise\ndf_test['y'] = df_test['y'] + noise_test","125665ca":"df.head()","14d70218":"df_test.head()","dc12fc25":"df1 = df.copy()\ndf2 = df.copy()\ny = df.y","a55cdec7":"df2.drop('y', axis = 1, inplace = True)","a5646d78":"df1.head()","2b8b841b":"df2.head()","2baa35c4":"from sklearn.decomposition import PCA","c0e03c42":"# We create two instances of the PCA\npca1 = PCA(n_components = 5)\npca2 = PCA(n_components = 5)","96294017":"# Fit transform of the PCA in each case\nprincipalcomponents1 = pca1.fit_transform(df1)\nprincipalcomponents2 = pca2.fit_transform(df2)","868b92ad":"# PCA explained variance in each dimension\n_, ax = plt.subplots(figsize = (10,5))\nax.plot(pca1.explained_variance_ratio_, color = 'b')\nax.plot(pca2.explained_variance_ratio_, color = 'r')","54efddb3":"# We take the data frame with the data represented in the principal components\nprincipalDf1 = pd.DataFrame(data = principalcomponents1, columns = ['1 - principal component {0}'.format(i+1) for i in range(5)])\nprincipalDf2 = pd.DataFrame(data = principalcomponents2, columns = ['2 - principal component {0}'.format(i+1) for i in range(5)])","42e5e35e":"from sklearn.linear_model import LinearRegression","d67d64f0":"linreg1 = LinearRegression()\nlinreg2 = LinearRegression()\n\nlinreg1.fit(principalDf1, y)\nlinreg2.fit(principalDf2, y)","efd5180f":"linreg1.coef_, linreg2.coef_","ff984a26":"df_test_1 = df_test.copy()\ndf_test_2 = df_test.copy()\n\ny_test = df_test.y\n\ndf_test_2.drop('y', axis = 1, inplace = True)","b53dff7b":"principalcomponents_test_1 = pca1.transform(df_test_1)\nprincipalcomponents_test_2 = pca2.transform(df_test_2) # Transforming our data with PCA\n\n# Constructing our data frames.\nprincipalDf1_test = pd.DataFrame(data = principalcomponents_test_1, columns = ['1 - principal component {0}'.format(i+1) for i in range(5)])\nprincipalDf2_test = pd.DataFrame(data = principalcomponents_test_2, columns = ['2 - principal component {0}'.format(i+1) for i in range(5)])\n\npredict_test_1 = linreg1.predict(principalDf1_test)\npredict_test_2 = linreg2.predict(principalDf2_test) # Making our predicitions in each case","0aa1e86e":"from sklearn.metrics import mean_squared_error","b8a18345":"e1 = mean_squared_error(predict_test_1, y_test)\ne2 = mean_squared_error(predict_test_2, y_test)\n\ne1, e2","8d6dbf16":"r = 0\nfor i in range(5):\n    r = r + np.std(df_test_2['Variable {0}'.format(i+1)])**2","01559ce3":"r","0a82bd22":"## Generating Data","b1fc67a5":"## PCA Time","888c897c":"#### Goal","879e2519":"#### We now make prediction with each model and test the results.","a55803d4":"#### Creating Target Variable","95d9ad82":"# A little experiment with PCA","06532a36":"Principal Component Analysis (PCA) is a well known technique for dimensionality reduction. In simple terms, it consists in the construction of a change of basis transformation such that the vectors in the new basis correspond to directions of maximum variance in the data set, and are in decreasing order in terms of variance.","4277a2cf":"Now, note:","e89e326d":"Suppose we are in the setting of a regression problem, i.e., our goal is to predict a continuously ranging variable, such as the canonical housing prices example. Furthermore, suppose we have a certain (large) number of numerical features that are relevant to use in our model (if there are just a few numerical features, maybe PCA is not necessary).\n\nWe want to understand what is the outcome if we fit our PCA instance with the target variable (which is, in this case, numerical) in the data set. Can this introduce data leakage in our model?\n\nTo test this, we'll construct a supervised learning problem and proceed with 2 versions of the model:\n\n1. We fit PCA including the target variable in the data set\n2. We fit PCA without the target variable in the data set\n\nIn the end, we compute some metrics to investigate the difference.","dbaf6165":"The models acually fit our data very differently!","3fb2e683":"#### Creating Features","d55e1da4":"From the second copy, we drop the target variable!","3844fa83":"The mean squared error of the second model approximates the sum of the variances of the features with least variance (which by the structure of our data, correspond to the first features). This means that error for the second model is exactly what we should expect, since we selected just 5 dimensions for the PCA, meaning that the features used to train the second models can't capture the smaller variance in the first features.\n\nNow, we get a unexpectedly good result for the first model. This most likely means that the first features of the PCA encoded most of the information about the target variable since it has the largest variance. ","ae319257":"#### Final tests and metrics","9660b4ad":"## Imports","53d0f341":"In order to test our assumptions, we'll create a data set with ten thousand instance as follows:\n\n1. Each column of the data is a normally distributed sample. Specifically, column j follows a normal distribution of mean 0 and standard deviation j.\n2. The target variable will be the sum of all variables with added noise. This noise is also normally distributed, with mean 0 and standard deviation 0.05.\n\nSince our data is artificially generated, there is no need to use 'train_test_split', we just create 2 sets (train and test).","d3cfe321":"## Linear Regression","dc85aa13":"Lets start by creating two copies of the training set, one to each case.","bee72103":"We fit a linear regression model for each case."}}