{"cell_type":{"1c20180e":"code","7396a626":"code","d1b450d1":"code","4e61056b":"code","31e34d01":"code","092579f3":"code","b65db977":"code","d08e8318":"code","79bd820d":"code","3f751335":"code","7be7e48a":"code","440b2c5b":"code","aeb334d4":"code","9b500aa3":"code","706da8c3":"code","ab9c65d5":"code","1f122fda":"code","f46845a3":"code","01015e35":"code","d5703fc0":"code","2eb0445b":"code","a59e69ae":"code","58ae0572":"code","57e588fc":"code","5e77c25d":"code","65467144":"code","4b9471d0":"code","f9d9fbf1":"code","e5fb5964":"code","444817aa":"code","896fe4ed":"code","0e33ca16":"code","68b135d6":"code","f034493d":"code","26b19302":"code","5975c85c":"code","701e1fe0":"code","2f21281b":"code","d14d76ec":"code","b5a7e64e":"code","188dbbea":"code","20c2fdf8":"code","842cda75":"code","113ef051":"code","82bf3ec8":"code","ce886460":"code","fe0a127b":"code","bb9dfaa6":"code","4bbb00ba":"code","4e16d96a":"code","dee706c4":"code","58f57586":"code","4fedeb29":"code","bdd2c862":"code","c3872cb8":"code","52363141":"code","1e028ec1":"code","a5eec5db":"code","2a694b76":"code","f59f8263":"code","e5b5f6af":"code","7d503095":"code","41fbbcb8":"code","bb7cb410":"code","4732ae00":"code","db5eff57":"code","c8734cbf":"code","839c619d":"code","2f7067f2":"code","85b16614":"code","2371beff":"code","25940f88":"code","85590c29":"code","445642a7":"code","de0424f3":"code","ef334cd0":"code","17e20df1":"code","03c72701":"code","42d581b7":"code","aa058454":"code","f8ca6b50":"code","908321ea":"code","6630c46d":"markdown","8cb57993":"markdown","64efb5cb":"markdown","cf763805":"markdown","ce7b96b3":"markdown","2193f464":"markdown","929dfbca":"markdown","0a3e864a":"markdown","01c7110a":"markdown","54a5cf9b":"markdown","66ef9b02":"markdown","82b1b164":"markdown","e5c9d3e0":"markdown","a2f59cca":"markdown","19f18f31":"markdown","b9ed64d6":"markdown","6d05b974":"markdown","9b8e1861":"markdown","faefc91b":"markdown","ce4af734":"markdown"},"source":{"1c20180e":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn import metrics\nfrom math import sqrt\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport sys\nimport datetime","7396a626":"class Classifier:\n    \n    def __init__(self):\n        pass\n    \n    def file_input(self,filename):\n        data = pd.read_csv(filename,engine='python',header=None)\n        return data\n    \n    def my_function(self,list1,list2,list3):\n        if list3 == 1:\n            if list1 in list2:\n                return 1\n            else:\n                return 0\n            \n    def convertCategory(self,x):\n        if x == 'S':\n            return -1\n        elif x in ['1','2','3','4','5','6','7','8','9','10','11','12']:\n            return x\n        else:\n            return 13\n    \n    def data_exploration_clicks(self,clicks):\n        \n        category_analaysis = clicks[(clicks['category'].isin(['S','0','1','2','3','4','5','6','7','8','9','10','11','12']))]\n      \n        chart_4 = sns.barplot(x=category_analaysis['category'].value_counts().index, y=category_analaysis['category'].value_counts())\n        chart_4.set_xticklabels(chart_4.get_xticklabels(),rotation=45)\n        plt.xlabel('Category')\n        plt.ylabel('Count')\n        plt.title('Count of clicks against each Category')\n        plt.show()\n   \n        chart_5 = sns.barplot(x=clicks['item_id'].value_counts().nlargest(10).index, y=clicks['item_id'].value_counts().nlargest(10))\n        chart_5.set_xticklabels(chart_5.get_xticklabels(),rotation=45)\n        plt.xlabel('Item_id')\n        plt.ylabel('Count')\n        plt.title('Top 10 Items having maximum clicks:')\n        plt.show()\n        \n    def data_exploration_buys(self,data):\n        \n        print(\"Top 10 Items which have been bought the maximum.\")\n        #print(buys['item_id'].value_counts().nlargest(10))\n        chart_1 = sns.barplot(x=buys['item_id'].value_counts().nlargest(10).index, y=buys['item_id'].value_counts().nlargest(10))\n        chart_1.set_xticklabels(chart_1.get_xticklabels(),rotation=45)\n        plt.xlabel('Item_id')\n        plt.ylabel('Count')\n        plt.title('Top 10 Items which have been bought the maximum.')\n        plt.show()\n        \n        print(\"Top 10 items which are purchased in larger quantities.\")\n        quantity_analysis = buys[['item_id','qty']].groupby('item_id').agg(total_quantity=pd.NamedAgg(column='qty',aggfunc=sum))\n        quant_analysis = quantity_analysis.sort_values('total_quantity',ascending=False).nlargest(10,columns='total_quantity')\n        chart_2 = sns.barplot(x = quant_analysis.index, y = quant_analysis['total_quantity'] ,data = quant_analysis)\n        chart_2.set_xticklabels(chart_2.get_xticklabels(),rotation=45)\n        plt.xlabel('Item_id')\n        plt.ylabel('Quantity')\n        plt.title('Top 10 items which are purchased in larger quantities.')\n        plt.show()\n        \n        print(\"Top 10 items Identifying the items having the maximum price.\")\n        buys_plot = buys[['item_id','price']].drop_duplicates().sort_values('price',ascending=False).nlargest(10,columns='price')\n        chart_3 = sns.barplot(x = buys_plot['item_id'] , y = buys_plot['price'] ,data = buys_plot)\n        chart_3.set_xticklabels(chart_3.get_xticklabels(),rotation=45)\n        plt.xlabel('Item_id')\n        plt.ylabel('Price')\n        plt.title('Top 10 items having the maximum cost price.')\n        plt.show()\n        \n    def transforming_buys(self,buys):\n        print(\"Transforming the buys file ...!!!\")\n        grouped = buys.groupby(\"session\")\n        buys_g = pd.DataFrame(index=grouped.groups.keys())        \n        buys_g[\"Number_items_bought\"] = grouped.item_id.count()\n        buys_g[\"unique_items_bought\"] = grouped.item_id.unique()\n        buys_g[\"is_buy\"] = 1\n        buys_g.index.name = \"session\"\n        print(\"Transformation of the buys file completed...!!!\")\n        return buys_g\n    \n    def chunk_load_data(self,chunk):\n        return chunk\n        \n        \n    def transforming_clicks(self,clicks):\n        \n        clicks_new = clicks.groupby('session')['timestamp'].agg([min,max])\n\n        clicks_new['dwell_time'] = clicks_new['max'] - clicks_new['min'] #cal the dwell time of the session.\n        clicks_new['dwell_time_seconds'] = clicks_new['dwell_time'].dt.total_seconds() #converting dwell time into seconds\n        \n        clicks.loc[clicks['category'] == 'S',['category']] = -1\n\n        grouped = clicks.groupby('session')\n            \n        #print(\"Calculating the total clicks\")\n        clicks_new['total_clicks'] = grouped.item_id.count()\n        \n        #print(\"Calculating the day of week\")\n        clicks_new['dayofweek'] = clicks_new['min'].dt.dayofweek\n        \n        #print(\"Calculating the day of month\")\n        clicks_new['dayofmonth'] = clicks_new['min'].dt.day\n        \n        #print(\"Calculating hour of click\")\n        clicks_new['hourofclick'] = clicks_new['min'].dt.hour\n        \n        #print(\"Calculating time of click\")\n        b = [0,4,8,12,16,20,24]\n        l = ['Late Night', 'Early Morning','Morning','Noon','Evening','Night']\n        clicks_new['timeofday'] = pd.cut(clicks_new['hourofclick'], bins=b, labels=l, include_lowest=True)\n        \n        #print(\"Calculating clickrate\")\n        clicks_new[\"click_rate\"] = clicks_new[\"total_clicks\"] \/ clicks_new[\"dwell_time_seconds\"]\n        clicks_new.click_rate = clicks_new.click_rate.replace(np.inf, np.nan)\n        clicks_new.click_rate = clicks_new.click_rate.fillna(0)\n        \n        #print(\"** Transformed**\")\n        return clicks_new\n\n    def transforming_clicks2(self,clicks):\n\n        grouped = clicks.groupby('session').agg({'item_id':['first','last','nunique'],'category':['nunique']})        \n        return grouped\n\n    def transforming_clicks3(self,clicks):\n       \n        keys, values = clicks.sort_values('session').values.T\n        ukeys, index = np.unique(keys, True)\n        arrays = np.split(values, index[1:])\n        df2 = pd.DataFrame({'a':ukeys, 'b':(a for a in arrays)})\n        return df2\n    \n    def transforming_clicks3_cat(self,clicks):\n       \n        keys, values = clicks.sort_values('session').values.T\n        ukeys, index = np.unique(keys, True)\n        arrays = np.split(values, index[1:])\n        df2 = pd.DataFrame({'a':ukeys, 'b':(a for a in arrays)})\n        return df2\n\n    def data_preparation(self,X,y):\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n        \n        return X_train, X_test, y_train, y_test\n    \n        \n    def undersampling(self,train_data):   \n        \n        count_class_0, count_class_1 = train_data['is_buy'].value_counts()\n        df_class_0 = train_data[train_data['is_buy'] == 0]\n        df_class_1 = train_data[train_data['is_buy'] == 1]\n        df_class_0_under = df_class_0.sample(count_class_1)\n        df_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n        df_test_under['is_buy'].value_counts()\n        return df_test_under\n        \n    def one_hot_encode(self,column_name,training_data):\n        temp = pd.get_dummies(training_data[column_name])\n        training_data = pd.concat([training_data, temp], axis=1)\n        return training_data\n        \n    def get_preds(self,threshold, probabilities):\n        return [1 if prob > threshold else 0 for prob in probabilities]\n        \n    def logit_model(self,train_x,train_y,test_x,test_y,thres=0.5):\n        \n        model = LogisticRegression(solver='sag')\n        model.fit(train_x,train_y.values.ravel())\n        probas = model.predict_proba(test_x)[:, 1]\n        print(\"Threshold Value : \",thres)\n        y_pred = self.get_preds(thres,probas)\n        return y_pred,probas\n\n    def calc_special_offer(self,x):\n        if -1 in x:\n            return 1\n        else:\n            return 0\n    \n    def error_metrics(self,prediction,test_y,probas):\n        \n        accuracy = accuracy_score(prediction,test_y)        \n        print('Accuracy =',accuracy)\n        print(\"\")\n        print(pd.DataFrame(confusion_matrix(test_y, prediction), columns=['Predicted 0', \"Predicted 1\"], index=['Actual 0', 'Actual 1']))\n        print(\"classification_Report:\")\n        print(classification_report(test_y,prediction))\n        fig, ax = plt.subplots(figsize=(10,7))\n        fpr, tpr, threshold = metrics.roc_curve(test_y,probas,pos_label=1)\n        i = np.arange(len(tpr)) \n        roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n        roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n        print(\"Optimum Threshold Value:\",list(roc_t['threshold']))\n        \n        auc = metrics.roc_auc_score(test_y, probas)\n        plt.plot(fpr,tpr,label=\"auc=\"+str(auc))\n        ax.plot(np.linspace(0, 1, 100),np.linspace(0, 1, 100),label='baseline',linestyle='--')\n        plt.title('Receiver Operating Characteristic Curve', fontsize=18)\n        plt.ylabel('TPR', fontsize=16)\n        plt.xlabel('FPR', fontsize=16)\n        plt.legend(fontsize=12);\n        plt.show()","d1b450d1":"clicks_file = \"\/input\/recsys-challenge-2015\/yoochoose-data\/yoochoose-clicks.dat\"\nbuys_file = \"\/input\/recsys-challenge-2015\/yoochoose-data\/yoochoose-buys.dat\"","4e61056b":"data = Classifier()","31e34d01":"result=None\ncount=1\nnames=[\"session\", \"timestamp\", \"item_id\", \"category\"]\nfor chunk in pd.read_csv(clicks_file,names=names,usecols = ['session','timestamp','item_id','category'],parse_dates=[\"timestamp\"],chunksize=500000):\n    print(\"Executing Chunk \",count,\"\/67\")\n    click_df = data.transforming_clicks(chunk)\n    if result is None:\n      result=click_df\n      count=count+1\n    else:\n      result = result.append(click_df)  \n      count=count+1\nprint(\"Done 1st Transformation of Clicks file to calculate \\n1.Session Start Time \\n2.Session End Time  \\n3.Session Dwell Time \\n4.Dwell time seconds \\n5.Total clicks \\n6.Dayofweek \\n7.Dayofmonth \\n8.Hourofclick \\n9.Timeofday \\n10.Click_Rate\")","092579f3":"result_2=None\ncount=1\nnames=[\"session\", \"timestamp\", \"item_id\", \"category\"]\nfor chunk in pd.read_csv(clicks_file,usecols = ['session','item_id','category'],names=names,chunksize=500000):\n    print(\"Executing Chunk \",count,\"\/67\")\n    click_df = data.transforming_clicks2(chunk)\n    if result_2 is None:\n      result_2=click_df\n      count=count + 1\n    else:\n      result_2 = result_2.append(click_df) \n      count= count + 1\nprint(\"Done Transforming Clicks input file to calculate \\n 1.First Clicked item \\n 2.Last Clicked Item \\n 3.Total Unique Items  \\n 4.Total Unique Categories \")\ncolnames=['first_clicked_item','last_clicked_item','total_unique_items','total_unique_categories']\nresult_2.columns = colnames\n#result_2.set_index('session')","b65db977":"result_3=None\ncount=1\nnames=[\"session\", \"timestamp\", \"item_id\", \"category\"]\nfor chunk in pd.read_csv(clicks_file,names=names,usecols = ['session','item_id'],chunksize=500000):\n    print(\"Executing Chunk \",count,\"\/67\")\n    click_df = data.transforming_clicks3(chunk)\n    if result_3 is None:\n      result_3=click_df\n      count = count + 1\n    else:\n      result_3 = result_3.append(click_df) \n      count = count + 1\n    \nprint(\"Done Transforming Clicks\")\ncolnames=['session','visited_items']\nresult_3.columns = colnames\nresult_3 = result_3.set_index('session')\nprint(datetime.datetime.now())","d08e8318":"result_4=None\ncount=1\nnames=[\"session\", \"timestamp\", \"item_id\", \"category\"]\nfor chunk in pd.read_csv(clicks_file,names=names,usecols = ['session','category'],converters={\"category\": data.convertCategory},chunksize=500000):\n    print(\"Executing Chunk \",count,\"\/67\")\n    click_df = data.transforming_clicks3_cat(chunk)\n    if result_4 is None:\n      result_4 = click_df\n      count = count + 1\n    else:\n      result_4 = result_4.append(click_df)  \n      count = count + 1\ncolnames=['session','visited_categories']\nresult_4.columns = colnames\nresult_4 = result_4.set_index('session')\nresult_4['Number_clicked_visited_categories'] = result_4['visited_categories'].apply(lambda x : len(x))\nresult_4['Special_offer_click']=result_4['visited_categories'].apply(data.calc_special_offer)\nprint(\"Done Transforming Clicks to calculate \\n 1.Unique Visited categories \\n 2. Total Visited Categories \\n 3.Special offer click \" )","79bd820d":"buys = data.file_input(buys_file)\nnames=[\"session\",\"timestamp\",\"item_id\",\"price\",\"qty\"]\nbuys.columns = names","3f751335":"data.data_exploration_buys(buys)","7be7e48a":"buys_g = data.transforming_buys(buys)","440b2c5b":"clicks_tranformed_updated = pd.concat([result,result_2,result_3,result_4], axis=1)","aeb334d4":"training_data = pd.merge(clicks_tranformed_updated,buys_g['is_buy'],how='left',left_index=True,right_index=True)\ntraining_data['is_buy'] = training_data['is_buy'].fillna(0)","9b500aa3":"training_data.head()","706da8c3":"print(\"calculating the popularity index for first and last item clicked\")\nnames=[\"session\", \"timestamp\", \"item_id\", \"category\"]\nresult_items = pd.concat([ chunk.apply(pd.Series.value_counts) for chunk in pd.read_csv(clicks_file,names=names,usecols = ['item_id'],index_col=0,chunksize=500000)])\ndf = pd.DataFrame(result_items.index.value_counts())\ndf.index.name = \"item_id\"\ndf.columns = ['count']\nval = df['count'].sum()\ndf['popularity'] = df['count'].apply(lambda x : x \/ val )\ndf['popularity'] = df['popularity'].round(5)\nprint(\"Done..!!\")","ab9c65d5":"updated_training_df = pd.merge(training_data, df, left_on='first_clicked_item',right_on=df.index,how='inner')\nupdated_training_df.rename(columns={'popularity': 'first_clicked_item_popularity'},inplace=True)","1f122fda":"updated_training_df = pd.merge(updated_training_df, df, left_on='last_clicked_item',right_on=df.index,how='inner')\nupdated_training_df.rename(columns={'popularity': 'last_clicked_item_popularity'},inplace=True)","f46845a3":"temp = training_data[training_data['is_buy'] == 1]\n\n#df['col_3'] = df.apply(lambda x: f(x.col_1, x.col_2), axis=1)\ntemp['first_item_probab_check'] = temp.apply(lambda x : data.my_function(x.first_clicked_item,x.unique_items_bought,x.is_buy),axis=1)\ntemp['last_item_probab_check'] = temp.apply(lambda x : data.my_function(x.last_clicked_item,x.unique_items_bought,x.is_buy),axis=1)\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nsns.barplot(x=temp.first_item_probab_check.value_counts().index, y=temp.first_item_probab_check.value_counts()\/temp.first_item_probab_check.value_counts().sum())\nplt.title('Probability of First clicked item being Purchased')\nplt.xlabel('First Clicked Item')\nplt.legend()\n\n\nsns.barplot(x=temp.last_item_probab_check.value_counts().index, y=temp.last_item_probab_check.value_counts()\/temp.last_item_probab_check.value_counts().sum())\nplt.title('Probability of Last clicked item being Purchased')\nplt.xlabel('Last Clicked Item')\nplt.legend()","01015e35":"training_data[training_data['is_buy'] == 1]['dwell_time_seconds'].mean()","d5703fc0":"training_data[training_data['is_buy'] == 0]['dwell_time_seconds'].mean()","2eb0445b":"ax = sns.barplot(x=training_data['dayofweek'].value_counts().index,y=training_data['dayofweek'].value_counts(),data=training_data)\nplt.xlabel('DaysofWeek')\nplt.ylabel('Countofsessions')\nplt.ticklabel_format(style='plain', axis='y')\nplt.title('Most Popular Days based on Number of sessions.[0 - 6] -> [Monday - Sunday]')","a59e69ae":"training_data_temp = training_data[training_data['is_buy'] == 1]\nax = sns.barplot(x=training_data_temp['dayofweek'].value_counts().index,y=training_data_temp['dayofweek'].value_counts(),data=training_data_temp)\nplt.xlabel('DaysofWeek')\nplt.ylabel('Countofsessions')\nplt.ticklabel_format(style='plain', axis='y')\nplt.title('Popular Days for Buying Events based on Number of Sessions.')","58ae0572":"training_data_temp['timeofday'].value_counts()\nax = sns.barplot(x=training_data_temp['timeofday'].value_counts().index,y=training_data_temp['timeofday'].value_counts(),data=training_data_temp)\nplt.xlabel('timeofday')\nplt.ylabel('Countofsessions')\nplt.ticklabel_format(style='plain', axis='y')\nplt.title('Best Time of the Day for a buying Event.')","57e588fc":"print(updated_training_df['is_buy'].value_counts())","5e77c25d":"new_balanced_data = data.undersampling(updated_training_df)\nprint(new_balanced_data['is_buy'].value_counts())","65467144":"updated_training_data = data.one_hot_encode(\"timeofday\",new_balanced_data)","4b9471d0":"preprocessed_training_data = updated_training_data.loc[:,~updated_training_data.columns.isin([\n    'min', 'max', 'dwell_time',\n       'first_clicked_item', 'last_clicked_item','timeofday',\n       'visited_items', 'visited_categories','hourofclick',\n       'Number_items_bought', 'unique_items_bought', 'count_x',\n       'count_y'\n])]","f9d9fbf1":"import seaborn as sn\nimport matplotlib.pyplot as plt\n\n\ncorrMatrix = preprocessed_training_data.corr()\nplt.figure(figsize=(20,10))\nsn.heatmap(corrMatrix, annot=True)\nplt.show()","e5fb5964":"X = preprocessed_training_data.loc[:,~preprocessed_training_data.columns.isin(['is_buy'])]\ny = preprocessed_training_data.loc[:,preprocessed_training_data.columns.isin(['is_buy'])]","444817aa":"X_train, X_test, y_train, y_test = data.data_preparation(X,y)","896fe4ed":"y_test['is_buy'] = pd.to_numeric(y_test['is_buy']).round(0).astype(int)\ny_train['is_buy'] = pd.to_numeric(y_train['is_buy']).round(0).astype(int)","0e33ca16":"pred,prob = data.logit_model(X_train,y_train,X_test,y_test,0.49)\ndata.error_metrics(pred,y_test['is_buy'],prob)","68b135d6":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nimport pandas as pd\nimport lightgbm as lgb\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import auc, accuracy_score, roc_auc_score, roc_curve\nfrom sklearn.model_selection import GridSearchCV","f034493d":"updated_training_data = data.one_hot_encode(\"timeofday\",new_balanced_data)\n\npreprocessed_training_data = new_balanced_data.loc[:,~new_balanced_data.columns.isin([\n    'min', 'max', 'dwell_time',\n       'first_clicked_item', 'last_clicked_item','timeofday',\n       'visited_items', 'visited_categories','hourofclick',\n       'Number_items_bought', 'unique_items_bought', 'count_x',\n       'count_y'\n])]","26b19302":"X = preprocessed_training_data.loc[:,~preprocessed_training_data.columns.isin(['is_buy'])]\ny = preprocessed_training_data.loc[:,preprocessed_training_data.columns.isin(['is_buy'])]","5975c85c":"X_train, X_test, y_train, y_test = data.data_preparation(X,y)","701e1fe0":"estimator = lgb.LGBMClassifier(boosting_type='gbdt', learning_rate = 0.125, metric = 'l1', n_estimators = 20, num_leaves = 38)\n\n\n#param_grid = {\n#    'n_estimators': [x for x in [150,200,250]],\n#    'learning_rate': [0.30,0.40,0.50],\n#    'num_leaves': [30,35,40],\n#    'boosting_type' : ['gbdt'],\n#    'objective' : ['binary'],\n#    'random_state' : [501]}\n     \nparam_grid = {\n    'n_estimators': [x for x in [150]],\n    'learning_rate': [0.25],\n    'num_leaves': [32],\n    'boosting_type' : ['gbdt'],\n    'objective' : ['binary'],\n    'random_state' : [501]}\n\ngridsearch = GridSearchCV(estimator, param_grid)\n\ngridsearch.fit(X_train, y_train.values.ravel(),eval_set = [(X_test, y_test)],eval_metric = ['auc', 'binary_logloss'],early_stopping_rounds = 10)","2f21281b":"print('Best parameters found by grid search are:', gridsearch.best_params_)","d14d76ec":"gbm = lgb.LGBMClassifier(learning_rate = 0.35, metric = 'l1', n_estimators = 150,num_leaves= 35)\n\n\ngbm.fit(X_train, y_train,\n        eval_set=[(X_test, y_test)],\n        eval_metric=['auc', 'binary_logloss'],\nearly_stopping_rounds=20)","b5a7e64e":"ax = lgb.plot_importance(gbm, height = 0.4, \n                         max_num_features = 15, \n                         xlim = (0,1000), ylim = (0,10), \n                         figsize = (10,6))\nplt.show()","188dbbea":"y_pred_prob = gbm.predict_proba(X_test)[:, 1]\nauc_roc_0 = str(roc_auc_score(y_test, y_pred_prob))\nprint('AUC: \\n' + auc_roc_0)","20c2fdf8":"pred = []\nfor i in y_pred_prob:\n    if i > 0.5:\n        pred.append(1)\n    else:\n        pred.append(0)","842cda75":"data.error_metrics(pred,y_test['is_buy'],y_pred_prob)","113ef051":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier","82bf3ec8":"preprocessed_training_data = new_balanced_data.loc[:,~new_balanced_data.columns.isin([\n    'min', 'max', 'dwell_time',\n       'first_clicked_item', 'last_clicked_item','timeofday',\n       'visited_items', 'visited_categories','hourofclick',\n       'Number_items_bought', 'unique_items_bought', 'count_x',\n       'count_y'\n])]","ce886460":"rfc=RandomForestClassifier(random_state=42)\nparam_grid = { \n    'n_estimators': [150,200,250],\n    'max_features': ['log2'],\n    'max_depth' : [4,6],\n    'criterion' :['gini']\n}\n\nCV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 2)\nCV_rfc.fit(X_train, y_train)","fe0a127b":"print(CV_rfc.best_params_)","bb9dfaa6":"rfc1=RandomForestClassifier(random_state=42, max_features='auto', n_estimators= 200, max_depth=4, criterion='gini')","4bbb00ba":"rfc1.fit(X_train, y_train)","4e16d96a":"probab_pred = rfc1.predict_proba(X_test)\npred = rfc1.predict(X_test)","dee706c4":"auc_roc_0 = str(roc_auc_score(y_test, pred))\nprint('AUC: \\n' + auc_roc_0)","58f57586":"print(list(zip(X_train,rfc1.feature_importances_)))","4fedeb29":"data.error_metrics(pred,y_test['is_buy'],probab_pred[:, 1])","bdd2c862":"from keras import models\nfrom keras import layers\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nfrom keras.callbacks import EarlyStopping\nfrom keras import backend as K\nfrom matplotlib import pyplot as plt","c3872cb8":"updated_training_data = data.one_hot_encode(\"timeofday\",new_balanced_data)\n\npreprocessed_training_data = updated_training_data.loc[:,~updated_training_data.columns.isin([\n    'min', 'max', 'dwell_time',\n       'first_clicked_item', 'last_clicked_item','timeofday',\n       'visited_items', 'visited_categories',#'hourofclick',\n       'Number_items_bought', 'unique_items_bought', 'count_x',\n       'count_y'\n])]","52363141":"preprocessed_training_data = preprocessed_training_data.reindex(columns=['dwell_time_seconds','total_clicks','dayofweek','dayofmonth','hourofclick','click_rate','total_unique_items','total_unique_categories','Number_clicked_visited_categories','Special_offer_click','first_clicked_item_popularity','last_clicked_item_popularity','Late Night','Early Morning','Morning','Noon','Evening','Night','is_buy'])","1e028ec1":"dataset = preprocessed_training_data.values\nprint(dataset.shape)","a5eec5db":"X = dataset[:,0:18]\ny = dataset[:,18]","2a694b76":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80,test_size=0.20, random_state=101)","f59f8263":"class Neural_Net_CLassifier:\n    \n    def predicted(self,prediction):\n        list1=[]\n        for i in prediction:\n            print(\"\")\n            for j in i:\n                if j > 0.5:\n                    list1.append(1)\n                else:\n                    list1.append(0)\n        return list1\n\n    def NN_arch4(self,lrate=0.0001):\n        #2 hidden layer with a relu activation\n        model = models.Sequential()\n        model.add(layers.Dense(18,input_dim = 18, activation='sigmoid'))\n        model.add(layers.Dense(6, activation='relu'))\n        model.add(layers.Dense(5, activation='relu'))\n        model.add(layers.Dense(1, activation='sigmoid'))\n        opt = tf.keras.optimizers.Adam(lr=lrate)\n        model.compile(optimizer=opt,loss='binary_crossentropy',metrics=['accuracy'])\n        return model\n\n    def model_fit(self,model,epoch_val=50):\n        model.fit(X_train, y_train, epochs=epoch_val,batch_size=100)\n        val_loss, val_acc = model.evaluate(X_test,y_test)\n        print(val_loss, val_acc)\n        return val_loss,val_acc\n\n    def history_plot(self,history):\n        training_loss = history.history['loss']\n        test_loss = history.history['val_loss']\n\n        training_acc = history.history['accuracy']\n        test_acc = history.history['val_accuracy']\n\n        # Create count of the number of epochs\n        epoch_count = range(1, len(training_loss) + 1)\n\n        # Visualize loss history\n        plt.figure(figsize=(5,3))\n\n        plt.plot(epoch_count, training_loss, 'r--')\n        plt.plot(epoch_count, test_loss, 'b-')\n        plt.legend(['Training Loss', 'Test Loss'])\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.show()\n\n        #Visualize accuracy history\n        plt.plot(epoch_count, training_acc, 'r--')\n        plt.plot(epoch_count, test_acc, 'b-')\n        plt.legend(['Training acc', 'Test acc'])\n        plt.xlabel('Epoch')\n        plt.ylabel('Acc')\n        plt.show();","e5b5f6af":"NN_classifier = Neural_Net_CLassifier()","7d503095":"model = NN_classifier.NN_arch4(0.0006)\n#print(\"Learning rate before second fit:\", model.optimizer.learning_rate.numpy())\nval_loss,val_acc = NN_classifier.model_fit(model,50)\n#history = model.fit(X_train,y_train,epochs=10,verbose=0,validation_data=(X_test, y_test)) \n#NN_classifier.history_plot(history)","41fbbcb8":"y_pred_keras = model.predict(X_test).ravel()\nyhat_classes = model.predict_classes([X_test], verbose=0)","bb7cb410":"from sklearn.metrics import roc_curve\nfpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_pred_keras)\nfrom sklearn.metrics import auc\nauc_keras = auc(fpr_keras, tpr_keras)\nplt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')\nplt.show()\n# Zoom in view of the upper left corner.","4732ae00":"auc_roc_0 = str(roc_auc_score(y_test, prediction))\nyhat_classes = yhat_classes[:, 0]\nprint('AUC: \\n' + auc_roc_0)","db5eff57":"accuracy = accuracy_score(y_test, yhat_classes)\nprint('Accuracy: %f' % accuracy)\n\n# precision tp \/ (tp + fp)\nprecision = precision_score(y_test, yhat_classes)\nprint('Precision: %f' % precision)\n\n# recall: tp \/ (tp + fn)\nrecall = recall_score(y_test, yhat_classes)\nprint('Recall: %f' % recall)\n\n# f1: 2 tp \/ (2 tp + fp + fn)\nf1 = f1_score(y_test, yhat_classes)\nprint('F1 score: %f' % f1)","c8734cbf":"class Item_Predictor:\n\n    def p_root(self,value, root): \n        root_value = 1 \/ float(root) \n        return round (Decimal(value) **Decimal(root_value), 3) \n\n    def minkowski_distance(self,x, y, p_value): \n        return (self.p_root(sum(pow(abs(a-b), p_value) for a, b in zip(x, y)), p_value))\n    \n    def data_transformation(self,result_4,df_clicks,df_buys):\n        #clicks = result_4[result_4['session'] == 11628]\n        clicks_new = result_4.groupby(['session','item_id','category']).size().reset_index()\n        clicks_updated = pd.merge(clicks_new,df_clicks['popularity'],left_on=clicks_new.item_id,right_on=df_clicks.index)\n        clicks_updated.columns = ['item_id_0','session','item_id','category','special_offer_click','click_popularity']\n        clicks_updated_2 = clicks_updated.merge(df_buys['popularity'],how='left',left_on=clicks_updated.item_id,right_on=df_buys.index)\n        click_updated_3 = clicks_updated_2.fillna(0)\n        return click_updated_3\n        \n    def purchace_item_validation_func(self,data):\n        if (data.Predicted_1 in data.Unique_Purchased_Items) or (data.Predicted_2 in data.Unique_Purchased_Items):\n            print('True')\n            list_1.append('True')\n        else:\n            list_1.append('False')\n            print('False')\n            \n    def predict(self,data,df,temp):\n        scores=[]        \n        data = data[data['session'] == temp]\n        if data.count()[0] > 1:\n            for i in range(0,len(list(zip(*[data[col] for col in data])))):\n                for j in range(i+1,len(list(zip(*[data[col] for col in data])))):    \n                    out_1 = [t for t in list(zip(*[data[col] for col in data]))[i]]\n                    out_2 = [t for t in list(zip(*[data[col] for col in data]))[j]]\n                    score = item_pred.minkowski_distance(out_1,out_2,2)\n                    scores.append([out_1[1],out_2[1],score])  \n            list_1 = sorted(scores,key=lambda l:l[2])\n            df.loc[temp] = [list_1[0][0],list_1[0][1]]            \n        else:\n            scores.append(data['item_id'])  \n            #print(\"Items to be Purchased(ItemID):\")\n            df.loc[temp] = [data.iloc[0]['item_id'],None]","839c619d":"item_pred = Item_Predictor()","2f7067f2":"def fun(chunk):\n    return chunk\n  \nresult_4=None\ncount=1\nnames=[\"session\", \"timestamp\", \"item_id\", \"category\"]\nfor chunk in pd.read_csv(clicks_file,names=names,usecols=[\"session\",\"item_id\",\"category\"],converters={\"category\": data.convertCategory},chunksize=500000):\n    print(\"Executing Chunk \",count,\"\/67\")\n    click_df = fun(chunk)\n    if result_4 is None:\n        result_4 = click_df\n        count = count + 1\n    else:\n        result_4 = result_4.append(click_df)  \n        count = count + 1\nprint(\"Done Transforming Clicks\" )\n\nresult_4 = result_4.set_index('session')\n\ncolnames=['item_id','category']\nresult_4.columns = colnames","85b16614":"buys_data = data.file_input(buys_file)\nnames=[\"session\", \"timestamp\", \"item_id\", \"price\", \"qty\"]\nbuys_data.columns = names","2371beff":"buys_transformed_data = data.transforming_buys(buys_data)","25940f88":"buys_transformed_data.head(10)","85590c29":"result_3=None\nnames=[\"session\", \"timestamp\", \"item_id\", \"category\"]\ncount=1\nfor chunk in pd.read_csv(clicks_file,names=names,usecols = ['session','item_id'],chunksize=500000):\n    print(\"Executing Chunk \",count,\"\/67\")\n    click_df = data.transforming_clicks3(chunk)\n    if result_3 is None:\n      result_3=click_df\n      count = count+1\n    else:\n      result_3 = result_3.append(click_df)  \n      count = count+1\nprint(\"Done Transforming Clicks to calculate \\n 1.Visited Items per Session\")\ncolnames=['session','visited_items']\nresult_3.columns = colnames\nresult_3 = result_3.set_index('session')","445642a7":"names=[\"session\", \"timestamp\", \"item_id\", \"category\"]\nresult_items = pd.concat([chunk.apply(pd.Series.value_counts) for chunk in pd.read_csv(clicks_file,names=names,usecols = ['item_id'],index_col=0,chunksize=500000)])\ndf_clicks = pd.DataFrame(result_items.index.value_counts())\ndf_clicks.index.name = \"item_id\"\ndf_clicks.columns = ['count']\nval = df_clicks['count'].sum()\ndf_clicks['popularity'] = df_clicks['count'].apply(lambda x : x \/ val )\ndf_clicks['popularity'] = df_clicks['popularity'].round(5)","de0424f3":"names=[\"session\", \"timestamp\", \"item_id\", \"price\", \"qty\"]\nresult_items = pd.concat([ chunk.apply(pd.Series.value_counts) for chunk in pd.read_csv(buys_file,names=names,usecols = ['item_id'],index_col=0,chunksize=500000)])\ndf_buys = pd.DataFrame(result_items.index.value_counts())\ndf_buys.index.name = \"item_id\"\ndf_buys.columns = ['count']\nval = df_buys['count'].sum()\ndf_buys['popularity'] = df_buys['count'].apply(lambda x : x \/ val )\ndf_buys['popularity'] = df_buys['popularity'].round(5)","ef334cd0":"transformed_data = item_pred.data_transformation(result_4,df_clicks,df_buys)\ntransformed_data.drop(['key_0','item_id_0'], axis=1, inplace=True)","17e20df1":"transformed_data_new = transformed_data.sort_values('session')\nbuys_sessions = np.unique(buys_transformed_data.index)\ntraining_data = transformed_data_new[transformed_data_new['session'].isin(buys_sessions)]","03c72701":"from math import *\nfrom decimal import Decimal \n\ndf = pd.DataFrame(columns=['Predicted_1','Predicted_2'])\n\nfor i in buys_sessions[0:5000]:\n    training_data_input = training_data[training_data['session'] == i]\n    item_pred.predict(training_data,df,i)\ndf = df.fillna(0)","42d581b7":"df_merged_predicted = df.merge(buys_transformed_data['unique_items_bought'],left_on=df.index,right_on=buys_transformed_data.index)\ndf_merged_predicted.columns=['session','Predicted_1','Predicted_2','Unique_Purchased_Items']\ndf_merged_predicted['Predicted_1'] = df_merged_predicted['Predicted_1'].astype(np.int64)\nlist_1=[]\ndf_merged_predicted.apply(item_pred.purchace_item_validation_func,axis=1)","aa058454":"count_T=0\ncount_F=0\nfor i in list_1:\n    if i == 'True':\n        count_T=count_T+1\n    else:\n        count_F=count_F+1","f8ca6b50":"print('Our model could predict :',count_T\/(count_T + count_F),'sample data correctly')","908321ea":"print(\"Prediction Items with session id are as follows:\")\nprint(df)","6630c46d":"HANDLING CLASS IMBALANCE..!!\n\n# -RANDOM UNDERSAMPLING","8cb57993":"# Average Dwell time in seconds for a non buying event.","64efb5cb":"# Predicting the Items customer would be Buying.","cf763805":"# Logistic Regression To Predict the Buy or Not Buy event for a Session.","ce7b96b3":"# Most Popular Days based on Number of sessions.\n[0 - 6] -> [Monday - Sunday]","2193f464":"# Total Count of Buy vs Not Buy events.","929dfbca":"# Average Dwell time in seconds for a buying event.","0a3e864a":"# Random Forest Classifier","01c7110a":"Finding click popularity","54a5cf9b":"Finding Item's buy popularity","66ef9b02":"# Calculating the popularity index for first and last item clicked","82b1b164":"Combining the newly transformed clicks file..!!","e5c9d3e0":"# Imbalance in the class..!!","a2f59cca":"# Best Time of the Day for a buying Event.","19f18f31":"# Gradient Boosting - LightGBM","b9ed64d6":"As the clicks data input file is too big to we would be handling the file in chunks of 500000 rows each","6d05b974":"# Neural Network","9b8e1861":"# Checking for the Probability of First Clicked item and Last Clicked item being Purchased","faefc91b":"# Popular Days for Buying Events based on Number of Sessions.\n[0 - 6] -> [Monday - Sunday]","ce4af734":"Combine the about df with the buys data."}}