{"cell_type":{"1511f94d":"code","5a267f7e":"code","8cc3c827":"code","76030408":"code","bca244bb":"code","8cff5713":"code","17c3310c":"code","38bc3da2":"code","f94b9e4f":"code","7f347020":"code","a677da22":"code","4ac000e3":"code","54b8842f":"code","b5a6412f":"code","f9173eab":"code","b0e26ed7":"code","078563cd":"code","ffca98d0":"code","5b38f35a":"code","0b21715b":"code","7f9f3e9e":"code","1329c39f":"code","b99d9d7f":"code","35f5df7f":"code","4abe1e3f":"code","8b0a05a1":"code","337e64e8":"code","c28e4d19":"code","01ff47ad":"code","bbb1e0cb":"code","18f887e4":"code","6a3510e4":"code","6607ab36":"code","bb431730":"code","2009f97c":"code","6e68c09c":"code","682dbf5a":"code","795a348e":"code","a9f35c31":"code","cda96bd3":"code","acbc24b4":"code","5347c118":"code","52fd8989":"code","6aa8bee7":"code","f45bb44a":"markdown","1b63f127":"markdown","5dada350":"markdown","f6be24e2":"markdown","86249811":"markdown","bfc8d34d":"markdown","eaec044b":"markdown","25c9938a":"markdown","69be969b":"markdown","9982bf53":"markdown","9b6a887b":"markdown","c475e584":"markdown","66713371":"markdown","745a43b0":"markdown","3cd287bf":"markdown","9d3c2d54":"markdown","c13f7e57":"markdown","7a03daa8":"markdown","10f7917c":"markdown","da8c33e2":"markdown","95b906f4":"markdown","bc4cf730":"markdown","76abdf76":"markdown","b33946ef":"markdown","e6bc118b":"markdown","b8509725":"markdown","79b51245":"markdown","e27c592f":"markdown","49050a81":"markdown","fd6326d3":"markdown","f964ce32":"markdown","207a284c":"markdown","df603167":"markdown","0d9fc1f3":"markdown"},"source":{"1511f94d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5a267f7e":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import StratifiedKFold\n\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')\n\nSEED = 42","8cc3c827":"def concat_df(train_data, test_data):\n    # Returns a concatenated df of training and test set on axis 0\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n\ndef divide_df(all_data):\n    # Returns divided dfs of training and test set\n    return all_data.loc[:890], all_data.loc[891:].drop(['Survived'], axis=1)\n\ndf_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\ndf_all = concat_df(df_train, df_test)\n\ndf_train.name = 'Training Set'\ndf_test.name = 'Test Set'\ndf_all.name = 'All Set' \n\ndfs = [df_train, df_test]\n\nprint('Number of Training Examples = {}'.format(df_train.shape[0]))\nprint('Number of Test Examples = {}\\n'.format(df_test.shape[0]))\nprint('Training X Shape = {}'.format(df_train.shape))\nprint('Training y Shape = {}\\n'.format(df_train['Survived'].shape[0]))\nprint('Test X Shape = {}'.format(df_test.shape))\nprint('Test y Shape = {}\\n'.format(df_test.shape[0]))\nprint(df_train.columns)\nprint(df_test.columns)","76030408":"print(df_train.info())\ndf_train.sample(3)","bca244bb":"print(df_test.info())\ndf_test.sample(3)","8cff5713":"def display_missing(df):    \n    for col in df.columns.tolist():          \n        print('{} column missing values: {}'.format(col, df[col].isnull().sum()))\n    print('\\n')\n    \nfor df in dfs:\n    print('{}'.format(df.name))\n    display_missing(df)","17c3310c":"df_all_corr = df_all.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_all_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_all_corr[df_all_corr['Feature 1'] == 'Age']","38bc3da2":"age_by_pclass_sex = df_all.groupby(['Sex', 'Pclass']).median()['Age']\n\nfor pclass in range(1, 4):\n    for sex in ['female', 'male']:\n        print('Median age of Pclass {} {}s: {}'.format(pclass, sex, age_by_pclass_sex[sex][pclass]))\nprint('Median age of all passengers: {}'.format(df_all['Age'].median()))\n\n# Filling the missing values in Age with the medians of Sex and Pclass groups\ndf_all['Age'] = df_all.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))","f94b9e4f":"df_all[df_all['Embarked'].isnull()]","7f347020":"# Filling the missing values in Embarked with S\ndf_all['Embarked'] = df_all['Embarked'].fillna('S')","a677da22":"df_all[df_all['Fare'].isnull()]","4ac000e3":"med_fare = df_all.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\n# Filling the missing value in Fare with the median Fare of 3rd class alone passenger\ndf_all['Fare'] = df_all['Fare'].fillna(med_fare)","54b8842f":"# Creating Deck column from the first letter of the Cabin column (M stands for Missing)\ndf_all['Deck'] = df_all['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n\ndf_all_decks = df_all.groupby(['Deck', 'Pclass']).count().drop(columns=['Survived', 'Sex', 'Age', 'SibSp', 'Parch', \n                                                                        'Fare', 'Embarked', 'Cabin', 'PassengerId', 'Ticket']).rename(columns={'Name': 'Count'}).transpose()\n\ndef get_pclass_dist(df):\n    \n    # Creating a dictionary for every passenger class count in every deck\n    deck_counts = {'A': {}, 'B': {}, 'C': {}, 'D': {}, 'E': {}, 'F': {}, 'G': {}, 'M': {}, 'T': {}}\n    decks = df.columns.levels[0]    \n    \n    for deck in decks:\n        for pclass in range(1, 4):\n            try:\n                count = df[deck][pclass][0]\n                deck_counts[deck][pclass] = count \n            except KeyError:\n                deck_counts[deck][pclass] = 0\n                \n    df_decks = pd.DataFrame(deck_counts)    \n    deck_percentages = {}\n\n    # Creating a dictionary for every passenger class percentage in every deck\n    for col in df_decks.columns:\n        deck_percentages[col] = [(count \/ df_decks[col].sum()) * 100 for count in df_decks[col]]\n        \n    return deck_counts, deck_percentages\n\ndef display_pclass_dist(percentages):\n    \n    df_percentages = pd.DataFrame(percentages).transpose()\n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M', 'T')\n    bar_count = np.arange(len(deck_names))  \n    bar_width = 0.85\n    \n    pclass1 = df_percentages[0]\n    pclass2 = df_percentages[1]\n    pclass3 = df_percentages[2]\n    \n    plt.figure(figsize=(20, 10))\n    plt.bar(bar_count, pclass1, color='#b5ffb9', edgecolor='white', width=bar_width, label='Passenger Class 1')\n    plt.bar(bar_count, pclass2, bottom=pclass1, color='#f9bc86', edgecolor='white', width=bar_width, label='Passenger Class 2')\n    plt.bar(bar_count, pclass3, bottom=pclass1 + pclass2, color='#a3acff', edgecolor='white', width=bar_width, label='Passenger Class 3')\n\n    plt.xlabel('Deck', size=15, labelpad=20)\n    plt.ylabel('Passenger Class Percentage', size=15, labelpad=20)\n    plt.xticks(bar_count, deck_names)    \n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    \n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 15})\n    plt.title('Passenger Class Distribution in Decks', size=18, y=1.05)   \n    \n    plt.show()    \n\nall_deck_count, all_deck_per = get_pclass_dist(df_all_decks)\ndisplay_pclass_dist(all_deck_per)","b5a6412f":"# Passenger in the T deck is changed to A\nidx = df_all[df_all['Deck'] == 'T'].index\ndf_all.loc[idx, 'Deck'] = 'A'","f9173eab":"df_all_decks_survived = df_all.groupby(['Deck', 'Survived']).count().drop(columns=['Sex', 'Age', 'SibSp', 'Parch', 'Fare', \n                                                                                   'Embarked', 'Pclass', 'Cabin', 'PassengerId', 'Ticket']).rename(columns={'Name':'Count'}).transpose()\n\ndef get_survived_dist(df):\n    \n    # Creating a dictionary for every survival count in every deck\n    surv_counts = {'A':{}, 'B':{}, 'C':{}, 'D':{}, 'E':{}, 'F':{}, 'G':{}, 'M':{}}\n    decks = df.columns.levels[0]    \n\n    for deck in decks:\n        for survive in range(0, 2):\n            surv_counts[deck][survive] = df[deck][survive][0]\n            \n    df_surv = pd.DataFrame(surv_counts)\n    surv_percentages = {}\n\n    for col in df_surv.columns:\n        surv_percentages[col] = [(count \/ df_surv[col].sum()) * 100 for count in df_surv[col]]\n        \n    return surv_counts, surv_percentages\n\ndef display_surv_dist(percentages):\n    \n    df_survived_percentages = pd.DataFrame(percentages).transpose()\n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M')\n    bar_count = np.arange(len(deck_names))  \n    bar_width = 0.85    \n\n    not_survived = df_survived_percentages[0]\n    survived = df_survived_percentages[1]\n    \n    plt.figure(figsize=(20, 10))\n    plt.bar(bar_count, not_survived, color='#b5ffb9', edgecolor='white', width=bar_width, label=\"Not Survived\")\n    plt.bar(bar_count, survived, bottom=not_survived, color='#f9bc86', edgecolor='white', width=bar_width, label=\"Survived\")\n \n    plt.xlabel('Deck', size=15, labelpad=20)\n    plt.ylabel('Survival Percentage', size=15, labelpad=20)\n    plt.xticks(bar_count, deck_names)    \n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    \n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 15})\n    plt.title('Survival Percentage in Decks', size=18, y=1.05)\n    \n    plt.show()\n\nall_surv_count, all_surv_per = get_survived_dist(df_all_decks_survived)\ndisplay_surv_dist(all_surv_per)","b0e26ed7":"df_all['Deck'] = df_all['Deck'].replace(['A', 'B', 'C'], 'ABC')\ndf_all['Deck'] = df_all['Deck'].replace(['D', 'E'], 'DE')\ndf_all['Deck'] = df_all['Deck'].replace(['F', 'G'], 'FG')\n\ndf_all['Deck'].value_counts()","078563cd":"# Dropping the Cabin feature\ndf_all.drop(['Cabin'], inplace=True, axis=1)\n\ndf_train, df_test = divide_df(df_all)\ndfs = [df_train, df_test]\n\nfor df in dfs:\n    display_missing(df)","ffca98d0":"survived = df_train['Survived'].value_counts()[1]\nnot_survived = df_train['Survived'].value_counts()[0]\nsurvived_per = survived \/ df_train.shape[0] * 100\nnot_survived_per = not_survived \/ df_train.shape[0] * 100\n\nprint('{} of {} passengers survived and it is the {:.2f}% of the training set.'.format(survived, df_train.shape[0], survived_per))\nprint('{} of {} passengers didnt survive and it is the {:.2f}% of the training set.'.format(not_survived, df_train.shape[0], not_survived_per))\n\nplt.figure(figsize=(10, 8))\nsns.countplot(df_train['Survived'])\n\nplt.xlabel('Survival', size=15, labelpad=15)\nplt.ylabel('Passenger Count', size=15, labelpad=15)\nplt.xticks((0, 1), ['Not Survived ({0:.2f}%)'.format(not_survived_per), 'Survived ({0:.2f}%)'.format(survived_per)])\nplt.tick_params(axis='x', labelsize=13)\nplt.tick_params(axis='y', labelsize=13)\n\nplt.title('Training Set Survival Distribution', size=15, y=1.05)\n\nplt.show()","5b38f35a":"df_train_corr = df_train.drop(['PassengerId'], axis=1).corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_train_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_train_corr.drop(df_train_corr.iloc[1::2].index, inplace=True)\ndf_train_corr_nd = df_train_corr.drop(df_train_corr[df_train_corr['Correlation Coefficient'] == 1.0].index)\n\ndf_test_corr = df_test.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_test_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_test_corr.drop(df_test_corr.iloc[1::2].index, inplace=True)\ndf_test_corr_nd = df_test_corr.drop(df_test_corr[df_test_corr['Correlation Coefficient'] == 1.0].index)","0b21715b":"# Training set high correlations\ncorr = df_train_corr_nd['Correlation Coefficient'] > 0.1\ndf_train_corr_nd[corr]","7f9f3e9e":"# Test set high correlations\ncorr = df_test_corr_nd['Correlation Coefficient'] > 0.1\ndf_test_corr_nd[corr]","1329c39f":"fig, axs = plt.subplots(nrows=2, figsize=(20, 20))\n\nsns.heatmap(df_train.drop(['PassengerId'], axis=1).corr(), ax=axs[0], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\nsns.heatmap(df_test.drop(['PassengerId'], axis=1).corr(), ax=axs[1], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\n\nfor i in range(2):    \n    axs[i].tick_params(axis='x', labelsize=14)\n    axs[i].tick_params(axis='y', labelsize=14)\n    \naxs[0].set_title('Training Set Correlations', size=15)\naxs[1].set_title('Test Set Correlations', size=15)\n\nplt.show()","b99d9d7f":"cont_features = ['Age', 'Fare']\nsurv = df_train['Survived'] == 1\n\nfig, axs = plt.subplots(ncols=2, nrows=2, figsize=(20, 20))\nplt.subplots_adjust(right=1.5)\n\nfor i, feature in enumerate(cont_features):    \n    # Distribution of survival in feature\n    sns.distplot(df_train[~surv][feature], label='Not Survived', hist=True, color='#e74c3c', ax=axs[0][i])\n    sns.distplot(df_train[surv][feature], label='Survived', hist=True, color='#2ecc71', ax=axs[0][i])\n    \n    # Distribution of feature in dataset\n    sns.distplot(df_train[feature], label='Training Set', hist=False, color='#e74c3c', ax=axs[1][i])\n    sns.distplot(df_test[feature], label='Test Set', hist=False, color='#2ecc71', ax=axs[1][i])\n    \n    axs[0][i].set_xlabel('')\n    axs[1][i].set_xlabel('')\n    \n    for j in range(2):        \n        axs[i][j].tick_params(axis='x', labelsize=20)\n        axs[i][j].tick_params(axis='y', labelsize=20)\n    \n    axs[0][i].legend(loc='upper right', prop={'size': 20})\n    axs[1][i].legend(loc='upper right', prop={'size': 20})\n    axs[0][i].set_title('Distribution of Survival in {}'.format(feature), size=20, y=1.05)\n\naxs[1][0].set_title('Distribution of {} Feature'.format('Age'), size=20, y=1.05)\naxs[1][1].set_title('Distribution of {} Feature'.format('Fare'), size=20, y=1.05)\n        \nplt.show()","35f5df7f":"cat_features = ['Embarked', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Deck']\n\nfig, axs = plt.subplots(ncols=2, nrows=3, figsize=(20, 20))\nplt.subplots_adjust(right=1.5, top=1.25)\n\nfor i, feature in enumerate(cat_features, 1):    \n    plt.subplot(2, 3, i)\n    sns.countplot(x=feature, hue='Survived', data=df_train)\n    \n    plt.xlabel('{}'.format(feature), size=20, labelpad=15)\n    plt.ylabel('Passenger Count', size=20, labelpad=15)    \n    plt.tick_params(axis='x', labelsize=20)\n    plt.tick_params(axis='y', labelsize=20)\n    \n    plt.legend(['Not Survived', 'Survived'], loc='upper center', prop={'size': 18})\n    plt.title('Count of Survival in {} Feature'.format(feature), size=20, y=1.05)\n\nplt.show()","4abe1e3f":"df_all = concat_df(df_train, df_test)\ndf_all.head()","8b0a05a1":"df_all['Fare'] = pd.qcut(df_all['Fare'], 13)\n\nfig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Fare', hue='Survived', data=df_all)\n\nplt.xlabel('Fare', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Fare'), size=15, y=1.05)\n\nplt.show()","337e64e8":"df_all['Age'] = pd.qcut(df_all['Age'], 10)\n\nfig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Age', hue='Survived', data=df_all)\n\nplt.xlabel('Age', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Survival Counts in {} Feature'.format('Age'), size=15, y=1.05)\n\nplt.show()","c28e4d19":"df_all['Family_Size'] = df_all['SibSp'] + df_all['Parch'] + 1\n\nfig, axs = plt.subplots(figsize=(20, 20), ncols=2, nrows=2)\nplt.subplots_adjust(right=1.5)\n\nsns.barplot(x=df_all['Family_Size'].value_counts().index, y=df_all['Family_Size'].value_counts().values, ax=axs[0][0])\nsns.countplot(x='Family_Size', hue='Survived', data=df_all, ax=axs[0][1])\n\naxs[0][0].set_title('Family Size Feature Value Counts', size=20, y=1.05)\naxs[0][1].set_title('Survival Counts in Family Size ', size=20, y=1.05)\n\nfamily_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\ndf_all['Family_Size_Grouped'] = df_all['Family_Size'].map(family_map)\n\nsns.barplot(x=df_all['Family_Size_Grouped'].value_counts().index, y=df_all['Family_Size_Grouped'].value_counts().values, ax=axs[1][0])\nsns.countplot(x='Family_Size_Grouped', hue='Survived', data=df_all, ax=axs[1][1])\n\naxs[1][0].set_title('Family Size Feature Value Counts After Grouping', size=20, y=1.05)\naxs[1][1].set_title('Survival Counts in Family Size After Grouping', size=20, y=1.05)\n\nfor i in range(2):\n    axs[i][1].legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 20})\n    for j in range(2):\n        axs[i][j].tick_params(axis='x', labelsize=20)\n        axs[i][j].tick_params(axis='y', labelsize=20)\n        axs[i][j].set_xlabel('')\n        axs[i][j].set_ylabel('')\n\nplt.show()","01ff47ad":"df_all['Ticket_Frequency'] = df_all.groupby('Ticket')['Ticket'].transform('count')\n\nfig, axs = plt.subplots(figsize=(12, 9))\nsns.countplot(x='Ticket_Frequency', hue='Survived', data=df_all)\n\nplt.xlabel('Ticket Frequency', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Ticket Frequency'), size=15, y=1.05)\n\nplt.show()","bbb1e0cb":"df_all['Title'] = df_all['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\ndf_all['Is_Married'] = 0\ndf_all['Is_Married'].loc[df_all['Title'] == 'Mrs'] = 1\n\nfig, axs = plt.subplots(nrows=2, figsize=(20, 20))\nsns.barplot(x=df_all['Title'].value_counts().index, y=df_all['Title'].value_counts().values, ax=axs[0])\n\naxs[0].tick_params(axis='x', labelsize=10)\naxs[1].tick_params(axis='x', labelsize=15)\n\nfor i in range(2):    \n    axs[i].tick_params(axis='y', labelsize=15)\n\naxs[0].set_title('Title Feature Value Counts', size=20, y=1.05)\n\ndf_all['Title'] = df_all['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss\/Mrs\/Ms')\ndf_all['Title'] = df_all['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr\/Military\/Noble\/Clergy')\n\nsns.barplot(x=df_all['Title'].value_counts().index, y=df_all['Title'].value_counts().values, ax=axs[1])\naxs[1].set_title('Title Feature Value Counts After Grouping', size=20, y=1.05)\n\nplt.show()","18f887e4":"def extract_surname(data):    \n    \n    families = []\n    \n    for i in range(len(data)):        \n        name = data.iloc[i]\n\n        if '(' in name:\n            name_no_bracket = name.split('(')[0] \n        else:\n            name_no_bracket = name\n            \n        family = name_no_bracket.split(',')[0]\n        title = name_no_bracket.split(',')[1].strip().split(' ')[0]\n        \n        for c in string.punctuation:\n            family = family.replace(c, '').strip()\n            \n        families.append(family)\n            \n    return families\n\ndf_all['Family'] = extract_surname(df_all['Name'])\ndf_train = df_all.loc[:890]\ndf_test = df_all.loc[891:]\ndfs = [df_train, df_test]","6a3510e4":"# Creating a list of families and tickets that are occuring in both training and test set\nnon_unique_families = [x for x in df_train['Family'].unique() if x in df_test['Family'].unique()]\nnon_unique_tickets = [x for x in df_train['Ticket'].unique() if x in df_test['Ticket'].unique()]\n\ndf_family_survival_rate = df_train.groupby('Family')['Survived', 'Family','Family_Size'].median()\ndf_ticket_survival_rate = df_train.groupby('Ticket')['Survived', 'Ticket','Ticket_Frequency'].median()\n\nfamily_rates = {}\nticket_rates = {}\n\nfor i in range(len(df_family_survival_rate)):\n    # Checking a family exists in both training and test set, and has members more than 1\n    if df_family_survival_rate.index[i] in non_unique_families and df_family_survival_rate.iloc[i, 1] > 1:\n        family_rates[df_family_survival_rate.index[i]] = df_family_survival_rate.iloc[i, 0]\n\nfor i in range(len(df_ticket_survival_rate)):\n    # Checking a ticket exists in both training and test set, and has members more than 1\n    if df_ticket_survival_rate.index[i] in non_unique_tickets and df_ticket_survival_rate.iloc[i, 1] > 1:\n        ticket_rates[df_ticket_survival_rate.index[i]] = df_ticket_survival_rate.iloc[i, 0]","6607ab36":"mean_survival_rate = np.mean(df_train['Survived'])\n\ntrain_family_survival_rate = []\ntrain_family_survival_rate_NA = []\ntest_family_survival_rate = []\ntest_family_survival_rate_NA = []\n\nfor i in range(len(df_train)):\n    if df_train['Family'][i] in family_rates:\n        train_family_survival_rate.append(family_rates[df_train['Family'][i]])\n        train_family_survival_rate_NA.append(1)\n    else:\n        train_family_survival_rate.append(mean_survival_rate)\n        train_family_survival_rate_NA.append(0)\n        \nfor i in range(len(df_test)):\n    if df_test['Family'].iloc[i] in family_rates:\n        test_family_survival_rate.append(family_rates[df_test['Family'].iloc[i]])\n        test_family_survival_rate_NA.append(1)\n    else:\n        test_family_survival_rate.append(mean_survival_rate)\n        test_family_survival_rate_NA.append(0)\n        \ndf_train['Family_Survival_Rate'] = train_family_survival_rate\ndf_train['Family_Survival_Rate_NA'] = train_family_survival_rate_NA\ndf_test['Family_Survival_Rate'] = test_family_survival_rate\ndf_test['Family_Survival_Rate_NA'] = test_family_survival_rate_NA\n\ntrain_ticket_survival_rate = []\ntrain_ticket_survival_rate_NA = []\ntest_ticket_survival_rate = []\ntest_ticket_survival_rate_NA = []\n\nfor i in range(len(df_train)):\n    if df_train['Ticket'][i] in ticket_rates:\n        train_ticket_survival_rate.append(ticket_rates[df_train['Ticket'][i]])\n        train_ticket_survival_rate_NA.append(1)\n    else:\n        train_ticket_survival_rate.append(mean_survival_rate)\n        train_ticket_survival_rate_NA.append(0)\n        \nfor i in range(len(df_test)):\n    if df_test['Ticket'].iloc[i] in ticket_rates:\n        test_ticket_survival_rate.append(ticket_rates[df_test['Ticket'].iloc[i]])\n        test_ticket_survival_rate_NA.append(1)\n    else:\n        test_ticket_survival_rate.append(mean_survival_rate)\n        test_ticket_survival_rate_NA.append(0)\n        \ndf_train['Ticket_Survival_Rate'] = train_ticket_survival_rate\ndf_train['Ticket_Survival_Rate_NA'] = train_ticket_survival_rate_NA\ndf_test['Ticket_Survival_Rate'] = test_ticket_survival_rate\ndf_test['Ticket_Survival_Rate_NA'] = test_ticket_survival_rate_NA","bb431730":"for df in [df_train, df_test]:\n    df['Survival_Rate'] = (df['Ticket_Survival_Rate'] + df['Family_Survival_Rate']) \/ 2\n    df['Survival_Rate_NA'] = (df['Ticket_Survival_Rate_NA'] + df['Family_Survival_Rate_NA']) \/ 2  ","2009f97c":"non_numeric_features = ['Embarked', 'Sex', 'Deck', 'Title', 'Family_Size_Grouped', 'Age', 'Fare']\n\nfor df in dfs:\n    for feature in non_numeric_features:        \n        df[feature] = LabelEncoder().fit_transform(df[feature])","6e68c09c":"cat_features = ['Pclass', 'Sex', 'Deck', 'Embarked', 'Title', 'Family_Size_Grouped']\nencoded_features = []\n\nfor df in dfs:\n    for feature in cat_features:\n        encoded_feat = OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n        n = df[feature].nunique()\n        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n        encoded_df.index = df.index\n        encoded_features.append(encoded_df)\n\ndf_train = pd.concat([df_train, *encoded_features[:6]], axis=1)\ndf_test = pd.concat([df_test, *encoded_features[6:]], axis=1)","682dbf5a":"df_all = concat_df(df_train, df_test)\ndrop_cols = ['Deck', 'Embarked', 'Family', 'Family_Size', 'Family_Size_Grouped', 'Survived',\n             'Name', 'Parch', 'PassengerId', 'Pclass', 'Sex', 'SibSp', 'Ticket', 'Title',\n            'Ticket_Survival_Rate', 'Family_Survival_Rate', 'Ticket_Survival_Rate_NA', 'Family_Survival_Rate_NA']\n\ndf_all.drop(columns=drop_cols, inplace=True)\n\ndf_all.head()","795a348e":"X_train = StandardScaler().fit_transform(df_train.drop(columns=drop_cols))\ny_train = df_train['Survived'].values\nX_test = StandardScaler().fit_transform(df_test.drop(columns=drop_cols))\n\nprint('X_train shape: {}'.format(X_train.shape))\nprint('y_train shape: {}'.format(y_train.shape))\nprint('X_test shape: {}'.format(X_test.shape))","a9f35c31":"single_best_model = RandomForestClassifier(criterion='gini', \n                                           n_estimators=1100,\n                                           max_depth=5,\n                                           min_samples_split=4,\n                                           min_samples_leaf=5,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=SEED,\n                                           n_jobs=-1,\n                                           verbose=1)\n\nleaderboard_model = RandomForestClassifier(criterion='gini',\n                                           n_estimators=1750,\n                                           max_depth=7,\n                                           min_samples_split=6,\n                                           min_samples_leaf=6,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=SEED,\n                                           n_jobs=-1,\n                                           verbose=1) ","cda96bd3":"N = 5\noob = 0\nprobs = pd.DataFrame(np.zeros((len(X_test), N * 2)), columns=['Fold_{}_Prob_{}'.format(i, j) for i in range(1, N + 1) for j in range(2)])\nimportances = pd.DataFrame(np.zeros((X_train.shape[1], N)), columns=['Fold_{}'.format(i) for i in range(1, N + 1)], index=df_all.columns)\nfprs, tprs, scores = [], [], []\n\nskf = StratifiedKFold(n_splits=N, random_state=N, shuffle=True)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n    print('Fold {}\\n'.format(fold))\n    \n    # Fitting the model\n    leaderboard_model.fit(X_train[trn_idx], y_train[trn_idx])\n    \n    # Computing Train AUC score\n    trn_fpr, trn_tpr, trn_thresholds = roc_curve(y_train[trn_idx], leaderboard_model.predict_proba(X_train[trn_idx])[:, 1])\n    trn_auc_score = auc(trn_fpr, trn_tpr)\n    # Computing Validation AUC score\n    val_fpr, val_tpr, val_thresholds = roc_curve(y_train[val_idx], leaderboard_model.predict_proba(X_train[val_idx])[:, 1])\n    val_auc_score = auc(val_fpr, val_tpr)  \n      \n    scores.append((trn_auc_score, val_auc_score))\n    fprs.append(val_fpr)\n    tprs.append(val_tpr)\n    \n    # X_test probabilities\n    probs.loc[:, 'Fold_{}_Prob_0'.format(fold)] = leaderboard_model.predict_proba(X_test)[:, 0]\n    probs.loc[:, 'Fold_{}_Prob_1'.format(fold)] = leaderboard_model.predict_proba(X_test)[:, 1]\n    importances.iloc[:, fold - 1] = leaderboard_model.feature_importances_\n        \n    oob += leaderboard_model.oob_score_ \/ N\n    print('Fold {} OOB Score: {}\\n'.format(fold, leaderboard_model.oob_score_))   \n    \nprint('Average OOB Score: {}'.format(oob))","acbc24b4":"importances['Mean_Importance'] = importances.mean(axis=1)\nimportances.sort_values(by='Mean_Importance', inplace=True, ascending=False)\n\nplt.figure(figsize=(15, 20))\nsns.barplot(x='Mean_Importance', y=importances.index, data=importances)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\nplt.title('Random Forest Classifier Mean Feature Importance Between Folds', size=15)\n\nplt.show()","5347c118":"def plot_roc_curve(fprs, tprs):\n    \n    tprs_interp = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n    f, ax = plt.subplots(figsize=(15, 15))\n    \n    # Plotting ROC for each fold and computing AUC scores\n    for i, (fpr, tpr) in enumerate(zip(fprs, tprs), 1):\n        tprs_interp.append(np.interp(mean_fpr, fpr, tpr))\n        tprs_interp[-1][0] = 0.0\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        ax.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC Fold {} (AUC = {:.3f})'.format(i, roc_auc))\n        \n    # Plotting ROC for random guessing\n    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=0.8, label='Random Guessing')\n    \n    mean_tpr = np.mean(tprs_interp, axis=0)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n    std_auc = np.std(aucs)\n    \n    # Plotting the mean ROC\n    ax.plot(mean_fpr, mean_tpr, color='b', label='Mean ROC (AUC = {:.3f} $\\pm$ {:.3f})'.format(mean_auc, std_auc), lw=2, alpha=0.8)\n    \n    # Plotting the standard deviation around the mean ROC Curve\n    std_tpr = np.std(tprs_interp, axis=0)\n    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label='$\\pm$ 1 std. dev.')\n    \n    ax.set_xlabel('False Positive Rate', size=15, labelpad=20)\n    ax.set_ylabel('True Positive Rate', size=15, labelpad=20)\n    ax.tick_params(axis='x', labelsize=15)\n    ax.tick_params(axis='y', labelsize=15)\n    ax.set_xlim([-0.05, 1.05])\n    ax.set_ylim([-0.05, 1.05])\n\n    ax.set_title('ROC Curves of Folds', size=20, y=1.02)\n    ax.legend(loc='lower right', prop={'size': 13})\n    \n    plt.show()\n\nplot_roc_curve(fprs, tprs)","52fd8989":"class_survived = [col for col in probs.columns if col.endswith('Prob_1')]\nprobs['1'] = probs[class_survived].sum(axis=1) \/ N\nprobs['0'] = probs.drop(columns=class_survived).sum(axis=1) \/ N\nprobs['pred'] = 0\npos = probs[probs['1'] >= 0.5].index\nprobs.loc[pos, 'pred'] = 1\n\ny_pred = probs['pred'].astype(int)","6aa8bee7":"submission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission_df['PassengerId'] = df_test['PassengerId']\nsubmission_df['Survived'] = y_pred.values\nsubmission_df.to_csv('submissions.csv', header=True, index=False)\nsubmission_df.head(10)","f45bb44a":"Family_Survival_Rate\uc740 training set\uc758 \uac00\uc871\uc5d0\uc11c \uacc4\uc0b0\ud55c \uac83\uc774\ub2e4. test set\uc5d0\ub294 Survived \ud56d\ubaa9\uc774 \uc5c6\uae30 \ub54c\ubb38\uc774\ub2e4.\ntraining set\uacfc test set \ub458\ub2e4\uc5d0\uc11c \ubcf4\uc774\ub294 \uac00\uc871\uc774\ub984\uc758 \ub9ac\uc2a4\ud2b8(non_unique_families)\ub97c \ub9cc\ub4e4\uc5c8\ub2e4. \uc774 \ub9ac\uc2a4\ud2b8\uc5d0\uc11c \uad6c\uc131\uc6d0\uc774 1\uba85\ubcf4\ub2e4 \ub9ce\uc740 \uac00\uc871\ub4e4\uc758 \uc0dd\uc874\ub960\uc744 \uacc4\uc0b0\ud588\uace0, Family_Survival_Rate \ud56d\ubaa9\uc5d0 \uc800\uc7a5\ud558\uc600\ub2e4.\n\ucd5c\ub300 \uacbd\uacc4\uc120\uc778 Family_Survival_Rate_NA\ub294 test set\uc5d0\uc11c\ub9cc \ubcf4\uc774\ub294 \ud2b9\uc774\ud55c \uac00\uc871\ub4e4\uc744 \uc704\ud574 \ub9cc\ub4e4\uc5c8\ub2e4. \uc774 \ud56d\ubaa9 \uc5ed\uc2dc \ud544\uc694\ud55c\ub370 \uadf8 \uc774\uc720\ub294 \uc774\ub7f0 \uac00\uc871\ub4e4\uc758 \uc0dd\uc874\ub960\uc744 \uacc4\uc0b0\ud560 \ubc29\ubc95\uc774 \uc5c6\uae30 \ub54c\ubb38\uc774\ub2e4. \uc774 \ud56d\ubaa9\uc740 \uadf8\ub4e4\uc758 \uc0dd\uc874\ub960\uc744 \uc54c\uc544\ub0bc \ubc29\ubc95\uc774 \uc5c6\uae30\ub54c\ubb38\uc5d0 \uac00\uc871\uc0dd\uc874\ub960\uc774 \uadf8\ub4e4\uc5d0\uac8c \uc801\uc6a9\ub418\uc9c0 \uc54a\uc74c\uc744 \uc54c\ub824\uc900\ub2e4.\nTicket_Survival_Rate\uacfc Ticket_Survival_Rate_NA \ud56d\ubaa9 \uc5ed\uc2dc \uac19\uc740 \ubc29\ubc95\uc73c\ub85c \ub9cc\ub4e4\uc5c8\ub2e4.\nTicket_Survival_Rate\uacfc Family_Survival_Rate\uc744 \ud3c9\uade0\ub0b4\uc5b4 Survival_Rate\uc774 \ub418\uace0, Ticket_Survival_Rate_NA\uc640 Family_Survival_Rate_NA \ub610\ud55c \ud3c9\uade0\ub0b4\uc5b4 Survival_Rate_NA\uac00 \ub41c\ub2e4.","1b63f127":"1.2.4 cabin\n\nCabin \ud56d\ubaa9\uc740 \uc880 \uc560\ub9e4\ud574\uc11c \ub354 \uae4a\uc740 \ud0d0\uad6c\uac00 \ud544\uc694\ud558\ub2e4. Cabin\uac12\uc774 \ub9ce\uc774 \uc5c6\ub294\ub370, \uc5b4\ub5a4 \uce90\ube48\uc5d0\uc11c \ub354 \ub192\uc740 \uc0dd\uc874\ub960\uc744 \ubcf4\uc77c \uc218 \uc788\uc73c\ubbc0\ub85c \uc644\uc804\ud788 \ubb34\uc2dc\ud560 \uc21c \uc5c6\ub2e4. Cabin\uc758 \uccab\uae00\uc790\ub294 \uce90\ube48\uc774 \uc704\uce58\ud55c \uac11\ud310\uc73c\ub85c \ub4dc\ub7ec\ub0ac\ub2e4. \uc774 \uac11\ud310\uc740 \uc8fc\ub85c \ud63c\uc790\uc778 \uc2b9\uac1d\ub4e4\uc5d0\uac8c \uc8fc\uc5b4\uc84c\uc9c0\ub9cc \uadf8\ub4e4 \uc911 \uc77c\ubd80\ub294 \ub2e4\uc778\uc6a9\uac1d\uc2e4\uc744 \uc774\uc6a9\ud588\ub2e4.\nboat deck\uc5d0\ub294 T, U, W, X, Y, Z\ub85c \uc774\ub984\ubd99\uc5ec\uc9c4 6\uac1c\uc758 \ubc29\uc774 \uc788\uc5c8\ub2e4. \uadf8\ub7ec\ub098 T\ub9cc\uc774 \ub370\uc774\ud130\uc14b\uc5d0 \ub0a8\uc544\uc788\ub2e4.\nA, B, C\ub294 1st class \uc2b9\uac1d\uc5d0\uac8c\ub9cc \uc8fc\uc5b4\uc84c\ub2e4.\nD, E\ub294 \ubaa8\ub4e0 class\ub97c \uc704\ud55c \uac83\uc774\uc5c8\ub2e4.\nF\uc640 G\ub294 2nd and 3rd class\ub97c \uc704\ud55c \ubc29\uc774\uc5c8\ub2e4.\nA\uc5d0\uc11c G\ub85c \uac08\uc218\ub85d \uacc4\ub2e8\uc73c\ub85c\uae4c\uc9c0\uc758 \uac70\ub9ac\uac00 \uc810\uc810 \uba40\uc5b4\uc9c4\ub2e4. \uc989 \uc0dd\uc874\uc5d0 \uc601\ud5a5\uc744 \uc904 \uc218 \uc788\ub2e4. \uc774\uac83\uc774 \uc0ac\uc2e4\uc774\ub77c\uba74 \uc0c8\ub85c\uc6b4 Deck \ud56d\ubaa9\uc740 \uc21c\uc11c\ub97c \ub098\ud0c0\ub0bc \uc218 \uc788\ub2e4.(ordinal)","5dada350":"2.1.2 age\n\nAge \ud56d\ubaa9\uc740 \uba87\uac1c\uc758 spikes\uc640 bumps \uac00\uc9c4 \uc815\uaddc\ubd84\ud3ec\ub97c \ub098\ud0c0\ub0b8\ub2e4.\n10\uad6c\uac04\uc73c\ub85c \ub098\ub234\ub2e4.\n\uccab\ubc88\uc9f8 \uad6c\uac04\uc740 \uc81c\uc77c \ub192\uc740 \uc0dd\uc874\ub960\uc744 \ubcf4\uc600\uc73c\uba70 4\ubc88\uc9f8 \uad6c\uac04\uc5d0\uc11c \uc81c\uc77c \ub0ae\uc740 \uc0dd\uc874\ub960\uc744 \ubcf4\uc600\ub2e4. \uc774 \ub450\uac1c\uac00 \uc81c\uc77c \ud070 spikes\ub97c \ubcf4\uc778\ub2e4.\n(34.0, 40.0)\uc5d0\uc11c \ub2e4\ub978 \uadf8\ub8f9\ub4e4\uacfc \ub2e4\ub974\uac8c \ub192\uc740 \uc0dd\uc874\ub960\uc744 \ubcf4\uc774\uace0 \uc788\ub2e4.","f6be24e2":"\n\n3.1 models\n\n2\uac1c\uc758 FandomForestClassifier\ub97c \ub9cc\ub4e0\ub2e4. \ud558\ub098\ub294 single model\uc774\uace0 \ub2e4\ub978 \ud558\ub098\ub294 k-fold cross validation\uc774\ub2e4.\nsingle_best_model\uc758 \uc81c\uc77c \ub192\uc740 \uc815\ud655\ub3c4\ub294 0.82775\uc774\ub2e4. \ud558\uc9c0\ub9cc k-fold cross validation\uc5d0\uc11c \ub354 \uc798 \uc791\ub3d9\ub418\uc9c4 \uc54a\ub294\ub2e4(it doesn't perform better in k-fold cross validation). experimenting\uacfc hyperparameter tuning\uc744 \uc2dc\uc791\ud558\ub294 \ub370\uc5d0\ub294 \uc88b\uc740 \ubaa8\ub378\uc774\ub2e4.\n5-fold cross validation\uc5d0\uc11c leaderboard_model\uc758 \uc81c\uc77c \ub192\uc740 \uc815\ud655\ub3c4\ub294 0.83732\uc774\ub2e4. \uc774 \ubaa8\ub378\uc740 leaderboard score(LB score)\ub97c \uc704\ud574 \ub9cc\ub4e4\uc5c8\uc73c\uba70 \uc0b4\uc9dd overfit\ub418\uac8c \uc870\uc815\ud588\ub2e4(it is tuned to overfit slightly). \ubaa8\ub4e0 fold\uc5d0\uc11c X_test\uc758 \uc608\uc0c1\ub41c \ud655\ub960\uc774 N(fold \ub418\ub294 \ud69f\uc218)\ub85c \ub098\ub220\uc9c0\uae30 \ub54c\ubb38\uc5d0 overfit\ud558\uac8c \uc870\uc815\ud588\ub2e4. \ub9cc\uc57d \uc774 \ubaa8\ub378\uc774 single model\ub85c \uc0ac\uc6a9\ub41c\ub2e4\uba74 \ub9ce\uc740 \uc0d8\ud50c\ub4e4\uc744 \ub9de\uac8c \uc608\uce21\ud558\ub294\ub370 \uc5b4\ub824\uc6c0\uc774 \uc788\uc5c8\uc744 \uac83\uc774\ub2e4.\n\uc5b4\ub5a4 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud574\uc57c \ud560\uae4c?\nleaderboard_model\uc740 test set\uc744 overfit\ud588\ub2e4. \ub530\ub77c\uc11c \uc2e4\uc81c \ud504\ub85c\uc81d\ud2b8\uc5d0\uc120 \uc4f0\uc9c0 \uc54a\uae30\ub97c \uad8c\ud55c\ub2e4.\nsingle_best_model\uc740 experimenting\ub97c \uc2dc\uc791\ud558\uace0 \uacb0\uc815\ud2b8\ub9ac\ub97c \ubc30\uc6b0\ub294\ub370 \uc88b\uc740 \ubaa8\ub378\uc774\ub2e4.\n","86249811":"\ub354 \uc815\ud655\ud558\uac8c \ud558\ub824\uba74 Sex \ud56d\ubaa9\uc744 groupby\uc758 \ub450\ubc88\uc9f8 level\ub85c \uc0ac\uc6a9\ud558\ub294 \uac83\uc774\ub2e4. \ubc11\uc5d0\uc11c \ubcf4\uc774\ub294 \uac83 \ucc98\ub7fc Pclass \uc640 Sex \uadf8\ub8f9\uc740 \ub2e4\ub978 \uc911\uac04 \ub098\uc774\uac12\uc744 \uac16\ub294\ub2e4. \uc2b9\uac1d\uc758 \ud074\ub798\uc2a4\uac00 \uc62c\ub77c\uac08\uc218\ub85d \ub0a8\uc790\uc640 \uc5ec\uc790\uc758 \uc911\uac04 \ub098\uc774\uac12\ub3c4 \ub3d9\uc2dc\uc5d0 \uc99d\uac00\ud55c\ub2e4. \uadf8\ub7ec\ub098, \uc5ec\uc790\ub294 \ub0a8\uc790\ubcf4\ub2e4 \uc57d\uac04 \ub0ae\uc740 \uc911\uac04 \ub098\uc774\uac12\uc744 \uac16\ub294 \uacbd\ud5a5\uc774 \uc788\ub2e4. Age \ud56d\ubaa9\uc740 Sex \uc640 Pclass \ud56d\ubaa9\uc744 \uace0\ub824\ud55c \uc911\uac04\uac12\uc73c\ub85c \uacb0\uce21\uac12\uc744 \ucc44\uc6e0\ub2e4.","bfc8d34d":"\ub0b4\uac00 \uc608\uc0c1\ud588\ub358 \ub300\ub85c \ubaa8\ub4e0 \uac11\ud310\uc740 \ub2e4\ub978 \uc0dd\uc874\ub960\uc744 \uac16\uace0\uc788\uc73c\uba70 \uc774 \uc815\ubcf4\ub294 \ubc84\ub9b4 \uc218 \uc5c6\ub2e4. B, C, D, E\ub294 \ub354 \ub192\uc740 \uc0dd\uc874\ub960\uc744 \uac16\uace0\uc788\uc73c\uba70 \uc8fc\ub85c 1st class\uac00 \ucc28\uc9c0\ud558\uace0 \uc788\ub2e4. M\uc740 \uac00\uc7a5 \ub0ae\uc740 \uc0dd\uc874\ub960\uc744 \ubcf4\uc600\ub294\ub370 \uc774\ub294 2nd\uc640 3rd class\uac00 \uc8fc\ub85c \ucc28\uc9c0\ud558\uace0 \uc788\uc5c8\ub2e4. \ub0b4 \uc0dd\uac01\uc5d0 M\uc774 \uac00\uc7a5 \ub0ae\uc740 \uc0dd\uc874\ub960\uc744 \ubcf4\uc774\ub294 \uac74 \ud53c\ud574\uc790\uc758 \uce90\ube48 \ub370\uc774\ud130\ub97c \ucc3e\uc9c0 \ubabb\ud574\uc11c\uc778 \uac83 \uac19\ub2e4. \uadf8\ub798\uc11c \ub09c M\uc774\ub77c\uace0 \uc774\ub984\uc9d3\ub294\uac8c \uacb0\uce21\uac12\uc744 \ub2e4\ub8e8\ub294 \ub370 \ud569\ub9ac\uc801\uc778 \ubc29\ubc95\uc774\ub77c\uace0 \uc0dd\uac01\ud55c\ub2e4. M\uc740 \uac19\uc740 \uc131\uc9c8\uc744 \uac00\uc9c4 \ud2b9\ubcc4\ud55c \uadf8\ub8f9\uc774\ub2e4. Deck \ud56d\ubaa9\uc740 \uc774\uc81c \ub192\uc740 \ucc28\ub840\uc131(cardinality)\uc744 \uac00\uc84c\uc73c\uba70 \uadf8\ub4e4\uc758 \ub2ee\uc740 \uc815\ub3c4\uc5d0 \ub9de\ucdb0 \uadf8\ub8f9\ud654\ub418\uc5c8\ub2e4.\n\nA, B, C\ub294 ABC\ub77c\uace0 \uc774\ub984\ubd99\uc600\ub2e4. \uc65c\ub0d0\ud558\uba74 \uadf8\ub4e4\uc740 \ubaa8\ub450 1st class\ub2c8\uae4c.\nD, E\ub294 DE\ub77c\uace0 \uc774\ub984\ubd99\uc600\ub2e4. \uc65c\ub0d0\ud558\uba74 \ube44\uc2b7\ud55c \uc2b9\uac1d class \ubd84\ud3ec\uc640 \uac19\uc740 \uc0dd\uc874\ub960\uc744 \ubcf4\uc774\ub2c8\uae4c.\nF, G\ub294 FG\ub77c\uace0 \ubd99\uc600\ub2e4. \uc55e\uc758 \uc774\uc720\uc640 \uac19\ub2e4.\nM\uc740 \ub2e4\ub978 \uac11\ud310\ub4e4\uacfc \uadf8\ub8f9\uc9c0\uc5b4\uc9c8 \ud544\uc694\uac00 \uc5c6\ub2e4. \uadf8\ub4e4\uacfc \ub9e4\uc6b0 \ub2e4\ub974\uace0 \ub0ae\uc740 \uc0dd\uc874\ub960\uc744 \ubcf4\uc774\ub2c8\uae4c.","eaec044b":"3.2.2. ROC Curve","25c9938a":"1. exploratory data analysis\n\n1.1 overview\n\nPassengerId \ud56d\ubaa9\uc740 \ud2b9\ubcc4\ud55c id\uc758 \uc5f4\uc774\uc9c0\ub9cc Survived\uc5d4 \uc601\ud5a5\uc744 \uc8fc\uc9c0 \uc54a\ub294\ub2e4.\nSurvived\ub294 0\uacfc 1\ub85c \uad6c\ubd84\ub418\uc5b4\uc788\ub2e4. 1\uc774 \uc0b4\uc544\ub0a8\uc740 \uc0ac\ub78c\uc774\ub2e4.\nPclass(Passenger Class) \ud56d\ubaa9\uc740 \uc2b9\uac1d\uc758 \uc0ac\ud68c,\uacbd\uc81c\uc801 \uc9c0\uc704\ub2e4. 3\uac00\uc9c0\ub85c \uad6c\ubd84\ub418\ub294 \uce74\ud14c\uace0\ub9ac\uc2dd \ud56d\ubaa9\uc774\ub2e4. 1\uc740 upper 2\ub294 middle 3\uc740 lower class\uc774\ub2e4.\nName, Sex and Age \ud56d\ubaa9\uc740 \uc124\uba85\uc744 \uc704\ud55c \ud56d\ubaa9\uc774\ub2e4.\nSibSp \ud56d\ubaa9\uc740 \uc2b9\uac1d\ub4e4\uc758 \uc790\ub9e4\uc640 \ubc30\uc6b0\uc790 \ucd1d \uba85\uc218 \uc774\ub2e4.\nTicket \ud56d\ubaa9\uc740 \uc2b9\uac1d\ub4e4\uc758 \ud2f0\ucf13\ub118\ubc84\uc774\ub2e4.\nFare \ud56d\ubaa9\uc740 \uc2b9\uac1d\ub4e4\uc774 \uc9c0\ubd88\ud55c \uc694\uae08\uc774\ub2e4.\nCabin \ud56d\ubaa9\uc740 \uc2b9\uac1d\ub4e4\uc758 \uce90\ube48\ub118\ubc84\uc774\ub2e4.\nEmbarked \ud56d\ubaa9\uc740 \ud56d\uad6c\ud0d1\uc2b9\uc704\uce58\ub2e4. 3\uac00\uc9c0\ub85c \uad6c\ubd84\ub418\ub294 \uce74\ud14c\uace0\ub9ac\uc2dd \ud56d\ubaa9\uc774\ub2e4. C\ub294 \uc138\ub974\ubd80\ub974, Q\ub294 \ud038\uc2a4\ud0c0\uc6b4, S\ub294 \uc0ac\uc6b0\uc0d8\ud504\ud134\uc774\ub2e4.","69be969b":"0.1. Libraries","9982bf53":"0.2 \ub370\uc774\ud130\uc14b \ub85c\ub529\n\ntraining set\uc740 891\uc5f4 test set\uc740 418\uc5f4\ntraining set\uc740 12\uac00\uc9c0 \ud56d\ubaa9 test set\uc740 11\uac00\uc9c0 \ud56d\ubaa9\ntraining set\uc758 \ud55c\uac00\uc9c0 \ub354 \ub9ce\uc740 \ud56d\ubaa9\uc740 \uc0b4\uc544\ub0a8\uc740 \uc0ac\ub78c(Survived)\uc774\uace0 1\uc5f4\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\ub2e4.","9b6a887b":"1.5 survival distribution in features\n\n1.5.1 continuous features\n\n\uc5f0\uc18d\uc801\uc778 \ud56d\ubaa9\uc778 Age\uc640 Fare \ubaa8\ub450 \uc88b\uc740 split points\ub97c \uac00\uc9c0\uba70 \uacb0\uc815\ud2b8\ub9ac \uc54c\uace0\ub9ac\uc998\uc758 \ubd84\uae30\uc810\uc774 \ub41c\ub2e4.(spikes for a decision tree algorithm to learn)\nAge\uc758 \ubd84\ud3ec\ub294 15\uc0b4 \uc774\uc804\uc758 \uc544\uc774\ub4e4\uc740 \ub2e4\ub978 \uc5b4\ub5a4 \ub098\uc774\ub300\uc758 \uadf8\ub8f9\ubcf4\ub2e4 \ub192\uc740 \uc0dd\uc874\ub960\uc744 \ubcf4\uc778\ub2e4.\nFare\uc758 \ubd84\ud3ec\ub294 \ubd84\ud3ec \ub05d\ucabd(tails)\uc5d0\uc11c \ub192\uc740 \uc0dd\uc874\ub960\uc744 \ubcf4\uc778\ub2e4. \ubd84\ud3ec\ub294 \ub610\ud55c \uc624\ub978\ucabd\uc5d0 \uaf2c\ub9ac\ub97c \uac00\uc9c4(\uc67c\ucabd\uc5d0 \ub370\uc774\ud130\uac00 \ub9ce\uc740)(positive skew) \ubaa8\uc2b5\uc744 \ub098\ud0c0\ub0b4\ub294\ub370 \uc774\ub294 \uadf9\ub3c4\ub85c \ubc97\uc5b4\ub098\ub294 \uac12\uc774 \uc788\uae30 \ub54c\ubb38\uc774\ub2e4.(extreme outliers)\n\ub450 \ud56d\ubaa9\uc5d0\uc11c \uc77c\uc5b4\ub0a0 \uc218 \uc788\ub294 \ubb38\uc81c\ub294 training set\uacfc test set\uc5d0\uc11c\uc758 \ubd84\ud3ec\uac00 \uc870\uae08 \ub2ec\ub77c\uc11c \uba38\uc2e0\ub7ec\ub2dd \uc54c\uace0\ub9ac\uc998\uc774 test set\uc744 \uc77c\ubc18\ud654\ud558\uc9c0 \ubabb\ud560 \uac83\uc774\ub77c\ub294 \uac83\uc774\ub2e4.","c475e584":"0. Introduction\n\n\uadf8\ub8f9\uc9c0\uc5b4 \ub9ce\uc740 \ud56d\ubaa9\ub4e4(features)\uc744 \ub9cc\ub4e4\uc5b4\ub0b4\uace0 \uc559\uc0c1\ube14 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud55c\ub2e4.\n\n\ub098\ub294 \uc774 \ucee4\ub110\uc744 \uc4f0\uae30\ub85c \ud588\ub294\ub370 \uadf8 \uc774\uc720\ub294 Titanic: Machine Learning from Disaster\uac00 \uce90\uae00\uc5d0\uc11c \ub0b4\uac00 \uc88b\uc544\ud558\ub294 \ub300\ud68c \uc911 \ud558\ub098\uc774\uae30 \ub54c\ubb38\uc774\ub2e4. \uc774\uac83\uc740 Exploratory Data Analysis and Feature Engineering\uc5d0 \uc911\uc810\uc744 \ub454 \uae30\ubcf8\uc801\uc778 \ucee4\ub110\uc774\ub2e4. \ub9ce\uc740 \uc0ac\ub78c\ub4e4\uc774 \uc774\uac83\uc73c\ub85c \uce90\uae00\uc744 \uc2dc\uc791\ud558\uace0 \ub9e4\uc6b0 \uae34 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0 \uae38\uc744 \uc783\ub294\ub2e4. \uc774\uac74 \ub2e4\ub978 \uac83\uc5d0 \ube44\ud558\uba74 \uc9e7\uc740 \ucee4\ub110\uc774\ub2e4. \ub09c \ub0b4 \uae00\uc774 \uc2dc\uc791\ud558\ub294 \uc0ac\ub78c\ub4e4\uc5d0\uac8c \uac00\uc774\ub4dc\uac00 \ub418\uc5b4 \uadf8\ub4e4\uc774 Feature engineering\uc5d0 \ub354 \ud765\ubbf8\uac00 \uc0dd\uae30\uae38 \ubc14\ub780\ub2e4.\n\nTitanic: Machine Learning from Disaster\ub294 feature engineering\uc5d0 \ub300\ud55c \uc9c0\uc2dd\uc744 \uc801\uc6a9\ud560 \uc218 \uc788\ub294 \uc88b\uc740 \ub300\ud68c\uc5ec\uc11c \ud0c0\uc774\ud0c0\ub2c9\uc5d0 \ub300\ud574 \ub9ce\uc774 \uc5f0\uad6c\ud558\uace0 \uacf5\ubd80\ud588\ub2e4. \ud0c0\uc774\ud0c0\ub2c9 \ub370\uc774\ud130\uc14b\uc5d0\ub294 \ub9ce\uc740 \ube44\ubc00\ub4e4\uc774 \uc788\ub2e4. \ub098\ub294 \uadf8 \ube44\ubc00\ub4e4 \uc911 \uc2b9\uac1d\uc758 \uc0dd\uc874\uc5d0 \uc601\ud5a5\uc744 \ub07c\uce58\ub294 \uba87\uac00\uc9c0 \uc694\uc18c\ub97c \ucc3e\uc558\ub2e4. \ub09c \uc544\uc9c1 \ub354 \ubc1c\uacac\ub420 \uac83\ub4e4\uc774 \ub9ce\ub2e4\uace0 \ubbff\uace0\uc788\ub2e4.\n\n\uc774 \ucee4\ub110\uc5d4 3\uac1c\uc758 \uba54\uc778 \ud30c\ud2b8\uac00 \uc788\ub2e4;  Exploratory Data Analysis, Feature Engineering and Machine Learning, \uadf8\ub9ac\uace0 \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8 \ubd84\ub958\uae30\ub97c \uc880\ub354 \uc190\ubcf4\uba74 \uc0c1\uc704 3%\uae4c\uc9c0 \ub2ec\uc131\ud560 \uc218 \uc788\ub2e4. \uc774\ub294 60\ucd08\ubc16\uc5d0 \uc548\uac78\ub9b0\ub2e4. \ub09c \uacc4\uc18d \uc774 \ucee4\ub110\uc744 \ub2e4\uc2dc \ub4e4\uc5b4\uc640\ubcf4\uace0 \uc788\uc73c\ub2c8 \uc774 \ucee4\ub110\uc744 \ud5a5\uc0c1\uc2dc\ud0ac \uc544\uc774\ub514\uc5b4\uac00 \uc788\uc73c\uba74 \ucf54\uba58\ud2b8 \ub0a8\uaca8\uc8fc\uac70\ub098 \uc2dc\ub3c4\ud574\ubd10\ub77c. \uc774\ud574\uc548\ub418\ub294 \ubd80\ubd84\uc774 \uc788\uc73c\uba74 \ud3b8\ud558\uac8c \ubb3c\uc5b4\ubd10\ub77c.","66713371":"3.3. Submission","745a43b0":"Ticket \uac12\uc740 \ud2b9\ubcc4\ud55c \uac12\uc774 \ub108\ubb34 \ub9ce\uae30\uc5d0 \uadf8\ub4e4\uc758 \ube48\ub3c4\ub85c \uadf8\ub8f9\ud654\ud558\ub294\uac8c \uc77c\ucc98\ub9ac\uac00 \ub354 \uc26c\uc6cc\uc9c4\ub2e4.\nFamily_Size\uc640 \uc5b4\ub5a4 \uc810\uc774 \ub2e4\ub978\uac00? \ub9ce\uc740 \uc2b9\uac1d\ub4e4\uc774 \uadf8\ub8f9\uc73c\ub85c \uc5ec\ud589\ud558\uc600\ub2e4. \uadf8 \uadf8\ub8f9\uc5d0\ub294 \uce5c\uad6c, \uc720\ubaa8, \uba54\uc774\ub4dc \ub4f1\uc774 \ud3ec\ud568\ub41c\ub2e4. \uadf8\ub4e4\uc740 \uac00\uc871\uc73c\ub85c \uc138\uc5b4\uc9c0\uc9c4 \uc54a\uc558\uc9c0\ub9cc \uac19\uc740 \ud2f0\ucf13\uc744 \uac16\uace0\uc788\uae34 \ud588\ub2e4.\n\uc65c \ud2f0\ucf13\uc744 \uc811\ub450\uc0ac\ub85c \uadf8\ub8f9\ud654\ud558\uc9c0 \uc54a\uc558\ub294\uac00? \ub9cc\uc57d \ud2f0\ucf13\uc758 \uc811\ub450\uc0ac\uac00 \ud2b9\ubcc4\ud55c \uc758\ubbf8\ub97c \uac00\uc84c\ub2e4\uba74 Pclass\ub098 Embarked \ud56d\ubaa9\uc5d0\uc11c \ubc1c\uacac\ub410\uc744\uac83\uc774\ub2e4. \uc774 \ub450\ud56d\ubaa9\ub9cc\uc774 Ticket \ud56d\ubaa9\uc5d0\uc11c \uc774\ub04c\uc5b4\ub0bc \uc218 \uc788\ub294 \ub17c\ub9ac\uc801\uc778 \uc815\ubcf4\uc774\uae30 \ub54c\ubb38\uc774\ub2e4.\n\uadf8\ub798\ud504\ub97c \ubcf4\uba74 2, 3, 4\uba85\uc758 \uadf8\ub8f9\uc774 \ub192\uc740 \uc0dd\uc874\ub960\uc744 \ubcf4\uc778\ub2e4. \ud63c\uc790 \uc5ec\ud589\ud55c \uc0ac\ub78c\uc740 \ub0ae\uc740 \uc0dd\uc874\ub960\uc744 \ubcf4\uc778\ub2e4. 4\uba85 \uc774\uc0c1\uc758 \uadf8\ub8f9\uc740 \uc0dd\uc874\ub960\uc774 \uae09\uaca9\ud558\uac8c \ub35c\uc5b4\uc9c4\ub2e4. \uc774 \ud328\ud134\uc740 Family_Size\uc640 \ub9e4\uc6b0 \ube44\uc2b7\ud558\uc9c0\ub9cc \uc870\uae08 \ub2e4\ub974\uae30\ub3c4 \ud558\ub2e4.\nTicket_Frequency \uac12\uc740 Family_Size\ucc98\ub7fc \uadf8\ub8f9\ud654\ud558\uc9c0 \uc54a\ub294\ub2e4. \uadf8\ub807\uac8c\ud558\uba74 \uc644\ubcbd\ud55c \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\uc774\ub294 \uac19\uc740 \ud56d\ubaa9\uc774 \ub098\uc624\uae30 \ub54c\ubb38\uc774\ub2e4. \uc774\ub7f0 \ud56d\ubaa9\uc740 \uc5b4\ub5a4 \uc815\ubcf4\ud68d\ub4dd\ub3c4 \uc8fc\uc9c0 \uc54a\ub294\ub2e4.","3cd287bf":"2.4 survival rate\n\n\uc774 \ud56d\ubaa9\uc758 \uccab \ud30c\ud2b8\uc778 Family_Survival_Rate\ub294 @volhaleusha\uc758 Titanic: Tutorial, Encoding, Feature Eng, 81.8%\uc758 \ucee4\ub110\uacfc \uacf5\uc720\ud588\ub2e4. \uc774\ub294 LB score\ub97c \ub208\uc5d0 \ub744\uac8c \ub192\uc600\ub2e4. \uadf8\ub7ec\ub098 \uc774\ub294 \uadf8\ub8f9\uc73c\ub85c \uc5ec\ud589\ud558\ub294 \uc2b9\uac1d\ub4e4\uc744 \uace0\ub824\uc0ac\ud56d\uc5d0 \ub123\uc9c4 \uc54a\uc558\ub2e4. \uc774 \uadf8\ub8f9\ub4e4\uc740 \uac00\uc871\uc774 \ub420 \ud544\uc694\ub294 \uc5c6\ub2e4. \uc774 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574\uc11c Ticket_Survival_Rate\uc740 Family_Survival_Rate\uc73c\ub85c \uacc4\uc0b0\ub418\uace0 \ud3c9\uade0\ub0b4\uc5b4\uc838\uc57c \ud55c\ub2e4.\nextract_surname \ud568\uc218\ub294 Name \ud56d\ubaa9\uc5d0\uc11c \uc2b9\uac1d\ub4e4\uc758 \uc131\uc744 \uac00\uc838\uc624\uae30 \uc704\ud574 \uc0ac\uc6a9\ud55c\ub2e4. Family \ud56d\ubaa9\uc740 \uac00\uc838\uc628 \uc131\uc744 \uac00\uc9c0\uace0 \ub9cc\ub4e4\uc5b4\uc9c4\ub2e4. \uc774 \uacfc\uc815\uc740 \uc2b9\uac1d\uc744 \uac00\uc871\uc73c\ub85c \ubb36\uae30 \uc704\ud574 \ud544\uc694\ud558\ub2e4.","9d3c2d54":"2.3 title & is married\n\nTitle\uc740 Name \ud56d\ubaa9 \uc55e\ubd80\ubd84\uc758 \uc811\ub450\uc0ac\ub97c \ube7c\ub0b4\uc640\uc11c \ub9cc\ub4e0 \uac83\uc774\ub2e4.\nIs_Married\ub294 Mrs \ud0c0\uc774\ud2c0\uc5d0 \uc758\ud574 \ub098\ub220\uc9c4 \ud56d\ubaa9\uc774\ub2e4. Mrs \ud0c0\uc774\ud2c0\uc740 \ub2e4\ub978 \uc5ec\uc131 \ud0c0\uc774\ud2c0\ubcf4\ub2e4 \uc81c\uc77c \ub192\uc740 \uc0dd\uc874\ub960\uc744 \ubcf4\uc600\ub2e4. \uc774 \ud0c0\uc774\ud2c0\uc740 \uadf8\uc790\uccb4\ub85c \ud56d\ubaa9\uc774 \ub418\uc5b4\uc57c \ud55c\ub2e4. \uc65c\ub0d0\ud558\uba74 \ubaa8\ub4e0 \uc5ec\uc131 \ud0c0\uc774\ud2c0\uc740 \uc11c\ub85c \uadf8\ub8f9\ud654\ub418\uc5c8\uc73c\ub2c8\uae4c. (This title needs to be a feature itself because all female titles are grouped with each other.)\n\uc544\ub798\uc758 \uadf8\ub798\ud504\uc5d0 \ub530\ub974\uba74 \ub9e4\uc6b0 \uc801\uc740 \ube48\ub3c4\ub97c \ubcf4\uc774\ub294 \ub9ce\uc740 \ud0c0\uc774\ud2c0\ub4e4\uc774 \uc788\ub2e4. \uba87\uba87 \ud0c0\uc774\ud2c0\uc740 \uc815\ud655\ud55c \uac83 \uac19\uc9c0 \uc54a\uc544 \ubcc0\uacbd\uc774 \ud544\uc694\ud558\ub2e4.\nMiss, Mrs, Ms, Mlle, Lady, Mme, the Countess, Dona \ud0c0\uc774\ud2c0\uc740 Miss\/Mrs\/Ms\ub85c \ubc14\uafbc\ub2e4. \uc65c\ub0d0\ud558\uba74 \ubaa8\ub450 \uc5ec\uc131\uc774\uae30 \ub54c\ubb38\uc774\ub2e4. Mlle, Mme and Dona \uac19\uc740 \uac12\uc740 \uc0ac\uc2e4 \uc2b9\uac1d\ub4e4\uc758 \uc774\ub984\uc774\uc9c0\ub9cc Name \ud56d\ubaa9\uc774 \ucf64\ub9c8\ub85c \uad6c\ubd84\ub418\uc5b4\uc9c0\uae30 \ub54c\ubb38\uc5d0 \ud0c0\uc774\ud2c0\ub85c \ubd84\ub958\ub418\uc5c8\ub2e4.\nDr, Col, Major, Jonkheer, Capt, Sir, Don and Rev \ud0c0\uc774\ud2c0\uc740 Dr\/Military\/Noble\/Clergy\ub85c \ubcc0\uacbd\ud558\uc600\ub2e4. \uc2b9\uac1d\ub4e4\uc774 \ube44\uc2b7\ud55c \ud2b9\uc9d5\uc744 \ubcf4\uc774\uae30 \ub54c\ubb38\uc774\ub2e4.\nMaster\ub294 \ud2b9\ubcc4\ud55c \ud0c0\uc774\ud2c0\uc774\ub2e4. 26\uc138 \uc774\ud558\uc758 \ub0a8\uc131\uc2b9\uac1d\uc5d0\uac8c \uc8fc\uc5b4\uc84c\uace0 \ubaa8\ub4e0 \ub0a8\uc131\uadf8\ub8f9\uc5d0\uc11c \uc81c\uc77c \ub192\uc740 \uc0dd\uc874\ub960\uc744 \ubcf4\uc600\ub2e4.","c13f7e57":"1.2 Dealing with missing values\n\n\ubc11\uc5d0\uc11c \ubcfc \uc218 \uc788\ub294 \uac83\uacfc \uac19\uc774 \uc5b4\ub5a4 \ud589\uc740 \uacb0\uce21\uac12\uc774 \uc788\ub2e4. display_missing \ud568\uc218\ub294 training set\uacfc test set\uc5d0\uc11c \uac01 \ud589\uc758 \uacb0\uce21\uac12\uc758 \uc218\ub97c \uc54c\ub824\uc900\ub2e4.\ntraining set\uc740 Age, Cabin, Embarked\uc5d0\uc11c \uacb0\uce21\uac12\uc774 \uc788\ub2e4.\ntest set\uc740 Age, Cabin, Fare\uc5d0\uc11c \uacb0\uce21\uac12\uc774 \uc788\ub2e4.\n\n\uacb0\uce21\uac12\uc744 \ub2e4\ub8f0 \ub54c \uc5f0\uacb0\ub370\uc774\ud130\uc9d1\ud569(concatenated)\uc778 training\uacfc test set\uc5d0\uc11c \uc77c\ud558\ub294 \uac83\uc774 \ub354 \ud3b8\ub9ac\ud558\ub2e4. \uadf8\ub807\uc9c0 \uc54a\uc73c\uba74 filled data\uc774 \uc624\ubc84\ud54f \ub41c\ub2e4...((the filled data may overfit to separate training or test sets.)) Age, Embarked and Fare \ud56d\ubaa9\uc5d0\uc11c \uacb0\uce21\uac12\uc740 \uc804\uccb4 \uc0d8\ud50c \uc218\uc5d0\uc11c \ube44\uad50\uc801 \uc801\uc740\ub370 Cabin\uc740 80%\uac00 \uacb0\uce21\uac12\uc774\ub2e4. Age, Embarked and Fare\ub294 \uae30\uc220\ud1b5\uacc4\uc801 \ubc29\ubc95\uc73c\ub85c \ucc44\uc6cc\ub123\uc744 \uc218 \uc788\uaca0\uc9c0\ub9cc Cabin\uc740 \uadf8\ub807\uac8c \ubabb\ud55c\ub2e4.","7a03daa8":"1.2.3 fare\n\n\ub531 \ud55c\uba85\ub9cc\uc774 Fare \uac12\uc774 \uc5c6\ub2e4. \uc6b0\ub9ac\ub294 Fare\uac00 \uac00\uc871\ud06c\uae30(Parch and SibSp)\uc640 Pclass \ud56d\ubaa9\uacfc \uc5f0\uad00\ub3fc\uc788\ub2e4\uace0 \uac00\uc815\ud560 \uc218 \uc788\ub2e4. third class \ud2f0\ucf13\uc744 \uac00\uc9c4 \uac00\uc871\uc774 \uc5c6\ub294 \ub0a8\uc131\uc758 \uc911\uac04 \uc694\uae08\uac12\uc774 \uacb0\uce21\uac12\uc744 \ucc44\uc6b0\ub294 \ub370 \ub17c\ub9ac\uc801\uc778 \uc120\ud0dd\uc774\ub2e4.","10f7917c":"3. Machine Learning","da8c33e2":"A, B, C\uc758 \uc804\ubd80\ub294 1st class\ub97c \uc704\ud55c \uac83\uc774\ub2e4.\nD\uc758 87%\ub294 1st class, 13%\ub294 2nd class\ub97c \uc704\ud55c \uac83\uc774\ub2e4.\nE\uc758 83%\ub294 1st class, 10%\ub294 2nd class, 7%\ub294 3rd class\ub97c \uc704\ud55c \uac83\uc774\ub2e4.\nF\uc758 62%\ub294 2nd class, 38%\ub294 3rd class\ub97c \uc704\ud55c \uac83\uc774\ub2e4.\nG\uc758 \uc804\ubd80\ub294 3rd class\ub97c \uc704\ud55c \uac83\uc774\ub2e4.\nT\uce90\ube48\uc5d0 \uc788\ub358 \uc0ac\ub78c\uc774 \ub531 \ud55c\uc0ac\ub78c \uc788\ub2e4. \uadf8 \uc0ac\ub78c\uc740 1st class\uc774\ub2e4. T\uce90\ube48 \uc2b9\uac1d\uc774 A\ub97c \uc4f0\ub294 \uc2b9\uac1d\uacfc \ub9ce\uc774 \ube44\uc2b7\ud558\ubbc0\ub85c \uadf8\ub97c A\uadf8\ub8f9\uc5d0 \ub123\uc5c8\ub2e4.\nM\uc774\ub77c\uace0 \uadf8\ub8f9\ud654\ub41c \uc2b9\uac1d\ub4e4\uc740 Cabin\ud56d\ubaa9\uc5d0\uc11c \uacb0\uce21\uac12\uc778\ub370((\uc804\uccb4 deck\ubcc4 \uc2b9\uac1d class \ud655\uc778\ud558\ub824\uace0 \ub9cc\ub4e0\uac70..?)) \uc774 \uc2b9\uac1d\ub4e4\uc758 \uc9c4\uc9dc deck\uc744 \ucc3e\ub294 \uac74 \uac00\ub2a5\ud558\uc9c0 \uc54a\uc744 \uac83 \uac19\ub2e4. \uadf8\ub798\uc11c M(missing)\uc774\ub77c\uace0 \ubd99\uc600\ub2e4.","95b906f4":"3.2 evaluation\n\n3.2.1. Feature Importance","bc4cf730":"2. Feature Engineering\n\n2.1 binning the continuous features (\uad6c\uac04\ud654)\n\n2.1.1 fare\n\nFare \ud56d\ubaa9\uc740 positively skewed \ud55c\ub370 \uc0dd\uc874\ub960\uc740 \uadf9\ub2e8\uc801\uc73c\ub85c \uc624\ub978\ucabd \ub05d\uc774 \ub192\ub2e4.\nFare \ud56d\ubaa9\uc744 \ub098\ub204\ub294 \ub370 13\uad6c\uac04\uc73c\ub85c \ub098\ub234\ub2e4. \uad6c\uac04\uc774 \ub9ce\uc744\uc9c0\ub77c\ub3c4 \uc5b4\uc9c0\uac04\ud55c \uc591\uc758 \uc815\ubcf4\ud68d\ub4dd\uc744 \uc81c\uacf5\ud55c\ub2e4.\n\uadf8\ub798\ud504\uc758 \uc67c\ucabd\ud3b8 \uadf8\ub8f9\uc740 \ub0ae\uc740 \uc0dd\uc874\ub960\uc744 \ubcf4\uc774\uace0 \uc624\ub978\ud3b8 \uadf8\ub8f9\uc740 \ub192\uc740 \uc0dd\uc874\ub960\uc744 \ubcf4\uc778\ub2e4. \uc774\ub7ec\ud55c \ub192\uc740 \uc0dd\uc874\ub960\uc740 \ubd84\ud3ec\uadf8\ub798\ud504(distribution graph)\uc5d0\uc120 \uc798 \ubcf4\uc774\uc9c0 \uc54a\uc558\ub2e4.\n\uc57d\uac04 \uacbd\ud5a5\uc131\uc5d0\uc11c \ubc97\uc5b4\ub098\ub294 (15.742, 23.25)\uadf8\ub8f9\ub3c4 \ubc1c\uacac\ub410\ub2e4.","76abdf76":"1.2.2 embarked\n\nEmbarked\ub294 \uce74\ud14c\uace0\ub9ac\uc2dd \ud56d\ubaa9\uc774\uace0 \uc804\uccb4 \ub370\uc774\ud130\uc14b\uc5d0\uc11c 2\uac1c\ub9cc\uc774 \uacb0\uce21\uac12\uc774\ub2e4. \ub450 \uac12 \ubaa8\ub450 \uc2b9\uac1d\uc774 \uc5ec\uc131\uc774\uace0 upper class\uc774\uba70 \uac19\uc740 \ud2f0\ucf13\ub118\ubc84\ub97c \uac16\uace0\uc788\ub2e4. \uc774 \ub9d0\uc740, \uadf8 \ub458\uc740 \uc11c\ub85c \uc54c\uba70 \uac19\uc740 \uacf3\uc5d0\uc11c \ud0d1\uc2b9\ud588\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. upper class\uc758 \uc5ec\uc131 \uc2b9\uac1d\uc758 Embarked \uae30\ubcf8\uac12(mode)\uc740 C(\uc138\ub974\ubd80\ub974)\uc774\uc9c0\ub9cc \uc774\uac83\uc774 \ubc18\ub4dc\uc2dc \uadf8\ub4e4\uc774 \uadf8 \ud56d\uad6c\uc5d0\uc11c \ud0d4\ub2e4\ub294 \uac74 \uc544\ub2c8\ub2e4.","b33946ef":"\ub0b4\uac00 Stone, Mrs. George Nelson (Martha Evelyn)\uc744 \uad6c\uae00\uc5d0\uc11c \ucc3e\uc558\uc744 \ub54c, \uadf8\ub294 S(\uc0ac\uc6b0\uc0d8\ud504\ud134)\uc5d0\uc11c \uadf8\ub140\uc758 \uba54\uc774\ub4dc\uc778 Amelie Icard\uc640 \ud0d4\uc74c\uc744 \ubc1c\uacac\ud588\ub2e4. \uc774\uac83\uc740 \uc815\ubcf4\uac00 \ud544\uc694\ud55c \ubd80\ubd84\uc774\uc5c8\ub2e4.","e6bc118b":"1.4 correlations\n\n\ud56d\ubaa9\ub4e4\uc740 \uc11c\ub85c \ud070 \uc0c1\uad00\uad00\uacc4\ub97c \uac16\uc73c\uba70 \uc758\uc874\uc801\uc774\ub2e4.\n\uc81c\uc77c \ub192\uc740 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\uc774\ub294 \ud56d\ubaa9\uc740 Fare\uc640 Pclass\uc0ac\uc774\uc758 \uad00\uacc4\ub85c training set\uc5d0\uc11c 0.549500\uc774\uace0 test set\uc5d0\uc11c 0.577147\uc774\ub2e4.\n\ub2e4\ub978 \ud56d\ubaa9\ub4e4\ub3c4 \ub192\uc740 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\uc778\ub2e4. 0.1\ubcf4\ub2e4 \ub192\uc740 \uac83\uc774 training set\uc5d0\uc11c 9\uac1c, test set\uc5d0\uc11c 6\uac1c\uc774\ub2e4.","b8509725":"1.2.1 age\n\nAge\uc758 \uacb0\uce21\uac12\uc740 \ub098\uc774\uc758 \uc911\uac04\uac12\uc73c\ub85c \ucc44\uc6cc\uc9c0\uc9c0\ub9cc \uc804\uccb4 \ub370\uc774\ud130\uc14b\uc758 \uc911\uac04\uac12\uc744 \uc4f0\ub294 \uac74 \uc88b\uc9c0 \ubabb\ud55c \uc120\ud0dd\uc774\ub2e4. group\uc758 \uc911\uac04 \ub098\uc774\uac12\uc744 \uc4f0\ub294 \uac8c \ub354 \ub098\uc740\ub370 \uc774\ub294 \uc0c8\ub85c\uc6b4 \uac12\uc774 \ub354 informative\ud558\uae30 \ub54c\ubb38\uc774\ub2e4. Pclass \uadf8\ub8f9\uc758 \uc911\uac04 \ub098\uc774\uac12\uc774 \uc81c\uc77c \uc88b\uc740 \uc120\ud0dd\uc774\ub2e4. \uc65c\ub0d0\ud558\uba74 Age\uc640 Survived\uc640\uc758 \uc0c1\uad00\uad00\uacc4\uac00 \ub192\uae30 \ub54c\ubb38\uc774\uace0 \uc138\uac00\uc9c0 \uac12(1,2,3)\ubc16\uc5d0 \uc5c6\uae30 \ub54c\ubb38\uc774\ub2e4..","79b51245":"2.2 family size & ticket frequency\n\nFamily_Size\ub294 SibSp\uc640 Parch\ub97c \ub354\ud558\uace0 \ub610 1\uc744 \ub354\ud574\uc11c \ub9cc\ub4e4\uc5c8\ub2e4. SibSp\ub294 \uc790\ub9e4\uc640 \ubc30\uc6b0\uc790\uc758 \uc218\ub97c, Parch\ub294 \ubd80\ubaa8\ub2d8\uacfc \uc544\uc774\ub4e4\uc758 \uc218\ub97c \uac00\ub9ac\ud0a8\ub2e4. \uc774 \uc5f4\ub4e4(columns)\uc740 \uac00\uc871\uc758 \uc804\uccb4 \ud06c\uae30\ub97c \ucc3e\uae30 \uc704\ud574 \ub354\ud574\uc84c\ub2e4. \ub9c8\uc9c0\ub9c9\uc5d0 1\uc744 \ub354\ud55c \uac74 \uc2b9\uac1d \ubcf8\uc778\uc744 \ub354\ud55c \uac83\uc774\ub2e4.\n\uadf8\ub798\ud504\ub294 \uac00\uc871\ud06c\uae30\uac00 \uc0dd\uc874\uc744 \uc608\uce21\ud558\ub294 \uc9c0\ud45c\ub85c \uc0ac\uc6a9\ub428\uc744 \ubcf4\uc5ec\uc8fc\uace0 \uc788\ub2e4. \uac00\uc871\ud06c\uae30\uc758 \ucc28\uc774\uac00 \uc0dd\uc874\ub960\uc758 \ucc28\uc774\ub97c \ubcf4\uc774\uae30 \ub54c\ubb38\uc774\ub2e4.\n\uac00\uc871\ud06c\uae30\uac00 1\uc774\uba74 Alone\n2, 3, 4\uc774\uba74 Smal\n5, 6\uc774\uba74 Medium\n7, 8, 11\uc774\uba74 Large\ub85c \uc774\ub984\ubd99\uc600\ub2e4","e27c592f":"2.5.2 one-hot encoding the categorical features ((\ub2e8\ud558\ub098\uc758 \uac12\ub9cc true(1)\uc774\uace0 \ub098\uba38\uc9c0\ub294 \ubaa8\ub450 False\uc778 \uc778\ucf54\ub529, \uc778\uc790\ub294 \uce74\ud14c\uace0\ub9ac\ud56d\ubaa9))\n\n\uce74\ud14c\uace0\ub9ac \ud56d\ubaa9\uc778 Pcalss, Sex, Deck, Embarked, Title\uc740 OneHotEncoder\ub97c \uc0ac\uc6a9\ud574 one-hot encoded \ud56d\ubaa9\uc73c\ub85c \ubc14\uafbc\ub2e4. Age\uc640 Fare \ud56d\ubaa9\uc740 \ubc14\ub00c\uc9c0 \uc54a\ub294\ub370 \uc774\ub294 \uc55e\uc758 \uac83\ub4e4\uacfc \ub2e4\ub974\uac8c \ub300\uc18c\uad00\uacc4\uac00 \uc911\uc694(ordinal)\ud558\uae30 \ub54c\ubb38\uc774\ub2e4.","49050a81":"2.6 conclusion\n\nAge\uc640 Fare \ud56d\ubaa9\uc740 \uad6c\uac04\uc9c0\uc5c8\ub2e4(are binned). Binning\uc740 outlier\ub97c \ucc98\ub9ac\ud558\ub294 \ub370 \uc27d\uace0 \uadf8 \ud56d\ubaa9\uc5d0\uc11c homogeneous group\uc744 \ucc3e\uc544\ub0b4\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc900\ub2e4.\nFamily_Size\ub294 Parch+SibSp+1\ub85c \ub9cc\ub4e4\uc5c8\ub2e4.\nTicket_Frequency\ub294 Ticket\uc758 (\ubc1c\uc0dd)\ud69f\uc218\ub85c \ub9cc\ub4e4\uc5c8\ub2e4.\nName \ud56d\ubaa9\uc740 \ub9e4\uc6b0 \uc720\uc6a9\ud558\ub2e4. \uba3c\uc800 Title\uacfc Is_Married \ud56d\ubaa9\uc740 \uc774\ub984\uc758 \uc811\ub450\uc5b4\uc5d0\uc11c \uac00\uc838\uc654\ub2e4. \ub450\ubc88\uc9f8\ub85c Family_Survival_Rate\uacfc Family_Survival_Rate_NA \ud56d\ubaa9\uc740 \uc2b9\uac1d\uc758 \uc131\uc744 target encoding\ud574\uc11c \ub9cc\ub4e4\uc5c8\ub2e4.\nTicket_Survival_Rate\uc740 Ticket \ud56d\ubaa9\uc744 target encoding\ud574\uc11c \ub9cc\ub4e4\uc5c8\ub2e4.\nSurvival_Rate \ud56d\ubaa9\uc740 Family_Survival_Rate\uacfc Ticket_Survival_Rate \ud56d\ubaa9\uc758 \ud3c9\uade0\uc73c\ub85c \ub9cc\ub4e4\uc5c8\ub2e4.\n\ub9c8\uc9c0\ub9c9\uc73c\ub85c, \uc22b\uc790\ud615\ud0c0\uc785\uc774 \uc544\ub2cc \ud56d\ubaa9\uc740 label encoding, \uce74\ud14c\uace0\ub9ac \ud56d\ubaa9\uc740 one-hot encoding\uc744 \uc0ac\uc6a9\ud588\ub2e4.\n5\uac1c\uc758 \uc0c8\ub85c\uc6b4 \ud56d\ubaa9(Family_Size, Title, Is_Married, Survival_Rate, Survival_Rate_NA)\uc744 \ub9cc\ub4e4\uc5c8\uc73c\uba70 \uc778\ucf54\ub529 \ud6c4 \ud544\uc694\uc5c6\ub294 \ud56d\ubaa9\uc740 \ubc84\ub838\ub2e4.","fd6326d3":"1.6 conclusion\n\n\ub300\ubd80\ubd84\uc758 \ud56d\ubaa9\uc774 \uc11c\ub85c \uc5f0\uad00\ub418\uc5b4\uc788\ub2e4. \uc774\ub7f0 \uad00\uacc4\ub294 \ud56d\ubaa9\uc804\ud658\uacfc \uc0c1\ud638\uc791\uc6a9(feature transformation and interaction)\uc744 \ud1b5\ud574 \uc0c8\ub85c\uc6b4 \ud56d\ubaa9\uc73c\ub85c \ub9cc\ub4dc\ub294 \ub370 \uc0ac\uc6a9\ub420 \uc218 \uc788\ub2e4. \ud0c0\uac9f \uc778\ucf54\ub529(target encoding, Target encoding is the process of replacing a categorical value with the mean of the target variable.)\ub3c4 \ub610\ud55c \uc720\uc6a9\ud560 \uc218 \uc788\ub2e4. Survived \ud56d\ubaa9\uacfc \ub192\uc740 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\uc774\uae30 \ub54c\ubb38\uc774\ub2e4.\nsplit points\uc640 spikes\ub294 \uc5f0\uc18d\uc801\uc778 \uc790\ub8cc\uc5d0\uc11c \ub208\uc5d0 \ub744\uc9c0\ub9cc \ubd88\ud589\ud788\ub3c4 2\uac1c\uc758 \ud56d\ubaa9\ubc16\uc5d0 \uc5c6\ub2e4. \uacb0\uc815\ud2b8\ub9ac \uc54c\uace0\ub9ac\uc998\uc73c\ub85c \uc27d\uac8c \uc7a1\uc544\ub0bc \uc218 \uc788\uc9c0\ub9cc \uc2e0\uacbd\ub9dd(neural networks)\uc774 \ubc1c\uacac \ubabb\ud560 \uc218 \uc788\ub2e4.\n\uce74\ud14c\uace0\ub9ac \ud56d\ubaa9\uc740 \ub2e4\ub978 \uc0dd\uc874\ub960\uc744 \ubcf4\uc774\ub294 \uad6c\ubcc4\ub418\ub294 \ud074\ub798\uc2a4\ub4e4\uc744 \uac16\uace0\uc788\ub2e4. \uadf8\ub7ec\ud55c \ud074\ub798\uc2a4\ub4e4\uc740 one-hot encoding\uc744 \ud1b5\ud574 \uc0c8\ub85c\uc6b4 \ud56d\ubaa9\uc73c\ub85c \uc4f0\uc77c \uc218 \uc788\ub2e4. \uba87\uba87\uc758 \ud074\ub798\uc2a4\ub4e4\uc740 \uc0c8\ub85c\uc6b4 \ud56d\ubaa9\uc744 \ub9cc\ub4e4\uae30 \uc704\ud574 \uc11c\ub85c \ud569\uccd0\uc9c8 \uc218 \uc788\ub2e4.\nExploratory Data Analysis \ubd80\ubd84\uc5d0\uc11c Deck \ud56d\ubaa9\uc744 \ub9cc\ub4e4\uace0 Cabin \ud56d\ubaa9\uc744 \ubc84\ub838\ub2e4.","f964ce32":"2.5 Feature transformation\n\n2.5.1 label encoding the non-numerical features((\ub370\uc774\ud130 \uc218\uce58\ud654\uc2dc\ud0a4\ub294 \uc791\uc5c5))\n\nEmbarked, Sex, Deck, Title, Family_Size_Grouped\ub294 \uac1d\uccb4(object)\ud0c0\uc785\uc774\uace0, Age\uc640 Fare \ud56d\ubaa9\uc740 \uce74\ud14c\uace0\ub9ac \ud0c0\uc785\uc774\ub2e4. \uc774\uac83\ub4e4\uc744 LabelEncoder\ub85c \uc22b\uc790\ud0c0\uc785\uc73c\ub85c \ubcc0\ud658\ud55c\ub2e4. LabelEncoder\ub294 \uae30\ubcf8\uc801\uc73c\ub85c \ud074\ub798\uc2a4\ub97c 0\ubd80\ud130 n\uae4c\uc9c0 \ub77c\ubca8\ub9c1\ud55c\ub2e4. \uc774 \uacfc\uc815\uc740 \uba38\uc2e0\ub7ec\ub2dd \uc54c\uace0\ub9ac\uc998\uc5d0\uc11c \ud56d\ubaa9\uc73c\ub85c\ubd80\ud130 \ubb34\uc5b8\uac08 \uc54c\uc544\ub0bc \ub54c\uc5d0 \ud544\uc694\ud558\ub2e4.","207a284c":"1.5.2 categorical features\n\n\ubaa8\ub4e0 \uce74\ud14c\uace0\ub9ac\uc2dd \ud56d\ubaa9\uc740 \uc801\uc5b4\ub3c4 \ud55c \uac1c\uc758 \uc0dd\uc874\ud558\uc9c0 \ubabb\ud55c \ube44\uc728\uc774 \ud070 \ud074\ub798\uc2a4\ub97c \uac16\uace0\uc788\ub2e4. \uadf8\ub7ec\ud55c \ud074\ub798\uc2a4\ub294 one-hot encoding\uc758 \ud56d\ubaa9\uc73c\ub85c \uc4f0\uc77c \ub54c \uc2b9\uac1d\ub4e4\uc774 \uc0dd\uc874\uc790\uc778\uc9c0 \ud53c\ud574\uc790\uc778\uc9c0 \uc608\uce21\ud558\ub294\ub370 \ud070 \ub3c4\uc6c0\uc774 \ub41c\ub2e4. \n\uc0ac\uc6b0\uc0d8\ud504\ud134\uc5d0\uc11c \ud0c4 \uc2b9\uac1d\ub4e4\uc740 \ub2e4\ub978 \ud56d\uad6c\uc5d0\uc11c \ud0c4 \uc2b9\uac1d\ubcf4\ub2e4 \ub0ae\uc740 \uc0dd\uc874\ub960\uc744 \ubcf4\uc778\ub2e4. \uc138\ub974\ubd80\ub974\uc5d0\uc11c \ud0c4 \uc808\ubc18\uc774 \ub118\ub294 \uc2b9\uac1d\ub4e4\uc740 \uc0dd\uc874\ud588\ub2e4. \uc774\ub7ec\ud55c \uad00\ucc30\uc740 Pclass\ud56d\ubaa9\uacfc \uc5f0\uacb0\ub420 \uc218 \uc788\ub2e4.\nParch\uc640 Sibsp \ud56d\ubaa9\uc740 \ub2e8 \ud558\ub098\uc758 \uac00\uc871\uad6c\uc131\uc6d0\uc744 \uac00\uc9c4 \uc2b9\uac1d\ub4e4\uc774 \ub192\uc740 \uc0dd\uc874\ub960\uc744 \ubcf4\uc778\ub2e4\uace0 \ub9d0\ud574\uc900\ub2e4.\n\uc81c\uc77c \uc88b\uc740 \uce74\ud14c\uace0\ub9ac\uc2dd \ud56d\ubaa9\uc740 Pclass\uc640 Sex\uc778\ub370 because they have the most homogenous classes.","df603167":"1.3 survival distribution\n\ntraining set\uc758 38.38% (342\/891)\uac00 Class 1. (\uc0dd\uc874)\ntraining set\uc758 61.62% (549\/891)\uac00 Class 0. (\uc0dd\uc874\ubabb\ud568)","0d9fc1f3":"\uc774 \ucee4\ub110\uc740 [Gunes\ub2d8\uc758 \ud0c0\uc774\ud0c0\ub2c9 \ud29c\ud1a0\ub9ac\uc5bc](https:\/\/www.kaggle.com\/gunesevitan\/advanced-feature-engineering-tutorial-with-titanic)\uc744 \ubc88\uc5ed\ud55c \ucee4\ub110\uc785\ub2c8\ub2e4.\n\nThis kernel is translated version of @Gunes Evitan's [Advanced Feature Engineering Tutorial with Titanic](https:\/\/www.kaggle.com\/gunesevitan\/advanced-feature-engineering-tutorial-with-titanic)."}}