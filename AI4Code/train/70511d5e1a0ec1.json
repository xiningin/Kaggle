{"cell_type":{"ba49af8d":"code","1beb2e19":"code","a6900409":"code","c2224200":"code","4c7c29d1":"code","29bbf71a":"code","82bc5cd9":"code","832ac955":"code","66f99e94":"code","e44f7f0c":"code","b3ccb47b":"code","05793a5b":"code","0cfe0532":"code","3f734897":"code","f733911f":"code","b2d906b6":"code","61eaa73c":"code","9d11b630":"code","d87202ae":"code","28559546":"code","ba975659":"code","6153ea0f":"code","c66dccb4":"code","141e9b22":"code","45810358":"code","4a799a51":"code","fc06b4ee":"code","b9239af7":"code","cc410dc7":"code","ccec9754":"code","24c087be":"code","b3730414":"markdown","b9ea38d5":"markdown","391830d2":"markdown","fae9dd69":"markdown","ba9136e3":"markdown","c251fafa":"markdown","cea02349":"markdown","769f7c8e":"markdown"},"source":{"ba49af8d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir('..\/input'))\n\n# Any results you write to the current directory are saved as output.\n\nimport zipfile\nwith zipfile.ZipFile('..\/input\/plates.zip', 'r') as zip_obj:\n   # Extract all the contents of zip file in current directory\n   zip_obj.extractall('\/kaggle\/working\/')\n    \nprint('After zip extraction:')\nprint(os.listdir('\/kaggle\/working\/'))","1beb2e19":"import warnings\nwarnings.filterwarnings('ignore') ","a6900409":"import time\nimport copy\nimport random\nimport shutil\n\nimport cv2\nimport torch\nimport torchvision\nfrom torchvision import transforms, models\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm","c2224200":"MEAN = np.array([0.485, 0.456, 0.406])\nSTD = np.array([0.229, 0.224, 0.225])","4c7c29d1":"DATA_ROOT = '\/kaggle\/working\/plates\/'\nTRAIN_DIR = 'train'\nTEST_DIR = 'test'","29bbf71a":"shutil.copytree(os.path.join(DATA_ROOT, TRAIN_DIR), TRAIN_DIR)","82bc5cd9":"shutil.copytree(os.path.join(DATA_ROOT, TEST_DIR), os.path.join(TEST_DIR, 'unknown'))","832ac955":"class DataPreprocessing:\n    \"\"\"Class for datasets preprocessing\"\"\"\n    \n    def _remove_background(self, path, file):\n        # TODO: Improve background removing and crop images\n        img = cv2.imread(path + file)\n    \n        img = np.array(img)\n        height, width = img.shape[:2]\n        mask = np.zeros([height, width], np.uint8)\n\n        bgdModel = np.zeros((1, 65),np.float64)\n        fgdModel = np.zeros((1, 65),np.float64)\n\n        rect = (15, 15, width-15, height-15)\n        cv2.grabCut(img, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)\n        mask = np.where((mask==2) | (mask==0), 0, 1).astype('uint8')\n        res = img * mask[:, :, np.newaxis]\n\n        # Get the background\n        background = img - res\n\n        # Change all pixels in the background that are not black to white\n        background[np.where((background > [0, 0, 0]).all(axis = 2))] = [255, 255, 255]\n\n        res = np.array(background + res)\n        cv2.imwrite(path + file, res)\n\n    def remove_background(self, image_folders):\n        for path in image_folders:\n            files = os.listdir(path)\n            files = list(filter(lambda x: x.endswith('.jpg'), files))\n\n            for file in tqdm(files):\n                self._remove_background(path, file)","66f99e94":"class ImageFolderWithPaths(torchvision.datasets.ImageFolder):\n    def __getitem__(self, index):\n        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n        path = self.imgs[index][0]\n        tuple_with_path = (original_tuple + (path,))\n        return tuple_with_path","e44f7f0c":"def show_input(input_tensor, title=''):\n    image = input_tensor.permute(1, 2, 0).numpy()\n    image = STD * image + MEAN\n    plt.imshow(image.clip(0, 1))\n    plt.title(title)\n    plt.show()","b3ccb47b":"preprocessor = DataPreprocessing()","05793a5b":"# Removing images background\npreprocessor.remove_background(image_folders=[\n    os.path.join(TRAIN_DIR, 'cleaned\/'),\n    os.path.join(TRAIN_DIR, 'dirty\/'),\n    os.path.join(TEST_DIR, 'unknown\/')\n])","0cfe0532":"!ls train\/cleaned","3f734897":"!ls train\/dirty","f733911f":"train_transforms = transforms.Compose([\n    transforms.RandomRotation(degrees=90, fill=255),\n    transforms.CenterCrop(180),\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.ColorJitter(hue=(0.1, 0.2)),\n    transforms.ToTensor(),\n    transforms.Normalize(MEAN, STD)\n])\n\ntrain_dataset = torchvision.datasets.ImageFolder(TRAIN_DIR, train_transforms)\n\nbatch_size = 8\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=batch_size\n)","b2d906b6":"len(train_dataloader), len(train_dataset)","61eaa73c":"class_names = ['cleaned', 'dirty']\n\nX_batch, y_batch = next(iter(train_dataloader))\n\nfor x_item, y_item in zip(X_batch, y_batch):\n    show_input(x_item, title=class_names[y_item])","9d11b630":"def train_model(model, dataloader, loss, optimizer, scheduler, num_epochs):\n    accuracies = {'train': []}\n    losses = {'train': []}\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}:'.format(epoch, num_epochs - 1), flush=True)\n\n        # Each epoch has a training phase\n        phase = 'train'\n        model.train()  # Set model to training mode\n\n        running_loss = 0.\n        running_acc = 0.\n\n        # Iterate over data\n        for inputs, labels in tqdm(dataloader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n\n            # Forward and backward\n            with torch.set_grad_enabled(True):\n                preds = model(inputs)\n                loss_value = loss(preds, labels)\n                preds_class = preds.argmax(dim=1)\n\n                loss_value.backward()\n                optimizer.step()\n\n            # Statistics\n            running_loss += loss_value.item()\n            running_acc += (preds_class == labels.data).float().mean()\n        \n        scheduler.step()\n            \n        epoch_loss = running_loss \/ len(dataloader)\n        epoch_acc = running_acc \/ len(dataloader)\n\n        accuracies[phase].append(epoch_acc)\n        losses[phase].append(epoch_loss)\n\n        print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc), flush=True)\n\n    return model, losses, accuracies","d87202ae":"# Choose seed for training reproduction\nseed = 21\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\n\n# model = models.resnet18(pretrained=True)\n# model = models.resnet34(pretrained=True)\nmodel = models.resnet50(pretrained=True)\n# model = models.resnet152(pretrained=True)\n\n# Disable grad for all conv layers\nfor param in model.parameters():\n    param.requires_grad = False\n\nmodel.fc = torch.nn.Linear(model.fc.in_features, 2)\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\nloss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=1.0)\n# optimizer = torch.optim.Adam(model.parameters(), amsgrad=True, lr=0.001)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","28559546":"model, losses, accuracies = train_model(model, train_dataloader, loss,\n                                        optimizer, scheduler, num_epochs=40)","ba975659":"plt.rcParams['figure.figsize'] = (10, 5)\nfor experiment_id in accuracies.keys():\n    plt.plot(accuracies[experiment_id], label=experiment_id)\nplt.legend()\nplt.title('Accuracy')","6153ea0f":"plt.rcParams['figure.figsize'] = (10, 5)\nfor experiment_id in losses.keys():\n    plt.plot(losses[experiment_id], label=experiment_id)\nplt.legend()\nplt.title('Loss')","c66dccb4":"test_transforms = transforms.Compose([\n    transforms.CenterCrop(180),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(MEAN, STD)\n])\n\ntest_dataset = ImageFolderWithPaths(os.path.join(TEST_DIR), transform=test_transforms)\n\ntest_dataloader = torch.utils.data.DataLoader(\n    test_dataset, batch_size=8, shuffle=False, num_workers=0\n)","141e9b22":"test_dataset","45810358":"model.eval()\n\ntest_predictions = []\ntest_img_paths = []\nfor inputs, labels, paths in tqdm(test_dataloader):\n    inputs = inputs.to(device)\n    labels = labels.to(device)\n    with torch.set_grad_enabled(False):\n        preds = model(inputs)\n    test_predictions.append(\n        torch.nn.functional.softmax(preds, dim=1)[:,1].data.cpu().numpy())\n    test_img_paths.extend(paths)\n\ntest_predictions = np.concatenate(test_predictions)","4a799a51":"inputs, labels, paths = next(iter(test_dataloader))\n\nfor img, pred in zip(inputs, test_predictions):\n    show_input(img, title=pred)","fc06b4ee":"submission_df = pd.DataFrame.from_dict({'id': test_img_paths, 'label': test_predictions})","b9239af7":"submission_df['label'] = submission_df['label'].map(lambda pred: 'dirty' if pred > 0.63 else 'cleaned')\nsubmission_df['id'] = submission_df['id'].str.replace('test\/unknown\/', '')\nsubmission_df['id'] = submission_df['id'].str.replace('.jpg', '')\nsubmission_df.set_index('id', inplace=True)\nsubmission_df.head(n=6)","cc410dc7":"submission_df['label'].value_counts()","ccec9754":"submission_df.to_csv(os.path.join(DATA_ROOT, 'submission.csv'))","24c087be":"!rm -rf train test","b3730414":"## **Transforms for test dataset**","b9ea38d5":"## **Classes and functions**","391830d2":"## **Submit predictions to file**","fae9dd69":"## **Transforms for train dataset**","ba9136e3":"## **Make predictions for test dataset**","c251fafa":"## **Make some preparations for work**","cea02349":"## **Model training**","769f7c8e":"## **Preprocess images**"}}