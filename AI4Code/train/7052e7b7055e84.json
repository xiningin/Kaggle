{"cell_type":{"c2dcae0d":"code","bdb30703":"code","8d0e265c":"code","7906ba42":"code","7c2fc416":"code","4e61faee":"code","59c23a43":"code","ad48b9d7":"markdown","ac6874ec":"markdown","1ba07439":"markdown"},"source":{"c2dcae0d":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datatable as dt\nfrom tensorflow import keras\nfrom tensorflow.random import set_seed\nfrom warnings import filterwarnings\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, InputLayer, Dropout\nfrom tensorflow.keras.regularizers import L1, L2\nfrom tensorflow.keras.models import load_model\nfrom sklearn.model_selection import StratifiedKFold\n\nfilterwarnings('ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'","bdb30703":"### set seeds\nmy_seed = 1\n\nnp.random.seed(my_seed)\nset_seed(my_seed)","8d0e265c":"from sklearn.preprocessing import StandardScaler\n\ndf_train = dt.fread('..\/input\/tabular-playground-series-nov-2021\/train.csv').to_pandas()\ndf_test = dt.fread('..\/input\/tabular-playground-series-nov-2021\/test.csv').to_pandas()\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')\n\nX = df_train.drop(columns=['id','target']).copy()\ny = df_train['target'].copy()\nX_test = df_test.drop(columns='id').copy()\n\nscaler = StandardScaler()\nX = pd.DataFrame(columns=X.columns, data=scaler.fit_transform(X))\nX_test = pd.DataFrame(columns=X_test.columns, data=scaler.transform(X_test))","7906ba42":"def lr_nn_callback():\n    lr_start    = 5e-5\n    lr_max1     = 0.002\n    lr_max2     = 0.001\n    lr_min      = 0\n    lr_ramp_ep  = 5\n    lr_sus_ep1  = 15\n    lr_sus_ep2  = 15\n    lr_decay    = 0.85\n    lr_decay_ep = 1\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max1 - lr_start) \/ lr_ramp_ep * epoch + lr_start   \n        elif epoch < lr_ramp_ep + lr_sus_ep1:\n            lr = lr_max1        \n        elif epoch < lr_ramp_ep + lr_sus_ep1 + lr_sus_ep2:\n            lr = lr_max2 \n        else:\n            lr = (lr_max2 - lr_min) * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep1 - lr_sus_ep2)\/\/lr_decay_ep) + lr_min    \n        return lr\n\n    lr_callback = LearningRateScheduler(lrfn, verbose=False)\n    return lr_callback","7c2fc416":"early_stopping = EarlyStopping(\n    monitor='val_loss', \n    min_delta=0, \n    patience=15, \n    verbose=0,\n    mode='min', \n    baseline=None, \n    restore_best_weights=True\n)\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss', \n    factor=0.2,\n    patience=5,\n    mode='min'\n)","4e61faee":"cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=my_seed)\n\nscores = {fold:None for fold in range(cv.n_splits)}\npredictions = []\n\nprint('Training Started.')\nfor fold, (idx_train, idx_valid) in enumerate(cv.split(X, y)):\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n\n    model = load_model('..\/input\/nov-baseline-nn\/base_model')\n    \n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['AUC']\n    )\n    \n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_valid, y_valid),\n        batch_size=4000,\n        epochs=200,\n        verbose=2,\n        callbacks=[\n            early_stopping,\n            reduce_lr,\n        ]\n    )\n    \n    print(f\"Fold {fold+1} || Best Validation AUC: {np.max(history.history['val_auc']):05f}\")\n    \n    prediction = model.predict(X_test).reshape(1,-1)[0]\n    predictions.append(prediction)\n\nprint('Training Finished')","59c23a43":"sample_submission['target'] = np.mean(np.column_stack(predictions), axis=1)\nsample_submission.to_csv('.\/submission.csv', index=False)","ad48b9d7":"Notebook copied from: https:\/\/www.kaggle.com\/mlanhenke\/tps-11-nn-baseline-keras","ac6874ec":"Best\nAUC:  0.759198","1ba07439":"### LR Callback\nLearning rate callback with warmup, dual piecewise learning rate, followed by learning rate decay"}}