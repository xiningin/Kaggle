{"cell_type":{"78b282a0":"code","f41158ff":"code","e523a093":"code","63ce3746":"code","3322d2b8":"code","7011fce7":"code","23233379":"code","7c6c0417":"code","d960a1b0":"code","efeffa8f":"code","c7612cfa":"code","ab3e4eeb":"code","c896e944":"code","3d4e985d":"code","213e32f1":"code","343bfeb6":"code","370542a5":"code","5dd491ab":"code","60877f53":"code","28b6963f":"code","d687ca0f":"code","40dcc5a5":"code","a77f553f":"code","93093643":"code","ead37b7e":"code","8c645bd4":"code","4be5d6df":"code","89c2cba8":"code","65923294":"code","b0e7c27d":"code","30b0740d":"code","2d9ac42f":"code","22c0878a":"code","2a1d81a6":"code","4db3315d":"code","1647e39d":"code","d3b4f593":"code","5b83bf15":"markdown","b890aaec":"markdown","3fe07a68":"markdown","3ca8672a":"markdown","5eced47e":"markdown","8058290a":"markdown","381d2b96":"markdown","dbeae4df":"markdown","edba2214":"markdown","b3b6fdaa":"markdown","3dda0754":"markdown","5c949508":"markdown","c783f44d":"markdown","f1495e22":"markdown"},"source":{"78b282a0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f41158ff":"df_train_x = pd.read_csv(\"..\/input\/X_train.csv\")\ndf_test_x = pd.read_csv(\"..\/input\/X_test.csv\")\ndf_train_y = pd.read_csv(\"..\/input\/y_train.csv\")","e523a093":"df_train_x.info()","63ce3746":"df_train_y.info()","3322d2b8":"df_train_x.sample()","7011fce7":"df_test_x.series_id.value_counts()","23233379":"df_train_y.sample(5)","7c6c0417":"df_train_x.sample(5)","d960a1b0":"df_train_y[df_train_y.series_id == 1690]","efeffa8f":"df_train_x[df_train_x.series_id == 1690].shape","c7612cfa":"\"\"\"series_id_y = df_train_y.series_id.tolist()\ngroup_id_y = df_train_y.group_id.tolist()\nsurface = df_train_y.surface.tolist()\nseries_id_x = df_train_x.series_id.tolist()\nmeasurment_id = df_train_x.measurement_number.tolist()\njust_check_y = list(zip(series_id_y,group_id_y))\njust_check_x = list(zip(series_id_x,measurment_id))\nstore = [-1]*len(just_check_x)\nfor i in just_check_y:\n    if i in just_check_x:\n        store[just_check_x.index(i)] = surface[just_check_y.index(i)]\ndf_train_x[\"target\"] = store\"\"\"","ab3e4eeb":"df = pd.merge(df_train_x,df_train_y,how='left',on='series_id')","c896e944":"df.sample()","3d4e985d":"df.surface.value_counts()","213e32f1":"df.sample(3)","343bfeb6":"df.info()","370542a5":"df.sample()","5dd491ab":"df.drop(columns=[\"row_id\",\"measurement_number\",\"group_id\"], inplace=True)\ndf.sample(2)","60877f53":"just_check =  df.groupby(\"series_id\").mean().reset_index()\ndf = pd.merge(just_check,df_train_y,how='left',on='series_id')\ndf.sample(3)","28b6963f":"df.drop(columns=[\"series_id\",\"group_id\"],inplace=True)","d687ca0f":"df.boxplot()\nplt.xticks(rotation = 90)","40dcc5a5":"df[df.columns[-4:]].boxplot()","a77f553f":"df.corr()","93093643":"df.sample(3)","ead37b7e":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ntrain = scaler.fit_transform(df[df.columns[:-1]])\ntrain_x, test_x, train_y, test_y = train_test_split(train,df[df.columns[-1]],test_size = 0.1)","8c645bd4":"train_x.shape","4be5d6df":"from sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(train_x, train_y)\ntarget = model.predict(test_x)\nmat = confusion_matrix(test_y, target)\nprint(\"******confusion******\\n\",mat)","89c2cba8":"\"\"\"from sklearn.metrics import confusion_matrix,accuracy_score\nfrom xgboost import XGBClassifier\nfor i in range(4,10):\n    model = XGBClassifier(model_depth = i)\n    model.fit(train_x, train_y)\n    target = model.predict(test_x)\n    print(\"accuracy : \", accuracy_score(target, test_y))\nmat = confusion_matrix(test_y, target)\nprint(\"******confusion******\\n\",mat)\"\"\"","65923294":"df_test_x.drop(columns=[\"row_id\",\"measurement_number\"],inplace=True)\ntesting = df_test_x.groupby(\"series_id\").mean().reset_index()\ntesting.sample(3)\ntesting.shape","b0e7c27d":"testing.shape","30b0740d":"from sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nfin_train = scale.fit_transform(df[df.columns[:-1]])\ntest = scale.transform(testing[testing.columns[1:]])","2d9ac42f":"from xgboost import XGBClassifier\n\nmodel = XGBClassifier()\nmodel.fit(fin_train,df[df.columns[-1]])\ntarget = model.predict(test)","22c0878a":"a = testing.series_id.tolist()\nb = target\nsubmission = pd.DataFrame({\"series_id\":a,\"surface\":b})","2a1d81a6":"submission.surface.value_counts()","4db3315d":"#encoding = submission.surface.map({\"concrete\":1,\"soft_pvc\":2,\"wood\":3,\"tiled\":4,\"fine_concrete\":5,\"soft_tiles\":6,\"hard_tiles_large_space\":7,\"carpet\":8,\"hard_tiles\":9})","1647e39d":"#submission[\"encoding\"] = encoding","d3b4f593":"submission.to_csv(\"submission.csv\",index=False)","5b83bf15":"Just checking whether any two columns correlate each other..\n\nWe can see that Orientation_W and orientation_Z are correlated. I would like to edit it in my next commit.","b890aaec":"Lets' analyze if there are any outliers that completely deviate and affect the model","3fe07a68":"NO null values...but size of two DFs (train and test) is different.","3ca8672a":"These outliers are informative and may help model. I would like to keep them. I will check the model by removing the outlieres in my next commit.","5eced47e":"Attached the target column to df_train_x\n* **I WOULD LIKE TO KNOW IF THERE'S ANOTHER SIMPLER WAY TO JOIN **","8058290a":"Please guve suggestions to improve my way of approach towards model. Also I would like to know how to improve the model.","381d2b96":"NO null values","dbeae4df":"Dropping the columns that are not necessary for building the model","edba2214":"* In the above groupby using target variable, I took the average values of all attributes. Based on them we can understand that there are some variations among the target variables and attributes.\n* **But values of llast three columns(linear accelerations) doesn't change much.**","b3b6fdaa":"Let's get started. This is like a starter kernel.","3dda0754":"let's see last 3 columns","5c949508":"So, I would like to append the surface column values to respective Series,measurement_number.","c783f44d":"I'm using Gradient Boosting for building my model","f1495e22":"So, for each series ID there are 127 records with different meaurment number"}}