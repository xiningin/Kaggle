{"cell_type":{"4ad2148b":"code","2d74cc13":"code","545b0477":"code","d344d2bf":"code","18d5308b":"code","ae249052":"code","6b2a61d5":"code","36c6d331":"code","c7da79db":"code","71187ca0":"code","d2dd405d":"code","bbf8c805":"code","9341f566":"code","f94fcf84":"code","34273b72":"code","f70a2218":"code","b2337e70":"code","6396d7f7":"code","e4cc42ab":"code","52aa51d0":"code","6f4f8062":"code","16dbdb36":"code","ca3f1783":"code","1c150d1d":"code","b38d7788":"code","f4d3b9df":"code","5799e879":"code","9f110b1b":"code","198ddad7":"code","f6c96c80":"code","ae46ab2f":"markdown","db55b0e8":"markdown","87dd4cdb":"markdown","adfbe26e":"markdown","738433ff":"markdown","908ae4aa":"markdown","96a98154":"markdown","166ecce7":"markdown","e1986c4f":"markdown","c5385a17":"markdown","debf45b2":"markdown","c6d388e6":"markdown","aa4a0e82":"markdown","c8168d16":"markdown","93fb8bec":"markdown","540698f0":"markdown","69b56e80":"markdown","18e55faf":"markdown","684bf8b5":"markdown","e19f9ff8":"markdown","4219cc71":"markdown","691e1680":"markdown","0d781b6d":"markdown","b0235650":"markdown","47146916":"markdown","d914445d":"markdown","80fb3c93":"markdown","e9335d4f":"markdown","edf5f42f":"markdown","4bf32e47":"markdown","ce7b59d7":"markdown","6c3d9de1":"markdown","579af969":"markdown","a637bada":"markdown","eb0ab99b":"markdown","dfafb545":"markdown","206b3519":"markdown","1f4dc886":"markdown","d8e7bd62":"markdown"},"source":{"4ad2148b":"import numpy as np\nimport pandas as pd\n\nimport altair as alt\nalt.renderers.enable('kaggle')\n\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.model_selection import cross_val_score, cross_validate, train_test_split, RandomizedSearchCV, GridSearchCV\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge, Lasso\n\nfrom scipy.stats import loguniform","2d74cc13":"admit_df = pd.read_csv('\/kaggle\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv')\nadmit_df","545b0477":"train_df, test_df = train_test_split(admit_df,\n                                     train_size=0.8,\n                                     random_state=2018)","d344d2bf":"train_df.info()","18d5308b":"train_df.describe()","ae249052":"unique_values = {}\nindex = np.arange(0, len(train_df.columns.to_list()))\nfor feature in train_df.columns.to_list():\n    unique_values[feature]=len(train_df[feature].unique())\n    \nunique_df = pd.DataFrame(unique_values, index=['unique_values']).T\nunique_df.T","6b2a61d5":"target_feature = ['Chance of Admit ']\ncategorical_features = ['Research']\ndrop_features = ['Serial No.']\nnumeric_features = list(\n    set(train_df.columns)\n    - set(categorical_features)\n    - set(target_feature)\n    - set(drop_features))\n\nprint(f\"The numeric features are {numeric_features}. \\n\"\n      f\"The categorical features are {categorical_features}. \\n\"\n     f\"The dropped features are  {drop_features} \\n\"\n     f\"The target feature is {target_feature}\")","36c6d331":"corr_df = train_df[numeric_features].corr('spearman').stack().reset_index(name='corr')\ncorr_df.loc[corr_df['corr'] == 1, 'corr'] = 0  # Remove diagonal\n# Use abs so that we can visualize the impact of negative correaltion  \ncorr_df['abs'] = corr_df['corr'].abs()\ncorr_df.sort_values('abs', ascending=False).head()","c7da79db":"alt.Chart(corr_df).mark_circle().encode(\n    x='level_0',\n    y='level_1',\n    size='abs',\n    color=alt.Color('corr',\n                    scale=alt.Scale(scheme='blueorange',\n                                    domain=(-1, 1)))).properties(\n    height=150,\n    width=150)","71187ca0":"alt.Chart(train_df).mark_circle(opacity=0.6).encode(\n    alt.X(alt.repeat(), type='quantitative',scale=alt.Scale(zero=False)),\n    y=alt.Y('Chance of Admit ',scale=alt.Scale(zero=False)),\n    color=alt.Color('count()',scale=alt.Scale(scheme='blueorange',\n                                    domain=(0, 10))),\n    size='count()').repeat(\n    numeric_features,\n    columns=2\n)","d2dd405d":"X_train, y_train = train_df.drop(columns=['Chance of Admit ']), train_df['Chance of Admit ']\nX_test, y_test = test_df.drop(columns=['Chance of Admit ']), test_df['Chance of Admit ']","bbf8c805":"preprocessor= make_column_transformer(\n    (StandardScaler(), numeric_features),\n    (OneHotEncoder(drop=\"if_binary\", dtype=\"int\"), categorical_features),\n    ((\"drop\"),drop_features)\n)\n\nfeature_names = numeric_features + categorical_features","9341f566":"def mean_cross_val_scores(model, X_train, y_train, **kwargs):\n    \"\"\"\n    Returns mean scores of cross validation\n\n    Parameters\n    ----------\n    model :\n        scikit-learn model\n    X_train : numpy array or pandas DataFrame\n        X in the training data\n    y_train :\n        y in the training data\n\n    Returns\n    ----------\n        pandas Series with mean scores from cross_validation\n    \"\"\"\n    \n    scores = cross_validate(model, X_train, y_train, **kwargs)\n\n    mean_scores = pd.DataFrame(scores).mean()\n    out_col = []\n\n    for i in range(len(mean_scores)):\n        out_col.append(round(mean_scores[i], 5))\n\n    return pd.Series(data=out_col, index=mean_scores.index)","f94fcf84":"results = {}\n\nmodels = {\n    'dummy_reg': make_pipeline(preprocessor, DummyRegressor()),\n    'base_linear': make_pipeline(preprocessor, LinearRegression()),\n    'base_ridge': make_pipeline(preprocessor, Ridge(random_state=2018)),\n    'base_lasso': make_pipeline(preprocessor, Lasso(random_state=2018))\n}","34273b72":"results[\"DummyRegressor\"] = mean_cross_val_scores(models['dummy_reg'], X_train, y_train,\n                                                     return_train_score=True)\npd.DataFrame(results)","f70a2218":"results[\"BaseLinear\"] = mean_cross_val_scores(models['base_linear'], X_train, y_train,\n                                                     return_train_score=True)\npd.DataFrame(results)","b2337e70":"results[\"BaseRidge\"] = mean_cross_val_scores(models['base_ridge'], X_train, y_train, \n                                                     return_train_score=True)\npd.DataFrame(results)","6396d7f7":"results[\"BaseLasso\"] = mean_cross_val_scores(models['base_lasso'], X_train, y_train,\n                                                     return_train_score=True)\npd.DataFrame(results)","e4cc42ab":"models['base_lasso'].fit(X_train, y_train);\nfeature_coef_lasso = pd.DataFrame(\n    data={\n        \"Coefficient\": models['base_lasso'].named_steps[\"lasso\"].coef_.flatten()\n    },\n    index=feature_names\n).sort_values(\"Coefficient\", ascending=False)\n\nfeature_coef_lasso","52aa51d0":"param_grid = {\n    \"lasso__alpha\": [pow(10, x) for x in range(-2, 2, 1)]\n}","6f4f8062":"tuned_lasso = GridSearchCV(\n    models['base_lasso'], param_grid, n_jobs=-1, return_train_score=True\n)","16dbdb36":"tuned_lasso.fit(X_train, y_train);\ntuned_lasso.best_params_","ca3f1783":"models['tuned_lasso'] = make_pipeline(preprocessor,\n                                      Lasso(alpha=tuned_lasso.best_params_['lasso__alpha'], random_state=2018))\nresults[\"TunedLasso\"] = mean_cross_val_scores(models['tuned_lasso'], X_train, y_train,\n                                                     return_train_score=True)\nresults_df = pd.DataFrame(results)\nresults_df","1c150d1d":"for model_name, model in models.items():\n    model.fit(X_train, y_train);","b38d7788":"feature_coef_lasso = pd.DataFrame(\n    data={\n        \"Lasso Coefficient\": models['tuned_lasso'].named_steps[\"lasso\"].coef_.flatten()\n    },\n    index=feature_names\n).sort_values(\"Lasso Coefficient\", ascending=False)\n\nfeature_coef_lasso","f4d3b9df":"feature_coef_ridge = pd.DataFrame(\n    data={\n        \"Ridge Coefficient\": models['base_ridge'].named_steps[\"ridge\"].coef_.flatten(),\n    },\n    index=feature_names\n).sort_values(\"Ridge Coefficient\", ascending=False)\n\nfeature_coef_linear = pd.DataFrame(\n    data={\n        \"Linear Coefficient\": models['base_linear'].named_steps[\"linearregression\"].coef_.flatten(),\n    },\n    index=feature_names\n).sort_values(\"Linear Coefficient\", ascending=False)","5799e879":"pd.concat([feature_coef_linear, feature_coef_lasso, feature_coef_ridge], axis=1).sort_values(\"Lasso Coefficient\", ascending=False)","9f110b1b":"index_as_list = results_df.index.tolist()\nidx = index_as_list.index('test_score')\nindex_as_list[idx] = 'validaton_score'\nresults_df.index = index_as_list\nresults_df","198ddad7":"# Caluclating the test scores for all the models\nresults = {'DummyRegressor': models['dummy_reg'].score(X_test, y_test),\n           'BaseLinear': models['base_linear'].score(X_test, y_test),\n           'BaseRidge': models['base_ridge'].score(X_test, y_test),\n           'BaseLasso': models['base_lasso'].score(X_test, y_test),\n           'TunedLasso': models['tuned_lasso'].score(X_test, y_test)\n          }\ntest_results = pd.DataFrame(data=results, index=['test_score'])","f6c96c80":"# Appending the results in the earlier dataframe\nresults_df = results_df.append(test_results)\nresults_df","ae46ab2f":"## 4.2. Ridge or Lasso?\nLasso it is! Why?\n- Ridge Regression uses L-2 regularization to calculate and optimize the weights associated with the features. This method allows features weights to be small but never zero. So, we are not elimintating features in it.\n- On the hand, Lasso uses L-1 regularization, for which feature weights can be zero as well. This acts as a pseudo feature selection model, eliminating some of the mutlicollinear features. Bingo!","db55b0e8":"In the analysis provided below, we are trying to understand the need for Lasso Regression even if it delivers a score which is lesser than Ridge Regression.","87dd4cdb":"# 7. Calculating the final test scores","adfbe26e":"We know that the training data has $400$ examples. So, basis the above table we can that the classification for categorical and numeric values can be done as follows:\n\n| Feature type    | Feature names                    |\n|-----------------|----------------------------------|\n| Numerical       | GRE Score, TOEFL Score, University Rating, SOP, LOR, CGPA            | \n| Categorical     | Research          | \n| Drop            | Serial No.        |\n\nFor the features `SOP`, `LOR` and `CGPA`, the encoding can be done as categorical as well (due to low number of unique values) but since it unnecssarily increases our feature matrix, we avoid it. We are dropping `Serial No.` since it does not give any useful information.\n\nNow, let us write some code to divide these features into seperate list of feature types.","738433ff":"# 7. Interpreting model coefficients\nBefore we interpret the regression coefficients, we need to fit all the models on the training set.","908ae4aa":"- So, it would seem unnecessary to use Lasso Regression here since Ridge gives a better score.\n- But what if we had 7 million features instead of just 7?\n- There arises the need for feature selection as well. Lasso would provide a **faster** model with **less features** and **comparable score**.","96a98154":"# 2. Data splitting","166ecce7":"## 4.3. Relationship between the features and the target","e1986c4f":"## 5.2. Making the column transformer","c5385a17":"## 4.1. Relationship among features","debf45b2":"All the features values are numeric. Still, we need to assess whether we want to treat the feature as numeric or ordinal or unordered categorical feature.","c6d388e6":"## 5.1. Splitting the target feature","aa4a0e82":"Oh wow! We have strongly correlated features in our training set. This is where some statistics is required. This situtation is called multicollinearity. So, when we are using basic Linear Regression on our dataset. We should think of which kind of Linear Regression would be more useful. Since, features are highly correlated, we would like to eliminate some of the unnecessary features which are not adding value to the final model.","c8168d16":"## 6.4. Baseline Lasso Regression","93fb8bec":"## 6.1. DummyRegressor for Baseline","540698f0":"# 1. Reading the dataset","69b56e80":"We can see that the coefficients for `Research` and `SOP` are small in Ridge and Normal Linear Regression but not zero like Lasso.","18e55faf":"# 4. EDA","684bf8b5":"The R2 score of the model is very poor. The validation score is even less than 0 which means that the model is doing as poor as chosing the mean of the target feature as our prediction.","e19f9ff8":"# 8. Comparing Ridge and Lasso","4219cc71":"# 6. Making Linear Models","691e1680":"Now we can see that our tuned Lasso Regression is better than before but the scores are not as good as Ridge or normal Linear Regression. Then, what is the point of using Lasso? To answer, let us explore the regressor coefficients.","0d781b6d":"- I'm grateful that you spent your time reading\/skimming all the way through. \n- Comments\/suggestions\/criticisms on the notebook would be highly appreciated.\n- Check out my other work on [Kaggle](https:\/\/www.kaggle.com\/rrrohit).","b0235650":"From the above graphs, we can observe that our target feature and the input features are linearly correlated.","47146916":"Note: Sklearn calls the validation score outputs from its cross_validate as test score and not validation score. <br> So, below we are renaming the test scores from previous results as validation scores for ease of understanding.","d914445d":"# 0. Importing packages","80fb3c93":"## 6.3. Baseline Ridge Regression","e9335d4f":"|                        | Linear Regression | Ridge                                         | Lasso                              | \n|------------------------|-------------------|-----------------------------------------------|------------------------------------|\n| Regularization         | No                | Yes                                           | Yes                                | \n| Type of regularization | NA                | L2-regularization for least squares           | L1-regularization                       | \n| Bias term | No | Yes | Yes |\n| Overfitting | Yes | decreased due to regularization | decreased due to regularization |\n| Mutlicollinearity      |   use Ridge\/Lasso                | uses information from less important features | eliminates less important features | \n| When to use?           |  - no inference required <br> - rudimentary analysis               | - high number of significant features are present <br> - regularization and we cannot eliminate features | - low number of significant features are there <br> - do feature selection and regularization |","edf5f42f":"We use the best optimized value of alpha = $0.01$ and make a new Lasso model using it.","4bf32e47":"Luckily, we do not have NANs values in the dataset.","ce7b59d7":"# 3. Preliminary EDA","6c3d9de1":"## 5.3. Function to return CV scores in pandas format","579af969":"# 5. Preparation for the model","a637bada":"Oh no! Our base Lasso Regressor is performing as bad as the Dummy Regressor. It is underfitting the training data.\n#### What is wrong here?\nGood thing about Regreesion models is that it is easy to interpret them using their regression coefficients. Let's do that.","eb0ab99b":"## 6.2. Baseline Linear Regression (no regularization)","dfafb545":"## 6.4. Hyperparameter optimization for Lasso\nCheck the entire list of hyperparameter associated with [Lasso](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Lasso.html?highlight=lasso#sklearn.linear_model.Lasso) in Sklearn's website.  \nWe use Grid Search to optimize hyperparamter `alpha` which is the regularization parameter that prevents overfitting.","206b3519":"All the regression coefficients are ZERO! Our default hyperparameter `alpha` of Lasso Regression is regularzing too much. The best practice is to optimize the hyperparameters of the model. Let us do that.","1f4dc886":"We are checking for missing values, scaling issues in the dataframe.","d8e7bd62":"Voila! We can see that `Lasso` has eliminated two of the features which were highly correlated with other features. This performs **feature selection** and **regularization** at the same time."}}