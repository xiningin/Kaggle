{"cell_type":{"d7459e7f":"code","7b1529ae":"code","a9c050d6":"code","fa0a7018":"code","44c841cd":"code","815342c2":"code","652d7e94":"code","a4c10e5c":"code","f23a5b17":"code","1a4bca45":"code","8dce65b6":"code","c10b901a":"code","bc30fa63":"code","08bd6536":"code","808a10a4":"code","40d509e4":"code","e2f55af4":"code","a42c32cc":"code","bf9f6b87":"code","5c758e6e":"code","19dab387":"code","8b46e7a0":"code","93aade4a":"code","31ada2da":"code","2af9539d":"code","83034764":"code","c3fc333b":"code","1abda2ab":"code","3c7cf6fb":"code","9f02919b":"code","9e744283":"code","ab52e8cd":"code","f6618d75":"code","c0c9923d":"code","4b7732c5":"code","46956a79":"code","f9c46304":"code","746ba27f":"code","cb527ecc":"code","4413d42a":"code","0fb8f862":"code","47e4a1dc":"code","279e6d84":"code","1bce8c17":"code","02a0869d":"code","c0ba6d95":"code","011e5aab":"code","aa7ed1e2":"code","d2601036":"code","1ea42f72":"code","32992fe9":"code","77ddb56e":"code","6aaf3e16":"code","96a2de58":"code","176cdddf":"code","e1f89f65":"code","7467a33a":"code","53e39f40":"code","fb66027c":"code","7f188ac0":"code","f0b2463c":"code","30f07e98":"code","ed182bf0":"code","88bbbc84":"code","d26b3331":"code","74ea8a32":"code","51c27b2c":"code","ed4f4d93":"code","57b11e95":"code","f7d15eb9":"code","5a16e168":"code","855d49f2":"code","f92b4bb6":"code","10e0f86b":"code","5790b3a9":"code","794b5dbb":"code","668d256c":"code","4f677c9d":"code","80fb3107":"code","01dcb4cb":"code","426699dc":"code","8023945e":"code","1d33b77a":"code","607826c8":"code","24e2954d":"code","3afbf1c4":"code","b386c32a":"code","1694e154":"code","7690eed6":"code","0895c513":"code","389908be":"code","40e0ee42":"code","a0b1a807":"code","ba32cd17":"code","5e6561d8":"code","5173f744":"code","8f8ec65b":"code","d5033a11":"code","37f6e835":"code","35714419":"code","35c72633":"code","94149a85":"code","43289d2c":"code","12d2be7e":"markdown","1682060f":"markdown","9628c0b9":"markdown","45ad9618":"markdown","264f7d5a":"markdown","9e1dc868":"markdown","41497d48":"markdown","1b724d16":"markdown","72c07495":"markdown","77cbb1b3":"markdown","0d0377b3":"markdown","4f39d4cd":"markdown","a9ee9bb3":"markdown","126ef1e8":"markdown","8436dfd1":"markdown","b4d7f67e":"markdown","a0d96382":"markdown","693903e9":"markdown","65472acf":"markdown","95a6b73b":"markdown","86900af1":"markdown","e240e480":"markdown","69d86832":"markdown"},"source":{"d7459e7f":"#!pip list","7b1529ae":"import gc\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\nfrom pandas import DataFrame, Series\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import roc_auc_score, mean_squared_error, mean_squared_log_error, log_loss\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom tqdm import tqdm_notebook as tqdm\nfrom category_encoders import OrdinalEncoder, OneHotEncoder, TargetEncoder\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\nimport re\nfrom hyperopt import fmin, tpe, hp, rand, Trials","a9c050d6":"#df_train = pd.read_csv('..\/input\/homework-for-students2\/train.csv', index_col=0, parse_dates=['issue_d', 'earliest_cr_line'], skiprows=lambda x: x%20!=0)\n#df_test = pd.read_csv('..\/input\/homework-for-students2\/test.csv', index_col=0, parse_dates=['issue_d', 'earliest_cr_line'], skiprows=lambda x: x%20!=0)\ndf_train = pd.read_csv('..\/input\/homework-for-students2\/train.csv', index_col=0, parse_dates=['issue_d', 'earliest_cr_line'])\ndf_test = pd.read_csv('..\/input\/homework-for-students2\/test.csv', index_col=0, parse_dates=['issue_d', 'earliest_cr_line'])\n#df_train_all = pd.read_csv('..\/input\/homework-for-students2\/train.csv', index_col=0)\n#df_test_all = pd.read_csv('..\/input\/homework-for-students2\/test.csv', index_col=0)","fa0a7018":"#df_train.describe()","44c841cd":"#df_statelatlong = pd.read_csv('..\/input\/homework-for-students2\/statelatlong.csv')#\n#df_statelatlong.columns = ['addr_state', 'state_lat', 'state_long', 'city']\n#df_statelatlong.drop(['city'], axis=1, inplace=True)","815342c2":"#df_gdp = pd.read_csv('..\/input\/homework-for-students2\/US_GDP_by_State.csv')\n\n#df_gdp_2013 = df_gdp[df_gdp['year'] == 2013].reset_index()\n#df_gdp_2013.columns = ['index', 'city', '2013Spending', '2013GSP','2013Growth%', '2013Population', 'year']\n#df_gdp_2013.drop(['index', 'year'], axis=1, inplace=True)\n#df_gdp_2013['2013GSP_per_million'] = df_gdp_2013['2013GSP'] \/ df_gdp_2013['2013Population']\n\n#df_gdp_2014 = df_gdp[df_gdp['year'] == 2014].reset_index()\n#df_gdp_2014.columns = ['index', 'city', '2014Spending', '2014GSP','2014Growth%', '2014Population', 'year']\n#df_gdp_2014.drop(['index', 'year'], axis=1, inplace=True)\n#df_gdp_2014['2014GSP_per_million'] = df_gdp_2014['2014GSP'] \/ df_gdp_2014['2014Population']\n\n#df_gdp_2015 = df_gdp[df_gdp['year'] == 2015].reset_index()\n#df_gdp_2015.columns = ['index', 'city', '2015Spending', '2015GSP','2015Growth%', '2015Population', 'year']\n#df_gdp_2015.drop(['index', 'year'], axis=1, inplace=True)\n#df_gdp_2015['2015GSP_per_million'] = df_gdp_2015['2015GSP'] \/ df_gdp_2015['2015Population']\n\n#df_2013_14 = pd.merge(df_gdp_2013, df_gdp_2014, on='city', how='inner')\n#df_2013_15 = pd.merge(df_2013_14, df_gdp_2015, on='city', how='inner')\n#df_2013_15['Spending_mean'] = (df_2013_15['2013Spending'] + df_2013_15['2014Spending'] + df_2013_15['2015Spending']) \/ 3\n#df_2013_15['GSP_mean'] = (df_2013_15['2013GSP'] + df_2013_15['2014GSP'] + df_2013_15['2015GSP']) \/ 3\n#df_2013_15['Growth%_mean'] = (df_2013_15['2013Growth%'] + df_2013_15['2014Growth%'] + df_2013_15['2015Growth%']) \/ 3\n#df_2013_15['Population_mean'] = (df_2013_15['2013Population'] + df_2013_15['2014Population'] + df_2013_15['2015Population']) \/ 3","652d7e94":"#df_spi = pd.read_csv('..\/input\/homework-for-students2\/spi.csv', parse_dates=['date'])\n#df_spi['issue_d_yyyy'] = df_spi.date.dt.year\n#df_spi['issue_d_mm'] = df_spi.date.dt.month\n#df_spi_monthly = df_spi.groupby(['issue_d_yyyy', 'issue_d_mm'], as_index=False)['close'].mean()\n\n#for j in range(12):\n#    for i in range(df_spi_monthly.shape[0] - j - 1):\n#        df_spi_monthly.loc[i+j+1, 'close-' + str(j+1)] = df_spi_monthly.loc[i, 'close']\n\n#for j in range(12):\n#    for i in range(df_spi_monthly.shape[0]):\n#        df_spi_monthly.loc[i, 'diff-' + str(j+1)] = df_spi_monthly.loc[i, 'close'] - df_spi_monthly.loc[i, 'close-' + str(j+1)]\n\n#for j in (3,6,12):\n#    for i in range(df_spi_monthly.shape[0]-j+1):\n#        tmp = 0\n#        for k in range(j):\n#            tmp += df_spi_monthly.loc[i+j-1-k, 'close']\n#        df_spi_monthly.loc[i+j-1, 'mean-' + str(j)] = tmp \/ j    ","a4c10e5c":"#\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306ehome_ownership\u5217\u306b\u306f'OTHER', 'NONE'\u304c\u306a\u3044\u305f\u3081\u3001'ANY'\u306b\u5909\u63db\ncol = 'home_ownership'\ndf_train[col] = df_train[col].map(lambda x: 'ANY' if x == 'OTHER' or x == 'NONE' else x)\n#df_train_all[col] = df_train_all[col].map(lambda x: 'ANY' if x == 'OTHER' or x == 'NONE' else x)","f23a5b17":"#\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306epurpose\u5217\u306b\u306f'educational'\u304c\u306a\u3044\u305f\u3081\u3001'other'\u306b\u5909\u63db\ncol = 'purpose'\ndf_train[col] = df_train[col].map(lambda x: 'other' if x == 'educational' else x)\n#df_train_all[col] = df_train_all[col].map(lambda x: 'other' if x == 'educational' else x)","1a4bca45":"# open_acc > total_acc \u306e\u30ec\u30b3\u30fc\u30c9\u306e\u9664\u53bb\ndf_train = df_train[(df_train['open_acc'] > df_train['total_acc']) == False] \n#df_train_all = df_train_all[(df_train_all['open_acc'] > df_train_all['total_acc']) == False]","8dce65b6":"#loan_amnt\u306e\u7bc4\u56f2\u3067\u533a\u5206 low: \uff5e$5,000, normal: $5,000\uff5e$25,000, high: $25,000\uff5e$30,000, very high: $30,000\uff5e\ncat_col = 'cat_loan_amnt'\nnum_col = 'loan_amnt'\ndf_train[cat_col] = df_train[num_col].map(lambda x: 'low' if x < 5000\n                                          else 'normal' if x >= 5000 and x < 25000\n                                          else 'high' if x > 25000 and x < 30000\n                                          else 'very high')\ndf_test[cat_col] = df_test[num_col].map(lambda x: 'low' if x < 5000\n                                        else 'normal' if x >= 5000 and x < 25000\n                                        else 'high' if x > 25000 and x < 30000\n                                        else 'very high')","c10b901a":"#annual_inc\u306e\u7bc4\u56f2\u3067\u533a\u5206 very low: \uff5e$35,000, low: $35,000\uff5e$50,000, normal: $50,000\uff5e$75,000, high: $75,000\uff5e$125,000, very high: $125,000\uff5e\ncat_col = 'cat_annual_inc'\nnum_col = 'annual_inc'\ndf_train[cat_col] = df_train[num_col].map(lambda x: 'very low' if x < 35000\n                                          else 'low' if x >= 35000 and x < 50000\n                                          else 'normal' if x >= 50000 and x < 75000\n                                          else 'high' if x >= 75000 and x < 125000\n                                          else 'very high' if x >= 125000\n                                          else 'missing')\ndf_test[cat_col] = df_test[num_col].map(lambda x: 'very low' if x < 35000\n                                        else 'low' if x >= 35000 and x < 50000\n                                        else 'normal' if x >= 50000 and x < 75000\n                                        else 'high' if x >= 75000 and x < 125000\n                                        else 'very high' if x >= 125000\n                                        else 'missing')","bc30fa63":"#cat_col = 'cat_dti'\n#num_col = 'dti'\n#df_train[cat_col] = df_train[num_col].map(lambda x: 'very low' if x < 12\n#                                          else 'low' if x >= 12 and x < 18\n#                                          else 'high' if x >= 18 and x < 24\n#                                          else 'very high' if x >= 24\n#                                          else 'missing')\n#df_test[cat_col] = df_test[num_col].map(lambda x: 'very low' if x < 12\n#                                        else 'low' if x >= 12 and x < 18\n#                                        else 'high' if x >= 18 and x < 24\n#                                        else 'very high' if x >= 24\n#                                       else 'missing')","08bd6536":"cat_col = 'cat_installment'\nnum_col = 'installment'\ndf_train[cat_col] = df_train[num_col].map(lambda x: 'very low' if x < 260\n                                          else 'low' if x >= 260 and x < 380\n                                          else 'high' if x >= 380 and x < 570\n                                          else 'very high')\ndf_test[cat_col] = df_test[num_col].map(lambda x: 'very low' if x < 260\n                                        else 'low' if x >= 260 and x < 380\n                                        else 'high' if x >= 380 and x < 570\n                                        else 'very high')","808a10a4":"cat_col = 'cat_last_record'\nnum_col = 'mths_since_last_record'\ndf_train[cat_col] = df_train[num_col].map(lambda x: 'no record' if x == 0\n                                          else 'recorded' if x > 0\n                                          else 'missing')\ndf_test[cat_col] = df_test[num_col].map(lambda x: 'no record' if x == 0\n                                        else 'recorded' if x > 0\n                                        else 'missing')","40d509e4":"cat_col = 'cat_collections'\nnum_col = 'collections_12_mths_ex_med'\ndf_train[cat_col] = df_train[num_col].map(lambda x: 'no collection' if x == 0\n                                          else 'collected' if x > 0\n                                          else 'missing')\ndf_test[cat_col] = df_test[num_col].map(lambda x: 'no collection' if x == 0\n                                        else 'collected' if x > 0\n                                        else 'missing')","e2f55af4":"#\u5e73\u5747loan_condition\u3067\u533a\u5206 good: 10%\u672a\u6e80, normal:10%\u4ee5\u4e0a25%\u672a\u6e80, bad: 25%\u4ee5\u4e0a\ndf_train['grade2'] = df_train['sub_grade'].map(lambda x: 'good' if re.match(r'A[1-5]', x) or x == 'B1'\n                                           else 'normal' if re.match(r'B[2-5]', x) or re.match(r'C[1-5]', x) or x == 'D1'\n                                           else 'bad')\ndf_test['grade2'] = df_test['sub_grade'].map(lambda x: 'good' if re.match(r'A[1-5]', x) or x == 'B1'\n                                         else 'normal' if re.match(r'B[2-5]', x) or re.match(r'C[1-5]', x) or x == 'D1'\n                                         else 'bad')\n#df_train_all['grade2'] = df_train_all['sub_grade'].map(lambda x: 'good' if re.match(r'A[1-5]', x) or x == 'B1'\n#                                           else 'normal' if re.match(r'B[2-5]', x) or re.match(r'C[1-5]', x) or x == 'D1'\n#                                           else 'bad')\n#df_test_all['grade2'] = df_test_all['sub_grade'].map(lambda x: 'good' if re.match(r'A[1-5]', x) or x == 'B1'\n#                                         else 'normal' if re.match(r'B[2-5]', x) or re.match(r'C[1-5]', x) or x == 'D1'\n#                                         else 'bad')","a42c32cc":"col1 = 'grade'\ncols = ['home_ownership', 'purpose', 'initial_list_status',\n        'cat_loan_amnt', 'cat_annual_inc', 'cat_installment', 'cat_last_record', 'cat_collections']\nfor col2 in cols:\n    col = col1 + '-' + col2\n    df_train[col] = df_train[col1].str.cat(df_train[col2], sep='-')\n    df_test[col] = df_test[col1].str.cat(df_test[col2], sep='-')","bf9f6b87":"df_train['grade-home_ownership'] = df_train['grade-home_ownership'].map(lambda x: 'ANY' if re.match('..ANY', x) else x)\ndf_test['grade-home_ownership'] = df_test['grade-home_ownership'].map(lambda x: 'ANY' if re.match('..ANY', x) else x)","5c758e6e":"df_train['grade-purpose'] = df_train['grade-purpose'].map(lambda x: 'G-other' if x in ('G-car', 'G-wedding', 'G-vacation', 'G-renewable_energy') else x)\ndf_test['grade-purpose'] = df_test['grade-purpose'].map(lambda x: 'G-other' if x in ('G-car', 'G-wedding', 'G-vacation', 'G-renewable_energy') else x)","19dab387":"col1 = 'home_ownership'\ncols = ['purpose', 'initial_list_status', 'application_type',\n        'cat_loan_amnt', 'cat_annual_inc', 'cat_installment', 'cat_last_record', 'cat_collections']\nfor col2 in cols:\n    col = col1 + '-' + col2\n    df_train[col] = df_train[col1].str.cat(df_train[col2], sep='-')\n    df_test[col] = df_test[col1].str.cat(df_test[col2], sep='-')\n    df_train[col] = df_train[col].map(lambda x: 'ANY' if re.match('^ANY-[A-Za-z0-9]+', x) else x)\n    df_test[col] = df_test[col].map(lambda x: 'ANY' if re.match('^ANY-[A-Za-z0-9]+', x) else x)","8b46e7a0":"col = 'home_ownership-cat_collections'\ndf_train[col] = df_train[col].map(lambda x: 'missing' if re.match('^[A-Za-z0-9]+-missing$', x) else x)\ndf_test[col] = df_test[col].map(lambda x: 'missing' if re.match('^[A-Za-z0-9]+-missing$', x) else x)","93aade4a":"col1 = 'purpose'\ncols = ['initial_list_status',\n        'cat_loan_amnt', 'cat_annual_inc', 'cat_installment', 'cat_last_record', 'cat_collections']\nfor col2 in cols:\n    col = col1 + '-' + col2\n    df_train[col] = df_train[col1].str.cat(df_train[col2], sep='-')\n    df_test[col] = df_test[col1].str.cat(df_test[col2], sep='-')","31ada2da":"col = 'purpose-cat_loan_amnt'\ndf_train[col] = df_train[col].map(lambda x: 'other-high' if x in ('vacation-high', 'moving-high ', 'renewable_energy-high', 'wedding-high') \n                                  else 'other-very high' if x in ('vacation-very high', 'renewable_energy-very high')\n                                  else x)\ndf_test[col] = df_test[col].map(lambda x: 'other-high' if x in ('vacation-high', 'moving-high ', 'renewable_energy-high', 'wedding-high') \n                                else 'other-very high' if x in ('vacation-very high', 'renewable_energy-very high')\n                                else x)","2af9539d":"col = 'purpose-cat_collections'\ndf_train[col] = df_train[col].map(lambda x: 'other-collected' if x in ('renewable_energy-collected') \n                                  else x)\ndf_test[col] = df_test[col].map(lambda x: 'other-collected' if x in ('renewable_energy-collected') \n                                else x)","83034764":"col1 = 'initial_list_status'\ncols = ['application_type', 'cat_loan_amnt', 'cat_annual_inc', 'cat_installment', 'cat_last_record', 'cat_collections']\nfor col2 in cols:\n    col = col1 + '-' + col2\n    df_train[col] = df_train[col1].str.cat(df_train[col2], sep='-')\n    df_test[col] = df_test[col1].str.cat(df_test[col2], sep='-')","c3fc333b":"col = 'initial_list_status-cat_last_record'\ndf_train[col] = df_train[col].map(lambda x: 'no record' if re.match('[fw]-no record', x) \n                                  else x)\ndf_test[col] = df_test[col].map(lambda x: 'no record' if re.match('[fw]-no record', x) \n                                else x)","1abda2ab":"col1 = 'application_type'\ncols = ['cat_annual_inc', 'cat_installment']\nfor col2 in cols:\n    col = col1 + '-' + col2\n    df_train[col] = df_train[col1].str.cat(df_train[col2], sep='-')\n    df_test[col] = df_test[col1].str.cat(df_test[col2], sep='-')","3c7cf6fb":"col = 'application_type-cat_annual_inc'\ndf_train[col] = df_train[col].map(lambda x: 'Joint App-high' if x == 'Joint App-very high' \n                                  else x)\ndf_test[col] = df_test[col].map(lambda x: 'Joint App-high' if x == 'Joint App-very high'  \n                                else x)","9f02919b":"col1 = 'cat_loan_amnt'\ncols = ['cat_annual_inc', 'cat_installment', 'cat_last_record', 'cat_collections']\nfor col2 in cols:\n    col = col1 + '-' + col2\n    df_train[col] = df_train[col1].str.cat(df_train[col2], sep='-')\n    df_test[col] = df_test[col1].str.cat(df_test[col2], sep='-')","9e744283":"col = 'cat_loan_amnt-cat_annual_inc'\ndf_train[col] = df_train[col].map(lambda x: 'very high-low' if x == 'high-low' \n                                  else 'normal-very low' if x in ('high-very low', 'very high-very low')\n                                  else x)\ndf_test[col] = df_test[col].map(lambda x: 'very high-low' if x == 'high-low' \n                                else 'normal-very low' if x in ('high-very low', 'very high-very low')\n                                else x)","ab52e8cd":"col = 'cat_loan_amnt-cat_installment'\ndf_train[col] = df_train[col].map(lambda x: 'very high-low' if x == 'high-low' \n                                  else 'normal-very low' if x in ('high-very low', 'very high-very low')\n                                  else x)\ndf_test[col] = df_test[col].map(lambda x: 'very high-low' if x == 'high-low' \n                                else 'normal-very low' if x in ('high-very low', 'very high-very low')\n                                else x)","f6618d75":"col1 = 'cat_annual_inc'\ncols = ['cat_installment', 'cat_last_record', 'cat_collections']\nfor col2 in cols:\n    col = col1 + '-' + col2\n    df_train[col] = df_train[col1].str.cat(df_train[col2], sep='-')\n    df_test[col] = df_test[col1].str.cat(df_test[col2], sep='-')","c0c9923d":"#col1 = 'cat_dti'\n#cols = ['cat_installment', 'cat_last_record', 'cat_collections']\n#for col2 in cols:\n#    col = col1 + '-' + col2\n#    df_train[col] = df_train[col1].str.cat(df_train[col2], sep='-')\n#    df_test[col] = df_test[col1].str.cat(df_test[col2], sep='-')","4b7732c5":"col1 = 'cat_installment'\ncols = ['cat_last_record', 'cat_collections']\nfor col2 in cols:\n    col = col1 + '-' + col2\n    df_train[col] = df_train[col1].str.cat(df_train[col2], sep='-')\n    df_test[col] = df_test[col1].str.cat(df_test[col2], sep='-')","46956a79":"col1 = 'cat_last_record'\ncol2 = 'cat_collections'\ncol = col1 + '-' + col2\ndf_train[col] = df_train[col1].str.cat(df_train[col2], sep='-')\ndf_test[col] = df_test[col1].str.cat(df_test[col2], sep='-')","f9c46304":"# \u6b20\u640d\u306e\u7dcf\u6570\ncol = 'num_nulls'\ndf_train[col] = df_train.isnull().sum(axis=1)\ndf_test[col] = df_test.isnull().sum(axis=1)","746ba27f":"# \u6b20\u640d\u30d5\u30e9\u30b0(\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u6b20\u640d\u304c\u3042\u308b\u3082\u306e\u306e\u307f)\ncols = ['emp_title', 'emp_length', 'title', 'dti', 'inq_last_6mths', 'mths_since_last_delinq',\n       'mths_since_last_record', 'revol_util', 'mths_since_last_major_derog']\nfor col in cols:\n    flag_name = col + '_isnull'\n    df_train[flag_name] = df_train[col].map(lambda x: 1 if pd.isnull(x) else 0) \n    df_test[flag_name] = df_test[col].map(lambda x: 1 if pd.isnull(x) else 0)","cb527ecc":"# \u6b20\u640d\u30d1\u30bf\u30fc\u30f3(\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u6b20\u640d\u304c\u3042\u308b\u3082\u306e\u306e\u307f)\ncol1 = 'missing_pattern'\ncols = ['emp_length_isnull', 'title_isnull', 'dti_isnull', 'inq_last_6mths_isnull', 'mths_since_last_delinq_isnull',\n       'mths_since_last_record_isnull', 'revol_util_isnull', 'mths_since_last_major_derog_isnull']\n\ndf_train[col1] = df_train['emp_title_isnull'].astype(str)\ndf_test[col1] = df_test['emp_title_isnull'].astype(str)\nfor col2 in cols:\n    df_train[col1] = df_train[col1].str.cat(df_train[col2].astype(str))\n    df_test[col1] = df_test[col1].str.cat(df_test[col2].astype(str))\n","4413d42a":"#df_test['missing_pattern'].value_counts()","0fb8f862":"y_train = df_train.loan_condition\nX_train = df_train.drop(['loan_condition'], axis=1)\n\nX_test = df_test","47e4a1dc":"X_all = pd.concat([X_train, X_test])","279e6d84":"X_all_desc = X_all.describe()","1bce8c17":"#X_all_desc","02a0869d":"#\u5185\u5bb9\u306e\u78ba\u8a8d(count: null\u3067\u306a\u3044\u884c\u6570\u3001mean: \u5e73\u5747\u5024\u3001std: \u6a19\u6e96\u504f\u5dee\u3001min\uff1a\u6700\u5c0f\u5024\u300125%\uff1a25%\u30bf\u30a4\u30eb\u5024\u300150%: \u4e2d\u592e\u5024\u300175%: \u30bf\u30a4\u30eb\u5024\u3001max: \u6700\u5927\u5024)\n#X_all_desc.loc['count', 'loan_amnt']","c0ba6d95":"X_all_mode = X_all.mode()","011e5aab":"#\u5185\u5bb9\u306e\u78ba\u8a8d \u6700\u983b\u5024\n#X_all_mode.loc[0,'loan_amnt']","aa7ed1e2":"col = 'issue_d'\nX_train['issue_d_yyyyq'] = X_train[col].dt.year * 10 + X_train[col].dt.quarter \nX_train['issue_d_yyyymm'] = X_train[col].dt.year * 100 + X_train[col].dt.month \nX_train['issue_d_yyyy'] = X_train[col].dt.year\nX_train['issue_d_q'] = X_train[col].dt.quarter\nX_train['issue_d_mm'] = X_train[col].dt.month\nX_test['issue_d_yyyyq'] = X_test[col].dt.year * 10 + X_test[col].dt.quarter \nX_test['issue_d_yyyymm'] = X_test[col].dt.year * 100 + X_test[col].dt.month \nX_test['issue_d_yyyy'] = X_test[col].dt.year\nX_test['issue_d_q'] = X_test[col].dt.quarter\nX_test['issue_d_mm'] = X_test[col].dt.month","d2601036":"col = 'earliest_cr_line'\nX_train['earliest_cr_line_yyyyq'] = X_train[col].map(lambda x: -1 if pd.isnull(x) else x.year * 10 + x.quarter) \nX_train['earliest_cr_line_yyyymm'] = X_train[col].map(lambda x: -1 if pd.isnull(x) else x.year * 100 + x.month) \nX_train['earliest_cr_line_yyyy'] = X_train[col].map(lambda x: -1 if pd.isnull(x) else x.year)\nX_train['earliest_cr_line_q'] = X_train[col].map(lambda x: -1 if pd.isnull(x) else x.quarter)\nX_train['earliest_cr_line_mm'] = X_train[col].map(lambda x: -1 if pd.isnull(x) else x.month)\nX_test['earliest_cr_line_yyyyq'] = X_test[col].dt.year * 10 + X_test[col].dt.quarter \nX_test['earliest_cr_line_yyyymm'] = X_test[col].dt.year * 100 + X_test[col].dt.month \nX_test['earliest_cr_line_yyyy'] = X_test[col].dt.year\nX_test['earliest_cr_line_q'] = X_test[col].dt.quarter\nX_test['earliest_cr_line_mm'] = X_test[col].dt.month","1ea42f72":"# \u30ed\u30fc\u30f3\u767a\u884c\u65e5\u307e\u3067\u306e\u53e3\u5ea7\u958b\u8a2d\u671f\u9593(\u65e5)\ncol = 'from_first_cr_line'\nX_train[col] = (X_train['issue_d'] - X_train['earliest_cr_line']).dt.days\nX_test[col] = (X_test['issue_d'] - X_test['earliest_cr_line']).dt.days\nX_train[col].fillna(-1, inplace=True)\nX_train[col].fillna(-1, inplace=True)","32992fe9":"#\u30af\u30ea\u30c3\u30d4\u30f3\u30b0\nX_train['earliest_cr_line_yyyy'].clip(1974, 2019, axis=0, inplace=True)\nX_test['earliest_cr_line_yyyy'].clip(1974, 2019, axis=0, inplace=True)\n\nX_train['earliest_cr_line_yyyymm'].clip(197412, 201901, axis=0, inplace=True)\nX_test['earliest_cr_line_yyyymm'].clip(197412, 201901, axis=0, inplace=True)\n\nX_train['earliest_cr_line_yyyyq'].clip(19744, 20191, axis=0, inplace=True)\nX_test['earliest_cr_line_yyyyq'].clip(19744, 20191, axis=0, inplace=True)","77ddb56e":"#X_train = pd.merge(X_train, df_spi_monthly, on=['issue_d_yyyy', 'issue_d_mm'], how='inner')\n#X_test = pd.merge(X_test, df_spi_monthly, on=['issue_d_yyyy', 'issue_d_mm'], how='inner')","6aaf3e16":"#X_train = pd.merge(X_train, df_statelatlong, on='addr_state', how='inner')\n#X_test = pd.merge(X_test, df_statelatlong, on='addr_state', how='inner')","96a2de58":"#X_train = pd.merge(X_train, df_2013_15, on='city', how='inner')\n#X_test = pd.merge(X_test, df_2013_15, on='city', how='inner')","176cdddf":"#X_train.drop(['city'], axis=1, inplace=True)\n#X_test.drop(['city'], axis=1, inplace=True)","e1f89f65":"#X_train.drop(['state_lat', 'state_long'], axis=1, inplace=True)\n#X_test.drop(['state_lat', 'state_long'], axis=1, inplace=True)","7467a33a":"#df_gdp_2013.columns = ['index', 'city', '2013Spending', '2013GSP','2013Growth%', '2013Population', 'year']\n#X_train.drop(['2013Spending', '2013GSP','2013Growth%', '2013Population', '2013GSP_per_million'], axis=1, inplace=True)\n#X_test.drop(['2013Spending', '2013GSP','2013Growth%', '2013Population', '2013GSP_per_million'], axis=1, inplace=True)","53e39f40":"TXT_emp_title_train = X_train.emp_title.copy()\nTXT_emp_title_test = X_test.emp_title.copy()\n\nTXT_title_train = X_train.title.copy()\nTXT_title_test = X_test.title.copy()\n\nX_train.drop(['emp_title', 'title'], axis=1, inplace=True)\nX_test.drop(['emp_title', 'title'], axis=1, inplace=True)","fb66027c":"cols = ['grade', 'sub_grade', 'addr_state', 'zip_code', 'cat_loan_amnt', 'cat_annual_inc', 'cat_installment', 'cat_last_record', 'cat_collections', 'grade2',\n        'grade-home_ownership', 'grade-purpose', 'grade-initial_list_status', 'grade-cat_loan_amnt', 'grade-cat_annual_inc', \n        'grade-cat_installment', 'grade-cat_last_record', 'grade-cat_collections',\n        'home_ownership-purpose', 'home_ownership-initial_list_status', 'home_ownership-application_type', 'home_ownership-cat_loan_amnt',\n        'home_ownership-cat_annual_inc', 'home_ownership-cat_installment', 'home_ownership-cat_last_record', 'home_ownership-cat_collections',\n        'purpose-initial_list_status', 'purpose-cat_loan_amnt', 'purpose-cat_annual_inc', 'purpose-cat_installment', 'purpose-cat_last_record', 'purpose-cat_collections',\n        'initial_list_status-application_type', 'initial_list_status-cat_loan_amnt', 'initial_list_status-cat_annual_inc', 'initial_list_status-cat_installment',\n        'initial_list_status-cat_last_record', 'initial_list_status-cat_collections',\n        'application_type-cat_annual_inc', 'application_type-cat_installment',\n        'cat_loan_amnt-cat_annual_inc', 'cat_loan_amnt-cat_installment', 'cat_loan_amnt-cat_last_record', 'cat_loan_amnt-cat_collections',\n        'cat_annual_inc-cat_installment', 'cat_annual_inc-cat_last_record', 'cat_annual_inc-cat_collections',\n        'cat_installment-cat_last_record', 'cat_installment-cat_collections', 'cat_last_record-cat_collections']\ntarget = 'loan_condition'\nX_temp = pd.concat([X_train, y_train], axis=1)\n\nfor col in cols:\n    # X_test\u306fX_train\u3067\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3059\u308b\n    summary = X_temp.groupby([col])[target].mean()\n    enc_test = X_test[col].map(summary) \n\n    # X_train\u306e\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3092oof\u3067\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3059\u308b\n    skf = StratifiedKFold(n_splits=5, random_state=71, shuffle=True)\n\n\n    enc_train = Series(np.zeros(len(X_train)), index=X_train.index)\n\n    for i, (train_ix, val_ix) in enumerate((skf.split(X_train, y_train))):\n        X_train_, _ = X_temp.iloc[train_ix], y_train.iloc[train_ix]\n        X_val, _ = X_temp.iloc[val_ix], y_train.iloc[val_ix]\n\n        summary = X_train_.groupby([col])[target].mean()\n        enc_train.iloc[val_ix] = X_val[col].map(summary)\n\n    enc_train[enc_train.isnull()] = 0.17447602091409\n    enc_test[enc_test.isnull()] = 0.17447602091409\n    \n    X_train[col] = enc_train\n    X_test[col] = enc_test","7f188ac0":"col = 'emp_length'\nencoder = OrdinalEncoder(mapping=[{'col':col,'mapping':{'< 1 year':0, '1 year':1, '2 years':2, '3 years':3, '4 years':4, '5 years':5, \n                                                        '6 years':6, '7 years':7, '8 years':8, '9 years':9, '10+ years':10}}], return_df=True)\nX_train = encoder.fit_transform(X_train)\nX_test = encoder.transform(X_test)","f0b2463c":"cols = ['home_ownership', 'purpose', \n#        'loan_amnt_range', 'annual_inc_range'\n       ] \n#ohe = OneHotEncoder(cols=cols, handle_unknown='indicator', use_cat_names=True)\nohe = OneHotEncoder(cols=cols, use_cat_names=True)\n\nenc_train = ohe.fit_transform(X_train[cols])\nenc_test = ohe.transform(X_test[cols])","30f07e98":"X_train = pd.concat([X_train, enc_train], axis=1)\nX_test = pd.concat([X_test, enc_test], axis=1)","ed182bf0":"#cols = ['missing_pattern'] \n#ohe = OneHotEncoder(cols=cols, handle_unknown='indicator', use_cat_names=True)\n#ohe = OneHotEncoder(cols=cols, use_cat_names=True)\n\n#enc_train = ohe.fit_transform(X_train[cols])\n#enc_test = ohe.transform(X_test[cols])","88bbbc84":"#X_train = pd.concat([X_train, enc_train], axis=1)\n#X_test = pd.concat([X_test, enc_test], axis=1)","d26b3331":"cols = ['home_ownership', 'purpose', 'initial_list_status', 'application_type', 'missing_pattern']\nfor col in cols:\n    summary = X_all[col].value_counts()\n    X_train[col] = X_train[col].map(summary)\n    X_test[col] = X_test[col].map(summary)","74ea8a32":"#X_all['home_ownership'].value_counts()","51c27b2c":"#X_test['home_ownership'].value_counts()","ed4f4d93":"# dtype\u304cobject\u306e\u30ab\u30e9\u30e0\u540d\u3068\u30e6\u30cb\u30fc\u30af\u6570\u3092\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n#cats = []\n#for col in X_train.columns:\n#    if X_train[col].dtype == 'object':\n#        cats.append(col)\n        \n#        print(col, X_train[col].nunique())","57b11e95":"#X_train.drop(['cat_dti'], axis=1, inplace=True)\n#X_test.drop(['cat_dti'], axis=1, inplace=True)","f7d15eb9":"cols = ['loan_amnt', 'installment', 'revol_bal', 'tot_coll_amt', 'tot_cur_bal']\ncol1 = 'annual_inc'\n\nfor col in cols:\n    new_col = col + '_to_' + col1 + '_ratio'\n\n    X_train[new_col] = X_train[col] \/ X_train[col1]\n    X_test[new_col] = X_test[col] \/ X_test[col1]\n    \n    #\u5206\u6bcd\u3001\u5206\u5b50\u306e\u3044\u305a\u308c\u304b\u304cNull\u3001\u307e\u305f\u306f0\/0\u306e\u3068\u304d\u306f-1\n    X_train[new_col].fillna(-1, inplace=True)\n    X_test[new_col].fillna(-1, inplace=True)\n    \n    #\u5206\u6bcd\u304c0\u3001\u5206\u5b50\u304c0\u4ee5\u5916\u306e\u6570\u5024\u306e\u5834\u5408\u306f999999\n    X_train[new_col].replace([np.inf, -np.inf], np.nan, inplace=True)\n    X_test[new_col].replace([np.inf, -np.inf], np.nan, inplace=True)\n    X_train[new_col].fillna(999999, inplace=True)\n    X_test[new_col].fillna(999999, inplace=True)","5a16e168":"cols = ['open_acc', 'acc_now_delinq']\ncol1 = 'total_acc'\n\nfor col in cols:\n    new_col = col + '_to_' + col1 + '_ratio'\n\n    X_train[new_col] = X_train[col] \/ X_train[col1]\n    X_test[new_col] = X_test[col] \/ X_test[col1]\n    \n    #\u5206\u6bcd\u3001\u5206\u5b50\u306e\u3044\u305a\u308c\u304b\u304cNull\u3001\u307e\u305f\u306f0\/0\u306e\u3068\u304d\u306f-1\n    X_train[new_col].fillna(-1, inplace=True)\n    X_test[new_col].fillna(-1, inplace=True)\n    \n    #\u5206\u6bcd\u304c0\u3001\u5206\u5b50\u304c0\u4ee5\u5916\u306e\u6570\u5024\u306e\u5834\u5408\u306f9999\n    X_train[new_col].replace([np.inf, -np.inf], np.nan, inplace=True)\n    X_test[new_col].replace([np.inf, -np.inf], np.nan, inplace=True)\n    X_train[new_col].fillna(9999, inplace=True)\n    X_test[new_col].fillna(9999, inplace=True)","855d49f2":"#for col in X_train.columns:\n#    print (col, X_train[col].isnull().sum())","f92b4bb6":"X_train.drop(['issue_d', 'earliest_cr_line'], axis=1, inplace=True)\nX_test.drop(['issue_d', 'earliest_cr_line'], axis=1, inplace=True)\n    \nX_train['mths_since_last_record'].fillna(X_all_mode.loc[0, 'mths_since_last_record'], inplace=True)\nX_test['mths_since_last_record'].fillna(X_all_mode.loc[0, 'mths_since_last_record'], inplace=True)\n\nX_train.fillna(X_all_desc.loc['50%',], inplace=True)\nX_test.fillna(X_all_desc.loc['50%',], inplace=True)\n\n#X_train.fillna(-9999, inplace=True)\n#X_test.fillna(-9999, inplace=True)","10e0f86b":"#X_train.drop(['title_isnull', 'dti_isnull', 'inq_last_6mths_isnull', 'revol_util_isnull'], axis=1, inplace=True)\n#X_test.drop(['title_isnull', 'dti_isnull', 'inq_last_6mths_isnull', 'revol_util_isnull'], axis=1, inplace=True)","5790b3a9":"#\u5bc6\u884c\u5217\u306e\u72b6\u614b\u3092\u4fdd\u5b58\nX_train_base = X_train\nX_test_base = X_test","794b5dbb":"TXT_emp_title_train.fillna('#', inplace=True)\nTXT_emp_title_test.fillna('#', inplace=True)\n\nTXT_title_train.fillna('#', inplace=True)\nTXT_title_test.fillna('#', inplace=True)","668d256c":"TXT_emp_title_train_base = TXT_emp_title_train\nTXT_emp_title_test_base = TXT_emp_title_test\nTXT_title_train_base = TXT_title_train\nTXT_title_test_base = TXT_title_test","4f677c9d":"tfidf_emp_title = TfidfVectorizer(max_features=200, use_idf=True)\ntfidf_title = TfidfVectorizer(max_features=150, use_idf=True)","80fb3107":"TXT_emp_title_train = tfidf_emp_title.fit_transform(TXT_emp_title_train_base)\nTXT_emp_title_test = tfidf_emp_title.transform(TXT_emp_title_test_base)\n\nTXT_title_train = tfidf_title.fit_transform(TXT_title_train_base)\nTXT_title_test = tfidf_title.transform(TXT_title_test_base)","01dcb4cb":"X_train_1 = sp.sparse.hstack([X_train_base.values, TXT_emp_title_train])\nX_test_1 = sp.sparse.hstack([X_test_base.values, TXT_emp_title_test])\n\nX_train_2 = sp.sparse.hstack([X_train_1, TXT_title_train])\nX_test_2 = sp.sparse.hstack([X_test_1, TXT_title_test])\n\nX_train = X_train_2.tocsr()\nX_test = X_test_2.tocsr()","426699dc":"#issue_d\u3067\u5b8c\u5168\u306b\u533a\u5225\u3067\u304d\u308b\u306e\u3067AUC1.0\u306b\u3002issue_d\u5916\u3057\u3066\u3084\u308b\u6642\u9593\u306a\u304b\u3063\u305f\u3002\n#X = sp.sparse.vstack([X_train, X_test], format='csr')\n#y = np.concatenate([np.zeros(X_train.shape[0]), np.ones(X_test.shape[0])])\n\n#scores = []\n\n#y_pred_test_av = np.zeros(X.shape[0])\n#cv_iteration = 0\n\n#skf = StratifiedKFold(n_splits=5, random_state=71, shuffle=True)\n\n#for i, (train_ix, test_ix) in enumerate(tqdm(skf.split(X, y))):\n#    X_train_, y_train_ = X_train.values[train_ix], y_train.values[train_ix]\n#    X_val, y_val = X_train.values[test_ix], y_train.values[test_ix]\n#    X_, y_ = X[train_ix], y[train_ix]\n#    X_val, y_val = X[test_ix], y[test_ix]\n    \n#    clf = LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.9,\n#                     importance_type='split', learning_rate=0.05, max_depth=-1,\n#                     min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n#                     n_estimators=9999, n_jobs=-1, num_leaves=15, objective=None,\n#                     random_state=71, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n#                     subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n    \n#    clf.fit(X_, y_, early_stopping_rounds=20, eval_metric='binary_logloss', eval_set=[(X_val, y_val)])\n#    y_pred = clf.predict_proba(X_val)[:,1]\n#    score = roc_auc_score(y_val, y_pred)\n#    scores.append(score)\n    \n#    y_pred_test_av += clf.predict_proba(X)[:,1]\n#    cv_iteration += clf.best_iteration_\n#    print('CV Score of Fold_%d is %f' % (i, score))","8023945e":"#print(np.mean(scores))\n#print(scores)\n#y_pred_test_av \/= 5\n","1d33b77a":"#X_train_pred = y_pred_test_av[:X_train.shape[0]]\n","607826c8":"#X_train_alike = X_train[np.argsort(X_train_pred[:, 0])][:X_train.shape[0] \/\/ 2]\n#y_train_alike = y_train[np.argsort(X_train_pred[:, 0])][:X_train.shape[0] \/\/ 2]\n","24e2954d":"#def objective(space):\n#    scores = []\n\n#    skf = StratifiedKFold(n_splits=5, random_state=71, shuffle=True)\n\n#    for i, (train_ix, test_ix) in enumerate(tqdm(skf.split(X_train, y_train))):\n#        X_train_, y_train_ = X_train[train_ix], y_train.values[train_ix]\n#        X_val, y_val = X_train[test_ix], y_train.values[test_ix]\n\n#        clf = LGBMClassifier(n_estimators=9999, **space) \n\n#        clf.fit(X_train_, y_train_, early_stopping_rounds=20, eval_metric='auc', eval_set=[(X_val, y_val)])\n#        y_pred = clf.predict_proba(X_val)[:,1]\n#        score = roc_auc_score(y_val, y_pred)\n#        scores.append(score)\n        \n#    scores = np.array(scores)\n#    print(scores.mean())\n    \n#    return -scores.mean()","3afbf1c4":"#space ={\n#        'max_depth': hp.choice('max_depth', np.arange(10, 30, dtype=int)),\n#        'subsample': hp.uniform ('subsample', 0.8, 1),\n#        'learning_rate' : hp.quniform('learning_rate', 0.025, 0.5, 0.025),\n#        'colsample_bytree' : hp.quniform('colsample_bytree', 0.5, 1, 0.05)\n#    }","b386c32a":"#trials = Trials()\n\n#best = fmin(fn=objective,\n#              space=space, \n#              algo=tpe.suggest,\n#              max_evals=30, \n#              trials=trials, \n#              rstate=np.random.RandomState(71) \n#             )","1694e154":"# CV\u3057\u3066\u30b9\u30b3\u30a2\u3092\u898b\u3066\u307f\u308b\u3002\u5c64\u5316\u62bd\u51fa\u3067\u826f\u3044\u304b\u306f\u5225\u9014\u3088\u304f\u8003\u3048\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002\n#scores = []\n#y_pred_test_gbc = np.zeros(X_test.shape[0])\n\n#skf = StratifiedKFold(n_splits=5, random_state=71, shuffle=True)\n\n##skf.split(X_train_2, y_train)\n#for i, (train_ix, test_ix) in tqdm(enumerate(skf.split(X_train, y_train))):\n#    X_train_, y_train_ = X_train[train_ix], y_train.values[train_ix]\n#    X_val, y_val = X_train[test_ix], y_train.values[test_ix]\n        \n#    clf = GradientBoostingClassifier(learning_rate=0.15, n_estimators=70, min_samples_split=500, max_depth=6, min_samples_leaf=70, max_features=25, subsample=0.7)\n    \n#    clf.fit(X_train_, y_train_)\n#    y_pred = clf.predict_proba(X_val)[:,1]\n#    score = roc_auc_score(y_val, y_pred)\n#    scores.append(score)\n\n#    y_pred_test_gbc += clf.predict_proba(X_test)[:,1]\n\n#    print('CV Score of Fold_%d is %f' % (i, score))","7690eed6":"#print(np.mean(scores))\n#print(scores)\n#y_pred_test_gbc \/= 5","0895c513":"# \u5168\u30c7\u30fc\u30bf\u3067\u518d\u5b66\u7fd2\u3057\u3001test\u306b\u5bfe\u3057\u3066\u4e88\u6e2c\u3059\u308b\n#clf.fit(X_train, y_train)\n#y_pred_gbc = clf.predict_proba(X_test)[:,1]","389908be":"#LGBMClassifier(**best)","40e0ee42":"%%time\n# CV\u3057\u3066\u30b9\u30b3\u30a2\u3092\u898b\u3066\u307f\u308b\n# \u306a\u304a\u3001\u305d\u3082\u305d\u3082StratifiedKFold\u304c\u9069\u5207\u306a\u306e\u304b\u306f\u5225\u9014\u8003\u3048\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\nscores = []\ny_pred_test = np.zeros(X_test.shape[0])\ncv_iteration = 0\n\nskf = StratifiedKFold(n_splits=5, random_state=71, shuffle=True)\n\nfor i, (train_ix, test_ix) in enumerate(tqdm(skf.split(X_train, y_train))):\n#    X_train_, y_train_ = X_train.values[train_ix], y_train.values[train_ix]\n#    X_val, y_val = X_train.values[test_ix], y_train.values[test_ix]\n    X_train_, y_train_ = X_train[train_ix], y_train.values[train_ix]\n    X_val, y_val = X_train[test_ix], y_train.values[test_ix]\n    \n    clf = LGBMClassifier(boosting_type='gbdt', class_weight=None,\n               colsample_bytree=0.6000000000000001, importance_type='split',\n               learning_rate=0.025, max_depth=-1, min_child_samples=20,\n               min_child_weight=0.001, min_split_gain=0.0, n_estimators=9999,\n               n_jobs=-1, num_leaves=31, objective=None, random_state=71,\n               reg_alpha=0.0, reg_lambda=0.0, silent=True,\n               subsample=0.8638066621345636, subsample_for_bin=200000,\n               subsample_freq=0)\n    \n    clf.fit(X_train_, y_train_, early_stopping_rounds=20, eval_metric='auc', eval_set=[(X_val, y_val)])\n    y_pred = clf.predict_proba(X_val)[:,1]\n    score = roc_auc_score(y_val, y_pred)\n    scores.append(score)\n    \n    y_pred_test += clf.predict_proba(X_test)[:,1]\n    cv_iteration += clf.best_iteration_\n    print('CV Score of Fold_%d is %f' % (i, score))","a0b1a807":"print(np.mean(scores))\nprint(scores)\ny_pred_test \/= 5\ncv_iteration = (cv_iteration \/\/ 5) + 1","ba32cd17":"cv_iteration","5e6561d8":"#fi = clf.booster_.feature_importance(importance_type='gain')\n#for i in range(100):\n#    print(i,  fi[i])\n\n#for i in range(len(fi)):\n#    if fi[i] == 0:\n#        print(i)\n\n#idx = np.argsort(fi)[::-1]\n#print(idx[fi == 0])\n#top_cols, top_importances = idx[:100], fi[idx[:100]]\n#print(top_cols, top_importances)","5173f744":"fig, ax = plt.subplots(figsize=(5, 30))\nlgb.plot_importance(clf, max_num_features=100, ax=ax, importance_type='gain')","8f8ec65b":"#for i in range(X_train_base.shape[1]):\n#    print(i, X_train_base.columns[i])","d5033a11":"# \u5168\u30c7\u30fc\u30bf\u3067\u518d\u5b66\u7fd2\u3057\u3001test\u306b\u5bfe\u3057\u3066\u4e88\u6e2c\u3059\u308b\nclf = LGBMClassifier(boosting_type='gbdt', class_weight=None,\n               colsample_bytree=0.6000000000000001, importance_type='split',\n               learning_rate=0.025, max_depth=-1, min_child_samples=20,\n               min_child_weight=0.001, min_split_gain=0.0, n_estimators=int(cv_iteration),\n               n_jobs=-1, num_leaves=31, objective=None, random_state=71,\n               reg_alpha=0.0, reg_lambda=0.0, silent=True,\n               subsample=0.8638066621345636, subsample_for_bin=200000,\n               subsample_freq=0)\nclf.fit(X_train, y_train, eval_metric='auc')\ny_pred = clf.predict_proba(X_test)[:,1]","37f6e835":"#fi = clf.booster_.feature_importance(importance_type='gain')\n\n#for i in range(len(fi)):\n#    if fi[i] == 0:\n#        print(i)","35714419":"fig, ax = plt.subplots(figsize=(5, 30))\nlgb.plot_importance(clf, max_num_features=100, ax=ax, importance_type='gain')","35c72633":"# \u3053\u3061\u3089\u3082\u30b9\u30e0\u30fc\u30ba\u306a\u9032\u884c\u306e\u305f\u3081\u306b20\u5206\u306e\uff11\u306b\u9593\u5f15\u3044\u3066\u3044\u307e\u3059\u304c\u3001\u672c\u756a\u3067\u306f\"skiprows=lambda x: x%20!=0\"\u3092\u524a\u9664\u3057\u3066\u7528\u3044\u3066\u304f\u3060\u3055\u3044\u3002\n#submission = pd.read_csv('..\/input\/homework-for-students2\/sample_submission.csv', index_col=0, skiprows=lambda x: x%20!=0)\n#submission = pd.read_csv('..\/input\/homework-for-students2\/sample_submission.csv', index_col=0)\n\n#submission.loan_condition = (y_pred + y_pred_test + y_pred_gbc + y_pred_test_gbc) \/ 4\n#submission.to_csv('submission_1126_ensemble_all_1.csv')","94149a85":"len(y_pred)","43289d2c":"submission = pd.read_csv('..\/input\/homework-for-students2\/sample_submission.csv', index_col=0)\n\nsubmission.loan_condition = (y_pred + y_pred_test) \/ 2\nsubmission.to_csv('submission_1126_ensemble_lgbm_1.csv')","12d2be7e":"## \u30ab\u30c6\u30b4\u30ea\u8ffd\u52a0","1682060f":"## Target \u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0","9628c0b9":"## One-Hot Encoding","45ad9618":"## \u30ab\u30ec\u30f3\u30c0\u30fc\u60c5\u5831\u306e\u62bd\u51fa","264f7d5a":"## \u6b20\u640d\u5024\u307e\u308f\u308a\uff08\u88dc\u5b8c\u306f\u5f8c\u3067\uff09","9e1dc868":"## Ordinal Encoding","41497d48":"# Adversarial Validation ","1b724d16":"## \u30c6\u30ad\u30b9\u30c8\u7279\u5fb4\u91cf\u306e\u5206\u96e2","72c07495":"## \u6bd4\u7387\u306e\u8a08\u7b97","77cbb1b3":"## \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u306e\u4fee\u6b63","0d0377b3":"# AI Academy Homework Stage 2","4f39d4cd":"# \u4e88\u6e2c\u30e2\u30c7\u30eb\u4f5c\u6210","a9ee9bb3":"## \u30c6\u30ad\u30b9\u30c8\u7279\u5fb4\u91cf","126ef1e8":"## \u6b20\u640d\u5024\u306e\u88dc\u5b8c","8436dfd1":"## \u30b5\u30d6\u30c7\u30fc\u30bf\u3068\u306e\u7d50\u5408\uff08\u3057\u306a\u3044\uff09","b4d7f67e":"# LightGBM","a0d96382":"# hyperopt\u306b\u3088\u308b\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0","693903e9":"# GradientBoostingClassifier\u3067\u3082\u4e88\u6e2c(Stage1\u306e\u907a\u7523)","65472acf":"## Count Encoding","95a6b73b":"# \u30b5\u30d6\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f(\u4e0d\u8981)","86900af1":"# \u7279\u5fb4\u91cf\u30a8\u30f3\u30b8\u30cb\u30a2\u30ea\u30f3\u30b0","e240e480":"## \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u3068\u691c\u8a3c\u30c7\u30fc\u30bf\u3092\u7d71\u5408\u3057\u3066\u96c6\u8a08","69d86832":"# \u5143\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f"}}