{"cell_type":{"0a082c0e":"code","bdb915b4":"code","3c66eb91":"code","c3c808d6":"code","4d6f77de":"code","02da51bf":"code","2904dab5":"code","0cb5c546":"code","c2b41c41":"code","5b386923":"code","47110101":"code","7e3d3483":"code","45f3924e":"code","5a12c7ed":"code","6a42738a":"code","bb7d51e6":"code","3f68404f":"code","d5519377":"code","6155f791":"code","0672ea8f":"code","a0359850":"code","df9ec6d1":"code","539f8e77":"code","d8e0661a":"code","99e687b2":"code","11fa1346":"code","88dce284":"code","abcc546a":"code","af6b776c":"code","df27924d":"code","91a7cc24":"code","abb15c9a":"code","406561c9":"code","c4a2842d":"code","e3349a77":"code","146131f1":"code","e6f14bf7":"markdown","25800b21":"markdown","92874c1d":"markdown","f832ecd4":"markdown","1e14c9d3":"markdown","c3b42b6d":"markdown","61307ab6":"markdown","424f5687":"markdown","8438d741":"markdown","1ed9c67c":"markdown","12fde0cd":"markdown","3f5d6f8a":"markdown","084f3dfb":"markdown","cc52e3b2":"markdown","c4fcc849":"markdown","71c004c4":"markdown","6ae5e961":"markdown","22a98e3e":"markdown","d809a830":"markdown","87ecc51a":"markdown","42a72814":"markdown","2a4edff4":"markdown","aa995c49":"markdown","a9ee1e71":"markdown","8546ea02":"markdown","a5e6873c":"markdown","17e3a7e0":"markdown","f4d467d1":"markdown","6397c6eb":"markdown","ea5a5cb1":"markdown","074b3130":"markdown","411f71a8":"markdown","2729800f":"markdown","b3bc7690":"markdown"},"source":{"0a082c0e":"#Import libraries for analysis\n\n#Analysis \nimport pandas as pd\nfrom pandas import Series,DataFrame\nimport numpy as np\nimport sklearn\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Visualisations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\n\n#Split model data and Accuracy Score \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error, make_scorer\n\n\n# Model Imports\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom scipy.stats import skew\n","bdb915b4":"#Import datasets for analysis\n\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\n#split out the SalePrice from the training set as the target variable\ny=train['SalePrice']\n\n#Combine the datasets to scub and explore data. We will split it before modelling by position using iloc on the dataframe\ncombined_df=train.append(test)","3c66eb91":"combined_df.head()","c3c808d6":"#missing data\nMissing_values_null = combined_df.isnull().sum()\nMissing_values_total = combined_df.isnull().count()\nPercent=Missing_values_null\/Missing_values_total\nprint(Percent.sort_values(ascending=False))","4d6f77de":"#Based on the above, we'll drop the first 4 entries\ncombined_df.drop(['PoolQC','MiscFeature','Alley','Fence'],axis=1,inplace=True)","02da51bf":"#Check distribution of SalePrice \nsns.displot(combined_df['SalePrice'],kde=True);","2904dab5":"y=np.log1p(train['SalePrice'])","0cb5c546":"\ncombined_df[\"SalePrice\"] = np.log1p(combined_df[\"SalePrice\"])\n#Check distribution of SalePrice \nsns.displot(combined_df['SalePrice'],kde=True);","c2b41c41":"# Correlation of numeric features to SalesPrice\nprint(\"Correlation of numeric features to SalesPrice\")\ncorr = combined_df.corr()\ncorr.sort_values([\"SalePrice\"], ascending = False, inplace = True)\nprint(corr.SalePrice)","5b386923":"#PairPlot\nsns.set()\ncolumns = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath']\nsns.pairplot(combined_df[columns], diag_kind=\"hist\", size=2)\nplt.show();","47110101":"#log transform skewed numeric features:\nnumeric_feats = combined_df.dtypes[combined_df.dtypes != \"object\"].index\n\n#computing skewness of features\nskewed_feats = combined_df[numeric_feats].apply(lambda x: skew(x.dropna()))\n#using the skewness \nskewed_feats = skewed_feats[skewed_feats > 0.5]\nskewed_feats = skewed_feats.index\n\ncombined_df[skewed_feats] = np.log1p(combined_df[skewed_feats])","7e3d3483":"combined_df=pd.get_dummies(combined_df)","45f3924e":"combined_df.fillna(combined_df.mean(),inplace=True)","5a12c7ed":"#Split the combined_df back into test and training sets (PassengerId starts at 1, but iloc Index starts at 0)\ntrain_split = combined_df.iloc[:1460,:]\ntest_split = combined_df.iloc[1460:,:]\n\n#Splitting out the X train and test dataset for submission\nX=train_split.drop(['Id','SalePrice'],axis=1)\nX_submission=test_split.drop(['Id','SalePrice'],axis=1)\n\n#y has been previously transformed in a step above\n# split into train test sets. Note that this \"test\" is within the train dataset, to evaluate model fit on unseen data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","6a42738a":"def rmse_cv_train(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring = \"neg_mean_squared_error\", cv = 5))\n    return(rmse)\n\ndef rmse_cv_test(model):\n    rmse= np.sqrt(-cross_val_score(model, X_test, y_test, scoring = \"neg_mean_squared_error\", cv = 5))\n    return(rmse)","bb7d51e6":"model_lr = LinearRegression()\nmodel_lr.fit(X_train, y_train)\n\n# Look at predictions on training and validation set\nprint(\"RMSE on Training set :\", rmse_cv_train(model_lr).mean())\nprint(\"RMSE on Test set :\", rmse_cv_test(model_lr).mean())\ny_train_pred = model_lr.predict(X_train)\ny_test_pred = model_lr.predict(X_test)","3f68404f":"resid_train= y_train_pred - y_train\nresid_test= y_test_pred - y_test\n\nplt.scatter(y_train_pred, resid_train, label = \"Training data\")\nplt.scatter(y_test_pred, resid_test,  label = \"Test data\")\nplt.title(\"Residuals vs Fitted: Linear Regression\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10, xmax = 14, color = \"red\")\nplt.show()","d5519377":"import statsmodels.api as sm\nsm.qqplot(y_train_pred, fit=True, line='45')\nplt.title(\"QQ Plot Train: Linear Regression\")\nsm.qqplot(y_test_pred, fit=True, line='45')\nplt.title(\"QQ Plot Test: Linear Regression\")\nplt.show()","6155f791":"# Plot most and leas important coefficients in the model (as taken from the Regularized Linear Models notebook)\ncoef = pd.Series(model_lr.coef_, index = X_train.columns)\nimp_coef = pd.concat([coef.sort_values().head(10),coef.sort_values().tail(10)])\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Linear Regression Model\")\nplt.show()","0672ea8f":"#Add model and regularisation parameters\nmodel_ridge = RidgeCV(alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75])\n\nmodel_ridge.fit(X_train, y_train)\n\n#Estimated regularization parameter (alpha_)\n\nalpha = model_ridge.alpha_\n\nprint(\"Estimated regularization alpha:\",alpha)\n","a0359850":"# Look at predictions on training and validation set\nprint(\"RMSE on Training set :\", rmse_cv_train(model_ridge).mean())\nprint(\"RMSE on Test set :\", rmse_cv_test(model_ridge).mean())\ny_train_pred = model_ridge.predict(X_train)\ny_test_pred = model_ridge.predict(X_test)","df9ec6d1":"resid_train= y_train_pred - y_train\nresid_test= y_test_pred - y_test\n\nplt.scatter(y_train_pred, resid_train, label = \"Training data\")\nplt.scatter(y_test_pred, resid_test,  label = \"Test data\")\nplt.title(\"Residuals vs Fitted: Ridge Regression\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10, xmax = 14, color = \"red\")\nplt.show()","539f8e77":"import statsmodels.api as sm\nsm.qqplot(y_train_pred, fit=True, line='45')\nplt.title(\"QQ Plot Train: Ridge Regression\")\nsm.qqplot(y_test_pred, fit=True, line='45')\nplt.title(\"QQ Plot Test: Ridge Regression\")\nplt.show()","d8e0661a":"# Plot most and least important coefficients in the model (as taken from the Regularized Linear Models notebook)\ncoef = pd.Series(model_ridge.coef_, index = X_train.columns)\nimp_coef = pd.concat([coef.sort_values().head(10),coef.sort_values().tail(10)])\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Ridge Regression Model\")\nplt.show()","99e687b2":"#Add model and regularisation parameters\nmodel_lasso = LassoCV(alphas = [0.1,0.3, 0.5, 0.8, 1])\n\nmodel_lasso.fit(X_train, y_train)\n\n#Estimated regularization parameter (alpha_)\n\nalpha = model_lasso.alpha_\n\nprint(\"Estimated regularization alpha:\",alpha)","11fa1346":"# Look at predictions on training and validation set\nprint(\"RMSE on Training set :\", rmse_cv_train(model_lasso).mean())\nprint(\"RMSE on Test set :\", rmse_cv_test(model_lasso).mean())\ny_train_pred = model_lasso.predict(X_train)\ny_test_pred = model_lasso.predict(X_test)","88dce284":"resid_train= y_train_pred - y_train\nresid_test= y_test_pred - y_test\n\nplt.scatter(y_train_pred, resid_train, label = \"Training data\")\nplt.scatter(y_test_pred, resid_test,  label = \"Test data\")\nplt.title(\"Residuals vs Fitted: Lasso Regression\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10, xmax = 14, color = \"red\")\nplt.show()","abcc546a":"import statsmodels.api as sm\nsm.qqplot(y_train_pred, fit=True, line='45')\nplt.title(\"QQ Plot Train: Lasso Regression\")\nsm.qqplot(y_test_pred, fit=True, line='45')\nplt.title(\"QQ Plot Test: Lasso Regression\")\nplt.show()","af6b776c":"# Plot most and least important coefficients in the model (as taken from the Regularized Linear Models notebook)\ncoef = pd.Series(model_lasso.coef_, index = X_train.columns)\nimp_coef = pd.concat([coef.sort_values().head(10),coef.sort_values().tail(10)])\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Regression Model\")\nplt.show()","df27924d":"model_elasticNet = ElasticNetCV(l1_ratio = [0.1,0.3, 0.5, 0.8, 1],\n                          alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, \n                                    0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                          max_iter = 50000, cv = 10)\nmodel_elasticNet.fit(X_train, y_train)\nalpha = model_elasticNet.alpha_\nratio = model_elasticNet.l1_ratio_\nprint(\"Estimated l1_ratio :\", ratio)\nprint(\"Estimated alpha :\", alpha )","91a7cc24":"# Look at predictions on training and validation set\nprint(\"RMSE on Training set :\", rmse_cv_train(model_elasticNet).mean())\nprint(\"RMSE on Test set :\", rmse_cv_test(model_elasticNet).mean())\ny_train_pred = model_elasticNet.predict(X_train)\ny_test_pred = model_elasticNet.predict(X_test)","abb15c9a":"resid_train= y_train_pred - y_train\nresid_test= y_test_pred - y_test\n\nplt.scatter(y_train_pred, resid_train, label = \"Training data\")\nplt.scatter(y_test_pred, resid_test,  label = \"Test data\")\nplt.title(\"Residuals vs Fitted: ElasticNet\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10, xmax = 14, color = \"red\")\nplt.show()","406561c9":"import statsmodels.api as sm\nsm.qqplot(y_train_pred, fit=True, line='45')\nplt.title(\"QQ Plot Train: ElasticNet\")\nsm.qqplot(y_test_pred, fit=True, line='45')\nplt.title(\"QQ Plot Test: ElasticNet\")\nplt.show()","c4a2842d":"# Plot most and least important coefficients in the model (as taken from the Regularized Linear Models notebook)\ncoef = pd.Series(model_lasso.coef_, index = X_train.columns)\nimp_coef = pd.concat([coef.sort_values().head(10),coef.sort_values().tail(10)])\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the ElasticNet Model\")\nplt.show()","e3349a77":"yhat_unseen = model_elasticNet.predict(X_submission)\n\nX=test_split[\"Id\"]\n\n#Create a dataframe and a csv for submission\nelasticNet_submission = pd.DataFrame({\n        \"Id\": X,\n        \"SalePrice\": np.expm1(yhat_unseen)\n    })\n\nelasticNet_submission.to_csv('submission.csv',index=False)","146131f1":"elasticNet_submission.head()","e6f14bf7":"## Import libraries and dataset","25800b21":"This [article](https:\/\/machinelearningmastery.com\/elastic-net-regression-in-python\/) has a good description of the ElasticNet model approach, which also attributes to [The Elements of Statistical Learning](https:\/\/web.stanford.edu\/~hastie\/Papers\/ESLII.pdf): \n\n\"Alpha assigns how much weight is given to each of the L1 and L2 penalties. Alpha is a value between 0 and 1 and is used to weight the contribution of the L1 penalty and one minus the alpha value is used to weight the L2 penalty:\n\n    elastic_net_penalty = (alpha * l1_penalty) + ((1 \u2013 alpha) * l2_penalty)\n\nFor example, an alpha of 0.5 would provide a 50 percent contribution of each penalty to the loss function. An alpha value of 0 gives all weight to the L2 penalty and a value of 1 gives all weight to the L1 penalty.\n\nThe benefit is that elastic net allows a balance of both penalties, which can result in better performance than a model with either one or the other penalty on some problems.\n\nAnother hyperparameter is provided called \u201clambda\u201d that controls the weighting of the sum of both penalties to the loss function. A default value of 1.0 is used to use the fully weighted penalty; a value of 0 excludes the penalty. Very small values of lambda, such as 1e-3 or smaller, are common.\n\n    elastic_net_loss = loss + (lambda * elastic_net_penalty) \n\"\n\n","92874c1d":"L2 penalises the sum of square weights (large values). However is not robust to outliers, and cannot be used for feature selection, but can be used for complex models. Taken from this [article](https:\/\/julienbeaulieu.gitbook.io\/wiki\/sciences\/machine-learning\/linear-regression\/regularization):\n\n\n\"This sheds light on the obvious disadvantage of ridge regression, which is model interpretability. It will shrink the coefficients for least important predictors, very close to zero. But it will never make them exactly zero. In other words, the final model will include all predictors.\"","f832ecd4":"Regularisation helps prevent model overfitting by penalising extreme parameter weights. This [article](https:\/\/medium.datadriveninvestor.com\/l1-l2-regularization-7f1b4fe948f2) provide a useful non academic view of the difference between Ridge and Lasso methods. \n\nElasticNet combines the benefits from both Ridge and Lasso models, and includes both L1 and L2 penalties during model training.","1e14c9d3":"At this stage, we're going to do some data exploration and preprocessing. Let's start by dropping features with many missing values (> 20% missing features here), excl. SalePrice","c3b42b6d":"### Linear Regression","61307ab6":"#### Ridge (L2) Regularisation","424f5687":"RMSE for Ridge is lower than that for Linear Regression. ","8438d741":"L1 penalises the sum of the absolute value of weights. It is robust to outliers and can be used for feature selection, but provides a simpler more interpretable model. Taken from this [article](https:\/\/julienbeaulieu.gitbook.io\/wiki\/sciences\/machine-learning\/linear-regression\/regularization):\n\n\"In the case of the lasso, the L1 penalty has the e\ufb00ect of forcing some of the coe\ufb03cient estimates to be exactly equal to zero when the tuning parameter \u03bb is su\ufb03ciently large. Therefore, the lasso method also performs variable selection and is said to yield sparse models.\"","1ed9c67c":"## Data Modelling","12fde0cd":"Plot residuals against model fit for the training and test data predictions","3f5d6f8a":"In addition, we will bulk log transform any skewed numeric features closer to a normal distribution before modelling. The logic here has come pretty much from this [notebook](https:\/\/www.kaggle.com\/ljr38687\/regularized-linear-models). A general (non-academic) reference I used for understanding skewness numbers is [here](https:\/\/help.gooddata.com\/doc\/en\/reporting-and-dashboards\/maql-analytical-query-language\/maql-expression-reference\/aggregation-functions\/statistical-functions\/predictive-statistical-use-cases\/normality-testing-skewness-and-kurtosis), which we will use to define which variables should be log transformed.\n\n* If skewness is less than -1 or greater than 1, the distribution is highly skewed.\n* If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed.\n* If skewness is between -0.5 and 0.5, the distribution is approximately symmetric.\n\nBased on this, any feature > 0.5 will be log transformed to approximate a normal distribution.\n\n","084f3dfb":"Split the model out into test and training sets. The target SalePrice has been transformed earlier, represented as y","cc52e3b2":"Training and test data errors appear to be randomly distributed, and so the model appears to be a good fit ","c4fcc849":"We can use RMSE to assess the fit of the models, due to reasons described [here](https:\/\/www.theanalysisfactor.com\/assessing-the-fit-of-regression-models\/):\n\n\"The RMSE is the square root of the variance of the residuals. It indicates the absolute fit of the model to the data\u2013how close the observed data points are to the model\u2019s predicted values. Whereas R-squared is a relative measure of fit, RMSE is an absolute measure of fit. As the square root of a variance, RMSE can be interpreted as the standard deviation of the unexplained variance, and has the useful property of being in the same units as the response variable. Lower values of RMSE indicate better fit. RMSE is a good measure of how accurately the model predicts the response, and it is the most important criterion for fit if the main purpose of the model is prediction\"","71c004c4":"The target variable of SalePrice is non-normal and skew. We will log transform the target variable closer to a normal distribution. ","6ae5e961":"Let's run a pairplot with some of the high correlation examples\n\nFirstly though, do some of these correlations make intuitive sense? \n\n1. OverallQual: Rates the overall material and finish of the house (Seems feasible as correlated to SalePrice)\n2. GrLivArea: Above grade (ground) living area square feet (The larger the house, the greater the SalePrice)\n3. GarageCars: Size of garage in car capacity (Seems feasible. Would GarageCars and GarageArea be collinear?) \n4. GarageArea: Size of garage in square feet (Seems feasible. Would GarageCars and GarageArea be collinear?)\n5. TotalBsmtSF: Total square feet of basement area (I'm surprised on this one, but basements are not a concept in South \nAfrica, but the dataset is from the US)\n6. FullBath: Full bathrooms above grade (Makes sense, more full bathrooms would have an effect on SalePrice) \n","22a98e3e":"#### Lasso (L1 ) Regularisation","d809a830":"Plotting QQ Plots for training and test data predictions to check normality assumptions","87ecc51a":"Lets check the shape of the target variable","42a72814":"### Regularisation (Ridge, Lasso and ElasticNet)","2a4edff4":"Based on our predictions fitting closely to the red line for both training and test datasets, it appears (after log transformation) that data is normally distributed.","aa995c49":"Visually, there appears to be some outliers related to GrLivArea and TotalBsmtSF. \n\nWithin our regression models to be tested later our choices of simple Linear Regression and L2 (Ridge) Regularisation are not robust to outliers. L1 (Lasso) Regularisation is however more robust in the presence of outliers. Elastic Net combines both L1 and L2 models.    \n\n","a9ee1e71":"The goal of this notebook is to get an understanding of how to apply regression models to the Ames housing dataset. I want to apply linear regression as well as regularisation methods (Ridge, Lasso and ElasticNet).\n\nSome of the sources I'm using include [*The Elements of Statistical Learning*](https:\/\/web.stanford.edu\/~hastie\/Papers\/ESLII.pdf) for theory (Note this links to the free pdf download by Hastie, Tibshirani and Friedman), as well as a couple of helpful Kaggle notebooks [here](https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models) and [here](https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset)","8546ea02":"## Introduction","a5e6873c":"## Data Exploration and Preprocessing","17e3a7e0":"At this point, we need to fill in any missing values. I've attempted to use an iterative imputer in a [prior notebook](https:\/\/www.kaggle.com\/ljr38687\/classification-using-python-titanic-dataset) I built, but I will keep it much simpler in this case and fill in the mean of each column. \n","f4d467d1":"Based on the lowest RMSE score, the ElasticNet will be the model used for submission","6397c6eb":"#### ElasticNet","ea5a5cb1":"Both Residuals vs Fitted and QQ Plots look reasonable in terms of model fit for Ridge Regression. ","074b3130":"### Submission File","411f71a8":"We need to encode categorical variables into dummy variables. I found this [post](https:\/\/stackoverflow.com\/questions\/36631163\/what-are-the-pros-and-cons-between-get-dummies-pandas-and-onehotencoder-sciki) useful to describe how to handle categorical data using either get_dummies or onehotencoder. \n\nBecause we're not having to worry about applying our model to new datasets which may have unseen new levels within a variable (e.g. the training set only has the variable location = \"London\", \"Sydney\" and the production data has an additional location= \"Oslo\"), get_dummies is simpler. In a production setting however, onehotencoder creates a function that persists and may give more control over a scenario like this.    ","2729800f":"There's a lot of predictor variables in this dataset, so I'm going to run a correlation to see which have a strong relationship with the target variable, \"SalePrice\".  \n\n\nWe will still however run a correlation before transformation on numeric features (see [here](https:\/\/stats.stackexchange.com\/questions\/3730\/pearsons-or-spearmans-correlation-with-non-normal-data) for an explanation on correlations on non-normal data and assumptions) to get a high level understanding of some of the feature relationships.","b3bc7690":"We will try a number of different alphas for our regularization parameter. In general, the higher the regularization parameter, the less prone to overfitting."}}