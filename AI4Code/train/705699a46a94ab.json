{"cell_type":{"1aa947c6":"code","8f329648":"code","f4069c94":"code","d405046a":"code","afb59eab":"code","747433c5":"code","13469281":"code","b937d6c9":"code","820a7663":"code","580f97a6":"code","fb4565e2":"code","b27e38ab":"markdown"},"source":{"1aa947c6":"P = {}\nP['EPOCHS'] = 70\nP['BACKBONE'] = 'efficientnetb4'\n# P['BACKBONE'] = 'inceptionv3' #https:\/\/iopscience.iop.org\/article\/10.1088\/1742-6596\/1544\/1\/012196\/pdf\n# P['BACKBONE'] = 'resnet34'\n\n# P['NFOLDS'] = 4\nP['NFOLDS'] = 1\nP['SEED'] = 0 # for reproducibility\nP['VERBOSE'] = 0\nP['DISPLAY_PLOT'] = True \nP['BATCH_COE'] = 16 #16#8, 4 # BATCH_SIZE = P['BATCH_COE'] * strategy.num_replicas_in_sync #16 is best\n\nP['TILING'] = [1024,512] # 1024,512 1024,256 1024,128 1536,512 768,384\nP['DIM'] = P['TILING'][1] \nP['DIM_FROM'] = P['TILING'][0]\n\nP['LR'] = 5e-4 \nP['OVERLAPP'] = True\nP['STEPS_COE'] = 3\n\nimport yaml\nwith open(r'params.yaml', 'w') as file:\n    yaml.dump(P, file)","8f329648":"! pip install segmentation_models -q\n%matplotlib inline\n\nimport os\nos.environ['SM_FRAMEWORK'] = 'tf.keras'\nimport glob\nimport segmentation_models as sm\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import get_custom_objects\n\nfrom kaggle_datasets import KaggleDatasets\nprint(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE","f4069c94":"# ENABLE TPU\ntry:\n    # detect and initialize tpu\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Connecting to tpu...')\n    print('device running at:', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    print('Initializing TPU...')\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    # instantiate a distribution strategy\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print(\"TPU initialized\")\nelse:\n    print('Using deafualt strategy...')\n    strategy = tf.distribute.get_strategy()\n\nBATCH_SIZE = P['BATCH_COE'] * strategy.num_replicas_in_sync\n\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)\nprint(\"BATCH_SIZE: \", str(BATCH_SIZE))","d405046a":"# TFREC_PATH = \"..\/input\/hubmap-tf-with-tpu-efficientunet-512x512-tfrecs\"\n# ALL_TRAINING_FILENAMES = tf.io.gfile.glob(TFREC_PATH + '\/train\/*.tfrec')\n\nGCS_PATH = KaggleDatasets().get_gcs_path(f'hubmap-tfrecords-1024-{P[\"DIM\"]}')\nALL_TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/train\/*.tfrec')\n\nALL_TRAINING_FILENAMES","afb59eab":"import re\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\nprint('NUM_TRAINING_IMAGES:' )\nprint(count_data_items(ALL_TRAINING_FILENAMES))","747433c5":"DIM = P['DIM']\ndef _parse_image_function(example_proto, augment = True):\n    image_feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'mask': tf.io.FixedLenFeature([], tf.string)\n    }\n    single_example = tf.io.parse_single_example(example_proto, image_feature_description)\n    image = tf.reshape( tf.io.decode_raw(single_example['image'],out_type=np.dtype('uint8')), (DIM,DIM, 3))\n    mask =  tf.reshape(tf.io.decode_raw(single_example['mask'],out_type='bool'),(DIM,DIM,1))\n    \n    if augment: # https:\/\/www.kaggle.com\/kool777\/training-hubmap-eda-tf-keras-tpu\n        # Using augmentation suggestions from Gallego, Pedraza, Lopez\n        if tf.random.uniform(()) > 0.5:\n            image = tf.image.flip_left_right(image)\n            mask = tf.image.flip_left_right(mask)\n\n        # https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/image\n        if tf.random.uniform(()) > 0.5:\n            image = tf.image.flip_up_down(image)\n            mask = tf.image.flip_up_down(mask)\n        \n        # Rotate 90 degrees\n        if tf.random.uniform(()) > 0.5:\n            image = tf.image.rot90(image, k=1)\n            mask = tf.image.rot90(mask, k=1)\n            \n#         # Rotate 270 degrees\n#         if tf.random.uniform(()) > 0.5:\n#             image = tf.image.rot90(image, k=3)\n#             mask = tf.image.rot90(mask, k=3)\n\n        # Color modification\n        if tf.random.uniform(()) > 0.45:\n            image = tf.image.random_saturation(image, 0.7, 1.3)\n\n        if tf.random.uniform(()) > 0.45:\n            image = tf.image.random_contrast(image, 0.8, 1.2)\n    \n    return tf.cast(image, tf.float32),tf.cast(mask, tf.float32)\n\ndef load_dataset(filenames, ordered=False, augment = True):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n    dataset = dataset.with_options(ignore_order)\n    dataset = dataset.map(lambda ex: _parse_image_function(ex, augment = augment), num_parallel_calls=AUTO)\n    return dataset\n\ndef get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES)\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(128, seed = P['SEED'])\n    dataset = dataset.batch(BATCH_SIZE,drop_remainder=True)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_validation_dataset(ordered=True):\n    dataset = load_dataset(VALIDATION_FILENAMES, ordered=ordered, augment = False)\n    dataset = dataset.batch(BATCH_SIZE,drop_remainder=True)\n    #dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTO)\n    return dataset","13469281":"# https:\/\/tensorlayer.readthedocs.io\/en\/latest\/_modules\/tensorlayer\/cost.html#dice_coe\ndef dice_coe(output, target, axis = None, smooth=1e-10):\n    output = tf.dtypes.cast( tf.math.greater(output, 0.5), tf. float32 )\n    target = tf.dtypes.cast( tf.math.greater(target, 0.5), tf. float32 )\n    inse = tf.reduce_sum(output * target, axis=axis)\n    l = tf.reduce_sum(output, axis=axis)\n    r = tf.reduce_sum(target, axis=axis)\n\n    dice = (2. * inse + smooth) \/ (l + r + smooth)\n    dice = tf.reduce_mean(dice, name='dice_coe')\n    return dice\n\n# # soft dice loss\n# def dice_loss(y_true, y_pred):\n#     loss = 1 - dice_coe(y_pred, y_true)\n#     return loss\n\n# https:\/\/www.kaggle.com\/bigironsphere\/loss-function-library-keras-pytorch#BCE-Dice-Loss\ndef dice_loss(y_true, y_pred, axis = None, smooth=1e-10):\n#     y_pred = tf.dtypes.cast( tf.math.greater(y_pred, 0.5), tf. float32 ) # THIS IS NOT DIFFERENTIABLE!!!\n#     y_true = tf.dtypes.cast( tf.math.greater(y_true, 0.5), tf. float32 )\n    inse = tf.reduce_sum(y_pred * y_true, axis=axis)\n    l = tf.reduce_sum(y_pred, axis=axis)\n    r = tf.reduce_sum(y_true, axis=axis)\n\n    dice = (2. * inse + smooth) \/ (l + r + smooth)\n    dice = tf.reduce_mean(dice)\n    \n    return 1 - dice\n\n# Dice loss with the standard binary cross-entropy (BCE) loss\n# def DiceBCELoss(targets, inputs, smooth=1):    \n#     #flatten label and prediction tensors\n#     inputs = K.flatten(inputs)\n#     targets = K.flatten(targets)\n    \n#     BCE = tf.keras.losses.binary_crossentropy(targets, inputs)\n#     intersection = K.sum(K.dot(targets, inputs))    \n#     dice_loss = 1 - (2*intersection + smooth) \/ (K.sum(targets) + K.sum(inputs) + smooth)\n#     Dice_BCE = BCE + dice_loss\n    \n#     return Dice_BCE\n\n# https:\/\/www.kaggle.com\/kool777\/training-hubmap-eda-tf-keras-tpu\n# tversky loss\ndef tversky(y_true, y_pred, alpha=0.7, beta=0.3, smooth=1):\n    y_true_pos = K.flatten(y_true)\n    y_pred_pos = K.flatten(y_pred)\n    true_pos = K.sum(y_true_pos * y_pred_pos)\n    false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n    false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n    return (true_pos + smooth) \/ (true_pos + alpha * false_neg + beta * false_pos + smooth)\n\ndef tversky_loss(y_true, y_pred):\n    return 1 - tversky(y_true, y_pred)\n\n# https:\/\/arxiv.org\/pdf\/2006.14822.pdf <- should work the best\ndef focal_tversky_loss(y_true, y_pred, gamma=0.75):\n    tv = tversky(y_true, y_pred)\n    return K.pow((1 - tv), gamma)\n\n\ndef FocalLoss(targets, inputs, alpha=0.8, gamma=2):    \n    \n    inputs = K.flatten(inputs)\n    targets = K.flatten(targets)\n    \n    BCE = K.binary_crossentropy(targets, inputs)\n    BCE_EXP = K.exp(-BCE)\n    focal_loss = K.mean(alpha * K.pow((1-BCE_EXP), gamma) * BCE)\n    \n    return focal_loss\n\n# Try Dice + topK loss\n\n# Try Dice + Focal loss\ndef dice_focal(ytrue, ypred, gamma=0.75):\n    dl = dice_loss(ytrue, ypred)\n    return K.pow((1 - dl), gamma)\n\ndef dice_focal_tversky(y_true, y_pred, gamma=0.75):\n    tv = tversky(y_true, y_pred)\n    return K.pow((1 - tv), gamma) + dice_loss(y_true, y_pred)\n\nget_custom_objects().update({\"dice_loss\": dice_loss})\n# get_custom_objects().update({\"tversky\": tversky_loss})\n# get_custom_objects().update({\"focal_tversky\": focal_tversky_loss})","b937d6c9":"# Courtesy of: https:\/\/www.kaggle.com\/paulorzp\/rle-functions-run-lenght-encode-decode\ndef decode_image_and_mask(image, mask, augment=True):\n    \n    '''\n    decode image and mask in order to\n    feed data to TPU.\n    --------------------------------\n    \n    Arguments:\n    image -- patches of huge tiff file in png format.\n    mask -- patches of mask in png format.\n    augment -- apply augmentations on images and masks.\n    \n    Return:\n    image \n    mask\n    '''\n    \n    # load raw data as string\n    image = tf.io.read_file(image)\n    mask = tf.io.read_file(mask)\n    \n    image = tf.io.decode_png(image, channels=3)  # convert compressed string to 3D uint8 tensor\n    mask = tf.io.decode_png(mask)  # convert compressed string to uinst8 tensor\n    \n    if augment:\n        \n        if tf.random.uniform(()) > 0.5:\n            image = tf.image.flip_left_right(image)\n            mask = tf.image.flip_left_right(mask)\n            \n        if tf.random.uniform(()) > 0.5:\n            image = tf.image.flip_up_down(image)\n            mask = tf.image.flip_up_down(mask)\n            \n        if tf.random.uniform(()) > 0.5:\n            image = tf.image.rot90(image, k=1)\n            mask = tf.image.rot90(mask, k=1)\n            \n#         # Rotate 270 degrees\n#         if tf.random.uniform(()) > 0.5:\n#             image = tf.image.rot90(image, k=3)\n#             mask = tf.image.rot90(mask, k=3)        \n            \n        if tf.random.uniform(()) > 0.45:\n            image = tf.image.random_saturation(image, 0.7, 1.3)\n            \n        if tf.random.uniform(()) > 0.45:\n            image = tf.image.random_contrast(image, 0.8, 1.2)\n    \n    image = tf.image.convert_image_dtype(image, tf.float32) # convert to floats in the [0,1] range\n    mask = tf.cast(mask, tf.float32)  # convert to floats 1. and 0.\n\n    image = tf.reshape(image, [*IMAGE_DIM, 3])  # reshaping image tensor\n    mask = tf.reshape(mask, [*IMAGE_DIM]) # reshaping mask tensor\n    \n    return image, mask\n\ndef generate_data(tiff, masks, batch_size=16, shuffle=True):\n    \n    '''\n    generate batches of tf.Dataset\n    object\n    --------------------------------\n    \n    Arguments:\n    tiff -- tf.data.Dataset object (tf.Tensor)\n    mask -- tf.data.Dataset object (tf.Tensor)\n    batch_size -- batches of image, mask pair\n    shuffle -- generate train if True or validation data if False\n    \n    Return:\n    ds - tf.data.Dataset dataset \n    '''\n    \n    \n    ds = Dataset.zip((tiff, masks)) # create dataset by zipping (image, mask) into pair\n    ds = ds.map(decode_image_and_mask, num_parallel_calls=AUTOTUNE) # decode raw data coming from GCS bucket to valid image, mask pair \n    ds = ds.cache() # cache dataset preprocessing work that doesn't fit in memory\n    ds = ds.repeat() \n    \n    # shuffle while training else set to False\n    if shuffle:\n        ds = ds.shuffle(buffer_size=1000)\n        \n    ds = ds.batch(batch_size) # generate batches of data\n    ds = ds.prefetch(buffer_size=AUTOTUNE) # fetch dataset while model is training\n    return ds","820a7663":"print(len(ALL_TRAINING_FILENAMES))","580f97a6":"# BASELINE MODEL, WITHOUT FOLDS\nM = {}\nmetrics = ['loss','dice_coe','accuracy']\nfor fm in metrics:\n    M['val_'+fm] = []\n\n# Split training data into train\/validation sets\n# fold = KFold(n_splits=P['NFOLDS'], shuffle=True, random_state=P['SEED'])\n# for fold,(tr_idx, val_idx) in enumerate(fold.split(ALL_TRAINING_FILENAMES)):\n\ntr_idx = list(range(0, 6)) # 6 imgs used for training\nval_idx = list(range(6,8)) # 2 imgs used for validation\n\nprint('#'*35); print('############ TRAINING #############'); print('#'*35);\nprint(f'Image Size: {DIM}, Batch Size: {BATCH_SIZE}')\n\n# CREATE TRAIN AND VALIDATION SUBSETS\nTRAINING_FILENAMES = [ALL_TRAINING_FILENAMES[fi] for fi in tr_idx]\nVALIDATION_FILENAMES = [ALL_TRAINING_FILENAMES[fi] for fi in val_idx]\n\nSTEPS_PER_EPOCH = P['STEPS_COE'] * count_data_items(TRAINING_FILENAMES) \/\/ BATCH_SIZE\nprint(\"Steps per epoch: \",STEPS_PER_EPOCH)\n\n# BUILD MODEL\nK.clear_session()\nwith strategy.scope():   \n    model = sm.Unet(P['BACKBONE'], encoder_weights='imagenet')\n    model.compile(optimizer = tf.keras.optimizers.Adam(lr = P['LR']),\n#                   loss = tf.keras.losses.BinaryCrossentropy(),#'focal_tversky',\n#                   loss = dice_focal,\n                  loss = sm.losses.binary_focal_dice_loss, \n                  metrics=[dice_coe,'accuracy'])\n\n# CALLBACKS\n\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(f'\/kaggle\/working\/ek-model-focal-dice-bs16-512.h5',\n                             verbose=P['VERBOSE'],monitor='val_dice_coe',patience = 10,\n                             mode='max',save_best_only=True)\n\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_dice_coe',mode = 'max', patience=10, restore_best_weights=True)\nreduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=8, min_lr=0.00001)\n\nprint(f'Training Model...')\nhistory = model.fit(\n    get_training_dataset(),\n    epochs = P['EPOCHS'],\n    steps_per_epoch = STEPS_PER_EPOCH,\n    callbacks = [checkpoint, reduce,early_stop],\n    validation_data = get_validation_dataset(),\n    verbose=P['VERBOSE']\n)   \n\n#with strategy.scope():\n#    model = tf.keras.models.load_model('\/kaggle\/working\/model-fold-%i.h5'%fold, custom_objects = {\"dice_coe\": dice_coe})\n\n# SAVE METRICS\nm = model.evaluate(get_validation_dataset(),return_dict=True)\nfor fm in metrics:\n    M['val_'+fm].append(m[fm])\n\n# PLOT TRAINING\n# https:\/\/www.kaggle.com\/cdeotte\/triple-stratified-kfold-with-tfrecords\nif P['DISPLAY_PLOT']:        \n    plt.figure(figsize=(15,5))\n    n_e = np.arange(len(history.history['dice_coe']))\n    plt.plot(n_e,history.history['dice_coe'],'-o',label='Train dice_coe',color='#ff7f0e')\n    plt.plot(n_e,history.history['val_dice_coe'],'-o',label='Val dice_coe',color='#1f77b4')\n    x = np.argmax( history.history['val_dice_coe'] ); y = np.max( history.history['val_dice_coe'] )\n    xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n    plt.scatter(x,y,s=200,color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,'max dice_coe\\n%.2f'%y,size=14)\n    plt.ylabel('dice_coe',size=14); plt.xlabel('Epoch',size=14)\n    plt.legend(loc=2)\n    plt2 = plt.gca().twinx()\n    plt2.plot(n_e,history.history['loss'],'-o',label='Train Loss',color='#2ca02c')\n    plt2.plot(n_e,history.history['val_loss'],'-o',label='Val Loss',color='#d62728')\n    x = np.argmin( history.history['val_loss'] ); y = np.min( history.history['val_loss'] )\n    ydist = plt.ylim()[1] - plt.ylim()[0]\n    plt.scatter(x,y,s=200,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n    plt.ylabel('Loss',size=14)\n    plt.legend(loc=3)\n    plt.show()\n    \n    print(\"Validation dice evolution: {}\".format(history.history['val_dice_coe']))\n    print(\"Training dice evolution: {}\".format(history.history['dice_coe']))\n    print(\"Validation Loss: {}\".format(history.history['val_loss']))\n    print(\"Train Loss: {}\".format(history.history['loss']))\n    \n    print(\"Final validation dice coe: {}\".format(history.history['val_dice_coe'][-1]))\n    ","fb4565e2":"# M = {}\n# metrics = ['loss','dice_coe','accuracy']\n# for fm in metrics:\n#     M['val_'+fm] = []\n\n    \n# fold = KFold(n_splits=P['NFOLDS'], shuffle=True, random_state=P['SEED'])\n# for fold,(tr_idx, val_idx) in enumerate(fold.split(ALL_TRAINING_FILENAMES)):\n    \n#     print('#'*35); print('############ FOLD ',fold+1,' #############'); print('#'*35);\n#     print(f'Image Size: {DIM}, Batch Size: {BATCH_SIZE}')\n    \n#     # CREATE TRAIN AND VALIDATION SUBSETS\n#     TRAINING_FILENAMES = [ALL_TRAINING_FILENAMES[fi] for fi in tr_idx]\n#     if P['OVERLAPP']:\n#         TRAINING_FILENAMES += [ALL_TRAINING_FILENAMES2[fi] for fi in tr_idx]\n    \n#     VALIDATION_FILENAMES = [ALL_TRAINING_FILENAMES[fi] for fi in val_idx]\n#     STEPS_PER_EPOCH = P['STEPS_COE'] * count_data_items(TRAINING_FILENAMES) \/\/ BATCH_SIZE\n    \n#     # BUILD MODEL\n#     K.clear_session()\n#     with strategy.scope():   \n#         model = sm.Unet(P['BACKBONE'], encoder_weights='imagenet')\n#         model.compile(optimizer = tf.keras.optimizers.Adam(lr = P['LR']),\n#                       loss = tf.keras.losses.BinaryCrossentropy(),#'focal_tversky',\n#                       metrics=[dice_coe,'accuracy'])\n        \n#     # CALLBACKS\n#     checkpoint = tf.keras.callbacks.ModelCheckpoint('\/kaggle\/working\/model-fold-%i.h5'%fold,\n#                                  verbose=P['VERBOSE'],monitor='val_dice_coe',patience = 10,\n#                                  mode='max',save_best_only=True)\n    \n#     early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_dice_coe',mode = 'max', patience=10, restore_best_weights=True)\n#     reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=8, min_lr=0.00001)\n        \n#     print(f'Training Model Fold {fold+1}...')\n#     history = model.fit(\n#         get_training_dataset(),\n#         epochs = P['EPOCHS'],\n#         steps_per_epoch = STEPS_PER_EPOCH,\n#         callbacks = [checkpoint, reduce,early_stop],\n#         validation_data = get_validation_dataset(),\n#         verbose=P['VERBOSE']\n#     )   \n    \n#     #with strategy.scope():\n#     #    model = tf.keras.models.load_model('\/kaggle\/working\/model-fold-%i.h5'%fold, custom_objects = {\"dice_coe\": dice_coe})\n    \n#     # SAVE METRICS\n#     m = model.evaluate(get_validation_dataset(),return_dict=True)\n#     for fm in metrics:\n#         M['val_'+fm].append(m[fm])\n    \n#     # PLOT TRAINING\n#     # https:\/\/www.kaggle.com\/cdeotte\/triple-stratified-kfold-with-tfrecords\n#     if P['DISPLAY_PLOT']:        \n#         plt.figure(figsize=(15,5))\n#         n_e = np.arange(len(history.history['dice_coe']))\n#         plt.plot(n_e,history.history['dice_coe'],'-o',label='Train dice_coe',color='#ff7f0e')\n#         plt.plot(n_e,history.history['val_dice_coe'],'-o',label='Val dice_coe',color='#1f77b4')\n#         x = np.argmax( history.history['val_dice_coe'] ); y = np.max( history.history['val_dice_coe'] )\n#         xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n#         plt.scatter(x,y,s=200,color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,'max dice_coe\\n%.2f'%y,size=14)\n#         plt.ylabel('dice_coe',size=14); plt.xlabel('Epoch',size=14)\n#         plt.legend(loc=2)\n#         plt2 = plt.gca().twinx()\n#         plt2.plot(n_e,history.history['loss'],'-o',label='Train Loss',color='#2ca02c')\n#         plt2.plot(n_e,history.history['val_loss'],'-o',label='Val Loss',color='#d62728')\n#         x = np.argmin( history.history['val_loss'] ); y = np.min( history.history['val_loss'] )\n#         ydist = plt.ylim()[1] - plt.ylim()[0]\n#         plt.scatter(x,y,s=200,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n#         plt.ylabel('Loss',size=14)\n#         plt.legend(loc=3)\n#         plt.show()","b27e38ab":"## Reference: https:\/\/www.kaggle.com\/wrrosa\/hubmap-tf-with-tpu-efficientunet-512x512-train\/notebook#Versions\n"}}