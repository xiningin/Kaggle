{"cell_type":{"9aeb6500":"code","70a3fa68":"code","1b1961d1":"code","1ca102c6":"code","39d58138":"code","c467d4d7":"code","cdab0128":"code","fe1c32c0":"code","2f0a0c12":"code","3015db2e":"markdown"},"source":{"9aeb6500":"# Basics\nimport numpy as np \nimport pandas as pd\nimport os\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Deep Learning\nimport keras\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense,  SeparableConv2D, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom keras.callbacks import ReduceLROnPlateau\nimport cv2","70a3fa68":"#Firstly we check several images sizes. Normally, the more resolution we get, the better the deep learning models perform.\nim1 = cv2.imread(\"..\/input\/chest-xray-pneumonia\/chest_xray\/train\/NORMAL\/IM-0115-0001.jpeg\")\nim2 = cv2.imread(\"..\/input\/chest-xray-pneumonia\/chest_xray\/test\/NORMAL\/IM-0006-0001.jpeg\")\nim3 = cv2.imread(\"..\/input\/chest-xray-pneumonia\/chest_xray\/val\/NORMAL\/NORMAL2-IM-1427-0001.jpeg\")\nprint(im1.shape, im2.shape, im3.shape)\ndel im1\ndel im2\ndel im3","1b1961d1":"#Reading the images with larger size might take longer\nlabels = ['PNEUMONIA', 'NORMAL']\nimg_size = 200\n#Setting to 1200 might help. For memory concern, I set it to 200.\ndef get_data(data_dir):\n    data = []\n    for label in labels:\n        path = os.path.join(data_dir, label)\n        class_num = labels.index(label)\n        for img in os.listdir(path):\n            try:\n                # I really tried to keep the RGB. But Kaggle keeps throwing memory error.\n                # I also tried to keep image size larger. Kaggle keeps throwing memory error in data augmentation.\n                # So I set img size to 200 and read in the images with grayscale. \n                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n                resized_arr = cv2.resize(img_arr, (img_size, img_size))\n                data.append([resized_arr, class_num])\n            except Exception as e:\n                print(e)\n    return np.array(data)\ntrain = get_data('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train')\ntest = get_data('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/test')\nval = get_data('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/val')\n\n#There are four images broken","1ca102c6":"#Routines for feature label separation.\n#Put the features into a list of arrays.\n#Put the label into y variable.\nx_train = []\ny_train = []\n\nx_val = []\ny_val = []\n\nx_test = []\ny_test = []\n\nfor feature, label in train:\n    x_train.append(feature)\n    y_train.append(label)\n\nfor feature, label in test:\n    x_test.append(feature)\n    y_test.append(label)\n    \nfor feature, label in val:\n    x_val.append(feature)\n    y_val.append(label)\n    \ndel train\ndel test\ndel val\n#Remember to empty the temp vars for more available memory.","39d58138":"#Normalize Images\ndef normalize_image(img_set):\n    img_set = np.array(img_set)\/255\n    return img_set\nx_train = normalize_image(x_train)\nx_val = normalize_image(x_val)\nx_test = normalize_image(x_test)\n\n#Resize for transfer learning \ndef train_reshape(img_set):\n    img_set = img_set.reshape(-1,img_size, img_size, 1)\n    return img_set\nx_train = train_reshape(x_train)\nx_val = train_reshape(x_val)\nx_test = train_reshape(x_test)\n\ny_train = np.array(y_train)\ny_val = np.array(y_val)\ny_test = np.array(y_test)","c467d4d7":"# We try to increase the train set with augmentation as much as possible. \n# However, rotation might influence the prediction.\ndatagen = ImageDataGenerator(\n        zoom_range = 0.2, # Randomly zoom image \n        width_shift_range=0.05,  # randomly shift images horizontally (fraction of total width)\n        horizontal_flip = False) # I set this to true at first. It is horrible for the model to detect.\ndatagen.fit(x_train)","cdab0128":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten,BatchNormalization, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import RMSprop\nfrom keras.applications import *\nfrom keras.models import Model\nimport keras","fe1c32c0":"monitor = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=10, verbose=1, mode='auto',\n        restore_best_weights=True)\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 5, verbose=1,factor=0.3, mode=\"max\")\n\ncallback_list = [monitor, learning_rate_reduction]\n\nmodel = Sequential()\n\n#1st\nmodel.add(Conv2D(filters = 16 , kernel_size=(3,3), padding = 'same' , activation = 'relu' , input_shape = (200,200,1)))\nmodel.add(Conv2D(filters = 16 , kernel_size=(3,3), padding = 'same' , activation = 'relu'))\nmodel.add(MaxPool2D((2,2)))\n\n#2nd\nmodel.add(SeparableConv2D(filters = 32, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\nmodel.add(SeparableConv2D(filters = 32, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2, 2)))\nmodel.add(Dropout(0.4))\n\n#3rd\nmodel.add(SeparableConv2D(filters = 64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\nmodel.add(SeparableConv2D(filters = 64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Dropout(0.3))\n\n#4th\nmodel.add(SeparableConv2D(filters = 128, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\nmodel.add(SeparableConv2D(filters = 128, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2, 2)))\nmodel.add(Dropout(0.3))\n\n#5th\nmodel.add(SeparableConv2D(filters = 32, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\nmodel.add(SeparableConv2D(filters = 32, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2, 2)))\nmodel.add(Dropout(0.3))\n\n#6th\nmodel.add(Conv2D(128, (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2, 2)))\n\n# Fully Connected Layers\nmodel.add(Flatten())\nmodel.add(Dense(512, activation = 'relu'))\nmodel.add(Dropout(0.6))\nmodel.add(Dense(256, activation = 'relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(64, activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n\nhistory = model.fit(datagen.flow(x_train,y_train, batch_size = 8) ,epochs = 30 , \\\n                    validation_data = datagen.flow(x_val, y_val) ,callbacks = callback_list)","2f0a0c12":"print(\"Loss \" , model.evaluate(x_test,y_test)[0])\nprint(\"Accuracy\" , model.evaluate(x_test,y_test))","3015db2e":"## Read the Images"}}