{"cell_type":{"c64cbf3a":"code","47a19760":"code","a91b6a1b":"code","41e5c2e2":"code","938cd6e9":"code","87e3df78":"code","3df68bf8":"code","382fb816":"code","31774b86":"code","62f93c83":"markdown","76402f4d":"markdown","9656d53c":"markdown","f177587c":"markdown","d07ecb2d":"markdown","77cbf7e0":"markdown","1e400abc":"markdown","13b61b43":"markdown","3aefbb62":"markdown","0bac438f":"markdown"},"source":{"c64cbf3a":"import torch\nimport pandas as pd\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle as rect\nfrom albumentations.pytorch.transforms import ToTensorV2\n%matplotlib inline\n\nimport os\nimport random\nfrom PIL import Image\n\n#To get reproducible transformations\nSEED = 42\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)","47a19760":"TRAIN_IMAGES_PATH = '..\/input\/global-wheat-detection\/train'\ndf = pd.read_csv('..\/input\/global-wheat-detection\/train.csv')","a91b6a1b":"def get_bbox(img):\n    df_img = df[df.image_id==img][['bbox']]\n    \n    bbox_list = []\n    for i in df_img.iterrows():\n        xmin,ymin,width,height = np.fromstring(i[1][0][1:-1],sep=',')\n        bbox_list.append([xmin,ymin,width,height])\n    return bbox_list\n    \n    \ndef display_images(rows, cols, image_paths):\n    fig, ax = plt.subplots(rows,cols, figsize=(15,5))\n    plt.suptitle('Original Images')\n    for j in range(cols):\n        arr = Image.open(image_paths[j])\n        img_id = image_paths[j][-13:-4]\n        ax[j].set_title(img_id)\n        ax[j].imshow(arr)\n        ax[j].axis('off')\n        \n        bboxes = get_bbox(img_id)\n        \n        for bbox in bboxes:            \n            ax[j].add_patch(rect((bbox[0],bbox[1]),bbox[2],bbox[3],linewidth=1,edgecolor='r',facecolor='none'))\n        \n\nimg_list =[TRAIN_IMAGES_PATH + '\/'+ s + '.jpg' for s in pd.unique(df.image_id)[0:3].tolist()]\n\ndisplay_images(1,3,img_list)","41e5c2e2":"\ndisplay_images(1,3,img_list)\n\nfig, ax = plt.subplots(1,3, figsize=(15,5))\nplt.suptitle(\"Transformed - ToGray()\")\nseed_everything(SEED)\ntransforms_1 = A.Compose([A.ToGray(p=0.4)]) #higher probability greater chance of getting gray scale\n\nfor i,img_path in enumerate(img_list):    \n    arr = np.array(Image.open(img_path))\n    img_id = img_path[-13:-4]\n    bboxes = get_bbox(img_id)\n    \n    arr = transforms_1(**{\"image\": arr})['image']\n    ax[i].set_title(img_id)\n    ax[i].imshow(arr)\n    for bbox in bboxes:  \n            ax[i].add_patch(rect((bbox[0],bbox[1]),bbox[2],bbox[3],linewidth=1,edgecolor='r',facecolor='none'))\n    ax[i].axis('off')","938cd6e9":"display_images(1,3,img_list)\n\nfig, ax = plt.subplots(1,3, figsize=(15,5))\nplt.suptitle(\"Transformed - HorizontalFlip\")\nseed_everything(SEED)\n#We need bbox_params so that we get correct bboxes after flipping. \ntransforms_1 = A.Compose([A.HorizontalFlip(p=0.5)],\n                         bbox_params=A.BboxParams(format='coco', min_area=0, \n                                               min_visibility=0, label_fields=['labels']) ) \n\nfor i,img_path in enumerate(img_list):    \n    arr = np.array(Image.open(img_path))\n    img_id = img_path[-13:-4]\n    bboxes = get_bbox(img_id)\n    #Here labels are all ones because wheat is the only class we have. \n    #For e.g.: Labels would be different if we have bboxes for different objects for e.g cat, dog in the same image. \n    transform = transforms_1(**{\"image\": arr , \"bboxes\": bboxes, \"labels\":np.ones(len(bboxes))})\n    arr = transform['image']\n    bboxes = transform['bboxes']\n    ax[i].set_title(img_id)\n    ax[i].imshow(arr)\n    for bbox in bboxes:  \n            ax[i].add_patch(rect((bbox[0],bbox[1]),bbox[2],bbox[3],linewidth=1,edgecolor='r',facecolor='none'))\n            \n    ax[i].axis('off')\n    ","87e3df78":"display_images(1,3,img_list)\n\nfig, ax = plt.subplots(1,3, figsize=(15,5))\nplt.suptitle(\"Transformed - Resize\")\nseed_everything(SEED)\n#We need bbox_params so that we get correct bboxes after flipping. \ntransforms_1 = A.Compose([A.Resize(height=512, width=512, p=1),], #p=1 because we want all images resized\n                         bbox_params=A.BboxParams(format='coco', min_area=0, \n                                               min_visibility=0, label_fields=['labels']) ) \n\nfor i,img_path in enumerate(img_list):    \n    arr = np.array(Image.open(img_path))\n    img_id = img_path[-13:-4]\n    bboxes = get_bbox(img_id)\n    #Here labels are all ones because wheat is the only class we have. \n    #For e.g.: Labels would be different if we have bboxes for different objects for e.g cat, dog in the same image. \n    transform = transforms_1(**{\"image\": arr , \"bboxes\": bboxes, \"labels\":np.ones(len(bboxes))})\n    arr = transform['image']\n    bboxes = transform['bboxes']\n    ax[i].set_title(img_id + ' ' + str(arr.shape))  #original images were 1024,1024,3\n    ax[i].imshow(arr)\n    for bbox in bboxes:  \n            ax[i].add_patch(rect((bbox[0],bbox[1]),bbox[2],bbox[3],linewidth=1,edgecolor='r',facecolor='none'))\n            \n    ax[i].axis('off')\n    ","3df68bf8":"display_images(1,3,img_list)\n\nfig, ax = plt.subplots(1,3, figsize=(15,5))\nplt.suptitle(\"Transformed - Cutout\")\nseed_everything(SEED)\ntransforms_1 = A.Compose( [A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=[255,255,255], p=0.5)] )\n#fill value = [255,255,255] = White\n#If you change p=0.7 all three images should have cutout.\n\nfor i,img_path in enumerate(img_list):    \n    arr = np.array(Image.open(img_path))\n    img_id = img_path[-13:-4]\n    transform = transforms_1(**{\"image\": arr })\n    arr = transform['image']\n    ax[i].set_title(img_id + ' ' + str(arr.shape))  #original images were 1024,1024,3\n    ax[i].imshow(arr)\n    ax[i].axis('off')","382fb816":"transforms_1 = A.Compose( [A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=[255,255,255], p=0.5)] )\n\nfor i,img_path in enumerate(img_list[0:1]):    \n    arr = np.array(Image.open(img_path))\n    img_id = img_path[-13:-4]\n    transform = transforms_1(**{\"image\": arr })\n    arr = transform['image']\n    print(type(arr))\n\n    \n#Converting to torch.tensor - which will be required for modeling\ntransforms_1 = A.Compose( [A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=[255,255,255], p=0.5),\n                           ToTensorV2() ] )\n\nfor i,img_path in enumerate(img_list[0:1]):    \n    arr = np.array(Image.open(img_path))\n    img_id = img_path[-13:-4]\n    transform = transforms_1(**{\"image\": arr })\n    arr = transform['image']\n    print(type(arr))\n","31774b86":"display_images(1,3,img_list)\n\nfig, ax = plt.subplots(1,3, figsize=(15,5))\nplt.suptitle(\"Transformed - ToGray + Resize + HorizontalFlip + Cutout\")\nseed_everything(SEED)\ntransforms_1 = A.Compose( [ A.ToGray(p=0.4),\n                            A.Resize(height=512, width=512, p=1),\n                            A.HorizontalFlip(p=0.5), \n                            A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=[255,255,255], p=0.5)],\n                          bbox_params=A.BboxParams(format='coco', min_area=0, \n                                               min_visibility=0, label_fields=['labels'])  )\n#fill value = [255,255,255] = White\n#If you change p=0.7 all three images should have cutout.\n\nfor i,img_path in enumerate(img_list):    \n    arr = np.array(Image.open(img_path))\n    img_id = img_path[-13:-4]\n    bboxes = get_bbox(img_id)\n    transform = transforms_1(**{\"image\": arr , \"bboxes\": bboxes, \"labels\":np.ones(len(bboxes))})\n    arr = transform['image']\n    bboxes = transform['bboxes']\n    ax[i].set_title(img_id + ' ' + str(arr.shape))  #original images were 1024,1024,3\n    ax[i].imshow(arr)\n    for bbox in bboxes:  \n            ax[i].add_patch(rect((bbox[0],bbox[1]),bbox[2],bbox[3],linewidth=1,edgecolor='r',facecolor='none'))\n            \n    ax[i].axis('off')\n    ","62f93c83":"### ToTensorV2 \n\n[Albumentations ToTensorV2](https:\/\/albumentations.readthedocs.io\/en\/latest\/api\/pytorch.html#albumentations.pytorch.transforms.ToTensorV2)","76402f4d":"Note that not all transformations got applied to all images due to setting different probabilities. Thankfully the seeds work, so results are reproducible. ","9656d53c":"## HorizontalFlip\n\n[Albumentations HorizontalFlip](https:\/\/albumentations.readthedocs.io\/en\/latest\/api\/augmentations.html#albumentations.augmentations.transforms.HorizontalFlip)","f177587c":"Only two images got gray scale transformation. If we set probability to higher value we can get more grayscale transformation (and vice versa)","d07ecb2d":"### ToGray + Resize + HorizontalFlip + Cutout","77cbf7e0":"### Cutout\n\n[Albumentations Cutout](https:\/\/albumentations.readthedocs.io\/en\/latest\/api\/augmentations.html#albumentations.augmentations.transforms.Cutout)\n\n- No bboxes required here","1e400abc":"Third image ('7b72ea0fb') isn't flipped. First two images did get flipped.","13b61b43":"### Resize \n\n[Albumentations Resize](https:\/\/albumentations.readthedocs.io\/en\/latest\/api\/augmentations.html#albumentations.augmentations.transforms.Resize)","3aefbb62":"### Display Original Images","0bac438f":"# Transformations\n\nThis is my attempt to understand transformations\/albumentations. I took some of the transformations mentioned in [this](https:\/\/www.kaggle.com\/shonenkov\/training-efficientdet\/notebook) kernel.\n\n[Albumentations - ToGray](https:\/\/albumentations.readthedocs.io\/en\/latest\/api\/augmentations.html#albumentations.augmentations.transforms.ToGray)\n"}}