{"cell_type":{"d6ee5b09":"code","5dd673b4":"code","36e13d74":"code","373786d9":"code","3225b081":"code","da91e8ab":"code","35fd45e7":"code","56dd5974":"code","4dfc182e":"code","a501e64f":"code","c7583e94":"code","148aa017":"code","25d1f536":"code","a917298f":"code","0ceffa0a":"code","c14b25ec":"code","2a961f28":"code","fd4b47e4":"code","3467cd48":"code","0d67b634":"code","2829dcfd":"code","3d025365":"code","ec991541":"code","3208ae25":"code","5b6ba3b1":"code","67cb625c":"code","96d5be01":"code","3908645b":"code","64d64d05":"code","7d00763b":"code","a4652d9a":"code","fcff7bac":"code","6b9d2d4e":"code","0e4a9ce1":"code","d53312e3":"code","d3598357":"code","0f679a44":"code","9384655b":"code","8d4fed58":"code","6278d511":"code","8fe0b518":"code","9365b0b9":"code","aaa57a35":"code","3e222e8e":"code","12b3c819":"code","5cb8cba3":"code","e88d0b5a":"code","5cce42a3":"code","1b2e3bfd":"markdown","7beb775e":"markdown","c61c1796":"markdown","710e8e14":"markdown","144396d7":"markdown","e2cebc8f":"markdown","fecbd911":"markdown","222bf67e":"markdown","761b45c7":"markdown","4d015538":"markdown","7a603c5d":"markdown","21647724":"markdown","db3e7d99":"markdown","3fa401b5":"markdown","516f2a4c":"markdown","bcddd452":"markdown","c63e23f0":"markdown","e017f2f7":"markdown","bf14c550":"markdown","722e303f":"markdown","660de664":"markdown","8e40e25c":"markdown","f1203c56":"markdown","0431e74b":"markdown","3af29719":"markdown","dd0336e3":"markdown","f0ee3d88":"markdown","afb96e04":"markdown","2d462a93":"markdown","53f12c47":"markdown","62d63090":"markdown","abd53602":"markdown","cd7fabf3":"markdown","1847642c":"markdown","d1eb01fe":"markdown","286ad1c8":"markdown","91e1013f":"markdown"},"source":{"d6ee5b09":"import pandas as pd\nimport numpy as np\n\nimport datetime \nimport time\n\n%matplotlib inline\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics","5dd673b4":"events_df = pd.read_csv('..\/input\/events.csv')\ncategory_tree_df = pd.read_csv('..\/input\/category_tree.csv')\nitem_properties_1_df = pd.read_csv('..\/input\/item_properties_part1.csv')\nitem_properties_2_df = pd.read_csv('..\/input\/item_properties_part2.csv')","36e13d74":"events_df.head()","373786d9":"#Which event has a value in its transaction id\nevents_df[events_df.transactionid.notnull()].event.unique()","3225b081":"#Which event\/s has a null value\nevents_df[events_df.transactionid.isnull()].event.unique()","da91e8ab":"item_properties_1_df.head()","35fd45e7":"category_tree_df.head()","56dd5974":"item_properties_1_df.loc[(item_properties_1_df.property == 'categoryid') & (item_properties_1_df.value == '1016')].sort_values('timestamp').head()","4dfc182e":"#Let's get all the customers who bought something\ncustomer_purchased = events_df[events_df.transactionid.notnull()].visitorid.unique()\ncustomer_purchased.size","a501e64f":"#Let's get all unique visitor ids as well\nall_customers = events_df.visitorid.unique()\nall_customers.size","c7583e94":"customer_browsed = [x for x in all_customers if x not in customer_purchased]","148aa017":"len(customer_browsed)","25d1f536":"#Another way to do it using Numpy\ntemp_array = np.isin(customer_browsed, customer_purchased)\ntemp_array[temp_array == False].size","a917298f":"#A sample list of the customers who bought something\ncustomer_purchased[:10]","0ceffa0a":"events_df[events_df.visitorid == 102019].sort_values('timestamp')","c14b25ec":"tz = int('1433221332')\nnew_time = datetime.datetime.fromtimestamp(tz)\nnew_time.strftime('%Y-%m-%d %H:%M:%S')","2a961f28":"tz = int('1438400163')\nnew_time = datetime.datetime.fromtimestamp(tz)\nnew_time.strftime('%Y-%m-%d %H:%M:%S')","fd4b47e4":"# Firstly let's create an array that lists visitors who made a purchase\ncustomer_purchased = events_df[events_df.transactionid.notnull()].visitorid.unique()\n    \npurchased_items = []\n    \n# Create another list that contains all their purchases \nfor customer in customer_purchased:\n\n    #Generate a Pandas series type object containing all the visitor's purchases and put them in the list\n    purchased_items.append(list(events_df.loc[(events_df.visitorid == customer) & (events_df.transactionid.notnull())].itemid.values))                                  ","3467cd48":"purchased_items[:5]","0d67b634":"# Number of unique items in transactions\nevents_df.loc[events_df.transactionid.notnull(), 'itemid'].unique().size","2829dcfd":"max_len=0\nfor tran in purchased_items:\n    if len(tran) > max_len:\n        max_len = len(tran)\n        \nprint(f'Biggest purchase size: {max_len} items')","3d025365":"from mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import apriori, association_rules","ec991541":"te = TransactionEncoder()\nte_ary = te.fit(purchased_items).transform(purchased_items)\ndf_pi = pd.DataFrame(te_ary, columns=te.columns_)\ndf_pi.shape","3208ae25":"df_pi.head()","5b6ba3b1":"frq_items = apriori(df_pi, min_support=0.001)\nfrq_items.head()","67cb625c":"rules = association_rules(frq_items, metric =\"confidence\", min_threshold = 0.4)\nrules","96d5be01":"# TODO: factorization matrix or asso rules with item category???","3908645b":"# Write a function that would show items that were bought together (same of different dates) by the same customer\ndef recommender_bought_bought(item_id, purchased_items):\n    \n    # Perhaps implement a binary search for that item id in the list of arrays\n    # Then put the arrays containing that item id in a new list\n    # Then merge all items in that list and get rid of duplicates\n    recommender_list = []\n    for x in purchased_items:\n        if item_id in x:\n            recommender_list += x\n    \n    #Then merge recommender list and remove the item id\n    recommender_list = list(set(recommender_list) - set([item_id]))\n    \n    return recommender_list","64d64d05":"recommender_bought_bought(302422, purchased_items)","7d00763b":"#Put all the visitor id in an array and sort it ascendingly\nall_visitors = events_df.visitorid.sort_values().unique()\nall_visitors.size","a4652d9a":"buying_visitors = events_df[events_df.event == 'transaction'].visitorid.sort_values().unique()\nbuying_visitors.size","fcff7bac":"viewing_visitors_list = list(set(all_visitors) - set(buying_visitors))\n","6b9d2d4e":"def create_dataframe(visitor_list):\n    \n    array_for_df = []\n    for index in visitor_list:\n\n        #Create that visitor's dataframe once\n        v_df = events_df[events_df.visitorid == index]\n\n        temp = []\n        #Add the visitor id\n        temp.append(index)\n\n        #Add the total number of unique products viewed\n        temp.append(v_df[v_df.event == 'view'].itemid.unique().size)\n\n        #Add the total number of views regardless of product type\n        temp.append(v_df[v_df.event == 'view'].event.count())\n\n        #Add the total number of purchases\n        number_of_items_bought = v_df[v_df.event == 'transaction'].event.count()\n        temp.append(number_of_items_bought)\n\n        #Then put either a zero or one if they made a purchase\n        if(number_of_items_bought == 0):\n            temp.append(0)\n        else:\n            temp.append(1)\n\n        array_for_df.append(temp)\n    \n    return pd.DataFrame(array_for_df, columns=['visitorid', 'num_items_viewed', 'view_count', 'bought_count', 'purchased'])","0e4a9ce1":"buying_visitors_df = create_dataframe(buying_visitors)","d53312e3":"buying_visitors_df.shape","d3598357":"#Let's shuffle the viewing visitors list for randomness\nimport random\nrandom.shuffle(viewing_visitors_list)","0f679a44":"viewing_visitors_df = create_dataframe(viewing_visitors_list[0:27820])","9384655b":"viewing_visitors_df.shape","8d4fed58":"main_df = pd.concat([buying_visitors_df, viewing_visitors_df], ignore_index=True)","6278d511":"#Let's shuffle main_df first\nmain_df = main_df.sample(frac=1)","8fe0b518":"#Plot the data\nsns.pairplot(main_df, x_vars = ['num_items_viewed', 'view_count', 'bought_count'],\n             y_vars = ['num_items_viewed', 'view_count', 'bought_count'],  hue = 'purchased')","9365b0b9":"X = main_df.drop(['purchased', 'visitorid', 'bought_count'], axis = 'columns')\ny = main_df.purchased","aaa57a35":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, train_size = 0.7)","3e222e8e":"logreg = LogisticRegression()","12b3c819":"logreg.fit(X_train, y_train)","5cb8cba3":"# Let's now use the model to predict the test features\ny_pred_class = logreg.predict(X_test)","e88d0b5a":"print('accuracy = {:7.4f}'.format(metrics.accuracy_score(y_test, y_pred_class)))","5cce42a3":"# Generate the prediction values for each of the test observations using predict_proba() function rather than just predict\npreds = logreg.predict_proba(X_test)[:,1]\n\n# Store the false positive rate(fpr), true positive rate (tpr) in vectors for use in the graph\nfpr, tpr, _ = metrics.roc_curve(y_test, preds)\n\n# Store the Area Under the Curve (AUC) so we can annotate our graph with theis metric\nroc_auc = metrics.auc(fpr, tpr)\n\n# Plot the ROC Curve\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange', lw = lw, label = 'ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color = 'navy', lw = lw, linestyle = '--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc = \"lower right\")\nplt.show()","1b2e3bfd":"So there were actually 1,395,861 unique site visitors who didn't buy anything, again assuming that there were no repeat users with different visitor IDs","7beb775e":"# Can we perhaps cluster the visitors and see if classes appear?\n\nFor that I will need to create a new dataframe and engineer a few features for it","c61c1796":"Let's plot main_df and see if anything comes up","710e8e14":"Let us load the Retail Rocket CSV files into DataFrames","144396d7":"# So our model's accuracy in predicting buying visitors is around 79.46%","e2cebc8f":"Out of 1,407,580 unique visitor ids, let's take out the ones that bought something","fecbd911":"Now lets create a function that creates a dataframe with new features: visitorid, number of items viewed, total viewcount, bought something or not","222bf67e":"If we want to convert the UNIX \/ Epoch time format to readable format then just do the code below","761b45c7":"I think it's prudent to start separating customers into two categories, those who purchased something and those who didn't","4d015538":"# What other insights can we gather from the items that were viewed, added to cart and sold?","7a603c5d":"The timestamp portion is in Unix Epoch format e.g. 1433221332117 will be converted to Tuesday, 2 June 2015 5:02:12.117 AM GMT\n\nVisitor Id is the unique user currently browsing the website\n\nEvent is what the user is currently doing in that current timestamp\n\nTransaction ID will only have value if the user made a purchase as shown below","21647724":"We separate the features (drop visitorid since it's categorical data and bought count) and the target (which is whether the visitor bought something or not)","db3e7d99":"I think I'll only get around 27,821 samples from the viewing visitors list so that there is a 70-30 split for training and test data. ","3fa401b5":"The rest of the events with NaN transaction ids are either view or add to cart","516f2a4c":"Now let's combine the two dataframes","bcddd452":"# Below is a snapshot of visitor id 102019 and their buying journey from viewing to transaction (purchase)","c63e23f0":"# Customer behaviour exploration","e017f2f7":"# Category IDs","bf14c550":"# Let's take a peek at the Events dataframe","722e303f":"Timestamp is still the same Unix \/ Epoch format\n\nItem id will be the unique item identifier\n\nProperty is the Item's attributes such as category id and availability while the rest are hashed for confidentiality purposes\n\nValue is the item's property value e.g. availability is 1 if there is stock and 0 otherwise\n\nNote: Values that start with \"n\" indicate that the value preceeding it is a number e.g. n277.200 is equal to 277.2","660de664":"# So now we can present to the visitor a list of the other items a customer previously bought along with what item the current visitor is viewing e.g. item number 302422","8e40e25c":"The graph above shows the accuracy of our binary classifier (Logistic Regression). Just means that the closer the orange curve leans to the top left hand part of the graph, the better the accuracy.","f1203c56":"The plot above clearly indicates that the higher the view count, the higher the chances of that visitor buying something. Duh!","0431e74b":"How many unique visitors did we have for the site from June 2, 2015 to August 1, 2015?\n\nShown below are the total number of visitors for that time duration (was also shown at the close to the start of this paper)","3af29719":"Category IDs explain the relationship of different products with each other e.g. Category ID 1016 is a child of Parent ID 213.\n\nBelow shows the number of items under category id 1016","dd0336e3":"That was a very crude way of recommending other items to the visitor","f0ee3d88":"# Modelling: Apriori and association rules","afb96e04":"# Since the relationship is Linear, let's try a simple Logistic Regression model to predict future visitor purchase behaviour","2d462a93":"# Now let's take a look at the Item Properties","53f12c47":"# Objective:\n    1. Show how the dataset is structured\n    2. Explore a bit about customer behaviour based on the data and do basic customer segmentation\n    3. Recommendations for future analysis","62d63090":"Let's apply this to buying visitors first","abd53602":"Out of 1,407,580 visitors, ony 11,719 bought something so around 1,395,861 visitors just viewed items","cd7fabf3":"# What insights can we offer the visitor to guide them in their buying journey?\n\n-perhaps we can offer them a list of what previous visitors bought together with the item they are currently viewing","1847642c":"This dataset was taken from the Retail Rocket Recommender System dataset: https:\/\/www.kaggle.com\/retailrocket\/ecommerce-dataset\/home\n\nAnd data was between June 2, 2015 and August 1, 2015","d1eb01fe":"Assumptions:\n    1. Since we have no information whether there were any repeat users who bought something from the site, I'll just have to assume for now that the 11,719 visitors are unique and made at least a single purchase","286ad1c8":"So now all items purchased together are presented as a list of lists, shown below are the first 5 samples","91e1013f":"# Thanks to @johnosorio. This work is a fork of his notebook \"Retail Rocket eCommerce Recommender System\".\nTaking advantage of the exploratory analysis and preprocessing of the data by @johnosorio, the added value is:\n1. Modelling section using Apriori algorithm and association rules.\n2. Deployment section using the AWS Sagemaker library."}}