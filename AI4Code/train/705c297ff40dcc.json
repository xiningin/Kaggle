{"cell_type":{"2c7801bb":"code","9c542599":"code","11c2eefc":"code","6469c338":"code","4cdee828":"code","2c8ac618":"code","e28fa081":"code","86068d49":"code","c9bac900":"code","28b42a70":"code","4e822291":"code","0dbeb39e":"code","5acdb34f":"code","105cd9cf":"code","0980ee7e":"code","4a2403fb":"code","3b1e4741":"code","562993ba":"code","d86cc641":"code","e93644e0":"code","fb575462":"code","20143b0e":"code","6f0c117c":"code","7855e9ea":"code","d86949a7":"code","99386c54":"code","c589d99f":"code","32334e34":"code","ea7a4584":"code","a2a46210":"code","71b513b2":"code","38ebf5c5":"code","cec5f30f":"code","0399b545":"code","83a258ce":"code","6938703d":"code","5517fd3a":"code","1fefb7e3":"code","684c240c":"code","e7cfe6f9":"code","119d6033":"code","f7fd556e":"code","b9a3f8b2":"code","6da996f3":"code","88c6a24b":"code","e12733d0":"code","05537639":"code","910155a8":"code","87a5b225":"code","52f43521":"code","f97508b9":"code","6765ad33":"code","d0a3de52":"code","01f80cbd":"code","271a69fc":"code","63c129c0":"code","a799d78b":"code","db858df3":"code","e5759ba6":"code","74e835b6":"code","00888041":"code","fba5d8ac":"code","4a4d8d36":"code","1a2f4b6e":"code","e03a3d69":"code","87b584e2":"code","f45999f8":"code","7bb6fc21":"code","3ff88040":"code","bd9304fe":"code","7ad6f996":"code","3624274c":"code","a2b90317":"code","8b32ca5e":"code","e6f643e6":"code","a537b2a3":"code","e6da20df":"code","5468cf92":"code","461eb3b9":"code","2dac5cf4":"code","0e4b6ba5":"code","f9507128":"code","07c129f8":"code","ed8a50b3":"code","5c1b85f9":"code","a2beacca":"code","76fed156":"code","8d1144ae":"code","7afd0d01":"code","65ff19db":"code","0e01c050":"code","af3b10ea":"code","94369f22":"code","16e84105":"code","b373bda2":"code","f5114c1d":"code","37be64f8":"code","12220732":"code","a8bb2a67":"code","5b7b1775":"code","47a9d0d7":"code","301ceb1b":"code","d4479351":"code","869aadef":"code","c1085a7f":"code","a3170222":"code","7ff5b9e2":"code","aea563c1":"code","ede650b2":"code","56908352":"code","77e5ef0b":"code","ff1c5969":"code","e4ae2146":"code","a51afca7":"code","c3e8362e":"code","38b1bf62":"code","d83f2a38":"code","ca5fd243":"code","5fd34e29":"code","2d9ddff1":"code","bed9cab7":"code","45cea464":"code","5df1df42":"code","2271ce20":"code","acc02c04":"code","37c2f625":"markdown","3ed81d44":"markdown","f9c1203a":"markdown","3b210348":"markdown","45a7d7a6":"markdown","f201db29":"markdown","99ac3503":"markdown","e1d1d39c":"markdown","a7e03ced":"markdown","2dc9370f":"markdown","0f773c69":"markdown","02d7ed12":"markdown","0a039602":"markdown","4fc17eed":"markdown","f36781ac":"markdown","46938b5e":"markdown","14cf9104":"markdown","106bcbd5":"markdown","29742766":"markdown","f8297eca":"markdown","1c74c1d3":"markdown","66a8e7c9":"markdown","1a262e42":"markdown","6ec925e5":"markdown","3f557378":"markdown","5d26cbd9":"markdown","06e7b5da":"markdown","c65022c0":"markdown","f02eebce":"markdown","aa80f853":"markdown","17eb4d33":"markdown","ba617dda":"markdown","0b332f27":"markdown","1ecf2e38":"markdown","26181afe":"markdown","0532bed7":"markdown","caee0076":"markdown","e8f90e4d":"markdown","253651ac":"markdown","f778b849":"markdown","25b9547a":"markdown","396922a4":"markdown","4e359b7c":"markdown","5621436c":"markdown","d8dc4273":"markdown","344028c3":"markdown"},"source":{"2c7801bb":"import os\nimport re\nimport string\nimport pandas as pd\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import pos_tag\nfrom nltk.chunk import ne_chunk\nfrom nltk.chunk import conlltags2tree, tree2conlltags\nfrom nltk.corpus import stopwords\nfrom pprint import pprint\nimport spacy\nfrom spacy import displacy\nfrom collections import Counter\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\nimport seaborn as sns\nimport matplotlib as mp\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom nltk.probability import FreqDist\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import plot_confusion_matrix,classification_report,confusion_matrix\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport warnings\nwarnings.filterwarnings('ignore')","9c542599":"data_dir='\/kaggle\/input\/feedback-prize-2021\/'","11c2eefc":"os.listdir(data_dir)","6469c338":"train_path=data_dir+\"\/train\/\"","4cdee828":"train_path","2c8ac618":"len(os.listdir(train_path)) # total 15594 files","e28fa081":"os.listdir(train_path)[0:10]","86068d49":"for files in os.listdir(train_path)[0:10]:\n  file = open(train_path+files,'r')\n  print('\\n')\n  print(files)\n  print('\\n')\n  while True:\n      next_line = file.readline()\n\n      if not next_line:\n          break;\n      print(next_line.strip())\n\n  file.close()","c9bac900":"file='62C57C524CD2.txt'","28b42a70":"\nwith open(train_path+file, 'r') as file:\n    data = file.read().replace('\\n', '')","4e822291":"data","0dbeb39e":"# Word tokenization and part of speech tagging\ntext = nltk.word_tokenize(data)\ntext = nltk.pos_tag(text)\nprint(text)","5acdb34f":"pattern = 'NP: {<DT>?<JJ>*<NN>}'","105cd9cf":"chunk_parser = nltk.RegexpParser(pattern)\ntext = chunk_parser.parse(text)\niob_tagged = tree2conlltags(text)\npprint(iob_tagged)","0980ee7e":"ne_tree = ne_chunk(pos_tag(word_tokenize(data)))\nprint(ne_tree)","4a2403fb":"doc=nlp(data)","3b1e4741":"\npprint([(X.text, X.label_) for X in doc.ents])","562993ba":"pprint([(X, X.ent_iob_, X.ent_type_) for X in doc])","d86cc641":"[(x.orth_,x.pos_) for x in [y \n                                      for y\n                                      in nlp(data) \n                                      if not y.is_stop and y.pos_ != 'PUNCT']]","e93644e0":"displacy.render(nlp(data), jupyter=True, style='ent')","fb575462":"for files in os.listdir(train_path)[0:10]:\n  file = open(train_path+files,'r')\n  print('file name:',file)\n  print('\\n')\n  data = file.read().replace('\\n', '')\n  print(data)\n  print('\\n')\n  print(\"Part of speech tagging\")\n  print(\"\\n\")\n  print([(x.orth_,x.pos_) for x in [y \n                                      for y\n                                      in nlp(data) \n                                      if not y.is_stop and y.pos_ != 'PUNCT']])\n  print('\\n')\n  displacy.render(nlp(data), jupyter=True, style='ent')\n  print('\\n')\n  file.close()","20143b0e":"test_path=data_dir+\"\/test\/\"","6f0c117c":"len(os.listdir(test_path)) # 5 text files","7855e9ea":"for files in os.listdir(test_path):\n  file = open(test_path+files,'r')\n  print('file name:',file)\n  print('\\n')\n  data = file.read().replace('\\n', '')\n  print(data)\n  print('\\n')\n  print(\"Part of speech tagging\")\n  print(\"\\n\")\n  print([(x.orth_,x.pos_) for x in [y \n                                      for y\n                                      in nlp(data) \n                                      if not y.is_stop and y.pos_ != 'PUNCT']])\n  print('\\n')\n  displacy.render(nlp(data), jupyter=True, style='ent')\n  print('\\n')\n  file.close()","d86949a7":"df_train = pd.read_csv(data_dir+\"\/train.csv\")\ndf_train.head()","99386c54":"# will subset the dataframe keeping the id,discourse_text,discourse_type\ndf_train = df_train[[\"id\",\"discourse_text\",\"discourse_type\"]]\ndf_train.head(10)","c589d99f":"df_train.shape","32334e34":"# lets check the distribution of the discourse_type\nsns.set(rc={'figure.figsize':(14,8)})\nsns.countplot(data=df_train,x='discourse_type')","ea7a4584":"df_train['discourse_type'].value_counts()","a2a46210":"# any null columns\ndf_train.isnull().sum()","71b513b2":"# remove all characters not number or characters\ndef cleanText(input_string):\n    modified_string = re.sub('[^A-Za-z0-9]+', ' ', input_string)\n    modified_string = re.sub('[0-9]+', ' ', modified_string)\n    modified_string=re.sub(\"[@]\",\"\",modified_string)\n    return(modified_string)\ndf_train['discourse_text'] = df_train.discourse_text.apply(cleanText)\ndf_train['discourse_text'][150]","38ebf5c5":"# Remove non printable characters\ndef remove_not_ASCII(text):\n    text = ''.join([word for word in text if word in string.printable])\n    return text\ndf_train['discourse_text'] = df_train.discourse_text.apply(remove_not_ASCII)\ndf_train['discourse_text'][150]","cec5f30f":"#converting to lower case\ndf_train['discourse_text']=df_train['discourse_text'].str.lower()","0399b545":"#removing punctuations\ndf_train['discourse_text']=df_train['discourse_text'].str.translate(str.maketrans('','',string.punctuation))","83a258ce":"nltk.download('stopwords')","6938703d":"stopWords=stopwords.words('english')\ndef removeStopWords(stopWords, rvw_txt):\n    newtxt = ' '.join([word for word in rvw_txt.split() if word not in stopWords])\n    return newtxt\ndf_train['discourse_text'] = [removeStopWords(stopWords,x) for x in df_train['discourse_text']]","5517fd3a":"#remove words containing numbers\ndf_train['discourse_text']=df_train['discourse_text'].apply(lambda x:re.sub('\\w*\\d\\w*' , '', x) )","1fefb7e3":"from nltk.util import ngrams","684c240c":"#splitting text into words\ntokenList=[]\nfor indx in range(len(df_train)):\n       token=word_tokenize(df_train['discourse_text'][indx])\n       tokenList.append(token)\ndf_train['text_tokens'] = tokenList\ndf_train.head()","e7cfe6f9":"cntvec = CountVectorizer(ngram_range=(1,1))\ncntvec.fit(df_train['discourse_text'])\nunigrams = cntvec.transform(df_train['discourse_text'])\ntotal_words = unigrams.sum(axis=0) \nngram_freq = [(word, total_words[0, index]) for word, index in cntvec.vocabulary_.items()]\nngram_freq =sorted(ngram_freq, key = lambda x: x[1], reverse=True)\nngram_freq[1:50]","119d6033":"df_ngrams = pd.DataFrame(ngram_freq, columns = ['word' , 'freq'])\ndf_ngrams = df_ngrams.groupby('word').sum()['freq'].sort_values(ascending=False)\ndf_ngrams=df_ngrams[1:50]\ndf_ngrams\n\n","f7fd556e":"fig = plt.figure(figsize=(25,10))\n\nax = df_ngrams.plot(kind='bar')\nax.set_title('Unigram Distribution')\nax.set_xlabel(\"Unigrams\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()\n","b9a3f8b2":"cntvec = CountVectorizer(ngram_range=(2,2))\ncntvec.fit(df_train['discourse_text'])\nbigrams = cntvec.transform(df_train['discourse_text'])\ntotal_words = bigrams.sum(axis=0) \nngram_freq = [(word, total_words[0, index]) for word, index in cntvec.vocabulary_.items()]\nngram_freq =sorted(ngram_freq, key = lambda x: x[1], reverse=True)\nngram_freq[1:20]","6da996f3":"df_ngrams = pd.DataFrame(ngram_freq, columns = ['word' , 'freq'])\ndf_ngrams = df_ngrams.groupby('word').sum()['freq'].sort_values(ascending=False)\ndf_ngrams=df_ngrams[1:50]\ndf_ngrams","88c6a24b":"fig = plt.figure(figsize=(25,10))\n\nax = df_ngrams.plot(kind='bar')\nax.set_title('Bigrams Distribution')\nax.set_xlabel(\"Bigrams\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()","e12733d0":"cntvec = CountVectorizer(ngram_range=(3,3))\ncntvec.fit(df_train['discourse_text'])\ntrigrams = cntvec.transform(df_train['discourse_text'])\ntotal_words = trigrams.sum(axis=0) \nngram_freq = [(word, total_words[0, index]) for word, index in cntvec.vocabulary_.items()]\nngram_freq =sorted(ngram_freq, key = lambda x: x[1], reverse=True)\nngram_freq[1:50]","05537639":"df_ngrams = pd.DataFrame(ngram_freq, columns = ['word' , 'freq'])\ndf_ngrams = df_ngrams.groupby('word').sum()['freq'].sort_values(ascending=False)\ndf_ngrams=df_ngrams[1:50]\ndf_ngrams","910155a8":"fig = plt.figure(figsize=(25,10))\n\nax = df_ngrams.plot(kind='bar')\nax.set_title('Trigrams Distribution')\nax.set_xlabel(\"Trigrams\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()","87a5b225":"df_train_claim = df_train[df_train.discourse_type == 'Claim']\ncntvec = CountVectorizer(ngram_range=(1,1))\ncntvec.fit(df_train_claim['discourse_text'])\nunigrams = cntvec.transform(df_train_claim['discourse_text'])\ntotal_words = unigrams.sum(axis=0) \nngram_freq = [(word, total_words[0, index]) for word, index in cntvec.vocabulary_.items()]\nngram_freq =sorted(ngram_freq, key = lambda x: x[1], reverse=True)\nngram_freq[1:30]","52f43521":"df_ngrams = pd.DataFrame(ngram_freq, columns = ['word' , 'freq'])\ndf_ngrams = df_ngrams.groupby('word').sum()['freq'].sort_values(ascending=False)\ndf_ngrams=df_ngrams[1:30]\ndf_ngrams\n","f97508b9":"fig = plt.figure(figsize=(25,10))\n\nax = df_ngrams.plot(kind='bar')\nax.set_title('Unigram Distribution for Discourse Type Claim')\nax.set_xlabel(\"Unigrams\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()","6765ad33":"cntvec = CountVectorizer(ngram_range=(2,2))\ncntvec.fit(df_train_claim['discourse_text'])\nbigrams = cntvec.transform(df_train_claim['discourse_text'])\ntotal_words = bigrams.sum(axis=0) \nngram_freq = [(word, total_words[0, index]) for word, index in cntvec.vocabulary_.items()]\nngram_freq =sorted(ngram_freq, key = lambda x: x[1], reverse=True)\nngram_freq[1:30]","d0a3de52":"df_ngrams = pd.DataFrame(ngram_freq, columns = ['word' , 'freq'])\ndf_ngrams = df_ngrams.groupby('word').sum()['freq'].sort_values(ascending=False)\ndf_ngrams=df_ngrams[1:30]\n\nfig = plt.figure(figsize=(25,10))\n\nax = df_ngrams.plot(kind='bar')\nax.set_title('Bigrams Distribution for Discourse Type Claim')\nax.set_xlabel(\"Bigrams\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()","01f80cbd":"cntvec = CountVectorizer(ngram_range=(3,3))\ncntvec.fit(df_train_claim['discourse_text'])\ntrigrams = cntvec.transform(df_train_claim['discourse_text'])\ntotal_words = trigrams.sum(axis=0) \nngram_freq = [(word, total_words[0, index]) for word, index in cntvec.vocabulary_.items()]\nngram_freq =sorted(ngram_freq, key = lambda x: x[1], reverse=True)\nngram_freq[1:30]","271a69fc":"df_ngrams = pd.DataFrame(ngram_freq, columns = ['word' , 'freq'])\ndf_ngrams = df_ngrams.groupby('word').sum()['freq'].sort_values(ascending=False)\ndf_ngrams=df_ngrams[1:30]\n\nfig = plt.figure(figsize=(25,10))\n\nax = df_ngrams.plot(kind='bar')\nax.set_title('Trigrams Distribution for Discourse Type Claim')\nax.set_xlabel(\"Trigrams\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()","63c129c0":"text=' ' .join([str(item) for item in df_train_claim['discourse_text'] ])\nwordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\nplt.figure(figsize=(15, 10), dpi=80)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","a799d78b":"df_train_evidence = df_train[df_train.discourse_type == 'Evidence']\ncntvec = CountVectorizer(ngram_range=(1,1))\ncntvec.fit(df_train_evidence['discourse_text'])\nunigrams = cntvec.transform(df_train_evidence['discourse_text'])\ntotal_words = unigrams.sum(axis=0) \nngram_freq = [(word, total_words[0, index]) for word, index in cntvec.vocabulary_.items()]\nngram_freq =sorted(ngram_freq, key = lambda x: x[1], reverse=True)\nngram_freq[1:30]","db858df3":"df_ngrams = pd.DataFrame(ngram_freq, columns = ['word' , 'freq'])\ndf_ngrams = df_ngrams.groupby('word').sum()['freq'].sort_values(ascending=False)\ndf_ngrams=df_ngrams[1:30]\ndf_ngrams","e5759ba6":"fig = plt.figure(figsize=(25,10))\n\nax = df_ngrams.plot(kind='bar')\nax.set_title('Unigram Distribution for Discourse Type Evidence')\nax.set_xlabel(\"Unigrams\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()","74e835b6":"cntvec = CountVectorizer(ngram_range=(2,2))\ncntvec.fit(df_train_evidence['discourse_text'])\nbigrams = cntvec.transform(df_train_evidence['discourse_text'])\ntotal_words = bigrams.sum(axis=0) \nngram_freq = [(word, total_words[0, index]) for word, index in cntvec.vocabulary_.items()]\nngram_freq =sorted(ngram_freq, key = lambda x: x[1], reverse=True)\nngram_freq[1:30]","00888041":"df_ngrams = pd.DataFrame(ngram_freq, columns = ['word' , 'freq'])\ndf_ngrams = df_ngrams.groupby('word').sum()['freq'].sort_values(ascending=False)\ndf_ngrams=df_ngrams[1:30]\n\nfig = plt.figure(figsize=(25,10))\n\nax = df_ngrams.plot(kind='bar')\nax.set_title('Bigrams Distribution for Discourse Type Evidence')\nax.set_xlabel(\"Bigrams\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()","fba5d8ac":"cntvec = CountVectorizer(ngram_range=(3,3))\ncntvec.fit(df_train_evidence['discourse_text'])\ntrigrams = cntvec.transform(df_train_evidence['discourse_text'])\ntotal_words = trigrams.sum(axis=0) \nngram_freq = [(word, total_words[0, index]) for word, index in cntvec.vocabulary_.items()]\nngram_freq =sorted(ngram_freq, key = lambda x: x[1], reverse=True)\nngram_freq[1:30]","4a4d8d36":"df_ngrams = pd.DataFrame(ngram_freq, columns = ['word' , 'freq'])\ndf_ngrams = df_ngrams.groupby('word').sum()['freq'].sort_values(ascending=False)\ndf_ngrams=df_ngrams[1:30]\n\nfig = plt.figure(figsize=(25,10))\n\nax = df_ngrams.plot(kind='bar')\nax.set_title('Trigrams Distribution for Discourse Type Evidence')\nax.set_xlabel(\"Trigrams\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()","1a2f4b6e":"text=' ' .join([str(item) for item in df_train_evidence['discourse_text'] ])\nwordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\nplt.figure(figsize=(15, 10), dpi=80)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","e03a3d69":"df_train_position = df_train[df_train.discourse_type == 'Position']\ncntvec = CountVectorizer(ngram_range=(1,1))\ncntvec.fit(df_train_position['discourse_text'])\nunigrams = cntvec.transform(df_train_position['discourse_text'])\ntotal_words = unigrams.sum(axis=0) \nngram_freq = [(word, total_words[0, index]) for word, index in cntvec.vocabulary_.items()]\nngram_freq =sorted(ngram_freq, key = lambda x: x[1], reverse=True)\nngram_freq[1:30]","87b584e2":"df_ngrams = pd.DataFrame(ngram_freq, columns = ['word' , 'freq'])\ndf_ngrams = df_ngrams.groupby('word').sum()['freq'].sort_values(ascending=False)\ndf_ngrams=df_ngrams[1:30]\ndf_ngrams","f45999f8":"fig = plt.figure(figsize=(25,10))\n\nax = df_ngrams.plot(kind='bar')\nax.set_title('Unigram Distribution for Discourse Type Position')\nax.set_xlabel(\"Unigrams\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()","7bb6fc21":"cntvec = CountVectorizer(ngram_range=(2,2))\ncntvec.fit(df_train_position['discourse_text'])\nbigrams = cntvec.transform(df_train_position['discourse_text'])\ntotal_words = bigrams.sum(axis=0) \nngram_freq = [(word, total_words[0, index]) for word, index in cntvec.vocabulary_.items()]\nngram_freq =sorted(ngram_freq, key = lambda x: x[1], reverse=True)\nngram_freq[1:30]","3ff88040":"df_ngrams = pd.DataFrame(ngram_freq, columns = ['word' , 'freq'])\ndf_ngrams = df_ngrams.groupby('word').sum()['freq'].sort_values(ascending=False)\ndf_ngrams=df_ngrams[1:30]\n\nfig = plt.figure(figsize=(25,10))\n\nax = df_ngrams.plot(kind='bar')\nax.set_title('Bigrams Distribution for Discourse Type Position')\nax.set_xlabel(\"Bigrams\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()","bd9304fe":"cntvec = CountVectorizer(ngram_range=(3,3))\ncntvec.fit(df_train_position['discourse_text'])\ntrigrams = cntvec.transform(df_train_position['discourse_text'])\ntotal_words = trigrams.sum(axis=0) \nngram_freq = [(word, total_words[0, index]) for word, index in cntvec.vocabulary_.items()]\nngram_freq =sorted(ngram_freq, key = lambda x: x[1], reverse=True)\nngram_freq[1:30]","7ad6f996":"df_ngrams = pd.DataFrame(ngram_freq, columns = ['word' , 'freq'])\ndf_ngrams = df_ngrams.groupby('word').sum()['freq'].sort_values(ascending=False)\ndf_ngrams=df_ngrams[1:30]\n\nfig = plt.figure(figsize=(25,10))\n\nax = df_ngrams.plot(kind='bar')\nax.set_title('Trigrams Distribution for Discourse Type Position')\nax.set_xlabel(\"Trigrams\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()","3624274c":"text=' ' .join([str(item) for item in df_train_position['discourse_text'] ])\nwordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\nplt.figure(figsize=(15, 10), dpi=80)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","a2b90317":"df_train_concstat = df_train[df_train.discourse_type == 'Concluding Statement']\ncntvec = CountVectorizer(ngram_range=(1,1))\ncntvec.fit(df_train_concstat['discourse_text'])\nunigrams = cntvec.transform(df_train_concstat['discourse_text'])\ntotal_words = unigrams.sum(axis=0) \nngram_freq = [(word, total_words[0, index]) for word, index in cntvec.vocabulary_.items()]\nngram_freq =sorted(ngram_freq, key = lambda x: x[1], reverse=True)\nngram_freq[1:30]","8b32ca5e":"df_ngrams = pd.DataFrame(ngram_freq, columns = ['word' , 'freq'])\ndf_ngrams = df_ngrams.groupby('word').sum()['freq'].sort_values(ascending=False)\ndf_ngrams=df_ngrams[1:30]\ndf_ngrams","e6f643e6":"fig = plt.figure(figsize=(25,10))\n\nax = df_ngrams.plot(kind='bar')\nax.set_title('Unigram Distribution for Discourse Type Concluding Statement')\nax.set_xlabel(\"Unigrams\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()","a537b2a3":"cntvec = CountVectorizer(ngram_range=(2,2))\ncntvec.fit(df_train_concstat['discourse_text'])\nbigrams = cntvec.transform(df_train_concstat['discourse_text'])\ntotal_words = bigrams.sum(axis=0) \nngram_freq = [(word, total_words[0, index]) for word, index in cntvec.vocabulary_.items()]\nngram_freq =sorted(ngram_freq, key = lambda x: x[1], reverse=True)\nngram_freq[1:30]","e6da20df":"df_ngrams = pd.DataFrame(ngram_freq, columns = ['word' , 'freq'])\ndf_ngrams = df_ngrams.groupby('word').sum()['freq'].sort_values(ascending=False)\ndf_ngrams=df_ngrams[1:30]\n\nfig = plt.figure(figsize=(25,10))\n\nax = df_ngrams.plot(kind='bar')\nax.set_title('Bigrams Distribution for Discourse Type Concluding Statement')\nax.set_xlabel(\"Bigrams\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()","5468cf92":"cntvec = CountVectorizer(ngram_range=(3,3))\ncntvec.fit(df_train_concstat['discourse_text'])\ntrigrams = cntvec.transform(df_train_concstat['discourse_text'])\ntotal_words = trigrams.sum(axis=0) \nngram_freq = [(word, total_words[0, index]) for word, index in cntvec.vocabulary_.items()]\nngram_freq =sorted(ngram_freq, key = lambda x: x[1], reverse=True)\nngram_freq[1:30]","461eb3b9":"df_ngrams = pd.DataFrame(ngram_freq, columns = ['word' , 'freq'])\ndf_ngrams = df_ngrams.groupby('word').sum()['freq'].sort_values(ascending=False)\ndf_ngrams=df_ngrams[1:30]\n\nfig = plt.figure(figsize=(25,10))\n\nax = df_ngrams.plot(kind='bar')\nax.set_title('Trigrams Distribution for Discourse Type Concluding Statement')\nax.set_xlabel(\"Trigrams\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()","2dac5cf4":"text=' ' .join([str(item) for item in df_train_concstat['discourse_text'] ])\nwordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\nplt.figure(figsize=(15, 10), dpi=80)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","0e4b6ba5":"df_train_lead = df_train[df_train.discourse_type == 'Lead']\ncntvec = CountVectorizer(ngram_range=(1,1))\ncntvec.fit(df_train_lead['discourse_text'])\nunigrams = cntvec.transform(df_train_lead['discourse_text'])\ntotal_words = unigrams.sum(axis=0) \nngram_freq = [(word, total_words[0, index]) for word, index in cntvec.vocabulary_.items()]\nngram_freq =sorted(ngram_freq, key = lambda x: x[1], reverse=True)\nngram_freq[1:30]","f9507128":"fig = plt.figure(figsize=(25,10))\n\nax = df_ngrams.plot(kind='bar')\nax.set_title('Unigram Distribution for Discourse Type Lead')\nax.set_xlabel(\"Unigrams\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()","07c129f8":"cntvec = CountVectorizer(ngram_range=(2,2))\ncntvec.fit(df_train_lead['discourse_text'])\nbigrams = cntvec.transform(df_train_lead['discourse_text'])\ntotal_words = bigrams.sum(axis=0) \nngram_freq = [(word, total_words[0, index]) for word, index in cntvec.vocabulary_.items()]\nngram_freq =sorted(ngram_freq, key = lambda x: x[1], reverse=True)\nngram_freq[1:30]","ed8a50b3":"df_ngrams = pd.DataFrame(ngram_freq, columns = ['word' , 'freq'])\ndf_ngrams = df_ngrams.groupby('word').sum()['freq'].sort_values(ascending=False)\ndf_ngrams=df_ngrams[1:30]\n\nfig = plt.figure(figsize=(25,10))\n\nax = df_ngrams.plot(kind='bar')\nax.set_title('Bigrams Distribution for Discourse Type Lead')\nax.set_xlabel(\"Bigrams\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()","5c1b85f9":"cntvec = CountVectorizer(ngram_range=(3,3))\ncntvec.fit(df_train_lead['discourse_text'])\ntrigrams = cntvec.transform(df_train_lead['discourse_text'])\ntotal_words = trigrams.sum(axis=0) \nngram_freq = [(word, total_words[0, index]) for word, index in cntvec.vocabulary_.items()]\nngram_freq =sorted(ngram_freq, key = lambda x: x[1], reverse=True)\nngram_freq[1:30]","a2beacca":"df_ngrams = pd.DataFrame(ngram_freq, columns = ['word' , 'freq'])\ndf_ngrams = df_ngrams.groupby('word').sum()['freq'].sort_values(ascending=False)\ndf_ngrams=df_ngrams[1:30]\n\nfig = plt.figure(figsize=(25,10))\n\nax = df_ngrams.plot(kind='bar')\nax.set_title('Trigrams Distribution for Discourse Type Lead')\nax.set_xlabel(\"Trigrams\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()\n","76fed156":"text=' ' .join([str(item) for item in df_train_lead['discourse_text'] ])\nwordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\nplt.figure(figsize=(15, 10), dpi=80)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","8d1144ae":"df_train_countclaim = df_train[df_train.discourse_type == 'Counterclaim']\ncntvec = CountVectorizer(ngram_range=(1,1))\ncntvec.fit(df_train_countclaim['discourse_text'])\nunigrams = cntvec.transform(df_train_countclaim['discourse_text'])\ntotal_words = unigrams.sum(axis=0) \nngram_freq = [(word, total_words[0, index]) for word, index in cntvec.vocabulary_.items()]\nngram_freq =sorted(ngram_freq, key = lambda x: x[1], reverse=True)\nngram_freq[1:30]","7afd0d01":"fig = plt.figure(figsize=(25,10))\n\nax = df_ngrams.plot(kind='bar')\nax.set_title('Unigram Distribution for Discourse Type Counter Claim')\nax.set_xlabel(\"Unigrams\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()","65ff19db":"cntvec = CountVectorizer(ngram_range=(2,2))\ncntvec.fit(df_train_countclaim['discourse_text'])\nbigrams = cntvec.transform(df_train_countclaim['discourse_text'])\ntotal_words = bigrams.sum(axis=0) \nngram_freq = [(word, total_words[0, index]) for word, index in cntvec.vocabulary_.items()]\nngram_freq =sorted(ngram_freq, key = lambda x: x[1], reverse=True)\nngram_freq[1:30]","0e01c050":"df_ngrams = pd.DataFrame(ngram_freq, columns = ['word' , 'freq'])\ndf_ngrams = df_ngrams.groupby('word').sum()['freq'].sort_values(ascending=False)\ndf_ngrams=df_ngrams[1:30]\n\nfig = plt.figure(figsize=(25,10))\n\nax = df_ngrams.plot(kind='bar')\nax.set_title('Bigrams Distribution for Discourse Type Counter Claim')\nax.set_xlabel(\"Bigrams\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()","af3b10ea":"cntvec = CountVectorizer(ngram_range=(3,3))\ncntvec.fit(df_train_countclaim['discourse_text'])\ntrigrams = cntvec.transform(df_train_countclaim['discourse_text'])\ntotal_words = trigrams.sum(axis=0) \nngram_freq = [(word, total_words[0, index]) for word, index in cntvec.vocabulary_.items()]\nngram_freq =sorted(ngram_freq, key = lambda x: x[1], reverse=True)\nngram_freq[1:30]","94369f22":"df_ngrams = pd.DataFrame(ngram_freq, columns = ['word' , 'freq'])\ndf_ngrams = df_ngrams.groupby('word').sum()['freq'].sort_values(ascending=False)\ndf_ngrams=df_ngrams[1:30]\n\nfig = plt.figure(figsize=(25,10))\n\nax = df_ngrams.plot(kind='bar')\nax.set_title('Trigrams Distribution for Discourse Type Counter Claim')\nax.set_xlabel(\"Trigrams\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()\n","16e84105":"text=' ' .join([str(item) for item in df_train_countclaim['discourse_text'] ])\nwordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\nplt.figure(figsize=(15, 10), dpi=80)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","b373bda2":"df_train_rebuttal = df_train[df_train.discourse_type == 'Rebuttal']\ncntvec = CountVectorizer(ngram_range=(1,1))\ncntvec.fit(df_train_rebuttal['discourse_text'])\nunigrams = cntvec.transform(df_train_rebuttal['discourse_text'])\ntotal_words = unigrams.sum(axis=0) \nngram_freq = [(word, total_words[0, index]) for word, index in cntvec.vocabulary_.items()]\nngram_freq =sorted(ngram_freq, key = lambda x: x[1], reverse=True)\nngram_freq[1:30]","f5114c1d":"fig = plt.figure(figsize=(25,10))\n\nax = df_ngrams.plot(kind='bar')\nax.set_title('Unigram Distribution for Discourse Type Counter Rebuttal')\nax.set_xlabel(\"Unigrams\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()","37be64f8":"cntvec = CountVectorizer(ngram_range=(2,2))\ncntvec.fit(df_train_rebuttal['discourse_text'])\nbigrams = cntvec.transform(df_train_rebuttal['discourse_text'])\ntotal_words = bigrams.sum(axis=0) \nngram_freq = [(word, total_words[0, index]) for word, index in cntvec.vocabulary_.items()]\nngram_freq =sorted(ngram_freq, key = lambda x: x[1], reverse=True)\nngram_freq[1:30]","12220732":"df_ngrams = pd.DataFrame(ngram_freq, columns = ['word' , 'freq'])\ndf_ngrams = df_ngrams.groupby('word').sum()['freq'].sort_values(ascending=False)\ndf_ngrams=df_ngrams[1:30]\n\nfig = plt.figure(figsize=(25,10))\n\nax = df_ngrams.plot(kind='bar')\nax.set_title('Bigrams Distribution for Discourse Type Rebuttal')\nax.set_xlabel(\"Bigrams\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()","a8bb2a67":"cntvec = CountVectorizer(ngram_range=(3,3))\ncntvec.fit(df_train_rebuttal['discourse_text'])\ntrigrams = cntvec.transform(df_train_rebuttal['discourse_text'])\ntotal_words = trigrams.sum(axis=0) \nngram_freq = [(word, total_words[0, index]) for word, index in cntvec.vocabulary_.items()]\nngram_freq =sorted(ngram_freq, key = lambda x: x[1], reverse=True)\nngram_freq[1:30]","5b7b1775":"df_ngrams = pd.DataFrame(ngram_freq, columns = ['word' , 'freq'])\ndf_ngrams = df_ngrams.groupby('word').sum()['freq'].sort_values(ascending=False)\ndf_ngrams=df_ngrams[1:30]\n\nfig = plt.figure(figsize=(25,10))\n\nax = df_ngrams.plot(kind='bar')\nax.set_title('Trigrams Distribution for Discourse Type Rebuttal')\nax.set_xlabel(\"Trigrams\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()","47a9d0d7":"text=' ' .join([str(item) for item in df_train_rebuttal['discourse_text'] ])\nwordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\nplt.figure(figsize=(15, 10), dpi=80)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","301ceb1b":"X = df_train['discourse_text']\ny = df_train['discourse_type']","d4479351":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)","869aadef":"tfidf = TfidfVectorizer(stop_words='english')","c1085a7f":"tfidf.fit(X_train)","a3170222":"X_train_tfidf = tfidf.transform(X_train)\nX_test_tfidf = tfidf.transform(X_test)","7ff5b9e2":"nb = MultinomialNB()\nnb.fit(X_train_tfidf,y_train)","aea563c1":"preds = nb.predict(X_test_tfidf)\npredicted_prob = nb.predict_proba(X_test_tfidf)\nprint(classification_report(y_test,preds))\nconfusion_matrix(y_test,preds)","ede650b2":"cntvec = CountVectorizer(ngram_range=(1,1))\ncntvec.fit(X_train)\nX_train_cntvec = cntvec.transform(X_train)\nX_test_cntvec = cntvec.transform(X_test)","56908352":"nb = MultinomialNB()\nnb.fit(X_train_cntvec,y_train)","77e5ef0b":"preds = nb.predict(X_test_cntvec)\npredicted_prob = nb.predict_proba(X_test_cntvec)\nprint(classification_report(y_test,preds))\nconfusion_matrix(y_test,preds)","ff1c5969":"X_train, X_test, y_train, y_test = train_test_split(df_train[['discourse_text']], df_train[['discourse_type']], test_size=0.2, random_state=101)","e4ae2146":"X_train.shape,y_train.shape,X_test.shape,y_test.shape","a51afca7":"X_train = X_train.reset_index()\ny_train = y_train.reset_index()\nX_test = X_test.reset_index()\ny_test = y_test.reset_index()","c3e8362e":"sentences = []\nlabels = []\ntest_sentences = []\ntest_labels = []","38b1bf62":"for index in range(1,len(X_train)):\n    sentences.append(X_train['discourse_text'][index])\n    labels.append(y_train['discourse_type'][index])","d83f2a38":"sentences[1:10], labels[1:10]","ca5fd243":"for index in range(1,len(X_test)):\n    test_sentences.append(X_test['discourse_text'][index])\n    test_labels.append(y_test['discourse_type'][index])","5fd34e29":"test_sentences[1:10], test_labels[1:10]","2d9ddff1":"#  Tokenization\nvocab_size=10000\noov_token = \"oov\"\nmax_length = 15\nembedding_dim=16","bed9cab7":"tokenizer = Tokenizer(num_words = vocab_size, oov_token= oov_token)\ntokenizer.fit_on_texts(sentences)\nword_index = tokenizer.word_index","45cea464":"training_sequences = tokenizer.texts_to_sequences(sentences)\ntraining_padded = pad_sequences(training_sequences,padding='post', maxlen=max_length)\nprint(training_padded[0])\nprint(training_padded.shape)","5df1df42":"testing_sequences = tokenizer.texts_to_sequences(test_sentences)\ntesting_padded = pad_sequences(testing_sequences,padding='post', maxlen=max_length)\nprint(testing_padded[0])\nprint(testing_padded.shape)","2271ce20":"nb = MultinomialNB()\nnb.fit(training_padded,labels)","acc02c04":"preds = nb.predict(testing_padded)\npredicted_prob = nb.predict_proba(testing_padded)\nprint(classification_report(test_labels,preds))\nconfusion_matrix(test_labels,preds)","37c2f625":"implement noun phrase chunking to identify named entities using a regular expression consisting of rules","3ed81d44":"#### for concluding statement top words as think,work,need,life,better,help etc","f9c1203a":"#### Claim discourse text has top words as student, school,college,teacher,car,driverless etc","3b210348":"NER tagging with NLTK and ne_chunk","45a7d7a6":"#### discourse type = Concluding Statement    ","f201db29":"NER tagging with Spacy","99ac3503":"#### top words for Rebuttal as think, learning,better,work,car,help etc","e1d1d39c":"#### With tfidf and countvectorizer, we can see that the recall for Claim,Evidence is good followed by Lead,Position,Concluding Statement","a7e03ced":"token-level entity annotation using the BILUO tagging scheme to describe the entity boundaries","2dc9370f":"#### training sequences","0f773c69":"# BoW model","02d7ed12":"#### top words as electoral college,popular,vote, community etc","0a039602":"discourse_type=Counterclaim","4fc17eed":"# Reading the files","f36781ac":"#### For the top ten text, lets do the NER and POS tagging","46938b5e":"#### n grams","14cf9104":"#### bigrams","106bcbd5":"Reading train.csv file\n","29742766":"#### test sequences","f8297eca":"Lets check the files in the train folder","1c74c1d3":"#### GPE : countries, cities, states.\n#### CARDINAL: numerals. \n#### NORP: nationalities or religious groups or political groups","66a8e7c9":"#### Refrence https:\/\/towardsdatascience.com\/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da","1a262e42":"Will do the same for the text files in the test directory","6ec925e5":"Reading the top ten files","3f557378":"discourse type = Lead","5d26cbd9":"#### The metrics is poorer compared to the tfidf bow model.","06e7b5da":"# Wordcloud visualization","c65022c0":"discourse type = Position","f02eebce":"#### Unigram","aa80f853":"#### discourse type = Evidence","17eb4d33":"\"B\" means the token begins an entity, \"I\" means it is inside an entity, \"O\" means it is outside an entity, and \"\" means no entity tag is set.","ba617dda":"#### Data preprocessing step","0b332f27":"#### discourse type=Rebuttal","1ecf2e38":"#### Wordcloud and ngrams by Discourse Type","26181afe":"IOB tags to represent the chunk structures","0532bed7":"Bs is tagged as a person which is not correct. only 1 is Cardinal correct and 4 percent as Percent is correct.","caee0076":"### trigrams","e8f90e4d":"This directory has 2 files and two folders test and train","253651ac":"#### tfidf BOW model","f778b849":"# NER and POS tagging with NLTK and Spacy","25b9547a":"#### tokenizer is only fit on the training sentences","396922a4":"#### for discourse type as lead top words as college, student,electoral, school, phone etc","4e359b7c":"#### top words as electoral college, teacher,student, problem,friend etc","5621436c":"a list of tuples containing the individual words in the sentence and their associated part-of-speech","d8dc4273":"#### Discourse Type = Claim","344028c3":"#### top words for counter claim as teacher,want,know,good,people,home etc"}}