{"cell_type":{"dfc10075":"code","337f6a37":"code","ee2b8c65":"code","4cff74c1":"code","ef469481":"code","53c981f0":"code","94a0dea0":"code","57ff3407":"code","248cc216":"code","785bacd2":"code","fc7451b7":"code","30e47e95":"code","82b5f4b2":"code","7ae6a396":"code","8f068cc8":"code","ba87ff96":"code","9dd39160":"code","3ab88b57":"code","3c3b39fd":"code","7dedf39f":"code","ec4d526c":"code","a523bc0d":"code","fc9a4a98":"code","8f8ae0e1":"code","2033abf6":"code","fc418f4d":"code","16238b44":"code","cdbbf82f":"code","bf59bef4":"code","581d84cb":"markdown","df2ff779":"markdown","95e97d02":"markdown","1ad22fff":"markdown","cdd8ac06":"markdown","3d114990":"markdown","49df3774":"markdown","3e5515e0":"markdown","f8520324":"markdown","e5ec54d9":"markdown","9d08db3f":"markdown"},"source":{"dfc10075":"import numpy\nimport gzip\nfrom collections import defaultdict\nimport sys\nimport numpy\nimport gzip\nfrom collections import defaultdict\nimport pandas as pd\nimport json\nimport gzip\nfrom tqdm import tqdm_notebook\nimport Levenshtein\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom shutil import copyfile\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"..\/input\/graphicalgenomeapi\/CCGG_extension.py\", dst = \"..\/working\/CCGG_extension.py\")\n\n# import all our functions\nimport CCGG_extension as CCGG","337f6a37":"def loadFasta(filename):\n    \"\"\" Parses a classically formatted and possibly \n        compressed FASTA file into a list of headers \n        and fragment sequences for each sequence contained.\n        The resulting sequences are 0-indexed! \"\"\"\n    if (filename.endswith(\".gz\")):\n        fp = gzip.open(filename, 'r')\n    else:\n        fp = open(filename, 'r')\n    # split at headers\n    data = fp.read().split('>')\n    fp.close()\n    # ignore whatever appears before the 1st header\n    data.pop(0)     \n    headers = []\n    sequences = []\n    for sequence in data:\n        lines = sequence.split('\\n')\n        headers.append(lines.pop(0))\n        sequences.append(''.join(lines))\n    return (headers, sequences)\n\ndef construct_kmers(filename, k, profiletype):\n    '''Construct kmer profile given a reference fasta.\n    filename - <str> input reference fastafile\n    k - <int> length of kmers\n    profiletype - <int> default 0 to query every non-overlapping kmers in the reference genome, \n                                1 to query every possible kmers in the reference genome\n    '''\n    def contig_name(headers):\n        X = []\n        for item in headers:\n            X.append(item.split(' ')[0])\n        return X\n\n    def Construct_nonoverlapping_kmerprofile(X, sequences, k):\n        '''Construct kmer profile for every non-overlapping kmers in the linear reference\n        X - list of contig name\n        sequences - list of reference sequences\n        k - length of kmer\n        '''\n        D = {}\n        for i in range(len(sequences)): \n            contig = X[i]\n            seq = sequences[i] # 0 offset\n            num = len(seq)\/\/k\n            for i in range(num):\n                D[contig] = D.get(contig, []) + [seq[i*k:(i+1)*k]]\n        return D\n    \n    def Construct_allpossible_kmerprofile(X, sequences, k):\n        '''Construct kmer profile for every kmers in the linear reference\n        X - list of contig name\n        sequences - list of reference sequences\n        k - length of kmer\n        '''\n        D = {}\n        for i in range(len(sequences)): \n            contig = X[i]\n            seq = sequences[i] # 0 offset\n            num = len(seq) - k + 1\n            for i in range(num):\n                D[contig] = D.get(contig, []) + [seq[i: i+k]]\n        return D\n\n    def main(filename, k, profiletype):\n        '''Construct kmer profile given a reference fasta.\n        filename - <str> input reference fastafile\n        k - <int> length of kmers\n        profiletype - <int> default 0 to query every non-overlapping kmers in the reference genome, \n                                    1 to query every possible kmers in the reference genome\n        '''\n        headers, sequences = loadFasta(filename)\n        X = contig_name(headers)\n        if profiletype == 0:\n            D = Construct_nonoverlapping_kmerprofile(X, sequences, k)\n        elif profiletype == 1:\n            D = Construct_allpossible_kmerprofile(X,sequences, k)\n        return D\n    \n    D = main(filename, k, profiletype)\n    return D\n\n# search kmer count\ndef mapping_position(genome, Kmer_Dict, k):   \n    def create_kmer_profile(Kmer_Dict):\n        KmerProfile = defaultdict(list)\n        for sample,kmers in Kmer_Dict.items():\n            for seq in kmers:\n                KmerProfile[seq]\n                rev = ''.join([{'A':'T','C':'G','G':'C','T':'A'}[base] for base in reversed(seq)]) \n                KmerProfile[rev]\n        return KmerProfile\n\n    def mapping(seq, KmerProfile, k):\n        for i in range(0, len(seq) - k + 1):\n            kmer = seq[i:i+k]\n\n            if 'N' in kmer:\n                continue\n\n            c = KmerProfile.get(kmer, -1)\n\n            if isinstance(c, int):\n                continue\n\n            KmerProfile[kmer] += [i]\n        return KmerProfile\n\n    KmerProfile = create_kmer_profile(Kmer_Dict)\n    PositionProfile = mapping(genome, KmerProfile, k)\n    return PositionProfile\n\ndef create_anchors(kmers, PositionProfile, k):\n    \"\"\"Select Unique kmers as anchor candidates and provide a unique name to each anchor candidates\"\"\"\n    def unique_kmers(kmers, PositionProfile):\n        candidatelist = list(kmers.values())[0]\n        candidates = []\n        for kmer in candidatelist:\n            if len(PositionProfile[kmer]) == 0:\n                print(kmer, PositionProfile[kmer])\n            krev = ''.join([{'A':'T','C':'G','G':'C','T':'A'}[base] for base in reversed(kmer)]) \n            poslist = PositionProfile[kmer] + PositionProfile[krev]\n            if len(poslist) == 1:\n                candidates.append((kmer, PositionProfile[kmer][0]))\n        return candidates\n\n    candidates = unique_kmers(kmers, PositionProfile)\n    anchor_info = numpy.array(candidates)\n    return anchor_info\n","ee2b8c65":"\n# genome registration\n# search kmer count\ndef genome_registration(genome, anchor_filename, k):   \n    def create_kmer_profile(kmerlist):\n        KmerProfile = defaultdict(list)\n    #         for sample,kmers in Kmer_Dict.iteritems():\n        for seq in kmerlist:\n            KmerProfile[seq]\n            rev = ''.join([{'A':'T','C':'G','G':'C','T':'A'}[base] for base in reversed(seq)]) \n            KmerProfile[rev]\n    #         print len(KmerProfile)\n        return KmerProfile\n\n    def mapping(seq, KmerProfile, k):\n        for i in range(0, len(seq) - k + 1):\n            kmer = seq[i:i+k]\n\n            if 'N' in kmer:\n                continue\n\n            c = KmerProfile.get(kmer, -1)\n\n            if isinstance(c, int):\n                continue\n\n            KmerProfile[kmer] += [i]\n        return KmerProfile\n\n    # determine uniqueness\n    def unique_kmers(kmerlist, PositionProfile):\n        candidates = []\n        for kmer in kmerlist:\n            if len(PositionProfile[kmer]) > 0:\n                krev = ''.join([{'A':'T','C':'G','G':'C','T':'A'}[base] for base in reversed(kmer)]) \n                poslist = PositionProfile[kmer] + PositionProfile[krev]\n\n                if len(poslist) == 1:\n                    #print PositionProfile[kmer]\n                    candidates.append((kmer, PositionProfile[kmer][0]))\n        return candidates\n\n    # determine monotonicity\n    def binary_search(arr, val, l, r): \n        if l == r: \n            if arr[l] > val: \n                return l \n            else: \n                return l+1\n        if l > r: \n            return l \n\n        mid = (l+r)\/2\n        if arr[mid] < val: \n            return binary_search(arr, val, mid+1, r) \n        elif arr[mid] > val: \n            return binary_search(arr, val, l, mid-1) \n        else: \n            return mid \n\n    # NlogN \n    def efficientDeletionSort(array):\n        subsequence_end = [0] # index of the element\n        predecessors = [-1] # predecessor index\n        for i in range(1,len(array)):\n            arr = array[i]\n            # can do binary search instead, just skip to make it faster\n            if arr > array[subsequence_end[-1]]:\n                predecessors += [subsequence_end[-1]]\n                subsequence_end += [i]\n            else:\n                # preform binary search\n                minimum_end = [array[j] for j in subsequence_end] # element in current subsequence\n\n                insert_point = binary_search(minimum_end, arr, 0, len(minimum_end)-1)\n                if insert_point > 0:\n                    predecessors += [subsequence_end[insert_point-1]]\n                else:\n                    predecessors += [-1]\n\n                if insert_point > len(subsequence_end)-1: # arr == array[subsequence_end[-1]] \n                    subsequence_end += [i]\n                elif arr < array[subsequence_end[insert_point]]:\n                    subsequence_end[insert_point] = i \n\n        # backtrack\n        pre = subsequence_end[-1]\n        listIndex = []\n        while pre != -1:\n            listIndex.append(pre)\n            pre = predecessors[pre]\n        listIndex.reverse()\n        longest_subsequence = [array[i] for i in listIndex]\n        return listIndex, longest_subsequence\n\n    def monotonic_kmers(candidates):\n        candidates = numpy.array(candidates)\n        poslist = candidates[:,1].astype(int)\n        listIndex, longest_subsequence = efficientDeletionSort(poslist)\n        monotonic_anchor = candidates[listIndex,:]\n        return monotonic_anchor\n\n    anchor_c = numpy.load(anchor_filename)\n    kmerlist = anchor_c[:,0]\n    KmerProfile = create_kmer_profile(kmerlist)\n    PositionProfile = mapping(genome, KmerProfile, k)\n    candidates = unique_kmers(kmerlist, PositionProfile)\n    final = monotonic_kmers(candidates)\n    \n    return final\n\n\ndef getsequence_info(genomefile):\n    def contig_name(headers):\n        X = []\n        for item in headers:\n            X.append(item.split(' ')[0])\n        return X\n    \n    header, sequence = loadFasta(genomefile)\n    samples = contig_name(header)\n  \n    return header, sequence, samples\n\ndef integrate_info(genomefile, anchorfile, k):\n    header, sequence, samples = getsequence_info(genomefile)\n    anchor_c = numpy.load(anchorfile)\n    Anchor_Info = {}\n    for i in range(len(header)):\n        genome = \"+\" + sequence[i].upper()\n        \n        # if not complete genome\n        if len(genome) < 10000:\n            continue\n            \n        # if too many ambiguous bases\n        basenum = genome.count(\"A\") + genome.count(\"G\") + genome.count('C') + genome.count(\"T\")\n        if float(basenum)\/len(genome) < 0.9:\n            continue\n        anchormapping = genome_registration(genome, anchorfile, k)\n\n        Final_Dict= dict(anchormapping) # current mapping\n\n        samplename = samples[i]\n        D = {}\n        for anchorseq, refpos in anchor_c:\n            anchorname = \"A%05d\" % (int(refpos)\/k + 1)\n            D[anchorname] = Final_Dict.get(anchorseq, \"?\")\n        Anchor_Info[samplename] = D\n    df = pd.DataFrame(Anchor_Info)\n    df.to_csv('RegistrationTable.csv')\n    return df\n    \n\ndef writeGraphFasta(filename, input_dict, keylist=[\"src\", \"dst\"]):\n    \"\"\"Write the given node or edge file as a FASTA file. Overwrites existing file. Will create file if it doesn't exist. \n    def writeFasta(self:<GraphicalGenome>, filename:<str>, input_dict:<dict>, keylist=[\"src\",\"dst\"]:<list[str]>) -> Void\n\n    Parameters:\n        filename: <str> - absolute path for the file you wish to write. \n        input_dict: <dict> - Node or Edge dictionary you wish to write. \n        keylist: <list[str]> - list of strings of dictionary keys that you want to ignore during write. \n                This will require you to write these values on your own or just discard them. \n    \"\"\"\n    sorted_keys = sorted(input_dict.keys()) \n    with open(filename, \"w+\") as fastafile:\n        # If iterating through the edges, write the edges in the correctly ordered format\n        if (sorted_keys[0][0] == \"E\"):\n            for edge in sorted_keys:\n                # If header has not been evaluated, just re-write the header wholesale without any analysis\n                if \"hdr\" in input_dict[edge].keys():\n                    line = \">\" + edge + \";\" + input_dict[edge][\"hdr\"] + \"\\n\"\n                    line += input_dict[edge][\"seq\"] + \"\\n\"\n                    continue\n                line = \">\" + edge + \";{\" \n                # Source\n                line += '\"src\":\"' + input_dict[edge][\"src\"] + '\",'\n                # Destination\n                line += '\"dst\":\"' + input_dict[edge][\"dst\"] + '\"'\n                for key in input_dict[edge].keys():\n                    if key == \"seq\":\n                        continue\n                    if key in keylist:\n                        continue\n                    line += ',\"' + key + '\":' + json.dumps(input_dict[edge][key], separators=(\",\", \":\"))\n                line += \"}\\n\"\n                line += input_dict[edge][\"seq\"] + \"\\n\"\n                fastafile.write(line)\n        # If iterating over nodes, just write the nodes normally\n        else:\n            for i in sorted_keys:\n                line = \">\" + i + \";\"\n                obj = {}\n                for j in input_dict[i].keys():\n                    if j == 'seq':\n                        continue\n                    obj[j] = input_dict[i][j]\n                line += json.dumps(obj, separators=(\",\", \":\"))\n                line += \"\\n\" + input_dict[i]['seq'] + \"\\n\"\n                fastafile.write(line)\n\n","4cff74c1":"# Dynamically updating the Registration Table\ndef update_registration_table(genomefile, anchorfile, registrationfile, k):\n    '''Updating newly released linear genome in the NCBI Virus Platform to the registration table\n    Input: \n    genomefile - <filename> newly released genome collection, sequences should include the assemblies \n    incorporated in the registration table but also contain new genomic sequences\n    anchorfile - <filename> numpy file (.npy) of the anchor sequences and their reference coordinates\n    registrationfile - <filename> anchor by assemblyID matrix in cvs file format\n    k - <int> anchor length\n    Output:\n    Integration - <dataframe> updated registration table with same row number, larger column number\n    '''\n    # integrating    \n    def get_new_sequences(newfile,registrationfile):\n        df = pd.read_csv(registrationfile, index_col= 0)\n        new_collection = loadFasta(newfile)\n\n        previous_sample = df.columns\n        current_header, current_seq, current_sample = getsequence_info(newfile)\n        new_index = [i for i in range(len(current_sample)) if current_sample[i] not in previous_sample]\n        new_h = [current_header[i] for i in new_index] \n        new_seq = [current_seq[i] for i in new_index] \n        new_s = [current_sample[i] for i in new_index] \n        return new_h, new_seq, new_s\n    \n    def get_new_matrix(genomefile, anchorfile, registrationfile, k):\n        header,sequence,samples = get_new_sequences(genomefile,registrationfile)\n        anchor_c = numpy.load(anchorfile)\n        Anchor_Info = {}\n        for i in range(len(header)):\n            genome = \"+\" + sequence[i].upper()\n            # if not complete genome\n            if len(genome) < 10000:\n                continue\n            # if too many ambiguous bases\n            basenum = genome.count(\"A\") + genome.count(\"G\") + genome.count('C') + genome.count(\"T\")\n            if float(basenum)\/len(genome) < 0.9:\n                continue\n            anchormapping = genome_registration(genome, anchorfile, k)\n            \n            Final_Dict= dict(anchormapping) # current mapping\n\n            samplename = samples[i]\n            D = {}\n            for anchorseq, refpos in anchor_c:\n                anchorname = \"A%05d\" % (int(refpos)\/k + 1)\n                D[anchorname] = Final_Dict.get(anchorseq, \"?\")\n            Anchor_Info[samplename] = D\n        return Anchor_Info\n\n    new_pos = pd.DataFrame(get_new_matrix(genomefile, anchorfile, registrationfile, k))\n    current_pos = pd.read_csv(registrationfile,header=0, index_col= 0)\n\n    if len(new_pos) == 0: \n        Integration = current_pos\n        print(\"No Assembly Added\")\n    else:\n        new_pos = new_pos.loc[current_pos.index, :]\n        Integration = pd.merge(new_pos, current_pos, left_index=True, right_index=True, how='outer')\n\n    return Integration","ef469481":"# Pangenome Construction\n\ndef dynamic_construct(df, exclude_strainlist = []):\n    '''Given anchor mapping dictionary, select conserved, unique and monotonically ordered anchors\n    Input: \n    df - <dataframe> registration table: anchor candidates by assembly id\n    exclude_strainlist - <list> assembly ID that are excluded from the original sets\n    Output:\n    AnchorList - <DataFrame> anchor by assemblyID matrix, each element is the unique mapping position of the anchor\n    '''\n    df = df.astype('str')\n    strainlist = list(df.columns)\n    strainlist = [item for item in strainlist if item not in exclude_strainlist]\n    df = df.loc[:,strainlist]\n    d = df == '?'\n    Anchorlist = df[d.sum(axis = 1) == 0]\n    return Anchorlist\n\n# collapse adjacent anchors\ndef collapsed_adjacent(AnchorDataFrame, k):\n    \"\"\"Given Anchor Table, Collapsed adjacent anchor candidates by only keeping the first and the last anchor in a continuous run\n    Input:\n    AnchorDataFrame: - <DataFrame>, anchor by assembly\n    k - <int> kmer length\n    Output:\n    AnchorDataFrame: <Dataframe>, anchor by assembly matrix after collapse adjacent anchors\n    \"\"\"\n    def find_blocks(Indexlist, k):\n        count = 0\n        Blocks = []\n        for i,s in enumerate(Indexlist):\n            if i == 0:\n                if Indexlist[i+1] - Indexlist[i] <= k:\n                    count += 1\n                    start = i\n            elif i > 0 and i < len(Indexlist)-1:\n                if Indexlist[i] - Indexlist[i-1] > k and Indexlist[i+1] - Indexlist[i] <= k:\n                    count +=1\n                    start = i\n                elif Indexlist[i] - Indexlist[i-1] <= k and Indexlist[i+1] - Indexlist[i] > k:\n                    end = i+1\n                    Blocks.append((start, end))\n            else:\n                if Indexlist[i] - Indexlist[i-1] <= k:\n                    end = i+1\n                    Blocks.append((start, end))\n        return count, Blocks\n\n\n    def sort_by_kmerlength(poslist, k):\n        poslist = numpy.array(poslist).astype(int)\n        errorindex = []\n        count, blocks = find_blocks(poslist, k)\n        for s,e in blocks:\n            if e - s < 3:\n                for i in range(s+1,e):\n                    errorindex.append(i)\n            else:\n                for i in range(s+1,e-1):\n                    errorindex.append(i)\n        return errorindex\n\n    columnnames = AnchorDataFrame.columns\n    for i in columnnames:\n        anchors = AnchorDataFrame.index\n        poslist = AnchorDataFrame[i].values.astype(int)\n        assert sum(sorted(poslist) == poslist) == len(poslist)\n        errorindex = sort_by_kmerlength(poslist, k)\n        index = [i for i in range(len(anchors)) if i not in errorindex]\n        AnchorDataFrame = AnchorDataFrame.iloc[index,:]\n        \n    return AnchorDataFrame\n\ndef create_pangenome(AnchorDataFrame, genomefile,k):\n    '''Create pangenome scaffold from Anchor Table and genome assembly\n    Input \n       AnchorDataFrame - <Dataframe> Table of anchors and their mapping position\n       genomefile - <fasta> filename of the sequence collection\n    Output:\n    Anchors - <dict> Anchor Information \n    Edges - <dict> Edge Information\n    '''\n    AnchorDataFrame = collapsed_adjacent(AnchorDataFrame,k)\n    \n    def genome_info(genomefile):\n        header, sequence, samples = getsequence_info(genomefile)\n        genome_Dict = {}\n        for i in range(len(header)):\n            genome = \"+\" + sequence[i]\n            genome_Dict[samples[i]] = genome\n        return genome_Dict\n    \n    def create_nodefile(genome_Dict, AnchorDataFrame,k):\n        genome_Dict = genome_info(genomefile)\n        Anchors = {}\n        refname = 'MN908947'\n        for anchor in AnchorDataFrame.index:\n            genome = genome_Dict[refname]\n            Anchors[anchor] = dict(AnchorDataFrame.loc[anchor])\n            pos = Anchors[anchor][refname]\n            Anchors[anchor]['seq'] = genome[int(pos):int(pos) + k]  \n        return Anchors\n    \n    def get_all_edge_sequence(AnchorDataFrame, genome_Dict, src, dst):\n        Strainlist = AnchorDataFrame.columns\n        D = {}\n        for strain in Strainlist:\n            seq = genome_Dict[strain].upper()\n            if src == 'SOURCE':\n                for i, s in enumerate(seq):\n                    if s!= \"N\" and s != '+':\n                        start = i\n                        break\n            else:\n                start = int(AnchorDataFrame.loc[src, strain]) + k\n\n            if dst == 'SINK':\n                end = len(seq)\n            else:\n                end = int(AnchorDataFrame.loc[dst, strain])\n\n            edge_seq = seq[start:end]\n            D[strain] = edge_seq\n\n        return D\n    \n    def get_edge_info(src,dst, AnchorDataFrame, genome_Dict):\n        D = get_all_edge_sequence(AnchorDataFrame, genome_Dict, src, dst)\n        Strain = Anchorlist.columns\n        Edge_D = {}\n        index = 0\n        my_k = 0\n        while my_k in range(len(Strain)):\n            index += 1\n            if src != \"SOURCE\":\n                edgename = 'E%08d' % (int(src[1:])*1000 + index)\n            else:\n                edgename = 'S%08d' % (index)\n            Edge_D[edgename] = {}\n            strain = Strain[my_k] \n            Name = []        \n            Name.append(strain)\n\n            if len(Strain) - my_k>1:\n                for j in range(my_k+1, len(Strain)):\n                    strain1 = Strain[j]\n                    if D[strain] == D[strain1]:\n                        Name.append(strain1)\n            Strain = [x for x in Strain if x not in Name]\n\n            Edge_D[edgename]['seq'] = D[strain]\n            Edge_D[edgename]['strain'] = sorted(Name)\n            Edge_D[edgename]['src'] = src\n            Edge_D[edgename]['dst'] = dst\n\n        return Edge_D    \n    \n    def create_edgefile(genome_Dict, AnchorDataFrame, k):\n        nodelist = [\"SOURCE\"] + list(AnchorDataFrame.index) + ['SINK']\n        Edge_info = {}\n        total = 0\n        for i in range(len(nodelist) - 1):\n            src = nodelist[i]\n            dst = nodelist[i + 1]\n            Edges = get_edge_info(src,dst, AnchorDataFrame, genome_Dict)\n            total += len(Edges)\n            Edge_info.update(Edges)    \n        return Edge_info\n    \n    genome_Dict = genome_info(genomefile)\n    Anchors = create_nodefile(genome_Dict,AnchorDataFrame,k)\n    Edges = create_edgefile(genome_Dict, AnchorDataFrame, k)\n    \n    return Anchors, Edges","53c981f0":"# add annotation\n\n# add genome feature\n# [seqid, source, type, start, end, score, strand, phase, attribute]\ndef get_gff_data(filename):\n    if filename.endswith('.gz'):\n        with gzip.open(filename, 'rb') as fp:\n            data = fp.readlines()\n    else:\n        with open(filename, 'rb') as fp:\n            data = fp.readlines()\n    return data\n\ndef parse_feature(data):\n    Parser = {}\n    count = 0\n    for line in data:\n        values = line.decode().split('\\t')\n        if len(values) < 9:\n            continue\n        start, end = int(values[3]), int(values[4])\n        # parse attributes\n        attributes = values[8]\n        Att = {}\n        attributes_info = attributes.split(';')\n        for item in attributes_info:\n            k, v = item.split('=')\n            Att[k] = v\n        #assert \"ID\" in Att.keys()\n        # build in   \n        name = Att[\"ID\"]\n        Parser[name] = {}\n        Parser[name][\"start\"] = start\n        Parser[name]['end'] = end\n        Parser[name]['att'] = Att\n        Parser[name]['strand'] = values[6]\n        Parser[name]['type'] = values[2]\n    return Parser\n\ndef annotatefeature(graph, filename,k):    \n    def write_cigar(g_start, g_end, i_start, i_end):\n        ''' input genestart, geneend, itemstart, item_end Gene include both ends [], items include the start [) '''\n        g_start = int(g_start)\n        g_end = int(g_end)\n        i_start = int(i_start)\n        i_end = int(i_end)\n        if i_start >= g_start and i_end - 1 <= g_end:\n            Cigar = str(i_end - i_start)+'M'\n\n        elif i_start < g_start and i_end - 1 <= g_end:\n            miss = g_start - i_start\n            Cigar = str(miss)+'S'+str(i_end-1-g_start+1)+'M'\n\n        elif i_start < g_start and i_end -1 > g_end: # gene inside a Node\n            miss = g_start - i_start\n            Cigar = str(miss)+'S'+str(g_end - g_start + 1)+'M' + str(i_end - 1 - g_end)+'S'\n\n        elif i_end - 1 > g_end:\n            miss = i_end -1 - g_end\n            Cigar = str(g_end - i_start + 1)+'M'+str(miss)+'S'\n\n        return Cigar\n\n\n    def find_itempos(Graph, item, k):\n        '''Input graph item, return start and end pos [istart, iend), end pos not included'''\n        if item.startswith('A'):\n            i_start = int(Graph.nodes[item]['MN908947'])\n            i_end = i_start + k\n        elif item.startswith('F') or item.startswith('B') or item == 'SOURCE' or item == 'SINK':\n            return \n        else:\n            snode = Graph.edges[item]['src']\n            if snode == 'SOURCE':\n                i_start = 1\n            else:\n                i_start = int(Graph.nodes[snode]['MN908947']) + k\n\n            i_end = i_start + len(Graph.edges[item]['seq'])\n\n        return (i_start, i_end)\n\n    def addannotation(Graph, Feature_info,k):\n        featurelist = Feature_info.keys()\n        for g_ID in featurelist:\n            D = Feature_info[g_ID]\n            g_start = int(D['start'])\n            g_end = int(D['end'])\n            strand = D['strand']\n            key = D['type']\n            sanchor, eanchor = Graph.boundingAnchors('MN908947', g_start, g_end)\n            itemlist = Graph.tracePath('MN908947', sanchor, eanchor)\n\n            for item in itemlist:\n                if item == 'SOURCE' or item == 'SINK':\n                    continue\n                i_start, i_end = find_itempos(graph, item,k)\n                if i_end <= g_start or i_start > g_end:\n                    continue\n                    \n                cigar = write_cigar(g_start, g_end, i_start, i_end)\n#                 if cigar == '0M':\n#                     print( cigar, item,g_start, g_end, i_start, i_end, len(Graph.edges[item]['seq']))\n                \n                if item.startswith('A'):\n                    annote = '|'.join((g_ID, strand, cigar))\n                    if annote in Graph.nodes[item].get(key, []):\n                        continue\n                    Graph.nodes[item][key] = Graph.nodes[item].get(key, []) + ['|'.join((g_ID, strand, cigar))]\n                else:\n                    annote = '|'.join((g_ID, strand, cigar))\n                    if annote in Graph.edges[item].get(key, []):\n                        continue\n                    Graph.edges[item][key] = Graph.edges[item].get(key, []) + ['|'.join((g_ID, strand, cigar))]\n        return Graph\n    \n    data = get_gff_data(filename)\n    feature_info = parse_feature(data)\n    graph = addannotation(graph, feature_info,k)\n    return graph\n\n# add variants\ndef addvariants(Graph, maxlength,  refstrain = \"MN908947\"):\n    def makeCigar(seq, ref):\n        if (len(seq) > 16384) or (len(ref) > 16384):\n            rmid = len(ref)\/2        \n            smid = len(seq)\/2\n            prox = makeCigar(seq[:smid],ref[:rmid])\n            dist = makeCigar(seq[smid:],ref[rmid:])\n            return prox+dist\n        ops = Levenshtein.editops(seq,ref)\n        code = ['=' for i in range(len(seq))]\n        offset = 0\n        for op, si, di in ops:\n            if (op == \"replace\"):\n                code[si+offset] = 'X'\n            elif (op == \"insert\"):\n                code.insert(si+offset,'D')\n                offset += 1\n            elif (op == \"delete\"):\n                code[si+offset] = 'I'# LM: fixed bug here 2019-04-15\n        cigar = ''\n        count = 1\n        prev = code[0]\n        for c in code[1:]:\n            if (c == prev):\n                count += 1\n            else:\n                cigar += \"%d%c\" % (count, prev)\n                count = 1\n                prev = c\n        cigar += \"%d%c\" % (count, prev)\n        return cigar\n\n    # check the length of the cigar string is valid\n    def search_valid_size(variants,seq):\n        ref_length,alt_length = 0,0\n        for v in variants:\n            base, op = v\n            base = int(base)\n            if op == '=' or op == 'X':\n                ref_length += base\n                alt_length += base\n            elif op == 'I':\n                alt_length += base\n            elif op == 'D':\n                ref_length += base\n\n        return len(seq) == alt_length \n\n    # check the length of the cigar string is valid\n    def valid_size(variants,seq,reference):\n        ref_length,alt_length = 0,0\n        for v in variants:\n            base, op = v\n            base = int(base)\n            if op == '=' or op == 'X':\n                ref_length += base\n                alt_length += base\n            elif op == 'I':\n                alt_length += base\n            elif op == 'D':\n                ref_length += base\n\n        assert len(seq) == alt_length \n        assert len(reference) == ref_length\n\n    def valid_eqaul_and_mismatch(variants,seq,reference):\n        ref_pos,alt_pos = 0,0\n        for v in variants:\n            base,op = v\n            base = int(base)\n            if op == '=':\n                assert seq[alt_pos:alt_pos+base] == reference[ref_pos:ref_pos+base]\n                ref_pos += base\n                alt_pos += base\n            elif op == 'X':\n                assert seq[alt_pos:alt_pos+base] != reference[ref_pos:ref_pos+base]\n                ref_pos += base\n                alt_pos += base\n            elif op == 'I':\n                alt_pos += base\n            elif op == 'D':\n                ref_pos += base\n\n    #assert len(reference) == ref_length\n    def split(variant):\n        splitVariants = []\n        previous = 0\n        for i in range(len(variant)):\n            v = variant[i]\n            if v.isdigit():\n                continue\n            else:\n                splitVariants += [variant[previous:i]]\n                splitVariants += [v]\n                previous = i + 1\n\n        numbers, types = [],[]\n        for i in range(len(splitVariants)):\n            v = splitVariants[i]\n            if i %2 == 0:\n                numbers += [v]\n            else:\n                types += [v]\n\n        variant = []\n        for i in range(len(numbers)):\n            variant += [(numbers[i],types[i])]\n        return variant\n\n    def findBedge(Graph, src, dst, refstrain):\n        edgelist = Graph.outgoing[src]\n        for edge in edgelist:\n            sequence = Graph.edges[edge]['seq'].upper()\n            strain = Graph.edges[edge]['strain']\n            if refstrain in strain:\n                B_dst = Graph.edges[edge]['dst']\n                if B_dst == dst:\n                    return edge\n                else:\n                    return \"\"\n        else:\n            return ''\n\n    # mask Ns on the alternative path\n    def maskNs(seq):\n        seq = seq.upper()\n        newseq = ''\n        for i in seq:\n            if i not in \"AGCT\":\n                newseq += i.lower()\n            else:\n                newseq += i\n        return newseq\n    \n    def add_variants(Graph, maxlength, refstrain, gapopen = False):\n        Nodelist = sorted(Graph.nodes.keys())\n        Nodelist = [\"SOURCE\"] + Nodelist\n        for node in Nodelist:\n            sanchor = node\n            eanchor = Graph.nextAnchor(sanchor)\n            edgelist = Graph.outgoing[sanchor]\n            ref_edge = findBedge(Graph, sanchor, eanchor, refstrain)\n\n            # fix redundant Ns on SOURCE\n            if sanchor == \"SOURCE\":\n                seq = Graph.edges[ref_edge]['seq'].upper()\n                for i, s in enumerate(seq):\n                    if s!= \"N\":\n                        start = i\n                        break\n                ref_seq = seq[start:]\n                ref_length = len(ref_seq)\n                Graph.edges[ref_edge]['seq'] = ref_seq  \n            # fix redundant Ns on SINK\n            elif eanchor == \"SINK\":\n                seq = [i for i in reversed(Graph.edges[ref_edge]['seq'].upper())]\n                for i, s in enumerate(seq):\n                    if s!= \"N\":\n                        start = i\n                        break         \n                ref_seq = seq[start:]\n                ref_seq = ''.join([i for i in reversed(ref_seq)])\n                ref_length = len(ref_seq)\n                Graph.edges[ref_edge]['seq'] =  ref_seq\n            else:\n                ref_seq = Graph.edges[ref_edge]['seq'].upper()\n                ref_length = len(ref_seq)\n\n\n\n            if ref_length > maxlength:\n                continue\n\n            for alt_edge in edgelist:\n                if alt_edge == ref_edge:\n                    Graph.edges[ref_edge]['variants'] = str(ref_length) + '='\n                    continue\n\n                alt_seq = maskNs(Graph.edges[alt_edge]['seq']) # mask Ns\n                alt_length = len(alt_seq)\n\n                if alt_length > maxlength:\n                    continue\n\n                if gapopen:\n                    delta = abs(alt_length - ref_length)\n                    if delta > 100:\n                        if delta > alt_length or delta > ref_length:\n                            cigar = GapOpenAligner(ref_seq, alt_seq)\n                            variants = split(cigar)\n                            valid_size(variants, alt_seq, ref_seq)\n                            valid_eqaul_and_mismatch(variants, alt_seq, ref_seq)\n                        else:\n                            cigar = makeCigar(alt_seq, ref_seq)\n                            variants = split(cigar)\n                            valid_size(variants, alt_seq, ref_seq)\n                            valid_eqaul_and_mismatch(variants, alt_seq, ref_seq)\n                    else:\n                        cigar = makeCigar(alt_seq, ref_seq)\n                        variants = split(cigar)\n                        valid_size(variants, alt_seq, ref_seq)\n                        valid_eqaul_and_mismatch(variants, alt_seq, ref_seq)\n                else:\n                    cigar = makeCigar(alt_seq, ref_seq)\n                    variants = split(cigar)\n                    valid_size(variants, alt_seq, ref_seq)\n                    valid_eqaul_and_mismatch(variants, alt_seq, ref_seq)\n\n                Graph.edges[alt_edge]['variants'] = cigar\n    \n    add_variants(Graph, maxlength, refstrain, gapopen = False)\n    return Graph\n","94a0dea0":"# Run\nfilename1 = '\/kaggle\/input\/sarscov2-reference-genbackmn908947\/GCA_009858895.3_ASM985889v3_genomic.fna'\nheader, seq = loadFasta(filename1)\ngenome = \"+\" + seq[0].upper()\n\n# Select Valid k\nfor k in range(5,12):\n    kmers = construct_kmers(filename1, k, profiletype = 0)\n    print( \"kmerlength:\", k, \",non-overlapping kmer num:\", len(kmers['MN908947.3']), \n          \",kmer Number:\", len(seq[0]) - k + 1, \"all possible kmer Number:\", 4 ** k)\nprint(\"Minimal k = \", 8)\n","57ff3407":"# Select anchor candidates\nk = 11\n%time kmers = construct_kmers(filename1, k, profiletype = 0)\n%time PositionProfile = mapping_position(genome, kmers, k)\n%time anchorcandidates = create_anchors(kmers, PositionProfile, k)\nnumpy.save(\"anchorcandidates\",anchorcandidates)\nprint(\"Number of Anchor Candidates:\", len(anchorcandidates))\n\nanchorcandidates","248cc216":"# Genome Registration\ngenomefile = '..\/input\/graphicalgenomeapi\/NCBI_567.fasta'\nanchorfile = \"..\/working\/anchorcandidates.npy\"\nk = 11\n%time registration_table = integrate_info(genomefile, anchorfile, k)\n#header, sequence = loadFasta(genomefile)","785bacd2":"# Pangenome Construction\n#genomefile = '..\/input\/graphicalgenomeapi\/NCBI_567.fasta'\n#registration_table = pd.read_csv(\"RegistrationTable.csv\", index_col = 0)\nAnchorlist = dynamic_construct(registration_table)\nprint(Anchorlist.shape)#Anchorlist1.shape\n%time Anchor_Dict,Edge_Dict = create_pangenome(Anchorlist, genomefile, 11)\nwriteGraphFasta(\"Cov_Nodefile.fa\",Anchor_Dict)\nwriteGraphFasta(\"Cov_Edgefile.fa\",Edge_Dict)","fc7451b7":"graph = CCGG.GraphicalGenome(\"Cov_Nodefile.fa\", \"Cov_Edgefile.fa\", nnamelen= 6, enamelen=9)\n%time graph = addvariants(graph, 3000)\n\nannotationfile = \"..\/input\/graphicalgenomeapi\/annotation.gff\"\n%time graph = annotatefeature(graph, annotationfile,11)\n\nwriteGraphFasta(\"Cov_Nodefile.fa\",graph.nodes)\nwriteGraphFasta(\"Cov_Edgefile.fa\",graph.edges)\n","30e47e95":"# validation\ngraph = CCGG.GraphicalGenome(\"Cov_Nodefile.fa\", \"Cov_Edgefile.fa\", nnamelen= 6, enamelen=9)\n\ndef Validate_sequence(Graph, genome_Dict, strain, path = 0):\n    conseq = Graph.reconstructSequence(strain, path)\n    lineargenome = genome_Dict[strain][1:].upper()\n    return conseq.upper() == lineargenome.upper()\n\nstrainlist = Anchorlist.columns\nprint( len(strainlist))\n\ndef genome_info(genomefile):\n    header, sequence, samples = getsequence_info(genomefile)\n    genome_Dict = {}\n    for i in range(len(header)):\n        genome = \"+\" + sequence[i]\n        genome_Dict[samples[i]] = genome\n    return genome_Dict\n\ngenome_Dict = genome_info(genomefile)\nprint(len(genome_Dict))\nfor strain in strainlist:\n    assert Validate_sequence(graph, genome_Dict, strain)\nprint(\"PASS\")","82b5f4b2":"# Graphical Genome Statistics\n\ngraph = CCGG.GraphicalGenome(\"Cov_Nodefile.fa\", \"Cov_Edgefile.fa\", nnamelen= 6, enamelen=9)\nrefstrain = 'MN908947'\ndef getfounders(Graph):\n    '''Input graph, return a list of assembly IDs incorporated in the graphical genome'''\n    edgelist = Graph.outgoing['SOURCE']\n    founder = []\n    for edge in edgelist:\n        founder += Graph.edges[edge]['strain']\n    return founder\n\n# Basic statistics\nfounders = getfounders(graph)\nprint( \"Number of linear genome\", len(founders))\ntotalsize = 0\nnodelist = graph.nodes.keys()\nedgelist = graph.edges.keys()\nprint( \"Anchor Num\", len(nodelist))\nprint( 'Edge Num', len(edgelist))\n\ntotalsize = 0\nfor edge in edgelist:\n    totalsize += len(graph.edges[edge]['seq'])\nfor node in nodelist:\n    totalsize += len(graph.nodes[node]['seq'])\nprint( \"Pangenome Size\", totalsize)\n\noriginalsize = 0\ngenome_Dict = genome_info(genomefile)\nfor strain in founders:\n    originalsize += len(genome_Dict[strain]) - 1 # delete the \"+\" sign\nprint(\"Original Size\", originalsize)\nprint( \"Compress Percentage\", float(totalsize)\/originalsize)\n","7ae6a396":"# Anchor Distribution\nrefseq = graph.reconstructSequence(refstrain,path=0)\nintervallength = 1000\nnodelist = sorted(graph.nodes.keys())\nAnchorpos = [int(graph.nodes[anchor][refstrain]) for anchor in nodelist]\ny,x = numpy.histogram(Anchorpos, bins = len(refseq)\/\/intervallength, range = [1, len(refseq) + 1])\nfig = plt.figure(figsize = [10,5])\nplt.plot(x[1:],y, color='r', linestyle='-',linewidth = 1.5)\n#plt.xlim([1,len(refseq)+1])\n#plt.xticks(numpy.arange(1, len(refseq)+intervallength, step= intervallength*2) ,rotation = -60) \nplt.xticks(x.astype(int) ,rotation = -60,fontsize = 8) \n\nplt.xlabel('Genome Position \/ 1Kb', fontsize = 12)\nplt.ylabel('Num of Anchors', fontsize = 12)\nplt.title(\"Anchor Distribution\", fontsize = 20)\nplt.show()\nplt.savefig(\"AnchorDistribution\")\n","8f068cc8":"# Edge statistics\nfounders = getfounders(graph)\nedgenum = numpy.zeros(len(founders)) # initialize a vector for edge statistics\n\n# how many edges within each gap\nnode = \"SOURCE\"\nwhile node != \"SINK\":\n    edgelist = graph.outgoing[node]\n    edgenum[len(edgelist)] += 1\n    node = graph.nextAnchor(node)\n    \nstat = edgenum.astype(int)\nHapNum = numpy.where(stat != 0)[0]\n\nplt.figure(figsize = [10,6])\nplt.bar(range(len(HapNum)), stat[HapNum])\nplt.xticks(range(len(HapNum)), HapNum, fontsize = 15)\nplt.xlabel('Edge Num', fontsize = 20)\nplt.ylabel(\"Number of Gaps\", fontsize = 20)\nplt.show()\nplt.savefig(\"EdgeNumberDistribution\")\n\n# What's the genomic fractions for each type of gaps (with 1,2...n haplotypes)?\nHapNums = numpy.where(stat != 0)[0] # all possible hap num in graph\nprint( HapNums)\nHaplotype_Fraction = numpy.zeros(len(HapNums)) # edge number between two anchors\nk = 11 # length of anchors\n\n# counting the number of reference bases within each gap\nnode = \"SOURCE\"\nwhile node != \"SINK\":\n    edgelist = graph.outgoing[node]\n    edge_num = len(edgelist)\n    if node.startswith('A'):\n        Haplotype_Fraction[numpy.where(HapNums == 1)[0][0]] += k\n    for edge in edgelist:\n        strain = graph.edges[edge]['strain']\n        if refstrain in strain:\n            Bseq = len(graph.edges[edge]['seq'])\n            break\n    Haplotype_Fraction[numpy.where(HapNums == edge_num)[0][0]] += Bseq\n    \n    node = graph.nextAnchor(node)\n\n# print(Haplotype_Fraction)\n# print(sum(Haplotype_Fraction))\n\nfig = plt.figure(figsize = [10,8])\nplt.bar(range(len(HapNums)), Haplotype_Fraction\/float(sum(Haplotype_Fraction)))\nplt.xticks(range(len(HapNums)),HapNums,fontsize = 12)\nplt.xlabel('Edge Num', fontsize = 20)\nplt.ylabel(\"Genomic Fraction\",fontsize = 20)\nplt.show()\nplt.savefig(\"EdgeNum_vs_GenomicFraction.png\")\n","ba87ff96":"# anchor distribution versus outgoing edge number\nanchor_edge = []\nnode = \"SOURCE\"\nwhile node != \"SINK\":\n    node = graph.nextAnchor(node)\n    if node == \"SINK\":\n        continue\n    pos = int(graph.nodes[node][refstrain])\n    hap = len(graph.outgoing[node])\n    anchor_edge.append((pos,hap))\nanchoredgeinfo = numpy.array(anchor_edge)[:-1, :]\nplt.figure(figsize=[10,8])\n#print( max(anchoredgeinfo[:,1]), numpy.argmax(anchoredgeinfo[:,1]), anchoredgeinfo[472,:])\nplt.bar(anchoredgeinfo[:,0], anchoredgeinfo[:,1], width=50)\nplt.xlabel(\"Anchor Position\", fontsize = 10)\nplt.ylabel(\"Outgoing Edge Num\", fontsize = 10)\nplt.title('Anchor Distribution and Edge Number within Gaps')\nplt.savefig(\"AnchorDist_vs_edgeNum.png\")\n","9dd39160":"# Whole genome Haplotype Distribution for each 500bp region\ndef get_gene_interval(annotationfilename):\n    data=get_gff_data(annotationfilename)\n    Parser = parse_feature(data)\n    gene_d = {}\n    interval = []\n    for name,D in Parser.items():\n        if D['type'] == 'gene':\n            Name = D['att']['Name']\n            gene_d[Name] = {}\n            gene_d[Name]['start'] = D['start']\n            gene_d[Name]['end'] = D['end']\n            interval.append((D['start'], D['end'], Name))\n    return interval\n\ninterval = get_gene_interval(annotationfile)\n\n\nintervallength = 500\nn = len(refseq)\/\/intervallength\n#print n, len(refseq)\n\nHapNum = []\n# get Hap Num\nfor i in range(n):\n    start = intervallength * i + 1\n    end = start + intervallength\n    sa, ea = graph.boundingAnchors(refstrain, start, end)\n    path = graph.getSubPath(sa,graph.prevAnchor(ea),init_strain= set(strainlist))\n    HapNum.append(len(path))\n    \npath = graph.getSubPath(graph.prevAnchor(ea), 'SINK',init_strain= set(strainlist))\nHapNum.append(len(path))\nprint(HapNum)\nprint( len(HapNum), max(HapNum[2:-1]))\n\n# plot\n# haplotype plot\nfig = plt.figure(figsize = [20,10])\nplt.plot(range(1,len(refseq), intervallength)[1:-2],HapNum[2:-1], linestyle='-',linewidth = 1.5)\n# overlay genes\ncolors = ['r','g','b','k','grey','orange','purple','pink','steelblue','darkblue','darkred']\n#plt.axhline(y=20, xmin=0, xmax= 1, color = 'red', linestyle='-', linewidth=5)\ni = 0\ninterval = sorted(interval)\ngenenames = []\nfor s, e, name in interval:\n    plt.axhline(y=max(HapNum[2:-1])+1+(i) + 0.5, xmin=float(s)\/(len(refseq)+intervallength), xmax= float(e)\/(len(refseq)+intervallength), \n                color = colors[i], linestyle='-', linewidth=2)\n    genenames.append(name)\n    if i < 2:\n        plt.text(s + (e-s)\/2, max(HapNum[2:-1])+1+(i) +1, name, va='center', ha='center', fontsize = 15)\n    else:        \n        plt.text(s + (e-s)\/2, max(HapNum[2:-1])+1+(i) +1, name, va='center', ha='center', fontsize = 10)    \n    i = i + 1\n\nplt.legend([\"HaplotypeNum\"] + genenames, loc = 'best', fancybox=True, shadow=False, fontsize = 13)# bbox_to_anchor=(1, .95),\n\n    \nplt.xlim([1,len(refseq)+1])\nplt.ylim([0,max(HapNum[2:-1])+1+(i)+0.8]) # exclude the fist and last bins\n\nplt.xticks(numpy.arange(1001, len(refseq)+intervallength, step= intervallength * 2) ,rotation = -30, fontsize = 15) \nplt.yticks(numpy.arange(1, max(HapNum[2:-1]), step= 2),fontsize = 10) \n\nplt.xlabel('Genome Position\/%sbp' % str(intervallength), fontsize = 20)\nplt.ylabel('Num of Paths', fontsize = 20)\nplt.title(\"Haplotype Distribution\", fontsize = 20)\nplt.show()\nfig.savefig(\"HaplotypeDistribution.png\")","3ab88b57":"# How many distinct versions for each gene?\ndef anchor_coordinates_exchanging(Graph, s, e, refstrain):\n    '''Given a list of intervals on the reference genome, mapping the start and the end coordinates to parallel edges, \n    return all possible distinct sequences through graph traversal\n    Input:\n    Graph - <class> Graphical Genome\n    s - <int> start coordinate\n    e - <int> end coordinate\n    refstrain - <str> the Assembly ID of the reference genome\n    \n    Output:\n    Haplotype - <dict> A dictionary of distinct sequence versions and a list of assemblies that share the pattern\n    '''\n    def find_offset_on_path(Graph, itemlist, entity, offset, k = 11):\n        prevoff = 0\n        for item in itemlist:\n            if item == entity:\n                path_coord = prevoff + offset\n                return path_coord\n            else:\n                if item.startswith('A'):\n                    prevoff += k\n                elif item.startswith('F') or item.startswith('B') or item == 'SOURCE' or item == 'SINK':\n                    prevoff += 0\n                else:\n                    prevoff += len(Graph.edges[item]['seq'])\n\n\n    def offsetexchange(cigar, B_offset):\n        alt_i = 0\n        ref_i = 0\n        for i, s in enumerate(cigar):\n            if ref_i == B_offset:\n                return alt_i   \n            if s == '=':\n                alt_i += 1\n                ref_i += 1\n\n            if s == 'I':\n                alt_i += 1\n            if s == 'D':\n                ref_i += 1\n            if s == 'X':\n                alt_i += 1\n                ref_i += 1\n\n    def get_all_altoffset(graph, s, refstrain):\n        Alt = []\n        item,bpos = graph.linear_to_entityoffset(s, refstrain)\n\n        # if start from anchor\n        if item.startswith('A'):\n            Alt.append((item, bpos))\n        else:\n        # start from edge\n            src = graph.edges[item]['src']\n            edgelist = graph.outgoing[src]\n\n            for edge in edgelist:\n                strain = graph.edges[edge]['strain']\n                cigar = graph.processCigar(graph.edges[edge]['variants'])\n                alt_pos = offsetexchange(cigar,bpos)\n                Alt.append((edge,alt_pos))\n        return Alt\n\n    def get_haplotype_sequence(graph, s, e, refstrain):\n        StartPos = get_all_altoffset(graph, s, refstrain)\n        EndPos = get_all_altoffset(graph, e, refstrain)\n\n        start, end = graph.boundingAnchors(refstrain, s, e) # Covgenome\n        Path = graph.getSubPath(start, end, init_strain= set(founders))\n        Haplotype = {}\n        for itemlist, strainlist in Path:\n            for item, pos in StartPos:\n                if item in itemlist:\n                    pathstart = find_offset_on_path(graph, itemlist, item, pos, k = 11)\n                    break\n\n            for item, pos in EndPos:\n                if item in itemlist:\n                    pathend = find_offset_on_path(graph, itemlist, item, pos, k = 11)\n                    break\n            seq = graph.findsequence(itemlist,countinganchor=True)[pathstart:pathend+1]\n            Haplotype[seq] = Haplotype.get(seq, []) + list(strainlist)\n        return Haplotype\n\n    start, end = Graph.boundingAnchors(refstrain, s, e) # Covgenome\n    H = get_haplotype_sequence(Graph, s, e, refstrain)\n    \n    return H\n\nGeneinfo = []\nfor s, e, name in interval:\n    start, end = graph.boundingAnchors(refstrain, s,e)\n    Hap = anchor_coordinates_exchanging(graph, s, e, refstrain)\n    Geneinfo.append((name,s,e, e-s, start, end, len(Hap), len(Hap)\/float(e-s)*1000))\nGeneinfo = pd.DataFrame(Geneinfo, columns = ['genename', 'refstart','refend', 'genelength', \n                                             'startanchor','endanchor','HapNum', \"HapNum\/kb\"])\nGeneinfo.sort_values(by=['HapNum\/kb'])","3c3b39fd":"def get_variant_position(cigar):\n    '''helper function\n    identify variants given an expanded cigar string\n    return the offset on the alternative edge and the mapping offset on reference path\n    '''\n    ref_pos = []\n    alt_pos = []\n    alt_i = 0\n    ref_i = 0\n    for i, s in enumerate(cigar):\n        if s == 'I':\n            if ref_i > 0:\n                ref_pos.append(ref_i-1)\n            else:\n                ref_pos.append(ref_i)    \n            alt_pos.append(alt_i)\n            alt_i += 1\n        if s == 'D':\n            ref_pos.append(ref_i)\n            if alt_i > 0:\n                alt_pos.append(alt_i-1)\n            else:\n                alt_pos.append(alt_i)\n            ref_i += 1\n            \n        if s == 'X':\n            ref_pos.append(ref_i)\n            alt_pos.append(alt_i)\n            alt_i += 1\n            ref_i += 1\n            \n        if s == '=':\n            alt_i += 1\n            ref_i += 1\n\n    return ref_pos, alt_pos\n\ndef find_allVar(graph, refstrain, founder, k):\n    '''find all variants, record variants coordinates and its mapping coordinates on the reference genome'''\n    Var = {} # Var[refpos]['edgename'] = ['A']\n    node = \"SOURCE\"\n    while node != \"SINK\":\n        edgelist = graph.outgoing[node]\n        \n        for edge in edgelist:\n            if refstrain in graph.edges[edge]['strain']:\n                refseq = graph.edges[edge]['seq']\n                refedge = edge\n                break\n                \n        for edge in edgelist:\n            cigar = graph.edges[edge]['variants']\n            if node != \"SOURCE\":\n                refstart = int(graph.Nodecoordinates(node, strainlist = founder)[refstrain]) + k\n            else:\n                refstart = int(graph.Nodecoordinates(node, strainlist = founder)[refstrain])\n            \n            refpos, altpos = get_variant_position(graph.processCigar(cigar))\n            #print refpos, altpos\n            alt_seq = graph.edges[edge]['seq']\n            for i, rp in enumerate(refpos):\n                pos = refstart + rp\n                Var[pos] = Var.get(pos, {})\n                Var[pos][edge] = Var[pos].get(edge, {})                \n                Var[pos][edge]['base'] = Var[pos][edge].get('base', \"\") + alt_seq[altpos[i]]\n                Var[pos][edge]['altoffset'] = Var[pos][edge].get('altoffset', []) + [altpos[i]]\n                Var[pos][refedge] = Var[pos].get(refedge, {})\n                Var[pos][refedge]['base'] = refseq[refpos[i]]\n        node = graph.nextAnchor(node)    #print cigar, refpos, altpos\n    return Var\n\ndef coordinate_exchange(Graph, coord, alter_strain, refstrain, k):\n    '''Given a linear coordinate on the reference path, return the mapping coordinates on alternative genome'''\n    def find_offset_on_path(Graph, itemlist, entity, offset):\n        prevoff = 0\n        for item in itemlist:\n            if item == entity:\n                path_coord = prevoff + offset\n                return path_coord\n            else:\n                if item.startswith('A'):\n                    prevoff += k\n                elif item.startswith('F') or item.startswith('B') or item == 'SOURCE' or item == 'SINK':\n                    prevoff += 0\n                else:\n                    prevoff += len(Graph.edges[item]['seq'])\n\n    def completecigar(Graph, edgelist, kmer = k):\n        cigar = ''\n        for edge in edgelist:\n            if edge.startswith('B') or edge.startswith('F') or edge == 'SOURCE' or edge == 'SINK':\n                continue\n            if edge.startswith('A'):\n                cigar += str(kmer) + '='\n                continue\n            if 'variants' in Graph.edges[edge].keys():\n                cigar += Graph.edges[edge]['variants']\n            else:\n                #print edge\n                return False\n        else:\n            return cigar\n\n    def findsequence(Graph, pathlist):\n        seq = ''\n        for item in pathlist:\n            if item.startswith('A'):\n                seq += '' # do not count anchor length\n            elif item.startswith('L') or item.startswith('E')or item.startswith('K'):\n                seq += Graph.edges[item]['seq']\n            elif item.startswith('S') and item != \"SOURCE\" and item != 'SINK':\n                seq += Graph.edges[item]['seq']\n            else:\n                seq += ''\n        return seq\n    \n    def offsetexchange(cigar, B_offset):\n        alt_i = 0\n        ref_i = 0\n        for i, s in enumerate(cigar):\n            if ref_i == B_offset:\n                return alt_i   \n            if s == '=':\n                alt_i += 1\n                ref_i += 1\n\n            if s == 'I':\n                alt_i += 1\n            if s == 'D':\n                ref_i += 1\n            if s == 'X':\n                alt_i += 1\n                ref_i += 1\n            \n    def Complete_Parallel_edge_offset_exchange(Graph, coord, alter_strain, founderlist, refstrain, output = 0):\n        \"\"\"Alignment-based entity offset exchange\n        (only apply to parallel edge, edge share the same src and dst, offset exchange in Path scale are not finished yet)\n        Given entity+offset on B path, return the position on the alternative strain\n        If alignment results are not applicable, return None\n\n        Parameters:\n            entity: <str> - Graph entity on B path\n            offset: <int> - offset on the entity\n            alter_strain: <str> - strain attributes for the target alternative path position\n            strain: <default> \"B\" path, when multi-alignment cigar are added, this could be further implemented\n        \"\"\"\n        entity, offset = Graph.linear_to_entityoffset(coord, refstrain)\n\n        if entity.startswith('A'):\n            alter_coord = Graph.Nodecoordinates(entity, strainlist=founderlist)[alter_strain] + offset\n            return alter_coord, offset\n        else:\n            src = Graph.edges[entity]['src']    \n            if src.startswith('A') or src == 'SOURCE':\n                s_anchor = src\n            else:\n                s_anchor = Graph.prevAnchor(src)\n            d_anchor = Graph.nextAnchor(s_anchor)\n\n            # construct path\n            Path = Graph.getSubPath(s_anchor, d_anchor, init_strain=founderlist)\n            for itemlist, strain in Path:\n                if refstrain in strain and alter_strain in strain:\n                    path_coord = find_offset_on_path(Graph, itemlist, entity, offset)\n                    alter_coord = Graph.Nodecoordinates(s_anchor, strainlist = founderlist)[alter_strain] + path_coord\n                    return alter_coord, path_coord\n                \n                elif refstrain in strain:\n                    B_offset = find_offset_on_path(Graph, itemlist, entity, offset)\n                    \n                elif alter_strain in strain:\n                    c_cigar = completecigar(Graph, itemlist)\n                    if c_cigar == False: # not applicable in covid19 pangenome\n                        for il, bstrain in Path:\n                            if refstrain in bstrain:\n                                ref_seq = Graph.findsequence(il, countinganchor=True)\n                                break\n                        alt_seq = Graph.findsequence(itemlist, countinganchor=True)\n                        cigar = makeCigar(alt_seq, ref_seq)\n                        cigar = Graph.processCigar(cigar)\n                    else:\n                        cigar = Graph.processCigar(c_cigar)\n\n            path_coord = offsetexchange(cigar, B_offset)\n            #print cigar, len(cigar),B_offset\n            alter_coord = Graph.Nodecoordinates(s_anchor, strainlist=founderlist)[alter_strain] + path_coord\n            return alter_coord, path_coord\n        \n    founderlist = []\n    for edge in Graph.outgoing[\"SOURCE\"]:\n        founderlist += Graph.edges[edge]['strain']\n    \n    alt_coord, path_coord = Complete_Parallel_edge_offset_exchange(Graph, coord, alter_strain, set(founderlist), refstrain)\n    \n    return alt_coord, path_coord\n\n# collapse adjacent \ndef collapsed_adjacent(Hap_Dict, step):\n    '''Given the dictionary of Variants, collapsed the allels on a continuous run'''\n    k = step\n    def find_blocks(Indexlist, k):\n        count = 0\n        Blocks = []\n        for i,s in enumerate(Indexlist):\n            if i == 0:\n                if Indexlist[i+1] - Indexlist[i] <= k:\n                    count += 1\n                    start = i\n            elif i > 0 and i < len(Indexlist)-1:\n                if Indexlist[i] - Indexlist[i-1] > k and Indexlist[i+1] - Indexlist[i] <= k:\n                    count +=1\n                    start = i\n                elif Indexlist[i] - Indexlist[i-1] <= k and Indexlist[i+1] - Indexlist[i] > k:\n                    end = i+1\n                    Blocks.append((start, end))\n            else:\n                if Indexlist[i] - Indexlist[i-1] <= k:\n                    end = i+1\n                    Blocks.append((start, end))\n        return count, Blocks\n\n\n    def sort_by_kmerlength(poslist, k):\n        poslist = numpy.array(poslist).astype(int)\n        errorindex = []\n        count, blocks = find_blocks(poslist, k)\n        pos_inblocks = []\n        for s,e in blocks:\n            pos_inblocks.append((poslist[s], poslist[e-1]))\n            \n        for s,e in blocks:\n            for i in range(s,e):\n                errorindex.append(i)\n                \n        return errorindex, pos_inblocks\n\n    RefIndexlist = sorted(Hap_Dict.keys())\n    errorindex, pos_inblocks = sort_by_kmerlength(RefIndexlist, k)\n    index = [RefIndexlist[i] for i in range(len(RefIndexlist)) if i not in errorindex]\n    index += pos_inblocks\n    \n    return index\n\ndef Var2Hap(Var, founder, k):\n    RefIndex = collapsed_adjacent(Var, 1)\n    \n    Hap = {} # Hap[refpos][haplotype] = [edges]\n    Ns = set()\n    #print len(RefIndex)\n    for refpos in RefIndex:\n        if isinstance(refpos, int):\n            Var_D = Var[refpos]\n        else:\n            s,e = refpos # both end included\n            Var_D = {}\n            sanchor, eanchor = graph.boundingAnchors(refstrain, s,e)\n            Path = graph.getSubPath(sanchor, eanchor, init_strain=set(founder))\n            \n            for itemlist, strains in Path:\n                edge = itemlist[1]\n                assert edge.startswith('E') or edge.startswith(\"S\")\n\n                sequence = graph.findsequence(itemlist,countinganchor=True)\n                alter_strain = list(strains)[0]\n                alts, soffset = coordinate_exchange(graph, s, alter_strain, refstrain, k)\n                alte, eoffset = coordinate_exchange(graph, e, alter_strain, refstrain, k)\n                haplotype = sequence[soffset:eoffset+1]\n                Var_D[edge] = {}\n                Var_D[edge]['altoffset'] = [range(soffset,eoffset+1)]\n                Var_D[edge]['base'] = haplotype\n        H = {}\n        for edge, D in Var_D.items():\n            haplotype = D['base']\n            \n            if \"N\" in haplotype: # exlude all Ns\n                Ns.add(edge)\n                continue\n            \n            altpos = D.get('altoffset', \"?\")\n            if altpos == \"?\":\n                H[haplotype] = H.get(haplotype, []) + [(edge, refpos)]\n            else:\n                H[haplotype] = H.get(haplotype, []) + [(edge, altpos)]\n            \n        # check \n        no_var = []\n        if len(H)>1:\n            Hap[refpos] = H\n        else:\n            no_var.append((H, refpos))\n            #print(H, refpos)\n    return Hap, Ns, no_var\n\ndef get_haplotype_table(graph, Hap):\n    Info = {}\n    for refpos, Var_D in Hap.items():\n        for haplotype, items in Var_D.items():\n            for edge, altpos in items:\n                strainlist = graph.edges[edge]['strain']\n                for strain in strainlist:\n                    Info[refpos] = Info.get(refpos, {})\n                    Info[refpos][strain] = haplotype\n    df = pd.DataFrame(Info)\n    df = df.dropna(axis=1) # delete position with ambiguous sequences\n    return df\n\ndef sortcolumns(data):\n    columns = []\n    for i in data.columns:\n        if isinstance(i, int):\n            columns.append(i)\n        else:\n            columns.append(i[0])\n\n    index = numpy.argsort(columns)\n    colorder = [data.columns[i] for i in index]\n    data = data.loc[:,colorder]\n    return data","7dedf39f":"# Run\nfounder = getfounders(graph) # strain list\nk = 11 # kmer length\nrefstrain = 'MN908947'\nVar = find_allVar(graph,refstrain, founder, k)\nprint(len(Var))\nHap, Nedge, no_var = Var2Hap(Var, founder, k)\ndf = get_haplotype_table(graph, Hap)\n\ndata = sortcolumns(df)\ndata.to_csv('CovVariants.csv')\ndata.shape","ec4d526c":"from matplotlib.colors import LogNorm\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LogNorm\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\n\ndef Plot_Indels(df,roworder = df.index, shift = -0.5, excludesample = [], excludepos = []):\n    '''Import the pandas dataframe where columns correspond to ref intervals, rows correspond to strains, \n    elements denote haplotypes\n     roworder is the order for samples, to get a pretty plot, input the clustered order\n     \n    '''\n    excludesample = [unicode(item) for item in excludesample]\n    roworder = [item for item in roworder if item not in excludesample]\n    \n    df = df.drop(excludesample, axis=0)\n    df = df.drop(excludepos, axis=1)\n    \n    def filternonvariantsposition(df, roworder):\n        data = df.loc[roworder]\n        colorder = data.columns\n\n        # filter position without variants (according sample set)\n        delcolumns = []\n        for i in range(len(colorder)):\n            if len(set(data.iloc[:,i].values)) < 2:\n                delcolumns.append(colorder[i])\n        colorder = [item for item in colorder if item not in delcolumns]\n\n\n        data = data.loc[:,colorder]\n        return data\n\n    data = filternonvariantsposition(df, roworder)\n    #print \"data\", data.shape\n\n    def expandforplot(data):\n        columns = []\n        for i in data.columns:\n            if isinstance(i, int):\n                columns.append(i)\n            else:\n                columns.append(i[0])\n\n        index = numpy.argsort(columns)\n        colorder = [data.columns[i] for i in index]\n        data = data.loc[:,colorder]\n        p = data.values            \n\n\n        def expand_indels(index, p):\n            r,c = p.shape\n            #print p[:,index]\n            m = [len(seq) for seq in p[:,index]]\n            m = max(m)\n            a = numpy.ndarray([r,m], dtype=object)\n\n            for i, seq in enumerate(p[:,index]):\n                for j in range(len(seq)):\n                    a[i,m - len(seq) + j] = seq[j]\n            return a\n\n\n\n        def expandmatrix(p):\n            mydata = expand_indels(0, p)\n            labels_y = [0] + [\"\"] * (mydata.shape[1]-2) + [33]\n            r, c = p.shape\n            for index in range(1,c):\n                index_ = colorder[index]\n                if isinstance(index_, int):\n                    mydata = numpy.hstack((mydata, p[:,index].reshape(-1,1)))\n                    assert len(set(p[:,index]))>1\n                    labels_y += [index_]\n                else:\n                    a = expand_indels(index, p)\n                    mydata = numpy.hstack((mydata, a))\n                    labels_y += [index_[0]] + [\"\"] * (a.shape[1]-2) + [index_[1]]\n            return mydata, labels_y\n\n        mydata, labels_y = expandmatrix(p)\n\n        return mydata, labels_y\n\n    mydata, labels_y = expandforplot(data)\n    #print mydata.shape, labels_y\n    \n    \n    # exchange character to number for plot\n    character = set()\n    for i in mydata:\n        for j in i:\n            character.add(j)\n\n    colorD = {}\n    count = 1\n    for i in ['A', 'C', 'G', 'T', 'S', 'R', 'W', 'Y']:\n        colorD[i] = count\n        count += 1\n\n    r,c = mydata.shape\n    p = mydata\n    \n    \n    for i in range(r):\n        for j in range(c):\n            p[i,j] = colorD.get(p[i,j], 0)\n    p = p.astype(float)\n    \n    # plot\n    colorDict = {'A': 'blue', \"G\":'green', 'C':'red', 'T': 'yellow', \"None\": 'grey'}\n    cmap = mpl.colors.ListedColormap([\"white\",'blue', 'red', 'green', 'yellow', 'cyan', 'pink', 'grey','orange'], name = 'colors', N = None)\n    n = mpl.colors.Normalize(vmin=0,vmax=3)\n\n    fig = plt.figure(figsize=[20,40])\n    AX = plt.gca()\n    im = AX.matshow(p, cmap=cmap)\n    plt.xlim([shift-0.5,c])\n    plt.ylim([shift-0.5,r])\n    # set ticks\n    plt.yticks(range(0,r), roworder,rotation=0, fontsize = 5)\n    plt.xticks(range(0,c), labels_y, rotation = 80, fontsize = 5)\n\n#     #plot grid\n    r,c = p.shape\n    \n    ax = np.arange(shift,c,1)\n    for z in ax:\n        plt.plot((z,z),(shift ,r+shift),'k-', linewidth=0.2)\n\n    ax = np.arange(shift,r,1)\n    for z in ax:\n        plt.plot((shift,c+shift),(z,z),'k-', linewidth=0.2)\n\n    character = [\"\",'A', 'C', 'G', 'T', 'S', 'R', 'W', 'Y']\n    divider = make_axes_locatable(AX)\n    cax = divider.append_axes(\"right\", size=\"1%\", pad=0.05)\n\n    cbar = plt.colorbar(im, cax=cax)\n    cbar.ax.get_yaxis().set_ticks([])\n\n    # annotate characters\n    for j, lab in enumerate(character):\n        cbar.ax.text(.5, (j), lab, ha='center', va='center',fontsize = 10)\n    # cbar.ax.get_yaxis().labelpad = 15\n    # cbar.ax.set_ylabel('# of contacts', rotation=270)\n\n    # save\n    plt.show()\n    fig.savefig(\"Variantsdisply.png\")","a523bc0d":"roworder = data.index\nprint( roworder)\nPlot_Indels(data,roworder, excludepos = [])","fc9a4a98":"import seaborn as sns\n\ndef binary_data(df):\n    r,c = df.shape\n    values = df.values\n    binary = numpy.zeros([r,c])\n    for i in range(c):\n        hap = values[:,i]\n        h, c = numpy.unique(hap, return_counts= True)\n        D = {}\n        for j in range(len(h)):\n            if c[j] == max(c):\n                D[h[j]] = 0\n            else:\n                D[h[j]] = 1\n\n        for x in range(r):\n            binary[x,i] = D[values[x,i]]\n    return binary\n\nf, ax = plt.subplots(figsize=(11, 9))\nbinary = binary_data(data)\nbinary_table = pd.DataFrame(binary, columns= df.columns, index = df.index)\n# Generate a custom diverging colormap\n#cmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.set(font_scale=1.5)\n# Draw the heatmap with the mask and correct aspect ratio\nsns_plot = sns.heatmap(binary_table)\n# , cmap=cmap, vmax=1, center=0,\n#                        xticklabels = True, yticklabels=True, annot_kws={\"size\": 0.5},\n#                         square=False, linewidths=.5)\n# save to file\nfig = sns_plot.get_figure()\nfig.savefig(\"BinaryTable.png\")\n","8f8ae0e1":"# get info \ndef get_sample_info(table):\n    sampleinfo = pd.read_csv(table, header=0, index_col=None)\n    def isNaN(num):\n        return num != num\n\n    SampleD = {}\n    for i in range(len(sampleinfo)):\n        sample = sampleinfo['Accession'][i]\n        info = str(sampleinfo['Geo_Location'][i])\n        SampleD[sample] = info\n    return SampleD\n\ntable = '..\/input\/graphicalgenomeapi\/NCBI_seqinfo.csv'\nSampleD = get_sample_info(table)","2033abf6":"# Sample Visualization : MDS\n\nfrom sklearn.manifold import MDS\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom scipy.cluster import hierarchy\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import pdist, squareform\n\nbinary = binary_data(data)\n\nY= pdist(binary, 'cityblock')\nbinary_d = squareform(Y)\nlabel = [SampleD[item] for item in df.index]\nmds = MDS(n_components=2)\n#X_transformed = embedding.fit_transform(binary_d)\ntranspos = mds.fit(binary_d).embedding_\n\n\nfig = plt.figure(1)\nax = plt.axes([0., 0., 1., 1.])\n\n\nplt.scatter(transpos[:, 0], transpos[:, 1], color='red',lw=0, label='MDS')#\nfor i in range(len(transpos)):\n    plt.text(transpos[i, 0], transpos[i, 1], label[i], fontsize=10, alpha = 0.5, c = 'grey', rotation = -45)\nplt.legend(scatterpoints=1, loc='best', shadow=False)\nfig.savefig('MDScluster.png')","fc418f4d":"# Sample Subtypes and Identity Mutations\n\nimport pylab\nimport scipy.cluster.hierarchy as sch\n\nfig = pylab.figure(figsize=(10,12))\nbinary = binary_data(data)\nprint(binary.shape)\n# Compute and plot sample dendrogram.\n\ndist = pdist(binary, 'cityblock')\nax1 = fig.add_axes([0.09,0.1,0.2,0.6])\nY = sch.linkage(dist, method='average', metric = 'cityblock')\nZ1 = sch.dendrogram(Y, orientation='left')\nax1.set_xticks([])\nax1.set_yticks([])\n\n# Compute and plot position dendrogram.\ndist1 = pdist(binary.T, 'cityblock')\nax2 = fig.add_axes([0.3,0.71,0.6,0.2])\nY2 = sch.linkage(dist1, method='average', metric = 'cityblock')\nZ2 = sch.dendrogram(Y2)\nax2.set_xticks([])\nax2.set_yticks([])\n\n# Plot distance matrix\naxmatrix = fig.add_axes([0.3,0.1,0.6,0.6])\nidx1 = Z1['leaves']\nidx2 = Z2['leaves']\nb = binary[idx1,:]\nb = b[:,idx2]\n\nim = axmatrix.matshow(b, aspect='auto', origin='lower', cmap=pylab.cm.Blues)\naxmatrix.set_xticks([])\naxmatrix.set_yticks([])\n\nfig.savefig(\"VariantsCluster.png\")","16238b44":"from scipy.cluster.hierarchy import fcluster\nsamplenames = data.index\nfl = fcluster(Y,7,criterion='maxclust')\nprint(fl[idx1])\n#print( numpy.unique(fl, return_counts= True))\nfor c in [1,2,3]:\n    print()\n    cluster1 = numpy.where(fl == c)[0]\n    clusterinfo = [SampleD[samplenames[item]] for item in cluster1]\n    place, occurrence = numpy.unique(clusterinfo,return_counts = True)\n    clusterinfo = dict(zip(place, occurrence))\n    print(\"Cluster%s\" % str(c), \"%s samples\" % str(len(cluster1)), clusterinfo)\n    # mutations\n    mutationpos = set(numpy.where(binary[cluster1,:] == 1)[1])\n    \n    for pos in mutationpos:\n        if sum(binary[cluster1,pos]) > len(cluster1)\/4.0:\n            r = numpy.where(binary[cluster1,pos]==1)[0]\n            print(\"RefCoord\", data.columns[pos], \"WithinClusterFrequency:\", sum(binary[cluster1,pos])\/len(cluster1), \n                  \"Rare Allel:\", \"\".join(list(set(data.values[cluster1,pos][r]))))\n    ","cdbbf82f":"# samples characterized by mutation within S (23,403bp)\nindex = numpy.where(data[23403] == 'G')[0]\nS_data = data[data[23403] == 'G']\nS_b = binary[index,:]\nS_specific = S_data.index\n\nclusterinfo = [SampleD[item] for item in S_specific]\nplace, occurrence = numpy.unique(clusterinfo,return_counts = True)\nclusterinfo = dict(zip(place, occurrence))\nprint(len(S_specific), \"Spike-mutation(23,403) samples\", clusterinfo)\n\n# mutations\nmutationpos = set(numpy.where(S_b == 1)[1])\n#print(mutationpos)\nfor pos in mutationpos:\n    #print(sum(S_b[:,pos]), S_data.columns[pos])\n    if sum(S_b[:,pos]) > len(S_specific)\/10.0:\n        r = numpy.where(S_b[:,pos]==1)[0]\n        print(\"RefCoord\", data.columns[pos], \"WithinClusterFrequency:\", sum(S_b[:,pos])\/len(S_specific), \n              \"Rare Allel:\", \"\".join(list(set(S_data.values[:,pos][r]))))\n","bf59bef4":"# identity mutations\n\npositionlist = data.columns\nfor i in Z2['leaves'][:20]:\n    p = positionlist[i]\n    for s, e, n in interval:\n        if isinstance(p, int):\n            pos = int(p)\n        else:\n            pos = p[0]\n        if pos >= s and pos <= e:\n            break\n    hap, count = numpy.unique(data.iloc[:,i], return_counts=True)\n    print( p,n, dict(zip(hap, count)))","581d84cb":"## 4) Pangenome scaffold Construction\nGiven the anchor registration table, we consistently partitioned linear assemblies into disjoint homologous regions. We merged the identical sequences in between and construct a graphical genome","df2ff779":"## Validation","95e97d02":"## Code TEST","1ad22fff":"## 5) adding annotations to the Pangenome\n\n### 5.1 We overlaid genomic features (e.g. genes) to the graph entities.\n### 5.2 We performed pair-wise alignment between alternative and reference sequences in the graphical genome\n","cdd8ac06":"## 3). Dynamic update\nThe genome assemblies in NCBI is updating frequently. We updated the registration table by adding newlly released assemblies columns dynamically.","3d114990":"# Summary\n\n# [Graphical Genome Construction](#ggc)\n![Pipeline.png](attachment:Pipeline.png)\n\n## 1. Anchor Candidates Selection\nInput: Linear Reference Genome  \nOutput: Anchor Candidate Set with unique name  \n1) Divide Reference genome into non-overlapping 11-mers  \n2) Linearly go through the reference genome and record the occurrence of each 11-mer on both positive and negative sense.\n3) Select the unique 11-mer as anchor candidates. (unique 11-mer in both forward and reverse-complement order)   \n4) Each anchor candidate was named uniquely.\n\n## 2. Linear Genome Registration\n1) Linearly go through each assembly and record the occurrence position of each anchor candidates in a dictionary  \n2) Transit the dictionary into a anchor-candidate by assembly Table. If an anchor candidates occurs in more than 1 time in a linear genome, replace the position list by \"?\".\n\n## 3. Pangenome Construction\n1) Select assembly IDs that incorporate to the Graphical Genome, partition the linear assemblies by anchors given the mapping position of each anchor in Registration Table (Create at step 2).  \n2) Merge the sequences between anchor pairs into edges, record the assembly IDs in the \"strain\" field of each edge.  \n3) Annotate the Biological Features such as genes, cds, to the reference path (path attributes to \"MN908947\")  \n4) Pair-wise alignment between reference edge and alternative edges, alignment results were recorded in the \"variants\" filed of each edge, in form of SAMTOOL compatible cigar strings.  \n\n\n# [SARS-CoV-2 Graphical Genome Properties and Haplotype Distribution](#stats) \n\nThe present SARS-CoV-2 incorporated 490 complete linear assemblies collected in NCBI Virus platform. It has 493 anchors and 1814 edges. The total number of bases in the pangenome is 437,135 bp, which is 3% of the original linear sequence size (14,613,354 bp). Most of the gap region separated by adjacent anchors contains only 1 edge. For each 500 bp region, we compute the number of  distrinct haplotype sequences through the graph traversal. We find the highly variable regions and relative conserved regions in the SARS-CoV-2 genome (The first and last bin not shown in this figure).\n![AnchorDist_vs_edgeNum.png](attachment:AnchorDist_vs_edgeNum.png)\n![HaplotypeDistribution.png](attachment:HaplotypeDistribution.png)\n\nWe further mapped the gene intervals annotated on the reference assemblies to every alternative path. The statistics of each gene and its haplotype number were listed here.\n\n| genename | refstart | refend | genelength | startanchor | endanchor | HapNum | HapNum\/kb |\n|----------|----------|--------|------------|-------------|-----------|--------|-----------|\n| orf1ab   | 266      | 21555  | 21289      | SOURCE      | A01961    | 268    | 12.588661 |\n| S        | 21563    | 25384  | 3821       | A01961      | A02309    | 102    | 26.694583 |\n| ORF8     | 27894    | 28259  | 365        | A02501      | A02611    | 14     | 38.356164 |\n| N        | 28274    | 29533  | 1259       | A02501      | A02692    | 50     | 39.714059 |\n| ORF7a    | 27394    | 27759  | 365        | A02471      | A02611    | 15     | 41.09589  |\n| M        | 26523    | 27191  | 668        | A02400      | A02501    | 29     | 43.413174 |\n| E        | 26245    | 26472  | 227        | A02386      | A02471    | 10     | 44.052863 |\n| ORF3a    | 25393    | 26220  | 827        | A02309      | A02386    | 37     | 44.740024 |\n| ORF10    | 29558    | 29674  | 116        | A02683      | SINK      | 7      | 60.344828 |\n| ORF6     | 27202    | 27387  | 185        | A02471      | A02501    | 14     | 75.675676 |\n\n\n\n# [Sample Subtypes and Identity mutations](#var)\n\nNext, we constructed a variant table from the pair-wise alignment between alternative and reference edges with each gap. The variants that mapped to a continuous run on the reference coordinates were collapsed to a single trait. We transformed the variant table to a binary table, where 0 corresponds to the common allel or sequence of this trait, 1 corresponds to the rare allel, which may not be unique. We calculate the distance between each sample and perform the multi-dimensional scaling (MDS). The results suggests substructures existed in the whole population, where a subset of samples collected from USA-WA are distinct from other samples.\n\n![MDScluster%20%282%29.png](attachment:MDScluster%20%282%29.png)\n\nWe further performed the hierarchical clustering for both samples and genomic position of these variants. We found several identity mutations that can characterize those subtypes of SARS-CoV-2 samples.\n\n| RefCoord | Gene   | Common Allel | Frequency | Rare Allel | Frequency |\n|----------|--------|--------------|-----------|------------|-----------|\n| 8782     | orf1ab | C            | 52.86%    | T, Y       | 46.94%    |\n| 18060    | orf1ab | C            | 62.45%    | T          | 37.55%    |\n| 17747    | orf1ab | C            | 63.47%    | T          | 36.53%    |\n| 17858    | orf1ab | A            | 63.47%    | G          | 36.53%    |\n| 23403    | S      | A            | 72.65%    | G, R       | 27.14%    |\n\n![SARSCoV2%20Graphical%20Genome.png](attachment:SARSCoV2%20Graphical%20Genome.png)\n\n\n# Approach Pros and Cons  \n## Pros\n1. Comprehensive reference framework  \n2. Fast Linear genome integration and comparison pipeline \n\n## Cons  \n1. Have to update source data and visualize the graph structure by hand. \n2. Didn't make use of the partial sequences released in the resource.\n\n## Future work\n1. Graph-based genome browser  \n2. How these mutations determine the viral fitness? Is amino acid sequence changing? Is protein structure changing?\n3. Graph alignar","49df3774":"# SARS-CoV-2 Graphical Pangenome Properties<div id=\"stats\"><\/div>\n\nThe graphical structure provide a natural method representing the sequence similarity and diversity. In this section, We investigated the general properties of SARS-CoV-2 graphical pangenome including the pangenome size, anchor distribution, edge numbers within each gap, haplotype distribution at the whole genome level.\n\n## 1. Anchor Distribution at whole genome Level\n\n##  2. Statistics of Edge Numbers within each Gap \n\n## 3. Haplotype Distribution at whole genome Level\n\n## 4. Gene homologous sequence identification\n\n\n","3e5515e0":"## 2). Genome Registration by anchor candidates","f8520324":"# SARS-CoV-2 Graphical Genome\n\n\n![SARS-CoV-2-460.png](attachment:SARS-CoV-2-460.png)\n\n## COVID-19 Challenge Task: \n### What do we know about virus genetics?    \nThe graphical genome allows tracking SARS-CoV-2 genetic variants in time. The graph structure enables efficient genome comparison and visualization. We identify conserved regions within the SARS-CoV-2 genome as well as highly variable regions. We performed sample subtyping based on their genetic variants and identified the characterized mutations for each subtype.\n\n### Is there any evidence to suggest geographic based virus mutations?  \nYes, we found a subset of genomes collected in USA-WA that are dinstinct to other samples, characterized by the 4 mutations within gene orf1ab. A subset of samples (mainly from USA and Europe) are characterized by 1 mutation within Spike gene.\n\n\n## Introduction\nThe emergence of infectious disease, COVID-19, caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2, previously named as 2019-nCoV) becomes a major threat to global public health. The future evolution, adaption and spread of the SARS-CoV-2 warrant urgent investigation. SARS-CoV-2 belongs to the Coronaviridae family, possessing a single-strand, positive-sense RNA genome, ranging from 26-32 kilobases. Due to high error rates of RNA replications, RNA viruses exist as quasi-species, presenting as an ensemble of genotypes with large numbers of variants (Bull et al. 2005). Genetic diversity within a quasi-species has been proposed to contribute to pathogenesis. \n\nReference genome served as the basis for downstream SARS-CoV-2 genome characterization and comparison. The reference genome of SARS-CoV-2 refers to the whole genome sequence (29,903 bp) of the bronchoalveolar lavage fluid sample collected from a single patient, who worked at Huanan Seafood Market at Wuhan (Wu et al. 2020), designated as NC_045512.2 in NCBI Reference Sequence and GeneBank accession number MN908947. However, the SARS-CoV-2 reference genome, presented as a single linear sequence, fails to capture the genetic diversity within the quasi-species population. The concept of pangenome, defined as a collection of genomic sequences to be analyzed jointly as a reference, has been wildely suggested as the path forward to comprehensively and efficiently represent the genomic diversity within a population (Computational Pan-Genomics 2018). Graph is a commonly used representation for pangenomes. It provides a natural framework for representing shared sequences and variations within a population. A comprehensive reference pangenome is also compatible to downstream tools and facilitate computational efficiency.\n\nIn this study, we presented a dynamic pangenome construction pipeline for fast and incrementally integrating public linear assemblies of SARS-CoV-2 genome into a single graph-based representation as a reference pangenome. This method has been previously used to construct a Graphical Genome for a genetic referece mouse population, Collaborative Cross (CC) (Su et al. 2019). We introduced anchor nodes, defined as the ***conserved***, ***unique*** and ***monotonically ordered*** 11-mers in every linear assembly. Anchors consistently partitioned multiple linear genomes into disjoint homologous regions. Sequences between anchor nodes are collapsed into edges in the graph. Anchor sequences provides a valid graph-based coordinate system and edges place variants existed in the population within haplotype context. We analyzed the haplotype distribution at the whole genome scale and identified the conserved and highly vairant regions. We further mapped gene intervals annotated on the reference coordinates to every alternative paths to investigate the haplotype number of each gene. Variants on each genome assembly are identified and compared to find the shared and private patterns of these samples. We clustered samples based on these mutation patterns and found 3 potential subtypes existed in the population. The identity mutations to cluster these samples were obtained, which provide information for further exploration. The graphical genome provide an efficient framework for multi-genome comparison and a fast pipeline to compare newly added linear sequences to the pangenome population. A real-time genome browser could be established based on this pangenome construction pipeline to monitor the mutation cumulation within SARS-CoV-2 population and its evolution path.\n\n## Input\n\n1. Genome assemblies of SARS-CoV-2 from NCBI Virus platform:   \nhttps:\/\/www.ncbi.nlm.nih.gov\/labs\/virus\/vssi\/#\/virus?SeqType_s=Nucleotide&VirusLineage_ss=SARS-CoV-2,%20taxid:2697049  \nCurrent Version: 567 genome assemblies (490 complete assemblies (length > 10kb) with high qualities(ATGC > 90%) were incorporated to the grpahical genome)\n\n2. Genome Annotation files (.gff) of NC_045512.2 in NCBI Reference Sequence\n\n## Content\n\n1. Graphical Genome Construction Pipeline\n\n2. SARS-COV-2 Graphical Genome Properties and Gene Haplotype Analysis\n\n3. Variation Analysis, Sample Subtype Identification and Identity Mutations\n\n\n## Terminology\n1. ***Anchor nodes***: 11-mers that are conserved (appear in every linear assembly), unique (occur only once in both positive and negative sense), monotonically ordered (consistently ordered relative to other anchor nodes)    \n**Common Attributes**:   \n*** assemblyID *** : linear coordinates in every assembly\n\n2. ***Edges*** : shared or private sequences between adjacent anchors  \n**Common Attributes**:  \n***src***: the proximal node  \n***dst***: the distal node  \n***strain***: assemblies that share the sequence  \n***variants***: pair-wise alignment results between the reference edge (edge on assembly MN908947)  \n*** Parallel Edge*** referes to edges that share the same proximal and distal nodes\n\n3. *** Path ***: Sequences between any pair of anchors through graph traversal, defined by a distinct set of graph entites including anchors and edges\n\n4. ***SOURCE and SINK nodes***: SOURCE and SINK node denote the start and the end of an aseembly. They do not have any sequence content, only occur in the ***src*** and ***dst*** dictionary of edges.\n\n5. ***gap***: disjoint genomic region partitioned by adjacent pair of anchors\n\n","e5ec54d9":"# Variants Analysis, Sample Subtype Cluster and Identity Mutations <div id=\"var\"><\/div>\n\nIn this section, we invested the variants occured in the population. We mapped all the variants (substitution \"X\", deletion \"D\", insertion \"I\") occured on the alternative to the reference coordinates. We collapsed the continuous run of reference coordinates with variants to an interval. The corresponding variants could occur in any of the alternative edges. We further mapping these intervals back to each assembly to get the corresponding alleles or sequences. We recorded the mapping results to a variants table, where rows corresponds to an assembly, each column corresponds to a base position or an interval on the reference genome. \n\nWe clustered assemblies based on the variants table and found potential subtypes within the whole population. The mutations for classifying each subtype are founded.","9d08db3f":"# Graphical Genome Construction <div id=\"ggc\"><\/div>\n## 1). Identify anchor candidates in the reference assembly."}}