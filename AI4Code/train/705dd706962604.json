{"cell_type":{"42bf8679":"code","7214819d":"code","8f7f67a4":"code","c33a3987":"code","efed2609":"code","b4516758":"code","95957fbe":"code","db65d66e":"code","bbd2ccaf":"code","99bd2d37":"code","be4ab399":"code","fbdc5189":"code","a8888d69":"code","d09bba8f":"code","17b16fe1":"code","4295eb5c":"code","66578c8b":"code","58992f69":"code","87cf9e1d":"code","6ee58bc2":"code","9ace1893":"code","2427f884":"code","b3016f4c":"code","a91e1614":"code","b90f11b2":"code","892c8293":"code","e1a8c2a4":"code","854b2771":"code","3e66ff60":"code","f5f61dab":"code","d33fecc2":"code","225e15da":"code","90ca0114":"code","3edf009e":"code","5f90ed8f":"code","bb3b727e":"code","46db42bb":"code","9f04c8cb":"code","8052a8ae":"code","9cf1bd68":"code","3a901ca0":"code","91f30855":"code","e2a8b7af":"code","34b06912":"code","33cf0991":"code","623c4590":"code","36f53740":"markdown","8d412a3d":"markdown","dfafffbf":"markdown","ecc27712":"markdown","a3cceee6":"markdown","14258e9a":"markdown","f855124b":"markdown","b9d538a6":"markdown","6927ff4e":"markdown","41441de3":"markdown","11bc6be9":"markdown","d7400bfd":"markdown","8765d4ee":"markdown","1794ba8a":"markdown","a6f3d980":"markdown","cc3de312":"markdown"},"source":{"42bf8679":"############## DIABETIES DATASET #################","7214819d":"#####importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\n\nimport statsmodels.formula.api as smf\nfrom scipy.stats import shapiro,levene","8f7f67a4":"########## importing the data ########################\ndata=pd.read_csv('..\/input\/diabetes.csv')\n#checking the head of the data\ndata.head()","c33a3987":"#describing the data\ndata.describe()","efed2609":"#getting the information regarding the data\ndata.info()","b4516758":"#checking the shape of the data\ndata.shape","95957fbe":"#checking for null values in the data and performing EDA\ndata.isnull().sum()","db65d66e":"#getting the individua count of the outcome yes or no in the dataset\ndata['Outcome'].value_counts()","bbd2ccaf":"#dropping the outcome in the x and considering it in y as y is the target variable\nx=data.drop('Outcome',axis=1)\nx.head()\ny=data['Outcome']\ny.head()","99bd2d37":"#pairplot to check the distribution of the data\nsns.pairplot(data=data,hue='Outcome',diag_kind='kde')\nplt.show()","be4ab399":"#### plotting a HISTOGRAM on the data\ndata.hist(figsize=(10,8))\nplt.show()","fbdc5189":"#### BOXPLOT for checking the outliers\ndata.plot(kind= 'box' , subplots=True,layout=(3,3), sharex=False, sharey=False, figsize=(10,8))","a8888d69":"#plotting the outcome yes or no for the data\nsns.countplot(data['Outcome'])","d09bba8f":"#### checking the correlation in matrix for variables using HEATMAP\nimport seaborn as sns\nsns.heatmap(data.corr(), annot = True)","17b16fe1":"X=data.iloc[:,:-1]\nX.head()\nY=data.iloc[:,-1]\nY.head()","4295eb5c":"#### splitting X and y into training and testing sets \nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.28, random_state=100)","66578c8b":"# Scaling the data\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","58992f69":"#logistic regression model\nmodel=LogisticRegression()\nmodel.fit(X_train,y_train)\nypred=model.predict(X_test)\nypred","87cf9e1d":"# accuracy score\nfrom sklearn import metrics\naccuracy=metrics.accuracy_score(y_test,ypred)\nprint(accuracy)","6ee58bc2":"#confusion matrix\ncm=metrics.confusion_matrix(y_test,ypred)\nprint(cm)\nplt.imshow(cm, cmap='binary')","9ace1893":"#sensitivity and specificity check\ntpr=cm[1,1]\/cm[1,:].sum()\nprint(tpr*100)\ntnr=cm[0,0]\/cm[0,:].sum()\nprint(tnr*100)","2427f884":"#checking roc and auc curves\nfrom sklearn.metrics import roc_curve,auc\nfpr,tpr,_=roc_curve(y_test,ypred)\nroc_auc=auc(fpr,tpr)\nprint(roc_auc)\nplt.figure()\nplt.plot(fpr,tpr)\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.show()","b3016f4c":"#### importing the classifier and building the model\nfrom sklearn import tree\n\nmodel = tree.DecisionTreeClassifier()","a91e1614":"model.fit(X_train, y_train)","b90f11b2":"from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score,confusion_matrix\nprint(\"Accuracy:\",accuracy_score(y_test, ypred))\nprint(\"Recall:\",recall_score(y_test, ypred, average=\"weighted\"))\nprint(\"Precision\",precision_score(y_test, ypred, average=\"weighted\"))\nprint(\"F1 Score:\",f1_score(y_test, ypred, average=\"weighted\"))\nprint(metrics.classification_report(y_test,ypred))","892c8293":"\ndot_data = tree.export_graphviz(model,\n                                feature_names=X.columns,\n                                out_file='tree1.dot',\n                                filled=True,\n                                rounded=True)\n\n!dot -Tpng tree1.dot > tree1.png\n\nfrom IPython.display import Image\nImage(filename='tree1.png')","e1a8c2a4":"model1 = tree.DecisionTreeClassifier(criterion='entropy')\nmodel1.fit(X_train, y_train)\npreds= model1.predict(X_test)\nprint(accuracy_score(y_test, ypred))\nprint(recall_score(y_test, ypred, average=\"weighted\"))\nprint(precision_score(y_test, ypred, average=\"weighted\"))\nprint(f1_score(y_test, ypred, average=\"weighted\"))\nprint(metrics.classification_report(y_test,ypred))\n\ndot_data = tree.export_graphviz(model1,\n                                feature_names=X.columns,\n                                out_file='tree1.dot',\n                                filled=True,\n                                rounded=True)\n\n!dot -Tpng tree1.dot > tree1.png\n\nfrom IPython.display import Image\nImage(filename='tree1.png')\n","854b2771":"#importing the random forest classifier\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(criterion = 'gini',\n                               n_estimators = 25,\n                               random_state = 1)\n","3e66ff60":"# fitting the model and checking the accuracy score\nmodel.fit(X_train, y_train)\ny_predict = model.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nprint (accuracy_score(y_test, y_predict))","f5f61dab":"#checking for top 3 variables in the given dataset\nImportance = pd.DataFrame({'Importance':model.feature_importances_*100}, index=X_train.columns)\nImportance.sort_values('Importance', axis=0, ascending=True).plot(kind='barh', color='r', )\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","d33fecc2":"X = data[['Glucose', 'BMI', 'Age']]\ny = data['Outcome']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(criterion = 'gini',\n                               n_estimators = 25,\n                               random_state = 1)\n\nmodel.fit(X_train, y_train)\ny_predict = model.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\n\nprint (accuracy_score(y_test, y_predict))\n","225e15da":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 5)\n\nx,y = data.drop('Outcome', axis = 1), data['Outcome']\n# x,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\n\nknn.fit(x,y)\nprediction = knn.predict(x)\nprint('Prediction: {}'.format(prediction))","90ca0114":"# train test split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\n","3edf009e":"display (x_train[:5])\nprint ()\ndisplay (x_test[:5])","5f90ed8f":"knn = KNeighborsClassifier(n_neighbors = 3)\n\n# x,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\n\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\n#print('Prediction: {}'.format(prediction))\nprint('With KNN (K=3) accuracy is: ',knn.score(x_test,y_test)) # accuracy","bb3b727e":"knn = KNeighborsClassifier(n_neighbors = 10, p = 12)\nfrom sklearn.model_selection import cross_val_score\nCVscore = cross_val_score(knn.fit(x, y),\n                        x_train,y_train, cv = 10)\nprint(CVscore)\nprint(CVscore.mean())","46db42bb":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=12, p=6, metric='minkowski')\nknn.fit(x_train, y_train)\n\nprint('The accuracy of the knn classifier is {:.2f} out of 1 on training data'.format(knn.score(x_train, y_train)))\nprint('The accuracy of the knn classifier is {:.2f} out of 1 on test data'.format(knn.score(x_test, y_test)))\n","9f04c8cb":"from sklearn.neighbors import KNeighborsClassifier\nneig = np.arange(1, 20)\ntrain_score_knn=[]\ntest_score_knn=[]\nfor i in range(1,20):\n    knn = KNeighborsClassifier(n_neighbors=12, p=i, metric='minkowski')\n    knn.fit(x_train, y_train)\n    train_score_knn.append(knn.score(x_train, y_train))\n    test_score_knn.append(knn.score(x_test, y_test))\n#     print('For k value=',i)\n#     print('The accuracy of the knn classifier is {:.2f} out of 1 on training data'.format(knn.score(x_train, y_train)))\n#     print('The accuracy of the knn classifier is {:.2f} out of 1 on test data'.format(knn.score(x_test, y_test)))\n#     print()\n","8052a8ae":"# Plot\nplt.figure(figsize=[13,4])\nplt.plot(neig, train_score_knn, label = 'Training Score')\nplt.plot(neig, test_score_knn, label = 'Testing Score')\nplt.legend()\nplt.title('Value V\/s Score',fontsize=20)\nplt.xlabel('Pregnancies',fontsize=20)\nplt.ylabel('outcome',fontsize=20)\nplt.xticks(neig)\nplt.grid()\nplt.show()\nprint(\"Best score is {} with K = {}\".format(np.max(test_score_knn),1+test_score_knn.index(np.max(test_score_knn))))","9cf1bd68":"X=data.iloc[:,:-1].values\nX\nY=data.iloc[:,-1].values\nY[:5]\n# splitting X and y into training and testing sets \nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.28, random_state=100) ","3a901ca0":"#importing naive bayes classifier and building the model\nfrom sklearn.naive_bayes import GaussianNB \nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)","91f30855":"y_pred = classifier.predict(X_test)\ny_pred","e2a8b7af":"cm = confusion_matrix(y_test, y_pred)\ncm\n\nprint(f1_score(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\nlabel = [\"0\",\"1\"]\nsns.heatmap(cm, annot=True, xticklabels=label, yticklabels=label)","34b06912":"print(metrics.classification_report(y_test,y_pred))","33cf0991":"#importing k-fold\nfrom sklearn.model_selection import KFold","623c4590":"#performing k-fold cross validation\nkf=KFold(n_splits=5,shuffle=True,random_state=2)\nacc=[]\nau=[]\nfor train,test in kf.split(X,Y):\n    M=LogisticRegression()\n    M.fit(X_train,y_train)\n    y_pred=M.predict(X_test)\n    acc.append(metrics.accuracy_score(y_test,y_pred))\n    fpr,tpr,_=roc_curve(y_test,y_pred)\n    au.append(auc(fpr,tpr))\n    \nprint(\"Cross-validated AUC Mean Score:%.2f%%\" % np.mean(au))\nprint(\"Cross-validated AUC Var Score:%.5f%%\" % np.var(au,ddof=1))\n","36f53740":"## BUILDING A DECISION TREE CLASSIFIER","8d412a3d":"## BUILDING A K-NEAREST NEIGHBOURS CLASSIFIER","dfafffbf":"## EVALUATING THE MODEL USING METRICS","ecc27712":"## K-FOLD VAIDATION","a3cceee6":"## PLOTS","14258e9a":"## CHECKING FOR THE ACCURACY WITH THE METRICS","f855124b":"## ACCURACY SCORE USING ENTROPY","b9d538a6":"## ACCURACY SCORE USING GINI INDEX","6927ff4e":"## BUILDING A LOGISTIC REGRESSION  MODEL","41441de3":"## DATA SCALING","11bc6be9":"## BUILDING A NAIVE BAYES CLASSIFIER","d7400bfd":"## SPLITTING THE DATA","8765d4ee":"## BUILDING A RANDOM FOREST CLASSIFIER WITH TOP 3 VARIABLES","1794ba8a":"## DIABETIES DATASET","a6f3d980":"## EDA","cc3de312":"## BUILDING A RANDOM FOREST CLASSIFIER"}}