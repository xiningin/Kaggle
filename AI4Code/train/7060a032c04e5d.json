{"cell_type":{"ea37eebd":"code","97452e4f":"code","5688011b":"code","0e73bdc4":"code","8c2685e3":"code","019e6ad8":"code","8f420207":"code","7a63b29b":"code","6b6e86cb":"code","280f952e":"code","f071434c":"code","44da7e4a":"code","dcd08cab":"code","e0bc1dff":"code","882cc81a":"code","ffd037dd":"code","c3234b24":"markdown","af8bfe2c":"markdown"},"source":{"ea37eebd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n# Import library\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","97452e4f":"# import data\ndata = pd.read_csv('..\/input\/column_2C_weka.csv')","5688011b":"# data info\ndata.info()","0e73bdc4":"# data head\ndata.head()","8c2685e3":"# before cleanin data and virtualize\nabnormal = data[data['class'] == 'Abnormal']\nnormal = data[data['class'] == 'Normal']\nplt.scatter(abnormal.pelvic_incidence,abnormal.pelvic_radius,color='red',label='Abnormal',alpha=0.4)\nplt.scatter(normal.pelvic_incidence,normal.pelvic_radius,color='green',label='Normal',alpha=0.4)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('pelvic_radius')\nplt.legend()\nplt.show()","019e6ad8":"# classification and normalization\ndata['class'] = [0 if x == 'Abnormal' else 1 for x in data['class']] # clasifications\ny = data['class']\nx_data = data.drop(['class'],axis=1)\nx = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data)) # normalizations","8f420207":"# data train and test splite\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.4, random_state = 1)\n#print(x_train, x_test, y_train, y_test )","7a63b29b":"# logistic regression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\nprint(\"test accuracy {}\".format(lr.score(x_test,y_test)))","6b6e86cb":"# knn\nknn = KNeighborsClassifier(n_neighbors=4)\nknn.fit(x_train,y_train)\nprint ('k: {} values score: {}'.format(4,knn.score(x_test, y_test)))","280f952e":"# Find the best k value and virtualization\nscore_list = []\n\nfor x in range(1,15):\n    knn_n = KNeighborsClassifier(n_neighbors=x)\n    knn_n.fit(x_train,y_train)\n    score_list.append(knn_n.score(x_test, y_test))\n    print ('k: {} values score: {}'.format(x,knn_n.score(x_test, y_test)))\n\nplt.plot(range(1,15), score_list)\nplt.xlabel('k values')\nplt.ylabel('accurasi')\nplt.show()","f071434c":"# SVM\nsvm = SVC(random_state=1)\nsvm.fit(x_train, y_train)\nprint ('accuracy of svmalgo: ',svm.score(x_test, y_test))","44da7e4a":"# Naive Bayes\ngnb = GaussianNB()\ngnb.fit(x_train, y_train)\nprint ('accuracy of naive bayes: ',gnb.score(x_test, y_test))","dcd08cab":"# Decision Classification\ndtc = DecisionTreeClassifier()\ndtc.fit(x_train, y_train)\nprint ('accuracy of naive decision tree: ',dtc.score(x_test, y_test))","e0bc1dff":"# Random Forest Classification\nrf = RandomForestClassifier(n_estimators=100,random_state=1)\nrf.fit(x_train,y_train)\nprint ('Random forest algo result: ',rf.score(x_test,y_test))","882cc81a":"# Confusion matrix\ny_predict = rf.predict(x_test)\ny_true = y_test\ncm = confusion_matrix(y_true,y_predict)\nprint ('confusion matrix: ',cm)","ffd037dd":"# Confusion matrix Virtualize\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor='red',fmt='.0f',ax=ax)\nplt.xlabel('y_pred')\nplt.ylabel('y_true')\nplt.show()","c3234b24":"# Content\n* \u0130mport data\n* Cleanin data\n* Data Classification and Normalization\n* Data train and test splite\n* Logistic reggresion fit, predict and score\n* KNN fit, score and print accuracy\n* Find the best k value and virtualization\n* SVM fit, score and print accuracy\n* Naive Bayes fit, score and print accuracy\n* Decision Classification fit, score and print accuracy\n* Random Forest Classification fit, score and print accuracy\n* Confusion Matrix fit, score and print accuracy\n* Confusion Matrix Virtualize\n* CONCLUSION","af8bfe2c":"# CONCLUSION\nI have examined this data with a few machine learning algorithms and got the highest score with random forest classification(0.838%) algorithm. test_sise increased from 02 to 0.4 random forest 87%\n"}}