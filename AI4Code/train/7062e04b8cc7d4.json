{"cell_type":{"168fe951":"code","ad7cf167":"code","9284dab6":"code","f273c43d":"code","60634a3e":"code","d5ab7e1f":"code","e6f340dc":"code","46dd997f":"code","ec55d81e":"code","3129270e":"code","dd753ad4":"code","bcd36bcf":"code","a31503bb":"code","b3f8107e":"code","c694739f":"code","59485e49":"code","c1e26cfb":"code","3e19ebcf":"code","c1478aa6":"code","2b7efb41":"markdown","dafff658":"markdown","eb08ab85":"markdown","cd0f26c0":"markdown","b4787723":"markdown","a14371ff":"markdown","add7e53e":"markdown","12b360b0":"markdown","7da4f804":"markdown","f092ccf3":"markdown","34e0042d":"markdown","b54d4108":"markdown","d092d644":"markdown","8361089b":"markdown","503975ef":"markdown","c5fa66e2":"markdown","115d0b29":"markdown"},"source":{"168fe951":"#Data handling libraries\nimport numpy as np\nimport pandas as pd\n\n#Data Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n\n#Data Preprocessing libraries\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n#models libraries\nimport statsmodels.api as sm\nfrom sklearn.linear_model import Ridge\nfrom sklearn.kernel_ridge import KernelRidge\nfrom lightgbm import LGBMRegressor\n\n#warning library\nimport warnings\nwarnings.filterwarnings('ignore')","ad7cf167":"df = pd.read_csv(r\"..\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv\")\n\n# Briefly read the data\ndf.info()","9284dab6":"#take a look of the actual data\ndf.head()","f273c43d":"#look inside the features\ndf.describe()","60634a3e":"#Drop the serial no.\ndf = df.drop(df.columns[0], axis=1)","d5ab7e1f":"fig = px.scatter_matrix(df,\n                        height=800, width=800, \n                       color='Chance of Admit ',\n                       )\n\nfig.update_layout(font_family='Helvetica', font_size=10,\n                 title=dict(text='Scatter Plot Matrix', x=0.5, y = 0.98, font=dict(size=30))\n                 )\n\nfig.show()","e6f340dc":"heatmp = sns.heatmap(np.corrcoef(df.values.T), annot=True, cbar=True, square=True, annot_kws={'size': 10}, fmt='.2f', yticklabels=df.columns, xticklabels=df.columns)\nplt.show()","46dd997f":"#split first to avoid data leakage\n#As the task mentioned, the last 100 rows of observations will be taken as validation set\ntrain_X = df[0:400]\nval_X = df[400:]","ec55d81e":"#Take a overview of the outliners and High leverage observations\n#with the use of statsmodels\nX_OLS = train_X.drop(columns=\"Chance of Admit \")\nX_OLS = sm.add_constant(X_OLS)\nmodel_OLS = sm.OLS(train_X[\"Chance of Admit \"], X_OLS).fit()\nfig, ax = plt.subplots(figsize=(12,8))\nfig = sm.graphics.influence_plot(model_OLS, ax=ax)\nplt.show()","3129270e":"data_inf = model_OLS.get_influence().summary_frame()\ndata_inf","dd753ad4":"#Drop the outliers\nstu_res = data_inf[abs(data_inf.student_resid)>3]\ntrain_X_Out = train_X.drop(stu_res.index)","bcd36bcf":"#Drop the high leverage points\nh_lev = data_inf[abs(data_inf.hat_diag)>0.05]\ntrain_X_OH = train_X_Out.drop(i for i in (h_lev.index & train_X_Out.index))","a31503bb":"train_X_OH.info()","b3f8107e":"# plot the influence plot again to see the difference\nX_OH_OLS = train_X_OH.drop(columns=\"Chance of Admit \")\nX_OH_OLS = sm.add_constant(X_OH_OLS)\nmodel_OH_OLS = sm.OLS(train_X_OH[\"Chance of Admit \"], X_OH_OLS).fit()\nfig, ax = plt.subplots(figsize=(12,8))\nfig = sm.graphics.influence_plot(model_OH_OLS, ax=ax)\nplt.show()","c694739f":"#Define the target variable\ntrain_y = train_X_OH.pop('Chance of Admit ')\nval_y = val_X.pop('Chance of Admit ')\n#Standardize the data\ntrain_X_std = StandardScaler().fit_transform(train_X_OH)\nval_X_std = StandardScaler().fit_transform(val_X)","59485e49":"model_pipeline = Pipeline([\n    ('clf', KernelRidge())\n])\n\nparameters = [\n    {\n        'clf':(Ridge(),),\n        'clf__alpha':(0.01, 0.025, 0.05, 0.075, 0.1),\n        'clf__max_iter':(10, 15, 25, 50),\n    },{\n        'clf':(KernelRidge(),),\n        'clf__alpha':(0.001, 0.01, 0.025, 0.1),\n        'clf__degree':(2,3,4,5,6),\n        'clf__kernel':('linear','polynomial'),\n        'clf__coef0':(1, 1.5, 2, 2.5),\n    },{\n        'clf':(LGBMRegressor(),),\n        'clf__num_leaves':(20, 30, 40),\n        'clf__max_depth':(3, 5, 7),\n        'clf__learning_rate':(0.01, 0.05, 0.1),\n        'clf__n_estimators':(10, 20, 50, 100),\n        'clf__min_child_weight':(0.001, 0.01, 0.1),\n        'clf__reg_lambda':(0.001, 0.01, 0.1),\n    }\n]","c1e26cfb":"#use grid search\ngrid_search = GridSearchCV(model_pipeline, parameters, \n                           scoring='neg_root_mean_squared_error',\n                          cv=4)\ngrid_search.fit(train_X_std, train_y)\nprint('The best score in the grid search is', grid_search.best_score_)\nprint('The best parameters in the grid search is \\n', grid_search.best_params_)","3e19ebcf":"#Take a look of the residuals\npred = grid_search.predict(val_X_std)\nsummary = pd.DataFrame(data={'Observed Value':val_y, \n                             'Predicted Value':pred, \n                             'Absolute Residual':abs(val_y-pred)})\nsummary.head(10)","c1478aa6":"validation_score = -grid_search.score(val_X_std, val_y)\nprint(\"The RMSE of the validation set is\", validation_score)","2b7efb41":"## Kernel Ridge Regression\nKernel ridge regression combines ridge regression with the kernel trick.","dafff658":"From the plot, we can see that almost  all the continuous features have positive relationships with the target variable.\n\n## Heatmap\nA heatmap is visualised below. We can get the correlation coefficient of the variables with `np.corrcoef()`.","eb08ab85":"# Detect Outliers and High Leverage Observations\nI would like to drop outliers or observations with high leverage if there are any in the dataset.\n\nOutliers are target variable points that are far from the value predicted by the model. Outliers can affect the amount of error estimate, e.g. RSE in regression model, and hence it may lead to a wrong interpretation of the fit.\n\nTo detect outliers, we can look at the studentized residuals. Studentized residuals equals each residual divided by the estimated standard error, which is better to classify outliers than ordinary residuals.","cd0f26c0":"# Exploratory Data Analysis\nAfter that, it's time to obtain a bigger picture of the dataset. What kind of relationships do the variables have? \n\n\n## Scatter Plot Matrix\nWe can understand the correlations between the features and how the features are related to the target variable(Chance of Admit).\n\nWith plotly, an interactive plot can be shown, including hover effect that allows us to know the data better.","b4787723":"# Import data\nThen, the data is imported. Take an overview of the data.","a14371ff":"### High Leverage\nA high leverage point indicates it has an unusual value in the variables. In regression, it will highly affect how the regression line is drawn. Observations that are high leverage will be dropped.\n<br>Leverage statistic lies between [1\/n, 1] and the average leverage will be $(p+1)\/n$, where p = number of feature and n = number of observation.\n<br>In this case, the average leverage score = (7+1)\/400 = 0.02. The observations that have a leverage score far greater than 0.02 will be considered as high leverage points.\n<br>In this case, the high leverage observations are predefined if the scores exceed 0.05.","add7e53e":"## Light GBM\n\uff02Light Gradient Boosting Machine is a gradient boosting framework that uses tree based learning algorithms.\uff02 \uff08from:\u3000https:\/\/lightgbm.readthedocs.io\/en\/latest\/\u3000\uff09\n\nIt adopts a leaf-wise tree growth therefore the accuracy is optimized with a lower loss. Go check its documentation for more details.","12b360b0":"It looks better than before. Time to standardize the data.","7da4f804":"As the task requests, RMSE will be used as evaluation metric.\n\nRMSE can be shown as$\\sqrt{{\\sum^N_{i=1}(x_i - \\hat x_i)^2\\over N}}$. It is the standard deviation of the residuals.","f092ccf3":"## Ridge Regression\nRidge Regression is one of the shrinkage methods.\n\n$\\sum^n_{i=1}\\left(y_i-\\beta_0-\\sum^p_{j=1}\\beta_jx_{ij}\\right)^2 + \\lambda\\sum^p_{j=1}\\beta^2_j$<br>$=RSS+\\lambda\\sum^p_{j=1}\\beta^2_j$\n\nwhere $\\lambda\\sum^p_{j=1}\\beta^2_j$ is the ridge penalty, and $\\lambda$ is a tuning parameter (alpha in the `Ridge()`).\n\nBy adding a shrinkage penalty in the least squares method, the chance of getting overfitted can be reduced with $\\lambda > 0$. When we lift the $\\lambda$, the model flexibility can be lowered and hence variance will be deducted with a cost of increase in bias.\n","34e0042d":"# Modelling\n## Grid Search\n#### Find the best model and its hyperparameters\nSince the dataset is not big, time and cost will be affordable to perform Grid Search. I want to find the best model and its hyperparameters from Ridge Regression, Kernel Ridge Regression and Light GBM. ","b54d4108":"# Reference:\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning.\n\nMohan S Acharya, Asfia Armaan, Aneeta S Antony : A Comparison of Regression Models for Prediction of Graduate Admissions, IEEE International Conference on Computational Intelligence in Data Science 2019","d092d644":"A total of 10 observations have been dropped.","8361089b":"# Import Libraries\nFirst of all, let's import all the libraries I would use in this kernel.","503975ef":"# Evaluation","c5fa66e2":"# Introduction\n\nhttps:\/\/www.kaggle.com\/mohansacharya\/graduate-admissions\n\nIn this kernel, I would like to demonstrate my understandings of outliers and high leverage observations, and the models I applied i.e. Ridge Regression, Kernel Ridge Regression, and Light GBM.\n<br>Moreover, I would also like to perform the skills of data visualisation, pipeline and grid search.\n\nMost of my understandings come from a textbook: An Introduction to Statistical Learning, from https:\/\/www.statlearning.com\/\n\nFeel free to comment and let us learn from each other!\n\nThanks to the owner of this great dataset, Mr Acharya. :)","115d0b29":"### Studentized Residual\nThe observations that exceed 3 absolute value in studentized residual will be taken as outliers. These observations will be dropped."}}