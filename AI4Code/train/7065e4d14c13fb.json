{"cell_type":{"caf3cb42":"code","1fde7020":"code","fe0eed87":"code","028db4d1":"code","94cee658":"code","8cba8253":"code","db1db200":"code","e298bee6":"code","966787cb":"code","169c7f22":"code","1baa3819":"code","a69d0021":"code","b62593f5":"code","edf6487f":"code","81a1d221":"code","4a6132b7":"code","91d2def0":"code","56103b2c":"code","6d33a03a":"code","119f009b":"code","6d5613d7":"code","402d528c":"code","3f1e41bc":"code","87e63ec8":"code","4f94bf46":"code","676eeb14":"code","9fcf2665":"code","1e78eecb":"code","d363656e":"code","218a85b9":"code","74757b63":"code","d379918b":"code","f49949e4":"code","4da03191":"code","aba6d58a":"code","50820a7e":"code","a2fb47e5":"code","ff87f43f":"code","e26e0123":"code","18bb2a54":"code","715fdd28":"code","dcca2ce6":"code","67936a51":"code","473a22b4":"code","654f9313":"code","1d76e44f":"code","e7f8e18f":"code","792c49b4":"code","ebdda289":"code","e017fbc7":"markdown","11cb201b":"markdown","e68b6bb0":"markdown","ff0d15af":"markdown"},"source":{"caf3cb42":"#!unzip '\/content\/kc_house_data.csv.zip'","1fde7020":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.stats import shapiro,kstest\nfrom scipy.stats import skew\nfrom scipy.stats import probplot\nfrom scipy.stats import boxcox_normmax\nfrom scipy.special import boxcox1p","fe0eed87":"data = pd.read_csv('..\/input\/housesalesprediction\/kc_house_data.csv')\ndata.head()","028db4d1":"data.info(memory_usage='deep')","94cee658":"print(data['date'].nunique())\ndata.date","8cba8253":"label = LabelEncoder()\ndata['date'] = label.fit_transform(data['date'])\nprint(data['date'].dtype)\nprint(data['date'].max())","db1db200":"data.drop('id',axis=1,inplace=True)\ndata['long'] = abs(data['long'])\ndata.head()","e298bee6":"for col in data.columns:\n  print(col+\" \" + str(data[col].nunique()))","966787cb":"corr = data.corr()\nplt.figure(figsize=(17,15))\nsns.heatmap(corr,annot=True)","169c7f22":"_,ax = plt.subplots(nrows=len(data.columns),ncols=1,figsize=(16,16))\n\nplt.subplots_adjust(top=6,hspace=0.5)\nfor i,col in enumerate(data.columns):\n  sns.histplot(data[col],ax=ax[i])\n  ax[i].set_xlabel(col,fontsize=20)","1baa3819":"def testing_on_normal_distrubution(df):\n  for col in df.columns:\n    print('P-value Shapiro Test for column {} : {}'.format(col,shapiro(df[col])[1]))\n    print('P-value Test Kolmogorova-Smirnova for column {} : {}\\n'.format(col,kstest(df[col],'norm')[1]))\n\ntesting_on_normal_distrubution(data)","a69d0021":"def determine_skewness(df):\n  for col in df.columns:\n    print('Skewness = {} for column {}\\n'.format(skew(df[col]),col))\ndetermine_skewness(data)    ","b62593f5":"sns.set_style('darkgrid')\ndef plotting_4_chart(df,col):\n  fig1 = plt.figure(figsize=(17,17))\n  grid = GridSpec(nrows=4,ncols=3,figure=fig1)\n\n  plt.subplots_adjust(hspace=0.5)\n\n  fig1_ax1 = fig1.add_subplot(grid[0,:2])\n  sns.histplot(df[col],ax=fig1_ax1)\n  fig1_ax1.axvline(df[col].mean(),color='red',label='Mean')\n  fig1_ax1.axvline(df[col].median(),color='orange',label='Median')\n  fig1_ax1.legend()\n\n  fig1_ax2 = fig1.add_subplot(grid[1,:2])\n  probplot(data[col],plot=fig1_ax2)\n\n  fig1_ax3 = fig1.add_subplot(grid[2:,:2])\n  sns.residplot(x=col,y='price',data=df,ax=fig1_ax3)\n\n  fig1_ax4 = fig1.add_subplot(grid[:,2])\n  sns.boxplot(y=col,data=df,ax=fig1_ax4)\n\n  fig2 = plt.figure(figsize=(15,15))\n  grid2 = GridSpec(nrows=1,ncols=1,figure=fig2)\n\n  fig2_ax = fig2.add_subplot(grid2[0,0])\n  sns.histplot(df['price'],ax=fig2_ax)\n\nplotting_4_chart(data,'sqft_living15')","edf6487f":"data.nunique()","81a1d221":"from scipy.stats import f_oneway\n\ncut1 = data.loc[data['condition']==1,'price']\ncut2 = data.loc[data['condition']==2,'price']\ncut3 = data.loc[data['condition']==3,'price']\ncut4 = data.loc[data['condition']==4,'price']\ncut5 = data.loc[data['condition']==5,'price']\n\nf_oneway(cut1,cut2,cut3,cut4,cut5)","4a6132b7":"def anova_one_way(col,df=data):\n  from scipy.stats import f_oneway\n  len = df[col].nunique()\n  all_means = [] \n\n  for i in range(len):\n    element = df[col].unique()[i]\n    new_col=df.loc[df[col]==element,'price']\n    all_means.append(new_col)\n\n  return f_oneway(*[x for x in all_means])[1]\n\nanova_one_way('condition')     ","91d2def0":"def selected_anova(cols,df=data):\n  assert type(cols) == list\n  for col in cols:\n    p_value = anova_one_way(col=col,df=df)\n    print('P_value for column \"{}\" equal {}'.format(col,p_value))\n\nselected_anova(df=data,cols=['condition','floors','waterfront','view','bedrooms','bathrooms','grade','lat'])","56103b2c":"from sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nprint(XGBClassifier)\nprint(RandomForestClassifier)\nrf = RandomForestClassifier()\nprint(rf.__class__.__name__)","6d33a03a":"def selected_with_model(model,type_model='ensemble',data=data):\n  model.fit(data.drop('price',axis=1),data['price'])\n  if type_model=='ensemble':\n    importance = model.feature_importances_\n    cols = list(data.columns)\n    cols.remove('price')\n    dataframe = pd.DataFrame({'Cols':cols,'Importance':importance}).sort_values(ascending=False,by='Importance',ignore_index=True)\n  elif type_model=='linear':\n    importance = abs(model.coef_)\n    cols = list(data.columns)\n    cols.remove('price')\n    dataframe = pd.DataFrame({'Cols':cols,'Importance':importance},index=cols).sort_values(ascending=False,by='Importance',ignore_index=True)\n  \n  plt.figure(figsize=(20,10))\n  sns.barplot(x='Cols',y='Importance',data=dataframe)    \n\n  return dataframe","119f009b":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\n\nlr = LinearRegression()\nselected_with_model(lr,type_model='linear')","6d5613d7":"rf = RandomForestRegressor()\nselected_with_model(model=rf,type_model='ensemble')","402d528c":"from lightgbm import LGBMRegressor\nlgb = LGBMRegressor()\nselected_with_model(model=lgb,type_model='ensemble')","3f1e41bc":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(data.drop('price',axis=1),data['price'],test_size=0.33,shuffle=True,random_state=25)","87e63ec8":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\n\nlr = LinearRegression()\n\ndef rmse(prediction):\n  return np.sqrt(mean_squared_error(prediction,y_test))\n\ndef progressive(model,X,y,X_test,flag = False):\n  import time\n\n  start = time.time()\n  model.fit(X,y)\n  pred = model.predict(X_test)\n  if flag:\n    pred= np.expm1(pred)\n  print('Time: ' + str(time.time()-start))  \n  print('MAE: '+ str(mean_absolute_error(y_test,pred)))\n  print('MSE:' + str(rmse(pred)))\n\nprogressive(lr,X_train,y_train,X_test=X_test)  ","4f94bf46":"y_train_log = np.log1p(y_train)\nlr1 = LinearRegression()\n\nprogressive(lr1,X_train,y_train_log,flag=True,X_test=X_test)","676eeb14":"X_train['long']","9fcf2665":"def deter_num_cols(df=X_train):\n  num = []\n  for col in X_train.columns:\n    if df[col].nunique() > 100 and col!='long':\n      num.append(col)\n  return num      ","1e78eecb":"def boxcox(dfs):\n  assert type(dfs) == list\n\n  for df in dfs:\n    for col in deter_num_cols():\n      df[col] = boxcox1p(df[col],0.15)\n\nboxcox([X_train,X_test])      ","d363656e":"print(X_train.isnull().sum())\nprint(y_train.isnull().sum())\nprint(y_train_log.isnull().sum())\nprint(X_train.isin([np.inf,-np.inf]).sum())","218a85b9":"y_train_log = np.log1p(y_train)\nlr2 = LinearRegression()\n\nprogressive(lr2,X_train,y_train_log,flag=True,X_test=X_test)","74757b63":"def detect_outliers(df,n):\n  from collections import Counter\n  outliers = []\n  for col in deter_num_cols():\n    Q1 = np.percentile(df[col],25)\n    Q3 = np.percentile(df[col],75)\n    IQR = Q3-Q1\n    step = IQR*1.5\n\n    outlier_index = df.loc[(df[col]<Q1 - step) | (df[col]>Q3 + step),col].index \n    outliers.extend(outlier_index)\n\n  dictionary = Counter(outliers)\n\n  multiple_outliers = [k for k,v in dictionary.items() if v>n]   \n  return  multiple_outliers","d379918b":"print(len(detect_outliers(X_train,2)))\nX_train_without_outliers = X_train.drop(detect_outliers(X_train,2),axis=0)\ny_train_log_out = y_train_log.drop(detect_outliers(X_train,2),axis=0)\n\nlr5 = LinearRegression()\nprogressive(lr5,X_train_without_outliers,y_train_log_out,flag=True,X_test=X_test)","f49949e4":"from sklearn.preprocessing import MinMaxScaler,StandardScaler\nscaler_min_max =  MinMaxScaler()\nstandard_scaler = StandardScaler()\n\nX_train_min_max = scaler_min_max.fit_transform(X_train)\nX_test_min_max = scaler_min_max.transform(X_test)\n\nX_train_standard = standard_scaler.fit_transform(X_train)\nX_test_standard = standard_scaler.transform(X_test)","4da03191":"lr3 = LinearRegression()\nlr4 = LinearRegression()\n\nprint(progressive(lr3,X_train_min_max,y_train_log,flag=True,X_test=X_test_min_max))\nprint()\nprint(progressive(lr4,X_train_standard,y_train_log,flag=True,X_test=X_test_standard))","aba6d58a":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor()\n\nprogressive(rf,X_train,y_train,flag=False,X_test=X_test)","50820a7e":"from sklearn.tree import DecisionTreeRegressor\ntree = DecisionTreeRegressor()\n\nprogressive(tree,X_train,y_train_log,flag=True,X_test=X_test)","a2fb47e5":"from sklearn.neighbors import KNeighborsRegressor\nknn = KNeighborsRegressor(n_neighbors=6)\n\nprogressive(knn,X_train,y_train,flag=False,X_test=X_test)","ff87f43f":"from xgboost import XGBRegressor\n\nxgb = XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n\nprogressive(xgb,X_train,y_train,flag=False,X_test=X_test)","e26e0123":"from sklearn.ensemble import GradientBoostingRegressor\n\ngb = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n\nprogressive(gb,X_train,y_train,flag=False,X_test=X_test)","18bb2a54":"from lightgbm import LGBMRegressor\n\nlgb = LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\nprogressive(lgb,X_train,y_train,flag=False,X_test=X_test)","715fdd28":"from sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.preprocessing import RobustScaler\n\nENet = make_pipeline(RobustScaler(),ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n\nprogressive(ENet,X_train,y_train_log,flag=True,X_test=X_test)","dcca2ce6":"from sklearn.ensemble import StackingRegressor\n\nstacked1 = StackingRegressor(estimators=[('xgb',xgb),('lgb',lgb)],final_estimator=gb)\n\nprogressive(stacked1,X_train,y_train,flag=False,X_test=X_test)","67936a51":"stacked2 = StackingRegressor(estimators=[('xgb',xgb),('gb',gb)],final_estimator=gb)\n\nprogressive(stacked2,X_train,y_train,flag=False,X_test=X_test)","473a22b4":"from sklearn.ensemble import VotingRegressor\n\nvote = VotingRegressor(estimators=[('xgb',xgb),('lgb',lgb),('gb',gb)])\n\nprogressive(vote,X_train,y_train,flag=False,X_test=X_test)\n","654f9313":"from sklearn.base import TransformerMixin,BaseEstimator,RegressorMixin,clone\nfrom sklearn.model_selection import KFold","1d76e44f":"class AveragingRegressor(TransformerMixin,BaseEstimator,RegressorMixin):\n  def __init__(self,models):\n    self.models = models\n  \n  def fit(self,X,y):\n    self.models_ = [clone(model) for model in self.models]\n\n    for model in self.models_:\n      model.fit(X,y)\n\n    #return self      \n\n  def predict(self,X):\n    predictions = np.column_stack([model.predict(X) for model in self.models_])      \n    return np.mean(predictions,axis=1)","e7f8e18f":"averaged = AveragingRegressor(models=(gb,xgb,lgb))\nprogressive(averaged,X_train,y_train,X_test,flag=False)","792c49b4":"class StackingAveragingRegressor(TransformerMixin,BaseEstimator,RegressorMixin):\n  def __init__(self,base_models,meta_model,n_folds=5):\n    self.base_models = base_models\n    self.meta_model  = meta_model\n    self.n_folds = n_folds\n\n  def fit(self,X,y):\n    self.base_models_ = [list() for model in self.base_models]\n    self.meta_model_  = clone(self.meta_model)\n    kfold = KFold(n_splits=self.n_folds,shuffle=True,random_state=42)\n\n    out_of_fold_predictions = np.zeros((X.shape[0],len(self.base_models)))\n    for i,model in enumerate(self.base_models):\n      for train,validation in kfold.split(X,y):\n        instance = clone(model)\n        instance.fit(X[train],y[train])\n        self.base_models_[i].append(instance)\n        y_pred = instance.predict(X[validation])\n        out_of_fold_predictions[validation,i] = y_pred\n\n    self.meta_model_.fit(out_of_fold_predictions,y)\n    #return self#\u0425\u043e\u0442\u044f \u043d\u0435\u0442 \u043d\u0435 \u043d\u0430\u0441\u0442\u043e\u043b\u044c\u043a\u043e \u0432\u0430\u0436\u043d\u043e\u0436)\n\n  def predict(self,X):\n    predictions = np.column_stack([np.column_stack([model.predict(X) for model in base_models]).mean(axis=1) for base_models in self.base_models_])\n    return self.meta_model_.predict(predictions)\n","ebdda289":"stacked = StackingAveragingRegressor(base_models=(xgb,lgb),meta_model=gb)\nprogressive(stacked,X_train.values,y_train.values,X_test.values,flag=False)","e017fbc7":"# Research and Preprocessing","11cb201b":"# Different functions for Feature Selection","e68b6bb0":"# Modeling","ff0d15af":"# Modeling by handle"}}