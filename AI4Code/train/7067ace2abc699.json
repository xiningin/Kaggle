{"cell_type":{"f98f95a5":"code","2f205688":"code","631834e5":"code","f0018a11":"code","312715b2":"code","359a5cdc":"code","d1622102":"code","aa8e8358":"code","ae4af3dd":"code","447504a5":"code","eae864d2":"code","eb1650e5":"code","0333fee7":"code","7ff86217":"code","f88481f8":"code","b0e2d213":"code","475119ae":"code","663c540d":"code","ae2cceac":"code","9ff3a18a":"code","56529d74":"code","ff1354f2":"code","3169e6d8":"code","41426a7a":"code","565fc938":"code","8f6ac901":"code","bc4cd0c1":"code","53e60bdd":"code","d6b6963d":"code","d49e67d3":"code","d8d297fa":"code","a0f853db":"code","2b4f7f89":"code","90f7d752":"code","4d92ba5c":"code","9d466cbe":"code","45f6dfa9":"code","84cb4893":"code","3de77898":"code","52d81c41":"code","26ae5494":"code","1638b967":"code","be89d308":"code","b7a00926":"code","dc1eec7f":"code","68a6cf0f":"code","89019d36":"code","c1683b54":"code","12e4226c":"code","57d5115a":"code","d9d79796":"code","d05c56b9":"code","2b0b3c79":"code","b9a81da7":"code","4c2c1848":"code","f7cf4713":"code","7438ba50":"code","624a9e0b":"code","93a380d3":"code","60d971e0":"code","727832ad":"code","a402834c":"code","ce2d292e":"code","f67344fc":"code","661799c8":"code","51c514a5":"code","fa5d24c4":"code","85226547":"code","f736ce38":"code","a17a1060":"code","c3c93820":"code","41433eb8":"code","243b3cc5":"code","88292ed0":"code","1a2ebd34":"code","bad1b39c":"code","004fc62c":"code","1a2538b0":"code","b7e30d97":"code","a177744b":"code","d0ab5597":"code","4a1d8529":"code","8f7e42f1":"code","a38751d8":"code","f803fa17":"code","7a4196db":"code","7faaef8f":"code","6741d378":"code","01c33a97":"code","8f8f2690":"code","c2bac2b0":"code","7c94cefa":"code","dd661aab":"markdown","4bbf8515":"markdown","32249002":"markdown","bfacc433":"markdown","7f6036ad":"markdown","85736f9a":"markdown","7903babb":"markdown","e81eae89":"markdown","f2a2106e":"markdown","2354ed60":"markdown","a36284ec":"markdown","b322a050":"markdown","5d4c0261":"markdown","9f396139":"markdown","98f1edef":"markdown","bcb883c5":"markdown","66260ee2":"markdown","98137566":"markdown","762be622":"markdown","bd8e75f7":"markdown","d8d696d7":"markdown","d62dbc93":"markdown","ba899d3f":"markdown","6dc651f6":"markdown","7fa93a48":"markdown","17b1f4a5":"markdown","a3573ce7":"markdown","efbbecaf":"markdown","5b2f9f59":"markdown","02b8a6da":"markdown","a82fefea":"markdown","9db875b6":"markdown","093d3044":"markdown","b772d0ec":"markdown","65ad47d4":"markdown","76c8ee65":"markdown","29088233":"markdown","cc286f81":"markdown","bd457c5d":"markdown","56e880d1":"markdown","2b3c55f8":"markdown","59c70e54":"markdown","3a8588be":"markdown","62116f38":"markdown","f796a829":"markdown","b7cccf68":"markdown"},"source":{"f98f95a5":"# Creaci\u00f3n de gr\u00e1ficos.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scikitplot as skplt\n# Manipulaci\u00f3n de datos \/ \u00e1lgebra lineal.\nimport numpy as np\nimport pandas as pd\n# Utilidades.\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics import confusion_matrix, recall_score, accuracy_score, precision_score\n# Algoritmos\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier as xgb\n# Otros\nimport warnings","2f205688":"# Algunas configuraciones.\n%matplotlib inline\nsns.set_style(\"darkgrid\")\nwarnings.filterwarnings(\"ignore\")\nplt.rc(\"font\", family=\"serif\", size=15)","631834e5":"telemetry = pd.read_csv(\"..\/input\/PdM_telemetry.csv\", error_bad_lines=False)\nerrors = pd.read_csv(\"..\/input\/PdM_errors.csv\", error_bad_lines=False)\nmaint = pd.read_csv(\"..\/input\/PdM_maint.csv\", error_bad_lines=False)\nfailures = pd.read_csv(\"..\/input\/PdM_failures.csv\", error_bad_lines=False)\nmachines = pd.read_csv(\"..\/input\/PdM_machines.csv\", error_bad_lines=False)","f0018a11":"telemetry.head()","312715b2":"telemetry.tail()","359a5cdc":"telemetry.info()","d1622102":"telemetry.dtypes","aa8e8358":"telemetry[\"machineID\"].nunique()","ae4af3dd":"# Cambiamos el formato de datetime ya que viene como string.\ntelemetry[\"datetime\"] = pd.to_datetime(telemetry[\"datetime\"], format=\"%Y-%m-%d %H:%M:%S\")\ntelemetry.head()","447504a5":"# el dtype de esta serie es objeto porque tiene tipos mixtos\ntelemetry.dtypes","eae864d2":"# Kernel Density function -> PDF (Funci\u00f3n de Densidad de Probabilidad)\n\n\"\"\"\nUna distribuci\u00f3n normal nos habla de una muestra representativa (Teorema del Limite Central).\nNos habla de un comportamiento natural.\nMuestra un \u00fanico grupo para trabajar (de lo contrario hacer clustering).\nLos modelos param\u00e9tricos esperan distribuciones normales porque utilizan la media, la std, etc.\nLos modelos no param\u00e9tricos e.g. CART (Classification and Regression Trees),\nson insensibles respecto a la distribuci\u00f3n de los datos,\nlo \u00fanico que importa es crear nodos que maximicen la separaci\u00f3n de clases.\nEn otros casos, aplicar a los datos transformaciones logar\u00edtmicas, regresi\u00f3n cuant\u00edlica, normalizaci\u00f3n.\nParam\u00e9trico: Regresi\u00f3n Lineal\nNo-param\u00e9trico: \u00c1rbol de Decisi\u00f3n\n\u00bfHay o no cambios a medida que el dataset crece?\n\"\"\"\n\ntelemetry[\"pressure\"].plot(kind=\"kde\")\nplt.title(\"Distribuci\u00f3n de la Presi\u00f3n\")\nplt.ylabel(\"Densidad\")\nplt.xlabel(\"Presi\u00f3n\")\nplt.show()","eb1650e5":"# Confirmamos integridad; totales, promedio, dsviasi\u00f3n est\u00e1ndar, m\u00ednimo, m\u00e1ximo, y cuantiles.\ntelemetry.describe()","0333fee7":"# Mostramos un gr\u00e1fico de ejemplo de los valores de voltaje para la m\u00e1quina 1 durante los primeros 6 meses del 2015.\n\nplot_df = telemetry.loc[\n    (telemetry[\"machineID\"] == 1)\n    & (telemetry[\"datetime\"] > pd.to_datetime(\"2015-01-01\"))\n    & (telemetry[\"datetime\"] < pd.to_datetime(\"2015-06-01\")), [\"datetime\", \"volt\"]\n]\n\nplt.figure(figsize=(12, 6))\nplt.plot(plot_df[\"datetime\"], plot_df[\"volt\"])\nplt.title(\"Variaci\u00f3n del Voltaje en la M\u00e1quina 1\")\nplt.ylabel(\"Voltaje\")\n\n# Hacemos legibles las etiquetas.\nadf = plt.gca().get_xaxis().get_major_formatter()\nadf.scaled[1.0] = \"%m-%d\"\nplt.xlabel(\"Tiempo\")\nplt.show()","7ff86217":"errors.head()","f88481f8":"errors.tail()","b0e2d213":"errors.info()","475119ae":"# Formateo del campo de fecha y hora que viene como string.\n# Las categor\u00edas permiten la comparaci\u00f3n entre valores, ordenamiento autom\u00e1tico, graficado m\u00e1s sencillo y otras funciones.\n# Tambi\u00e9n menos memoria (similar a \"factor\" en R).\nerrors[\"datetime\"] = pd.to_datetime(errors[\"datetime\"], format=\"%Y-%m-%d %H:%M:%S\")\nerrors[\"errorID\"] = errors[\"errorID\"].astype(\"category\")\n\nerrors.head()","663c540d":"sns.set_style(\"darkgrid\")\nplt.figure(figsize=(8, 4))\nerrors[\"errorID\"].value_counts().plot(kind=\"bar\", rot=0)\nplt.title(\"Distribuci\u00f3n de los Tipos de Error\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Error Type\")\nplt.show()","ae2cceac":"maint.head()","9ff3a18a":"maint.tail()","56529d74":"maint.info()","ff1354f2":"# Formateo del campo de fecha y hora que viene como string.\nmaint[\"datetime\"] = pd.to_datetime(maint[\"datetime\"], format=\"%Y-%m-%d %H:%M:%S\")\nmaint[\"comp\"] = maint[\"comp\"].astype(\"category\")\nmaint.dtypes","3169e6d8":"sns.set_style(\"darkgrid\")\nplt.figure(figsize=(8, 4))\nmaint[\"comp\"].value_counts().plot(kind=\"bar\", rot=0)\nplt.title(\"Distribuci\u00f3n de Reemplazos de Componentes\")\nplt.ylabel(\"Cantidad\")\nplt.xlabel(\"Componente\")\nplt.show()","41426a7a":"machines.head()","565fc938":"machines.tail()","8f6ac901":"machines.shape","bc4cd0c1":"machines.dtypes","53e60bdd":"# Revisamos si existen varias colinas ya que puede sugerir dos grupos diferentes.\nmachines[\"age\"].plot(kind=\"kde\")\nplt.title(\"Distribuci\u00f3n de Edades de M\u00e1quinas\")\nplt.xlabel(\"Edad\")\nplt.ylabel(\"Densidad\")\nplt.show()","d6b6963d":"# Aplicamos logaritmo natural para normalizar.\nnp.log(machines[machines[\"age\"] != 0].iloc[:, 0]).plot(kind=\"kde\")\nplt.show()","d49e67d3":"machines[\"model\"] = machines[\"model\"].astype(\"category\")\nmachines.dtypes","d8d297fa":"plt.figure(figsize=(8, 6))\n_, bins, _ = plt.hist([\n    machines.loc[machines[\"model\"] == \"model1\", \"age\"],\n    machines.loc[machines[\"model\"] == \"model2\", \"age\"],\n    machines.loc[machines[\"model\"] == \"model3\", \"age\"],\n    machines.loc[machines[\"model\"] == \"model4\", \"age\"]],\n    20, stacked=True, label=[\"model1\", \"model2\", \"model3\", \"model4\"\n])\nplt.title(\"Distribuci\u00f3n de Edades por Modelo\")\nplt.xlabel(\"Edad (a\u00f1os)\")\nplt.ylabel(\"Cantidad\")\nplt.legend()\nplt.show()","a0f853db":"failures.head()","2b4f7f89":"failures.tail()","90f7d752":"failures.info()","4d92ba5c":"# Formateamos el datetime que viene como string\nfailures[\"datetime\"] = pd.to_datetime(failures[\"datetime\"], format=\"%Y-%m-%d %H:%M:%S\")\nfailures[\"failure\"] = failures[\"failure\"].astype(\"category\")\nfailures.dtypes","9d466cbe":"failures.describe(include=\"all\")","45f6dfa9":"plt.figure(figsize=(8, 4))\nfailures[\"failure\"].value_counts().plot(kind=\"bar\", rot=0)\nplt.title(\"Distribuci\u00f3n de Fallas de Componentes\")\nplt.ylabel(\"Cantidad\")\nplt.xlabel(\"Componentes\")\nplt.show()","84cb4893":"# Calculamos valores promedio para caracter\u00edsticas de telemetr\u00eda\n\ntemp = []\nfields = [\"volt\", \"rotate\", \"pressure\", \"vibration\"]\n\n# pivotamos porque necesitamos el datetime como \u00edndice para para que \"resample\" funcione\n# resample crea el lagging\n# closed = 'right' => (6:00, 9:00] o 6:00 < x <= 9:00\n# closed='left'  => [6:00, 9:00) o 6:00 <= x < 9:00\n# no pueden ser ambos\n# unstack: devuelve df al formato original\n# tenemos 100 m\u00e1quinas, 4 sensores = 400 columnas\n# unstack muestra un dataseries el las columnas como \u00edndice y regresa serie (si hay varios \u00edndices se reacomodan).\n# Cada dataframe en temp tiene los valores del campo que le corresponde en ese momento.\n\ntemp = [\n    pd.pivot_table(\n        telemetry,\n        index=\"datetime\",\n        columns=\"machineID\",\n        values=col).resample(\"3H\", closed=\"left\", label=\"right\").mean().unstack()\n    for col in fields\n]\ntemp[0].head()","3de77898":"telemetry_mean_3h = pd.concat(temp, axis=1) # Unimos las series.\ntelemetry_mean_3h.columns = [col + \"mean_3h\" for col in fields] # Asignamos nombres de columnas.\ntelemetry_mean_3h.reset_index(inplace=True) # Aplanamos el frame.\ntelemetry_mean_3h.head()","52d81c41":"# Repetimos para la desviaci\u00f3n est\u00e1ndar.\ntemp = [\n    pd.pivot_table(\n        telemetry,\n        index=\"datetime\",\n        columns=\"machineID\",\n        values=col).resample(\"3H\", closed=\"left\", label=\"right\").std().unstack()\n    for col in fields\n]\ntemp[0].head()","26ae5494":"telemetry_sd_3h = pd.concat(temp, axis=1)\ntelemetry_sd_3h.columns = [i + \"sd_3h\" for i in fields]\ntelemetry_sd_3h.reset_index(inplace=True)\ntelemetry_sd_3h.head()","1638b967":"# Para capturar un efecto a mayor plazo, las funciones de lagging de 24 horas tambi\u00e9n se calculan.\n# Creamos nuevos valores con promedios de 24 horas, y luego seleccionamos el primer resultado cada 3 horas.\n# De esta manera podremos unir los resultados con las caracter\u00edsticas de lagging anteriores (calculadas a 3 horas).\n\ntemp = []\nfields = [\"volt\", \"rotate\", \"pressure\", \"vibration\"]\n\ntemp = [\n    pd.pivot_table(\n        telemetry,\n        index=\"datetime\",\n        columns=\"machineID\",\n        values=col).rolling(window=24).mean().resample(\"3H\", closed=\"left\", label=\"right\").first().unstack()\n    for col in fields\n]\ntemp[0].head()","be89d308":"telemetry_mean_24h = pd.concat(temp, axis=1)\ntelemetry_mean_24h.columns = [i + \"mean_24h\" for i in fields]\ntelemetry_mean_24h.reset_index(inplace=True)\n# Debido al m\u00e9todo de la media m\u00f3vil, los primeros 23 registros son nulos; hay que eliminarlos.\n# No ocurre al final del frame porque rolling topa al final.\n# Terminamos con un frame de menos datos que el original de telemetr\u00eda as\u00ed como el anterior de 3H.\ntelemetry_mean_24h = telemetry_mean_24h.loc[-telemetry_mean_24h[\"voltmean_24h\"].isnull()]","b7a00926":"telemetry_mean_24h.head()","dc1eec7f":"# Repetimos para la desviaci\u00f3n est\u00e1ndar\ntemp = []\nfields = [\"volt\", \"rotate\", \"pressure\", \"vibration\"]\n\ntemp = [\n    pd.pivot_table(\n        telemetry,\n        index=\"datetime\",\n        columns=\"machineID\",\n        values=col).rolling(window=24).std().resample(\"3H\", closed=\"left\", label=\"right\").first().unstack(level=-1)\n    for col in fields\n]\ntemp[0].head()","68a6cf0f":"telemetry_sd_24h = pd.concat(temp, axis=1)\ntelemetry_sd_24h.columns = [i + \"sd_24h\" for i in fields]\ntelemetry_sd_24h.reset_index(inplace=True)\ntelemetry_sd_24h = telemetry_sd_24h.loc[-telemetry_sd_24h[\"voltsd_24h\"].isnull()]","89019d36":"telemetry_sd_24h.head(10)","c1683b54":"# Combinamos las caracter\u00edsticas creadas hasta ahora.\n# Tomamos los valores 2:6 para evitar ID y fechas duplicadas.\n# axis=0 nos movemos en direcci\u00f3n de las filas, axis=1, nos movemos en direcci\u00f3n de las columnas.\ntelemetry_feat = pd.concat([\n    telemetry_mean_3h,\n    telemetry_sd_3h.iloc[:, 2:6],\n    telemetry_mean_24h.iloc[:, 2:6],\n    telemetry_sd_24h.iloc[:, 2:6]], axis=1).dropna()\ntelemetry_feat.head()","12e4226c":"telemetry_feat.describe()","57d5115a":"\"\"\"\nComenzamos por reformatear los datos de error para tener una entrada por m\u00e1quina por tiempo\ncuando ocurri\u00f3 al menos un error.\nCreamos una columna para cada tipo de error.\n\"\"\"\nerror_count = pd.get_dummies(errors) # Ponemos un 1 si el error aparece para esa m\u00e1quina, 0 de lo contrario.\nerror_count.columns = [\"datetime\", \"machineID\", \"error1\", \"error2\", \"error3\", \"error4\", \"error5\"]\nerror_count.head(15)","d9d79796":"\"\"\"\nLas fechas del dataframe se repiten, as\u00ed que agrupamos por fecha.\nCombinamos errores para una m\u00e1quina dada en una hora espec\u00edfica.\nHacemos suma en caso de que existan m\u00faltiples erroes del mismo tipo al mismo tiempo, pero no esperado.\n\"\"\"\nerror_count_grouped = error_count.groupby([\"machineID\", \"datetime\"]).sum().reset_index()\nerror_count_grouped.head(15)","d05c56b9":"\"\"\"\nRevisamos que los errores registrados existan en las m\u00e1quinas disponibles llenano con 0\nlas no coincidencias por eso solo buscamos coincidencia con datetime y machineID.\n\"\"\"\nerror_count_filtered = telemetry[[\"datetime\", \"machineID\"]].merge(\n    error_count_grouped,\n    on=[\"machineID\", \"datetime\"],\n    how=\"left\"\n).fillna(0.0)\n\nerror_count_filtered.head()","2b0b3c79":"# Revisamos que no existan anomal\u00edas.\nerror_count_filtered.describe()","b9a81da7":"# Calculamos la cantidad total de errores para cada tipo de error durante lapsos de 24 horas. \n# Tomaremos puntos cada 3 horas.\n\ntemp = []\nfields = [\n    \"error%d\" % i\n    for i in range(1,6)\n]\n\ntemp = [\n    pd.pivot_table(\n        error_count_filtered,\n        index=\"datetime\",\n        columns=\"machineID\",\n        values=col).rolling(window=24).sum().resample(\"3H\", closed=\"left\", label=\"right\").first().unstack()\n    for col in fields\n]\ntemp[0].head(10)","4c2c1848":"error_count_total = pd.concat(temp, axis=1)\nerror_count_total.columns = [i + \"count\" for i in fields]\nerror_count_total.reset_index(inplace=True)\nerror_count_total = error_count_total.dropna()\nerror_count_total.head()","f7cf4713":"error_count_total[\"error5count\"].unique()","7438ba50":"error_count_total.describe()","624a9e0b":"maint.head()","93a380d3":"# creamos una columna para cada tipo de error\ncomp_rep = pd.get_dummies(maint)\ncomp_rep.columns = [\"datetime\", \"machineID\", \"comp1\", \"comp2\", \"comp3\", \"comp4\"]\ncomp_rep.head()","60d971e0":"# Combinamos reparaciones para una cierta m\u00e1quina en cierto momento.\n# Si no agrupamos por fecha podemos ver otra perspectiva.\n# Encontramos qu\u00e9 componenetes fallan juntos, ya que agrupamos por fecha.\ncomp_rep = comp_rep.groupby([\"machineID\", \"datetime\"]).sum().reset_index()\ncomp_rep.head()","727832ad":"# hay que agregar los timepos donde no hubo reemplazos\ncomp_rep = telemetry[[\"datetime\", \"machineID\"]].merge(\n    comp_rep,\n    on=[\"datetime\", \"machineID\"],\n    how=\"outer\").fillna(0).sort_values(by=[\"machineID\", \"datetime\"]\n)\ncomp_rep.head()","a402834c":"components = [\"comp1\", \"comp2\", \"comp3\", \"comp4\"]\nfor comp in components:\n    # Queremos obtener la fecha del cambio del componente m\u00e1s reciente.\n    comp_rep.loc[comp_rep[comp] < 1, comp] = None # Llenamos con nulo las muestras sin reemplazo.\n    # las fechas de las entradas que s\u00ed tienen reemplazos.\n    comp_rep.loc[-comp_rep[comp].isnull(), comp] = comp_rep.loc[-comp_rep[comp].isnull(), \"datetime\"]\n    # Hacemos un forward-fill de las fechas m\u00e1s recientes de un cambio de componente.\n    # Llenamos con el \u00faltimo valor v\u00e1lido encontrado top-bottom.\n    comp_rep[comp] = pd.to_datetime(comp_rep[comp].fillna(method=\"ffill\"))\n\n# eliminamos muestras del 2014, podr\u00edan tener nulos, los manenimientos comenzaron ese a\u00f1o.\ncomp_rep = comp_rep.loc[comp_rep[\"datetime\"] > pd.to_datetime(\"2015-01-01\")]\ncomp_rep.head(50)","ce2d292e":"# Reemplazamos las fechas m\u00e1s recientes de cambios por la cantidad de d\u00edas desde el cambio m\u00e1s reciente.\nfor comp in components: comp_rep[comp] = (comp_rep[\"datetime\"] - pd.to_datetime(comp_rep[comp])) \/ np.timedelta64(1, \"D\")\ncomp_rep.head()","f67344fc":"comp_rep.describe()","661799c8":"# Finalmente unimos todas las caracter\u00edsticas creadas.\nfinal_feat = telemetry_feat.merge(error_count_total, on=[\"datetime\", \"machineID\"], how=\"left\")\nfinal_feat = final_feat.merge(comp_rep, on=[\"datetime\", \"machineID\"], how=\"left\")\nfinal_feat = final_feat.merge(machines, on=[\"machineID\"], how=\"left\")\nfinal_feat.head()","51c514a5":"final_feat.describe()","fa5d24c4":"final_feat.head()","85226547":"\"\"\"\nLe estamos diciendo al modelo que cualquier valor similar a los que se encuentran dentro de la ventana de 24 horas\nes una falla de ese componente, por eso que las m\u00e1quinas se repiten.\nUsamos limit=7 porque tenemos separaciones de 3 horas; 8 * 3 = las 24 horas\npero tenemos en cuenta el primer valor no nulo, por lo que es 7.\n\"\"\"\nlabeled_features = final_feat.merge(failures, on=[\"datetime\", \"machineID\"], how=\"left\")\n# Aplicamos un backward-fill de hasta 24h.\n# fillna no funciona con tipos categ\u00f3ricos por el momento (\u00bfc\u00f3mo encajar\u00eda la categor\u00eda nueva? tal vez).\n# Pasamos a object o string, aplicamos la operaci\u00f3n, y regresamos a categor\u00eda.\nlabeled_features[\"failure\"] = labeled_features[\"failure\"].astype(object).fillna(method=\"bfill\", limit=7)\nlabeled_features[\"failure\"] = labeled_features[\"failure\"].fillna(\"none\")\nlabeled_features[\"failure\"] = labeled_features[\"failure\"].astype(\"category\")\nlabeled_features.head()","f736ce38":"model_dummies = pd.get_dummies(labeled_features[\"model\"])\nlabeled_features = pd.concat([labeled_features, model_dummies], axis=1)\nlabeled_features.drop(\"model\", axis=1, inplace=True)","a17a1060":"labeled_features.head()","c3c93820":"### An\u00e1lisis de Correlaci\u00f3n","41433eb8":"# Es necesario eliminar las variables con alta correlaci\u00f3n (s\u00f3lo una), considerar > 70%.\nf, ax = plt.subplots(figsize=(10, 8))\ncorr = labeled_features.corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)\nplt.title(\"Correlaci\u00f3n Entre Variables\")\nplt.show()","243b3cc5":"# Guardamos para aplicar optimizaci\u00f3n de hiper-par\u00e1metros.\n#labeled_features.to_pickle(\"final_datset.pickle\")","88292ed0":"# Establecemos los tiempos correspondientes a los registros que se utilizar\u00e1n para entrenamiento y pruebas.\nthreshold_dates = [\n    pd.to_datetime(\"2015-09-30 01:00:00\"), pd.to_datetime(\"2015-10-01 01:00:00\")\n]","1a2ebd34":"test_results = []\nmodels = []\ntotal = len(threshold_dates)\n\n# Hacemos la partici\u00f3n de fechas separadas.\nlast_train_date = threshold_dates[0]\nfirst_test_date = threshold_dates[1]","bad1b39c":"# T\u00edpicamente se utiliza entre el 20 y el 30% de los datos.\nntraining = labeled_features.loc[labeled_features[\"datetime\"] < last_train_date]\nntesting = labeled_features.loc[labeled_features[\"datetime\"] > first_test_date]\nprint(f\"{ntraining.shape[0]} registros para entrenamiento.\")\nprint(f\"{ntesting.shape[0]} registros para pruebas.\")\nprint(f\"{ntesting.shape[0] \/ ntraining.shape[0] * 100:0.1f}% de los datos se usar\u00e1n para pruebas.\")","004fc62c":"fails_train = ntraining[ntraining[\"failure\"] != \"none\"].shape[0]\nno_fails_train = ntraining[ntraining[\"failure\"] == \"none\"].shape[0]\nfails_test = ntesting[ntesting[\"failure\"] != \"none\"].shape[0]\nno_fails_test = ntesting[ntesting[\"failure\"] == \"none\"].shape[0]\n\nprint(f\"{fails_train \/ no_fails_train * 100:0.1f}% de los casos son fallas en set de entrenamiento.\")\nprint(f\"{fails_test \/ no_fails_test * 100:0.1f}% de los casos son fallas en set de pruebas.\")","1a2538b0":"# Asignamos los valores correspondientes a entrenamiento y pruebas.\ntrain_y = labeled_features.loc[labeled_features[\"datetime\"] < last_train_date, \"failure\"]\ntrain_X = labeled_features.loc[labeled_features[\"datetime\"] < last_train_date].drop([\"datetime\",\n                                                                                    \"machineID\",\n                                                                                    \"failure\"], axis=1)\ntest_y = labeled_features.loc[labeled_features[\"datetime\"] > first_test_date, \"failure\"]\ntest_X = labeled_features.loc[labeled_features[\"datetime\"] > first_test_date].drop([\"datetime\",\n                                                                                   \"machineID\",\n                                                                                   \"failure\"], axis=1)","b7e30d97":"# Entrenamiento del modelo.\n# model = GradientBoostingClassifier(random_state=42)\nmodel = xgb(n_jobs=-1)\nmodel.fit(train_X, train_y);","a177744b":"# Obtenemos resultados sobre el set de pruebas.\ntest_result = pd.DataFrame(labeled_features.loc[labeled_features[\"datetime\"] > first_test_date])\ntest_result[\"predicted_failure\"] = model.predict(test_X)\ntest_results.append(test_result)\nmodels.append(model)","d0ab5597":"# Below, we plot the feature importances in the (first) trained model\nplt.figure(figsize=(10, 10))\nlabels, importances = zip(*sorted(zip(test_X.columns, models[0].feature_importances_), reverse=False, key=lambda x: x[1]))\nplt.yticks(range(len(labels)), labels)\n_, labels = plt.xticks()\nplt.setp(labels, rotation=0)\nplt.barh(range(len(importances)), importances)\nplt.ylabel(\"Caracter\u00edsticas\")\nplt.xlabel(\"Importancia (%)\")\nplt.title(\"Importancias de las Caracter\u00edsticas de Acuerdo al Modelo\")\nplt.show()","4a1d8529":"NOTA: Para selecci\u00f3n de variables\n    Correlaci\u00f3n Pearson: Dependencia lineal entre dos variables cont\u00ednuas.\n    LDA: Buscar la mejor combinaci\u00f3n lineal de variables capaz de separar clases categ\u00f3ricas.\n    ANOVA: Combinaci\u00f3n de variables categ\u00f3ricas para estimar variable cont\u00ednua. \n    Chi-Square: Busca correlaciones entre grupos de variables categ\u00f3ricas.\n    Selecci\u00f3n hacia adelante: Agregar variables una por una y observar desempe\u00f1o.\n    Eliminaci\u00f3n hacia atr\u00e1s: Eliminar variables una por una y observar desempe\u00f1o.\n    Eliminaci\u00f3n recursiva: Algoritmo de optimizaci\u00f3n para pruebas con diferentes subconjuntos de variables.\n    Revolvemos las columnas una por una y evaluamos resultados.","8f7e42f1":"# Hay un desbalance esperado.\nplt.figure(figsize=(8, 4))\nlabeled_features[\"failure\"].value_counts().plot(kind=\"bar\", rot=0)\nplt.title(\"Distribuci\u00f3n de Causas de Fallos\")\nplt.xlabel(\"Componente\")\nplt.ylabel(\"Cantidad\")\nplt.show()","a38751d8":"def Evaluate(predicted, actual, labels):\n    output_labels = []\n    output = []\n    \n    # Calculate and display confusion matrix\n    cm = confusion_matrix(actual, predicted, labels=labels)\n    #print(\"Confusion matrix\\n- x-axis is true labels (none, comp1, etc.)\\n- y-axis is predicted labels\")\n    #print(cm)\n    \n    # Calculate precision, recall, and F1 score\n    accuracy = np.array([float(np.trace(cm)) \/ np.sum(cm)] * len(labels))\n    precision = precision_score(actual, predicted, average=None, labels=labels)\n    recall = recall_score(actual, predicted, average=None, labels=labels)\n    f1 = 2 * precision * recall \/ (precision + recall)\n    output.extend([accuracy.tolist(), precision.tolist(), recall.tolist(), f1.tolist()])\n    output_labels.extend([\"accuracy\", \"precision\", \"recall\", \"F1\"])\n    \n    # Calculate the macro versions of these metrics\n    output.extend([[np.mean(precision)] * len(labels),\n                   [np.mean(recall)] * len(labels),\n                   [np.mean(f1)] * len(labels)])\n    output_labels.extend([\"macro precision\", \"macro recall\", \"macro F1\"])\n    \n    # Find the one-vs.-all confusion matrix\n    cm_row_sums = cm.sum(axis = 1)\n    cm_col_sums = cm.sum(axis = 0)\n    s = np.zeros((2, 2))\n    for i in range(len(labels)):\n        v = np.array([[cm[i, i],\n                       cm_row_sums[i] - cm[i, i]],\n                      [cm_col_sums[i] - cm[i, i],\n                       np.sum(cm) + cm[i, i] - (cm_row_sums[i] + cm_col_sums[i])]])\n        s += v\n    s_row_sums = s.sum(axis = 1)\n    \n    # Add average accuracy and micro-averaged  precision\/recall\/F1\n    avg_accuracy = [np.trace(s) \/ np.sum(s)] * len(labels)\n    micro_prf = [float(s[0,0]) \/ s_row_sums[0]] * len(labels)\n    output.extend([avg_accuracy, micro_prf])\n    output_labels.extend([\"average accuracy\",\n                          \"micro-averaged precision\/recall\/F1\"])\n    \n    # Compute metrics for the majority classifier\n    mc_index = np.where(cm_row_sums == np.max(cm_row_sums))[0][0]\n    cm_row_dist = cm_row_sums \/ float(np.sum(cm))\n    mc_accuracy = 0 * cm_row_dist; mc_accuracy[mc_index] = cm_row_dist[mc_index]\n    mc_recall = 0 * cm_row_dist; mc_recall[mc_index] = 1\n    mc_precision = 0 * cm_row_dist\n    mc_precision[mc_index] = cm_row_dist[mc_index]\n    mc_F1 = 0 * cm_row_dist;\n    mc_F1[mc_index] = 2 * mc_precision[mc_index] \/ (mc_precision[mc_index] + 1)\n    output.extend([mc_accuracy.tolist(), mc_recall.tolist(),\n                   mc_precision.tolist(), mc_F1.tolist()])\n    output_labels.extend([\"majority class accuracy\", \"majority class recall\",\n                          \"majority class precision\", \"majority class F1\"])\n        \n    # Random accuracy and kappa\n    cm_col_dist = cm_col_sums \/ float(np.sum(cm))\n    exp_accuracy = np.array([np.sum(cm_row_dist * cm_col_dist)] * len(labels))\n    kappa = (accuracy - exp_accuracy) \/ (1 - exp_accuracy)\n    output.extend([exp_accuracy.tolist(), kappa.tolist()])\n    output_labels.extend([\"expected accuracy\", \"kappa\"])\n    \n\n    # Random guess\n    rg_accuracy = np.ones(len(labels)) \/ float(len(labels))\n    rg_precision = cm_row_dist\n    rg_recall = np.ones(len(labels)) \/ float(len(labels))\n    rg_F1 = 2 * cm_row_dist \/ (len(labels) * cm_row_dist + 1)\n    output.extend([rg_accuracy.tolist(), rg_precision.tolist(),\n                   rg_recall.tolist(), rg_F1.tolist()])\n    output_labels.extend([\"random guess accuracy\", \"random guess precision\",\n                          \"random guess recall\", \"random guess F1\"])\n    \n    # Random weighted guess\n    rwg_accuracy = np.ones(len(labels)) * sum(cm_row_dist**2)\n    rwg_precision = cm_row_dist\n    rwg_recall = cm_row_dist\n    rwg_F1 = cm_row_dist\n    output.extend([rwg_accuracy.tolist(), rwg_precision.tolist(),\n                   rwg_recall.tolist(), rwg_F1.tolist()])\n    output_labels.extend([\"random weighted guess accuracy\",\n                          \"random weighted guess precision\",\n                          \"random weighted guess recall\",\n                          \"random weighted guess F1\"])\n\n    output_df = pd.DataFrame(output, columns=labels)\n    output_df.index = output_labels\n                  \n    return output_df","f803fa17":"evaluation_results = []\ntest_result = test_results[0]\nevaluation_result = Evaluate(actual = test_result[\"failure\"],\n                             predicted = test_result[\"predicted_failure\"],\n                             labels = [\"none\", \"comp1\", \"comp2\", \"comp3\", \"comp4\"])\nskplt.metrics.plot_confusion_matrix(\n    test_result[\"failure\"],\n    test_result[\"predicted_failure\"],\n    normalize=False,\n    title=\"Matriz de Confusi\u00f3n\"\n)\n\nskplt.metrics.plot_confusion_matrix(\n    test_result[\"failure\"],\n    test_result[\"predicted_failure\"],\n    normalize=True,\n    title=\"Matriz de Confusi\u00f3n Normalizada\",\n)\nplt.show()\n\nevaluation_results.append(evaluation_result)\nevaluation_results[0]  # show full results for first split only","7a4196db":"# Para un problema de clasificaci\u00f3n binaria por lo general se utiliza la curva ROC-AUC.\n# Para este caso multi-clase utilizaremos precisi\u00f3n vs sensivilidad.\nskplt.metrics.plot_precision_recall_curve(\n    test_y,\n    model.predict_proba(test_X),\n    title=\"Curva Precisi\u00f3n-Sensibilidad\",\n    figsize=(10,10)\n)\nplt.show()","7faaef8f":"evaluation_results[0].loc[\"recall\"].values","6741d378":"recall_df = pd.DataFrame([evaluation_results[0].loc[\"recall\"].values],\n                         columns=[\"none\", \"comp1\", \"comp2\", \"comp3\", \"comp4\"],\n                         index=[\"Sensibilidad por Componente\"])\nrecall_df.T","01c33a97":"test_values = train_X.iloc[0].values\ntest_values","8f8f2690":"# XGBoost acepta \u00fanicamente matrices de 2 dimensiones.\nsingle_test = pd.DataFrame([test_values], columns=test_X.columns, index=[0])\nsingle_test","c2bac2b0":"probas = model.predict_proba(single_test)\nprediction = model.predict(single_test)\nordered_classes = np.unique(np.array(test_y))","7c94cefa":"results = pd.DataFrame(probas,\n                       columns=ordered_classes,\n                       index=[0])\nprint(f\"Predicci\u00f3n: {prediction[0]}\")\nresults","dd661aab":"## Evaluaci\u00f3n ##","4bbf8515":"## Ingenier\u00eda de Caracter\u00edsticas (Feature Engineering) ##","32249002":"#### Construcci\u00f3n de la Etiqueta (Variable Objetivo \/ Dependiente) ####","bfacc433":"## An\u00e1lisis Exploratorio y Limpieza ##","7f6036ad":"Los modelos predictivos no tienen un conocimiento avanzado de las tendencias cronol\u00f3gicas futuras: en la pr\u00e1ctica, es probable que dichas tendencias existan y tengan un impacto adverso en el rendimiento del modelo. Para obtener una evaluaci\u00f3n precisa del rendimiento de un modelo predictivo, se recomienda realizar el entrenamiento en registros m\u00e1s antiguos y la validaci\u00f3n \/ prueba utilizando los registros m\u00e1s nuevos.","85736f9a":"#### Variables","7903babb":"Por ambas razones, una estrategia de divisi\u00f3n de registros dependiente del tiempo es una excelente opci\u00f3n para los modelos de mantenimiento predictivo. La divisi\u00f3n se efect\u00faa eligiendo un punto en el tiempo seg\u00fan el tama\u00f1o deseado de los sets de entrenamiento y prueba: todos los registros anteriores al punto de tiempo se usan para entrenar el modelo, y todos los registros restantes se usan para la prueba.","e81eae89":"\\* Generadas artificialmente\n- Historial de fallas: El historial de fallas de una m\u00e1quina o componente dentro de la m\u00e1quina.\n- Historial de mantenimiento: El historial de reparaciones de una m\u00e1quina, por ejemplo, c\u00f3digos de error, actividades de mantenimiento anteriores o sustituci\u00f3n de componentes.\n- Condiciones y uso de las m\u00e1quinas: Las condiciones de funcionamiento de una m\u00e1quina, por ejemplo, datos recogidos de los sensores.\n- Caracterisitcas de las m\u00e1quinas: Las caracter\u00edsticas de una m\u00e1quina, por ejemplo, tama\u00f1o del motor, marca y modelo, edad.","f2a2106e":"Registros que corresponden <b>tanto a inspecciones regulares como a fallas<\/b>.\nSe genera un registro si un componente se reemplaza durante la inspecci\u00f3n programada o se reemplaza debido a una falla.\nLos registros que se crean debido a fallas se llamar\u00e1n como tal, fallas.\nLos datos de mantenimiento tienen registros tanto de 2014 como de 2015.","2354ed60":"NOTA: Escalamiento para unidades disparejas: sklearn.preprocessing.StandardScaler.\nNOTA: Evitar transformaciones o nuevas variables en el dataset completo.\nNOTA: Transformaciones\n    1\/x para voltear unidades.\n    log para lidiar con desbalances.\n    a^b para limpieza de datos f\u00edsicos \/ biol\u00f3gicos.\n    a^3 similar a log pero lidia con ceros.","a36284ec":"# Machine Learning para Mantenimiento Predictivo","b322a050":"#### Fallas de Componentes ####","5d4c0261":"Las caracter\u00edsticas de las m\u00e1quinas se pueden utilizar sin modificaciones adicionales. Estos incluyen informaci\u00f3n descriptiva sobre el tipo de cada m\u00e1quina y su antig\u00fcedad (n\u00famero de a\u00f1os en servicio). ","9f396139":"NOTA:\n    Para variables con alta cardinalidad buscar agrupar en base a relaci\u00f3n con otras variables\n    Para transformaci\u00f3n de variables num\u00e9ricas a categ\u00f3ricas, usar \"binning\" (pandas: cut)","98f1edef":"####  Caracter\u00edsticas de las M\u00e1quinas ####","bcb883c5":"#### Telemetr\u00eda ####","66260ee2":"El objetivo de este proyecto es conocer la probabilidad de falla de un componente de una m\u00e1quina en el futuro cercano. <br>\nDe esta manera, es posible lidiar con costos relacionados a fallas mec\u00e1nicas con anticipaci\u00f3n actuando de manera preventiva ahorrando tiempo y dinero.","98137566":"Este dataset incluye informaci\u00f3n sobre las m\u00e1quinas: tipo de modelo y edad (a\u00f1os en servicio).","762be622":"#### Mantenimiento ####","bd8e75f7":"Estos son errores de <b>advertencia<\/b> generados cuando la m\u00e1quina a\u00fan est\u00e1 operando y no se consideran fallas.\nLa fecha y las horas de error se redondean a la hora m\u00e1s cercana ya que los datos de telemetr\u00eda se recopilan por hora.","d8d696d7":"#### Caracter\u00edsticas Lag en Errores ####","d62dbc93":"NOTA: Para problemas de desbalance de clases\n    Obtener m\u00e1s datos.\n    Muestra menor de la clase mayoritario.\n    SMOTE (Synthetic Minority Over-sampling Technique).\n    Asignar pesos a clase minoritaria.","ba899d3f":"Cuando trabaje con series de tiempo como en este ejemplo, la partici\u00f3n de registro en entrenamiento, validaci\u00f3n y pruebas deben realizarse con cuidado para evitar sobreestimar el rendimiento de los modelos. En el mantenimiento predictivo, las caracter\u00edsticas generalmente se generan utilizando agregados retrasados: los registros en la misma ventana de tiempo probablemente tendr\u00e1n etiquetas id\u00e9nticas y valores de caracter\u00edsticas similares. Estas correlaciones pueden dar a un modelo una \"ventaja injusta\" al predecir un registro perteneciente al set de pruebas que comparte su ventana de tiempo con un registro del set de entrenamiento. Por lo tanto, dividimos los registros en conjuntos de entrenamiento, validaci\u00f3n y prueba en grandes porciones, para minimizar el n\u00famero de intervalos de tiempo compartidos entre ellos.","6dc651f6":"NOTA\n    precisi\u00f3n: TP \/ (TP + FP), encontramos de m\u00e1s. \n    recall: TP \/ (TP + FN), encontramos de menos. Si decimos que todos fallan: 1\n    F1: 2 * (precision * recall) \/ (precision + recall)","7fa93a48":"Los registros de mantenimiento contienen la informaci\u00f3n de los registros de reemplazo de componentes.\nLas posibles caracter\u00edsticas de este conjunto de datos pueden ser calcular cu\u00e1nto tiempo ha pasado desde que se reemplaz\u00f3 un componente por \u00faltima vez, ya que cuanto m\u00e1s tiempo se usa un componente, mayor es la degradaci\u00f3n.","17b1f4a5":"Los datos de telemetr\u00eda casi siempre vienen con sellos de tiempo, lo que los hace adecuados para calcular las caracter\u00edsticas retrasadas (lagging features, estad\u00edstica de ciertos valores dentr de una ventana de tiempo).\n\nElegiremos el tama\u00f1o de una ventana y calcularemos las medidas agregadas (media, desviaci\u00f3n est\u00e1ndar, m\u00ednimo, m\u00e1ximo, etc.)\npara representar el historial a corto plazo.\n\nA continuaci\u00f3n, calcularemos la media m\u00f3vil y la desviaci\u00f3n est\u00e1ndar de los datos de telemetr\u00eda en la \u00faltima ventana de retraso de 3 horas.","a3573ce7":"Al igual que los datos de telemetr\u00eda, los errores vienen con marcas de tiempo. Contamos el n\u00famero de errores de cada tipo en una ventana retrasada.","efbbecaf":"## Modelado ##","5b2f9f59":"Datos en series de tiempo de telemetr\u00eda con mediciones de voltaje, rotaci\u00f3n, presi\u00f3n y vibraci\u00f3n recolectadas de 100 m\u00e1quinas en tiempo real promediadas en cada hora recolectada durante el a\u00f1o 2015.","02b8a6da":"Cuando se usa la clasificaci\u00f3n de m\u00faltiples clases para predecir fallas debido a un problema, el etiquetado se realiza tomando una ventana de tiempo antes de la falla de un activo y etiquetando los registros de caracter\u00edsticas que caen en esa ventana como \"a punto de fallar debido a un problema\" mientras etiquetando todos los dem\u00e1s registros como \"normal\". Esta ventana de tiempo debe seleccionarse de acuerdo a la regla de negocio: en algunas situaciones puede ser suficiente predecir las fallas con horas de anticipaci\u00f3n, mientras que en otras se pueden necesitar d\u00edas o semanas de anticipaci\u00f3n.","a82fefea":"#### Errores ####","9db875b6":"## Descripci\u00f3n ##","093d3044":"Caso sensores NASA: Predictive Maintenance ML (IIoT):\nhttps:\/\/www.kaggle.com\/billstuart\/predictive-maintenance-ml-iiot\n\nTelco Customer Churn:\nhttps:\/\/www.kaggle.com\/blastchar\/telco-customer-churn\n\nEntrevistas:\nhttps:\/\/towardsdatascience.com\/data-science-interview-guide-4ee9f5dc778\n\nDe SQL a Pandas:\nhttps:\/\/medium.com\/jbennetcodes\/how-to-rewrite-your-sql-queries-in-pandas-and-more-149d341fc53e?mkt_tok=eyJpIjoiWldSa01EZGtPRFE0TTJRNCIsInQiOiJoV0V6RFZ6UFVHTnc5NGVzaHNzSTZaS1pxc1Z3WVJsYWE5NmRocmNBOW9UdkJTSDV5d1cyRVJ4czBVYnJPZnZZekJiXC9QWE1xSFBDdjVOOEtlT2FoZ0xVUElxWWMzMk0xamt2Y09nN1pIdCtSQnVic3F1YmluZHQ5WGlpYlpcL09CIn0%3D\n\nTidyverse:\nhttp:\/\/www.dartistics.com\/tidyverse.html","b772d0ec":"#### Tiempo desde el \u00daltimo Reemplazo por Mantenimiento ####","65ad47d4":"Estos son los registros de reemplazos de componentes debido a <b>fallas<\/b>. Cada registro tiene una fecha y hora, ID de m\u00e1quina y el componente fallido.","76c8ee65":"En esta secci\u00f3n, el conocimiento del dominio juega un papel importante en la comprensi\u00f3n de los factores predictivos de un problema.\nA continuaci\u00f3n, los d\u00edas desde la \u00faltima sustituci\u00f3n de componentes se calculan para cada tipo de componente como caracter\u00edsticas a partir de los datos de mantenimiento.","29088233":"NOTA: Explorar valores SHAP","cc286f81":"#### M\u00e1quinas ####","bd457c5d":"NOTA:\n    Para selecci\u00f3n de variables usar FeatureSelector.\n    Casos de limpieza de datos:\n        Nombre de columnas\n        Caracteres incorrectos\n        Valores nulos\n        Tipos de dato incorrecto\n        Descubrir valores incorrectos\n        Valores duplicados\n        Aplanar dataset","56e880d1":"El objetivo es calcular la probabilidad de que una m\u00e1quina falle en las pr\u00f3ximas 24 horas debido a un determinado fallo del componente\n\nA continuaci\u00f3n, se crea una caracter\u00edstica de falla categ\u00f3rica para servir como etiqueta. Todos los registros dentro de una ventana de 24 horas antes de una falla\ndel componente 1 tiene un error = comp1, y as\u00ed sucesivamente para los componentes 2, 3 y 4; Todos los registros que no est\u00e9n dentro de las 24 horas posteriores a una falla del componente tienen falla = none","2b3c55f8":"Estamos interesados en cuanto errores fueron encontrados (recall).","59c70e54":"## Enlaces \u00datiles ##","3a8588be":"#### Entrenamiento, Validaci\u00f3n y Pruebas ####","62116f38":"<i>Este notebook de Python muestra una soluci\u00f3n a un caso de Mantenimiento Predictivo con Aprendizaje Autom\u00e1tico. <\/i>\n* Editado por <b>Jos\u00e9 Gonz\u00e1lez A.<\/b> (josegzza@gmail.com)\n* Basado en la versi\u00f3n creada por <b>Mary Wahl<\/b> (https:\/\/gallery.azure.ai\/Notebook\/Predictive-Maintenance-Modelling-Guide-Python-Notebook-1)\n* Primera versi\u00f3n creada por <b>Fidan Boylu Uz<\/b> (https:\/\/gallery.azure.ai\/Notebook\/Predictive-Maintenance-Implementation-Guide-R-Notebook-2)","f796a829":"## Caso de Prueba Individual ####","b7cccf68":"#### Librer\u00edas y Configuraci\u00f3n ####"}}