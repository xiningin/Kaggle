{"cell_type":{"01438411":"code","28db9f02":"code","b54d6385":"code","697bea3c":"code","0115b530":"code","620da0d2":"code","78eae35e":"code","c5eff270":"code","e065d658":"code","77899e73":"code","89d507c7":"code","84e83995":"code","7932035d":"code","0ed5cffe":"code","22b8f728":"code","d428c69b":"code","2d1dd2c5":"code","834f1275":"markdown","2ba6b133":"markdown","32c9d6f0":"markdown","fddf1ef0":"markdown","138eeaa5":"markdown","c457a46d":"markdown","650c7cc6":"markdown","01c1e14b":"markdown","ccbc7e65":"markdown","35c6d5d0":"markdown","c4eb0c73":"markdown","2ebffa0e":"markdown","ff5d0b08":"markdown","aade0c82":"markdown","00620990":"markdown","59129f38":"markdown","b3b3c6c3":"markdown","95dba16b":"markdown"},"source":{"01438411":"# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nplt.style.use('seaborn-whitegrid')\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\nfrom keras.optimizers import SGD\nimport math\nfrom sklearn.metrics import mean_squared_error","28db9f02":"# Some functions to help out with\ndef return_rmse(test,predicted):\n    rmse = math.sqrt(mean_squared_error(test, predicted))\n    print(\"The root mean squared error is {}.\".format(rmse))","b54d6385":"import os\nfileList = os.listdir(\"..\/input\")","697bea3c":"companyList = []\nfor file in fileList:\n    companyName = file.split(\"_\")[0]\n    if companyName != \"all\":\n        companyList.append(companyName)\nprint(companyList)","0115b530":"# First, we get the data\nstockList = [\"GE\", \"MSFT\", \"GOOGL\", \"AAPL\", \"AMZN\", \"IBM\", \"CSCO\"]\ndf_ = {}\nfor i in stockList:\n    df_[i] = pd.read_csv(\"..\/input\/\" + i + \"_2006-01-01_to_2018-01-01.csv\", index_col=\"Date\", parse_dates=[\"Date\"])","620da0d2":"def split(dataframe, border, col):\n    return dataframe.loc[:border,col], dataframe.loc[border:,col]\n\ndf_new = {}\nfor i in stockList:\n    df_new[i] = {}\n    df_new[i][\"Train\"], df_new[i][\"Test\"] = split(df_[i], \"2015\", \"Close\")","78eae35e":"for i in stockList:\n    plt.figure(figsize=(14,4))\n    plt.plot(df_new[i][\"Train\"])\n    plt.plot(df_new[i][\"Test\"])\n    plt.ylabel(\"Price\")\n    plt.xlabel(\"Date\")\n    plt.legend([\"Training Set\", \"Test Set\"])\n    plt.title(i + \" Closing Stock Price\")","c5eff270":"# Scaling the training set\ntransform_train = {}\ntransform_test = {}\nscaler = {}\n\nfor num, i in enumerate(stockList):\n    sc = MinMaxScaler(feature_range=(0,1))\n    a0 = np.array(df_new[i][\"Train\"])\n    a1 = np.array(df_new[i][\"Test\"])\n    a0 = a0.reshape(a0.shape[0],1)\n    a1 = a1.reshape(a1.shape[0],1)\n    transform_train[i] = sc.fit_transform(a0)\n    transform_test[i] = sc.fit_transform(a1)\n    scaler[i] = sc\n    \ndel a0\ndel a1","e065d658":"for i in transform_train.keys():\n    print(i, transform_train[i].shape)\nprint(\"\\n\")    \nfor i in transform_test.keys():\n    print(i, transform_test[i].shape)","77899e73":"trainset = {}\ntestset = {}\nfor j in stockList:\n    trainset[j] = {}\n    X_train = []\n    y_train = []\n    for i in range(60,2516):\n        X_train.append(transform_train[j][i-60:i,0])\n        y_train.append(transform_train[j][i,0])\n    X_train, y_train = np.array(X_train), np.array(y_train)\n    trainset[j][\"X\"] = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\n    trainset[j][\"y\"] = y_train\n    \n    testset[j] = {}\n    X_test = []\n    y_test = []    \n    for i in range(60, 755):\n        X_test.append(transform_test[j][i-60:i,0])\n        y_test.append(transform_test[j][i,0])\n    X_test, y_test = np.array(X_test), np.array(y_test)\n    testset[j][\"X\"] = np.reshape(X_test, (X_test.shape[0], X_train.shape[1], 1))\n    testset[j][\"y\"] = y_test","89d507c7":"arr_buff = []\nfor i in stockList:\n    buff = {}\n    buff[\"X_train\"] = trainset[i][\"X\"].shape\n    buff[\"y_train\"] = trainset[i][\"y\"].shape\n    buff[\"X_test\"] = testset[i][\"X\"].shape\n    buff[\"y_test\"] = testset[i][\"y\"].shape\n    arr_buff.append(buff)\n\npd.DataFrame(arr_buff, index=stockList)","84e83995":"%%time\n# The LSTM architecture\nregressor = Sequential()\n# First LSTM layer with Dropout regularisation\nregressor.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1],1)))\nregressor.add(Dropout(0.2))\n# Second LSTM layer\nregressor.add(LSTM(units=50, return_sequences=True))\nregressor.add(Dropout(0.2))\n# Third LSTM layer\nregressor.add(LSTM(units=50, return_sequences=True))\nregressor.add(Dropout(0.5))\n# Fourth LSTM layer\nregressor.add(LSTM(units=50))\nregressor.add(Dropout(0.5))\n# The output layer\nregressor.add(Dense(units=1))\n\n# Compiling the RNN\nregressor.compile(optimizer='rmsprop', loss='mean_squared_error')\n# Fitting to the training set\nfor i in stockList:\n    print(\"Fitting to\", i)\n    regressor.fit(trainset[i][\"X\"], trainset[i][\"y\"], epochs=10, batch_size=200)","7932035d":"pred_result = {}\nfor i in stockList:\n    y_true = scaler[i].inverse_transform(testset[i][\"y\"].reshape(-1,1))\n    y_pred = scaler[i].inverse_transform(regressor.predict(testset[i][\"X\"]))\n    MSE = mean_squared_error(y_true, y_pred)\n    pred_result[i] = {}\n    pred_result[i][\"True\"] = y_true\n    pred_result[i][\"Pred\"] = y_pred\n    \n    plt.figure(figsize=(14,6))\n    plt.title(\"{} with MSE {:10.4f}\".format(i,MSE))\n    plt.plot(y_true)\n    plt.plot(y_pred)","0ed5cffe":"time_index = df_new[\"CSCO\"][\"Test\"][60:].index\ndef lagging(df, lag, time_index):\n    df_pred = pd.Series(df[\"Pred\"].reshape(-1), index=test_index)\n    df_true = pd.Series(df[\"True\"].reshape(-1), index=test_index)\n    \n    df_pred_lag = df_pred.shift(lag)\n    \n    print(\"MSE without Lag\", mean_squared_error(np.array(df_true), np.array(df_pred)))\n    print(\"MSE with Lag 5\", mean_squared_error(np.array(df_true[:-5]), np.array(df_pred_lag[:-5])))\n\n    plt.figure(figsize=(14,4))\n    plt.title(\"Prediction without Lag\")\n    plt.plot(df_true)\n    plt.plot(df_pred)\n\n    MSE_lag = mean_squared_error(np.array(df_true[:-5]), np.array(df_pred_lag[:-5]))\n    plt.figure(figsize=(14,4))\n    plt.title(\"Prediction with Lag\")\n    plt.plot(df_true)\n    plt.plot(df_pred_lag)\n","22b8f728":"lagging(pred_result[\"IBM\"], -5, time_index)","d428c69b":"# The GRU architecture\nregressorGRU = Sequential()\n# First GRU layer with Dropout regularisation\nregressorGRU.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\nregressorGRU.add(Dropout(0.2))\n# Second GRU layer\nregressorGRU.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\nregressorGRU.add(Dropout(0.2))\n# Third GRU layer\nregressorGRU.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\nregressorGRU.add(Dropout(0.2))\n# Fourth GRU layer\nregressorGRU.add(GRU(units=50, activation='tanh'))\nregressorGRU.add(Dropout(0.2))\n# The output layer\nregressorGRU.add(Dense(units=1))\n# Compiling the RNN\nregressorGRU.compile(optimizer=SGD(lr=0.01, decay=1e-7, momentum=0.9, nesterov=False),loss='mean_squared_error')\n# Fitting to the training set\nfor i in stockList:\n    print(\"Fitting to\", i)\n    regressorGRU.fit(trainset[i][\"X\"], trainset[i][\"y\"],epochs=50,batch_size=150)","2d1dd2c5":"for i in stockList:\n    y_true = scaler[i].inverse_transform(testset[i][\"y\"].reshape(-1,1))\n    y_pred = scaler[i].inverse_transform(regressorGRU.predict(testset[i][\"X\"]))\n    MSE = mean_squared_error(y_true, y_pred)\n    \n    plt.figure(figsize=(14,6))\n    plt.title(\"{} with MSE {:10.4f}\".format(i,MSE))\n    plt.plot(y_true)\n    plt.plot(y_pred)","834f1275":"## Long Short Term Memory(LSTM)\nLong short-term memory (LSTM) units (or blocks) are a building unit for layers of a recurrent neural network (RNN). A RNN composed of LSTM units is often called an LSTM network. A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell is responsible for \"remembering\" values over arbitrary time intervals; hence the word \"memory\" in LSTM. Each of the three gates can be thought of as a \"conventional\" artificial neuron, as in a multi-layer (or feedforward) neural network: that is, they compute an activation (using an activation function) of a weighted sum. Intuitively, they can be thought as regulators of the flow of values that goes through the connections of the LSTM; hence the denotation \"gate\". There are connections between these gates and the cell.\n\nThe expression long short-term refers to the fact that LSTM is a model for the short-term memory which can last for a long period of time. An LSTM is well-suited to classify, process and predict time series given time lags of unknown size and duration between important events. LSTMs were developed to deal with the exploding and vanishing gradient problem when training traditional RNNs.\n\nSource: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Long_short-term_memory)\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/0*LyfY3Mow9eCYlj7o.\">\n\nSource: [Medium](https:\/\/codeburst.io\/generating-text-using-an-lstm-network-no-libraries-2dff88a3968)\n\nThe best LSTM explanation on internet: https:\/\/medium.com\/deep-math-machine-learning-ai\/chapter-10-1-deepnlp-lstm-long-short-term-memory-networks-with-math-21477f8e4235\n\nRefer above link for deeper insights.","2ba6b133":"We pick the tech companies from our list. We import each data and place it in a dictionary with key is its ticker symbol","32c9d6f0":"## Working of gates in LSTMs\nFirst, LSTM cell takes the previous memory state C<sub>t-1<\/sub> and does element wise multiplication with forget gate (f) to decide if  present memory state C<sub>t<\/sub>. If forget gate value is 0 then previous memory state is completely forgotten else f forget gate value is 1 then previous memory state is completely passed to the cell ( Remember f gate gives values between 0 and 1 ).\n\n**C<sub>t<\/sub> = C<sub>t-1<\/sub> * f<sub>t<\/sub>**\n\nCalculating the new memory state: \n\n**C<sub>t<\/sub> = C<sub>t<\/sub> + (I<sub>t<\/sub> * C\\`<sub>t<\/sub>)**\n\nNow, we calculate the output:\n\n**H<sub>t<\/sub> = tanh(C<sub>t<\/sub>)**","fddf1ef0":"We make all prices prior to 2015 as a training set and the rest as test set","138eeaa5":"We think 60 feature will be enough training.   We prepare shape our test and train set for neural network input","c457a46d":"#### Takeaway\n\n* When stock price is high, the MSE tend to get high but from the graph it predict as good as the rest. \n\n* LSTM and GRU done great job forecasting pricing of each company. Compared to non-neural network time series forecasting, neural network done superb job but with caveat. Neural network is only good at predicting  but not extracting properties of time series data.  Statistical relation cannot be explained by neural network. This maybe fine for someone who does cannot change the course of future data but for someone who need to make decision how future data is going need many information not just forecasting. \n\n* Combining time-series\/signal analysis with neural network architecture will yield a good result with good interpretability","650c7cc6":"The current version version uses a dense GRU network with 100 units as opposed to the GRU network with 50 units in previous version","01c1e14b":"We plot all companies we pick and paint which one is the training set and test set","ccbc7e65":"Check the shape (again) before start training","35c6d5d0":"## Components of LSTMs\nSo the LSTM cell contains the following components\n* Forget Gate \u201cf\u201d ( a neural network with sigmoid)\n* Candidate layer \u201cC\"(a NN with Tanh)\n* Input Gate \u201cI\u201d ( a NN with sigmoid )\n* Output Gate \u201cO\u201d( a NN with sigmoid)\n* Hidden state \u201cH\u201d ( a vector )\n* Memory state \u201cC\u201d ( a vector)\n\n* Inputs to the LSTM cell at any step are X<sub>t<\/sub> (current input) , H<sub>t-1<\/sub> (previous hidden state ) and C<sub>t-1<\/sub> (previous memory state).  \n* Outputs from the LSTM cell are H<sub>t<\/sub> (current hidden state ) and C<sub>t<\/sub> (current memory state)","c4eb0c73":"We rescale all stock price to zero for the lowest and 1 for the highest. Each of company have their own scale. We make another two dictionary which contain scaled price for each company. One contain train set and another contain test set. \n\nWe also create another dictionary for collecting the scaller. This will be useful when we want to inverse transform our prediction.","2ebffa0e":"We can improve our prediction by introducing shifting\/lagging. Essentialy we slide our prediction for a period of time. This is a common practice in signal processing subfield.\n\nWhen we displace to make our prediction start earlier, we call it lagging. As for consequences, lagged prediction will have last -- equal to how much we displace the prediction -- value equal to NaN. If we lag it by 5 day, then the last 5 day prediction will become NaN. \n\nWhen we displace to make our prediction start later, we call it shifting. As for consequenceses, shifted prediction will have first -- equal to how much we displace the prediction -- value equal to NaN. If we shift it by 5 day, then the last 5 day prediction will become NaN.","ff5d0b08":"### Importing Library and Packages","aade0c82":"We print the shape of our transformed set. Few company have the more than the other. This over value should be removed so the input will be uniform in term of shape","00620990":"## Recurrent Neural Networks\nIn a recurrent neural network we store the output activations from one or more of the layers of the network. Often these are hidden later activations. Then, the next time we feed an input example to the network, we include the previously-stored outputs as additional inputs. You can think of the additional inputs as being concatenated to the end of the \u201cnormal\u201d inputs to the previous layer. For example, if a hidden layer had 10 regular input nodes and 128 hidden nodes in the layer, then it would actually have 138 total inputs (assuming you are feeding the layer\u2019s outputs into itself \u00e0 la Elman) rather than into another layer). Of course, the very first time you try to compute the output of the network you\u2019ll need to fill in those extra 128 inputs with 0s or something.\n\nSource: [Quora](https:\/\/www.quora.com\/What-is-a-simple-explanation-of-a-recurrent-neural-network)\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*NKhwsOYNUT5xU7Pyf6Znhg.png\">\n\nSource: [Medium](https:\/\/medium.com\/ai-journal\/lstm-gru-recurrent-neural-networks-81fe2bcdf1f9)\n\nLet me give you the best explanation of Recurrent Neural Networks that I found on internet: https:\/\/www.youtube.com\/watch?v=UNmqTiOnRfg&t=3s","59129f38":"Truth be told. That's one awesome score. \n\nLSTM is not the only kind of unit that has taken the world of Deep Learning by a storm. We have **Gated Recurrent Units(GRU)**. It's not known, which is better: GRU or LSTM becuase they have comparable performances. GRUs are easier to train than LSTMs.\n\n## Gated Recurrent Units\nIn simple words, the GRU unit does not have to use a memory unit to control the flow of information like the LSTM unit. It can directly makes use of the all hidden states without any control. GRUs have fewer parameters and thus may train a bit faster or need less data to generalize. But, with large data, the LSTMs with higher expressiveness may lead to better results.\n\nThey are almost similar to LSTMs except that they have two gates: reset gate and update gate. Reset gate determines how to combine new input to previous memory and update gate determines how much of the previous state to keep. Update gate in GRU is what input gate and forget gate were in LSTM. We don't have the second non linearity in GRU before calculating the outpu, .neither they have the output gate.\n\nSource: [Quora](https:\/\/www.quora.com\/Whats-the-difference-between-LSTM-and-GRU-Why-are-GRU-efficient-to-train)\n\n<img src=\"https:\/\/cdnpythonmachinelearning.azureedge.net\/wp-content\/uploads\/2017\/11\/GRU.png?x31195\">","b3b3c6c3":"|Ticker Symbol | Company|\n|:---|:---|\n|CSCO | Cisco Systems Inc. |\n|TRV | Travelers Companies Inc. |\n|IBM | International Business Machine Corporation |\n|UTX | United Technologies Corporation |\n|PFE| Pfizer Inc. |\n|JNJ | Johnson and Johnson |\n|AXP | American Express Company |\n|GS | Goldman Sachs Group Inc. |\n|GOOGL | Google Inc. |\n|GE | General Electric |\n|KO | The Coca Cola Company |\n|VZ | Verizon Communications |\n|AABA | Altaba Inc. |\n|BA | Boeing Co|\n|NKE | Nike Inc. |\n|CVX | Chevron Corporation |\n|AMZN | Amazon Inc. |\n|UNH| United Health Group |\n|WMT| Walmart Inc. |\n|MMM| 3M Co |\n|JPM| JP Morgan Chase & Co. |\n|DIS| Walt Disney Co |\n|CAT| Caterpillar Co |\n|MRK| Merck & Co., Inc|\n|XOM| Exxon Mobil Corporation |\n|MSFT| Microsoft |\n|HD| Home Depot Inc |\n|INTC| Intel Corporation |\n|PG| Procter & Gamble Co |\n|AAPL| Apple Inc. |","95dba16b":"Now, even though RNNs are quite powerful, they suffer from  **Vanishing gradient problem ** which hinders them from using long term information, like they are good for storing memory 3-4 instances of past iterations but larger number of instances don't provide good results so we don't just use regular RNNs. Instead, we use a better variation of RNNs: **Long Short Term Networks(LSTM).**\n\n### What is Vanishing Gradient problem?\nVanishing gradient problem is a difficulty found in training artificial neural networks with gradient-based learning methods and backpropagation. In such methods, each of the neural network's weights receives an update proportional to the partial derivative of the error function with respect to the current weight in each iteration of training. The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training. As one example of the problem cause, traditional activation functions such as the hyperbolic tangent function have gradients in the range (0, 1), and backpropagation computes gradients by the chain rule. This has the effect of multiplying n of these small numbers to compute gradients of the \"front\" layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially with n while the front layers train very slowly.\n\nSource: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Vanishing_gradient_problem)\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1460\/1*FWy4STsp8k0M5Yd8LifG_Q.png\">\n\nSource: [Medium](https:\/\/medium.com\/@anishsingh20\/the-vanishing-gradient-problem-48ae7f501257)"}}