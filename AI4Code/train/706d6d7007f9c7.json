{"cell_type":{"b89e1b0d":"code","9fa849da":"code","2c68df47":"code","d47f6e7d":"code","4ee4a57b":"code","0b5d2ac0":"code","129207d2":"code","4bdc1e92":"code","3a8acbe4":"code","e91fcbfa":"code","ce232cdf":"code","bf608767":"code","05e5d43d":"code","c742aa49":"code","2a5d4ef5":"code","a645cd62":"code","0acd63a3":"code","d94da45f":"code","52803b13":"code","16d3067d":"code","d99cded0":"code","35ca80d3":"code","b87d2b16":"code","abfedc67":"code","bf7d5148":"code","95fd84ab":"code","5c8acfbb":"code","234017ba":"code","770462ad":"code","c7697fe0":"code","d0b36474":"code","c4582e28":"code","35fc005f":"code","447eb540":"code","c4d91ba7":"code","3b46238e":"code","915cc406":"code","32103416":"code","876312d6":"code","4e3e5b45":"code","fe7da115":"code","3f94dc58":"code","222e2245":"code","2ec984da":"code","e8addcd4":"code","0c2008b4":"code","9fb4d5e8":"markdown","dbc3c04b":"markdown","0cead7e7":"markdown","8962f0ba":"markdown","032f765e":"markdown","3473bc55":"markdown","de7ed4b1":"markdown","a3fc802f":"markdown","ffd4f8cc":"markdown","cba68456":"markdown","08be28d7":"markdown","5e5978a3":"markdown","3e85c986":"markdown","a39591a9":"markdown","fc431ec2":"markdown","18da1ac5":"markdown","ed16cd56":"markdown","821c15a1":"markdown","fe04c76b":"markdown","f3c66119":"markdown","bd2b2077":"markdown","21b3e085":"markdown","fc722698":"markdown","ef9a637b":"markdown","70e97a8e":"markdown","af2a8d52":"markdown","f829e4ca":"markdown","484d1172":"markdown","9c0443fe":"markdown","7b805e87":"markdown","0456c971":"markdown","2917b43f":"markdown","0e536407":"markdown","46ed7f75":"markdown","021d80bc":"markdown","a003c11e":"markdown","d3042366":"markdown","05f3b399":"markdown","0681238a":"markdown","f0029546":"markdown","cc432d7e":"markdown","72b38a54":"markdown","893ff87f":"markdown","a3c7b47a":"markdown","62c2b760":"markdown","94023e5d":"markdown","a71dead1":"markdown"},"source":{"b89e1b0d":"## Libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom category_encoders.one_hot import OneHotEncoder\nimport imblearn \nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import classification_report,confusion_matrix,roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import RidgeClassifier, LinearRegression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom xgboost.sklearn import XGBClassifier\nimport shap\n","9fa849da":"income_df = pd.read_csv(\"..\/input\/adult-census-income\/adult.csv\")\nprint('Full data shape is {}'.format(income_df.shape))\nincome_df.head()","2c68df47":"# Before dividing the data set between train and test. We will do some quick cleaning\n\n# # Deleting duplicates \nincome_df.duplicated().sum()\n\n#Replacing '?' for nans\nincome_df = income_df.replace('?', np.NaN)\n\n\n# Mapping already our target variable in  1 and 0.\nincome_df.income = income_df.income.map({'<=50K':0, '>50K':1}) ## mapping already our target variable in  1 and 0.\n","d47f6e7d":"income_df.head()","4ee4a57b":"train_data, test_data = train_test_split(income_df,\n                                         test_size=0.2,\n                                         stratify=income_df['income'],\n                                         random_state=5)\nprint('Train data shape is {}'.format(train_data.shape))\nprint('Test data shape is {}'.format(test_data.shape))\n","0b5d2ac0":"#In order for LabelEncoder to work we need to drop all nans\ncorr_data = train_data.dropna()\ncategorical_data =corr_data.select_dtypes(include=['object'])\nnumerical_data = corr_data.select_dtypes(exclude=['object'])\nencoder = LabelEncoder()\ncategorical_data = categorical_data.apply(encoder.fit_transform)\n### concatenating numerical and categorical data which was enconded\ncorr_data = pd.concat([numerical_data, categorical_data], axis=1)\n\nfig = plt.figure(figsize=(15,7))\nax1 = fig.add_subplot(111)\n\n#Correlation Matrix with  full data\ncorrelations = corr_data.corr()\nmatrix = np.triu(correlations.corr())\nsns.heatmap(correlations, annot= True,  mask=matrix, cmap='coolwarm', ax=ax1)","129207d2":"fig = plt.figure(figsize=((7,7)))\nax2 = fig.add_subplot(111)\n#Correlation Matix with Target Variable\nsns.heatmap(corr_data.corr()[['income']].sort_values('income').tail(10),vmin=0, cmap='Blues', annot=True, ax=ax2)\nax2.invert_yaxis()","4bdc1e92":"cat_cols = ['workclass', 'education', 'marital.status', 'occupation',\n       'relationship', 'race', 'sex', 'native.country']\ntrain_data[cat_cols].describe()","3a8acbe4":"num_cols =['age', 'education.num', 'capital.gain', 'capital.loss']\nMean =train_data[num_cols].mean()\nStandard_deviation = train_data[num_cols].std()\nresult = pd.DataFrame({'Standard_deviation': Standard_deviation, 'Mean': Mean})\nresult","e91fcbfa":"train_data.income.value_counts()\/len(train_data)","ce232cdf":"fig = plt.figure(figsize=(20,4))\nax1 = fig.add_subplot(111)\n\ndata_over50k=train_data[train_data['income']==1]\ndata_less50k= train_data[train_data['income']==0]\nsns.kdeplot(data_less50k['age'], label = '<=50K', shade=True, color='#ADC6DF', ax=ax1)\nsns.kdeplot(data_over50k['age'], label = '>50K', shade=True, color='#9B73B5', ax=ax1)\n\n#Removing lines from the graph\nax1.spines['right'].set_visible(False)\nax1.spines['top'].set_visible(False)\nax1.spines['left'].set_visible(False)\nax1.spines['bottom'].set_visible(False)\n\n#Title\nax1.set_title(\"Income by Age\", loc='center',fontweight='bold',fontsize=14 )\nax1.set_xlabel(' ')\nax1.set_ylabel(' ')\n#X-Axis\nplt.xticks(np.arange(10,100,10))\n\n#Legend\nline_labels = [\"<=50K\", \">50K\"]\nax1.legend(\n    loc=\"upper right\",\n    labels=line_labels)   ","bf608767":"fig = plt.figure(figsize=(20,4))\nax1 = fig.add_subplot(111)\nsns.countplot(x='age', hue='income', data= train_data, palette=['#ADC6DF','#9B73B5'])\n#Removing lines from the graph\nax1.spines['right'].set_visible(False)\nax1.spines['top'].set_visible(False)\nax1.spines['left'].set_visible(False)\nax1.spines['bottom'].set_visible(False)\n\n#Title\nax1.set_title(\"Income by Age\", loc='center',fontweight='bold',fontsize=14)\nax1.set_xlabel(\" \")\nax1.set_ylabel(' ')\n\n#Legend\nline_labels = [\"<=50K\", \">50K\"]\nax1.legend(\n    loc=\"upper right\",\n    labels=line_labels)   ","05e5d43d":"fig = plt.figure(figsize=(15,15))\nax1= fig.add_subplot(411)\nax2= fig.add_subplot(412)\nax3= fig.add_subplot(413)\nax4= fig.add_subplot(414)\n\ndata_workclass = round(pd.crosstab(train_data.workclass, train_data.income).div(pd.crosstab(train_data.workclass, train_data.income).apply(sum,1),0),2)\ndata_occupation = round(pd.crosstab(train_data.occupation, train_data.income).div(pd.crosstab(train_data.occupation, train_data.income).apply(sum,1),0),2)\n\n## Setting space between both subplots\nplt.subplots_adjust(left=None,\n                    bottom=None, \n                    right=None, \n                    top=1, \n                    wspace=None, \n                    hspace=0.5)\n\n## Grapphing\nsns.countplot(x='workclass', hue='income', data= train_data, ax=ax1, palette=['#ADC6DF','#9B73B5'])\ndata_workclass.plot.bar(color=['#ADC6DF','#9B73B5'], ax=ax2, edgecolor='w',linewidth=1.3)\n\nsns.countplot(x='occupation', hue='income', data= train_data, ax=ax3, palette=['#ADC6DF','#9B73B5'])\ndata_occupation.plot.bar(color=['#ADC6DF','#9B73B5'], ax=ax4, edgecolor='w',linewidth=1.3 )\n\n## Removing lines from the graph\nax1.spines['right'].set_visible(False)\nax1.spines['top'].set_visible(False)\nax1.spines['left'].set_visible(False)\nax1.spines['bottom'].set_visible(False)\nax2.spines['right'].set_visible(False)\nax2.spines['top'].set_visible(False)\nax2.spines['left'].set_visible(False)\nax2.spines['bottom'].set_visible(False)\nax3.spines['right'].set_visible(False)\nax3.spines['top'].set_visible(False)\nax3.spines['left'].set_visible(False)\nax3.spines['bottom'].set_visible(False)\nax4.spines['right'].set_visible(False)\nax4.spines['top'].set_visible(False)\nax4.spines['left'].set_visible(False)\nax4.spines['bottom'].set_visible(False)\n\n\n## Removing subplots legends\nax1.get_legend().remove()\nax2.get_legend().remove()\nax1.set_xticklabels(ax1.get_xticklabels(), rotation=30, ha='right')\nax2.set_xticklabels(ax2.get_xticklabels(), rotation=30, ha='right')\nax3.set_xticklabels(ax3.get_xticklabels(), rotation=30, ha='right')\nax4.set_xticklabels(ax4.get_xticklabels(), rotation=30, ha='right')\nax3.get_legend().remove()\nax4.get_legend().remove()\n\n\n\n## Title\nax1.set_title(\"Workclass\", loc='center',fontweight='bold',fontsize=14)\nax3.set_title(\"Occupation\", loc='center',fontweight='bold',fontsize=14)\nax2.set_title(\"Ratio of Workclass\", loc='center',fontweight='bold',fontsize=14)\nax4.set_title(\"Ratio of Occupation\", loc='center',fontweight='bold',fontsize=14)\nax1.set_xlabel(\" \")\nax1.set_ylabel(' ')\nax2.set_xlabel(\" \")\nax2.set_ylabel(' ')\nax3.set_xlabel(\" \")\nax3.set_ylabel(' ')\nax4.set_xlabel(\" \")\nax4.set_ylabel(' ')\n\n\n## Legend\nline_labels = [\"<=50K\", \">50K\"]\nfig.legend(\n    loc=\"upper right\",\n    labels=line_labels) \n","c742aa49":"fig = plt.figure(figsize=(15,7.5))\nax1= fig.add_subplot(211)\nax2= fig.add_subplot(212)\ndata_education = round(pd.crosstab(train_data.education, train_data.income).div(pd.crosstab(train_data.education, train_data.income).apply(sum,1),0),2)\n## Setting space between both subplots\nplt.subplots_adjust(left=None,\n                    bottom=None, \n                    right=None, \n                    top=1, \n                    wspace=None, \n                    hspace=0.5)\n\n## Grapphing\nsns.countplot(x='education', hue='income', data= train_data, ax=ax1, palette=['#ADC6DF','#9B73B5'],order=train_data.education.value_counts().index)\ndata_education.sort_values(by=[0], ascending=False).plot.bar(color=['#ADC6DF','#9B73B5'], ax=ax2, edgecolor='w',linewidth=1.3)\n\n## Removing lines from the graph\nax1.spines['right'].set_visible(False)\nax1.spines['top'].set_visible(False)\nax1.spines['left'].set_visible(False)\nax1.spines['bottom'].set_visible(False)\nax2.spines['right'].set_visible(False)\nax2.spines['top'].set_visible(False)\nax2.spines['left'].set_visible(False)\nax2.spines['bottom'].set_visible(False)\n\n## Removing subplots legends\nax1.get_legend().remove()\nax2.get_legend().remove()\nax1.set_xticklabels(ax1.get_xticklabels(), rotation=30, ha='right')\nax2.set_xticklabels(ax2.get_xticklabels(), rotation=30, ha='right')\n\n## Title\nax1.set_title(\"Education\", loc='center',fontweight='bold',fontsize=14)\nax2.set_title(\"Ratio of Education\", loc='center',fontweight='bold',fontsize=14)\nax1.set_xlabel(\" \")\nax1.set_ylabel(' ')\nax2.set_xlabel(\" \")\nax2.set_ylabel(' ')\n\n## Legend\nline_labels = [\"<=50K\", \">50K\"]\nfig.legend(\n    loc=\"upper right\",\n    labels=line_labels) \n","2a5d4ef5":"fig = plt.figure(figsize=(20,10))\nax1= fig.add_subplot(221)\nax2= fig.add_subplot(222)\n\ndata_race = round(pd.crosstab(train_data.race, train_data.income).div(pd.crosstab(train_data.race, train_data.income).apply(sum,1),0),2)\n\n## Grapphing\nsns.countplot(x='race', hue='income', data= train_data, ax=ax1, palette=['#ADC6DF','#9B73B5'])\ndata_race.sort_values(by=[0], ascending=False).plot.bar(color=['#ADC6DF','#9B73B5'], ax=ax2, edgecolor='w',linewidth=1.3 )\n\n#Removing lines from the graph\nax1.spines['right'].set_visible(False)\nax1.spines['top'].set_visible(False)\nax1.spines['left'].set_visible(False)\nax1.spines['bottom'].set_visible(False)\nax2.spines['right'].set_visible(False)\nax2.spines['top'].set_visible(False)\nax2.spines['left'].set_visible(False)\nax2.spines['bottom'].set_visible(False)\n\nax1.set_xticklabels(ax1.get_xticklabels(), rotation=40, ha='right')\nax2.set_xticklabels(ax2.get_xticklabels(), rotation=40, ha='right')\n\n## Title\nax1.set_title(\"Race\", loc='center',fontweight='bold',fontsize=14)\nax2.set_title(\"Ratio of Race\", loc='center',fontweight='bold',fontsize=14)\nax1.set_xlabel(\" \")\nax1.set_ylabel(' ')\nax2.set_xlabel(\" \")\nax2.set_ylabel(' ')\n\n#Removing subplots legends\nax1.get_legend().remove()\nax2.get_legend().remove()\n\n#Legend\nline_labels = [\"<=50K\", \">50K\"]\nfig.legend(\n    loc=\"upper right\",\n    labels=line_labels)","a645cd62":"fig = plt.figure(figsize=(20,9))\nax1= fig.add_subplot(221)\nax2= fig.add_subplot(222)\n\ndata_gender = round(pd.crosstab(train_data.sex, train_data.income).div(pd.crosstab(train_data.sex, train_data.income).apply(sum,1),0),2)\n\n## Graphing\nsns.countplot(x='sex', hue='income', data= train_data, ax=ax1, palette=['#ADC6DF','#9B73B5'])\ndata_gender.sort_values(by=[0], ascending=False).plot.bar(color=['#ADC6DF','#9B73B5'], ax=ax2, edgecolor='w',linewidth=1.3 )\n\n#Removing lines from the graph\nax1.spines['right'].set_visible(False)\nax1.spines['top'].set_visible(False)\nax1.spines['left'].set_visible(False)\nax1.spines['bottom'].set_visible(False)\nax2.spines['right'].set_visible(False)\nax2.spines['top'].set_visible(False)\nax2.spines['left'].set_visible(False)\nax2.spines['bottom'].set_visible(False)\n\nax1.set_xticklabels(ax1.get_xticklabels(), rotation=40, ha='right')\nax2.set_xticklabels(ax2.get_xticklabels(), rotation=40, ha='right')\n\n#Removing subplots legends\nax1.get_legend().remove()\nax2.get_legend().remove()\n\n\n## Title\nax1.set_title(\"Gender\", loc='center',fontweight='bold',fontsize=14)\nax2.set_title(\"Ratio of Gender\", loc='center',fontweight='bold',fontsize=14)\nax1.set_xlabel(\" \")\nax1.set_ylabel(' ')\nax2.set_xlabel(\" \")\nax2.set_ylabel(' ')\n\n#Legend\nline_labels = [\"<=50K\", \">50K\"]\nfig.legend(\n    loc=\"upper right\",\n    labels=line_labels)\n","0acd63a3":"train_data.drop_duplicates(inplace=True)\ntrain_data.duplicated().sum()","d94da45f":"train_data.duplicated().sum()","52803b13":"train_data.isnull().mean().sum()","16d3067d":"cols_to_drop = ['fnlwgt', 'education']\ntrain_data.drop(cols_to_drop, inplace=True, axis=1)","d99cded0":"X = train_data.drop('income', axis=1)\ny = train_data['income']\n\n#Transforming categorical columns\ncategorical_columns= X.select_dtypes(object).columns\n# Using pipeline\nsteps = [('encoding', OneHotEncoder(cols=categorical_columns)),('imputer',SimpleImputer(missing_values=np.NaN, strategy='most_frequent')),('model',RandomForestClassifier())]\npipeline = Pipeline(steps=steps)\nstratified_kfold = StratifiedKFold(n_splits = 5, random_state=5, shuffle=True)\n\nscores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=stratified_kfold, n_jobs=3)\n\nprint(f'Random Forest AUC imputing missing values: {round(mean(scores),3)}')","35ca80d3":"train_data_without_na = train_data.dropna()\n\nX = train_data_without_na.drop('income', axis=1)\ny = train_data_without_na['income']\n#Transformin categorical columns\ncategorical_columns= X.select_dtypes(object).columns\n# Using pipeline\nsteps = [('encoding', OneHotEncoder(cols=categorical_columns)),('model',XGBClassifier())]\npipeline = Pipeline(steps=steps)\n\nstratified_kfold = StratifiedKFold(n_splits = 5, random_state=5, shuffle=True)\n\nscores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=stratified_kfold, n_jobs=3)\nprint(f'XGBClassifier AUC dropping missing values: {round(mean(scores),3)}')\n","b87d2b16":"train_data_without_na = train_data.dropna()\n\nX = train_data_without_na.drop('income', axis=1)\ny = train_data_without_na['income']\n#Transformin categorical columns\ncategorical_columns= X.select_dtypes(object).columns\n# Using pipeline\nsteps = [('encoding', OneHotEncoder(cols=categorical_columns)),('model',GradientBoostingClassifier())]\npipeline = Pipeline(steps=steps)\n\nstratified_kfold = StratifiedKFold(n_splits = 5, random_state=5, shuffle=True)\n\nscores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=stratified_kfold, n_jobs=-1)\nprint(f'Gradient Boosting AUC dropping missing values: {round(mean(scores),3)}')\n","abfedc67":"train_data_without_na = train_data.dropna()\n\nX = train_data_without_na.drop('income', axis=1)\ny = train_data_without_na['income']\n#Transformin categorical columns\ncategorical_columns= X.select_dtypes(object).columns\n# Using pipeline\nsteps = [('encoding', OneHotEncoder(cols=categorical_columns)),('model',LogisticRegression())]\npipeline = Pipeline(steps=steps)\n\nstratified_kfold = StratifiedKFold(n_splits = 5, random_state=5, shuffle=True)\n\nscores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=stratified_kfold, n_jobs=3)\nprint(f'Logistic regression AUC dropping missing values: {round(mean(scores),3)}')\n","bf7d5148":"X = train_data.drop('income', axis=1)\ny = train_data['income']\n\n#Transforming categorical columns\ncategorical_columns= X.select_dtypes(object).columns\n# Using pipeline\nsteps = [('encoding', OneHotEncoder(cols=categorical_columns)),('imputer',SimpleImputer(missing_values=np.NaN, strategy='most_frequent')),('model',RandomForestClassifier())]\npipeline = Pipeline(steps=steps)\nstratified_kfold = StratifiedKFold(n_splits = 5, random_state=5, shuffle=True)\n\nscores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=stratified_kfold, n_jobs=3)\n\nprint(f'Random Forest AUC imputing missing values: {round(mean(scores),3)}')","95fd84ab":"X = train_data.drop('income', axis=1)\ny = train_data['income']\n\n#Transforming categorical columns\ncategorical_columns= X.select_dtypes(object).columns\n# Using pipeline\nsteps = [('encoding', OneHotEncoder(cols=categorical_columns)),('imputer',SimpleImputer(missing_values=np.NaN, strategy='most_frequent')),('model',XGBClassifier())]\npipeline = Pipeline(steps=steps)\nstratified_kfold = StratifiedKFold(n_splits = 5, random_state=5, shuffle=True)\n\nscores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=stratified_kfold, n_jobs=3)\n\nprint(f'XGBClassifier AUC imputing missing values: {round(mean(scores),3)}')","5c8acfbb":"X = train_data.drop('income', axis=1)\ny = train_data['income']\n\n#Transforming categorical columns\ncategorical_columns= X.select_dtypes(object).columns\n# Using pipeline\nsteps = [('encoding', OneHotEncoder(cols=categorical_columns)),('imputer',SimpleImputer(missing_values=np.NaN, strategy='most_frequent')),('model',GradientBoostingClassifier())]\npipeline = Pipeline(steps=steps)\nstratified_kfold = StratifiedKFold(n_splits = 5, random_state=5, shuffle=True)\n\nscores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=stratified_kfold, n_jobs=3)\n\nprint(f'Gradient Boosting  AUC imputing missing values: {round(mean(scores),3)}')","234017ba":"X = train_data.drop('income', axis=1)\ny = train_data['income']\n\n#Transforming categorical columns\ncategorical_columns= X.select_dtypes(object).columns\n# Using pipeline\nsteps = [('encoding', OneHotEncoder(cols=categorical_columns)),('imputer',SimpleImputer(missing_values=np.NaN, strategy='most_frequent')),('model',LogisticRegression())]\npipeline = Pipeline(steps=steps)\nstratified_kfold = StratifiedKFold(n_splits = 5, random_state=5, shuffle=True)\n\nscores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=stratified_kfold, n_jobs=3)\n\nprint(f'Logistic Regression AUC imputing missing values: {round(mean(scores),3)}')","770462ad":"X = train_data.drop('income', axis=1)\ny = train_data['income']\n\n#Transforming categorical columns\ncategorical_columns= X.select_dtypes(object).columns\n# Using pipeline\nsteps = [('encoding', OneHotEncoder(cols=categorical_columns)),('imputer',SimpleImputer(missing_values=np.NaN, strategy='most_frequent')),('over', SMOTE()),('model',XGBClassifier())]\npipeline = Pipeline(steps=steps)\nstratified_kfold = StratifiedKFold(n_splits = 5, random_state=5)\n\nscores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=stratified_kfold, n_jobs=3)\n\nprint(f'XGBClassifier AUC imputing missing values + SMOTE: {round(mean(scores),3)}')","c7697fe0":"X = train_data.drop('income', axis=1)\ny = train_data['income']\n\n#Transforming categorical columns\ncategorical_columns= X.select_dtypes(object).columns\n# Using pipeline\nsteps = [('encoding', OneHotEncoder(cols=categorical_columns)),('imputer',SimpleImputer(missing_values=np.NaN, strategy='most_frequent')),('over', SMOTE()),('model',GradientBoostingClassifier())]\npipeline = Pipeline(steps=steps)\nstratified_kfold = StratifiedKFold(n_splits = 5, random_state=5)\n\nscores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=stratified_kfold, n_jobs=3)\n\nprint(f'Gradient Boosting AUC imputing missing values + SMOTE: {round(mean(scores),3)}')","d0b36474":"X = train_data.drop('income', axis=1)\ny = train_data['income']\n\n#Transforming categorical columns\ncategorical_columns= X.select_dtypes(object).columns\n# Using pipeline\nsteps = [('encoding', OneHotEncoder(cols=categorical_columns)),('imputer',SimpleImputer(missing_values=np.NaN, strategy='most_frequent')),('under', RandomUnderSampler(random_state=5)),('model',XGBClassifier())]\npipeline = Pipeline(steps=steps)\nstratified_kfold = StratifiedKFold(n_splits = 5, random_state=5)\n\nscores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=stratified_kfold, n_jobs=3)\n\nprint(f'XGBClassifier AUC imputing missing values + UNDER: {round(mean(scores),3)}')","c4582e28":"X = train_data.drop('income', axis=1)\ny = train_data['income']\n\n#Transforming categorical columns\ncategorical_columns= X.select_dtypes(object).columns\n# Using pipeline\nsteps = [('encoding', OneHotEncoder(cols=categorical_columns)),('imputer',SimpleImputer(missing_values=np.NaN, strategy='most_frequent')),('under', RandomUnderSampler(random_state=5)),('model',GradientBoostingClassifier())]\npipeline = Pipeline(steps=steps)\nstratified_kfold = StratifiedKFold(n_splits = 5, random_state=5)\n\nscores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=stratified_kfold, n_jobs=3)\n\nprint(f'Gradient Boosting AUC imputing missing values + UNDER: {round(mean(scores),3)}')","35fc005f":"X = train_data.drop('income', axis=1)\ny = train_data['income']\n\n#Transforming categorical columns\ncategorical_columns= X.select_dtypes(object).columns\n# Using pipeline\nsteps = [('encoding', OneHotEncoder(cols=categorical_columns)),\n         ('imputer',SimpleImputer(missing_values=np.NaN, strategy='most_frequent')),\n         ('model',XGBClassifier())]\npipeline = Pipeline(steps=steps)\nstratified_kfold = StratifiedKFold(n_splits = 5, random_state=5)\n\nparam_grid = {\n    'model__n_estimators': [500],\n    'model__learning_rate': [0.1],\n    'model__max_depth': [3]\n\n}\n\n\ngrid = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=stratified_kfold.split(X, y),\n                    scoring='roc_auc',  n_jobs=5, verbose=True)\n","447eb540":"grid.fit(X,y) ","c4d91ba7":"AUC = grid.best_score_\nbest_param = grid.best_params_\nprint(f'XGBClassifier AUC: {round(AUC,4)}')\nprint(f'The best Parameter for XGBClassifier is: {best_param}')","3b46238e":"full_model = grid.best_estimator_\nfull_model","915cc406":"encoded_variables = full_model['encoding'].transform(X)\nencoded_variables.head()\n","32103416":"explainer = shap.TreeExplainer(full_model['model'])\n\nshap_values = explainer.shap_values(encoded_variables)\nshap.summary_plot(shap_values, encoded_variables)","876312d6":"shap.summary_plot(shap_values, encoded_variables, plot_type='bar')","4e3e5b45":"test_data.shape","fe7da115":"test_data.duplicated().sum()","3f94dc58":"cols_to_drop = ['fnlwgt', 'education']\ntest_data.drop(cols_to_drop, inplace=True, axis=1)","222e2245":"test_data.head()","2ec984da":"X_test = test_data.drop('income', axis=1)\ny_test = test_data['income']","e8addcd4":"predictions = full_model.predict(X_test)\nprint(classification_report(y_test,predictions))","0c2008b4":"predicted_probabilities = full_model.predict_proba(X_test)\n\nprint(f'Test AUC score is : {roc_auc_score(y_test,predicted_probabilities[:,1])}')","9fb4d5e8":"We can see that our sample is monstly White followed by Black race. One interesting thing we can see is that Asian-Pac-Islander have the highest ratio of >50K, followed by White, although they are very similar in terms of the ratio betwee our two classes. ","dbc3c04b":"### Age vs income","0cead7e7":"<a><\/a>\n\n## Table of Contents\n\n\n[1. Abstract](#1)\n\n[2. Division of the data between train and test](#2)\n\n[3. Data Exploration with target varaible](#3)\n\n[4. Data pre-processing](#4)\n\n[5. Baseline model:](#5)\n\n   -Model with missing values\n   \n   -Model imputing missing values\n   \n   -Models with SMOTE and UNDER techniques for imbalanced datasets\n\n[6. Tuning and testing](#6)\n\n[7. Shapply values - Feature importance](#7)\n\n[8. Prediction and Evaluation](#8)\n\n[9. Conclusion](#9)\n\n\n    ","8962f0ba":"Here we can see interesting things about our data:\n\n\n**Workclass**: \n    From our data Private is the largest category by number.  If we look at the ratio, self-empl-inc has almost the same amount of people who earn more than 50K and less than 50K, and is the only category where more people earn above 50K. \n    \n**Occupation**:\n\n   We can see that Adm-Clerical and Machine-op-Inspect are the most frequent jobs. In terms of income, we can say that Exec-Managerical has the highest rate of >50K, followed by Armed Forces.  The job with the lowest >50K is Priv-house-serv. \n    ","032f765e":"## 5.2 Models handling imbalanced data","3473bc55":"We can see that:\n- Marital_status_1 (which is Married-civ-spouse) impacts the result of our model on average of 1.0. This means that married civilians have higher probability to have income higher that 50K. This agrees with our Data exploration section where we mentioned that Married-civ-spouse tend to be more likely to have >50K income.\n- The second one is age. As explained above, the older one is, the more experience one has and therefore likely to have a higher salary.\n- Capital gain is the third most important feature and we deduct that having better access to capital is a key factor in improving income.\nLastly, education is the fourth most important feature. Again, this is logical, as the more education you have, you more likely you are to have a higher salary.","de7ed4b1":"### Descriptive statistics","a3fc802f":"### 5.2.1 Using Ovesampling technique","ffd4f8cc":"#### For numerical variables","cba68456":"\n#### Summary:\n\n- Imputing missing values did helped us to increase  AUC in the models. However, the increase it is not huge.\n\n- Using SMOTE and UNDER techniques did note helped our model to get better results\n\n- Best model: XGBClassifier AUC imputing missing values: 0.928","08be28d7":"####  ? Dropping missing values","5e5978a3":"#### Dropping duplicates","3e85c986":"Here we can see that HS-grad is the most frequent education followed by Some college. \nAdditionally, we can see in the second graph, ratio of education, how the number of <=50 tends to decrease as people tend to have higher education. \n\nFor example:  Doctorate. It seems that just 0.20 people who have doctorate tend to have <=50, the 0.80 tend to have more that 50K per year, which makes sense.  We observe similar ratios with prof-school and Masters.\n\nTherefore, as we have seen before, Education_num has a huge impact in the income variable. This variable has the stronges correlation with our target variable: 0.34.\n","a39591a9":"### 5.2.2 Using undersample  technique","fc431ec2":"\n# 1.  Abstract \n\n\n\n\nThis data set was provided by Ronny Kohavi and Barry Becker from the 1994 US Cenus and retrieved from: \"http:\/\/archive.ics.uci.edu\/ml\/datasets\/Adul\".\n\nThe task of this project is to classify whether a person makes more or less than 50K per year.\n\nThe dataset is composed of 14 variables (6 continuous and 8 nominal).\n    \n\nData with missing values: \n    - 0 : 76%\n    - 1: 24%\n    \nData dropping missing values: \n    - 0: 0.7510% \n    - 1: 0.2589%\n\n\n****Imbalance data set probelm**\n**","18da1ac5":"While tuning our best parameters were as follows:\n\n**The best Parameter for XGBClassifier were: {'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__n_estimators': 500}**\n\n","ed16cd56":"#### Dropping unncessary columns ","821c15a1":"## 5.1 Baseline model dropping missing values","fe04c76b":"We can observe that the average age is around 38.6 and education_num is 10.07 (which equals to some college).\n\nWe can see that capital_gain and capital_loss standard_deviation is higher than mean. This shows that people vary greatly in terms of capital gain and loss. \n","f3c66119":"### Race vs Income","bd2b2077":"## 5. Baseline model","21b3e085":"Another problem we have encounter was our data was imbalanced: \n       - 0    0.76%\n       - 1    0.24%\nHaving an imbalanced data set can create model performance with classification problems. Our class is not very severe therefore we will still try to use: \n\n\n- SMOTE (oversampling): creating more samples for the minority class.\n- UNDER : decreasing the majority class.\n\nIn both cases we can see that the models using SMOTE techinique the models performed worse than in UNDER technique. However, still, our data set was not that severly imbalanced and we can see that our model performed better without using any technique. \n","fc722698":"# 6. Tuning and testing","ef9a637b":"## 7. Shapply values - Feature importance","70e97a8e":"We are missing 13% of our data. We will create a baseline and compare if dropping these instances will drastically change our model performance.","af2a8d52":"### Gender vs Income","f829e4ca":" \n# 3. Data Exploration with target varaible","484d1172":"#### Correlation with the target variable","9c0443fe":"##### For categorical variables","7b805e87":"We can observe that our target variable is imbalanced ( <= 50K:76%, > 50k: 24%). Therefore, we will address this problem later with some techniques for imbalanced classification.","0456c971":"\n## 3.1 Data visualization with target variable","2917b43f":"# 4. Data pre-processing","0e536407":"Since our missing values are categorical variables:\n- workclass  \n- occupation    \n- native_country      \n\nWe will use simple imputer with strategy ='most_frequent' (mode).","46ed7f75":"# 9. Conclusion","021d80bc":"### Education vs Income","a003c11e":"## 5.2 Baseline imputing mising values","d3042366":"We can observe that we do not have highly correlated variables. Only 0.58 sex and relationship, followed by education and education_num 0.35. \n\nAdditionally, we can observe the above correlation matrix graph where we can see the correlation with our target variable.\n- Education_num is positively correlated with income (0.34). This is reasonable since the more education you have, you more likely you are to have a higher salary. \n- Age is another variable  that is positively correlated with income (0.23). However, this variable is parabolic and can bee seen in the below visualiztion that there is a point in the age where income will start decreasing. \n- Hours per week is another variable postively correlated with income (0.23). The more hours you work, the higher your salary will tend to be. ","05f3b399":"We will drop the following columns\n\n- Final weight: this column represents the number of people the census believes the entry represents. Therefore, we will drop this column since it does not give any meaningful information to our model.\n- Education: We already have an Education_num column which presents this information as an ordinal variable. We will drop it to avoid redundant information.","0681238a":"#### Correlation Matrix","f0029546":"Our ROC_AUC is almost the same as in our train set: 0.928 versus 0.926.\n\nWe can conclude that our model is not overfitted and we are very happy with the results.","cc432d7e":"#### Dropping unnecesary columns","72b38a54":"**Please give it a vote if you like it**","893ff87f":"# 8. Prediction and Evaluation","a3c7b47a":"We can see that younger people are more likely to earn less than 50K per year. Which is obvious since those people do not have any work experience. The rates of people earning more than 50K start to increase as people age into their 30s. \n\nThis is also seen in the purple line(>50K). The line starts to increase from 20  to 45 years old and then slowly decreasing.","62c2b760":"\n# 2. Division of the data between train and test.","94023e5d":"We can observe that male gender is the most frequent and also we observe that there are some disproportions distribuitions regarding income.\nClose to 10% of females earn more that 50.000 dollars compared to Male gender, where almost 40 percent earn more than 50.000 dollars.\n","a71dead1":"### Workclass and occupation vs Income"}}