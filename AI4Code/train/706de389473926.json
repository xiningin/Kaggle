{"cell_type":{"735f2f47":"code","49e47ceb":"code","65799018":"code","a3c9a57f":"code","80357988":"code","d4ea9a2c":"code","00a3d64b":"code","6064a9d3":"code","75180cd6":"markdown","2251de66":"markdown","45d19fb3":"markdown","8f6df2e7":"markdown"},"source":{"735f2f47":"import numpy as np \nimport pandas as pd ","49e47ceb":"train_data = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/train.csv')\ntest_data  = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/test.csv')\n\nprint('Train: ', train_data.shape)\nprint('Test: ', test_data.shape)","65799018":"y = train_data['target']\nX = train_data.drop(columns=['target', 'id'])\nX_test = test_data.drop(columns='id')","a3c9a57f":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.05, random_state=22)","80357988":"# from xgboost import XGBRegressor\n\n# xgb = XGBRegressor(random_state=22\n#                   , n_estimators=3000\n#                   , early_stopping_rounds=10\n#                   , learning_rate=0.05\n#                   , subsample=0.9\n#                   , colsample_bytree=0.9\n#                   , n_jobs=-1)\n\n# xgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=1)\n\n# predictions = xgb.predict(X_test)","d4ea9a2c":"# thanks to:\n# https:\/\/www.kaggle.com\/hamditarek\/tabular-playground-series-xgboost-lightgbm\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error\nfrom lightgbm import LGBMRegressor\n\nparams = {'objective': 'regression',\n 'metric': 'rmse',\n 'verbosity': -1,\n 'boosting_type': 'gbdt',\n 'feature_pre_filter': False,\n 'learning_rate': 0.007,\n 'num_leaves': 102,\n 'min_child_samples': 20,\n 'sub_feature': 0.4,\n 'sub_row': 1,\n 'subsample_freq': 0,\n 'lambda_l1': 4.6,\n 'lambda_l2': 1.9}\n\n\nN_FOLDS = 10\n\nkf = KFold(n_splits = N_FOLDS)\noof = np.zeros(len(y))\noof_vanilla = np.zeros(len(y))\npreds = np.zeros(len(X_test))\nparams['learning_rate'] = 0.005\nparams['num_iterations'] = 5000\n\nfor train_ind, test_ind in tqdm(kf.split(X)):\n    Xtrain = X.iloc[train_ind]\n    Xval = X.iloc[test_ind]\n    ytrain = y.iloc[train_ind]\n    yval = y.iloc[test_ind]\n\n    model = LGBMRegressor(**params)\n    vanilla_model = LGBMRegressor()\n    \n    model.fit(Xtrain, ytrain, eval_set = ((Xval,yval)), early_stopping_rounds = 50, verbose = 0)\n    vanilla_model.fit(Xtrain, ytrain)\n    p = model.predict(Xval)\n    p_vanilla = vanilla_model.predict(Xval)\n    oof[test_ind] = p\n    oof_vanilla[test_ind] = p_vanilla\n    \n    preds += model.predict(X_test)\/N_FOLDS\n    \nprint(f'mean square error on training data (vanilla model): {np.round(mean_squared_error(y, oof_vanilla, squared=False),5)}')    \nprint(f'mean square error on training data (with tuning): {np.round(mean_squared_error(y, oof, squared=False),5)}')","00a3d64b":"output = pd.DataFrame({\"id\":test_data.id, \"target\":preds})\noutput.to_csv('submission.csv', index=False)","6064a9d3":"print('Finish!')","75180cd6":"## Creating submission file","2251de66":"## XGB regressor","45d19fb3":"## Part 1 - ordinary XGB ;)","8f6df2e7":"## Part 2 - CV and lightGBM"}}