{"cell_type":{"e139c2b6":"code","e0ed28ca":"code","93dcf589":"code","585a8174":"code","984715c2":"code","f71fb5ac":"code","980c0411":"code","c60b25c5":"code","cf2c5140":"code","1458204b":"code","12e3b15f":"code","90e4db56":"code","695aa29e":"code","67a7a8fe":"code","838abf0e":"code","900cc86f":"code","f84d396a":"code","3418cab7":"code","9536e3e7":"code","2956419c":"code","33bbcf94":"code","1b690280":"code","9d112c3d":"code","229e667e":"code","f618c30e":"code","1a0b2fa8":"code","f0c2def6":"code","fb5c1371":"code","3320d742":"code","bdbd5016":"code","2caec0d4":"code","071630c6":"code","0255431a":"code","766ff85c":"code","5c49d60d":"code","b4a5aa69":"code","b504b61f":"code","c4dbcfc1":"code","ca1d2ffc":"code","117dd2dd":"code","f95ad2fa":"code","00ec5a09":"code","978cc61b":"code","c13061c2":"code","79f3fc32":"code","67044f8e":"code","71bd2f93":"code","34584b0a":"code","e355e4f9":"code","845e0ad8":"code","27d43d36":"code","34a50fce":"code","c6755c68":"code","e748f3d2":"code","608ce044":"code","4da52b6b":"code","7b2a4210":"code","a06c09c0":"code","eedfc260":"code","018077dd":"code","84d6525c":"code","0fc90d17":"code","4a4705c4":"code","62366147":"code","8cbfbac5":"code","33a383d7":"code","78d90854":"code","33fe6d3a":"code","2cd99ff2":"code","69812f4d":"code","85fc7d7c":"code","f5447b1d":"code","b01e7546":"code","d254eaa6":"code","4da870c3":"code","3ed6eafe":"code","b2272eb0":"code","2bdfa75c":"code","1885b42f":"code","9b72ff8b":"code","73a129da":"code","c0c99bf2":"code","aaaa1549":"code","e1e98628":"code","38845a14":"code","92e02ca4":"code","06420af2":"code","5228d3a7":"code","98257606":"code","9f7e4486":"code","1ce97601":"code","f2226b67":"code","a2a2b87a":"code","d9856918":"code","963652a5":"code","f2cf3cb2":"code","fd4c58e8":"code","13303ceb":"code","000651c2":"code","472e0d08":"code","5f7e18a3":"code","43120c0e":"code","b4e41aa2":"code","c797020d":"code","e035b15a":"code","e37ff28b":"code","c7687101":"code","637496fb":"code","85c63360":"code","dee7975b":"code","061cf755":"code","0101019a":"code","d076bc24":"code","f7a210ac":"code","a30f757f":"code","ee8e06fa":"code","9ebd76ad":"code","b103dbcb":"code","0ee94bf3":"code","993f5861":"code","9665305b":"code","09b97372":"code","de9c64db":"code","e113e59a":"code","e8ac44bb":"code","935dafdc":"code","1974e052":"code","3652aa0f":"code","e630737b":"code","8fb54f89":"code","cc2064ca":"code","69d36816":"code","b931aaa4":"code","b6f2254d":"code","bdaaa83f":"code","e18ea6bc":"code","d02c62a3":"code","e3c7ce45":"code","a5e7b015":"code","60f51892":"code","accfda24":"code","8df6020f":"code","4a583adf":"code","7688e8cc":"code","3549f0b3":"code","f393dffc":"code","b54cac48":"code","631c4a94":"code","48ac4bc2":"code","a2c73455":"code","c5098b08":"code","9776087f":"code","959b2edb":"code","9892b78e":"code","23071a09":"code","0f38ff02":"code","8be66c23":"code","2cb9e7ea":"code","924ef899":"code","f1a399b4":"code","3af1f514":"code","e6429bf1":"code","feb000b5":"code","d63e38e7":"code","9c54e1ab":"code","7bd7cb7f":"code","c4a49054":"code","d8b9483e":"code","93ea74bc":"code","d56ac7bb":"code","1efea973":"code","1fe43269":"code","d887b629":"code","abbce625":"code","58b6b2cd":"code","d9a6cce0":"markdown","cafea790":"markdown","aa70b284":"markdown","2156dd2b":"markdown","51b2580c":"markdown","52ad38f3":"markdown","32e92fb1":"markdown","47446c7d":"markdown","fdae8c92":"markdown","74f6ff82":"markdown","43795d2f":"markdown","6569f7ef":"markdown","fcef1e79":"markdown","0b7bfb5f":"markdown","13aec3ae":"markdown","1b89ef52":"markdown","e0ebbd84":"markdown","05fb8454":"markdown","f9f3e034":"markdown","63600cb6":"markdown","1b58188e":"markdown","60f8f925":"markdown","ce921e29":"markdown","0e80ee49":"markdown","665d0f85":"markdown","1901d9f3":"markdown","f095963d":"markdown","46db22fd":"markdown","3738ff07":"markdown","ebed5ae1":"markdown","e3bf5c64":"markdown","8b276fbd":"markdown","9bc6f34b":"markdown","da6d1d29":"markdown","cb64a377":"markdown","1a89ff16":"markdown","a1b4f6ac":"markdown","08688bb6":"markdown","3a580e4b":"markdown","72b9927b":"markdown","35ea669d":"markdown","e5ac510b":"markdown","d9e62a70":"markdown","d858ba32":"markdown","a78830cc":"markdown","78fdf166":"markdown","41b14716":"markdown","0dc48b73":"markdown","57c7f1cb":"markdown","f6a3bd17":"markdown","df8a97a2":"markdown","f726442a":"markdown","625c5488":"markdown","67ad4c9c":"markdown","0131974a":"markdown","8904f0cb":"markdown","117afac5":"markdown","a1f893c8":"markdown","03a95adf":"markdown","606196fd":"markdown","e23e9437":"markdown","e9870c03":"markdown","0a357439":"markdown","f29b070d":"markdown","51041d75":"markdown","e50b82ea":"markdown","7c0a55fe":"markdown","bac51fd2":"markdown","47b2a6d0":"markdown","2bfbeffe":"markdown","e5a98dd4":"markdown","06619778":"markdown","bd7c118c":"markdown","5463fde8":"markdown","81c1f32c":"markdown","222e65fc":"markdown","b7f2190a":"markdown","94cf0f75":"markdown","6cfa23ad":"markdown","ddddaff8":"markdown","df143f58":"markdown","af8af0ad":"markdown","114c166a":"markdown"},"source":{"e139c2b6":"import numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warnings (as always)\n\n\nimport plotly.figure_factory as ff\nimport plotly.offline as py\n##for online plotting use import plotly.plotly as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\nfrom plotly import tools\nimport gc\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn import ensemble, tree, linear_model\nfrom sklearn.model_selection import train_test_split, cross_val_score","e0ed28ca":"df2 = pd.read_csv(\"..\/input\/crime-rates\/report.csv\", encoding='latin1')\ndf2","93dcf589":"ax=plt.figure(figsize=(10,22))\nax = sns.barplot(df2[\"violent_crimes\"]+df2[\"homicides\"]+\n                 df2[\"rapes\"]+df2[\"assaults\"]+df2[\"robberies\"],                 \n                 y=df2[\"agency_jurisdiction\"],estimator=sum,color=\"y\")\nax.set_title('Total number of crimes in US during in 40 years')\nax.set(xlabel='Total number of crimes', ylabel='City and state')","585a8174":"\ndf2[\"relative_crime\"]=(df2[\"violent_crimes\"]+df2[\"homicides\"]+\n                 df2[\"rapes\"]+df2[\"assaults\"]+df2[\"robberies\"])\/df2.population\n\n","984715c2":"ax=plt.figure(figsize=(10,22))\nax = sns.barplot(x=df2[\"relative_crime\"],y=df2[\"agency_jurisdiction\"],color=\"b\")\nax.set_title('Total number of crimes in US during in 40 years')\nax.set(xlabel='Total number of crimes', ylabel='City and state')","f71fb5ac":"i = df2[(df2.agency_jurisdiction == \"United States\")].index\ndf2.drop(i,inplace=True)\n","980c0411":"import plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\na = set(df2[\"agency_jurisdiction\"])\na = list(a)\n\ndoubles = dict()\nfor i in range(0,len(a)):\n    doubles[i] = df2[df2['agency_jurisdiction'].str.contains(a[i])]\n\ntrace = dict()\nfor i in range(0,len(a)):\n    trace[i] = go.Scatter(x = doubles[i]['report_year'],y=doubles[i]['violent_crimes'],name = a[i],opacity = 0.8)\n\ndata = [trace[0],trace[1],trace[2],trace[3],trace[4],trace[5],trace[6],trace[7],trace[8],trace[9],\n        trace[10],trace[11],trace[12],trace[13],trace[14],trace[15],trace[16],trace[17],trace[18],trace[19],\n         trace[20],trace[21],trace[22],trace[23],trace[24],trace[25],trace[26],trace[27],trace[28],trace[29],\n          trace[30],trace[31],trace[32],trace[33],trace[34],trace[35],trace[36],trace[37],trace[38],trace[39],\n           trace[40],trace[41],trace[42],trace[43],trace[44],trace[45],trace[46],trace[47],trace[48],trace[49],\n            trace[50],trace[51],trace[52],trace[53],trace[54],trace[55],trace[56],trace[57],trace[58],trace[59],\n             trace[60],trace[61],trace[62],trace[63],trace[64],trace[65],trace[66],trace[67]]\n\nlayout = dict(title = \"Total Crimes in US during in 40 years\",\n              xaxis = dict(title = 'Time Span'),\n              yaxis = dict(title = 'Cumulative crimes'),)\n\nfig = dict(data=data, layout=layout)\npy.iplot(fig)","c60b25c5":"NYC = df2[df2.agency_jurisdiction == 'New York City, NY']\n\nax=plt.figure(5,figsize=(18,12))\nax = plt.plot([1975, 1975], [0, 193000], 'y-', lw=2)\nax = plt.text(1975, 193000, 'Abraham Beame',color='blue',horizontalalignment='center')\nax = plt.plot([1978, 1978], [0, 193000], 'r-', lw=2)\nax = plt.text(1978, 193000, 'Ed Koch',color='blue',horizontalalignment='left')\nax = plt.plot([1990, 1990], [0, 193000], 'g-', lw=2)\nax = plt.text(1990, 193000, 'David Dinkins',color='blue',horizontalalignment='center')\nax = plt.plot([1994, 1994], [0, 193000], 'y-', lw=2)\nax = plt.text(1994, 193000, 'Rudy Giuliani',color='red',horizontalalignment='center')\nax = plt.plot([2002, 2002], [0, 193000], 'r-', lw=2)\nax = plt.text(2002, 193000, 'Michael Bloomberg',color='red',horizontalalignment='center')\nax = plt.plot([2014, 2014], [0, 193000], 'g-', lw=2)\nax = plt.text(2014, 193000, 'Bill de Blasio',color='blue',horizontalalignment='center')\nax = plt.plot(NYC[\"report_year\"],NYC[\"violent_crimes\"])\nplt.title('Crimes while mayors')\nplt.xlabel(\"Year\")\nplt.ylabel(\"Sum of all crimes\")\nplt.ylim([0,220000])\nplt.xlim([1970,2020])\nplt.show()","cf2c5140":"\nax=plt.figure(figsize=(15,10))\nplt.xticks(rotation='vertical')\nax=sns.barplot(NYC[\"report_year\"], NYC[\"homicides\"],palette=\"cubehelix\")\n\nplt.ylabel(\"Number of Homicides\")","1458204b":"ax=plt.figure(figsize=(15,10))\nplt.xticks(rotation='vertical')\nsns.barplot(NYC[\"report_year\"], NYC[\"rapes\"],palette=\"cubehelix\")\nplt.ylabel(\"Number of Rapes\")","12e3b15f":"ax=plt.figure(figsize=(15,10))\nplt.xticks(rotation='vertical')\nsns.barplot(NYC[\"report_year\"], NYC[\"assaults\"],palette=\"cubehelix\")\nplt.ylabel(\"Number of Assaults\")","90e4db56":"ax=plt.figure(figsize=(15,10))\nplt.xticks(rotation='vertical')\nsns.barplot(NYC[\"report_year\"], NYC[\"robberies\"],palette=\"cubehelix\")\nplt.ylabel(\"Number of Robberies\")","695aa29e":"df3 = pd.read_csv(\"..\/input\/unemployment-by-county-us\/output.csv\", encoding='latin1')\ndf3.head()\ndf3['County']=df3['County'].astype(str).str.split().str[0]\ndf3['County']","67a7a8fe":"df3.rename(columns={'Rate':'Unemployment rate'}, inplace=True)","838abf0e":"df3.head()","900cc86f":"df3.isna().sum().sum()","f84d396a":"df4 = pd.read_csv(\"..\/input\/rent-index\/price.csv\", encoding='latin1')\ndf4.head()","3418cab7":"df4.describe()","9536e3e7":"years = list(set([y.split( )[1] for y in df4.columns[6:]]))\nmonths = df4.columns[6:]","2956419c":"nyc_rent = df4[df4[\"Metro\"]==\"New York\"]\nny_njer_rent = df4.groupby(\"State\")[months].median()\n","33bbcf94":"trace1 = go.Scatter(x = months, \n                  y= np.nanmedian(nyc_rent[nyc_rent[\"State\"]==\"NY\"][months], axis = 0),\n                    mode='markers', marker=dict(size=5,color = ('aqua')), \n                   name = \"New York\")\ntrace2 = go.Scatter(x = months, \n                  y= np.nanmedian(nyc_rent[nyc_rent[\"State\"]==\"NJ\"][months], axis = 0),\n                    mode='markers', marker=dict(size=5,color = ('navy')),\n                   name = \"New Jersey\")\n\n\n\nfig = tools.make_subplots(rows= 1 , cols=2, subplot_titles=('Median NY Rent','Median NJ Rent'))\nfig.append_trace(trace1, 1,1)\nfig.append_trace(trace2, 1,2)\n\nfig['layout'].update(showlegend=False, title='Median Rent Price of New York VS New Jersey')\npy.iplot(fig)","1b690280":"df4.isna().sum().sum()","9d112c3d":"df4[df4.isnull().any(axis=1)].head()","229e667e":"sns.heatmap(df4.isnull(), cbar=False)","f618c30e":"df4.interpolate(method='piecewise_polynomial', inplace=True, limit_direction=\"both\")","1a0b2fa8":"df4","f0c2def6":"df4.isna().sum().sum()","fb5c1371":"df4.interpolate(method='linear', inplace=True, limit_direction=\"both\")","3320d742":"df4.drop([\"Metro\"], axis=1,inplace=True)","bdbd5016":"df4.isna().sum().sum()","2caec0d4":"df2.isna().sum().sum()","071630c6":"sns.heatmap(df2.isnull(), cbar=False)","0255431a":"df2=df2[df2[\"report_year\"]>2009]","766ff85c":"df2.drop([\"months_reported\",\"agency_code\"], axis=1,inplace=True)","5c49d60d":"df2[df2.isnull().any(axis=1)]","b4a5aa69":"df2.dropna(inplace=True)","b504b61f":"df2.isna().sum().sum()","c4dbcfc1":"df2.head()","ca1d2ffc":"df2[['City',\"State\"]] = df2[\"agency_jurisdiction\"].str.split(',',expand=True)","117dd2dd":"df2.drop([\"City\",\"agency_jurisdiction\"], axis=1,inplace=True)","f95ad2fa":"df2.head()","00ec5a09":"d = {'State': [\"Alabama\",\"Alaska\",\"Arizona\",\"Arkansas\",\"California\",\"Colorado\",\"Connecticut\",\"Delaware\",\"Florida\",\"Georgia\",\"Hawaii\",\"Idaho\",\"Illinois\", \"Indiana\",\"Iowa\",\"Kansas\",\"Kentucky\",\"Louisiana\",\"Maine\",\"Maryland\",\"Massachusetts\",\"Michigan\",\"Minnesota\",\"Mississippi\",\"Missouri\",\"Montana\",\"Nebraska\",\"Nevada\",\"New Hampshire\",\"New Jersey\",\"New Mexico\",\"New York\",\"North Carolina\",\"North Dakota\",\"Ohio\",\"Oklahoma\",\"Oregon\",\"Pennsylvania\", \"Rhode Island\",\"South Carolina\",\"South Dakota\",\"Tennessee\",\"Texas\",\"Utah\",\"Vermont\",\"Virginia\",\"Washington\",\"West Virginia\",\"Wisconsin\",\"Wyoming\"], 'Abb': [\"AL\",\"AK\",\"AZ\",\"AR\",\"CA\",\"CO\",\"CT\",\"DE\",\"FL\",\"GA\",\"HI\",\"ID\",\"IL\",\"IN\",\"IA\",\"KS\",\"KY\",\"LA\",\"ME\",\"MD\",\"MA\",\"MI\",\"MN\",\"MS\",\"MO\",\"MT\",\"NE\",\"NV\",\"NH\",\"NJ\",\"NM\",\"NY\",\"NC\",\"ND\",\"OH\",\"OK\",\"OR\",\"PA\",\"RI\",\"SC\",\"SD\",\"TN\",\"TX\",\"UT\",\"VT\",\"VA\",\"WA\",\"WV\",\"WI\",\"WY\"]}\ndf_abv = pd.DataFrame(data=d)","978cc61b":"df_abv.head()","c13061c2":"df3=df3.merge(df_abv)\ndf3.drop([\"State\"], axis=1,inplace=True)","79f3fc32":"df3.rename(columns={'Abb':'State'}, inplace=True)","67044f8e":"df3.head()","71bd2f93":"df = df3.merge(df4)\ndf.drop([\"Month\",\"County\",\"City Code\",\"Population Rank\",\"City\"], axis=1,inplace=True)\ndf=df[df[\"Year\"]>2009]\n","34584b0a":"df.State = df.State.astype(str)\ndf2.State = df2.State.astype(str)\ndf.State=df.State.str.strip()\ndf2.State=df2.State.str.strip()\n","e355e4f9":"df=df.groupby([\"State\",\"Year\"]).mean()","845e0ad8":"df.reset_index(inplace=True)","27d43d36":"df.head()","34a50fce":"final_df=df2.merge(df)\n","c6755c68":"del df2,df3,df4,df\ngc.collect()\ngc.collect()\ngc.collect()\ngc.collect()","e748f3d2":"X_train_set_df=final_df[final_df[\"Year\"]<2015].drop([\"State\",\"report_year\",\"Year\"], axis=1)\nY_train_set_df=final_df[final_df[\"Year\"]<2015].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',\n       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',\n       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',\n       'State', 'Year', 'Unemployment rate', 'November 2010',\n       'December 2010', 'January 2011', 'February 2011', 'March 2011',\n       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',\n       'September 2011', 'October 2011', 'November 2011', 'December 2011',\n       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',\n       'June 2012', 'July 2012', 'August 2012', 'September 2012',\n       'October 2012', 'November 2012', 'December 2012', 'January 2013',\n       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',\n       'July 2013', 'August 2013', 'September 2013', 'October 2013',\n       'November 2013', 'December 2013', 'January 2014', 'February 2014',\n       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',\n       'August 2014', 'September 2014', 'October 2014', 'November 2014',\n       'December 2014', 'January 2015', 'February 2015', 'March 2015',\n       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',\n       'September 2015', 'October 2015', 'November 2015', 'December 2015',\n       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',\n       'June 2016', 'July 2016', 'August 2016', 'September 2016',\n       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)","608ce044":"X_test_set_df=final_df[final_df[\"Year\"]>2014].drop([\"State\",\"report_year\",\"Year\"], axis=1)\nY_test_set_df=final_df[final_df[\"Year\"]>2014].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',\n       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',\n       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',\n       'State', 'Year', 'Unemployment rate', 'November 2010',\n       'December 2010', 'January 2011', 'February 2011', 'March 2011',\n       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',\n       'September 2011', 'October 2011', 'November 2011', 'December 2011',\n       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',\n       'June 2012', 'July 2012', 'August 2012', 'September 2012',\n       'October 2012', 'November 2012', 'December 2012', 'January 2013',\n       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',\n       'July 2013', 'August 2013', 'September 2013', 'October 2013',\n       'November 2013', 'December 2013', 'January 2014', 'February 2014',\n       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',\n       'August 2014', 'September 2014', 'October 2014', 'November 2014',\n       'December 2014', 'January 2015', 'February 2015', 'March 2015',\n       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',\n       'September 2015', 'October 2015', 'November 2015', 'December 2015',\n       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',\n       'June 2016', 'July 2016', 'August 2016', 'September 2016',\n       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)","4da52b6b":"# Prints R2 and RMSE scores\ndef get_score(prediction, lables):    \n    print('R2: {}'.format(r2_score(prediction, lables)))\n    print('RMSE: {}'.format(np.sqrt(mean_squared_error(prediction, lables))))\n\n# Shows scores for train and validation sets    \ndef train_test(estimator, x_trn, x_tst, y_trn, y_tst):\n    prediction_train = estimator.predict(x_trn)\n    # Printing estimator\n    print(estimator)\n    # Printing train scores\n    get_score(prediction_train, y_trn)\n    prediction_test = estimator.predict(x_tst)\n    # Printing test scores\n    print(\"Test\")\n    get_score(prediction_test, y_tst)","7b2a4210":"ENSTest = linear_model.ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000).fit(X_train_set_df, Y_train_set_df)\ntrain_test(ENSTest, X_train_set_df, X_test_set_df, Y_train_set_df, Y_test_set_df)","a06c09c0":"df = pd.read_csv('..\/input\/crimes-un-datacsv\/Crimes_UN_data.csv', skiprows=[0], encoding='latin1')\ndf.head()","eedfc260":"cols = list(df.columns)\ncols[1] = 'Region'\ndf.columns = cols\npivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)\nun_crimes=pivoted\npivoted.head()","018077dd":"pivoted=pivoted.fillna(0)","84d6525c":"pivoted.loc[pivoted.index.get_level_values(0)]","0fc90d17":"pivoted_max=pivoted.max(level='Region')\npivoted_max=pivoted_max.fillna(0)\npivoted_max[\"Intentional homicide rates per 100,000\"]=pivoted_max[\"Intentional homicide rates per 100,000\"].astype('float')\npivoted_max[\"Robbery at the national level, rate per 100,000 population\"]=pivoted_max[\"Robbery at the national level, rate per 100,000 population\"].astype('float')\npivoted_max[\"Assault rate per 100,000 population\"]=pivoted_max[\"Assault rate per 100,000 population\"].astype('float')\n\npivoted_max1=pivoted_max.sort_values(by=\"Intentional homicide rates per 100,000\",ascending=False)\npivoted_max","4a4705c4":"import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(20, 10))\npivoted_max1['Intentional homicide rates per 100,000'].head(15).plot(kind='bar', rot=90,  title='Intentional homicide rates per 100,000')","62366147":"pivoted_max2=pivoted_max.sort_values(by=\"Robbery at the national level, rate per 100,000 population\",ascending=False)\nfig, ax = plt.subplots(figsize=(20, 10))\npivoted_max2['Robbery at the national level, rate per 100,000 population'].head(15).plot(kind='bar', rot=90,  title='Robbery at the national level, rate per 100,000 population')","8cbfbac5":"pivoted_max3=pivoted_max.sort_values(by=\"Assault rate per 100,000 population\",ascending=False)\nfig, ax = plt.subplots(figsize=(20, 10))\npivoted_max3[\"Assault rate per 100,000 population\"].head(15).plot(kind='bar', rot=90,  title='Assault rate per 100,000 population')","33a383d7":"del pivoted_max3,pivoted_max,df,pivoted_max2,pivoted\n\n\ngc.collect()","78d90854":"pd.options.display.max_rows = 20\npd.options.display.max_columns = 20\n","33fe6d3a":"nm = pd.read_csv('..\/input\/nationmaster\/Nationmaster.csv', encoding='latin1')\nnm.rename(columns={'Unnamed: 0': \"Country\"}, inplace=1)\nnm.shape\n","2cd99ff2":"#dropping data that is irrelevant and \nnm.drop(columns=[\n\t\"Age of criminal responsibility %28notes%29\", \"Background\",\n\t\"Illicit drugs\",\n\t\"Minimum to serve before eligibility for requesting parole\",\n\t\"Prosecution rate\",\n\t\"Prosecution rate per million\",\n\t\"%25 sales\"\n], inplace=True)\n\nprint(\"deleting average countries: \\n\", nm[nm[\"Country\"].str.match(\".*average\")][\"Country\"])\nnm.drop(nm[nm[\"Country\"].str.match(\".*average\")].index, inplace=True)\n","69812f4d":"#dropping countries we do not have data about\nlimit = 20  # drop all countries with data in less than x columns\nbad_countries = (~nm.isna()).sum(axis=1)\nprint(\"deleting \", (bad_countries < 20).sum())\nnm.drop(nm[bad_countries < 20].index, inplace=True)\nnm.shape\n","85fc7d7c":"nm[nm[\"Country\"].str.match(\".*Un.*\")]  # country name search\n","f5447b1d":"nm2 = nm.copy()\n","b01e7546":"#dropping columns we do not have rows for enough countries\nnm = nm2.copy()\nlimit = 60  # drop columns with more than x countries missing\nbad_cols = nm.isna().sum(axis=0)\nprint(\"deleting \", (bad_cols > limit).sum(), \":\\n\", nm.columns[bad_cols > limit])\nnm.drop(columns=nm.columns[bad_cols > limit],inplace=True)\nprint(nm.shape)\nnm","d254eaa6":"import plotly.offline as py\n\nimport plotly.graph_objs as go\n\npy.init_notebook_mode(connected=True)","4da870c3":"countriesraw = pd.read_csv('..\/input\/plotly_countries\/2014_world_gdp_with_codes.csv')\ncountries=countriesraw.copy()\n\ncountries[\"Country\"]=countries[\"COUNTRY\"]\ncountries=countries.drop(columns=[\"COUNTRY\"])\n\nnm.reset_index(inplace=True)\n\nnm=nm.merge(countries,how=\"outer\")\n\nnm.sort_index(inplace=True)\nnm.set_index(\"Country\",inplace=True)\nnm.columns","3ed6eafe":"column_name='Total crimes per 1000'\ndata = [ dict(\n        type = 'choropleth',\n        locations = nm['CODE'],\n        z = nm[column_name].fillna(0),\n        text = nm.index,\n        colorscale = [[0,\"rgb(5, 10, 172)\"],[0.35,\"rgb(40, 60, 190)\"],[0.5,\"rgb(70, 100, 245)\"],\\\n            [0.6,\"rgb(90, 120, 245)\"],[0.7,\"rgb(106, 137, 247)\"],[1,\"rgb(220, 220, 220)\"]],\n        autocolorscale = False,\n        reversescale = True,\n        marker = dict(\n            line = dict (\n                color = 'rgb(180,180,180)',\n                width = 0.5\n            ) ),\n        colorbar = dict(\n            autotick = False,\n            title = column_name),\n      ) ]\n\nlayout = dict(\n    title = 'world plot of '+column_name,\n    geo = dict(\n        showframe = False,\n        showcoastlines = False,\n        projection = dict(\n            type = 'Mercator'\n        )\n    )\n)\n\nfig = dict( data=data, layout=layout )\npy.iplot( fig, validate=False, filename='d3-world-map' )\n\nnm[[column_name]].sort_values(by=column_name,ascending=0)\n","b2272eb0":"unodc=None\nfields=[\"Assault\",\"Kidnapping\",\"Theft\",\"Robbery\",\"Burglary\",\"Domestic_Burglary\",\"Theft_of_Private_Cars\",\"Motor_Vehicle_Theft\",\n        \"Total_Sexual_Violence\",\"Rape\",\"Sexual_Offences_ag_Children\"]\nstring=\"abcdefghijk\"\n\nfor i in range(len(string)):\n    l=string[i]\n    names=[\"%s_Rel_20%02d\"%(fields[i],j) for j in range(3,15)]\n    \n    tmp= pd.read_csv('..\/input\/unodc_crime\/Publication Reports_1%s.csv'%l,sep=\";\",index_col=[2],header=None,skipinitialspace=True,skiprows=10, encoding='latin1',\n                 names=[\n        \"Region\",\"Sub-Region\",\"Country\",\n        *[\"%s_Abs_20%02d\"%(fields[i],j) for j in range(3,15)],\n                     \"nothing\",\n        *[\"%s_Rel_20%02d\"%(fields[i],j) for j in range(3,15)]  \n        ],\n        \n    )\n    tmp.reset_index(inplace=True)\n    tmp[\"Country\"]=tmp[\"Country\"].str.replace(\"*\",\"\").str.replace(\" of America\",\"\").str.replace(\" (England and Wales)\",\"\",regex=False).str.replace(\"Russian Federation\",\"Russia\").str.replace(\"Republic of \",\"\").str.replace(\" (Plurinational State of)\",\"\",regex=False).str.replace(\"Korea\",\"Korea, South\",regex=False)\n    tmp.set_index(\"Country\",inplace=True)\n    tmp.drop(columns=[\"Region\",\"Sub-Region\",\"nothing\"],inplace=True)\n    for n in names:\n        tmp[n]=tmp[n].str.replace(\".\",\"\").str.replace(\",([0-9])$\",lambda m: \".\"+m[1]).astype(float)\n    tmp=tmp.loc[tmp.index.dropna()]\n    \n    unodc=tmp  if unodc is None else unodc.merge(tmp,how=\"outer\",on=\"Country\")\nunodc.drop(\"Country\/territory\",inplace=True)\n#unodc.rename(columns={'Country\/territory':'Subcontinent'},inplace=True)","2bdfa75c":"countries=pd.read_csv('..\/input\/plotly_countries\/2014_world_gdp_with_codes.csv')\ncountries[\"Country\"]=countries[\"COUNTRY\"]\ncountries=countries.drop(columns=[\"COUNTRY\",\"GDP (BILLIONS)\"])\n\n\n#unodc[\"CC\"]=countries.loc[unodc[\"Country\"].str.replace(\"*\",\"\")].reset_index()[\"CODE\"]\nunodc=unodc.reset_index().merge(countries,how=\"outer\")\n\nunodc.sort_index(inplace=True)\nunodc.set_index(\"Country\",inplace=True)","1885b42f":"unodc_mult=pd.DataFrame()\ntmp=pd.DataFrame()\nfor i in range(3,15):\n    tmp=unodc[[\"%s_Abs_20%02d\"%(f,i) for f in fields]+[\"%s_Rel_20%02d\"%(f,i) for f in fields]]\n    tmp.columns=tmp.columns.str.replace(\"_20%02d\"%i,\"\")\n    tmp[\"Year\"]=2000+i\n    tmp=tmp.reset_index().set_index([\"Country\",\"Year\"])\n    unodc_mult=unodc_mult.append(tmp)\nunodc_mult=unodc_mult.sort_index()\nunodc_mult = unodc_mult.reset_index()\nunodc_mult.rename(columns={'Country':'Region'},inplace=True)\nunodc_mult = unodc_mult.set_index(['Region','Year'])\nunodc_mult[:15]","9b72ff8b":"df=pd.read_csv('..\/input\/crimes-un-datacsv\/Crimes_UN_data.csv', skiprows=[0],thousands=\",\", encoding='latin1')\ncols = list(df.columns)\ncols[1] = 'Region'\ndf.columns = cols\npivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)\n\nun_crimes=pivoted\nun_crimes=un_crimes.rename({\"United States of America\": \"United States\"}, axis='index')","73a129da":"unodc_un=unodc_mult.merge(un_crimes, left_index=True, right_index=True,how=\"outer\")\nunodc_un.columns=unodc_un.columns.str.replace(\" per 100,000\",\" Rel\",regex=False).str.replace(\" population\",\"\",regex=False)\nunodc_un.index.levels[0][unodc_un.index.levels[0].str.match(\"United\")]","c0c99bf2":"column=fields[2]\nyear=3\ncolumn_name=\"%s_Rel_20%02d\"%(column,year) \nplot=unodc[column_name].fillna(0)#.str.replace(',', '')\nCCs=unodc['CODE']\ndata = [ dict(\n        type = 'choropleth',\n        locations = CCs,\n        z = plot,\n        text = unodc.index,\n        colorscale = [[0,\"rgb(5, 10, 172)\"],[0.35,\"rgb(40, 60, 190)\"],[0.5,\"rgb(70, 100, 245)\"],\\\n            [0.6,\"rgb(90, 120, 245)\"],[0.7,\"rgb(106, 137, 247)\"],[1,\"rgb(220, 220, 220)\"]],\n        autocolorscale = False,\n        reversescale = True,\n        marker = dict(\n            line = dict (\n                color = 'rgb(180,180,180)',\n                width = 0.5\n            ) ),\n        colorbar = dict(\n            autotick = False,\n            title = column_name.replace(\"_\",\" \")),\n      ) ]\n\nlayout = dict(\n    title = 'world plot of '+column_name.replace(\"_\",\" \"),\n    geo = dict(\n        showframe = False,\n        showcoastlines = False,\n        projection = dict(\n            type = 'Mercator'\n        )\n    )\n)\n\nfig = dict( data=data, layout=layout )\npy.iplot( fig, validate=False, filename='d3-world-map' )\n#plot.sort_values(ascending=False)[:20]","aaaa1549":"unodc_avg_year=pd.DataFrame()\nunodc_avg=pd.DataFrame()\nunodc_max_cat=pd.DataFrame()\nfor f in fields: # trying to use only these to not let rare crimes play big role, but no difference:\n                #[\"Robbery\",\"Assault\",\"Kidnapping\",\"Burglary\",\"Theft_of_Private_Cars\"]:\n    names=[\"%s_Rel_20%02d\"%(f,j) for j in range(3,15)]\n    unodc_avg_year[f]=unodc[names].mean(axis=1)\nun_fields=['Assault rate Rel',\n       'Intentional homicide rates Rel',\n       'Kidnapping at the national level, rate Rel',\n       'Percentage of male and female intentional homicide victims, Female',\n       'Percentage of male and female intentional homicide victims, Male',\n       'Robbery at the national level, rate Rel',\n       'Theft at the national level, rate Rel',\n       'Total Sexual Violence at the national level, rate Rel']\nfor f in un_fields:\n    print(f)\n    names=[\"%s_Rel_20%02d\"%(f,j) for j in range(3,15)]\n    unodc_avg_year[f]=unodc_un[f].unstack(level=1).astype(float).mean(axis=1)\n    \nunodc_avg_year-=unodc_avg_year.mean()\nunodc_avg_year\/=unodc_avg_year.std()\nunodc_avg[\"crime_score\"]=unodc_avg_year.mean(axis=1)\nunodc_avg[\"crime_score\"]-=unodc_avg[\"crime_score\"].min()\nunodc_avg[\"crime_score\"].fillna(0,inplace=True)\nunodc_avg[\"CODE\"]=unodc[\"CODE\"]\nunodc_avg_year.loc[[\"United States\",\"Sweden\",\"Austria\",\"Mexico\",\"Morocco\",\"Serbia\"]]","e1e98628":"column_name=\"crime_score\"\ndata = [ dict(\n        type = 'choropleth',\n        locations = unodc_avg['CODE'],\n        z = unodc_avg['crime_score'],\n        text = unodc.index,\n        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\\\n            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],\n        autocolorscale = False,\n        reversescale = False,\n        marker = dict(\n            line = dict (\n                color = 'rgb(180,180,180)',\n                width = 0.5\n            ) ),\n        colorbar = dict(\n            #autotick = False,\n            title = column_name.replace(\"_\",\" \")),\n      ) ]\n\nlayout = dict(\n    title = 'world plot of '+column_name.replace(\"_\",\" \"),\n    geo = dict(\n        showframe = False,\n        showcoastlines = False,\n        projection = dict(\n        )\n    )\n)\n\nfig = dict( data=data, layout=layout )\npy.iplot( fig, validate=False, filename='d3-world-map' )\n#import plotly.io as pio\n#pio.write_image(fig, 'world_avg_cr.pdf')\nunodc_avg[[\"crime_score\"]].sort_values(by=\"crime_score\",ascending=False)[:10]","38845a14":"corruption=pd.read_csv(\"..\/input\/unodc_crime\/theglobaleconomy_corruption.csv\", sep=\", \")\ncorruption[\"Rule of law\"]=corruption[\"Rule of law\"].astype(float)\n\ncorruption[\"Corruption perceptions - Transparency International\"]=corruption[\"Corruption perceptions - Transparency International\"].str.replace(\",\",\"\").fillna(-1).astype(int).replace(-1, np.nan)\ncorruption.set_index([\"Country\",\"Code\",\"Year\"],inplace=True)\ncorruption_avg=pd.DataFrame()\ncorruption\ncorruption_avg[\"Rule of law\"]=2.5-corruption[\"Rule of law\"].unstack(level=2).mean(axis=1)\ncorruption_avg[\"Corruption Perception\"]=100-corruption[\"Corruption perceptions - Transparency International\"].unstack(level=2).mean(axis=1)\ncorruption_avg[[\"Rule of law\",\"Corruption Perception\"]]-=corruption_avg[[\"Rule of law\",\"Corruption Perception\"]].mean(axis=0)\ncorruption_avg[[\"Rule of law\",\"Corruption Perception\"]]\/=corruption_avg[[\"Rule of law\",\"Corruption Perception\"]].std(axis=0)\ncorruption_avg[\"corruption_score\"]=corruption_avg.mean(axis=1)\ncorruption_avg.reset_index(inplace=True)\ncorruption_avg.set_index([\"Country\"],inplace=True)\ncorruption_avg[:10]","92e02ca4":"column_name=\"corruption\"\ndata = [ dict(\n        type = 'choropleth',\n        locations = corruption_avg['Code'],\n        z = corruption_avg['corruption_score'],\n        text = corruption_avg.index,\n        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\\\n            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],\n        autocolorscale = False,\n        reversescale = False,\n        marker = dict(\n            line = dict (\n                color = 'rgb(180,180,180)',\n                width = 0.5\n            ) ),\n        colorbar = dict(\n            #autotick = False,\n            title = column_name.replace(\"_\",\" \")),\n      ) ]\n\nlayout = dict(\n    title = 'world plot of '+column_name.replace(\"_\",\" \"),\n    geo = dict(\n        showframe = False,\n        showcoastlines = False,\n        projection = dict(\n        )\n    )\n)\n\nfig = dict( data=data, layout=layout )\npy.iplot( fig, validate=False, filename='d3-world-map' )\n#import plotly.io as pio\n#pio.write_image(fig, 'world_avg_cr.pdf')\ncorruption_avg[[\"corruption_score\"]].sort_values(by=\"corruption_score\",ascending=False)[:10]","06420af2":"unodc_w=unodc_avg.reset_index().merge(corruption_avg,left_on=\"CODE\",right_on=\"Code\").set_index(\"Country\")\nunodc_w[\"crime_score_w\"]=unodc_w[\"crime_score\"]+(unodc_w[\"corruption_score\"]\/3)\nunodc_w.sort_values(by=\"crime_score_w\",ascending=False)[:10]","5228d3a7":"column_name=\"crime_score_including_corruption\"\ndata = [ dict(\n        type = 'choropleth',\n        locations = unodc_w['CODE'],\n        z = unodc_w['crime_score_w'],\n        text = unodc_w.index,\n        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\\\n            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],\n        autocolorscale = False,\n        reversescale = False,\n        marker = dict(\n            line = dict (\n                color = 'rgb(180,180,180)',\n                width = 0.5\n            ) ),\n        colorbar = dict(\n            #autotick = False,\n            title = column_name.replace(\"_\",\" \")),\n      ) ]\n\nlayout = dict(\n    title = 'world plot of '+column_name.replace(\"_\",\" \"),\n    geo = dict(\n        showframe = False,\n        showcoastlines = False,\n        projection = dict(\n        )\n    )\n)\n\nfig = dict( data=data, layout=layout )\npy.iplot( fig, validate=False, filename='d3-world-map' )\n#import plotly.io as pio\n#pio.write_image(fig, 'world_avg_cr.pdf')","98257606":"unodc_max_cat[\"max_cat\"]=unodc_avg_year[fields].idxmax(axis=1)\nunodc_max_cat[\"CODE\"]=unodc[\"CODE\"]\nunodc_max_cat[\"max_cat_ind\"]=unodc_max_cat[\"max_cat\"].map(dict((f[1],f[0]+1)for f in enumerate(fields)))\nunodc_max_cat[\"max_cat_ind\"]=unodc_max_cat[\"max_cat_ind\"].fillna(0)","9f7e4486":"column_name=\"world_max_crime_category\"\ncolors=['#ffffff','#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', \n'#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#000000']\ndata = [ dict(\n        type = 'choropleth',\n        locations = unodc_max_cat['CODE'],\n        z = unodc_max_cat['max_cat_ind'],\n        text = unodc_max_cat[\"max_cat\"].str.replace(\"_\",\" \"),\n        colorscale = [[i\/11, colors[i]] for i in range(0,12)],\n        autocolorscale = False,\n        reversescale = False,\n        show_scale=True,\n        marker = dict(\n            line = dict (\n                color = 'rgb(180,180,180)',\n                width = 0.5\n            ) ),\n        colorbar = dict(\n            autotick = True,\n            title = \"colors\"\n        ),\n      ) ]\n\nlayout = dict(\n    title = 'world plot of '+column_name.replace(\"_\",\" \"),\n    geo = dict(\n        showframe = False,\n        showcoastlines = False,\n        projection = dict(\n            type = 'Mercator'\n        )\n    )\n)\n\nfig = dict( data=data, layout=layout )\npy.iplot( fig, validate=False, filename='d3-world-map2' )\nunodc_max_cat[[\"max_cat\"]].loc[[\"Austria\",\"United States\",\"United Kingdom\",\"Serbia\",\"Croatia\",\"India\"]]","1ce97601":"df8 = pd.read_csv('..\/input\/economdata\/economfaktoren.csv', encoding='latin1')\ndf8.head()","f2226b67":"cols2 = list(df8.columns)\ncols2[0] = 'Region'\ncols2[2] = 'Year'\ncols2[3] = 'Inflation rate(%)'\ncols2[4] = 'Unemployment rate(%)'\ncols2[5] = 'GDP per capita'\ndf8.columns = cols2\ndf8 = df8.drop([\"Code\"],1)\ndf8.head()","a2a2b87a":"#filling the data set\ndf8.ix[0:4,2]=[4.2,4.9,5.1,11.7,11.2] #data on Afghanistan Inflation rate 2000-2004(sources: The World Bank, International Monetery Fund) )\ndf8.ix[0,4]=[176]#Afghanistan GDP per capita 2000(source:countryeconomy.com)\ndf8.set_index(['Region','Year'],inplace=True)#setting the index\ndf8.loc [['Argentina'], ['Inflation rate(%)']] = [-0.9,-1.1,25.9,13.4,4.4,9.6,10.9,8.8,8.6,6.3,10.5,9.8,10.0,10.6,38.0,26.7]#data on Argentinian inflation rate 2000-2015(sources:Index mundi, FocusEconomics)\ndf8.loc [['Antigua and Barbuda'], ['Unemployment rate(%)']] = [8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4]#there is no data on Antigua and Barbada unemployment rate by year so I just wrote an average unempoyment rate found on Index mundi \ndf8.loc [['Bosnia and Herzegovina'],['Inflation rate(%)']] = [5,4.6,0.3,0.5,0.3,3.6,6.1,1.5,7.4,-0.4,2.1,3.7,2.1,-0.1,-0.9,-1]#data on BIH inflation rate 2000-2015(sources:Index mundi)\ndf8.loc [['Comoros'], ['Inflation rate(%)']] = [5.9,5.6,3.5,3.8,4.5,3,3.4,4.5,1.7,4.4,3.4,1.8,6.3,-4.3,1.35,2]#sources:KNOEMA.com, STATISTA.com\ndf8.loc [['Cuba'],['Inflation rate(%)']] = [2.6,4.9,7.1,4.1,3.1,7,5,3.1,3.4,-0.5,2.9,4.7,5.5,6,5.3,4.6]#source:Index mundi\ndf8.loc [['Iraq'],['GDP per capita']] = [1097,781,761,609,1391.82,1849.6,2351.81,3129.22,4521.03,3735.14,4502.75,5854.61,6651.12,6925.22,6703.07,4974.03]#source:countryeconomy.com\n\n#Data on Dominica Unemployment rate\nYouth_unemployment = [13.6,14,13.6,14.4,13.9,12.9,10.5,11.8,10.2,12,10.3,13.3,14.3,16.9,12.9,13.6]#data on youth employment in Dominica\nRatio = [23\/13.6]*16 #Ratio between only year that I found on unemployment rate(2000,source:Index mundi) and youth unemployment rate in 2000 \nNew_unemployment_rate =[] \nend_index = len(Youth_unemployment)\n\nfor i in range(end_index):\n    New_unemployment_rate.append(Youth_unemployment[i]*Ratio[i])\n    \nNew_unemployment_rate = [round(x,1) for x in New_unemployment_rate] #rounding on one decimal place\ndf8.loc [['Dominica'],['Unemployment rate(%)']] = New_unemployment_rate \n\n##there wasn't much data on Grenada unemployment rate\ndf8.loc [['Grenada'],['Unemployment rate(%)']] = [12.5,9.9,14.4,14.4,14.4,18.8,22,22,25,25,25,25,28.8,32.5,29.3,28.9]#sources:Index mundi, GOV.gd,2002-2004(took average of 2001 and 2005),2006,2007(took average with respect to 2005 and 2008),2009,2010(avg of 2008,2011),2012(avg of 2011 and 2013)\n\ndf8.loc [['Guinea'],['Inflation rate(%)']] = [6.8,5.4,3,11,17.5,31.4,34.7,22.8,18.4,4.7,15.5,21.4,15.2,11.9,9.7,8.2]#source:Index mundi\ndf8.loc [['Lebanon'],['Inflation rate(%)']] = [-0.4,-0.4,1.8,1.3,1.7,-0.7,5.6,4.1,10.8,1.2,4,5,6.6,4.8,1.9,-3.7]#source:Index mundi\ndf8.loc [['Liberia'],['Inflation rate(%)']] = [5,8,14.2,10.3,7.8,10.8,7.3,11.4,17.5,7.4,7.3,8.5,6.8,7.6,9.9,7.7]#source:Index mundi\ndf8.loc [['Libya'],['Inflation rate(%)']] = [-2.9,-8.8,-9.8,-2.2,-2.2,2.7,1.5,6.3,10.4,2.5,2.8,15.5,6.1,2.6,2.4,9.8]#source:Statistic.com\ndf8.loc [['Maldives'],['Inflation rate(%)']] = [-1.2,0.7,4.2,-1.3,-1.7,1.3,2.7,6.8,12,4.5,6.1,11.3,10.9,3.8,2.1,1]#source:FactFish\ndf8.loc [['Montenegro'],['Inflation rate(%)']] = [29.9,23.7,19.7,7.5,3.1,3.4,2.9,4.3,8.8,3.5,0.7,3.5,4.1,2.2,-0.7,1.5]#source:KNOEMA.com\ndf8.loc [['Mozambique'],['Inflation rate(%)']] = [12.7,9.1,16.8,13.5,12.6,6.4,13.2,8.2,14.5,3.8,12.4,11.2,2.6,4.3,2.6,3.6]#source:KNOEMA.com\ndf8.loc [['Namibia'],['Inflation rate(%)']] = [10.2,10.2,12.7,7.1,4.1,2.3,5.0,6.5,9.1,9.5,4.9,5.0,6.7,5.6,5.3,3.4]#source:KNOEMA.com\ndf8.loc [['Oman'],['Inflation rate(%)']] = [-1.2,-0.8,-0.3,0.2,0.8,1.9,3.2,6.0,12.1,3.9,3.2,4.0,2.9,1.0,1.0,0.1]#source:KNOEMA.com\ndf8.loc [['Puerto Rico'],['Inflation rate(%)']] = [5.7,0.6,5,6.5,2.5,5.6,5.2,4.2,5.2,0.3,2.5,3.6,1.3,0.9,0.9,-0.3]#sources:FactFish,Index mundi,FocusEconomics\ndf8.loc [['Sao Tome and Principe'],['GDP per capita']] = [568,505.79,551.20,643.95,687.55,804.13,836.80,886.67,1126.46,1099.57,1129.76,1304.33,1380.95,1619.53,1824.38,1613.48]#source:IMF\ndf8.loc [['Seychelles'],['Unemployment rate(%)']] = [3.1,4.4,4.1,3.2,3.5,3.6,2.5,1.9,1.7,5.1,4.6,4.1,3.7,3.3,3.0,2.7]#sources:STATISTA, Index mundi\ndf8.loc [['Sierra Leone'],['Inflation rate(%)']] = [-0.9,2.6,0.1,4.0,12.9,13.7,10.5,11.6,8.2,7.5,7.2,6.8,6.6,5.5,4.6,6.7]#sources:KNOEMA.com\ndf8.loc [['Syria'],['Inflation rate(%)']] = [-3.8,3.0,-0.1,5.8,4.4,7.2,10.0,3.9,15.7,2.9,4.4,4.8,36.7,89.6,29.2,38.1]#sources:CIA World Factbook\ndf8.loc [['Syria'],['GDP per capita']] = [1177.63,1258.42,1263.01,1253.39,1408.85,1577.46,1762.25,2058.04,2557.2,2557.3,2806.7,2573,3608,2800,3300,2900]#sources:CIA, countryeconomy.com\ndf8.loc [['Tajikistan'],['Inflation rate(%)']] = [32.9,38.6,12.2,16.3,7.1,7.1,10.0,13.1,20.5,6.4,6.4,12.4,5.8,5.0,6.1,5.7]#source:CIA World Factbook\ndf8.loc [['Turkmenistan'],['Inflation rate(%)']] = [8.0,11.6,8.8,5.6,5.9,10.7,8.2,6.3,14.5,-2.7,4.4,5.3,5.3,6.8,6.0,7.4]#source:KNOEMA.com\ndf8.loc [['United Arab Emirates'],['Inflation rate(%)']] = [1.3,2.8,2.9,3.1,5.0,6.2,9.3,11.1,12.3,1.6,0.9,0.9,0.7,1.1,2.3,4.1]#source:KNOEMA.com\ndf8.loc [['Uzbekistan'],['Inflation rate(%)']] = [25,27.3,27.3,12.5,7.3,10.7,13.1,11.2,13.1,12.3,12.3,12.4,11.9,11.7,9.1,8.5]#source:KNOEMA.com\ndf8.loc [['Venezuela'],['Inflation rate(%)']] = [13,12.3,31.2,31.1,22.4,16.0,15.8,18.7,30.4,27.1,28.2,26.1,21.1,40.6,62.2,121.7]#source:Index mundi\ndf8.loc [['Venezuela'],['GDP per capita']] = [4783.53,4926.31,3655.98,3232.52,4271.37,5432.69,6735.80,8318.80,11227.23,11536.15,13545.26,10741.58,12755.00,12237.19,15692.41,30775.00]#source:CEIC\ndf8.loc [['Yemen'],['Inflation rate(%)']] = [4.6,11.9,12.2,10.8,12.5,11.8,10.8,7.9,19.0,5.4,11.2,19.5,9.9,11.0,8.1,12]#source:statistic.com\ndf8.loc [['Zimbabwe'],['Inflation rate(%)']] = [4.5,-37.2,-34.4,-8.6,113.6,-31.5,33.0,-72.7,157.0,6.2,3.0,3.5,3.7,1.6,-0.2,-2.4]#source:KNOEMA.com\n\n\ndf8.head()","d9856918":"#setting all values as floats\ndf8.loc[:,'Inflation rate(%)'] = df8.loc[:,'Inflation rate(%)'].astype('float')\/100\n#we want to have unemployment per capita for our last model so I am dividing whole column with 100\ndf8.loc[:,'Unemployment rate(%)'] = df8.loc[:,'Unemployment rate(%)'].astype('float')\/100\ndf8.loc[:,'GDP per capita'] = df8.loc[:,'GDP per capita'].astype('float') \n\ncols3 = list(df8.columns)\ncols3[0] = 'Inflation rate' \ncols3[1] = 'Unemployment rate per capita'\ndf8.columns = cols3\n#pd.set_option('display.max_rows',3000)\ndf_econ_max=df8\ndf8.head()","963652a5":"df8_plot=df8.reset_index()\n\ncolumn_name='GDP per capita'\ndata = [ dict(\n        type = 'choropleth',\n        locations = df8_plot['Region'].astype(str),\n        locationmode = 'country names',\n        z = df8_plot[column_name].astype(float),\n        #text = df8['Code'].astype(str),\n        colorscale = [[0,\"rgb(5, 10, 172)\"],[0.35,\"rgb(40, 60, 190)\"],[0.5,\"rgb(70, 100, 245)\"],\\\n            [0.6,\"rgb(90, 120, 245)\"],[0.7,\"rgb(106, 137, 247)\"],[1,\"rgb(220, 220, 220)\"]],\n        autocolorscale = False,\n        reversescale = True,\n        marker = dict(\n            line = dict (\n                color = 'rgb(180,180,180)',\n                width = 0.5\n            ) ),\n        colorbar = dict(\n            autotick = False,\n            title = column_name),\n      ) ]\n\nlayout = dict(\n    title = 'world plot of GDP per capita (2000)',\n    geo = dict(\n        showframe = False,\n        showcoastlines = False,\n        projection = dict(\n            type = 'Mercator'\n        )\n    )\n)\n\nfig = dict( data=data, layout=layout )\npy.iplot( fig, validate=False, filename='d8-world-map' )\n","f2cf3cb2":"column_name='Inflation rate'\ndata = [ dict(\n        type = 'choropleth',\n        locations = df8_plot['Region'].astype(str),\n        locationmode = 'country names',\n        z = df8_plot[column_name].astype(float),\n        #text = df8['Code'].astype(str),\n        colorscale = [[0,\"rgb(172, 10, 5)\"],[0.35,\"rgb(190, 60, 40)\"],[0.5,\"rgb(245, 100, 70)\"],\\\n            [0.6,\"rgb(245, 120, 90)\"],[0.7,\"rgb(247, 137, 106)\"],[1,\"rgb(220, 220, 220)\"]],\n        autocolorscale = False,\n        reversescale = True,\n        marker = dict(\n            line = dict (\n                color = 'rgb(180,180,180)',\n                width = 0.5\n            ) ),\n        colorbar = dict(\n            autotick = False,\n            title = column_name),\n      ) ]\n\nlayout = dict(\n    title = 'world plot of Inflation rate (2000)',\n    geo = dict(\n        showframe = False,\n        showcoastlines = False,\n        projection = dict(\n            type = 'Mercator'\n        )\n    )\n)\n\nfig = dict( data=data, layout=layout )\npy.iplot( fig, validate=False, filename='d8-world-map' )","fd4c58e8":"column_name='Unemployment rate per capita'\ndata = [ dict(\n        type = 'choropleth',\n        locations = df8_plot['Region'].astype(str),\n        locationmode = 'country names',\n        z = df8_plot[column_name].astype(float),\n        #text = df8['Code'].astype(str),\n        colorscale = [[0,\"rgb(5, 172, 10)\"],[0.35,\"rgb(40, 190, 60)\"],[0.5,\"rgb(70, 245, 100)\"],\\\n            [0.6,\"rgb(90, 245, 120)\"],[0.7,\"rgb(106, 247, 137)\"],[1,\"rgb(220, 220, 220)\"]],\n        autocolorscale = False,\n        reversescale = True,\n        marker = dict(\n            line = dict (\n                color = 'rgb(180,180,180)',\n                width = 0.5\n            ) ),\n        colorbar = dict(\n            autotick = False,\n            title = column_name),\n      ) ]\n\nlayout = dict(\n    title = 'world plot of Unemployment rate per capita (2000)',\n    geo = dict(\n        showframe = False,\n        showcoastlines = False,\n        projection = dict(\n            type = 'Mercator'\n        )\n    )\n)\n\nfig = dict( data=data, layout=layout )\npy.iplot( fig, validate=False, filename='d8-world-map' )\n","13303ceb":"df_total_pop = pd.read_csv('..\/input\/population-un-data\/Total_population_both_sexes.csv',\n                           sep=';', encoding='latin', decimal = ',',skiprows=[0])\ndf_total_pop.head()","000651c2":"df_total_pop_formatted = pd.melt(df_total_pop, ['Country'], var_name = 'Year', value_name = 'Total population in 1,000')\ndf_total_pop_formatted.columns = ['Region', 'Year', 'Total population in 1,000']\ndf_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])\ncountry_names = df_total_pop_formatted.Region\ndf_total_pop_formatted.Year = df_total_pop_formatted.Year.astype(int)\ndf_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].str.replace(' ', '')\ndf_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].astype(np.float64)\ndf_total_pop_formatted = df_total_pop_formatted.loc[df_total_pop_formatted['Year'] >1999,:]","472e0d08":"country_names2 = country_names.drop_duplicates()","5f7e18a3":"false_names = ['Ivory Coast+A168', 'Viet Nam','Cabo Verde']\nnew_names = ['Ivory Coast', 'Vietnam','Cape Verde']\ndf_total_pop_formatted = df_total_pop_formatted.replace(to_replace=false_names, value=new_names)\ndf_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])","43120c0e":"country_names = df_total_pop_formatted.Region\ncountry_names2 = country_names.drop_duplicates()","b4e41aa2":"df_pop_density = pd.read_csv('..\/input\/population-un-data\/Population_density.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])\ndf_pop_density_formatted = pd.melt(df_pop_density, ['Country'], var_name = 'Year', value_name = 'Population density (persons per km2)')\ndf_pop_density_formatted.columns = ['Region', 'Year','Population density (persons per km2)']\ndf_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])\ndf_pop_density_formatted.Year = df_pop_density_formatted.Year.astype(int) \ndf_pop_density_formatted = df_pop_density_formatted.loc[df_pop_density_formatted['Year'] >1999,:]","c797020d":"country_names3 = df_pop_density_formatted.Region\ncountry_names3 = country_names3.drop_duplicates()\nfalse_names = country_names3[country_names3.isin(country_names2)==False]\nfalse_names = false_names.tolist()\nfalse_names","e035b15a":"new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao','Czech Republic', 'Ivory Coast', 'Korea', 'Falkland Islands','Holy See',\n             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Sint Maarten', 'Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']\ndf_pop_density_formatted = df_pop_density_formatted.replace(to_replace=false_names, value=new_names)\ndf_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])\n\ncountry_names2 = country_names2.set_value(country_names2.size, 'Holy See')","e37ff28b":"df_crude_birth_rate = pd.read_csv('..\/input\/population-un-data\/Crude_birth_rate.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])\ndf_crude_birth_rate.head()","c7687101":"cols = df_crude_birth_rate.columns\ncol_loc = 0\nfor year in cols:\n    if 'Country' in year:\n        colloc=+1\n        continue\n    else:\n        curr_col = df_crude_birth_rate[year]\n        new_col = curr_col\/5\n        first, last = year.split('-')\n        \n        for i in range(int(first),int(last)):\n            df_crude_birth_rate[str(i)] = new_col\n        \n        df_crude_birth_rate = df_crude_birth_rate.drop(year,1)\n        \n        col_loc=+1\ndf_crude_birth_rate[str(i+1)] = new_col","637496fb":"df_crude_birth_rate_formatted = pd.melt(df_crude_birth_rate, ['Country'], var_name = 'Year', value_name = 'Crude birth rate (births per 1,000)')\ndf_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Country','Year'])\ndf_crude_birth_rate_formatted.columns = ['Region', 'Year', 'Crude birth rate (births per 1,000)']\ndf_crude_birth_rate_formatted.Year = df_crude_birth_rate_formatted.Year.astype(int) \ndf_crude_birth_rate_formatted = df_crude_birth_rate_formatted.loc[df_crude_birth_rate_formatted['Year'] >1999,:]","85c63360":"country_names3 = df_crude_birth_rate_formatted.Region\ncountry_names3 = country_names3.drop_duplicates()\nfalse_names = country_names3[country_names3.isin(country_names2)==False]\nfalse_names = false_names.tolist()\nfalse_names","dee7975b":"new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao', 'Czech Republic','Ivory Coast', 'Korea',\n             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']\ndf_crude_birth_rate_formatted = df_crude_birth_rate_formatted.replace(to_replace=false_names, value=new_names)\ndf_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Region','Year'])","061cf755":"df_fertility = pd.read_csv('..\/input\/population-un-data\/Total_fertility_rate.csv', sep = ',',delimiter='\"')\ndf_fertility.head()","0101019a":"#!tr -d '\"' <..\/input\/population-un-data\/Total_fertility_rate.csv >Total_fertility_rate1.csv\n\ndf_fertility = pd.read_csv('..\/input\/population-un-data\/Total_fertility_rate1.csv', sep=',', usecols = (0,1,3))\ndf_fertility.columns = ['Country', 'Year', 'Total fertility rate']","d076bc24":"df_fertility2 = df_fertility.loc[17:29,:]\n\nfor k in range(1,240):\n    df_fertility2 = df_fertility2.append(df_fertility.loc[k*30 + 17:k*30 + 29,:])","f7a210ac":"df_fertility3 = pd.DataFrame(columns = ['Country', 'Year', 'Total fertility rate'])\nfor index, row in df_fertility2.iterrows():\n    region = row['Country']\n    year = row['Year']\n    val = row['Total fertility rate']\n    \n    first, last = year.split('-')\n    for i in range(int(first),int(last)):\n        new_row = pd.DataFrame([[region,i,val]], columns = ['Country', 'Year', 'Total fertility rate'])\n        df_fertility3 = pd.concat([df_fertility3,new_row])\ndf_fertility3 = df_fertility3.sort_values(by = ['Country', 'Year'])\ndf_fertility3.columns = ['Region', 'Year', 'Total fertility rate']\ndf_fertility3.Year = df_fertility3.Year.astype(int)\ndf_fertility3 = df_fertility3.loc[df_fertility3['Year'] >1999,:]","a30f757f":"country_names3 = df_fertility3.Region\ncountry_names3 = country_names3.drop_duplicates()\nfalse_names = country_names3[country_names3.isin(country_names2)==False]\nfalse_names = false_names.tolist()\nfalse_names","ee8e06fa":"new_names = ['Bolivia','Cape Verde','Curacao','Czech Republic','Korea', 'Falkland Islands',\n             'Iran', 'Laos', 'Polynesia','Moldova','Russia','Sint Maarten', 'Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']\ndf_fertility3 = df_fertility3.replace(to_replace=false_names, value=new_names)\ndf_fertility3 = df_fertility3.sort_values(by = ['Region','Year'])\n\ncountry_names2 = country_names2.set_value(country_names2.size,'Polynesia')","9ebd76ad":"#!tr -d '\"' <..\/input\/population-un-data\/Enrolment_in_Grade_1_of_primary_education.csv >Enrolment_in_Grade_1_of_primary_education1.csv\n\ndf_education_grade1 = pd.read_csv('..\/input\/population-un-data\/Enrolment_in_Grade_1_of_primary_education1.csv', sep = \",\", usecols = (0,1,2,5))\ndf_education_grade1.columns = ['Region', 'Year', 'Sex', 'Enrolment in Grade 1 of primary education']\ndf_education_grade1 = df_education_grade1.loc[df_education_grade1['Year'] >1999,:]\n#df_education_grade1","b103dbcb":"country_names3 = df_education_grade1.Region\ncountry_names3 = country_names3.drop_duplicates()\nfalse_names = country_names3[country_names3.isin(country_names2)==False]\nfalse_names = false_names.tolist()\nfalse_names","0ee94bf3":"new_names = ['Ivory Coast','Korea','Hong Kong', 'Laos', 'Lybia','Macao','Netherlands Antilles','Palestine','Russia', 'Syria', 'Macedonia',\n             'Tanzania', 'Venezuela','Vietnam']\ndf_education_grade1 = df_education_grade1.replace(to_replace=false_names, value=new_names)\ndf_education_grade1 = df_education_grade1.sort_values(by = ['Region','Year'])","993f5861":"country_names2 = country_names2.set_value(country_names2.size,'Netherlands Antilles')\ncountry_names2 = country_names2.set_value(country_names2.size,'Palestine')\ncountry_names2 = country_names2.set_value(country_names2.size,'R\u00e9union')","9665305b":"df_pop_ed = pd.merge(df_total_pop_formatted,df_education_grade1, on=['Region','Year'], how='inner')\npop = df_pop_ed.iloc[:,2]\ned = df_pop_ed.iloc[:,4]\ned_rate = ed\/(pop*1000)*100\ndf_pop_ed['Enrolment in first grade of primary education (rate in percent)'] = ed_rate\ndf_education_grade1 = df_pop_ed.drop(['Total population in 1,000', 'Enrolment in Grade 1 of primary education'],axis=1)","09b97372":"df_education_grade1_female = df_education_grade1.loc[df_education_grade1['Sex'] == 'Female',:]\ndf_education_grade1_female = df_education_grade1_female.drop(['Sex'], axis=1)\ndf_education_grade1_female.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, females)']\n\ndf_education_grade1_all_genders = df_education_grade1.loc[df_education_grade1['Sex'] == 'All genders',:]\ndf_education_grade1_all_genders = df_education_grade1_all_genders.drop(['Sex'], axis=1)\ndf_education_grade1_all_genders.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, all genders)']","de9c64db":"#!tr -d '\"' <..\/input\/population-un-data\/Primary_education_Drop-out_rate.csv >Primary_education_Drop-out_rate1.csv\n\ndf_education_drop_out = pd.read_csv('..\/input\/population-un-data\/Primary_education_Drop-out_rate1.csv', usecols = (0,1,2,5))\ndf_education_drop_out.columns = ['Region','Year', 'Sex', 'Primary Education drop-out rate (in percent)']\ndf_education_drop_out.Year = df_education_drop_out.Year.astype(int)\ndf_education_drop_out.iloc[:,3] = df_education_drop_out.iloc[:,3].astype(np.float64)\ndf_education_drop_out = df_education_drop_out.loc[df_education_drop_out['Year'] >1999,:]","e113e59a":"country_names3 = df_education_drop_out.Region\ncountry_names3 = country_names3.drop_duplicates()\nfalse_names = country_names3[country_names3.isin(country_names2)==False]\nfalse_names = false_names.tolist()\nfalse_names","e8ac44bb":"new_names = ['Ivory Coast','Hong Kong','Laos','Macao','Palestine','Russia', 'Syria', 'Macedonia',\n             'Tanzania', 'Venezuela','Vietnam']\ndf_education_drop_out = df_education_drop_out.replace(to_replace=false_names, value=new_names)\ndf_education_drop_out = df_education_drop_out.sort_values(by = ['Region','Year'])","935dafdc":"df_education_drop_out_female = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Female',:]\ndf_education_drop_out_female = df_education_drop_out_female.drop(['Sex'], axis=1)\ndf_education_drop_out_female.columns = ['Region','Year','Primary Education drop-out rate (females)']\n\ndf_education_drop_out_male = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Male',:]\ndf_education_drop_out_male = df_education_drop_out_male.drop(['Sex'], axis=1)\ndf_education_drop_out_male.columns = ['Region','Year','Primary Education drop-out rate (in percent, males)']\n\ndf_education_drop_out_all_genders = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'All genders',:]\ndf_education_drop_out_all_genders = df_education_drop_out_all_genders.drop(['Sex'], axis=1)\ndf_education_drop_out_all_genders.columns = ['Region','Year','Primary Education drop-out rate (in percent, all genders)']","1974e052":"#!tr -d '\"' <..\/input\/population-un-data\/Graduates_from_tertiary_education_both_sexes.csv >Graduates_from_tertiary_education_both_sexes1.csv\n\ndf_education_graduates = pd.read_csv('..\/input\/population-un-data\/Graduates_from_tertiary_education_both_sexes1.csv',usecols = (0,1,2,5))\ndf_education_graduates.columns = ['Region', 'Year', 'Sex','Graduates from tertiary education']\ndf_education_graduates = df_education_graduates.loc[df_education_graduates['Year'] >1999,:]","3652aa0f":"country_names3 = df_education_graduates.Region\ncountry_names3 = country_names3.drop_duplicates()\nfalse_names = country_names3[country_names3.isin(country_names2)==False]\nfalse_names = false_names.tolist()\nfalse_names","e630737b":"new_names = ['Hong Kong', 'Laos','Macao','Palestine','Russia', 'Syria','Macedonia','United Kingdom','Tanzania', 'Venezuela','Vietnam']\ndf_education_graduates = df_education_graduates.replace(to_replace=false_names, value=new_names)\ndf_education_graduates = df_education_graduates.sort_values(by = ['Region','Year'])","8fb54f89":"df_pop_ed1 = pd.merge(df_total_pop_formatted,df_education_graduates, on=['Region','Year'], how='inner')\npop = df_pop_ed1.iloc[:,2]\ned = df_pop_ed1.iloc[:,4]\ned_rate = ed\/(pop*1000)*100\ndf_pop_ed1['Graduates from tertiary education (rate in percent)'] = ed_rate\ndf_education_graduates = df_pop_ed1.drop(['Total population in 1,000', 'Graduates from tertiary education'],axis=1)","cc2064ca":"df_education_graduates_female = df_education_graduates.loc[df_education_graduates['Sex'] == 'Female',:]\ndf_education_graduates_female = df_education_graduates_female.drop(['Sex'], axis=1)\ndf_education_graduates_female.columns = ['Region','Year','Graduates from tertiary education (rate in percent, females)']\n\ndf_education_graduates_all_genders = df_education_graduates.loc[df_education_graduates['Sex'] == 'All genders',:]\ndf_education_graduates_all_genders = df_education_graduates_all_genders.drop(['Sex'], axis=1)\ndf_education_graduates_all_genders.columns = ['Region','Year','Graduates from tertiary education (rate in percent, all genders)']","69d36816":"df_edu = pd.read_csv('..\/input\/population-un-data\/education_theglobaleconomy.csv',usecols=(0,2,3),na_values=' ')\ndf_edu.rename(columns = {'Country':'Region',\n                            ' Female to male ratio students at tertiary level education':\n                             'Female to male ratio students at tertiary level education'},inplace=True)\ndf_edu = df_edu.loc[df_edu['Year'] <2016,:]","b931aaa4":"country_names3 = df_edu.reset_index().Region\ncountry_names3 = country_names3.drop_duplicates()\nfalse_names = country_names3[country_names3.isin(country_names2)==False]\nfalse_names = false_names.tolist()\nfalse_names","b6f2254d":"new_names = ['Brunai Darussalam','Myanmar','North Korea','Congo','Korea']\ndf_edu = df_edu.replace(to_replace=false_names, value=new_names)\ndf_edu = df_edu.sort_values(by = ['Region','Year'])","bdaaa83f":"#country_names2\nnames_dens = df_pop_density_formatted.Region.drop_duplicates()\nnames_dens = names_dens.tolist()\n\nnames_birth = df_crude_birth_rate_formatted.Region.drop_duplicates()\nnames_birth = names_birth.tolist()\n\nnames_fert = df_fertility3.Region.drop_duplicates()\nnames_fert = names_fert.tolist()\n\nnames_ed1 = df_education_grade1_all_genders.Region.drop_duplicates()\nnames_ed1 = names_ed1.tolist()\n\nnames_ed_drop = df_education_drop_out_all_genders.Region.drop_duplicates()\nnames_ed_drop = names_ed_drop.tolist()\n\nnames_edu = df_edu.Region.drop_duplicates()\nnames_edu = names_edu.tolist()\n\nnames1 = [itm for itm in country_names2 if itm in names_dens]\nnames2 = [itm for itm in names_birth if itm in names_fert]\nnames3 = [itm for itm in names_ed1 if itm in names_ed_drop]\nnames4 = [itm for itm in names1 if itm in names2]\nnames5 = [itm for itm in names3 if itm in names4]\nnames = [itm for itm in names5 if itm in names_edu]","e18ea6bc":"df_pop = df_total_pop_formatted[df_total_pop_formatted.Region.isin(names)==True]\ndf_dens = df_pop_density_formatted[df_pop_density_formatted.Region.isin(names)==True]\ndf_birth = df_crude_birth_rate_formatted[df_crude_birth_rate_formatted.Region.isin(names)==True]\ndf_fert = df_fertility3[df_fertility3.Region.isin(names)==True]\ndf_ed1 = df_education_grade1_all_genders[df_education_grade1_all_genders.Region.isin(names)==True]\ndf_drop_out = df_education_drop_out_all_genders[df_education_drop_out_all_genders.Region.isin(names)==True]\n#df_grad = df_education_graduates_all_genders[df_education_graduates_all_genders.Region.isin(names)==True]\ndf_edu2 = df_edu[df_edu.Region.isin(names)==True]","d02c62a3":"df_social = pd.merge(df_pop, df_dens, on = ['Region','Year'], how = 'outer')\ndf_social = pd.merge(df_social, df_birth, on = ['Region','Year'], how = 'outer')\ndf_social = pd.merge(df_social, df_fert, on = ['Region','Year'], how = 'outer')\ndf_social = pd.merge(df_social, df_ed1, on = ['Region','Year'], how = 'outer')\ndf_social = pd.merge(df_social, df_drop_out, on = ['Region','Year'], how = 'outer')\n#df_social = pd.merge(df_social, df_grad, on = ['Region','Year'], how = 'outer')\ndf_social = pd.merge(df_social, df_edu2, on = ['Region','Year'], how = 'outer')","e3c7ce45":"from fancyimpute import IterativeImputer\n\nimp_cols = df_social.iloc[:,2:9].columns.values\nto_impute = df_social.iloc[:,2:9]\n\ndf_social.iloc[:,2:9] = pd.DataFrame(IterativeImputer().fit_transform(to_impute),columns = imp_cols)\ndf_social = df_social.sort_values(by = ['Region','Year'])\ndf_social = df_social.set_index(['Region','Year'])","a5e7b015":"df_social.isna().sum().sum()","60f51892":"country_names3 = df8.reset_index().Region\ncountry_names3 = country_names3.drop_duplicates()\nfalse_names = country_names3[country_names3.isin(country_names2)==False]\nfalse_names = false_names.tolist()\nfalse_names","accfda24":"new_names = ['Brunai Darussalam','Myanmar','Congo','Korea','United States of America']\ndf8 = df8.reset_index()\ndf8.Region = df8.Region.replace(to_replace=false_names, value=new_names)\ndf8 = df8.sort_values(by = ['Region','Year'])\neconom_isi = df8[df8.Region.isin(names)==True]\ndf8 = df8.set_index(['Region','Year'])\ndf8.head()","8df6020f":"unodc_mult = unodc_mult.reset_index()\ncountry_names3 = unodc_mult.Region\ncountry_names3 = country_names3.drop_duplicates()\nfalse_names = country_names3[country_names3.isin(country_names2)==False]\nfalse_names = false_names.tolist()\nfalse_names","4a583adf":"new_names = ['Bahamas','Brunai Darussalam','Myanmar', 'Cape Verde','Congo democratic','Congo', 'Ivory Coast','Falkland Islands',\n             'Faeroe Islands','Gambia','Guernsey','Hong Kong','Iran','Iraq','Jersey','North Korea','Korea','Kosovo',\n             'Kosovo under UNSCR 1244','Macao','Macau','Micronesia','Sint Maarten',\n            'Saint Kitts and Nevis','Saint Vincent and the Grenadines','Syria','Macedonia',\n            'United Kingdom (Northern Ireland)','United Kingdom (Scotland)',\n            'United States of America','British Virgin Islands','West Bank']\n#unodc_mult = unodc_mult.reset_index()\nunodc_mult.Region = unodc_mult.Region.replace(to_replace = false_names, value = new_names)\nunodc_mult = unodc_mult.sort_values(by = ['Region','Year'])\nunodc_mult_isi = unodc_mult[unodc_mult.Region.isin(names)==True]\nunodc_mult_isi\n\nunodc_mult = unodc_mult.set_index(['Region','Year'])","7688e8cc":"df_social = df_social.reset_index()\nunodc_mult_isi = unodc_mult_isi.reset_index()\neconom_isi = econom_isi.reset_index()\n\nnames_social = df_social.Region.drop_duplicates().tolist()\nnames_uno = unodc_mult_isi.Region.drop_duplicates().tolist()\nnames_econom = econom_isi.Region.drop_duplicates().tolist()\n\nnames = [itm for itm in names_social if itm in names_econom]\nnames = [itm for itm in names if itm in names_uno]\n\nsocial_isi = df_social[df_social.Region.isin(names)==True]\nunodc_mult_isi = unodc_mult_isi[unodc_mult_isi.Region.isin(names)==True]\neconom_isi = econom_isi[econom_isi.Region.isin(names)==True]\neconom_isi = econom_isi.drop(['index'],axis=1)\n\ndf_all = pd.merge(unodc_mult_isi,econom_isi, on=['Region','Year'],how='outer')\ndf_all = pd.merge(df_all,social_isi, on=['Region','Year'],how='outer')\n\ndf_social = df_social.set_index(['Region','Year'])\nunodc_mult_isi = unodc_mult_isi.set_index(['Region','Year'])\neconom_isi = econom_isi.set_index(['Region','Year'])\n\ndf_all = df_all.drop(['index'],axis=1)\n\ndf_all = df_all.sort_values(by = ['Region','Year'])","3549f0b3":"df_all = df_all.drop(['Assault_Abs', 'Kidnapping_Abs', 'Theft_Abs', 'Robbery_Abs',\n       'Burglary_Abs', 'Domestic_Burglary_Abs', 'Theft_of_Private_Cars_Abs',\n       'Motor_Vehicle_Theft_Abs', 'Total_Sexual_Violence_Abs', 'Rape_Abs',\n       'Sexual_Offences_ag_Children_Abs'], axis = 1)","f393dffc":"df_all.columns","b54cac48":"df_all.rename(columns = {'Assault_Rel':'Assault rate per 100,000 population',\n                        'Kidnapping_Rel':'Kidnapping rate per 100,000 population',\n                        'Theft_Rel':'Theft rate per 100,000 population',\n                        'Robbery_Rel':'Robbery rate per 100,000 population',\n                        'Burglary_Rel':'Burglary rate per 100,000 population',\n                        'Domestic_Burglary_Rel':'Domestic Burglary rate per 100,000 population',\n                        'Theft_of_Private_Cars_Rel':'Theft of Private Cars rate per 100,000 population',\n                        'Motor_Vehicle_Theft_Rel':'Motor Vehicle Theft rate per 100,000 population',\n                        'Total_Sexual_Violence_Rel':'Total Sexual Violence rate per 100,000 population',\n                        'Rape_Rel':'Rape rate per 100,000 population',\n                        'Sexual_Offences_ag_Children_Rel':'Sexual Offences agains Children rate per 100,000 population'},\n                inplace = True)","631c4a94":"df_all_red = df_all.loc[df_all['Year'] > 2002,:]\ndf_all_red = df_all_red.loc[df_all_red['Year'] < 2014,:]","48ac4bc2":"print('Those are the countries that are not suitable for interpolation')\nc = list()\nfor country in names:\n    curr_country = df_all_red.loc[df_all_red['Region']==country,:]\n    first = curr_country.iloc[2,:]\n    last = curr_country.iloc[10,:]\n    \n    if first.isna().sum() > 0:\n        print(country)\n        c.append(country)\n        continue\n    if last.isna().sum()  > 0:\n        print(country)\n        c.append(country)\nlen(c)","a2c73455":"df_social_max=df_social","c5098b08":"my_countries=unodc_mult.index.levels[0]\nplot_countries=[\"United States\",\"Sweden\",\"Austria\",\"Mexico\",\"Croatia\"]\nfields_rel=[f+\"_Rel\" for f in fields]\nunodc_avg_region_all=pd.DataFrame()\nunodc_avg_region=unodc_mult.loc[my_countries][fields_rel]\nunodc_avg_region-=unodc_avg_region.mean()\nunodc_avg_region\/=unodc_avg_region.std()\nunodc_avg_region_all[\"crime_score_raw\"]=unodc_avg_region.mean(axis=1)\n\nunodc_avg_region_all[\"corruption_score\"]=0\nfor cntr in unodc_avg_region_all.index.levels[0]:\n    try:\n        unodc_avg_region_all[\"corruption_score\"].loc[cntr]=corruption_avg[\"corruption_score\"].loc[cntr]\n    except Exception:\n        continue\nunodc_avg_region_all[\"crime_score\"]=unodc_avg_region_all[\"crime_score_raw\"]+unodc_avg_region_all[\"corruption_score\"]\/4\n#unodc_avg_region_all[\"crime_score\"]-=unodc_avg_region_all[\"crime_score\"].min()\nunodc_avg_region_all[:20]","9776087f":"unodc_avg_region_all.drop(columns=[\"corruption_score\",\"crime_score_raw\"],inplace=True)","959b2edb":"import matplotlib.pyplot as plt\nunodc_avg_region_all.loc[plot_countries].unstack(level=0).plot(figsize=(16, 9))","9892b78e":"import numpy as np\nmodels={}\nunodc_avg_region_all[\"crime_score_lin\"]=0\ncrime_score_grad=pd.Series()\nfor c in unodc_avg_region_all.index.levels[0]:\n    tmp=unodc_avg_region_all.loc[c]\n    tmp=tmp[~tmp[\"crime_score\"].isna()]\n    if tmp.shape[0]<12:\n        continue\n    alpha=np.polyfit(\n        (tmp.index-2000).tolist(), \n        tmp[\"crime_score\"].tolist(), \n        2\n    )\n    for y in range(3,15):\n        unodc_avg_region_all[\"crime_score_lin\"].loc[c,2000+y]=y*y*alpha[0]+y*alpha[1]+alpha[2]\n    crime_score_grad[c]=abs(alpha[0]) # taking the coefficient of x**2 as this is telling wether or not there was a change","23071a09":"import matplotlib.pyplot as plt\nunodc_avg_region_all.loc[plot_countries].unstack(level=0).plot(figsize=(16, 9))","0f38ff02":"interesting_countries=crime_score_grad.sort_values(ascending=False).head(7).index\n\nimport matplotlib.pyplot as plt\nunodc_avg_region_all[\"crime_score\"].loc[interesting_countries].unstack(level=0).plot(figsize=(16, 9))","8be66c23":"trend_plot=unodc_avg_region_all[[\"crime_score\"]].loc[interesting_countries].copy()\ntrend_plot=trend_plot.merge(df_social_max,left_index=True,right_index=True,how=\"outer\")\ntrend_plot=trend_plot.merge(df_econ_max,left_index=True,right_index=True,how=\"outer\")\ntrend_plot-=trend_plot.mean()\ntrend_plot\/=trend_plot.std()\ntrend_plot.loc[\"Australia\"]","2cb9e7ea":"for c in interesting_countries:\n    trend_plot[[\"crime_score\",\"GDP per capita\",\"Inflation rate\",\"Unemployment rate per capita\",\"Primary Education drop-out rate (in percent, all genders)\"]].loc[c].plot(title=c,figsize=(16, 9))","924ef899":"crime_normalised=unodc_avg_region_all[\"crime_score\"].reset_index()","f1a399b4":"df_all_red[\"total_crime\"]=crime_normalised[\"crime_score\"]","3af1f514":"df_all_red.interpolate(method='linear',inplace=True)","e6429bf1":"df_all_red.fillna(method=\"bfill\",inplace=True)","feb000b5":"df_all_red.fillna(method=\"ffill\",inplace=True)","d63e38e7":"df_all_red.head()","9c54e1ab":"\ndf_all_red.drop(['Region'],axis=1,inplace=True)\n\n# calculate the correlation matrix\ncorr = df_all_red.corr()\n\n# plot the heatmap\nsns_plot2=sns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns)\n","7bd7cb7f":"df_all.head()","c4a49054":"y_pred=df_all_red[\"total_crime\"]","d8b9483e":"df_all_red.drop(['total_crime'], axis=1, inplace=True)","93ea74bc":"X_train=df_all_red.loc[df_all_red['Year'] < 2013,:]","d56ac7bb":"X_test1=df_all_red.loc[df_all_red['Year'] > 2012,:]","1efea973":"X_test1.head()","1fe43269":"import lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport time","d887b629":"n_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=11)","abbce625":"def train_model(X=X_train, X_test=X_test1, y=y_pred, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n\n    oof = np.zeros(len(X))\n    prediction = np.zeros(len(X_test))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        \n        if model_type == 'lgb':\n            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',verbose=10000, early_stopping_rounds=200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_tr.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_tr.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)\n            \n        if model_type == 'rcv':\n            model = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0, 1000.0), scoring='neg_mean_absolute_error', cv=5)\n            model.fit(X_train, y_train)\n            print(model.alpha_)\n\n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = mean_absolute_error(y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. MAE: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict(X_test).reshape(-1,)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = mean_absolute_error(y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. MAE: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict(X_test).reshape(-1,)\n        \n        if model_type == 'cat':\n            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(mean_absolute_error(y_valid, y_pred_valid))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction \/= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] \/= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction\n    \n    else:\n        return oof, prediction","58b6b2cd":"lgb_params = {'num_leaves': 80,\n              'min_child_weight': 28,\n              'min_split_gain': 0.745,\n          'min_data_in_leaf': 79,\n          'objective': 'huber',\n          'max_depth': 25,\n          'learning_rate': 0.01,\n          \"boosting\": \"gbdt\",\n          \"bagging_freq\": 4,\n          \"bagging_fraction\": 0.8126672064208567,\n          \"bagging_seed\": 11,\n          \"metric\": 'mae',\n          \"verbosity\": -1,\n          'reg_alpha': 0.1058,\n          'reg_lambda': 0.2209,\n          'feature_fraction': 0.9201\n         }\noof_lgb, prediction_lgb, feature_importance = train_model(params=lgb_params, model_type='lgb', plot_feature_importance=True)","d9a6cce0":"**Graduates from tertiary education**","cafea790":"To answer the last question we are going to build a model and try to predict all of the crimes combined using other (socio-economic) factors","aa70b284":"<a id=\"2\"><\/a> <br>\n# US States Analysis\n\n\nNow let us load an US data set containing 4 types of violent crime in major US cities (cities with over 250 000 citizens). This should serve as a templete to an (almost) perfect data set that we need to find. We have 40 years of data, there are 4 types of different crimes reported all through out the year. Further more we have clearly distinct areas with additional number of population for each area.","2156dd2b":"As we can see this data set goes from 2000 to 2015 and there are still some non-existent values that we need to fill with right sources or best techiniques possible. First we are going to change names of some columns and drop one column and then fill the data set.","51b2580c":"WE have from 2009 until 2015 unemployment rates.","52ad38f3":"<a id=\"5\"><\/a> <br>\n### Question 4: Do different countries have different types of crimes dominating?","32e92fb1":"df3 is a bit problematic, that is why we are going to need to create another dataframe that is going to help us to have abbreviations for states and not the full names.","47446c7d":"Very ugly data frame, let us try to clean it up and keep only the desired columns","fdae8c92":"Now here we can clearly see that the missing values are (mostly!) just missing yearly measures. Since we already know that we only have 2010 values for rent across the whole USA, than thats what we are going to be looking at here, hence only 74 rows with (atleast one) missing value. Effectively we only have 3 rows that have missing values. Let us just drop them.","74f6ff82":"### INFLATION RATE PLOT","43795d2f":"**df2-Data frame**, dataset of crimes for the entire US, including the desired column \"relative_crime\" also has some NaN, let us inspect whats wrong","6569f7ef":"<a id=\"1\"><\/a> <br>\n# 1. How can the crime rate of a country be measured?\n","fcef1e79":"We reshape the datasets into tidy format and do some preprocessing...","0b7bfb5f":"How do we even impute this? Basic methods like foreward fill and others are out of the question since they are obviously false for many reasons. There is an option to import another data set containing socio-economic variables and based on that try to predict the missing values. In essence build a model to predict the missing values. Problem with that is that we have too little training data, i.e. for Afganistan we have only two. Algorithm can not learn with that. I decided to set the NaN values to zero and atleast extract some statistics and interesing plots from the data at hand. And we are going to supplement with other data sources later on.","13aec3ae":"Before we proceede we need to prepare the final dataframe a bit more. One thing I do want to note. There we a LOT! of NaN values, and MICE (ITerativeImputer) did not impute the values as expected (only on crime data since thats were we had a lot of NaN on socio and economic data it worked splendid). Interpolation (the logical choice) was out of the question since we did not have data points on both ends, i.e. there were missing from the gekko or entirely from the end. So we needed to compromise regarding interpolation, polynomial where possible and bfil, ffil otherwise.","1b89ef52":"Now from here we can set a lot of interesting question. One of them would be why was crime in NYC in 90-s so high? Well there seems to be a couple of factors (we state them, not test them) For example, the police force in New York City grew by 35 percent in the 1990s, the numbers of prison inmates rose 24 percent, and there were demographic changes, including a decline in the number of youths. The national unemployment rate declined 25 percent between 1990 and 1999, and by 39 percent in the city between 1992 and 1999. This study shows that a single percentage point decline in the jobless rate decreases burglary by 2.2 percent and motor vehicle theft by 1.8 percent. Increases in the real minimum wage also significantly reduce robberies and murders: 3.4 to 3.7 percent fewer robberies with a 10 percent increase in the minimum wage and 6.3 to 6.9 percent fewer murders. And ofcourse police measures.The police measure that most consistently reduces crime is the arrest rate of those involved in crime, the study finds. Felony arrest rates (except for motor vehicle thefts) rose 50 to 70 percent in the 1990s. When arrests of burglars increased 10 percent, the number of burglaries fell 2.7 to 3.2 percent. When the arrest rate of robbers rose 10 percent, the number of robberies fell 5.7 to 5.9 percent. In the case of murder, the decline was 3.9 to 4 percent; in the case of assault, 2 to 2.4 percent; and for motor vehicle theft, 5 to 5.1 percent. Could a mayor have helped in reducing crime?","e0ebbd84":"Unfortunately these are too many...","05fb8454":"Seems like 'pandas.read_csv' does not like the double quotes. I tried several optional arguments, nothing worked fine. I decided to just delete the double quotes from the csv file and then read the csv again.","f9f3e034":"Before merging with df2, the crime dataset, we have to think about memory of the df dataset. There are a lot of rows that we basically wont need. hence we can drop them","63600cb6":"# Merging","1b58188e":"We intersect the country names from all dataframes","60f8f925":"WE have seen how crime indicators moves with other socio-economiv indicators. Let us quantify it in a corr plot.","ce921e29":"Let us now jumpt to conculusions, correlation is not causation but David Dinkins must have helped. New York city seems interesting since on the absolute scale it has the most crime in the entire United States. Let us dig deeper in NYC.","0e80ee49":"We have analyzed NYC enough, let us now try to find some socio-economic data, and try to see if we can use it to predict crime rate all around US. But how do we quantify successful state? NY state is quite successful at the first glance, but it is at the top of our list for crime. A couple that we should look for and that are at the top of my mind are: Unemployment Rate,Worker Productivity,Startup Activity,Median Household Income,Upward Mobility Rates,Road Quality,Public Utility Fee Rates,Overall Tax Burden  etc etc... I managed to dig up 2 interesting datasets. First if US unemplyment by county and state from 1990 until 2016 and Zillow Rent Index from 2010 until 2017. Hypothesis is simple, where there was a lot of unemplyment and biggest living factor (rent) was high ->>> more crime. Let us test that. But before we do that, we need to merge the datasets together.","665d0f85":"Now us do some EDA to get to know our complete data set","1901d9f3":"**Some more preprocessing and joining the 3 Big data frames--- Social, Economic and Crime Factors**","f095963d":"That paints another picture, now top of the list is St.Louis, Atlanta, Miami etc. That was interesting, but was it always like that? What was the number of crimes over the years?","46db22fd":"It seems that all of the missing values are mostly just in the first year. Interpolation is hard because we need values from both sides, and NaN start right from the start. WE could imputate with mean but we reduce the volatility of the data set. Another thing to do here is to cut the first year (so we are starting with November 2011) and for the rest just use backfill. This approach is bad, since we are only throwing less then 1\/6 (still a considerable amount of data!).\n### Solution: Just use linear regression since the values do follow it","3738ff07":"We do not need any predicted values for the future and remove them.","ebed5ae1":"This procedure wasn't the easiest because there a lot of countries which aren't really transparent or they just don't keep track on some of the economic factors. We mostly tried to find another sources other than our primary source (THEGLOBALECONOMY.COM). These sources included sites: Index mundi, FOCUSECONOMICS.com, STATISTA, CIA World Factbook, GOV.gd, FACTFISH.com, KNOEMA.com, Interational Monetary Fund, COUNTRYECONOMY.com, STATISTIC.com. We couldn't find a lot of information about Dominica and Grenada so we used some techinques. For Dominica we found only one year (2000) concerning unemployment rate but we managed to find another data set that should be highly correlated to Dominica unemployment rate and that is youth unemployment rate. We took ratio between unemployment rate (2000) and youth unemployment rate (2000) and multiplied youth unenmployment rate list with that ratio. In the case of Grenada unemployment rate set, we had a lot of holes in our list, so what we did was mostly manual interpolation with a help of some internet sites (mostly KNOEMA.com) that provided average unemployment rates for some years. ","e3bf5c64":"**Total population**","8b276fbd":"<a id=\"4\"><\/a> <br>\n### Question 2: Which countries have the highest crime crates?\nApproach: take all the different categories of crimes we have data about, average them over the years if existing, then normalising them to \nmy=0, sigma=1, and then finding an average of the categories for each country. ","9bc6f34b":"How to interpret this heatmap? y-axis are indices (in this case nubmers) Because of the shere number of them we can only approximately find them, but atleast we see what are the troubling columns.","da6d1d29":"<a id=\"7\"><\/a> <br>\n# Question 5: Are there also country characteristics that predict trends in crime rates or types of crimes? ","cb64a377":"# **SOCIAL FACTORS**","1a89ff16":"To create a target variable we will use the normalised aggregated crime rates from before:","a1b4f6ac":"Some interesting plots (albeit not complete data! only introductory!) but it seems that there is a suprising prevelance of assault rate in unexpected places(as already stated in the beginning). Sweden, Belgium, Germany certainly do not classify as dangerous places to live. Assumption is that these countries have better\/more effective police force that is able to cover and handle these types of crimes more effectively hence there are more assaults. If we were to compare that with worse crimes like murder, than the police can not look the other way and it is well documented that usuall countries will the top of the list.","08688bb6":"# ECONOMIC FACTORS\n","3a580e4b":"We split the time interval columns into 5 different ones by dividing the value of each column by 5. ","72b9927b":"For memory issues we will have to make this aggregation, in order to save space. Alternative is this kernel https:\/\/www.kaggle.com\/o7ronak7\/how-to-save-time-and-memory-with-big-datasets","35ea669d":"(interesting and some) Conclusions from the correlation matrix: \n\n*  WE can see some correlation between crimes and inflation rate & GDP rate\n*  Between crimes we can see (and confirm) that assault rate is correted with rape rate \n*  And socio-economic that GDP is correlated to Female to male student ratio","e5ac510b":"**Crude birth rate**","d9e62a70":"It appears that the second column \"Intentional homicide rates per 100,000\" is mostly populated, so we can make some inference from it!","d858ba32":"Starting of with the first question we can use the official definiton of FBI which classifies homicide, rape, robbery and assault  as violent crime and the true criminality of a country wont be skeewed by non-relevant crimes (if such thing exists). Ofcourse some other categories can come into the story, but I think these should be the main aspect. Same metrics that apply for the countries of the world apply for US States (the first analysis) BUT when we collected these measuraments for the countries of the world a strange pattern emerged: Sweden, German etc were on top of the list. And to quote myself as I said (underneath when second question was answered)\n\n\"WHAT? It does not make any sence. Sweden? It is because people have more trust, less corruption, better reporting system punishments etc... IN ORDER To mitigate this effect and get the TRUE levels we ought to normalise these values with all of the values\/data that I just named, and even more since these are just some of the factors that influence the odd results. That implies finding ALL of these datasets that coincide with ALL of the countries on ALL of the years. \"\n\nSo we searched for some indicators that could remove this effect and we found \"Rule of law\" and \"Corruption index\" (its underneath when reading in the crime data for the entire world). And we did find that this approach paints a better picture of a crime rate of  a country. (As we shall see later on)\n","a78830cc":"There are some names we want to change...","78fdf166":"### UNEMPLOYMENT RATE PER CAPITA PLOT","41b14716":"### GDP PER CAPITA PLOT","0dc48b73":"Similar to the crude birht rate we extend the time interval to 5 different rows.","57c7f1cb":"There are a couple of things that are interesting here. First of all city \"United States\" is the sum of them all. Further more New York, LA, Chicago seem to be most violent, or is it because of the shere number of citizens in these towns? How can we investigate it? Let us creeate a new variable that is relative to the number of citizens, i.e. total number of crime divided with total citizens. (NOTE: Black line is simply confidence interval of estimation)","f6a3bd17":"We check the countries' names of the other dataframes ...","df8a97a2":"Usually I extend this code and apply the Bayesian Hyperparameter optimisation but in this case we are not that interested in having the best of the best predictions and it would take away from memory and run-time and not help that much with the predictions, so I ommited it and used parameters from another project (they are all anyways in a standard intervall for a regression).","f726442a":"# Importing UNO DC Crime data","625c5488":"<a id=\"3\"><\/a> <br>\n# Crime Data","67ad4c9c":"But where are these missing values mostly missing?","0131974a":"First of all we are going to do some **introductory analysis** that should serve as an baby example for the actual one. **Crime Analysis of US States** (with some small Analysis of NYC). Datasets for USA is usually excellant and we did not have to wrestle with it just find a way to answer the questions. If one whishes to skip analysis of USA than scroll down over to \n### Crime Data","8904f0cb":"Second question can partially be answered with the dataset at hand:\n\n1. Which countries have the highest\/lowest crime rates?(Full\/complementary answer later on)\n\n3. Obviously very rialable signs of a country crime rate are 5 columns : Assault rate per 100,000 population, Intentional homicide rates per 100,000, Robbery at the national level, rate per 100,000 population, Theft at the national level, rate per 100,000 population, Total Sexual Violence at the national level, rate per 100,000 They are all normed to 100 000 citizens and describe various hard-crimes.\n\n4. We can answer with Table of maximum values for all 5 different types of crime","117afac5":"Preparing data set on economic factors for merging","a1f893c8":"It seems that we missed a spot, it is because of the piecewise polynomial method. No problem, for the rest 1245 Values (not rows!) we can use linear regression, since (looking at the graphs polynomials seem more reasonable but linear regression is not far from truth!)","03a95adf":"# Main metric that we are trying to predict:","606196fd":"**Female to male ratio students at tertiary level education**","e23e9437":"Modelling: Elastic Net (in essence regression with penalization term) regression","e9870c03":"Now that we cleaned our datasets we can merge them. But before that we ought to split the \"agency_jurisdiction\" into two columns containing counties.","0a357439":"We reshape again into tidy format.","f29b070d":"**Population density**","51041d75":"In this section we are going to load and preprocess data concerning economic factors such as: Inflation rate, Unemployment rate and GDP per capita. We have chosen this three factors because we think they are the most important and a lot of other factors are highly correlated to this three. With this and social factors data sets we are trying to answer the question: \"Are there typical characteristics of countries with high\/low crime rates?\".","e50b82ea":" <a id=\"top\"><\/a> <br>\n## Notebook  Content\n*  [Introduction\/  1.How can the crime rate of a country be measured?](#1)\n*  [Analysis of USA first](#2)\n1. [Crime data (loading and pre-processing is a bit lentghy)](#3)\n1. [2. Which countries have the highest\/lowest crime rates? ](#4)\n1. [4. Are there countries that have different types of crimes that are dominant? ](#5)\n1. [3.Are there typical characteristics of countries with high\/low crime rates? ](#6)\n1. [5. Are there also country characteristics that predict trends in crime rates or types of crimes?](#7)\n","7c0a55fe":"That was a small taste of what an analysis should look like on a US states. For a larger scale analysis first we have to gather\/scrape data from verious sources.\n\nWe need to find datasets of Crime, Socio and Economic factors of different countries expanding over a large time horizont.","bac51fd2":"Interpretation: We need to neglect the cirminal variables (since I did not filter them out) and just look at the socio-economic. We can see that population, fertility, Unemployment and education are deal breakers!","47b2a6d0":"WHAT? It does not make any sence. Sweden? It is because people have more trust, less corruption, better reporting system punishments etc... IN ORDER To mitigate this effect and get the TRUE levels we ought to normalise these values with all of the values\/data that I just named, and even more since these are (probably) correct assumptions of why the results are odd. That implies finding ALL of these datasets that coincide with ALL of the countries on ALL of the years. We are going find some data two rows underneath and than normalise total crime for a better result.","2bfbeffe":"<a id=\"6\"><\/a> <br>\n# 3. Are there typical characteristics of countries with high\/low crime rates?","e5a98dd4":"## FILLING THE DATA SET","06619778":"Handling missing values with fancyimpute","bd7c118c":"![](https:\/\/images.indiatvnews.com\/crimecrime\/orld_s_cities_1993.jpg)\n\n\n\n\n#  This kernel sets out to answer following questions all with scraping and extracting data from internet. (In other words without pre-existing, pretty formated datasets):\n\n#  1. How can the crime rate of a country be measured?\n#  2. Which countries have the highest\/lowest crime rates? \n# 3.Are there typical characteristics of countries with high\/low crime rates? \n#  4. Are there countries that have different types of crimes that are dominant? \n# 5. Are there also country characteristics that predict trends in crime rates or types of crimes?","5463fde8":"**Primary education drop-out rate**","81c1f32c":"We have to analyse and deal with there missing values. First of all let us consult dataset description, maybe there something there to be found. According to the documentation time series goes back to november 2010 but nor for every city\/county\/state. (Obviously) So we are left with no hints on how to handle the NaN. MICE\/ using other columns as indicators to imputate missing values makes really no sense since these are rising time series, on monthly basis.  Sinc","222e65fc":"Lets have a look at rent at New Jersey and NYC, monthly! Before that we had to split the months from the dataframe","b7f2190a":"# Nationmaster\n\nStarting of with finding the crime data for the whole world, for a big number of years:","94cf0f75":"We have joined all of the sets. Aggregated all of the crimes  into one column and now will use other variables to predict it (regression problem). If we were to use light gradient boosting method (lgbm) than there is also an option to use showcase important features, i.e. feature that had the most predictive characteristics. When we have such information than typical characteristics of countries with high\/low crime rates are those features that had the most importance when predicting.","6cfa23ad":"Only rows that have missing values:","ddddaff8":"## PLOTTING","df143f58":"The following datasets are observations for 1950-2015 for 201-232 different countries.","af8af0ad":"**Total fertility rate**","114c166a":"**Enrolment in first grade of primary education**"}}