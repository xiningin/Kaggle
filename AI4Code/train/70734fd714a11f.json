{"cell_type":{"ff87906d":"code","8c9b0555":"code","b32ee106":"code","55358a51":"code","acd855e4":"code","5264f25b":"code","6052cec1":"code","748a2713":"code","9c9b5ec8":"code","3d98a919":"code","3c232d30":"code","89283928":"code","398fc195":"code","87b09307":"code","069dd940":"code","7f65e12e":"code","36383482":"code","ef76a5f3":"markdown","d1505a4a":"markdown","7e8ddd07":"markdown"},"source":{"ff87906d":"%%capture\n!pip install git+https:\/\/github.com\/huggingface\/transformers.git\n!pip uninstall fsspec -qq -y\n!pip install --no-index --find-links ..\/input\/hf-datasets\/wheels datasets -qq\n!pip install -U wandb\n!pip install rouge_score\n# !pip install deepspeed","8c9b0555":"import logging\nimport os\nimport sys\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nimport datasets\nimport nltk  # Here to have a nice missing dependency error message early on\nimport numpy as np\nfrom datasets import load_dataset, load_metric\nimport torch\n\nimport transformers\nfrom filelock import FileLock\nfrom transformers import (\n    AutoConfig,\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    DataCollatorForSeq2Seq,\n    HfArgumentParser,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    set_seed,\n)","b32ee106":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nchaii_df = pd.read_csv(\"..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv\", usecols=[\"context\",\"question\"])\nmlqa_df = pd.read_csv(\"..\/input\/mlqa-hindi-processed\/mlqa_hindi.csv\", usecols=[\"context\",\"question\"])\nxquad_df = pd.read_csv(\"..\/input\/mlqa-hindi-processed\/xquad.csv\", usecols=[\"context\",\"question\"])","55358a51":"# just use chaii datasets for validation\ntrain_df, val_df = train_test_split(chaii_df, test_size=0.2)\ntrain_df = pd.concat([train_df, mlqa_df, xquad_df], axis=0, ignore_index=True)","acd855e4":"train_df.to_csv(\"train.csv\", index=False)\nval_df.to_csv(\"val.csv\", index=False)","5264f25b":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nkey = user_secrets.get_secret(\"wandb\")\n\nimport wandb\n\nwandb.login(key=key)\nos.environ[\"WANDB_PROJECT\"] = \"chaii-qa\"","6052cec1":"class CFG:\n    run_name = \"mt5-base-q-gen-chaii-mlqa-xquad-hindi\"\n    \n    seed = 0\n    \n    model_name_or_path = \"google\/mt5-base\"\n    train_file = \"train.csv\"\n    validation_file = \"val.csv\"\n    text_column = \"context\"\n    summary_column = \"question\"\n    \n    max_source_length = 1024\n    max_target_length = 128\n    val_max_target_length = 128\n    pad_to_max_length = False\n    num_beams = 4\n    output_dir = \"output\"\n    per_device_train_batch_size = 1\n    per_device_eval_batch_size = 1\n    gradient_accumulation_steps = 2\n    num_train_epochs = 5\n    evaluation_strategy = 'steps'\n    eval_steps = 75\n    learning_rate = 3e-4\n    weight_decay = 0.01\n    warmup_ratio = 0.1\n    logging_steps = 20\n    save_total_limit = 2\n\n    \n    source_prefix = \"question\"\n    ignore_pad_token_for_loss = True","748a2713":"set_seed(CFG.seed)\n\ndata_files = {}\nif CFG.train_file is not None:\n    data_files[\"train\"] = CFG.train_file\n    extension = CFG.train_file.split(\".\")[-1]\nif CFG.validation_file is not None:\n    data_files[\"validation\"] = CFG.validation_file\n    extension = CFG.validation_file.split(\".\")[-1]\n\nraw_datasets = load_dataset(extension, data_files=data_files)\n\n# See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n# https:\/\/huggingface.co\/docs\/datasets\/loading_datasets.html.","9c9b5ec8":"# Load pretrained model and tokenizer\n#\n# Distributed training:\n# The .from_pretrained methods guarantee that only one local process can concurrently\n# download model & vocab.\nconfig = AutoConfig.from_pretrained(CFG.model_name_or_path)\ntokenizer = AutoTokenizer.from_pretrained(CFG.model_name_or_path)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    CFG.model_name_or_path,\n    config=config,\n)\n\nmodel.resize_token_embeddings(len(tokenizer))\n\nif model.config.decoder_start_token_id is None:\n    raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n\nprefix = CFG.source_prefix if CFG.source_prefix is not None else \"\"","3d98a919":"# Preprocessing the datasets.\n# We need to tokenize inputs and targets.\ncolumn_names = raw_datasets[\"train\"].column_names\n\n\n# Get the column names for input\/target.\ntext_column = CFG.text_column\nsummary_column = CFG.summary_column\n\n\n# Temporarily set max_target_length for training.\nmax_target_length = CFG.max_target_length\npadding = \"max_length\" if CFG.pad_to_max_length else False\n\n\ndef preprocess_function(examples):\n    inputs = examples[text_column]\n    targets = examples[summary_column]\n    inputs = [prefix + inp for inp in inputs]\n    model_inputs = tokenizer(inputs, max_length=CFG.max_source_length, padding=padding, truncation=True)\n\n    # Setup the tokenizer for targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n\n    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n    # padding in the loss.\n    if padding == \"max_length\" and CFG.ignore_pad_token_for_loss:\n        labels[\"input_ids\"] = [\n            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n        ]\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","3c232d30":"# Train dataset\ntrain_dataset = raw_datasets[\"train\"]\n\ntrain_dataset = train_dataset.map(\n        preprocess_function,\n        batched=True,\n        remove_columns=column_names,\n        desc=\"Running tokenizer on train dataset\",\n    )\n\n# Eval dataset\n\nmax_target_length = CFG.val_max_target_length\n\neval_dataset = raw_datasets[\"validation\"]\n\n\neval_dataset = eval_dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=column_names,\n    desc=\"Running tokenizer on validation dataset\",\n)","89283928":"# Data collator\nlabel_pad_token_id = -100 if CFG.ignore_pad_token_for_loss else tokenizer.pad_token_id\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    label_pad_token_id=label_pad_token_id,\n)\n\n# Metric\nmetric = load_metric(\"rouge\")\n\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [label.strip() for label in labels]\n\n    # rougeLSum expects newline after each sentence\n    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n\n    return preds, labels\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    if CFG.ignore_pad_token_for_loss:\n        # Replace -100 in the labels as we can't decode them.\n        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Some simple post-processing\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    # Extract a few results from ROUGE\n    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    result = {k: round(v, 4) for k, v in result.items()}\n    return result","398fc195":"training_args = Seq2SeqTrainingArguments(\n    do_train=True,\n    do_eval=True,\n    per_device_train_batch_size=CFG.per_device_train_batch_size,\n    per_device_eval_batch_size=CFG.per_device_eval_batch_size,\n    num_train_epochs=CFG.num_train_epochs,\n    output_dir=CFG.output_dir,\n    evaluation_strategy=CFG.evaluation_strategy,\n    learning_rate=CFG.learning_rate,\n    weight_decay=CFG.weight_decay,\n    warmup_ratio=CFG.warmup_ratio,\n    logging_steps=CFG.logging_steps,\n    save_total_limit=CFG.save_total_limit,\n    eval_steps=CFG.eval_steps,\n    run_name=CFG.run_name,\n    gradient_accumulation_steps=CFG.gradient_accumulation_steps\n)","87b09307":"# Initialize our Trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset if training_args.do_train else None,\n    eval_dataset=eval_dataset if training_args.do_eval else None,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics if training_args.predict_with_generate else None,\n)\n\n# Training\nif training_args.do_train:\n\n    train_result = trainer.train()\n    trainer.save_model()  # Saves the tokenizer too for easy upload\n\n    metrics = train_result.metrics\n\n    metrics[\"train_samples\"] = len(train_dataset)\n\n    trainer.log_metrics(\"train\", metrics)\n    trainer.save_metrics(\"train\", metrics)\n    trainer.save_state()\n\n# Evaluation\nresults = {}\nif training_args.do_eval:\n    print(\"*** Evaluate ***\")\n\n    metrics[\"eval_samples\"] = len(eval_dataset)\n\n    trainer.log_metrics(\"eval\", metrics)\n    trainer.save_metrics(\"eval\", metrics)","069dd940":"import gc\n\ndel trainer\ndel train_dataset\ndel eval_dataset\n\ngc.collect()\ntorch.cuda.empty_cache()","7f65e12e":"from transformers import MT5ForConditionalGeneration\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = MT5ForConditionalGeneration.from_pretrained(CFG.output_dir).to(device)\n\nmodel.eval()\n\ndef show_context_and_question(context, model):\n    prefix = CFG.source_prefix\n    full_ctx = f\"{prefix}: \" + context\n    \n    with torch.no_grad():\n        inputs = tokenizer(full_ctx, return_tensors='pt') \n        inputs = {k:v.to(device) for k, v in inputs.items()}\n\n        output = model.generate(input_ids=inputs[\"input_ids\"])\n\n        question = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(output.detach().squeeze()))\n        \n        if question.startswith(\"<pad> <extra_id_0> \"):\n            question = question[len(\"<pad> <extra_id_0> \"):]\n        if question.endswith(\"<\/s>\"):\n            question = question[:-len(\"<\/s>\")]\n    \n    return (context, question)","36383482":"for i in range(5):\n    ctx, q = show_context_and_question(val_df[\"context\"].values[i], model)\n    print(\"CONTEXT:\", ctx, \"\\n\\n\", \"QUESTION:\", q, \"\\n\\n\", \"*\"*100, \"\\n\")","ef76a5f3":"### It might not be great, but I'm sure there are people out there who can make this a lot better! The concept is really what I'm trying to get across. \ud83d\udd25","d1505a4a":"# Problem: Not enough training data \ud83d\ude1f\n\n## Solution: \ud83d\udca1\n### 1. Train a model to make questions \u2753\n### 2. Use the question-generating model to create questions on new texts \ud83d\udcdc\u2753\u2753\n### 3. Use a trained QA model to answer those questions, creating new training data. \ud83d\udcca\n### 4. Train a new and improved QA model using the pseudo-labels. \ud83d\udcaa\n### 5. If unsatisfied with model, go to step 2 \ud83d\udd04\n### 6. ??? \ud83e\udd37\u200d\u2642\ufe0f\n### 7. Profit \ud83d\udcb0\n\n#### Nearly everything in this notebook is from here: https:\/\/github.com\/huggingface\/transformers\/blob\/master\/examples\/pytorch\/summarization\/run_summarization.py\n\n\ud83e\udd17 \ud83d\udc96\n\nThis is just a proof of concept using a small model. There are probably better hyperparameters and bigger, better models. If you have questions, comments, or feedback, you know where to leave them! \ud83d\ude0a","7e8ddd07":"# Seq2Seq for Question Generation\n\nJust like how models can make a summary from a text, why not train a model to make a question? We already have the dataset -- we can just use the `context` and `question` columns."}}