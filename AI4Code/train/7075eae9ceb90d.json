{"cell_type":{"486bb938":"code","0ead0c5f":"code","350f4235":"code","891a6f31":"code","3b332e62":"code","bdbc39f6":"code","d534952b":"code","47bbd0ac":"code","f2070b96":"code","15856f38":"code","90fad369":"code","ea1286e4":"code","ddf904c5":"code","d34ba10b":"code","612b919a":"code","dbf687b2":"code","183e6275":"code","fe9c6dea":"markdown","16480205":"markdown","8917fc45":"markdown","e0d14066":"markdown","64880172":"markdown","4fe9b86d":"markdown","0dbeaf14":"markdown","09994acc":"markdown","d80d1629":"markdown"},"source":{"486bb938":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport warnings","0ead0c5f":"colors = ['#001c57','#50248f','#a6a6a6','#38d1ff','#ca7beb','#c8aef8','#9154f8','#cef3f5']\nsns.palplot(sns.color_palette(colors))","350f4235":"#train = pd.read_csv('..\/kaggle\/input\/mercedesbenz-greener-manufacturing\/train.csv')\n\n#test = pd.read_csv('..\/kaggle\/input\/mercedesbenz-greener-manufacturing\/test.csv')\n\ntrain = pd.read_csv(\"..\/input\/mercedes-benz-greener-manufacturing\/train.csv.zip\")\ntest = pd.read_csv(\"..\/input\/mercedes-benz-greener-manufacturing\/test.csv.zip\")\n\nprint('Shape of the training data: ',train.shape)\nprint('Shape of the testing data: ',train.shape)","891a6f31":"train.head()","3b332e62":"train['y'].isnull().sum()","bdbc39f6":"    plt.figure(figsize=(15,5))\n    plt.subplot(121)\n    sns.distplot(train.y.values, bins=20, color=colors[4])\n    plt.title('Target Value Distribution \\n',fontsize=15)\n    plt.xlabel('Target Value in Seconds'); plt.ylabel('Occurances');\n\n    plt.subplot(122)\n    sns.boxplot(train.y.values, color=colors[0])\n    plt.title('Target Value Distribution \\n',fontsize=15)\n    plt.xlabel('Target Value in Seconds');","d534952b":"plt.figure(figsize=(15, 5))\nplt.plot(train.y.values, color=colors[0])\nplt.xlabel('Row ID')\nplt.ylabel('Target value')\nplt.title('Change in target value over the dataset')\nplt.show()\nplt.figure(figsize=(15, 5))\nplt.plot(train.y.values[:100], color=colors[0])\nplt.xlabel('Row ID')\nplt.ylabel('Target value')\nplt.title('Change in target value over the dataset (first 100 samples)')\nprint()","47bbd0ac":"dtype_df = train.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()","f2070b96":"train.dtypes[train.dtypes=='object']","15856f38":"obj_dtype = train.dtypes[train.dtypes=='object'].index\nfor i in obj_dtype:\n    print(i, train[i].unique())","90fad369":"fig,ax = plt.subplots(len(obj_dtype), figsize=(18,80))\n\nfor i, col in enumerate(obj_dtype):\n    sns.boxplot(x=col, y='y', data=train, ax=ax[i])\n    \n#for c in counts[2]:\n#value_counts = df_train[c].value_counts()\n#fig, ax = plt.subplots(figsize=(10, 5))\n#plt.title('Categorical feature {} - Cardinality {}'.format(c, len(np.unique(df_train[c])))\n#)\n#   plt.xlabel('Feature value')\n#    plt.ylabel('Occurences')\n#    plt.bar(range(len(value_counts)), value_counts.values, color=pal[1])\n#    ax.set_xticks(range(len(value_counts)))\n#    ax.set_xticklabels(value_counts.index, rotation='vertical')\n#    plt.show()    \n    \n    ","ea1286e4":"num = train.dtypes[train.dtypes=='int'].index[1:]","ddf904c5":"nan_num = []\nfor i in num:\n    if (train[i].var()==0):\n        print(i, train[i].var())\n        nan_num.append(i)","d34ba10b":"usable_columns = list(set(train.columns) - set(['ID', 'y']))\ny_train = train['y'].values\nid_test = test['ID'].values\nx_train = train[usable_columns]\nx_test = test[usable_columns]\nfor column in usable_columns:\n    cardinality = len(np.unique(x_train[column]))\n    if cardinality == 1:\n        x_train.drop(column, axis=1) # Column with only one value is useless so we drop it\n        x_test.drop(column, axis=1)\n    if cardinality > 2: # Column is categorical\n        mapper = lambda x: sum([ord(digit) for digit in x])\n        x_train[column] = x_train[column].apply(mapper) \n        x_test[column] = x_test[column].apply(mapper)\nx_train.head()","612b919a":"import xgboost as xgb\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=10)\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\nd_test = xgb.DMatrix(x_test)\nparams = {}\nparams['objective'] = 'reg:linear'\nparams['eta'] = 0.02\nparams['max_depth'] = 4\ndef xgb_r2_score(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'r2', r2_score(labels, preds)\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\nclf = xgb.train(params, d_train, 1000, watchlist, early_stopping_rounds=50, feval=xgb_r2_score\n, maximize=True, verbose_eval=10)","dbf687b2":"p_test = clf.predict(d_test)\n\nsub = pd.DataFrame()\nsub['ID'] = id_test\nsub['y'] = p_test\nsub.to_csv('xgb_results.csv',index=False)","183e6275":"sub.head()","fe9c6dea":"## XGBoost Starter","16480205":"# Mercedes-Benz Greener Manufacturing\n\n### DESCRIPTION\n\n#### Reduce the time a Mercedes-Benz spends on the test bench.\n\n##### Problem Statement Scenario:\nSince the first automobile, the Benz Patent Motor Car in 1886, Mercedes-Benz has stood for important automotive innovations. These include the passenger safety cell with a crumple zone, the airbag, and intelligent assistance systems. Mercedes-Benz applies for nearly 2000 patents per year, making the brand the European leader among premium carmakers. Mercedes-Benz is the leader in the premium car industry. With a huge selection of features and options, customers can choose the customized Mercedes-Benz of their dreams.\n\nTo ensure the safety and reliability of every unique car configuration before they hit the road, the company\u2019s engineers have developed a robust testing system. As one of the world\u2019s biggest manufacturers of premium cars, safety and efficiency are paramount on Mercedes-Benz\u2019s production lines. However, optimizing the speed of their testing system for many possible feature combinations is complex and time-consuming without a powerful algorithmic approach.\n\nYou are required to reduce the time that cars spend on the test bench. Others will work with a dataset representing different permutations of features in a Mercedes-Benz car to predict the time it takes to pass testing. Optimal algorithms will contribute to faster testing, resulting in lower carbon dioxide emissions without reducing Mercedes-Benz\u2019s standards.\n\nFollowing actions should be performed:\n\nIf for any column(s), the variance is equal to zero, then you need to remove those variable(s).\nCheck for null and unique values for test and train sets.\nApply label encoder.\nPerform dimensionality reduction.\nPredict your test_df values using XGBoost.","8917fc45":"Looking at the above we have the following:- \n369 integer variables. \n8 object (likely a string) variables\n1 target variable\n\nLet us look at the cardinality of our features?","e0d14066":"We have a set of numeric variables, where the value is set to 1 or 0, so there is no need to carry out volumetric analysis. In this case, we should be interested in whether the value of indicators changes within the variables, for this we examine the variance of these variables, use the var () function, and select only those where the variance is zero (that is, always 0, or 1 on the entire dataset in variable cut)","64880172":"### Load training and testing data and look at the number of rows and colums for the same ","4fe9b86d":"From the above, what we can visualize is a standard distribution, which is centred around 100. There is a single outlier at 265 seconds where every other value is below 180.\n\nThe fact that our ID is not equal to the row ID seems to suggest that the train and test sets were randomly sampled from the same dataset, which could have some special order to it, for example a time series. Let's take a look at how this target value changes over time in order to understand whether we're given time series data.","0dbeaf14":"### Inference from the graphs:\n##### 1) Since there is a need to reduce the testing time, the best values in the variables at which this time is minimal are az and bc (X0), y (X1), n (X2), x and h (X5) (hypothesis: on y?)\n\n##### 2) Variables X3, X5, X6, X8 have similar distributions of values, where there are no special differences within the feature between values in the context of means and quartiles\n\n##### 3) X0 and X2 have the greatest variety within variables, which can potentially indicate a greater usefulness of these features","09994acc":"Look at the distribution of Target Values ","d80d1629":"## Feature Analysis"}}