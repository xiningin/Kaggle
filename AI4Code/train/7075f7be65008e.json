{"cell_type":{"1fe326ca":"code","db43d873":"code","cc32c32c":"code","0a8bfe33":"code","a6eaedaf":"code","5a3695ba":"code","8d27c52e":"code","db5241d8":"code","c87136a5":"code","2d95bf17":"code","c6cf59fe":"code","1c70d3b1":"code","32fa7910":"code","67643cab":"code","5da665c0":"code","de7ab802":"code","adaf579a":"code","dc07266c":"code","95e22b8c":"code","7bb231d5":"code","74981582":"code","30e58674":"code","3b8cbdbd":"code","8234239c":"code","c92b5d43":"code","d15f98bb":"code","0e265e1e":"code","53cd7750":"code","3d0289c0":"code","98c38378":"code","82834f47":"code","c32889f4":"code","68fd9475":"code","8c6e3fc0":"code","f3a27b48":"code","fad22fe4":"code","eb29bf89":"code","f8538786":"code","9aa1af00":"code","7e28eefa":"code","a2771e1c":"code","bc0a496b":"code","ccd1b222":"code","d321a5ea":"code","cbaf6b9d":"code","5416a0be":"markdown","6dad49b1":"markdown","f473864a":"markdown","9ad8f994":"markdown","3088b981":"markdown","ad00c062":"markdown","7301958a":"markdown","95dfdf0e":"markdown","24c8387d":"markdown","28aa01af":"markdown","946e0dc2":"markdown"},"source":{"1fe326ca":"from  datetime import datetime, timedelta\nimport gc\nimport numpy as np, pandas as pd\nfrom catboost import Pool, CatBoostRegressor\n#import lightgbm as lgb","db43d873":"import pandas as pd\nimport shap","cc32c32c":"import matplotlib.pyplot as plt","0a8bfe33":"plt.style.use('fivethirtyeight')","a6eaedaf":"pd.options.display.max_columns = 50","5a3695ba":"CAL_DTYPES={\"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n        \"month\": \"int16\", \"year\": \"int16\"}\nPRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }","8d27c52e":"h = 28 \nmax_lags = 57\ntr_last = 1913\nfday = datetime(2016,4, 25) \nfday","db5241d8":"def create_dt(is_train = True, nrows = None, first_day = 1200):\n    prices = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/sell_prices.csv\", dtype = PRICE_DTYPES)\n    for col, col_dtype in PRICE_DTYPES.items():\n        if col_dtype == \"category\":\n            prices[col] = prices[col].cat.codes.astype(\"int16\")\n            prices[col] -= prices[col].min()\n            \n    cal = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/calendar.csv\", dtype = CAL_DTYPES)\n    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n    for col, col_dtype in CAL_DTYPES.items():\n        if col_dtype == \"category\":\n            cal[col] = cal[col].cat.codes.astype(\"int16\")\n            cal[col] -= cal[col].min()\n    \n    start_day = max(1 if is_train  else tr_last-max_lags, first_day)\n    numcols = [f\"d_{day}\" for day in range(start_day,tr_last+1)]\n    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n    dtype = {numcol:\"float32\" for numcol in numcols} \n    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n    dt = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/sales_train_validation.csv\", \n                     nrows = nrows, usecols = catcols + numcols, dtype = dtype)\n    \n    for col in catcols:\n        if col != \"id\":\n            dt[col] = dt[col].cat.codes.astype(\"int16\")\n            dt[col] -= dt[col].min()\n    \n    if not is_train:\n        for day in range(tr_last+1, tr_last+ 28 +1):\n            dt[f\"d_{day}\"] = np.nan\n    \n    dt = pd.melt(dt,\n                  id_vars = catcols,\n                  value_vars = [col for col in dt.columns if col.startswith(\"d_\")],\n                  var_name = \"d\",\n                  value_name = \"sales\")\n    \n    dt = dt.merge(cal, on= \"d\", copy = False)\n    dt = dt.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n    \n    return dt","c87136a5":"## modified from the original\ndef create_fea(dt, lags, windows):\n    print(\"creating window: \", windows, \" lag: \", lags)\n    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n    for lag, lag_col in zip(lags, lag_cols):\n        dt[lag_col] = dt[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n\n    for win in windows :\n        for lag,lag_col in zip(lags, lag_cols):\n            dt[f\"rmean_{lag}_{win}\"] = dt[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean())\n\n\n    ## drop lag col\n    dt.drop(lag_cols, axis=1, inplace=True)\n    \n    \n    date_features = {\n        \n        \"wday\": \"weekday\",\n        \"week\": \"weekofyear\",\n        \"month\": \"month\",\n        \"quarter\": \"quarter\",\n        \"year\": \"year\",\n        \"mday\": \"day\",\n#         \"ime\": \"is_month_end\",\n#         \"ims\": \"is_month_start\",\n    }\n    \n#     dt.drop([\"d\", \"wm_yr_wk\", \"weekday\"], axis=1, inplace = True)\n    \n    for date_feat_name, date_feat_func in date_features.items():\n        if date_feat_name in dt.columns:\n            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n        else:\n            dt[date_feat_name] = getattr(dt[\"date\"].dt, date_feat_func).astype(\"int16\")","2d95bf17":"#FIRST_DAY = 350 # If you want to load all the data set it to '1' -->  Great  memory overflow  risk !\n# I used 350 as the first day \n# To avoid maxing out the memory\nFIRST_DAY = 1000","c6cf59fe":"%%time\n\ndf = create_dt(is_train=True, first_day= FIRST_DAY)\ndf.columns","1c70d3b1":"df.info()","32fa7910":"df.drop(['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI'], axis=1,\\\n       inplace=True)","67643cab":"gc.collect()","5da665c0":"%%time\ncreate_fea(df, lags=[7,14, 28, 30, 50, 60], windows=[7, 28])\ndf.shape","de7ab802":"df.info()","adaf579a":"# X_train.corr()","dc07266c":"df.dropna(inplace = True)\ndf.shape","95e22b8c":"cat_feats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id']\nuseless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\"]\ntrain_cols = df.columns[~df.columns.isin(useless_cols)]\nX_train = df[train_cols]\ny_train = df[\"sales\"]","7bb231d5":"X_train.shape","74981582":"%%time\n\nnp.random.seed(777)\n\nfake_valid_inds = np.random.choice(X_train.index.values, int(X_train.shape[0]*.2), replace = False)\ntrain_inds = np.setdiff1d(X_train.index.values, fake_valid_inds)","30e58674":"fake_valid_inds_2 = np.random.choice(X_train.index.values, int(X_train.shape[0]*.002), replace = False)\n","3b8cbdbd":"import seaborn as sns","8234239c":"plt.figure(figsize=(10, 10))\nsns.heatmap(X_train.loc[fake_valid_inds_2].corr())","c92b5d43":"gc.collect()","d15f98bb":"## load model\nfrom catboost import Pool, CatBoostRegressor\n\nfrom_file = CatBoostRegressor()\nmodel = from_file.load_model('..\/input\/catboost-baseline\/catboost_model_0430')\n","0e265e1e":"%%time\n\n####### I uploaded the model to avoid memory error\ncat_feats_ind = [i for (i, j) in enumerate(X_train.columns) if j in cat_feats]\n\n# # put in pool\ntrain_pool = Pool(X_train.loc[train_inds], y_train.loc[train_inds], cat_features=cat_feats_ind)\ntest_pool = Pool(X_train.loc[fake_valid_inds], y_train.loc[fake_valid_inds], cat_features=cat_feats_ind)\n\n# # train\n# # I used iteration = 80 and depths = 3 at first\n# model =CatBoostRegressor(iterations=80, \\\n#                       depth=3, \\\n#                       learning_rate=1, \\\n#                       loss_function='RMSE',l2_leaf_reg=1)\n\n\n###############################################################\n\n\n#model.fit(train_pool, eval_set=test_pool, verbose=False)\n\n# plot\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(5,10))\nplt.barh(train_pool.get_feature_names(), model.get_feature_importance())\nplt.xticks(rotation=90)\nplt.show()\n","53cd7750":"gc.collect()","3d0289c0":"feature_df = pd.DataFrame({'Feature': X_train.columns, \"Imp\": model.get_feature_importance()})","98c38378":"feature_df.sort_values('Imp', ascending=False)","82834f47":"cat_feats_ind = [i for (i, j) in enumerate(X_train.columns) if j in cat_feats]","c32889f4":"cat_feats_ind","68fd9475":"gc.collect()","8c6e3fc0":"def create_lag_features_for_test(dt, day, lags, windows):\n    # modified the original \n    # create lag feaures just for single day (faster)\n    #lags = [7, 28]\n    lag_cols = [f\"lag_{lag}\" for lag in lags]\n    for lag, lag_col in zip(lags, lag_cols):\n        dt.loc[dt.date == day, lag_col] = \\\n            dt.loc[dt.date ==day-timedelta(days=lag), 'sales'].values  # !!! main\n\n    #windows = [7, 28]\n    for window in windows:\n        for lag in lags:\n            df_window = dt[(dt.date <= day-timedelta(days=lag)) & (dt.date > day-timedelta(days=lag+window))]\n            df_window_grouped = df_window.groupby(\"id\").agg({'sales':'mean'}).reindex(dt.loc[dt.date==day,'id'])\n            dt.loc[dt.date == day,f\"rmean_{lag}_{window}\"] = \\\n                df_window_grouped.sales.values     ","f3a27b48":"def create_date_features_for_test(dt):\n    # copy of the code from `create_dt()` above\n    date_features = {\n        \"wday\": \"weekday\",\n        \"week\": \"weekofyear\",\n        \"month\": \"month\",\n        \"quarter\": \"quarter\",\n        \"year\": \"year\",\n        \"mday\": \"day\",\n    }\n\n    for date_feat_name, date_feat_func in date_features.items():\n        if date_feat_name in dt.columns:\n            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n        else:\n            dt[date_feat_name] = getattr(\n                dt[\"date\"].dt, date_feat_func).astype(\"int16\")","fad22fe4":"%%time\n\nalphas = [1.028, 1.023, 1.018]\nweights = [1\/len(alphas)]*len(alphas)  # equal weights\n\nte0 = create_dt(False)  # create master copy of `te`\ncreate_date_features_for_test (te0)","eb29bf89":"model = model\nfor icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n    te = te0.copy()  # just copy\n    cols = [f\"F{i}\" for i in range(1, 29)]\n\n    for tdelta in range(0, 28):\n        day = fday + timedelta(days=tdelta)\n        print(tdelta, day.date())\n        tst = te[(te.date >= day - timedelta(days=max_lags))\n                 & (te.date <= day)].copy()\n#         create_fea(tst)  # correct, but takes much time\n        create_lag_features_for_test(tst, day, lags=[7], windows=[7])  # faster  \n        tst = tst.loc[tst.date == day, train_cols]\n        \n        ## put tst in pool\n        tst_pool = Pool(data=tst, cat_features=cat_feats_ind)\n        te.loc[te.date == day, \"sales\"] = \\\n            alpha * model.predict(tst_pool)  # magic multiplier by kyakovlev\n\n    te_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()\n\n    te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\n        \"id\"].cumcount()+1]\n    te_sub = te_sub.set_index([\"id\", \"F\"]).unstack()[\n        \"sales\"][cols].reset_index()\n    te_sub.fillna(0., inplace=True)\n    te_sub.sort_values(\"id\", inplace=True)\n    te_sub.reset_index(drop=True, inplace=True)\n    te_sub.to_csv(f\"submission_{icount}.csv\", index=False)\n    if icount == 0:\n        sub = te_sub\n        sub[cols] *= weight\n    else:\n        sub[cols] += te_sub[cols]*weight\n    \nsub2 = sub.copy()\nsub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\nsub = pd.concat([sub, sub2], axis=0, sort=False)\nsub.to_csv(\"submission_catboost_add_iteration_04_27.csv\",index=False)\n#print(icount, alpha, weight)","f8538786":"del sub,sub2","9aa1af00":"gc.collect()","7e28eefa":"shap_values = model.get_feature_importance(tst_pool,type=\"ShapValues\")\nexpected_value = shap_values[0,-1]\nshap_values = shap_values[:,:-1]","a2771e1c":"# we used one of the test data for shap value calculation\nshap.initjs()\nshap.force_plot(expected_value, shap_values[3,:], tst.iloc[3,:])","bc0a496b":"%%time\nshap.summary_plot(shap_values, tst, plot_type='bar')\n","ccd1b222":"feature = ['rmean_7_7', 'rmean_7_28', 'rmean_60_7', 'rmean_50_7', 'rmean_50_28',\\\n           'rmean_14_7','rmean_14_28', 'rmean_28_7']","d321a5ea":"X_train.shape","cbaf6b9d":"model.calc_feature_statistics(X_train[23940000:], y_train[23940000:],feature=feature, plot=True)","5416a0be":"**Most of the code on feature engineering are from this notebook by @kkiller: **\nhttps:\/\/www.kaggle.com\/kneroma\/m5-first-public-notebook-under-0-50\n\nAlso referenced: \n\n\nhttps:\/\/www.kaggle.com\/mayer79\/m5-forecast-poisson-loss\n\n\nhttps:\/\/towardsdatascience.com\/deep-dive-into-catboost-functionalities-for-model-interpretation-7cdef669aeed\n","6dad49b1":"## Goal\n\nThere are multiple solutions here used rolling mean and lags as features and lots of them turned out pretty useful. So what is the optimal rolling mean and lags in this case?\n\nHere I used catboost to build a basic model and analyzed it by \n\n* Feature Importance\n* SHAP value\n* Model analysis plot\n\n(Note: to avoid memory error, a few parameters and data scope has been modified. )\n\n\n## Referenced Notebook ","f473864a":"## Prediction","9ad8f994":"## Model Analysis Plots\n\nCatboost has a special way of plotting features to help you understand **how dispersed each feature has and the discrepancy between predicted value and true value**.\n\nSee this [link](https:\/\/catboost.ai\/docs\/concepts\/python-reference_catboostregressor_calc_feature_statistics.html) for detail explanation.","3088b981":"* Feature Importance: rmean_7_28 > rmean_7_7 > rmean_60_7 > rmean_50_7 > rmean_50_28\n* SHAP value: rmean_7_28 > rmean_7_7 > rmean_14_28 > rmean_28_7 > rmean_14_7\n* model plot analysis: rmean_7_28 > rmean_7_7 > rmean_14_28 > rmean_50_28 > rmean_14_7","ad00c062":"You can see the SHAP value has a different interpretation than the feature importance.","7301958a":"## Catboost","95dfdf0e":"## Summary","24c8387d":"## Split train and validation","28aa01af":"## Shap values\n\nShape values explain how far each feature 'pushes' the target values to larger or smaller values.\n\n\nSee [this link](https:\/\/github.com\/slundberg\/shap) for details.","946e0dc2":"## (Auto)correlation between features"}}