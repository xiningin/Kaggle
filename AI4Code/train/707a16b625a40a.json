{"cell_type":{"3f2a3189":"code","c84a2e7b":"code","a2c67aac":"code","87277e90":"code","0e81b2a6":"code","4b1a8caa":"code","8ab7e071":"code","f3e05e77":"code","f5fd382c":"code","4d0a4120":"code","b99c6ca3":"code","86f55053":"code","c28feb96":"code","b9193d69":"code","758a8032":"code","45b6582d":"code","755984bb":"code","152315a9":"code","062e66e4":"code","08d83d85":"code","82c3d9c3":"code","6ed936c5":"code","6073b9c6":"code","dd98ec36":"code","94a02d9a":"code","2f698bbd":"code","8b26d520":"code","d41cf2ba":"code","da14b16f":"code","7f85f479":"code","116a7e5f":"code","4552a8b4":"code","9b907d6c":"code","d7d6a38a":"code","e2a2e8b4":"code","aa4f4610":"code","08e98be3":"code","bfbe3cb6":"code","e72e2da4":"markdown","ea50fb96":"markdown","28023943":"markdown","42fe0607":"markdown","8328bba9":"markdown","b1275bac":"markdown","fc9cda7e":"markdown","cbecafa4":"markdown","37573400":"markdown","8b85f909":"markdown","dd631c08":"markdown","002be271":"markdown","f16e9a1c":"markdown","a4da5095":"markdown","f4041f57":"markdown","ab9f5420":"markdown","2e550d49":"markdown","fd529254":"markdown","d15a7307":"markdown","6133feea":"markdown","71c2d19d":"markdown"},"source":{"3f2a3189":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score, mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor","c84a2e7b":"train_data = pd.read_csv('..\/input\/gene-expression\/data_set_ALL_AML_train.csv')\ntest_data = pd.read_csv('..\/input\/gene-expression\/data_set_ALL_AML_independent.csv')\nlabels = pd.read_csv('..\/input\/gene-expression\/actual.csv')","a2c67aac":"print(f'columns for train data: {train_data.columns.tolist()}')","87277e90":"train_data.shape","0e81b2a6":"train_data","4b1a8caa":"train_data.info()","8ab7e071":"labels['cancer'].unique()","f3e05e77":"call_values = train_data['call'].unique()\nprint(f'Values for call columns: {call_values}')","f5fd382c":"print(f'The test data shape: {test_data.shape}')","4d0a4120":"test_data","b99c6ca3":"test_data['call'].unique()","86f55053":"# remove columns that contain Call data\ntrain_columns = [col for col in train_data.columns if \"call\" not in col]\ntest_columns = [col for col in test_data.columns if \"call\" not in col]\n\ntrain_X = train_data[train_columns]\ntest_X = test_data[test_columns]","c28feb96":"# transpose the dataframe so that each row is a patient and each column is a gene\ntrain_X = train_X.T\ntest_X = test_X.T\ntrain_X.head()","b9193d69":"train_X.columns","758a8032":"train_X.columns = train_X.iloc[1]\ntrain_X.columns","45b6582d":"# delete Gene Description and Gene Accession Number\ntrain_X = train_X.drop([\"Gene Description\", \"Gene Accession Number\"]).apply(pd.to_numeric)\n\n# do the same for test data\ntest_X.columns = test_X.iloc[1]\ntest_X = test_X.drop([\"Gene Description\", \"Gene Accession Number\"]).apply(pd.to_numeric)","755984bb":"print(f'Shape for train data: {train_X.shape}')\nprint(f'Shape for test data: {test_X.shape}')","152315a9":"label_encoder = LabelEncoder()\nlabels['CancerColumn'] = label_encoder.fit_transform(labels['cancer'])\n\ncancer_types_before_encoding = labels['cancer'].unique()\nprint('\\n--- Cancer types before label encoding: \\n', cancer_types_before_encoding)\n\ncancer_types_after_encoding = labels['CancerColumn'].unique()\nprint('\\n--- Cancer types after label encoding: \\n', cancer_types_after_encoding)","062e66e4":"#drop the 'cancer' column\nlabels = labels.drop('cancer', axis = 1)","08d83d85":"# data sets for training\ntrain_X = train_X.reset_index(drop=True)\ntrain_y = labels[labels.patient <= 38].reset_index(drop=True)\n\n# data sets for testing\ntest_X = test_X.reset_index(drop=True)\ntest_y = labels[labels.patient > 38].reset_index(drop=True)","82c3d9c3":"train_y = train_y.iloc[:,1]\ntrain_y = list(train_y)\ntest_y = test_y.iloc[:,1]\ntest_y = list(test_y)","6ed936c5":"train_y","6073b9c6":"test_y","dd98ec36":"# convert from integer to float\ntrain_X = train_X.astype(float, 64)\ntest_X = test_X.astype(float, 64)\n\nscaler = StandardScaler()\ntrain_X_scaled = scaler.fit_transform(train_X)\ntest_X_scaled = scaler.fit_transform(test_X) ","94a02d9a":"pca = PCA()\ntrain_X_scaled_copy = train_X_scaled\npca.fit_transform(train_X_scaled_copy)","2f698bbd":"total = sum(pca.explained_variance_)\nno_features = 0\ncurrent_variance = 0\nwhile current_variance\/total < 0.90:\n    current_variance += pca.explained_variance_[no_features]\n    no_features = no_features + 1\n    \nprint(f'{no_features} features explain around 90% of the variance')\n\npca = PCA(n_components = no_features)\ntrain_X_pca = pca.fit_transform(train_X)\ntest_X_pca = pca.transform(test_X)\n\nvar_exp = pca.explained_variance_ratio_.cumsum()\nvar_exp = var_exp * 100\nplt.bar(range(no_features), var_exp);","8b26d520":"def train_and_predict_using_dec_tree(train_X, train_y, test_X):\n    regressor = DecisionTreeRegressor(random_state=0)\n    regressor.fit(train_X, train_y)\n    predictions = regressor.predict(test_X)\n    \n    return predictions\n    \n    \n# use PCA \ndef preprocess_data(no_components, train_X_scaled, test_X_scaled):\n    pca = PCA(n_components = no_components)\n    train_X_pca = pca.fit_transform(train_X_scaled)\n    train_X_pca = pd.DataFrame(train_X_pca)\n    \n    test_X_pca = pca.transform(test_X_scaled)\n    test_X_pca = pd.DataFrame(test_X_pca)\n    data = (train_X_pca, test_X_pca)\n    \n    return data","d41cf2ba":"train_X_28, test_X_28 = preprocess_data(28, train_X_scaled, test_X_scaled)","da14b16f":"print('*** Decision tree regressor using 28 features ***')\npredictions1 = train_and_predict_using_dec_tree(train_X_28, train_y, test_X_28)","7f85f479":"predictions1","116a7e5f":"test_y","4552a8b4":"mae1 = mean_absolute_error(test_y, predictions1)\nprint(f'MAE: {mae1}')\n\nacc_score1 = accuracy_score(test_y, predictions1)\nprint(f'Accuracy score: {acc_score1}')","9b907d6c":"train_X_15, test_X_15 = preprocess_data(15, train_X_scaled, test_X_scaled)","d7d6a38a":"print('*** Decision tree regressor using 15 features ***')\npredictions2 = train_and_predict_using_dec_tree(train_X_15, train_y, test_X_15)","e2a2e8b4":"mae2 = mean_absolute_error(test_y, predictions2)\nprint(f'MAE: {mae2}')\n\nacc_score2 = accuracy_score(test_y, predictions2)\nprint(f'Accuracy score: {acc_score2}')","aa4f4610":"train_X_7, test_X_7 = preprocess_data(7, train_X_scaled, test_X_scaled)\nprint('*** Decision tree regressor using 7 features ***')\npredictions3 = train_and_predict_using_dec_tree(train_X_7, train_y, test_X_7)\n\nmae3 = mean_absolute_error(test_y, predictions3)\nprint(f'MAE: {mae3}')\n\nacc_score3 = accuracy_score(test_y, predictions3)\nprint(f'Accuracy score: {acc_score3}')","08e98be3":"train_X_3, test_X_3 = preprocess_data(3, train_X_scaled, test_X_scaled)\nprint('*** Decision tree regressor using 3 features ***')\npredictions4 = train_and_predict_using_dec_tree(train_X_3, train_y, test_X_3)\n\nmae4 = mean_absolute_error(test_y, predictions4)\nprint(f'MAE: {mae4}')\n\nacc_score4 = accuracy_score(test_y, predictions4)\nprint(f'Accuracy score: {acc_score4}')","bfbe3cb6":"train_X_2, test_X_2 = preprocess_data(2, train_X_scaled, test_X_scaled)\nprint('*** Decision tree regressor using 2 features ***')\npredictions5 = train_and_predict_using_dec_tree(train_X_2, train_y, test_X_2)\n\nmae5 = mean_absolute_error(test_y, predictions5)\nprint(f'MAE: {mae5}')\n\nacc_score5 = accuracy_score(test_y, predictions5)\nprint(f'Accuracy score: {acc_score5}')","e72e2da4":"I'll also remove the columns named \"Gene Description\" and \"Gene Accession Number\" both for training and testing data sets.","ea50fb96":"As mentioned earlier, I'll remove the columns starting with \"call\", so the data set will no longer contain categorical values. ","28023943":"#### When using 28, 15 and 7 features, the accuracy was around 0.62. Then I used only 3 features and I obtained the same results as previously obtained, but after using only 2 features, the accuracy decreased. Hence, we can conclude that only 3 features are determinant for the data set.","42fe0607":"### Using 3 features","8328bba9":"A stands for Absent, P stands for Present and M stands for Marginal","b1275bac":"As we can see, the data set has categorical values but only for the columns starting with \"call\". \nI won't use the columns having categorical values, but I'll remove them.","fc9cda7e":"### For 15 features","cbecafa4":"# Load the data sets and explore them","37573400":"### Find the number of the most important features\n\nIn order to find out the most important features, I used [this kernel](https:\/\/www.kaggle.com\/rainbowmiha\/all-aml-classification)","8b85f909":"After label encoding, we have 0 for ALL cancer type and 1 for AML type.","dd631c08":"# Import necessary libraries","002be271":"# Predict using Decision Tree Regressor","f16e9a1c":"# Functions","a4da5095":"## Scale the data sets","f4041f57":"# Principal Component Analysis(PCA)","ab9f5420":"### Using only 7 features","2e550d49":"The data used in this project, also used as a test case in the paper, is gene expression data from patients with acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL). The project's purpose is to train a classifier which would diagnose new patients with one of the two conditions.","fd529254":"# Data preprocessing","d15a7307":"### Using 2 features","6133feea":"As we can see from the shapes of the 2 data sets, in the training data set we have 38 patients and in the test data set we have 34 patients.\nNow I have to assign the right labels for each data set. At the moment, the labels are stored in \"labels\" variable.\nI'll split this one, such that train_y will contain the first 38 columns from 'labels' and test_y will contain\nthe remaining 34 columns. I'll need to reset the index as the indexes of two dataframes need to be the same before I combine them.\n\nBut first, I'll have to convert the values for \"cancer\" column from categorical to numerical values. To do this, I'll use LabelEncoder.","71c2d19d":"### Take the most important 28 features and make predictions"}}