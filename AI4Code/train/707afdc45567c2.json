{"cell_type":{"0b0af18e":"code","9a876746":"code","e103d45d":"code","f16d3621":"code","9df722d3":"code","365caaa1":"code","e5cb35e3":"code","8c76b1cf":"code","2e5f6399":"markdown"},"source":{"0b0af18e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport time, random, os, cv2, ast, glob, numba\nfrom numba import jit\nfrom tqdm.autonotebook import tqdm\nfrom pprint import pprint\nimport sys\n\nimport torch, torchvision\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as T\nfrom torchvision.models.detection.faster_rcnn import AnchorGenerator, FastRCNNPredictor, FasterRCNN\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models import mobilenet_v2, resnet101, vgg19","9a876746":"test_dir = '..\/input\/global-wheat-detection\/test\/'\ntest_df = pd.read_csv('..\/input\/global-wheat-detection\/sample_submission.csv')","e103d45d":"# Dataset class for evaluation.\nclass eval_dataset(Dataset):\n\n    def __init__(self, dataframe, image_dir, transforms=None):\n        super().__init__()\n\n        self.image_ids = dataframe['image_id'].unique()\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        records = self.df[self.df['image_id'] == image_id]\n\n        img = cv2.imread(f'{self.image_dir}\/{image_id}.jpg', cv2.IMREAD_COLOR)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n        img \/= 255.0\n\n        if self.transforms is not None:\n            img = self.transforms(img)\n\n        return img, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","f16d3621":"# collate_fn is called with a list of data samples at each time.\n# It is expected to collate the input samples into a batch for\n# yielding from the data loader iterator.\n# https:\/\/discuss.pytorch.org\/t\/how-to-use-collate-fn\/27181\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\n# Custom dataloader for test data.\ntest_dataset = eval_dataset(test_df, test_dir, transforms=T.Compose([T.ToTensor()]))\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=8, drop_last=False, collate_fn=collate_fn)","9df722d3":"def format_prediction_string(boxes, scores):\n    pred_strings = []\n    for j in zip(scores, boxes):\n        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n\n    return \" \".join(pred_strings)","365caaa1":"def evaluate(device, model):\n    \n    model.eval()\n    detection_threshold = 0.5\n    results = []\n\n    for images, image_ids in test_loader:\n\n        images = list(image.to(device) for image in images)\n        outputs = model(images)\n\n        for i, image in enumerate(images):\n\n            boxes = outputs[i]['boxes'].data.cpu().numpy()\n            scores = outputs[i]['scores'].data.cpu().numpy()\n\n            boxes = boxes[scores >= detection_threshold].astype(np.int32)\n            scores = scores[scores >= detection_threshold]\n            image_id = image_ids[i]\n\n            result = {\n                'image_id': image_id,\n                'PredictionString': format_prediction_string(boxes, scores)\n            }\n\n            results.append(result)\n            \n    return results","e5cb35e3":"DEVICE = torch.device('cuda')\nmodel_path = '..\/input\/saved-model\/frcnn_best_model_epoch_8'\nmodel = torch.load(model_path)\n\nresults = evaluate(DEVICE, model = model) ","8c76b1cf":"test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\ntest_df.to_csv('submission.csv', index=False)","2e5f6399":"This notebook is just for submission purpose.  \n[Pytorch tutorial training notebook is here](https:\/\/www.kaggle.com\/sanchitvj\/global-wheat-detection)"}}