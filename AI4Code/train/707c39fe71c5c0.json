{"cell_type":{"c0bf8ad6":"code","f20b1f23":"code","df224cd1":"code","634f021a":"code","29e8562c":"code","4edc886c":"code","48552c09":"code","cbc34c2b":"code","7c0220ac":"code","9223e106":"code","81aaf3a2":"code","11007b42":"code","35932a6f":"code","54ff1ec5":"code","e9bf3e5b":"code","75700453":"code","32048a09":"code","405741d6":"code","8760c113":"code","85d0b1d3":"code","bf567cf9":"code","a013e20b":"code","8b2f00c8":"code","53d4f0be":"code","f5ed06fb":"code","aece94f0":"markdown","52fd5a5f":"markdown","8054b385":"markdown","3e9185e0":"markdown","1e1a94c8":"markdown","859660c2":"markdown","8f46d7ae":"markdown","6251bb99":"markdown","8e33b4a1":"markdown","11f4d0dd":"markdown","1b63d849":"markdown","8b7f06c4":"markdown","4aa36889":"markdown","32072b80":"markdown","28abffe0":"markdown","c555e4b4":"markdown","e00e965c":"markdown","dd3a88bf":"markdown","a0df812e":"markdown","115e15ee":"markdown","0c179c84":"markdown","ffd8f80e":"markdown","57bc7ddc":"markdown","96073c15":"markdown"},"source":{"c0bf8ad6":"import os\nos.environ['WANDB_SILENT'] = 'True'","f20b1f23":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom transformers import AutoTokenizer\nfrom scipy.special import softmax","df224cd1":"MODEL_NAME = 'distilbert-base-uncased'\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ndef sentence_input(sentence: str, max_len: int = 512, device = 'cpu'):\n    encoded = tokenizer.encode_plus(sentence, add_special_tokens=True, \n                                    pad_to_max_length=True, max_length=max_len, \n                                    return_tensors=\"pt\",).to(device)\n    model_input = (encoded['input_ids'], encoded['attention_mask'])\n    return model_input","634f021a":"test_sentence = \"Super Cute: First of all, I LOVE this product. When I bought it my husband jokingly said that it looked cute and small in the picture, but was really HUGE in real life. Don't tell him I said so, but he was right. It is huge and the cord is really long. Although I wish it was smaller, I still love it. It works really well when we travel and need to plug a lot of things in and although the length is annoying, it's very useful.\"\nmodel_input = sentence_input(test_sentence)","29e8562c":"print(test_sentence)\nprint(model_input)","4edc886c":"import torch.nn as nn\nimport torch","48552c09":"class DistilBert(nn.Module):\n    \"\"\"\n    Simplified version of the same class by HuggingFace.\n    See transformers\/modeling_distilbert.py in the transformers repository.\n    \"\"\"\n\n    def __init__(self, pretrained_model_name: str, num_classes: int = None):\n        \"\"\"\n        Args:\n            pretrained_model_name (str): HuggingFace model name.\n                See transformers\/modeling_auto.py\n            num_classes (int): the number of class labels\n                in the classification task\n        \"\"\"\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(\n             pretrained_model_name)\n\n        self.distilbert = AutoModel.from_pretrained(pretrained_model_name,\n                                                    config=config)\n        self.pre_classifier = nn.Linear(config.dim, config.dim)\n        self.classifier = nn.Linear(config.dim, num_classes)\n        self.dropout = nn.Dropout(config.seq_classif_dropout)\n\n    def forward(self, features, attention_mask=None, head_mask=None):\n        \"\"\"Compute class probabilities for the input sequence.\n\n        Args:\n            features (torch.Tensor): ids of each token,\n                size ([bs, seq_length]\n            attention_mask (torch.Tensor): binary tensor, used to select\n                tokens which are used to compute attention scores\n                in the self-attention heads, size [bs, seq_length]\n            head_mask (torch.Tensor): 1.0 in head_mask indicates that\n                we keep the head, size: [num_heads]\n                or [num_hidden_layers x num_heads]\n        Returns:\n            PyTorch Tensor with predicted class probabilities\n        \"\"\"\n        assert attention_mask is not None, \"attention mask is none\"\n        distilbert_output = self.distilbert(input_ids=features,\n                                            attention_mask=attention_mask,\n                                            head_mask=head_mask)\n        # we only need the hidden state here and don't need\n        # transformer output, so index 0\n        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n        # we take embeddings from the [CLS] token, so again index 0\n        pooled_output = hidden_state[:, 0]  # (bs, dim)\n        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n        logits = self.classifier(pooled_output)  # (bs, dim)\n\n        return logits","cbc34c2b":"from transformers import AutoConfig, AutoTokenizer, AutoModel\nmodel = DistilBert(pretrained_model_name=MODEL_NAME,\n                                           num_classes=2)","7c0220ac":"from catalyst.dl.utils import trace\ndef load_chechpoint(model, path):\n    mod = trace.load_checkpoint(path)\n    model.load_state_dict(mod['model_state_dict'])\n    return model","9223e106":"\n\nmodel = load_chechpoint(model, '..\/input\/sentiment-all-models\/last 0.9622.pth')\n","81aaf3a2":"model.eval()\n\ntraced_cpu = torch.jit.trace(model, model_input)\ntorch.jit.save(traced_cpu, \"cpu.pth\")\n\n#to load\ncpu_model = torch.jit.load(\"cpu.pth\")\n\n# GPU\n# traced_gpu = torch.jit.trace(model.cuda(), gpu_model_input)\n# torch.jit.save(traced_gpu, \"gpu.pth\")\n\n# gpu_model = torch.jit.load(\"gpu.pth\")","11007b42":"print(cpu_model.graph)","35932a6f":"quantized_model = torch.quantization.quantize_dynamic(model)","54ff1ec5":"print(quantized_model)","e9bf3e5b":"!pip install onnx onnxruntime onnxruntime-tools\n#For GPU Inference: install onnxruntime-gpu","75700453":"torch.onnx.export(model, model_input, \"model_512.onnx\",\n                  export_params=True,\n                  input_names=[\"input_ids\", \"attention_mask\"],\n                  output_names=[\"targets\"],\n                  dynamic_axes={\n                      \"input_ids\": {0: \"batch_size\"},\n                      \"attention_mask\": {0: \"batch_size\"},\n                      \"targets\": {0: \"batch_size\"}\n                  },\n                  verbose=True)","32048a09":"import onnx\nonnx_model = onnx.load('model_512.onnx')\nonnx.checker.check_model(onnx_model, full_check=True)\nonnx.helper.printable_graph(onnx_model.graph)","405741d6":"from onnxruntime_tools import optimizer\noptimized_model_512 = optimizer.optimize_model(\"model_512.onnx\", model_type='bert', \n                                               num_heads=12, hidden_size=768,\n                                              use_gpu=False, opt_level=99)\n\noptimized_model_512.save_model_to_file(\"optimized_512.onnx\")","8760c113":"import onnxruntime as ort\nprint(ort.get_device())\nOPTIMIZED_512 = ort.InferenceSession('.\/optimized_512.onnx')","85d0b1d3":"def to_numpy(tensor):\n    if tensor.requires_grad:\n        return tensor.detach().cpu().numpy()\n    return tensor.cpu().numpy()\n\ndef prediction_onnx(model, sentence: str, max_len: int = 512):\n    encoded = tokenizer.encode_plus(sentence, add_special_tokens=True, \n                                    pad_to_max_length=True, max_length=max_len,\n                                    return_tensors=\"pt\",)\n    # compute ONNX Runtime output prediction\n    input_ids = to_numpy(encoded['input_ids'])\n    attention_mask = to_numpy(encoded['attention_mask'])\n    onnx_input = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n    logits = model.run(None, onnx_input)\n    preds = softmax(logits[0][0])\n    print(f\"Class: {['Negative' if preds.argmax() == 0 else 'Positive'][0]}, Probability: {preds.max():.4f}\")","bf567cf9":"prediction_onnx(OPTIMIZED_512, test_sentence)","a013e20b":"df = pd.DataFrame([[506, 273, 151, 89.1, 0],\n                  [507, 263, 145, 82.7, 5.2],\n                  [516, 237, 126, 72.4, 19],\n                  [388, 180, 92.2, 49.7, 56.2]], index = ['Pytorch', 'TorchScript', \n                                                    'ONNX Runtime', 'Quantization'],\n                  columns=['512', '256', '128', '64', \"Av.SpeedUp (%)\"])\ndisplay(df)","8b2f00c8":"cpu = pd.DataFrame([[64, 'Pytorch', 89.1],\n                  [64, 'TorchScript', 82.7],\n                  [64, 'ONNX Runtime', 72.4],\n                  [64, 'Quantization', 49.4],\n                   [128, 'Pytorch', 151],\n                   [256, 'Pytorch', 273],\n                   [512, 'Pytorch', 506],\n                   [128, 'TorchScript', 145],\n                   [256, 'TorchScript', 263],\n                   [512, 'TorchScript', 507],\n                   [128, 'ONNX Runtime', 126],\n                   [256, 'ONNX Runtime', 237],\n                   [512, 'ONNX Runtime', 516],\n                   [128, 'Quantization', 92.2],\n                   [256, 'Quantization', 180],\n                    [512, 'Quantization', 388]],\n                  columns=['Sequence', 'Optimization', 'Time (ms)'])\n\nsns.set_style(\"darkgrid\")\nsns.catplot(x='Optimization', y='Time (ms)', data=cpu, kind='bar',\n            ci=None, col='Sequence', col_wrap=2,\n           col_order=[512,256,128,64]);","53d4f0be":"gpu_df = pd.DataFrame([[16.1, 12.1, 11.9, 11.9, 0],\n                  [15.9, 11.2, 9.2, 8.92, 18],\n                  [14.2, 10, 8.14, 7.57, 35]], index = ['Pytorch', 'TorchScript', \n                                                    'ONNX Runtime'],\n                  columns=['512', '256', '128', '64', \"Av.SpeedUp (%)\"])\ndisplay(gpu_df)","f5ed06fb":"gpu = pd.DataFrame([[64, 'Pytorch', 11.9],\n                  [64, 'TorchScript', 8.92],\n                  [64, 'ONNX Runtime', 7.57],\n                   [128, 'Pytorch', 11.9],\n                   [256, 'Pytorch', 12.1],\n                   [512, 'Pytorch', 16.1],\n                   [128, 'TorchScript', 9.2],\n                   [256, 'TorchScript', 11.2],\n                   [512, 'TorchScript', 15.9],\n                   [128, 'ONNX Runtime', 8.14],\n                   [256, 'ONNX Runtime', 10],\n                   [512, 'ONNX Runtime', 14.2]],\n                  columns=['Sequence', 'Optimization', 'Time (ms)'])\n\nsns.catplot(x='Optimization', y='Time (ms)', data=gpu, kind='bar',\n            ci=None, col='Sequence', col_wrap=2,\n           col_order=[512,256,128,64]);","aece94f0":"For GPU Inference, we can use following methods:\n* change_input_to_int32() - int32 will be used as input, can get better performance.\n* change_input_output_float32_to_float16() - half-precision will be used in computation.\n* convert_model_float32_to_float16() - decreasing model size (255MB -> 128MB)","52fd5a5f":"To export model, first, we need to put our model in eval() mode. Then, provide model_input. Because input size is fixed, we need to specify `dynamic_axis`.\n\n*Note: for 4 sequence lengths - we need 4 different models.*","8054b385":"# NLP Transformers Inference Optimization\n\n![](https:\/\/mk0spaceflightnoa02a.kinstacdn.com\/wp-content\/uploads\/2019\/06\/65025135_2531780803519285_6381814664434548736_o.jpg)","3e9185e0":"# GPU Results","1e1a94c8":"Hello everyone!\n\nIn this notebook, we'll compare performance of our models for inference on CPU and GPU after several optimizations. They all could be applied to a lot of nlp [transformers](https:\/\/github.com\/huggingface\/transformers), including BERT, DistilBERT, RoBERTa, ALBERT, GPT-2, DistilGPT2.\n\nWe\u2019ll take a look on three things that can be done after training to improve inference speed:\n* [TorchScript](https:\/\/pytorch.org\/docs\/stable\/jit.html)\n* [Dynamic Quantization](https:\/\/pytorch.org\/docs\/stable\/quantization.html)\n* [ONNX](https:\/\/pytorch.org\/docs\/stable\/onnx.html) and [ONNX Runntime](https:\/\/github.com\/microsoft\/onnxruntime)","859660c2":"Our model size decreased from 255 to 132 MB. If we calculate the total size of word embedding table ~ 4 (Bytes\/FP32) * 30522(Vocabulary Size) * 768(Embedding Size) = 90 MB. Then the model size reduced from 165 to 42MB (INT8 Model)","8f46d7ae":"# ONNX Runtime","6251bb99":"To check that the model is well formed","8e33b4a1":"Here we can see that quantization gave us the most significant improvement in inference speed. After checking validation accuracy, we can see the drop from 96.22 to 96.03%. It\u2019s not serious considering model size drop and speedup. If we extend maximum sequence lengths further to 32 and 16, then we can observe that speedup ~ 85% in [16, 32, 64, 128].","11f4d0dd":"Post Training Dynamic Quantization: This is the form of quantization where the weights are quantized ahead of time but the activations are dynamically quantized during inference.\n\nDynamic quantization support in PyTorch converts a float model to a quantized model with static int8 or float16 data types for the weights and dynamic quantization for the activations. The activations are quantized dynamically (per batch) to int8 when the weights are quantized to int8.\n\nThe mapping is performed by converting the floating point tensors using:\n\n![](https:\/\/pytorch.org\/docs\/stable\/_images\/math-quantizer-equation.png)","1b63d849":"In order to run the model with ONNX Runtime, we need to create an inference session for the model.","8b7f06c4":"As an example, we\u2019ll use trained DistilBERT model on Amazon review dataset. Training part you can find in this [notebook](https:\/\/www.kaggle.com\/alexalex02\/sentiment-analysis-distilbert-amazon-reviews). Our result was 96.22% accuracy and model size was 255 MB.","4aa36889":"TorchScript is a way to create serializable and optimizable models from PyTorch code. The models can be run independently from Python environment, such as C++.\n\nTo trace our model, we must define model input first. \n\n*Note: For GPU inference we must change device to 'cuda'.*","32072b80":"# TorchScript","28abffe0":"Inference time presented in milliseconds.","c555e4b4":"# Dynamic Quantization","e00e965c":"To optimize our model, we import optimizer. `opt_level` is a proper graph optimization level: 0 - disable all (default), 1 - basic, 2 - extended, 99 - all, `use_gpu` for GPU Inference","dd3a88bf":"### Measurements and Environments","a0df812e":"We'll use batch size of 1 which is useful for online inference. Maximum sequence length - [64, 128, 256, 512]\n\nTime: `%timeit -r 30 -n 3` to provide stable result\n\nKaggle Kernel Setup\n\nCPU: Intel(R) Xeon(R) CPU @ 2.30GHz 4 CPU(s)\n\nGPU: Tesla P100 16GB, Intel(R) Xeon(R) CPU @ 2.00GHz 2 CPU(s)\n","115e15ee":"GPU support isn\u2019t provided for quantization in Pytorch yet.","0c179c84":"Although TorchScript wasn't created for speedup improvement, it still yield solid 20% boost versus non-traced model.\n\nFP16 ONNX model showed us very good performance gains. And there are more optimization available, such as disable\/enable some fusions and GPU support for quantization.","ffd8f80e":"# CPU Results","57bc7ddc":"ONNX provides an open source format for AI models, both deep learning and traditional ML. It defines an extensible computation graph model, as well as definitions of built-in operators and standard data types.\n\nONNX Runtime is a performance-focused engine(written in C++) for ONNX models, which inferences efficiently across multiple platforms and hardware","96073c15":"### Converting model - CPU and GPU"}}