{"cell_type":{"e7156054":"code","817098e3":"code","4f1a8534":"code","47ecde86":"code","a7869d3a":"code","ea991cf3":"code","8c6acd00":"code","930a207d":"code","438634ef":"code","48458e87":"code","287ad0d4":"code","5d8eea17":"code","02ae50c9":"code","440ec5c3":"code","d7d6a344":"code","a2557a9c":"code","359d8287":"code","cc84a133":"code","7b03f6bb":"code","b4c7ba02":"code","3df07d02":"code","40d84507":"code","2a76e4c0":"code","8f5e56bf":"code","0e8903e5":"code","85d26bec":"code","f6cda07c":"code","6c305d9f":"code","98340af7":"code","a2866213":"code","53a21b5c":"code","51fd07e6":"code","9b714193":"code","5aa7dfe8":"code","c2151c35":"code","f91456e8":"code","25e80aa4":"code","dfb63402":"code","daa5c60a":"code","60ffba45":"code","e052c3c9":"code","035b4d44":"code","4eae7222":"code","d25609da":"markdown","ad085b3e":"markdown","3d1201fb":"markdown","dabe8ed7":"markdown","bd51e0d2":"markdown","58d8fef6":"markdown","e318e789":"markdown","47751b72":"markdown","49470156":"markdown","a7850d81":"markdown","04de0169":"markdown","063780f4":"markdown","f7813fdd":"markdown","b6e89e88":"markdown","ef9f4117":"markdown","318f9a57":"markdown","1f778d0e":"markdown","111926a0":"markdown","c848f5d7":"markdown","807e70df":"markdown","9f9f59cc":"markdown","1d0ea20f":"markdown","5c19a91c":"markdown","bdb45b34":"markdown","b9ec18d2":"markdown","b090fd8f":"markdown","cdefa363":"markdown","a2a9d870":"markdown","72392044":"markdown","b8ee9d44":"markdown","572fda45":"markdown","f68fe23c":"markdown","5f8eb6f7":"markdown","39ad1354":"markdown","55b9d35d":"markdown","dd27c18a":"markdown","2283e8b8":"markdown","7545f211":"markdown"},"source":{"e7156054":"!pip install spacy","817098e3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4f1a8534":"import spacy\nnlp = spacy.load('en_core_web_sm')","47ecde86":"introduction_text = ('This tutorial is about Natural'\\\n                     ' Language Processing in Spacy.')\nintroduction_doc = nlp(introduction_text)\n# Extract tokens for the given doc\nprint ([token.text for token in introduction_doc])","a7869d3a":"!echo \"This tutorial is about Natural Language Processing in Spacy.\" >> introduction.txt","ea991cf3":"file_name = 'introduction.txt'\nintroduction_file_text = open(file_name).read()\nintroduction_file_doc = nlp(introduction_file_text)\n# Extract tokens for the given doc\nprint ([token.text for token in introduction_file_doc])","8c6acd00":"about_text = ('Syed Riaz is a Applied AI developer currently' \\\n              ' working for a Indian-based Anuncio' \\\n              ' Technologies. He is interested in exploring' \\\n              ' Natural Language Processing.')\nabout_doc = nlp(about_text)\nsentences = list(about_doc.sents)\nlen(sentences)","930a207d":"for sentence in sentences:\n    print (sentence)","438634ef":"def set_custom_boundaries(doc):\n    # Adds support to use `...` as the delimiter for sentence detection\n    for token in doc[:-1]:\n        if token.text == '...':\n            doc[token.i+1].is_sent_start = True\n    return doc\n\nellipsis_text = ('Syed, can you, ... never mind, I forgot' \\\n                 ' what I was saying. So, do you think' \\\n                 ' we should ...')\n# Load a new model instance\ncustom_nlp = spacy.load('en_core_web_sm')\ncustom_nlp.add_pipe(set_custom_boundaries, before='parser')\ncustom_ellipsis_doc = custom_nlp(ellipsis_text)\ncustom_ellipsis_sentences = list(custom_ellipsis_doc.sents)\nfor sentence in custom_ellipsis_sentences:\n    print(sentence)","48458e87":"# Sentence Detection with no customization\nellipsis_doc = nlp(ellipsis_text)\nellipsis_sentences = list(ellipsis_doc.sents)\nfor sentence in ellipsis_sentences:\n    print(sentence)","287ad0d4":"for token in about_doc:\n    print (token, token.idx)","5d8eea17":"for token in about_doc:\n    print (token, token.idx, token.text_with_ws,\n           token.is_alpha, token.is_punct, token.is_space,\n           token.shape_, token.is_stop)","02ae50c9":"import re\nimport spacy\nfrom spacy.tokenizer import Tokenizer\ncustom_nlp = spacy.load('en_core_web_sm')\nprefix_re = spacy.util.compile_prefix_regex(custom_nlp.Defaults.prefixes)\nsuffix_re = spacy.util.compile_suffix_regex(custom_nlp.Defaults.suffixes)\ninfix_re = re.compile(r'''[-~]''')\ndef customize_tokenizer(nlp):\n    # Adds support to use `-` as the delimiter for tokenization\n    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n                     suffix_search=suffix_re.search,\n                     infix_finditer=infix_re.finditer,\n                     token_match=None\n                    )\n\ncustom_nlp.tokenizer = customize_tokenizer(custom_nlp)\ncustom_tokenizer_about_doc = custom_nlp(about_text)\nprint([token.text for token in custom_tokenizer_about_doc])","440ec5c3":"import spacy\nspacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\nlen(spacy_stopwords)","d7d6a344":"for stop_word in list(spacy_stopwords)[:10]:\n    print(stop_word)","a2557a9c":"for token in about_doc:\n    if not token.is_stop:\n        print (token)","359d8287":"about_no_stopword_doc = [token for token in about_doc if not token.is_stop]\nprint (about_no_stopword_doc)","cc84a133":"conference_help_text = ('Syed Riaz is helping organize a developer'\n                        ' conference on Applications of Natural Language'\n                        ' Processing. He keeps organizing local AI meetups'\n                        ' and several internal talks at his workplace.')\nconference_help_doc = nlp(conference_help_text)\nfor token in conference_help_doc:\n    print (token, token.lemma_)","7b03f6bb":"from collections import Counter\ncomplete_text = ('Syed Riaz is a Applied AI research engineer currently'\n                 'working for a Bangalore-based Anuncio Technologies. He is'\n                 ' interested in exploring Natural Language Processing.'\n                 ' There is a developer conference happening on 13 March'\n                 ' 2020 in Bangalore. It is titled \"Applications of Natural'\n                 ' Language Processing\". There is a helpline number '\n                 ' available at +1-1234567891. Syed is helping organize it.'\n                 ' He keeps organizing local AI meetups and several'\n                 ' internal talks at his workplace. Syed is also presenting'\n                 ' a talk. The talk will introduce the reader about \"Use'\n                 ' cases of Natural Language Processing in AI industry\".'\n                 ' Apart from his work, he is very passionate about travelling.'\n                 ' Syed would like to see whole world. He has planned '\n                 ' to travel different countries one at a time.'\n                 ' He is also planning to create travel videos'\n                 ' for which he is planning to join a film making course.')\n\ncomplete_doc = nlp(complete_text)\n# Remove stop words and punctuation symbols\nwords = [token.text for token in complete_doc\n         if not token.is_stop and not token.is_punct]\n\nword_freq = Counter(words)\n\n# 5 commonly occurring words with their frequencies\ncommon_words = word_freq.most_common(5)\nprint (common_words)","b4c7ba02":"# Unique words\nunique_words = [word for (word, freq) in word_freq.items() if freq == 1]\nprint (unique_words)","3df07d02":"words_all = [token.text for token in complete_doc if not token.is_punct]\nword_freq_all = Counter(words_all)\n# 5 commonly occurring words with their frequencies\ncommon_words_all = word_freq_all.most_common(5)\nprint (common_words_all)","40d84507":"for token in about_doc:\n    print (token, token.tag_, token.pos_, spacy.explain(token.tag_))","2a76e4c0":"nouns = []\nadjectives = []\nfor token in about_doc:\n    if token.pos_ == 'NOUN':\n        nouns.append(token)\n    if token.pos_ == 'ADJ':\n        adjectives.append(token)\n\nprint(nouns)\nprint(adjectives)","8f5e56bf":"from spacy import displacy\nabout_interest_text = ('He is interested in learning'\n                       ' Natural Language Processing.')\nabout_interest_doc = nlp(about_interest_text)\ndisplacy.render(about_interest_doc, style='dep', jupyter=True)\n","0e8903e5":"def is_token_allowed(token):\n    '''\n    Only allow valid tokens which are not stop words\n    and punctuation symbols.\n    '''\n    if (not token or not token.string.strip() or\n        token.is_stop or token.is_punct):\n        return False\n    return True\n\ndef preprocess_token(token):\n    # Reduce token to its lowercase lemma form\n    return token.lemma_.strip().lower()\n\ncomplete_filtered_tokens = [preprocess_token(token)\n                            for token in complete_doc if is_token_allowed(token)]\nprint(complete_filtered_tokens)","85d26bec":"from spacy.matcher import Matcher\nmatcher = Matcher(nlp.vocab)\n\ndef extract_full_name(nlp_doc):\n    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n    matcher.add('FULL_NAME', None, pattern)\n    matches = matcher(nlp_doc)\n    for match_id, start, end in matches:\n        span = nlp_doc[start:end]\n        return span.text\n\nextract_full_name(about_doc)","f6cda07c":"from spacy.matcher import Matcher\n\nmatcher = Matcher(nlp.vocab)\n\nconference_org_text = ('There is a developer conference'\n                       'happening on 13 March 2020 in Bangalore. It is titled'\n                       ' \"Applications of Natural Language Processing\".'\n                       ' There is a helpline number available'\n                       ' at (123) 456-789')\n\ndef extract_phone_number(nlp_doc):\n    pattern = [{'ORTH': '('}, {'SHAPE': 'ddd'},\n               {'ORTH': ')'}, {'SHAPE': 'ddd'},\n               {'ORTH': '-', 'OP': '?'},\n               {'SHAPE': 'ddd'}]\n    matcher.add('PHONE_NUMBER', None, pattern)\n    matches = matcher(nlp_doc)\n    for match_id, start, end in matches:\n        span = nlp_doc[start:end]\n        return span.text\n\nconference_org_doc = nlp(conference_org_text)\nextract_phone_number(conference_org_doc)","6c305d9f":"travel_text = 'Syed is planning to travel planet earth.'\ntravel_doc = nlp(travel_text)\nfor token in travel_doc:\n    print (token.text, token.tag_, token.head.text, token.dep_)","98340af7":"#displacy.serve(travel_doc, style='dep')\ndisplacy.render(travel_doc, style='dep', jupyter=True)","a2866213":"one_line_about_text = ('Syed Riaz is a Applied AI research engineer'\n                       ' currently working for a Bangalore-based Anuncio Technologies')\n\none_line_about_doc = nlp(one_line_about_text)\n# Extract children of `engineer`\nprint([token.text for token in one_line_about_doc[7].children])","53a21b5c":"# Extract previous neighboring node of `engineer`\nprint (one_line_about_doc[7].nbor(-1))","51fd07e6":"# Extract next neighboring node of `engineer`\nprint (one_line_about_doc[7].nbor())","9b714193":"# Extract all tokens on the left of `engineer`\nprint([token.text for token in one_line_about_doc[7].lefts])","5aa7dfe8":"# Extract tokens on the right of `engineer`\nprint([token.text for token in one_line_about_doc[7].rights])","c2151c35":"# Print subtree of `engineer`\nprint (list(one_line_about_doc[7].subtree))","f91456e8":"def flatten_tree(tree):\n    return ''.join([token.text_with_ws for token in list(tree)]).strip()\n\n# Print flattened subtree of `engineer`\nprint (flatten_tree(one_line_about_doc[7].subtree))","25e80aa4":"conference_text = ('There is a AI developer conference'\n                   ' happening on 13 March 2020 in Bangalore.')\nconference_doc = nlp(conference_text)\n# Extract Noun Phrases\nfor chunk in conference_doc.noun_chunks:\n    print (chunk)","dfb63402":"!pip install textacy","daa5c60a":"import textacy\nabout_talk_text = ('The talk will introduce reader about Use'\n                   ' cases of Natural Language Processing in'\n                   ' AI industry')\npattern = r'(<VERB>?<ADV>*<VERB>+)'\nabout_talk_doc = textacy.make_spacy_doc(about_talk_text,\n                                        lang='en_core_web_sm')\nverb_phrases = textacy.extract.pos_regex_matches(about_talk_doc, pattern)\n# Print all Verb Phrase\nfor chunk in verb_phrases:\n    print(chunk.text)","60ffba45":"# Extract Noun Phrase to explain what nouns are involved\nfor chunk in about_talk_doc.noun_chunks:\n    print (chunk)","e052c3c9":"anuncio_class_text = ('Anuncio Technologies is situated'\n                      ' near Manyatha Tech Park or the City of Bangalore and has'\n                      ' world-class AI developers.')\nanuncio_class_doc = nlp(anuncio_class_text)\nfor ent in anuncio_class_doc.ents:\n    print(ent.text, ent.start_char, ent.end_char,\n          ent.label_, spacy.explain(ent.label_))","035b4d44":"#displacy.serve(anuncio_class_doc, style='ent')\ndisplacy.render(anuncio_class_doc, style='dep', jupyter=True)","4eae7222":"survey_text = ('Out of 5 people surveyed, Syed Riaz,'\n               ' Satyadev Shetty and Praveen Kumar like'\n               ' apples. Gourav Sinha and Vikrant Dharmshi'\n               ' like oranges.')\n\ndef replace_person_names(token):\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON':\n        return '[REDACTED] '\n    return token.string\n\ndef redact_names(nlp_doc):\n    for ent in nlp_doc.ents:\n        ent.merge()\n        tokens = map(replace_person_names, nlp_doc)\n        return ''.join(tokens)\n\nsurvey_doc = nlp(survey_text)\nredact_names(survey_doc)","d25609da":"# Named Entity Recognition\nNamed Entity Recognition (NER) is the process of locating named entities in unstructured text and then classifying them into pre-defined categories, such as person names, organizations, locations, monetary values, percentages, time expressions, and so on.\n\nYou can use NER to know more about the meaning of your text. For example, you could use it to populate tags for a set of documents in order to improve the keyword search. You could also use it to categorize customer support tickets into relevant categories.\n\nspaCy has the property ents on Doc objects. You can use it to extract named entities","ad085b3e":"Here, two attributes of the Token class are accessed:\n\n- tag_ lists the fine-grained part of speech.\n- pos_ lists the coarse-grained part of speech.\n\nspacy.explain gives descriptive details about a particular POS tag. spaCy provides a [complete tag list](https:\/\/spacy.io\/api\/annotation#pos-tagging) along with an explanation for each tag.\n\nUsing POS tags, you can extract a particular category of words","3d1201fb":"Note that the complete_filtered_tokens does not contain any stop word or punctuation symbols and consists of lemmatized lowercase tokens.\n\n# Rule-Based Matching Using spaCy\nRule-based matching is one of the steps in extracting information from unstructured text. It\u2019s used to identify and extract tokens and phrases according to patterns (such as lowercase) and grammatical features (such as part of speech).\n\nRule-based matching can use [regular expressions](https:\/\/en.wikipedia.org\/wiki\/Regular_expression) to extract entities (such as phone numbers) from an unstructured text. It\u2019s different from extracting text using regular expressions only in the sense that regular expressions don\u2019t consider the lexical and grammatical attributes of the text.\n\nWith rule-based matching, you can extract a first name and a last name, which are always proper nouns","dabe8ed7":"What Are NLP and spaCy?\nNLP is a subfield of Artificial Intelligence and is concerned with interactions between computers and human languages. NLP is the process of analyzing, understanding, and deriving meaning from human languages for computers.\n\nNLP helps you extract insights from unstructured text and has several use cases, such as:\n\n- Automatic summarization\n- Named entity recognition\n- Question answering systems\n- Sentiment analysis\n\nspaCy is a free, open-source library for NLP in Python. It\u2019s written in Cython and is designed to build information extraction or natural language understanding systems. It\u2019s built for production use and provides a concise and user-friendly API.","bd51e0d2":"You can remove stop words from the input text:","58d8fef6":"Note that custom_ellipsis_sentences contain three sentences, whereas ellipsis_sentences contains two sentences. These sentences are still obtained via the sents attribute, as you saw before.\n\n# Tokenization in spaCy\nTokenization is the next step after sentence detection. It allows you to identify the basic units in your text. These basic units are called tokens. Tokenization is useful because it breaks a text into meaningful units. These units are used for further analysis, like part of speech tagging.\n\nIn spaCy, you can print tokens by iterating on the Doc object:","e318e789":"Now that you have textacy installed, you can use it to extract verb phrases based on grammar rules","47751b72":"# Using spaCy\nIn this section, you\u2019ll use spaCy for a given input string and a text file. Load the language model instance in spaCy:","49470156":"Stop words like is, a, for, the, and in are not printed in the output above. You can also create a list of tokens not containing stop words:","a7850d81":"In this example, only the pattern is updated in order to match phone numbers from the previous example. Here, some attributes of the token are also used:\n\n- ORTH gives the exact text of the token.\n- SHAPE transforms the token string to show orthographic features.\n- OP defines operators. Using ? as a value means that the pattern is optional, meaning it can match 0 or 1 times.\n\n**Note**: For simplicity, phone numbers are assumed to be of a particular format: (123) 456-789. You can change this depending on your use case.\n\nRule-based matching helps you identify and extract tokens and phrases according to lexical patterns (such as lowercase) and grammatical features(such as part of speech).\n\n# Dependency Parsing Using spaCy\nDependency parsing is the process of extracting the dependency parse of a sentence to represent its grammatical structure. It defines the dependency relationship between headwords and their dependents. The head of a sentence has no dependency and is called the root of the sentence. The verb is usually the head of the sentence. All other words are linked to the headword.\n\nThe dependencies can be mapped in a directed graph representation:\n\n- Words are the nodes.\n- The grammatical relationships are the edges.\n\nDependency parsing helps you know what role a word plays in the text and how different words relate to each other. It\u2019s also used in shallow parsing and named entity recognition.\n\nHere\u2019s how you can use dependency parsing to see the relationships between words","04de0169":"# Sentence Detection\nSentence Detection is the process of locating the start and end of sentences in a given text. This allows you to you divide a text into linguistically meaningful units. You\u2019ll use these units when you\u2019re processing your text to perform tasks such as part of speech tagging and entity extraction.\n\nIn spaCy, the sents property is used to extract sentences. Here\u2019s how you would extract the total number of sentences and the sentences for a given input text","063780f4":"In the above example, notice how the text is converted to an object that is understood by spaCy. You can use this method to convert any text into a processed Doc object and deduce attributes, which will be covered in the coming sections.\n\n# How to Read a Text File\nIn this section, you\u2019ll create a processed Doc object for a text file:","f7813fdd":"You can use NER to redact people\u2019s names from a text. For example, you might want to do this in order to hide personal information collected in a survey. You can use spaCy to do that","b6e89e88":"Table of Contents\n\n- What Are NLP and spaCy?\n- Installation\n- How to Install spaCy\n- How to Download Models and Data\n- Using spaCy\n- How to Read a String\n- How to Read a Text File\n- Sentence Detection\n- Tokenization in spaCy\n- Stop Words\n- Lemmatization\n- Word Frequency\n- Part of Speech Tagging\n- Visualization: Using displaCy\n- Preprocessing Functions\n- Rule-Based Matching Using spaCy\n- Dependency Parsing Using spaCy\n- Navigating the Tree and Subtree\n- Shallow Parsing\n- Noun Phrase Detection\n- Verb Phrase Detection\n- Named Entity Recognition\n- Conclusion","ef9f4117":"You can use this to derive insights, remove the most common nouns, or see which adjectives are used for a particular noun.\n\n# Visualization: Using displaCy\nspaCy comes with a built-in visualizer called displaCy. You can use it to visualize a dependency parse or named entities in a browser or a Jupyter notebook.\n\nYou can use displaCy to find POS tags for tokens:","318f9a57":"In order for you to customize, you can pass various parameters to the Tokenizer class:\n\n- nlp.vocab is a storage container for special cases and is used to handle cases like contractions and emoticons.\n- prefix_search is the function that is used to handle preceding punctuation, such as opening parentheses.\n- infix_finditer is the function that is used to handle non-whitespace separators, such as hyphens.\n- suffix_search is the function that is used to handle succeeding punctuation, such as closing parentheses.\n- token_match is an optional boolean function that is used to match strings that should never be split. It overrides the previous rules and is useful for entities like URLs or numbers.\n\nNote: spaCy already detects hyphenated words as individual tokens. The above code is just an example to show how tokenization can be customized. It can be used for any other character.\n\n# Stop Words\nStop words are the most common words in a language. In the English language, some examples of stop words are the, are, but, and they. Most sentences need to contain stop words in order to be full sentences that make sense.\n\nGenerally, stop words are removed because they aren\u2019t significant and distort the word frequency analysis. spaCy has a list of stop words for the English language","1f778d0e":"You can use this function to print all the tokens in a subtree.\n\n# Shallow Parsing\nShallow parsing, or chunking, is the process of extracting phrases from unstructured text. Chunking groups adjacent tokens into phrases on the basis of their POS tags. There are some standard well-known chunks such as noun phrases, verb phrases, and prepositional phrases.\n\n# Noun Phrase Detection\nA noun phrase is a phrase that has a noun as its head. It could also include other kinds of words, such as adjectives, ordinals, determiners. Noun phrases are useful for explaining the context of the sentence. They help you infer what is being talked about in the sentence.\n\nspaCy has the property noun_chunks on Doc object. You can use it to extract noun phrases","111926a0":"In this example, the verb phrase introduce indicates that something will be introduced. By looking at noun phrases, you can see that there is a talk that will introduce the reader to use cases of Natural Language Processing or AI industry.\n\nNote: In the previous example, you could have also done dependency parsing to see what the [relationships](https:\/\/nlp.stanford.edu\/software\/dependencies_manual.pdf) between the words were.","c848f5d7":"Four out of five of the most common words are stop words, which don\u2019t tell you much about the text. If you consider stop words while doing word frequency analysis, then you won\u2019t be able to derive meaningful insights from the input text. This is why removing stop words is so important.\n\n# Part of Speech Tagging\nPart of speech or POS is a grammatical role that explains how a particular word is used in a sentence. There are eight parts of speech:\n\n1. Noun\n2. Pronoun\n3. Adjective\n4. Verb\n5. Adverb\n6. Preposition\n7. Conjunction\n8. Interjection\n\nPart of speech tagging is the process of assigning a POS tag to each token depending on its usage in the sentence. POS tags are useful for assigning a syntactic category like noun or verb to each word.\n\nIn spaCy, POS tags are available as an attribute on the Token object:","807e70df":"In the above example, ent is a Span object with various attributes:\n\n- text gives the Unicode text representation of the entity.\n- start_char denotes the character offset for the start of the entity.\n- end_char denotes the character offset for the end of the entity.\n\nlabel_ gives the label of the entity.\nspacy.explain gives descriptive details about an entity label. The spaCy model has a pre-trained [list of entity classe](https:\/\/spacy.io\/api\/annotation#named-entities)s. You can use displaCy to visualize these entities","9f9f59cc":"You can construct a function that takes a subtree as an argument and returns a string by merging words in it:","1d0ea20f":"In this example, some of the commonly required attributes are accessed:\n\n- text_with_ws prints token text with trailing space (if present).\n- is_alpha detects if the token consists of alphabetic characters or not.\n- is_punct detects if the token is a punctuation symbol or not.\n- is_space detects if the token is a space or not.\n- shape_ prints out the shape of the word.\n- is_stop detects if the token is a stop word or not.\n\nNote: You\u2019ll learn more about stop words in the next section.\n\nYou can also customize the tokenization process to detect tokens on custom characters. This is often used for hyphenated words, which are words joined with hyphen. For example, \u201cIndian-based\u201d is a hyphenated word.\n\nspaCy allows you to customize tokenization by updating the tokenizer property on the nlp object:","5c19a91c":"In this example, replace_person_names() uses ent_iob. It gives the IOB code of the named entity tag using [inside-outside-beginning (IOB) tagging](https:\/\/en.wikipedia.org\/wiki\/Inside%E2%80%93outside%E2%80%93beginning_(tagging)). Here, it can assume a value other than zero, because zero means that no entity tag is set.\n\n# Conclusion\nspaCy is a powerful and advanced library that is gaining huge popularity for NLP applications due to its speed, ease of use, accuracy, and extensibility. Congratulations! You now know:\n\n- What the foundational terms and concepts in NLP are\n- How to implement those concepts in spaCy\n- How to customize and extend built-in functionalities in spaCy\n- How to perform basic statistical analysis on a text\n- How to create a pipeline to process unstructured text\n- How to parse a sentence and extract meaningful insights from it","bdb45b34":"# Verb Phrase Detection\nA verb phrase is a syntactic unit composed of at least one verb. This verb can be followed by other chunks, such as noun phrases. Verb phrases are useful for understanding the actions that nouns are involved in.\n\nspaCy has no built-in functionality to extract verb phrases, so you\u2019ll need a library called textacy:\n\nNote:\n\nYou can use pip to install textacy:","b9ec18d2":"In this example, the sentence contains three relationships:\n\n1. nsubj is the subject of the word. Its headword is a verb.\n2. aux is an auxiliary word. Its headword is a verb.\n3. dobj is the direct object of the verb. Its headword is a verb.\n\nThere is a detailed [list of relationships](https:\/\/nlp.stanford.edu\/software\/dependencies_manual.pdf) with descriptions. You can use displaCy to visualize the dependency tree:","b090fd8f":"# Installation\nIn this section, you\u2019ll install spaCy and then download data and models for the English language.\n\n# How to Install spaCy\nspaCy can be installed using pip, a Python package manager. You can use a virtual environment to avoid depending on system-wide packages.\n\nCreate a new virtual environment:","cdefa363":"Note how spaCy preserves the starting index of the tokens. It\u2019s useful for in-place word replacement. spaCy provides various attributes for the Token class:","a2a9d870":"spaCy is a free and open-source library for Natural Language Processing (NLP) in Python with a lot of in-built capabilities. It\u2019s becoming increasingly popular for processing and analyzing data in NLP. Unstructured textual data is produced at a large scale, and it\u2019s important to process and derive insights from unstructured data. To do that, you need to represent the data in a format that can be understood by computers. NLP can help you do that.\n\nIn this tutorial, we\u2019ll learn:\n\n- What the foundational terms and concepts in NLP are\n- How to implement those concepts in spaCy\n- How to customize and extend built-in functionalities in spaCy\n- How to perform basic statistical analysis on a text\n- How to create a pipeline to process unstructured text\n- How to parse a sentence and extract meaningful insights from it","72392044":"By looking at noun phrases, you can get information about your text. For example, a AI developer conference indicates that the text mentions a conference, while the date 13 Marchlets you know that conference is scheduled for 13 March. You can figure out whether the conference is in the past or the future. Bangalore tells you that the conference is in Bangalore.","b8ee9d44":"# Preprocessing Functions\nYou can create a preprocessing function that takes text as input and applies the following operations:\n\n- Lowercases the text\n- Lemmatizes each token\n- Removes punctuation symbols\n- Removes stop words\n\nA preprocessing function converts text to an analyzable format. It\u2019s necessary for most NLP tasks. Here\u2019s an example","572fda45":"about_no_stopword_doc can be joined with spaces to form a sentence with no stop words.\n\n# Lemmatization\nLemmatization is the process of reducing inflected forms of a word while still ensuring that the reduced form belongs to the language. This reduced form or root word is called a lemma.\n\nFor example, organizes, organized and organizing are all forms of organize. Here, organize is the lemma. The inflection of a word allows you to express different grammatical categories like tense (organized vs organize), number (trains vs train), and so on. Lemmatization is necessary because it helps you reduce the inflected forms of a word so that they can be analyzed as a single item. It can also help you normalize the text.\n\nspaCy has the attribute lemma_ on the Token class. This attribute has the lemmatized form of a token:","f68fe23c":"In the above example, spaCy is correctly able to identify sentences in the English language, using a full stop(.) as the sentence delimiter. You can also customize the sentence detection to detect sentences on custom delimiters.\n\nHere\u2019s an example, where an ellipsis(...) is used as the delimiter","5f8eb6f7":"In this example, pattern is a list of objects that defines the combination of tokens to be matched. Both POS tags in it are PROPN (proper noun). So, the pattern consists of two objects in which the POS tags for both tokens should be PROPN. This pattern is then added to Matcher using FULL_NAME and the the match_id. Finally, matches are obtained with their starting and end indexes.\n\nYou can also use rule-based matching to extract phone numbers:","39ad1354":"This is how you can convert a text file into a processed Doc object.\n\nNote:\n\nYou can assume that:\n\n- Variable names ending with the suffix _text are Unicode string objects.\n- Variable name ending with the suffix _doc are spaCy\u2019s language model objects.","55b9d35d":"Here, the nlp object is a language model instance. You can assume that, throughout this tutorial, nlp refers to the language model loaded by en_core_web_sm. Now you can use spaCy to read a string or a text file.\n\n# How to Read a String\nYou can use spaCy to create a processed Doc object, which is a container for accessing linguistic annotations, for a given input string:","dd27c18a":"By looking at the common words, you can see that the text as a whole is probably about Syed, Bangalore, or Natural Language Processing. This way, you can take any unstructured text and perform statistical analysis to know what it\u2019s about.\n\nHere\u2019s another example of the same text with stop words","2283e8b8":"In this example, organizing reduces to its lemma form organize. If you do not lemmatize the text, then organize and organizing will be counted as different tokens, even though they both have a similar meaning. Lemmatization helps you avoid duplicate words that have similar meanings.\n\n# Word Frequency\nYou can now convert a given text into tokens and perform statistical analysis over it. This analysis can give you various insights about word patterns, such as common words or unique words in the text:","7545f211":"# Navigating the Tree and Subtree\nThe dependency parse tree has all the properties of a [tree](https:\/\/en.wikipedia.org\/wiki\/Tree_(data_structure)). This tree contains information about sentence structure and grammar and can be traversed in different ways to extract relationships.\n\nspaCy provides attributes like children, lefts, rights, and subtree to navigate the parse tree:"}}