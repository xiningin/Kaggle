{"cell_type":{"cbff8e16":"code","6ea12367":"code","86cc1e88":"code","3ec0cd2f":"code","58a96792":"code","15f774b9":"code","74a61da8":"code","4a6b37cb":"code","3ae32f69":"code","8990b108":"code","6311cdbd":"code","530fcde2":"code","2d7e3814":"code","b5c82110":"code","0a42d758":"code","7f79da16":"code","ae9efec4":"code","d43def62":"code","aaaac730":"code","8a36d378":"code","6e9b13b8":"code","f91987fa":"code","51dcb5a6":"code","891bc93f":"code","53b1ad43":"code","72508d8b":"code","9052b05d":"code","45581539":"code","806e7734":"code","b046a10e":"code","6fbd44ae":"code","307f2e4c":"code","30835103":"code","fca5d291":"code","fa0c72f4":"code","a062cc57":"code","18cd5dff":"code","fc530e6e":"code","90e1be7c":"code","ef82968a":"code","6525b850":"code","2c197038":"code","66b231d6":"code","3e7bd85a":"code","6ec0e07e":"code","2af6f716":"code","99ce8655":"code","a4d6af18":"code","ebad6615":"code","ce82dbda":"code","65e9b4b6":"code","ee15412d":"code","d0732754":"code","f3bbc718":"code","c5a9633a":"code","4071cdc0":"code","c36a8c79":"markdown","f686f4e6":"markdown","39625633":"markdown","6c7484cd":"markdown","ce39ef9d":"markdown","718922a6":"markdown","22829ae9":"markdown","4c37b472":"markdown","59f8a315":"markdown","5fad4047":"markdown","d3ce36ef":"markdown","af2bef74":"markdown","105b95fa":"markdown","2c91cf57":"markdown","6a1b7bfc":"markdown","0d3da22a":"markdown"},"source":{"cbff8e16":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nsns.set_style('whitegrid')\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:80% !important; }<\/style>\"))\n\nimport Geohash as geo\nimport matplotlib.ticker as plticker\nfrom datetime import timedelta\n\n## for preprocessing and machine learning\nfrom sklearn.cluster import KMeans, k_means\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport sklearn.linear_model as linear_model\nfrom sklearn.preprocessing import MinMaxScaler\n\n## for Deep-learing:\nimport keras\nfrom keras import models\nfrom keras import layers\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout","6ea12367":"df = pd.read_csv('..\/input\/training.csv')\ndf.head()","86cc1e88":"df.info() #no null values","3ec0cd2f":"plt.figure(figsize=(10,6))\nsns.distplot(df['demand'], label='demand')\n\n#demand follows a lognormal distribution","58a96792":"fig, ax = plt.subplots(figsize=(20,6))\ndd_ts = df.groupby('day')['demand'].sum().reset_index()\nsns.lineplot(x='day', y='demand', data=dd_ts, ax=ax)\nplt.show()\n#day trend shows demand growing with cyclical effects","15f774b9":"from datetime import timedelta\ndf['day_time'] = df['day'].astype('str') +':'+ df['timestamp']\ndf['day_time2'] = df['day_time'].apply(lambda x: x.split(':'))\ndf['day_time3'] = df['day_time2'].apply(lambda x: timedelta(days=int(x[0]),hours=int(x[1]),minutes=int(x[2])))\ndf['dum_time'] = pd.Timestamp(2019,1,1).normalize() + df['day_time3']\ndf.drop(['day_time','day_time2','day_time3'],axis=1,inplace=True) # drop irrelevant columns\ndf.head()","74a61da8":"df['time'] = df['dum_time'].dt.time\ndf['hour'] = df['dum_time'].dt.hour\ndf['minute'] = df['dum_time'].dt.minute","4a6b37cb":"fig, ax = plt.subplots(figsize=(20,10))\ndd_ts = df.groupby('hour')['demand'].sum().reset_index()\nsns.lineplot(x='hour', y='demand', data=dd_ts, ax=ax)\nloc = plticker.MultipleLocator() # this locator puts ticks at regular intervals\nax.xaxis.set_major_locator(loc)\nplt.show()\n#time trend shows peak in the morning and trough at 1900-2000hrs","3ae32f69":"daycycle_dict1 = {}\nfor i in range(1,8):\n    j = i\n    while j <= 61:\n        daycycle_dict1[j] = i\n        j+=7\ndf['daycycle'] = df['day'].apply(lambda x: daycycle_dict1[x])","8990b108":"fig, ax = plt.subplots(figsize=(20,10))\ndd_ts = df.groupby('daycycle')['demand'].sum().reset_index()\nsns.lineplot(x='daycycle', y='demand', data=dd_ts, ax=ax)\nplt.show()\n#Each day of the week has different demand. The 5th & 6th days could be the weekends due to the change in demand behaviour.","6311cdbd":"df['geohash6'].value_counts().sort_values().head()\n#there are locations with 1 or little datapoints","530fcde2":"df.groupby('geohash6').agg(\n    {\n         'demand':\"median\",    # median demand for each location\n         'geohash6': \"count\",  # no. of datapoints for each location\n    }\n).plot(x=\"geohash6\", y=\"demand\", kind=\"scatter\")","2d7e3814":"df['latitude'] = df['geohash6'].apply(lambda x: geo.decode_exactly(x)[0])\ndf['longitude'] = df['geohash6'].apply(lambda x: geo.decode_exactly(x)[1])","b5c82110":"fig, ax = plt.subplots(figsize=(20,15))\ndd_loc = df.groupby(['latitude','longitude'])['demand'].sum().reset_index()\nsns.scatterplot(x='longitude', y='latitude', size='demand', sizes=(40, 400), data=dd_loc, ax=ax)\nplt.show()","0a42d758":"df2 = df.groupby(['geohash6','latitude','longitude'])['demand'].agg(['sum','std']).fillna(0).reset_index() #treat those with nan standard deviation as 0\ndf2.head()","7f79da16":"X = df2.drop('geohash6',axis=1)\nXs  = StandardScaler().fit_transform(X)\nXs  = pd.DataFrame(Xs , columns = X.columns.values)\nXs.head()","ae9efec4":"def opt_clusters(X, scaling=StandardScaler, k=11):\n    #choosing clusters with elbow within cluster sum square errors and silhouette score\n    inertia = []\n    silh = []\n    #standardizing required\n    Xs = StandardScaler().fit_transform(X)\n    Xs = pd.DataFrame(Xs, columns = X.columns.values)\n    for i in range(1,k):\n        model = KMeans(n_clusters=i, random_state=0).fit(Xs)\n        predicted = model.labels_\n        inertia.append(model.inertia_)#low inertia = low cluster sum square error. Low inertia -> Clusters are more compact.\n        if i>1:\n            silh.append(silhouette_score(Xs, predicted, metric='euclidean')) #High silhouette score = clusters are well separated. The score is based on how much closer data points are to their own clusters (intra-dist) than to the nearest neighbor cluster (inter-dist): (cohesion + separation).  \n    plt.plot(np.arange(1, k, step=1), inertia)\n    plt.title('Innertia vs clusters')\n    plt.xlabel('No. of clusters')\n    plt.ylabel('Within Clusters Sum-sq (WCSS)')\n    plt.show()\n    plt.scatter(np.arange(2, k, step=1), silh)\n    plt.title('Sihouette vs clusters')\n    plt.xlabel('No. of clusters')\n    plt.ylabel('Silhouette score')\n    plt.show()","d43def62":"opt_clusters(Xs, scaling=StandardScaler, k=11)","aaaac730":"#getting prediction and centroids\n#select 6 clusters based on silhouette and WCSS\nkmeans = KMeans(n_clusters=6, random_state=0).fit(Xs)\npredicted = kmeans.labels_\ncentroids = kmeans.cluster_centers_\nXs['predicted'] = predicted #or X['predicted'] = predicted","8a36d378":"df2['cluster'] = Xs['predicted']","6e9b13b8":"fig, ax = plt.subplots(figsize=(20,15))\nsns.scatterplot(x='longitude', y='latitude', size='sum', hue='cluster', palette=sns.color_palette(\"Dark2\", 6), sizes=(40, 400), data=df2, ax=ax)\nplt.show()","f91987fa":"#create a dictionary for these locations\ncluster_dict = df2[['geohash6','cluster']].set_index('geohash6')['cluster'].to_dict()\ndf['cluster'] = df['geohash6'].apply(lambda x: cluster_dict[x])\nloc_dict = {0:'clust0', 1:'clust1', 2:'clust2', 3:'clust3', 4:'clust4', 5:'clust5'}\ndf['cluster'] = df['cluster'].apply(lambda x: loc_dict[x])","51dcb5a6":"df.head()","891bc93f":"#Grouping and pivoting each cluster's demand\nlstm_df = df.groupby(['dum_time','cluster','daycycle','hour','minute'])['demand'].sum().reset_index()\nlstm_df.set_index('dum_time', inplace=True)\n\nlstm_df2 = pd.pivot_table(lstm_df, values = 'demand', index=['dum_time','daycycle','hour','minute'], columns = 'cluster').reset_index()\nlstm_df2.set_index('dum_time',inplace=True)","53b1ad43":"lstm_df2.head()","72508d8b":"#Standardizing demand min max for better processing of neural nets\nseries = lstm_df2.drop(['daycycle','hour','minute'],axis=1)\nscaler = MinMaxScaler(feature_range = (0,1))\nscaled = scaler.fit_transform(series.values)\nseries_ss = pd.DataFrame(scaled)\n\nseries_ss.columns = list(series.columns)\nseries_ss.set_index(series.index, inplace=True)\nseries_ss.head()","9052b05d":"#get dataset for each time series forecast t+1, t+2, t+3, t+4, t+5\ntseries_dict = {}\nfor step in range(1,6):\n    y = series_ss.copy()\n    ts_prior = series_ss.shift(step)\n    y.columns = [j + str(step) for j in list(series_ss.columns)]\n    dummies = pd.get_dummies(lstm_df2[['daycycle','hour','minute']], columns = ['daycycle','hour','minute'],drop_first=True)\n    tseries_dict[step] = pd.concat([ts_prior,dummies,y],axis=1).dropna()","45581539":"###### function to split into train and test sets\ndef train_test_prep(data):\n    values = data.values\n    n_train_time = round(0.9*len(data))\n    train = values[:n_train_time, :]\n    test = values[n_train_time:, :]\n    ##test = values[n_train_time:n_test_time, :]\n    # split into input and outputs\n    train_X, train_y = train[:, :-6], train[:, -6:]\n    test_X, test_y = test[:, :-6], test[:, -6:]\n    # reshape input to be 3D [samples, timesteps, features]\n    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n    train_X.shape, train_y.shape, test_X.shape, test_y.shape\n    return train_X, train_y, test_X, test_y\n# We reshaped the input into the 3D format as expected by LSTMs, namely [samples, timesteps, features].","806e7734":"# print(round(0.9*len(tseries_dict[1])))\n# tseries_dict[1].iloc[5256,:]\n# date where training set starts is 2019-02-25 22:00:00","b046a10e":"#split into train\/test X&y datasets for each step forecast\nXts_train = {}\nXts_test = {}\nyts_train = {}\nyts_test = {}\nfor step in range(1,6):\n    Xts_train[step], yts_train[step], Xts_test[step], yts_test[step] = train_test_prep(tseries_dict[step])\n\ntseries_dict[step].shape","6fbd44ae":"#define training model\ndef model_train(train_X,train_y,test_X,test_y):\n    model = Sequential()\n    model.reset_states()\n    model.add(LSTM(input_shape=(train_X.shape[1], train_X.shape[2]), output_dim=50, return_sequences = True))\n    model.add(Dropout(0.5))\n    model.add(LSTM(256))\n    model.add(Dropout(0.5))\n    model.add(Dense(6))\n    model.compile(loss='mean_squared_error', optimizer='adam')    \n    history = model.fit(train_X, train_y, epochs=20, batch_size=100, validation_data=(test_X, test_y), verbose=0, shuffle=False)        \n    return model","307f2e4c":"#train and save models forecasting t+1,t+2,...,t+5\nmodels_dict = {}\n\nfor step in range(1,6):\n    model = model_train(Xts_train[step], yts_train[step], Xts_test[step], yts_test[step])\n    model.save('timestep_'+str(i)+'.h5')\n    models_dict[step] = model","30835103":"#get error and forecast plot on test set\ndef model_predict(model,test_X,test_y):\n    # make a prediction\n    yhat = model.predict(test_X)\n    # invert scaling for forecast\n    yhat = yhat.reshape((len(test_y), 6))\n    inv_yhat = scaler.inverse_transform(yhat)\n\n    # invert scaling for actual\n    test_y = test_y.reshape((len(test_y), 6))\n    inv_y = scaler.inverse_transform(test_y)\n\n    rmse_ls = []\n    for i in range(0,6):\n        rmse_ls.append(mean_squared_error(pd.DataFrame(inv_y)[i], pd.DataFrame(inv_yhat)[i]))\n\n    mean_val = pd.DataFrame(inv_yhat).mean()\n    error_df = pd.concat([pd.DataFrame(rmse_ls, columns=['rmse']),mean_val],axis=1)\n    error_df['%_error'] = error_df['rmse']\/error_df[0]*100\n    error_df.columns = ['rmse','mean_val','%_error']\n    print(error_df)\n    \n#     for i in range(0,6):\n#         print(series.columns[i])\n#         plt.plot(inv_y[:,i], marker='.', label=\"actual\")\n#         plt.plot(inv_yhat[:,i], 'r', label=\"prediction\")\n#         plt.legend(fontsize=10)\n#         plt.show()\n    \n    fig, ax = plt.subplots(2, 3, figsize=(20,10))\n    ax[0, 0].plot(inv_y[:,0], marker='.', label=\"actual\")\n    ax[0, 1].plot(inv_y[:,1], marker='.', label=\"actual\")\n    ax[0, 2].plot(inv_y[:,2], marker='.', label=\"actual\")\n    ax[1, 0].plot(inv_y[:,3], marker='.', label=\"actual\")\n    ax[1, 1].plot(inv_y[:,4], marker='.', label=\"actual\")\n    ax[1, 2].plot(inv_y[:,5], marker='.', label=\"actual\")\n    ax[0, 0].plot(inv_yhat[:,0], marker='.', label=\"prediction\")\n    ax[0, 1].plot(inv_yhat[:,1], marker='.', label=\"prediction\")\n    ax[0, 2].plot(inv_yhat[:,2], marker='.', label=\"prediction\")\n    ax[1, 0].plot(inv_yhat[:,3], marker='.', label=\"prediction\")\n    ax[1, 1].plot(inv_yhat[:,4], marker='.', label=\"prediction\")\n    ax[1, 2].plot(inv_yhat[:,5], marker='.', label=\"prediction\")\n    ax[0, 0].title.set_text('Cluster0')\n    ax[0, 1].title.set_text('Cluster1')\n    ax[0, 2].title.set_text('Cluster2')\n    ax[1, 0].title.set_text('Cluster3')\n    ax[1, 1].title.set_text('Cluster4')\n    ax[1, 2].title.set_text('Cluster5')\n    ax[0, 0].legend(fontsize=10,loc='upper right')\n    plt.show()\n    return inv_yhat","fca5d291":"#save predicted cluster demand for each cluster for t+1,..t+5 into a data dictionary\ny_pred = {}\nfor step in range(1,6):\n    print('Forecast time ahead ', step)\n    y_pred[step] = model_predict(models_dict[step],Xts_test[step], yts_test[step])\n\n#error increase as timestep ahead to predict increases","fa0c72f4":"#standardize lat and long for analysis later\nfrom sklearn.preprocessing import MinMaxScaler\nlat_long = MinMaxScaler(feature_range=(0, 1), copy=True).fit_transform(df[['latitude','longitude']])\nlat_long = pd.DataFrame(lat_long)\nlat_long.columns = ['latitude','longitude']\n\ndf['latitude'] = lat_long['latitude']\ndf['longitude'] = lat_long['longitude']","a062cc57":"#get proportion of demand for each geolocation, relative to the cluster and time\nsector_dd = df.groupby(['cluster','dum_time'])['demand'].sum().reset_index()\ndf_sect_dd = df.merge(sector_dd, left_on = ['cluster','dum_time'], right_on = ['cluster','dum_time'],how = 'inner',suffixes=['','_sect'])\ndf_sect_dd['prop_dd'] = df_sect_dd['demand']\/df_sect_dd['demand_sect']\ndf_sect_dd['prop_dd'] = df_sect_dd['prop_dd'].fillna(0)\ndf_sect_dd.head()","18cd5dff":"#get required data for cross section analysis\ndf3 = df_sect_dd[['dum_time','geohash6','latitude','longitude','daycycle','cluster','hour','minute','demand','demand_sect','prop_dd']]\n#training set will be prior to '2019-02-25 22:00:00', as per time series forecast\ndf3_train = df3[df3['dum_time'] < pd.Timestamp(2019,2,25,22,0)].drop(['dum_time'],axis=1)\ndf3_test = df3[df3['dum_time'] >= pd.Timestamp(2019,2,25,22,0)].drop(['dum_time'],axis=1)","fc530e6e":"#feature engineer for train set\n#insert train set demand & prop_dd further statistics - mean,median,std,min,max\n\nf = {'demand': ['median','std','mean','min','max'],'prop_dd': ['median','std','mean','min','max']}\ndf3_train2 = df3_train.groupby(['geohash6','cluster','daycycle','hour','minute']).agg(f).reset_index()\ndf3_train2.columns = [\"\".join(x) for x in df3_train2.columns.ravel()]\ndf3_train2 = df3_train2.fillna(0)","90e1be7c":"#Check consistency\/fluctuations of location proportion dd for each day cycle's time interval\nprint('median prop_dd:', np.log(df3_train2['prop_ddstd'].median()))\nsns.distplot(np.log(df3_train2[df3_train2['prop_ddstd']!=0]['prop_ddstd']))\nplt.show()\n#fluctuation of prop_dd has a lognormal distribution but a heavier left tail. Most geolocation has low fluctuations in proportion demand, consistency is present \/ low data available for these geolocations as well.. ","ef82968a":"#as we are predicting prop_dd (before getting actual demand from multiplying cluster forecast), \n#remove demand from the data set and use prop_dd as target variable.\n#merge geolocation historic features from training set to test set (features are acquired prior to test time)\ndf3_train_feat = df3_train.drop(['demand'],axis=1).merge(df3_train2, left_on = ['geohash6','cluster','daycycle','hour','minute'], \n                                                         right_on = ['geohash6','cluster','daycycle','hour','minute'],\n                                                         how = 'inner',suffixes=['','_feat'])\ndf3_test_feat = df3_test.drop(['demand'],axis=1).merge(df3_train2, left_on = ['geohash6','cluster','daycycle','hour','minute'], \n                                                       right_on = ['geohash6','cluster','daycycle','hour','minute'],\n                                                       how = 'inner',suffixes=['','_feat'])","6525b850":"df3_train_feat.head()","2c197038":"#train-test split\n#dummy obj variables\n#dont use geohash6 as dummy, giving too sparse matrix. use lat and longitude instead, under continuous variables\nX_train = pd.get_dummies(df3_train_feat,columns=['cluster','daycycle','hour','minute'],\n                         drop_first=True).drop(['prop_dd','geohash6'],axis=1).values\ny_train = df3_train_feat['prop_dd'].values\n\nX_test = pd.get_dummies(df3_test_feat,columns=['cluster','daycycle','hour','minute'],\n                        drop_first=True).drop(['prop_dd','geohash6'],axis=1).values\ny_test = df3_test_feat['prop_dd'].values","66b231d6":"lm = linear_model.LinearRegression()\n#fit model\nmodel_lm = lm.fit(X_train, y_train)\npredictions = model_lm.predict(X_test)","3e7bd85a":"print('In-sample R-sq:',model_lm.score(X_train, y_train))\nprint('Out-sample R-sq:',model_lm.score(X_test, y_test))\nprint('MSE:', mean_squared_error(y_test, predictions))","6ec0e07e":"resid = y_test - predictions\nplt.scatter(resid, y_test)\n#there are still unobserved linear relationship present, must further tease out in next iteration","2af6f716":"#get dummy daterange for the forecasted clusters' demand\ndate_first = pd.Timestamp(2019,2,25,22,0)\ndate_last = df['dum_time'].max() \ntest_dum_time = pd.date_range(date_first, date_last, freq='15min')","99ce8655":"ts_ypred1 = pd.DataFrame(y_pred[1])\nts_ypred1['dum_time'] = test_dum_time\nts_ypred1.columns=['clust0','clust1','clust2','clust3','clust4','clust5','dum_time']\nts_ypred1 = pd.melt(ts_ypred1, id_vars=['dum_time'], value_vars=['clust0','clust1','clust2','clust3','clust4','clust5'])\nts_ypred1.columns=['dum_time','cluster','demand_sect']\nts_ypred1.head()","a4d6af18":"test_df = df3[df3['dum_time'] >= pd.Timestamp(2019,2,25,22,0)]\ntest_df.head()\ntest_df = test_df.merge(ts_ypred1, left_on = ['dum_time','cluster'], right_on = ['dum_time','cluster'],how = 'inner',suffixes=['','_pred'] )","ebad6615":"test_df.head()","ce82dbda":"#merge geolocation historic features from training set (features acquired prior to test time)\ntest_df_feat = test_df.merge(df3_train2, left_on = ['geohash6','cluster','daycycle','hour','minute'], \n                             right_on = ['geohash6','cluster','daycycle','hour','minute'],\n                             how = 'inner',suffixes=['','_feat'])\ntest_df_feat.head()","65e9b4b6":"#get dummies into test set, remove irrelevant columns\ntest_df_feat2 = pd.get_dummies(test_df_feat,columns=['cluster','daycycle','hour','minute'],\n                           drop_first=True).drop(['prop_dd','geohash6'],axis=1)\n\n# use demand_sect_pred instead of demand_sect\nX_test_cols = ['latitude', 'longitude', 'demand_sect_pred', 'demandmedian', 'demandstd',\n       'demandmean', 'demandmin', 'demandmax', 'prop_ddmedian', 'prop_ddstd',\n       'prop_ddmean', 'prop_ddmin', 'prop_ddmax', 'cluster_clust1',\n       'cluster_clust2', 'cluster_clust3', 'cluster_clust4', 'cluster_clust5',\n       'daycycle_2', 'daycycle_3', 'daycycle_4', 'daycycle_5', 'daycycle_6',\n       'daycycle_7', 'hour_1', 'hour_2', 'hour_3', 'hour_4', 'hour_5',\n       'hour_6', 'hour_7', 'hour_8', 'hour_9', 'hour_10', 'hour_11', 'hour_12',\n       'hour_13', 'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18',\n       'hour_19', 'hour_20', 'hour_21', 'hour_22', 'hour_23', 'minute_15',\n       'minute_30', 'minute_45']\nX_test_ts1 = test_df_feat2[X_test_cols].values\ny_test_ts1 = test_df_feat2['demand'].values","ee15412d":"pred_propdd_ts1 = model_lm.predict(X_test_ts1)","d0732754":"test_df_feat['pred_propdd_ts1'] = pd.DataFrame(pred_propdd_ts1)\ntest_df_feat['pred_demand_ts1'] = test_df_feat['demand_sect_pred'] * test_df_feat['pred_propdd_ts1']\n#cap prediction with demand>1 to 1\ntest_df_feat['pred_demand_ts1'] = test_df_feat['pred_demand_ts1'].apply(lambda x: 1 if x>1 else x)\ntest_df_feat['resid_ts1'] = test_df_feat['demand'] - test_df_feat['pred_demand_ts1']","f3bbc718":"mean_val = test_df_feat['demand'].mean()\nrmse = mean_squared_error(test_df_feat['pred_demand_ts1'], test_df_feat['demand'])\nerror_perc_ts1 = rmse\/mean_val*100\nprint('Demand mean value:', mean_val)\nprint('RMSE:', rmse)\nprint('%_error:', error_perc_ts1)","c5a9633a":"plt.scatter(x=test_df_feat['pred_demand_ts1'], y=test_df_feat['demand'])\n#prediction largely aligns with actual demand","4071cdc0":"plt.scatter(x=test_df_feat['resid_ts1'], y=test_df_feat['demand'])\n#prediction largely aligns with actual demand","c36a8c79":"Create day cycles of 7s","f686f4e6":"# This kernel is based on a data competition by Grab to forecast traffic demand.\nhttps:\/\/www.aiforsea.com\/ <br>\nThe data provided contains a column for geohash6, a column for day 1 to 60, a column for time and a column for normalised demand.\nThe challenge would be to engineer features from these limited columns and make accurate forecast for the minute locations.","39625633":"Preprocessing to create cluster time series dataframe","6c7484cd":"The X predictors could explain about 64% of the target (proportion of demand) variation.","ce39ef9d":"# Use basic linear regression. Have tried various other models - ridge, lasso, neural nets. Performance slightly better but with much longer processing time. Not worth it.**","718922a6":"# Find location clusters based on proximity and demand indicators","22829ae9":"Low demand areas indeed has less datapoints. <br>\nAssume the missing datapoint is due to zero demand at location for the particular day time. <br>","4c37b472":"# Understanding Behavioural Patterns before forecast\nTo acquire good features for forecasting, various features have to be engineered so as to have a better underrstanding of different demand behaviour by the population across time and space. <br>\nPeople demand for transport differently throughout the time of day (going to work and coming home), the day of the week (weekdays vs weekends). <br>\nDemand behaviours are also different from city centers, auxiliary towns and more rural regions. For example, demand could be higher in the morning when people travel from auxiliary towns into city centers and vice veras in the evenings. <br>\n\nThe behavioural analysis will first look into different time trends (across the entire 60 days, time of the day and day of the week). <br>\nNext we will do some clustering analysis to understand demand patterns from different regions and how they interact with time trends. <br>\n\nThroughout the analysis, complementary features will be engineered for the forecasting. <br>\nThe forecasting will be conducted largely in two parts.<br>\n1. A time series aggregated demand forecast for each 15 minute intervals at clustered regions.\n2. This is followed by a distribution or proportionate forecast of the aggregated demand across individual geolocations in each region.","59f8a315":"# Training Cross Sector Analysis\nThis analysis is to tease out the relationship of geolocation demand proportion to its overall cluster demand. <br>\nThe relationship is teased out at the level of geolocation's daycycle, hour & 15minute intervals.  ","5fad4047":"The graph shows a plot of time step 1 ahead forecasted demand (x-axis) vs actual demand (y-axis). <br>","d3ce36ef":"Residuals (x-aixs) vs actual demand (y-axis) plot. Residuals are not normally distributed. There is remaining linear relationship present to be teased out. For further iterations.","af2bef74":"# Looks like the k-mean models has clustered 3 main type of locations\n1. 1st tier size clusters which probably are the centres of activities\n2. 2nd tier size clusters which probably are the busier areas nearby centres\n3. Areas with lower activities, seperated into NE NW SE SW","105b95fa":"# Check two layer model performance. \n1. First get the predicted cluster time series demand.\n2. Second get the distributed predicted demand for each geolocation at the specific time.\n3. Compare these predictions against the actual demand.","2c91cf57":"#  Create a dummy datetime using 2019 1st Jan as startdate","6a1b7bfc":"# Tried VAR Time series forecast. Results were not encouraging. Use LSTM to predict cluster demand for t+1,t+2,...,t+5","0d3da22a":"## Get lat & long"}}