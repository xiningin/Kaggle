{"cell_type":{"d699016d":"code","ee2c71e2":"code","162d904d":"code","7aa69c02":"code","735a7b7f":"code","101c3f9a":"code","207a66ef":"markdown","c643344d":"markdown"},"source":{"d699016d":"import os\nprint(os.listdir(\"..\/input\/noteevents1\"))\n","ee2c71e2":"# import library packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# input data from csv files or database sources\npath1 = '..\/input\/noteevents1\/'\n#pd.read_csv(path1+\"ADMISSIONS.csv\")\nnotes = pd.read_csv(path1+\"NOTEEVENTS1.csv\")\nnotes.head\n\n","162d904d":"# select text notes under category \"Discharge summary\"\nnotes_dis = notes.loc[notes.CATEGORY == 'Discharge summary']\n# check if there are empty text notes\nnotes_dis.TEXT.isnull().sum() \/ len(notes_dis.TEXT) # no empty text notes\n\ndef preprocess_text(inputs):\n# This function preprocesses the text by filling not a \n# number and replacing new lines ('\\n') and carriage returns ('\\r')\n    inputs.TEXT = inputs.TEXT.fillna(' ')\n    inputs.TEXT = inputs.TEXT.str.replace('\\n', ' ')\n    inputs.TEXT = inputs.TEXT.str.replace('\\t', ' ')\n    return inputs\n# slice a part of data to see text preprocessing results\nnotes_dis_sample = preprocess_text(notes_dis.loc[:3,:])\n\n","7aa69c02":"# tokenize doctor's notes using nltk package and a token-optimizing function\nfrom nltk import word_tokenize\nword_tokenize(notes_dis_sample.TEXT[1])# initial tokens include punctuation and numbers\nimport string\ndef tokenizer_better(text):\n    # tokenize the text by replacing punctuation and numbers with spaces and lowercase all words\n    punc_list = string.punctuation+'0123456789'\n    t = str.maketrans(dict.fromkeys(punc_list, \" \"))\n    text = text.lower().translate(t)\n    tokens = word_tokenize(text)\n    return tokens\ntokenizer_better(notes_dis_sample.TEXT[1])[40:60]\n","735a7b7f":"# using bag-of-words approach to count the numbers of each word\n# 1. vectorize text using sci-learn package\nfrom  sklearn.feature_extraction.text import CountVectorizer\nmodel1 = CountVectorizer(tokenizer = tokenizer_better)\nmodel1.fit(notes_dis_sample.TEXT)\nsparse_vec = model1.transform(notes_dis_sample.TEXT) # sparse matrix, only non-zero values are kept\nd = sparse_vec.toarray()\nd.shape\n# plot to see the most used words and select stop words\ntoken_num_sum = np.squeeze(np.sum(sparse_vec,axis=0))\ntoken_num_sum_df = pd.DataFrame(token_num_sum,columns=model1.get_feature_names()).sort_values(by=0,axis=1,ascending=False)\nplot1 = pd.Series(token_num_sum_df.iloc[0],index=token_num_sum_df.columns)\nax = plot1[:50].plot(kind='bar', figsize=(10,6), width=.5, fontsize=14, rot=90,color = 'b')\nax.title.set_size(18)\nplt.ylabel('count')\nplt.show()\nmy_stop_words = ['the','and','to','of','was','with','a','on','in','for','name',\n                 'is','patient','s','he','at','as','or','one','she','his','her','am',\n                 'were','you','pt','pm','by','be','had','your','this','date',\n                'from','there','an','that','p','are','have','has','h','but','o',\n                'namepattern','which','every','also','d','q']\n#include stop words in the vectorizing model\nmodel2 = CountVectorizer(tokenizer = tokenizer_better,stop_words=my_stop_words)\nmodel2.fit(notes_dis_sample.TEXT)\nsparse_vec = model2.transform(notes_dis_sample.TEXT)\nsparse_vec.toarray()\ntoken_num_sum = np.squeeze(np.sum(sparse_vec,axis=0))\ntoken_num_sum_df = pd.DataFrame(token_num_sum,columns=model2.get_feature_names()).sort_values(by=0,axis=1,ascending=False)\nplot1 = pd.Series(token_num_sum_df.iloc[0],index=token_num_sum_df.columns)\nax = plot1[:50].plot(kind='bar', figsize=(10,6), width=.5, fontsize=14, rot=90,color = 'b')\nax.title.set_size(18)\nplt.ylabel('count')\nplt.show()\n","101c3f9a":"#path2 = R\"\"\"C:\\Users\\Administator\\Desktop\\NLP-key_extraction\\NLP-doctors-notes\\\\\"\"\"\npath2='..\/input\/symptoms\/'\nwith open(path2+'symptoms.txt','r') as f:\n    symptom_words = f.readlines()\nsymptom_words_lib = [x.lstrip() for x in symptom_words[0].split(',')]\nmodel3 = CountVectorizer(tokenizer = tokenizer_better,stop_words=my_stop_words)\nmodel3.fit(symptom_words_lib)\nsparse_vec = model3.transform(notes_dis_sample.TEXT)\nsparse_vec.toarray()\ntoken_num_sum = np.squeeze(np.sum(sparse_vec,axis=0))\ntoken_num_sum_df = pd.DataFrame(token_num_sum,columns=model3.get_feature_names()).sort_values(by=0,axis=1,ascending=False)\nplot1 = pd.Series(token_num_sum_df.iloc[0],index=token_num_sum_df.columns)\nax = plot1[:50].plot(kind='bar', figsize=(10,6), width=.5, fontsize=14, rot=90,color = 'b')\nax.title.set_size(18)\nplt.ylabel('count')\nplt.title('Symptom Words extraction')\nplt.show()","207a66ef":"# 2. Symptom words extraction\n\n","c643344d":"# 1. Dataset handling and model training"}}