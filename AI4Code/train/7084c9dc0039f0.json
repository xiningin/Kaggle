{"cell_type":{"069e4142":"code","0e282ae6":"code","2d076710":"code","aeb38411":"code","c86ab1f1":"code","bf1f7df0":"code","bf1e891d":"code","e43db20d":"code","e07b3f9a":"code","f4cc8414":"code","ad73c8ba":"code","9fbcc931":"code","d0cd7a99":"code","cb5e615d":"code","1acb971d":"code","a1f58225":"code","e1af3d2d":"code","16e3de9a":"code","a19de4a7":"code","809c13b9":"code","9983d7b1":"code","e879cac2":"code","9d819d00":"code","286a6592":"code","338ab8a2":"code","22489b19":"code","f2e79631":"code","05b2a59b":"code","d64fd617":"code","c861d72b":"code","edeeea5d":"code","f796880f":"code","55ddc2b8":"code","3af83c27":"code","1438604a":"markdown","1c7cc99c":"markdown","1062db6e":"markdown","9294999b":"markdown","ec7de41e":"markdown","63f26795":"markdown","30cca366":"markdown","27f44c76":"markdown","c5447bcf":"markdown","74a9ec68":"markdown","e3a2dfe8":"markdown","bb0c23d7":"markdown","15551e87":"markdown","4bb2becf":"markdown","bbebc301":"markdown","ebf70da9":"markdown","0f710ffa":"markdown","3077c03d":"markdown","e471776c":"markdown","90a7165c":"markdown","34d6ba06":"markdown","5f1efc73":"markdown"},"source":{"069e4142":"# Install jax and flax (high-level API)\n!pip install jax jaxlib  --quiet\n!pip install flax  --quiet","0e282ae6":"import jax\nimport flax\nimport jax.numpy as jnp\nimport flax.linen as nn\n\nimport os\nimport time\nimport operator\nfrom functools import partial\nfrom functools import reduce\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nprint('Jax version', jax.__version__)\nprint('Flax version', flax.__version__)\nrandom_key = jax.random.PRNGKey(0)\n\nprint(\"Available devices:\", jax.devices())","2d076710":"def squeeze(x):\n    x = jnp.reshape(x, (x.shape[0], \n                        x.shape[1] \/\/ 2, 2, \n                        x.shape[2] \/\/ 2, 2,\n                        x.shape[-1]))\n    x = jnp.transpose(x, (0, 1, 3, 2, 4, 5))\n    x = jnp.reshape(x, x.shape[:3] + (4 * x.shape[-1],))\n    return x","aeb38411":"print(\"Example\")\nx = jax.random.randint(random_key, (1, 4, 4, 1), 0, 10)\nprint('x = ', '\\n     '.join(' '.join(str(v[0]) for v in row) for row in x[0]))\nprint('\\nbecomes\\n')\nx = squeeze(x)\nprint('y with shape', x.shape, 'where')\nprint('\\n'.join(f'  y[{i}, {j}] = {x[0, i, j]}' for i in range(2) for j in range(2)))","c86ab1f1":"def unsqueeze(x):\n    x = jnp.reshape(x, (x.shape[0], x.shape[1], x.shape[2], \n                        2, 2, x.shape[-1] \/\/ 4))\n    x = jnp.transpose(x, (0, 1, 3, 2, 4, 5))\n    x = jnp.reshape(x, (x.shape[0], \n                        2 * x.shape[1],\n                        2 * x.shape[3],\n                        x.shape[5]))\n    return x","bf1f7df0":"print(\"Sanity check for  reversibility\")\ndef sanity_check():\n    x = jax.random.randint(random_key, (1, 4, 4, 16), 0, 10)\n    y = unsqueeze(squeeze(x))\n    z = squeeze(unsqueeze(x))\n    print(\"  \\033[92m\u2713\\033[0m\" if np.array_equal(x, y) else \"  \\033[91mx\\033[0m\", \n          \"unsqueeze o squeeze = id\")\n    print(\"  \\033[92m\u2713\\033[0m\" if np.array_equal(x, z) else \"  \\033[91mx\\033[0m\", \n          \"squeeze o unsqueeze = id\")\nsanity_check()","bf1e891d":"def split(x):\n    return jnp.split(x, 2, axis=-1)\n\ndef unsplit(x, z):\n    return jnp.concatenate([z, x], axis=-1)","e43db20d":"class ConvZeros(nn.Module):\n    features: int\n        \n    @nn.compact\n    def __call__(self, x, logscale_factor=3.0):\n        \"\"\"A simple convolutional layers initializer to all zeros\"\"\"\n        x = nn.Conv(self.features, kernel_size=(3, 3),\n                    strides=(1, 1), padding='same',\n                    kernel_init=jax.nn.initializers.zeros,\n                    bias_init=jax.nn.initializers.zeros)(x)\n        return x\n\n\nclass Split(nn.Module):\n    key: jax.random.PRNGKey = jax.random.PRNGKey(0)\n        \n    @nn.compact\n    def __call__(self, x, reverse=False, z=None, eps=None, temperature=1.0):\n        \"\"\"Args (reverse = True):\n            * z: If given, it is used instead of sampling (= deterministic mode).\n                This is only used to test the reversibility of the model.\n            * eps: If z is None and eps is given, then eps is assumed to be a \n                sample from N(0, 1) and rescaled by the mean and variance of \n                the prior. This is used during training to observe how sampling\n                from fixed latents evolve. \n               \n        If both are None, the model samples z from scratch\n        \"\"\"\n        if not reverse:\n            del z, eps, temperature\n            z, x = jnp.split(x, 2, axis=-1)\n            \n        # Learn the prior parameters for z\n        prior = ConvZeros(x.shape[-1] * 2, name=\"conv_prior\")(x)\n            \n        # Reverse mode: Only return the output\n        if reverse:\n            # sample from N(0, 1) prior (inference)\n            if z is None:\n                if eps is None:\n                    eps = jax.random.normal(self.key, x.shape) \n                eps *= temperature\n                mu, logsigma = jnp.split(prior, 2, axis=-1)\n                z = eps * jnp.exp(logsigma) + mu\n            return jnp.concatenate([z, x], axis=-1)\n        # Forward mode: Also return the prior as it is used to compute the loss\n        else:\n            return z, x, prior","e07b3f9a":"class AffineCoupling(nn.Module):\n    out_dims: int\n    width: int = 512\n    eps: float = 1e-8\n    \n    @nn.compact\n    def __call__(self, inputs, logdet=0, reverse=False):\n        # Split\n        xa, xb = jnp.split(inputs, 2, axis=-1)\n        \n        # NN\n        net = nn.Conv(features=self.width, kernel_size=(3, 3), strides=(1, 1),\n                      padding='same', name=\"ACL_conv_1\")(xb)\n        net = nn.relu(net)\n        net = nn.Conv(features=self.width, kernel_size=(1, 1), strides=(1, 1),\n                      padding='same', name=\"ACL_conv_2\")(net)\n        net = nn.relu(net)\n        net = ConvZeros(self.out_dims, name=\"ACL_conv_out\")(net)\n        mu, logsigma = jnp.split(net, 2, axis=-1)\n        # See https:\/\/github.com\/openai\/glow\/blob\/master\/model.py#L376\n        # sigma = jnp.exp(logsigma)\n        sigma = jax.nn.sigmoid(logsigma + 2.)\n        \n        # Merge\n        if not reverse:\n            ya = sigma * xa + mu\n            logdet += jnp.sum(jnp.log(sigma), axis=(1, 2, 3))\n        else:\n            ya = (xa - mu) \/ (sigma + self.eps)\n            logdet -= jnp.sum(jnp.log(sigma), axis=(1, 2, 3))\n            \n        y = jnp.concatenate((ya, xb), axis=-1)\n        return y, logdet","f4cc8414":"class ActNorm(nn.Module):\n    scale: float = 1.\n    eps: float = 1e-8\n\n    @nn.compact\n    def __call__(self, inputs, logdet=0, reverse=False):\n        # Data dependent initialization. Will use the values of the batch\n        # given during model.init\n        axes = tuple(i for i in range(len(inputs.shape) - 1))\n        def dd_mean_initializer(key, shape):\n            \"\"\"Data-dependant init for mu\"\"\"\n            nonlocal inputs\n            x_mean = jnp.mean(inputs, axis=axes, keepdims=True)\n            return - x_mean\n        \n        def dd_stddev_initializer(key, shape):\n            \"\"\"Data-dependant init for sigma\"\"\"\n            nonlocal inputs\n            x_var = jnp.mean(inputs**2, axis=axes, keepdims=True)\n            var = self.scale \/ (jnp.sqrt(x_var) + self.eps)\n            return var\n        \n        # Forward\n        shape = (1,) * len(axes) + (inputs.shape[-1],)\n        mu = self.param('actnorm_mean', dd_mean_initializer, shape)\n        sigma = self.param('actnorm_sigma', dd_stddev_initializer, shape)\n        \n        logsigma = jnp.log(jnp.abs(sigma))\n        logdet_factor = reduce(\n            operator.mul, (inputs.shape[i] for i in range(1, len(inputs.shape) - 1)), 1)\n        if not reverse:\n            y = sigma * (inputs + mu)\n            logdet += logdet_factor * jnp.sum(logsigma)\n        else:\n            y = inputs \/ (sigma + self.eps) - mu\n            logdet -= logdet_factor * jnp.sum(logsigma)\n        \n        # Logdet and return\n        return y, logdet","ad73c8ba":"print(\"Sanity check for data-dependant init in ActNorm\")\n\ndef sanity_check():\n    x = jax.random.normal(random_key, (1, 256, 256, 3))\n    model = ActNorm()\n    init_variables = model.init(random_key, x)\n    y, _ = model.apply(init_variables, x)\n    m = jnp.mean(y); v = jnp.std(y); eps = 1e-5\n    print(\"  \\033[92m\u2713\\033[0m\" if abs(m) < eps else \"  \\033[91mx\\033[0m\", \"Mean:\", m)\n    print(\"  \\033[92m\u2713\\033[0m\" if abs(v  - 1) < eps else \"  \\033[91mx\\033[0m\",\n          \"Standard deviation\", v)\nsanity_check()","9fbcc931":"class Conv1x1(nn.Module):\n    channels: int\n    key: jax.random.PRNGKey = jax.random.PRNGKey(0)\n\n    def setup(self):\n        \"\"\"Initialize P, L, U, s\"\"\"\n        # W = PL(U + s)\n        # Based on https:\/\/github.com\/openai\/glow\/blob\/master\/model.py#L485\n        c = self.channels\n        # Sample random rotation matrix\n        q, _ = jnp.linalg.qr(jax.random.normal(self.key, (c, c)), mode='complete')\n        p, l, u = jax.scipy.linalg.lu(q)\n        # Fixed Permutation (non-trainable)\n        self.P = p\n        self.P_inv = jax.scipy.linalg.inv(p)\n        # Init value from LU decomposition\n        L_init = l\n        U_init = jnp.triu(u, k=1)\n        s = jnp.diag(u)\n        self.sign_s = jnp.sign(s)\n        S_log_init = jnp.log(jnp.abs(s))\n        self.l_mask = jnp.tril(jnp.ones((c, c)), k=-1)\n        self.u_mask = jnp.transpose(self.l_mask)\n        # Define trainable variables\n        self.L = self.param(\"L\", lambda k, sh: L_init, (c, c))\n        self.U = self.param(\"U\", lambda k, sh: U_init, (c, c))\n        self.log_s = self.param(\"log_s\", lambda k, sh: S_log_init, (c,))\n        \n        \n    def __call__(self, inputs, logdet=0, reverse=False):\n        c = self.channels\n        assert c == inputs.shape[-1]\n        # enforce constraints that L and U are triangular\n        # in the LU decomposition\n        L = self.L * self.l_mask + jnp.eye(c)\n        U = self.U * self.u_mask + jnp.diag(self.sign_s * jnp.exp(self.log_s))\n        logdet_factor = inputs.shape[1] * inputs.shape[2]\n        \n        # forward\n        if not reverse:\n            # lax.conv uses weird ordering: NCHW and OIHW\n            W = jnp.matmul(self.P, jnp.matmul(L, U))\n            y = jax.lax.conv(jnp.transpose(inputs, (0, 3, 1, 2)), \n                             W[..., None, None], (1, 1), 'same')\n            y = jnp.transpose(y, (0, 2, 3, 1))\n            logdet += jnp.sum(self.log_s) * logdet_factor\n        # inverse\n        else:\n            W_inv = jnp.matmul(jax.scipy.linalg.inv(U), jnp.matmul(\n                jax.scipy.linalg.inv(L), self.P_inv))\n            y = jax.lax.conv(jnp.transpose(inputs, (0, 3, 1, 2)),\n                             W_inv[..., None, None], (1, 1), 'same')\n            y = jnp.transpose(y, (0, 2, 3, 1))\n            logdet -= jnp.sum(self.log_s) * logdet_factor\n            \n        return y, logdet","d0cd7a99":"class FlowStep(nn.Module):\n    nn_width: int = 512\n    key: jax.random.PRNGKey = jax.random.PRNGKey(0)\n        \n    @nn.compact\n    def __call__(self, x, logdet=0, reverse=False):\n        out_dims = x.shape[-1]\n        if not reverse:\n            x, logdet = ActNorm()(x, logdet=logdet, reverse=False)\n            x, logdet = Conv1x1(out_dims, self.key)(x, logdet=logdet, reverse=False)\n            x, logdet = AffineCoupling(out_dims, self.nn_width)(x, logdet=logdet, reverse=False)\n        else:\n            x, logdet = AffineCoupling(out_dims, self.nn_width)(x, logdet=logdet, reverse=True)\n            x, logdet = Conv1x1(out_dims, self.key)(x, logdet=logdet, reverse=True)\n            x, logdet = ActNorm()(x, logdet=logdet, reverse=True)\n        return x, logdet","cb5e615d":"# Utils to display Jax model in a similar way as flax summary\ndef get_params_size(v, s=0):\n    \"\"\"Get cumulative size of parameters contained in a FrozenDict\"\"\"\n    if isinstance(v, flax.core.FrozenDict):\n        return s + sum(get_params_size(x)  for x in v.values())\n    else:\n        return s + v.size\n\ndef summarize_jax_model(variables, \n                        max_depth=1, \n                        depth=0,\n                        prefix='',\n                        col1_size=60,\n                        col2_size=30):\n    \"\"\"Print summary of parameters + size contained in a jax model\"\"\"\n    if depth == 0:\n        print('-' * (col1_size + col2_size))\n        print(\"Layer name\" + ' ' * (col1_size - 10) + 'Param #')\n        print('=' * (col1_size + col2_size))\n    for name, v in variables.items():\n        if isinstance(v, flax.core.FrozenDict) and depth < max_depth:\n            summarize_jax_model(v, max_depth=max_depth, depth=depth + 1, \n                                prefix=f'{prefix}\/{name}')\n        else:\n            col1 = f'{prefix}\/{name}'\n            col1 = col1[:col1_size] + ' ' * max(0, col1_size - len(col1))\n            print(f'{col1}{get_params_size(v)}')\n            print('-' * (col1_size + col2_size))\n            \n# Summarize a flow step\ndef summary():\n    x = jax.random.normal(random_key, (32, 10, 10, 6))\n    model = FlowStep(key=random_key)\n    init_variables = model.init(random_key, x)\n    summarize_jax_model(init_variables, max_depth=2)\nsummary()","1acb971d":"class GLOW(nn.Module):\n    K: int = 32                                       # Number of flow steps\n    L: int = 3                                        # Number of scales\n    nn_width: int = 512                               # NN width in Affine Coupling Layer\n    learn_top_prior: bool = False                     # If true, learn prior N(mu, sigma) for zL\n    key: jax.random.PRNGKey = jax.random.PRNGKey(0)\n        \n        \n    def flows(self, x, logdet=0, reverse=False, name=\"\"):\n        \"\"\"K subsequent flows. Called at each scale.\"\"\"\n        for k in range(self.K):\n            it = k + 1 if not reverse else self.K - k\n            x, logdet = FlowStep(self.nn_width, self.key, name=f\"{name}\/step_{it}\")(\n                x, logdet=logdet, reverse=reverse)\n        return x, logdet\n        \n    \n    @nn.compact\n    def __call__(self, x, reverse=False, z=None, eps=None, sampling_temperature=1.0):\n        \"\"\"Args:\n            * x: Input to the model\n            * reverse: Whether to apply the model or its inverse\n            * z (reverse = True): If given, use these as intermediate latents (deterministic)\n            * eps (reverse = True, z!=None): If given, use these as Gaussian samples which are later \n                rescaled by the mean and variance of the appropriate prior.\n            * sampling_temperature (reverse = True, z!=None): Sampling temperature\n        \"\"\"\n        \n        ## Inputs\n        # Forward pass: Save priors for computing loss\n        # Optionally save zs (only used for sanity check of reversibility)\n        priors = []\n        if not reverse:\n            del z, eps, sampling_temperature\n            z = []\n        # In reverse mode, either use the given latent z (deterministic)\n        # or sample them. For the first one, uses the top prior.\n        # The intermediate latents are sampled in the `Split(reverse=True)` calls\n        else:\n            if z is not None:\n                assert len(z) == self.L\n            else:\n                x *= sampling_temperature\n                if self.learn_top_prior:\n                    # Assumes input x is a sample from N(0, 1)\n                    # Note: the inputs to learn the top prior is zeros (unconditioned)\n                    # or some conditioning e.g. class information.\n                    # If not learnable, the model just uses the input x directly\n                    # see https:\/\/github.com\/openai\/glow\/blob\/master\/model.py#L109\n                    prior = ConvZeros(x.shape[-1] * 2, name=\"prior_top\")(jnp.zeros(x.shape))\n                    mu, logsigma = jnp.split(prior, 2, axis=-1)\n                    x = x * jnp.exp(logsigma) + mu\n                \n        ## Multi-scale model\n        logdet = 0\n        for l in range(self.L):\n            # Forward\n            if not reverse:\n                x = squeeze(x)\n                x, logdet = self.flows(x, logdet=logdet,\n                                       reverse=False,\n                                       name=f\"flow_scale_{l + 1}\/\")\n                if l < self.L - 1:\n                    zl, x, prior = Split(\n                        key=self.key, name=f\"flow_scale_{l + 1}\/\")(x, reverse=False)\n                else:\n                    zl, prior = x, None\n                    if self.learn_top_prior:\n                        prior = ConvZeros(zl.shape[-1] * 2, name=\"prior_top\")(jnp.zeros(zl.shape))\n                z.append(zl)\n                priors.append(prior)\n                    \n            # Reverse\n            else:\n                if l > 0:\n                    x = Split(key=self.key, name=f\"flow_scale_{self.L - l}\/\")(\n                        x, reverse=True, \n                        z=z[-l - 1] if z is not None else None,\n                        eps=eps[-l - 1] if eps is not None else None,\n                        temperature=sampling_temperature)\n                x, logdet = self.flows(x, logdet=logdet, reverse=True,\n                                       name=f\"flow_scale_{self.L - l}\/\")\n                x = unsqueeze(x)\n                \n        ## Return\n        return x, z, logdet, priors","a1f58225":"print(\"Sanity check for reversibility (no sampling in reverse pass)\")\n\ndef sanity_check():\n    # Input\n    x_1 = jax.random.normal(random_key, (32, 32, 32, 6))\n    K, L = 16, 3\n    model = GLOW(K=K, L=L, nn_width=128, key=random_key, learn_top_prior=True)\n    init_variables = model.init(random_key, x_1)\n\n    # Forward call\n    _, z, logdet, priors = model.apply(init_variables, x_1)\n\n    # Check output shape\n    expected_h = x_1.shape[1] \/\/ 2**L\n    expected_c = x_1.shape[-1] * 4**L \/\/ 2**(L - 1)\n    print(\"  \\033[92m\u2713\\033[0m\" if z[-1].shape[1] == expected_h and z[-1].shape[-1] == expected_c \n          else \"  \\033[91mx\\033[0m\",\n          \"Forward pass output shape is\", z[-1].shape)\n\n    # Check sizes of the intermediate latent\n    correct_latent_shapes = True\n    correct_prior_shapes = True\n    for i, (zi, priori) in enumerate(zip(z, priors)):\n        expected_h = x_1.shape[1] \/\/ 2**(i + 1)\n        expected_c = x_1.shape[-1] * 2**(i + 1)\n        if i == L - 1:\n            expected_c *= 2\n        if zi.shape[1] != expected_h or zi.shape[-1] != expected_c:\n            correct_latent_shapes = False\n        if priori.shape[1] != expected_h or priori.shape[-1] != 2 * expected_c:\n            correct_prior_shapes = False\n    print(\"  \\033[92m\u2713\\033[0m\" if correct_latent_shapes else \"  \\033[91mx\\033[0m\",\n          \"Check intermediate latents shape\")\n    print(\"  \\033[92m\u2713\\033[0m\" if correct_latent_shapes else \"  \\033[91mx\\033[0m\",\n          \"Check intermediate priors shape\")\n\n    # Reverse the network without sampling\n    x_3, *_ = model.apply(init_variables, z[-1], z=z, reverse=True)\n\n    print(\"  \\033[92m\u2713\\033[0m\" if np.array_equal(x_1.shape, x_3.shape) else \"  \\033[91mx\\033[0m\", \n          \"Reverse pass output shape = Original shape =\", x_1.shape)\n    diff = jnp.mean(jnp.abs(x_1 - x_3))\n    print(\"  \\033[92m\u2713\\033[0m\" if diff < 1e-4 else \"  \\033[91mx\\033[0m\", \n          f\"Diff between x and Glow_r o Glow (x) = {diff:.3e}\")\nsanity_check()","e1af3d2d":"@jax.vmap\ndef get_logpz(z, priors):\n    logpz = 0\n    for zi, priori in zip(z, priors):\n        if priori is None:\n            mu = jnp.zeros(zi.shape)\n            logsigma = jnp.zeros(zi.shape)\n        else:\n            mu, logsigma = jnp.split(priori, 2, axis=-1)\n        logpz += jnp.sum(- logsigma - 0.5 * jnp.log(2 * jnp.pi) \n                         - 0.5 * (zi - mu) ** 2 \/ jnp.exp(2 * logsigma))\n    return logpz","16e3de9a":"def map_fn(image_path, num_bits=5, size=256, training=True):\n    \"\"\"Read image file, quantize and map to [-0.5, 0.5] range.\n    If num_bits = 8, there is no quantization effect.\"\"\"\n    image = tf.io.decode_jpeg(tf.io.read_file(image_path))\n    # Resize input image\n    image = tf.cast(image, tf.float32)\n    image = tf.image.resize(image, (size, size))\n    image = tf.clip_by_value(image, 0., 255.)\n    # Discretize to the given number of bits\n    if num_bits < 8:\n        image = tf.floor(image \/ 2 ** (8 - num_bits))\n    # Send to [-1, 1]\n    num_bins = 2 ** num_bits\n    image = image \/ num_bins - 0.5\n    if training:\n        image = image + tf.random.uniform(tf.shape(image), 0, 1. \/ num_bins)\n    return image\n\n\n@jax.jit\ndef postprocess(x, num_bits):\n    \"\"\"Map [-0.5, 0.5] quantized images to uint space\"\"\"\n    num_bins = 2 ** num_bits\n    x = jnp.floor((x + 0.5) * num_bins)\n    x *= 256. \/ num_bins\n    return jnp.clip(x, 0, 255).astype(jnp.uint8)","a19de4a7":"def sample(model, \n           params, \n           eps=None, \n           shape=None, \n           sampling_temperature=1.0, \n           key=jax.random.PRNGKey(0),\n           postprocess_fn=None, \n           save_path=None,\n           display=True):\n    \"\"\"Sampling only requires a call to the reverse pass of the model\"\"\"\n    if eps is None:\n        zL = jax.random.normal(key, shape) \n    else: \n        zL = eps[-1]\n    y, *_ = model.apply(params, zL, eps=eps, sampling_temperature=sampling_temperature, reverse=True)\n    if postprocess_fn is not None:\n        y = postprocess_fn(y)\n    plot_image_grid(y, save_path=save_path, display=display,\n                    title=None if save_path is None else save_path.rsplit('.', 1)[0].rsplit('\/', 1)[1])\n    return y\n\n\nfrom mpl_toolkits.axes_grid1 import ImageGrid\ndef plot_image_grid(y, title=None, display=True, save_path=None, figsize=(10, 10)):\n    \"\"\"Plot and optionally save an image grid with matplotlib\"\"\"\n    fig = plt.figure(figsize=figsize)\n    num_rows = int(np.floor(np.sqrt(y.shape[0])))\n    grid = ImageGrid(fig, 111, nrows_ncols=(num_rows, num_rows), axes_pad=0.1)\n    for ax in grid: \n        ax.set_axis_off()\n    for ax, im in zip(grid, y):\n        ax.imshow(im)\n    fig.suptitle(title, fontsize=18)\n    fig.subplots_adjust(top=0.98)\n    if save_path is not None:\n        plt.savefig(save_path, bbox_inches=\"tight\")\n    if display:\n        plt.show()\n    else:\n        plt.close()","809c13b9":"def train_glow(train_ds,\n               val_ds=None,\n               num_samples=9,\n               image_size=256,\n               num_channels=3,\n               num_bits=5,\n               init_lr=1e-3,\n               num_epochs=1,\n               num_sample_epochs=1,\n               num_warmup_epochs=10,\n               num_save_epochs=1,\n               steps_per_epoch=1,\n               K=32,\n               L=3,\n               nn_width=512,\n               sampling_temperature=0.7,\n               learn_top_prior=True,\n               key=jax.random.PRNGKey(0),\n               **kwargs):\n    \"\"\"Simple training loop.\n    Args:\n        train_ds: Training dataset iterator (e.g. tensorflow dataset)\n        val_ds: Validation dataset (optional)\n        num_samples: Number of samples to generate at each epoch\n        image_size: Input image size\n        num_channels: Number of channels in input images\n        num_bits: Number of bits for discretization\n        init_lr: Initial learning rate (Adam)\n        num_epochs: Numer of training epochs\n        num_sample_epochs: Visualize sample at this interval\n        num_warmup_epochs: Linear warmup of the learning rate to init_lr\n        num_save_epochs: save mode at this interval\n        steps_per_epochs: Number of steps per epochs\n        K: Number of flow iterations in the GLOW model\n        L: number of scales in the GLOW model\n        nn_width: Layer width in the Affine Coupling Layer\n        sampling_temperature: Smoothing temperature for sampling from the \n            Gaussian priors (1 = no effect)\n        learn_top_prior: Whether to learn the prior for highest latent variable zL.\n            Otherwise, assumes standard unit Gaussian prior\n        key: Random seed\n    \"\"\"\n    del kwargs\n    # Init model\n    model = GLOW(K=K,\n                 L=L, \n                 nn_width=nn_width, \n                 learn_top_prior=learn_top_prior,\n                 key=key)\n    \n    # Init optimizer and learning rate schedule\n    params = model.init(random_key, next(train_ds))\n    opt = flax.optim.Adam(learning_rate=init_lr).create(params)\n    \n    def lr_warmup(step):\n        return init_lr * jnp.minimum(1., step \/ (num_warmup_epochs * steps_per_epoch + 1e-8))\n    \n    # Helper functions for training\n    bits_per_dims_norm = np.log(2.) * num_channels * image_size**2\n    @jax.jit\n    def get_logpx(z, logdets, priors):\n        logpz = get_logpz(z, priors)\n        logpz = jnp.mean(logpz) \/ bits_per_dims_norm        # bits per dimension normalization\n        logdets = jnp.mean(logdets) \/ bits_per_dims_norm\n        logpx = logpz + logdets - num_bits                  # num_bits: dequantization factor\n        return logpx, logpz, logdets\n        \n    @jax.jit\n    def train_step(opt, batch):\n        def loss_fn(params):\n            _, z, logdets, priors = model.apply(params, batch, reverse=False)\n            logpx, logpz, logdets = get_logpx(z, logdets, priors)\n            return - logpx, (logpz, logdets)\n        logs, grad = jax.value_and_grad(loss_fn, has_aux=True)(opt.target)\n        opt = opt.apply_gradient(grad, learning_rate=lr_warmup(opt.state.step))\n        return logs, opt\n    \n    # Helper functions for evaluation \n    @jax.jit\n    def eval_step(params, batch):\n        _, z, logdets, priors = model.apply(params, batch, reverse=False)\n        return - get_logpx(z, logdets, priors)[0]\n    \n    # Helper function for sampling from random latent fixed during training for comparison\n    eps = []\n    if not os.path.exists(\"samples\"): os.makedirs(\"samples\")\n    if not os.path.exists(\"weights\"): os.makedirs(\"weights\")\n    for i in range(L):\n        expected_h = image_size \/\/ 2**(i + 1)\n        expected_c = num_channels * 2**(i + 1)\n        if i == L - 1: expected_c *= 2\n        eps.append(jax.random.normal(key, (num_samples, expected_h, expected_h, expected_c)))\n    sample_fn = partial(sample, eps=eps, key=key, display=False,\n                        sampling_temperature=sampling_temperature,\n                        postprocess_fn=partial(postprocess, num_bits=num_bits))\n    \n    # Train\n    print(\"Start training...\")\n    print(\"Available jax devices:\", jax.devices())\n    print()\n    bits = 0.\n    start = time.time()\n    try:\n        for epoch in range(num_epochs):\n            # train\n            for i in range(steps_per_epoch):\n                batch = next(train_ds)\n                loss, opt = train_step(opt, batch)\n                print(f\"\\r\\033[92m[Epoch {epoch + 1}\/{num_epochs}]\\033[0m\"\n                      f\"\\033[93m[Batch {i + 1}\/{steps_per_epoch}]\\033[0m\"\n                      f\" loss = {loss[0]:.5f},\"\n                      f\" (log(p(z)) = {loss[1][0]:.5f},\"\n                      f\" logdet = {loss[1][1]:.5f})\", end='')\n                if np.isnan(loss[0]):\n                    print(\"\\nModel diverged - NaN loss\")\n                    return None, None\n                \n                step = epoch * steps_per_epoch + i + 1\n                if step % int(num_sample_epochs * steps_per_epoch) == 0:\n                    sample_fn(model, opt.target, \n                              save_path=f\"samples\/step_{step:05d}.png\")\n\n            # eval on one batch of validation samples \n            # + generate random sample\n            t = time.time() - start\n            if val_ds is not None:\n                bits = eval_step(opt.target, next(val_ds))\n            print(f\"\\r\\033[92m[Epoch {epoch + 1}\/{num_epochs}]\\033[0m\"\n                  f\"[{int(t \/\/ 3600):02d}h {int((t % 3600) \/\/ 60):02d}mn]\"\n                  f\" train_bits\/dims = {loss[0]:.3f},\"\n                  f\" val_bits\/dims = {bits:.3f}\" + \" \" * 50)\n            \n            # Save parameters\n            if (epoch + 1) % num_save_epochs == 0 or epoch == num_epochs - 1:\n                with open(f'weights\/model_epoch={epoch + 1:03d}.weights', 'wb') as f:\n                    f.write(flax.serialization.to_bytes(opt.target))\n    except KeyboardInterrupt:\n        print(f\"\\nInterrupted by user at epoch {epoch + 1}\")\n        \n    # returns final model and parameters\n    return model, opt.target","9983d7b1":"# Data hyperparameters for 1 GPU training\n# Some small changes to the original model so \n# everything fits in memory\n# In particular, I had  to use shallower\n# flows (smaller K value)\nconfig_dict = {\n    'image_path': \"..\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba\",\n    'train_split': 0.6,\n    'image_size': 64,\n    'num_channels': 3,\n    'num_bits': 5,\n    'batch_size': 64,\n    'K': 16,\n    'L': 3,\n    'nn_width': 512, \n    'learn_top_prior': True,\n    'sampling_temperature': 0.7,\n    'init_lr': 1e-3,\n    'num_epochs': 13,\n    'num_warmup_epochs': 1,\n    'num_sample_epochs': 0.2, # Fractional epochs for sampling because one epoch is quite long \n    'num_save_epochs': 5,\n}\n\noutput_hw = config_dict[\"image_size\"] \/\/ 2 ** config_dict[\"L\"]\noutput_c = config_dict[\"num_channels\"] * 4**config_dict[\"L\"] \/\/ 2**(config_dict[\"L\"] - 1)\nconfig_dict[\"sampling_shape\"] = (output_hw, output_hw, output_c)","e879cac2":"import glob\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\ntf.config.experimental.set_visible_devices([], 'GPU')\n\ndef get_train_dataset(image_path, image_size, num_bits, batch_size, skip=None, **kwargs):\n    del kwargs\n    train_ds = tf.data.Dataset.list_files(f\"{image_path}\/*.jpg\")\n    if skip is not None:\n        train_ds = train_ds.skip(skip)\n    train_ds = train_ds.shuffle(buffer_size=20000)\n    train_ds = train_ds.map(partial(map_fn, size=image_size, num_bits=num_bits, training=True))\n    train_ds = train_ds.batch(batch_size)\n    train_ds = train_ds.repeat()\n    return iter(tfds.as_numpy(train_ds))\n\n\ndef get_val_dataset(image_path, image_size, num_bits, batch_size, \n                    take=None, repeat=False, **kwargs):\n    del kwargs\n    val_ds = tf.data.Dataset.list_files(f\"{image_path}\/*.jpg\")\n    if take is not None:\n        val_ds = val_ds.take(take)\n    val_ds = val_ds.map(partial(map_fn, size=image_size, num_bits=num_bits, training=False))\n    val_ds = val_ds.batch(batch_size)\n    if repeat:\n        val_ds = val_ds.repeat()\n    return iter(tfds.as_numpy(val_ds))","9d819d00":"%%time\nnum_images = len(glob.glob(f\"{config_dict['image_path']}\/*.jpg\"))\nconfig_dict['steps_per_epoch'] = num_images \/\/ config_dict['batch_size']\ntrain_split = int(config_dict['train_split'] * num_images)\nprint(f\"{num_images} training images\")\nprint(f\"{config_dict['steps_per_epoch']} training steps per epoch\")\n\n#Train data\ntrain_ds = get_train_dataset(**config_dict, skip=train_split)\n\n# Val data\n# During training we'll only evaluate on one batch of validation \n# to save on computations\nval_ds = get_val_dataset(**config_dict, take=config_dict['batch_size'], repeat=True)\n\n# Sample\nplot_image_grid(postprocess(next(val_ds), num_bits=config_dict['num_bits'])[:25], \n                title=\"Input data sample\")","286a6592":"model, params = train_glow(train_ds, val_ds=val_ds, **config_dict)","338ab8a2":"print(\"Random samples evolution during training\")\nfrom PIL import Image\n\n# filepaths\nfp_in = \"samples\/step_*.png\"\nfp_out = \"sample_evolution.gif\"\n\n# https:\/\/pillow.readthedocs.io\/en\/stable\/handbook\/image-file-formats.html#gif\nimg, *imgs = [Image.open(f) for f in sorted(glob.glob(fp_in))]\nimg.save(fp=fp_out, format='GIF', append_images=imgs,\n         save_all=True, duration=200, loop=0)\n\nfrom IPython.core.display import display, HTML\ndisplay(HTML('<img src=\"sample_evolution.gif\">'))","22489b19":"# Optional, example code to load trained weights\nif False:\n    model = GLOW(K=config_dict['K'],\n                 L=config_dict['L'], \n                 nn_width=config_dict['nn_width'], \n                 learn_top_prior=config_dict['learn_top_prior'])\n\n    with open('weights\/model_epoch=100.weights', 'rb') as f:\n        params = model.init(random_key, jnp.zeros((config_dict['batch_size'],\n                                                     config_dict['image_size'],\n                                                     config_dict['image_size'],\n                                                     config_dict['num_channels'])))\n        params = flax.serialization.from_bytes(params, f.read())","f2e79631":"def reconstruct(model, params, batch):\n    global config_dict\n    x, z, logdets, priors = model.apply(params, batch, reverse=False)\n    rec, *_ = model.apply(params, z[-1], z=z, reverse=True)\n    rec = postprocess(rec, config_dict[\"num_bits\"])\n    plot_image_grid(postprocess(batch, config_dict[\"num_bits\"]), title=\"original\")\n    plot_image_grid(rec, title=\"reconstructions\")\n    \n\ndef interpolate(model, params, batch, num_samples=16):\n    global config_dict\n    i1, i2 = np.random.choice(range(batch.shape[0]), size=2, replace=False)\n    in_ = np.stack([batch[i1], batch[i2]], axis=0)\n    x, z, logdets, priors = model.apply(params, in_, reverse=False)\n    # interpolate\n    interpolated_z = []\n    for zi in z:\n        z_1, z_2 = zi[:2]\n        interpolate = jnp.array([t * z_1 + (1 - t) * z_2 for t in np.linspace(0., 1., 16)])\n        interpolated_z.append(interpolate)\n    rec, *_ = model.apply(params, interpolated_z[-1], z=interpolated_z, reverse=True)\n    rec = postprocess(rec, config_dict[\"num_bits\"])\n    plot_image_grid(rec, title=\"Linear interpolation\")","05b2a59b":"batch = next(val_ds)\nreconstruct(model, params, batch)","d64fd617":"sample(model, params, shape=(16,) + config_dict[\"sampling_shape\"],  key=random_key,\n       postprocess_fn=partial(postprocess, num_bits=config_dict[\"num_bits\"]),\n       save_path=\"samples\/final_random_sample_T=1.png\");","c861d72b":"sample(model, params, shape=(16,) + config_dict[\"sampling_shape\"], \n       key=jax.random.PRNGKey(1), sampling_temperature=0.7,\n       postprocess_fn=partial(postprocess, num_bits=config_dict[\"num_bits\"]),\n       save_path=\"samples\/final_random_sample_T=0.7.png\");","edeeea5d":"sample(model, params, shape=(16,) + config_dict[\"sampling_shape\"], \n       key=jax.random.PRNGKey(2), sampling_temperature=0.7,\n       postprocess_fn=partial(postprocess, num_bits=config_dict[\"num_bits\"]),\n       save_path=\"samples\/final_random_sample_T=0.7.png\");","f796880f":"sample(model, params, shape=(16,) + config_dict[\"sampling_shape\"], \n       key=jax.random.PRNGKey(3), sampling_temperature=0.5,\n       postprocess_fn=partial(postprocess, num_bits=config_dict[\"num_bits\"]),\n       save_path=\"samples\/final_random_sample_T=0.5.png\");","55ddc2b8":"interpolate(model, params, batch)","3af83c27":"interpolate(model, params, batch)","1438604a":"### Split\n\n#### Intuition\nThe split operation essentially \"retains\" part of the information at each scale: The channel dimension is effectively cut in half after each scale,  which makes the model a bit more lightweight computationally. This also introduces a *hierarchy* of latent variables.\n\nFor all scale except the last one:\n\\begin{align}\nz_i, x_i &= \\mbox{split}(x_{i - 1})\\\\\nx_{i} &= \\mbox{flow_at_scale_i}(x_i)\n\\end{align}\n\nWhere $z_i$ are the latent variables of the model.","1c7cc99c":"# GLOW\nGlow is based on the variational auto-encoder framework with normalizing flows. \nA normalizing flow *g* is a reversible operation with an easy to compute gradient, which allows for exact computation of the likelihood, via the chain rule equation:\n\n\\begin{align}\nx &\\leftrightarrow g_1(x) \\leftrightarrow \\dots \\leftrightarrow z\\\\\nz &= g_N \\circ \\dots \\circ g_1(x)\\\\\n\\log p(x) &= \\log p(z) + \\sum_{i=1}^N \\log \\det | \\frac{d g_{i}}{d g_{i - 1}} | \\ \\ \\ \\ \\ (1)\n\\end{align}\n\nwhere $x$ is the input data to model, and $z$ is the latent, as in the standard VAE framework\n\n**Note**: In the Glow setup, the architecture is fully reversible, i.e., it is only composed of normalizing flow operations, which means we can compute $p(x)$ exactly. This also implies that there is no loss of information, i.e. $z$, and the intermediate variables, have as many parameters as $x$.\n\n## Overall architecture \nSimilar to [Real NVP](https:\/\/arxiv.org\/abs\/1605.08803), the Glow architecture has a multi-scale structure with $L$ scales, each containing $K$ iterations of  normalizing flow. Each block is separated by squeeze\/split operations.\n\n![Glow_figure](https:\/\/ameroyer.github.io\/images\/posts\/glow.png)\n\n\n### Squeeze\nThe goal of the squeeze operation is to trade-off spatial dimensions for channel dimensions; This preserves information, but has an impact on the field of view and computational efficiency (larger matrix multiplicatoins but fewer convolutional operations). `squeeze` simply splits the feature maps in `2x2xc` blocks and flatten each block to shape `1x1x4c`.","1062db6e":"**Note on jax.vmap:** The `vmap` function decorator can be used to indicate that a function should be vectorized across a given axis of its inputs (default is first axis). This is very useful to model a function that can be parallelized across a batch, e.g. a loss function like here or metrics.\n\n### Dequantization\n\nIn [A note on the evaluation of generative models](https:\/\/arxiv.org\/pdf\/1511.01844.pdf), the authors observe that typical generative models work with probability densities, considering images as continuous variables, even though images are typically discrete inputs in [0; 255]. A common technique to *dequantize* the data, is to add some small uniform noise to the input training images, which we can incorporate in the output pipeline.\n\nIn the original Glow implementation, they also introduce a `num_bits` parameter which allows for further controlling the quantization level of the input images (8 = standard `uint8`, 0 = binary images)","9294999b":"**Note on multi-devices training:** To extend the code for training on multi-devices we can make use of the `jax.vmap` operator (parallelize across XLA devices) instead of `jax.jit`, we also need to share the current parameters with all the devices (use `flax.jax_utils.replicate` on the optimizer before training), and finally to split the data across the devices, which can be handled with the `tf.data` in the input pipeline. [There is a more complete tutorial with an example here](https:\/\/flax.readthedocs.io\/en\/stable\/howtos\/ensembling.html)\n\n# Experiments\n\n**Note:** We run for 12 epochs due to time limits (roughly 40k training steps) + smaller flow depth (*K*) to fit into single GPU memory","ec7de41e":"## Train","63f26795":"This notebook contains an introduction of the paper [Glow: Generative Flow with Invertible 1\u00d71 Convolutions](https:\/\/arxiv.org\/pdf\/1807.03039.pdf) with an implementation in `jax`. I've also incorporated some of the \"tricks\" from the authors' original [tensorflow repository](https:\/\/github.com\/openai\/glow\/blob\/master\/model.py#L559), though not all (e.g. of .","30cca366":"### Affine Coupling (ACL)\n\n**Forward**\n\\begin{align}\nx_a, x_b &= \\mbox{split}(x)\\\\\n(\\log \\sigma, \\mu) &= \\mbox{NN}(x_b)\\\\\ny_a &= \\sigma \\odot x_a + \\mu\\\\\ny &= \\mbox{concat}(y_a, x_b)\n\\end{align}\n\n**Backward**\n\\begin{align}\ny_a, y_b &= \\mbox{split}(y)\\\\\n(\\log \\sigma, \\mu) &= \\mbox{NN}(y_b)\\\\\nx_a &= (x_a - \\mu) \/ \\sigma\\\\\nx &= \\mbox{concat}(x_a, y_b)\n\\end{align}\n\n\n**Log-det:**\n$\\log \\det \\mbox{ACL} = \\sum \\log (| \\sigma |)$\n","27f44c76":"### Latent space\nFinally, we can look at the linear interpolation in the learned latent space: We generate embedding $z_1$ and $z_2$ by feeding two validation set images to Glow. Then we plot the decoded images for latent vectors $t + z_1 + (1 - t) z_2$ for $t \\in [0, 1]$ (at all level of the latent hierarchy).\n\n**Note on conditional modeling**  The model can also be extented to conditional generation (in the original code this is done by (i) learning the top prior from one-hot class embedding rather than all zeros input, and (ii) adding a small classifier on top of the output latent which should aim at predicting the correct class).\n\nIn the original paper, this allows them to do \"semantic manipulation\" on the Celeba dataset by building representative centroid vectors for different attributes\/classes (e.g.g $z_{smiling}$ and $z_{non-smiling}$). They can use then use the vector direction $z_{smiling}$ - $z_{non-smiling}$ as a guide to browse the latent space (in that example, to make images more or less \"smiling\").","c5447bcf":"## Training loop","74a9ec68":"## Final model\n\nOnce we have the flow step definition, we can finally buid the multi-scale Glow architecture. The naming of the different modules is important as it guarantees that the parameters are shared adequately between the forward and reverse pass.","e3a2dfe8":"### Sampling\nNow let's take some random samples from the model, starting from the learned priors","bb0c23d7":"And we can of course implement the reverse operation as follows:","15551e87":"### Activation Norm\n\n**Forward**\n\\begin{align}\ny = x * \\sigma + \\mu\n\\end{align}\n\n**Backward**\n\\begin{align}\nx = (y - \\mu) \/ \\sigma\n\\end{align}\n\n\n**Log-det:**\n$\\log \\det \\mbox{ActNorm} = h \\times w \\times \\sum \\log (| \\sigma |)$\n\nNote that $\\mu$ and $\\sigma$ are trainable variables (contrary to batch norm) and are initialized in a data-dependant manner, such that the first batch of data used for initialization is normalized to zero-mean and unit-variance.","4bb2becf":"# Training the model\n\n## Training loss\n\n### Latent Log-likelihood\nFollowing equation (1), we now only need to compute the likelihood of the latent variables, $\\log p(z)$ term; The remaining loss term is computed by accumulating the log-determinant when passing through every block of the normalizing flow.\n\nSince each $p(z)$ is a Gaussian by definition, the corresponding likelihood is easy to estimate:","bbebc301":"### Reconstructions\nAs a sanity check let's first look at image reconstructions: since the model is invertible these should always be perfect, up to small float errors, except in very bad cases e.g. NaN values or other numerical errors","ebf70da9":"## Flow step\nThe normalizing flow step in Glow is composed of 3 operations:\n  * `Affine Coupling Layer`: A coupling layer which splits the input data along channel dimensions, using the first half to estimate parameters of a transformation then applied to the second half (similar to [`RealNVP`](https:\/\/arxiv.org\/abs\/1605.08803)).\n  * `ActNorm`: Normalization layer similar to batch norm, except that the mean and standard deviation statistics are trainable parameters rather than estimated from the data (this is in particular useful here because the model sometimes has to be trained with very small batch sizes due to memory requirements)\n  * `Conv 1x1`: An invertible 1x1 convolution layer. This is a generalization of the channel permutation used in [`RealNVP`](https:\/\/arxiv.org\/abs\/1605.08803)\n  \nSee following sections for more details on each operation.","0f710ffa":"#### Learnable prior\nRemember that, in order to estimate the model likelihood $\\log p(x)$ in Equation (1), we need to compute the prior $p(z)$ on all the latent variables($z_1, \\dots, z_L$). In the original code, the prior for each $z_i$ is assumed to be Gaussian whose mean and standard deviation $\\mu_i$ and $\\sigma_i$ are learned.\n\nMore specifically, after the split operation, we obtain $z_i$, the latent variable at the current scale, and $x$, the remaining output that will be propagated down the next scale. Following the original repo, we add one convolutional layer on top of $x$, to estimate the $\\mu$ and $\\sigma$ parameters of the prior $p(z_i) = \\mathcal N(\\mu, \\sigma)$. \n\nIn summary, the **forward pass** (estimate the prior) becomes:\n\n\\begin{align}\nz, x &= split(x_{i})\\\\\n\\mu, \\sigma &= \\mbox{MLP}_{\\mbox{prior}}(x)\\\\\ny &= \\mbox{flow_at_scale_i}(x)\n\\end{align}\n\nand the reverse pass is \n\n\\begin{align}\nx &= \\mbox{flow_at_scale_i}^{-1}(y)\\\\\n\\mu, \\sigma &= \\mbox{MLP}_{\\mbox{prior}}(x)\\\\\nz &\\sim \\mathcal{N}(\\mu, \\sigma)\\\\\nx &= \\mbox{concat}(z, x)\n\\end{align}\n\nThe MLP is initialized with all zeros weights, which corresponds to a $\\mathcal N(0, 1)$ prior. The `split` operation combines with the learnable prior becomes:","3077c03d":"### Wrap-up: Normalizing flow","e471776c":"### Evaluation","90a7165c":"### Invertible Convolution\n\n\n\n**Forward**\n\\begin{align}\ny = W x\n\\end{align}\n\n**Backward**\n\\begin{align}\nx = W^{-1} y\n\\end{align}\n\n\n**Log-det:**\n$\\log \\det \\mbox{ActNorm} = h \\times w \\times \\sum \\log (| \\det (W)|)$\n\nIn order to make the determinant computation more efficient, the authors propose to work directly with the LU-decomposition of $W$ (*see original paper, section 3.2*), which is initialized as a rotation matrix.","34d6ba06":"**Note on jax.jit**: The `jit` decorator is essentially an optimization that compiles a block of operations acting on the same device together. See also the [jax doc](https:\/\/jax.readthedocs.io\/en\/latest\/notebooks\/quickstart.html)\n\n## Sampling\n\nDrawing a parallel to the standard VAE, we can see the *encoder* as a forward pass of the Glow module, and the *decoder* as a reverse pass (Glow$^{-1}$).  However, due to the reversible nature of the network, we do not actually need the reverse pass to compute the exact training objective, $p(x)$ as it depends only on the prior $p(z)$, and from the log-determinants of the normalizing flows leading from $x$ to $z$.\n\nIn other words, we only need the encoder for the training phase. The \"decoder\" (i.e., reverse Glow) is used for sampling only. A sampling pass is thus:","5f1efc73":"## Loading the data"}}