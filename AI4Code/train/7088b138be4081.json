{"cell_type":{"549702f8":"code","abb633a7":"code","3d3024cb":"code","b0414015":"code","d012fcf8":"code","63899419":"code","9d388360":"code","5a9b784a":"code","bd39db71":"code","e4c352d2":"code","a112c777":"code","d38bc24e":"code","9b7b893c":"code","3657c3d2":"code","3d9e2df7":"code","52179a55":"code","1d20b08b":"code","0acad9b4":"code","b3bc18dc":"code","b7edaaea":"code","c2ae2746":"code","241e2e02":"code","dfc48876":"markdown","95be33c0":"markdown","c1d1d085":"markdown","69524833":"markdown","57d32748":"markdown","06278e59":"markdown","f630feb3":"markdown","1516a942":"markdown","c5e672b2":"markdown","47f8a8c8":"markdown","df3a469e":"markdown","2dce3a17":"markdown","2202368f":"markdown","722d1207":"markdown","23a51f96":"markdown","01407e72":"markdown","622fc674":"markdown"},"source":{"549702f8":"\"\"\"Handle data\"\"\"\nimport numpy as np\nimport pandas as pd\n\n\"\"\"Metrics\"\"\"\nfrom sklearn.metrics import mean_absolute_error\n\n\"\"\"Feature Selection\"\"\"\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import RFECV\n\n\"\"\"Regressors\"\"\"\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","abb633a7":"train = pd.read_csv('..\/input\/train.csv')\n#test = pd.read_csv('test.csv')","3d3024cb":"#test_ID = test['ID']\ny_train = train['target']\n#y_train = np.log1p(y_train)\ntrain.drop(\"ID\", axis = 1, inplace = True)\ntrain.drop(\"target\", axis = 1, inplace = True)\n#test.drop(\"ID\", axis = 1, inplace = True)","b0414015":"columns_one_value = [element for element, ele in train.items() \n                     if (pd.Series(train[element], name=element)).nunique() == 1]","d012fcf8":"train = train.drop(columns_one_value, axis=1)\n#test = test.drop(columns_one_value, axis=1)\ntrain = train.round(16)\n#test = test.round(16)","63899419":"colsToRemove = []\ncolumns = train.columns\nfor i in range(len(columns)-1):\n    v = train[columns[i]].values\n    dupCols = []\n    for j in range(i + 1,len(columns)):\n        if np.array_equal(v, train[columns[j]].values):\n            colsToRemove.append(columns[j])\n            \ntrain.drop(colsToRemove, axis=1, inplace=True) \n#test.drop(colsToRemove, axis=1, inplace=True) ","9d388360":"print(\"Shape train: \", train.shape)\n#print(\"Shape test: \", test.shape)","5a9b784a":"pca = PCA()\npca.fit(train)","bd39db71":"# Plotting to visualize the best number of elements\nplt.figure(1, figsize=(9, 8))\nplt.clf()\nplt.axes([.2, .2, .7, .7])\nplt.plot(pca.explained_variance_ratio_, linewidth=2)\nplt.axis('tight')\nplt.xlabel('Number of Feautres')\nplt.ylabel('Variance Ratio')","e4c352d2":"ytrain = np.array(y_train)\nytrain = ytrain.astype('int')","a112c777":"# Initialize SelectKBest function\nX = SelectKBest(chi2, k=100).fit_transform(train, ytrain)","d38bc24e":"X.shape","9b7b893c":"RandForest_K_best = RandomForestRegressor()      \nRandForest_K_best.fit(X, ytrain)","3657c3d2":"ypred = RandForest_K_best.predict(X)","3d9e2df7":"mae = mean_absolute_error(ytrain, ypred)\nprint(\"MAE with 100 features: \", mae)","52179a55":"xg_reg = xgb.XGBRegressor(objective ='reg:linear', learning_rate=0.1, max_depth=3, n_estimators=300) \n# Initialize the RFECV function setting 3-fold cross validation\nrfecv = RFECV(estimator=xg_reg, step=1, cv=3)","1d20b08b":"rfecv = rfecv.fit(X, y_train)","0acad9b4":"print('Best number of features :', rfecv.n_features_)","b3bc18dc":"# Plotting the best features with respect to the Cross Validation Score\nplt.figure()\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Score of Selected Features\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","b7edaaea":"Xnew = X[:,rfecv.support_]\nprint(Xnew.shape)","c2ae2746":"xg_reg.fit(Xnew ,ytrain)","241e2e02":"ypred = xg_reg.predict(Xnew)\nmae = mean_absolute_error(ytrain, ypred)\nprint(\"MAE with 57 features: \", mae)","dfc48876":"I proceed removing columns with only one value.","95be33c0":"## 2.1 Dimensionality Reduction","c1d1d085":"Having a dataset with 100 features, I will try to extract the most important features from this, so I will apply a more sophisticated method based on applying Croos Validation through a regressor, in this case I will use XGBoost.","69524833":"In this part I try to test how is the performance with just 100 features using Random Forest","57d32748":"## 2.4 Applying Random Forest with Cross Validation based on XGBoost","06278e59":"This is one of the most <b>important<\/b> things we should priorize. There are many ways in how to extract the most important features. In this case I will proceed with this methodology:\n\n* Using PCA I will look at what is the best number of features\n* Having the \"best number of features\" I will proceed to extract the features with the method \"selection of K best\" tested with a random forest.\n* After that, I will apply Random Forest with Croos Validation based on XGBoost","f630feb3":"# Santander Value Prediction\n\nIn this notebook I will show you an approach in how to handle a dataset with several features. I will implement two techniques which are:\n\n* PCA\n* Selection of K best\n* Random Forest With Cross Validation\n\nUsing both techniques I'll try to show a way in how to extract important features from this dataset. I use Extreme Gradient Boost for better results.\n\n### Important!\n##### For future work I will try to apply permutation for each feature column and show you how the MAE is improved or worsen","1516a942":"Obtaining name of columns with only one value, which are going to be dropped from train and test datasets","c5e672b2":"## 2.3 Testing with Random Forest","47f8a8c8":"# 2. Feature Extraction","df3a469e":"## 3. Conclusion\n\nWe can observe the follows results:\n\n* <b>MAE - RandomForest<\/b> with 100 features: 3658356.481798614\n* <b>MAE - XGBoost<\/b> with 57 features: 4853294.47253908\n\nFurther we could apply:\n* <b>Permutation<\/b> to see how could be the model behavior for every feature. \n* <b>Kolmogorov Smirnov<\/b> to test the null hypothesis for every distribution of each feature.","2dce3a17":"## 1.1 Reading csv and dropping garbage","2202368f":"## 2.5 Apply XGBoost","722d1207":"As we can see, the best score is obtained for 57 features. So lets modify our dataset and apply a regressor.","23a51f96":"## 2.2 Selecting the K best Features","01407e72":"# 1. Loading libraries","622fc674":"So as we can see, the best number of features is in the range between [50-100] aprox. I will proceed to choose the best 100 features."}}