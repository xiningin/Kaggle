{"cell_type":{"c7f40951":"code","9901fef6":"code","0b33def9":"code","527a719f":"code","82f1ff86":"code","992789c5":"code","8a87da32":"code","390a48d9":"code","ce04ee67":"code","934a9bc8":"code","b3b13e7d":"code","7c840219":"code","dd46562c":"code","90bc434c":"code","55e92592":"code","c8e7e489":"code","b15d056f":"code","1da74618":"code","9daf8be9":"code","ff9233b0":"code","d0733f50":"code","11ed0703":"code","a962356b":"code","0541c522":"code","eceb83d4":"code","bc7c93ee":"code","21d89ccd":"code","11474001":"code","395fd5e9":"code","17a63af7":"code","ddae0766":"code","b3b97035":"code","d02ff82d":"code","d95c78ad":"code","ab1cff80":"code","0b3526fb":"code","e3cc4bd6":"code","404ebe0f":"code","06533a53":"code","5b48e98c":"code","5bcdf6ef":"code","999e745e":"code","6f9e80d9":"code","de4e9937":"code","496a9e7b":"code","42e6776e":"code","3734d775":"code","861563d2":"code","3ed2cf39":"code","6ea7ca67":"code","17509862":"code","a587ef8d":"code","0f5a7178":"code","f2ee8dfe":"code","bb239cd5":"code","38da1a2c":"code","c0d5d25f":"code","32fe45c5":"code","3fd5b92c":"code","10980d38":"code","3764c8c9":"code","d40b5611":"code","b1523c03":"code","df2712e8":"markdown","25c6bc32":"markdown","230b65da":"markdown","18b08d32":"markdown","5ef7c240":"markdown","7af88d9f":"markdown","7678dc15":"markdown","bd19685b":"markdown","1631a6c6":"markdown","ad482767":"markdown","25d2ca35":"markdown","6a99b967":"markdown","2f49ebad":"markdown","f103fc4c":"markdown","9d18797e":"markdown","2eafdb95":"markdown","5eed8628":"markdown","2e626e0f":"markdown","5f52c0a8":"markdown"},"source":{"c7f40951":"from datetime import datetime\n\nprint(\"last update: {}\".format(datetime.now())) ","9901fef6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebrafor dirname, _, filenames in os.walk('\/kaggle\/input'):\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0b33def9":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","527a719f":"# Read the data\nX_original = pd.read_csv('..\/input\/train.csv', index_col='Id')\nX_test_full = pd.read_csv('..\/input\/test.csv', index_col='Id')\nX = X_original.copy()","82f1ff86":"print('Train_shape:', X_original.shape, 'test_shape:', X_test_full.shape)","992789c5":"X.describe().iloc[:,15:]","8a87da32":"#X.dtypes All variables are in type int64","390a48d9":"# Target variable is Cover_Type\nset(X.Cover_Type)","ce04ee67":"# There are 30 types of soil, named Soil_type1, Soil_type2 so on unitil Soil_type30\nsoil_fields = ['Soil_Type'+ str(i) for i in range(1,41)]\ngf = pd.DataFrame()\nfor feature in soil_fields:\n    gf = pd.concat([gf,X[feature].value_counts()],axis = 1)","934a9bc8":"gf","b3b13e7d":"gf.loc[:, gf.isna().any()]","7c840219":"X_test_full.head()","dd46562c":"gf2 = pd.DataFrame()\nfor feature in ['Soil_Type'+ str(i) for i in [7,15]]:\n    gf2 = pd.concat([gf2,X_test_full[feature].value_counts()], axis=1)\n    \ngf2.index = ['not_present', 'present']\ngf2","90bc434c":"#Create single soil field (reverse the one hot encoding)\nsoil_fields = ['Soil_Type'+ str(i) for i in range(1,41)]\ntrain_soil = X[soil_fields]\nX['Soil_Type'] = train_soil.idxmax(axis = 1).astype(str)\nX.head()","55e92592":"#Visiluazing what we said about soil of type 7 and 15\nax = X['Soil_Type'].value_counts().plot(kind = 'barh', figsize = (10, 12))\n# create a list to collect the plt.patches data\ntotals = []\n\n# find the values and append to list\nfor i in ax.patches:\n    totals.append(i.get_width())\n\n# set individual bar lables using above list\ntotal = sum(totals)\n\n# set individual bar lables using above list\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+.3, i.get_y()+.38, \\\n            str(round((i.get_width()\/total)*100, 2))+'%', fontsize=15,\ncolor='dimgrey')","c8e7e489":"#Create single wilderness area field (reverse the one hot encoding)\nWilderness_Area_Fields = ['Wilderness_Area'+ str(i) for i in range(1,5)]\n\ntrain_wilderness = X[Wilderness_Area_Fields]\nX['Wilderness_Area'] = train_wilderness.idxmax(axis = 1)\nX.head()","b15d056f":"# There are 4 types of Wilderness area field, named Wilderness_Area1 , 2, 3  and 4\nWilderness_Areas = ['Wilderness_Area'+ str(i) for i in range(1,5)]\nwf = pd.DataFrame()\nfor feature in Wilderness_Areas:\n    wf = pd.concat([wf,X[feature].value_counts()],axis = 1)\n\nwf.index = ['not present', 'present']\nwf","1da74618":"# I don't know if this graph is useful, but let's get a look at!\nwf.plot(kind = 'bar')\n","9daf8be9":"eda = X.copy()\neda.head()","ff9233b0":"eda.drop(eda.columns[10:54], axis = 1, inplace = True)","d0733f50":"eda.head()","11ed0703":"cover_type_index_to_name = {1: 'Spruce\/Fir', 2: 'Lodgepole Pine', 3 : 'Ponderosa Pine', 4: 'Cottonwood\/Willow',5: 'Aspen',6: 'Douglas-fir',7: 'Krummholz'}","a962356b":"cover_type_index_to_name[2]","0541c522":"eda['Cover_Type_Name'] = eda['Cover_Type'].map(cover_type_index_to_name)","eceb83d4":"eda.head()","bc7c93ee":"eda.drop(['Cover_Type'], axis = 1, inplace = True)","21d89ccd":"eda.head()","11474001":"import scipy.stats as st","395fd5e9":"contengency = pd.crosstab(eda.Soil_Type, eda.Cover_Type_Name, margins=True, margins_name='Total')\ncontengency","17a63af7":"st_chi2, st_p, st_dof, st_exp = st.chi2_contingency(contengency)","ddae0766":"print('Khi2_calculate:', st_chi2, 'P value of test:', st_p, 'degree of freedom:', st_dof)","b3b97035":"heatData1 = (contengency.values - st_exp)**2\/st_exp\nheatData1 = pd.DataFrame(heatData1, columns=contengency.columns, index = contengency.index)\nplt.figure(figsize=(20, 20))\nsns.heatmap(heatData1, cmap=\"YlGnBu\", annot=True)","d02ff82d":"#barplot of soil type\ncontengency.iloc[0:-1,0:-1].T.plot(kind = 'barh', figsize=(17,15))\nplt.grid()\nplt.show()","d95c78ad":"# expected_Table in case of independance\nexpected_Table = pd.DataFrame(st_exp, columns=contengency.columns, index = contengency.index)\nexpected_Table","ab1cff80":"contengency2 = pd.crosstab(eda.Wilderness_Area, eda.Cover_Type_Name, margins=True, margins_name='Total')\ncontengency2","0b3526fb":"st_chi22, st_p2, st_dof2, st_exp2 = st.chi2_contingency(contengency2)","e3cc4bd6":"heatData = (contengency2.values - st_exp2)**2\/st_exp2","404ebe0f":"heatData = pd.DataFrame(heatData, columns=contengency2.columns, index = contengency2.index)\nheatData","06533a53":"plt.figure(figsize=(10, 10))\nsns.heatmap(heatData, cmap=\"YlGnBu\", annot=True)","5b48e98c":"print('Khi2_calculate:', st_chi22, 'P value of test:', st_p2, 'degree of freedom:', st_dof2)","5bcdf6ef":"\ncontengency2.iloc[0:-1,0:-1].T.plot(kind = 'barh', figsize = (10,12))\nplt.grid()","999e745e":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\n\nExperiments = {\"Algo\":[\"Decicion Tree\", \"knnclassifier\", \"LogisticRegression\", \"svc_classif\", 'XGBClassifier'],\n              \"object\": [lambda: DecisionTreeClassifier(),\n                        lambda: KNeighborsClassifier(),\n                        lambda: LogisticRegression(random_state=0,solver ='newton-cg', multi_class='multinomial'),\n                        lambda: SVC(gamma='scale', kernel='rbf', probability=True),\n                        lambda: XGBClassifier( learning_rate =0.1, n_estimators=1000)],\n              \"prediction\": [[] for _ in range(5)]}\n\nX_copy = X_original.copy()\nscale = StandardScaler()\nX_copy.iloc[:,0:10] = scale.fit_transform(X_copy.iloc[:,0:10])\n[_.shape for _ in train_test_split(X_copy.drop('Cover_Type', axis = 1), X['Cover_Type'], test_size = 0.25)]","6f9e80d9":"actuals = []\nfor _ in range(3):\n    X_train, X_valid, y_train, y_valid = train_test_split(X_copy.drop('Cover_Type', axis = 1), X['Cover_Type'], test_size = 0.25)\n    for i, obj in enumerate(Experiments[\"object\"]):\n        if i == 0:\n            model = obj()\n            model.fit(X_train.iloc[:,10:], y_train)\n            Experiments[\"prediction\"][i] +=list(model.predict(X_valid.iloc[:,10:]))\n        else:\n            model = obj()\n            model.fit(X_train, y_train)\n            Experiments[\"prediction\"][i] +=list(model.predict(X_valid))\n\n    actuals += list(y_valid)\nactuals = pd.Series(actuals)\nExperiments[\"prediction\"] = list(map(pd.Series, Experiments[\"prediction\"]))","de4e9937":"agg_model = pd.DataFrame()\nagg_model_col = []\nfor i, algo in enumerate(Experiments[\"Algo\"]):\n    agg_model = pd.concat([agg_model, Experiments[\"prediction\"][i]], axis = 1)\n    agg_model_col.append(algo)\n\nagg_model = pd.concat([agg_model, actuals], axis = 1)\nagg_model_col.append(\"Real_Prediction\")\nagg_model.columns = agg_model_col\nagg_model.head(10)","496a9e7b":"from sklearn.metrics import confusion_matrix, accuracy_score\nfor i, algo in enumerate(Experiments[\"Algo\"]):\n    print(\"Confusion Matrix for {}\\n\".format(algo))\n    cm = confusion_matrix(agg_model.iloc[:,-1], agg_model.iloc[:,i])\n    df_cm = pd.DataFrame(data = cm, columns=np.unique(agg_model.iloc[:,-1]), index = np.unique(agg_model.iloc[:,-1]))\n    print(\"Acc_score: {}\\n\".format(accuracy_score(agg_model.iloc[:,-1], agg_model.iloc[:,i])))\n    print(str(df_cm),\"\\n\\n\")","42e6776e":"from sklearn.manifold import TSNE\nfrom sklearn import preprocessing\nfrom sklearn import decomposition\nimport random","3734d775":"random.seed(0)\nrge=random.sample(range(15120),  2000)","861563d2":"eda_matrix = eda.iloc[rge,:10].values","3ed2cf39":"std_scale = preprocessing.StandardScaler().fit(eda_matrix)\neda_matrix_scaled = std_scale.transform(eda_matrix)","6ea7ca67":"pca = decomposition.PCA(n_components=7)\npca.fit(eda_matrix_scaled )","17509862":"print (pca.explained_variance_ratio_)\nprint (pca.explained_variance_ratio_.sum())","a587ef8d":"X_projected = pca.transform(eda_matrix_scaled)\npca_data = np.vstack((X_projected[:, 0], X_projected[:, 1], eda.Cover_Type_Name[rge])).T\npca_data = pd.DataFrame(data = pca_data)\npca_data.columns = ['dim1 28,55%', 'dim2 22,66%', 'name']\npca_data.head(10)","0f5a7178":"type(X_projected[:, 0])","f2ee8dfe":"plt.figure(figsize = (10, 10))\nsns.scatterplot(x=\"dim1 28,55%\", y=\"dim2 22,66%\",\n              hue=\"name\",\n              data=pca_data)","bb239cd5":"from sklearn.manifold import TSNE","38da1a2c":"eda_tsne = TSNE(n_components=3, random_state=0)","c0d5d25f":"tsne_data = eda_tsne.fit_transform(X_projected)","32fe45c5":"tsne_data\n","3fd5b92c":"tsne_df = pd.DataFrame(data = tsne_data, columns = ['dim1', 'dim2', 'dim3'])\ntsne_df.shape","10980d38":"len(eda.Cover_Type_Name[rge].values)","3764c8c9":"tsne_df['name'] = list(eda.Cover_Type_Name[rge])\ntsne_df.head(10)","d40b5611":"plt.figure(figsize = (10, 10))\nsns.scatterplot(x=\"dim1\", y=\"dim2\",\n              hue=\"name\",\n              data=tsne_df)","b1523c03":"### Visualising Tsne in 3 dimensiosn\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(figsize = (10,10))\nax = Axes3D(fig)\nax.scatter(tsne_df.iloc[:,0], tsne_df.iloc[:,1], tsne_df.iloc[:,2], cmap='tab10')\nplt.show()","df2712e8":"### Some statistics about columns","25c6bc32":"Display chi2 and the p_value of test in ","230b65da":"#### 2. Wilderness area field","18b08d32":"We get all (p_value<0.01) to rejet the hypothesis of independance between cover_type and soil type, so that this two variable are correlated.","5ef7c240":"#### The explained Variance is low, abandon with pca","7af88d9f":"#### 1. Soil Type ","7678dc15":"cottonwood\/Willow forest type have his area wilderness just in type 4. Area wildeness of type 2 is too scarce, and just belongs in forest of spruce-fr, Krummholz and Lodgepol pine","bd19685b":"### It's time now to perform data analysis:\n\n1. Firstly we will start studying the relationship between the target variable Cover_Type_Name and our two categoricals variable (i.e see if variables are indenpedent)\n\n2. secondly We will do multivariate analysis with numerical variables (PCA, TSNE, color by target variable). This can helps us fit our classification\n\n\n","1631a6c6":"#### Multivariate analysis","ad482767":"No comment Soil of type 7 and 15 do not appear!","25d2ca35":"It appear That the soils of type 7 and 15 are rares. 105\/565892 for type 7 and 3\/565892 for type 15. They do not appear in the given training set. \nWe are going to clearly find how to deal with these variables.\nAnalysis is capital because it is important that our model generalize well on unknown data.","6a99b967":"## See the distributions of numericals columns ","2f49ebad":"### Statistics on categoricals values","f103fc4c":"A simple decision trees with just those two variable could be good to predict the type of forest, because of their dependances with cover type. It 's just an intuition!\n\nOnly looking for example the previous graph we could therefore say that:\n\n- if forest have only his **wilderness_area of type 4** we could predict that the forest type is **cottonwood\/Willow**\n\n- We could predict other type of forest using both wilderness_area type and the soil type\n\n\n#### An Experiment to confirm our intution about the tree decision with it ability to good predict cottonwood\/willow forest\n","9d18797e":"#### Khi 2 test","2eafdb95":"It appear that soil of type 7 and 15 just have unique value (0).\nLet's see if this kind of soil type appear in the given test set","5eed8628":"## First Part: Exploratory Data Analysis.","2e626e0f":"Our intuition is not so well.\nHowever Our Experiment show that KNeighborsClassifier and svc can be good! We will combine its in our final model, using **VotingClassifier** from sklearn.ensemble\n","5f52c0a8":"Not easy to visualize differents groups in 2 dimensions"}}