{"cell_type":{"998cc73d":"code","4e525aad":"code","2d1cebb3":"code","b81d63ed":"code","97fe45fd":"code","3b3977db":"code","48ebe94a":"code","4b7435eb":"code","23c84a39":"code","d3ab1205":"code","2c1d77cb":"code","39fc509c":"code","bfb6fe8b":"code","11889765":"code","5c84a326":"code","13144bf1":"code","25e8dced":"code","9dfebc4d":"code","81433c95":"code","a4cdcf17":"code","6d85889b":"code","3d6aa1a2":"code","e5dcaa8b":"code","7333432c":"code","70411155":"code","4a5cc189":"code","60ac0c24":"code","056ba123":"code","e97fd309":"code","ba8523dd":"code","9fa1d06f":"code","cea5d3a0":"code","7f9e137e":"code","e57a8dd9":"code","7f5603a7":"code","0716cca2":"code","c1390c2c":"code","d81dd832":"code","06f493a9":"code","e720e5cd":"code","384441b0":"code","cda91a2d":"code","e1fa1548":"code","ec3603d2":"code","9bed0989":"code","7b317575":"code","c70877e8":"code","741b539c":"code","c81078d2":"code","56bbf101":"code","e23b4287":"code","1c8bcf4c":"code","0eac03e8":"code","3deb30dd":"code","2c10a67d":"code","cfeb5df4":"markdown","3234864e":"markdown","bf25e08e":"markdown","cab51cf3":"markdown","4718bd51":"markdown","dbf09fa7":"markdown","26ba73df":"markdown","56ddcceb":"markdown","d7befe41":"markdown","2278468b":"markdown"},"source":{"998cc73d":"!pip install git+https:\/\/github.com\/tensorflow\/examples.git","4e525aad":"# GENERAL\n\nimport os\nimport os.path\nfrom pathlib import Path\nimport time\nimport pandas as pd\nimport numpy as np\nimport random\nimport time\nimport math\nimport glob\nimport cv2\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\n\n# I PACKAGES\n\nimport tensorflow as tf\nimport tensorflow.keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n                        Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN,\\\nLSTM, GlobalAveragePooling2D, SeparableConv2D, ZeroPadding2D, Convolution2D, ZeroPadding2D,Reshape, Conv2DTranspose, LeakyReLU, ReLU, Conv2DTranspose\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom tensorflow_examples.models.pix2pix import pix2pix\n\n# WARNINGS\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\",category=DeprecationWarning)\nfilterwarnings(\"ignore\", category=FutureWarning) \nfilterwarnings(\"ignore\", category=UserWarning)\n\n\nAUTOTUNE = tf.data.AUTOTUNE","2d1cebb3":"BYZANTINE_ARC_PATH = Path(\"..\/input\/architecture-dataset\/arcDataset\/Byzantine architecture\")\nGOTHIC_ARC_PATH = Path(\"..\/input\/architecture-dataset\/arcDataset\/Gothic architecture\")","b81d63ed":"BYZANTINE_JPG_LIST = list(BYZANTINE_ARC_PATH.glob(r\"*.jpg\"))\nGOTHIC_JPG_LIST = list(GOTHIC_ARC_PATH.glob(r\"*.jpg\"))","97fe45fd":"print(len(BYZANTINE_JPG_LIST))\nprint(len(GOTHIC_JPG_LIST))","3b3977db":"GOTHIC_JPG_LIST = GOTHIC_JPG_LIST[:83]","48ebe94a":"print(len(GOTHIC_JPG_LIST) == len(BYZANTINE_JPG_LIST))","4b7435eb":"GOTHIC_SERIES = pd.Series(GOTHIC_JPG_LIST,name=\"GOTHIC\").astype(str)\nBYZANTINE_SERIES = pd.Series(BYZANTINE_JPG_LIST,name=\"BYZANTINE\").astype(str)\n\nMAIN_ARC = pd.concat([BYZANTINE_SERIES,GOTHIC_SERIES],axis=1)","23c84a39":"MAIN_ARC","d3ab1205":"figure,axis = plt.subplots(3,3,figsize=(14,14))\n\nfor i_ind,i_ops in enumerate(axis.flat):\n    \n    IMAGE_READING = cv2.cvtColor(cv2.imread(MAIN_ARC[\"BYZANTINE\"][i_ind]),cv2.COLOR_BGR2RGB)\n    i_ops.set_title(\"BYZANTINE\")\n    i_ops.axis(\"off\")\n    i_ops.imshow(IMAGE_READING)\n    \nplt.tight_layout()\nplt.show()    ","2c1d77cb":"figure,axis = plt.subplots(3,3,figsize=(14,14))\n\nfor i_ind,i_ops in enumerate(axis.flat):\n    \n    IMAGE_READING = cv2.cvtColor(cv2.imread(MAIN_ARC[\"GOTHIC\"][i_ind]),cv2.COLOR_BGR2RGB)\n    i_ops.set_title(\"GOTHIC\")\n    i_ops.axis(\"off\")\n    i_ops.imshow(IMAGE_READING)\n    \nplt.tight_layout()\nplt.show()    ","39fc509c":"BUFFER_SIZE = 400\nBATCH_SIZE = 1\nIMG_WIDTH = 256\nIMG_HEIGHT = 256\nOUTPUT_CHANNELS = 3\nLAMBDA = 100\nEPOCHS = 300\nTYPE_ARRAY = \"float32\"","bfb6fe8b":"BYZANTINE_TARGET = []\nGOTHIC_TARGET = []\n\nfor i_byzantine,i_gothic in zip(MAIN_ARC.BYZANTINE,MAIN_ARC.GOTHIC):\n    \n    BYZ_READING = cv2.cvtColor(cv2.imread(i_byzantine),cv2.COLOR_BGR2RGB)\n    GOT_READING = cv2.cvtColor(cv2.imread(i_gothic),cv2.COLOR_BGR2RGB)\n    \n    BYZ_RESIZE = cv2.resize(BYZ_READING,(IMG_WIDTH,IMG_HEIGHT))\n    GOT_RESIZE = cv2.resize(GOT_READING,(IMG_WIDTH,IMG_HEIGHT))\n    \n    BYZ_NORMALIZE = (BYZ_RESIZE \/ 127.5) - 1\n    GOT_NORMALIZE = (GOT_RESIZE \/ 127.5) - 1\n    \n    BYZANTINE_TARGET.append(BYZ_NORMALIZE)\n    GOTHIC_TARGET.append(GOT_NORMALIZE)","11889765":"print(np.shape(np.array(BYZANTINE_TARGET)))\nprint(np.shape(np.array(GOTHIC_TARGET)))","5c84a326":"figure,axis = plt.subplots(3,3,figsize=(15,15))\n\nfor x_number,x_operators in enumerate(axis.flat):\n    \n    EXAMPLE_PICKING = BYZANTINE_TARGET[x_number]\n    \n    x_operators.set_title(\"BYZANTINE\")\n    x_operators.axis(\"off\")\n    x_operators.imshow(EXAMPLE_PICKING)\n    \nplt.tight_layout()\nplt.show()","13144bf1":"figure,axis = plt.subplots(3,3,figsize=(15,15))\n\nfor x_number,x_operators in enumerate(axis.flat):\n    \n    EXAMPLE_PICKING = GOTHIC_TARGET[x_number]\n    \n    x_operators.set_title(\"GOTHIC\")\n    x_operators.axis(\"off\")\n    x_operators.imshow(EXAMPLE_PICKING)\n    \nplt.tight_layout()\nplt.show()","25e8dced":"ARRAY_BYZ = np.array(BYZANTINE_TARGET,dtype=TYPE_ARRAY)\nARRAY_GOT = np.array(GOTHIC_TARGET,dtype=TYPE_ARRAY)","9dfebc4d":"print(ARRAY_BYZ.shape)\nprint(ARRAY_GOT.shape)","81433c95":"BYZANTINE_TENSOR = tf.data.Dataset.from_tensor_slices(ARRAY_BYZ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\nGOTHIC_TENSOR = tf.data.Dataset.from_tensor_slices(ARRAY_GOT).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","a4cdcf17":"print(BYZANTINE_TENSOR.element_spec)\nprint(GOTHIC_TENSOR.element_spec)","6d85889b":"def DOWN_SAMPLE_CREATION(filters,size,apply_batchnorm=True):\n    \n    Initializer_Kernel = tf.random_normal_initializer(0.,0.02)\n    \n    Model = Sequential()\n    \n    Model.add(Conv2D(filters,size,\n                     strides=2,\n                     padding=\"same\",\n                     kernel_initializer=Initializer_Kernel,\n                     use_bias=False))\n    \n    if apply_batchnorm:\n        Model.add(BatchNormalization())\n        \n        \n    return Model","3d6aa1a2":"DOWN_MODEL = DOWN_SAMPLE_CREATION(3,4)\nRESULT_DOWN = DOWN_MODEL(tf.expand_dims(ARRAY_BYZ[0],0))","e5dcaa8b":"print(RESULT_DOWN.shape)","7333432c":"def UP_SAMPLE_CREATION(filters,size,apply_dropout=False):\n    \n    Initializer_Kernel = tf.random_normal_initializer(0.,0.02)\n    \n    Model = Sequential()\n    \n    Model.add(Conv2DTranspose(filters,size,\n                     strides=2,\n                     padding=\"same\",\n                     kernel_initializer=Initializer_Kernel,\n                     use_bias=False))\n    \n    Model.add(BatchNormalization())\n    \n    \n    if apply_dropout:\n        \n        Model.add(Dropout(0.3))\n        \n    Model.add(ReLU())\n    \n    return Model\n    ","70411155":"UP_MODEL = UP_SAMPLE_CREATION(3,4)\nRESULT_UP = UP_MODEL(RESULT_DOWN)","4a5cc189":"print(RESULT_UP.shape)","60ac0c24":"def MODEL_GENERATOR(w_shape,h_shape,dim_shape):\n    \n    INPUT = tf.keras.layers.Input(shape=[w_shape,h_shape,dim_shape])\n    \n    DOWN_STACK = [\n        \n        DOWN_SAMPLE_CREATION(64,4,apply_batchnorm=False),\n        DOWN_SAMPLE_CREATION(128,4),\n        DOWN_SAMPLE_CREATION(256,4),\n        DOWN_SAMPLE_CREATION(512,4),\n        DOWN_SAMPLE_CREATION(512,4),\n        DOWN_SAMPLE_CREATION(512,4),\n        DOWN_SAMPLE_CREATION(512,4),\n        DOWN_SAMPLE_CREATION(512,4),\n    ]\n    \n    \n    UP_STACK = [\n        \n        UP_SAMPLE_CREATION(512,4,apply_dropout=True),\n        UP_SAMPLE_CREATION(512,4,apply_dropout=True),\n        UP_SAMPLE_CREATION(512,4,apply_dropout=True),\n        UP_SAMPLE_CREATION(512,4),\n        UP_SAMPLE_CREATION(256,4),\n        UP_SAMPLE_CREATION(128,4),\n        UP_SAMPLE_CREATION(64,4),\n    ]\n    \n    \n    INITIALIZER_KERNEL = tf.random_normal_initializer(0.,0.02)\n    \n    LAST_LAY = Conv2DTranspose(OUTPUT_CHANNELS,\n                               4,\n                               strides=2,padding=\"same\",\n                              kernel_initializer=INITIALIZER_KERNEL,\n                              activation=\"tanh\")\n    \n    x = INPUT\n    \n    skips_lay = []\n    \n    for x_down in DOWN_STACK:\n        \n        x = x_down(x)\n        skips_lay.append(x)\n        \n    skips_lay = reversed(skips_lay[:-1])\n    \n    for x_up,skipping in zip(UP_STACK,skips_lay):\n        \n        x = x_up(x)\n        x = tf.keras.layers.Concatenate()([x,skipping])\n        \n    x = LAST_LAY(x)\n    \n    \n    return tf.keras.Model(inputs=INPUT,outputs=x)","056ba123":"Generator_Model = MODEL_GENERATOR(IMG_WIDTH,IMG_HEIGHT,OUTPUT_CHANNELS)","e97fd309":"tf.keras.utils.plot_model(Generator_Model, show_shapes=True, dpi=64)","ba8523dd":"OUTPUT_GENERATOR_EXAMPLE = Generator_Model(ARRAY_BYZ[0][tf.newaxis,...],training=False)","9fa1d06f":"plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.imshow(OUTPUT_GENERATOR_EXAMPLE[0])\nplt.show()","cea5d3a0":"def MODEL_DISCRIMINATOR(w_shape,h_shape,dim_shape):\n    \n    INITIALIZER_KERNEL = tf.random_normal_initializer(0.,0.02)\n    \n    INPUT = tf.keras.layers.Input(shape=[w_shape,h_shape,dim_shape],name=\"INPUT_IMAGE\")\n    TARGET = tf.keras.layers.Input(shape=[w_shape,h_shape,dim_shape],name=\"TARGET_IMAGE\")\n    \n    \n    x = tf.keras.layers.concatenate([INPUT,TARGET])\n    \n    DOWN_L_1 = DOWN_SAMPLE_CREATION(64,4,False)(x)\n    DOWN_L_2 = DOWN_SAMPLE_CREATION(64,4)(DOWN_L_1)\n    DOWN_L_3 = DOWN_SAMPLE_CREATION(64,4)(DOWN_L_2)\n    \n    ZERO_PADDING_1 = tf.keras.layers.ZeroPadding2D()(DOWN_L_3)\n    CONV2D_1 = Conv2D(512,\n                      4,\n                      strides=1,\n                      kernel_initializer=INITIALIZER_KERNEL,\n                     use_bias=False)(ZERO_PADDING_1)\n    \n    BATCHNORM_1 = BatchNormalization()(CONV2D_1)\n    LEAKY_RELU_1 = LeakyReLU()(BATCHNORM_1)\n    \n    ZERO_PADDING_2 = tf.keras.layers.ZeroPadding2D()(LEAKY_RELU_1)\n    \n    LAST_LAY = Conv2D(1,\n                      4,\n                      strides=1,\n                      kernel_initializer=INITIALIZER_KERNEL)(ZERO_PADDING_2)\n    \n    \n    return tf.keras.Model(inputs=[INPUT,TARGET],outputs=LAST_LAY)","7f9e137e":"Discriminator_Model = MODEL_DISCRIMINATOR(IMG_WIDTH,IMG_HEIGHT,OUTPUT_CHANNELS)","e57a8dd9":"tf.keras.utils.plot_model(Discriminator_Model, show_shapes=True, dpi=64)","7f5603a7":"OUTPUT_DISCRIMINATOR_EXAMPLE = Discriminator_Model([ARRAY_BYZ[0][tf.newaxis,...],OUTPUT_GENERATOR_EXAMPLE],\n                                                   training=False)","0716cca2":"plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.imshow(OUTPUT_DISCRIMINATOR_EXAMPLE[0,...,-1],cmap='RdBu_r')\nplt.show()","c1390c2c":"LOSS_FUNCTION = tf.keras.losses.BinaryCrossentropy(from_logits=True)","d81dd832":"def Generator_Loss(discriminator_output,generator_output,target):\n    \n    GAN_LOSS = LOSS_FUNCTION(tf.ones_like(discriminator_output),discriminator_output)\n    \n    L1_MEAN_ABS_ERROR = tf.reduce_mean(tf.abs(target-generator_output))\n    \n    TOTAL_LOSS = GAN_LOSS + (LAMBDA * L1_MEAN_ABS_ERROR)\n    \n    return TOTAL_LOSS,GAN_LOSS,L1_MEAN_ABS_ERROR","06f493a9":"def Discriminator_Loss(real_out,generated_out):\n    \n    REAL_LOSS = LOSS_FUNCTION(tf.ones_like(real_out),real_out)\n    GEN_LOSS = LOSS_FUNCTION(tf.zeros_like(generated_out),generated_out)\n    \n    TOTAL_LOSS = REAL_LOSS + GEN_LOSS\n    \n    return TOTAL_LOSS","e720e5cd":"GENERATOR_OPTIMIZER = tf.keras.optimizers.Adam(lr=0.0003, beta_1=0.5)\nDISCRIMINATOR_OPTIMIZER = tf.keras.optimizers.Adam(lr=0.0003, beta_1=0.5)","384441b0":"os.mkdir(\".\/TRIP\")\ndef generate_images_for_example(model, test_input, tar, number_i):\n    \n    prediction = model(test_input, training=True)\n    \n    RESULT_IN_OUT_LIST = [test_input[0], tar[0], prediction[0]]\n    \n    \n    figure,axis = plt.subplots(1, 3,figsize=(14,14))\n\n    axis[0].imshow(RESULT_IN_OUT_LIST[0] * 0.5 + 0.5)\n    axis[0].axis('off')\n\n    axis[1].imshow(RESULT_IN_OUT_LIST[1] * 0.5 + 0.5)\n    axis[1].axis('off')\n\n    axis[2].imshow(RESULT_IN_OUT_LIST[2] * 0.5 + 0.5)\n    axis[2].axis('off')\n    plt.savefig('.\/TRIP\/trip_{:04d}.png'.format(number_i))\n    plt.tight_layout()\n    plt.show()","cda91a2d":"os.mkdir(\".\/SINGLE\")\ndef generate_images_single(model, test_input, tar, number_i):\n    \n    prediction = model(test_input, training=True)\n    \n    plt.figure(figsize=(15, 15))\n    plt.imshow(prediction[0] * 0.5 + 0.5)\n    plt.savefig('.\/SINGLE\/single_{:04d}.png'.format(number_i))\n    plt.axis('off')\n    plt.tight_layout()    \n    plt.show()","e1fa1548":"def generate_images(model, test_input, tar, number_i):\n    \n    prediction = model(test_input, training=True)\n    \n    plt.figure(figsize=(15, 15))\n    plt.imshow(prediction[0] * 0.5 + 0.5)\n    plt.savefig('output_{:04d}.png'.format(number_i))\n    plt.axis('off')\n    plt.tight_layout()    \n    plt.show()","ec3603d2":"for x_input,x_output in zip(BYZANTINE_TENSOR.take(1),GOTHIC_TENSOR.take(1)):\n    \n    generate_images_for_example(Generator_Model,x_input,x_output,1000000)","9bed0989":"for x_input,x_output in zip(BYZANTINE_TENSOR.take(1),GOTHIC_TENSOR.take(1)):\n    \n    generate_images(Generator_Model,x_input,x_output,1000000)","7b317575":"@tf.function\n\ndef Training_Step(INPUT_IMG,TARGET_IMG,EPOCH):\n    \n    with tf.GradientTape() as GEN_TAPE, tf.GradientTape() as DISC_TAPE:\n        \n        GENERATOR_OUT = Generator_Model(INPUT_IMG,training=True)\n        \n        DISCRIMINATOR_REAL = Discriminator_Model([INPUT_IMG,TARGET_IMG],training=True)\n        DISCRIMINATOR_GENERATED = Discriminator_Model([INPUT_IMG,GENERATOR_OUT],training=True)\n        \n        GEN_TOTAL_LOSS,GEN_GENERATOR_LOSS, GEN_L1_MEAN_ABS = Generator_Loss(DISCRIMINATOR_GENERATED,\n                                                                           GENERATOR_OUT,\n                                                                            TARGET_IMG)\n        \n        DISC_LOSS = Discriminator_Loss(DISCRIMINATOR_REAL,DISCRIMINATOR_GENERATED)\n        \n        \n    Generator_Gradients = GEN_TAPE.gradient(GEN_TOTAL_LOSS,Generator_Model.trainable_variables)\n    Discriminator_Gradients = DISC_TAPE.gradient(DISC_LOSS,Discriminator_Model.trainable_variables)\n    \n    GENERATOR_OPTIMIZER.apply_gradients(zip(Generator_Gradients,Generator_Model.trainable_variables))\n    DISCRIMINATOR_OPTIMIZER.apply_gradients(zip(Discriminator_Gradients,Discriminator_Model.trainable_variables))","c70877e8":"def Training_Start(TRAIN_DATA,TARGET_DATA,EPOCH):\n    \n    EXAMPLE_INP,EXAMPLE_TAR = next(iter(TRAIN_DATA.take(1))),next(iter(TARGET_DATA.take(1)))\n    \n    Starting_Time = time.time()\n    \n    for epc, (input_image_x,target_x) in enumerate(zip(TRAIN_DATA,TARGET_DATA)):\n        \n        if (epc) % 1000 == 0:\n            \n            clear_output(wait=True)\n            \n            if epc != 0:\n                print(f'TIME FOR 1000 STEPS: {time.time()-Starting_Time:.2f} sec\\n')\n                \n            Starting_Time = time.time()\n            \n            generate_images(Generator_Model,EXAMPLE_INP,EXAMPLE_TAR)\n            \n        Training_Step(input_image_x,target_x,epc)\n            \n        if (epc + 1) % 10 == 0:\n            print(\"-\",end=\"\",flush=True)","741b539c":"EXAMPLE_INP,EXAMPLE_TAR = next(iter(BYZANTINE_TENSOR.take(1))),next(iter(GOTHIC_TENSOR.take(1)))\n\ncounting_img = 0\n\nfor epoch in range(EPOCHS):\n\n    n = 0\n    \n    for image_x, image_y in tf.data.Dataset.zip((BYZANTINE_TENSOR, GOTHIC_TENSOR)):\n        \n        Training_Step(image_x, image_y, epoch)\n        \n        if n % 10 == 0:\n            \n            print ('.', end='')\n        n += 1\n\n    clear_output(wait=True)\n\n    generate_images(Generator_Model,\n                    image_x,\n                    image_y,\n                    counting_img)\n    \n    generate_images_single(Generator_Model,\n                           EXAMPLE_INP,\n                           EXAMPLE_TAR,\n                           counting_img)\n    \n    generate_images_for_example(Generator_Model,\n                                image_x,\n                                image_y,\n                                counting_img)\n    \n    counting_img = counting_img + 1","c81078d2":"#Training_Start(BYZANTINE_TENSOR,GOTHIC_TENSOR,EPOCHS) # \u0130F YOU WANT TO USE AS A FUNCTION, RUN THIS LINE INSTEAD OF RUNNING PREVIOUS","56bbf101":"SINGLE_TARGET_PATH = Path(\".\/TRIP\")\nSINGLE_JPG_TARGET = list(SINGLE_TARGET_PATH.glob(r\"*.png\"))\nSERIES_OUTPUT_TARGET = pd.Series(SINGLE_JPG_TARGET,name=\"SERIES_OUTPUT\").astype(str)","e23b4287":"print(len(SERIES_OUTPUT_TARGET))","1c8bcf4c":"TARGET_PATH_READING = []\n\nfor x_im in SERIES_OUTPUT_TARGET.values:\n\n    IMG_TARGET = cv2.cvtColor(cv2.imread(x_im),cv2.COLOR_BGR2RGB)\n    TARGET_PATH_READING.append(IMG_TARGET)","0eac03e8":"TARGET_PATH_READING = TARGET_PATH_READING[:100]","3deb30dd":"def displaying_resource(source):\n    \n    figure = plt.figure(figsize=(14,14))\n    \n    Image_List = []\n    plt.style.use(\"dark_background\")\n    for counting,indexing in enumerate(source):\n        \n        \n        Read_IMG = plt.imshow(indexing, animated=True)\n        plt.axis('off')\n        Image_List.append([Read_IMG])\n\n    Animation_Func = animation.ArtistAnimation(figure, Image_List, interval=244, repeat_delay=1000)\n    \n    plt.close()\n    \n    return Animation_Func","2c10a67d":"HTML(displaying_resource(TARGET_PATH_READING).to_html5_video())","cfeb5df4":"# LOSS FUNCTION","3234864e":"# OPTIMIZER AND GENERATED IMAGE","bf25e08e":"# DISCRIMINATOR","cab51cf3":"#### YOU CAN USE AS A FUNCTION IF YOU WANT","4718bd51":"# PREPARING","dbf09fa7":"# BEFORE TRAINING","26ba73df":"# GENERATOR","56ddcceb":"# LAYER FUNCTIONS","d7befe41":"# TRAINING FUNCTION","2278468b":"# TRAINING"}}