{"cell_type":{"d6d52b5d":"code","4c051c0d":"code","947f0f2d":"code","59b5574c":"code","479d55b7":"code","92e04623":"code","13b215f9":"code","51cd6272":"code","80e9ac2f":"code","661d71d2":"code","8142192e":"code","36659d72":"code","665a98ef":"code","d48b4d9f":"code","98afe1bf":"code","7d19cf5e":"code","0c8b05ab":"code","35805c0c":"code","843d9f97":"code","6f879af2":"code","9d48f5c7":"code","b32b1b24":"code","68b915ce":"code","593eb38c":"code","9f46c375":"code","3843f534":"code","a1d5b60f":"code","e9575889":"code","15b4641a":"code","0e233621":"code","0d78a519":"code","8b97522e":"code","c932f639":"code","dc6d8fc5":"code","b3c8efd9":"code","67f4d199":"code","6b1c5eda":"code","8124f747":"code","1885858d":"code","d693fbc6":"code","5e939064":"code","4b0b01ee":"code","6d8845d7":"code","e9e41477":"code","a0c8dc2e":"code","4f56cfc7":"code","257a2660":"code","b0cc6648":"code","adcda4fe":"code","9e22a7c6":"code","362ab6ee":"code","9b57578d":"code","0bc0b6b7":"code","e0a62c3b":"code","7f7a4888":"code","6b370434":"code","5c99b0f1":"code","82494f3e":"code","68bb48ee":"code","7f1c84f2":"code","47156b88":"code","9e94d6ae":"code","00a3c986":"code","ddbd7909":"code","bbc9190c":"code","02e88dd1":"code","ae50dc96":"code","6f9bfbb0":"code","5a8ce9ab":"code","6cd2a780":"code","9579e1f8":"code","474f0e35":"code","d3959c36":"code","cca0ede9":"code","93b3c1ba":"code","0fd7f3c7":"code","e6772b83":"code","e0737e79":"code","01a50fb3":"code","82a7c6e1":"code","a63b67bd":"code","83816c29":"code","0bf994eb":"code","86e6134b":"code","499e6be2":"code","3a2e014a":"markdown","321fc4fd":"markdown","0d2e8edd":"markdown","51d4b374":"markdown","a28158d5":"markdown","3f80ad14":"markdown","f4340dd8":"markdown","76da7ec2":"markdown","423ed404":"markdown","fa2b86be":"markdown","9f0045b5":"markdown","4da408cd":"markdown","8dd1b00a":"markdown","8ba71870":"markdown","d92c0240":"markdown","1a999b1d":"markdown","4ba66043":"markdown","47b90bef":"markdown","93208204":"markdown","fbc3ea0d":"markdown","97056c9f":"markdown","d1ca17be":"markdown","8bb967c4":"markdown","5955f8de":"markdown","c49acaf0":"markdown","7a472ee0":"markdown","45885924":"markdown","0f2d9117":"markdown","d78aad2d":"markdown","082da912":"markdown","44c5a9b4":"markdown","0b4e8c8d":"markdown","01efc629":"markdown","7422b3f7":"markdown","5fb0d72f":"markdown","9e64deb7":"markdown","22830d32":"markdown","e31de494":"markdown","60ee3831":"markdown","3b05581b":"markdown","94c8db5d":"markdown","b80d971a":"markdown","f165cfa3":"markdown","ff8b9940":"markdown","4f5a5561":"markdown","3c878d01":"markdown","083bed2d":"markdown","f24cbe23":"markdown","be98303c":"markdown","1c0817c0":"markdown","16b8f287":"markdown","cd7880fd":"markdown","c377bafc":"markdown","55d0d8dc":"markdown","355843cd":"markdown"},"source":{"d6d52b5d":"!pip install TPOT","4c051c0d":"!pip install pycaret","947f0f2d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.express as px\n\nimport cufflinks as cf \n\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import  RadiusNeighborsClassifier\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import NuSVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nimport xgboost as xgb\nfrom sklearn.linear_model import RidgeClassifier\nfrom catboost import Pool, CatBoostClassifier, cv\nimport lightgbm as lgb\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode(connected=True)  \nimport plotly.figure_factory as ff\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\nimport random\n","59b5574c":"def random_colors(number_of_colors):\n    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n                 for i in range(number_of_colors)]\n    return color","479d55b7":"train = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")","92e04623":"table = ff.create_table(train.head().round(3))\niplot(table,filename='jupyter-table1')","13b215f9":"train.columns\n","51cd6272":"train.shape\n","80e9ac2f":"iplot(ff.create_table(train.dtypes.to_frame().reset_index().round(3)),filename='jupyter-table2')","661d71d2":"iplot(ff.create_table(train.describe().reset_index().round(3)),filename='jupyter-table2')","8142192e":"train.isnull().sum()","36659d72":"msno.bar(train, color = 'b', figsize = (10,8))","665a98ef":"msno.matrix(train)","d48b4d9f":"species_count = train['target'].value_counts()\ndata = [go.Bar(\n    x = species_count.index,\n    y = species_count.values,\n    marker = dict(color = random_colors(3),line=dict(color='#000000', width=2))\n)]\n\nlayout = go.Layout(\n   {\n      \"title\":\"Healthy VS Non Healthy\",\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","98afe1bf":"trace = go.Pie(labels = list(train.target.unique()), values = list(train.target.value_counts()),\n                            hole = 0.2,\n               marker=dict(colors = random_colors(3), \n                           line=dict(color='#000000', width=2)\n                           ))\ndata = [trace]\nlayout = go.Layout(\n   {\n      \"title\":\"Healthy VS Non Healthy\",\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","7d19cf5e":"data = [go.Heatmap(z = np.array(train.corr().values),\n                   x = np.array(train.corr().columns),\n                   y = np.array(train.corr().columns),\n                     colorscale='Viridis',)\n       ]\nlayout = go.Layout(dict(title = 'Correlation Matrix for variables',\n                            #autosize = False,\n                            #height  = 1400,\n                            #width   = 1600,\n                            margin  = dict(r = 0 ,l = 100,\n                                           t = 0,b = 100,\n                                         ),\n                            yaxis   = dict(tickfont = dict(size = 9)),\n                            xaxis   = dict(tickfont = dict(size = 9)),\n                           )\n                      )\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","0c8b05ab":"cols = \"oldpeak\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","35805c0c":"\nPresent = train[(train['target'] != 0.0)]\nNot_Present = train[(train['target'] != 1.0)]\n\n\ntmp1 = Present[cols]\n\ntmp2 = Not_Present[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['Present', 'Not Present']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","843d9f97":"fig = go.Figure(\n    data=[go.Histogram(x=train[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","6f879af2":"fig = px.violin(train, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","9d48f5c7":"cols = \"thalach\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","b32b1b24":"\nPresent = train[(train['target'] != 0.0)]\nNot_Present = train[(train['target'] != 1.0)]\n\n\ntmp1 = Present[cols]\n\ntmp2 = Not_Present[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['Present', 'Not Present']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","68b915ce":"fig = go.Figure(\n    data=[go.Histogram(x=train[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","593eb38c":"fig = px.violin(train, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","9f46c375":"cols = \"chol\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","3843f534":"\nPresent = train[(train['target'] != 0.0)]\nNot_Present = train[(train['target'] != 1.0)]\n\n\ntmp1 = Present[cols]\n\ntmp2 = Not_Present[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['Present', 'Not Present']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","a1d5b60f":"fig = go.Figure(\n    data=[go.Histogram(x=train[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","e9575889":"fig = px.violin(train, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","15b4641a":"cols = \"trestbps\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","0e233621":"\nPresent = train[(train['target'] != 0.0)]\nNot_Present = train[(train['target'] != 1.0)]\n\n\ntmp1 = Present[cols]\n\ntmp2 = Not_Present[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['Present', 'Not Present']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","0d78a519":"fig = go.Figure(\n    data=[go.Histogram(x=train[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","8b97522e":"fig = px.violin(train, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","c932f639":"trace0 = go.Box(\n    name = 'age',\n    y = train[\"age\"]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":\"age \",\n   }\n)\n\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","dc6d8fc5":"cols = \"age\"\nPresent = train[(train['target'] != 0.0)]\nNot_Present = train[(train['target'] != 1.0)]\n\n\ntmp1 = Present[cols]\n\ntmp2 = Not_Present[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['Present', 'Not Present']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","b3c8efd9":"fig = go.Figure(\n    data=[go.Histogram(x=train['age'])],layout_title_text=' Age Distribution ',layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","67f4d199":"fig = px.violin(train, y=\"age\", box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","6b1c5eda":"cols = \"sex\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\ntrace0 = go.Bar(x = train[cols].value_counts().index,y=train[cols].value_counts())\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","8124f747":"survivedvssex = train[[\"sex\",\"target\"]].groupby([\"sex\"], as_index = False).mean().sort_values(by=\"target\",ascending = False)\nfig = px.bar(survivedvssex, x=\"sex\", y=\"target\",color=\"target\") \nfig.show()","1885858d":"cols = \"cp\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\ntrace0 = go.Bar(x = train[cols].value_counts().index,y=train[cols].value_counts())\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","d693fbc6":"survivedvssex = train[[\"cp\",\"target\"]].groupby([\"cp\"], as_index = False).mean().sort_values(by=\"target\",ascending = False)\nfig = px.bar(survivedvssex, x=\"cp\", y=\"target\",color=\"target\") \nfig.show()","5e939064":"cols = \"fbs\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\ntrace0 = go.Bar(x = train[cols].value_counts().index,y=train[cols].value_counts())\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","4b0b01ee":"survivedvssex = train[[\"fbs\",\"target\"]].groupby([\"fbs\"], as_index = False).mean().sort_values(by=\"target\",ascending = False)\nfig = px.bar(survivedvssex, x=\"fbs\", y=\"target\",color=\"target\") \nfig.show()","6d8845d7":"cols = \"restecg\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\ntrace0 = go.Bar(x = train[cols].value_counts().index,y=train[cols].value_counts())\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","e9e41477":"survivedvssex = train[[\"restecg\",\"target\"]].groupby([\"restecg\"], as_index = False).mean().sort_values(by=\"target\",ascending = False)\nfig = px.bar(survivedvssex, x=\"restecg\", y=\"target\",color=\"target\") \nfig.show()","a0c8dc2e":"cols = \"exang\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\ntrace0 = go.Bar(x = train[cols].value_counts().index,y=train[cols].value_counts())\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","4f56cfc7":"survivedvssex = train[[\"exang\",\"target\"]].groupby([\"exang\"], as_index = False).mean().sort_values(by=\"target\",ascending = False)\nfig = px.bar(survivedvssex, x=\"exang\", y=\"target\",color=\"target\") \nfig.show()","257a2660":"cols = \"slope\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\ntrace0 = go.Bar(x = train[cols].value_counts().index,y=train[cols].value_counts())\n\ndata = [trace0]\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","b0cc6648":"survivedvssex = train[[\"slope\",\"target\"]].groupby([\"slope\"], as_index = False).mean().sort_values(by=\"target\",ascending = False)\nfig = px.bar(survivedvssex, x=\"slope\", y=\"target\",color=\"target\") \nfig.show()","adcda4fe":"cols = \"ca\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\ntrace0 = go.Bar(x = train[cols].value_counts().index,y=train[cols].value_counts())\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","9e22a7c6":"survivedvssex = train[[\"ca\",\"target\"]].groupby([\"ca\"], as_index = False).mean().sort_values(by=\"target\",ascending = False)\nfig = px.bar(survivedvssex, x=\"ca\", y=\"target\",color=\"target\") \nfig.show()","362ab6ee":"cols = \"thal\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\ntrace0 = go.Bar(x = train[cols].value_counts().index,y=train[cols].value_counts())\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","9b57578d":"survivedvssex = train[[\"thal\",\"target\"]].groupby([\"thal\"], as_index = False).mean().sort_values(by=\"target\",ascending = False)\nfig = px.bar(survivedvssex, x=\"thal\", y=\"target\",color=\"target\") \nfig.show()","0bc0b6b7":"X = train.iloc[:,:-1].values\ny = train.iloc[:,-1].values\nencoder = LabelEncoder()\ny = encoder.fit_transform(y)\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)","e0a62c3b":"## Logistic Regression\n\nModel = LogisticRegression()\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","7f7a4888":"# K-Nearest Neighbours\n\nModel = KNeighborsClassifier(n_neighbors=8)\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","6b370434":"## Decision Tree\n\nModel = DecisionTreeClassifier()\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","5c99b0f1":"## Naive Bayes\n\nModel = GaussianNB()\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","82494f3e":"# Linear Discriminant Analysis\n\nModel = LinearDiscriminantAnalysis()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","68bb48ee":"## Light GBM\n\nparams = {'objective':'binary', 'metric':'accuracy'}\n  \n# create dataset for lightgbm\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)\n\nModel = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_test], verbose_eval=10)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred.round()))\n","7f1c84f2":"\nModel=CatBoostClassifier(eval_metric='Accuracy',use_best_model=True,random_seed=42)\nModel.fit(X_train,y_train,eval_set=(X_test,y_test))","47156b88":"## CatBoost\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","9e94d6ae":"## XGBoost\n\nModel=xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","00a3c986":"## Ridge Classifier\n\nModel=RidgeClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","ddbd7909":"## Quadratic Discriminant Analysis\n\nModel=QuadraticDiscriminantAnalysis()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","bbc9190c":"## Bagging Classifier\n\nModel=BaggingClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","02e88dd1":"## MLPClassifier\n\nModel=MLPClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\n# Summary of the predictions\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","ae50dc96":"## Linear Support Vector Classification\n \nModel = LinearSVC()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","6f9bfbb0":"## Nu-Support Vector Classification\n\nModel = NuSVC()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","5a8ce9ab":"## BernoulliNB\n\nModel = BernoulliNB()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","6cd2a780":"## Passive Aggressive Classifier\n\nModel = PassiveAggressiveClassifier()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","9579e1f8":"## Gradient Boosting Machine\nModel = GradientBoostingClassifier(n_estimators=100, random_state=9)\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))\n","474f0e35":"## Adaboost\n\nModel = AdaBoostClassifier(n_estimators=100, random_state=9)\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))\n","d3959c36":"## Extra Trees\n\nModel = ExtraTreesClassifier(n_estimators=100, max_features=3)\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))\n","cca0ede9":"\n## Random Forest\n\nModel = RandomForestClassifier(n_estimators=100, max_features=3)\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","93b3c1ba":"## Support Vector Machine\n\nModel = SVC()\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","0fd7f3c7":"import h2o\nfrom h2o.automl import H2OAutoML\nh2o.init(max_mem_size='16G')","e6772b83":"data = h2o.import_file('\/kaggle\/input\/heart-disease-uci\/heart.csv')\n\n# Identify predictors and response\nx = data.columns\ny = \"target\"\nx.remove(y)\n\ndata[y] = data[y].asfactor()\n\naml = H2OAutoML(max_models=20, max_runtime_secs=1500, seed=1)\naml.train(x=x, y=y, training_frame=data)","e0737e79":"lb = aml.leaderboard\nlb.head()","01a50fb3":"from tpot import TPOTClassifier\nfrom tpot import TPOTRegressor\n\ntpot = TPOTClassifier(generations=5, verbosity=2)\ntpot.fit(X_train,y_train)\n","82a7c6e1":"y_pred=tpot.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","a63b67bd":"! pip install -U pycaret # Quite large depencies to install !","83816c29":"from pycaret.classification import *\n\nclf1 = setup(data = train, \n             target = 'target',\n             silent = True)\n","0bf994eb":"compare_models()\n","86e6134b":"lgbm  = create_model('catboost')      ","499e6be2":"plot_model(estimator = lgbm, plot = 'learning')\n","3a2e014a":"<a id=\"5.20\"><\/a>\n<font color=\"blue\" size=+2.5><b> Extra Trees <\/b><\/font>\n\n*Extra Trees are another modification of bagging where random trees are constructed from samples of the training dataset. You can construct an Extra Trees model for classification using the ExtraTreesClassifier class*","321fc4fd":"<a id=\"3.42\"><\/a>\n<font color=\"blue\" size=+2.5><b> exang Variable Analysis <\/b><\/font>","0d2e8edd":"<a id=\"5.4\"><\/a>\n<font color=\"blue\" size=+2.5><b> Naive Bayes <\/b><\/font>\n\n*Naive Bayes calculates the probability of each class and the conditional probability of each class\ngiven each input value. These probabilities are estimated for new data and multiplied together,\nassuming that they are all independent (a simple or naive assumption). When working with\nreal-valued data, a Gaussian distribution is assumed to easily estimate the probabilities for\ninput variables using the Gaussian Probability Density Function. You can construct a Naive\nBayes model using the GaussianNB class*","51d4b374":"<a id=\"5.23\"><\/a>\n<font color=\"blue\" size=+2.5><b> H2O <\/b><\/font>","a28158d5":"\n\n<a id=\"1\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>  Introduction  <\/center><\/h2>","3f80ad14":"<a id=\"5.7\"><\/a>\n<font color=\"blue\" size=+2.5><b> CatBoost <\/b><\/font>\n\n*CatBoost is an algorithm for gradient boosting on decision trees. It is developed by Yandex researchers and engineers, and is used for search, recommendation systems, personal assistant, self-driving cars, weather prediction and many other tasks at Yandex and in other companies, including CERN, Cloudflare, Careem taxi. It is in open-source and can be used by anyone.*","f4340dd8":"<a id=\"2.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Import Dataset <\/b><\/font>\n","76da7ec2":"<a id=\"5.25\"><\/a>\n<font color=\"blue\" size=+2.5><b> Pycaret <\/b><\/font>\n","423ed404":"<a id=\"3.1\"><\/a>\n<font color=\"blue\" size=+2.5><b> Missing Value Analysis <\/b><\/font>\n","fa2b86be":"<a id=\"5.8\"><\/a>\n<font color=\"blue\" size=+2.5><b> XGBoost <\/b><\/font>\n\n*XGBoost stands for Extreme Gradient Boosting, it is a performant machine learning library based on the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman. XGBoost implements a Gradient Boosting algorithm based on decision trees.*","9f0045b5":"<a id=\"3.42\"><\/a>\n<font color=\"blue\" size=+2.5><b> Cp Variable Analysis <\/b><\/font>","4da408cd":"<a id=\"4.1\"><\/a>\n<font color=\"blue\" size=+2.5><b> Data Handling and Preparation <\/b><\/font>\n","8dd1b00a":"<a id=\"5.13\"><\/a>\n<font color=\"blue\" size=+2.5><b> Linear Support Vector Classification  <\/b><\/font>\n\n*Similar to **SVC** with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.*","8ba71870":"<a id=\"3.42\"><\/a>\n<font color=\"blue\" size=+2.5><b> slope Variable Analysis <\/b><\/font>","d92c0240":"<a id=\"5.21\"><\/a>\n<font color=\"blue\" size=+2.5><b> Random Forest <\/b><\/font>\n\n*Random Forests is an extension of bagged decision trees. Samples of the training dataset are taken with replacement, but the trees are constructed in a way that reduces the correlation between individual classifiers. Specifically, rather than greedily choosing the best split point in the construction of each tree, only a random subset of features are considered for each split. You can construct a Random Forest model for classification using the RandomForestClassifier class.*","1a999b1d":"<a id=\"3.41\"><\/a>\n<font color=\"blue\" size=+2.5><b> Age Variable Analysis <\/b><\/font>","4ba66043":"<a id=\"5.12\"><\/a>\n<font color=\"blue\" size=+2.5><b> MLPClassifier  <\/b><\/font>\n\n*MLPClassifier stands for Multi-layer Perceptron classifier which in the name itself connects to a Neural Network. Unlike other classification algorithms such as Support Vectors or Naive Bayes Classifier, MLPClassifier relies on an underlying Neural Network to perform the task of classification.*","47b90bef":"<font size=\"+3\" color=blue><b> <center><u> Heart Disease + EDA + (25+) Models For Beginners <\/u><\/center><\/b><\/font>","93208204":"<a id=\"3.42\"><\/a>\n<font color=\"blue\" size=+2.5><b> Sex Variable Analysis <\/b><\/font>","fbc3ea0d":"<a id=\"5.6\"><\/a>\n<font color=\"blue\" size=+2.5><b> LightGBM <\/b><\/font>\n\n*LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:*\n\nFaster training speed and higher efficiency.\nLower memory usage.\nBetter accuracy.\nSupport of parallel and GPU learning.\nCapable of handling large-scale data.","97056c9f":"<font size=\"+2\" color=blue ><b>Please Upvote my kernel and keep it in your favourite section if you think it is helpful.<\/b><\/font>","d1ca17be":"<a id=\"3.42\"><\/a>\n<font color=\"blue\" size=+2.5><b> thal Variable Analysis <\/b><\/font>","8bb967c4":"<a id=\"5.9\"><\/a>\n<font color=\"blue\" size=+2.5><b> Ridge Classifier <\/b><\/font>\n\n*Classifier using Ridge regression. This classifier first converts the target values into {-1, 1} and then treats the problem as a \nregression task (multi-output regression in the multiclass case).*","5955f8de":"<a id=\"3.42\"><\/a>\n<font color=\"blue\" size=+2.5><b> restecg Variable Analysis <\/b><\/font>","c49acaf0":"<a id=\"3.62\"><\/a>\n<font color=\"blue\" size=+2.5><b> Correlation <\/b><\/font>\n","7a472ee0":"<a id=\"5.18\"><\/a>\n<font color=\"blue\" size=+2.5><b> Stochastic Gradient Boosting <\/b><\/font>\n\n*Stochastic Gradient Boosting (also called Gradient Boosting Machines) are one of the most sophisticated ensemble techniques. It is also a technique that is proving to be perhaps one of the best techniques available for improving performance via ensembles. You can construct a Gradient Boosting model for classification using the GradientBoostingClassifier class*","45885924":"<a id=\"3.43\"><\/a>\n<font color=\"blue\" size=+2.5><b> oldpeak Variable Analysis <\/b><\/font>","0f2d9117":"<a id=\"5.5\"><\/a>\n<font color=\"blue\" size=+2.5><b> Linear Discriminant Analysis <\/b><\/font>\n\n*Linear Discriminant Analysis or LDA is a statistical technique for binary and multiclass\nclassification. It too assumes a Gaussian distribution for the numerical input variables. You can\nconstruct an LDA model using the LinearDiscriminantAnalysis class.*","d78aad2d":"<a id=\"5.24\"><\/a>\n<font color=\"blue\" size=+2.5><b> TPOT <\/b><\/font>","082da912":"<a id=\"5\"><\/a>\n<font color=\"blue\" size=+2.5><b> Model Training <\/b><\/font>\n","44c5a9b4":"<a id=\"5.10\"><\/a>\n<font color=\"blue\" size=+2.5><b> Quadratic Discriminant Analysis <\/b><\/font>\n\n\n*A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule.\nThe model fits a **Gaussian** density to each class.*","0b4e8c8d":"<a id=\"5.19\"><\/a>\n<font color=\"blue\" size=+2.5><b> \nAdaBoost <\/b><\/font>\n\n*AdaBoost was perhaps the first successful boosting ensemble algorithm. It generally works by weighting instances in the dataset by how easy or difficult they are to classify, allowing the algorithm to pay or less attention to them in the construction of subsequent models. You can construct an AdaBoost model for classification using the AdaBoostClassifier class*","01efc629":"<a id=\"5.14\"><\/a>\n<font color=\"blue\" size=+2.5><b> Nu-Support Vector Classification <\/b><\/font>\n\n*Similar to SVC but uses a parameter to control the number of support vectors.*","7422b3f7":"<a id=\"2.1\"><\/a>\n<font color=\"blue\" size=+2.5><b> Import Libraries <\/b><\/font>\n","5fb0d72f":"<a id=\"3.42\"><\/a>\n<font color=\"blue\" size=+2.5><b> ca Variable Analysis <\/b><\/font>","9e64deb7":"<a id=\"5.22\"><\/a>\n<font color=\"blue\" size=+2.5><b> Support Vector Machine <\/b><\/font>\n\n*Support Vector Machines (or SVM) seek a line that best separates two classes. Those data instances that are closest to the line that best separates the classes are called support vectors and influence where the line is placed. SVM has been extended to support multiple classes Of particular importance is the use of different kernel functions via the kernel parameter .A powerful Radial Basis Function is used by default. You can construct an SVM model using the SVC class.*","22830d32":"![image.png](attachment:image.png)","e31de494":"<a id=\"3.43\"><\/a>\n<font color=\"blue\" size=+2.5><b> chol Variable Analysis <\/b><\/font>","60ee3831":"<a id=\"5.1\"><\/a>\n<font color=\"blue\" size=+2.5><b> Logistic Regression <\/b><\/font>\n\n*Logistic regression assumes a Gaussian distribution for the numeric input variables and can\nmodel binary classification problems. You can construct a logistic regression model using the\nLogisticRegression class*","3b05581b":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Target Variable Analysis <\/b><\/font>\n","94c8db5d":"<a id=\"5.15\"><\/a>\n<font color=\"blue\" size=+2.5><b> BernoulliNB <\/b><\/font>\n\n*Like MultinomialNB, this classifier is suitable for **discrete data**. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary\/boolean features.*","b80d971a":"<a id=\"5.16\"><\/a>\n<font color=\"blue\" size=+2.5><b> Passive Aggressive Classifier <\/b><\/font>\n\n*Like MultinomialNB, this classifier is suitable for **discrete data**. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary\/boolean features.*\nThe Passive-Aggressive algorithms are a family of Machine learning algorithms that are not very well known by beginners and even intermediate Machine Learning enthusiasts. However, they can be very useful and efficient for certain applications.","f165cfa3":"<a id=\"3.43\"><\/a>\n<font color=\"blue\" size=+2.5><b> trestbps Variable Analysis <\/b><\/font>","ff8b9940":"# Upvote The Kernel If you like my work","4f5a5561":"<a id=\"1.1\"><\/a>\n<font color=\"blue\" size=+2.5><b> About Data<\/b><\/font>\n<br\/>\n<br\/>\n\nThis database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to\nthis date. The \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4.\n\n\n## The columns in this dataset are:\n* age\n* sex\n* chest pain type (4 values)\n* resting blood pressure\n* serum cholestoral in mg\/dl\n* fasting blood sugar > 120 mg\/dl\n* resting electrocardiographic results (values 0,1,2)\n* maximum heart rate achieved\n* exercise induced angina\n* oldpeak = ST depression induced by exercise relative to rest\n* the slope of the peak exercise ST segment\n* number of major vessels (0-3) colored by flourosopy\n* thal: 3 = normal; 6 = fixed defect; 7 = reversable defect","3c878d01":"<a id=\"3\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>  Exploratory Data Analysis <\/center><\/h2>","083bed2d":"<a id=\"3.42\"><\/a>\n<font color=\"blue\" size=+2.5><b> FBS Variable Analysis <\/b><\/font>","f24cbe23":"<a id=\"5.3\"><\/a>\n<font color=\"blue\" size=+2.5><b> Decision Tree (CART) <\/b><\/font>\n\n*Classification and Regression Trees (CART or just decision trees) construct a binary tree from\nthe training data. Split points are chosen greedily by evaluating each attribute and each value\nof each attribute in the training data in order to minimize a cost function (like the Gini index).\nYou can construct a CART model using the DecisionTreeClassifier class*","be98303c":"<a id=\"3.43\"><\/a>\n<font color=\"blue\" size=+2.5><b> thalach Variable Analysis <\/b><\/font>","1c0817c0":"<a id=\"top\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center> Table of content <\/center><\/h2>\n\n<font color=\"blue\" size=+1><b>Introduction<\/b><\/font>\n* [About Data ](#1.1)\n* [Data Dictionary ](#1.3)\n* [Data Variable](#1.4)\n    \n<font color=\"blue\" size=+1><b> Load and Check Data <\/b><\/font>\n* [Importing Library](#2.1)\n* [Load Dataset](#2.2)\n\n<font color=\"blue\" size=+1><b> Exploratory Data Analysis <\/b><\/font>\n* [Missing Value Analysis](#3.1)\n* [Target Variable Analysis](#3.2)    \n* [Non-Target Variable Analysis](#3.4)   \n \n<font color=\"blue\" size=+1><b> Data Handling and Preparation <\/b><\/font>\n* [Handing Missing Data ](#4.1)\n* [Train Test Split ](#4.2)\n\n<font color=\"blue\" size=+1><b> Model Training <\/b><\/font>\n* [Logistic Regression ](#5.1)\n* [K-Nearest Neighbours ](#5.2)    \n* [Decision Tree ](#5.3)\n* [Naive Bayes ](#5.4)    \n* [Linear Discriminant Analysis ](#5.5)\n* [LightLGM ](#5.6)    \n* [CatBoost ](#5.7)\n* [XGBoost ](#5.8)    \n* [Ridge Classifier ](#5.9)\n* [Quadratic Discriminant Analysis ](#5.10)    \n* [Bagging classifier ](#5.11)\n* [MLPClassifier](#5.12)    \n* [Linear Support Vector Classification ](#5.13)\n* [Nu-Support Vector Classification ](#5.14)    \n* [BernoulliNB ](#5.15)\n* [Passive Aggressive Classifier ](#5.16)    \n* [Stochastic Gradient Boosting ](#5.18)    \n* [AdaBoost ](#5.19)\n* [Extra Trees ](#5.20)    \n* [Random Forest ](#5.21)\n* [SVC ](#5.22)    \n* [H2O ](#5.23)\n* [TPOT ](#5.24)    \n* [PyCaret ](#5.25)\n","16b8f287":"<a id=\"5.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> K-Nearest Neighbours <\/b><\/font>\n\n*The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems*","cd7880fd":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>Objective  <\/center><\/h2>\n\nGoal of this kernel is following:\n- Basic Exploratory Data Analysis.\n- Beginners guide on Heart Disease UCI Dataset.\n- Feature Analysis\n- Modelling on 25+ Models","c377bafc":"<a id=\"5.11\"><\/a>\n<font color=\"blue\" size=+2.5><b> Bagging classifier  <\/b><\/font>\n\n*A Bagging classifier is an ensemble **meta-estimator** that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.*","55d0d8dc":"<a id=\"3.4\"><\/a>\n<font color=\"blue\" size=+2.5><b> Non-Target Variable Analysis <\/b><\/font>\n","355843cd":"<a id=\"2\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>  Load and Check Data  <\/center><\/h2>"}}