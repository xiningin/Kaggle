{"cell_type":{"a09bdb60":"code","a8907244":"code","907622db":"code","dd47ea0d":"code","3c5d3ce0":"code","89fc7960":"code","96087832":"code","5b6abcc6":"code","15d748b0":"code","3a7e8868":"code","e4833915":"code","b00eb1ea":"code","925f068c":"code","505b4311":"code","bc89fc2f":"code","3b068150":"code","fe42073b":"code","04848dda":"code","0e019081":"code","9c817f50":"code","7b3a46f8":"code","bbe62963":"code","f76aa910":"code","a5746faf":"code","59cb48d2":"code","f4e5910c":"code","583d0a43":"code","71b0b114":"code","daef161e":"markdown"},"source":{"a09bdb60":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a8907244":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns","907622db":"data = pd.read_csv('\/kaggle\/input\/age-gender-and-ethnicity-face-data-csv\/age_gender.csv')\ndata.head()","dd47ea0d":"data.shape","3c5d3ce0":"data.describe()","89fc7960":"data.isnull().describe()","96087832":"# Actually images are in string format, let's transform it in more useful type of data.\n\ndata['pixels'] = data['pixels'].map(lambda x: np.array(x.split(' '), dtype=np.float32).reshape(48, 48))","5b6abcc6":"## normalizing pixels data\ndata['pixels'] = data['pixels'].apply(lambda x: x\/255)\n\n## calculating distributions\nage_dist = data['age'].value_counts()\nethnicity_dist = data['ethnicity'].value_counts()\ngender_dist = data['gender'].value_counts().rename(index={0:'Male',1:'Female'})\n\ndef ditribution_plot(x,y,name):\n    fig = go.Figure([\n        go.Bar(x=x, y=y)\n    ])\n\n    fig.update_layout(title_text=name)\n    fig.show()","15d748b0":"import plotly.graph_objects as go\nimport plotly.express as px\nditribution_plot(x=age_dist.index, y=age_dist.values, name='Age Distribution')","3a7e8868":"ditribution_plot(x=ethnicity_dist.index, y=ethnicity_dist.values, name='Ethnicity Distribution')","e4833915":"ditribution_plot(x=gender_dist.index, y=gender_dist.values, name='Gender Distribution')","b00eb1ea":"# Plot some pictures\nfig, axes = plt.subplots(1, 5, figsize=(20, 10))\n\nfor i in range(5):\n    random_face = np.random.choice(len(data))\n    \n    age = data['age'][random_face]\n    ethnicity = data['ethnicity'][random_face]\n    gender = data['gender'][random_face]\n    \n    axes[i].set_title('Age: {0}, Ethnicity: {1}, Sex: {2}'.format(age, ethnicity, gender))\n    axes[i].imshow(data['pixels'][random_face])\n    axes[i].imshow(data['pixels'][random_face])\n    axes[i].axis('off')","925f068c":"X = np.array(data['pixels'].tolist())\n\n## Converting pixels from 1D to 3D\nX = X.reshape(X.shape[0],48,48,1)","505b4311":"# Normalise images\nif np.max(X) > 1: \n    X = X\/255","bc89fc2f":"# Set some useful variables\ninput_shape = X.shape[1:] \n\nepochs = 20\nbatch_size = 64\nrandom_seeds = 42","3b068150":"from sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D,InputLayer, Dropout, BatchNormalization, Flatten, Dense, MaxPooling2D\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam","fe42073b":"# split the data into train ad test\ny = data['age'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.22, random_state=random_seeds)","04848dda":"AgeModel = Sequential()\n\nAgeModel.add(Conv2D(64, kernel_size=(3,3), input_shape=input_shape, activation='relu'))\nAgeModel.add(MaxPooling2D(pool_size=(2,2)))\nAgeModel.add(BatchNormalization())\n\nAgeModel.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\nAgeModel.add(MaxPooling2D(pool_size=(2,2)))\nAgeModel.add(Dropout(0.2))\nAgeModel.add(BatchNormalization())\n\nAgeModel.add(Conv2D(256, kernel_size=(3,3), activation='relu'))\nAgeModel.add(MaxPooling2D(pool_size=(2,2)))\nAgeModel.add(Dropout(0.5))\nAgeModel.add(BatchNormalization())\n\nAgeModel.add(Flatten())\nAgeModel.add(Dense(128, activation='relu'))\nAgeModel.add(Dropout(0.4))\nAgeModel.add(Dense(1))\n\nAgeModel.compile(optimizer='adam',\n              loss='mean_squared_error',\n              metrics=['mse'])\n\n\n# Callbacks for age model\ncallbacks = [tf.keras.callbacks.EarlyStopping(patience=4, monitor='val_loss', mode='min'), \n             tf.keras.callbacks.ReduceLROnPlateau(patience=2, verbose=1)]\n\nAgeModel.summary()","0e019081":"history = AgeModel.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), batch_size=batch_size, callbacks=callbacks)","9c817f50":"pd.DataFrame(history.history).plot()","7b3a46f8":"valid_score = AgeModel.evaluate(X_test, y_test, verbose=1)","bbe62963":"# Make predictions \ny_pred = AgeModel.predict(X_test)\n","f76aa910":"# split the data into train ad test\ny = data['gender'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.22, random_state=random_seeds)\n","a5746faf":"gender_model = Sequential([\n    InputLayer(input_shape=(48,48,1)),\n    Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n    BatchNormalization(),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Flatten(),\n    Dense(64, activation='relu'),\n    Dropout(rate=0.5),\n    Dense(1, activation='sigmoid')\n])\n\ngender_model.compile(optimizer='sgd',\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=['accuracy'])\n\n\n## Stop training when validation loss reach 0.2700\nclass myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('val_loss')<0.2000):\n            print(\"\\nReached 0.2000 val_loss so cancelling training!\")\n            self.gender_model.stop_training = True\n        \ncallback = myCallback()\n\ngender_model.summary()","59cb48d2":"history = gender_model.fit(\n    X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), batch_size=batch_size, callbacks=[callback])\n","f4e5910c":"pd.DataFrame(history.history).plot();","583d0a43":"valid_score = gender_model.evaluate(X_test, y_test, verbose=1)","71b0b114":"# Make predictions \ny_pred = gender_model.predict(X_test)\n","daef161e":"# Model for Age and Gender Prediction\n*# first we make model for age and then gender*"}}