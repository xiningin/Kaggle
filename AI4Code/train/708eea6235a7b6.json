{"cell_type":{"f7d7d115":"code","d556fb97":"code","c62d70f8":"code","6d8d8b5a":"code","67d89a68":"code","3a6e63ce":"code","b0c5667b":"code","f58241e6":"code","00d09155":"code","9cbaf1d3":"code","c8455137":"code","28401dc6":"code","de7ef6b5":"code","513a49a1":"code","c54888c9":"code","cbbab92a":"code","5ff6a404":"code","45d00e70":"code","2e641009":"code","596614f4":"code","5d3f2652":"code","6502b041":"code","b5b5693e":"code","94204cd5":"code","e29412d0":"code","710e2f02":"code","5535dd05":"code","bd43678c":"code","add98f10":"code","4a38263c":"code","dbf602de":"code","7149fc75":"code","7eccea67":"code","3e74ce70":"code","d8603306":"code","b69f7809":"markdown","ea8873d9":"markdown","f879d1a7":"markdown","a9556b0f":"markdown","62c79d12":"markdown","4015a60f":"markdown","e1ea3db7":"markdown","4681306e":"markdown"},"source":{"f7d7d115":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d556fb97":"df = pd.read_csv('\/kaggle\/input\/vehicle-dataset-from-cardekho\/car data.csv')","c62d70f8":"df.head()","6d8d8b5a":"df.shape","67d89a68":"df.describe()","3a6e63ce":"df.info()","b0c5667b":"df.columns","f58241e6":"final_dataset=df[['Year', 'Selling_Price', 'Present_Price', 'Kms_Driven',\n       'Fuel_Type', 'Seller_Type', 'Transmission', 'Owner']]","00d09155":"final_dataset.head()","9cbaf1d3":"final_dataset['Current_Year']=2020\nfinal_dataset['no_year']=final_dataset['Current_Year']-final_dataset['Year']\nfinal_dataset.drop(['Year', 'Current_Year'], axis=1, inplace=True)","c8455137":"final_dataset.head()","28401dc6":"final_dataset=pd.get_dummies(final_dataset, drop_first=True)","de7ef6b5":"final_dataset.head()","513a49a1":"corr_matrix = final_dataset.corr()\ncorr_matrix","c54888c9":"import seaborn as sns\nsns.pairplot(final_dataset)","cbbab92a":"%matplotlib inline\nimport matplotlib.pyplot as plt","5ff6a404":"corrmat=final_dataset.corr()\ntop_corr_features=corrmat.index\nplt.figure(figsize=(20,20))\n# plot heat map\ng=sns.heatmap(final_dataset[top_corr_features].corr(), annot=True, cmap=\"RdYlGn\")","45d00e70":"from sklearn.model_selection import train_test_split\nX = final_dataset.iloc[:,1:]\ny = final_dataset.iloc[:,0]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","2e641009":"X.head()","596614f4":"y.head()","5d3f2652":"from sklearn.ensemble import ExtraTreesRegressor\nmodel=ExtraTreesRegressor()\nmodel.fit(X,y)","6502b041":"print(model.feature_importances_)","b5b5693e":"#plot graph of feature importances for better visualization \n\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns) \nfeat_importances.nlargest(5).plot(kind='barh') \nplt.show()","94204cd5":"# Use the random grid to search for best hyperparameters \n# First create the base model to tune \n\nfrom sklearn.ensemble import RandomForestRegressor\nrf=RandomForestRegressor()","e29412d0":"## Hyperparameters\nimport numpy as np\nn_estimators=[int(x) for x in np.linspace(start=100, stop=1200, num=12)]\nprint(n_estimators)","710e2f02":"#Randomized Search CV\n\n#Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt'] \n#Haximum number of Levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num= 6)] \n#max_depth.append(None) #Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n#Minimum number of samples required at each teaf node\nmin_samples_leaf = [1, 2, 5, 10]\n                ","5535dd05":"from sklearn.model_selection import RandomizedSearchCV","bd43678c":"# Create the random_grid \n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features, \n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split, \n               'min_samples_leaf': min_samples_leaf}\n\nprint (random_grid)","add98f10":"rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid, scoring='neg_mean_squared_error', n_iter=10, cv=5, verbose=2, random_state=42, n_jobs=1)","4a38263c":"rf_random.fit(X_train, y_train)","dbf602de":"y_pred=rf_random.predict(X_test)\ny_pred","7149fc75":"from sklearn.metrics import mean_squared_error\nimport math\nprint(mean_squared_error(y_test, y_pred))\nprint(math.sqrt(mean_squared_error(y_test, y_pred)))","7eccea67":"sns.distplot(y_test-y_pred)","3e74ce70":"plt.scatter(y_test, y_pred)","d8603306":"import statsmodels.api as sm\nX_addC = sm.add_constant(X_test)\nresult = sm.OLS(y_pred, X_addC).fit()\nprint(result.rsquared, result.rsquared_adj)","b69f7809":"## Model","ea8873d9":"## R-Square and Adjusted R-Square","f879d1a7":"## Feature Importance","a9556b0f":"## Correlation Matrix","62c79d12":"## Plot Visualizations","4015a60f":"## Evaluation","e1ea3db7":"## Train-Test Splitting","4681306e":"## Converting categorical features to one hot encoded"}}