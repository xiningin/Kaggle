{"cell_type":{"aef0b6ca":"code","cf49cb8e":"code","fa624cb8":"code","9d7ef87a":"code","4643b1d7":"code","701f3439":"code","d23e6408":"code","2d145b7e":"code","3c89fa96":"code","cf2abd6c":"code","e72ab7f3":"code","a2dafba8":"code","4ae66102":"code","29b77053":"code","6c05f909":"code","9aa5837c":"code","3e833f85":"code","5005f046":"code","70cc2c9a":"code","89bb5ef5":"code","39e301b5":"code","d2fc72f2":"code","7251ae90":"code","65ae7a4b":"code","5a7f6e8e":"code","572e2b83":"code","c875e166":"code","00a292ff":"code","32781687":"code","5cf86a39":"code","2a80b0b3":"code","28bc81dd":"code","c466b76b":"code","86dce772":"code","ae0dbf21":"code","71a26b4b":"code","bcf29b7f":"code","370e5645":"code","ab591178":"code","45fea429":"code","03694ce4":"code","8d71f2e7":"code","45fc3db2":"code","f505acee":"code","1b20d545":"code","73c9cf8c":"code","bb3b0cf2":"code","dfc1f62c":"code","3508d178":"code","0d35828d":"code","ee561594":"code","9c2f2e38":"code","f462a87a":"code","fd039d8f":"code","3e02aa94":"code","ed1e0e5e":"code","86256de4":"code","32329866":"code","02f0a3a0":"code","87bcbb50":"markdown","03b8ac0a":"markdown","b3b9cad4":"markdown","fd2ff764":"markdown","c5e18e69":"markdown","8512acc1":"markdown","d7dd6907":"markdown","da942d16":"markdown","0a99645d":"markdown","18490518":"markdown","245a88c4":"markdown","5aedbb62":"markdown","0e5889b6":"markdown","32e55a0a":"markdown","0473fa65":"markdown","12575e02":"markdown","603a8d98":"markdown","36ca08d8":"markdown","d600d5e3":"markdown","139d3d5a":"markdown","58ed3782":"markdown","ee50b4fa":"markdown","98427aa2":"markdown","ff9f0a6e":"markdown","0757691b":"markdown","9b7b34b5":"markdown","64adcc98":"markdown"},"source":{"aef0b6ca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","cf49cb8e":"# Imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport string\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import StratifiedKFold\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nSEED = 2019","fa624cb8":"# helper Functions\n\ndef list_helpers():\n    \"\"\" Prints all helper functions available in current notebook \"\"\"\n    print(' missing_percentage(data)')\n\ndef missing_percentage(data):\n    \"\"\"\n    Prints the count of missing values and overall percentage missing for each feature\n    \"\"\"\n    rows, cols = data.shape\n    num_missing = data.isnull().sum()\n    missing_percent = (((data.isnull().sum())\/data.shape[0]) * 100)\n    print(pd.concat([num_missing, missing_percent], axis=1).rename(columns={0:'Num_Missing',1:'Missing_Percent'}).sort_values(by='Missing_Percent', ascending=False))\n    \ndef feature_correlations(corr, feature):\n    \"\"\"\n    Return a series containing all correlations with a given feature \n    \"\"\"\n    return corr[corr['Feature 1'] == feature]\n    \n\ndef concat_df(train, test):\n    return pd.concat([train, test], sort=True).reset_index(drop=True)\n\ndef divide_df(all_data, split):\n    # Returns divided dfs of training and test set\n    return all_data.loc[:split-1], all_data.loc[split:].drop(['Survived'], axis=1)","9d7ef87a":"# Read Data\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\", index_col=0)\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\", index_col=0)\nfull = concat_df(train, test)\ndfs = [train, test]\n\nsplit = train.shape[0]\n\n# Constructs a two index dataframe containing the correlation between two features\ncorr = full.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ncorr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\n\nprint('Number of Training Examples = {}'.format(train.shape[0]))\nprint('Number of Test Examples = {}\\n'.format(test.shape[0]))\nprint('Training X Shape = {}'.format(train.shape))\nprint('Training y Shape = {}\\n'.format(train['Survived'].shape[0]))\nprint('Test X Shape = {}'.format(test.shape))\nprint('Test y Shape = {}\\n'.format(test.shape[0]))\nprint(train.columns)\nprint(test.columns)","4643b1d7":"print(train.info())","701f3439":"train.sample(3)","d23e6408":"print(test.info())","2d145b7e":"test.sample(3)","3c89fa96":"print(missing_percentage(full))","cf2abd6c":"print(\"{} \\n\\n {}\".format(feature_correlations(corr, 'Age'), full.groupby(['Sex','Pclass']).median()['Age']))","e72ab7f3":"# Filling the missing values in Age with the medians of Sex and Pclass groups\nfull['Age'] = np.where((full.Sex == 'female') & (full.Pclass == 1) & (full.Age.isnull()), 36,\n                       np.where((full.Sex == 'female') & (full.Pclass == 2) & (full.Age.isnull()), 28,\n                           np.where((full.Sex == 'female') & (full.Pclass == 3) & (full.Age.isnull()), 22,\n                               np.where((full.Sex == 'male') & (full.Pclass == 1) & (full.Age.isnull()), 42,\n                                   np.where((full.Sex == 'male') & (full.Pclass == 2) & (full.Age.isnull()), 29.5,\n                                       np.where((full.Sex == 'male') & (full.Pclass == 3) & (full.Age.isnull()), 25, full.Age))))))","a2dafba8":"full[full['Embarked'].isnull()]","4ae66102":"full['Embarked'].fillna('S', inplace=True)","29b77053":"full['Fare'].fillna(full.groupby(['Pclass','Parch','SibSp']).Fare.median()[3][0][0], inplace=True)","6c05f909":"# Replace all numerical Cabins with 'M'\nfull['Deck'] = full['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')","9aa5837c":"# The distribution of Decks and count of each Pclass per deck; it's a two level dataframe with Deck and Pclass level 0 and level 1 respectively\nfull_decks = full.groupby(['Deck', 'Pclass']).count().drop(columns=['Survived', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Cabin', 'Ticket']).rename(columns={'Name': 'Count'}).transpose()\n\n# Dictionary to track the number of passengers belongin to each group\ndeck_counts = {'A': {}, 'B': {}, 'C': {}, 'D': {}, 'E': {}, 'F': {}, 'G': {}, 'M': {}, 'T': {}}\n\n# full_decks is a two index df, we select the first level with .levels[0] which returns 'Deck'\ndecks = full_decks.columns.levels[0]\n\nfull_decks","3e833f85":"# For each deck, we update deck counts with the count of passengers per Pclass\nfor deck in decks:\n    for pclass in range(1, 4):\n        try:\n            count = full_decks[deck][pclass][0]\n            deck_counts[deck][pclass] = count \n        except KeyError:\n            deck_counts[deck][pclass] = 0\n\ndeck_dist = pd.DataFrame(deck_counts)    \ndeck_dist","5005f046":"# The distribution percetnage of Pclass 1, 2, and 3\ndeck_percentages = {}\nfor col in deck_dist.columns:\n        deck_percentages[col] = [(count \/ deck_dist[col].sum()) * 100 for count in deck_dist[col]]\n\ndeck_percentages","70cc2c9a":"Pclass1, Pclass2, Pclass3 = [],[],[]\nfor deck in deck_percentages:\n    Pclass1.append(deck_percentages[deck][0])\n    \nfor deck in deck_percentages:\n    Pclass2.append(deck_percentages[deck][0] + deck_percentages[deck][1])\n    \nfor deck in deck_percentages:\n    Pclass3.append(deck_percentages[deck][0] + deck_percentages[deck][1] + deck_percentages[deck][2])\n    \nplt.figure(figsize=(16,6))\nsns.barplot(x=deck_dist.columns, y=Pclass3, label=\"Pclass3\", color=\"#2B95DB\")\nsns.barplot(x=deck_dist.columns, y=Pclass2, label=\"Pclass2\", color=\"#5EC4FF\")\nsns.barplot(x=deck_dist.columns, y=Pclass1, label=\"Pclass1\", color=\"#9BD8FF\")\n\n\nplt.title(\"sdf\")\nplt.xlabel(\"Decks\")\nplt.legend()","89bb5ef5":"full[full['Deck'] == 'T']","39e301b5":"full.Deck = np.where((full.Deck == 'T'),'A',full.Deck)","d2fc72f2":"full.groupby(['Deck','Survived']).count().drop(columns=['Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Cabin', 'Ticket', 'Pclass']).rename(columns={'Name': 'Count'}).transpose()        \ndeck_surv = full.groupby(['Deck', 'Survived']).count().drop(columns=['Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Pclass', 'Cabin', 'Ticket']).rename(columns={'Name':'Count'}).transpose()\ndeck_surv","7251ae90":"df_decks_percent = {'A': {}, 'B': {}, 'C': {}, 'D': {}, 'E': {}, 'F': {}, 'G': {}, 'M': {}}\n\nfor deck in deck_surv.columns.levels[0]:\n    df_decks_percent[deck] = deck_surv[deck][0].sum() \/ (deck_surv[deck][0].sum() + deck_surv[deck][1].sum())\n\nsurvived,not_survived = [], []\n\nfor key in df_decks_percent:\n    survived.append(df_decks_percent[key])\n    not_survived.append(1)\n\nplt.figure(figsize=(16,6))\nsns.barplot(x=deck_surv.columns.levels[0], y=not_survived, label=\"not survived\", color=\"#9BD8FF\")\nsns.barplot(x=deck_surv.columns.levels[0], y=survived, label=\"survived\", color=\"#2B95DB\")\nplt.legend()","65ae7a4b":"print(deck_dist)\nfull['Deck'].replace(['A', 'B', 'C'], 'ABC', inplace=True)\nfull['Deck'].replace(['D', 'E'], 'DE', inplace=True)\nfull['Deck'].replace(['F', 'G'], 'FG', inplace=True)\nfull['Deck'].value_counts()","5a7f6e8e":"full.drop(['Cabin'], axis = 1, inplace=True)","572e2b83":"missing_percentage(full)","c875e166":"train, test = divide_df(full, split)","00a292ff":"print(train['Survived'].value_counts())\nsurvived = train['Survived'].value_counts()[1]\nnot_survived = train['Survived'].value_counts()[0]","32781687":"sns.barplot(x=['Survived','Not Survived'], y=[survived, not_survived])","5cf86a39":"train_corr = train.corr().abs().unstack().sort_values(ascending=False).reset_index()\ntrain_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ntrain_corr.drop(train_corr[train_corr['Correlation Coefficient'] == 1.0].index, inplace=True)\ntrain_corr[train_corr['Correlation Coefficient'] > .1]","2a80b0b3":"plt.figure(figsize=(16,6))\nsns.heatmap(train.corr(), annot=True, square=True, cmap='coolwarm', annot_kws={'size': 12})","28bc81dd":"cont_features = ['Age', 'Fare']\nsurv = train['Survived'] == 1\n\nfig, axs = plt.subplots(ncols=2, nrows=2, figsize=(20, 20))\nplt.subplots_adjust(right=1.5)\n\nfor i, feature in enumerate(cont_features):    \n    # Distribution of survival in feature\n    sns.distplot(train[~surv][feature], label='Not Survived', hist=True, color='#e74c3c', ax=axs[0][i])\n    sns.distplot(train[surv][feature], label='Survived', hist=True, color='#2ecc71', ax=axs[0][i])\n    \n    # Distribution of feature in dataset\n    sns.distplot(train[feature], label='Training Set', hist=False, color='#e74c3c', ax=axs[1][i])\n    sns.distplot(test[feature], label='Test Set', hist=False, color='#2ecc71', ax=axs[1][i])\n    \n    axs[0][i].set_xlabel('')\n    axs[1][i].set_xlabel('')\n    \n    for j in range(2):        \n        axs[i][j].tick_params(axis='x', labelsize=20)\n        axs[i][j].tick_params(axis='y', labelsize=20)\n    \n    axs[0][i].legend(loc='upper right', prop={'size': 20})\n    axs[1][i].legend(loc='upper right', prop={'size': 20})\n    axs[0][i].set_title('Distribution of Survival in {}'.format(feature), size=20, y=1.05)\n\naxs[1][0].set_title('Distribution of {} Feature'.format('Age'), size=20, y=1.05)\naxs[1][1].set_title('Distribution of {} Feature'.format('Fare'), size=20, y=1.05)\n        \nplt.show()\n","c466b76b":"# Cut the Fare into 13 bins (we normally would normally not want a bin count > 15)\nfull['Fare'] = pd.qcut(full['Fare'], 13)\n\nplt.figure(figsize=(22,9))\n\n# We plot the Fare with respect to those who survived and did not survive\nsns.countplot(x='Fare', hue='Survived', data=full)\n\n# labels\nplt.xlabel('Fare', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Fare'), size=15, y=1.05)","86dce772":"# There are 99 unique values, being close to 100 I cut Age into 10 bins\nfull['Age'] = pd.qcut(full['Age'], 10)\n\nplt.figure(figsize=(22,9))\n\n# We plot the Age with respect to those who survived and did not survive\nsns.countplot(x='Age', hue='Survived', data=full)\n\nplt.ylim=(0,)","ae0dbf21":"full.Age.value_counts()","71a26b4b":"# Look at the survival and non-survival rate distributions with respect to each feature \ncat_features = ['Embarked', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Deck']\n\nfig, axs = plt.subplots(ncols=2, nrows=3, figsize=(20, 20))\nplt.subplots_adjust(right=1.5, top=1.25)\n\nfor i, feature in enumerate(cat_features, 1):    \n    plt.subplot(2, 3, i)\n    sns.countplot(x=feature, hue='Survived', data=train)\n    \n    plt.xlabel('{}'.format(feature), size=20, labelpad=15)\n    plt.ylabel('Passenger Count', size=20, labelpad=15)    \n    plt.tick_params(axis='x', labelsize=20)\n    plt.tick_params(axis='y', labelsize=20)\n    \n    plt.legend(['Not Survived', 'Survived'], loc='upper center', prop={'size': 18})\n    plt.title('Count of Survival in {} Feature'.format(feature), size=20, y=1.05)\n\nplt.show()","bcf29b7f":"full = concat_df(train, test)","370e5645":"full['Fare'] = pd.qcut(full['Fare'], 13)\n\nfig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Fare', hue='Survived', data=full)\n\nplt.xlabel('Fare', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Fare'), size=15, y=1.05)\n\nplt.show()","ab591178":"full['Age'] = pd.qcut(full['Age'], 10)\n\nfig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Age', hue='Survived', data=full)\n\nplt.xlabel('Age', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Survival Counts in {} Feature'.format('Age'), size=15, y=1.05)\n\nplt.show()","45fea429":"full['Family_Size'] = full['SibSp'] + full['Parch'] + 1\n\nfig, axs = plt.subplots(figsize=(20, 20), ncols=2, nrows=2)\nplt.subplots_adjust(right=1.5)\n\nsns.barplot(x=full['Family_Size'].value_counts().index, y=full['Family_Size'].value_counts().values, ax=axs[0][0])\nsns.countplot(x='Family_Size', hue='Survived', data=full, ax=axs[0][1])\n\naxs[0][0].set_title('Family Size Feature Value Counts', size=20, y=1.05)\naxs[0][1].set_title('Survival Counts in Family Size ', size=20, y=1.05)\n\nfamily_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\nfull['Family_Size_Grouped'] = full['Family_Size'].map(family_map)\n\nsns.barplot(x=full['Family_Size_Grouped'].value_counts().index, y=full['Family_Size_Grouped'].value_counts().values, ax=axs[1][0])\nsns.countplot(x='Family_Size_Grouped', hue='Survived', data=full, ax=axs[1][1])\n\naxs[1][0].set_title('Family Size Feature Value Counts After Grouping', size=20, y=1.05)\naxs[1][1].set_title('Survival Counts in Family Size After Grouping', size=20, y=1.05)\n\nfor i in range(2):\n    axs[i][1].legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 20})\n    for j in range(2):\n        axs[i][j].tick_params(axis='x', labelsize=20)\n        axs[i][j].tick_params(axis='y', labelsize=20)\n        axs[i][j].set_xlabel('')\n        axs[i][j].set_ylabel('')\n\nplt.show()","03694ce4":"full['Ticket_Frequency'] = full.groupby('Ticket')['Ticket'].transform('count')\n\nfig, axs = plt.subplots(figsize=(12, 9))\nsns.countplot(x='Ticket_Frequency', hue='Survived', data=full)\n\nplt.xlabel('Ticket Frequency', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Ticket Frequency'), size=15, y=1.05)\n\nplt.show()","8d71f2e7":"full['Title'] = full['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n\nfig, axs = plt.subplots(nrows=2, figsize=(20, 12))\n\n\nsns.barplot(x=full['Title'].value_counts().index, y=full['Title'].value_counts().values, ax=axs[0])\n\naxs[0].tick_params(axis='x', labelsize=10)\naxs[1].tick_params(axis='x', labelsize=15)\n\nfor i in range(2):    \n    axs[i].tick_params(axis='y', labelsize=15)\n\naxs[0].set_title('Title Feature Value Counts', size=20, y=1.05)\n\n# Reduce cardinality of Title\nfull['Title'] = full['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss\/Mrs\/Ms')\nfull['Title'] = full['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr\/Military\/Noble\/Clergy')\n\nsns.barplot(x=full['Title'].value_counts().index, y=full['Title'].value_counts().values, ax=axs[1])\naxs[1].set_title('Title Feature Value Counts After Grouping', size=20, y=1.05)\n\nplt.show()","45fc3db2":"full['Is_Married'] = 0\nfull['Is_Married'].loc[full['Title'] == 'Mrs'] = 1","f505acee":"def extract_surname(data):    \n    \n    families = []\n    \n    for i in range(len(data)):        \n        name = data.iloc[i]\n        \n        if '(' in name:\n            name_no_bracket = name.split('(')[0] \n        else:\n            name_no_bracket = name\n            \n        family = name_no_bracket.split(',')[0]\n        title = name_no_bracket.split(',')[1].strip().split(' ')[0]\n        \n        for c in string.punctuation:\n            family = family.replace(c, '').strip()\n            \n        families.append(family)\n            \n    return families\n\nfull['Family'] = extract_surname(full['Name'])\ntrain = full.loc[:890]\ntest = full.loc[891:]\ndfs = [train, test]","1b20d545":"# Creating a list of families and tickets that are occuring in both training and test set\nnon_unique_families = [x for x in train['Family'].unique() if x in test['Family'].unique()]\nnon_unique_tickets = [x for x in train['Ticket'].unique() if x in test['Ticket'].unique()]\n\n\ndf_family_survival_rate = train.groupby('Family')['Survived', 'Family','Family_Size'].median()\ndf_ticket_survival_rate = train.groupby('Ticket')['Survived', 'Ticket','Ticket_Frequency'].median()\n\nfamily_rates = {}\nticket_rates = {}\n\n# Checking a family exists in both training and test set, and has members more than 1\n# If family is in train and test and more than 1 individual\nfor i in range(len(df_family_survival_rate)):\n    if df_family_survival_rate.index[i] in non_unique_families and df_family_survival_rate.iloc[i, 1] > 1:\n        family_rates[df_family_survival_rate.index[i]] = df_family_survival_rate.iloc[i, 0]\n        \n# Checking a ticket exists in both training and test set, and has members more than 1\n# If ticket is in train and test and more than 1 occurence\nfor i in range(len(df_ticket_survival_rate)):    \n    if df_ticket_survival_rate.index[i] in non_unique_tickets and df_ticket_survival_rate.iloc[i, 1] > 1:\n        ticket_rates[df_ticket_survival_rate.index[i]] = df_ticket_survival_rate.iloc[i, 0]","73c9cf8c":"# Overall mean survival rate of the training set\nmean_survival_rate = np.mean(train['Survived'])\n\ntrain_family_survival_rate = []\ntrain_family_survival_rate_NA = []\ntest_family_survival_rate = []\ntest_family_survival_rate_NA = []\n\n# For each family in train , if it's in family rates, append the family size to family survival rate. Also, append 1 if present\n# If not present, append mean survival rate and 0\nfor i in range(len(train)):\n    if train['Family'][i] in family_rates:\n        train_family_survival_rate.append(family_rates[train['Family'][i]])\n        train_family_survival_rate_NA.append(1)\n    else:\n        train_family_survival_rate.append(mean_survival_rate)\n        train_family_survival_rate_NA.append(0)\n    \n# For each family in test , if it's in family rates, append the family size to family survival rate. Also, append 1 if present\n# If not present, append mean survival rate and 0\nfor i in range(len(test)):\n    if test['Family'].iloc[i] in family_rates:\n        test_family_survival_rate.append(family_rates[test['Family'].iloc[i]])\n        test_family_survival_rate_NA.append(1)\n    else:\n        test_family_survival_rate.append(mean_survival_rate)\n        test_family_survival_rate_NA.append(0)\n        \n# Add these survival rates to both train and test data\ntrain['Family_Survival_Rate'] = train_family_survival_rate\ntrain['Family_Survival_Rate_NA'] = train_family_survival_rate_NA\ntest['Family_Survival_Rate'] = test_family_survival_rate\ntest['Family_Survival_Rate_NA'] = test_family_survival_rate_NA\n\ntrain_ticket_survival_rate = []\ntrain_ticket_survival_rate_NA = []\ntest_ticket_survival_rate = []\ntest_ticket_survival_rate_NA = []\n\n# For each ticket in train , if it's in ticket rates, append the ticket frequency size to ticket survival rate. Also, append 1 if present\n# If not present, append mean survival rate and 0\nfor i in range(len(train)):\n    if train['Ticket'][i] in ticket_rates:\n        train_ticket_survival_rate.append(ticket_rates[train['Ticket'][i]])\n        train_ticket_survival_rate_NA.append(1)\n    else:\n        train_ticket_survival_rate.append(mean_survival_rate)\n        train_ticket_survival_rate_NA.append(0)\n\n# For each ticket in test , if it's in ticket rates, append the ticket frequency size to ticket survival rate. Also, append 1 if present\n# If not present, append mean survival rate and 0        \nfor i in range(len(test)):\n    if test['Ticket'].iloc[i] in ticket_rates:\n        test_ticket_survival_rate.append(ticket_rates[test['Ticket'].iloc[i]])\n        test_ticket_survival_rate_NA.append(1)\n    else:\n        test_ticket_survival_rate.append(mean_survival_rate)\n        test_ticket_survival_rate_NA.append(0)\n\n# Add these survival rates to both train and test data\ntrain['Ticket_Survival_Rate'] = train_ticket_survival_rate\ntrain['Ticket_Survival_Rate_NA'] = train_ticket_survival_rate_NA\ntest['Ticket_Survival_Rate'] = test_ticket_survival_rate\ntest['Ticket_Survival_Rate_NA'] = test_ticket_survival_rate_NA","bb3b0cf2":"dfs = [train, test]","dfc1f62c":"# Let survival rates for train and test sets be the average of both ticket and family survival rates\nfor df in dfs:\n    df['Survival_Rate'] = (df['Ticket_Survival_Rate'] + df['Family_Survival_Rate']) \/ 2\n    df['Survival_Rate_NA'] = (df['Ticket_Survival_Rate_NA'] + df['Family_Survival_Rate_NA']) \/ 2    ","3508d178":"non_numeric_features = ['Embarked', 'Sex', 'Deck', 'Title', 'Family_Size_Grouped', 'Age', 'Fare']\nfor df in dfs:\n    for feature in non_numeric_features:     \n        df[feature] = LabelEncoder().fit_transform(df[feature])","0d35828d":"cat_features = ['Pclass', 'Sex', 'Deck', 'Embarked', 'Title', 'Family_Size_Grouped']\nencoded_features = []\n\nfor df in dfs:\n    for feature in cat_features:\n        encoded_feat = OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n        n = df[feature].nunique()\n        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n        encoded_df.index = df.index\n        encoded_features.append(encoded_df)\n\ntrain = pd.concat([train, *encoded_features[:6]], axis=1)\ntest = pd.concat([test, *encoded_features[6:]], axis=1)","ee561594":"full = concat_df(train, test)\ndrop_cols = ['Deck', 'Embarked', 'Family', 'Family_Size', 'Family_Size_Grouped', 'Survived',\n             'Name', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Ticket', 'Title',\n            'Ticket_Survival_Rate', 'Family_Survival_Rate', 'Ticket_Survival_Rate_NA', 'Family_Survival_Rate_NA']\n\nfull.drop(columns=drop_cols, inplace=True)","9c2f2e38":"X_train = StandardScaler().fit_transform(train.drop(columns=drop_cols))\ny_train = train['Survived'].values\nX_test = StandardScaler().fit_transform(test.drop(columns=drop_cols))\ny_test = test['Survived'].values\n\nprint('X_train shape: {}'.format(X_train.shape))\nprint('y_train shape: {}'.format(y_train.shape))\nprint('X_test shape: {}'.format(X_test.shape))\nprint('y_test shape: {}'.format(y_test.shape))","f462a87a":"\"\"\"\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\npipe = Pipeline(steps=[('classifier', RandomForestClassifier(random_state = 2019))])\n\nparam_grid = { \n    'classifier__criterion': ['gini','entropy'],\n    'classifier__n_estimators': list(range(1,10,1)) + list(range(10,50,10)) + list(range(50,2001,50)),\n    'classifier__max_depth': range(1,11,1),\n    'classifier__min_samples_split': range(2,5,1),\n    'classifier__min_samples_leaf': range(2,11,1),\n    'classifier__max_features': ['auto', 'sqrt', 'log2'],\n    'classifier__oob_score': [True],\n    'classifier__verbose': range(0,5,1)}\n\n\nCV = GridSearchCV(pipe, param_grid, n_jobs= 1)\nCV.fit(X_train, y_train)  \nprint(CV.best_params_)    \nprint(CV.best_score_)\n\"\"\"","fd039d8f":"single_best_model = RandomForestClassifier(criterion='gini', \n                                           n_estimators=1100, # The number of trees in the forest.\n                                           max_depth=5, #The maximum depth of the tree \n                                           min_samples_split=4, # The minimum number of samples required to split an internal node:\n                                           min_samples_leaf=5, # The minimum number of samples required to be at a leaf node. A split point at any depth will \n                                                               # only be considered if it leaves at least min_samples_leaf training samples in each of the \n                                                               # left and right branches. This may have the effect of smoothing the model, especially in\n                                                               # regression.\n                                           max_features='auto', # The number of features to consider when looking for the best split:\n                                           oob_score=True, # Whether to use out-of-bag samples to estimate the generalization accuracy.\n                                           random_state=SEED,\n                                           n_jobs=-1, # -1 means use all processors\n                                           verbose=1)\n\nleaderboard_model = RandomForestClassifier(criterion='gini',\n                                           n_estimators=1750,\n                                           max_depth=7,\n                                           min_samples_split=6,\n                                           min_samples_leaf=6,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=SEED,\n                                           n_jobs=-1,\n                                           verbose=1) ","3e02aa94":"N = 5 # Number of golds\noob = 0 # out of bag score\n\n# Creates a table to contain each fold probability\nprobs = pd.DataFrame(np.zeros((len(X_test), N * 2)), columns=['Fold_{}_Prob_{}'.format(i, j) for i in range(1, N + 1) for j in range(2)])\nimportances = pd.DataFrame(np.zeros((X_train.shape[1], N)), columns=['Fold_{}'.format(i) for i in range(1, N + 1)], index=full.columns)\nfprs, tprs, scores = [], [], []\n\n# Provides train\/test indices to split data in train\/test sets.\nskf = StratifiedKFold(n_splits=N, random_state=N, shuffle=True)\n\n# For enumerate, the second parameter returns the fold number\n# skf -> provides train\/test indices to split data in train\/test sets.\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n    print('Fold {}\\n'.format(fold))\n    \n    # Fitting the model\n    leaderboard_model.fit(X_train[trn_idx], y_train[trn_idx])\n    \n    # Computing Train AUC score\n    # roc_curve takes y_true binary labels, y_scores and returns false positive rate, true positive rate, and decreasing thresholds\n    trn_fpr, trn_tpr, trn_thresholds = roc_curve(y_train[trn_idx], leaderboard_model.predict_proba(X_train[trn_idx])[:, 1])\n    trn_auc_score = auc(trn_fpr, trn_tpr)\n    \n    # Computing Validation AUC score\n    val_fpr, val_tpr, val_thresholds = roc_curve(y_train[val_idx], leaderboard_model.predict_proba(X_train[val_idx])[:, 1])\n    val_auc_score = auc(val_fpr, val_tpr)  \n      \n    # Append scores\n    scores.append((trn_auc_score, val_auc_score))\n    fprs.append(val_fpr)\n    tprs.append(val_tpr)\n    \n    # X_test probabilities\n    probs.loc[:, 'Fold_{}_Prob_0'.format(fold)] = leaderboard_model.predict_proba(X_test)[:, 0]\n    probs.loc[:, 'Fold_{}_Prob_1'.format(fold)] = leaderboard_model.predict_proba(X_test)[:, 1]\n    \n    # Sets all rows in fold-1 column\n    importances.iloc[:, fold - 1] = leaderboard_model.feature_importances_\n        \n    oob += leaderboard_model.oob_score_ \/ N\n    print('Fold {} OOB Score: {}\\n'.format(fold, leaderboard_model.oob_score_))   \n    \nprint('Average OOB Score: {}'.format(oob))","ed1e0e5e":"importances['Mean_Importance'] = importances.mean(axis=1)\nimportances.sort_values(by='Mean_Importance', inplace=True, ascending=False)\n\nplt.figure(figsize=(15, 20))\nsns.barplot(x='Mean_Importance', y=importances.index, data=importances)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\nplt.title('Random Forest Classifier Mean Feature Importance Between Folds', size=15)\n\nplt.show()","86256de4":"def plot_roc_curve(fprs, tprs):\n    \n    tprs_interp = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n    f, ax = plt.subplots(figsize=(15, 15))\n    \n    # Plotting ROC for each fold and computing AUC scores\n    for i, (fpr, tpr) in enumerate(zip(fprs, tprs), 1):\n        # mean_fpr -> np.linspace -> The x-coordinates at which to evaluate the interpolated values.\n        # fpr -> x -> The x-coordinates of the data points, must be increasing if argument period is not specified\n        # tpr -> y -> The y-coordinates of the data points, same length as xp.\n        # interp -> returns the one-dimensional piecewise linear interpolant to a function with given discrete data points (xp, fp), evaluated at x.\n        tprs_interp.append(np.interp(mean_fpr, fpr, tpr)) # line for each fold\n        tprs_interp[-1][0] = 0.0\n        roc_auc = auc(fpr, tpr) # auc score for fold i\n        aucs.append(roc_auc) # append to list of scores\n        ax.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC Fold {} (AUC = {:.3f})'.format(i, roc_auc)) # plot each fold\n        \n    # Plotting ROC for random guessing\n    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=0.8, label='Random Guessing')\n    \n    mean_tpr = np.mean(tprs_interp, axis=0) # mean of all lines\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr) # mean scores\n    std_auc = np.std(aucs) # std of scores\n    \n    # Plotting the mean ROC\n    ax.plot(mean_fpr, mean_tpr, color='b', label='Mean ROC (AUC = {:.3f} $\\pm$ {:.3f})'.format(mean_auc, std_auc), lw=2, alpha=0.8)\n    \n    # Plotting the standard deviation around the mean ROC Curve\n    std_tpr = np.std(tprs_interp, axis=0)\n    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label='$\\pm$ 1 std. dev.')\n    \n    ax.set_xlabel('False Positive Rate', size=15, labelpad=20)\n    ax.set_ylabel('True Positive Rate', size=15, labelpad=20)\n    ax.tick_params(axis='x', labelsize=15)\n    ax.tick_params(axis='y', labelsize=15)\n    ax.set_xlim([-0.05, 1.05])\n    ax.set_ylim([-0.05, 1.05])\n\n    ax.set_title('ROC Curves of Folds', size=20, y=1.02)\n    ax.legend(loc='lower right', prop={'size': 13})\n    \n    plt.show()\n\nplot_roc_curve(fprs, tprs)","32329866":"# Gets all survived columns\nclass_survived = [col for col in probs.columns if col.endswith('Prob_1')]\n\n# Sums all columns for fold 1, and divide by N to get average\nprobs['1'] = probs[class_survived].sum(axis=1) \/ N\nprobs['0'] = probs.drop(columns=class_survived).sum(axis=1) \/ N\n\n# Add new column called pred\nprobs['pred'] = 0\n\n# Get index of all probabilities >= .5\npos = probs[probs['1'] >= 0.5].index\n\n# Locate all indexes that match pos and set 'pred' = 1\nprobs.loc[pos, 'pred'] = 1\n\n# Use probs['pred'], convert to int, and use them to predict\ny_pred = probs['pred'].astype(int)","02f0a3a0":"submission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\n\n# Set PassengerId to text.index + 1\nsubmission_df['PassengerId'] = test.index + 1\n\n# Set survived to y_pred values\nsubmission_df['Survived'] = y_pred.values\nsubmission_df.to_csv('submissions.csv', header=True, index=False)\nsubmission_df.head(10)","87bcbb50":"> # Marriage","03b8ac0a":"> # Look at the number of passengers who survived \/ not survived in our training set and see how they compare","b3b9cad4":">>> There's only 1 row with Deck T. Seeing that the passenger is Pclass 1, we instead put him in Cabin A","fd2ff764":"# Feature Engineering","c5e18e69":">> We then look at the survival rate distribution among cabin decks","8512acc1":"># Fare\n>> The price of a ticket is likely determined by the individual's class and their family size (Pclass + Parch + 1) so we want those combinations\n\n\n>># Learning \n>>> 1. When we groupby Pclass, Parch, and SibSp to get every unique row combination of the 3 features. \n>>> 2. Then select the **Fare** column and find the median of each unique combination.  \n>>> 3. For the missing row, Pclass = 3, Parch = 0, and SibSp = 0; Select by **full.groupby(['Pclass','Parch','SibSp']).Fare.median()[3][0][0]**\n","d7dd6907":"># Cabins\n\n>> 1. We replace all Cabins that are numerical with 'M' for missing\n>> 2. We look at the distribution of passengers of each Pclass for each deck\n>> 3. We visually represent each deck \n\n> # Learning\n>> \n\n","da942d16":"> Examine how the survival \/ non survial rates are distributed across the bins for **Fare**","0a99645d":"![Titantic Cabin](http:\/\/vignette.wikia.nocookie.net\/titanic\/images\/f\/f9\/Titanic_side_plan.png\/revision\/latest?cb=20180322183733)","18490518":"># Embarked\n\n>> 1. Check all rows where Embarked is missing\n>> 2. After researching information about the passengers with missing Embarked, we see both belong to 'S'\n\n>> \"Mrs Stone boarded the Titanic in Southampton on 10 April 1912 and was travelling in first class with her maid Amelie Icard. She occupied cabin B-28.\" - https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/martha-evelyn-stone.html","245a88c4":">> 1. Examine the distribution of Age and Fare with respect to those who survived and did not survive\n>> 2. Examine the distribution of Age and Fare with respect to the train and test sets ","5aedbb62":"# MISSING","0e5889b6":"> # Age\n>> There is a high correlation between **Age** and **Pclass**. We look at the male and female median ages for each Pclass and use these values to fill in missing values\n\n>> # Learning\n>>> 1. Look at the correlation of Age with other features to help determine how to fill missing values\n>>> 2. After seeing high correlation between Pclass and Age, we check to see how it differentiates between male and female for each Pclass","32e55a0a":"> 1. To do feature engineering, we combine the train and test sets ","0473fa65":"# Machine Learning\n","12575e02":"> # Title\n>> 1. Extract the title from passengers\n>> 2. Reduce the cardinality of title by grouping them","603a8d98":">> # Correlations\n>>> Select all pairs of features with a correlation coefficient > .1\n\n>> # Learn\n>>> 1. .corr() returns the correlation of all numerical values in the dataset\n>>> 2. .abs() returns the absolute values of all correlations \n>>> 3. .unstack() pivots a level of the index labels\n>>> 4. .reset_index() resets the index, or a level of it.","36ca08d8":"> Examine how the survival \/ non survial rates are distributed across the bins for **Age**","d600d5e3":"# Feature Engineering","139d3d5a":"\u3002\u3002\u3002","58ed3782":"> 1. Examine how the family size is distributed with respect to passenger count and survival counts\n> 2. Group Families based and size and examine distribution with respect to passenger count and survival counts","ee50b4fa":"> # Survival of family rates and ticket rates\n\n> # Learning\n>> 1. Whenever we groupby, they are index levels corresponding to 0, 1, ..., n respectively \n>> 2. If grouping by 1 item, can use .index otherwise must specify level first\n>> 3. .iloc[row, column], give row then the column you want to select","98427aa2":">> # Plot Pclass distribution among Cabin Decks\n\n","ff9f0a6e":"> # Plot the correlations","0757691b":" # After handling all missing values, split data back into train and test sets","9b7b34b5":"> # Tickets\n>> We transforms the tickets to be the frequency which they occur in the data ","64adcc98":"> # Extract last name"}}