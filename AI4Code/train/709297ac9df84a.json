{"cell_type":{"e506c97f":"code","36b86fbf":"code","a00e5978":"code","8c1ea028":"code","ac04600f":"code","c07614f5":"code","af3e3e2c":"code","e3d31f35":"code","139160e3":"code","05f170d9":"code","8ad02c5e":"code","8e296210":"code","3ec177fb":"code","5397ce2a":"code","66ec67ff":"code","d669e241":"code","71a4b45d":"code","0bca2310":"markdown","6efe6462":"markdown"},"source":{"e506c97f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom matplotlib import pyplot as plt\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold\nimport pytorch_lightning as pl\n\npl.utilities.seed.seed_everything(42, workers=True)\n","36b86fbf":"data_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv')\n","a00e5978":"data_df.claim.value_counts(normalize=True)","8c1ea028":"data_df.isna().sum()","ac04600f":"data_df = data_df[data_df.columns[1:]]\n\ndata_df['fold'] = -1\nskf = StratifiedKFold(n_splits=13, shuffle=True)\nfor fold_i, (train_inds, val_inds) in enumerate(skf.split(data_df, data_df['claim'])):\n    data_df.fold.iloc[val_inds] = fold_i","c07614f5":"!pip install ..\/input\/monaiweekly07\/monai_weekly-0.7.dev2135-py3-none-any.whl","af3e3e2c":"from torch.utils.data import TensorDataset, DataLoader\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom monai.metrics import ROCAUCMetric\nfrom pytorch_lightning.core.memory import ModelSummary\n\n\nclass Model(pl.LightningModule):\n    def __init__(self, in_size, lr):\n        super().__init__()\n        \n        self.conv1 = nn.Conv1d(1, 8, kernel_size=1)\n        self.conv2 = nn.Conv1d(8, 16, kernel_size=1)\n        self.conv3 = nn.Conv1d(16, 32, kernel_size=1)\n        self.conv4 = nn.Conv1d(32, 64, kernel_size=1)\n        self.fc1 = nn.Linear(7552, 256)\n        self.fc2 = nn.Linear(256, 1)\n        self.relu = F.relu\n        self.flatten = nn.Flatten()\n        self.roc_auc_metric = ROCAUCMetric()\n        self.bnorm1 = nn.BatchNorm1d(32)\n        self.bnorm2 = nn.BatchNorm1d(64)\n        self.lr = lr\n        self.apply(self.init_weights)\n        \n    def init_weights(self, m):\n        if isinstance(m, nn.Conv1d):\n            torch.nn.init.kaiming_normal_(m.weight)\n            m.bias.data.fill_(0.01)\n    \n    def forward(self, x):\n        n, s = x.shape\n        x = x.reshape(n, 1, s)\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x = self.relu(self.bnorm1(self.conv3(x)))\n        x = self.relu(self.conv4(x))\n        x = self.flatten(x)\n#         print(x.shape)\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n    \n    def training_step(self, batch, batch_idx):\n        X, y = batch\n        y_hat = self(X).squeeze(1)\n        loss = F.binary_cross_entropy_with_logits(y_hat, y)  \n        return {\n            'loss': loss,\n        }\n    \n    \n    def validation_step(self, batch, batch_idx):\n        X, y = batch\n        y_hat = self(X).squeeze(1)\n        self.roc_auc_metric(y_hat, y)\n    \n    def validation_epoch_end(self, training_step_outputs):\n        roc_auc = self.roc_auc_metric.aggregate()\n        self.roc_auc_metric.reset()\n        print('roc_auc:', roc_auc)\n        self.log('roc_auc', roc_auc)\n        \n    def predict_step(self, X, batch_idx: int, dataloader_idx: int = None):\n        return self(X[0])    \n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.RMSprop(self.parameters(), lr=self.lr)\n        return optimizer","e3d31f35":"import gc\npipes = []\nfor fold_i in range(13):\n    print('Fold:', fold_i)\n    train_data = data_df[data_df.fold!=fold_i]\n    val_data = data_df[data_df.fold==fold_i]\n    X_train = train_data.drop(['claim', 'fold'], axis=1)\n    y_train = train_data.claim\n    X_val = val_data.drop(['claim', 'fold'], axis=1)\n    y_val = val_data.claim\n\n    pipe = Pipeline([\n            ('imputer', SimpleImputer(strategy='mean')),\n            (\"scaler\", StandardScaler()),\n    ])\n\n    pipe.fit(X_train)\n    pipes.append(pipe)\n    \n    X_train = pd.DataFrame(pipe.transform(X_train), columns=X_train.columns, index=X_train.index)\n    X_val = pd.DataFrame(pipe.transform(X_val), columns=X_val.columns, index=X_val.index)\n\n\n    \n    train_ds = TensorDataset(torch.FloatTensor(X_train.values), torch.FloatTensor(y_train.values))\n    val_ds = TensorDataset(torch.FloatTensor(X_val.values), torch.FloatTensor(y_val.values))\n\n    train_dl = DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=16)\n    val_dl = DataLoader(val_ds, batch_size=256, shuffle=False, num_workers=16)\n\n    model = Model(X_train.shape[1], 0.0001)\n\n    checkpoint_callback = pl.callbacks.ModelCheckpoint(dirpath='models', filename=f'model_{fold_i}_' + '{roc_auc:.3}', monitor='roc_auc', mode='max', save_weights_only=True)\n\n    print(ModelSummary(model))\n    trainer = pl.Trainer(\n        fast_dev_run=False, max_epochs=10, \n         gpus=1,precision=16,\n         auto_lr_find=True,  limit_train_batches=1.0, limit_val_batches=1.0, \n         num_sanity_val_steps=0, val_check_interval=0.33, \n         callbacks=[checkpoint_callback]\n     )\n\n    trainer.fit(model, train_dl, val_dl)\n    \n    del model, trainer, val_data, train_data, X_train, X_val, y_train, y_val, train_ds, val_ds, train_dl, val_dl\n    gc.collect()\n    torch.cuda.empty_cache()","139160e3":"!ls models","05f170d9":"trained_models = [\n    'model_0_roc_auc=0.807.ckpt',\n    'model_1_roc_auc=0.81.ckpt',\n    'model_2_roc_auc=0.807.ckpt',\n    'model_3_roc_auc=0.808.ckpt',\n    'model_4_roc_auc=0.807.ckpt',\n    'model_5_roc_auc=0.807.ckpt',\n    'model_6_roc_auc=0.809.ckpt',\n    'model_7_roc_auc=0.806.ckpt',\n    'model_8_roc_auc=0.808.ckpt',\n    'model_9_roc_auc=0.807.ckpt',\n    'model_10_roc_auc=0.804.ckpt',\n    'model_11_roc_auc=0.807.ckpt',\n    'model_12_roc_auc=0.809.ckpt'\n]","8ad02c5e":"all_preds = []\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\ntest_df = test_df.drop(['id'], axis=1)\n\nsample_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\ntrainer = pl.Trainer(gpus=1)\n\nmodel = Model(118, 0.0001)\nfor model_name in trained_models:\n    fold_n = int(model_name.split('_')[1])\n    pipe = pipes[fold_n]\n    test_data = pipe.transform(test_df)\n    \n    model.load_state_dict(torch.load('models\/' + model_name)['state_dict'])\n    test_ds = TensorDataset(torch.FloatTensor(test_data))\n    test_dl = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=16)\n    preds = trainer.predict(model, test_dl)\n    preds = torch.cat(preds).cpu().numpy().flatten()\n    all_preds.append(preds)","8e296210":"np_all_preds = np.array(all_preds)\nnp_all_preds[:, :4], np_all_preds[:, -4:]","3ec177fb":"avg_preds = np.mean(np_all_preds, axis=0)\navg_preds[:4], avg_preds[-4:]","5397ce2a":"# avg_preds = data_df.claim.mean() + (avg_preds - avg_preds.mean()) * data_df.claim.std() \/ avg_preds.std()","66ec67ff":"avg_preds[:5]","d669e241":"sample_df['claim'] = avg_preds\nsample_df.to_csv('submission.csv', index=False)","71a4b45d":"!ls models","0bca2310":"<h1> Network implementation <\/h1>","6efe6462":"<h1> Inference <\/h1>"}}