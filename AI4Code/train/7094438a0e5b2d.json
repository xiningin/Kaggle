{"cell_type":{"dce7e967":"code","6ec0d73d":"code","c530d529":"code","8b6188f4":"code","2320b1d8":"code","705e0480":"code","6117346c":"code","928d54b8":"code","0e5de268":"code","56e2b5b6":"code","08a4a9c4":"code","918106ed":"code","8842fa2d":"code","b3658328":"code","334bfa6c":"code","01d51208":"code","cf3df979":"code","ebe57472":"code","a7112525":"code","c21fac2f":"code","43ca2b10":"code","ce5e987d":"code","f4e0c04f":"code","8b5a9372":"code","9d11bb3e":"code","439b7505":"code","19b7e017":"code","5ee78216":"code","bef43315":"code","613feb32":"code","e8b448ea":"code","e0e9a6df":"code","48d08445":"code","d31ddcae":"code","78c75264":"code","e8ab8d61":"code","7c823fed":"code","ac0f56df":"code","e4b49e7d":"code","b7e7ac42":"code","11277510":"code","c6a4f6fd":"code","db21e935":"code","0da2b02f":"code","664dd39f":"code","c8359e7a":"code","29e95c4c":"code","6fd9fa7c":"code","6350b0b0":"code","d1f327d0":"code","b8665a15":"code","f9ee1f1a":"code","9c2a4e68":"code","0752faa0":"code","e5c4057d":"code","0102304a":"code","5bb4e218":"code","91c07155":"code","4d8d1ff9":"code","737dab1a":"code","264c6611":"code","2d5bd7a5":"code","2b6695fb":"code","e20f46f2":"code","04725094":"code","500be03e":"code","9fd9a35f":"code","6fe1c601":"code","45a2a28b":"code","66164873":"code","1ce6dca3":"code","2ab2231b":"code","6309de40":"markdown","2a46d0a6":"markdown","5bab909b":"markdown","51115501":"markdown","ccb5d898":"markdown","efb36499":"markdown","f4fdbc57":"markdown","2c361820":"markdown","4640845f":"markdown","252ad18a":"markdown"},"source":{"dce7e967":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6ec0d73d":"from keras import layers\nfrom keras.layers import Input, Dense, Dropout, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, Embedding, Add\nfrom keras.layers import Conv1D, GlobalAveragePooling1D, AveragePooling2D, MaxPooling2D, MaxPool1D, ZeroPadding1D, GlobalMaxPooling2D, GlobalAveragePooling2D, LSTM, SpatialDropout1D\nfrom keras.models import Sequential, Model\nfrom keras.preprocessing import image\nfrom keras.utils import plot_model\nfrom keras.applications.inception_v3 import InceptionV3\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nfrom keras.layers.merge import concatenate","c530d529":"mit_test_data = pd.read_csv('\/kaggle\/input\/heartbeat\/mitbih_test.csv', header=None)\nmit_train_data = pd.read_csv('\/kaggle\/input\/heartbeat\/mitbih_train.csv', header=None)","8b6188f4":"from keras.utils import to_categorical\nX, y = mit_train_data.iloc[: , :-1], mit_train_data.iloc[: , -1]\nX, valX, y, valy= train_test_split(X,y,test_size=0.2)\ntestX, testy = mit_test_data.iloc[: , :-1], mit_test_data.iloc[: , -1]\ny = to_categorical(y)\ntesty = to_categorical(testy)\nvaly=to_categorical(valy)","2320b1d8":"print(\"X shape=\" +str(X.shape))\nprint(\"y shape=\" +str(y.shape))\nprint(\"valX shape=\" +str(valX.shape))\nprint(\"valy shape=\" +str(valy.shape))\nprint(\"testX shape=\" +str(testX.shape))\nprint(\"testy shape=\" +str(testy.shape))","705e0480":"ann_model = Sequential()\nann_model.add(Dense(50, activation='relu', input_shape=(187,)))\nann_model.add(Dense(50, activation='relu'))\nann_model.add(Dense(50, activation='relu'))\nann_model.add(Dense(50, activation='relu'))\nann_model.add(Dense(5, activation='softmax'))","6117346c":"ann_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","928d54b8":"ann_model.summary()","0e5de268":"plot_model(ann_model)","56e2b5b6":"ann_model_history = ann_model.fit(X, y,validation_data=(valX, valy), epochs=20)\n","08a4a9c4":"plt.plot(ann_model_history.history['accuracy'])\nplt.plot(ann_model_history.history['val_accuracy'])\nplt.legend([\"accuracy\",\"val_accuracy\"])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')","918106ed":"plt.plot(ann_model_history.history['loss'])\nplt.plot(ann_model_history.history['val_loss'])\nplt.legend([\"loss\",\"val_loss\"])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')","8842fa2d":"y_true=[]\nfor element in testy:\n    y_true.append(np.argmax(element))\nprediction_proba=ann_model.predict(testX)\nprediction=np.argmax(prediction_proba,axis=1)","b3658328":"ann_model_cf_matrix = confusion_matrix(y_true, prediction)\nsns.heatmap(ann_model_cf_matrix\/np.sum(ann_model_cf_matrix), annot=True,fmt='.3%', cmap='Blues')","334bfa6c":"from sklearn.utils import shuffle\nmit_train_data[187] = mit_train_data[187].astype('int32')\nmit_test_data[187] = mit_test_data[187].astype('int32')\nX_train = np.array(mit_train_data.iloc[:, :187])\nX_test = np.array(mit_test_data.iloc[:, :187])\ny_train = np.array(mit_train_data[187])\ny_test = np.array(mit_test_data[187])\nX_train, y_train = shuffle(X_train, y_train, random_state = 101)\nX_test, y_test = shuffle(X_test, y_test, random_state = 101)\nX_train = np.expand_dims(X_train, 2)\nX_test = np.expand_dims(X_test, 2)\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","01d51208":"lenet_5_model=Sequential()\n\nlenet_5_model.add(Conv1D(filters=6, kernel_size=3, padding='same', activation='relu', input_shape=(187,1)))\nlenet_5_model.add(BatchNormalization())\nlenet_5_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nlenet_5_model.add(Conv1D(filters=16, strides=1, kernel_size=5, activation='relu'))\nlenet_5_model.add(BatchNormalization())\nlenet_5_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nlenet_5_model.add(Flatten())\n\nlenet_5_model.add(Dense(64, activation='relu'))\n\nlenet_5_model.add(Dense(32, activation='relu'))\n\nlenet_5_model.add(Dense(5, activation = 'softmax'))\n","cf3df979":"lenet_5_model.summary()","ebe57472":"plot_model(lenet_5_model)","a7112525":"lenet_5_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","c21fac2f":"lenet_5_model_history = lenet_5_model.fit(X_train, y_train, epochs = 20, batch_size = 100, validation_data = (X_test, y_test))","43ca2b10":"plt.plot(lenet_5_model_history.history['accuracy'])\nplt.plot(lenet_5_model_history.history['val_accuracy'])\nplt.legend([\"accuracy\",\"val_accuracy\"])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')","ce5e987d":"plt.plot(lenet_5_model_history.history['loss'])\nplt.plot(lenet_5_model_history.history['val_loss'])\nplt.legend([\"loss\",\"val_loss\"])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')","f4e0c04f":"y_true=[]\nfor element in y_test:\n    y_true.append(np.argmax(element))\nprediction_proba=lenet_5_model.predict(X_test)\nprediction=np.argmax(prediction_proba,axis=1)","8b5a9372":"lenet_5_model_cf_matrix = confusion_matrix(y_true, prediction)\nsns.heatmap(lenet_5_model_cf_matrix\/np.sum(lenet_5_model_cf_matrix), annot=True,fmt='.3%', cmap='Blues')","9d11bb3e":"alexNet_model=Sequential()\n\nalexNet_model.add(Conv1D(filters=96, activation='relu', kernel_size=11, strides=4, input_shape=(187,1)))\nalexNet_model.add(BatchNormalization())\nalexNet_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nalexNet_model.add(Conv1D(filters=256, kernel_size=5, padding='same', activation='relu'))\nalexNet_model.add(BatchNormalization())\nalexNet_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nalexNet_model.add(Conv1D(filters=384, padding='same', kernel_size=3, activation='relu'))\nalexNet_model.add(Conv1D(filters=384, kernel_size=3, activation='relu'))\nalexNet_model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\nalexNet_model.add(BatchNormalization())\nalexNet_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nalexNet_model.add(Flatten())\nalexNet_model.add(Dense(4096, activation='relu'))\nalexNet_model.add(Dropout(0.4))\nalexNet_model.add(Dense(4096, activation='relu'))\nalexNet_model.add(Dropout(0.4))\nalexNet_model.add(Dense(5, activation='softmax'))","439b7505":"alexNet_model.summary()","19b7e017":"plot_model(alexNet_model)","5ee78216":"alexNet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","bef43315":"alexNet_model_history = alexNet_model.fit(X_train, y_train, epochs = 20, batch_size = 100, validation_data = (X_test, y_test))","613feb32":"plt.plot(alexNet_model_history.history['accuracy'])\nplt.plot(alexNet_model_history.history['val_accuracy'])\nplt.legend([\"accuracy\",\"val_accuracy\"])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')","e8b448ea":"plt.plot(alexNet_model_history.history['loss'])\nplt.plot(alexNet_model_history.history['val_loss'])\nplt.legend([\"loss\",\"val_loss\"])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')","e0e9a6df":"y_true=[]\nfor element in y_test:\n    y_true.append(np.argmax(element))\nprediction_proba=alexNet_model.predict(X_test)\nprediction=np.argmax(prediction_proba,axis=1)","48d08445":"alexNet_model_cf_matrix = confusion_matrix(y_true, prediction)\nsns.heatmap(alexNet_model_cf_matrix\/np.sum(alexNet_model_cf_matrix), annot=True,fmt='.3%', cmap='Blues')","d31ddcae":"vgg_16_model=Sequential()\n\nvgg_16_model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu',  input_shape=(187,1)))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nvgg_16_model.add(Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nvgg_16_model.add(Conv1D(filters=256, kernel_size=3, activation='relu', padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Conv1D(filters=256, kernel_size=3, activation='relu', padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Conv1D(filters=256, kernel_size=3, activation='relu', padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nvgg_16_model.add(Conv1D(filters=512, kernel_size=3, activation='relu', padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Conv1D(filters=512, kernel_size=3, activation='relu', padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Conv1D(filters=512, kernel_size=3, activation='relu', padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nvgg_16_model.add(Conv1D(filters=512, kernel_size=3, activation='relu', padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Conv1D(filters=512, kernel_size=1, activation='relu', padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(Conv1D(filters=512, kernel_size=1, activation='relu', padding='same'))\nvgg_16_model.add(BatchNormalization())\nvgg_16_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n\nvgg_16_model.add(Flatten())\nvgg_16_model.add(Dense(4096, activation='relu'))\nvgg_16_model.add(Dropout(0.4))\nvgg_16_model.add(Dense(4096, activation='relu'))\nvgg_16_model.add(Dropout(0.4))\nvgg_16_model.add(Dense(5, activation='softmax'))","78c75264":"vgg_16_model.summary()","e8ab8d61":"plot_model(vgg_16_model)","7c823fed":"vgg_16_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","ac0f56df":"vgg_16_model_history = vgg_16_model.fit(X_train, y_train, epochs = 20, batch_size = 100, validation_data = (X_test, y_test))","e4b49e7d":"plt.plot(vgg_16_model_history.history['accuracy'])\nplt.plot(vgg_16_model_history.history['val_accuracy'])\nplt.legend([\"accuracy\",\"val_accuracy\"])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')","b7e7ac42":"plt.plot(vgg_16_model_history.history['loss'])\nplt.plot(vgg_16_model_history.history['val_loss'])\nplt.legend([\"loss\",\"val_loss\"])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')","11277510":"y_true=[]\nfor element in y_test:\n    y_true.append(np.argmax(element))\nprediction_proba=vgg_16_model.predict(X_test)\nprediction=np.argmax(prediction_proba,axis=1)","c6a4f6fd":"vgg_16_model_cf_matrix = confusion_matrix(y_true, prediction)\nsns.heatmap(vgg_16_model_cf_matrix\/np.sum(vgg_16_model_cf_matrix), annot=True,fmt='.3%', cmap='Blues')","db21e935":"def identity_block(X, f, filters):\n    F1, F2, F3 = filters\n    \n    X_shortcut = X\n    \n    X = Conv1D(filters = F1, kernel_size = 1, activation='relu', strides = 1, padding = 'valid')(X)\n    X = BatchNormalization()(X)\n    \n    X = Conv1D(filters = F2, kernel_size = f, activation='relu', strides = 1, padding = 'same')(X)\n    X = BatchNormalization()(X)\n\n    X = Conv1D(filters = F3, kernel_size = 1, activation='relu', strides = 1, padding = 'valid')(X)\n    X = BatchNormalization()(X)\n\n    X = Add()([X,X_shortcut])\n    X = Activation('relu')(X)\n    \n    return X","0da2b02f":"def convolutional_block(X, f, filters, s = 2):\n    F1, F2, F3 = filters\n    \n    X_shortcut = X\n\n    X = Conv1D(F1, 1, activation='relu', strides = s)(X)\n    X = BatchNormalization()(X)\n    \n    X = Conv1D(F2, f, activation='relu', strides = 1,padding = 'same')(X)\n    X = BatchNormalization()(X)\n\n    X = Conv1D(F3, 1, strides = 1)(X)\n    X = BatchNormalization()(X)\n\n    X_shortcut = Conv1D(F3, 1, strides = s)(X_shortcut)\n    X_shortcut = BatchNormalization()(X_shortcut)\n    \n    X = Add()([X,X_shortcut])\n    X = Activation('relu')(X)\n    \n    return X","664dd39f":"def ResNet50(input_shape = (187,1)):\n    \n    X_input = Input(input_shape)\n\n    X = ZeroPadding1D(3)(X_input)\n    \n    X = Conv1D(64, 7, activation='relu', strides = 2)(X)\n    X = BatchNormalization()(X)\n    X = MaxPool1D(pool_size=2, strides=2, padding='same')(X)\n\n    X = convolutional_block(X, f = 3, filters = [64, 64, 256], s = 1)\n    X = identity_block(X, 3, [64, 64, 256])\n    X = identity_block(X, 3, [64, 64, 256])\n\n    X = convolutional_block(X, f = 3, filters = [128,128,512], s = 2)\n    X = identity_block(X, 3, [128,128,512])\n    X = identity_block(X, 3, [128,128,512])\n    X = identity_block(X, 3, [128,128,512])\n\n    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], s = 2)\n    X = identity_block(X, 3, [256, 256, 1024])\n    X = identity_block(X, 3, [256, 256, 1024])\n    X = identity_block(X, 3, [256, 256, 1024])\n    X = identity_block(X, 3, [256, 256, 1024])\n    X = identity_block(X, 3, [256, 256, 1024])\n\n    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], s = 2)\n    X = identity_block(X, 3, [512, 512, 2048])\n    X = identity_block(X, 3, [512, 512, 2048])\n\n    X = MaxPool1D(pool_size=2, strides=2, padding='same')(X)\n    \n    X = Flatten()(X)\n    X = Dense(5,activation='softmax')(X)\n    \n    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n\n    return model","c8359e7a":"resNet50_model = ResNet50(input_shape = (187,1))","29e95c4c":"resNet50_model.summary()","6fd9fa7c":"plot_model(resNet50_model)","6350b0b0":"resNet50_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","d1f327d0":"resNet50_model_history = resNet50_model.fit(X_train, y_train, epochs = 20, batch_size = 100, validation_data = (X_test, y_test))","b8665a15":"plt.plot(resNet50_model_history.history['accuracy'])\nplt.plot(resNet50_model_history.history['val_accuracy'])\nplt.legend([\"accuracy\",\"val_accuracy\"])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')","f9ee1f1a":"plt.plot(resNet50_model_history.history['loss'])\nplt.plot(resNet50_model_history.history['val_loss'])\nplt.legend([\"loss\",\"val_loss\"])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')","9c2a4e68":"y_true=[]\nfor element in y_test:\n    y_true.append(np.argmax(element))\nprediction_proba=resNet50_model.predict(X_test)\nprediction=np.argmax(prediction_proba,axis=1)","0752faa0":"resNet50_model_cf_matrix = confusion_matrix(y_true, prediction)\nsns.heatmap(resNet50_model_cf_matrix\/np.sum(resNet50_model_cf_matrix), annot=True,fmt='.3%', cmap='Blues')","e5c4057d":"def inception_block(prev_layer):\n    \n    conv1=Conv1D(filters = 64, kernel_size = 1, activation='relu', padding = 'same')(prev_layer)\n    \n    conv3=Conv1D(filters = 64, kernel_size = 1, activation='relu', padding = 'same')(prev_layer)\n    conv3=Conv1D(filters = 64, kernel_size = 3, activation='relu', padding = 'same')(conv3)\n    \n    conv5=Conv1D(filters = 64, kernel_size = 1, activation='relu', padding = 'same')(prev_layer)\n    conv5=Conv1D(filters = 64, kernel_size = 5, activation='relu', padding = 'same')(conv5)\n    \n    pool= MaxPool1D(pool_size=3, strides=1, padding='same')(prev_layer)\n    convmax=Conv1D(filters = 64, kernel_size = 1, activation='relu', padding = 'same')(pool)\n    \n    layer_out = concatenate([conv1, conv3, conv5, convmax], axis=1)\n    return layer_out","0102304a":"def inception_model(input_shape):\n    X_input=Input(input_shape)\n    \n    X = Conv1D(filters = 64, kernel_size = 7, activation='relu', padding = 'same')(X_input)\n    X = MaxPool1D(pool_size=3, strides=2, padding='same')(X)\n    \n    X = Conv1D(filters = 64, kernel_size = 1, activation='relu', padding = 'same')(X)\n    \n    X = inception_block(X)\n    X = inception_block(X)\n    X = inception_block(X)\n    X = inception_block(X)\n    \n    X = MaxPool1D(pool_size=7, strides=2, padding='same')(X)\n    \n    X = Flatten()(X)\n    X = Dense(5,activation='softmax')(X)\n    \n    model = Model(inputs = X_input, outputs = X, name='Inception')\n    \n    return model","5bb4e218":"inception_model = inception_model(input_shape = (187,1))","91c07155":"inception_model.summary()","4d8d1ff9":"plot_model(inception_model)","737dab1a":"inception_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","264c6611":"inception_model_history = inception_model.fit(X_train, y_train, epochs = 1, batch_size = 100, validation_data = (X_test, y_test))","2d5bd7a5":"y_true=[]\nfor element in y_test:\n    y_true.append(np.argmax(element))\nprediction_proba=inception_model.predict(X_test)\nprediction=np.argmax(prediction_proba,axis=1)","2b6695fb":"inception_model_cf_matrix = confusion_matrix(y_true, prediction)\nsns.heatmap(inception_model_cf_matrix\/np.sum(inception_model_cf_matrix), annot=True,fmt='.3%', cmap='Blues')","e20f46f2":"lstm_model = Sequential()\nlstm_model.add(LSTM(64, input_shape=(187,1)))\nlstm_model.add(Dense(128, activation = 'relu'))\nlstm_model.add(Dropout(0.3))\nlstm_model.add(Dense(5, activation = 'softmax'))","04725094":"lstm_model.summary()","500be03e":"plot_model(lstm_model)","9fd9a35f":"lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","6fe1c601":"lstm_model_history = lstm_model.fit(X_train, y_train, epochs = 20, batch_size = 100, validation_data = (X_test, y_test))","45a2a28b":"plt.plot(lstm_model_history.history['accuracy'])\nplt.plot(lstm_model_history.history['val_accuracy'])\nplt.legend([\"accuracy\",\"val_accuracy\"])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')","66164873":"plt.plot(lstm_model_history.history['loss'])\nplt.plot(lstm_model_history.history['val_loss'])\nplt.legend([\"loss\",\"val_loss\"])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')","1ce6dca3":"y_true=[]\nfor element in y_test:\n    y_true.append(np.argmax(element))\nprediction_proba=lstm_model.predict(X_test)\nprediction=np.argmax(prediction_proba,axis=1)","2ab2231b":"lstm_model_cf_matrix = confusion_matrix(y_true, prediction)\nsns.heatmap(lstm_model_cf_matrix\/np.sum(lstm_model_cf_matrix), annot=True,fmt='.3%', cmap='Blues')","6309de40":"# VGG-16","2a46d0a6":"# LSTM","5bab909b":"# ResNet50","51115501":"# LENET-5","ccb5d898":"# CNN","efb36499":"# RNN","f4fdbc57":"# Inception","2c361820":"**This dataset contains 5 different classes.**\n\n    0) N means \"Normal beat\"\n    \n    1) S means \"Supraventricular premature beat\"\n    \n    2) V means \"Premature ventricular contraction\"\n    \n    3) F means \"Fusion of ventricular and normal beat\"\n    \n    4) Q means \"Unclassifiable beat\"","4640845f":"# AlexNet","252ad18a":"# Simple ANN"}}