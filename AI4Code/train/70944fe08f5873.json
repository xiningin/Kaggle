{"cell_type":{"0f67dcea":"code","2dba38ad":"code","3e35b2c3":"code","5ff3d24d":"code","4ac889f3":"code","a65a3494":"code","9bc2e321":"code","18b49066":"code","2d614b83":"code","31b07acc":"code","e53bdc07":"code","b3c5192b":"code","1e88ebcb":"code","903e0a76":"code","c40ac9ba":"code","4a3ec8cd":"code","9f02db92":"code","38e9da70":"code","19e7c720":"code","9460e32b":"code","6c58e046":"code","87f361a6":"code","2cd4895c":"code","373d02bd":"code","e521e003":"code","e3affc7e":"code","9c1b70f2":"code","36c9ec92":"markdown","b2b4a544":"markdown","ad1c0174":"markdown"},"source":{"0f67dcea":"import tensorflow as tf\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Analyse xml files in annotations e.g. web crawling\nfrom lxml import etree\nimport numpy as np\nimport glob\nfrom matplotlib.patches import Rectangle","2dba38ad":"image_path = glob.glob('..\/input\/oxfordiitpetdatasetfromciel\/Oxford-IIT-Pet\/images\/*.jpg')\nimage_path[:5] + image_path[-5:]","3e35b2c3":"len(image_path)","5ff3d24d":"xml_path = glob.glob('..\/input\/oxfordiitpetdatasetfromciel\/Oxford-IIT-Pet\/annotations\/xmls\/*.xml')","4ac889f3":"len(xml_path)","a65a3494":"xml_path[:3] + xml_path[-3:]","9bc2e321":"xml_names = [path.split('xmls\/')[1].split('.xml')[0] for path in xml_path]\nxml_names[:3]","18b49066":"img_train = ['..\/input\/oxfordiitpetdatasetfromciel\/Oxford-IIT-Pet\/images\/' + name + '.jpg' for name in xml_names]\nimg_train[:3]","2d614b83":"img_test = [path for path in image_path if path not in img_train]\nlen(img_test)","31b07acc":"def to_labels(path):\n    xml = open(path).read()\n    sel = etree.HTML(xml)\n    width = int(sel.xpath('\/\/size\/width\/text()')[0])\n    height = int(sel.xpath('\/\/size\/height\/text()')[0])\n    x_min = int(sel.xpath('\/\/bndbox\/xmin\/text()')[0])\n    x_max = int(sel.xpath('\/\/bndbox\/xmax\/text()')[0])\n    y_min = int(sel.xpath('\/\/bndbox\/ymin\/text()')[0])\n    y_max = int(sel.xpath('\/\/bndbox\/ymax\/text()')[0])\n    return [x_min\/width, y_min\/height, x_max\/width, y_max\/height]","e53bdc07":"labels = [to_labels(path) for path in xml_path]","b3c5192b":"labels[:3]","1e88ebcb":"out1, out2, out3, out4 = zip(*labels)\nout1[:5]","903e0a76":"label_ds = tf.data.Dataset.from_tensor_slices((np.array(out1), np.array(out2), np.array(out3), np.array(out4)))\nlabel_ds","c40ac9ba":"def load_img(path):\n    img = tf.io.read_file(path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, [224, 224])\n    img = img\/127.5 - 1\n    return img","4a3ec8cd":"img_ds = tf.data.Dataset.from_tensor_slices(img_train)\nimg_ds = img_ds.map(load_img)\nimg_ds","9f02db92":"ds = tf.data.Dataset.zip((img_ds, label_ds))\nds","38e9da70":"ds = ds.repeat().shuffle(3686).batch(32)\nds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)","19e7c720":"for img, label in ds.take(3):\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(img[0]))\n    out1, out2, out3, out4 = label\n    x_min, y_min, x_max, y_max = out1[0]*224, out2[0]*224, out3[0]*224, out4[0]*224\n    rect = Rectangle((x_min, y_min), (x_max - x_min), (y_max - y_min), fill=False, color='blue')\n    ax = plt.gca()\n    ax.axes.add_patch(rect)\n    plt.show()","9460e32b":"xception = tf.keras.applications.Xception(weights='imagenet',\n                                          include_top=False,\n                                          input_shape=(224, 224, 3))","6c58e046":"inputs = tf.keras.layers.Input(shape=(224, 224, 3))\nx = xception(inputs)\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\nx = tf.keras.layers.Dense(2048, activation='relu')(x)\nx = tf.keras.layers.Dense(256, activation='relu')(x)\n\nout1 = tf.keras.layers.Dense(1)(x)\nout2 = tf.keras.layers.Dense(1)(x)\nout3 = tf.keras.layers.Dense(1)(x)\nout4 = tf.keras.layers.Dense(1)(x)\n\nouts = [out1, out2, out3, out4]\n\nmodel = tf.keras.models.Model(inputs=inputs, outputs=outs)","87f361a6":"model.compile(tf.keras.optimizers.Adam(learning_rate=0.0001),\n              loss='mse', \n              metrics=['mae'])","2cd4895c":"test_ds = ds.take(640)\ntrain_ds = ds.skip(640)\nsteps_per_epoch = (3686-640)\/\/32\nvalidation_steps = 640\/\/32","373d02bd":"history = model.fit(train_ds, steps_per_epoch=steps_per_epoch, \n                    validation_data=test_ds, validation_steps=validation_steps,\n                    epochs=10)","e521e003":"loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(10)\n\nplt.figure()\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'bo', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","e3affc7e":"model.save('detect_target.h5')\nnew_model = tf.keras.models.load_model('detect_target.h5')","9c1b70f2":"plt.figure(figsize=(8, 24))\n\ntest_images = tf.data.Dataset.from_tensor_slices(img_test)\ntest_images = test_images.map(load_img)\ntest_images = test_images.batch(32)\n\nfor imgs in test_images.take(1):\n    out1, out2, out3, out4 = new_model.predict(imgs)\n    for i in range(3):\n        plt.subplot(3, 1, i+1)\n        plt.imshow(tf.keras.preprocessing.image.array_to_img(imgs[i]))\n        x_min, y_min, x_max, y_max = out1[i]*224, out2[i]*224, out3[i]*224, out4[i]*224\n        ax = plt.gca()\n        rect = Rectangle((x_min, y_min), (x_max - x_min), (y_max - y_min), fill=False, color='blue')\n        ax.axes.add_patch(rect)","36c9ec92":"## Create Model for Localization\n\nTreat localization as a regression problem!\n\nXception + Functional API\n\nVector 4096 --> Fully Connected (4096-4) --> Box Coord --> l2 loss","b2b4a544":"## Cons of Localizing Directly by Regression\n\n1. \u56de\u5f52\u4f4d\u7f6e\u4e0d\u7cbe\u786e\n2. \u6cdb\u5316\u80fd\u529b\u4e0d\u5f3a\n3. \u53ea\u80fd\u9884\u6d4b\u5355\u4e2a\u5b9e\u4f8b\n\n## Optimization of Localization\n\n1. \u5148\u5927\u540e\u5c0f\n2. \u6ed1\u52a8\u7a97\u53e3\n3. \u9488\u5bf9\u4e0d\u5b9a\u4e2a\u6570\u7684\u9884\u6d4b\u95ee\u9898\n4. \u4f7f\u7528\u5168\u5377\u79ef\u7f51\u7edc\uff0c\u53bb\u6389\u5168\u8fde\u63a5\u5c42\uff0c\u53d8\u56de\u5f52\u4e3a\u5206\u7c7b\u95ee\u9898\n\n## Evaluation of Model Prediction\n\n*IoU*: Intersection over Union\n\n## Applications\ne.g. \u4eba\u4f53\u59ff\u6001\u4f30\u8ba1","ad1c0174":"## Preprocessing"}}