{"cell_type":{"a996b088":"code","17ae242b":"code","33e0852b":"code","cc04533c":"code","5de98889":"code","dbfae78f":"code","321c95ff":"code","1c08698b":"code","94e201fc":"code","34b9cb76":"code","bec7fadd":"code","7ee24b84":"code","f8b469eb":"code","544d30dc":"code","beff28c3":"code","a4e5ef50":"code","802b26eb":"code","2a5d80ed":"code","699fd773":"code","44b71852":"code","5717c2d1":"code","9c8c1762":"code","fd4d0c15":"code","6feaf7b7":"code","df22bdac":"code","994957cb":"code","3435fe9d":"code","d600932f":"code","d88c8a47":"code","a86d2ecd":"code","030be997":"code","0a4963e4":"code","f5b7a020":"code","6f38a4e6":"markdown","3e9e994b":"markdown","27c56c5c":"markdown","0e1de4b5":"markdown","f8226155":"markdown","bd8755e6":"markdown","3fdadffe":"markdown","bbcc86a0":"markdown","e0953ea9":"markdown","3a885c4c":"markdown","1bb11faa":"markdown","a62f93c8":"markdown"},"source":{"a996b088":"import pandas as pd\nimport re\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.preprocessing import MinMaxScaler\nfrom gensim.models.keyedvectors import KeyedVectors\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import make_scorer","17ae242b":"df = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')","33e0852b":"df = df.fillna('Missing')","cc04533c":"%%time\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('stopwords')\n\ndef cleaner(excerpt):\n    clean = nltk.word_tokenize(re.sub(\"[^a-zA-Z]\", \" \", excerpt).lower())\n    clean = [word for word in clean if not word in set(stopwords.words(\"english\"))]\n\n    lem = nltk.WordNetLemmatizer()\n    clean = [lem.lemmatize(word) for word in clean]\n    return \" \".join(clean)\n\ndf.excerpt = df.excerpt.apply(cleaner)","5de98889":"%%time\nwdf = pd.read_csv('..\/input\/english-word-frequency\/unigram_freq.csv')\n\nwdf['ncol'] = wdf.word.apply(lambda x: True if (x not in set(stopwords.words(\"english\"))) else False)\nnwdf = wdf[wdf.ncol==True]\n\nlem1 = nltk.WordNetLemmatizer()\nnwdf['lword'] = nwdf.word.apply(lambda x: lem1.lemmatize(str(x)))\nnwdf = nwdf.sort_values('count')\n\nnwdf['scaled_count'] = nwdf['count'] \nword_freq = dict(zip(nwdf.word, nwdf.scaled_count))\n\ndef get_score(excerpt):\n    score = 0\n\n    for i in excerpt.split(' '):\n        try:\n            score += word_freq[i]\n        except KeyError:\n            pass\n\n    return score","dbfae78f":"df['ex_len'] = df.excerpt.apply(lambda x: len(x))","321c95ff":"print(df.ex_len.min(), df.ex_len.median())\ndf.excerpt = df.excerpt.apply(lambda x: x[0:586])","1c08698b":"df['excerpt_score'] = df.excerpt.apply(get_score)","94e201fc":"df.drop(['ex_len'], axis=1, inplace=True)","34b9cb76":"mms2 = MinMaxScaler()\n\ndf.excerpt_score = mms2.fit_transform(np.reshape(list(df.excerpt_score), (-1,1)))","bec7fadd":"y = df['target']\ndf.drop(['id', 'url_legal', 'license', 'target', 'standard_error'], axis=1, inplace=True)","7ee24b84":"df.excerpt_score = df.excerpt_score.apply(lambda x: np.round(x, 2))","f8b469eb":"df","544d30dc":"# If you are running the notebook on Colab then uncomment the code below to get the Word2Vec model\n\n# !sudo apt install wget\n# !wget -c \"https:\/\/s3.amazonaws.com\/dl4j-distribution\/GoogleNews-vectors-negative300.bin.gz\"\n# !gzip -d GoogleNews-vectors-negative300.bin.gz\n\npath = '..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin'\n\n\n\nword2vec_model = KeyedVectors.load_word2vec_format(path, binary=True)\nEMBEDDING_DIM=300\n\nprint(word2vec_model.vectors.shape)","beff28c3":"def avg_feature_vector(sentence, model, num_features):\n    words = sentence.split()\n    feature_vec = np.zeros((num_features,),dtype=\"float32\")\n    i=0\n    for word in words:\n        try:\n            feature_vec = np.add(feature_vec, model[word])\n        except KeyError as error:\n            feature_vec \n            i = i + 1\n    if len(words) > 0:\n        feature_vec = np.divide(feature_vec, len(words)- i)\n    return feature_vec\n\nword2vec_train = np.zeros((len(df.index),300),dtype=\"float32\")\n\nfor i in range(len(df.index)):\n    word2vec_train[i] = avg_feature_vector(df[\"excerpt\"][i],word2vec_model, 300)\n    \nprint(word2vec_train.shape)\nprint(y.shape)\n","a4e5ef50":"names_df = pd.DataFrame(data=word2vec_train)\ndf = pd.concat([df, names_df], axis=1)","802b26eb":"df.shape","2a5d80ed":"df.drop(['excerpt'], axis=1, inplace=True)","699fd773":"df","44b71852":"X = df","5717c2d1":"\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","9c8c1762":"from sklearn.metrics import mean_squared_error as mse\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nmse(y_test, y_pred)","fd4d0c15":"# mse = make_scorer(mean_squared_error,greater_is_better=False)\n\n# model = RandomForestRegressor()\n\n# params = {\n#               \"max_features\": [1, 3, 10],\n#               \"min_samples_split\": [2, 3, 10],\n#               \"min_samples_leaf\": [1, 3, 10],\n#               \"n_estimators\" :[100, 300, 500, 1000, 1500]}\n\n\n# model = GridSearchCV(model,param_grid = params, cv=3, scoring=mse, n_jobs= -1, verbose = 1)\n\n# model.fit(X_train,y_train)\n\n# model = model.best_estimator_","6feaf7b7":"# y_pred = model.predict(X_test)\n\n# from sklearn.metrics import mean_squared_error as mse\n# mse(y_test, y_pred)","df22bdac":"tdf = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n\ntdf = tdf.fillna('Missing')\n\n# tdf['ex_len'] = tdf.excerpt.apply(lambda x: len(x))\ntdf.excerpt = tdf.excerpt.apply(cleaner)\ntdf.excerpt = tdf.excerpt.apply(lambda x: x[:586])\ntdf['excerpt_score'] = tdf.excerpt.apply(get_score)\n\ntdf.drop(['id', 'url_legal', 'license'], axis=1, inplace=True)","994957cb":"tdf","3435fe9d":"word2vec_test = np.zeros((len(tdf.index),300),dtype=\"float32\")\n\nfor i in range(len(tdf.index)):\n    word2vec_test[i] = avg_feature_vector(tdf[\"excerpt\"][i],word2vec_model, 300) \n\nprint(word2vec_test.shape)","d600932f":"tdf.drop(['excerpt'], axis=1, inplace=True)","d88c8a47":"tdf.excerpt_score = mms2.transform(np.reshape(list(tdf.excerpt_score), (-1,1)))\n# tdf.ex_len = mms3.transform(np.reshape(list(tdf.ex_len), (-1,1)))","a86d2ecd":"names_df = pd.DataFrame(data=word2vec_test)\ntdf = pd.concat([tdf, names_df], axis=1)\n\ntdf.shape","030be997":"ypred = model.predict(tdf)","0a4963e4":"ypred","f5b7a020":"submission = pd.DataFrame({ 'id' : pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')['id'], \n                           'target': list(ypred)})\n\nsubmission.to_csv('submission.csv', index=False)\n","6f38a4e6":"We will use pre-trained Word2Vec.","3e9e994b":"If you find this notebook helpful, please <b>UPVOTE<\/b>","27c56c5c":"So we will be using the word vectors to encode all paragraphs and will also keep two more columns we created.","0e1de4b5":"We must do the same with test data, that is create two columns and use Word2Vec on the text data.","f8226155":"The missing values are in url_legal and license columns, which we will drop anyways, so we can fill them with anything.","bd8755e6":"Now I am going to create one extra feature by using another freely available dataset. The dataset gives us usage frequency of english words. \n\nSo if we sum up usage frequency of all words, we end up with a score of the paragraph. This score will be more if the words in the paragraph are used more in daily life and the score will be less if they are not used often. \n\nWhen words are not used often, they surely can be hard for people to understand. \n\nThere can be a better way to use this word frequency but I am going forward with this basic approach for now.","3fdadffe":"### Test Data - Creating Submission","bbcc86a0":"Scaling should help us get better results.","e0953ea9":"We keep this score as excerpt_score and we will create another feature to take into account how long the paragraphs are, that feature is ex_len.","3a885c4c":"Now we must try to get rid of anything which is not an alphabet, anything which is a stopword and then lemmatize the words.","1bb11faa":"Now it is time to let go of the text column.","a62f93c8":"We will be dropping some columns."}}