{"cell_type":{"4937b0c6":"code","0ff715c8":"code","eb8e4c93":"code","d315e60d":"code","3f031e8b":"code","a695d392":"code","b814861e":"code","74bac920":"code","a1327162":"code","eb70877d":"code","f23f7e05":"code","7e942c70":"code","11901016":"code","d76b4d08":"code","82d63e5a":"code","0082d78a":"code","5fedd221":"code","593aacb7":"code","c113ea89":"code","2e7b2eaa":"code","dccc90ad":"markdown","22de0695":"markdown","ec5bea0d":"markdown"},"source":{"4937b0c6":"import numpy as np \nimport pandas as pd\nimport datetime\nimport random\nimport glob\nimport cv2\nimport os\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import BatchNormalization, Activation, Dropout, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import BatchNormalization,Activation,Dropout,Dense,concatenate,Input\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.layers import Flatten, Conv2D, MaxPooling2D,GlobalAveragePooling2D\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback, ReduceLROnPlateau\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Input\nfrom tensorflow.keras import regularizers\nfrom keras.applications.resnet50 import ResNet50\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom category_encoders import OrdinalEncoder, OneHotEncoder, TargetEncoder\nfrom tensorflow.keras.applications import VGG16, VGG19\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n# \u4e71\u6570\u30b7\u30fc\u30c9\u56fa\u5b9a\nseed_everything(2020)\n\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","0ff715c8":"train = pd.read_csv('\/kaggle\/input\/5th-datarobot-ai-academy-deep-learning\/train.csv')\ntrain = train.sort_values('id')\ntest = pd.read_csv('\/kaggle\/input\/5th-datarobot-ai-academy-deep-learning\/test.csv')\ntest = test.sort_values('id')\n#display(train.shape), display(test.shape)\n#display(train.head()), display(test.head())","eb8e4c93":"#\u63d0\u51fa\u30d5\u30a1\u30a4\u30eb\nsubmission = pd.read_csv('\/kaggle\/input\/5th-datarobot-ai-academy-deep-learning\/sample_submission.csv', index_col=0)\n#display(submission.head())","d315e60d":"cols=['bedrooms','bathrooms','area','zipcode']\ntarget=['price']","3f031e8b":"#\u6b20\u6e2c\u5024\ntrain[cols]=train[cols].fillna(-99)\ntest[cols] = test[cols].fillna(-99)","a695d392":"# area\ntrain['area']=train['area'].astype(str).str[:1]\ntest['area']=test['area'].astype(str).str[:1]\n\ntemp = train.groupby(['area'],as_index=False)[target].mean()\ntemp = temp.rename(columns={'price': 'area_price'})\n\ntrain_1 = pd.merge(train, temp, on='area', how='left')\ntrain_1 = train_1.drop('area', axis=1)\ntrain = train_1.rename(columns={'area_price': 'area'})\n\ntest_1 = pd.merge(test, temp, on='area', how='left')\ntest_1 = test_1.drop('area', axis=1)\ntest = test_1.rename(columns={'area_price': 'area'})\n\n#display(train.head())\n#display(test.head())","b814861e":"# zipcode\ntrain['zipcode']=train['zipcode'].astype(str).str[:1]\ntest['zipcode']=test['zipcode'].astype(str).str[:1]\n\ntemp = train.groupby(['zipcode'],as_index=False)[target].mean()\ntemp = temp.rename(columns={'price': 'zipcode_price'})\n\ntrain_1 = pd.merge(train, temp, on='zipcode', how='left')\ntrain_1 = train_1.drop('zipcode', axis=1)\ntrain = train_1.rename(columns={'zipcode_price': 'zipcode'})\n\ntest_1 = pd.merge(test, temp, on='zipcode', how='left')\ntest_1 = test_1.drop('zipcode', axis=1)\ntest = test_1.rename(columns={'zipcode_price': 'zipcode'})\n\n#display(train.head()), display(test.head())","74bac920":"train[cols]=train[cols].fillna(-99)\ntest[cols] = test[cols].fillna(-99)\n\n# \u6b63\u898f\u5316\nscaler = StandardScaler()\n#X_all = pd.concat([train, test], axis=0)\ntrain[cols] = scaler.fit_transform(train[cols])\ntest[cols] = scaler.fit_transform(test[cols])\n#train = X_all.iloc[:train.shape[0], :]\n#test = X_all.iloc[train.shape[0]:, :]\n#display(train.head())\n#display(test.head())","a1327162":"#\u6b20\u6e2c\u5024\ntrain[cols]=train[cols].fillna(-99)\ntest[cols] = test[cols].fillna(-99)","eb70877d":"#\u753b\u50cf\u3092\u7d50\u5408\u3057\u3066\u8aad\u307f\u8fbc\u307f\ndef load_images_unit(df,inputPath,size):\n    images = []\n    for i in df['id']:\n        basePath0 = os.path.sep.join([inputPath, \"{}_{}*\".format(i,'bathroom')])\n        basePath1 = os.path.sep.join([inputPath, \"{}_{}*\".format(i,'bedroom')])\n        basePath2 = os.path.sep.join([inputPath, \"{}_{}*\".format(i,'frontal')])\n        basePath3 = os.path.sep.join([inputPath, \"{}_{}*\".format(i,'kitchen')])\n        housePaths0 = sorted(list(glob.glob(basePath0)))\n        housePaths1 = sorted(list(glob.glob(basePath1)))\n        housePaths2 = sorted(list(glob.glob(basePath2)))\n        housePaths3 = sorted(list(glob.glob(basePath3)))\n        for housePath in housePaths3:\n            image0 = cv2.imread(housePaths0[0])\n            image1 = cv2.imread(housePaths1[0])\n            image2 = cv2.imread(housePaths2[0])\n            image3 = cv2.imread(housePaths3[0])\n            image0 = cv2.cvtColor(image0, cv2.COLOR_BGR2RGB)\n            image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n            image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n            image3 = cv2.cvtColor(image3, cv2.COLOR_BGR2RGB)\n            image0 = cv2.resize(image0, (size, size))\n            image1 = cv2.resize(image1, (size, size))\n            image2 = cv2.resize(image2, (size, size))\n            image3 = cv2.resize(image3, (size, size))\n            image_h0 = cv2.hconcat([image0, image1])\n            image_h1 = cv2.hconcat([image2, image3])\n            image = cv2.vconcat([image_h0, image_h1])\n        images.append(image)\n    return np.array(images) \/ 255.0\n\n# load train images\ninputPath = '\/kaggle\/input\/5th-datarobot-ai-academy-deep-learning\/images\/train_images\/'\ninputPath2 = '\/kaggle\/input\/5th-datarobot-ai-academy-deep-learning\/images\/test_images\/'\nsize = 32\ntrain_images = load_images_unit(train,inputPath,size)\ntest_images = load_images_unit(test,inputPath2,size)\n#display(train_images.shape)\n#display(train_images[0][0][0])","f23f7e05":"#\u753b\u50cf\u3092\u305d\u308c\u305e\u308c\u8aad\u307f\u8fbc\u307f\ndef load_images(df,inputPath,size,roomType):\n    images = []\n    for i in df['id']:\n        basePath = os.path.sep.join([inputPath, \"{}_{}*\".format(i,roomType)])\n        housePaths = sorted(list(glob.glob(basePath)))\n        for housePath in housePaths:\n            image = cv2.imread(housePath)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            image = cv2.resize(image, (size, size))\n        images.append(image)\n    return np.array(images) \/ 255.0\n\n# load train images\ninputPath = '\/kaggle\/input\/5th-datarobot-ai-academy-deep-learning\/images\/train_images\/'\ninputPath2 = '\/kaggle\/input\/5th-datarobot-ai-academy-deep-learning\/images\/test_images\/'\nsize = 64\nroomType = 'bathroom'\ntrain_bathroom = load_images(train,inputPath,size,roomType)\ntest_bathroom = load_images(test,inputPath2,size,roomType)\nroomType = 'bedroom'\ntrain_bedroom = load_images(train,inputPath,size,roomType)\ntest_bedroom = load_images(test,inputPath2,size,roomType)\nroomType = 'frontal'\ntrain_frontal = load_images(train,inputPath,size,roomType)\ntest_frontal = load_images(test,inputPath2,size,roomType)\nroomType = 'kitchen'\ntrain_kitchen = load_images(train,inputPath,size,roomType)\ntest_kitchen = load_images(test,inputPath2,size,roomType)\n#display(train_images.shape)\n#display(train_images[0][0][0])","7e942c70":"def create_model(inputShape ,TableShape):    \n    \n    backbone = VGG16(weights='imagenet',\n                     include_top=False,\n                     input_shape=inputShape)\n    \n    for layer in backbone.layers[:15]:\n        layer.trainable = False\n\n    vgg = Sequential(layers=backbone.layers)   \n    \n    vgg.add(GlobalAveragePooling2D())\n    vgg.add(Dense(units=256, activation='relu',kernel_initializer='he_normal')) \n    vgg.add(Dropout(0.2))\n    vgg.add(Dense(units=32, activation='relu',kernel_initializer='he_normal'))    \n    vgg.add(Dense(units=1, activation='linear'))    \n    \n\n    L1_L2 = regularizers.l1_l2(l1=0.005,l2=0.005)\n    input_df=Input(shape=(len(TableShape),))\n    \n    nn = Sequential()\n    \n    nn = Dense(units=256,input_shape=(TableShape,),kernel_initializer='he_normal',activation='relu')(input_df)\n    nn = BatchNormalization()(nn)\n    \n    nn = Dense(units=128,kernel_initializer='he_normal',activation='relu')(nn)\n    nn = Dropout(0.2)(nn)\n    \n    nn = Dense(units=64,kernel_initializer='he_normal',activation='relu')(nn)\n    nn = Dropout(0.2)(nn)\n    \n    nn = Dense(units=32,kernel_initializer='he_normal',activation='relu',kernel_regularizer=L1_L2)(nn)\n    nn = Dropout(0.2)(nn)\n    \n    nn = Model(inputs=input_df, outputs=nn)\n\n\n    merge = concatenate([vgg.output, nn.output])\n#     merge = concatenate([vgg.output, cnn.output, nn.output])\n    \n    mm = Dense(units=256, activation='relu',kernel_initializer='he_normal')(merge)\n    mm = Dropout(0.2)(mm)\n    mm = Dense(units=32, activation='relu',kernel_initializer='he_normal',kernel_regularizer=L1_L2)(mm)\n    mm = Dropout(0.2)(mm)\n    mm = Dense(units=1, activation='linear')(mm)\n    model = Model(inputs=[vgg.input,nn.input],outputs=mm) \n    model.compile(loss='mape', optimizer='adam', metrics=['mape'])\n    \n    return model","11901016":"train_x = train.drop(['price', 'id'], axis=1)\ntrain_y = train.price\ntest_x = test.drop(['id'], axis=1)","d76b4d08":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n#CV\nscores = []\n\nkf = KFold(n_splits=3, shuffle=True)\n\nfor i, (train_ix, valid_ix) in enumerate(kf.split(train_x)):\n\n    train_x_, train_y_ = train_x.iloc[train_ix].values, train_y.iloc[train_ix].values\n    valid_x_, valid_y_ = train_x.iloc[valid_ix].values, train_y.iloc[valid_ix].values\n    \n    train_image_x_ = train_images[train_ix]\n    valid_image_x_ = train_images[valid_ix]\n    \n    # callback parameter\n    filepath = \"cnn_best_model_CV\"+str(i)+\".hdf5\" \n    es = EarlyStopping(patience=5, mode='min', verbose=1) \n    checkpoint = ModelCheckpoint(monitor='val_loss', filepath=filepath, save_best_only=True, mode='auto') \n    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss',  patience=5, verbose=1,  mode='min')\n\n    # \u8a13\u7df4\u5b9f\u884c\n    inputShape = (size, size, 3)\n    tableShape = cols\n    \n    model = create_model(inputShape,tableShape)\n    history = model.fit([train_image_x_, train_x_],train_y_, \n                    validation_data=([valid_image_x_, valid_x_],valid_y_),\n                    epochs=100, batch_size=32,callbacks=[es, checkpoint, reduce_lr_loss])\n    \n    # load best model weights\n    if os.path.exists(filepath):\n        model.load_weights(filepath)\n\n    # \u8a55\u4fa1\n    valid_pred = model.predict([valid_image_x_, valid_x_],batch_size=5).reshape((-1,1))\n    mape_score = mean_absolute_percentage_error(valid_y_, valid_pred)","82d63e5a":"loss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(loss))\nplt.plot(epochs, loss, 'bo' ,label = 'training loss')\nplt.plot(epochs, val_loss, 'b' , label= 'validation loss')\nplt.title('Training and Validation loss')\nplt.legend()\nplt.show()","0082d78a":"plot_model(model, to_file='cnn.png')","5fedd221":"model.summary()","593aacb7":"print (mape_score)","c113ea89":"price1 = model.predict([test_images,test_x], batch_size=16).reshape((-1,1))\n#price1","2e7b2eaa":"submission.price = price1 \nsubmission.to_csv('.\/submission.csv')\nsubmission.head()","dccc90ad":"# \u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u30a4\u30f3\u30dd\u30fc\u30c8","22de0695":"# \u753b\u50cf\u51e6\u7406","ec5bea0d":"# \u6570\u5024\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f"}}