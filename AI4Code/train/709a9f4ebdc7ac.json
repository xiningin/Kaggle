{"cell_type":{"605d579c":"code","4bec60ed":"code","7fe4e8cf":"code","50c69e29":"code","936e7e78":"code","19cd076e":"code","b303da1f":"code","561577ed":"code","4b1083f7":"code","e255a218":"code","6ff96b14":"code","74fdc84c":"code","419c28f7":"code","ebdc15ce":"code","4d2f7b0c":"code","9ca190ef":"code","7a2e8262":"code","6ff39cf0":"code","b44705d9":"code","54d02b58":"code","7084b247":"code","b1f60ab1":"code","770a9ef7":"code","33981e42":"code","fb34b94b":"code","96ea05db":"code","47b7143b":"code","3da79a15":"code","885a9a1c":"code","5bea6bf6":"code","05e9488d":"code","1bcdb995":"code","b97cef36":"code","1c88ea34":"code","8f6e6536":"code","9877fe2c":"code","d8b8bc88":"code","eeb3254d":"code","53c3169f":"code","ec567c3f":"code","0bcb3118":"code","15a30726":"code","491fde45":"code","5b357e6b":"code","55842107":"code","9d66dd97":"code","94d3430a":"code","0745d7f9":"code","83e4d9cd":"code","6515c9bd":"code","ee913fbd":"code","f1cc580d":"code","625d6fbb":"code","818395fb":"code","a8a5f131":"code","5f10ae49":"code","a447541c":"code","f26e83a9":"markdown","c7fbaf31":"markdown","2307e509":"markdown","de357a2a":"markdown","f011f9ac":"markdown","a560a834":"markdown","0edb60db":"markdown","73ee1140":"markdown","c7920578":"markdown"},"source":{"605d579c":"import numpy as np\nimport pandas as pd\n#from fancyimpute import KNN\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import roc_auc_score,accuracy_score\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n# from imblearn.over_sampling import SMOTE\n# from sklearn.linear_model import LogisticRegression","4bec60ed":"\n# Use scikit-learn to grid search the batch size and epochs\nimport numpy\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.models import Sequential\nfrom keras.layers import Dense , Input , Dropout\nfrom keras.wrappers.scikit_learn import KerasRegressor","7fe4e8cf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","50c69e29":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error","936e7e78":"import os\nprint((os.listdir('..\/input\/')))","19cd076e":"df_train = pd.read_csv('..\/input\/wecrec2020\/Train_data.csv')\ndf_train.head()","b303da1f":"df_test = pd.read_csv('..\/input\/wecrec2020\/Test_data.csv')\ndf_test.head()","561577ed":"df_train.describe()","4b1083f7":"cat_feat=[\"F3\",\"F4\",\"F5\",\"F7\",\"F8\",\"F9\",\"F11\",\"F12\"]","e255a218":"test_index=df_test['Unnamed: 0']","6ff96b14":"\ndf_train.drop(['Unnamed: 0','F1', 'F2'], axis = 1, inplace = True)\n","74fdc84c":"df_test.drop(['Unnamed: 0','F1', 'F2'], axis = 1, inplace = True)","419c28f7":"df_test.nunique()","ebdc15ce":"df_train.nunique()","4d2f7b0c":"(df_test.isnull().sum()).sum()","9ca190ef":"#13 and 15 too much corr","7a2e8262":"Y_train=df_train['O\/P']\nY_train=Y_train.to_numpy()","6ff39cf0":"X_train=df_train.loc[:, df_train.columns != 'O\/P']\nX_test=df_test.loc[:,df_test.columns!='O\/P']","b44705d9":"X_train.dtypes","54d02b58":"# X_train[\"F17\"]=X_train[\"F17\"].astype(str)\n# X_test[\"F17\"]=X_test[\"F17\"].astype(str)","7084b247":"from catboost import CatBoostRegressor,Pool","b1f60ab1":"x=list(df_train.columns)\nnum_feat=[]\nfor k in x:\n    if k in cat_feat or k=='O\/P':\n        continue\n    else:\n        num_feat.append(k)\nnum_feat","770a9ef7":"X_trg_norm=(X_train-X_train.min())\/(X_train.max()-X_train.min())\nX_tsg_norm=(X_test-X_test.min())\/(X_test.max()-X_test.min())\nXtrnorm=X_train\nXtsnorm=X_test\nXtrnorm[num_feat]=X_trg_norm[num_feat]\nXtsnorm[num_feat]=X_tsg_norm[num_feat]\nXtrnorm.describe()","33981e42":"Xtsnorm.describe()","fb34b94b":"cbdat=Pool(data=X_train,label=Y_train,cat_features=cat_feat)\ncbr=CatBoostRegressor()","96ea05db":"#39.008-->f10 and f17--->lb 0.89\n#38.88-->f10-->lb 0.7\n#38.5somethign-->no normal all feat-->lb 0.67\n#38.48 2000 trees\n","47b7143b":"grid = {'learning_rate': [0.1,0.07,0.12],\n        'depth': [9],\n        'l2_leaf_reg': [2],\n       'iterations':[1500,1700,2000,2200,2500]}\n\ngrid_search_result = cbr.grid_search(grid, \n                                       X=cbdat,  \n                                       plot=True)","3da79a15":"cbr=CatBoostRegressor(depth=9,learning_rate=0.1,l2_leaf_reg=2,iterations=2000)\ncbr.fit(cbdat)","885a9a1c":"res=cbr.predict(X_test)","5bea6bf6":"combo=pd.concat(objs=[X_train,X_test])\ncombo.describe()","05e9488d":"combo.nunique()","1bcdb995":"combo=pd.get_dummies(data=combo,columns=[\"F3\",\"F4\",\"F5\",\"F7\",\"F8\",\"F9\",\"F11\",\"F12\"],dummy_na=True,drop_first=True)","b97cef36":"combo.head()","1c88ea34":"X_train_dummy=pd.DataFrame(data=combo[0:Y_train.shape[0]])\nX_train_dummy.describe()","8f6e6536":"X_test_dummy=pd.DataFrame(data=combo[Y_train.shape[0]:])\nX_test_dummy.describe()","9877fe2c":"X_train_normalized_full = scale(X_train_dummy)\nX_test_normalized=scale(X_test_dummy)","d8b8bc88":"X_train_normalized_full.shape\nXx=X_train_normalized_full.T.T\nXx.shape","eeb3254d":"from keras.constraints import maxnorm","53c3169f":"def create_model(wextra,wwextra,xtra,kk,k=0.5,wc=1):\n# create model\n    \n    \n    model = Sequential()\n    model.add(Input(shape=(64,)))\n    model.add(Dense(xtra,kernel_initializer='glorot_uniform', activation='relu', kernel_constraint=maxnorm(wc)))\n    model.add(Dropout(k))\n    model.add(Dense(wextra,kernel_initializer='glorot_uniform', activation='relu',kernel_constraint=maxnorm(wc)))\n    model.add(Dropout(kk))\n    model.add(Dense(wwextra,kernel_initializer='glorot_uniform', activation='relu'))\n    model.add(Dense(1, kernel_initializer='glorot_uniform', activation='linear'))\n    # Compile model\n    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy','mean_squared_error'])\n    return model","ec567c3f":"seed = 7\nnumpy.random.seed(seed)\nmodel = KerasRegressor(build_fn=create_model, epochs=20, batch_size=200, verbose=5)\nweight_constraint = [5]\ndropout_rate = [0.1,0.2,0.3]\n# dropout_rate2 = [0.1,0.2,0.3]\nx=[40,35,30,25,20,15,10]\n# z=[10,11,12]\ny=[3,4,5,6,7,8]\nparam_grid = dict(k=dropout_rate, wc=weight_constraint,xtra=x,wextra=y)#,wwextra=z,kk=dropout_rate2)\nprint(param_grid)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3,verbose=10,scoring='neg_root_mean_squared_error')\ngrid_result = grid.fit(Xx, Y_train)","0bcb3118":"grid_result.best_score_","15a30726":"seed = 7\nnumpy.random.seed(seed)\nmodel = KerasRegressor(build_fn=create_model, epochs=100, batch_size=200, verbose=5)\nweight_constraint = [5]\ndropout_rate = [0.1]\ndropout_rate2 = [0.1]\nx=[40]\nz=[12]\ny=[5]\nparam_grid = dict(k=dropout_rate, wc=weight_constraint,xtra=x,wextra=y,wwextra=z,kk=dropout_rate2)\nprint(param_grid)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5,verbose=10,scoring='neg_root_mean_squared_error')\ngrid_result = grid.fit(Xx, Y_train)","491fde45":"X_train, X_test, y_train, y_test = train_test_split(X_train_normalized,Y_train, test_size=0.2, random_state=42)","5b357e6b":"import lightgbm as lgb","55842107":"lgbm = lgb.LGBMRegressor()\nxgb=XGBRegressor()","9d66dd97":"parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n              'objective':['reg:linear'],\n              'learning_rate': [0.1], #so called `eta` value\n              'max_depth': [5,6],\n              'subsample': [0.7,0.5],\n              'colsample_bytree': [0.7,0.5],\n              'n_estimators': [1500,1000,2000,2500]}\nxgb_grid = GridSearchCV(xgb,\n                        parameters,\n                        cv = 3,\n                        n_jobs = -1,\n                        verbose=10,scoring='neg_root_mean_squared_error')\n\nxgb_grid.fit(X_train_normalized_full,\n         Y_train)\n\n","94d3430a":"# -75.319","0745d7f9":"params={'colsample_bytree': 0.5,\n 'learning_rate': 0.1,\n 'max_depth': 5,\n 'n_estimators': 1500,\n 'nthread': 4,\n 'objective': 'reg:linear',\n 'subsample': 0.7}","83e4d9cd":"xgb.fit(X_train_normalized_full,Y_train,**params)\n","6515c9bd":"xgb_grid.best_score_","ee913fbd":"xgb_grid.best_params_","f1cc580d":"df_test = df_test.loc[:, 'F3':'F17']\npred = res","625d6fbb":"#contains old\npred2=pred","818395fb":"print(pred2)","a8a5f131":"print(pred)","5f10ae49":"result=pd.DataFrame()\nresult['Id'] = test_index\nresult['PredictedValue'] = pd.DataFrame(pred)\nresult.head()","a447541c":"result.to_csv('output_2000.csv', index=False)\n","f26e83a9":"## Packing it into output file","c7fbaf31":"## Visualizing input data","2307e509":"# The code is messy but the key takeaways i got from this competition are:\n* -->CatBoost consistently outperformed other algs(xgb,lgbm,rf,mlp)\n* -->Feature selection using Boruta dint improve results\n* -->GridSearch and CV was crucial \n* -->Normalisation didnt affect performance much\n* -->Removing least important features made score worse suggesting very minimal noise in dataset?(not sure)\n\n\n# Feel free to comment any other insights you guys had or anything i got wrong","de357a2a":"## Initializing a regression model","f011f9ac":"## Separating the input and output fields","a560a834":"### Dropping F1 as it is just index and F2 is datestamp\n","0edb60db":"## Fitting the classifier","73ee1140":"## Reading input data","c7920578":"## Predicting output for test set"}}