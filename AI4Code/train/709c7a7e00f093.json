{"cell_type":{"9d4e5a67":"code","7b7fa0a5":"code","329f6b14":"code","65afa2c2":"code","e8885d4d":"code","156215a9":"code","4086cac7":"code","08dc3f37":"code","b8f2c0d5":"code","451cd911":"code","bac68453":"code","05d44fb5":"code","23f7a7c8":"code","2ce440f9":"code","a4935b88":"code","6faca4a7":"code","a7536d2c":"code","99b52205":"code","caaf7124":"code","b11eb4cc":"markdown","2860b1b5":"markdown","2c50dc84":"markdown","164e14ad":"markdown","70e8aa3f":"markdown","22f6448d":"markdown","d15387c2":"markdown","2fd91554":"markdown","14c4b044":"markdown","6e4ffc38":"markdown"},"source":{"9d4e5a67":"import numpy as np\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom nltk.corpus import stopwords \nfrom collections import Counter\nimport string\nimport re\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.model_selection import train_test_split","7b7fa0a5":"is_cuda = torch.cuda.is_available()\n\n# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\nif is_cuda:\n    device = torch.device('cuda')\n    \nelse:\n    device = torch.device('cpu')\n    print(\"GPU not available, CPU used\")","329f6b14":"base_csv = '\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv'\ndf = pd.read_csv(base_csv)\ndf.head()","65afa2c2":"X, y = df['review'].values, df['sentiment'].values\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y)\nprint(f'shape of train data is {x_train.shape}')\nprint(f'shape of test data is {x_test.shape}')","e8885d4d":"dd = pd.Series(y_train).value_counts()\nsns.barplot(x=np.array(['negative', 'positive']), y=dd.values)\nplt.show()","156215a9":"def preprocess_string(s):\n    # Remove all non-word characters (everything except numbers and letters)\n    s = re.sub(r\"[^\\w\\s]\", '', s)\n    # Replace all runs of whitespaces with no space\n    s = re.sub(r\"\\s+\", '', s)\n    # replace digits with no space\n    s = re.sub(r\"\\d\", '', s)\n    return s\n\n\ndef tokenize(x_train, y_train, x_val, y_val):\n    word_list = []\n\n    stop_words = set(stopwords.words('english')) \n    \n    for sent in x_train:\n        for word in sent.lower().split():\n            word = preprocess_string(word)\n            if word not in stop_words and word != '':\n                word_list.append(word)\n                \n    corpus = Counter(word_list)\n    print(corpus.get)\n    # sorting on the basis of most common words\n    corpus_ = sorted(corpus, key=corpus.get, reverse=True)[:1000]\n    \n    # creating a dict\n    onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}\n    \n    # tokenize\n    final_list_train,final_list_test = [],[]\n    \n    for sent in x_train:\n            final_list_train.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n                                     if preprocess_string(word) in onehot_dict.keys()])\n    for sent in x_val:\n            final_list_test.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n                                    if preprocess_string(word) in onehot_dict.keys()])\n            \n    encoded_train = [1 if label =='positive' else 0 for label in y_train]  \n    encoded_test = [1 if label =='positive' else 0 for label in y_val] \n    return np.array(final_list_train), np.array(encoded_train),np.array(final_list_test), np.array(encoded_test),onehot_dict","4086cac7":"x_train, y_train, x_test, y_test, vocab = tokenize(x_train, y_train, x_test, y_test)","08dc3f37":"print(f'Length of vocabulary is {len(vocab)}')","b8f2c0d5":"rev_length = [len(i) for i in x_train]\npd.Series(rev_length).hist()\nplt.show()\n\npd.Series(rev_length).describe()","451cd911":"def padding_(sentences, seq_len):\n    features = np.zeros((len(sentences), seq_len),dtype=int)\n    for ii, review in enumerate(sentences):\n        if len(review)!=0:\n            features[ii, -len(review):] = np.array(review)[:seq_len]\n        \n    return features","bac68453":"#we have very less number of reviews with length > 500.\n#So we will consideronly those below it.\nx_train_pad = padding_(x_train,500)\nx_test_pad = padding_(x_test,500)","05d44fb5":"# create Tensor datasets\n\ntrain_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\nvalid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n\n# dataloaders\nbatch_size = 50\n\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\nvalid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)","23f7a7c8":"# obtain one batch of training data\ndataiter = iter(train_loader)\nsample_x, sample_y = dataiter.next()\n\nprint('Sample input size: ', sample_x.size()) # batch_size, seq_length\nprint('Sample input: \\n', sample_x)\nprint('Sample input: \\n', sample_y)","2ce440f9":"class SentimentRNN(nn.Module):\n    def __init__(self, no_layers, vocab_size, hidden_dim, embedding_dim, drop_prob=0.5):\n        super(SentimentRNN, self).__init__()\n        \n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n        \n        self.no_layers = no_layers\n        self.vocab_size = vocab_size\n        \n        # embedding and LSTM layers\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        \n        # LSTM\n        self.lstm = nn.LSTM(input_size=embedding_dim, \n                           hidden_size=self.hidden_dim,\n                           num_layers=no_layers,\n                           batch_first=True)\n        \n        # dropout layer\n        self.dropout = nn.Dropout(0.3)\n        \n        # linear and sigmoid layer\n        self.fc = nn.Linear(self.hidden_dim, output_dim)\n        self.sig = nn.Sigmoid()\n        \n    def forward(self, x, hidden):\n        batch_size = x.size(0)\n        \n        # embedding and lstm_out\n        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n        #print(embeds.shape)  #[50, 500, 1000]\n        \n        lstm_out, hidden = self.lstm(embeds, hidden)\n        \n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n         # dropout and fully connected layer\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        \n        # sigmoid function\n        sig_out = self.sig(out)\n        \n        # reshape to be batch_size first\n        sig_out = sig_out.view(batch_size, -1)\n        \n        sig_out = sig_out[:, -1] # get last batch of labels\n        \n        # return last sigmoid output and hidden state\n        return sig_out, hidden\n    \n    def init_hidden(self, batch_size):\n        h0 = torch.zeros((self.no_layers, batch_size, self.hidden_dim)).to(device)\n        c0 = torch.zeros((self.no_layers, batch_size, self.hidden_dim)).to(device)\n        \n        hidden = (h0, c0)\n        return hidden","a4935b88":"no_layers = 2\nvocab_size = len(vocab) + 1 #extra 1 for padding\nembedding_dim = 64\noutput_dim = 1\nhidden_dim = 256\n\nmodel = SentimentRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n\n\n#moving to gpu\nmodel.to(device)\n\nprint(model)","6faca4a7":"\n\n# loss and optimization functions\nlr=0.001\n\ncriterion = nn.BCELoss()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n# function to predict accuracy\ndef acc(pred, label):\n    pred = torch.round(pred.squeeze())\n    return torch.sum(pred==label.squeeze()).item()","a7536d2c":"clip = 5\nepochs = 5 \nvalid_loss_min = np.Inf\n# train for some number of epochs\nepoch_tr_loss,epoch_vl_loss = [],[]\nepoch_tr_acc,epoch_vl_acc = [],[]\n\nfor epoch in range(epochs):\n    train_losses = []\n    train_acc = 0.0\n    \n    model.train()\n    \n    # initialize hidden state\n    h = model.init_hidden(batch_size)\n    \n    for inputs, labels in train_loader:\n        \n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n        \n        model.zero_grad()\n        \n        output, h = model(inputs, h)\n        \n        # calculate the loss and perform backprop\n        loss = criterion(output.squeeze(), labels.float())\n        \n        loss.backward()\n        train_losses.append(loss.item())\n        \n        # calculating accuracy\n        accuracy = acc(output,labels)\n        train_acc += accuracy\n        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs \/ LSTMs.\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        \n    val_h = model.init_hidden(batch_size)\n    val_losses = []\n    val_acc = 0.0\n    \n    model.eval()\n    \n    for inputs, labels in valid_loader:\n        val_h = tuple([each.data for each in val_h])\n        \n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        output, val_h = model(inputs, val_h)\n        val_loss = criterion(output.squeeze(), labels.float())\n\n        val_losses.append(val_loss.item())\n\n        accuracy = acc(output,labels)\n        val_acc += accuracy\n            \n    epoch_train_loss = np.mean(train_losses)\n    epoch_val_loss = np.mean(val_losses)\n    epoch_train_acc = train_acc\/len(train_loader.dataset)\n    epoch_val_acc = val_acc\/len(valid_loader.dataset)\n    epoch_tr_loss.append(epoch_train_loss)\n    epoch_vl_loss.append(epoch_val_loss)\n    epoch_tr_acc.append(epoch_train_acc)\n    epoch_vl_acc.append(epoch_val_acc)\n    print(f'Epoch {epoch+1}') \n    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n    if epoch_val_loss <= valid_loss_min:\n        torch.save(model.state_dict(), '..\/working\/state_dict.pt')\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n        valid_loss_min = epoch_val_loss\n    print(25*'==')","99b52205":"fig = plt.figure(figsize=(10, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(epoch_tr_acc, label='Train Acc')\nplt.plot(epoch_vl_acc, label='Validation Acc')\nplt.title(\"Accuracy\")\nplt.legend()\nplt.grid()\n\nplt.subplot(1, 2, 2)\nplt.plot(epoch_tr_loss, label='Train loss')\nplt.plot(epoch_vl_loss, label='Validation loss')\nplt.title(\"Loss\")\nplt.legend()\nplt.grid()\n\nplt.show()","caaf7124":"def predict_text(text):\n        word_seq = np.array([vocab[preprocess_string(word)] for word in text.split() \n                         if preprocess_string(word) in vocab.keys()])\n        word_seq = np.expand_dims(word_seq,axis=0)\n        pad =  torch.from_numpy(padding_(word_seq,500))\n        inputs = pad.to(device)\n        batch_size = 1\n        h = model.init_hidden(batch_size)\n        h = tuple([each.data for each in h])\n        output, h = model(inputs, h)\n        return(output.item())","b11eb4cc":"### Model","2860b1b5":"#### Analyzing Sentiment","2c50dc84":"#### Analyzing Review length","164e14ad":"### Splitting to train and test data\n\nWe will split data to train and test initially.Doing this on earlier stage allows to avoid data lekage.","70e8aa3f":"### Inference","22f6448d":"### Padding\n\nNow we will pad each of the sequence to max length","d15387c2":"### Training","2fd91554":"### Tokenization","14c4b044":"Observations :\n\na) Mean review length = around 69.\n\nb) minimum length of reviews is 2.\n\nc) There are quite a few reviews that are extremely long, we can manually investigate them to check whether we need to include or exclude them from our analysis","6e4ffc38":"#### Batching and loading as tensor"}}