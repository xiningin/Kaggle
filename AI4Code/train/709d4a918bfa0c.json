{"cell_type":{"527b5357":"code","044da90c":"code","f587fb58":"code","eee3ef59":"code","b8f30762":"code","5b297641":"code","9b45c04e":"code","d498e9be":"code","56a5b983":"code","5ef82a9c":"code","2dac097e":"code","1570c037":"code","3f1c083d":"code","bb06afbe":"code","dacaab45":"code","0f5ef8b7":"code","888e7626":"code","fd4fb2ef":"code","82187da2":"code","8f796db3":"code","cc1509b8":"code","73e8e3c6":"code","64ced4c6":"code","13fb058f":"code","cadad904":"code","31f5d95b":"code","31ff7961":"code","40034173":"code","c510d0d2":"code","4f3b85f4":"code","a9f9d875":"code","f568514e":"code","3c5bee9f":"code","f2651ae7":"code","18d01983":"code","ad77661c":"code","c97090ab":"code","dd3dce8d":"code","84decb99":"code","fe914d8b":"code","3d0356f0":"code","961a33df":"code","b08e89b4":"code","3a1fb1a8":"code","deefb80f":"code","964b2bb1":"code","893d17f7":"code","eb281c4e":"code","09753839":"code","99c69edf":"code","568763f4":"code","a491bc4d":"code","15cd241b":"code","9bc45aed":"code","136239c1":"markdown","9e13ea2d":"markdown","4b949645":"markdown","b6ecf3ce":"markdown","b61e63c4":"markdown","194e3c6c":"markdown","dfb9707f":"markdown","d420dc17":"markdown","d16b73e8":"markdown","5d17b624":"markdown","37002f59":"markdown","378264b4":"markdown","868b515a":"markdown","b03f2c85":"markdown","9a7bd367":"markdown","df9da15d":"markdown","edb5f97f":"markdown","940b495b":"markdown","3709ecc5":"markdown","7da4fa95":"markdown","24b4e927":"markdown","df713056":"markdown","494ee5a8":"markdown","a18842d5":"markdown","cd10afa1":"markdown","46309e5d":"markdown","ad233b0f":"markdown","6db48114":"markdown","175e14fd":"markdown","50376bf9":"markdown","79466b29":"markdown","ec0b2f3e":"markdown","3646a641":"markdown","64d37a2f":"markdown","a4c212c8":"markdown","214d64ed":"markdown","d63d4b2f":"markdown","39ee11af":"markdown","1f44580c":"markdown","88ab6772":"markdown","6092dd39":"markdown"},"source":{"527b5357":"import numpy as np \nimport pandas as pd \nfrom itertools import chain\nimport os\nprint(os.listdir(\"..\/input\"))","044da90c":"from kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()","f587fb58":"(market_train_df, news_train_df) = env.get_training_data()","eee3ef59":"market_train_df.head()","b8f30762":"market_train_df.tail()","5b297641":"market_train_df.shape","9b45c04e":"market_train_df.columns","d498e9be":"market_train_df.info()","56a5b983":"market_train_df.describe()","5ef82a9c":"market_train_df.isnull().any()","2dac097e":"market_train_df.isnull().sum()*100\/market_train_df.shape[0]","1570c037":"import matplotlib.pyplot as plt\npercent = (market_train_df.isnull().sum()*100\/market_train_df.shape[0]).sort_values(ascending=False)\npercent.plot(kind=\"bar\", figsize = (20,10), fontsize = 20)\nplt.xlabel(\"Columns\", fontsize = 20)\nplt.ylabel(\"Value Percent(%)\", fontsize = 20)\nplt.title(\"Total Missing Value by market_obs_df\", fontsize = 20)","3f1c083d":"plt.figure(figsize=(10,10))\nmarket_train_df['returnsClosePrevMktres1'].plot(kind='box')","bb06afbe":"( market_train_df['returnsClosePrevMktres1']<market_train_df['returnsClosePrevMktres1'].mean()).value_counts()","dacaab45":"( market_train_df['returnsClosePrevMktres1']<market_train_df['returnsClosePrevMktres1'].median()).value_counts()","0f5ef8b7":"market_train_df['returnsClosePrevMktres1'].fillna(market_train_df['returnsClosePrevMktres1'].mean(),inplace=True)","888e7626":"market_train_df.isnull().any()","fd4fb2ef":"market_train_df['returnsOpenPrevMktres1'].plot.box()","82187da2":"market_train_df['returnsClosePrevMktres10'].plot.box()","8f796db3":"market_train_df['returnsOpenPrevMktres10'].plot.box()","cc1509b8":"market_train_df['returnsOpenPrevMktres1'].fillna(market_train_df['returnsOpenPrevMktres1'].median(),inplace=True)\nmarket_train_df['returnsClosePrevMktres10'].fillna(market_train_df['returnsClosePrevMktres10'].median(),inplace=True)\nmarket_train_df['returnsOpenPrevMktres10'].fillna(market_train_df['returnsOpenPrevMktres10'].median(),inplace=True)","73e8e3c6":"market_train_df.isnull().any()","64ced4c6":"news_train_df.head()","13fb058f":"news_train_df.tail()","cadad904":"news_train_df.shape","31f5d95b":"news_train_df.columns","31ff7961":"news_train_df.info()","40034173":"news_train_df.describe()","c510d0d2":"news_train_df.isnull().any()","4f3b85f4":"import plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\ndata = []\nfor asset in np.random.choice(market_train_df['assetCode'].unique(), 10):\n    asset_df = market_train_df[(market_train_df['assetCode'] == asset)]\n\n    data.append(go.Scatter(\n        x = asset_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = asset_df['close'].values,\n        name = asset\n    ))\nlayout = go.Layout(dict(title = \"Closing prices of 10 random assets\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","a9f9d875":"market_train_df['price_diff'] = market_train_df['close'] - market_train_df['open']\nmarket_train_df.sort_values('price_diff')[:10]","f568514e":"news_train_df = news_train_df.loc[news_train_df['time'] >= '2009-01-01 22:00:00+0000']\nmarket_train_df = market_train_df.loc[market_train_df['time'] >= '2009-01-01 22:00:00+0000']","3c5bee9f":"corr=market_train_df.corr()\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)\nfig.colorbar(cax)\nticks = np.arange(0,len(corr.columns),1)\nax.set_xticks(ticks)\nplt.xticks(rotation=90)\nax.set_yticks(ticks)\nax.set_xticklabels(corr.columns)\nax.set_yticklabels(corr.columns)\nplt.show()","f2651ae7":"corr_with_mkt = market_train_df.corr()[\"returnsOpenNextMktres10\"].sort_values(ascending=False)\nplt.figure(figsize=(14,7))\ncorr_with_mkt.drop(\"returnsOpenNextMktres10\").plot.bar()\nplt.show()","18d01983":"corr2=news_train_df.corr()\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(corr2,cmap='coolwarm', vmin=-1, vmax=1)\nfig.colorbar(cax)\nticks = np.arange(0,len(corr2.columns),1)\nax.set_xticks(ticks)\nplt.xticks(rotation=90)\nax.set_yticks(ticks)\nax.set_xticklabels(corr2.columns)\nax.set_yticklabels(corr2.columns)\nplt.show()","ad77661c":"corr_with_mkt = news_train_df.corr()[\"sentimentClass\"].sort_values(ascending=False)\nplt.figure(figsize=(14,7))\ncorr_with_mkt.drop(\"sentimentClass\").plot.bar()\nplt.show()","c97090ab":"market = market_train_df.head(1_500_000)\nnews = news_train_df.head(3_500_000)","dd3dce8d":"news_cols_agg = {\n    'urgency': ['min', 'count'],\n    'takeSequence': ['max'],\n    'bodySize': ['min', 'max', 'mean'],\n    'wordCount': ['min', 'max', 'mean'],\n    'sentenceCount':['min', 'max', 'mean'],\n    'companyCount': ['min', 'max', 'mean'],\n    'relevance': ['min', 'max', 'mean'],\n    'sentimentClass': ['min', 'max', 'mean'],\n    'sentimentNegative': ['min', 'max', 'mean'],\n    'sentimentNeutral': ['min', 'max', 'mean'],\n    'sentimentPositive': ['min', 'max', 'mean'],\n    'sentimentWordCount': ['min', 'max', 'mean'],\n    'noveltyCount12H': ['min', 'max', 'mean'],\n    'noveltyCount24H': ['min', 'max', 'mean'],\n    'noveltyCount3D': ['min', 'max', 'mean'],\n    'noveltyCount5D': ['min', 'max', 'mean'],\n    'noveltyCount7D':['min', 'max', 'mean'],\n    'volumeCounts12H': ['min', 'max', 'mean'],\n    'volumeCounts24H': ['min', 'max', 'mean'],\n    'volumeCounts3D': ['min', 'max', 'mean'],\n    'volumeCounts5D': ['min', 'max', 'mean'],\n    'volumeCounts7D': ['min', 'max', 'mean']\n}","84decb99":"def merge_data(news_train_df,market_train_df):\n    #rendre les assetCodes dans une liste\n    news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\.\/]+)'\")\n    news_train_df.time= news_train_df.time.dt.date\n    market_train_df.time= market_train_df.time.dt.date\n    #faire sortir \u00e9l\u00e9ment par \u00e9l\u00e9ment de la liste\n    assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n    #cr\u00e9ation d'un array ayant les indexes r\u00e9p\u00e9t\u00e9s de chaque ligne dupliqu\u00e9e\n    assetCodes_index = news_train_df.index.repeat( news_train_df['assetCodes'].apply(len) )\n    #Cr\u00e9ation d'un dataframe contenant les assetCodes et leurs indexes\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n    # Creation d'un news dataframe o\u00f9 les lignes sont r\u00e9p\u00e9t\u00e9es\n    news_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())\n    #je dois ajouter get dummies pour provider \u00e0 ce niveau l\u00e0\n    news_train_df_expanded = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0',\n                                  right_index=True, suffixes=(['','_old']))\n     # Free memory\n    del news_train_df, df_assetCodes\n    news_train_df_aggregated = news_train_df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n     # Free memory\n    del news_train_df_expanded\n    # Convert to float32 to save memory\n    news_train_df_aggregated = news_train_df_aggregated.apply(np.float32)\n    # Flat columns\n    news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n    # Join with train\n    market_train_df = market_train_df.join(news_train_df_aggregated, on=['time', 'assetCode'])\n    # Free memory\n    del news_train_df_aggregated\n    return market_train_df","fe914d8b":"data_m=merge_data(news,market)","3d0356f0":"data_m=data_m.dropna()\ndata_m.shape ","961a33df":"corr_with_mkt1 = data_m.corr()[\"returnsOpenNextMktres10\"].sort_values(ascending=False)\nplt.figure(figsize=(14,7))\ncorr_with_mkt1.drop(\"returnsOpenNextMktres10\").plot.bar()\nplt.show()","b08e89b4":"Y=data_m.returnsOpenNextMktres10.values \ndata_f=data_m.drop(['returnsOpenNextMktres10'],axis=1)\ndata_final=data_f.drop(['assetName'],axis=1) #On a deja l'assetCode ","3a1fb1a8":"print(data_final.dtypes)","deefb80f":"from sklearn import preprocessing\ndel data_final['time']\nle = preprocessing.LabelEncoder()\nle.fit(data_final['assetCode'])\ndata_final['assetCode']=le.fit_transform(data_final['assetCode'])","964b2bb1":"data_final=data_final.astype(float)","893d17f7":"print(data_final.dtypes)","eb281c4e":"X=data_final.values","09753839":"from sklearn.preprocessing import MinMaxScaler,Imputer\n# d\u00e9finir un dictionnaire pour stocker nos rankings \nranks = {}\n# cr\u00e9er une fonction qui stocke le classement des caract\u00e9ristiques \ndef ranking(ranks, names, order=1):\n    minmax = MinMaxScaler()\n    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n    ranks = map(lambda x: round(x,2), ranks)\n    return dict(zip(names, ranks))","99c69edf":"from sklearn.feature_selection import RFE, f_regression\nfrom sklearn.preprocessing import StandardScaler\nX_scaled=StandardScaler().fit_transform(X)\nfrom sklearn.linear_model import (LinearRegression, Ridge, Lasso,LogisticRegression)\ncolnames=data_final.columns\nlr = LinearRegression(normalize=True)\nlr.fit(X_scaled,Y)\nranks[\"LinReg\"] = ranking(np.abs(lr.coef_), colnames)\n\n#  Ridge \nridge = Ridge(alpha = 7)\nridge.fit(X_scaled,Y)\nranks['Ridge'] = ranking(np.abs(ridge.coef_), colnames)\n\n#  Lasso\nlasso = Lasso(alpha=.05)\nlasso.fit(X_scaled, Y)\nranks[\"Lasso\"] = ranking(np.abs(lasso.coef_), colnames)","568763f4":"r = {}\nfor name in colnames:\n    r[name] = round(np.mean([ranks[method][name] \n                             for method in ranks.keys()]), 2)\n \nmethods = sorted(ranks.keys())\nranks[\"Mean\"] = r\nmethods.append(\"Mean\")\n \nprint(\"\\t%s\" % \"\\t\".join(methods))\nfor name in colnames:\n    print(\"%s\\t%s\" % (name, \"\\t\".join(map(str, \n                         [ranks[method][name] for method in methods]))))","a491bc4d":"# mettre la moyenne dans un dataframe Pandas\nmeanplot = pd.DataFrame(list(r.items()), columns= ['Feature','Mean Ranking'])\n\n# Trier le  dataframe\nmeanplot = meanplot.sort_values('Mean Ranking', ascending=False)","15cd241b":"import seaborn as sns\nsns.factorplot(x=\"Mean Ranking\", y=\"Feature\", data = meanplot, kind=\"bar\", \n               size=20, aspect=1.9, palette='coolwarm')","9bc45aed":"features=list(meanplot[meanplot['Mean Ranking']!=0].Feature)","136239c1":"***1.Market Dataset***","9e13ea2d":"On va savoir la corr\u00e9lation entre les donn\u00e9es du Market premi\u00e8rement et puis la corr\u00e9lation entre les caract\u00e9ristiques et la cible","4b949645":"alors on a pas de valeurs manquantes dans news dataset.","b6ecf3ce":"***2.EDA pour Market Data:***","b61e63c4":"* Donc on va remplacer par la moyenne:","194e3c6c":"Et voila , on trouve qu'il ya trois colonnes qui sont pas des float , c'est: time , AssetCode, et AssetName.\nMaintenant on peut extraire quelques statistiques pour ces donn\u00e9es:","dfb9707f":"**A. Exploratory Data Analysis EDA**","d420dc17":"Passant maintenant au autres colonnes contenant les valeurs nulles:","d16b73e8":"Dans l'approche Features Engineering , on essaye de garder les caract\u00e9ristiques les plus pertinents et les plus utils dans notre \u00e9tude , pour cela on va dans un premier moment , on va faire une \u00e9tude de corr\u00e9lation pour les donn\u00e9es s\u00e9par\u00e9s , puis on va faire une jointure des deux tables (Market et News) et appliquer des mod\u00e8les afin de savoir l'importance des Caract\u00e9ristiques .","5d17b624":"![](http:\/\/)Par rapport \u00e0 la moyenne:","37002f59":"Normalement on a les donn\u00e9es de 2007 jusqu'a maintenant , Or dans 2008 il y avait une crise mondiale s'appellle  la crise bancaire et financi\u00e8re de l'automne 2008 , donc on va jeter un coup d'eil sur les donn\u00e9es de Market dans cette p\u00e9riode:********","378264b4":"Maintenant les donn\u00e9es sont prets , il reste \u00e0 appliquer les mod\u00e8les ","868b515a":"Maintenant on va retourner vers Market dataset et visualiser les prix des quelques  assetcodes ","b03f2c85":"Maintenant , on va faire une jointure de donn\u00e9es , et savoir l'importance de chaque caract\u00e9ristique pour le returnsOpenNextMktres10 .","9a7bd367":"et voila on a obtenu 414779 lignes de donn\u00e9es. Maintenant il reste \u00e0 decouvrire la corr\u00e9lation entre la cible et les autres colonnes","df9da15d":"Par rapport \u00e0 la m\u00e9diane:","edb5f97f":"Avant de faire la jointure , on va prendre que 1_500_000 ligne du market data et 3_500_000 du News data.","940b495b":"Alors il faut avoir le meme type (float64) pour tous les donn\u00e9es","3709ecc5":"Maintenant on veux savoir la corr\u00e9lation entre les caract\u00e9ristiques du news data et le sentiments class:","7da4fa95":"Parmis les algorithmes qu'on va appliquer , on trouve LinearRegression, Ridge, Lasso et LogisticRegression, puis on va extraire l'importance de chaque caract\u00e9ristique pour chaque algorithme et faire la moyenne par la suite :","24b4e927":"Il ya des caract\u00e9ristiques qui ont une forte corr\u00e9lation avec la cible et il ya ceux qui ont une faible corr\u00e9lation , maintenat on va appliquer quelques mod\u00e8les pour savoir l'importance de ces caract\u00e9ristiques , mais avant \u00e7a il faut pr\u00e9parer ces donn\u00e9es","df713056":"***2.News Dataset***","494ee5a8":"On a plusieurs ouliers , donc on va remplacer les valeurs nulles par la m\u00e9diane:","a18842d5":"Alors on constate qu'il ya des caract\u00e9ristiques qui ont une forte corr\u00e9lation entre eux et il ya ceux qui ont une faible corr\u00e9lation entre eux , mais l'essentiel pour nous est de savoir la corr\u00e9lation entre la cible et les autres caract\u00e9ristiques","cd10afa1":"On va se debarasser des valeurs nulles :","46309e5d":"On a 4 colonnes contenant les valeurs nulles, on va decouvrire le pourcentage de ces valeurs dans notre dataset et puis les visualiser:","ad233b0f":"D'abord , on a une id\u00e9e g\u00e9n\u00e9rale sur nos donn\u00e9es , il reste \u00e0 decouvrire s'il ya des valeurs nulles dans le dataset ou pas:","6db48114":"News dataset contient 9328750 ligne et 35 colonne.","175e14fd":"**B.Features Engineering**","50376bf9":"Maintenant , on a une id\u00e9e sur les noms des colonnes , il reste qu'afficher le type de chaque colonne","79466b29":"***3.EDA News Data***","ec0b2f3e":"La fonction pour faire la jointure","3646a641":"\nLes colonnes returnsClosePrevMktres1 , returnsOpenPrevMktres1,returnsClosePrevMktres10,returnsOpenPrevMktres1 ont des valeurs nulles , ces colonnes ref\u00e8rent au Residual returns qui se calcule par la formule suivante:**mktres=raw\u2212\u03b2*rmarket** .\nDans notre cas on peut pas calculer le \u03b2 ,donc on va remplacer ces valeurs nulles par la m\u00e9diane ou bien la moyenne , \u00e7a depends des outiliers qu'on va les visualiser tout de suite:\n","64d37a2f":"On a deux datasets , on va traiter chacune \u00e0 part","a4c212c8":"Donc, le prix des actions \"Towers Watson & Co\" et Bank of New York Mellon Corp  \u00e9tait presque 10k ... ce qui est anormale .\nMaintenant, on pense qu'il est temps de jeter une vieille partie du dataset. Laissons seulement les donn\u00e9es depuis l'ann\u00e9e 2009 , pour se  d\u00e9barrasser des donn\u00e9es de la plus grande crise.","214d64ed":"***1.Importer les Librairies et acqu\u00e9rir les donn\u00e9es:***","d63d4b2f":"Maintenant on va prendre que les caract\u00e9ristiques qui ont une importance pour notre cible","39ee11af":"Ici On a un nombre faible des outliers , donc on va savoir la distribution par rapport \u00e0 la m\u00e9diane et la moyenne:","1f44580c":"**Pour Bien Comprendre nos Datasets (NEWS & MARKET) , et d\u00e9couvrire la structure des donn\u00e9es , on va utiliser l'approche EDA.**","88ab6772":"Maintenant on a trait\u00e9 les valeurs nulles de Market Data , passant \u00e0 l'exploration du News Data.","6092dd39":"Alors on constate que Market dataset contient 4072956 lignes et 16 colonnes. mais quelles sont ces colonnes?"}}