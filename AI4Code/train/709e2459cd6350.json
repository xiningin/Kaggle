{"cell_type":{"594ec190":"code","914227d3":"code","1e9899e2":"code","388c91c3":"code","1da48ee2":"code","45e03251":"code","7da4114f":"code","8078b383":"code","94e7894f":"code","fe8ff15f":"code","9487227f":"code","b9e5c3f4":"code","fcf17897":"code","c9992f42":"code","21c25ecc":"code","d041d136":"code","7e630261":"code","b3f9fd5d":"code","7cf34e37":"code","cd576e3e":"code","7bcfa74c":"code","0bd91709":"code","b1ea6649":"code","6630dfd2":"code","19a1193b":"code","10bb392a":"code","693e035e":"code","1a9af477":"code","9a9b2f6c":"code","41f44049":"code","d5a77324":"code","eee55523":"code","b5c0f150":"code","726f1855":"code","6866d663":"code","f689b2f2":"code","47b8a97e":"code","5826383b":"code","4332eb72":"code","b1f7ce23":"code","82d68004":"markdown"},"source":{"594ec190":"!pip install transformers","914227d3":"import fastai\nimport transformers\nprint('fastai version :', fastai.__version__)\nprint('transformers version :', transformers.__version__)","1e9899e2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport time\nfrom datetime import datetime\n\nimport torch\nimport torch.optim as optim\n\nimport random \n\n# fastai\nfrom fastai import *\nfrom fastai.text import *\nfrom fastai.callbacks import *\n\n# transformers\nfrom transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n\nfrom transformers import BertForSequenceClassification, BertTokenizer, BertConfig\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\nfrom transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\nfrom transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\nfrom transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig\n\nfrom transformers import AdamW\nfrom functools import partial\n\nfrom sklearn.model_selection import train_test_split\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# from google.cloud import storage\n# from google.cloud import automl_v1beta1 as automl\n\n# from automlwrapper import AutoMLWrapper","388c91c3":"# a dictionary that allows loading the correct classes by just specifying the correct model type name\nMODEL_CLASSES = {\n    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n    'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)}\n\n# Parameters\nseed = 40\nuse_fp16 = False\nbs = 64\n\nmodel_type = 'roberta'\npretrained_model_name = 'roberta-base'\n\n# model_type = 'bert'\n# pretrained_model_name='bert-base-uncased'\n\n# model_type = 'distilbert'\n# pretrained_model_name = 'distilbert-base-uncased'\n\n#model_type = 'xlm'\n#pretrained_model_name = 'xlm-clm-enfr-1024'\n\n#model_type = 'xlnet'\n#pretrained_model_name = 'xlnet-base-cased'\n\nmodel_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]","1da48ee2":"def seed_all(seed_value):\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False\nseed_all(seed)","45e03251":"# inherits from BaseTokenizer and overwrite a new tokenizer function\nclass TransformersBaseTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', **kwargs):\n        self._pretrained_tokenizer = pretrained_tokenizer\n        self.max_seq_len = pretrained_tokenizer.max_len\n        self.model_type = model_type\n\n    def __call__(self, *args, **kwargs): \n        return self\n\n    def tokenizer(self, t:str) -> List[str]:\n        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n        CLS = self._pretrained_tokenizer.cls_token\n        SEP = self._pretrained_tokenizer.sep_token\n        if self.model_type in ['roberta']:\n            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n            tokens = [CLS] + tokens + [SEP]\n        else:\n            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n            if self.model_type in ['xlnet']:\n                tokens = tokens + [SEP] +  [CLS]\n            else:\n                tokens = [CLS] + tokens + [SEP]\n        return tokens\n        \ntransformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\ntransformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\nfastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])","7da4114f":"# custom numericalizer\nclass TransformersVocab(Vocab):\n    def __init__(self, tokenizer: PreTrainedTokenizer):\n        super(TransformersVocab, self).__init__(itos = [])\n        self.tokenizer = tokenizer\n    \n    def numericalize(self, t:Collection[str]) -> List[int]:\n        \"Convert a list of tokens `t` to their ids.\"\n        return self.tokenizer.convert_tokens_to_ids(t)\n        #return self.tokenizer.encode(t)\n\n    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n        \"Convert a list of `nums` to their tokens.\"\n        nums = np.array(nums).tolist()\n        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)\n    \n    def __getstate__(self):\n        return {'itos':self.itos, 'tokenizer':self.tokenizer}\n\n    def __setstate__(self, state:dict):\n        self.itos = state['itos']\n        self.tokenizer = state['tokenizer']\n        self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})","8078b383":"# custom processor\ntransformer_vocab =  TransformersVocab(tokenizer = transformer_tokenizer)\nnumericalize_processor = NumericalizeProcessor(vocab=transformer_vocab)\n\ntokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, \n                                       include_bos=False, \n                                       include_eos=False)\n\ntransformer_processor = [tokenize_processor, numericalize_processor]","94e7894f":"# load data\ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","fe8ff15f":"print(train.shape)\ntrain.head()","9487227f":"print(test.shape)\ntest.head()","b9e5c3f4":"from nltk.tokenize import TweetTokenizer\ntwt = TweetTokenizer(strip_handles=True)\ndef tweets(r):\n    s = ' '.join(twt.tokenize(r['text']))\n    s = re.sub(r'http\\S+', '', s)\n    s = re.sub(r'https\\S+', '', s)    \n    return s","fcf17897":"train['ptext'] = train.apply(tweets, axis=1)\ntest['ptext'] = test.apply(tweets, axis=1)","c9992f42":"train.head()","21c25ecc":"# data bunch\npad_first = bool(model_type in ['xlnet'])\npad_idx = transformer_tokenizer.pad_token_id\n\ndatabunch = (TextList.from_df(train, cols='ptext', processor=transformer_processor)\n             .split_by_rand_pct(0.1, seed=seed)\n             .label_from_df(cols='target')\n             .add_test(test)\n             .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))","d041d136":"class CustomTransformerModel(nn.Module):\n  \n    def __init__(self, transformer_model: PreTrainedModel):\n        super(CustomTransformerModel,self).__init__()\n        self.transformer = transformer_model\n        \n    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n        \n        attention_mask = (input_ids!=pad_idx).type(input_ids.type()) # attention_mask for distilBERT\n        token_type_ids = (attention_mask==-2).type(input_ids.type())\n            \n        logits = self.transformer(input_ids,\n                                attention_mask = attention_mask, token_type_ids=token_type_ids)[0]   \n        return logits","7e630261":"# num of classes\nconfig = config_class.from_pretrained(pretrained_model_name)\nconfig.num_labels = 2\n\ntransformer_model = model_class.from_pretrained(pretrained_model_name, config = config)\n# transformer_model = model_class.from_pretrained(model_name, num_labels = 2)\ncustom_transformer_model = CustomTransformerModel(transformer_model = transformer_model)","b3f9fd5d":"# custom optimizer (AdamW)\nCustomAdamW = partial(AdamW, correct_bias=False)\n\nlearner = Learner(databunch, \n                  custom_transformer_model, \n                  opt_func = CustomAdamW,\n                  metrics=[accuracy, FBeta(beta=1)])\n\n# Show graph of learner stats and metrics after each epoch.\nlearner.callbacks.append(ShowGraph(learner))\n\n# Put learn in FP16 precision mode. --> Seems to not working\nif use_fp16: learner = learner.to_fp16()","7cf34e37":"# Discriminative Fine-tuning and Gradual unfreezing\nlist_layers = [learner.model.transformer.roberta.embeddings,\n              learner.model.transformer.roberta.encoder.layer[0],\n              learner.model.transformer.roberta.encoder.layer[1],\n              learner.model.transformer.roberta.encoder.layer[2],\n              learner.model.transformer.roberta.encoder.layer[3],\n              learner.model.transformer.roberta.encoder.layer[4],\n              learner.model.transformer.roberta.encoder.layer[5],\n              learner.model.transformer.roberta.encoder.layer[6],\n              learner.model.transformer.roberta.encoder.layer[7],\n              learner.model.transformer.roberta.encoder.layer[8],\n              learner.model.transformer.roberta.encoder.layer[9],\n              learner.model.transformer.roberta.encoder.layer[10],\n              learner.model.transformer.roberta.encoder.layer[11],\n              learner.model.transformer.roberta.pooler]\n               \nlearner.split(list_layers)\nnum_groups = len(learner.layer_groups)\nprint('Learner split in',num_groups,'groups')\nprint(learner.layer_groups)","cd576e3e":"learner.save('untrain')","7bcfa74c":"seed_all(seed)\nlearner.load('untrain');","0bd91709":"# unfreeze all except for classifier\n learner.freeze_to(-1)\n#learner.unfreeze()","b1ea6649":"learner.summary()","6630dfd2":"learner.lr_find()\nlearner.recorder.plot(suggestion=True)","19a1193b":"# find a good lr (For Slanted Triangular Learning Rates)\n# 0\t0.482129\t0.387862\t0.853482\t0.813076\t00:20\n# 1\t0.370521\t0.407280\t0.833771\t0.804633\t00:19\n# 2\t0.274855\t0.420265\t0.839028\t0.807843\t00:20\n\nlearner.fit_one_cycle(1,max_lr=5e-04,moms=(0.8,0.7))","10bb392a":"learner.save('first_cycle')","693e035e":"seed_all(seed)\nlearner.load('first_cycle');","1a9af477":"#learner.freeze_to(-5)\nlearner.unfreeze()\nlr = 1e-5","9a9b2f6c":"learner.fit_one_cycle(2, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","41f44049":"#learner.save('second_cycle')","d5a77324":"#seed_all(seed)\n#learner.load('second_cycle');","eee55523":"#learner.freeze_to(-3)","b5c0f150":"#learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","726f1855":"#learner.save('third_cycle')","6866d663":"#seed_all(seed)\n#learner.load('third_cycle');","f689b2f2":"#learner.unfreeze()","47b8a97e":"#learner.fit_one_cycle(2, max_lr=slice(lr*0.90**num_groups, lr), moms=(0.8, 0.9))","5826383b":"#learner.recorder.plot_losses()\n#learner.recorder.plot_metrics()","4332eb72":"#learner.export()\n#learner.save('final')","b1f7ce23":"def get_preds_as_nparray(ds_type) -> np.ndarray:\n    \"\"\"\n    the get_preds method does not yield the elements in order by default\n    we borrow the code from the RNNLearner to resort the elements into their correct order\n    \"\"\"\n    preds = learner.get_preds(ds_type)[0].detach().cpu().numpy()\n    sampler = [i for i in databunch.dl(ds_type).sampler]\n    reverse_sampler = np.argsort(sampler)\n    return preds[reverse_sampler, :]\n    \ntest_preds = get_preds_as_nparray(DatasetType.Test)\npreds = []\nfor i in test_preds:\n    preds.append(np.argmax(i))\n\nsub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\nsub['target'] = preds\nsub.to_csv('submission.csv', index=False)\nsub.head(10)","82d68004":"totally based on https:\/\/towardsdatascience.com\/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2"}}