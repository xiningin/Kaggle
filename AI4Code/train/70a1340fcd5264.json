{"cell_type":{"2e4cf42d":"code","58b8c73e":"code","ce0044a9":"code","e67c3dce":"code","ba64ad07":"code","fb50321e":"code","b508f9c9":"code","137580ea":"code","7ab5fd2a":"code","34dc9582":"code","ffc47207":"code","f5310eae":"code","b4ce1419":"code","513e937f":"code","f1d69baf":"code","2b71eb57":"code","166e45ae":"code","cf33d05b":"code","7bdf90c0":"code","42211ddd":"code","9a9f2144":"code","a8aaf9ad":"code","66705cb1":"code","c9383b46":"code","cc0d4117":"code","690f811e":"code","862f7cd7":"code","16fb1e6d":"code","fdebc1d5":"code","60f3099b":"code","cef19a0d":"code","d1995f40":"code","2a062056":"code","d79ce56e":"code","bbe88f95":"code","48315c88":"code","c57010e2":"code","484f7e8e":"code","b426dcda":"code","4b2a8d05":"code","57211468":"code","4fd8493b":"code","99d6d231":"code","0d778022":"code","d27d3393":"code","e131e90b":"code","f6204a33":"code","aaa58e3e":"code","c67a4519":"code","81a934e6":"markdown","08225bad":"markdown","a0adbb92":"markdown","0b308dbf":"markdown","6a038832":"markdown","73c71693":"markdown","6ee153d3":"markdown","bd376253":"markdown","a02dec6d":"markdown","7f5ff1b2":"markdown","2a9baa85":"markdown","31597a89":"markdown","728150e3":"markdown","1ece991a":"markdown","349336e5":"markdown","762b3d54":"markdown","fbafac1a":"markdown","6aa0ef8c":"markdown","90d612e9":"markdown","7b5a6824":"markdown","250651f0":"markdown","d17c2d41":"markdown"},"source":{"2e4cf42d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom scipy.stats import norm\nfrom scipy import stats\n%matplotlib inline\nplt.style.use('ggplot')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","58b8c73e":"train_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\ntrain_data.head()\ntest_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')\n","ce0044a9":"train_data['SalePrice'].describe()","e67c3dce":"sns.displot(train_data['SalePrice'])\nplt.show()","ba64ad07":"train_data['SaleCondition'].unique()","fb50321e":"plt.figure()\nsns.boxplot(train_data['SaleCondition'], train_data['SalePrice'])\nplt.show()","b508f9c9":"plt.figure()\nsns.boxplot(train_data['SaleType'], train_data['SalePrice'])\nplt.show()","137580ea":"plt.scatter(train_data['SalePrice'], train_data['GrLivArea'])\nplt.xlabel('SalePrice')\nplt.ylabel('GrLiveArea')\nplt.show()","7ab5fd2a":"plt.scatter(train_data['TotalBsmtSF'], train_data['SalePrice'] )\nplt.show()","34dc9582":"sns.boxplot(train_data['OverallQual'], train_data['SalePrice'])\nplt.show()","ffc47207":"plt.figure(figsize=(15,12))\nsns.boxplot(train_data['YearBuilt'], train_data['SalePrice'])\nplt.show()","f5310eae":"corr = train_data.corr()\nplt.figure(figsize=(15,15))\nsns.heatmap(corr, square=True, vmax=1)","b4ce1419":"k=10\nplt.figure(figsize=(10,10))\ncols = corr.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train_data[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, square=True, cbar=True, annot=True, fmt='.2f', annot_kws={'size':10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","513e937f":"cols","f1d69baf":"cols = cols.drop(['GarageArea', '1stFlrSF', 'TotRmsAbvGrd'])","2b71eb57":"cols","166e45ae":"sns.pairplot(train_data[cols], size=2)\nplt.show()","cf33d05b":"# missing data in dataset \ntotal = train_data.isnull().sum().sort_values(ascending=False)\npercent = (train_data.isnull().sum() \/ train_data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","7bdf90c0":"train_data = train_data.drop((missing_data[missing_data['Total'] > 1]).index, 1)\ntrain_data = train_data.drop(train_data.loc[train_data['Electrical'].isnull()].index)\n","42211ddd":"train_data.isnull().sum().max() # checking that there's no missing data in our dataset ","9a9f2144":"sales_price = train_data['SalePrice'][:, np.newaxis]\nlow_range = sales_price[sales_price[:, 0].argsort()[:10]]\nlow_range","a8aaf9ad":"# standardizing data \nsalesprice_scaled = StandardScaler().fit_transform(train_data['SalePrice'][:, np.newaxis])\nlow_range = salesprice_scaled[salesprice_scaled[:, 0].argsort()[:10]]\nhigh_range = salesprice_scaled[salesprice_scaled[:,0].argsort()[-10:]]\nprint(low_range)\nprint('------')\nprint(high_range)","66705cb1":"# bivariate analysis saleprice\/grlivarea\n\nplt.scatter(train_data['GrLivArea'], train_data['SalePrice'])","c9383b46":"train_data.loc[1299, 'GrLivArea']","cc0d4117":"train_data[train_data.index == 1299]","690f811e":"train_data[train_data.index == 1299]","862f7cd7":"# deleteing points \n\ntrain_data.sort_values(by='GrLivArea', ascending=False)[:2]\ntrain_data = train_data.drop(train_data[train_data.index == 1299].index)\ntrain_data = train_data.drop(train_data[train_data.index == 524].index)","16fb1e6d":"# bivariate analysis saleprice\/grlivarea\nplt.scatter(train_data['GrLivArea'], train_data['SalePrice'])","fdebc1d5":"# bivariate analysis saleprice\/totalbsmtsf\nplt.scatter(train_data['TotalBsmtSF'], train_data['SalePrice'])\nplt.show()\n","60f3099b":"# histogram and normal probalility plot \nsns.distplot(train_data['SalePrice'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(train_data['SalePrice'], plot=plt)","cef19a0d":"# log transformation \ntrain_data['SalePrice'] = np.log(train_data['SalePrice'])","d1995f40":"# histogram and normal probaliblity plot after transformed \nsns.distplot(train_data['SalePrice'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(train_data['SalePrice'], plot=plt)","2a062056":"sns.distplot(train_data['GrLivArea'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(train_data['GrLivArea'], plot=plt)","d79ce56e":"# log transformation \ntrain_data['GrLivArea'] = np.log(train_data['GrLivArea'])","bbe88f95":"sns.distplot(train_data['GrLivArea'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(train_data['GrLivArea'], plot=plt)","48315c88":"# histogram and normal probability plot \nsns.distplot(train_data['TotalBsmtSF'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(train_data['TotalBsmtSF'], plot=plt)","c57010e2":"train_data['TotalBsmtSF'] = train_data['TotalBsmtSF'] + 0.00001\n\ntrain_data['TotalBsmtSF'] = np.log(train_data['TotalBsmtSF'])","484f7e8e":"# after log transformation\nsns.distplot(train_data['TotalBsmtSF'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(train_data['TotalBsmtSF'], plot=plt)","b426dcda":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error","4b2a8d05":"X = train_data[['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF',\n       'FullBath', 'YearBuilt']]\n\ny = train_data[['SalePrice']]\nprint(X.shape)\nprint(y.shape)\nprint(X.tail())\nprint(y.tail())","57211468":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=62)\n","4fd8493b":"lr = LinearRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\nmean_absolute_error(y_test, y_pred)","99d6d231":"test_data.isnull().sum()","0d778022":"test_data = test_data[['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF',\n       'FullBath', 'YearBuilt']]\ntest_data.isnull().sum()\n","d27d3393":"test_data['GarageCars'].describe()\ntest_data['TotalBsmtSF'].describe()\n","e131e90b":"test_data[test_data['GarageCars'].isnull()], test_data[test_data['TotalBsmtSF'].isnull()]","f6204a33":"test_data.loc[2577, 'GarageCars'] = 1\nprint(test_data[test_data['GarageCars'].isnull()])\ntest_data.loc[2121, 'TotalBsmtSF'] = 988\nprint(test_data[test_data['TotalBsmtSF'].isnull()])\ntest_data.shape\n","aaa58e3e":"y_pred = lr.predict(test_data)\n","c67a4519":"output = pd.DataFrame({'Id': test_data.index, 'SalePrice': y_pred.flatten()})\noutput.to_csv('my_submission1.csv', index=False)\nprint('Succesful')\n","81a934e6":"'SalePrice' is not normal. We see a positive skewness which does not follow the digonal line. \n\nA simple data transformation can solve the problem: \n- In case positive skewness, log transformations usually solve kind problem. ","08225bad":"What we should do now? \n - we can see a significant number of observations with value zero (house without basement) \n - Zero dosn't allow us to do log transformations \n\nSimple trick. Add to each value very low value to our series data \n","a0adbb92":"We see also a relationship linear or exponential between SalePrice and TotalBsmtSF","0b308dbf":"Problem solved! Check varible 'GrLivArea' ","6a038832":"In summary, \nVariables GrLivArea and TotalBsmtSF has a strong relationship between Sale Price, both relationship is positive, which means if first variable increase second also increase. In case between TotalBsmtSF and SalePrice we can see slope of the linear relationship is particuraly high than between SalePrice and GrLivArea.","73c71693":"Above on scatter plot we see our result with eliminate outliars. Done! ","6ee153d3":"We don't see a strong tendency but, we rather spent more money for a new item then old item","bd376253":"Next variable to check...","a02dec6d":"# In the search for normality\n\n- Histogram - Kurtosis and skewness. \n- Normal probalbity plot - data distribution should closly follow digonal that represents the normal distribution. \n\n","7f5ff1b2":"Outliars\n\nWe should be aware of this. Outliars can be a negative imply to our models and can be valueable source of information.\n\nOutliars topic is complex subject and it need more attention. In next step we do a quick analyse throught standard devitaion od 'SalePrice' and set a scatter plots.\n\n\nUnivariate analysis.\n\n\nOur target is to establish a threshold that defines an observation as an outlier. To do so, we should standardize the data set. Data standardization means converting data values to have mean of 0 and a standard deviation of 1. ","2a9baa85":"In scatter plot we can see two bigger values 'GrLivArea' which seems very suspicious. Therefore we'll define them as outliers and delete them from dataset. \n\nThe two observations in the top of the plot are this 7.something that we said we should be careful about them. we see this two observations following be the trend. For the resason we should keep them.","31597a89":"# Missing data\n- how prelevant is missing data? \n- Is missing data random or does it have a pattern? \n\nThis question is important for our dataset, missing data can imply a reduction of the sample size and missing data can prevent us from preprocceding with the analysis. We need to ensure that the missing data processis not biased and hidding inconvinient truth. ","728150e3":"How looks now 'SalePrice'\n\n- Low range values are similiar and not too far from 0\n- High range values are far from 0 and the 7 values are really out of range. \n\nFor now, we'll not consider ant of these values as an outliner but we should be careful with those 7. values. ","1ece991a":"# Modelling data","349336e5":"Explore whole our dataset\n\n- Correlation matrix (heatmap)\n- 'SalePrice' correlation matrix \n- create a scatter plots between the most correleted variables ","762b3d54":"We use the same logic like before","fbafac1a":"- This mega scatter plot gives us some information reasponsalbe idea about variables relationship.\n- Between 'GrLivArea' and 'TotalBsmtSF' we see liner bounderie. Basement areas can be equal to the above grounding living area, but this is not expected that basement area is bigger than grouding living area (maybe basemenat area is good to place to live?)  \n- The plot 'SalePrice' and 'YearBuilt' is good choice to consider. We can see between this varibles exponential function. We can also see this tendency in the upper limits of the dots cloud. Also, notice how to the sets dots regarding the last years tend to stay above this limit. Nowadays price house are increasing faster ","6aa0ef8c":"We see a linear relationship between SalePrice and GrLivArea","90d612e9":"In scatter plot we could eliminate some observation where 'TotalBsmtSF' > 3000 but i think it's not worth it. ","7b5a6824":"Let's analyse this to understand how to hanlde with missing data. \n- if is more then 15% missing data, we should delete the correspoding variable this mean we not use any trick and magic skill to fill missing data. According this, we should delete a set of variables (e.g 'PoolQC', 'MiscFeature', 'Alley' etc)\n- the most import inforamtion regarding garages is expressed in 'GarageCars' and considering that in this type varible we have 5% missing data. I'll delete this type variables \n- we should use the same  logic like before in 'GarageX' varibles for 'BsmtX' \n- For varibles MasX we also drop it from our dataset, our dataset will not lose a lot information because this varible is corrleted with 'GaragesArea' and 'YearBuilt'\n- Finally, we had a last variable to considering, we should drop row from our dataset with this gap. ","250651f0":"- 'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'. Done! \n- GarageArea and GarageCars is also strong correleted with SalePrice, but We see GarageArea is strongly correlated with GarageCars.\n- If we have more area in garage we get more place to parking car, it's simple! \n- We can keep 'GarageCars' since its correlation with 'SalePrice' is higter \n- 'TotalBsmtSF' and '1stFlrSF' also seem like a twin brothers. We can keep 'TotalBsmtSF' this is our first guess  \n- FullBath? \n- 'TotRmsAbvGrd' and 'GrLivArea' also seem like a twin broother. We can drop variable 'TotRmsAvbGrd' \n- Last our varible YearBuild is a litte sightly corrleted with SalePrice. it's correct? ","d17c2d41":"Bivariate analysis\n"}}