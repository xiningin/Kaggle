{"cell_type":{"c62e0a41":"code","6101e92a":"code","c370c82f":"code","3b9de35e":"code","94c8f360":"code","4ef4bf0d":"code","32b58780":"code","a0007257":"code","01f343b3":"code","365a2fa4":"code","1792e768":"code","a44e1ef9":"code","a194d188":"code","217f2701":"code","70fe977e":"code","e06c90db":"code","2927177c":"code","c25357a2":"code","f8235b5a":"code","7b0b4da7":"code","a529cba3":"code","6a456f86":"code","7282029c":"code","12fbf000":"code","f656b90e":"code","7d197540":"code","1b525adb":"code","fff2d050":"code","772112e3":"code","717d9b9c":"code","7df93f4e":"code","77d11c56":"markdown","bd723bfc":"markdown","aa84ea33":"markdown","d99023de":"markdown","2bc22c72":"markdown","4b69696b":"markdown","ab789aa3":"markdown"},"source":{"c62e0a41":"import os\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nimport torchvision.transforms.functional as TF\nfrom torchvision.utils import save_image, make_grid","6101e92a":"img_dir = '..\/input\/chest-xray-masks-and-labels\/Lung Segmentation\/CXR_png'\nmask_dir = '..\/input\/chest-xray-masks-and-labels\/Lung Segmentation\/masks'\nmask_names = os.listdir (mask_dir)\n\nprint (len (os.listdir (img_dir)))\nprint (len (os.listdir (mask_dir)))","c370c82f":"print (os.listdir (img_dir) [-4])\nprint (os.listdir (mask_dir) [-4])","3b9de35e":"print (os.listdir (img_dir) [-3])\nprint (os.listdir (mask_dir) [-3])","94c8f360":"def shuffle_split (mask_names, val_pct = 0.15, seed = 99):\n    \"\"\" shuffling dataset with random state and split to tran and valid\"\"\"\n    n_val = int (len (mask_names) * val_pct)\n    np.random.seed (seed)\n    idx = np.random.permutation (len (mask_names))\n    mask_names = np.array (mask_names) [idx]\n    \n    return mask_names [n_val:], mask_names [:n_val]","4ef4bf0d":"class lungDataset (Dataset):\n    \"\"\" create a dataset and transform it to tensor and resize it to 512*512\"\"\"\n    def __init__ (self, image_dir, mask_dir, mask_names, transform = None):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.mask_names = mask_names\n        self.transform = transform\n        \n    def __getitem__ (self, index):\n        mask_path = os.path.join (self.mask_dir, self.mask_names [index])\n        image_path = os.path.join (self.image_dir, self.mask_names [index].replace ('_mask.png', '.png') if 'mask' in self.mask_names [index] else self.mask_names [index])\n        \n        mask = np.array (Image.open (mask_path).convert ('L'), dtype = np.float32) # 8 byte black & white pixels\n        image = np.array (Image.open (image_path).convert ('L'), dtype = np.float32) #3 * 8byte True pixels\n        \n        if self.transform is not None:\n            augmentations = self.transform (image = image, image0 =  mask)\n            image = augmentations ['image']\n            mask = augmentations ['image0']\n            \n        return mask, image\n    \n    def __len__ (self):\n        return len (self.mask_names)","32b58780":"trn_tfms = A.Compose (\n[\n    A.Resize (height = 256, width = 256),\n    A.HorizontalFlip (p = 0.25),\n#     A.VerticalFlip (p = 0.1),\n#     A.Rotate (limit = 35, p = 1.0),\n    A.Normalize (0.5, 0.5, max_pixel_value = 255.0),\n    ToTensorV2 ()\n], additional_targets = {'image0': 'image'})\n\nval_tfms = A.Compose (\n[\n    A.Resize (height = 256, width = 256),\n    A.Normalize (0.5, 0.5, max_pixel_value = 255.0),\n    ToTensorV2 ()\n], additional_targets = {'image0': 'image'})","a0007257":"# create dataset and data loaders\ntrn_mask_names, val_mask_names = shuffle_split (mask_names, seed = 1)\n\ntrain_ds = lungDataset (img_dir, mask_dir, trn_mask_names, trn_tfms)\nvalid_ds = lungDataset (img_dir, mask_dir, val_mask_names, val_tfms)\n\ntrain_dl = DataLoader (train_ds, batch_size = 16, shuffle = True, num_workers = 2, pin_memory = True)\nvalid_dl = DataLoader (train_ds, batch_size = 16, shuffle = False, num_workers = 2, pin_memory = True)","01f343b3":"def imshow (img, title = None):\n    \"\"\" a function to show tensor images\"\"\"\n    img = img.numpy ().transpose (1, 2, 0)\n    mean = 0.5\n    std = 0.5\n    img = img * std + mean\n    img = np.clip (img, 0, 1)\n    \n    plt.figure (figsize = (10, 8))\n    plt.axis ('off')\n    plt.imshow (img)\n    if title:\n        plt.title (title)","365a2fa4":"print('mask shape: ',train_ds [0][0].shape)\nprint('target shape: ',train_ds [0][1].shape)","1792e768":"msk, img = next (iter (train_dl))\nmsk = torchvision.utils.make_grid (msk, nrow = 4)\nimg = torchvision.utils.make_grid (img, nrow = 4)","a44e1ef9":"imshow (msk, 'random batch of masks')","a194d188":"imshow (img, 'random batch of targets')","217f2701":"class Conv2D (nn.Module):\n    \"\"\" conv + bn + leaky \"\"\"\n    def __init__ (self, in_channels, out_channels, stride = 2):\n        super (Conv2D, self).__init__ ()\n        \n        self.conv = nn.Sequential (\n            nn.Conv2d (in_channels, out_channels, 4, stride, bias = False, padding_mode = 'reflect'),\n            nn.BatchNorm2d (out_channels),\n            nn.LeakyReLU (0.2)\n        )\n        \n    def forward (self, x):\n        return self.conv (x)","70fe977e":"class Discriminator (nn.Module):\n    \"\"\" 256x256 => 27x27 \"\"\"\n    def __init__ (self, in_channels = 1, features = [64, 128, 256, 512]):\n        super (Discriminator, self).__init__ ()\n        \n        self.initial = nn.Sequential (\n            nn.Conv2d (\n                in_channels * 2, features [0], kernel_size = 4, stride = 2, padding = 1, padding_mode = 'reflect'\n            ),\n            nn.LeakyReLU (0.2),\n        )\n        \n        layers  = []\n        in_channels = features [0]\n        for feature in features [1:]:\n            layers.append (Conv2D (in_channels, feature, stride = 1 if feature == features [-1] else 2))\n            in_channels = feature\n            \n        self.model = nn.Sequential (*layers)\n        \n    def forward (self, x, y):\n        x = torch.cat ([x, y], dim = 1)\n        x = self.initial (x)\n        \n        return self.model (x)","e06c90db":"def test ():\n    x = torch.randn ((1, 1, 256, 256))\n    y = torch.randn ((1, 1, 256, 256))\n    model = Discriminator ()\n    preds = model (x, y)\n    print (preds.shape)\n    \ntest ()","2927177c":"class Block (nn.Module):\n    def __init__ (self, in_channels, out_channels, direction = 'down', act = 'relu', use_dropout = False):\n        super (Block, self).__init__ ()\n        \n        self.conv = nn.Sequential (\n            nn.Conv2d (in_channels, out_channels, 4, 2, 1, bias = False, padding_mode = 'reflect')\n            if direction == 'down'\n            else nn.ConvTranspose2d (in_channels, out_channels, 4, 2, 1, bias = False),\n            nn.BatchNorm2d (out_channels),\n            nn.ReLU () if act == 'relu' else nn.LeakyReLU (0.2),\n        )\n        \n        self.use_dropout = use_dropout\n        self.dropout = nn.Dropout (0.5)\n        \n    def forward (self, x):\n        x = self.conv (x)\n        return self.dropout (x) if self.use_dropout else x","c25357a2":"class Generator (nn.Module):\n    \"\"\" generatore model based on unet \"\"\"\n    def __init__ (self, in_channels = 1, features = 64):\n        super (Generator, self).__init__ ()\n        \n        self.initial_down = nn.Sequential (\n            nn.Conv2d (in_channels, features, 4, 2, 1, padding_mode = 'reflect'),                              # 128x128\n            nn.LeakyReLU (0.2),\n        )\n        \n        self.down1 = Block (features, features * 2, direction = 'down', act = 'leaky', use_dropout = False)    # 64x64\n        self.down2 = Block (features * 2, features * 4, direction = 'down', act = 'leaky', use_dropout = False)# 32x32\n        self.down3 = Block (features * 4, features * 8, direction = 'down', act = 'leaky', use_dropout = False)# 16x16\n        self.down4 = Block (features * 8, features * 8, direction = 'down', act = 'leaky', use_dropout = False)# 8x8\n        self.down5 = Block (features * 8, features * 8, direction = 'down', act = 'leaky', use_dropout = False)# 4x4\n        self.down6 = Block (features * 8, features * 8, direction = 'down', act = 'leaky', use_dropout = False)# 2x2\n        self.bottleneck = nn.Sequential (\n            nn.Conv2d (features * 8, features * 8, 4, 2, 1, padding_mode = 'reflect'),                          # 1x1\n            nn.ReLU (),\n        )\n        \n        self.up1 = Block (features * 8    , features * 8, direction = 'up', act = 'relu', use_dropout = True)\n        self.up2 = Block (features * 8 * 2, features * 8, direction = 'up', act = 'relu', use_dropout = True)\n        self.up3 = Block (features * 8 * 2, features * 8, direction = 'up', act = 'relu', use_dropout = True)\n        self.up4 = Block (features * 8 * 2, features * 8, direction = 'up', act = 'relu', use_dropout = False)\n        self.up5 = Block (features * 8 * 2, features * 4, direction = 'up', act = 'relu', use_dropout = False)\n        self.up6 = Block (features * 4 * 2, features * 2, direction = 'up', act = 'relu', use_dropout = False)\n        self.up7 = Block (features * 2 * 2, features    , direction = 'up', act = 'relu', use_dropout = False)\n        self.final_up = nn.Sequential (\n            nn.ConvTranspose2d (features * 2, in_channels, 4, 2, 1),\n            nn.Tanh (),\n        )\n        \n    def forward (self, x):\n        d1 = self.initial_down (x)\n        d2 = self.down1 (d1)\n        d3 = self.down2 (d2)\n        d4 = self.down3 (d3)\n        d5 = self.down4 (d4)\n        d6 = self.down5 (d5)\n        d7 = self.down6 (d6)\n        bottleneck = self.bottleneck (d7)\n        up1 = self.up1 (bottleneck)\n        up2 = self.up2 (torch.cat ([up1, d7], dim = 1))\n        up3 = self.up3 (torch.cat ([up2, d6], dim = 1))\n        up4 = self.up4 (torch.cat ([up3, d5], dim = 1))\n        up5 = self.up5 (torch.cat ([up4, d4], dim = 1))\n        up6 = self.up6 (torch.cat ([up5, d3], dim = 1))\n        up7 = self.up7 (torch.cat ([up6, d2], dim = 1))\n        \n        return self.final_up (torch.cat ([up7, d1], dim = 1))","f8235b5a":"def test1 ():\n    x = torch.randn ((1, 1, 256, 256))\n    model1 = Generator (in_channels = 1, features = 64)\n    preds = model1 (x)\n    print (preds.shape)\n    \ntest1 ()","7b0b4da7":"def denorm (x):\n    out = x * 0.5 + 0.5\n    return out.clamp (0, 1)","a529cba3":"# Hyper Function and Initializiation\ndevice = torch.device ('cuda' if torch.cuda.is_available () else 'cpu')\n\ndisc = Discriminator (in_channels = 1).to(device)\ngen = Generator (in_channels = 1).to(device)\n\ndisc_optimizer = torch.optim.Adam (disc.parameters (), lr = 2e-4, betas = (0.5, 0.999))\ngen_optimizer = torch.optim.Adam (gen.parameters (), lr = 2e-4, betas = (0.5, 0.999))\n\nBCE = nn.BCEWithLogitsLoss ()\nL1 = nn.L1Loss ()","6a456f86":"def train_discriminator (disc, gen, masks, targets, BCE, disc_optimizer):\n    disc.train ()\n    \n    # generate fake targets and calculate real and fake outputs\n    fake_targets = gen (masks)\n    disc_real = disc (masks, targets)\n    disc_fake = disc (masks, fake_targets.detach ())\n    \n    # real and fake loss\n    disc_real_loss = BCE (disc_real, torch.ones_like (disc_real))\n    disc_fake_loss = BCE (disc_fake, torch.zeros_like (disc_fake))\n    disc_loss = (disc_real_loss + disc_fake_loss) \/ 2\n    \n    # Reset gradients\n    disc.zero_grad ()\n    \n    # Compute gradients\n    disc_loss.backward ()\n    \n    # Adjust the parameters using backprop\n    disc_optimizer.step ()\n    \n    return fake_targets, disc_loss","7282029c":"def train_generator (disc, masks, targets, fake_targets, BCE, L1, gen_optimizer):\n    gen.train ()\n    \n    # Generate fake images and calculate loss\n    disc_fake = disc (masks, fake_targets.detach ())\n    \n    gen_fake_loss = BCE (disc_fake, torch.ones_like (disc_fake))\n    l1 = L1 (fake_targets, targets) * 100\n    gen_loss = gen_fake_loss + l1\n    \n    # Backprop and optimize\n    gen_optimizer.zero_grad ()\n    gen_loss.backward ()\n    gen_optimizer.step ()\n    \n    return gen_loss","12fbf000":"def save_some_example (gen, valid_dl, device, denorm, epoch, folder = 'evaluation'):\n    if not os.path.exists (folder):\n        os.makedirs (folder)\n        \n    masks, targets = next (iter (valid_dl))\n    batch_size = masks.shape [0]\n    masks = masks.to(device)\n    targets = targets.to(device)\n    \n    gen.eval ()\n    \n    with torch.no_grad ():\n        fake_targets = gen (masks)\n        fake_target = make_grid (fake_targets, nrow = 4)\n        real_target = make_grid (targets, nrow = 4)\n        \n        print ('saving images...')\n        save_image (fake_target, folder + f'\/fake_target_{epoch}.png')\n        save_image (real_target, folder + f'\/real_target_{epoch}.png')\n        \n        if epoch == 1:\n            mask = make_grid (masks, nrow = 4)\n            save_image (mask, folder + f'\/masks_{epoch}.png')","f656b90e":"def fit (disc, gen, train_dl, valid_dl, BCE, L1, num_epochs, disc_optimizer, gen_optimizer, device, denorm, save_fn = None):\n    \n    d_losses, g_losses = [], []\n    total_step = len (train_dl)\n    step = 0\n    \n    for epoch in range (num_epochs):\n        for i, (masks, targets) in enumerate (train_dl):\n            \n            masks = masks.to(device)\n            targets = targets.to(device)\n\n            # Train the descriminator and generator\n            fake_targets, disc_loss = train_discriminator (disc, gen, masks, targets, BCE, disc_optimizer)\n            gen_loss = train_generator (disc, masks, targets, fake_targets, BCE, L1, gen_optimizer)\n\n            # Inspect the losses\n            if (i + 1) % 19 == 0:\n                d_losses.append (disc_loss.item ())\n                g_losses.append (gen_loss.item ())\n\n                print ('Epoch [{}\/{}], Step [{}\/{}], disc_loss: {:.4f}, gen_loss: {:.4f}'\n                .format (epoch + 1, num_epochs, i + 1, total_step, disc_loss.item (), gen_loss.item ()))\n                    \n                step += 1\n                    \n\n        # Sample and save images\n        if save_fn is not None:\n            save_fn (gen, valid_dl, device, denorm, epoch + 1)\n        \n    return dict (disc_loss = d_losses, gen_loss = g_losses)","7d197540":"history = fit (disc, gen, train_dl, valid_dl, BCE, L1, 150, disc_optimizer, gen_optimizer, device, denorm, save_some_example)","1b525adb":"# Save the model checkpoints\ntorch.save (gen.state_dict (), 'G_LungPix2Pix.pth')\ntorch.save (disc.state_dict (), 'D_LungPix2Pix.pth')","fff2d050":"from IPython.display import FileLink\n\nsample_dir = 'evaluation'\nvid_fname = 'lung_pix2pix_training.avi'\n\nfiles = [os.path.join (sample_dir, f) for f in os.listdir (sample_dir) if 'fake_target' in f]\nfiles.sort ()\nfiles","772112e3":"# create a video that show training results\nout = cv2.VideoWriter (vid_fname, cv2.VideoWriter_fourcc (*'FMP4'), 8, (1034, 1034))\n[out.write (cv2.imread (fname)) for fname in files]\nout.release ()\nFileLink ('lung_pix2pix_training.avi')","717d9b9c":"plt.plot (history ['disc_loss'], '-')\nplt.plot (history ['gen_loss'], '-')\nplt.xlabel ('epoch')\nplt.ylabel ('loss')\nplt.legend (['Discriminator', 'Generator'])\nplt.title ('Losses');","7df93f4e":"masks, targets = next (iter (valid_dl))\n\nmasks = masks.to(device)\n\nfake_targets = gen (masks)\nfake_targets = denorm (fake_targets).cpu ().detach ()\nmasks = denorm (masks).cpu ()\ntargets = denorm (targets)\n\nplt.figure (figsize = (15, 20))\nfor i in range (4):\n    plt.subplot (4, 3, 3 * i + 1)\n    plt.imshow (masks [i][0], cmap = 'gray')\n    plt.title ('Mask')\n    plt.axis ('off')\n    \n    plt.subplot (4, 3, 3 * i + 2)\n    plt.imshow (targets [i][0], cmap = 'gray')\n    plt.title ('Orginal Image')\n    plt.axis ('off')\n    \n    plt.subplot (4, 3, 3 * i + 3)\n    plt.imshow (fake_targets [i][0], cmap = 'gray')\n    plt.title ('Generated Image')\n    plt.axis ('off')\n    \nplt.show ()","77d11c56":"# Models","bd723bfc":"## show gernerated images from masks","aa84ea33":"# Visualize","d99023de":"# PreProcess","2bc22c72":"# Train","4b69696b":"## ploting results","ab789aa3":"# Training"}}