{"cell_type":{"17730fc5":"code","28160837":"code","0eb8a710":"code","d7046b36":"code","448d6341":"code","6cd73f54":"code","1a55a0f6":"code","34f24134":"code","c696bc57":"markdown","5798926c":"markdown","902c92a6":"markdown","1b70481d":"markdown","610ca12a":"markdown","37a54992":"markdown","a2e0dee4":"markdown","3492743a":"markdown","54be4e97":"markdown"},"source":{"17730fc5":"# Familiar imports\nimport numpy as np\nimport pandas as pd\n\n# For ordinal encoding categorical variables, splitting data\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\n\n# For training random forest model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import RepeatedKFold\nfrom tqdm import tqdm\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')","28160837":"# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\n# Preview the data\ntrain.head()","0eb8a710":"# Separate target from features\ny = train['target']\nfeatures = train.drop(['target','cat2','cat6'], axis=1)\n\nX = features.copy()\nX_test = test.copy()\nX_test = X_test.drop(['cat2','cat6'], axis=1)\nX['cont12s'] = X['cont12']*X['cont12']\nX['cont10s'] = X['cont10']*X['cont10']\nX['cont9s'] = X['cont9']*X['cont9']\nX['cont2s'] = X['cont2']*X['cont2']\nX['cont7s'] = X['cont7']*X['cont7']\nX['cont1210s'] = X['cont12']*X['cont10']\n\nX_test['cont12s'] = X_test['cont12']*X_test['cont12']\nX_test['cont10s'] = X_test['cont10']*X_test['cont10']\nX_test['cont9s'] = X_test['cont9']*X_test['cont9']\nX_test['cont2s'] = X_test['cont2']*X_test['cont2']\nX_test['cont7s'] = X_test['cont7']*X_test['cont7']\nX_test['cont1210s'] = X_test['cont12']*X_test['cont10']\nX_test.head()","d7046b36":"label = LabelEncoder()\ncategorical_feature = np.where(X.dtypes != 'float64')[0].tolist()\ncategorical_feature_columns = X.select_dtypes(exclude=['float64']).columns\n\nfor column in categorical_feature_columns:\n        label.fit(X[column])        \n        X[column] = label.transform(X[column])\n        X_test[column] = label.transform(X_test[column])\n        \nX.head()","448d6341":"lgbm_parameters = {\n    'metric': 'rmse', \n    #'n_jobs': -1,        \n    'n_estimators': 50000,    \n    'reg_alpha': 10.924491968127692,\n    'reg_lambda': 17.396730654687218,\n    'learning_rate': 0.09985133666265425,\n    'max_depth': 5,\n    'num_leaves': 5,\n    'min_child_samples': 10,    \n    'max_bin': 523,\n    'n_jobs': 4,\n    'colsample_bytree': 0.11807135201147481,    \n}","6cd73f54":"lgbm_val_pred = np.zeros(len(y))\nlgbm_test_pred = np.zeros(len(test))\nmse = []\nkf = KFold(n_splits=20, shuffle=True, random_state=1)\nfor trn_idx, val_idx in tqdm(kf.split(X,y)):\n    x_train_idx = X.iloc[trn_idx]\n    y_train_idx = y.iloc[trn_idx]\n    x_valid_idx = X.iloc[val_idx]\n    y_valid_idx = y.iloc[val_idx]\n    \n    lgbm_model = LGBMRegressor(**lgbm_parameters)\n    lgbm_model.fit(x_train_idx, y_train_idx, eval_set = ((x_valid_idx,y_valid_idx)),verbose = -1, early_stopping_rounds = 2000,categorical_feature=categorical_feature)          \n    mse1 = mean_squared_error(y_valid_idx, lgbm_model.predict(x_valid_idx))\n    pred = lgbm_model.predict(X_test)\n    lgbm_test_pred = lgbm_test_pred + pred\n    mse.append(mse1)\nlgbm_test_pred = lgbm_test_pred\/20","1a55a0f6":"lgbmdf = pd.DataFrame({'id':X_test.index,'target':lgbm_test_pred})\nlgbmdf = lgbmdf.set_index('id')\nX_train_new = pd.concat([test, lgbmdf], axis=1)\nX_train_new.reindex()\ntrain1 = train.copy()\ntrain1 = train1.append([X_train_new])\n\n# Separate target from features\ny = train1['target']\nfeatures = train1.drop(['target','cat2','cat6'], axis=1)\n\nX = features.copy()\nX['cont12s'] = X['cont12']*X['cont12']\nX['cont10s'] = X['cont10']*X['cont10']\nX['cont9s'] = X['cont9']*X['cont9']\nX['cont2s'] = X['cont2']*X['cont2']\nX['cont7s'] = X['cont7']*X['cont7']\nX['cont1210s'] = X['cont12']*X['cont10']\n\nlabel = LabelEncoder()\ncategorical_feature = np.where(X.dtypes != 'float64')[0].tolist()\ncategorical_feature_columns = X.select_dtypes(exclude=['float64']).columns\n\nfor column in categorical_feature_columns:\n        label.fit(X[column])        \n        X[column] = label.transform(X[column])       ","34f24134":"lgbm_val_pred = np.zeros(len(y))\nlgbm_test_pred = np.zeros(len(test))\nmse = []\nkf = KFold(n_splits=20, shuffle=True, random_state=1)\nfor trn_idx, val_idx in tqdm(kf.split(X,y)):\n    x_train_idx = X.iloc[trn_idx]\n    y_train_idx = y.iloc[trn_idx]\n    x_valid_idx = X.iloc[val_idx]\n    y_valid_idx = y.iloc[val_idx]\n    \n    lgbm_model = LGBMRegressor(**lgbm_parameters)\n    lgbm_model.fit(x_train_idx, y_train_idx, eval_set = ((x_valid_idx,y_valid_idx)),verbose = -1, early_stopping_rounds = 2000,categorical_feature=categorical_feature)          \n    mse1 = mean_squared_error(y_valid_idx, lgbm_model.predict(x_valid_idx))\n    pred = lgbm_model.predict(X_test)\n    lgbm_test_pred = lgbm_test_pred + pred\n    mse.append(mse1)\nlgbm_test_pred = lgbm_test_pred\/20\npd.DataFrame({'Id':X_test.index,'target':lgbm_test_pred}).to_csv('submission.csv', index=False)","c696bc57":"Welcome to the **[30 Days of ML competition](https:\/\/www.kaggle.com\/c\/30-days-of-ml\/overview)**!  In this notebook, you'll learn how to make your first submission.\n\nBefore getting started, make your own editable copy of this notebook by clicking on the **Copy and Edit** button.\n\n# Step 1: Import helpful libraries\n\nWe begin by importing the libraries we'll need.  Some of them will be familiar from the **[Intro to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)** course and the **[Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)** course.","5798926c":"Once you have run the code cell above, follow the instructions below to submit to the competition:\n1. Begin by clicking on the **Save Version** button in the top right corner of the window.  This will generate a pop-up window.  \n2. Ensure that the **Save and Run All** option is selected, and then click on the **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the **Submit** button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\nIf you want to keep working to improve your performance, select the **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.","902c92a6":"# Step 3: Prepare the data\n\nNext, we'll need to handle the categorical columns (`cat0`, `cat1`, ... `cat9`).  \n\nIn the **[Categorical Variables lesson](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables)** in the Intermediate Machine Learning course, you learned several different ways to encode categorical variables in a dataset.  In this notebook, we'll use ordinal encoding and save our encoded features as new variables `X` and `X_test`.","1b70481d":"The next code cell separates the target (which we assign to `y`) from the training features (which we assign to `features`).","610ca12a":"# Step 2: Load the data\n\nNext, we'll load the training and test data.  \n\nWe set `index_col=0` in the code cell below to use the `id` column to index the DataFrame.  (*If you're not sure how this works, try temporarily removing `index_col=0` and see how it changes the result.*)","37a54992":"Next, we break off a validation set from the training data.","a2e0dee4":"# Step 6: Keep Learning!\n\nIf you're not sure what to do next, you can begin by trying out more model types!\n1. If you took the **[Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)** course, then you learned about **[XGBoost](https:\/\/www.kaggle.com\/alexisbcook\/xgboost)**.  Try training a model with XGBoost, to improve over the performance you got here.\n\n2. Take the time to learn about **Light GBM (LGBM)**, which is similar to XGBoost, since they both use gradient boosting to iteratively add decision trees to an ensemble.  In case you're not sure how to get started, **[here's a notebook](https:\/\/www.kaggle.com\/svyatoslavsokolov\/tps-feb-2021-lgbm-simple-version)** that trains a model on a similar dataset.","3492743a":"In the code cell above, we set `squared=False` to get the root mean squared error (RMSE) on the validation data.\n\n# Step 5: Submit to the competition\n\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file.","54be4e97":"# Step 4: Train a model\n\nNow that the data is prepared, the next step is to train a model.  \n\nIf you took the **[Intro to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)** courses, then you learned about **[Random Forests](https:\/\/www.kaggle.com\/dansbecker\/random-forests)**.  In the code cell below, we fit a random forest model to the data."}}