{"cell_type":{"65010751":"code","5a7086cc":"code","e3560d77":"code","8c3e1079":"code","cc685db5":"code","935caf04":"code","e4decd5c":"code","52a99a2d":"code","175c9fd3":"code","53034d88":"code","f61e463b":"code","c1369db2":"code","cd1d2f85":"code","b808123f":"code","c72e4ce7":"code","09018f8b":"code","ab127be6":"code","d593bfda":"code","97b4f457":"markdown","a0254770":"markdown","daf260f9":"markdown","adc58b62":"markdown","ff164223":"markdown","d9df96f5":"markdown","c794721a":"markdown","e5cf4db2":"markdown","27f8fd9f":"markdown","389c2de4":"markdown","453e4ebf":"markdown","34e9c530":"markdown","2f813b4e":"markdown","6aacd92d":"markdown","94a1c668":"markdown","062f8614":"markdown","29475147":"markdown","2d070eb5":"markdown","6d871272":"markdown","01855a9d":"markdown","25771f2c":"markdown"},"source":{"65010751":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\npd.set_option('display.max_columns', None)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5a7086cc":"DATA_DIR = os.path.abspath('..\/input\/home-data-for-ml-course\/')\nTRAIN_DIR = os.path.join(DATA_DIR, 'train.csv')\nTEST_DIR = os.path.join(DATA_DIR, 'test.csv')\n\nhousing = pd.read_csv(TRAIN_DIR, index_col = 'Id')\n\nhousing = housing.dropna(subset = ['SalePrice'])\nhousing['SalePrice'] = np.log1p(housing['SalePrice'])","e3560d77":"housing.head()","8c3e1079":"missing = housing.isnull().mean()*100\nmissing = missing[missing>0].sort_values(ascending = False)\nmissing.plot.bar()","cc685db5":"# drop columns where more than 40% of data is unavailable\ncols_to_drop = [column for column in list(housing) if housing[column].isnull().mean() > 0.4]\nhousing = housing.drop(cols_to_drop, axis = 1)","935caf04":"from sklearn.model_selection import train_test_split\n\nhousing_train, housing_valid = train_test_split(housing, random_state = 42)\n\n# list of categorical feature names\nattr_cat = [column for column in list(housing_train) if (housing_train[column].dtype == 'object' and column != 'SalePrice')]\n\n# list of numerical feature names\nattr_num = [column for column in list(housing_train) if (housing_train[column].dtype in ['int64', 'float64'] and column != 'SalePrice')]","e4decd5c":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig = plt.figure(figsize=(18,24))\nfor i, attr in enumerate(attr_cat):\n    fig.add_subplot(8,5,i+1)\n    sns.countplot(housing_train[attr])\n    plt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()","52a99a2d":"cat_to_drop = ['Street', 'LandContour', 'Utilities', 'LandSlope', 'Condition1', 'Condition2', 'RoofMatl', 'Heating', 'SaleType', 'PavedDrive', 'GarageCond', 'GarageQual', 'Functional', 'Electrical', 'CentralAir', 'Heating', 'BsmtFinType2', 'BsmtCond','ExterCond']\n\nhousing_train = housing_train.drop(cat_to_drop, axis = 1)\nattr_cat = list(set(attr_cat) - set(cat_to_drop))","175c9fd3":"fig = plt.figure(figsize=(18,24))\nfor i, attr in enumerate(attr_num):\n    fig.add_subplot(10,6,i+1)\n    sns.distplot(housing_train[attr], kde=False)\nplt.tight_layout()\nplt.show()","53034d88":"num_to_drop = ['3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'EnclosedPorch', 'KitchenAbvGr', 'LowQualFinSF', 'BsmtHalfBath', 'BsmtFinSF2']\n\nhousing_train = housing_train.drop(num_to_drop, axis = 1)\nattr_num = list(set(attr_num) - set(num_to_drop))","f61e463b":"fig = plt.figure(figsize=(24,28))\nfor i, attr in enumerate(attr_num):\n    fig.add_subplot(10,6,i+1)\n    sns.boxplot(y = housing_train[attr])\nplt.tight_layout()\nplt.show()","c1369db2":"attr_num_out = ['LotArea', 'OpenPorchSF', 'WoodDeckSF', 'BsmtUnfSF', 'BsmtFinSF1', 'LotFrontage', 'GarageArea', 'TotalBsmtSF', 'GrLivArea','MasVnrArea']\nhousing_train[attr_num_out].describe()","cd1d2f85":"housing_train = housing_train.drop(housing_train[housing_train['LotArea']>25000].index)\nhousing_train = housing_train.drop(housing_train[housing_train['OpenPorchSF']>150].index)\nhousing_train = housing_train.drop(housing_train[housing_train['WoodDeckSF']>400].index)\nhousing_train = housing_train.drop(housing_train[housing_train['BsmtUnfSF']>1600].index)\nhousing_train = housing_train.drop(housing_train[housing_train['BsmtFinSF1']>1400].index)\nhousing_train = housing_train.drop(housing_train[housing_train['LotFrontage']>160].index)\nhousing_train = housing_train.drop(housing_train[housing_train['GarageArea']>1200].index)\nhousing_train = housing_train.drop(housing_train[housing_train['TotalBsmtSF']>2600].index)\nhousing_train = housing_train.drop(housing_train[housing_train['GrLivArea']>3600].index)\nhousing_train = housing_train.drop(housing_train[housing_train['MasVnrArea']>320].index)","b808123f":"num_correlation = housing_train[attr_num].corr()\nplt.figure(figsize=(20,20))\nplt.title('High Correlation')\nsns.heatmap(num_correlation > 0.8, annot=True, square=True)","c72e4ce7":"attr_num_corr = ['GarageCars', 'TotRmsAbvGrd', 'GarageYrBlt']\nhousing_train = housing_train.drop(attr_num_corr, axis = 1)\nattr_num = list(set(attr_num) - set(attr_num_corr))","09018f8b":"plt.figure(figsize=(20,20))\n\nfig = plt.figure(figsize=(20,20))\nfor i, attr in enumerate(attr_num):\n    fig.add_subplot(10,6,i+1)\n    sns.scatterplot(x = attr, y = 'SalePrice', data = housing_train)\nplt.tight_layout()\nplt.show()\n","ab127be6":"## 1 - categorical pipeline\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\ncat_pipeline = Pipeline(steps = [\n    ('impute', SimpleImputer(strategy = 'most_frequent')),\n    ('encode', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n## 2 - numerical pipeline\n\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n    ('impute', SimpleImputer(strategy = 'mean')),\n    ('scale', StandardScaler()),\n])\n\n\n## whole preprocessing pipeline\n\nfrom sklearn.compose import ColumnTransformer\n\npreprocessing_pipeline = ColumnTransformer([\n    ('cat', cat_pipeline, attr_cat),\n    ('num', num_pipeline, attr_num)\n])\n\n# having feature importances by fitting a basic XGBRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nimport scipy.stats as stats\n\nxgb_reg = XGBRegressor(n_jobs = -1, random_state = 123)\n\n\n# X_train, y_train, X_valid, y_valid\n\nattr = attr_cat + attr_num\n\nX_train = housing_train[attr]\ny_train = housing_train['SalePrice']\nX_valid = housing_valid[attr]\ny_valid = housing_valid['SalePrice']\n\nX_train_preprocessed = preprocessing_pipeline.fit_transform(X_train)\nX_valid_preprocessed = preprocessing_pipeline.transform(X_valid)\n\n\nxgb_reg.fit(X_train_preprocessed, y_train)\n\nfeature_importances = xgb_reg.feature_importances_\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\ndef indices_of_top_k(arr, k):\n    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n\nclass TopFeatureSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, feature_importances, k):\n        self.feature_importances = feature_importances\n        self.k = k\n    def fit(self, X, y=None):\n        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)\n        return self\n    def transform(self, X):\n        return X[:, self.feature_indices_]","d593bfda":"# Creating whole pipeline by adding feature selection to the pipeline\n\nestimator = Pipeline([\n    ('preprocessing', preprocessing_pipeline),\n    ('feature_selection', TopFeatureSelector(feature_importances, k=20)),\n    ('inference', xgb_reg)\n])\n\n# preprocessing parameter grid\nparam_dist = {'strategy': ['most_frequent', 'median', 'mean']\n             }\n\npreprocessing_param_grid = {'preprocessing__num__impute__'+k:v for k,v in param_dist.items()}\n\n\n# feature selection parameter grid\nparam_dist = {'k': stats.randint(10, 30),\n             }\n\nfeature_selection_param_grid = {'feature_selection__'+k:v for k,v in param_dist.items()}\n\n\n# inference parameter grid\nparam_dist = {'n_estimators': stats.randint(1000, 4000),\n              'learning_rate': stats.expon(0.005, 0.1),\n             }\n\ninference_param_grid = {'inference__'+k:v for k,v in param_dist.items()}\n\nparam_grid = {**preprocessing_param_grid, **feature_selection_param_grid, **inference_param_grid}\n\nestimator_opt = RandomizedSearchCV(estimator, param_grid, n_jobs = -1,n_iter = 50, scoring = 'neg_mean_absolute_error', cv = 5, random_state = 123, return_train_score = True)\n\n\nestimator_opt.fit(X_train, y_train)\npreds = estimator_opt.predict(X_valid)\n\nfrom sklearn.metrics import mean_squared_error\n\n## Inference on test set\n\nX_test = pd.read_csv(TEST_DIR, index_col = 'Id')\n\ntest_preds = estimator_opt.predict(X_test)\n\nsubmission = pd.DataFrame({'Id': X_test.index , 'SalePrice' : np.exp(test_preds)})\n\nsubmission.to_csv('submission.csv', index = False)","97b4f457":"The way to go when it comes to visualizing categorical data is to perform count plots and see how each category is distributed.\nAs you will see in the plots below, some of the categorical features do not provide useful information since their entries belong predominantly to one category.\nThose features are dropped.","a0254770":"In the last part of the code above, we have created a custom transformer. It has mainly two parameters :\n\n* feature_importances array\n* k an integer\n\nThis transformer keeps the top-k most important features based on the feature_importances array. This will help us have a lighter model and protect us from overfitting.","daf260f9":"The aim of this notebook is to provide a base code presenting best practices for Data Analysis, Data preprocessing, Machine Learning Pipelines and Model Selection. We use the famous kaggle house prices dataset to illustrate these concepts.\n\nThe outline of the Notebook is as follows:\n\n* 1. Loading the data\n* 2. Data Visualization - Univariate Analysis\n* 3. Data Visualization - Bivariate Analysis\n* 4. Machine Learning Pipelines and Model Selection","adc58b62":"### 2.1. Analysing categorical features","ff164223":"### 3.1. Correlation Matrix","d9df96f5":"List of columns with too many outliers\n\n- LotArea\n- OpenPorchSF\n- WoodDeckSF\n- BsmtUnfSF\n- BsmtFinSF1\n- LotFrontage\n- GarageArea\n- TotalBsmtSF\n- GlivArea\n- MasVnrArea","c794721a":"## 1. Loading the data","e5cf4db2":"## 3. Bivariate Analysis","27f8fd9f":"Visualizing distributions for numerical features is essential. A lot of Machine Learning models assume that features have a gaussian distributions. On the other hand, ensembling methods such as RandomForests and XGBoost are not sensitive to that assumption. Many numerical features do not provide useful insight about the data. Those features are either capped to a max(or min) value or have an almost constant value. They will be deleted.","389c2de4":"### 3.2. Scatter plot","453e4ebf":"## 2. Univariate Analysis","34e9c530":"We would like to know which features have missing values the most. Then we are going to drop those where more than 40% of the data is unavailable","2f813b4e":"Numerical and Categorical Features are processed differently. We will create a tranformation pipeline for each. Then we are going to merge the pipelines using sklearn's ColumnTransformer. After that, we fit our transformed data to an XGBRegressor to have the feature importances.","6aacd92d":"## 4. Modeling","94a1c668":"Boxplot allow us to have a better idea about the presence of outliers and how badly they may affect our predictions later.","062f8614":"We start by loading the training data (i.e. data for which we have labels). Then we drop the samples for which we do not have the target value.","29475147":"Correlation matrices allow to visually inspect possible correlations between features. If correlation exists, it may be wise to keep only one of the correlated features. This will help us avoid overfitting and have a lighter model later for the machine learning part.\n\n**Caution :** Correlation matrix won't help us detect non-linear or multi-variate feature relations though.","2d070eb5":"# House price prediction","6d871272":"Before going any further, we start by splitting the training data into a training set and a validation set.\nCategorical and numerical features have different characteristics. Thus, we'll perform different visualizations and analysis for each. ","01855a9d":"**Whole pipeline with estimator included**","25771f2c":"### 2.2. Analysing numerical features"}}