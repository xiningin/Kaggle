{"cell_type":{"3c7a486d":"code","78bc8594":"code","f2bfefe0":"code","0f6dcaad":"code","4a5392dc":"code","e4ab4646":"code","64da02b3":"code","d54d3ea7":"code","ec580c46":"code","161e9e2f":"code","8233d497":"code","915cd880":"code","6135aa35":"code","6b36f32a":"code","c1905e0a":"code","4682b339":"code","692e00c5":"code","772d3a55":"code","92457ce3":"code","506acc9c":"code","839e3144":"code","c87abed0":"code","87a425cf":"code","30390006":"code","6a0ab477":"code","35be86d5":"code","77facca9":"code","8c921d71":"code","fd23f702":"code","32798644":"code","12d56c6c":"code","174dbdc0":"code","f3222d17":"code","9b6d8a93":"code","2286cf7e":"code","7f2e27cf":"code","de0c46bf":"code","189ade7f":"code","e567593e":"code","4b3414f3":"code","8ffe4d91":"code","1b83bb5f":"markdown","911c5584":"markdown","7715de73":"markdown","16b6cfd8":"markdown","db0687cb":"markdown"},"source":{"3c7a486d":"!pip install tldextract","78bc8594":"import numpy as np\nimport json\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\nimport gc\nimport random\nimport os\nimport pickle\nimport tensorflow as tf\nfrom tensorflow.python.util import deprecation\nfrom urllib.parse import urlparse\nimport tldextract","f2bfefe0":"from sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import models, layers, backend, metrics\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.utils.vis_utils import plot_model\nfrom PIL import Image\nfrom sklearn.metrics import confusion_matrix, classification_report","0f6dcaad":"# set random seed\nos.environ['PYTHONHASHSEED'] = '0'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nnp.random.seed(0)\nrandom.seed(0)\ntf.set_random_seed(0)","4a5392dc":"# other setup\n%config InlineBackend.figure_format = 'retina'\npd.set_option('max_colwidth', 500)\ndeprecation._PRINT_DEPRECATION_WARNINGS = False","e4ab4646":"# load data\ndata = pd.read_csv('..\/input\/data.csv')\n# shuffle data\ndata = data.sample(frac=1, random_state=0)\nprint(f'Data size: {data.shape}')\ndata.head()","64da02b3":"val_size = 0.2\ntrain_data, val_data = train_test_split(data, test_size=val_size, stratify=data['label'], random_state=0)\nprint(f'Train shape: {train_data.shape}, Validation shape: {val_data.shape}')","d54d3ea7":"data.label.value_counts().plot.barh()\nplt.title('All Data')\nplt.show()","ec580c46":"good, bad = data.label.value_counts()\nprint(f'Ratio of data between target labels (bad & good) is {bad\/\/bad}:{good\/\/bad}')","161e9e2f":"def parsed_url(url):\n    # extract subdomain, domain, and domain suffix from url\n    # if item == '', fill with '<empty>'\n    subdomain, domain, domain_suffix = ('<empty>' if extracted == '' else extracted for extracted in tldextract.extract(url))\n    return [subdomain, domain, domain_suffix]","8233d497":"def extract_url(data):\n    # parsed url\n    extract_url_data = [parsed_url(url) for url in data['url']]\n    extract_url_data = pd.DataFrame(extract_url_data, columns=['subdomain', 'domain', 'domain_suffix'])\n    # concat extracted feature with main data\n    data = data.reset_index(drop=True)\n    data = pd.concat([data, extract_url_data], axis=1)\n    return data","915cd880":"train_data = extract_url(train_data)\nval_data = extract_url(val_data)\nprint(val_data.head())","6135aa35":"def plot(train_data, val_data, column):\n    plt.figure(figsize=(10, 17))\n    plt.subplot(411)\n    plt.title(f'Train data {column}')\n    plt.ylabel(column)\n    train_data[column].value_counts().head(10).plot.barh()\n    plt.subplot(412)\n    plt.title(f'Validation data {column}')\n    plt.ylabel(column)\n    val_data[column].value_counts().head(10).plot.barh()\n    plt.subplot(413)\n    plt.title(f'Train data {column} (groupped)')\n    plt.ylabel(f'(label, {column})')\n    train_data.groupby('label')[column].value_counts().head(10).plot.barh()\n    plt.subplot(414)\n    plt.title(f'Validation data {column} (groupped)')\n    plt.ylabel(f'(label, {column})')\n    val_data.groupby('label')[column].value_counts().head(10).plot.barh()\n    plt.show()","6b36f32a":"plot(train_data, val_data, 'subdomain')","c1905e0a":"plot(train_data, val_data, 'domain')","4682b339":"plot(train_data, val_data, 'domain_suffix')","692e00c5":"train_data[(train_data['domain'] == 'google') & (train_data['label'] == 'bad')].head()","772d3a55":"train_data[(train_data['domain'] == 'twitter') & (train_data['label'] == 'bad')].head()","92457ce3":"tokenizer = Tokenizer(filters='', char_level=True, lower=False, oov_token=1)\n# fit only on training data\ntokenizer.fit_on_texts(train_data['url'])\nn_char = len(tokenizer.word_index.keys())\nprint(f'N Char: {n_char}')","506acc9c":"train_seq = tokenizer.texts_to_sequences(train_data['url'])\nval_seq = tokenizer.texts_to_sequences(val_data['url'])\nprint('Before tokenization: ')\nprint(train_data.iloc[0]['url'])\nprint('\\nAfter tokenization: ')\nprint(tokenizer.word_index)","839e3144":"print(val_seq[0])\n","c87abed0":"sequence_length = np.array([len(i) for i in train_seq])\nsequence_length = np.percentile(sequence_length, 99).astype(int)\nprint(f'Sequence length: {sequence_length}')","87a425cf":"train_seq = pad_sequences(train_seq, padding='post', maxlen=sequence_length)\nval_seq = pad_sequences(val_seq, padding='post', maxlen=sequence_length)\nprint('After padding: ')\nprint(train_seq[0])\nprint(val_seq[0])","30390006":"train_seq = train_seq \/ n_char\nval_seq = val_seq \/ n_char","6a0ab477":"with open('tokenizer.josn', 'wb') as f:\n    pickle.dump(tokenizer, f)\n    \n   ","35be86d5":"def encode_label(label_index, data):\n    try:\n        return label_index[data]\n    except:\n        return label_index['<unknown>']","77facca9":"unique_value = {}\nfor feature in ['subdomain', 'domain', 'domain_suffix']:\n    # get unique value\n    label_index = {label: index for index, label in enumerate(train_data[feature].unique())}\n    # add unknown label in last index\n    label_index['<unknown>'] = list(label_index.values())[-1] + 1\n    # count unique value\n    unique_value[feature] = label_index['<unknown>']\n    # encode\n    train_data.loc[:, feature] = [encode_label(label_index, i) for i in train_data.loc[:, feature]]\n    val_data.loc[:, feature] = [encode_label(label_index, i) for i in val_data.loc[:, feature]]\n\n    # save label index\n    with open(f'{feature}.pkl', 'wb') as f:\n        pickle.dump(label_index, f)\n\n    with open(f'{feature}.json','w') as file:\n        json.dump(label_index,file)","8c921d71":"with open('domain_suffix.pkl', 'rb') as f:\n    data = pickle.load(f)","fd23f702":"for data in [train_data, val_data]:\n    data.loc[:, 'label'] = [0 if i == 'good' else 1 for i in data.loc[:, 'label']]","32798644":"print(f\"Unique subdomain in Train data: {unique_value['subdomain']}\")\nprint(f\"Unique domain in Train data: {unique_value['domain']}\")\nprint(f\"Unique domain suffix in Train data: {unique_value['domain_suffix']}\")","12d56c6c":"def convolution_block(x):\n    # 3 sequence conv layer\n    conv_3_layer = layers.Conv1D(64, 3, padding='same', activation='elu')(x)\n    # 5 sequence conv layer\n    conv_5_layer = layers.Conv1D(64, 5, padding='same', activation='elu')(x)\n    # concat conv layer\n    conv_layer = layers.concatenate([x, conv_3_layer, conv_5_layer])\n    # flatten\n    conv_layer = layers.Flatten()(conv_layer)\n    return conv_layer","174dbdc0":"def embedding_block(unique_value, size):\n    input_layer = layers.Input(shape=(1,))\n    embedding_layer = layers.Embedding(unique_value, size, input_length=1)(input_layer)\n    return input_layer, embedding_layer","f3222d17":"def create_model(sequence_length, n_char, n_subdomain, n_domain, n_domain_suffix):\n    input_layer = []\n    # sequence input layer\n    sequence_input_layer = layers.Input(shape=(sequence_length,))\n    input_layer.append(sequence_input_layer)\n    # convolution block\n    char_embedding = layers.Embedding(n_char + 1, 32, input_length=sequence_length)(sequence_input_layer)\n    conv_layer = convolution_block(char_embedding)\n    # entity embedding\n    entity_embedding = []\n    for n in [n_subdomain, n_domain,n_domain_suffix]:\n        size = 4\n        input_l, embedding_l = embedding_block(n, size)\n        embedding_l = layers.Reshape(target_shape=(size,))(embedding_l)\n        input_layer.append(input_l)\n        entity_embedding.append(embedding_l)\n    # concat all layer\n    fc_layer = layers.concatenate([conv_layer, *entity_embedding])\n    fc_layer = layers.Dropout(rate=0.5)(fc_layer)\n    # dense layer\n    fc_layer = layers.Dense(128, activation='elu')(fc_layer)\n    fc_layer = layers.Dropout(rate=0.2)(fc_layer)\n    # output layer\n    output_layer = layers.Dense(1, activation='sigmoid')(fc_layer)\n    model = models.Model(inputs=input_layer, outputs=output_layer)\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[metrics.Precision(), metrics.Recall()])\n    return model","9b6d8a93":"# reset session\nbackend.clear_session()\nos.environ['PYTHONHASHSEED'] = '0'\nnp.random.seed(0)\nrandom.seed(0)\ntf.set_random_seed(0)\n# create model\nmodel = create_model(sequence_length, n_char, unique_value['subdomain'], unique_value['domain'], unique_value['domain_suffix'])\nmodel.summary()","2286cf7e":"plot_model(model, to_file='model.png')\nmodel_image = mpimg.imread('model.png')\nplt.figure(figsize=(75, 75))\nplt.imshow(model_image)\nplt.show()","7f2e27cf":"train_x = []\ntrain_y = train_data['label']\nval_x = [val_seq, val_data['subdomain'], val_data['domain'], val_data['domain_suffix']]\nval_y = val_data['label']","de0c46bf":"train_x = [train_seq, train_data['subdomain'], train_data['domain'], train_data['domain_suffix']]\ntrain_y = train_data['label'].values\n\nearly_stopping = [EarlyStopping(monitor='val_precision', patience=5, restore_best_weights=True, mode='max')]\nhistory = model.fit(train_x, train_y, batch_size=64, epochs=25, verbose=1, validation_split=0.2, shuffle=True, callbacks=early_stopping)\nmodel.save('model.h5')","189ade7f":"plt.figure(figsize=(20, 5))\nfor index, key in enumerate(['loss', 'precision', 'recall']):\n    plt.subplot(1, 3, index+1)\n    plt.plot(history.history[key], label=key)\n    plt.plot(history.history[f'val_{key}'], label=f'val {key}')\n    plt.legend()\n    plt.title(f'{key} vs val {key}')\n    plt.ylabel(f'{key}')\n    plt.xlabel('epoch')","e567593e":"val_pred = model.predict(val_x)","4b3414f3":"data = parsed_url(\"drive.google.com\/uc?export=download&amp;id=0B7XzN8DNbJKiQlFNRHdVTmpCd0U\")\nprint(data)\nprint(encode_label(label_index,'com'))\nprint(encode_label(label_index,'drive'))\nprint(encode_label(label_index,'google'))\n\ndata = parsed_url(\"en.wikipedia.org\/wiki\/Claude_Lemieux\t\")\nprint(data)\nprint(encode_label(label_index,'en'))\nprint(encode_label(label_index,'wikipedia'))\nprint(encode_label(label_index,'org'))","8ffe4d91":"val_pred = model.predict(val_x)\n\nval_pred = np.where(val_pred[:, 0] >= 0.5, 1, 0)\nprint(f'Validation Data:\\n{val_data.label.value_counts()}')\nprint(f'\\n\\nConfusion Matrix:\\n{confusion_matrix(val_y, val_pred)}')\nprint(f'\\n\\nClassification Report:\\n{classification_report(val_y, val_pred)}')","1b83bb5f":"# Create CNN Model","911c5584":"# Model Training","7715de73":"In this notebook the validation method used is the holdout method. The holdout method is a method that separates training and test data by 80% and 20%","16b6cfd8":"# Load Data","db0687cb":"# Model Validation"}}