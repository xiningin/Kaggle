{"cell_type":{"420a59d3":"code","51b4377a":"code","ce5be7bf":"code","62bcc8d9":"code","0c2126e8":"code","be8d66bb":"code","605be996":"code","2ae29235":"code","b0e17e08":"code","904d11fb":"code","bddefcbf":"code","b9748f57":"code","0c93ede4":"code","5fd1984b":"code","9770a3b2":"code","f196841b":"code","88a90d1e":"code","d1585f68":"code","f866b05d":"code","617e5a95":"markdown","7b9f89cf":"markdown","b040b907":"markdown","6b46dcd5":"markdown","f83264a2":"markdown","641bedbc":"markdown"},"source":{"420a59d3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math, copy, time, os\n\nfrom kaggle.competitions import nflrush\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, BaggingRegressor, GradientBoostingRegressor, VotingRegressor\nfrom sklearn.linear_model import Lasso, ElasticNet, LogisticRegression, Ridge\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.svm import LinearSVR\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n%matplotlib inline\n\npd.set_option('mode.chained_assignment', None)\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n","51b4377a":"def grid_plot_histograms(df,grid_columns = 3,sample_size = 5000,figsize = (16,8)):\n    \"\"\"I created this setup to flexibly create subplots of histograms.\"\"\"\n    features = list(df.columns)\n    grid_count = len(features)\n\n    fig, axarr = plt.subplots(math.ceil(grid_count\/grid_columns),grid_columns,figsize = figsize)\n    \n    # this uses the iterator \"i\" to determine which subplot to fill\n    for i, feature in enumerate(features):\n        axarr[math.floor(i\/grid_columns)][i%grid_columns].hist(df[feature].sample(sample_size),bins = 30);\n        axarr[math.floor(i\/grid_columns)][i%grid_columns].set_title('{} \\n {}'.format(feature,feature_dict[feature]));\n    \n    # This uses i again to delete the remaining subplots\n    for i in range(grid_count,math.ceil(grid_count\/grid_columns)*grid_columns):\n        fig.delaxes(axarr[math.floor(i\/grid_columns)][i%grid_columns]);\n    \n    plt.tight_layout();","ce5be7bf":"train = pd.read_csv('\/kaggle\/input\/nfl-big-data-bowl-2020\/train.csv', low_memory = False)","62bcc8d9":"dist = train[\"Yards\"].hist(density = True, cumulative = True, bins = 200)\nplt.title('Cumulative Distribution - Yards');","0c2126e8":"#Datatypes\n\ndtypes = train.dtypes\nint_feats = list(dtypes[dtypes =='int64'].index)\nfloat_feats = list(dtypes[dtypes =='float64'].index)\nobj_feats = list(dtypes[dtypes =='object'].index)\ndtypes.value_counts()","be8d66bb":"feature_dict = {\n    'GameId':'a unique game identifier',\n    'PlayId':'a unique play identifier',\n    'Team':'home or away',\n    'X' : 'player position along the long axis of the field.',\n    'Y' : 'player position along the short axis of the field.',\n    'S' :'speed in yards\/second',\n    'A' : 'acceleration in yards\/second^2',\n    'Dis' : 'distance traveled from prior time point, in yards',\n    'Orientation' : 'orientation of player (deg)',\n    'Dir' : 'angle of player motion (deg)',\n    'NflId': 'a unique identifier of the player',\n    'DisplayName':' player\\'s name',\n    'JerseyNumber':'jersey number',\n    'Season' : 'year of the season',\n    'YardLine' : 'the yard line of the line of scrimmage',\n    'Quarter' : 'game quarter (1-5, 5 == overtime)',\n    'GameClock' : 'time on the game clock',\n    'PossessionTeam' : 'team with possession',\n    'Down' : 'the down (1-4)',\n    'Distance' : 'yards needed for a first down',\n    'FieldPosition' : 'which side of the field the play is happening on',\n    'HomeScoreBeforePlay' : 'home team score before play started',\n    'VisitorScoreBeforePlay' :'visitor team score before play started',\n    'NflIdRusher' : 'the NflId of the rushing player',\n    'OffenseFormation' : 'offense formation',\n    'OffensePersonnel' : 'offensive team positional grouping',\n    'DefendersInTheBox' : 'number of defenders lined up near the line',\n    'DefensePersonnel' : 'defensive team positional grouping',\n    'PlayDirection' : 'direction the play is headed',\n    'TimeHandoff' :'UTC time of the handoff',\n    'TimeSnap' : 'UTC time of the snap',\n    'Yards' : 'the yardage gained on the play',\n    'PlayerHeight' :'player height (ft-in)',\n    'PlayerWeight' : 'player weight (lbs)',\n    'PlayerBirthDate' : 'birth date (mm\/dd\/yyyy)',\n    'PlayerCollegeName' : 'where the player attended college',\n    'HomeTeamAbbr' : 'home team abbreviation',\n    'VisitorTeamAbbr' : 'visitor team abbreviation',\n    'Week' : 'week into the season',\n    'Stadium' : 'stadium where the game is being played',\n    'Location' : 'city where the game is being player',\n    'StadiumType' : 'description of the stadium environment',\n    'Turf' : 'description of the field surface',\n    'GameWeather' : 'description of the game weather',\n    'Temperature' : 'temperature (deg F)',\n    'Humidity' : 'humidity',\n    'WindSpeed' : 'wind speed in miles\/hour',\n    'WindDirection' : 'wind direction',\n    'SecondsToHandoff' : 'Custom Feature: TimeHandoff minus TimeSnap in seconds'\n    }","605be996":"grid_plot_histograms(train[float_feats].dropna(),3,5000,(16,10))","2ae29235":"# A few examples to get familiar\ntrain[float_feats].head(8).transpose()","b0e17e08":"grid_plot_histograms(train[int_feats],4,5000,(16,10))","904d11fb":"# A few examples to get familiar\ntrain[int_feats].head(8).transpose()","bddefcbf":"def time_seconds(x):\n    x = time.strptime(x[:-5], \"%Y-%m-%dT%H:%M:%S\")\n    x = time.mktime(x)\n    return x\n\ndef create_cdf(y):\n    y_cdf = copy.deepcopy(y.to_frame())\n    y_cdf.columns = ['Yards']\n    y_cdf.head()\n    for i in list(range(-99,100)):\n    #for i in list(range(0,10)):    \n        y_cdf['Yards' + str(i)] = y_cdf['Yards'].apply(lambda x: 1 if i >= x else 0)\n    y_cdf.drop('Yards',1,inplace = True)\n    #print('y_cdf.shape {}'.format(y_cdf.shape))\n    y_cdf.head(3)\n    return y_cdf\n\ndef add_custom_feats(train):\n    train = train.loc[train['NflId']==train['NflIdRusher']]\n    train['SecondsToHandoff'] = train['TimeHandoff'].apply(time_seconds) - train['TimeSnap'].apply(time_seconds)\n    train['OffenseTeam'] = train['PossessionTeam']\n    train['adj_height'] = train['PlayerHeight'].apply(lambda x: int(x[0])*12 + int(x[2:4]))\n    train['OffenseHome'] = train[['OffenseTeam','HomeTeamAbbr']].apply(lambda x: 1 if x[0] == x[1] else 0, axis = 1)\n    train['DefenseTeam'] = train[['OffenseHome','HomeTeamAbbr','VisitorTeamAbbr']].apply(lambda x: x[2] if x[0] == 1 else x[1], axis = 1)\n    train['OffenseLead'] = train[['OffenseHome','HomeScoreBeforePlay','VisitorScoreBeforePlay']].apply(lambda x: x[1]-x[2] if x[0] == 1 else x[2]-x[1], axis = 1)\n    train['YardsToGo'] = train[['FieldPosition','OffenseTeam','YardLine']].apply( \\\n        lambda x: (50-x['YardLine'])+50 if x['OffenseTeam']==x['FieldPosition'] else x['YardLine'],1)\n    train['SecondsToHandoff'] = train['TimeHandoff'].apply(time_seconds) - train['TimeSnap'].apply(time_seconds)\n    train['turf'] = train['Turf'].apply(lambda x: int('turf' in x.lower() or 'artific' in x.lower()))\n    train['quarter_seconds_left'] = train['GameClock'].apply(lambda x: int(x[0:2])*60 + int(x[3:5]))\n    train['game_seconds_left'] = train['Quarter'].map({1:2700, 2:1800, 3:900, 4:0}) + train['quarter_seconds_left']\n    train['game_seconds_passed'] = 3600 - train['game_seconds_left']\n    train['OffensePoints'] = train[['OffenseHome','HomeScoreBeforePlay','VisitorScoreBeforePlay']].apply(lambda x: x[1] if x[0] == 1 else x[2], axis = 1)\n    train['OffensePointsPerMinute'] = train['OffensePoints'] \/ train['game_seconds_passed']*60\n    return train\n\ndef add_yard_bins(test_df, test_data, bins = [0,2,5,8,10,15,20,30,40,50,60,70,80,100]):\n    # Add yard_bins\n    test_df['YardsToGo'] = test_df[['FieldPosition','OffenseTeam','YardLine']].apply( \\\n        lambda x: (50-x['YardLine'])+50 if x['OffenseTeam']==x['FieldPosition'] else x['YardLine'],1)\n    yard_bins = pd.get_dummies(pd.cut(test_df['YardsToGo'],bins = bins))\n    yard_bins.columns = [str(i) for i in yard_bins.columns.tolist()]\n    test_data = pd.merge(test_data,yard_bins,left_index = True, right_index = True)\n    test_data.columns = [str(x).replace(']','').replace('(','') for x in test_data.columns]\n    return test_data\n\ndef add_categories(train_data, cat_feats, top_n_categories = 120, count_min = 50):\n    #print('train_data shape: {}'.format(train_data.shape))\n    categorical_stats = pd.DataFrame()\n    for i in cat_feats:\n        stats = train[[i,'Yards']].groupby(i).agg(['mean','count','std','max','min'])\n        stats.columns = stats.columns.droplevel(0)\n        stats['feature'] = i\n        stats['feature_type'] = stats.index.values\n        stats.reset_index(inplace = True)\n        stats.drop(i,1,inplace = True)\n        stats = stats[['feature','feature_type','mean','count','std','max','min']]\n        categorical_stats = categorical_stats.append(stats)\n\n    # I basically just made up this calculation to select the most significant categorical features-values, and it seems to work well\n    categorical_stats['mean_difference'] = (categorical_stats['mean'] - 4.21).abs()\n    categorical_stats['significance'] = categorical_stats['mean_difference'] * categorical_stats['count']**0.2\n\n    cat_feats_keep = categorical_stats[categorical_stats['count']>count_min] \\\n            .sort_values('significance',ascending = False).head(top_n_categories)\n    \n    #print('train_data shape: {}'.format(train_data.shape))\n    for feature in set(cat_feats_keep['feature']):\n        feature_types = cat_feats_keep[cat_feats_keep['feature']==feature]['feature_type'].tolist()\n        feature_dummies = pd.get_dummies(train[feature].apply(lambda x: 0 if x not in feature_types else x),prefix = feature)\n        feature_dummies.drop(feature + '_0',1,inplace = True)\n        train_data = pd.merge(train_data,feature_dummies,left_index = True, right_index = True)\n#        print('{} Added: new train_data shape: {}'.format(feature,train_data.shape))    \n    return train_data, cat_feats_keep\n\ndef add_player_feats_df(train_data, train_original, top_n_player_counts = 500, top_n_players_significance = 100):\n    players = train_original[['NflId','Yards']].groupby('NflId').agg(['mean','count'])\n    players.columns = ['mean','count']\n    players['weight'] = players['count']**.2\n    players['difference'] = players['mean'] - 4.21\n    players['significance'] = (players['difference'] * players['weight']).abs()\n    #players['significance'] = players['count']\n    players_keep = players\n    players_keep = players_keep.sort_values('count',ascending = False).head(top_n_player_counts)\n    players_keep = players_keep.sort_values('significance',ascending = False).head(top_n_players_significance)\n    #### Creating player_feats (get_dummies) for Top Players\n    player_list = list(players_keep.index)\n    player_feats = train_original[['PlayId','NflId']][train_original['NflId'].isin(player_list)]\n    player_feats.set_index('PlayId',inplace = True)\n    player_feats = pd.get_dummies(player_feats['NflId'], prefix = 'NflId')\n    player_feats = player_feats.groupby(player_feats.index).sum()\n    train_data = pd.merge(train_data, player_feats, left_index = True, right_index = True, how = 'left')\n    train_data.fillna(0,inplace = True)\n    return train_data, player_list\n\ndef array_in_range(pred):\n    ones = np.ones(pred.shape)\n    zeros = np.zeros(pred.shape)\n    pred = np.maximum(pred,zeros)\n    pred = np.minimum(pred,ones)\n    return pred\n\ndef array_increasing(pred):\n    for i in range(1,pred.size):\n        pred[0][i] = max(pred[0][i],pred[0][i-1])\n    return pred\n\ndef model_scores(models, X_train, y_train, X_test, y_test, params, description):\n    model_results = pd.DataFrame()\n    model_data = {}\n    for model in models:    \n        model.fit(X_train, y_train)\n        model_name = model.__class__.__name__\n        test_predictions = model.predict(X_test)\n        train_predictions = model.predict(X_train)\n        test_score = round(mean_squared_error(y_test, test_predictions),4)\n        train_score = round(mean_squared_error(y_train, train_predictions),4)\n        test_mae = round(mean_absolute_error(y_test, test_predictions),4)\n        train_mae = round(mean_absolute_error(y_train, train_predictions),4)\n        model_data['model_name'] = [model_name]\n        model_data['test_score'] = [test_score]\n        model_data['MAE'] = [test_mae]\n        model_data['params'] = [str(params)]\n        model_data['feature_count'] = [X_train.shape[1]]\n        model_data['description'] = [description]\n        model_data['model'] = [model]\n        model_data['train_score'] = [train_score]\n        print('{} MSE: {}'.format(model_name,test_score))\n        print('{} MAE: {}'.format(model_name,test_mae))\n        model_results = model_results.append(pd.DataFrame.from_dict(model_data, orient = 'columns'))\n    model_results.sort_values('test_score', ascending = True, inplace = True)\n    return model_results\n\ndef transform_pred_cdf(prediction,sample_prediction_df):\n    prediction = array_increasing(array_in_range(prediction))\n    pred_target = pd.DataFrame(index = sample_prediction_df.index, \\\n                               columns = sample_prediction_df.columns, \\\n                               data = prediction)\n    return pred_target\n\ndef dummy_all(train_data, train, features):\n    for i in features:\n        train_data = pd.merge(train_data, pd.get_dummies(train[i], prefix = i), left_index = True, right_index = True)\n    return train_data\n\ndef fit_transform_num_feats(train, num_feats, scaler, imp):\n    train_data = train[num_feats]\n    train_data = pd.DataFrame(index = train_data.index, columns = train_data.columns, data = scaler.fit_transform(train_data))\n    train_data = pd.DataFrame(index = train_data.index, columns = train_data.columns, data = imp.fit_transform(train_data))\n    return train_data\n\ndef print_parameters(params, num_feats, bins, cat_feats):\n    print('params:{} - num_feats:{} - bins:{} - cat_feats:{}'.format(params,num_feats,bins,cat_feats))\n\ndef add_team_stats(train,train_original):\n    trainXY = copy.deepcopy(train[['X','Y','S','A','Position','Team']])\n    origXY = copy.deepcopy(train_original[['X','Y','PlayId','S','A','Position','Team']])\n    origXY.set_index('PlayId', inplace = True)\n    trainXY.columns = ['train_' + x for x in trainXY.columns]\n    #print(set(train_original['Position']))\n    locations = pd.merge(trainXY,origXY, left_index = True, right_index = True, how = 'left')\n    locations['PlayerDistance'] = locations.apply \\\n        (lambda x: np.sqrt(np.square(x['train_X'] - x['X']) +  np.square(x['train_Y'] - x['Y'])), axis = 1)\n    locations.reset_index(inplace = True)\n    distances = locations.groupby(['PlayId','Team'])['PlayerDistance','S','A'].mean().reset_index()\n    team_stats = distances.pivot_table(index = ['PlayId'], columns = ['Team'], values = ['PlayerDistance','S','A'])\n    team_stats.columns = ['%s%s' % (a, '|%s' % b if b else '') for a, b in team_stats.columns]\n    team_stats['speed_difference'] = team_stats['S|home'] - team_stats['S|away']\n    train = pd.merge(train, team_stats,left_index = True, right_index = True)\n    return train, team_stats.columns\n\ndef add_position_stats(train,train_original):\n    trainXY = copy.deepcopy(train[['X','Y','S','A','Position','Team','OffenseDefense']])\n    origXY = copy.deepcopy(train_original[['X','Y','PlayId','S','A','Position','Team','OffenseDefense']])\n    origXY.set_index('PlayId', inplace = True)\n    trainXY.columns = ['train_' + x for x in trainXY.columns]\n    #print(set(train_original['Position']))\n    locations = pd.merge(trainXY,origXY, left_index = True, right_index = True, how = 'left')\n    locations['PlayerDistance'] = locations.apply \\\n        (lambda x: np.sqrt(np.square(x['train_X'] - x['X']) +  np.square(x['train_Y'] - x['Y'])), axis = 1)\n    locations.reset_index(inplace = True)\n\n    distances = locations.groupby(['PlayId','OffenseDefense','Position'])['PlayerDistance','S','A'].mean().reset_index()\n    team_stats = distances.pivot_table(index = ['PlayId'], columns = ['OffenseDefense','Position'], values = ['S'])\n    team_stats.columns = ['{}|{}|{}'.format(a,b,c) for a, b, c in team_stats.columns]\n    team_stats.fillna(0,inplace = True)\n    train = pd.merge(train, team_stats,left_index = True, right_index = True)\n    return train, team_stats.columns","b9748f57":"train_original = pd.read_csv('\/kaggle\/input\/nfl-big-data-bowl-2020\/train.csv', low_memory = False)\ntrain = copy.deepcopy(train_original[train_original['NflId']==train_original['NflIdRusher']])\ntrain.index = train['PlayId']","0c93ede4":"description_suffix = 0\nexperiments = pd.DataFrame()","5fd1984b":"### Train Feature Engineering\nscaler = StandardScaler()\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\n\n### Setup\ndescription_suffix += 1\n\ndescription = 'old team stats' + str(description_suffix)\ntop_n_player_counts = 900\ntop_n_players_significance = 120\ntop_n_categories = 120\ncount_min = 100\nbins = [0,2,4,6,8,10,15,20,30,40,50,60,70,80,90,100]\nparams = str((top_n_player_counts, top_n_players_significance, top_n_categories, count_min))\n\n##Custom Feats\ntrain = add_custom_feats(train)\n\n##Number Feats\ntrain, team_stats_columns = add_team_stats(train, train_original)\n#train, position_stats_columns = add_position_stats(train, train_original)\n\n# New stuff since last commit\noffense = ['RB','TE','WR','OL']\ndefense = ['DL','LB','DB']\n\nfor i in offense:\n    train[i] = train['OffensePersonnel'].apply(lambda x: int(x[x.find(i) - 2]) if x.find(i)>0 else 0)\nfor i in defense:\n    train[i] = train['DefensePersonnel'].apply(lambda x: int(x[x.find(i) - 2]) if x.find(i)>0 else 0)\n\n\nnum_feats = ['S', 'A', 'Dis', 'adj_height', 'Temperature', 'Humidity', \\\n'SecondsToHandoff', 'Distance', 'PlayerWeight', 'OffenseLead', 'quarter_seconds_left', \\\n'game_seconds_left', 'YardsToGo']\n#num_feats.extend(['S|defense', 'S|offense', 'speed_difference'])\nnum_feats.extend(team_stats_columns)\nnum_feats.extend(offense)\nnum_feats.extend(defense)\n\ntrain_data = fit_transform_num_feats(train, num_feats, scaler, imp)\n\n#Player Feats\ntrain_data, player_list = add_player_feats_df(train_data, train_original, top_n_player_counts, top_n_players_significance)\n\n#Yard Bins\ntrain_data = add_yard_bins(train, train_data, bins) #clean up columns\n\n# Categories\ncat_feats = ['DisplayName','PlayerCollegeName','OffensePersonnel','DefensePersonnel','Position', \\\n            'OffenseFormation','Down','OffenseTeam','NflIdRusher']\ntrain_data, cat_feats_keep = add_categories(train_data, cat_feats, top_n_categories, count_min)\n\n# Dummies:\n#train_data = dummy_all(train_data, train, ['DefendersInTheBox'])\n\n# Train\/Test Split\nX, y = train_data, train['Yards']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n\n# Exhaustive List\ntest_models = [\n    Ridge(),\n    Lasso(),\n    ElasticNet(),\n    Ridge(**{'alpha': 220, 'solver': 'lsqr'})\n    #LogisticRegression(),\n    #Ridge(),\n    #MLPRegressor(),\n    #inearSVR(),\n    #BaggingRegressor(),\n    #LGBMRegressor(),\n    #VotingRegressor([('r',Ridge()),('f',RandomForestRegressor())]),\n    #GradientBoostingRegressor(),\n    #XGBRegressor()\n    ]\n\nprint_parameters(params, num_feats, bins, cat_feats)\n\nmodel_results = model_scores(test_models, X_train, y_train, X_test, y_test, params, description)\nexperiments = experiments.append(model_results)\nexperiments.sort_values('test_score', ascending = True)","9770a3b2":"y_cdf = create_cdf(y)\nfinal_model = Ridge(**{'alpha': 220, 'solver': 'lsqr'}).fit(X, y_cdf)","f196841b":"# Setup\nenv = nflrush.make_env()\niter_test = env.iter_test()","88a90d1e":"def prep_prediction_submission(test_df, sample_prediction_df, final_model):\n    #(test_df, sample_prediction_df) = next(iter_test)\n    test_df_original = copy.deepcopy(test_df)\n    test_df = test_df.loc[test_df['NflId']==test_df['NflIdRusher']]\n    test_df = add_custom_feats(test_df)\n    test_df.set_index('PlayId',inplace = True) #only on test\n    test_df, test_df_columns = add_team_stats(test_df, test_df_original)\n    \n    offense = ['RB','TE','WR','OL']\n    defense = ['DL','LB','DB']\n    for i in offense:\n        test_df[i] = test_df['OffensePersonnel'].apply(lambda x: int(x[x.find(i) - 2]) if x.find(i)>0 else 0)\n    for i in defense:\n        test_df[i] = test_df['DefensePersonnel'].apply(lambda x: int(x[x.find(i) - 2]) if x.find(i)>0 else 0)\n        \n    test_data = test_df[num_feats]\n    test_data = pd.DataFrame(index = test_data.index, columns = test_data.columns, data = scaler.transform(test_data))\n    test_data = pd.DataFrame(index = test_data.index, columns = test_data.columns, data = imp.transform(test_data))\n    # Add categories\n    for feature in set(cat_feats_keep['feature']):\n        feature_types = cat_feats_keep[cat_feats_keep['feature']==feature]['feature_type'].tolist()\n        feature_dummies = pd.get_dummies(test_df[feature].apply(lambda x: 0 if x not in feature_types else x),prefix = feature)\n        if feature + '_0' in list(feature_dummies.columns):\n            feature_dummies.drop(feature + '_0',1,inplace = True)\n        if feature_dummies.shape[1] > 0:\n            test_data = pd.merge(test_data,feature_dummies,left_index = True, right_index = True)\n    # Add player feats\n    add_feats = set(test_df_original['NflId']).intersection(player_list)\n    for i in add_feats:\n        test_data[i] = 1\n    test_data = add_yard_bins(test_df, test_data,bins) #clean up columns\n    # Columns (missing and order)\n    new_columns = np.setdiff1d(list(train_data.columns),list(test_data.columns))\n    for i in new_columns:\n        test_data[i] = 0\n    test_data = test_data[train_data.columns]\n    prediction = final_model.predict(test_data)\n    pred_target = transform_pred_cdf(prediction,sample_prediction_df)\n    return pred_target\n","d1585f68":"progress_counter = 0\n\nfor (test_df, sample_prediction_df) in iter_test:\n    progress_counter+=1\n    if progress_counter%250 == 0:\n        print(progress_counter)\n    pred_target = prep_prediction_submission(test_df, sample_prediction_df, final_model)\n    env.predict(pred_target)","f866b05d":"env.write_submission_file()","617e5a95":"You can run tons of experiments here and keep track of performance of each model and the the parameters or other statistics in case you need to go back and use it again.","7b9f89cf":"It might be a bit harder to read this way, but I created all these functions so I could test different models and make small feature changes for quick experimentation.  Basically, all the functions allowed me to compile stats on different features... keep reading and you'll see what I mean.","b040b907":"# Experiment Tester","6b46dcd5":"# Data Pulls","f83264a2":"Just FYI - This is completely original work.","641bedbc":"# Functions"}}