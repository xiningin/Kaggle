{"cell_type":{"c8d04efc":"code","71b44055":"code","5e5aa749":"code","b4eb92c9":"code","171e6f3c":"code","abf6df66":"code","33cfe7f2":"code","b89f6d65":"code","eaa8c743":"code","1f68589b":"code","76d4d202":"code","b2076e74":"code","4b60a94a":"code","2443019f":"code","befe7c3c":"code","971e99e5":"code","55dd801d":"code","ae9bd213":"code","94448b9b":"code","e751f5e6":"code","7a51bdaa":"code","a617449e":"markdown","140f42e3":"markdown","2b20a234":"markdown","b7bd53fd":"markdown","1dbef4b0":"markdown","60306739":"markdown","d34931dd":"markdown","e48b8b73":"markdown","21c890ae":"markdown","ae5da077":"markdown"},"source":{"c8d04efc":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandas_profiling import ProfileReport\n\n\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","71b44055":"from sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\n\nfrom sklearn.metrics import mean_squared_error\n","5e5aa749":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\n\ntest_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')\nsub = pd.read_csv('..\/input\/30-days-of-ml\/sample_submission.csv')","b4eb92c9":"cat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]","171e6f3c":"print('Rows and Columns in train dataset:', train_df.shape)\nprint('Rows and Columns in test dataset:', test_df.shape)","abf6df66":"train_df.describe().T","33cfe7f2":"test_df.describe().T","b89f6d65":"train_df.drop(\"id\", axis=1, inplace=True)\ntest_df.drop(\"id\", axis=1, inplace=True)\n","eaa8c743":"report = ProfileReport(train_df)\nreport","1f68589b":"# Numerical features distribution \ni = 1\nplt.figure()\nfig, ax = plt.subplots(7, 2,figsize=(20, 24))\nfor feature in cont_features:\n    plt.subplot(7, 2,i)\n    sns.histplot(train_df[feature],color=\"purple\", kde=True,bins=100, label='train')\n    sns.histplot(test_df[feature],color=\"yellow\", kde=True,bins=100, label='test')\n    plt.xlabel(feature, fontsize=9); \n    plt.legend()\n    i += 1\n","76d4d202":"# Categorical features distribution \ni = 1\nplt.figure()\nfig, ax = plt.subplots(5, 2,figsize=(28, 44))\nfor feature in cat_features:\n    plt.subplot(5, 2,i)\n    sns.histplot(train_df[feature],color=\"purple\", label='train')\n    sns.histplot(test_df[feature],color=\"yellow\", label='test')\n    plt.xlabel(feature, fontsize=9); \n    plt.legend()\n    i += 1","b2076e74":"corr = train_df[cont_features+['target']].corr()\ncorr.style.background_gradient(cmap='hsv').set_precision(2)\n","4b60a94a":"# missing value\nprint('Missing values in train dataset:', sum(train_df.isnull().mean()*100))\nprint('Missing values in test dataset:', sum(test_df.isnull().mean()*100))","2443019f":"# duplicates data\ntrain_df.drop_duplicates()\ntrain_df.shape","befe7c3c":"x = train_df.drop(['target'], axis=1)\ny = train_df['target']\nX_test = test_df.copy()","971e99e5":"# data encoding\nfrom sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder()\nx[cat_features] = ordinal_encoder.fit_transform(x[cat_features])\nX_test[cat_features] = ordinal_encoder.transform(X_test[cat_features])\nx.head()","55dd801d":"# trai test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(x, y,test_size=0.2, random_state=22)\n","ae9bd213":"\nmodel_rf = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=30,\n           max_features='sqrt', max_leaf_nodes=None,\n           min_impurity_decrease=0.0, min_impurity_split=None,\n           min_samples_leaf=2, min_samples_split=5,\n           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n           oob_score=False, random_state=22, verbose=0, warm_start=False)\n\nmodel_rf.fit(X_train, y_train)\npred_Rand = model_rf.predict(X_valid)\nprint(mean_squared_error(y_valid, pred_Rand, squared=False))\n","94448b9b":"xgb_params = {'objective': 'reg:squarederror',\n              'n_estimators': 10000,\n              'learning_rate': 0.036,\n              'subsample': 0.926,\n              'colsample_bytree': 0.118,\n              'grow_policy':'lossguide',\n              'max_depth': 3,\n              'booster': 'gbtree', \n              'reg_lambda': 45.1,\n              'reg_alpha': 34.9,\n              'random_state': 42,\n              'reg_lambda': 0.00087,\n              'reg_alpha': 23.132}\n\nmodel_XGB = XGBRegressor(**xgb_params)\nmodel_XGB.fit(X_train, y_train) \npred_XGB = model_XGB.predict(X_valid)\nprint(mean_squared_error(y_valid, pred_XGB, squared=False))","e751f5e6":"pred = model_XGB.predict(X_test)\n\n# Save the predictions to a CSV file\nsub['target']=pred\nsub.to_csv('submission.csv', index=False)","7a51bdaa":"sub","a617449e":"# Submit to the competition","140f42e3":"## Random forest ","2b20a234":"# Preparation for Model","b7bd53fd":"## Xgboost","1dbef4b0":"# Pre Processing","60306739":"# Create Model\n","d34931dd":"# Hi  Welcome to this competition\n","e48b8b73":"# EDA","21c890ae":"## import libery","ae5da077":"# Import Date"}}