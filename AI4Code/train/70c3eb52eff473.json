{"cell_type":{"d1b15cc0":"code","89bca4e1":"code","45b103f5":"code","5d94be71":"code","bc1dd772":"code","20a9a7b5":"code","24e92d9d":"code","10a78963":"code","515d86d9":"code","e5ac5adc":"code","085964eb":"code","a492d9d4":"code","70a357c1":"code","b50f689d":"code","2ff55123":"code","c02e47f5":"code","76db0d36":"code","76240f5f":"code","46f8068e":"code","83a0f7bc":"code","9c20d876":"code","fce78ccd":"code","415648db":"code","ed552376":"code","4a2e5481":"code","53c8ad00":"code","7230514e":"code","ea955264":"code","483f64ff":"code","b343b1ac":"code","459de94d":"code","087cc0a2":"code","2bfd40b8":"code","c6cbf49a":"code","70ffdc4b":"code","1232bac4":"code","f2c76e89":"code","3a12a242":"code","8463c6b7":"code","95607001":"code","357efe84":"code","dfc5e00a":"code","3e838eab":"code","e0ac5fda":"code","8dbcd194":"code","56b2edfd":"code","17e7b27b":"markdown","0d8aa200":"markdown","1a3cb1e5":"markdown","1c7822c7":"markdown","9ec4049e":"markdown","5835af23":"markdown","ce20b650":"markdown","4d74ce7f":"markdown","dd30e051":"markdown","4fb05def":"markdown","29599760":"markdown","ac2422aa":"markdown","8919a6a7":"markdown","7b4573ae":"markdown","d0296ebc":"markdown","8e8b8584":"markdown","e7fc0dba":"markdown","43768f05":"markdown","9e2e12c2":"markdown","c6bc1cf3":"markdown","a3a710f6":"markdown","2f2bf7b4":"markdown","9f2fe350":"markdown","1ef0c5d9":"markdown","bf9d01f6":"markdown","0930bec6":"markdown","6ed80ce5":"markdown","c57b18b7":"markdown","fd410f75":"markdown","6ad1b259":"markdown","f2b323bb":"markdown"},"source":{"d1b15cc0":"#Importing All the libraries\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom math import sqrt\nfrom sklearn.model_selection import train_test_split\nregex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\nfrom sklearn.svm import SVR\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error","89bca4e1":"#mounting google drive to get dataset from drive\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n#reading the data \ndata=pd.read_csv('..\/input\/cancer-dataset-aggregated\/cancer_reg.csv', encoding='latin-1')  #Here Latin1 encodes only the first 256 code points of the Unicode character set.\ndata.head(10) #First 10 rows of dataset","45b103f5":"#Checking if dataset has any null values\n\ndata.isnull().sum()","5d94be71":"#Replacing Null values with mean of all values of PctEmployed16_Over column\nmean = data[\"PctEmployed16_Over\"].mean()\ndata[\"PctEmployed16_Over\"].fillna(mean,inplace=True)\nmean1 = data[\"PctPrivateCoverageAlone\"].mean()\ndata[\"PctPrivateCoverageAlone\"].fillna(mean1,inplace=True)\nmean3 = data[\"PctSomeCol18_24\"].mean()\ndata[\"PctSomeCol18_24\"].fillna(mean3,inplace=True)","bc1dd772":"#As 2 columns of dataset has object data type hence doing One Hot Encoding which is required for further analysis\n\nx1=pd.get_dummies(data['Geography'].apply(lambda x:x.split(',')[1]))\nx2=pd.get_dummies(data['binnedInc'])\n\ndef merg(df,data):\n  for col in df.columns:\n    data[col]=df[col]\n  return data\ndata=merg(x1,data)   \ndata=merg(x2,data)    ","20a9a7b5":"#Dropping 2 columns as their dummies are created\ndata.drop(columns=[\"Geography\", \"binnedInc\"], inplace=True, axis=1)","24e92d9d":"#dropping duplicate values\n\ndata.drop_duplicates()\ndata.shape","10a78963":"data.head(10)","515d86d9":"data.info()","e5ac5adc":"\n# Creating Correlation Matrix and plotting HeatMap\nplt.figure(figsize=(100,100))\nsns.heatmap(data.corr(),linewidths=0.30,cmap ='RdYlGn',annot=True)\nplt.show()","085964eb":"data.plot.scatter(x=\"TARGET_deathRate\", y=\"avgDeathsPerYear\", figsize=(10,8))\nplt.title(\"TARGET_deathRate VS avgDeathsPerYear\")\nplt.show()","a492d9d4":"data.plot.scatter(x=\"TARGET_deathRate\", y=\"incidenceRate\", figsize=(10,8))\nplt.title(\"TARGET_deathRate VS incidenceRate\")\nplt.show()","70a357c1":"data.plot.scatter(x=\"TARGET_deathRate\", y=\"medIncome\", figsize=(10,8))\nplt.title(\"TARGET_deathRate VS medIncome\")\nplt.show()","b50f689d":"data.plot.scatter(x=\"TARGET_deathRate\", y=\"PercentMarried\", figsize=(10,8))\nplt.title(\"TARGET_deathRate VS PercentMarried\")\nplt.show()","2ff55123":"data.plot.scatter(x=\"TARGET_deathRate\", y=\"PctEmpPrivCoverage\", figsize=(10,8))\nplt.title(\"TARGET_deathRate VS PctEmpPrivCoverage\")\nplt.show()","c02e47f5":"#Splitting data into train and test before featurization because to avoid overfitting\ndata.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in data.columns.values]\nX = data.drop('TARGET_deathRate',axis=1)\ny = data['TARGET_deathRate'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","76db0d36":"from sklearn.feature_selection import mutual_info_regression\n#determine the Mutual information\nmutual_info= mutual_info_regression(X_train.fillna(0), y_train)\nmutual_info","76240f5f":"mutual_info =pd.Series(mutual_info)\nmutual_info.index=X_train.columns\nmutual_info.sort_values(ascending=False)","46f8068e":"mutual_info.sort_values(ascending=False).plot.bar(figsize=(15,5)) #plotting the features in the descending order of their dependency values","83a0f7bc":"from sklearn.feature_selection import SelectKBest\n\n#Taking top 20 important features\nX_train_new= SelectKBest(mutual_info_regression, k=20).fit_transform(X_train.fillna(0),y_train)\nprint(X_train_new.shape,y_train.shape)","9c20d876":"mutual_info= mutual_info_regression(X_test.fillna(0), y_test)\nmutual_info","fce78ccd":"mutual_info =pd.Series(mutual_info)\nmutual_info.index=X_test.columns\nmutual_info.sort_values(ascending=False)","415648db":"mutual_info.sort_values(ascending=False).plot.bar(figsize=(15,5))","ed552376":"X_test_new= SelectKBest(mutual_info_regression, k=20).fit_transform(X_test.fillna(0),y_test)\nX_test_new.shape","4a2e5481":"model_1=XGBRegressor()\n\nmodel_1.fit(X_train_new,y_train)\nmodel_1_train_pred=model_1.predict(X_train_new)  #Predictng for train values\nmodel_1_test_pred=model_1.predict(X_test_new)  #predicting for test values\n\n\n#calculating Mean absolute error for both train and test values\nprint(\"The train_MAE for XGB model\",mean_absolute_error(y_train,model_1_train_pred))\nprint(\"The test_MAE for XGB model\",mean_absolute_error(y_test,model_1_test_pred))\n\n#creating dataframe for actual and predicted values\nXGB_table=pd.DataFrame({'Predicted value':model_1_test_pred, 'Actual_value': y_test})\n\n#Creating another column Absolute Difference(ADF) which finds abs diff between actual and predicted values\nXGB_table[\"ADF\"]= abs(XGB_table[\"Predicted value\"]-XGB_table[\"Actual_value\"])\n\nXGB_table.head()","53c8ad00":"ax = XGB_table.plot.kde() ","7230514e":"sns.distplot(y_test-model_1_test_pred)","ea955264":"model_2=ExtraTreesRegressor(n_estimators=400)\n\nmodel_2.fit(X_train_new,y_train)\nmodel_2_train_pred=model_2.predict(X_train_new)  #Predictng for train values\nmodel_2_test_pred=model_2.predict(X_test_new)  #predicting for test values\n\n\n#calculating Mean absolute error for both train and test values\nprint(\"The train_MAE  for LR model\",mean_absolute_error(y_train,model_2_train_pred))\nprint(\"The test_MAE for LR model\",mean_absolute_error(y_test,model_2_test_pred))\n\n\n#creating dataframe for actual and predicted values\nExtratree_table=pd.DataFrame({'Predicted value': model_2_test_pred, 'Actual_value': y_test})\n\n#Creating another column Absolute Difference(ADF) which finds abs diff between actual and predicted values\nExtratree_table[\"ADF\"]= abs(Extratree_table[\"Predicted value\"]-Extratree_table[\"Actual_value\"])\n\nExtratree_table.head()","483f64ff":"yz =Extratree_table.plot.kde()","b343b1ac":"sns.distplot(y_test-model_2_test_pred)","459de94d":"model_3=DecisionTreeRegressor(max_depth=10, splitter='random',min_samples_leaf=3)\n\nmodel_3.fit(X_train_new,y_train)\nmodel_3_train_pred=model_3.predict(X_train_new)  #Predictng for train values\nmodel_3_test_pred=model_3.predict(X_test_new)  #predicting for test values\n\n#calculating Mean absolute error for both train and test values\nprint(\"The train_MSE  for DT model\",mean_absolute_error(y_train,model_3_train_pred))\nprint(\"The test_MSE for DT model\",mean_absolute_error(y_test,model_3_test_pred))\n\n\n#creating dataframe for actual and predicted values\nDecisiontree_table=pd.DataFrame({'Predicted value': model_3_test_pred, 'Actual_value': y_test})\n\n#Creating another column Absolute Difference(ADF) which finds abs diff between actual and predicted values\nDecisiontree_table[\"ADF\"]= abs(Decisiontree_table[\"Predicted value\"]-Decisiontree_table[\"Actual_value\"])\n\nDecisiontree_table.head()","087cc0a2":"yz =Decisiontree_table.plot.kde()","2bfd40b8":"sns.distplot(y_test-model_3_test_pred)","c6cbf49a":"model_4=RandomForestRegressor()\n\nmodel_4.fit(X_train_new,y_train)\nmodel_4_train_pred=model_4.predict(X_train_new)  #Predictng for train values\nmodel_4_test_pred=model_4.predict(X_test_new)  #predicting for test values\n\n#calculating Mean absolute error for both train and test values\nprint(\"The train_MAE  for RF model\",mean_absolute_error(y_train,model_4_train_pred))\nprint(\"The test_MAE for RF model\",mean_absolute_error(y_test,model_4_test_pred))\n\n#creating dataframe for actual and predicted values\nRF_table=pd.DataFrame({'Predicted value': model_4_test_pred, 'Actual_value': y_test})\n\n#Creating another column Absolute Difference(ADF) which finds abs diff between actual and predicted values\nRF_table[\"ADF\"]= abs(RF_table[\"Predicted value\"]-RF_table[\"Actual_value\"])\n\nRF_table.head()","70ffdc4b":"yz =RF_table.plot.kde()","1232bac4":"sns.distplot(y_test-model_4_test_pred)","f2c76e89":"model_5=SVR()\n\nmodel_5.fit(X_train_new,y_train)\nmodel_5_train_pred=model_5.predict(X_train_new)  #Predictng for train values\nmodel_5_test_pred=model_5.predict(X_test_new)  #predicting for test values\n\n#calculating Mean absolute error for both train and test values\nprint(\"The train_MAE  for SVR model\",mean_absolute_error(y_train,model_5_train_pred))\nprint(\"The test_MAE for SVR model\",mean_absolute_error(y_test,model_5_test_pred))\n\n\n#creating dataframe for actual and predicted values\nSVR_table=pd.DataFrame({'Predicted value': model_5_test_pred, 'Actual_value': y_test})\n\n#Creating another column Absolute Difference(ADF) which finds abs diff between actual and predicted values\nSVR_table[\"ADF\"]= abs(SVR_table[\"Predicted value\"]-SVR_table[\"Actual_value\"])\n\nSVR_table.head()\n\nyz =SVR_table.plot.kde()","3a12a242":"sns.distplot(y_test-model_5_test_pred)","8463c6b7":"model_6=LGBMRegressor()\n\nmodel_6.fit(X_train_new,y_train)\nmodel_6_train_pred=model_6.predict(X_train_new)  #Predictng for train values\nmodel_6_test_pred=model_6.predict(X_test_new)  #predicting for test values\n\n#calculating Mean absolute error for both train and test values\nprint(\"The train_MAE  for LGBM model\",mean_absolute_error(y_train,model_6_train_pred))\nprint(\"The test_MAE for LGBM model\",mean_absolute_error(y_test,model_6_test_pred))\n\n#creating dataframe for actual and predicted values\nLGBM_table=pd.DataFrame({'Predicted value': model_6_test_pred, 'Actual_value': y_test})\n\n#Creating another column Absolute Difference(ADF) which finds abs diff between actual and predicted values\nLGBM_table[\"ADF\"]= abs(LGBM_table[\"Predicted value\"]-LGBM_table[\"Actual_value\"])\n\nLGBM_table.head()\n\nyz =LGBM_table.plot.kde()","95607001":"sns.distplot(y_test-model_6_test_pred)","357efe84":"model_7=LinearRegression()\n\nmodel_7.fit(X_train_new,y_train)\nmodel_7_train_pred=model_7.predict(X_train_new)  #Predictng for train values\nmodel_7_test_pred=model_7.predict(X_test_new)  #predicting for test values\n\n\n#calculating Mean absolute error for both train and test values\nprint(\"The train_MAE  for LR model\",mean_absolute_error(y_train,model_7_train_pred))\nprint(\"The test_MAE for LRmodel\",mean_absolute_error(y_test,model_7_test_pred))\n\n#creating dataframe for actual and predicted values\nLR_table=pd.DataFrame({'Predicted value': model_7_test_pred, 'Actual_value': y_test})\n\n#Creating another column Absolute Difference(ADF) which finds abs diff between actual and predicted values\nLR_table[\"ADF\"]= abs(LR_table[\"Predicted value\"]-LR_table[\"Actual_value\"])\n\nLR_table.head()\n\nyz =LR_table.plot.kde()","dfc5e00a":"#Innitialising the Parameters\n\nparameters={\n            \"max_depth\" : [5,7,9],\n            \"n_estimators\": [300,350,400,450],\n            \"learning_rate\": [0.1,0.01,0.001]\n           }","3e838eab":"tuning_model=GridSearchCV(XGBRegressor(),param_grid=parameters,scoring='neg_mean_absolute_error',cv=3,verbose=3)\ntuning_model.fit(X_train_new,y_train)","e0ac5fda":"# best hyperparameters \ntuning_model.best_params_","8dbcd194":"# best model score\ntuning_model.best_score_","56b2edfd":"tuned_hyper_model=XGBRegressor(max_depth=5, n_estimators=450, learning_rate=0.1)\ntuned_hyper_model.fit(X_train_new, y_train)\ntunedmodel_train_pred=tuned_hyper_model.predict(X_train_new)  #Predictng for train values\ntunedmodel_test_pred=tuned_hyper_model.predict(X_test_new)  #predicting for test values\ntunedmodel_table=pd.DataFrame({'Predicted value':tunedmodel_test_pred, 'Actual_value': y_test})\n\ntunedmodel_table[\"ADF\"]= abs(tunedmodel_table[\"Predicted value\"]-tunedmodel_table[\"Actual_value\"])\n\nprint(\"The train_MAE for tunedmodel model\",mean_absolute_error(y_train,tunedmodel_train_pred))\nprint(\"The test_MAE for tunedmodel model\",mean_absolute_error(y_test,tunedmodel_test_pred))","17e7b27b":"**Data Visualization**","0d8aa200":"sns.distplot(y_test-model_7_test_pred)","1a3cb1e5":"**SVR**","1c7822c7":"This is my first notebook in kaggle. Any suggestions would be accepted please upvote and comment so that I can improve my self.","9ec4049e":"**RandomForestRegressor**","5835af23":"Looking at above data there are 3 column where there are Null values. column  PctSomeCol18_24 has 2285 Null values and PctPrivateCoverageAlone  which has 609 Null values and column PctEmployed16_Over  has 150 null values. Null values of all these columns are replaced   with the mean of all values in that perticular column. ","ce20b650":"**Observation:** As per above graph PercentMarried as TARGET_deathRate increases. Around 38% of married people have deathrate of more than 350 which is heighest","4d74ce7f":"**Data Cleaning**","dd30e051":"**DecissionTreeRegressor**","4fb05def":"**Problem Statement** :  Based on the given dataset with huge variables need to predict the cancer mortality rate per capita.","29599760":"**XGBRegressor Hyper-Parameter Tuning**","ac2422aa":"**Observation:** As per above graph incidenceRate increases as TARGET_deathRate increases. But for the maximum value of TARGET_deathRate 350 the avgDeathsPerYear is 14000 which is heighest.","8919a6a7":"**Features Selection for Train Values**","7b4573ae":"**Observation:**data.plot.scatter(x=\"TARGET_deathRate\", y=\"PercentMarried\", figsize=(10,8))\nplt.title(\"TARGET_deathRate VS PercentMarried\")\nplt.show() As per above graph medIncome decreases as TARGET_deathRate increases. But for the maximum value of medIncome the TARGET_deathRate is around 125","d0296ebc":"Firstly All Models are trained with default Parameters. Which ever model have better MAE and less overfitt model is considered and hyper-parameter is done on that.","8e8b8584":"**LinearRegression**","e7fc0dba":"**LGBMRegressor**","43768f05":"**PREDICT THE CANCER MORTALITY RATE PER CAPITA**","9e2e12c2":"For feature selection mutual_info_regression is used. **Mutual information** between two random variables is a non-negative value, which measures the dependency between the the variables with the Target variable. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.","c6bc1cf3":"**Features Selection for Test Values**","a3a710f6":"By observing all above graphs it is like features are not completely or even around 50% dependent on Target variable. We have total 93 features which are very high to build a model. Hence featurization is required to select top features","2f2bf7b4":"**Observation:** There is no much relation between the target variable TARGET_deathRate and avgDeathPerYear. Most of avgDeathsPerYear is stagnent as TARGET_deathRate increases. But for the TARGET_deathRate 150 the avgDeathsPerYear is 14000 which is heighest.","9f2fe350":"This is very huge to understand. Hence visualize few features vs target variable to undertsand the relation between them.","1ef0c5d9":"**MODELLING**","bf9d01f6":"**Perfomance Metric**:\n**MAE**: Mean Absolute Error is the performance matric which is used here. This is because as the columns are more it is very difficult to find the outliers in each column and deal with them individually. Hence in order to ignore the outliers and  check the performance MAE is used.","0930bec6":"**XGBRegressor**","6ed80ce5":"**Observation:** After doing a hyper paremeter tuning the MAE of test value is decreased but the model is overfiting. Tried to tune the model with more parameters but model is still getting overfitt.","c57b18b7":"The defination of Cancer Mortality rate is can be defined as the cancer death rate (cancer mortality) is 158.3 per 100,000 men and women per year (based on 2013\u20132017 deaths). The cancer mortality rate is higher among men than women (189.5 per 100,000 men and 135.7 per 100,000 women).","fd410f75":"**Observation:** We are getting nearly bell shape curve for all most all the models which does not mean that our model is working good. Because good bell curve only tell us the range of predicted values are with in the same range as our original data range values are.\n\nBy Looking at all the Models the minimum MAE is for XGBRegressor model which is 16.28. This can be further imporved by Hyper Parameter Tunning.","6ad1b259":"**ExtraTreesRegressor**","f2b323bb":"**Featurization**"}}