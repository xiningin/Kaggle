{"cell_type":{"644976a3":"code","39091a27":"code","48881329":"code","d86a0632":"code","5fa21d59":"code","4394f177":"code","faee1d49":"code","f6e9df42":"code","91284f85":"code","515a79ad":"code","52f0f981":"code","ac09671f":"code","451f0fb6":"code","bf8ae63e":"code","35060ff8":"code","8353a221":"code","1e7b47a3":"code","234857b4":"code","0b17e19d":"code","e3d8abd6":"code","0184b810":"code","fe0f835c":"code","331e27bc":"code","161a9142":"code","762d30f6":"code","694c9ea7":"markdown","2035f604":"markdown","3e655198":"markdown","458f039b":"markdown","ae75285a":"markdown","cbea9c29":"markdown","d54f8354":"markdown","b363e2b1":"markdown","0ddc10ca":"markdown","2c2a554a":"markdown","88f52f76":"markdown","64dfa6a9":"markdown","ad4e39b9":"markdown","3f818c74":"markdown","1fffd2ad":"markdown","247c6a5f":"markdown","9be4359e":"markdown","60e4f3a5":"markdown","135d06c0":"markdown","53294137":"markdown","33e0a7c8":"markdown","1e3856d4":"markdown","faea1643":"markdown","84542ad2":"markdown","8472fa74":"markdown","2c208908":"markdown","91bda03c":"markdown","ac611d6b":"markdown","093a5e01":"markdown","97963fb3":"markdown","1e2f5b54":"markdown"},"source":{"644976a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","39091a27":"train_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-3\/train.csv\", low_memory=False)\ntest_data =  pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-3\/eval.csv\", low_memory=False)\nsample_submission =  pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-3\/sample_submission.csv\", low_memory=False)","48881329":"train_data.describe()","d86a0632":"train_data.head()","5fa21d59":"train_data = train_data.drop(['id'],axis=1)\n\nfor i in train_data.columns:\n    print(\"Train Data Column {}: {} is null\".format(i, train_data[i].isnull().sum()))\ntrain_data.describe()\ntrain_data.info()\ntrain_data.columns","4394f177":"import matplotlib.pyplot as plt\ny = train_data['Eat']\nfor i in train_data.columns:\n    plt.scatter(train_data[i],y)\n    plt.xlabel(i)\n    plt.ylabel(\"Enthalpy of Atomisation\")\n    plt.show()","faee1d49":"x = 1274\ni = 0\nsteps = 50\nj = 0\ndef increment(i,j,steps):\n    features = []\n    for k in range(steps):\n        features.append(str(1274-i))\n        plt.scatter(train_data[str(1274-i)],y)\n        plt.xlabel(1274-i)\n        plt.ylabel(\"Enthalpy of Atomisation\")\n        plt.show()\n        j+=1\n        if(steps != j):\n            i+=j\n            i+=1\n    return features\nfeaturesList = increment(i,j,steps)\nprint(featuresList)","f6e9df42":"import keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\n\ncolumns = len(featuresList)\n\nmodel = Sequential()\n\nmodel.add(Dense(50, activation='relu', input_shape=(columns, )))\n\nmodel.add(Dense(32, activation='relu'))\n\nmodel.add(Dense(1))\n","91284f85":"model.summary()\nmodel.compile(optimizer='adam',loss='mean_squared_error')","515a79ad":"from sklearn.model_selection import train_test_split\nrs = 123\n\ndef dataSplit(featuresList,train_data):\n    y = train_data['Eat']\n    X = train_data[featuresList]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = .80, random_state=rs)\n    return (X_train,X_test,y_train,y_test)\n(X_train,X_test,y_train,y_test) = dataSplit(featuresList,train_data)\n","52f0f981":"firstModel = model.fit(X_train,y_train, epochs=100)","ac09671f":"# def dist(model, X_train, y_train, distVal):\n#     distList = []\n#     for i in range(distVal):\n#         (X_trainDist,y_trainDist,X_testDist,y_testDist) = train_test_split(X_train, y_train, train_size = .80, random_state=rs)\n#         distModal = model.fit(X_trainDist,y_trainDist, epochs=25)\n#         predicted = model.predict(X_testDist)\n#         distList.append(model.evaluate(y_testDist,predicted))\n#     return distList\n# distVal = dist(model,X_train,y_train,10)\n# print(distVal)","451f0fb6":"plt.plot(firstModel.history['loss'])\nplt.xlabel('Epochs')\nplt.ylabel('Mean Squared Error')\n\n\nresults = model.evaluate(X_test,y_test)","bf8ae63e":"submissionY = model.predict(test_data[featuresList])","35060ff8":"submissionName = 'firstModelOutput.csv'\ndef processOutput(sample_submission,submissionY, submissionName):\n    newOutput = []\n    for i in submissionY:\n        newOutput.append(i[0])\n\n    output = pd.DataFrame({'id':sample_submission.id, 'Eat':newOutput})\n    output.to_csv(submissionName,index=False)\n    return print(output)\nprocessOutput(sample_submission,submissionY,submissionName)","8353a221":"import keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nimport math\ncolumns = len(featuresList)\n\ndef goldenRatio(steps):\n    ratio = 1.61803399\n    ratioList = []\n    for i in range(steps):\n        if(i == 0):\n            ratioList.append(math.ceil(1+ratio))\n        else:\n            ratioList.append(math.ceil(ratioList[i-1]+(ratio*(ratioList[i-1]))))\n    ratioList.insert(0,1)\n    ratioList.reverse()\n    return ratioList\nlayers = goldenRatio(6)\n\ndef createModel(layers,columns,activation,optimizer,normalize):\n    model = Sequential()\n    depths = len(layers)\n    for i in range(depths):\n        if(i==0):\n            model.add(Dense(layers[i], activation=activation, input_shape=(columns, )))\n            if(normalize):\n                normalizeModel(model)\n        if(i==depths-1):\n            model.add(Dense(layers[i]))\n        elif(i>0):\n            model.add(Dense(layers[i], activation=activation))\n            if(normalize):\n                normalizeModel(model)\n    model.compile(optimizer=optimizer,loss='mean_squared_error')\n    return model\n\nmodel_2=createModel(goldenRatio(6),columns,'relu','adam',False)\nprint(model_2.summary())","1e7b47a3":"secondModel = model_2.fit(X_train,y_train, epochs=100)","234857b4":"plt.plot(secondModel.history['loss'])\nplt.xlabel('Epochs')\nplt.ylabel('Mean Squared Error')\n\n\nresults = model_2.evaluate(X_test,y_test)","0b17e19d":"submissionY = model_2.predict(test_data[featuresList])","e3d8abd6":"submissionName = 'secondModelOutput.csv'\nprocessOutput(sample_submission,submissionY,submissionName)","0184b810":"import keras\nfrom keras.layers import Dense, LayerNormalization\nfrom keras.models import Sequential\nimport math\n\ncolumns = len(featuresList)\n\nmodel = Sequential()\n\ndef normalizeModel(model):\n    return model.add(LayerNormalization())\n\nnormalize = True\n\nactivations = ['relu','elu']\noptimizations = ['adam','adamax']\n\nmodelList = []\nfor i in activations:\n    for j in optimizations:\n        modelList.append(createModel(goldenRatio(6),columns,i,j,normalize))","fe0f835c":"print(modelList)\nthirdModelList = []\nfor i in modelList:\n    thirdModelList.append(i.fit(X_train,y_train, epochs=100))","331e27bc":"print(thirdModelList)\nk=0\nfor i in thirdModelList:\n    plt.plot(i.history['loss'])\n    plt.xlabel('Epochs')\n    plt.ylabel('Mean Squared Error')\n    plt.show()\n\n    results = modelList[k].evaluate(X_test,y_test)\n    k+=1","161a9142":"submissionY = modelList[0].predict(test_data[featuresList])","762d30f6":"submissionName = 'thirdModelOutput.csv'\nprocessOutput(sample_submission,submissionY,submissionName)","694c9ea7":"# !!! Load Data !!!","2035f604":"## I chose this model because it was my only one for model 2, model 3 will give us more choices","3e655198":"## No Empty Values","458f039b":"## Drop Features - id, its just indexing so can be dropped","ae75285a":"# First Distribution","cbea9c29":"## Third Output","d54f8354":"### Since its our first model, I went with the simplest found on datacamp, with the same node values. Picking the correct features will net us a better result. Its the best model because its the only model, and our first model to build upon","b363e2b1":"## Train Second Model","0ddc10ca":"# !!! Output first model !!!","2c2a554a":"## Train first model","88f52f76":"## We will use those features to run through our test data since its the most consistent on the y axis. Although the data between x-i can help 'fill in the gaps', we want to see how these features that consistently show values on the same y axis at a specific point x, is able to predict the value of the Enthalpy of Atomisation","64dfa6a9":"## Looking at the scatterplots we notice a pattern in reverse. that from id 1274, there is no variance of Eat for every feature x-i, where i increments by 1, and x is feature 1274. Lets isolate those features.","ad4e39b9":"## Build First Model","3f818c74":"## For Activations, I tested relu, sigmoid, tanh and elu, but narrowed it down to relu, elu as the best values\n## For Optimizations I tested adam, sgd, adamax and ftrl, but narrowed it down to adam and adamax (FTRL and SGD were not converging fast enough)","1fffd2ad":"### Outliers - I will probably lose points for this again, but in a dataset the features are displaying a unique,consistent pattern that can be mathematically isolated, I chose to keep those values in its purest form as opposed to selecting the features between x-i.We will also use normalization in our third model to take into account any outliers that exist. Normalization in the model and standardization in the dataset can actually produce worst results as emphasized in class, so selecting one or using just model normalization will standardize the data.\n\n### Other Features? There is a possibility that we can use the midpoint between x-i and x-i+1 as a midway plot to fill in the gaps, but I wanted to run the data this way first before attempting adding those additional 'filler' features","247c6a5f":"### Initially I started with 4 optimizers and 4 activation values, but 2 activators and 2 optimizers were clearly giving values in the tens for the MSE while relu\/elu adam\/adamax were converging upon zero the larger the epoch was. ","9be4359e":"## Second Output","60e4f3a5":"## Evaluate our Model - Although we are checking the Mean Squared error, the difference is simply sqrt(mse)","135d06c0":"## For our model, we are creating a reusable function. In our second model, we are simply deepening the layer and node value which will carry over to the third model using relu\/adam as our activation and optimizer values respectively","53294137":"# !!! First Simple Model !!!","33e0a7c8":"# Split the data","1e3856d4":"## Look for empty values","faea1643":"## Lets run an output with the current featuresList","84542ad2":"## First Model Output","8472fa74":"## We have isolated the features with the unique pattern in the plot and will run our models against it","2c208908":"## For our second model, I decided to go with the goldenRatio as a way to choose node values for each layer. the steps represent how many layers we expect. Given a golden ration formula of a+b\/a  , we start with a = goldenRatio ~ 1.618, b = our expected Dense(output) at 1. This gives us ~2.6.. We push the ceiling of this value into an list, and use that value as our new b value. Since a is just a factor of the golden ratio, our second step is 2*a + b \/ 2*a. We continue this with every step. ","91bda03c":"## Although Adamax optimization had showed several instances of rapid convergence for both relu\/elu, and in cases of its MSE closer to zero then Adam, this particular submission the relu\/adam score was ~.0193, while the elu\/adamax was .0321 and elu\/adam was .0211, it was what the model produced at that particular like for me to choose that model","ac611d6b":"# !!!Third Model!!!","093a5e01":"# !!! Build Second Model !!!","97963fb3":"## Trying to write a function for dist and it failed miserably","1e2f5b54":"# !!! Describe Data !!!"}}