{"cell_type":{"93f12f6c":"code","3aeed5c2":"code","25911ed1":"code","3d87a684":"code","93bc6ad7":"code","669963e6":"code","2d423df9":"code","c17d50af":"code","671fb685":"code","85175967":"code","f2a76881":"code","39364eaa":"code","3bba3f86":"code","e4ce1579":"code","bbe4f749":"code","c2409385":"code","94858694":"code","dbc2642c":"code","f911bdc8":"code","14ea0508":"code","0e7019ca":"code","66d8af39":"code","944d71e6":"code","868c8977":"code","62c2f081":"code","fe0203e5":"code","d3d517ed":"code","b39f2f06":"code","fd32805d":"code","4cc83876":"code","41aad663":"code","bd026d54":"code","8c278c93":"code","74092e9b":"code","f36110a1":"code","4d12a0fc":"markdown","67ad67d2":"markdown","1ec01e73":"markdown","52a2cb98":"markdown","2e30e093":"markdown","dcf9b55f":"markdown","45f545c3":"markdown","7bd83dd1":"markdown","6a0cb633":"markdown","1ed79474":"markdown","ee07ad86":"markdown","22fc75d0":"markdown","106ef1dd":"markdown","0c95b374":"markdown","ebc150e1":"markdown","53f5851a":"markdown","0a4daf30":"markdown","1599736b":"markdown","95cae31a":"markdown","6d19343d":"markdown","e5992eaa":"markdown","ad9ae9a9":"markdown","2cc603ff":"markdown","1e95f54c":"markdown","ed4197e9":"markdown","72781f6a":"markdown","2fa5a1f2":"markdown","07f86f21":"markdown","06601551":"markdown","8df6b1e9":"markdown","c998b33c":"markdown","c6c6b2be":"markdown","00f546a7":"markdown","63cf2bd4":"markdown"},"source":{"93f12f6c":"from IPython.display import Image\nImage(filename=\"..\/input\/skoltech-anomaly-benchmark-skab-teaser\/look.png\", width=1000, height=500)","3aeed5c2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n%matplotlib inline","25911ed1":"# Reading the data\nraw_data = pd.read_csv('..\/input\/skoltech-anomaly-benchmark-skab-teaser\/SkAB teaser.csv', \n                   sep=';', \n                   index_col='datetime', \n                   parse_dates=True).drop('index',axis=1)","3d87a684":"# Showing the first 10 rows of the table with the data\nraw_data.head(10)","93bc6ad7":"# Pivoting the table\nraw_data = raw_data.pivot_table(values='value', index=raw_data.index, columns='id')","669963e6":"# Showing first 5 rows of the table\nraw_data.head()","2d423df9":"raw_data.info()","c17d50af":"# Plotting\nraw_data.plot(figsize=(12,6), marker='o', markersize=3);","671fb685":"# Cutting off\nraw_data = raw_data['2019-07-08 17:52:29':]","85175967":"# Plotting\nraw_data.plot(figsize=(12,6), marker='o', markersize=3);","f2a76881":"# Saving processed data\n# raw_data.to_csv('raw_data.csv')","39364eaa":"print(f'The shape of the table (dataframe): {raw_data.shape}')","3bba3f86":"raw_data.describe()","e4ce1579":"# Plotting separate pictures for signals\nfor name in raw_data.columns:\n    raw_data[name].plot(figsize=(12,3), marker='o', markersize=2)\n    plt.xlabel('Time')\n    plt.ylabel('Value')\n    plt.title(f'Signal: {name}')\n    plt.show()","bbe4f749":"# todo\ndef preprocessing(raw_data):\n    data = raw_data.copy()\n    \n    # your code\n    \n    return data\n\ndata = preprocessing(raw_data=raw_data)","c2409385":"# Showing the training, validation and test sets\ndata.plot(figsize=(12,6))\nplt.axvspan(data.index[0], \n            '2019-07-08 18:25', \n            color='green', \n            alpha=0.1, \n            label='Training set')\nplt.axvspan('2019-07-08 18:25', \n            '2019-07-08 18:35', \n            color='yellow', \n            alpha=0.1, \n            label='Validation set')\nplt.axvspan('2019-07-08 18:35', \n            data.index[-1], \n            color='red', \n            alpha=0.1, \n            label='Test set')\nplt.legend(bbox_to_anchor =(0.8, -0.2), ncol = 3)\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('Training, validation and test sets');","94858694":"# Scaler initialization\nStSc = StandardScaler()\n# Fitting Scaler on the training and validation sets\nStSc.fit(data[:'2019-07-08 18:25'])\n\n# Applying scaler\n# training set\ntrain_sc = StSc.transform(data[:'2019-07-08 18:25'])\n# validation set\nval_sc = StSc.transform(data['2019-07-08 18:25':'2019-07-08 18:35'])\n# all data\ndata_sc = StSc.transform(data)","dbc2642c":"from tensorflow.keras.layers import Input, Dense, BatchNormalization, Activation, Dropout\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom itertools import product","f911bdc8":"def Random(seed_value):\n    # 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n    import os\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n\n    # 2. Set `python` built-in pseudo-random generator at a fixed value\n    import random\n    random.seed(seed_value)\n\n    # 3. Set `numpy` pseudo-random generator at a fixed value\n    import numpy as np\n    np.random.seed(seed_value)\n\n    # 4. Set `tensorflow` pseudo-random generator at a fixed value\n    import tensorflow as tf\n    tf.random.set_seed(seed_value)","14ea0508":"# Function for specific architecture fitting\ndef arch(param, data):\n    \"\"\"Specific architecture fitting\n\n    Parameters\n    ----------\n    param : list\n    \n    data : np.array\n    \"\"\"\n    Random(0)\n    input_dots = Input((8,))\n\n    x = Dense(param[0])(input_dots)\n    x = BatchNormalization()(x)\n    x = Activation('elu')(x)\n\n    x = Dense(param[1])(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    bottleneck = Dense(param[2], activation='linear')(x)\n\n    x = Dense(param[1])(bottleneck)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Dense(param[0])(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    out = Dense(8, activation='linear')(x)\n\n    model = Model(input_dots, out)\n    model.compile(optimizer=Adam(param[3]), loss='mae', metrics=[\"mse\"])\n    \n    model.fit(data, data,\n                validation_split=0.2,\n                epochs=10,\n                batch_size=param[4],\n                verbose=0,\n                shuffle=True,\n               )\n    return model","0e7019ca":"model = arch(param=(6, 5, 4, 0.0001, 30), data=train_sc)","66d8af39":"for i in range(val_sc.shape[1]):\n    plt.figure(figsize=(12,3))\n    plt.plot(StSc.inverse_transform(val_sc)[:, i])\n    plt.plot(StSc.inverse_transform(model.predict(val_sc))[:, i])\n    plt.show()","944d71e6":"mean_absolute_error(val_sc, model.predict(val_sc))","868c8977":"# Selecting the parameners' grid for checking\nn1=[6, 5]\nn2=[4, 3]\nn3=[2, 1]\nlr=[0.05, 0.01]\nbatch_size=[32, 64]\n\nparameters = product(n1, n2, n3, lr, batch_size)\nparameters_list = list(parameters)\nprint(f'Total number of parameter combinations: {len(parameters_list)}')","62c2f081":"# Table with the parameters' grid\npd.DataFrame(parameters_list, columns=['neurons 1st layer',\n                                      'neurons 2nd layer',\n                                      'neurons 3rd layer',\n                                      'learning rate',\n                                      'batch size']).head()","fe0203e5":"from tqdm.notebook import tqdm","d3d517ed":"# Greedy brute force\nerrors = []\nfor params in tqdm(parameters_list):\n    \n    model = arch(params, train_sc)\n    train_pred = model.predict(train_sc, batch_size=params[4])\n    val_pred = model.predict(val_sc, batch_size=params[4])\n    \n    train_error = mean_absolute_error(train_sc, train_pred)\n    val_error = mean_absolute_error(val_sc, val_pred)\n    \n    errors.append(list(params)+[train_error, val_error])\n\n# Sort the parameters by the error value\ndf_errors = pd.DataFrame(errors,\n                         columns=['neurons 1st layer', \n                                  'neurons 2nd layer', \n                                  'neurons 3rd layer', \n                                  'learning rate', \n                                  'batch size', \n                                  'mae train', \n                                  'mae val'])\ndf_errors.sort_values('mae val').head()","b39f2f06":"best_params = parameters_list[df_errors.sort_values('mae val').index[0]]\n\nmodel = arch(best_params, train_sc)\nmodel.summary()","fd32805d":"for i in range(val_sc.shape[1]):\n    plt.figure(figsize=(12,3))\n    plt.plot(StSc.inverse_transform(val_sc)[:, i])\n    plt.plot(StSc.inverse_transform(model.predict(val_sc))[:, i])\n    plt.show()","4cc83876":"test_residuals = data_sc - model.predict(data_sc)\n\npd.DataFrame(test_residuals, columns=data.columns, index = data.index).plot(figsize=(12,6))\nplt.xlabel('Time')\nplt.ylabel('Residuals')\nplt.title('Residuals')\nplt.show()","41aad663":"train_residuals = train_sc - model.predict(train_sc)\nval_residuals = val_sc - model.predict(val_sc)\n\nUCL = pd.DataFrame(val_residuals).abs().sum(axis=1).quantile(0.99)","bd026d54":"# Health indicator\npd.DataFrame(test_residuals, index=data.index).abs().sum(axis=1).plot(marker='o', \n                                                                      markersize=2, \n                                                                      alpha=0.2, \n                                                                      figsize=(12,6), \n                                                                      label='Health indicator')\n# Health indicator with the median filter\npd.DataFrame(test_residuals, index=data.index).abs().sum(axis=1).rolling(3).median().plot(marker='o', \n                                                                                          markersize=2, \n                                                                                          alpha=0.7, \n                                                                                          figsize=(12,6),\n                                                                                          label='Smoothed Health indicator')\n\nplt.axvspan(data.index[0], \n            '2019-07-08 18:25', \n            color='green', \n            alpha=0.1, \n            label='Training sample')\nplt.axvspan('2019-07-08 18:25', \n            '2019-07-08 18:35', \n            color='yellow', \n            alpha=0.1, \n            label='Validation sample')\nplt.axvspan('2019-07-08 18:35', \n            data.index[-1], \n            color='red', \n            alpha=0.1, \n            label='Test sample')\n\nplt.axhline(UCL, color='r', label='Upper control limit')\nplt.ylim([0, 4*UCL])\nplt.xlabel('Time')\nplt.ylabel('Health indicator value')\nplt.legend(bbox_to_anchor =(0.8, -0.2), ncol = 3)\nplt.show()","8c278c93":"fig, ax = plt.subplots(figsize=(12,6))\nax.axvspan(\n    data.index[data.index=='2019-07-08 18:39:22'][0],\n    data.index[data.index=='2019-07-08 18:42:32'][0],\n    alpha=0.2, \n    color='red')\nax.axvspan(\n    data.index[data.index=='2019-07-08 18:44:36'][0],\n    data.index[data.index=='2019-07-08 18:46:51'][0],\n    alpha=0.2, \n    color='red')\nax.axvspan(\n    data.index[data.index=='2019-07-08 19:06:57'][0],\n    data.index[data.index=='2019-07-08 19:11:31'][0],\n    alpha=0.2, \n    color='red')\nax.axvspan(\n    data.index[data.index=='2019-07-08 19:14:40'][0],\n    data.index[data.index=='2019-07-08 19:21:16'][0],\n    alpha=0.2, \n    color='red', label='Anomalies (incidents)')\nax.plot(data.index, pd.DataFrame(test_residuals).abs().sum(axis=1), \n        marker='o', markersize=2, alpha=0.2, label='Health indicator')\nax.plot(data.index, pd.DataFrame(test_residuals).abs().sum(axis=1).rolling(3).median(), \n        marker='o', markersize=2, alpha=0.7, label='Smoothed Health indicator')\n\nax.axhline(UCL, color='r', label='Upper control limit')\nax.set_ylim([0, 4*UCL])\nax.set_xlabel('Time')\nax.set_ylabel('Health indicator value')\nplt.legend(bbox_to_anchor =(0.8, -0.1), ncol = 3)\nplt.show()","74092e9b":"def feature_importance(residuals, analysis_type=\"collective\", date_from=None, date_till=None, weigh=True):\n    \"\"\"Feature importance calculation\n\n    Parameters\n    ----------\n    residuals : pandas.DataFrame()\n\n    analysis_type : str, \"single\"\/\"collective\", \"single\" by default\n\n    date_from : str in format 'yyyy-mm-dd HH:MM:SS', None by default\n\n    date_till : str in format 'yyyy-mm-dd HH:MM:SS', None by default\n\n    weigh : boolean, True by default\n        If analysis_type == \"collective\".\n\n    Returns\n    -------\n    data : pandas.DataFrame().\n    \"\"\"\n    if date_from is None:\n        start = 0\n    if date_till is None:\n        end = -1\n    data = residuals[date_from:date_till].abs().copy()\n\n    if (analysis_type == \"collective\") & (weigh == False):\n        data = data.div(data.sum(axis=1), axis=0) * 100\n        return pd.DataFrame(data.mean(), columns=['Feature importance, %']).T\n    elif (analysis_type == \"collective\") & (weigh == True):\n        data = data.mean().div(data.mean().sum(), axis=0) * 100\n        return pd.DataFrame(data, columns=['Feature importance, %']).T\n    elif analysis_type == \"single\":\n        return data.div(data.sum(axis=1), axis=0) * 100","f36110a1":"for dates in [['2019-07-08 18:39:22','2019-07-08 18:42:32'],\n              ['2019-07-08 18:44:36','2019-07-08 18:46:51'],\n              ['2019-07-08 19:06:57','2019-07-08 19:11:31'],\n              ['2019-07-08 19:14:40','2019-07-08 19:21:16']]:\n    print(f'Incident since {dates[0]} till {dates[1]}')\n    display(feature_importance(pd.DataFrame(test_residuals, index=data.index, columns=data.columns), date_from=dates[0], date_till=dates[1]))\n    print('\\n')","4d12a0fc":"### Data loading","67ad67d2":"### Features","1ec01e73":"## Useful links\n\n- Autoencoder Explained:\nhttps:\/\/www.youtube.com\/watch?v=H1AllrJ-_30\n\n- Outlier Detection with Autoencoder Ensembles:\nhttps:\/\/saketsathe.net\/downloads\/autoencode.pdf\n\n- About batch normalization layer:\nhttps:\/\/arxiv.org\/pdf\/1502.03167v2.pdf","52a2cb98":"### Libraries importing","2e30e093":"Front panel and composition of the water circulation, control and monitoring systems: 1,2 - solenoid valve (amount - 1); 3 - a tank with water (1); 4 - a water pump (1); 5 - emergency stop button (1); 6 - electric motor (1); 7 - inverter (1); 8 - compactRIO (1); 9 - a mechanical lever for shaft misalignment (1). Not shown parts - vibration sensor (2); pressure meter (1); flow meter (1); thermocouple (2).","dcf9b55f":"### Function for results reproducibility","45f545c3":"### Results of the model selection","7bd83dd1":"### Let's fit the random model (architecture)","6a0cb633":"Most of all ML algorithms need data to be scaled before fitting.","1ed79474":"## Problem statement","ee07ad86":"The dataset contains 4 anomalies (incidents):\n\n- MISALIGNMENT OF THE PUMP AND ENGINE SHAFT (abruptly)  \nAbrupt appearance of a defect: 18:39:22  \nAbrupt defect shutdown: 18:42:32\n\n- MISALIGNMENT OF PUMP AND MOTOR SHAFT (slow)  \nSlow appearance of the defect: 18:44:36-18:45:49  \nAbrupt defect shutdown: 18:46:51\n\n- REDUCTION OF FLOW AREA SECTION-1 (top)  \nSlow appearance of the defect: 19:06:57-19:07:37  \nSlow defect shutdown: 19:10:45-19:11:31\n\n- REDUCTION OF FLOW AREA SECTION-2 (bottom)  \nSlow appearance of the defect: 19:14:40-19:16:24  \nSlow defect shutdown: 19:19:15-19:21:16","22fc75d0":"![ae](https:\/\/miro.medium.com\/max\/700\/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png)","106ef1dd":"### Fitting the best model","0c95b374":"Let's pivot the table, because we need each signal to be in the separate column.","ebc150e1":"## Testbed description","53f5851a":"# Anomaly detection in technical systems: anomaly in the water circulation loop","0a4daf30":"- Link to the Keras with TensorFlow backend course: https:\/\/youtu.be\/qFJeN9V1ZsI","1599736b":"Mean Absolute Error (MAE)\n\n$\\text{MAE} = \\frac{1}{N} \\sum^{N}_{i=1}|x_i - \\hat{x}_i|$,\n\nwhere $N$ - Total amount of data instances,  \n$x_i$ - true value at a time moment $i$,  \n$\\hat{x}_i$ - predicted value at a time moment $i$.","95cae31a":"### Additional data processing","6d19343d":"Here you can implement and calculate Average Detection Delay (ADD)\n\n$\\text{ADD} = \\frac{1}{|Y|}\\sum_{y \\in Y} ( \\tau_y - \\theta_y )$,\n\nwhere $|Y|$ - total amount of changepoints,  \n$\\tau_y$ - moment of detection,  \n$\\theta_y$ - moment of changepoint (anomaly appearing).","e5992eaa":"- training sample \u2014 sample for model's parameters optimization.\n- validation sample \u2014 sample for selecting the best model from the set of models built on the training sample.\n- test sample \u2014 sample for assesing the quality of the problem solution.\n\nDescriptions and options for the definitions of training, validation and test samples are presented in the [article](https:\/\/medium.com\/@tekaround\/train-validation-test-set-in-machine-learning-how-to-understand-6cdd98d4a764).\n\n[Article](https:\/\/hunch.net\/?p=22) about overfitting in ML.","ad9ae9a9":"Let's cut off the beginning interval of the data to exclude the transition period (warming up).","2cc603ff":"### Hyperparameters to optimize","1e95f54c":"## Thank you for your attention!","ed4197e9":"### Libraries importing","72781f6a":"### Feature importance calculation","2fa5a1f2":"- **DS problem in terms of business:** We need to detect anomalies as soon as they appear.\n\nMetric:\nAverage Detection Delay (ADD)\n\n$\\text{ADD} = \\frac{1}{|Y|}\\sum_{y \\in Y} ( \\tau_y - \\theta_y )$,\n\nwhere $|Y|$ - total amount of changepoints,  \n$\\tau_y$ - moment of detection,  \n$\\theta_y$ - moment of changepoint (anomaly appearing).\n\n- **DS problem in terms of math:** We need to propose (construct) the model, that most accurately describes the normal operation of the testbed.\n\nMetric:\nMean Absolute Error (MAE)\n\n$\\text{MAE} = \\frac{1}{N} \\sum^{N}_{i=1}|x_i - \\hat{x}_i|$,\n\nwhere $N$ - Total amount of data instances,  \n$x_i$ - true value at a time moment $i$,  \n$\\hat{x}_i$ - predicted value at a time moment $i$.","07f86f21":"### Splitting the data into training, validation and test sets","06601551":"### Autoencoder description","8df6b1e9":"## Fitting the model","c998b33c":"The testbed of the industrial Internet of things is intended for:\n\n- Demonstration of the possibilities and benefits associated with the implementation of industrial Internet of things technologies;\n- Approbation, verification and validation of new technologies related to the industrial Internet of Things in laboratory conditions in order to determine a set of scientific theories, algorithms and practical tools for the application of these technologies in urgent industrial problems;\n- Conducting educational and research work on the subject of the industrial Internet of things.\n\nThe simple and intuitive user interface of the program allows you to operate the systems in accordance with the instructions presented in the user manual separately.\n\nThe testbed is a closed-loop water pump, a power supply system, control systems, data collection and monitoring system, built using industrial Internet of things technologies.\nThe stand consists of the following systems:\n\n1. Water circulation system.\n2. Control system of the water circulation system (hereinafter referred to as the Control System).\n3. System for monitoring the state of the water circulation system (hereinafter - Monitoring System).\n4. TSN technology demonstration system.\n5. System for storing, processing and visualizing data.\n\nThe water circulation system is designed to simulate a water supply system in a laboratory environment and circulates water through water pipes using a water pump.\nThe water circulation system simulates the following faults:\n\n- Introduction of imbalance on the connecting shaft (misalignment) of the motor and the water pump;\n- Changing the flow rate of the valve at the pump inlet (REDUCTION OF FLOW AREA);\n- Changing the flow rate of the valve at the pump outlet (REDUCTION OF FLOW AREA).\n\nThe system consists of the following components:\n- Water pump\n- Electric motor\n- Inverter\n- Electrovalve (1)\n- Electrovalve (2)\n- Mechanical lever for misalignment\n- Vibration sensors\n- Water tank with pipes\n- Pressure sensor\n- Flow meter\n- Thermocouple","c6c6b2be":"## Working with the data","00f546a7":"### Data scaling (normalizing)","63cf2bd4":"### Health indicator"}}