{"cell_type":{"9cc5aa12":"code","77785c06":"code","5f8ee9fb":"code","abf59127":"code","a75577d0":"code","73defed8":"code","9067b06c":"code","2886c9f3":"code","470a3f97":"code","8abf1c23":"code","f3bb5756":"code","50cf5fc6":"code","c9a68d0c":"code","e9e623cb":"code","5510ddad":"code","1189e941":"code","10ca39cc":"code","c8a84f3f":"code","2ba551fb":"code","2e7a9b92":"code","921b1532":"code","165520ef":"code","314ef98e":"code","ebc57766":"code","dd04b5ef":"code","8392b388":"code","c13756af":"code","e95c335e":"code","7c332c67":"code","1251a31d":"code","c61458bb":"code","7bfd0bd9":"code","3ffde3cf":"code","2b6e8147":"code","c69e304e":"code","05850117":"code","771f423e":"markdown","849fa5d6":"markdown","c09ec254":"markdown","e0d1ad7d":"markdown","460796cf":"markdown","894e8cb2":"markdown","ec3d898d":"markdown"},"source":{"9cc5aa12":"!pip uninstall fastai -y","77785c06":"import sys,os","5f8ee9fb":"sys.path.append('..\/input\/fastaiv1')","abf59127":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom collections import defaultdict\nimport os\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.callbacks import *\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport matplotlib.image as immg\nfrom sklearn.model_selection import StratifiedKFold,KFold","a75577d0":"!pip install object-detection-fastai","73defed8":"from object_detection_fastai.helper.object_detection_helper import *\nfrom object_detection_fastai.loss.RetinaNetFocalLoss import RetinaNetFocalLoss\nfrom object_detection_fastai.models.RetinaNet import RetinaNet\nfrom object_detection_fastai.callbacks.callbacks import BBLossMetrics, BBMetrics, PascalVOCMetric","9067b06c":"sns.set_style('darkgrid')","2886c9f3":"df = pd.read_csv('..\/input\/nfl-impact-detection\/image_labels.csv')","470a3f97":"path = Path('..\/input\/nfl-impact-detection')","8abf1c23":"tr = df.image.value_counts()\ntr = pd.DataFrame({'image':tr.index,'image_count':tr.values})\ntr = tr.sample(frac=1.,random_state=2020).reset_index(drop=True)","f3bb5756":"tr.head()","50cf5fc6":"df.sample(n=10)","c9a68d0c":"df_grp = df.groupby(['image'])","e9e623cb":"name = '58146_002671_Endzone_frame304.jpg'\nloc = '..\/input\/nfl-impact-detection\/images\/'+name\ntemp = df_grp.get_group(name)\nlb = temp.loc[:, (['label','left', 'width', 'top', 'height'])].values\nimg = immg.imread(loc)\nfig,ax = plt.subplots(figsize=(20,12))\nax.imshow(img)\nfor s in lb:\n    l,b = s[0],s[1:]\n    rect = [b[0],b[2],b[1],b[3]]\n    draw_rect(ax,rect,text=l,text_size=10,color='red')","5510ddad":"def get_lbl_img(train):\n    helmet2bbox = {}\n    grp = df.image.unique()\n    tr_gr = df.groupby(['image'])\n    from tqdm.notebook import tqdm\n    for i in tqdm(range(len(grp))):\n        name = str(grp[i])\n        bbox = []\n        lbls = []\n        temp_b = []\n        temp = tr_gr.get_group(grp[i])\n        tt = temp.loc[:, (['label','left', 'width', 'top', 'height'])].values\n        for j in range(len(temp)):\n            lbls.append(tt[j][0])\n            b = tt[j][1:]  \n            t = [b[0],b[2],b[1],b[3]] # x,y, width, height\n            # Currently our coordinates are x,w,l,h and we want x1,y1,x2,y2\n            # To convert it, we need to add our width and height to the respective x and y.\n            t[2],t[3] = t[0]+t[2],t[1]+t[3]  \n            t1 = [t[1],t[0],t[3],t[2]]\n\n            temp_b.append(t1)\n        bbox.append(temp_b)\n        bbox.append(lbls)\n        helmet2bbox[name] = bbox\n    return helmet2bbox","1189e941":"helmet2bbox = get_lbl_img(df)","10ca39cc":"chk = helmet2bbox['57802_001673_Endzone_frame0932.jpg']","c8a84f3f":"img = immg.imread(str(path\/'images'\/'57802_001673_Endzone_frame0932.jpg'))","2ba551fb":"fig,ax = plt.subplots(figsize=(16,10))\nax.imshow(img)\nlbl,bbxs = chk[1],chk[0]\nfor l, b in zip(lbl,bbxs):\n    rect = [b[0],b[1],b[3]-b[1],b[2]-b[0]]\n    rect1 = [rect[1],rect[0],rect[3],rect[2]]\n    draw_rect(ax,rect1,text=l,text_size=12,color='red')","2e7a9b92":"img.shape[0]\/2,img.shape[1]\/2","921b1532":"get_y_func = lambda o: helmet2bbox[Path(o).name] ","165520ef":"data = (ObjectItemList.from_df(tr.sample(frac=0.5),path, folder = 'images' ,cols='image')\n        #Where are the images? ->\n        .split_by_rand_pct(0.2)                          \n        #How to split in train\/valid? -> randomly with the default 20% in valid\n        .label_from_func(get_y_func)\n        #How to find the labels? -> use get_y_func on the file name of the data\n        .transform(size=512,resize_method=ResizeMethod.SQUISH)\n        #.add_test(ts)\n        #Data augmentation? -> Standard transforms; also transform the label images\n        .databunch(bs=2, collate_fn=bb_pad_collate))","314ef98e":"data.show_batch( 1, figsize = (20,12))","ebc57766":"len(data.train_ds),len(data.valid_ds)","dd04b5ef":"data.classes,data.c","8392b388":"size = 512","c13756af":"anchors = create_anchors(sizes=[(32,32),(16,16),(8,8),(4,4)], ratios=[0.5, 1, 2], scales=[0.15, 0.25, 0.35, 0.45, 0.55, 0.75])","e95c335e":"fig,ax = plt.subplots(figsize=(10,10))\nax.imshow(image2np(data.valid_ds[0][0].data))\n\nfor i, bbox in enumerate(anchors[:18]):\n    bb = bbox.numpy()\n    x = (bb[0] + 1) * size \/ 2 \n    y = (bb[1] + 1) * size \/ 2 \n    w = bb[2] * size \/ 2\n    h = bb[3] * size \/ 2\n    \n    rect = [x,y,w,h]\n    draw_rect(ax,rect)","7c332c67":"len(anchors)","1251a31d":"n_classes = data.train_ds.c\n\ncrit = RetinaNetFocalLoss(anchors)\n\nencoder = create_body(models.resnet18, True, -2)\n\nmodel = RetinaNet(encoder, n_classes=data.train_ds.c, n_anchors=18, sizes=[32,16,8,4], chs=32, final_bias = -4., n_conv = 2)","c61458bb":"voc = PascalVOCMetric(anchors, size, [i for i in data.train_ds.y.classes[1:]])\nlearn = Learner(data,\n                model, \n                loss_func=crit,\n                callback_fns=[BBMetrics],\n                metrics=[voc],\n                model_dir = '\/kaggle\/working\/')","7bfd0bd9":"learn.split([model.encoder[6], model.c5top5]);\nlearn.freeze_to(-2)\n#learn = learn.to_fp16()","3ffde3cf":"#learn.unfreeze()\nlearn.fit_one_cycle(3, 1e-3 ,callbacks = [SaveModelCallback(learn, every ='improvement', monitor ='AP-Helmet', name ='best_model',mode='max')])","2b6e8147":"learn.recorder.plot_losses()","c69e304e":"learn.load('best_model');","05850117":"show_results_side_by_side(learn, anchors, detect_thresh=0.3, nms_thresh=0.1, image_count=10)","771f423e":"### Anchors","849fa5d6":"## Results","c09ec254":"## Sample Check 2","e0d1ad7d":"## Learner","460796cf":"## DataLoader","894e8cb2":"## Encoder and Model","ec3d898d":"## Printing a sample"}}