{"cell_type":{"f877319a":"code","5c927db3":"code","ddffff0a":"code","499493e1":"code","c6548c87":"code","080fc833":"code","12141e24":"code","9250d044":"code","c83a2ad0":"code","8798241f":"code","4e8c3dc2":"code","5065c3cd":"code","e1b1343f":"markdown","27c94268":"markdown"},"source":{"f877319a":"# Load packages\nimport time\nfrom scipy.stats import uniform\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import make_scorer, accuracy_score\n\nfrom hyperopt import hp, fmin, tpe\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom bayes_opt import BayesianOptimization\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option(\"display.max_columns\", None)","5c927db3":"# Make scorer: accuracy\nacc_score = make_scorer(accuracy_score)","ddffff0a":"# Load dataset\ntrainSet = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')\ntestSet = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv')\nsubmitSet = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/sample_submission.csv')\n\ntrainSet.head()\n\n# Remove not used variables\ntrain = trainSet.drop(columns=['Name', 'Ticket'])\ntrain['Cabin_letter'] = train['Cabin'].str[0:1]\ntrain['Cabin_no'] = train['Cabin'].str[1:]\n\ntrain.head()\n\n# Feature generation: training data\ntrain = trainSet.drop(columns=['Name', 'Ticket', 'Cabin'])\ntrain = train.dropna(axis=0)\ntrain = pd.get_dummies(train)\n\ntrain.head()","499493e1":"# train validation split\nX_train, X_val, y_train, y_val = train_test_split(train.drop(columns=['PassengerId','Survived'], axis=0),\n                                                  train['Survived'],\n                                                  test_size=0.2, random_state=111,\n                                                  stratify=train['Survived'])","c6548c87":"# GridSearchCV\nparam_grid = {'max_depth':[3,4,5,6,7,8,9,10],\n              'max_features':[0.8,0.9,1],\n              'learning_rate':[0.01,0.1,1],\n              'n_estimators':[80,100,120,140,150],\n              'subsample': [0.8,0.9,1]}\n\ngrid = GridSearchCV(estimator=GradientBoostingClassifier(), param_grid=param_grid, scoring=acc_score, cv=5)\n\ngrid.fit(X_train.iloc[1:100,], y_train.iloc[1:100,])","080fc833":"# RandomizedSearhCV\nparam_rand = {'max_depth':uniform(3,10),\n              'max_features':uniform(0.8,1),\n              'learning_rate':uniform(0.01,1),\n              'n_estimators':uniform(80,150),\n              'subsample':uniform(0.8,1)}\n\nrand = RandomizedSearchCV(estimator=GradientBoostingClassifier(), param_distributions=param_rand, scoring=acc_score, cv=5)\n\nrand.fit(X_train.iloc[1:100,], y_train.iloc[1:100,])","12141e24":"# RandomizedSearhCV\nparam_rand = {'max_depth':uniform(3,10),\n              'max_features':uniform(0.8,1),\n              'learning_rate':uniform(0.01,1),\n              'n_estimators':uniform(80,150),\n              'subsample':uniform(0.8,1)}\n\nrand = RandomizedSearchCV(estimator=GradientBoostingClassifier(), param_distributions=param_rand, scoring=acc_score, cv=5)\n\nrand.fit(X_train.iloc[1:100,], y_train.iloc[1:100,])","9250d044":"# Gradient Boosting Machine\ndef gbm_cl_bo(max_depth, max_features, learning_rate, n_estimators, subsample):\n    params_gbm = {}\n    \n    params_gbm['max_depth'] = round(max_depth)\n    params_gbm['max_features'] = max_features\n    params_gbm['learning_rate'] = learning_rate\n    params_gbm['n_estimators'] = round(n_estimators)\n    params_gbm['subsample'] = subsample\n    \n    scores = cross_val_score(GradientBoostingClassifier(random_state=123, **params_gbm),\n                             X_train, y_train, scoring=acc_score, cv=5).mean()\n    \n    score = scores.mean()\n    return score","c83a2ad0":"# Run Bayesian Optimization\nstart = time.time()\n\nparams_gbm ={\n    'max_depth':(3, 10),\n    'max_features':(0.8, 1),\n    'learning_rate':(0.01, 1),\n    'n_estimators':(80, 150),\n    'subsample': (0.8, 1)\n}\n\ngbm_bo = BayesianOptimization(gbm_cl_bo, params_gbm, random_state=111)\ngbm_bo.maximize(init_points=20, n_iter=4)\n\nprint('It takes %s minutes' % ((time.time() - start)\/60))","8798241f":"params_gbm = gbm_bo.max['params']\n\nparams_gbm['max_depth'] = round(params_gbm['max_depth'])\nparams_gbm['n_estimators'] = round(params_gbm['n_estimators'])\n\nparams_gbm","4e8c3dc2":"# Run Bayesian Optimization from hyperopt\nstart = time.time()\n\nspace_lr = {'max_depth': hp.randint('max_depth', 3, 10),\n            'max_features': hp.uniform('max_features', 0.8, 1),\n            'learning_rate': hp.uniform('learning_rate',0.01, 1),\n            'n_estimators': hp.randint('n_estimators', 80,150),\n            'subsample': hp.uniform('subsample',0.8, 1)}\n\ndef gbm_cl_bo2(params):\n    params = {'max_depth': params['max_depth'],\n              'max_features': params['max_features'],\n              'learning_rate': params['learning_rate'],\n              'n_estimators': params['n_estimators'],\n              'subsample': params['subsample']}\n    \n    gbm_bo2 = GradientBoostingClassifier(random_state=111, **params)\n    \n    best_score = cross_val_score(gbm_bo2, X_train, y_train, scoring=acc_score, cv=5).mean()\n    \n    return 1 - best_score\n\ngbm_best_param = fmin(fn=gbm_cl_bo2,\n                space=space_lr,\n                max_evals=24,\n                rstate=np.random.RandomState(42),\n                algo=tpe.suggest)\n\nprint('It takes %s minutes' % ((time.time() - start)\/60))","5065c3cd":"gbm_best_param","e1b1343f":"# Bayes_opt","27c94268":"# hyperopt"}}