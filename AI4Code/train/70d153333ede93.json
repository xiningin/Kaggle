{"cell_type":{"91471bf5":"code","49a4d57f":"code","8a483e5d":"code","900a81f4":"code","c5e171b8":"code","840a9bf4":"code","d122df5d":"code","e37fc03d":"code","361f7302":"code","b719bd39":"code","f54aa791":"code","013a48bc":"code","d673e4e5":"code","60a3e2cb":"code","164c27d5":"code","64f6a1cd":"code","ba130657":"code","2b82a9f3":"code","26155579":"code","aa507529":"code","7bc1031f":"code","c60a4a5b":"code","b1b76890":"code","40aff113":"code","2bf53396":"code","feee14da":"code","74e8c21e":"code","2736f622":"markdown","bb5dfa95":"markdown","34fe5c25":"markdown","e4c7fc1b":"markdown","3da5e591":"markdown","391fa4c0":"markdown","2d471a8f":"markdown","08a66fa8":"markdown","678dbdb0":"markdown","f4d96f76":"markdown","c88e3a9c":"markdown","09ce1380":"markdown","ff5ec632":"markdown","050eba7e":"markdown","ce47d622":"markdown","462ed4f4":"markdown","895cc723":"markdown","aebca696":"markdown","79ba8de3":"markdown","10948158":"markdown","1dc49024":"markdown","6d1dbe30":"markdown","fccffc7f":"markdown","ece672ab":"markdown","c63abdbd":"markdown","4473f1a4":"markdown","013f2f23":"markdown"},"source":{"91471bf5":"# Import all we need\n\nimport keras\nfrom keras import layers\nfrom keras.datasets import mnist, fashion_mnist\nimport numpy as np\nfrom ipywidgets import interact\nfrom keras import backend as K\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nprint ('import completed')\n","49a4d57f":"# Load the data, MNIST is nowadays incorporated in Keras\/Tensorflow, so rather easy to use\n\n(x_train, x_train_label), (x_test, x_test_label) = mnist.load_data()\n#(x_train, x_train_label), (x_test, x_test_label) = fashion_mnist.load_data() # In case you want to try fashion mnist\n\nx_train = x_train.astype('float32') \/ 255.\nx_test = x_test.astype('float32') \/ 255.\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n\n# Show the dimensions\nprint(x_train.shape)\nprint(x_test.shape)\n\n# verify the labels are distributed evenly\ng = sns.countplot(x_train_label)","8a483e5d":"# Define the autoencoder model\n\n# These are the dimensions of the original input, the intermediate layer and latent layer\noriginal_dim = 784\nintermediate_dim = 256\nlatent_dim = 2 # \n\n# autoencoder model\n# In this example I used 2 dense neural network layers. You could also use deeper structures or RNN\/LSTM or convolutional layers\n# The encoder and decoder don't have to be the exact same network structure\ninput_img = keras.Input(shape=(original_dim,))\nencoded = layers.Dense(intermediate_dim, activation='relu')(input_img)\nencoded = layers.Dense(latent_dim, activation='relu')(encoded)\n# at this point the representation has dimension: latent_dim\ndecoded = layers.Dense(intermediate_dim, activation='relu')(encoded)\ndecoded = layers.Dense(original_dim, activation='sigmoid')(decoded)\nautoencoder = keras.Model(input_img, decoded, name='autencoder')\nprint (autoencoder.summary())\n\n# Compile the model\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Next the encoder part and decoder model, in order to inspect the inner representation, \n# referencing the autoencode layers (the 3 models share there weights)\n# This part can be omitted in case you don't want to use the inner latent representation \n\n# encoder model (first part of the autoencoder)  \nencoder = keras.Model(input_img, encoded, name='encoder')\nprint (encoder.summary())\n\n# decoder model (second part of the autoencoder) to be able to generate an image from an inner representation,  basically the encoder in reverse.\nencoded_input = keras.Input(shape=(latent_dim,))\ndecoder_layer = autoencoder.layers[-2](encoded_input) # Retrieve the last layers of the autoencoder model\ndecoder_layer = autoencoder.layers[-1](decoder_layer)\ndecoder = keras.Model(encoded_input, decoder_layer, name='decoder')\nprint (decoder.summary())\n\n","900a81f4":"# Train the autoencoder\n\nautoencoder.fit(x_train, x_train,\n                epochs=50,\n                batch_size=256,\n                shuffle=True,\n                validation_data=(x_test, x_test))","c5e171b8":"# Encode and decode some digits, show the inner latent representation\n\n# If you want to show the inner latent representation, use encoder-decoder lines\nencoded_imgs = encoder.predict(x_test)\ndecoded_imgs = decoder.predict(encoded_imgs)\n# If you are not interested in the inner latent representation, use the autoencoder line directly\n#decoded_imgs = autoencoder.predict(x_test)\n\n#Show results: original, encode inner latent representation, decoded image\nimport matplotlib.pyplot as plt\n\nn = 10  # How many digits we will display\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # Display original\n    ax = plt.subplot(2, n, i + 1)\n    plt.imshow(x_test[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    \n    # Display reconstruction\n    ax = plt.subplot(2, n, i + 1 + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    ax.set_xlabel(\"{}\".format(np.array2string(encoded_imgs[i],max_line_width=10)))\n    plt.gray()\n    ax.get_yaxis().set_visible(False)\n    ;\nplt.show()","840a9bf4":"# Showcase the inner hidden\/latent space by plotting all the test images on the latent space\nx_test_encoded = encoder.predict(x_test, batch_size=128)\nplt.figure(figsize=(6, 6))\nplt.scatter(x_test_encoded[:,0], x_test_encoded[:,1], c=x_test_label, cmap='rainbow')\nplt.colorbar()\nplt.show()\n\n","d122df5d":"@interact\n\ndef morph(x = 150, y=150):\n    x_decoded = decoder.predict(np.array([[x, y]]))\n    plt.imshow(x_decoded.reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    return plt.figure()","e37fc03d":"K.clear_session()\n# This are the dimensions of the original input, the intermediate layer and latent layer\noriginal_dim = 784\nintermediate_dim = 256\nlatent_dim = 2 # \n\n# Variational autoencoder model\ninput_img = keras.Input(shape=(original_dim,))\nencoded = layers.Dense(intermediate_dim, activation='relu')(input_img)\nx_mean = layers.Dense(latent_dim)(encoded)\nx_log_var = layers.Dense(latent_dim)(encoded) # implementation choice to encode the log variance i.s.o. the standard deviation\n\ndef sampling(args):\n    # reparameterization trick\n    # instead of sampling from Q(z|x), sample eps = N(0,I)\n    # then x = x_mean + x_sigma*eps= x_mean + sqrt(e^(x_log_var))*eps = x_mean + e^(0.5 * x_log_var)*eps\n    z_mean, z_log_var = args\n    epsilon = K.random_normal(shape=(K.shape(x_mean)[0], latent_dim), mean=0.,\n                              stddev=1.0)\n    return x_mean + K.exp(0.5 * x_log_var) * epsilon # (e^a)^b=e^ab\nx = layers.Lambda(sampling, output_shape=(latent_dim,))([x_mean, x_log_var])\n# at this point the representation has dimension: latent_dim \n\ndecoded = layers.Dense(intermediate_dim, activation='relu')(x)\ndecoded = layers.Dense(original_dim, activation='sigmoid')(decoded)\nvae = keras.Model(input_img, decoded, name='vae')\n#print (vae.summary())\n\n# Create the loss function and compile the model\n# The loss function as defined by paper Kingma\nreconstruction_loss = original_dim * keras.metrics.binary_crossentropy(input_img, decoded) \nkl_loss =  -0.5 * K.sum(1 + x_log_var - K.square(x_mean) -K.exp(x_log_var), axis=-1)\nvae_loss = K.mean(reconstruction_loss + kl_loss)\nvae.add_loss(vae_loss)\nvae.compile(optimizer='adam')\n\n# Next the encoder part and decoder model, in order to inspect the inner representation, referencing the autoencode layers (the 3 models share there weights)\n# This part can be ommitted in case you don't want to use the inner latent representation \n\n# encoder model (first part of the variotional autoencoder) \nencoder = keras.Model(input_img, [x_mean, x_log_var, x], name='encoder')\n#print (encoder.summary())\n\n# decoder model (second part of the autoencoder) to be able to generate an image from an inner representation\nencoded_input = keras.Input(shape=(latent_dim,))\ndecoder_layer = vae.layers[-2](encoded_input) # Retrieve the last layers of the autoencoder model\ndecoder_layer = vae.layers[-1](decoder_layer)\ndecoder = keras.Model(encoded_input, decoder_layer, name='decoder')\n#print (decoder.summary())\n\n\n","361f7302":"keras.utils.plot_model(vae, to_file='model.png', show_shapes=True)","b719bd39":"keras.utils.plot_model(encoder, to_file='model.png', show_shapes=True)\n","f54aa791":"keras.utils.plot_model(decoder, to_file='model.png', show_shapes=True)","013a48bc":"vae.fit(x_train,None,\n        shuffle=True,\n        epochs=50,\n        batch_size=256,\n        validation_data=(x_test, None))","d673e4e5":"# Encode and decode some digits, show the inner latent variable, inner representation\n\nencoded_imgs = encoder.predict(x_test)[2]\ndecoded_imgs = decoder.predict(encoded_imgs)\n#decoded_imgs = vae.predict(x_test)\n\n#Show results: original, encode inner latent representation, decoded image\nimport matplotlib.pyplot as plt\n\nn = 10  # How many digits we will display\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # Display original\n    ax = plt.subplot(2, n, i + 1)\n    plt.imshow(x_test[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    \n    # Display reconstruction\n    ax = plt.subplot(2, n, i + 1 + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    ax.set_xlabel(\"{}\".format(np.array2string(encoded_imgs[i],max_line_width=10)))\n    plt.gray()\n    ax.get_yaxis().set_visible(False)\n    #ax.get_xaxis().set_visible(False)\n    ;\nplt.show()","60a3e2cb":"# Showcase the inner hidden\/latent space by plotting all the test images on the latent space\nx_test_encoded = encoder.predict(x_test, batch_size=128)[0]\nplt.figure(figsize=(6, 6))\nplt.scatter(x_test_encoded[:,0], x_test_encoded[:,1], c=x_test_label, cmap='rainbow')\nplt.colorbar()\nplt.show()","164c27d5":"# Display a 2D manifold of the digits\nn = 15  # figure with 15x15 digits\ndigit_size = 28\nfigure = np.zeros((digit_size * n, digit_size * n))\n# Ssample n points within [-3, 3] standard deviations\ngrid_x = np.linspace(-3, 3, n)\ngrid_y = np.linspace(-3, 3, n)\n\nfor i, yi in enumerate(grid_x):\n    for j, xi in enumerate(grid_y):\n        z_sample = np.array([[xi, yi]])\n        x_decoded = decoder.predict(z_sample)\n        digit = x_decoded[0].reshape(digit_size, digit_size)\n        figure[i * digit_size: (i + 1) * digit_size,\n               j * digit_size: (j + 1) * digit_size] = digit\n\nplt.figure(figsize=(10, 10))\nplt.imshow(figure)\nplt.axis('off')\nplt.show()","64f6a1cd":"@interact\n\ndef morph(x = 3, y=3):\n    x_decoded = decoder.predict(np.array([[x, y]]))\n    plt.imshow(x_decoded.reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    return plt.figure()","ba130657":"#Load the dataset and show the meta-data\n!git clone https:\/\/github.com\/deepmind\/dsprites-dataset\n%cd dsprites-dataset\n%ls\ndataset_zip = np.load('dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz', encoding='latin1',allow_pickle=True)\nprint('Keys in the dataset:', dataset_zip.keys())\nimgs = dataset_zip['imgs']\n\nlatents_values = dataset_zip['latents_values']\nlatents_classes = dataset_zip['latents_classes']\nmetadata = dataset_zip['metadata'][()]\n\nprint('Metadata: \\n', metadata)\n\n\n","2b82a9f3":"# Create the training and test data set, code re-used from: https:\/\/github.com\/deepmind\/dsprites-dataset\nlatents_sizes = metadata['latents_sizes']\nprint(latents_sizes)\nlatents_bases = np.concatenate((latents_sizes[::-1].cumprod()[::-1][1:],\n                                np.array([1,])))\n\ndef latent_to_index(latents):\n  return np.dot(latents, latents_bases).astype(int)\n\n\ndef sample_latent(size=1):\n  samples = np.zeros((size, latents_sizes.size))\n  for lat_i, lat_size in enumerate(latents_sizes):\n    samples[:, lat_i] = np.random.randint(lat_size, size=size)\n\n  return samples\n\n# Sample the training set (60000)\nx_train_latents = sample_latent(size=60000)\nx_train_indices = latent_to_index(x_train_latents)\nx_train = imgs[x_train_indices]\n\n# Sample the test set (5000)\nx_test_latents = sample_latent(size=5000)\nx_test_indices = latent_to_index(x_test_latents)\nx_test = imgs[x_test_indices]","26155579":"print(x_train.shape)\nnrows = 4\nncols = 4\nfig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True, figsize=(nrows * 3, ncols *3))\nfor row in range(nrows):\n    for col in range(ncols):\n        rand_example = np.random.choice(len(x_train))\n        ax[row,col].imshow(x_train[rand_example],cmap='gray')\n        ax[row,col].set_title(\"Latent: {}\".format(x_train_latents[rand_example]),fontsize='small')\n        #ax[row,col].axis('off')\n        ax[row,col].set_xticks([])\n        ax[row,col].set_yticks([])","aa507529":"# Showcase the latent\/hidden state format, the values used to generate the image\nprint(x_train_latents.shape)\nprint('Training image nr 2 latent space: ', x_train_latents[2])\nprint(\"('color', 'shape', 'scale', 'orientation', 'posX', 'posY')\")\nprint(x_test_latents.shape)\nprint('Test image nr 2 latent space: ',x_test_latents[2])\n","7bc1031f":"# I will use a plain dense neural network in the variational autoencoder, so need to reshape 64*64 images into 1024 flat\nimport tensorflow as tf\nprint('shape ', x_train.shape)\nx_train = np.expand_dims(x_train, axis=-1)\nprint('shape ', x_train.shape)\nx_train = tf.image.resize(x_train, [32,32]) # resize \nprint('shape ', x_train.shape)\nx_train = np.squeeze(x_train, axis=-1)\nprint('shape ', x_train.shape)\nx_train = x_train.reshape([len(x_train), 32*32])\nprint('shape ', x_train.shape)\nprint('length ', len(x_train))\n# No need to normalize the data of the images, pixels are on (1) or off (0)\n\n#x_test = x_test.reshape([len(x_test), 64*64])\nx_test = np.expand_dims(x_test, axis=-1)\nx_test = tf.image.resize(x_test, [32,32]) # resize \nx_test = np.squeeze(x_test, axis=-1)\nx_test = x_test.reshape([len(x_test), 32*32])","c60a4a5b":"#Quick check the reshaped images still look good\nprint(x_train.shape)\nnrows = 4\nncols = 4\nfig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True, figsize=(nrows * 3, ncols *3))\nfor row in range(nrows):\n    for col in range(ncols):\n        rand_example = np.random.choice(len(x_train))\n        #ax[row,col].imshow(x_train[rand_example],cmap='gray')\n        ax[row,col].imshow(x_train[rand_example].reshape(32, 32),cmap='gray')\n        ax[row,col].set_title(\"Latent: {}\".format(x_train_latents[rand_example]),fontsize='small')\n        #ax[row,col].axis('off')\n        ax[row,col].set_xticks([])\n        ax[row,col].set_yticks([])","b1b76890":"# verify the labels are distributed evenly\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\n\nfig.suptitle('label distribution')\nsns.countplot(x_train_latents[:,0],ax=axes[0, 0])\nsns.countplot(x_train_latents[:,1],ax=axes[0, 1])\nsns.countplot(x_train_latents[:,2],ax=axes[0, 2])\nsns.countplot(x_train_latents[:,3],ax=axes[1, 0])\nsns.countplot(x_train_latents[:,4],ax=axes[1, 1])\nsns.countplot(x_train_latents[:,5],ax=axes[1, 2])","40aff113":"K.clear_session()\n\n# This are the dimensions of the original input, the intermediate layer and latent layer\noriginal_dim = 1024\nintermediate_dim = 1200\nlatent_dim = 8 # \n\n# Variational autoencoder model\ninput_img = keras.Input(shape=(original_dim,))\nencoded = layers.Dense(intermediate_dim, activation='relu')(input_img)\nencoded = layers.Dense(intermediate_dim, activation='relu')(encoded)\nx_mean = layers.Dense(latent_dim)(encoded)\nx_log_var = layers.Dense(latent_dim)(encoded) # implementation choice to encode the log variance i.s.o. the standard deviation\n\ndef sampling(args):\n    # reparameterization trick\n    # instead of sampling from Q(z|x), sample eps = N(0,I)\n    # then x = x_mean + x_sigma*eps= x_mean + sqrt(e^(x_log_var))*eps = x_mean + e^(0.5*x_log_var)*eps\n    x_mean, x_log_var = args\n    epsilon = K.random_normal(shape=(K.shape(x_mean)[0], latent_dim), mean=0.,\n                              stddev=1.0)\n    return x_mean + K.exp(0.5 * x_log_var) * epsilon # (e^a)^b=e^ab\nx = layers.Lambda(sampling, output_shape=(latent_dim,))([x_mean, x_log_var])\n# at this point the representation has dimension: latent_dim \n\ndecoded = layers.Dense(intermediate_dim, activation='tanh')(x)\ndecoded = layers.Dense(intermediate_dim, activation='tanh')(decoded)\ndecoded = layers.Dense(original_dim, activation='sigmoid')(decoded)\nvae = keras.Model(input_img, decoded, name='vae')\nprint (vae.summary())\n\n# Create the loss function and compile the model\n# The loss function as defined by paper Kingma\nreconstruction_loss = original_dim * keras.metrics.binary_crossentropy(input_img, decoded) \nkl_loss =  -0.5 * K.sum(1 + x_log_var - K.square(x_mean) -K.exp(x_log_var), axis=-1)\nbeta = 4 # disentangled variational autoencoder <---------------------------------------------\nvae_loss = K.mean(reconstruction_loss + beta*kl_loss)\nvae.add_loss(vae_loss)\nopt = keras.optimizers.Adam(learning_rate=0.01)\nvae.compile(optimizer=opt)\n\n# Next the encoder part and decoder model, in order to inspect the inner representation, referencing the autoencode layers (the 3 models share there weights)\n# This part can be ommitted in case you don't want to use the inner latent representation \n\n# encoder model (first part of the variational autoencoder) \nencoder = keras.Model(input_img, [x_mean, x_log_var, x], name='encoder')\nprint (encoder.summary())\n\n# decoder model (second part of the autoencoder) to be able to generate an image from an inner representation\nencoded_input = keras.Input(shape=(latent_dim,))\ndecoder_layer = vae.layers[-3](encoded_input) # Retrieve the last layers of the autoencoder model\ndecoder_layer = vae.layers[-2](decoder_layer)\ndecoder_layer = vae.layers[-1](decoder_layer)\ndecoder = keras.Model(encoded_input, decoder_layer, name='decoder')\nprint (decoder.summary())\n","2bf53396":"vae.fit(x_train,None,\n        shuffle=True,\n        epochs=15,\n        batch_size=256,\n        validation_data=(x_test, None))","feee14da":"# Encode and decode some digits, show the inner latent variable, inner representation\n\nencoded_imgs = encoder.predict(x_test)[2]\ndecoded_imgs = decoder.predict(encoded_imgs)\n#decoded_imgs = vae.predict(x_test)\n\n#Show results: original, encode inner latent representation, decoded image\nimport matplotlib.pyplot as plt\n\nnp.set_printoptions(precision=2)\nn = 8 # How many digits we will display\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # Display original\n    ax = plt.subplot(2, n, i + 1)\n    plt.imshow(x_test[i].reshape(32, 32),cmap='gray')\n    ax.set_title(x_test_latents[i],fontsize='small')\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    \n    # Display reconstruction\n    ax = plt.subplot(2, n, i + 1 + n)\n    latent = encoded_imgs[i,0]+encoded_imgs[i,1]\n    plt.imshow(decoded_imgs[i].reshape(32, 32),cmap='gray')\n    ax.set_xlabel(\"{}\".format(np.array2string(encoded_imgs[i],max_line_width=10)))\n    plt.gray()\n    ax.get_yaxis().set_visible(False)\n    ;\nplt.show()","74e8c21e":"@interact\n\ndef morph(x0 = 1, x1=1, x2 = 1, x3=1,x4 = 1, x5=1, x6 = 1, x7=1):\n    x_decoded = decoder.predict(np.array([[x0, x1,x2, x3,x4, x5,x6, x7]]))\n    plt.imshow(x_decoded.reshape(32, 32))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    return plt.figure()","2736f622":"## The variational autoencoder model\nAlso in this example, I use for the encoder and decoder both plain dense neural networks with both 2 layers. As per variational autoencoder setup, the encoder splits into two parameters in the latent space: the mean and the (log) variance. Followed by the sampling layer based on the reparameterizationn trick. A decoder neural network maps the latent space back to the original input data.\n\nLike in the previous example, the autoencoder is defined as one model, plus separately the encoder model and decoder model, referencing the autoencoder layers (the 3 models share their weights), in order to inspect the inner representation (this part can be omitted in case you don't want to use\/show the inner latent representation)\n\nThe parameters of the model are trained via the Free Energy loss function: a reconstruction loss part and the KL divergence loss part.\n","bb5dfa95":"# The promise of variational autoencoders\nI am fascinated by the possibilities of variational autoencoders. Why? Because it has the capability to abstract a compressed representation of the data, without the need to label the data. Moreover, the variational autoencoder estimates the latent\/hidden states according to the leading neuroscientific approach that might explain the structure and function of the brain, starting from the very fact that we exist!\n\nThis capability to get a compressed representation out of the original input data has the potential to abstract meaning out of data. Something that we, humans, do every minute of the day. The higher brain functions translate lower-level sensory information (e.g. pixel blob on the retina) to meaning (e.g. it is an apple) and link all the various information around that meaning together. (e.g. I know how it tastes, or if we were talking about stock prices I know it is the company that makes iPhones). Something that we have not been able to replicate in artificial neural networks. Abstracting meaning or knowledge representation is still one of the key challenges in the field of artificial intelligence research. The variational autoencoder approach looks very promising.\n\nSo, if you are interested please keep on reading and upvote top right, leave a comment or contact me directly.\n\nFor a quick introduction of autoencoders please watch this [video](https:\/\/www.youtube.com\/watch?v=9zKuYvjFFS8&t=3s), it gives you a good overview, and it inspired me to reproduce the steps taken in this notebook.","34fe5c25":"And because the latent space is 2 dimensions, we can make the internal probabilistic model of the world (the generative model) nicely visible. The generative model $p(y_{observation} , x_{hypotheses})=p(y_{observation}\\mid x_{hypotheses})p(x_{hypotheses}) $ which is the internal representation + decoder.\n","e4c7fc1b":"## The results\n\nLet's have a look at the result, and especially the latent representation.","3da5e591":"<a id='secA'><\/a>\n# Appendix\n\n<a id='secA1'><\/a>\n## The Free Energy bounds the surprise in the sensory observations\n\nBelow you will find a short math overview how the Free Energy bounds the surprise in sensory observations. For an extensive overview including the explanations, please visit the [Free Energy Principle tutorial without a PhD](https:\/\/www.kaggle.com\/charel\/learn-by-example-active-inference-in-the-brain-1) tutorial series.\n\n### Bayesian inference\n\nLet's define:  \n*  $p(x_{hypotheses})$ as the probability density of hypotheses x. \n*  $p(x_{hypotheses}\\mid  y_{observation})$ as the probability density of hypotheses x  given the observation y (conditional probability) \n*  $p(x_{hypotheses}, y_{observation})$ as the probability density of hypotheses x and observation y together (joint probability)  \n\nBayes' theorem can be derived from basic probability rules:\n\nSymmetric property:\n$$p(y_{observation} , x_{hypotheses})=p(x_{hypotheses},  y_{observation})$$  \nApply the product rule: \n$$p(x_{hypotheses}\\mid  y_{observation})p(y_{observation})=p(y_{observation}\\mid x_{hypotheses})p(x_{hypotheses})$$ \nWhich leads to Bayes' theorem:  \n$$p(x_{hypotheses}\\mid  y_{observation})=\\frac{p(y_{observation}\\mid  x_{hypotheses}) * p(x_{hypotheses})}{p(y_{observation})} $$  \n\n### Variational Bayesian Inference\n\nUnfortunately, we do have a challenge calculating the denominator $p(y_{observation})$ of Bayes' theorem. It can be easily seen by writing out (applying the sum rule, the probability of x equals the probability of the sum of the probability of x given y for all possible y) the denominator leading to equation [2a] for discrete variables or equation [2b] for continues variables:  \n$$p(x_{hypotheses}\\mid  y_{observation})=\\frac{p(y_{observation}\\mid  x_{hypotheses}) * p(x_{hypotheses})}{\\sum_xp(y_{observation}\\mid  x_{hypotheses}) * p(x_{hypotheses})} \\:\\:\\:\\:\\:   [2a]$$ \n$$p(x_{hypotheses}\\mid  y_{observation})=\\frac{p(y_{observation}\\mid  x_{hypotheses}) * p(x_{hypotheses})}{\\int p(y_{observation}\\mid  x_{hypotheses}) * p(x_{hypotheses})dx} \\:\\:\\:\\:\\:   [2b]$$ \n\nThe denominator normalizes the probability, in other words, you need to average over all possible occurrences of the sensory observations (how to even know all possible observations?). The part in the nominator: $p(y_{observation}\\mid  x_{hypotheses}) * p(x_{hypotheses})$ is just one of the cases of the denominator as is easily seen in the discrete equation. Let alone calculating an integral $\\int p(y_{observation}\\mid  x_{hypotheses}) * p(x_{hypotheses})dx$ for continues variables. Which is unavailable in a complex high dimensional real world, or requires exponential time to compute! In short, the denominator $p(y_{observation})$ is generally intractable.  \n\nThe solution in active inference is to invoke Variational Bayesian inference methods: a set of techniques to *approximate* the posterior  $p(x_{hypotheses}\\mid  y_{observation})$ probability density. Find a \u201crecognition\u201d probability density $q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})$ that approximates the target density $p(x_{hypotheses}\\mid  y_{observation})$. \n* From a family of \u201cnice\u201d probability densities, e.g. a Gaussian probability density.\n* Is specified by its sufficient statistics $\\zeta$ (e.g. for a Gaussian density: mean $\\mu$ and variance $\\sigma^2$)\n* Find the right $\\zeta$ such that $q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})$ is an approximation for $p(x_{hypotheses}\\mid  y_{observation})$, this can be done by minimizing the Free Energy (is equivalent to maximizing the ELBO).\n\n\n### The Free Energy or negative Evidence Lower BOund (ELBO)\n\nThe [Kullback-Leibler divergence](https:\/\/en.wikipedia.org\/wiki\/Kullback%E2%80%93Leibler_divergence) measures the difference between probability densities. This [video](https:\/\/www.youtube.com\/watch?v=ErfnhcEV1O8) gives an introduction to KL-Divergence.\n\nFor the brain making an internal model of the world $q_{\\zeta}(x_{hypotheses}\\mid  y_{observation})$ to approximate posterior  $p(x_{hypotheses}\\mid  y_{observation})$ probability density it needs to minimize KL with respect to the sufficient statistics $\\zeta$ (e.g. for a Gaussian density: mean $\\mu$, variance $\\sigma^2$)\n\n$$\\underset{\\zeta }{Argmin}\\:  D_{KL}(q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})\\parallel p(x_{hypotheses}\\mid  y_{observation}))$$\n\nThe Kullback-Leiber is defined as:\n\n$$D_{KL}(q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})\\parallel p(x_{hypotheses}\\mid  y_{observation})) = \\int q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})ln(\\frac{q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})}{p(x_{hypotheses}\\mid  y_{observation})})dx_{hypotheses}$$\n\nsince $p(x,y)=p(x \\mid y)p(y)$  \n$$=\\int q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})ln(\\frac{p(y_{observation})q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})}{p(x_{hypotheses},  y_{observation})})dx_{hypotheses}$$  \nsince ln(ab)=ln(a)+ln(b)  \n$$= \\int q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})ln(\\frac{q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})}{p(x_{hypotheses},  y_{observation})})dx_{hypotheses} + \\int q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})ln(p(y_{observation}))dx_{hypotheses} $$\nsince there is no $x_{hypotheses}$ in $p(y_{observation})$  \n$$= \\int q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})ln(\\frac{q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})}{p(x_{hypotheses},  y_{observation})})dx_{hypotheses} + ln(p(y_{observation}))\\int q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})dx_{hypotheses} $$\nsince the integral over a probability density function equals one \n$$= \\int q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})ln(\\frac{q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})}{p(x_{hypotheses},  y_{observation})})dx_{hypotheses} + ln(p(y_{observation}))$$  \nsince ln(a\/b)=-ln(b\/a)\n$$= -\\int q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})ln(\\frac{p(x_{hypotheses},  y_{observation})}{q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})})dx_{hypotheses} + ln(p(y_{observation}))$$ \n\nThe Free Energy is defined as:\n\n$$\\mathcal{F}(y_{observation},\\zeta)= \\int q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})ln(\\frac{q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})}{p(x_{hypotheses},  y_{observation})})dx_{hypotheses} $$\n\nHence we have that\n$$\\mathcal{F}(y_{observation},\\zeta)= D_{KL}(q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})\\parallel p(x_{hypotheses}\\mid  y_{observation}))-ln(p(y_{observation}))$$\n\nAnd since the KL-Divergence $\\geq $ 0 it follows that the Free Energy bounds the surprise in sensory observations: \n$$\\mathcal{F}(y_{observation},\\zeta) \\geq  - ln(p(y_{observation}))$$\n\nwhere surprise is defined in information theory as $- ln(p(y))$\n\n<img src=\"https:\/\/i.imgur.com\/V2eX3F2.jpg\" width=800>","391fa4c0":"In the experiment in the paper flattened 32x32 images are used, so reshaping the 64x64 images to reconstruct the experiment","2d471a8f":"# Table of Contents\n1. [Autoencoder](#sec1)\n2. [Variational autoencoder](#sec2)\n3. [Disentangled Variational autoencoder](#sec3)\n4. [Appendix - The Free Energy bounds the surprise in the sensory observations](#secA)\n\nI would like to acknowledge these sources that helped me better understand the (variational) autoencoder, learning by example:  \nhttps:\/\/blog.keras.io\/building-autoencoders-in-keras.html  \nhttps:\/\/www.slideshare.net\/JonathanRonen\/jonathan-ronen-variational-autoencoders-tutorial  \nhttps:\/\/nbviewer.jupyter.org\/gist\/jonathanronen\/69902c1a97149ab4aae42e099d1d1367  \nhttps:\/\/www.kaggle.com\/rvislaywade\/visualizing-mnist-using-a-variational-autoencoder\/  ","08a66fa8":"Verify the labels are distributed evenly","678dbdb0":"## Training\nThe disentangled variational autoencoder is trained the same way the the (variational) autoencoder. No need to supply the labels. ","f4d96f76":"<a id='sec1'><\/a>\n# Autoencoder\nAn autoencoder consists out of 2 parts: encoder (reducing dimensionality to a compressed representation) + decoder (reconstructing the original input from the compressed representation). It is a neural network the can learn without the need for label annotation because the difference between the original input and reconstructed output is used as the loss function to optimize the network. Reducing dimensionality is enforced by a small or bottleneck in the inner part of the auto-encoder, the so-called latent space.\n\nFor the first autoencoder example, I will use the well-known MNIST dataset. The 28*28 images are compressed to a 2 neuron latent representation, and I am especially interested in how this inner representation looks like.\n\n\n![](https:\/\/i.imgur.com\/M8yBf54.jpg)\n\n\n\n","c88e3a9c":"## The results\n\nLet's have a look at the result, and especially the latent representation.","09ce1380":"## The disentangled variational autoencoder model\n\nThe same code\/model as the variational autoencoder model, except the Beta weighting factor is set to 4 and the encoder\/decoder neural networks have an additional fully connected layer, the same as in the experiment described in the paper.","ff5ec632":"## The results\n\nLet's have a look at the result, and especially the latent representation.","050eba7e":"## Dsprites\nThe   [Dsprites](https:\/\/github.com\/deepmind\/dsprites-dataset) dataset is a great way to assess the capability of unsupervised learning methods to reconstruct\/abstract latent variables. Based on 5 independent variables images are generated, and it is interesting to see if and how a variable auto-encoder can discover these 5 latents variables out of the images. The latent variables used to generate the images:\n+ Color: 1 value: white (fixed to 1, hence 5 latent variables and not 6)\n+ Shape: 3 values: square, ellipse, heart\n+ Scale: 6 values: linearly spaced in [0.5, 1]\n+ Orientation: 40 values in [0, 2 pi]\n+ Position X: 32 values in [0, 1]\n+ Position Y: 32 values in [0, 1]  \n\nThe dataset contains 1*3*6*40*32*32=737280 total images, every posible combination once.\nThe images are: (737280 x 64 x 64, uint8) in black and white.\n\nExample images in the dataset, picture from the dataset github page:  \n![](https:\/\/github.com\/deepmind\/dsprites-dataset\/blob\/master\/dsprites.gif?raw=true)","ce47d622":"If you copy and run this notebook (top-right), the cell below will give you 2 slide bars to explore the inner latent space and you can see how the picture morphs into the different numbers as shown above. Quite fun to experiment with.\n","462ed4f4":"## Training\nTraining is done like a normal neural network, except you don't need to supply the labels. The input and the desired output are the same.","895cc723":"## The reparameterization trick\n\nThe neural network $\\theta$ and $\\phi$ parameters can be optimized at the same time using stochastic gradient descent. However, a reparameterization trick is needed to create a differentiable inference model in order to use backpropagation (this cannot be done for a random sampling process). The smart trick is to sample a hidden state x via x = x_mean + x_standard_dev * epsilon, where epsilon is a random normal zero-mean distribution with standard dev 1. Now the random node is not blocking the path for backpropagation for $\\mu$ and $\\sigma$. \n\n<img src=\"https:\/\/i.imgur.com\/Re0AO8z.jpg\" width=600>\n\nIn this [video](https:\/\/youtu.be\/rK6bchqeaN8?t=3140) you can find a great explanation of why the reparameterization trick works: \"separating the stochastic part from the deterministic parts\" and \"thus gradient of averages is replaced by taking the gradient and then taking the average, this reduces the variance hugely\".  \n\nTo summarize it in one slide:\n\n\n![](https:\/\/i.imgur.com\/fwua9AB.jpg)\n\nNote: In the free energy Kaggle notebooks I used x as the hidden state and y as the sensory observations, hence I used the same set-up in this notebook. In autoencoder literature is is often custom to use z as the latent space notation. Just a notation thing.\n\n","aebca696":"By compressing the inner representation to 2 dimensions the generated pictures become \"blurry\". If you would take a higher latent dimension the autoencoder can generate sharper pictures. In the slide\/picture inserted above (introducing the autoencoder), you can compare the results with the different compressions to get an impression. Or you can copy this notebook and try for yourself with different latent dimensions, different encoder\/decoder neural network setups, etc.\n\nThe advantage of compressing the inner latent representation to 2 dimensions is that you can make that latent representation visible in a scatterplot and showcase that it has structure. The internal representation of the numbers group together, interesting!","79ba8de3":"## The autoencoder model\n\nIn this example, I use for the encoder and decoder both plain dense neural networks, both 2 layers. You could also use deeper structures or RNN\/LSTM or convolutional layers. The code is parameterized, so you can experiment for yourself on the dimension of e.g. the latent space. Note that the encoder and decoder don't have to have the exact same network structure.  \nThe autoencoder is defined as one model, plus separately the encoder model and decoder model, referencing the autoencoder layers (the 3 models share their weights), in order to inspect the inner representation (this part can be omitted in case you don't want to use\/show the inner latent representation)","10948158":"By compressing the inner representation to 2 dimensions the generated pictures become \"blurry\", like this happened with the autoencoder. If you would take a higher latent dimension the variational autoencoder can generate sharper pictures. In the slide\/picture inserted above (introducing the variational autoencoder), you can compare the results with the different compressions to get an impression. \n\nAnd again, the advantage of compressing the inner latent representation to 2 dimensions is that you can make that latent representation visible in a scatterplot and showcase that it has structure. The internal representation of the numbers group tigher together if you compare it to the auoencoder latent space scatterplot (e.g. look at the dimensions of the scatterplot).","1dc49024":"<a id='secA2'><\/a>\n## From Free Energy to the VAE loss function\n\nThe Free Energy is defined as:\n\n$$\\mathcal{F}(y_{observation}; \\theta, \\phi)= \\int q_{ \\phi  }(x_{hypotheses}\\mid  y_{observation})ln(\\frac{q_{ \\phi  }(x_{hypotheses}\\mid  y_{observation})}{p_{\\theta}(x_{hypotheses},  y_{observation})})dx_{hypotheses} $$\n\n$$= \\int q_{ \\phi  }(x_{hypotheses}\\mid  y_{observation})ln(\\frac{q_{ \\phi  }(x_{hypotheses}\\mid  y_{observation})}{p_{\\theta}(x_{hypotheses})p_{\\theta}( y_{observation} \\mid x_{hypotheses} )})dx_{hypotheses} $$\n\n$$= \\int q_{ \\phi  }(x_{hypotheses}\\mid  y_{observation})[ln(\\frac{q_{ \\phi  }(x_{hypotheses}\\mid  y_{observation})}{p_{\\theta}(x_{hypotheses})})-ln(p_{\\theta}( y_{observation} \\mid x_{hypotheses} ))]dx_{hypotheses}$$\n\n$$= \\int q_{ \\phi  }(x_{hypotheses}\\mid  y_{observation})ln(\\frac{q_{ \\phi  }(x_{hypotheses}\\mid  y_{observation})}{p_{\\theta}(x_{hypotheses})})dx_{hypotheses} - \\int q_{ \\phi  }(x_{hypotheses}\\mid  y_{observation}) ln(p_{\\theta}( y_{observation} \\mid x_{hypotheses} ))dx_{hypotheses}$$\n\n$$= \\underset{KL-loss, \\:divergence\\: from \\:prior}{\\underbrace{D_{KL}(q_{ \\phi }( x_{hypotheses}\\mid y_{observation} )\\parallel p_{\\theta}(x_{hypotheses}))}} - \\underset{reconstruction \\: loss}{\\underbrace{\\mathbb{E}_{q_{ \\theta }(x_{hypotheses}\\mid  y_{observation})} [ln(p_{\\theta}  (y_{observation}\\mid x_{hypotheses}))]}} $$\n\n$$=\\mathcal{Loss}(y_{observation};\\theta, \\phi)$$","6d1dbe30":"Note that the figure changes whenever you do a new run, it leads to different latent structures.\n\nWe can now also take some latent representation and see what picture is generated. The internal representation + decoder has become a kind of generative model.\nIf you would copy and run this notebook (top-right) the cell below will give you 2 slide bars to explore the inner latent space and you can see how the picture morphs into the different numbers. Quite fun to experiment with.","fccffc7f":"Below, again, you'll find the code to interact with the results. The effect I hoped to see is that certain latent variables of the autoencoder start to correspond one-to-one with the hidden states of the original data. That is, one latent variable corresponds with the horizontal position, vertical position, the size, orientation, or the type of image.\n\nPlease use the interaction below (copy-paste the notebook and run the cell) to find out for yourself. In my trials, I saw that \n+ 1 of the latent variables was clearly corresponding to seize \n+ 2 latent variables to the position (it was not a straight forward x and y correlation, but more 2 half arcs) \n+ 1 or 2 latent variables did something with the shape (but the shapes were blurry, so I could not conclude if it was trying to get the shape or orientation right).  \n\nI can conclude I saw a some form of correlation between the factors that generated the data sets and the latent variables, interesting!","ece672ab":"\n## Minimizing the Free Energy or minimizing the negative Evidence Lower BOund (ELBO)\n\nThe Free Energy bounds the surprise in the sensory observations (See [Appendix A.1](#secA1))\n$$\\mathcal{F}(y_{observation},\\zeta)= D_{KL}(q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})\\parallel p(x_{hypotheses}\\mid  y_{observation}))-ln(p(y_{observation}))\\geq  -ln(p(y_{observation}))$$\n\nWhere the Free Energy is defined as:\n\n$$\\mathcal{F}(y_{observation},\\zeta)= \\int q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})ln(\\frac{q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})}{p(x_{hypotheses},  y_{observation})})dx_{hypotheses} $$\n\nBy minimizing the Free energy you will get:\n+ a better approximation of $q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})$ for $p(x_{hypotheses}\\mid  y_{observation})$, in other words $q_{ \\zeta }(x_{hypotheses}\\mid  y_{observation})$ becomes a more reliable approximation of the true hidden states (they are called hidden because they are not directly observable, hence they need to be inferenced). \n+ a lower surprise in the sensory observations $-ln(p(y_{observation}))$, thus the generative model becomes better generating the predicted sensory observations. In other words, a better internal world model.\n\nThe Free energy equals the negative ELBO, as you can see by comparing it with the ELBO definition in this paper [Auto-encoding Variational Bayes](https:\/\/arxiv.org\/pdf\/1312.6114.pdf) and corresponding [tutorial](https:\/\/arxiv.org\/pdf\/1906.02691.pdf):\n$$\\mathcal{ELBO}(y_{observation}; \\theta, \\phi)= ln(p_{\\theta}(y_{observation})) - D_{KL}(q_{ \\phi }(x_{hypotheses}\\mid  y_{observation})\\parallel p_{\\theta}(x_{hypotheses}\\mid  y_{observation}))$$\n\nwhere $ \\theta$ and $\\phi$ are the neural network parameters of the encoder resp decoder.\n\nMinimizing the free energy by a gradient descent, equals maximizing the model evidence by gradient ascent on the ELBO, which equals minimizing the negative ELBO by gradient descent. I hope I didn't confuse everybody by now :-)  \nIn short, the Free energy (or negative ELBO) is the loss function of the variational autoencoder.\n\nThis loss function can be mathematically rewritten (see [appendix A2](#secA2)) as\n\n$$\\mathcal{Loss}(y_{observation};\\theta, \\phi)= \\underset{KL-loss, \\:divergence\\: from \\:prior}{\\underbrace{D_{KL}(q_{ \\phi }( x_{hypotheses}\\mid y_{observation} )\\parallel p_{\\theta}(x_{hypotheses}))}} + \\underset{reconstruction \\: loss}{\\underbrace{-\\mathbb{E}_{q_{ \\theta }(x_{hypotheses}\\mid  y_{observation})} [ln(p_{\\theta}  (y_{observation}\\mid x_{hypotheses}))]}} $$\n\nwhich you will often see in machine learning papers. It breaks down into 2 parts.  \n \nA reconstruction loss (second part of the above equation) forcing the decoded samples to match the initial inputs (just like in our previous autoencoder). Maximizing the reconstruction likelihood $\\mathbb{E}_{q_{ \\theta }(x_{hypotheses}\\mid  y_{observation})} [ln(p_{\\theta}  (y_{observation}\\mid x_{hypotheses})]$ -> minimizing the negative reconstruction likelihood -> minimizing the reconstruction loss of the variational autoencoder.\n\n$$\\mathcal{Loss}(y_{observation};\\theta, \\phi)= \\underset{KL-loss, \\:divergence\\: from \\:prior}{\\underbrace{D_{KL}(q_{ \\phi }( x_{hypotheses}\\mid y_{observation} )\\parallel p_{\\theta}(x_{hypotheses}))}} + \\underset{reconstruction \\: loss}{\\underbrace{binary \\: crossentropy \\: (input \\: image, reconstructed \\: image)}} $$\n\nA KL divergence loss (first part of the above equation) between the learned latent distribution and the prior distribution, acting as a regularization term to learn well-formed latent spaces. The probability functions are assumed to be Gaussian\/Normal distributions, they can be mathematically efficiently described with just 2 parameters: mean ($\\mu$ - the average value of all points) and standard deviation ($\\sigma$ - the spread of the points, or can be referred to as $\\sigma^2$ - the variance ). Therefor you will find the $\\mu$ and $\\sigma$ in the latent space of the variational autoencoder. Note that actually a full covariance matrix should be learned to build a true multivariate Gaussian model including covariances between the latent dimensions. However, a simplifying assumption is made that the covariance matrix only has nonzero values on the diagonal, allowing a simple vector in the latent space.\n\nWith Gaussian\/Normal distributions the KL divergence simplifies to:\n\n$$\\mathcal{Loss}(y_{observation};\\theta, \\phi)= \\underset{KL-loss, \\:divergence\\: from \\:prior}{\\underbrace{-\\frac{1}{2}\\sum_j(1+log(\\sigma_j^2)-\\mu_j^2-\\sigma_j^2)}} + \\underset{reconstruction \\: loss}{\\underbrace{binary \\: crossentropy \\: (input \\: image, reconstructed \\: image)}} $$\n\nWhere j is the dimension of the latent space. This formula you can find back in the code below as the loss function.\n","c63abdbd":"## Training\nThe variational autoencoder is trained the same way the the regular autoencoder. No need to supply the labels. ","4473f1a4":"<a id='sec3'><\/a>\n# Disentangled variational autoencoder\n\nOne further enhancement to the variational autoencoder was suggested in this [paper](https:\/\/openreview.net\/pdf?id=Sy2fzU9gl) to get well-formed latent spaces, branded as the disentangled variational autoencoder. It basically boils down to adding an extra weight factor on the KL-loss part of the loss function, to enhance the forming of better-formed latent spaces, even into interpretable factorized latent representations.  \nOne of their experiments is reproduced below. By generating a data-set of images out of 5 independent latent variables, can the disentangled variational autoencoder reconstruct these latent variables, by learning them from the images? Yes, it can do so (to a certain extend). Interesting! And note the word factorized, it means the generated latent variables discovered by the disentangled variational autoencoder should correspond 1:1 to the original latent spaces used to generate the images.\n","013f2f23":"<a id='sec2'><\/a>\n# Variational autoencoder\nThe variational autoencoder extends the capabilities of autoencoder by not only learning the representation of data but also the distribution of data by using variational Bayesian inference. It is a probabilistic autoencoder. Why is that important? Because according to the leading neuroscientific approach, the [Free Energy Principle](https:\/\/www.wired.com\/story\/karl-friston-free-energy-principle-artificial-intelligence\/) (FEP) by [Karl Friston](https:\/\/www.fil.ion.ucl.ac.uk\/~karl\/), the brain is a probabilistic prediction mechanism, one that attempts to minimize the error of its hypothesis about the world and the sensory input it receives:\n\n+ The skull-bound brain needs to infer the world.\n+ The brain builds an internal probabilistic model of the world using Bayesian inference (the generative model).\n+ Discrepancies between the internal model (the prediction) and the sensory observations result in prediction error.\n+ The brain aims to minimize the prediction errors by minimizing the Free Energy. It can do so by improving perception, acting on the environment, learning the generative model, etc\n\nThe Free energy is equivalent to the negative of the Evidence Lower BOund (ELBO), the loss function of the variational autoencoder. That is why I started this notebook with: the variational autoencoder estimates the latent\/hidden states according to the leading neuroscientific approach!\n\nI hope this point got your attention!\n\nUnderstanding papers published about the free energy has a steep learning curve.  I started a tutorial series on Kaggle which I positioned as [Free Energy Principle tutorial without a PhD](https:\/\/www.kaggle.com\/charel\/learn-by-example-active-inference-in-the-brain-1) and recorded my notes with examples to make it much more accessible.\n\n<img src=\"https:\/\/i.imgur.com\/Jt5TANO.jpg\" width=800>\n\n\n"}}