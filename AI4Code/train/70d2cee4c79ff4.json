{"cell_type":{"63382f58":"code","a2de226f":"code","738c9545":"code","9852d271":"code","835154cb":"code","2c0c5fa1":"code","410cb018":"code","0be450a1":"code","5668715d":"code","2a2ac3ea":"code","14bcee96":"markdown","91ab91e9":"markdown","bb923773":"markdown","7919fd37":"markdown","85a69b4f":"markdown","e9fd4a6e":"markdown","34fa9b31":"markdown","e0223137":"markdown","9e04fd85":"markdown"},"source":{"63382f58":"#Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n#generating some random data in a two-dimensional\nX= -2 * np.random.rand(100,2)\nX1 = 1 + 2 * np.random.rand(50,2)\nX[50:100, :] = X1\nplt.scatter(X[ : , 0], X[ :, 1], s = 50, c = 'b')\nplt.show()\n\n#k-means\nKmean = KMeans(n_clusters=2)\nKmean.fit(X)\n\n#Finding the centroid\nKmean.cluster_centers_\n\nplt.scatter(X[ : , 0], X[ : , 1], s =50, c='b')\nplt.scatter(-0.94665068, -0.97138368, s=200, c='g', marker='s')\nplt.scatter(2.01559419, 2.02597093, s=200, c='r', marker='s')\nplt.show()\n\n#Testing the algorithm\nKmean.labels_\n\n#predicting the cluster of a data point\nsample_test=np.array([-3.0,-3.0])\nsecond_test=sample_test.reshape(1, -1)\nKmean.predict(second_test)","a2de226f":"# Modules\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets.samples_generator import (make_blobs,\n                                                make_circles,\n                                                make_moons)\nfrom sklearn.cluster import KMeans, SpectralClustering\n#from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nplt.style.use('fivethirtyeight')\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n# Import the data\ndf = pd.read_csv('..\/input\/old-faithful\/faithful.csv')\ndf = df[[\"eruptions\",\"waiting\"]]\n\n# Plot the data\nplt.figure(figsize=(6, 6))\nplt.scatter(df.iloc[:, 0], df.iloc[:, 1])\nplt.xlabel('Eruption time in mins')\nplt.ylabel('Waiting time to next eruption')\nplt.title('Visualization of raw data');\n\n# Standardize the data\nX_std = StandardScaler().fit_transform(df)\n\n# Run local implementation of kmeans\nkm = KMeans(n_clusters=2, max_iter=20, random_state=20)\nkm.fit(X_std)\ncentroids = km.cluster_centers_\n\n# Plot the clustered data\nfig, ax = plt.subplots(figsize=(6, 6))\nplt.scatter(X_std[km.labels_ == 0, 0], X_std[km.labels_ == 0, 1],\n            c='green', label='cluster 1')\nplt.scatter(X_std[km.labels_ == 1, 0], X_std[km.labels_ == 1, 1],\n            c='blue', label='cluster 2')\nplt.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=300,\n            c='r', label='centroid')\nplt.legend()\nplt.xlim([-2, 2])\nplt.ylim([-2, 2])\nplt.xlabel('Eruption time in mins')\nplt.ylabel('Waiting time to next eruption')\nplt.title('Visualization of clustered data', fontweight='bold')\nax.set_aspect('equal');","738c9545":"# Modules\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\n\ndataset = load_iris()\n\nx = dataset.data;\n\n# Standardizing the features\nx = StandardScaler().fit_transform(x)\n\nkm = KMeans(n_clusters=3, max_iter=1000)\nkm.fit(x)\n\nkm.cluster_centers_","9852d271":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\ndataset = load_iris()\n\nx = dataset.data;\ny = dataset.target.reshape(-1,1);\n\n# Standardizing the features\nx = StandardScaler().fit_transform(x)\n\n#PCA\npca = PCA(n_components=1)\nfeatures = pca.fit_transform(x)\n\npca.explained_variance_ratio_\n\n#classification\nencoder = OneHotEncoder()\ntargets = encoder.fit_transform(y)\n\ntrain_features, test_features, train_targets, test_targets = train_test_split(features,targets, test_size=0.2)\n\nmodel = Sequential()\n# first parameter is output dimension\nmodel.add(Dense(10, input_dim=1, activation='relu'))\nmodel.add(Dense(10, input_dim=10, activation='relu'))\nmodel.add(Dense(10, input_dim=10, activation='relu'))\nmodel.add(Dense(3, activation='softmax'))\n\n#we can define the loss function MSE or negative log lokelihood\n#optimizer will find the right adjustements for the weights: SGD, Adagrad, ADAM ...\nmodel.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\nmodel.summary()\n\nmodel.fit(train_features, train_targets, epochs=10, batch_size=20, verbose=2)\n\nloss, accuracy = model.evaluate(test_features, test_targets)\n\nprint(\"Accuracy on the test dataset: %.2f\" % accuracy)\n\n","835154cb":"# PCA2\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\ndataset = load_iris()\n\nx = dataset.data;\ny = dataset.target.reshape(-1,1);\n\n# Standardizing the features\nx = StandardScaler().fit_transform(x)\n\n#PCA\npca = PCA(n_components=2)\nfeatures = pca.fit_transform(x)\n\npca.explained_variance_ratio_\n\n#classification\nencoder = OneHotEncoder()\ntargets = encoder.fit_transform(y)\n\ntrain_features, test_features, train_targets, test_targets = train_test_split(features,targets, test_size=0.2)\n\nmodel = Sequential()\n# first parameter is output dimension\nmodel.add(Dense(10, input_dim=2, activation='relu'))\nmodel.add(Dense(10, input_dim=10, activation='relu'))\nmodel.add(Dense(10, input_dim=10, activation='relu'))\nmodel.add(Dense(3, activation='softmax'))\n\n#we can define the loss function MSE or negative log lokelihood\n#optimizer will find the right adjustements for the weights: SGD, Adagrad, ADAM ...\nmodel.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\nmodel.summary()\n\nmodel.fit(train_features, train_targets, epochs=10, batch_size=20, verbose=2)\n\nloss, accuracy = model.evaluate(test_features, test_targets)\n\nprint(\"Accuracy on the test dataset: %.2f\" % accuracy)","2c0c5fa1":"# PCA 3\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\ndataset = load_iris()\n\nx = dataset.data;\ny = dataset.target.reshape(-1,1);\n\n# Standardizing the features\nx = StandardScaler().fit_transform(x)\n\n#PCA\npca = PCA(n_components=3)\nfeatures = pca.fit_transform(x)\n\npca.explained_variance_ratio_\n\n#classification\nencoder = OneHotEncoder()\ntargets = encoder.fit_transform(y)\n\ntrain_features, test_features, train_targets, test_targets = train_test_split(features,targets, test_size=0.2)\n\nmodel = Sequential()\n# first parameter is output dimension\nmodel.add(Dense(10, input_dim=3, activation='relu'))\nmodel.add(Dense(10, input_dim=10, activation='relu'))\nmodel.add(Dense(10, input_dim=10, activation='relu'))\nmodel.add(Dense(3, activation='softmax'))\n\n#we can define the loss function MSE or negative log lokelihood\n#optimizer will find the right adjustements for the weights: SGD, Adagrad, ADAM ...\nmodel.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\nmodel.summary()\n\nmodel.fit(train_features, train_targets, epochs=10, batch_size=20, verbose=2)\n\nloss, accuracy = model.evaluate(test_features, test_targets)\n\nprint(\"Accuracy on the test dataset: %.2f\" % accuracy)","410cb018":"# PCA 4\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\ndataset = load_iris()\n\nx = dataset.data;\ny = dataset.target.reshape(-1,1);\n\n# Standardizing the features\nx = StandardScaler().fit_transform(x)\n\n#PCA\npca = PCA(n_components=4)\nfeatures = pca.fit_transform(x)\n\npca.explained_variance_ratio_\n\n#classification\nencoder = OneHotEncoder()\ntargets = encoder.fit_transform(y)\n\ntrain_features, test_features, train_targets, test_targets = train_test_split(features,targets, test_size=0.2)\n\nmodel = Sequential()\n# first parameter is output dimension\nmodel.add(Dense(10, input_dim=4, activation='relu'))\nmodel.add(Dense(10, input_dim=10, activation='relu'))\nmodel.add(Dense(10, input_dim=10, activation='relu'))\nmodel.add(Dense(3, activation='softmax'))\n\n#we can define the loss function MSE or negative log lokelihood\n#optimizer will find the right adjustements for the weights: SGD, Adagrad, ADAM ...\nmodel.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\nmodel.summary()\n\nmodel.fit(train_features, train_targets, epochs=10, batch_size=20, verbose=2)\n\nloss, accuracy = model.evaluate(test_features, test_targets)\n\nprint(\"Accuracy on the test dataset: %.2f\" % accuracy)","0be450a1":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom keras.datasets import mnist\n\ndataset = mnist.load_data()\n\nx = dataset.data;\ny = dataset.target.reshape(-1,1);\n\n# Standardizing the features\nx = StandardScaler().fit_transform(x)\n\n#PCA\npca = PCA(n_components=1)\nfeatures = pca.fit_transform(x)\n\npca.explained_variance_ratio_\n\n#classification\nencoder = OneHotEncoder()\ntargets = encoder.fit_transform(y)\n\ntrain_features, test_features, train_targets, test_targets = train_test_split(features,targets, test_size=0.2)\n\nmodel = Sequential()\n# first parameter is output dimension\nmodel.add(Dense(10, input_dim=1, activation='relu'))\nmodel.add(Dense(10, input_dim=10, activation='relu'))\nmodel.add(Dense(10, input_dim=10, activation='relu'))\nmodel.add(Dense(3, activation='softmax'))\n\n#we can define the loss function MSE or negative log lokelihood\n#optimizer will find the right adjustements for the weights: SGD, Adagrad, ADAM ...\nmodel.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\nmodel.summary()\n\nmodel.fit(train_features, train_targets, epochs=10, batch_size=20, verbose=2)\n\nloss, accuracy = model.evaluate(test_features, test_targets)\n\nprint(\"Accuracy on the test dataset: %.2f\" % accuracy)","5668715d":"from sklearn.datasets import load_wine\nimport pandas as pd\nimport numpy as np\nnp.set_printoptions(precision=4)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nwine = load_wine()\nX = pd.DataFrame(wine.data, columns=wine.feature_names)\ny = pd.Categorical.from_codes(wine.target, wine.target_names)\n\nX.shape\n\nX.head()\n\nwine.target_names\n\n#create a DataFrame containing both the features and classes\ndf = X.join(pd.Series(y, name='class'))\n\n# create a vector with the means of each feature.\nclass_feature_means = pd.DataFrame(columns=wine.target_names)\nfor c, rows in df.groupby('class'):\n    class_feature_means[c] = rows.mean()\nclass_feature_means\n\n#within class scatter matrix\nwithin_class_scatter_matrix = np.zeros((13,13))\nfor c, rows in df.groupby('class'):\n    rows = rows.drop(['class'], axis=1)\n    \n    s = np.zeros((13,13))\nfor index, row in rows.iterrows():\n        x, mc = row.values.reshape(13,1), class_feature_means[c].values.reshape(13,1)\n        \n        s += (x - mc).dot((x - mc).T)\n    \n        within_class_scatter_matrix += s\n    \n#between class scatter matrix\nfeature_means = df.mean()\nbetween_class_scatter_matrix = np.zeros((13,13))\nfor c in class_feature_means:    \n    n = len(df.loc[df['class'] == c].index)\n    \n    mc, m = class_feature_means[c].values.reshape(13,1), feature_means.values.reshape(13,1)\n    \n    between_class_scatter_matrix += n * (mc - m).dot((mc - m).T)\n    \n#solve the generalized eigenvalue problem\neigen_values, eigen_vectors = np.linalg.eig(np.linalg.inv(within_class_scatter_matrix).dot(between_class_scatter_matrix))\n\n#the eigenvalue maps to the same eigenvector after sorting\npairs = [(np.abs(eigen_values[i]), eigen_vectors[:,i]) for i in range(len(eigen_values))]\npairs = sorted(pairs, key=lambda x: x[0], reverse=True)\nfor pair in pairs:\n    print(pair[0])\n    \neigen_value_sums = sum(eigen_values)\nprint('Explained Variance')\nfor i, pair in enumerate(pairs):\n    print('Eigenvector {}: {}'.format(i, (pair[0]\/eigen_value_sums).real))\n    \n#create a matrix W with the first two eigenvectors.\nw_matrix = np.hstack((pairs[0][1].reshape(13,1), pairs[1][1].reshape(13,1))).real\n\n# dot product of X and W into a new matrix Y\nX_lda = np.array(X.dot(w_matrix))\n\nle = LabelEncoder()\ny = le.fit_transform(df['class'])\n\n#plot the data\nplt.xlabel('LD1')\nplt.ylabel('LD2')\nplt.scatter(\n    X_lda[:,0],\n    X_lda[:,1],\n    c=y,\n    cmap='rainbow',\n    alpha=0.7,\n    edgecolors='b'\n)","2a2ac3ea":"from sklearn.datasets import load_wine\nimport pandas as pd\nimport numpy as np\nnp.set_printoptions(precision=4)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nwine = load_wine()\nX = pd.DataFrame(wine.data, columns=wine.feature_names)\ny = pd.Categorical.from_codes(wine.target, wine.target_names)\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda = LinearDiscriminantAnalysis()\nX_lda = lda.fit_transform(X, y)\n\nlda.explained_variance_ratio_\n\nplt.xlabel('LD1')\nplt.ylabel('LD2')\nplt.scatter(\n    X_lda[:,0],\n    X_lda[:,1],\n    c=y,\n    cmap='rainbow',\n    alpha=0.7,\n    edgecolors='b'\n)\n\nX_train, X_test, y_train, y_test = train_test_split(X_lda, y, random_state=1)\n\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\nconfusion_matrix(y_test, y_pred)","14bcee96":"# K-means Clustering\nK-means clustering is one of the simplest and popular unsupervised machine learning algorithms.","91ab91e9":"## Checkpoint 2\nChange the data dimention using PCA from 1-4 and observe the results.","bb923773":"![](http:\/\/www.learnbymarketing.com\/wp-content\/uploads\/2015\/01\/method-k-means-steps-example.png)","7919fd37":"# Kmeans on Geyser\u2019s Eruptions Segmentation","85a69b4f":"## Checkpoint 3\nImplement PCA on MNIST data and then use that data to classify output.","e9fd4a6e":"Implementing the Linear Discriminant Analysis algorithm, can use the predefined LinearDiscriminantAnalysis class made available to us by the scikit-learn library","34fa9b31":"# Principal Component Analysis (PCA)","e0223137":"# Linear Discriminant Analysis (LDA)","9e04fd85":"## Checkpoint 1\nImplement k-means on Iris dataset with k=3"}}