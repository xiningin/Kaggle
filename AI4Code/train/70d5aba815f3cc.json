{"cell_type":{"af087846":"code","1b4e54f6":"code","fc4f1149":"code","aca56b10":"code","818d2f12":"code","81da20ff":"code","843c9287":"code","ac90976d":"code","1091453f":"code","3b92b034":"code","a7e5e9f8":"code","c9b7422b":"code","55ecf7e2":"code","db0feb2e":"code","fcdf1b57":"code","8ca519ca":"code","cebfe722":"code","ec9afaef":"markdown","f66dc2fb":"markdown","c42b736f":"markdown"},"source":{"af087846":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV faya_2015\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1b4e54f6":"import seaborn as sns\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n%matplotlib inline","fc4f1149":"#Read data first\ntrain_df=pd.read_csv(\"..\/input\/train.csv\")\ntest_df=pd.read_csv(\"..\/input\/test.csv\")\nsample_df=pd.read_csv(\"..\/input\/sample_submission.csv\")","aca56b10":"# This is a binary classification problem\ntrain_df.target.value_counts()","818d2f12":"#Lets look at the dimensions of the training data\ntrain_df.shape","81da20ff":"train_df.columns.values\n# Looks like a bunch of anonymized variables","843c9287":"# Any missing values?\nnull_data = train_df[train_df.isnull().any(axis=1)]\nnull_data","ac90976d":"#Let's do a heatmap to see the correlations\ntrain_df_corr=train_df.corr()","1091453f":"train_df_corr.head()","3b92b034":"sns.heatmap(train_df_corr)","a7e5e9f8":"#It does not look like any of the variables are correlated\nupper = train_df_corr.abs().where(np.triu(np.ones(train_df_corr.abs().shape), k=1).astype(np.bool))\n\n# Find features with correlation greater than 0.2\ncorrelated = [column for column in upper.columns if any(upper[column] > 0.2)]\n\ncorrelated\n\n# no variables are correlated","c9b7422b":"# there are too many input columns, let's try dimensionality reduction\n#1. PCA\n#2. Important features using Random forest\n#3. PCa using random forest","55ecf7e2":"#convert it to numpy arrays\nX=train_df.drop(columns=['ID_code','target']).values\n\n#Scaling the values\nX = scale(X)\n\npca = PCA(n_components=200)\n\npca.fit(X)\n\n#The amount of variance that each PC explains\nvar= pca.explained_variance_ratio_\n\n#Cumulative Variance explains\nvar1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n\nprint(var1)\n\nplt.plot(var1)\n\n#pd.DataFrame(pca.components_).round(2).to_csv(\"xyz.csv\")\n\n# now look at heatmap and figure out explainability","db0feb2e":"import lightgbm as lgb\nparam = {\n        'num_leaves': 10,\n        'max_bin': 119,\n        'min_data_in_leaf': 11,\n        'learning_rate': 0.02,\n        'min_sum_hessian_in_leaf': 0.00245,\n        'bagging_fraction': 1.0, \n        'bagging_freq': 5, \n        'feature_fraction': 0.05,\n        'lambda_l1': 4.972,\n        'lambda_l2': 2.276,\n        'min_gain_to_split': 0.65,\n        'max_depth': 14,\n        'save_binary': True,\n        'seed': 1337,\n        'feature_fraction_seed': 1337,\n        'bagging_seed': 1337,\n        'drop_seed': 1337,\n        'data_random_seed': 1337,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': False,\n    }","fcdf1b57":"features = [c for c in train_df.columns if c not in ['ID_code', 'target']]\ntarget = train_df['target']","8ca519ca":"%%time\nimport time\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, roc_curve\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019)\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\nfeature_importance_df = pd.DataFrame()\n\nstart = time.time()\n\n\nfor fold_, (trn_idx, val_idx) in enumerate(skf.split(train_df.values, target.values)):\n    print(\"fold n\u00b0{}\".format(fold_))\n    trn_data = lgb.Dataset(train_df.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train_df.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 100)\n    oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test_df[features], num_iteration=clf.best_iteration) \/ 5\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))","cebfe722":"##submission\nsub_df = pd.DataFrame({\"ID_code\":test_df[\"ID_code\"].values})\nsub_df[\"target\"] = predictions\nsub_df.to_csv(\"lgb_submission.csv\", index=False)","ec9afaef":"## Before we train let's do some cross validation","f66dc2fb":"# Machine Learning","c42b736f":"### Looks like PCA wont work, there are no features that stand out"}}