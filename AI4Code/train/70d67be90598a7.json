{"cell_type":{"a1785f9d":"code","20dafe58":"code","09fcc928":"code","e2925f1f":"code","1b9060f8":"code","6bcce2e5":"code","9f4f50b4":"code","e5ae1da1":"code","23c28176":"code","e318ebc3":"code","eeb3f3ef":"code","945ae0f7":"code","54bfc2fb":"code","28cb79a8":"code","ed5ae5f7":"code","485844db":"code","cfcc9804":"code","a673fff4":"code","927500ea":"code","64e5a8f8":"code","9e6e0494":"code","63bae18e":"code","03c2e4f9":"code","69b0b64f":"code","be383b10":"code","b0bf9e0a":"code","cbc25e83":"code","84d88ded":"code","3ccd9e43":"code","3b0ed061":"code","3bd7b551":"markdown","e7c25022":"markdown","ef859baa":"markdown","3c41d151":"markdown","e85a08c7":"markdown","165ed22b":"markdown","942906f8":"markdown","4a50cde9":"markdown","2ac79d39":"markdown","b3c7028d":"markdown","e747af88":"markdown","30a65edf":"markdown","67f60193":"markdown","f3c6fc98":"markdown","5d1fc117":"markdown","38021a6f":"markdown","f89a8b5d":"markdown","7c525b6f":"markdown","7c09e04a":"markdown","54310201":"markdown","baba493c":"markdown","9627782b":"markdown"},"source":{"a1785f9d":"#importing player stats for 2019\/2020 English Premier League season. \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\n\n\nepl_stats = pd.read_csv(\"..\/input\/epl-players-deep-stats-20192020\/epl_players_stats.csv\")\nepl_stats.head()\n","20dafe58":"# importing transfer data set for EPL 19\/20\n\nepl_transfer = pd.read_csv(\"..\/input\/epltransfer1920\/english_premier_league.csv\")\nepl_transfer.head()","09fcc928":"# Combine the datasets and remove NaN values (mostly from free transfers) \ncombine_epl = pd.merge(left=epl_transfer, right=epl_stats, left_on='player_name', right_on='name')\ncombine_epl.fillna(\"0\")","e2925f1f":"#explore the data and seeing averages for each column\n\ncombine_epl.mean()\n\n","1b9060f8":"#Returning max transfer value in Euros\n\ncombine_epl[\"fee_cleaned\"].max()","6bcce2e5":"#Correlation Heatmap says height and weight are correlated, as well as all the goalie stats\nsns.set(rc={'figure.figsize':(12,12)})\nsns.heatmap(combine_epl.corr(), cmap=\"PuRd\", square=True)","9f4f50b4":"#Cleaning dataset \n#Removing Accurate Crosses because of the correlation with Key Passes Made\n#Removing Height and Weight because they aren't as relevant as the other stats. It doesn't matter \n# How tall you are if you win every header.\n#Also removed a lot of Goalkeeper values\nfeature_cols = ['team_id', 'age_y', \"games\", \"minutes\", \"rating_m\", \n                \"goals\", \"assists\", \"yel_cards\", \"red_cards\", \"shots_m\", \"aerials_won_m\", \"motm\", \"successful_passes_pt\", \"key_passes_m\",\n                \"dribbles_won_m\", \"fouls_given_m\", \"offside_m\", \"dispossessed_m\", \"passes_m\", \"accurate_long_passes_m\",\n                \"accurate_through_passes_m\", \"tackles_m\", \"interceptions_m\", \"fouls_m\", \"clearances_m\", \"dribbled_past_m\",\n                \"gk_saves_m\"]\n\nX = combine_epl[feature_cols]\ny = combine_epl['fee_cleaned'].fillna(0)\n#Look at our beautiful data\nprint(X)\nprint(y)","e5ae1da1":"X.shape","23c28176":"y.shape","e318ebc3":"plt.figure(figsize=(20,20))\nplt.scatter(combine_epl[\"team\"], y, )","eeb3f3ef":"# Lets start preprocessing the data\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.formula.api as smf\nfrom sklearn.metrics import r2_score\n\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n\nEplReg = LinearRegression()\nEplReg.fit(X, y)\n\ny_pred = EplReg.predict(X_test)\n\n# print intercept and coefficients\nprint(EplReg.intercept_)\nprint(EplReg.coef_)\n\nprint(EplReg.predict(X_test))\n\n\n# # create a fitted model with all three features\n#to_square = smf.ols(formula='combine_epl ~ team_id + age_y', data=combine_epl).fit()\n\nrsquared = r2_score(y_test, y_pred)\nprint(f\"the R squared is { rsquared }\")","945ae0f7":"print(\"Predictions\")\nprint(EplReg.predict(X_test))\n\nprint(\"Actual targets\")\nprint(y_test.values)","54bfc2fb":"#Lets make some sense of this data \nplt.scatter(EplReg.predict(X_test), y_test.values )","28cb79a8":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\nmeansquared = mean_squared_error(y_test, y_pred)\n\n\nprint(f\"Our mean squared error is {meansquared}\")","ed5ae5f7":"#Emile Smith Rowe\nprint(X_test.iloc[0])\nprint(EplReg.predict(X_test.iloc[0].to_numpy().reshape(1,-1)))","485844db":"#Joshua King\nprint(X_test.iloc[4])\nprint(EplReg.predict(X_test.iloc[4].to_numpy().reshape(1,-1)))","cfcc9804":"from pprint import pprint as pp\n\nprint(X_test.iloc[33])\nprint(EplReg.predict(X_test.iloc[33].to_numpy().reshape(1,-1)))","a673fff4":"15# Performing K Means Clustering on Dataset\n#Scaling the data\n\nfrom sklearn import preprocessing\n\nepl_df = preprocessing.scale(X)\nepl_y = preprocessing.scale(y)\nepl_df\n","927500ea":"# Elbow Method to show optimal K\n#Doing elbow analysis to find the correct number of clusters.\nimport numpy as np\nfrom scipy.spatial import distance\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\ndistortions = []\nK = range(1, 10)\nfor k in K:\n    # fit the k-means for a given k to the data (X)\n    km = KMeans(n_clusters=k)\n    km.fit(X)\n    # distance.cdist finds the squared distances\n    # axis=1 allows us to keep the min for each sample, not just the min across the entire dataset\n    # find the closest distance for each sample to a center, and take the average\n    distortions.append(sum(np.min(distance.cdist(X, km.cluster_centers_, 'euclidean'), axis=1)) \/ X.shape[0])\n\n# Plot the elbow: bx- = use a solid (-) blue (b) line, \n# and mark the x-axis points with an x (x)\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()","64e5a8f8":"# Looks like Two clusters to me. This is surprising. \n\nkm = KMeans(n_clusters=2)\n\n# Fit the k-means object to the data\nkm.fit(epl_df)\ny_kmeans = km.predict(epl_df)\n\n\n# Visualize the data and the clustering\nplt.scatter(epl_df[:, 0], epl_df[:, 1], c=y_kmeans, s=50, cmap='viridis')\n\ncenters = km.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c='black', s=300, alpha=0.75)","9e6e0494":"#Doing some PCA \n#importing Libraries\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n#Fitting object with 2 because of results from Elbow Method\npca = PCA(2)\npca.fit(epl_df)\n\nX_with_pca = pca.transform(epl_df)\n\nprint(pca.explained_variance_)\nprint(pca.explained_variance_ratio_)\nprint(pca.explained_variance_ratio_.cumsum())","63bae18e":"# Looks good, lets plot it\n\nplt.scatter(X_with_pca[:, 0], X_with_pca[:, 1], c=y_kmeans, s=50, cmap='viridis')\n\ncenters = km.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c='black', s=300, alpha=0.75)","03c2e4f9":"\ncombine_epl[km.labels_==0]","69b0b64f":"combine_epl[km.labels_==1]","be383b10":"#First categorization of incoming transfers\ntransfer_in = combine_epl[km.labels_==0]\ntransfer_print = transfer_in[transfer_in[\"transfer_movement\"] == \"in\"]\ntransfer_print\n\n# transfer_in = combine_epl[combine_epl[\"transfer_movement\"] == \"in\"]\n# transfer_in[km.labels_==0]","b0bf9e0a":"#Second categorization of incoming players\ntransfer_in_two = combine_epl[km.labels_==1]\ntransfer_in_two = transfer_in_two[transfer_in_two[\"transfer_movement\"] == \"in\"]\ntransfer_in_two.iloc[:50]","cbc25e83":"transfer_out = combine_epl[km.labels_==0]\ntransfer_out = transfer_out[transfer_out[\"transfer_movement\"] == \"out\"]\ntransfer_out","84d88ded":"transfer_out_two = combine_epl[km.labels_==1]\ntransfer_out_two = transfer_out_two[transfer_out_two[\"transfer_movement\"] == \"out\"]\ntransfer_out_two","3ccd9e43":"print(transfer_in.mean())\nprint(transfer_in_two.mean())","3b0ed061":"print(transfer_out.mean())\nprint(transfer_out_two.mean())","3bd7b551":"Now lets make sure our data shapes are the same. The model will yell at us if they aren't. \n\nAlso I forgot to mention we have to be careful to replace NaN values for transfers. We replace those NaN's with zero because a NaN transfer is normally a free transfer which has no fee associated with it. ","e7c25022":"Just for fun, lets look at the average values of each of the columns. This will give us a reference point for players to see if they're above or below average anywhere. I won't be doing that type of analysis, but it will be interesting to match this up with the PCA categorization which happens later. ","ef859baa":"Next we make a heatmap. We can see from this heatmap that height and weight have a strong correlation, which is expected, as well as the goalie stats. We will drop most the goalie stats, both height and weight (thats a judgement call, I'm not convinced height gives any useful data if you have data on headers won), and we can also drop some of the passing columns. \n\nWe will keep key passes, and get rid of accurate crosses because an accurate cross would in most cases be considered a key pass. ","3c41d151":"So what we did was print out all the transfers for each cluster group, grouped them by in and out, and this is the result. \n\nTo me, it seems that above average and below average players have been clustered together. The first cluster has players which are traditionally seen as \"good players\" while the second cluster has players which would be considered below average. I'll take the group with Ainsley Maitland-Niles and Tomas Soucek anyday. \n\nSo with this we could try and see the values of the players, see what their cluster they're in, and we can then determine is a player is over valued or undervalued relative to their group clustering. Would be interesting, but out of scope for this notebook. I will leave that as an exercise for the reader :) ","e85a08c7":"Here we perform an Elbow Method Analysis to select the optimal number of clusters. This is important for implementing our K Means model. ","165ed22b":"Here's a Scatter plot. This scatter plot allows us to visualize the spread of data. We can see the amount of transfers conducted by each team, and the value of those transfers. It does not differentiate between transfers in and out at this point, but I would imagine these values are much lower than average because of the impact Covid had on tream finances. ","942906f8":"Now we're going to process the data and import it into our Machine Learning model.\n\nWe will be using a Linear Regression model and also calculating the r-squared to see how close our model gets. ","4a50cde9":"But who are in these clusters? That's the big question. It's a nice clear clustering into two categories, but can we make a guess as to what it's clustering by? ","2ac79d39":"Looks about right to me. We're printing out the explained varience, the ratio for explained variance, and the cumulutative sum. ","b3c7028d":"Here we grabbed some datasets we found in the internet. The Player transfer dataset I found in a random github that can be found here: https:\/\/github.com\/ewenme\/transfers It also has a lot of data for different leagues in different leagues, but we were limited by the availability of corresponding statistical data. \n\nThe statsitical data is grabbed from kaggle, and can be seen here: https:\/\/www.kaggle.com\/cashncarry\/epl-players-deep-stats-20192020\n\n\nNow we're going to combine these datasets on all the players which have had a transfer occur, whether in or out. ","e747af88":"This is a cell for Emile Smith Rowe. We can see the predicted value for ESR was 8.87 million Euros. This passes the smell test, as ESR wasn't a breakout player, but was still seen as a bright prospect. It wasn't until that season and the following season that he broke out to being a mainstay in the team. ","30a65edf":"The first thing we have to do is import our datasets.","67f60193":"#And for fun we print out the averages for the columns to test the suspicion that is grouped them by quality, and we can see the averages for the first group are much higher than the averages for the second group. Horray, we did it! \n\nTo contact me with questions or comments:\nTwitter: ReadyBrianRoss\n github: brianross93 \n","f3c6fc98":"Now we can make a set of predictions on the data. This is what that looks like in raw numerical form, but we can manually look at some players too and see what it says. But before we do, how about a scatter of our predicted values and actual values, see how close we are. ","5d1fc117":"This notebook conducts a Linear Regression Machine Learning analysis to predict English Premier League player values for the 2019\/2020 season. This season is an interesting season to look at as it was during the outbreak of Covid-19, and this had a dramatic affect on the transfer market. This notebook conducts and Linear Regression modeling as well as a K Means clustering of players with a Priciple Component Analysis\n\nIf you don't know what any of that means, that's totally cool. Basically this predicts player values and then groups them based on what the Machine Learning model thinks the players have in common. The results are fascinating. \n\nTo perform this analysis, we will be using the Pandas, Seaborn, Matplotlib, Numpy, and various models from SkLearn. \n\n","38021a6f":"Here, we see a grouping around 0. 0 is good, it means we're accurate. There are some outliers which means our model isn't perfect, but on average it gets pretty close. Check out the mean squared error. ","f89a8b5d":"Swag. They're the same shape. ","7c525b6f":"Lets do 1 more because I'm having fun. Let's predict a high profile player on a big team. I found Rodri from Manchester City in the dataset, lets predict him. \n\nWe can see his value for this season was about 11.1 million Euros. This number is very low. Manchester City bought him for 68 million dollars which is about 58 million Euros. Directionally, however, it is correct in assesing Rodri as higher than the other players. Interesting. ","7c09e04a":"# Now we're going to perform a K Means clustering on the dataset to see what clusters the Machine Learning can find. ","54310201":"The next prediction is for Joshue King, a striker at Bournemouth. Bournemouth ended up getting relegated, but Joshue King was still seen as a decent player. Joshua King ended up leaving to Everton for a fee of 5 million, which is close to our prediction. The reason for the lower value would be that Bournemouth got relegated, and thus all player values decrease as the team has little leverage to keep the players. \n","baba493c":"The Elbow Method has spoken. Looks like it sees two clusters as that is the most dramatic uptick. We'll use two clusters for our K Means Clustering. This clustering uses the K Means algorithm to create classifications. ","9627782b":"## This map doesn't make too much sense, but since we have two components, I wonder if we can do a Principle Component Analysis on it and see if that gives us a clearer picture. \n\n---\n\n"}}