{"cell_type":{"4d992810":"code","1fc72310":"code","42acd550":"code","f2a9c5b2":"code","3d196f1d":"code","7031d379":"code","0a8abcdb":"code","cc2db5f2":"code","2de9fa84":"code","812f9a78":"code","c05b1287":"code","036a5921":"code","24eeea92":"code","f6ab22dc":"code","46e85ed9":"code","52cc126c":"code","786dfbba":"code","127ba343":"code","54df2d96":"code","b2ebd208":"code","f7d83a43":"code","58e513fe":"code","59ddb5fc":"markdown","3c32cae2":"markdown","2a2b918a":"markdown","fe2f522e":"markdown"},"source":{"4d992810":"import pandas as pd\nimport numpy as np\nimport requests\n!pip install feedparser\nimport feedparser\nfrom bs4 import BeautifulSoup\nfrom textblob import TextBlob\nimport re, string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\n","1fc72310":"# update textblob\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('stopwords')\nnltk.download('wordnet')","42acd550":"rssurl = 'https:\/\/forums.macrumors.com\/threads\/kuo-apple-to-accelerate-adoption-of-mini-led-displays-in-ipad-and-mac-notebook-lineups.2255816\/'","f2a9c5b2":"article = requests.get(rssurl)\narticles = BeautifulSoup(article.content, 'html.parser')\nprint('Subject: ', articles.title.string)\nprint('Url: ', rssurl)","3d196f1d":"rows = []\nfor pc in articles.find_all(\"div\", {\"class\": \"bbWrapper\"})[1:]:\n  row=''\n  for txt in pc.strings:\n    row += txt\n  rows.append(row)","7031d379":"# setup cores\ncolors = ['PowderBlue', 'Tomato', 'MidnightBlue', 'Goldenrod', 'MediumOrchid',\n          'Salmon', 'Lime', 'PapayaWhip', 'DeepSkyBlue', 'LightPink']\n\n# setup estrutura de graficos\n# pie\nexplode = (0.01, 0.01)\nlabels = ['Positive', 'Negative']\n\npos = 0\nneg = 0\nfor r in rows:\n  txt = TextBlob(r)\n  polarity = txt.sentiment.polarity\n  if polarity != 0:\n    if polarity > 0:\n        pos += 1\n    else:\n        neg += 1","0a8abcdb":"plt.figure(figsize=(15,8))\nplt.pie([pos, neg], labels=labels, colors=colors, startangle=90, explode = explode, autopct = '%1.2f%%')\nplt.axis('equal') \nplt.title(articles.title.string)\nplt.show()","cc2db5f2":"\ndf = pd.DataFrame()\ndf['comments'] = rows\ndf.head()","2de9fa84":"swords = stopwords.words('english')\ndef cleaningText(txt):\n  txt = txt.lower() # lowercase\n  txt = re.sub('@','',txt) # remove @ \n  txt = re.sub('\\[.*\\]','',txt) # remove contents between brackets\n  txt = re.sub('<.*?>+','',txt) # remove contents between less and more signs\n  txt = re.sub('https?:\/\/\\S+|www\\.\\S+', '', txt) # remove URLs\n  txt = re.sub(re.escape(string.punctuation), '', txt) # remove punctuation\n  txt = re.sub(r'[^a-zA-Z ]+', '', txt) # remove numbers\n  txt = re.sub('\\n', '', txt) # remove line break\n  txt = nltk.word_tokenize(txt) # creating a word's list\n  txt = [word for word in txt if word not in swords] \n\n  return txt","812f9a78":"test = 'Today our Economy Professionals talking about \"Brazilian Economy Estrategy\". See the video here https:\/\/www.1.com'\ntest = cleaningText(test)\ntest","c05b1287":"df.comments = df.comments.apply(lambda c: cleaningText(c))\ndf.head()","036a5921":"def lemmatization(txt):\n    txt = [WordNetLemmatizer().lemmatize(i) for i in txt]\n    txt = [WordNetLemmatizer().lemmatize(i, 'v') for i in txt]\n    return txt\n  \n","24eeea92":"test2 = lemmatization(test)\ntest2","f6ab22dc":"df.comments = df.comments.apply(lambda l: lemmatization(l))\ndf.head()","46e85ed9":"# creating list with all words frm dataframe\nwordsDF = []\nfor listdf in df.comments.to_list():\n  for w in listdf:\n    wordsDF.append(w)","52cc126c":"fig, ax1 = plt.subplots(sharey=True, figsize=(15,9))\nsns.barplot(x=pd.Series(wordsDF).value_counts()[:20].index, \n            y=pd.Series(wordsDF).value_counts()[:20].values,\n            ax=ax1).set_title('Top 20')\nplt.xlabel('word')\nplt.ylabel('count')\nplt.xticks(rotation=80)","786dfbba":"bwords = []\nfor i in nltk.bigrams(wordsDF):\n    bwords.append(i)","127ba343":"bigrams = []\n\nfor b in bwords:\n  bigrams.append(b)\nprint('Mean Bigrams: ', pd.Series(bigrams).value_counts().mean())\n","54df2d96":"fig, ax1 = plt.subplots(sharey=True, figsize=(15,9))\nsns.barplot(x=pd.Series(bwords).value_counts()[:20].index, \n            y=pd.Series(bwords).value_counts()[:20].values,\n            ax=ax1).set_title('Top 20 bigrams')\nplt.xlabel('brigram')\nplt.ylabel('count')\nplt.xticks(rotation=80)","b2ebd208":"pos = 0\nneg = 0\nfor r in range(0, len(df)):\n  txt = TextBlob(' '.join(df.comments[r]))\n  polarity = txt.sentiment.polarity\n  if polarity != 0:\n    if polarity > 0:\n        pos += 1\n    else:\n        neg += 1","f7d83a43":"fig, ax1 = plt.subplots(sharey=True, figsize=(10,5))\nsns.barplot(x=labels, palette=colors,\n            y=[pos, neg],\n            ax=ax1).set_title('Polarity '+articles.title.string)\nplt.xlabel('Status')\nplt.ylabel('Qty')\nplt.xticks(rotation=80)","58e513fe":"wordcloud = WordCloud(width = 1400, height = 800, \n                background_color ='white', \n                min_font_size = 10).generate(' '.join(wordsDF))\n\n# plot the WordCloud image                       \nplt.figure(figsize = (15, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) ","59ddb5fc":"Testing function","3c32cae2":"**by Italo Costa**","2a2b918a":"# **Cleaning text**\n\nNow, we will be cleaning the data. For this, we'll use pandas dataframe and nltk's features","fe2f522e":"# **Exploratory Data Analysis**"}}