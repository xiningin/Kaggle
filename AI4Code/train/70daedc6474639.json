{"cell_type":{"d6ee9e93":"code","6b077509":"code","1c81c928":"code","bc79b038":"code","14c06e31":"code","85e049cd":"code","997b93ea":"code","3bafa560":"code","6a36ddc2":"code","fb16873d":"code","14390ec9":"code","fe546c4b":"code","ba232925":"code","3969e10d":"markdown","2f55f10a":"markdown","3ae2490f":"markdown"},"source":{"d6ee9e93":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n#!pip install numba\n#from numba import jit\n\n# Any results you write to the current directory are saved as output.","6b077509":"import pandas as pd\nimport numpy as np\n\n\nnp.random.seed(42)\ndata = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\nprint(data.shape)\ndata = data.sample(frac=1)\nprint(data[['label']].groupby('label').size().reset_index())\n\none_hot = pd.get_dummies(data['label'].unique())\none_hot['label'] = one_hot.index\n\ndata = pd.merge(data,one_hot)\n#data = data.drop('label',axis=1)\ndata = data.sample(frac=1)\n\ndata_train = data\ndata_test = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')\ndata_test = pd.merge(data_test,one_hot)\ndata_train.drop('label',axis=1,inplace=True)\n\ndata_test.drop('label',axis=1,inplace=True)\n\n## Create the train and test set\nX_train = np.array(data_train.drop([0,1,2,3,4,5,6,7,8,9],axis=1).values)\/255\ny_train = np.array(data_train[[0,1,2,3,4,5,6,7,8,9]].values)\nX_test = np.array(data_test.drop([0,1,2,3,4,5,6,7,8,9],axis=1).values)\/255\ny_test = np.array(data_test[[0,1,2,3,4,5,6,7,8,9]].values)","1c81c928":"\n\none_hot","bc79b038":"X_train = X_train.T\ny_train = y_train.T\nprint(X_train.shape)\nprint(y_train.shape)\nX_test = X_test.T\ny_test = y_test.T\n\ndef sigmoid(x):\n    return(1.\/(1+np.exp(-x)))\n\ndef softmax(x): \n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\" \n\n    e_x = np.exp(x - np.max(x)) \n\n    return (e_x \/ e_x.sum(axis=0)) \n\nimport random\nrandom.seed(42)\nw1 = np.random.rand(128,784)\/np.sqrt(784)\nb0 = np.zeros((128,1))\/np.sqrt(784)\nw2 = np.random.rand(10,128)\/np.sqrt(128)\nb1 = np.zeros((10,1))\/np.sqrt(128)\nloss=[]\nbatches = 1000\n\nlr = 0.1\nbatch_size = 200\nbeta = 0.9\ncount = 0\nepochs = 500\n    \n    \n    \n    \n    \n\n","14c06e31":"\nloss_weight_dict = {\n    \n}\n### Forward Pass\nfor i in range(epochs):\n#     if i%100==0:\n#         print('Epoch :',i)\n    permutation = np.random.permutation(X_train.shape[1])\n    X_train_shuffled = X_train[:, permutation]\n    Y_train_shuffled = y_train[:, permutation]\n    \n    for j in range(batches):\n        \n        begin = j * batch_size\n        end = min(begin + batch_size, X_train.shape[1] - 1)\n        if begin>end:\n            continue\n        X = X_train_shuffled[:, begin:end]\n        Y = Y_train_shuffled[:, begin:end]\n        m_batch = end - begin\n        x1 = sigmoid(w1@X+b0)\n        x2 = softmax(w2@x1+b1)\n        delta_2 = (x2-Y)\n        delta_1 = np.multiply(w2.T@delta_2, np.multiply(x1,1-x1))\n        if i==0 :\n            dW1 = delta_1@X.T\n            dW2 = delta_2@x1.T\n            db0 = np.sum(delta_1,axis=1,keepdims=True)\n            db1 = np.sum(delta_2,axis=1,keepdims=True)\n        else:\n            dW1_old = dW1\n            dW2_old = dW2\n            db0_old = db0\n            db1_old = db1\n            dW1 = delta_1@X.T\n            dW2 = delta_2@x1.T\n            db0 = np.sum(delta_1,axis=1,keepdims=True)\n            db1 = np.sum(delta_2,axis=1,keepdims=True)\n            ## Using the past gradients to calculate the present gradients\n            dW1 = (beta * dW1_old + (1. - beta) * dW1)\n            db0 = (beta * db0_old + (1. - beta) * db0)\n            dW2 = (beta * dW2_old + (1. - beta) * dW2)\n            db1 = (beta * db1_old + (1. - beta) * db1)\n\n\n        w1 = w1 - (1.\/m_batch)*(dW1)*lr\n        b0 = b0 - (1.\/m_batch)*(db0)*(lr)\n        w2 = w2 - (1.\/m_batch)*(dW2)*lr\n        b1 = b1 - (1.\/m_batch)*(db1)*(lr)\n    \n    x1 = sigmoid(w1@X_train+b0)\n    x2_train = softmax(w2@x1+b1)\n    x2_train_df = pd.DataFrame(x2_train)\n    x2_train_df = (x2_train_df == x2_train_df.max()).astype(int)\n    x2_train_df = x2_train_df.transpose()\n    x2_train_df = pd.merge(x2_train_df,one_hot)\n    x2_train_df = x2_train_df[['label']]\n    y_train_df = pd.merge(pd.DataFrame(y_train.T),one_hot)\n    x2_train_df['label_actual'] = y_train_df['label']\n    train_accuracy = np.sum(x2_train_df['label_actual']==x2_train_df['label'])\/x2_train_df.shape[0]\n\n    \n#     print('Training Loss...')\n#     print(-np.mean(np.multiply(y_train,np.log(x2))))\n    add_loss = {\n        'loss' : -np.mean(np.multiply(y_train,np.log(x2_train))),\n        'weight_1' : w1,\n        'weight_2':w2,\n        'b0' : b0,\n        'b1': b1,\n        'train_accuracy': train_accuracy\n    }\n    \n    \n    \n    \n    \n    x1 = sigmoid(w1@X_test+b0)\n    x2_test = softmax(w2@x1+b1)\n    x2_test_df = pd.DataFrame(x2_test)\n    x2_test_df = (x2_test_df == x2_test_df.max()).astype(int)\n    x2_test_df = x2_test_df.transpose()\n    x2_test_df = pd.merge(x2_test_df,one_hot)\n    x2_test_df = x2_test_df[['label']]\n    y_test_df = pd.merge(pd.DataFrame(y_test.T),one_hot)\n    x2_test_df['label_actual'] = y_test_df['label']\n    test_accuracy = np.sum(x2_test_df['label_actual']==x2_test_df['label'])\/x2_test_df.shape[0]\n    print('Epoch: ',i)\n\n    print('Testing Accuracy :',test_accuracy)\n    print('Training Accuracy :',train_accuracy)\n    print('----------------------------------------')\n    \n    \n    \n#     print('Testing Loss...')\n#     print(-np.mean(np.multiply(y_test,np.log(x2))))\n    \n    add_loss['testing_loss'] = -np.mean(np.multiply(y_test,np.log(x2_test)))\n    add_loss['test_accuracy'] = test_accuracy\n    loss_weight_dict[count] = add_loss\n    count = count + 1\n","85e049cd":"train_accuracy = []\n\nfor i in range(len(loss_weight_dict)):\n    train_accuracy.append(loss_weight_dict[i]['train_accuracy'])\nimport matplotlib.pyplot as plt\nplt.plot(train_accuracy)\nplt.xlabel('Epochs')\nplt.ylabel('Training Accuracy')\nplt.show()","997b93ea":"test_accuracy = []\n\nfor i in range(len(loss_weight_dict)):\n    test_accuracy.append(loss_weight_dict[i]['test_accuracy'])\nimport matplotlib.pyplot as plt\nplt.plot(test_accuracy)\nplt.xlabel('Epochs')\nplt.ylabel('Testing Accuracy')\nplt.show()\n","3bafa560":"### Getting the weight matrices at index where test accuracy is the largest\n\n\nindex_max = test_accuracy.index(max(test_accuracy))\nweight_1 = loss_weight_dict[index_max]['weight_1']\nweight_2 = loss_weight_dict[index_max]['weight_2']\nb0 = loss_weight_dict[index_max]['b0']\nb1 = loss_weight_dict[index_max]['b1']\n","6a36ddc2":"# # Getting Test Data\ntest_data = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')\n","fb16873d":"test_data_mod = pd.merge(test_data,one_hot)\ntest_data_algo = test_data_mod.drop(['label',0,1,2,3,4,5,6,7,8,9],axis=1)","14390ec9":"test_vector = np.array(test_data_algo.values)\ntest_vector = test_vector.T\ntest_vector = test_vector\/255\nx1 = sigmoid(weight_1@test_vector+b0)\nx2 = softmax(weight_2@x1+b1)\nx2_df = pd.DataFrame(x2)\nx2_df = (x2_df == x2_df.max()).astype(int)\nx2_df = x2_df.transpose()\nx2_df = pd.merge(x2_df,one_hot)\nx2_df['label_actual'] = test_data_mod['label']","fe546c4b":"print('Test Accuracy :',np.sum(x2_df['label_actual']==x2_df['label'])\/x2_df.shape[0])","ba232925":"# loss_weight_dict","3969e10d":"## Test Accuracy\n","2f55f10a":"## Training Accuracy","3ae2490f":"## Test Accuracy"}}