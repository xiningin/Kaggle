{"cell_type":{"3ac757d1":"code","42ca6232":"code","44757cf9":"code","089b7da4":"code","be1004e0":"code","ca1a86e5":"code","0cf31058":"code","a6229633":"code","5cbe69d2":"code","531feb87":"code","e31b4e38":"code","85a15458":"code","ee971fbb":"markdown","d34f1755":"markdown","cdfd93f7":"markdown","f9afc049":"markdown","785ef396":"markdown","f5708d73":"markdown","fc9bc4db":"markdown","58d2be0e":"markdown","a0ee7052":"markdown","62d3e372":"markdown"},"source":{"3ac757d1":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport cv2\nimport os\nimport subprocess\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom pathlib import Path\nfrom collections import Counter\n\n\n#import zipfile\n#with zipfile.ZipFile('..input\/data\/dataset\/multi_digit_images_10k.zip') 'r') as zip_ref:\n#zip_ref.extractall('..\/input\/data\/dataset\/multi_digit_images_10k\/')\n#I just want to use the 10k data for my project\n","42ca6232":"##############Preprocessing and preparing the images for training\n# Path to the data directory\ndata_dir = Path(\"..\/input\/dataset\/multi_digit_images_40k\/\")\n\n# Get list of all the images\nimages = sorted(list(map(str, list(data_dir.glob(\"*.png\")))))\n# image ch\u1ee9a \u0111\u01b0\u1eddng d\u1eabn c\u1ee7a image\nlabels = [img.split(os.path.sep)[-1].split(\".png\")[0] for img in images]\ncharacters = set(char for label in labels for char in label)\n\nprint(\"Number of images found: \", len(images))\nprint(\"Number of labels found: \", len(labels))\nprint(\"Number of unique characters: \", len(characters))\nprint(\"Characters present: \", characters)\n\n# Batch size for training and validation\nbatch_size = 16\n\n# Desired image dimensions\nimg_width = 128\nimg_height = 32\n\n# Factor by which the image is going to be downsampled\n# by the convolutional blocks. We will be using two\n# convolution blocks and each block will have\n# a pooling layer which downsample the features by a factor of 2.\n# Hence total downsampling factor would be 4.\ndownsample_factor = 4\n\n# Maximum length of any captcha in the dataset\nmax_length = max([len(label) for label in labels])","44757cf9":"def preprocess(img, imgSize ):\n    ''' resize, transpose and standardization grayscale images '''\n    # create target image and copy sample image into it\n    widthTarget, heightTarget = imgSize \n    height, width = img.shape \n    factor_x = width \/ widthTarget\n    factor_y = height \/ heightTarget\n\n    factor = max(factor_x, factor_y)\n    # scale according to factor\n    newSize = (min(widthTarget, int(width \/ factor)), min(heightTarget, int(height \/ factor)))\n\n    img = cv2.resize(img, newSize)\n    target = np.ones(shape=(heightTarget, widthTarget), dtype='uint8') * 255\n    target[0:newSize[1], 0:newSize[0]] = img\n    # transpose\n    img = cv2.transpose(target)\n    # standardization\n    mean, stddev = cv2.meanStdDev(img)\n    mean = mean[0][0]\n    stddev = stddev[0][0]\n    img = img - mean\n    img = img \/\/ stddev if stddev > 0 else img\n    return img","089b7da4":"train_size = int(0.8 * len(labels))\nvalid_size= int(len(labels) - train_size)\n# 80% data is for training. The rest of them is for validation\n\nprint ('\\ntrain_size',train_size,'  valid_size',valid_size)","be1004e0":"train_x = []\nvalid_x = []\ni=0\nfor image in images:\n    image = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n    image = preprocess(image, (128,32)) \n    image = image\/255.\n    if i < train_size:\n        train_x.append(image)\n    else:\n        valid_x.append(image)\n    i = i+1\n\ntrain_x = np.array(train_x).reshape(-1, 128, 32, 1)\nvalid_x = np.array(valid_x).reshape(-1, 128, 32, 1)\n\nprint ('\\n train_x.shape',train_x.shape)\nprint ('\\n valid_x.shape',valid_x.shape)\n\nlabel_train = labels[0:train_size]\nlabel_valid = labels[train_size:len(labels)]\n# They usually slit images into 2 folders, but I just have 1 so I have to label it again.\n\n#print ('\\n label_train',label_train)\nprint('\\n Example of label_valid',label_valid[3])\n\nplt.figure(num='multi digit',figsize=(9,18))\nfor i in range(3):\n    plt.subplot(3,3,i+1) \n    plt.title(label_valid[i])\n    plt.imshow(np.squeeze(valid_x[i,:,:,]))\nplt.show()\n","ca1a86e5":"alphabets = u\"0123456789' \"\nmax_str_len = 10 # max length of input labels\n# My project have 7 digits per image, but as long as the max_str_len > the number of digit per image, \n#it's just fine\nnum_of_characters = len(alphabets) + 1 # +1 for ctc pseudo blank\nnum_of_timestamps = 32 # max length of predicted labels\n# I find out that if the num_of_timestamps ... I forgot it... \n\ndef label_to_num(label):\n    label_num = []\n    for ch in label:\n        label_num.append(alphabets.find(ch))\n        \n    return np.array(label_num)\n\ndef num_to_label(num):\n    ret = \"\"\n    for ch in num:\n        if ch == -1:  # CTC Blank\n            break\n        else:\n            ret+=alphabets[ch]\n    return ret\n\nname = '39816931'\nprint(name, '\\n',label_to_num(name))","0cf31058":"train_y = np.ones([train_size, max_str_len]) * -1\ntrain_label_len = np.zeros([train_size, 1])\ntrain_input_len = np.ones([train_size, 1]) * (num_of_timestamps-2)\ntrain_output = np.zeros([train_size])\n\nfor i in range(train_size):\n    train_label_len[i] = len(label_train[i])\n    train_y[i, 0:len(label_train[i])]= label_to_num(label_train[i])  \n    \n\nvalid_y = np.ones([valid_size, max_str_len]) * -1\nvalid_label_len = np.zeros([valid_size, 1])\nvalid_input_len = np.ones([valid_size, 1]) * (num_of_timestamps-2)\nvalid_output = np.zeros([valid_size])\n\nfor i in range(valid_size):\n    valid_label_len[i] = len(label_valid[i])\n    valid_y[i, 0:len(label_valid[i])]= label_to_num(label_valid[i])   \n\nprint('\\n True label_train  : ',label_train[10] , '\\ntrain_y : ',train_y[10],\n      '\\ntrain_label_len : ',train_label_len[10], '\\ntrain_input_len : ', train_input_len[10])\n\nprint('\\n True label_valid : ',label_valid[10] , '\\ntrain_y : ',valid_y[10],\n      '\\ntrain_label_len : ',valid_label_len[10], '\\ntrain_input_len : ', valid_input_len[10])\n","a6229633":"#y_true: tensor (samples, max_string_length) containing the truth labels.\n#y_pred: tensor (samples, time_steps, num_categories) containing the prediction, or output of the softmax.\n#input_length: tensor (samples, 1) containing the sequence length of slices coming out from RNN for each batch item in y_pred.\n#label_length: tensor (samples, 1) containing the sequence length of label for each batch item in y_true.\n\n\ndef build_model(img_width = 128,img_height = 32, max_str_len = 10):\n    # Inputs to the model\n\n    input_img = layers.Input(\n        shape=(img_width, img_height, 1), name=\"image\", dtype=\"float32\"\n    )\n    \n\n    # First conv block\n    x = layers.Conv2D(\n        32,\n        (3, 3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv1\",\n    )(input_img)\n    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n\n    # Second conv block\n    x = layers.Conv2D(\n        64,\n        (3, 3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv2\",\n    )(x)\n    x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n\n    # We have used two max pool with pool size and strides 2.\n    # Hence, downsampled feature maps are 4x smaller. The number of\n    # filters in the last layer is 64. Reshape accordingly before\n    # passing the output to the RNN part of the model\n    new_shape = ((img_width \/\/ 4), (img_height \/\/ 4) * 64)\n    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n    x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n    x = layers.Dropout(0.2)(x)\n\n    # RNNs\n    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n\n    # Output layer\n    y_pred = layers.Dense(10 + 1, activation=\"softmax\", name=\"dense2\")(x) # y pred\n    model = keras.models.Model(inputs=input_img, outputs=y_pred, name=\"functional_1\")\n\n    def ctc_lambda_func(args):\n        y_pred, labels, input_length, label_length = args\n        # the 2 is critical here since the first couple outputs of the RNN\n        # tend to be garbage\n        y_pred = y_pred[:, 2:, :]\n        return tf.keras.backend.ctc_batch_cost(labels, y_pred, input_length, label_length)\n\n    labels = layers.Input(name='gtruth_labels', shape=[max_str_len], dtype='float32')\n    input_length = layers.Input(name='input_length', shape=[1], dtype='int64')\n    label_length = layers.Input(name='label_length', shape=[1], dtype='int64')\n\n    ctc_loss = keras.layers.Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n    model_final = keras.models.Model(inputs=[input_img, labels, input_length, label_length], outputs=ctc_loss, name = \"ocr_model_v1\")\n    \n    return model, model_final\n\nmodel, model_final = build_model()\nmodel.summary()\nmodel_final.summary()","5cbe69d2":"opt = keras.optimizers.Adam()\n\nearly_stopping_patience = 5\n# Add early stopping\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n)\n\nmodel_final.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=keras.optimizers.Adam(lr = 0.0001))\n\nmodel_final.fit(x=[train_x, train_y, train_input_len, train_label_len], y=train_output, \n                validation_data=([valid_x, valid_y, valid_input_len, valid_label_len], valid_output),\n                epochs=60, \n                batch_size=128)\n","531feb87":"model.save('\/kaggle\/working\/mymodel')","e31b4e38":"preds = model.predict(valid_x)\n#print('\\n preds',preds)\ndecoded = tf.keras.backend.get_value(tf.keras.backend.ctc_decode(preds, input_length=np.ones(preds.shape[0])*preds.shape[1], \n                                   greedy=True)[0][0])\nprint ('\\n decoded',decoded)\nprediction = []\nfor i in range(valid_size):\n    prediction.append(num_to_label(decoded[i]))\n    \nprint ('\\n predict',num_to_label(decoded[0]))\ny_true = label_valid\ncorrect_char = 0\ntotal_char = 0\ncorrect = 0\n\nfor i in range(valid_size):\n    pr = prediction[i]\n    tr = y_true[i]\n    total_char += len(tr)\n    \n    for j in range(min(len(tr), len(pr))):\n        if tr[j] == pr[j]:\n            correct_char += 1\n            \n    if pr == tr :\n        correct += 1 \n    \nprint('Correct characters predicted : %.2f%%' %(correct_char*100\/total_char))\nprint('Correct words predicted      : %.2f%%' %(correct*100\/valid_size))\n\n","85a15458":"import random\nplt.figure(figsize=(15, 10))\nfor i in range(6):\n    ax = plt.subplot(2, 3, i+1)\n    fold_dir = '..\/input\/dataset\/multi_digit_images_10k\/multi_digit_images\/'\n    filename = random.sample((os.listdir(fold_dir)),1)\n    filename = ( \"\".join( str(e) for e in filename ) ) # b\u1ecf ngo\u1eb7c\n    print ('\\n filename',filename)\n    img_dir = fold_dir+str(filename) \n    image = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n    plt.imshow(image, cmap='gray')\n    image = preprocess(image,(128,32))\n    image = image\/255.\n    pred = model.predict(image.reshape(1, 128, 32, 1))\n    decoded = tf.keras.backend.get_value(tf.keras.backend.ctc_decode(pred, input_length=np.ones(pred.shape[0])*pred.shape[1], \n                                       greedy=True)[0][0])\n\n    plt.title(num_to_label(decoded[0]), fontsize=12)\n    plt.axis('off')\n    \n","ee971fbb":"You may ask why we need 2 models. The final with CTC is for training.\nThe functional is for saving and predicting.\nThe detail is in the comment here\nhttps:\/\/www.kaggle.com\/samfc10\/handwriting-recognition-using-crnn-in-keras\/comments","d34f1755":"# Build the model\nMy funtional model is quite different from the original above because it's here:\nhttps:\/\/keras.io\/examples\/vision\/captcha_ocr\/\nBut the final model is the same\nyeah I had some trouble with the CTC inputs and outputs of catpcha","cdfd93f7":"* train_y contains the true labels converted to numbers and padded with -1. The length of each label is equal to max_str_len.\n* train_label_len contains the length of each true label (without padding)\n* train_input_len contains the length of each predicted label. The length of all the predicted labels is constant i.e number of timestamps - 2.\n* train_output is a dummy output for ctc loss.","f9afc049":"# Predict on some test image\nI wil load some image and test","785ef396":"# Training\n","f5708d73":"# Preparing the labels for CTC Loss\nLearn more about CTC loss and why its amazing for text recognition from here.\n\nThe labels have to be converted to numbers which represent each character in the training set. The 'alphabets' consist of A-Z and three special characters (- ' and space).","fc9bc4db":"#  Preprocessing and preparing the images for training\nThe images are loaded as grayscale and resized to width 128 and height 32. (orginal shape is 196x28)\nTranspose the image and standardization\nThe image is then normalized to range [0, 1]\nReference: https:\/\/www.kaggle.com\/samfc10\/handwriting-recognition-using-crnn-in-keras","58d2be0e":"# Some prediction on valid set","a0ee7052":"I am not really good at English so....\nSr for not having time to shorten the code, you may find some silly things in this code.\n\nYou can find the code i use to generate picture of multi digit from MNSI dataset here:\nhttps:\/\/www.kaggle.com\/bomaich\/multi-digit-images-generate-mnist\nThe following is the amount of pictures and some variables needed for the code to run (some are not needed :)) )","62d3e372":"<a href=\"mymodel\"> Download File <\/a>"}}