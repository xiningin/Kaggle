{"cell_type":{"9787c13c":"code","35fd87d6":"code","61fbbc96":"code","003d3452":"code","653767cf":"code","d1a1c514":"code","1af49613":"code","4b5c043e":"code","a332467f":"code","30510e31":"code","9b65e43d":"code","f1d2f4c2":"code","4eb4dfea":"code","3c42f911":"code","b3c6b8e0":"code","edfd6b8a":"code","6c61cd31":"code","97978209":"code","c9b0dc51":"code","4f65e14c":"code","3990018c":"code","8cdaf6be":"code","07d88eab":"code","c70bfa5b":"code","842a71f3":"code","df820ec5":"code","3d1e9103":"code","6f84f3cb":"code","5cce6117":"code","bda2fe8f":"code","cfb64579":"code","3929e698":"code","2e351fd9":"code","e356cb41":"code","dd3daf61":"markdown","d91310d0":"markdown","79726773":"markdown","bbd12ff4":"markdown","04b93655":"markdown","3a051bfa":"markdown","2725bddf":"markdown","a3e229d0":"markdown","e9a88709":"markdown","e4db0a0c":"markdown","a4e60a04":"markdown","88e13fbd":"markdown","f471c9d7":"markdown","6f4e6df2":"markdown","639251c6":"markdown","53cb2dde":"markdown"},"source":{"9787c13c":"from IPython.display import clear_output\n!pip install imutils\nclear_output()\n\nfrom keras.applications.vgg19 import VGG19,preprocess_input\nfrom keras.applications.xception import Xception,preprocess_input\nfrom keras.applications.inception_v3 import InceptionV3,inception_v3","35fd87d6":"import numpy as np \nfrom tqdm import tqdm\nimport cv2\nimport os\nimport shutil\nimport itertools\nimport imutils\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import tools\n\nfrom keras.preprocessing.image import ImageDataGenerator\n#from keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras.applications import MobileNet\nfrom keras.applications.mobilenet import preprocess_input\n\nfrom keras import layers\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.callbacks import EarlyStopping\n\ninit_notebook_mode(connected=True)\nRANDOM_SEED = 123","61fbbc96":"!apt-get install tree\nclear_output()\n# create new folders\n!mkdir TRAIN TEST VAL TRAIN\/YES TRAIN\/NO TEST\/YES TEST\/NO VAL\/YES VAL\/NO\n!tree -d","003d3452":"IMG_PATH = '..\/input\/brain-mri-images-for-brain-tumor-detection\/brain_tumor_dataset\/'\n# split the data by train\/val\/test\nfor CLASS in os.listdir(IMG_PATH):\n    if not CLASS.startswith('.'):\n        IMG_NUM = len(os.listdir(IMG_PATH + CLASS))\n        for (n, FILE_NAME) in enumerate(os.listdir(IMG_PATH + CLASS)):\n            img = IMG_PATH + CLASS + '\/' + FILE_NAME\n            if n < 5:\n                shutil.copy(img, 'TEST\/' + CLASS.upper() + '\/' + FILE_NAME)\n            elif n < 0.8*IMG_NUM:\n                shutil.copy(img, 'TRAIN\/'+ CLASS.upper() + '\/' + FILE_NAME)\n            else:\n                shutil.copy(img, 'VAL\/'+ CLASS.upper() + '\/' + FILE_NAME)","653767cf":"def load_data(dir_path, img_size=(100,100)):\n    \"\"\"\n    Load resized images as np.arrays to workspace\n    \"\"\"\n    X = []\n    y = []\n    i = 0\n    labels = dict()\n    for path in tqdm(sorted(os.listdir(dir_path))):\n        if not path.startswith('.'):\n            labels[i] = path\n            for file in os.listdir(dir_path + path):\n                if not file.startswith('.'):\n                    img = cv2.imread(dir_path + path + '\/' + file)\n                    X.append(img)\n                    y.append(i)\n            i += 1\n    X = np.array(X)\n    y = np.array(y)\n    print(f'{len(X)} images loaded from {dir_path} directory.')\n    return X, y, labels\n\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize = (6,6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    cm = np.round(cm,2)\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","d1a1c514":"TRAIN_DIR = 'TRAIN\/'\nTEST_DIR = 'TEST\/'\nVAL_DIR = 'VAL\/'\nIMG_SIZE = (224,224)\n\n# use predefined function to load the image data into workspace\nX_train, y_train, labels = load_data(TRAIN_DIR, IMG_SIZE)\nX_test, y_test, _ = load_data(TEST_DIR, IMG_SIZE)\nX_val, y_val, _ = load_data(VAL_DIR, IMG_SIZE)","1af49613":"y = dict()\ny[0] = []\ny[1] = []\nfor set_name in (y_train, y_val, y_test):\n    y[0].append(np.sum(set_name == 0))\n    y[1].append(np.sum(set_name == 1))\n\ntrace0 = go.Bar(\n    x=['Train Set', 'Validation Set', 'Test Set'],\n    y=y[0],\n    name='No',\n    marker=dict(color='#33cc33'),\n    opacity=0.7\n)\ntrace1 = go.Bar(\n    x=['Train Set', 'Validation Set', 'Test Set'],\n    y=y[1],\n    name='Yes',\n    marker=dict(color='#ff3300'),\n    opacity=0.7\n)\ndata = [trace0, trace1]\nlayout = go.Layout(\n    title='Count of classes in each set',\n    xaxis={'title': 'Set'},\n    yaxis={'title': 'Count'}\n)\nfig = go.Figure(data, layout)\niplot(fig)","4b5c043e":"def plot_samples(X, y, labels_dict, n=50):\n    \"\"\"\n    Creates a gridplot for desired number of images (n) from the specified set\n    \"\"\"\n    for index in range(len(labels_dict)):\n        imgs = X[np.argwhere(y == index)][:n]\n        j = 10\n        i = int(n\/j)\n\n        plt.figure(figsize=(15,6))\n        c = 1\n        for img in imgs:\n            plt.subplot(i,j,c)\n            plt.imshow(img[0])\n\n            plt.xticks([])\n            plt.yticks([])\n            c += 1\n        plt.suptitle('Tumor: {}'.format(labels_dict[index]))\n        plt.show()","a332467f":"plot_samples(X_train, y_train, labels, 10)","30510e31":"RATIO_LIST = []\nfor set in (X_train, X_test, X_val):\n    for img in set:\n        RATIO_LIST.append(img.shape[1]\/img.shape[0])\n        \nplt.hist(RATIO_LIST)\nplt.title('Distribution of Image Ratios')\nplt.xlabel('Ratio Value')\nplt.ylabel('Count')\nplt.show()","9b65e43d":"def crop_imgs(set_name, add_pixels_value=0):\n    \"\"\"\n    Finds the extreme points on the image and crops the rectangular out of them\n    \"\"\"\n    set_new = []\n    for img in set_name:\n        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        gray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n        # threshold the image, then perform a series of erosions +\n        # dilations to remove any small regions of noise\n        thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n        thresh = cv2.erode(thresh, None, iterations=2)\n        thresh = cv2.dilate(thresh, None, iterations=2)\n\n        # find contours in thresholded image, then grab the largest one\n        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cnts = imutils.grab_contours(cnts)\n        c = max(cnts, key=cv2.contourArea)\n\n        # find the extreme points\n        extLeft = tuple(c[c[:, :, 0].argmin()][0])\n        extRight = tuple(c[c[:, :, 0].argmax()][0])\n        extTop = tuple(c[c[:, :, 1].argmin()][0])\n        extBot = tuple(c[c[:, :, 1].argmax()][0])\n\n        ADD_PIXELS = add_pixels_value\n        new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n        set_new.append(new_img)\n\n    return np.array(set_new)","f1d2f4c2":"img = cv2.imread('..\/input\/brain-mri-images-for-brain-tumor-detection\/brain_tumor_dataset\/yes\/Y108.jpg')\nimg = cv2.resize(\n            img,\n            dsize=IMG_SIZE,\n            interpolation=cv2.INTER_CUBIC\n        )\ngray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\ngray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n# threshold the image, then perform a series of erosions +\n# dilations to remove any small regions of noise\nthresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\nthresh = cv2.erode(thresh, None, iterations=2)\nthresh = cv2.dilate(thresh, None, iterations=2)\n\n# find contours in thresholded image, then grab the largest one\ncnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = imutils.grab_contours(cnts)\nc = max(cnts, key=cv2.contourArea)\n\n# find the extreme points\nextLeft = tuple(c[c[:, :, 0].argmin()][0])\nextRight = tuple(c[c[:, :, 0].argmax()][0])\nextTop = tuple(c[c[:, :, 1].argmin()][0])\nextBot = tuple(c[c[:, :, 1].argmax()][0])\n\n# add contour on the image\nimg_cnt = cv2.drawContours(img.copy(), [c], -1, (0, 255, 255), 4)\n\n# add extreme points\nimg_pnt = cv2.circle(img_cnt.copy(), extLeft, 8, (0, 0, 255), -1)\nimg_pnt = cv2.circle(img_pnt, extRight, 8, (0, 255, 0), -1)\nimg_pnt = cv2.circle(img_pnt, extTop, 8, (255, 0, 0), -1)\nimg_pnt = cv2.circle(img_pnt, extBot, 8, (255, 255, 0), -1)\n\n# crop\nADD_PIXELS = 0\nnew_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()","4eb4dfea":"plt.figure(figsize=(15,6))\nplt.subplot(141)\nplt.imshow(img)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 1. Get the original image')\nplt.subplot(142)\nplt.imshow(img_cnt)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 2. Find the biggest contour')\nplt.subplot(143)\nplt.imshow(img_pnt)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 3. Find the extreme points')\nplt.subplot(144)\nplt.imshow(new_img)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 4. Crop the image')\nplt.show()","3c42f911":"# apply this for each set\nX_train_crop = crop_imgs(set_name=X_train)\nX_val_crop = crop_imgs(set_name=X_val)\nX_test_crop = crop_imgs(set_name=X_test)","b3c6b8e0":"plot_samples(X_train_crop, y_train, labels, 10)","edfd6b8a":"def save_new_images(x_set, y_set, folder_name):\n    i = 0\n    for (img, imclass) in zip(x_set, y_set):\n        if imclass == 0:\n            cv2.imwrite(folder_name+'NO\/'+str(i)+'.jpg', img)\n        else:\n            cv2.imwrite(folder_name+'YES\/'+str(i)+'.jpg', img)\n        i += 1","6c61cd31":"# saving new images to the folder\n!mkdir TRAIN_CROP TEST_CROP VAL_CROP TRAIN_CROP\/YES TRAIN_CROP\/NO TEST_CROP\/YES TEST_CROP\/NO VAL_CROP\/YES VAL_CROP\/NO\n\nsave_new_images(X_train_crop, y_train, folder_name='TRAIN_CROP\/')\nsave_new_images(X_val_crop, y_val, folder_name='VAL_CROP\/')\nsave_new_images(X_test_crop, y_test, folder_name='TEST_CROP\/')","97978209":"def preprocess_imgs(set_name, img_size):\n    \"\"\"\n    Resize and apply VGG-15 preprocessing\n    \"\"\"\n    set_new = []\n    for img in set_name:\n        img = cv2.resize(\n            img,\n            dsize=img_size,\n            interpolation=cv2.INTER_CUBIC\n        )\n        set_new.append(preprocess_input(img))\n    return np.array(set_new)","c9b0dc51":"X_train_prep = preprocess_imgs(set_name=X_train_crop, img_size=IMG_SIZE)\nX_test_prep = preprocess_imgs(set_name=X_test_crop, img_size=IMG_SIZE)\nX_val_prep = preprocess_imgs(set_name=X_val_crop, img_size=IMG_SIZE)","4f65e14c":" plot_samples(X_train_prep, y_train, labels, 10)","3990018c":"# set the paramters we want to change randomly\ndemo_datagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.05,\n    height_shift_range=0.05,\n    rescale=1.\/255,\n    shear_range=0.05,\n    brightness_range=[0.1, 1.5],\n    horizontal_flip=True,\n    vertical_flip=True\n)","8cdaf6be":"os.mkdir('preview')\nx = X_train_crop[0]  \nx = x.reshape((1,) + x.shape) \n\ni = 0\nfor batch in demo_datagen.flow(x, batch_size=1, save_to_dir='preview', save_prefix='aug_img', save_format='jpg'):\n    i += 1\n    if i > 20:\n        break ","07d88eab":"plt.imshow(X_train_crop[0])\nplt.xticks([])\nplt.yticks([])\nplt.title('Original Image')\nplt.show()\n\nplt.figure(figsize=(15,6))\ni = 1\nfor img in os.listdir('preview\/'):\n    img = cv2.cv2.imread('preview\/' + img)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.subplot(3,7,i)\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\n    i += 1\n    if i > 3*7:\n        break\nplt.suptitle('Augemented Images')\nplt.show()","c70bfa5b":"!rm -rf preview\/","842a71f3":"TRAIN_DIR = 'TRAIN_CROP\/'\nVAL_DIR = 'VAL_CROP\/'\n\ntrain_datagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    brightness_range=[0.5, 1.5],\n    horizontal_flip=True,\n    vertical_flip=True,\n    preprocessing_function=preprocess_input\n)\n\ntest_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input\n)\n\n\ntrain_generator = train_datagen.flow_from_directory(\n    TRAIN_DIR,\n    color_mode='rgb',\n    target_size=IMG_SIZE,\n    batch_size=32,\n    class_mode='binary',\n    seed=RANDOM_SEED\n)\n\n\nvalidation_generator = test_datagen.flow_from_directory(\n    VAL_DIR,\n    color_mode='rgb',\n    target_size=IMG_SIZE,\n    batch_size=16,\n    class_mode='binary',\n    seed=RANDOM_SEED\n)","df820ec5":"# load base model\n#from keras.applications import ResNet50\n!pip install -U git+https:\/\/github.com\/keras-team\/keras git+https:\/\/github.com\/keras-team\/keras-applications\nfrom keras.applications import  resnet\n\n#resnet50_weight_path = '..\/input\/keras-pretrained-models\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\nresnet101 =resnet.ResNet101(include_top=False, weights='imagenet', input_shape=IMG_SIZE + (3,))\nNUM_CLASSES = 1\n\nmodel = Sequential()\nmodel.add(resnet101)\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(NUM_CLASSES, activation='sigmoid'))\n\nmodel.layers[0].trainable = False\nmodel.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=1e-4), metrics=['accuracy'])\n#model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.0003, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False), metrics=[\"accuracy\"])\nfrom keras.callbacks import ModelCheckpoint\nfilepath = \"saved-model-Resnet101{epoch:02d}-{val_acc:.2f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]\n\nmodel.summary()\nresnet101 = model.fit_generator(\n    train_generator,steps_per_epoch=50,\n    epochs=25,validation_data=validation_generator,validation_steps=25,callbacks=callbacks_list)","3d1e9103":"#history_1=vgg\n#history_2=inception_v3_model\nhistory_3=resnet101","6f84f3cb":"def ModelGraph(history,epoch,model_name):\n    \n\n    # Plot training & validation accuracy values\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n\n\n    plt.title('Model accuracy of ' +model_name)\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    # Plot training & validation loss values\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()","5cce6117":"def ModelGraphTrainngSummary(history,N,model_name):\n    # set the matplotlib backend so figures can be saved in the background\n    # plot the training loss and accuracy\n    import sys\n    import matplotlib\n    print(\"Generating plots...\")\n    sys.stdout.flush()\n    matplotlib.use(\"Agg\")\n    matplotlib.pyplot.style.use(\"ggplot\")\n    matplotlib.pyplot.figure()\n    matplotlib.pyplot.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n    matplotlib.pyplot.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n    matplotlib.pyplot.plot(np.arange(0, N), history.history[\"acc\"], label=\"train_acc\")\n    matplotlib.pyplot.plot(np.arange(0, N), history.history[\"val_acc\"], label=\"val_acc\")\n    matplotlib.pyplot.title(\"Training Loss and Accuracy on diabetic retinopathy detection\")\n    matplotlib.pyplot.xlabel(\"Epoch #\")\n    matplotlib.pyplot.ylabel(\"Loss\/Accuracy of \"+model_name)\n    matplotlib.pyplot.legend(loc=\"lower left\")\n    matplotlib.pyplot.savefig(\"plot.png\")","bda2fe8f":"ModelGraphTrainngSummary(history_3,25,'ResNet101')","cfb64579":"# validate on val set\npredictions = model.predict(X_val_prep)\npredictions = [1 if x>0.5 else 0 for x in predictions]\n\naccuracy = accuracy_score(y_val, predictions)\nprint('Val Accuracy = %.2f' % accuracy)\n\nconfusion_mtx = confusion_matrix(y_val, predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)","3929e698":"# validate on test set\npredictions = model.predict(X_test_prep)\npredictions = [1 if x>0.5 else 0 for x in predictions]\n\naccuracy = accuracy_score(y_test, predictions)\nprint('Test Accuracy = %.2f' % accuracy)\n\nconfusion_mtx = confusion_matrix(y_test, predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)","2e351fd9":"ind_list = np.argwhere((y_test == predictions) == False)[:, -1]\nif ind_list.size == 0:\n    print('There are no missclassified images.')\nelse:\n    for i in ind_list:\n        plt.figure()\n        plt.imshow(X_test_crop[i])\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(f'Actual class: {y_val[i]}\\nPredicted class: {predictions[i]}')\n        plt.show()","e356cb41":"# clean up the space\n!rm -rf TRAIN TEST VAL TRAIN_CROP TEST_CROP VAL_CROP\n# save the model\nmodel.save('Brain_Tumor_ResNet101_model.h5')","dd3daf61":"# <a id='cnn'>4. CNN Model<\/a>\n\nI was using [Transfer Learning](https:\/\/towardsdatascience.com\/keras-transfer-learning-for-beginners-6c9b8b7143e) with VGG-16 architecture , xception,InceptionV3 and weights as a base model.\n\n## <a id='aug'>4.1. Data Augmentation<\/a>\n\nSince I had small data set I used the technique called [Data Augmentation](https:\/\/blog.keras.io\/building-powerful-image-classification-models-using-very-little-data.html) which helps to \"increase\" the size of training set.\n\n### <a id='demo'>4.1.1. Demo<\/a>\n\nThat's the example from one image how does augmentation look like.","d91310d0":"**<center><font size=5>Brain Tumor Detection with Transefer Learning MobileNet V2 Model<\/font><\/center>**\n***\n\n **Table of Contents**\n- <a href='#intro'>1. Project Overview and Objectives<\/a> \n    - <a href='#dataset'>1.1. Data Set Description<\/a>\n    - <a href='#tumor'>1.2. What is Brain Tumor?<\/a>\n- <a href='#env'>2. Setting up the Environment<\/a>\n- <a href='#import'>3. Data Import and Preprocessing<\/a>\n- <a href='#cnn'>4. CNN Model<\/a>\n    - <a href='#aug'>4.1. Data Augmentation<\/a>\n        - <a href='#demo'>4.1.1. Demo<\/a>\n        - <a href='#apply'>4.1.2. Apply<\/a>\n    - <a href='#build'>4.2. Transfer Learning Tutorial <\/a>\n    - <a href='#perf'>4.3. Model Performance<\/a>\n- <a href='#concl'>5. Conclusions<\/a>","79726773":"## <a id='perf'>4.3. Model Performance<\/a>","bbd12ff4":"As you can see, images have different `width` and `height` and diffent size of \"black corners\". Since the image size for VGG-16 imput layer is `(224,224)` some wide images may look weird after resizing. Histogram of ratio distributions (`ratio = width\/height`):","04b93655":"### <a id='apply'>4.1.2. Apply<\/a>","3a051bfa":"# <a id='concl'>5. Conclusions<\/a>\n\nThis project was a combination of CNN model classification problem (to predict wheter the subject has brain tumor or not) & Computer Vision problem (to automate the process of brain cropping from MRI scans). The final accuracy is much higher than 50% baseline (random guess). However, it could be increased by larger number of train images or through model hyperparameters tuning.\n\nWell we've done very well just about a cycle brother just that it's a record I think I'm happy with that result and I'm also happy to share some science with you.\nNow it is your turn to use lots and lots of your own models, you can publish your next research paper entitled Transefer Learning .\nThis was a great lesson and thank you for following up and thank this man for his efforts at this kernel\n\n Ruslan Klymentiev\n\n","2725bddf":"The first step of \"normalization\" would be to crop the brain out of the images. I used technique which was perfectly described in [pyimagesearch](https:\/\/www.pyimagesearch.com\/2016\/04\/11\/finding-extreme-points-in-contours-with-opencv\/) blog and I highly suggest to looks deeper into it.","a3e229d0":"First scan looks a bit misleading - what is that light spot in the middle? \ud83e\udd14 I can clearly see why model classified it as `Tumor`.","e9a88709":"# <a id='env'>2. Setting up the Environment<\/a>","e4db0a0c":"# <a id='import'>3. Data Import and Preprocessing<\/a>","a4e60a04":"Let's take a look at the distribution of classes among sets:","88e13fbd":"The next step would be resizing images to `(224,224)` and applying preprocessing needed for VGG-16 model input.","f471c9d7":"## <a id='build'>4.2. Transfer Learning Tutorial<\/a>","6f4e6df2":"**Confusion matrix of MobileNet **","639251c6":"Right now all images are in one folder with `yes` and `no` subfolders. I will split the data into `train`, `val` and `test` folders which makes its easier to work for me. The new folder heirarchy will look as follows:","53cb2dde":"Let's look at example what this function will do with MRI scans:"}}