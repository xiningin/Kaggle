{"cell_type":{"ee432645":"code","8b24d192":"code","27724ef4":"code","861a5f8a":"code","8b5599a5":"code","4d7490a7":"code","96a87b0a":"code","b50f6a67":"code","6129d6e4":"code","167df615":"code","6dcc9eca":"code","ead7f9c0":"code","0e727ec6":"code","03500989":"code","f639c9e1":"markdown","df190911":"markdown","e65c5bb1":"markdown"},"source":{"ee432645":"!pip install transformers[\"ja\"]\n!pip install accelerate","8b24d192":"import os\nimport gc\nimport sys\nimport cv2\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom accelerate import Accelerator\n\nfrom transformers import (AutoModel,\n                          AutoModelForSequenceClassification,\n                          AutoTokenizer,\n                          AutoConfig,\n                          get_cosine_schedule_with_warmup\n                         )\n\nfrom colorama import Fore, Back, Style\n\nfrom transformers import T5Tokenizer\n\nr_ = Fore.RED\nb_ = Fore.BLUE\nc_ = Fore.CYAN\ng_ = Fore.GREEN\ny_ = Fore.YELLOW\nm_ = Fore.MAGENTA\nsr_ = Style.RESET_ALL\n","27724ef4":"# \u8a2d\u5b9a\nconfig = {\n    'lr': 2e-5,\n    'wd':0.01,\n    'batch_size':16,\n    'valid_step':50,\n    'max_len':256,\n    'epochs':1,\n    'nfolds':5,\n    'seed':42,\n    'model_name':\"rinna\/japanese-roberta-base\" # https:\/\/huggingface.co\/rinna\/japanese-roberta-base\n}","861a5f8a":"# \u30b7\u30fc\u30c9\u5024\u306e\u56fa\u5b9a\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])\n","8b5599a5":"# \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\ntrain_all_df = pd.read_csv('..\/input\/fakenews-nlp\/train.csv')\ntest_df = pd.read_csv('..\/input\/fakenews-nlp\/test.csv')\nsample_sub = pd.read_csv('..\/input\/fakenews-nlp\/sample_submission.csv')\n\ndisplay(train_all_df.head())\ndisplay(test_df.head())\ndisplay(sample_sub.head())","4d7490a7":"# fold\u306e\u5272\u5f53\ntrain_all_df['Fold'] = -1\nkfold = StratifiedKFold(n_splits=config['nfolds'],shuffle=True,random_state=config['seed'])\nfor k , (train_idx,valid_idx) in enumerate(kfold.split(X=train_all_df,y=train_all_df['isfake'])):\n    train_all_df.loc[valid_idx,'Fold'] = k","96a87b0a":"# \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5b9a\u7fa9\nclass SeqDataset(Dataset):\n    def __init__(self,df,tokenizer,max_len=128):\n        self.targets = df['isfake'].to_numpy()\n        self.context = df['context'].to_numpy()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.context[idx],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)\n        \n        target = torch.tensor(self.targets[idx],dtype=torch.float) \n        return encode, target\n    \n    def __len__(self):\n        return len(self.context)","b50f6a67":"# \u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\n\nclass AttentionHead(nn.Module):\n    ''' \n    BERT \u306e\u30d8\u30c3\u30c9\u306b\u3064\u3051\u308b\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u6a5f\u69cb\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u3082\u306e\u3092\u7528\u3044\u3066\u3082\u826f\u3044\u304c\u3001\u30aa\u30ea\u30b8\u30ca\u30eb\u306e\u3082\u306e\u3092\u4f5c\u6210\u3057\u3066\u5b66\u7fd2\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\n    '''\n    def __init__(self, in_features, hidden_dim):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n    \n\nclass Model(nn.Module):\n    '''\n    \u30e2\u30c7\u30eb\u672c\u4f53\n    '''\n    def __init__(self,path):\n        super(Model,self).__init__()\n        self.roberta = AutoModel.from_pretrained(path)  \n        self.config = AutoConfig.from_pretrained(path)\n        self.head = AttentionHead(self.config.hidden_size,self.config.hidden_size)\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(self.config.hidden_size,1)\n\n    def forward(self,**xb):\n        x = self.roberta(**xb)[0]\n        x = self.head(x)\n        x = self.dropout(x)\n        x = self.linear(x)\n        return x","6129d6e4":"def run(fold,verbose=True):\n    \n    def loss_fn(outputs,targets):\n        outputs = outputs.view(-1)\n        targets = targets.view(-1)\n        return torch.sqrt(nn.MSELoss()(outputs,targets))\n    \n    def train_and_evaluate_loop(train_loader,valid_loader, model, loss_fn,optimizer,epoch,fold,best_loss,valid_step=10,lr_scheduler=None):\n        train_loss = 0\n        for i, (inputs1,targets1) in enumerate(tqdm(train_loader)):\n            model.train()\n            optimizer.zero_grad()\n            inputs1 = {key:val.reshape(val.shape[0],-1) for key,val in inputs1.items()}\n            outputs1 = model(**inputs1)\n            loss1 = loss_fn(outputs1,targets1)\n            loss1.backward()\n            optimizer.step()\n            \n            train_loss += loss1.item()\n            \n            if lr_scheduler:\n                lr_scheduler.step()\n            \n            #evaluating for every valid_step\n            if (i % valid_step == 0) or ((i + 1) == len(train_loader)):\n                model.eval()\n                valid_loss = 0\n                with torch.no_grad():\n                    for j, (inputs2,targets2) in enumerate(valid_loader):\n                        inputs2 = {key:val.reshape(val.shape[0],-1) for key,val in inputs2.items()}\n                        outputs2 = model(**inputs2)\n                        loss2 = loss_fn(outputs2,targets2)\n                        valid_loss += loss2.item()\n                     \n                    valid_loss \/= len(valid_loader)\n                    if valid_loss <= best_loss:\n                        if verbose:\n                            print(f\"epoch:{epoch} | Train Loss:{train_loss\/(i+1)} | Validation loss:{valid_loss}\")\n                            print(f\"{g_}Validation loss Decreased from {best_loss} to {valid_loss}{sr_}\")\n\n                        best_loss = valid_loss\n                        torch.save(model.state_dict(),f'.\/model{fold}\/model{fold}.bin')\n                        tokenizer.save_pretrained(f'.\/model{fold}')\n                        \n        return best_loss\n    \n    \n    accelerator = Accelerator()\n    print(f\"{accelerator.device} is used\")\n    \n    train_df, valid_df= train_all_df.query(f\"Fold != {fold}\"), train_all_df.query(f\"Fold == {fold}\")\n#     tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n    tokenizer =  T5Tokenizer.from_pretrained(config['model_name'])\n    model = Model(config['model_name'])\n\n    train_ds = SeqDataset(train_df, tokenizer,config['max_len'])\n    train_dl = DataLoader(train_ds,\n                        batch_size = config[\"batch_size\"],\n                        shuffle=True,\n                        num_workers = 4,\n                        pin_memory=True,\n                        drop_last=False)\n\n    valid_ds = SeqDataset(valid_df,tokenizer,config['max_len'])\n    valid_dl = DataLoader(valid_ds,\n                        batch_size = config[\"batch_size\"],\n                        shuffle=False,\n                        num_workers = 4,\n                        pin_memory=True,\n                        drop_last=False)\n\n    optimizer = optim.AdamW(model.parameters(),lr=config['lr'],weight_decay=config['wd'])\n    lr_scheduler = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps= 10 * len(train_dl))\n\n    model,train_dl,valid_dl,optimizer,lr_scheduler = accelerator.prepare(model,train_dl,valid_dl,optimizer,lr_scheduler)\n\n    print(f\"Fold: {fold}\")\n    os.makedirs(f'model{fold}',exist_ok=True)\n    best_loss = 9999\n    for epoch in range(config[\"epochs\"]):\n        print(f\"Epoch Started:{epoch}\")\n        best_loss = train_and_evaluate_loop(train_dl,valid_dl,model,loss_fn,\n                                            optimizer,epoch,fold,best_loss,\n                                            valid_step=config['valid_step'],lr_scheduler=lr_scheduler)","167df615":"for f in range(config['nfolds']):\n    run(f)\n    break # \u3053\u308c\u306f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306a\u306e\u3067\u3068\u308a\u3042\u3048\u305afold0\u3060\u3051","6dcc9eca":"class TestSeqDataset(Dataset):\n    def __init__(self,df,tokenizer):\n        self.excerpt = df['context'].to_numpy()\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],return_tensors='pt',\n                                max_length=config['max_len'],\n                                padding='max_length',truncation=True)\n        return encode\n    \n    def __len__(self):\n        return len(self.excerpt)","ead7f9c0":"def get_prediction(df,path,device='cuda'):        \n    model = Model(config['model_name'])\n#     tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n    tokenizer =  T5Tokenizer.from_pretrained(config['model_name'])\n    model.load_state_dict(torch.load(path,map_location=device))\n    model.to(device)\n    model.eval()\n    \n    test_ds = TestSeqDataset(df,tokenizer)\n    test_dl = DataLoader(test_ds,\n                        batch_size = config[\"batch_size\"],\n                        shuffle=False,\n                        num_workers = 4,\n                        pin_memory=True)\n    \n    predictions = list()\n    for i, (inputs) in tqdm(enumerate(test_dl)):\n        inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n        outputs = model(**inputs)\n        outputs = outputs.cpu().detach().numpy().ravel().tolist()\n        predictions.extend(outputs)\n        \n    torch.cuda.empty_cache()\n    return np.array(predictions)","0e727ec6":"pred1 = get_prediction(test_df,'.\/model0\/model0.bin')\nsample_sub['isfake'] = pred1\nsample_sub.to_csv('submission.csv',index=False)","03500989":"sample_sub.head()","f639c9e1":"# \u63a8\u8ad6","df190911":"# BERT\u3092\u7528\u3044\u305fbenchmark rinna\u7248","e65c5bb1":"# \u5b66\u7fd2"}}