{"cell_type":{"170f7674":"code","2336d8ac":"code","eec8f2ce":"code","5f27fa9f":"code","fb073753":"code","5ad73051":"code","ec69549d":"code","baee4c87":"code","f8748614":"code","3f1661f2":"code","973d76a7":"code","c6da0436":"code","fa25a74d":"code","f0786e01":"code","3348f62e":"code","930cb941":"code","db90414e":"code","2d5151ea":"code","3da59e46":"code","ef44c96e":"code","6bb9b3d7":"code","5e8e57ae":"code","b02c33b1":"code","bb1323f2":"code","27f32c76":"markdown","3eb6a40c":"markdown"},"source":{"170f7674":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas_profiling as profile","2336d8ac":"df = pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\ndf.head(5)","eec8f2ce":"df.shape","5f27fa9f":"df.columns","fb073753":"df.describe().T","5ad73051":"import missingno as msno\nn = msno.bar(df,color=\"gray\")\nprint(n)","ec69549d":"profile.ProfileReport(df)","baee4c87":"sns.countplot(x=\"output\",data = df)\nplt.show()","f8748614":"plt.figure(figsize=(20,10))\nsns.boxplot(data = df,palette = \"Set1\")\nplt.xticks(rotation=90)\nplt.show()","3f1661f2":"def removeOutlier(att, df):\n\n    lowerbound = att.mean() - 3 * att.std()\n    upperbound = att.mean() + 3 * att.std()\n\n    print('lowerbound: ',lowerbound,' -------- upperbound: ', upperbound )\n\n    df1 = df[(att > lowerbound) & (att < upperbound)]\n\n    print((df.shape[0] - df1.shape[0]), ' number of outliers from ', df.shape[0] )\n    print(' ******************************************************')\n    \n    df = df1.copy()\n\n    return df","973d76a7":"df = removeOutlier(df.trtbps, df)\ndf = removeOutlier(df.chol, df)\ndf","c6da0436":"corrmat = df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,10))\ng = sns.heatmap(df[top_corr_features].corr(),annot = True,cmap = \"RdYlGn\")","fa25a74d":"df.output.value_counts()","f0786e01":"from sklearn.utils import resample\n\n# Separate Target Classes\ndf_1 = df[df.output==1]\ndf_2 = df[df.output==0]\n \n# Upsample minority class\ndf_upsample_1 = resample(df_2, \n                                 replace=True,     # sample with replacement\n                                 n_samples=163,    # to match majority class\n                                 random_state=123) # reproducible results\n\n# Combine majority class with upsampled minority class\ndf_upsampled = pd.concat([df_1, df_upsample_1])\n \n# Display new class counts\ndf_upsampled.output.value_counts()","3348f62e":"x = df_upsampled.drop('output', axis = 1)\ny = df_upsampled['output'] \n","930cb941":"from sklearn.model_selection import train_test_split as tts\n\nx_train,x_test, y_train, y_test = tts(x,y, test_size = 0.3)","db90414e":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","2d5151ea":"# Function to Evaluate\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score\nfrom sklearn.preprocessing import LabelBinarizer\nfrom mlxtend.plotting import plot_confusion_matrix\n\ndef evaluator(y_test, y_pred):    \n    \n    # Accuracy:\n    print('Accuracy is: ', accuracy_score(y_test,y_pred))\n    print('')\n    # Classification Report:\n    print('Classification Report: \\n',classification_report(y_test,y_pred))\n\n    # Area Under The Curve Score:\n\n    lb = LabelBinarizer()\n    y_test1 = lb.fit_transform(y_test)\n    y_pred1 =lb.transform(y_pred)\n    print('AUC_ROC Score: ',roc_auc_score(y_test1,y_pred1,average='macro'),'\\n\\n')\n\n    print('Confusion Matrix: \\n\\n')\n    plt.style.use(\"ggplot\")\n    cm = confusion_matrix(y_test,y_pred)\n    plot_confusion_matrix(conf_mat = cm,figsize=(8,6),show_normed=True)","3da59e46":"from sklearn.ensemble import RandomForestClassifier\n\nrf_classifier = RandomForestClassifier()\n\nrf_classifier.fit(x_train,y_train)","ef44c96e":"pred_rf = rf_classifier.predict(x_test)\n\nevaluator(y_test, pred_rf)","6bb9b3d7":"from catboost import CatBoostClassifier\n\ncat_classifier = CatBoostClassifier(iterations=1000, verbose = 0)\n\ncat_classifier.fit(x_train, y_train)","5e8e57ae":"pred_cat = cat_classifier.predict(x_test)\n\nevaluator(y_test, pred_cat)","b02c33b1":"important_features = pd.DataFrame({'Features': x.columns, \n                                   'Importance': rf_classifier.feature_importances_})\n\n# sort the dataframe in the descending order according to the feature importance\nimportant_features = important_features.sort_values('Importance', ascending = False)\n\n# create a barplot to visualize the features based on their importance\nsns.barplot(x = 'Importance', y = 'Features', data = important_features)\n\n# add plot and axes labels\n# set text size using 'fontsize'\nplt.title('Feature Importance', fontsize = 15)\nplt.xlabel('Importance', fontsize = 15)\nplt.ylabel('Features', fontsize = 15)\n\n# display the plot\nplt.show()","bb1323f2":"from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom matplotlib import pyplot\n# define lists to collect scores\ntrain_scores, test_scores = list(), list()\n# define the tree depths to evaluate\nvalues = [i for i in range(1, 21)]\n# evaluate a decision tree for each depth\nfor i in values:\n    # configure the model\n    model = DecisionTreeClassifier(max_depth=i)\n    # fit model on the training dataset\n    model.fit(x_train, y_train)\n    # evaluate on the train dataset\n    train_yhat = model.predict(x_train)\n    train_acc = accuracy_score(y_train, train_yhat)\n    train_scores.append(train_acc)\n    # evaluate on the test dataset\n    test_yhat = model.predict(x_test)\n    test_acc = accuracy_score(y_test, test_yhat)\n    test_scores.append(test_acc)\n    # summarize progress\n    print('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))\n# plot of train and test scores vs tree depth\npyplot.plot(values, train_scores, '-o', label='Train')\npyplot.plot(values, test_scores, '-o', label='Test')\npyplot.legend()\npyplot.show()","27f32c76":"Checking for overfitting","3eb6a40c":"---\n## Heart Attack Prediction\n---\n### Aurthor: Avinash Bagul\n##### MSc Artificial Intelligence (University of Aberdeen)"}}