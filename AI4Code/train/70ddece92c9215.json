{"cell_type":{"8938c662":"code","b8f093e3":"code","08cf39ea":"code","5a7cddd8":"code","7baae364":"code","838bf812":"code","8d3fef8d":"code","f5762c8b":"code","d0e59dd7":"code","0ace5c3f":"code","f5e50be3":"code","94505da9":"code","4fef9a6a":"code","c691b053":"code","e3ec1ee5":"code","a52e1527":"code","17a91757":"code","a88aa04c":"code","c566e9fb":"code","4d938603":"code","7a973f2c":"code","0beca3c6":"code","d2deaa82":"code","2b75e02c":"code","a92eb478":"code","9515ba53":"code","1fc0b221":"code","a05d7622":"code","830dcd50":"code","930c9999":"code","08c3c2a9":"code","29337107":"markdown","5e478131":"markdown","7bdda553":"markdown","74e58a03":"markdown"},"source":{"8938c662":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nfrom sklearn import metrics\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import text, sequence\nfrom keras.models import load_model\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, SpatialDropout1D, Activation, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Flatten, GlobalMaxPooling1D\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import EarlyStopping\n\nfrom sklearn.model_selection import train_test_split\n\nimport gc\n\n# Any results you write to the current directory are saved as output.","b8f093e3":"TEXT_COL = 'comment_text'\nEMB_PATH = '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec'\ntrain_df = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv', index_col='id')\ntest_df = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv', index_col='id')","08cf39ea":"train_df.head()","5a7cddd8":"train_df.shape","7baae364":"train_df.describe()","838bf812":"train_df.isna().sum()","8d3fef8d":"train_df.columns","f5762c8b":"train_df.target.plot.hist()","d0e59dd7":"train_df.target = np.where(train_df.target> 0.5, 1, 0)\nprint(train_df.target.value_counts())\nsns.countplot(train_df.target)","0ace5c3f":"#train_df['rating'].value_counts()\ntrain_df['rating'] = np.where(train_df['rating'] == \"approved\", 1, 0)\ntrain_df['rating'].value_counts()\nsns.countplot(train_df['rating'])","f5e50be3":"features = ['severe_toxicity', 'obscene',\n       'identity_attack', 'insult', 'threat', 'asian', 'atheist', 'bisexual',\n       'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n       'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n       'jewish', 'latino', 'male', 'muslim', 'other_disability',\n       'other_gender', 'other_race_or_ethnicity', 'other_religion',\n       'other_sexual_orientation', 'physical_disability',\n       'psychiatric_or_mental_illness', 'transgender', 'white', 'rating', 'funny', 'wow',\n       'sad', 'likes', 'disagree', 'sexual_explicit',\n       'identity_annotator_count', 'toxicity_annotator_count']\n\n\ntoxicity_features = [\"severe_toxicity\", \"obscene\", \"threat\", \"insult\", \"identity_attack\", \"sexual_explicit\"]\n\nidentity_features = [\"male\", \"female\", \"transgender\", \"other_gender\", \"heterosexual\", \"homosexual_gay_or_lesbian\",\n                     \"bisexual\", \"other_sexual_orientation\", \"christian\", \"jewish\", \"muslim\", \"hindu\", \"buddhist\",\n                     \"atheist\", \"other_religion\", \"black\", \"white\", \"asian\", \"latino\", \"other_race_or_ethnicity\",\n                     \"physical_disability\", \"intellectual_or_learning_disability\", \"psychiatric_or_mental_illness\", \"other_disability\"]\n\nmetadata_features = [\"rating\", \"funny\", \"wow\", \"sad\", \"likes\", \"disagree\", \"toxicity_annotator_count\", \"identity_annotator_count\"]","94505da9":"train_df[features].head()","4fef9a6a":"print('Distributions columns')\nplt.figure(figsize=(20, 150))\nfor i, col in enumerate(toxicity_features):\n    plt.subplot(40, 4, i + 1)\n    plt.hist(train_df[col]) \n    plt.title(col)\nplt.tight_layout()","c691b053":"print('Distributions columns')\nplt.figure(figsize=(20, 150))\nfor i, col in enumerate(identity_features):\n    plt.subplot(40, 4, i + 1)\n    plt.hist(train_df[col]) \n    plt.title(col)\nplt.tight_layout()","e3ec1ee5":"print('Distributions columns')\nplt.figure(figsize=(20, 150))\nfor i, col in enumerate(metadata_features):\n    plt.subplot(40, 4, i + 1)\n    plt.hist(train_df[col]) \n    plt.title(col)\nplt.tight_layout()","a52e1527":"print('Distributions columns')\nplt.figure(figsize=(20, 150))\nfor i, col in enumerate(toxicity_features):\n    plt.subplot(40, 4, i + 1)\n    plt.hist(train_df[col]) \n    plt.hist(train_df[train_df[\"target\"] == 0][col], alpha=0.5, label='0', color='b')\n    plt.hist(train_df[train_df[\"target\"] == 1][col], alpha=0.5, label='1', color='r') \n    plt.title(col)\nplt.tight_layout()","17a91757":"print('Distributions columns')\nplt.figure(figsize=(20, 150))\nfor i, col in enumerate(identity_features):\n    plt.subplot(40, 4, i + 1)\n    plt.hist(train_df[col]) \n    plt.hist(train_df[train_df[\"target\"] == 0][col], alpha=0.5, label='0', color='b')\n    plt.hist(train_df[train_df[\"target\"] == 1][col], alpha=0.5, label='1', color='r') \n    plt.title(col)\nplt.tight_layout()","a88aa04c":"print('Distributions columns')\nplt.figure(figsize=(20, 150))\nfor i, col in enumerate(metadata_features):\n    plt.subplot(40, 4, i + 1)\n    plt.hist(train_df[col]) \n    plt.hist(train_df[train_df[\"target\"] == 0][col], alpha=0.5, label='0', color='b')\n    plt.hist(train_df[train_df[\"target\"] == 1][col], alpha=0.5, label='1', color='r') \n    plt.title(col)\nplt.tight_layout()","c566e9fb":"plt.close();\ngc.collect();","4d938603":"train_data = train_df[\"comment_text\"]\nlabel_data = train_df[\"target\"]\ntest_data = test_df[\"comment_text\"]\ntrain_data.shape, label_data.shape, test_data.shape","7a973f2c":"tokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(list(train_data) + list(test_data))","0beca3c6":"train_data = tokenizer.texts_to_sequences(train_df['comment_text'])\ntest_data = tokenizer.texts_to_sequences(test_df['comment_text'])","d2deaa82":"MAX_LEN = 200\ntrain_data = sequence.pad_sequences(train_data, maxlen=MAX_LEN)\ntest_data = sequence.pad_sequences(test_data, maxlen=MAX_LEN)\n\nxtrain, xvalid, ytrain, yvalid = train_test_split(train_data, label_data, stratify=train_df.target, random_state=42, test_size=0.2, shuffle=True)","2b75e02c":"max_features = len(tokenizer.word_index) + 1\nmax_features","a92eb478":"embedding_path1 = \"..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec\"\n#embedding_path2 = \"..\/input\/glove840b300dtxt\/glove.840B.300d.txt\"\nembed_size = 300\n\ndef get_coefs(word,*arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef build_matrix(embedding_path, tokenizer):\n    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n\n    word_index = tokenizer.word_index\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.zeros((nb_words + 1, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features:\n            continue\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix\n\nembedding_matrix = build_matrix(embedding_path1, tokenizer)","9515ba53":"del train_data;\ndel train_df;\ndel test_df;\ndel tokenizer;\ngc.collect();","1fc0b221":"def plot_history(history):\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training acc')\n    plt.plot(x, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()","a05d7622":"NUM_HIDDEN = 256\nEMB_SIZE = 300\nLABEL_SIZE = 1\nMAX_FEATURES = max_features\nDROP_OUT_RATE = 0.2\nDENSE_ACTIVATION = \"sigmoid\"\nNUM_EPOCH = 5\nconv_size = 128\n\nBATCH_SIZE = 512\nLOSS_FUNC = \"binary_crossentropy\"\nOPTIMIZER_FUNC = \"adam\"\nMETRICS = [\"accuracy\"]\n\nfrom numpy.random import seed\nseed(42)\nfrom tensorflow import set_random_seed\nset_random_seed(42)\n\n\nmodel=Sequential()\nmodel.add(Embedding(max_features, EMB_SIZE, weights=[embedding_matrix], trainable=False))\n#model.add(keras.layers.Embedding(max_features, EMB_SIZE))\nmodel.add(SpatialDropout1D(DROP_OUT_RATE))\nmodel.add(LSTM(NUM_HIDDEN, return_sequences=True))\n#model.add(Dropout(rate=DROP_OUT_RATE))\nmodel.add(Conv1D(conv_size, 2, activation='relu', padding='same'))\nmodel.add(MaxPooling1D(5, padding='same'))\nmodel.add(Conv1D(conv_size, 3, activation='relu', padding='same'))\nmodel.add(GlobalMaxPooling1D())\n#model.add(Flatten())\nmodel.add(Dense(LABEL_SIZE, activation=DENSE_ACTIVATION))\n\ncheckpointer = ModelCheckpoint(monitor='val_acc', mode='max', filepath='model.hdf5', verbose=2, save_best_only=True)\nearlyStopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=0, mode='max')\n\nmodel.compile(loss=LOSS_FUNC, optimizer=OPTIMIZER_FUNC, metrics=METRICS)\n\nhistory_lstm = model.fit(\n    xtrain, \n    ytrain, \n    batch_size = BATCH_SIZE, \n    epochs = NUM_EPOCH, callbacks=[checkpointer, earlyStopping],\nvalidation_data=(xvalid, yvalid))","830dcd50":"plot_history(history_lstm)","930c9999":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\n\ny_pred_lstm = model.predict_classes(xvalid, verbose=1, batch_size = BATCH_SIZE)\nprint(classification_report(yvalid, y_pred_lstm))\n\nprint()\nprint(\"accuracy_score\", accuracy_score(yvalid, y_pred_lstm))\n\nprint()\nprint(\"Weighted Averaged validation metrics\")\nprint(\"precision_score\", precision_score(yvalid, y_pred_lstm, average='weighted'))\nprint(\"recall_score\", recall_score(yvalid, y_pred_lstm, average='weighted'))\nprint(\"f1_score\", f1_score(yvalid, y_pred_lstm, average='weighted'))\n\nprint()\nfrom sklearn.metrics import confusion_matrix\nimport scikitplot as skplt\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_confusion_matrix(yvalid, y_pred_lstm)","08c3c2a9":"submission_in = '..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv'\nresult = model.predict(test_data, verbose=1, batch_size = BATCH_SIZE)\n\nsubmission = pd.read_csv(submission_in, index_col='id')\nsubmission['prediction'] = result\nsubmission.reset_index(drop=False, inplace=True)\nsubmission.to_csv('submission.csv',index=False)","29337107":"## EDA","5e478131":"Since for evaluation, the test set examples with target >= 0.5 will be considered to be in the positive class (toxic). The same notion will be applied here; The target from the train set will be transformed as bescribed above. ","7bdda553":"# Jigsaw Unintended Bias in Toxicity Classification, EDA + DL (Keras LSTM)\n\n## Detect toxicity across a diverse range of conversations\n\n![](https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/jigsaw\/003-avatar.png)\n[image source](https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/jigsaw\/003-avatar.png)\n\nIn this competition, you\u2019re challenged to build a multi-headed model that\u2019s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate better than Perspective\u2019s current models. You\u2019ll be using a dataset of comments from Wikipedia\u2019s talk page edits. Improvements to the current model will hopefully help online discussion become more productive and respectful.\n\n## *Kernel in progress, is continuously being updated and extended*","74e58a03":"#### Rating Univariate analysis\nConverting the character feature 'rating' which takes 2 values; approved and rejected into 1 and 0 respectively."}}