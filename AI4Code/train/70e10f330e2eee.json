{"cell_type":{"632f5f0f":"code","906a54b5":"code","da13fe41":"code","79705ff3":"code","59a06ff1":"code","1f30c8f9":"code","b884ec29":"code","ef741e72":"code","ccfe56cc":"code","97b1fc16":"code","71e29fd7":"code","923f6582":"code","f008fe98":"code","0d8ed663":"code","d2fcb384":"code","9820acae":"code","a3d8944c":"code","9fd2dd30":"code","21eb7923":"code","d52d83e6":"code","76e48515":"code","d5a1c4ee":"code","2853dcff":"code","513d146a":"code","c83fe5eb":"code","75d4e78e":"code","94d8f252":"code","a8e2e634":"code","fee6094c":"code","2a835b30":"code","b80842fb":"code","3a08f295":"code","63c7fd04":"code","555c2ffc":"code","cfe11678":"code","880d15f3":"code","cb6bfe87":"code","8312d3e3":"code","c92dbac1":"code","d84cb634":"code","dc8d5842":"code","895279d1":"code","7c5beefc":"code","2c1eaf4b":"code","ba7ebc18":"code","f57a0ec7":"code","b0d3cc9d":"code","c7b8d451":"markdown","3b20211f":"markdown","eefaac8d":"markdown","07f2e4bf":"markdown","8b783665":"markdown","8934a8c0":"markdown","b02b1c87":"markdown","a5039887":"markdown","69c454ed":"markdown","9b0325bf":"markdown","6fbc0615":"markdown","2408223f":"markdown","c9030eff":"markdown","11bfd570":"markdown","ab658705":"markdown","3e94c7c0":"markdown","19d67c51":"markdown","c6ef7fc4":"markdown","5dbd30ae":"markdown","27c50cd7":"markdown","54ecac8a":"markdown","f105259a":"markdown","18e074c8":"markdown","a24638f9":"markdown"},"source":{"632f5f0f":"\n\nimport numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","906a54b5":"pip install BeautifulSoup4","da13fe41":"fake_news = pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv')\nfake_news['credibility'] = 0\nfake_news","79705ff3":"real_news = pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv')\nreal_news['credibility'] = 1\nreal_news","59a06ff1":"# checking for the content of some rows\npd.set_option('max_colwidth', None)\nall_news = fake_news.append(real_news, ignore_index=True)\nall_news.sample(3)","1f30c8f9":"all_news.info()","b884ec29":"import seaborn as sns\n\n# checking for class imbalance\nsns.set(rc={'figure.figsize':(11,5)})\nsns.countplot(x='credibility', data=all_news)","ef741e72":"# checking for duplicate text\n\nfrom hashlib import sha256\nfrom tqdm import tqdm\nlist_ = [ ]\nfor text in tqdm(all_news['text']):\n    hash_ = sha256(text.encode('utf-8')).hexdigest()\n    list_.append(hash_)\nall_news['hash'] = list_\npd.reset_option('max_colwidth')\nall_news","ccfe56cc":"t = all_news.groupby(['hash']).size().reset_index(name='count')\nduplicate = t[t['count']>1]\nprint('there are ',duplicate.shape[0], 'duplicate texts')","97b1fc16":"# removing rows with duplicate text\nall_news.drop_duplicates(subset='hash', inplace=True)\nall_news.reset_index(inplace=True, drop=True)\nall_news.drop('hash', axis=1, inplace=True)\nall_news","71e29fd7":"# checking for class imbalance after dropping duplicates\nsns.set(rc={'figure.figsize':(11,5)})\nsns.countplot(x='credibility', data=all_news)","923f6582":"import seaborn as sns\n\n# checking for relationship between credibility and subject\nsns.set(rc={'figure.figsize':(11,5)})\nsns.countplot(x='subject', data=all_news, hue='credibility')","f008fe98":"#converting date string to datetime format\n\n#removing url in date column\nurl_pattern = \"http\"\nfilter1 = all_news['date'].str.contains(url_pattern)\nall_news = all_news[~filter1]\nall_news","0d8ed663":"# removing other texts in date column\ndate_pattern = \"Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec\"\nfilter2 = all_news['date'].str.contains(date_pattern)\nall_news = all_news[filter2]\nall_news.reset_index(drop=True, inplace=True)","d2fcb384":"# converting date string to datetime format\nall_news_copy = all_news.copy()\nall_news_copy['date'] = pd.to_datetime(all_news_copy['date'])\nall_news_copy.sort_values(by=['date'], inplace=True)\nall_news_copy.reset_index(drop=True, inplace=True)\npd.reset_option('max_rows')\nall_news_copy","9820acae":"# creating a dataframe of fake news counts by date\nfake = all_news_copy[all_news_copy['credibility']==0]\nfake['count'] = 0\nfake = fake.groupby(['date'])['count'].count()\nfake = pd.DataFrame(fake)\nfake","a3d8944c":"# creating a dataframe of real news counts by date\nreal = all_news_copy[all_news_copy['credibility']==1]\nreal['count'] = 0\nreal = real.groupby(['date'])['count'].count()\nreal = pd.DataFrame(real)\nreal","9fd2dd30":"\n# creating lineplots of fake and real news over time\nsns.set(rc={'figure.figsize':(11,5)})\nsns.lineplot(x=fake.index, y=fake['count'])\nsns.lineplot(x=real.index, y=real['count'])","21eb7923":"# word cloud for real news title\n\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\n\nreal_words = \"\"\nfor line in all_news[all_news['credibility']==1]['title']:\n    line = str(line) # change each line item to string\n    tokens = line.split() # split line text into word tokens\n    \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() # convert each token into lower case\n    real_words += \" \".join(tokens)+\" \"\n    \nwordcloud_ = WordCloud(stopwords=stopwords).generate(real_words)\nplt.figure(figsize = (12, 16), facecolor = None) \nplt.axis('off')\nplt.imshow(wordcloud_)","d52d83e6":"# word cloud for fake news title\nfake_words = \"\"\nfor line in all_news[all_news['credibility']==0]['title']:\n    line = str(line) # change each line item to string\n    tokens = line.split() # split line text into word tokens\n    \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() # convert each token into lower case\n    fake_words += \" \".join(tokens)+\" \"\n\nwordcloud_ = WordCloud(stopwords=stopwords).generate(fake_words)\nplt.figure(figsize = (12, 16), facecolor = None) \nplt.axis('off')\nplt.imshow(wordcloud_)","76e48515":"# word cloud for real news text\nreal_words = \"\"\nfor line in all_news[all_news['credibility']==1]['text']:\n    line = str(line) # change each line item to string\n    tokens = line.split() # split line text into word tokens\n    \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() # convert each token into lower case\n    real_words += \" \".join(tokens)+\" \"\n    \nwordcloud_ = WordCloud(stopwords=stopwords).generate(real_words)\nplt.figure(figsize = (12, 16), facecolor = None) \nplt.axis('off')\nplt.imshow(wordcloud_)","d5a1c4ee":"# word cloud for fake news text\n\nfake_words = \"\"\nfor line in all_news[all_news['credibility']==0]['text']:\n    line = str(line) # change each line item to string\n    tokens = line.split() # split line text into word tokens\n    \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() # convert each token into lower case\n    fake_words += \" \".join(tokens)+\" \"\n\nwordcloud_ = WordCloud(stopwords=stopwords).generate(fake_words)\nplt.figure(figsize = (12, 16), facecolor = None) \nplt.axis('off')\nplt.imshow(wordcloud_)","2853dcff":"all_news['news_text'] = all_news['title'] + ' ' + all_news['text']+ ' ' + all_news['subject'] + ' ' + all_news['date']\nall_news.drop(['title', 'text', 'subject', 'date'], axis=1, inplace=True)\nall_news","513d146a":"pd.set_option('max_colwidth', None)\nall_news = all_news[['news_text', 'credibility']]\nall_news.sample()","c83fe5eb":"import nltk\nfrom nltk.corpus import stopwords \n\nnltk.download('words')\nnltk.download('stopwords')\nstop = stopwords.words('english')","75d4e78e":"import re\nfrom bs4 import BeautifulSoup\n\ndef clean_text(text):\n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    text = remove_stopwords(text)\n    text = remove_twitter_handles(text)\n    text = remove_parenthesis(text)\n    return text\n\ndef strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#removing the square brackets\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n\n#remove twitter handles\ndef remove_twitter_handles(text):\n    return re.sub(r'\\(@([A-Za-z0-9_]+)\\)', '', text)\n\n# Removing URL's\ndef remove_between_square_brackets(text):\n    return re.sub(r'http\\S+', '', text)\n# removing parenthesis\ndef remove_parenthesis(text):\n    return re.sub(r'\\([^()]*\\)', '', text)\n\n#Removing the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\n\n\n#Apply function on review column\nall_news['news_text']=all_news['news_text'].apply(clean_text)\nall_news.sample()","94d8f252":"\nfrom nltk import word_tokenize\nall_news_1 = all_news.copy()\nall_news_1['news_text'] = all_news_1['news_text'].apply(lambda x: word_tokenize(str(x)))\nall_news_1.sample()","a8e2e634":"from nltk.stem import SnowballStemmer\n\nsnowball = SnowballStemmer(language='english')\nall_news_1['news_text'] = all_news_1['news_text'].apply(lambda x: [snowball.stem(y) for y in x])\nall_news_1.sample()","fee6094c":"all_news_1['news_text'] = all_news_1['news_text'].apply(lambda x: ' '.join(x))\nall_news_1.sample()","2a835b30":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    all_news_1['news_text'],all_news_1['credibility'], \n    test_size=0.3, \n    stratify=all_news_1['credibility']\n)","b80842fb":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\ntrain_vectors = vectorizer.fit_transform(X_train)\ntest_vectors = vectorizer.fit_transform(X_test)\n\nprint(\"Train vector shape:\",train_vectors.shape)\nprint(\"Test vector shape:\", test_vectors.shape)\n","3a08f295":"\nimport scipy\nfrom scipy.sparse import csr_matrix\n\ntrain_vectors = csr_matrix(train_vectors)\ntest_vectors = csr_matrix(test_vectors, shape = (test_vectors.shape[0], train_vectors.shape[1])) \n# creates a sparse matrix with the given shape\ntest_vectors","63c7fd04":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\n\n\nclf = MultinomialNB()\nclf.fit(train_vectors, y_train)\n\ny_pred = clf.predict(test_vectors)\nprint(\"Accuracy:\", accuracy_score(y_pred, y_test))","555c2ffc":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(solver='liblinear', penalty='l1', C=100)\nclf.fit(train_vectors, y_train)\n\ny_pred = clf.predict(test_vectors)\nprint(\"Accuracy:\", accuracy_score(y_pred, y_test))","cfe11678":"all_news_2 = all_news.copy()\nall_news_2.sample()","880d15f3":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(all_news_2['news_text']\n                                                    ,all_news_2['credibility'], test_size=0.30, random_state=1)","cb6bfe87":"import tensorflow \nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train) \n# updates internal vocabulary with words in train set\n# each word is represented with an integer based on the frequency of the word in the entire train data\n# words with a higher frequency gets lower integer values\n\n\nsequences = tokenizer.texts_to_sequences(X_train)\n# creates a sequence of integers that represents each word in each row of train data\n\n\nword_index = tokenizer.word_index \n# creates a dictionary of unique words and their integer values \n\nvocab_size = len(word_index)\nprint('Training vocabulary size: ', vocab_size)\n\ntest_tokens = Tokenizer()\ntest_tokens.fit_on_texts(X_test)\ntest_sequences = test_tokens.texts_to_sequences(X_test)\ntest_word_index = test_tokens.word_index\ntest_vocab_size = len(test_word_index)\nprint('Testing vocabulary size: ', test_vocab_size )","8312d3e3":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nX_train = pad_sequences(sequences, padding = 'post')\nX_test = pad_sequences(test_sequences, padding = 'post')","c92dbac1":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n\nembedding_dim=200\n\nmodel = Sequential([\n  Embedding(vocab_size + 1, embedding_dim, name=\"embedding\"),\n  GlobalAveragePooling1D(),\n  Dense(16, activation='relu'),\n  Dense(1)\n])\n\nmodel.summary()","d84cb634":"model.compile(optimizer='adam',\n              loss=tensorflow.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])","dc8d5842":"history = model.fit(\n    X_train,\n    y_train,\n    validation_data=(X_test, y_test),\n    epochs=15\n    )\n","895279d1":"import matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","7c5beefc":"glove_dir = '..\/input\/glove6b100dtxt\/glove.6B.100d.txt'\nembedding_dimension = 100 \n\nembeddings_index = {}\nf = open(glove_dir)\nprint('Loading GloVe from:', glove_dir,'...', end='')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    embeddings_index[word] = np.asarray(values[1:], dtype='float32')\nf.close()\nprint(\"Done.\\n Proceeding with Embedding Matrix...\", end=\"\")\n\n# for train data\nembedding_matrix = np.random.random((len(word_index) + 1, embedding_dimension))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\n\n# for test data\ntest_embedding_matrix = np.random.random((len(test_word_index) + 1, embedding_dimension))\nfor word, i in test_word_index.items():\n    test_embedding_vector = embeddings_index.get(word)\n    if test_embedding_vector is not None:\n        test_embedding_matrix[i] = test_embedding_vector\nprint(\" Completed!\")","2c1eaf4b":"\nmodel = Sequential([\n  Embedding(vocab_size + 1, embedding_dimension, weights = [embedding_matrix], \n            name=\"embedding\"),\n  GlobalAveragePooling1D(),\n  Dense(32, activation='relu'),\n  Dense(16, activation='relu'),\n  Dense(1)\n])\n\nmodel.summary()\n","ba7ebc18":"model.compile(optimizer='adam',\n              loss=tensorflow.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])","f57a0ec7":"history_glove = model.fit(X_train,\n    y_train,\n    validation_data=(X_test, y_test),\n    epochs=15)","b0d3cc9d":"import matplotlib.pyplot as plt\n\nplt.plot(history_glove.history['accuracy'])\nplt.plot(history_glove.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","c7b8d451":"### Using Pre-trained Word Embeddings: GloVe","3b20211f":"From the above, it is clear that the dataset is balanced for both fake and real news","eefaac8d":"From the plot below, it seems there is some correlation between date a news article was created and its credibility. There was a sharp rise in fake news in later years, while real news dropped marginally.","07f2e4bf":"# Feature Extraction and Model Training","8b783665":"### Checking for relationship between news title, news text and news credibility","8934a8c0":"## Data Exploration","b02b1c87":"## Checking for Relationship between features(subject, date, title) and labels(credibility)","a5039887":"### Checking for relationship between news subject and news credibility","69c454ed":"### Checking for relationship between news date and news credibility","9b0325bf":"Similar common words in both fake news and real news titles include: Trump, Obama, etc. But there are words like White House, US, North Korea, Russia, that are very common in real news titles but are not so common in fake news titles. On the other hand, there are words like Video, tweet, hillary, watch, gop, that are common in fake news titles, but are not so common in real news titles. This shows that there is some distinguishing feature between most real and fake news titles, and including titles in our analysis can add some information to our model","6fbc0615":"* From the plot above, it is clear that real news are only centered around politicNews and worldnews subject areas, while fake news are centered around the other subject areas.\n* This indicates that the subject area can help determine if news is fake or real","2408223f":"## Using Word Embeddings","c9030eff":"# Fake News Detection - Embeddings + Neural Networks: Content\n\n1. Initial Data Cleaning and Exploration\n    * Checking for and removing duplicate news\n    * Deciding which features to use for analysis by checking for relationship between features and         labels.\n    \n    \n2. Data Preprocessing\n    * Removing punctions and unneeded characters from news text.\n    * Removing stop words\n    * tokenization\n    * stemmatization\n\n\n3. Feature Extraction and Model Training\n    * Using TF-IDF and basic classification algorithms(Naive Bayes and Logistic Regression)\n    * Using Word embeddings from scracth + neural networks\n    * Using pre-trained word embeddings(GloVe) + neural networks","11bfd570":"### Checking for relationship between news text and credibility","ab658705":"The newstext column contains characters like brackets, @symbols, links, and a lot of other characters or texts that might not add much information to our model, so have to clean and preprocess the data to remove such characters before we fit the text to our model","3e94c7c0":"## Using TF-IDF","19d67c51":"### Creating Word Embedding from Scratch","c6ef7fc4":"Although there are similar common words in both real news text and real news titles, there are still some distinguishing common words like people, featured image, percent, wednesday, thursday, tuesday, US, one, etc. This shows that the text of a news article is also a determinate factor in its credibility.","5dbd30ae":"* there's a mismatch in test vector shape and train vector shape\n* as a result, we need to reshape test vectors\n","27c50cd7":"# Data Preprocessing","54ecac8a":"The Embedding layer that maps from integer indices (which stand for specific words) to dense vectors (their embeddings). The dimensionality (or width) of the embedding is a parameter you can experiment with to see what works well for your problem.","f105259a":"Note: In previous versions of this notebook, I vectorized the data before I did the train_test split and I got an accuracy of about 95%, but I received feedback that doing so before splitting the data causes information from the training set to mix with that of the test set. After doing the train test split before vectorizing, I the accuracy has reduced drastically for each of the regression models used. This confirms that the initial 95% accuracy was not really representative of the actual model performance.","18e074c8":"# Next Step\n\nAlthough the accuracies were high in both models, the validation accuracies did not improve much and were lower which shows both models were overfitting. The next step would be to improve the architecture of the neural networks to see if the validation accuracies improve.","a24638f9":"After dropping duplicates, the count of fake news has reduced, meaning most of the duplicate text were from fake news. However, the dataset set is still balanced"}}