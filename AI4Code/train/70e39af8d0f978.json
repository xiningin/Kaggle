{"cell_type":{"e46542f8":"code","ebd27b4c":"code","0a2bc728":"code","535a5e38":"code","5ac252d1":"code","74a8cee7":"code","b9f0ff34":"code","ab04bd9f":"code","9a614d18":"code","06241d43":"code","dfba44a5":"code","c74b2032":"code","9c3cc778":"code","931b6e28":"code","f1d73cff":"code","3f2e0583":"code","46dc49e7":"code","d8a38246":"code","7e5a1c3f":"code","c55eba95":"code","3c10fe07":"code","c23c001c":"code","2e38cca2":"code","f99c6dea":"code","d32ef1ed":"code","57f60803":"code","4c6c9c0c":"code","ab971dcd":"code","a9b65aa1":"code","830a882d":"code","7efb5cb4":"code","bf47e753":"code","6717d8bd":"code","5d7a1e24":"code","82f71463":"code","31f9abf9":"code","ad68828e":"code","5d47317b":"code","7a9bfa01":"code","3f560449":"markdown","26aabdfd":"markdown","4fd8a086":"markdown","b3439b94":"markdown","d0960ac7":"markdown","c5bd79f5":"markdown","ace6f0c1":"markdown","ccbe8eb1":"markdown"},"source":{"e46542f8":"from IPython.display import Image\nprint ('Model Architecture')\nImage(\"..\/input\/outputs\/Model_Sbert.JPG\")\n","ebd27b4c":"print ('Product Recommendations')\nImage(\"..\/input\/outputs\/Product_Recommendations.PNG\")","0a2bc728":"print ('Network Recommendations 1')\nImage(\"..\/input\/outputs\/Network_Recommendation1.JPG\")","535a5e38":"print ('Network Recommendations 2')\nImage(\"..\/input\/outputs\/Network_Recommedation2.JPG\")","5ac252d1":"!pip install -U sentence-transformers","74a8cee7":"import pandas as pd\nimport os \nimport ast\nimport sentence_transformers  #### This is the package which we will use for encoding recipes using pretrained embedding\nimport matplotlib.pyplot as plt \nimport networkx as nx #### Network x will be used to create graph based algorithms\nimport pickle ### We will use pickleto save files for later access\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity ### Cosine Similary\nfrom scipy import sparse ### Sparse Matrix\nmodel = SentenceTransformer('bert-large-nli-stsb-mean-tokens') ### We will use this senetnce encodings","b9f0ff34":"\n### Read the interactions csv as required \ninteractions = pd.read_csv('..\/input\/food-com-recipes-and-user-interactions\/RAW_interactions.csv')\n\nprint (\"Lets look at the basic stats about the data\")\nprint (\"The shape of the data is\")\nprint (interactions.shape)\nprint (\" The columns in the data are as follows\")\nprint (interactions.columns)\nprint (\" The first few columns are \")\nprint (interactions.head(5))","ab04bd9f":"interactions['rating'].value_counts()","9a614d18":"### Interestingly most of the recipes are rated at highest rating. \ninteractions.groupby('recipe_id')['rating'].mean().reset_index().rating.plot(kind ='hist',title='Histograms of Avg Rating Recipe')\nplt.xlabel(\"Average Ratings\")\nplt.ylabel(\"Number of recipes\")\nplt.show()\n\n","06241d43":"### We will restrict our analysis only to those recipes which has been reviewed by more than 2 people\n### Analysis have shown that most recipes are only added but never seen\n### Print for the poc purposes we will restrict\ng = {'rating' : ['mean'],'user_id' : ['nunique']}\nint_summary = interactions.groupby(['recipe_id']).agg(g).reset_index()\n### Its gives a muti index output convert it to single index by cobining bothe level\nind = pd.Index([e[0] + \"_\" +e[1] for e in int_summary.columns.tolist()])\n### Assign the column names \nint_summary.columns = ind\nint_summary.columns = ['recipe_id', 'rating_mean', 'user_id_nunique']\n### We will keep only those recipes in considerstaion which have been reviewed by more than 2 people\nint_summary_94k = int_summary[ (int_summary['user_id_nunique'] > 2)]","dfba44a5":"recipes = pd.read_csv('..\/input\/food-com-recipes-and-user-interactions\/RAW_recipes.csv')\nprint (recipes.columns)","c74b2032":"filter_recipe = pd.merge(recipes,int_summary_94k,right_on = ['recipe_id'],left_on = ['id'],how = 'inner')","9c3cc778":"filter_recipe.head(10)","931b6e28":"### The steps recipe is in list. We will combine list into one string\nfilter_recipe['dish_recipe'] = filter_recipe['steps'].apply(lambda x : \" \".join(ast.literal_eval(x)))","f1d73cff":"#### We will encode the recipes and store it in pickle file\n#encodings_recipe= model.encode(filter_recipe['dish_recipe'])\n# Pickle embedding as it has a run time\n#pickle.dump(encodings_recipe,open(\"recipe_embedding.pickle\",'wb'))","3f2e0583":"#### Load the pickle files of encoding and create a dataframe out of it\nencodings_recipe_df = pickle.load(open(\"recipe_embedding.pickle\",'rb'))\nprint (\"Encoding are loaded\")\ndata_encoding = pd.DataFrame(encodings_recipe_df)","46dc49e7":"### As we don't have enough memory create cosine similary for only 10000 recipes\nencoding_sparse = sparse.csr_matrix(encodings_recipe_df[0:5000])","d8a38246":"# similarities = cosine_similarity(encoding_sparse)\n# print('pairwise dense output:\\n {}\\n'.format(similarities))\n\n#also can output sparse matrices\nimport datetime\ntime = datetime.datetime.now()\nprint (time)\nsimilarities_sparse = cosine_similarity(encoding_sparse)\n# print('pairwise sparse output:\\n {}\\n'.format(similarities_sparse))\nprint (\"Time taken is :\",)\nprint (datetime.datetime.now()-time)","7e5a1c3f":"### Convert the pickle file to datafarme and dump\ndf1 = pd.DataFrame(similarities_sparse)\npickle.dump(df1,open('similarities_sparse.pickle','wb'))","c55eba95":"df1 = pickle.load(open('similarities_sparse.pickle','rb'))\ndata_similarity = df1.unstack().reset_index() ","3c10fe07":"data_similarity.columns = ['recipe1','recipe2','cosine_similarity']","c23c001c":"# Remove similarity = 1, as it implies same recipes\ndata_similarity = data_similarity[data_similarity['cosine_similarity']<0.9999]\n\n","2e38cca2":"data_similarity.shape","f99c6dea":"recipe_dict = {}\nfor j,i in enumerate(filter_recipe['name']):\n    recipe_dict[j] = i\nprint (\"Dictionary is created :\")\n    ","d32ef1ed":"data_similarity['recipe1_name'] = data_similarity['recipe1'].map(recipe_dict)\ndata_similarity['recipe2_name'] = data_similarity['recipe2'].map(recipe_dict)","57f60803":"data_similarity.head(5)","4c6c9c0c":"data_similarity['similarity_rank'] = data_similarity.groupby(['recipe1'])['cosine_similarity'].rank(\"dense\", ascending=False)","ab971dcd":"data_similarity = data_similarity[data_similarity['similarity_rank'] <= 5].reset_index()","a9b65aa1":"data_similarity.to_csv('similarity.csv')","830a882d":"def find_similar_dishes(list_names):\n    for i in list_names:\n        dummy_data =  data_similarity[data_similarity['recipe1_name'] == i]\n        print (\"As you liked dish :\",i)\n        print (\"You must try following 4 dishes with slight variations\")\n        dummy_data.sort_values(inplace = True,by =['similarity_rank']) \n        for j,i in enumerate(dummy_data['recipe2_name'].unique()):\n            print (\"             \", i)\n            if j == 3:\n                break","7efb5cb4":"import warnings\nwarnings.filterwarnings('ignore')\nfind_similar_dishes(['aaloo mattar   indian style peas and potatoes','avocado ranch burgers with smoked cheddar',\n                    'apricot pancakes','1 2 3 rice and chili burritos'\n])\n\n","bf47e753":"G = nx.from_pandas_edgelist(data_similarity,'recipe1_name','recipe2_name')","6717d8bd":"data_sample = data_similarity\nimport matplotlib.pyplot as plt\nimport networkx as nx\nplt.figure(figsize=(250,250))\nplt.rcParams['axes.facecolor'] ='white'\nG = nx.Graph()\nfor i in range(0,5000):\n    G.add_edge(data_sample['recipe1_name'][i], data_sample['recipe2_name'][i], weight=data_sample['cosine_similarity'][i])\n\nelarge = [(u, v) for (u, v, d) in G.edges(data=True) if d['weight'] > 0.8]\n\n\npos = nx.spring_layout(G)  # positions for all nodes\n\n# nodes\nnx.draw_networkx_nodes(G, pos, node_size=50)\n\n# edges\nnx.draw_networkx_edges(G, pos, edgelist=elarge,\n                       width=5)\n\n\n# labels\nnx.draw_networkx_labels(G, pos, font_size=10, font_family='sans-serif')\nplt.savefig(\"Network.pdf\", bbox_inches='tight')\nplt.axis('off')\nplt.show()\n\n","5d7a1e24":"import networkx.algorithms.community as nxcom\ncommunities = sorted(nxcom.greedy_modularity_communities(G), key=len, reverse=True)\n    # Count the communities\nprint(f\"The karate club has {len(communities)} communities.\")","82f71463":"for i in communities:\n    print (i)","31f9abf9":"pos = nx.spring_layout(G, k=0.1)\nplt.rcParams.update({'figure.figsize': (15, 10)})\nnx.draw_networkx(\n    G, \n    pos=pos, \n    node_size=0, \n    edge_color=\"#444444\", \n    alpha=0.05, \n    with_labels=False)","ad68828e":"communities = sorted(nxcom.greedy_modularity_communities(G), key=len, reverse=True)\nlen(communities)","5d47317b":"def set_node_community(G, communities):\n    '''Add community to node attributes'''\n    for c, v_c in enumerate(communities):\n        for v in v_c:\n            # Add 1 to save 0 for external edges\n            G.nodes[v]['community'] = c + 1\n\ndef set_edge_community(G):\n    '''Find internal edges and add their community to their attributes'''\n    for v, w, in G.edges:\n        if G.nodes[v]['community'] == G.nodes[w]['community']:\n            # Internal edge, mark with community\n            G.edges[v, w]['community'] = G.nodes[v]['community']\n        else:\n            # External edge, mark as 0\n            G.edges[v, w]['community'] = 0\n\ndef get_color(i, r_off=1, g_off=1, b_off=1):\n    '''Assign a color to a vertex.'''\n    r0, g0, b0 = 0, 0, 0\n    n = 16\n    low, high = 0.1, 0.9\n    span = high - low\n    r = low + span * (((i + r_off) * 3) % n) \/ (n - 1)\n    g = low + span * (((i + g_off) * 5) % n) \/ (n - 1)\n    b = low + span * (((i + b_off) * 7) % n) \/ (n - 1)\n    return (r, g, b)          ","7a9bfa01":"plt.rcParams.update(plt.rcParamsDefault)\nplt.rcParams.update({'figure.figsize': (15, 10)})\n\n\n# Set node and edge communities\nset_node_community(G, communities)\nset_edge_community(G)\n\n# Set community color for internal edges\nexternal = [(v, w) for v, w in G.edges if G.edges[v, w]['community'] == 0]\ninternal = [(v, w) for v, w in G.edges if G.edges[v, w]['community'] > 0]\ninternal_color = [\"black\" for e in internal]\nnode_color = [get_color(G.nodes[v]['community']) for v in G.nodes]\n# external edges\nnx.draw_networkx(\n    G, \n    pos=pos, \n    node_size=0, \n    edgelist=external, \n    edge_color=\"silver\",\n    node_color=node_color,\n    alpha=0.2, \n    with_labels=False)\n# internal edges\nnx.draw_networkx(\n    G, pos=pos, \n\n    edgelist=internal, \n    edge_color=internal_color,\n    node_color=node_color,\n    alpha=0.5, \n    with_labels=False)","3f560449":"## Look at the data","26aabdfd":"# Create a function to find similar dish given a recipe name","4fd8a086":"### Read the recipes data and keep only data for recipes which are reviewed by more than 2 people","b3439b94":"#### Import all the packages required for creating this algorithms ","d0960ac7":"### Do the inner join with subset data","c5bd79f5":"# Data Exploration and Analysis","ace6f0c1":"# As we have restricted to 5000 recipes results are Suboptimal in production this should use all reciped i.e. 94","ccbe8eb1":"### Recipe Recommendation - Using NLP (In absence of any customer data)\n\nThe objective of this work is to use free text of recipes from food-com to create a NLP engine which can identify similar recipes based on Semantic similarity of recipe text. Once we have encodes recipes we can apply varipus algorithm and network methods to create a recommendation engine\n\nThe USP of this work is that we are not using any Customer preference or transation data to create this recommendation engine\n\n### About Data : \nThis dataset consists of 180K+ recipes and 700K+ recipe reviews covering 18 years of user interactions and uploads on Food.com (formerly GeniusKitchen). used in the following paper:\n\nGenerating Personalized Recipes from Historical User Preferences\n\nBodhisattwa Prasad Majumder*, Shuyang Li*, Jianmo Ni, Julian McAuley\n\nEMNLP, 2019\n\nhttps:\/\/www.aclweb.org\/anthology\/D19-1613\/\n\nStatistics About Data :\n1. Number of Distinct Recipes : ~231K\n2. Number of Distinct Users : ~119K\n\nAfter looking into data we observed following :\n\n1.  Raw Interactions data shows that more than 95% of ratings are > 4 which clearly indicates some Bias.Further analysis shows that most of ratings are from person posting the recipes. \n\n2.  For our POC, we have filtered out only those recipes which have more than 2 reviewers. This leaves us with only 94k recipes. (This is just POC thats why otherwise it makes more sense to keep al recipes which can help us discover New Unknown Recipes)\n\n### Data Preparation :\n1.Sumamrise the Raw interaction data at recipes level and count unique number of users who have reviewed it\n2.The Raw Recipes data we have has recice text as list of steps, conbine by recipes id to form a single string\n3.Join tables from step 1 & step at recipe level(recipe level data) and keep only those recipes where number of reviewer are greater than 2\n4. Now the data is ready we will start creating Recipe Embedding\n\n### Embedding Creation:\n\nFrom my previous experience, in Quora Questions Pairs etc word level embedding does not explain complete sentences properly. Also, i have seen from my previous experience though BERT cls embedding alsd does not give state of the art results.\n\nSo in our current work we will use pretrained Siamese Bert Model, which has been fine tuned for differentiate between similar sentences semantically. It builds over BERT model by taking average of all the word vectors and finetuning them to fit the training task. The model architecture is defined below in cell1:\n\n\nAs we did not have any taggings we have used pretrained models. But if we have good amount of tag data we can use \n\n\n### Algorithm 1 : Given a recipe return top 4 similar recipes\nIn this function we make use of cosine similarity between input recipe  and all other recipes vector and return top 4 recipes with highest cosine similarity. It is interesting to note that with this we get similar recipes without any transaction data\n\nAlso, the similarity varies at various level. if you look at examples below\n\nExample 1 : You get prodcts similar based on ingredients but also based on type i.e Desserts\n\nExample 2 : It returns recipes which are similar because they follows same steps\n\nExample 3 : It returns you burgers with differenr preprations and ingredients\n\nExample 4 : It return products which have similar main ingredients i.e. Potato but different prep strategies\n\nExample 5 : For Taco, you get all mexican recipes because maybe they have similar preparation strategy\n\nThe results are as follows in cell1  :\n\n\n \nThe problem with this algorithm is we cannot applying commutative properties (If A is similar to B and B is similar to C) to this is quite expensive operation. We will make use of Network to solve this issue\n\n### Algorithm 2 : Create a complete network using cosine similarity between Recipes as edges and Recipes as Nodes. Currently we have restriced edges to top 5 similar recipes.The network help us mine certain important relationships.\n\nSome examples are as follows :If someone likes 3 Hour Old Fashioned Pot Roast We may recommend him Apple Bacon Thanks Giving Dressing or Flaky Suasage Foldovers even when they don't have direct relationship between them\n\nSee cell below\n\nExample 2 : If some likes Chicken Wings or Artichoke chickens you amy recommend him 5 star chicken sun dried tomato afred or angry chicken though they don't have direct link \n\nSee cell below\n\nYou can explore various variations of graphs exported in pdf in outputs folder\n\nThough the results makes a lot of sense, if i had more time i could have tried follwoing to further improve\n\n1. Use other information i.e Tags Data and ingredient data which calculating similarity of products\n\n2. Give user a functionaliy to select recipes based on ingredient, preparation or both\n\n3. Instead of using pretrained embedding we van train our own embedding if we had similarity data\n\nRefrences :\n\n1. https:\/\/arxiv.org\/abs\/1908.10084 : I have used only this paper otherwise complete project was just my idea"}}