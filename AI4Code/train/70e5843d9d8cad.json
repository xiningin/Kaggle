{"cell_type":{"22d79b81":"code","d7267e41":"code","ec259447":"code","aa80fbf4":"code","6395291a":"code","61ac94f0":"code","bb1c5289":"code","24a1c80d":"code","1e04092a":"code","f19e4ee5":"code","309daa57":"code","0f0bb664":"code","2b3f0c13":"code","4c64e167":"code","b824341f":"code","1a2a941f":"code","9d8d3b04":"code","d0d9ae74":"code","053ef03e":"code","a6ad387f":"code","1e3f1ff5":"code","fa3b6dd2":"code","2d31381a":"code","1a2e6d5b":"code","68bba6e5":"code","090dba5e":"code","972b473d":"code","595b6baf":"code","575dc99c":"markdown","b5139665":"markdown","e8d62caf":"markdown","57cccc08":"markdown","8bbc784d":"markdown","c9a321d0":"markdown","1497fadd":"markdown"},"source":{"22d79b81":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder","d7267e41":"data=pd.read_csv(\"..\/input\/3dprinter\/data.csv\",sep=\",\")","ec259447":"data","aa80fbf4":"data.shape","6395291a":"# The data is small, but it is beautiful.\ndata.isnull().sum()","61ac94f0":"# We note that the data we have has a different type of data than the digital type, so we will convert it.\ndata.info()","bb1c5289":"list(data.columns)","24a1c80d":"#Now we will convert the two data types, to the numbers data type.\ndata_2=data.copy()\nobj=LabelEncoder()\ndata_2[\"infill_pattern\"] = obj.fit_transform(data[\"infill_pattern\"])\ndata_2[\"material\"] = obj.fit_transform(data[\"material\"])\n# And then we notice that change \/ transformation.","1e04092a":"data_2.head()","f19e4ee5":"# Now it is our turn to extract the rewards and unique characteristics of each column.\nfor i in data.columns:\n  print(i, data[i].unique())","309daa57":"# Here is another formula that we can extract the advantages for each column.\n#for column in data.columns:\n #   print(\"{} : {}\".format(column,data[column].unique()))","0f0bb664":"#Note that the number 3 in the code indicates the number of digits after the decimal point for each cell.\ncorrelation = data_2.corr()\ncorrelation.style.background_gradient(cmap='coolwarm').set_precision(3)","2b3f0c13":"# Now we will separate the goal from the rest of the features.\ntarget_data= data_2[\"material\"].values\nfeature_data= data_2.drop([\"material\",\"print_speed\",\"infill_pattern\",\"bed_temperature\",\"layer_height\"],axis=1)","4c64e167":"target_data","b824341f":"feature_data","1a2a941f":"# Now I will do the process of smoothing the data.\n#Import Libraries\nfrom sklearn.preprocessing import StandardScaler\n#Standard Scaler for Data\nscaler = StandardScaler(copy=True, with_mean=True, with_std=True)\nX = scaler.fit_transform(feature_data)\n","9d8d3b04":"# Now I will do the partitioning of the data.\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X,target_data,test_size = 0.1,random_state=1)","d0d9ae74":"print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)","053ef03e":"# This algorithm is for prediction.\n#Import Libraries\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import confusion_matrix\nDecisionTreeRegressorModel=DecisionTreeRegressor(criterion='mse', splitter='best', max_depth=3,min_samples_split=2,\n                                   min_samples_leaf=1,min_weight_fraction_leaf=0.0, max_features=None,\n                                   random_state=None, max_leaf_nodes=5\n                                  )","a6ad387f":"DecisionTreeRegressorModel.fit(x_train, y_train)","1e3f1ff5":"#Calculating Details\nprint('DecisionTreeRegressor Train Score is : ' , DecisionTreeRegressorModel.score(x_train, y_train))\nprint('DecisionTreeRegressor Test Score is : ' , DecisionTreeRegressorModel.score(x_test, y_test))","fa3b6dd2":"#Calculating Prediction\ny_pred = DecisionTreeRegressorModel.predict(x_test)\nprint('Predicted Value for DecisionTreeRegressorModel is : ' , y_pred[:10])\n#Here we are working on printing the data that we have put to the test\nprint(\"test values :\" , y_test[:10] )","2d31381a":"# This algorithm is for classification.\n#Import Libraries\nfrom sklearn.tree import DecisionTreeClassifier\nDecisionTreeClassifierModel =DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=3,min_samples_split=2,\n                                    min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=None,\n                                    random_state=0, max_leaf_nodes=5)","1a2e6d5b":"DecisionTreeClassifierModel.fit(x_train, y_train)","68bba6e5":"#Calculating Details\nprint('DecisionTreeClassifierModel Train Score is : ' , DecisionTreeClassifierModel.score(x_train, y_train))\nprint('DecisionTreeClassifierModel Test Score is : ' , DecisionTreeClassifierModel.score(x_test, y_test))","090dba5e":"print('DecisionTreeClassifierModel Classes are : ' , DecisionTreeClassifierModel.classes_)\nprint('DecisionTreeClassifierModel feature importances are : ' , DecisionTreeClassifierModel.feature_importances_)","972b473d":"#Calculating Prediction\ny_pred = DecisionTreeClassifierModel.predict(x_test)\n# Now we calculate the probability of choosing the output for any division\ny_pred_prob = DecisionTreeClassifierModel.predict_proba(x_test)\nprint('Predicted Value for DecisionTreeClassifierModel is : ' , y_pred[:10])\n# These are the values that we categorized.\nprint(\"test values :\" ,y_test[:10] )\nprint('Prediction Probabilities Value for DecisionTreeClassifierModel is : ' , y_pred_prob[:10])\n","595b6baf":"#Calculating Confusion Matrix\nCM = confusion_matrix(y_test, y_pred)\nprint('Confusion Matrix is : \\n', CM)\n\n# drawing confusion matrix\nsns.heatmap(CM, center = True)\nplt.show()","575dc99c":"# Now I will build our own model.","b5139665":"Now the use of neural networks in this process is a luxury to some extent.\nTherefore, I will not use neural networks, although I can do so by reducing the size of the network and reducing the number of layers in it, and it will also be of the type of hollow networks.","e8d62caf":"Now we're somewhat done with that software page.\nThank you very much for\nhttps:\/\/www.kaggle.com\/parag46\/kernel8c0883347d\n# And you too, thank you very much.","57cccc08":"We note that the amount of data used in the test is very small because \n\n---\n\nthe data is very few, of course.","8bbc784d":"# 3d-printer-data-classification\n\nIn that software page, we applied some algorithms from machine learning to the data of 3D printers (classification) and we got results in the test data that have reached completion..","c9a321d0":"We have very little data, so it is not likely that we will use neural networks in this process, but we will try, my friend.","1497fadd":"In the end, I obtained the accuracy of the test results, which reached the full percentage."}}