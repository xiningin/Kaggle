{"cell_type":{"c278577a":"code","8b3de0c7":"code","5c59d17b":"code","47108115":"code","8a08176b":"code","aa233e96":"code","250eedad":"code","d11e9f97":"code","ab3f597c":"code","3f8bc333":"code","3ba57196":"code","abc76181":"code","1676b78c":"code","47dd4f3e":"code","7c4f1b4d":"code","fb82f86e":"code","d988745a":"code","75752140":"code","5e5f8ee8":"code","18fb0b9b":"code","3e23ec73":"code","e84ecff3":"code","79500969":"code","6de2b029":"code","d5575b51":"code","9597c843":"code","b351545f":"code","ce465183":"code","b2b00cf1":"code","7a05f50d":"code","5e78f686":"code","bbe73a94":"code","be1dabbc":"code","f632627f":"code","8cce84cf":"code","9ad6162d":"code","bfefd199":"code","d1ef9e8a":"code","09a23f4c":"code","364f7d4a":"code","50f4f4dc":"code","f525ab24":"code","4014fa24":"code","cc64aec5":"code","64a93355":"code","41d7f36e":"code","2944aa37":"code","3000575a":"code","71092cb3":"code","2a06e9a8":"code","307de44a":"code","ba9b7763":"code","a070c2ec":"code","6aeab2cc":"code","55ed8539":"code","bf2dbc64":"code","189b664d":"code","4b4bb7f7":"code","8dfe0b04":"code","93fc979d":"code","f9bc62d7":"code","03809d63":"code","d3c8d234":"code","637440ff":"code","8761d043":"code","2b40b028":"code","c46f5742":"code","51f20ce7":"code","e4b1a85d":"code","a165f11b":"code","458fb575":"code","162b3f98":"code","2067e850":"code","04c4e10d":"code","45258817":"code","e7bea940":"code","c9a25cdd":"code","529a5f4e":"code","8ce72f7e":"code","0ac455ca":"code","47990a13":"code","b92eba77":"code","a118a2da":"code","89077321":"code","778240ad":"code","c3bd5717":"code","c960d861":"code","f52840f2":"code","328ca5c5":"code","025f48ac":"code","5ed24f9a":"code","3f1efcbc":"code","39b86ae9":"code","80274463":"code","240518c8":"code","9f90c49c":"code","4a8d4380":"code","f73af2ac":"code","2fc14b61":"code","e91c0862":"code","46adde1d":"code","677804ac":"code","22b4bab4":"code","956f2e6a":"code","d58be327":"code","023f148b":"code","36cd2391":"code","99b4098d":"code","ae12c649":"code","8ef42975":"code","49172378":"code","4e8aaca6":"code","218a0d6d":"code","50384831":"code","eae7790b":"code","e1dd3076":"code","8dad2833":"code","aa23e9f7":"code","066151f9":"code","5ddef73f":"code","8c99ce2c":"code","f476d004":"code","856e9c19":"code","9aa86f09":"code","d100c3aa":"code","6cbe93f0":"code","1cc7e5b6":"code","38dea44f":"code","3ea08c34":"code","9aece37c":"code","30cada69":"code","1dd0935c":"code","6c632f95":"code","0bc36896":"code","a1914aef":"code","037923a0":"code","9e595375":"code","b72fc168":"code","737e2605":"code","5fef82fc":"code","4b0e1ad0":"markdown","dbc1f2ea":"markdown","9b4f3c05":"markdown","13802644":"markdown","238da269":"markdown","ece6b252":"markdown","1787c7fd":"markdown","822e9968":"markdown"},"source":{"c278577a":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport ast\nfrom wordcloud import WordCloud","8b3de0c7":"train = pd.read_csv('..\/input\/tmdb-box-office-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/tmdb-box-office-prediction\/test.csv')","5c59d17b":"train.info()","47108115":"test[\"revenue\"] = np.nan\ndata = train.append(test,ignore_index=True)","8a08176b":"data.describe()","aa233e96":"# Lets see the Unique Value Distribution of Object features\nObject_col = data.select_dtypes('object').columns\nAnalysis = []\n\nfor columns in Object_col:\n    Unique = data[columns].nunique()\n    Analysis.append([str(columns),Unique])\n\nAnalysis = np.array(Analysis)\nplt.figure(figsize = (10,10))\nplt.bar(x = Analysis[:,0],height = Analysis[:,1],width = 0.6,color = \"red\",edgecolor = \"black\")\nplt.gca().invert_xaxis()\nplt.xlabel(\"Features\")\nplt.ylabel(\"Unique Count\")\nplt.title(\"Distribution of Object Column\")\nplt.xticks(rotation = 50)\nprint(\"Total Size of dataset: {}\".format(len(data)))\n","250eedad":"# Lets Deal with Every category one by one\nobject_columns = data.select_dtypes('object').columns\nprint(object_columns)","d11e9f97":"for idx,instance in enumerate(data.loc[:3,'belongs_to_collection']):\n    print(idx,instance)","ab3f597c":"# Lets Extract out the names and make it as feature\nValue = []\nfor instance in data.loc[:,'belongs_to_collection']:\n    if isinstance(instance,str):  # Because nan in float instance and rest is string instance\n        Value.append(ast.literal_eval(instance)[0]['name'])  # Extracting out the name\n    else:\n        Value.append(np.nan)","3f8bc333":"data['belongs_to_collection'] = Value","3ba57196":"data['belongs_to_collection'].head(5)","abc76181":"x, y = np.ogrid[:1000, :1000]\n\nmask = (x - 500) ** 2 + (y - 500) ** 2 > 400 ** 2\nmask = 255 * mask.astype(int)\nCount = data['belongs_to_collection'].value_counts()\nplt.figure(figsize = (15,15))\nwordcloud = WordCloud(background_color=\"white\",width=1920, height=1080,mask = mask).generate_from_frequencies(Count)\nplt.imshow(wordcloud,interpolation = 'bilinear')\nplt.axis('off')\nplt.show()","1676b78c":"for idx,instance in enumerate(data.loc[:10,'genres']):\n    print(idx,instance)","47dd4f3e":"# Lets Extract out the name first\nValue = []\nCount = []\nfor instance in data.loc[:,'genres']:\n    if isinstance(instance,str):\n        X = ast.literal_eval(instance)\n        Count.append(len(X))   # Count Keep tracks of total Genres of One Movie\n        for i in X:\n            Value.append(str(i['name']))\n    else:\n        Count.append(0)","7c4f1b4d":"x, y = np.ogrid[:1000, :1000]\n\nmask = (x - 500) ** 2 + (y - 500) ** 2 > 400 ** 2\nmask = 255 * mask.astype(int)\n\nTotal_Genre = pd.DataFrame(Value).value_counts()\nTotal_Genre.index = ['Drama','Comedy','Thriller','Action','Romance','Adventure','Crime','Science Fiction',\n                     'Horror','Family','Fantasy','Mystery','Animation','History','Music','War','Documentary',\n                     'Western','Foreign','TV Movie']\n\nplt.figure(figsize = (15,15))\nplt.subplot(121)\nwordcloud = WordCloud(background_color=\"white\",mask=mask,width=2000, height=1180).generate_from_frequencies(Total_Genre)\nplt.imshow(wordcloud,interpolation = 'bilinear')\nplt.axis('off')\n\nplt.subplot(122)\nTotal_Genre.plot(kind = 'bar',color = 'blue',edgecolor = 'black')\nplt.show()\n\n# Drama And Comedy are the most Repeating ones","fb82f86e":"data['genres'] = Count","d988745a":"for idx,instance in enumerate(data.loc[:10,'homepage']):\n    print(idx,instance)","75752140":"data['homepage'].value_counts()   # This we can Remove. Because its given in dataset description that\n# even though two different movies have same homepage they must be considered as two different movies.","5e5f8ee8":"data.drop(['homepage'],axis = 1,inplace = True)","18fb0b9b":"for idx,instance in enumerate(data.loc[:10,'imdb_id']):\n    print(idx,instance)","3e23ec73":"data['imdb_id'].value_counts()       # All Unique Value We can Remove this attribute.\nprint(np.any(np.array(data['imdb_id'].value_counts()) == 1))","e84ecff3":"data.drop(['imdb_id'],axis = 1,inplace = True)","79500969":"for idx,instance in enumerate(data.loc[:5,'original_language']):\n    print(idx,instance)","6de2b029":"data['original_language'].value_counts().head(5)    # We can simply use Linear Encoding in here","d5575b51":"for idx,instance in enumerate(data.loc[:10,'original_title']):\n    print(idx,instance)","9597c843":"data['original_title'].value_counts()  # I can remove this same reason for homepage","b351545f":"data.drop(['original_title'],axis = 1,inplace = True)","ce465183":"for idx,instance in enumerate(data.loc[:1,'overview']):\n    print(idx,instance)","b2b00cf1":"data.drop(['overview'],axis = 1,inplace = True)","7a05f50d":"for idx,instance in enumerate(data.loc[:1,'poster_path']):\n    print(idx,instance)","5e78f686":"data['poster_path'].value_counts()      # All Unique Value We can Remove this attribute.\nprint(np.any(np.array(data['poster_path'].value_counts()) == 1))","bbe73a94":"data.drop(['poster_path'],axis = 1,inplace = True)","be1dabbc":"for idx,instance in enumerate(data.loc[:10,'production_companies']):\n    print(idx,instance)","f632627f":"# Lets Extract out the name first\nValue = []\nCount = []\nfor instance in data.loc[:,'production_companies']:\n    if isinstance(instance,str):\n        X = ast.literal_eval(instance)\n        Count.append(len(X))   # Count Keep tracks of total Production Companirs of One Movie\n        for i in X:\n            Value.append(str(i['name']))\n    else:\n        Count.append(0)","8cce84cf":"pd.DataFrame(Value).value_counts().index[:5] # We have to work on it and bring in right format","9ad6162d":"Correct_index = []\nfor index in pd.DataFrame(Value).value_counts().index:\n    Correct_index.append(index[0])\nprint(Correct_index[:5])","bfefd199":"x, y = np.ogrid[:1000, :1000]\n\nmask = (x - 500) ** 2 + (y - 500) ** 2 > 400 ** 2\nmask = 255 * mask.astype(int)\n\ncompanies = pd.DataFrame(Value).value_counts()\ncompanies.index = Correct_index\n\nplt.figure(figsize = (15,15))\nwordcloud = WordCloud(background_color=\"white\",mask=mask,width=2000, height=1180).generate_from_frequencies(companies)\nplt.imshow(wordcloud,interpolation = 'bilinear')\nplt.axis('off')\n\n# Most pictures are from Paramount Pictures,Warner Bors and Universal Pictures","d1ef9e8a":"data['production_companies'] = Count","09a23f4c":"for idx,instance in enumerate(data.loc[:10,'production_countries']):\n    print(idx,instance)","364f7d4a":"# Same treatment we can give it as above\n# Lets Extract out the name first\nValue = []\nCount = []\nfor instance in data.loc[:,'production_countries']:\n    if isinstance(instance,str):\n        X = ast.literal_eval(instance)\n        Count.append(len(X))   # Count Keep tracks of total Production Countries of One Movie\n        for i in X:\n            Value.append(str(i['name']))\n    else:\n        Count.append(0)","50f4f4dc":"Value = pd.DataFrame(Value).rename(columns = {0:\"Name\"})\ndf = Value.value_counts()","f525ab24":"df.loc[df > 50].plot(kind = \"bar\",color = \"blue\",edgecolor = \"black\",figsize = (20,5)) \nplt.xticks(rotation = 50)\nplt.xlabel(\"Production Countries\")\nplt.ylabel(\"Count of Total Movies\")\n# Plotting the the Countries who created more than 50 movies","4014fa24":"data['production_countries'] = Count","cc64aec5":"for idx,instance in enumerate(data.loc[:20,'release_date']):\n    print(idx,instance)","64a93355":"data['release_date'] = pd.to_datetime(data['release_date'],format=\"%m\/%d\/%y\")\n\n# format ==> https:\/\/strftime.org\/","41d7f36e":"data['release_date'].head(10)","2944aa37":"for idx,instance in enumerate(data.loc[:5,'spoken_languages']):\n    print(idx,instance)","3000575a":"# We can Do same treatment as Earlier\n# Lets Extract out the name first\nValue = []\nCount = []\nfor instance in data.loc[:,'spoken_languages']:\n    if isinstance(instance,str):\n        X = ast.literal_eval(instance)\n        Count.append(len(X))   # Count Keep tracks of total Languages of One Movie\n        for i in X:\n            Value.append(str(i['name']))\n    else:\n        Count.append(0)","71092cb3":"Value[:5]","2a06e9a8":"Correct_index = []\nfor index in pd.DataFrame(Value).value_counts().index:\n    Correct_index.append(index[0])\nprint(Correct_index[:5])","307de44a":"x, y = np.ogrid[:1000, :1000]\n\nmask = (x - 500) ** 2 + (y - 500) ** 2 > 400 ** 2\nmask = 255 * mask.astype(int)\n\nLang = pd.DataFrame(Value).value_counts()\nLang.index = Correct_index\n\nplt.figure(figsize = (7,7))\nwordcloud = WordCloud(background_color=\"white\",mask=mask,width=2000, height=1180).generate_from_frequencies(Lang)\nplt.imshow(wordcloud,interpolation = 'bilinear')\nplt.axis('off')\n\n# Most pictures are in English,Francais,Espanol usw","ba9b7763":"data['spoken_languages'] = Count","a070c2ec":"for idx,instance in enumerate(data.loc[:5,'status']):\n    print(idx,instance)         # We can simply do Label Encoding in it","6aeab2cc":"for idx,instance in enumerate(data.loc[:5,'tagline']):\n    print(idx,instance)","55ed8539":"data.drop(['tagline'],inplace = True,axis = 1)","bf2dbc64":"for idx,instance in enumerate(data.loc[:5,'title']):\n    print(idx,instance)","189b664d":"data['title'].value_counts() \n# We can remove it as I said earlier even though Ghost has 3 instances all must be treated as differnt. So meaning this attribute is Unique.","4b4bb7f7":"data.drop(['title'],inplace = True,axis = 1)","8dfe0b04":"for idx,instance in enumerate(data.loc[:5,'Keywords']):\n    print(idx,instance)","93fc979d":"# We can Do same treatment as Earlier\n\nCount = []\nfor instance in data.loc[:,'Keywords']:\n    if isinstance(instance,str):\n        X = ast.literal_eval(instance)\n        Count.append(len(X))   # Count Keep tracks of total Keywords of One Movie\n    else:\n        Count.append(0)","f9bc62d7":"data['Keywords'] = Count","03809d63":"for idx,instance in enumerate(data.loc[:1,'cast']):\n    print(idx,instance)","d3c8d234":"# We can Do same treatment as Earlier\n\nCount = []\nfor instance in data.loc[:,'cast']:\n    if isinstance(instance,str):\n        X = ast.literal_eval(instance)\n        Count.append(len(X))   # Count Keep tracks of total casts of One Movie\n    else:\n        Count.append(0)","637440ff":"data['cast'] = Count","8761d043":"for idx,instance in enumerate(data.loc[:1,'crew']):\n    print(idx,instance)","2b40b028":"# We can Do same treatment as Earlier\n\nCount = []\nfor instance in data.loc[:,'crew']:\n    if isinstance(instance,str):\n        X = ast.literal_eval(instance)\n        Count.append(len(X))   # Count Keep tracks of total crew of One Movie\n    else:\n        Count.append(0)","c46f5742":"data['crew'] = Count","51f20ce7":"data.describe()","e4b1a85d":"data.info()","a165f11b":"# Lets see Null values Now.\nNull_Values = pd.DataFrame(data.isnull().sum()).rename(columns = {0:'Total'})\nNull_Values['Percent'] = Null_Values['Total']\/len(data)\nNull_Values.sort_values('Percent',ascending=False).head()","458fb575":"# Lets Manually first store the index where Null values are present becasue Label Encoding will also encode Null values\nindex = list(Null_Values[Null_Values['Total'] >= 1].index)\nindex.remove('revenue')\nPosition = []\nfor col in index:\n    temp = data[data[col].isnull()].index\n    Position.append([col,temp])   ","162b3f98":"from sklearn.preprocessing import LabelEncoder\n# Lets Apply Label Encoding to object Columns\nObject_Columns = data.select_dtypes('object').columns\ndata[Object_Columns] = data.select_dtypes('object').apply(LabelEncoder().fit_transform)","2067e850":"for instance in Position:\n    data.loc[instance[1],instance[0]] = np.nan","04c4e10d":"# Extracting day,month,year,and quarter of the year\ndata['Release_month'] = data['release_date'].dt.month\ndata['Release_Year'] = data['release_date'].dt.year\ndata['Release_Day'] = data['release_date'].dt.day\ndata['Release_Quarter'] = data['release_date'].dt.quarter\ndata.drop(['release_date'],axis = 1,inplace = True)","45258817":"# First Lets Compress our dataset\ndef Reduce_Me(dataset):\n    Initial = data.memory_usage().sum()\/ 1024**2\n    print(\"Initial Memory : {:.2f} MB\".format(Initial))\n    Columns = dataset.columns\n    for column in Columns:\n        Dtype = str(data[column].dtype)\n        \n        min_ = data[column].min()\n        max_ = data[column].max()\n            \n        if 'int' in Dtype:\n            if min_ > np.iinfo(np.int8).min and max_ < np.iinfo(np.int8).max:\n                data[column] = data[column].astype(np.int8)\n            elif min_ > np.iinfo(np.int16).min and max_ < np.iinfo(np.int16).max:\n                data[column] = data[column].astype(np.int16)\n            elif min_ > np.iinfo(np.int32).min and max_ < np.iinfo(np.int32).max:\n                data[column] = data[column].astype(np.int32)\n            elif min_ > np.iinfo(np.int64).min and max_ < np.iinfo(np.int64).max:\n                data[column] = data[column].astype(np.int64)\n        else:\n            if min_ > np.finfo(np.float16).min and max_ < np.finfo(np.float16).max:\n                data[column] = data[column].astype(np.float16)\n            elif min_ > np.finfo(np.float32).min and max_ < np.finfo(np.float32).max:\n                data[column] = data[column].astype(np.float32)\n            elif min_ > np.finfo(np.float64).min and max_ < np.finfo(np.float64).max:\n                data[column] = data[column].astype(np.float64)\n    Final = data.memory_usage().sum()\/1024**2\n    print(\"Final Memory : {:.2f} MB\".format(Final))\n    print(\"Reduced By: {:.2f}%\".format((Initial-Final)\/Initial * 100))\n    return dataset\n","e7bea940":"data.drop(['id'],inplace = True,axis = 1)\ndata = Reduce_Me(data)","c9a25cdd":"# Lets see how many Unique Values are there in Integer columns\nInteger_Columns = [col for col in data if 'int' in str(data[col].dtypes)]\ndata[Integer_Columns].nunique().value_counts().sort_index().plot(kind = 'bar',figsize = (8,8),edgecolor = 'blue',color = 'red')\nplt.xlabel('Unique Count if the Feature')\nplt.ylabel('Number of Feature')\nplt.show()","529a5f4e":"import warnings\nwarnings.filterwarnings('ignore',category = FutureWarning)\n\n# Lets See the Distribution of Each of them\nimport seaborn as sns\nplt.figure(figsize=(20,20))\ncolor = ['red','green','blue','orange','pink','yellow']\nfor idx,col in enumerate(Integer_Columns):\n    ax = plt.subplot(4,3,idx+1)\n    color_idx = np.random.randint(0,len(color))\n    sns.violinplot(data[col],ax=ax,color=color[color_idx],linewidth=2)\n\n    \n# Its clear that the datapoints are highly skewed. We will see later if we should apply some transformations on it","8ce72f7e":"# Lets see how they are related to our target Variable\nimport warnings\nwarnings.filterwarnings('ignore',category = FutureWarning)\n\n# Lets See the Distribution of Each of them\nimport seaborn as sns\nplt.figure(figsize=(20,20))\ncolor = ['red','green','blue','orange','pink','yellow']\nfor idx,col in enumerate(Integer_Columns):\n    ax = plt.subplot(4,3,idx+1)\n    color_idx = np.random.randint(0,len(color))\n    sns.scatterplot(data[col],data['revenue'],ax=ax,color=color[color_idx],linewidth=2)\n\n    \n# No Clear Correlation are Visible. Budget and revenue seems to be correlated","0ac455ca":"import warnings\nwarnings.filterwarnings('ignore',category = FutureWarning)\n\nFloat_columns = [col for col in data if 'float' in str(data[col].dtypes)]\nFloat_columns.remove('revenue')\n# Lets See the Distribution of Float columns Each of them\nimport seaborn as sns\nplt.figure(figsize=(20,20))\ncolor = ['red','green','blue','orange','pink','yellow']\nfor idx,col in enumerate(Float_columns):\n    ax = plt.subplot(4,3,idx+1)\n    color_idx = np.random.randint(0,len(color))\n    Mean,Median = np.mean(data[col]),np.median(data[col])\n    sns.kdeplot(data[col],ax=ax,color=color[color_idx],linewidth=2)\n\n    \n# Some of them are Skewed. Some of them are almost gaussian centered at their respective mean","47990a13":"# Lets see how they are related to our target Variable\nimport warnings\nwarnings.filterwarnings('ignore',category = FutureWarning)\n\n# Lets See the Distribution of Each of them\nimport seaborn as sns\nplt.figure(figsize=(20,20))\ncolor = ['red','green','blue','orange','pink','yellow']\nfor idx,col in enumerate(Float_columns):\n    ax = plt.subplot(4,3,idx+1)\n    color_idx = np.random.randint(0,len(color))\n    sns.scatterplot(data[col],data['revenue'],ax=ax,color=color[color_idx],linewidth=2)\n\n    \n# No Clear Correlation are Visible. Budget and revenue seems to be correlated","b92eba77":"figure,ax = plt.subplots()\ndata.plot(x = \"revenue\",y = \"runtime\",ax = ax)\ndata.plot(x = \"revenue\",y = \"Release_Year\",ax = ax,secondary_y = True)\n\n# It simply says at almost fixed runtime the revenue tends to increase similar argument for Release Year","a118a2da":"figure,ax = plt.subplots()\ndata.plot(x = \"revenue\",y = \"budget\",ax = ax)\ndata.plot(x = \"revenue\",y = \"cast\",ax = ax,secondary_y = True)","89077321":"# Lets see Null values Again.\nNull_Values = pd.DataFrame(data.isnull().sum()).rename(columns = {0:'Total'})\nNull_Values['Percent'] = Null_Values['Total']\/len(data)\nNull_Values.sort_values('Percent',ascending=False).head(10)","778240ad":"# Lets see what values we should Put in these\nNull_Columns = list(Null_Values[Null_Values['Total'] >= 1].index)\nNull_Columns.remove('revenue')\nimport warnings\nwarnings.filterwarnings('ignore',category = FutureWarning)\n\n# Lets See the Distribution of Each of them\nimport seaborn as sns\nplt.figure(figsize=(20,20))\ncolor = ['red','green','blue','orange','pink','yellow']\nfor idx,col in enumerate(Null_Columns):\n    ax = plt.subplot(3,3,idx+1)\n    color_idx = np.random.randint(0,len(color))\n    sns.histplot(data[col],ax=ax,color=color[color_idx],linewidth=2)\n\n# Lets fill them with the median Value ","c3bd5717":"# Lets Start Feature Engineering\n# Lets first remove any redundant feature having correlation greater than 95%\nCorrelation_Matrix = data.corr()\nTriu = Correlation_Matrix.where(np.triu(np.ones(Correlation_Matrix.shape),k = 1).astype(np.bool))  # Will return Upper triangle\nRedundant_col = [col for col in Triu if np.any(Triu[col].abs() >= 0.95)]","c960d861":"data.drop(Redundant_col,axis = 1,inplace = True)","f52840f2":"data.info()","328ca5c5":"from sklearn.preprocessing import StandardScaler\nOriginalLabel = data.loc[data['revenue'].notnull(),'revenue']\nColumns = data.columns","025f48ac":"scaler = StandardScaler()  # Null values will remain np.nan so no problem there\ndata = scaler.fit_transform(data)\ndata = pd.DataFrame(data,columns=list(Columns))","5ed24f9a":"# Lets Manually Create Some Features\nimport featuretools as ft\nes = ft.EntitySet(id = 'TMDB')\nes = es.entity_from_dataframe(entity_id=\"data_tmdb\",dataframe=data,make_index=True,index = 'TMDB_id')","3f1efcbc":"es","39b86ae9":"es['data_tmdb'].variables","80274463":"feature_matrix,feature_dfs = ft.dfs(entityset=es,target_entity=\"data_tmdb\",trans_primitives=[\"add_numeric\",\"cum_mean\",\"cum_sum\",\"percentile\"],max_depth=2)","240518c8":"feature_matrix.shape","9f90c49c":"Label = feature_matrix[\"revenue\"]\nRemove_col = []\nfor col in feature_matrix:\n    if \"revenue\" in str(col):\n        Remove_col.append(col)\nRemove_col[:5]","4a8d4380":"feature_matrix.drop(Remove_col,axis = 1,inplace = True)","f73af2ac":"feature_matrix.head()","2fc14b61":"Correlation_Matrix = feature_matrix.corr()\nTriu = Correlation_Matrix.where(np.triu(np.ones(Correlation_Matrix.shape),k = 1).astype(np.bool))  # Will return Upper triangle\nRedundant_col = [col for col in Triu if np.any(Triu[col].abs() >= 0.95)]","e91c0862":"feature_matrix.drop(Redundant_col,axis = 1,inplace = True)","46adde1d":"feature_matrix.head()","677804ac":"feature_matrix['revenue'] = Label\ndel Label","22b4bab4":"Train = feature_matrix.loc[feature_matrix['revenue'].notnull(),:]\nTest = feature_matrix.loc[feature_matrix['revenue'].isnull(),:]","956f2e6a":"Label = OriginalLabel\nTrain.drop(['revenue'],inplace = True,axis = 1)\nTest.drop(['revenue'],inplace = True,axis = 1)","d58be327":"Features = Train.columns","023f148b":"# Lets start Machine Learning From Here\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score,make_scorer\n#Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse).\n#A constant model that always predicts the expected value of y, disregarding the input features,\n#would get a R^2 score of 0.0.\n\nimputer = SimpleImputer(strategy=\"median\")\nscore_fn = make_scorer(r2_score,greater_is_better=True)","36cd2391":"Train = imputer.fit_transform(Train)\nTest = imputer.transform(Test)","99b4098d":"Label = Label.values","ae12c649":"from sklearn.model_selection import cross_val_score","8ef42975":"from sklearn.ensemble import RandomForestRegressor   # Our Base Model \nfrom sklearn.model_selection import cross_val_score\n\nRandomForest = RandomForestRegressor(n_estimators=100,n_jobs=-1,random_state=42)\ncv_score = cross_val_score(RandomForest,Train,Label,cv = 10,scoring=score_fn)","49172378":"print(\"10 Fold Cross Validation Mean R2_SCORE: {0} with Deviation: {1}\".format(round(np.mean(cv_score),3),round(cv_score.std(),3)))","4e8aaca6":"RandomForest.fit(Train,Label)\nfeature_importance = pd.DataFrame({'Features':Features,'Importance':RandomForest.feature_importances_})","218a0d6d":"def Plot_Importance(df,count = 5,threshold = 0.95):\n    df['Importance'] = df['Importance']\/df['Importance'].sum()\n    df = df.sort_values('Importance',ascending = False).reset_index(drop = True)\n    \n    df['cum_sum'] = np.cumsum(df['Importance'])\n    \n    # Plotting Values\n    df.loc[:count,:].plot(kind = 'barh',x = 'Features',y = 'Importance',color = 'green',\n                          edgecolor = 'blue',figsize = (5,5),linewidth = 2)\n    plt.xlabel(\"Normalized Importance\")\n    plt.xscale(\"log\")\n    plt.gca().invert_yaxis()\n    \n    plt.show()\n    \n    if threshold:\n        min_ = np.min(np.where(df['cum_sum'] > threshold))\n        plt.xlabel('# Features', size = 10)\n        plt.ylabel('Cumulative Importance', size = 10)\n        plt.title('Cumulative Importance of Features', size = 10)\n        plt.plot(np.arange(len(df)),df['cum_sum'],color = 'red')\n        plt.vlines(min_ + 1,ymin = 0,ymax = 1,color = 'black',linestyles='dotted')\n        plt.show()\n        print('Number of Columns required for {0} threshold is: {1}'.format(threshold,min_+1))\n    return df","50384831":"feature_importance = Plot_Importance(feature_importance,count=10)","eae7790b":"from sklearn.exceptions import ConvergenceWarning\n\nwarnings.filterwarnings('ignore',category=ConvergenceWarning)\nwarnings.filterwarnings('ignore',category=UserWarning)\nwarnings.filterwarnings('ignore',category=DeprecationWarning)\n\nResult = pd.DataFrame(columns = ['Model','CV_Mean','CV_Std'])\n\ndef Check_Model(Model,Name,cv = 10,WantResult = True):\n    \n    global Result\n    cv_score = cross_val_score(Model,Train,Label,cv = cv,scoring=score_fn)\n    print(\"10 Fold Cross Validation Mean R2_SCORE: {0} with Deviation: {1}\".format(round(cv_score.mean(),3),round(cv_score.std(),3)))\n    \n    if WantResult:\n        Result = Result.append(pd.DataFrame({'Model':Name,'CV_Mean':round(np.mean(cv_score),3),'CV_Std':round(np.std(cv_score),3)},index = [0]),ignore_index=True)\n        return Result","e1dd3076":"from sklearn.ensemble import RandomForestRegressor \nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.svm import LinearSVR,SVR\nfrom sklearn.neural_network import MLPRegressor\n\nforest_regressor = RandomForestRegressor(n_estimators = 100,n_jobs = -1,random_state=42)\nL_Regress = LinearRegression()\nLog_Regress = LogisticRegression(n_jobs = -1)\nL_SVR = LinearSVR()\nN_SVR = SVR()\nMLP_Regressor = MLPRegressor(hidden_layer_sizes=[100,100,100],early_stopping=True)","8dad2833":"Result = Check_Model(forest_regressor,\"RandomForest\",WantResult=True)","aa23e9f7":"Result = Check_Model(L_Regress,\"LinearRegression\",WantResult=True)","066151f9":"Result = Check_Model(Log_Regress,\"LogistikRegression\",WantResult=True)","5ddef73f":"Result = Check_Model(L_SVR,\"LinearSVR\",WantResult=True)","8c99ce2c":"Result = Check_Model(N_SVR,\"SVR\",WantResult=True)","f476d004":"Result = Check_Model(MLP_Regressor,\"MLPRegressor\",WantResult=True)","856e9c19":"Result.set_index(\"Model\",inplace = True)","9aa86f09":"Result[\"CV_Mean\"].plot(kind = \"bar\",edgecolor = \"black\",color = \"yellow\",yerr = list(Result[\"CV_Std\"]))\nplt.hlines(0,xmin = -2,xmax = 10,color = \"red\",linestyles=\"dotted\")\nplt.axis([-2,10,-2,10])\n\n# Only Random Forest and MLP Classifier seems to do good","d100c3aa":"#Hyperparameter optimization for Random Forest\n\nfrom hyperopt import STATUS_OK\n\ndef Objective_function(params):\n    \n    model = RandomForestRegressor(**params,n_jobs = -1)\n    cvscore = cross_val_score(model,Train,Label,cv = 10,scoring=score_fn)\n    score = 1 - np.mean(cvscore)\n    return {\"loss\":score,'params':params,\"status\":STATUS_OK}","6cbe93f0":"# Domain Space\nfrom hyperopt import hp\nspace = {\n    'n_estimators':hp.choice('n_estimators',range(10,1000)),\n    'max_depth':hp.choice('max_depth',range(1,50)),\n    'max_features':hp.choice('max_features',['auto','sqrt','log2']),\n    'max_leaf_nodes':hp.choice('max_leaf_nodes',range(10,150)),\n    'min_samples_split':hp.choice('min_samples_split',range(2,100)),\n    'max_leaf_nodes':hp.choice('max_leaf_nodes',[None,2,5,10,15,20,25,50,100])\n}","1cc7e5b6":"from hyperopt import Trials\n\nbayes_trials = Trials()","38dea44f":"from hyperopt import fmin,tpe\n\nbest = fmin(Objective_function,space,max_evals=50,trials=bayes_trials,algo = tpe.suggest)","3ea08c34":"# Lets Optimize MLP Classifier\n\ndef MLPObjectiveFunction(params):\n    model = MLPRegressor(**params,early_stopping=True)\n    cvscore = cross_val_score(model,Train,Label,cv = 10,scoring=score_fn)\n    score = 1 - np.mean(cvscore)\n    return {\"loss\":score,'params':params,\"status\":STATUS_OK}","9aece37c":"MLP_space = {\n    'hidden_layer_sizes':hp.choice('hidden_layer_sizes',[100,[100,100],[100,100,100],[100,100,100,100]]),\n    'activation':hp.choice('activation',['identity', 'logistic', 'tanh', 'relu']),\n    'alpha':hp.choice('alpha',np.linspace(0.0001,1,num=1000)),\n    'learning_rate':hp.choice('learning_rate',['constant','adaptive','invscaling']),\n    'learning_rate_init':hp.choice('learning_rate_init',np.linspace(0.001,1,1000))\n}","30cada69":"MLP_Trial = Trials()","1dd0935c":"MLP_best = fmin(MLPObjectiveFunction,MLP_space,max_evals=50,trials=MLP_Trial,algo = tpe.suggest)","6c632f95":"from hyperopt import space_eval\nparams = space_eval(space,best)    # These are the best parameters for Random Forest","0bc36896":"params","a1914aef":"Best_model = RandomForestRegressor(**params,n_jobs = -1)","037923a0":"cvscore = cross_val_score(Best_model,Train,Label,cv = 10,n_jobs = -1)\nprint(\"10 Fold Cross Validation Mean R2_SCORE: {0} with Deviation: {1}\".format(round(np.mean(cv_score),3),round(cv_score.std(),3)))","9e595375":"# Lets use Gradient boosting as see how it works we will use lightgbm implementation\n\nimport lightgbm as lgb\nimport numpy as np\n\ndef Objective_function_lgn(params):\n        \n    boost_type = params['boosting_type']['boosting_type']\n    del params['boosting_type']\n    params['boosting_type'] = boost_type\n\n    model = lgb.LGBMRegressor(**params,n_jobs = -1)\n    score = cross_val_score(model,Train,Label,cv = 10,scoring=score_fn)\n    loss = 1 - np.mean(score)\n    return {'loss':loss,'params':params,'status':STATUS_OK}","b72fc168":"from hyperopt import hp,Trials,fmin,tpe\nspace = {\n        'class_weight': hp.choice('class_weight', [None, 'balanced']),\n        'boosting_type': hp.choice('boosting_type',\n                                   [{'boosting_type': 'gbdt'},                                     \n                                    {'boosting_type': 'dart'},\n                                    {'boosting_type': 'goss'}]),\n        'num_leaves': hp.choice('num_leaves', np.arange(30, 150)),\n        'subsample_for_bin': hp.choice('subsample_for_bin', np.arange(20000, 300000)),\n        'feature_fraction': hp.choice('feature_fraction', np.random.rand(1,1000)),\n        'bagging_fraction': hp.choice('bagging_fraction', np.random.rand(1,1000)), \n        'lambda_l1': hp.choice('lambda_l1', [0, hp.loguniform('lambda_l1_positive', -16, 2)]),\n        'lambda_l2': hp.choice('lambda_l2', [0, hp.loguniform('lambda_l2_positive', -16, 2)]),\n        'min_child_samples': hp.choice('min_child_samples', np.arange(20, 500)),\n        'reg_alpha': hp.choice('reg_alpha', np.random.rand(1,1000)),\n        'reg_lambda': hp.choice('reg_lambda', np.random.rand(1,1000)),\n        'colsample_bytree': hp.choice('colsample_by_tree', np.random.rand(1,1000)),\n        'objective': \"regression\",\n        'n_estimators': hp.choice('n_estimator',np.arange(10,1000)),\n    }","737e2605":"lgbm_trial = Trials()","5fef82fc":"GBM_best = fmin(Objective_function_lgn,space,max_evals=200,trials=lgbm_trial,algo = tpe.suggest)","4b0e1ad0":"**Lets Deal with \"homepage\" now.**","dbc1f2ea":"**Lets Deal with \"homepage\" now:**","9b4f3c05":"**Lets Deal with \"Original language\" now:**","13802644":"**Lets See \"production_countries\" now**","238da269":"**Lets see \"poster_path\" now:**","ece6b252":"**Lets Deal with \"belongs_to_collection\" first.**","1787c7fd":"**Lets deal with 'Genre' Now:**","822e9968":"**Dealing with \"release_date\" attribute:**"}}