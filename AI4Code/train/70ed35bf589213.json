{"cell_type":{"e55ce436":"code","73608318":"code","95cb88d0":"code","f6a7e453":"code","37808b29":"code","47da883f":"code","ea17888f":"code","29ab3409":"code","1ec0c47a":"code","1d2b550a":"code","dec4548b":"code","c2efd4f4":"code","17fe979b":"code","7c30f768":"code","328d4145":"code","118c4bcb":"code","121580e5":"code","0be1cbe6":"code","22c18ce9":"code","85e87a87":"code","7dfe7daf":"code","fb74fd25":"code","a4a71532":"code","584d56de":"code","ab193211":"code","9827c2b3":"code","fcef1308":"code","065534c4":"code","167a2403":"markdown","69ea2df9":"markdown","801369ee":"markdown","e9dd4b2e":"markdown","c34a9fa7":"markdown","0e86a292":"markdown","d7a8a1d6":"markdown","df6c0bea":"markdown","16baf76b":"markdown","ba1b5c9e":"markdown","68785ad0":"markdown","7e11ff26":"markdown","ab886ea8":"markdown","88741ac6":"markdown","c492e75a":"markdown","3a50d966":"markdown","112a8520":"markdown","e27de5dc":"markdown","fee7c542":"markdown","bdcbda1b":"markdown","5039651f":"markdown","3db0a0cb":"markdown"},"source":{"e55ce436":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","73608318":"\nimport tensorflow as tf\nimport keras.preprocessing.image as process_im\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom keras.applications import vgg19\nfrom keras.models import Model\nfrom tensorflow.python.keras import models \nfrom tensorflow.python.keras import losses\nfrom tensorflow.python.keras import layers\nfrom tensorflow.python.keras import backend as K\nimport functools\nimport IPython.display","95cb88d0":"content_path='\/kaggle\/input\/images33\/5e86f171d5873a0d6a6616d3.jpg'\nstyle_path = '\/kaggle\/input\/new-im\/vincent_van_gogh_self_portrait_painting_musee_dorsay_via_wikimedia_commons_promojpg.jpg'","f6a7e453":"def load_file(image_path):\n    image =  Image.open(image_path)\n    max_dim=512\n    factor=max_dim\/max(image.size)\n    image=image.resize((round(image.size[0]*factor),round(image.size[1]*factor)),Image.ANTIALIAS)\n    im_array = process_im.img_to_array(image)\n    im_array = np.expand_dims(im_array,axis=0) #adding extra axis to the array as to generate a \n                                               #batch of single image \n    \n    return im_array","37808b29":"def show_im(img,title=None):\n    img=np.squeeze(img,axis=0) #squeeze array to drop batch axis\n    plt.imshow(np.uint8(img))\n    if title is None:\n        pass\n    else:\n        plt.title(title)\n    plt.imshow(np.uint8(img))","47da883f":"content = load_file(content_path)\nstyle = load_file(style_path)","ea17888f":"plt.figure(figsize=(10,10))\ncontent = load_file(content_path)\nstyle = load_file(style_path)\nplt.subplot(1,2,1)\nshow_im(content,'Content Image')\nplt.subplot(1,2,2)\nshow_im(style,'Style Image')\nplt.show()","29ab3409":"def img_preprocess(img_path):\n    image=load_file(img_path)\n    img=tf.keras.applications.vgg19.preprocess_input(image)\n    return img","1ec0c47a":"def deprocess_img(processed_img):\n  x = processed_img.copy()\n  if len(x.shape) == 4:\n    x = np.squeeze(x, 0)\n  assert len(x.shape) == 3 #Input dimension must be [1, height, width, channel] or [height, width, channel]\n  \n  \n  # perform the inverse of the preprocessing step\n  x[:, :, 0] += 103.939\n  x[:, :, 1] += 116.779\n  x[:, :, 2] += 123.68\n  x = x[:, :, ::-1] # converting BGR to RGB channel\n\n  x = np.clip(x, 0, 255).astype('uint8')\n  return x","1d2b550a":"im=img_preprocess(content_path)","dec4548b":"content_layers = ['block5_conv2']\nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1', \n                'block4_conv1', \n                'block5_conv1']\nnumber_content=len(content_layers)\nnumber_style =len(style_layers)","c2efd4f4":"def get_model():\n    \n    vgg=tf.keras.applications.vgg19.VGG19(include_top=False,weights='imagenet')\n    vgg.trainable=False\n    content_output=[vgg.get_layer(layer).output for layer in content_layers]\n    style_output=[vgg.get_layer(layer).output for layer in style_layers]\n    model_output= style_output+content_output\n    return models.Model(vgg.input,model_output)","17fe979b":"model=tf.keras.applications.vgg19.VGG19(include_top=False,weights='imagenet')\nmodel.summary()","7c30f768":"model=get_model()\nmodel.summary()","328d4145":"def get_content_loss(noise,target):\n    loss = tf.reduce_mean(tf.square(noise-target))\n    return loss","118c4bcb":"def gram_matrix(tensor):\n    channels=int(tensor.shape[-1])\n    vector=tf.reshape(tensor,[-1,channels])\n    n=tf.shape(vector)[0]\n    gram_matrix=tf.matmul(vector,vector,transpose_a=True)\n    return gram_matrix\/tf.cast(n,tf.float32)","121580e5":"def get_style_loss(noise,target):\n    gram_noise=gram_matrix(noise)\n    #gram_target=gram_matrix(target)\n    loss=tf.reduce_mean(tf.square(target-gram_noise))\n    return loss\n    ","0be1cbe6":"def get_features(model,content_path,style_path):\n    content_img=img_preprocess(content_path)\n    style_image=img_preprocess(style_path)\n    \n    content_output=model(content_img)\n    style_output=model(style_image)\n    \n    content_feature = [layer[0] for layer in content_output[number_style:]]\n    style_feature = [layer[0] for layer in style_output[:number_style]]\n    return content_feature,style_feature\n    ","22c18ce9":"def compute_loss(model, loss_weights,image, gram_style_features, content_features):\n    style_weight,content_weight = loss_weights #style weight and content weight are user given parameters\n                                               #that define what percentage of content and\/or style will be preserved in the generated image\n    \n    output=model(image)\n    content_loss=0\n    style_loss=0\n    \n    noise_style_features = output[:number_style]\n    noise_content_feature = output[number_style:]\n    \n    weight_per_layer = 1.0\/float(number_style)\n    for a,b in zip(gram_style_features,noise_style_features):\n        style_loss+=weight_per_layer*get_style_loss(b[0],a)\n        \n    \n    weight_per_layer =1.0\/ float(number_content)\n    for a,b in zip(noise_content_feature,content_features):\n        content_loss+=weight_per_layer*get_content_loss(a[0],b)\n        \n    style_loss *= style_weight\n    content_loss *= content_weight\n    \n    total_loss = content_loss + style_loss\n    \n    \n    return total_loss,style_loss,content_loss","85e87a87":"def compute_grads(dictionary):\n    with tf.GradientTape() as tape:\n        all_loss=compute_loss(**dictionary)\n        \n    total_loss=all_loss[0]\n    return tape.gradient(total_loss,dictionary['image']),all_loss","7dfe7daf":"model=tf.keras.applications.vgg19.VGG19(include_top=False,weights='imagenet')","fb74fd25":"model.summary()","a4a71532":"def run_style_transfer(content_path,style_path,epochs=500,content_weight=1e3, style_weight=1e-2):\n    \n    model=get_model()\n    \n    for layer in model.layers:\n        layer.trainable = False\n        \n    content_feature,style_feature = get_features(model,content_path,style_path)\n    style_gram_matrix=[gram_matrix(feature) for feature in style_feature]\n    \n    noise = img_preprocess(content_path)\n    noise=tf.Variable(noise,dtype=tf.float32)\n    \n    optimizer = tf.keras.optimizers.Adam(learning_rate=5, beta_1=0.99, epsilon=1e-1)\n    \n    best_loss,best_img=float('inf'),None\n    \n    loss_weights = (style_weight, content_weight)\n    dictionary={'model':model,\n              'loss_weights':loss_weights,\n              'image':noise,\n              'gram_style_features':style_gram_matrix,\n              'content_features':content_feature}\n    \n    norm_means = np.array([103.939, 116.779, 123.68])\n    min_vals = -norm_means\n    max_vals = 255 - norm_means   \n  \n    imgs = []\n    for i in range(epochs):\n        grad,all_loss=compute_grads(dictionary)\n        total_loss,style_loss,content_loss=all_loss\n        optimizer.apply_gradients([(grad,noise)])\n        clipped=tf.clip_by_value(noise,min_vals,max_vals)\n        noise.assign(clipped)\n        \n        if total_loss<best_loss:\n            best_loss = total_loss\n            best_img = deprocess_img(noise.numpy())\n            \n         #for visualization   \n            \n        if i%5==0:\n            plot_img = noise.numpy()\n            plot_img = deprocess_img(plot_img)\n            imgs.append(plot_img)\n            IPython.display.clear_output(wait=True)\n            IPython.display.display_png(Image.fromarray(plot_img))\n            print('Epoch: {}'.format(i))        \n            print('Total loss: {:.4e}, ' \n              'style loss: {:.4e}, '\n              'content loss: {:.4e}, '.format(total_loss, style_loss, content_loss))\n    \n    IPython.display.clear_output(wait=True)\n    \n    \n    return best_img,best_loss,imgs","584d56de":"best, best_loss,image = run_style_transfer(content_path, \n                                     style_path, epochs=500)\n","ab193211":"plt.figure(figsize=(15,15))\nplt.subplot(1,3,3)\nplt.imshow(best)\nplt.title('Style transfer Image')\nplt.xticks([])\nplt.yticks([])\nplt.subplot(1,3,1)\nshow_im(content,'Content Image')\nplt.xticks([])\nplt.yticks([])\nplt.subplot(1,3,2)\nshow_im(style,'Style Image')\nplt.xticks([])\nplt.yticks([])\nplt.show()","9827c2b3":"plt.figure(figsize=(15,15))\nplt.subplot(1,3,3)\nplt.imshow(best)\nplt.title('Style transfer Image')\nplt.xticks([])\nplt.yticks([])\nplt.subplot(1,3,1)\nshow_im(content,'Content Image')\nplt.xticks([])\nplt.yticks([])\nplt.subplot(1,3,2)\nshow_im(style,'Style Image')\nplt.xticks([])\nplt.yticks([])\nplt.show()","fcef1308":"plt.figure(figsize=(15,15))\nplt.subplot(1,3,3)\nplt.imshow(best)\nplt.title('Style transfer Image')\nplt.xticks([])\nplt.yticks([])\nplt.subplot(1,3,1)\nshow_im(content,'Content Image')\nplt.xticks([])\nplt.yticks([])\nplt.subplot(1,3,2)\nshow_im(style,'Style Image')\nplt.xticks([])\nplt.yticks([])\nplt.show()\n","065534c4":"plt.figure(figsize=(15,15))\nplt.subplot(1,3,3)\nplt.imshow(best)\nplt.xticks([])\nplt.yticks([])\nplt.subplot(1,3,1)\nshow_im(content,'Content Image')\nplt.xticks([])\nplt.yticks([])\nplt.subplot(1,3,2)\nshow_im(style,'Style Image')\nplt.xticks([])\nplt.yticks([])\nplt.show()","167a2403":"# **Nueral Style transfer with pre-trained VGG19 model**\n\n\nNeural style transfer is the technique to compose images or videos in the style of another image (for e.g. a famous artwork) using deep learning networks.Neural style transfer is based on the idea that it is possible to separate the style representation and content representations in a CNN, learnt during a computer vision task (e.g. image recognition task).\n\n\n![cf5bc18c3b8a3761e341056c3f131012001d4248.jpeg](attachment:cf5bc18c3b8a3761e341056c3f131012001d4248.jpeg)\n\nAs seen, the generated image is having the content of the content image and style of the style image.","69ea2df9":"# **Style Transfer Visualization**","801369ee":"**Define content loss**\n\nEssentially content loss captures the root mean squared error between the activations produced by the generated image and the content image.","e9dd4b2e":"# **Thanks**","c34a9fa7":"**Define function to get vgg19 model with pretrained weights**","0e86a292":"**Define function to plot image**","d7a8a1d6":"# **Comment if you have any queries or find anything wrong with the code**","df6c0bea":"**Define function to deprocess image **","16baf76b":"VGG networks are trained on image with each channel normalized by mean = [103.939, 116.779, 123.68]and with channels BGR.","ba1b5c9e":"**Define style loss**\n\n\nThe goal is to compute a style matrix for the generated image and the style image. Then the style loss is defined as the root mean square difference between the two style matrices. Style information is measured as the amount of correlation present between features maps in a given layer. Next, a loss is defined as the difference of correlation present between the feature maps computed by the generated image and the style image. The gram matrix is used to find the correlation between the feature maps of a convolution layer.","68785ad0":"# **References**\n\n1. https:\/\/hackernoon.com\/how-do-neural-style-transfers-work-7bedaee0559a \n\n2. https:\/\/arxiv.org\/pdf\/1701.01036.pdf\n\n3. https:\/\/towardsdatascience.com\/artistic-style-transfer-b7566a216431\n\n4. https:\/\/arxiv.org\/abs\/1508.06576\n\n","7e11ff26":"**Get necessary layers from vgg19 model**","ab886ea8":"**Model architecture**","88741ac6":"# **Upvote if you like it**","c492e75a":"**Define function to load images and return numpy array**","3a50d966":"**Define function to process image for input to vgg19 model**","112a8520":"# **Importing necessary libraries**","e27de5dc":"**Define function to compute total loss**","fee7c542":"**Note** - why VGG model and not any other deep CNN ?\nPrimarily since the authors in the original paper suggested using VGG architecture for obtaining the best results and secondly as I learnt after doing some research online,it seems that due to the complexity of architecture of other very deep CNNs(like Resnet), the feature maps do not work give as optimal results as well as those of VGG16 or VGG19 .\n","bdcbda1b":"**Define function to calculate gradient**","5039651f":"**Plot Image**","3db0a0cb":"# **Loss Functions**\n\nNeural style transfer is done by defining two loss functions that try to minimise the differences between a content image, a style image and a generated image. Take the base input image, the content image and the style image that needs to be matched and transform the base input image by minimizing the content and style distances (losses) with backpropagation, creating an image that matches the content of the content image and the style of the style image.\n\nThe content loss function ensures that the activations of the higher layers are similar between the content image and the generated image. The style loss function makes sure that the correlation of activations in all the layers are similar between the style image and the generated image. "}}