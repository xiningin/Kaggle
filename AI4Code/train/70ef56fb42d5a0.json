{"cell_type":{"1c66316c":"code","e45cc4ac":"code","87b93546":"code","29b4a75a":"code","89de0151":"code","e73e721c":"code","03a6d1b1":"code","6b7331dd":"code","09d48530":"code","d8270dab":"markdown","6b61a571":"markdown","91fed179":"markdown","73f399b1":"markdown","2b9b6f42":"markdown","aea6e0c4":"markdown","b71cd762":"markdown","1ac0285b":"markdown","4fc1966e":"markdown"},"source":{"1c66316c":"import warnings  \nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import ElasticNet, Ridge, Lasso\nfrom xgboost import XGBRegressor","e45cc4ac":"X_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\nX_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col=\"Id\")\nX_train.dropna(axis=0, subset=['SalePrice'], inplace=True)","87b93546":"plt.figure(figsize=(16,6))\nsns.scatterplot(x=X_train['GrLivArea'], y=X_train['SalePrice'])","29b4a75a":"plt.figure(figsize=(16,6))\nsns.scatterplot(x=X_train['LotArea'], y=X_train['SalePrice'])","89de0151":"X_train = X_train[(X_train['SalePrice'] > 50000) & (X_train['SalePrice'] < 750000) & (X_train['LotArea'] < 110000) & (X_train['GrLivArea'] < 4000)]\ny = X_train['SalePrice']\nX_train.drop(['SalePrice'], axis=1, inplace=True)\ntrain_size = len(X_train)\n\ndef investigate_missing(df):\n    for col in df:\n        missing = df[col].isnull().sum()\n        if missing > 0:\n            print(\"{}: {} missing --- type: {}\".format(col, missing, df[col].dtype))\n            \ninvestigate_missing(pd.concat([X_train, X_test]))","e73e721c":"def featureProcessing(df):\n    \n    df['MSSubClass'] = df['MSSubClass'].astype(str)\n    df['YrSold'] = df['YrSold'].astype(str)\n    df['MoSold'] = df['MoSold'].astype(str)\n    \n    df['LotFrontage'] = df.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n    \n    df['Shed?'] = np.where(df['MiscFeature']=='Shed', 1, 0)\n    df.drop('MiscFeature', axis=1, inplace=True)\n    \n    df['MasVnrType'].replace(['None'], 'NA')\n    df.drop(['GarageYrBlt', 'Condition2', 'Functional'],  axis=1, inplace=True)\n    \n    df['QualitySF'] = df['GrLivArea'] * df['OverallQual']\n    \n    # Numerical columns for which we want to impute missing values with 0\n    fill_num_col=[\n        'MasVnrArea'\n    ]\n    \n    # Categorical columns for which we want to impute missing values with 'NA'\n    fill_cat_col=[\n        'BsmtQual',\n        'BsmtCond',\n        'BsmtExposure',\n        'BsmtFinType1',\n        'BsmtFinType2',\n        'FireplaceQu',\n        'GarageType',\n        'GarageFinish',\n        'GarageQual',\n        'GarageCond',\n        'PoolQC',\n        'Fence'  \n    ]\n    \n    # Categorical columns for which we want to impute missing values with their mode\n    mode_cat_col=[\n        'Alley',\n        'MasVnrType',\n        'Electrical'\n    ]\n    \n    for feat in fill_num_col:\n        df[feat].fillna(0, inplace=True)\n    for feat in fill_cat_col:\n        df[feat].fillna('NA', inplace=True)\n    for feat in mode_cat_col:\n        df[feat].fillna(df[feat].mode(), inplace=True)\n    \n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    area_features = [feat for feat in X_train.columns if \"SF\" in feat or \"Area\" in feat]\n    \n    numericals = df.select_dtypes(include=numerics).columns.values\n    categoricals = df.select_dtypes(include=['object']).columns.values\n    \n    for feat in area_features:\n        df[feat] = df[feat].map(lambda x: np.sqrt(x))\n    \n    skewness = df.select_dtypes(include=numerics).apply(lambda x: skew(x))\n    skew_index = skewness[abs(skewness) >= 0.5].index\n    for col in skew_index:\n        df[col] = boxcox1p(df[col], 0.15) \n    for feat in numericals:\n        df[feat].fillna(0, inplace=True)\n        df[feat] = RobustScaler().fit_transform(df[feat].apply(float).values.reshape(-1,1))\n    for feat in categoricals:\n        df[feat].fillna('NA', inplace=True)\n        dummies = pd.get_dummies(df[feat])\n        dummies.columns = [col + feat for col in dummies.columns.values]\n        df.drop(feat, axis=1, inplace=True)\n        df = df.join(dummies)\n\n    return df\n\n\nX_full = featureProcessing(pd.concat([X_train, X_test]))\n\nX_train = X_full[:train_size]\nX_test = X_full[train_size:]","03a6d1b1":"#Tunes the respective hyperparameters\n\n'''\n\ndef RMSLE_cross_val(model):\n    return (-cross_val_score(model, X_train, np.log1p(y), scoring=\"neg_mean_squared_error\", cv=5)).mean()\n\n#Hyperparameter vector space for ElasticNet\nalphas_en = np.arange(0.0005, 0.01, 0.0005)\nl1_ratios = np.arange(0.1, 0.9, 0.025)\nbest_alpha_en, best_l1 = 0.0001, 0.1\n\n#Hyperparameter vector space for Ridge\na_r = np.arange(1,10, 1, dtype=float)\nb_r = np.arange(-6, 5, 1, dtype=float)\nbest_a_r, best_b_r = 1, -6\n\n#Hyperparameter vector space for Lasso\na_l = np.arange(1,10, 1, dtype=float)\nb_l = np.arange(-6, 5, 1, dtype=float)\nbest_a_l, best_b_l = 1, -6\n\n#Hyperparameter vector space for XGBoost\nn_estimators = np.arange(100, 1200, 100)\nlearning_rates = np.arange(0.005, 0.15, 0.005)\nbest_n, best_learn = 100, 0.005\n\nbest_score = float('inf')\nprint(\"ELASTICNET\")\nfor alpha in alphas_en:\n    for l1_ratio in l1_ratios:\n        score = RMSLE_cross_val(ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=10000, random_state=0))\n        print(alpha, l1_ratio, score)\n        if score < best_score:\n            best_alpha_en, best_l1, best_score = alpha, l1_ratio, score\nprint(\"BEST PARAMETERS: alpha = {}, l1-ratio = {} --- SCORE: {}\\n:\".format(best_alpha_en, best_l1, best_score))\n\nbest_score = float('inf')\nprint(\"RIDGE\")\nfor a in a_r:\n    for b in b_r:\n        alpha = a * 10**b\n        score = RMSLE_cross_val(Ridge(alpha=alpha, max_iter=10000, random_state=0))\n        print(alpha, score)\n        if score < best_score:\n            best_a_r, best_b_r, best_score = a, b, score\nprint(\"BEST PARAMETERS: alpha = {} --- SCORE = {}\\n:\".format(best_a_r * 10**best_b_r, best_score))\n\nbest_score = float('inf')\nprint(\"LASSO\")\nfor a in a_l:\n    for b in b_l:\n        alpha = a * 10**b\n        score = RMSLE_cross_val(Lasso(alpha=alpha, max_iter=10000, random_state=0))\n        print(alpha, score)\n        if score < best_score:\n            best_a_l, best_b_l, best_score = a, b, score\nprint(\"BEST PARAMETERS: alpha = {} --- SCORE: {}\\n:\".format(best_a_l * 10**best_b_l, best_score))\n\nbest_score = float('inf')\nprint(\"XGBOOST\")\nfor n in n_estimators:\n    for lr in learning_rates:\n        score = RMSLE_cross_val(XGBRegressor(n_estimators=n, learning_rate=lr, objective=\"reg:squarederror\", n_jobs=-1, random_state=0))\n        print(n, lr, score)\n        if score < best_score:\n            best_n, best_learn, best_score = n, lr, score\nprint(\"BEST PARAMETERS: n_estimators = {}, learning_rate = {} --- SCORE: {}\\n:\".format(best_n, best_learn, best_score))\n\n'''","6b7331dd":"model_EN = ElasticNet(alpha=0.0025, l1_ratio=0.15, max_iter=10000, random_state=0)\nmodel_EN.fit(X_train, np.log1p(y))\nEN_preds = model_EN.predict(X_test)\n\nmodel_R = Ridge(alpha=30, max_iter=10000, random_state=0)\nmodel_R.fit(X_train, np.log1p(y))\nR_preds = model_R.predict(X_test)\n\nmodel_L = Lasso(alpha=0.0006, max_iter=10000, random_state=0)\nmodel_L.fit(X_train, np.log1p(y))\nL_preds = model_L.predict(X_test)\n\nmodel_XGB = XGBRegressor(n_estimators=1000, learning_rate=0.055, objective=\"reg:squarederror\", n_jobs=-1, random_state=0)\nmodel_XGB.fit(X_train, np.log1p(y))\nXGB_preds = model_XGB.predict(X_test)\n\nensemble_preds = 0.75*EN_preds + 0.05*R_preds + 0.05*L_preds + 0.15*XGB_preds ","09d48530":"preds = np.expm1(ensemble_preds)\n\noutput = pd.DataFrame({'Id': X_test.index,\n                      'SalePrice': preds})\n\noutput.to_csv('submission.csv', index=False)","d8270dab":"Plot living area vs. sale price to spot outliers in either dimension. Extremes in home size or final sale price will distort patterns in the dataset.","6b61a571":"Similar to above, but now we plot lot area vs. sale price.","91fed179":"Import the training and testing data.","73f399b1":"<h1>Importing data and detecting outliers<\/h1>\n\nFirst, import the necessary models and functions.","2b9b6f42":"<h1>Modelling<\/h1>\n\nWe are now ready to start modelling our data. We will use ```ElasticNet```, ```Ridge```, ```Lasso```, and ```XGBoost``` in our ensemble. But first, we need to tune the hyperparameters for these models.\n\nFor ```ElasticNet``` we need to tune ```alpha``` and ```l1_ratio```.\n\nFor ```Ridge``` and ```Lasso```, we need to tune ```alpha```.\n\nFor ```XGBoost```, we need to tune ```n_estimators``` and ```learning_rate```.\n\nFor each model, we do this using a grid search with k-fold cross validation.\n\n**Important**: Since the scoring metric used for this competition is root mean squared *log* error, we use ```np.log1p(y)``` as our target, not just ```y```. We just have to remember to apply the reverse transformation ```np.expm1()``` when we submit our predictions.","aea6e0c4":"Remove the outliers and start digging into missing values that require imputation.","b71cd762":"We apply ```npexp1()``` to our predictions and export them for submission.","1ac0285b":"<h1>Training the models and blending<\/h1>\n\nWe have obtained the optimal values for our hyperparameters, so we start training the models.\n\nWe take a weighted average of the predictions from these 4 models to use as our final ensemble prediction.\n\nIt should be noted that this is quite a crude blending technique, a more sophisticated approach would yield more accurate predictions.","4fc1966e":"<h1>Feature processing<\/h1>\n\nNow comes the time to cleanup a lot of the features and possibly engineer new ones.\n\nWe start by converting some numeric features to strings. Months of the year being encoded as integers doesnt make much sense.\n\nWe then look into ```LotFrontage```, which has *483* missing values. ```Neighborhood``` is present in every observation, so for each house with missing ```LotFrontage```, we can lookup which ```Neighborhood``` it belongs to and impute the median ```LotFrontage``` for that ```Neighborhood```.\n\n```MiscFeature``` is missing in the majority of observations, but after some investigation turns out to contain information on sheds - namely if the the house has one. We make a new feature ```Shed?``` and drop ```MiscFeature```.\n\n```MasVnrType``` contains redundant values **None** and **NA**, so we get rid of the **None**.\n\n```GarageYrBlt``` is difficult to impute. There are a lot of missing values, which likely stems from the home not having a garage. So what do we impute? 0? The mean? The build year of the home? These don't make much sense. In the end, we drop ```GarageYrBlt```. The main information encoded here is whether or not the home has a garage, and this information is captured in the other garage predictors.\n\n```Condition2``` and ```Functional``` have very little relation to sale price, so we drop them, too.\n\nIn terms of feature engineering, only one came to mind: quality square footage. Raw square footage is a good predictor, but it has some disadvantages. A big house can be of poor quality (think an old rundown mansion), the same way a smaller house can be of very fine quality. Combining ```OverallQual``` and ```GrLivArea``` into one feature, ```QualitySF```, is useful to us.\n\nThe rest of the imputation is done on a by-case basis, explained in comments in the code below.\n\nFor any feature describing area, we take the square root to get a better linear predictor.\n\nNext, we apply a box-cox transformation to all skewed features.\n\nWe do one final imputation pass for all features in case we missed any, filling 0 in numerical features and **NA** in categorical ones.\n\nFinally, we use ```RobustScaler``` on numerical features and we one-hot encode the categoricals.\n\nThe resulting ```DataFrame``` is cleaned and processed and ready for modelling."}}