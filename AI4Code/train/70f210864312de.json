{"cell_type":{"4c5c5168":"code","ca959cfa":"code","3d8e7d77":"code","cfe7ccb8":"code","bcb07e8e":"code","4d9b09fc":"code","c3506495":"code","0129c145":"code","c4aea752":"code","fbd69ef0":"code","ec8243a4":"code","d267bbdf":"code","fa9d2b21":"code","b87dc2dd":"code","101f91a0":"code","e715e3e6":"code","3a5822dc":"code","b59195e9":"code","e025636a":"code","9732821c":"code","02087fa7":"code","337cefe0":"code","19f70ba3":"code","0ea11e79":"code","15359064":"code","9a14a2fc":"code","8d53bbfc":"code","9225f473":"code","8ba0ea61":"code","b173ee03":"code","9f3379bd":"code","2a3d9e70":"code","289eb46e":"code","eeb1ae16":"code","3de01560":"code","ba2c278b":"code","ea774102":"code","0bef2104":"code","2323d132":"code","2b5e83d8":"code","6d6a50a0":"code","71fa23ae":"code","a91edc58":"code","028907c3":"code","1b46812c":"code","275d5373":"code","f473996d":"code","413bb910":"code","ae0547f5":"code","4621ccab":"code","7d071f3d":"code","4037d487":"code","4ff56059":"markdown","cc23422e":"markdown","ba4196b7":"markdown","f9feece5":"markdown","5361ef70":"markdown","913a086a":"markdown","43ecaa0d":"markdown","3217425d":"markdown","2b5c5f98":"markdown"},"source":{"4c5c5168":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ca959cfa":"# linear algebra\nimport numpy as np \n\n# data processing\nimport pandas as pd \n\n# data visualization\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score","3d8e7d77":"#load data\ntrain_df=pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_df=pd.read_csv(\"..\/input\/titanic\/test.csv\")","cfe7ccb8":"#printing 5 rows of data\ntrain_df.head()","bcb07e8e":"train_df.info() #to get a quick description of data","4d9b09fc":"train_df.describe()","c3506495":"#Handling the missing value\ntrain_df.isnull().sum()","0129c145":"test_df.isnull().sum()","c4aea752":"sns.heatmap(train_df.isnull(),yticklabels=False,cmap='viridis')","fbd69ef0":"train_df.isnull().mean()","ec8243a4":"test_df.isnull().mean()","d267bbdf":"sns.countplot(x=\"Survived\",data=train_df)#Analysing wheather the data is imbalance or not","fa9d2b21":"sns.countplot(x='Survived',hue='Sex',data=train_df)","b87dc2dd":"sns.countplot(x='Survived',hue='Pclass',data=train_df)","101f91a0":"sns.countplot(x='Survived',hue='Embarked',data=train_df)","e715e3e6":"train_df = train_df.drop(\"PassengerId\", axis=1)","3a5822dc":"#filling the missing value with random sample imputation\nfor df in [train_df, test_df]:\n    df[\"Age_random\"]=df[\"Age\"]\n    random_sample=df[\"Age\"].dropna().sample(df[\"Age\"].isnull().sum())\n    random_sample.index=df[df[\"Age\"].isnull()].index\n    df.loc[df[\"Age\"].isnull(),\"Age_random\"]=random_sample","b59195e9":"data=[train_df,test_df]\nfor dataset in data:\n    dataset[\"Embarked\"].fillna(dataset['Embarked'].value_counts().index[0],inplace=True)#replacing the missing values with most frequent value(mode)","e025636a":"data=[train_df,test_df]\nfor dataset in data:\n    dataset[\"Cabin_val\"]=np.where(dataset[\"Cabin\"].isnull(),1,0)\n    dataset[\"Cabin_cap\"]=dataset[\"Cabin\"].fillna(dataset['Cabin'].value_counts().index[0])","9732821c":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","02087fa7":"sns.heatmap(train_df.isnull(),yticklabels=False,cmap='viridis')","337cefe0":"#Creating a new feature \"Fare_Per_Person\" which can be useful\n\ndata = [train_df, test_df]\nfor dataset in data:\n    dataset['Fare_Per_Person'] = dataset['Fare']\/(dataset['SibSp'] + dataset['Parch']+1)\n    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)","19f70ba3":"train_df.corr()\nsns.heatmap(train_df.corr())","0ea11e79":"#Feature scaling\nfrom sklearn.preprocessing import StandardScaler\nstd=StandardScaler()\ncolum_to_scale=['Age_random','Fare_Per_Person','Parch','SibSp']\ndata = [train_df, test_df]\nfor dataset in data:\n    dataset[colum_to_scale]=std.fit_transform(dataset[colum_to_scale])","15359064":"\ndata = [train_df, test_df]\nfor dataset in data:\n    dataset.drop(['Name','Ticket','Age','Cabin','Fare'],axis=1,inplace=True)","9a14a2fc":"#mean encoding\ntrain_df['Cabin_cap']=train_df[\"Cabin_cap\"].astype(str).str[0]\ntest_df['Cabin_cap']=test_df[\"Cabin_cap\"].astype(str).str[0]\nmean=train_df.groupby(['Cabin_cap'])['Survived'].mean().to_dict()\ntrain_df[\"Cabin_cap\"]=train_df[\"Cabin_cap\"].map(mean)\ntest_df[\"Cabin_cap\"]=test_df[\"Cabin_cap\"].map(mean)\n","8d53bbfc":"#one hot encoding\ngender = pd.get_dummies(train_df['Sex'],drop_first=True)\nembarked= pd.get_dummies(train_df['Embarked'],drop_first=True)\ntrain_df = pd.concat([train_df,gender,embarked],axis=1)\ntrain_df.drop(['Sex','Embarked'],axis=1,inplace=True)\n\ngender1 = pd.get_dummies(test_df['Sex'],drop_first=True)\nembarked1= pd.get_dummies(test_df['Embarked'],drop_first=True)\ntest_df = pd.concat([test_df,gender1,embarked1],axis=1)\ntest_df.drop(['Sex','Embarked'],axis=1,inplace=True)","9225f473":"#separating the dependent and independent features\nX = train_df.drop(\"Survived\", axis=1)\nY = train_df[\"Survived\"]\ntest_X  = test_df.drop(\"PassengerId\", axis=1).copy()","8ba0ea61":"#split the data into test and training\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.25,random_state=42)","b173ee03":"#Stochastic Gradient Descent (SGD):\nSGD = SGDClassifier(max_iter=5, tol=None)\nSGD.fit(X_train, Y_train)\nY_pred1 = SGD.predict(X_test)\n\nSGD.score(X_train, Y_train)\n\naccuracy_SGD = round(SGD.score(X_train, Y_train) * 100, 2)\n\ntest_accuracy_SGD = round(accuracy_score(Y_test,Y_pred1)*100,2)","9f3379bd":"#RANDOM fOREST CLASSIFIER\nrandom_forest = RandomForestClassifier()\nrandom_forest.fit(X_train, Y_train)\n\nY_pred2 = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\naccuracy_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\ntest_accuracy_random = round(accuracy_score(Y_test,Y_pred2)*100,2)","2a3d9e70":"#LOGISTIC REGRESSION\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nY_pred3 = logreg.predict(X_test)\n\naccuracy_logreg = round(logreg.score(X_train, Y_train) * 100, 2)\ntest_accuracy_log = round(accuracy_score(Y_test,Y_pred3)*100,2)","289eb46e":"#K NEAREST NEIGHBOUR\nknn = KNeighborsClassifier(n_neighbors = 4)\nknn.fit(X_train, Y_train)\nY_pred4 = knn.predict(X_test)\naccuracy_knn = round(knn.score(X_train, Y_train) * 100, 2)\ntest_accuracy_knn = round(accuracy_score(Y_test,Y_pred4)*100,2)","eeb1ae16":"#GAUSSIAN NEIVE BAYS\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred5 = gaussian.predict(X_test)\naccuracy_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\ntest_accuracy_gaussian = round(accuracy_score(Y_test,Y_pred5)*100,2)","3de01560":"#LINEAR SUPPORT VECTOR MACHINE\nSVC = LinearSVC()\nSVC.fit(X_train, Y_train)\n\nY_pred6 = SVC.predict(X_test)\n\naccuracy_svc = round(SVC.score(X_train, Y_train) * 100, 2)\ntest_accuracy_svc = round(accuracy_score(Y_test,Y_pred6)*100,2)","ba2c278b":"#DECISION TREE\ndtree = DecisionTreeClassifier()\ndtree.fit(X_train, Y_train)\nY_pred7 = dtree.predict(X_test)\naccuracy_dtree = round(dtree.score(X_train, Y_train) * 100, 2)\ntest_accuracy_dtree = round(accuracy_score(Y_test,Y_pred7)*100,2)","ea774102":"train_result = pd.DataFrame({\n    'ML Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', \n              'Stochastic Gradient Decent', \n              'Decision Tree'],\n    'Score': [accuracy_SGD, accuracy_knn, accuracy_logreg, \n              accuracy_random_forest, accuracy_gaussian, accuracy_svc, test_accuracy_dtree]})\ntrain_result = train_result.sort_values(by='Score', ascending=False)\ntrain_result.head(9)","0bef2104":"test_result = pd.DataFrame({\n    'ML Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', \n              'Stochastic Gradient Decent', \n              'Decision Tree'],\n    'Score': [test_accuracy_SGD, test_accuracy_knn, test_accuracy_log, \n              test_accuracy_random, test_accuracy_gaussian, test_accuracy_svc, accuracy_dtree]})\ntest_result = test_result.sort_values(by='Score', ascending=False)\ntest_result.head(9)","2323d132":"from sklearn.model_selection import cross_val_score\nrd = RandomForestClassifier()\nscores = cross_val_score(rd, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","2b5e83d8":"Imp= pd.DataFrame({'Feature':X_train.columns,'Importance':np.round(random_forest.feature_importances_,3)})\nImp = Imp.sort_values('Importance',ascending=False)\nImp.head(8)","6d6a50a0":"rd.get_params().keys()","71fa23ae":"from sklearn.model_selection import RandomizedSearchCV\nparam_test1 = {\n    'n_estimators': [100,200,300,350,500,750, 1000],\n    'criterion' : ['gini', 'entropy'], \n    'max_depth':[5,10,15,20,25,30],    \n    'min_samples_leaf' : [1,2,5, 10], \n    'min_samples_split' : [1,2, 10,25]\n}\ngsearch1 = RandomizedSearchCV(estimator = rd,param_distributions = param_test1,scoring='roc_auc', cv=5)\n\ngsearch1.fit(X_train, Y_train)","a91edc58":"gsearch1.best_params_","028907c3":"gsearch1.best_estimator_","1b46812c":"rd1=gsearch1.best_estimator_ #RandomForestClassifier(min_samples_leaf=2,min_samples_split=10, n_estimators=500)\nrd1.fit(X_train, Y_train)\npredictions = rd1.predict(X_test)\nprint(predictions)","275d5373":"from sklearn.model_selection import cross_val_predict\ny_train_pred = cross_val_predict(rd1, X_train, Y_train, cv=10)\nconfusion_matrix(Y_train, y_train_pred)","f473996d":"from sklearn.metrics import precision_score, recall_score\n\nprint(\"Precision:\", precision_score(Y_train, y_train_pred))\nprint(\"Recall:\",recall_score(Y_train, y_train_pred))","413bb910":"from sklearn.metrics import precision_recall_curve\n\n# getting the probabilities of our predictions\ny_scores = rd1.predict_proba(X_train)\ny_scores = y_scores[:,1]\n\nprecision, recall, threshold = precision_recall_curve(Y_train, y_scores)\ndef plot_precision_and_recall(precision, recall, threshold):\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth=5)\n    plt.plot(threshold, recall[:-1], \"b\", label=\"recall\", linewidth=5)\n    plt.xlabel(\"threshold\", fontsize=19)\n    plt.legend(loc=\"upper right\", fontsize=19)\n    plt.ylim([0, 1])\n\nplt.figure(figsize=(14, 7))\nplot_precision_and_recall(precision, recall, threshold)\nplt.show()","ae0547f5":"from sklearn.metrics import roc_curve\n# compute true positive rate and false positive rate\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(Y_train, y_scores)\n# plotting them against each other\ndef plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n    plt.ylabel('True Positive Rate (TPR)', fontsize=16)\n\nplt.figure(figsize=(14, 7))\nplot_roc_curve(false_positive_rate, true_positive_rate)\nplt.show()","4621ccab":"from sklearn.metrics import roc_auc_score\nr_a_score = roc_auc_score(Y_train, y_scores)\nprint(\"ROC-AUC-Score:\", r_a_score)","7d071f3d":"#predicting the test data\npred_y=rd1.predict(test_X).astype(int)","4037d487":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": pred_y})\nsubmission.to_csv('submission.csv', index=False)","4ff56059":"# Importing necessary libraries","cc23422e":"# Hyperparameter Tuning","ba4196b7":"# Exploratory Data Analysis","f9feece5":"# Data Preprocessing","5361ef70":"We can use seaborn to crate heatmap to check where we have missing values.","913a086a":"# Building Machine Learning Models","43ecaa0d":"Dropping the passenger_id from training dataset because it dosent contribute to the passenger survival probability","3217425d":"****From the above plot we can be concluded that:\n\n1.The Survival rate of women passenger is higher than men passenger\n\n2.The Survival rate of the passenger from higher class is comparatively more than other classes\n\n3.The passengers who embarked from the port \u2018Southampton' having higher chances of survival","2b5c5f98":"![](http:\/\/)In training dataser approximately 20 percent of Age data is missing. We have to deal with 177 missing value which can be tricky. But 77 percent of Cabin data is missing which is very large .The Embarked feature has only 2 missing values, which can easily be filled."}}