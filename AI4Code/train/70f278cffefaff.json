{"cell_type":{"7b6a88d6":"code","fc9e0608":"code","95e5ea81":"code","9395fefb":"code","bc940ef3":"code","6d019ee4":"code","c0b0f096":"code","75f24da5":"code","60da0a42":"code","1253ef1a":"code","bf1b0310":"code","e1bc8361":"code","3aea86da":"code","3aa9197c":"code","cbc85cdf":"code","c5ef8a5e":"code","835412d5":"code","495bc6b6":"code","f70d97dc":"code","cba710ea":"code","710f56d5":"code","0c2c5911":"code","79a4f7cc":"code","9bdd0cbb":"code","58de887a":"code","278ae782":"code","2ff4279c":"code","f40b2913":"code","5086a5d3":"code","f9d08b8d":"code","22023bd6":"code","14d0ddcc":"code","c658389d":"code","58e6a8c1":"code","838277e2":"code","29e6e58d":"code","76cdcc92":"code","308e5496":"code","682e4701":"code","7d195f6f":"code","26c76123":"code","4d569b10":"markdown","dedcf850":"markdown","e5e6d60e":"markdown","05552e86":"markdown","c7ee4079":"markdown","3adfb6f2":"markdown","b2e79026":"markdown","6655e0a4":"markdown","edaa72bb":"markdown","dd1f09b6":"markdown","805a557f":"markdown","b49cc886":"markdown","4b51aafe":"markdown","a0fefc3e":"markdown","b45dd483":"markdown","93114adb":"markdown","d83265a5":"markdown","84b41117":"markdown"},"source":{"7b6a88d6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fc9e0608":"df = pd.read_csv('..\/input\/covid19-case-surveillance-public-use-dataset\/COVID-19_Case_Surveillance_Public_Use_Data.csv')","95e5ea81":"df.head()","9395fefb":"df.shape","bc940ef3":"df = df.drop(['pos_spec_dt','onset_dt'],axis =1)","6d019ee4":"df = df.dropna()","c0b0f096":"df.isnull().sum()","75f24da5":"df.shape","60da0a42":"df.info()","1253ef1a":"colums = ['current_status', 'sex', 'age_group', 'Race and ethnicity (combined)', 'hosp_yn','icu_yn', 'death_yn', 'medcond_yn']\nfor col in colums:\n    print(col)\n    print(df[colums].value_counts())\n    print(\"______________________\")","bf1b0310":"df.describe().T","e1bc8361":"#Unique values in data\ndf.nunique()","3aea86da":"plt.figure(figsize=(30,10))\nplt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9,\n                      wspace=0.5, hspace=0.2)\nplt.subplot(141)\nplt.title('medcond_yn ',fontsize = 20)\ndf['medcond_yn'].value_counts().plot.pie(autopct=\"%1.1f%%\")","3aa9197c":"plt.figure(figsize=(30,10))\nplt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9,\n                      wspace=0.5, hspace=0.2)\nplt.subplot(141)\nplt.title('death_yn',fontsize = 20)\ndf['death_yn'].value_counts().plot.pie(autopct=\"%1.1f%%\")","cbc85cdf":"plt.figure(figsize=(30,10))\nplt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9,\n                      wspace=0.5, hspace=0.2)\nplt.subplot(141)\nplt.title('hosp_yn',fontsize = 20)\ndf['hosp_yn'].value_counts().plot.pie(autopct=\"%1.1f%%\")","c5ef8a5e":"plt.figure(figsize=(30,10))\nplt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9,\n                      wspace=0.5, hspace=0.2)\nplt.subplot(141)\nplt.title('icu_yn',fontsize = 20)\ndf['icu_yn'].value_counts().plot.pie(autopct=\"%1.1f%%\")","835412d5":"plt.figure(figsize=(30,10))\nplt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9,wspace=0.5, hspace=0.2)\nplt.subplot(141)\nplt.title('Race and ethnicity (combined)',fontsize = 20)\ndf['Race and ethnicity (combined)'].value_counts().plot.pie(autopct=\"%1.1f%%\")","495bc6b6":"plt.figure(figsize=(30,10))\nplt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9,\n                      wspace=0.5, hspace=0.2)\nplt.subplot(141)\nplt.title('current_status',fontsize = 20)\ndf['current_status'].value_counts().plot.pie(autopct=\"%1.1f%%\")","f70d97dc":"plt.figure(figsize=(10,6))\nsns.countplot(y=\"current_status\",hue ='sex',data=df)","cba710ea":"plt.figure(figsize=(10,6))\nsns.countplot(y=\"medcond_yn\",hue ='sex',data=df)","710f56d5":"plt.figure(figsize=(10,6))\nsns.countplot(y=\"death_yn\",hue ='sex',data=df)","0c2c5911":"plt.figure(figsize=(10,6))\nsns.countplot(y=\"age_group\",data=df)","79a4f7cc":"plt.figure(figsize=(10,8))\nsns.countplot(y=\"age_group\",hue ='sex',data=df)","9bdd0cbb":"plt.figure(figsize=(10,8))\nsns.catplot(x=\"age_group\", hue=\"sex\", col=\"current_status\", data=df, kind=\"count\", height=5, aspect=.7);","58de887a":"print(\"Start Date:\", df['cdc_report_dt'].min())\nprint(\"End Date:\", df['cdc_report_dt'].max())","278ae782":"df['age_group'].value_counts()","2ff4279c":"data = df.copy()","f40b2913":"data['sex'].value_counts()","5086a5d3":"from sklearn.preprocessing import LabelEncoder\n\nlb_make = LabelEncoder()","f9d08b8d":"# mapp = {'Female':1,'Male':2,'Unknown':3,'Missing':4,'Other':5}\n# data['sex'] = data['sex'].apply(lambda x:mapp[x])\n\ndata['sex'] = data['sex'].map({'Female':0,'Male':1,'Unknown':2,'Missing':3,'Other':4})\nprint(data.head())","22023bd6":"data[\"current_status\"] = lb_make.fit_transform(data[\"current_status\"])\ndata[\"hosp_yn\"] = lb_make.fit_transform(data[\"hosp_yn\"])\ndata[\"icu_yn\"] = lb_make.fit_transform(data[\"icu_yn\"])\ndata[\"death_yn\"] = lb_make.fit_transform(data[\"death_yn\"])\ndata[\"medcond_yn\"] = lb_make.fit_transform(data[\"medcond_yn\"])","14d0ddcc":"data.head()","c658389d":"data.shape","58e6a8c1":"X =  data[['current_status','hosp_yn','icu_yn','medcond_yn','sex']]\ny = data['death_yn']","838277e2":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","29e6e58d":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, y_train) * 100, 2)\nacc_log","76cdcc92":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\nacc_decision_tree","308e5496":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Decision Tree'],\n    'Score': [ acc_log, acc_decision_tree]})\nsorted_model=models.sort_values(by='Score', ascending=False)\nsorted_model","682e4701":"plt.figure(figsize=(20,10))\nfig = plt.bar(sorted_model['Model'], sorted_model['Score'],color='aqua')\nplt.grid()\nplt.show()","7d195f6f":"# # Logistic Regression\n\n# logreg = LogisticRegression()\n# logreg.fit(X_train, Y_train)\n# Y_pred = logreg.predict(X_test)\n# acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n# acc_log\n\n# # Support Vector Machines\n\n# svc = SVC()\n# svc.fit(X_train, Y_train)\n# Y_pred = svc.predict(X_test)\n# acc_svc = round(svc.score(X_train, Y_train) * 100, 2)\n# acc_svc\n\n\n# knn = KNeighborsClassifier(n_neighbors = 3)\n# knn.fit(X_train, Y_train)\n# Y_pred = knn.predict(X_test)\n# acc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n# acc_knn\n\n# # Gaussian Naive Bayes\n\n# gaussian = GaussianNB()\n# gaussian.fit(X_train, Y_train)\n# Y_pred = gaussian.predict(X_test)\n# acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\n# acc_gaussian\n\n\n# # Perceptron\n\n# perceptron = Perceptron()\n# perceptron.fit(X_train, Y_train)\n# Y_pred = perceptron.predict(X_test)\n# acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\n# acc_perceptron\n\n\n\n# # Linear SVC\n\n# linear_svc = LinearSVC()\n# linear_svc.fit(X_train, Y_train)\n# Y_pred = linear_svc.predict(X_test)\n# acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\n# acc_linear_svc\n\n\n# # Stochastic Gradient Descent\n\n# sgd = SGDClassifier()\n# sgd.fit(X_train, Y_train)\n# Y_pred = sgd.predict(X_test)\n# acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\n# acc_sgd\n\n\n# # Decision Tree\n\n# decision_tree = DecisionTreeClassifier()\n# decision_tree.fit(X_train, Y_train)\n# Y_pred = decision_tree.predict(X_test)\n# acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n# acc_decision_tree\n\n\n# # Random Forest\n\n# random_forest = RandomForestClassifier(n_estimators=100)\n# random_forest.fit(X_train, Y_train)\n# Y_pred = random_forest.predict(X_test)\n# random_forest.score(X_train, Y_train)\n# acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n# acc_random_forest\n\n# models = pd.DataFrame({\n#     'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n#               'Random Forest', 'Naive Bayes', 'Perceptron', \n#               'Stochastic Gradient Decent', 'Linear SVC', \n#               'Decision Tree'],\n#     'Score': [acc_svc, acc_knn, acc_log, \n#               acc_random_forest, acc_gaussian, acc_perceptron, \n#               acc_sgd, acc_linear_svc, acc_decision_tree]})\n# sorted_model=models.sort_values(by='Score', ascending=False)\n# sorted_model\n","26c76123":"# submission = pd.DataFrame({\n#         \"PatientId\": test_df[\"patient\"],\n#         \"death_yn\": Y_pred\n#     })\n# submission.to_csv('submission2.csv', index=False)","4d569b10":"Age group from 20-29 has the highest count","dedcf850":"From the above pie charts we can see there are lot of missing data to conclude.","e5e6d60e":"Logistic Regression is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution","05552e86":"Dropping the date columns and then the null values from dataset","c7ee4079":"## Data Preparation For Model","3adfb6f2":"Death ratio of male and female are same, while missing rate is highest","b2e79026":"### Converting categorical feature to numeric","6655e0a4":"Pie chart on the basis of Ethnicity\/Race","edaa72bb":"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees.\n\nThe model confidence score is the highest among models evaluated so far.","dd1f09b6":"#### Model evaluation\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Logisitic Regression score the different, we choose to use Decision Tree, habit of overfitting to their training set.","805a557f":"Converting a categorical feature\nNow we can convert features which contain strings to numerical values. This is required by most model algorithms. Doing so will also help us in achieving the feature completing goal.\n\nLet us start by converting Sex feature to a new feature called Gender where female=0 and male=1 and so on.","b49cc886":"Age Group with respect to their gender.","4b51aafe":"## Some Interesting Insights from Visualization","a0fefc3e":"## Model Building with Logistic Regression and Decision Tree:","b45dd483":"### Load the data","93114adb":"### Please upvote, if it helps :)","d83265a5":"Medical condition with respect to gender, while we have lot of missing data","84b41117":"## Can Try with below Models as well."}}