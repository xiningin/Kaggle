{"cell_type":{"4b0c69b3":"code","75b6ff7f":"code","1666a48e":"code","0c8e8e6d":"code","519d6c88":"code","82865d5c":"code","c02a21f3":"code","df01a02f":"code","61b25aaa":"code","1918abd6":"code","0a2e6ba0":"code","a6f344ba":"code","a53b69f1":"code","2064a2d9":"code","859e9478":"code","887387d5":"code","706a2256":"code","da2be883":"code","0906a95c":"code","00be3d2a":"code","5aad42a1":"code","17fa27c6":"code","8f78ddae":"code","c503a18a":"code","504ec92a":"code","e0d2f99d":"code","3b622847":"code","86920736":"code","d509c4b3":"code","ba15268c":"code","20a56659":"code","153017a3":"code","ddf3ce3d":"markdown","880fe755":"markdown","29f56652":"markdown","6057c18f":"markdown","cb9cf478":"markdown","c7ffe0ca":"markdown","b34c5204":"markdown","1d6ac745":"markdown","8f247aa6":"markdown"},"source":{"4b0c69b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","75b6ff7f":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np \nimport pandas as pd \n\ntrain = pd.read_csv('..\/input\/train.tsv',sep = '\\t')\ntest = pd.read_csv('..\/input\/test.tsv', sep = '\\t')\nprint(\"Train set: {0}\".format(train.shape))\nprint(\"Test set: {0}\".format(test.shape))\n\ndf = pd.concat([train, test])\nprint(\"All df set: {0}\".format(df.shape))\n\ndf.head()","1666a48e":"sub = pd.read_csv('..\/input\/sampleSubmission.csv', sep = ',')\nprint(\"Submission: {0}\".format(sub.shape))\n\nsub.head()","0c8e8e6d":"x = train.groupby(['Sentiment'])['PhraseId'].count()\nx.plot.bar()","519d6c88":"print(\"Training set distribution: \", train.groupby(['Sentiment']).size()\/train.shape[0])","82865d5c":"import re\nfrom nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\n\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()","c02a21f3":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"[-()\\\"#\/@;:<>{}+=~|.?,]\", \"\", text)\n    review_lemma=[]\n    for word in text.split():\n        word_lemma = wordnet_lemmatizer.lemmatize(word)\n        review_lemma.append(word_lemma)\n    review_lemma=' '.join(review_lemma)\n    return review_lemma","df01a02f":"train['clean_phrase'] = train['Phrase'].apply(clean_text)\ntest['clean_phrase'] = test['Phrase'].apply(clean_text)\ndf['clean_phrase'] = df['Phrase'].apply(clean_text)","61b25aaa":"train.head()","1918abd6":"from keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize\nfrom nltk import FreqDist","0a2e6ba0":"train_text=train.clean_phrase.values\ntest_text=test.clean_phrase.values\ntarget=train.Sentiment.values\ny=to_categorical(target)\nprint(train_text.shape,target.shape,y.shape)","a6f344ba":"X_train_text,X_val_text,y_train,y_val=train_test_split(train_text,y,test_size=0.2,stratify=y,random_state=123)\nprint(X_train_text.shape,y_train.shape)\nprint(X_val_text.shape,y_val.shape)","a53b69f1":"all_words = ' '.join(X_train_text)\nword2count = {}\nfor word in all_words.split():\n    if word not in word2count:\n        word2count[word] = 1\n    else:\n        word2count[word] += 1\nprint(\"Number of unique words: \", len(word2count.keys()))","2064a2d9":"df['length_review'] = df['clean_phrase'].apply(lambda x: len(x.split()))\nprint(\"Max phrase length: \", max(df['length_review']))","859e9478":"d = pd.DataFrame(list(word2count.items()), columns=['word', 'count'])\nd.head()","887387d5":"all_phrases = [X_train_text]\nall_phrases","706a2256":"# from sklearn.feature_extraction.text import TfidfTransformer\n\n# sklearn_tfidf = TfidfTransformer()\n# sklearn_representation = sklearn_tfidf.fit_transform(all_phrases)","da2be883":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","0906a95c":"MAX_REVIEW_LENGTH = 49\nFEATURE_LENGTH = 12011\nBATCH_SIZE = 1000\nEPOCHS = 100\nNUM_CLASSES = 5","00be3d2a":"tokenizer = Tokenizer(num_words = FEATURE_LENGTH)\ntokenizer.fit_on_texts(list(np.concatenate((train_text, test_text), axis=0)))\nX_train = tokenizer.texts_to_sequences(X_train_text)\nX_val = tokenizer.texts_to_sequences(X_val_text)\nX_test = tokenizer.texts_to_sequences(test_text)","5aad42a1":"X_train = pad_sequences(X_train, maxlen=MAX_REVIEW_LENGTH)\nX_val = pad_sequences(X_val, maxlen=MAX_REVIEW_LENGTH)\nX_test= pad_sequences(X_test, maxlen=MAX_REVIEW_LENGTH)","17fa27c6":"from keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Embedding,LSTM,Conv1D,GlobalMaxPooling1D\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adam","8f78ddae":"model=Sequential()\nmodel.add(Embedding(FEATURE_LENGTH,250,mask_zero=True))\nmodel.add(LSTM(128,dropout=0.4, recurrent_dropout=0.4,return_sequences=True))\nmodel.add(LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\nmodel.add(Dense(NUM_CLASSES,activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.001),metrics=['accuracy'])\nmodel.summary()","c503a18a":"history = model.fit(X_train, y_train, validation_data=(X_val, y_val),epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)","504ec92a":"y_pred = model.predict_classes(X_test)","e0d2f99d":"test[\"Sentiment\"] = y_pred","3b622847":"test[['PhraseId', 'Sentiment']].to_csv('submission_lstm.csv', index = False)","86920736":"from keras.layers import Flatten","d509c4b3":"ann_model = Sequential()\nann_model.add(Embedding(FEATURE_LENGTH,250, input_length=MAX_REVIEW_LENGTH))\nann_model.add(Dense(output_dim = 100, init = 'uniform', activation = 'relu'))\nann_model.add(Flatten())\nann_model.add(Dense(output_dim = 50, activation='tanh'))\nann_model.add(Dense(output_dim = 10, activation = 'relu'))\nann_model.add(Dense(NUM_CLASSES,activation='softmax'))\nann_model.compile(optimizer=Adam(lr=0.001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\nann_model.summary()","ba15268c":"ann_history = ann_model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size = BATCH_SIZE, epochs = EPOCHS)","20a56659":"y_pred = ann_model.predict_classes(X_test)","153017a3":"test[\"Sentiment\"] = y_pred\ntest[['PhraseId', 'Sentiment']].to_csv('submission_ann.csv', index = False)","ddf3ce3d":"## Feature Engineering: tf-idf","880fe755":"Let's see the distribution of the each group","29f56652":"## Tokenizer and Sequence padding","6057c18f":"## Import Data","cb9cf478":"## Count Features","c7ffe0ca":"## Clean data","b34c5204":"## LSTM Model","1d6ac745":"## Random Forest (baseline)","8f247aa6":"## ANN"}}