{"cell_type":{"5f75e4da":"code","00eaf0e7":"code","a276eca7":"code","c39b3760":"code","8efd5be6":"code","35fc833a":"code","18939d47":"code","4e78aab9":"code","9439b405":"code","e1a6bb4d":"code","73ed32c3":"code","fe9c02f6":"code","bf92678b":"code","ec0672cf":"code","f7c53bb6":"code","eb545355":"code","c09d2816":"code","753a22ff":"code","b2b24302":"code","043fff59":"code","cf9813c7":"code","1938c651":"code","23d5b9f3":"code","dcc621a8":"code","e2518b77":"code","bcaf7abd":"code","cf0b5f06":"code","9116d934":"code","985c4c31":"markdown","53992d47":"markdown","d9e78f1f":"markdown","cc4556a0":"markdown","85a19fbb":"markdown","f9341cb3":"markdown","7cc6d93a":"markdown","4febd6b5":"markdown","c5f39db5":"markdown","b2b46bed":"markdown"},"source":{"5f75e4da":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n!pip install -U seaborn\nimport seaborn as sns\n%matplotlib inline\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","00eaf0e7":"df=pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/train.csv')","a276eca7":"df_test=pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/test.csv')","c39b3760":"df.info()","8efd5be6":"df.head()","35fc833a":"df.describe()","18939d47":"age=pd.cut(df['Age'],bins=5,labels=['A1','A2','A3','A4','A5'])\nrc=pd.cut(df['Region_Code'],bins=5,labels=['A1','A2','A3','A4','A5'])\nap=pd.cut(df['Annual_Premium'],bins=5,labels=['A1','A2','A3','A4','A5'])\nvin=pd.cut(df['Vintage'],bins=5,labels=['A1','A2','A3','A4','A5'])","4e78aab9":"age2=pd.cut(df_test['Age'],bins=5,labels=['A1','A2','A3','A4','A5'])\nrc2=pd.cut(df_test['Region_Code'],bins=5,labels=['A1','A2','A3','A4','A5'])\nap2=pd.cut(df_test['Annual_Premium'],bins=5,labels=['A1','A2','A3','A4','A5'])\nvin2=pd.cut(df_test['Vintage'],bins=5,labels=['A1','A2','A3','A4','A5'])","9439b405":"df['age']=age             \ndf['rc']=rc \ndf['ap']=ap\ndf['vin']=vin","e1a6bb4d":"df_test['age2']=age2       \ndf_test['rc2']=rc2\ndf_test['ap2']=ap2\ndf_test['vin2']=vin2","73ed32c3":"df_dum=pd.get_dummies(df[['age','rc','ap','vin','Gender','Vehicle_Age','Vehicle_Damage','Vehicle_Damage']])\ndf_dum2=pd.get_dummies(df_test[['age2','rc2','ap2','vin2','Gender','Vehicle_Age','Vehicle_Damage','Vehicle_Damage']])","fe9c02f6":"df=pd.concat([df,df_dum],axis=1)\ndf_test=pd.concat([df_test,df_dum2],axis=1)","bf92678b":"df.drop(['Gender','Age','age','Region_Code','rc','Vehicle_Age','Vehicle_Damage','Annual_Premium','ap','Vintage','vin'],axis=1,inplace=True)\ndf_test.drop(['Gender','Age','age2','Region_Code','rc2','Vehicle_Age','Vehicle_Damage','Annual_Premium','ap2','Vintage','vin2'],axis=1,inplace=True)","ec0672cf":"from sklearn.preprocessing import StandardScaler as sc","f7c53bb6":"scaler=sc()\nscaled_df=scaler.fit_transform(df.drop(['id','Response'],axis=1))\nscaled_dft=scaler.fit_transform(df_test.drop('id',axis=1))","eb545355":"from sklearn.model_selection import train_test_split\nxtr,xte,ytr,yte=train_test_split(scaled_df,df['Response'],random_state=42,test_size=0.27)","c09d2816":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import RandomizedSearchCV","753a22ff":"sgc=SGDClassifier(penalty='l1',loss='modified_huber',early_stopping=True)\nmodel_params={'alpha':[0.0012,0.0011,0.0013],\n              'learning_rate':['invscaling'],\n              'max_iter':[690,700,710],\n              'validation_fraction':[0.46,0.47,0.48],\n              'eta0':[0.65,0.66,0.64]}\nran=RandomizedSearchCV(sgc,param_distributions=model_params,cv=5,n_jobs=-1,verbose=2,n_iter=100)","b2b24302":"ran.fit(scaled_df,df['Response'])","043fff59":"ran.best_params_","cf9813c7":"ran.best_score_","1938c651":"y_pred=ran.predict(scaled_dft)","23d5b9f3":"submissions=pd.DataFrame({'id':df_test['id'],'Response':y_pred})","dcc621a8":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks","e2518b77":"model=keras.Sequential([layers.Dense(units=1024,activation='relu',input_shape=[32]),\n                        layers.BatchNormalization(),\n                        layers.Dense(units=1024,activation='relu'),\n                        layers.Dropout(0.27),\n                        layers.BatchNormalization(),\n                        layers.Dense(units=1024,activation='relu'),\n                        layers.Dropout(0.27),\n                        layers.BatchNormalization(),\n                        layers.Dense(units=1024,activation='relu'),\n                        layers.Dropout(0.27),\n                        layers.BatchNormalization(),\n                        layers.Dense(units=1024,activation='relu'),\n                        layers.Dropout(0.27),\n                        layers.BatchNormalization(),\n                        layers.Dense(units=1024,activation='relu'),\n                        layers.Dropout(0.27),\n                        layers.BatchNormalization(),\n                        layers.Dense(units=1024,activation='relu'),\n                        layers.Dropout(0.27),\n                        layers.BatchNormalization(),\n                        layers.Dense(units=1,activation='sigmoid')])","bcaf7abd":"model.compile(optimizer='Adam',loss='binary_crossentropy', metrics=['accuracy'])","cf0b5f06":"early_stop=callbacks.EarlyStopping(min_delta=0.001, patience=20, restore_best_weights=True)\nhistory = model.fit(\n    xtr, ytr,\n    validation_data=(xte, yte),\n    batch_size=512,\n    epochs=100,\n    callbacks=[early_stop]\n)","9116d934":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot();\nprint(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))","985c4c31":"# Building our Neural-Net model","53992d47":"## 87.6% Accuracy","d9e78f1f":"# Scaling the data","cc4556a0":"### Creating dummy columns","85a19fbb":"### Dividing the data","f9341cb3":"# Exploring dataset","7cc6d93a":"## Adding columns to our datasets","4febd6b5":"### Creating Bins","c5f39db5":"# Preparing data for fitting the model","b2b46bed":"# Selecting the right model with tuned Hyper-Parameters"}}