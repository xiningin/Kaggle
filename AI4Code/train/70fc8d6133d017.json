{"cell_type":{"cb130abd":"code","c6554058":"code","5edab3aa":"code","bfcc7547":"code","11944697":"code","5fd6760c":"code","66ab865e":"code","0df00433":"code","6eaf7e03":"code","b3631990":"code","28215b60":"code","d90b9fee":"code","17e8dc38":"code","90856057":"code","2ee0e5b7":"code","14b46fb2":"code","75a1a657":"code","b1e2ad9b":"code","a3466e4a":"code","a1daf68e":"code","d7ff6c91":"code","36549894":"code","6ca2f386":"code","330da8c7":"code","cd60d514":"code","c4f10a4b":"code","234b7918":"code","a80ead79":"code","bc48c066":"code","74ab3fb2":"code","9e1dfa65":"code","4aa5171a":"code","799a8252":"code","cadb8675":"code","745494a3":"code","f37817e6":"code","042daba1":"code","2a73fc61":"code","1e760a9b":"code","47ad23a7":"code","9ae1f7eb":"code","5ab1419d":"code","0a9a5f3d":"code","6c1208b7":"code","1ecc4bc4":"code","87d2f625":"code","0cfcbe68":"code","f18195dd":"code","b5fb02c1":"code","2c6b8052":"code","991bca29":"code","96240541":"code","cd977b4d":"code","1bad2607":"code","d2d531b7":"code","96eb7edb":"code","38c78b1e":"code","bdbfa93e":"code","abe72ab2":"code","b0ed94af":"code","8d1c3967":"code","276d7e55":"code","f0983e43":"code","b81d5694":"code","0942f09f":"markdown","e1dfec29":"markdown","768eb1de":"markdown","37db6707":"markdown","09eec639":"markdown","53a62021":"markdown","d55c5d0b":"markdown","dd838ac1":"markdown","2ab765d8":"markdown","7ef768d8":"markdown","47bd0949":"markdown","a1ddb436":"markdown","8c35fe70":"markdown","a1b88944":"markdown","fcca2e60":"markdown","761c896c":"markdown","8ebb416e":"markdown","63908e49":"markdown","9127223b":"markdown","e3674539":"markdown","5c44a0d2":"markdown","655eb786":"markdown","1b16349d":"markdown","64999dd2":"markdown","c66c3e7f":"markdown","4203377f":"markdown","3d7a80e5":"markdown"},"source":{"cb130abd":"# Importing libraries\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.style.use('ggplot')","c6554058":"df = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")","5edab3aa":"df.head()","bfcc7547":"df.drop(['id', 'Unnamed: 32'], axis = 1, inplace = True)","11944697":"df.diagnosis.unique()","5fd6760c":"df['diagnosis'] = df['diagnosis'].apply(lambda val: 1 if val == 'M' else 0)","66ab865e":"df.head()","0df00433":"df.describe()","6eaf7e03":"df.info()","b3631990":"df.shape","28215b60":"#counting duplicate \n\ndf.duplicated().sum()","d90b9fee":"#relating vareiable plot\n\nsns.relplot(x=\"radius_mean\",y=\"texture_mean\",hue=\"diagnosis\",data=df);","17e8dc38":"#aggegation and representing uncertainty\nsns.relplot(x=\"radius_mean\",y=\"texture_mean\",kind=\"line\",data=df)","90856057":"#how does the relationship between two variable chance as fuction of a thired variable\n\nsns.lmplot(x=\"radius_mean\",y=\"texture_mean\",hue=\"diagnosis\",data=df);","2ee0e5b7":"#here we going to plot histogram for see frequency dist\ndf[\"radius_mean\"].plot(kind = 'hist',bins = 200,figsize = (6,6))\nplt.title(\"radius_mean\")\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"Frequency\")\nplt.show()","14b46fb2":"sns.violinplot(x=\"diagnosis\",y=\"radius_mean\",data=df)\nsns.swarmplot(x=\"diagnosis\",y=\"radius_mean\",data=df)","75a1a657":"#here wr checking outliers \nf,ax=plt.subplots()\nsns.violinplot(data=df.iloc[:,1:3])\nsns.despine(offset=10,trim=True)","b1e2ad9b":"f,ax=plt.subplots()\nsns.violinplot(data=df.iloc[:,6:8])\nsns.despine(offset=10,trim=True)","a3466e4a":"f,ax=plt.subplots()\nsns.violinplot(data=df.iloc[:,9:11])\nsns.despine(offset=10,trim=True)","a1daf68e":"plt.figure(figsize = (20, 15))\nplotnumber = 1\n\nfor column in df:\n    if plotnumber <= 30:\n        ax = plt.subplot(5, 6, plotnumber)\n        sns.distplot(df[column])\n        plt.xlabel(column)\n        \n    plotnumber += 1\n\nplt.tight_layout()\nplt.show()","d7ff6c91":"# heatmap \n\nplt.figure(figsize = (20, 12))\n\ncorr = df.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\n\nsns.heatmap(corr, mask = mask, linewidths = 1, annot = True, fmt = \".2f\")\nplt.show()","36549894":"# removing highly correlated features\n\ncorr_matrix = df.corr().abs() \n\nmask = np.triu(np.ones_like(corr_matrix, dtype = bool))\ntri_df = corr_matrix.mask(mask)\n\nto_drop = [x for x in tri_df.columns if any(tri_df[x] > 0.92)]\n\ndf = df.drop(to_drop, axis = 1)\n\nprint(f\"The reduced dataframe has {df.shape[1]} columns.\")","6ca2f386":"def mod_outlier(df):\n        df1 = df.copy()\n        df = df._get_numeric_data()\n\n\n        q1 = df.quantile(0.25)\n        q3 = df.quantile(0.75)\n\n        iqr = q3 - q1\n\n        lower_bound = q1 -(1.5 * iqr) \n        upper_bound = q3 +(1.5 * iqr)\n\n\n        for col in df.columns:\n            for i in range(0,len(df[col])):\n                if df[col][i] < lower_bound[col]:            \n                    df[col][i] = lower_bound[col]\n\n                if df[col][i] > upper_bound[col]:            \n                    df[col][i] = upper_bound[col]    \n\n\n        for col in df.columns:\n            df1[col] = df[col]\n\n        return(df1)\n\ndf = mod_outlier(df)","330da8c7":"# creating features and label \n\nX = df.drop('diagnosis', axis = 1)\ny = df['diagnosis']","cd60d514":"# splitting data into training and test set\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)","c4f10a4b":"# scaling data\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","234b7918":"# fitting data to model\n\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)","a80ead79":"# model predictions\n\ny_pred = log_reg.predict(X_test)","bc48c066":"# accuracy score\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nprint(accuracy_score(y_train, log_reg.predict(X_train)))\n\nlog_reg_acc = accuracy_score(y_test, log_reg.predict(X_test))\nprint(log_reg_acc)","74ab3fb2":"# confusion matrix\n\nprint(confusion_matrix(y_test, y_pred))","9e1dfa65":"# classification report\n\nprint(classification_report(y_test, y_pred))","4aa5171a":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)","799a8252":"# model predictions \n\ny_pred = knn.predict(X_test)","cadb8675":"# accuracy score\n\nprint(accuracy_score(y_train, knn.predict(X_train)))\n\nknn_acc = accuracy_score(y_test, knn.predict(X_test))\nprint(knn_acc)","745494a3":"# confusion matrix\n\nprint(confusion_matrix(y_test, y_pred))","f37817e6":"# classification report\n\nprint(classification_report(y_test, y_pred))","042daba1":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\nsvc = SVC()\nparameters = {\n    'gamma' : [0.0001, 0.001, 0.01, 0.1],\n    'C' : [0.01, 0.05, 0.5, 0.1, 1, 10, 15, 20]\n}\n\ngrid_search = GridSearchCV(svc, parameters)\ngrid_search.fit(X_train, y_train)","2a73fc61":"# best parameters\n\ngrid_search.best_params_","1e760a9b":"svc = SVC(C = 15, gamma = 0.01)\nsvc.fit(X_train, y_train)","47ad23a7":"# model predictions \n\ny_pred = svc.predict(X_test)","9ae1f7eb":"# accuracy score\n\nprint(accuracy_score(y_train, svc.predict(X_train)))\n\nsvc_acc = accuracy_score(y_test, svc.predict(X_test))\nprint(svc_acc)","5ab1419d":"# confusion matrix\n\ncf_matrix=confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(10,10))\nsns.heatmap(cf_matrix,annot=True,fmt='d')","0a9a5f3d":"# classification report\n\nprint(classification_report(y_test, y_pred))","6c1208b7":"from sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nparameters = {\n    'alpha' : [0.0001, 0.001, 0.01, 0.1, 1],\n    'loss' : ['hinge', 'log'],\n    'penalty' : ['l1', 'l2']\n}\n\ngrid_search = GridSearchCV(sgd, parameters, cv = 10, n_jobs = -1)\ngrid_search.fit(X_train, y_train)","1ecc4bc4":"# best parameter \n\ngrid_search.best_params_","87d2f625":"sgd = SGDClassifier(alpha = 0.01, loss = 'log', penalty = 'l2')\nsgd.fit(X_train, y_train)","0cfcbe68":"# model predictions \n\ny_pred = sgd.predict(X_test)","f18195dd":"# accuracy score\n\nprint(accuracy_score(y_train, sgd.predict(X_train)))\n\nsgd_acc = accuracy_score(y_test, sgd.predict(X_test))\nprint(sgd_acc)","b5fb02c1":"# classification report\n\nprint(classification_report(y_test, y_pred))","2c6b8052":"from sklearn.ensemble import RandomForestClassifier\n\nrand_clf = RandomForestClassifier(criterion = 'entropy', max_depth = 11, max_features = 'auto',\n                                  min_samples_leaf = 2, min_samples_split = 3, n_estimators = 130)\nrand_clf.fit(X_train, y_train)","991bca29":"y_pred = rand_clf.predict(X_test)","96240541":"# accuracy score\n\nprint(accuracy_score(y_train, rand_clf.predict(X_train)))\n\nran_clf_acc = accuracy_score(y_test, y_pred)\nprint(ran_clf_acc)","cd977b4d":"# classification report\n\nprint(classification_report(y_test, y_pred))","1bad2607":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbc = GradientBoostingClassifier()\n\nparameters = {\n    'loss': ['deviance', 'exponential'],\n    'learning_rate': [0.001, 0.1, 1, 10],\n    'n_estimators': [100, 150, 180, 200]\n}\n\ngrid_search_gbc = GridSearchCV(gbc, parameters, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search_gbc.fit(X_train, y_train)","d2d531b7":"# best parameters \n\ngrid_search_gbc.best_params_","96eb7edb":"gbc = GradientBoostingClassifier(learning_rate = 1, loss = 'exponential', n_estimators = 180)\ngbc.fit(X_train, y_train)","38c78b1e":"y_pred = gbc.predict(X_test)","bdbfa93e":"# accuracy score\n\nprint(accuracy_score(y_train, gbc.predict(X_train)))\n\ngbc_acc = accuracy_score(y_test, y_pred)\nprint(gbc_acc)","abe72ab2":"# classification report\n\nprint(classification_report(y_test, y_pred))","b0ed94af":"from xgboost import XGBClassifier \n\nxgb = XGBClassifier(objective = 'binary:logistic', learning_rate = 0.5, max_depth = 5, n_estimators = 180)\n\nxgb.fit(X_train, y_train)","8d1c3967":"y_pred = xgb.predict(X_test)","276d7e55":"# accuracy score\n\nprint(accuracy_score(y_train, xgb.predict(X_train)))\n\nxgb_acc = accuracy_score(y_test, y_pred)\nprint(xgb_acc)","f0983e43":"# classification report\n\nprint(classification_report(y_test, y_pred))","b81d5694":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'KNN', 'SVC', 'SGD Classifier', 'Random Forest Classifier', \n            'Gradient Boosting Classifier','XgBoost'],\n    'Score': [log_reg_acc, knn_acc, svc_acc, sgd_acc,ran_clf_acc, gbc_acc, xgb_acc]\n})\n\nmodels.sort_values(by = 'Score', ascending = False)","0942f09f":"## If you like my work, please do a upvote.","e1dfec29":"by the above plot you can findout relationship between features and outcome","768eb1de":"Diagnosis (M = malignant, B = benign)\nhere we going to label encoding (M=1 & B=0)","37db6707":"# Support Vector Classifier (SVC)","09eec639":"## Exploratory Data Analysis (EDA)","53a62021":"**using StandardScaler to scal the data**","d55c5d0b":"# Pair plot","dd838ac1":"1st we going to drop Id becouse it does not impact prediction\nand Unnamed: 32 becouse it have lots of missing values","2ab765d8":"## Swarm plot","7ef768d8":"# Extreme Gradient Boosting","47bd0949":"# Welcome! to Breast Cancer Classification \n**Greeting all !**\nHope you all are doing well\n","a1ddb436":"**By the above plot we can see data distribution and also identify outliers**","8c35fe70":"# Logistic Regression","a1b88944":"**Using info() we can see three is no Nan values and all features are numerical**","fcca2e60":"# K Neighbors Classifier (KNN)","761c896c":"# Random Forest Classifier","8ebb416e":"# Gradient Boosting Classifier","63908e49":"##  Importing libraries","9127223b":"**Thank you**","e3674539":"# #  dealing with Outliers","5c44a0d2":"## Data Preprocessing","655eb786":"# SGD Classifier","1b16349d":"## Attribute Information:\n\n-  ID number \n- Diagnosis (M = malignant, B = benign)\n\n### Ten real-valued features are computed for each cell nucleus:\n\n- radius (mean of distances from center to points on the perimeter)\n- texture (standard deviation of gray-scale values)\n- perimeter\n- area\n- smoothness (local variation in radius lengths)\n- compactness (perimeter^2 \/ area - 1.0)\n- concavity (severity of concave portions of the contour)\n- concave points (number of concave portions of the contour)\n- symmetry\n- fractal dimension (\"coastline approximation\" - 1)","64999dd2":"So here you can see three is no duplicate value found","c66c3e7f":"## Load the data","4203377f":"### Best model for diagnosing breast cancer is \"Support Vector Classifier\" with an accuracy of 98.9%.","3d7a80e5":"### We can see that there are many columns which are very highly correlated which causes multicollinearity so we have to remove highly correlated features."}}