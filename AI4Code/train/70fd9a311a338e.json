{"cell_type":{"afc8c691":"code","50379388":"code","6b9ebb24":"code","6d3311c3":"code","31c568f9":"code","dd10b290":"code","ea06fa58":"code","9924fc88":"code","514929c1":"code","99fdd40e":"code","555a1933":"code","98e2603d":"code","4badf4c8":"code","bb8f2673":"code","f7991659":"code","da82e19a":"markdown","c5f6d9cc":"markdown","0c8fc3a0":"markdown","375aad9c":"markdown","f453d6c9":"markdown"},"source":{"afc8c691":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","50379388":"data   = pd.read_csv('..\/input\/metallic-glass-forming\/Metallic_Glass_Forming_with_features.csv')\nTARGET = 'Trg'\n\ndata.head(2)","6b9ebb24":"plt.figure(figsize=(20, 6))\nsns.heatmap(data.corr(), annot=True, vmin=-1, vmax=1)\nplt.title('Correlation')\nplt.show()","6d3311c3":"import re\n\n#data['Material compositions'].apply(lambda x: re.split('(\\d+)', x)[:-1])","31c568f9":"print(f\"There are {data.shape[0]} rows and {data.shape[1]} columns ({data.shape[0]*data.shape[1]} values in total).\")\nprint(f\"There are {data.isna().sum().sum()} missing values.\")","dd10b290":"numericals = data.dtypes[data.dtypes == 'float64'].index.tolist()\ncategoricals = ['main_element']  # Only focus on this one for now\n\nprint(f\"There are {len(numericals)} numerical  variables.\")\nprint(f\"There are {len(categoricals)} categoricl variables.\")\n\nnumericals.remove(TARGET)","ea06fa58":"# One-hot encode categorical variable\ncol  = categoricals[0]\ndata = pd.concat([data.drop(columns=col), pd.get_dummies(data[col])], axis=1)\n\ndata.head(2)","9924fc88":"from sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\n\nSEED  = 42\n\ndef modelling(X, y, model, all_data=False, f_importance=False, fit=False):\n    # Type of modelling : Train & Test basic splitting\n    importance, tt_train_score, tt_test_score  = train_test_model(X, y, model, f_importance=f_importance)\n    \n    # Type of modelling : KFold Train & Test splitting\n    kf_train_score, kf_test_score = kfold_model(X, y, model)\n    \n    if fit:\n        model.fit(X, y)\n        return model, tt_test_score, kf_test_score\n    \n    return importance, tt_test_score, kf_test_score\n\ndef train_test_model(X, y, model, f_importance=True):\n    \n    importance = None\n    \n    # Train & test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=SEED)\n    \n    # Fitting\n    model.fit(X_train, y_train)\n    \n    # Scores\n    train_pred = model.predict(X_train)\n    test_pred  = model.predict(X_test)\n    \n    train_score = mean_squared_error(y_train, model.predict(X_train), squared=False)\n    test_score = mean_squared_error(y_test, model.predict(X_test), squared=False)\n    \n    # Feature importances\n    if f_importance:\n        try:\n            importance = model.feature_importances_\n            features   = X.columns.tolist()\n            importance = pd.Series(index=features, data=importance)\n            return importance, train_score, test_score\n        except:\n            pass\n        \n    # Model, RMSE on train, RMSE on test\n    return importance, train_score, test_score\n\ndef kfold_model(X, y, model):\n    # Parameters & variables\n    K            = 5\n    kf           = KFold(K)\n    train_scores = list() \n    test_scores  = list() \n    \n    # Looping over the folds\n    for train_index, test_index in kf.split(X):\n        \n        # Define datasets\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        \n        # Fitting\n        model.fit(X_train, y_train)\n        \n        # Scores\n        train_pred = model.predict(X_train)\n        test_pred  = model.predict(X_test)\n        \n        train_score = mean_squared_error(y_train, model.predict(X_train), squared=False)\n        test_score = mean_squared_error(y_test, model.predict(X_test), squared=False)\n        \n        # Increments\n        train_scores.append(train_score)\n        test_scores.append(test_score)\n    \n    kf_train_score = np.mean(train_scores)\n    kf_test_score  = np.mean(test_scores)\n    \n    return kf_train_score, kf_test_score","514929c1":"# For display\nSPACE   = 26\ndisplay = lambda: print((SPACE * 3 + 7) * '-')\n\ndef multiple_modelling(X, y, models):\n    display()\n    print(f\"|{'Model'.rjust(SPACE)} |{'Basic RMSE'.rjust(SPACE)} |{'K-Fold CV RMSE'.rjust(SPACE)} |\")\n    display()\n        \n    for model in models:\n        _, basic_score, kfold_score = modelling(X, y, model, f_importance=False)\n        print(f\"|{type(model).__name__.rjust(SPACE)} |{str(round(basic_score, 5)).rjust(SPACE)} |{str(round(kfold_score, 5)).rjust(SPACE)} |\")\n        display()","99fdd40e":"features = data.dtypes[data.dtypes != 'O'].index.tolist()\nfeatures.remove(TARGET)\n\nX = data[features]\ny = data[TARGET]","555a1933":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nmodels = [\n    LinearRegression(),\n    Ridge(),\n    Lasso(),\n    DecisionTreeRegressor(),\n    ExtraTreeRegressor(),\n    AdaBoostRegressor(),\n    GradientBoostingRegressor(),\n    RandomForestRegressor(),\n    XGBRegressor(),\n    LGBMRegressor()\n]\n\nmultiple_modelling(X, y, models)","98e2603d":"# Select a model\nmodel = XGBRegressor()\n\n# Fit the model\nmodel.fit(X, y)","4badf4c8":"plt.figure(figsize=(16, 12))\npd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=True).plot.barh()\nplt.title('Feature importance')\nplt.show()","bb8f2673":"fig, ax = plt.subplots(2, 2, figsize=(20, 12))\n\npreds = model.predict(X)\n\n# First plot\nsns.kdeplot(x=y, alpha=.5, fill=True, label='Real', color='green', ax=ax[0][0])\nsns.kdeplot(x=preds, alpha=.3, fill=True, label='Predicted', color='crimson', ax=ax[0][0])\n\n# Second plot : MeltingT_difference\nsns.scatterplot(x=data['MeltingT_difference'], y=y, label='Real', alpha=0.5, color='green', ax=ax[0][1])\nsns.scatterplot(x=data['MeltingT_difference'], y=preds, label='Predicted', alpha=0.5, color='crimson', ax=ax[0][1])\n\n# Third plot  : IsBoron_composition_average\nsns.scatterplot(x=data['IsBoron_composition_average'], y=y, label='Real', alpha=0.5, color='green', ax=ax[1][0])\nsns.scatterplot(x=data['IsBoron_composition_average'], y=preds, label='Predicted', alpha=0.5, color='crimson', ax=ax[1][0])\n\n# Fourth plot : Density_composition_average\nsns.scatterplot(x=data['Density_composition_average'], y=y, label='Real', alpha=0.5, color='green', ax=ax[1][1])\nsns.scatterplot(x=data['Density_composition_average'], y=preds, label='Predicted', alpha=0.5, color='crimson', ax=ax[1][1])\n\nplt.title('Real vs Predicted')\nplt.show()","f7991659":"data['predictions'] = model.predict(X)\n\ndata[['Material compositions', 'predictions']].to_csv('submission.csv', index=False)","da82e19a":"# Build model","c5f6d9cc":"# Analysis","0c8fc3a0":"# Quick look","375aad9c":"# Metallic Glass Forming\n*Predict the reduced glass transition temperature*\n\n<div style=\"align:center\">\n    <img src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/1157198\/1939968\/b7739e8a2876adfa38ff4cc3ad9bed83\/dataset-cover.jpg?t=2021-02-14-10-30-04\">\n<\/div>\n\n<br>\n\nThis dataset has values of reduced glass transition temperature (Trg) for a variety of metallic alloys. An additional column is included for majority element for each alloy, which can be an interesting property to group on during tests.\n\n\n> X features\n:\nThe metallic glass dataset gives two columns with information about the material\nComposition. The first is the overall composition, and the second is the highest\nComposition element. The columns from four to the end are the MAGPIE features that\nhave been generated from the material composition column and give values such as\nproperties averaged over the material composition as well as features that are only for \nthe majority element in each alloy [3]. The majority element features are labelled as \n\"site1\".\n\n> Y property\n:\nThe reduced glass transition temperature (Trg) has historically been used as a rough \npredictor for Glass Forming Ability (GFA). By making a model to predict Trg for an \narbitrary alloy, it could be possible to use these values to estimate GFA directly, or as \ninput for another model to then predict GFA.\n\n> Acknowledgements\n:\nMorgan, Dane (2018): Machine Learning Materials Datasets. figshare. Dataset. https:\/\/doi.org\/10.6084\/m9.figshare.7017254.v5\n\n> License\n:\nMIT","f453d6c9":"# Setup"}}