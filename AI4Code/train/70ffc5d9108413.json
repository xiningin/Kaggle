{"cell_type":{"364c7354":"code","8e4d61aa":"code","40136ab3":"code","840fff12":"code","fe93e3b7":"code","bb6a83b7":"code","f7e3aa10":"code","173b5966":"code","86dd681a":"code","e2ac565a":"code","77aa1de8":"code","1a191e44":"code","1ee4dc54":"code","49dab653":"code","ec63b4b5":"code","2f1072f1":"code","60543462":"code","f33a538c":"code","618e46f4":"code","407abd1c":"code","799114b0":"code","32935ce5":"code","a121f80d":"code","cec75d65":"code","9fbf5140":"code","8177456f":"code","da500dd7":"code","8aa87efb":"code","186d4c5e":"code","a8e6f4af":"code","0f525f31":"code","eca99bf1":"code","251ca149":"code","17155b0e":"code","9e1a51de":"code","b5114155":"code","b8fd26cd":"code","21fd08d1":"code","17bef96e":"code","52016fbf":"code","51a96a27":"code","0ef8912a":"markdown","ec4efdb7":"markdown","60f5134b":"markdown","d0a75ac8":"markdown","222657ad":"markdown","7bfb2491":"markdown","34884025":"markdown","a35d0fc6":"markdown","315ccdc7":"markdown","34d43bb3":"markdown","80bb030a":"markdown","406e6d64":"markdown","c338df3d":"markdown","1529900a":"markdown","8002e98a":"markdown","64a93aec":"markdown","ffbf800f":"markdown","cdfeec82":"markdown","8f55a07c":"markdown","1b46f872":"markdown","0df00a2c":"markdown","e515fadd":"markdown"},"source":{"364c7354":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8e4d61aa":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns","40136ab3":"# Load the data\ntweets_df = pd.read_csv('\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv')\ntweets_df.head()","840fff12":"tweets_df.info()","fe93e3b7":"tweets_df.describe()","bb6a83b7":"tweets_df['tweet']","f7e3aa10":"tweets_df.drop('id', axis=1, inplace=True)\ntweets_df.head()","173b5966":"tweets_df.hist(bins=30, figsize=(15,5), color='b')","86dd681a":"sns.countplot(tweets_df['label'], label='count')","e2ac565a":"# let's save the length of each tweets(character) in a sepearte column\ntweets_df['length'] = tweets_df['tweet'].apply(len)\ntweets_df.head()","77aa1de8":"# plot a histogram on the character count\/tweet length\ntweets_df['length'].plot(bins=50, kind='hist')","1a191e44":"positive = tweets_df[tweets_df['label']==0]\npositive","1ee4dc54":"negative = tweets_df[tweets_df['label']==1]\nnegative","49dab653":"sentences = tweets_df['tweet'].to_list()\nsentences[:10]","ec63b4b5":"len(sentences)","2f1072f1":"single_sentence = ' '.join(sentences)\nsingle_sentence[:500]","60543462":"!pip install WordCloud\nfrom wordcloud import WordCloud","f33a538c":"# Plot the WordCloud for all tweets\nplt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate(single_sentence))","618e46f4":"# Plot the WorldCloud for positive words\npositive_sentences = positive['tweet'].to_list()\nsingle_positive = ' '.join(positive_sentences)\nplt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate(single_positive))","407abd1c":"# Plot the WorldCloud for negative words\nnegative_sentences = negative['tweet'].to_list()\nsingle_negative = ' '.join(negative_sentences)\nplt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate(single_negative))","799114b0":"import string\nstring.punctuation","32935ce5":"# Lets test our string punctuation with a test string\nTest = 'Good morning beautiful people :)... I am having fun learning Machine learning and AI!!'","a121f80d":"test_punc_remove = ''.join([c for c in Test if c not in string.punctuation])\ntest_punc_remove","cec75d65":"import nltk\nnltk.download('stopwords')","9fbf5140":"# let imprt the stopwords and see them\nfrom nltk.corpus import stopwords\nstopwords.words('english')","8177456f":"test_punc_clean = [word for word in test_punc_remove.split() if word.lower() not in stopwords.words('english')]\ntest_punc_clean","da500dd7":"def message_cleaning(message):\n    punc_removed = [char for char in message if char not in string.punctuation]\n    punc_removed_join = ''.join(punc_removed)\n    punc_removed_join_clean = [word for word in punc_removed_join.split() if word.lower() not in stopwords.words('english')]\n    return punc_removed_join_clean","8aa87efb":"# Let's apply the function to our tweet dataset\ntweets_df_clean = tweets_df['tweet'].apply(message_cleaning)","186d4c5e":"print(tweets_df_clean[5]) # cleaned up version\nprint(tweets_df['tweet'][5]) # show the orignal version","a8e6f4af":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(analyzer=message_cleaning, dtype='uint8')\ntweets_countvectorizer = vectorizer.fit_transform(tweets_df['tweet']).toarray()","0f525f31":"vectorizer.get_feature_names()[:10]","eca99bf1":"tweets_countvectorizer","251ca149":"tweets_countvectorizer.shape","17155b0e":"# Let's define the features(X) and labels(y) for our model\nX = tweets_countvectorizer\nX","9e1a51de":"X.shape","b5114155":"y = tweets_df['label']\ny","b8fd26cd":"y.shape","21fd08d1":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","17bef96e":"from sklearn.naive_bayes import MultinomialNB\n\nNB_classifier = MultinomialNB()\nNB_classifier.fit(X_train, y_train)","52016fbf":"np.set_printoptions(precision=3)\nfrom sklearn.metrics import classification_report, confusion_matrix\ny_predict_test = NB_classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_predict_test)\nsns.heatmap(cm, annot=True, fmt='.2f')","51a96a27":"print(classification_report(y_test, y_predict_test))","0ef8912a":"## 9. Train the Naive Bayes Classifier Model","ec4efdb7":"The 'sentences' is still separate by commas and spaces lets join them such that it is single corpus.","60f5134b":"## 2. Importing the Libraries and Datasets","d0a75ac8":"Seems like majority of the tweets are between 70 to 100 character.","222657ad":"Great, we have removed all the punctuation now.","7bfb2491":"Lets seperate the positive and negative tweets in different lists.","34884025":"## 9. Check our Model Accuracy through Confusion Matrix\nA confusion matrix is a table that is often used to describe the performance of a classification model (or \u201cclassifier\u201d) on a set of test data for which the true values are known.\n\nMore details can be found in my article here: https:\/\/medium.com\/analytics-vidhya\/clarity-in-confusion-matrix-17fb1da6dabf","a35d0fc6":"#TODO: WordCloud details","315ccdc7":"This dataset is labelled for hatred\/negetive tweets, hence \"label: 1\"  indicates negetive tweets and \"label: 0\" is positive sentiments.","34d43bb3":"## 4. Plot the WordCloud","80bb030a":"Notice that all the labels are discreate values of 0 and 1 as discussed earlier, hence this is binary class problem.","406e6d64":"## 3. Exploring the Dataset","c338df3d":"* ## Create a function for Step 5 and Step 6","1529900a":"## 5. Data Cleaning: Remove Punctuation","8002e98a":"Since we are analyzing the \"tweets\" and the labels, we don't reuqire the \"id\" columns. Hence, lets drop it.","64a93aec":"Notice that this is class bias situation meaning data with label: 1 is much more that label: 0.","ffbf800f":"## 1: Understanding the \n* Natural Language Processing (NLP) works by converting words (texts) into numbers.\n* These numbers are then used to train an AI\/ML model to make predictions.\n* In this case, we will analyze thousands of Twitter tweets to predict people's sentiment","cdfeec82":"## 8. Naive Bayes\nNaive Bayes is a classification technique based on Bayes' Theorem. Bayes\u2019 theorem is based conditional probability which states the likelihood the occurrence of event \u201cA\u201d given another event \u201cB\u201d has already happened.\nThere are 3 type of Na\u00efve Bayes:\n* Gaussian ->The model assume that the data follows normal distribution and all our features are continuous.\n* Bernoulli -> It assumes that all our features are binary such that they only take two values: 0s and 1s.\n* Multinomial -> It assumes that the data has discreate value such as ratings between 1 to 5.\n\nMore on Naive Bayes can be found in my article here: https:\/\/medium.com\/analytics-vidhya\/na%C3%AFve-bayes-classifiers-fafde4f0a411","8f55a07c":"Special thanks to Ryan Ahmed from Coursera https:\/\/www.coursera.org\/projects\/twitter-sentiment-analysis","1b46f872":"## 7. Count Vectorization or Tokenization\nIn order to use textual data for predictive modeling, the text must be parsed to remove certain words \u2013 this process is called tokenization. These words need to then be encoded as integers, or floating-point values, for use as inputs in machine learning algorithms. This process is called feature extraction (or vectorization).","0df00a2c":"## 6. Data Cleaning: Remove Stopwords","e515fadd":"## Now this tweets_countvectorizer is the features for our model"}}