{"cell_type":{"a02e2f06":"code","222c656b":"code","af23de86":"code","aea6776c":"code","29835d59":"code","a4f4d992":"code","a0e94c95":"code","5ea76f6a":"code","4373ae72":"code","cc291654":"code","ef70a2c2":"code","5dd1bdc4":"code","51ebc58a":"code","66efcd94":"code","ffbbe92e":"code","100fd4f5":"code","740b611f":"code","816f7c23":"code","2cf563c5":"code","06855f82":"code","67c2f5b6":"code","94720a23":"code","189f9ad2":"code","a016aa4d":"code","6f353a5d":"code","3a52961f":"code","61ff0b3b":"code","7ee406b6":"code","01d724fb":"markdown","ce4578be":"markdown","07ae8115":"markdown","71cf4c87":"markdown","308e6eb5":"markdown","5f2fd4a1":"markdown","f3776bbc":"markdown","8dfca71f":"markdown","4a22aba5":"markdown","dfda4744":"markdown","a46ec3e6":"markdown","8f2d08af":"markdown","4c968e07":"markdown","c85869c1":"markdown","c7f057bb":"markdown","28c4b499":"markdown"},"source":{"a02e2f06":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mplimg\nfrom matplotlib.pyplot import imshow\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader,Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport time \nimport tqdm\nfrom PIL import Image\ntrain_on_gpu = True\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n\nimport warnings\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\n\nfrom collections import OrderedDict\nimport cv2","222c656b":"!pip install albumentations > \/dev\/null 2>&1","af23de86":"!pip install pretrainedmodels > \/dev\/null 2>&1","aea6776c":"import albumentations\nfrom albumentations import torch as AT\nimport pretrainedmodels","29835d59":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntrain_df.head()","a4f4d992":"print(f\"There are {len(os.listdir('..\/input\/train'))} images in train dataset with {train_df.Id.nunique()} unique classes.\")\nprint(f\"There are {len(os.listdir('..\/input\/test'))} images in test dataset.\")","a0e94c95":"fig = plt.figure(figsize=(25, 4))\ntrain_imgs = os.listdir(\"..\/input\/train\")\nfor idx, img in enumerate(np.random.choice(train_imgs, 20)):\n    ax = fig.add_subplot(2, 20\/\/2, idx+1, xticks=[], yticks=[])\n    im = Image.open(\"..\/input\/train\/\" + img)\n    plt.imshow(im)\n    lab = train_df.loc[train_df.Image == img, 'Id'].values[0]\n    ax.set_title(f'Label: {lab}')","5ea76f6a":"train_df.Id.value_counts().head()","4373ae72":"for i in range(1, 4):\n    print(f'There are {train_df.Id.value_counts()[train_df.Id.value_counts().values==i].shape[0]} classes with {i} samples in train data.')","cc291654":"plt.title('Distribution of classes excluding new_whale');\ntrain_df.Id.value_counts()[1:].plot(kind='hist');","ef70a2c2":"np.array(im).shape","5dd1bdc4":"data_transforms = transforms.Compose([\n                                      transforms.Resize((100, 100)),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                                             std=[0.229, 0.224, 0.225])\n    ])\ndata_transforms_test = transforms.Compose([\n                                           transforms.Resize((100, 100)),\n                                           transforms.ToTensor(),\n                                           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                                                 std=[0.229, 0.224, 0.225])\n])","51ebc58a":"def prepare_labels(y):\n    # From here: https:\/\/www.kaggle.com\/pestipeti\/keras-cnn-starter\n    values = np.array(y)\n    label_encoder = LabelEncoder()\n    integer_encoded = label_encoder.fit_transform(values)\n\n    onehot_encoder = OneHotEncoder(sparse=False)\n    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n\n    y = onehot_encoded\n    return y, label_encoder","66efcd94":"y, le = prepare_labels(train_df['Id'])","ffbbe92e":"class WhaleDataset(Dataset):\n    def __init__(self, datafolder, datatype='train', df=None, transform = transforms.Compose([transforms.ToTensor()]), y=None):\n        self.datafolder = datafolder\n        self.datatype = datatype\n        self.y = y\n        if self.datatype == 'train':\n            self.df = df.values\n        self.image_files_list = [s for s in os.listdir(datafolder)]\n        self.transform = transform\n\n\n    def __len__(self):\n        return len(self.image_files_list)\n    \n    def __getitem__(self, idx):\n        if self.datatype == 'train':\n            img_name = os.path.join(self.datafolder, self.df[idx][0])\n            label = self.y[idx]\n            \n        elif self.datatype == 'test':\n            img_name = os.path.join(self.datafolder, self.image_files_list[idx])\n            label = np.zeros((5005,))\n\n        image = Image.open(img_name).convert('RGB')\n        image = self.transform(image)\n        if self.datatype == 'train':\n            return image, label\n        elif self.datatype == 'test':\n            # so that the images will be in a correct order\n            return image, label, self.image_files_list[idx]","100fd4f5":"train_dataset = WhaleDataset(datafolder='..\/input\/train\/', datatype='train', df=train_df, transform=data_transforms, y=y)\ntest_set = WhaleDataset(datafolder='..\/input\/test\/', datatype='test', transform=data_transforms_test)","740b611f":"train_sampler = SubsetRandomSampler(list(range(len(os.listdir('..\/input\/train')))))\nvalid_sampler = SubsetRandomSampler(list(range(len(os.listdir('..\/input\/test')))))\nbatch_size = 512\nnum_workers = 0\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n# less size for test loader.\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=32, num_workers=num_workers)","816f7c23":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, 7, padding=1)\n        self.conv2_bn = nn.BatchNorm2d(32)\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)        \n        self.pool2 = nn.AvgPool2d(3, 3)\n        \n        self.fc1 = nn.Linear(64 * 4 * 4 * 16, 1024)\n        self.fc2 = nn.Linear(1024, 5005)\n\n        self.dropout = nn.Dropout(0.5)        \n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv2_bn(self.conv1(x))))\n        x = self.pool2(F.relu(self.conv2(x)))\n        x = x.view(-1, 64 * 4 * 4 * 16)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x","2cf563c5":"model_conv = Net()\n\ncriterion = nn.BCEWithLogitsLoss()\n\noptimizer = optim.Adam(model_conv.parameters(), lr=0.01)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)","06855f82":"# model_conv.cuda()\n# n_epochs = 10\n# for epoch in range(1, n_epochs+1):\n#     print(time.ctime(), 'Epoch:', epoch)\n\n#     train_loss = []\n#     exp_lr_scheduler.step()\n\n#     for batch_i, (data, target) in enumerate(train_loader):\n#         #print(batch_i)\n#         data, target = data.cuda(), target.cuda()\n\n#         optimizer.zero_grad()\n#         output = model_conv(data)\n#         loss = criterion(output, target.float())\n#         train_loss.append(loss.item())\n\n#         loss.backward()\n#         optimizer.step()\n\n#     print(f'Epoch {epoch}, train loss: {np.mean(train_loss):.4f}')\n\n# sub = pd.read_csv('..\/input\/sample_submission.csv')\n\n# model_conv.eval()\n# for (data, target, name) in test_loader:\n#     data = data.cuda()\n#     output = model_conv(data)\n#     output = output.cpu().detach().numpy()\n#     for i, (e, n) in enumerate(list(zip(output, name))):\n#         sub.loc[sub['Image'] == n, 'Id'] = ' '.join(le.inverse_transform(e.argsort()[-5:][::-1]))\n        \n# sub.to_csv('basic_model.csv', index=False)","67c2f5b6":"class WhaleDataset(Dataset):\n    def __init__(self, datafolder, datatype='train', df=None, transform = transforms.Compose([transforms.ToTensor()]), y=None\n                ):\n        self.datafolder = datafolder\n        self.datatype = datatype\n        self.y = y\n        if self.datatype == 'train':\n            self.df = df.values\n        self.image_files_list = [s for s in os.listdir(datafolder)]\n        self.transform = transform\n\n\n    def __len__(self):\n        return len(self.image_files_list)\n    \n    def __getitem__(self, idx):\n        if self.datatype == 'train':\n            img_name = os.path.join(self.datafolder, self.df[idx][0])\n            label = self.y[idx]\n            \n        elif self.datatype == 'test':\n            img_name = os.path.join(self.datafolder, self.image_files_list[idx])\n            label = np.zeros((5005,))\n\n        img = cv2.imread(img_name)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        image = self.transform(image=img)\n        image = image['image']\n        if self.datatype == 'train':\n            return image, label\n        elif self.datatype == 'test':\n            # so that the images will be in a correct order\n            return image, label, self.image_files_list[idx]","94720a23":"data_transforms = albumentations.Compose([\n    albumentations.Resize(160, 320),\n    albumentations.HorizontalFlip(),\n    albumentations.RandomBrightness(),\n    albumentations.ShiftScaleRotate(rotate_limit=15, scale_limit=0.10),\n    albumentations.JpegCompression(80),\n    albumentations.HueSaturationValue(),\n    albumentations.Normalize(),\n    AT.ToTensor()\n    ])\ndata_transforms_test = albumentations.Compose([\n    albumentations.Resize(160, 320),\n    albumentations.Normalize(),\n    AT.ToTensor()\n    ])\n\ntrain_dataset = WhaleDataset(datafolder='..\/input\/train\/', datatype='train', df=train_df, transform=data_transforms, y=y)\ntest_set = WhaleDataset(datafolder='..\/input\/test\/', datatype='test', transform=data_transforms_test)\n\ntrain_sampler = SubsetRandomSampler(list(range(len(os.listdir('..\/input\/train')))))\nvalid_sampler = SubsetRandomSampler(list(range(len(os.listdir('..\/input\/test')))))\nbatch_size = 10\nnum_workers = 2\n# prepare data loaders (combine dataset and sampler)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n#valid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=10, num_workers=num_workers)","189f9ad2":"# model_conv = torchvision.models.resnet101(pretrained=True)\n# for i, param in model_conv.named_parameters():\n#     param.requires_grad = False","a016aa4d":"# final_layer = nn.Sequential(OrderedDict([\n#                           ('fc1', nn.Linear(2048, 1024)),\n#                           ('relu', nn.ReLU()),\n#                           ('dropout', nn.Dropout(0.1)),\n#                           ('fc2', nn.Linear(1024, 5005)),\n#                           ]))\n# model_conv.fc = final_layer\n#model_conv.fc = nn.Linear(2048, 5005)","6f353a5d":"model_conv = pretrainedmodels.resnext101_64x4d()\nmodel_conv.avg_pool = nn.AvgPool2d((5,10))\nmodel_conv.last_linear = nn.Linear(model_conv.last_linear.in_features, 5005)","3a52961f":"model_conv.cuda()\ncriterion = nn.BCEWithLogitsLoss()\n\noptimizer = optim.Adam(model_conv.parameters(), lr=0.01)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)","61ff0b3b":"n_epochs = 4\nfor epoch in range(1, n_epochs+1):\n    print(time.ctime(), 'Epoch:', epoch)\n\n    train_loss = []\n    \n\n    for batch_i, (data, target) in enumerate(train_loader):\n        # print(f'Batch {batch_i} of 50')\n        data, target = data.cuda(), target.cuda()\n\n        optimizer.zero_grad()\n        output = model_conv(data)\n        loss = criterion(output, target.float())\n        train_loss.append(loss.item())\n\n        loss.backward()\n        optimizer.step()\n    exp_lr_scheduler.step()\n\n    print(f'Epoch {epoch}, train loss: {np.mean(train_loss):.4f}')","7ee406b6":"sub = pd.read_csv('..\/input\/sample_submission.csv')\n\nmodel_conv.eval()\nfor (data, target, name) in test_loader:\n    data = data.cuda()\n    output = model_conv(data)\n    output = output.cpu().detach().numpy()\n    for i, (e, n) in enumerate(list(zip(output, name))):\n        sub.loc[sub['Image'] == n, 'Id'] = ' '.join(le.inverse_transform(e.argsort()[-5:][::-1]))\n        \nsub.to_csv('submission.csv', index=False)","01d724fb":"## General information\n\nIn this kernel I'll build a CNN in Pytorch to identify Whales. The first attempt was done by using a simple CNN from scratch, but it didn't work well, so I'll use pre-trained nets.\n\n![](https:\/\/s23444.pcdn.co\/wp-content\/uploads\/2014\/04\/Gold-Coast-whale.jpg.optimal.jpg)","ce4578be":"### Data overview","07ae8115":"### Preparing data for Pytorch\nData for Pytorch needs to be prepared:\n* we need to define transformations;\n* then we need to initialize a dataset class;\n* then we need to create dataloaders which will be used by the model;","71cf4c87":"But this was an example. The basic model isn't really good. You can try it, but it is much better to use pre-trained models.","308e6eb5":"### Basic CNN\n\nNow we can define the model. For now I'll use a simple architecture with two convolutional layers.","5f2fd4a1":"### Pretrained model\n\nNow I'll use a pretrained model. First this is changing transformations: I use a bigger image size and add augmentations.","f3776bbc":"#### Encoding labels\nLabels need to be one-hot encoded.","8dfca71f":"#### Initializing model\n\nWe need to define model, loss, oprimizer and possibly a scheduler.","4a22aba5":"#### Loaders\n\nNow we create loaders. Here we define which images will be used, batch size and other things.","dfda4744":"25k images in train and 5k different whales!\nLet's have a look at them","a46ec3e6":"#### Transformations\n\nBasic transformations include only resizing the image to the necessary size, converting to Pytorch tensor and normalizing","8f2d08af":"For train data we have a DataFrame with image names and ids. And of course for train and test we have images in separate folders.","4c968e07":"#### Dataset\n\nNow we need to create a dataset. Sadly, default version won't work, as images for each class are supposed to be in separate folders. So I write a custom WhaleDataset.","c85869c1":"At least some images are quite big","c7f057bb":"### Versions\nHere I'll list significant changes in the notebook.\n* v10 - added augmentations and scheduler\n* v18 - changed augmentations to albumentations","28c4b499":"We can see that there is a huge disbalance in the data. There are many classes with only one or several samples , some classes have 50+ samples and \"default\" class has almost 10k samples."}}