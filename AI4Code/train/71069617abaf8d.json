{"cell_type":{"eed8d5de":"code","9ecbef7a":"code","0f21d539":"code","8a55eb43":"code","12a2421c":"code","8673660b":"code","ae10a093":"code","b24136d0":"code","1ce65859":"code","771a73d8":"code","fa6d633c":"code","aaea5f39":"code","3598389a":"code","8b8e9c1e":"code","d8dcb797":"code","f4f3bb4d":"code","e2431005":"code","09519f7d":"code","3c123dbf":"code","16e461f2":"code","12e5db91":"code","d3ebb4a8":"code","91b28927":"code","18ff73ea":"code","5a7d0943":"code","17c3f021":"code","ec34a320":"code","197c98eb":"code","b9e75d1a":"code","6ad35cba":"code","d7b2e559":"code","ec3d9dc8":"code","49af4646":"code","fcad75e9":"code","f3a20292":"code","80b26514":"code","fa6f9596":"code","88807f6e":"code","80504df2":"code","170d47af":"code","ad5afa40":"code","7db94e86":"code","eacdf5ee":"code","41166b22":"code","e700c506":"code","00755fef":"code","c3e16695":"code","a042f4f9":"markdown","2319a691":"markdown","8fc805ac":"markdown","2fde15f5":"markdown","71875f3d":"markdown","dd8c179e":"markdown","f8484230":"markdown","fcb3c4d4":"markdown","55c53dfd":"markdown","3db04ee3":"markdown","7b22e706":"markdown","757cb77f":"markdown","b054c3b8":"markdown","8c744037":"markdown","e0db319f":"markdown","7f84d8f8":"markdown","4fb58df6":"markdown","ec4b4bc7":"markdown","208c924a":"markdown","ff59e971":"markdown","bc1039d8":"markdown","255825c0":"markdown","a4a34fa6":"markdown","3312c4d2":"markdown","40f25c27":"markdown","66501c8d":"markdown","52672200":"markdown","27bf23f6":"markdown","22f0dc75":"markdown","5b8692b8":"markdown","8dbc5216":"markdown","fd68433b":"markdown","ad2d4f93":"markdown","009e4941":"markdown"},"source":{"eed8d5de":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9ecbef7a":"import re\nimport matplotlib.pyplot as plt\nimport string\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import LancasterStemmer\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nimport nltk\nfrom gensim.utils import simple_preprocess\nfrom nltk.corpus import stopwords\nimport gensim\nfrom sklearn.model_selection import train_test_split\nimport spacy\nimport pickle\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport matplotlib.pyplot as plt \nimport tensorflow as tf\nimport keras\nfrom keras.utils.vis_utils import plot_model\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly as pl\nimport datetime\n\n#for confusion matrix\nimport seaborn\nfrom sklearn.metrics import confusion_matrix\n\n\nnltk.download('wordnet')\nprint('Done')","0f21d539":"train = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/train.csv\")","8a55eb43":"train.head(10)","12a2421c":"train['sentiment'].unique()","8673660b":"train.groupby('sentiment').nunique()","ae10a093":"train = train[['selected_text','sentiment']]\ntrain.head()","b24136d0":"if (train[\"selected_text\"].isnull().sum() > 0):\n    train[\"selected_text\"].fillna(\"No content\", inplace = True)","1ce65859":"def depure_data(data):\n    \n    #Removing URLs with a regular expression\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    data = url_pattern.sub(r'', data)\n\n    # Remove Emails\n    data = re.sub('\\S*@\\S*\\s?', '', data)\n\n    # Remove new line characters\n    data = re.sub('\\s+', ' ', data)\n\n    # Remove distracting single quotes\n    data = re.sub(\"\\'\", \"\", data)\n        \n    return data\n\ntemp = []\n#Splitting pd.Series to list\ndata_to_list = train['selected_text'].values.tolist()\nfor i in range(len(data_to_list)):\n    temp.append(depure_data(data_to_list[i]))\nlist(temp[:5])","771a73d8":"def sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n        \n\ndata_words = list(sent_to_words(temp))\n\nprint(data_words[:10])","fa6d633c":"def detokenize(text):\n    return TreebankWordDetokenizer().detokenize(text)\n    \ndata = []\nfor i in range(len(data_words)):\n    \n    #for j in range(len(data_words[i])):\n        #stemming\n        #data_words[i][j] = lancaster.stem(data_words[i][j])\n        #lemmatization\n        #data_words[i][j] = data_words[i][j].format(data_words[i][j],wordnet_lemmatizer.lemmatize(data_words[i][j]))\n    \n    data.append(detokenize(data_words[i]))\nprint(data[:10])","aaea5f39":"data = np.array(data)","3598389a":"labels = np.array(train['sentiment'])\ny = []\nfor i in range(len(labels)):\n    if labels[i] == 'neutral':\n        y.append(0)\n    if labels[i] == 'negative':\n        y.append(1)\n    if labels[i] == 'positive':\n        y.append(2)\ny = np.array(y)\nlabels = tf.keras.utils.to_categorical(y, 3, dtype=\"float32\")\ndel y","8b8e9c1e":"from keras.models import Sequential\nfrom keras import layers\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import regularizers\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint\nmax_words = 5000\nmax_len = 200\n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(data)\nsequences = tokenizer.texts_to_sequences(data)\ntweets = pad_sequences(sequences, maxlen=max_len)\nprint(tweets)","d8dcb797":"print(labels)","f4f3bb4d":"#Splitting the data\nX_train, X_test, y_train, y_test = train_test_split(tweets,labels, random_state=0)\nprint (len(X_train),len(X_test),len(y_train),len(y_test))","e2431005":"model1 = Sequential()\nmodel1.add(layers.Embedding(max_words, 20))\nmodel1.add(layers.LSTM(15,dropout=0.5))\nmodel1.add(layers.Dense(3,activation='softmax'))\n\n\nmodel1.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n#Implementing model checkpoins to save the best metric and do not lose it on training.\ncheckpoint1 = ModelCheckpoint(\"best_model1.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\nhistory = model1.fit(X_train, y_train, epochs=1,validation_data=(X_test, y_test),callbacks=[checkpoint1])","09519f7d":"model2 = Sequential()\nmodel2.add(layers.Embedding(max_words, 40, input_length=max_len))\nmodel2.add(layers.Bidirectional(layers.LSTM(20,dropout=0.6)))\nmodel2.add(layers.Dense(3,activation='softmax'))\nmodel2.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n#Implementing model checkpoins to save the best metric and do not lose it on training.\n#model2 = keras.models.load_model('.\/best_model2.hdf5')\ncheckpoint2 = ModelCheckpoint(\"best_model2.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\nhistory = model2.fit(X_train, y_train, epochs=1,validation_data=(X_test, y_test),callbacks=[checkpoint2])","3c123dbf":"from keras import regularizers\nmodel3 = Sequential()\nmodel3.add(layers.Embedding(max_words, 40, input_length=max_len))\nmodel3.add(layers.Conv1D(20, 6, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=2e-3, l2=2e-3),bias_regularizer=regularizers.l2(2e-3)))\nmodel3.add(layers.MaxPooling1D(5))\nmodel3.add(layers.Conv1D(20, 6, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=2e-3, l2=2e-3),bias_regularizer=regularizers.l2(2e-3)))\nmodel3.add(layers.GlobalMaxPooling1D())\nmodel3.add(layers.Dense(3,activation='softmax'))\nmodel3.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['acc'])\ncheckpoint3 = ModelCheckpoint(\"best_model3.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\nhistory = model3.fit(X_train, y_train, epochs=1,validation_data=(X_test, y_test),callbacks=[checkpoint3])","16e461f2":"#Let's load the best model obtained during training\nbest_model = keras.models.load_model(\".\/best_model2.hdf5\")","12e5db91":"test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=2)\nprint('Model accuracy: ',test_acc)","d3ebb4a8":"predictions = best_model.predict(X_test)","91b28927":"matrix = confusion_matrix(y_test.argmax(axis=1), np.around(predictions, decimals=0).argmax(axis=1))\nconf_matrix = pd.DataFrame(matrix, index = ['Neutral','Negative','Positive'],columns = ['Neutral','Negative','Positive'])\n#Normalizing\nconf_matrix = conf_matrix.astype('float') \/ conf_matrix.sum(axis=1)[:, np.newaxis]\nplt.figure(figsize = (15,15))\nseaborn.heatmap(conf_matrix, annot=True, annot_kws={\"size\": 15})","18ff73ea":"sentiment = ['Neutral','Negative','Positive']","5a7d0943":"sequence = tokenizer.texts_to_sequences(['the trump administration failed to deliver on vaccine promises shocker covidiots coronavirus covidvaccine'])\ntest = pad_sequences(sequence, maxlen=max_len)\nsentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]","17c3f021":"sequence = tokenizer.texts_to_sequences(['this data science article is the best ever'])\ntest = pad_sequences(sequence, maxlen=max_len)\nsentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]","ec34a320":"sequence = tokenizer.texts_to_sequences(['i hate youtube ads, they are annoying'])\ntest = pad_sequences(sequence, maxlen=max_len)\nsentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]","197c98eb":"sequence = tokenizer.texts_to_sequences(['i really loved how the technician helped me with the issue that i had'])\ntest = pad_sequences(sequence, maxlen=max_len)\nsentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]","b9e75d1a":"covidtweets = pd.read_csv('..\/input\/all-covid19-vaccines-tweets\/vaccination_all_tweets.csv')\ncovidtweets.head(10)","6ad35cba":"print(covidtweets['id'].nunique())\nprint(covidtweets['user_name'].nunique())\nprint(covidtweets['user_location'].nunique())\nprint(covidtweets['user_description'].nunique())\nprint(covidtweets['user_created'].nunique())\nprint(covidtweets['user_friends'].nunique())\nprint(covidtweets['user_favourites'].nunique())\nprint(covidtweets['user_verified'].nunique())\nprint(covidtweets['date'].nunique())\nprint(covidtweets['text'].nunique())\nprint(covidtweets['hashtags'].nunique())\nprint(covidtweets['source'].nunique())\nprint(covidtweets['retweets'].nunique())\nprint(covidtweets['favorites'].nunique())\nprint(covidtweets['is_retweet'].nunique())","d7b2e559":"covidtweets[\"text\"].isnull().sum()","ec3d9dc8":"temp = []\n#Splitting pd.Series to list\ndata_to_list = covidtweets[\"text\"].values.tolist()\nfor i in range(len(data_to_list)):\n    temp.append(depure_data(data_to_list[i]))\nlist(temp[:5])","49af4646":"data_words = list(sent_to_words(temp))\nprint(data_words[:10])","fcad75e9":"data = []\nfor i in range(len(data_words)):\n    \n    #for j in range(len(data_words[i])):\n        #stemming\n        #data_words[i][j] = lancaster.stem(data_words[i][j])\n        #lemmatization\n        #data_words[i][j] = data_words[i][j].format(data_words[i][j],wordnet_lemmatizer.lemmatize(data_words[i][j]))\n        \n    data.append(detokenize(data_words[i]))\nprint(data[:5])","f3a20292":"data = np.array(data)","80b26514":"covidtweets[\"sentiment\"] = np.nan","fa6f9596":"#covidtweets[\"sentiment\"] = best_model.predict_classes(data)]\nfor i in range(len(temp)):\n    sequence = tokenizer.texts_to_sequences([temp[i]])\n    test = pad_sequences(sequence, maxlen=max_len)\n    covidtweets[\"sentiment\"][i] = sentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]\n    if (i % 461 == 0):\n        percentage = i\/len(temp) * 100\n        formatted_percentage = \"{:.0f}\".format(percentage)\n        print(formatted_percentage,\"%\")","88807f6e":"pd.set_option('display.max_colwidth', None)\ncovidtweets[[\"text\", \"sentiment\"]].head(50)","80504df2":"# Plot sentiment value counts\ncovidtweets['sentiment'].value_counts(normalize=True).plot.bar();\n\ntimeline = covidtweets.groupby(['sentiment']).agg(**{'tweets': ('id', 'count')}).reset_index().dropna()\nprint(timeline)\nfig = px.bar(timeline,\n            x='sentiment', y=\"tweets\", color='sentiment', color_discrete_sequence=[\"#EF553B\", \"#636EFA\", \"#00CC96\"]\n        )\nfig.show()","170d47af":"# Convert dates\ncovidtweets['date'] = pd.to_datetime(covidtweets['date'], errors='coerce').dt.date\n\n# Get counts of number of tweets by sentiment for each date\ntimeline = covidtweets.groupby(['date', 'sentiment']).agg(**{'tweets': ('id', 'count')}).reset_index().dropna()\n\nfig = px.line(timeline, x='date', y='tweets', color='sentiment', color_discrete_sequence=[\"#EF553B\", \"#636EFA\", \"#00CC96\"], category_orders={'sentiment': ['neutral', 'negative', 'positive']},\n             title='Timeline showing sentiment of tweets about COVID-19 vaccines')\nfig.show()","ad5afa40":"all_vax = ['covaxin', 'sinopharm', 'sinovac', 'moderna', 'pfizer', 'biontech', 'oxford', 'astrazeneca', 'sputnik']\n\n# Function to filter the data to a single vaccine and plot the timeline\n# Note: a lot of the tweets seem to contain hashtags for multiple vaccines even though they are specifically referring to one vaccine - not very helpful!\ndef filtered_timeline(df, vax, title):\n    df = df.dropna()\n    title_str = 'Timeline showing sentiment of tweets about the '+title+' vaccine'\n    df_filt = pd.DataFrame()\n    for o in vax:\n        df_filt = df_filt.append(df[df['text'].str.lower().str.contains(o)])\n    other_vax = list(set(all_vax)-set(vax))\n    for o in other_vax:\n        df_filt = df_filt[~df_filt['text'].str.lower().str.contains(o)]\n    df_filt = df_filt.drop_duplicates()\n    timeline = df_filt.groupby(['date', 'sentiment']).agg(**{'tweets': ('id', 'count')}).reset_index()\n    fig = px.line(timeline, x='date', y='tweets', color='sentiment', category_orders={'sentiment': ['neutral', 'negative', 'positive']},title=title_str)\n    fig.show()\n    return df_filt\n\ncovaxin = filtered_timeline(covidtweets, ['covaxin'], title='Covaxin')\nsinovac = filtered_timeline(covidtweets, ['sinovac'], title='Sinovac')\nsinopharm = filtered_timeline(covidtweets, ['sinopharm'], title='Sinopharm')\nmoderna = filtered_timeline(covidtweets, ['moderna'], title='Moderna')\nsputnikv = filtered_timeline(covidtweets, ['sputnik'], title='Sputnik V')\noxford = filtered_timeline(covidtweets, ['oxford', 'astrazeneca'], title='Oxford\/AstraZeneca')\npfizer = filtered_timeline(covidtweets, ['pfizer', 'biontech'], title='Pfizer\/BioNTech')\n\n# Get z scores of sentiment for each vaccine\nvax_names = {'Covaxin': covaxin, 'Sputnik V': sputnikv, 'Sinovac': sinovac, 'Sinopharm': sinopharm,\n            'Moderna': moderna, 'Oxford\/AstraZeneca': oxford, 'PfizerBioNTech': pfizer}\nsentiment_zscores = pd.DataFrame()\nfor k, v in vax_names.items():\n    senti = v['sentiment'].value_counts(normalize=True)\n    senti['vaccine'] = k\n    sentiment_zscores = sentiment_zscores.append(senti)\nfor col in ['Negative', 'Neutral', 'Positive']:\n    sentiment_zscores[col+'_zscore'] = (sentiment_zscores[col] - sentiment_zscores[col].mean())\/sentiment_zscores[col].std(ddof=0)\nsentiment_zscores.set_index('vaccine', inplace=True)\n\n# Plot the results\nax = sentiment_zscores.sort_values('Negative_zscore')['Negative_zscore'].plot.barh(title='Z scores of negative sentiment')\nax.set_ylabel('Vaccine')\nax.set_xlabel('Z score');","7db94e86":"# Function to filter the data to a single date and print tweets from users with the most followers\ndef date_filter(df, date):\n    return df[df['date'].astype(str)==date].sort_values('user_followers', ascending=False)[['date' ,'text']]\n\ndef date_printer(df, dates, num=10): \n    for date in dates:\n        display(date_filter(df, date).head(num))\n\ndate_printer(moderna, ['2021-01-01', '2021-03-03'])","eacdf5ee":"\ncovidtweets[covidtweets['text'].str.lower().str.contains(\"pfizer\")].head(15)","41166b22":"## Word clouds","e700c506":"!pip install wordninja pyspellchecker\nfrom wordcloud import WordCloud, ImageColorGenerator\nimport wordninja\nfrom spellchecker import SpellChecker\nfrom collections import Counter\nimport nltk\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords \nstop_words = set(stopwords.words('english'))  \nstop_words.add(\"amp\")\nimport math\nimport random","00755fef":"# FUNCTIONS REQUIRED\n\ndef flatten_list(l):\n    return [x for y in l for x in y]\n\ndef is_acceptable(word: str):\n    return word not in stop_words and len(word) > 2\n\n# Color coding our wordclouds \ndef red_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n    return f\"hsl(0, 100%, {random.randint(25, 75)}%)\" \n\ndef green_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n    return f\"hsl({random.randint(90, 150)}, 100%, 30%)\" \n\ndef yellow_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n    return f\"hsl(42, 100%, {random.randint(25, 50)}%)\" \n\n# Reusable function to generate word clouds \ndef generate_word_clouds(neg_doc, neu_doc, pos_doc):\n    # Display the generated image:\n    fig, axes = plt.subplots(1,3, figsize=(20,10))\n    \n    wordcloud_neg = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(neg_doc))\n    axes[0].imshow(wordcloud_neg.recolor(color_func=red_color_func, random_state=3), interpolation='bilinear')\n    axes[0].set_title(\"Negative Words\")\n    axes[0].axis(\"off\")\n\n    wordcloud_neu = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(neu_doc))\n    axes[1].imshow(wordcloud_neu.recolor(color_func=yellow_color_func, random_state=3), interpolation='bilinear')\n    axes[1].set_title(\"Neutral Words\")\n    axes[1].axis(\"off\")\n\n    wordcloud_pos = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(pos_doc))\n    axes[2].imshow(wordcloud_pos.recolor(color_func=green_color_func, random_state=3), interpolation='bilinear')\n    axes[2].set_title(\"Positive Words\")\n    axes[2].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show();\n\ndef get_top_percent_words(doc, percent):\n    # Returns a list of \"top-n\" most frequent words in a list \n    top_n = int(percent * len(set(doc)))\n    counter = Counter(doc).most_common(top_n)\n    top_n_words = [x[0] for x in counter]\n    \n    return top_n_words\n    \ndef clean_document(doc):\n    spell = SpellChecker()\n    lemmatizer = WordNetLemmatizer()\n    \n    # Lemmatize words (needed for calculating frequencies correctly )\n    doc = [lemmatizer.lemmatize(x) for x in doc]\n    \n    # Get the top 10% of all words. This may include \"misspelled\" words \n    top_n_words = get_top_percent_words(doc, 0.1)\n\n    # Get a list of misspelled words \n    misspelled = spell.unknown(doc)\n    \n    # Accept the correctly spelled words and top_n words \n    clean_words = [x for x in doc if x not in misspelled or x in top_n_words]\n    \n    # Try to split the misspelled words to generate good words (ex. \"lifeisstrange\" -> [\"life\", \"is\", \"strange\"])\n    words_to_split = [x for x in doc if x in misspelled and x not in top_n_words]\n    split_words = flatten_list([wordninja.split(x) for x in words_to_split])\n    \n    # Some splits may be nonsensical, so reject them (\"llouis\" -> ['ll', 'ou', \"is\"])\n    clean_words.extend(spell.known(split_words))\n    \n    return clean_words\n\ndef get_log_likelihood(doc1, doc2):    \n    doc1_counts = Counter(doc1)\n    doc1_freq = {\n        x: doc1_counts[x]\/len(doc1)\n        for x in doc1_counts\n    }\n    \n    doc2_counts = Counter(doc2)\n    doc2_freq = {\n        x: doc2_counts[x]\/len(doc2)\n        for x in doc2_counts\n    }\n    \n    doc_ratios = {\n        # 1 is added to prevent division by 0\n        x: math.log((doc1_freq[x] +1 )\/(doc2_freq[x]+1))\n        for x in doc1_freq if x in doc2_freq\n    }\n    \n    top_ratios = Counter(doc_ratios).most_common()\n    top_percent = int(0.1 * len(top_ratios))\n    return top_ratios[:top_percent]\n\n# Function to generate a document based on likelihood values for words \ndef get_scaled_list(log_list):\n    counts = [int(x[1]*100000) for x in log_list]\n    words = [x[0] for x in log_list]\n    cloud = []\n    for i, word in enumerate(words):\n        cloud.extend([word]*counts[i])\n    # Shuffle to make it more \"real\"\n    random.shuffle(cloud)\n    return cloud\n","c3e16695":"# Convert string to a list of words\ncovidtweets['words'] = covidtweets.text.apply(lambda x:re.findall(r'\\w+', x ))\n\ndef get_smart_clouds(df):\n\n    neg_doc = flatten_list(df[df['sentiment']=='Negative']['words'])\n    neg_doc = [x for x in neg_doc if is_acceptable(x)]\n\n    pos_doc = flatten_list(df[df['sentiment']=='Positive']['words'])\n    pos_doc = [x for x in pos_doc if is_acceptable(x)]\n\n    neu_doc = flatten_list(df[df['sentiment']=='Neutral']['words'])\n    neu_doc = [x for x in neu_doc if is_acceptable(x)]\n\n    # Clean all the documents\n    neg_doc_clean = clean_document(neg_doc)\n    neu_doc_clean = clean_document(neu_doc)\n    pos_doc_clean = clean_document(pos_doc)\n\n    # Combine classes B and C to compare against A (ex. \"positive\" vs \"non-positive\")\n    top_neg_words = get_log_likelihood(neg_doc_clean, flatten_list([pos_doc_clean, neu_doc_clean]))\n    top_neu_words = get_log_likelihood(neu_doc_clean, flatten_list([pos_doc_clean, neg_doc_clean]))\n    top_pos_words = get_log_likelihood(pos_doc_clean, flatten_list([neu_doc_clean, neg_doc_clean]))\n\n    # Generate syntetic a corpus using our loglikelihood values \n    neg_doc_final = get_scaled_list(top_neg_words)\n    neu_doc_final = get_scaled_list(top_neu_words)\n    pos_doc_final = get_scaled_list(top_pos_words)\n\n    # Visualise our synthetic corpus\n    generate_word_clouds(neg_doc_final, neu_doc_final, pos_doc_final)\n    \nget_smart_clouds(covidtweets)","a042f4f9":"# Model building","2319a691":"Again, the model's score is very poor, but keep in mind it hasn't gone through hyperparameter tuning. Let's see how it perfoms on some sample texts:","8fc805ac":"We neeed to exclude last day's data from the set as is it incomplete:","2fde15f5":"To make sure that the sentiment data is correct, let's check that it does not contain any other sentiment values than positive\/negative\/neutral:","71875f3d":"Adding empty sentiment column:","dd8c179e":"# COVID-19 Vaccination set\n\n\n","f8484230":"Now let's take a look at the distribution of the dataset:","fcb3c4d4":"If you check the val_accuracy metric in the training logs you won't find better score than the one achieved by the BidRNN. Again, the previous model is not the best for this task becaue is majorly used for short translation tasks, but the good thing to notice is its speed to train.\n\nLet's move on.","55c53dfd":"## Model 2: Bidirectional LSTM","3db04ee3":"## Pre-processing","7b22e706":"## Applying model to COVID dataset","757cb77f":"Is there any null value?","b054c3b8":"# Training set import\n\nBelow we are importing training set for our models, taken from:\nhttps:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction","8c744037":"# Label encoding\n\nAs the dataset is categorical, we need to convert the sentiment labels from Neutral, Negative and Positive to a float type that our model can understand. To achieve this task, we'll implement the to_categorical method from Keras.","e0db319f":"# Welcome\nThis is a supplementary Jupyter Notebook to a paper \"Sentiment Analysis of SARS-CoV-2 vaccination tweet\" made for 7088CEM - Artificial Neural Networks - 2021JANMAY module at Coventry University, as part MSc Computer Science programme. ","7f84d8f8":"# Data pre-processing\n\nThis section covers the techniques described in Section 5 of attached paper.\n\nFirst of all, let's remove all the unncessary columns apart fomr text and sentiment. As this is only a training set, we do not actually use this dataset for sentiment analysis.","4fb58df6":"## Load set","ec4b4bc7":"Let's also ensure that there is no null value.\n\nIf there is, let's fill it.","208c924a":"Just like with training set, we need to pre-process this set with the same rules:","ff59e971":"Let's examine the columns:","bc1039d8":"## Model 3: One-dimensional Convolutional Neural Network\n\n","255825c0":"Let's view first 10 results:","a4a34fa6":"## Detokenization, Stemming and Lemmatization","3312c4d2":"### The next steps about data cleaning will be:\n\n* Remove URLs from the tweets\n* Tokenize text\n* Remove emails\n* Remove new lines characters\n* Remove distracting single quotes\n* Remove all punctuation signs\n* Lowercase all text\n* Detokenize text\n* Convert list of texts to Numpy array","40f25c27":"## Data analysis\n\nFirst let's plot the distribution chart of different sentiments:","66501c8d":"Z scores of negative sentiment for each vaccine:","52672200":"# Data sequencing and splitting\n\nWe'll implement the Keras tokenizer as well as its pad_sequences method to transform our text data into 3D float data, otherwise our neural networks won't be able to be trained on it.","27bf23f6":"Applying the model to COVID-19 Vaccination set","22f0dc75":"## Sentiment analysis setup\n\nLet's first import all the necessary modules that our notebook will use as part of this project.","5b8692b8":"Let's test whether it was loaded correctly by printing first 10 rows:","8dbc5216":"# Setup\n\n## Kaggle default setup","fd68433b":"## Confusion matrix\n\nAs accuracy is not a good metric to measure the quality of the model,\nwe are going to plot a confusion matrix:","ad2d4f93":"# Best model validation\n\nRunning the training has shown that Bidirectional RNN provides best accuracy.\nBecause of that, that's the network we're going to choose for our sentiment analysis.","009e4941":"## Model 1: SingleLSTM"}}