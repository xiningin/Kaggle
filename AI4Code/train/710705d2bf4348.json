{"cell_type":{"add78284":"code","c2a37636":"code","9b80b78b":"code","5813a7fb":"code","880cb586":"code","bbb8f370":"code","19510d9b":"code","b7b357dd":"code","ad1cfb2d":"code","5240fac7":"code","835973ee":"code","368bff9e":"code","20968224":"code","e782c089":"code","d440b6e9":"code","83520cbd":"code","4038ee42":"code","c0d29a7c":"code","2b43d257":"code","68d2e3c8":"code","29a7f919":"code","d83dc749":"code","11c84565":"code","6ea608a6":"code","1b879ae1":"code","10d59c09":"code","32cf7216":"code","814c2a9d":"code","270fb086":"code","df29604e":"code","b17778db":"code","f37d4c78":"code","86182059":"code","c26b6a72":"code","9a4f4c9f":"code","42513cc9":"code","54c688d9":"code","636ddf64":"code","224c5756":"code","8a1359f5":"code","be6ecead":"code","a30141db":"code","19b54886":"code","d8a6b3f6":"code","47c228f7":"code","942e1c2b":"code","70cc0a15":"code","ed7b8d35":"code","df3850a9":"code","0d65eb23":"code","d7cefacd":"code","e111c001":"code","c1e598de":"code","51fca1c8":"code","4a8da31e":"markdown","156e0135":"markdown","36af0b8e":"markdown","6b4464f2":"markdown","59d5c7b1":"markdown","869a5013":"markdown","33f3416c":"markdown","970a127b":"markdown","87b647b5":"markdown","c2843dd1":"markdown","bc199307":"markdown","1d4be6a0":"markdown","962c7e41":"markdown","92574c21":"markdown","b7a309da":"markdown","ae393493":"markdown"},"source":{"add78284":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom tqdm import tqdm\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c2a37636":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","9b80b78b":"# fill NaN values of v2a1 = 0 when house_house_status = 0\ntrain['v2a1'][train['tipovivi1']==1] = 0\ntest['v2a1'][test['tipovivi1']==1] = 0","5813a7fb":"# fill NaN values of v2a1 = 0 when house_house_status = 0\ntrain['v18q1'][train['v18q']==0] = 0\ntest['v18q1'][test['v18q']==0] = 0","880cb586":"train['eviv1'] = np.logical_and(np.array(train['eviv1']), np.logical_not(np.array(train['pisonotiene'])))*1\ntest['eviv1'] = np.logical_and(np.array(test['eviv1']), np.logical_not(np.array(test['pisonotiene'])))*1","bbb8f370":"train = train.replace('no', 0)\ntrain = train.replace('yes', 1)\ntest = test.replace('no', 0)\ntest = test.replace('yes', 1)","19510d9b":"print(\"Training shape: \", train.shape)\nprint(\"Training info: \")\ntrain.info()\nprint(\"\\n-----------------------------------------\\n\")\nprint(\"Test shape: \", test.shape)\nprint(\"Test info: \")\ntest.info()","b7b357dd":"print(\"Test\/Train raito: \", test.shape[0]\/float(train.shape[0]))","ad1cfb2d":"train[['r4t3', 'tamhog', 'hhsize', 'tamviv']].describe()","5240fac7":"print(\"Data type of columns:\")\ntrain.describe()","835973ee":"print(\"Check NaN values in Train set:\")\nisnull = train.isnull().sum().reset_index()\n#isnull[isnull>0]\nisnull.columns = ['Feature', 'Total_null']\ntotal_null = isnull[isnull['Total_null']>0]\ntotal_null","368bff9e":"print(\"Check NaN values in Test set:\")\nisnull = test.isnull().sum().reset_index()\n#isnull[isnull>0]\nisnull.columns = ['Feature', 'Total_null']\ntotal_null = isnull[isnull['Total_null']>0]\ntotal_null","20968224":"test.describe()","e782c089":"\"\"\"\nfeature_used = ['hacdor', 'rooms', 'hacapo', 'v14a', 'refrig', 'tamhog', 'paredblolad', 'paredzocalo',\n                'paredpreb', 'pareddes', 'paredmad', 'paredzinc', 'paredfibras' ,'paredother' ,'pisomoscer',\n                'pisocemento', 'pisoother', 'pisonatur', 'pisonotiene', 'pisomadera', 'techozinc', \n                'techoentrepiso', 'techocane', 'techootro', 'cielorazo', 'abastaguadentro', 'abastaguafuera',\n                'abastaguano', 'public', 'planpri', 'noelec', 'coopele', 'sanitario1', 'sanitario2', 'sanitario3',\n                'sanitario5', 'sanitario6', 'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4',\n                'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', 'elimbasu5', 'elimbasu6', 'epared1', 'epared2',\n                'epared3', 'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3']\n\"\"\"","d440b6e9":"feature_used = ['cielorazo', 'v18q1', 'computer', 'television', 'qmobilephone', 'refrig', 'bedrooms', 'hacdor', 'overcrowding', 'rooms', \n                'hacapo', 'v14a','paredpreb', 'pareddes', 'paredmad', 'paredzinc', 'paredfibras', 'paredother', 'paredblolad', 'paredzocalo',\n               'abastaguadentro', 'abastaguafuera', 'abastaguano', 'public', 'planpri', 'noelec', 'coopele', 'sanitario1', 'sanitario2', \n                'sanitario3', 'sanitario5', 'sanitario6', 'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', 'elimbasu1', \n                'elimbasu2', 'elimbasu3', 'elimbasu4','elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3', 'etecho1', 'etecho2', 'etecho3', \n               'pisomadera', 'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'eviv1', 'eviv2', 'eviv3', 'pisonotiene', 'pisomoscer', \n                'pisocemento', 'pisoother', 'pisonatur', 'pisomadera', 'lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6',\n               'area1', 'area2', 'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5']","83520cbd":"train_groupby = train.groupby(['idhogar']).mean().reset_index()\ntest_groupby = test.groupby(['idhogar']).mean().reset_index()\n\ntest_missing_rent = test_groupby[test_groupby['v2a1'].isnull()]\ntest_rent = test_groupby[~test_groupby['v2a1'].isnull()]\n\ntrain_missing_rent = train_groupby[train_groupby['v2a1'].isnull()]\ntrain_rent = train_groupby[~train_groupby['v2a1'].isnull()]\n\ntemp_frame = [train_rent, test_rent]\nrent_training = pd.concat(temp_frame)\n\ntemp_frame = [train_missing_rent, test_missing_rent]\nrent_test = pd.concat(temp_frame)\n\nX = rent_training[feature_used]\nY = rent_training[['v2a1']]\n\nX_test = rent_test[feature_used]\nidhogar_test = rent_test['idhogar']\n\n# normalize data\nfrom sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler()\nmin_max_scaler.fit(X)\nmin_max_scaler.fit(X_test)\nX_scale = min_max_scaler.transform(X)\n\n\n# train on linear regression model\n# split train\/test set\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\nnp.random.seed(0)\nX_train, X_val, Y_train, Y_val = train_test_split(X_scale, Y, test_size=0.1, random_state=0)\n\nmodel = LinearRegression().fit(X_train, Y_train)\nprint(\"Training score: \", model.score(X_train, Y_train))\nprint(\"Validation score: \", model.score(X_val, Y_val))\n\n# predict rent for missing values\nidhogar_test_df = pd.DataFrame(idhogar_test, columns=['idhogar'])\nidhogar_test_df = idhogar_test_df.reset_index().drop(columns=['index'])\n\nrent_predict = model.predict(X_test)\nrent_predict_df = pd.DataFrame(rent_predict, columns=['v2a1'])\ntest_result_predict = pd.concat([idhogar_test_df, rent_predict_df], axis=1, join='inner')\n\n# merge predicting values with original train\/test dataframe\ntemp_train = pd.merge(train, test_result_predict, on=['idhogar', 'idhogar'], how='left')\ntemp_train['v2a1_x'].fillna(temp_train['v2a1_y'], inplace=True)\ntemp_train = temp_train.drop(columns='v2a1_y')\ntemp_train = temp_train.rename(index=str, columns={'v2a1_x':'v2a1'})\n\ntemp_test = pd.merge(test, test_result_predict, on=['idhogar', 'idhogar'], how='left')\ntemp_test['v2a1_x'].fillna(temp_test['v2a1_y'], inplace=True)\ntemp_test = temp_test.drop(columns='v2a1_y')\ntemp_test = temp_test.rename(index=str, columns={'v2a1_x':'v2a1'})","4038ee42":"# store the original data\noriginal_train = train\noriginal_test = test\n\n# set train\/test data to the new one which using predicting monthly rent values\ntrain = temp_train\ntest = temp_test","c0d29a7c":"print(\"Check NaN values in Train set:\")\nisnull = train.isnull().sum().reset_index()\n#isnull[isnull>0]\nisnull.columns = ['Feature', 'Total_null']\ntotal_null = isnull[isnull['Total_null']>0]\ntotal_null","2b43d257":"print(\"Check NaN values in Test set:\")\nisnull = test.isnull().sum().reset_index()\n#isnull[isnull>0]\nisnull.columns = ['Feature', 'Total_null']\ntotal_null = isnull[isnull['Total_null']>0]\ntotal_null","68d2e3c8":"train.describe()","29a7f919":"temp_train = train[['Id', 'idhogar', 'r4h3', 'r4m3', 'r4t3']]\ntemp_train.columns=['Id','idhogar','Total_male','Total_female', 'Total_person']\ntemp_train.head()","d83dc749":"train.loc[train['idhogar'] == '2b58d945f']","11c84565":"# replace all NaN value to -1\n#train.fillna(-1, inplace=True)\n#test.fillna(-1, inplace=True)\ntrain = train.fillna(train.mean())\ntest = test.fillna(test.mean())","6ea608a6":"train = train.replace('no', 0)\ntrain = train.replace('yes', 1)\ntest = test.replace('no', 0)\ntest = test.replace('yes', 1)","1b879ae1":"def extract_features(df):\n    df['bedrooms_to_rooms'] = df['bedrooms']\/df['rooms']\n    df['rent_to_rooms'] = df['v2a1']\/df['rooms']\n    df['rent_to_bedrooms'] = df['v2a1']\/df['bedrooms']\n    df['tamhog_to_rooms'] = df['tamhog']\/df['rooms'] # tamhog - size of the household\n    df['tamhog_to_bedrooms'] = df['tamhog']\/df['bedrooms']\n    df['r4t3_to_tamhog'] = df['r4t3']\/df['tamhog'] # r4t3 - Total persons in the household\n    df['r4t3_to_rooms'] = df['r4t3']\/df['rooms'] # r4t3 - Total persons in the household\n    df['r4t3_to_bedrooms'] = df['r4t3']\/df['bedrooms']\n    df['rent_to_r4t3'] = df['v2a1']\/df['r4t3']\n    df['v2a1_to_r4t3'] = df['v2a1']\/(df['r4t3'] - df['r4t1'])\n    df['hhsize_to_rooms'] = df['hhsize']\/df['rooms']\n    df['hhsize_to_bedrooms'] = df['hhsize']\/df['bedrooms']\n    df['rent_to_hhsize'] = df['v2a1']\/df['hhsize']\n    df['qmobilephone_to_r4t3'] = df['qmobilephone']\/df['r4t3']\n    df['qmobilephone_to_v18q1'] = df['qmobilephone']\/df['v18q1']\n    \nextract_features(train)\nextract_features(test)","10d59c09":"test.info()","32cf7216":"individual_features = ['idhogar','dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4', \n                       'estadocivil5', 'estadocivil6', 'estadocivil7', 'parentesco1', 'parentesco2', 'parentesco3', \n                       'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9', \n                       'parentesco10', 'parentesco11', 'parentesco12', 'meaneduc', 'instlevel1', 'instlevel2', \n                       'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n                       'instlevel9', 'age']","814c2a9d":"from sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n\n# get list features which will be scaled\nlist_features = list(set(list(train)) - set(['Id', 'idhogar', 'Target']))\n\n# create a temp set\nscaled_train = train.copy()\nscaled_test = test.copy()\n\n# fit scaler\nscaled_train[list_features] = min_max_scaler.fit_transform(train[list_features])\nmin_max_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\nscaled_test[list_features] = min_max_scaler.fit_transform(test[list_features])\n\n# transform\n#scaled_train[list_features] = min_max_scaler.transform(train[list_features])\n#scaled_test[list_features] = min_max_scaler.transform(test[list_features])\n","270fb086":"scaled_train.describe()","df29604e":"train.describe()","b17778db":"#scaled_train = scaled_train.drop(columns=['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq'])\n#scaled_test = scaled_test.drop(columns=['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq'])","f37d4c78":"head_household_train = scaled_train[scaled_train['parentesco1']==1]\nhead_household_test = scaled_test[scaled_test['parentesco1']==1]","86182059":"member_household_train = scaled_train[scaled_train['parentesco1']!=1][individual_features]\nmember_household_test = scaled_test[scaled_test['parentesco1']!=1][individual_features]\n#member_household_train = scaled_train[scaled_train['parentesco1']!=1].drop(columns=['Id', 'Target'])\n#member_household_test = scaled_test[scaled_test['parentesco1']!=1].drop(columns=['Id'])","c26b6a72":"def concatenate_features(head_household, member_household):\n    \"\"\"\n    inputs are the dataframe\n    \"\"\"    \n    list_idhogar = []\n    features = -np.ones((head_household.shape[0], (head_household.shape[1]-2)+(member_household.shape[1]-1)*12))\n    for i in tqdm(range(len(head_household))):\n        idhogar = head_household.iloc[i]['idhogar']\n        members = member_household[member_household['idhogar']==idhogar].sort_values(by=['age'])\n        members = members.drop(columns=['idhogar'])\n        list_idhogar.append(idhogar)\n        temp_head_household = head_household[head_household['idhogar']==idhogar].drop(columns=['idhogar', 'Id'])\n        current_index = temp_head_household.shape[1]\n        features[i][:current_index] = np.array(temp_head_household)\n        for j in range(len(members)):\n            next_index = current_index + members.shape[1]\n            features[i][current_index:next_index] = np.array(members.iloc[j])\n            current_index = next_index\n    return (features, list_idhogar)\n            ","9a4f4c9f":"train_target = head_household_train['Target']\nhead_household_train = head_household_train.drop(columns='Target')\ntrain_features, train_idhogar = concatenate_features(head_household_train, member_household_train)","42513cc9":"test_features, test_idhogar = concatenate_features(head_household_test, member_household_test)","54c688d9":"Y_train = np.array(train_target)\nX_train = train_features\nX_test = test_features","636ddf64":"\"\"\"\nfrom sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\nmin_max_scaler.fit(train_features)\nmin_max_scaler.fit(test_features)\nX_train = min_max_scaler.transform(train_features)\nX_test = min_max_scaler.transform(test_features)\n\nprint(\"X_train shape: \", X_train.shape)\nprint(\"Y_train shape: \", Y_train.shape)\nprint(\"X_test shape: \", X_test.shape)\n\"\"\"","224c5756":"# calculate class weigths because of imbalanced classes\nfrom sklearn.utils import class_weight\n\nclass_weights = class_weight.compute_class_weight('balanced', np.unique(Y_train.flatten()), Y_train.flatten())\ndict_class_weights = dict(enumerate(class_weights))\nprint(\"Class weights: \", dict_class_weights)","8a1359f5":"# Transform Y_train to multi-class matrix\nY_train = np.array(Y_train, dtype=int)\nlb = preprocessing.LabelBinarizer()\nlb.fit(Y_train)\nprint(\"Class: \", lb.classes_)\nY_train = lb.transform(Y_train)\nprint(Y_train[0:4])","be6ecead":"# import library\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.optimizers import Adam\nfrom keras.regularizers import l2\nfrom keras.callbacks import EarlyStopping\nfrom sklearn import preprocessing","a30141db":"# second model:\n# constructing model\nnp.random.seed(0)\nmodel_neuron = Sequential()\n#model.add(Dense(output_dim=2048, input_shape=(X_train.shape[1],),\n#               W_regularizer=l2(1.0), activation='relu'))\n#model.add(Dense(output_dim=512,activation='relu'))\n#model.add(Dropout(.3))\n#model.add(Dense(output_dim=256,activation='relu',input_shape=(X_train.shape[1],)))\n#model.add(Dense(output_dim=128,activation='relu',input_shape=(X_train.shape[1],)))\n#model.add(Dense(output_dim=64,activation='relu',input_shape=(X_train.shape[1],), W_regularizer=l2(1.0)))\nmodel_neuron.add(Dense(output_dim=4, input_shape=(X_train.shape[1],), W_regularizer=l2(1.0)))\nmodel_neuron.add(Activation('softmax'))\nmodel_neuron.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-5), metrics=['accuracy'])\nprint(model_neuron.summary())","19b54886":"history = model_neuron.fit(X_train, Y_train, nb_epoch=200, batch_size=32, validation_split=0.1, callbacks=[EarlyStopping(patience=10)], class_weight=dict_class_weights)","d8a6b3f6":"import matplotlib.pyplot as plt\n# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","47c228f7":"print(X_train.shape)\nprint(Y_train.shape)","942e1c2b":"from sklearn.metrics import confusion_matrix\ny = [np.argmax(i)+1 for i in Y_train]\nY_predict = model_neuron.predict(X_train)\nclass_predict = [np.argmax(i)+1 for i in Y_predict]\nconfusion_matrix(y, class_predict, labels=[1,2,3,4])","70cc0a15":"# split train\/test set\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LinearRegression","ed7b8d35":"temp_class_weights = {}\nfor (class_, weight) in dict_class_weights.items():\n    temp_class_weights[class_+1] = weight","df3850a9":"# train on SVM\nnp.random.seed(1)\nX = X_train\nY = y\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.1, random_state=0)\nmodel = SVC(kernel='linear', C=1, class_weight=temp_class_weights).fit(X_train, Y_train)\nprint(\"Training score: \", model.score(X_train, Y_train))\nprint(\"Validation score: \", model.score(X_val, Y_val))\nfrom sklearn.metrics import f1_score\ny_pred = model.predict(X_train)\nprint(\"F1 score training: \", f1_score(Y_train, y_pred, average='macro'))\ny_pred = model.predict(X_val)\nprint(\"F1 score validation: \", f1_score(Y_val, y_pred, average='macro'))","0d65eb23":"class_predict = model.predict(X_train)\nconfusion_matrix(Y_train, class_predict, labels=[1,2,3,4])","d7cefacd":"temp_test = test[['Id', 'idhogar', 'parentesco1']]\n#temp_test = temp_test[temp_test['parentesco1']==1]\ntemp_test_matrix = temp_test.as_matrix()\nclass_predict = model.predict(X_test)\ntest_id = []\npredict_target = []\nfor row_index in temp_test_matrix:\n    try:\n        idhogar_index = test_idhogar.index(row_index[1])\n        predict_target.append(class_predict[idhogar_index])\n    except ValueError:\n        predict_target.append(4)\n    test_id.append(row_index[0])","e111c001":"print(sum(np.array(class_predict)==1))\nprint(sum(np.array(class_predict)==2))\nprint(sum(np.array(class_predict)==3))\nprint(sum(np.array(class_predict)==4))","c1e598de":"# transfer prediction\nsub = pd.DataFrame({'Id':test_id,'Target':predict_target})\noutput = sub[['Id','Target']]\noutput.to_csv(\"output_linear.csv\",index = False)","51fca1c8":"output","4a8da31e":"**Analyzing columns... **","156e0135":"**Try to use linear regression**","36af0b8e":"**Check r4t3, tamhog and hhsize are they the same?**\n","6b4464f2":"**Training**","59d5c7b1":"**Add other features...**","869a5013":"**NOTE:**\n- Because the target is the same for all members in a household. The row indicates information of the person. To combine the information from each member in the household, I separate the features of the household (which are the same for all members) and the individual features (different from each member). And then, I combine those features to denote the final feature for the given household.\n\n**To Do: **\n- Garther row by household\n- Concatenate features\n- Train on single-layer neural network","33f3416c":"\"**Get data of not head household**","970a127b":"**Using information from the household: combine individual in household**","87b647b5":"**Remove(SQBXXX and agesq) out of the training and test set **\n","c2843dd1":"**Predicting null monthly rent values for household**","bc199307":"List features of individual: \n- dis, =1 if disable person\n- male, =1 if male\n- female, =1 if female\n- estadocivil1, =1 if less than 10 years old\n- estadocivil2, =1 if free or coupled uunion\n- estadocivil3, =1 if married\n- estadocivil4, =1 if divorced\n- estadocivil5, =1 if separated\n- estadocivil6, =1 if widow\/er\n- estadocivil7, =1 if single\n- parentesco1, =1 if household head\n- parentesco2, =1 if spouse\/partner\n- parentesco3, =1 if son\/doughter\n- parentesco4, =1 if stepson\/doughter\n- parentesco5, =1 if son\/doughter in law\n- parentesco6, =1 if grandson\/doughter\n- parentesco7, =1 if mother\/father\n- parentesco8, =1 if father\/mother in law\n- parentesco9, =1 if brother\/sister\n-parentesco10, =1 if brother\/sister in law\n- parentesco11, =1 if other family member\n- parentesco12, =1 if other non family member\n- meaneduc,average years of education for adults (18+)\n- instlevel1, =1 no level of education\n- instlevel2, =1 incomplete primary\n- instlevel3, =1 complete primary\n- instlevel4, =1 incomplete academic secondary level\n- instlevel5, =1 complete academic secondary level\n- instlevel6, =1 incomplete technical secondary level\n- instlevel7, =1 complete technical secondary level\n- instlevel8, =1 undergraduate and higher education\n- instlevel9, =1 postgraduate higher education\n- age, Age in years\n\n\n","1d4be6a0":"**Analyzing a given household members**","962c7e41":"**Get data of head household**","92574c21":"**Comeback soon....**","b7a309da":"Scale train and test set","ae393493":"Preprocessing"}}