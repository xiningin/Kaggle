{"cell_type":{"44d2c88d":"code","5217b481":"code","3ee37cf6":"code","da75f5ed":"code","40b18aa7":"code","67639656":"code","e2c7d6cf":"code","160b8853":"code","0859027c":"code","39ed042c":"code","020a3a77":"code","ba95b19f":"code","15d9df03":"code","9dad0646":"code","18c8b67e":"code","0cc501bf":"code","f004cf24":"code","e6df2725":"code","138781d7":"code","5215bb26":"code","96e09345":"code","cf082895":"code","d6f216ff":"code","7ac9ae14":"code","a40ebb9c":"code","3c2b80b9":"code","566b7863":"code","9f54a54a":"code","2390ba56":"code","64ea2b3e":"code","72a1d43b":"markdown","9c97c3df":"markdown","1c3974ac":"markdown","61b8d8ed":"markdown"},"source":{"44d2c88d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5217b481":"df_train = pd.read_csv(\"..\/input\/metal-furnace-dataset\/Train.csv\")\ndf_test = pd.read_csv(\"..\/input\/metal-furnace-dataset\/Test.csv\")\ndf_train[:5]","3ee37cf6":"import matplotlib.pyplot as plt\nimport seaborn as sns","da75f5ed":"plt.rcParams['figure.figsize'] = (10,8)\nsns.countplot(x=df_train[\"f0\"],hue=df_train['grade'])\nplt.xticks(rotation = 90)\nplt.show()","40b18aa7":"df_train.drop(['f9'],inplace = True,axis=1)\ndf_test.drop(['f9'],inplace=True,axis=1)","67639656":"cols = df_train.columns\nn_rows = 9\nn_cols = 3\n# plt.xlabel(fontsize=12\nfor i in range(n_rows):\n    fg,ax = plt.subplots(nrows=1,ncols = n_cols,figsize = (16,8))\n    for j in range(n_cols):\n        sns.violinplot(y = cols[i*n_cols+j],data  = df_train,ax = ax[j])","e2c7d6cf":"cols = df_train.columns\nn_cols  = 2\nn_rows = 14\nfor i in range(n_rows):\n    fg,ax = plt.subplots(nrows = 1,ncols = n_cols,figsize = (17,6))\n    for j in range(n_cols):\n        sns.countplot(x = cols[i*n_cols+j],hue = 'grade',data = df_train,ax = ax[j])","160b8853":"from sklearn.model_selection import train_test_split,cross_val_predict\nfrom sklearn.metrics import log_loss\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier,GradientBoostingClassifier,BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport xgboost as xgb\nimport lightgbm as lgb","0859027c":"df_train.shape,df_test.shape","39ed042c":"df_train.grade.value_counts()","020a3a77":"df_train.isna().sum()","ba95b19f":"plt.rcParams['figure.figsize'] = (14,8)\nsns.countplot(df_train[\"grade\"],hue = df_train[\"grade\"],palette = 'dark')\nplt.title(\"Grade Distribution\",fontsize = 20)\nplt.xlabel(\"Grade\",fontsize = 15)\nplt.ylabel(\"Count\",fontsize = 15)\nplt.show()","15d9df03":"features = list(set(df_train.columns)-set(['grade','f9']))\ntarget = 'grade'\nlen(features)","9dad0646":"def metric(y,y0):\n    return log_loss(y,y0)\ndef cross_valid(model,train,features,target,cv):\n    results = cross_val_predict(model, train[features], train[target], method=\"predict_proba\",cv=cv)\n    return metric(train[target],results)","18c8b67e":"\nmodels = [lgb.LGBMClassifier(), xgb.XGBClassifier(), GradientBoostingClassifier(), LogisticRegression(), \n              RandomForestClassifier(), AdaBoostClassifier()\n             ]\nfor i in models:\n    error =  cross_valid(i,df_train,features,target,5)\n    print(str(i).split(\"(\")[0], error)","0cc501bf":"def xgb_model(train, features, target, plot=True):    \n    evals_result = {}\n    trainX, validX, trainY, validY = train_test_split(train[features], train[target], test_size=0.2, random_state=13)\n    print(\"XGB Model\")\n    \n    dtrain = xgb.DMatrix(trainX, label=trainY)\n    dvalid = xgb.DMatrix(validX, label=validY)\n    watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n    \n    MAX_ROUNDS=2000\n    early_stopping_rounds=100\n    params = {\n        'booster': 'gbtree',\n        'objective': 'multi:softprob',\n        'eval_metric': 'mlogloss',\n        'learning_rate': 0.01,\n        'num_round': MAX_ROUNDS,\n        'max_depth': 8,\n        'seed': 25,\n        'nthread': -1,\n        'num_class':5\n    }\n    \n    model = xgb.train(\n        params,\n        dtrain,\n        evals=watchlist,\n        num_boost_round=MAX_ROUNDS,\n        early_stopping_rounds=early_stopping_rounds,\n        verbose_eval=50\n        #feval=metric_xgb\n    \n    )\n    \n    print(\"Best Iteration :: {} \\n\".format(model.best_iteration))\n    \n    \n    if plot:\n        # Plotting Importances\n        fig, ax = plt.subplots(figsize=(24, 24))\n        xgb.plot_importance(model, height=0.4, ax=ax)","f004cf24":"xgb_model(df_train, features, target, plot=True)\n","e6df2725":"xgb1 = xgb.XGBClassifier(\n    booster='dart',\n    objective='multi:softprob',\n    learning_rate= 0.01,\n    num_round= 775,\n    max_depth=8,\n    seed=25,\n    nthread=3,\n    eval_metric='mlogloss',\n    num_class=5\n\n)","138781d7":"trainX, validX, trainY, validY = train_test_split(df_train[features], df_train[target], test_size=0.2,stratify=df_train[target], random_state=13)","5215bb26":"model  = xgb1\ncross_valid(model,df_train,features,target,5)","96e09345":"model = xgb1\nmodel.fit(trainX[features],trainY)\ny_pred_valid = model.predict_proba(validX[features])\nprint(\"Validation Score:\",metric(validY,y_pred_valid))\ny_pred_test = model.predict(df_test[features])\nresult1 = pd.DataFrame(y_pred_test)","cf082895":"result1[:5]","d6f216ff":"y_pred_test = model.predict_proba(df_test[features])\nresult = pd.DataFrame(y_pred_test)\nresult","7ac9ae14":"result1","a40ebb9c":"X= df_train[features]\ny = df_train[target]\ntrainX, validX, trainY, validY = train_test_split(df_train[features], df_train[target], test_size=0.2,stratify=df_train[target], random_state=13)","3c2b80b9":"from sklearn import ensemble\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn import decomposition\nfrom sklearn import pipeline\n\n\nclassifier = ensemble.RandomForestClassifier(n_jobs = -1)\n\nparam_grid = {\n    \"n_estimators\":[100,200,300,400],\n    \"max_depth\":[1,3,5,7],\n    \"criterion\":[\"gini\",\"entropy\"]\n}\nmodel = model_selection.GridSearchCV(\n    estimator = classifier,\n    param_grid  = param_grid,\n    scoring = \"accuracy\",\n    verbose = 10,\n    n_jobs = 1,\n    cv=5\n)\n\nmodel.fit(trainX,trainY)\nprint(model.best_score_)\nprint(model.best_estimator_.get_params())\n","566b7863":"\nfrom sklearn.metrics import classification_report\npreds = model.predict(validX)\nprint(metrics.accuracy_score(preds,validY))\nprint(classification_report(preds,validY))","9f54a54a":"from sklearn import preprocessing\npca = decomposition.PCA() \nrf  = ensemble.RandomForestClassifier(n_jobs = -1)\nscl = preprocessing.StandardScaler()\nclassifier1 = pipeline.Pipeline([(\"scaling\",scl),(\"pca\",pca),(\"rf\",rf)])\n\nparam_grid = {\n    \"pca__n_components\":np.arange(10,15),\n    \"rf__n_estimators\":np.arange(100,1500,100),\n    \"rf__max_depth\":np.arange(1,20),\n    \"rf__criterion\": [\"gini\",\"entropy\"]\n}\n\nmodel = model_selection.RandomizedSearchCV(\n    estimator = classifier1,\n    param_distributions = param_grid,\n    n_iter = 10,\n    scoring = 'accuracy',\n    verbose = 10,\n    n_jobs = 1,\n    cv = 5\n)\n\nmodel.fit(trainX,trainY)\nprint(model.best_score_)\nprint(model.best_estimator_.get_params())","2390ba56":"from sklearn.metrics import classification_report\npreds = model.predict(validX)\nprint(metrics.accuracy_score(preds,validY))\nprint(classification_report(preds,validY))","64ea2b3e":"def optimize(params,param_name,x,y):\n    params = dict(zip(param_name,params))\n    model = ensemble.RandomForestClassifier(**params)\n    kf =model_selection.StratifiedKFold(n_splits=5)\n    accuracies = []\n    for idx in kf.split(X=x,y=y):\n        train_idx , test_idx = idx[0],idx[1]\n        xtrain = X[train_idx]\n        ytrain = y[train_idx]\n        \n        xtest = X[test_idx]\n        ytest = y[test_idx]\n        \n        model.fit(xtrain,ytrain)\n        preds = model.predict(xtest)\n        accuracies.append(metrics.accuracy_score(preds,ytest))\n    return -1.0 * np.mean(accuracies)  \n\nfrom functools import partial\nfrom skopt import space\nfrom skopt import gp_minimize\n\ntrainX, validX, trainY, validY = train_test_split(df_train[features], df_train[target], test_size=0.2,stratify=df_train[target], random_state=13)\n\nparam_space = [\n    space.Integer(3,15,name = \"max_depth\"),\n    space.Integer(100,600,name = \"n_estimators\"),\n    space.Categorical([\"gini\",\"entropy\"],name = \"criterion\"),\n    space.Real(0.01,1,prior = 'uniform', name = \"max_features\")\n    \n]\nparam_names = [\n    \"max_depth\",\n    \"n_estimators\",\n    \"criterion\",\n    \"max_features\"\n]\noptim_func = partial(\n    optimize,\n    param_name = param_names,\n    x=trainX,\n    y = trainY\n)\n\n\nresult = gp_minimize(\n            optim_func,\n            dimensions = param_space,\n            n_calls = 15,\n            n_random_starts = 10,\n            verbose = 10,\n)\n\nprint(dict(zip(param_names,result.x)))","72a1d43b":"df_test.drop(labels = \"\")","9c97c3df":"* Randomiazed Search CV","1c3974ac":"* Sklearn Optimization  (Skopt)","61b8d8ed":"# Hyperparameter optimization(RandomForest)\n* Grid Search CV"}}