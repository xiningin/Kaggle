{"cell_type":{"067fb01f":"code","513a1e7d":"code","6084c14e":"code","db8da352":"code","27a621c3":"code","b133ecbc":"code","671fd024":"code","a8fc28b3":"code","54e37929":"code","96a70a75":"code","ecae597d":"code","2a16a07a":"code","411e52b9":"code","eb94ca5c":"code","edb3ad00":"code","3495e030":"code","99f8fb78":"code","391e0754":"code","56de7a3c":"code","a44637d6":"code","f2e93cc7":"code","23641add":"code","ca2691dc":"code","2dcbf783":"code","140e45f6":"code","a187236d":"code","e8cdf50f":"code","5fd6a985":"code","0e74dde4":"code","8ce088b8":"code","c68ead5d":"code","856ca4a3":"code","f741bc17":"code","31678fc0":"code","21df75c6":"code","441f4897":"code","fddf772d":"code","0c29a014":"code","75353568":"code","99e2facf":"code","cf631523":"code","d9d85487":"code","e60f4e2b":"code","13c8880f":"code","f06fffe4":"code","2b2b3233":"code","e004e3a8":"code","8204457d":"code","b10a6211":"code","f41640b9":"code","7db4178f":"code","6f91375e":"code","7ceb9d42":"code","22cba71d":"code","697ec608":"code","6120e51e":"code","18864875":"code","95ed0acd":"code","ecaac67b":"code","5542ee17":"code","8fa38611":"code","3e375db2":"code","aa2a6f37":"code","493dce75":"code","16c1c1fa":"code","2db5479f":"code","9742fb50":"markdown","40f6346f":"markdown","ea334fcb":"markdown","cd61eac5":"markdown","f3a1b677":"markdown","7fa8fb05":"markdown","b8c4b59e":"markdown","33cbb1e0":"markdown","bf9469ae":"markdown","0359ecd1":"markdown","1bd7b669":"markdown","da74d9d5":"markdown","7c2a8a35":"markdown","5fdd8ec4":"markdown","bb52ef71":"markdown","3fd27eeb":"markdown","d97a3cde":"markdown","9da8dcd5":"markdown","0faf3b7a":"markdown","ebcbd3b4":"markdown","44aa0251":"markdown","02f68624":"markdown","f4d28cd1":"markdown","04b0e7a1":"markdown","3f1d4e38":"markdown","feb7d2d5":"markdown"},"source":{"067fb01f":"dfTrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndfTest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nprint(dfTrain.shape)\nprint(dfTest.shape)","513a1e7d":"print('There are {t} passengers in train data, of which {s} made it and {d} did not'.format(t = dfTrain.PassengerId.nunique(), s = dfTrain[dfTrain['Survived'] == 1].PassengerId.nunique(), d = dfTrain[dfTrain['Survived'] == 0].PassengerId.nunique()))","6084c14e":"dfTrain.head()","db8da352":"dfTest.head()","27a621c3":"print(dfTrain.columns)\ndfTrain.describe()","b133ecbc":"# Turn Pclass into a cateogrical variable\ndfTrain['Pclass'] = dfTrain.Pclass.astype('category')","671fd024":"# Show missing value counts by columns\ndfTrain.isnull().sum()","a8fc28b3":"dfTrain.groupby(['Sex', 'Pclass']).agg({'Age': ['mean', 'median', 'min', 'max']}).reset_index()","54e37929":"# assign passengers with missing age the average age of gender and class\navgAge = dfTrain.groupby(['Sex', 'Pclass']).Age.mean().reset_index()\ndfTrain = dfTrain.merge(avgAge, 'left', on = ['Sex', 'Pclass'], suffixes = ('_orig', '_avg'))\ndfTrain.head(5)","96a70a75":"# Fill in average age if original age value is NaN\ndef fill_in_avg_age(row):\n    if np.isnan(row['Age_orig']):\n        return row['Age_avg']\n    else:\n        return row['Age_orig']\n\ndfTrain['Age'] = dfTrain.apply(lambda row: fill_in_avg_age(row), axis = 1) ","ecae597d":"# Double check if things are filled in correctly\ndfTrain[dfTrain.Age_orig.isnull()].head()","2a16a07a":"print(dfTrain.Cabin.unique())\nprint('There are {n} different unique Cabin values'.format(n = dfTrain.Cabin.nunique()))\ndfTrain[dfTrain.Cabin.isnull()].groupby('Pclass').PassengerId.nunique()","411e52b9":"# Fill unknown Cabin with 'U'\ndfTrain.loc[dfTrain.Cabin.isnull(), 'Cabin'] = 'U'\n\n# extract first letter of Cabin to indicate location on the ship\ndfTrain['CabinLoc'] = dfTrain['Cabin'].str[0]\n\n# assign an indicator to show if the passenger has Cabin assignment or not\ndef assign_cabin_ind(row):\n    if row['Cabin'] == 'U':\n        return 0\n    else:\n        return 1\n\ndfTrain['CabinInd'] = dfTrain.apply(lambda x: assign_cabin_ind(x), axis = 1).astype('category')","eb94ca5c":"print(dfTrain.groupby('CabinLoc').PassengerId.nunique().reset_index())\nprint(dfTrain.groupby(['Pclass', 'CabinInd']).PassengerId.nunique().reset_index())\nprint(dfTrain.groupby(['Pclass', 'CabinInd']).agg({'PassengerId': 'count', 'Fare': 'mean'}).reset_index())","edb3ad00":"dfTrain['CabinInd'] = dfTrain['CabinInd'].astype('category')","3495e030":"# assign embark based on people's fare\ndfTrain.groupby(['Embarked']).agg({'Fare': ['min', 'mean', 'max']}).reset_index()","99f8fb78":"dfTrain['CLow'] = dfTrain[dfTrain['Embarked'] == 'C'].Fare.quantile(0.15)\ndfTrain['CHigh'] = dfTrain[dfTrain['Embarked'] == 'C'].Fare.quantile(0.85)\ndfTrain['QLow'] = dfTrain[dfTrain['Embarked'] == 'Q'].Fare.quantile(0.15)\ndfTrain['QHigh'] = dfTrain[dfTrain['Embarked'] == 'Q'].Fare.quantile(0.85)\ndfTrain['SLow'] = dfTrain[dfTrain['Embarked'] == 'S'].Fare.quantile(0.15)\ndfTrain['SHigh'] = dfTrain[dfTrain['Embarked'] == 'S'].Fare.quantile(0.85)","391e0754":"dfTrain.loc[dfTrain.Embarked.isnull(), 'Embarked'] = 'U'","56de7a3c":"def assign_missing_embarked(row):\n    if row['Embarked'] != 'U':\n        return row['Embarked']\n    else:\n        if row['Fare'] <= row['CHigh'] and row['Fare'] >= row['CLow']:\n            return 'C'\n        elif row['Fare'] <= row['SHigh'] and row['Fare'] >= row['SLow']:\n            return 'S'\n        elif row['Fare'] <= row['QHigh'] and row['Fare'] >= row['QLow']:\n            return 'Q'\n        else:\n            return 'U'\n\ndfTrain['Embarked_clean'] = dfTrain.apply(lambda x: assign_missing_embarked(x), axis = 1)","a44637d6":"dfTrain.loc[dfTrain['Embarked'] == 'U', ['Embarked', 'Embarked_clean']]","f2e93cc7":"dfTrain = dfTrain.drop(columns = ['Age_orig', 'Embarked', 'Age_avg', 'Cabin', 'CLow', 'CHigh', 'QLow', 'QHigh', 'SLow', 'SHigh'])\ndfTrain = dfTrain.rename(columns = {'Embarked_clean': 'Embarked'})\nprint(dfTrain.columns)","23641add":"print(dfTrain.isnull().sum())\nprint(dfTrain.shape)","ca2691dc":"# code to extract titles from names\ndef extract_title(row):\n    return row['Name'].split(',')[1].split('.')[0].strip()\n\ndfTrain['title'] = dfTrain.apply(lambda x: extract_title(x), axis = 1)","2dcbf783":"dfTrain.groupby('title').PassengerId.nunique()","140e45f6":"# categorize titles into Military, Religion, Noble and Civilian\ndef categorize_titles(row):\n    if row['title'] in ['Capt', 'Col', 'Major']:\n        return 'Military'\n    elif row['title'] in ['Rev', 'Dr']:\n        return 'Religion'\n    elif row['title'] in ['Don', 'Dona', 'Jonkheer', 'Lady', 'Master', 'Sir', 'the Countess']:\n        return 'Noble'\n    else:\n        return 'Civilian'\n\ndfTrain['TitleCate'] = dfTrain.apply(lambda x: categorize_titles(x), axis = 1)","a187236d":"dfTrain.groupby('TitleCate').PassengerId.count()","e8cdf50f":"import re\ndef extract_names(row):\n    if row['title'] in ['Mrs', 'the Countess']:\n        s = row['Name'].split(',')[1]\n        return re.sub('^.*\\((.*?)\\)[^\\(]*$', '\\g<1>', s)\n    else:\n        return row['Name'].split(',')[1].split('.')[1].strip() + ' ' + row['Name'].split(',')[0]\n        \n\ndfTrain['RealName'] = dfTrain.apply(lambda x: extract_names(x), axis = 1)","5fd6a985":"dfTrain.head()","0e74dde4":"# for Mrs's, extract their husbands name and create a list of husband names\ndef extract_husband_name(row):\n    if row['title'] == 'Mrs':\n        return row['Name'].split(',')[1].split('.')[1].split('(')[0].strip() + ' ' + row['Name'].split(',')[0].strip()\n    else:\n        return 'Unknown'\n    \ndfTrain['HusbandName'] = dfTrain.apply(lambda x: extract_husband_name(x), axis = 1)","8ce088b8":"husband_list = dfTrain[dfTrain['HusbandName'] != 'Unknown'].HusbandName.tolist()\nname_list = dfTrain['RealName'].tolist()","c68ead5d":"dfTrain.head()","856ca4a3":"\ndef assign_couple_onboard_ind(row):\n    if row['title'] == 'Mrs':\n        if row['HusbandName'] in name_list:\n            return 1\n        else:\n            return 0\n    else:\n        if row['RealName'] in husband_list:\n            return 1\n        else:\n            return 0\n\ndfTrain['CoupleOnboardInd'] = dfTrain.apply(lambda x: assign_couple_onboard_ind(x), axis = 1)","f741bc17":"dfTrain['CoupleOnboardInd'] = dfTrain['CoupleOnboardInd'].astype('category')","31678fc0":"dfTrain.groupby(['CoupleOnboardInd']).PassengerId.count()","21df75c6":"dfTrain.head()","441f4897":"dfTrain = dfTrain.drop(columns = ['Name', 'RealName', 'HusbandName', 'Ticket'])","fddf772d":"dfTrain['TravelCompanionSize'] = dfTrain['SibSp'] + dfTrain['Parch']\ndef travel_companions(row):\n    if row['TravelCompanionSize'] == 0:\n        return 'Single Traveler'\n    elif row['TravelCompanionSize'] <= 4:\n        return 'Small Travel Group'\n    else:\n        return 'Big Travel Group'\n\ndef family_size(row):\n    if row['Parch'] == 0:\n        return 'Not with family'\n    elif row['Parch'] <= 4:\n        return 'Small family'\n    else:\n        return 'Big family'\n\ndfTrain['TravelType'] = dfTrain.apply(lambda x: travel_companions(x), axis = 1)\ndfTrain['FamilySize'] = dfTrain.apply(lambda x: family_size(x), axis = 1)","0c29a014":"dfTrain.head()","75353568":"import matplotlib.pyplot as plt\nimport seaborn as sns","99e2facf":"f, axes = plt.subplots(3, 2, figsize = (15, 20))\nsns.countplot(x = 'Pclass', hue = 'Sex', data = dfTrain, ax = axes[0, 0])\nsns.boxplot(x = \"Pclass\", y = \"Age\", hue = \"Sex\", data = dfTrain, ax = axes[0, 1])\nsns.stripplot(x = 'Pclass', y = 'SibSp', jitter = False, data = dfTrain, ax = axes[1, 0])\nsns.stripplot(x = 'Pclass', y = 'Parch', jitter = False, data = dfTrain, ax = axes[1, 1])\nsns.boxplot(x = 'Pclass', y = 'Fare', data = dfTrain, ax = axes[2, 0])\nsns.boxplot(x = 'Embarked', y = 'Fare', data = dfTrain, ax = axes[2, 1])\nf.show()","cf631523":"f, axes = plt.subplots(2, 4, figsize = (30, 15))\nsns.countplot(x = 'Sex', hue = 'Survived', data = dfTrain, ax = axes[0, 0])\nsns.countplot(x = 'Pclass', hue = 'Survived', data = dfTrain, ax = axes[0, 1])\nsns.boxplot(x = \"Survived\", y = \"Age\", hue = \"Sex\", data = dfTrain, ax = axes[0, 2])\nsns.countplot(y = 'CabinLoc', hue = 'Survived', order = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'T', 'U'], data = dfTrain, ax = axes[0, 3])\nsns.countplot(x = 'TravelType', hue = 'Survived', data = dfTrain, ax = axes[1, 0])\nsns.countplot(x = 'FamilySize', hue = 'Survived', data = dfTrain, ax = axes[1, 1])\nsns.countplot(x = 'CoupleOnboardInd', hue = 'Survived', data = dfTrain, ax = axes[1, 2])\nsns.countplot(x = 'TitleCate', hue = 'Survived', data = dfTrain, ax = axes[1, 3])\nf.show()","d9d85487":"dfTest.describe()","e60f4e2b":"dfTest.isnull().sum()","13c8880f":" # data processing function to feature engineer test data\ndef feature_engineering(dat):\n    df = dat.copy()\n    \n    # fill missing Age\n    avgAge = df.groupby(['Sex', 'Pclass']).Age.mean().reset_index()\n    df = df.merge(avgAge, 'left', on = ['Sex', 'Pclass'], suffixes = ('_orig', '_avg'))\n    df['Age'] = df.apply(lambda row: fill_in_avg_age(row), axis = 1) \n    \n    # fill missing Fare\n    avgFare = df.groupby('Embarked').Fare.mean().reset_index()\n    df = df.merge(avgFare, 'left', on = 'Embarked', suffixes = ('_orig', '_avg'))\n    def fill_fare(row):\n        if np.isnan(row['Fare_orig']):\n            return row['Fare_avg']\n        else:\n            return row['Fare_orig']\n    df['Fare'] = df.apply(lambda x: fill_fare(x), axis = 1)\n    \n    # fill missing Cabin\n    df.loc[df.Cabin.isnull(), 'Cabin'] = 'U'\n    df['CabinLoc'] = df['Cabin'].str[0]\n    df['CabinInd'] = df.apply(lambda x: assign_cabin_ind(x), axis = 1).astype('category')\n    \n    # add feature engineered columns\n    df['title'] = df.apply(lambda x : extract_title(x), axis = 1)\n    df['TitleCate'] = df.apply(lambda x: categorize_titles(x), axis = 1)\n    df['RealName'] = df.apply(lambda x: extract_names(x), axis = 1)\n    df['HusbandName'] = df.apply(lambda x: extract_husband_name(x), axis = 1)\n    husband_list = df[df['HusbandName'] != 'Unknown'].HusbandName.tolist()\n    name_list = df['RealName'].tolist()\n    df['CoupleOnboardInd'] = df.apply(lambda x: assign_couple_onboard_ind(x), axis = 1)\n    \n    # final clean-up\n    df['Pclass'] = df['Pclass'].astype('category')\n    df['CabinInd'] = df['CabinInd'].astype('category')\n    df['CoupleOnboardInd'] = df['CoupleOnboardInd'].astype('category')\n    df = df.drop(columns = ['Age_orig', 'Age_avg', 'Fare_orig', 'Fare_avg', 'Name', 'RealName', 'HusbandName', 'Ticket'])\n    \n    return df\n    ","f06fffe4":"dfTestClean = feature_engineering(dfTest)\ndfTestClean.head()","2b2b3233":"from sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, classification_report, confusion_matrix\nimport time as t","e004e3a8":"colID = ['PassengerId']\ncolLabel = ['Survived']\ncolNum = ['Age', 'SibSp', 'Parch', 'Fare']\ncolCat = ['Sex', 'Pclass', 'CabinLoc', 'CabinInd', 'Embarked', 'title', 'TitleCate', 'CoupleOnboardInd']\ny = dfTrain['Survived'].astype('category')\nX = dfTrain[colNum + colCat]","8204457d":"XTrain, XValid, yTrain, yValid = train_test_split(X, y, test_size = 0.15, random_state = 777, stratify = y)","b10a6211":"# Center and scale numeric variables and one hot coding for categorical variables\n# train encoders on training data and apply it on validation and test data\nscaler = StandardScaler().fit(XTrain[colNum])\nencoder = OneHotEncoder(handle_unknown = 'ignore').fit(XTrain[colCat])\ndef apply_scaler_encoder(dat):\n    \n    df = dat.copy()\n    print('Shape of original data')\n    print(df.shape)\n    dfScaled = scaler.transform(df[colNum])\n    dfEncoded = encoder.transform(df[colCat]).toarray()\n    dfFinal = np.concatenate([dfScaled, dfEncoded], axis = 1)\n    print('Shape of processed data')\n    print(dfFinal.shape)\n    \n    return dfFinal","f41640b9":"XTrainFinal = apply_scaler_encoder(XTrain)\nXValidFinal = apply_scaler_encoder(XValid)","7db4178f":"modelsToFit = {\n    'Logistic Regression': LogisticRegression(random_state = 777),\n    'SVM': SVC(random_state = 777, probability = True),\n    'Naive Bayes': GaussianNB(),\n    'Decision Tree': DecisionTreeClassifier(random_state = 777),\n    'Random Forest': RandomForestClassifier(random_state = 777),\n    'AdaBoost': AdaBoostClassifier(random_state = 777),\n    'GBT': GradientBoostingClassifier(random_state = 777),\n    'XGB': XGBClassifier(random_state = 777)\n}\n\n\n\ndef batch_fit_models(xT, yT, xV, yV, models):\n\n    # initiate a dictionary to record model results\n    resultCols = [\n        'Model', 'Train Time', \n        'Train Accuracy', 'Validation Accuracy',\n        'Train Precision', 'Validation Precision',\n        'Train Recall', 'Validation Recall',\n        'Train f1', 'Validation f1',\n        'Train AUC', 'Validation AUC'\n    ]\n\n    result = dict([(key, []) for key in resultCols])\n    \n    # batch train models\n    for model_name, model in models.items():\n        \n        result['Model'].append(model_name)\n        \n        # train model and record time laps\n        trainStart = t.process_time()\n        fit = model.fit(xT, yT)\n        trainEnd = t.process_time()\n        \n        # back fit the model on train data\n        predLabelTrain = fit.predict(xT)\n        predScoreTrain = fit.predict_proba(xT)[:,1]\n        \n        # fit the model on validation data\n        predLabel = fit.predict(xV)\n        predScore = fit.predict_proba(xV)[:,1]\n        \n        # create data for result dict\n        result['Train Time'].append(trainEnd - trainStart)\n        result['Train Accuracy'].append(accuracy_score(yT, predLabelTrain))\n        result['Validation Accuracy'].append(accuracy_score(yV, predLabel))\n        result['Train Precision'].append(precision_score(yT, predLabelTrain))\n        result['Validation Precision'].append(precision_score(yV, predLabel))\n        result['Train Recall'].append(recall_score(yT, predLabelTrain))\n        result['Validation Recall'].append(recall_score(yV, predLabel))\n        result['Train f1'].append(f1_score(yT, predLabelTrain))\n        result['Validation f1'].append(f1_score(yV, predLabel))\n        result['Train AUC'].append(roc_auc_score(yT, predScoreTrain))\n        result['Validation AUC'].append(roc_auc_score(yV, predScore))\n        \n    # turn result dict into a df\n    dfResult = pd.DataFrame.from_dict(result)\n    \n    return dfResult","6f91375e":"batch_fit_models(XTrainFinal, yTrain, XValidFinal, yValid, modelsToFit).sort_values(by = 'Validation AUC', ascending = False)","7ceb9d42":"svmFit = modelsToFit['SVM'].fit(XTrainFinal, yTrain)","22cba71d":"svmFit.get_params()","697ec608":"svmPredLabel = svmFit.predict(XValidFinal)\nsvmPredScore = svmFit.predict(XValidFinal)","6120e51e":"print(classification_report(yValid, svmPredLabel))","18864875":"confusion_matrix(yValid, svmPredLabel)","95ed0acd":"from sklearn.model_selection import GridSearchCV","ecaac67b":"paramGrid = {\n    'kernel': ('linear', 'rbf', 'poly'),\n    'gamma': ('auto', 'scale'),\n    'C': [0.1, 0.5, 1],\n    'degree': [3, 5]\n}\n\n\nsvcTune = GridSearchCV(\n    estimator = modelsToFit['SVM'],\n    param_grid = paramGrid,\n    scoring = 'accuracy'\n)\n\nsvcCVResults = svcTune.fit(XTrainFinal, yTrain)","5542ee17":"print('Best model parameters')\nprint(svcCVResults.best_params_)\nprint('Best model score')\nprint(svcCVResults.best_score_)","8fa38611":"bestSVC = SVC(\n    random_state = 777,\n    probability = True,\n    C = 0.5,\n    degree = 3,\n    gamma = 'scale',\n    kernel = 'rbf'\n)","3e375db2":"print(classification_report(yValid, bestSVC.fit(XTrainFinal, yTrain).predict(XValidFinal)))","aa2a6f37":"testID = dfTestClean[colID]\nXTest = dfTestClean[colNum + colCat]\nXTestFinal = apply_scaler_encoder(XTest)","493dce75":"testPred = bestSVC.predict(XTestFinal)","16c1c1fa":"submission = pd.concat([testID, pd.DataFrame(testPred)], axis = 1)\nsubmission = submission.rename(columns = {0: 'Survived'})\nsubmission.to_csv('titanic_submission_20200601.csv', index = False)","2db5479f":"submission","9742fb50":"## 3. Fit the model to test data and submit predictions!","40f6346f":"## 2.Train models","ea334fcb":"From the Name it is also possible to extract names of husbands from married female passengers (Mrs's) and check male passengers in a pool of married gentlemen to indicate if the husband is onboard with the wife. ","cd61eac5":"### 2.1 Train baseline models and pick the best model for further tuning\nNext let's split the train data for training and validation and batch-train a bunch of models","f3a1b677":"Now that we have examined and cleaned up the data, let's take a look of columns and keep\/rename of those we want to keep.","7fa8fb05":"In general, there are more male and lower class passengers. Upper class passengers tend to be older and have smaller family size\/less travel companions.","b8c4b59e":"### 2.0 Process test data\nFirst I want to decide what variables to include and categorize them. Then I want to write up a function to process data and do it for test data set as well.","33cbb1e0":"Next let's examine Cabin. It seems that Cabin assignment may be related to Pclass so we can also take a look at missing values by Pclass.","bf9469ae":"Here is the list of best models in terms of ...\n* Accuracy: SVM\n* Precision: SVM\n* Recall: AdaBoost\n* f1 score: GBT\n* AUC: SVM\n\nSince my model will be scored on accuracy, I'll use accuracy as my metric as well and pick SVM to do parameter tuning.","0359ecd1":"### 0.0 Describe train data and process missing values\/outliers","1bd7b669":"## 1. More EDA and visualization","da74d9d5":"Next let's find out missing Embarked from fare range. I'm going to examine the range of Fare for each embarkment port and use 15th and 85th percentile as lower and higher ends. For passengers missing the port, if their Fare price falls within the range for a port, the port will be assigned to them. The order of assignment will be C, S, Q.","7c2a8a35":"## 0.Load data and basic feature engineering\nAs the first step, let's load titanic train and test data and take a look at the data records.","5fdd8ec4":"Then write up a function to batch train and evaluate several models. Pick the best performing one for parameter\/hyperparameter tuning later.","bb52ef71":"Some initial observations:\n* From the statistics above there doesn't seem to be obvious outliers\n* Pclass is taken as a numeric variable from the original dataset, but apparently it should be categorical. Let's turn it into a categorical variable","3fd27eeb":"Titanic is such a trategy and mystery. There are lots of theories and anecdotes about why \"the ship that will never sink\" actually sunk as well as who are the survivors VS non-survivors. It would be interesting to see how data analysis and machine learning algorithm can give us more insights on the characteristics and likelihood of survival of Titanic shipwreck. \n\nIn this analysis I would like to focus on the following topics:\n* Data processing and feature engineering\n* Model training and selection\n* Parameter\/hyperparameter tuning for selected model","d97a3cde":"Next let's check for missing values and come up with strategies to fill them in.","9da8dcd5":"In summary, after more feature engineering, I can add three more variables to the original data set: CabinLoc, CabinInd, title, TitleCate and CoupleOnboardInd. I want to drop name related variables and Ticket because I don't think they could help with further EDA and modeling\/scoring. The final data set looks like below:","0faf3b7a":"### 0.1 More featuring engineering\nThere are two variables that may give out more information about the passegners: Name and Ticket.\nLet's look at ticket first. Ideally the ticket information may tell you important information such as where the passenger boarded\/unboarded and the starting port and destination port, etc. But unfortunately this time I don't have enough knowledge to know the coding of ticket numbers for this data set so won't be able to find out pattern (although I did notice a 'Paris' in one ticket number) so I'll pass this one for now. \n\nAs for Name variable, I noticed I can extract title from the names, and for married women their own names and husbands' names, so maybe it is possible to know more detailed background of passengers (like if they are noble, military officers, religion\/academic professionals or civilians).","ebcbd3b4":"Now that I have the model-ready data set, I would like to create some visualizations to further look at details of the data. There are two topics I'm interested:\n* General profiles of passengers\n* Characteristics of survivors vs non-survivors","44aa0251":"### 2.2 Parameter\/Hyperparameter tuning for SVM","02f68624":"SibSp and Parch can determine # travel companions and family size","f4d28cd1":"There are 147 different values of Cabin, and they are in the format of 'X00'. Also the upper class tickets have much less missing values for Cabin, indicating that upper class passengers have an assigned room on the ship while lower class passengers may not (just standby?). We can extract the first letter of Cabin variable and find out.","04b0e7a1":"There are three variables with missing values: Age, Cabin and Embarked. Here are my thoughts on strategies to handle missing values:\n1. Age: since Pclass and Sex do not have missing values, we can calculate average age group by Pclass and Sex and fill in missing values accordingly\n2. Cabin: this variable may indicate where the passenger's cabin room is in the ship, and may have an impact on survival rate. But since people could move around in the ship this variable may not contribute too much useful information, especially given the # of missing values. But let's take a look and see what we can find\n3. Embarked: this should be related to Fare. We can take a look at Fare range for each existing Embarked variable and decide which Embarked value should be given the Fare","3f1d4e38":"The survivors may have the following characteristics:\n1. Upper class\/noble female passengers between 20 - 40\n2. On deck B-F\n3. Travel with smaller group or smaller family size\n4. Their spouse\/partner is on deck as well\n\nLet's fit some models and see if model results align with what we observed","feb7d2d5":"The best SVC model has the following parameters: C = 0.5, gamma is 1 \/ (n_features * X.var()), and kernel is rbf"}}