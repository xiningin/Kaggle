{"cell_type":{"c269360a":"code","96f0b591":"code","a63ec745":"code","baa825fd":"code","62524099":"code","9c9ead5b":"code","4f4e7c48":"code","e3ce1c18":"code","23dd7683":"code","1d32acf2":"code","7a64df29":"code","27009210":"code","bbae52ba":"code","4607d56a":"code","2844ae7e":"code","55f938a9":"code","0bb88b69":"code","0e903eaf":"code","828c3c64":"code","a6b51a63":"code","95499d6c":"code","1c9427bc":"code","7e451ede":"code","e70f95cb":"code","fe3b2916":"code","71baa60a":"code","7af6f6eb":"code","da3a4faa":"markdown","c828317f":"markdown","b813e9cb":"markdown","62c131e8":"markdown","0f4f7238":"markdown","34bf6fdc":"markdown","a7569a02":"markdown","bd6fcd4b":"markdown","720cdac1":"markdown","4065bf23":"markdown","e2061b37":"markdown","459f29f2":"markdown","87c27057":"markdown","15a1517e":"markdown","94758d26":"markdown","dcd4ac38":"markdown","1c632c47":"markdown","7450f985":"markdown","1d626468":"markdown","1b00d66c":"markdown","d9b918a9":"markdown","05f67f9f":"markdown","b718efc3":"markdown","b5c023e1":"markdown","cb9e0f04":"markdown","4560de61":"markdown","2ecf98cb":"markdown","d105c9c7":"markdown","14dba605":"markdown","0129b38b":"markdown","0f8dce2c":"markdown","501e5695":"markdown"},"source":{"c269360a":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")","96f0b591":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","a63ec745":"sns.distplot(df_train.SalePrice)","baa825fd":"sns.boxplot(df_train['SalePrice'])","62524099":"print('Skewness: ',df_train['SalePrice'].skew())\nprint('Kurtosis: ',df_train['SalePrice'].kurt())","9c9ead5b":"df_train['SalePrice']=np.log1p(df_train['SalePrice'])\n\nsns.distplot(df_train['SalePrice'])","4f4e7c48":"fig, ax = plt.subplots()\nax.scatter(x = df_train['GrLivArea'], y = df_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","e3ce1c18":"df_train.drop(df_train[df_train['GrLivArea']>4000].index, inplace=True)","23dd7683":"ntrain = df_train.shape[0] #we do this because we are going to concatenate train and test and we will need this later\nntest = df_test.shape[0] \n\ny_train = df_train.SalePrice.values\n\ndf_all = pd.concat((df_train, df_test)).reset_index(drop=True)\ndf_all.drop(['SalePrice'], axis=1, inplace=True)\n\ndf=df_all","1d32acf2":"def overview(df):\n    print('SHAPE: ', df.shape)\n    print('columns: ', df.columns.tolist())\n    col_nan=df.columns[df.isnull().any()].tolist()\n    print('columns with missing data: ',df[col_nan].isnull().sum())\n\noverview(df)","7a64df29":"all_data_na = (df.isnull().sum() \/ len(df)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data","27009210":"corrmat = df_train.corr()\ncorrmat['SalePrice'].sort_values(ascending=False)","bbae52ba":"import matplotlib.pyplot as plt#visualization\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nplt.figure(figsize=(10,10))\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 15}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","4607d56a":"cat_cols = df.select_dtypes('object').columns.tolist()\n\nfor i in cat_cols:\n    print(df[i].value_counts())","2844ae7e":"int_cols =  df.select_dtypes(['int64','float64']).columns.tolist()\ndf[int_cols].describe().T","55f938a9":"def handling_missing(df):\n    cols_none=['PoolQC','MiscFeature','Alley','Fence','FireplaceQu','GarageFinish','GarageQual','GarageCond',\n               'GarageType','BsmtExposure','BsmtCond','BsmtQual','BsmtFinType2','BsmtFinType1','Exterior2nd',\n               'Exterior1st']\n    for i in cols_none:\n        df[i] = df[i].fillna('None')\n    \n    cols_zero=['GarageYrBlt','BsmtHalfBath','BsmtFullBath','MasVnrArea','TotalBsmtSF','BsmtFinSF2','BsmtFinSF1',\n               'BsmtUnfSF']\n    for i in cols_zero:\n        df[i] = df[i].fillna(0)\n    \n    cols_mode=['MasVnrType','MSZoning','Utilities','SaleType','GarageArea','GarageCars','KitchenQual','Electrical']\n    for i in cols_mode:\n        df[i] = df[i].fillna(df[i].mode()[0])\n    \n    df[\"Functional\"] = df[\"Functional\"].fillna(\"Typ\") #tells you to do this in the data description\n    \n    df['LotFrontage'] = df['LotFrontage'].fillna(df['LotFrontage'].mean()) \n    \n    df=df.drop(['Id'],axis=1) # Let's drop the Id column while we are at it\n    \n    return df","0bb88b69":"df=handling_missing(df)","0e903eaf":"qual_dict = {'None': 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n\ndf[\"ExterQual\"] = df[\"ExterQual\"].map(qual_dict).astype(int)\ndf[\"ExterCond\"] = df[\"ExterCond\"].map(qual_dict).astype(int)\ndf[\"BsmtQual\"] = df[\"BsmtQual\"].map(qual_dict).astype(int)\ndf[\"BsmtCond\"] = df[\"BsmtCond\"].map(qual_dict).astype(int)\ndf[\"HeatingQC\"] = df[\"HeatingQC\"].map(qual_dict).astype(int)\ndf[\"KitchenQual\"] = df[\"KitchenQual\"].map(qual_dict).astype(int)\ndf[\"FireplaceQu\"] = df[\"FireplaceQu\"].map(qual_dict).astype(int)\ndf[\"GarageQual\"] = df[\"GarageQual\"].map(qual_dict).astype(int)\ndf[\"GarageCond\"] = df[\"GarageCond\"].map(qual_dict).astype(int)","828c3c64":"# Divide up the years between 1871 and 2010 in slices of 20 years.\nyear_map = pd.concat(pd.Series(\"YearBin\" + str(i+1), index=range(1871+i*20,1891+i*20)) for i in range(0, 7))\n\ndf['YearBuilt']=df['YearBuilt'].map(year_map)\ndf['YearRemodAdd']=df['YearRemodAdd'].map(year_map)\ndf['GarageYrBlt']=df['GarageYrBlt'].map(year_map)\ndf['YrSold']=df['YrSold'].map(year_map)","a6b51a63":"cols_numcat=['MSSubClass','MoSold']\n\nfor i in cols_numcat:\n    df[i]=df[i].astype('object')","95499d6c":"from scipy.stats import skew\n\nnumeric_features = df.dtypes[df.dtypes != \"object\"].index\n\nnumeric_df=df[numeric_features]\n  \nskewed = df[numeric_features].apply(lambda x: skew(x.dropna().astype(float)))\nskewed = skewed[skewed > 0.75]\nskewed = skewed.index\n\ndf[skewed]=np.log1p(df[skewed])","1c9427bc":"from sklearn.preprocessing import StandardScaler\n\nstd=StandardScaler()\nscaled=std.fit_transform(df[numeric_features])\nscaled=pd.DataFrame(scaled,columns=numeric_features)\n\ndf_original=df.copy()\ndf=df.drop(numeric_features,axis=1)\n\ndf=df.merge(scaled,left_index=True,right_index=True,how='left')","7e451ede":"df=pd.get_dummies(df)","e70f95cb":"train = df[:ntrain]\ntest = df[ntrain:]","fe3b2916":"#this is the metric we use to validate the model\nfrom sklearn.metrics import mean_squared_error\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))","71baa60a":"from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nimport xgboost as xgb\n\nalphas_ridge = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas_lasso = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\n# Kernel Ridge Regression : made robust to outliers\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_ridge, cv=kfolds))\n\n# LASSO Regression : made robust to outliers\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, \n                    alphas=alphas_lasso,random_state=42, cv=kfolds))\n\n# XGBOOST Regressor : The parameters of this model I took from another notebook\nregr = xgb.XGBRegressor(\n                 colsample_bytree=0.2,\n                 gamma=0.0,\n                 learning_rate=0.01,\n                 max_depth=4,\n                 min_child_weight=1.5,\n                 n_estimators=7200,                                                                  \n                 reg_alpha=0.9,\n                 reg_lambda=0.6,\n                 subsample=0.2,\n                 seed=42,\n                 silent=1)\n\n\nlasso_model = lasso.fit(train,y_train)\nridge_model = ridge.fit(train,y_train)\nxgb_model = regr.fit(train,y_train)\n\n# model blending function using fitted models to make predictions\ndef blend_models(X):\n    return ((xgb_model.predict(X)) + (lasso_model.predict(X)) + (ridge_model.predict(X)))\/3\n\ny_pred=blend_models(train)\n\nprint(\"blend score on training set: \", rmse(y_train, y_pred))","7af6f6eb":"y_pred_blend = blend_models(test)\ny_pred_exp_blend = np.exp(y_pred_blend)\n\npred_df_blend = pd.DataFrame(y_pred_exp_blend, index=df_test[\"Id\"], columns=[\"SalePrice\"])\npred_df_blend.to_csv('output.csv', header=True, index_label='Id')","da3a4faa":"Any comments or suggestions are very welcome.","c828317f":"In this notebook I put together different concepts I found in the following notebooks. Kudos to these guys for the great work:\n* [Stacked Regressions : Top 4% on LeaderBoard](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard)\n* [Simple approach to predict house price](https:\/\/www.kaggle.com\/shubhammahajan3110\/simple-approach-to-predict-house-price)\n* [XGBoost + Lasso](https:\/\/www.kaggle.com\/humananalog\/xgboost-lasso)\n","b813e9cb":"Now that we have tranformed the target variable, we can put together train a test data","62c131e8":"## GUIDE FOR REGRESSION PROBLEMS","0f4f7238":"We encode the categorical variables ","34bf6fdc":"Quality measurements are stored as text but we can convert them to numbers where a higher number means higher quality.","a7569a02":"In order to address this problem we can apply a [Box-Cox](https:\/\/blog.minitab.com\/blog\/applying-statistics-in-quality-projects\/how-could-you-benefit-from-a-box-cox-transformation) transformation. In this case I will use a Box-Cox with lambda= 0 wich is basically a logarithmic transformation:","bd6fcd4b":"# STEP 3 : MODELLING","720cdac1":"We normalize the numeric variables","4065bf23":"We apply a logarithmic transformation to numeric features with high skewness","e2061b37":"Importing the data:","459f29f2":"The problem we will be solving is this:  [House Prices: Advanced Regression Techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/overview)","87c27057":"Ok, we begin by importing a set of packages we'll be needing:","15a1517e":"It doesnt perform as well on the test data so there's some overfitting to fix. It should get you around 0.12","94758d26":"It looks like we have a lot of columns with missing data. Let's rank the variables with the hisghest % of missing data","dcd4ac38":"We find a couple of outliers when we plot SalePrice against the top 2 most correlated feature, 'GrLivArea'. Let's plot them and drop those rows","1c632c47":"# STEP 0 : EXPLORE TARGET VARIABLE","7450f985":"Let's take a look at the values of all the **categorical** variables","1d626468":"Let's see how variables correlate with each other","1b00d66c":"Let's take a look at the 10 variables that are more correlated to the target","d9b918a9":"In this notebook I will solve a regression problem. **The goal for this notebook is to give a idea of the steps we need to follow to solve a regression problem** and to show some interesting techniques to do it.","05f67f9f":"First of all, let's analyze the target column 'SalePrice'","b718efc3":"Looking at the data description of the data we see that most of the missing values are not really missin values, but actual null categories or zeros (for categorical and numerical features). **For example, PoolQC, is the Pool Quality, so having a missing value here means that the house doesn't have a pool, so we can fill the missing values with 'None'**. \n* **Categorical (not really missing data)**: fill missing values with **'None'**\n* **Categorical (actually missing data)**: fill missing values with **mode**\n* **Numerical**: fill missing values with **0**\n\nThere are a couple of exceptions.","b5c023e1":"Let's take a look at the values of all the **numerical** variables","cb9e0f04":"# STEP 1 : EXPLORATORY DATA ANALYSIS (EDA)","4560de61":"We split the Year features into 7 groups of 20 years","2ecf98cb":"We can see that the distribution is skewed to the right (positive skew). Let's take a look at the [skewness and kurtosis](https:\/\/codeburst.io\/2-important-statistics-terms-you-need-to-know-in-data-science-skewness-and-kurtosis-388fef94eeaa):","d105c9c7":"After trying out varios options, this one seems to get the best result. It's a **blend of 3 different models**","14dba605":"These two variables are categorical instead of numeric","0129b38b":"# STEP 2 : DATA PREPROCESSING","0f8dce2c":"# STEP 4 : PREPARING THE SUBMISSION FILE","501e5695":"Overview of the dataset "}}