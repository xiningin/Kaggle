{"cell_type":{"52c01a7a":"code","a0f1d58b":"code","9966a4c8":"code","bb3dbd88":"code","2268dce5":"code","f1c50462":"code","c101fdc7":"code","b66e83c4":"code","e3149cf6":"code","1327cd0c":"code","556f50a7":"code","a6f09c6c":"code","9fe6c2a9":"code","82761381":"code","42812b42":"code","acef6f7e":"code","2ba61a1c":"code","379952af":"code","407288b4":"code","b4547597":"code","7a244a5e":"markdown","198ffb30":"markdown","4a6ae7c6":"markdown","0dfe745b":"markdown","125803a5":"markdown"},"source":{"52c01a7a":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import f1_score\nfrom collections import Counter\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.decomposition import PCA\nfrom tqdm.notebook import tqdm\nimport gensim\nimport os\nos.listdir(\"..\/input\/ykc-cup-2nd\/\")","a0f1d58b":"train = pd.read_csv(\"..\/input\/ykc-cup-2nd\/train.csv\")\ntest = pd.read_csv(\"..\/input\/ykc-cup-2nd\/test.csv\")\nsub = pd.read_csv(\"..\/input\/ykc-cup-2nd\/sample_submission.csv\")\ntrain.shape, test.shape, sub.shape","9966a4c8":"train.head()","bb3dbd88":"test.head()","2268dce5":"sub.head()","f1c50462":"## train\u3068test\u3092\u304f\u3063\u3064\u3051\u3066\u4e00\u62ec\u3067\u7279\u5fb4\u91cf\u4f5c\u6210\u3092\u3059\u308b\ndf = pd.concat([train, test])\ndf = df.reset_index(drop=True)\ndf.shape","c101fdc7":"train[train[\"department_id\"] == 3].head()\n## \u91ce\u83dc\u3068\u304b\u679c\u7269\uff1f","b66e83c4":"train[train[\"department_id\"] == 12].head()\n## \u8abf\u5473\u6599\uff1f","e3149cf6":"train[train[\"department_id\"] == 16].head()\n##\u6d17\u6fef\u7528\u5177\u3068\u304b","1327cd0c":"df[\"product_name\"] = df[\"product_name\"].apply(lambda words : words.lower().replace(\",\", \"\").replace(\"&\", \"\").split(\" \"))\ndf.head()","556f50a7":"## \u8a13\u7df4\u6e08\u307f\u306e\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u3092\u8aad\u307f\u8fbc\u3093\u3067\uff0cproduct_name\u306b\u542b\u307e\u308c\u308b\u5358\u8a9e\u3092\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3057\u3066\u5e73\u5747\u3092\u53d6\u308b\u3053\u3068\u3067\uff0c\u5404product_id\u306b\u5bfe\u3057\u3066\u7279\u5fb4\u91cf\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210\u3059\u308b\n\n## gensim\u3067.vec\u304b\u3089\u8aad\u307f\u8fbc\u3080\u3068\u304d\u306b\u6642\u9593\u304c\u304b\u304b\u308b\u306e\u3067\uff0c\u4ed6\u306enotebook\u3067pickle\u3067\u4fdd\u5b58\u3057\u305f\u3082\u306e\u3092\u4f7f\u7528\u3057\u3066\u3044\u308b\nmodel = pd.read_pickle(\"..\/input\/ykc-cup-2nd-save-fasttext\/fasttext_gensim_model.pkl\") \n\n## gensim\u3067vec\u304b\u3089\u8aad\u307f\u8fbc\u3080\u5834\u5408\uff08\uff15\u5206\u3050\u3089\u3044\u304b\u304b\u308b\uff09\n# model = gensim.models.KeyedVectors.load_word2vec_format('..\/input\/ykc-2nd\/wiki-news-300d-1M.vec\/wiki-news-300d-1M.vec')\n\nfrom collections import defaultdict\nunused_words = defaultdict(int)\ndef to_vec(x, model):\n    v = np.zeros(model.vector_size)\n    for w in x:\n        try:\n            v += model[w] ## \u5358\u8a9e\u304c\u8a13\u7df4\u6e08\u307f\u30e2\u30c7\u30eb\u306evocab\u306b\u3042\u3063\u305f\u3089\n        except:\n            unused_words[w] += 1 ## \u30d9\u30af\u30c8\u30eb\u304c\u5b58\u5728\u3057\u306a\u304b\u3063\u305f\u5358\u8a9e\u3092\u30e1\u30e2\n    v = v \/ (np.sqrt(np.sum(v ** 2)) + 1e-16) ## \u9577\u3055\u30921\u306b\u6b63\u898f\u5316\n    return v    \nvecs = df[\"product_name\"].apply(lambda x : to_vec(x, model))\nvecs = np.vstack(vecs)\nfasttext_pretrain_cols = [f\"fasttext_pretrain_vec{k}\" for k in range(vecs.shape[1])]\nvec_df = pd.DataFrame(vecs, columns=fasttext_pretrain_cols)\ndf = pd.concat([df, vec_df], axis = 1)\ndf.head()","a6f09c6c":"sorted(unused_words.items(), key=lambda x: x[1], reverse = True)[:100]","9fe6c2a9":"## fasttext\u304b\u3089\u5f97\u3089\u308c\u305f\u30d9\u30af\u30c8\u30eb\u30922\u6b21\u5143\u306b\u843d\u3068\u3057\u3066\u6563\u5e03\u56f3\u3092\u66f8\u3044\u3066\u307f\u308b\npc = PCA(2).fit_transform(df[fasttext_pretrain_cols])\nfor department_id in range(21):\n    idx = np.where(df[\"department_id\"] == department_id)[0]\n    plt.scatter(pc[idx,0], pc[idx,1], label = department_id, s = 0.5, alpha = 0.5)\nplt.legend()","82761381":"features = fasttext_pretrain_cols + [\"order_rate\", \"order_dow_mode\", \"order_hour_of_day_mode\"] ## \u4e88\u6e2c\u306b\u4f7f\u7528\u3059\u308b\u7279\u5fb4\u91cf\u306e\u540d\u524d\ntarget = \"department_id\" ## \u4e88\u6e2c\u5bfe\u8c61\nn_split = 5 ## cross validation\u306efold\u6570","42812b42":"## train\u3068test\u3092\u5206\u96e2\ntrain = df[~df[target].isna()]\ntest = df[df[target].isna()]","acef6f7e":"## cross validation\npreds_test = []\nscores = []\nkfold = KFold(n_splits=n_split, shuffle = True, random_state=42)\nfor i_fold, (train_idx, valid_idx) in enumerate(kfold.split(train)):\n    print(f\"--------fold {i_fold}-------\")\n    \n    ## train data\n    x_tr = train.loc[train_idx, features]\n    y_tr = train.loc[train_idx, target]\n\n    ## valid data\n    x_va = train.loc[valid_idx, features]\n    y_va = train.loc[valid_idx, target]\n\n    ## train LGBM model\n    model = LGBMClassifier(colsample_bytree=0.2)\n    model.fit(x_tr, y_tr, )\n    \n    ## predict on valid\n    pred_val = model.predict_proba(x_va)\n    \n    ## evaluate\n    score = {\n        \"logloss\"  : log_loss(y_va, pred_val),\n        \"f1_micro\" : f1_score(y_va, np.argmax(pred_val, axis = 1), average = \"micro\")}\n    print(score)\n    scores.append(score)\n    \n    ## predict on test\n    pred_test = model.predict_proba(test[features])\n    preds_test.append(pred_test)","2ba61a1c":"score_df = pd.DataFrame(scores)\nscore_df","379952af":"score_df.mean()","407288b4":"## cv\u306e\u5404fold\u3067\u8a08\u7b97\u3057\u305f\u4e88\u6e2c\u5024\u306e\u5e73\u5747\u3092\u6700\u7d42\u7684\u306a\u4e88\u6e2c\u5024\u306b\npred_test_final = np.array(preds_test).mean(axis = 0)\npred_test_final = np.argmax(pred_test_final, axis = 1)","b4547597":"sub[\"department_id\"] = pred_test_final\nsub.to_csv(\"submission.csv\", index = False)\nsub.head()","7a244a5e":"## train","198ffb30":"## feature engineering","4a6ae7c6":"## EDA","0dfe745b":"## read data ","125803a5":"## submission"}}