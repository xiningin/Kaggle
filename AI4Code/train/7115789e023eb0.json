{"cell_type":{"72312974":"code","d92cb176":"code","239725cf":"code","cbda5044":"code","1b71fb28":"code","4b1c4a28":"code","ced88478":"code","05d28f9c":"code","5f81f81b":"code","aa072d4e":"code","e42f175d":"code","33b9290b":"code","4b7ae03c":"markdown","e26a4553":"markdown","049b8414":"markdown","90155b76":"markdown","82b58fd8":"markdown","a765b3ca":"markdown","d984d77a":"markdown","f8edef02":"markdown","6900a9d1":"markdown","9e1ab61a":"markdown","5abc549e":"markdown","8b405807":"markdown","880d1a0c":"markdown"},"source":{"72312974":"import sklearn \nimport pandas as pd\nimport numpy as np","d92cb176":"from sklearn.feature_selection import VarianceThreshold","239725cf":"X = [[0,1,2,3],[0,4,5,3],[0,4,6,3]]","cbda5044":"X_df = pd.DataFrame(X)","1b71fb28":"X_df","4b1c4a28":"feature_selector = VarianceThreshold()\nnew_features = feature_selector.fit_transform(X_df)","ced88478":"new_features","05d28f9c":"feature_selector.variances_","5f81f81b":"feature_selector01 = VarianceThreshold(threshold = 2.5)\nnew_features01 = feature_selector01.fit_transform(X_df)\nnew_features01","aa072d4e":"# Create feature matrix with two highly correlated features\nX = np.array([[1, 1, 1],\n              [2, 2, 0],\n              [3, 3, 1],\n              [4, 4, 0],\n              [5, 5, 1],\n              [6, 6, 0],\n              [7, 7, 1],\n              [8, 7, 0],\n              [9, 7, 1]])\n\n# Convert feature matrix into DataFrame\ndf = pd.DataFrame(X)\n\n# View the data frame\ndf","e42f175d":"# Create correlation matrix\ncorr_matrix = df.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]","33b9290b":"# Drop features \ndf.drop(df[to_drop], axis=1)","4b7ae03c":"### Variance Thresholds","e26a4553":"Variance thresholds remove features whose values don't change much from sample to sample (i.e. their variance falls below a threshold). These features provide little value or no information.\n\nFor example, if you had a public health dataset where 96% of observations were for 50-year-old men, then the 'Age' and 'Gender' features can be eliminated without a major loss in information.\n\nBecause variance is dependent on scale, you should always **normalize** your features first.But in sklearn implementation of Variance Threshold it is taken care of the algorithm and no need to normalize the data when using it as illustrated below example.\n\n**Strengths:** Applying variance thresholds is based on solid intuition: features that don't change much also don't add much information. This is an easy and relatively safe way to reduce dimensionality at the start of your modeling process.\n\n\n**Weaknesses:** If your problem does require dimensionality reduction, applying variance thresholds is rarely sufficient. Furthermore, you must manually set or tune a variance threshold hyper parameter, which could be tricky. I recommend starting with a conservative (i.e. lower) threshold.\n\nsee the simple example below and\n\n\nFor more info on see [sklearn Implementation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.VarianceThreshold.html).","049b8414":" Reminder for me: explain the 'curse of dimensionality' with the concept of coin search on one dimension line, rectangle field,3D housing complex and a row of 3D housing complexes. ","90155b76":"## Feature Selection","82b58fd8":"## The Curse of Dimensionality\n\nIn machine learning, **dimensionality** simply refers to the number of features (i.e. input variables) in your dataset.\n\nWhen the number of features is very large relative to the number of observations in your dataset, certain algorithms struggle to train effective models. This is called the **Curse of Dimensionality,** and it\u2019s especially relevant for **clustering** algorithms that rely on distance calculations between datapoints.","a765b3ca":"# Dimensionality Reduction","d984d77a":"![alt text](https:\/\/quantlabs.net\/blog\/wp-content\/uploads\/2015\/10\/genetic-algo-wikievolution.jpg \"Genetic Algorithm\")","f8edef02":"Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones.\n\nTo be clear, some supervised algorithms already have built-in feature selection, such as Regularized Regression(specfically L1 regularization) and Random Forests. Typically, I recommend starting with these algorithms if they fit your task. \n\nAs a stand-alone task, feature selection can be unsupervised (e.g. Variance Thresholds) or supervised (e.g. Genetic Algorithms). You can also combine multiple methods if needed.\n\n- Variance Threshold (unsupervised)\n- Correlation Threshold (unsupervised)\n- Genetic Algorithm (supervised)\n- Stepwise Search (supervised)\n","6900a9d1":"In machine learning, GA's have two main uses. The first is for optimization, such as finding the best weights for a neural network.\n\nThe second is for supervised feature selection. In this use case, \"genes\" represent individual features and the \"organism\" represents a candidate set of features. Each organism in the \"population\" is graded on a fitness score such as model performance on a hold-out set. The fittest organisms survive and reproduce, repeating until the population converges on a solution some generations later.\n\n**Strengths:** Genetic algorithms can efficiently select features from very high dimensional datasets, where exhaustive search is unfeasible. When you need to preprocess data for an algorithm that doesn't have built-in feature selection (e.g. nearest neighbors) and when you must preserve the original features (i.e. no PCA allowed), GA's are likely your best bet. These situations can arise in business\/client settings that require a transparent and interpretable solution.\n\n\n**Weaknesses:** GA's add a higher level of complexity to your implementation, and they aren't worth the hassle in most cases. If possible, it's faster and simpler to use PCA or to directly use an algorithm with built-in feature selection.\n\n\nImplementations: [Python]( https:\/\/pypi.org\/project\/deap\/)","9e1ab61a":"Dimensionality Reduction, is broken into Feature Selection and Feature Extraction. In general, these tasks are rarely performed in isolation. Instead, they\u2019re often preprocessing steps to support other tasks.\n\n- Feature Selection\n- Feature Extraction","5abc549e":"### Genetic Algorithms (GA)\n\nGenetic algorithms (GA) are a broad class of algorithms that can be adapted to different purposes. They are search algorithms that are inspired by evolutionary biology and natural selection, combining mutation and cross-over to efficiently traverse large solution spaces. Here's a [great intro to the intuition behind GA's.](https:\/\/www.obitko.com\/tutorials\/genetic-algorithms\/ga-basic-description.php)","8b405807":"###  Stepwise Search\n\nStepwise search is a supervised feature selection method based on sequential search, and it has two flavors: forward and backward. For forward stepwise search, you start without any features. Then, you'd train a 1-feature model using each of your candidate features and keep the version with the best performance. You'd continue adding features, one at a time, until your performance improvements stall.\n\nBackward stepwise search is the same process, just reversed: start with all features in your model and then remove one at a time until performance starts to drop substantially.\n\nI note this algorithm purely for historical reasons. Despite many textbooks listing stepwise search as a valid option, it almost always under performs other supervised methods such as regularization. Stepwise search has [many documented flaws](https:\/\/www.quora.com\/Regression-statistics-Why-is-stepwise-so-criticised-What-are-the-flaws-in-this-method), one of the most fatal being that it's a greedy algorithm that can't account for future effects of each change. I don't recommend this method.","880d1a0c":"### Correlation Thresholds\n\nCorrelation thresholds remove features that are highly correlated with others (i.e. its values change very similarly to another's). These features provide redundant information.\n\nFor example, if you had a real-estate dataset with 'Plot Area (Sq. Ft.)' and 'Plot Area (Sq. Meters)' as separate features, you can safely remove one of them.\n\nWhich one should you remove? Well, you'd first calculate all pair-wise correlations(correlation matrix). Then, if the correlation between a pair of features is above a given threshold, you'd remove the one that has larger mean absolute correlation with other features.\n\n**Strengths:** Applying correlation thresholds is also based on solid intuition: similar features provide redundant information. Some algorithms are not robust to correlated features, so removing them can boost performance.\n\n\n**Weaknesses:** Again, you must manually set or tune a correlation threshold hyper parameter, which can be tricky to do. Plus, if you set your threshold too low, you risk dropping useful information. Whenever possible, we prefer algorithms with built-in feature selection over correlation thresholds. Even for algorithms without built-in feature selection, Principal Component Analysis (PCA)(to be dicussed later) is often a better alternative.\n\nsee the example  below and dicussion [here](https:\/\/stackoverflow.com\/questions\/29294983\/how-to-calculate-correlation-between-all-columns-and-remove-highly-correlated-on) for more info\n"}}