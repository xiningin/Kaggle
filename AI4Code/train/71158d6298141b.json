{"cell_type":{"cf3870b6":"code","a19ad2b7":"code","a4c32af8":"code","1be897a4":"code","e29db815":"code","e65f26f9":"code","952584bb":"code","ff731463":"code","2cd67157":"code","7ca09760":"code","af689c9b":"code","3e44820f":"code","5b7886a6":"code","7a0f32a7":"code","02442417":"code","330e2eea":"code","8eeb2782":"code","9ee68f6b":"markdown","0722a22d":"markdown","e8c2783d":"markdown","a5fd152b":"markdown","b9329f02":"markdown","357a098c":"markdown","3bdd49f1":"markdown","4f27d41c":"markdown","2d7efda8":"markdown","b281b95b":"markdown","9134bd5f":"markdown","fe841117":"markdown","71b9fa16":"markdown","eafb27cd":"markdown","adae7ece":"markdown","5fc1b564":"markdown"},"source":{"cf3870b6":"#pip install pyspellchecker","a19ad2b7":"pip install openpyxl","a4c32af8":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt #plotting\n\n#nlp toolkit\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n# from spellchecker import SpellChecker\nfrom string import punctuation\n\n#dataset splitting\nfrom sklearn.model_selection import train_test_split\n\n#nn using keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras import layers","1be897a4":"def load_data():\n    \"\"\"\n    loads original dataset and returns dataframe of relevant columns\n    \"\"\"\n    path = \"..\/input\/yelp-review-with-sentiments-and-features\/\"\n    filename = \"Yelp Labelled Review Dataset with Sentiments and Features.xlsx\"\n    df = pd.read_excel(path+filename, usecols=['Rating','Review','Spam(1) and Not Spam(0)','Sentiment'])\n        \n    df.drop(df.loc[(df.Sentiment == 'Neutral') | \\\n                   ((df.Sentiment == 'Positive') & (df.Rating < 4)) | \\\n                   ((df.Sentiment == 'Negative') & (df.Rating > 2)) | \\\n                   (df['Spam(1) and Not Spam(0)'] == 1)].index, inplace=True)\n    return df","e29db815":"def create_even_dataset(df, col='Sentiment'):\n    \"\"\"\n    Returns even dataset, according to specified column, defaul = sentiment\n    \"\"\"\n    col_count = df[col].value_counts()\n    lower_col_size = min(col_count)\n    lower_col = col_count.index[col_count==lower_col_size][0]\n    df_reduced_col = df.loc[df[col] != lower_col].sample(n=lower_col_size)\n    dataset = pd.concat([df.loc[df[col] == lower_col], df_reduced_col], axis = 0)  \n    \n    return dataset","e65f26f9":"def remove_stopwords(text):\n    \"\"\"\n    Removes all the stop words like \"is,the,a, etc.\"\n    \"\"\"\n    stop_words = stopwords.words('english') \n    output = []\n    for sentence in text:\n        output.append(' '.join([w for w in nltk.word_tokenize(sentence) if not w in stop_words]))\n    return output\n\ndef correct_spelling(text):\n    spell = SpellChecker()\n    output = []\n    for sentence in text:\n        output.append(' '.join([spell.correction(t) if len(spell.unknown([t]))>0 else t for t in nltk.word_tokenize(sentence)]))\n    return output\n\ndef lemmatize(text):\n    \"\"\"\n    Applies lemmatization\n    \"\"\"\n    wordnet_lemmatizer = WordNetLemmatizer()\n    lemmatized_text = []\n    \n    for sentence in text:\n        lemmatized_word = [wordnet_lemmatizer.lemmatize(w) for w in nltk.word_tokenize(sentence)]\n        lemmatized_text.append(\" \".join(lemmatized_word))\n        \n    return lemmatized_text\n\ndef remove_numbers(text):\n    \"\"\"\n    Removes all numbers\n    \"\"\" \n    output = []\n    for sentence in text:\n        output.append(''.join(c for c in sentence if not c.isdigit()))\n    return output\n    \ndef remove_punct(text):\n    \"\"\"\n    Removes all punctuation characters: !\"#$%&'()*+,-.\/:;<=>?@[\\]^_`{|}~\n    \"\"\"\n    output = []\n    for sentence in text:\n        output.append(''.join(c for c in sentence if c not in punctuation))\n    return output\n\ndef remove_short_words(text):\n    \"\"\"\n    Removes words of length <= 1\n    \"\"\"\n    output = []\n    for sentence in text:\n        output.append(' '.join([w for w in nltk.word_tokenize(sentence) if len(w)<2]))\n    return output","952584bb":"def preprocess(dataset):\n    \"\"\"\n    Apply standard text preprocessing on reviews:\n        1. lower case\n        2. Sentence tokenizing\n        3. Remove stopwords\n        4. Spell correction\n        5. Lemmatization\n        6. Remove numbers\n        7. Remove punctuation and special characters\n        8. Remove all words of length <= 1 character\n        \n    Convert text to sequences.\n    \"\"\"\n    dataset = dataset.apply(lambda x: x.lower())\n    dataset = dataset.apply(lambda x: nltk.sent_tokenize(x))\n    dataset = dataset.apply(lambda x: remove_stopwords(x))\n    #dataset = dataset.apply(lambda x: (correct_spelling(x)))\n    dataset = dataset.apply(lambda x: lemmatize(x))\n    dataset = dataset.apply(lambda x: remove_numbers(x))\n    dataset = dataset.apply(lambda x: remove_punct(x))\n    dataset = dataset.apply(lambda x: remove_short_words(x))\n    \n    max_words = 5000\n\n    tokenizer = Tokenizer(num_words=max_words)\n    tokenizer.fit_on_texts(dataset)\n    sequences = tokenizer.texts_to_sequences(dataset)\n    dataset = pad_sequences(sequences)\n    \n    return dataset","ff731463":"def build_RNN(max_words=2000,max_len=200):\n    \n    model = Sequential()\n    model.add(layers.Embedding(max_words, 64, input_length=max_len))\n    model.add(layers.SimpleRNN(32,dropout=0.6))\n    model.add(layers.Dense(2,activation='softmax'))\n    model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n\n    return model","2cd67157":"def build_BID_LSTM(max_words=2000,max_len=200):\n    \n    model = Sequential()\n    model.add(layers.Embedding(max_words, 64, input_length=max_len))\n    model.add(layers.Bidirectional(layers.LSTM(32,dropout=0.6)))\n    model.add(layers.Dense(2,activation='softmax'))\n    model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","7ca09760":"def plot_result_comparison_2_models(history):\n    \"\"\"\n    Plot training and validation loss and accuracy for each model.\n    \"\"\"\n    fig = plt.figure(figsize = (20, 6))\n    plt.subplot(1, 2, 1)\n    plt.plot(history[0].history['accuracy'], label='RNN_Train Acc') #training list\n    plt.plot(history[0].history['val_accuracy'], label='RNN_Validation Acc') #validation list\n    plt.plot(history[1].history['accuracy'], label='BID-LSTM_Train Acc') #training list\n    plt.plot(history[1].history['val_accuracy'], label='BID-LSTM_Validation Acc') #validation list\n    plt.title(\"Accuracy\")\n    plt.legend()\n    plt.grid()\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(history[0].history['loss'], label='RNN_Train loss') #training loss\n    plt.plot(history[0].history['val_loss'], label='RNN_Validation loss') #validation loss\n    plt.plot(history[1].history['loss'], label='BID-LSTM_Train loss') #training loss\n    plt.plot(history[1].history['val_loss'], label='BID-LSTM_Validation loss') #validation loss\n    plt.title(\"Loss\")\n    plt.legend()\n    plt.grid()\n    plt.show()","af689c9b":"df = load_data()\ndataset = create_even_dataset(df)","3e44820f":"y = pd.get_dummies(dataset['Sentiment']).values\n    \nx_train, x_test, y_train, y_test = train_test_split(\n    dataset.Review,\n    y,\n    test_size=0.1,\n    stratify = y)\n    \nx_train = preprocess(x_train)\nx_test = preprocess(x_test)","5b7886a6":"BID_model = build_BID_LSTM()\nRNN_model = build_RNN()","7a0f32a7":"history_BID = BID_model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))","02442417":"history_RNN = RNN_model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))","330e2eea":"history = [history_BID,history_RNN]\nplot_result_comparison_2_models(history)","8eeb2782":"x = pd.Series(['Our room was excellent! The view was superb, we had a fantastic time.'])\nsentiment = ['Negative','Positive']\n\ntest = preprocess(x)\nresult_BID = sentiment[np.around(BID_model.predict(test), decimals=0).argmax(axis=1)[0]]\nresult_RNN = sentiment[np.around(RNN_model.predict(test), decimals=0).argmax(axis=1)[0]]\n\nprint(f'Test review: {x[0]}')\nprint(f'Sentiment analyzed by bidirectional LSTM: {result_BID}')\nprint(f'Sentiment analyzed by RNN: {result_RNN}')","9ee68f6b":"It seems that under the above circumstances (same set of hyperparameters and inputs), both models are very accurate, but the RNN is slightly faster in running time.","0722a22d":"Plot loss and accuracy for both models.","e8c2783d":"Test models on new input.","a5fd152b":"**Main**","b9329f02":"**Imports**","357a098c":"Create binary label vector (y).\nSplit dataset into train (90%) and test set (10%), preserving class ratios.\nPreprocess train and test inputs, seperately.","3bdd49f1":"**Comparing a bidirectional LSTM to RNN for the task of sentiment analysis, both models built with Keras.**","4f27d41c":"Plot function.","2d7efda8":"Train models, validate with test set.","b281b95b":"Preprocessing functions","9134bd5f":"Build a both models using keras.","fe841117":"Create networks with keras.","71b9fa16":"**Installations**","eafb27cd":"First load data set, then convert to an even set of input samples of each type (the same amount of positive and negative reviews).","adae7ece":"Data loading functions","5fc1b564":"**Functions**"}}