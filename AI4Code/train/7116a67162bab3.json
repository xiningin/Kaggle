{"cell_type":{"310cd92e":"code","e69c07fc":"code","1ce746d3":"code","e70daf3b":"code","69c9423a":"code","96f5ce36":"code","059e882b":"code","dbde4dc5":"code","e01327cd":"code","660895a0":"code","3811c057":"code","6c89a5c2":"code","aeab1b52":"code","396ac990":"code","7ac4e57d":"code","2f149585":"code","c064464f":"code","aea0366c":"code","5f07f721":"code","cab35350":"code","460199ac":"code","c2214440":"code","21d8b10d":"code","65f6c983":"code","6d52ba70":"code","787d1ad7":"code","ee21d31d":"code","68114f74":"code","63e2067c":"code","7982e975":"code","294dc070":"code","d5061424":"code","f87136ee":"code","a7cfa56e":"code","7fe97ba9":"code","55461cef":"code","96f16498":"code","d2bb2eff":"code","3ffb8470":"code","40ba5a72":"code","184c3a11":"code","51b257f1":"code","1aaaab4c":"code","360928e8":"code","4d963362":"code","c1bc8603":"code","e11fa44b":"code","61860867":"code","7338d6e4":"code","4f29ec78":"code","d3ffff72":"code","307116f2":"code","d13cecb5":"code","c33ecad3":"code","e039655e":"code","8592db05":"code","6eae74ce":"code","298a1198":"code","2b409666":"code","6072e9ef":"code","0842df03":"code","14945120":"code","1cbbdfef":"code","e5a022c5":"code","379cdc2d":"code","ebcfe6a0":"code","8a5ddb9a":"code","c6ec701f":"code","88a7d93b":"code","7a624831":"code","bc13e9a2":"code","e14405a5":"code","b6160ae4":"code","305eb932":"code","3fbcf1c0":"code","9ba0550b":"code","da5950df":"code","3c03c502":"code","181678be":"code","1dc6dbe4":"code","fb50c34f":"code","fd4a7f20":"code","1b4a2614":"code","6c4387b3":"code","7d84efa2":"code","0ed01049":"code","d722e91c":"code","b2e61014":"code","c4386cdf":"code","3e4b7a84":"code","45af2001":"code","fd04c8f6":"code","8d33947a":"code","62295658":"code","f49ad379":"code","9cd4ffe0":"code","d7f9b21e":"code","b2f41066":"code","244d3523":"markdown","1ef62424":"markdown","b1a4c2ef":"markdown","147480c2":"markdown","d9cc3caa":"markdown","c0877ee7":"markdown","64f7002b":"markdown","341c197d":"markdown","f65538dd":"markdown"},"source":{"310cd92e":"\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nri = pd.read_csv(\"..\/input\/police.csv\")\nted = pd.read_csv(\"..\/input\/ted.csv\")\n","e69c07fc":"ted.head()","1ce746d3":"ted.shape","e70daf3b":"ted.dtypes","69c9423a":"#6 speakers with nan occupation.\nted.isna().sum()","96f5ce36":"ted.head()","059e882b":"#you should sort the data with respect to online comments-Only the col itself.\nted.comments.sort_values(ascending=False)","dbde4dc5":"#this could work out also and also its much more better approach.Also its good idea to check event date.Bcs theyre relevant\nted.sort_values(\"comments\",ascending=False)","e01327cd":"#but how can we use event date? We can use views.its much more clear approach.\nted[\"comments_per_view\"]=ted.comments\/ted.views","660895a0":"ted.sort_values(\"comments_per_view\",ascending=False)","3811c057":"#another approach-same mentality\nted[\"views_per_comment\"]=ted.views\/ted.comments","6c89a5c2":"ted.sort_values(\"views_per_comment\")","aeab1b52":"\nted.shape","396ac990":"#x is index,y is # of comments\nted.comments.plot()\n#kind->line,bar,barh,hist,box,kde,density,area,pie.default is line which is not very informative","7ac4e57d":"#most of the comments (nearly all of them) has btw 0 to ~600 comments.but changing binwidth could be more helpful.\nted.comments.plot(kind=\"hist\")","2f149585":"#via using seaborn we could get much more good looking visuals however this is pandas workout so...\nted[ted.comments<=1000].comments.plot(kind=\"hist\")\n#this could help out.","c064464f":"#using loc\nted.loc[ted.comments<=1000,\"comments\"].plot(kind=\"hist\")\n#loc is very good,u can select single col,multi col,list of cols,range of cols.","aea0366c":"#increasing bin size makes our plot more informative.50-100 comment range is the real deal.\nted.loc[ted.comments<=1000,\"comments\"].plot(kind=\"hist\",bins=20)\n#choose ","5f07f721":"#choose your plot type carefully\n#histogram for distribution\n#barplot for comparing categories\n#lineplot good for timeseries(change w time)\n#scatterplot good for comparing multiple variable (comparing 2 variables)\n#check pandas visualization page for more info\n#https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/visualization.html\n#pandas plots are more exploratory data analysis friendly however matplotlib is much more customizable","cab35350":"ted.head()","460199ac":"#as u can see data doesnt always have \"ted2014\" format. so ted.event.str.split(2,6) doesnt work.\nted.event.value_counts()","c2214440":"#value_counts() do the job however just for sake of using dif argument lets use sample\nted.event.sample(10)","21d8b10d":"#so event col doesnt help us lets look another field, film_date which is created by unix timestamps\nted.film_date.head()\n#believe it or not pd.to_datetime very smart tool...","65f6c983":"#its somewhat achievement but not totally.\n#https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.to_datetime.html\n#search the bottom of the manual\/documentation to see sth about unix timestamps.\npd.to_datetime(ted.film_date).head()","6d52ba70":"#this is much better.\npd.to_datetime(ted.film_date,unit=\"s\").head()","787d1ad7":"#lets store it\nted[\"film_datetime\"]=pd.to_datetime(ted.film_date,unit=\"s\")","ee21d31d":"#lets check it out.but not with head() this time.we should use other data control tools from time to time\nted.loc[:,[\"event\",\"film_datetime\"]].sample(10)","68114f74":"#OR\nted[[\"event\",\"film_datetime\"]].sample(10)","63e2067c":"#checking data types is useful.our film_datetime is datetime dtype.\nted.dtypes","7982e975":"#datetime methods\/attributes have same logic like string(str) methods(attributes) data.col.str.method()\nted.film_datetime.dt.year.head()","294dc070":"#ted.film_datetime.dt.year.value_counts().index -> 2013,2011 etc are all indexex\/indices.check their order...\nted.film_datetime.dt.year.value_counts()","d5061424":"#plotting time.lets try couple.\nted.film_datetime.dt.year.value_counts().plot(kind=\"bar\")\n#not the one we are looking for,barplots are good for catg data.and you cant consider years as categories for this case.","f87136ee":"#x=no of talks y=no of occurence\/freq. however this doesnt help us either.\nted.film_datetime.dt.year.value_counts().plot(kind=\"hist\")","a7cfa56e":"#this could help us,however there is a problem.its sort index issue.\nted.film_datetime.dt.year.value_counts().plot(kind=\"line\")","7fe97ba9":"#looks like there is a sharp decline in tedtalk talk counts.lets investigate further.\nted.film_datetime.dt.year.value_counts().sort_index().plot(kind=\"line\")","55461cef":"#latest talk datetime.so we cant be sure for present time(2019)\nted.film_datetime.max()","96f16498":"#tip:read the documentation if u have a clue about how to achieve sth but you dont get a proper result completely.\n#always remember to_datetime\/datetime when you are working w date.","d2bb2eff":"ted","3ffb8470":"#count the no of talks? 1. parameter to look up to.\nted.event.value_counts().head()","40ba5a72":"#here is a long series of explanation..starts from here\n#this data has 896 values.groupby uses max 5 values from that dset for each group.therefore dset w 2550 rows becomes 896\nted.groupby(\"event\").event.head()","184c3a11":"#and here is their views col.\nted.groupby(\"event\").views.head()","51b257f1":"#here is the whole dataframe.\nted.groupby(\"event\").head()","1aaaab4c":"#lets check out multiple aggregate functions at once.dont get confused not all ted talks has \"TED\" in their event names.\nted.groupby(\"event\").views.mean().sort_values(ascending=False)\n#why use mean->because some talks occurred on same place.","360928e8":"#ted.groupby(\"event\").views.count().sort_values(ascending=False) is unnecessary to look bcs\n#ted.event.value_counts() does the same job.\n#Now lets try to put count and mean in same table.","4d963362":"ted.groupby(\"event\").views.agg([\"count\",\"mean\"]).sort_values(by=\"mean\",ascending=False)","c1bc8603":"#lets add sum too.Now we can see the total no of views\nted.groupby(\"event\").views.agg([\"count\",\"mean\",\"sum\"]).sort_values(by=\"sum\",ascending=False)\n#there are many criterias to measure performance.1 time mass hit talk vs many talks-good amount of views...","e11fa44b":"ted.head()","61860867":"#ratings=there used to be a way on ted website to tag talks for site visiters.\nted.ratings.head()","7338d6e4":"#to get first row.\n#ted.ratings[0]\nted.loc[0,\"ratings\"]","4f29ec78":"#ratings col data is \"stringified list of dictionaries\" its not a list of dictionaries,its a string.\ntype(ted.ratings[0])\n#now how can we unpack this complex data?","d3ffff72":"import ast\n#abstract syntax tree.","307116f2":"#if i enter string that looks like a list, literal_eval returns a list.\nast.literal_eval(\"[1,2,3]\")","d13cecb5":"type(ast.literal_eval(\"[1,2,3]\"))\n#stringified integer,stringified list.. it can deal with it.","c33ecad3":"#here is our list.its a list of dictionaries.now we need to apply this to all col.\nast.literal_eval(ted.ratings[0])","e039655e":"#first solve it via f()\ndef str_to_list(ratings_str):\n    return ast.literal_eval(ratings_str)","8592db05":"#that result looks good.str_to_list(ratings) doesnt needed.bcs we are looking to the ratings col.python gets what we are\n#trying to accomplish.\nted.ratings.apply(str_to_list).head()","6eae74ce":"#however no function is necessary.\nted.ratings.apply(ast.literal_eval).head()","298a1198":"#lets do lambda f() version to see what is lambda f() does\nted.ratings.apply(lambda x: ast.literal_eval(x)).head()","2b409666":"#lets save it as actual col\nted[\"rating_list\"]=ted.ratings.apply(lambda x: ast.literal_eval(x))","6072e9ef":"#rating_list considered as object.its not a string columns!,its a column that contains list.\nted.dtypes","0842df03":"#apply and map f() are closely related.if i want to apply x function to whole col i use apply.\n#i use map to do dictionary mapping. {\"a\":1,\"b\":2} etc. For creating new col with old cols data.\n#tip:pay attention to dtypes,use apply even its considered slow (from time to time)","14945120":"ted.head()","1cbbdfef":"#function time (step by step)\ndef get_num_ratings(list_of_dicts):\n    return list_of_dicts[0]","e5a022c5":"get_num_ratings(ted.rating_list[0])","379cdc2d":"def get_num_ratings2(list_of_dicts):\n    return list_of_dicts[0][\"count\"]","ebcfe6a0":"get_num_ratings2(ted.rating_list[0])","8a5ddb9a":"def get_num_ratings3(list_of_dicts):\n    num=0\n    for d in list_of_dicts:\n        num=num+d[\"count\"]\n    return num","c6ec701f":"get_num_ratings3(ted.rating_list[0])","88a7d93b":"ted.rating_list.apply(get_num_ratings3)","7a624831":"ted[\"num_ratings\"]=ted.rating_list.apply(get_num_ratings3)","bc13e9a2":"ted.num_ratings.describe()","e14405a5":"#alternative methods.\n#for first item in rating_list these are the results.\npd.DataFrame(ted.rating_list[0])","b6160ae4":"#sort it out\npd.DataFrame(ted.rating_list[0]).sort_values(\"count\",ascending=False)\n#for first talk in list top count is inspiring with 24k, id is emotion_id(10 is for inspiring)\n#for first talk in the list there are total of 93k tags.If u put 1 inside of sqr brackets u see the dif results for 2nd data","305eb932":"#to get total count\n#u cant use use .count.sum() bcs colname conflicts with attribute.thats the result.U can change it to 1-2 etc to see others.\npd.DataFrame(ted.rating_list[0])[\"count\"].sum()","3fbcf1c0":"#step by step process.lets dive in.\n#step1:count the no of funny ratings","9ba0550b":"ted.rating_list.head()","da5950df":"#we want to check if all talks have funny tags in them\nted.ratings.str.contains(\"Funny\").value_counts()\n#its always there.","3c03c502":"#function time again.\ndef get_funny_ratings(list_of_dicts):\n    for d in list_of_dicts:\n        if d[\"name\"]==\"Funny\":\n            return d[\"count\"]","181678be":"ted[\"funny_ratings\"]=ted.rating_list.apply(get_funny_ratings)","1dc6dbe4":"#now we have every talks \"funny\" tag count in id order.\nted.funny_ratings.head()","fb50c34f":"#step2:lets get a percentage approach.\n#lets get percentage based approach to see how much of our \"tags\" are funny in that specific talk\nted[\"funny_rate\"]=ted.funny_ratings\/ted.num_ratings","fd4a7f20":"#lets do a fact check.to see our data is correct-here is the whole data.\n#what should i check from here to see if our approach is correct.speakers.occupation.comedian=higher,scientist=lower...\nted.sort_values(\"funny_rate\",ascending=False).head()","1b4a2614":"#reasonable.these dudes are funniest by their sheer funny_rate sorting results.\nted.sort_values(\"funny_rate\",ascending=False).speaker_occupation.head()","6c4387b3":"#least funny dudes.\nted.sort_values(\"funny_rate\",ascending=True).speaker_occupation.head()","7d84efa2":"#step3:Analyze the funny rate by occupation","0ed01049":"#x by y or x for each y wordsets used with groupby most of the time.\n#for each occupation analyze funny rate.Is this dude's occupation funny or Is it about that dude specificly?","d722e91c":"\nted.groupby(\"speaker_occupation\").funny_rate.mean().sort_values(ascending=False)\n#a lot of these occupations have very small sample size 1 most of the time.","b2e61014":"#as u can see.why its 2544 rather than 2550.bcs there are 6 null cases.\nted.speaker_occupation.describe()","c4386cdf":"#another approach-more cleaner one.the most highest ones have only 1 or 2 samples.\nted.groupby(\"speaker_occupation\").funny_rate.agg([\"mean\",\"count\"]).sort_values(by=\"mean\",ascending=False)","3e4b7a84":"#step4:focus on occupations that are well represented in the data.","45af2001":"occupation_counts=ted.speaker_occupation.value_counts()\noccupation_counts","fd04c8f6":"#lets define a frequency for our sample.then use filters.\noccupation_counts[occupation_counts>=5]","8d33947a":"#save these top occupations.\ntop_occupations=occupation_counts[occupation_counts>=5].index\ntop_occupations","62295658":"#here is the result's type.Index.Index can be treated like a list.thats the key.\ntype(top_occupations)","f49ad379":"#filter time using isin() returns true for the ones thats inside of that col and only true results ll be shown.\nted[ted.speaker_occupation.isin(top_occupations)]\n#isin is sth like multiple OR's.lets save it.","9cd4ffe0":"ted_top_occupations=ted[ted.speaker_occupation.isin(top_occupations)]","d7f9b21e":"#from 2544 speaker occupations,this is what we have left.786.\nted_top_occupations.shape","b2f41066":"ted_top_occupations.groupby(\"speaker_occupation\").funny_rate.mean().sort_values(ascending=False)\n#weaknesses of these approach=5 is small sample size.\n#also that performance poet dude that has done at least 5 ted talks however its all the same dude so result for performance\n#poets is quite high.\n#data is problematic in a sense that most of the speakers have more than 1 occupation..","244d3523":"<a id=\"3\"><\/a> \n## Visualizing Comment Distribution","1ef62424":"<a id=\"7\"><\/a> \n## Total Number of Ratings Received by Each Talk","b1a4c2ef":"<a id=\"5\"><\/a> \n## Best Event in TED History","147480c2":"# Crash Course Pandas 2\nIm creating these series for people that have beginner\/intermediate knowledge about pandas library.I m going to share my working notes(after tidying) to pass the info,nothing more. Certain codeblocks could get repetitive. Some info looks contradictory even though i have tried to remove most of them. Sorry for it beforehand. Also goodluck on your learning journey. This is my first kernel as well. Wish me luck. Contact me for any question, i will help you out if i can.\n\nContents:\n* [Dataset](#1)\n* [Most provoking Talks in Online Discussion](#2)\n* [Visualizing Comment Distribution](#3)\n* [Number of Talks From Each Year](#4)\n* [Best Event in TED History](#5)\n* [Unpacking Ratings Column](#6)\n* [Total Number of Ratings Received by Each Talk](#7)\n* [Which Occupations Have the Funniest TED Talks on AVG](#8)\n","d9cc3caa":"<a id=\"4\"><\/a> \n## Number of Talks From Each Year","c0877ee7":"<a id=\"6\"><\/a> \n## Unpacking Ratings Column","64f7002b":"<a id=\"1\"><\/a> \n## Dataset","341c197d":"<a id=\"2\"><\/a> \n## Most provoking Talks in Online Discussion","f65538dd":"<a id=\"8\"><\/a> \n## Which Occupations Have the Funniest TED Talks on AVG"}}