{"cell_type":{"88d06c55":"code","6513d6a0":"code","2725cf56":"code","e96e2017":"code","4bc63286":"code","07817518":"code","ac4d5d35":"code","69821368":"code","46273228":"code","8ff28183":"code","6a3bc182":"code","84e69c42":"code","61af70f7":"code","da3862bf":"code","4e0a8d96":"code","9449f439":"code","0d3f81d5":"code","56adf1d9":"code","fab39394":"code","dce7e3bb":"code","e7f21b1f":"code","393dbd53":"code","26ddcce1":"code","1fc62d82":"code","81dd6d99":"code","1033180e":"code","8550af70":"code","a62b1585":"code","76175808":"code","5deb270d":"code","4ffbdfe2":"code","365da163":"code","38ab6cdc":"code","09b0b5a2":"code","d36e70a7":"markdown"},"source":{"88d06c55":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6513d6a0":"# Import dataset\ncar_df = pd.read_csv('\/kaggle\/input\/vehicle-dataset-from-cardekho\/CAR DETAILS FROM CAR DEKHO.csv', encoding='ISO-8859-1')","2725cf56":"car_df.head()","e96e2017":"car_df.info()","4bc63286":"# Visualize dataset\nsns.pairplot(car_df)","07817518":"# Correlation matrix (heatmap style)\ncorrmat = car_df.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","ac4d5d35":"#histogram and normal probability plot\nsns.distplot(car_df['selling_price'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(car_df['selling_price'], plot=plt)","69821368":"#skewness and kurtosis\nprint(\"Skewness: %f\" % car_df['selling_price'].skew())\nprint(\"Kurtosis: %f\" % car_df['selling_price'].kurt())","46273228":"#applying log transformation\ncar_df['selling_price'] = np.log(car_df['selling_price'])","8ff28183":"#transformed histogram and normal probability plot\nsns.distplot(car_df['selling_price'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(car_df['selling_price'], plot=plt)","6a3bc182":"#scatter plot year\/selling_price\nvar = 'year'\ndata = pd.concat([car_df['selling_price'], car_df[var]], axis=1)\ndata.plot.scatter(x=var, y='selling_price', ylim=(0,800000));","84e69c42":"#scatter plot fuel\/selling_price\nvar = 'fuel'\ndata = pd.concat([car_df['selling_price'], car_df[var]], axis=1)\ndata.plot.scatter(x=var, y='selling_price', ylim=(0,800000));","61af70f7":"X = car_df.drop(['name', 'transmission', 'seller_type', 'fuel', 'owner','selling_price'], axis = 1)\nX","da3862bf":"y = car_df['selling_price']\ny.shape","4e0a8d96":"from sklearn.preprocessing import MinMaxScaler\n\nscaler_x = MinMaxScaler()\nX_scaled = scaler_x.fit_transform(X)","9449f439":"scaler_x.data_max_","0d3f81d5":"scaler_x.data_min_","56adf1d9":"print(X_scaled)","fab39394":"X_scaled.shape","dce7e3bb":"X","e7f21b1f":"y.shape","393dbd53":"y = y.values.reshape(-1,1)","26ddcce1":"y.shape","1fc62d82":"y","81dd6d99":"scaler_y = MinMaxScaler()\n\ny_scaled = scaler_y.fit_transform(y)","1033180e":"y_scaled","8550af70":"# Training the model\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size = 0.25)","a62b1585":"# Let's import required libraries\nimport tensorflow.keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.preprocessing import MinMaxScaler\n\nmodel = Sequential()\nmodel.add(Dense(25, input_dim=5, activation='relu'))\nmodel.add(Dense(25, activation='relu'))\nmodel.add(Dense(1, activation='linear'))\nmodel.summary()","76175808":"model.compile(optimizer='adam', loss='mean_squared_error')","5deb270d":"epochs_hist = model.fit(X_train, y_train, epochs=20, batch_size=25,  verbose=1, validation_split=0.2)","4ffbdfe2":"# Evaluating the model\nprint(epochs_hist.history.keys())","365da163":"car_df.columns","38ab6cdc":"plt.plot(epochs_hist.history['loss'])\nplt.plot(epochs_hist.history['val_loss'])\n\nplt.title('Model Loss Progression During Training\/Validation')\nplt.ylabel('Training and Validation Losses')\nplt.xlabel('Epoch Number')\nplt.legend(['Training Loss', 'Validation Loss'])","09b0b5a2":"# name, year, selling_price, km_driven, fuel, seller_type, transmission, owner  \n\n# ***(Note that input data must be normalized)***\n\nX_test_sample = np.array([[0, 0.4370344,  0.53515116, 0.57836085, 0.22342985]])\n#X_test_sample = np.array([[1, 0.53462305, 0.51713347, 0.46690159, 0.45198622]])\n\ny_predict_sample = model.predict(X_test_sample)\n\nprint('Expected selling price=', y_predict_sample)\ny_predict_sample_orig = scaler_y.inverse_transform(y_predict_sample)\nprint('Expected selling price=', y_predict_sample_orig)","d36e70a7":"our success probability is 95.107979%."}}