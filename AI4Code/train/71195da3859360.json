{"cell_type":{"9f564276":"code","e7a816b4":"code","ad16b17d":"code","ff06a72c":"code","7da26438":"code","ea4d4a31":"code","094150a3":"code","a7517861":"code","d76f32fc":"code","d4c4aed8":"code","a81535aa":"code","cf35af7d":"code","dad2c7d3":"code","284def44":"code","d3fe0060":"code","f6daa7ed":"code","9cde3ec6":"code","74bfa4d6":"code","05b804a8":"code","9e971be0":"code","0ee36437":"code","fce9111b":"code","f6b123e7":"code","398990ce":"code","5faccce7":"code","51e17647":"code","08ba72f8":"code","dfbf8648":"code","db85c369":"code","bf93a4c4":"code","0d944d47":"code","90e2bbe8":"code","5455673f":"code","12dcb57e":"code","9dd23ff1":"code","7a574f19":"code","51d3d001":"markdown","a7acc82e":"markdown","0142d4d0":"markdown","350a365f":"markdown","6e0fbab5":"markdown","d829e0c9":"markdown","4571f462":"markdown","9919f3f8":"markdown","ed15deb2":"markdown","a439c140":"markdown","0fa45cdc":"markdown","fd78c6a9":"markdown","3520c9e8":"markdown","e22b5896":"markdown","32e3d1b5":"markdown","ff159e3f":"markdown","8255567e":"markdown","57599766":"markdown","9659e042":"markdown","ffeba052":"markdown","fedfc87c":"markdown","7394db3a":"markdown","dcf95366":"markdown","9ae2d0cf":"markdown","9775d59c":"markdown","28e87ce9":"markdown","1ec79ca2":"markdown","fdef14ed":"markdown","f50ad991":"markdown","ae292cf0":"markdown","7b0de620":"markdown","84dab7f8":"markdown","6d8e8e63":"markdown","066b063b":"markdown","c2b15dcb":"markdown","6b24c410":"markdown","4d8f48c4":"markdown","a200ff0c":"markdown","b494804b":"markdown","79ef6a5f":"markdown","eca73e62":"markdown","38c4ec71":"markdown","dd18c7cc":"markdown","79335ee3":"markdown","9c535327":"markdown","e5da4871":"markdown","cb6d57f3":"markdown","5b4f4687":"markdown","07eb581f":"markdown","27b5aa01":"markdown","02f82a1f":"markdown","705204c4":"markdown","0324e9a9":"markdown","fb85de0f":"markdown","14e9c1da":"markdown","d36eabc1":"markdown","f6806584":"markdown","1cbf3ab0":"markdown","5c918634":"markdown","d45b9cae":"markdown","b5d8c8a3":"markdown","26da4dc7":"markdown","94c93f42":"markdown","49dba1c7":"markdown","e898550a":"markdown","82bb2d98":"markdown","2a8909b2":"markdown","de4510b4":"markdown","e43d3cea":"markdown","05dac74f":"markdown","4c07d6b7":"markdown","b92d268d":"markdown","67a5013c":"markdown","6b91bebd":"markdown","bb4c531d":"markdown","8c5bc540":"markdown","909693de":"markdown","8010c484":"markdown","7be59c34":"markdown","3c23d0fd":"markdown","bd604a10":"markdown","dbdd6a5a":"markdown","541e6b2a":"markdown","9bd96ea7":"markdown"},"source":{"9f564276":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split,KFold,cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nimport plotly \nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.offline as py\nfrom plotly.offline import iplot\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n\nimport missingno as msno\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e7a816b4":"df = pd.read_csv('..\/input\/water-potability\/water_potability.csv')\ndf.head()","ad16b17d":"df.shape","ff06a72c":"df.info()","7da26438":"df.describe()","ea4d4a31":"df['Potability'].value_counts(normalize=True)","094150a3":"df.drop('Potability', axis=1).skew()","a7517861":"df.drop('Potability', axis=1).hist(figsize=(12,8));","d76f32fc":"df.corr()","d4c4aed8":"index_vals = df['Potability'].astype('category').cat.codes\n\nfig = go.Figure(data=go.Splom(\n                dimensions=[dict(label='ph',\n                                 values=df['ph']),\n                            dict(label='Hardness',\n                                 values=df['Hardness']),\n                            dict(label='Solids',\n                                 values=df['Solids']),\n                            dict(label='Chloramines',\n                                 values=df['Chloramines']),\n                           dict(label='Sulfate',\n                                 values=df['Sulfate']),\n                            dict(label='Conductivity',\n                                 values=df['Conductivity']),\n                            dict(label='Organic_carbon',\n                                 values=df['Organic_carbon']),\n                            dict(label='Trihalomethanes',\n                                 values=df['Trihalomethanes']),\n                           dict(label='Turbidity',\n                                 values=df['Turbidity'])],\n                showupperhalf=False, \n                text=df['Potability'],\n                marker=dict(color=index_vals,\n                            showscale=False,\n                            line_color='white', line_width=0.5)\n                ))\n\n\nfig.update_layout(\n    title='Water Quality',\n    width=1000,\n    height=1000,\n)\n\nfig.show()","a81535aa":"fig = go.Figure(go.Heatmap(z=df.corr(), x=df.corr().columns.tolist(), y=df.corr().columns.tolist(), colorscale='agsunset'))\nfig.show()","cf35af7d":"df.isnull().sum()","dad2c7d3":"df.isnull().mean()*100 ","284def44":"df.isnull().mean().plot.bar(figsize=(12,6)) \nplt.ylabel('Percentage of missing values') \nplt.xlabel('Features') \nplt.title('Missing Data in Percentages');","d3fe0060":"msno.bar(df);","f6daa7ed":"msno.matrix(df);","9cde3ec6":"df = pd.read_csv('..\/input\/water-potability\/water_potability.csv')\nX = df.drop('Potability', axis=1)\ny = df['Potability']\n\nmodel = LinearDiscriminantAnalysis()\n\ncv = KFold(n_splits=3, shuffle=True, random_state=42)\n\nresult = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n\nprint (f'Accuracy:: %{round(result.mean() * 100.0,3)}')","74bfa4d6":"df = pd.read_csv('..\/input\/water-potability\/water_potability.csv')\nX = df.drop('Potability', axis=1)\ny = df['Potability']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42)\nprint(f'Size of the X_train: {X_train.shape[0]}')\nmodel = XGBClassifier(eval_metric='logloss')\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, predictions)\nprint (f'Accuracy:: {round(accuracy * 100.0,3)}')","05b804a8":"print(f'Before dropping missing values, we have {df.shape[0]} instances')\n\ndf_dropped = df.dropna()\nprint(f'After dropping missing values, we have {df_dropped.shape[0]} instances')\n\n# to see how mnay instances we have lost\nprint(f'We have lost {df.shape[0]-df_dropped.shape[0]} instances, which means % {round((df.shape[0]-df_dropped.shape[0])\/ (float(df.shape[0])),2)*100} data we have lost')\n","9e971be0":"df_dropped.isnull().sum()","0ee36437":"X = df_dropped.drop('Potability', axis=1)\ny = df_dropped['Potability']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42)\nprint(f'Size of the X_train: {X_train.shape[0]}')\nmodel = XGBClassifier(eval_metric='logloss')\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, predictions)\nprint (f'Accuracy:: {round(accuracy * 100.0,3)}')","fce9111b":"\ndf = pd.read_csv('..\/input\/water-potability\/water_potability.csv')\n\nX = df.drop('Potability', axis=1)\ny = df['Potability']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42)","f6b123e7":"X_train.head()","398990ce":"X_train.describe()","5faccce7":"X_train.skew()","51e17647":"X_train.hist(figsize=(12,8));","08ba72f8":"X_train.isnull().mean()*100","dfbf8648":"imputer = SimpleImputer(strategy='median') # median imputation ('mean' for mean and 'most_frequent' for mode imputations)\n\nimputer.fit(X_train) # SimpleImputer() learns the median values from the train data\n\nX_train = imputer.transform(X_train) # replace missing values with medians\n\nX_test = imputer.transform(X_test) # replace missing values with medians","db85c369":"print(type(X_train))\n\ntype(X_test)","bf93a4c4":"X_test=pd.DataFrame(X_test)\nX_train= pd.DataFrame(X_train)\nX_train.head()","0d944d47":"model = XGBClassifier(eval_metric='logloss')\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\nprint(f'Size of the X_train: {X_train.shape[0]}')\naccuracy = accuracy_score(y_test, predictions)\nprint (f'Accuracy:: {round(accuracy * 100.0,3)}')","90e2bbe8":"df = pd.read_csv('..\/input\/water-potability\/water_potability.csv')\n\nX = df.drop('Potability', axis=1)\ny = df['Potability']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42)\n\n\nimputer = SimpleImputer(strategy='constant', fill_value=9999) # arbitrary number imputation\n\nimputer.fit(X_train) # SimpleImputer() learns the arbitrary number from the train data\n\nX_train = imputer.transform(X_train) # replace missing values with arbitrary number\n\nX_test = imputer.transform(X_test) # replace missing values with arbitrary number","5455673f":"X_test=pd.DataFrame(X_test)\nX_train= pd.DataFrame(X_train)\nX_train.head()","12dcb57e":"model = XGBClassifier(eval_metric='logloss')\n\nmodel.fit(X_train, y_train)\n\npredictions = model.predict(X_test)\n\nprint(f'Size of the X_train: {X_train.shape[0]}')\n\naccuracy = accuracy_score(y_test, predictions)\nprint (f'Accuracy:: {round(accuracy * 100.0,3)}')","9dd23ff1":"df = pd.read_csv('..\/input\/water-potability\/water_potability.csv')\n\nX = df.drop('Potability', axis=1)\ny = df['Potability']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42)\n\nimputer = SimpleImputer(strategy='median')\n\nmodel = XGBClassifier(eval_metric='logloss')\n\npipeline = Pipeline(steps=[('imputer', imputer),('model', model)])\n\nkfold = KFold(n_splits=10, random_state=42)\nresults = cross_val_score(model, X, y, cv=kfold)\nprint(f'Accuracy: Results Mean : %{round(results.mean()*100,3)}, Results Standard Deviation : {round(results.std()*100,3)}')\n","7a574f19":"df = pd.read_csv('..\/input\/water-potability\/water_potability.csv')\n\nX = df.drop('Potability', axis=1)\ny = df['Potability']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42)\n\nimputer = KNNImputer()\n\nmodel = XGBClassifier(eval_metric='logloss')\n\npipeline = Pipeline(steps=[('imputer', imputer),('model', model)])\n\nkfold = KFold(n_splits=10, random_state=42)\nresults = cross_val_score(model, X, y, cv=kfold)\nprint(f'Accuracy: Results Mean : %{round(results.mean()*100,3)}, Results Standard Deviation : {round(results.std()*100,3)}')","51d3d001":"<a id=\"5\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Machine Learning Algorithms with the Missing Values (Example)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n","a7acc82e":"![](https:\/\/www.researchgate.net\/profile\/Ming-Zong-2\/publication\/312507655\/figure\/fig5\/AS:668881756639250@1536485246987\/Regression-and-missing-value-imputation-using-the-kNN-method-with-a-fixed-k-value-that.jpg)","0142d4d0":"- Even though most of the machine learning algorithms don't work with the missing values, we have a few exceptions. One of them is **XGBoost Algorithm**. \n- For that reason we will use XGBoost Algorithm in this study to see with and without missing values, what kind of changes, if any, we will observe in our predictions.","350a365f":"- Let's see the missing values","6e0fbab5":"- **Missingno matrix** gives us the location and patterns (if any) of the missing data.","d829e0c9":"XGBoost with the Missing Data","4571f462":"#### In this study, we have covered different aspects of how to deal with missing values.\n#### We have made a detailed analysis of the most used methods to deal with missing values.\n#### I strongly suggest using different methods to deal with missing values and not just delete all missing values.","9919f3f8":"- When the training set is small or moderate in size, K-nearest neighbors can be a quick and effective method for imputing missing values.\n\n- This procedure identifies a sample with one or more missing values. Then it identifies the K most similar samples in the training data that are complete. Similarity of samples for this method is defined by a distance metric.\n\n- Once the K neighbors are found, their values are used to impute the missing data.","ed15deb2":"- As seen in the correlation matrix,scatterplots and heatmap, there is no strong relationship between the variables.","a439c140":"Image Credit: https:\/\/miro.medium.com\/","0fa45cdc":"- Our features have very close to normal distribution. \n- Solids have slightly right skewness.","fd78c6a9":"- OK let's use the scikit learn **SimpleImputer** to imput the missing values\n\n- Even though our features have very low-level skewness, it would be better to use 'median', instead of 'mean' as an imput value.","3520c9e8":"- **Missingno bar** gives us bar cahrt version of the completeness & nullity of the variables.","e22b5896":"- Replacing missing values with  an arbitrary value. Some commonly used values include 999, 9999.","32e3d1b5":"Now let's see the percentage of missing values in each feature.","ff159e3f":"- We have 3276 instances with the 9 input and 1 output variable","8255567e":"### **Bivariate Analysis**","57599766":"- Machine Learning algorithms, with a few exceptions, do not tolerate missing values.","9659e042":"### <u>Important Points to Remember<\/u>","ffeba052":"<a id=\"1\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Why do we need to handle Missing Data?<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n","fedfc87c":"Image Credit : https:\/\/www.statstest.com\/","7394db3a":"- Visualizations and getting numeric summaries are the first step in understanding the missing information in a dataset.\n- Identify how many and what type of missing values are in our dataset is one of the first steps when dealing with missing values.\n","dcf95366":"- Our accuracy rate  is %58.301\n- We used 2293 of instances as a training data.","9ae2d0cf":"![](https:\/\/miro.medium.com\/max\/1400\/1*FUZS9K4JPqzfXDcC83BQTw.png)","9775d59c":"Image Credit: https:\/\/www.researchgate.net\/figure\/Regression-and-missing-value-imputation-using-the-kNN-method-with-a-fixed-k-value-that_fig5_312507655","28e87ce9":"<a id=\"toc\"><\/a>\n\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Contents<\/h3>\n    \n* [What is Missing Data](#0)   \n* [Why do we need to handle Missing Data?](#1)\n* [Data](#2)\n* [Exploratory Data Analysis](#3)\n* [Missing Values](#4)\n* [Machine Learning Algorithms with the Missing Values (Example)](#5)\n* [The Most Used Methods to Deal Missing Values](#6)\n* [Removing Observations with Missing Data](#7)\n* [Performing Mean & Median & Mode Imputation](#8)\n* [Replacing Missing Values with an Arbitrary Number](#9)\n* [Dealing Missing Values in Categorical Variables](#10)\n* [Imputing Values in a Machine Learning Pipeline](#11)\n* [Nearest Neighbor Imputation (K-Nearest Neighbor (KNN) Imputation](#12)\n* [Conclusion](#13)\n\n* [Further Reading](#14)\n* [References](#15)","1ec79ca2":" <a id=\"10\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Dealing Missing Values in Categorical Variables<\/b><\/font>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","fdef14ed":"As we have seen,\n\n* ph : 14.98 %\n* Sulfate : 23.84 %\n* Trihalomethanes : 4.94 %","f50ad991":"![](https:\/\/soprasteriaanalyticsdotse.files.wordpress.com\/2020\/01\/missingdata.png?w=950)","ae292cf0":"- Our accuracy rate  is %65.717\n- We used 2293 of instances as a training data.","7b0de620":"'In statistics, missing data, or missing values, occur when no data value is stored for the variable in an observation. Missing data are a common occurrence and can have a significant effect on the conclusions that can be drawn from the data.'\n\nReference: https:\/\/en.wikipedia.org\/wiki\/Missing_data","84dab7f8":"- Our accuracy rate  is %58.301\n- We used 2293 of instances as a training data.","6d8e8e63":"- We have seen several NaN values and different scales of the features.\n\n- In this study we will use XGBoost Algorithm, and XGBoost is an ensemble algorithm of decision trees, it does not require normalization for the inputs. So we will not focus on normalization.","066b063b":"![](https:\/\/www.statstest.com\/wp-content\/uploads\/2020\/05\/Linear-Discriminant-Analysis.jpg)","c2b15dcb":"- Another method to show the missing values by using **[missingno library](https:\/\/github.com\/ResidentMario\/missingno)** \n","6b24c410":"- OK. We don't have any missing values and have lost %39 percent of our data.\n- Let's move on to modeling and prediction.","4d8f48c4":"- OK. It is enough with the basic theory, and let's move to the coding and the action ","a200ff0c":"Context\nAccess to safe drinking-water is essential to health, a basic human right and a component of effective policy for health protection. This is important as a health and development issue at a national, regional and local level. In some regions, it has been shown that investments in water supply and sanitation can yield a net economic benefit, since the reductions in adverse health effects and health care costs outweigh the costs of undertaking the interventions.\n\nContent\nThe water_potability.csv file contains water quality metrics for 3276 different water bodies.\n\n1. pH value:\nPH is an important parameter in evaluating the acid\u2013base balance of water. It is also the indicator of acidic or alkaline condition of water status. WHO has recommended maximum permissible limit of pH from 6.5 to 8.5. The current investigation ranges were 6.52\u20136.83 which are in the range of WHO standards.\n\n2. Hardness:\nHardness is mainly caused by calcium and magnesium salts. These salts are dissolved from geologic deposits through which water travels. The length of time water is in contact with hardness producing material helps determine how much hardness there is in raw water. Hardness was originally defined as the capacity of water to precipitate soap caused by Calcium and Magnesium.\n\n3. Solids (Total dissolved solids - TDS):\nWater has the ability to dissolve a wide range of inorganic and some organic minerals or salts such as potassium, calcium, sodium, bicarbonates, chlorides, magnesium, sulfates etc. These minerals produced un-wanted taste and diluted color in appearance of water. This is the important parameter for the use of water. The water with high TDS value indicates that water is highly mineralized. Desirable limit for TDS is 500 mg\/l and maximum limit is 1000 mg\/l which prescribed for drinking purpose.\n\n4. Chloramines:\nChlorine and chloramine are the major disinfectants used in public water systems. Chloramines are most commonly formed when ammonia is added to chlorine to treat drinking water. Chlorine levels up to 4 milligrams per liter (mg\/L or 4 parts per million (ppm)) are considered safe in drinking water.\n\n5. Sulfate:\nSulfates are naturally occurring substances that are found in minerals, soil, and rocks. They are present in ambient air, groundwater, plants, and food. The principal commercial use of sulfate is in the chemical industry. Sulfate concentration in seawater is about 2,700 milligrams per liter (mg\/L). It ranges from 3 to 30 mg\/L in most freshwater supplies, although much higher concentrations (1000 mg\/L) are found in some geographic locations.\n\n6. Conductivity:\nPure water is not a good conductor of electric current rather\u2019s a good insulator. Increase in ions concentration enhances the electrical conductivity of water. Generally, the amount of dissolved solids in water determines the electrical conductivity. Electrical conductivity (EC) actually measures the ionic process of a solution that enables it to transmit current. According to WHO standards, EC value should not exceeded 400 \u03bcS\/cm.\n\n7. Organic_carbon:\nTotal Organic Carbon (TOC) in source waters comes from decaying natural organic matter (NOM) as well as synthetic sources. TOC is a measure of the total amount of carbon in organic compounds in pure water. According to US EPA < 2 mg\/L as TOC in treated \/ drinking water, and < 4 mg\/Lit in source water which is use for treatment.\n\n8. Trihalomethanes:\nTHMs are chemicals which may be found in water treated with chlorine. The concentration of THMs in drinking water varies according to the level of organic material in the water, the amount of chlorine required to treat the water, and the temperature of the water that is being treated. THM levels up to 80 ppm is considered safe in drinking water.\n\n9. Turbidity:\nThe turbidity of water depends on the quantity of solid matter present in the suspended state. It is a measure of light emitting properties of water and the test is used to indicate the quality of waste discharge with respect to colloidal matter. The mean turbidity value obtained for Wondo Genet Campus (0.98 NTU) is lower than the WHO recommended value of 5.00 NTU.\n\n10. Potability:\nIndicates if water is safe for human consumption where 1 means Potable and 0 means Not potable.\n\nReference: https:\/\/www.kaggle.com\/adityakadiwal\/water-potability ","b494804b":"- It is by far the most used one by beginners.\n- It is easy to use.\n- For me, if it is possible not to use it. Otherwise, it should be the last resort to use handling the missing data.\n- Data is precious and needs to being used as much as possible.\n- Missing data can be a sign of a significant concept in our data collection stage or give a strong signal about the data (missing income data in the tax-related study \ud83d\ude01 )","79ef6a5f":"### **Univariate Analysis**","eca73e62":"Image Credit: https:\/\/soprasteriaanalyticsdotse.files.wordpress.com\/","38c4ec71":"<a id=\"0\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>What is Missing Data ?<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n","dd18c7cc":"OK. We have 3 features with the mssing values.\n\n* ph : 491\n* Sulfate : 781\n* Trihalomethanes : 162\n","79335ee3":"image Credit: https:\/\/visualstudiomagazine.com","9c535327":"- Let's import libraries","e5da4871":"###  Mean, median imputation should be calculated by using train data, <u>not whole data !!!<\/u>\n###  Imputation should usually occur before scaling and centering, otherwise,the resulting means and standard deviations will inherit the potential biases and issues incurred from data deletion.\n\nReference :  [Feature Engineering and Selection](https:\/\/www.amazon.com\/Feature-Engineering-Selection-Chapman-Science\/dp\/1032090855\/ref=sr_1_1?crid=19T9G95E1W7VJ&dchild=1&keywords=feature+engineering+and+selection+kuhn&qid=1628050948&sprefix=feature+engineering+and+%2Cdigital-text%2C293&sr=8-1)\n","cb6d57f3":"- Result is lesser than the previous ones.","5b4f4687":"- Our accuracy rate  is %65.819\n- We used 2293 of instances as a training data.","07eb581f":"<a id=\"14\"><\/a>\n<font color=\"darkblue\" size=+1.5><b>Further Reading<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n\n\n[Machine Learning - Beginner &Intermediate-Friendly BOOKS](https:\/\/www.kaggle.com\/general\/255972)","27b5aa01":"- Our features have very close to normal distribution. \n- Solids have slightly right skewness.","02f82a1f":"- XGBoost Algorithm is one of the exceptions.","705204c4":"#### Since our dataset's features, except target variable, are numerical variables. Our missing values are in the numerical variables.  We couldn't be able to use the 'Missing' category method.\n\n#### I will put the recipe here, and we will use this recipe in our future notebooks.\n\ndf = pd.read_csv('sample.csv')\n\nX = df.drop('Target', axis=1)\ny = df['Target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42)\n\n\nimputer = SimpleImputer(strategy='constant', fill_value='Missing') # 'Missing' imputation for categorical variables\n\nimputer.fit(X_train) # SimpleImputer() \n\nX_train = imputer.transform(X_train) # replace missing values with 'Missing' \n\nX_test = imputer.transform(X_test) # replace missing values with 'Missing' \n\n","0324e9a9":"Image Credit : https:\/\/upload.wikimedia.org","fb85de0f":"#### In this study, we focus on the **MISSING VALUES**.\n\n#### After finishing the beginner-friendly explanation series, I will publish ML algorithms in action.\n\n\n#### By the way, when you like the topic, you can show it by supporting \ud83d\udc4d\n\n####  **Feel free to leave a comment**. \n\n#### All the best \ud83e\udd18","14e9c1da":"- It is very close to original dataset.","d36eabc1":"<a id=\"12\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Nearest Neighbor Imputation (K-Nearest Neighbor (KNN) Imputation)<\/b><\/font>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","f6806584":"<a id=\"8\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Performing Mean & Median & Mode Imputation<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n\n","1cbf3ab0":"- Read csv and see the top 5 instances of the data","5c918634":"<a id=\"15\"><\/a>\n<font color=\"darkblue\" size=+1.5><b>References:<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n\n\n-  https:\/\/machinelearningmastery.com\/handle-missing-data-python\/ \n\n- [Python Feature Engineering Cookbook](https:\/\/www.amazon.com\/Python-Feature-Engineering-Cookbook-transforming\/dp\/1789806313\/ref=sr_1_1?dchild=1&keywords=feature+engineering+cookbook&qid=1627628487&s=books&sr=1-1) \n- [Feature Engineering Made Easy](https:\/\/www.amazon.com\/Feature-Engineering-Made-Easy-Identify-ebook\/dp\/B077N6MK5W)\n- [Feature Engineering and Selection](https:\/\/www.amazon.com\/Feature-Engineering-Selection-Chapman-Science\/dp\/1032090855\/ref=sr_1_1?crid=19T9G95E1W7VJ&dchild=1&keywords=feature+engineering+and+selection+kuhn&qid=1628050948&sprefix=feature+engineering+and+%2Cdigital-text%2C293&sr=8-1)\n","d45b9cae":"#### Hi all.  \ud83d\ude4b\u200d\u2642\ufe0f \n\n#### I have recently published [Beginner Friendly Detailed Explained EDAs \u2013 For anyone at the beginnings of DS\/ML journey](https:\/\/www.kaggle.com\/general\/253911#1393015) series.\n\n#### After getting positive feedback and requests for Beginner-Intermediate Friendly Machine Learning series, I started to publish the Machine Learning Basic Series, which would help anyone who wants to learn or refresh the basics of ML.\n\n#### What we have covered: \n\n#### [BIAS & VARIANCE TRADEOFF](https:\/\/www.kaggle.com\/kaanboke\/ml-basics-bias-variance-tradeoff) \u2714\ufe0f\n\n#### [LINEAR ALGORITHMS](https:\/\/www.kaggle.com\/kaanboke\/ml-basics-linear-algorithms)  \u2714\ufe0f\n\n#### [NONLINEAR ALGORITHMS](https:\/\/www.kaggle.com\/kaanboke\/nonlinear-algorithms)  \u2714\ufe0f\n\n#### In this post, I want to share with you :\n\n#### One of the essential concepts of any data analysis also Machine Learning Journey: **MISSING VALUES**\n\n#### In this series, I will focus on the basic concepts with detailed explanations.\n\n#### After finishing the beginner-friendly explanation series, I will publish ML algorithms in action.\n\n\n#### **By the way, when you like the topic, you can show it by supporting** \ud83d\udc4d\n\n####  **Feel free to leave a comment in the notebook**. \n\n\n#### All the best \ud83e\udd18\n","b5d8c8a3":"- Our accuracy rate  is %65.728\n- We used 1407 instances as training data. \n- We can observe a drop in the prediction (prediction rate is not the main concern of the study).\n- But the important thing, our model trained with lesser data.\n- Since we are dealing with ML models, we need more data to train to get better predictions.\n- Originally, our data has 3276 instances\n- we have lost %39 of the data after dropping the missing values.","26da4dc7":"<a id=\"3\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Exploratory Data Analysis<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","94c93f42":"<a id=\"9\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Replacing Missing Values with an Arbitrary Number<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","49dba1c7":"![](https:\/\/miro.medium.com\/max\/1400\/1*1kjLMDQMufaQoS-nNJfg1Q.png)","e898550a":"<a id=\"11\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Imputing Values in a Machine Learning Pipeline<\/b><\/font>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","82bb2d98":"### **Target Variable**","2a8909b2":"<a id=\"7\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Removing Observations with Missing Data<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n","de4510b4":"<a id=\"13\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Conclusion<\/b><\/font>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","e43d3cea":" #### Linear Discriminant Analysis with the Missing Data","05dac74f":"- Mostly Machine Learning algorithms don't work with the missing values (We will see one example in this study).\n- For those algorithms, missing values needs to be removed or converted to numbers.\n- Handling with missing values should be done before the modeling.\n- Several factors can cause the missing data: Structural deficiencies in data (for example, clinical studies- patient dies, etc.), merging with other datasets, random events, failures of measurement, etc.\n- In this study, we will focus on the practical side of how to handle the missing data.\n- We are not going to cover the theoretical background, which is not the aim of this study. \n- But if you are interested in the details of the missing data and its causes, follow the further readings below.","4c07d6b7":"<a id=\"2\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Data<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","b92d268d":"- Almost %61 of the instances of our target variable is 'Potable'\n- %39  of the instances of our target variable is 'Not Potable'","67a5013c":"#### Water Quality Drinking water potability","6b91bebd":"<a id=\"6\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>The Most Used Methods to Deal Missing Values<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n","bb4c531d":"### **Skewness**","8c5bc540":"\n- 'In statistics, imputation is the process of replacing missing data with substituted values.' (reference : https:\/\/en.wikipedia.org\/wiki\/Imputation_(statistics) )\n\n- It is another preferred method by beginners. Substitute missing values with a mean of the numerical variable.\n\n- I mostly agree with the idea of imputation with mean\/median\/mode than dropping the missing values.   \n- The amount of the missing value is crucial to decide the data imputation technique because, in essence, we want our imputed values should be as close as possible to their original values.\n\n- In the literature, mostly %20 missing data is suggested as a threshold to impute the data. But, in real life, as data scientists, we have many other factors to consider before deciding on it.","909693de":"<a id=\"4\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Missing Values<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","8010c484":"- Before moving on to the methods of dealing with missing values, we will use Linear Discriminant Analysis and  XGBoost Algorithm to make predictions with the missing data.","7be59c34":"- Based on the mean-median values of the 9 input variables, our input variables seem to have very close to normal distibution with slight skewness.\n- Also, as we have mentioned before, input variables have different scales.","3c23d0fd":"Image Credit: https:\/\/www.chaussurevip.fr\/","bd604a10":"In this study we will see the most used methods to deal the missing data:\n\n- Removing observations with missing data\n- Performing mean or median imputation\n- Replacing missing values with an arbitrary number\n- Dealing Missing Values in Categorical Variables\n- Imputing values in a machine learning pipeline\n- Nearest neighbor imputation (k-nearest neighbor (KNN) imputation)","dbdd6a5a":"- 'One definition of an ML pipeline is a means of automating the machine learning workflow by enabling data to be transformed and correlated into a model that can then be analyzed to achieve outputs. This type of ML pipeline makes the process of inputting data into the ML model fully automated.' (reference: https:\/\/algorithmia.com\/blog\/ml-pipeline)  \n\n- In our Machine Learning models,  we have to avoid  **'Data Leakage'**.\n\n- When we use pipeline in our models, data pass through different automated steps before reaching the final output.\n\n- In our case, imputer will fit only on the training dataset, not the entire dataset or test set.\n\n- Let's see the coding.","541e6b2a":"![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/e8\/Imputation.jpg)","9bd96ea7":"![](https:\/\/visualstudiomagazine.com\/-\/media\/ECG\/visualstudiomagazine\/Images\/introimages\/0915vsm_MooneyInDepth_b.jpg)"}}