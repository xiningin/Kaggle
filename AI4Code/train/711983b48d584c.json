{"cell_type":{"745f5819":"code","bb841bf6":"code","6d1ec095":"markdown","c2d7ab44":"markdown","a82e6ef6":"markdown","afbf5fc5":"markdown","7f574685":"markdown"},"source":{"745f5819":"from torch import nn\nclass TrainingModule(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n        self.crit = nn.MSELoss()\n\n    def forward(self, target=None, standard_error=None, **kwargs):\n        out = self.model(**kwargs)\n        logits = out.logits\n        loss = self.crit(logits.view(-1), target)\n        return loss\n\n    def predict(self, target=None, standard_error=None, **kwargs):\n        out = self.model(**kwargs)\n        logits = out.logits\n        return logits\n# model_name_or_path = 'roberta-base'\n# model = AutoModelForSequenceClassification.from_pretrained(\n#         model_name_or_path, num_labels=1\n# )\n# model = TrainingModule(model)\n# # dataset, dataloader\n# loss = model(**batch)","bb841bf6":"num_heads = 32\nclass TrainingModule(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n        self.crit = nn.MSELoss()\n\n    def forward(self, target=None, standard_error=None, **kwargs):\n        mean = einops.repeat(target, \"b -> b n\", n=num_heads)\n        std = einops.repeat(standard_error, \"b -> b n\", n=num_heads)\n        targets = torch.normal(mean=mean, std=std)\n        out = self.model(**kwargs)\n        logits = out.logits\n        loss = self.crit(logits, targets)\n        return loss\n\n    def predict(self, target=None, standard_error=None, **kwargs):\n        out = self.model(**kwargs)\n        logits = out.logits\n        return logits.mean(-1)\n# model_name_or_path = 'roberta-base'\n# model = AutoModelForSequenceClassification.from_pretrained(\n#         model_name_or_path, num_labels=num_heads\n# )\n# model = TrainingModule(model)\n# # dataset, dataloader\n# loss = model(**batch)","6d1ec095":"## Motivation\n\n* there is a `standard_error` term in the training data, we should try to utilize it!\n\n## Introduction\n\n* First, \"gaussian multi-target regularization\" is just a random name I came up with.\n* The idea is to have a model that predicts multiple outputs, where the targets are sampled from a Gaussian distribution, based on `target` and `standard_error`\n\n## Benefits\n\nBefore diving into code, let's first think what are the potential benefits of this strategy.\n\n1. data augmentation\n    * sampling the target from a normal distribution should have a regularization effect\n2. ensemble\n    * multi-target means we will have multiple slightly different heads for ensemble, which should improve performance\n \n## Code\n\n* To make it short, I will only include code snippet copied from my local enviroment.\n\n\n## Result\n\n* Based on my local experiments, the proposed method is indeed better!\n    * The validation RMSE is lower compared to the baseline.\n    * The validation RMSE has lower variations between runs, compared to the baseline.\n* Since we are only increasing the number of output in the last layer, the increase in memory\/compute is fairly negligible. (I tried at most 1024 targets.)\n* As shown below, the code change is also minimal, so try it out!\n* Please give this notebook a upvote if you find it useful!","c2d7ab44":"# Improve performance and robustness with Gaussian Multi-Target Regularization","a82e6ef6":"Hi all! In this notebook I will share a strategy that can easily be added to your model, which improves both model performance and robustness! I will also share a minimal implementation of this strategy.","afbf5fc5":"## Code for proposed strategy","7f574685":"## Baseline code"}}