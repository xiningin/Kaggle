{"cell_type":{"111f0811":"code","3a1ec547":"code","f76aa1b0":"code","072e2013":"code","5397d6d5":"code","264a5787":"code","03b9e205":"code","3511f802":"code","039613bb":"code","c3f4216b":"markdown","a58983ba":"markdown","a221df75":"markdown","5e0ec16a":"markdown","41865462":"markdown"},"source":{"111f0811":"%%bash\n\npip uninstall -y typing  # trouble for gazpacho\npip install asks trio gazpacho","3a1ec547":"from pathlib import Path\n\nimport asks\nimport trio\nimport gazpacho as gzp\nfrom tqdm.notebook import tqdm","f76aa1b0":"Path('\/home\/vids').mkdir(exist_ok=True)\n\npath_gen = Path('..\/input\/parler').glob('**\/*.mp4')\nexisting = [p.name for p in path_gen]\nprint(f'{len(existing)} videos in the dataset.')\n      ","072e2013":"def get_links(start_page):\n    html = gzp.get(start_page)\n    soup = gzp.Soup(html)\n    ancs = soup.find('a')\n    refs = [a.attrs['href'] for a in ancs]\n    vids_new = [r for r in refs if '.mp4' in r and \\\n                r not in existing][:32]  # testing\n    print(f'Getting {len(vids_new)} videos.')\n    return vids_new","5397d6d5":"async def fetch_vid(s, vid):\n    url = f\"https:\/\/www.tommycarstensen.com\/terrorism\/{vid}\"\n    r = await s.get(url)\n    return r.content\n\n\nasync def save_vid(s, vid):\n    content = await fetch_vid(s, vid)\n    filename = f\"\/home\/vids\/{vid}\"\n    with open(filename,'wb') as f:\n        f.write(content)\n\n    \nasync def main(start_page):\n    vids_new = get_links(start_page)\n    dname = '\/'.join(start_page.split('\/')[:3])\n    s = asks.sessions.Session(dname, connections=16)\n    async with trio.open_nursery() as n:\n        for vid in vids_new:\n            n.start_soon(save_vid, s, vid)\n","264a5787":"%%time\n\nstart_page = 'https:\/\/www.tommycarstensen.com\/terrorism\/index.html'\ntrio.run(main, start_page)","03b9e205":"import requests\n\ndef fetch_vid(vid):\n    url = f\"https:\/\/www.tommycarstensen.com\/terrorism\/{vid}\"\n    r = requests.get(url, stream=True)\n    return r.content\n\n\ndef save_vid(vid):\n    content = fetch_vid(vid)\n    filename = f\"\/home\/vids\/{vid}\"\n    with open(filename,'wb') as f:\n        f.write(content)\n\n    \ndef main(start_page):\n    vids_new = get_links(start_page)\n    for vid in tqdm(vids_new):\n        save_vid(vid)\n","3511f802":"start_page = 'https:\/\/www.tommycarstensen.com\/terrorism\/index.html'\nmain(start_page)","039613bb":"%%bash\n\n# zip -r capitol_vids3.zip \/home\/vids\/*.mp4\nls -U  \/home\/vids | head -10","c3f4216b":"## Regular way to download","a58983ba":"## Asynchronous, multi-threaded way to download","a221df75":"## Check for existing videos.","5e0ec16a":"## Use Gazpacho to get links from the index page.","41865462":"Web crawling is a common task. Here I'm using some newer libraries to speed up the job and still keep simple code.\n\nA comparison shows these libraries can download 32 videos in 00:20, compared to about 02:00 using a synchronous, single-threaded pipeline."}}