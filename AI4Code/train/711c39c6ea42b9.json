{"cell_type":{"ce9a35b2":"code","acf7513d":"code","b7670f47":"code","7f6af517":"code","a8cc8f78":"code","df46cdb7":"code","d4b81e99":"code","4a104dc9":"code","2c09d009":"code","cc8af6ba":"code","e99e5649":"code","a2657cb7":"code","99fb4fa6":"code","d1e166f1":"code","20d3ee4f":"code","f6e448fb":"code","d51034ea":"code","abc45afa":"code","7183c80d":"code","0ceb07a4":"code","54108cfe":"code","0138b01c":"code","26ed1d3e":"code","3e94582e":"code","bba38765":"code","a009b5e9":"code","43d79838":"code","f1392ae1":"code","9bbee811":"code","32237f70":"code","09612dfb":"code","ef7d8a3d":"code","d56b83a3":"code","e77bf9fc":"code","d31e3e6f":"code","4a8236c5":"code","448e267e":"code","be884093":"code","0a38b517":"code","34aee7b1":"code","2e1fbc3e":"code","5e5921b5":"markdown","b76fb5b3":"markdown","8ef67ca5":"markdown","6099c026":"markdown","1d949b19":"markdown","f2ba6edf":"markdown","d526228d":"markdown","8f840e11":"markdown","22120641":"markdown","903f4826":"markdown","de2e3e91":"markdown","cede5dcc":"markdown","c7c3ee13":"markdown","77b88bc6":"markdown","a98ca8aa":"markdown","9b2ccb08":"markdown","28ffd725":"markdown"},"source":{"ce9a35b2":"#making the imports \/ ignore warnings\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.display import Image\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"..\/input\"))","acf7513d":"# although not related I will show some images to get an idea about where the most earthquakes occur\nglobal_earth_quakes = Image('..\/input\/earth-quake-images\/global_earth_quakes.jpg', width = 1000)\nglobal_earth_quakes","b7670f47":"#image showing nuclear plant locations and earth quake hot zones\nnuclear_plants_locations = Image('..\/input\/earth-quake-images\/earth_quakes_nuclear_p_locations.jpg')\nnuclear_plants_locations","7f6af517":"#reading the training file (warning: huge size) specify data types to save memory\n#I will be using garbage collection frequently to clear the memory\n\ndata_type = {'acoustic_data': np.int16, 'time_to_failure': np.float32}\ntrain = pd.read_csv('..\/input\/LANL-Earthquake-Prediction\/train.csv', dtype=data_type)\ntrain.head()","a8cc8f78":"#garbage collection\ngc.collect()","df46cdb7":"# plot to see the relation between given variable and target variable\n\ntrain_ad_sample_df = train['acoustic_data'].values[::1000]\ntrain_ttf_sample_df = train['time_to_failure'].values[::1000]\n\n#function for plotting based on both features\ndef plot_acc_ttf_data(train_ad_sample_df, train_ttf_sample_df, title=\"Acoustic data and time to failure: 1% sampled data\"):\n    fig, ax1 = plt.subplots(figsize=(12, 8))\n    plt.title(title)\n    plt.plot(train_ad_sample_df, color='r')\n    ax1.set_ylabel('acoustic data', color='r')\n    plt.legend(['acoustic data'], loc=(0.01, 0.95))\n    ax2 = ax1.twinx()\n    plt.plot(train_ttf_sample_df, color='g')\n    ax2.set_ylabel('time to failure', color='g')\n    plt.legend(['time to failure'], loc=(0.01, 0.9))\n    plt.grid(True)\n\nplot_acc_ttf_data(train_ad_sample_df, train_ttf_sample_df)\n","d4b81e99":"#delete the old frame\ndel train_ad_sample_df\ndel train_ttf_sample_df","4a104dc9":"#plot to show zoomed in view\n\ntrain_ad_sample_df = train['acoustic_data'].values[:6291455]\ntrain_ttf_sample_df = train['time_to_failure'].values[:6291455]\nplot_acc_ttf_data(train_ad_sample_df, train_ttf_sample_df, title=\"Acoustic data and time to failure: 1% of data\")\ndel train_ad_sample_df\ndel train_ttf_sample_df","2c09d009":"#garbage collection\ngc.collect()","cc8af6ba":"#lets create a function to generate some statistical features based on the training data\n# this is necessay as only one variable [acoustic data] is given to us in training set\n\ndef generate_features(X):\n    strain = []\n    strain.append(X.mean())\n    strain.append(X.std())\n    strain.append(X.min())\n    strain.append(X.max())\n    strain.append(X.kurtosis())\n    strain.append(X.skew())\n    strain.append(np.quantile(X,0.01))\n    strain.append(np.quantile(X,0.05))\n    strain.append(np.quantile(X,0.95))\n    strain.append(np.quantile(X,0.99))\n    strain.append(np.abs(X).max())\n    strain.append(np.abs(X).mean())\n    strain.append(np.abs(X).std())\n    return pd.Series(strain)","e99e5649":"# check the head\ntrain.head()","a2657cb7":"# lets apply feature generation function\n# also we will read the training file in chunks. chunk size specifies the number of rows which pandas will \n# read in one chunk\n\nc_s = 10 ** 6\ntrain = pd.read_csv('..\/input\/LANL-Earthquake-Prediction\/train.csv', iterator=True, chunksize= c_s, dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})\n\nX_train = pd.DataFrame()\ny_train = pd.Series()\nfor df in train:\n    ch = generate_features(df['acoustic_data'])\n    X_train = X_train.append(ch, ignore_index=True)\n    y_train = y_train.append(pd.Series(df['time_to_failure'].values[-1]))","99fb4fa6":"#describe the data\n\nX_train.describe()","d1e166f1":"#garbage collection\ngc.collect()","20d3ee4f":"# just a base line for cat boost\n# get the best score without hyper parameter tuning\n\nfrom catboost import CatBoostRegressor, Pool\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\n\n\ntrain_pool = Pool(X_train, y_train)\nm = CatBoostRegressor(iterations=10000, loss_function='MAE', boosting_type='Ordered')\nm.fit(X_train, y_train, silent=True)\nm.best_score_","f6e448fb":"# now lets try SVM with rbf kernel + grid search for hyper paramter tuning\n\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.model_selection import KFold\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\n\nfolds = KFold(n_splits= 5, shuffle= True, random_state= 101)\n\nparameters = [{'gamma': [0.001, 0.005, 0.01, 0.02, 0.05, 0.1],\n               'C': [0.1, 0.2, 0.25, 0.5, 1, 1.5, 2]}]\n               \n\nreg1 = GridSearchCV(SVR(kernel='rbf', tol=0.01), parameters, cv= folds, scoring='neg_mean_absolute_error')\nreg1.fit(X_train_scaled, y_train.values.flatten())\ny_pred1 = reg1.predict(X_train_scaled)\n\nprint(\"Best CV score: {:.4f}\".format(reg1.best_score_))\nprint(reg1.best_params_)","d51034ea":"#garbage collection\ngc.collect()","abc45afa":"#making the imports\n#TQDM is a progress bar library with good support for nested loops and Jupyter\/IPython notebooks.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom tqdm import tqdm","7183c80d":"#reading the training file with data types int16 and float32\n\ndata_type = {'acoustic_data': np.int16, 'time_to_failure': np.float32}\ntrain_data = pd.read_csv('..\/input\/LANL-Earthquake-Prediction\/train.csv', dtype=data_type)\ntrain_data.head()","0ceb07a4":"#garbage collection\ngc.collect()","54108cfe":"# making the training file ready to be fed into a NN\n\nrows = 150000\nsegments = int(np.floor(train_data.shape[0] \/ rows))\n\nX_train = pd.DataFrame(index = range(segments),dtype = np.float32,columns = ['mean','std','99quat','50quat','25quat','1quat'])\ny_train = pd.DataFrame(index = range(segments),dtype = np.float32,columns = ['time_to_failure'])","0138b01c":"# generating the features like mean\/std\/quantiles\n\nfor segment in tqdm(range(segments)):\n    x = train_data.iloc[segment*rows:segment*rows+rows]\n    y = x['time_to_failure'].values[-1]\n    x = x['acoustic_data'].values\n    X_train.loc[segment,'mean'] = np.mean(x)\n    X_train.loc[segment,'std']  = np.std(x)\n    X_train.loc[segment,'99quat'] = np.quantile(x,0.99)\n    X_train.loc[segment,'50quat'] = np.quantile(x,0.5)\n    X_train.loc[segment,'25quat'] = np.quantile(x,0.25)\n    X_train.loc[segment,'1quat'] =  np.quantile(x,0.01)\n    y_train.loc[segment,'time_to_failure'] = y","26ed1d3e":"#using standard scaler to scale the data\n\nscaler = StandardScaler()\nX_scaler = scaler.fit_transform(X_train)","3e94582e":"#garbage collection\ngc.collect()","bba38765":"#compiling the sequential model. Simple model with input shape 6 and activation function rectified linear\n# as it is a regression task so use Mean Absolute Error as measuring matrix\n# will use default optimizer adam\n\nmodel = Sequential()\nmodel.add(Dense(32,input_shape = (6,),activation = 'relu'))\nmodel.add(Dense(32,activation = 'relu'))\nmodel.add(Dense(32,activation = 'relu'))\nmodel.add(Dense(1))\nmodel.compile(loss = 'mae',optimizer = 'adam')","a009b5e9":"#train the model (30 epochs)\n#feed in the scaled training data\n\nmodel.fit(X_scaler,y_train.values.flatten(),epochs = 30)","43d79838":"#reading the submission file from input directory\n\nsub_data = pd.read_csv('..\/input\/LANL-Earthquake-Prediction\/sample_submission.csv',index_col = 'seg_id')","f1392ae1":"#building the test data frame using same columns as X_train\n\nX_test = pd.DataFrame(columns = X_train.columns,dtype = np.float32,index = sub_data.index)","9bbee811":"#feature generation for test data\n\nfor seq in tqdm(X_test.index):\n    test_data = pd.read_csv('..\/input\/LANL-Earthquake-Prediction\/test\/'+seq+'.csv')\n    x = test_data['acoustic_data'].values\n    X_test.loc[seq,'mean'] = np.mean(x)\n    X_test.loc[seq,'std']  = np.std(x)\n    X_test.loc[seq,'99quat'] = np.quantile(x,0.99)\n    X_test.loc[seq,'50quat'] = np.quantile(x,0.5)\n    X_test.loc[seq,'25quat'] = np.quantile(x,0.25)\n    X_test.loc[seq,'1quat'] =  np.quantile(x,0.01)","32237f70":"#garbage collect\ngc.collect()","09612dfb":"#scale the test data using pre-defined scaler\n\nX_test_scaler = scaler.transform(X_test)","ef7d8a3d":"#making the predictions\npred = model.predict(X_test_scaler)","d56b83a3":"sub_data.head()","e77bf9fc":"#import xgboost (we will use regressor)\n\nimport xgboost as xgb","d31e3e6f":"#use the same already scaled X and y from NN part\n\nxgb_model = xgb.XGBRegressor()\n\nxgb_model.fit(X_scaler,y_train.values)","4a8236c5":"#predictions without hyper paramter tuning\n\npred = xgb_model.predict(X_test_scaler)","448e267e":"# hyperparameter tuning with XGBoost (will take some time to run)\n\n# creating a KFold object with 3 splits\n\nfolds = KFold(n_splits= 3, shuffle= True, random_state= 101)\n\n# specify range of hyperparameters\nparam_grid = {'learning_rate': [0.01, 0.1, 0.2, 0.3], \n             'subsample': [0.3, 0.6, 0.9, 1],\n              'n_estimators' : [5, 10, 15, 20],\n              'max_depth' :[2,4,6,8]          \n             }          \n\n\n# specify model\nxgb_model = xgb.XGBRegressor()\n\n# set up GridSearchCV()\nmodel_cv = GridSearchCV(estimator = xgb_model, \n                        param_grid = param_grid, \n                        scoring='neg_mean_absolute_error', \n                        cv = folds, \n                        verbose = 1,\n                        return_train_score=True, \n                        n_jobs= -1)      ","be884093":"#train the model\nmodel_cv.fit(X_scaler,y_train.values)","0a38b517":"# printing the optimal accuracy score and hyperparameters\nprint('We can get neg_mean_absolute_error:',model_cv.best_score_,'using',model_cv.best_params_)","34aee7b1":"# define model with best paramters and train plus make predictions\n\nxgb_model = xgb.XGBRegressor(learning_rate= 0.2, max_depth= 4, n_estimators= 10, subsample= 0.9)\n\nxgb_model.fit(X_scaler,y_train.values)\n\npred = xgb_model.predict(X_test_scaler)\n\n# read the submission file and populate it with predictions\n\nsample_submission = pd.read_csv('..\/input\/LANL-Earthquake-Prediction\/sample_submission.csv')\nsample_submission['time_to_failure'] = pred\n\nprint(sample_submission.shape)\n\nprint('\\n')\n\nsample_submission.head()","2e1fbc3e":"#write to csv file\n\nsample_submission.to_csv('Final_EQ_sub.csv', index=False)","5e5921b5":"### Package Imports and EDA","b76fb5b3":"# Support Vector Machines","8ef67ca5":"# Neural Nets","6099c026":"We can see that there are series of little jumps in the acoustic data and then there is a big spike which is followed by failure event. ","1d949b19":"## Xgboost","f2ba6edf":"# Feature Enineering","d526228d":"XGBoost is well known to provide better solutions than other machine learning algorithms. In fact, since its inception, it has become the \"state-of-the-art\u201d machine learning algorithm to deal with structured data.\n\nFor hyper parameter tuning of xgboost go through the below link:\n\nhttps:\/\/towardsdatascience.com\/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e","8f840e11":"### Reading the training file","22120641":"## Data Description\n\nWe will use the (acoustic_data) input signal to predict the time remaining before the next laboratory earthquake (time_to_failure). Please note that all this data has been generated in lab. If you want to know how a laboratory earth quake happens then please check out the below youtube video.\n\nhttps:\/\/www.youtube.com\/watch?v=m_dBwwDJ4uo\n\nThe training data is a single, continuous segment of experimental data. The test data consists of a folder containing many small segments. The data within each test file is continuous, but the test files do not represent a continuous segment of the experiment; thus, the predictions cannot be assumed to follow the same regular pattern seen in the training file.\n\nFor each seg_id in the test folder, we have to predict a single time_to_failure corresponding to the time between the last row of the segment and the next laboratory earthquake.\n\n**Data Fields**\n\n* acoustic_data - the seismic signal [int16]\n* time_to_failure - the time (in seconds) until the next laboratory earthquake [float64]\n* seg_id - the test segment ids for which predictions should be made (one prediction per segment)","903f4826":"Go through the below link to get an idead about the NNs. \n\nhttps:\/\/towardsdatascience.com\/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9","de2e3e91":"# Catboost ","cede5dcc":"The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N \u2014 the number of features) that distinctly classifies the data points. It seems line it is for classification tasks but \nit can be used for regression also. \n\nMore details @\n\nhttps:\/\/towardsdatascience.com\/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47\n","c7c3ee13":"Will not submit this file. Lets wait for Xgboost with hyper parameter tuning to give us the best predictions. ","77b88bc6":"CatBoost is an algorithm for gradient boosting on decision trees. It is developed by Yandex researchers and engineers (Russian), and is used at Yandex for search, recommendation systems, personal assistant, self-driving cars, weather prediction and many other tasks. It is in open-source and can be used by anyone now.\n\nMore details @\n\nhttps:\/\/catboost.ai\/docs\/concepts\/about.html\n","a98ca8aa":"# Earth Quake Prediction\n\n## Overview\nEvery year many lives are lost and infastructure worh billions of dollars is destroyed in earh quakes. What if we could predict when an earh quake will occur so that this demage can be avoided or reduced.\n\nIn this project we predict the time remaining before laboratory earthquakes occurs from real-time seismic data.\n\nFor details about the kaggle challene please visit the below link;\n\nhttps:\/\/www.kaggle.com\/c\/LANL-Earthquake-Prediction","9b2ccb08":"As we can see in this heat map that most of the earth quakes happen in Asia Pacific and South America Region.","28ffd725":"From this image we can tell that nuclear plants located in Japan are at risk (if they get some demage in earth quake). Just as a precaution nuclear facilites should not be developed in high risk areas."}}