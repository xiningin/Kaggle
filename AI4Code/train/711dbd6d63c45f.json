{"cell_type":{"76cd03d9":"code","2f30cb19":"code","cf42b0d0":"code","829a4a5c":"code","d5f91f89":"code","92e5987d":"code","6acab98e":"code","372ecdd0":"code","547e27d5":"code","9cb0629a":"code","e7ef7837":"code","174521e4":"code","f18d0440":"code","88dadb90":"code","c5e697a6":"code","186c4a2e":"markdown","e8b20575":"markdown","f4321a8f":"markdown","1c87915f":"markdown","f8958bd6":"markdown","0e97d003":"markdown","d28c800b":"markdown","520a5630":"markdown","a576889b":"markdown","cbeece21":"markdown","43b7da0b":"markdown","4997d0f6":"markdown","e1cb51a8":"markdown","2a4fb2eb":"markdown"},"source":{"76cd03d9":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers import Adam \nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import models\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split","2f30cb19":"#%% Load data and reshape\ntrain = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nX_train = train.drop('label', axis = 1)\nY_train = train.label\nX_test = test\nX_train = X_train.values.reshape(-1, 28, 28, 1) \/ 255 \nY_train = to_categorical(Y_train.values, 10) # one-hot encoding\nX_test = X_test.values.reshape(-1, 28, 28, 1) \/ 255","cf42b0d0":"random_ints = np.random.randint(0, X_train.shape[0], size = 40) # to plot random images\nplt.figure(figsize=(15,6))\nfor i in range(40):  \n    plot = random_ints[i]\n    plt.subplot(4, 10, i+1)\n    plt.imshow(X_train[plot],cmap=plt.cm.binary)\n    plt.title(\"label=%d\" % np.argmax(Y_train[plot]),y=0.9)\n    plt.axis('off')\nplt.subplots_adjust(wspace=0.3, hspace=-0.1)\nplt.show()","829a4a5c":"#%% Define model and hyperparameters\nlr = 0.001 # initial learning rate\nepochs = 50 # number of passes through the data\ndropout = 0.2 # probability of dropout\nbatch_size = 64 # amount of data passed within epoch before updating weights\nvalidation_split = 0.1 # proportion of training data allocated to validation set\n\nmodel = models.Sequential()\nmodel.add(keras.layers.Conv2D(64, (5,5), activation ='relu', \n                     input_shape = (28,28,1), padding='same'))\nmodel.add(keras.layers.BatchNormalization())\n    \nmodel.add(keras.layers.Conv2D(64, (5,5), activation ='relu', \n                     padding='same'))\nmodel.add(keras.layers.BatchNormalization())\n    \nmodel.add(keras.layers.MaxPool2D(pool_size=(2,2)))\nmodel.add(keras.layers.Dropout(dropout))\n    \nmodel.add(keras.layers.Conv2D(64, (3,3), activation ='relu', \n                     padding='same'))\nmodel.add(keras.layers.BatchNormalization())\n    \nmodel.add(keras.layers.Conv2D(64, (3,3), activation ='relu', \n                     padding='same'))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(keras.layers.Dropout(dropout))\n    \nmodel.add(keras.layers.Conv2D(64, (3,3), activation ='relu', \n                     padding='same'))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Dropout(dropout))\n    \nmodel.add(keras.layers.Flatten())\nmodel.add(keras.layers.Dense(256, activation = \"relu\"))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Dropout(dropout))\n    \nmodel.add(keras.layers.Dense(10, activation = \"softmax\"))\n\nmodel.summary() # print model overview","d5f91f89":"datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    featurewise_center=False,\n    featurewise_std_normalization=False,\n    rotation_range=5,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    zoom_range=0.1,\n    horizontal_flip=False,\n    validation_split=0)\n    #,preprocessing_function  = add_noise) # can add your own preprocessing function here if needed, e.g. noise. I found noise didn't help a whole lot. ","92e5987d":"learning_rate_reduction = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","6acab98e":"es = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',min_delta=0.001, \n                               patience = 20, \n                               verbose=2,\n                               restore_best_weights = True)","372ecdd0":"opt = Adam(learning_rate=lr)\nmodel.compile(loss=\"categorical_crossentropy\", # note that instead of using one-hot encoding above, could use sparse_categorical_crossentropy here \noptimizer=opt,\nmetrics=['accuracy'])","547e27d5":"X_t, X_v, Y_t, Y_v = train_test_split(X_train, Y_train, \n                                              test_size = validation_split,\n                                              shuffle = True)","9cb0629a":"history = model.fit(datagen.flow(X_t, Y_t, batch_size=batch_size),\n        validation_data = (X_v, Y_v), \n        epochs = epochs,\n        steps_per_epoch=X_t.shape[0] \/\/ batch_size,\n        validation_steps = X_v.shape[0] \/\/ batch_size,\n        verbose=2, callbacks=[es, learning_rate_reduction])","e7ef7837":"fig, ax = plt.subplots(2,1)\nax[0].plot(history.history['accuracy'], label = 'training accuracy')\nax[0].plot(history.history['val_accuracy'], label = 'validation accuracy')\nax[0].set_ylim([0.95,1.0]) \nax[0].set_ylabel('accuracy')\n\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['loss'], label = 'training loss')\nax[1].plot(history.history['val_loss'], label = 'validation loss')\nax[1].set_ylim([0,0.1]) \nax[1].set_ylabel('loss')\n\nlegend = ax[1].legend(loc='best', shadow=True)\n\nax[1].set_xlabel('epoch')","174521e4":"Y_test = model.predict(X_test)\nY_test_classes = np.argmax(Y_test, axis = 1) ","f18d0440":"random_ints = np.random.randint(0, X_test.shape[0], size = 40)\nplt.figure(figsize=(15,6))\nfor i in range(40):  \n    plot = random_ints[i]\n    plt.subplot(4, 10, i+1)\n    plt.imshow(X_test[plot],cmap=plt.cm.binary)\n    plt.title(\"label=%d\" % Y_test_classes[plot],y=0.9)\n    plt.axis('off')\nplt.subplots_adjust(wspace=0.3, hspace=-0.1)","88dadb90":"Y_v_pred = model.predict(X_v)\nY_v_classes = np.argmax(Y_v_pred, axis = 1)\nwrong_class_index = np.argwhere((np.argmax(Y_v, axis = 1) - Y_v_classes) != 0)\n\nrandom_ints = np.random.randint(0, X_v[np.squeeze(wrong_class_index)].shape[0], size = 5)\nplt.figure(figsize=(15,6))\nfor i in range(5):  \n    plot = random_ints[i]\n    plt.subplot(4, 10, i+1)\n    plt.imshow(X_v[np.squeeze(wrong_class_index[plot])],cmap=plt.cm.binary)\n    plt.title(\"label=%d\" % np.argmax(Y_v_pred[np.squeeze(wrong_class_index[plot])]),y=0.9)\n    plt.axis('off')\nplt.subplots_adjust(wspace=0.3, hspace=-0.1)","c5e697a6":"submission = pd.DataFrame(Y_test_classes)\nsubmission.columns = ['label'] # change header\nsubmission['ImageId'] = submission.index + 1\nsubmission = submission[['ImageId', 'label']]\nsubmission.to_csv('submission.csv', index=False)","186c4a2e":"# Early stopping and return to best weights.","e8b20575":"# Visualise some of the data with their labels.","f4321a8f":"# Split the training data.","1c87915f":"# Out of interest, let's plot some of the incorrectly labelled data.","f8958bd6":"# Data augmentation.","0e97d003":"# Plot some data in the test set along with our predictions.","d28c800b":"# Define the model and hyperparameters.","520a5630":"# Submit our predictions on the test set.","a576889b":"# Adaptive learning rate.","cbeece21":"# Load the data and reshape for input into CNN.","43b7da0b":"# Fit the model.","4997d0f6":"# Compile the model.","e1cb51a8":"# Make model predictions on test set.","2a4fb2eb":"# Plot model performance metrics."}}