{"cell_type":{"604b346c":"code","e9699219":"code","a9ac35f6":"code","a1990291":"code","d1018a60":"code","e7887c0f":"code","5179cbf1":"code","7a6ba31b":"code","e5427c57":"code","278d9131":"code","e897c89d":"code","1b44ba7c":"code","6eda9b8b":"code","7787d9a9":"code","8b6d5e20":"code","22b3c85f":"code","20e1744d":"code","4fc4fbd9":"code","109ced1e":"code","d863c317":"code","55adb6a7":"code","0e42b2df":"code","f4c0b3a6":"code","fb3e4a51":"code","ffa4d62a":"code","04670784":"code","9730dc64":"code","4e07ff4e":"code","c432e57a":"code","ad539380":"code","b027ec44":"code","e44b9f68":"code","c39a4130":"code","4cad0ff8":"code","9da6c8c6":"code","e1006cdc":"markdown","80870a6a":"markdown","6b3be165":"markdown","6da5db51":"markdown","9b1b07fd":"markdown","103756dd":"markdown","82806fdf":"markdown","9f70a468":"markdown","f5a807d0":"markdown","04b126d8":"markdown","3a358a18":"markdown","064f17a7":"markdown"},"source":{"604b346c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport os\nfrom PIL import Image\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom IPython import display","e9699219":"# loading pre-trained model and weights\nbase_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')","a9ac35f6":"base_model.summary()","a1990291":"# Open the first image\nimg_1 = Image.open('..\/input\/mars-eiffel-deepdream\/mars.jpg')\nimg_1","d1018a60":"# opening second image\nimg_2 = Image.open('..\/input\/mars-eiffel-deepdream\/eiffel.jpg')\nimg_2","e7887c0f":"# Blend the two images\nimg_0 = Image.blend(img_1, img_2, alpha=0.5)# alpha --> The interpolation alpha factor. If alpha is 0.0, a copy of the first image is returned.\n                                                # If alpha is 1.0, a copy of the second image is returned.\nimg_0","5179cbf1":"Image.blend(img_1, img_2, 0.1)","7a6ba31b":"np.shape(img_1)","e5427c57":"type(img_1)","278d9131":"# Convert to numpy array\nsample_image = tf.keras.preprocessing.image.img_to_array(img_0)\n# Confirm that the image is converted to Numpy array\ntype(sample_image)","e897c89d":"# Obtain the max and min values\nprint('Maximum value of pixel {} & minimum value of pixel {}'.format(sample_image.max(), sample_image.min()))","1b44ba7c":"# Normalize the input image\nsample_image = np.array(sample_image)\/255.\n# verify normalized images values!\nprint('Maximum value of pixel {} & minimum value of pixel {}'.format(sample_image.max(), sample_image.min()))","6eda9b8b":"sample_image = tf.expand_dims(sample_image, axis=0)\nnp.shape(sample_image)","7787d9a9":"names = ['mixed3', 'mixed5', 'mixed8', 'mixed9']\n\nlayers = [base_model.get_layer(name).output for name in names]\n\n#create feature extracion model\ndeepdream_model = tf.keras.Model(base_model.input, layers)\ndeepdream_model.summary()","8b6d5e20":"# Let's run the model by feeding in our input image and taking a look at the activations \"Neuron outputs\"\nactivations = deepdream_model(sample_image)\nactivations","22b3c85f":"len(activations) # equal to the number of layers we selected","20e1744d":"sample_image.shape","4fc4fbd9":"# Since the cal_closs function includes expand dimension, let's squeeze the image (reduce_dims)\nsample_image = tf.squeeze(sample_image, axis=0)\nsample_image.shape","109ced1e":"#LOSS CALCULATION\n\n# REFERANCE: https:\/\/www.tensorflow.org\/tutorials\/generative\/deepdream\ndef calc_loss(image, model):\n    '''\n    Function will calculate loss function\n    it works by feedforwarding the input image through the network and generate activations\n    the obtain the average sum\n    '''\n    img = tf.expand_dims(image, axis=0) # converting image to bach format\n    layer_activation = model(img) #extracting activation results from model\n\n    print('ACTIVATION VALUES (LAYER OUTPUT) =\\n', layer_activation)\n    \n    losses = []\n    for act in layer_activation:\n        l = tf.math.reduce_mean(act) #calculate mean of each activation\n        losses.append(l)\n    \n    print('LOSSES (FROM MULTIPLE ACTIVAION LAYERS)= ',losses)\n    print('LOSSES SHAPE (FROM ALL ACTIVATION LAYERS)= ',np.shape(losses))\n    print('SUM OF ALL LOSSES (OF ALL LAYER)= ',tf.reduce_sum(losses))\n    \n    return tf.reduce_sum(losses)  #calculate sum\n\ncalc_loss(tf.Variable(sample_image), deepdream_model)","d863c317":"#CALCULATE THE GRADIENT\n\n##loss that has been calculated in the previous step and calculate the gradient with respect to the given input image and then\n   #add it to the input original image.\n##Doing so iteratively will result in feeding images that continiously and increasingly excite the neurons and generate more \n   #dreamy like images!\n    \n# When you annotate a function with tf.function, the function can be called like any other python defined function. \n# The benefit is that it will be compiled into a graph so it will be much faster and could be executed over TPU\/GPU\n@tf.function\ndef deepdream(model, image, step_size):\n    with tf.GradientTape() as tape:\n        # this need gradient relative to 'img'\n        # 'GradientTape' on;y watches 'tf.Variable' by default\n        tape.watch(image)\n        loss = calc_loss(image, model)   #calling function to caluclate loss\n        \n    # Calculate the gradient of the loss with respect to the pixels of the input image.\n    gradients = tape.gradient(loss, image)\n    print('GRADIENT =\\n', gradients)\n    print('GRADIENTS SHAPE =\\n', np.shape(gradients))\n    \n    # tf.math.reduce_std computes the standard deviation of elements across dimensions of a tensor\n    gradients \/= tf.math.reduce_std(gradients)\n    \n    # In `gradient ascent`, the \"loss\" is maximized so that the input image increasingly \"excites\" the layers.\n    image = image + gradients * step_size\n    image = tf.clip_by_value(image, -1,1) #normalize the image as addition may icrease the pixels values \n    \n    return loss, image","55adb6a7":"def deprocess(image):\n    image = 255*(image +1)\/2.\n    return tf.cast(image, tf.uint8)\n\ndef run_deepdream(model, image, steps=100, step_size=0.01):\n    # convert from unit8 to range expected by model\n    \n    image = tf.keras.applications.inception_v3.preprocess_input(image)\n    \n    var = image\n    alpha = 0.5\n    for s in range(steps):\n        loss, image = deepdream(model, image, step_size)\n        \n        if s%100==0:\n            plt.figure(figsize=(8,8))\n            plt.imshow(deprocess(image))\n            plt.title(\"STEP {}, LOSS {}\".format(s, loss))\n            plt.show()\n            #plt.title(\"STEP {}, LOSS {}\".format(s, loss))\n        \n    plt.figure(figsize=(8,8))\n    plt.imshow(deprocess(image))\n    plt.show()\n    \n    return deprocess(image)","0e42b2df":"img_0.save(\"img_0.jpg\", \"JPEG\", quality=80, optimize=True, progressive=True)","f4c0b3a6":"\nSample_Image = np.array(load_img('.\/img_0.jpg'))\n\ndream_img = run_deepdream(model=deepdream_model,\n                          image=Sample_Image,\n                          steps=1000,\n                          step_size=0.02\n                         )","fb3e4a51":"# name of folder\ndream_name = 'inception_dream'\npath = '.\/' + dream_name\n\nif not os.path.exists(path):\n    os.makedirs(path)\n\n# saving image in one dir to make video\nimg_0.save(path + '\/img_0.jpg', \"JPEG\", quality=80, optimize=True, progressive=True)","ffa4d62a":"# defining function again by removing imshow satatement\ndef run_deepdream(model, image, steps=100, step_size=0.01):\n    # convert from unit8 to range expected by model\n    image = tf.keras.applications.inception_v3.preprocess_input(image)\n    \n    for s in range(steps):\n        loss, image = deepdream(model, image, step_size)\n        \n        if s%100==0:\n            #plt.figure(figsize=(8,8))\n            #plt.imshow(deprocess(image))\n            #plt.show()\n            print(\"STEP {}, LOSS {}\".format(s, loss))\n            \n    #plt.figure(figsize=(8,8))\n    #plt.imshow(deprocess(image))\n    #plt.show()\n    \n    return deprocess(image)\n\n\n# This helper function loads an image and returns it as a numpy array of floating points\n\ndef load_image(filename):\n    image = Image.open(filename)\n    return np.float32(image)\n\ndef save_dream(model, path):\n    '''\n    this function will create the image and save it direcory of \n    creating video\n    '''\n    \n    # Blended image dimension\n    x_size = 910 # larger the image longer is going to take to fetch the frames \n    y_size = 605\n\n    #zoom the image\n    x_zoom = 3\n    y_zoom = 2\n        \n    for i in range(60):\n        img = load_image(path + '\/img_{}.jpg'.format(i))\n        \n        #chop off the edges of the image and resize it back to original shape\n        img = img[0+x_zoom:y_size-y_zoom, 0+y_zoom:x_size-x_zoom]\n        img = cv2.resize(img, (x_size, y_size))\n        \n        # adjust RGB values (not necessary just experimental)\n        img[:, :, 0] +=2 #red\n        img[:, :, 1] +=2 #green\n        img[:, :, 2] +=2 #blue\n        \n        # DEEP DREAM MODEL\n        img = run_deepdream(model=deepdream_model, image=img, steps=500, step_size=0.02)\n        \n        #clip the image\n        img = np.clip(img, 0., 255.)\n        img = img.astype(np.uint8)\n        res = Image.fromarray(img, mode='RGB')\n        \n        # the save generated image\n        res.save(path + '\/img_{}.jpg'.format(i+1))","04670784":"save_dream(deepdream_model, path)","9730dc64":"# creating dream video\n\ndef get_video(path, fname):\n    out = cv2.VideoWriter(fname, cv2.VideoWriter_fourcc(*'MP4V'), 8,(910, 605))\n    \n    # The frames per second value is depends on few important things\n    # 1. The number of frames we have created. Less number of frames brings small fps\n    # 2. The larger the image the bigger the fps value. For example, 1080 pixel image can bring 60 fps \n    i = 0\n    while True:\n        if os.path.isfile(path + '\/img_{}.jpg'.format(i+1)):\n            i +=1\n        # Figure out how long the dream is \n        else:\n            dream_length = i\n            break\n\n    for i in range(dream_length):\n        try:\n            img = os.path.join(path, 'img_{}.jpg'.format(i))\n            print(img)\n            frame = cv2.imread(img)\n            out.write(frame)\n        except Exception as e:\n            print(e)\n    out.release()","4e07ff4e":"get_video('.\/inception_dream', 'inceptiondream.mp4')","c432e57a":"display.YouTubeVideo(id='a_9dF6UUPbI', height=605, width=910)","ad539380":"base_model =  tf.keras.applications.DenseNet201(include_top=False, weights='imagenet')\nbase_model.summary()","b027ec44":"names = ['conv3_block8_concat', 'conv5_block26_concat', 'conv5_block28_concat']\n\nlayers = [base_model.get_layer(name).output for name in names]\n\n#create feature extracion model\ndense_model = tf.keras.Model(base_model.input, layers)\ndense_model.summary()","e44b9f68":"def run_deepdream(model, image, steps=100, step_size=0.01):\n    # convert from unit8 to range expected by model\n    image = tf.keras.applications.densenet.preprocess_input(image)\n    \n    for s in range(steps):\n        loss, image = deepdream(model, image, step_size)\n        \n        if s%100==0:\n            # plt.figure(figsize=(8,8))\n            # plt.imshow(deprocess(image))\n            # plt.title(\"STEP {}, LOSS {}\".format(s, loss))\n            # plt.show()\n            print(\"STEP {}, LOSS {}\".format(s, loss))\n            \n   # plt.figure(figsize=(8,8))\n   # plt.imshow(deprocess(image))\n   # plt.show()\n    \n    return deprocess(image)\n","c39a4130":"# name of folder\ndream_name = 'dense_dream'\npath = '.\/' + dream_name\n\nif not os.path.exists(path):\n    os.makedirs(path)\n\n# saving image in one dir to make video\nimg_0.save(path + '\/img_0.jpg', \"JPEG\", quality=80, optimize=True, progressive=True)\n\nsave_dream(dense_model, path)","4cad0ff8":"get_video('.\/dense_dream', 'densedream.mp4')","9da6c8c6":"display.YouTubeVideo(id='S2z_UO0L3Aw', height=605, width=910)","e1006cdc":"This Notebook shows implementaion of Google Deep Dream algorithm\n\n**CONTENTS**\n\n1. IMPORT MODEL WITH PRE-TRAINED WEIGHTS\n2.  VISUALIZATING IMAGE AND PRE-PROCESS IT!\n3. RUN THE PRETRAINED MODEL, SELECTING LAYERS AND EXPLOREING ACTIVATIONS\n4. intitution: UNDERSTANDING DEEPDREAM\n5. IMPLEMENT DEEP DREAM ALGORITHM USING INCEPTIONNET\n6. (VIDEO) APPLY DEEPDREAM TO GENERATE A SERIES OF IMAGES\n7. DEEPDREAM USING DENSENET","80870a6a":"Deep Dream is a computer vision program created by Google.\nUses a convolutional neural network to find and enhance patterns in images\nwith powerful AI algorithms.\nCreating a dreamlike hallucinogenic appearance in the deliberately\nover-processed images.\n\nWhen you look for shapes in the clouds, you\u2019ll often find things you see every day: Dogs, people, cars. It turns out that artificial \u201cbrains\u201d do the same thing. Google calls this phenomenon \u201cInceptionism,\u201d and it\u2019s a shocking look into how advanced artificial neural networks really are. ON trained upon, each CNN layer learns filters of increasing complexity. The first layers learn basic feature detection filters such as edges and corners. The middle layers learn filters that detect parts of objects \u2014 for faces, they might learn to respond to eyes and noses. The last layers have higher representations: they learn to recognize full objects, in different shapes and positions.\n\n* If we feed an image to CNN model it it starts detecting various featue in a image, the initial layers will detect low-level features like edges and shape\n* AS deepper as we go high level features will be detected like color, face , tree, buliding...\n* It is expermented that is a network is trained over say dog then it try to detect a dog in the image, or a plant or any face.\n* From here the idea of deep dream rises that what is we try to magnify what the network is detecting.\n* When we feed an image in to CNN, the neurons will fire up generate results, we call them activations\n* The deep dream algorithm works in the way that it will try to change the input image by making soome of these neurons fire more\n* We can select layers that we want to keep in the network & bhy the combination of different layers, intersting patterns will drawn in image\n* For Ex. if input a image of water to network which is trained on fish images, then network will to identify fishes in the image and will generate output of we magnify these activations. More patterns will apper in image.\n\n\n\"The final few layers assemble those into complete interpretations\u2014these neurons activate in response to very complex things such as entire buildings or trees,\u201d Google\u2019s engineers explain.\n\n\nNormally when an image is feed to network depending on the problem we select a loss function and try to minimize it which is called `gradient decend` and feed the image again, but in deep dream we try to maximize this loss (gradient ascend). Instead of changing model weights we are changinf the input\n\n\n**DEEP DREAM STEPS**\n1. First we train a deep netwok\n2. We select the layers we want ot keep in network\n3. Calculate the activations coming from them\n4. Calculte the gradient and loss of these activations \n5. Modify the input image by incresing these activation, thus enhancing the pattern\n6. Feed the obtained image to network again\n7. This process is repeated number of times\n\n![alt text](https:\/\/drive.google.com\/uc?id=1R7_C4r4vy2tqIB5Pi-ltyY2N_WC6jSYF)","6b3be165":"Final words i creted this dream using Inception and Dense Nets you can try any other model like VGG or MobileNet, or use your own model and train it on a dataset og your chosing and create awesome dreams. Can't wait to see other Notebooks\n\n* Your feedback in comments is much appreciated, Comment if you have any doubts or for inprovement\n* Please **UPVOTE if you LIKE this notebook**, it will keep me motivated\n\n**REFRENCES**\n* https:\/\/www3.cs.stonybrook.edu\/~cse352\/T12talk.pdf\n* https:\/\/www.topbots.com\/advanced-topics-deep-convolutional-neural-networks\/\n* https:\/\/wccftech.com\/nvidia-demo-skynet-gtc-2014-neural-net-based-machine-learning-intelligence\/\n* https:\/\/gizmodo.com\/these-are-the-incredible-day-dreams-of-artificial-neura-1712226908\n* https:\/\/ai.googleblog.com\/2015\/06\/inceptionism-going-deeper-into-neural.html\n* SuperDataScience","6da5db51":"# 5: IMPLEMENT DEEP DREAM ALGORITHM","9b1b07fd":"# 2: VISUALIZATING IMAGE AND PRE-PROCESS IT!","103756dd":"# 4: UNDERSTANDING DEEPDREAM","82806fdf":"# 1: IMPORT MODEL WITH PRE-TRAINED WEIGHTS","9f70a468":"# 6: (VIDEO) APPLY DEEPDREAM TO GENERATE A SERIES OF IMAGES","f5a807d0":"**BELOW ARE SOME BEAUTIFUL WORKS**\n![A](https:\/\/i.kinja-img.com\/gawker-media\/image\/upload\/c_fit,f_auto,g_center,pg_1,q_60,w_1465\/1302697548279136403.png)\n![aa](https:\/\/i.kinja-img.com\/gawker-media\/image\/upload\/c_fit,f_auto,g_center,pg_1,q_60,w_1465\/1302697548325886611.png)\n\n![vv](https:\/\/i.kinja-img.com\/gawker-media\/image\/upload\/c_fit,f_auto,g_center,pg_1,q_60,w_1315\/1302697548458347155.png)\n![aa](https:\/\/i.kinja-img.com\/gawker-media\/image\/upload\/c_fit,f_auto,g_center,pg_1,q_60,w_1465\/1302697548523125139.png)","04b126d8":"# 3: RUN THE PRETRAINED MODEL, SELECTING LAYERS AND EXPLOREING ACTIVATIONS\n\n* Inception network has multiple concatenated layers named `mixed` (refer model summary)\n* we can select any layer depending on the feature we want such as edges, shapes\n* deep layers generate more compleex features such as entire face, building or tree","3a358a18":"# 7: DEEPDREAM USING DENSENET","064f17a7":"I was unable to display video using `IPython.display.Video or HTML`, i had no other choice but to put the video on youtube. If any one of you figure it out please do comment"}}