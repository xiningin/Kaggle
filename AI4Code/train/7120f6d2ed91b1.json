{"cell_type":{"b36f6a2a":"code","54399748":"code","b0aad307":"code","400452c9":"code","e20039ad":"code","8532eaab":"code","c7924f6e":"code","23ea30f4":"code","e51afc61":"code","169ad653":"code","5f24f697":"code","cfed38c7":"code","4e362198":"code","c5be12fb":"code","7b2973fd":"code","1a07b16c":"code","19834e58":"code","ed12ddff":"code","1551cfe1":"code","b4ea3851":"code","f3a41f23":"code","85c9c565":"code","dd409df3":"code","14ee1ce2":"code","10b89d7f":"code","f8dc2539":"code","0000e6a2":"code","de8d84e9":"code","8217e12c":"code","11351258":"code","7828ef28":"code","b75c9d93":"code","a4bf729c":"code","3995efb6":"code","df71088f":"code","95eeb83c":"code","0c60860a":"code","a941f1e8":"code","bbfb2f42":"markdown","4ddc026f":"markdown","53d22d3a":"markdown","6e4e345d":"markdown","e3ea8c11":"markdown","ad8f45e7":"markdown","d096b270":"markdown"},"source":{"b36f6a2a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","54399748":"\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nimport sklearn\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\n%matplotlib inline\nfrom sklearn.datasets import *\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras import optimizers \n\n\n#\ud544\uc694\ud55c \ub77c\uc774\ube0c\ub7ec\ub9ac\ub4e4\uc744 import \ud558\uaca0\uc2b5\ub2c8\ub2e4.","b0aad307":"#Loading dataset\nwine = pd.read_csv('..\/input\/winequality-red.csv')","400452c9":"#Let's check the form of data\nwine.head()","e20039ad":"#Information about the data columns\nwine.info()","8532eaab":"\nfig = plt.figure(figsize = (15,9))\nsns.boxplot(x = 'quality', y = 'fixed acidity', data = wine)\n\nfig = plt.figure(figsize = (15,9))\nsns.boxplot(x = 'quality', y = 'volatile acidity', data = wine)\n\n\nfig = plt.figure(figsize = (15,9))\nsns.boxplot(x = 'quality', y = 'citric acid', data = wine)\n\nfig = plt.figure(figsize = (15,9))\nsns.boxplot(x = 'quality', y = 'residual sugar', data = wine)\n\nfig = plt.figure(figsize = (15,9))\nsns.boxplot(x = 'quality', y = 'chlorides', data = wine)\n\nfig = plt.figure(figsize = (15,9))\nsns.boxplot(x = 'quality', y = 'free sulfur dioxide', data = wine)\n\nfig = plt.figure(figsize = (15,9))\nsns.boxplot(x = 'quality', y = 'total sulfur dioxide', data = wine)\n\nfig = plt.figure(figsize = (15,9))\nsns.boxplot(x = 'quality', y = 'sulphates', data = wine)\n\nfig = plt.figure(figsize = (15,9))\nsns.boxplot(x = 'quality', y = 'alcohol', data = wine)\n\nfig = plt.figure(figsize = (15,9))\nsns.boxplot(x = 'quality', y = 'pH', data = wine)\n\n\nfig = plt.figure(figsize = (15,9))\nsns.boxplot(x = 'quality', y = 'density', data = wine)\n","c7924f6e":"# Quality \uc640 \uc694\uc18c\ubcc4 \uadf8\ub798\ud504\ub97c \uc2dc\uac01\ud654\ud568\uc73c\ub85c\uc368, \uc5b4\ub5a4 \uc694\uc18c\uac00 \uc0c1\uad00\uad00\uacc4\uac00 \uc788\ub294\uc9c0 \ud30c\uc545\ud574\ubcf4\uc558\uc2b5\ub2c8\ub2e4.\n# \uadf8 \uacb0\uacfc, \ubaa8\ub4e0 \uc694\uc18c\uac00 quality\uc5d0 \uc601\ud5a5\uc744 \uc8fc\ub294 \uac83\uc740 \uc544\ub2c8\ub77c\ub294 \uac78 \ud655\uc778\ud560 \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4.","23ea30f4":"#1 - Bad \/ 2 - Average \/ 3 - Excellent \uc73c\ub85c \ub4f1\uae09\uc744 \uc14b\uc73c\ub85c \ub098\ub204\uc5b4 \ubaa8\ub378\uc744 \ub9cc\ub4e4\uc5b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n#quality = 2,3 --> Bad\n#quality = 4,5,6 --> Average\n#quality = 7,8 --> Excellent\n#Create an empty list called Reviews\n\nwine_reviews = []\nfor i in wine['quality']:\n    if i >= 2 and i <= 4:\n        wine_reviews.append('1')\n    elif i >= 5 and i <= 6:\n        wine_reviews.append('2')\n    elif i >= 7 and i <= 8:\n        wine_reviews.append('3')\nwine['Grade'] = wine_reviews\n\nwine['Grade'].value_counts()\n\nsns.countplot(wine['Grade'])","e51afc61":"\"\"\"\n#Now seperate the dataset as response variable and feature variabes\nX = wine.drop('quality', axis = 1)\nX = X.drop('Grade', axis=1)\nX = X.drop('fixed acidity', axis=1)\nX = X.drop('residual sugar', axis=1)\nX = X.drop('density', axis=1)\nX = X.drop('pH', axis=1)\nX = X.drop('alcohol', axis=1)\ny = wine['Grade']\n\nprint(X.head)\nwine=X\n\"\"\"","169ad653":"\n\nX = wine.drop('quality',axis = 1)\nX = X.drop('Grade',axis=1)\ny = wine['Grade']\nprint(X.head)","5f24f697":"#view final data\nprint(X.columns)\nprint(X.head(10))\nprint(y.head(10))\n","cfed38c7":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(X)\n","4e362198":"#view the scaled features\nprint(X)","c5be12fb":"from sklearn.decomposition import PCA\npca = PCA()\nX_pca = pca.fit_transform(X)","7b2973fd":"#plot the graph to find the principal components\nplt.figure(figsize=(10,10))\nplt.plot(np.cumsum(pca.explained_variance_ratio_), 'ro-')\nplt.grid()","1a07b16c":"#AS per the graph, we can see that 8 principal components attribute for 90% of variation in the data. \n#we shall pick the first 8 components for our prediction.\npca_new = PCA(n_components=8)\nX_new = pca_new.fit_transform(X)","19834e58":"print(X_new)\nX_new = pd.DataFrame(X_new)\nprint(type(X_new))","ed12ddff":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X_new, y, test_size = 0.2,random_state = 42)","1551cfe1":"print(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)\nprint(type(x_train))","b4ea3851":"rfc = RandomForestClassifier(n_estimators=220)\nrfc.fit(x_train, y_train)\n","f3a41f23":"pred_rfc = rfc.predict(x_test)\n#Let's see how our model performed\nprint(classification_report(y_test, pred_rfc))\n\n#Confusion matrix for the random forest classification\nprint(confusion_matrix(y_test, pred_rfc))","85c9c565":"print(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)\nprint(type(x_train))\nprint(type(y_train))\nprint(type(x_test))\nprint(type(y_test))","dd409df3":"from sklearn.utils import resample\n\n# concatenate our training data back together\nX = pd.concat([x_train, y_train], axis=1)\nprint(X.head(10))\nprint(X.Grade.head(10))\n\n#X_new = pd.DataFrame(X_new)\n#mask = df['A'] == 'foo'\n#df.query('A == \"foo\"')\n# separate minority and majority classes\nBad = X.query('Grade == \"1\"')\n\nAverage = X.query('Grade == \"2\"')\nExcellent = X.query('Grade == \"3\"')\n\nprint(Bad.head(20))\nprint(Average.head(20))\nprint(Excellent.head(20))\n# upsample minority\nBad_upsampled = resample(Bad,\n                          replace=True, # sample with replacement\n                          n_samples=int(len(Average)), # match number in majority class\n                          random_state=42) # reproducible results\n\nExcellent_upsampled = resample(Excellent,\n                          replace=True, # sample with replacement\n                          n_samples=int(len(Average)), # match number in majority class\n                          random_state=42) # reproducible results\n\n\n# combine majority and upsampled minority\nupsampled = pd.concat([Average, Bad_upsampled,Excellent_upsampled])\nupsampled = upsampled.sample(frac=1)\n# check new class counts\n\nprint(type(upsampled))\nupsampled.Grade.value_counts()\nprint(upsampled)\n","14ee1ce2":"x_train = upsampled.drop('Grade',axis=1)\ny_train = upsampled['Grade']","10b89d7f":"rfc = RandomForestClassifier(n_estimators=220)\nrfc.fit(x_train, y_train)\n","f8dc2539":"pred_rfc = rfc.predict(x_test)\n#Let's see how our model performed\nprint(classification_report(y_test, pred_rfc))\n\n#Confusion matrix for the random forest classification\nprint(confusion_matrix(y_test, pred_rfc))","0000e6a2":"Bad = X.query('Grade == \"1\"')\n\nAverage = X.query('Grade == \"2\"')\nExcellent = X.query('Grade == \"3\"')\n","de8d84e9":"# downsample majority\nAverage_downsampled = resample(Average,\n                          replace=False, # sample with replacement\n                          n_samples=len(Bad), # match number in majority class\n                          random_state=42) # reproducible results\n\n\n# combine majority and upsampled minority\ndownsampled = pd.concat([Average_downsampled, Bad ,Excellent])\ndownsampled = downsampled.sample(frac=1)\n# check new class counts\n\nprint(type(downsampled))\ndownsampled.Grade.value_counts()\nprint(downsampled)","8217e12c":"x_train = downsampled.drop('Grade',axis=1)\ny_train = downsampled['Grade']","11351258":"rfc = RandomForestClassifier(n_estimators=220)\nrfc.fit(x_train, y_train)\n","7828ef28":"pred_rfc = rfc.predict(x_test)\n#Let's see how our model performed\nprint(classification_report(y_test, pred_rfc))\n\n#Confusion matrix for the random forest classification\nprint(confusion_matrix(y_test, pred_rfc))","b75c9d93":"Bad = X.query('Grade == \"1\"')\n\nAverage = X.query('Grade == \"2\"')\nExcellent = X.query('Grade == \"3\"')\n","a4bf729c":"# upsample minority\nBad_upsampled = resample(Bad,\n                          replace=True, # sample with replacement\n                          n_samples=(int(len(Average)*(1\/1.65))), # match number in majority class\n                          random_state=42) # reproducible results\n\nExcellent_upsampled = resample(Excellent,\n                          replace=True, # sample with replacement\n                          n_samples=(int(len(Average)*(1\/1.65))), # match number in majority class\n                          random_state=42) # reproducible results\n","3995efb6":"# downsample majority\nAverage_downsampled = resample(Average,\n                          replace=False, # sample with replacement\n                          n_samples=(int(len(Average)*(1\/1.65))), # match number in majority class\n                          random_state=42) # reproducible results\n","df71088f":"mixsampled = pd.concat([Bad_upsampled,Average_downsampled,Excellent_upsampled])\nmixsampled = mixsampled.sample(frac=1)","95eeb83c":"x_train = mixsampled.drop('Grade',axis=1)\ny_train = mixsampled['Grade']","0c60860a":"rfc = RandomForestClassifier(n_estimators=220)\nrfc.fit(x_train, y_train)\n","a941f1e8":"pred_rfc = rfc.predict(x_test)\n\n#Let's see how our model performed\nprint(classification_report(y_test, pred_rfc))\n\n#Confusion matrix for the random forest classification\nprint(confusion_matrix(y_test, pred_rfc))","bbfb2f42":"> \uacb0\uacfc\uac12... \ucd1d\uccb4\uc801 \ub09c\uad6d... \uc624\ud788\ub824 \uc548\ud558\ub290\ub2c8\ub9cc \ubabb\ud574\uc84c\ub2e4.\n\uc544\ub9c8 \ud6c8\ub828\uc2dc\ud0ac data set\uc758 \ud06c\uae30 \uc790\uccb4\uac00 \ub9e4\uc6b0 \ucd95\uc18c\ub418\uc5b4 \uc774\ub7f0 \ud604\uc0c1\uc774 \ubc8c\uc5b4\uc9c4 \ub4ef \ud558\ub2e4.\n\uc801\uc808\ud558\uac8c \uc870\uc728\ud560 \ubaa9\uc801\uc73c\ub85c- Randomsampling \uc744 over\uc640 under \ub97c \uc11e\uc5b4\uc11c \uc368\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.","4ddc026f":"> \ub530\ub77c\uc11c, \uc0dd\uac01\ud574\ubcfc \uc218 \uc788\ub294 \ud574\uacb0\ucc45\uc740 1) \ub2e4\ub978 \ubc29\ubc95\uc758 oversampling \uc744 \uc120\ud0dd\ud55c\ub2e4. 2) undersampling\uc744 \ud574\ubcf8\ub2e4 3) RandomForest \uac00 \uc544\ub2cc \ub2e4\ub978 \ubaa8\ub378\uc744 \uc801\uc6a9\ud574\ubcf8\ub2e4   ;; \uc815\ub3c4\ub85c \uc0dd\uac01\ud55c\ub2e4.\n\uc6b0\uc120\uc740, undersampling\uc744 \uc2dc\ub3c4\ud574\ubcf4\uaca0\ub2e4.","53d22d3a":"> \uc2e4\ud328 \uae30\ub85d : \uc721\uc548\uc73c\ub85c \ubcf4\uc558\uc744 \ub54c \uc601\ud5a5\uc744 \uc904 \uac83 \uac19\uc740 \uc694\uc18c\ub4e4\ub9cc \ud3ec\ud568\ud558\ub294 \uac83\uc73c\ub85c data \ub97c \uc870\uc815\ud588\uc5c8\ub294\ub370, \n\uadf8\uac78 \uc704\uc640 \uac19\uc774 drop \uc73c\ub85c \ud558\ub294 \uac8c \uc544\ub2c8\ub77c PCA \ub97c \uc774\uc6a9\ud558\ub294 \uac8c \ud6e8\uc52c \ub354 \uae54\ub054\ud558\uace0 \uc5c4\ubc00\ud558\ub2e4\ub294 \uac78 \uc54c\uac8c \ub418\uc5b4 \ucf54\ub4dc\ub97c \uc218\uc815\ud558\uc600\uc2b5\ub2c8\ub2e4.","6e4e345d":"**Now scale the data using StandardScalar for PCA**\nWhy? : \uc601\ud5a5\uc744 \uc8fc\ub294 \uc8fc \uc694\uc18c\uc758 \uac2f\uc218\uac00 \uc801\ub2e4\ub294 \uac78 \ud30c\uc545\ud588\uc73c\ubbc0\ub85c, \uc880 \ub354 \uac04\ub2e8\ud558\uac8c \ud558\uae30 \uc704\ud558\uc5ec!!","e3ea8c11":"> randomsampling \uc744 \uc11e\uc5b4\uc11c \uc368\uc8fc\ub2c8- \ud655\uc2e4\ud788 Grade 1\uc640 Grade 3\uc758 f1- score \uac00 \uc0c1\uc2b9\ud568\uc744 \ubcfc \uc218 \uc788\uc5c8\ub2e4. \n> Grade 3\ub294 \ub9e4\uc6b0 \uc18c\ud3ed \uc0c1\uc2b9\ud558\uc600\uc73c\uba70, Grade 1\uc758 \uc9c0\ud45c\uac00 \ud655\uc2e4\ud788 \uac1c\uc120\ub418\ub294 \uac78 \ud655\uc778\ud560 \uc218 \uc788\uc5c8\ub2e4. \n> \ud558\uc9c0\ub9cc, \uc544\uc9c1 \ub9ce\uc774 \ubd80\uc871\ud558\ub2e4. \uc5b4\ub5bb\uac8c \uc774 \ubb38\uc81c\ub97c \ud574\uacb0\ud560 \uc218 \uc788\uc744\uae4c?","ad8f45e7":"> 8\uac1c \uc815\ub3c4\uc758 \uc6d0\uc18c\ub4e4\ub85c \ub300\ubd80\ubd84 quality \uac00 \uc124\uba85 \uac00\ub2a5\ud558\ub2e4\ub294 \uac78 \uadf8\ub798\ud504\ub97c \ud1b5\ud574 \ud655\uc778\ud558\uc600\uc73c\ub2c8, \nPCA \ub85c \uc62e\uaca8\uc9c0\ub294 dim(X) \ub97c 8\ub85c \uc124\uc815\ud558\uaca0\uc2b5\ub2c8\ub2e4.","d096b270":"> \ubb38\uc81c\uc810 : f1-score \ub97c \ud1b5\ud574\uc11c oversampling\ud558\uc9c0 \uc54a\uc740 Randomforest \ubaa8\ub378\uacfc \ube44\uad50\ud558\uc5ec \uc774 \ubaa8\ub378\uc744 \ud3c9\uac00\ud558\uc600\uc744 \ub54c, \nGrade 3 (Excellent)\uac00 \uc18c\ud3ed \uc0c1\uc2b9\ud558\uace0 , Grade 1(Bad)\uac00 \uac80\ucd9c\ub418\uc5c8\uc73c\uba70,  Grade 2(Average) \uc758 \uacbd\uc6b0 \uac70\uc758 \ubcc0\ud654\ud558\uc9c0 \uc54a\uc558\uc74c\uc744 \uc54c \uc218 \uc788\uc5c8\ub2e4.\n\ub530\ub77c\uc11c \uc9c0\uae08 \uc6b0\ub9ac\ub294 \ub450 \uac00\uc9c0\uc758 \uac1c\uc120\ud560 \ubd80\ubd84\uc744 \uc0dd\uac01\ud574\ubcfc \uc218 \uc788\ub2e4 ::\n1)Oversampling \uc758 \ubc29\ubc95\ub860\uc774 \uc798\ubabb \ub418\uc5c8\ub2e4 ; oversampling \uc744 \uac70\ucce4\uc74c\uc5d0\ub3c4 data imbalance\ub97c \ud574\uacb0\ud558\uc9c0 \ubabb\ud558\uc5ec \ubb38\uc81c\uac00 \ubc1c\uc0dd\ud558\uc600\ub2e4. \/\n2)RandomForest \ubaa8\ub378\uc774 \uc801\uc808\ud558\uc9c0 \uc54a\uc740 \ubaa8\ub378\uc774\ub2e4 \n"}}