{"cell_type":{"ef421a05":"code","24d71e1a":"code","e4e805ca":"code","89f934e2":"code","de7cbb99":"code","d3ed8f23":"code","d7a9ddf9":"code","fa03e593":"code","653096d5":"code","0569247d":"code","a45f2a8a":"code","f2ad93f7":"code","9e6f0c94":"code","e3bd2d01":"code","0223991a":"code","ed046a80":"code","8eefc75c":"code","07da727b":"code","02a53729":"code","05ee4dce":"code","c3cf8f14":"code","4cf10c77":"code","0da51bf5":"code","584ff2b6":"code","ba0c2929":"code","7aac43de":"code","2ef257fa":"code","38b8f5d7":"code","6e4122a3":"code","26894704":"code","87da8046":"code","2592c46c":"code","62f34dbd":"code","8eab16bb":"code","32597070":"code","73219978":"code","069b4b17":"code","9fa2f040":"code","5e032596":"code","9ad84033":"code","1683f4f6":"code","6d9a229f":"markdown","f32f0012":"markdown","bb50a455":"markdown","bbcd90d6":"markdown","548b50ab":"markdown","31b08d56":"markdown","c8b0166b":"markdown","f1c12f29":"markdown","a9288a07":"markdown","85a3a57d":"markdown"},"source":{"ef421a05":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","24d71e1a":"import matplotlib.pyplot as plt","e4e805ca":"train=pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/test.csv')\nsubmit=pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/sample_submission.csv')","89f934e2":"train.describe(include='O')","de7cbb99":"train.describe()","d3ed8f23":"train.head()","d7a9ddf9":"train.Gender=train['Gender'].replace({'Male':1,'Female':0})\ntrain.Vehicle_Age=train['Vehicle_Age'].replace({'> 2 Years':3, '1-2 Year':2,'< 1 Year':1})\ntrain.Vehicle_Damage=train['Vehicle_Damage'].replace({'Yes':1,'No':0})","fa03e593":"train.head()","653096d5":"from sklearn.model_selection import train_test_split\nX_tr,X_test,y_tr, y_test=train_test_split(train[['Region_Code','Response', 'Policy_Sales_Channel']], train.Response, test_size=0.3, random_state=42)","0569247d":"X_tr.groupby('Region_Code')['Response'].mean().sort_values().index","a45f2a8a":"pc_ordered=X_tr.groupby('Policy_Sales_Channel')['Response'].mean().sort_values().index\npc_ordered","f2ad93f7":"ordered_labels=X_tr.groupby('Region_Code')['Response'].mean().sort_values().index","9e6f0c94":"pc_labels={k:i for i , k in enumerate(pc_ordered,0)}\npc_labels","e3bd2d01":"ordinal_label={k:i for i , k in enumerate(ordered_labels,0)}\nordinal_label","0223991a":"train['Region_Code_TE']=train.Region_Code.map(ordinal_label)","ed046a80":"train['Policy_Sales_Channel_TE']=train.Policy_Sales_Channel.map(pc_labels)","8eefc75c":"train.head()","07da727b":"train.Policy_Sales_Channel_TE.isnull().sum()","02a53729":"train[train.isna().any(axis=1)]","05ee4dce":"train=train.dropna()","c3cf8f14":"train.isnull().sum()","4cf10c77":"X=train.drop(['id','Response','Policy_Sales_Channel','Region_Code'], axis=1)\ny=train['Response']","0da51bf5":"# check version number\nimport imblearn\nprint(imblearn.__version__)\nfrom imblearn.under_sampling import NearMiss","584ff2b6":"undersample=NearMiss(version=3, n_neighbors=5)\nX,y=undersample.fit_resample(X,y)","ba0c2929":"y.value_counts()","7aac43de":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=21)","2ef257fa":"# Running XGBoost\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nimport xgboost\nimport sklearn\nparams={\n \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n    \n}","38b8f5d7":"clf=xgboost.XGBClassifier(objective='binary:logistic')\nrandom_search=RandomizedSearchCV(clf,param_distributions=params,n_iter=5,scoring='neg_brier_score',n_jobs=-1,cv=5,verbose=3)\nrandom_search.fit(X_train,y_train )\nbest_regressor3 = random_search.best_estimator_","6e4122a3":"random_search.best_estimator_","26894704":"pred_prob=best_regressor3.predict_proba(X_test)[:,1]\npred_prob","87da8046":"# Kernel density estimator\nimport seaborn as sns\nsns.kdeplot(pred_prob, label='prob density plot')","2592c46c":"from sklearn import metrics\n\n\nplt.figure(figsize=(8,6))\nplt.plot([0, 1], [0, 1],'r--')\n\npred = pred_prob\nlabel = y_test\nfpr, tpr, thresh = metrics.roc_curve(label, pred)\nauc = metrics.roc_auc_score(label, pred)\nplt.plot(fpr, tpr, label=f'XGB, auc = {str(round(auc,3))}')\n\n\nplt.ylabel(\"True Positive Rate\")\nplt.xlabel(\"False Positive Rate\")\nplt.title(\"AUC-ROC for two models\")\nplt.legend()\nplt.show()","62f34dbd":"\nfrom sklearn.calibration import calibration_curve\n\n\ndef plot_calibration_curve(name, fig_index, probs):\n    \"\"\"Plot calibration curve for est w\/o and with calibration. \"\"\"\n\n    fig = plt.figure(fig_index, figsize=(10, 10))\n    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n    ax2 = plt.subplot2grid((3, 1), (2, 0))\n    \n    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n    \n    frac_of_pos, mean_pred_value = calibration_curve(y_test, probs, n_bins=10)\n\n    ax1.plot(mean_pred_value, frac_of_pos, \"s-\", label=f'{name}')\n    ax1.set_ylabel(\"Fraction of positives\")\n    ax1.set_ylim([-0.05, 1.05])\n    ax1.legend(loc=\"lower right\")\n    ax1.set_title(f'Calibration plot ({name})')\n    \n    ax2.hist(probs, range=(0, 1), bins=10, label=name, histtype=\"step\", lw=2)\n    ax2.set_xlabel(\"Mean predicted value\")\n    ax2.set_ylabel(\"Count\")","8eab16bb":"plot_calibration_curve(\"XGB\", 1, pred_prob)","32597070":"pred=best_regressor3.predict(X_test)","73219978":"from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test, pred))","069b4b17":"confusion_matrix(y_test, pred)","9fa2f040":"import matplotlib.pyplot as plt\nimport scikitplot as skplt\npredicted_prob=best_regressor3.predict_proba(X_test)\nskplt.metrics.plot_cumulative_gain(y_test, predicted_prob)\nplt.show()","5e032596":"!pip install kds\nimport kds","9ad84033":"kds.metrics.report(y_test, predicted_prob[:,1],plot_style='ggplot')","1683f4f6":"kds.metrics.decile_table(y_test, predicted_prob[:,1])","6d9a229f":"## Target Encoding for Region Code and Policy Sales Channel","f32f0012":"Accuracy is a totally different conversation than calibration. We can have a perfectly accurate model that is not calibrated at all and, on the other hand, a model that is no better than random, which is perfectly calibrated nonetheless.Calibration seems pretty decent","bb50a455":"## Decile Charts","bbcd90d6":"## Constructing Gain Charts","548b50ab":"Also, keep in mind that the accuracy of the model might be lower after calibration. Thus, we can see that in some case we might have a trade-off between accuracy and calibration to consider. ","31b08d56":"## Model calibration","c8b0166b":"#### By selecting the top 75 percent (in order of their propensities) of the class 1 responders, we can get a cumulative gain of ~100%","f1c12f29":"#### Also the primary goal in our problem is to improve our customer base for vehicle insurance as well. Hence, we might want to focus on decreasing the type II error (Classifying a customer who could have reacted positively to our offer as negative). By excluding the top 20% of the non responders from the marketing campaign, we'll capture 40% of our non-responders and still be left with most of the customer base to market to.\n","a9288a07":"#### Calibrating the model\nThe two most popular methods of calibrating a machine learning model are the isotonic and Platt's method.\nScikit-learn provides a base estimator for calibrating models through the CalibratedClassifierCV class. For this example, we will use the Platt's method, which is equivalent to setting the method argument in the constructor of the class to sigmoid. If you want to use the isotonic method you can pass that instead. Since the calibration looks fine, not proceedind  with these methods.\n","85a3a57d":"We are \u201cbinning\u201d our respondents correctly from most likely to respond to least likely to respond. A model exhibiting a good staircase decile analysis is one you can consider moving forward with."}}