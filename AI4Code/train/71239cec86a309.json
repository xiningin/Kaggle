{"cell_type":{"59a259a6":"code","0d0f32aa":"code","3f080ef4":"code","61682cb1":"code","cc185935":"code","00c72854":"code","e1b5066a":"code","b8da97a8":"code","61e88097":"code","6facf7b8":"code","f598fcdf":"code","cc921961":"code","bfab38a9":"code","d18b1e3d":"code","cb1dea8d":"code","3472c6f5":"code","60b786b4":"code","9ea40e9c":"code","9314bc75":"code","f15b12a5":"code","22d9e89c":"code","0609a506":"code","edeb0038":"code","c9d71d61":"code","5187dea8":"code","cde11ec2":"code","0e699a93":"code","6592285d":"code","a84a5d3d":"code","fd39a60e":"code","455be593":"code","f961d31d":"code","30c3c7e8":"code","440e9c71":"code","1e5872b5":"code","cadd1348":"code","1b2a6009":"code","09a212d6":"code","8bc65a79":"code","1236e000":"code","7e44d16e":"code","6a5c15c0":"code","ae4afed3":"code","a8418e42":"code","f3bf6207":"code","1aa3794f":"code","69bbfd9a":"code","ccd99806":"code","58c56189":"code","535ff242":"code","e3ed0b43":"code","754b6acc":"code","ec001034":"code","971811ca":"code","d9852f5f":"code","6246422e":"code","536c7d65":"code","96ed6495":"code","a16c1df2":"code","ffc7e9c2":"code","893810fe":"code","df2f2f4a":"code","c31f6e8a":"code","ab7c15d4":"code","58c9a38a":"code","40dfa66b":"code","1e918526":"code","acc695b6":"code","eb3f515f":"code","f1192ae3":"code","7f7def1e":"code","09bcffe9":"code","c3a90ec2":"code","98ef3f52":"markdown","89910b0e":"markdown","7a37d5ab":"markdown","4dad752b":"markdown","3cc637f4":"markdown","57747dfb":"markdown","97f69a18":"markdown","df560c5e":"markdown","1afb99b8":"markdown","18e453d2":"markdown","00205887":"markdown","f24893d8":"markdown","739f90fd":"markdown","e103e917":"markdown","59f46930":"markdown","d9e42097":"markdown","630f14ee":"markdown","c999cc59":"markdown","8b085cb9":"markdown","f2aff5c8":"markdown","b7782734":"markdown","698da7bd":"markdown","394d9bbc":"markdown","42dc8a37":"markdown","7e4bd965":"markdown","accb2843":"markdown","8c739dea":"markdown","5354f4fc":"markdown","cba18ae2":"markdown","386732de":"markdown","ee353a40":"markdown","101ea750":"markdown","36e92506":"markdown","748a3e2d":"markdown","56728586":"markdown","033842a3":"markdown","464ff61c":"markdown","abe94112":"markdown","226627ac":"markdown","ad067fe1":"markdown","fae82cc5":"markdown","61616391":"markdown","7a01ce08":"markdown","58e986c6":"markdown"},"source":{"59a259a6":"!pip install arch","0d0f32aa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom datetime import datetime,timedelta\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.svm import SVC, LinearSVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder \nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.stattools import adfuller\nfrom arch import arch_model\n\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Bidirectional,TimeDistributed, RepeatVector, Input\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport time\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3f080ef4":"Q_data = pd.read_csv('\/kaggle\/input\/stock-market-wide-datasets\/000000000000Q',nrows=50000)\nAM_data = pd.read_csv('\/kaggle\/input\/stock-market-wide-datasets\/AM',nrows=50000)\nnews = pd.read_csv('\/kaggle\/input\/stock-market-wide-datasets\/news')\nevent = pd.read_csv('\/kaggle\/input\/stock-market-wide-datasets\/event')","61682cb1":"Q_data.head()","cc185935":"Q_data.describe()","00c72854":"AM_data.head()","e1b5066a":"AM_data.describe()","b8da97a8":"news.head()","61e88097":"event.head()","6facf7b8":"combined_data = AM_data.merge(Q_data,how='inner',left_on='symbol',right_on='ticker')\ncombined_data.drop(['time_y','symbol'],axis=1,inplace=True)","f598fcdf":"combined_data.head()","cc921961":"combined_data.columns","bfab38a9":"print(\"Unique tickers available in the dataset\",combined_data['ticker'].unique())","d18b1e3d":"AAL_data = combined_data[combined_data['ticker']=='AAL']","cb1dea8d":"AAL_data['hour'] = pd.to_datetime(AAL_data['time_x']).apply(lambda x:x.strftime('%H'))\nAAL_data['date'] = pd.to_datetime(AAL_data['time_x']).apply(lambda x:x.strftime('%Y-%m-%d'))","3472c6f5":"dependent_variables = ['volume','accumulated_volume','VWAP','open_price','high_price','low_price',\n                      'close_price','average_price','ask_price','ask_size','bid_price','bid_size']\none_hour_data = AAL_data[AAL_data['hour']=='15']\nmean_one_hour_data = one_hour_data.groupby('time_x').mean()[dependent_variables]\nmean_one_hour_data['time_x'] = list(mean_one_hour_data.index)\nmean_one_hour_data['time_x'] = pd.to_datetime(mean_one_hour_data['time_x']).apply(lambda x:x.strftime('%H:%M:%S'))","60b786b4":"plt.rcParams['figure.figsize'] = (20,30)\ncount = 1\nfor val in dependent_variables:\n    plt.subplot(4,3,count)\n    sns.lineplot(x='time_x',y=val,data=mean_one_hour_data)\n    plt.ylabel('Mean '+val)    \n    plt.xlabel('time')\n    plt.xticks(rotation=90)\n    count+=1\nprint()","9ea40e9c":"df = one_hour_data[dependent_variables]\nsns.pairplot(data = df)","9314bc75":"features = ['time_x','volume','accumulated_volume','VWAP','open_price','high_price','low_price','close_price',\n            'average_price','bid_price','bid_size','ask_price','ask_size','ticker']\ncorr_df = combined_data[:100000][features]\ncorr_df['hour'] = pd.to_datetime(corr_df['time_x']).apply(lambda x:int(x.strftime('%H')))\ncorr_df['weekday'] = pd.to_datetime(corr_df['time_x']).apply(lambda x:int(x.strftime('%w')))\ncorr_df['year'] = pd.to_datetime(corr_df['time_x']).apply(lambda x:int(x.strftime('%Y')))\ncorr_df['time'] = pd.to_datetime(corr_df['time_x']).apply(lambda x:x.strftime('%H:%M:%S'))","f15b12a5":"cat_features = corr_df['ticker'].unique()\nencoder = OneHotEncoder(categories=[cat_features])\ncorr_df[cat_features] = encoder.fit_transform(corr_df['ticker'].values.reshape(-1,1)).toarray()","22d9e89c":"plt.rcParams['figure.figsize'] = (20,10)\nsns.heatmap(corr_df.corr(),cmap='Blues',annot=True)","0609a506":"corr_df = corr_df.sort_values(by='time_x')","edeb0038":"selected_features = ['volume','accumulated_volume','VWAP','open_price','low_price','high_price','close_price',\n                     'average_price','bid_price','bid_size','ask_price','hour','weekday']\nX = corr_df[selected_features]\nY = corr_df[['AMD','AES','MO']]","c9d71d61":"X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.3,random_state=0)","5187dea8":"print(\"X_train shape: {}, Y_train shape: {}\".format(X_train.shape,Y_train.shape))\nprint(\"X_test shape: {}, Y_test shape: {}\".format(X_test.shape, Y_test.shape))","cde11ec2":"#Function to convert one-hot encoded output to label encoded output\ndef convert_label(val):\n    label_dict = {0:[1,0,0], 1:[0,1,0], 2:[0,0,1]}\n    result = 0\n    for i in label_dict:\n        if (label_dict[i]==val).all():\n            result = i\n    return result","0e699a93":"log_reg_clf = MultiOutputClassifier(LogisticRegression())\nlog_reg_clf.fit(X_train,Y_train)\nY_train_pred = log_reg_clf.predict(X_train) \nY_test_pred = log_reg_clf.predict(X_test)\nprint(\"Train accuracy\",accuracy_score(Y_train,Y_train_pred))\nprint(\"Test accuracy\",accuracy_score(Y_test,Y_test_pred))","6592285d":"Y_test_label = [convert_label(val) for val in Y_test.values]\nY_test_pred_label = [convert_label(val) for val in Y_test_pred]\nlabel_list = ['AMD','AES','MO']\nconfusion_matrix_df = pd.DataFrame(data = confusion_matrix(Y_test_label,Y_test_pred_label), \n                                   columns=label_list, index=label_list)\nsns.heatmap(confusion_matrix_df,cmap='Blues_r',annot=True)","a84a5d3d":"KNN_clf = MultiOutputClassifier(KNeighborsClassifier(7))\nKNN_clf.fit(X_train,Y_train)\nY_train_pred = KNN_clf.predict(X_train) \nY_test_pred = KNN_clf.predict(X_test)\nprint(\"Train accuracy\",accuracy_score(Y_train,Y_train_pred))\nprint(\"Test accuracy\",accuracy_score(Y_test,Y_test_pred))","fd39a60e":"Y_test_label = [convert_label(val) for val in Y_test.values]\nY_test_pred_label = [convert_label(val) for val in Y_test_pred]\nlabel_list = ['AMD','AES','MO']\nconfusion_matrix_df = pd.DataFrame(data = confusion_matrix(Y_test_label,Y_test_pred_label), \n                                   columns=label_list, index=label_list)\nsns.heatmap(confusion_matrix_df,cmap='Blues_r',annot=True)","455be593":"svc_clf = MultiOutputClassifier(SVC())\nsvc_clf.fit(X_train,Y_train)\nY_train_pred = svc_clf.predict(X_train) \nY_test_pred = svc_clf.predict(X_test)\nprint(\"Train accuracy\",accuracy_score(Y_train,Y_train_pred))\nprint(\"Test accuracy\",accuracy_score(Y_test,Y_test_pred))","f961d31d":"Y_test_label = [convert_label(val) for val in Y_test.values]\nY_test_pred_label = [convert_label(val) for val in Y_test_pred]\nlabel_list = ['AMD','AES','MO']\nconfusion_matrix_df = pd.DataFrame(data = confusion_matrix(Y_test_label,Y_test_pred_label), \n                                   columns=label_list, index=label_list)\nsns.heatmap(confusion_matrix_df,cmap='Blues_r',annot=True)","30c3c7e8":"directory = '\/kaggle\/input\/stock-market-wide-datasets\/AM'\nAM_data = pd.read_csv(directory)","440e9c71":"AAL_data = AM_data[AM_data['symbol']=='AAL']","1e5872b5":"AAL_data_ts = AAL_data[['time','open_price']]\nAAL_data_ts['time'] = pd.to_datetime(AAL_data_ts['time'])\nAAL_data_ts = AAL_data_ts.groupby('time').mean()","cadd1348":"plt.title('Time Series plot for AAL ticker')\nplt.plot(AAL_data_ts)\nplt.ylabel('mean open price')\nplt.xlabel('time')","1b2a6009":"def stationary_test(timeseries):\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)\nstationary_test(AAL_data_ts)","09a212d6":"train_ts = AAL_data_ts[:3501]\ntest_ts = AAL_data_ts[3501:]\nmodel = ARIMA(train_ts, order=(10,0,2))  \nresults_ARIMA = model.fit(disp=-1)  \n\n#Future Forecasting\nhistory = list(train_ts['open_price'])\npredictions = []\ntest_data = list(test_ts['open_price'])\nfor i in range(len(test_data)):\n    model = ARIMA(history, order=(10,0,2))\n    model_fit = model.fit(disp=-1)\n    output = model_fit.forecast()\n    yhat = output[0]\n    predictions.append(float(yhat))\n    obs = test_data[i]\n    history.append(obs)\nplt.rcParams['figure.figsize'] = (20,10)\nplt.plot(AAL_data_ts)\nplt.plot(results_ARIMA.fittedvalues, color='red')\nplt.plot(test_ts.index,predictions,color='green')\nplt.axvline(AAL_data_ts.index[3501],color='orange',dashes=(5,2,1,2))\nplt.legend(['actual values','predicted values for train data','predicted values for test data'])","8bc65a79":"print(results_ARIMA.summary())\n# plot residual errors\nresiduals = pd.DataFrame(results_ARIMA.resid)\nresiduals.plot(kind='kde')\nprint(residuals.describe())","1236e000":"plt.rcParams['figure.figsize'] = (10,5)\n#10 days rolling standard deviation\nAAL_data_ts.rolling(10).std().plot(style='b')\nplt.title('Moving Standard Deviation')\nplt.ylabel('Standard Deviation')\n#10 days rolling mean\nAAL_data_ts.rolling(10).mean().plot(style='r')\nplt.title('Moving Mean')\nplt.ylabel('Mean')","7e44d16e":"model = arch_model(AAL_data_ts, mean='ARX', vol='GARCH', p=10)","6a5c15c0":"model_fit = model.fit()","ae4afed3":"y = model_fit.forecast(horizon=30)\nplt.ylabel('variance')\nplt.xlabel('epochs')\nplt.plot(y.variance.values[-1],color='red')","a8418e42":"# split a univariate sequence into samples\ndef split_sequence(sequence, n_steps_in, n_steps_out):\n    X, y = [],[]\n    for i in range(len(sequence)):\n        end_ix = i + n_steps_in\n        out_end_ix = end_ix + n_steps_out\n        if out_end_ix > len(sequence):\n            break\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n    return np.array(X), np.array(y)","f3bf6207":"train_ts = AAL_data_ts['open_price'][:2001]\ntest_ts = AAL_data_ts['open_price'][2001:]\nX_train, Y_train = split_sequence(train_ts,3,1)\nY_train = np.squeeze(Y_train)\nX_train = X_train.reshape(X_train.shape[0],X_train.shape[1],1)","1aa3794f":"def r2_score(y_true, y_pred):\n    from tensorflow.keras import backend as K\n    SS_res = K.sum(K.square(y_true-y_pred))\n    SS_tot = K.sum(K.square(y_true-K.mean(y_true)))\n    return (1-SS_res\/(SS_tot+K.epsilon()))","69bbfd9a":"# define model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(3, 1)))\nmodel.add(LSTM(100, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse', metrics=[r2_score])\nmodel.summary()","ccd99806":"training = model.fit(X_train,Y_train,epochs=10)","58c56189":"history = list(train_ts)\npredictions = []\nfor i in range(len(test_ts)):\n    pred = np.squeeze(model.predict(np.array(history[-3:]).reshape(1,3,1)))\n    history.append(float(pred))\n    predictions.append(float(pred))","535ff242":"plt.plot(AAL_data_ts['open_price'])\nplt.plot(train_ts,color='red')\nplt.plot(test_ts.index,predictions,color='red')\nplt.axvline(AAL_data_ts.index[2001],color='orange',dashes=(5,2,1,2))\nplt.rcParams['figure.figsize'] = (25,10)","e3ed0b43":"model = Sequential()\nmodel.add(Input((3, 1)))\nmodel.add(Bidirectional(LSTM(100, activation='relu', return_sequences=True)))\nmodel.add(Bidirectional(LSTM(100, activation='relu')))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse', metrics=[r2_score])\nmodel.summary()","754b6acc":"training = model.fit(X_train,Y_train,epochs=10)","ec001034":"history = list(train_ts)\npredictions = []\nfor i in range(len(test_ts)):\n    pred = np.squeeze(model.predict(np.array(history[-3:]).reshape(1,3,1)))\n    history.append(float(pred))\n    predictions.append(float(pred))\nplt.plot(AAL_data_ts['open_price'])\nplt.plot(train_ts,color='red')\nplt.plot(test_ts.index,predictions,color='red')\nplt.axvline(AAL_data_ts.index[2001],color='orange',dashes=(5,2,1,2))\nplt.rcParams['figure.figsize'] = (25,10)","971811ca":"AAL_AM_data = AM_data[AM_data['symbol']=='AAL']\nAAL_AM_data = AAL_AM_data.groupby('time').mean()\nAAL_AM_data = AAL_AM_data.sort_values(by='time')\nAAL_AM_data_ts = AAL_AM_data['open_price']","d9852f5f":"model = ARIMA(AAL_AM_data_ts, order=(10,0,2))\ntime_diff = (datetime(2020,10,22)-datetime(2020,8,21)).days\nresults_ARIMA = model.fit(disp=-1)  \ndatetime_list = []\n#Future Forecasting\nhistory = list(AAL_AM_data_ts)\npredictions = []\nfor i in range(time_diff):\n    model = ARIMA(history, order=(10,0,2))\n    model_fit = model.fit(disp=-1)\n    datetime_list.append(str(pd.to_datetime(list(AAL_AM_data_ts.index)[-1])+timedelta(days=i+1)))\n    output = model_fit.forecast()\n    yhat = output[0]\n    predictions.append(float(yhat))\n    history.append(yhat)\nplt.rcParams['figure.figsize'] = (20,10)\nplt.plot(results_ARIMA.fittedvalues, color='red')\nplt.plot(datetime_list,predictions, color='green')\nplt.axvline(datetime_list[0],color='orange',dashes=(5,2,1,2))\nplt.legend(['actual values','forecasted values'])","6246422e":"print(\"Predicted value of AAL ticker 5 days prior to report date:\",predictions[-5])","536c7d65":"from sklearn.metrics import r2_score","96ed6495":"directory = '\/kaggle\/input\/stock-market-wide-datasets\/AM'\nAM_data = pd.read_csv(directory)","a16c1df2":"stock_list = list(set(AM_data.symbol))[:10]\ndf_list = []\nfor stock in stock_list:\n    stock_AM_data = AM_data[AM_data['symbol']==stock]\n    stock_AM_data = stock_AM_data.groupby('time').mean()\n    stock_AM_data = stock_AM_data.sort_values(by='time')\n    stock_AM_data['time'] = list(stock_AM_data.index)\n    stock_AM_data_ts = stock_AM_data['open_price']\n    stock_AM_data.index = range(len(stock_AM_data))\n    stock_news_data = news[news['stock']==stock]\n    stock_AM_data['time'] = pd.to_datetime(stock_AM_data['time']).apply(lambda x:x.strftime('%Y-%m-%d'))\n    stock_news_data['time'] = pd.to_datetime(stock_news_data['datetime']).apply(lambda x:x.strftime('%Y-%m-%d'))\n    stock_news_AM = stock_AM_data.merge(stock_news_data,how='inner',right_on='time',left_on='time')\n    stock_news_AM = stock_news_AM[['stock','average_price','summary']]\n    df_list.append(stock_news_AM)\nstock_news_AM = pd.concat(df_list)","ffc7e9c2":"corpus = list(stock_news_AM['summary'].apply(lambda x: x.lower()))","893810fe":"tfidf = TfidfVectorizer(stop_words={'english'},max_df=0.3,ngram_range=(5,5),min_df=7)\nX = tfidf.fit_transform(corpus).toarray()\nY = stock_news_AM['average_price']","df2f2f4a":"X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.5,random_state=0)","c31f6e8a":"lin_reg = LinearRegression()\nlin_reg.fit(X_train,Y_train)","ab7c15d4":"print(\"Train R2-score: \",r2_score(Y_train,lin_reg.predict(X_train)))\nprint(\"Test R2-score:\",r2_score(Y_test,lin_reg.predict(X_test)))","58c9a38a":"def create_keyword_weight_df(model):\n    keywords = np.array(sorted(tfidf.vocabulary_.items(), key=lambda x: x[1]))[:,0]\n    weights = model.coef_\n    keywords_weights = []\n    for k,w in zip(keywords,weights):\n        keywords_weights.append([k,w])\n    keywords_weights = pd.DataFrame(data = keywords_weights,columns=['keywords','weights'])\n    keywords_weights = keywords_weights.sort_values(by='weights',ascending=False)\n    return keywords_weights","40dfa66b":"keywords_weights = create_keyword_weight_df(lin_reg)\nkeywords_weights[:50]","1e918526":"keywords_weights[-50:]","acc695b6":"lin_svr = LinearSVR()\nlin_svr.fit(X_train,Y_train)","eb3f515f":"print(\"Train R2-score: \",r2_score(Y_train,lin_svr.predict(X_train)))\nprint(\"Test R2-score:\",r2_score(Y_test,lin_svr.predict(X_test)))","f1192ae3":"print(\"Top 50 documents with highest positive weights:\")\nkeywords_weights = create_keyword_weight_df(lin_svr)\nkeywords_weights[:50]","7f7def1e":"print(\"Top 50 documents with lowest negative weights:\")\nkeywords_weights[-50:]","09bcffe9":"from textblob import TextBlob\nstock_news_AM[['polarity', 'subjectivity']] = stock_news_AM['summary'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))","c3a90ec2":"stock_news_AM","98ef3f52":"### Ticker feature based statistical analysis on a one-hour interval","89910b0e":"It appears that documents with more positive weights express positive sentiments about the market like more tempting, effort to identify, patched critical etc.","7a37d5ab":"## EDA and Visualizations","4dad752b":"* Accuracy of AMD = 9700\/(9700+42) = 0.995\n* Accuracy of AES = 17000\/(17000+120) = 0.992\n* Accuracy of MO = 750\/(750+70+1900) = 0.275\n","3cc637f4":"### Relating to news dataset","57747dfb":"#### We will plot data for one hour of duration between 15:00 and 16:00.","97f69a18":"This function is used to display the model weights assigned to various documents which were helpful in making the average price predictions.","df560c5e":"### Using LSTM for time-series predictions\nHere, we will be using LSTM  as a time-series model to predict the open-price of the stock.","1afb99b8":"I am merging the two datasets AM and news based on the date of the stock average price and the date of the news given and then try to establish the impact of news on the everyday average price.","18e453d2":"* Accuracy of AMD = 9800\/(9800+0+0) = 1\n* Accuracy of AES = 18000\/(18000+0+0) = 1\n* Accuracy of MO = 27000\/(27000+0+0) = 1","00205887":"### Analyzing Movement of Ticker 5 days prior to report Date in Event Dataset.","f24893d8":"* One can observe that in the one hour duration between 15:00 and 16:00, the mean ask price remains saturated for the first 30 mins with more than 40 units and momentarily falls down to the low point for 3-4 mins and rises to same level of saturation. The mean bid price follows the same trend but in different range.\n\n* The trends observed in mean volume generated is interesting, reaching constant peaks and troughs in their respective neighbourhood and in general decreasing towards the lower value. \n\n* The trends observed in mean accumulated is increasing towards the peak of 1 units and then gradually decreasing to low point of 0.5 followed by sinusoid level of fluctuations between 0.5 and 0.8 units. \n\n* The plot associated with Mean VWAP, Mean open price, Mean close price, Mean high price, Mean low price, Mean average price is same in the one hour following the increasing at first and then sinusoidal pattern fluctuations within the same ranges.\n\n* The mean bid size and mean ask size remains constant throughout their one hour interval data.","739f90fd":"### Using Linear SVC","e103e917":"### Analyzing sentiments in detail using similarity and polarity\nWe will try to further analyze the sentiments behind the news using similarity and polarity. Here, Similarity indicates how much the news is biased towards his\/her personal opinions lies between 0 and 1 and Polarity indicates whether the sentiment is positive or negative and lies between -1 and 1. -1 -> negative sentiments, 1-> positive sentiments  \n","59f46930":"Bidirectional-LSTM doesn't offer any significant improvements in long distance predictions. ","d9e42097":"#### Let us select any one ticker like AAL for the sake of simplicity.","630f14ee":"* It appears that bid-price and ask price share a very strong correlation amongst the given pair-set of features.\n* Other features related to VWAP, Average Price, high price, low price, open price, close price, accumulated volume also show a significant positive correlation with each other. ","c999cc59":"### Plotting Residual Errors","8b085cb9":"### Using Logistic Regression","f2aff5c8":"### Heatmap to explore inter-dependence in detail\nWe will explore the collinearity in features on our data of 100,000 rows through Heatmap. ","b7782734":"### Vanilla LSTM","698da7bd":"#### One-hot Encoding Categorical Features","394d9bbc":"### Bidirectional LSTMs","42dc8a37":"### Using KNN Classifier","7e4bd965":"#### We will be merging the Q_data and AM_data using inner join wrt ticker and symbol columns for the analysis.","accb2843":"### Using Traditional ML algorithms\nWe will use Logistic Regression, Support Vector Classifier and KNN Classifier to identify the ticker based on the figures in the AM and Q dataset. We will make use of ML algorithms in detail while segregating news articles dataset to identify which tickers were impacted in pricing with the given news. ","8c739dea":"****CONCLUSION:**** So, we have analyzed the inter-dependency of various features in detail, made predictions using the ARIMA, LSTM, checked our model volatility using GARCH and created NLP model to relate news to the price predictions. I would keep learning and working with you with same level of enthusiasm, if given the intership opportunity. Any suggestions and improvements in the approach is most welcome. Cheers!   ","5354f4fc":"### Time-series Forecasting\nWe will be predicting the Time-Series using Auto-ARIMA for predicting the market opening price and then test the model using backfilling process. Let us do this forecasting for AAL ticker from the AM dataset.","cba18ae2":"### Using Linear Regression ","386732de":"### Pair-wise feature based statistical analysis on a one-hour interval AAL Data\nWe will be using the same interval between 15:00 and 16:00, one hour data as used above for the analysis. Pair-wise feature analysis excluding time will help in understanding the inter-dependency and possible associations within them.  ","ee353a40":"### Importing Required Libraries","101ea750":"### Checking Model Accuracy through Confusion Matrix","36e92506":"The 5 worded documents above shows that years and amount reported in the news like 2020, 16 milliarden dollar etc, have very important role in average price prediction with highly positive weights as they are frequently available in the corpus.","748a3e2d":"### Using GARCH model to estimate Model volatility through variance change in time series.","56728586":"### NLP model for news dataset relating it to stock and interval columns\nHere, we will be predicting average price interval based on the news of the given stock provided in the news data. I have only done for average price, the model can be build for predicting other intervals like open price, close price, bid price too.","033842a3":"### Supervised Learning","464ff61c":"A couple of negative sentiments about the market have been assigned negative weights by our model like bomb threat, searched on the tarmac etc. ","abe94112":"### Using Support Vector Classifier","226627ac":"Generally, ARIMA model works very well on stationary data, which has three conditions to be met:\n* Constant Mean\n* Constant Variance\n* The autocovariance of the time-series function at time t$_1$ and t$_2$ is only dependent between the interval t$_1$ and t$_2$.\n\nSo, we will run the augumented Dickey-Fuller Test here to check whether it is stationary.","ad067fe1":"So, we see that vanilla LSTM predicts the opening price values over a short distance. As the distance increases, the predictions becomes more unreliable. Let us try Bidirectional LSTMs to see whether it is any improvement over present model.   ","fae82cc5":"### Viewing Different Datasets","61616391":"It appears that transactional and numerical information related to money like 120 billion in 2020 and no. of people in agreement with some deal also have the significant impact on the daily average price reported.  ","7a01ce08":"* Accuracy of AMD = 6200\/(6200+3500+0) = 0.6391\n* Accuracy of AES = 17000\/(17000+120+0) = 0.9929\n* Accuracy of MO = 0\/(470+2200+0) = 0 = 0","58e986c6":"Results indicates that p-value is greater than 0.05. So, the null hypothesis of presence of unit root cannot be rejected. In other words, our ARIMA model won't give accurate predictions on the data due to being non-stationary. Here, p = 10 (based on no.of lags in ADF test), d = 0 (because at first glance on the dataset, difference is not needed due to non-zero mean), q = 2 (default value taken for auto ARIMA).   "}}