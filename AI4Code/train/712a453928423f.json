{"cell_type":{"26cc9e54":"code","70a5e27a":"code","546cc515":"code","10ebb777":"code","72675c1e":"code","b017c7b9":"code","57238c5f":"code","61426b79":"code","987d32a7":"code","a105812a":"code","8fcd9fd8":"code","1d1f58fb":"code","ef39a61e":"code","a43fa9cf":"markdown","04ab3db8":"markdown","57de3187":"markdown","9182a153":"markdown","7c8c1ee7":"markdown","362a51eb":"markdown","79c4a593":"markdown","937354ce":"markdown","d5a1ac75":"markdown","21b4f5bd":"markdown","6c25ef4a":"markdown"},"source":{"26cc9e54":"import os\nfrom IPython.display import display, HTML\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches","70a5e27a":"def return_paths(path_dir):\n    \"\"\" Return the paths, filenames of the image files, and also the unique IDs of each image-annotation combo.\n    \n    Parameters\n    ----------\n    path_dir : str\n        Path to the directory holding any, but here, the image files.\n        \n    Returns\n    -------\n    paths : list\n        The paths to each image file.\n    filenames : list\n        The file name of each image file.\n    ids : list\n        The unique ID of each image file and annotation file combo.\n    \"\"\"\n    paths = []\n    ids = []\n    \n    for (dirpath, _, filenames) in os.walk(path_dir):\n        for filename in filenames:\n            paths.append(os.path.join(dirpath, filename))\n            ids.append(filename.split(\".\")[0])\n    \n    return paths, filenames, ids\n\n\ndef build_annotation_paths_and_names(ids, path_annotations_dir):\n    \"\"\" Generate the paths and the file names to the annotation files. \n    \n    Parameters\n    ----------\n    ids : list\n        A list of the unique image and image annotation IDs.\n    path_annotations_dir : str\n        Path to the directory holding the annotation files.\n        \n    Returns\n    -------\n    paths_annotations : list\n        The paths to each annotation file.\n    names_annotations : list\n        The file name of each annotation file. \n    \"\"\"\n    paths_annotations = []\n    names_annotations = []\n    \n    for id_ in ids:\n        # annotation file name from id\n        name_annotation = str(id_) + \".txt\"\n        paths_annotations.append(os.path.join(path_annotations_dir, name_annotation))\n        names_annotations.append(name_annotation)\n    \n    return paths_annotations, names_annotations\n\n# kaggle specific paths\npath_images_dir = \"..\/input\/object-detection-batteries-dices-and-toy-cars\/dataset-new\/dataset\/images\"\npath_annotations_dir = \"..\/input\/object-detection-batteries-dices-and-toy-cars\/dataset-new\/dataset\/annotations\"\n\n# get the unique ID of each image file and annotation file combo, and the paths and file names of each image and annotation \npaths_images, names_images, ids = return_paths(path_images_dir)\npaths_annotations, names_annotations = build_annotation_paths_and_names(ids, path_annotations_dir)\n\nprint(f\"example image paths: \\n{paths_images[:5]}\")\nprint(f\"example annotation names: \\n{names_annotations[:5]}\")\nprint(f\"example ids: \\n{ids[:5]}\")","546cc515":"def parse_kitti_annotation(path_annotation, path_image, name_annotation, name_image):\n    \"\"\" Parse a the KITTI annotation into a DataFrame of a single image file.\n    \n    Parameters\n    ----------\n    path_annotation : str\n        Path to an annotation file.\n    path_image : str\n        Path to an image file.\n    name_annotation : str\n        The file name of an annotation file.\n    name_image : str\n        The file name of an image file.\n        \n    Returns\n    -------\n    annotation_df : pandas.DataFrame\n        The annotation of each object in the image as DataFrame.\n    \"\"\"\n    # extract id from annotation file name\n    id_ = name_annotation.split(\".\")[0]\n    sep = \" \"\n    names = [\"type\", \"truncated\", \"occluded\", \"alpha\", \n             \"bbox_left\", \"bbox_top\", \"bbox_right\", \"bbox_bottom\", \n             \"dimensions_height\", \"dimensions_width\", \"dimensions_length\",\n            \"location_x\", \"location_y\", \"location_z\",\n            \"rotation\", \"score\"]\n    \n    annotation_df = pd.read_csv(\n        filepath_or_buffer=path_annotation,\n        sep=sep,\n        names=names)\n    \n    annotation_df.insert(loc=0, column=\"id\", value=id_)\n    annotation_df.insert(loc=len(annotation_df.columns), column=\"path_annotation\", value=path_annotation)\n    annotation_df.insert(loc=len(annotation_df.columns), column=\"path_image\", value=path_image)\n    annotation_df.insert(loc=len(annotation_df.columns), column=\"name_annotation\", value=name_annotation)\n    annotation_df.insert(loc=len(annotation_df.columns), column=\"name_image\", value=name_image)\n    \n    return annotation_df\n    \n    \ndef parse_kitti_annotations(paths_annotations, path_images, annotation_names, image_names):\n    \"\"\" Parse a the KITTI annotation into a DataFrame of all of the image files in the dataset.\n    \n    Parameters\n    ----------\n    paths_annotations : list\n        Paths of the annotation files.\n    path_images : list\n        Paths of the image files.\n    annotation_names : list\n        The file names of the annotation files.\n    image_names : list\n        The file names of the image files.\n        \n    Returns\n    -------\n    df : pandas.DataFrame\n        The annotations of each object in each image in the dataset as DataFrame.\n    \"\"\"\n    objs = []\n    for path_annotation, path_image, name_annotation, name_image in zip(paths_annotations, path_images, names_annotations, names_images):\n        objs.append(parse_kitti_annotation(path_annotation, path_image, name_annotation, name_image))\n    df = pd.concat(objs=objs, ignore_index=True)\n    return df\n\n\ndf = parse_kitti_annotations(paths_annotations, paths_images, names_annotations, names_images)","10ebb777":"example_idx = 20\ndisplay(HTML(df[:example_idx].to_html()))","72675c1e":"w = df[\"bbox_right\"] - df[\"bbox_left\"]\nh = df[\"bbox_bottom\"] - df[\"bbox_top\"]\nsize = w * h\n\ntry:\n    df.insert(loc=list(df.columns).index(\"bbox_bottom\")+1, column=\"w\", value=w.values)\nexcept ValueError as e:\n    print(e)\n    \ntry:\n    df.insert(loc=list(df.columns).index(\"w\")+1, column=\"h\", value=h.values)\nexcept ValueError as e:\n    print(e)\n    \ntry:\n    df.insert(loc=list(df.columns).index(\"h\")+1, column=\"size\", value=size.values)\nexcept ValueError as e:\n    print(e)","b017c7b9":"def visualize_bboxes_in_image(group_df):\n    \"\"\" Visualize the bounding boxes of each object in an image.\n    \n    Parameters\n    ----------\n    group_df : pandas.DataFrame\n        A DataFrame of the annotations of each object in a single image.\n        \n    Returns\n    -------\n    None\n    \"\"\"\n    # path to image and open image\n    path_image = group_df[\"path_image\"].iloc[0]\n    im = Image.open(path_image)\n\n    # Create figure and axes\n    fig, ax = plt.subplots()\n    \n    # Display the image\n    ax.imshow(im)\n    \n    # lay the bounding boxes of objects on top of the image\n    for idx, g in group_df.iterrows():\n        type_ = g[\"type\"]\n        bbox_left = g[\"bbox_left\"]\n        bbox_top = g[\"bbox_top\"]\n        bbox_right = g[\"bbox_right\"]\n        bbox_bottom = g[\"bbox_bottom\"]\n        w = g[\"w\"]\n        h = g[\"h\"]\n        rect = patches.Rectangle((bbox_left, bbox_top), w, h, linewidth=2, edgecolor=\"r\", facecolor='none')\n        ax.add_patch(rect)\n        ax.text(bbox_left-10,bbox_top-10, type_, color='red', fontsize=10)\n    \n    plt.show()\n\n    \n# group by unique ID (one group is an image)\ngp = df.groupby(\"id\")\n\n# select randomly some images (by their unique ids)\nn_show = 10\nids_idx = np.random.choice(ids, n_show)\n\n# visualize the bounding boxes in the selected images\nfor idx in ids_idx:\n    display(HTML(gp.get_group(idx).to_html()))\n    visualize_bboxes_in_image(gp.get_group(idx))\n    ","57238c5f":"# object names and colour map\nobject_names = list(pd.unique(df[\"type\"]))\nprint(f\"objects: {', '.join(object_names)}\")\ncolor_map = {k:v for (k,v) in zip(object_names, [\"blue\", \"orange\", \"green\", \"red\", \"magenta\", \"brown\"])}\nprint(color_map)","61426b79":"# number of images and objects in the dataset\nn_images = pd.unique(df[\"id\"]).size\nprint(f\"The number of images in the dataset is {n_images}\")\n\nn_objects = df.shape[0]\nprint(f\"There are, in overall, {n_objects} objects in all of the {n_images} images\")","987d32a7":"n_objects_per_class = df[\"type\"].value_counts()\nax = n_objects_per_class.plot.bar(rot=0, figsize=(8,5))\nax.set_title(\"Frequency of objects per class in the dataset\")\nax.set_xlabel(\"Object class\")\nax.set_ylabel(\"Frequency of object class\")\nplt.show()","a105812a":"# get the frequency per class per image table\ndf_frequency = \\\n    df.groupby([\"type\", \"id\"], as_index=False).size().\\\n    groupby([\"type\", \"size\"], as_index=False).count().\\\n    rename(columns={\"size\":\"n_objects\", \"id\":\"count\"})\n\n# subplots for each class\nfig, axs = plt.subplots(nrows=len(object_names)\/\/2, ncols=2, figsize=(15,18))\nmax_n_objects = df_frequency[\"n_objects\"].max()\nmin_n_objects = df_frequency[\"n_objects\"].min()\nxticks = range(min_n_objects-1, max_n_objects)\nxtickslabels = range(min_n_objects, max_n_objects+1)\n\n# visualize the frequencies\nfor idx, object_name in enumerate(object_names):\n    ax = axs[idx\/\/2, idx%2]\n    df_frequency[df_frequency[\"type\"] == object_name].plot.bar(x=\"n_objects\", y=\"count\", rot=0, ax=ax, color=color_map[object_name])\n    ax.set_xticks(xticks)\n    ax.set_xticklabels(xtickslabels)\n    ax.set_xlabel(f\"Number of {object_name} objects per image\")\n    ax.set_ylabel(f\"Number of images\")\n    ax.grid()\n    ax.get_legend().remove()\n    ax.set_title(f\"The number of {object_name} objects in images\")\n    \nplt.show()","8fcd9fd8":"example_idx = \"00001459\"\ndisplay(HTML(gp.get_group(example_idx).to_html()))\nvisualize_bboxes_in_image(gp.get_group(example_idx))","1d1f58fb":"# based on: https:\/\/matplotlib.org\/stable\/gallery\/lines_bars_and_markers\/scatter_hist.html\n# location and spacing parameters of histograms\nleft, width = 0.1, 0.65\nbottom, height = 0.1, 0.65\nspacing = 0.04\n\n# histograms\nrect_scatter = [left, bottom, width, height]\nrect_histx = [left, bottom + height + spacing, width, 0.2]\nrect_histy = [left + width + spacing, bottom, 0.2, height]\n\n# figure\nfig = plt.figure(figsize=(12, 10))\n\n# add the scatter plot, and the histograms to the figure\nax = fig.add_axes(rect_scatter)\nax_histx = fig.add_axes(rect_histx, sharex=ax)\nax_histy = fig.add_axes(rect_histy, sharey=ax)\n\n# no labels\nax_histx.tick_params(axis=\"x\", labelbottom=False)\nax_histy.tick_params(axis=\"y\", labelleft=False)\n\n# plto the scatter plot, for each object class\nfor idx, object_name in enumerate(object_names):\n    x = df[df[\"type\"] == object_name][\"w\"].values\n    y = df[df[\"type\"] == object_name][\"h\"].values\n    ax.scatter(x, y, label=object_name, color=color_map[object_name])\n\n# bin width of histograms and limits\nbinwidth = 10.0\nxymax = max(np.max(np.abs(df[\"w\"])), np.max(np.abs(df[\"h\"])))\nlim = (int(xymax\/binwidth) + 1) * binwidth\nxmax = np.max(np.abs(df[\"w\"]))\nymax = np.max(np.abs(df[\"h\"]))\nxlim = (int(xmax\/binwidth) + 1) * binwidth\nylim = (int(ymax\/binwidth) + 1) * binwidth\n\n# bins of histograms\nxbins = np.arange(0, xlim + binwidth, binwidth)\nybins = np.arange(0, ylim + binwidth, binwidth)\n\n# plot the histograms\nax_histx.hist(df[\"w\"], bins=xbins, color=\"grey\")\nax_histy.hist(df[\"h\"], bins=ybins, orientation='horizontal', color=\"grey\")\nax_histx.set_xlabel(f\"Marginal frequency distribution of width [pixel]\")\nax_histy.set_ylabel(f\"Marginal frequency distribution of height [pixel]\")\n\n# legend, grid, and axis labels\nax.legend()\nax.grid()\nax.set_xlabel(\"Bounding box width [pixel]\")\nax.set_ylabel(\"Bounding box height [pixel]\")\n                 \nplt.show()","ef39a61e":"df_summary = df.groupby([\"type\"], as_index=False)[[\"w\", \"h\", \"size\"]].agg([\"mean\", \"min\", \"max\", \"std\"])\ndisplay(HTML(df_summary.to_html()))","a43fa9cf":"Let's visualize the frequency of objects per class in the dataset.","04ab3db8":"In overall, there are a minimum of 1 object of an arbitrary class per image. For instance, there are no images with 3 or more highlighters. There are more images with 5 dice than with 3 or 4.\n\nNote that there are many images with objects of more than one object class. For intance, the image with ID 00001459.","57de3187":"Finall, let's see the summary statistics of width, height, and size of bounding boxes per class, that summarizes the bivariate and marginal distributions (and their product as the bounding box size). The values below are all in pixels.","9182a153":"Check out some of the DataFrame.","7c8c1ee7":"Let's now visualize the bivariate and marginal distributions of bounding box heights and widths.","362a51eb":"Let's now visulaize the number of occurences of objects per class per image. That is, how many objects of each class are there in the images.","79c4a593":"## Visualizing the Objects in the Images <a class=\"anchor\" id=\"visualizing-the-objects-in-the-images\"><\/a>\n\nLet's visualize the bounding boxes laid on top of the objects in the images.","937354ce":"## Summary Statistics and EDA <a class=\"anchor\" id=\"summary-statistics-and-eda\"><\/a>\n\nLet's discover the number of objects in the dataset, their frequency per image, and the sizes of the bounding boxes.","d5a1ac75":"# Explore the Dataset\n\nThis notebook helps exlpore the dataset via eploratory data analysis (EDA). Enjoy and thanks for checking out my dataset!\n\n## Contents\n* [Introduction](#introduction)\n* [Importing the Dataset](#importing-the-dataset)\n* [Visualizing the Objects in the Images](#visualizing-the-objects-in-the-images)\n* [Summary Statistics and EDA](#summary-statistics-and-eda)\n\n\n## Introduction  <a class=\"anchor\" id=\"introduction\"><\/a>\n\nThe dataset was built in 2020 as part of the thesis project titled \"Real-Time Object Detection with Deep Learning on an Embedded GPU System\" by M\u00e1rk Antal Csizmadia that was submitted in partial fulfillment of the requirements for the degree of Bachelor of Engineering in Electronic Engineering at the University of Manchester, UK. The dataset is part of the public domain.\n\nThe annotated objects in the dataset include six-sided board game dice (dice), AAA, AA, and 9 V batteries (battery), toy cars (toycar), spoons (spoon), highlighters (highlighter), and tea candles (candle). The dataset was built through different means that included scraping images off the Internet with the [Bing Image Search API](https:\/\/www.microsoft.com\/en-us\/bing\/apis\/bing-image-search-api), remixing [existing datasets](https:\/\/public.roboflow.com\/object-detection\/dice) from the public domain, extracting video frames from videos downloaded from YouTube in line with its [fair-use policy](https:\/\/support.google.com\/youtube\/answer\/9783148?hl=en), and manually taking photographs.\n\nThere are in overall 1644 images in the dataset that contain 2815 objects. The distribution of the objects in the dataset are as shown in the table below.\n\n| class      | number of objects in dataset |\n| ----------- | ----------- |\n| battery      | 928       |\n| dice   | 895        |\n| toycar      | 755       |\n| candle   | 101        |\n| highlighter      | 90       |\n| spoon   | 46        |\n\nThe images were resized into 640 x 640 pixels and were padded to keep the original aspect ratio. The resized images were annotated using an [annotation tool](https:\/\/alpslabel.wordpress.com\/2017\/01\/26\/alt\/) published in the public domain. The annotation of the full dataset took around 5 weeks. This, unfortunately, should have been done as a pre-processing step before training an algorithm, but at the time when I built this dataset, I was not yet aware of that.\n\nThe specific labeling tool was selected since it produces annotation data in the [KITTI format](https:\/\/github.com\/bostondiditeam\/kitti\/blob\/master\/resources\/devkit_object\/readme.txt). The format defines a set of parameters for each object in each image that includes type, truncated, occluded, alpha, bbox, dimensions, location, rotation_y, and score. The type parameter describes the object type which can be one of \u201cdice\u201d, \u201ctoycar\u201d, \u201cbattery\u201d, \u201ccandle\u201d, \"spoon\", and \"highlighter\". The bbox parameter is an ordered set of four coordinates that define the top-left, and the bottom-right vertices of the ground-truth bounding box. The rest of the parameters are further described in the original source.\n\nUnfortunately, there are some missing annotations of the objects of interest, such as that in 00000331.jpg. This issue is not significant and does allow to train accurate object detection models.\n\nFirst thing first, let's import the packages needed to explore the dataset.","21b4f5bd":"## Importing the Dataset <a class=\"anchor\" id=\"importing-the-dataset\"><\/a>\n\nLet's import the images of the objects and their respective annotations. The annotations in the [KITTI format](https:\/\/github.com\/bostondiditeam\/kitti\/blob\/master\/resources\/devkit_object\/readme.txt) are parsed and inserted into a DataFrame.","6c25ef4a":"Add the widths, heights, and sizes (in pixels) of the bounding boxes to the DataFrame to make subsequent processing and visualizations easier."}}