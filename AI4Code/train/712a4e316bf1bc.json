{"cell_type":{"acb7dbd1":"code","a775729b":"code","72acf3fb":"code","bbad35dc":"code","55e7f6b3":"code","1e4ee2f9":"code","276a834c":"code","2cf421cc":"code","1df71298":"code","accf859b":"code","81e7514a":"code","976bf9af":"code","b26cd694":"code","b501a15a":"code","cdf7fe49":"code","a5c126b0":"code","b4f0addc":"code","e681c52d":"markdown","76c6290b":"markdown","ae2228fa":"markdown","edde5f2a":"markdown","8200def3":"markdown","96360a34":"markdown","b985016f":"markdown","638e0478":"markdown","0939e853":"markdown","b5a289f0":"markdown","832d562e":"markdown","f3f04ad2":"markdown","90f071d4":"markdown","ff73744e":"markdown","96c9d72c":"markdown","8ed53e3b":"markdown"},"source":{"acb7dbd1":"# Some imports :)\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport re\nfrom tqdm.notebook import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import iplot\nfrom wordcloud import WordCloud\nfrom plotly.offline import iplot\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nplt.style.use('classic')\nsns.set_palette(sns.color_palette('winter_r'))","a775729b":"def cprint(string:str, end=\"\\n\"):\n    \"\"\"\n    A little utility function for printing and stuff\n    \"\"\"\n    _pprint(f\"[black]{string}[\/black]\", end=end)","72acf3fb":"# Importing data\ntraining_file = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest_file = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")","bbad35dc":"training_file.head()","55e7f6b3":"test_file.head()","1e4ee2f9":"submission.head()","276a834c":"plt.figure(figsize=(8, 8))\nplt.title(f\"Target Column Distribution\")\nsns.histplot(training_file['target'], stat='density')\nsns.kdeplot(training_file['target'], color='blue')\nplt.axvline(training_file['target'].mean(), color='red', linestyle='--', linewidth=0.8)\nmin_ylim, max_ylim = plt.ylim()\nplt.text(training_file['target'].mean()*1.05, max_ylim*0.96, 'Mean (\u03bc): {:.2f}'.format(training_file['target'].mean()))\nplt.xlabel(\"Value\")\nplt.ylabel(\"Density\")\nplt.show()","2cf421cc":"plt.figure(figsize=(8, 8))\nplt.title(f\"Standard Error Column Distribution\")\nsns.histplot(training_file['standard_error'], stat='density')\nsns.kdeplot(training_file['standard_error'], color='magenta')\nplt.axvline(training_file['standard_error'].mean(), color='red', linestyle='--', linewidth=0.8)\nmin_ylim, max_ylim = plt.ylim()\nplt.text(training_file['standard_error'].mean()*1.05, max_ylim*0.96, 'Mean (\u03bc): {:.2f}'.format(training_file['standard_error'].mean()))\nplt.xlabel(\"Value\")\nplt.ylabel(\"Density\")\nplt.show()","1df71298":"char_freq_count = training_file['excerpt'].str.len()\n\nplt.figure(figsize=(8, 8))\nplt.title(f\"Character Frequency Count\")\nsns.histplot(char_freq_count, stat='density', color=\"#00a9ff\")\nsns.kdeplot(char_freq_count, color='purple')\nplt.axvline(np.mean(char_freq_count), color='red', linestyle='--', linewidth=0.8)\nmin_ylim, max_ylim = plt.ylim()\nplt.text(np.mean(char_freq_count)*0.64, max_ylim*0.96, 'Mean (\u03bc): {:.2f}'.format(np.mean(char_freq_count)))\nplt.xlabel(\"Count\")\nplt.ylabel(\"Density\")\nplt.show()","accf859b":"word_count = training_file['excerpt'].str.split().map(lambda x: len(x))\n\nplt.figure(figsize=(8, 8))\nplt.title(f\"Word Count Distribution\")\nsns.histplot(word_count, stat='density', color=\"magenta\")\nsns.kdeplot(word_count, color='purple')\nplt.axvline(np.mean(word_count), color='red', linestyle='--', linewidth=0.8)\nmin_ylim, max_ylim = plt.ylim()\nplt.text(np.mean(word_count)*0.86, max_ylim*0.93, 'Mean (\u03bc): {:.2f}'.format(np.mean(word_count)))\nplt.xlabel(\"Word Count\")\nplt.ylabel(\"Density\")\nplt.show()","81e7514a":"unq_word_count = training_file['excerpt'].apply(lambda x: len(set(str(x).split()))).to_list()\n\nfig = ff.create_distplot([unq_word_count], ['Excerpt Text'])\nfig.update_layout(title_text=\"Unique Word Count Distribution\")\niplot(fig)","976bf9af":"# Generate WordCloud\nwords = \" \".join(training_file['excerpt'].tolist())\nwc = WordCloud(width = 5000, height = 4000, background_color ='black', min_font_size = 10).generate(words)\n\n# Plot it\nplt.figure(figsize = (12, 12), facecolor = 'k', edgecolor = 'k' ) \nplt.imshow(wc) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0)\n\nplt.show()","b26cd694":"# Split the data roughly\ndata = training_file[['excerpt', 'target']]\ndata = data.sample(frac=1).reset_index(drop=True)\nexcerpt, targets = training_file['excerpt'].values, training_file['target'].values\n\nt_X, v_X = excerpt[:2750], excerpt[2750:]\nt_Y, v_Y = targets[:2750], targets[2750:]\n\nprint(t_X.shape, v_X.shape)\nprint(t_Y.shape, v_Y.shape)","b501a15a":"# Make an Sklearn pipeline for this Ridge Regression\nbackbone_ridge = Ridge(fit_intercept=True, normalize=False)\npipeline_ridge = make_pipeline(\n    TfidfVectorizer(binary=True, ngram_range=(1, 1)),\n    backbone_ridge\n)\n\n# Do training\npipeline_ridge.fit(t_X, t_Y)\n\n# Evaluate the performance on validation set\npreds = pipeline_ridge.predict(v_X)\nmse_loss = mean_squared_error(v_Y, preds)\n\nprint(f\"MSE Loss using Ridge and TfIdfVectorizer: {mse_loss}\")","cdf7fe49":"# Make an Sklearn pipeline for this Linear Regression\nbackbone_linear = LinearRegression(fit_intercept=True, normalize=False)\npipeline_linear = make_pipeline(\n    TfidfVectorizer(binary=True, ngram_range=(1, 1)),\n    backbone_linear\n)\n\n# Do training\npipeline_linear.fit(t_X, t_Y)\n\n# Evaluate the performance on validation set\npreds = pipeline_linear.predict(v_X)\nmse_loss = mean_squared_error(v_Y, preds)\n\nprint(f\"MSE Loss using Linear Regression and TfIdfVectorizer: {mse_loss}\")","a5c126b0":"# Weights for blending later\nlin_wgt = 0.2\nrig_wgt = 0.8","b4f0addc":"# Get the testing file\ntest = test_file[['id', 'excerpt']]\ntest_ids = test['id'].tolist()\ntest_text = test['excerpt'].values\n\n# Do Predictions on testing set\ntest_preds_ridge = pipeline_ridge.predict(test_text)\ntest_preds_linear = pipeline_linear.predict(test_text)\n\n# Form a submissions file and save it\nsubmission = pd.DataFrame()\nsubmission['id'] = test_ids\nsubmission['target'] = (test_preds_ridge + test_preds_linear) \/ 2\nsubmission.to_csv(\"submission.csv\", index=None)","e681c52d":"**If you found this notebook helpful, you can leave a vote!**\n\n\ud83d\udccc**PyTorch BERT Multi-Model Trainer + KFolds\ud83c\udfaf (Training Notebook) - https:\/\/www.kaggle.com\/heyytanay\/pytorch-bert-multi-model-trainer-kfolds**\n\n\ud83d\udccc**Vanilla PyTorch BERT Starter (Submission Notebook)\ud83c\udfaf - https:\/\/www.kaggle.com\/heyytanay\/submission-nb-vanilla-pytorch-bert-starter**\n\n## What is this Competition about? \ud83e\udd37\u200d\u2642\ufe0f\n\n* In this competition, we are required to build algorithms to rate the complexity of reading passages for grade 3-12 classroom use.\n\n* To accomplish this, we'll have to pair our machine learning skills with a dataset that includes readers from a wide variety of age groups and a large collection of texts taken from various domains. Winning models will be sure to incorporate text cohesion and semantics.\n\n* If successfull, students will benefit from feedback on the complexity and readability of their work, making it far easier to improve essential reading skills.\n\n<hr>","76c6290b":"The `target` columns looks very much normally distributed.","ae2228fa":"## Analysis on Excerpt Columns (`excerpt`) \ud83c\udfaf\n\nLet's now do some text related analysis on `excerpt` column and see how the text in it is structured.","edde5f2a":"*The above word count distribution can help us identify what `max_len` should we use when training our models*","8200def3":"## And what is our task? \ud83c\udfaf\n\nIn technical terms,\n\n* Given a `training.csv` file in which we will have (among other) 2 columns: `excerpt` and `target`, we will have to train Machine Learning model(s) that can approximate the relationship between excerpt and the target.\n\n* In simple words, we will have to train a Model which can predict the target value given a text excerpt.\n\n* This can be formulated as a Regression problem with text\n\n<hr>","96360a34":"### Character Frequency Count","b985016f":"<h2>Evaluation Metrics \ud83d\udd8a<\/h2>\n\nSubmissions are scored on the root mean squared error. RMSE is defined as:\n\n$$RMSE = \\sqrt{\\frac{1}{n}\\Sigma_{i=1}^{n}{\\Big(\\frac{y_i - \\hat{y_i}}{\\sigma_i}\\Big)^2}}$$\n\nwhere $y_i$ is the predicted value, $\\hat{y_i}$ is the original value, and $n$ is the number of rows in the test data.","638e0478":"### Word Count Distribution\n\n","0939e853":"## Modelling\n\nLet's try some basic modelling and then make a submission using that!","b5a289f0":"<h1 style=\"color:blue\"><center>CommonLit Readability Prize Competition<\/center><\/h1>\n<h3><center>What's it's all about?<\/center><\/h3>","832d562e":"## Peeking at the `standard_error` column \ud83d\udc40\n\nLet's take a look at the `standard_error` column to see how it looks like.","f3f04ad2":"### WordCloud","90f071d4":"## Files and what they contain \ud83d\udcc2\n\nIn this competition, we are provided with 3 files:\n\n* \ud83d\udcc4 `train.csv` :  This is the main training file, it consists of 6 columns: `id`, `excerpt`, `license`, `url_legal`, `target`, `standard_error`.\n    \n* \ud83d\udcc4 `test.csv` :  This is the testing file, it consists of 4 columns: `id`, `url_legal`, `license`, `excerpt`\n    \n* \ud83d\udcc4 `sample_submission.csv` :  This is a sample submission file that guides us how to form our submission file during inference\n\n\n<hr>","ff73744e":"<hr>\n<h2> EDA time! \ud83d\udcca <\/h2>\n\nEnough talking, let's do some light EDA!","96c9d72c":"## Peeking at the `target` column \ud83d\udc40\n\nLet's take a look at the target column to see how it is distributed.","8ed53e3b":"### Unique Word Count"}}