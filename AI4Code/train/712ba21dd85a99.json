{"cell_type":{"cae59f27":"code","180b2410":"code","12ac8531":"code","f4850acf":"code","fe3a6f8f":"code","bb168edb":"code","5168549f":"code","95467021":"code","440c07d1":"code","a89922f6":"code","c9cdd4a5":"code","34fa4a96":"code","17ca3e08":"code","ea60b229":"code","80cbb52a":"code","2461277a":"code","43c84943":"code","6955404b":"code","d6fbeacc":"code","aa7e5f96":"code","c3697632":"code","4bdf874f":"code","1ba8e91e":"code","3b81da09":"code","a5315f61":"code","1db18333":"code","4b16de65":"code","27aff6cf":"code","511ef5b6":"code","492f6f1a":"code","17556092":"code","fad6c59e":"markdown","4cb71221":"markdown","2689a82b":"markdown","448ecd2e":"markdown","0c1dedb8":"markdown","04541646":"markdown","1414dee4":"markdown","71eeae1b":"markdown","8322c7cd":"markdown"},"source":{"cae59f27":"## Data Analysis Phase\n## MAin aim is to understand more about the data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n## Display all the columns of the dataframe\n\npd.pandas.set_option('display.max_columns',None) # display all the columns","180b2410":"dataset=pd.read_csv('..\/input\/water-potability\/water_potability.csv')\n\n## print shape of dataset with rows and columns\nprint(dataset.shape)","12ac8531":"## print the top5 records\ndataset.head()","f4850acf":"print(dataset.info())\nprint(dataset.describe())","fe3a6f8f":"# Correlations\ndataset.corr()","bb168edb":"# heatmap for correlations\nsns.heatmap(dataset.corr())\n## seems like most of the data are not much correlated","5168549f":"## Here we will check the percentage of nan values present in each feature\n## 1 -step make the list of features which has missing values\nfeatures_with_na=[features for features in dataset.columns if dataset[features].isnull().sum()>1]\n## 2- step print the feature name and the percentage of missing values\n\nfor feature in features_with_na:\n    print(feature, np.round(dataset[feature].isnull().mean()*100, 4),  ' % missing values')","95467021":"# Try to understand the distribution for imputing hte missing values\nfor feature in dataset.columns:\n    data=dataset.copy()\n    data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()\n# we observe that all the features are mostly gaussian distributed","440c07d1":"import sklearn\nprint('The scikit-learn version is {}.'.format(sklearn.__version__))","a89922f6":"## Imputing missing values by KNN Imputer at n_neighbours=3\nfrom sklearn.impute import KNNImputer\n\nimputer = KNNImputer(n_neighbors=3)\nimputed = imputer.fit_transform(dataset)\ndf_imputed = pd.DataFrame(imputed, columns=dataset.columns)","c9cdd4a5":"## checking any missing value\ndf_imputed.info()","34fa4a96":"water_df =df_imputed.copy()","17ca3e08":"## Box plot\nfor feature in water_df.columns:\n    data=water_df.copy()\n    sns.boxplot(x=data[feature],data=data.drop('Potability',axis=1))\n    plt.xlabel(feature)\n    plt.ylabel(\"Value\")\n    plt.title(feature)\n    plt.show()","ea60b229":"## Let us plot some scatter plots to understand more about the relations\nsns.pairplot(water_df,hue= 'Potability')\n## Seems no visual relations between features","80cbb52a":"# let us see more closely on behaviour of outliers on classes \n# let us take example of two classes with high outliers like chloramites and sulfates\nsns.scatterplot(x=water_df['Chloramines'],y=water_df['Sulfate'],hue=water_df['Potability'])\n# we leeave the outlier as it as it is not have a clear impact on classes.","2461277a":"# Building model\n# As potability is important, so we need to have less false positive hence need higher precision\n# Try with boosting ensemble technique of Adaboost and xgboost\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold,cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score","43c84943":"## Defining input and target features\ny =water_df['Potability'].values\nx =water_df.drop('Potability',axis=1).values","6955404b":"## let us scale the input variables\nscaler =StandardScaler()\nscaled_x= scaler.fit_transform(x)","d6fbeacc":"## split for train and test, use stratify to balance class in between test and train\nX_train, X_test, y_train, y_test = train_test_split(scaled_x, y, test_size=0.30, random_state=42,stratify=y)","aa7e5f96":"# Fitting a general model\nfrom sklearn.tree import DecisionTreeClassifier\nada_clf= AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=200,learning_rate=0.5, algorithm =\"SAMME.R\")\nada_clf.fit(X_train,y_train)\ny_pred = ada_clf.predict(X_test)\nprint(accuracy_score(y_test,y_pred))\nprint(roc_auc_score(y_test,y_pred))  ","c3697632":"# Searching param grid\nfrom sklearn.metrics import make_scorer\nscoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\nparam_grid={'learning_rate':[1,0.5,0.1,0.01,0.001],'n_estimators':[50,100,200,250,300,400,500]}\ngrid=GridSearchCV(ada_clf,param_grid,scoring=scoring, refit='AUC',cv=5)\ngrid.fit(X_train,y_train)\ngrid.best_params_","4bdf874f":"ada_clf= AdaBoostClassifier(DecisionTreeClassifier(max_depth=2),n_estimators=100,learning_rate=0.1, algorithm =\"SAMME.R\")\nada_clf.fit(X_train,y_train)\ny_pred = ada_clf.predict(X_test)\nprint(accuracy_score(y_test,y_pred))\nprint(roc_auc_score(y_test,y_pred))","1ba8e91e":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))","3b81da09":"xgb_clf= XGBClassifier()\nxgb_clf.fit(X_train,y_train)\ny_pred = xgb_clf.predict(X_test)\nprint(accuracy_score(y_test,y_pred))\nprint(roc_auc_score(y_test,y_pred))","a5315f61":"# Searching param grid\nscoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\nparam_grid={'learning_rate':[1,0.5,0.1,0.01,0.001],'n_estimators':[50,100,200,250,300,400,500],'max_depth':[1,2,3,4,5,6]}\ngrid=GridSearchCV(xgb_clf,param_grid,scoring=scoring, refit='AUC',cv=5)\ngrid.fit(X_train,y_train)\ngrid.best_params_","1db18333":"xgb_clf= XGBClassifier(learning_rate= 0.01,\n max_depth= 6,\n max_features= 'auto',\n n_estimators= 300)\nxgb_clf.fit(X_train,y_train)\ny_pred = xgb_clf.predict(X_test)\nprint(accuracy_score(y_test,y_pred))\nprint(roc_auc_score(y_test,y_pred))","4b16de65":"from sklearn.ensemble import RandomForestClassifier \nfrom sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 100)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(1, 50, num = 5)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","27aff6cf":"#Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)","511ef5b6":"rf_clf=rf_random.best_estimator_\nrf_clf.fit(X_train,y_train)\ny_pred = rf_clf.predict(X_test)\nprint(accuracy_score(y_test,y_pred))\nprint(roc_auc_score(y_test,y_pred))","492f6f1a":"from sklearn.ensemble import VotingClassifier\nvot_clf = VotingClassifier(estimators = [('ada', ada_clf), ('xgb', xgb_clf), ('rf', rf_clf)], voting = 'hard')\nvot_clf.fit(X_train, y_train)\ny_pred = vot_clf.predict(X_test)\nprint(accuracy_score(y_test,y_pred))\nprint(roc_auc_score(y_test,y_pred))","17556092":"vot_clf = VotingClassifier(estimators = [('ada', ada_clf), ('xgb', xgb_clf), ('rf', rf_clf)], voting = 'soft')\nvot_clf.fit(X_train, y_train)\ny_pred = vot_clf.predict(X_test)\nprint(accuracy_score(y_test,y_pred))\nprint(roc_auc_score(y_test,y_pred))","fad6c59e":"##Random Forest","4cb71221":"###Soft Voting","2689a82b":"## Exploring data","448ecd2e":"###Hard Voting","0c1dedb8":"##Constructing a Voting classifier","04541646":"## Outliers","1414dee4":"##XGB","71eeae1b":"## ADAboost","8322c7cd":"## Missing values"}}