{"cell_type":{"b1639513":"code","172672a6":"code","3fa5bf07":"code","a775b2f9":"code","65f7223f":"code","25a71768":"code","e75e9e2f":"code","34c74cb8":"markdown","7404567c":"markdown","af14a536":"markdown","8cd1e1a6":"markdown","d475331c":"markdown","5fa7928c":"markdown","a274d42a":"markdown","00a4319b":"markdown","23724ff4":"markdown"},"source":{"b1639513":"\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\ndata = pd.read_csv('..\/input\/melbourne-housing-snapshot\/melb_data.csv')\n\n# Separate target from predictors\ny = data.Price\nX = data.drop(['Price'], axis=1)\n\n# Drop columns with missing values (simplest approach).\n# Use X in the list comprehension, instead of X_train_full or X_valid_full,\n# because it might be the case that X_train_full has columns with missing\n# values and you used X_valid_full in the list comprehension which does not\n# have the columns with missing values (or vice versa).\ncols_with_missing = [\n    col\n    for col in X.columns\n    if X[col].isnull().any()\n] \nX.drop(cols_with_missing, axis=1, inplace=True)\n\n# Divide data into training and validation subsets\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(\n    X, y, train_size=0.8, test_size=0.2, random_state=0)\n\n# \"Cardinality\" means the number of unique values in a column.\n# Select categorical columns with relatively low cardinality (convenient but\n# arbitrary).\n# Use X in the list comprehension, instead of X_train_full or X_valid_full,\n# because it is always better to use more values (rows) in order to determine\n# the cardinality of a column.\nlow_cardinality_cols = [\n    cname\n    for cname in X.columns\n    if X[cname].nunique() < 10 and X[cname].dtype == \"object\"\n]\n\n# Select numerical columns.\n# Prefer to use X in the list comprehension, instead of X_train_full or\n# X_valid_full, in order to have consistency with the code above.\nnumerical_cols = [\n    cname\n    for cname in X.columns\n    if X[cname].dtype in ['int64', 'float64']\n]\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()","172672a6":"X_train.head()","3fa5bf07":"# Get list of categorical variables\ns = (X_train.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","a775b2f9":"\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","65f7223f":"drop_X_train = X_train.select_dtypes(exclude=['object'])\ndrop_X_valid = X_valid.select_dtypes(exclude=['object'])\n\nprint(\"MAE from Approach 1 (Drop categorical variables):\")\nprint(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))","25a71768":"from sklearn.preprocessing import OrdinalEncoder\n\n# Make copy to avoid changing original data \nlabel_X_train = X_train.copy()\nlabel_X_valid = X_valid.copy()\n\n# Apply ordinal encoder to each column with categorical data\nordinal_encoder = OrdinalEncoder()\nlabel_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\nlabel_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])\n\nprint(\"MAE from Approach 2 (Ordinal Encoding):\") \nprint(score_dataset(label_X_train, label_X_valid, y_train, y_valid))","e75e9e2f":"from sklearn.preprocessing import OneHotEncoder\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_valid = X_valid.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n\nprint(\"MAE from Approach 3 (One-Hot Encoding):\") \nprint(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))","34c74cb8":"We take a peek at the training data with the `head()` method below. ","7404567c":"---\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https:\/\/www.kaggle.com\/learn-forum\/161289) to chat with other Learners.*","af14a536":"### Score from Approach 2 (Ordinal Encoding)\n\nScikit-learn has a [`OrdinalEncoder`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OrdinalEncoder.html) class that can be used to get ordinal encodings.  We loop over the categorical variables and apply the ordinal encoder separately to each column.","8cd1e1a6":"### Define Function to Measure Quality of Each Approach\n\nWe define a function `score_dataset()` to compare the three different approaches to dealing with categorical variables. This function reports the [mean absolute error](https:\/\/en.wikipedia.org\/wiki\/Mean_absolute_error) (MAE) from a random forest model.  In general, we want the MAE to be as low as possible!","d475331c":"In this tutorial, you will learn what a **categorical variable** is, along with three approaches for handling this type of data.\n\n\n# Introduction\n\nA **categorical variable** takes only a limited number of values.  \n\n- Consider a survey that asks how often you eat breakfast and provides four options: \"Never\", \"Rarely\", \"Most days\", or \"Every day\".  In this case, the data is categorical, because responses fall into a fixed set of categories.\n- If people responded to a survey about which what brand of car they owned, the responses would fall into categories like \"Honda\", \"Toyota\", and \"Ford\".  In this case, the data is also categorical.\n\nYou will get an error if you try to plug these variables into most machine learning models in Python without preprocessing them first.  In this tutorial, we'll compare three approaches that you can use to prepare your categorical data.\n\n# Three Approaches\n\n### 1) Drop Categorical Variables\n\nThe easiest approach to dealing with categorical variables is to simply remove them from the dataset.  This approach will only work well if the columns did not contain useful information.\n\n### 2) Ordinal Encoding\n\n**Ordinal encoding** assigns each unique value to a different integer.\n\n![tut3_ordinalencode](https:\/\/i.imgur.com\/tEogUAr.png)\n\nThis approach assumes an ordering of the categories: \"Never\" (0) < \"Rarely\" (1) < \"Most days\" (2) < \"Every day\" (3).\n\nThis assumption makes sense in this example, because there is an indisputable ranking to the categories.  Not all categorical variables have a clear ordering in the values, but we refer to those that do as **ordinal variables**.  For tree-based models (like decision trees and random forests), you can expect ordinal encoding to work well with ordinal variables. \n\n### 3) One-Hot Encoding\n\n**One-hot encoding** creates new columns indicating the presence (or absence) of each possible value in the original data.  To understand this, we'll work through an example.\n\n![tut3_onehot](https:\/\/i.imgur.com\/TW5m0aJ.png)\n\nIn the original dataset, \"Color\" is a categorical variable with three categories: \"Red\", \"Yellow\", and \"Green\".  The corresponding one-hot encoding contains one column for each possible value, and one row for each row in the original dataset.  Wherever the original value was \"Red\", we put a 1 in the \"Red\" column; if the original value was \"Yellow\", we put a 1 in the \"Yellow\" column, and so on.  \n\nIn contrast to ordinal encoding, one-hot encoding *does not* assume an ordering of the categories.  Thus, you can expect this approach to work particularly well if there is no clear ordering in the categorical data (e.g., \"Red\" is neither _more_ nor _less_ than \"Yellow\").  We refer to categorical variables without an intrinsic ranking as **nominal variables**.\n\nOne-hot encoding generally does not perform well if the categorical variable takes on a large number of values (i.e., you generally won't use it for variables taking more than 15 different values). \n\n# Example\n\nAs in the previous tutorial, we will work with the [Melbourne Housing dataset](https:\/\/www.kaggle.com\/dansbecker\/melbourne-housing-snapshot\/home).  \n\nWe won't focus on the data loading step. Instead, you can imagine you are at a point where you already have the training and validation data in `X_train`, `X_valid`, `y_train`, and `y_valid`. ","5fa7928c":"### Score from Approach 1 (Drop Categorical Variables)\n\nWe drop the `object` columns with the [`select_dtypes()`](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.select_dtypes.html) method. ","a274d42a":"Next, we obtain a list of all of the categorical variables in the training data.\n\nWe do this by checking the data type (or **dtype**) of each column.  The `object` dtype indicates a column has text (there are other things it could theoretically be, but that's unimportant for our purposes).  For this dataset, the columns with text indicate categorical variables.","00a4319b":"# Which approach is best?\n\nIn this case, dropping the categorical columns (**Approach 1**) performed worst, since it had the highest MAE score.  As for the other two approaches, since the returned MAE scores are so close in value, there doesn't appear to be any meaningful benefit to one over the other.\n\nIn general, one-hot encoding (**Approach 3**) will typically perform best, and dropping the categorical columns (**Approach 1**) typically performs worst, but it varies on a case-by-case basis. \n\n# Conclusion\n\nThe world is filled with categorical data. You will be a much more effective data scientist if you know how to use this common data type!\n\n# Your Turn\n\nPut your new skills to work in the **[next exercise](https:\/\/www.kaggle.com\/kernels\/fork\/3370279)**!","23724ff4":"In the code cell above, for each column, we randomly assign each unique value to a different integer.  This is a common approach that is simpler than providing custom labels; however, we can expect an additional boost in performance if we provide better-informed labels for all ordinal variables.\n\n### Score from Approach 3 (One-Hot Encoding)\n\nWe use the [`OneHotEncoder`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html) class from scikit-learn to get one-hot encodings.  There are a number of parameters that can be used to customize its behavior.  \n- We set `handle_unknown='ignore'` to avoid errors when the validation data contains classes that aren't represented in the training data, and\n- setting `sparse=False` ensures that the encoded columns are returned as a numpy array (instead of a sparse matrix).\n\nTo use the encoder, we supply only the categorical columns that we want to be one-hot encoded.  For instance, to encode the training data, we supply `X_train[object_cols]`. (`object_cols` in the code cell below is a list of the column names with categorical data, and so `X_train[object_cols]` contains all of the categorical data in the training set.)"}}