{"cell_type":{"77a33a3a":"code","81db6b13":"code","500f7462":"code","1f6e758c":"code","491c175b":"code","e9038d29":"code","9166844d":"code","097369f7":"code","15cc87d1":"code","5a216ebe":"code","094a1d1c":"code","a24cce64":"code","eec9c965":"code","0f7262ad":"code","69fa9d8b":"code","4013d123":"code","62847557":"code","ad3bd85d":"code","228daf1e":"code","fe9a29ca":"code","af5b1f71":"code","3371f987":"code","467aa6f0":"code","f4954621":"code","a0207148":"code","644c8330":"code","471e3254":"code","dfa8d4ee":"code","75ebd84e":"code","7f9e88ae":"code","512fcf04":"code","e3bcd229":"code","61391e7e":"code","d6b43ca8":"code","02f12e28":"code","35df8c91":"code","a75e4c96":"code","1a553ee2":"code","8b450c94":"code","0aebb2ed":"code","0a34a6b8":"code","4f22b415":"code","cf447e2d":"code","d1727f39":"code","cb05fe07":"code","b12dfa5c":"code","8e249877":"code","37d2d9df":"code","577b5997":"code","9858a78f":"code","ffd76e57":"code","a2858dce":"code","1006df3f":"code","8cc80df4":"code","5bed28ae":"code","6d08db61":"code","577487d7":"code","926d6969":"code","6ad28dc2":"code","66c66e17":"code","264f4690":"code","99f3577e":"code","e8b1fd0b":"code","43e967c0":"code","b3aa46b7":"code","80fd55fa":"code","a6328ebf":"code","8e4c2894":"code","4e6cf340":"code","b1188b86":"code","14692305":"code","6602ab1c":"code","41e514f6":"code","1e62b703":"code","754177de":"code","a7dd030e":"code","c965ecd3":"code","4f578db7":"code","7140be2d":"code","d95a0d41":"code","e073cd6f":"code","580ecfcf":"code","95841e3b":"code","5d38cd01":"code","0fb1782d":"code","0320b5df":"code","19045ac3":"code","77007929":"code","def7b7c1":"code","c43ee87f":"code","00463a12":"code","2545d87c":"code","0ce76478":"code","83a37e14":"code","b4928e40":"code","7794a5f7":"code","0adf6970":"code","086f3921":"code","57096414":"code","69be91fa":"code","b6d2d766":"code","4e0b4e33":"code","09d22293":"markdown","80730ff8":"markdown","4dea00f8":"markdown","beedd0a5":"markdown","99729d38":"markdown","1a80ee06":"markdown","83784bc1":"markdown","73f33916":"markdown","af2150b0":"markdown","a06da49e":"markdown","429121ab":"markdown","62a80d1d":"markdown","34641525":"markdown","3a3587cc":"markdown","faf173c8":"markdown","b7dfd191":"markdown","63995baf":"markdown","26e08438":"markdown","ce00e19c":"markdown","e9e44cc6":"markdown","da1dda23":"markdown","543ea6ca":"markdown","82d6790b":"markdown","2d7fed79":"markdown","eb9dec8c":"markdown","1b5d135c":"markdown","e42635f5":"markdown","b6f4dafc":"markdown","f3415a23":"markdown","ffbe1f02":"markdown","564cf595":"markdown","e1745059":"markdown","c0a8f14c":"markdown","f98b60e0":"markdown","c88fa605":"markdown","16bb966d":"markdown","86a21da9":"markdown","d96707a9":"markdown","bb04413e":"markdown","affc7142":"markdown","91669c9a":"markdown","e53aa547":"markdown","e38ef5b8":"markdown","907506e2":"markdown","5bea1de6":"markdown","019d533a":"markdown","2d8866d1":"markdown","eda418e8":"markdown","2a9d963b":"markdown","afaefac5":"markdown","5cf77388":"markdown","09944b44":"markdown","8227d40c":"markdown","50397ae1":"markdown","d7f98ff9":"markdown","89b3ff8d":"markdown","652e09e6":"markdown","c43ffb0f":"markdown","1dd6011a":"markdown","f3cb4133":"markdown","3eadd559":"markdown","61ae3fa3":"markdown","4f4fdb87":"markdown","39edac32":"markdown","70fb53bb":"markdown","7bea6f18":"markdown","11a07000":"markdown","a2ba0481":"markdown","7086022c":"markdown","8a14fe1d":"markdown","ecb850ba":"markdown","89dd7ec5":"markdown","e451949d":"markdown","c434b251":"markdown","992afdb0":"markdown","6d7a0ad4":"markdown","20505c9c":"markdown","c82f380e":"markdown","c9feef38":"markdown","840b8f54":"markdown"},"source":{"77a33a3a":"%matplotlib inline","81db6b13":"!pip install ForceAtlas2\n!pip install node2vec","500f7462":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport random\nimport re\nimport itertools\nimport os\nimport pickle\nfrom collections import Counter\n\n\nimport networkx as nx\nimport cairocffi\nfrom igraph.drawing.text import TextDrawer\nimport igraph as ig\nimport forceatlas2\nimport community\nfrom node2vec import Node2Vec\n\nfrom geopy import distance\nfrom geopy.geocoders import Nominatim\nfrom geopy.extra.rate_limiter import RateLimiter\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.manifold import TSNE\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport ipywidgets as widgets\nfrom ipywidgets import interact, Layout\n\nimport warnings","1f6e758c":"ME = 'Aaron Wise'\nwarnings.simplefilter('ignore')","491c175b":"with open(\"..\/input\/facebook-friends\/friends.txt\", encoding=\"UTF8\") as file:\n    friends = file.read()","e9038d29":"print(friends[:123])","9166844d":"my_friends_list = re.findall(r'Node\\n\\n(.*)', friends)\nprint('Count of nodes:', len(my_friends_list))\nassert len(my_friends_list) == len(set(my_friends_list)), 'Duplicates detected!'","097369f7":"print('Count of nodes with no information:', len(re.findall(r'NA\\n', friends)))","15cc87d1":"splitted_friends = re.split(r'Node\\n\\n(.*)', friends)\nedge_dict = {}\nnode = None\nfor i in splitted_friends:\n    if splitted_friends.index(i) % 2 == 1:\n        node = i\n    else:\n        edge_dict[node] = i\nprint('Count of elements in dict:', len(edge_dict))","5a216ebe":"edge_frame = pd.DataFrame.from_dict(edge_dict, orient='index', columns=['target'])\nedge_frame = edge_frame.reset_index().rename(columns={\"index\": \"source\"})\nedge_frame.head()","094a1d1c":"edge_frame = edge_frame[1::]\nedge_frame.target = edge_frame.target.str.split('\\n\\n')\nedge_frame.head()","a24cce64":"rows = list()\nfor row in edge_frame[['source', 'target']].iterrows():\n    r = row[1]\n    for target in r.target:\n        rows.append((r.source, target))\n\nedge_frame = pd.DataFrame(rows, columns=['source', 'target'])\nedge_frame.head()","eec9c965":"assert len(my_friends_list) == len(edge_frame.source.unique()), 'Records missmatch!'","0f7262ad":"edge_frame = edge_frame[~edge_frame.target.isin(['', 'Edges', 'NA'])].reset_index(drop=True)\nedge_frame.head()","69fa9d8b":"len(edge_frame)","4013d123":"pattern = r'(?P<relationship>Add Friend|Friend\\nFriends|Friend Request Sent|Acquaintance\\nFriends|Respond to Friend Request)\\n(?P<target>.*)(?P<num_friends>\\n\\d*,?\\d*|\\n.*)?(?P<description>.*)?'","62847557":"regex_frame = edge_frame.target.str.extractall(pattern).reset_index()\nregex_frame.tail()","ad3bd85d":"missed_records = edge_frame[~edge_frame.index.isin(regex_frame.level_0)]\nmissed_records.head()","228daf1e":"missed_records.count()","fe9a29ca":"miss_pattern = r'(?P<target>.*)\\n(?P<num_friends>\\d*,?\\d*|\\n.*)?(?P<description>.*)'\nmissed_records = missed_records.target.str.extractall(miss_pattern).reset_index()\nmissed_records.head()","af5b1f71":"combined_regex = pd.concat(objs=[regex_frame, missed_records], sort=False).reset_index(drop=True)","3371f987":"miss_again = edge_frame[~edge_frame.index.isin(combined_regex.level_0)]\nprint('Count of missed records: ', len(miss_again))","467aa6f0":"edge_frame = edge_frame[edge_frame.index.isin(combined_regex.level_0)]\nedge_frame.tail()","f4954621":"combined_regex.tail()","a0207148":"len(edge_frame) == len(combined_regex)","644c8330":"edge_frame = pd.merge(left=edge_frame, right=combined_regex, left_on=edge_frame.index, right_on=combined_regex.level_0)\nedge_frame.head()","471e3254":"edge_frame = edge_frame.drop(columns=['key_0', 'target_x', 'level_0', 'match', ])\nedge_frame = edge_frame.rename(columns={'target_y': 'target'})\nedge_frame.head()","dfa8d4ee":"edge_frame = edge_frame[['source', 'target', 'relationship', 'description', 'num_friends']]\nedge_frame.head()","75ebd84e":"print('NA values: \\n\\n', edge_frame.source.isna().value_counts(), '\\n\\n',\n      edge_frame.target.isna().value_counts(), '\\n\\n',\n      edge_frame.description.isna().value_counts(), '\\n\\n',\n      edge_frame.relationship.isna().value_counts())","7f9e88ae":"edge_frame = edge_frame[edge_frame.target.notna()]","512fcf04":"edge_frame.description = edge_frame.description.fillna(' ')\nedge_frame.relationship = edge_frame.relationship.fillna(' ')","e3bcd229":"print('NA values: \\n\\n', edge_frame.source.isna().value_counts(), '\\n\\n',\n      edge_frame.target.isna().value_counts(), '\\n\\n',\n      edge_frame.description.isna().value_counts(), '\\n\\n',\n      edge_frame.relationship.isna().value_counts())","61391e7e":"edge_frame.target.value_counts().head(20)","d6b43ca8":"edge_frame[edge_frame.target == 'Ivan Ivanov'].head()","02f12e28":"node_frame = edge_frame[['target', 'relationship', 'description', 'num_friends']].rename(columns={'target': 'node'})\nnode_frame.head()","35df8c91":"friend_nodes = node_frame[node_frame.node.isin(my_friends_list)]\nfriend_nodes.relationship.value_counts()","a75e4c96":"friend_nodes = friend_nodes[friend_nodes.relationship != 'Add Friend']\nlen(friend_nodes.node.unique())","1a553ee2":"friend_nodes['node_id'] = friend_nodes.groupby('node').ngroup()\nfriend_nodes.head()","8b450c94":"node_frame['node_id'] = friend_nodes.node_id\nnode_frame.tail()","0aebb2ed":"not_friend_nodes = node_frame[node_frame.node_id.isna()]\nnot_friend_nodes.tail()","0a34a6b8":"not_friend_nodes = not_friend_nodes.fillna(' ')\nnot_friend_nodes['unique_string'] = not_friend_nodes.node + not_friend_nodes.relationship + not_friend_nodes.description + not_friend_nodes.num_friends\nnot_friend_nodes.tail()","4f22b415":"not_friend_nodes.node_id = not_friend_nodes.groupby('unique_string').ngroup() + len(friend_nodes.node_id.unique())\nnot_friend_nodes.tail()","cf447e2d":"not_friend_nodes.num_friends = not_friend_nodes.num_friends.str.strip()\nnot_friend_nodes.num_friends = not_friend_nodes.num_friends.str.replace(',', '')\nnot_friend_nodes.num_friends = pd.to_numeric(not_friend_nodes.num_friends, errors='coerce')\nnot_friend_nodes.dtypes","d1727f39":"not_friend_nodes = not_friend_nodes[(not_friend_nodes.num_friends > 5) | \\\n                                    ((not_friend_nodes.num_friends.isna()) & (not_friend_nodes.description!=' '))]","cb05fe07":"not_friend_nodes[(not_friend_nodes.node=='Ivan Ivanov') & (not_friend_nodes.description=='Varna, Bulgaria')]","b12dfa5c":"node_frame.node_id = node_frame.node_id.fillna(not_friend_nodes.node_id)\nnode_frame.tail()","8e249877":"edge_frame['node_id'] = node_frame.node_id\nedge_frame = edge_frame[edge_frame.node_id.notna()]","37d2d9df":"friend_ids = friend_nodes[['node', 'node_id']].drop_duplicates()\nedge_frame = pd.merge(edge_frame, friend_ids, left_on='source', right_on='node')\nedge_frame = edge_frame.rename(columns={'node_id_y': 'source_id', 'node_id_x': 'target_id'})\nedge_frame = edge_frame.drop(columns=['num_friends', 'relationship', 'node'])\nedge_frame.target_id = edge_frame.target_id.astype('int')\nedge_frame.head()","577b5997":"edge_frame[edge_frame.duplicated(keep=False)].head()","9858a78f":"edge_frame = edge_frame.drop_duplicates(keep=False)","ffd76e57":"print('Number of friends that showed their friendlist: ', len(edge_frame.source.unique()))\nprint('Number of usable connections: ', len(edge_frame))","a2858dce":"ego_frame = edge_frame.copy()\nego_frame = ego_frame[ego_frame.target_id.isin(friend_ids.node_id)]\nego_G = nx.from_pandas_edgelist(df=ego_frame, source='source_id', target='target_id')\nprint(nx.info(ego_G))","1006df3f":"plt.figure(figsize=(30, 30))\nspring_layout = nx.spring_layout(ego_G, seed=42)\n\nnx.draw_networkx_nodes(ego_G, spring_layout, node_color='r', edgecolors='k')\nnx.draw_networkx_edges(ego_G, spring_layout, alpha=0.7)\n\nplt.title('Ego graph with spring layout', fontdict={'fontsize': 40})\nplt.axis('off')\nplt.show()","8cc80df4":"# have friendlist\ninfo_nodes = list(ego_frame.source_id.unique())\n\n# don't have friendlist but are connected to my friends\nmystery_nodes = list(ego_frame[~ego_frame.target_id.isin(ego_frame.source_id)].target_id.unique())\n\n# don't have friendlist and are not connected to my friends\nno_info_nodes = [node for node in dict(ego_G.degree).keys() if ego_G.degree[node] < 2]","5bed28ae":"plt.figure(figsize=(30, 30))\n\nmy_id = edge_frame[edge_frame.source==ME].source_id.unique()[0]\nnodelist = [n for n in info_nodes if (not n==my_id) and (n not in no_info_nodes)]\nnx.draw_networkx_nodes(ego_G, spring_layout, nodelist=nodelist, node_color='r', edgecolors='k')\nnodelist = [n for n in mystery_nodes if n not in no_info_nodes]                       # yes I know it's not gonna visualize\nnx.draw_networkx_nodes(ego_G, spring_layout, nodelist=nodelist, node_color='r', edgecolors='k', node_shape='$\ud83e\udd14$')\nedgelist = [e for e in ego_G.edges if my_id not in e]\nnx.draw_networkx_edges(ego_G, spring_layout, edgelist=edgelist, alpha=0.7)\n\nplt.title('Ego graph with spring layout (filtered)', fontdict={'fontsize': 40})\nplt.legend(['Information node', 'Mystery node'], fontsize=30)\nplt.axis('off')\nplt.show()","6d08db61":"clauset_newman_moore = nx.community.greedy_modularity_communities(ego_G)","577487d7":"def get_paired_color_palette(size):\n    palette = []\n    for i in range(size*2):\n        palette.append(plt.cm.Paired(i))\n    return palette","926d6969":"plt.figure(figsize=(30, 30))\nclusters_count = len(clauset_newman_moore)\nlight_colors = get_paired_color_palette(clusters_count)[0::2]\ndark_colors = get_paired_color_palette(clusters_count)[1::2]\n\nfor i in range(clusters_count):\n    nodelist = [n for n in ego_G.nodes if (n not in no_info_nodes) & (n in list(clauset_newman_moore[i]))]\n    edgelist = [e for e in ego_G.edges if (my_id not in e) and (e[0] in list(clauset_newman_moore[i]) or e[1] in list(clauset_newman_moore[i]))]\n    node_color = [light_colors[i] for _ in range(len(nodelist))]\n    edge_color = [dark_colors[i] for _ in range(len(edgelist))]\n    nx.draw_networkx_nodes(ego_G, spring_layout, nodelist=nodelist, node_color=node_color, edgecolors='k')                                                                                                           \n    nx.draw_networkx_edges(ego_G, spring_layout, edgelist=edgelist, alpha=1\/clusters_count, edge_color=edge_color)\n\nplt.title('Ego graph with Clauset-Newman-Moore clustering', fontdict={'fontsize': 40})\nplt.axis('off')\nplt.show()","6ad28dc2":"louvain = community.best_partition(ego_G, random_state=42)\nplt.figure(figsize=(30, 30))\nclusters_count = len(set(louvain.values()))\nlight_colors = get_paired_color_palette(clusters_count)[0::2]\ndark_colors = get_paired_color_palette(clusters_count)[1::2]\n\nfor i in set(louvain.values()):\n    nodelist = [n for n in ego_G.nodes if (louvain[n]==i) and (n not in no_info_nodes)]\n    edgelist = [e for e in ego_G.edges if (my_id not in e) and ((louvain[e[0]]==i) or (louvain[e[1]]==i))]\n    node_color = [light_colors[i] for _ in range(len(nodelist))]\n    edge_color = [dark_colors[i] for _ in range(len(edgelist))]\n    nx.draw_networkx_nodes(ego_G, spring_layout, nodelist=nodelist, node_color=node_color, edgecolors='k')                                                                                                           \n    nx.draw_networkx_edges(ego_G, spring_layout, edgelist=edgelist, alpha=1\/clusters_count, edge_color=edge_color)\n\nplt.title('Ego graph with Louvain clustering', fontdict={'fontsize': 40})\nplt.axis('off')\nplt.show()","66c66e17":"labels = edge_frame[['target_id', 'target']].set_index('target_id').to_dict()['target']\nfor key, value in friend_ids.set_index('node_id').to_dict()['node'].items():\n    labels[key] = value\nego_labels = {k:v for k, v in labels.items() if k in ego_G.nodes}","264f4690":"ego_G.add_nodes_from([(k, {'name': v}) for k, v in ego_labels.items()])\nnx.write_gml(ego_G, 'ego_G.gml')","99f3577e":"ego_g = ig.Graph.Read_GML('ego_G.gml')\nprint(nx.info(ego_G), '\\n')\nprint(ego_g.summary())","e8b1fd0b":"# fixes mojibake bug\nfor i in range(len(ego_g.vs['name'])):\n    ego_g.vs[i]['name'] = ego_labels[int(ego_g.vs[i]['label'])]","43e967c0":"infomap = ego_g.community_infomap(trials=100)\nig.plot(infomap, layout=ego_g.layout(layout='fr'), mark_groups=False, vertex_size=8, vertex_label=None)","b3aa46b7":"# need to transform all the graphs in the same format for the below function to work\nclusters = []\nfor cluster in range(len(set(louvain.values()))):\n    cluster_list = []\n    for k, v in louvain.items():\n        if v == cluster:            \n            cluster_list.append(k)\n    clusters.append(cluster_list)\nlouvain = clusters\n\nego_g.vs['membership'] = infomap.membership\nclusters = []\nfor i in range(len(infomap.sizes())):\n    cluster = []\n    for vertex in ego_g.vs:        \n        if vertex['membership'] == i:\n            cluster.append(int(vertex['label']))\n    clusters.append(cluster)\ninfomap, clusters = clusters, None","80fd55fa":"algorithms = ['clauset_newman_moore', 'louvain', 'infomap']\nfig = go.Figure(data=[\n    go.Bar(name='Modularity', x=algorithms, y=[nx.community.modularity(ego_G, eval(algorithm)) for algorithm in algorithms]),\n    go.Bar(name='Coverage', x=algorithms, y=[nx.community.coverage(ego_G, eval(algorithm)) for algorithm in algorithms]),\n    go.Bar(name='Performance', x=algorithms, y=[nx.community.performance(ego_G, eval(algorithm)) for algorithm in algorithms])])\n\nfig.update_layout(barmode='group',\n                  title=go.layout.Title(text='Comparison of metrics between clustering algorithms', xref=\"paper\"))\nfig.show()","a6328ebf":"description_frame = edge_frame.copy()\ndescription_frame.description = description_frame.description.str.replace(r'.*at ', ' ')\ndescription_frame.description = description_frame.description.str.strip()\ndescription_frame.description.value_counts().head()","8e4c2894":"stop_word = ['mutual friends', 'friends', 'Facebook', 'Self-Employed']\ndescription_frame = description_frame[~description_frame.description.isin(stop_word)]\ndescription_frame.description.value_counts().head()","4e6cf340":"for node in ego_G.nodes:\n    ego_G.nodes[node]['top'] = list(description_frame.description[(description_frame.target_id!=my_id) & (description_frame.source_id==node)].value_counts()[:3].keys())","b1188b86":"kamada_kawai_layout = nx.kamada_kawai_layout(ego_G)\nforce_atlas_2 = forceatlas2.forceatlas2_networkx_layout(ego_G, niter=1000)","14692305":"# fix node attributes\ndef interactive_clustering(hide=True,\n                           attributes=False,\n                           search=None,\n                           layout='spring_layout',\n                           clustering='clauset_newman_moore',\n                           k=32,\n                           node_attr='top'):\n    \n    # hides single-edge nodes\n    if hide:\n        nodelist = [n for n in ego_G.nodes if (n not in no_info_nodes) and (not n==my_id)]\n        edgelist = [e for e in ego_G.edges if (my_id not in e) and ((e[0] not in no_info_nodes) and (e[1] not in no_info_nodes))]\n    else:\n        nodelist = [n for n in ego_G.nodes]\n        edgelist = [e for e in ego_G.edges]\n    \n    # edges\n    edge_x = []\n    edge_y = []\n    pos=eval(layout)\n    for edge in edgelist:\n        x0, y0 = pos[edge[0]]\n        x1, y1 = pos[edge[1]]\n        edge_x.append(x0)\n        edge_x.append(x1)\n        edge_x.append(None)\n        edge_y.append(y0)\n        edge_y.append(y1)\n        edge_y.append(None)\n    edge_trace = go.Scatter(x=edge_x,\n                            y=edge_y,\n                            line=dict(width=0.5, color='#888'),\n                            hoverinfo='none',\n                            mode='lines')   \n    # nodes\n    node_x = []\n    node_y = []\n    for node in nodelist:\n        x, y = pos[node]\n        node_x.append(x)\n        node_y.append(y)    \n    node_trace = go.Scatter(x=node_x,\n                            y=node_y,\n                            mode='markers',\n                            hoverinfo='text',\n                            marker=dict(showscale=False,\n                                        colorscale='hsv',\n                                        size=10,\n                                        line_width=2))\n    \n    # clustering, attributes preparation\n    cluster_dict = {}\n    cluster_attr = {}\n    counter = Counter()\n    c = eval(clustering)\n    # if it's hierarchical clustering\n    if len(c) > 307:\n        for cluster in range(len(c[k-1])):\n            for node in c[k-1][cluster]:\n                cluster_dict[node] = cluster+1\n                if attributes:\n                    if ego_G.nodes[node][node_attr]:\n                        counter[ego_G.nodes[node][node_attr][0]] += 1\n            cluster_attr[cluster+1] = [i[0] for i in counter.most_common(3)]\n            counter.clear()\n    # if it's optimal clustering\n    else:\n        for cluster in range(len(c)):\n            for node in c[cluster]:\n                cluster_dict[node] = cluster+1\n                if attributes:\n                    if ego_G.nodes[node][node_attr]:\n                        counter[ego_G.nodes[node][node_attr][0]] += 1\n            cluster_attr[cluster+1] = [i[0] for i in counter.most_common(3)]\n            counter.clear()\n    \n    # colors, names, attributes\n    node_text = []\n    node_colors = []\n    for node_id in nodelist:\n        node_colors.append(cluster_dict[node_id])\n        if attributes:\n            node_text.append(f\"{labels[node_id]} - \u2116{cluster_dict[node_id]}\\\n            <br>Node attributes:<br>{ego_G.nodes[node_id][node_attr]}\\\n            <br>Cluster attributes:<br>{cluster_attr[cluster_dict[node_id]]}\")\n        else:\n            node_text.append(f\"{labels[node_id]} - \u2116{cluster_dict[node_id]}\")\n    node_trace.marker.color = node_colors\n    node_trace.text = node_text\n    \n    # actual fig drawing\n    fig = go.Figure(data=[edge_trace, node_trace],\n                    layout=go.Layout(width=600,\n                                     height=600,\n                                     title=f'Ego graph with {clustering} clustering',\n                                     titlefont_size=16,\n                                     showlegend=False,\n                                     hovermode='closest',\n                                     margin=dict(b=20,l=5,r=5,t=40),\n                                     annotations=[dict(text=\"P. Bogdanov - Software University\",\n                                                       showarrow=False,\n                                                       xref=\"paper\",\n                                                       yref=\"paper\",\n                                                       x=0.005,\n                                                       y=-0.002 )],\n                                     xaxis=dict(showgrid=False,\n                                                zeroline=False,\n                                                showticklabels=False),\n                                     yaxis=dict(showgrid=False,\n                                                zeroline=False,\n                                                showticklabels=False),\n                                     plot_bgcolor='white'))\n\n    # search functionality\n    if search:\n        for node in node_trace['text']:\n            if search in node:\n                index = node_trace['text'].index(node)                \n                fig.add_scatter(x=[node_trace.x[index]],\n                                y=[node_trace.y[index]],\n                                mode=\"text\",\n                                text=search)\n\n    fig.show()","6602ab1c":"interact(interactive_clustering,\n         hide=widgets.Checkbox(value=True,\n                               description='Hide single edge nodes'),\n         attributes=widgets.Checkbox(value=False,\n                                     description='Show attributes'),\n         search=widgets.Combobox(options=tuple([labels[i] for i in labels.keys() if i in ego_G.nodes] + ['']),\n                                 placeholder='Your name here',\n                                 description='Search: ',\n                                continuous_update=False),\n         layout=widgets.RadioButtons(options=['spring_layout',\n                                              'force_atlas_2',\n                                              'kamada_kawai_layout'],\n                                     value='force_atlas_2',\n                                     description='Layout: '),\n         clustering=widgets.RadioButtons(options=['clauset_newman_moore', 'louvain', 'infomap'],\n                                         value='infomap',\n                                         description='Algorithm: '),         \n         k=widgets.IntSlider(layout=Layout(display='None'), disabled = True),\n         node_attr=widgets.Text(value='top',layout=Layout(display='None'), disabled = True))","41e514f6":"girvan_newman = []\nfor i in nx.community.girvan_newman(ego_G):\n    girvan_newman.append(i)","1e62b703":"walktr = ego_g.community_walktrap()\nwalktrap = []\nfor clust_count in range(1, len(ego_G.nodes)+1):\n    clust = walktr.as_clustering(n=clust_count)\n    ego_g.vs['membership'] = clust.membership\n    clusters = []\n    for i in range(clust_count):\n        cluster = []\n        for vertex in ego_g.vs:        \n            if vertex['membership'] == i:\n                cluster.append(int(vertex['label']))\n        clusters.append(cluster)\n    walktrap.append(clusters)","754177de":"asyn_fluidc = []\nfor i in range(1, len(ego_G)):\n    instance = nx.community.asyn_fluidc(ego_G, i, max_iter=300, seed=42)\n    asyn_fluidc.append([com for com in instance])","a7dd030e":"interact(interactive_clustering,\n         hide=widgets.Checkbox(value=True,\n                               description='Hide single edge nodes'),\n         attributes=widgets.Checkbox(value=False,\n                                     description='Show attributes'),\n         search=widgets.Combobox(options=tuple([ego_labels[i] for i in ego_labels.keys()] + ['']),\n                                 placeholder='Your name here',\n                                 description='Search: '),\n         layout=widgets.RadioButtons(options=['spring_layout',\n                                              'force_atlas_2',\n                                              'kamada_kawai_layout'],\n                                     value='force_atlas_2',\n                                     description='Layout: '),\n         clustering=widgets.RadioButtons(options=['girvan_newman', 'asyn_fluidc', 'walktrap'],\n                                         value='girvan_newman',\n                                         description='Algorithm: '),         \n         k=widgets.IntSlider(min=1,\n                             max=len(ego_G.nodes)-1,\n                             step=1,\n                             value=32,\n                             layout=Layout(width='90%'),\n                             description='Clusters: ',\n                             continuous_update=False),\n        node_attr=widgets.Text(value='top',layout=Layout(display='None'), disabled = True))","c965ecd3":"optimal = ['clauset_newman_moore', 'louvain', 'infomap']\nhierarchical = ['girvan_newman', 'asyn_fluidc', 'walktrap']\nmetrics_list = ['modularity', 'coverage', 'performance']","4f578db7":"optimal_results = []\nfor algorithm in optimal:\n    optimal_dict = {}\n    optimal_dict['modularity'] = [nx.community.modularity(ego_G, eval(algorithm))]\n    optimal_dict['coverage'] = [nx.community.coverage(ego_G, eval(algorithm))]\n    optimal_dict['performance'] = [nx.community.performance(ego_G, eval(algorithm))]\n    optimal_results.append(optimal_dict)","7140be2d":"hierarchical_results = []\nfor algorithm in hierarchical:\n    hierarchical_dict = {}\n    hierarchical_dict['modularity'] = [nx.community.modularity(ego_G, eval(algorithm)[i]) for i in range(len(eval(algorithm)))]\n    hierarchical_dict['coverage'] = [nx.community.coverage(ego_G, eval(algorithm)[i]) for i in range(len(eval(algorithm)))]\n    hierarchical_dict['performance'] = [nx.community.performance(ego_G, eval(algorithm)[i]) for i in range(len(eval(algorithm)))]\n    hierarchical_results.append(hierarchical_dict)","d95a0d41":"def metrics(G, metrics_list, hierarchical_algorithms, hierarchical_results, optimal_algorithms, optimal_results):\n    for metric in metrics_list:\n        fig = go.Figure()\n        for algorithm in hierarchical_algorithms:\n\n            fig.add_trace(go.Scatter(x=[i+1 for i in range(len(eval(algorithm)))],\n                                     y=hierarchical_results[hierarchical_algorithms.index(algorithm)][metric],\n                                     mode='lines',\n                                     name=algorithm))\n        for algorithm in optimal_algorithms:\n            fig.add_trace(go.Scatter(x=[(len(eval(algorithm)))],\n                                     y=optimal_results[optimal_algorithms.index(algorithm)][metric],\n                                     mode='markers',\n                                     name=algorithm))\n        fig.update_layout(title=go.layout.Title(text=metric[:1].upper()+metric[1:]+\" in clustering algorithms\", xref=\"paper\"),\n                          xaxis=go.layout.XAxis(title=go.layout.xaxis.Title(text=\"Number of clusters\")),\n                          yaxis=go.layout.YAxis(title=go.layout.yaxis.Title(text=metric[:1].upper()+metric[1:])))\n        fig.show()","e073cd6f":"metrics(ego_G, metrics_list, hierarchical, hierarchical_results, optimal, optimal_results)","580ecfcf":"locator = Nominatim(user_agent=\"FacebookLocator\")\ngeocode = RateLimiter(locator.geocode, min_delay_seconds=1, return_value_on_exception='Error')","95841e3b":"locations = pd.DataFrame(description_frame.description.value_counts().keys(), columns=['address'])\nnot_real_locations = ['BMW', 'The Krusty Krab', '\u041c\u0412\u0420', 'YouTube', 'Tumblr', 'Freelancer', 'McDonald\\'s', 'Oriflame', 'None', '']\nlocations = locations[~locations.address.isin(not_real_locations)]\nprint(\"Count of unique descriptions:\", len(locations))","5d38cd01":"# # takes about a day\n# locations['location'] = locations['address'].apply(geocode)\n# locations = locations.reset_index(drop=True)\n# unknown_locations = locations[locations.location.isnull()]\n# locations = locations[locations.location.notnull()]      \nwith open(f\"..\/input\/locations\/locations.pickle\", \"rb\") as f:\n    locations = pickle.load(f)\nprint(\"Count of geocoded locations:\", len(locations))","0fb1782d":"locations['point'] = locations['location'].apply(lambda loc: tuple(loc.point) if loc else None)\nlocations['raw'] = locations['location'].apply(lambda loc: loc.raw if loc else None)","0320b5df":"additional = locations.raw.apply(pd.Series)\nlocations = locations.join(additional, rsuffix='index')\nlocations = pd.merge(description_frame, locations, left_on='description', right_on='address', how='left')","19045ac3":"map_df = locations.dropna().copy()\nmap_df['nodes_count'] = map_df.groupby('osm_id').osm_id.transform('count')\nmap_df.osm_id = map_df.osm_id.drop_duplicates()\nmap_df = map_df.dropna()\n\nscaler = MinMaxScaler(feature_range=(0.02, 1))\nscaler.fit(map_df[['nodes_count']])\nmap_df['nodes_count_scaled'] = scaler.transform(map_df[['nodes_count']])\n\nfig = px.scatter_geo(map_df,\n                     lat=\"lat\",\n                     lon=\"lon\",\n                     hover_name=\"address\",\n                     text='nodes_count',\n                     opacity=0.5,\n                     color='nodes_count_scaled',\n                     size='nodes_count_scaled',\n                     title='Available locations')\nfig.update_layout(margin={\"r\":0,\"t\":80,\"l\":0,\"b\":0})\nfig.show()","77007929":"def propagate_by(by='full'):    \n    # different options for propagation\n    education_list = ['school', 'university', 'college', 'educational_institution', 'music_school', 'language_school']\n    if by=='education':        \n        location = locations[locations.type.isin(education_list)]\n    elif by=='area':\n        location = locations[(locations.type.isin(['city', 'town', 'village'])) | (locations['class']=='boundary')]\n    else:\n        location = locations[locations.osm_id.notna()]\n    \n    # the algorithm implementation needs the ids to start from 0\n    location = location.assign(location_id=(locations['osm_id']).astype('category').cat.codes)\n    by_dict = location[['target_id', 'location_id']].set_index('target_id').to_dict()['location_id']\n    G = nx.from_pandas_edgelist(edge_frame, source='source_id', target='target_id')\n    \n    # we're using igraph so we need to transform our graph to the proper fromat\n    G.add_nodes_from([(k, {'name': v}) for k, v in labels.items()])\n    G.add_nodes_from([(k, {by: v}) for k, v in by_dict.items()])\n    nx.write_gml(G, 'G.gml')\n    g = ig.Graph.Read_GML('G.gml')\n    \n    # fix mojibake and prepare attributes\n    for i in range(len(g.vs['name'])):\n        g.vs[i]['name'] = labels[int(g.vs[i]['label'])]\n    for i in range(len(g.vs[by])):\n        try:\n            g.vs[i][by] = by_dict[int(g.vs[i]['label'])]\n            g.vs[i]['fixed'] = True\n        except:        \n            g.vs[i][by] = -1\n            g.vs[i]['fixed'] = False\n    \n    # the actual model\n    label_propagation = g.community_label_propagation(initial=g.vs[by], fixed=g.vs['fixed'])\n    \n    # make it human readable\n    id_to_address = location[['location_id', 'address']].set_index('location_id').to_dict()['address']\n    \n    # transform format to nx-like clusters, leave only ego_G nodes and change node_attr\n    label_prop = []\n    attributes = {}\n    for i in range(len(label_propagation)):\n        cluster = []\n        cluster_locations = set(label_propagation.subgraph(i).vs[by])\n        for node in label_propagation.subgraph(i).vs:        \n            if int(node['label']) in ego_labels.keys():\n                cluster.append(int(node['label']))\n                if len(cluster_locations) > 1:\n                    ego_G.nodes[int(node['label'])][by] = [id_to_address[sorted(list(cluster_locations))[1]]]\n                else:\n                    ego_G.nodes[int(node['label'])][by] = [id_to_address[sorted(list(cluster_locations))[0]]]\n        if cluster:\n            label_prop.append(cluster)\n    return label_prop","def7b7c1":"label_prop_by_area = propagate_by('area')\nlabel_prop_by_education = propagate_by('education')\nlabel_prop_full = propagate_by('full')","c43ee87f":"interact(interactive_clustering,\n         hide=widgets.Checkbox(value=False,\n                               description='Hide single edge nodes'),\n         attributes=widgets.Checkbox(value=True,\n                                     description='Show attributes'),\n         search=widgets.Combobox(options=tuple([labels[i] for i in labels.keys() if i in ego_G.nodes] + ['']),\n                                 placeholder='Your name here',\n                                 description='Search: ',\n                                 continuous_update=False),\n         layout=widgets.RadioButtons(options=['spring_layout',\n                                              'force_atlas_2',\n                                              'kamada_kawai_layout'],\n                                     value='spring_layout',\n                                     description='Layout: '),\n         clustering=widgets.RadioButtons(options=[\"label_prop_by_area\", \"label_prop_by_education\", \"label_prop_full\"],\n                                        value=\"label_prop_by_education\",\n                                         description='Algorithm: '),         \n         k=widgets.IntSlider(layout=Layout(display='None'), disabled = True),\n         node_attr=widgets.RadioButtons(options=[\"top\", \"area\", \"education\", \"full\"],\n                                        value='education', description='Attributes: '))","00463a12":"algorithms = [\"label_prop_by_area\", \"label_prop_by_education\", \"label_prop_full\"]\nfig = go.Figure(data=[\n    go.Bar(name='Modularity', x=algorithms, y=[nx.community.modularity(ego_G, eval(algorithm)) for algorithm in algorithms]),\n    go.Bar(name='Coverage', x=algorithms, y=[nx.community.coverage(ego_G, eval(algorithm)) for algorithm in algorithms]),\n    go.Bar(name='Performance', x=algorithms, y=[nx.community.performance(ego_G, eval(algorithm)) for algorithm in algorithms])])\n\nfig.update_layout(barmode='group',\n                  title=go.layout.Title(text='Comparison of metrics between label propagation algorithm', xref=\"paper\"))\nfig.show()","2545d87c":"map_df = pd.DataFrame()\nattributes = ['area', 'education', 'full']\nfor attr in attributes:\n    loc_dict = {}\n    for i in ego_G.nodes:\n        loc_dict[i] = ego_G.nodes[i][attr]\n    for i in loc_dict.keys():\n        point = locations[locations.address==loc_dict[i][0]].point.unique()[0][:2]\n        loc_dict[i] = (loc_dict[i][0], point[0], point[1], attr)\n        \n    df = pd.DataFrame.from_dict(loc_dict).T.rename(columns={0: 'name', 1:'lat', 2:'lon', 3:'type'})\n    df['friends_count'] = df.groupby('name').name.transform('count')\n    scaler = MinMaxScaler(feature_range=(0.2, 1))\n    scaler.fit(df[['friends_count']])\n    df['friends_count_scaled'] = scaler.transform(df[['friends_count']])\n    map_df = pd.concat([map_df, df])\n\nfig = px.scatter_geo(map_df,\n                     lat=\"lat\",\n                     lon=\"lon\",\n                     hover_name=\"name\",\n                     text='friends_count',\n                     size='friends_count_scaled',\n                     animation_frame='type',\n                     color='type',\n                     title='Friends locations')\nfig.update_layout(margin={\"r\":0,\"t\":80,\"l\":0,\"b\":0})\nfig.show()","0ce76478":"ego_node2vec = Node2Vec(ego_G)\nego_model = ego_node2vec.fit()","83a37e14":"kmeans = []\nsilhouettes = {}\ninertia = {}\nfor i in range(1, len(ego_G.nodes())-1):\n    X = ego_model.wv[ego_model.wv.vocab]\n    kmeans_model = KMeans(n_clusters=i, n_init=20, n_jobs=2, random_state=42)\n    kmeans_model.fit(X)\n    kmeans_labels = kmeans_model.labels_\n    # make compatible with interactive function\n    vector_clustering = []\n    for j in range(i+1):\n        cluster_list = []\n        for cluster, node in zip(kmeans_labels, ego_model.wv.vocab):\n            if (cluster==j) and (int(node) in ego_G.nodes()):\n                cluster_list.append(int(node))\n        if cluster_list:\n            vector_clustering.append(cluster_list)\n    kmeans.append(vector_clustering)\n    # scoring to plot\n    inertia[i] = kmeans_model.inertia_\n    \n    if i>1:\n        silhouettes[i] = silhouette_score(X, kmeans_labels, metric='euclidean')","b4928e40":"embeddings = np.array([ego_model.wv[x] for x in ego_model.wv.vocab])\ntsne = TSNE(n_components=2, random_state=42, perplexity=15)\nembeddings_2d = tsne.fit_transform(embeddings)\n\n# make compatible with our interactive function\nembeddings_layout = spring_layout.copy()\ncounter = 0\nfor x in ego_model.wv.vocab:\n    for k, v in embeddings_layout.items():\n        if str(k) == x:\n            embeddings_layout[k] = embeddings_2d[counter]\n            counter +=1","7794a5f7":"interact(interactive_clustering,\n         hide=widgets.Checkbox(value=True,\n                               description='Hide single edge nodes'),\n         attributes=widgets.Checkbox(value=False,\n                                     description='Show attributes'),\n         search=widgets.Combobox(options=tuple([ego_labels[i] for i in ego_labels.keys()] + ['']),\n                                 placeholder='Your name here',\n                                 description='Search: '),\n         layout=widgets.RadioButtons(options=['spring_layout',\n                                              'force_atlas_2',\n                                              'kamada_kawai_layout',\n                                              'embeddings_layout'],\n                                     value='embeddings_layout',\n                                     description='Layout: '),\n         clustering=widgets.RadioButtons(options=['girvan_newman', 'asyn_fluidc', 'walktrap', 'kmeans'],\n                                         value='kmeans',\n                                         description='Algorithm: '),         \n         k=widgets.IntSlider(min=1,\n                             max=308,\n                             step=1,\n                             value=32,\n                             layout=Layout(width='90%'),\n                             description='Clusters: ',\n                             continuous_update=False),\n        node_attr=widgets.Text(value='top',layout=Layout(display='None'), disabled = True))","0adf6970":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=list(inertia.keys()), y=list(inertia.values()), mode='lines'))\nfig.update_layout(title=go.layout.Title(text=\"Elbow method\", xref=\"paper\"),\n                  xaxis=go.layout.XAxis(title=go.layout.xaxis.Title(text=\"Number of clusters\")),\n                  yaxis=go.layout.YAxis(title=go.layout.yaxis.Title(text=\"Inertia\")))\nfig.show()","086f3921":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=list(silhouettes.keys()), y=list(silhouettes.values()), mode='lines'))\nfig.update_layout(title=go.layout.Title(text=\"Silhouette coefficients\", xref=\"paper\"),\n                  xaxis=go.layout.XAxis(title=go.layout.xaxis.Title(text=\"Number of clusters\")),\n                  yaxis=go.layout.YAxis(title=go.layout.yaxis.Title(text=\"Silhouette coefficient\")))\nfig.show()","57096414":"hierarchical.append('kmeans')\nhierarchical_dict = {}\nhierarchical_dict['modularity'] = [nx.community.modularity(ego_G, kmeans[i]) for i in range(len(kmeans))]\nhierarchical_dict['coverage'] = [nx.community.coverage(ego_G, kmeans[i]) for i in range(len(kmeans))]\nhierarchical_dict['performance'] = [nx.community.performance(ego_G, kmeans[i]) for i in range(len(kmeans))]\nhierarchical_results.append(hierarchical_dict)\n\nmetrics(ego_G, metrics_list, hierarchical, hierarchical_results, optimal, optimal_results)","69be91fa":"def most_similar(name):\n    try:\n        for node, percent in ego_model.wv.most_similar(str({v:k for k, v in ego_labels.items()}[name])):\n                print(ego_labels[float(node)], percent)\n    except:\n        print(\"\")","b6d2d766":"most_similar('Alexander Collins')","4e0b4e33":"interact(most_similar, name=widgets.Combobox(options=tuple(sorted(ego_labels.values())),\n                                             placeholder='Your name here',\n                                             description='Search: '))","09d22293":"And now to check again if there are still records we missed.","80730ff8":"# <center>Finding communities in social networks<\/center>\n## <center>Unsupervised and semi-supervised clustering of friends<\/center>\n### <center>Pavel Bogdanov - Software University<\/center>","4dea00f8":"If we take for example the most common name in the dataframe(excluding mine) we will see it's obviously not the same person.","beedd0a5":"Great, we have locations all over the world. Not only that, but we have the type of the location as well. That gives us some additional options to choose from.","99729d38":"And finally move up to the original `edge_frame` we were using.","1a80ee06":"Then we can remove all the descriptions that don't give us any information.","83784bc1":"We're going to start simple. Let's do an ego graph that shows only my friends with no additional filtering.","73f33916":"As we can see, structurally, the three main clusters are still present, but this time they are not so homogeneous. During the clustering process the sparsely connected nodes aren't getting divided first, as is the case with other hierarchical algorithms. Instead, K-means' random initialization results in a simultaneous grouping over the span of the whole graph.\nIn a way, this type of clustering is a combination of walktrap's random walks and fluid communities' random initialization.\n\nThis time we can check other types of metrics. Inertia is the sum of squared distances of samples to their closest cluster center. The elbow method is finding the \"elbow\" point after which the inertia start decreasing in a linear fashion. That will give us the optimal amount of clusters.","af2150b0":"311 of the people in the dataset are my friends.","a06da49e":"Now we can transform the dataframe into a source-target format.","429121ab":"Check how much missing values we have.","62a80d1d":"That's a small enough number to be ignored so we can remove these 13 records.","34641525":"Here's the tricky part. For some of the records we can't know if they are the same person or not. We simply don't have enough information. So we will give a unique ID to the ones with more information and remove the one where we don't have enough.","3a3587cc":"We skipped the `num_friends` column for now because we're going to use it as numeric later.","faf173c8":"We need to give the `source` column an ID as well.","b7dfd191":"There is more than just the name we can get from each profile. Every bit of information can help us in the clustering process.","63995baf":"The rest of them can be empty.","26e08438":"We can see there is a slight missmatch in the count of the original index with the new one. That means that we missed some records.","ce00e19c":"### Semi-supervised community detection\n\nSo far we've tried unsupervised aproaches that rely solely on the connections between the nodes. But that's not all we've got. Some of our nodes have descriptions that can be used in a semi-supervised approach.\n\n#### Label propagation\n[Label Propagation](http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.14.3864&rep=rep1&type=pdf) is a semi-supervised machine learning algorithm that assigns labels to previously unlabeled data points. At initial condition, the nodes carry a label that denotes the community they belong to. Membership in a community changes based on the labels that the neighboring nodes possess. This change is subject to the maximum number of labels within one degree of the nodes. Every node is initialized with a unique label, then the labels diffuse through the network. Consequently, densely connected groups reach a common label quickly. When many such dense groups are created throughout the network, they continue to expand outwards until it is impossible to do so.\n\nCurrently our \"labels\" are nothing more than strings. Some of them refer to the same location, but are written in a different way. If we want to use them in a meaningfull way, we need to transform them.<br> Geopy will do the trick. It offers a free geocoding functionallity that can give us not only the geographic coordinates of the location, but also its type. The only limitation is that if we want to use it in bulk we need to put a rate limiter on the queries.","e9e44cc6":"But that's not enough. Two people with the same common name can still have for example 2 mutual friends with me and live in the same city or study in the same university. So we're going to take only the ones with `num_friends` greater than some number.","da1dda23":"To get a 2D representation of this high-dimensional space we can use TSNE. Note that the clustering we made earlier was in higher dimensions and this is simply a visualization of that space.","543ea6ca":"We can change the number of clusters for each algorithm and eyeball it _or_ we can look at the metrics for more insight.","82d6790b":"Now to combine these ID's with my friends.","2d7fed79":"A nice way of pairing node color with a different shade of the same color for the edges:","eb9dec8c":"Let's format the data into something more readable, like a pandas dataframe.","1b5d135c":"Great, this time we have a lot more clusters, but we can't compare the graphs just by looking at them! We can evaluate the algorithms by the following metrics:\n\n**Modularity** is the most popular measure of the structure of networks. It is the fraction of the edges that fall within the given groups minus the expected fraction if edges were distributed at random. Networks with high modularity have dense connections between the nodes within clusters but sparse connections between nodes in different clusters.\n\n**The coverage** of a clustering is given as the fraction of the weight of all intra-cluster edges with respect to the total weight of all edges in the whole graph. Higher values of coverage mean that there are more edges inside the clusters than edges linking different clusters, which translates to a better clustering.\n\n**Performance** counts the number of internal edges in a cluster along with the edges that don\u2019t exist between the cluster\u2019s nodes and other nodes in the graph. Higher values indicate that a cluster is both internally dense and externally sparse and, therefore, a better cluster.","e42635f5":"That's better. We can see that even with missing information about half of the nodes, the other half compensates and forms some nice clusters. Now let's try and actually separate them by color.\n\n### Optimal community detection methods\n\n#### Clauset-Newman-Moore\n\nClauset-Newman-Moore greedy modularity maximization works like this: Initially, every node belongs to its own community, then, at each step, the algorithm repeatedly merges pairs of communities together and chooses the merger for which the resulting modularity is the largest. The algorithm stops when all the nodes in the network are in a single community after (count of nodes \u2212 1) steps of merging. What we get at the end is the communities with the best modularity score.","b6f4dafc":"Check to see if we have any duplicates.","f3415a23":"We're going to reuse this function further in the notebook, so we'll add some additional functionality that won't be usable at this moment.","ffbe1f02":"We can see that the three algorithms are vastly different from each other but they do have some crossing points. Almost all of them peak early on at about 6 clusters, except Girvan-Newman, which is barely changing at that level. This can be explained by the fact that the other methods separate the biggest clusters in the beginning, while Girvan-Newman trims the one-edge nodes first and works its way up to the bigger clusters from there. An interesting point is the secondary peak at cluster count 32, most notable in Girvan-Newman but present in Walktrap as well. This is a clustering we can't get with the optimal community findig algorithms, because they all converge early on around the first peak. Now we have a choice - if we want a broader view of the communities we can choose a cluster count of 6, but if we want more details then our cluster count is 32.","564cf595":"This is going to take a while so we better minimize the amount of queries we'll need.","e1745059":"We need to modify our regex a bit to catch these records.","c0a8f14c":"### Graph Embedding\n\nOne of the limitations of graphs remains the absence of vector features. Besides reducing the engineering effort, these vector representations can lead to greater predictive power and allow us to use a broader range of machine learning tools. \n\n\n#### Node2Vec\n\nOne well-known algorithm that extracts information about entities using context alone is word2vec. The input to word2vec is a set of sentences, and the output is an embedding for each word. Similarly to the way text describes the context of each word via the words surrounding it, graphs describe the context of each node via neighbor nodes. The embeddings are learned in the same way as word2vec\u2019s skip-gram embeddings are learned, using a skip-gram model. To generate the corpus, we use random walks sampling strategy","f98b60e0":"Each profile in the `target` column is separated by two new lines.","c88fa605":"We can immediately see 3 major cluster forming even without doing any machine learning. That's good, but we also see a lot of lonely nodes, with just a single edge, connected to me. There's no way of knowing which cluster they belong to at this moment so to make the graph more clear we can simply hide them. And to make the clusters stand out more we can hide my edges too. Note that the nodes and edges we hide are still in the graph affecting the rest of the nodes, we simply don't visualize them.\n\nAs an extra piece of information, we can separate the nodes for which we have a friendlist and the ones for which we don't. That way we can see if the clustering works for nodes with missing information.","16bb966d":"Let's try it with these 3 options:","86a21da9":"There is no way to know if these are just the same person with multiple profiles or it's an entirely diferent person, so we will remove them just in case.","d96707a9":"#### Visualising the data","bb04413e":"The silhouette index gives us another confirmation that a smaller number of clusters is probably more optimal. However, even the highest values of the index are pretty low, which means that the whole structure is weak and we might have made artificial clusters.\n\nFinally, we can compare it to the rest of the algorithms:","affc7142":"### Intro\nHave you ever been in a situation where you introduce your work friends to your old highschool friends and you realize how different those two groups are? Even though they're both your friends, they have no common topics to talk about. Now imagine trying to plan the seating of a wedding. With each friend you consider it becomes increasingly difficult to keep track who's friends with whom and in what group he belong to. This is where unsupervised machine learning comes in. We can simply take a bunch of Facebook friendlists and let the algorithms do all the work for us.\n\n#### Loading dependencies","91669c9a":"### The Data\n\n#### Data aquisition\nYou might be wondering how I aquired all of these friendlists I talk about. Does Facebook have some sort of API that allows us to do just that? It used to, but now in order to make it work you need to have an app and each of your friends needs to give permission to that app. So I must have used a scraper, right? Well yes, but actually no. If I did, I'd infringe Facebook's community standards and probably get blocked. So what, I copied all of these friendlists by hand? Yes, yes I did.\n\n#### Reading the data\nThe data is in a dict-like structure with nodes and edges as keys and (list of) facebook profiles as values.","e53aa547":"Now they should be equal in length.","e38ef5b8":"Unfortunately we have no clear cut point that shows us the optimal amount of clusters, as any number between 5 and 50 can be that point.\n\nAnother metric is the silhouette index. Silhouette values near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster.","907506e2":"### Hierarchical community detection methods\n#### Girvan-Newman\nThe Girvan-Newman algorithm detects communities by progressively removing edges from the original network. The connected components of the remaining network are the communities. Instead of trying to construct a measure that tells us which edges are the most central to communities, the Girvan-Newman algorithm focuses on edges that are most likely \"between\" communities.","5bea1de6":"And then we can add to the sequence of node ID's we have by looking at the unique string.","019d533a":"The way we're going to differentiate between the friends is by combining all the information we have about them into a single unique string.","2d8866d1":"Extract all available information we have for each location:","eda418e8":"#### Fluid Communities\nThe [Fluid Communities](https:\/\/arxiv.org\/pdf\/1703.09307.pdf) algorithm is based on the simple idea of fluids interacting in an environment, expanding and pushing each other. First each of the initial k communities is initialized in a random vertex in the graph. Then the algorithm iterates over all vertices in a random order, updating the community of each vertex based on its own community and the communities of its neighbours. This process is performed several times until convergence. At all times, each community has a total density of 1, which is equally distributed among the vertices it contains. If a vertex changes of community, vertex densities of affected communities are adjusted immediately. When a complete iteration over all vertices is done, such that no vertex changes the community it belongs to, the algorithm has converged and returns.","2a9d963b":"If we apply Node2Vec to the full list of friends we can even add people we don't know to our communities. But that is another task, more related to link prediction and recommendation systems than clustering.\n\n### Conclusion\n\nOptimal community finding methods work well enough for low resolution clustering, but if we want to find smaller communities - hierarchical methods work better. Semi-supervised learning with label propagation returns mixed results, but can give us additional insight into the current communities, as long as we already have some \"labels\" for them. Graph embedding is useful if we are trying to artificially fit into a certain amount of equaly sized clusters, even if they are not the optimal amount for the graph. It is unclear if the same results apply to weighted and\/or directed graphs, as further research is required.\n\n### Refferences\n\n\n[M. E. J Newman \"Networks: An Introduction\" Oxford University Press 2011](http:\/\/math.sjtu.edu.cn\/faculty\/xiaodong\/course\/Networks%20An%20introduction.pdf)\n\n[Clauset, A., Newman, M. E., & Moore, C. \u201cFinding community structure in very large networks.\u201d Physical Review E 70(6), 2004.](https:\/\/arxiv.org\/pdf\/cond-mat\/0408187.pdf)\n\n[Vincent D. Blondel, Jean-Loup Guillaume, Renaud Lambiotte and Etienne Lefebvre: Fast unfolding of communities in large networks](https:\/\/arxiv.org\/pdf\/0803.0476.pdf)\n\n[M. Rosvall, D. Axelsson, and C. T. Bergstrom, The map equation, Eur. Phys. J. Special Topics 178, 13 (2009)](https:\/\/arxiv.org\/pdf\/0906.1405.pdf)\n\n[Girvan M. and Newman M. E. J., Community structure in social and biological networks](https:\/\/www.pnas.org\/content\/pnas\/99\/12\/7821.full.pdf)\n\n[Pascal Pons, Matthieu Latapy: Computing communities in large networks using random walks](https:\/\/arxiv.org\/pdf\/physics\/0512106.pdf)\n\n[Par\u00e9s F., Garcia-Gasulla D. et al. \u201cFluid Communities: A Competitive and Highly Scalable Community Detection Algorithm\u201d.](https:\/\/arxiv.org\/pdf\/1703.09307.pdf)\n\n[Xiaojin Zhu & Zoubin Ghahramani \"Learning from Labeled and Unlabeled Data with Label Propagation\"](http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.14.3864&rep=rep1&type=pdf)\n\n[node2vec: Scalable Feature Learning for Networks. A. Grover, J. Leskovec. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2016.](https:\/\/arxiv.org\/pdf\/1607.00653.pdf)","afaefac5":"It looks like each algorithm has its advantages and disadvantages. CNM has the highest coverage, but Louvain has higher modularity and performance score. Even though they both aim to maximize the modularity score, Louvain appears to do better, which makes sense as the algorithm was developed by improving the already existing CNM. Infomap on the other hand doesn't rely on modularity and instead uses the map equation as a scoring function which gives it the highest performance score of them all.\n\nAll these graphs are great, but how are we to evaluate the communities if we can't even see people's names? Let's make an interactive graph, that let's us decide for ourselves if the clustering is correct.<br>\nFirst, we can clean up each person's description into something more readable.","5cf77388":"And transfer that ID to the `node_frame` such that only my friends will have an ID.","09944b44":"#### Infomap\nFinds the community structure of the network according to the Infomap method of Martin Rosvall and Carl T. Bergstrom.\nThe core of the algorithm follows closely the Louvain method: neighboring nodes are joined into modules, which subsequently are joined into supermodules and so on. First, each node is assigned to its own module. Then, in random sequential order, each node is moved to the neighboring module that results in the largest decrease of the [**map equation**](https:\/\/arxiv.org\/pdf\/0906.1405.pdf). If no move results in a decrease of the map equation, the node stays in its original module. This procedure is repeated, each time in a new random sequential order, until no move generates a decrease of the map equation.","8227d40c":"Then we will give each of my friends a unique ID.","50397ae1":"Rearange the columns for aesthetics.","d7f98ff9":"In conclusion:","89b3ff8d":"We can merge the two so that we can see `source` and `target` in the same dataframe.","652e09e6":"Now we can explore communities at various resolutions by specifying the number of clusters we want.","c43ffb0f":"Drop unused columns.","1dd6011a":"177 of those friends didn't show their friendlist so we have no direct information about their friends.","f3cb4133":"And since almost all of the people in my Ego graph have no description, we can assign the three most common descriptions from each person's friendlist as his own.","3eadd559":"As we can see, the method is very similar to Fluid Communities and is even sub-optimal in higher cluster count.\n\nEven though the metrics are not kind to this clustering, it did provide us with a different type of grouping. While Girvan-Newman and Walktrap keep most of the nodes inside the bigger clusters unified and slowly trim them down untill there's no more left and Fluid Communities \"spills out\" at higher cluster count, returning more randomly distributed clusters, Node2Vec embedding combined with k-means clustering provides uniformly distributed communities and finds connections between nodes where other methods can't. This can be useful if we want to separate the groups by equal amount of nodes.\n\nBut the biggest advantage of graph embedding might be the availability to see the distances between nodes.","61ae3fa3":"The advantages of this algorithm are obvious. We can extract useful information about single nodes that didn't belong to any cluster in the previous algoriths. It's pretty accurate for the sparsely connected unlabeled nodes, but the opposite is true for the ones that have a lot of connections between them. They tend to acquire each other's labels and end up being missclassified. In addition, due to the randomness of the algorithm it produces highly variable results. Nevertheless it is still usefull and can show us connections that the other algorithms can not.\n\nWe can compare the metrics but they migh be different on each run.","4f4fdb87":"But we have a problem. We don't have a unique ID for each profile. That means that if we only look at the names, it will appear as though the most popular people are the ones with the most common name.","39edac32":"Let's see all the locations we have available.","70fb53bb":"We already checked my friends for duplicates so we know they are unique to each other. Let's concentrate on their friends in the `target` column.","7bea6f18":"Looks like this algorithm is a good start, but we can see a few smaller clusters that are not separated from the larger ones. \n\n#### Louvain\n\nThe [Louvain](https:\/\/arxiv.org\/pdf\/0803.0476.pdf) method of community detection also uses a greedy approach and initially assigns each node to an individual community. However, instead of a search over all edges, the Louvain method executes a local search over the edges of each node. Each node is combined with the neighbour that most increases its modularity. This process of reassigning communities is repeated over several iterations, until modularity is increased. Once this first-phase allocation of communities is determined, the Louvain method joins nodes within a community into supernodes. The inner iteration is then repeated over these supernodes. The steps of the inner and outer iterations are executed repeatedly until the number of communities is suitably small and the modularity cannot be increased any further.","11a07000":"We can see one additional cluster, but it doesn't differentiate between the small communities inside it. This is a known drawback of modularity, as it suffers a resolution limit and is unable to detect smaller communities in graphs.\n\nNetworkx has a couple more community finding algorithms but we will come back to them later. First we will test one more methods from the `igraph` library.<br>\nThere doesn't seem to be a way to make it communicate with networkx so we will have to export the graph in a format that igraph can read.","a2ba0481":"I checked the topmost duplicated names on Facebook and did not find different people with the same amount of mutual friends larger than 5. So this is going to be our threshold. Naturally we also need to remove all friends with no description, because we can't differentiate between them as well.","7086022c":"Now instead of a graph specific algorithm, we can use any of the well known ML clustering approaches like K-means.","8a14fe1d":"First we need to separate my friends from other friends with the same name.","ecb850ba":"#### Data preprocessing","89dd7ec5":"Clean it up a little.","e451949d":"Looking at the graph while knowing who each node is is much better. The three major clusters common in the three algorithms are people from my highschool, people from my university and the people which I go out with. It's interesting to see that the people from my last job are connected to the university cluster, but infomap algorithm separates them into a different community. Even more interesting is that of those people it has separated those who have graduated from the university from the rest of my job colleagues. The rest of the mini-clusters are separated by city, some more correctly than others.\n\nIt is clear that there are many levels of potential communities to be found. If we want to see those levels we will need a hierarchical algorithm.","c434b251":"Merge locations with nodes:","992afdb0":"One last thing we can do is visualise the locations on the map.","6d7a0ad4":"While we're at it, why not add a few more layouts.","20505c9c":"The `target` column can't have NA values so we have to remove these records.","c82f380e":"#### Walktrap\n\nThe [walktrap](https:\/\/www-complexnetworks.lip6.fr\/~latapy\/Publis\/communities.pdf) algorithm of Latapy & Pons tries to find communities in a graph via random walks. The idea is that short random walks on a graph tend to get \"trapped\" into densely connected parts corresponding to communities. Using some properties of random walks on graphs it defines a measurement of the structural similarity between nodes and between communities, thus defining a distance which can be used in a hierarchical clustering algorithm","c9feef38":"### Abstract\nWe use unweighed undirected graphs to represent connections between people and visualize the clustering between them. Different metrics are used to compare optimal and hierarchical community detection methods. In addition, we explore a semi-supervised location-based approach with label propagation. Finally, we do graph embedding with Node2Vec and use k-means for clustering.","840b8f54":"Unfortunately there is no workaround for a case like this, where both the person's name and his description are common, but these records are not that likely and should not affect the final clustering."}}