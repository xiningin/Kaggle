{"cell_type":{"e7707792":"code","4b23dd02":"code","c592bfbd":"code","8e5a8510":"code","f3a2ff4e":"code","c1e05b98":"code","93668dbe":"code","97fddfab":"code","638961e0":"code","892c0200":"code","facfa8e5":"code","4b9e9678":"code","57a68a30":"code","7c55f9ce":"code","4a9c203e":"code","3573d034":"code","28e8144b":"code","eb48a84e":"code","2dec78b6":"code","4d6889e3":"code","2d0e196a":"code","94c0f666":"code","9022c668":"code","44742d1f":"code","e7bef81c":"code","babdd1e2":"code","83a4816a":"code","2acb5b53":"code","11890c32":"code","3b85ee7b":"code","0e348b2f":"code","045c8db7":"code","d453e9db":"code","01035968":"code","b8e34e69":"code","feaac424":"code","b30c231d":"code","1f554f86":"code","2eee53a3":"code","bd7465c0":"code","d9c57bed":"code","b395bfd0":"code","5619c736":"code","2eb191a1":"markdown","5acfdb2c":"markdown","fe622a49":"markdown","43882bc6":"markdown","468fd434":"markdown","a16df855":"markdown","4a753692":"markdown","a6b9950a":"markdown","f2333286":"markdown","91b116fe":"markdown","2471c18f":"markdown","c4b5c235":"markdown","c2973344":"markdown","58071ac6":"markdown","804c5b08":"markdown","b460686c":"markdown","9fedbdbb":"markdown","96b22801":"markdown","e3ac711a":"markdown","9f968432":"markdown","c18a0803":"markdown","bf7d8a92":"markdown","97ec9299":"markdown","aad4bcf9":"markdown"},"source":{"e7707792":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","4b23dd02":"# Following PEP8 makes a Python programmer healthy and wealthy\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold","c592bfbd":"df_train = pd.read_csv(\"..\/input\/berlin-airbnb-data\/listings.csv\")\nprint(f'Train data has {df_train.shape[0]} rows and {df_train.shape[1]} colummns')","8e5a8510":"df_train.head(1)","f3a2ff4e":"# checking for null values in train data\ndf_train.isna().sum()","c1e05b98":"df_test = pd.read_csv('..\/input\/berlin-test\/test_cleaned.csv')\nprint(f'Test data has {df_test.shape[0]} rows and {df_test.shape[1]} colummns')","93668dbe":"df_test.head(1)","97fddfab":"# removing attributes of test dataset not in train\ndf_test.drop(['Unnamed: 0'], axis=1, inplace=True)","638961e0":"import plotly.express as px\n\nfig = px.line_mapbox(df_train, lat=\"latitude\", lon=\"longitude\", color=\"neighbourhood_group\", zoom=1, height=550)\n\nfig.update_layout(mapbox_style=\"stamen-terrain\", mapbox_zoom=9, mapbox_center_lat = 52.5027778,\n    margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()\n","892c0200":"outlier_ids = set()\noutlier_ids = outlier_ids.union(set(df_train[df_train['price']>400].index.values))\noutlier_ids = outlier_ids.union(set(df_train[df_train['price']==0].index.values))","facfa8e5":"print(f'No of outlier columns: {len(outlier_ids)}')","4b9e9678":"df_train_viz = df_train.drop(list(outlier_ids))","57a68a30":"import seaborn as sns\nsns.distplot(df_train_viz['price'])","7c55f9ce":"fig = px.scatter(df_train_viz, x=\"room_type\", y=\"price\", color=\"room_type\")\nfig.show()","4a9c203e":"fig = px.scatter_matrix(df_train_viz, dimensions=[\"availability_365\", \"room_type\", \"price\", \"neighbourhood\"], color=\"neighbourhood\")\nfig.show()","3573d034":"target=df_train['price']\ndf_train.drop(['price'],axis=1,inplace=True)","28e8144b":"# check for null values\ndf_train.isna().sum()","eb48a84e":"def fill(df):\n    df['name'].fillna(method='ffill',inplace=True)\n    df['host_name'].fillna(method='ffill',inplace=True)\n    df['last_review'].fillna(method='ffill',inplace=True)\n    df['reviews_per_month'].fillna(method='ffill',inplace=True)\n    df=df.fillna(df.mean())\n    return df","2dec78b6":"df_train=fill(df_train)","4d6889e3":"# Ensure if data is cleaned or not\ndf_train.isna().sum()","2d0e196a":"def date_features(df):\n    df['last_review']=pd.to_datetime(df['last_review'],  errors='coerce')\n    df['year']=df['last_review'].dt.year\n    df['month']=df['last_review'].dt.month\n    df['day']=df['last_review'].dt.day\n    #df['day']=df['day'].astype(int)\n    df.drop(['last_review'],axis=1,inplace=True)\n    \n    return df\n\ndf_train=date_features(df_train)","94c0f666":"from geopy.distance import great_circle","9022c668":"def distance_to_mid(lat, lon):\n    berlin_centre = (52.5027778, 13.404166666666667)\n    accommodation = (lat, lon)\n    return great_circle(berlin_centre, accommodation).km\n","44742d1f":"df_train.columns","e7bef81c":"df_train['distance'] = df_train.apply(lambda x: distance_to_mid(x.latitude, x.longitude), axis=1)\ndf_test['distance']=df_test.apply(lambda x: distance_to_mid(x.latitude, x.longitude), axis=1)","babdd1e2":"cat=[]\nfor col in df_train.columns:\n    if type(df_train[col][0])==str :\n        cat.append(col)","83a4816a":"# Categorical variables\ncat","2acb5b53":"final=pd.concat([df_train,df_test])","11890c32":"from sklearn.preprocessing import LabelEncoder\n\nfor col in cat:\n    lb=LabelEncoder()\n    final[col]=lb.fit_transform(final[col].values)\n    \n    \ntrain=final[:len(df_train)]\ntest=final[len(df_train):]\n","3b85ee7b":"for col in df_train.columns:\n#     if col not in cat:\n        \n    minmax=MinMaxScaler()\n    train[col] = minmax.fit_transform(train[col].values.reshape(-1,1))\n    test[col]=minmax.transform(test[col].values.reshape(-1,1))\n","0e348b2f":"n_folds=5\nfinal_predictions=np.zeros((1,len(test)))\n# using KFold for cross validation\nfolds= KFold(n_splits=n_folds,shuffle=False)\n\nfor fold_,(train_idx,test_idx) in enumerate(folds.split(train)):\n    x_train=train.iloc[train_idx]\n    y_train=target[train_idx]\n    x_val=train.iloc[test_idx]\n    y_val=target[test_idx]\n\n    lgbm=lgb.LGBMRegressor(max_depth=15, n_leaves=40)\n    lgbm.fit(x_train,y_train,eval_set=(x_val,y_val),eval_metric='mea',verbose=False)\n    \n    y_pre=lgbm.predict(x_val)\n    y_pre=np.clip(y_pre,1,400)\n    y_pre = np.around(y_pre, decimals=1)\n    print(\"mse\",np.sqrt(mean_squared_error(y_pre,y_val)))\n    \n    predictions=lgbm.predict(test)\n    final_predictions+=predictions","045c8db7":"final_predictions=final_predictions\/n_folds","d453e9db":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import StratifiedKFold\n\nn_folds=5\nfinal_predictions_two=np.zeros((1,len(test)))\nfolds= KFold(n_splits=n_folds,shuffle=False,random_state=12)\nfor fold_,(train_idx,test_idx) in enumerate(folds.split(train,target)):\n    x_train=train.iloc[train_idx]\n    y_train=target[train_idx]\n    x_val=train.iloc[test_idx]\n    y_val=target[test_idx]\n    \n    rf=RandomForestRegressor(n_estimators=300,verbose=True,max_depth=10)\n    rf.fit(x_train,y_train)\n    \n    y_pre=lgbm.predict(x_val)\n    y_pre=np.clip(y_pre,0,400)\n    print('mse ',mean_squared_error(y_pre,y_val) )\n    \n    predictions=lgbm.predict(test)\n    final_predictions_two+=predictions","01035968":"final_predictions_two=final_predictions_two\/n_folds","b8e34e69":"import xgboost as xgb\n\ndef model(X_train,y_train,final_test,n_splits=3):\n    n_folds=n_splits\n    scores=[]\n    pars = {\n        'colsample_bytree': 0.7,                 \n        'learning_rate': 0.08,\n        'max_depth': 15,\n        'subsample': 1,\n        'objective':'reg:squarederror',\n        'eval_metric':'rmse',\n        'min_child_weight':5,\n        'gamma':0.25,\n        'n_estimators':500\n    }\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    y_pre=np.zeros((1,len(final_test)),dtype=float)\n    final_test=xgb.DMatrix(final_test)\n\n\n    for train_index, val_index in kf.split(X_train):\n        train_X = X_train.iloc[train_index]\n        val_X = X_train.iloc[val_index]\n        train_y = y_train[train_index]\n        val_y = y_train[val_index]\n        xgb_train = xgb.DMatrix(train_X, train_y)\n        xgb_eval = xgb.DMatrix(val_X, val_y)\n\n        xgb_model = xgb.train(pars,\n                      xgb_train,\n                      num_boost_round=1000,\n                      evals=[(xgb_train, 'train'), (xgb_eval, 'val')],\n                      verbose_eval=False,\n                      early_stopping_rounds=20\n                     )\n\n        val_X=xgb.DMatrix(val_X)\n        pred_val=xgb_model.predict(val_X)\n        score=np.sqrt(mean_squared_error(pred_val,val_y))\n        scores.append(score)\n        #print('score :',score)\n        pred=xgb_model.predict(final_test)\n        y_pre+=pred\n\n    pred = y_pre\/n_folds\n    print('Mean score:',np.mean(scores))\n    \n    return xgb_model,pred\n","feaac424":"xgb_model,final_predictions_three=model(train,target,test,n_folds)","b30c231d":"final_predictions=np.average([final_predictions,final_predictions_two,final_predictions_three],axis=0,weights=[0.2,0.4,0.4])","1f554f86":"# Random forest feature importance\n\nprint(rf.feature_importances_)","2eee53a3":"# from rfpimp import permutation_importances\n\n# def r2(rf, X_train, y_train):\n#     return mean_squared_error(y_train, rf.predict(X_train))\n\n# perm_imp_rfpimp = permutation_importances(rf, X_train, y_train, r2)","bd7465c0":"import eli5\nfrom eli5.sklearn import PermutationImportance\n","d9c57bed":"perm = PermutationImportance(rf).fit(x_val, y_val)\neli5.show_weights(perm)","b395bfd0":"fig, ax = plt.subplots(figsize=(10,10))\nxgb.plot_importance(xgb_model, max_num_features=50, height=0.5, ax=ax,importance_type='gain')\nplt.show()","5619c736":"sub=pd.DataFrame()\nsub['id']=df_test['id']\nsub['price']=final_predictions[0]\nsub.to_csv('submission_berlin.csv',index=False)\nsub.head()","2eb191a1":"## Train data","5acfdb2c":"## Test data","fe622a49":"# Baseline predictions with LightBGM <a class=\"anchor\" id=\"baseline\"><\/a>","43882bc6":"# Ensembling with Random forest and XGBoost <a class=\"anchor\" id=\"ensembling\"><\/a>","468fd434":"## Scatter matrix","a16df855":"## Distance of Apartment from Center of berlin","4a753692":"# Berlin Apartment price prediction\n\nThis is one of the kerenels which I wrote to participate in a Kaggle In-class competition and eventually won first prize in it! \n\nAirbnb has successfully disrupted the traditional hospitality industry as more and more travelers decide to use Airbnb as their primary accommodation provider. Since its inception in 2008, Airbnb has seen an enormous growth, with the number of rentals listed on its website growing exponentially each year.In Germany, no city is more popular than Berlin. That implies that Berlin is one of the hottest markets for Airbnb in Europe, with over 22,552 listings as of November 2018. With a size of 891 km\u00b2, this means there are roughly 25 homes being rented out per km\u00b2 in Berlin on Airbnb!\n\nOur task with this kernel is to have a generalised machine learning model to a dataset in order to predict Airbnb rental prices. After fitting on data, you can late submit your predictions to [Kaggle In-class competition](https:\/\/www.kaggle.com\/c\/data-scientist-q1\/) and see how good your results!\n\nI have tried to train my ML model after cleaning the data, and a bit of feature engineering. Our solution uses an ensemble of the algorithms like Random Forest, LightGBM and XGBoost.This notebook can be used for almost any regression problem\n\n![housesbanner.png](attachment:housesbanner.png)\nPicture from Kaggle Intermediate ML Challenge\n\nIf you like to work with it, just fork and build upon this for any regression task\n\nAnd please... **don't forget to UPVOTE! ;-p**","a6b9950a":"We are going to use label encoding for handling categorical variables! In this method we change every categorical data to a number.That is each type will be subtuted by a number.for example we will substitute 1 for Grandmaster,2 for master ,3 for expert etc.. For implementing this we will first import Labelencoder from sklearn module.\n\n![label_encoding.png](attachment:label_encoding.png)\n\nIf you want to check out more encoding methods [check out this amazing kerenel](https:\/\/www.kaggle.com\/shahules\/an-overview-of-encoding-techniques)","f2333286":"[Download file](submission_berlin.csv)\n\n\nNow you can submit this kernel and evaluate your performance in [the competition](https:\/\/www.kaggle.com\/c\/data-scientist-q1\/submit)","91b116fe":"# Data exploration & cleaning <a class=\"anchor\" id=\"eda\"><\/a>\n\n","2471c18f":"# Looking at the Data <a class=\"anchor\" id=\"peek\"><\/a>","c4b5c235":"**It's evident from this visualisation that majority of Apartments in berlin in fall in category of apparments between 30-125 Euros**","c2973344":"# Final model submission <a class=\"anchor\" id=\"submit\"><\/a>","58071ac6":"## Analysing apartments by their neighbourhood groups","804c5b08":"You are able to see where each apartment is located in this graph and is colured according to the neighbourhood in berlin it belongs to","b460686c":"# Data preparation & Feature engineering <a class=\"anchor\" id=\"dataprep\"><\/a>\n","9fedbdbb":"## Handling categorical variables","96b22801":"## Scaling","e3ac711a":"## Cleaning data","9f968432":"# Feature importance <a class=\"anchor\" id=\"feature_imp\"><\/a>","c18a0803":"### Table of contents\n\n1. [Prepare to start](#prepare)\n2. [Looking at the data](#peek) \n3. [Data exploration & cleaning](#eda) \n4. [Data preparation & Feature engineering](#dataprep)\n5. [Baseline predictions with LightBGM](#baseline)\n6. [Ensembling with random forest and Xgboost](#ensembling)\n7. [Feature importance](#feature_imp)\n8. [Final model submission](#submit)","bf7d8a92":"# Prepare to Start <a class=\"anchor\" id=\"prepare\"><\/a>\n## Importing Libraries","97ec9299":"## Scatter plot of categories of  rooms\n","aad4bcf9":"### Removing outliers in train data"}}