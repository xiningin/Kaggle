{"cell_type":{"cc03cbe9":"code","01bd05b3":"code","75054e3e":"code","d1f1a203":"code","bbd9a717":"code","ca3320b4":"code","1cf2f1a3":"code","88efa39c":"code","95e7dc2e":"code","cbeab4f8":"code","26bea539":"code","039efdf2":"code","183f7cb9":"code","25bfefcd":"code","5c550fb2":"code","39eb9c2f":"code","1d04be97":"code","ba2f5833":"code","0c54b34b":"code","7840eb16":"code","7737df78":"code","6862102c":"code","d80084ce":"code","569a5a1f":"code","1a89e9a3":"code","c08b262a":"code","cc9d5d3c":"code","4d8f5071":"code","3a871f92":"code","c851e02f":"code","899508ec":"code","39afe0c3":"code","7073712c":"code","98862c75":"code","9e9b74fe":"code","aaca05f2":"code","a0e0c210":"code","a0965c6e":"code","d08795ec":"code","763f26af":"code","ff7c9f8e":"code","31de9deb":"code","8658fd64":"code","ba89c048":"code","1e78c827":"code","a703c8a1":"code","4bf5bca7":"code","4194383a":"code","7fa43519":"code","3321df9c":"markdown","d5fd3d5b":"markdown","e719e6be":"markdown","47a82d16":"markdown","30df5b1c":"markdown","bb48d04f":"markdown","cfe2ede6":"markdown","64705851":"markdown","da8ec208":"markdown","92f6452f":"markdown","6e0150e0":"markdown","9105c2b4":"markdown","f89ee331":"markdown"},"source":{"cc03cbe9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","01bd05b3":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","75054e3e":"fish=pd.read_csv(\"\/kaggle\/input\/fish-market\/Fish.csv\")","d1f1a203":"fish.head(5)","bbd9a717":"fish.info()","ca3320b4":"fish.describe()","1cf2f1a3":"round(fish.isnull().sum()\/len(fish),2)","88efa39c":"fish.drop_duplicates(inplace=True)","95e7dc2e":"plt.figure(figsize=(20,15))\nsns.pairplot(fish)\nplt.show()","cbeab4f8":"plt.figure(figsize=(20,15))\nplt.subplot(1,2,1)\nsns.boxplot(x='Species',y='Weight',data=fish)","26bea539":"plt.figure(figsize=(20,15))\nsns.heatmap(fish.corr())\nplt.show()","039efdf2":"Species=pd.get_dummies(fish['Species'],drop_first=True)\nfish=pd.concat([fish,Species],axis=1)\nfish.drop(['Species'],axis=1,inplace=True)","183f7cb9":"fish.head(5)","25bfefcd":"from sklearn.model_selection import train_test_split\nnp.random.seed(0)\ndf_train,df_test=train_test_split(fish,test_size=0.2,random_state=42)","5c550fb2":"df_train.shape","39eb9c2f":"df_test.shape","1d04be97":"from sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()","ba2f5833":"num_vars=['Weight','Length1','Length2','Length3','Height','Width']","0c54b34b":"df_train[num_vars]=scaler.fit_transform(df_train[num_vars])\ndf_test[num_vars]=scaler.transform(df_test[num_vars])","7840eb16":"plt.figure(figsize=(20,15))\nsns.heatmap(df_train.corr())\nplt.show()","7737df78":"df_train.head(5)","6862102c":"y_train=df_train.pop('Weight')\nX_train=df_train","d80084ce":"from sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE","569a5a1f":"lm=LinearRegression()\nlm.fit(X_train,y_train)","1a89e9a3":"rfe=RFE(lm,7)\nrfe.fit(X_train,y_train)","c08b262a":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","cc9d5d3c":"col=X_train.columns[rfe.support_]\ncol","4d8f5071":"X_train_rfe=X_train[col]","3a871f92":"import statsmodels.api as sm","c851e02f":"X_train_sm1=sm.add_constant(X_train_rfe)","899508ec":"lm1=sm.OLS(y_train,X_train_sm1).fit()","39afe0c3":"print(lm1.summary())","7073712c":"# Calculate the VIFs for the model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_rfe\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","98862c75":"X_train_new = X_train_rfe.drop([\"Length3\"], axis = 1)","9e9b74fe":"X_train_sm2 = sm.add_constant(X_train_new)\n\n# Create a first fitted model\nlr2 = sm.OLS(y_train, X_train_sm2).fit()","aaca05f2":"print(lr2.summary())","a0e0c210":"X_train_new = X_train_new.drop([\"Height\"], axis = 1)\nX_train_sm3 = sm.add_constant(X_train_new)\n\n# Create a first fitted model\nlr3 = sm.OLS(y_train, X_train_sm3).fit()","a0965c6e":"print(lr3.summary())","d08795ec":"X_train_new = X_train_new.drop([\"Length1\"], axis = 1)\nX_train_sm4 = sm.add_constant(X_train_new)\n\n# Create a first fitted model\nlr4 = sm.OLS(y_train, X_train_sm4).fit()","763f26af":"print(lr4.summary())","ff7c9f8e":"# Calculate the VIFs for the final model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","31de9deb":"y_train_pred=lr4.predict(X_train_sm4)","8658fd64":"res = y_train-y_train_pred\n# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((res), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18) ;","ba89c048":"y_test=df_test.pop('Weight')\nX_test=df_test","1e78c827":"X_test_new=sm.add_constant(X_test)","a703c8a1":"#Selecting the variables that were part of final model.\ncol1=X_train_new.columns\nX_test=X_test[col1]\n# Adding constant variable to test dataframe\nX_test_lm4 = sm.add_constant(X_test)\nX_test_lm4.info()","4bf5bca7":"y_test_pred=lr4.predict(X_test_lm4)","4194383a":"from sklearn.metrics import r2_score\nr2_score(y_train,y_train_pred)","7fa43519":"from sklearn.metrics import r2_score\nr2_score(y_test,y_test_pred)","3321df9c":"# Checking Assumptions","d5fd3d5b":"we have got a better accuracy around 94% on test data set.","e719e6be":"# train test split and feature scaling","47a82d16":"## Conclusion\n\n- If you have any suggestions or suggestions, please write to me. I wil be happy to help.\n\n- Thank you for your suggestion and votes ;)","30df5b1c":"# Making prediction on test data set","bb48d04f":"# Understanding the data","cfe2ede6":"1.Error terms are normally distributed with mean zero (not X, Y)","64705851":"# EDA","da8ec208":"# Model Evaluation","92f6452f":"From the above histogram, we could see that the Residuals are normally distributed. Hence our assumption for our model is valid.","6e0150e0":"# Feature selection and model buliding using statsmodels api","9105c2b4":"### Creating Dummy Variables","f89ee331":"### Observations\n- This model looks good, as there are VERY LOW Multicollinearity between the predictors and the p-values for all the predictors seems to be significant. For now, we will consider this as our final model (unless the Test data metrics are not significantly close to this number)."}}