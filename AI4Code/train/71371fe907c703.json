{"cell_type":{"4e3e732c":"code","ab32fbb7":"code","85f2aff9":"code","75f67c71":"code","04bdd017":"code","7256c919":"code","cc42fc8e":"code","6385184c":"code","08ca3ef3":"code","0a0d46ec":"code","38d0e591":"code","34996e00":"code","f353e61c":"code","f28c9da1":"code","c923411c":"code","c3d45834":"code","7da305b1":"code","1303075c":"code","41bf7ef4":"code","c6530c7a":"code","e1624d25":"code","fa80c2a9":"code","b20dc85e":"code","da7f5da8":"code","778e1072":"code","505fd8cd":"code","b202161d":"code","4b1df9a3":"code","90dd052b":"code","87f33010":"code","3c2cd786":"code","93cf55f1":"code","7b341d53":"code","15fd0a8c":"code","0c64990a":"code","f4a08e79":"code","d6e3d929":"code","3fe2010a":"code","361a0f3d":"code","db0c5dc0":"code","0d3d3b39":"code","327d8b2a":"markdown","d652f254":"markdown","df450dbf":"markdown","42116815":"markdown","5cb5a6a9":"markdown","61243b31":"markdown","dfc8e036":"markdown","ba42edee":"markdown","4518356f":"markdown","13bbf87b":"markdown","60ac4e46":"markdown","e1abf2b9":"markdown","4f72d07a":"markdown","bac2ba93":"markdown","d434be97":"markdown","99e9106a":"markdown","54347613":"markdown","b44fd274":"markdown","adbb29ec":"markdown","bc09c7e2":"markdown"},"source":{"4e3e732c":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","ab32fbb7":"data = pd.read_csv('\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_3C_weka.csv') #read to file\ndata","85f2aff9":"data[\"class\"] = [1 if each == \"Hernia\" else 0 for each in data[\"class\"]]\n# Hernia = 1\n# Normal = 0","75f67c71":"data","04bdd017":"data.info()","7256c919":"df.describe().T","cc42fc8e":"plt.scatter(data.pelvic_radius,data.sacral_slope)\nplt.xlabel(\"pelvic radius\")\nplt.ylabel(\"sacral slope\")\nplt.show()","6385184c":"from sklearn.linear_model import LinearRegression\n\nlinear_reg = LinearRegression()\n\n# sacral_slope VS pelvic_radius\nprint(\"sacral_slope type: \", type(data.sacral_slope))\nprint(\"pelvic_radius type: \", type(data.pelvic_radius))","08ca3ef3":"x = data.sacral_slope.values.reshape(-1,1)\ny = data.pelvic_radius.values.reshape(-1,1)\n\nlinear_reg.fit(x,y)","0a0d46ec":"b0 = linear_reg.predict([[0]]) # You can write the desired value instead of 0. Here we wrote 0 to find the point where the line crosses the y-axis\nprint(\"b0: \", b0)","38d0e591":"# another way: \nb0 = linear_reg.intercept_\nprint(\"b0: \", b0)","34996e00":"b1 = linear_reg.coef_\nprint(\"b1 = \", b1)","f353e61c":"print(linear_reg.predict([[45]]))","f28c9da1":"array = np.array([5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100]).reshape(-1,1)\nplt.scatter(x,y)\ny_head = linear_reg.predict(array)\nplt.plot(array,y_head,color=\"red\")\nplt.show()","c923411c":"y = data[\"class\"].values.reshape(-1,1)\nx = data.drop([\"class\"],axis=1).values # axis = 1 => for columns","c3d45834":"multiple_linear_regression = LinearRegression()\nmultiple_linear_regression.fit(x,y)","7da305b1":"print(\"b0: \",multiple_linear_regression.intercept_)  # The point where it intersects the y axis\nprint(\"b1,b2,b3,b4,b5,b6: \",multiple_linear_regression.coef_) # The slopes of the line","1303075c":"multiple_linear_regression.predict(np.array([[\n    57.26,\n    19.98,\n    38.63,\n    31.43,\n    115.098,\n    4.4512\n]]))","41bf7ef4":"x = data[\"class\"].values.reshape(-1,1)\ny = data[\"sacral_slope\"].values.reshape(-1,1)","c6530c7a":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(x,y)","e1624d25":"y_head = tree_reg.predict(x)\n\nplt.scatter(x,y,color=\"red\")\nplt.plot(x,y_head,color=\"green\")\nplt.xlabel(\"Grandstand Level\")\nplt.ylabel(\"Price\")\nplt.show()","fa80c2a9":"plt.scatter(data[data[\"class\"]==1].pelvic_radius,data[data[\"class\"]==1].sacral_slope,color=\"red\",label=\"hernia\",alpha= 0.6)\nplt.scatter(data[data[\"class\"]==0].pelvic_radius,data[data[\"class\"]==0].sacral_slope,color=\"green\",label=\"normal\",alpha= 0.6)\nplt.xlabel(\"pelvic_radius\")\nplt.ylabel(\"sacral_slope\")\nplt.legend()\nplt.show()","b20dc85e":"y = data[\"class\"].values\nx_data = data.drop([\"class\"],axis=1)","da7f5da8":"x = (x_data - np.min(x_data))\/(np.max(x_data) - np.min(x_data))","778e1072":"# train test split\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=1)","505fd8cd":"# knn model\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=3) # n_neighbors => key count\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)","b202161d":"prediction # looks nice :)","4b1df9a3":"print(\"{} knn score: {} \".format(3,knn.score(x_test,y_test)*100)) # accuracy = 79.5%","90dd052b":"# find k value\nscore_list = []\n\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors=each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","87f33010":"print(\"{} knn score: {} \".format(13,knn.score(x_test,y_test)*100))","3c2cd786":"from IPython.display import Image\nImage(url=\"https:\/\/www.researchgate.net\/publication\/304611323\/figure\/fig8\/AS:668377215406089@1536364954428\/Classification-of-data-by-support-vector-machine-SVM.png\")","93cf55f1":"y = data[\"class\"].values\nx_data = data.drop([\"class\"],axis=1)\n\n# normalazition\nx = (x_data - np.min(x_data))\/(np.max(x_data) - np.min(x_data))\n\n# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=1)","7b341d53":"from sklearn.svm import SVC\n\nsvm = SVC(random_state=42)\nsvm.fit(x_train,y_train)\n\nprint(\"Accuracy of SVM algo: \", svm.score(x_test,y_test)*100)","15fd0a8c":"y = data[\"class\"].values\nx_data = data.drop([\"class\"],axis=1)\n\n# normalazition\nx = (x_data - np.min(x_data))\/(np.max(x_data) - np.min(x_data))\n\n# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=1)","0c64990a":"from sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\nnb.fit(x_train,y_train)\nprint(\"Accuracy of naive_bayes algo:\",nb.score(x_test,y_test)*100)","f4a08e79":"y = data[\"class\"].values\nx_data = data.drop([\"class\"],axis=1)\n\n# normalazition\nx = (x_data - np.min(x_data))\/(np.max(x_data) - np.min(x_data))\n\n# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=1)","d6e3d929":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(random_state=42)\ndt.fit(x_train,y_train)\n\nprint(\"Score: \", dt.score(x_test,y_test)*100)","3fe2010a":"y = data[\"class\"].values\nx_data = data.drop([\"class\"],axis=1)\n\n# normalazition\nx = (x_data - np.min(x_data))\/(np.max(x_data) - np.min(x_data))\n\n# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=1)","361a0f3d":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=100,random_state=42) # n_estimators = number of trees\nrf.fit(x_train,y_train)\nprint(\"Score: \", rf.score(x_test,y_test)*100)","db0c5dc0":"y = data[\"class\"].values.reshape(-1,1)\nx_data = data.drop([\"class\"],axis=1).values\n\n# normalazition\nx = (x_data - np.min(x_data))\/(np.max(x_data) - np.min(x_data))\n\nlinear_reg.fit(x,y)\ny_head = linear_reg.predict(x) ","0d3d3b39":"from sklearn.metrics import r2_score\n\nprint(\"r_square score: \",r2_score(y,y_head))","327d8b2a":"By examining the values \u200b\u200babove, we can better understand our data.","d652f254":"# Naive Bayes Classification\nNaive Bayes classifiers are a collection of classification algorithms based on Bayes' Theorem. It is not a single algorithm but a family of algorithms where all of them share a common principle, i.e. every pair of features being classified is independent of each other.","df450dbf":"# Support Vector Machine (SVM)\nA support vector machine (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they're able to categorize new text. So you're working on a text classification problem.","42116815":"**As can be seen, we did not get a complete yield in the chart here. This part is better described at the bottom.**","5cb5a6a9":"# Configuring the Data\n","61243b31":"# Random Forest Classification\nRandom forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.","dfc8e036":"Since it is not 1, we can understand that the result is not a hernia.","ba42edee":"# KNN Algorithm\nThe k-nearest neighbors (KNN) algorithm is a simple, supervised machine learning algorithm that can be used to solve both classification and regression problems.","4518356f":"Now I have to enter the values. For example, you are a doctor and you have this data. When we enter this data in order, it will show us whether there is a hernia. <br><br>\n\n* pelvic_invidince = 57.26\n* pelvic_tilt = 18.98\n* lumbar_lordosis_angle = 38.63\n* sacral_slope = 31.43\n* pelvic_radius = 115.098\n* degree_spondylolisthesis = 4.4512","13bbf87b":"# Linear Regression\nIn statistics, linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. <br>\n\n* y = b0 + b1*x\n* b0 = constant\n* b1 = coeff\n* x = value","60ac4e46":"For example: if sacral slope data is 45, pelvic radius data takes the following value.","e1abf2b9":"# Evaluation Regression Models\nAfter building a number of different regression models, there is a wealth of criteria by which they can be evaluated and compared. RMSE is a popular formula to measure the error rate of a regression model.","4f72d07a":"# Multiple Linear Regression\nMultiple linear regression (MLR), also known simply as multiple regression, is a statistical technique that uses several explanatory variables to predict the outcome of a response variable. Multiple regression is an extension of linear (OLS) regression that uses just one explanatory variable.\n","bac2ba93":"#  Decision Tree Classification","d434be97":"When we examine the table, we see that the most appropriate value is 13.","99e9106a":"# Polynomial Regression\nPolynomial Regression is a form of linear regression in which the relationship between the independent variable x and dependent variable y is modeled as an nth degree polynomial. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y |x)\n","54347613":"### normalization","b44fd274":"We can use the following method to see the change of pelvic radius values \u200b\u200baccording to sacral slope values.","adbb29ec":"- Choose K value\n- Find the nearest data points in K\n- Calculate how many of the class nearest neighbors in K\n- Determine which class of point or data we tested belongs to","bc09c7e2":"# Decision Tree Regression\nDecision Tree - Regression. Decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. ... Decision trees can handle both categorical and numerical data."}}