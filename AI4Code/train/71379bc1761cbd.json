{"cell_type":{"4540dfbb":"code","fe22e707":"code","ae6cf127":"code","bdc154bd":"code","c0566549":"code","01f0d208":"code","c7841f2c":"code","e8cc6919":"code","4760a262":"code","0cb8033c":"code","f4c9cc80":"code","b6bc6170":"code","1aaf5f96":"code","1763bbff":"code","cb06b56b":"code","1d386baf":"code","f78967a1":"code","ddaa6af8":"code","d56bc2ee":"code","ff95b30d":"code","8012edac":"code","71554301":"code","eced189a":"code","b4470d9e":"code","77993b41":"code","12e57e9c":"code","1a0c9acb":"code","fcf1a522":"markdown","2d0c44e2":"markdown","23b38b77":"markdown","378de49f":"markdown","44e03fa7":"markdown","3281867e":"markdown","cb3bd307":"markdown","88ba474a":"markdown","dbc85673":"markdown","6f06d486":"markdown","6e6a761a":"markdown","e4bd7ef9":"markdown","9cee8f42":"markdown","a5ab36ba":"markdown","5c65cbcb":"markdown","dd1c9ed6":"markdown","c6165dd4":"markdown","be31b570":"markdown","b60cb0df":"markdown","4e0bc496":"markdown","6123f188":"markdown","fb934553":"markdown","480f89ae":"markdown","9e13b8c9":"markdown","fb1db8f1":"markdown","da45021d":"markdown","52f1886c":"markdown","a58fa893":"markdown","51e83fe9":"markdown","7f19d45a":"markdown","d8ad98b7":"markdown","5ea5ac50":"markdown"},"source":{"4540dfbb":"#import libraries \n\n#structures\nimport numpy as np\nimport pandas as pd\n\n#visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nfrom mpl_toolkits.mplot3d import Axes3D\n\n#get model duration\nimport time\nfrom datetime import date\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fe22e707":"#load dataset\ndata = '..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv'\ndataset = pd.read_csv(data)\ndataset.shape","ae6cf127":"dataset.dtypes","bdc154bd":"dataset.describe()","c0566549":"#check for missing data\ndataset.isnull().any().any()","01f0d208":"#check for unreasonable data\ndataset.applymap(np.isreal)","c7841f2c":"sns_plot = sns.pairplot(dataset)","e8cc6919":"sns_plot = sns.distplot(dataset['quality'])","4760a262":"#set x and y\nfrom sklearn.preprocessing import StandardScaler\n\nX = dataset.iloc[:,0:11]\ny = dataset['quality']\n\n#stadardize data\nX_scaled = StandardScaler().fit_transform(X)","0cb8033c":"dataset.head()","f4c9cc80":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler","b6bc6170":"pca = PCA(n_components=6)\npc_X = pca.fit_transform(X_scaled)\npc_columns = ['pc1','pc2','pc3','pc4','pc5','pc6']\nprint(pca.explained_variance_ratio_.sum())","1aaf5f96":"print(pca.explained_variance_ratio_)","1763bbff":"#get correlation map\ncorr_mat=dataset.corr()","cb06b56b":"#visualise data\nplt.figure(figsize=(13,5))\nsns_plot=sns.heatmap(data=corr_mat, annot=True, cmap='GnBu')\nplt.show()","1d386baf":"#check for highly correlated values to be removed\ntarget = 'quality'\ncandidates = corr_mat.index[\n    (corr_mat[target] > 0.5) | (corr_mat[target] < -0.5)\n].values\ncandidates = candidates[candidates != target]\nprint('Correlated to', target, ': ', candidates)","f78967a1":"#import libraries\nfrom sklearn.metrics import f1_score\nfrom sklearn.cluster import KMeans","ddaa6af8":"#try to find optimal k using the elbow method\nwcss = []\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300, n_init=12, random_state=0)\n    kmeans.fit(X_scaled)\n    wcss.append(kmeans.inertia_)\nf3, ax = plt.subplots(figsize=(8, 6))\nplt.plot(range(1,11),wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","d56bc2ee":"#Applying kmeans to the dataset, set k=2\nkmeans = KMeans(n_clusters = 2)\nstart_time = time.time()\nclusters = kmeans.fit_predict(X_scaled)\ntoday = date.today()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\nlabels = kmeans.labels_","ff95b30d":"#2D plot\ncolors = 'rgbkcmy'\nfor i in np.unique(clusters):\n    plt.scatter(X_scaled[clusters==i,0],\n               X_scaled[clusters==i,1],\n               color=colors[i], label='Cluster' + str(i+1))\nplt.legend()","8012edac":"# Visualise the clusterds considerig fixed acidity, residual sugar, and alcohol\nfig = plt.figure(figsize=(20, 15))\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=15, azim=40)\n\nax.scatter(X_scaled[:,0], X_scaled[:,3], X_scaled[:,10],c=y, edgecolor='k')\nax.set_xlabel('Acidity')\nax.set_ylabel('Sugar')\nax.set_zlabel('Alcohol')\nax.set_title('K=2: Acidity, Sugar, Alcohol', size=22)","71554301":"#evaluate model\nfrom sklearn import metrics\nfrom sklearn.metrics import pairwise_distances\nmetrics.silhouette_score(X_scaled, labels, metric='euclidean')","eced189a":"kmeans.inertia_","b4470d9e":"#Applying kmeans to the dataset, set k=2\nkmeans = KMeans(n_clusters = 2)\nstart_time = time.time()\nclusters = kmeans.fit_predict(pc_X)\ntoday = date.today()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\nlabels = kmeans.labels_","77993b41":"#2D plot\ncolors = 'rgbkcmy'\nfor i in np.unique(clusters):\n    plt.scatter(pc_X[clusters==i,0],\n               pc_X[clusters==i,1],\n               color=colors[i], label='Cluster' + str(i+1))\nplt.legend()","12e57e9c":"#evaluate model\nmetrics.silhouette_score(pc_X, labels, metric='euclidean')","1a0c9acb":"kmeans.inertia_","fcf1a522":"**Direction of relationship** <br>\nAcidity (-0.39), chlorides (-0.13), free sulfur dioxide (-0.051), total sulfur dioxide (-0.19), density (-0.17) and PH (-0.058) are negatively correlated to the quality of wine; as these variables decrease, the quality of wine will increase vice versa. <br> <br>\n\nConversely, fixed acidity (0.12), citric acid, residual sugar (0.014), sulphates (0.25) and alcohol (0.48) are positively correlated to the quality of wine; as these variables increase, the quality of wine improves.","2d0c44e2":"Training time \u2013 0.062 seconds Training time is observed to have reduced slightly.","23b38b77":"## Data Visualization of Clustering","378de49f":"Now, the silhouette score of the model will be measured. The silhouette score ranges from -1 to +1. <br>\nThe high silhouette score indicates that the objects are well matched to its own cluster and not to its neighbouring clusters. <br>\n(The higher the silhouette score \u2013 the better the clustering)","44e03fa7":"An extremely high inertia value of 14330.119 was obtained. It is an indicative of the \u201ccurse of dimensionality\u201d. <br>\nWe are using 11 dimensions of data in this model. <br>\nIn this case, we will explore the model again using PCA (principle component analysis).","3281867e":"The silhouette score obtained is considered low. It means clusters are neither dense nor well separated. <br>\nNext, let\u2019s measure the inertia value.","cb3bd307":"# Pre-processing","88ba474a":"Our purpose of applying principal component analysis is to reduce dimension. <br>\nIn this dataset, we reduced the 11-dimensional data to 6-dimensional data during PCA.","dbc85673":"# Description of data","6f06d486":"# 1. Principal component analysis","6e6a761a":"As expected, we can see an improvement in the silhouette score. But it is still considered low which means there are still some overlapping of clusters or incorrect grouping. <br>\n\nAlthough the silhouette score increased with PCA, it still low; clusters are overlapping or incorrectly grouped.","e4bd7ef9":"# 2. Pearson's Correlation","9cee8f42":"# Data visualisation","a5ab36ba":"K-means clustering has poor clustering result for high dimensional data. Even with the implementation of PCA, the silhouette score can only be improved to some extent but is considered low. Also the inertia value is observed to be extremely high. In an ideal situation, the inertia value should be as low as possible. Hence, we can conclude that this is not a good model fit to the data.","5c65cbcb":"Training Time \u2013 0.072 seconds","dd1c9ed6":"Note: This Red-Wine Analysis using K-Means is just one part of a group project which I had done together with my team-mates. For full analysis using different machine learning models - please refer to my another notebook \"Red-Wine Analysis (Full)\".","c6165dd4":"# Importing Libraries","be31b570":"In this model, the entire dataset has been used as a training data. <br>\nThen an elbow method will be used to find out an optimal number of \u201cK\u201d clusters.","b60cb0df":"1. Feature extraction: Principal component analysis\n2. Feature selection: Pearson's correlation","4e0bc496":"# Feature Engineering","6123f188":"It can be seen that clusters are not well separated. Some members of Cluster 2 can be seen in Cluster 1 and vice versa.","fb934553":"\u201cK\u201d value of 2 will be used as a dip can be seen around 2 which is our elbow in a graph above. <br> <br>\n\nFirst, clustering will be performed with K-Means on dataset without applying principle component analysis (PCA).\nNote that the total dimension of dataset is 11.","480f89ae":"After implementing PCA, it can be seen that clustering is improved. So it is expected to see a higher silhouette score.","9e13b8c9":"# Data Cleaning","fb1db8f1":"The inertia value is also decreased but still extremely high.","da45021d":"## Data Visualization of Clustering in 3D Plot (Fixed Acidity, Residual Sugar, Alcohol)","52f1886c":"## Data Visualization of Clustering","a58fa893":"The red wine data consists of 1599 rows and 12 columns.","51e83fe9":"From the heatmap, it can be seen that most features are weakly correlated to the quality of wine the exception of alcohol (0.48) which is a moderate correlation.","7f19d45a":"Using a correlation of 0.6 to -0.5 as benchmark, a correlation matrix has been created to sieve out features that are highly correlated to the quality of red wine. Our results show that all features are within the acceptable range of 0.6 to -0.5.","d8ad98b7":"# K-Means (Without PCA)","5ea5ac50":"## K-means with PCA"}}