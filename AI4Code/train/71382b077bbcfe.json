{"cell_type":{"36447438":"code","5f67e4aa":"code","ecfa6e06":"code","ad498cf9":"code","07c915b3":"code","fe4d1aef":"code","8059751b":"code","ecd683b9":"code","b481821e":"code","1c44a834":"code","57159170":"code","cedf2187":"code","57372fcd":"code","bed299e5":"code","acc7dda4":"code","5bec8479":"code","8c07cde4":"code","aa8e1676":"code","27f04717":"code","3480ecdd":"code","52c528c6":"code","18ba83e2":"code","61819a75":"code","831e6f1d":"code","ee880c3f":"markdown","2ad1bf42":"markdown","f14b8dbc":"markdown","11279a08":"markdown","58721236":"markdown","9a9308aa":"markdown","196f4004":"markdown","40a43913":"markdown","d5683521":"markdown","3ff9582c":"markdown","3366f482":"markdown","00b38e39":"markdown","4991ac97":"markdown","17856849":"markdown","23f54b48":"markdown","add0bf51":"markdown","5528b3b4":"markdown","eae628a9":"markdown","6062711f":"markdown","5d96c2f0":"markdown","7d7c81f8":"markdown","bec0e2ff":"markdown","c788f708":"markdown","544fc5e8":"markdown","256078b8":"markdown","ec2ced20":"markdown"},"source":{"36447438":"!pip install -U spacy","5f67e4aa":"!python -m spacy download en_core_web_lg","ecfa6e06":"!python -m spacy download en_core_web_sm","ad498cf9":"!pip install wordcloud","07c915b3":"import spacy\nnlp = spacy.load('en_core_web_sm')","fe4d1aef":"import spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\ndoc = nlp(\"My friend param is starting its own Company which is GenerationX\")\nfor token in doc:\n    print(token.text, token.pos_, token.dep_)","8059751b":"import spacy\nnlp = spacy.load('en_core_web_sm')\n\n# Create a nlp object\ndoc = nlp(\"Manvendra is playing chess and eating pizza\")","ecd683b9":"nlp.pipe_names","b481821e":"nlp.disable_pipes('tagger', 'parser')","1c44a834":"nlp.pipe_names","57159170":"import spacy\n\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(\"Akash has been buyed by byju's in 73,000 Core's\")\nfor token in doc:\n    print(token.text)","cedf2187":"import spacy \nnlp = spacy.load('en_core_web_sm')\n\n# Create an nlp object\ndoc = nlp(\"I am Ritesh,currently a Computer Science and NLP Researcher\")\n \n# Iterate over the tokens\nfor token in doc:\n    # Print the token and its part-of-speech tag\n    print(token, token.tag_, token.pos_, spacy.explain(token.tag_))","57372fcd":"import spacy\nfrom spacy import displacy\n\ndoc = nlp(\"I am Ritesh,currently a Computer Science and NLP Researcher\")\ndisplacy.render(doc, style=\"dep\" , jupyter=True)","bed299e5":"import spacy \nnlp = spacy.load('en_core_web_sm')\n\n# Create an nlp object\ndoc = nlp(\"I am Ritesh,currently a Computer Science and NLP Researcher\")\n \n# Iterate over the tokens\nfor token in doc:\n    # Print the token and its part-of-speech tag\n    print(token.text, \"-->\", token.dep_)","acc7dda4":"spacy.explain(\"nsubj\"), spacy.explain(\"ROOT\"), spacy.explain(\"aux\"),spacy.explain('nmod'), spacy.explain(\"advcl\"), spacy.explain(\"dobj\")","5bec8479":"import spacy \nnlp = spacy.load('en_core_web_sm')\n\n# Create an nlp object\ndoc = nlp(\"Reliance is looking at buying U.K. based analytics startup for $7 billion\")\n \n# Iterate over the tokens\nfor token in doc:\n    # Print the token and its part-of-speech tag\n    print(token.text, \"-->\", token.lemma_)","8c07cde4":"import spacy \nnlp = spacy.load('en_core_web_sm')\n\n# Create an nlp object\ndoc = nlp(\"Reliance is looking at buying U.K. based analytics startup for $7 billion.This is India.India is great\")\n \nsentences = list(doc.sents)\nlen(sentences)","aa8e1676":"for sentence in sentences:\n     print (sentence)","27f04717":"import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Reliance is looking at buying U.K. based analytics startup for $7 billion\")\n#See the entity present\nprint(doc.ents)\nfor ent in doc.ents:\n    print(ent.text, ent.start_char, ent.end_char, ent.label_)","3480ecdd":"import spacy\nfrom spacy import displacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc= nlp(u\"\"\"The Amazon rainforest,[a] alternatively, the Amazon Jungle, also known in English as Amazonia, is a moist broadleaf tropical rainforest in the Amazon biome that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 km2 (2,700,000 sq mi), of which 5,500,000 km2 (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations.\n\nThe majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Bolivia, Ecuador, French Guiana, Guyana, Suriname, and Venezuela. Four nations have \"Amazonas\" as the name of one of their first-level administrative regions and France uses the name \"Guiana Amazonian Park\" for its rainforest protected area. The Amazon represents over half of the planet's remaining rainforests,[2] and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.[3]\n\nEtymology\nThe name Amazon is said to arise from a war Francisco de Orellana fought with the Tapuyas and other tribes. The women of the tribe fought alongside the men, as was their custom.[4] Orellana derived the name Amazonas from the Amazons of Greek mythology, described by Herodotus and Diodorus.[4]\n\nHistory\nSee also: History of South America \u00a7 Amazon, and Amazon River \u00a7 History\nTribal societies are well capable of escalation to all-out wars between tribes. Thus, in the Amazonas, there was perpetual animosity between the neighboring tribes of the Jivaro. Several tribes of the Jivaroan group, including the Shuar, practised headhunting for trophies and headshrinking.[5] The accounts of missionaries to the area in the borderlands between Brazil and Venezuela have recounted constant infighting in the Yanomami tribes. More than a third of the Yanomamo males, on average, died from warfare.[6]\"\"\")\n\nentities=[(i, i.label_, i.label) for i in doc.ents]\nentities","52c528c6":"displacy.render(doc, style = \"ent\",jupyter = True)","18ba83e2":"import spacy\n\nnlp = spacy.load(\"en_core_web_lg\")\ntokens = nlp(\"Let's Visit Taj Mahel, and then go to Goa\")\n\nfor token in tokens:\n    print(token.text, token.has_vector, token.vector_norm, token.is_oov)","61819a75":"import spacy\n\nnlp = spacy.load(\"en_core_web_lg\")  # make sure to use larger model!\ntokens = nlp(\"Dog Cat Monkey\")\n\nfor token1 in tokens:\n    for token2 in tokens:\n        print(token1.text, token2.text, token1.similarity(token2))","831e6f1d":"import spacy\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nnlp = spacy.load(\"en_core_web_lg\")  # make sure to use larger model!\ntokens = nlp(\"Bitcoin is the name of the best-known cryptocurrency, the one for which blockchain technology was invented. A cryptocurrency is a medium of exchange, such as the US dollar, but is digital and uses encryption techniques to control the creation of monetary units and to verify the transfer of funds,A blockchain is a decentralized ledger of all transactions across a peer-to-peer network. Using this technology, participants can confirm transactions without a need for a central clearing authority. Potential applications can include fund transfers, settling trades, voting, and many other issues.\")\n\nnewText =''\nfor word in tokens:\n if word.pos_ in ['ADJ', 'NOUN']:\n  newText = \" \".join((newText, word.text.lower()))\n\nwordcloud = WordCloud(stopwords=STOPWORDS).generate(newText)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","ee880c3f":"### 2.6 Named Entity Recognition (NER) <a id=\"26\"><\/a> <br>\n\nA named entity is a \u201creal-world object\u201d that\u2019s assigned a name \u2013 for example, a person, a country, a product or a book title. spaCy can recognize various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn\u2019t always work perfectly and might need some tuning later, depending on your use case.\n\nNamed entities are available as the ents property of a Doc:","2ad1bf42":"## WordCloud","f14b8dbc":"### 2.5 Sentence Boundary Detection (SBD) <a id=\"25\"><\/a> <br>\n\n**Sentence Boundary Detection** is the process of locating the start and end of sentences in a given text. This allows you to you divide a text into linguistically meaningful units. You\u2019ll use these units when you\u2019re processing your text to perform tasks such as part of speech tagging and entity extraction.\n\nIn spaCy, the sents property is used to extract sentences. Here\u2019s how you would extract the total number of sentences and the sentences for a given input text:\n","11279a08":"![](https:\/\/miro.medium.com\/max\/595\/1*ax2uBqfp963n4PQVqmGplQ.png)<a id=\"TOC2\"><\/a> <br>\n\n## Table of Contents\n1. [**What is spaCy**](#1)\n\n    1.1 [**What spaCy is NOT**](#11)\n    \n    1.2 [**Installation**](#12)\n    \n    1.3 [**Statistical Models**](#13)\n    \n    1.4 [**Dependency Parsing**](#14)\n    \n    1.5 [**spaCy\u2019s Processing Pipeline**](#15)\n        \n1. [**Features**](#2)\n    \n    2.1 [**Tokenization**](#21)\n    \n    2.2 [**Part-Of-Speech (POS) Tagging**](#22)\n    \n    2.3 [**Dependency Parsing**](#23)\n    \n    2.4 [**Lemmatization**](#24)\n    \n    2.5 [**Sentence Boundary Detection (SBD)**](#25)\n    \n    2.6 [**Named Entity Recognition (NER)**](#26)\n    \n    2.7 [**Entity Linking (EL)**](#27)\n    \n    2.8 [**Similarity**](#28)\n    \n         \n1. [References](#3)  \n\n1. [Conclusion](#4)          ","58721236":"Just in case you wish to disable the pipeline components and keep only the tokenizer up and running, then you can use the code below to disable the pipeline components:","9a9308aa":"## 2. Features <a id=\"2\"><\/a> <br>\n### 2.1 Tokenization <a id=\"21\"><\/a> <br>\n\n\tSegmenting text into words, punctuations marks etc.\n\nDuring processing, spaCy first tokenizes the text, i.e. segments it into words, punctuation and so on. This is done by applying rules specific to each language.","196f4004":"In this case, the model\u2019s predictions are pretty on point. A dog is very similar to a cat, whereas a monkey is not very similar to either of them. Identical tokens are obviously 100% similar to each other (just not always exactly 1.0, because of vector math and floating point imprecisions).","40a43913":"Let's again check our active pipeline component:","d5683521":"\n### 1.4 Linguistic annotations <a id=\"14\"><\/a> <br>\n\n\nspaCy provides a variety of linguistic annotations to give you insights into a text\u2019s grammatical structure. This includes the word types, like the parts of speech, and how the words are related to each other. For example, if you\u2019re analyzing text, it makes a huge difference whether a noun is the subject of a sentence, or the object \u2013 or whether \u201cgoogle\u201d is used as a verb, or refers to the website or company in a specific context.\n\nOnce you have downloaded and installed a model, you can load it via spacy.load(). This will return a Language object containing all components and data needed to process text. We usually call it nlp. Calling the nlp object on a string of text will return a processed Doc:","3ff9582c":"### 1.3 Statistical models <a id=\"13\"><\/a> <br>\n\nSome of spaCy\u2019s features work independently, others require statistical models to be loaded, which enable spaCy to predict linguistic annotations \u2013 for example, whether a word is a verb or a noun. spaCy currently offers statistical models for a variety of languages, which can be installed as individual Python modules. Models can differ in size, speed, memory usage, accuracy and the data they include. The model you choose always depends on your use case and the texts you\u2019re working with. For a general-purpose use case, the small, default models are always a good start. They typically include the following components:\n\n* **Binary weights** for the part-of-speech tagger, dependency parser and named entity recognizer to predict those annotations in context.\n* **Lexical entries** in the vocabulary, i.e. words and their context-independent attributes like the shape or spelling.\n* **Data files** like lemmatization rules and lookup tables.\n* **Word vectors**, i.e. multi-dimensional meaning representations of words that let you determine how similar they are to each other.\n* **Configuration** options, like the language and processing pipeline settings, to put spaCy in the correct state when you load in the model.\n\nThese models are the power engines of spaCy. These models enable spaCy to perform several NLP related tasks, such as part-of-speech tagging, named entity recognition, and dependency parsing.\n\nI\u2019ve listed below the different statistical models in spaCy along with their specifications:\n\n* en_core_web_sm: English multi-task CNN trained on OntoNotes. Size \u2013 11 MB\n\n* en_core_web_md: English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Size \u2013 91 MB\n\n* en_core_web_lg: English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Size \u2013 789 MB\n\nImporting these models is super easy. We can import a model by just executing spacy.load(\u2018model_name\u2019) as shown below:","3366f482":"### 1.2 Installation <a id=\"12\"><\/a> <br>\n\nSpacy, its data, and its models can be easily installed using python package index and setup tools. Use the following command to install spacy in your machine:","00b38e39":"Using spaCy\u2019s built-in **displaCy** visualizer,The quickest way to visualize Doc is to use displacy.serve. This will spin up a simple web server and let you view the result straight from your browser. displaCy can either take a single Doc or a list of Doc objects as its first argument. This lets you construct them however you like \u2013 using any model or modifications you like.Here\u2019s what our example sentence and its dependencies look like:","4991ac97":"You can use the below code to figure out the active pipeline components:","17856849":"### 2.2 Part-Of-Speech (POS) Tagging <a id=\"22\"><\/a> <br>\n\nPart of speech or POS is a grammatical role that explains how a particular word is used in a sentence. There are eight parts of speech.\n\n* Noun\n* Pronoun\n* Adjective\n* Verb\n* Adverb\n* Preposition\n* Conjunction\n* Interjection\n\nPart of speech tagging is the process of assigning a POS tag to each token depending on its usage in the sentence. POS tags are useful for assigning a syntactic category like noun or verb to each word.\n\nAfter tokenization, spaCy can parse and tag a given Doc. This is where the statistical model comes in, which enables spaCy to make a prediction of which tag or label most likely applies in this context. A model consists of binary data and is produced by showing a system enough examples for it to make predictions that generalize across the language \u2013 for example, a word following \u201cthe\u201d in English is most likely a noun.\n\nLinguistic annotations are available as Token attributes. Like many NLP libraries, spaCy encodes all strings to hash values to reduce memory usage and improve efficiency. So to get the readable string representation of an attribute, we need to add an underscore _ to its name.\n\nIn English grammar, the parts of speech tell us what is the function of a word and how it is used in a sentence. Some of the common parts of speech in English are Noun, Pronoun, Adjective, Verb, Adverb, etc.\n\nPOS tagging is the task of automatically assigning POS tags to all the words of a sentence. It is helpful in various downstream tasks in NLP, such as feature engineering, language understanding, and information extraction.\n\nPerforming POS tagging, in spaCy, is a cakewalk.\n\nIn spaCy, POS tags are available as an attribute on the Token object:","23f54b48":"## 4. Conclusion <a id=\"4\"><\/a> <br>\nI hope you have a good understanding on how to use spaCy by now . \n## Please do leave your comments \/suggestions and if you like this notebook please do <font color='red'>UPVOTE","add0bf51":"Using this technique, we can identify a variety of entities within the text. The spaCy documentation provides a full list of supported entity types, and we can see from the short example above that it\u2019s able to identify a variety of different entity types, including specific locations (GPE), date-related words (DATE), important numbers (CARDINAL), specific individuals (PERSON), etc.\n\nUsing displaCy we can also visualize our input text, with each identified entity highlighted by color and labeled. We\u2019ll use style = \"ent\" to tell displaCy that we want to visualize entities here.","5528b3b4":"The words \u201cLet\u201d, \u201cVisit\u201d and \u201cGoa\u201d are all pretty common in English, so they\u2019re part of the model\u2019s vocabulary, and come with a vector. The word \u201cMahel\u201d on the other hand is a lot less common and out-of-vocabulary \u2013 so its vector representation consists of 300 dimensions of 0, which means it\u2019s practically nonexistent. If your application will benefit from a large vocabulary with more vectors, you should consider using one of the larger models or loading in a full vector package, for example, en_vectors_web_lg, which includes over 1 million unique vectors.\n\nspaCy is able to compare two objects, and make a prediction of how similar they are. Predicting similarity is useful for building recommendation systems or flagging duplicates. For example, you can suggest a user content that\u2019s similar to what they\u2019re currently looking at, or label a support ticket as a duplicate if it\u2019s very similar to an already existing one.\n\nEach Doc, Span and Token comes with a .similarity() method that lets you compare it with another object, and determine the similarity. Of course similarity is always subjective \u2013 whether \u201cGoa\u201d and \u201ctaj\u201d are similar really depends on how you\u2019re looking at it. spaCy\u2019s similarity model usually assumes a pretty general-purpose definition of similarity.","eae628a9":"### 2.7 Entity Detection <a id=\"27\"><\/a> <br>\n\n**Entity detection**, also called entity recognition, is a more advanced form of language processing that identifies important elements like places, people, organizations, and languages within an input string of text. This is really helpful for quickly extracting information from text, since you can quickly pick out important topics or indentify key sections of text.\n\nLet\u2019s try out some entity detection using a few paragraphs from this recent article in the Washington Post. We\u2019ll use .label to grab a label for each entity that\u2019s detected in the text, and then we\u2019ll take a look at these entities in a more visual format using spaCy\u2018s displaCy visualizer.","6062711f":"### 2.8 Similarity <a id=\"28\"><\/a> <br>\n\n**Similarity** is determined by comparing word vectors or \u201cword embeddings\u201d, multi-dimensional meaning representations of a word. Word vectors can be generated using an algorithm like word2vec and usually look like this:\n\nSpacy also provides inbuilt integration of dense, real valued vectors representing distributional similarity information.\n\nModels that come with built-in word vectors make them available as the Token.vector attribute. Doc.vector and Span.vector will default to an average of their token vectors. You can also check if a token has a vector assigned, and get the L2 norm, which can be used to normalize vectors.","5d96c2f0":"The dependency tag ROOT denotes the main verb or action in the sentence. The other words are directly or indirectly connected to the ROOT word of the sentence. You can find out what other tags stand for by executing the code below:","7d7c81f8":"### 2.4 Lemmatization <a id=\"24\"><\/a> <br>\n\n**Lemmatization** is the process of reducing inflected forms of a word while still ensuring that the reduced form belongs to the language. This reduced form or root word is called a lemma.\n\nFor example, organizes, organized and organizing are all forms of organize. Here, organize is the lemma. The inflection of a word allows you to express different grammatical categories like tense (organized vs organize), number (trains vs train), and so on. Lemmatization is necessary because it helps you reduce the inflected forms of a word so that they can be analyzed as a single item. It can also help you normalize the text.\n\nspaCy has the attribute lemma_ on the Token class. This attribute has the lemmatized form of a token:","bec0e2ff":"### 2.3 Dependency Parsing <a id=\"23\"><\/a> <br>\n\nDependency parsing is the process of extracting the dependency parse of a sentence to represent its grammatical structure. It defines the dependency relationship between headwords and their dependents. The head of a sentence has no dependency and is called the root of the sentence. The verb is usually the head of the sentence. All other words are linked to the headword.\n\nThe dependencies can be mapped in a directed graph representation:\n\n* Words are the nodes.\n* The grammatical relationships are the edges.\n\nDependency parsing helps you know what role a word plays in the text and how different words relate to each other. It\u2019s also used in shallow parsing and named entity recognition.\n\nHere\u2019s how you can use dependency parsing to see the relationships between words:\n![](https:\/\/www.researchgate.net\/profile\/Michael_Ringgaard\/publication\/220816955\/figure\/fig2\/AS:667852638019597@1536239885253\/Dependency-Parse-Tree-with-Alignment-for-a-Sentence-with-Preposition-Modifier.png)\nPerforming dependency parsing is again pretty easy in spaCy. We will use the same sentence here that we used for POS tagging:","c788f708":"## 1. What is spaCy <a id=\"1\"><\/a> <br>\n    \nspaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python. It\u2019s written in Cython and is designed to build information extraction or natural language understanding systems. It\u2019s built for production use and provides a concise and user-friendly API.\n\nIf you\u2019re working with a lot of text, you\u2019ll eventually want to know more about it. For example, what\u2019s it about? What do the words mean in context? Who is doing what to whom? What companies and products are mentioned? Which texts are similar to each other?\n\nspaCy is designed specifically for production use and helps you build applications that process and \u201cunderstand\u201d large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning.\n\n### 1.1 What spaCy is NOT <a id=\"11\"><\/a> <br>\n\n* **spaCy is not a platform** or \u201can API\u201d. Unlike a platform, spaCy does not provide a software as a service, or a web application. It\u2019s an open-source library designed to help you build NLP applications, not a consumable service.\n\n* **spaCy is not an out-of-the-box chat bot engine**. While spaCy can be used to power conversational applications, it\u2019s not designed specifically for chat bots, and only provides the underlying text processing capabilities.\n\n* **spaCy is not research software**. It\u2019s built on the latest research, but it\u2019s designed to get things done. This leads to fairly different design decisions than NLTK or CoreNLP, which were created as platforms for teaching and research. The main difference is that spaCy is integrated and opinionated. spaCy tries to avoid asking the user to choose between multiple algorithms that deliver equivalent functionality. Keeping the menu small lets spaCy deliver generally better performance and developer experience.\n\n* **spaCy is not a company.** It\u2019s an open-source library. Our company publishing spaCy and other software is called Explosion AI.\n\n","544fc5e8":"First, the raw text is split on whitespace characters, similar to text.split(' '). Then, the tokenizer processes the text from left to right. On each substring, it performs two checks:\n\n* Does the substring match a tokenizer exception rule? For example, \u201cbyju's\u201d does not contain whitespace, but should be split into two tokens, \u201cbyju\u201d and \u201c's\u201d, while \u201cakash\u201d should always remain one token.\n\n* Can a prefix, suffix or infix be split off? For example punctuation like commas, periods, hyphens or quotes.\n\nIf there\u2019s a match, the rule is applied and the tokenizer continues its loop, starting with the newly split substrings. This way, spaCy can split complex, nested tokens like combinations of abbreviations and multiple punctuation marks.\n\n![](https:\/\/d33wubrfki0l68.cloudfront.net\/fedbc2aef51d678ae40a03cb35253dae2d52b18b\/3d4b2\/tokenization-57e618bd79d933c4ccd308b5739062d6.svg)","256078b8":"## 3. References <a id=\"3\"><\/a> <br>\n\n* https:\/\/medium.com\/@ashiqgiga07\/rule-based-matching-with-spacy-295b76ca2b68\n* https:\/\/spacy.io\/usage\/spacy-101#whats-spacy","ec2ced20":"Even though a Doc is processed \u2013 e.g. split into individual words and annotated \u2013 it still holds all information of the original text, like whitespace characters. You can always get the offset of a token into the original string, or reconstruct the original by joining the tokens and their trailing whitespace. This way, you\u2019ll never lose any information when processing text with spaCy.\n\nspaCy\u2019s Processing Pipeline\n### 1.5 spaCy\u2019s Processing Pipeline <a id=\"15\"><\/a> <br>\nThe first step for a text string, when working with spaCy, is to pass it to an NLP object. This object is essentially a pipeline of several text pre-processing operations through which the input text string has to go through.\n\n![](https:\/\/d33wubrfki0l68.cloudfront.net\/16b2ccafeefd6d547171afa23f9ac62f159e353d\/48b91\/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg)\n\nAs you can see in the figure above, the NLP pipeline has multiple components, such as tokenizer, tagger, parser, ner, etc. So, the input text string has to go through all these components before we can work on it.\n\nLet me show you how we can create an nlp object:\n\n"}}