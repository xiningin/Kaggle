{"cell_type":{"6fc6ad4a":"code","e8451e54":"code","30f2349a":"code","3841c518":"code","c25f881b":"code","ea1e68a7":"code","f1f5c422":"code","c5174a70":"code","9de81473":"code","ebbe2a57":"code","a4e16b04":"code","b37ef0f2":"code","7b3c939d":"code","dc09beb9":"code","6d41e849":"code","7f851373":"code","b73af227":"code","a267cb62":"code","89743e0f":"code","25b67556":"code","b1997c58":"code","23ff5620":"code","ea58cb06":"code","d3af49d9":"code","2179a93f":"code","4cdd6aea":"code","540d9fc5":"code","c3399dc2":"code","a5995498":"code","c2e2ba81":"code","ae719cbe":"code","13dce333":"code","bd54cf94":"code","77f38e22":"code","89038fd5":"code","b591596f":"code","77aea1cd":"code","02205f60":"code","891928e3":"code","31457bc6":"code","a1610b5d":"code","9d0ec7d5":"code","3d0b7032":"code","84b575c1":"code","8fe2e0dc":"code","5e1e68f8":"markdown","34bffaec":"markdown","b687711f":"markdown","b1922a15":"markdown","91700ef3":"markdown","3995c7b5":"markdown","ef8bdcde":"markdown","8fa7da41":"markdown","cedb1dd2":"markdown","50150c0d":"markdown","2bbb0ae5":"markdown","c7ccf435":"markdown","e2969082":"markdown","7a6c3cb2":"markdown","ff737353":"markdown","545b6da6":"markdown","70c0ec8a":"markdown","317778a0":"markdown","d2b35aa9":"markdown","9c62c872":"markdown","2bcf6750":"markdown","3892924d":"markdown","8587be85":"markdown","27853b61":"markdown","716a6f85":"markdown","596c77f5":"markdown","0ebfbf01":"markdown","3725f40f":"markdown"},"source":{"6fc6ad4a":"# import packages\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\nimport copy\n\n# show files\nimport os\nprint(os.listdir(\"..\/input\"))","e8451e54":"# set pyplot parameters to make things pretty\nplt.rc('axes', linewidth = 1.5)\nplt.rc('xtick', labelsize = 14)\nplt.rc('ytick', labelsize = 14)\nplt.rc('xtick.major', size = 3, width = 1.5)\nplt.rc('ytick.major', size = 3, width = 1.5)","30f2349a":"# read data\nlinearData = pd.read_csv('..\/input\/linear.csv')\nlinearData.head()","3841c518":"# Let's first plot (x,y) and see what it looks like\nplt.plot('x','y',data = linearData, marker = 'o', linestyle = '', label = 'data')\nplt.xlabel('x',fontsize = 18)\nplt.ylabel('y', fontsize = 18)\nplt.legend(fontsize = 14)\nplt.show()","c25f881b":"x = linearData['x'].tolist()\ny = linearData['y'].tolist()\n\n# Don't forget - adding ones to the x matrix\nxb = np.c_[np.ones((len(x),1)),x]\n# calculate linear regression parameters theta using the normal equation\nthetaHat = np.linalg.inv(xb.T.dot(xb)).dot(xb.T).dot(y)","ea1e68a7":"print(thetaHat)\n# thetaHat[0] is intercept, thetaHat[1] is slope. This is determined by the column order of matrix xb.","f1f5c422":"# plot the fit and the data\nxFit = np.linspace(0,100,num = 200)\nxFitb = np.c_[np.ones((len(xFit),1)), xFit]\nyFit = xFitb.dot(thetaHat)\n\nplt.plot('x','y',data = linearData, marker = 'o', linestyle = '', label = 'data')\nplt.plot(xFit, yFit, color = 'r', lw = 3, linestyle = '--', label = 'Linear fit')\nplt.xlabel('x',fontsize = 18)\nplt.ylabel('y', fontsize = 18)\nplt.legend(fontsize = 14)\nplt.show()","c5174a70":"# create the model\nlin_reg = LinearRegression()\n# format x so that LinearRegression recognize it.\nx = np.array(x).reshape(-1,1)\n# fit the model\nlin_reg.fit(x,y)\nlin_reg.intercept_, lin_reg.coef_","9de81473":"xb = sm.add_constant(x) # again, add a column of ones to x\nmodel = sm.OLS(y,xb) # OLS = Ordinary Least Squares\nresults = model.fit()\nprint(results.summary())","ebbe2a57":"learningRate = 0.0002\nnumIterations = 100000\ny = np.array(y).reshape(-1,1)\nm = len(y) # number of samples\n\n# random initialization with standard normal distribution\ntheta = np.random.randn(2,1)\n\n# start gradient descent\nfor i in range(numIterations):\n    gradient = 2\/m * xb.T.dot(xb.dot(theta) - y) # dimension: (2,1)\n    theta = theta - learningRate * gradient","a4e16b04":"theta","b37ef0f2":"# define the function to calculate MSE\n# can also use sklearn.metrics.mean_squared_error\ndef MSE(xb,y,theta):\n    return np.sum(np.square(xb.dot(theta)-y))\/len(y)","7b3c939d":"learningRate = 0.0002\nnumIterations = 100000\ny = np.array(y).reshape(-1,1)\nm = len(y) # number of samples\n\n# random initialization with standard normal distribution\ntheta = np.random.randn(2,1)\n\ncost = []\n# start gradient descent\nfor i in range(numIterations):\n    gradient = 2\/m * xb.T.dot(xb.dot(theta) - y) # dimension: (2,1)\n    theta = theta - learningRate * gradient\n    cost.append(MSE(xb,y,theta))","dc09beb9":"fig,ax = plt.subplots(nrows = 1, ncols = 2, figsize = (10,4))\nax[0].plot(range(0,100),cost[0:100])\nax[1].plot(range(10000,20001), cost[10000:20001])\nplt.subplots_adjust(wspace=0.5)\nax[0].set_xlabel('# of iterations', fontsize = 14)\nax[1].set_xlabel('# of iterations', fontsize = 14)\nax[0].set_ylabel('MSE', fontsize = 14)\nax[1].set_ylabel('MSE', fontsize = 14)","6d41e849":"learningRate = 0.0003\nnumIterations = 100\ny = np.array(y).reshape(-1,1)\nm = len(y) # number of samples\n\n# random initialization with standard normal distribution\ntheta = np.random.randn(2,1)\n\ncost = []\n# start gradient descent\nfor i in range(numIterations):\n    gradient = 2\/m * xb.T.dot(xb.dot(theta) - y) # dimension: (2,1)\n    theta = theta - learningRate * gradient\n    cost.append(MSE(xb,y,theta))\n\nfig,ax = plt.subplots(nrows = 1, ncols = 1, figsize = (5,4))\nax.plot(range(0,100), cost[0:100])\nax.set_xlabel('# of iterations', fontsize = 14)\nax.set_ylabel('MSE', fontsize = 14)","7f851373":"theta = np.random.randn(2,1)\ngradient = 2\/m * xb.T.dot(xb.dot(theta) - y)\nprint(gradient)","b73af227":"xbStandard = copy.deepcopy(xb) # we don't want to mess with xb! xbStandard = xb will lead to xb being normalized, too.\n# save the shift and scaling\nmu = np.mean(xbStandard[:,1]) \nsigma = np.std(xbStandard[:,1])\n# standardization\nxbStandard[:,1]=(xbStandard[:,1]-mu)\/sigma\nprint(xbStandard[0:5])\nprint(mu)\nprint(sigma)","a267cb62":"learningRate = 0.1\nnumIterations = 1000\n\nm = len(y) # number of samples\n\n# random initialization with standard normal distribution\ntheta = np.random.randn(2,1)\n\ncost = []\n# start gradient descent\nfor i in range(numIterations):\n    gradient = 2\/m * xbStandard.T.dot(xbStandard.dot(theta) - y) # dimension: (2,1)\n    theta = theta - learningRate * gradient\n    cost.append(MSE(xbStandard,y,theta))\n\nfig,ax = plt.subplots(nrows = 1, ncols = 1, figsize = (5,4))\nplt.plot(cost)\nax.set_xlabel('# of iterations', fontsize = 14)\nax.set_ylabel('MSE', fontsize = 14)\nplt.show()","89743e0f":"print(theta)","25b67556":"xFit = np.linspace(0,100,num = 200)\nxFitStandard = (xFit - mu)\/sigma # use the previously saved mean and standard deviation\nxFitStandardb = np.c_[np.ones((len(xFitStandard),1)), xFitStandard]\nyFit = xFitStandardb.dot(theta)\n\nplt.plot('x','y',data = linearData, marker = 'o', linestyle = '', label = 'data')\nplt.plot(xFit, yFit, color = 'r', lw = 3, linestyle = '--', label = 'Linear fit')\nplt.xlabel('x',fontsize = 18)\nplt.ylabel('y', fontsize = 18)\nplt.legend(fontsize = 14)\nplt.show()","b1997c58":"numEpochs = 1000\n\n# a simple learning schedule\ndef learningSchedule(step):\n    return 5\/(50000 + step)\n\n# visualize the learning schedule\ns = np.linspace(0,15000, num = 15001)\nl = learningSchedule(s)\nm = len(y) # sample size \n\nfig, ax = plt.subplots(figsize = (8,4))\nplt.plot(s,l, lw = 2)\n#plt.xlim(0,15000)\nplt.xlabel('step', fontsize = 18)\nplt.ylabel('learning rate', fontsize = 18)\nplt.show()","23ff5620":"theta = np.random.randn(2,1)\n\ncost = []\nfor epoch in range(numEpochs):\n    for i in range(m):\n        idx = np.random.randint(m) # 0,1,...,m-1\n        xbi = xb[idx:idx+1]\n        yi = y[idx:idx+1]\n        gradient = 2*xbi.T.dot(xbi.dot(theta)-yi) # sample size is one\n        learningRate = learningSchedule(epoch*m + i) # step = epoch*m + i\n        theta = theta - learningRate * gradient\n        cost.append(MSE(xb,y,theta))","ea58cb06":"print(theta)","d3af49d9":"fig, ax = plt.subplots(figsize = (8,4))\nplt.plot(cost)\n#plt.xlim(0,20000)\nplt.ylabel('cost', fontsize = 18)\nplt.xlabel('step', fontsize = 18)\nplt.show()","2179a93f":"numEpochs = 100\n\n# a simple learning schedule\ndef learningSchedule(step):\n    return 5\/(10000 + step)\n\ntheta = np.random.randn(2,1)\n\ncost = []\nfor epoch in range(numEpochs):\n    for i in range(m):\n        idx = np.random.randint(m) # 0,1,...,m-1\n        xbi = xb[idx:idx+1]\n        yi = y[idx:idx+1]\n        gradient = 2*xbi.T.dot(xbi.dot(theta)-yi) # sample size is one\n        learningRate = learningSchedule(epoch*m + i) # step = epoch*m + i\n        theta = theta - learningRate * gradient\n        cost.append(MSE(xb,y,theta))","4cdd6aea":"plt.plot(cost)\nplt.show()","540d9fc5":"from sklearn.linear_model import SGDRegressor","c3399dc2":"# max_iter is the total number of epochs, eta0 is the starting learning rate\n# penalty = None, meaning there is no regularization.\nmodel = SGDRegressor(eta0 = 0.0005, penalty = None, max_iter = 10000)\nmodel.fit(x,y.ravel())","a5995498":"model.intercept_, model.coef_","c2e2ba81":"from sklearn.utils import shuffle","ae719cbe":"batchsize = 30 # size of each of the mini batch\ntheta = np.random.randn(2,1)\n\nnumEpochs = 5000\nlearningRate = 0.0002\n\nfor epoch in range(numEpochs):\n    xbShuffled, yShuffled = shuffle(xb, y) # shuffle your dataset at the beginning of each epoch.\n    for i in range(0, xbShuffled.shape[0], batchsize):\n        xbi = xbShuffled[i:i+batchsize]\n        yi = yShuffled[i:i+batchsize]\n        gradient = 2\/batchsize*xbi.T.dot(xbi.dot(theta)-yi)\n        theta = theta - learningRate*gradient\n        ","13dce333":"print(theta)","bd54cf94":"# read data\nadvancedData = pd.read_csv('..\/input\/advanced.csv')\nadvancedData.head()","77f38e22":"plt.plot('fixed acidity','pH', data = advancedData, marker = 'o', linestyle = '') # fixed acidity, pH\nplt.xlabel('fixed acidity', fontsize = 18)\nplt.ylabel('pH', fontsize = 18)\nplt.show()","89038fd5":"reg = LinearRegression()\nx = advancedData['fixed acidity'].as_matrix().reshape(-1,1)\ny = advancedData['pH'].as_matrix().reshape(-1,1)\nreg.fit(x,y)","b591596f":"xFit = np.linspace(4,16,num=100).reshape(-1,1)\nyFit = reg.predict(xFit)\nplt.plot('fixed acidity','pH', data = advancedData, marker = 'o', linestyle = '') \nplt.plot(xFit,yFit, color = 'r',lw=3)\nplt.xlabel('fixed acidity', fontsize = 18)\nplt.ylabel('pH', fontsize = 18)\nplt.show()","77aea1cd":"# compute MSE\nxb = np.c_[np.ones((len(x),1)),x]\ntheta = np.array([reg.intercept_[0],reg.coef_[0][0]]).reshape(-1,1)\n\nlinMSE = MSE(xb,y,theta)\nprint(linMSE)","02205f60":"from sklearn.preprocessing import PolynomialFeatures","891928e3":"poly = PolynomialFeatures(degree=2, include_bias = False)\nxPoly = poly.fit_transform(x)\nprint(xPoly[0:5])","31457bc6":"reg = LinearRegression()\nreg.fit(xPoly,y)\nprint(reg.intercept_, reg.coef_)","a1610b5d":"xFit=np.linspace(4,16,num=100).reshape(-1,1)\nxFit=poly.fit_transform(xFit)\n#print(xFit[0:5])\nyFit = reg.predict(xFit)","9d0ec7d5":"plt.plot('fixed acidity','pH', data = advancedData, marker = 'o', linestyle = '') \nplt.plot(xFit[:,0],yFit, color = 'r',lw=3)\nplt.xlabel('fixed acidity', fontsize = 18)\nplt.ylabel('pH', fontsize = 18)\nplt.show()","3d0b7032":"# compute MSE\nxb = np.c_[np.ones((len(x),1)),xPoly]\ntheta = np.array([reg.intercept_[0],reg.coef_[0][0],reg.coef_[0][1]]).reshape(-1,1)\n\npolyMSE = MSE(xb,y,theta)\nprint(polyMSE)","84b575c1":"poly = PolynomialFeatures(degree=20, include_bias = False)\nxPoly = poly.fit_transform(x)\n#print(xPoly[0:1])\n\nreg = LinearRegression()\nreg.fit(xPoly,y)\n\nxFit = np.linspace(4,16,num=100).reshape(-1,1)\nxFit = poly.fit_transform(xFit)\nyFit = reg.predict(xFit)","8fe2e0dc":"plt.plot('fixed acidity','pH', data = advancedData, marker = 'o', linestyle = '') \nplt.plot(xFit[:,0],yFit, color = 'r',lw=3)\nplt.xlabel('fixed acidity', fontsize = 18)\nplt.ylabel('pH', fontsize = 18)\nplt.show()","5e1e68f8":"### Mini-batch Gradient Descent\nMini-batch gradient descent is somewhere between batch gradient descent and stochastic gradient descent.  \nFor each iteration, you caluclate gradient based on a random selected sub-sample of the training set, not all of them (batch GD), nor only one of them (stochastic GD).","34bffaec":"Now, let's try to standardize the input features and run GD again.","b687711f":"## Normal equation\nGiven an input sample $\\vec{x}=[x_1, x_2,...,x_n]$, linear regression predicts a $y$ value using the following equation:\n<center>\n    $\\hat{y}=\\theta_0+\\theta_1 x_1 + \\theta_2 x_2 +...+\\theta_n x_n$\n<\/center>\nGiven $m$ training samples $\\vec{x}^{(1)}, \\vec{x}^{(2)},...,\\vec{x}^{(m)}$, linear regression finds $\\vec{\\theta}$ that minimizes the Mean Square Error (MSE) between $\\hat{y}$ and $y$:\n<center>\n    $MSE(\\vec{\\theta})=\\frac{1}{m}\\sum_{i=1}^{m} (\\hat{y}^{(i)}-y)^2=\\frac{1}{m}\\sum_{i=1}^{m} (\\vec{\\theta}\\cdot\\vec{x}^{(i)}-y)^2$\n<\/center>\nwhere $y$ is the true value and $\\hat{y}$ is the predicted value. The solution of this minimization problem is given by: $\\partial MSE\/\\partial \\vec{\\theta}=0$, which gives us:\n<center>\n    $\\hat{\\vec{\\theta}} = (\\mathbf{X}^T\\cdot \\mathbf{X})^{-1}\\cdot \\mathbf{X}^T\\cdot \\vec{y}$\n<\/center>\nwhere $\\mathbf{X}$ is the input data matrix of size $m \\times (n+1)$. Each row of $\\mathbf{X}$ corresponds to a sample, each columns corresponds to a feature. There are $(n+1)$ columns since there is a column of 1s added to the $n$ features, corresponding to $\\theta_0$. The $\\vec{y}$ is a vector of true target values of size $m \\times 1$, and $\\hat{\\vec{\\theta}}$ is a vector of size $(n+1) \\times 1$.\n**The above equation is called the normal equation. The $\\hat{\\vec{\\theta}}$ is the set of parameters that minimizes the cost function of linear regression $MSE(\\vec{\\theta})$.**\n\nNext we will fit the data we read using the normal equation. Let's first plot the data to see what it looks like:","b1922a15":"-------------------------------\nUpdated 12-10-2018.","91700ef3":"You can see that since we did not standardize the $\\mathbf{X}$ matrix, gradient in the second direction is much larger than that in the first direction. If the learning rate is not small enough, $\\vec{\\theta}$ will take a big step in the second direction, potentially leading to cost function increasing instead of decreasing.  \n In practice, if you wish to use gradient descent on regression analysis, it is important to normalzie the input features. When predicting, you will need to scale the input x first **using the same shift and scaling as the training set**, then make a prediction.  ","3995c7b5":"Now that we have $\\hat{\\vec{\\theta}}$, let's plot the fit:","ef8bdcde":"Not bad. But note that with SGD, due to the random nature of the algorithm, $\\vec{\\theta}$ will be hopping around the optimized value. Simulated annealing (mentioned above) can help with this issue.  \nWe can plot the cost function (MSE) to see what it looks like:","8fa7da41":"Looks like a good enough dataset to try polynomial regression on.  \nBut first, let's see how linear fit performs on this data.","cedb1dd2":"The \"fit_transform\" function just transforms the orginal data $x$ to include second-order features. As you can see, xPoly's first column contains the original data, and xPoly's second column contains the second-order features.\n\nWe then conduct polynomial fit: y = a + b*x + c*x^2","50150c0d":"# Linear Regression\nWe will use a simple linearized data to illustrate linear regression. Let's read the data first:","2bbb0ae5":"Then, use normal equation to solve for the optimizied set of parameters, $\\hat{\\vec{\\theta}}$ (thetaHat):","c7ccf435":"#### Table of Content:\n- Linear Regression<br\/>\n    - Normal equation \n    - Using Scikit-Learn\n    - Gradient descent\n        - batch gradient descent\n        - stochastic gradient descent\n        - mini-batch gradient descent\n- Polynomial Regression","e2969082":"With learning rate at 0.0003, the MSE shoots up with number of iterations.  \nThis is partly due to using $\\mathbf{X}$ matrix without standardizing, which leads to very different scales in the gradient vector.  \nWe can examine this by looking at the gradient for the first iteration:","7a6c3cb2":"## Using Scikit-Learn\nThere are many existing packages that can conduct linear regression for you. Here we will introduce **Scikit-Learn's** LinearRegression():","ff737353":"The \"lin_reg = LinearRegression()\" created a linear regressor named lin_reg. The \"lin_reg.fit(x,y)\" fit the regressor to the training data and obtain the best paramters intercept - ($\\theta_0$) and slope ($\\theta_1$). **This is a common pattern of Scikit-Learn's regressors and classifiers, as you will see throughout the whole tutorial series.**\n\nAs you can see, this method gives the same result as the normal equation.  \n\nWhat if we want some statistics on how significant the fit is? In this case we can use the package **statsmodels** (already imported at the beginning of this notebook):","545b6da6":"## Gradient Descent:\nAnother way to do linear regression is to use Gradient Descent (GD). GD is most useful when the cost function (here it is the MSE) does not have a clean and nice analytical solution. But it can also accelerate the training when you have a lot of input features or a large training set. \n\n**The basic idea of gradient descent is that the cost function decreases most steeply along the the opposite direction of its gradient**. In our case, the cost function is $MSE(\\vec{\\theta})$. The gradient of $MSE(\\vec{\\theta})$ over $\\vec{\\theta}$ is:\n<center>\n    $\\nabla_{\\vec{\\theta}}MSE(\\vec{\\theta}) = \\frac{2}{m} \\mathbf{X}^T \\cdot (\\mathbf{X}\\cdot \\vec{\\theta}-\\vec{y})$\n<\/center>\nAgain, $m$ is the number of samples in the training set. $\\mathbf{X}$ is the input data matrix of size $m \\times (n+1)$, $\\vec{\\theta}$ is the parameter vector of size $(n+1) \\times 1$, and $\\vec{y}$ is the vector of true target values of size $m \\times 1$. **The pseudo algorithm for gradient descent is**:\n* Initialize parameter vector $\\vec{\\theta}$\n* Choose learning rate $\\mu$\n* For each step, update the parameter vector using $\\vec{\\theta} = \\vec{\\theta} - \\mu \\times \\nabla_{\\vec{\\theta}}MSE(\\vec{\\theta})$. Then update $MSE(\\vec{\\theta})$ using the updated $\\vec{\\theta}$.\n* Iterate until certain number of steps has passed, or when the decrease of $MSE(\\vec{\\theta})$ is smaller than a tolerance.\n\nIn this section, we will show how to do linear regression with gradient descent. Two tips to keep in mind:\n* Make sure all features have similar scale, otherwise the training might take a long time.\n* A convex cost function is easy to optimize, but most of the time GD leads to local minima.","70c0ec8a":"This 20-degree polynomial model is overfitting, as you can see it trying to wiggle through as many data points as possible.  \nA good way to reduce overfitting is regularization, which will be introduced in the [next tutorial](https:\/\/www.kaggle.com\/fengdanye\/machine-learning-2-regularized-lm-early-stopping).","317778a0":"### Batch gradient descent:\nBatch gradient descent calculates the gradient using all $m$ training samples:","d2b35aa9":"You can also run SGD using **Scikit-Learn**'s SGDRegressor():","9c62c872":"With much larger learning rate and much fewer iterations, we have achieved satisfying linear fit of the data!  \nThe following examples will go back to using the original data, but in practice input features should be standardized for all GD problems.","2bcf6750":"Finding the right learning rate is a little tricky here. If you increase the learning rate, you can see theta blowing up and miss the minimum. <br\/>\nWe can also plot the cost function to examine the effects of learning rate:","3892924d":"You can see that MSE has some fluctuations, but in general it is decreasing.  \nAgain, if the learning rate starts off too high, the cost function will overshoot:","8587be85":"### Stochastic Gradient Descent\nStochastic Gradient Descent (SGD) use one sample at a time to calculate gradient. Instead of trying to decrease $MSE(\\vec{\\theta})$ of all samples ($\\frac{1}{m}\\sum_{i=1}^{m} (\\hat{y}^{(i)}-y)^2$) at each step, SGD tries to decrease $MSE(\\vec{\\theta})$ of one randomly selected sample ($(\\hat{y}^{(i)}-y)^2$, $i$ is randomly selected) at each step. The gradient term at each step becomes:\n<center>\n    $2\\times \\vec{x}^{(i)T} \\cdot (\\vec{x}^{(i)}\\cdot \\vec{\\theta}-y^{(i)})$\n<\/center>\n\n**There are several terminologies and concepts that I will be using in the following code:**\n* Simulated annealing: gradually decrease learning rate. A common technique in SGD to help it reach the minima toward the end of the learning period.\n* Learning Schedule: the function that determines how learning rate changes with iterations.\n* Epochs: the number of rounds of $m$ iterations to run (see code below).","27853b61":"Now, let's try a second degree polynomial fit. To transform data to include polynomial features, we will use Scikit-Learn's PolynomialFeatures():","716a6f85":"There are several key statistics in the above chart:  \n* R-squared: SSR\/SST = 1 - SSE\/SST, proportion of variance explained. Can increase just because more predictors are added.\n* Adj. R-squared: R-squared but counts for the number of predictors. Only increases if model is improved.\n* AIC: Akaike information criterion. Learn more at https:\/\/en.wikipedia.org\/wiki\/Akaike_information_criterion\n* BIC: Bayesian information criterion. Learn more at https:\/\/en.wikipedia.org\/wiki\/Bayesian_information_criterion  \n\nHere R-squared = Adj. R-squared = 0.989. Therefore we can conclude that the fit is good.","596c77f5":"The MSE is smaller than the first-degree linear regression by ~0.001 (~8.5%), a small improvement. <br\/>\nWhat if we do polynomial degree = 20?","0ebfbf01":"# Polynomial Regression\nThe above simple linear regression model can be easily extended to a polynomial regression model. To illustrate polynomial regression, we will use another dataset:","3725f40f":"As you can see, with learning rate at 0.0002, the MSE steadily decreases with the number of iterations. Now let's try to increase the learning rate a little bit:"}}