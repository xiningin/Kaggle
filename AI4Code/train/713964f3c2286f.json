{"cell_type":{"130f43e6":"code","ae8e1340":"code","fd3b943f":"code","03ba6978":"code","ac8b33f4":"code","70a039e1":"code","58a7f278":"code","7537f5eb":"code","dcf2dcae":"code","6cad6534":"code","688062a1":"code","600bb827":"code","79145f13":"code","26f3ef8a":"code","89b2929f":"code","77644d1b":"code","fc32aebe":"code","2e3d481b":"code","9522ebbc":"code","df0d6931":"code","1f12834d":"code","c0f70fca":"code","626a6a53":"code","927ef338":"code","fdc596f5":"code","412ff3c7":"code","15f3d7ef":"code","8a3053dc":"code","fc56187e":"code","2b88f95d":"code","0c0a19e0":"code","a2e3aa75":"code","b5bc6139":"code","244335df":"code","d3343b98":"code","b0ec1928":"code","f2e39c6f":"code","32979482":"code","ef137a68":"code","ac339e58":"code","e1a3b9fb":"code","c6432618":"code","3565e901":"code","8d2256cb":"code","db1c8f66":"code","7635724c":"code","90e76d08":"code","01c62155":"code","f6e02751":"code","8b023a55":"code","b8bcee39":"code","13e36654":"code","fb3e5ed5":"code","1531206a":"code","d7357bf9":"code","d02e62e6":"code","d611007a":"code","f49c2bd2":"code","6c03dcb1":"code","3605c39f":"code","928ba43e":"code","9b7a816b":"code","c85df748":"code","521a3c2e":"code","98ba9b96":"code","cac5c489":"code","f3f516b4":"code","1230b0d7":"code","62ce294e":"code","e7f3b4ed":"code","e2bf1989":"code","dcac8543":"code","6cbd1074":"code","1fd92714":"code","3b15f9aa":"code","543d6b44":"code","c87ccfa4":"code","21d8c7f0":"code","96fecbde":"markdown","e4b1599c":"markdown","ed385877":"markdown","31b7a630":"markdown","01709f21":"markdown","666b478e":"markdown","bf83fd6f":"markdown","c1eed12b":"markdown","e1273bfa":"markdown","7532d6b5":"markdown","292c450c":"markdown","e310880d":"markdown","4717b69b":"markdown","591b035a":"markdown","92d4f56e":"markdown","a0e4eeb7":"markdown","8571aec1":"markdown","a72e74a0":"markdown","54cc8cbd":"markdown","95eafe24":"markdown","0611096b":"markdown","4fe7594e":"markdown","fe942d8a":"markdown","7e980f52":"markdown","bce5bf4d":"markdown","00125ba7":"markdown","6cdc1795":"markdown","26b7a297":"markdown","3334bc6e":"markdown","75969afe":"markdown","ceff42f0":"markdown","ab10c6b8":"markdown","ce580b23":"markdown","fbae9af7":"markdown","bbba5f9c":"markdown","d6725089":"markdown","29b29287":"markdown","a76466f4":"markdown","19e5a46d":"markdown","21e1cedc":"markdown","7b44b99c":"markdown","c0e1def2":"markdown","5f497a3f":"markdown","608e9636":"markdown","6dd4cc42":"markdown","f516e3e8":"markdown","ca132aec":"markdown","2eb249af":"markdown","d24a6954":"markdown","89c1a609":"markdown","9ff12253":"markdown","596205f6":"markdown","bab597ec":"markdown","4fae91e3":"markdown","b7145c9b":"markdown","a1a0b76c":"markdown","d2148f6a":"markdown","1cbdd883":"markdown","a2da3ea6":"markdown","17036ab5":"markdown","5d6b1a9c":"markdown","7ac8fb9c":"markdown","973a73cd":"markdown","437ec5c3":"markdown","dd9900a4":"markdown","8254ba73":"markdown","bcd4071a":"markdown","1c650943":"markdown","1dca04e9":"markdown","660d12dc":"markdown","9ba17b5f":"markdown","425ca9f9":"markdown","512c802f":"markdown","dba302ae":"markdown","6511f1bd":"markdown","c57e9288":"markdown","7f3ac4ba":"markdown","88d93343":"markdown","3b0b47d8":"markdown","7525f93a":"markdown"},"source":{"130f43e6":"# Directive pour afficher les graphiques dans Jupyter\n%matplotlib inline","ae8e1340":"# Pandas : librairie de manipulation de donn\u00e9es\n# NumPy : librairie de calcul scientifique\n# MatPlotLib : librairie de visualisation et graphiques\n# SeaBorn : librairie de graphiques avanc\u00e9s\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","fd3b943f":"# Lecture des donn\u00e9es d'apprentissage et de test\nt = pd.read_csv(\"..\/input\/titanic\/train.csv\")","03ba6978":"t.head().T","ac8b33f4":"t.describe()","70a039e1":"t.Sex.value_counts()      # nombre d'hommes et de femmes","58a7f278":"t.Sex.count()              # nombre total hommes+femmes","7537f5eb":"t.Cabin.count()","dcf2dcae":"t.count()                  # Comptage par colonnes","6cad6534":"t[np.isnan(t.Age)].Survived.value_counts()","688062a1":"hommes = (t.Sex==\"male\")","600bb827":"t[hommes].head()        # t[hommes] est le tableau o\u00f9 on ne retient que lignes pour lesquelles hommes est True","79145f13":"t[hommes].Survived.value_counts()","26f3ef8a":"femmes = t.Sex==\"female\"\nclasse1 = t.Pclass == 1\nclasse2 = t.Pclass == 2\nclasse3 = t.Pclass == 3\nsurvivant = t.Survived == 1\nmort = ~ survivant","89b2929f":"jack = hommes & classe3\nrose = femmes & classe1","77644d1b":"p_jack = t[jack & survivant].Sex.count()\/t[jack].Sex.count()\nprint(p_jack)","fc32aebe":"p_rose = t[rose & survivant].Sex.count()\/t[rose].Sex.count()\nprint(p_rose)","2e3d481b":"fig = sns.FacetGrid(t, hue=\"Survived\", aspect=3, palette=\"Set2\")\nfig.map(sns.kdeplot, \"Age\", shade=True)\nfig.add_legend()","9522ebbc":"fig = sns.FacetGrid(t, hue=\"Pclass\", aspect=3, palette=\"Set2\")\nfig.map(sns.kdeplot, \"Fare\", shade=True)\nfig.add_legend()","df0d6931":"fig = sns.FacetGrid(t, hue=\"Pclass\", aspect=3, palette=\"Set2\")\nfig.map(sns.kdeplot, \"Age\", shade=True)\nfig.add_legend()","1f12834d":"t.columns","c0f70fca":"# On \u00e9limine les colonnes non pertinentes pour la pr\u00e9diction\ntitanic = t.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)","626a6a53":"titanic.count()","927ef338":"titanic[np.isnan(titanic.Age)]","fdc596f5":"titanic1 = titanic.fillna(value = {'Age':titanic.Age.mean()})","412ff3c7":"plt.hist(titanic1.Age, bins=80)","15f3d7ef":"titanic = titanic.fillna(method='pad')","8a3053dc":"titanic = titanic.fillna(method='pad')","fc56187e":"titanic.count()","2b88f95d":"plt.hist(titanic.Age, bins=80)","0c0a19e0":"sns.distplot(titanic.Fare, color='blue')","a2e3aa75":"titanic['log_fare'] = np.log(titanic.Fare+1)","b5bc6139":"sns.kdeplot(titanic.log_fare, color='blue')","244335df":"titanic = titanic.drop(['Fare'], axis=1)","d3343b98":"titanic[['Age','log_fare']].describe()","b0ec1928":"sns.kdeplot(titanic.log_fare, color='blue')\nsns.kdeplot(titanic.Age, color='red')","f2e39c6f":"from sklearn import preprocessing","32979482":"minmax = preprocessing.MinMaxScaler(feature_range=(0, 1))\ntitanic[['Age', 'log_fare']] = minmax.fit_transform(titanic[['Age', 'log_fare']])","ef137a68":"sns.distplot(titanic.log_fare, color='blue')\nsns.distplot(titanic.Age, color='red')","ac339e58":"scaler = preprocessing.StandardScaler()\ntitanic[['Age', 'log_fare']] = scaler.fit_transform(titanic[['Age', 'log_fare']])","e1a3b9fb":"sns.kdeplot(titanic.log_fare, color='blue')\nsns.kdeplot(titanic.Age, color='red')","c6432618":"titanic.info()","3565e901":"titanic.Sex = titanic.Sex.map({\"male\":0, \"female\":1})","8d2256cb":"titanic = pd.get_dummies(data=titanic, columns=['Pclass', 'Embarked'])","db1c8f66":"titanic.head()","7635724c":"X = titanic.drop(['Survived'], axis=1)\ny = titanic.Survived","90e76d08":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","01c62155":"print(X_train.shape)\nprint(X_test.shape)","f6e02751":"from sklearn.linear_model import LogisticRegression","8b023a55":"lr = LogisticRegression()\nlr.fit(X_train,y_train)","b8bcee39":"y_lr = lr.predict(X_test)","13e36654":"# Importation des m\u00e9thodes de mesure de performances\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score","fb3e5ed5":"print(confusion_matrix(y_test,y_lr))","1531206a":"print(accuracy_score(y_test,y_lr))","d7357bf9":"print(classification_report(y_test, y_lr))","d02e62e6":"probas = lr.predict_proba(X_test)","d611007a":"print(probas)","f49c2bd2":"dfprobas = pd.DataFrame(probas,columns=['proba_0','proba_1'])\ndfprobas['y'] = np.array(y_test)","6c03dcb1":"dfprobas","3605c39f":"plt.figure(figsize=(10,10))\nsns.distplot(1-dfprobas.proba_0[dfprobas.y==0], bins=50)\nsns.distplot(dfprobas.proba_1[dfprobas.y==1], bins=50)","928ba43e":"false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,probas[:, 1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint (roc_auc)","9b7a816b":"plt.figure(figsize=(12,12))\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')        # plus mauvaise courbe\nplt.plot([0,0,1],[0,1,1],'g:')     # meilleure courbe\nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","c85df748":"from sklearn import ensemble\nrf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)","521a3c2e":"print(classification_report(y_test, y_rf))","98ba9b96":"cm = confusion_matrix(y_test, y_rf)\nprint(cm)","cac5c489":"rf1 = ensemble.RandomForestClassifier(n_estimators=10, min_samples_leaf=10, max_features=3)\nrf1.fit(X_train, y_train)\ny_rf1 = rf.predict(X_test)\nprint(classification_report(y_test, y_rf1))","f3f516b4":"from sklearn.model_selection import validation_curve\nparams = np.arange(1, 300,step=30)\ntrain_score, val_score = validation_curve(rf, X, y, 'n_estimators', params, cv=7)\nplt.figure(figsize=(12,12))\nplt.plot(params, np.median(train_score, 1), color='blue', label='training score')\nplt.plot(params, np.median(val_score, 1), color='red', label='validation score')\nplt.legend(loc='best')\nplt.ylim(0, 1)\nplt.xlabel('n_estimators')\nplt.ylabel('score');","1230b0d7":"from sklearn import model_selection","62ce294e":"param_grid = {\n              'n_estimators': [10, 100, 500],\n              'min_samples_leaf': [1, 20, 50]\n             }\nestimator = ensemble.RandomForestClassifier()\nrf_gs = model_selection.GridSearchCV(estimator, param_grid)","e7f3b4ed":"rf_gs.fit(X_train, y_train)","e2bf1989":"print(rf_gs.best_params_)","dcac8543":"rf2 = rf_gs.best_estimator_","6cbd1074":"y_rf2 = rf2.predict(X_test)","1fd92714":"print(classification_report(y_test, y_rf2))","3b15f9aa":"importances = rf2.feature_importances_\nindices = np.argsort(importances)","543d6b44":"plt.figure(figsize=(8,5))\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), X_train.columns[indices])\nplt.title('Importance des caracteristiques')","c87ccfa4":"# Sous Jupyter, si xgboost n'est pas d\u00e9j\u00e0 install\u00e9\n!pip install xgboost","21d8c7f0":"import xgboost as XGB\nxgb  = XGB.XGBClassifier()\nxgb.fit(X_train, y_train)\ny_xgb = xgb.predict(X_test)\ncm = confusion_matrix(y_test, y_xgb)\nprint(cm)\nprint(classification_report(y_test, y_xgb))","96fecbde":"<img src=\"https:\/\/i.stack.imgur.com\/gKyb9.png\">","e4b1599c":"On voit qu'il manque des donn\u00e9es, en particulier pour la colonne *'age'*  \nIl existe plusieurs approches pour compl\u00e9ter les donn\u00e9es manquantes :  \n- **suppression** des donn\u00e9es manquantes (par exemple avec la fonction *dropna*). C'est une m\u00e9thode simple, mais qui \u00e9limine de l'information\n- **remplacement** des donn\u00e9es manquantes. Par exemple, on pourrait remplacer les informations manquantes pour l'\u00e2ge par la moyenne de la colonne (mais on introduit un biais sur cette valeur), ou par un nombre al\u00e9atoire g\u00e9n\u00e9r\u00e9 par une loi normale de m\u00eame moyenne et variance ...\n- **estimation** des param\u00e8tres manquants avec une m\u00e9thode de pr\u00e9diction (par exemple avec une r\u00e9gression)","ed385877":"# Rose & Jack","31b7a630":"L'attribut *feature_importances_* renvoie un tableau du poids de chaque caract\u00e9ristique dans la d\u00e9cision :","01709f21":"## Conditionnement des donn\u00e9es","666b478e":"On met les probabilit\u00e9s de pr\u00e9diction de la valeur 1 dans un dataframe, avec les valeurs effectives, pour faciliter la visualisation :","bf83fd6f":"## Exercice : appliquer les m\u00e9thodes sur le dataset *Indian Diabete*","c1eed12b":"On peut compter les hommes survivants ou non :","e1273bfa":"Que peut-on dire des voyageurs de 1ere, 2eme et 3eme classe ?","7532d6b5":"Tracer l'histogramme des \u00e2ges. Qu'observez-vous ?","292c450c":"Il manque des valeurs, par exemple pour la colonne *age*","e310880d":"On affiche la distribution des probabilit\u00e9s de pr\u00e9diction de 1, et celle des non probabilit\u00e9s de pr\u00e9diction de 0 :","4717b69b":"Ici on a choisi des valeurs pour le nombres d'arbres dans la for\u00eat al\u00e9atoire (*'n_estimators'*) et le nombre minimum d'\u00e9chantillons pour une feuille. On pourrait tester d'autres valeurs, et d'autres param\u00e8tres, cf :  \nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html","591b035a":"On remarque qu'il manque des valeurs pour 'age' et 'embarked' (pr\u00e9sence de valeurs ind\u00e9finies 'NaN')","92d4f56e":"## Mesures de performance","a0e4eeb7":"<img src = \"http:\/\/scikit-learn.org\/0.16\/_static\/ml_map.png\">","8571aec1":"## Ajustement des hyperparam\u00e8tres (Random Forests)","a72e74a0":"Sous Anaconda prompt :\n*pip install xgboost*  \n(d\u00e9j\u00e0 disponible sous Kaggle)","54cc8cbd":"## Cr\u00e9ation des jeux d'apprentissage et de test","95eafe24":"## Interpr\u00e9tation des param\u00e8tres","0611096b":"## XGBoost","4fe7594e":"*value_counts* permet de compter le nombre d'\u00e9l\u00e9ments par cat\u00e9gorie d'une s\u00e9rie","fe942d8a":"La fonction *fillna* permet de compl\u00e9ter simplement les param\u00e8tres manquants. ","7e980f52":"Tracer les courbes de distribution de l'\u00e2ge selon la classe (utiliser *FacetGrid*)","bce5bf4d":"## Exercice : explorer d'autres m\u00e9thodes de classification","00125ba7":"Certaines distributions sont d\u00e9s\u00e9quilibr\u00e9es, et \u00e9loign\u00e9es d'une loi normale :","6cdc1795":"On utilise la fonction *get_dummies* de Pandas pour transformer les colonnes multimodales (par exemple 'embarked') en plusieurs colonnes binaires (par exemple 'embarked_C' dont les valeurs sont 1 si le passager a embarqu\u00e9 \u00e0 Cherbourg et 0 sinon) :","26b7a297":"On s\u00e9lectionne le meilleur estimateur :","3334bc6e":"## Lecture des donn\u00e9es","75969afe":"On teste les for\u00eats al\u00e9atoires :","ceff42f0":"Appliquer une r\u00e9gression logistique pour classifier sur l'ensemble de test","ab10c6b8":"Eliminer les colonnes non pertinentes pour la pr\u00e9diction (on peut utiliser une liste de colonnes dans *drop*), et placer le r\u00e9sultat dans la variable *titanic* :","ce580b23":"Jack est un homme en 3\u00e8me classe, et Rose une femme en 1\u00e8re (d\u00e9finir les bool\u00e9ens *jack* et *rose*) :","fbae9af7":"On lance l'entrainement :","bbba5f9c":"La m\u00e9thode XGBoost est d\u00e9riv\u00e9e des arbres de d\u00e9cision, et tr\u00e8s efficace, en particulier pour de grandes quantit\u00e9s de donn\u00e9es.","d6725089":"*validation_curve* permet de tracer la courbe du score sur un ensemble d'apprentissage et sur un ensemble de test (*cross validation*), en faisant varier un param\u00e8tre, par exemple *n_estimators* :","29b29287":"La **matrice de confusion** permet de compter les vrais positifs, faux positifs, ...","a76466f4":"Tracer l'histogramme pour *age* :","19e5a46d":"### Importance des caract\u00e9ristiques","21e1cedc":"La pertinence (ou *accuracy*) mesure le nombre de bonnes pr\u00e9dictions sur le nombre total d'observations","7b44b99c":"L'option *method='pad'* permet d'utiliser la pr\u00e9c\u00e9dente valeur non manquante :","c0e1def2":"Tracer diff\u00e9rentes repr\u00e9sentations du dataset","5f497a3f":"# Le Titanic","608e9636":"*predict_proba* donne un tableau de couples de probabilit\u00e9s : *[probabilit\u00e9 de pr\u00e9diction 0, probabilit\u00e9 de pr\u00e9diction 1]*","6dd4cc42":"On peut voir les param\u00e8tres s\u00e9lectionn\u00e9s et le score :","f516e3e8":"D\u00e9finir les bool\u00e9ens pour *femmes, classe1, classe2, classe3, survivant, ...*","ca132aec":"Les valeurs inconnues sont affich\u00e9es comme **NaN** (*Not a Number*).  \nOn peut tester si une valeur est **NaN** avec la fonction *np.isnan(valeur)*  \nAfficher les lignes pour lesquelles l'\u00e2ge est inconnu :","2eb249af":"On s\u00e9pare le dataset en deux parties :\n- un ensemble d'apprentissage (entre 70% et 90% des donn\u00e9es), qui va permettre d'entra\u00eener le mod\u00e8le\n- un ensemble de test (entre 10% et 30% des donn\u00e9es), qui va permettre d'estimer la pertinence de la pr\u00e9diction","d24a6954":"Afficher la matrice de confusion :","89c1a609":"On a am\u00e9lior\u00e9 la performance du mod\u00e8le","9ff12253":"## R\u00e9gression logistique","596205f6":"Calculer la probabilit\u00e9 de survie de Jack :","bab597ec":"### Donn\u00e9es manquantes","4fae91e3":"Calculer la probabilit\u00e9 de survie de Rose :","b7145c9b":"La distribution id\u00e9ale permet de s\u00e9parer totalement la pr\u00e9diction des positifs et n\u00e9gatifs :  \n<img src=\"https:\/\/miro.medium.com\/max\/660\/1*Uu-t4pOotRQFoyrfqEvIEg.png\">\nLe cas le plus d\u00e9favorable consiste en une distribution \u00e9quivalente pour les positifs et les n\u00e9gatifs :  \n<img src=\"https:\/\/miro.medium.com\/max\/538\/1*iLW_BrJZRI0UZSflfMrmZQ.png\">","a1a0b76c":"## Exercice : tester diff\u00e9rentes visualisations sur le dataset","d2148f6a":"Dans ce cas, une transformation log peut am\u00e9liorer l'\u00e9quilibre :","1cbdd883":"**Exercice** : tracer les courbes de validation pour les param\u00e8tres *min_samples_leaf* et *max_features* (attention pour ce dernier, le nombre max est le nombre de caract\u00e9ristiques \/ colonnes du tableau)","a2da3ea6":"N\u00e9anmoins cette mesure peut \u00eatre fauss\u00e9e dans certains cas, en particulier si le nombre de 0 et de 1 est d\u00e9s\u00e9quilibr\u00e9.\nOn a donc d'autres estimateurs :\n- la **pr\u00e9cision** est le nombre de pr\u00e9dictions positives correctes sur le nombre total de pr\u00e9dictions positives : *precision = VP\/(VP+FP)*\n- la **sensibilit\u00e9** (*recall*) est le nombre de pr\u00e9dictions positives sur le nombre effectif de \"oui\" : *recall = VP:(VP+FN)*\n- le **score F1** est la moyenne pond\u00e9r\u00e9e de la pr\u00e9cision et de la sensibilit\u00e9 : *f1-score = 2xprecisionxrecall\/(precision+recall)*","17036ab5":"On utilise ces distributions pour construire la **courbe ROC** (Receiving Operator Characteristic) qui repr\u00e9sente le taux de vrais positifs par rapport aux taux de faux positifs.  \nLa mesure de l'aire sous la courbe **AUC** (Area Under Curve) est un bon indicateur de performance  \nPour plus de d\u00e9tails : http:\/\/www.xavierdupre.fr\/app\/mlstatpy\/helpsphinx\/c_metric\/roc.html","5d6b1a9c":"La plupart des algorithmes ont besoin de donn\u00e9es num\u00e9riques, et n'acceptent pas les cha\u00eenes de caract\u00e8res :","7ac8fb9c":"## D\u00e9s\u00e9quilibre des distributions","973a73cd":"### Encodage binaire des donn\u00e9es qualitatives (*one hot encoding*)","437ec5c3":"La distribution des \u00e2ges n'est pas significativement modifi\u00e9e ...","dd9900a4":"On peut normaliser les valeurs min et \u00e0 max (valeurs ramen\u00e9es entre 0 et 1) :","8254ba73":"On peut visualiser ces degr\u00e9s d'importance avec un graphique \u00e0 barres par exemple :","bcd4071a":"On peut d\u00e9finir un bool\u00e9en pour abr\u00e9ger une caract\u00e9ristique :","1c650943":"La librairie *sklearn* comporte une librairie de pr\u00e9traitement des donn\u00e9es","1dca04e9":"On peut \u00e9galement utiliser le *StandardScaler* pour ramener la moyenne \u00e0 0 et l'\u00e9cart type \u00e0 1 :","660d12dc":"La m\u00e9thode *GridSearchCV* permet de tester plusieurs combinaisons de param\u00e8tres (list\u00e9s dans une grille de param\u00e8tres) et de s\u00e9lectionner celle qui donne la meilleure pertinence","9ba17b5f":"On voit qu'il y a une forte diff\u00e9rente de distribution entre les deux s\u00e9ries.  \nCertains algorithmes demandent une distribution normalis\u00e9e. Pour une discussion d\u00e9taill\u00e9e sur ce sujet, cf par exemple :  \nhttp:\/\/www.faqs.org\/faqs\/ai-faq\/neural-nets\/part2\/section-16.html  \nhttp:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html","425ca9f9":"En comparant les valeurs pr\u00e9dites et les valeurs r\u00e9elles, on a plusieurs possibilit\u00e9s :\n- *Vrais positifs* (VP ou TP) : on pr\u00e9dit \"oui\" et la valeur attendue est \"oui\"\n- *Vrais n\u00e9gatifs* (VN ou TN) : on pr\u00e9dit \"non\" et la valeur attendue est \"non\"\n- *Faux positifs* (FP) : on pr\u00e9dit \"oui\" et la valeur attendue est \"non\"\n- *Faux n\u00e9gatifs* (FN) : on pr\u00e9dit \"non\" et la valeur attendue est \"oui\"\n\nPar exemple, si veut pr\u00e9dire le d\u00e9c\u00e8s, le nombre de vrais positifs est le nombre de fois o\u00f9 on a pr\u00e9dit 0 pour des passagers effectivement morts sur le Titanic (*survived = 0*)","512c802f":"Globalement plus la classe est proche de 1 plus les passagers sont ag\u00e9s.\nAussi on remarque qu'il y a beaucoup d'enfant en bas age en 2 et 3 i\u00e8me classe. Ce qui d\u00e9forme la gaussienne. \n\nC'est peut-\u00eatre la trace d'un manque de donn\u00e9es.","dba302ae":"### Mise \u00e0 l'\u00e9chelle des donn\u00e9es quantitatives","6511f1bd":"<img src=\"https:\/\/www.scienceabc.com\/wp-content\/uploads\/2016\/04\/titanic-jack-and-rose-plank-scene.webp\">","c57e9288":"Cr\u00e9er les jeux d'apprentissage et de test","7f3ac4ba":"## Exercice : quelle est la probabilit\u00e9 de survie de Rose et Jack ?","88d93343":"Parmi les hyperparam\u00e8tres de l'algorithme qui peuvent avoir un impact sur les performances, on a :\n- **n_estimators** : le nombre d'arbres de d\u00e9cision de la for\u00eat al\u00e9atoire\n- **min_samples_leaf** : le nombre d'\u00e9chantillons minimum dans une feuille de chaque arbre\n- **max_features** : le nombre de caract\u00e9ristiques \u00e0 prendre en compte lors de chaque split\n\nPour chaque algorithme de *sklearn*, on peut trouver la liste des param\u00e8tres dans la documentation, avec des exemples :  \nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html  ","3b0b47d8":"- survived - Survival (0 = No; 1 = Yes)\n- pclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n- name - Name\n- sex - Sex\n- age - Age\n- sibsp - Number of Siblings\/Spouses Aboard\n- parch - Number of Parents\/Children Aboard\n- ticket - Ticket Number\n- fare - Passenger Fare\n- cabin - Cabin\n- embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n","7525f93a":"http:\/\/scikit-learn.org\/0.16\/tutorial\/machine_learning_map\/index.html"}}